{
  "title": "HogRider: Champion Agent of Microsoft Malmo Collaborative AI Challenge",
  "url": "https://openalex.org/W2788984134",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A3000373745",
      "name": "Yanhai Xiong",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A2105556960",
      "name": "Haipeng Chen",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A2562723257",
      "name": "Mengchen Zhao",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A2120220493",
      "name": "Bo An",
      "affiliations": [
        "Nanyang Technological University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2136418668",
    "https://openalex.org/W6624680365",
    "https://openalex.org/W6675807139",
    "https://openalex.org/W6688601061",
    "https://openalex.org/W2139993574",
    "https://openalex.org/W6758929085",
    "https://openalex.org/W2103437045",
    "https://openalex.org/W6642410368",
    "https://openalex.org/W2395575420",
    "https://openalex.org/W2202744812",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W6681371452",
    "https://openalex.org/W2604925955",
    "https://openalex.org/W2120182648",
    "https://openalex.org/W1932734735",
    "https://openalex.org/W2029280655",
    "https://openalex.org/W1607392272",
    "https://openalex.org/W6636893639",
    "https://openalex.org/W2097381042",
    "https://openalex.org/W2740302738",
    "https://openalex.org/W6775686901",
    "https://openalex.org/W1606056663",
    "https://openalex.org/W2146544759",
    "https://openalex.org/W3011120880",
    "https://openalex.org/W1968092545",
    "https://openalex.org/W938188971",
    "https://openalex.org/W4302285995",
    "https://openalex.org/W2215257094",
    "https://openalex.org/W2104357911",
    "https://openalex.org/W2168356773",
    "https://openalex.org/W2911747849",
    "https://openalex.org/W1657704689",
    "https://openalex.org/W2746247060",
    "https://openalex.org/W2963000099"
  ],
  "abstract": "It has been an open challenge for self-interested agents to make optimal sequential decisions in complex multiagent systems, where agents might achieve higher utility via collaboration. The Microsoft Malmo Collaborative AI Challenge (MCAC), which is designed to encourage research relating to various problems in Collaborative AI, takes the form of a Minecraft mini-game where players might work together to catch a pig or deviate from cooperation, for pursuing high scores to win the challenge. Various characteristics, such as complex interactions among agents, uncertainties, sequential decision making and limited learning trials all make it extremely challenging to find effective strategies. We present HogRider---the champion agent of MCAC in 2017 out of 81 teams from 26 countries. One key innovation of HogRider is a generalized agent type hypothesis framework to identify the behavior model of the other agents, which is demonstrated to be robust to observation uncertainty. On top of that, a second key innovation is a novel Q-learning approach to learn effective policies against each type of the collaborating agents. Various ideas are proposed to adapt traditional Q-learning to handle complexities in the challenge, including state-action abstraction to reduce problem scale, a warm start approach using human reasoning for addressing limited learning trials, and an active greedy strategy to balance exploitation-exploration. Challenge results show that HogRider outperforms all the other teams by a significant edge, in terms of both optimality and stability.",
  "full_text": "HogRider: Champion Agent of Microsoft\nMalmo Collaborative AI Challenge\nYanhai Xiong,∗ Haipeng Chen,∗Mengchen Zhao, Bo An\nNanyang Technological University\n{yxiong003,chen0939,zhao0204,boan}@ntu.edu.sg\nAbstract\nIt has been an open challenge for self-interested agents to\nmake optimal sequential decisions in complex multiagent\nsystems, where agents might achieve higher utility via col-\nlaboration. The Microsoft Malmo Collaborative AI Challenge\n(MCAC), which is designed to encourage research relating\nto various problems in Collaborative AI, takes the form of a\nMinecraft mini-game where players might work together to\ncatch a pig or deviate from cooperation, for pursuing high\nscores to win the challenge. V arious characteristics, such as\ncomplex interactions among agents, uncertainties, sequential\ndecision making and limited learning trials all make it ex-\ntremely challenging to ﬁnd effective strategies. We present\nHogRider - the champion agent of MCAC in 2017 out of 81\nteams from 26 countries. One key innovation of HogRider is\na generalized agent type hypothesis framework to identify the\nbehavior model of the other agents, which is demonstrated to\nbe robust to observation uncertainty. On top of that, a second\nkey innovation is a novel Q-learning approach to learn ef-\nfective policies against each type of the collaborating agents.\nV arious ideas are proposed to adapt traditional Q-learning to\nhandle complexities in the challenge, including state-action\nabstraction to reduce problem scale, a warm start approach\nusing human reasoning for addressing limited learning tri-\nals, and an active greedy strategy to balance exploitation-\nexploration. Challenge results show that HogRider outper-\nforms all the other teams by a signiﬁcant edge, in terms of\nboth optimality and stability.\nIntroduction\nIn complex multiagent systems, sequential decision making\nhas long been a signiﬁcant challenge with many characteris-\ntics. One prominent characteristic is theinteractions among\nthe agents. In many practical scenarios (e.g., the Stag-Hunt\ngame), agents need tocooperate with each other to achieve a\ncommon goal with high rewards (e.g., hunting stags), while\neach individual agent is self-interested and might deviate\nfrom the cooperation for less risky goals with low rewards\n(e.g., hunting rabbits). Another critical characteristic comes\nfrom various uncertainties. One type of uncertainty arises\nfrom the lack of accurate knowledge of environment and\n∗Denotes equal contribution.\nCopyright c⃝ 2018, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nother agents where the uncertain information can be mod-\nelled probabilistically. A more challenging type of uncer-\ntainty lies in the environment-related factors which we do\nnot know how to model.\nThese two challenges are signiﬁcantly ampliﬁed when it\ncomes tosequential decision making, where we need to look\nat not only short term rewards but also rewards in the long\nrun. Therefore, one has to consider subsequent effect of the\ncurrent action, especially in a dynamic environment. An-\nother crucial characteristic is thelimited learning trials.O n\nthe Minecraft platform, it usually takes several seconds to\ncomplete one episode of game. Therefore, it is extremely\ntime consuming to learn an effective policy.\nThe Microsoft Malmo Collaborative AI Challenge\n(MCAC), which is designed to encourage research relat-\ning to various problems in Collaborative AI, builds on a\nMinecraft mini-game (Mojang 2017) called “Pig Chase”.\nPig Chase is played on a 9 × 9 grid where agents can ei-\nther work together to catch the pig and achieve high scores,\nor give up cooperation and achieve low scores. After play-\ning certain episodes (e.g., 100) of games, the agent who\nachieves the highest average scores wins the challenge. De-\nspite its simple rules, this game has all the above stated key\ncharacteristics. Though there are numerous papers study-\ning sequential decision making in complex multiagent sys-\ntems, they only address a subset of these characteristics. We\nhope to shed some light on solving this class of problems\nby presenting HogRider, a champion agent which won 2017\nMicrosoft Malmo Collaborative AI Challenge (Kuno and\nSchwiderski-Grosche 2017), with the following key contri-\nbutions.\nFirst, via enormous simulations, we provide thorough\nanalysis of the Minecraft game, and identify its key chal-\nlenges. Speciﬁcally, we discover several important aspects\nof the game that are not revealed by the MCAC rule, such\nas the pig’s uncertain behavior model, the uncertainty in ob-\nserving the other agent’s actions, among others.\nSecond, we propose a novel agent type hypothesis ap-\nproach for dealing uncertainties about the type of the other\nagent and the observation of the other agent’s actions. When\nforming ad hoc teams, it is important to identify the types of\nthe collaborating agents. A typical approach is to maintain\na belief of their type, and update it based on the actions of\nthe other agents using Bayes theorem (Barrett, Stone, and\nThe Thirty-Second AAAI Conference\non Artificial Intelligence (AAAI-18)\n4767\nKraus 2011). However, we discover that the actions of the\nother agents could be incorrectly observed. To this end, we\nderive two adaptations to the traditional Bayes theorem to\ncope with this observation uncertainty.\nThird, we come up with a novel Q-learning framework to\nlearn an optimal policy against each type of agent, with the\nfollowing novelties. First, state-action abstraction is utilized\nto reduce the huge state-action space. Besides, in classical\ntabular Q-learning, Q-values are usually randomly initial-\nized, which requires a particularly large number of episodes\nfor training. While it takes several seconds to complete an\nepisode on the Minecraft platform, it is extremely time con-\nsuming to train an effective policy. We propose a warm start\napproach to initialize the tabular Q-function, which utilizes\na decision tree framework derived from human reasoning.\nMoreover, we show that when learning trials are limited,\nrandom exploration of potential optimal actions is inefﬁcient\nand sometimes even harmful for searching optimal policies.\nTherefore, we propose an active-ε-greedy method to make\nbetter tradeoff between exploitation and exploration.\nLast, we conduct extensive experiments to evaluate\nHogRider. We demonstrate that by considering uncertainty\nin agent action observation, HogRider outperforms exist-\ning agent type belief update methods. We also show im-\nproved performance of HogRider compared with traditional\nQ-learning methods. Challenge results show that HogRider\noutperforms all the other teams by a signiﬁcant edge, in\nterms of both optimality and stability. Moreover, we show\nthat HogRider performs even better than human players.\nRelated Work\nOne line of related research is teamwork within a mul-\ntiagent system where agents cooperate through commu-\nnication (Tambe 1997; Grosz and Kraus 1996) or pre-\ncoordination (Horling et al. 1999). These methods assume\nfully cooperative agents and do not consider uncertainty\nabout cooperating agents’ types. Recently, research efforts\nhave been put on ad hoc interactions among agents, where\nagents need to interact without prior knowledge of the other\nagents (Stone et al. 2010). They study both cooperative (Bar-\nrett et al. 2013; Albrecht, Crandall, and Ramamoorthy 2016)\nand competitive (Southey et al. 2012; Albrecht, Crandall,\nand Ramamoorthy 2016) scenarios, where agent type hy-\npothesis and opponent modelling are used to handle the un-\ncertainty. However, they do not consider the uncertainty in\nagent action observation and limited learning trials.\nMulti-agent reinforcement learning (MARL) has also\nbeen applied to study interactions among a set of\nagents (Singh, Kearns, and Mansour 2000; Conitzer and\nSandholm 2007; Foerster et al. 2016). These approaches,\nfrom dealing with small to large scale problems, usually fo-\ncus on obtaining Nash equilibrium via self-play, or learning\noptimal strategies against stationary opponents.\nAnother line of research studies the sequential multiagent\nplanning problem via game theoretic approaches (Dunne et\nal. 2010; De Cote et al. 2012). Brafman et al. (2009) ap-\nply game theoretic solutions to coalition planning games\nand auction planning games. Jord´an and Onaindia (2015)\nFigure 1: Setting of MCAC. The left hand side of the ﬁgure\nis the ﬁrst person view of the game, while the right hand side\nof the ﬁgure is the symbolic global view.\npropose a two-game proposal for solving congestion games\namong a group of autonomous vehicles (Jord´an et al. 2017).\nSome other papers (Sandholm 2015; Hor´ak, Bosansk`y,\nand Pechoucek 2017) apply a stochastic game framework to\nmultiagent planning problems. Despite the theoretical mer-\nits, pure game theoretic approaches usually suffer from com-\nputational issues that arise from large-scale state and strat-\negy spaces. Other classical approaches to sequential plan-\nning in multiagent systems include decentralized partially\nobservable Markov decision process and its variants (Seuken\nand Zilberstein 2008; Agmon and Stone 2012). These ap-\nproaches also suffer from high computational complexity.\nChallenge Description and Analysis\nWe brieﬂy introduce the challenge background and analyze\nits key characteristics.\nSettings & Rules\nMCAC builds on a mini-game played within a9 × 9 grid\non the Minecraft platform (Figure 1). The green grids rep-\nresent the grass where agents can walk on, the yellow grids\nrepresent the blocks where agents cannot walk on or pass\nthrough, and the two black grids represent two exits. There\nare two agents and a pig (which can be viewed as part of\nthe environment). The red arrow represents the agent (RED)\nthat we are going to design, the blue arrow represents the\nagent (BLUE) that agent RED needs to play with, and the\npink dot represents a pig which needs to be caught. Thele-\ngal movements of the agents include: move forward (MF),\nturn left (TL) and turn right (TR). Agent BLUE is provided\nby MCAC, which is designed to be either aRandom agent\nor a F ocusedagent in each episode of game. A Random\nagent takes any of the three legal movements with an equal\nprobability of1/3, while aF ocusedagent always chases the\npig through the shortest path using A\n∗ search algorithm. In\neach episode of game, there is a probability of.25 for agent\nBLUE to beRandom, and a probability of.75 to beF ocused.\nThis type does not change within one episode of game.\nThe goal of the challenge is to achieve a high score in cer-\ntain episodes (e.g., 100 or 500) of games. After an episode\nstarted, two agents take actions in turn. If the pig is caught,\neach agent receives 25 points. To catch a pig, the pig has\nto be completely surrounded by blocks and the agents. Fig-\nure 2(a) shows an example where a pig can be caught by\na single agent, i.e., when the pig goes to the exit, and one\n4768\n(a) Catch a pig alone\n (b) Catch a pig together\nFigure 2: Two types of situations where a pig is caught\n2 4 6 8 10\n2\n4\n6\n8\n10\n(a)\n2 4 6 8 10\n2\n4\n6\n8\n10\n(b)\n2 4 6 8 10\n2\n4\n6\n8\n10\n0\n0.004\n0.008\n0.012\n0.016\n0.02\n(c)\nFigure 3: Probability distribution of the pig’s positions af-\nter agent RED takes one step, where the grid in the center\nis the original position of the pig, and darker color means\nhigher probability. In Figures 3(a)-3(c), agent RED respec-\ntively waits for (a) no time (b) 1 second, and (c) 3 seconds\nbefore it takes a step. Note the pig does have a very short\ntime (around 0.1 seconds) moving even if agent RED does\nnot wait. This time is the response time of the Minecraft\nplatform.\nagent goes to the only grass grid beside the exit. Figure 2(b)\nshows an example where a pig can only be caught by two\nagents. An alternative is to give up cooperation and go to the\nexit. In this case, only the agent whoﬁrst reaches the exit\ncan get 5 points. Our agent loses 1 point whenever one step\nis made, and one episode of game terminates as long as one\nof the following conditions is met: 1) the pig is caught, 2)\none agent reaches the exit, 3) 25 steps are taken, or 4) the\nmaximum game time (about 100 seconds) is used up.\nAnalysis and Key Challenges\nUncertain behavior model of the pig. The behavior model\nof the pig is crucial to generating efﬁcient policies, whereas\nit is not released by MCAC. We keep statistics of the pig’s\npositions after our agent takes one step. After10,000 steps,\nwe draw the probability distribution of the pig’s positions.\nAs shown in Figure 3, we can see that 1) the pig does not\nfollow the legal movements as human agents, and is able to\nmove multiple steps or even make turns when human agent\ntakes one step, 2) the probability of the pig moving to each\ndirection is almost equal, and 3) with longer waiting time\nbefore our agent RED takes a step, the higher probability\nthe pig moves. The statistics gives us a hint that it might be\nbeneﬁcial to wait for a few seconds when the pig is in a po-\nsition that is uncatchable, so that it might move to positions\nthat are catchable.\nUncertainties in the type of agent BLUE & observation of\nits actions. In each episode of game, agent BLUE has a.25\nand a .75 probability of beingRandom or F ocused, respec-\ntively. While correctly predicting its type relies on accurate\nobservation of its movements, we noticed that the action ob-\nserved (returned by the Minecraft simulator) has a probabil-\nity of around.25 to be incorrect. It would be highly risky if\nthis observation uncertainty is neglected.\nCoexistence of cooperation & self-interest.To catch a pig\n(25 points) in situations like in Figure 2(b), agent RED must\ncooperate with agent BLUE. Under some other situations,\nit might be more beneﬁcial to deviate from the cooperation\nand move to the exit (5 points), especially when agent RED\nis close to the door, or there are few steps left.\nSequential decision making. Apart from the above chal-\nlenges, our agent needs to sequentially make decisions dur-\ning each episode of game. This signiﬁcantly adds to the com-\nputational complexity of the problem.\nLimited learning trials.The simulations are performed on\nthe Minecraft platform, which usually takes several seconds\nfor one episode of game. As a result, it is extremely time\nconsuming to run a large number of simulations. For exam-\nple, if one episode runs for 5 seconds, it would take around\n6 days to train100,000 episodes.\n1\nSolution Approach\nIn this section, we present key ideas behind HogRider. First,\nwe design an agent type hypothesis framework to update the\nbelief over the types of agent BLUE, which is robust to un-\ncertainty in agent action observation. On top of that, we pro-\npose a novel Q-learning approach to learn an optimal policy\nagainst each type of agent BLUE.\nBelief Update for Agent Types\nFormally, we denote a set of agent types as T =\n{T} = {Random,Focused }. We have a prior probabil-\nity distribution over the agent types, i.e., P(Random)=\n0.25,P (Focused)=0 .75. At each position, we also know\nthe conditional probabilityP(a|T) of a typeT agent taking\nmovement a ∈{ MF,TL,TR }. According to the Bayes\ntheorem, the posterior probability of an agent being typeT\nwhen it takes movementa is:\nP(T|a)= P(a|T) ·P(T)\nP(a) , (1)\nwhere P(a) is the probability of movementa being taken.\nGeneralizing Bayes theorem.As the movements of agent\nBLUE might be incorrectly observed with a probability of\nP\nu ≈ .25. We now derive the belief update rule which con-\nsiders the uncertainty in observing agents’ actions.\n1As we will introduce in the next section, this number is smaller\nthan half of the state-action space size. An alternative to the\nMinecraft platform is to build our own simulator by formalizing\na model for the game. However, the uncertainties discussed above\ncannot be accurately modelled.\n4769\nTheorem 1. With the existence of uncertainty in observing\nthe action of agent BLUE, the agent belief update rule is\nP(T|a)= P(T)·[(1−Pu)·P(a|T)+Pu·P(a|u)]\n(1−Pu)·∑\nT ∈T P(a|T)·P(T)+Pu·P(a|u)\n= P(T)ϕ(T,a ) (for simplicity) (2)\nwhere P(a|u) is the conditional probability of movementa\nbeing observed when the observation is incorrect.\nProof. With observation uncertainty, we rewriteP(a|T) as\nP′(a|T)=( 1 −Pu) ·P(a|T)+ Pu ·P(a|u). (3)\nSimilarly, the probability P(a) of an action being taken is\nreformulated as\nP′(a)=( 1 −Pu) ·\n∑\nT∈T\nP(a|T) ·P(T)+ Pu ·P(a|u). (4)\nSubstituting Eqs.(3)-(4) to Eq.(1), we obtain Eq.(2).\nSquash Bayes update with hyperbolic tangent function.\nEven with the generalization to Bayes theorem, a single\nwrong action observation could drastically drop the pos-\nterior probability of the correct agent type. To mitigate\nthis issue, we propose a more conservative update method,\nwhich uses a shifted hyperbolic tangent functionsq(x)=\ntanh(x−1)+1 =\ne2(x−1)−1\ne2(x−1)+1 +1 to squash the factorϕ(T,a )\nto a value between0.5 and 2, such that the agent type belief\nis updated to a new value within(0.5,2) times of the original\nvalue. Formally, the Squash Bayes update function is:\nP(T|a)= P(T)ϕ′(T,a ), (5)\nwhere\nϕ′(T,a )=\n{sq(ϕ(T,a )),ϕ (T,a ) ≥ 1\n1\nsq(1/ϕ(T,a)) ,ϕ (T,a ) < 1 . (6)\nAdapt Q-Learning to Various Challenges\nQ-learning (Watkins 1989) is a model-free reinforcement\nlearning technique for ﬁnding optimal policies in sequential\ndecision making problems. We adapt Q-learning to deal with\nvarious complexities in the problem. Formally, we formulate\nthe Minecraft game as a Markov Decision Process (MDP).\nThe state s ∈S is deﬁned as a four-dimension vector, which\nincludes the position and facing direction of agent RED, the\nposition and facing direction of agent BLUE, the position\nof the pig (the pig’s behavior does not depend on its facing\ndirection), as well as the steps that have already been taken.\nThe actionA ∈A of the MDP is a tupleA = ⟨w,a⟩, where\nw is a binary variable indicating whether HogRider needs\nto wait for a certain time period (usually 3 seconds), anda\nis the legal movement after the wait option.\n2 A Q-function\nmaps a state action pair(s,A) into real values:\nQ : S×A→R (7)\nA policy π(A|s) is a mapping from any state s to an ac-\ntion A that should be taken in that state. In Q-learning, an\n2Note that action is different from the legalmovement in the\nsense that it has one more dimension of variablew.\nimportant challenge is to perform the “exploitation” of the\ncurrent optimal action, while balancing the “exploration”\nof potentially better actions. For example, in theε-greedy\npolicy, it selects the action with the largest Q-value (i.e.,\nA\n∗ =a r g m a xQ(s,A)) with probability1 −ε, and selects\na random action with probabilityε.\nUsually, Q-learning starts with a randomized initializa-\ntion of Q-function Q(s,A). It then generates an episode\n(e.g., one round of game on the Minecraft platform) by per-\nforming actions derived from the policy, while updating Q-\nfunction with a stochastic gradient descent method, using\nthe state-action values returned from the generated episode.\nQ-learning is guaranteed to converge after sufﬁcient obser-\nvations of each action at each state (Watkins 1989).\nState-Action Abstraction. For each agent, the number of\nits feasible positions is25−4=2 1 and there are 4 directions\nfor each position. For the pig, since it can stay at the two ex-\nits, the number of feasible positions is 23. Consequently, the\npossible combinations of the agents and the pig’s positions\nare 21 × 4 × 21 × 4 × 23 = 162,288. Taking the last di-\nmension of state (i.e., the number of steps have already been\ntaken) into consideration, the possible number of states is\n162,288×25 = 4,057,200, which is a huge number consid-\nering the average running time for an episode. While playing\nthe game, we notice that:\nIt is the relative positions (i.e., distances) between the\nagents, the pig and the exits, rather than the absolute po-\nsitions, that determines a player’s strategy.\nWe propose a state abstraction method based on the intu-\nition. An abstracted state has ﬁve dimensions, i.e.,PigStat\nwhich indicates whether the pig is uncatchable (PigStat =\n0), catchable by a single agent (PigStat =1 ) or catchable\nby 2 agents (PigStat =2 ), d(R,P ) which indicates the\ndistance between agent RED and the pig,d(B,P ) which de-\nnotes the distance between agent BLUE and the pig,d(R,E)\nwhich denotes the distance between agent RED and the near-\nest exit, and step which denotes the number of steps that\nhave been taken. Since PigStat has three possible states,\nthe distance of an agent to the pig ranges from 0 to 9, and\nthe distance of agent RED to the nearest exit ranges from 0\nto 7, the total number of states in the abstracted state repre-\nsentation is3×\n10×10×8×25 = 60,000, which is less than\n15% of the original state space. To transform from the orig-\ninal representation of states to the new one, we only need\nto calculate the three types of distances based on the origi-\nnal state representation, the calculation overhead of which is\ntrivial.\nOne issue raised from the abstracted state representation\nis that, in the new state space, we ignore the direction of\nagent RED, making it infeasible to directly decide its legal\nmovement. To handle this issue, we propose a corresponding\naction representation, i.e., a binary variablew which is the\nsame as the original action representation, and a binary vari-\nable a\n′ which indicates whether to chase the pig or to head\nfor the exit. To translate the new action representation to the\noriginal one, we only need to relate to the position of agent\nRED, the computation overhead of which is also negligible.\nMoreover, the action space is reduced from2 × 3=6 to\n2 × 2=4 . With new representations of states and actions,\n4770\nStart\nܳǡۦͲǡͲ̴ۧ݁ݐܽ݉݅ݐݏܧ\nܳ\n࢙ \nǡ\nۦ\nͲ\nǡ\nܳǡۦǡͳۧ௘௫௜௧\nܳ\n࢙ \nǡ\nۦ\nܳǡۦͳǡͲ̴ۧ݁ݐܽ݉݅ݐݏܧ\n0\n 1\n 2\nYes\nY\nܳǡۦͲǡͲۧ݁ݐܽ݉݅ݐݏܧ\nܳ\n࢙ \nǡ\nۦ\nͲ\nǡ\nͲ\nۧ\n՚\n݊݅ܯ\n̴\n݁ݐܽ݉݅ݐݏܧ\nܳǡۦǡͳۧ௘௫௜௧\nܳ\n࢙ \nǡ\nۦ\nݓ\nǡ\nͳ\nۧ\n՚\n݋ܿݏݎ\n݁\n௘௫௜௧\nܳǡۦͳǡͲۧ݁ݐܽ݉݅ݐݏܧ\nNo\nͲ\nۧ\n՚\nݔܽܯ\n݁ݐܽ݉݅ݐݏܧ \nͲ\nۧ\n՚\nݔܽܯ\n̴\n݁ݐܽ݉݅ݐݏܧ\nۦ\nݓ\nͳ\nۧ\n՚\n݋ܿݏݎ\n݁\n௜௧\nۦ\nݓ\nǡ\nͳ\nۧ\n՚\nݎ݋ܿݏ\n݁\n௘௫௜௧\nͲ\nۧ\n՚\n݊݅ܯ\n̴\nܧݐܽ݉݅ݐݏ݁\nۧ\no\nReturnܳfunction\nn\nܳ\nfu\nYes\nNo\nN\nN\nNo\nYes\nYe\nYe\n o\n݀ൌ Ͳǫ\nYes\ns\nN\nY\nAll states visited\nEnd\nGenerate a state\nൌۦ݀݀݀݌݁ݐݏۧ\n2\n0\n0\n 2\n1\nൌǫ\nFigure 4: Flow of the human reasoning-derived decision tree\nwe can reduce the size of the tabular Q-function to less than\n1%, with only240,000 state-action pairs.\nWarm Start Using Human Reasoning. As analyzed\nabove, even after abstraction, there are still240,000 state-\naction pairs. In traditional Q-learning, where Q-function is\nusually arbitrarily initialized, it is required to run millions\nof episodes to ensure convergence, which takes days due\nto the long simulation time. Human demonstration (Wang\nand Taylor 2017) was proposed to improve efﬁciency of re-\ninforcement learning, where humans directly assign values\nto initialize the Q-function. However, human demonstration\nbecomes rather inefﬁcient when there are too many state-\naction pairs. We take a further step by utilizing a human\nreasoning-derived decision tree for Q-function’s warm start.\nThe key principles of the warm start approach include\n1) when making instant decisions (i.e., w=0), we ignore\nthe pig’s movement according to Figure 3(a); 2) we wait\nfor the pig to move when it is at an uncatchable position;\nand 3) we always choose the action with the highest esti-\nmated score. Figure 4 shows the ﬂow of the decision tree\nfor playing with the F ocusedagent. It starts with generat-\ning state s = ⟨PigStat,d (B,P ),d(R,P ),d(R,E),step⟩.\nWe ﬁrst check the value ofPigStat.I f PigStat =1 ,w e\nfurther check distance d(R,P ). Then we initialize the Q\nvalues of different actions accordingly. For simplicity, we\nuse w =0 (or 1) to represent an action without (or with)\nwaiting time anda\n′ =0 (or 1) to represent chasing the pig\n(or leaving the game) forA = ⟨w,a′⟩. For state s, we can\ncompute the probabilityNm(0) (and Nm(1)) that pig stays\nunmoved with 25−step steps remaining for w =0 (and\nw =1 ). scoreexit is what agent RED can get by leaving the\ngame through exits.Min Estimate is the estimated score\nwhen the pig stays unmoved with probability Nm(0) for\nPigStat =0 (or moves away with probability1 − Nm(1)\nfor PigStat > 0); Max Estimate is the score for agent\nRED when the pig is caught at its current position (or the\nnearest catchable position). We initialize the Q-function for\nplaying withRandom agent in a similar way.\nActive-ε-Greedy. To explore a potentially better action,\nclassical tabular Q-learning methods usually explore the en-\ntire action space (e.g.,ε-greedy). Counter-intuitively, experi-\nmental results (to be explained later in Figure 6(a)) show that\nthe performance ofε-greedy (the line with solid circle mark-\ners) declines in the early stage of learning. This is because\nthe optimal actions estimated by our constructed decision\ntree are usually very good. Therefore, exploring suboptimal\nactions would lead to performance deterioration in a short\nterm. Meanwhile, if we always deterministically exploit an\naction that has the highest Q-value, there would be very lim-\nited improvement to the current policy. To this end, we pro-\npose anactive-ε-greedy approach to balance the exploration\nand exploitation. In active-ε-greedy, we still select the opti-\nmal action based on the Q-function with the probability of\n1 − ε. Different fromε-greedy, we explore sub-optimal ac-\ntions with probability ε, when the Q-value of that action is\nno less than certain percentage (hand tuned as50% in prac-\ntice) of the highest Q-value in the same state.\nTraining HogRider. In the training process, the type of\nagent BLUE is ﬁxed as eitherRandom or F ocused. As shown\nin Algorithm 1, training starts with initialization via the de-\ncision tree derived from human reasoning. It then enters the\nouter loop (Lines 2-8). In each iteration (episode) of the\nouter loop, HogRider ﬁrst initializes the states (Line 3). It\nthen enters the inner loop (Lines 4-7). In each iteration (step)\nof the inner loop, HogRider ﬁrst chooses an action A ac-\ncording to the active-ε-greedy method(Line 5). It then takes\nthe action (after that, agent BLUE will alternatively take an\naction) and observes the immediate rewardR and the next\nstate s\n′ (Line 6), which will be assigned as the starting state\nof the next step (Line 7). The inner loop terminates when\na terminal state is reached. After that, Q-values for all the\nstate-action pairs (s,A) that have been visited during this\nepisode are updated (Line 8), whereα is the learning rate.\nV(s,A)= ∑\nterminal\ns′=s R(s′,A) denotes the “real” value of\nthe state-action pair(s,A)), which is the sum of immediate\nreward of all the state action pairs that are visited after state\ns during the episode. The entire outer loop terminates after\nM (e.g., 3,000) episodes.\nAlgorithm 1:Training HogRider\n1 Q(s,A) ← human reasoning,∀s,A;\n2 while #episodes ≤ M do\n3 Initialize states;\n4 while s is not terminaldo\n5 Choose A with active-ε-greedy method;\n6 Take actionA, observeR(s,A) and s′;\n7 s ← s′;\n8 Q(s,A)←(1−α)Q(s,A)+αV(s,A), ∀ visited (s,A)\n9 return Q(s,A), ∀s,A;\nHogRider\nAfter training, we obtain Q-functions for both agentsRan-\ndom and F ocused, which are denoted asQR and QF, respec-\ntively. We now present HogRider (Algorithm 2) which inte-\ngrates the learned type-speciﬁc Q-function with the agent\n4771\ntype hypothesis framework. Note that unlike the training\nprocess, agent BLUE can be either Random or F ocused\nfor different episodes of games. At the beginning of each\nepisode of game, HogRider initializes the belief of agent\nBLUE being Random as P(Random)=0 .25 (Line 1). At\nthe same time, HogRider observes the initial states of the\ngame and takes the ﬁrst action according to the initial belief\n(Line 3). After that, it observes the new states\n′(Line 4) and\nenters the while loop (Lines 5-9).\nAlgorithm 2:HogRider\n1 P(Random) ← 0.25;\n2 Observe initial states;\n3 Take actionA w.r.t. s and P(Random);\n4 Observe s′;\n5 while s′ is not terminaldo\n6 Observe action of agent BLUE;\n7 Update P(Random) using Eq.(5);\n8 Take actionA w.r.t. s′ and P(Random);\n9 s ← s′ and observe new states′;\nDifferent strategies of selecting an optimal action. In\neach iteration, it observes the action of agent BLUE in\nlast step (Line 6) according to the difference between s\nand s\n′, updates the belief accordingly (Line 7), takes an\naction A based on the current state s′ and P(Random)\n(Line 8). There are three ways to select an action. One\nway (called Separate) is to compare P(Random) with\na threshold P\n0, and chooseA∗ =a r g m a xQR(s,A) when\nP(Random) ≥ P0 (vice versa). The second way (called\nWeighted) is to maintain an expected Q-value for each ac-\ntion A: ¯Q(s,A)= P(Random)·QR(s,A)+ P(Focused)·\nQF(s,A), and select A∗ =a r g m a x¯QR(s,A). These two\nmethods are deterministic. The last way (called Mixed)\nis to use A∗ =a r g m a xQR(s,A) with a probability\nP(Random), and useA∗ =a r gm a xQF(s,A) with a prob-\nability of P(Focused), which is non-deterministic. After\ntaking an action, it stores the current state ins and observes a\nnew sates′(Line 9). This process untils′is a terminal state.\nExperimental Evaluations\nIn this section, we present the challenge results of the top\n5 teams in MCAC 2017. We also conduct extensive exper-\niments to evaluate performance of various ideas from the\nproposed solution algorithm.\n3\nMCAC Results Table 1 shows the performance of the top\n5 teams in MCAC 2017. In terms of both per step mean\nscore and variance/mean, HogRider achieves the best perfor-\nmance. Compared to the second best team, HogRider wins\nwith an edge of13% in mean score, and an edge of21.7%\n3There are two ways to measure the performance of different\nmethods, i.e., per step score and per game (episode) score. Due to\npage limit, we only show results for per step scores, while we refer\nto our online appendix (Xiong et al. 2017) for results of per episode\nscores, as well as ﬁgures showing the detailed curves of the results\nthat cannot be presented with tables.\nin variance/mean. This demonstrates that HogRider outper-\nforms other teams in both optimality and stability.\nTable 1: Performance of different teams in MCAC.↑ means\nthe higher, the better, while↓ means the opposite. The best\nperformance is highlighted for all the tables.\nTeam Mean (↑) V ariance V ariance/Mean (↓)\nHogRider 2.752 2.55 0.9266\nThe Danish Puppeteers 2.435 2.88 1.1828\nBacon Gulch 1.681 2.18 1.2968\nVillage People 1.618 2.05 1.2670\nAASMA 0.591 1.07 1.8105\nStrategies for Selecting the Optimal Action As we de-\nscribed, there are three strategies to select an optimal action\nbased on the Q-function, i.e.,Separate, Weighted and\nMixed. Table 2 shows the performance of the three different\nstrategies, where Separate has the best performance in\nboth optimality and stability. In the following experiments,\nwe utilizeSeparate as the base strategy for optimal action\nselection.\nTable 2: Strategies for selecting the optimal action\nMean score (↑) V ariance V ariance/Mean (↓)\nSeparate 2.752 2.55 0.9266\nWeighted 2.527 2.36 0.9339\nMixed 2.478 2.81 1.1340\nBelief Update Table 3 evaluates the performance of\nHogRider using different methods for updating the beliefs\nover agent types, where Bayes means traditional Bayes the-\norem in Eq.(1), G-Bayes means the extension of Bayes the-\norem in Eq.(2), and G-Bayes+tanh means G-Bayes with a\nhyperbolic tangent squashing function in Eq.(5).\nTable 3: Different methods for agent type belief update\nMethod Accuracy Mean(↑) Va r. V ar./Mean(↓)\nBayes 0.578, 0.955 (0.672) 2.456 2.45 0.9976\nG-Bayes 0.701, 0.886 (0.747) 2.505 2.27 0.9062\nG-Bayes+tanh 0.961, 0.863 (0.937) 2.752 2.55 0.9266\nNote that for the Accuracy column, the three numbers are\nrespectively the detection accuracies of two agent BLUE\ntypes and the expected overall accuracy. In general, G-\nBayes+tanh has the highest overall detection accuracy and\noptimality among all methods, while being slightly inferior\nto G-Bayes in stability. Compared with the second best ap-\nproach, G-Bayes+tanh is better with an edge of9.9% in op-\ntimality, and an edge of25% in overall detection accuracy.\nWarm Start Figure 5 shows the learning curve and valida-\ntion curve of HogRider with human reasoning-aided initial-\nization playing with theF ocusedagent. The validation curve\nshows that, using initialization, the mean score of HogRider\n4772\n0 500 1000 1500 2000 2500 30002.5\n3\n3.5\n4\nLearning episodes\nAverage score / step\nNo Init.\nWith Init.\n(a) Learning curve\n (b) V alidation curve\nFigure 5: Evaluate human reasoning-aided initialization\n0 500 1000 1500 2000 2500 30002.5\n3\n3.5\n4\nLearning episodes\nAverage score / step\nDeterministic\nε−Greedy\nSemi−Deterministic\nActive ε−greedy\n(a) Learning curve\n0 200 400 600 800 10002.7\n2.8\n2.9\n3\n3.1\n3.2\n3.3\n3.4\n3.5\nValidation episodes\nAverage score / step\nDeterministic\nε−Greedy\nSemi−Deterministic\nActive ε−greedy\n(b) V alidation curve\nFigure 6: Evaluate different exploration methods\nsigniﬁcantly improves by 10.9%, while variance/mean de-\ncreases by 5.7%. The learning curve indicates that we can\nlearn a better Q-function much faster.\nDifferent Exploration Methods Figure 6 compares the\nlearning curve and validation curve of the active-ε-greedy\nwith other exploration methods (playing with Focused\nagent), where Deterministic always selects the current op-\ntimal action A\n∗ =a r g m a xQ(s,A)), ε-greedy selects A∗\nwith probability 1 −ε and randomly explores other actions\nwith probabilityε, Semi-Deterministic selectsA∗ with prob-\nability 1 −ε and explores the second best action with prob-\nability ε. From the learning curve, we can see that 1) arbi-\ntrary exploration (i.e.,ε-greedy) can be rather harmful when\nlearning trials are limited, 2) deterministic methods pro-\nvide limited help in improving the current policy, 3) semi-\ndeterministic methods always explore the second-best action\nwithout considering the estimated value of it, which is also\ninefﬁcient, and 4) with active-ε-greedy, the potential opti-\nmal actions (with at least 50% estimated value of the best\naction) are exploited and the policy keeps improving along\nthe training process. The validation curve demonstrates the\neffectiveness of active-ε-greedy method. In terms of mean\nscore, active-ε-greedy is better with an edge of4.1% com-\npared with the second best approach (Deterministic), and\nbetter with an edge of6.7% compared withε-greedy.\nCompare HogRider with Human Players We also in-\nvited 10 human players from our university to play the Pig\nChase game, all of whom are PhD candidates. Surprisingly,\nthe results show that HogRider performs even better than\nhuman players with an edge od28.1% in mean increase and\n29.6% in variance/mean decrease.\nTable 4: Comparison with human players\nApproach Mean(↑) V ariance V ariance/Mean(↓)\nHogRider 2.752 2.55 0.9266\nHuman 2.149 2.83 1.3169\nLessons Learned\nDeveloping HogRider has been a progressive experience.\nDifferent from the constructive ﬂow that presents HogRider,\nit was rather a step by step process where new discover-\nies and ideas interchangeably emerge. We gained several\ninsights into solving challenging problems in the areas of\ncollaborative AI, as well as other more general research do-\nmains. First, “if you know yourself and your enemy, you\nwill never lose a battle”.\n4 It has been a long exploration\nbefore we set up the integrated framework of agent type hy-\npothesis and Q-learning. Our initial plan was to simply learn\nan effective policy with Q-learning without differentiating\ndifferent types of agents. Despite the model-free property of\nQ-learning, its direct application worked poorly simply be-\ncause we did not exploit the problem structure. While look-\ning for cutting-edge tools is important, learning the foun-\ndations of the problem ensures the right direction.Second,\nhuman intuition can take machines to the heights.Due\nto its popularity, we initially decided to use DQN (Glorot\nand Bengio 2010) as the learning approach to learn policies\nagainst each agent type. Despite its power in expressing Q-\nfunctions, initialization does not work due to the parame-\nterized Q-functions, which poses DQN at a disadvantageous\nplace against tabular Q-learning approaches. In fact, a few\nteams using DQN had poor performance in the challenge. It\nis rather surprising and enlightening how much human rea-\nsoning could help. This type of initialization could be uti-\nlized in other areas where domain knowledge can help de-\nrive human reasoning.Moreover, models and solution al-\ngorithms should be kept updating with new discoveries\nof latent properties.For example, the uncertainty in agent\naction observation was not discovered until the algorithm\nwas almost built. To this end, we made two additional adap-\ntations to the traditional Bayes theorem which brought sig-\nniﬁcant improvement to the algorithm.\nConclusion & Future Research\nThere are many characteristics such as complex interactions\namong agents, uncertainties, and limited learning trials that\nmake effective sequential planning extremely challenging.\nThe MCAC, which is designed to encourage research re-\nlating to various problems in Collaborative AI, is a typical\ntest bed with all these challenges. We introduce HogRider,\nchampion agent of MCAC in 2017, which has two key inno-\nvations. First, we design a generalized agent type hypothesis\nframework to handle the uncertainties in agent type and ob-\nservation in the other agent’s action. Second, we propose a\nnovel Q-learning approach to learn effective policies against\neach type of collaborating agents with various novel ideas,\nincluding state-action abstraction, a human reasoning-aided\n4It is from an ancient Chinese military treatise,The Art of War.\n4773\napproach to initialize Q-function, and an active greedy strat-\negy to balance the exploitation and exploration.\nOne potential future research is to look at scenarios where\nagents are completely unknown to each other before they\nmeet. For example, there are no prior knowledge of the dis-\ntribution of agent types, or there are no pre-deﬁned types\nfor the collaborating agents. Another potential research is to\npropose solution algorithms which can be generalized to dif-\nferent types of environments. For example, if the positions\nof the blocks are replaced, more blocks are added, or the\nsize of grids are varied, how could the policy learned from\nthe old setting applied to the new setting? In these scenarios,\nalgorithms might have to combine off-linely trained policies\nwith online learning (Anderson 2008), which resembles the\nidea of Endgame solving (Ganzfried and Sandholm 2015)\nin Computer Poker games, and the set of transfer learning\nalgorithms that are designed for reinforcement learning do-\nmains (Taylor and Stone 2009).\nAcknowledgement\nWe thank Microsoft Research for organising the challenge.\nThis research is supported by the National Research Foun-\ndation, Prime Ministers Ofﬁce, Singapore under its IDM\nFutures Funding Initiative, and is supported by NRF2015\nNCR-NCR003-004 and NCR2016NCR-NCR001-002.\nReferences\nAgmon, N., and Stone, P . 2012. Leading ad hoc agents in\njoint action settings with multiple teammates. In AAMAS,\n341–348.\nAlbrecht, S. V .; Crandall, J. W.; and Ramamoorthy, S. 2016.\nBelief and truth in hypothesised behaviours.Artiﬁcial Intel-\nligence 235:63–94.\nAnderson, T. 2008. The Theory and Practice of Online\nLearning. Athabasca University Press.\nBarrett, S.; Stone, P .; Kraus, S.; and Rosenfeld, A. 2013.\nTeamwork with limited knowledge of teammates. InAAAI,\n102–108.\nBarrett, S.; Stone, P .; and Kraus, S. 2011. Empirical evalua-\ntion of ad hoc teamwork in the pursuit domain. InAAMAS,\n567–574.\nBrafman, R. I.; Domshlak, C.; Engel, Y .; and Tennenholtz,\nM. 2009. Planning games. InIJCAI, 73–78.\nConitzer, V ., and Sandholm, T. 2007. Awesome: A gen-\neral multiagent learning algorithm that converges in self-\nplay and learns a best response against stationary opponents.\nMachine Learning67(1-2):23–43.\nDe Cote, E. M.; Chapman, A. C.; Sykulski, A. M.; and Jen-\nnings, N. R. 2012. Automated planning in repeated adver-\nsarial games. arXiv preprint arXiv:1203.3498.\nDunne, P . E.; Kraus, S.; Manisterski, E.; and Wooldridge,\nM. 2010. Solving coalitional resource games. Artiﬁcial\nIntelligence 174(1):20–50.\nFoerster, J.; Assael, Y . M.; de Freitas, N.; and Whiteson, S.\n2016. Learning to communicate with deep multi-agent rein-\nforcement learning. InNIPS, 2137–2145.\nGanzfried, S., and Sandholm, T. 2015. Endgame solving in\nlarge imperfect-information games. InAAMAS, 37–45.\nGlorot, X., and Bengio, Y . 2010. Understanding the difﬁ-\nculty of training deep feedforward neural networks. InAIS-\nTATS, 249–256.\nGrosz, B. J., and Kraus, S. 1996. Collaborative plans for\ncomplex group action.Artiﬁcial Intelligence86(2):269–357.\nHor´ak, K.; Bosansk`y, B.; and Pechoucek, M. 2017. Heuris-\ntic search value iteration for one-sided partially observable\nstochastic games. InAAAI, 558–564.\nHorling, B.; Lesser, V .; Vincent, R.; Wagner, T.; Raja, A.;\nZhang, S.; Decker, K.; and Garvey, A. 1999. The TAEMS\nwhite paper.\nJord´an, J., and Onaindia, E. 2015. Game-theoretic approach\nfor non-cooperative planning. InAAAI, 1357–1363.\nJord´an, J.; de Weerdt, M.; Onaindia, E.; et al. 2017. A\nbetter-response strategy for self-interested planning agents.\nApplied Intelligence 1–21.\nKuno, N., and Schwiderski-Grosche, S. 2017. Present-\ning the winners of the Project Malmo Collaborative AI\nChallenge. https://www.microsoft.com/en-us/research/blog/\nmalmo-collaborative-ai-challenge-winners/.\nMojang. 2017. Minecraft. https://minecraft.net/en-us/.\nSandholm, T. 2015. Abstraction for solving large\nincomplete-information games. InAAAI, 4127–4131.\nSeuken, S., and Zilberstein, S. 2008. Formal models and\nalgorithms for decentralized decision making under uncer-\ntainty. In AAAMAS, 190–250.\nSingh, S.; Kearns, M.; and Mansour, Y . 2000. Nash conver-\ngence of gradient dynamics in general-sum games. InUAI,\n541–548.\nSouthey, F.; Bowling, M. P .; Larson, B.; Piccione, C.; Burch,\nN.; Billings, D.; and Rayner, C. 2012. Bayes’ bluff: Oppo-\nnent modelling in poker.arXiv preprint arXiv:1207.1411.\nStone, P .; Kaminka, G. A.; Kraus, S.; Rosenschein, J. S.;\net al. 2010. Ad hoc autonomous agent teams: Collaboration\nwithout pre-coordination. InAAAI, 1504–1509.\nTambe, M. 1997. Towards ﬂexible teamwork. Journal of\nArtiﬁcial Intelligence Research7:83–124.\nTaylor, M. E., and Stone, P . 2009. Transfer learning for rein-\nforcement learning domains: A survey.Journal of Machine\nLearning Research10:1633–1685.\nWang, Z., and Taylor, M. E. 2017. Improving reinforcement\nlearning with conﬁdence-based demonstrations. In IJCAI,\n3027–3033.\nWatkins, C. J. C. H. 1989.Learning from Delayed Rewards.\nPh.D. Dissertation, King’s College, Cambridge.\nXiong, Y .; Chen, H.; Zhao, M.; and An, B. 2017. Hogrider\nappendix. http://AAAImalmo.weebly.com.\n4774",
  "topic": "Champion",
  "concepts": [
    {
      "name": "Champion",
      "score": 0.8755356073379517
    },
    {
      "name": "Computer science",
      "score": 0.6908577680587769
    },
    {
      "name": "Key (lock)",
      "score": 0.5687456727027893
    },
    {
      "name": "Abstraction",
      "score": 0.5225809812545776
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4846406579017639
    },
    {
      "name": "Action (physics)",
      "score": 0.4393235445022583
    },
    {
      "name": "Computer security",
      "score": 0.12504786252975464
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I172675005",
      "name": "Nanyang Technological University",
      "country": "SG"
    }
  ],
  "cited_by": 8
}