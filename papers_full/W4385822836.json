{
  "title": "LanSER: Language-Model Supported Speech Emotion Recognition",
  "url": "https://openalex.org/W4385822836",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2155366730",
      "name": "Taesik Gong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4309998036",
      "name": "Josh Belanich",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2564966726",
      "name": "Krishna Somandepalli",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2727507056",
      "name": "Arsha Nagrani",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1211615679",
      "name": "Brian Eoff",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2223703688",
      "name": "Brendan Jou",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3166396011",
    "https://openalex.org/W2146334809",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3176191472",
    "https://openalex.org/W2970062726",
    "https://openalex.org/W3175825020",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2752234108",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2970200208",
    "https://openalex.org/W4311000453",
    "https://openalex.org/W3197457832",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W3036601975",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4312754226",
    "https://openalex.org/W4309426823",
    "https://openalex.org/W4378189609",
    "https://openalex.org/W4307106469",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3107806026",
    "https://openalex.org/W3197580070",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3206495532",
    "https://openalex.org/W2030931454",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W3197236059"
  ],
  "abstract": "Speech emotion recognition (SER) models typically rely on costly\\nhuman-labeled data for training, making scaling methods to large speech\\ndatasets and nuanced emotion taxonomies difficult. We present LanSER, a method\\nthat enables the use of unlabeled data by inferring weak emotion labels via\\npre-trained large language models through weakly-supervised learning. For\\ninferring weak labels constrained to a taxonomy, we use a textual entailment\\napproach that selects an emotion label with the highest entailment score for a\\nspeech transcript extracted via automatic speech recognition. Our experimental\\nresults show that models pre-trained on large datasets with this weak\\nsupervision outperform other baseline models on standard SER datasets when\\nfine-tuned, and show improved label efficiency. Despite being pre-trained on\\nlabels derived only from text, we show that the resulting representations\\nappear to model the prosodic content of speech.\\n",
  "full_text": "LanSER: Language-Model Supported Speech Emotion Recognition\nTaesik Gong1, Josh Belanich2∗, Krishna Somandepalli2∗, Arsha Nagrani2, Brian Eoff2, Brendan\nJou2\n1KAIST, Republic of Korea\n2Google Research\ntaesik.gong@kaist.ac.kr, {joshbelanich,ksoman,anagrani,beoff,bjou}@google.com\nAbstract\nSpeech emotion recognition (SER) models typically rely on\ncostly human-labeled data for training, making scaling meth-\nods to large speech datasets and nuanced emotion taxonomies\ndifficult. We present LanSER, a method that enables the use of\nunlabeled data by inferring weak emotion labels via pre-trained\nlarge language models through weakly-supervised learning. For\ninferring weak labels constrained to a taxonomy, we use a tex-\ntual entailment approach that selects an emotion label with the\nhighest entailment score for a speech transcript extracted via au-\ntomatic speech recognition. Our experimental results show that\nmodels pre-trained on large datasets with this weak supervision\noutperform other baseline models on standard SER datasets\nwhen fine-tuned, and show improved label efficiency. Despite\nbeing pre-trained on labels derived only from text, we show that\nthe resulting representations appear to model the prosodic con-\ntent of speech.\nIndex Terms : speech emotion recognition, large language\nmodels, weakly-supervised learning\n1. Introduction\nIn conversations, humans rely on both what is said (i.e., lexical\ncontent), and how it is said (i.e., prosody), to infer the emotion\nexpressed by a speaker. State-of-the-art methods in speech emo-\ntion recognition (SER) leverage the interplay of these two com-\nponents for modeling emotional expression in speech. How-\never, such methods still show limitations on in-the-wild scenar-\nios due to the variability in natural speech, and the reliance on\nhuman ratings using limited emotion taxonomies. Extending\nmodel training to large, natural speech datasets labeled by hu-\nmans for nuanced emotion taxonomies is expensive and is fur-\nther complicated by the subjective nature of emotion perception.\nDespite both lexical content and prosody being comple-\nmentary for emotion perception, the two components arecorre-\nlated, and in many cases the content is predictive of the prosody.\nFor example, when someone says, “I won the lottery” – an up-\nbeat and lively prosody would sound congruent, and one might\nperceive the emotional expression as elation or triumphant. In\nthis work, we investigate how we might leverage the emo-\ntions congruent with lexical content in large unlabeled speech\ndatasets to serve as weak supervision for developing SER mod-\nels.\nWe turn to Large Language Models (LLMs) to infer ex-\npressed emotion categories in textual content. Due to the knowl-\nedge they embed from pre-training on large text corpora [1, 2],\nLLMs have demonstrated capabilities in numerous downstream\ntasks [3], including a few subjective tasks such as social and\n*Equal contribution.\nSER model\nTextualEntailment SpeechASRTranscript Predicted emotion (weak label)\nWeak labelPredicted emotion\nSER modelSpeech Predicted emotion\nPrompt\nSpeech\n1. Weak label generation\n2. Weakly-supervised learning\n[Fine-tuning / Inference]\nGrouth-truth labels\n[Training]\nTaxonomy\nLLM\nFigure 1: The overview of LanSER. LLMs and textual entail-\nment are used to infer weak emotion labels from speech content\nwhich are used to pre-train a SER model.\nemotion reasoning [4]. In domains such as computer vision,\nLLMs were explored to reduce the need for labeled data, e.g.,\nfor visual question answering [5]. However, to our knowledge,\nthey have not been studied for emotion recognition tasks, par-\nticularly from natural speech.\nWe proposeLanSER, that uses LLMs to infer emotion cate-\ngories from speech content i.e., transcribed text, which serve as\nweak labels for SER (Figure 1). Overall, LanSER enables pre-\ntraining a SER model on large speech datasets without human\nlabels by (1) extracting text transcripts from utterances using au-\ntomatic speech recognition (ASR), (2) using pre-trained LLMs\nto infer weak emotion labels with an engineered prompt and\npredetermined taxonomy, and (3) pre-training the SER model\nwith the weak labels. We demonstrate that LanSER improves\nSER performance and label efficiency by fine-tuning on bench-\nmark datasets. Moreover, we show that despite the emotion la-\nbels being derived from speech content only, LanSER captures\nspeech prosody information that is relevant to SER.\n2. Related Work\nSER with LLMs : Recently, LLMs were used to generate\npseudo-labels for semi-supervised learning for speech senti-\nment analysis [6]. Here, LLMs were fine-tuned on a labeled\nsentiment dataset to explore narrow sentiment classes of neg-\native, positive, and neutral. In contrast, our work avoids fine-\ntuning LLMs on task-specific datasets by inferring weak labels\nvia textual entailment, enabling exploration with wider emo-\ntion taxonomies. In the context of multi-modal emotion recog-\nnition, MEmoBERT [7] used audio, visual, and text informa-\ntion with prompt learning for unsupervised emotion recogni-\ntion. Herein, the visual model is pre-trained on a large labeled\nemotion dataset. In contrast, in our work, pre-training on large\nhuman-annotated emotion datasets is not necessary.\nSelf-supervised learning: Self-supervised learning has become\na popular method using large amounts of unlabeled speech data\narXiv:2309.03978v1  [cs.CL]  7 Sep 2023\nText generation\nFilling mask\nInput: “Stop. I hate you. Do you hear me?”“This person is expressing”Outputs: 1.“disgust with me? I'm not that dumb. I hate you because of me…”2.“her hatred, what for? All I did was say. 'You're not just some angry…”3.“personal anger against you. I don't know what to think about that's you…”\nInput: “Stop. I hate you. Do you hear me?”“This person is expressing {MASK}.”Outputs:1.“hate”2.“pain”3.“himself”\nTranscriptPrompt\nTextual entailmentHypothesis: “Stop. I hate you. Do you hear me?”\nPremise 2: “This person is expressing happiness.”Premise 1: “This person is expressing anger.”\nOutput: “anger”\nentailment score\nFigure 2: Comparison of three weak label generation ap-\nproaches: text generation, filling mask, and textual entailment.\nfor pre-training [8, 9]. Recent studies found that large pre-\ntrained models via self-supervised learning show effectiveness\nin various downstream speech tasks [10], including many par-\nalinguistic tasks [9]. We view self-supervised learning and our\nweak supervision from LLMs as complementary, since the two\nmethodologies can be combined for training SER models.\n3. Methodology\nAn overview of the training and inference process of LanSER\nin shown in Figure 1. During pre-training, we use ASR to gen-\nerate transcripts from speech utterances, which are fed into a\nLLM with appropriate prompt to extract weak emotion labels\nin predetermined taxonomy via textual entailment. These la-\nbels are used to pre-train a SER model via weakly-supervised\nlearning. The pre-trained SER model can then either be used\ndirectly to output emotion predictions according to the emotion\ntaxonomy used to extract weak labels, or can be adapted for a\ndifferent taxonomy or dataset by fine-tuning.\nWe note that the emotions inferred using LLMs from\nspeech content are proxies for the emotion being expressed, and\nmay not capture the larger context or intent of the speaker. Thus,\nwe treat them as “weak” emotion labels in our work.\n3.1. Weak label generation via textual entailment\nThere are multiple ways to use LLMs for extracting weak emo-\ntion labels. Two dominant approaches in the literature are\n(i) text generation [2] and (ii) filling mask [11, 1, 7]. Fig-\nure 2 demonstrates the behaviors of text generation and fill-\ning mask for weak emotion label prediction. We used repre-\nsentative LLMs for each approach: GPT-2 for text generation\nand BERT [11] for filling mask. While these approaches show\nsome success, the common limitation in a zero-shot setting is\nthat they often output undesirable “noise”, like irrelevant words\n(text generation), or non-emotional responses (e.g., “himself”\nin filling mask in the Fig. 2).\nThus, we want to constrain the LLM model to output only\nwords relevant to emotion perception. To this end, we use tex-\ntual entailment [12] to generate weak labels that also allows\nus to constrain the emotion taxonomy apriori. Figure 2 illus-\ntrates the entailment-based weak emotion label generation; at\na high-level, this method calculates the entailment scores be-\ntween an input transcript (called hypothesis) and prompts with\ncandidate labels from the taxonomy (called premise), and then\nselects the item with the highest score as the weak label. For-\nmally, let x ∈ X denote ASR transcripts from speech and\ny ∈ Ydenote a candidate label in taxonomy Y. A prompting\nfunction g(·) prepends a predefined prompt to the given input.\nf(x, g(y)) denotes the entailment score between a hypothesisx\nand a prompted label g(y). The resulting weak emotion label ˆy\nfor a given transcript x is calculated as:\nˆy := arg max\ny∈Y\nf(x, g(y)). (1)\nThe entailment scoring function f is a function typically pa-\nrameterized by a neural network and fine-tuned on the entail-\nment task. In our case, we use RoBERTa [13] fine-tuned on the\nMulti-genre Natural Language Inference (MNLI) [14] dataset.\nThe MNLI dataset is composed of hypothesis and premise pairs\nfor diverse genres, which is specialized for the textual entail-\nment approach, and do not explicitly focus on emotion-related\nconcepts.\n3.2. Prompt engineering\nPrompt engineering is a task-specific description embedded in\ninputs to LLMs (e.g., a question format) [15]. It is a critical\ncomponent affecting zero-shot performance of LLMs on vari-\nous downstream tasks [1, 16, 17]. In Section 4.2 we explore\nvarious prompts in order to understand the impact of prompt\nengineering for the entailment task. Ultimately, we found that\nthe prompt “The emotion of the conversation is {}.” performed\nbest, and we use this prompt throughout our experiments.\n3.3. Taxonomy\nThe choice of emotion taxonomy is critical in developing\nSER models as emotion perception and expression is nuanced.\nCommon SER benchmarks typically use 4–6 emotion cate-\ngories [18, 19], which do not capture the variability in emotion\nperception [20]. Thus we experiment with BRA VE-43, a finer-\ngrained taxonomy [21]. We adopted and modified the BRA VE\ntaxonomy which originally contains 42 self-reported emotions\nlabels. We converted several two-word emotions to one-word\nemotions for simplicity and added “shock” to capture a negative\nversion of “surprise”, resulting in a total of 43 categories. Note\nthis taxonomy is not speech-specific. We investigate the impact\nof taxonomy selection in Section 4.5. We expect fine-grained\ntaxonomies to help learn effective representations by using the\nhigh degree of the expressiveness of LLMs.\n4. Experiments\nOur overarching hypothesis is that, given a sufficiently large\namount of data, pre-training speech-only models on weak emo-\ntion labels derived from text improves performance on SER\ntasks. As such, throughout this paper, we focus on speech-only\nemotion recognition models. Additionally, our goal is not to\nobtain state-of-the-art results on downstream tasks but to as-\nsess, given a fixed model capacity, whether models pre-trained\nvia LanSER achieve improved performance.\n4.1. Data preparation\nPre-training data : We investigate two large-scale speech\ndatasets for LanSER pre-training: People’s Speech [22] and\nCondensed Movies [23]. People’s Speech is currently the\nlargest English speech recognition corpus, containing 30K\nhours of general speech. Condensed Movies is comprised\nof 1,000 hours of video clips from 3,000 movies, where we\nuse only the audio. We explore these two large-scale speech\ndatasets to understand the impact of the amount of data and their\ndistributions; while People’s Speech has more samples from\nless emotional data sources (e.g., government, interview, health,\netc.), Condensed Movies has fewer samples from a more emo-\ntional data source (movies). We use Whisper ASR [24] (“small”\nvariant) to segment and generate transcripts for People’s Speech\nand Condensed Movies datasets, resulting in 4,321,002 and\n1,030,711 utterances, respectively.\nDownstream tasks : We use two common SER benchmarks\nfor downstream tasks: IEMOCAP [18] and CREMA-D [19].\nIEMOCAP is an acted, multi-speaker database containing 5,531\naudio clips from 12 hours of speech. We follow the com-\nmonly used four-class (anger, happiness, sadness, and neutral)\nsetup [7, 25, 10, 9] and use speaker-independent train:val:test\nsplits. CREMA-D has 7,441 audio clips collected from 91 ac-\ntors. An important characteristic of CREMA-D is that it is\nlinguistically constrained, having only 12 sentences each pre-\nsented using six different emotions (anger, disgust, fear, happy,\nneutral, and sad). We use CREMA-D to validate that our mod-\nels indeed learn prosodic representations, and do not just learn\nto use language to predict the emotional expression.\n4.1.1. Baselines\nWe compare LanSER models fine-tuned on downstream\ndatasets with the following four baselines:\nMajority: Output the most prevalent class in the dataset [12].\nGT Transcript + Word2Vec[26]: Each word in a ground-truth\ntranscript is converted to a Word2Vec embedding. We compute\nthe cosine similarity between the averaged transcript embed-\nding and each class label, outputting the class with the highest\nsimilarity.\nGT Transcript + LLM + Entailment [12]: Using the same\nmethodology for producing weak labels, we process the ground-\ntruth transcript with an LLM and entailment to output a classi-\nfication according to the dataset’s taxonomy.\nSupervised: Supervised learning using the same model archi-\ntecture as LanSER but without pre-training.\nWe include two language-based methods (Word2Vec and\nEntailment) to better understand how LanSER compares with\nmodels using lexical content alone. Note that the language base-\nlines assume GT transcripts are available. In practice, these\nbaselines would require an ASR pipeline to get transcripts,\nwhich may involve additional computational and developmental\ncost.\n4.1.2. Implementation\nWe extracted mel-spectrogram features (frame length 32ms,\nframe steps 25ms, 50 bins from 60–3600Hz) from the audio\nwaveforms as input to the model and used ResNet-50 [27]\nas the backbone network for training. For both pre-training\nand fine-tuning, we minimized the cross-entropy loss with the\nAdam [28] optimizer and implemented in TensorFlow [29].\nFor pre-training, we adopted a warm-up learning rate\nschedule where the rate warmed up for the initial 5% of up-\ndates to a peak of 5 × 10−4 and then linearly decayed to zero.\nWe used a batch size of 256 and trained for 100K iterations.\nFor fine-tuning on the downstream tasks, we loaded the pre-\ntrained weights and used a fixed learning rate of 10−4. We set\nthe batch size as 64 and trained for 10K iterations. We split\nthe downstream datasets into a 6:2:2 (train:valid:test) ratio, and\nselected the best model on the validation set for testing.\nTable 1: Accuracy of extracted weak emotion labels with vari-\nous prompts. {} indicates the masked position.\nPrompts Acc.\nThis example is{}. 42.0%\nI am{}. [7] 39.9%\nI feel{}. 41.8%\nI am feeling{}. 45.0%\nThis person is expressing{}emotion. 43.7%\nA speech seems to express a feeling like{}. [16] 38.0%\nA transcript seems to express a feeling like{}. [16] 38.9%\nA conversation seems to express some feelings like{}. [16] 39.0%\nThe emotion of the conversation is{}. 45.6%\nThe emotion of the previous conversation is{}. 44.1%\nThe overall emotion of the conversation is{}. 45.1%\n4.2. Prompt engineering\nWe investigated the impact of various prompts to infer weak\nemotion labels using IEMOCAP. We chose IEMOCAP because\nit has transcripts and human-rated labels with majority agree-\nment referred here as “ground-truth”. To evaluate the prompts,\nwe compute accuracy by comparing the weak labels with the\nground-truth. We also examined prompts used in previous\nemotion recognition studies [16, 7] and modified a few vision-\nspecific prompts [16] for our study by replacing words such as\n“photo” or “image” with “speech”.\nTable 1 shows the accuracy for each prompt. The prompt\n(“I am {}.)” used in the related sentiment work [7] was not\nas effective at capturing emotional signals. Similarly, adapting\nvision-specific prompts [16] was ineffective. This suggests that\nit is worthwhile to tailor the prompt to the SER task. Among\nthe prompts we explored, “The emotion of the conversation is\n{}.” had the highest accuracy. We adopt this prompt to infer\nweak labels in all our experiments. We leave additional prompt\ntuning [30] as future work.\n4.3. Fine-tuning\nWe fine-tune all models on the downstream tasks to evaluate\ntheir label efficiency and performance. To measure label effi-\nciency, we varied the percentage of seen training data from 10%\nto 100% for each dataset. Table 2 shows the result. “LanSER\n(People’s Speech)” means pre-training with Peoples Speech,\nwhile “LanSER (Condensed Movies)” refers to pre-training\nwith Condensed Movies. In all cases, we used the BRA VE tax-\nonomy (see Sec. 3.3) as the label space.\nFirst, NLP baselines (Word2Vec and Entailment) fail on\nCREMA-D, as they only use lexical speech content. Interest-\ningly, LanSER’s results on CREMA-D suggest that the model\ncan learn prosodic representations via weak supervision from\nLLMs. We attribute this result to pre-training with large-scale\ndata, and it offers evidence to our hypothesis that speech and\ntext emotions are correlated enough that SER models can learn\nto use prosodic features even with labels from text only given a\nsufficiently large amount of data.\nOverall, LanSER outperforms the NLP and majority class\nbaselines. Notably, LanSER pre-trained with the Condensed\nMovies showed improved accuracy than with the People’s\nSpeech. While People’s Speech is comprised of fairly neu-\ntral speech data (e.g., government, interviews, etc.), Con-\ndensed Movies is comprised of movies having more expres-\nsive speech; from the emotion recognition perspective, Peoples\nSpeech might introduce more noise than Condensed Movies.\nTo assess that performance improvements are being driven\nby the emotion labels inferred using LLMs, and not just the\nscale of the pre-training data, we compare the fine-tuning per-\nformance of LanSER to a model pre-trained on Condensed\nTable 2: Unweighted accuracy (%) of fine-tuning for downstream tasks with varying the percentage of fine-tuning data (10%, 30%,\n50%, 70%, and 100%). Bold fonts indicate the highest accuracy.\nDownstream task Method \\ Fine-tuning data: 10% 30% 50% 70% 100%\nMajority 30.9% 30.9% 30.9% 30.9% 30.9%\nGT Transcript + Word2Vec [26] 34.9% 34.9% 34.9% 34.9% 34.9%\nGT Transcript + Entailment [12]47.5% 47.5% 47.5% 47.5% 47.5%\nSupervised 38.5% 41.8% 45.5% 46.0% 47.6%\nLanSER (People’s Speech) 42.0% 47.1% 47.2% 50.0% 50.6%\nIEMOCAP [18]\nLanSER (Condensed Movies) 50.0% 51.7% 48.0% 45.4% 54.5%\nMajority 17.1% 17.1% 17.1% 17.1% 17.1%\nGT Transcript + Word2Vec [26] 19.1% 19.1% 19.1% 19.1% 19.1%\nGT Transcript + Entailment [12] 16.1% 16.1% 16.1% 16.1% 16.1%\nSupervised 37.8% 43.2% 48.2% 53.4% 57.2%\nLanSER (People’s Speech) 35.5% 48.2% 51.5% 52.7% 55.8%\nCREMA-D [19]\nLanSER (Condensed Movies) 43.7% 49.9% 52.2% 53.6% 58.7%\nTable 3: Unweighted accuracy (%) for fine-tuning on down-\nstream tasks. LanSER (random labels) is pre-trained on Con-\ndensed Movies with BRAVE taxonomy labels assigned ran-\ndomly.\nDownstream task Method Accuracy\nLanSER (random labels) 47.6%IEMOCAP [18] LanSER (weak labels) 54.5%\nLanSER (random labels) 50.6%CREMA-D [19] LanSER (weak labels) 58.7%\nTable 4: Zero-shot unweighted accuracy (%) of SER models.\nDownstream task Method Accuracy\nScratch 22.9%\nLanSER (People’s Speech) 30.9% IEMOCAP [18]\nLanSER (Condensed Movies) 34.3%\nScratch 16.3%\nLanSER (People’s Speech) 15.9% CREMA-D [19]\nLanSER (Condensed Movies) 23.5%\nMovies using random uniformly sampled labels. As shown in\nTable 3, models pre-trained with weak labels outperform ones\ntrained with random labels suggesting that the weak emotion\nlabels inferred using LLMs are meaningful.\n4.4. Zero-shot classification accuracy\nA unique advantage of LanSER over self-supervised learn-\ning [8, 9] is that it enables SER models to support zero-shot\nclassification. Table 4 shows the zero-shot classification ac-\ncuracy: for LanSER, SER models were pre-trained with the\ntaxonomy of the downstream dataset instead of BRA VE and\nevaluated in a zero-shot setting. We use models with randomly\ninitialized weights and no training as a lower-bound of perfor-\nmance, referred to as “Scratch”. Overall, LanSER shows higher\naccuracy than the baseline, although not as good as fine-tuning.\nThese results suggest the potential of training large SER mod-\nels that can perform well on various downstream tasks, without\nfurther fine-tuning. Improving zero-shot performance further\nusing our proposed framework is part of our future work.\n4.5. Impact of taxonomy\nFigure 3 shows the impact of taxonomy selection. We com-\npared the BRA VE taxonomy with downstream task’s tax-\nonomies. “PS” and “CM” refers to People’s Speech and Con-\ndensed Movies, respectively. “IEMOCAP”, “CREMA-D”, and\n“BRA VE” means taxonomy used to generate weak labels. As\nshown, pre-training with the finer taxonomy (BRA VE) shows\ngenerally better accuracy when fine-tuned, with 4.2% accuracy\nimprovement on average. This indicates that a fine-grained tax-\nonomy is beneficial to learn effective representations by lever-\n10% 30% 50% 70% 100%\nFine-tuning data (%)\n0\n10\n20\n30\n40\n50\n60Classification error (%)\nIEMOCAP\nLanSER (PS/IEMOCAP)\nLanSER (CM/IEMOCAP)\nLanSER (PS/BRAVE)\nLanSER (CM/BRAVE)\n10% 30% 50% 70% 100%\nFine-tuning data (%)\n0\n10\n20\n30\n40\n50\n60Classification error (%)\nCREMA-D\nLanSER (PS/CREMA-D)\nLanSER (CM/CREMA-D)\nLanSER (PS/BRAVE)\nLanSER (CM/BRAVE)\nFigure 3: Impact of taxonomy selection for pre-training.\naging the high degree of the expressiveness of LLMs.\n5. Caveats\nDeveloping machine perception models of apparent emotional\nexpression remains an open area of investigation. The models\nin this work do not aim to infer the internal emotional state of\nindividuals, but rather model proxies from speech utterances.\nThis is especially true when training on the output of LLMs,\nsince LLMs may not take into account prosody, cultural back-\nground, situational or social context, personal history, and other\ncues relevant to human emotion perception. ASR transcription\nerrors add another layer of noise.\nThe benchmark datasets we use in this work are rela-\ntively small and are labeled with limited emotion taxonomies.\nCREMA-D, while useful for its fixed lexical content, is an acted\ndataset where the emotional expression of its utterances may not\nwell-represent natural speech.\n6. Conclusion and Future Work\nIn this work, we proposed LanSER, a novel language-model\nsupported speech emotion recognition method that leverages\nlarge unlabeled speech datasets by generating weak labels\nvia textual entailment using LLMs. Our experimental results\nshowed that LanSER can learn effective emotional representa-\ntions including prosodic features.\nWe note several possible areas of future work. It may be\npossible to reduce the weak label noise via filtering mecha-\nnisms, or by modifying prompts to include more conversational\ncontext, like the previous and next utterances, or scene descrip-\ntions. Additionally, using LLMs to generate weak labels in\nan open-set taxonomy may better leverage their expressiveness.\nFinally, while in this work we used ResNet-50 as our backbone\nmodel, higher capacity models like Conformers [9] might better\ncapture the complex relationship between speech and emotion\non the pre-training datasets we explored. We believe that the\ninitial investigation and findings of this work provide valuable\ninsights for future SER research on large-scale unlabeled data.\n7. References\n[1] A. Radford et al., “Learning transferable visual models from nat-\nural language supervision,” in Proceedings of the 38th Interna-\ntional Conference on Machine Learning, ser. Proceedings of Ma-\nchine Learning Research, vol. 139. PMLR, 18–24 Jul 2021, pp.\n8748–8763.\n[2] T. Brown et al., “Language models are few-shot learners,” in Ad-\nvances in Neural Information Processing Systems, H. Larochelle,\nM. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., vol. 33.\nCurran Associates, Inc., 2020, pp. 1877–1901.\n[3] P. Liang et al., “Holistic evaluation of language models,” CoRR,\nvol. abs/2211.09110, 2022.\n[4] M. Sap, H. Rashkin, D. Chen, R. L. Bras, and Y . Choi, “Social\nIQa: Commonsense reasoning about social interactions,” in Pro-\nceedings of the 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint Conference\non Natural Language Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019 . Association for Computa-\ntional Linguistics, 2019, pp. 4462–4472.\n[5] A. Yang, A. Miech, J. Sivic, I. Laptev, and C. Schmid, “Just ask:\nLearning to answer questions from millions of narrated videos,”\nin 2021 IEEE/CVF International Conference on Computer Vision,\nICCV 2021, Montreal, QC, Canada, October 10-17, 2021. IEEE,\n2021, pp. 1666–1677.\n[6] S. Shon, P. Brusco, J. Pan, K. J. Han, and S. Watanabe, “Leverag-\ning Pre-Trained Language Model for Speech Sentiment Analysis,”\nin Proc. Interspeech 2021, 2021, pp. 3420–3424.\n[7] J. Zhao, R. Li, Q. Jin, X. Wang, and H. Li, “Memobert:\nPre-training model with prompt-based learning for multimodal\nemotion recognition,” in ICASSP 2022 - 2022 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Processing\n(ICASSP), 2022, pp. 4703–4707.\n[8] A. Baevski, Y . Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0:\nA framework for self-supervised learning of speech representa-\ntions,” in Advances in Neural Information Processing Systems ,\nvol. 33. Curran Associates, Inc., 2020, pp. 12 449–12 460.\n[9] J. Shor, A. Jansen, W. Han, D. Park, and Y . Zhang, “Universal\nparalinguistic speech representations using self-supervised con-\nformers,” in ICASSP 2022 - 2022 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP), 2022, pp.\n3169–3173.\n[10] S. Yang et al. , “SUPERB: speech processing universal perfor-\nmance benchmark,” inInterspeech 2021, 22nd Annual Conference\nof the International Speech Communication Association, Brno,\nCzechia, 30 August - 3 September 2021. ISCA, 2021, pp. 1194–\n1198.\n[11] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding,” in NAACL. Minneapolis, Minnesota: Association for\nComputational Linguistics, Jun. 2019, pp. 4171–4186.\n[12] W. Yin, J. Hay, and D. Roth, “Benchmarking zero-shot text clas-\nsification: Datasets, evaluation and entailment approach,” in Pro-\nceedings of the 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-IJCNLP). Hong\nKong, China: Association for Computational Linguistics, Nov.\n2019, pp. 3914–3923.\n[13] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,\nM. Lewis, L. Zettlemoyer, and V . Stoyanov, “Roberta: A\nrobustly optimized BERT pretraining approach,” CoRR, vol.\nabs/1907.11692, 2019.\n[14] A. Williams, N. Nangia, and S. Bowman, “A broad-coverage chal-\nlenge corpus for sentence understanding through inference,” in\nNAACL. Association for Computational Linguistics, 2018, pp.\n1112–1122.\n[15] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig,\n“Pre-train, prompt, and predict: A systematic survey of prompt-\ning methods in natural language processing,”ACM Comput. Surv.,\nvol. 55, no. 9, pp. 195:1–195:35, 2023.\n[16] S. Deng, L. Wu, G. Shi, L. Xing, and M. Jian, “Learning to com-\npose diversified prompts for image emotion classification,”CoRR,\nvol. abs/2201.10963, 2022.\n[17] T. Gao, A. Fisch, and D. Chen, “Making pre-trained language\nmodels better few-shot learners,” in Proceedings of the 59th An-\nnual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers). Association for Computa-\ntional Linguistics, Aug. 2021, pp. 3816–3830.\n[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower,\nS. Kim, J. N. Chang, S. Lee, and S. S. Narayanan, “Iemocap:\nInteractive emotional dyadic motion capture database,”Language\nresources and evaluation, vol. 42, no. 4, pp. 335–359, 2008.\n[19] H. Cao, D. G. Cooper, M. K. Keutmann, R. C. Gur, A. Nenkova,\nand R. Verma, “Crema-d: Crowd-sourced emotional multimodal\nactors dataset,”IEEE Transactions on Affective Computing, vol. 5,\nno. 4, pp. 377–390, 2014.\n[20] A. S. Cowen and D. Keltner, “Self-report captures 27 distinct cat-\negories of emotion bridged by continuous gradients,”Proceedings\nof the national academy of sciences, vol. 114, no. 38, pp. E7900–\nE7909, 2017.\n[21] A. Cowen et al., “How emotion is experienced and expressed in\nmultiple cultures: a large-scale experiment,” 2021.\n[22] D. Galvez, G. Diamos, J. M. C. Torres, J. F. Cer ´on, K. Achorn,\nA. Gopi, D. Kanter, M. Lam, M. Mazumder, and V . J. Reddi,\n“The people’s speech: A large-scale diverse english speech recog-\nnition dataset for commercial usage,” in Thirty-fifth Conference\non Neural Information Processing Systems Datasets and Bench-\nmarks Track (Round 1), 2021.\n[23] M. Bain, A. Nagrani, A. Brown, and A. Zisserman, “Con-\ndensed movies: Story based retrieval with contextual embed-\ndings,” in Proceedings of the Asian Conference on Computer Vi-\nsion (ACCV), November 2020.\n[24] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and\nI. Sutskever, “Robust speech recognition via large-scale weak su-\npervision,” Tech. Rep., Technical report, OpenAI, Tech. Rep.,\n2022.\n[25] J. Zhao, R. Li, and Q. Jin, “Missing modality imagination net-\nwork for emotion recognition with uncertain missing modalities,”\nin Proceedings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International Joint Con-\nference on Natural Language Processing (Volume 1: Long Pa-\npers). Online: Association for Computational Linguistics, Aug.\n2021, pp. 2608–2618.\n[26] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean,\n“Distributed representations of words and phrases and their com-\npositionality,” in Proceedings of the 26th International Confer-\nence on Neural Information Processing Systems - Volume 2 , ser.\nNIPS’13. Red Hook, NY , USA: Curran Associates Inc., 2013,\np. 3111–3119.\n[27] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” in2016 IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR). Los Alamitos, CA, USA: IEEE\nComputer Society, jun 2016, pp. 770–778.\n[28] D. P. Kingma and J. Ba, “Adam: A method for stochastic\noptimization,” in 3rd International Conference on Learning\nRepresentations, ICLR 2015, San Diego, CA, USA, May 7-9,\n2015, Conference Track Proceedings , Y . Bengio and Y . LeCun,\nEds., 2015. [Online]. Available: http://arxiv.org/abs/1412.6980\n[29] M. Abadi et al., “TensorFlow: Large-scale machine learning on\nheterogeneous systems,” 2015, software available from tensor-\nflow.org.\n[30] B. Lester, R. Al-Rfou, and N. Constant, “The power of scale for\nparameter-efficient prompt tuning,” in Proceedings of EMNLP .\nOnline and Punta Cana, Dominican Republic: Association for\nComputational Linguistics, Nov. 2021, pp. 3045–3059.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7693301439285278
    },
    {
      "name": "Natural language processing",
      "score": 0.6724270582199097
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6093007922172546
    },
    {
      "name": "Speech recognition",
      "score": 0.570319652557373
    },
    {
      "name": "Textual entailment",
      "score": 0.5411026477813721
    },
    {
      "name": "Emotion recognition",
      "score": 0.507777988910675
    },
    {
      "name": "Language model",
      "score": 0.4960683286190033
    },
    {
      "name": "Logical consequence",
      "score": 0.36737024784088135
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I157485424",
      "name": "Korea Advanced Institute of Science and Technology",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ],
  "cited_by": 9
}