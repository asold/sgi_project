{
  "title": "Pale Transformer: A General Vision Transformer Backbone with Pale-Shaped Attention",
  "url": "https://openalex.org/W4226424839",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2774750402",
      "name": "Si-Tong Wu",
      "affiliations": [
        "National Engineering Laboratory of Deep Learning Technology and Application",
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2137234310",
      "name": "Tianyi Wu",
      "affiliations": [
        "Baidu (China)",
        "National Engineering Laboratory of Deep Learning Technology and Application"
      ]
    },
    {
      "id": "https://openalex.org/A3176913300",
      "name": "Hao-Ru Tan",
      "affiliations": [
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2116438665",
      "name": "Guodong Guo",
      "affiliations": [
        "Baidu (China)",
        "National Engineering Laboratory of Deep Learning Technology and Application"
      ]
    },
    {
      "id": "https://openalex.org/A2774750402",
      "name": "Si-Tong Wu",
      "affiliations": [
        "National Engineering Laboratory of Deep Learning Technology and Application",
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2137234310",
      "name": "Tianyi Wu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3176913300",
      "name": "Hao-Ru Tan",
      "affiliations": [
        "National Engineering Laboratory of Deep Learning Technology and Application",
        "Baidu (China)",
        "University of Chinese Academy of Sciences",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2116438665",
      "name": "Guodong Guo",
      "affiliations": [
        "Baidu (China)",
        "National Engineering Laboratory of Deep Learning Technology and Application"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3187216907",
    "https://openalex.org/W6794053914",
    "https://openalex.org/W6735463952",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W2798812533",
    "https://openalex.org/W6639102338",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2740711318",
    "https://openalex.org/W6776188000",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3011199263",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W6791943378",
    "https://openalex.org/W2901189993",
    "https://openalex.org/W6766867183",
    "https://openalex.org/W6753421600",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W6799770928",
    "https://openalex.org/W3139633126",
    "https://openalex.org/W2907052985",
    "https://openalex.org/W2507296351",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W4287203089",
    "https://openalex.org/W3170188883",
    "https://openalex.org/W3097065222",
    "https://openalex.org/W3192174868",
    "https://openalex.org/W3174402370",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W3204214911",
    "https://openalex.org/W3176153963",
    "https://openalex.org/W3168124404",
    "https://openalex.org/W4302343710",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2884822772",
    "https://openalex.org/W4308536459",
    "https://openalex.org/W3152698000",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W3172801447",
    "https://openalex.org/W3157528469",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W3139049060",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W4287025584",
    "https://openalex.org/W4214713996",
    "https://openalex.org/W4313007769",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W3156811085",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W4312599212",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W3190216403",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W4295308583",
    "https://openalex.org/W3168114581",
    "https://openalex.org/W3112503277",
    "https://openalex.org/W3181925591",
    "https://openalex.org/W3177183540",
    "https://openalex.org/W3168101492",
    "https://openalex.org/W3151130473",
    "https://openalex.org/W3175515048",
    "https://openalex.org/W3172661913",
    "https://openalex.org/W3166942762",
    "https://openalex.org/W3160694286",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4287324101"
  ],
  "abstract": "Recently, Transformers have shown promising performance in various vision tasks. To reduce the quadratic computation complexity caused by the global self-attention, various methods constrain the range of attention within a local region to improve its efficiency. Consequently, their receptive fields in a single attention layer are not large enough, resulting in insufficient context modeling. To address this issue, we propose a Pale-Shaped self-Attention (PS-Attention), which performs self-attention within a pale-shaped region. Compared to the global self-attention, PS-Attention can reduce the computation and memory costs significantly. Meanwhile, it can capture richer contextual information under the similar computation complexity with previous local self-attention mechanisms. Based on the PS-Attention, we develop a general Vision Transformer backbone with a hierarchical architecture, named Pale Transformer, which achieves 83.4%, 84.3%, and 84.9% Top-1 accuracy with the model size of 22M, 48M, and 85M respectively for 224x224 ImageNet-1K classification, outperforming the previous Vision Transformer backbones. For downstream tasks, our Pale Transformer backbone performs better than the recent state-of-the-art CSWin Transformer by a large margin on ADE20K semantic segmentation and COCO object detection &amp; instance segmentation. The code will be released on https://github.com/BR-IDL/PaddleViT.",
  "full_text": "Pale Transformer: A General Vision Transformer Backbone with Pale-Shaped\nAttention\nSitong Wu1,2, Tianyi Wu1,2, Haoru Tan3, Guodong Guo1,2 *\n1Institute of Deep Learning, Baidu Research, Beijing, China\n2National Engineering Laboratory for Deep Learning Technology and Application, Beijing, China\n3School of Artiﬁcial Intelligence, University of Chinese Academy of Sciences, Beijing, China\nwusitong98@gmail.com, {wutianyi01, guoguodong01}@baidu.com, tanhaoru2018@ia.ac.cn\nAbstract\nRecently, Transformers have shown promising performance\nin various vision tasks. To reduce the quadratic computation\ncomplexity caused by the global self-attention, various meth-\nods constrain the range of attention within a local region to\nimprove its efﬁciency. Consequently, their receptive ﬁelds in\na single attention layer are not large enough, resulting in in-\nsufﬁcient context modeling. To address this issue, we propose\na Pale-Shaped self-Attention (PS-Attention), which performs\nself-attention within a pale-shaped region. Compared to the\nglobal self-attention, PS-Attention can reduce the computa-\ntion and memory costs signiﬁcantly. Meanwhile, it can cap-\nture richer contextual information under the similar compu-\ntation complexity with previous local self-attention mecha-\nnisms. Based on the PS-Attention, we develop a general Vi-\nsion Transformer backbone with a hierarchical architecture,\nnamed Pale Transformer, which achieves 83.4%, 84.3%, and\n84.9% Top-1 accuracy with the model size of 22M, 48M,\nand 85M respectively for 224 \u0002 224 ImageNet-1K classiﬁ-\ncation, outperforming the previous Vision Transformer back-\nbones. For downstream tasks, our Pale Transformer back-\nbone performs better than the recent state-of-the-art CSWin\nTransformer by a large margin on ADE20K semantic seg-\nmentation and COCO object detection & instance segmen-\ntation. The code will be released on https://github.com/BR-\nIDL/PaddleViT.\nIntroduction\nInspired by the success of Transformer (Vaswani et al. 2017)\non a wide range of tasks in natural language processing\n(NLP) (McCann et al. 2017; Howard and Ruder 2018), Vi-\nsion Transformer (ViT) (Dosovitskiy et al. 2021) ﬁrst em-\nployed a pure Transformer architecture for image classiﬁ-\ncation, which shows the promising performance of Trans-\nformer architecture for vision tasks. However, the quadratic\ncomplexity of the global self-attention results in expensive\ncomputation costs and memory usage especially for high-\nresolution scenarios, making it unaffordable for applications\nin various vision tasks.\nA typical way to improve the efﬁciency is to replace the\nglobal self-attention with local ones. A crucial and challeng-\ning issue is how to enhance the modeling capability under\n*Corresponding author.\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nthe local settings. For example, Swin (Liu et al. 2021) and\nShufﬂe Transformer (Huang et al. 2021) proposed shifted\nwindow and shufﬂed window, respectively (Figure 1(b)),\nand alternately used two different window partitions (i.e.,\nregular window and the proposed window) in consecutive\nblocks to build cross-window connections. MSG Trans-\nformer (Fang et al. 2021) manipulated the messenger to-\nkens to exchange information across windows. Axial self-\nattention (Wang et al. 2020) treated the local attention re-\ngion as a single row or column of the feature map (Figure\n1(c)). CSWin (Dong et al. 2021) proposed cross-shaped win-\ndow self-attention (Figure 1(d)), which can be regarded as a\nmultiple row and column expansion of axial self-attention.\nAlthough these methods achieve excellent performance and\nare even superior to the CNN counterparts, the dependencies\nin each self-attention layer are not rich enough for capturing\nsufﬁcient contextual information.\nIn this work, we propose a Pale-Shaped self-Attention\n(PS-Attention) to capture richer contextual dependencies\nefﬁciently. Speciﬁcally, the input feature maps are ﬁrst\nsplit into multiple pale-shaped regions spatially. Each pale-\nshaped region (abbreviating as pale) is composed of the\nsame number of interlaced rows and columns of the fea-\nture map. The intervals between adjacent rows or columns\nare equal for all the pales. For example, the pink shadow in\nFigure 1(e) indicates one of the pales. Then, self-attention\nis performed within each pale. For any token, it can directly\ninteract with other tokens within the same pale, which en-\ndows our method with the capacity of capturing richer con-\ntextual information in a single PS-Attention layer. To fur-\nther improve the efﬁciency, we develop a more efﬁcient par-\nallel implementation of the PS-Attention. Beneﬁt from the\nlarger receptive ﬁelds and stronger context modeling capa-\nbility, our PS-Attention shows superiority to the existing lo-\ncal self-attention mechanisms illustrated in Figure 1.\nBased on the proposed PS-Attention, we design a gen-\neral vision transformer backbone with a hierarchical archi-\ntecture, named Pale Transformer. We scale our approach\nup to get a series of models, including Pale-T (22M),\nPale-S (48M), and Pale-B (85M), reaching signiﬁcantly\nbetter performance than previous approaches. Our Pale-T\nachieves 83.4% Top-1 classiﬁcation accuracy on ImageNet-\n1k, 50.4% single-scale mIoU on ADE20K (semantic seg-\nmentation), 47.4 box mAP (object detection) and 42.7 mask\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n2731\n(a) Global Self-Attention\nRegular Window\n Shifted Window\n Shufﬂed Window\n Messenger\n(b) Window-based Self-Attention\n(c) Axial Self-Attention (d) Cross-Shaped (e) Pale-Shaped\nWindow Self-Attention Self-Attention (ours)\nFigure 1: Illustration of different self-attention mechanisms in Transformer backbones. (a) is the standard global self-attention.\n(b) Window-based self-attention mechanisms perform attention inside each window, and introduce various strategies to build\ncross-window connections. Different colors in (b) represent different windows. In (c), (d), and (e), the input features are ﬁrst\nsplit into multiple groups, one of which is illustrated by the shadow area, and the self-attention is conducted within each group.\nThus, for a reference token denoted by the red dot, it can interact directly with the tokens covered by the shadow area.\nmAP (instance segmentation) on COCO, outperforming the\nstate-of-the-art backbones by +0.7%, +1.1%, +0.7, and +0.5,\nrespectively. Furthermore, our largest variant Pale-B is also\nsuperior to the previous methods, achieving 84.9% Top-\n1 accuracy on ImageNet-1K, 52.2% single-scale mIoU on\nADE20K, 49.3 box mAP and 44.2 mask mAP on COCO.\nRelated Work\nViT (Dosovitskiy et al. 2021), which takes the input image\nas a sequence of patches, has paved a new way and shown\npromising performance for many vision tasks dominated by\nCNNs over the years. A line of previous Vision Transformer\nbackbones mainly focused on the following two aspects to\nbetter adapt to vision tasks: (1) Enhancing the locality of\nVision Transformers. (2) Seeking a better trade-off between\nperformance and efﬁciency.\nLocally-Enhanced Vision Transformers\nDifferent from CNNs, the inductive bias for local connec-\ntions is not involved in the original Transformer, which may\nlead to insufﬁcient extraction of local structures, such as\nlines, edges, and color conjunctions. Many works are de-\nvoted to strengthening the local feature extraction of Vision\nTransformers. The earliest approach is to replace the single-\nscale architecture of ViT with a hierarchical one to obtain\nmulti-scale features (Wang et al. 2021b). Such design is fol-\nlowed by many works afterward (Liu et al. 2021; Huang\net al. 2021; Yang et al. 2021; Dong et al. 2021). Another\nway is to combine CNNs and Transformers. Mobile-Former\n(Chen et al. 2021b), Conformer (Peng et al. 2021) and DS-\nNet (Mao et al. 2021) integrated the CNN and Transformer\nfeatures by the well-designed dual-branch structures. In con-\ntrast, Local ViT (Li et al. 2021b), CvT (Wu et al. 2021a) and\nShufﬂe Transformer (Huang et al. 2021) only inserted sev-\neral convolutions into some components of Transformer. Be-\nsides, some works obtain richer features by fusing the multi-\nbranch with different scales (Chen, Fan, and Panda 2021)\nor cooperating with local attention (Han et al. 2021; Zhang\net al. 2021; Chu et al. 2021a; Li et al. 2021a; Yuan et al.\n2021b).\nEfﬁcient Vision Transformers\nThe mainstream research on improving the efﬁciency for\nVision Transformer backbones has two folds: reducing the\nredundant calculations via pruning strategies and designing\nmore efﬁcient self-attention mechanisms.\nPruning Strategies for Vision Transformers. For prun-\ning, the existing methods can be divided into three cate-\ngories: (1) Token Pruning. DVT (Wang et al. 2021d) pro-\nposed a cascade Transformer architecture to adaptively ad-\njust the number of tokens according to the hardness for clas-\nsiﬁcation of the input image. Considering that tokens with\nirrelevant or even confusing information may be detrimen-\ntal to image classiﬁcation, some works proposed to locate\ndiscriminative regions and progressively drop less informa-\ntive tokens by learnable sampling (Rao et al. 2021; Yue\net al. 2021) and reinforcement learning (Pan et al. 2021)\nstrategies. However, such unstructured sparsity results in in-\ncompatibility with dense prediction tasks. Some structure-\npreserving token selection strategies were implemented via\ntoken pooling (Chen et al. 2021a) and a slow-fast updat-\ning (Xu et al. 2021). (2) Channel Pruning. VTP (Zhu et al.\n2732\nFigure 2: (a) The overall architecture of our Pale Transformer. (b) The composition of each block. (c) Illustration of parallel\nimplementation of PS-Attention. For a reference token (red dot), it can directly interact with the tokens within the shadow area.\n2021a) presented a simple but effective framework to re-\nmove the reductant channels. (3) Attention Sharing. Based\non the observation that attention maps from continuous\nblocks are highly correlated, PSViT (Chen et al. 2021a) was\nproposed to reuse the attention calculation process between\nadjacent layers.\nEfﬁcient Self-Attention Mechanisms. Considering that\nthe quadratic computation complexity is caused by self-\nattention, many methods are committed to improving its\nefﬁciency while avoiding performance decay (Wang et al.\n2021b; Zhu et al. 2021b; Liu et al. 2021; Huang et al. 2021).\nOne way is to reduce the sequence length of key and value.\nPVT (Wang et al. 2021b) proposed a spatial reduction atten-\ntion to downsample the scale of key and value before com-\nputing attention. Deformable attention (Zhu et al. 2021b)\nused a linear layer to select several keys from the full set,\nwhich can be regarded as a sparse version of global self-\nattention. However, excessive downsampling will lead to in-\nformation confusion, and deformable attention relies heav-\nily on a high-level feature map learned by CNN and may\nnot be directly used on the original input image. Another\nway is to replace the global self-attention with local self-\nattention, which limits the range of each self-attention layer\ninto a local region. As shown in Figure 1(b), the feature maps\nare ﬁrst divided into several non-overlapping square regu-\nlar windows (indicated with diverse colors), and the self-\nattention is performed within each window individually. The\nkey challenge for the design of local self-attention mecha-\nnisms is to bridge the gap between local and global recep-\ntive ﬁelds. A typical manner is to build connections across\nregular square windows. For example, alternately using reg-\nular window and another newly designed window partition\nmanner (shifted window (Liu et al. 2021) or shufﬂed win-\ndow (Huang et al. 2021) in Figure 1(b)) in consecutive\nblocks, and manipulating messenger tokens to exchange in-\nformation across windows (Fang et al. 2021). Besides, axial\nself-attention (Wang et al. 2020) achieves longer-range de-\npendencies in horizontal and vertical directions respectively\nby performing self-attention in each single row or column\nof the feature map. CSWin (Dong et al. 2021) proposed a\ncross-shaped window self-attention region including mul-\ntiple rows and columns. Although these existing local at-\ntention mechanisms can provide opportunities for breaking\nthrough the local receptive ﬁelds to some extent, their depen-\ndencies are not rich enough to capture sufﬁcient contextual\ninformation in a single self-attention layer, which limits the\nmodeling capacity of the whole network.\nThe most related to our work is CSWin (Dong et al. 2021),\nwhich developed a cross-shaped window self-attention\nmechanism for computing self-attention in the horizontal\nand vertical stripes, while our proposed PS-Attention com-\nputes self-attention in the pale-shaped regions. Moreover,\nthe receptive ﬁelds of each token in our method are much\nwider than CSWin, which also endows our approach with\nstronger context modeling capacity.\nMethodology\nIn this section, we ﬁrst present our Pale-Shaped self-\nAttention (PS-Attention) and its efﬁcient parallel implemen-\ntation. Then, the composition of the Pale Transformer block\n2733\nis given. Finally, we describe the overall architecture and\nvariants conﬁgurations of our Pale Transformer backbone.\nPale-Shaped self-Attention\nFor capturing dependencies varied from short-range to\nlong-range, we propose Pale-Shaped self-Attention (PS-\nAttention), which computes self-attention within a pale-\nshaped region (abbreviating as pale). As shown in the pink\nshadow of Figure 1(e), one pale contains sr interlaced rows\nand sc interlaced columns, which covers a region containing\n(srw+ sch−srsc) tokens. We deﬁne (sr;sc) as the pale\nsize. Given an input feature map X ∈ Rh×w×c, we ﬁrst\nsplit it into multiple pales {P1;:::;P N }with the same size\n(sr;sc), where Pi ∈R(srw+sch−srsc)×c;i ∈{1;2;:::;N }.\nThe number of pales is equal toN = h\nsr\n= w\nsc\n, which can be\nensured by padding or interpolation operation. For all pales,\nintervals between adjacent rows or columns are the same.\nThe self-attention is then performed within each pale indi-\nvidually. As illustrated in Figure 1, the receptive ﬁeld of PS-\nAttention is signiﬁcantly wider and richer than all the previ-\nous local self-attention mechanisms, enabling more power-\nful context modeling capacity.\nEfﬁcient Parallel Implementation. To further improve\nthe efﬁciency, we decompose the vanilla PS-Attention men-\ntioned above into row-wise and column-wise attention,\nwhich perform self-attention within row-wise and column-\nwise token groups, respectively. Speciﬁcally, as shown in\nFigure 2(c), we ﬁrst divide the input feature X ∈Rh×w×c\ninto two independent parts Xr ∈ Rh×w×c\n2 and Xc ∈\nRh×w×c\n2 in the channel dimension, which are then split into\nmultiple groups for row-wise and column-wise attention re-\nspectively.\nXr = [X1\nr ;:::;X Nr\nr ];Xc = [X1\nc ;:::;X Nc\nc ]; (1)\nwhere Nr = h=sr, Nc = w=sc, Xi\nr ∈Rsr×w×c contains sr\ninterlaced rows, and Xj\nc ∈Rh×sc×c contains sc interlaced\ncolumns.\nThen, the self-attention is conducted within each row-\nwise and column-wise token group, respectively. Similar to\n(Wu et al. 2021a), we use three separable convolution layers\n\u001eQ, \u001eK, and \u001eV to generate the query, key, and value.\nYi\nr = MSA(\u001eQ(Xi\nr);\u001eK(Xi\nr);\u001eV (Xi\nr));\nYi\nc = MSA(\u001eQ(Xi\nc);\u001eK(Xi\nc);\u001eV (Xi\nc));\n(2)\nwhere i ∈{1;2;:::;N }, and MSA indicates the Multi-head\nSelf-Attention (Dosovitskiy et al. 2021).\nFinally, the outputs of row-wise and column-wise atten-\ntion are concatenated along channel dimension, resulting in\nthe ﬁnal output Y ∈Rh×w×c,\nY = Concat(Yr;Yc); (3)\nwhere Yr = [Y1\nr ;:::;Y Nr\nr ] and Yc = [Y1\nc ;:::;Y Nc\nc ].\nCompared to the vanilla implementation of PS-Attention\nwithin the whole pale, such a parallel mechanism has a lower\ncomputation complexity. Furthermore, the padding opera-\ntion only needs to ensure h can be divisible by sr and w\ncan be divisible by sc, rather than h\nsr\n= w\nsc\n. Therefore, it is\nalso conducive to avoiding excessive padding.\nComplexity Analysis. Given the input feature of size h×\nw×cand pale size(sr;sc), the standard global self-attention\nhas a computational complexity of\nOGlobal = 4hwc2 + 2c(hw)2; (4)\nhowever, our proposed PS-Attention under the parallel im-\nplementation has a computational complexity of\nOPale = 4hwc2 + hwc(sch+ srw+ 27)<<OGlobal; (5)\nwhich can obviously alleviate the computation and memory\nburden compared with the global one, since2hw>> (sch+\nsrw+ 27)always holds. The detailed derivations of Eq. (4)\nand Eq. (5) are provided in the supplementary material.\nPale Transformer Block\nAs shown in Figure 2(b), our Pale Transformer block con-\nsists of three sequential parts, the conditional position en-\ncoding (CPE) for dynamically generating the positional em-\nbedding, the proposed PS-Attention module for capturing\ncontextual information, and the MLP module for feature\nprojection. The forward pass of the l-th block can be for-\nmulated as follows:\n~Xl = Xl−1 + CPE(Xl−1); (6)\n^Xl = ~Xl + PS-Attention\n\u0010\nLN( ~Xl)\n\u0011\n; (7)\nXl = ^Xl + MLP\n\u0010\nLN( ^Xl)\n\u0011\n; (8)\nwhere LN(·) refers to layer normalization (Ba, Kiros, and\nHinton 2016). The CPE (Chu et al. 2021b) is implemented\nas a simple depth-wise convolution, which is widely used\nin previous works (Wu et al. 2021b; Chu et al. 2021a) for\nits compatibility with an arbitrary size of input. The PS-\nAttention module deﬁned in Eq. (7) is constructed by se-\nquentially performing Eq. (1) to Eq. (3). The MLP module\ndeﬁned in Eq. (8) consists of two linear projection layers to\nexpand and contract the embedding dimension sequentially,\nwhich is the same as (Dosovitskiy et al. 2021) for fair com-\nparisons.\nOverall Architecture and Variants\nAs illustrated in Figure 2(a), the Pale Transformer consists\nof four hierarchical stages for capturing multi-scale fea-\ntures by following the popular design in CNNs (He et al.\n2016) and Transformers (Liu et al. 2021; Dong et al. 2021).\nEach stage contains a patch merging layer and multiple Pale\nTransformer blocks. The patch merging layer aims to spa-\ntially downsample the input features by a certain ratio and\nexpand the channel dimension by twice for a better represen-\ntation capacity. For fair comparisons, we use the overlapping\nconvolution for patch merging, the same as (Wu et al. 2021a;\nDong et al. 2021). Speciﬁcally, the spatial downsampling ra-\ntio is set to 4 for the ﬁrst stage and 2 for the last three stages,\nimplementing by 7 ×7 convolution with stride 4 and 3 ×3\nconvolution with stride 2, respectively. The outputs of the\npatch merging layer are fed into the subsequent Pale Trans-\nformer blocks, with the number of tokens kept constant. Fol-\nlowing (Liu et al. 2021; Dong et al. 2021), we simply apply\nan average pooling operation on the top of the last block to\nobtain a representative token for the ﬁnal classiﬁcation head,\nwhich is composed of a single linear projection layer.\n2734\nStage Layer Pale-T Pale-S Pale-B\n1\nPatch\nMerging\nP1 = 4\nC1 = 64\nP1 = 4\nC1 = 96\nP1 = 4\nC1 = 128\nPale\nBlock\n2\n4\nS1 = 7\nH1 = 2\nR1 = 4\n3\n5 \u0002 2\n2\n4\nS1 = 7\nH1 = 2\nR1 = 4\n3\n5 \u0002 2\n2\n4\nS1 = 7\nH1 = 4\nR1 = 4\n3\n5 \u0002 2\n2\nPatch\nMerging\nP2 = 2\nC2 = 128\nP2 = 2\nC2 = 192\nP2 = 2\nC2 = 256\nPale\nBlock\n2\n4\nS2 = 7\nH2 = 4\nR2 = 4\n3\n5 \u0002 2\n2\n4\nS2 = 7\nH2 = 4\nR2 = 4\n3\n5 \u0002 2\n2\n4\nS2 = 7\nH2 = 8\nR2 = 4\n3\n5 \u0002 2\n3\nPatch\nMerging\nP3 = 2\nC3 = 256\nP3 = 2\nC3 = 384\nP3 = 2\nC3 = 512\nPale\nBlock\n2\n4\nS3 = 7\nH3 = 8\nR3 = 4\n3\n5 \u0002 16\n2\n4\nS3 = 7\nH3 = 8\nR3 = 4\n3\n5 \u0002 16\n2\n4\nS3 = 7\nH3 = 16\nR3 = 4\n3\n5 \u0002 16\n4\nPatch\nMerging\nP4 = 2\nC4 = 512\nP4 = 2\nC4 = 768\nP4 = 2\nC4 = 1024\nPale\nBlock\n2\n4\nS4 = 7\nH4 = 16\nR4 = 4\n3\n5 \u0002 2\n2\n4\nS4 = 7\nH4 = 16\nR4 = 4\n3\n5 \u0002 2\n2\n4\nS4 = 7\nH4 = 32\nR4 = 4\n3\n5 \u0002 2\nTable 1: Detailed conﬁgurations of Pale Transformer Vari-\nants.\nVariants. The deﬁnitions of model hyper-parameters for\nthe i-th stage are listed below:\n• Pi: the spatial reduction factor for patch merging layer,\n• Ci: the embedding dimension of tokens,\n• Si: the pale size for the PS-Attention,\n• Hi: the head number for the PS-Attention,\n• Ri: the expansion ratio for the MLP module.\nBy varying the hyper-parameters Hi and Ci in each\nstage, we design three variants of our Pale Transformer,\nnamed Pale-T (Tiny), Pale-S (Small), and Pale-B (Base), re-\nspectively. Table 1 shows the detailed conﬁgurations of all\nvariants. Note that all variants have the same depth with\n[2;2;16;2] in four stages. In each stage of these variants,\nwe set the pale size sr = sc = Si = 7, and use the same\nMLP expansion ratio of Ri = 4. Thus, the main differences\namong Pale-T, Pale-S, and Pale-B lie in the embedding di-\nmension of tokens and the head number for the PS-Attention\nin four stages, i.e., variants vary from narrow to wide.\nExperiments\nWe ﬁrst compare our Pale Transformer with the state-of-the-\nart Transformer backbones on ImageNet-1K (Russakovsky\net al. 2015) for image classiﬁcation. To further demonstrate\nthe effectiveness and generalization of our backbone, we\nconduct experiments on ADE20K (Zhou et al. 2019) for se-\nmantic segmentation, and COCO (Lin et al. 2014) for object\ndetection & instance segmentation. Finally, we dig into the\ndesign of key components of our Pale Transformer to better\nunderstand the method.\nMethod Params FLOPs Top-1\n(%)\nRegNetY-4G (Radosavovic et al. 2020) 21M 4.0G 80.0\nDeiT-S (Touvron et al. 2021) 22M 4.6G 79.8\nPVT-S (Wang et al. 2021b) 25M 3.8G 79.8\nT2T-14 (Yuan et al. 2021a) 22M 6.1G 80.7\nDPT-S (Chen et al. 2021c) 26M 4.0G 81.0\nTNT-S (Han et al. 2021) 24M 5.2G 81.3\nSwin-T (Liu et al. 2021) 29M 4.5G 81.3\nTwins-SVT-S (Chu et al. 2021a) 24M 2.8G 81.3\nCvT-13 (Wu et al. 2021a) 20M 4.5G 81.6\nViL-S (Zhang et al. 2021) 25M 4.9G 82.0\nPVTv2-B2 (Wang et al. 2021a) 25M 4.0G 82.0\nFocal-T (Yang et al. 2021) 29M 4.9G 82.2\nShufﬂe-T (Huang et al. 2021) 29M 4.6G 82.5\nCSWin-T (Dong et al. 2021) 23M 4.3G 82.7\nLV-ViT-S? (Jiang et al. 2021) 26M 6.6G 83.3\nPale-T (ours) 22M 4.2G 83.4\nPale-T? (ours) 22M 4.2G 84.2\nRegNetY-8G (Radosavovic et al. 2020) 39M 8.0G 81.7\nPVT-M (Wang et al. 2021b) 44M 6.7G 81.2\nT2T-19 (Yuan et al. 2021a) 39M 9.8G 81.4\nDPT-M (Chen et al. 2021c) 46M 6.9G 81.9\nCvT-21 (Wu et al. 2021a) 32M 7.1G 82.5\nSwin-S (Liu et al. 2021) 50M 8.7G 83.0\nMViT-B-24 (Fan et al. 2021) 54M 10.9G 83.1\nTwins-SVT-B (Chu et al. 2021a) 56M 8.3G 83.1\nPVTv2-B3 (Wang et al. 2021a) 45M 6.9G 83.2\nViL-M (Zhang et al. 2021) 40M 8.7G 83.3\nFocal-S (Yang et al. 2021) 51M 9.1G 83.5\nShufﬂe-S (Huang et al. 2021) 50M 8.9G 83.5\nCSWin-S (Dong et al. 2021) 35M 6.9G 83.6\nReﬁned-ViT-S (Zhou et al. 2021) 25M 7.2G 83.6\nVOLO-D1? (Yuan et al. 2021b) 27M 6.8G 84.2\nPale-S (ours) 48M 9.0G 84.3\nPale-S? (ours) 48M 9.0G 85.0\nRegNetY-16G (Radosavovic et al. 2020) 84M 16.0G 82.9\nViT-B/16z (Dosovitskiy et al. 2021) 86M 55.4G 77.9\nPVT-L (Wang et al. 2021b) 61M 9.8G 81.7\nDeiT-B (Touvron et al. 2021) 86M 17.5G 81.8\nT2T-24 (Yuan et al. 2021a) 64M 15.0G 82.2\nTNT-B (Han et al. 2021) 66M 14.1G 82.8\nViL-B (Zhang et al. 2021) 56M 13.4G 83.2\nSwin-B (Liu et al. 2021) 88M 15.4G 83.3\nTwins-SVT-L (Chu et al. 2021a) 99M 14.8G 83.3\nPVTv2-B5 (Wang et al. 2021a) 82M 11.8G 83.8\nFocal-B (Yang et al. 2021) 90M 16.0G 83.8\nShufﬂe-B (Huang et al. 2021) 88M 15.6G 84.0\nLV-ViT-M? (Jiang et al. 2021) 56M 16.0G 84.1\nCSWin-B (Dong et al. 2021) 78M 15.0G 84.2\nReﬁned-ViT-M (Zhou et al. 2021) 55M 13.5G 84.6\nVOLO-D2? (Yuan et al. 2021b) 59M 14.1G 85.2\nPale-B (ours) 85M 15.6G 84.9\nPale-B? (ours) 85M 15.6G 85.8\nTable 2: Comparisons of different backbones on ImageNet-\n1K validation set. All the approaches are trained and eval-\nuated with the size of 224 ×224, except for the ViT-B ‡\nwith size 384×384. The superscript “?” indicates employing\nMixToken and token labeling loss (Jiang et al. 2021) during\ntraining.\n2735\nBackbone Params FLOPs Mask R-CNN (1x)\nAPbox APbox\n50 APbox\n75 APmask APmask\n50 APmask\n75\nResNet-50 (He et al. 2016) 44M 260G 38.0 58.6 41.4 34.4 55.1 36.7\nPVT-S (Wang et al. 2021b) 44M 245G 40.4 62.9 43.8 37.8 60.1 40.3\nViL-S (Zhang et al. 2021) 45M 174G 41.8 64.1 45.1 38.5 61.1 41.4\nTwins-S (Chu et al. 2021a) 44M 228G 42.7 65.6 46.7 39.6 62.5 42.6\nDPT-S (Chen et al. 2021c) 46M - 43.1 65.7 47.2 39.9 62.9 43.0\nSwin-T (Liu et al. 2021) 48M 264G 43.7 66.6 47.6 39.8 63.3 42.7\nRegionViT-S+ (Chen, Panda, and Fan 2021) 51M 183G 44.2 67.3 48.2 40.8 64.1 44.0\nFocal-T (Yang et al. 2021) 49M 291G 44.8 67.7 49.2 41.0 64.7 44.2\nPVTv2-B2 (Wang et al. 2021a) 45M - 45.3 67.1 49.6 41.2 64.2 44.4\nCSWin-T (Dong et al. 2021) 42M 279G 46.7 68.6 51.3 42.2 65.6 45.4\nPale-T (ours) 41M 306G 47.4 69.2 52.3 42.7 66.3 46.2\nResNeXt-101-32 (He et al. 2016) 63M 340G 41.9 62.5 45.9 37.5 59.4 40.2\nPVT-M (Wang et al. 2021b) 64M 302G 42.0 64.4 45.6 39.0 61.6 42.1\nViL-M (Zhang et al. 2021) 60M 261G 43.4 65.9 47.0 39.7 62.8 42.1\nDPT-M (Chen et al. 2021c) 66M - 43.8 66.2 48.3 40.3 63.1 43.4\nTwins-B (Chu et al. 2021a) 76M 340G 45.1 67.0 49.4 41.1 64.1 44.4\nRegionViT-B+ (Chen, Panda, and Fan 2021) 93M 307G 45.4 68.4 49.6 41.6 65.2 44.8\nPVTv2-B3 (Wang et al. 2021a) 65M - 47.0 68.1 51.7 42.5 65.7 45.7\nFocal-S (Yang et al. 2021) 71M 401G 47.4 69.8 51.9 42.8 66.6 46.1\nCSWin-S (Dong et al. 2021) 54M 342G 47.9 70.1 52.6 43.2 67.1 46.2\nPale-S (ours) 68M 432G 48.4 70.4 53.2 43.7 67.7 47.1\nResNeXt-101-64 (He et al. 2016) 101M 493G 42.8 63.8 47.3 38.4 60.6 41.3\nPVT-L (Wang et al. 2021b) 81M 364G 42.9 65.0 46.6 39.5 61.9 42.5\nViL-B (Zhang et al. 2021) 76M 365G 45.1 67.2 49.3 41.0 64.3 44.2\nTwins-L (Chu et al. 2021a) 120M 474G 45.2 67.5 49.4 41.2 64.5 44.5\nPVTv2-B4 (Wang et al. 2021a) 82M - 47.5 68.7 52.0 42.7 66.1 46.1\nFocal-B (Yang et al. 2021) 110M 533G 47.8 70.2 52.5 43.2 67.3 46.5\nCSWin-B (Dong et al. 2021) 97M 526G 48.7 70.4 53.9 43.9 67.8 47.3\nPale-B (ours) 105M 595G 49.3 71.2 54.1 44.2 68.1 47.8\nTable 3: Comparisons on COCO val2017 with Mask R-CNN framework and 1x training schedule for object detection and\ninstance segmentation.\nImage Classiﬁcation on ImageNet-1K\nSettings. All the variants are trained from scratch for 300\nepochs on 8 V100 GPUs with a total batch size of 1024. Both\nthe training and evaluation are conducted with the input size\nof 224 ×224 on ImageNet-1K dataset.\nResults. Table 2 compares the performance of our Pale\nTransformer with the state-of-the-art CNNs and Vision\nTransformer backbones on ImageNet-1K validation set.\nCompared to the advanced CNNs, our Pale variants are\n+3.4%, +2.6%, and +2.0% better than the well-known Reg-\nNet models, respectively, under the similar computation\ncomplexity. Meanwhile, our Pale Transformer variants out-\nperform the state-of-the-art Transformer-based backbones,\nand is +0.7% higher than the most related CSWin Trans-\nformer. Note that LV-ViT (Jiang et al. 2021) and VOLO\n(Yuan et al. 2021b), using additional MixToken augmenta-\ntion and token labeling loss (Jiang et al. 2021) for training,\nseem to be on par with our approach. For fair comparisons,\nwe use these two tricks on our Pale models, labeled by ?\nas the superscript. Pale-T ? achieves +0.9% gain than LV-\nViT-S? with fewer computation costs. Pale-S ? and Pale-B?\nachieve 85.0% and 85.8%, outperforming VOLO by +0.8%\nand +0.6%, respectively.\nSemantic Segmentation on ADE20K\nSettings. To demonstrate the superiority of our Pale Trans-\nformer for dense prediction task (Wu et al. 2021b), we con-\nduct experiments on ADE20K with the widely-used Uper-\nNet (Xiao et al. 2018) as decoder for fair comparisons to\nother backbones. We report both the single-scale (SS) and\nmulti-scale (MS) mIoU for better comparison.\nResults. As shown in Table 4, our Pale variants are consis-\ntently superior to the state-of-the-art method by a large mar-\ngin. Speciﬁcally, our Pale-T and Pale-S outperform the state-\nof-the-art CSWin by +1.1% and +1.2% SS mIoU, respec-\ntively. Besides, our Pale-B achieves 52.5%/53.0% SS/MS\nmIoU, surpassing the previous best by +1.3% and +1.2%,\nrespectively. These results demonstrate the stronger context\nmodeling capacity of our Pale Transformer.\nObject Detection and Instance Segmentation on\nCOCO\nSettings. We evaluate the performance of our Pale Trans-\nformer backbone on COCO benchmark for object detection\nand instance segmentation, utilizing Mask R-CNN (He et al.\n2017) framework under 1x schedule (12 training epochs).\n2736\nBackbone Params FLOPs SS\nmIoU\nMS\nmIoU\nDeiT-S (Touvron et al. 2021) 52M 1099G - 44.0\nSwin-T (Liu et al. 2021) 60M 945G 44.5 45.8\nFocal-T (Yang et al. 2021) 62M 998G 45.8 47.0\nShufﬂe-T (Huang et al. 2021) 60M 949G 46.6 47.6\nCrossFormer-S (Wang et al. 2021c) 62M 980G 47.6 48.4\nLV-ViT-S (Jiang et al. 2021) 44M - 47.9 48.6\nCSWin-T (Dong et al. 2021) 60M 959G 49.3 50.4\nPale-T (ours) 52M 996G 50.4 51.2\nSwin-S (Liu et al. 2021) 81M 1038G 47.6 49.5\nFocal-S (Yang et al. 2021) 85M 1130G 48.0 50.0\nShufﬂe-S (Huang et al. 2021) 81M 1044G 48.4 49.6\nVOLO-D1 (Yuan et al. 2021b) - - - 50.5\nLV-ViT-M (Jiang et al. 2021) 77M - 49.4 50.6\nCrossFormer-B (Wang et al. 2021c) 84M 1090G 49.7 50.6\nCSWin-S (Dong et al. 2021) 65M 1027G 50.0 50.8\nPale-S (ours) 80M 1135G 51.2 52.2\nSwin-B (Liu et al. 2021) 121M 1188G 48.1 49.7\nShufﬂe-B (Huang et al. 2021) 121M 1196G 49.0 50.5\nFocal-B (Yang et al. 2021) 126M 1354G 49.0 50.5\nCrossFormer-L (Wang et al. 2021c) 126M 1258M 50.4 51.4\nCSWin-B (Dong et al. 2021) 109M 1222G 50.8 51.7\nLV-ViT-L (Jiang et al. 2021) 209M - 50.9 51.8\nPale-B (ours) 119M 1311G 52.2 53.0\nTable 4: Comparisons of different backbones with UperNet\nas decoder on ADE20K validation set for semantic segmen-\ntation. All backbones are pretrained on ImageNet-1K with\nthe size of 224 ×224. FLOPs are calculated with a resolu-\ntion of 512 ×2048.\nResults. As shown in Table 3, for object detection, our\nPale-T, Pale-S, and Pale-B achieve 47.4, 48.4, and 49.2\nbox mAP for object detection, surpassing the previous best\nCSWin Transformer by +0.7, +0.5, and +0.6, respectively.\nBesides, our variants also have consistent improvement on\ninstance segmentation, which are +0.5, +0.5, and +0.3 mask\nmAP higher than the previous best backbone.\nAblation Study\nWe conduct ablation studies for the key designs of our Pale\nTransformer on image classiﬁcation and downstream tasks.\nAll the experiments are performed with the Pale-T under the\nsame training settings as mentioned above.\nEffect of Pale Size. The pale sizes of four stages\n{S1;S2;S3;S4}control the trade-off between the richness\nof contextual information and computation costs. As shown\nin Table 5, increasing the pale size (from 1 to 7) can contin-\nuously improve performance across all tasks, while further\nup to 9 does not bring obvious and consistent improvements\nbut more FLOPs. Therefore, we use Si = 7;i ∈{1;2;3;4}\nfor all the tasks by default.\nComparisons with Different Implementations of PS-\nAttention. We compare three implementations of our PS-\nAttention. The vanilla PS-Attention directly conducts self-\nattention within the whole pale region, which can be approx-\nimated as two more efﬁcient implementations, sequential\nPale size\nin four stages\nImageNet-1K ADE20K COCO\nTop-1 (%) SS mIoU (%) APbox APmask\n1 1 1 1 82.4 47.9 46.1 41.5\n3 3 3 3 82.9 49.4 46.7 42.3\n5 5 5 5 83.1 49.7 46.8 42.4\n7 7 7 7 83.4 50.4 47.4 42.7\n9 9 9 9 83.3 50.6 47.4 42.6\nTable 5: Ablation study for different choices of pale size.\nThe complete table with parameters and FLOPs can be\nfound in the supplementary material.\nAttention mode ImageNet-1K ADE20K COCO\nTop-1 (%) SS mIoU (%) APbox APmask\nAxial 82.4 47.9 46.1 41.5\nCross-Shaped 82.8 49.0 46.6 42.2\nPale (vanilla) 83.4 50.3 47.1 42.3\nPale (sequential) 82.9 49.5 46.9 42.2\nPale (parallel) 83.4 50.4 47.4 42.7\nTable 6: Ablation study for different attention modes.\nand parallel. The sequential one computes self-attention in\nrow and column directions alternately in consecutive blocks,\nwhile the parallel one performs row-wise and column-wise\nattention in parallel within each block. As shown in Table\n6, the parallel PS-Attention achieves the best results on all\ntasks, even slightly better than the vanilla one by +0.3/0.4\nbox/mask mAP on COCO. We attribute this to that the ex-\ncessive padding for the non-square input size in vanilla PS-\nAttention will result in slight performance degradation.\nComparisons with other Axial-based Attentions. In or-\nder to compare our PS-Attention with the most related axial-\nbased self-attention mechanisms directly, we replace the PS-\nAttention of our Pale-T with the axial self-attention (Wang\net al. 2020) and cross-shaped window self-attention (Dong\net al. 2021), respectively. As shown in Table 6, our PS-\nAttention outperforms these two mechanisms obviously.\nConclusion\nThis work presented a new effective and efﬁcient self-\nattention mechanism, termed Pale-Shaped self-Attention\n(PS-Attention), which performs self-attention in a pale-\nshaped region. PS-Attention can model richer contextual\ndependencies than the previous local self-attention mech-\nanisms. In order to further improve its efﬁciency, we de-\nsigned a parallel implementation for PS-Attention, which\ndecomposes the self-attention within the whole pale into\nrow-wise and column-wise attention. Based on the proposed\nPS-Attention, we developed a general Vision Transformer\nbackbone, called Pale Transformer, which can achieve state-\nof-the-art performance on ImageNet-1K for image classiﬁ-\ncation. Furthermore, our Pale Transformer is superior to the\nprevious Vision Transformer backbones on ADE20K for se-\nmantic segmentation, and COCO for object detection & in-\nstance segmentation.\n2737\nReferences\nBa, J. L.; Kiros, J. R.; and Hinton, G. E. 2016. Layer nor-\nmalization. arXiv preprint arXiv:1607.06450.\nChen, B.; Li, P.; Li, B.; Li, C.; Bai, L.; Lin, C.; Sun, M.;\nYan, J.; and Ouyang, W. 2021a. PSViT: Better Vision Trans-\nformer via Token Pooling and Attention Sharing. arXiv\npreprint arXiv:2108.03428.\nChen, C.-F.; Fan, Q.; and Panda, R. 2021. Crossvit: Cross-\nattention multi-scale vision transformer for image classiﬁca-\ntion. arXiv preprint arXiv:2103.14899.\nChen, C.-F.; Panda, R.; and Fan, Q. 2021. RegionViT:\nRegional-to-Local Attention for Vision Transformers. arXiv\npreprint arXiv:2106.02689.\nChen, Y .; Dai, X.; Chen, D.; Liu, M.; Dong, X.; Yuan, L.;\nand Liu, Z. 2021b. Mobile-Former: Bridging MobileNet and\nTransformer. arXiv preprint arXiv:2108.05895.\nChen, Z.; Zhu, Y .; Zhao, C.; Hu, G.; Zeng, W.; Wang, J.;\nand Tang, M. 2021c. DPT: Deformable Patch-Based Trans-\nformer for Visual Recognition, 2899–2907. New York,\nNY , USA: Association for Computing Machinery. ISBN\n9781450386517.\nChu, X.; Tian, Z.; Wang, Y .; Zhang, B.; Ren, H.; Wei, X.;\nXia, H.; and Shen, C. 2021a. Twins: Revisiting the design\nof spatial attention in vision transformers. arXiv preprint\narXiv:2104.13840, 1(2): 3.\nChu, X.; Tian, Z.; Zhang, B.; Wang, X.; Wei, X.; Xia, H.;\nand Shen, C. 2021b. Conditional Positional Encodings for\nVision Transformers. arXiv preprint arXiv:2102.10882.\nDong, X.; Bao, J.; Chen, D.; Zhang, W.; Yu, N.; Yuan, L.;\nChen, D.; and Guo, B. 2021. CSWin Transformer: A Gen-\neral Vision Transformer Backbone with Cross-Shaped Win-\ndows. arXiv preprint arXiv:2107.00652.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. In International Conference on Learn-\ning Representations.\nFan, H.; Xiong, B.; Mangalam, K.; Li, Y .; Yan, Z.; Malik, J.;\nand Feichtenhofer, C. 2021. Multiscale Vision Transform-\ners. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision (ICCV), 6824–6835.\nFang, J.; Xie, L.; Wang, X.; Zhang, X.; Liu, W.; and Tian, Q.\n2021. MSG-Transformer: Exchanging Local Spatial Infor-\nmation by Manipulating Messenger Tokens. arXiv preprint\narXiv:2105.15168.\nHan, K.; Xiao, A.; Wu, E.; Guo, J.; Xu, C.; and Wang,\nY . 2021. Transformer in transformer. arXiv preprint\narXiv:2103.00112.\nHe, K.; Gkioxari, G.; Doll´ar, P.; and Girshick, R. 2017. Mask\nr-cnn. In Proceedings of the IEEE international conference\non computer vision, 2961–2969.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep resid-\nual learning for image recognition. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, 770–778.\nHoward, J.; and Ruder, S. 2018. Universal Language Model\nFine-tuning for Text Classiﬁcation. In ACL.\nHuang, Z.; Ben, Y .; Luo, G.; Cheng, P.; Yu, G.; and Fu, B.\n2021. Shufﬂe Transformer: Rethinking Spatial Shufﬂe for\nVision Transformer. arXiv preprint arXiv:2106.03650.\nJiang, Z.; Hou, Q.; Yuan, L.; Zhou, D.; Shi, Y .; Jin, X.;\nWang, A.; and Feng, J. 2021. All Tokens Matter: Token\nLabeling for Training Better Vision Transformers. arXiv\npreprint arXiv:2104.10858.\nLi, J.; Yan, Y .; Liao, S.; Yang, X.; and Shao, L. 2021a. Local-\nto-Global Self-Attention in Vision Transformers. arXiv\npreprint arXiv:2107.04735.\nLi, Y .; Zhang, K.; Cao, J.; Timofte, R.; and Van Gool, L.\n2021b. LocalViT: Bringing Locality to Vision Transformers.\narXiv preprint arXiv:2104.05707.\nLin, T.-Y .; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ra-\nmanan, D.; Doll ´ar, P.; and Zitnick, C. L. 2014. Microsoft\ncoco: Common objects in context. In European conference\non computer vision, 740–755. Springer.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin Transformer: Hierarchical Vi-\nsion Transformer Using Shifted Windows. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion (ICCV), 10012–10022.\nMao, M.; Zhang, R.; Zheng, H.; Gao, P.; Ma, T.; Peng, Y .;\nDing, E.; and Han, S. 2021. Dual-stream Network for Visual\nRecognition. arXiv preprint arXiv:2105.14734.\nMcCann, B.; Bradbury, J.; Xiong, C.; and Socher, R. 2017.\nLearned in Translation: Contextualized Word Vectors. In\nNIPS.\nPan, B.; Jiang, Y .; Panda, R.; Wang, Z.; Feris, R.; and\nOliva, A. 2021. IA-RED 2: Interpretability-Aware Redun-\ndancy Reduction for Vision Transformers. arXiv preprint\narXiv:2106.12620.\nPeng, Z.; Huang, W.; Gu, S.; Xie, L.; Wang, Y .; Jiao, J.; and\nYe, Q. 2021. Conformer: Local Features Coupling Global\nRepresentations for Visual Recognition. arXiv preprint\narXiv:2105.03889.\nRadosavovic, I.; Kosaraju, R. P.; Girshick, R.; He, K.; and\nDoll´ar, P. 2020. Designing network design spaces. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 10428–10436.\nRao, Y .; Zhao, W.; Liu, B.; Lu, J.; Zhou, J.; and Hsieh,\nC.-J. 2021. DynamicViT: Efﬁcient Vision Transform-\ners with Dynamic Token Sparsiﬁcation. arXiv preprint\narXiv:2106.02034.\nRussakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.;\nMa, S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein, M.;\net al. 2015. Imagenet large scale visual recognition chal-\nlenge. International journal of computer vision, 115(3):\n211–252.\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and J´egou, H. 2021. Training data-efﬁcient image trans-\nformers & distillation through attention. In International\nConference on Machine Learning, 10347–10357. PMLR.\n2738\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in neural information\nprocessing systems, 5998–6008.\nWang, H.; Zhu, Y .; Green, B.; Adam, H.; Yuille, A.;\nand Chen, L.-C. 2020. Axial-deeplab: Stand-alone axial-\nattention for panoptic segmentation. In European Confer-\nence on Computer Vision, 108–126. Springer.\nWang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang,\nD.; Lu, T.; Luo, P.; and Shao, L. 2021a. Pvtv2: Improved\nbaselines with pyramid vision transformer. arXiv preprint\narXiv:2106.13797.\nWang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.;\nLu, T.; Luo, P.; and Shao, L. 2021b. Pyramid Vision Trans-\nformer: A Versatile Backbone for Dense Prediction With-\nout Convolutions. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision (ICCV), 568–578.\nWang, W.; Yao, L.; Chen, L.; Cai, D.; He, X.; and Liu, W.\n2021c. CrossFormer: A Versatile Vision Transformer Based\non Cross-scale Attention. arXiv preprint arXiv:2108.00154.\nWang, Y .; Huang, R.; Song, S.; Huang, Z.; and Huang, G.\n2021d. Not All Images are Worth 16x16 Words: Dynamic\nVision Transformers with Adaptive Sequence Length.arXiv\npreprint arXiv:2105.15075.\nWu, H.; Xiao, B.; Codella, N.; Liu, M.; Dai, X.; Yuan, L.;\nand Zhang, L. 2021a. CvT: Introducing Convolutions to Vi-\nsion Transformers. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision (ICCV), 22–31.\nWu, S.; Wu, T.; Lin, F.; Tian, S.; and Guo, G. 2021b. Fully\nTransformer Networks for Semantic Image Segmentation.\narXiv preprint arXiv:2106.04108.\nWu, T.; Tang, S.; Zhang, R.; Cao, J.; and Zhang, Y . 2020.\nCgnet: A light-weight context guided network for semantic\nsegmentation. IEEE Transactions on Image Processing, 30:\n1169–1179.\nWu, T.; Tang, S.; Zhang, R.; and Guo, G. 2021c. Consensus\nfeature network for scene parsing. IEEE Transactions on\nMultimedia.\nXiao, T.; Liu, Y .; Zhou, B.; Jiang, Y .; and Sun, J. 2018. Uni-\nﬁed perceptual parsing for scene understanding. In Pro-\nceedings of the European Conference on Computer Vision\n(ECCV), 418–434.\nXu, Y .; Zhang, Z.; Zhang, M.; Sheng, K.; Li, K.; Dong, W.;\nZhang, L.; Xu, C.; and Sun, X. 2021. Evo-ViT: Slow-Fast\nToken Evolution for Dynamic Vision Transformer. arXiv\npreprint arXiv:2108.01390.\nYang, J.; Li, C.; Zhang, P.; Dai, X.; Xiao, B.; Yuan,\nL.; and Gao, J. 2021. Focal Self-attention for Local-\nGlobal Interactions in Vision Transformers. arXiv preprint\narXiv:2107.00641.\nYuan, L.; Chen, Y .; Wang, T.; Yu, W.; Shi, Y .; Jiang, Z.-H.;\nTay, F. E.; Feng, J.; and Yan, S. 2021a. Tokens-to-Token ViT:\nTraining Vision Transformers From Scratch on ImageNet. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), 558–567.\nYuan, L.; Hou, Q.; Jiang, Z.; Feng, J.; and Yan, S. 2021b.\nV olo: Vision outlooker for visual recognition.arXiv preprint\narXiv:2106.13112.\nYue, X.; Sun, S.; Kuang, Z.; Wei, M.; Torr, P. H.; Zhang,\nW.; and Lin, D. 2021. Vision Transformer With Progressive\nSampling. In Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV), 387–396.\nZhang, P.; Dai, X.; Yang, J.; Xiao, B.; Yuan, L.; Zhang, L.;\nand Gao, J. 2021. Multi-Scale Vision Longformer: A New\nVision Transformer for High-Resolution Image Encoding.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV), 2998–3008.\nZhang, R.; Tang, S.; Zhang, Y .; Li, J.; and Yan, S. 2019.\nPerspective-adaptive convolutions for scene parsing. IEEE\ntransactions on pattern analysis and machine intelligence,\n42(4): 909–924.\nZhou, B.; Zhao, H.; Puig, X.; Xiao, T.; Fidler, S.; Barriuso,\nA.; and Torralba, A. 2019. Semantic understanding of scenes\nthrough the ade20k dataset. International Journal of Com-\nputer Vision, 127(3): 302–321.\nZhou, D.; Shi, Y .; Kang, B.; Yu, W.; Jiang, Z.; Li, Y .; Jin, X.;\nHou, Q.; and Feng, J. 2021. Reﬁner: Reﬁning Self-attention\nfor Vision Transformers. arXiv preprint arXiv:2106.03714.\nZhu, M.; Han, K.; Tang, Y .; and Wang, Y . 2021a. Visual\nTransformer Pruning. arXiv preprint arXiv:2104.08500.\nZhu, X.; Su, W.; Lu, L.; Li, B.; Wang, X.; and Dai, J.\n2021b. Deformable DETR: Deformable Transformers for\nEnd-to-End Object Detection. In International Conference\non Learning Representations.\n2739",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.726154088973999
    },
    {
      "name": "Computation",
      "score": 0.7237779498100281
    },
    {
      "name": "Computer science",
      "score": 0.681565523147583
    },
    {
      "name": "Segmentation",
      "score": 0.6085708737373352
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48109668493270874
    },
    {
      "name": "Computer vision",
      "score": 0.3712911009788513
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.34550154209136963
    },
    {
      "name": "Algorithm",
      "score": 0.2568150758743286
    },
    {
      "name": "Engineering",
      "score": 0.12009021639823914
    },
    {
      "name": "Electrical engineering",
      "score": 0.10535535216331482
    },
    {
      "name": "Voltage",
      "score": 0.07581183314323425
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I98301712",
      "name": "Baidu (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210129579",
      "name": "National Engineering Laboratory of Deep Learning Technology and Application",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    }
  ],
  "cited_by": 57
}