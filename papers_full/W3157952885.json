{
  "title": "Medical Transformer: Universal Brain Encoder for 3D MRI Analysis",
  "url": "https://openalex.org/W3157952885",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3110474463",
      "name": "Jun, Eunji",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2706472263",
      "name": "Jeong, Seungwoo",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Heo, Da-Woon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222093882",
      "name": "Suk, Heung-Il",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W219040644",
    "https://openalex.org/W2346062110",
    "https://openalex.org/W2755930428",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W1912982817",
    "https://openalex.org/W2148349024",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3101123465",
    "https://openalex.org/W2963863924",
    "https://openalex.org/W2581082771",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2899635607",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W3033333779",
    "https://openalex.org/W2592929672",
    "https://openalex.org/W2979888373",
    "https://openalex.org/W2966744887",
    "https://openalex.org/W2253429366",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2771252144",
    "https://openalex.org/W3033671339",
    "https://openalex.org/W2929753309"
  ],
  "abstract": "Transfer learning has gained attention in medical image analysis due to limited annotated 3D medical datasets for training data-driven deep learning models in the real world. Existing 3D-based methods have transferred the pre-trained models to downstream tasks, which achieved promising results with only a small number of training samples. However, they demand a massive amount of parameters to train the model for 3D medical imaging. In this work, we propose a novel transfer learning framework, called Medical Transformer, that effectively models 3D volumetric images in the form of a sequence of 2D image slices. To make a high-level representation in 3D-form empowering spatial relations better, we take a multi-view approach that leverages plenty of information from the three planes of 3D volume, while providing parameter-efficient training. For building a source model generally applicable to various tasks, we pre-train the model in a self-supervised learning manner for masked encoding vector prediction as a proxy task, using a large-scale normal, healthy brain magnetic resonance imaging (MRI) dataset. Our pre-trained model is evaluated on three downstream tasks: (i) brain disease diagnosis, (ii) brain age prediction, and (iii) brain tumor segmentation, which are actively studied in brain MRI research. The experimental results show that our Medical Transformer outperforms the state-of-the-art transfer learning methods, efficiently reducing the number of parameters up to about 92% for classification and",
  "full_text": "1\nMedical Transformer: Universal Brain Encoder\nfor 3D MRI Analysis\nEunji Jun, Student Member, IEEE,Seungwoo Jeong, Da-Woon Heo, and Heung-Il Suk, Member, IEEE\nAbstract—Transfer learning has gained attention in medical image analysis due to limited annotated 3D medical datasets for training\ndata-driven deep learning models in the real world. Existing 3D-based methods have transferred the pre-trained models to downstream\ntasks, which achieved promising results with only a small number of training samples. However, they demand a massive amount of\nparameters to train the model for 3D medical imaging. In this work, we propose a novel transfer learning framework, called Medical\nTransformer, that effectively models 3D volumetric images in the form of a sequence of 2D image slices. To make a high-level\nrepresentation in 3D-form empowering spatial relations better, we take a multi-view approach that leverages plenty of information from\nthe three planes of 3D volume, while providing parameter-efﬁcient training. For building a source model generally applicable to various\ntasks, we pre-train the model in a self-supervised learning manner for masked encoding vector prediction as a proxy task, using a\nlarge-scale normal, healthy brain magnetic resonance imaging (MRI) dataset. Our pre-trained model is evaluated on three downstream\ntasks: (i) brain disease diagnosis, (ii) brain age prediction, and (iii) brain tumor segmentation, which are actively studied in brain MRI\nresearch. The experimental results show that our Medical Transformer outperforms the state-of-the-art transfer learning methods,\nefﬁciently reducing the number of parameters up to about 92% for classiﬁcation and regression tasks, and 97% for segmentation, and\nstill performs well in the scenario where only partial training samples are used.\nIndex Terms—Transfer Learning; Medical Image Analysis; Brain Age Prediction; Brain Tumor Segmentation; Brain Disease Diagnosis;\nStructural MRI; Transformer; Deep Learning\n!\n1 I NTRODUCTION\nM\nEDICAL image analysis has achieved remarkable pro-\ngresses with the use of deep learning for the past\nfew years [1]. However, building a large 3D dataset for\ntraining deep learning models is challenging in the ﬁeld\nof medical imaging, due to difﬁculty of data acquisition\nand annotation. The lack of annotated data has prompted\nthe development of transfer learning beyond the traditional\nsupervised learning, which allows model training with a\nsmall-scale available dataset.\nFor the transfer learning in medical imaging, the stan-\ndard approach was to pre-train the existing 2D-based deep\nneural architectures, e.g., ResNet [2] and DenseNet [3], using\na large-scale natural image datasets such as ImageNet, and\nthen ﬁne-tune the model on small-sized medical imaging\ndata [4, 5]. This approach yields better performance than\nthe random initial-based training strategy, especially when\na small set of samples are available, but the 3D volume data\nwas split into 2D slices from three planes and pre-trained by\n2D models without recovering to 3D-form representations,\nwhich inevitably caused the model to lose the 3D spatial\ninformation. Accordingly, [6] resolved this issue by exploit-\ning the 3D public available models, e.g., recurrent neural\nnetworks, trained from the 3D Kinetics dataset [7].\n• E. Jun is with the Department of Brain and Cognitive Engineering, Korea\nUniversity, Seoul 02841, Republic of Korea (e-mail: ejjun92@korea.ac.kr).\n• S. Jeong and D.-W. Heo are with the Department of Artiﬁcial\nIntelligence, Korea University, Seoul 02841, Republic of Korea (e-mail:\nsw jeong@korea.ac.kr, daheo@korea.ac.kr).\n• H.-I. Suk is with the Department of Artiﬁcial Intelligence and the\nDepartment of Brain and Cognitive Engineering, Korea University, Seoul\n02841, Republic of Korea (e-mail: hisuk@korea.ac.kr).\nHowever, despite the same 3D structure, it is worth\nemphasizing that the medical dataset of interest is very\ndifferent to the natural images benchmark datasets in com-\nputer vision. In the case of 3D medical data, all scans are reg-\nistered, resulting into sophisticated yet recurrent patterns.\nLearning a 3D medical image network transferred from a\nnatural scene video without considering this characteristic\nleads to a strong bias in medical image analysis. Thus, it mo-\ntivates the need to learn a source model for transfer learning\nby directly utilizing the medical imaging datasets capable of\nlearning subtle differences among medical datasets.\nTo build a source model directly from 3D medical vol-\nume data, recent works pre-trained their models for target\ntasks such as brain parsing and organ segmentation over\nlabeled datasets [8, 9]. But they were pre-trained for speciﬁc\napplications with a small set of data, thus not well suited for\nthe source model. Meanwhile, [9] built a large-scale dataset\nfrom a collection of eight annotated medical datasets, and\npre-trained the 3D residual network for the segmentation\ntask. Although these fully-supervised approaches may yield\nmore powerful target models, they demanded a volume of\nannotation efforts to obtain the source model.\nAccordingly, many works have adopted a self-\nsupervised pre-training to learn image representations from\nunlabeled medical imaging data [10, 11, 12, 13, 14]. For\nexample, [11, 12] introduced a proxy task for 3D represen-\ntation learning by recovering the rearranged and rotated\nRubik’s cube. In addition, [13] proposed a set of ﬁve 3D\nself-supervised learning (SSL) tasks that extend the exist-\ning 2D SSL tasks to 3D form for medical imaging. More\nrecently, [14] devised an image restoration task from a\nmixture of transformations to pre-train the model via self-\nsupervision without a labeled dataset. However, despite\narXiv:2104.13633v1  [cs.CV]  28 Apr 2021\n2\nself-supervision, the existing 3D-based transfer learning\nframeworks require too many learnable parameters to prac-\ntically apply to high-dimensional medical datasets.\nIn this work, we propose a novel transfer learning frame-\nwork, called Medical Transformer , that effectively models\n3D volumetric images in the form of a sequence of 2D\nimage slices. To make a high-level representation in 3D-\nform empowering spatial relations better, we take a multi-\nview approach that leverages plenty of information from the\nthree planes of 3D volume, while providing a parameter-\nefﬁcient training. Speciﬁcally, a given 3D volumetric image\nis split into 2D slices from three planes (sagittal, coronal,\naxial), and these 2D image slices are fed to the network\nas inputs. First of all, we pre-train a backbone network\nconsisting of a convolutional encoder and a transformer\nfor masked encoding vector prediction that generates the\nencoding vectors of the randomly masked images, as our\nSSL proxy task. Then, after passing through the pre-trained\nbackbone network, the 2D slice features are recovered by\ntheir combinations into 3D-form representations, and ﬁnally\nfed into the prediction network for downstream tasks. To\nvalidate the effectiveness of our Medical Transformer, we\nevaluated our pre-trained model on the three target tasks,\ni.e., brain disease diagnosis, brain age prediction, and brain\ntumor segmentation which are actively studied in brain\nMRI research, and achieved remarkable performances for\nall three tasks in comparison to the respective state-of-the-\nart (SOTA) models.\n2 R ELATED WORK\n2.1 Transformers in Vision Models:\nRecently, many works have adopted transformers in vision\nmodels. Among them, Vision Transformer (ViT) [15] divided\nan image into 16 ×16 patches, and fed these patches into\na transformer [16] by treating them as tokens. Despite the\nsimple application of a standard transformer, it showed\na comparable performance to the convolutional variants\nby capturing dense and repeatable patterns. However, ViT\nhas a fetal drawback that requires an enormous amount of\ncomputation cost and dataset to outperform the competing\nconvolutional methods.\nOur approach is similar to ViT in the sense that we\nuse the transformer as a means of dividing image into\nsubsets and modeling their dependencies. However, to take\nadvantage of both convolutional encoder and transformer,\nthe convolutional encoder extracts high-level spatial fea-\ntures, and the transformer extracts their relational features\ntaking into account inter-slice dependencies. In addition, the\nspatial attention of transformer focuses on important slices,\ninstead of treating each image slice equally, which yields\nsuperior performances over 3D-based competing convolu-\ntional methods.\n2.2 Fully Supervised Pre-training\nDue to few annotated datasets in the medical imaging do-\nmain, modern approaches have used the pre-trained model\nusing ImageNet dataset containing over 14 million anno-\ntated images [17, 18]. Practically, their pre-trained weights\nfrom 2D models, such as ResNet [2] and DenseNet [3], are\nﬁne-tuned on medical image analyses, including skin cancer\nidentiﬁcation [19], Alzheimer’s disease (AD) diagnosis [20],\nand pulmonary embolism detection [21]. But this strategy\nwas limited to apply for 3D medical imaging modalities,\ne.g., computed tomography (CT) and MRI, inevitably losing\n3D anatomical information. Accordingly, in order to capture\n3D spatial information, [6] utilized the Inﬂated 3D (I3D) [7]\nwhich is trained from the 3D Kinetics dataset, as a feature\nextractor. However, despite the same 3D structure, there\nexists a large domain gap between temporal video data of\nnatural scenes and medical data, which leads to improper\nmodeling of medical contexts. In order to alleviate these\nlimitations, recent works have tried to pre-train a source\nmodel directly using 3D medical volume data. For example,\nNiftyNet [8] publicized the pre-trained models, i.e., model\nzoo, for speciﬁc applications such as brain parcellation and\norgan segmentation. But they were trained with a small-\nscale dataset, not suitable as a source model for transfer\nlearning. For training the source model from a large-scale\ndataset, [9] aggregated eight annotated medical datasets,\nand employed them to pre-train a 3D residual network for\na segmentation task.\nThese aforementioned fully supervised pre-training ap-\nproaches demand massive, high-quality annotated datasets\nto obtain the source models for transfer learning. On the\nother hand, our proposed method utilizes a self-supervised\nlearning framework without the need of labeled datasets for\n3D medical image analysis.\n2.3 Self-supervised Pre-training:\nSelf-supervised learning has recently gained attention in\nthe ﬁeld of medical imaging, aiming at learning image\nrepresentation from unlabeled data [10, 11, 12, 13, 14]. The\nkey challenge for self-supervised learning is to deﬁne a\nsuitable proxy task from the unlabeled dataset. For example,\nthe proxy tasks in the computer vision include colorization\nthat recovers gray-scale image to a colored image and image\nrestoration that restores the shufﬂed small regions within an\nimage to their original position in the image. In the mean-\ntime, in the medical imaging domain, [11, 12] introduced a\nproxy task of 3D representation learning by recovering the\nre-arranged and rotated Rubik’s cube. [13] proposed a set\nof ﬁve 3D self-supervised tasks for medical Imaging such\nas contrastive predictive coding, rotation prediction, jigsaw\npuzzles, relative patch location, and exemplar networks.\n[14] has developed a transfer learning framework, called\nModel Genesis, to learn general image representation by\nrecovering the original sub-volumes of images from their\ntransformed ones derived from non-linear transformation,\nlocal shufﬂing, outer-cutout, and inner cutout.\nOur approach differs from the previous self-supervised\napproaches in obtaining the encoding vector in the represen-\ntation space, while many related works utilized an image\nrestoration for a proxy task that recovers the transformed\nimages in an input space. Our masked encoding vector\nprediction of the proposed method can effectively learn\nuniversal representations. Additionally, compared to the\n3D-based competing methods that require a huge amount\nof learnable parameters, our method provides a parameter-\nefﬁcient transfer learning framework, still showing superior\nperformances for three medical imaging tasks.\n3\n1. P r e-tr aining C on v olutional E nc oder\n2. P r e-tr aining T r ansf ormer\n3. F ine-tuning B ackbone and P r ediction N e tw ork\n3D V olume tric \nF ea tur e M ap\nB r ain Ag e P r ediction\nChr onolo gical Ag e\nP r edict ed Ag e\nB r ain D isease D iagnosis\nD iseaseN ormal\nB r ain T umor  S egmen ta tion\nT r ansf ormer\nP ositional enc oding\nS egmen t enc oding\nAxial\nS agittal\nC or onal\n......\n......\n......\n......\n......\n......\nP r ediction N e tw ork\nFig. 1: Schematic diagram of the proposed Medical Transformer. Based on a multi-view approach, a given 3D volumetric\nimage is split into 2D slices from three planes (sagittal, coronal, axial), and these 2D image slices are fed to the network\nas inputs. First of all, we pre-train a backbone network that consists of a convolutional encoder and a transformer in a\nself-supervised learning scheme. Then, after passing through the pre-trained backbone network, the 2D slice features are\nrecovered by their combinations into 3D-form representations, and ﬁnally fed into the prediction network for three medical\nimaging tasks.\n3 P ROPOSED METHOD\nThe motivation of our work is to transfer a backbone net-\nwork pre-trained with a large-scale 3D brain MRI dataset to\ndownstream tasks of 3D MRI analyses. In this section, we\nillustrate the overall framework of our Medical Transformer\nas shown in Figure 1. For learning universal representations\nof 3D brain MRI, we ﬁrst build a large-scale 3D brain MRI\ndataset of normal, healthy subjects by collecting three pub-\nlicly available datasets, i.e., Information eXtraction from Im-\nages (IXI)1, Cambridge Centre for Ageing and Neuroscience\n(Cam-CAN)2, and Autism Brain Imaging Data Exchange\n(ABIDE)3. To handle the data variation problems, i.e.., do-\nmain shift caused by data acquisition from multi-centers, we\nconduct spatial and intensity distribution normalization.\nBased on a backbone network including convolutional en-\ncoder and transformer and a prediction network, our Medical\nTransformer takes a multi-view approach, in which a given\n3D volumetric image is split into 2D slices from three planes\n(sagittal, coronal, axial), and these 2D image slices are fed to\nthe network as inputs. For transfer learning, it performs the\nfollowing three steps: (i) pre-training convolutional encoder\nwith an exemplar network, (ii) pre-training transformer for\na masked encoding vector prediction as our proxy task of\na self-supervised learning (SSL), and (iii) ﬁne-tuning the\nbackbone and prediction network for downstream medical\ntasks.\n3.1 Notations\nIn this work, we employ a multi-view approach that re-\ngards a 3D volume image as 2D slice images from three\nplanes, i.e., coronal, sagittal, and axial. Given a 3D volume\nMRI image Xpl (pl ∈ {cor,sag,axial}), we denote a set\nof 2D slice images for each plane, Xpl = {Xpl\n1 ,..., Xpl\nNpl}\n(Xpl ∈ Rw×d×h), where Npl is the number of slices for\nthe corresponding plane, and w,d,h present width, depth,\nand height of 3D volume image, respectively. The 2D slice\nimages are fed into the convolutional encoder, leading to a\n1. https://brain-development.org/ixi-dataset/\n2. https://www.cam-can.org/\n3. http://fcon 1000.projects.nitrc.org/indi/abide/\nset of embedding vectors Zpl = {zpl\n1 ,..., zpl\nNpl}(zpl\nn ∈Rdemb ),\nwhere demb denotes dimension of an embedding vector.\nThen, we conduct a positional and segment encoding step\nbefore entering the transformer, and obtain the encoding\nvectors, Epl = {epl\n1 ,..., epl\nNpl}(epl\nn ∈Rdemb ).\n3.2 Medical Transformer Network\nOur Medical Transformer basically consists of a backbone\nnetwork to be pre-trained by a SSL proxy task, and a\nprediction network based on linear fully connected layers\nfor ﬁnal prediction. For the backbone network, we use a\nconvolutional encoder and a transformer, in which the con-\nvolutional encoder extracts the high-level spatial features\nof the 2D slices for each plane, and then the transformer\nextracts their relational features by modeling the inter-slice\ndependencies via attention mechanism, enabling to capture\nthe dependencies of neighboring and distant slices. Here,\nwe employ an independent convolutional encoder for each\nplane that does not share model parameters as the 3D MRI\nconsists of three planes, i.e., axial, coronal, and sagittal, that\nhave different views for each plane. Then, the resulting\nspatial features go through positional encoding for ordering\nslices, and segment encoding for discerning planes before\nbeing fed into the transformer. After passing through the\ntransformer, described in detail below, a 3D volume feature\nmap is formed by a combination of slice-wise features for all\nplanes. Our multi-view approach of integrating slice-wise\nfeatures from all planes efﬁciently reduces the model pa-\nrameters, still sufﬁciently capturing plenty of 3D volumetric\nrepresentations. Finally, the prediction network outputs the\nﬁnal task-speciﬁc prediction. Depending on the downstream\nmedical tasks, we take a multi-scale approach to capture\nboth low- and high-level features for segmentation task,\nand a single-scale approach to capture high-level features\nfor classiﬁcation and regression tasks.\n3.3 Transfer Learning\n3.3.1 Pre-training Medical Transformer\nTo make convolutional encoder output spatially meaningful\nembedding features, we ﬁrst pre-train the convolutional\n4\nAxial\nS agittal C or onal\nS ingle type \no f  plane\n..................\n...\n...\n...\nN ega tiv e\nP ositiv e\nO riginal\nT riple t L oss\nFig. 2: Illustration of pre-training a convolutional encoder.\nencoder in a self-supervised learning manner, as shown in\nFigure 2. To this end, we use an exemplar network [22] to\nderive supervision labels, for which it relies on a non-linear\ntransformation for learning appearances as one of the image\naugmentation techniques. The absolute or relative intensity\nvalues of medical images deliver crucial information about\nthe image structure. To preserve intensity of anatomies\nduring image transformation, we use a Bezier Curve as\na smooth and monotonous transformation, which assigns\nevery pixel a unique value, ensuring a one-to-one mapping.\nFor training the exemplar network, the triplet loss [23]\nis employed. For each plane, all slice-wise features are\naverage-pooled, and compared with positive pairs (trans-\nformed features) and negative pairs (features of samples\nfrom other batches) to calculate the triplet loss Lpl as fol-\nlows:\nLpl = 1\nNT\nNT∑\ni=1\nmax{0,D(˜zpl\n(i),˜zpl,+\n(i) ) −D(˜zpl\n(i),˜zpl,−\n(i) ) + α}\n(1)\nwhere ˜zpl\n(i) is an embedding vector of random training sam-\nple that is averaged over slices, ˜zpl,+\n(i) is that of transformed\nversion (positive example), ˜zpl,−\n(i) is that of different sample\nfrom the dataset (negative example), NT is the number\nof training samples, α is a margin between positive and\nnegative sample, and D is a function for distance measure,\ne.g., L2 distance. The ﬁnal loss is deﬁned by averaging all\nthe losses from the three planes.\nMotivated by a successful adoption of the masked lan-\nguage modeling as a proxy task of SSL in BERT [24], we\npropose to learn universal 3D MRI representations for a\nmasked encoding vector prediction as our SSL proxy task.\nSpeciﬁcally, we regard a slice-wise encoding vector epl\nn as\na token, and a set of encoding vectors Epl = {epl\n1 ,..., epl\nNpl}\ni.e., an image-level feature, as a sentence. We randomly mask\nout some input slice images for each plane, and predict the\ncorresponding encoding vectors of the masked images from\nencoding vectors of the remaining non-masked slices. Here,\nthe convolutional encoder pre-trained in the previous step\nis frozen, and only the transformer is trained for the masked\nencoding vector prediction task, so that the transformer can\npredict the masked encoding vector by ﬁguring out the\ninter-slice dependencies.\nFor modeling inter-slice dependencies, we employ a\nstandard transformer over slice-wise encoding features\nEpl ∈ RNpl×demb . The mathematical formulations are as\nfollows:\n¯Epl = Epl + Softmax\n(\nEpl(Epl)⊤\n√demb\n)\nEpl (2)\nˆEpl = ¯Epl + σ(¯EplF1)F2 (3)\nwhere σ(·) is a ReLU function for non-linearity, and F1 ∈\nRdemb×dff and F2 ∈Rdff ×demb denote two point-wise feed-\nforward convolutions.\n3.3.2 Fine-tuning Medical Transformer\nThe proposed Medical Transformer aims to build the back-\nbone network that works as a universal brain encoder gen-\nerally transferable for various downstream medical tasks. In\norder to validate the effectiveness of our pre-trained back-\nbone network, we ﬁne-tune the backbone and prediction\nnetwork for downstream tasks. To this end, we take two\ndifferent approaches, i.e., a single-scale approach to capture\nhigh-level features for classiﬁcation and regression tasks,\nand a multi-scale approach to capture both low- and high-level\nfeatures for a segmentation task. Speciﬁcally, the multi-scale\napproach integrates all outputs from the initial convolution\nlayers in the encoder. For instance as shown in Figure 3,\nboth low- and high-level features from the convolutional\nblocks, #2 to #4, are taken into account, which empowers\nthe representations for the segmentation task.\nAfter passing through the backbone network, the encod-\ning vectors ˆEpl ∈RNpl×demb for each plane construct a 3D\nvolume feature map Fpl ∈ R(w×d×h)×demb . In the single-\nscale approach, feature maps for all planes are integrated\ninto a ﬁnal 3D volume feature map F∈ R(w×d×h)×3demb via\nconcatenation, and in the multi-scale approach, the feature\nmaps from the preceding convolutional blocks are addition-\nally concatenated to the aforementioned ﬁnal feature map,\nwhich is fed into the prediction network. The prediction\nnetwork consists of a linear fully connected (FC) layer for\nsegmentation task, and a linear FC layer followed average\npooling for classiﬁcation and regression task. During the\nﬁne-tuning phase, both the pre-trained backbone and the\nprediction network are ﬁne-tuned on downstream tasks.\n4 E XPERIMENTS\nIn this section, we evaluated the proposed Medical Trans-\nformer for the three medical tasks on publicly available\nbrain MRI datasets. To show the superiority of our pro-\nposed method, we compared the results of three tasks with\nother state-of-the-art methods in the literature, i.e., I3D [7],\nNiftyNet [8], MedicalNet [9], 3D self-supervised methods\n[13], and Model Genesis [14]. In addition, we conducted\nextensive ablation studies for our model to evaluate the\neffects of different components in the proposed method. In\naddition to the quantitative validation, we also compared\nthe qualitative results of a segmentation task. For repro-\nducibility of the results, our code is publicly available at\n“https://open-after-acceptance”.\n4.1 Experimental Settings\n4.1.1 Pre-training Medical Transformer\nTo build a large-scale brain MRI dataset, we collected T1-\nweighted structural MRI scans of normal, healthy 1,783\n5\nC on v olutional \nB lock #1\nC on v olutional \nB lock #2\nC on v olutional \nB lock #3\nC on v olutional \nB lock #4\nC on v olutional \nB lock #5\nConv olutional \nBlock #1\nConv olutional \nBlock #2\nConv olutional \nBlock #3\nConv olutional \nBlock #4\nConv olutional \nBlock #5\nConv olutional \nBlock #1\nConv olutional \nBlock #2\nConv olutional \nBlock #3\nConv olutional \nBlock #4\nConv olutional \nBlock #5\nT r ansf ormerT r ansf ormerT r ansf ormer\n..................\nS agittal\nC or onal\nAxial T rilinear Interpolation\nPr ediction Network\nS ingle-sc ale \nAppr oach\nM ulti-sc ale \nAppr oach\n...\n...\n...\nAD\n MCI\n CN\nB r ain Ag e P r edictionB r ain D isease D iagnosis\nC h r onolo gical Ag e\nP r e d ic t e d  Ag e\nB r ain T um or  S eg m en t a tion\nO u tp u t\nI n p u t\nW h ole t umor\nE n h ancing t umor  c or e\nT umor  c or e\nFig. 3: Illustration of ﬁne-tuning phase.\nTABLE 1: Performance comparison between competitive methods and the proposed method for three target tasks\n(brain disease diagnosis (AD/MCI/NC multi-class classiﬁcation), brain age prediction (regression), and brain tumor\nsegmentation) after cross-validation (mean ±std). The entries in bold highlight the best performance among the different\napproaches for three target tasks.\nPre-training Approach\nTarget tasks\nClassiﬁcation Regression Segmentation\nmAUC MAE (years) Dice WT Dice TC Dice ET\nNo Training from scratch 0.7728±0.0077 4.4357±0.3260 0.8649±0.0094 0.6392±0.0556 0.4830±0.0485\n(Fully) supervised\nI3D [7] 0.7325±0.0164 4.6561±0.3209 0.6607±0.0542 0.4708±0.0593 0.0569±0.0159\nNiftyNet [8] 0.5031±0.0165 4.6580±0.3161 0.8395±0.0065 0.5295±0.0148 0.5046±0.0278\nMedicalNet [9] 0.6910±0.0063 4.6443±0.3626 0.7885±0.0378 0.5681±0.0572 0.0809±0.0298\nSelf-supervised\n3D-RPL [13] 0.4849±0.0333 5.1237±0.7086 0.8555±0.0462 0.6595±0.0322 0.3897±0.0078\n3D-Rotation [13] 0.4965±0.0077 4.9799±0.4365 0.8672±0.0344 0.6756±0.0204 0.3717±0.0452\n3D-Jigsaw [13] 0.4950±0.0202 4.7719±0.4784 0.8671±0.0453 0.6739±0.0218 0.3789±0.0415\n3D-CPC [13] 0.4943±0.0109 5.0091±0.7856 0.8879±0.0089 0.6844±0.0086 0.3760±0.0159\n3D-Exemplar [13] 0.5085±0.0163 5.4434±0.9623 0.8975±0.0123 0.6912±0.0120 0.3819±0.0134\nModel Genesis [14] 0.4997±0.0004 4.6377±0.3411 0.8505±0.0203 0.6201±0.0289 0.0896±0.0329\nMedical Transformer (Ours) 0.8347±0.0072 3.4924±0.0863 0.8733±0.0086 0.6969±0.0470 0.5882±0.0437\nsubjects from IXI (#=566), Cam-CAN (#=653) and ABIDE\n(#=564) datasets. Only subjects who are 13 years of age or\nolder are considered in ABIDE I and ABIDE II. For pre-\nprocessing, we conducted Brain ExTraction (BET), FMRIB\nLinear Image Registration Toolkit (FLIRT), bias correction,\nand image intensities min-max normalization. Finally, each\n3D MRI volume of 193 ×229 ×193 in size was average-\npooled into the size of 96 ×114 ×96.\nThe backbone network comprised the ResNet-18 and the\ntransformer. The network architecture of the transformer\nwas deﬁned with the dimension of an embedding vec-\ntor ( demb)=16, the dimension of a feed-forward network\n(dff )=64, the number of heads ( h)=4, respectively. For the\npositional encoding, we used a sinusoidal function. We\ntrained our models using the Adam optimizer [25] with an\ninitial learning rate of 1e−4 and a multiplicative decay of\n0.99 for 150 epochs (early stopping of 30 patience) using\nmini-batches of 10 samples. We divided samples to ﬁve\nfolds, where one fold for the validation set, one fold for\nthe test set, and the remaining folds for the training set,\nand chose the ﬁnal optimal model based on the perfor-\nmance over the validation set. For the masked encoding\nvector prediction task, the masking ratio was set 10% so\nthat universal representation was effectively learned by\nreasonable difﬁculties, i.e., 9, 11, 9 slices are masked for\nsagittal, coronal, and axial plane, respectively. Pre-training\nthe Medical Transformer was implemented with PyTorch,\nTABLE 2: Socio-demographic information summary of\nADNI cohort.\nInformation CN MCI AD\n#Subjects (Male) 433 (214) 748 (447) 359 (194)\nAge (Years) 74.76 ± 5.82 73.29 ± 7.57 75.29 ± 7.87\nEducation (Years) 16.29 ± 2.72 15.92 ± 2.86 15.15 ± 3.05\nMMSE 29.07 ± 1.12 27.48 ± 1.82 23.21 ± 2.05\nand trained on GPU NVIDIA GeForce RTX 2080 TI.\n4.1.2 Fine-tuning Medical Transformer\nTo validate the effectiveness of our transfer learning frame-\nwork, we ﬁne-tuned the pre-trained model on several med-\nical tasks, i.e., brain disease diagnosis, brain age prediction,\nand brain tumor segmentation tasks, which are actively\nstudied in brain MRI researches. The prediction network\nwas built with 1 fully connected hidden layer. With re-\nspect to the classiﬁcation and regression task, the resulting\n3D map after prediction network was average pooled. We\nreported the performances of our proposed method and\ncompeting methods with the average results from the 5-\nfold cross validation for all tasks. Fine-tuning our Medical\nTransformer including the comparative methods was imple-\nmented with PyTorch, and trained on GPU NVIDIA GeForce\nRTX 2080 TI for the classiﬁcation and regression tasks, and\nTITAN RTX for the segmentation task.\n6\nBrain Disease Diagnosis: Our target task is to differ-\nentiate between Alzheimer’s disease (AD), mild cognitive\nimpairment (MCI), and cognitively normal (CN), i.e., multi-\nclass classiﬁcation, which is a crucial, but difﬁcult goal in the\nstudy of AD, due to the subtle and diverse morphological\nchanges in the spectrum of AD, MCI, and CN. To this\nend, we utilized T1-weighted structural MRI scans from\nthe Alzheimer’s Disease Neuroimaging Initiative (ADNI)\ndataset4. The demographic information of the dataset is\nsummarized in Table 2. The classiﬁcation performance was\nevaluated in terms of the multi-class area under the receiver\noperating curve (mAUC) [26].\nBrain Age Prediction: To verify the versatility of our\nMedical Transformer on a regression task, we conducted a\nbrain age prediction based on a brain MRI sample, which is\nregarded as one of the most effective ways for brain aging\nunderstanding. T1-weighted structural MRI scans from the\nADNI dataset were used same as the classiﬁcation task,\nas shown in Table 2. The regression results were reported\nin terms of the mean absolute error (MAE) between the\nchronological age and the predicted age.\nBrain T umor Segmentation To evaluate the proposed\nmethod on brain tumor segmentation, we used the Brain\nTumor Segmentation (BraTS) 2020 dataset 5. It consists of\n369 training samples and 125 validation samples including\nT1, post-contrast T1-weighted (T1Gd), T2-weighted (T2),\nand T2-FLAIR. The dataset was pre-processed with skull-\nstriping, interpolation to a uniform isotropic resolution of\n1mm3 and registered to the SRI24 space with a dimension\nof 240×240×155. The original image size is 240×240×155,\nbut the largest crop size of 160 ×192 ×128 was used, which\nensures that most image content remains within the crop\narea.\nSince the BraTS dataset consists of four different MRI\nmodalities as compared to a single modality in other\ndatasets, there is a difference in the input channel size of the\nResNet in the pre-trained backbone network, which causes\nﬁne-tuning the backbone network to fail. To handle this\nissue, we duplicated the weights of the pre-trained input\nlayer by the number of modalities. In terms of performance\nevaluation, we calculated a dice score for each tumor region,\ni.e., whole tumor (WT), tumor core (TC), and enhancing\ntumor core (ET).\n4.2 Results\n4.2.1 Brain Disease Diagnosis\nTable 1 compares the results of our proposed method with\nthose of the comparative models for AD, MCI, and NC\nclassiﬁcation in terms of mAUC. For the competing meth-\nods, we took an encoder from the pre-trained model, and\nappended a fully-connected layer similar to our prediction\nnetwork. Our Medical Transformer achieved the best clas-\nsiﬁcation performance when comparing to 3D-based SOTA\nmethods. Compared to training from scratch that randomly\ninitialized the model weights, adopting the transfer learn-\ning scheme showed a clear superiority in performance by\ntaking advantage of the universal brain representations and\n4. http://adni.loni.usc.edu/\n5. https://www.med.upenn.edu/cbica/brats2020/data.html\nTABLE 3: Comparison of the number of model parameters\nbetween competitive methods and the proposed method for\nthree target tasks (brain disease diagnosis (AD/MCI/NC\nmulti-class classiﬁcation), brain age prediction (regression),\nand brain tumor segmentation). 3D-RPL, 3D-Rotation, 3D-\nJigsaw, 3D-CPC and 3D-Exemplar have same architecture,\nso we refer to it as 3D-SSL. The entries in bold indicate\nthe least number of model parameters among the different\napproaches.\nApproach Target tasks\nClassiﬁcation Regression Segmentation\nI3D [7] 12.378M 12.375M 17.412M\nNiftyNet [8] 24.068M 24.067M 45.614M\nMedicalNet [9] 33.052M 33.051M 85.798M\n3D-SSL [13] 3.552M 3.551M 5.649M\nModel Genesis [14] 7.093M 7.093M 19.076M\nMedical Transformer 2.402M 2.401M 2.410M\naccordingly better initialization weights in ﬁne-tuning the\ntarget task model. Among the comparative methods, I3D\ndemonstrated a competitive performance by utilizing the\nrecurrent layers as a feature extractor, as the classiﬁcation\ntask beneﬁts from a high level of embedding features. In\naddition, MedicalNet presents a comparable performance\nby pre-training the backbone network using a large-scale\nmedical dataset.\nThese experimental results validated the efﬁcacy of our\nmulti-view approach to extract 3D volumetric characteris-\ntics as well as our SSL proxy task to effectively learn univer-\nsal representations. In addition, pre-training the backbone\nnetwork using a large corpus MRI dataset attributed to the\nsuperior performance of our method.\n4.2.2 Brain Age Prediction\nWe evaluated our Medical Transformer with the competing\nbaseline methods on brain age prediction task in Table 1.\nIt is noteworthy that our proposed method outperformed\nall counterparts with a large margin. In particular, while all\ncompeting methods showed inferior performance for this\nregression task, training from the scratch and Medical Trans-\nformer achieved remarkable performance with least MAE,\nsupporting the effectiveness of our multi-view approach to\nmake 3D high-level representations from the three planes of\na volume.\n4.2.3 Brain Tumor Segmentation\nAs shown in Table 1, it can be seen that our Medical Trans-\nformer achieved the best or comparable performance on\nthe brain tumor segmentation task, speciﬁcally comparable\nperformance in the evaluation of Dice WT and TC, and the\nbest in the evaluation of Dice ET, which is the most difﬁcult\namong tumor types.\nThe 3D-SSL methods showed consistently better perfor-\nmances in terms of Dice WT and Dice TC, but presented a\nrelatively poor result in Dice ET. This implies that they ef-\nfectively learn 3D structural representations through 3D SSL\nproxy tasks, but are inferior to capturing the sophisticated\ntumor regions. On the contrary, I3D performed the poorest\namong the comparison methods, due to a large domain gap\nof transfer learning between the 3D Kinetics dataset and the\nmedical dataset. In addition, MedicalNet showed relatively\n7\nTraining from scratch I3D NiftyNet MedicalNet\n3D-RPL 3D-Rotation 3D-Jigsaw 3D-CPC 3D-Exemplar\nModel GenesisGround Truth\nMedical Transformer\nFig. 4: Visualization of the segmentation results between our proposed Medical Transformer and the competing methods\nfor comparison of qualitative results. The whole tumor (WT) class includes all visible labels (a union of green, blue and red\nlabels), the tumor core (TC) class if a union of blue and red, and the enhancing tumor core (ET) class is shown in red.\nTABLE 4: Performance comparison between supervised learning and transfer learning scheme depending on a ratio of\ntraining samples for AD/MCI/NC multi-class classiﬁcation, brain age prediction, and brain tumor segmentation task. The\nentries in bold indicate the performance of the transfer learning scheme using a partial dataset that has reached that of the\nsupervised learning scheme using the entire dataset. (mAUC: multi-class area under the receiver operating curve, MAE:\nmean absolute error, WT: whole tumor, TC: tumor core, ET: enhancing tumor core)\nScheme Ratio Classiﬁcation Regression Segmentation\nmAUC MAE (years) Dice WT Dice TC Dice ET\nSupervised\nLearning\n10% 0.5984 ± 0.0111 4.7720 ± 0.4769 0.6814 ± 0.0205 0.1417 ± 0.1668 0.1710 ± 0.1537\n30% 0.6301 ± 0.0130 4.5880 ± 0.3542 0.7921 ± 0.0125 0.5181 ± 0.0513 0.5157 ± 0.0234\n50% 0.6749 ± 0.0166 4.5291 ± 0.3377 0.8276 ± 0.0062 0.6509 ± 0.0415 0.5744 ± 0.0254\n70% 0.6936 ± 0.0075 4.5055 ± 0.3515 0.8334 ± 0.0245 0.6487 ± 0.0558 0.5641 ± 0.0355\n100% 0.7728 ± 0.0077 4.4357 ± 0.3260 0.8649 ± 0.0094 0.6392 ± 0.0556 0.4830 ± 0.0485\nTransfer\nlearning\n10% 0.6112 ± 0.0288 4.3531 ± 0.4846 0.7699 ± 0.0259 0.4610 ± 0.1460 0.4221 ± 0.0906\n30% 0.7259 ± 0.0267 4.0142 ± 0.4403 0.8236 ± 0.0192 0.6117 ± 0.0410 0.5632 ± 0.0337\n50% 0.7729 ± 0.0132 3.7819 ± 0.2780 0.8417 ± 0.0081 0.6850 ± 0.0233 0.6216 ± 0.0110\n70% 0.7983 ± 0.0147 3.6834 ± 0.1955 0.8576 ± 0.0227 0.6769 ± 0.0610 0.5389 ± 0.0488\n100% 0.8347 ± 0.0072 3.4924 ± 0.0863 0.8733 ± 0.0086 0.6969 ± 0.0470 0.5882 ± 0.0437\nlow performances in all metrics, despite the largest number\nof parameters among the competing methods.\nOur result of obtaining the best performance in the Dice\nET evaluation showed the effectiveness of our method for\ncapturing more sophisticated tumor regions by generat-\ning 3D volumetric features from a combination of three\nplane encoding features via our multi-view approach. Fur-\nthermore, the multi-scale approach during ﬁne-tuning at-\ntributed the performance improvement by integrating low-\nand high-level features.\nFigure 4 compares the qualitative results of segmentation\nbetween the competing methods and ours. We observed\nthat the segmentation result of the proposed method was\nsimilar to that of 3D-SSL, and although it is not possible\nto accurately capture the border region of the WT, but the\nthree tumor types were better distinguished among the\ncomparison methods. This result is reasonable as in this\nwork, we focus on proposing a transfer learning framework\nthat is generally applicable to a variety of downstream tasks\nTABLE 5: Performance comparison between w/ and w/o\ntransformer. The entries in bold present better performance\nbetween w/o and w/ transformer.\nTask Measure w/o Transformer w/ Transformer\nClassiﬁcation mAUC 0.8204±0.0200 0.8347±0.0072\nRegression MAE (years) 3.6373±0.2432 3.4924±0.0863\nSegmentation\nDice WT 0.8695±0.0070 0.8733±0.0086\nDice TC 0.6363±0.0576 0.6969±0.0470\nDice ET 0.5063±0.0511 0.5882±0.0437\nin a parameter-efﬁcient manner.\n4.2.4 Number of Parameters\nTo investigate the power of our parameter-efﬁcient Medical\nTransformer, we compared the number of model parameters\nbetween ours and the comparative methods for three down-\nstream tasks. As shown in Table 3, the proposed method has\nfewer parameters as much as 92.74 % for classiﬁcation, 92.73\n% for regression, and 97.19 % for segmentation compared to\n8\nthe comparison method, but still showing superior perfor-\nmances.\n4.2.5 Ablation Studies\nFirst of all, to investigate the effect of transfer learning that\nperforms well with a handful of training samples, we sim-\nulated experiments with a fewer labeled data. Speciﬁcally,\nwe ﬁne-tuned the model with a partial dataset, i.e., only\n10%, 30%, 50%, and 70% samples of training dataset for\nthree target tasks. Table 4 compares the results of supervised\nlearning (training from scratch) and transfer learning for\neach task. The performance of transfer learning consistently\nshowed a signiﬁcant performance improvement over the\nsupervised learning for the three tasks in all ratio scenarios.\nSpeciﬁcally, for brain disease diagnosis, the transfer learning\nscheme using about 50% training data already achieved\nthe similar performance to the supervised learning scheme\nusing the entire data, thus reducing nearly 50% annotation\nefforts. In addition, using only 30% of training samples, the\nperformance for brain age prediction was better than that\nof training from scratch using full samples, which suggests\nthat about 70% of the annotation cost from supervised learn-\ning scheme could be saved by initializing with our Medical\nTransformer. In terms of brain tumor segmentation, Medical\nTransformer achieved similar performances to training from\nscratch by using nearly 70% on WT, 40% on TC, and 20% on\nET, where in particular, the effect of reducing annotation\nefforts was remarkable in the evaluation of ET. Therefore,\nthese experimental results suggest that our Medical Trans-\nformer can alleviate the lack of annotated 3D MRI samples\nthrough an annotation-efﬁcient transfer learning scheme for\na variety of downstream tasks.\nFurthermore, to examine the effectiveness of using the\ntransformer in our framework, we compared the perfor-\nmance between without and with transformer as shown in\nTable 5. In fact, w/ transformer achieved relatively higher\nperformances, compared to w/o transformer for three tasks.\nThus, this experimental result suggests that our approach of\nadopting the transformer helps to capture the volumetric\nfeatures of 3D MRI by allowing the model to take into\naccount the relations over the neighboring and distant slices.\n5 C ONCLUSION\nIn this work, we proposed a novel transfer learning frame-\nwork, called Medical Transformer, that effectively models\n3D volumetric images in the form of a sequence of 2D image\nslices. To learn high-level 3D volumetric representations,\nwe took a multi-view approach that leveraged plenty of\ninformation from the three planes of a volume image, while\nproviding parameter-efﬁcient training. As a result of eval-\nuating our pre-trained model on brain disease diagnosis,\nbrain age prediction, and brain tumor segmentation tasks,\nour Medical Transformer outperformed the SOTA transfer\nlearning methods, efﬁciently reducing the number of pa-\nrameters up to about 92% for classiﬁcation and regression\ntasks, and 97% for segmentation.\nREFERENCES\n[1] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Se-\ntio, F. Ciompi, M. Ghafoorian, J. A. Van Der Laak,\nB. Van Ginneken, and C. I. S ´anchez, “A survey on\ndeep learning in medical image analysis,”Medical image\nanalysis, vol. 42, pp. 60–88, 2017.\n[2] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual\nlearning for image recognition,” in Proceedings of the\nIEEE conference on computer vision and pattern recognition,\n2016, pp. 770–778.\n[3] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Wein-\nberger, “Densely connected convolutional networks,”\nin Proceedings of the IEEE conference on computer vision\nand pattern recognition, 2017, pp. 4700–4708.\n[4] Q. Yu, L. Xie, Y. Wang, Y. Zhou, E. K. Fishman, and A. L.\nYuille, “Recurrent saliency transformation network: In-\ncorporating multi-stage visual cues for small organ\nsegmentation,” in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , 2018, pp. 8280–\n8289.\n[5] X. Han, “Automatic liver lesion segmentation using\na deep convolutional neural network method,” arXiv\npreprint arXiv:1704.07239, 2017.\n[6] D. Ardila, A. P . Kiraly, S. Bharadwaj, B. Choi, J. J.\nReicher, L. Peng, D. Tse, M. Etemadi, W. Ye, G. Corrado\net al. , “End-to-end lung cancer screening with three-\ndimensional deep learning on low-dose chest com-\nputed tomography,” Nature medicine, vol. 25, no. 6, pp.\n954–961, 2019.\n[7] J. Carreira and A. Zisserman, “Quo vadis, action recog-\nnition? a new model and the kinetics dataset,” in pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, 2017, pp. 6299–6308.\n[8] E. Gibson, W. Li, C. Sudre, L. Fidon, D. I. Shakir,\nG. Wang, Z. Eaton-Rosen, R. Gray, T. Doel, Y. Hu\net al., “Niftynet: a deep-learning platform for medical\nimaging,” Computer methods and programs in biomedicine,\nvol. 158, pp. 113–122, 2018.\n[9] S. Chen, K. Ma, and Y. Zheng, “Med3d: Transfer\nlearning for 3d medical image analysis,” arXiv preprint\narXiv:1904.00625, 2019.\n[10] N. Tajbakhsh, Y. Hu, J. Cao, X. Yan, Y. Xiao, Y. Lu,\nJ. Liang, D. Terzopoulos, and X. Ding, “Surrogate su-\npervision for medical image analysis: Effective deep\nlearning from limited quantities of labeled data,” in\n2019 IEEE 16th International Symposium on Biomedical\nImaging. IEEE, 2019, pp. 1251–1255.\n[11] X. Zhuang, Y. Li, Y. Hu, K. Ma, Y. Yang, and Y. Zheng,\n“Self-supervised feature learning for 3d medical im-\nages by playing a rubik’s cube,” in International Confer-\nence on Medical Image Computing and Computer-Assisted\nIntervention. Springer, 2019, pp. 420–428.\n[12] J. Zhu, Y. Li, Y. Hu, K. Ma, S. K. Zhou, and Y. Zheng,\n“Rubik’s cube+: A self-supervised feature learning\nframework for 3d medical image analysis,” Medical\nImage Analysis, vol. 64, p. 101746, 2020.\n[13] A. Taleb, W. Loetzsch, N. Danz, J. Severin,\nT. Gaertner, B. Bergner, and C. Lippert, “3d self-\nsupervised methods for medical imaging,” arXiv\npreprint arXiv:2006.03829, 2020.\n[14] Z. Zhou, V . Sodha, J. Pang, M. B. Gotway, and J. Liang,\n“Models genesis,” Medical image analysis , vol. 67, p.\n101840, 2021.\n[15] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weis-\n9\nsenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Min-\nderer, G. Heigold, S. Gelly et al. , “An image is worth\n16x16 words: Transformers for image recognition at\nscale,” arXiv preprint arXiv:2010.11929, 2020.\n[16] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin,\n“Attention is all you need,” in NIPS, 2017.\n[17] H.-C. Shin, H. R. Roth, M. Gao, L. Lu, Z. Xu, I. Nogues,\nJ. Yao, D. Mollura, and R. M. Summers, “Deep convo-\nlutional neural networks for computer-aided detection:\nCnn architectures, dataset characteristics and transfer\nlearning,” IEEE transactions on medical imaging , vol. 35,\nno. 5, pp. 1285–1298, 2016.\n[18] N. Tajbakhsh, J. Y. Shin, S. R. Gurudu, R. T. Hurst,\nC. B. Kendall, M. B. Gotway, and J. Liang, “Convolu-\ntional neural networks for medical image analysis: Full\ntraining or ﬁne tuning?” IEEE transactions on medical\nimaging, vol. 35, no. 5, pp. 1299–1312, 2016.\n[19] A. Esteva, B. Kuprel, R. A. Novoa, J. Ko, S. M. Swetter,\nH. M. Blau, and S. Thrun, “Dermatologist-level clas-\nsiﬁcation of skin cancer with deep neural networks,”\nnature, vol. 542, no. 7639, pp. 115–118, 2017.\n[20] Y. Ding, J. H. Sohn, M. G. Kawczynski, H. Trivedi,\nR. Harnish, N. W. Jenkins, D. Lituiev, T. P . Copeland,\nM. S. Aboian, C. Mari Aparici et al., “A deep learning\nmodel to predict a diagnosis of Alzheimer disease by\nusing 18F-FDG PET of the brain,” Radiology, vol. 290,\nno. 2, pp. 456–464, 2019.\n[21] N. Tajbakhsh, J. Y. Shin, M. B. Gotway, and J. Liang,\n“Computer-aided detection and visualization of pul-\nmonary embolism using a novel, compact, and discrim-\ninative image representation,” Medical image analysis ,\nvol. 58, p. 101541, 2019.\n[22] A. Dosovitskiy, J. T. Springenberg, M. Riedmiller, and\nT. Brox, “Discriminative unsupervised feature learning\nwith convolutional neural networks.” Citeseer, 2014.\n[23] X. Wang and A. Gupta, “Unsupervised learning of\nvisual representations using videos,” in Proceedings of\nthe IEEE international conference on computer vision, 2015,\npp. 2794–2802.\n[24] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova,\n“BERT: Pre-training of deep bidirectional trans-\nformers for language understanding,” arXiv preprint\narXiv:1810.04805, 2018.\n[25] D. P . Kingma and J. Ba, “Adam: A method for\nstochastic optimization,” in 3rd International Conference\non Learning Representations, ICLR 2015, San Diego,\nCA, USA, May 7-9, 2015, Conference Track Proceedings ,\nY. Bengio and Y. LeCun, Eds., 2015. [Online]. Available:\nhttp://arxiv.org/abs/1412.6980\n[26] D. J. Hand and R. J. Till, “A simple generalisation of the\narea under the roc curve for multiple class classiﬁcation\nproblems,” Machine learning, vol. 45, no. 2, pp. 171–186,\n2001.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7836146354675293
    },
    {
      "name": "Encoder",
      "score": 0.6798038482666016
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6271694898605347
    },
    {
      "name": "Segmentation",
      "score": 0.6057667136192322
    },
    {
      "name": "Transformer",
      "score": 0.5930903553962708
    },
    {
      "name": "Deep learning",
      "score": 0.587615430355072
    },
    {
      "name": "Transfer of learning",
      "score": 0.582208514213562
    },
    {
      "name": "Feature learning",
      "score": 0.5348727703094482
    },
    {
      "name": "Medical imaging",
      "score": 0.4829672873020172
    },
    {
      "name": "Machine learning",
      "score": 0.47835391759872437
    },
    {
      "name": "Visualization",
      "score": 0.44695717096328735
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.418287068605423
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": []
}