{
  "title": "Gender-tuning: Empowering Fine-tuning for Debiasing Pre-trained Language Models",
  "url": "https://openalex.org/W4385571213",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2560996647",
      "name": "Somayeh Ghanbarzadeh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098668577",
      "name": "Yan Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2009839542",
      "name": "Hamid Palangi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4208886161",
      "name": "Radames Cruz Moreno",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2571757561",
      "name": "Hamed Khanpour",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963612262",
    "https://openalex.org/W2889624842",
    "https://openalex.org/W3093211917",
    "https://openalex.org/W3206487987",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W2972413484",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3131157458",
    "https://openalex.org/W3103639864",
    "https://openalex.org/W3100355250",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3134354193",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W2954275542",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2950866572",
    "https://openalex.org/W3177468621",
    "https://openalex.org/W2997588435",
    "https://openalex.org/W3035241006",
    "https://openalex.org/W3035591180",
    "https://openalex.org/W2898081668",
    "https://openalex.org/W2791170418",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2963078909",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2950018712",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2949969209"
  ],
  "abstract": "Recent studies have revealed that the widely-used Pre-trained Language Models (PLMs) propagate societal biases from the large unmoderated pre-training corpora. Existing solutions require debiasing training processes and datasets for debiasing, which are resource-intensive and costly. Furthermore, these methods hurt the PLMs’ performance on downstream tasks. In this study, we propose Gender-tuning, which debiases the PLMs through fine-tuning on downstream tasks’ datasets. For this aim, Gender-tuning integrates Masked Language Modeling (MLM) training objectives into fine-tuning’s training process. Comprehensive experiments show that Gender-tuning outperforms the state-of-the-art baselines in terms of average gender bias scores in PLMs while improving PLMs’ performance on downstream tasks solely using the downstream tasks’ dataset. Also, Gender-tuning is a deployable debiasing tool for any PLM that works with original fine-tuning.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 5448–5458\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nGender-tuning: Empowering Fine-tuning for Debiasing Pre-trained\nLanguage Models\nSomayeh Ghanbarzadeh\nUniversity of North Texas\nsomayehghanbarzadeh@my.unt.edu\nYan Huang\nUniversity of North Texas\nyan.huang@unt.edu\nHamid Palangi1\nMicrosoft Research\nRadames Cruz Moreno2\nMicrosoft Research\nHamed Khanpour3\nMicrosoft Research\n{hpalangi1,radames.cruz2,hamed.khanpour3}@microsoft.com\nAbstract\nRecent studies have revealed that the widely-\nused Pre-trained Language Models (PLMs)\npropagate societal biases from the large un-\nmoderated pre-training corpora. Existing so-\nlutions require debiasing training processes\nand datasets for debiasing, which are resource-\nintensive and costly. Furthermore, these meth-\nods hurt the PLMs’ performance on down-\nstream tasks. In this study, we propose Gender-\ntuning, which debiases the PLMs through fine-\ntuning on downstream tasks’ datasets. For this\naim, Gender-tuning integrates Masked Lan-\nguage Modeling (MLM) training objectives\ninto fine-tuning’s training process. Compre-\nhensive experiments show that Gender-tuning\noutperforms the state-of-the-art baselines in\nterms of average gender bias scores in PLMs\nwhile improving PLMs’ performance on down-\nstream tasks solely using the downstream tasks’\ndataset. Also, Gender-tuning is a deployable\ndebiasing tool for any PLM that works with\noriginal fine-tuning.\n1 Introduction\nPre-trained Language Models (PLMs) have\nachieved state-of-the-art performance across vari-\nous tasks in natural language processing (Devlin\net al., 2019; Liu et al., 2019; Clark et al., 2020).\nOne of the crucial reasons for this success is pre-\ntraining on large-scale corpora, which is collected\nfrom unmoderated sources such as the internet.\nPrior studies (Caliskan et al., 2017; Zhao et al.,\n2018; May et al., 2019; Kurita et al., 2019; Gehman\net al., 2020) have shown that PLMs capture a sig-\nnificant amount of social biases existing in the pre-\ntraining corpus. For instance, they showed that\nthe PLMs learned that the word \"he\" is closer to\nthe word \"engineer\" because of the frequent co-\noccurrence of this combination in the training cor-\npora, which is known as social gender biases. Since\nPLMs are increasingly deployed in real-world sce-\nnarios, there is a serious concern that they propa-\ngate discriminative prediction and unfairness.\nSeveral solutions for mitigating the social bi-\nases have been proposed, including: using banned\nword lists (Raffel et al., 2020), building deliber-\nated training datasets (Bender et al., 2021), balanc-\ning the biased and unbiased terms in the training\ndataset (Dixon et al., 2018; Bordia and Bowman,\n2019), debiasing embedding spaces (Liang et al.,\n2020; Cheng et al., 2021), and self-debiasing in\ntext generation (Schick et al., 2021). Although all\nthese solutions have shown different levels of suc-\ncess, they tend to limit the PLMs’ ability (Meade\net al., 2022). For example, the banned words solu-\ntion prevent gaining knowledge of topics related to\nbanned words. Also, some of them hurt the PLMs’\nperformance on downstream tasks. Furthermore,\ndataset curation and pre-training are two resource-\nintensive tasks needed for most of the above solu-\ntions (Schick et al., 2021).\nIn this study, we address the challenges men-\ntioned above by proposing an effective approach\nnamed Gender-tuning for debiasing the PLMs\nthrough fine-tuning on downstream tasks’ datasets.\nFor this goal, Gender-tuning perturbs the training\nexamples by first finding the gender-words in the\ntraining examples based on a given gender-word\nlist. Then Gender-tuning replaces them with the\nnew words to interrupt the association between the\ngender-words and other words in the training ex-\namples (Table 1). Finally, Gender-tuning classifies\nthe examples with the replaced words according\nto the original training examples’ ground-truth la-\nbels to compute a joint loss from perturbation and\nclassification for training the Gender-tuning. The\nkey advantage of our method is integrating the de-\nbiasing process into the fine-tuning that allows the\ndebiasing and fine-tuning to perform simultane-\nously. Thus, Gender-tuning does not require sepa-\nrate pre-training or additional training data. Also,\nthis integration makes Gender-tuning a plug-and-\n5448\n1 Original example:\n\"[he] is at 22 a powerful [actor].\"\nPerturbed examples:\nepoch 1⇒\"[girl] is at 22 a powerful [UNK].\"\nepoch 2⇒\"[boy] is at 22 a powerful [actor].\"\nepoch 3⇒\"[She] is at 22 a powerful [actress].\"\n2 Original example:\n\"[she] beautifully chaperon the [girls] in the kitchen.\"\nPerturbed examples:\nepoch 1⇒\"[lady] beautifully chaperon the [women] in the kitchen.\"\nepoch 2⇒\"[girl] beautifully chaperon the [boys] in the kitchen.\"\nepoch 3⇒\"[he] beautifully chaperon the [men] in the kitchen.\"\nTable 1: Some perturbed examples generated by Gender-\ntuning through three training epochs.\nplay debiasing tool for any PLMs that works with\noriginal fine-tuning.\nTo evaluate the effectiveness of our proposed\nmethod, we conducted comprehensive experiments\nfollowing two state-of-the-art debiasing baselines:\nSENT_DEBIAS (Sent-D) (Liang et al., 2020) and\nFairFil (FairF) (Cheng et al., 2021). The results\nshow that Gender-tuning outperforms both base-\nlines in terms of the average gender-bias scores in\nthe BERT model while improving its performance\non the downstream tasks. In addition, we reported\nthe performance of Gender-tuning applied to the\nRoBERTa that shows considerable improvement.\nFinally, our ablation studies demonstrate that all\ncomponents of Gender-tuning, including two train-\ning phases and joint loss, play an essential role in\nachieving success.\n2 Methodology\nWe propose a novel debiasing approach, named\nGender-tuning (Figure 1), that performs the de-\nbiasing process and fine-tuning simultaneously\non the downstream tasks’ dataset. For this aim,\nGender-tuning integrates two training objectives:\n1) Masked Language Modeling (MLM) training\nobjective for gender-word perturbation and 2) Fine-\ntuning for classification. In each training batch,\nGender-tuning works as follows:\nGender-tuning uses MLM to perturb training ex-\namples by masking the existing gender-word(s)1.\nThe MLM training objective is to predict masked\ntoken(s) with a mean cross-entropy loss that we\ndenote as perturbation-loss ( Lperturb). The train-\ning examples with predicted tokens, called gender-\nperturbed examples (Table 1), are fed into fine-\n1We use the feminine and masculine word lists created by\n(Zhao et al., 2018)\nFigure 1: Illustration of Gender-tuning training process.\nMLM and PLM be trained based on Gender-tuning loss.\nThe examples without any gender-word are directly fed\nto fine-tuning.\ntuning to be classified according to the original ex-\namples’ ground-truth label (y). Then pθ(y′= y|ˆx)\nis the fine-tuning classification function to predict\nthe gender-perturbed example’s label ( y′) based\non the gender-perturbed example ( ˆx) to compute\nthe fine-tuning loss (Lfine−tuning), where θis the\nPLM’s parameters for the fine-tuning. A weighted\naggregation of the perturbation loss and fine-tuning\nloss, called joint-loss (Ljoint), is used for training\nthe Gender-tuning as follows:\nLjoint = αLperturb + (1−α)Lfine−tuning (1)\nwhere αis a weighting factor that is employed to\nadjust the contribution of the two training losses in\ncomputing the joint-loss.\nThe Gender-tuning training objective is to mini-\nmize joint-loss to ensure that the label of the per-\nturbed example is the same as the label of the\noriginal training example. In the following, we\npresent how joint-loss impacts the training process\nof Gender-tuning in each training batch:\nSuppose the MLM predicts an incorrect token.\nFor instance, the example: \"the film affirms the\npower of the [actress]\" changes to \"the film affirms\nthe power of the [trauma]\". In this example, the\npredicted word [trauma] is a non-related gender-\nword that raises perturbation-loss value (Lperturb\n> 0). In this case, even if fine-tuning classifies the\nperturbed example correctly, joint-loss is still big\nenough to force Gender-tuning to continue training.\nAlso, suppose Gender-tuning creates social gen-\nder bias through gender perturbation. For instance,\nthe example: \"angry black [actor]\" changes to \"an-\ngry black [woman]\" that \"woman\" and \"actor\" are\nnot close semantically that raises perturbation-loss\n5449\nSST-2 BERT RoBERTaOriginSent-D FairF Gender-tuningrandomGender-tuning (ours)OriginGender-tuningrandomGender-tuning (ours)Names, Career/Family0.03 0.10 0.21 0.46 0.03 0.07 0.08 0.14Terms, Career/Family0.01 0.05 0.37 0.03 0.16 0.33 0.44 0.01Terms, Math/Art0.21 0.22 0.26 0.05 0.39 1.32 1.25 0.57Names, Math/Art1.15 0.75 0.09 0.65 0.31 1.34 1.12 1.11Terms, Science/Art0.10 0.08 0.12 0.42 0.07 0.25 0.12 0.47Names, Science/Art0.22 0.04 0.005 0.38 0.10 0.47 0.62 0.47Avg. Abs. e-size0.2910.212 0.182 0.331 0.176 0.630 0.605 0.461Accuracy 91.9789.10 91.60 92.66 92.10 93.57 93.92 93.69\nCoLANames, Career/Family0.0090.14 0.03 0.34 0.09 0.29 0.15 0.05Terms, Career/Family0.19 0.18 0.11 0.15 0.03 0.26 0.08 0.00Terms, Math/Art0.26 0.31 0.09 0.55 0.08 0.06 0.02 0.15Names, Math/Art0.15 0.30 0.10 0.72 0.24 0.06 0.25 0.07Terms, Science/Art0.42 0.16 0.24 0.05 0.07 0.32 0.57 0.70Names, Science/Art0.03 0.19 0.12 0.28 0.07 0.27 0.14 0.03Avg. Abs. e-size0.181.217 0.120 0.343 0.096 0.210 0.201 0.166Accuracy 56.5155.40 56.50 56.85 56.60 57.35 57.55 58.54\nQNLINames, Career/Family0.260.05 0.10 0.01 0.02 0.04 0.38 0.17Terms, Career/Family0.150.0040.20 0.13 0.04 0.22 0.10 0.04Terms, Math/Art0.580.08 0.32 0.30 0.08 0.53 0.16 0.09Names, Math/Art0.580.62 0.28 0.23 0.16 0.48 0.06 0.03Terms, Science/Art0.080.71 0.24 0.25 0.21 0.47 0.57 0.53Names, Science/Art0.520.44 0.16 0.15 0.04 0.36 0.47 0.52Avg. Abs. e-size0.3650.321 0.222 0.178 0.091 0.350 0.290 0.230Accuracy 91.3090.60 90.80 91.61 91.32 92.03 92.51 92.09\nTable 2: Comparing the debiasing performance of Gender-tuning and two state-of-the-art baselines. First six rows\nmeasure binary SEAT effect size (e-size; lower is better) for sentence-level tests from (Caliskan et al., 2017). The\nseventh row presents the average absolute e-size. The eighth row shows the classification accuracy on downstream\ntasks. The Gender-tuningrandom masks the input example randomly (not only gender-words). Gender-tuning gains\nthe lowest average bias in both models and all datasets.\nvalue (Lperturb > 0). In this case, the output of the\nfine-tuning might be correct ( Lfine−tuning ≈0)\ndue to the PLMs’ learned biases (\"angry black\nwoman\" is a known gender/race bias). However,\ndue to the big value of perturbation-loss, the join-\nloss is big enough to override fine-tuning results\nand forces Gender-tuning to continue training.\nMoreover, we observed that sometimes exam-\nple perturbation changes the concept/label of train-\ning examples. For instance, the input: \"[He]\nis an excellent [actor] (label: positive)\" changes\nto \"[She] is a wonderful [murderer] (label: posi-\ntive)\", and fine-tuning classification output is cor-\nrect (Lfine−tuning ≈0). In this example, the pre-\ndicted word [murderer] is conceptually far from\ngender-related words [actor]. So, perturbation loss\nbecomes significant, which creates a big value for\njoint-loss to force Gender-tuning to continue train-\ning. Finally, we found examples that MLM re-\nplaces the gender-word with the [UNK] token. In\nthese examples, the perturbation-loss is close to\nzero ( Lperturb ≈ 0) and the output of the fine-\ntuning classifier is incorrect (Lfine−tuning > 0). In\nthis case, the joint-loss is big enough to continue\ntraining and provide a new chance for MLM to pre-\ndict a meaningful token instead of a [UNK]. More\nanalysis of our perturbation strategy can be found\nin Section 4.1 and Table 3.\n3 Experimental Setup\nTo evaluate our proposed method, we conduct\nexperiments by following the evaluation process\nof the two state-of-the-art baselines (Sent-D and\nFairF) such as the bias evaluation metric (SEAT),\napplied PLMs, and downstream tasks’ datasets.2\nWe report the SEAT effect size (e-size), aver-\nage absolute e-size, and classification accuracy on\ndownstream tasks for three different setups: 1)\nOrigin: fine-tuning the PLMs on the downstream\ntask datasets using huggingface transformers code\n(Wolf et al., 2020). 2) Gender-tuningrandom: in-\nstead of replacing the gender-words in an training\nexample, Gender-tuningrandom replaces a certain\npercentage of an input tokens randomly (5% of\neach input sequence). 3) Gender-tuning: the pro-\nposed method. We used the same hyperparameter\nfor all three setups for a fair comparison.\n4 Results and Discussion\nTable 2 illustrates SEAT absolute effect size (e-\nsize) (lower is better) on sentence templates of\nTerms/Names under different gender domains pro-\nvided by (Caliskan et al., 2017), average abso-\nlute e-size (lower is better), and classification\naccuracy on downstream tasks (higher is better)\n2Details of the baselines, bias evaluation metric, PLMs,\ndatasets, and hyperparameters are presented in Appendix A.\n5450\nTraining input Perturbed Type Label\nwith [his] usual intelligence and subtlety.with [the]usual intelligence and subtlety.neutral 1\nby casting an [actress] whose face projectsby casting an [image] whose face projects\nthat [woman] ’s doubts and yearnings , that [person] ’s doubts and yearnings , neutral 1\nit succeeds. it succeeds.\ncertainly has a new career ahead of [him] if certainly has a new career ahead of [her] if convert-gender1\n[he] so chooses. [she] so chooses.\nby [men] of marginal intelligence , withby [people] of marginal intelligence , withneutral 0\nreactionary ideas. reactionary ideas.\nwhy this distinguished [actor] would stoop so low.why this distinguished [man] would stoop so low.same-gender 0\nit is very awful - - and oozing with creepy [men]. it is very awful - - and oozing with creepy [UNK] .deleting 0\nProves once again [he] hasn’t lost. Proves once again [he] hasn’t lost . identical 1\nTable 3: The illustration of the different types of perturbation outputs generated by Gender-tuning and their ground-\ntruth label.\nfor three experiment setups (Section 3) and two\nstate-of-the-art baselines. The results show that\nGender-tuning outperforms the baselines regard-\ning the average absolute effect size for both PLMs\non all datasets. Also, in contrast with the base-\nlines, Gender-tuning improves the accuracy of both\nPLMs on all downstream tasks. It shows that the\nproposed method preserves the useful semantic\ninformation of the training data after debiasing.\nThe Gender-tuningrandom results show an incon-\nsistent effect on the bias scores. Although Gender-\ntuningrandom improves the PLMs’ accuracy on the\ndownstream tasks, it significantly magnifies the\nbias score in the BERT model on SST-2 and CoLA.\nAlso, it slightly reduces the average bias score in\nthe RoBERTa on all datasets and in BERT on the\nQNLI.\n4.1 Perturbation Analysis\nThe PLMs achieved state-of-the-art performance\non the downstream tasks datasets by applying the\nMLM for the example perturbation in pre-training\nphase. Thus we hypothesize that the MLM can gen-\nerate realistic gender-perturbed examples that can\nconsiderably modify the gender relation between\nthe input tokens without affecting the label. How-\never, there is a concern that the pre-trained MLM\ntransfers the gender bias through the perturbation\nprocess.\nTo address this concern, we investigate the pre-\ndicted tokens that the pre-trained MLM replaces\nwith the gender-words. We randomly select 300\nexamples from training dataset including 150 exam-\nples with feminine words and 150 examples with\nmasculine words. Based on these 300 examples,\nwe observe five types of perturbation as shown\nthrough some examples in Table 3:\n• Neutral; replace the gender-words with neu-\ntral word such as people, they, their, and etc.\n• Convert-gender; replace the gender-words\nwith opposite gender. the word \"he\" change\nto \"she\".\n• Same-gender; replace the gender-words with\nthe same gender. change the word \"man\" to\n\"boy\".\n• Deleting; replace the gender-words with un-\nknown token ([UNK]). In 300 examples, it\nonly happens when there are several masked\ntokens.\n• Identical; replace the gender-word with it-\nself. It mostly happens when there is only one\ngender-word.\nIn our investigation with 300 examples, we had\n46% Neutral, 29% Identical, 17% Convert-gender,\n7% Same-gender, and 1% Deleting perturbation.\nAs illustrated in Table 3, Gender-tuning does not\nmake a meaningful change in identical and same-\ngender perturbation. These examples likely con-\nform to the gender biases in the MLM. Suppose\nidentical, or same-gender perturbation gets the cor-\nrect output from the perturbation process (Lperturb.\n≈0). In this case, the only way to learn the biases\nin the MLM is to get the correct output from fine-\ntuning step and joint-loss close to zero. This issue\nstops the MLM and fine-tuning model from further\nupdate. However, joint-loss plays an essential role\n5451\nSST-2 BERT RoBERTaOriginGender-tuning/ Gender-tuning/ Gender-tuningOriginGender-tuning/ Gender-tuning/ Gender-tuningno-joint-train no-joint-loss (ours) no-joint-train no-joint-loss (ours)Names, Career/Family0.03 0.22 0.16 0.03 0.07 0.18 0.62 0.14Terms, Career/Family0.01 0.31 0.37 0.16 0.33 0.09 0.41 0.01Terms, Math/Art0.21 0.75 0.49 0.39 1.32 0.99 1.02 0.57Names, Math/Art1.15 0.55 0.56 0.31 1.34 0.92 0.97 1.11Terms, Science/Art0.10 0.01 0.32 0.07 0.25 0.76 0.00 0.47Names, Science/Art0.22 0.07 0.47 0.10 0.47 0.76 0.56 0.47Avg. Abs. e-size0.291 0.318 0.395 0.176 0.630 0.616 0.596 0.461Accuracy 91.97 92.88 92.66 92.10 93.57 94.38 92.54 93.69\nCoLANames, Career/Family0.09 0.37 0.04 0.09 0.29 0.07 0.16 0.05Terms, Career/Family0.19 0.06 0.11 0.03 0.26 0.16 0.11 0.00Terms, Math/Art0.26 0.89 0.96 0.08 0.06 0.41 0.29 0.15Names, Math/Art0.15 1.03 0.82 0.24 0.06 0.22 0.87 0.07Terms, Science/Art0.42 0.47 0.19 0.07 0.32 0.42 0.80 0.70Names, Science/Art0.03 0.49 0.32 0.07 0.27 0.36 0.88 0.03Avg. Abs. e-size0.181 0.551 0.406 0.096 0.210 0.273 0.518 0.166Accuracy 56.51 56.32 56.70 56.60 57.35 62.11 57.27 58.54\nQNLINames, Career/Family0.26 0.03 0.15 0.02 0.04 0.12 0.14 0.17Terms, Career/Family0.15 0.20 0.41 0.04 0.22 0.31 0.11 0.04Terms, Math/Art0.58 0.47 0.03 0.08 0.53 0.50 0.62 0.09Names, Math/Art0.58 0.94 0.04 0.16 0.48 0.38 0.42 0.03Terms, Science/Art0.08 0.12 0.27 0.21 0.47 0.25 0.50 0.53Names, Science/Art0.52 0.54 0.11 0.04 0.36 0.03 0.20 0.52Avg. Abs. e-size0.365 0.383 0.168 0.091 0.350 0.265 0.331 0.230Accuracy 91.30 91.57 91.28 91.32 92.03 92.58 91.69 92.09\nTable 4: Comparing the debiasing performance of two ablation experiments and Gender-tuning (ours) on three\ndownstream task datasets. The results show that Gender-tuning achieved the least average bias score and consistently\nimproved the classification accuracy.\nin alleviating learning gender bias from identical\nand same-gender perturbations.\nTo clarify the role of joint-loss in overcoming\nabove problem, we investigated fine-tuning output\non identical and same-gender perturbations. We\nobserved that fine-tuning gets the incorrect output\nfrom 60% of the identical and 75% of the same-\ngender perturbation. Thus these examples return\nto training iteration because their joint-loss is large\nenough to update the language models and perform\na new training iteration. New training iteration\nmeans re-perturbing and re-fine-tuning result on\nthese examples. Therefore, training based on both\ntraining steps’ loss and computing joint-loss persis-\ntently prevents learning from gender bias in MLM\nas well as the PLM.\n5 Ablation\nWe conduct the ablation experiments to demon-\nstrate the effectiveness of Gender-tuning com-\nponents, including 1) joint-training process and\n2) joint-loss in Gender-tuning’s debiasing perfor-\nmance (Table 4). The experiments are as follows:\n1) Gender-tuningno−joint−training: first we used\nMLM to train the PLM through the gender-word\nperturbation on downstream task datasets. Then\nwe fine-tuned the PLM on the downstream task\ndataset. 2) Gender-tuningno−joint−loss: we train\nGender-tuning based on only fine-tuning loss.\nIn both PLMs, results illustrate that Gender-\ntuning is more effective for reducing the average\ngender bias than in two ablation experiments. The\ntwo ablation experiments magnify the bias scores\nnoticeably, while Gender-tuning gains the small-\nest SEAT absolute effect size, especially in the\nBERT model. Results also show that the ablation\nexperiment setups that do not benefit from joint-\nloss cannot update the MLM and PLM when the\noutput of the fine-tuning classification is correct\n(Lfine−tuning ≈0), even though the correct output\nlikely bases on the gender biases in the PLMs.\n6 Conclusion\nWe propose a novel approach for debiasing PLMs\nthrough fine-tuning on downstream tasks’ datasets.\nThe proposed method is an aggregation of bias-\nword perturbation using MLM and fine-tuning clas-\nsification. In this study, we evaluated our proposed\nmethod on gender biases and named it Gender-\ntuning. Comprehensive experiments prove that\nGender-tuning outperforms two state-of-the-art de-\nbiasing methods while improving the performance\nof the PLMs on downstream tasks. The key advan-\ntage of our approach is using the fine-tuning setting\nthat allows the training process to be carried out\nwithout needing additional training processes or\ndatasets. Also, it makes Gender-tuning a plug-and-\nplay debiasing tool deployable to any PLMs.\n5452\n7 Limitation\nAlthough Gender-tuning succeeds in reducing the\ngender bias scores in the pre-trained language mod-\nels, there are some limitations to performing debi-\nasing. Gender-tuning only works on gender-related\nwords list. Thus Gender-tuning cannot cover the\nprobable gender biases that do not exist in its’ list.\nWe defer the gender-related word list modification\nto future research. All our experiments ran on En-\nglish language texts with English gender-word mor-\nphology.\nReferences\nSoumya Barikeri, Anne Lauscher, Ivan Vuli´c, and Goran\nGlavaš. 2021. Redditbias: A real-world resource\nfor bias evaluation and debiasing of conversational\nlanguage models. arXiv preprint arXiv:2106.03521.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\npages 610–623.\nTolga Bolukbasi, Kai-Wei Chang, James Y Zou,\nVenkatesh Saligrama, and Adam T Kalai. 2016. Man\nis to computer programmer as woman is to home-\nmaker? debiasing word embeddings. Advances\nin neural information processing systems, 29:4349–\n4357.\nShikha Bordia and Samuel R Bowman. 2019. Identify-\ning and reducing gender bias in word-level language\nmodels. NAACL HLT 2019, page 7.\nMarc-Etienne Brunet, Colleen Alkalay-Houlihan, Ash-\nton Anderson, and Richard Zemel. 2019. Under-\nstanding the origins of bias in word embeddings. In\nInternational conference on machine learning, pages\n803–811. PMLR.\nAylin Caliskan, Joanna J Bryson, and Arvind Narayanan.\n2017. Semantics derived automatically from lan-\nguage corpora contain human-like biases. Science,\n356(6334):183–186.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and\nGeoffrey Hinton. 2020. A simple framework for\ncontrastive learning of visual representations. In In-\nternational conference on machine learning, pages\n1597–1607. PMLR.\nPengyu Cheng, Weituo Hao, Siyang Yuan, Shijing Si,\nand Lawrence Carin. 2021. Fairfil: Contrastive neu-\nral debiasing method for pretrained text encoders. In\nInternational Conference on Learning Representa-\ntions.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than genera-\ntors.\nSunipa Dev, Tao Li, Jeff M Phillips, and Vivek Sriku-\nmar. 2020. On measuring and mitigating biased in-\nferences of word embeddings. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol-\nume 34, pages 7659–7666.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), pages 4171–\n4186.\nEmily Dinan, Angela Fan, Adina Williams, Jack Ur-\nbanek, Douwe Kiela, and Jason Weston. 2020.\nQueens are powerful too: Mitigating gender bias in\ndialogue generation. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 8173–8188.\nLucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain,\nand Lucy Vasserman. 2018. Measuring and mitigat-\ning unintended bias in text classification. In Proceed-\nings of the 2018 AAAI/ACM Conference on AI, Ethics,\nand Society, pages 67–73.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A Smith. 2020. Realtoxici-\ntyprompts: Evaluating neural toxic degeneration in\nlanguage models. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing: Findings, pages 3356–3369.\nMasahiro Kaneko and Danushka Bollegala. 2019.\nGender-preserving debiasing for pre-trained word\nembeddings. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1641–1650.\nKeita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black,\nand Yulia Tsvetkov. 2019. Measuring bias in contex-\ntualized word representations. In Proceedings of the\nFirst Workshop on Gender Bias in Natural Language\nProcessing, pages 166–172.\nPaul Pu Liang, Irene Mengze Li, Emily Zheng,\nYao Chong Lim, Ruslan Salakhutdinov, and Louis-\nPhilippe Morency. 2020. Towards debiasing sentence\nrepresentations. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\n5453\nThomas Manzini, Lim Yao Chong, Alan W Black, and\nYulia Tsvetkov. 2019. Black is to criminal as cau-\ncasian is to police: Detecting and removing multi-\nclass bias in word embeddings. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 615–621.\nChandler May, Alex Wang, Shikha Bordia, Samuel R\nBowman, and Rachel Rudinger. 2019. On measuring\nsocial biases in sentence encoders. In NAACL-HLT\n(1).\nNicholas Meade, Elinor Poole-Dayan, and Siva Reddy.\n2022. An empirical survey of the effectiveness of\ndebiasing techniques for pre-trained language models.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 1878–1898.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. Journal of Machine Learning Research, 21:1–\n67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100, 000+ questions for\nmachine comprehension of text. In EMNLP.\nShauli Ravfogel, Yanai Elazar, Hila Gonen, Michael\nTwiton, and Yoav Goldberg. 2020. Null it out: Guard-\ning protected attributes by iterative nullspace projec-\ntion. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n7237–7256.\nTimo Schick, Sahana Udupa, and Hinrich Schütze. 2021.\nSelf-diagnosis and self-debiasing: A proposal for\nreducing corpus-based bias in nlp. arXiv preprint\narXiv:2103.00453.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 conference on empiri-\ncal methods in natural language processing, pages\n1631–1642.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. Glue:\nA multi-task benchmark and analysis platform for\nnatural language understanding. In Proceedings of\nthe 2018 EMNLP Workshop BlackboxNLP: Analyz-\ning and Interpreting Neural Networks for NLP, pages\n353–355.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2019.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. In 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association for Computational\nLinguistics, 7:625–641.\nKellie Webster, Xuezhi Wang, Ian Tenney, Alex Beutel,\nEmily Pitler, Ellie Pavlick, Jilin Chen, Ed Chi, and\nSlav Petrov. 2020. Measuring and reducing gendered\ncorrelations in pre-trained models. arXiv preprint\narXiv:2010.06032.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nJieyu Zhao, Yichao Zhou, Zeyu Li, Wei Wang, and\nKai-Wei Chang. 2018. Learning gender-neutral word\nembeddings. In EMNLP.\nRan Zmigrod, Sabrina J Mielke, Hanna Wallach, and\nRyan Cotterell. 2019. Counterfactual data augmenta-\ntion for mitigating gender stereotypes in languages\nwith rich morphology. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 1651–1661.\nA Appendix\nA.1 Baselines\nFor comparison purposes, we chose two state-\nof-the-art baselines which focus on debiasing\nsentence-level pre-trained text encoders in PLMs.\nA.1.1 SENT-DEBIAS\nSENT-DEBIAS (Liang et al., 2020) is an exten-\nsion of the HARD-DEBIAS method (Bolukbasi\net al., 2016) to debias sentences for both binary\nand multi-class bias attributes spanning gender and\nreligion. The key advantage of Sent-D is the con-\ntextualization step in which bias-attribute words are\nconverted into bias-attribute sentences by using a\ndiverse set of sentence templates from text corpora.\nSent-D is a four-step process that involves: identi-\nfying words that exhibit biased attributes, contextu-\nalizing them in sentences that contain these biases,\ncreating sentence representations, estimating the\nsubspace of the bias represented in the sentences,\nand debiasing general sentences by removing the\nprojection onto this subspace.\n5454\nA.1.2 FairFil\nFairF (Cheng et al., 2021) is the first neural debias-\ning method for pretrained sentence encoders. For\na given pretrained encoder, FairF learns a fair fil-\nter (FairFil) network, whose inputs are the original\nembedding of the encoder, and outputs are the de-\nbiased embedding. Inspired by the multi-view con-\ntrastive learning (Chen et al., 2020), for each train-\ning sentence, FairF first generates an augmentation\nthat has the same semantic meaning but in a differ-\nent potential bias direction. FairFil is contrastively\ntrained by maximizing the mutual information be-\ntween the debiased embeddings of the original\nsentences and corresponding augmentations. To\nfurther eliminate bias from sensitive words in sen-\ntences, FairF uses debiasing regularizer, which min-\nimizes the mutual information between debiased\nembeddings and the sensitive words’ embeddings.\nA.2 Bias Evaluation Metric\nFollowing the prior studies (Sent-D and FairF), we\nuse Sentence Encoder Association Test (SEAT)\n(May et al., 2019) to measure the gender bias\nscores in the pre-trained language models that\ntrained using Gender-tuning. SEAT extended\nthe Word Embedding Association Test (WEAT;\ncaliskan2017semantics) to sentence-level represen-\ntations. WEAT compares the distance of two sets.\nTwo sets of target words (e.g., {family, child, par-\nent,...} and {work, office, profession,...} ) that char-\nacterize particular concepts family and careerre-\nspectively. Two sets of attribute words (e.g., {man,\nhe, him,...} and {woman, she, her,...} ) that charac-\nterize a type of bias. WEAT evaluates whether the\nrepresentations for words from one particular at-\ntribute word set tend to be more closely associated\nwith the representations for words from one partic-\nular target word set. For instance, if the female\nattribute words listed above tend to be more closely\nassociated with the family target words, this may\nindicate bias within the word representations.\nLet’s denote Aand Bas sets of attribute words\nand X and Y the set of target words. As described\nin (Caliskan et al., 2017) the WEAT test statistic is:\ns(X,Y,A,B ) =\n∑\nx∈X\ns(x,A,B )−\n∑\ny∈Y\ns(y,A,B )\n(2)\nwhere for a specific word w, s(w,A,B ) is defined\nas the difference between w’s mean cosine simi-\nlarity with the words from Aand w’s mean cosine\nsimilarity with the word from B. They report an\neffective size given by:\nd= µ([s(x,A,B )]x∈X −µ([s(y,A,B )]y∈Y)\nσ([s(t,X,Y )]t∈A∪B)\n(3)\nwhere µand σdenote the mean and standard devi-\nation respectively. Hence, an effect size closer to\nzero represents smaller degree of bias in the word\nrepresentation. The SEAT test extended WEAT\nby replacing the word with a collection of tem-\nplate sentences (i.e., \"this is a [word]\", \"that is\na [word]\"). Then the WEAT test statistic can be\ncomputed on a given sets of sentences including\nattribute and target words using sentence represen-\ntations from a language model.\nA.3 PLMs\nTwo widely used pre-trained language models have\nbeen chosen for this study, BERT-base (Devlin\net al., 2019)and RoBERTa-base (Liu et al., 2019).\nBERT-base is a bidirectional encoder with 12 layers\nand 110M parameters that is pre-trained on 16GB\nof text. RoBERTa-base has almost the same ar-\nchitecture as BERT but is pre-trained on ten times\nmore data (160GB) with significantly more pre-\ntraining steps than BERT.\nA.4 Datasets\nWe conducted empirical studies on the following\nthree tasks from the GLUE 3 benchmark (Wang\net al., 2019):\n(1) SST-2: Stanford Sentiment Treebank is used for\nbinary classification for sentences extracted from\nmovie reviews (Socher et al., 2013). It contains\n67K training sentences.\n(2) CoLA: Corpus of Linguistic Acceptability\n(Warstadt et al., 2019) consists of English accept-\nability judgment. CoLA contains almost 9K train-\ning examples.\n(3) QNLI: Question Natural Language Inference\n(Wang et al., 2018) is a QA dataset which is derived\nfrom the Stanford Question Answering Dataset (Ra-\njpurkar et al., 2016) and used for binary classifica-\ntion. QNLI contains 108K training pairs.\nAlso, we use the feminine and masculine word\nlists created by (Zhao et al., 2018) for gender-word\nperturbation in Gender-tuning.\n3https://gluebenchmark.com/tasks\n5455\nA.5 Hyperparameters\nThe hyperparameters of the models, except batch\nsize, are set to their default4 values (e.g., epoch =\n3, learning-rate = 2×10−5, and etc.). After trying\nseveral trials run, the batch size has been selected\namong {8,16,32}. We empirically selected the\noptimal value for αby a grid search in 0 < α< 1\nwith 0.1 increments. For each downstream task, the\nbest value of αsets to 0.7. All experiments were\nperformed with three training epochs and using an\nNVIDIA V100 GPU.\nA.6 Related Works\nDebiasing Database; The most straightforward\napproach for reducing the social biases in the\ntraining corpora is bias-neutralization. In this\nway, the training corpus is directly re-balanced\nby swapping or removing bias-related words and\ncounterfactual data augmentation (CDA) (Zmigrod\net al., 2019; Dinan et al., 2020; Webster et al.,\n2020; Dev et al., 2020; Barikeri et al., 2021). Also,\nGehman et al. (2020) proposed domain-adaptive\npre-training on unbiased corpora. Although the\nresults showed these proposed methods mitigated\nthe social biases in the pre-trained models, they\nneed to be re-trained on a larger scale of the\ncorpora. For example, Webster et al. (2020)\nproposed a CDA that needs an additional 100k\nsteps of training on the augmented dataset. Data\naugmentation and collecting a large-scale unbiased\ncorpus are both computationally costly.\nDebiasing Embedding ; There are several\nsolutions for debiasing static word embedding\n(Bolukbasi et al., 2016; Kaneko and Bollegala,\n2019; Manzini et al., 2019; Ravfogel et al., 2020)\nand debiasing contextualized word-embedding\n(Caliskan et al., 2017; Brunet et al., 2019) and\nsentence-embedding (Liang et al., 2020; Cheng\net al., 2021). Compared to debiasing static word\nembedding, where the semantic representation of a\nword is limited to a single vector, contextualized\nword/sentence embedding models are more\nchallenging (Kaneko and Bollegala, 2019). Since\nthe key to the pre-trained language models’\nsuccess is due to powerful embedding layers\n(Liang et al., 2020), debiasing embedding might\naffect transferring of the accurate information and\nperformance of these models on the downstream\ntasks. Also, they need some pre-training for\n4https://github.com/huggingface/transformers\ndebiasing the embedding layer before fine-tuning\non downstream tasks.\n5456\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\n7\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. Left blank.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nLeft blank.\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0017 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nNot applicable. Left blank.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Left blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. Left blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nLeft blank.\nC □\u0013 Did you run computational experiments?\n3\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\n3\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n5457\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\n3\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n4\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\n3\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n5458",
  "topic": "Debiasing",
  "concepts": [
    {
      "name": "Debiasing",
      "score": 0.9897987842559814
    },
    {
      "name": "Computer science",
      "score": 0.7798448801040649
    },
    {
      "name": "Fine-tuning",
      "score": 0.6101359128952026
    },
    {
      "name": "Process (computing)",
      "score": 0.5681549310684204
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4771106541156769
    },
    {
      "name": "Language model",
      "score": 0.4432833790779114
    },
    {
      "name": "Downstream (manufacturing)",
      "score": 0.4335898160934448
    },
    {
      "name": "Machine learning",
      "score": 0.3802969753742218
    },
    {
      "name": "Natural language processing",
      "score": 0.36569976806640625
    },
    {
      "name": "Psychology",
      "score": 0.12370181083679199
    },
    {
      "name": "Engineering",
      "score": 0.0647059977054596
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Cognitive science",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I123534392",
      "name": "University of North Texas",
      "country": "US"
    }
  ]
}