{
  "title": "Utilizing Large Language Models to Simplify Radiology Reports: a comparative analysis of ChatGPT3.5, ChatGPT4.0, Google Bard, and Microsoft Bing",
  "url": "https://openalex.org/W4379378720",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2515430585",
      "name": "Rushabh Doshi",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2982400332",
      "name": "Kanhai Amin",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2912455786",
      "name": "Pavan Khosla",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A4297477026",
      "name": "Simar Bajaj",
      "affiliations": [
        "Harvard College Observatory"
      ]
    },
    {
      "id": "https://openalex.org/A1559122907",
      "name": "Sophie Chheang",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2171433332",
      "name": "Howard P. Forman",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2515430585",
      "name": "Rushabh Doshi",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2982400332",
      "name": "Kanhai Amin",
      "affiliations": [
        "Yale New Haven Health System",
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2912455786",
      "name": "Pavan Khosla",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4297477026",
      "name": "Simar Bajaj",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1559122907",
      "name": "Sophie Chheang",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2171433332",
      "name": "Howard P. Forman",
      "affiliations": [
        "Yale New Haven Health System",
        "Yale University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4280555552",
    "https://openalex.org/W4200235258",
    "https://openalex.org/W4285818572",
    "https://openalex.org/W4283275765",
    "https://openalex.org/W2951299481",
    "https://openalex.org/W3031507022",
    "https://openalex.org/W1978722665",
    "https://openalex.org/W4292833579",
    "https://openalex.org/W4206584060",
    "https://openalex.org/W4210391548",
    "https://openalex.org/W3198383045",
    "https://openalex.org/W4205305054",
    "https://openalex.org/W1485761988",
    "https://openalex.org/W2278617356",
    "https://openalex.org/W3093338228",
    "https://openalex.org/W2324556660",
    "https://openalex.org/W4283803485",
    "https://openalex.org/W4224215926",
    "https://openalex.org/W4327810494",
    "https://openalex.org/W4323350039",
    "https://openalex.org/W6903655558",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W4322723456",
    "https://openalex.org/W3210682857",
    "https://openalex.org/W3082826915",
    "https://openalex.org/W2613925192",
    "https://openalex.org/W3175341609",
    "https://openalex.org/W1507711477",
    "https://openalex.org/W2048587526",
    "https://openalex.org/W2999964172",
    "https://openalex.org/W1967966556",
    "https://openalex.org/W6851775633",
    "https://openalex.org/W4396870811",
    "https://openalex.org/W4372231834",
    "https://openalex.org/W3158076758",
    "https://openalex.org/W4387356888",
    "https://openalex.org/W4322617328",
    "https://openalex.org/W4225757461"
  ],
  "abstract": "Abstract This paper investigates the application of Large Language Models (LLMs), specifically OpenAI’s ChatGPT3.5, ChatGPT4.0, Google Bard, and Microsoft Bing, in simplifying radiology reports, thus potentially enhancing patient understanding. We examined 254 anonymized radiology reports from diverse examination types and used three different prompts to guide the LLMs’ simplification processes. The resulting simplified reports were evaluated using four established readability indices. All LLMs significantly simplified the reports, but performance varied based on the prompt used and the specific model. The ChatGPT models performed best when additional context was provided (i.e., specifying user as a patient or requesting simplification at the 7th grade level). Our findings suggest that LLMs can effectively simplify radiology reports, although improvements are needed to ensure accurate clinical representation and optimal readability. These models have the potential to improve patient health literacy, patient-provider communication, and ultimately, health outcomes.",
  "full_text": "Utilizing Large Language Models to Simplify Radiology Reports: a comparative analysis \nof ChatGPT3.5, ChatGPT4.0, Google Bard, and Microsoft Bing \n \nRushabh Doshi* MSc, MPH1; Kanhai Amin*,2; Pavan Khosla BA1; Simar Bajaj3, Sophie Chheang MD, MBA4, Howard \nP. Forman MD, MBA4,5,6 \n*= equal contribution, co-first authors  \n \n1. Yale School of Medicine, New Haven, CT, USA  \n2. Yale College, New Haven, CT, USA.  \n3. Harvard College, Cambridge, MA, USA \n4. Department of Radiology and Biomedical Imaging, Yale School of Medicine, New Haven, CT, USA.  \n5. Yale School of Management, New Haven, CT, USA  \n6. Department of Health Policy and Management, Yale School of Public Health, New Haven,  CT, USA \n \nAbstract  \nThis paper investigates the application of Large Language Models (LLMs), specifically OpenAI's \nChatGPT3.5, ChatGPT4.0, Google Bard, and Microsoft Bing, in simplifying radiology reports, thus \npotentially enhancing patient understanding. We examined 254 anonymized radiology reports from \ndiverse examination types and used three different prompts to guide the LLMs' simplification processes. \nThe resulting simplified reports were evaluated using four established readability indices. All LLMs \nsignificantly simplified the reports, but performance varied based on the prompt used and the specific \nmodel. The ChatGPT models performed best when additional context was provided (i.e., specifying user \nas a patient or requesting simplification at the 7th grade level). Our findings suggest that LLMs can \neffectively simplify radiology reports, although improvements are needed to ensure accurate clinical \nrepresentation and optimal readability. These models have the potential to improve patient health literacy, \npatient-provider communication, and ultimately, health outcomes. \n \nIntroduction \nImaging reports are a cornerstone of medical decision-making, providing information for diagnosis, \ntreatment planning, and monitoring disease progression. Historically, only the radiologist and referring \nprovider accessed these reports, but the rise of telemedicine and patient portals, as well as regulatory \nchanges, most recently the 21st Century Cures Act, have increased access to electronic health records \nand transformed patients’ relationship with their medical information.1–4  \nDigital health literacy, defined as the degree to which a patient can obtain, process, and understand \nelectronic information,5 is critical for patients to fully benefit from this transformation.6 Radiology reports, \nhowever, are filled with technical jargon, making them relatively uninterpretable to individuals without a \nclinical background.7 Expanded access to these reports could thus exacerbate patient anxiety, \nmisunderstanding, and emotional distress, particularly with abnormal findings.8–10 Improving radiological \nliteracy could help address these concerns, with other spillover benefits to safety and transparency,11 \nshared decision-making,12 treatment compliance,13 and reducing health disparities.14   \nFifteen years ago, The Joint Commission mandated that health care organizations \"encourage \npatients' active involvement in their own care as a patient safety strategy,”11 and a linchpin of that \nrequirement is data transparency and accessibility. Launched in 2010, the OpenNotes program, which \nallowed patients to access their electronic medical records, demonstrated that 99% of patients wanted the \nprogram to continue and 85% reported that access would inform their future provider and health system \nchoices.15 In radiology, approaches such as leaving a summary statement at the end of the report,16 \nstructured templates with standardized lexicon,17,18 and video reports19 have all been used to improve \ndigital health literacy. Largely underexplored are emerging artificial intelligence (AI) tools to support \npatient understanding. \nUsing deep learning techniques, large language models (LLMs), such as OpenAI’s ChatGPT, Google \nBard, and Microsoft Bing, have emerged as promising tools for the simplification of complex medical \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 7, 2023. ; https://doi.org/10.1101/2023.06.04.23290786doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\ninformation.20,21 More specifically, these models leverage natural language processing (NLP) technologies \nto generate human-like text in response to a user’s prompts. To date, a comparative analysis of these \nLLMs in radiology has not been fully explored.  \nIn this study, we compared the performance of several popular LLMs in producing simplified reports. \nOur objective was to evaluate the effectiveness of LLMs and provide insights into their potential for \nenhancing patient health literacy and promoting better patient–provider communication. \n \nMethods: \nTo investigate the efficacy of four Large Language Models (LLMs) in simplifying radiology reports, \nwe designed a comparative study focusing on OpenAI's ChatGPT3.5 and ChatGPT4.0, Google Bard, and \nMicrosoft Bing. Given that Bing has three conversational styles, we elected to use the precise setting over \nthe creative or balanced settings. Our primary outcome was readability score, using an existing open-\nsource dataset of reports.  \n \nDataset Selection and Modification \nWe used the MIMIC-III database, which is a comprehensive public database from the Beth Israel \nDeaconess Medical Center.22,23 A random selection of 254 anonymized reports was made to ensure \nrepresentation of various examination types (MRI, CT, US (ultrasound), X-ray, Mammogram), anatomical \nregions, and lengths. This dataset allowed us to evaluate LLM performance across diverse clinical \nsituations. \nThe reports in the datasets contained redacted information, so we altered the reports to state “Dr. \nSmith” where a physician name was redacted. Further, we changed redacted dates to “prior,” as many \nreports compared findings to previous studies.  \n \nPrompt Selection \nWe first tested the prompt “Simplify this radiology report:” (Prompt 1). We then tested the prompt \n“I am a patient. Simplify this radiology report:” (Prompt 2).24 Lastly, we tested the prompt, “Simplify this \nradiology report at the 7th grade level” (Prompt 3). Each prompt was followed with the radiology reports \nfrom the MIMIC-III database. \n \nProcessing Radiology Reports and Readability Assessment \nEach of the 254 radiology reports were processed individually by the 4 LLMs (accessed on May \n1st, 2023: ChatGPT3.5 Legacy, ChatGPT4.0, Microsoft Bing, Google Bard) generating simplified versions \nof the original reports for each of the three prompts. In order to standardize the outputs and ensure equal \ncomparison, we removed all formatting, including bullet points and numbered lists, as is consistent with \nprevious readability studies.25,26 Ancillary information, such as “Sure I understand you would like a \nsimplified version of your radiology report” and “please note I am not a medical professional,” was also \nremoved to focus the analysis on the clinical content.    \nWe assessed the LLMs' ability to simplify complex radiology reports by employing four \nestablished readability indices: Gunning Fog (GF), Flesch-Kincaid Grade Level (FK), Automated \nReadability Index (ARI), and Coleman-Liau (CL) indices.27 Each index outputs a score which corresponds \nto a reading grade level (RGL). RGL relates directly to educational attainment: an RGL of 6 corresponds \nto a sixth-grade level, an RGL of 12 corresponds to a high school senior level, and an RGL of 17 \ncorresponds to a four-year college graduate level.28–31  \nAs previously described,25 we averaged the GF, FK, ARI, and CL readability scores for each \noutput to calculate an averaged reading grade level score (aRGL). We applied the non-parametric \nWilcoxon signed-rank and rank-sum tests to compare RGLs and aRGLs.  \n \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 7, 2023. ; https://doi.org/10.1101/2023.06.04.23290786doi: medRxiv preprint \nResults \n \nWe tested the LLMs with the 3 distinct prompts across 5 imaging modalities: X-ray (N=45), US \n(N=11), MRI (N=47), CT (N=107), and mammogram (N=33). Original radiologist reports had a median \naRGL of 17.2 overall, with X-rays at 13.7, ultrasounds at 14.6, MRIs at 16.5, CTs at 18.4, and \nmammograms at 18.8 (Table 1). When comparing original radiologist reports, X-ray reports were \nsignificantly more readable than CT, mammogram, and MRI reports (p<0.001), and ultrasound reports \nwere significantly more readable than reports for CTs and mammograms (p<0.001, Suppl. Fig. 2). \nDespite these relative differences, original X-ray and ultrasound reports were still approximately at the \ncollege RGLs.  \n \nAll four LLMs significantly simplified original radiology reports from baseline complexity across all \nthree prompts for MRI, CT, and mammogram (Figures 1-3, Suppl. Fig. 3). For X-ray and ultrasound, \nChatGPT3.5, ChatGPT4.0, and Bing similarly achieved statistically significant simplification across all \nprompts, but Bard only simplified ultrasounds with Prompt 1 and X-rays with Prompt 2 and 3. \n \n \n \n \n \n \nTable 1: Median of the aRGL for each LLM and \nprompt based on examination type. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 7, 2023. ; https://doi.org/10.1101/2023.06.04.23290786doi: medRxiv preprint \nPrompt 1: “Simplify this radiology finding:” \n \n \n \nUsing Prompt 1, Bing and Bard achieved significantly lower combined median aRGL (9.4 and 8.1) \nthan ChatGPT3.5 and ChatGPT4.0 (10.5 and 10.5, p<0.0001, Figure 1). Bard and Bing otherwise \nperformed similarly, with Bard having the lowest combined median aRGLs for MRI (8.6, p<0.001), \nmammogram (9.3), and overall (9.1) reports and Bing for CT (8.1) and ultrasound (6.6). With Prompt 1, \nChatGPT3.5 and ChatGPT4.0 performed similarly to each another, with typically higher aRGLs than Bing \nand Bard. The only exception was X-rays where ChatGPT3.5 had the lowest median aRGL (10.4), \nsignificantly lower than Bard and Bing. \n \n \n \n \n \nFigure 1 Readability scores of radiologist reports and LLMs using Prompt 1 – “Simplify this radiology finding:” *, \n**, ***, **** correspond to p<0.05, p<0.01, p<0.001, and p<0.0001, respectively. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 7, 2023. ; https://doi.org/10.1101/2023.06.04.23290786doi: medRxiv preprint \nPrompt 2: “I am a patient. Simplify this radiology finding:” \n \n \n \n \nWith the added context of Prompt 2, ChatGPT3.5 and ChatGPT4.0 produced outputs with \nsignificantly lower aRGLs overall compared to Bard and Bing (p<0.0001) and for all imaging modalities \ntested (p<0.05, Figure 2). While there were no significant differences between ChatGPT3.5 and \nChatGPT4.0, ChatGPT3.5 had the lowest median aRGL outputs for all imaging modalities (overall 7.6, \nCT 7.8, X-ray 8.5, MRI 7.2, US 6.5, and mammogram 7.0, Table 1). \n  \n \n \n \nFigure 2 Readability scores of radiologist reports and LLMs using Prompt2 – “I am a patient. Simplify this radiology \nfinding:” *, **, ***, **** correspond to p<0.05, p<0.01, p<0.001, and p<0.0001, respectively. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 7, 2023. ; https://doi.org/10.1101/2023.06.04.23290786doi: medRxiv preprint \nPrompt 3: “Simplify this radiology finding at the 7th grade level:” \n \n \n \n \n \nUsing Prompt 3 revealed similar outcomes to Prompt 2. The ChatGPT models significantly \noutperformed Bard and Bing overall and across all modalities (at least p<0.01, Figure 3), except for X-\nrays where no difference was found between Bard and ChatGPT4. Despite the two versions performing \nsomewhat similarly, ChatGPT3.5 again produced the lowest aRGL outputs across our analysis (overall \n6.7, CT 6.9, X-ray 8.0, MRI 6.6, ultrasound 4.6, and mammogram 5.5; Table 1).  \n  \n \n \n \nFigure 3 Readability scores of radiologist reports and LLMs using Prompt 3 – “Simplify this radiology finding at the \n7th grade level:” *, **, ***, **** correspond to p<0.05, p<0.01, p<0.001, and p<0.0001, respectively. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 7, 2023. ; https://doi.org/10.1101/2023.06.04.23290786doi: medRxiv preprint \nPrompt 1 vs Prompt 2 vs Prompt 3 \n \n. \n \nFinally, we analyzed performance for each LLM across the three prompt combinations (Fig. 4). \nThe ChatGPT models performed better at reducing aRGL with Prompt 2 and Prompt 3 than with Prompt 1 \n(p<0.0001); Prompt 3 also outperformed Prompt 2 (p<0.01). On the other hand, Bard and Bing performed \nbetter with Prompt 1 when compared to Prompt 2 and Prompt 3 (p<0.0001). We also observed that \nPrompt 3 outperforms Prompt 2 in producing lower aRGL outputs for Bard and Bing as well (p<0.0001). \n \n \n \nFigure 4 Comparison of each prompt within LLM. *, **, ***, **** correspond to p<0.05, p<0.01, p<0.001, and \np<0.0001, respectively. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 7, 2023. ; https://doi.org/10.1101/2023.06.04.23290786doi: medRxiv preprint \nDiscussion \n \nIn this study, we showed that the baseline readability of radiology reports across CT, X-ray, MRI, \nultrasound, and mammograms are above the college graduate level but OpenAI’s ChatGPT3.5 and \nChatGPT4.0, Google Bard, and Microsoft Bing can all successfully simplify these reports. The success of \neach of the LLMs varied, however, according to the specific prompt wording. Microsoft Bing and Google \nBard performed best with a straightforward request to simplify a radiology report (Prompt 1), while the \nChatGPT models performed best when provided with added context, such as the user specifying they \nwere a patient (Prompt 2) or requesting simplification at the 7th grade level (Prompt 3). \nOut of countless potential prompts that could have been tested, we focused our analysis on these \nthree to determine how different types of context impacted readability. Prompt 1 was the simplest, \nspecifying only that the inputted text will be a radiology report and that the LLM is tasked with simplifying \nit. The other two prompts offered additional context. For Prompt 3, we specified the 7th grade level \nbecause the American Medical Association and National Institutes of Health recommend that patient \neducation materials should be written between the third- and seventh-grade levels given that the average \nAmerican reads at the eighth-grade level.18,32,33 As expected, Prompt 3 outperformed Prompt 2 across all \nLLMs tested, although we recognize that requesting simplification at a specific grade level is less \naccessible for most users than specifying that “I am a patient.” Unexpectedly, however, Prompt 1 \nobtained the lowest aRGLs for two of the four LLMs tested, Microsoft Bing and Google Bard,—suggesting \nthat richer context does not always equate to improved readability for every LLM. \nSeveral explanations may underlie the observed differences in readability scores across the \nLLMs. For one, variations in training data and preprocessing techniques could impact the different LLMs’ \nability to handle the jargon, abbreviations, and numerical information found in radiology reports.34 \nFurthermore, there may simply be fundamental differences in LLM architectures and algorithms that make \ncertain models more amenable to simplifying medical information.35 We nonetheless found the \ndifferences between Microsoft Bing and Open AI’s ChatGPT models remarkable because Bing is \npowered by OpenAI. The finding that ChatGPT3.5 produced similar outputs to ChatGPT4.0 was also \nnotable because it suggests that updated software does not automatically equate to improved \nperformance, at least in regards to readability.  \nWith patients already using these LLMs to simplify medical information,36 providers cannot ignore \nhow the information-sharing landscape has changed and should consider accordingly. For instance, \nradiologists may consider using LLMs proactively to create a patient-friendly report, inputting it into the \nelectronic medical record alongside their original report to help alleviate patient anxiety, \nmisunderstanding, and emotional distress.37 Epic, Cerner, and other electronic health record companies \nmay soon integrate LLMs into their software such that radiologists would not need to leave the interface \nto rely on third party tools.38 \nWhile LLMs demonstrate promise in helping patients better understand their radiology reports, \nthe ultimate goal should be to strike a balance between readability and preserving clinical fidelity.39 \nIndeed, excessive simplification could contribute to clinical inaccuracies and actually cause patients \ngreater anxiety, so the role of healthcare providers in facilitating communication and understanding \nshould not be overlooked. We believe LLMs could eventually be used as supplementary tools to aid \npatient-provider communication rather than a replacement for personal interaction and discussion, \nhowever, it is essential to study the accuracy and fidelity of these outputs before recommending their \nusage on a wider-scale.40  \nThis study has limitations. For one, radiologists or medical professionals did not assess simplified \noutputs, so we cannot speak to the accuracy, fidelity, and clinical utility of these reports. The readability \nmetrics used in this study are similarly limited because they are language- and structure-focused, so \nthese measures do not necessarily capture relevance or comprehensibility from a medical perspective. \nFurthermore, due to the formulaic nature of these metrics, outputted RGLs were sometimes above a \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 7, 2023. ; https://doi.org/10.1101/2023.06.04.23290786doi: medRxiv preprint \nmeaningful grade level (i.e., a score of 30) and thus held little interpretability on their own. In this study, \nwe were interested in assessing the readability of reports after LLM simplification and evaluating relative \ndifferences from baseline. Finally, we extracted radiology reports from the MIMIC-III dataset, which is \nderived from a single hospital, and employed a cross-sectional design, which may not be ideal for \ncapturing continuous changes in LLMs’ performance. A longitudinal study design, as well as a larger, \nmore diverse dataset, might have improved these results’ validity and generalizability. \n \nConclusion \nOur study highlights how radiology reports are complex medical documents that implement \nlanguage and style above the college graduate reading level, but LLMs are powerful tools for simplifying \nthese reports. Our findings should not be viewed as an endorsement for any particular LLM, instead \ndemonstrating that each LLM tested has the ability to simplify radiology reports across modalities. Careful \nfine-tuning and customization for each LLM may ensure optimal simplification while maintaining the \nclinical integrity of the reports.  \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 7, 2023. ; https://doi.org/10.1101/2023.06.04.23290786doi: medRxiv preprint \n \nTable of Contents \n \neFigure 1: GF, FK, ARI, and CL for Prompt 1. \neFigure 2: GF, FK, ARI, and CL readability scores using Prompt 2. \neFigure 3: GF, FK, ARI, and CL readability scores using Prompt 3. \neTable 1: Median scores across LLM, prompt, modality, and readability index. \neTable 2: Comparison of each modality within each prompt and LLM. \neTable 3: Comparison of each prompt and LLM combination within modality. \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 7, 2023. ; https://doi.org/10.1101/2023.06.04.23290786doi: medRxiv preprint \n \neFigure 1: GF, FK, ARI, and CL readability scores using Prompt 1. *, **, ***, **** correspond to p<0.05, \np<0.01, p<0.001, and p<0.0001, respectively \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 7, 2023. ; https://doi.org/10.1101/2023.06.04.23290786doi: medRxiv preprint \neFigure 2: GF, FK, ARI, and CL readability scores using Prompt 2. *, **, ***, **** correspond to p<0.05, \np<0.01, p<0.001, and p<0.0001, respectively \n \n \n \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 7, 2023. ; https://doi.org/10.1101/2023.06.04.23290786doi: medRxiv preprint \neFigure 3: GF, FK, ARI, and CL readability scores using Prompt 3. *, **, ***, **** correspond to p<0.05, \np<0.01, p<0.001, and p<0.0001, respectively \n \n \n \n \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 7, 2023. ; https://doi.org/10.1101/2023.06.04.23290786doi: medRxiv preprint \neTable 1: Median scores across LLM, prompt, modality, and readability index. *, **, ***, **** \ncorrespond to p<0.05, p<0.01, p<0.001, and p<0.0001, respectively \n \n \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 7, 2023. ; https://doi.org/10.1101/2023.06.04.23290786doi: medRxiv preprint \neTable 2: Comparison of each modality within each prompt and LLM. \nDescription: Modality listed in matrix represents modality with lower median. Significance levels are \nshown. *, **, ***, **** correspond to p<0.05, p<0.01, p<0.001, and p<0.0001, respectively \n \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 7, 2023. ; https://doi.org/10.1101/2023.06.04.23290786doi: medRxiv preprint \neTable 3: Comparison of each prompt and LLM combination within modality. \nDescription: P1- Prompt 1, P2 – Prompt 2, P3 – Prompt 3. Combination listed in matrix has lower median; \nsignificance levels are shown. *, **, ***, **** correspond to p<0.05, p<0.01, p<0.001, and p<0.0001, \nrespectively \n \n \n \n \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 7, 2023. ; https://doi.org/10.1101/2023.06.04.23290786doi: medRxiv preprint \nREFERENCES \n \n1.  Dworkowitz A. Provider Obligations For Patient Portals Under The 21st Century Cures Act. Health Aff     \nForefr. Published online May 16, 2022. doi:10.1377/forefront.20220513.923426 \n \n2. Jain B, Bajaj SS, Stanford FC. All Infrastructure Is Health Infrastructure. Am J Public Health. \n2022;112(1):24-26. doi:10.2105/AJPH.2021.306595 \n3. Lo B, Charow R, Laberge S, Bakas V, Williams L, Wiljer D. Why are Patient Portals Important in the \nAge of COVID-19? Reflecting on Patient and Team Experiences From a Toronto Hospital Network. J \nPatient Exp. 2022;9:23743735221112216. doi:10.1177/23743735221112216 \n4. Yee V, Bajaj SS, Stanford FC. Paradox of telemedicine: building or neglecting trust and equity. Lancet \nDigit Health. 2022;4(7):e480-e481. doi:10.1016/S2589-7500(22)00100-5 \n5. Dunn P, Hazzard E. Technology approaches to digital health literacy. Int J Cardiol. 2019;293:294-296. \ndoi:10.1016/j.ijcard.2019.06.039 \n6. Rodriguez JA, Clark CR, Bates DW. Digital Health Equity as a Necessity in the 21st Century Cures Act \nEra. JAMA. 2020;323(23):2381-2382. doi:10.1001/jama.2020.7858 \n7. Bruno MA, Petscavage-Thomas JM, Mohr MJ, Bell SK, Brown SD. The “Open Letter”: Radiologists’ \nReports in the Era of Patient Web Portals. J Am Coll Radiol. 2014;11(9):863-867. \ndoi:10.1016/j.jacr.2014.03.014 \n8. Kim E, Table B, Ring D, Fatehi A, Crijns TJ. Linguistic tones in MRI reports correlate with severity of \npathology for rotator cuff tendinopathy. Arch Orthop Trauma Surg. Published online August 23, 2022. \ndoi:10.1007/s00402-022-04543-w \n9. Bruno B, Steele S, Carbone J, Schneider K, Posk L, Rose SL. Informed or anxious: patient \npreferences for release of test results of increasing sensitivity on electronic patient portals. Health \nTechnol. 2022;12(1):59-67. doi:10.1007/s12553-021-00628-5 \n10. Mehan WA, Gee MS, Egan N, Jones PE, Brink JA, Hirsch JA. Immediate Radiology Report \nAccess: A Burden to the Ordering Provider. Curr Probl Diagn Radiol. 2022;51(5):712-716. \ndoi:10.1067/j.cpradiol.2022.01.012 \n11. Agency for Healthcare Research and Quality. Patient Engagement and Safety. Patient Safety \nNetwork. Published September 7, 2019. Accessed May 22, 2023. \nhttps://psnet.ahrq.gov/primer/patient-engagement-and-safety \n12. Waseem N, Kircher S, Feliciano JL. Information Blocking and Oncology: Implications of the 21st \nCentury Cures Act and Open Notes. JAMA Oncol. 2021;7(11):1609-1610. \ndoi:10.1001/jamaoncol.2021.3520 \n13. Assiri G. The Impact of Patient Access to Their Electronic Health Record on Medication \nManagement Safety: A Narrative Review. Saudi Pharm J SPJ. 2022;30(3):185-194. \ndoi:10.1016/j.jsps.2022.01.001 \n14. Berkman ND, Sheridan SL, Donahue KE, et al. Health literacy interventions and outcomes: an \nupdated systematic review. Evid ReportTechnology Assess. 2011;(199):1-941. \n15. Esch T, Mejilla R, Anselmo M, Podtschaske B, Delbanco T, Walker J. Engaging patients through \nopen notes: an evaluation using mixed methods. BMJ Open. 2016;6(1):e010034. \ndoi:10.1136/bmjopen-2015-010034 \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 7, 2023. ; https://doi.org/10.1101/2023.06.04.23290786doi: medRxiv preprint \n16. Kadom N, Tamasi S, Vey BL, et al. Info-RADS: Adding a Message for Patients in Radiology \nReports. J Am Coll Radiol. 2021;18(1):128-132. doi:10.1016/j.jacr.2020.09.049 \n17. Panicek DM, Hricak H. How Sure Are You, Doctor? A Standardized Lexicon to Describe the \nRadiologist’s Level of Certainty. AJR Am J Roentgenol. 2016;207(1):2-3. doi:10.2214/AJR.15.15895 \n18. Vincoff NS, Barish MA, Grimaldi G. The patient-friendly radiology report: history, evolution, \nchallenges and opportunities. Clin Imaging. 2022;89:128-135. doi:10.1016/j.clinimag.2022.06.018 \n19. Recht MP, Westerhoff M, Doshi AM, et al. Video Radiology Reports: A Valuable Tool to Improve \nPatient-Centered Radiology. Am J Roentgenol. 2022;219(3):509-519. doi:10.2214/AJR.22.27512 \n20. Lyu Q, Tan J, Zapadka ME, et al. Translating Radiology Reports into Plain Language using \nChatGPT and GPT-4 with Prompt Learning: Promising Results, Limitations, and Potential. Published \nonline March 28, 2023. doi:10.48550/arXiv.2303.09038 \n21. Ali SR, Dobbs TD, Hutchings HA, Whitaker IS. Using ChatGPT to write patient clinic letters. \nLancet Digit Health. 2023;5(4):e179-e181. doi:10.1016/S2589-7500(23)00048-1 \n22. Johnson, Alistair, Pollard, Tom, Mark, Roger. MIMIC-III Clinical Database. Published online \nSeptember 4, 2016. doi:10.13026/C2XW26 \n23. Johnson AEW, Pollard TJ, Shen L, et al. MIMIC-III, a freely accessible critical care database. Sci \nData. 2016;3(1):160035. doi:10.1038/sdata.2016.35 \n24. Ahn S. The impending impacts of large language models on medical education. Korean J Med \nEduc. 2023;35(1):103-107. doi:10.3946/kjme.2023.253 \n25. Pearson K, Ngo S, Ekpo E, et al. Online Patient Education Materials Related to Lipoprotein(a): \nReadability Assessment. J Med Internet Res. 2022;24(1):e31284. doi:10.2196/31284 \n26. Rodriguez F, Ngo S, Baird G, Balla S, Miles R, Garg M. Readability of Online Patient Educational \nMaterials for Coronary Artery Calcium Scans and Implications for Health Disparities. J Am Heart \nAssoc. 2020;9(18):e017372. doi:10.1161/JAHA.120.017372 \n27. Chen W, Durkin C, Huang Y, Adler B, Rust S, Lin S. Simplified Readability Metric Drives \nImprovement of Radiology Reports: an Experiment on Ultrasound Reports at a Pediatric Hospital. J \nDigit Imaging. 2017;30(6):710-717. doi:10.1007/s10278-017-9972-7 \n28. Habeeb A. How readable and reliable is online patient information on chronic rhinosinusitis? J \nLaryngol Otol. 2021;135(7):644-647. doi:10.1017/S0022215121001559 \n29. Kincaid JP, Fishburne Jr, Robert P. R, Richard L. C, Brad S. Derivation of New Readability \nFormulas (Automated Readability Index, Fog Count and Flesch Reading Ease Formula) for Navy \nEnlisted Personnel: Defense Technical Information Center; 1975. doi:10.21236/ADA006655 \n30. Coleman M, Liau TL. A computer readability formula designed for machine scoring. J Appl \nPsychol. 1975;60:283-284. doi:10.1037/h0076540 \n31. Sare A, Patel A, Kothari P, Kumar A, Patel N, Shukla PA. Readability Assessment of Internet-\nbased Patient Education Materials Related to Treatment Options for Benign Prostatic Hyperplasia. \nAcad Radiol. 2020;27(11):1549-1554. doi:10.1016/j.acra.2019.11.020 \n32. Weiss BD. Health Literacy and Patient Safety: Help Patients Understand: Manual for Clinicians. \nAMA Foundation; 2007. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 7, 2023. ; https://doi.org/10.1101/2023.06.04.23290786doi: medRxiv preprint \n33. Hansberry DR, Agarwal N, Baker SR. Health literacy and online educational resources: an \nopportunity to educate patients. AJR Am J Roentgenol. 2015;204(1):111-116. \ndoi:10.2214/AJR.14.13086 \n34. Zhao WX, Zhou K, Li J, et al. A Survey of Large Language Models. Published online May 7, \n2023. Accessed May 22, 2023. http://arxiv.org/abs/2303.18223 \n35. Fan L, Li L, Ma Z, Lee S, Yu H, Hemphill L. A Bibliometric Review of Large Language Models \nResearch from 2017 to 2023. Published online April 3, 2023. Accessed May 22, 2023. \nhttp://arxiv.org/abs/2304.02020 \n36. Lee TC, Staller K, Botoman V, Pathipati MP, Varma S, Kuo B. ChatGPT Answers Common \nPatient Questions About Colonoscopy. Gastroenterology. Published online May 5, 2023. \ndoi:10.1053/j.gastro.2023.04.033 \n37. Mezrich JL, Jin G, Lye C, Yousman L, Forman HP. Patient Electronic Access to Final Radiology \nReports: What Is the Current Standard of Practice, and Is an Embargo Period Appropriate? Radiology. \n2021;300(1):187-189. doi:10.1148/radiol.2021204382 \n38. Landi H. HIMSS23: Epic taps Microsoft to integrate generative AI into EHRs with Stanford, UC \nSan Diego as early adopters. Fierce Healthcare. Published April 17, 2023. Accessed May 22, 2023. \nhttps://www.fiercehealthcare.com/health-tech/himss23-epic-taps-microsoft-integrate-generative-ai-\nehrs-stanford-uc-san-diego-early \n39. Jeblick K, Schachtner B, Dexl J, et al. ChatGPT Makes Medicine Easy to Swallow: An \nExploratory Case Study on Simplified Radiology Reports. Published online December 30, 2022. \nAccessed May 22, 2023. http://arxiv.org/abs/2212.14882 \n40. Doshi RH, Bajaj SS, Krumholz HM. ChatGPT: Temptations of Progress. Am J Bioeth. \n2023;23(4):6-8. doi:10.1080/15265161.2023.2180110 \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted June 7, 2023. ; https://doi.org/10.1101/2023.06.04.23290786doi: medRxiv preprint ",
  "topic": "Readability",
  "concepts": [
    {
      "name": "Readability",
      "score": 0.9678256511688232
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6465439200401306
    },
    {
      "name": "Health literacy",
      "score": 0.5648292899131775
    },
    {
      "name": "Computer science",
      "score": 0.5459458231925964
    },
    {
      "name": "Representation (politics)",
      "score": 0.5065840482711792
    },
    {
      "name": "Microsoft excel",
      "score": 0.447265088558197
    },
    {
      "name": "Radiology",
      "score": 0.3470362424850464
    },
    {
      "name": "Data science",
      "score": 0.32998186349868774
    },
    {
      "name": "Medicine",
      "score": 0.304415762424469
    },
    {
      "name": "Health care",
      "score": 0.18932238221168518
    },
    {
      "name": "Programming language",
      "score": 0.11923915147781372
    },
    {
      "name": "Geography",
      "score": 0.09757530689239502
    },
    {
      "name": "Political science",
      "score": 0.07702299952507019
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I32971472",
      "name": "Yale University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210106258",
      "name": "Harvard College Observatory",
      "country": "US"
    }
  ]
}