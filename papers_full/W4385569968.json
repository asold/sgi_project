{
  "title": "Making Language Models Better Reasoners with Step-Aware Verifier",
  "url": "https://openalex.org/W4385569968",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2097413118",
      "name": "Yifei Li",
      "affiliations": [
        "Microsoft (Finland)",
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2117288342",
      "name": "Zeqi Lin",
      "affiliations": [
        "Microsoft (Finland)"
      ]
    },
    {
      "id": "https://openalex.org/A2320059993",
      "name": "Shizhuo Zhang",
      "affiliations": [
        "Microsoft (Finland)"
      ]
    },
    {
      "id": "https://openalex.org/A1914230261",
      "name": "Qiang Fu",
      "affiliations": [
        "Microsoft (Finland)"
      ]
    },
    {
      "id": "https://openalex.org/A2096510293",
      "name": "Bei Chen",
      "affiliations": [
        "Microsoft (Finland)"
      ]
    },
    {
      "id": "https://openalex.org/A4212328323",
      "name": "Jian-Guang Lou",
      "affiliations": [
        "Microsoft (Finland)"
      ]
    },
    {
      "id": "https://openalex.org/A2108390110",
      "name": "Wei‐Zhu Chen",
      "affiliations": [
        "Microsoft (Finland)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4286901992",
    "https://openalex.org/W4287209352",
    "https://openalex.org/W3034643750",
    "https://openalex.org/W4281250694",
    "https://openalex.org/W3182778088",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W2970609357",
    "https://openalex.org/W2898695519",
    "https://openalex.org/W4308900200",
    "https://openalex.org/W3021649351",
    "https://openalex.org/W4226369848",
    "https://openalex.org/W3101082165",
    "https://openalex.org/W3162945537",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W4308900213",
    "https://openalex.org/W2963559137",
    "https://openalex.org/W4225603598",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4281483047",
    "https://openalex.org/W4288262459",
    "https://openalex.org/W2963829073",
    "https://openalex.org/W3212993480",
    "https://openalex.org/W2963159690",
    "https://openalex.org/W3138392969",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2983995706",
    "https://openalex.org/W3205949070",
    "https://openalex.org/W4287207937",
    "https://openalex.org/W3035438620",
    "https://openalex.org/W2892280852",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4385574286",
    "https://openalex.org/W2971107062",
    "https://openalex.org/W3035428952",
    "https://openalex.org/W2963895422",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W2962922117",
    "https://openalex.org/W4221160826",
    "https://openalex.org/W2919420119",
    "https://openalex.org/W4206778668",
    "https://openalex.org/W2996164352",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W4224316956",
    "https://openalex.org/W3034284249",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3167602185",
    "https://openalex.org/W2251935656",
    "https://openalex.org/W4385573007",
    "https://openalex.org/W3042471895",
    "https://openalex.org/W4306754402",
    "https://openalex.org/W4287393336",
    "https://openalex.org/W2970742161",
    "https://openalex.org/W4320086632",
    "https://openalex.org/W2276364082"
  ],
  "abstract": "Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 5315–5333\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nMaking Large Language Models Better Reasoners with Step-Aware Verifier\nYifei Li1,2∗, Zeqi Lin2, Shizhuo Zhang2, Qiang Fu2, Bei Chen2,\nJian-Guang Lou2, Weizhu Chen2\n1 National Key Laboratory for Multimedia Information Processing,\nSchool of Computer Science, Peking University\n2 Microsoft Corporation\n{yifeili, zeqi.lin, v-shizzhang, qifu, bei.chen, jlou, wzchen}@microsoft.com\nliyifei@stu.pku.edu.cn\nAbstract\nFew-shot learning is a challenging task that\nrequires language models to generalize from\nlimited examples. Large language models\nlike GPT-3 and PaLM have made impressive\nprogress in this area, but they still face diffi-\nculties in reasoning tasks such as GSM8K, a\nbenchmark for arithmetic problems. To im-\nprove their reasoning skills, previous work has\nproposed to guide the language model with\nprompts that elicit a series of reasoning steps\nbefore giving the final answer, achieving a sig-\nnificant improvement on GSM8K from 17.9%\nto 58.1% in problem-solving rate. In this pa-\nper, we present DIVERSE (Diverse Verifier on\nReasoning Step), a novel approach that further\nenhances the reasoning capability of language\nmodels. DIVERSE has three main components:\nfirst, it generates diverse prompts to explore dif-\nferent reasoning paths for the same question;\nsecond, it uses a verifier to filter out incorrect\nanswers based on a weighted voting scheme;\nand third, it verifies each reasoning step indi-\nvidually instead of the whole chain. We eval-\nuate DIVERSE on the latest language model\ncode-davinci-002 and show that it achieves new\nstate-of-the-art results on six of eight reasoning\nbenchmarks (e.g., GSM8K 74.4% →83.2%).\n1 Introduction\nLarge pretrained language models (PLMs) have\nshown remarkable performance on various natural\nlanguage processing tasks, either by few-shot learn-\ning with prompts (Radford et al., 2019; Le Scao\nand Rush, 2021; Jin et al., 2022) or by fine-tuning\n(Houlsby et al., 2019; Hu et al., 2021; He et al.,\n2022). However, despite the increasing size and\ncapacity of PLMs such as GPT-3 with 175B param-\neters (Brown et al., 2020) and PaLM with 540B\nparameters (Chowdhery et al., 2022), their reason-\ning abilities are still limited and often require mul-\n∗Work was done during an internship at Microsoft Re-\nsearch Asia.\nFigure 1: Our proposed method, DIVERSE (Diverse\nVerifier on Reasoning Step).\ntiple steps to produce correct answers, especially\nfor tasks involving arithmetic, commonsense, or\ninductive reasoning (Cobbe et al., 2021).\nRecent works (Wei et al., 2022; Zhou et al., 2022;\nKojima et al., 2022; Lampinen et al., 2022) have\ndemonstrated that PLMs possess some latent rea-\nsoning capabilities, but they need carefully de-\nsigned prompts to activate them. For instance, Wei\net al. (2022) proposed chain-of-thought reasoning,\nwhich inserts multi-step reasoning paths before gen-\nerating the final answers, and achieved significant\nimprovement on the GSM8K arithmetic benchmark\n(Cobbe et al., 2021). Wang et al. (2022c) further\nintroduced a voting mechanism to select the most\nconsistent answer among different reasoning paths,\nand achieved state-of-the-art results on several rea-\nsoning benchmarks using the PaLM model (Chowd-\nhery et al., 2022). Building on these successes, this\npaper continues this line of research and advances\nthe reasoning capabilities of PLMs in three aspects,\nas illustrated in Figure 1.\nFirst, we propose to increase the diversity of rea-\nsoning paths by not only sampling from a single\nprompt, but also varying the prompt itself. We hy-\npothesize that different prompts can elicit different\nways of thinking, while the correct answer should\nbe robust to these variations. Second, we propose\nto use a verifier to score the quality of each rea-\nsoning path and guide the voting mechanism. We\nargue that not all reasoning paths are equally good\n5315\nChain-Of-Thought Reasoning for GSM8K Math Word Problem\nQ: If there are 3 cars in the parking lot and 2 more\ncars arrive, how many cars are in the parking lot?\nA: There are 3 cars in the parking lot already. 2 more\narrive. Now there are 3 + 2 = 5 cars. The answer is 5.\n...\nQ: Janet’s ducks lay 16 eggs per day. She eats three\nfor breakfast every morning and bakes muffins for her\nfriends every day with four. She sells the remainder\nfor $2 per egg. How much does she make every day?\nA: She has 16 - 3 - 4 = 9 eggs left. So she makes\n2 ∗9 =18 per day. The answer is 18.\nFigure 2: Chain-of-thought reasoning for GSM8K math\nword problem. The prompt is colored black and the rea-\nsoning path produced by the language model is colored\nteal. This reasoning path contains two reasoning steps.\nor reliable, and some may contain errors or incon-\nsistencies that can be detected by the verifier. Third,\nwe propose to assign a fine-grained label to each\nstep of the reasoning path and use a step-aware\nverifier to attribute the correctness or wrongness of\nthe final answer to each step. We conjecture that\nsome steps may be correct but followed by wrong\nsteps or vice versa, and identifying these cases can\nhelp diagnose and improve the reasoning process.\nWe name our method as DIVERSE (diverse ver-\nifier on reasoning step) and evaluate it on eight\nreasoning benchmarks that require different types\nof reasoning skills. We use three OpenAI PLMs\n(davinci, text-davinci-002, and code-davinci-002)\nand compare our results with recent state-of-the-art\nmethods. We find that DIVERSE can consistently\nand significantly improve the performance of PLMs\non these tasks, and achieve new state-of-the-art re-\nsults on six of them1: GSM8K (74.4% →83.2%),\nAsDiv (81.9% →88.7%), MultiArith (99.3% →\n99.8%), SV AMP(86.6% → 87.0%), SingleEq\n(79.5% → 94.9%), and CLUTRR ( 67.0% →\n95.9%).\nOur data is publicly available at https://github.\ncom/microsoft/DiVeRSe.\n2 Diverse Verifier on Reasoning Step\nFigure 1 shows the overview of DIVERSE. The\nkey insights are three-fold: (1) leveraging diverse\nprompts to induce more diverse reasoning paths\nfrom the language models (Section 2.1); (2) train-\n1Most of the previous SOTA results were achieved by self-\nconsistency on PaLM-540B(Chowdhery et al., 2022).\ning a voting verifier to better derive the final an-\nswers from multiple reasoning paths (Section 2.2);\n(3) leveraging step correctness to further boost the\nvoting verifier (Section 2.3).\n2.1 Diverse Prompts\nTo reason effectively, it is beneficial to explore\ndiverse reasoning paths, following the idea that\n“All Roads lead to Rome ”. Wang et al. (2022c)\nproposed to generate various reasoning paths from\nlanguage models by sampling decoding. However,\ntheir method relies on a fixed set of exemplars for\nall prompts, which may introduce bias and limit\nthe diversity of the generated reasoning paths. To\naddress this issue, we randomly selectM1 different\nprompts for each question, and then sample M2\nreasoning paths for each prompt using sampling\ndecoding. This way, we obtain M = M1 ×M2\ndiverse reasoning paths for each question.2\n2.2 Voting Verifier\nVerifier. The verifier takes a question and a candi-\ndate reasoning path as input, and outputs the prob-\nability that the reasoning path leads to the correct\nanswer. We use deberta-v3-large (He et al., 2021)\nas the backbone model, with a small scalar head\nthat outputs predictions on the [CLS] token.\nTraining the verifier. For each training question,\nwe generate multiple candidate reasoning paths\nusing chain-of-thought reasoning. We regard the\nreasoning paths that match the ground truth final\nanswer as positive, and the others as negative.\nVoting Verifier. Wang et al. (2022c) use major-\nity voting to aggregate the predictions of different\nreasoning paths. This method may fail when the\nmajority of the reasoning paths are misled, while\nthe minority of the reasoning paths are reasonable.\nWe propose voting verifier, which leverages both\nvoting and verifier:\nˆy = arg max\ny\nM∑\ni=1\n1 yi=y ·f(xi,zi,yi), (1)\nwhere 1 yi=y is an indicator function that returns 1\n(or 0) if yi = y (or not), and f(·) is the probability\nproduced by the verifier.\n2.3 Step-aware Voting Verifier\nEach reasoning path consists of several steps. We\nhypothesize that not all the steps in an incorrect\n2Our main experiments use M1 = 5and M2 = 20.\n5316\nFigure 3: How step-level labels are extracted. This\nfigure shows four reasoning paths for a math word prob-\nlem: the first two are positive and the bottom two are\nnegative. The path 7 →9 →18 means that the first step\ncalculates 7, the second step calculates 9, and the third\nstep calculates the final answer 18. For the last path, the\nthird step (which calculates 8) has never occurred in any\npositive reasoning paths, thus we regard this step and\nall steps after it as negative steps.\nreasoning path are equally wrong, and some steps\nmay still be useful for reasoning. To exploit this,\nwe extend the voting verifier to a step-aware voting\nverifier by introducing an extended loss function:\nL= L0 + α·L1,\nL1 =\n|ˆD|∑\ni=1\n|Si|∑\nj=1\nBCE(labeli,j,f′(inputi,j)).\n(2)\nα is a hyperparameter to balance the original\nloss L0 and the step-level auxiliary loss L1;\nSi,1,Si,2,...,S i,|Si|are the steps in zi; labeli,j in-\ndicates whether Si,j is correct or not; f′(inputi,j)\nrepresents the probability of the positive label for\nSi,j.3\nTo obtain the step-level labels (i.e., labeli,j) for\nnegative training data with wrong answers, we de-\nsign an algorithm that compares intermediate re-\nsults among steps in positive/negative reasoning\npaths. Figure 3 illustrates this algorithm. This\nalgorithm can not only work on math word prob-\nlems, but also generalize to other reasoning tasks:\nwe use an off-the-shelf natural language inference\nmodel, roberta-large-mnli (Liu et al., 2019), to\ncheck whether two reasoning steps are semanti-\ncally equivalent or not. Given a reasoning step, if\nwe cannot find any semantically equivalent step in\n3Specifically, f′(inputi, j) is predicted from the hidden\nstate of the last token of Si,j in DEBERTA -V3-LARGE , similar\nto token classification tasks.\nthe positive reasoning paths, we label it and all the\nsubsequent steps as negative steps.\n3 Experimental Setup\n3.1 Reasoning Tasks\nArithmetic Reasoning. Following Wang et al.\n(2022c), we use AsDiv (Miao et al., 2020), Sin-\ngleEq (Koncel-Kedziorski et al., 2015), MultiArith\n(Roy and Roth, 2015), SV AMP (Patel et al., 2021),\nand GSM8K (Cobbe et al., 2021).\nCommonsense Reasoning. Following Wang\net al. (2022c), we use CommonsenseQA (Talmor\net al., 2019) and StrategyQA (Geva et al., 2021).\nInductive Reasoning. We use CLUTRR (Sinha\net al., 2019), a diagnostic benchmark for induc-\ntive reasoning, requiring inferring kinship relations\nbetween characters in short stories.\n3.2 Details\nLanguage Models. We use three OpenAI lan-\nguage models: davinci, text-davinci-002 and code-\ndavinci-002. We use the default parameters except\na temperature of 0.5 in sampling.\nExemplars. For arithmetic/commonsense/induc-\ntive reasoning, each prompt contains 5/7/7 exem-\nplars. For DIVERSE, each question has 5 differ-\nent prompts, and 20 reasoning paths are sampled\nfrom the language model for each prompt. For\narithmetic reasoning, the exemplars are randomly\nsampled from the training dataset of GSM8K; for\nCLUTRR, the exemplars are sampled from its train-\ning dataset, with reasoning paths synthesized by\nhandcraft rules (detailed settings for CLUTRR are\nlisted in Appendix D); for StrategyQA and Com-\nmonsenseQA, their original datasets do not contain\nenough exemplars with well-annotated reasoning\npaths, so we construct 1,000 pseudo exemplars by\n“self-teaching” (the approach and the noise issue are\ndiscussed in Appendix B) from “seed” exemplars\nprovided by Wei et al. (2022).\nTraining Datasets. For each task, we sample\n1,000 ⟨question,answer⟩pairs from the training\ndataset to train the verifier.\nVerifier. We fine-tunedeberta-v3-large (He et al.,\n2021) with learning rate 1 ×10−5 and batch size\n128. For the step-aware verifier, we select the best\nαamong 0.0/0.1/0.2/0.3.\n5317\nMethod GSM8K AsDiv MultiArith SV AMP SingleEq CommonsenseQA StrategyQA CLUTRR\nPrevious SOTA (Fine-tuning) 57 a 75.3b 60.5c 57.4d 32.5e 91.2f 73.9g 67.0 h\n9–12 year olds (Cobbe et al., 2021) 60 - - - - - - -\nLaMDA 137B:\nGreedy Decode 17.1 49.0 51.8 38.9 56.6 57.9 65.4 -\nSelf-Consistency 27.7 58.2 75.7 53.3 - 63.1 67.8 -\nPaLM 540B:\nGreedy Decode 56.5 74.0 94.7 79.0 79.5 79.0 75.3 -\nSelf-Consistency 74.4 81.9 99.3 86.6 - 80.7 81.6 -\nGPT-3 davinci (175B):\nGreedy Decode 8.7 31.4 31.4 21.2 38.2 48.2 59.2 33.6\nSelf-Consistency 18.9 52.8 68.6 44.6 59.6 57.4 65.6 42.5\nDIVERSE 30.9 (+12.0) 57.6 (+4.8) 87.6 (+19.0) 46.9 (+2.3) 65.1 (+5.5) 75.0 (+17.6) 66.3 (+0.7) 92.5 (+50.0)\ntext-davinci-002:\nGreedy Decode 37.1 60.8 70.7 60.0 73.3 65.5 57.8 32.4\nSelf-Consistency 58.2 76.9 88.4 78.2 87.2 72.9 69.8 34.9\nDIVERSE 70.2 (+12.0) 83.5 (+6.6) 96.4 (+8.0) 82.7 (+4.5) 86.5 (-0.7) 79.2 (+6.3) 74.8 (+5.0) 93.8 (+58.9)\ncode-davinci-002:\nGreedy Decode 55.3 75.5 88.8 70.5 87.5 73.4 72.0 32.9\nSelf-Consistency 76.7 86.2 98.6 85.8 93.7 77.3 77.6 35.6\nDIVERSE 82.3 (+5.6) 88.7 (+1.5) 99.8 (+1.2) 87.0 (+1.2) 94.9 (+1.2) 79.9 (+2.6) 78.6 (+1.0) 95.9 (+60.3)\nTable 1: The comparison ofDIVERSE, Greedy Decode and Self-Consistency. The previous SOTA results (fine-tuned\non non-gigantic pretrained transformers) are: a: Cobbe et al. (2021), b: Miao et al. (2020), c: Roy and Roth (2015),\nd: Pi et al. (2022), e: Hu et al. (2019a), f: Xu et al. (2021), g: Chowdhery et al. (2022), h: Sinha et al. (2019). The\nparameter number of either text-davinci-002 or code-davinci-002 is hidden to us.\nQuestion Self-Consistency DIVERSE\n[GSM8K]Mark decides to buy packs\nof old magic cards and open them to\nsell. He buys 3 packs for $1800 each.\nHe gets 1 card that is worth $4000\nand another card worth $1000. There\nare 30 more cards worth an average\nof $50 each. How much money profit\ndid he make?\nMark bought 3 packs for $1800 each for a total of\n$1800 * 3 = 5400(68.1%). He got 1 card worth\n$4000 and another card worth $1000 for a total of\n$4000 + $1000 = $5000(72.1%). There were 30\nmore cards worth an average of $50 each for a total\nof $50 * 30 = 1500(63.7%). Mark made a profit\nof $5000 + $1500 = 6500(40.5%). The answer is\n6500.\nAnswer: 6500 (Wrong) Score:62.3%\nThe cheaper cards are worth a total of 30 * 50 =\n1500 (84.4%). So all the cards together are worth\n1500 + 4000 + 1000 = 6500(83.3%). The cost\nfor the cards was 1800 * 3 = 5400(80.1%). So\nhe made a profit of 6500 - 5400 = 1100(70.0%).\nThe answer is 1100.\nAnswer: 1100 (Correct) Score:91.2%\nTable 2: A GSM8K example (code-davinci-002) with step-level scores given by the step-aware verifier. The scores\ncan not only improve the performance but also help the understanding of where the reasoning paths start to be\nincorrect.\n4 Main Results\nTable 1 shows the overall experimental results. We\nmainly compare DIVERSE with two baselines: (1)\ngreedily decoding a single reasoning path (Wei\net al., 2022), referred to as Greedy Decode; (2)\nsampling 100 reasoning paths, then select the final\nanswer via majority voting (Wang et al., 2022c),\nreferred to as Self-Consistency.\n4.1 Effectiveness\nExperimental results clearly demonstrate that DI-\nVERSE can bring significant and consistent im-\nprovements over recent strong baselines. The im-\nprovements are across different models ( davinci,\ntext-davinci-002 and code-davinci-002) as well as\ndifferent reasoning skills (eight tasks in three rea-\nsoning skills). Taking GSM8K as an example, com-\npared to Greedy Decoding and Self-Consistency,\nDIVERSE brings improvements of 22.2%/12.0%\non davinci, 33.1%/12.0% on text-davinci-002, and\n27.0%/5.6% on code-davinci-002. Compared to\nSelf-Consistency, DIVERSE achieves average im-\nprovements of 5.6%/5.1%/54.3% on the three rea-\nsoning skills, respectively.\n4.2 Comparing to Previous SOTAs\nIn Table 1, we also compare DIVERSE with: (1)\nprevious SOTA results based on fine-tuning; (2)\nrecent SOTA results (Wei et al., 2022) based on\nPaLM (Chowdhery et al., 2022), a gigantic lan-\nguage model with 540 billion parameters.4\nOn all the five arithmetic reasoning tasks, DI-\nVERSE (with code-davinci-002) achieves new\nSOTA results, with an average improvement of\n6.2%. On the two commonsense reasoning tasks,\nthe performance of DIVERSE is slightly lower\n(−1.9%) than that of PaLM-based self-consistency.\nWe speculate that the reason might be: these two\ncommonsense reasoning tasks are multiple-choice\ntasks rather than open-ended generation tasks, re-\n4DIVERSE can also be applied to PaLM, but PaLM is not\npublicly available.\n5318\nMethod GSM8K CQA CLUTRR\ndavinci:\nM1 = 1, M2 = 100 18.9 57.4 42.5\nM1 = 5, M2 = 20 21.3 57.5 45.9\ntext-davinci-002:\nM1 = 1, M2 = 100 58.2 72.9 34.9\nM1 = 5, M2 = 20 61.3 77.3 35.6\ncode-davinci-002:\nM1 = 1, M2 = 100 76.7 77.3 35.6\nM1 = 5, M2 = 20 80.0 78.8 43.8\nTable 3: The effectiveness of diverse prompts (⟨5,20⟩)\ncompared to pure sampling decoding (Wang et al.,\n2022c), under majority voting.\n⟨M 1 , M2 ⟩ GSM8K\nM 1 = 1, M2 = 100 76.7\nM 1 = 5, M2 = 20 80.0\nM 1 = 10 , M2 = 10 79.8\nM 1 = 100 , M2 = 1 73.0\nTable 4: GSM8K majority voting results for different\n⟨M1,M2⟩settings on code-davinci-002.\nsulting in more false-positive exemplars in the\npseudo exemplar base (Details will be discussed in\nSection B.2). Regarding inductive reasoning, DI-\nVERSE achieves a surprisingly good performance\nof 95.9% on the CLUTRR task, outperforming\n(+28.9%) previous SOTA result with fine-tuning\n(Sinha et al., 2019).5\n5 Case Study\nTable 2 shows an example of step-level scores given\nby the step-aware verifier. Steps in the correct\nreasoning path have relatively high scores, while\nthe scores in the wrong reasoning path show where\nthe path starts to be wrong. This indicates that\nbesides improving the performance, the step-aware\nverifier can also bring interpretability to show the\nstep-level correctness. We also show some extra\nexamples of majority-voting in Table 10.\n6 Analysis\nWe also conduct ablation experiments and analysis\nto investigate the keys to the success of DIVERSE.\n5Sinha et al. (2019) also introduced a method with 100%\naccuracy. We do not take it into the comparison, as this method\nrequires a domain-specific system with complicated rules to\nextract a knowledge graph for each input text.\ntext-002 code-002\n30\n40\n50\n60\n/uni000003ef/uni000003f1/uni00000358/uni000003f2/uni00000003/uni0000037e/uni0000043d/uni000003ee/uni00000358/uni000003ef/uni0000037f\n/uni000003f1/uni000003ec/uni00000358/uni000003ee/uni00000003/uni0000037e/uni0000043d/uni000003ef/uni00000358/uni000003f4/uni0000037f\nReasoning Paths\nDiverse Prompts\nFixed Prompts\ntext-002 code-002\n10\n15\n20\n/uni000003ed/uni000003f0/uni00000358/uni000003ef/uni00000003/uni0000037e/uni0000043d/uni000003ec/uni00000358/uni000003ed/uni0000037f\n/uni000003ed/uni000003f4/uni00000358/uni000003ed/uni00000003/uni0000037e/uni0000043d/uni000003ed/uni00000358/uni000003ed/uni0000037f\nFinal Answers\nDiverse Prompts\nFixed Prompts\nFigure 4: Diverse prompts increase the diversity of\nGSM8K reasoning paths and their final answers. This\nis beneficial for the voting verifier. Left: the average\nnumber of distinct reasoning paths per question (we\nconsider two reasoning paths to be the same if they have\nthe same intermediate result chain as shown in Figure\n3). Right: the average number of distinct final answers\nper question.\n6.1 The Effectiveness of Diverse Prompts\nBy diversifying both prompts and reasoning paths\n(⟨M1 = 5,M2 = 20⟩), we consistently improve\nperformance over the sampling decoding approach\n(⟨M1 = 1,M2 = 100⟩) of Wang et al. (2022c), as\nshown in Table 3. Both methods use majority vot-\ning. Table 4 further reveals that neither only using\ndiverse prompts nor only using sampling is optimal.\nIn other words, the best performance is achieved by\ncombining diverse prompts and sampling. More-\nover, Figure 4 demonstrates that diverse prompts\nlead to more diverse reasoning paths. We hypoth-\nesize that this diversity contributes to the perfor-\nmance improvement by: (1) making correct results\nmore distinguishable from varied errors during in-\nference; and (2) providing more diverse negative\nsamples for enhancing the verifier’s generalizabil-\nity during training.\n6.2 The Effectiveness of Voting Verifier\nWe compare three algorithms to conclude the agree-\nment from diverse reasoning paths: majority vot-\ning, verifier, and voting verifier. Table 5 shows\nthe results. Compared to majority voting, our vot-\ning verifier can significantly and consistently boost\nreasoning performance across different tasks and\ndifferent language models. Verifier without voting\noften outperforms majority voting, but extending it\nto voting verifier can further boost the performance.\n5319\nMethod GSM8K CQA CLUTRR\ndavinci:\nV oting 21.3 57.4 45.9\nVerifier 27.0 74.1 93.2\nV oting Verifier 30.6 75.0 92.5\ntext-davinci-002:\nV oting 61.3 77.3 35.6\nVerifier 62.7 77.9 93.8\nV oting Verifier 68.9 79.2 93.8\ncode-davinci-002:\nV oting 80.0 75.4 43.8\nVerifier 65.9 78.8 95.9\nV oting Verifier 82.3 78.8 95.9\nTable 5: The effectiveness of voting verifier. All exepri-\nments in this table use ⟨M1,M2⟩= ⟨5,20⟩.\nRandom\nSelected\nVerifier Step\nVerifier\n0\n15\n30\n45 /uni000003f0/uni000003ed\n/uni000003ef/uni000003ed\n/uni000003ee/uni000003ec\n(a) The number of correct rea-\nsoning paths containing re-\ndundant steps.\nStep Verifier (33%)\nEqual (50%)\nVerifier (17%)\n(b) With the step-aware mech-\nanism, incorrect paths contain\nmore correct steps.\nFigure 5: Human evaluation on GSM8K shows the\neffectiveness of the step-aware mechanism for verifier.\n6.3 The Effectiveness of Step-aware Verifier\nWe evaluate the impact of incorporating step-level\ninformation into the voting verifier of DIVERSE.\nTable 6 shows the performance of DIVERSE with\nand without the step-aware mechanism on both the\nGSM8K and the CommonsenseQA datasets. We\nfind that using the step-aware verifier improves the\nperformance in most of the experiments. The only\nexception is code-davinci-002 on GSM8K, where\nthe step-aware verifier slightly lowers the perfor-\nmance. We hypothesize that code-davinci-002 is\nmore capable of generating high-quality reasoning\npaths, and thus does not benefit much from the\nstep-level information.\nDetailed Human Evaluation of Reasoning Steps.\nWe further analyze the quality of generated rea-\nsoning steps, by asking human annotators to judge\nFormulation Error (95%)\nMissing Steps (2%)\nCalculation Error (2%)\nNumber Hallucination (1%)\nFigure 6: The distribution of error types in incorrect\nreasoning steps.\nwhether the GSM8K reasoning steps produced by\nDIVERSE (with/without step-aware mechanism)\nare good or not. Here “good” means not only cor-\nrect formulas and calculation results but also tex-\ntual fluency and logical coherence.\nWe further examine the quality of the reasoning\nsteps generated by DIVERSE (with/without step-\naware mechanism) for GSM8K, by asking human\nannotators to rate them based on correctness, flu-\nency, and coherence. For each test question, we\ncompare three reasoning paths produced by code-\ndavinci-002: the one with the highest verifier score,\nthe one with the highest step-aware verifier score,\nand a randomly chosen one. The annotators (master\nstudents) label any incorrect or unsatisfactory rea-\nsoning steps in each path (single-blind) and explain\nwhy. We collect annotations for 200 test questions,\nhalf of which have correct final answers from all\nthree paths, and half of which have incorrect final\nanswers from all three paths.\nWe find that all the reasoning paths with correct\nfinal answers are also correct in every interme-\ndiate step, which shows that code-davinci-002 can\nreliably generate accurate reasoning steps, not just\nlucky guesses. However, we also find that many\nof the correct reasoning paths have unnecessary\nsteps. Figure 5(a) shows that 40% of the random\npaths have redundant steps, and the verifier can\nlower this percentage to 31%. We also find that the\nstep-aware verifier can further eliminate redun-\ndant reasoning steps from 31% to 20%.\nFurthermore, for the incorrect reasoning paths, we\nfind that the step-aware mechanism helps pro-\nduce more correct steps before making mistakes.\nFor each failed test question, we compare the num-\nber of correct steps in the path with the highest ver-\nifier score and the path with the highest step-aware\nverifier score (by human evaluation). Figure 5(b)\n5320\nGSM8K CommonsenseQA\ndavinci:\nDIVERSE (without step) 30.6 75.0\nDIVERSE (with step) 30.9 76.0\ntext-davinci-002:\nDIVERSE (without step) 68.9 79.2\nDIVERSE (with step) 70.2 79.8\ncode-davinci-002:\nDIVERSE (without step) 82.3 78.8\nDIVERSE (with step) 81.5 79.9\nTable 6: The effectiveness of step-aware voting verifier,\nwith ⟨M1,M2⟩= ⟨5,20⟩.\nshows that for 33%/17% of the failed test cases,\nthe step-aware verifier generates more/fewer cor-\nrect steps than the verifier without the step-aware\nmechanism.\nStep Error Types. Figure 6 shows the distribu-\ntion of error types in the incorrect reasoning steps.\nWe see that 95% of the errors are caused by incor-\nrect formulations (i.e., using wrong intermediate\nresults or operators and generating invalid formu-\nlas, which lead to incorrect answers). We also see\nthat, although code-davinci-002 often makes divi-\nsion calculation errors (e.g., 10/3 = 3), both the\nverifier and the step-aware verifier can effectively\nassign low scores to such paths, thus improving the\nperformance.\n6.4 How Many Diverse Outputs Do We Need?\nFigure 7 shows the accuracy at different M values,\nwhere M is the number of reasoning paths sam-\npled from the 100 generated paths for each ques-\ntion. We observe that: (1) the accuracy increases\nwith more reasoning paths, but the improvement\nbecomes marginal at M ≥50; (2) DIVERSE out-\nperforms self-consistency significantly and consis-\ntently at different M values.\n6.5 How Many Training Data Do We Need?\nDIVERSE requires a dataset with reasoning paths\nfor training the verifier. Figure 8 shows how the\nsize of this dataset affects the performance. We\nobserve that: the performance is only reduced by\nabout 2%, even if the size of training data is cut by\n75% (from 1,000 to 250). With the same reasoning\npaths, voting verifier performs better than majority\nvoting, while verifier without voting causes signifi-\ncant performance drops.\n50 100\n#Reasoning Paths\n15\n20\n25\n30Solve Rate (%)\ndavinci\n50 100\n#Reasoning Paths\n50\n55\n60\n65\ntext-davinci-002\n50 100\n#Reasoning Paths\n67.5\n70.0\n72.5\n75.0\n77.5\n80.0\n82.5\ncode-davinci-002\nDiVeRSe Self-Consistency\nGSM8K\nFigure 7: GSM8K accuracy at different M values (how\nmany reasoning paths are used for each question).\n6.6 The Impact of the Number of Exemplars\nWe conduct experiments for k = 3/5/8 (kis the\nnumber of exemplars used in each prompt) on\nGSM8K. Figure 9 shows the results. We observe\nthat: using 8 exemplars in each prompt can further\nboost the accuracy of GSM8K to 83.2%.\n7 Related Work\nReasoning Skills. Researchers in the literature\nhave proposed many benchmarks requiring various\nreasoning skills, including commonsense reasoning\n(Zellers et al., 2018; Talmor et al., 2019; Bhaga-\nvatula et al., 2019; Geva et al., 2021) numerical\nreasoning (Dua et al., 2019), multi-hop reasoning\n(Yang et al., 2018), arithmetic reasoning (Koncel-\nKedziorski et al., 2015; Roy and Roth, 2015; Miao\net al., 2020; Patel et al., 2021; Cobbe et al., 2021),\nlogical reasoning (Liu et al., 2020; Yu et al., 2020),\ninductive reasoning (Sinha et al., 2019) and tabular\nreasoning (Chen et al., 2020; Zhu et al., 2021).\nReasoning with Symbolic Systems. Much re-\nsearch in the literature enhances the reasoning\ncapabilities of machine learning systems by ex-\nploiting symbolic systems, including knowledge\ngraphs (Mihaylov and Frank, 2018; Bauer et al.,\n2018; Kundu et al., 2019; Wang et al., 2019; Lin\net al., 2019; Ding et al., 2019; Feng et al., 2020;\nWang et al., 2022b), or question taxonomies (Dua\net al., 2019; Andor et al., 2019; Hu et al., 2019b;\nWang et al., 2022a). Although these methods work\nwell on specific benchmarks, they usually require\ndomain-specific designs and human efforts, thus\nlimiting the generalizability.\nReasoning via Language Models. This line of\nwork aims to address reasoning tasks in a gen-\neral sequence-to-sequence manner, empowered by\nreasoning-aware pre-training or fine-tuning of lan-\nguage models. For example, Deng et al. (2021)\n5321\n0 250 500 1000\nSampled (or Generated) Training Data Size\n50\n55\n60\n65\n70\n75\n80\nSolve Rate (%)\nGSM8K\nVoting Verifier Voting Verifier\nFigure 8: DIVERSE performance (code-davinci-002)\non GSM8K with different sizes of the training dataset\n(without labeled reasoning paths).\nproposed to train the language model with crawled\ndata from the internet; Asai and Hajishirzi (2020)\nproposed a logic-guided data augmentation method\nto pre-train the language model; Shen et al. (2021);\nCobbe et al. (2021) proposed to train a verifier to\nrank solutions sampled from fine-tuned language\nmodels; Geva et al. (2020); Yoran et al. (2022);\nCampagna et al. (2020); Wang et al. (2022a) pro-\nposed to equip language models with reasoning\nabilities by generating training examples with\nhuman-designed templates; Pi et al. (2022) pro-\nposed to inject reasoning capabilities into language\nmodels by continual pre-training on program exe-\ncution data.\nReasoning via Prompting Gigantic Language\nModels. Gigantic language models like GPT-3\n(Brown et al., 2020) have demonstrated impressive\nfew-shot learning capabilities in many tasks and\nhave attracted many research interests on making\ngigantic language models better few-shot learners\n(Zhao et al., 2021; Holtzman et al., 2021; Min et al.,\n2021; Liu et al., 2022; Lu et al., 2021; Rubin et al.,\n2021; Min et al., 2022). However, these methods\nstruggle to address tasks requiring reasoning skills.\nTo mitigate this, recently there is a line of research\nthat focuses on unleashing the reasoning capabili-\nties of gigantic language models via better prompt-\ning strategies. Wei et al. (2022) proposed chain-\nof-thought reasoning, of which the key insight is\nthe insertion of multi-step reasoning paths before\ngenerating the final answers; Wang et al. (2022c)\nproposed to improve chain-of-thought reasoning\nvia self-consistency, of which the key insight is\nto conclude the most consistent answer from dif-\nferent reasoning paths sampled from the language\nmodel; Zhou et al. (2022); Creswell et al. (2022)\n3 5 8\nNumber of Exemplars in One Prompt\n50\n55\n60\n65\n70\n75\n80\n85\nSolve Rate (%)\n51.8\n55 56\n79.5 80 79.6\n82 82.3 83.2\nGSM8K\nGreedy Decode DivPrompts+Voting DiVeRSe\nFigure 9: DIVERSE performance (code-davinci-002)\non GSM8K when each prompt contains 3/5/8 exem-\nplars.\nproposed to leverage gigantic language models to\ndecompose questions into sub-questions, thereby\naddressing them in an iterative manner; Kojima\net al. (2022) proposed that gigantic language mod-\nels can even be good zero-shot reasoners, by design-\ning prompts that can induce language models to do\nreasoning step-by-step; Lampinen et al. (2022) pro-\nposed building a prompt by selecting examples and\nexplanations together, thus substantially improving\nperformance over selecting examples alone. De-\nspite their great successes, these works come with\ntheir limitations. This paper is a continuation of\nthis line of research, focusing on diverse verifier on\nreasoning steps.\n8 Conclusion and Future Work\nIn this paper, we present DIVERSE, a novel and\ngeneral method to enhance the reasoning abilities\nof large language models. Our method builds on\nthe idea of prompting language models with multi-\nstep reasoning paths, but introduces three key in-\nnovations: diverse prompts, voting verifier, and\nstepwise verifier. The latter is especially novel and\neffective, as it verifies each reasoning step sepa-\nrately and we provides a detailed analysis of the\nmodel’s behavior in each step. We demonstrate the\nsuperiority of DIVERSE through extensive experi-\nments. For instance, using code-davinci-002, our\nmethod achieves state-of-the-art performance on\nmost reasoning tasks, surpassing the 540B PaLM\nmodel with previous prompting techniques.\nThere are many directions for our future work. (1)\nAs discussed in Appendix B.2, we will continue to\ninvestigate how to reduce or recognize false posi-\ntive pseudo exemplars. (2) We plan to investigate\nmechanisms to produce better diverse prompts than\n5322\nsimple sampling. (3) We will extend DIVERSE to\nother tasks and continue to design better prompting\ntechniques to elicit the power of gigantic language\nmodels.\n9 Limitations\nComputing Resources. Despite the surprising\nperformance it achieves, our framework needs to\nbe applied to large language models like GPT-\n3 or PaLM. Inference with these models costs\nmore time and budgets than fine-tuning models\nlike RoBERTa (Liu et al., 2019).\nFaithfulness. Although DIVERSE can signifi-\ncantly improve the accuracy of final answers, we\nstill cannot guarantee that the reasoning paths pro-\nduced by the language models are 100 percent faith-\nful. This is the key challenge and future direction\nfor this line of research (chain-of-thought reason-\ning).\nMore Training Data. DIVERSE needs more la-\nbeled data with well-annotated reasoning paths\nto construct diverse prompts, and it also needs a\ntraining dataset for supervising the verifier. How-\never, from another point of view, this limitation\ncan also be regarded as a contribution that studies\nhow chain-of-thought reasoning can be further im-\nproved if we have more training data than just a\nfew exemplars.\nHuman Evaluation of Reasoning Steps. We use\nhuman evaluation to measure the quality of the in-\ntermediate steps in reasoning paths since few cur-\nrent works provide reliable frameworks to evaluate\nthe quality of reasoning steps.\nReferences\nDaniel Andor, Luheng He, Kenton Lee, and Emily\nPitler. 2019. Giving BERT a calculator: Finding\noperations and arguments with reading compre-\nhension. In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 5947–5952, Hong Kong, China.\nAssociation for Computational Linguistics.\nAkari Asai and Hannaneh Hajishirzi. 2020. Logic-\nguided data augmentation and regularization for\nconsistent question answering. In Proceedings of\nthe 58th Annual Meeting of the Association for\nComputational Linguistics, pages 5642–5650,\nOnline. Association for Computational Linguis-\ntics.\nLisa Bauer, Yicheng Wang, and Mohit Bansal.\n2018. Commonsense for generative multi-hop\nquestion answering tasks. In Proceedings of the\n2018 Conference on Empirical Methods in Nat-\nural Language Processing , pages 4220–4230,\nBrussels, Belgium. Association for Computa-\ntional Linguistics.\nChandra Bhagavatula, Ronan Le Bras, Chaitanya\nMalaviya, Keisuke Sakaguchi, Ari Holtzman,\nHannah Rashkin, Doug Downey, Scott Wen-tau\nYih, and Yejin Choi. 2019. Abductive common-\nsense reasoning.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sas-\ntry, Amanda Askell, et al. 2020. Language mod-\nels are few-shot learners. Advances in neural\ninformation processing systems, 33:1877–1901.\nGiovanni Campagna, Agata Foryciarz, Mehrad\nMoradshahi, and Monica Lam. 2020. Zero-shot\ntransfer learning with synthesized data for multi-\ndomain dialogue state tracking. In Proceedings\nof the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 122–132,\nOnline. Association for Computational Linguis-\ntics.\nWenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan\nXiong, Hong Wang, and William Yang Wang.\n2020. HybridQA: A dataset of multi-hop ques-\ntion answering over tabular and textual data. In\nFindings of the Association for Computational\nLinguistics: EMNLP 2020 , pages 1026–1036,\nOnline. Association for Computational Linguis-\ntics.\nAakanksha Chowdhery, Sharan Narang, Jacob De-\nvlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung,\nCharles Sutton, Sebastian Gehrmann, et al. 2022.\nPalm: Scaling language modeling with pathways.\narXiv preprint arXiv:2204.02311.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavar-\nian, Jacob Hilton, Reiichiro Nakano, Christo-\npher Hesse, and John Schulman. 2021. Training\nverifiers to solve math word problems. arXiv\npreprint arXiv:2110.14168.\n5323\nAntonia Creswell, Murray Shanahan, and Irina Hig-\ngins. 2022. Selection-inference: Exploiting large\nlanguage models for interpretable logical reason-\ning.\nXiang Deng, Yu Su, Alyssa Lees, You Wu, Cong\nYu, and Huan Sun. 2021. ReasonBERT: Pre-\ntrained to reason with distant supervision. InPro-\nceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, pages\n6112–6127, Online and Punta Cana, Dominican\nRepublic. Association for Computational Lin-\nguistics.\nMing Ding, Chang Zhou, Qibin Chen, Hongxia\nYang, and Jie Tang. 2019. Cognitive graph\nfor multi-hop reading comprehension at scale.\nIn Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics,\npages 2694–2703, Florence, Italy. Association\nfor Computational Linguistics.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi,\nGabriel Stanovsky, Sameer Singh, and Matt\nGardner. 2019. DROP: A reading comprehen-\nsion benchmark requiring discrete reasoning\nover paragraphs. In Proceedings of the 2019\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long\nand Short Papers), pages 2368–2378, Minneapo-\nlis, Minnesota. Association for Computational\nLinguistics.\nYanlin Feng, Xinyue Chen, Bill Yuchen Lin,\nPeifeng Wang, Jun Yan, and Xiang Ren. 2020.\nScalable multi-hop relational reasoning for\nknowledge-aware question answering.\nMor Geva, Ankit Gupta, and Jonathan Berant. 2020.\nInjecting numerical reasoning skills into lan-\nguage models. In Proceedings of the 58th Annual\nMeeting of the Association for Computational\nLinguistics, pages 946–958, Online. Association\nfor Computational Linguistics.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar\nKhot, Dan Roth, and Jonathan Berant. 2021. Did\naristotle use a laptop? a question answering\nbenchmark with implicit reasoning strategies.\nTransactions of the Association for Computa-\ntional Linguistics, 9:346–361.\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor\nBerg-Kirkpatrick, and Graham Neubig. 2022.\nTowards a unified view of parameter-efficient\ntransfer learning. In International Conference\non Learning Representations.\nPengcheng He, Xiaodong Liu, Jianfeng Gao,\nand Weizhu Chen. 2021. Deberta: Decoding-\nenhanced bert with disentangled attention. In\nInternational Conference on Learning Represen-\ntations.\nAri Holtzman, Peter West, Vered Shwartz, Yejin\nChoi, and Luke Zettlemoyer. 2021. Surface form\ncompetition: Why the highest probability answer\nisn’t always right.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzeb-\nski, Bruna Morrone, Quentin De Laroussilhe,\nAndrea Gesmundo, Mona Attariyan, and Syl-\nvain Gelly. 2019. Parameter-efficient transfer\nlearning for nlp. In International Conference on\nMachine Learning, pages 2790–2799. PMLR.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. 2021. Lora: Low-rank adap-\ntation of large language models. arXiv preprint\narXiv:2106.09685.\nMinghao Hu, Yuxing Peng, Zhen Huang, and\nDongsheng Li. 2019a. A multi-type multi-span\nnetwork for reading comprehension that requires\ndiscrete reasoning. In Proceedings of the 2019\nConference on Empirical Methods in Natural\nLanguage Processing and the 9th International\nJoint Conference on Natural Language Process-\ning (EMNLP-IJCNLP), pages 1596–1606.\nMinghao Hu, Yuxing Peng, Zhen Huang, and\nDongsheng Li. 2019b. A multi-type multi-span\nnetwork for reading comprehension that requires\ndiscrete reasoning. In Proceedings of the 2019\nConference on Empirical Methods in Natural\nLanguage Processing and the 9th International\nJoint Conference on Natural Language Process-\ning (EMNLP-IJCNLP), pages 1596–1606, Hong\nKong, China. Association for Computational\nLinguistics.\nWoojeong Jin, Yu Cheng, Yelong Shen, Weizhu\nChen, and Xiang Ren. 2022. A good prompt\nis worth millions of parameters: Low-resource\nprompt-based learning for vision-language mod-\nels. In Proceedings of the 60th Annual Meeting\n5324\nof the Association for Computational Linguis-\ntics (Volume 1: Long Papers), pages 2763–2775,\nDublin, Ireland. Association for Computational\nLinguistics.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid,\nYutaka Matsuo, and Yusuke Iwasawa. 2022.\nLarge language models are zero-shot reasoners.\nRik Koncel-Kedziorski, Hannaneh Hajishirzi,\nAshish Sabharwal, Oren Etzioni, and Siena Du-\nmas Ang. 2015. Parsing algebraic word prob-\nlems into equations. Transactions of the Associ-\nation for Computational Linguistics, 3:585–597.\nSouvik Kundu, Tushar Khot, Ashish Sabharwal,\nand Peter Clark. 2019. Exploiting explicit paths\nfor multi-hop reading comprehension. In Pro-\nceedings of the 57th Annual Meeting of the As-\nsociation for Computational Linguistics, pages\n2737–2747, Florence, Italy. Association for\nComputational Linguistics.\nAndrew K Lampinen, Ishita Dasgupta,\nStephanie CY Chan, Kory Matthewson,\nMichael Henry Tessler, Antonia Creswell,\nJames L McClelland, Jane X Wang, and\nFelix Hill. 2022. Can language models learn\nfrom explanations in context? arXiv preprint\narXiv:2204.02329.\nTeven Le Scao and Alexander M Rush. 2021. How\nmany data points is a prompt worth? In Pro-\nceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Com-\nputational Linguistics: Human Language Tech-\nnologies, pages 2627–2636.\nBill Yuchen Lin, Xinyue Chen, Jamin Chen, and\nXiang Ren. 2019. KagNet: Knowledge-aware\ngraph networks for commonsense reasoning. In\nProceedings of the 2019 Conference on Em-\npirical Methods in Natural Language Process-\ning and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2829–2839, Hong Kong, China.\nAssociation for Computational Linguistics.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill\nDolan, Lawrence Carin, and Weizhu Chen. 2022.\nWhat makes good in-context examples for GPT-\n3? In Proceedings of Deep Learning Inside Out\n(DeeLIO 2022): The 3rd Workshop on Knowl-\nedge Extraction and Integration for Deep Learn-\ning Architectures, pages 100–114, Dublin, Ire-\nland and Online. Association for Computational\nLinguistics.\nJian Liu, Leyang Cui, Hanmeng Liu, Dandan\nHuang, Yile Wang, and Yue Zhang. 2020.\nLogiqa: A challenge dataset for machine reading\ncomprehension with logical reasoning.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei\nDu, Mandar Joshi, Danqi Chen, Omer Levy,\nMike Lewis, Luke Zettlemoyer, and Veselin\nStoyanov. 2019. Roberta: A robustly opti-\nmized bert pretraining approach. arXiv preprint\narXiv:1907.11692.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian\nRiedel, and Pontus Stenetorp. 2021. Fantasti-\ncally ordered prompts and where to find them:\nOvercoming few-shot prompt order sensitivity.\nShen-Yun Miao, Chao-Chun Liang, and Keh-Yih\nSu. 2020. A diverse corpus for evaluating and\ndeveloping english math word problem solvers.\nIn Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics,\npages 975–984.\nTodor Mihaylov and Anette Frank. 2018. Knowl-\nedgeable reader: Enhancing cloze-style read-\ning comprehension with external commonsense\nknowledge. In Proceedings of the 56th Annual\nMeeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 821–\n832, Melbourne, Australia. Association for Com-\nputational Linguistics.\nSewon Min, Mike Lewis, Luke Zettlemoyer, and\nHannaneh Hajishirzi. 2021. Metaicl: Learning\nto learn in context.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel\nArtetxe, Mike Lewis, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2022. Rethinking the role of\ndemonstrations: What makes in-context learning\nwork?\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\n2021. Are nlp models really able to solve simple\nmath word problems? In Proceedings of the\n2021 Conference of the North American Chap-\nter of the Association for Computational Lin-\nguistics: Human Language Technologies, pages\n2080–2094.\n5325\nXinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi,\nZeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou,\nand Weizhu Chen. 2022. Reasoning like program\nexecutors. arXiv preprint arXiv:2201.11473.\nAlec Radford, Jeffrey Wu, Rewon Child, David\nLuan, Dario Amodei, Ilya Sutskever, et al. 2019.\nLanguage models are unsupervised multitask\nlearners. OpenAI blog, 1(8):9.\nSubhro Roy and Dan Roth. 2015. Solving general\narithmetic word problems. In Proceedings of the\n2015 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1743–1752.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\n2021. Learning to retrieve prompts for in-context\nlearning.\nJianhao Shen, Yichun Yin, Lin Li, Lifeng Shang,\nXin Jiang, Ming Zhang, and Qun Liu. 2021. Gen-\nerate & rank: A multi-task framework for math\nword problems. In Findings of the Association\nfor Computational Linguistics: EMNLP 2021 ,\npages 2269–2279.\nKoustuv Sinha, Shagun Sodhani, Jin Dong, Joelle\nPineau, and William L Hamilton. 2019. Clutrr:\nA diagnostic benchmark for inductive reason-\ning from text. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Lan-\nguage Processing and the 9th International Joint\nConference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4506–4515.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie,\nand Jonathan Berant. 2019. Commonsenseqa: A\nquestion answering challenge targeting common-\nsense knowledge. In Proceedings of the 2019\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long\nand Short Papers), pages 4149–4158.\nSiyuan Wang, Wanjun Zhong, Duyu Tang,\nZhongyu Wei, Zhihao Fan, Daxin Jiang, Ming\nZhou, and Nan Duan. 2022a. Logic-driven con-\ntext extension and data augmentation for logical\nreasoning of text. In Findings of the Associa-\ntion for Computational Linguistics: ACL 2022,\npages 1619–1629, Dublin, Ireland. Association\nfor Computational Linguistics.\nXiaoyan Wang, edu Kapanipathi, Ryan Musa,\nMo Yu, Kartik Talamadupula, Ibrahim Abde-\nlaziz, Maria Chang, Achille Fokoue, Bassem\nMakni, Nicholas Mattei, and Michael Witbrock.\n2019. Improving natural language inference\nusing external knowledge in the science ques-\ntions domain. In Proceedings of the Thirty-Third\nAAAI Conference on Artificial Intelligence and\nThirty-First Innovative Applications of Artificial\nIntelligence Conference and Ninth AAAI Sympo-\nsium on Educational Advances in Artificial In-\ntelligence, AAAI’19/IAAI’19/EAAI’19. AAAI\nPress.\nXiting Wang, Kunpeng Liu, Dongjie Wang, Le Wu,\nYanjie Fu, and Xing Xie. 2022b. Multi-level rec-\nommendation reasoning over knowledge graphs\nwith reinforcement learning. In The Web Confer-\nence 2022.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc\nLe, Ed Chi, Sharan Narang, Aakanksha Chowd-\nhery, and Denny Zhou. 2022c. Self-consistency\nimproves chain of thought reasoning in language\nmodels.\nJason Wei, Xuezhi Wang, Dale Schuurmans,\nMaarten Bosma, Ed Chi, Quoc Le, and Denny\nZhou. 2022. Chain of thought prompting elic-\nits reasoning in large language models. arXiv\npreprint arXiv:2201.11903.\nYichong Xu, Chenguang Zhu, Shuohang Wang,\nSiqi Sun, Hao Cheng, Xiaodong Liu, Jianfeng\nGao, Pengcheng He, Michael Zeng, and Xue-\ndong Huang. 2021. Human parity on common-\nsenseqa: Augmenting self-attention with exter-\nnal attention. arXiv preprint arXiv:2112.03254.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua\nBengio, William Cohen, Ruslan Salakhutdinov,\nand Christopher D. Manning. 2018. HotpotQA:\nA dataset for diverse, explainable multi-hop\nquestion answering. In Proceedings of the 2018\nConference on Empirical Methods in Natural\nLanguage Processing, pages 2369–2380, Brus-\nsels, Belgium. Association for Computational\nLinguistics.\nOri Yoran, Alon Talmor, and Jonathan Berant. 2022.\nTurning tables: Generating examples from semi-\nstructured tables for endowing language models\nwith reasoning skills. In Proceedings of the 60th\n5326\nAnnual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers) ,\npages 6016–6031, Dublin, Ireland. Association\nfor Computational Linguistics.\nWeihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi\nFeng. 2020. Reclor: A reading comprehension\ndataset requiring logical reasoning.\nEric Zelikman, Yuhuai Wu, and Noah D Goodman.\n2022. Star: Bootstrapping reasoning with rea-\nsoning. arXiv preprint arXiv:2203.14465.\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and\nYejin Choi. 2018. SW AG: A large-scale adversar-\nial dataset for grounded commonsense inference.\nIn Proceedings of the 2018 Conference on Empir-\nical Methods in Natural Language Processing,\npages 93–104, Brussels, Belgium. Association\nfor Computational Linguistics.\nTony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein,\nand Sameer Singh. 2021. Calibrate before use:\nImproving few-shot performance of language\nmodels.\nDenny Zhou, Nathanael Schärli, Le Hou, Jason\nWei, Nathan Scales, Xuezhi Wang, Dale Schuur-\nmans, Olivier Bousquet, Quoc Le, and Ed Chi.\n2022. Least-to-most prompting enables complex\nreasoning in large language models.\nFengbin Zhu, Wenqiang Lei, Youcheng Huang,\nChao Wang, Shuo Zhang, Jiancheng Lv, Fuli\nFeng, and Tat-Seng Chua. 2021. Tat-qa: A ques-\ntion answering benchmark on a hybrid of tabular\nand textual content in finance.\n5327\n[STRATEGY QA] Yes or no: Could a llama birth twice\nduring War in Vietnam (1945-46)? ▷ The War in Vietnam\nwas 6 months. The gestation period for a llama is 11\nmonths. So a llama could not give birth twice during the\nWar in Vietnam. The answer is no.\n[CLUTRR] Roy was eating lunch with his son John and\nhis wife Mary. What kind of relative is John to Mary? ▷\nJohn is the son of Roy. Roy is the husband of Mary. Thus,\nJohn is the son of Mary. The answer is son.\nTable 7: Besides arithmetic reasoning, we also inves-\ntigate commonsense and inductive reasoning.\nThis is the Appendix for the paper: “On the Ad-\nvance of Making Language Models Better Reason-\ners”.\nA Preliminaries\nPrompting. Prompting means prepending a few\nexemplars to the task input x and generating the\noutput y from the pretrained language model:\np(y|C,x) =\n|y|∏\nt=1\npLM(yt|C,x,y<t), (3)\nwhere Cis the concatenation of Kexemplars:\nC = (x1,y1); (x2,y2); ...; (xK,yK). (4)\nWe denote prompt as the concatenation of the ex-\nemplars Cand the input x.\nReasoning Paths. For reasoning tasks that aim\nto generate an answer y for a question x, Wei et al.\n(2022) proposed the insertion of a reasoning path z\nbefore generating the answer y:\nC′= (x1,z1,y1); ...; (xK,zK,yK), (5)\nwhere zi is a text reasoning path of how the an-\nswer yi is reasoned step-by-step for question xi.\nThen, during inference, a reasoning path z will be\ngenerated before the answer y:\np(y|C′,x) =p(z|C′,x) ·p(y|C′,x,z). (6)\nFigure 10 demonstrates this idea in arithmetic rea-\nsoning (GSM8K), and Table 7 demonstrates this\nidea in commonsense reasoning (StrategyQA) and\ninductive reasoning (CLUTRR).\nPrompt & Samples\nQ: If there are 3 cars in the parking lot and 2 more\ncars arrive, how many cars are in the parking lot?\nA: There are 3 cars in the parking lot already. 2 more\narrive. Now there are 3 + 2 = 5 cars. The answer is 5.\n...\nQ: Janet’s ducks lay 16 eggs per day. She eats three\nfor breakfast every morning and bakes muffins for her\nfriends every day with four. She sells the remainder\nfor $2 per egg. How much does she make every day?\nA:\n[Sample 1] She has 16 - 3 - 4 = 9 eggs left. So she\nmakes 2 ∗9 =18 per day. The answer is 18. (✓)\n[Sample 2] This means she uses 3 + 4 = 7 eggs every\nday. So in total she sells 7 * 2 =14 per day. The\nanswer is 14. (/enc-37)\n[Sample 3] She eats 3 for breakfast, so she has 16 - 3\n= 13 left. Then she bakes muffins, so 13 - 4 = 9 eggs\nleft. She makes 9 * $2 = $18. The answer is 18. ( ✓)\nFigure 10: Prompting large language models to generate\ndifferent reasoning paths, then selecting the final answer\nvia majority voting (Wang et al., 2022c).\nB Boosting Reasoning Paths via\nSelf-Teaching\nIn this section, we first introduce self-teaching,\nthe method we use to construct a larger exemplar\nbase when the original dataset does not contain\nenough data with well-annotated reasoning paths\n(Appendix B.1). We then discuss the noise issue\nwhen facing multiple-choice tasks (Appendix B.2).\nB.1 Self Teaching\nA critical issue of DIVERSE is how to provide\ndiverse prompts.6 Supposing that there is an ex-\nemplar base E, we can sampleKexemplars from it\nto construct a prompt, and repeat this M1 times in-\ndependently to construct M1 prompts with diverse\nexemplars.\nFor scenarios that do not have sufficient exem-\nplars (i.e., |E|< K∗M1), we propose to boot-\nstrap the diversity of prompts by “self-teaching”,\ni.e., generating pseudo reasoning paths from a\nfew exemplars and some ⟨question,answer⟩pairs\nwithout reasoning paths. 7 Suppose that D is\na dataset without reasoning paths, consisting of\n6Wang et al. (2022c) tried an ensemble-based approach,\ni.e., to permutate exemplars in the original prompt. However,\nthis strategy does not increase diversity in terms of exemplars.\n7This is motivated by Zelikman et al. (2022).\n5328\nDataset N Example Question\nGSM8K 1319 James decides to run 3 sprints 3 times a week. He runs 60 meters each\nsprint. How many total meters does he run a week?\nAsDiv 2096 Seven red apples and two green apples are in the basket. How many\napples are in the basket?\nMultiArith 600 The school cafeteria ordered 42 red apples and 7 green apples for students\nlunches. But, if only 9 students wanted fruit, how many extra did the\ncafeteria end up with?\nSV AMP 1000 Paco had 26 salty cookies and 17 sweet cookies. He ate 14 sweet cookies\nand 9 salty cookies. How many salty cookies did Paco have left?\nSingleEq 508 Terez has 44 cows on his farm. 50 percent of the cows are female, and 50\npercent of the females are pregnant. How many pregnant female cows\ndoes Terez have?\nCommonsenseQA 3387 Sammy wanted to go to where the people were. Where might he go?\nOptions: (a) race track (b) populated areas (c) desert (d) apartment (e)\nroadblock\nStrategyQA 2280 Could you go to New York Public Library and the Six Flags Great Escape\nin the same day?\nCLUTRR 447 Kelly and her mother Ernest made breakfast together. Constance and her\nhusband Ernest wanted a child badly What kind of relative is Kelly to\nConstance? The possible relationships are: sister, son, aunt,\ngranddaughter, father, grandfather, grandmother, mother-in-law, uncle,\nniece, mother, brother, daughter, nephew, grandson, son-in-law,\nfather-in-law, daughter-in-law.\nTable 8: Reasoning benchmarks we use in this paper with examples. N means the number of test cases.\n(x,y∗) pairs. Given the small exemplar base E, for\neach (x,y∗) ∈D, we can use prompting to gener-\nate a reasoning path z and the predicted answer y.\nWe define the pseudo exemplar base E′as:\nE′= {(x,z,y)|(x,y∗) ∈D,y = y∗}, (7)\nthen E∪E′can be regarded as the new exemplar\nbase for generating diverse prompts.\nB.2 Noises in Multiple Choice Tasks\nIn our experimental setup, StrategyQA and Com-\nmonsenseQA are more challenging than other tasks,\nas they use pseudo exemplars generated through\n“self-teaching” (Appendix B.1).\n“Self-teaching” may lead to bad exemplars, whose\nreasoning paths are invalid but happen to yield\nanswers coinciding with the ground truth. Ques-\ntions in StrategyQA/CommonsenseQA are two-\nchoice/four-choice questions, respectively. There-\nfore, such noise would be more serious in Strate-\ngyQA than in CommonsenseQA. This somehow\nexplains why DIVERSE can achieve comparable\nperformance (−0.8%) as the PaLM-based SOTA\non CommonsenseQA, while it sees a 3.0% perfor-\nmance decline to PaLM on StrategyQA, which has\nonly two choices. In other words, it is easier for\nStrategyQA to yield a right answer but a misleading\nreasoning path.\nC Data Statistics\nTable 8 shows the reasoning benchmarks we use in\nthis paper with examples. We use the same test sets\nas Wei et al. (2022) for GSM8K, AsDiv, MultiArith,\nSV AMP, SingleEq, and CommonsenseQA.\nFor StrategyQA, there are 2,290 test cases (i.e.,\nquestions paired with TRUE/FALSE labels), but\nthere is no other case that can be leveraged by\nDIVERSE to construct diverse exemplars (as in-\ntroduced in Section 2.1). To address this problem,\nwe randomly divide these 2,290 test cases into two\nequal parts (denoted as T1 and T2). For each DI-\n5329\nVERSE experiment of SQA, we conduct two runs:\nusing T1 to construct diverse exemplars and T2 as\nthe test set, and vice versa. The final reported solve\nrate is the average solve rate of these two runs.\nFor CLUTRR, Sinha et al. (2019) provided several\nversions: clean, supporting, irrelevant, and discon-\nnected. The clean version is the basic dataset, while\nthe others are the perturbed variations of it. Our\nexperiments are conducted on the clean version.\nD Our Changes to CLUTRR\nIn our experiments, two changes are applied to the\nCLUTRR benchmark: (1) appending candidate an-\nswers to each question; (2) constructing reasoning\npaths based on rules. Table 9 shows an example of\nCLUTRR data after our modification.\nCandidate Answers. Besides the original ques-\ntions (e.g., “Mary, a female, took her husband who\nis a male, Roy, out for lunch. Ernest bought to\ndress for his father Roy. What kind of relative is\nErnest to Mary?”), we also provide all the candi-\ndate answers (i.e., “The possible relationships are:\nsister, son, aunt, granddaughter, father, grandfather,\ngrandmother, mother-in-law, uncle, niece, mother,\nbrother, daughter, nephew, grandson, son-in-law,\nfather-in-law, daughter-in-law”) in the input se-\nquence. Our preliminary experiments show that,\nthe gigantic language models cannot reach more\nthan 50% accuracy without the sequence of candi-\ndate answers.\nReasoning Paths. For each question, Sinha et al.\n(2019) also provided a knowledge graph that formu-\nlates the relations directly mentioned in the ques-\ntion. Each knowledge graph consists of several\n⟨e1,r,e2⟩triplets, which means there is a rela-\ntion r from e1 to e2. Take the aforementioned\nquestion as an example, the knowledge graph con-\nsists of two triplets: ⟨Mary,husband,Roy⟩and\n⟨Ernest,father,Roy⟩.\nFor each question, we construct the reasoning path\nbased on its knowledge graph. We first topologi-\ncally sort all triplets in the knowledge graph. For\neach triplet, we convert it to a reasoning step using\nthe template “{e2} is the {r} of {e1}”. After that, we\ncan get the reasoning path by concatenating these\nreasoning steps. Take the aforementioned question\nas an example, the reasoning path is: “ Roy is the\nhusband of Mary. Roy is the father of Ernest. Thus,\nErnest is the son of Mary.”\nVariant Input Example\nCLUTRR\nfor NLI\n(Original)\nInput:Story: Kelly and her mother Ernest made\nbreakfast together. Constance and her husband\nErnest wanted a child badly. Query: Kelly, Con-\nstance\nOutput: daughter\nCLUTRR\nfor NLG\n(Ours)\nInput:Kelly and her mother Ernest made break-\nfast together. Constance and her husband Ernest\nwanted a child badly. What kind of relative is Kelly\nto Constance? The possible relationships are: sis-\nter, son, aunt, granddaughter, father, grandfather,\ngrandmother, mother-in-law, uncle, niece, mother,\nbrother, daughter, nephew, grandson, son-in-law,\nfather-in-law, daughter-in-law.\nOutput:Kelly is the daughter of Ernest. Ernest is\nthe husband of Constance. Thus, Kelly is the\ndaughter of Constance. So the answer isdaughter.\nTable 9: Examples of the original and our constructed\nCLUTRR data. In our experiments, we concatenated\nthe class information (marked with red) after the input\nquestion as the input, and constructed reasoning paths\n(marked with blue) via a simple text template as the\noutput.\n5330\nQuestion Self-Consistency DIVERSE\n[GSM8K]Siobhan has 2 fewer jewels\nthan Aaron. Aaron has 5 more jewels\nthan half of Raymond’s jewels. If\nRaymond has 40 jewels, how many\njewels does Siobhan have?\nAaron has 2 more jewels than half of Raymond’s\njewels. So Aaron has40 / 2 + 2 = 22jewels. Siob-\nhan has 2 fewer jewels than Aaron so she has22 - 2\n= 20jewels. So the answer is20.\nAnswer: 20 (Wrong) Count:33\nRaymond has 40 jewels. Half of Raymond’s jewels\nis40 / 2 = 20. Aaron has 5 more jewels than half\nof Raymond’s jewels, so he has20 + 5 = 25jewels.\nSiobhan has 2 fewer jewels than Aaron, so she has\n25 - 2 = 23jewels. So the answer is23.\nAnswer: 23 (Correct) Count:32\n[StrategyQA]Could you go to New\nYork Public Library and the Six Flags\nGreat Escape in the same day?\nThe New York Public Library is in Manhattan. The\nSix Flags Great Escape is in Queensbury. Manhat-\ntan and Queensbury are too far apart to go to both\nin a single day. Thus, you could not go to New York\nPublic Library and the Six Flags Great Escape in\nthe same day. So the answer isno.\nAnswer: no (Wrong) Count:62\nThe New York Public Library is in New York City.\nThe Six Flags Great Escape is in Queensbury, New\nYork. Queensbury is about 3.5 hours away from\nNew York City by car. Thus, you could go to the\nNew York Public Library and the Six Flags Great\nEscape in the same day. So the answer isyes.\nAnswer: yes (Correct) Count:38\nTable 10: Examples of code-davinci-002 on GSM8K. Compared to self-consistency (majority voting), DIVERSE\ncan select the correct-but-not-most answer out of the sampled candidates, thus improving the reasoning performance.\n5331\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection 9\n□\u0017 A2. Did you discuss any potential risks of your work?\nLeft blank.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nSection 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0017 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nNo response.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNo response.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNo response.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNo response.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNo response.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nNo response.\nC □\u0013 Did you run computational experiments?\nSection 3 introduces the experiment setups of our work, and Section 4 contains the experimental results.\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nAs mentioned in Section 3, the three models we used in our work are GPT-3 davinci, text-davinci-002\nand code-davinci-002. However, we don’t know the inner details of these models and thus cannot\nestimate the total computational budget or the number of parameters.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n5332\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 3\n□\u0017 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nDue to the cost of running large PLMs like GPT-3, we only run once to get the result under each\nsetting.\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection 3\nD □\u0013 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nSection 6.3\n□\u0013 D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nSection 6.3\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. The data to be annotated are model outputs during the experiment. They are not\nobtained from other people or institutions.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. The data to be annotated are model outputs during the experiment. They are not\nobtained from other people or institutions.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n5333",
  "topic": "Chen",
  "concepts": [
    {
      "name": "Chen",
      "score": 0.800178050994873
    },
    {
      "name": "Computer science",
      "score": 0.6272366046905518
    },
    {
      "name": "Zhàng",
      "score": 0.6221383213996887
    },
    {
      "name": "Programming language",
      "score": 0.45270392298698425
    },
    {
      "name": "Computational linguistics",
      "score": 0.44544118642807007
    },
    {
      "name": "Natural language processing",
      "score": 0.3420335054397583
    },
    {
      "name": "Political science",
      "score": 0.12489452958106995
    },
    {
      "name": "China",
      "score": 0.0926487147808075
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ]
}