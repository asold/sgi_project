{
  "title": "APoLLo : Unified Adapter and Prompt Learning for Vision Language Models",
  "url": "https://openalex.org/W4389523744",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2340056598",
      "name": "Sanjoy Chowdhury",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    },
    {
      "id": "https://openalex.org/A2598953747",
      "name": "Sayan Nag",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A2146473740",
      "name": "Dinesh Manocha",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3174770825",
    "https://openalex.org/W4226182655",
    "https://openalex.org/W4386076084",
    "https://openalex.org/W2962707369",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2533598788",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W4224545477",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4386790226",
    "https://openalex.org/W2155904486",
    "https://openalex.org/W4366330698",
    "https://openalex.org/W3128513979",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4281838252",
    "https://openalex.org/W2017814585",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4312310776",
    "https://openalex.org/W4312424618",
    "https://openalex.org/W3205270560",
    "https://openalex.org/W4386065353",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4378473852",
    "https://openalex.org/W4386065763",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2962945654",
    "https://openalex.org/W2138011018",
    "https://openalex.org/W4281987380",
    "https://openalex.org/W4386071687",
    "https://openalex.org/W2963211188",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4386065836",
    "https://openalex.org/W4366458249",
    "https://openalex.org/W2047643928",
    "https://openalex.org/W4321337794",
    "https://openalex.org/W3198377975",
    "https://openalex.org/W4312784228",
    "https://openalex.org/W1846799578",
    "https://openalex.org/W4312446817",
    "https://openalex.org/W4386071547",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W4312884055",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4296151208",
    "https://openalex.org/W4229453513",
    "https://openalex.org/W2947707615",
    "https://openalex.org/W4368321032",
    "https://openalex.org/W24089286",
    "https://openalex.org/W3215626407",
    "https://openalex.org/W12634471",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W3037492894",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2971296908",
    "https://openalex.org/W4280496682",
    "https://openalex.org/W4389519448",
    "https://openalex.org/W3134652006",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4313068342",
    "https://openalex.org/W4284961860",
    "https://openalex.org/W2953937638",
    "https://openalex.org/W3126337491",
    "https://openalex.org/W4310557340",
    "https://openalex.org/W4385801086",
    "https://openalex.org/W4312933868",
    "https://openalex.org/W4385571139",
    "https://openalex.org/W3213647938",
    "https://openalex.org/W4362655426",
    "https://openalex.org/W3177096435"
  ],
  "abstract": "The choice of input text prompt plays a critical role in the performance of Vision-Language Pretrained (VLP) models such as CLIP. We present APoLLo, a unified multi-modal approach that combines Adapter and Prompt learning for Vision-Language models. Our method is designed to substantially improve the generalization capabilities of VLP models when they are fine-tuned in a few-shot setting. We introduce trainable cross-attention-based adapter layers in conjunction with vision and language encoders to strengthen the alignment between the two modalities. We enforce consistency between the respective encoder branches (receiving augmented inputs) to prevent overfitting in downstream tasks. Our method is evaluated on three representative tasks: generalization to novel classes, cross-dataset evaluation, and unseen domain shifts. In practice, APoLLo achieves a relative gain up to 6.03% over MaPLe (SOTA) on novel classes for 10 diverse image recognition datasets.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 10173–10187\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nAPoLLo\n : Unified Adapter and Prompt Learning for Vision Language\nModels\nSanjoy Chowdhury1∗ Sayan Nag2∗ Dinesh Manocha1\n1University of Maryland, College Park 2University of Toronto\n{sanjoyc,dmanocha}@umd.edu sayan.nag@mail.utoronto.ca\nProject page - https://gamma.umd.edu/pro/vision_language/apollo/\nAbstract\nThe choice of input text prompt plays a criti-\ncal role in the performance of Vision-Language\nPretrained (VLP) models such as CLIP. We\npresent APoLLo , a unified multi-modal ap-\nproach that combines Adapter and Prompt\nlearning for Vision-Language models. Our\nmethod is designed to substantially improve\nthe generalization capabilities of VLP models\nwhen they are fine-tuned in a few-shot setting.\nWe introduce trainable cross-attention-based\nadapter layers in conjunction with vision and\nlanguage encoders to strengthen the alignment\nbetween the two modalities. We enforce consis-\ntency between the respective encoder branches\n(receiving augmented inputs) to prevent over-\nfitting in downstream tasks. Our method is\nevaluated on three representative tasks: gener-\nalization to novel classes, cross-dataset evalu-\nation, and unseen domain shifts. In practice,\nAPoLLo achieves a relative gain up to 6.03%\nover MaPLe (SOTA) on novel classes for 10\ndiverse image recognition datasets.\n1 Introduction\nRecent years have witnessed tremendous success\nof largescale pre-trained language models (Devlin\net al., 2018; Liu et al., 2019; Raffel et al., 2020;\nBiderman et al., 2023; Scao et al., 2022; Shen et al.,\n2023; Touvron et al., 2023) as well as visual mod-\nels (Dosovitskiy et al., 2020; Liu et al., 2021b;\nTan and Le, 2019) leading to a surge in pre-trained\nVision-Language models (Dou et al., 2022; Li et al.,\n2022a; Yang et al., 2022; Wang et al., 2023a; Li\net al., 2023b,a; Wang et al., 2023b) for multi-modal\ndownstream tasks. Despite being largely successful\nin terms of generalization capabilities, these VLP\nmodels such as CLIP (Radford et al., 2021) are\ndifficult to fine-tune for few-shot learning-based\ndownstream tasks (Khattak et al., 2023). This is\nmainly because of the massive scale of these mod-\nels coupled with the deficiency of training data\n∗Equal contribution.\nCaltech101(Novel)\nImageNet(Novel)\nStanford Cars(Base)Flowers(Novel)\nOxfordPets(Base)\nSUN397(Base)\nDTD(Base)\nUCF101(Base) Food101(Base)\nFGVCAircraft(Base)\n94.3695.63\n70.54\n73.4372.94\n78.16\n72.46\n77.83\n95.43\n97.11\n80.8282.97\n80.36\n83.95 83.00\n87.88\n90.71\n91.35\n37.4440.62\nAPoLLo(Ours)\nMaPLe (SOTA)\nFigure 1: APoLLo sets a new state-of-the-art across\nseveral datasets for the base-to-novel class generaliza-\ntion task by learning a vision-language representation\nusing a unified adapter and prompt tuning strategy under\na few-shot setting. Each axis denotes an accuracy value,\neither on base or novel class (specified accordingly) for\nthe corresponding dataset (refer to Table 1).\n(Khattak et al., 2023). Recent studies in literature\npropose fine-tuning of these models by introduc-\ning and calibrating the parameters by prompting\n(Zhou et al., 2022a) and adapter (Gao et al., 2021a)\ntechniques. While the former injects tunable pa-\nrameters through learned embeddings in one or\nmore modalities, the latter incorporates in-situ ad-\njustments conventionally near the prediction head.\nThe effectiveness of such fine-tuning-based\nmethods notwithstanding recent investigations re-\nveal (Zhou et al., 2022b) the caveats of these ap-\nproaches – such as neglecting the useful knowledge\ngained from the preceding pre-training step and\noverfitting with the downstream tasks. Although\nwe find a considerable wealth of studies that in-\nvolve text-based prompt learning (Lu et al., 2022;\nShu et al., 2022; Huang et al., 2022), the same for\nthe visual pipeline still remains an under-explored\narea.\nMain Results: We present APoLLo , a novel\nunified adapter and prompt learning approach for\nVLP models to tackle the generalizability prob-\nlems in few-shot scenarios. We enforce consistency\nbetween intra-modal encoders using consistency-\n10173\nguided contrastive loss (Wei et al., 2020). This is\ndone to teach the association between the query and\nthe semantically similar negative in-batch keys. To\nfurther enhance cross-modal alignment, we employ\ncross-attention in the modality-specific adapter lay-\ners. This leads to better awareness of the multi-\nmodal features. Experimental results over a varied\nrange of recognition datasets demonstrate the effi-\ncacy of our approach. Some novel aspects of our\nwork include:\n(1) To the best of our knowledge, ours is the first\nmethod that combines adapter and prompt tun-\ning for VLP models (e.g., CLIP) in a unified\nmanner. This facilitates learning new tasks in a\nfew-shot setting without compromising on their\nzero-shot generalizability.\n(2) We propose a novel multi-modal augmen-\ntation strategy by leveraging LLMs to gener-\nate descriptive texts as augmented samples in the\ntext branch, and text-conditioned diffusion mod-\nels to generate image augmentations for the image\nbranch.\n(3) Our novel application of multi-modal cross-\nattention adapter layers bridges the gap between\nthe two modalities by generating text-guided visual\nfeatures and vice-versa. This promotes the synergy\nbetween the two modalities.\n(4) Extensive evaluation on 10 challenging datasets\ndemonstrates the effectiveness of APoLLo as it\noutperforms existing methods by a significant\nmargin and set a new SOTA (Figure 1) for a range\nof downstream tasks including base-to-novel gen-\neralization, cross-dataset recognition, and domain\ngeneralization.\n2 Related Works\n2.1 Vision Language Models\nRecent research has indicated that effectively min-\ning image-text pairs can enable VLP models to\nachieve highly satisfactory results on relevant\ndownstream tasks when compared against uni-\nmodal frameworks. For example, models like CLIP\n(Radford et al., 2021), ALIGN (Jia et al., 2021),\nand Florence (Yuan et al., 2021) demonstrate ex-\nceptional performance on a wide range of few-shot\nand zero-shot visual recognition tasks. However,\nthey are impractical to adapt to challenging down-\nstream tasks. Prior work include specially designed\napproaches pursuing object detection (Bangalath\net al., 2022; Zang et al., 2022; Zhou et al., 2022c)\nand, few-shot image recognition (Zhang et al.,\n2021; Gao et al., 2021b; Kim et al., 2021) that fare\nmuch better compared to off-the-shelf VLP models\n(Radford et al., 2021; Jia et al., 2021). In this paper,\nwe present a novel image and text augmentation-\nbased technique for a unified adapter and prompt\nlearning to enable CLIP based model to generalize\nwell under few-shot and zero-shot visual recogni-\ntion tasks.\n2.2 Prompt Tuning\nPrompt tuning (Li and Liang, 2021; Liu et al.,\n2021a; Lester et al., 2021) typically refers to\nprepending language instructions to the input text\nto facilitate a better understanding of the down-\nstream tasks. For example, instead of feeding\nthe system with a fixed template ‘a photo of\na <CLASS>’, task-specific additional information\ncould be more helpful for the model. More so,\ntuning-based methods achieve comparable perfor-\nmance as full finetuning but with∼1000× fewer pa-\nrameters. To this end, Context Optimization(CoOp)\n(Zhou et al., 2022b) proposes a replacement of\nthe hand-crafted prompts with the learnable soft\nprompts. However, their approach lacks gener-\nalizability and demonstrates suboptimal perfor-\nmance under a zero-shot setting. CoCoOp (Zhou\net al., 2022a) generates an image-conditional con-\ntext along with text conditioning through prompt\ntuning, while ProDA (Lu et al., 2022) incorpo-\nrates the prompt’s prior distribution learning. Pro-\nGrad (Zhu et al., 2022) performs prompt updates\nwhere the gradients are aligned with the original\nprompt’s knowledge. Recent works (Bahng et al.,\n2022; Khattak et al., 2023) leverage a multi-modal\nprompting (image + text) technique to exploit the\nnatural association across modalities.\n2.3 Adapter Tuning\nAdapter-based methods (Houlsby et al., 2019) in-\nject additional trainable parameters into a pre-\ntrained model to facilitate customized learning for\ndownstream tasks. Yuan et al. (2021); Sung et al.\n(2022) introduce additional layers near the predic-\ntion head to enrich pre-trained models with addi-\ntional parameters. Initial efforts to develop adap-\ntive methods for computer vision tasks involved in-\ncremental learning (Rosenfeld and Tsotsos, 2018)\nand domain adaption methods (Rebuffi et al., 2017,\n2018). APoLLo leverages adapter and prompting\nto boost the performance of the model for down-\nstream tasks, while contrastive-consistency loss en-\nsures the generalizability of the model.\n10174\nImage Encoder Text Encoder❄ Image Encoder Text Encoder❄ ❄ ❄ \nPatch Embed❄ Patch Embed❄ Word Embed Word Embed❄ ❄ \n❄ ❄ \n ❄ ❄ \nA photo of a cat\nA small, carnivorous\nmammal with soft fur,\nretractable claws, and a tail\nImage Adapter Text Adapter\nSelf Attention\nFeed Forward\nSelf Attention\nFeed Forward\nCross Attention Cross Attention\nA B \nA B Image Adapter Text Adapter\nCross Attention\nA B \n❄ \nFrozen Params Trainable Params\nDiffusion ❄ ❄ LLM \nFigure 2: Overview of the proposedAPoLLo framework for a unified multi-modal adapter and prompt learning in\nVLP models. APoLLo unifies prompt and adapter tuning for both the image and text branches. The image (blue) and\ntext (yellow) adapter layers are coupled with each other through cross-modal interactions via attention which further\nimproves the alignment between the two modalities. Each modality contains two branches receiving augmented\nversions of input texts and images generated using LLM and text-conditioned diffusion models respectively.\n3 Methodology\n3.1 Brief Review of CLIP\nAmong the existing Vision-Language models,\nCLIP (Radford et al., 2021) demonstrates strong\ngeneralizability for zero-shot image recognition.\nBeing a multi-modal framework it’s comprised of\nimage and text encoder modules. In our set-up, we\ndeploy a transformer-based image encoder (θ) and\ntext encoder (ϕ).\n3.1.1 Image Encoder\nOur patch-based image embedding module is com-\nprised of Ktransformer layers that split an image\ninto Mfixed-sized patches which are projected\ninto patch embeddings E0 ∈RM×dv . Patch em-\nbedding Ei is fed into the next transformer block\ncombined with a learnable class token ci where\ni∈{1,... K}. Therefore,\n[ci+1,Ei+1] = θi+1([ci,Ei]) (1)\nThe final image representation is obtained by\nprojecting class token ck of the last transformer\nlayer to a common latent space:\nu= IProj (cK) ; u∈Rdvl . (2)\n3.1.2 Text Encoder\nText encoder generates feature embedding by tok-\nenizing the word sequence. At the (i+ 1)th step\nWi is fed as input to the transformer text encoder:\n[Wi+1] = ϕi+1(Wi) ; ∀i∈{1,... K}. (3)\nLike its image counterpart, the final text repre-\nsentation vis obtained by projecting the text em-\nbedding of the last transformer blockϕKto a shared\nlatent space:\nv= TProj (wK) ; v∈Rdvl . (4)\n3.1.3 Zero-Shot Classification\nDuring zero-shot inference, text prompts are pro-\nvided with class labels y ∈{1,2,...C }. Class\nlabel ˆy with the highest cosine similarity score\n(sim(·)) w.r.t the input imagexis computed, where\nτ is the temperature parameter.\np(ˆy|x) = exp (sim (u,vˆy) /τ)∑C\ni=1 exp (sim (u,vi))\n. (5)\nThis template-based approach was found to pro-\nduce suboptimal results across domains. To ad-\ndress this, CoOp (Zhou et al., 2022b) proposed an\nalternative approach that replaces the hand-crafted\nprompt with a set of learnable context vectors that\ngenerate task-specific text embeddings. These are\naugmented with the previously generated word em-\nbeddings. Likewise, learnable context vectors are\nconcatenated with the patch embeddings. We ex-\ntend MaPLe (Khattak et al., 2023) to present a\nnovel multi-modal prompt learning approach to en-\nsure generalizability across different downstream\ntasks under a zero-shot classification setting.\n3.2 Our Approach: APoLLo\nWe note that prior methods are based on uni-modal\nprompt tuning approaches to finetune CLIP to per-\nform downstream image recognition. On the other\nhand, MaPLe is the only work that uses multi-\nmodal prompt tuning. In a major departure from\nthis approach, we introduce a novel unified adapter\n10175\nand prompt learning method with a novel aug-\nmentation strategy to the text and image inputs\nto facilitate further regularization. To this end, we\nsupplement our adapter layers with multi-modal\ncross-attention modules that ensure the text and\nthe image modalities are aligned. The text input\nis passed through a pre-trained LLM to obtain a\nmore descriptive expression of itself while the im-\nage branch comprises a text-conditioned diffusion\nmodel to obtain augmentation of the input image\nas shown in Figure 2.\n3.2.1 Image and Language Prompting\nWe facilitate deep image and language prompting\nacross the hierarchies in the image and text trans-\nformer backbones (of CLIP) by introducing learn-\nable tokens at every transformer layer. It has been\nshown (Khattak et al., 2023) that systematic prompt\nsharing in successive stages is more intuitive over\nindependent prompts as consecutive transformer\nblocks ensure densely correlated feature learning.\n3.2.2 Input Augmentation\nGenerative data augmentation schemes have been\nleveraged very recently (Hu et al., 2023; Trabucco\net al., 2023; Azizi et al., 2023; Shipard et al., 2023;\nYin et al., 2023; Whitehouse et al., 2023), however,\nnot in a multi-modal context (Appendix A.4). In\nour method, we add regularization by using aug-\nmented versions of respective modality inputs in\nthe corresponding frozen encoder branches. In\nparticular, for the text branch, we employ a pre-\ntrained frozen LLM (Brown et al., 2020) to gen-\nerate sentences about the object referred to in the\ntext. These sentences are typically descriptions of\nthe usual characteristics of the object. For example,\nas shown in Figure 2, we provide a hand-crafted\ntemplate (‘a photo of a <CLASS>’) as input to\nthe LLM. Considering Cat as the ‘CLASS’, LLM\noutputs a descriptive text as: ‘a small carnivorous\nmammal with soft fur, retractable claws, and a\ntail’. In this regard, KgCoOp (Yao et al., 2023) also\nintroduces textual augmentations. However, they\nget an average embedding from a pre-defined num-\nber of sentences, whereas we restrict our model\nto generate a single sentence (on the fly) which is\neasier and more diverse. For the image branch we\nintroduce a text-conditioned diffusion (Rombach\net al., 2022) based augmentation strategy that gen-\nerates novel examples of the same class (see Figure\n2). Experimental results back our claim that adding\nthese two separate prompting modules indeed re-\nsults in enhanced performance (also see Section\n4.4 for detailed ablations).\n3.2.3 Multi-modal Cross-attention Adapter\n(MCA):\nLately, adapter layers have played a pivotal role in\nmodel finetuning for adaptation to new downstream\ntasks (Yuan et al., 2021; Li et al., 2022b; Chen et al.,\n2022; Hao et al., 2023). Typically, these adapter\nlayers are trainable parameters that are added on\ntop of an encoder to transform the embedding vec-\ntor to better adapt to new tasks. Inspired by Chen\net al. (2022), we maintain trainable cross-attention-\nbased adapter layers, for the respective encoders\nindividually. In a major departure from MaPLe,\nwhere the authors used multi-modal prompt cou-\npling, we introduce cross-attention modules that\nenforce inter-modal alignment.\n3.2.4 Loss Formulation\nOur training paradigm involves the computation\nof two types of losses between the multi-modal\nencoders.\nIntra-modal Contrastive Consistency: In a typi-\ncal contrastive learning setting, the heterogeneous\nsimilarities between the query and other in-batch\nnegatives are ignored resulting in suboptimal repre-\nsentation learning. In order to mitigate this ‘class-\ncollision’ problem and better deal with sampling\nbias (Wei et al., 2020) proposed a consistency-\nbased contrastive loss. We employ this loss to\nenforce consistency between the respective intra-\nmodality branches as given below:\nLcon = α1 ∗LI\nCO2 + α2 ∗LT\nCO2 . (6)\nIn practice we keep α1 = α2 = αfor all exper-\niments. We add more details about LCO2 in the\nAppendix A.2.2.\nInter-modal Similarity Maximization: The inter-\nmodality loss is responsible for maximizing the\nsimilarity between the multi-modal branches. The\nimage-text pairs in a mini-batch are passed through\nthe respective encoders followed by the adapter lay-\ners. The normalized image and text embeddings are\nsubsequently utilized to compute the inter-modality\nImage-Text Contrastive (ITC) loss given as LIT\nsim\n(refer to Appendix A.3 for further details).\nThe total loss is calculated by the weighted aver-\nage of the above two losses:\nLtotal = LIT\nsim + α∗Lcon. (7)\n10176\nMethod ImageNet Caltech101 OxfordPets StanfordCars Flowers102\nBase Novel Base Novel Base Novel Base Novel Base Novel\nCLIP 72.43 68.14 96.84 94.00 91.17 97.26 63.37 74.89 72.08 77.80\nCoOp 76.47 67.88 98.00 89.81 93.67 95.29 78.12 60.40 97.60 59.67\nCo-CoOp 75.98 70.43 97.96 93.81 95.20 97.69 70.49 73.59 94.87 71.75\nProGrad 77.02 66.66 98.02 93.89 95.07 97.63 77.68 68.63 95.54 71.87\nKgCoOp 75.83 69.96 97.72 94.39 94.65 97.76 71.76 75.04 95.00 74.73\nMaPLe 76.66 70.54 97.74 94.36 95.43 97.76 72.94 74.00 95.92 72.46\nAPoLLo 78.91 73.43 98.69 95.63 97.11 98.94 78.16 75.66 98.07 77.83\n∆APoLLo−MaPLe +2.25 +2.89 +0.95 +1.27 +1.68 +1.18 +5.22 +1.66 +2.15 +5.37\nMethod Food101 FGVCAircraft SUN397 DTD UCF101\nBase Novel Base Novel Base Novel Base Novel Base Novel\nCLIP 90.10 91.22 27.19 36.29 69.36 75.35 53.24 59.90 70.53 77.50\nCoOp 88.33 82.26 40.44 22.30 80.60 65.89 79.44 41.18 84.69 56.05\nCo-CoOp 90.70 91.29 33.41 23.71 79.74 76.86 77.01 56.00 82.33 73.45\nProGrad 90.37 89.59 40.54 27.57 81.26 74.17 77.35 52.35 84.33 74.94\nKgCoOp 90.50 91.70 36.21 33.55 80.29 76.53 77.55 54.99 82.89 76.67\nMaPLe 90.71 92.05 37.44 35.61 80.82 78.70 80.36 59.18 83.00 78.66\nAPoLLo 91.35 92.58 40.62 39.87 82.97 80.62 83.95 65.21 87.88 80.52\n∆APoLLo−MaPLe +0.64 +0.53 +3.18 +4.26 +2.15 +1.92 +3.59 +6.03 +4.88 +1.86\nTable 1: Comparison of APoLLo with SOTA methods on a Base-to-Novel Class Generalization task.\nAPoLLo shows strong generalization capabilities across all 10 datasets and outperforms MaPLe (previous SOTA)\nin all of them. Best accuracy values are shown in bold and the differences with respect to MaPLe are given in blue.\n4 Experimental Details\n4.1 Downstream Tasks\nBase-to-Novel Class Generalization: We follow\nthe standard practice of base-to-novel generaliza-\ntion under zero-shot evaluation with few-shot fine-\ntuning. Datasets are partitioned into base and novel\nclasses with the model being trained on the base\nclasses and evaluated on both base and novel cate-\ngories.\nCross-Dataset Evaluation: To analyze the zero-\nshot generalizability of APoLLo , we perform a\ncross-dataset assessment. To facilitate this, the\nmodel trained solely on the ImageNet dataset was\nexposed to other 9 datasets. To be consistent with\nthe prior art, we follow MaPLe and CoCoOp to\ntrain our model under a few-shot setting for a fair\nassessment.\nDomain Generalization: Taking one step further\ntowards a more robust evaluation, the performance\nof the model was analyzed on its out-of-distribution\ngeneralization capabilities. Like cross-dataset eval-\nuation, the performance of the ImageNet trained\nmodel was observed under its four other variants:\nImageNetV2, ImageNetS, ImageNetA, and Ima-\ngeNetR as these are known to contain sufficient\ndomain shifts.\n4.2 Datasets\nTo extensively evaluate our model under differ-\nent setting, we consider a total of 10 image clas-\nsification datasets covering various recognition\ntasks including object classification datasets Im-\nageNet (Deng et al., 2009), Caltech 101 (Fei-\nFei et al., 2004); fine-grained datasets Oxford-\nPets (Parkhi et al., 2012), StanfordCars (Krause\net al., 2013), Flowers 102 (Nilsback and Zisser-\nman, 2008), Food101 (Bossard et al., 2014), FGV-\nCAAircraft (Maji et al., 2013); scene recognition\ndataset SUN397 (Xiao et al., 2010); action recog-\nnition dataset UCF101 (Soomro et al., 2012); and\ntexture recognition dataset DTD (Cimpoi et al.,\n2014). For domain generalization, we rigorously\nevaluate on four ImageNet variants: ImageNetV2\n(Recht et al., 2019), ImageNetSketch (Wang et al.,\n2019), ImageNet-A (Hendrycks et al., 2021b), and\nImageNet-R (Hendrycks et al., 2021a).\n4.3 Main Results\n4.3.1 Base-to-Novel Class Generalization\nWe subdivide the generalization evaluation into the\nfollowing two categories.\nGeneralization to Unseen Classes: Table 1\npresents the comparison of our method with CLIP,\nCoOp (Zhou et al., 2022b), CoCoOp (Zhou et al.,\n2022a), ProGrad (Zhu et al., 2022), KgCoOp (Yao\n10177\nMethod Target\nCaltech Pets Cars Flowers Food Aircraft SUN DTD UCF\nCoOp 93.70 89.14 64.51 68.71 85.30 18.47 64.15 41.92 66.55\nCo-CoOp 94.43 90.14 65.32 71.88 86.06 22.94 67.36 45.73 68.21\nMaPLe 93.53 90.49 65.57 72.23 86.20 24.74 67.01 46.49 68.69\nAPoLLo 95.12 91.56 66.21 73.15 86.82 24.96 67.98 47.63 70.38\n∆APoLLo−MaPLe +1.59 +1.07 +0.64 +0.92 +0.62 +0.22 +0.97 +1.14 +1.69\nTable 2: Comparison of APoLLo with SOTA methods on a cross-dataset evaluation taskwhere the model is\ntrained on ImageNet and evaluated on the target datasets in a zero-shot manner. APoLLo obtains the best accuracy\namong the existing methods suggesting better generalization capabilities. Best accuracies are presented in bold and\nimprovements over MaPLe (previous SOTA) are shown in blue.\nMethod Target\nImNetV2 ImNetS ImNetA ImNetR\nCLIP 60.83 46.15 47.77 73.96\nCoOp 64.20 47.99 49.71 75.21\nCo-CoOp 64.07 48.75 50.63 76.18\nProGrad 64.73 47.61 49.39 74.58\nKgCoOp 64.10 48.97 50.69 76.70\nMaPLe 64.07 49.15 50.90 76.98\nAPoLLo 64.89 50.17 51.74 78.33\n∆APoLLo−MaPLe +0.82 +1.02 +0.84 +1.35\nTable 3: Comparison of APoLLo with SOTA\nmethods on a domain generalization task.\nAPoLLo demonstrates the best performance across\nall datasets. Best accuracies are present in bold and\nimprovements over MaPLe (previous SOTA) are shown\nin blue.\net al., 2023), MaPLe on novel classes. Experimen-\ntal results show the superiority of the proposed\napproach as it outperforms all existing methods by\na significant margin on zero-shot generalization. It\nachieves a relative gain of up to 6.03% over the\nmost recent baseline. Evaluating on all 10 datasets,\nwe find an average improvement of 2.69% over\nMaPLe on novel categories. As is evident from the\ntables none of the existing methods beat pre-trained\nCLIP (except MaPLe) underlining the challenges\nof achieving satisfactory zero-shot performance\nwhile learning a new task in a few-shot setting.\nOur method also enjoys an average improvement\nof 2.80% over CLIP on novel classes across all the\ndatasets. This can be attributed to our unified multi-\nmodal adapter and prompting technique that en-\nables the model to leverage the mutual association\nbetween visual and language modalities.\nGeneralization and Performance on Base\nClasses: CoCoOp extends CoOp by introducing\nimage-conditioned prompts thus securing consid-\nerable generalizability and obtaining a significant\nperformance boost over the latter. The downside is\nunsatisfactory performance on base classes as it is\nonly effective on 2 / 10 datasets with a large drop\nin average performance. MapLe on the other hand\nbetters CoCoOp on most of the datasets (Table 1).\nNote that CoOp lacks generalizability over novel\nclasses due to its massive overfitting with the base\nclasses. In contrast, APoLLo alleviates this limi-\ntation and sees a large gain over its predecessors\nproving its effectiveness in learning new classes\nwhile maintaining a stable performance on base\nclasses. Note that, our method outperforms the\ncurrent benchmark over all 10 datasets to obtain\nfew-shot performance improvements up to 5.22%\non base categories and 2.66% overall. This sug-\ngests that the gain under a zero-shot setting does\nnot affect its few-shot performance or the other way\naround.\n4.3.2 Cross-Dataset Evaluation\nTable 2 presents the results on a cross-dataset eval-\nuation where the model is trained solely on Ima-\ngeNet and evaluated on other 9 datasets under the\nzero-shot setting. We find APoLLo surpasses all\nother methods and observe the best zero-shot im-\nprovement of 1.69% over MaPLe (SOTA) on the\nUCF dataset.\n4.3.3 Domain Generalization\nTable 3 illustrates domain generalization results\nof APoLLo . We use the ImageNet dataset as\nthe base and test our model on ImageNetV2, Im-\nageNetS, ImageNetA, and ImageNetR where all\nof them are from vastly different distributions. We\nnotice strong generalizability across different do-\nmains as APoLLo outperforms all prior baselines\nby achieving the best zero-shot improvement of\n1.35% over MaPLe on the ImageNetR dataset.\n10178\nFigure 3: Impact of Prompt Layer, Adapter Layer, and Loss Balancing Factor (α) on the performance of APoLLo .\nIntra-modalAdap.Cross-attn.Avg.(Base) Avg.(Novel)\n✗ ✗ ✗ 81.49 75.62\n✓ ✗ ✗ 82.36 76.45\n✓ ✓ ✗ 83.05 77.22\n✗ ✓ ✗ 82.30 76.21\n✗ ✓ ✓ 82.93 77.07\n✓ ✓ ✓ 83.77 78.03\nTable 4: Impact of Intra-modal contrastive consis-\ntency, Adapter tuning, and Cross-attention strategies\non the performance of APoLLo . Our method gives the\nbest accuracy values (averaged on 10 datasets for base-\nto-novel generalization task) when all three components\nare considered.\n4.4 Ablations\nImpact of Intra-modal Contrastive Consis-\ntency, Adapter, and Cross-attention: We assess\nthe importance of the individual components in\nAPoLLo and report the average accuracy scores\n(on 10 datasets) for the Base-to-Novel Class Gener-\nalization task (see Table 4). We show 3 main com-\nponents in APoLLo , i.e., Intra-modal Contrastive\nConsistency, Adapter Layers, and the utility of the\nCross-attention strategy. Note that Prompt Learn-\ning is present in all these experiments. Further-\nmore, input augmentation is present in cases where\nintra-modal consistency is taken into account. We\nobserve the lowest performance with only prompt\nlearning, shown in (1st row) in Table 4. Upon addi-\ntion of intra-modal contrastive consistency, we no-\ntice a significant boost (+0.87% in base and +0.83\n% in novel) in performance. Therefore, enforcing\nthe mutual synergy between the same modalities re-\nsults in a much better generalization across several\ndatasets. Intra-modal consistency when paired with\nan adapter gives a further improvement (+0.69%\nin base and +0.77 % in novel) in performance sug-\ngesting that they mutually aid each other in the\ngeneralization task. Finally, when cross attention\nis employed in the adapter layers we obtain +0.63\n% and +0.86 % relative improvements in the base\nand novel classes respectively.\nImpact of Prompt Layers: We evaluate the av-\nerage performance of APoLLo on 10 datasets for\nthe base-to-novel generalization with respect to the\nprompt depth, i.e., the number of prompt layers (see\nFigure 3). Unlike MaPLe (Khattak et al., 2023),\nwhile conducting ablation, we consider the same\ndepth (num layers) in both vision and language\nencoders. The general trend suggests that with\nincreased prompt depth, the overall performance\nimproves. Further, unlike MaPLe, we obtain a\nslight improvement in accuracy values for the gen-\neralization task when prompt depth is increased\nfrom 9 to 12. The reason for such an observation\ncan be attributed to the presence of adapter lay-\ners and intra-modal contrastive consistency losses\nwhich negatively impact performance related to in-\ncreasing prompt depth beyond 9. Therefore, we\nmaintain the value of prompt depth to 12 in all our\nexperiments.\nImpact of Adapter Layers: To evaluate the im-\nportance of adapters we employ a dual setup strat-\negy. First, we study the utility of adapters in in-\ndividual modalities (see Appendix Table 8). We\nnotice that adding a text adapter is more beneficial\nthan adding only an image adapter. This is consis-\ntent with the findings outlined in Gao et al. (2021c).\nHowever, unlike them, we notice an increase in\naccuracy values when adapters are used with both\nmodalities. A key difference in this respect (as\ncompared to Gao et al. (2021c)) is the presence\nof cross-attention which plays an important role\nin alignment. This demonstrates the importance\nof yet another component of APoLLo , i.e., the\nintra-modal contrastive consistency. This under-\nlines that dual-modality adapters are not beneficial\nin the case of naive few-shot tuning, whereas our\napproach (having both cross-attention and intra-\nmodal consistency) leads to better generalization\nwith more tunable parameters on both modalities.\nSecond, we study the role of the number of adapter\nlayers using 4 different configurations as shown in\n10179\nFigure 4: Cross-attention visualizations as heatmaps superimposed on the respective original images showing\nhow objects (in red) in text prompts attend to relevant regions in the images. These maps depict the cross-modal\nalignment which improves generalization in downstream tasks.\nFigure 3. Note that increasing adapter depth from\n1 to 2 leads to an improvement in performance in-\ndicating its role in understanding more complex\nrelationships between the two modalities via cross-\nmodal alignment. However, the performance drops\nbeyond the depth of 2 suggesting that overparame-\nterization may lead to an overfitting issue under a\nfew-shot setting.\nImpact of Input Augmentations: Augmentation\nplays a pivotal role in enforcing consistency across\ndifferent branches (Chen et al., 2020; Zbontar et al.,\n2021). For text augmentation, we use two strategies\n– Easy Data Augmentation (Wei and Zou, 2019) and\nLLM-generated augmentation. We show that us-\ning a more descriptive text of the object employing\nLLM is more beneficial for learning intra-modal\nconsistency than using the EDA-based approach\n(see Table 9). For image augmentation, we consider\ntwo schemes – standard image augmentations such\nas cropping, flipping, solarization, and color inver-\nsion, and text-conditioned diffusion-based augmen-\ntation. We observe that the latter performs better in\nour case (Table 9).\nImpact of Loss Balancing Factor: To study the\nimpact of the Loss Balancing (Weighting) Factor\n(α), we report the average accuracy values (on 10\ndatasets) of Base and Novel classes in the Base-\nto-Novel Generalization task. As shown in Figure\n3, an increase in the value of α leads to an in-\ncreased weightage of intra-modal loss compared\nto the inter-modal similarity loss – this leads to\nan improvement in performance. We achieve op-\ntimal results for α = 2 which we use in all the\nexperiments. However, an over-emphasis on the\nintra-modal loss leads to poor cross-modal align-\nment affecting the overall performance.\nDifferent LLMs as Text Augmentors Table 5 re-\nports the performances of ApoLLo under different\nLLM-based text augmenters.\nLLM Text AugmentorAvg.(Base) Avg.(Novel)\nVICUNA 83.16 77.29\nGPT 83.77 78.03\nTable 5: Performance comparison between different\nLLM-based text augmenters.\nAdapters on Different Image and Text Branches\nOur method contains two image and text branches\n(where Image Branch 1, Image Branch 2, Text\nBranch 1, and Text Branch 2 are in order from left\nto right in Figure 2 and represent the augmented im-\nage, original image, original text, and augmented\ntext respectively). Here, we add the following table\nwhich demonstrates the effects of adapter layers\non different image and text branch combinations.\nPlease note that all these combinations involve both\nself- and cross-attentions. We obtain similar accu-\nracy values in two cases: (a) Image Branch 2 -\nText Branch 1 combination, and (b) when all four\nbranches are taken into account. However, we se-\nlect case (a) with Image Branch 2 and Text Branch\n1 combination because the number of trainable pa-\nrameters in this scenario is half that in the latter.\nMoreover, Image Branch 2 - Text Branch 1 combi-\nnation of adapters gives the highest (mean) accu-\nracy values on base and novel classes as shown in\nTable 6.\n10180\nImg. Br. 1Img. Br. 2Txt. Br. 1Txt. Br. 2Avg.(Base) Avg.(Novel)\n✓ ✗ ✓ ✗ 83.28 77.32✗ ✓ ✗ ✓ 83.19 77.24✓ ✗ ✗ ✓ 83.16 77.23✓ ✓ ✓ ✓ 83.77 78.01✗ ✓ ✓ ✗ 83.77 78.03\nTable 6: Impact of adding adapters on different im-\nage and text branches.\nIncorporating Attentions in Adapter Layers\nWe add the following ablations in Table 7 w.r.t\nthe Adapter layers: (a) Adapter layers having no\nattention (b) Adapter layers with only self-attention\n(c) Adapter layers with self + cross attention. Ex-\nperimental results in Table 7) demonstrate optimal\nperformance is achieved in (c) when cross attention\nis employed between the adapters leading to better\nmulti-modal alignment.\nNo-attn.Self-attn.Cross-attn.Avg.(Base) Avg.(Novel)\n✓ ✗ ✗ 82.54 76.98\n✗ ✓ ✗ 83.05 77.22\n✗ ✓ ✓ 83.77 78.03\nTable 7: Impact of incorporating attention in adapter\nlayers.\n4.5 Qualitative Results\nThrough Figure 4 and Appendix Figure 6 we visu-\nalize cross-attention maps guided by the respective\ntext prompts. The cross-attention scores between\nthe image and the associated text token referring to\nthe object (highlighted in red in each prompt) are\nextracted and bilinearly interpolated to match the\nimage dimension and superimposed on the original\nimage. These maps depict the alignment between\nimages and corresponding texts for a variety of\nclasses, including fish, food, and vehicles. Such\ncross-modal awareness leads to better downstream\ntasks.\nQualitative Analysis on Multi-object Setting\nExamples of the cross-attention visualization on\nimages with multiple objects are shown in Figure\n5. Here, each of the images contains more than\none object category. We generate textual prompts\nusing the same template ‘A photo of a <CLASS>’\nfor individual objects as shown in the figure. The\nattention maps are activated in the corresponding re-\ngions showing fine-grained alignment of APoLLo\nunder challenging scenarios with objects from mul-\ntiple categories. Please note that here we did a\nzero-shot transfer on these images with the model\ntrained on ImageNet using our method.\nFigure 5: Cross-attention visualizations as heatmaps\nsuperimposed on the respective original images in a\nmulti-object setting.\n5 Conclusions and Future Work\nWe present APoLLo , a unified multi-modal\nadapter and prompt tuning method to facilitate few-\nshot learning without compromising zero-shot gen-\neralizability. To this end, we leverage contrastive-\nconsistency loss and ensure cross-modal synergy\nwhich results in an improved performance on three\ncarefully chosen downstream tasks: base-to-novel\ngeneralization, cross-dataset evaluation, and do-\nmain generalization on 10 benchmark datasets as\nobserved from the experimental results. Moreover,\nthe ablation analysis outlines the salient contribu-\ntions of each of the sub-modules. Future work can\ninclude assessing the performance of such frame-\nworks for fine-grained tasks which remains an open\nproblem. Moreover, the adapter layers may lead\nto over-parameterization which can be further opti-\nmized in subsequent works.\nAcknowledgement: The research has been sup-\nported by ARO grants W911NF2310352 and\nW911NF2110026.\n10181\nReferences\nShekoofeh Azizi, Simon Kornblith, Chitwan Saharia,\nMohammad Norouzi, and David J Fleet. 2023. Syn-\nthetic data from diffusion models improves imagenet\nclassification. arXiv preprint arXiv:2304.08466.\nHyojin Bahng, Ali Jahanian, Swami Sankaranarayanan,\nand Phillip Isola. 2022. Exploring visual prompts\nfor adapting large-scale models. arXiv preprint\narXiv:2203.17274, 1(3):4.\nHanoona Bangalath, Muhammad Maaz, Muham-\nmad Uzair Khattak, Salman H Khan, and Fahad Shah-\nbaz Khan. 2022. Bridging the gap between object\nand image-level representations for open-vocabulary\ndetection. Advances in Neural Information Process-\ning Systems, 35:33781–33794.\nStella Biderman, Hailey Schoelkopf, Quentin Anthony,\nHerbie Bradley, Kyle O’Brien, Eric Hallahan, Mo-\nhammad Aflah Khan, Shivanshu Purohit, USVSN Sai\nPrashanth, Edward Raff, et al. 2023. Pythia: A suite\nfor analyzing large language models across training\nand scaling. arXiv preprint arXiv:2304.01373.\nLukas Bossard, Matthieu Guillaumin, and Luc Van Gool.\n2014. Food-101–mining discriminative components\nwith random forests. In Computer Vision–ECCV\n2014: 13th European Conference, Zurich, Switzer-\nland, September 6-12, 2014, Proceedings, Part VI 13,\npages 446–461. Springer.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and\nGeoffrey Hinton. 2020. A simple framework for\ncontrastive learning of visual representations. In In-\nternational conference on machine learning, pages\n1597–1607. PMLR.\nZhe Chen, Yuchen Duan, Wenhai Wang, Junjun He,\nTong Lu, Jifeng Dai, and Yu Qiao. 2022. Vision trans-\nformer adapter for dense predictions. arXiv preprint\narXiv:2205.08534.\nMircea Cimpoi, Subhransu Maji, Iasonas Kokkinos,\nSammy Mohamed, and Andrea Vedaldi. 2014. De-\nscribing textures in the wild. In Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition, pages 3606–3613.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. 2009. Imagenet: A large-scale hier-\narchical image database. In 2009 IEEE conference\non computer vision and pattern recognition, pages\n248–255. Ieee.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2020.\nAn image is worth 16x16 words: Transformers\nfor image recognition at scale. arXiv preprint\narXiv:2010.11929.\nZi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang,\nShuohang Wang, Lijuan Wang, Chenguang Zhu,\nPengchuan Zhang, Lu Yuan, Nanyun Peng, et al.\n2022. An empirical study of training end-to-end\nvision-and-language transformers. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 18166–18176.\nLi Fei-Fei, Rob Fergus, and Pietro Perona. 2004. Learn-\ning generative visual models from few training ex-\namples: An incremental bayesian approach tested on\n101 object categories. In 2004 conference on com-\nputer vision and pattern recognition workshop, pages\n178–178. IEEE.\nPeng Gao, Shijie Geng, Renrui Zhang, Teli Ma,\nRongyao Fang, Yongfeng Zhang, Hongsheng Li,\nand Yu Qiao. 2021a. Clip-adapter: Better vision-\nlanguage models with feature adapters. arXiv\npreprint arXiv:2110.04544.\nPeng Gao, Shijie Geng, Renrui Zhang, Teli Ma,\nRongyao Fang, Yongfeng Zhang, Hongsheng Li,\nand Yu Qiao. 2021b. Clip-adapter: Better vision-\nlanguage models with feature adapters. arXiv\npreprint arXiv:2110.04544.\nPeng Gao, Shijie Geng, Renrui Zhang, Teli Ma,\nRongyao Fang, Yongfeng Zhang, Hongsheng Li,\nand Yu Qiao. 2021c. Clip-adapter: Better vision-\nlanguage models with feature adapters. arXiv\npreprint arXiv:2110.04544.\nTianxiang Hao, Hui Chen, Yuchen Guo, and Guiguang\nDing. 2023. Consolidator: Mergeable adapter with\ngrouped connections for visual adaptation. arXiv\npreprint arXiv:2305.00603.\nDan Hendrycks, Steven Basart, Norman Mu, Saurav\nKadavath, Frank Wang, Evan Dorundo, Rahul Desai,\nTyler Zhu, Samyak Parajuli, Mike Guo, et al. 2021a.\nThe many faces of robustness: A critical analysis of\nout-of-distribution generalization. In Proceedings\nof the IEEE/CVF International Conference on Com-\nputer Vision, pages 8340–8349.\nDan Hendrycks, Kevin Zhao, Steven Basart, Jacob Stein-\nhardt, and Dawn Song. 2021b. Natural adversarial\nexamples. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition,\npages 15262–15271.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for nlp. In In-\nternational Conference on Machine Learning, pages\n2790–2799. PMLR.\n10182\nXuming Hu, Aiwei Liu, Zeqi Tan, Xin Zhang, Chenwei\nZhang, Irwin King, and Philip S Yu. 2023. Gda:\nGenerative data augmentation techniques for relation\nextraction tasks. arXiv preprint arXiv:2305.16663.\nTony Huang, Jack Chu, and Fangyun Wei. 2022. Unsu-\npervised prompt learning for vision-language models.\narXiv preprint arXiv:2204.03649.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen\nLi, and Tom Duerig. 2021. Scaling up visual and\nvision-language representation learning with noisy\ntext supervision. In International Conference on\nMachine Learning, pages 4904–4916. PMLR.\nMuhammad Uzair Khattak, Hanoona Rasheed, Muham-\nmad Maaz, Salman Khan, and Fahad Shahbaz Khan.\n2023. Maple: Multi-modal prompt learning. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 19113–19122.\nKonwoo Kim, Michael Laskin, Igor Mordatch, and\nDeepak Pathak. 2021. How to adapt your large-scale\nvision-and-language model.\nJonathan Krause, Michael Stark, Jia Deng, and Li Fei-\nFei. 2013. 3d object representations for fine-grained\ncategorization. In Proceedings of the IEEE inter-\nnational conference on computer vision workshops,\npages 554–561.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. arXiv preprint arXiv:2104.08691.\nHao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hong-\nsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao, Xi-\naogang Wang, Wenhai Wang, et al. 2023a. Uni-\nperceiver v2: A generalist model for large-scale vi-\nsion and vision-language tasks. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 2691–2700.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven\nHoi. 2022a. Blip: Bootstrapping language-image\npre-training for unified vision-language understand-\ning and generation. In International Conference on\nMachine Learning, pages 12888–12900. PMLR.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. arXiv\npreprint arXiv:2101.00190.\nYanghao Li, Haoqi Fan, Ronghang Hu, Christoph\nFeichtenhofer, and Kaiming He. 2023b. Scaling\nlanguage-image pre-training via masking. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 23390–23400.\nYanghao Li, Hanzi Mao, Ross Girshick, and Kaim-\ning He. 2022b. Exploring plain vision transformer\nbackbones for object detection. In Computer Vision–\nECCV 2022: 17th European Conference, Tel Aviv,\nIsrael, October 23–27, 2022, Proceedings, Part IX,\npages 280–296. Springer.\nXiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam,\nZhengxiao Du, Zhilin Yang, and Jie Tang. 2021a.\nP-tuning v2: Prompt tuning can be comparable to\nfine-tuning universally across scales and tasks. arXiv\npreprint arXiv:2110.07602.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\nZheng Zhang, Stephen Lin, and Baining Guo. 2021b.\nSwin transformer: Hierarchical vision transformer\nusing shifted windows. In Proceedings of the\nIEEE/CVF international conference on computer vi-\nsion, pages 10012–10022.\nYuning Lu, Jianzhuang Liu, Yonggang Zhang, Yajing\nLiu, and Xinmei Tian. 2022. Prompt distribution\nlearning. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition,\npages 5206–5215.\nSubhransu Maji, Esa Rahtu, Juho Kannala, Matthew\nBlaschko, and Andrea Vedaldi. 2013. Fine-grained\nvisual classification of aircraft. arXiv preprint\narXiv:1306.5151.\nMaria-Elena Nilsback and Andrew Zisserman. 2008.\nAutomated flower classification over a large number\nof classes. In 2008 Sixth Indian Conference on Com-\nputer Vision, Graphics & Image Processing, pages\n722–729. IEEE.\nOmkar M Parkhi, Andrea Vedaldi, Andrew Zisserman,\nand CV Jawahar. 2012. Cats and dogs. In 2012\nIEEE conference on computer vision and pattern\nrecognition, pages 3498–3505. IEEE.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from\nnatural language supervision. In International confer-\nence on machine learning, pages 8748–8763. PMLR.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485–5551.\nSylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea\nVedaldi. 2017. Learning multiple visual domains\nwith residual adapters. Advances in neural informa-\ntion processing systems, 30.\nSylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea\nVedaldi. 2018. Efficient parametrization of multi-\ndomain deep neural networks. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern\nRecognition, pages 8119–8127.\n10183\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt,\nand Vaishaal Shankar. 2019. Do imagenet classifiers\ngeneralize to imagenet? In International conference\non machine learning, pages 5389–5400. PMLR.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Björn Ommer. 2022. High-\nresolution image synthesis with latent diffusion mod-\nels. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages\n10684–10695.\nAmir Rosenfeld and John K Tsotsos. 2018. Incremen-\ntal learning through deep adaptation. IEEE transac-\ntions on pattern analysis and machine intelligence,\n42(3):651–663.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nSheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne\nLongpre, Jason Wei, Hyung Won Chung, Barret\nZoph, William Fedus, Xinyun Chen, et al. 2023.\nFlan-moe: Scaling instruction-finetuned language\nmodels with sparse mixture of experts.arXiv preprint\narXiv:2305.14705.\nJordan Shipard, Arnold Wiliem, Kien Nguyen Thanh,\nWei Xiang, and Clinton Fookes. 2023. Diversity is\ndefinitely needed: Improving model-agnostic zero-\nshot classification via stable diffusion. In Proceed-\nings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 769–778.\nManli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom\nGoldstein, Anima Anandkumar, and Chaowei Xiao.\n2022. Test-time prompt tuning for zero-shot gener-\nalization in vision-language models. arXiv preprint\narXiv:2209.07511.\nKhurram Soomro, Amir Roshan Zamir, and Mubarak\nShah. 2012. Ucf101: A dataset of 101 human ac-\ntions classes from videos in the wild. arXiv preprint\narXiv:1212.0402.\nYi-Lin Sung, Jaemin Cho, and Mohit Bansal. 2022.\nVl-adapter: Parameter-efficient transfer learning for\nvision-and-language tasks. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 5227–5237.\nMingxing Tan and Quoc Le. 2019. Efficientnet: Re-\nthinking model scaling for convolutional neural net-\nworks. In International conference on machine learn-\ning, pages 6105–6114. PMLR.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nBrandon Trabucco, Kyle Doherty, Max Gurinas, and\nRuslan Salakhutdinov. 2023. Effective data aug-\nmentation with diffusion models. arXiv preprint\narXiv:2302.07944.\nHaohan Wang, Songwei Ge, Zachary Lipton, and Eric P\nXing. 2019. Learning robust global representations\nby penalizing local predictive power. Advances in\nNeural Information Processing Systems, 32.\nJinpeng Wang, Pan Zhou, Mike Zheng Shou, and\nShuicheng Yan. 2023a. Position-guided text prompt\nfor vision-language pre-training. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 23242–23251.\nWenhui Wang, Hangbo Bao, Li Dong, Johan\nBjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal,\nOwais Khan Mohammed, Saksham Singhal, Subhojit\nSom, et al. 2023b. Image as a foreign language: Beit\npretraining for vision and vision-language tasks. In\nProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 19175–\n19186.\nChen Wei, Huiyu Wang, Wei Shen, and Alan Yuille.\n2020. Co2: Consistent contrast for unsuper-\nvised visual representation learning. arXiv preprint\narXiv:2010.02217.\nJason Wei and Kai Zou. 2019. Eda: Easy data augmenta-\ntion techniques for boosting performance on text clas-\nsification tasks. arXiv preprint arXiv:1901.11196.\nChenxi Whitehouse, Monojit Choudhury, and Al-\nham Fikri Aji. 2023. Llm-powered data augmen-\ntation for enhanced crosslingual performance. arXiv\npreprint arXiv:2305.14288.\nJianxiong Xiao, James Hays, Krista A Ehinger, Aude\nOliva, and Antonio Torralba. 2010. Sun database:\nLarge-scale scene recognition from abbey to zoo. In\n2010 IEEE computer society conference on computer\nvision and pattern recognition, pages 3485–3492.\nIEEE.\nZhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei\nHu, Faisal Ahmed, Zicheng Liu, Yumao Lu, and Li-\njuan Wang. 2022. Unitab: Unifying text and box\noutputs for grounded vision-language modeling. In\nComputer Vision–ECCV 2022: 17th European Con-\nference, Tel Aviv, Israel, October 23–27, 2022, Pro-\nceedings, Part XXXVI, pages 521–539. Springer.\nHantao Yao, Rui Zhang, and Changsheng Xu. 2023.\nVisual-language prompt tuning with knowledge-\nguided context optimization. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 6757–6767.\nYuwei Yin, Jean Kaddour, Xiang Zhang, Yixin Nie,\nZhenguang Liu, Lingpeng Kong, and Qi Liu. 2023.\nTtida: Controllable generative data augmentation via\ntext-to-text and text-to-image models. arXiv preprint\narXiv:2304.08821.\n10184\nLu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,\nXiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong\nHuang, Boxin Li, Chunyuan Li, et al. 2021. Florence:\nA new foundation model for computer vision. arXiv\npreprint arXiv:2111.11432.\nYuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, and\nChen Change Loy. 2022. Open-vocabulary detr with\nconditional matching. In Computer Vision–ECCV\n2022: 17th European Conference, Tel Aviv, Israel,\nOctober 23–27, 2022, Proceedings, Part IX, pages\n106–122. Springer.\nJure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and\nStéphane Deny. 2021. Barlow twins: Self-supervised\nlearning via redundancy reduction. In International\nConference on Machine Learning, pages 12310–\n12320. PMLR.\nRenrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao,\nKunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng\nLi. 2021. Tip-adapter: Training-free clip-adapter\nfor better vision-language modeling. arXiv preprint\narXiv:2111.03930.\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and\nZiwei Liu. 2022a. Conditional prompt learning\nfor vision-language models. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 16816–16825.\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and\nZiwei Liu. 2022b. Learning to prompt for vision-\nlanguage models. International Journal of Computer\nVision, 130(9):2337–2348.\nXingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp\nKrähenbühl, and Ishan Misra. 2022c. Detecting\ntwenty-thousand classes using image-level supervi-\nsion. In Computer Vision–ECCV 2022: 17th Eu-\nropean Conference, Tel Aviv, Israel, October 23–27,\n2022, Proceedings, Part IX, pages 350–368. Springer.\nBeier Zhu, Yulei Niu, Yucheng Han, Yue Wu, and\nHanwang Zhang. 2022. Prompt-aligned gradient for\nprompt tuning. arXiv preprint arXiv:2205.14865.\n10185\nA Appendix\nA.1 Details for Radar Chart Figure 1\nIn this section, we explain the details of the radar\nchart shown in Figure 1. Each axis denotes an accu-\nracy value, either on base or novel class (specified\naccordingly) for the Base-to-Novel Generalization\ntask. In total, there are 10 datasets for this task and\ntherefore we plot 10 vertices where each denotes\na ratio relative to our performance. This is calcu-\nlated on the basis of normalizing the performance\nof either APoLLo or previous SOTA (i.e., MaPLe)\nby that of APoLLo , and is therefore kept in the\nrange of (0, 1]. Here we set the radar chart’s origin\nto be 90% and the outermost frame to be 100%.\nThis essentially separates the adjacent frames for\nbetter readability. The accuracy values annotated\nagainst the vertices are the absolute values (without\nnormalization) obtained using both methods.\nA.2 Intra-modal Contrastive Consistency\nLoss\nOur intra-modal contrastive consistency loss differs\nfrom a conventional contrastive loss in the aspect\nthat it contains consistency terms that preventclass-\ncollision problem (Wei et al., 2020). Therefore, it\nhas two terms - a contrastive loss and a consistency\nloss.\nA.2.1 Contrastive Loss\nA typical contrastive loss function in the form of\nan InfoNCE loss is given as:\nLNCE = −log exp(q ·p/τ)\nexp(q ·p/τ) + ∑\nnk\nexp(q ·nk/τ)\n(8)\nwhere p represents the positive key,q represents\nthe query, and nk represents the negative key in a\nminibatch.\nA.2.2 Consistency Loss\nTaking inspiration from semi-supervised learning,\nconsistency loss is proposed by Wei et al. (2020) to\nstrengthen the consistency between the similarities\nof the query data and the positive data.\nThe similarity between the query q and the neg-\native keys nk can be represented in the form of a\nprobability Q(i) which is denoted as:\nQ(i) = exp(q ·ni/τcon)∑\nnk\nexp(q ·nk/τcon) (9)\nThe similarity between the positive p and the neg-\native keys ni(i∈{1,2,...,K }) in the form of a\nprobability P(i) is written as:\nP(i) = exp(p ·ni/τcon)∑\nnk\nexp(p ·nk/τcon) (10)\nConsistency between the probability distributions\nPand Qis imposed in the form of a Symmetric\nKL Divergence Loss.\nLconsistency = 1\n2(KL(P,Q)+ KL(Q,P)) (11)\nThe predicted similarity distribution of the positive\nkey to each crop of the other data, P, acts as a\nsoft pseudo label to that of the query, Q. The total\nloss is a weighted combination of contrastive and\nconsistency losses given as:\nLCO2 = LNCE + βLconsistency (12)\nwhere βis the balancing coefficient. We follow the\nrecommended values of τcon and βas mentioned in\nWei et al. (2020) since they have shown to perform\nthe best in our experiments.\nA.3 Inter-modal Similarity Maximization\nDuring training we consider a mini-batch (N) con-\ntaining image-captions pairs, {Ij,Tj}N\nj=1, where\nIj and Tj represent jth image and text pair, respec-\ntively. After passing these image-text pairs through\nrespective encoders (and adapter layers) we obtain\nthe normalized image embedding as zI\nj ∈Rd and\ntext embedding as zT\nj ∈Rd. These representa-\ntions are also aware of each other (modality-wise).\nThey are subsequently used to compute this inter-\nmodality Image-Text Contrastive (ITC) loss given\nas LIT\nsim.\nLIT\nsim = − 1\n2N\nN∑\nj=1\nlog\n\n\nexp\n(\n⟨zI\nj ,zT\nj ⟩/τ\n)\n∑N\nl=1 exp\n(\n⟨zI\nj ,zT\nl ⟩/τ\n)\n\n\n  \nContrasting images with the texts\n− 1\n2N\nN∑\nl=1\nlog\n\n exp\n(\n⟨zI\nl ,zT\nl ⟩/τ\n)\n∑N\nj=1 exp\n(\n⟨zI\nj ,zT\nl ⟩/τ\n)\n\n\n  \nContrasting texts with the images\n(13)\nwhere ⟨·,·⟩denotes inner product, and τ is the\ntemperature parameter.\n10186\nFigure 6: Cross-attention visualizations as heatmaps superimposed on the respective original images showing\nhow objects (in red) in text prompts attend to relevant regions in the images (Extension of Figure 4).\nA.4 Generative Data Augmentation\nGenerative data augmentation is an emerging field\nwhere generative models are used to generate aug-\nmented (perturbed) samples of the data (Hu et al.,\n2023; Trabucco et al., 2023; Azizi et al., 2023;\nShipard et al., 2023; Yin et al., 2023; Whitehouse\net al., 2023). It is prominent in the field of Natural\nLanguage Processing where Large Language Mod-\nels (LLMs) are used as data augmenters (White-\nhouse et al., 2023; Yin et al., 2023). Given a\nprompt, LLMs provide several descriptive versions\nof it which can potentially be used to increase\nthe size of the dataset, especially in a low-dataset\nregime. Further, text-to-image generation and dif-\nfusion models (Yin et al., 2023; Trabucco et al.,\n2023; Azizi et al., 2023; Shipard et al., 2023) are\nalso used for effective data augmentation strategies.\nHowever, these types of augmentation schemes\nhave not been explored together previously in a\nmulti-modal (vision-language) context. To the best\nof our knowledge, ours is the first to leverage the\ncapabilities of LLMs and text-conditioned image\ngeneration (diffusion) models together in a unified\nframework for effective augmentation in a multi-\nmodal learning context.\nA.5 Implementation Details\nFollowing MaPLe (Khattak et al., 2023), we uti-\nlize the pre-trained ViT-B/16 CLIP model as our\nvision-language backbone. In all experiments, we\nfine-tune the model employing a few-shot train-\ning strategy with 16 samples randomly sampled\nfrom each class for all known classes. We train\nour model on a single GPU for 10 epochs using an\nSGD optimizer with a base learning rate of 0.004.\nWe follow MaPLe for setting the values of prompt\ndepths and lengths in our experiments.\nA.6 Ablation Tables\nA.6.1 Impact of Adapter Layers\nThe impact of adapter layers is shown in Table 8.\nImage Text Average\nBase Novel\n✓ ✗ 83.16 77.31\n✗ ✓ 83.41 77.48\n✓ ✓ 83.77 78.03\nTable 8: Impact of using different adapters on the\nperformance of APoLLo . Our method gives the best\naccuracy values when both image and language adapters\nare used.\nA.6.2 Impact of Input Augmentations\nImpact of input augmentation is shown in Table 9.\nModality Augmentation Average\nBase Novel\nImage Standard 83.28 77.45\nDiffusion 83.77 78.03\nText EDA 82.81 77.04\nLLM 83.77 78.03\nTable 9: Impact of different input augmentation\nstrategies on the performance of APoLLo . Our\nmethod gives the best accuracy values when LLM is\nused for text augmentation and the text-condition image\ngeneration model is used for image augmentation.\n10187",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8186718225479126
    },
    {
      "name": "Adapter (computing)",
      "score": 0.6892479658126831
    },
    {
      "name": "Generalization",
      "score": 0.5924952626228333
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5819377899169922
    },
    {
      "name": "Overfitting",
      "score": 0.5583437085151672
    },
    {
      "name": "Encoder",
      "score": 0.4800727963447571
    },
    {
      "name": "Modalities",
      "score": 0.45667123794555664
    },
    {
      "name": "Machine learning",
      "score": 0.36858969926834106
    },
    {
      "name": "Artificial neural network",
      "score": 0.13543885946273804
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ]
}