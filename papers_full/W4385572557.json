{
  "title": "Second Language Acquisition of Neural Language Models",
  "url": "https://openalex.org/W4385572557",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5104152387",
      "name": "Miyu Oba",
      "affiliations": [
        "Nara Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5062574320",
      "name": "Tatsuki Kuribayashi",
      "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence",
        "Tohoku University"
      ]
    },
    {
      "id": "https://openalex.org/A5109372838",
      "name": "Hiroki Ouchi",
      "affiliations": [
        "Nara Institute of Science and Technology",
        "RIKEN"
      ]
    },
    {
      "id": "https://openalex.org/A5102510525",
      "name": "Taro Watanabe",
      "affiliations": [
        "Nara Institute of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2144862731",
    "https://openalex.org/W3034775979",
    "https://openalex.org/W1526974435",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2788924045",
    "https://openalex.org/W3104570641",
    "https://openalex.org/W2127863960",
    "https://openalex.org/W2040086463",
    "https://openalex.org/W2345767696",
    "https://openalex.org/W2884554299",
    "https://openalex.org/W3173829323",
    "https://openalex.org/W4385572950",
    "https://openalex.org/W2571532437",
    "https://openalex.org/W2251743902",
    "https://openalex.org/W4385573436",
    "https://openalex.org/W2595715041",
    "https://openalex.org/W4205537036",
    "https://openalex.org/W162778178",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W3035064549",
    "https://openalex.org/W2142990334",
    "https://openalex.org/W2180877453",
    "https://openalex.org/W4226089239",
    "https://openalex.org/W2140059918",
    "https://openalex.org/W3035807844",
    "https://openalex.org/W2996728628",
    "https://openalex.org/W4232979704",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W630532510",
    "https://openalex.org/W3042795397",
    "https://openalex.org/W2615234252",
    "https://openalex.org/W89093025",
    "https://openalex.org/W2753713012",
    "https://openalex.org/W4303648904",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W3208641770",
    "https://openalex.org/W2989539713",
    "https://openalex.org/W3122458782",
    "https://openalex.org/W2805159187"
  ],
  "abstract": "With the success of neural language models (LMs), their language acquisition has gained much attention. This work sheds light on the second language (L2) acquisition of LMs, while previous work has typically explored their first language (L1) acquisition. Specifically, we trained bilingual LMs with a scenario similar to human L2 acquisition and analyzed their cross-lingual transfer from linguistic perspectives. Our exploratory experiments demonstrated that the L1 pretraining accelerated their linguistic generalization in L2, and language transfer configurations (e.g., the L1 choice, and presence of parallel texts) substantially affected their generalizations. These clarify their (non-)human-like L2 acquisition in particular aspects.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 13557‚Äì13572\nJuly 9-14, 2023 ¬©2023 Association for Computational Linguistics\nSecond Language Acquisition of Neural Language Models\nMiyu Oba1 Tatsuki Kuribayashi2,3 Hiroki Ouchi1,4 Taro Watanabe1\n1Nara Institute of Science and Technology\n2MBZUAI 3Tohoku University 4RIKEN\n{oba.miyu.ol2, hiroki.ouchi, taro}@is.naist.jp\ntatsuki.kuribayashi@mbzuai.ac.ae\nAbstract\nWith the success of neural language models\n(LMs), their language acquisition has gained\nmuch attention. This work sheds light on the\nsecond language (L2) acquisition of LMs,\nwhile previous work has typically explored\ntheir first language (L1) acquisition. Specif-\nically, we trained bilingual LMs with a sce-\nnario similar to human L2 acquisition and ana-\nlyzed their cross-lingual transfer from linguis-\ntic perspectives. Our exploratory experiments\ndemonstrated that the L1 pretraining acceler-\nated their linguistic generalization in L2, and\nlanguage transfer configurations (e.g., the L1\nchoice, and presence of parallel texts) substan-\ntially affected their generalizations. These clar-\nify their (non-)human-like L2 acquisition in\nparticular aspects.1\n1 Introduction\nCross-lingual transferability of language models\n(LMs) has attracted much attention. For example,\nlarge English LMs show some translation perfor-\nmance even when using a small amount of non-\nEnglish languages as training data (Brown et al.,\n2020; Shi et al., 2023), which indicates the efficient\nlanguage transfer from English to others. Such\ncross-lingual transferability has been evaluated by\nholistic measures, such as perplexity and accuracy\non downstream tasks (Papadimitriou and Jurafsky,\n2020; Deshpande et al., 2022; Blevins et al., 2022).\nOn the other hand, there is much room for inves-\ntigating them from linguistic perspectives ; e.g.,\ngrammatical knowledge acquisition and language\ntransfer tendencies among languages.\nIn this study, we investigate the cross-lingual\ntransferability of LMs from a perspective of sec-\nond language (L2) acquisition. Our main research\nquestion is how first language (L1) acquisition of\nLMs affects the efficiency of grammar acquisition\n1Our codes are available at /gtbhttps://github.com/\nmlieynua/sla-of-nlm\n \n  \nDifferentL1 modelsExposure to L2 (English)\nTest L2 knowledge\nLM in Fr\n√á√†LM in Ge\n√ú√ü LM in Ja\n„ÅÇ„Ç¢LM in Ru\n–î–∏\nüéì√á√†Aa\nüéì√ú√üAa\nüéì–î–∏Aa\nüéì„ÅÇ„Ç¢Aa\nAaAaAa√á√† –î–∏„ÅÇ„Ç¢&& Aa√ú√ü&&Corpus\n: Benchmark for evaluating   syntactic abilitiesBLiMPAa\nFigure 1: Experimental Procedure. First, we pretrain\nthe monolingual masked language model on the first lan-\nguage (first language acquisition; L1 acquisition). Then,\nthe model is additionally trained under the bilingual\nsetting (second language acquisition; L2 acquisition).\nFinally, we analyze the effect of L1 on L2 via a gram-\nmatical judgment test in L2.\nin L2. To answer this question, we design an ex-\nperimental procedure (Section 2): (i) pretraining\nLMs in a certain language (assumed to be the L1\nspeakers), (ii) further training them in English as\nan L2, and (iii) evaluating and analyzing their lin-\nguistic generalization in L2. As L1s, we chose four\nlanguages with different levels of difficulty in trans-\nferring to English, i.e., French, German, Russian,\nand Japanese. The size of training data is restricted\nto match the human-like L2 acquisition scenario,\nwhich enables better comparison with human L2\nacquisition tendencies and, hopefully, provides in-\nsights into L2 acquisition from a computational\nlinguistic perspective.\nWe begin with exploring the inductive biases of\nseveral L2 training methods (Section 3). Specif-\nically, we compared some variations of L2 data\nsettings, such as training on only the L2 texts or\non L1‚ÄìL2 translation pairs. We observed that, for\nexample, feeding L1‚ÄìL2 translation pairs into LMs\n13557\nslowed down their L2 grammar acquisition, com-\npared to only feeding L2 monolingual texts every\ntwo epochs.\nIn our main experiments, we conducted ex-\nploratory analyses of the effects of L1 training on\nL2 grammar acquisition (Section 4). We gained\nthree main insights. First, L1 knowledge pro-\nmotes better linguistic generalization in L2 (Sec-\ntion 4.1). Second, different L1s incur differ-\nent generalizations in L2 (Section 4.2). More\nspecifically, Japanese and Russian are far behind\nFrench and German, which is consistent with the\nhuman-defined difficulty levels of language trans-\nfer (Chiswick and Miller, 2004). Third, L1 pre-\ntraining gives different effects on different types of\ngrammar items (Section 4.3). In particular, mor-\nphological and syntactic items get larger gains than\nsemantic and syntax&semantic items.\nIn more detail, we analyzed the process of L2\nacquisition (Section 5). We investigated how L2\nknowledge acquisition progresses (Section 5.1)\nand found that L2 knowledge acquisition does not\nprogress so much until seeing the whole dataset\nmany times (e.g., 50-100 times), implying their\ndata inefficiency. Furthermore, we also observed\nthe L1 knowledge degrade during L2 acquisition;\nthis motivates us to balance the source‚Äìtarget lin-\nguistic knowledge during language transfer.\n2 Second language acquisition of LMs\nOverview: We are interested in how L1 knowl-\nedge affects the linguistic generalization of LMs\nin L2. Figure 1 shows an overview of the exper-\nimental procedure. First, in our L1 acquisition\nsimulation, we train LMs on a monolingual cor-\npus of a specific language. Second, in our L2\nacquisition simulation, we additionally train the\npretrained LMs with a corpus including L2 texts\n(English). Finally, we evaluate the grammatical\njudgment ability of the LMs in the L2 (English)\nusing BLiMP (Warstadt et al., 2020).\n2.1 Language exposure\nFirst and second languages: We used French,\nGerman, Russian, and Japanese as L1 and em-\nployed English as L2 (Table 1). We expect that\nthe transfer to English becomes more difficult in or-\nder of French, German, Russian, and Japanese from\nmultiple perspectives: linguistic distance (Grimes\nand Grimes, 2002; Chiswick and Miller, 2004) and\nLang. Family Order Script Rank\nFrench IE SVO Alphabet 1\nGerman IE SOV Alphabet 2\nRussian IE SVO Cyrillic 3\nJapanese N-IE SOV Kana/Kanji 4\nEnglish IE SVO Alphabet -\nTable 1: Characteristics of the four languages used in\nour experiment. English is employed as L2, and the oth-\ners are L1s. ‚ÄúRank‚Äù indicates the transfer difficulty from\nthe corresponding language to English, based on the lin-\nguistic distance and FSI rank; a higher value indicates\na greater gap to English from the language acquisition\nperspective. ‚ÄúFamily‚Äù indicates if Indo-European (IE)\nor not (N-IE). ‚ÄúOrder‚Äù indicates canonical word order\nin the corresponding language.\nlearning difficulty level2.\nL1 acquisition: We first train LMs in particular\nL1 language using a monolingual corpus of approx-\nimately 100M words sampled from CC-100 (Con-\nneau et al., 2020; Wenzek, 2020). The corpus size\nis roughly similar to the number of words exposed\nto humans during language acquisition. We trained\nthe models with 100 epochs.3\nL2 acquisition: We then further train the L1\nLMs under bilingual input (Section 3). We trained\nthe models with 100 epochs, but the intermedi-\nate checkpoints will also be analyzed to track the\nprocess of L2 acquisition in Section 5. We used\nTatoeba (Tiedemann, 2012)4 as parallel corpora\nin L2 acquisition. Tatoeba is a multilingual paral-\nlel corpus consisting of example sentences origi-\nnally collected for human foreign language learners.\nFrom the L2 acquisition perspective, this amount\nwould be large enough for human learners to learn\nthe top 95% English words in frequency (Nation,\n2014).\nNote that there would be several scenarios of\nhuman L2 learning/acquisition, such as through\n2https://www.state.gov/\nforeign-language-training/ Note that these diffi-\nculty levels only indicate the difficulty of transferring from\nEnglish to a specific language. In our study, we tentatively\nassume the symmetry of the source and target language in\nterms of learning difficulty.\n3This number of epochs might be cognitively-implausible\nsince humans would not face the same example 100 times,\nbut it is also argued that the memory of language experience\ncontinues to affect the learning multiple times (Bybee, 2013).\n4https://opus.nlpl.eu/Tatoeba.php\n13558\nlanguage classes or natural communications. Fol-\nlowing Krashen et al. (1979), we refer to L2 acqui-\nsition as the latter scenario of acquiring L2 through\nnatural language exposure, e.g., raw texts.\n2.2 Learners\nWe largely followed the settings of the cross-lingual\nlanguage model (XLM) (Conneau and Lample,\n2019), which is typically used in cross-lingual lan-\nguage modeling in the field of natural language\nprocessing (NLP). In short, this is a Transformer-\nbased bidirectional LM, but the input consists of\nbilingual text pairs. The tokens in the bilingual text\nwere randomly masked, and the model predicts the\nmasked tokens on both L1 and L2 sides. During\nthe L1 training, the L1 side is the only input.\nThe bilingual XLM is trained from scratch (L1\ntraining and L1‚ÄìL2 training), rather than using the\noff-the-shelf pre-trained XLM that is trained across\ndozens of languages (Conneau and Lample, 2019;\nConneau et al., 2020). From a cognitive perspec-\ntive, such a super-multilingual setting is unrealistic\nsince humans hardly face dozens of languages in a\nmultilingual acquisition scenario. Rather, we hope\nthat such a bilingual transfer will gain much at-\ntention from the adjacent areas, such as pedagogy\nand cognitive science of exploring human second\nlanguage acquisition/learning.\nTechnically, we randomly initialized the param-\neters of the XLM (18M), constructed a bilingual\ntokenizer using byte pair encoding on the bilingual\ntexts, and trained the model separately for each\nL1‚ÄìL2 combination. For each L1‚ÄìL2 setting, we\ntrained four models with different seeds; the re-\nsults reported in Sections 4 and 5 are the average of\nscores from four models. See Table 5 in Appendix\nfor the hyperparameters and detailed settings.\n2.3 Evaluation\nDataset: We used BLiMP (Warstadt et al., 2020),\na benchmark of English grammatical judgment test\nto evaluate the models‚Äô L2 linguistic generaliza-\ntion. The dataset consists of 12 test suites; each\ncorresponds to a specific linguistic phenomenon\nand falls into one of four coarse linguistic cate-\ngories: morphology, syntax, semantics, and syn-\ntax&semantics. Each test suite has 1,000 minimal\nsentence pairs. Each pair consists of grammatically\nacceptable and unacceptable ones as follows:\n(1) a. Many teenagers were helping themselves.\nb. * Many teenagers were helping herself.\n</s>I [MASK]hungry\nam\n</s>\n</s>Elle[MASK]‚Ä¶ speaks‚Ä¶\nparleShe\n</s>[MASK]\n‚áÑ\n(iii) TLM-drop</s>Elle[MASK]francais[MASK]speaksFrench</s></s></s>\n(ii) TLM-para\n(i) TLM-nopara\n</s>Elle[MASK]francaisI [MASK]hungry</s>\nam\n</s></s>\n√á√†Aa\nAa\n√á√†Aa\n√á√†Aa\nparle\nparle\nShe\nFigure 2: Training settings investigated in Section 3: (i)\nL1‚ÄìL2 text pairs without translation relationship (TLM-\nnopara), (ii) translation pairs (TLM-para), and (iii) a\nmixed setting where parallel L1 text is removed every\nother epoch (TLM-drop).\nGrammatical judgement: To select one sen-\ntence in each pair, we adopted pseudo-perplexity,\ncommonly used in exploring the linguistic behav-\niors of LMs (Lau et al., 2020). Specifically, if the\nmodel can assign a lower pseudo-perplexity to the\ngrammatical sentence than to the paired ungram-\nmatical one, we regard it as correct. Following\nSalazar et al. (2020), psuedo-perplexity (PPPL) of\nsentence s = [w1,w2,¬∑¬∑¬∑ ,wn] is computed using\nthe bidirectional LM Œ∏:\nPPPL(s) =\nn‚àè\nt=1\npŒ∏(wt|s\\wt )\n1\n|s| , (1)\nwhere wt denotes the t-th token in sentence, and\ns\\wt denotes all the tokens in the sentence except\nfor wt; [w1,¬∑¬∑¬∑ ,wt‚àí1,wt+1,¬∑¬∑¬∑ ,wn]. The prob-\nability of wt given its bidirectional context s\\wt\nis calculated by the model Œ∏. Based on the se-\nlected sentences, we calculated an accuracy score\non each test suit of BliMP. We also report the\nmacro-average of accuracies among all the test\nsuits. Note that all the accuracy scores reported\nin the tables/figures in this paper are multiplied by\n100 for readability.\n13559\n3 Preliminary experiment: L2 exposure\nconfigurations\nFirst, we investigate the inductive bias of L2 train-\ning settings. While existing studies use parallel\ndata as an input for cross-lingual training (Conneau\nand Lample, 2019), we investigate the bias in this\nsetting from L2 grammar acquisition perspectives.\nSettings: We set up the three training settings\nwith different input data: (i) L1‚ÄìL2 text pairs with-\nout the translation relationship (TLM-nopara), (ii)\nL1-L2 translation pairs (TLM-para), and (iii) a\nmixed setting where L2 text concatenated with L1\nparallel text is used as input or only L2 text is used\nas input (TLM-drop)5. An overview of the exper-\nimental settings is shown in Figure 2 (see details\nin Appendix A). Note that the original XLM (Con-\nneau and Lample, 2019) adopts a setting similar to\nthe TLM-drop. In this experiment, we report the\nmacro-average BLiMP accuracies across the test\nsuites. Table 2 shows the results (see Table 7 for\nthe results in fine-grained test suits).\nTranslation pairs does not facilitate L2 acqui-\nsition: One notable point in Table 2 is that the\nresults in the TLM-nopara setting were better than\nthose in the TLM-para setting.6 This suggests that\nparallel data input does not facilitate L2 acquisi-\ntion. Perhaps, the TLM-para task was too easy for\nLMs to learn syntactic knowledge; the TLM-para\ntask could partially be solved solely by relying on\nlexical knowledge, i.e., capturing the lexical cor-\nrespondences between the tokens in L1 and L2\nsentences and predicting the word found only in\none of them. In this sense, the TLM-nopara setting,\nby contrast, might impose a more difficult prob-\nlem on LMs and promote their effective learning\nof linguistic knowledge.\nSwitching between using L2 text with and with-\nout its parallel L1 text during training facilitates\nL2 acquisition: Another notable point is that the\nTLM-drop was the most effective for acquiring lin-\nguistic knowledge in L2 for LMs.6 Since we switch\nbetween using L1 text as input or not every epoch,\nthere is a possibility that monolingual and bilingual\ntraining have a complementary positive effect. In\n5In other words, the bilingual and monolingual settings are\nswitched alternately for each epoch.\n6Statistical differences between the settings are tested\nacross seeds√ólanguages with Mann-Whitney U tests (p=4.6e-\n2 for TLM-para and TLM-nopara, p=1.0e-2 for TLM-nopara\nand TLM-drop)\nModel\n(TLM)\nSettings First language\npara. drop Fr De Ru Ja\nnopara 52.0 57 .6 51 .2 52 .5\npara ‚úì 51.1 53 .6 48 .9 51 .3\ndrop ‚úì ‚úì 58.0 61.1 52.8 56.2\nTable 2: Performance of bilingual LMs on BLiMP in\ndifferent training settings. The para. column indicates\nwhether parallel corpus was used. The drop column\nindicates whether the L2-side input is removed every\nother epoch.\naddition, this might mitigate the training-inference\nmismatch in evaluating LMs‚Äô linguistic knowledge\nusing BLiMP. Here, a single sentence is used as\ninput, which is compatible with the phase of using\nonly L2 text during the TLM-drop training. In sub-\nsequent experiments, we will use the TLM-drop\nsetting as it was the most effective training setting\nfor L2 grammar acquisition.\n4 Experiments: L1 ‚ÜíL2 effects on\nlinguistic generalization\nWe investigate howpretraining with L1 affects L2\ngrammar acquisition in LMs. We exploratorily\ncompare the linguistic generalization of LM trained\nin the settings with or without L1 pretraining. Ta-\nble 3 shows the grammaticality judgment ability\nafter additional training. The OVERALL column\nindicates the macro-average accuracy score across\nthe grammar items. The ‚àÜ rows show the differ-\nence in BLiMP accuracy between models with and\nwithout pretraining. Here, the models without pre-\ntraining were trained only with bilingual corpus\nwithout L1 monolingual corpus pretraining.\n4.1 L1s promote L2 generalization\nTable 3 shows the effect of pretraining with L1 on\nL2 grammar acquisition. Most of the ‚àÜ values\nare positive; i.e., the models pretrained with L1s\nachieved better results than those without pretrain-\ning. This demonstrates that pretraining in a par-\nticular language generally improves English gram-\nmatical ability.7 This positive effect is in light\nof the assumptions that different languages share\nsome grammatical universals, and learners could\nuse such a common property in language trans-\n7The standard deviation over the different seeds is an aver-\nage of 1.2 (0‚Äì100 scale) for the scores in Table 3, which means\nthat the seed randomness does not overturn the conclusion.\n13560\nMorphology Syntax Semantics Syntax & Sem.\nLang. L1 OVERALLANA\n. AGR\nD-\nN AGR\nIRREGULARS-\nV AGR\nARG\n. STR\nELLIPSISFILLER\n-GAP\nISLANDNPI QUANTIFIERSBINDINGCTRL\n. R\nAIS\n.\nFr ‚úì 58.0 55 .8 69 .5 73 .0 60 .4 55 .4 67 .7 54 .6 52 .2 40 .5 56 .5 51 .8 58 .6\n‚àÜ 5.3 2.3 14.5 3.8 8.8 7.2 17.0 4.5 ‚àí0.8 3.5 ‚àí1.2 1.9 1.5\nDe ‚úì 61.1 43 .1 68 .7 69 .3 67 .0 53 .1 63 .5 68 .2 47 .7 54 .6 80 .5 65 .2 52 .2\n‚àÜ 5.2 5.9 11.1 ‚àí11.5 14.3 4.6 14.7 4.8 4.5 9.8 4.6 1.4 ‚àí2.2\nRu ‚úì 52.8 52 .9 58 .6 72 .7 54 .9 47 .0 54 .2 52 .4 49 .3 32 .8 56 .2 40 .7 61 .4\n‚àÜ 0.7 ‚àí3.1 3.2 0.3 2.9 0.5 5.2 4.1 ‚àí1.9 1.1 ‚àí4.5 ‚àí0.1 0.3\nJa ‚úì 56.2 61 .5 65 .8 70 .5 53 .0 52 .1 55 .3 51 .3 54 .0 41 .0 50 .6 61 .0 57 .8\n‚àÜ 1.5 0.7 5.0 ‚àí3.3 1.1 2.0 4.3 1.1 4.6 2.2 ‚àí2.9 0.8 2.3\nTable 3: English (L2) grammatical knowledge of bilingual LMs with different L1s. OVERALL indicates the\nmacro-average accuracy over all linguistic phenomena. The rows with ‚úìin the L1 column exhibit the accuracy of\nbilingual LMs on BLiMP. The rows with ‚àÜ in the L1 column exhibit the performance difference between the LMs\nwith and without L1 pretraining. The coarse categories (e.g., Morphology) are from the metadata of BLiMP.\nL1 Morph. Syntax Semantics Syn.&Sem.\nFr 7.3 7 .0 1 .2 1 .7\nDe 5.0 7 .2 7 .2 ‚àí0.4\nRu 0.8 1 .9 ‚àí1.7 0 .1\nJa 0.9 3 .0 ‚àí0.3 1 .5\nAvg. 3.5 4 .8 1 .6 0 .7\nTable 4: Performance difference between the LMs with\nand without L1 pretraining for each coarse category of\ngrammatical items (morphology, syntax, semantics, and\nsyntax&semantics).\nfer (Cook, 1985). Ri and Tsuruoka (2022) also re-\nported that pretraining in a natural language other\nthan English improves the overall syntactic parsing\nperformance in English. Our results are consistent\nwith such positive effects. Besides, our experiment\nprovides the results of each of more fine-grained\ngrammatical items related to morphological, syn-\ntactic, and semantic phenomena.\n4.2 Differences in L1s\nThe ‚àÜ values in the OVERALL column in Table 3\ndiffer across the L1s. French is the highest, fol-\nlowed closely by German, and Japanese and Rus-\nsian are far behind these two languages; pretrain-\ning in French and German is much more effec-\ntive than in Japanese and Russian. This ordering\nshows parallels with the presumed language learn-\ning difficulty order: French, German, Russian, and\nJapanese. This suggests that the difficulty of acquir-\ning an L2 grammatical ability is somewhat close\nbetween LMs and humans.\n4.3 Differences in grammar items\nThe ‚àÜ scores in Table 3 exhibit that different gram-\nmatical items obtain different degrees of gains. Ta-\nble 4 shows the average ‚àÜ scores for each course\ngrammar category. There was a general tendency\nfor morphological and syntactic items to get larger\ngains from the L1 pretraining than semantic and\nsyntax&semantic items except for particular set-\ntings, e.g., IRREGULAR . It has been shown that\nlinguistic phenomena related to semantics such as\nNPI (negative polarity item) and QUANTIFIERS\nwere relatively difficult for LMs to learn (Warstadt\net al., 2020). Based on this, there is a concern that\nLMs failed to learn enough such linguistic knowl-\nedge in L1 to transfer it to another language.\n4.4 Differences in L1 √ógrammar-item\nNotably, in specific combinations of L1s and gram-\nmar items, the L1 pretraining hurt the L2 gener-\nalization, i.e., negative transfer problem. For ex-\nample, the performance in the IRREGULAR item\nwas not so much enhanced or even degraded by\nL1 pretraining. The IRREGULAR (Irregular forms)\nitem targets English-specific irregular verb conjuga-\ntions; its less effect by L1 pretraining is due to the\nconcern that the irregular patterns generally could\n13561\nMorphology\nSyntax\nSemanticsSyntax & Semantics\nFirst language:\nFigure 3: Grammar learning trajectories in each test suite on BLiMP (the L2 side). The x-axis denotes the epoch\nduring L2 acquisition, and the y-axis denotes the accuracy in the corresponding test suite.\nnot be predictable by other language‚Äôs knowledge.\nWe also found that the same grammatical item\nwas affected differently depending on the L1. For\nexample, the ‚àÜ values on the FILLER -GAP item\nin Table 3 differ across the L1s, e.g., 4.8 in Ger-\nman and 1.1 in Japanese. At least in this FILLER -\nGAP aspect, there is an interesting parallel between\nour results and linguistic notions; Japanese is the\nonly language where gap precedes filler in wh-\nconstruction among the L1s we used, and the trans-\nfer from Japanese to English was indeed limited\n(‚àÜ = 1.1) compared to other L1s. There might be\na possibility that such linguistic (dis)similarities are\nreflected in the results. Nevertheless, concluding\nthe exact consistency between our L1√ógrammar-\nitem results (Table 3) and the L1‚ÄìL2 grammati-\ncal similarity requires further interdisciplinary re-\nsearch.\n5 Analysis: acquisition process\nThis section sheds light on the process of L2 ac-\nquisition. We investigate how L2 knowledge ac-\nquisition progresses (Section 5.1) and how origi-\nnal L1 knowledge changes during L2 acquisition\n(Section 5.2). As for the L1 knowledge during L2\nacquisition, there is a concern, for example, that\nLMs exhibit catastrophic forgetting about their L1.\n5.1 L1 ‚ÜíL2 effects\nSettings: We evaluate the L2 linguistic knowl-\nedge of intermediate checkpoints of LMs (Fig-\nure 3). Specifically, we evaluate LMs after {1,\n2, 3, 4, 5, 10, 20, 30, 40, 50, 100} training epochs.\nNote that we analyzed the same models used in\nSection 4; the results after 100 epochs are the same\nones reported in Section 4.\nGeneral improvement after dozens of epochs:\nThe trajectory of the OVERALL scores in Figure 3\nsuggest that linguistic ability generally improves\nalong with the number of epochs. There was a\ntendency for large improvements to emerge after\ndozens of epochs; in other words, the models be-\ngan to acquire L2 knowledge after seeing the same\nexamples many times, e.g., 50-100 times. Note that\nhumans are argued to acquire a vocabulary after en-\ncountering the same word about 12 times (Nation,\n2014), and of course, the lexical and syntactic ac-\nquisition is not comparable, but the observation that\nthe L2 knowledge improves after 50-100 rounds\nof the corpus may be in the direction that LMs are\ninefficient at acquiring a new language.\nDifferences in grammar items: Focusing on\nthe general trajectory shapes for each grammati-\ncal item, we observed at least four patterns: (i)\nspike-at-the-end (D-N AGR ., IRREGULAR , S-V\n13562\nL1-only \nVpcoordination (long) Vpcoordination (short)Across object rel. clauseWithin object rel. clauseAcross subject rel. clauseAcross prepositional phraseSimple agreement\nTLM-drop GermanRussianFrench\nFigure 4: Grammar learning trajectories in each test suite on CLAM (the L1 side). The x-axis denotes the epoch\nduring L2 acquisition, and the y-axis denotes the accuracy in the corresponding test suite. The upper parts show the\nscores of the bilingual LMs (L1 pretraining and bilingual training). The lower parts show the scores of the L1-only\nLMs (L1 pretraining and further training on L1 texts collected from the parallel corpus).\nAGR .), (ii) flat (ARG.STR ., CTRL .RAIS ., ISLAND ),\n(iii) bumpy (ANA.AGR ., ELLIPSIS , NPI, QUANTI -\nFIERS ), and (iv) mixed (FILLER -GAP , BINDING ).\nIn addition, these groups roughly mirror the linguis-\ntic categories of the grammar items (morphology,\nsyntax, semantics, and syntax&semantics); for ex-\nample, all the items in the spike-at-the-end group\nare morphological phenomena, while all the se-\nmantic categories ( NPI , QUANTIFIERS ) yielded\nthe bumpy patterns. Note that existing studies\nreported that low-level (e.g., morphological) lin-\nguistic skills could be acquired earlier and vice\nversa (Liu et al., 2021; Blevins et al., 2022); but\nat least in our cognitively-inspired bilingual train-\ning scenario, we did not observe such an explicit\ntendency.\nInter-L1s differences: In terms of the change of\ninter-L1s differences of accuracies in each gram-\nmar item, there are several different patterns: (i)\nconverging (IRREGULAR , ISLAND ), (ii) diverging\n(ARG.STR ., BINDING , D-N AGR ., S-V AGR .), and\n(iii) none of them. Considering IRREGULAR as\nan example of the converging group, the accura-\ncies were substantially different across the L1s in\nthe initial stage of training; these differences, how-\never, gradually reduced along with epochs. On the\nother hand, considering S-V AGR . as an example\nof the diverging group, the accuracies gradually\ndiffered in the latter stage of training among the\nLMs. In the third group, the inter-L1s accuracy\ndifferences remain the same or unstable during\nL2 tranining (ANA.AGR ., CTRL .RAIS ., ELLIPSIS ,\nFILLER -GAP , NPI , QUANTIFIERS ). At least the\nformer two groups imply that pretraining with dif-\nferent L1s differently affects the efficiency of L2\nacquisition (e.g., different slopes for different L1s).\nTo sum up, we clarified that different L1s and\ngrammar items exhibit different learning dynamics\nof LMs. The cognitive plausibility of these patterns\ncould be the next important investigation.\n5.2 L2 ‚ÜíL1 effects\nIn contrast to the previous analysis, which analyzed\nthe impact in the L1‚ÜíL2 direction, we further an-\nalyze the L2 ‚ÜíL1 impact. In applied linguistics,\nL1‚ÄìL2 impact in both directions is of interest; for\nexample, it is reported that the L1 ability is some-\ntimes hurt by the increase of L2 exposure (Kecskes,\n2008; Haman et al., 2017).\nSettings: In the same way as Section 5.1, we eval-\nuate the grammatical knowledge of LMs during L2\ntraining, but the focus is on the L1 knowledge.\nWe used a multilingual benchmark of grammatical\njudgment test CLAMS (Mueller et al., 2020). This\ndataset consists of seven syntactic test suites for\nseveral languages. Similarly to BLiMP, the dataset\nconsists of pairs of sentences, where one is gram-\nmatical, and the other is ungrammatical. We report\nthe accuracy scores in terms of whether the LMs\ncould assign lower pseudo-perplexity to the gram-\nmatically correct sentence. We analyze French-L1,\n13563\nGerman-L1, and Russian-L1 LMs since this dataset\ncovers these three L1s.\nAs a baseline, we also evaluate the L1-only LMs\nthat are first pretrained in L1, then additionally\ntrained with the L1-side texts collected from the\ncorresponding parallel corpus; that is, the only dif-\nference between bilingual LMs and L1-only base-\nlines was the presence of L2 texts during the L2\nacquisition phase.\nL2 effects once occur, but diminish: Figure 4\nshows the results (see Table 8 for the exact scores).\nWe found that the L1 knowledge is greatly influ-\nenced by the L2, especially at the initial stage of L2\ntraining. For example, the French-L1 and German-\nL1 LMs gained positive effects, and the Russian-\nL1 LM got a negative influence (the top row of\nFigure 4). In addition, in the latter stage, the L2\neffects on the L1 knowledge gradually fade, in ei-\nther a good or bad sense, and the L1 linguistic\nknowledge is converging to the original level. For\nexample, French-L1 and German-L1 LMs exhib-\nited better performance after bilingual language\nmodeling once, e.g., at the 10 epochs, but the gain\ndecreased after further bilingual training.\nL2 negatively affects L1 knowledge: In Fig-\nure 4), the L1 knowledge in bilingual LMs was\ncompetitive or even inferior to that in L1-only LMs\nin the end, although there were also exceptional\ncases in specific grammar items in German. For\nexample, in the French-L1 LMs, the L1 syntactic\ngeneralization performance after L2 training con-\nverged below 0.7 points, while the L1-only baseline\nmodel generally achieved above 0.7 points. Con-\ntrasting the L1 results with L2 ones (Section 5.1),\nthere is an asymmetry effect between L1 and L2.\nFrom a perspective of developing linguistically bet-\nter multi-lingual LMs, these suggest that balancing\nthe linguistic knowledge of the languages, espe-\ncially enhancing the L1 knowledge, during lan-\nguage transfer is challenging even if the model\nis exposed to L1 during bilingual training. Ad-\ndressing these challenges with, for example, some\nregularization will be a promising direction from\nan engineering perspective.\n6 Related work\nCognitively-motivated analysis of neural LMs:\nInvestigations into the ability of neural models in\nlanguage acquisition began in the 1980s with the\ninterest of whether language can be acquired with-\nout innate knowledge (Rumelhart and McClelland,\n1986; Pinker and Prince, 1988). The initial inves-\ntigation was made with simple neural networks;\nafter the development of Neural NLP (Manning,\n2015), the classical questions posed by cognitive\nscience are currently revisited (Kirov and Cotterell,\n2018). A typical movement is a growing interest\nin the probing of neural LMs‚Äô linguistic knowl-\nedge (Linzen et al., 2016; Warstadt and Bowman,\n2020). In this context, our study analyzes L2 acqui-\nsition in neural LMs, while existing studies have\ntypically focused on L1 acquisition.\nLanguage transfer in computational models:\nLanguage transfer of NLP models is actively re-\nsearched from both engineering and scientific per-\nspectives. In the engineering context, to mitigate\nthe English-centric focus of NLP techniques, mod-\nels that can handle more languages have been de-\nveloped (Dong et al., 2015; Conneau and Lam-\nple, 2019; Conneau et al., 2020). From the sci-\nentific perspective, the mechanism and linguistic\nproperties of LMs‚Äô language transfer have been ex-\nplored (P√©rez-Mayos et al., 2021; Tyler et al., 2022;\nBlevins et al., 2022), sometimes beyond the transfer\nbetween natural languages (Ri and Tsuruoka, 2022;\nPapadimitriou and Jurafsky, 2020). One of the mo-\ntivations of such analyses is to quantify the transfer-\nable universals behind (non-)languages. Notably,\nsimulation of L2 acquisition is also explored from\npedagogical motivations (Settles et al., 2018).\nLanguage transfer in humans: L2 acqui-\nsition/learning has long been studied in ap-\nplied linguistics, psycholinguistics, and pedagogy\nfields (Krashen, 1981; Hatch, 1983; Ellis, 2010).\nThese fields articulated several hypotheses/theories\non human language learning, e.g., input hypothe-\nsis (Krashen, 1977). Analyzing the LMs‚Äô L2 acqui-\nsition in a more direct light with these hypotheses\nwould be interesting for future work.\n7 Conclusions\nWe have investigated the L2 acquisition of LMs, es-\npecially focusing on their grammatical knowledge\nin L1 and L2. Specifically, we have trained bilin-\ngual LMs under a similar scenario to the human L2\nacquisition and then analyzed their cross-lingual\ntransfer. Our experiments have demonstrated that\nL1 pretraining promotes their linguistic generaliza-\ntion in L2, and there are interesting variations in L1\npretraining effects with respect to the L1 choice,\n13564\ntraining settings, and grammar items. The results\nhave also implied that their L2 acquisition is not\nhuman-like in particular aspects.\nLimitations\nCoverage of experiments\nThere is room for further exploration in terms of\nmodel architectures, data, and evaluation settings\nin our study. Experiments in more diverse settings\nwould enhance the generality of the conclusions.\nModels: The architecture was fixed to XLM\n(14M), although we tested four models with differ-\nent seeds in each setting. Specifically, testing uni-\ndirectional LMs will be in a reasonable direction,\nconsidering the humans‚Äô incremental language pro-\ncessing. Related to this, the measure of pseudo-\nperplexity might also induce unintended biases\nin our results, this is a common metric in NLP\nthough. In addition, there are different methods\nto fine-tune the model to multiple languages, e.g.,\nusing adapters. Comparing these methods with our\nscheme will be an interesting direction.\nData: There are possible variations of L1‚ÄìL2\ncombinations. Although we selected L1 from one\nof the four languages (German, French, Russian,\nJapanese) and fixed L2 as English, increasing the\ncoverage of languages will lead to more general-\nized conclusions. Furthermore, the performance\nof LMs was generally not so good on the BLiMP\ndataset. We suspect that this is due to the limited L2\ntraining data size; it is also worth exploring scaling\nup the experiments into typical NLP experiments.\nEvaluation: While our focus is on morphology,\nsyntax, and semantic generalization, L2 acquisition\nstudies are conducted from broader perspectives,\nsuch as the growth of vocabulary size. In addi-\ntion, our observations are from the perspective of\nthe LMs; the contrast between LMs‚Äô and humans‚Äô\nL2 learning is more important from an interdisci-\nplinary perspective.\nPerformance was overall poor\nOne reviewer is concerned that our results on the\nBLiMP are generally near the chance level, and it\nmay be difficult to derive findings from such poor\nresults. We thank the reviewer and would like to\nshare our thoughts and limitations here.\nFirst, comparisons to the chance rate are not al-\nways meaningful. In BLiMP, the task is typically to\nselect a correct generalization over an incorrect one.\nOccasionally, neural models overly prefer incorrect\ngeneralizations more than chance level. For in-\nstance, in the linear vs. hierarchical generalization\ncontrast, neural models often favor linear, causing\naccuracy to drop near 0, far below chance (McCoy\net al., 2018). In such cases, achieving accuracies\naround 50 indicates more than random guessing,\nas models avoid an excessive preference for incor-\nrect generalizations, moving toward a more neutral\nstance. Thus, we believe that it is also worthwhile\nto observe how much performance improves from\nbelow the chance level.\nFurthermore, in BLiMP‚Äôs finer-grained test suits,\nour models sometimes exhibit an accuracy of 0\nor 100 (resulting in an overall score of around\n50.0), highlighting that our models do not always\nact as random guessing baselines. The full results\nacross more fine-grained test suits are shown in\nAppendix 7.\nEthics Statement\nThere might be a possibility that the texts we used\n(CC-100 and Tatoeba) have socially biased, de-\nspite their popular use in the NLP community. We\nadopted cognitively-plausible restricted settings\nwith respect to data size, which can potentially be\naligned with environmentally friendly, green NLP.\nAcknowledgements\nWe would like to express our gratitude for the\nanonymous reviewers who provided many insight-\nful comments that have improved our paper. Spe-\ncial thanks also go to the members of NAIST\nand Tohoku NLP Laboratory for the interesting\ncomments and energetic discussions. This work\nwas supported by JSPS KAKENHI Grant Number\nJP19K20351.\nReferences\nTerra Blevins, Hila Gonen, and Luke Zettlemoyer. 2022.\nAnalyzing the mono- and cross-lingual pretraining\ndynamics of multilingual language models. In Pro-\nceedings of EMNLP, pages 3575‚Äì3590, Abu Dhabi,\nUnited Arab Emirates. Association for Computa-\ntional Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\n13565\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877‚Äì1901. Curran Associates,\nInc.\nJoan L. Bybee. 2013. 49 Usage-based Theory and Ex-\nemplar Representations of Constructions. In The\nOxford Handbook of Construction Grammar. Oxford\nUniversity Press.\nBarry Chiswick and Paul Miller. 2004. Linguistic dis-\ntance: A quantitative measure of the distance between\nenglish and other languages. Technical Report 1246,\nInstitute of Labor Economics (IZA).\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm√°n, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of ACL, pages 8440‚Äì8451, Online. Associa-\ntion for Computational Linguistics.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances in\nNeural Information Processing Systems, volume 32.\nCurran Associates, Inc.\nVivian J Cook. 1985. Chomsky‚Äôs universal grammar\nand second language learning. Applied linguistics,\n6(1):2‚Äì18.\nAmeet Deshpande, Partha Talukdar, and Karthik\nNarasimhan. 2022. When is BERT multilingual?\nisolating crucial ingredients for cross-lingual transfer.\nIn Proceedings of NAACL, pages 3610‚Äì3623, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nDaxiang Dong, Hua Wu, Wei He, Dianhai Yu, and\nHaifeng Wang. 2015. Multi-task learning for mul-\ntiple language translation. In Proceedings of ACL,\npages 1723‚Äì1732, Beijing, China. Association for\nComputational Linguistics.\nRod Ellis. 2010. Second language acquisition, teacher\neducation and language pedagogy. Language teach-\ning, 43(2):182‚Äì201.\nBarbara F Grimes and JE Grimes. 2002. Ethnologue:\nLanguages of the world. 14, h edition. Dallas, TX:\nSIL International.\nEwa Haman, Zofia Wodniecka, Marta Marecka,\nJakub Szewczyk, Marta Bia≈Çecka-Pikul, Agnieszka\nOtwinowska, Karolina Mieszkowska, Magdalena ≈Åu-\nniewska, Joanna Ko≈Çak, Aneta MiÀõ ekisz, Agnieszka\nKacprzak, Natalia Banasik, and Ma≈Çgorzata Fory ¬¥s-\nNogala. 2017. How does L1 and L2 exposure im-\npact L1 performance in bilingual children? evidence\nfrom Polish-English migrants to the united kingdom.\nFront. Psychol., 8:1444.\nEvelyn Marcussen Hatch. 1983. Psycholinguistics: A\nsecond language perspective. ERIC.\nIstvan Kecskes. 2008. The effect of the second language\non the first language. Babylonia, 2(2):31‚Äì34.\nChristo Kirov and Ryan Cotterell. 2018. Recurrent neu-\nral networks in linguistic theory: Revisiting pinker\nand prince (1988) and the past tense debate. Transac-\ntions of the Association for Computational Linguis-\ntics, 6:651‚Äì665.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\nBrooke Cowan, Wade Shen, Christine Moran,\nRichard Zens, Chris Dyer, OndÀárej Bojar, Alexandra\nConstantin, and Evan Herbst. 2007. Moses: Open\nsource toolkit for statistical machine translation. In\nProceedings of ACL, pages 177‚Äì180, Prague, Czech\nRepublic. Association for Computational Linguistics.\nStephen Krashen. 1977. Some issues relating to the\nmonitor model. On Tesol, 77(144-158).\nStephen Krashen. 1981. Second language acquisition.\nSecond Language Learning, 3(7):19‚Äì39.\nStephen D Krashen, Michael A Long, and Robin C\nScarcella. 1979. Age, rate and eventual attainment\nin second language acquisition. TESOL Quarterly,\n13(4):573‚Äì582.\nJey Han Lau, Carlos Armendariz, Shalom Lappin,\nMatthew Purver, and Chang Shu. 2020. How fu-\nriously can colorless green ideas sleep? sentence\nacceptability in context. Transactions of the Associa-\ntion for Computational Linguistics, 8:296‚Äì310.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of LSTMs to learn syntax-\nsensitive dependencies. Transactions of the Associa-\ntion for Computational Linguistics, 4:521‚Äì535.\nZeyu Liu, Yizhong Wang, Jungo Kasai, Hannaneh Ha-\njishirzi, and Noah A. Smith. 2021. Probing across\ntime: What does RoBERTa know and when? In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2021, pages 820‚Äì842, Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nChristopher D. Manning. 2015. Last words: Computa-\ntional linguistics and deep learning. Computational\nLinguistics, 41(4):701‚Äì707.\nR Thomas McCoy, Robert Frank, and Tal Linzen. 2018.\nRevisiting the poverty of the stimulus: hierarchical\ngeneralization without a hierarchical bias in recurrent\nneural networks. In In Proceedings of CogSci, pages\n2093‚Äì2098, Madison, WI.\nAaron Mueller, Garrett Nicolai, Panayiota Petrou-\nZeniou, Natalia Talmina, and Tal Linzen. 2020.\nCross-linguistic syntactic evaluation of word predic-\ntion models. In Proceedings of ACL, pages 5523‚Äì\n5539, Online. Association for Computational Lin-\nguistics.\n13566\nPaul Nation. 2014. How much input do you need to\nlearn the most frequent 9,000 words? Reading in a\nForeign Language, 26(2):2.\nGraham Neubig and Shinsuke Mori. 2010. Word-based\npartial annotation for efficient corpus construction.\nIn Proceedings of LREC, Valletta, Malta. European\nLanguage Resources Association (ELRA).\nGraham Neubig, Yosuke Nakata, and Shinsuke Mori.\n2011. Pointwise prediction for robust, adaptable\nJapanese morphological analysis. In Proceedings of\nACL, pages 529‚Äì533, Portland, Oregon, USA. Asso-\nciation for Computational Linguistics.\nIsabel Papadimitriou and Dan Jurafsky. 2020. Learning\nMusic Helps You Read: Using transfer to study lin-\nguistic structure in language models. In Proceedings\nof EMNLP, pages 6829‚Äì6839, Online. Association\nfor Computational Linguistics.\nLaura P√©rez-Mayos, Alba T√°boas Garc√≠a, Simon Mille,\nand Leo Wanner. 2021. Assessing the syntactic ca-\npabilities of transformer-based multilingual language\nmodels. In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021 , pages\n3799‚Äì3812, Online. Association for Computational\nLinguistics.\nSteven Pinker and Alan Prince. 1988. On language\nand connectionism: Analysis of a parallel distributed\nprocessing model of language acquisition. Cognition,\n28(1):73‚Äì193.\nRyokan Ri and Yoshimasa Tsuruoka. 2022. Pretraining\nwith artificial language: Studying transferable knowl-\nedge in language models. In Proceedings of ACL,\npages 7302‚Äì7315, Dublin, Ireland. Association for\nComputational Linguistics.\nDavid E Rumelhart and James L McClelland. 1986. On\nlearning the past tenses of english verbs. In Paral-\nlel distributed processing: Explorations in the mi-\ncrostructure of cognition, pages 216‚Äì271. MIT Press,\nCambridge, MA.\nJulian Salazar, Davis Liang, Toan Q. Nguyen, and Ka-\ntrin Kirchhoff. 2020. Masked language model scor-\ning. In Proceedings of ACL, pages 2699‚Äì2712, On-\nline. Association for Computational Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of ACL, pages 1715‚Äì\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nBurr Settles, Chris Brust, Erin Gustafson, Masato Hagi-\nwara, and Nitin Madnani. 2018. Second language ac-\nquisition modeling. In Proceedings of the Thirteenth\nWorkshop on Innovative Use of NLP for Building Ed-\nucational Applications, pages 56‚Äì65, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang,\nSuraj Srivats, Soroush V osoughi, Hyung Won Chung,\nYi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das,\nand Jason Wei. 2023. Language models are multi-\nlingual chain-of-thought reasoners. In The Eleventh\nInternational Conference on Learning Representa-\ntions.\nJ√∂rg Tiedemann. 2012. Parallel data, tools and inter-\nfaces in opus. In Proceedings of LREC , Istanbul,\nTurkey. European Language Resources Association\n(ELRA).\nChang Tyler, Tu Zhuowen, and Benjamin Berge. 2022.\nThe geometry of multilingual language model rep-\nresentations. In Proceedings of EMNLP, pages 119‚Äì\n136, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nAlex Warstadt and Samuel R. Bowman. 2020. Can\nneural networks acquire a structural bias from raw\nlinguistic data? In In Proceedings of CogSci.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R.\nBowman. 2020. BLiMP: The benchmark of linguis-\ntic minimal pairs for English. Transactions of the\nAssociation for Computational Linguistics , 8:377‚Äì\n392.\nGuillaume Wenzek. 2020. Ccnet: Extracting high qual-\nity monolingual datasets from web crawl data. In\nProceedings of LREC, pages 4003‚Äì4012, Marseille,\nFrance. European Language Resources Association.\n13567\ndropout 0.1\nattention_dropout 0.1\naccumulate_gradients 4\nemb_dim 256\nffn_embed_dim 1024\ngelu_activation True\nOptimizer adam_inverse_sqrt\nlr=0.00020\neps=0.000001\nwarmup_updates=30000\nbeta1=0.9\nbeta2=0.999\nweight_decay=0.01\nepoch 100\nn_heads 8\nn_layers 12\nclip_grad_norm 1.0\namp 2\nfp16 True\nTable 5: Hyperparameters of the LMs\nA Experimental Procedure\nWe list the hyperparameters in Table 5. The ver-\nsions and licenses of used tools and datasets are\nlisted in Table 6.\nL1 Acquisition: We used mosesdecoder (Koehn\net al., 2007) as French, German and Russian to-\nkenizer, kytea8(Neubig et al., 2011; Neubig and\nMori, 2010) as Japanese tokenizer and segmented\nwords into subwords with fastBPE9(Sennrich et al.,\n2016) The dataset was split into train/dev/test in\nthe ratio of 8:1:1. We set 14,000 vocabulary size\nfor any language. We trained our models with 4\nparallel GPUs (VRAM 48G), which took 6 days\nper model.\nL2 Acquisition: We added English tokens from\nthe parallel corpus into BPE codes and vocabulary\nused in L1 Acquisition and removed duplicated\ntokens and vocabulary. As for models not using\na monolingual corpus, we created the codes and\nvocabulary using a parallel corpus of both L1 and\nL2. We used mosesdecoder (Koehn et al., 2007) as\nEnglish tokenizer. As for other languages, we use\ntokenizers the same as L1 Acquisition. The dataset\nwas split into train/dev/test in the ratio of 8:1:1. As\n8http://www.phontron.com/kytea/\n9https://github.com/glample/fastBPE\nName Version License\nfastBPE 0.1.0 MIT License\nkytea 0.4.7 Apache 2.0\nmosesdecoder 0.4.0 LGPL 2.1\nXLM 0.1.0 CC BY-NC 4.0\nBLiMP 0.1.0 CC BY-NC 4.0\nCC100 (CC-Net) 1.0.0 MIT License\nCLAMS 0.1.0 Apache 2.0\nTatoeba v2022-03-03 CC‚ÄìBY 2.0 FR\nTable 6: The versions and licenses of used tools and\ndatasets. These tools and datasets used in this study\nwere designed for the purposes of research and language\nlearning.\nwe increased the number of vocabularies in the em-\nbedding layer, the weights/biases in the final layer\nwere also increased. Our four LMs were trained\nwith different 3 seeds and reported their averages\nas results. Compared models in our preliminary\nexperiment (Sec. 3) are shown in Figure 2. We\ntrained our models with 2‚Äì4 GPUs (VRAM 48G),\nwhich took around 5 hours per model.\n13568\nCoarse Specific Challenge First Language\nFr De Ru Ja\nMorphology\nANA.AGR anaphor_gender_agreement 61.5 23 .7 52 .7 57 .7\nanaphor_number_agreement 50.1 62 .5 53 .1 65 .4\nD-N AGR\ndeterminer_noun_agreement_1 80.8 74 .5 69 .1 67 .9\ndeterminer_noun_agreement_2 70.1 76 .5 62 .6 79 .8\ndeterminer_noun_agreement_irregular_1 65.8 63 .0 58 .3 54 .5\ndeterminer_noun_agreement_irregular_2 64.9 72 .3 56 .5 78 .3\ndeterminer_noun_agreement_with_adj_1 77.9 69 .2 59 .9 61 .1\ndeterminer_noun_agreement_with_adj_2 62.8 66 .9 56 .1 65 .7\ndeterminer_noun_agreement_with_adj_irregular_1 72.7 64 .1 54 .0 50 .5\ndeterminer_noun_agreement_with_adj_irregular_2 61.1 63 .0 51 .9 68 .4\nIRREGULAR irregular_past_participle_adjectives 75.1 61 .6 95 .3 79 .1\nirregular_past_participle_verbs 70.9 77 .0 50 .0 62 .0\nS-V AGR\ndistractor_agreement_relational_noun 50.9 65 .5 39 .8 42 .6\ndistractor_agreement_relative_clause 48.6 51 .7 46 .1 45 .4\nirregular_plural_subject_verb_agreement_1 64.2 68 .9 56 .7 54 .4\nirregular_plural_subject_verb_agreement_2 65.5 74 .5 63 .6 58 .9\nregular_plural_subject_verb_agreement_1 65.9 72 .2 60 .7 59 .5\nregular_plural_subject_verb_agreement_2 67.3 69 .2 62 .5 57 .2\nSyntax\nARG.STR\nanimate_subject_passive 65.1 55 .9 51 .1 53 .9\nanimate_subject_trans 44.7 44 .1 31 .7 37 .3\ncausative 39.6 53 .4 35 .8 38 .8\ndrop_argument 57.5 44 .6 44 .0 54 .1\ninchoative 51.6 43 .5 37 .7 45 .2\nintransitive 56.0 47 .1 40 .5 52 .4\npassive_1 61.4 62 .6 60 .9 67 .7\npassive_2 62.7 70 .0 65 .1 66 .2\ntransitive 59.8 56 .8 56 .1 53 .8\nELLIPSIS ellipsis_n_bar_1 52.1 50 .2 46 .4 46 .0\nellipsis_n_bar_2 83.2 76 .8 62 .1 64 .5\nFILLER-GAP\nwh_questions_object_gap 34.7 87 .2 36 .6 31 .4\nwh_questions_subject_gap 66.8 89 .2 64 .9 61 .8\nwh_questions_subject_gap_long_distance 68.8 95 .5 63 .8 62 .4\nwh_vs_that_no_gap 68.4 97 .1 60 .5 63 .2\nwh_vs_that_no_gap_long_distance 61.3 97 .8 56 .1 53 .5\nwh_vs_that_with_gap 39.8 6 .6 40 .1 38 .8\nwh_vs_that_with_gap_long_distance 42.6 4 .1 45 .2 47 .7\nISLAND\nadjunct_island 49.0 52 .5 50 .0 56 .8\ncomplex_NP_island 47.7 43 , 0 43 .9 53 .9\ncoordinate_structure_constraint_complex_left_branch 33.7 40 .2 41 .1 39 .0\ncoordinate_structure_constraint_object_extraction 68.8 78 .9 57 .7 53 .9\nleft_branch_island_echo_question 36.6 37 .2 27 .1 37 .2\nleft_branch_island_simple_question 63.1 71 .7 68 .0 69 .1\nsentential_subject_island 50.5 50 .1 52 .3 55 .4\nwh_island 68.0 8 .1 54 .5 66 .6\nSemantics\nNPI\nmatrix_question_npi_licensor_present 46.2 46 .0 14 .3 30 .9\nnpi_present_1 52.1 94 .5 41 .3 72 .3\nnpi_present_2 56.4 94 .8 39 .5 72 .5\nonly_npi_licensor_present 1.0 0 .2 0 .1 0 .2\nonly_npi_scope 35.9 55 .2 50 .4 52 .1\nsentential_negation_npi_licensor_present 32.1 34 .2 39 .6 11 .8\nsentential_negation_npi_scope 59.6 57 .6 44 .6 47 .1\nQUANTIFIERS\nexistential_there_quantifiers_1 85.2 84 .0 69 .6 62 .3\nexistential_there_quantifiers_2 7.5 61 .2 4 .2 2 .8\nsuperlative_quantifiers_1 45.2 92 .0 60 .6 55 .4\nsuperlative_quantifiers_2 87.9 84 .9 90 .3 82 .0\nSyntax & Semantics\nBINDING\nprinciple_A_c_command 61.4 49 .6 69 .8 53 .5\nprinciple_A_case_1 52.0 99 .8 11 .4 99 .9\nprinciple_A_case_2 62.0 58 .7 54 .8 49 .6\nprinciple_A_domain_1 41.3 93 .7 0 .7 83 .9\nprinciple_A_domain_2 51.3 62 .9 49 .9 45 .2\nprinciple_A_domain_3 52.1 49 .4 48 .5 48 .7\nprinciple_A_reconstruction 42.4 42 .2 49 .6 46 .3\nCTRL. RAIS\nexistential_there_object_raising 68.2 53 .8 68 .5 69 .9\nexistential_there_subject_raising 55.1 53 .3 69 .7 51 .1\nexpletive_it_object_raising 70.2 54 .7 67 .9 69 .1\ntough_vs_raising_1 56.4 44 .6 27 .2 62 .3\ntough_vs_raising_2 43.2 54 .8 73 .8 36 .8\nTable 7: Results for each fine-grained test suit in BLiMP.\n13569\nModel L1 Epoch\nlong_vp_coord\nobj_rel_within_anim\nsimple_agrmt\nvp_coord\nobj_rel_across_anim\nprep_anim\nsubj_rel\nTLM-drop\nFr\n5 90.9 75 .7 78 .2 81 .4 85 .1 77 .3 78 .5\n50 75.7 66 .4 71 .2 72 .3 69 .2 69 .1 70 .7\n100 67.3 63 .4 64 .6 66 .5 65 .2 64 .9 64 .9\nDe\n5 98.7 83 .6 88 .5 87 .3 89 .8 86 .4 87 .6\n50 85.9 77 .4 82 .9 81 .3 81 .3 81 .0 81 .4\n100 67.9 65 .6 72 .8 71 .0 73 .4 70 .6 71 .4\nRu\n5 82.1 77 .6 75 .0 69 .4 59 .0 73 .5 69 .6\n50 74.1 77 .0 74 .7 74 .6 73 .5 74 .1 75 .3\n100 64.2 75 .1 74 .7 77 .9 75 .9 74 .3 76 .6\nL1-only\nFr\n5 92.3 79 .3 72 .0 75 .6 76 .3 77 .3 80 .1\n50 97.2 82 .3 74 .9 77 .6 78 .3 77 .3 80 .0\n100 94.5 80 .1 72 .4 77 .1 77 .3 76 .7 80 .9\nDe\n5 54.7 60 .1 61 .1 71 .6 71 .3 74 .0 72 .1\n50 56.8 58 .2 55 .7 67 .2 67 .9 71 .4 70 .5\n100 53.8 57 .4 56 .4 68 .0 66 .3 68 .9 66 .0\nRu\n5 85.7 77 .9 87 .5 85 .6 86 .9 84 .9 85 .5\n50 90.8 83 .2 88 .6 87 .7 88 .1 89 .3 91 .8\n100 92.1 72 .3 81 .8 82 .9 82 .2 82 .7 86 .4\nTable 8: Results for each fine-grained test suit in CLAMS.\n13570\nACL 2023 Responsible NLP Checklist\nA For every submission:\n‚ñ°\u0013 A1. Did you describe the limitations of your work?\nIn Limitation section\n‚ñ°\u0013 A2. Did you discuss any potential risks of your work?\nIn Ethics Statement section\n‚ñ°\u0013 A3. Do the abstract and introduction summarize the paper‚Äôs main claims?\nAbstract section is located before introduction section at page 1. The section number of introduction\nis 1.\n‚ñ°\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB ‚ñ°\u0013 Did you use or create scientiÔ¨Åc artifacts?\nIn Section 2, 3, 4, 5 and Appendix\n‚ñ°\u0013 B1. Did you cite the creators of artifacts you used?\nIn Appendix section\n‚ñ°\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nIn Appendix section\n‚ñ° B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciÔ¨Åed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNo response.\n‚ñ° B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiÔ¨Åes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNo response.\n‚ñ°\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nIn Section 2\n‚ñ°\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiÔ¨Åcant, while on small test sets they may not be.\nIn our Appendix\nC ‚ñ°\u0013 Did you run computational experiments?\nIn Section 2, 3, 4, 5 and Appendix\n‚ñ°\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nIn Section 2 and Appnedix\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n13571\n‚ñ°\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nIn Appendix section\n‚ñ°\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nIn Appendix section\n‚ñ°\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nIn Appendix section\nD ‚ñ°\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n‚ñ° D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n‚ñ° D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants‚Äô demographic\n(e.g., country of residence)?\nNo response.\n‚ñ° D3. Did you discuss whether and how consent was obtained from people whose data you‚Äôre\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n‚ñ° D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n‚ñ° D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n13572",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7388803362846375
    },
    {
      "name": "Second-language acquisition",
      "score": 0.6326856017112732
    },
    {
      "name": "Language acquisition",
      "score": 0.6088141202926636
    },
    {
      "name": "Generalization",
      "score": 0.5113252997398376
    },
    {
      "name": "First language",
      "score": 0.48950278759002686
    },
    {
      "name": "Natural language processing",
      "score": 0.48415476083755493
    },
    {
      "name": "Developmental linguistics",
      "score": 0.47818854451179504
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4485805630683899
    },
    {
      "name": "Comprehension approach",
      "score": 0.4110780954360962
    },
    {
      "name": "Linguistics",
      "score": 0.3975582718849182
    },
    {
      "name": "Natural language",
      "score": 0.3738968074321747
    },
    {
      "name": "Mathematics",
      "score": 0.067486971616745
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I75917431",
      "name": "Nara Institute of Science and Technology",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I4210113480",
      "name": "Mohamed bin Zayed University of Artificial Intelligence",
      "country": "AE"
    },
    {
      "id": "https://openalex.org/I201537933",
      "name": "Tohoku University",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I4210110652",
      "name": "RIKEN",
      "country": "JP"
    }
  ]
}