{
  "title": "ChatGPT v.s. media bias: A comparative study of GPT-3.5 and fine-tuned language models",
  "url": "https://openalex.org/W4387895563",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5113030085",
      "name": "Zehao Wen",
      "affiliations": [
        "Hodges University"
      ]
    },
    {
      "id": "https://openalex.org/A5035110831",
      "name": "Rabih Younes",
      "affiliations": [
        "Duke University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4387895563",
    "https://openalex.org/W4320167623",
    "https://openalex.org/W2102145889",
    "https://openalex.org/W4367189616",
    "https://openalex.org/W2951700705",
    "https://openalex.org/W2149166749",
    "https://openalex.org/W4364385323",
    "https://openalex.org/W3047171714",
    "https://openalex.org/W4389523957",
    "https://openalex.org/W4212772002",
    "https://openalex.org/W3007491602",
    "https://openalex.org/W4321276803",
    "https://openalex.org/W2909299092",
    "https://openalex.org/W3198866930",
    "https://openalex.org/W4288088047",
    "https://openalex.org/W2523934214"
  ],
  "abstract": "In our rapidly evolving digital sphere, the ability to discern media bias becomes crucial as it can shape public sentiment and influence pivotal decisions. The advent of large language models (LLMs), such as ChatGPT, noted for their broad utility in various natural language processing (NLP) tasks, invites exploration of their efficacy in media bias detection. Can ChatGPT detect media bias? This study seeks to answer this question by leveraging the Media Bias Identification Benchmark (MBIB) to assess ChatGPT's competency in distinguishing six categories of media bias, juxtaposed against fine-tuned models such as Bidirectional and Auto-Regressive Transformers (BART), Convolutional Bidirectional Encoder Representations from Transformers (ConvBERT), and Generative Pre-trained Transformer 2 (GPT-2). The findings present a dichotomy: ChatGPT performs at par with fine-tuned models in detecting hate speech and text-level context bias, yet faces difficulties with subtler elements of other bias detections, namely, fake news, racial, gender, and cognitive biases.",
  "full_text": "ChatGPT v.s. media bias: A comparative study of GPT-3.5 \nand fine-tuned language models \nZehao Wen1,3,5, Rabih Younes2,4 \n1Shenzhen College of International Education, Shenzhen, 518043, China \n2Duke University, Durham, NC 27704 \n \n3s21447.wen@stu.scie.com.cn \n4rabih.younes@duke.edu \n5corresponding author \nAbstract. In our rapidly evolving digital sphere, the ability to discern media bias becomes crucial \nas it can shape public sentiment and influence pivotal decisions. The advent of large language \nmodels (LLM s), such as ChatGPT, noted for their broad utility in various natural language \nprocessing (NLP) tasks, invites exploration of their efficacy in media bias detection. Can \nChatGPT detect media bias? This study seeks to answer this question by leveraging the Media \nBias Identification Benchmark (MBIB) to assess ChatGPT's competency in distinguishing six \ncategories of media bias, juxtaposed against fine -tuned models such as Bidirectional and Auto-\nRegressive Transformers (BART) , Convolutional Bidirectional Encode r Representations from \nTransformers (ConvBERT) , and Generative Pre-trained Transformer 2 (GPT -2). The findings \npresent a dichotomy: ChatGPT performs at par with fine -tuned models in detecting hate speech \nand text-level context bias, yet faces difficulties with subtler elements of other bias detections, \nnamely, fake news, racial, gender, and cognitive biases. \nKeywords: media bias detection, large language model, comparative analysis. \n1.  Introduction \nMedia bias encompasses the selective presentation of content that favors a particular perspective, \npotentially influencing people‚Äôs perceptions of events or issues [1]. The majority of Americans hold the \nbelief that mass media organizations demonstrate bias [2]. Extensive research has focused on examining \nthe influence of media bias, and it has been shown that this type of bias could exert a substantial impact \non public opinion, that is, on elect ions and the societal reception of tobacco use. It also contributes to \nthe dissemination of misleading information, impacting the decision -making process and eroding \npeople's trust in news sources [1,3]. Therefore, detecting and understanding media bias ha s become \ncrucial in the digital age, in which information is consumed rapidly and passively. Various methods \nhave been proposed to identify bias in media, ranging from manual content analysis by human \nevaluators, e.g. [4, 5, 6, 7], to computational approac hes employing machine learning and natural \nlanguage processing techniques [8, 9, 10, 11]. However, these approaches typically focus on a particular \ntype within the range of media bias, e.g., political bias or fake news, and suffer from challenges and \nlimitations such as scalability and complexity of detecting linguistic nuances that contribute to bias. \nProceedings of the 5th International Conference on Computing and Data ScienceDOI: 10.54254/2755-2721/21/20231153\n¬© 2023 The Authors. This is an open access article distributed under the terms of the Creative Commons Attribution License 4.0(https://creativecommons.org/licenses/by/4.0/).\n249\nIn light of these limitations, the potential of Language Learning Models (LLMs) in understanding \nand generating text offers a new avenue for exploring the de tection of media bias. LLMs, including \ntransformer-based models like GPT, have been instrumental in revolutionizing several areas of natural \nlanguage processing. ChatGPT from OpenAI is one such model 1 . Based on the architecture of \nGenerative Pre-trained Transformers, ChatGPT utilizes GPT -3.5 as an engine through the process of \nadapting the model to human preferences. As per the official statement, ChatGPT possesses the ability \nto address subsequent inquiries, acknowledge any mistakes, challenge inaccurate assumptions, and \ndiminish inappropriate demands due to its conversational character. Most significantly, ChatGPT has \ndemonstrated considerable potential in various conventional natural language processing (NLP) tasks, \nincluding translation, sentiment analysis, reasoning, and summarization [12, 13, 14, 15].  \nWhile prior research has explored the application of human evaluation and AI models for media bias \nrecognition, to the best of our knowledge, this is the first study to employ ChatGPT, a type of LLMs, \nfor this purpose. We hope that our research will not only illuminate more about the capabilities of \nChatGPT in identifying media bias, but it will also spark further investigation into the possibilities of \nAI in ensuring a balanced and accurate media landscape. \nThis paper proposes the use of ChatGPT to identify media bias across various spectra, testing its \nability to understand and recognize biased language in a given text. The performance of ChatGPT is \ncompared with fine -tuned language models like  Bidirectional and Auto -Regressive Transformers \n(BART) [16], providing a comparative understanding of its strengths and weaknesses. ChatGPT is tested \nfor identifying six types of bias, including racial bias, gender bias, cognitive bias, text-level context bias, \nhate speech, and fake news, giving an overview of ChatGPT's capability of dealing with different biases.  \n2.  Related work \nThis section reviews existing research on the detection of media bias, the evolution from manual to \nautomated methods, and the capabilities and performance of large language models, specifically GPT-\nbased models, in diverse tasks. These provide insights into the challenges and opportunities in detecting \nmedia bias and the potential role of GPT-based models in addressing these challenges. \n2.1.  Identification of media bias \nEarly approaches to detecting media bias relied primarily on human evaluation. For instance, Groseclose \nand Milyo evaluated media outlets' ideological leanings by comparing citation patterns with members \nof Congress' views, revealing  a liberal bias in most of the outlets they examined  [4]. A common \ntechnique of manual content analysis involves employing coders to systematically examine news texts \nand annotate sections of the texts they identify as manifesting media bias. To ensure con sistency and \nprecision in this process, codebooks containing definitions, detailed rules, and illustrative examples that \nprovide clear instructions on how to annotate the texts are often utilized. For example, Papacharissi and \nOliveira used a codebook to study framing orientations of coverage of terrorism in newspapers from the \nUnited States and the United Kingdom [5]. Similarly, Smith et al. developed a codebook to analyze the \nframing of protest events, suggesting that media often portray protests in ways that are \ncounterproductive to social movement agendas. A codebook -based approach was applied to detect \ngender bias [6]. Van der Pas and Aaldering found that women received less attention and more negative \ncoverage than men did, highlighting the importance of these methods in revealing different facets of \nmedia bias [10]. \nWhile human -based methods are instrumental in identifying media bias, their time - and labor -\nintensive nature often restricts the scope of research, making it difficult to analyze extensive datasets. \nAs a result, automated approaches have emerged. For instance, D'Alonzo and Tegmark [8] developed a \nmethod to identify publishing newspapers based on automatic phrase frequency analysis, resulting in a \nprobability distribution that allows for the automatic mapping of newspapers and phrases into a space \nrepresenting bias. As computing power has surged, deep learning methods have become increasingly \n \n1 chat.openai.com/chat  \nProceedings of the 5th International Conference on Computing and Data ScienceDOI: 10.54254/2755-2721/21/20231153\n250\neffective and popular for numerous NLP tasks. Thota et al. presented a dense neural network architecture \nfor identifying fake news, using Tf -Idf vector representation, preprocessed engineered features, and \ncosine similarity measures as input features [11]. Their model demonstrated impressive performance on \nthe test data. Attention mechanisms have also been  utilized, as in the headline attention network \nproposed in [10], which achieved a high accuracy in detecting political bias. Lastly, Spinde et al. \ndemonstrated the efficacy of a Transformer-based architecture trained on multiple bias-related datasets, \nachieving an impressive F1 score of 0.776, underlining the significant potential of automated bias \ndetection methods [7]. \nHowever, current automated approaches face challenges and limitations that must be addressed for \nmore comprehensive and accurate media bi as detection. Primarily, existing models, while highly \nperformant on specific tasks or datasets, may not generalize well across different types of media biases. \nAdditionally, most automated methods rely heavily on labeled datasets for training, and the pro cess of \nlabeling data is not only resource-intensive, but also inherently subject to bias. This has led to growing \ninterest in exploring unsupervised or semi -supervised methods for media bias detection. These \nchallenges necessitate further research and imp rovement in the field of automated bias detection. This \nstudy is the first to evaluate whether pre -trained large language models can be used as an automated \napproach.  \n2.2.  Evaluation of the LLMs \nLanguage Language Models (LLMs) have emerged as powerful tools capable of generating and \nunderstanding human language by learning from extensive textual data. Among LLMs, GPT \n(Generative Pretrained Transformer) has emerged as a particularly influential model, showcasing \nimpressive performance across diverse NLP tasks. For instance, Jiao et al. rigorously assessed the \nperformance of ChatGPT in translation tasks against numerous benchmark test sets  [12]. Their results \nhighlighted that ChatGPT rivals other commercially available translation products, especially for high-\nresource European languages. Furthermore, investigations into ChatGPT's zero-shot reasoning capacity \nhave been conducted, demonstrating competitive performance with fine -tuned models across many \nbenchmarks, despite inferior results on some specific da tasets [13]. In addition, ChatGPT has been \nshown to generate new summaries on par with humans and even surpass traditional fine -tuned models \nin terms of aspect-based summarization [13, 14]. Finally, ChatGPT, in its role as a zero-shot sentiment \nanalyzer, matches the performance of fine -tuned BERT and other state -of-the-art models trained on \ndomain-specific labeled data [15]. Given the demonstrated proficiency of ChatGPT in processing and \nunderstanding human language, it is compelling to explore its efficacy  in detecting media bias within \ntext. This examination reveals the capability of ChatGPT as a sufficient tool for fostering a more \nbalanced media landscape, thus contributing to the ongoing academic discourse on enhancing objectivity \nin media reporting and consumption. \n3.  Experiment setup \nThis section delineates our experimental setup, which includes the preparation and division of the Media \nBias Identification Benchmark (MBIB) data for testing, the fine -tuning of three existing models for \ncomparison against ChatGPT, and the design of prompts for eliciting ChatGPT's bias detection abilities \nacross six distinct bias identification tasks. These elements are vital for understanding the experiment's \nprocedure, validating the subsequent results, and enabling replication of our study. \n3.1.  Test data \nThe analysis presented in this paper utilizes a subset of the Media Bias Identification Benchmark \n(MBIB) for test data. MBIB is a comprehensive dataset compiled by Wessel et al. that serves as a \ncommon framework for evaluating different media bias detection techniques  [1]. Following a \nmeticulous review of 115 datasets, the authors curated a selection of nine tasks and 22 associated \ndatasets, specifically tailored to evaluate media bias detection techniques. The datasets within the MBIB \nundergo a specific preprocessing step to convert the labels into a binary format, simplifying the \nProceedings of the 5th International Conference on Computing and Data ScienceDOI: 10.54254/2755-2721/21/20231153\n251\nintegration of different datasets by removing the need for diverse model heads and keeping the task \nformulation straightforward. For datasets with continuous labels, binarization is achieved by defining a \nthreshold, with the author's recommended threshold followed wherever possible.  \nOwing to data availability, our study focuses on six of the nine tasks represented in the MBIB. \nNonetheless, the selected tasks offer a broad assessment of ChatGPT's capabilities in detecting media \nbias. Table 1 introduces the six tasks involved in this work. \n \nTable 1. The description of each bias that ChatGPT is tested on. \nTask Description \nText-Level \nContext Bias \nThis task looks at how the context of a text can shape the reader's perspective \nthrough the use of words and statements. \nCognitive Bias This task deals with the biases that arise from readers' selections of articles to read \nand the sources they choose to trust, which can be magnified through the \ninfluence of social media. \nHate Speech This task includes the use of language that expresses animosity towards a \nparticular group or seeks to patronize, embarrass, or provoke offense. \nFake News This task pertains to the dissemination of published material that relies on false \nassertions and premises, presented as factual to mislead the reader. \nRacial Bias This task involves negative or positive portrayals of racial groups. \nGender Bias This task encompasses the issue of gender-based discrimination in media, \ninvolving the underrepresentation or negative depiction of one gender. \n \nGiven the varying sizes of the datasets within MBIB, each dataset is proportionally split into training \nand testing subsets for the purposes of this study. For most bias identification tasks, an 80 -20 training-\ntesting split is employed on the datasets. However, due to the large number of examples included in the \ncognitive bias and hate speech tasks, i.e., 2 million examples, we randomly selected 10% of each dataset \npertaining to these tasks and perform the 80-20 training-testing split to this subset of data. Table 2 reveals \nthe adopted size of each task. \n \nTable 2. The adopted size for each bias identification task. \nTask Training Size Test Size \nText-Level Context Bias 7213 1805 \nCognitive Bias 39066 9768 \nHate Speech 27879 6972 \nFake News 9063 2675 \nRacial Bias 7830 1958 \nGender Bias 32072 8020 \n3.2.  Fine-tuned models \nIn this study, we establish a point of comparison for evaluating the performance of ChatGPT in detecting \nmedia bias. Three well-regarded models are selected for this purpose: ConvBERT, BART, and GPT-2. \nThese pre -trained language models, recognized for their strong performance across various Natural \nLanguage Processing (NLP) tasks, are briefly described below. \nBART is a denoising autoencoder that maps both the original sequence and a corrupted version to \nthe same latent space, allowing it to perform a variety of generation and understanding tasks [16]. \nConvolutional Bidirectional Encoder Representations from Transformers (ConvBERT) is a variation of \nBERT that incorporates convolutional layers into the architecture to capture local dependencies within \nthe text [ 17]. Generative Pre-trained Transformer 2 (GPT -2), developed by OpenAI, is a large -scale \nProceedings of the 5th International Conference on Computing and Data ScienceDOI: 10.54254/2755-2721/21/20231153\n252\ntransformer-based language model that generates coherent and contextually relevant sentences by \nanticipating the subsequent word in a sequence using the context provided by the preceding words [18]. \nWe fine-tune them on each identification task with the split training set as mentioned in section 3.1, \nat a learning rate of 5ùëí‚àí5, batch size of 128, and epoch number of 5. The performance of these models \non the test datasets serves as a comparison against the performance of ChatGPT. \n3.3.  Prompt engineering \nThis research employs a specialized version of ChatGPT, the GPT -3.5-turbo, recognized for its \nproficiency and cost -effectiveness. To facilitate replicability and consistency in the outcomes, the \ntemperature of the model has bee n set to zero, ensuring that the model functions in a deterministic \nfashion and produces identical responses to identical prompts. The methodology employed for \ngenerating queries to interact with the model is adapted from the methods used in [18]. We ask ChatGPT \nto provide three concise prompts for eliciting its full ability in identifying each of the six types of bias. \nFor example, our query for identifying racial bias is: \nPlease give me three concise prompts for eliciting your full capability in identifying whether there is \nracial bias in a text given to you (i.e., whether the text includes negative or positive portrayals of racial \ngroups). \nThen, as suggested by [18], the three model-generated prompts for each identification task are tested \non a limited se t (i.e., 60 examples) derived from the original datasets. Given that a task incorporates \nmultiple datasets, we have instituted a procedure for random selection of equivalent quantities of \nexamples from each dataset. The chosen examples also include an identical number of positively labeled \ninstances (signifying the presence of a specific type of bias) and negatively labeled instances (indicating \nthe absence of such bias), all of which are excluded from the test set. Based on these outcomes, the most \nplausible and effective prompt is selected, with final selections presented in Table 3. \n \nTable 3. The final prompts we used to test ChatGPT under each bias identification task.  \nBias Type Prompt \nRacial Bias Identify any language or descriptions that may perpetuate stereotypes or \nreinforce negative perceptions of a particular racial group. \nGender Bias Consider the overall message or theme of the text and evaluate whether it \nperpetuates any harmful stereotypes or reinforces gender roles. \nFake News Does the text use emotionally charged language or appeal to personal biases? \nAre there any logical fallacies present in the argument presented? \nHate Speech Consider the overall tone and intent of the text, and whether it appears to be \npromoting or inciting hatred or discrimination towards a specific group. \nCognitive Bias What is the author's tone and perspective? Are they objective or subjective? Are \nthere any underlying biases or prejudices evident in the language used? \nText-Level \nContext Bias \nIdentify any loaded language or emotionally charged words in the text and \nanalyze how they may influence the reader's perspective. \n \nAdditionally, a directive is attached behind the task prompt to ensure that the model responds in a \nmanner that can be processed aut omatically. This directive requests the model to render its output in \nJSON format, incorporating a 'bias' column that denotes either 1 or 0, signifying the presence or absence, \nrespectively, of bias within the provided text. \n4.  Evaluation \nThis section delves into a comprehensive examination of ChatGPT's performance in comparison to other \nfine-tuned models on six media bias identification tasks. This assessment is essential to understand the \nProceedings of the 5th International Conference on Computing and Data ScienceDOI: 10.54254/2755-2721/21/20231153\n253\neffectiveness of these models in recognizing and mitigating bias in various contexts, contributing to the \ndevelopment of more balanced AI systems. By analyzing the model performance using specific metrics, \nwe offer insights into their strengths, limitations, and potential areas for improvement. \n4.1.  Evaluation metrics \nAs recommended by the original authors of MBIB [1], two metrics are employed to assess the \nperformance of ChatGPT compared with other fine-tuned models: \n‚ö´ Micro Average F1-Score: A single F1-score is deduced for the predictions made by a model on \nthe complete test set. This method disregards the variation in the dataset from which each example is \nfrom. This metric gives an easy look into the overall performance of the model.  \n‚ö´ Macro Average F1-Score: A F1-score is calculated solely for each individual dataset in the test \nset, and the results are averaged to obtain a macro average score. This approach ensures that all datasets \ncontribute evenly to the final score regardless of their size. \nThe performances of ChatGPT and other fine-tuned models are presented in Table 4. \n \nTable 4. The performance of models over each bias identification task, measured by two metrics: micro \naverage f1-score and macro average f1-score. \nBias Type Model Micro-Score Macro-Score \nRacial Bias ChatGPT (zero-shot) 0.6288 0.7207 \n BART 0.7873 0.8679 \n ConvBERT 0.7540 0.8268 \n GPT-2 0.7792 0.8736 \nFake News ChatGPT (zero-shot) 0.5021 0.4346 \n BART 0.7060 0.6800 \n ConvBERT 0.6759 0.6844 \n GPT-2 0.6739 0.6613 \nText-level Context Bias ChatGPT (zero-shot) 0.7445 0.4414 \n BART 0.7602 0.4083 \n ConvBERT 0.7873 0.3965 \n GPT-2 0.7818 0.3941 \nHate Speech ChatGPT (zero-shot) 0.6929 0.8236 \n BART 0.8725 0.8697 \n ConvBERT 0.8784 0.8745 \n GPT-2 0.8702 0.8390 \nGender Bias ChatGPT (zero-shot) 0.4945 0.5945 \n BART 0.8262 0.7824 \n ConvBERT 0.8263 0.7779 \n GPT-2 0.8212 0.7776 \nCognitive Bias ChatGPT (zero-shot) 0.2362 0.2533 \n BART 0.6582 0.4711 \n ConvBERT 0.6673 0.5153 \n GPT-2 0.6729 0.4846 \nProceedings of the 5th International Conference on Computing and Data ScienceDOI: 10.54254/2755-2721/21/20231153\n254\n \nFigure 1. Bar charts showcasing the results of zero-shot ChatGPT and fined-tuned language models on \nthe six media bias identification tasks.  \n \nThe fine -tuned methods have similar performance, whereas the performance of ChatGPT varies \nsignificantly across tasks. The full data is attached at the appendix (Figure 1). \n4.2.  Analysis \nOverall, zero-shot ChatGPT performs weakly on most bias identification tasks when compared to fine-\ntuned methods such as BART, ConvBERT, and GPT-2. In general, this disparity might be reflective of \nthe inherent subjectivity involved in bias labeling. Despite the use of detailed codebooks to guide the \nlabeling process, the interpretation of bias varies from one individual to another; what might be \nperceived as bias by one person may not be considered by another. Fine -tuned methods like BART, \nConvBERT, and GPT -2 have the advantage of being explicitly trained to adapt to the patterns and \nsubtleties of how human labelers identify bias, and therefore they achieve higher scores. In contrast, the \nzero-shot nature of ChatGPT limits its proficiency in bias identification, as it only depends on overall \npatterns within the extensive training data it is exposed to, without learning the way the constructors of \nthe datasets label. \nIn terms of gender an d racial bias, ChatGPT significantly underperforms the fine -tuned methods. \nInterestingly, ChatGPT tends to overestimate in these domains, marking a high number of false \npositives. This discrepancy arises when ChatGPT interprets content as biased, which human evaluators \nand other models classify as neutral. An example includes the statement, \"I can't stand a Yankee voice \ncommentating on football. CRINGE,\"  which ChatGPT labels as biased, explained by \"It reinforces \ngender roles by assuming that football comme ntary is a male -dominated field\". This over -sensitivity \ncould be attributed to the exposure to bias during training, causing the model to associate specific \nstereotypes or biases with certain words or phrases, which, in this case, it associates the term \"Y ankee \nvoice\" with the assumption that football commentary is predominantly done by males. \nIn the realm of cognitive bias and fake news detection, ChatGPT struggles substantially, largely \nunderperforming compared with fine -tuned methods such as BART, ConvBE RT, and GPT -2. Even \nhumans‚Äô scuffle to identify these biases. Cognitive biases are often contextually embedded or subtly \nexpressed, without explicit words, compared to tasks such as hate speech, requiring a nuanced \nunderstanding of the subject matter and current events, which is difficult to encapsulate within a single-\nsentence input in a zero -shot learning context. Additionally, fake news adds to this challenge because \nProceedings of the 5th International Conference on Computing and Data ScienceDOI: 10.54254/2755-2721/21/20231153\n255\nof its ambiguous and often deceptive nature, making it difficult to distinguish from the  truth based on \nlanguage cues alone. Therefore, it is reasonable for ChatGPT to perform at this standard in a zero -shot \nmanner. \nHowever, ChatGPT demonstrates relatively competitive performance in detecting hate speech, \nalthough slightly trailing behind the fine-tuned models. Hate speech typically manifests through explicit, \noften aggressive language patterns and stark negative sentiments, making it more identifiable compared \nto subtler forms of bias. This comparative ease of detection illuminates ChatGPT's inherent capabilities \nin discerning explicit negativity and aggression despite the absence of task -specific fine-tuning. With \nregard to text -level context bias detection, ChatGPT exhibits results comparable to those of the fine -\ntuned methods. The good performance of ChatGPT in text-level context bias detection might stem from \nthe benefits of its large -scale architecture. The extensive architecture is better equipped to understand \nlanguage intricacies and subtle meanings inherent in human communication. The rich, multifaceted \nlinguistic understanding that ChatGPT has gained through its comprehensive training enables it to \ndecipher how contextual elements can influence conveyed meaning even in a zero-shot setting.  \nThe noticeably low macro -average scores acros s all models for this bias type can be attributed to \ninconsistent model performance across the two available datasets under this task. Specifically, all \nmodels struggle to perform well on one dataset, which contains a small number of test examples (i.e., \n100 examples), while showing better performance on the other. However, given the limited dataset \nquantity, the macro -average score, in this case, may not be an accurate representation of model \nperformance.  \nIn conclusion, while ChatGPT exhibits a certain proficiency, it may not serve as a definitive detector \nfor media bias in its present form. However, it has been shown that few-shot prompting could potentially \nimprove ChatGPT's performance across natural language processing tasks [18], but this is not done in \nthis work considering the subjectivity and variability of bias detection. The hint given to ChatGPT \nthrough few-shot prompting may be inconsistent with other examples in the datasets. In addition, human \nevaluation may be carried out to test the effectiveness of ChatGPT in accurately identifying bias in real-\nlife scenarios. This could serve to underscore the model's strengths and weaknesses, paving the way for \nunderstanding and fully eliciting its ability to help with a more balanced and healthy information system. \n5.  Conclusion \nThis research contrasts ChatGPT's proficiency in detecting various media biases with fine-tuned models \nBART, ConvBERT, and GPT-2. Although ChatGPT demonstrates notable success in identifying hate \nspeech and text-level context bias, it underperforms in tasks requiring deeper contextual understanding, \nsuch as gender, racial, and cognitive biases. These results suggest that while large language models have \nmade significant strides in language understanding, they still fall short in tasks that require a deeper, \nmore nuanced understanding of context and bias. Other factors, such as the subjectivity of bias criteria \nand the data on which ChatGPT is trained, might also contribute to the disparities in zero-shot ChatGPT's \nperformance with other fine-tuned models. The current study provides a stepping stone towards a deeper \nunderstanding of the role that large language models can play in ensuring a balanced and healthy \ninformation ecosystem. Future studies should investigate the enhancement of su ch models using \ntechniques such as few-shot prompting and human evaluation, as strides towards a balanced information \necosystem. \nReferences \n[1] Wessel, M., Horych, T., Ruas, T., Aizawa, A., Gipp, B., & Spinde, T. Introducing MBIB --the \nfirst Media Bias Identification Benchmark Task and Dataset Collection. 2023, arXiv preprint \narXiv:2304.13148. \n[2] Puglisi, R., & Snyder Jr, J. M. Empirical studies of media bias. 2015, Handbook media Econ. 1, \n647-667. \n[3] Hamborg, F., Donnay, K., & Gipp, B. Automated identification of media bias in news articles: an \ninterdisciplinary literature review. 2019, Inter. J. Digital Libraries, 20(4), 391-415. \nProceedings of the 5th International Conference on Computing and Data ScienceDOI: 10.54254/2755-2721/21/20231153\n256\n[4] Groseclose, T., & Milyo, J. A measure of media bias. 2015, The Quarterly J. Economics, 120(4), \n1191-1237. \n[5] Papacharissi, Z., & de Fatima Oliveira, M. News frames terrorism: A comparative analysis of \nframes employed in terrorism coverage in US and UK newspapers. 2008 The Inter. J. press, \n13(1), 52-74. \n[6] Smith, J., McCarthy, J. D., McPhail, C., & Augustyn, B. From protest to agenda building: \nDescription bias in media coverage of protest events in Washington, DC. 2001, Social Forces, \n79(4), 1397-1423. \n[7] Van der Pas, D. J., & Aaldering, L. Gender differences in political media coverage: A meta -\nanalysis.2020 J. Communication, 70(1), 114-143. \n[8] D‚ÄôAlonzo, S., & Tegmark, M. Machine-learning media bias. 2022 Plos one, 17(8), e0271947. \n[9] Spinde, T., Krieger, J. D., Ruas, T., Mitroviƒá, J., G√∂tz -Hahn, F., Aizawa, A., & Gipp, B. \nExploiting transformer -based multitask learning for the detection of media bias in news \narticles. 2022, Inter. Conf. Virtual Event, 225-235. \n[10] Gangula, R. R. R., Duggenpudi, S. R., & Mamidi, R. Detecting political bias in news articles \nusing headline attention. 2019 Anal. Interpret. Neur. Net. NLP  77-84. \n[11] Thota, A., Tilak, P., Ahluwalia, S., & Lohia, N. Fake news detection: a deep learning approach. \n2018, SMU Data Science Review, 1(3), 10. \n[12] Jiao, W., Wang, W. X., Huang, J. T., Wang, X., & Tu, Z. P. Is ChatGPT a good translator? Yes \nwith GPT-4 as the engine. 2023 arXiv preprint arXiv:2301.08745. \n[13] Qin, C., Zhang, A., Zhang, Z., Chen, J., Yasunaga, M., & Yang, D. Is ChatGPT a general-purpose \nnatural language processing task solver?.2023 arXiv preprint arXiv:2302.06476. \n[14] Yang, X., Li, Y., Zhang, X., Chen, H., & Cheng, W. Exploring the limits of chatgpt for query or \naspect-based text summarization. 2023 arXiv preprint arXiv:2302.08081. \n[15] Wang, Z., Xie, Q., Ding, Z., Feng, Y., & Xia, R. Is ChatGPT a good sentiment analyzer? A \npreliminary study. 2023 arXiv preprint arXiv:2304.04339. \n[16] Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Zettlemoyer, L. Bart: \nDenoising sequence-to-sequence pre-training for natural language generation, translation, and \ncomprehension. 2019, arXiv preprint arXiv:1910.13461. \n[17] Jiang, Z. H., Yu, W., Zhou, D., Chen, Y., Feng, J., & Yan, S. Convbert: Improving bert with span-\nbased dynamic convolution. 2023 Adv. Neur. Infor. Proc. Sys. 33, 12837-12848. \n[18] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. Language mo dels are \nunsupervised multitask learners. 2019, OpenAI blog, 1(8), 9. \n \nProceedings of the 5th International Conference on Computing and Data ScienceDOI: 10.54254/2755-2721/21/20231153\n257",
  "topic": "Media bias",
  "concepts": [
    {
      "name": "Media bias",
      "score": 0.6892882585525513
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5942259430885315
    },
    {
      "name": "Computer science",
      "score": 0.48267605900764465
    },
    {
      "name": "Identification (biology)",
      "score": 0.45604684948921204
    },
    {
      "name": "Cognitive bias",
      "score": 0.4447489380836487
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4436300992965698
    },
    {
      "name": "Gender bias",
      "score": 0.42981287837028503
    },
    {
      "name": "Psychology",
      "score": 0.4194602966308594
    },
    {
      "name": "Cognitive psychology",
      "score": 0.41830191016197205
    },
    {
      "name": "Cognition",
      "score": 0.3410188555717468
    },
    {
      "name": "Social psychology",
      "score": 0.25426948070526123
    },
    {
      "name": "Political science",
      "score": 0.14214393496513367
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I316699675",
      "name": "Hodges University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I170897317",
      "name": "Duke University",
      "country": "US"
    }
  ]
}