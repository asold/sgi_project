{
    "title": "Understanding Unintended Memorization in Language Models Under Federated Learning",
    "url": "https://openalex.org/W3170672407",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A4298618086",
            "name": "Om Dipakbhai Thakkar",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2508746783",
            "name": "Swaroop Ramaswamy",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2907465900",
            "name": "Rajiv Mathews",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A234857393",
            "name": "Françoise Beaufays",
            "affiliations": [
                "Google (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2980154417",
        "https://openalex.org/W2761138375",
        "https://openalex.org/W4318619660",
        "https://openalex.org/W2781189798",
        "https://openalex.org/W2535838896",
        "https://openalex.org/W4287905288",
        "https://openalex.org/W2947160092",
        "https://openalex.org/W3037491300",
        "https://openalex.org/W2963741669",
        "https://openalex.org/W2541884796",
        "https://openalex.org/W1873763122",
        "https://openalex.org/W2986692543",
        "https://openalex.org/W2944525415",
        "https://openalex.org/W2952087428",
        "https://openalex.org/W2051267297",
        "https://openalex.org/W4288358239",
        "https://openalex.org/W4288023537",
        "https://openalex.org/W2530417694",
        "https://openalex.org/W3006488141",
        "https://openalex.org/W4294106961",
        "https://openalex.org/W3102891118",
        "https://openalex.org/W4288333953",
        "https://openalex.org/W2788502731",
        "https://openalex.org/W2998982695",
        "https://openalex.org/W2766255512",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W3102407811",
        "https://openalex.org/W2949461276",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2888919756",
        "https://openalex.org/W2952604841",
        "https://openalex.org/W2753855453",
        "https://openalex.org/W1981029888",
        "https://openalex.org/W2512472178",
        "https://openalex.org/W2767079719",
        "https://openalex.org/W4292084264",
        "https://openalex.org/W2985986882",
        "https://openalex.org/W2594311007",
        "https://openalex.org/W2963612912",
        "https://openalex.org/W2900120080",
        "https://openalex.org/W1980287119",
        "https://openalex.org/W1557833142",
        "https://openalex.org/W2981206218",
        "https://openalex.org/W3038028469",
        "https://openalex.org/W4297687186",
        "https://openalex.org/W2535690855",
        "https://openalex.org/W2963559079"
    ],
    "abstract": "Recent works have shown that language models (LMs), e.g., for next word prediction (NWP), have a tendency to memorize rare or unique sequences in the training data. Since useful LMs are often trained on sensitive data, it is critical to identify and mitigate such unintended memorization. Federated Learning (FL) has emerged as a novel framework for large-scale distributed learning tasks. It differs in many aspects from the well-studied central learning setting where all the data is stored at the central server, and minibatch stochastic gradient descent is used to conduct training. This work is motivated by our observation that NWP models trained under FL exhibited remarkably less propensity to such memorization compared to the central learning setting. Thus, we initiate a formal study to understand the effect of different components of FL on unintended memorization in trained NWP models. Our results show that several differing components of FL play an important role in reducing unintended memorization. First, we discover that the clustering of data according to users—which happens by design in FL—has the most significant effect in reducing such memorization. Using the Federated Averaging optimizer with larger effective minibatch sizes for training causes a further reduction. We also demonstrate that training in FL with a user-level differential privacy guarantee results in models that can provide high utility while being resilient to memorizing out-of-distribution phrases with thousands of insertions across over a hundred users in the training set.",
    "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1–10\nJuly 5–10, 2020. ©2020 Association for Computational Linguistics\nhttps://doi.org/10.26615/978-954-452-056-4_001\n1\nUnderstanding Unintended Memorization in Language Models\nUnder Federated Learning\nOm Thakkar and Swaroop Ramaswamy and Rajiv Mathews and Françoise Beaufays\nGoogle LLC,\nMountain View, CA, U.S.A.\n{omthkkr, swaroopram, mathews, fsb} @google.com\nAbstract\nRecent works have shown that language mod-\nels (LMs), e.g., for next word prediction\n(NWP), have a tendency to memorize rare or\nunique sequences in the training data. Since\nuseful LMs are often trained on sensitive data,\nit is critical to identify and mitigate such un-\nintended memorization. Federated Learning\n(FL) has emerged as a novel framework for\nlarge-scale distributed learning tasks. It differs\nin many aspects from the well-studied central\nlearning setting where all the data is stored\nat the central server, and minibatch stochas-\ntic gradient descent is used to conduct train-\ning. This work is motivated by our observa-\ntion that NWP models trained under FL exhib-\nited remarkably less propensity to such mem-\norization compared to the central learning set-\nting. Thus, we initiate a formal study to under-\nstand the effect of different components of FL\non unintended memorization in trained NWP\nmodels. Our results show that several differing\ncomponents of FL play an important role in\nreducing unintended memorization. First, we\ndiscover that the clustering of data according\nto users—which happens by design in FL—\nhas the most signiﬁcant effect in reducing such\nmemorization. Using the Federated Averaging\noptimizer with larger effective minibatch sizes\nfor training causes a further reduction. We also\ndemonstrate that training in FL with a user-\nlevel differential privacy guarantee results in\nmodels that can provide high utility while be-\ning resilient to memorizing out-of-distribution\nphrases with thousands of insertions across\nover a hundred users in the training set.\n1 Introduction\nThere is a growing line of work (Fredrikson et al.,\n2015; Wu et al., 2016; Shokri et al., 2017; Carlini\net al., 2018; Song and Shmatikov, 2019) demon-\nstrating that neural networks can leak information\nabout the underlying training data in unexpected\nways. Many of these works show that language\nmodels (LMs), which include commonly-used next\nword prediction (NWP) models, are prone to unin-\ntentionally memorize rarely-occurring phrases in\nthe data. Large-scale LM training often involves\ntraining over sensitive data, and such memorization\ncan result in blatant leaks of privacy (e.g., (Munroe,\n2019)). Thus, it is crucial to measure such mem-\norization in trained LMs, and identify mitigation\ntechniques to ensure privacy of the training data.\nThe framework of Federated Learning\n(FL) (McMahan et al., 2017a; McMahan and\nRamage, 2017) has emerged as a popular approach\nfor training neural networks on a large corpus\nof decentralized on-device data (e.g., (Kone ˇcný\net al., 2016; Konecný et al., 2016; Bonawitz et al.,\n2017; Hard et al., 2018; Bonawitz et al., 2019)).\nFL operates in an iterative fashion: in each round,\nsampled client devices receive the current global\nmodel from a central server to compute an update\non their locally-stored data, and the server aggre-\ngates these updates using, for e.g., the Federated\nAveraging (FedAvg) algorithm (McMahan et al.,\n2017a), to build a new global model. A hallmark\nof FL is that each participating device only sends\nmodel weights to the central server; raw data never\nleaves the device, remaining locally-cached. This,\nby itself, is not sufﬁcient to provide formal privacy\nguarantees for the training data. However, this\nwork is motivated by the observation (described\nin detail in Section 3) that NWP models trained\nunder the canonical setting of FL exhibited\nresilience to memorize rare phrases in spite of\nhundreds of occurrences in the training data. Note\nthat FL does differ in many aspects from the\nwell-studied (Shokri et al., 2017; Carlini et al.,\n2018; Song and Shmatikov, 2019) central learning\nsetting where all the data is stored at a central\nserver, and minibatch stochastic gradient descent\n(SGD) is used to conduct training. While training\nNWP models via central learning, we observed that\nphrases with even tens of occurrences were easily\n2\n(a) A user selected as a secret sharer\nfor a canary.\n(b) An example in a secret sharer’s lo-\ncal dataset being replaced by the ca-\nnary.\nFigure 1: An illustration of our federated secret-sharer framework, using the canary “My SSN is 123-45-6789\".\nmemorized, in line with prior work (Carlini et al.,\n2018) that showed the propensity of such models\nto memorize phrases with even one occurrence in\nthe training set. Thus, we initiate a formal study to\nunderstand the effect of the different components\nof FL, compared to the central learning setting, on\nunintended memorization in trained NWP models.\nWe also study the extent to which a guarantee of\nDifferential Privacy (DP) (Dwork et al., 2006c,a)\nreduces such memorization. DP has become the\nstandard for performing learning tasks over sensi-\ntive data, and has been adopted by companies like\nGoogle (Erlingsson et al., 2014; Bittau et al., 2017;\nErlingsson et al., 2020), Apple (Apple, 2017), Mi-\ncrosoft (Ding et al., 2017), and LinkedIn (Rogers\net al., 2020), as well as the US Census Bureau (Kuo\net al., 2018). Intuitively, DP prevents an adver-\nsary from conﬁdently making conclusions about\nwhether any particular user’s data was used to train\na model, even while having access to the model\nand arbitrary external side information.\nThe Federated Secret Sharer:We build on the “se-\ncret sharer\" framework (Carlini et al., 2018) that\nwas designed to measure the unintended memoriza-\ntion in generative models. At a high-level, out-of-\ndistribution examples (called canaries) are inserted\ninto a training corpus, and a model trained on this\ncorpus is then evaluated using various techniques\nto measure the extent to which the model has mem-\norized the canaries. Since datasets in FL are in-\nherently partitioned according to users, we adapt\nthe secret sharer framework to the FL regime by\nintroducing two parameters to control the presence\nof a canary in such settings. An illustration of our\nfederated secret sharer framework is shown in Fig-\nure 1. Given a canary with parameters pu and pe,\nwe let pu be the probability with which each user\nin a dataset is selected to be a “secret sharer\" of the\ncanary (Figure 1a), whereas pe denotes the prob-\nability with which each example in such a secret\nsharer’s data is replaced by the canary (Figure 1b).\nWe use Poisson sampling for both user-selection\nand example-replacement. The secret sharer selec-\ntion phase precedes canary insertion to model real-\nworld settings where occurrences of user-speciﬁc\nunique or rare out-of-distribution canaries are typ-\nically limited to a small group of users, but such\nusers can exhibit high usage for those canaries.\nContributions: Our empirical evaluations demon-\nstrate the following key contributions. First, we\nobserve that clustering training data according to\nusers, which happens by design in distributed learn-\ning settings like FL, has a signiﬁcant effect in re-\nducing unintended memorization for NWP models.\nNext, given a dataset partitioned by users, we show\nthat replacing the learning optimizer from SGD to\nFederated Averaging and increasing the effective\nminibatch size provides a further reduction in such\nmemorization. Lastly, we demonstrate that training\nin FL with user-level differential privacy (DP) re-\nsults in models that can provide comparable utility\nwhile being resilient to memorizing canaries with\nthousands of insertions spread across over a hun-\ndred users in the training set. Prior work (Carlini\net al., 2018) has shown that models trained with\nrecord-level DP do not exhibit unintended mem-\norization for a single insertion of a canary. We\nprovide evidence of models being resilient to mem-\norizing canaries for orders of magnitude higher\ninsertions, at the stronger user-level privacy.\n1.1 Related Work\nApart from (Carlini et al., 2018) which this work\nbuilds upon, other works (Song and Shmatikov,\n2019) have also studied memorization in generative\ntext models. The FL paradigm, which is a major\n3\nfocus of this work, has been used to train mul-\ntiple production scale models (Hard et al., 2018;\nRamaswamy et al., 2019; Chen et al.). Kairouz\net al. (2019) provides an excellent overview of the\nstate-of-the-art in the ﬁeld, along with a suite of in-\nteresting open problems. This work also studies the\neffectiveness of a user-level DP guarantee in reduc-\ning unintended memorization. While many works\non DP focus on record-level DP guarantees (which\nusually cannot be directly extended to strong user-\nlevel DP guarantees), recent works (e.g., (McMa-\nhan et al., 2017b; Jain et al., 2018; Augenstein\net al., 2020; Andrew et al., 2021)) have designed\ntechniques tailored to user-level DP guarantees.\n2 Contrasting Federated Learning with\nCentral Learning\nNow, we take a deeper look at how the well-studied\ncentral learning framework differs from the canoni-\ncal setting of FL for LM training. We are interested\nin differences that might have an effect on unin-\ntended memorization. We identify three such com-\nponents: (1) Data Processed per Update: Central\nlearning typically ingests data as records/sentences.\nOn the other hand, FL operates at the granularity\nof a user, with each user having their own set of\nsentences locally. Typically, the amount of data pro-\ncessed per model update in central learning is much\nsmaller in comparison to FL. (2) Learning Tech-\nnique: In central learning, the model is updated via\nSGD on a minibatch of records. In the canonical\nsetting of FL, a model update typically corresponds\nto Federated Averaging over a minibatch of users:\nan average of the differences between the current\nmodel and the model obtained after several SGD\nsteps on the local data of a user. (3) Independent\nand Identically Distributed (IID) Data: To reduce\nvariance in learning, the data in central learning\nis shufﬂed before training (and/or each update in-\nvolves a randomly sampled minibatch). Thus, each\nminibatch can be estimated to be drawn IID from\nthe data. Datasets in FL are naturally grouped\naccording to potentially heterogeneous users, re-\nsulting in non-IID data even though each minibatch\nof users may be randomly sampled.1\n1We do not discuss unbalanced datasets, i.e., the fact that\nusers can have varying amounts of local data, since Federated\nAveraging in FL deals with such imbalances by weighing each\nclient update according to the size of its local data.\n3 Empirical Evaluation\nExperimental Setup: Our model architecture\n(1.3M parameters) mirrors the one used in Hard\net al. (2018). We create a modiﬁed version of the\nStack Overﬂow dataset (Overﬂow, 2018) hosted by\nTensorFlow Federated (Ingerman and Ostrowski,\n2019), containing 392K users (93M records). For\nan IID version of this dataset, we randomly shuf-\nﬂe all the records, and create synthetic users hav-\ning data assigned sequentially from the shufﬂed\nrecords. Since our model is a word-level language\nmodel, we follow the methodology used by Carlini\net al. (2018) for their experiments with the GMail\nSmart Compose model (Chen et al., 2019). We in-\nsert random 5-word canaries with conﬁgurations in\nthe cross product of pu ∈{1/50K,3/50K,1/5K}\nand pe ∈{1%,10%,100%}, with 10 different ca-\nnaries for each (pu,pe) conﬁguration, resulting in\nthe insertion of 90 different canaries. Given a pre-\nﬁx of a canary, we use two methods to evaluate the\nunintended memorization of the sufﬁx for a model:\nRandom Sampling (RS), which for a 2-word preﬁx\nmeasures if the canary has the least log-perplexity\namong 2M random sufﬁxes, and Beam Search (BS),\nwhich uses a greedy beam search to see if the ca-\nnary is in its top 5 most-likely 5-word continuations\nfrom a 1-word preﬁx. We measure the utility of\na model with accuracy and perplexity on the test\npartition of the unmodiﬁed Stack Overﬂow dataset.\nEmpirical Results: We present the results of our\nexperiments on evaluating unintended memoriza-\ntion under different training regimes ranging from\ncanonical FL to central learning. For all our ex-\nperiments using SGD, we train models for 37.5M\nsteps, whereas we train for 8000 rounds for the ex-\nperiments using FedAvg. For the largest minibatch\nsizes used in both settings (256 records for SGD,\nand 5000 users for FedAvg), these checkpoints cor-\nrespond to training for 100 epochs. Table 1 shows\nthe number of canaries (out of 90) that show up\nas memorized via both the RS and BS methods.\nThe utility of all the evaluated models is similar;\naccuracy varies from 23.7 −24.6%, and perplexity\nvaries from 57.3 −64.3 across all models.2\nTraining in FL with DP Federated Averaging\n(DP-FedAvg): Next, we evaluate the extent to\nwhich training using DP-FedAvg is resilient to such\n2We defer the utility measurements to Table 3.\n4\nOptimizer Data Batch Size RS BS\n500 users 21 0\n1K users 23 1\nNon-IID 2K users 19 1\nFedAvg 5K users 26 2\n500 users 66 56\nIID 1K users 69 58\n2K users 67 56\n5K users 65 58\n32 records 37 19\n64 records 49 36\nNon-IID 128 records 48 34\n256 records 51 39\nSGD 32 records 54 42\nIID 64 records 54 42\n128 records 52 45\n256 records 53 43\nTable 1: Results for the number of inserted canaries\n(out of 90) memorized via the Random Sampling (RS)\nand Beam Search (BS) methods for various models\nevaluated at 8000 rounds when sampling users (Fe-\ndAvg), and37.5M steps when sampling records (SGD).\nmemorization. To provide the strongest user-level\nDP while obtaining high utility, we conduct exper-\niments only for our largest minibatch size of 5K\nusers. The results are presented in Table 2.\nOptimizer RS BS Acc. % Perp.\nFedAvg 26 2 24.5 58.2\nDP-FedAvg 12 0 23.3 68.5\nTable 2: Unintended memorization and utility for a\nmodel trained with (18.8,10−7)-DP in FL (non-IID\nusers) with 5k users/round for 100 epochs.\n3.1 Discussion\nClustering data according to users:The results\nfrom our experiments strongly indicate that cluster-\ning data according to users signiﬁcantly reduces un-\nintended memorization. This is evident by consider-\ning the measurements in Table 1 in pairs where the\nonly differing component among them is whether\nthe data is IID or not. The number of epochs taken\nover the dataset to train the models on which we\nmeasure memorization is the same for any particu-\nlar minibatch size, irrespective of whether the data\nis IID. Thus, the number of times the inserted ca-\nnaries were encountered during training is the same.\nHowever, the amount of memorization observed\nis always lower when the data is Non-IID. This\neffect is more pronounced in the settings where Fe-\ndAvg is used as the training method. For instance,\nfor a minibatch size of bu = 500 users, training\nwith FedAvg on IID data results in 66 (56) canaries\nshowing up as memorized via the RS (BS) method.\nThe same conﬁguration on Non-IID data results\nin the RS method classifying only 21 canaries as\nmemorized, and the BS method not being able to\nextract any canary even after 8000 rounds of train-\ning. In addition to the data being clustered, the\ninserted canaries are clustered as well, which we\nconjecture to be playing a crucial role in reducing\nsuch memorization.\nVarying data per update:Fixing the optimizer to\nSGD/FedAvg and the data to be IID/non-IID, we\ndo not see any signiﬁcant effect of varying the\nminibatch size on such memorization.3\nTraining non-IID user data with FedAvg and\nlarger effective minibatches:The smallest mini-\nbatch size for our FedAvg experiments is 500\nusers,4 and as each user contains ≈250 records,\nthe effective minibatch size is ≈125K records. In\ncomparison, the largest minibatch size for which\nwe are able to conduct SGD training is 256 records.\nFocusing on the results in Table 1 using Non-IID\ndata, we ﬁnd that using FedAvg and having larger\neffective minibatches per round causes a signiﬁ-\ncant reduction in unintended memorization when\ncompared to training with SGD and smaller mini-\nbatches.5\nTraining with DP-FedAvg in FL:Our aim is to\ntest the extent to which NWP models trained with\nDP-FedAvg in FL are resilient to such memoriza-\ntion. By deﬁnition, a user-level DP guarantee is\nintended to be resilient to changes w.r.t. any one\n3For the case of SGD and Non-IID data, while unintended\nmemorization does seem to increase with the minibatch size,\nwe do not observe the increase consistently. Additional inves-\ntigation for potential causes of such a trend are beyond the\nscope of this work.\n4We do not train with smaller user minibatch sizes as we\nwant the number of training epochs for the lowest settings in\nFedAvg and SGD to be similar at 8000 training rounds.\n5Looking at the same set of results for IID data, the trend\nseems to be moving in the direction of increasing memoriza-\ntion. However, since the magnitude of the effect is much\nsmaller, we deem that further investigation is required for this\ncase, which we leave for future work.\n5\nuser’s data. Some of our inserted canaries are\nshared by ∼100 users (with ∼24.5K occurrences\nin the training data). In spite of such high levels\nof canary insertion, and our FL models exhibiting\nthe least amount of unintended memorization (Ta-\nble 1), we see that training with DP-FedAvg results\nin a signiﬁcantly reduced memorization. Our re-\nsults are noteworthy as, in spite of our DP model\nexhibiting extremely low unintended memoriza-\ntion, it also provides comparable utility as a model\ntrained via FedAvg, along with a user-level guar-\nantee of (18.8,10−7)-DP. While strengthening the\nprivacy guarantee of DP-FedAvg by increasing the\nnoise added to the model update in each training\nround can further reduce such memorization, it can\nalso start signiﬁcantly affecting model utility. De-\nsigning methods that improve the privacy-utility\ntrade-offs is an interesting direction, which is be-\nyond the scope of this work.\n4 Conclusion\nIn this work, we conduct a formal study to under-\nstand the effect of the different components of Fed-\nerated Learning (FL), on the unintended memoriza-\ntion in trained next word prediction (NWP) models,\nas compared to the well-studied central learning.\nFrom our results, we observe that the components\nof FL exhibit a synergy in reducing such memo-\nrization. To our surprise, user-based clustering of\ndata (which occurs as a natural consequence in the\nFL setting) has the most signiﬁcant effect in the\nreduction. Moreover, training using Federated Av-\neraging and larger effective minibatches reduces\nsuch memorization further. Lastly, we observe that\ntraining in FL with a user-level differential privacy\nguarantee results in models that can provide com-\nparable utility while being resilient to memorizing\ncanaries with thousands of insertions across over a\nhundred users in the training set.\nRecent work (Karimireddy et al., 2019) has\nshown that, in general, such heterogeneity in the\ntraining data can result in a slower and unstable\nconvergence due to factors such as “client-drift\".\nFor all of the experiments with non-IID data, we\nobserve that the utility of the trained models is com-\nparable to those trained on IID data, and we leave\nfurther exploration into why client-drift may not\nplay a signiﬁcant role in our experiments for future\nwork. Next, while our extensive evaluation is for a\npractical NWP model on a real-world benchmark\ndataset, the degree of unintended memorization in\ngeneral can depend on the model architecture and\nthe dataset used for training. Lastly, the secret-\nsharer line of methods for measuring unintended\nmemorization operate at the granularity of a record.\nFor future work, it will be interesting to design\nstronger attacks targeting data at the granularity of\na user, and measure the resilience of models trained\nvia FL, against such memorization.\nAcknowledgements\nThe authors would like to thank Mingqing Chen,\nAndrew Hard, and Gautam Kamath for their helpful\ncomments towards improving the paper.\nReferences\nGalen Andrew, Om Thakkar, H. Brendan McMahan,\nand Swaroop Ramaswamy. 2021. Differentially pri-\nvate learning with adaptive clipping.\nDifferential Privacy Team Apple. 2017. Learning with\nprivacy at scale.\nSean Augenstein, H. Brendan McMahan, Daniel\nRamage, Swaroop Ramaswamy, Peter Kairouz,\nMingqing Chen, Rajiv Mathews, and Blaise Agüera\ny Arcas. 2020. Generative models for effective\nML on private, decentralized datasets. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nAndrea Bittau, Úlfar Erlingsson, Petros Maniatis, Ilya\nMironov, Ananth Raghunathan, David Lie, Mitch\nRudominer, Ushasree Kode, Julien Tinnés, and Bern-\nhard Seefeld. 2017. Prochlo: Strong privacy for ana-\nlytics in the crowd. In Proceedings of the 26th Sym-\nposium on Operating Systems Principles, Shanghai,\nChina, October 28-31, 2017, pages 441–459. ACM.\nKeith Bonawitz, Hubert Eichner, Wolfgang Grieskamp,\nDzmitry Huba, Alex Ingerman, Vladimir Ivanov,\nChloé Kiddon, Jakub Konecný, Stefano Mazzocchi,\nH. Brendan McMahan, Timon Van Overveldt, David\nPetrou, Daniel Ramage, and Jason Roselander. 2019.\nTowards federated learning at scale: System design.\nCoRR, abs/1902.01046.\nKeith Bonawitz, Vladimir Ivanov, Ben Kreuter, Anto-\nnio Marcedone, H. Brendan McMahan, Sarvar Patel,\nDaniel Ramage, Aaron Segal, and Karn Seth. 2017.\nPractical secure aggregation for privacy-preserving\nmachine learning. In Proceedings of the 2017\nAssociation for Computing Machinery (ACM) Spe-\ncial Interest Group on Security, Audit and Control\n(SIGSAC) Conference on Computer and Communi-\ncations Security, CCS ’17, pages 1175–1191, New\nYork, NY , USA. ACM.\n6\nNicholas Carlini, Chang Liu, Jernej Kos, Úlfar Erlings-\nson, and Dawn Song. 2018. The secret sharer: Mea-\nsuring unintended neural network memorization &\nextracting secrets. Computing Research Repository\n(CoRR), abs/1802.08232.\nMia Xu Chen, Benjamin N Lee, Gagan Bansal, Yuan\nCao, Shuyuan Zhang, Justin Lu, Jackie Tsay, Yinan\nWang, Andrew M Dai, Zhifeng Chen, et al. 2019.\nGmail smart compose: Real-time assisted writing.\nIn Proceedings of the 25th ACM SIGKDD Interna-\ntional Conference on Knowledge Discovery & Data\nMining, pages 2287–2295.\nMingqing Chen, Ananda Theertha Suresh, Rajiv Math-\news, Adeline Wong, Cyril Allauzen, Françoise Bea-\nufays, and Michael Riley. Federated learning of\nn-gram language models. In Proceedings of the\n23rd Conference on Computational Natural Lan-\nguage Learning, CoNLL 2019, Hong Kong, China,\nNovember 3-4, 2019.\nBolin Ding, Janardhan Kulkarni, and Sergey Yekhanin.\n2017. Collecting telemetry data privately. In Ad-\nvances in Neural Information Processing Systems\n30: Annual Conference on Neural Information Pro-\ncessing Systems 2017, 4-9 December 2017, Long\nBeach, CA, USA, pages 3571–3580.\nCynthia Dwork, Krishnaram Kenthapadi, Frank McSh-\nerry, Ilya Mironov, and Moni Naor. 2006a. Our data,\nourselves: Privacy via distributed noise generation.\nIn EUROCRYPT, pages 486–503.\nCynthia Dwork, Krishnaram Kenthapadi, Frank Mcsh-\nerry, Ilya Mironov, and Moni Naor. 2006b. Our data,\nourselves: Privacy via distributed noise generation.\nIn EUROCRYPT, pages 486–503.\nCynthia Dwork, Frank McSherry, Kobbi Nissim, and\nAdam Smith. 2006c. Calibrating noise to sensitivity\nin private data analysis. In Theory of Cryptography\nConference, pages 265–284. Springer.\nÚlfar Erlingsson, Vitaly Feldman, Ilya Mironov,\nAnanth Raghunathan, Shuang Song, Kunal Talwar,\nand Abhradeep Thakurta. 2020. Encode, shufﬂe, an-\nalyze privacy revisited: Formalizations and empiri-\ncal evaluation. CoRR, abs/2001.03618.\nÚlfar Erlingsson, Vasyl Pihur, and Aleksandra Ko-\nrolova. 2014. Rappor: Randomized aggregatable\nprivacy-preserving ordinal response. In Proceedings\nof the 2014 Association for Computing Machinery\n(ACM) SIGSAC conference on computer and commu-\nnications security , pages 1054–1067. Association\nfor Computing Machinery (ACM).\nMatt Fredrikson, Somesh Jha, and Thomas Risten-\npart. 2015. Model inversion attacks that exploit\nconﬁdence information and basic countermeasures.\nIn Proceedings of the 22Nd Association for Com-\nputing Machinery (ACM) SIGSAC Conference on\nComputer and Communications Security , CCS ’15,\npages 1322–1333, New York, NY , USA. Association\nfor Computing Machinery (ACM).\nAndrew Hard, Kanishka Rao, Rajiv Mathews,\nFrançoise Beaufays, Sean Augenstein, Hubert\nEichner, Chloé Kiddon, and Daniel Ramage. 2018.\nFederated learning for mobile keyboard prediction.\nCoRR, abs/1811.03604.\nAlex Ingerman and Krzys Ostrowski. 2019. Introduc-\ning tensorﬂow federated.\nPrateek Jain, Om Thakkar, and Abhradeep Thakurta.\n2018. Differentially private matrix completion revis-\nited. In Proceedings of the 35th International Con-\nference on Machine Learning, ICML 2018, Stock-\nholmsmässan, Stockholm, Sweden, July 10-15, 2018,\npages 2220–2229.\nPeter Kairouz, H. Brendan McMahan, Brendan Avent,\nAurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji,\nKeith Bonawitz, Zachary Charles, Graham Cor-\nmode, Rachel Cummings, Rafael G. L. D’Oliveira,\nSalim El Rouayheb, David Evans, Josh Gard-\nner, Zachary Garrett, Adrià Gascón, Badih Ghazi,\nPhillip B. Gibbons, Marco Gruteser, Zaïd Har-\nchaoui, Chaoyang He, Lie He, Zhouyuan Huo,\nBen Hutchinson, Justin Hsu, Martin Jaggi, Tara Ja-\nvidi, Gauri Joshi, Mikhail Khodak, Jakub Konecný,\nAleksandra Korolova, Farinaz Koushanfar, Sanmi\nKoyejo, Tancrède Lepoint, Yang Liu, Prateek Mittal,\nMehryar Mohri, Richard Nock, Ayfer Özgür, Ras-\nmus Pagh, Mariana Raykova, Hang Qi, Daniel Ra-\nmage, Ramesh Raskar, Dawn Song, Weikang Song,\nSebastian U. Stich, Ziteng Sun, Ananda Theertha\nSuresh, Florian Tramèr, Praneeth Vepakomma,\nJianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Fe-\nlix X. Yu, Han Yu, and Sen Zhao. 2019. Advances\nand open problems in federated learning. CoRR,\nabs/1912.04977.\nSai Praneeth Karimireddy, Satyen Kale, Mehryar\nMohri, Sashank J. Reddi, Sebastian U. Stich, and\nAnanda Theertha Suresh. 2019. SCAFFOLD:\nstochastic controlled averaging for on-device feder-\nated learning. CoRR, abs/1910.06378.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nJakub Konecný, H. Brendan McMahan, Felix X. Yu,\nPeter Richtárik, Ananda Theertha Suresh, and Dave\nBacon. 2016. Federated learning: Strategies for im-\nproving communication efﬁciency. Computing Re-\nsearch Repository (CoRR), abs/1610.05492.\nJakub Kone ˇcný, H. Brendan McMahan, Daniel Ram-\nage, and Peter Richtárik. 2016. Federated optimiza-\ntion: Distributed machine learning for on-device in-\ntelligence. ArXiv, abs/1610.02527.\nYu-Hsuan Kuo, Cho-Chun Chiu, Daniel Kifer, Michael\nHay, and Ashwin Machanavajjhala. 2018. Dif-\nferentially private hierarchical count-of-counts his-\ntograms. PVLDB, 11(11):1509–1521.\n7\nBrendan McMahan, Eider Moore, Daniel Ramage,\nSeth Hampson, and Blaise Agüera y Arcas. 2017a.\nCommunication-efﬁcient learning of deep networks\nfrom decentralized data. In Proceedings of the 20th\nInternational Conference on Artiﬁcial Intelligence\nand Statistics, AISTATS 2017, 20-22 April 2017,\nFort Lauderdale, FL, USA, pages 1273–1282.\nBrendan McMahan and Daniel Ramage. 2017. Feder-\nated learning: Collaborative machine learning with-\nout centralized training data. Google Research Blog,\n3.\nH. Brendan McMahan, Daniel Ramage, Kunal Talwar,\nand Li Zhang. 2017b. Learning differentially private\nlanguage models without losing accuracy. CoRR,\nabs/1710.06963.\nI. Mironov. 2017. Rényi differential privacy. In\n2017 Institute of Electrical and Electronics Engi-\nneers (IEEE) 30th Computer Security Foundations\nSymposium (CSF), pages 263–275.\nRandall Munroe. 2019. xkcd: Predictive models.\nhttps://xkcd.com/2169/.\nStack Overﬂow. 2018. The Stack Overﬂow\nData. https://www.kaggle.com/\nstackoverflow/stackoverflow.\nNing Qian. 1999. On the momentum term in gradi-\nent descent learning algorithms. Neural Networks,\n12(1):145–151.\nSwaroop Ramaswamy, Rajiv Mathews, Kanishka Rao,\nand Françoise Beaufays. 2019. Federated learning\nfor emoji prediction in a mobile keyboard.\nRyan Rogers, Subbu Subramaniam, Sean Peng, David\nDurfee, Seunghyun Lee, Santosh Kumar Kan-\ncha, Shraddha Sahay, and Parvez Ahammad. 2020.\nLinkedin’s audience engagements api: A privacy\npreserving data analytics system at scale.\nR. Shokri, M. Stronati, C. Song, and V . Shmatikov.\n2017. Membership inference attacks against ma-\nchine learning models. In 2017 Institute of Electri-\ncal and Electronics Engineers (IEEE) Symposium on\nSecurity and Privacy (SP), pages 3–18.\nCongzheng Song and Vitaly Shmatikov. 2019. Au-\nditing data provenance in text-generation models.\nIn Proceedings of the 25th ACM SIGKDD Interna-\ntional Conference on Knowledge Discovery & Data\nMining, KDD 2019, Anchorage, AK, USA, August\n4-8, 2019, pages 196–206. ACM.\nYu-Xiang Wang, Borja Balle, and Shiva Prasad Ka-\nsiviswanathan. 2019. Subsampled renyi differen-\ntial privacy and analytical moments accountant. In\nThe 22nd International Conference on Artiﬁcial In-\ntelligence and Statistics, AISTATS 2019, 16-18 April\n2019, Naha, Okinawa, Japan, pages 1226–1235.\nX. Wu, M. Fredrikson, S. Jha, and J. F. Naughton. 2016.\nA methodology for formalizing model-inversion at-\ntacks. In 2016 Institute of Electrical and Electronics\nEngineers (IEEE) 29th Computer Security Founda-\ntions Symposium (CSF), pages 355–370.\nA Preliminaries\nA.1 Measuring Unintended Memorization\nFollowing (Carlini et al., 2018), this work assumes\na threat model of curious or malevolent users hav-\ning a black-box query access to models, in that\nthey see only the models’ output probabilities (or\nlogits). We also assume that users can adaptively\nquery models multiple times, thus posing a threat\nof extracting uncommon word combinations.\nNow, we describe the Secret Sharer framework\nof (Carlini et al., 2018). First, random sequences\ncalled canaries are inserted into the training data.\nThe canaries are constructed based on a preﬁxed\nformat sequence. For instance, to design the frame-\nwork for a character-level model, the format could\nbe “My SSN is xxx-xx-xxxx\", where each xcan\ntake a random value from digits 0 to 9. Next, the\ntarget model is trained on the modiﬁed dataset con-\ntaining the canaries. Lastly, methods like Random\nSampling and Beam Search (both formally deﬁned\nin Section 3) are used to efﬁciently measure the\nextent to which the model has “memorized\" the in-\nserted random canaries, and whether it is possible\nfor an adversary with partial knowledge to extract\nthe canary. For instance, if a canary is classiﬁed\nas memorized via our Beam Search method, then\ngiven black-box access to the trained model, an ad-\nversary with the knowledge of only the ﬁrst word of\nthe inserted canary can extract it completely with a\nsimple beam search.\nA.2 Differential Privacy\nTo establish the notion of differential privacy\n(Dwork et al., 2006c,b), we ﬁrst deﬁne neighboring\ndatasets. We will refer to a pair of datasets D,D′\nas neighbors if D′can be obtained by the addition\nor removal of all the examples associated with one\nuser from D, to be able to provide a user-level DP\nguarantee.6\nDeﬁnition A.1 (Differential privacy (Dwork et al.,\n2006c,b)). A randomized algorithm Ais (ϵ,δ)-\ndifferentially private if, for any pair of neighboring\n6This is in contrast to a record-level DP guarantee, where\nneighboring datasets differ in the addition/removal of exactly\none example.\n8\ndatasets D and D′, and for all events Sin the\noutput range of A, we have\nPr[A(D) ∈S] ≤eϵ ·Pr[A(D′) ∈S] +δ\nwhere the probability is taken over the random\ncoins of A.\nFor meaningful privacy guarantees, ϵis assumed\nto be a small constant, and δ≪1/|D|.\nTo train models with DP guarantees, we fol-\nlow the variant of DP Federated Averaging (DP-\nFedAvg) (McMahan et al., 2017b) used in (Au-\ngenstein et al., 2020), where the only change is\nsampling ﬁxed-sized minibatches in each training\nround.7\nA.3 Differentially Private Federated\nAveraging\nWe now present the technique used to train our DP\nmodel in FL. It closely follows the DP-FedAvg\ntechnique in (McMahan et al., 2017b), in that per-\nuser updates are clipped to have a bounded L2\nnorm, and calibrated Gaussian noise is added to the\nweighted average update to be used for computing\nthe model to be sent in the next round. A slight\ndifference between the DP-FedAvg algorithm in\n(McMahan et al., 2017b) and our approach is the\nway in which client devices are sampled to partic-\nipate in a given federated round of computation.\nDP-FedAvg uses Poisson sampling, where for each\nround, each user is selected independently with a\nﬁxed probability. In this work (also, following (Au-\ngenstein et al., 2020)), we instead use ﬁxed-size\nfederated rounds, where a ﬁxed number of users\nis randomly sampled to participate in each round.\nFor reference, we provide a pseudo-code for the\ntechnique in Algorithm 1.\nPrivacy analysis: Following the analysis of this\ntechnique in (Augenstein et al., 2020), we obtain\nour DP guarantees by using the following:\n1. the analytical moments accountant (Wang\net al., 2019) to obtain the Rényi differential\nprivacy (RDP) guarantee for a federated round\nof computation that is based on the subsam-\npled Gaussian mechanism,\n2. Proposition 1 (Mironov, 2017) for computing\nthe RDP guarantee of the composition involv-\ning all the rounds, and\n7Due to a technical limitation of the simulation framework,\nour experiments use sampling with replacement instead of\nwithout replacement; this should have negligible impact on\nthe metrics of the trained models.\nMain training loop:\nparameters: round participation fraction q ∈(0,1], total\nuser population N ∈N, noise scale z∈R+, clip parame-\nter S ∈R+\nInitialize model θ0, moments accountant M\nSet σ= zS\nqW\nfor each round t= 0,1,2,... do\nCt ←(sample without replacement qN users from pop-\nulation)\nfor each user k∈Ct in parallel do\n∆t+1\nk ←UserUpdate(k,θt)\n∆t+1 = 1\nqN\n∑\nk∈Ct\n∆t+1\nk\nθt+1 ←θt + ∆t+1 + N(0,Iσ2)\nM.accum_priv_spending(z)\nprint M.get_privacy_spent()\nUserUpdate(k,θ0):\nparameters: number of local epochs E ∈N, batch size\nB ∈N, learning rate η∈R+, clip parameter S ∈R+,\nloss function ℓ(θ; b)\nθ←θ0\nfor each local epoch ifrom 1 to Edo\nB← (k’s data split into sizeBbatches)\nfor each batch b∈B do\nθ←θ−η▽ℓ(θ; b)\n∆ = θ−θ0\nreturn update ∆k = ∆ ·min\n(\n1, S\n∥∆∥\n)\n// Clip\nAlgorithm 1: Differentially Private Federated Aver-\naging (DP-FedAvg) with ﬁxed-size federated rounds,\nused to train our DP NWP model.\n3. Proposition 3 (Mironov, 2017) to obtain a DP\nguarantee from the composed RDP guarantee.\nB Additional Empirical Evaluation\nIn this section, we present the results of our addi-\ntional empirical evaluation that was omitted from\nthe main body.\nUtility Metrics:Here, we present the utility metrics\nfor all the models for which the unintended memo-\nrization results were presented in Table 1. It is easy\nto see that the utility of all the evaluated models\nis similar; the accuracy varies from 23.7 −24.6%,\nand the perplexity varies from 57.3 −64.3 across\nall the models.\nUsing Different Optimizers:In Table 4, we pro-\nvide the results for using different optimizers like\nMomentum (Qian, 1999) and Adam (Kingma and\nBa, 2015) for training. We conduct experiments\nusing only the smallest batch size in both the granu-\nlarities (32 records, or 500 users). For Momentum,\nwe set the momentum parameter to 0.9, and for\n9\nOptimizer Data Batch size Acc. Perp.\n500 users 24.4 58.8\nNon- 1K users 24.3 59.5\nIID 2K users 24.5 58.3\nFedAvg 5K users 24.5 58.2\n500 users 24.6 57.5\nIID 1K users 24.6 57.3\n2K users 24.6 57.4\n5K users 24.6 57.3\n32 records 23.7 64.3\nNon- 64 records 24.1 61.8\nIID 128 records 24.1 61.5\n256 records 24.1 61.3\nSGD 32 records 24 62.2\nIID 64 records 24.1 61.5\n128 records 24 62\n256 records 24.1 61.1\nTable 3: Results for the utility metrics for various mod-\nels evaluated at 8000 rounds when sampling users (Fe-\ndAvg), and37.5M steps when sampling records (SGD).\nAcc. denotes test accuracy (in %), and Perp. denotes\ntest perplexity.\nAdam, we set the learning rate to 10−4. First, we\nobserve that using Momentum increases the ob-\nserved unintended memorization but has a similar\nutility as SGD. On the other hand, we see that using\nAdam decreases such memorization, but the utility\nof the models is also noticeably reduced as com-\npared to SGD. We observe a similar trend when\nAdam is combined with FedAvg.\nTraining with DP-FedAvg on IID data:Now, we\npresent in Table 5 the results of using DP-FedAvg\nas the optimizer on IID data consisting of synthetic\nusers. We observe that using DP-FedAvg provides\na signiﬁcant reduction in unintended memorization\ncompared to using FedAvg.\nOnly Clipping:To bound the contribution by each\nparticipating user, DP-FedAvg clips each user up-\ndate before aggregating them from a minibatch of\nusers and adding calibrated noise to guarantee DP.\nFollowing (Carlini et al., 2018), we present results\n(rows containing “FedAvg+Clip\" in Table 6) for the\ncase when user updates are clipped to a value of\n0.2, but no noise is added. This results in an(∞,δ)-\nDP guarantee for any δ∈(0,1), which is vacuous\nas a privacy guarantee. However, this experiment\nData Batch Opt. RS, Acc. (%),\nSize BS Perp.\nSGD 54, 42 24, 62.2\n32 Mom. 64, 50 24.2, 60.6\nIID rec. Adam 56, 42 22.7, 70.8\nFedAvg 66, 56 24.6, 57.5\n500 FedAvg 60, 46 23.7, 63.7\nusers + Adam\nSGD 37, 19 23.7, 64.3\n32 Mom. 48, 36 24.3, 59.9\nNon- rec. Adam 21, 13 21.5, 84.1\nIID FedAvg 21, 0 24.4, 58.8\n500 FedAvg 8, 0 23.3, 66.5\nusers + Adam\nTable 4: Unintended memorization, and utility met-\nrics for models using different optimizers evaluated\nat 37.5M steps when sampling records (e.g., SGD),\nand 8000 rounds when sampling users (e.g., FedAvg).\nMom. denotes the Momentum optimizer.\nOptimizer RS BS Acc. % Perp.\nFedAvg 65 58 24.6 57.3\nDP-FedAvg 48 42 23.9 63\nTable 5: Unintended memorization and utility for mod-\nels trained with (18.8,10−7)-DP using DP-FedAvg on\nIID data and 5K users/round for 100 epochs.\nhelps us observe the extent to which only clipping\nreduces the propensity of unintended memorization\nexhibited by trained models. With IID data, for the\nsetting evaluated in Section 3 (8000 rounds, i.e.,\n100 epochs for minibatch size of 5000 users), we\nobserve that the RS method extracts 58 canaries\nwith clipping, which is 7 fewer canaries when com-\npared to without clipping. The BS method extracts\n49, which is 9 fewer than without clipping. For\nNon-IID data, we observe a similar trend but it is\nmore pronounced: the RS method extracts 11 ca-\nnaries with clipping, which is 15 fewer canaries\nwhen compared to without clipping, whereas the\nBS method is not able to extract any of the inserted\ncanaries.\nEvaluating for Same Training Epochs: In Ta-\nble 7, we provide the results for evaluating models\ntrained for the same number of epochs over the\ntraining data. For the runs using SGD, we start\n10\nData Optimizer RS, Acc. (%),\nBS Perp.\nIID FedAvg 65, 58 24.6, 57.3\nFedAvg+Clip 58, 49 24.2, 60\nNon-IID FedAvg 26, 2 24.5, 58.2\nFedAvg+Clip 11, 0 24, 61.5\nTable 6: Unintended memorization, lowest (by inser-\ntion frequency) canary conﬁguration memorized, and\nutility for models trained with Clipping/DP and 5000\nusers/round for 100 epochs. The models trained with\nDP-FedAvg satisfy (18.8,10−7)-DP.\nwith a batch size of 32 records and a tuned learning\nrate of 0.005, and we increase the learning rate by\n≈\n√\n2 for every 2x increase in the batch size. For\nall the experiments with FedAvg, we ﬁnd that us-\ning a constant learning rate provides the best utility\nacross the different batch sizes, and thus, we keep\nit ﬁxed.\nOpt. Data Batch RS, Acc. (%),\nSize BS Perp.\n32 rec. 49, 43 23.9, 63\nIID 64 rec. 51, 39 23.5, 65.1\n128 rec. 46, 36 23.2, 67.6\nSGD 256 rec. 46, 32 23, 69.7\n32 rec. 40, 29 23.7, 64.2\nNon- 64 rec. 38, 32 23.6, 65.6\nIID 128 rec. 35, 26 23.2, 68.2\n256 rec. 40, 31 23, 69.8\n500 users 66, 56 24.6, 57.5\nIID 1k users 27, 18 24.5, 58\n2k users 28, 18 24.4, 59.3\nFed- 5k users 28, 18 24, 62.5\nAvg 500 users 21, 0 24.4, 58.8\nNon- 1k users 10, 0 24, 61.2\nIID 2k users 0, 0 24, 61.9\n5k users 0, 0 23.3, 67.9\nTable 7: Results for the number of inserted canaries\n(out of 90) memorized via the RS and BS methods,\nand utility metrics for various models evaluated at≈10\nepochs of training.\nFor the models trained with SGD, for both IID\nand non-IID data we observe that unintended mem-\norization remains comparable for models trained\nwith different batch sizes. However, we see a de-\ncrease in the utility as the batch size increases. The\ndecrease in utility is observed for models trained\nusing FedAvg as well, but we also observe a signif-\nicant drop in the such memorization when training\nis performed with at least 1000 users per round.\nMoreover, once the training involves at least 2000\nusers on non-IID data, both the RS and BS meth-\nods are unsuccessful in classifying any of the 90\ninserted canaries as memorized."
}