{
    "title": "UFO: A UniFied TransfOrmer for Vision-Language Representation Learning",
    "url": "https://openalex.org/W3217799312",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A1901876785",
            "name": "Wang Jian-feng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2349235520",
            "name": "Hu Xiaowei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2365636915",
            "name": "Gan Zhe",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2357137484",
            "name": "Yang, Zhengyuan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222155691",
            "name": "Dai, Xiyang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2352283912",
            "name": "Liu, Zicheng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4226099901",
            "name": "Lu, Yumao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1578702133",
            "name": "Wang Li-juan",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3211483028",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3213472962",
        "https://openalex.org/W2277195237",
        "https://openalex.org/W3190043560",
        "https://openalex.org/W2970231061",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W2970608575",
        "https://openalex.org/W2506483933",
        "https://openalex.org/W3154596443",
        "https://openalex.org/W2975761646",
        "https://openalex.org/W3014611590",
        "https://openalex.org/W2963084599",
        "https://openalex.org/W2971274815",
        "https://openalex.org/W3135367836",
        "https://openalex.org/W2912371042",
        "https://openalex.org/W3035524453",
        "https://openalex.org/W2963530300",
        "https://openalex.org/W2955425717",
        "https://openalex.org/W3091588028",
        "https://openalex.org/W2109586012",
        "https://openalex.org/W2995460200",
        "https://openalex.org/W1956340063",
        "https://openalex.org/W3110662498",
        "https://openalex.org/W2886641317",
        "https://openalex.org/W2745461083",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W2613718673",
        "https://openalex.org/W2560207749",
        "https://openalex.org/W3104279398",
        "https://openalex.org/W1905882502",
        "https://openalex.org/W3035497460",
        "https://openalex.org/W3172507542",
        "https://openalex.org/W3203833108",
        "https://openalex.org/W3120237956",
        "https://openalex.org/W2968124245",
        "https://openalex.org/W3119786062",
        "https://openalex.org/W3167118264",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3035688398",
        "https://openalex.org/W3126337491",
        "https://openalex.org/W2998356391",
        "https://openalex.org/W2185175083",
        "https://openalex.org/W2966645965",
        "https://openalex.org/W3090449556",
        "https://openalex.org/W2133459682",
        "https://openalex.org/W2560730294"
    ],
    "abstract": "In this paper, we propose a single UniFied transfOrmer (UFO), which is capable of processing either unimodal inputs (e.g., image or language) or multimodal inputs (e.g., the concatenation of the image and the question), for vision-language (VL) representation learning. Existing approaches typically design an individual network for each modality and/or a specific fusion network for multimodal tasks. To simplify the network architecture, we use a single transformer network and enforce multi-task learning during VL pre-training, which includes the image-text contrastive loss, image-text matching loss, and masked language modeling loss based on the bidirectional and the seq2seq attention mask. The same transformer network is used as the image encoder, the text encoder, or the fusion network in different pre-training tasks. Empirically, we observe less conflict among different tasks and achieve new state of the arts on visual question answering, COCO image captioning (cross-entropy optimization) and nocaps (in SPICE). On other downstream tasks, e.g., image-text retrieval, we also achieve competitive performance.",
    "full_text": "UFO: A UniFied TransfOrmer for Vision-Language Representation Learning\nJianfeng Wang, Xiaowei Hu, Zhe Gan, Zhengyuan Yang,\nXiyang Dai, Zicheng Liu, Yumao Lu, Lijuan Wang\nMicrosoft\n{jianfw,Xiaowei.Hu,zhe.gan,zhengyang,xiyang.dai,zliu,yumaolu,yumaolu}@microsoft.com\nAbstract\nIn this paper, we propose a single UniFied transfOrmer\n(UFO), which is capable of processing either unimodal in-\nputs (e.g., image or language) or multimodal inputs (e.g.,\nthe concatenation of the image and the question), for\nvision-language (VL) representation learning. Existing ap-\nproaches typically design an individual network for each\nmodality and/or a speciÔ¨Åc fusion network for multimodal\ntasks. To simplify the network architecture, we use a single\ntransformer network and enforce multi-task learning during\nVL pre-training, which includes the image-text contrastive\nloss, image-text matching loss, and masked language mod-\neling loss based on the bidirectional and the seq2seq atten-\ntion mask. The same transformer network is used as the\nimage encoder, the text encoder, or the fusion network in\ndifferent pre-training tasks. Empirically, we observe less\nconÔ¨Çict among different tasks and achieve new state of the\narts on visual question answering, COCO image captioning\n(cross-entropy optimization) and nocaps (in SPICE). On\nother downstream tasks, e.g., image-text retrieval, we also\nachieve competitive performance.\n1. Introduction\nRecent years have seen tremendous progress in vision-\nlanguage (VL) representation learning, where the model\nis designed to understand the vision/language signals and\nthe relation between the modalities. Applications in-\nclude image captioning [1, 31], visual question answering\n(VQA) [15], image-text retrieval, etc. Typical approaches\nÔ¨Årst extract the features from each modality and then feed\nthem to a fusion network to jointly learn the representation.\nRegarding how the fusion network is designed, we roughly\ngroup the existing approaches into two categories: light fu-\nsion and heavy fusion, as shown in Fig. 1(a) and (b).\nLight fusion (Fig. 1(a)) adopts a separate encoder for\nboth the image and the text, such as in CLIP [35] and\nALIGN [21]. The image encoder can be ResNet [17] or vi-\nsion transformer [12], while the text encoder is typically a\nimage text\ntextimage\nlight fusion heavy fusion(a) light fusion(b) heavy fusion(c) UniFied transfOrmer (UFO)\nimage text\ntextimage\nlight fusion heavy fusionsharedshared\nFigure 1. Different network designs for VL representation learn-\ning. (a) Light fusion: few parameters are dedicated to multimodal\nfusion. (b) Heavy fusion: a transformer network is applied to fuse\nthe multimodal inputs. Before fusion, the image can be encoded\nas region features, patch features, or CNN grid features, and the\ntext can be encoded through an embedding layer or a transformer\nnetwork. (c) Our UniFied transfOrmer (UFO): a single network\nis reused as image encoder, text encoder, and fusion network for\ndifferent tasks.\ntransformer network [45]. The fusion is the contrastive loss\nbased on the cosine similarity such that the representation\nfrom the two modalities can be aligned into the same se-\nmantic space. A favorable application is the image-text re-\ntrieval, where each image or text description is represented\nas a Ô¨Åxed vector for fast similarity search.\nWhile few parameters are allocated in the light fusion,\nheavy fusion approaches, shown in Fig. 1(b), applies a\ntransformer network on top of the unimodal features to\njointly learn the representation. The image can be encoded\nas object features [5,6,13,18,27,29,30,30,32,39,42,47,53,\n54] through a Faster RCNN [36], as grid features [20, 22]\nfrom a convolutional neural network, or as patch features\n[25] from a linear projection on raw pixels. The text can\nbe encoded as token representations by a transformer net-\nwork as in [28, 42] or by a simple embedding layer as\nin [18, 25, 30, 47, 53]. With a heavier fusion network, the\nÔ¨Ånal representation can better capture the contextual con-\nnection between the modalities. A typical application is the\nVQA, where the network predicts the answer based on both\nthe image and the question.\narXiv:2111.10023v1  [cs.CV]  19 Nov 2021\nExisting approaches design different network architec-\ntures for different tasks. As the transformer network can be\nused for all these components, in this paper we attempt to\ndesign a single UniFied transfOrmer (UFO) for both light-\nfusion and heavy-fusion scenarios, as shown in Fig. 1(c).\nFor the light-fusion tasks, the transformer is used as both the\nimage encoder and the text encoder. For the heavy-fusion\ntasks, the same transformer is reused as a fusion module to\nprocess the two signals together.\nBefore feeding into the transformer, the raw image pixels\nare grouped into patches and projected by a linear mapping.\nThe text description is projected into the same dimension by\nan embedding layer. In this way, we allocate as few as possi-\nble learnable parameters in the modality-speciÔ¨Åc processing\nand devote the majority in the shared transformer network.\nThe network is adjusted automatically on how much rep-\nresentation power should be allocated for each individual\nmodality and how much for the joint representation.\nTo empower the network with the capability for uni-\nmodal inputs, we apply the image-text contrastive (ITC)\nloss on the outputs during VL pre-training (VLP). For mul-\ntimodal fusion capability, we apply the image-text matching\n(ITM) loss and the masked language modeling (MLM) loss\nbased on the bidirectional andseq2seq attention. To opti-\nmize the network with multiple tasks, we randomly choose\none of the tasks in each iteration for efÔ¨Åciency and lever-\nage a momentum teacher motivated by [16, 28] to guide\nthe learning. With extensive ablation study, we observe\nless conÔ¨Çict among these tasks. In certain cases, differ-\nent tasks even help each other. For example, the MLM\ntask improves the retrieval task based on ITC signiÔ¨Åcantly.\nMeanwhile, we also achieve new state of the arts1 in VQA,\nCOCO image captioning (cross-entropy optimization), and\nnocaps(in SPICE), and competitive performance on other\ndownstream tasks, e.g., image-text retrieval.\n2. Related Work\n2.1. Light Fusion\nCLIP [35] aligns both the image and the text descrip-\ntion through a contrastive loss. The image is encoded by\nResNet [17] or vision transformer [12] and is represented as\na single vector after a pooling operator. The text description\nis encoded by a transformer network and is also represented\nby a single vector. During pre-training, the contrastive loss\naligns the representations of the image-text pair to be sim-\nilar, while the representations from mismatched image-text\npairs to be dissimilar. ALIGN [21] further scales up the\ncontrastive loss on the large-scale noisy image-text data and\nmainly adopts EfÔ¨ÅcientNet [43] as the image encoder. Be-\nyond the grid-level representation, LightningDOT [41] ex-\nplores the region-level features through a Faster RCNN [36]\n1As of 11/2021 among peer-reviewed publications.\nnetwork to obtain the image representation. A favorable ap-\nplication is the image-text retrieval. For zero-shot image\nclassiÔ¨Åcation, the image classes can be interpreted as a text\ndescription and the problem can be converted as a text re-\ntrieval task.\nV ATT [2] extends the contrastive learning from the im-\nage domain to the video domain and aligns the video\nframes, audios, and texts. A shared transformer network\nis also studied in [2] as a modal-agnostic network. The dif-\nference with our work is that V ATT [2] belongs to the light-\nfusion, while we focus on unifying the light-fusion and the\nheavy-fusion.\n2.2. Heavy Fusion\nHeavy-fusion networks allocate much more parameters\nin the modality fusion. Before fusion, each modality is en-\ncoded into the same dimensional space. For the image, one\nwidely-adopted approach is to use an off-the-shelf Faster\nRCNN model [5,36] to extract multiple region features,e.g.,\nas in [5,6,13,18,27,29,30,30,32,39,42,47,54]. VinVL [53]\nexplores an even stronger region detector to push the per-\nformance, while MiniVLM [47] designs an efÔ¨Åcient region\ndetector for real-time VL applications. Instead of the region\n(sparse) features, another recent direction is to use the grid\n(dense) features, which can be extracted from ResNet as\nin [20,22,23], or from a vision transformer as in [28,51]. An\nadvantage of the grid feature is that the image encoder can\nbe trained or Ô¨Åne-tuned together with other network com-\nponents without the bounding box annotations. For the text,\na simple approach is to apply one embedding layer after to-\nkenization, such as the models in [6,18,25,30,47,53], or to\nleverage a speciÔ¨Åc transformer network as in [28, 42].\nWith the extracted features, the fusion network is applied\nto learn the contextual representation. The structure can\nbe based on the cross-attention module between different\nmodalities as in [32, 42], or on the self-attention module as\nin [6, 30, 47, 53] where the features of multiple modalities\nare simply concatenated. In [7, 23], a transformer encoder\nand decoder are applied on the visual features and text pre-\nÔ¨Åx. In [7], multiple tasks are uniÔ¨Åed as a text generation\nprocess. This uniÔ¨Åcation is still under the category of the\nheavy fusion. That is, the learned network cannot be used\nto process the unimodal inputs, e.g., image only for light-\nfusion applications. We attempt to unify in the network\nlevel, while [7] in the task level.\n3. UniFied TransfOrmer\nThe key idea is to leverage only one transformer net-\nwork, which is reused as an image encoder, a text encoder,\nor a fusion encoder. We follow the widely-used pretraining-\nthen-Ô¨Ånetuning scheme to train the network. During the\npre-training, we are given a large corpus of image-text pairs\nand multiple losses are enforced to empower the network\nsharedsharedimage encodertext encoderfusion networkITMMLMS-MLMITC\nMulti-head AttentionFeed ForwardùëÅ √ó\nMulti-head AttentionFeed ForwardùëÅ √ó\nMulti-head AttentionFeed ForwardùëÅ √ó\nMomentumTeacherimage text image text\nFigure 2. Vision-language pre-training of our UniFied transfOrmer (UFO). A single transformer is learnt to behave as an image encoder, a\ntext encoder and a fusion network. The pre-training losses include the image-text contrastive (ITC) loss, image-text matching (ITM) loss,\nmasked language modeling loss based on the bidirectional (MLM) and seq2seq attention mask (S-MLM). ITC empowers the network\nto understand the unimodal inputs (image or text), while the rest three focus on the joint inputs. In each iteration, one of the losses is\nrandomly selected and is guided by a momentum teacher if the loss is ITC/MLM/S-MLM.\nfor different roles. For unimodal signals, we apply the\nimage-text contrastive loss [35]. For multimodal fusion\ntask, we apply the image-text matching loss (ITM) and the\nmasked language modeling loss based on both the bidirec-\ntional (MLM) and unidirectional (S-MLM) attention. Fig. 2\nillustrates the pre-training framework.\n3.1. Network Structure\nWe adopt the transformer network as the backbone.\nThe main reason is that the transformer network has been\ndemonstrated to perform well on the image tasks [12], the\nlanguage tasks [45], and VL tasks [6, 18, 25, 30, 47, 53].\nOther choices are convolutional neural network (CNN) or\nall-MLP [44] structures, but it is unclear how to effectively\napply such networks to all the three roles.\nThe input to the transformer network is a sequence of to-\nkens, each of which is represented as a d-dimensional vec-\ntor. To tokenize the image, we split the raw image into\ndisjoint patches, each of which is linearly projected into\nthe d-dimensional space with a learnable linear layer as\nin [12, 25]. A learnable 2-D positional embedding is added\nto each patch representation and an image [CLS] token is\npre-appended. The text description is Ô¨Årst tokenized and\nthen embedded to the d-dimensional space through an em-\nbedding matrix. A starting token of [CLS] and an end-\ning token of EOS are added to wrap the text sequence. A\nlearnable 1-D positional embedding is added to each text\ntoken. Here, the image [CLS] and the text [CLS] are\ntwo different tokens. Before feeding the input to the trans-\nformer network, a modality-speciÔ¨Åc embedding is added to\nthe corresponding input. For VL tasks, the two modality\ninputs are concatenated before sending to the transformer.\nAlthough the inputs may have different token lengths, the\ntransformer network is naturally capable of handling vari-\nant input lengths.\n3.2. Pre-training Tasks\nImage-Text Contrastive Loss. The image-text con-\ntrastive (ITC) loss is to train the network to process either\nthe image or the text and aligns the matched pairs into sim-\nilar representations. For the image, the network is used\nas an image encoder, and the output corresponding to the\n[CLS] token is chosen as the representation. For the text,\nthe network is reused as a text encoder, and the one cor-\nresponding to the [EOS] token is as the representation.\nThe text [CLS] is used for the image-text matching loss.\nLet Ii and Ti be the i-th image and text representation (l2-\nnormalized), respectively. Given N pairs within a training\nbatch, the loss is\nlITC = 1\n2(l1 + l2), (1)\nl1 = ‚àí 1‚àë\ni,j Œ¥i,j\n‚àë\ni,j\nŒ¥i,j log exp{IT\ni Tj/t}‚àë\nk exp{IT\ni Tk/t}, (2)\nl2 = ‚àí 1‚àë\ni,j Œ¥i,j\n‚àë\ni,j\nŒ¥i,j log exp{IT\ni Tj/t}‚àë\nk exp{IT\nk Tj/t}, (3)\nwhere tis initialized as 1 and is learnable as in [21]. The\nindicator of Œ¥i,t is 1 if the i-th image is paired with the j-\nth text, and 0 otherwise. This is to handle the data where\nan image (or a text) is associated with multiple texts (or\nimages), e.g., COCO [31].\n111111111111111111111111111111111111imagetextimagetext\n111111111000000000111111111100110111imagetextimagetext\n(a) bidirectional (b) seq2seq\nFigure 3. Bidirectional and seq2seq attention masks for MLM\nand S-MLM, respectively. If (i, j) is 1, the i-th output can depend\non the j-th input; otherwise, not.\nImage-Text Matching Loss. The network input is the\nconcatenation of the matched or mismatched image-text\npairs. The mismatched pairs are constructed by randomly\nselecting a text description in the dataset for a given image2.\nThe network is required to predict whether it is matched or\nnot, which is a binary classiÔ¨Åcation task. Here, we use the\nrepresentation of the text[CLS] as the joint representation,\nfollowed by a MLP layer for prediction. The cross-entropy\nloss is applied to penalize the incorrect prediction.\nMotivated by [6, 25], we apply the image-patch align-\nment loss. The image and text tokens are Ô¨Årst connected\nthrough solving an optimal transport problem by the inexact\nproximal point method [49]. The resulting distance is max-\nimized for mismatched image-text pairs and minimized for\nthe matched pairs. This loss is weighted by 0.1 as in [6,25].\nOverall, we use ITM to denote the sum of the two losses.\nMasked Language Modeling Loss. The network input\nis the concatenation of the image tokens and the par-\ntially masked text tokens, while the transformer network is\ntrained to predict the masked tokens. As a common prac-\ntice [10], 15% of the text tokens are selected for prediction.\nEach selected token is replaced with a [MASK] token 80%\nof the time; a random token 10% of the time and the un-\nchanged token 10% of the time. The text is masked in the\nword level rather than the token level as in [25]. An MLM\nhead is applied on the outputs of the masked tokens for pre-\ndiction with a cross-entropy loss3, denoted as lMLM.\nWhen applying the pre-trained model to the image cap-\ntioning task, we change the attention mask such that the\ncurrent text token can only depend on the preceding to-\nkens [30, 47, 53]. Therefore, similar as in [11, 54], we also\nincorporate another masked language modeling loss based\non the seq2seq attention, and denote it as lS-MLM. Fig. 3\nshows the attention mask for MLM and S-MLM.\n2In the implementation, we randomly select the mismatched pairs from\nthe current training batch to reduce the disk I/O.\n3Label smoothing is applied in experiments.\nITC ITM MLM S-MLM\nInput matched (mis)matched masked masked\nNetwork Role uni. multi. multi. multi.\nAttention bi. bi. bi. seq.\nTable 1. In different pre-training losses, the input is matched pairs,\nmatched with mismatched pairs, or masked pairs. The network\nis used as either an unimodal (uni.) encoder or a multimodal\n(multi.) encoder. The attention mask is either bidirectional (bi.)\nor seq2seq (seq.).\n3.3. Pre-training Strategies\nOne Loss per Iteration. Table 1 summarizes the character-\nistics of each pre-training loss. In different losses, the input\nformat is different, the transformer is used as different roles,\nand the attention mask may also be different. For simplicity,\nin each iteration, we randomly sample one pre-training loss\nand calculate the gradient for parameter update. Empiri-\ncally, we Ô¨Ånd this strategy is more effective than calculating\nall losses in each iteration when the total number of loss\ncalculation is the same.\nMomentum Teacher. Motivated by [16, 28, 50], we add a\nmomentum teacher to guide the pre-training. SpeciÔ¨Åcally,\nthe momentum teacher is a clone of the transformer net-\nwork, and the parameter is updated as the exponential mov-\ning average of the target network‚Äôs parameter. Let Œ∏be the\ntarget network‚Äôs parameter, and ÀÜŒ∏be the teacher‚Äôs parame-\nter. Then, in each iteration we have\nÀÜŒ∏= mÀÜŒ∏+ (1‚àím)Œ∏, (4)\nwhere mis set as 0.999 in experiments. For the pre-training\nloss of ITC/MLM/S-MLM, the same input and the attention\nmask are fed to the momentum teacher as well, and the out-\nput is used as a soft target of the target network‚Äôs output. In\nITC, let Si,j = IT\ni Ti/t be the similarity between the i-th\nimage and the j-th text, ÀÜSi,j be the similarity from the mo-\nmentum teacher network. Then, a distillation loss is added\non top of ITC as\nÀÜlITC = 1\n2(ÀÜl1 + ÀÜl2), (5)\nÀÜl1 = 1‚àë\ni 1\n‚àë\ni\nKL(Si,¬∑,ÀÜSi,¬∑) (6)\nÀÜl2 = 1‚àë\nj 1\n‚àë\nj\nKL(S¬∑,j,ÀÜS¬∑,j), (7)\nwhere KL (¬∑,¬∑) denotes the Kullback‚ÄìLeibler divergence\nloss on the softmax of the inputs. For MLM loss, let g be\nthe predicted logits corresponding to the masked token, and\nÀÜg be the logits from the momentum teacher. Then, the dis-\ntillation loss is ÀÜlMLM = KL(g,ÀÜg). Similarly, we have the\ndistillation loss for S-MLM.\n4. Experiments\n4.1. Experimental Settings\nNetwork Backbone. We use ViT-B/32, ViT-L/32, and\nViT-L/16 [12] as the backbones. The number (32 or 16)\nis the patch size. ViT-B/32 contains 12 transformer layers\nwith 768 as the hidden size. ViT-L/32 and ViT-L/16 have\n24 transformer layers with 1024 as the hidden size. Corre-\nspondingly, we name our models as UFO-B/32, UFO-L/32\nand UFO-L/16. Only the base model (UFO-B/32) is used\nfor ablation studies.\nVision-Language Pre-training. We combine 4 datasets\nfor vision-language pre-training: MS COCO [31], Concep-\ntual Captions (CC) [38], SBU [33], and Visual Genome\n(VG) [26]. These datasets result in 4 million images with 10\nmillion associated captions. We pre-train the model with80\nepochs when comparing with the state-of-the-art methods.\nFor ablation study, it is 40 epochs unless explicitly speci-\nÔ¨Åed. The image is randomly cropped and resized into s√ós\nwhere sranges from 224 to 384 with the patch size as the\nstep. The batch size is 4096 for UFO-B/32 and UFO-L/32,\nand 2048 for UFO-L/16. The weight decay is 0.01 and is\nimposed on all parameters except the bias in linear layers\nand the layer norm layers. The learning rate Ô¨Årst linearly\nwarms up to lrand then linearly decreases to 0. The learn-\ning rate lr is set to 0.0002 for UFO-B/32 and UFO-L/32,\nand 0.0001 for UFO-L/16. During pre-training, the target\nmodel‚Äôs parameters are converted to float16 except the\nloss-related head layers. The momentum teacher‚Äôs parame-\nters are kept as float32, but the network forward is sped-\nup by the mixed precision calculation. The implementation\nis based on Pytorch 4 and Apex5. Our model is initialized\nwith the ImageNet pre-trained model, and the last check-\npoint is used to Ô¨Åne-tune for all downstream tasks.\nDownstream Evaluation. To evaluate the performance\non the light-fusion preferred tasks, we mainly focus on the\nimage-text retrieval task based on the inner product as de-\ntailed below. For the heavy-fusion favored tasks, we eval-\nuate the performance on VQA, image captioning, NLVR 2,\nand SNLI-VE.\n1) Image-text retrieval.The task is to retrieve similar text\ndescriptions based on the image or vice versa. The key is to\nscore the similarity between an image and a text description.\nAs our pre-training incorporates the ITC loss, the similar-\nity can be calculated by the inner product without Ô¨Ånetun-\ning for zero-shot (ZS) application. In the Ô¨Ånetuning stage,\nwe simply continue to train the network with the ITC loss.\nThe image encoder and the text encoder are not shared for\n4https://github.com/pytorch/pytorch\n5https://github.com/NVIDIA/apex\nhigher accuracy and initialized from the same pre-trained\nmodel. Experiments are conducted on MS COCO [31] and\nFlicker30k [52] datasets with the Karpathy split [24]. The\ntop-Krecall is reported for the corresponding test set.\n2) Visual Question Answering (VQA). The task [15] is to\nanswer a question with natural language based on the image\ncontext, and thus requires a deep understanding of the ques-\ntion and the image. As a common practice, we cast it as a\nclassiÔ¨Åcation problem where each class corresponds to one\nanswer. The network input is the concatenation of the im-\nage and the question embeddings. The representation of the\ntext [CLS] is used to predict the answer over a shared set\nof 3129 answers with an MLP layer. The loss is the binary\ncross-entropy loss, and the inference is to select the answer\nwith the highest conÔ¨Ådence.\n3) Image Captioning. The task is to describe an image with\na natural language sentence. As we have S-MLM loss, we\nreuse this cross-entropy loss to Ô¨Ånetune the network on the\ndownstream dataset. Instead of using the word-level mask-\ning in pre-training, we change it to the token-level mask-\ning. In inference, the [MASK] token is appended recur-\nsively to the generated tokens to predict the next token one\nby one. The beam search size is set as 1, and the accuracy is\nevaluated with BLEU@4 [34], METEOR [9], CIDEr [46],\nand SPICE [3]. No SCST [37] and CBS [4] are applied.\nThe dataset is COCO [31] with Karpathy split [24], and the\nmodel is also evaluated against the val and test sets of the\nnocaps [1] benchmark.\n4) Natural Language Visual Reasoning for Real\n(NLVR2). The task‚Äôs input is a pair of images and a nat-\nural description, and the goal [40] is to predict whether\nthe description is true about the image pair. To Ô¨Åne-tune\nthe network, we construct two input sequences, each con-\ntaining the concatenation of the description and one image.\nEach sequence is fed to the transformer, and the two out-\nputs corresponding to [CLS] are concatenated as the joint\nrepresentation for a binary linear classiÔ¨Åer through an MLP\nlayer.\n5) Visual Entailment. The task is evaluated on SNLI-\nVE [48] and is to predict the relation between a premise\nand a sentence hypothesis as one of entailment, neutral or\ncontradiction. The premise here is an image for the VL task.\nTo Ô¨Ånetune the model, we append an MLP layer on top of\nthe text [CLS] token as a three-way classiÔ¨Åcation task. The\nnetwork input is the concatenation of the image patch fea-\ntures and hypothesis embeddings.\nHyperparameters. The input image size is 384 in the ab-\nlation study as default and is increased properly in the com-\nparison with the state-of-the-art methods. The batch size is\n512 and the model is Ô¨Åne-tuned with 20 epochs. The last\ncheckpoint is used for evaluation. Other hyperparameters\nare summarized in the supplementary materials.\nMethod COCO Flickr30k VQAv2 COCO nocaps NLVR2 SNLI-VE\nTR TR-ZS TR test-dev test-std CIDEr CIDEr SPICE dev test-P dev test\n(a)\nLight.DOT [41]D 60.1 - 83.9 - - - - - - - - -\nCLIP [35] - 88.0 - - - - - - - - - -\nALIGN [21] 77.0 88 .6 95 .3 - - - - - - - - -\n(b)\nViLT [25] 62.9 73 .2 83 .7 71 .26 - - - - 75.70 76.13 - -\nSOHO [19] 66.4 - 86.5 73 .25 73 .47 - - - 76.37 77.32 85.0 84.95\nOSCAR [30]D,S 73.5 - - 73.61 73 .82 127 .8 80.9 11.7 79.12 80.37 - -\nUNITER [6]D 65.7 83 .6 87 .3 73 .82 74 .02 - - - 79.12 79.98 79.39 79.38\nVisParsing [51] - - 87.0 74 .00 74 .17 - - - 77.61 78.05 84.75 85.08\nVILLA [14]D - - 87.9 74 .69 74 .87 - - - 79.76 81.47 80.18 80.02\nALBEF [28](4M)M 73.1 90 .5 94 .3 74 .54 74 .70 - - - 80.24 80.50 - -\nALBEF [28](14M)M 77.6 94.1 95.9 75.84 76 .04 - - - 82.55 83.14 - -\nVinVL [53]D,S 75.4 - - 76.52 76 .60 130 .8 92.46 13.07 82.67 83.98 - -\n(c)\nUFO-B/32(4M) 74.1 71 .6 91 .5 74 .21 74.32 122.8 78.79 12.47 76.35 76.79 77.90 77.41\nUFO-L/32(4M) 76.9 78 .8 93 .6 75.73 75.74 128 .5 88.54 13.31 78.27 78.37 78.13 77.65\nUFO-L/16(4M) 75.7 74 .0 94 .1 76.64 76.76 131.2 92.26 13.61 78.76 79.55 78.46 78.56\nTable 2. Compare our model UFO with the state-of-the-art methods. The retrieval task is reported with the top-1 recall on the text retrieval\n(TR). TR-ZS: zero-shot TR. Superscript of M: in retrieval tasks, the candidates are Ô¨Åltered by the inner product based on unimodal\nencoders and then reÔ¨Åned by the heavy fusion. Superscript of D: the approach depends on an object detector. Superscript of S: SCST [37]\nis applied for nocaps. Rows of (a)/(b)/(c): corresponding to Fig. 1 (a)/(b)/(c), respectively. Methods in (b) are slower than those in (a) and\n(c) as they require to compute the similarity through a heavy-fusion network with each or Ô¨Åltered candidate. The number in the parenthesis\nis the number of images in VLP. nocapsis on the test set.\nLoss VQA Zero-Shot TR@1\ntest-dev Flickr30k COCO\nFull 70.23 64.5 55.5\nRandom 71.39 68.7 58.7\nTable 3. Comparison between the full loss and randomly-selected\nloss in each iteration. Momentum teacher is disabled.\n4.2. Comparison with the State-of-the-art\nTable 2 shows the comparison with the existing state-of-\nthe-art methods, which are divided into two groups: light-\nfusion in rows of (a) and heavy-fusion in rows of (b). Based\non the results, we make the following discussions.\nApplicability on downstream tasks.The light-fusion ap-\nproaches (in rows of (a)) achieve strong performance on the\nCOCO and Flickr30k retrieval tasks, but are less applica-\nble to other VL tasks such as VQA. In contrast, the heavy-\nfusion approaches (in rows of (b)) are more suitable for the\nunderstanding tasks, as multiple transformer layers are ded-\nicated to learn the relationship between the modalities. Ta-\nble 2 contains the results on retrieval tasks for heavy fusion,\nbut the approaches use the fusion network to score the sim-\nilarity between the query and each of the candidates. This\nprocess is time-consuming [41], especially when the dataset\nis large. Comparably, our model (in rows of (c)) can achieve\ncompetitive retrieval performance with fast speed similar to\nthe light-fusion approaches, and also strong results for other\nVL understanding tasks.\nBackbones. Our UFO-B/32 shares the same backbone with\nViLT [25], but achieves signiÔ¨Åcantly stronger performance\nexcept on NLVR2, where our approach is slightly better. For\nexample of the COCO retrieval task, our model improves\nViLT [25] from 62.9 to 72.1 for the text retrieval at the\ntop-1. On VQA, the improvement is from 71.26 to 74.21\non test-dev. Most methods rely on an object detector\nto extract object features, while our approach removes this\ndependency and is also in line with other end-to-end ap-\nproaches. Comparing UFO-B/32 with UFO-L/32, we can\nsee a stronger backbone leads to a moderate or signiÔ¨Åcant\nimprovement on all tasks.\nRetrieval tasks.Our best model (UFO-L/32 or UFO-L/16)\nachieves better or comparative results than all approaches\nexcept ALBEF with 14 million images, CLIP with 400 mil-\nlion image-text pairs, and ALIGN with 1.8 billion image-\ntext pairs, all of which use a substantially larger pre-training\ndataset than ours. Compared to ALBEF [28] with the same\namount of images, our model achieves better retrieval per-\nformance (76.9 vs. 73.1) on COCO and comparable Ô¨Åne-\ntuned performance on Flickr30k (94.1 vs. 94.3). It is worth\nnoting that ALBEF reÔ¨Ånes the retrieval results with a fusion\nnetwork, while we simply use the inner product for fast re-\ntrieval speed.\nUnderstanding tasks. On the challenging VQA task, our\nmodel achieves new state-of-the-art accuracy with 76.76\non test-std. This is better than VinVL [53] (76.60),\nPre-training Task Zero-Shot Performance Finetune Performance\nITC ITM MLM S-MLM F. TR@1 C. Caption C. TR@1 F. TR@1 VQA C. Caption nocaps\n(a) ‚úì 54.5 0.0 65.5 83.6 68.96 108.7 55.98\n(b) ‚úì 0.1 0.0 - - 68.12 100.0 48.39\n(c) ‚úì 5.0 20.3 62.5 78.4 69.84 112.1 71.46\n(d) ‚úì 2.3 84.8 61.8 78.8 69.36 115.0 75.94\n(e) ‚úì ‚úì 72.1 23.9 70.3 88.0 71.26 114.9 72.95\n(f) ‚úì ‚úì 70.5 72.2 70.3 89.4 70.30 116.5 75.32\n(g) ‚úì ‚úì 0.1 15.0 63.2 70.8 71.33 113.1 71.42\n(h) ‚úì ‚úì ‚úì 71.0 71.7 70.0 87.0 71.31 116.8 76.46\n(i) ‚úì ‚úì ‚úì 68.9 72.1 70.6 87.7 71.24 117.5 74.96\n(j) ‚úì ‚úì ‚úì 68.3 18.8 71.1 88.3 71.84 116.5 73.15\n(k) ‚úì ‚úì ‚úì 0.2 84.5 63.6 78.2 70.86 117.8 76.35\n(l) ‚úì ‚úì ‚úì ‚úì 70.3 70.7 70.6 88.0 71.87 119.0 77.09\nTable 4. Impact of different pre-training tasks on downstream tasks with 20 epochs for each pre-training loss. C. TR@1: Text Retrieval\nat top-1 on COCO; F. TR@1: Text Retrieval at top-1 on Flickr30k. C. Caption: captioning performance in CIDEr on COCO. VQA is on\ntest-dev. nocapsis on val. Momentum teacher network is disabled. The highest number for each task is bolded.\nwhich relies on a strong object detector, and better than\nALBEF [28](14M) (76.04), which uses even more image-\ntext pairs. On the COCO captioning task, we achieve 131.2\nCIDEr score, slightly higher than the previous state of the\nart (130.8) with cross-entropy optimization. On nocaps,\nour model achieves the best performance in SPICE (13.61\nvs. 13.07) and is competitive in CIDEr (92.26 vs. 92.46).\n4.3. Ablation Study\nDifferent Pre-training Strategies. We randomly choose\none task in each iteration. An alternative is to run all tasks\nin each iteration where the gradient is more stable. To make\na fair comparison, we adjust the training epochs such that\nthe total number of loss calculations is the same, in which\nthe training cost is roughly the same. The comparison is\nshown in Table 3, and we can see that the accuracy with\nthe randomly-selected loss shows better accuracy than full\nlosses in each iteration. Therefore, the model may favor\nmore iteration updates than more stable gradients.\nDifferent Pre-training Tasks. We have multiple pre-\ntraining losses, and one question is that how each loss im-\npacts the performance of the downstream tasks. We conduct\nexperiments in two settings. The Ô¨Årst is to run each loss 20\nepochs to study whether more losses help the downstream\ntasks, and results are shown in Table 4. On VQA, COCO\nCaption and nocaps, the one with all pre-training tasks\nleads to the highest accuracy after Ô¨Åne-tuning. For the other\ntasks, we can also observe competitive accuracy with all\nlosses. Other observations are detailed below.\n1. (a) vs. (b) vs. (c) vs. (d): With a single pre-training\nloss, S-MLM and ITC give the best performance on cap-\ntioning and retrieval tasks, respectively. This is reason-\nable as the pre-training task is consistent with the corre-\nsponding downstream task. For VQA task, MLM gives\nthe best accuracy.\n2. (a) vs. (e) or (f): MLM or S-MLM improves the ITC\ntask and shows signiÔ¨Åcant improvement on the retrieval\ntask, e.g., from 83.6% to 88.0% for the text retrieval\nat the top- 1 with Ô¨Ånetuning on Flickr30k. With MLM\nor S-MLM, the captioning task and VQA can also be\nimproved by a large margin on top of the ITC loss.\n3. (c) vs. (e); (d) vs. (f): On top of MLM or S-MLM,\nITC gives clear improvement for VQA, e.g., 69.36 (b)\nto 70.30 (e).\n4. (l) vs. (h, i, j, k): For VQA, we can see signiÔ¨Åcant accu-\nracy drop by removing ITM, or MLM, or ITC, compared\nwith all pre-training losses. In spite of less drop (71.87\n‚Üí71.84) on VQA by removing S-MLM, the caption-\ning task drops a lot (77.09 to 73.15 on nocaps). This\nalso shows all these pre-training losses help on the VL\nunderstanding tasks. For retrieval tasks, the accuracy is\nsimilar as long as ITC and at least MLM or S-MLM ex-\nists.\nThe second experiment setting is that the total number of\npre-training epochs is 40 such that the pre-training cost is\nroughly the same as only one loss is randomly selected in\neach iteration. Overall, we observe strong performance with\nall pre-training losses. The experiment details are in the\nsupplementary materials.\nMomentum Distillation. We apply the momentum dis-\ntillation [28] to regularize the model training in the pre-\ntraining stage. Table 5 shows the ablation study by turn-\nw/ or w/o Momentum Teacher Downstream Performance\nPre-training Finetuning C. TR@1 F.TR@1 VQA C. Caption nocaps NLVR2 SNLI-VE\n(a) 70.6 88.0 71.87 119.0 77.09 75.4 77.4\n(b) ‚úì 71.3 88.4 71.94 118.6 76.76 75.6 76.9\n(c) ‚úì 72.1 89.9 72.42 119.4 77.53 76.2 77.5\n(d) ‚úì ‚úì 72.0 89.2 72.51 119.9 78.03 76.2 77.8\nTable 5. Effectiveness of the momentum teacher in pre-training and Ô¨Ånetuning stages. VQA is on test-dev. nocaps, NLVR2 and\nSNLI-VE are on the validation split. Pre-training is with 80 epochs.\nInput size VQA COCO Caption NOCAPS Flickr-TR@1 COCO-TR@1 NLVR2 SNLI-VE\n384 72.42 119.4 77.53 88.8 72.1 76.2 77.5\n480 73.29 121.4 79.21 88.3 72.7 76.4 77.6\n576 72.87 121.8 79.72 89.4 72.3 76.2 77.9\n672 74.00 122.0 80.00 90.8 72.8 75.7 77.8\n768 74.21 122.8 80.74 90.8 73.6 - 77.7\n960 73.49 122.8 80.01 91.3 74.1 - 77.9\n1024 73.07 - - 91.5 73.9 - -\n1280 73.49 - - 90.5 74.0 - -\nTable 6. Impact of different image sizes during Ô¨Ånetuning. VQA is on test-dev; nocaps, NLVR2 and SNLI-VE are on val.\ning it on or off in both pre-training and Ô¨Ånetuning stages.\nComparing (a) with (c), we can see the performance im-\nproves signiÔ¨Åcantly on VQA and retrieval tasks by enabling\nthe momentum distillation in the pre-training stage, and\nshows slight improvement on other tasks. However, dur-\ning the Ô¨Åne-tuning stage 6, it shows almost little improve-\nment in our setting. The reason may be that the datasets in\nthe downstream tasks are well annotated, while the massive\npre-training dataset is noisy. As the momentum teacher can\nreduce the impact of the data noise [28], it helps more the\npretraining than the Ô¨Åne-tuning.\nMulti-scale vs. single-scale. In VLP, we use the multi-\nscale image inputs (224 ‚àº384). Another way is to use the\nsingle-scale input ( 384 always). The former could reduce\nthe training time as the size is smaller or equal to 384, and\nthe model can be more robust for scale changes. Table 7\nshows the comparison. On VQA, the accuracy with the\nmulti-scale improves, while in the zero-shot retrieval task\non Flickr30k, the performance drops. Considering the re-\nduced training cost, we always choose the multi-scale.\nIncreasing the image size Table 6 shows the study with\ndifferent input image sizes for each downstream task. On\nVQA, image captioning and retrieval tasks, the accuracy\nimproves a lot with a larger input size. For example on\nVQA, the accuracy is improved from 72.42 to 74.21 by in-\ncreasing the input size from 384 to 768. On NLVR 2 and\n6We search the best weight among {1.0, 0.1, 0.01, 0.001} on the dis-\ntillation loss in the Ô¨Åne-tuning. A Ô¨Åner hyperparameter search may poten-\ntially improve the results\nScale Hours VQA ZS Flickr TR@1\nsingle 17 71.19 70.4\nmulti 14 71.39 68.7\nTable 7. Comparison between the multi-scale and single-scale im-\nage inputs during VLP on 32 V100.\nSNLI-VE, the improvement is minor. Meanwhile, the op-\ntimal input size is also different on different tasks, which\nindicates that different tasks may expect different granular-\nity levels of image understanding.\n5. Conclusion\nWe propose a single uniÔ¨Åed transformer (UFO), which is\ncapable of processing both the unimodal input of images or\ntext, and the multimodal input. During the vision-language\npre-training, the network is learned to understand differ-\nent signals through multiple losses, including the image-\ntext contrastive loss, the image-text matching loss, and the\nmasked language modeling loss based on the bidirectional\nand seq2seq masks. Extensive experiments show that our\nuniÔ¨Åed model can achieve competitive results compared to\nexisting methods, which typically design speciÔ¨Åc networks\nfor each modality and modality fusion. As our model is up\nto large-sized (24 layers) on only 4 million images in VLP,\nwe expect it is beneÔ¨Åcial to scale up both the model size and\nthe pre-training data, which is left as future work.\nDatasets COCO [31] CC [38] SBU [33] VG [26] Total\n#Images 113K 3.1M 875K 108K 4.2M\n#Captions 567K 3.1M 875K 5.4M 9.9M\nTable 8. Dataset statistics in VL pretraining.\nModel UFO-B/32 UFO-L/32 UFO-L/16\nBatch size 4096 4096 2048\nNumber of V100 32 64 64\nMax GPU Mem (GB) 17 16 27\nTime cost (hour) 32 60 177\nTable 9. Vision language pretraining statistics for 80 epochs in\nour experiment. With different implementations and hardware set-\ntings, the cost can be greatly different.\nThe supplementary materials follow the same structure\nwith the main paper, but provides more details and studies.\nA. Experiment\nA.1. Settings\nA.1.1 Vision-Language Pre-training\nDataset. Table 8 shows the statistics of each dataset.\nData Preprocessing.For faster data loading, we downsize\neach image such that the shorter side is not larger than 384\nand the longer side is not larger than 640 while keeping\nthe aspect ratio. Meanwhile, all images are re-compressed\nas the JPEG format with the quality being 90. In Visual\nGenome, as the caption is region-level, for each caption we\ncrop a sub region which is 4 times of the associated box an-\nnotation, and take this extended region as the input image.\nData Augmentation. We apply the multi-scale and ran-\ndom cropping as the data augmentation. SpeciÔ¨Åcally, the\ncropped region area takes at least80% of the whole image7.\nThe region is resized into s√ós and s is a random value\nranging from 224 to 384 with the patch size as the step.\nPretraining Statistics. Table 9 shows the vision langauge\npretraining statistics with 80 epochs.\nA.1.2 Downstream Evaluation\nDuring Ô¨Åne-tuning, no preprocessing, e.g. JPEG re-\ncomopression, is applied. The image is resized such that\nthe shorter side is not larger thansand the longer side is not\nlonger than 1333s/800 with the aspect ratio kept. No ran-\ndom crop and multi-scale augmentation are applied here.\nFollowing [25], we apply the RandAugment [8] except the\ncolor inversion to the image. Table 10 shows the input sizes\nand the learning rate when comparing with the state-of-the-\nart approaches (shown in Table 2 of the main paper). The\n7Implemented as RandomResizedCrop(scale=(0.8, 1.0),ratio=(1.0,\n1.0)) in Pytorch\ncorresponding learning rate is the default setting for the ab-\nlation study, where the input size sis 384 as default.\nA.2. Comparison with the State-of-the-art Ap-\nproaches\nIn the Table 3 of the main paper, we present the compar-\nison with the state-of-the-art approaches. For the retrieval\ntask, the result is based on the text retrieval at the top-1 and\nfor the caption task, it is the CIDEr score. Here, we show\nthe results with other widely-used metrics. Table 11 illus-\ntrates the complete result on the retrieval task. The obser-\nvation is consistent with the discussion in the main paper.\nUnder the same pretraining data scale with ALBEF [28],\nour approach achieves better accuracy on the COCO dataset\nand competitive results on Flickr30k. Meanwhile, our best\nmodel is also competitive compared with the best model\nwith much larger pretraining data scale. Table 12 and 13\nshow the complete results on the COCO captioning task and\nnocapsdataset, respectively.\nA.3. Ablation Study\nDifferent Pretraining Tasks.In the main paper, we studied\nthe results with different pretraining tasks when each task\nruns with the same number of iterations. Table 14 shows\nthe results when the total number of iterations is the same,\nand thus the pretraining cost is similar. We can see that\nthe pretraining with all losses can achieve descent perfor-\nmance on all downstream tasks compared with the best per-\nformance. It is noted that the row (j) gives the best VQA\nperformance by removing S-MLM loss, but sacriÔ¨Åce a lot\non the captioning tasks. Thus, we stick to apply all losses\nduring pretraining.\nWeight of Distillation Losses.In the pretraining, we add\nthe distillation loss. Table 15 shows the experiments with\ndifferent weights on the loss items with 40 epochs. With\na positive weight, the zero-shot retrieval task is improved\nabout 1 to 3 points, but after the Ô¨Åne-tuning, the improve-\nment vanishes. On VQA, it gives non-trivial 0.1 ‚àº 0.4\npoints‚Äô improvement. On COCO captioning task, we can\nobserve about 1 points improvement when the weight is0.6\nor 1.0. Overall, we consider the optimal weight is around\n0.6 ‚àº1.0. Table 16 shows the results with 80 pretraining\nepochs. As we can see, 1.0 gives the best accuracy over all\ntasks, and we also apply 1.0 for other network structures.\nA.3.1 Temperature in Image-Text Contrastive loss\nIn the image-text contrastive loss, we set the temperature\nlearnable as in [21]. Fig. 4 shows the comparison among\ndifferent ways to set the temperature, including the man-\nually tuned values and the learnable results. With an im-\nproper pre-deÔ¨Åned temperature, the accuracy on the re-\ntrieval drops a lot. The VQA performance is relatively more\nModel Param Retrieval VQA Captioning NLVR2 SNLI-VE\nCOCO Flickr COCO NOCAPS\nUFO-B/32 Input size 960 1024 768 768 768 480 576\nLearning rate 2.5e-5 2.5e-5 5e-5 5e-5 5e-5 5e-5 2.5e-5\nUFO-L/32 Input size 768 768 768 768 768 384 480\nLearning rate 1e-5 1e-5 5e-5 5e-5 5e-5 5e-5 1e-5\nUFO-L/16 Input size 480 384 576 576 672 384 672\nLearning rate 1e-5 1.25e-5 3e-5 1.5e-5 1e-5 5e-5 1e-5\nTable 10. Input size and the peak learning rate in each downstream task when comparing with the state-of-the-art approaches in Table 2 of\nthe main paper. The learning rate is also used as the default setting for all ablation studies. NOCAPS shares the same training data with the\nCOCO captioning task.\nMethod Flickr COCO\nTR@1 TR@5 TR@10 IR@1 IR@5 IR@10 TR@1 TR@5 TR@10 IR@1 IR@5 IR@10\n(a) Light.DOT [41]D 83.9 97 .2 98 .6 69.9 91.1 95.2 60.1 85.1 91.8 45.8 74.6 83.8\nAlign [21](1.8B) 95.3 99 .8 100 .0 84.9 97.4 98.6 77.0 93.5 96.9 59.9 83.3 89.8\n(b)\nVILT [25] 83.5 96.7 98.6 64.4 88.7 93.8 61.5 86.3 92.7 42.7 72.9 83.1\nLight.DOT [41]D,M 87.2 98.3 99.0 75.6 94.0 96.5 74.2 92.4 96.0 57.4 82.7 89.9\nUNITER [6]D 87.3 98.0 99.2 75.6 94.1 96.8 65.7 88.6 93.8 52.9 79.9 88.0\nVILLA [14]D 87.9 97.5 98.8 76.3 94.2 96.8 - - - - - -\nOSCAR [30]D - - - - - - 73.5 92.2 96.0 57.5 82.8 89.8\nALBEF [28](4M)M 94.3 99.4 99.8 82.8 96.7 98.4 73.1 91.4 96.0 56.8 81.5 89.2\nVinVL [53]D - - - - - - 75.4 92.9 96.3 58.8 83.5 90.3\nALBEF [28](14M)M 95.9 99.8 100.0 85.6 97.5 98.9 77.6 94.3 97.2 60.7 84.3 90.5\n(c)\nUFO-B/32(4M) 91.5 99.2 99.7 79.0 95.2 97.6 74.1 93.2 96.8 56.4 82.4 89.5\nUFO-L/32(4M) 93.6 99.0 99.8 81.1 95.9 98.1 76.9 94.1 97.1 59.1 83.7 90.4\nUFO-L/16(4M) 94.1 99.5 99.9 80.7 96.7 98.3 75.7 93.7 97.1 59.2 83.6 90.5\nTable 11. Compare our model UFO with the state-of-the-art approaches in the retrieval task after Ô¨Åne-tuning. Superscript of M: the\nretrieval candidate is Ô¨Årst Ô¨Åltered by the innner product and then reÔ¨Åned by the heavy fusion. Superscript of D: the approach is based on\nan object detector. The number in parenthesis: the number of images in pretraining. TR: text retrieval. IR: image retrieval. The approaches\nin the rows (a) and (c) are based on the inner product and thus the retrieval speed can be fast.\nMethod BLEU@4 METEOR CIDEr SPICE\nMiniVLM [47] 35.6 28 .6 119 .8 21 .6\nOSCAR [30] 37.4 30.7 127.8 23.5\nVinVL [53] 38.5 30.4 130.8 23.4\nUFO-B/32(4M) 36.0 28.9 122.8 22.2\nUFO-L/32(4M) 37.6 29.7 128.5 23.0\nUFO-L/16(4M) 38.7 30.0 131.2 23.3\nTable 12. Compare our model UFO with the state-of-the-art ap-\nproaches in the captioning task on COCO based on the cross-\nentropy loss.\nstable. When the temperature is learnable, it can achieve a\nstrong retrieval performance of68.7% and a descent perfor-\nmance on the VQA, and thus we always set the temperature\nas a learnable parameter.\nA.3.2 Momentum-based Image-Text Contrastive Loss\nFor the image-text contrastive loss, our implementationlITC\nis based on the in-batch samples, where the negative sam-\nples are from the current batch, as in [21, 35]. In [28], the\nimage-text contrastive loss is implemented with a momen-\ntum encoder [16], which is a drop-in replacement of lITC.\nThe beneÔ¨Åt is that the number of negative samples is inde-\npendent with the batch size and can be very large,e.g. 65536\nin [16, 28]. However, the negative samplers are calculated\nwith the momentum encoder which is less accurate. We use\nlm-ITC to denote this momentum-based loss.\nThe temperature is also learnable inlm-ITC for a fair com-\nparision and the result is shown in Table 17. As we can see,\nlITC achieves better performance on both VQA and the re-\ntrieval task in our setting.\nMethod\nValidataion set Test set\nin-domain ne.-domain ou.-domain overall in-domain ne.-domain ou.-domain overall\nC S C S C S C S C S C S C S C S\nOSCAR [30]S,C 85.4 11.9 84.0 11.7 80.3 10.0 83.4 11.4 84.8 12.1 82.1 11.5 73.8 9.7 80.9 11.3\nHuman [1] 84.4 14.3 85.0 14.3 95.7 14.0 87.1 14.2 80.6 15.0 84.6 14.7 91.6 14.2 85.3 14.6\nVIVO [18]S,C 92.2 12.9 87.8 12.6 87.5 11.5 88.3 12.4 89.0 12.9 87.8 12.6 80.1 11.1 86.6 12.4\nVinVL [53]S 103.7 13.7 95.6 13.4 83.8 11.9 94.3 13.1 98.0 13.6 95.2 13.4 78.0 11.5 92.5 13.1\nUFO-B/32 94.5 13.4 82.7 12.8 64.9 11.0 80.7 12.5 90.6 13.6 81.3 12.7 60.6 10.7 78.8 12.5\nUFO-L/32 99.8 14.0 91.1 13.4 75.6 11.7 89.2 13.1 96.1 14.1 90.9 13.5 74.2 11.8 88.5 13.3\nUFO-L/16 103.9 14.5 95.5 13.8 83.5 12.3 94.3 13.6 98.9 14.3 94.7 13.9 77.9 12.1 92.3 13.6\nTable 13. Compare our model UFO with the state-of-the-art approaches in nocapswith Ô¨Ånetuning. C: CIDEr. S: SPICE. ne.-domain:\nnear-domain. ou.-domain: out-of-domain. The highest score is bolded (The human performance is excluded). Superscript of S: SCST [37]\nis applied. Superscipt of C: CBS [4] is applied.\nPretraining task Zero-Shot Performance Finetune Performance\nITC ITM MLM S-MLM F. TR@1 C. Caption C. TR@1 F. TR@1 VQA C. Caption NOCAPS\n(d) ‚úì 65.5 0.0 68.3 85.5 69.58 109.1 56.91\n(a) ‚úì 0.2 0.0 - - 68.37 99.0 45.86\n(b) ‚úì 5.0 21.9 62.8 78.3 70.42 113.2 71.46\n(c) ‚úì 0.8 78.6 61.6 77.9 69.38 117.0 77.06\n(e) ‚úì ‚úì 72.1 23.9 70.3 88.0 71.26 114.9 72.95\n(f) ‚úì ‚úì 70.5 72.2 70.3 89.4 70.30 116.5 75.32\n(g) ‚úì ‚úì 0.1 15.0 63.2 70.8 71.33 113.1 71.42\n(h) ‚úì ‚úì ‚úì 69.0 72.2 69.8 86.0 70.93 116.1 74.65\n(i) ‚úì ‚úì ‚úì 66.3 72.7 70.2 87.1 71.11 117.1 73.64\n(j) ‚úì ‚úì ‚úì 68.7 25.4 69.8 87.0 71.65 114.7 71.26\n(k) ‚úì ‚úì ‚úì 0.1 76.3 63.6 78.2 70.77 117.0 74.31\n(l) ‚úì ‚úì ‚úì ‚úì 68.7 72.7 69.1 87.6 71.39 116.7 74.77\nTable 14. Impact of different pretraining tasks on downstream tasks with 40 epochs in total for all pretraining losses. C. TR@1: Text\nRetrieval at top-1 on COCO; F. TR@1: Text Retrieval at top- 1 on Flickr. C. Caption: captioning performance in CIDEr on the COCO\ndataset. VQA is on test-dev. NOCAPS is on validation in terms of CIDEr. Momentum teacher network is disabled. The highest\nnumber for each task is bolded.\nA.3.3 Representative Text Token in Image-Text Con-\ntrastive Loss\nThe image input contains only one special [CLS] token,\nwhile the text contains two special tokens:[CLS] and EOS.\nAs the text [CLS] is used in the ITM loss, we use the\n[EOS] token in the ITC loss. Table 18 shows the com-\nparison of which special text token is used in the ITC loss.\nCompared with CLS, the EOS token achieves slightly bet-\nter accuracy in VQA, but worse performance in the retrieval\ntask. Thus, we conclude that both performs similarly and in\nall other experiments, we always use [EOS] to represent\nthe text input for ITC.\nA.3.4 Modal-Type Embedding\nTable 19 shows the result by removing the modal-speciÔ¨Åc\nembedding. From the result, the removal of the modal-\nspeciÔ¨Åc embedding shows no improvement and thus we al-\nways add it in all other settings.\nReferences\n[1] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen,\nRishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Ste-\nfan Lee, and Peter Anderson. nocaps: novel object caption-\ning at scale. In ICCV, 2019. 1, 5, 11\n[2] Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong\nChuang, Shih-Fu Chang, Yin Cui, and Boqing Gong. V ATT:\ntransformers for multimodal self-supervised learning from\nraw video, audio and text. In ICCV, 2021. 2\n[3] Peter Anderson, Basura Fernando, Mark Johnson, and\nStephen Gould. SPICE: semantic propositional image cap-\ntion evaluation. In ECCV, 2016. 5\n[4] Peter Anderson, Basura Fernando, Mark Johnson, and\nStephen Gould. Guided open vocabulary image captioning\nwith constrained beam search. In EMNLP, 2017. 5, 11\nweight Retrieval Task VQA Captioning\nZS F. TR@1 F. TR@1 C. TR@1 test-dev COCO nocaps\n0 68.7 87.2 69.1 71.39 116.7 74.77\n0.4 71.5 86.5 67.1 71.43 116.4 74.44\n0.6 72.2 87.6 68.9 71.58 118.1 74.65\n0.8 70.2 87.4 67.7 71.77 116.7 75.24\n1.0 69.5 87.3 68.5 71.75 117.5 74.11\nTable 15. Different weights on the distillation loss in pretraining with 40 epochs. Retrieval task and COCO captioning task are on test split\nwhile nocapsis on validation split.\nweight Retrieval Task VQA Captioning\nZS F. TR@1 F. TR@1 C. TR@1 test-dev COCO nocaps\n0.6 69.4 89.0 71.3 72.26 118.9 77.07\n0.8 71.6 88.8 71.6 72.24 119.4 76.55\n1.0 71.6 89.9 72.1 72.42 119.4 77.53\nTable 16. Different weights on the distillation loss in pretraining with 80 epochs. Retrieval task and COCO captioning task are on test split\nwhile nocaps is on validation split.\n0.03 0.04 0.05 0.06 0.07\n55.0\n57.5\n60.0\n62.5\n65.0\n67.5\n70.0\n72.5\nZS Flickr TR@1\n70.00\n70.25\n70.50\n70.75\n71.00\n71.25\n71.50\n71.75\nVQALearnable\nFigure 4. Different ways to set the temperatures in the image-text\ncontrastive loss, including the manually tuned values and the learn-\nable. For the learnable, the Ô¨Ånal learned value is shown in the Ô¨Åg-\nure as a red star. The left y-axis represents the text retrieval at the\ntop-1 on Flickr in a zero-shot setting after pretraining. The righty-\naxis represents the Ô¨Åne-tuned VQA performance on test-dev.\nLoss VQA ZS Flickr TR@1\nlm-ITC 70.49 61.6\nlITC 71.39 68.7\nTable 17. Comparison between the in-batch image-text contrastive\nloss lITC and the momemtum-based image-text contrastive loss\nlm-ITC.\nText token VQA ZS Flickr TR@1\n[CLS] 71.26 70.9\n[EOS] 71.39 68.7\nTable 18. Comparision between which text token should be used\nin the image-text contrastive loss.\nEmbedding VQA ZS Flickr TR@1\nNo 71.33 69.7\nYes 71.58 72.2\nTable 19. Comparison of the models with or without the modal-\ntype embedding.\n[5] Peter Anderson, Xiaodong He, Chris Buehler, Damien\nTeney, Mark Johnson, Stephen Gould, and Lei Zhang.\nBottom-up and top-down attention for image captioning and\nvisual question answering. In CVPR, 2018. 1, 2\n[6] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,\nFaisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu.\nUNITER: universal image-text representation learning. In\nECCV, 2020. 1, 2, 3, 4, 6, 10\n[7] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unify-\ning vision-and-language tasks via text generation. In ICML,\n2021. 2\n[8] Ekin D. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V .\nLe. Randaugment: Practical data augmentation with no sep-\narate search. CoRR, abs/1909.13719, 2019. 9\n[9] Michael J. Denkowski and Alon Lavie. Meteor universal:\nLanguage speciÔ¨Åc translation evaluation for any target lan-\nguage. In Proceedings of the Ninth Workshop on Statistical\nMachine Translation, 2014. 5\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: pre-training of deep bidirectional trans-\nformers for language understanding. In NAACL-HLT, 2019.\n4\n[11] Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu,\nYu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon.\nUniÔ¨Åed language model pre-training for natural language un-\nderstanding and generation. In NeurIPS, 2019. 4\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In ICLR, 2021. 1, 2, 3, 5\n[13] Zhiyuan Fang, Jianfeng Wang, Xiaowei Hu, Lijuan Wang,\nYezhou Yang, and Zicheng Liu. Compressing visual-\nlinguistic model via knowledge distillation. In ICCV, 2021.\n1, 2\n[14] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng,\nand Jingjing Liu. Large-scale adversarial training for vision-\nand-language representation learning. In NeurIPS, 2020. 6,\n10\n[15] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-\ntra, and Devi Parikh. Making the V in VQA matter: El-\nevating the role of image understanding in visual question\nanswering. In CVPR, 2017. 1, 5\n[16] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and\nRoss B. Girshick. Momentum contrast for unsupervised vi-\nsual representation learning. In CVPR, 2020. 2, 4, 10\n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\n2016. 1, 2\n[18] Xiaowei Hu, Xi Yin, Kevin Lin, Lijuan Wang, Lei Zhang,\nJianfeng Gao, and Zicheng Liu. VIVO: surpassing human\nperformance in novel object captioning with visual vocabu-\nlary pre-training. In AAAI, 2021. 1, 2, 3, 11\n[19] Zhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu,\nDongmei Fu, and Jianlong Fu. Seeing out of the box: End-to-\nend pre-training for vision-language representation learning.\nIn CVPR, 2021. 6\n[20] Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu,\nand Jianlong Fu. Pixel-bert: Aligning image pixels with\ntext by deep multi-modal transformers. arXiv preprint\narXiv:2004.00849, 2020. 1, 2\n[21] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,\nHieu Pham, Quoc V . Le, Yun-Hsuan Sung, Zhen Li, and Tom\nDuerig. Scaling up visual and vision-language representation\nlearning with noisy text supervision. In Marina Meila and\nTong Zhang, editors, ICML, 2021. 1, 2, 3, 6, 9, 10\n[22] Huaizu Jiang, Ishan Misra, Marcus Rohrbach, Erik Learned-\nMiller, and Xinlei Chen. In defense of grid features for visual\nquestion answering. In CVPR, 2020. 1, 2\n[23] Aishwarya Kamath, Mannat Singh, Yann LeCun, Ishan\nMisra, Gabriel Synnaeve, and Nicolas Carion. MDETR -\nmodulated detection for end-to-end multi-modal understand-\ning. In ICCV, 2021. 2\n[24] Andrej Karpathy and Fei-Fei Li. Deep visual-semantic align-\nments for generating image descriptions. In CVPR, 2015. 5\n[25] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-\nand-language transformer without convolution or region su-\npervision. In Marina Meila and Tong Zhang, editors, ICML,\n2021. 1, 2, 3, 4, 6, 9, 10\n[26] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and\nFei-Fei Li. Visual genome: Connecting language and vision\nusing crowdsourced dense image annotations.arXiv preprint\narXiv:1602.07332, 2016. 5, 9\n[27] Gen Li, Nan Duan, Yuejian Fang, Ming Gong, and Daxin\nJiang. Unicoder-vl: A universal encoder for vision and lan-\nguage by cross-modal pre-training. In AAAI, 2020. 1, 2\n[28] Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Got-\nmare, ShaÔ¨Åq R. Joty, Caiming Xiong, and Steven C. H. Hoi.\nAlign before fuse: Vision and language representation learn-\ning with momentum distillation. In NeurIPS, 2021. 1, 2, 4,\n6, 7, 8, 9, 10\n[29] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh,\nand Kai-Wei Chang. Visualbert: A simple and perfor-\nmant baseline for vision and language. arXiv preprint\narXiv:1908.03557, 2019. 1, 2\n[30] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei\nHu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu\nWei, Yejin Choi, and Jianfeng Gao. Oscar: Object-semantics\naligned pre-training for vision-language tasks. In ECCV,\n2020. 1, 2, 3, 4, 6, 10, 11\n[31] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D.\nBourdev, Ross B. Girshick, James Hays, Pietro Perona,\nDeva Ramanan, Piotr Doll¬¥ar, and C. Lawrence Zitnick. Mi-\ncrosoft COCO: common objects in context. arXiv preprint\narXiv:1405.0312, 2014. 1, 3, 5, 9\n[32] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert:\nPretraining task-agnostic visiolinguistic representations for\nvision-and-language tasks. In NeurIPS, 2019. 1, 2\n[33] Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.\nIm2text: Describing images using 1 million captioned pho-\ntographs. In NeurIPS, 2011. 5, 9\n[34] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing\nZhu. Bleu: a method for automatic evaluation of machine\ntranslation. In ACL, 2002. 5\n[35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever. Learning transferable visual\nmodels from natural language supervision. In Marina Meila\nand Tong Zhang, editors, ICML, 2021. 1, 2, 3, 6, 10\n[36] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun.\nFaster R-CNN: towards real-time object detection with re-\ngion proposal networks. In NeurIPS, 2015. 1, 2\n[37] Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jerret\nRoss, and Vaibhava Goel. Self-critical sequence training for\nimage captioning. In CVPR, 2017. 5, 6, 11\n[38] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\nSoricut. Conceptual captions: A cleaned, hypernymed, im-\nage alt-text dataset for automatic image captioning. In ACL,\n2018. 5, 9\n[39] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu\nWei, and Jifeng Dai. VL-BERT: pre-training of generic\nvisual-linguistic representations. In ICLR, 2020. 1, 2\n[40] Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun\nBai, and Yoav Artzi. A corpus for reasoning about natural\nlanguage grounded in photographs. In ACL, 2019. 5\n[41] Siqi Sun, Yen-Chun Chen, Linjie Li, Shuohang Wang, Yuwei\nFang, and Jingjing Liu. Lightningdot: Pre-training visual-\nsemantic embeddings for real-time image-text retrieval. In\nNAACL-HLT, 2021. 2, 6, 10\n[42] Hao Tan and Mohit Bansal. LXMERT: learning cross-\nmodality encoder representations from transformers. In\nEMNLP, 2019. 1, 2\n[43] Mingxing Tan and Quoc V . Le. EfÔ¨Åcientnet: Rethinking\nmodel scaling for convolutional neural networks. In ICML,\n2019. 2\n[44] Ilya O. Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lu-\ncas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung,\nAndreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario\nLucic, and Alexey Dosovitskiy. Mlp-mixer: An all-mlp ar-\nchitecture for vision. In NeurIPS, 2021. 3\n[45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 1,\n3\n[46] Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi\nParikh. Cider: Consensus-based image description evalua-\ntion. In CVPR, 2015. 5\n[47] Jianfeng Wang, Xiaowei Hu, Pengchuan Zhang, Xiujun Li,\nLijuan Wang, Lei Zhang, Jianfeng Gao, and Zicheng Liu.\nMinivlm: A smaller and faster vision-language model. arXiv\npreprint arXiv:2012.06946, 2020. 1, 2, 3, 4, 10\n[48] Ning Xie, Farley Lai, Derek Doran, and Asim Kadav. Visual\nentailment: A novel task for Ô¨Åne-grained image understand-\ning. arXiv preprint arXiv:1901.06706, 2019. 5\n[49] Yujia Xie, Xiangfeng Wang, Ruijia Wang, and Hongyuan\nZha. A fast proximal point method for computing exact\nwasserstein distance. In Amir Globerson and Ricardo Silva,\neditors, UAI, 2019. 4\n[50] Mengde Xu, Zheng Zhang, Han Hu, Jianfeng Wang, Lijuan\nWang, Fangyun Wei, Xiang Bai, and Zicheng Liu. End-to-\nend semi-supervised object detection with soft teacher. In\nICCV, 2021. 4\n[51] Hongwei Xue, Yupan Huang, Bei Liu, Houwen Peng, Jian-\nlong Fu, Houqiang Li, and Jiebo Luo. Probing inter-\nmodality: Visual parsing with self-attention for vision-\nlanguage pre-training. In NeurIPS, 2021. 2, 6\n[52] Peter Young, Alice Lai, Micah Hodosh, and Julia Hocken-\nmaier. From image descriptions to visual denotations: New\nsimilarity metrics for semantic inference over event descrip-\ntions. TACL, 2014. 5\n[53] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang,\nLei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao.\nVinvl: Making visual representations matter in vision-\nlanguage models. In CVPR, 2021. 1, 2, 3, 4, 6, 10, 11\n[54] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Ja-\nson J. Corso, and Jianfeng Gao. UniÔ¨Åed vision-language pre-\ntraining for image captioning and VQA. In AAAI, 2020. 1,\n2, 4"
}