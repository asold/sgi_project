{
  "title": "From Artificially Real to Real: Leveraging Pseudo Data from Large Language Models for Low-Resource Molecule Discovery",
  "url": "https://openalex.org/W4393152796",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2104385254",
      "name": "Yuhan Chen",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5066758808",
      "name": "Nuwa Xi",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3094039654",
      "name": "Yanrui Du",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2158212157",
      "name": "Hao-Chun Wang",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2104851717",
      "name": "Jianyu Chen",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2918201438",
      "name": "Sendong Zhao",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2098930276",
      "name": "Bing Qin",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2104385254",
      "name": "Yuhan Chen",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5066758808",
      "name": "Nuwa Xi",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3094039654",
      "name": "Yanrui Du",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2158212157",
      "name": "Hao-Chun Wang",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2104851717",
      "name": "Jianyu Chen",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2918201438",
      "name": "Sendong Zhao",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2098930276",
      "name": "Bing Qin",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6644357179",
    "https://openalex.org/W3209056694",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W2123306226",
    "https://openalex.org/W6844053373",
    "https://openalex.org/W6688013296",
    "https://openalex.org/W4225000967",
    "https://openalex.org/W3211951295",
    "https://openalex.org/W4288066876",
    "https://openalex.org/W6839035525",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W4221167546",
    "https://openalex.org/W4377130766",
    "https://openalex.org/W3101980583",
    "https://openalex.org/W2461708070",
    "https://openalex.org/W6803129872",
    "https://openalex.org/W6898505805",
    "https://openalex.org/W6671884998",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2886544065",
    "https://openalex.org/W6647608673",
    "https://openalex.org/W6810889274",
    "https://openalex.org/W3154863804",
    "https://openalex.org/W2405035126",
    "https://openalex.org/W2284660317",
    "https://openalex.org/W6600238479",
    "https://openalex.org/W2251658415",
    "https://openalex.org/W2338373933",
    "https://openalex.org/W4317869829",
    "https://openalex.org/W2911588830",
    "https://openalex.org/W4212837331",
    "https://openalex.org/W4385572408",
    "https://openalex.org/W4385570408",
    "https://openalex.org/W4312091890",
    "https://openalex.org/W4320086632",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4380552032",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4297162632",
    "https://openalex.org/W2085313140",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4288091849",
    "https://openalex.org/W4385572894",
    "https://openalex.org/W4386614603",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W1975447903",
    "https://openalex.org/W4385574293",
    "https://openalex.org/W2767891136",
    "https://openalex.org/W2971296908",
    "https://openalex.org/W2200017991",
    "https://openalex.org/W2963216553",
    "https://openalex.org/W4318718899",
    "https://openalex.org/W4387874094",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4287891464",
    "https://openalex.org/W3160021293",
    "https://openalex.org/W4386614539",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W4313493205",
    "https://openalex.org/W3116865743",
    "https://openalex.org/W4320013936",
    "https://openalex.org/W1988037271",
    "https://openalex.org/W4206636317",
    "https://openalex.org/W4292779060"
  ],
  "abstract": "Molecule discovery serves as a cornerstone in numerous scientific domains, fueling the development of new materials and innovative drug designs. Recent developments of in-silico molecule discovery have highlighted the promising results of cross-modal techniques, which bridge molecular structures with their descriptive annotations. However, these cross-modal methods frequently encounter the issue of data scarcity, hampering their performance and application. In this paper, we address the low-resource challenge by utilizing artificially-real data generated by Large Language Models (LLMs). We first introduce a retrieval-based prompting strategy to construct high-quality pseudo data, then explore the optimal method to effectively leverage this pseudo data. Experiments show that using pseudo data for domain adaptation outperforms all existing methods, while also requiring a smaller model scale, reduced data size and lower training cost, highlighting its efficiency. Furthermore, our method shows a sustained improvement as the volume of pseudo data increases, revealing the great potential of pseudo data in advancing low-resource cross-modal molecule discovery.",
  "full_text": "From Artificially Real to Real: Leveraging Pseudo Data from Large Language\nModels for Low-Resource Molecule Discovery\nYuhan Chen, Nuwa Xi, Yanrui Du, Haochun Wang, Jianyu Chen, Sendong Zhao*, Bing Qin\nResearch Center for Social Computing and Information Retrieval\nHarbin Institute of Technology, China\n{yuhanchen, sdzhao}@ir.hit.edu.cn\nAbstract\nMolecule discovery serves as a cornerstone in numerous\nscientific domains, fueling the development of new mate-\nrials and innovative drug designs. Recent developments of\nin-silico molecule discovery have highlighted the promis-\ning results of cross-modal techniques, which bridge molec-\nular structures with their descriptive annotations. However,\nthese cross-modal methods frequently encounter the issue of\ndata scarcity, hampering their performance and application.\nIn this paper, we address the low-resource challenge by utiliz-\ning artificially-real data generated by Large Language Mod-\nels (LLMs). We first introduce a retrieval-based prompting\nstrategy to construct high-quality pseudo data, then explore\nthe optimal method to effectively leverage this pseudo data.\nExperiments show that using pseudo data for domain adap-\ntation outperforms all existing methods, while also requir-\ning a smaller model scale, reduced data size and lower train-\ning cost, highlighting its efficiency. Furthermore, our method\nshows a sustained improvement as the volume of pseudo data\nincreases, revealing the great potential of pseudo data in ad-\nvancing low-resource cross-modal molecule discovery.\nIntroduction\nMolecule discovery plays a critical role in numerous sci-\nentific domains including chemistry (Wang et al. 2023b;\nCuzzucoli Crucitti et al. 2023), pharmacology (Patani and\nLaV oie 1996; Anderson 2003), and materials science (Cur-\ntarolo et al. 2013). However, traditional molecule design\nmethods are frequently faced with challenges such as high\ncosts, lengthy development processes, and limited success\nrates. Introducing a new drug to the market, for instance,\nmight demand over a billion dollars and more than a decade\nof development (Gaudelet et al. 2021).\nWith the advent of artificial intelligence (AI), innovative\ncross-modal methods are ushering in new ways to synthe-\nsize and analyze complex molecular structures, enhancing\nefficiency and reshaping the fields of computational chem-\nistry and material science. Edwards et al. (2022) proposed\na novel approach to directly translate molecules to corre-\nsponding captions and generate molecular structures from\nnatural language text, shown in Figure 1. This cross-modal\n*Corresponding author\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nThe molecule is an \nalipha�c alcohol that \nis octane subs�tuted \nby a hydroxy group at \nposi�on 3...\nCCCCCC(CC)O\n(a) Molecular captioning\nThe molecule is an \nalipha�c alcohol that \nis octane subs�tuted \nby a hydroxy group at \nposi�on 3...\nCCCCCC(CC)O\n(b) Text-Based de novo Molecule Generation\nFigure 1: Illustration of translation between molecule and\ndescription in cross-modal molecule discovery.\nmethod heralds a future in which the design and study of\nspecialized molecules can be achieved through simple natu-\nral language sentences.\nVarious attempts have been made to resolve these tasks.\nMolT5 (Edwards et al. 2022) uses SMILES (Simplified\nMolecular Input Line Entry System) (Weininger 1988)\nand molecule description respectively for masked language\nmodeling (MLM) (Raffel et al. 2020) as pre-training. Liu\net al. (2023) pre-train models with causal language mod-\neling (CLM) on the sequences that blend biomedical lit-\nerature with molecular structural representations, derived\nfrom replacing molecular entities with their SMILES rep-\nresentations. However, these studies are limited by the\nscarcity of parallel molecule-description pairs, rendering di-\nrect sequence-to-sequence training unfeasible. The effec-\ntiveness of sequence-to-sequence (seq2seq) training is ev-\nident in Christofidellis et al. (2023), where the annotated\ndata from the downstream dataset is incorporated for pre-\ntraining, albeit in a significantly lower ratio compared to the\nunannotated data. The primary bottleneck is the annotation\nprocess itself: the annotation of these pairs demands spe-\ncialized knowledge in molecular chemistry, rendering large-\nscale human annotation both expensive and difficult.\nInspired by the great success of LLMs in natural language\nprocessing (NLP) and related fields (Bagal et al. 2021; Frey\net al. 2022; Ferruz, Schmidt, and H¨ocker 2022), we propose\nto mitigate the low-resource difficulty by using artificially-\nreal data generated by LLMs. Unlike “real data”, which orig-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21958\ninates from genuine experimental or observational sources,\nthis “pseudo data” or “artificially-real data” is crafted arti-\nficially. While it mirrors the format of real data, its content\ndoes not depict actual real-world observations, making it po-\ntentially unsuitable for direct real-world applications.\nOur approach begins by creating a comprehensive pseudo\ndataset intended for seq2seq pre-training. We collect 1M\nunlabeled molecules from PubChem and use the in-context\nlearning ability of LLMs to generate descriptive captions for\nthese molecules. To ensure the integrity and diversity of this\npseudo data, we adopt a retrieval-based one-shot prompting\nstrategy during generation. Through this way, we construct\nthe first artificially-real dataset, PseudoMD-1M, consisting\nof 1,020,139 pseudo molecule-description pairs.\nBased on this dataset, we explore the optimal method to\nleverage pseudo data. We propose two primary methods: 1)\nusing pseudo data exclusively during pre-training for do-\nmain adaptation, and 2) integrating pseudo data with real\ndata during fine-tuning as a data augmentation technique.\nTo offer a comprehensive evaluation, we further compile\nDrugBank-23, a novel dataset derived from a different data\nsource than existing datasets.\nIn summary, our contributions are as follows:\n• We are the first to incorporate LLMs for low-resource\nmolecule discovery. Using artificially-real data generated\nby LLMs, we are able to mitigate the data scarcity for\nthe tasks. We release PseudoMD-1M, the first artificially-\nreal dataset for cross-modal molecule discovery, which is\n33× larger than existing real datasets.\n• We explore the effective construction and utilization of\npseudo data. We specifically investigate two principal\ntechniques, including using pseudo data as domain adap-\ntation and data augmentation. We conduct comprehen-\nsive experiments on existing datasets, and provide our\nnew dataset called DrugBank-23, which adds a novel\ndata source compared to current datasets.\n• Experimental results show that despite smaller model\nsize and amount of pre-training data, models using\nartificially-real data as domain adaptation outperform all\nprior methods. Furthermore, our method shows contin-\nuous improvement with increasing volumes of pseudo\ndata, underscoring its promising future applications.\nRelated Work\nCross-Modal Molecule Discovery\nWith the advancement of in-silico molecule discovery meth-\nods, the field of molecule exploration is undergoing a trans-\nformative shift away from its resource-intensive and costly\norigins (Rifaioglu et al. 2019; Gaudelet et al. 2021). Ed-\nwards, Zhai, and Ji (2021) introduce a new task Text2Mol,\nwhich uses descriptions as search queries to retrieve the tar-\nget molecules. Following this, Edwards et al. (2022) propose\ntwo innovative tasks: molecule captioning and text-guided\nde novo molecule generation. These tasks aim at translat-\ning between molecular structures and natural language texts.\nMolXPT (Liu et al. 2023) leverages literature annotations of\nmolecules to construct a pre-training dataset. Christofidel-\nlis et al. (2023) further improves the field with multi-task\nlearning, which combines single-domain and cross-domain\ndatasets for joint training. Most recently, Li et al. (2023)\npropose a strategy that enables LLMs to accomplish both\nmolecule captioning and text-guided molecule generation\ntasks. Here we take one step further to construct a large num-\nber of high-quality parallel data pairs, in response to the data\nscarcity that limits the performance of the above approaches.\nLarge Language Models\nLLMs have achieved significant success in natural language\nprocessing by scaling up to billions of parameters (Brown\net al. 2020; Ouyang et al. 2022). Trained on vast corpora\n(Singhal et al. 2023), LLMs show more general intelligence\n(Bubeck et al. 2023) and remarkable capabilities such as in-\ncontext learning (Rubin, Herzig, and Berant 2022; Min et al.\n2022). They have also obtained promising performance in\nchemical (Bagal et al. 2021; Frey et al. 2022), biological\n(Ferruz, Schmidt, and H¨ocker 2022; Xi et al. 2023) and med-\nical (Wang et al. 2023a; Du et al. 2023) domains. Due to\ntheir great generation capability, numerous works have re-\nlied on LLMs to generate data for various purposes, includ-\ning creating semantic textual similarity datasets (Schick and\nSch¨utze 2021), augmenting natural language inference (Liu\net al. 2022), automatically formulating instructions (Wang\net al. 2022) and improving few-shot retrieval (Dai et al.\n2022). Inspired by these achievements, we aim to employ\nLLMs to generate parallel data, addressing data scarcity in\ncross-modal molecule discovery.\nMethodology\nTask Overview\nHere we introduce two primary tasks for cross-modal\nmolecule discovery. First proposed by Edwards et al. (2022),\nthe two tasks act as a bridge between molecule discovery and\nNLP and can be considered as cross-modal translation tasks.\nMolecular Captioning As illustrated in Figure 1a, Given\nthe SMILES representation SM of molecule M, the task is\nto generate the corresponding descriptions DM.\nText-Based De Novo Molecule Generation As shown in\nFigure 1b, given the descriptions DM of molecules M, the\ntask is to generate its corresponding SMILES SM.\nArtificially-Real Data Generation\nHigh-quality pseudo data is the foundation for further ex-\nploration. Here we propose PseudoMD-1M, the first pseudo\ndataset composed of 1M parallel molecule-description data\npairs. To acquire sufficient data, we leverage a vast num-\nber of unlabeled molecules and use LLMs to generate cor-\nresponding descriptions. We begin by collecting 1.1 million\nunannotated SMILES strings of molecules from PubChem\n(Kim et al. 2023). We then employ a rigorous filtering proce-\ndure to filter out the SMILES in downstream datasets to en-\nsure that there is no overlap between the collected molecules\nand those contained in the real datasets (Edwards, Zhai, and\nJi 2021; Zeng et al. 2022). By doing so, we ensure that no\nsupplementary information about the molecules present in\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21959\nCCC1=NCC(N=C1C)C\nStage #2: Few-Shot Promp�ng\nAs an AI model, you have been trained with vast amounts of biomedical, \nscien�ﬁc, and pharmaceu�cal data.\n# Role Deﬁni�on\nYou need to convert Simpliﬁed Molecular Input Line Entry System (SMILES) \nformulas of drugs into comprehensive descrip�ons, which typically include the \ndrug's proper�es, indica�ons, and pharmacological mechanisms.\n# Task Descrip�on\n# Few-shot Example\n# Example 1:\nSMILES: {SMILES Place Holder}\nDescrip�on: {Descrip�on Place Holder}\n# Example 2: …\n# Output Control\nUsing these examples as a guide, and provide a direct response without any \nintroductory or addi�onal text. Now generate a similar detailed descrip�on for \nthe following SMILES formula of a drug: {SMILES Place Holder}\nStage #1: Molecule RetrievalInput\nMolecule\n[0, …,1,….0,1,…0]\nMorgan FingerPrints\nRDkit\nLocal \nDatabase\nTanimoto \nSimilarity\nOutput\nDescrip�onTop-k results\n# Example 1\n# Example 2\nThe molecule is a \nmember of the \nclass of pyrazines.\nCCC1=NC=C(N=C1)C\n...\n...\nThe molecule represents \na compound that is a \npyrazole deriva�ve with \na piperazine ring fused to \nit. It is a selec�ve \ninhibitor of the enzyme \nphosphodiesterase 5.\nIn-context\nLearning\nThe molecule \nappears as a \ncolorless to \nyellow liquid.\nCCC1=CN=C(C=C1)C\n...\nFigure 2: The workflow for pseudo data generation. Starting with an unlabeled molecule represented by its Morgan Fingerprints,\ntwo stages are involved. In stage 1, the input molecule serves as a search query to retrieve the top-k similar molecules from a\nlocal database containing 37,898 annotated molecule-caption pairs. In stage 2, the retrieved molecules and their captions are\nintegrated into a prompt. Then LLMs perform in-context learning and generate a description for the input molecule.\nFigure 3: Comparison of data quality. We use the method\nproposed by Edwards et al. (2022) to evaluate the similar-\nity between molecule-description pairs as an estimation of\nthe data quality. The distribution is visualized using Ker-\nnel Distribution Estimation. A higher Text2Mol score signi-\nfies closer molecule-description resemblance, and “Density”\nrepresents the data concentration in a given region.\nthe real datasets is accidentally incorporated, thereby main-\ntaining the integrity and independence of the training pro-\ncess. With ChatGPT API, we generate textual descriptions\nthat encompass key aspects such as properties and struc-\ntural features for each unannotated molecule. To improve the\nquality of generated descriptions, we implement a retrieval-\nbased prompt paradigm that comprises two main stages as\nfollows: Molecule Retrieval and Few-Shot Prompting.\nMolecule Retrieval In-context learning (Brown et al.\n2020) is one of the emergent abilities of LLMs, and the\ninstances used in the prompts given to the LLMs play an\nimportant role in the generation quality. As molecules with\nsimilar structures often display corresponding characteris-\ntics (Wang et al. 2016), we retrieve the descriptions of an-\nnotated molecules that resemble the unlabeled molecule, us-\ning them as the few-shot instance during prompting. Specif-\nically, we collect 37,898 annotated molecules with captions\nfrom PubChem(Kim et al. 2023), then retrieve the molecules\nwith top-k Tanimoto similarity (Tanimoto 1958), a stan-\ndard measure in cheminformatics. To prevent information\nleakage during testing, we exclude the molecules that are\ncontained in the real data test set (Edwards, Zhai, and Ji\n2021; Zeng et al. 2022). This process enables the models\nto learn from the information embedded within the descrip-\ntions of molecules that possess similar properties, ensur-\ning a more tailored and accurate representation. Figure 3\nshows the estimate of the data quality, indicating that the\nfew-shot prompting approach (in blue) yields higher-quality\ndata, more closely resembling real data than without.\nFew-Shot Prompting Upon retrieving the top-k results\nfor each unlabeled molecule from our local database, we\nselect one example using a weighted distribution, where\nmolecules with higher similarity have a greater chance of\nbeing chosen. This selected example is then incorporated\ninto the final prompt. We opt for one-shot prompting to min-\nimize generation costs, as expenses increase linearly with\nthe number of instances included in few-shot prompts. This\nweighted selection method prevents repetitive selection of\nthe same molecule as the few-shot example, thereby improv-\ning the diversity during generation while maintaining the\nsimilarity between the molecule to be annotated and the few-\nshot example. As shown in Figure 2, the complete prompt\ncomprises role definition, task description, few-shot exam-\nple, and output control. The role definition and task descrip-\ntion give LLMs the general context and enable its learned\nknowledge, while the few-shot example acts like a supple-\nmentary material for the LLMs to refer to. Then, with the\noutput control for format clarification, the LLMs should be\nable to generate the desired description.\nApproaches to Utilize Artificially Real Data\nThe ways to utilize the pseudo data decide how the model\nwill perform on real data. We propose and explore two pri-\nmary strategies to optimize the use of pseudo data.\nPseudo Data as Data Augmentation Data augmentation\nstrategy can be roughly categorized into two kinds, modifi-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21960\nReal data\nModel for\nAdapted Domain\nPseudo data Real data\nTraining \nProcess\nTradi�onal\nTraining\nPseudo data as \ndata augmenta�on Pseudo data as domain adapta�on\nModel\nReal data Pseudo data\nShuﬄed\nModel\nModel for\nGeneral Domain\n① ②\nFigure 4: Different methods for utilizing pseudo data. Traditional training employs only the real dataset for fine-tuning. The data\naugmentation approach fine-tunes the model on the combined dataset with pseudo data incorporated. In the domain adaptation\nmethod, the model is (1) initially pre-trained on two concurrent cross-modal translation tasks using pseudo data as domain\nadaptation, and (2) further trained on each task using real data.\ncation of existing data and generation of pseudo data. The\nformer takes an existing data instance and makes certain\nalterations to it without changing its inherent meaning or\nlabel, such as rotation, flipping, and cropping for images\n(Krizhevsky, Sutskever, and Hinton 2012), or synonym re-\nplacement for text (Wang and Yang 2015; Wei and Zou\n2019; Miao et al. 2020). This method is more about adding\nvariability and noise to existing data instances than gener-\nating completely new ones. The latter, on the other hand,\ninvolves creating new data instances that did not exist in the\noriginal dataset based on the characteristics and distribution\nof the original data, which is an efficient alternative when\nreal data is scarce or when creating new real data is costly\nor unfeasible. Existing applications include back translation\nfor text (Sennrich, Haddow, and Birch 2016), and GANs for\nimages (Goodfellow et al. 2014).\nInspired by the latter techniques, we explore the use of\npseudo data as data augmentation. As shown in Figure 4, we\nkeep the original data in the training set and augment them\nwith pseudo data during fine-tuning. Using the same method\nas described in Figure 3, we assess the distribution of the real\ntraining set and the sample the augmented pseudo data based\non the same distribution, ensuring consistency in the overall\ndataset distribution before and after data augmentation. We\nhope that this data augmentation approach using pseudo data\nwill expose the model to a broader range of data patterns and\nscenarios, thus enhancing its ability to recognize complex\npatterns and generalize its learning to unseen data.\nPseudo Data as Domain Adaptation Models pre-trained\non general domain might perform less ideally when it is\napplied to specific domains for which they were not ex-\nplicitly trained (Malte and Ratadiya 2019). In our case, the\nSMILES appears as an unfamiliar symbol to such mod-\nels, making the direct fine-tuning approach less efficient.\nTo bridge this gap, we use pseudo data as a second pre-\ntraining stage for domain adaptation. As shown in Figure\n4, we train the model using pseudo data for two concur-\nrent cross-modal translation tasks: molecular captioning and\ntext-based de novo molecule generation. Using a direct and\nbidirectional seq2seq approach, this stage is intended to em-\npower the model to not only recognize the SMILES repre-\nsentation but also to grasp the relationship between natural\nlanguage and SMILES. Given that our primary focus at this\nstage is not on data authenticity, pseudo data emerges as a\npreferable choice, particularly because it provides a large\nnumber of parallel data pairs for supervised seq2seq train-\ning compared to real datasets. We then further fine-tune it\non real data to refine and enhance the model’s understand-\ning of SMILES for further authenticity – a critical aspect for\napplications like drug discovery.\nExperiments\nTo validate the effectiveness of using pseudo data, we con-\nduct comprehensive experiments comparing our proposed\napproaches with existing methods. We further conduct ex-\nperiments to demonstrate how the balance between real data\nand pseudo data could affect model performance. All the ex-\nperiments are conducted on both molecular captioning and\nmolecule generation. The implementation details are listed\nin Appendix C.\nSettings\nDatasets Currently, only a few datasets with parallel\nmolecule-description pairs exist, including ChEBI-20 (Ed-\nwards, Zhai, and Ji 2021) and PCdes (Zeng et al. 2022), both\nconstructed using data from PubChem (Kim et al. 2023). To\nenhance evaluation comprehensiveness, we assemble a new\ndataset called DrugBank-23, based on DrugBank (Wishart\net al. 2018). We experiment on all three datasets (ChEBI-20,\nPCdes, and DrugBank-23). The detailed information about\nthese datasets is listed in Table 1.\nModels We evaluate the following methods:\n• T5 (Raffel et al. 2020). T5 directly fine-tuned on down-\nstream datasets.\n• MolT5 (Edwards et al. 2022). T5 pre-trained with MLM\nusing SMILES and molecule descriptions respectively,\nthen fine-tuned on downstream datasets.\n• ChatGPT (Li et al. 2023). GPT-3.5-Turbo using\nfew-shot prompting strategy. We cite the results from the\noriginal paper on ChEBI-20, then apply the same strategy\nto test on the other datasets.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21961\nInfo ChEBI-20 PCdes DrugBank-23\nTrain 26,407 10,500 17,109\nValidation 3,301 1,500 3,667\nTest 3,300 3,000 3,666\nLSMILES 81.56 56.47 54.11\nLDescription 52.88 72.47 65.04\nData source PubChem DrugBank\nTable 1: Details about the existing datasets and ours\n(DrugBank-23). LSMILES denotes the average length of\nSMILES while LDescription denotes the average word count\nper description.\nModel Data scale Steps Backbone\nMolT5 500M 1M T5 large\nMolXPT 8M 200k GPT2 medium\nText&Chem T5 33.5M 131k T5 base\nAug-T5 0 0 T5 small\nAda-T5 1M 100k T5 small\nTable 2: Pre-training details for different Models. “M”\nstands for million and “k” denotes thousand.\n• MolXPT (Liu et al. 2023). GPT-2 pre-trained with CLM\nusing abstracts of biomedical literature where molecules\nare replaced with the corresponding SMILES, then fine-\ntuned on downstream datasets. As the model is currently\nunavailable, we cite their results on ChEBI-20.\n• Text&Chem T5 (Christofidellis et al. 2023). T5 pre-\ntrained using multi-task learning, then fine-tuned on\ndownstream datasets.\n• Aug-T5 (ours). T5 fine-tuned on datasets augmented\nwith pseudo data from PseudoMD-1M, sampled from 1k\nto 512k, doubling at each step. We report the optimal per-\nformances for each dataset. See Appendix D for details.\n• Ada-T5 (ours). T5 pre-trained using molecule-\ndescription pairs from PseudoMD-1M as domain\nadaptation, then fine-tuned on downstream datasets.\nAs shown in Table 2, both our proposed methods utilize\nthe smallest model scale, pre-training data, and steps, while\nAug-T5 requires no additional pre-training. We first test our\nmethods on T5 small (Aug-T5/Ada-T5) and then apply them\nto T5base (Aug-T5base/Ada-T5base).\nMetrics Following existing studies (Edwards et al. 2022;\nLiu et al. 2023; Christofidellis et al. 2023), we evaluate\nthe results for molecular captioning with BLEU-2, BLEU-\n4 (Papineni et al. 2002), ROUGE-1, ROUGE-2, ROUGE-L\n(Lin 2004) and METEOR (Banerjee and Lavie 2005), and\nBLEU-4 (Papineni et al. 2002), Accuracy (Edwards et al.\n2022), Validity (Polykovskiy et al. 2020), Levenshtein dis-\ntance (Miller, Vandome, and McBrewster 2009), MACCS-\nFTS (Durant et al. 2002), RDK-FTS (Schneider, Sayle, and\nLandrum 2015), Morgan-FTS (Rogers and Hahn 2010) and\nFCD (Preuer et al. 2018) for text-based de novo molecule\ngeneration. Selected metrics are presented in Tables 3, 4 and\nFigures 5 and 6, with comprehensive results in Appendix D.\nComparison with Existing Methods\nResults on Molecular Captioning Table 3 shows the re-\nsults of different models for molecule captioning. Ada-T5\noutperforms all previous methods and achieves the state-\nof-the-art on all three datasets across all the metrics. Com-\npared to the previous state-of-the-art, Ada-T5 uses less than\n3% of the pre-training data and only a third of the model\nparameters, yet requires fewer training steps, demonstrat-\ning the effectiveness and computational efficiency of high-\nquality pseudo data. On the other hand, Aug-T5 outperforms\nT5, MolT5, ChatGPT and has comparable performance with\nMolXPT and Text&Chem T5, using 9%-30% of the param-\neters and requires no pre-training. This highlights the ben-\nefit from the enhanced diversity of descriptions by incorpo-\nrating pseudo data into the training set. Meanwhile, Ada-\nT5base makes an extra but relatively little progress compared\nto Ada-T5, indicating that although using pseudo data for\ndomain adaptation could also benefit from the expansion of\nmodel size like most methods, the exploitation of pseudo\ndata only demands a relatively small number of parameters.\nIn contrast, Aug-T5base mirrors the results of its smaller ver-\nsion, indicating that for data augmentation, simply increas-\ning the model scale may not offer substantial benefits. One\nthing to notice is that despite the data used to train the model\nis generated by ChatGPT API, both our trained models can\nstill beat ChatGPT across different metrics. This indicates\nthat although ChatGPT can accomplish the task to a certain\nextent, the data it generated can still help the models achieve\na more seamless transition through pre-training from general\ndomain to this domain.\nResults on Text-Based Molecule Generation Table 4\npresents the results of different models for molecule gen-\neration. Ada-T5 achieves the best performance in all three\ndatasets across almost all metrics, demonstrating its capa-\nbility to generate high-quality SMILES. The only exception\nis that the MolXPT slightly surpasses Ada-T5 by 0.009 in\nChEBI-20 dataset on the validity metric, which is calcu-\nlated using RDkit to simply check whether the string can be\nsuccessfully converted to a molecule object without errors\nand whether the molecule represents a realistic and feasible\nchemical structure, without any comparison to the targeted\nSMILES and the input descriptions. Despite this one slight\nsuperiority, MolXPT performs significantly worse than Ada-\nT5 on other metrics, meaning that although it can generate\nslightly more valid SMILES, it does not take into account\nthe designated instructions, ergo making it one step away\nfrom real-world application.\nOn the other hand, Aug-T5 surpasses some existing meth-\nods in certain datasets on specific metrics. However, its\nconsistency falls short compared to Ada-T5. This variabil-\nity may be traced back to the construction of molecule-\ndescription data pairs in pseudo data: the LLMS use the real\nSMILES are used as the input, leaving only the description\npart of the pseudo data genuinely “pseudo”. This means that\nwhen training Aug-T5 on molecule captioning, it gets the\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21962\nModel Parameters ChEBI-20 PCdes DrugBank-23\nBL RG MET BL RG MET BL RG MET\nT5 800M 0.467†∗ 0.478†∗ 0.586†∗ 0.252†∗ 0.259†∗ 0.367†∗ 0.272†∗ 0.299†∗ 0.396†∗\nMolT5 800M 0.508† 0.510†∗ 0.614† 0.266† 0.272† 0.380†∗ 0.293† 0.317† 0.416†\nMolXPT 350M 0.505†∗ 0.511†∗ 0.626† - - - - - -\nText&Chem T5 250M 0.542† 0.543† 0.648† 0.266† 0.274† 0.382† 0.280†∗ 0.312†∗ 0.413†∗\nChatGPT - 0.482†∗ 0.450†∗ 0.585†∗ 0.194†∗ 0.193†∗ 0.315†∗ 0.191†∗ 0.218†∗ 0.325†∗\nAug-T5 77M 0.515 0.517 0.621 0.270 0.275 0.385 0.297 0.322 0.421\nAug-T5base 250M 0.516 0.520 0.620 0.268 0.272 0.383 0.294 0.316 0.416\nAda-T5 77M 0.553 0.552 0.652 0.295 0.295 0.406 0.310 0.337 0.435\nAda-T5base 250M 0.564 0.562 0.660 0.295 0.297 0.409 0.322 0.346 0.445\nTable 3: Results of different models for molecular captioning on ChEBI-20, PCdes and DrugBank-23 datasets.†/∗ denotes that\nAda-T5base/Aug-T5base perform significantly better than baselines at p−value < 0.01 using t-test. The best scores are in bold.\nBL: BLEU-4. RG: ROUGE-2. MET: METEOR.\nModel Parameters ChEBI-20 PCdes DrugBank-23\nAcc Val MAC Acc Val MAC Acc Val MAC\nT5 800M 0.279†∗ 0.902†∗ 0.823†∗ 0.089† 0.910†∗ 0.698† 0.131†∗ 0.923†∗ 0.682†\nMolT5 800M 0.311†∗ 0.905†∗ 0.834†∗ 0.097† 0.925† 0.695† 0.145†∗ 0.947† 0.686†\nMolXPT 350M 0.215†∗ 0.983 0.859†∗ - - - - - -\nText&Chem T5 250M 0.322†∗ 0.943†∗ 0.901† 0.105† 0.849†∗ 0.697† 0.149† 0.898†∗ 0.705\nChatGPT - 0.139†∗ 0.887†∗ 0.847†∗ 0.044†∗ 0.867†∗ 0.671†∗ 0.048†∗ 0.852†∗ 0.665†∗\nAug-T5 77M 0.305 0.907 0.877 0.070 0.892 0.700 0.141 0.911 0.685\nAug-T5base 250M 0.386 0.955 0.884 0.098 0.927 0.696 0.158 0.952 0.681\nAda-T5 77M 0.449 0.967 0.905 0.135 0.945 0.725 0.170 0.955 0.696\nAda-T5base 250M 0.486 0.974 0.911 0.150 0.956 0.743 0.192 0.969 0.706\nTable 4: Results of different models for molecule generation on ChEBI-20, PCdes and DrugBank-23 datasets. †/∗ denotes that\nAda-T5base/Aug-T5base perform significantly better than baselines at p−value < 0.01 using t-test. The best scores are in bold.\nAcc: Accuracy. Val: Validity. MAC: MACCS FTS.\nauthentic SMILES; but when training on molecule genera-\ntion, it gets the pseudo description. Consequently, the gap\nbetween the input training data leads to the gap between the\nmodel performance on different tasks. Furthermore, com-\npared with the results for molecular captioning, the base\ncounterparts of both methods for molecule generation ex-\nhibit pronounced enhancements, which could also attributed\nto the gap between the input data, as using the ”pseudo”\npart as the input for molecule generation might offer more\nspace for improvements, especially for larger-scale models\nthat can better tolerate the “pseudo” data nuances.\nThe difference between Aug-T5 and Ada-T5 also indi-\ncates the importance of data authenticity and the difference\nbetween real data and pseudo data: as Ada-T5 is later fine-\ntuned with 100% real data (in comparison with Aug-T5,\nwhich is fine-tuned with the mix of real data and pseudo\ndata), its misunderstandings about SMILES during domain\nadaptation through pseudo data are corrected and therefore\nhas a better overall performance. This further stresses that\nusing pseudo data for direct application may not be the opti-\nmal way to exploit its potential.\nEffect of the Amount of Pseudo Data\nIn order to further demonstrate how the amount of pseudo\ndata could affect model performance, we experiment on\nChEBI-20, the largest and most widely used dataset, with\nvarying numbers of pseudo data samplesN from 1k to 512k.\nResults on Molecular Captioning Figure 5 shows the re-\nsults of Ada-T5 and Aug-T5 for molecular captioning with\ndifferent amounts of pseudo data. Both Ada-T5 and Aug-\nT5 exhibit significant improvements when a modest amount\nof pseudo data is incorporated into their training. With just\n1k pseudo data, both methods can surpass T5 large and Chat-\nGPT and achieve a comparative performance to MolT5 large\nand MolXPT. This phenomena is often seen in other data\naugmentation strategies (Wei and Zou 2019; Sennrich, Had-\ndow, and Birch 2016), and can be attributed to the moder-\nate noise introduced by the pseudo data, which in turn bol-\nsters model generalization. As the amount of pseudo data\nincreases, Ada-T5 and Aug-T5 exhibit different tendencies.\nThe performance of Aug-T5 begins to decline when the\nnumber of pseudo data samples reaches 4k, and sees a sharp\ndrop when it exceeds 32k. This is possibly due to the im-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21963\n0.2\n0.3\n0.4\n0.5\n0.6\n0k\n1k\n2k\n4k\n8k\n16k\n32k\n64k\n128k\n256k\n512k\nAda-T5\nAug-T5\n(a) BLEU-4\n0.2\n0.3\n0.4\n0.5\n0.6\n0k\n1k\n2k\n4k\n8k\n16k\n32k\n64k\n128k\n256k\n512k\nAda-T5\nAug-T5 (b) ROUGE-2\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0k\n1k\n2k\n4k\n8k\n16k\n32k\n64k\n128k\n256k\n512k\nAda-T5\nAug-T5 (c) ROUGE-L\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0k\n1k\n2k\n4k\n8k\n16k\n32k\n64k\n128k\n256k\n512k\nAda-T5\nAug-T5 (d) METEOR\nFigure 5: Results of molecular captioning task using different amount of pseudo data.\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0k\n1k\n2k\n4k\n8k\n16k\n32k\n64k\n128k\n256k\n512k\nAda-T5\nAug-T5\n(a) Accuracy\n0.7\n0.8\n0.9\n1.0\n0k\n1k\n2k\n4k\n8k\n16k\n32k\n64k\n128k\n256k\n512k\nAda-T5\nAug-T5 (b) Validity\n0.7\n0.8\n0.9\n1.0\n0k\n1k\n2k\n4k\n8k\n16k\n32k\n64k\n128k\n256k\n512k\nAda-T5\nAug-T5 (c) MACCS FTS\n0.5\n0.6\n0.7\n0.8\n0.9\n0k\n1k\n2k\n4k\n8k\n16k\n32k\n64k\n128k\n256k\n512k\nAda-T5\nAug-T5 (d) RDK FTS\nFigure 6: Results of molecule generation task using different amount of pseudo data.\nbalance between real data and pseudo data: As the model\nbecomes increasingly exposed to unreal patterns from the\npseudo data, it might shift its attention away from genuine\npatterns. Consequently, the real patterns are overlooked by\nthe model that focuses on the artificial ones. In contrast,\nAda-T5 thrives with the increasing amount of pseudo data,\nevidenced by the growth of overall metrics. One possible\nexplanation is that Ada-T5 only uses pseudo data for pre-\ntraining, with follow-up fine-tuning using real data. Thus,\nthe increase of pseudo data does not twist its grasp of gen-\nuine patterns, but instead, further amplifies the proficiency\nof the model during subsequent training.\nResults on Text-Based Molecule Generation Figure 6\nshows the results of Ada-T5 and Aug-T5 for molecule gener-\nation with different amounts of pseudo data. Ada-T5 shows\nthe same superiority and trend as it does in molecular cap-\ntioning with more pseudo data incorporated, while Aug-T5\ndisplays a non-linear trend, with the optimal choice of the\namount of pseudo data significantly larger than when ap-\nplying Aug-T5 for molecular captioning. The reason might\nlie in the dual nature of pseudo data: it introduces both lin-\nguistic patterns and noise. Initially, a little bit of pseudo\ndata bolsters model generalization by acting as a regular-\nizer. But as more is added, an overbundance of noise de-\ngrades the results. However, once a critical mass of pseudo\ndata is reached, the model starts to recognize more sub-\ntle and broader linguistic patterns amidst the noise, which\nhelps in generating more accurate SMILES strings, leading\nto the observed spike in performance. After this peak, the\noverwhelming volume of pseudo data might reintroduce the\ndominance of noise, causing a decrease in performance.\nThe distinct behavior of Aug-T5 in molecular caption-\ning versus molecule generation highlights their inherent dif-\nferences. Molecular captioning, being more flexible, can\nbuffer linguistic variations, downplaying minor gains from\npseudo data and instead more affected by noise. In contrast,\nmolecule generation requires recognizing specific linguistic\ncues from descriptions that lead to exact structural changes\nin the SMILES output, making it more receptive to the sub-\ntle intricacies but can also discern and benefit from the subtle\npatterns present in pseudo data. Overall, these results indi-\ncate that the impact of pseudo data varies, depending on its\ninherent nature and the specific task at hand.\nConclusion\nIn this paper, we introduce a novel approach that enhances\nlow-resource cross-modal molecule discovery by leverag-\ning artificially-real data generated by LLMs. By incorpo-\nrating a retrieval-based few-shot prompting strategy, we are\nable to produce high-quality pseudo molecule-description\npairs. To mitigate the scarcity of data, we released two\ndatasets: PseudoMD-1M, the first artificially-real dataset for\nmolecule description, and DrugBank-23, a real molecule-\ndescription dataset constructed from a novel source. We pro-\npose to use pseudo data for domain adaptation and for data\naugmentation to explore its optimal utilization. Experiments\nacross different datasets show that the former can best ex-\nploit the potential of pseudo data, achieving better perfor-\nmance with less parameters and training data. Furthermore,\nas the performance of the model continues to benefit from\nthe increasing amount of pseudo data, our approach shows\nthe great potential of pseudo data, thereby providing a novel\nand promising approach for addressing low-resource chal-\nlenge in cross-modal molecule discovery.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21964\nAcknowledgements\nWe express our gratitude to the anonymous reviewers for\ntheir valuable feedback. This research was supported by the\nNational Key R&D Program of China (2021ZD0113302),\nthe National Natural Science Foundation of China Youth\nFund (62206079), and the Heilongjiang Provincial Natural\nScience Foundation of China (YQ2022F006). We also ap-\npreciate Du Xiaoman Technology’s support for our research.\nReferences\nAnderson, A. C. 2003. The process of structure-based drug\ndesign. Chemistry & biology, 10(9): 787–797.\nBagal, V .; Aggarwal, R.; Vinod, P.; and Priyakumar, U. D.\n2021. MolGPT: molecular generation using a transformer-\ndecoder model. Journal of Chemical Information and Mod-\neling, 62(9): 2064–2076.\nBanerjee, S.; and Lavie, A. 2005. METEOR: An automatic\nmetric for MT evaluation with improved correlation with hu-\nman judgments. In Proceedings of the acl workshop on in-\ntrinsic and extrinsic evaluation measures for machine trans-\nlation and/or summarization, 65–72.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners. Ad-\nvances in neural information processing systems, 33: 1877–\n1901.\nBubeck, S.; Chandrasekaran, V .; Eldan, R.; Gehrke, J.;\nHorvitz, E.; Kamar, E.; Lee, P.; Lee, Y . T.; Li, Y .; Lundberg,\nS.; et al. 2023. Sparks of artificial general intelligence: Early\nexperiments with gpt-4. arXiv preprint arXiv:2303.12712.\nChristofidellis, D.; Giannone, G.; Born, J.; Winther, O.;\nLaino, T.; and Manica, M. 2023. Unifying molecular and\ntextual representations via multi-task language modelling.\narXiv preprint arXiv:2301.12586.\nCurtarolo, S.; Hart, G. L.; Nardelli, M. B.; Mingo, N.; San-\nvito, S.; and Levy, O. 2013. The high-throughput highway\nto computational materials design. Nature materials, 12(3):\n191–201.\nCuzzucoli Crucitti, V .; Ilchev, A.; Moore, J. C.; Fowler,\nH. R.; Dubern, J.-F.; Sanni, O.; Xue, X.; Husband, B. K.;\nDundas, A. A.; Smith, S.; et al. 2023. Predictive Molecular\nDesign and Structure–Property Validation of Novel Terpene-\nBased, Sustainably Sourced Bacterial Biofilm-Resistant Ma-\nterials. Biomacromolecules, 24(2): 576–591.\nDai, Z.; Zhao, V . Y .; Ma, J.; Luan, Y .; Ni, J.; Lu, J.; Bakalov,\nA.; Guu, K.; Hall, K.; and Chang, M.-W. 2022. Prompta-\ngator: Few-shot Dense Retrieval From 8 Examples. In The\nEleventh International Conference on Learning Representa-\ntions.\nDu, Y .; Zhao, S.; Chen, Y .; Bai, R.; Liu, J.; Wu, H.; Wang,\nH.; and Qin, B. 2023. The CALLA Dataset: Probing LLMs’\nInteractive Knowledge Acquisition from Chinese Medical\nLiterature. arXiv preprint arXiv:2309.04198.\nDurant, J. L.; Leland, B. A.; Henry, D. R.; and Nourse, J. G.\n2002. Reoptimization of MDL keys for use in drug discov-\nery. Journal of chemical information and computer sciences,\n42(6): 1273–1280.\nEdwards, C.; Lai, T.; Ros, K.; Honke, G.; Cho, K.; and Ji,\nH. 2022. Translation between Molecules and Natural Lan-\nguage. In 2022 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2022.\nEdwards, C.; Zhai, C.; and Ji, H. 2021. Text2mol: Cross-\nmodal molecule retrieval with natural language queries. In\nProceedings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, 595–607.\nFerruz, N.; Schmidt, S.; and H ¨ocker, B. 2022. ProtGPT2\nis a deep unsupervised language model for protein design.\nNature communications, 13(1): 4348.\nFrey, N.; Soklaski, R.; Axelrod, S.; Samsi, S.; Gomez-\nBombarelli, R.; Coley, C.; and Gadepally, V . 2022. Neural\nscaling of deep chemical models.\nGaudelet, T.; Day, B.; Jamasb, A. R.; Soman, J.; Regep, C.;\nLiu, G.; Hayter, J. B.; Vickers, R.; Roberts, C.; Tang, J.; et al.\n2021. Utilizing graph machine learning within drug dis-\ncovery and development. Briefings in bioinformatics, 22(6):\nbbab159.\nGoodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;\nWarde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y .\n2014. Generative adversarial nets. Advances in neural in-\nformation processing systems, 27.\nKim, S.; Chen, J.; Cheng, T.; Gindulyte, A.; He, J.; He, S.;\nLi, Q.; Shoemaker, B. A.; Thiessen, P. A.; Yu, B.; et al. 2023.\nPubChem 2023 update. Nucleic acids research, 51(D1):\nD1373–D1380.\nKrizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Im-\nagenet classification with deep convolutional neural net-\nworks. Advances in neural information processing systems,\n25.\nLi, J.; Liu, Y .; Fan, W.; Wei, X.-Y .; Liu, H.; Tang, J.; and Li,\nQ. 2023. Empowering Molecule Discovery for Molecule-\nCaption Translation with Large Language Models: A Chat-\nGPT Perspective. arXiv preprint arXiv:2306.06615.\nLin, C.-Y . 2004. Rouge: A package for automatic evaluation\nof summaries. In Text summarization branches out, 74–81.\nLiu, A.; Swayamdipta, S.; Smith, N. A.; and Choi, Y . 2022.\nW ANLI: Worker and AI Collaboration for Natural Language\nInference Dataset Creation. In Findings of the Association\nfor Computational Linguistics: EMNLP 2022, 6826–6847.\nLiu, Z.; Zhang, W.; Xia, Y .; Wu, L.; Xie, S.; Qin, T.; Zhang,\nM.; and Liu, T.-Y . 2023. MolXPT: Wrapping Molecules\nwith Text for Generative Pre-training. In Proceedings of the\n61st Annual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), 1606–1616. Toronto,\nCanada: Association for Computational Linguistics.\nMalte, A.; and Ratadiya, P. 2019. Evolution of transfer\nlearning in natural language processing. arXiv preprint\narXiv:1910.07370.\nMiao, Z.; Li, Y .; Wang, X.; and Tan, W.-C. 2020. Snippext:\nSemi-supervised opinion mining with augmented data. In\nProceedings of The Web Conference 2020, 617–628.\nMiller, F. P.; Vandome, A. F.; and McBrewster, J. 2009.\nLevenshtein distance: Information theory, computer science,\nstring (computer science), string metric, damerau? Leven-\nshtein distance, spell checker, hamming distance.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21965\nMin, S.; Lewis, M.; Zettlemoyer, L.; and Hajishirzi, H. 2022.\nMetaICL: Learning to Learn In Context. In Proceedings\nof the 2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Lan-\nguage Technologies, 2791–2809.\nOuyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.;\nMishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.;\net al. 2022. Training language models to follow instructions\nwith human feedback. Advances in Neural Information Pro-\ncessing Systems, 35: 27730–27744.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.\nBleu: a method for automatic evaluation of machine trans-\nlation. In Proceedings of the 40th annual meeting of the\nAssociation for Computational Linguistics, 311–318.\nPatani, G. A.; and LaV oie, E. J. 1996. Bioisosterism: a ra-\ntional approach in drug design. Chemical reviews, 96(8):\n3147–3176.\nPolykovskiy, D.; Zhebrak, A.; Sanchez-Lengeling, B.; Golo-\nvanov, S.; Tatanov, O.; Belyaev, S.; Kurbanov, R.; Arta-\nmonov, A.; Aladinskiy, V .; Veselov, M.; et al. 2020. Molec-\nular sets (MOSES): a benchmarking platform for molecular\ngeneration models. Frontiers in pharmacology, 11: 565644.\nPreuer, K.; Renz, P.; Unterthiner, T.; Hochreiter, S.; and\nKlambauer, G. 2018. Fr ´echet ChemNet distance: a met-\nric for generative models for molecules in drug discovery.\nJournal of chemical information and modeling, 58(9): 1736–\n1741.\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2020. Explor-\ning the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research,\n21(1): 5485–5551.\nRifaioglu, A. S.; Atas, H.; Martin, M. J.; Cetin-Atalay, R.;\nAtalay, V .; and Do˘gan, T. 2019. Recent applications of deep\nlearning and machine intelligence on in silico drug discov-\nery: methods, tools and databases. Briefings in bioinformat-\nics, 20(5): 1878–1912.\nRogers, D.; and Hahn, M. 2010. Extended-connectivity fin-\ngerprints. Journal of chemical information and modeling,\n50(5): 742–754.\nRubin, O.; Herzig, J.; and Berant, J. 2022. Learning To\nRetrieve Prompts for In-Context Learning. In Proceedings\nof the 2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Lan-\nguage Technologies, 2655–2671.\nSchick, T.; and Sch ¨utze, H. 2021. Generating Datasets\nwith Pretrained Language Models. In Proceedings of the\n2021 Conference on Empirical Methods in Natural Lan-\nguage Processing, 6943–6951. Online and Punta Cana, Do-\nminican Republic: Association for Computational Linguis-\ntics.\nSchneider, N.; Sayle, R. A.; and Landrum, G. A. 2015.\nGet Your Atoms in Order An Open-Source Implementa-\ntion of a Novel and Robust Molecular Canonicalization Al-\ngorithm. Journal of chemical information and modeling,\n55(10): 2111–2120.\nSennrich, R.; Haddow, B.; and Birch, A. 2016. Improv-\ning Neural Machine Translation Models with Monolingual\nData. In Proceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long\nPapers), 86–96.\nSinghal, K.; Azizi, S.; Tu, T.; Mahdavi, S. S.; Wei, J.; Chung,\nH. W.; Scales, N.; Tanwani, A.; Cole-Lewis, H.; Pfohl, S.;\net al. 2023. Large language models encode clinical knowl-\nedge. Nature, 1–9.\nTanimoto, T. T. 1958. Elementary mathematical theory of\nclassification and prediction.\nWang, H.; Zhao, S.; Qiang, Z.; Li, Z.; Xi, N.; Du, Y .; Cai, M.;\nGuo, H.; Chen, Y .; Xu, H.; et al. 2023a. Knowledge-tuning\nLarge Language Models with Structured Medical Knowl-\nedge Bases for Reliable Response Generation in Chinese.\narXiv preprint arXiv:2309.04175.\nWang, W. Y .; and Yang, D. 2015. That’s so annoying!!!: A\nlexical and frame-semantic embedding based data augmen-\ntation approach to automatic categorization of annoying be-\nhaviors using# petpeeve tweets. In Proceedings of the 2015\nconference on empirical methods in natural language pro-\ncessing, 2557–2563.\nWang, Y .; Kordi, Y .; Mishra, S.; Liu, A.; Smith, N. A.;\nKhashabi, D.; and Hajishirzi, H. 2022. Self-instruct: Align-\ning language model with self generated instructions. arXiv\npreprint arXiv:2212.10560.\nWang, Z.; Liang, L.; Yin, Z.; and Lin, J. 2016. Improving\nchemical similarity ensemble approach in target prediction.\nJournal of cheminformatics, 8: 1–10.\nWang, Z.; Liu, T.; Peng, H.; and Fang, Y . 2023b. Advances\nin molecular design and photophysical engineering of pery-\nlene bisimide-containing polyads and multichromophores\nfor film-based fluorescent sensors. The Journal of Physical\nChemistry B, 127(4): 828–837.\nWei, J.; and Zou, K. 2019. EDA: Easy Data Augmentation\nTechniques for Boosting Performance on Text Classification\nTasks. In Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), 6382–6388.\nWeininger, D. 1988. SMILES, a chemical language and in-\nformation system. 1. Introduction to methodology and en-\ncoding rules. Journal of chemical information and computer\nsciences, 28(1): 31–36.\nWishart, D. S.; Feunang, Y . D.; Guo, A. C.; Lo, E. J.; Marcu,\nA.; Grant, J. R.; Sajed, T.; Johnson, D.; Li, C.; Sayeeda, Z.;\net al. 2018. DrugBank 5.0: a major update to the DrugBank\ndatabase for 2018. Nucleic acids research, 46(D1): D1074–\nD1082.\nXi, N.; Zhao, S.; Wang, H.; Liu, C.; Qin, B.; and Liu, T.\n2023. UniCoRN: Unified Cognitive Signal Reconstruc-\ntioN bridging cognitive signals and human language. arXiv\npreprint arXiv:2307.05355.\nZeng, Z.; Yao, Y .; Liu, Z.; and Sun, M. 2022. A deep-\nlearning system bridging molecule structure and biomedical\ntext with comprehension comparable to human profession-\nals. Nature communications, 13(1): 862.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21966",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5782079100608826
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.5702911019325256
    },
    {
      "name": "Data science",
      "score": 0.33680635690689087
    },
    {
      "name": "Computer network",
      "score": 0.07246610522270203
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I204983213",
      "name": "Harbin Institute of Technology",
      "country": "CN"
    }
  ],
  "cited_by": 3
}