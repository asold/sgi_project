{
  "title": "Softmax Bottleneck Makes Language Models Unable to Represent Multi-mode Word Distributions",
  "url": "https://openalex.org/W4285298148",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4316344404",
      "name": "Haw-Shiuan Chang",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    },
    {
      "id": "https://openalex.org/A2077104176",
      "name": "Andrew McCallum",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2951210602",
    "https://openalex.org/W2983681322",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W3207166518",
    "https://openalex.org/W4386506836",
    "https://openalex.org/W3176904855",
    "https://openalex.org/W3133264589",
    "https://openalex.org/W2888799392",
    "https://openalex.org/W2970184163",
    "https://openalex.org/W4287633642",
    "https://openalex.org/W2966610483",
    "https://openalex.org/W3103662468",
    "https://openalex.org/W3175049034",
    "https://openalex.org/W4311887664",
    "https://openalex.org/W4286900915",
    "https://openalex.org/W4293718192",
    "https://openalex.org/W4206297651",
    "https://openalex.org/W2980360762",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W2963462013",
    "https://openalex.org/W1579853615",
    "https://openalex.org/W3119519251",
    "https://openalex.org/W2951143519",
    "https://openalex.org/W4287391717",
    "https://openalex.org/W2963127401",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W3035507081",
    "https://openalex.org/W3035618147",
    "https://openalex.org/W3123673616",
    "https://openalex.org/W3105816068",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W4226099034",
    "https://openalex.org/W2114082868",
    "https://openalex.org/W4385574293",
    "https://openalex.org/W3101056292",
    "https://openalex.org/W3014104201",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3166738839",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W2007548261",
    "https://openalex.org/W2963537482",
    "https://openalex.org/W3125516434",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2583105957",
    "https://openalex.org/W4394667011"
  ],
  "abstract": "Neural language models (LMs) such as GPT-2 estimate the probability distribution over the next word by a softmax over the vocabulary. The softmax layer produces the distribution based on the dot products of a single hidden state and the embeddings of words in the vocabulary. However, we discover that this single hidden state cannot produce all probability distributions regardless of the LM size or training data size because the single hidden state embedding cannot be close to the embeddings of all the possible next words simultaneously when there are other interfering word embeddings between them. In this work, we demonstrate the importance of this limitation both theoretically and practically. Our work not only deepens our understanding of softmax bottleneck and mixture of softmax (MoS) but also inspires us to propose multi-facet softmax (MFS) to address the limitations of MoS. Extensive empirical analyses confirm our findings and show that against MoS, the proposed MFS achieves two-fold improvements in the perplexity of GPT-2 and BERT.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 8048 - 8073\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nSoftmax Bottleneck Makes Language Models Unable to Represent\nMulti-mode Word Distributions\nHaw-Shiuan Chang and Andrew McCallum\nCICS, University of Massachusetts, Amherst\n140 Governors Dr., Amherst, MA, USA 01003\n{hschang,mccallum}@cs.umass.edu\nAbstract\nNeural language models (LMs) such as GPT-\n2 estimate the probability distribution over the\nnext word by a softmax over the vocabulary.\nThe softmax layer produces the distribution\nbased on the dot products of a single hidden\nstate and the embeddings of words in the vo-\ncabulary. However, we discover that this sin-\ngle hidden state cannot produce all probabil-\nity distributions regardless of the LM size or\ntraining data size because the single hidden\nstate embedding cannot be close to the embed-\ndings of all the possible next words simulta-\nneously when there are other interfering word\nembeddings between them. In this work, we\ndemonstrate the importance of this limitation\nboth theoretically and practically. Our work\nnot only deepens our understanding of soft-\nmax bottleneck and mixture of softmax (MoS)\nbut also inspires us to propose multi-facet soft-\nmax (MFS) to address the limitations of MoS.\nExtensive empirical analyses conﬁrm our ﬁnd-\nings and show that against MoS, the proposed\nMFS achieves two-fold improvements in the\nperplexity of GPT-2 and BERT.\n“The greater the ambiguity, the greater the plea-\nsure.” — Milan Kundera\n1 Introduction\nRecently, researchers have found that transformer-\nbased language models (LMs), such as GPT-2, can\npredict the next/masked word distribution better\nas their sizes grow (Radford et al., 2019; Brown\net al., 2020; Kaplan et al., 2020). Compared to\ngreedily outputting the most probable next word,\nsampling the next word from the predicted distri-\nbution allows a LM to generate more diverse and\nhigh-quality text sequences (Holtzman et al., 2020).\nBy autoregressively sampling the next word ac-\ncording to its predicted probability, large LMs can\nbe used to assist creative writing (Akoury et al.,\n2020), reduce the cost of building datasets (West\net al., 2021; Liu et al., 2022), generate codes (Li\net al., 2022), solve math problems (Cobbe et al.,\n2021), etc. As a result, one natural question arises:\nDo modern language modeling architectures still\nhave restrictions in their ability to represent the ap-\npropriate distribution over next words or masked\nwords?\nIn this paper, we discover that, when predict-\ning the next word probabilities given an ambigu-\nous context, GPT-2 is often incapable of assigning\nthe highest probabilities to the appropriate non-\nsynonym candidates. For example, given the in-\nput prompt “After debating whether to bow to\nthe woman or the king ﬁrst, the jester decided on\nthe [MASK]” , we would expect the distribution\nover the [MASK] ﬁllers to put high probabilities\non woman or king or their synonyms. However,\nGPT-2 might incorrectly assign the second-highest\nprobability to “queen” as in Figure 1.\nIn the ﬁnal softmax layer of GPT-2, the log prob-\nabilities of the woman and king are computed based\non the dot product between a single hidden state\nembedding and the global word embeddings of\nthe woman and king, respectively. To have the\nhighest but similar dot products for the two op-\ntions, the transformer encoder in GPT-2 wants to\noutput the hidden state that is close to the aver-\nage of the woman embedding and the king embed-\nding. However, the words queen, king, woman, and\nman tend to form a parallelogram in the embed-\nding space (Mikolov et al., 2013; Ethayarajh et al.,\n2019; Wang et al., 2019)1, which means the man\nand queen also have a similar average. Therefore,\nGPT-2 is forced to also output man or queen when\nit wants to output woman or king.\nThe problem not only happens to GPT-2 or the\nwords whose embeddings form a parallelogram\nshape. Even though the hidden state embeddings\nof LMs are contextualized, the embedding of each\n1Section 2.1 provides more background knowledge about\nthe parallelogram shape and the softmax bottleneck.\n8048\nOutput Word Embedding Space\nGPT-2 Encoder\nAfter debating whether to bow to the woman or the king ﬁrst, the jester decided on the\n… king … queen … man … woman …\nDot Product\nSoftmax\nVocabulary Size\nGPT-2 + Multi-embedding Encoder\nAfter debating … king ﬁrst, the jester decided on the\nTop prediction candidates of \nGPT-2\nDot Product\nSoftmax\nWeighted Sum\nTop prediction candidates of \nmulti-embedding GPT-2\nking\nwoman queen\nman\nWord Probability\nking 0.70\nqueen 0.15\nwoman 0.05\nman 0.02\n… …\nWord Probability\nking 0.50\nwoman 0.40\nqueen 0.01\nman 0.01\n… …\n… king … woman …\nFigure 1: Comparison between the softmax layers using a single embedding and multiple embeddings when the\nnext word should be either woman or king. In GPT-2 and multi-embedding GPT-2, the hidden states of the context\nare visualized by the single facet and multiple facets , respectively. The word embeddings are visualized using\n•••••••••••. GPT-2 cannot output woman and king as the top two words because queen and man are close to the midpoint\nof woman and king. The improvement in this type of ambiguous context will be quantiﬁed in Section 5.\nword in the softmax layer is global and static dur-\ning the inference time. Globally dissimilar words\ncould all become the suitable next word in a con-\ntext while other interfering words might be between\nthem, which makes the ideal next word embedding\ndistribution have multiple modes and cannot be\nmodeled by the single embedding representation.\nIn this work, we propose theorems showing that\ngiven any LM using the output softmax layer, when\nthere are more than N word embeddings in a N−1\ndimensional subspace/hyperplane (e.g., four em-\nbeddings in a two-dimensional plane), we can al-\nways ﬁnd a set of possible next words (e.g., woman\nand king) such that there are some other interfering\nwords between them (e.g., man or queen). That is,\nthe multimodal next word distribution must exist if\na few word embeddings are linearly dependent.\nRecently, mixture of softmax (MoS) (Yang et al.,\n2018) regains attention as one of the few effec-\ntive architecture modiﬁcations for transformer LM\n(Narang et al., 2021; Anonymous, 2021). In the\nmeanwhile, Parthiban et al. (2021) show that the\nsoftmax bottleneck (Yang et al., 2018) theory is not\nsufﬁcient to explain the improvement of MoS. As a\nremedy, our theorems not only provide geometrical\nintuitions of why and when the multiple embed-\nding representation such as MoS would do better\nbut also suggest that the softmax bottleneck might\nnot be completely solved even if we adopt a very\nlarge hidden state size. For example, no matter\nhow large the hidden state size is, as long as queen\n- king = woman - man in the embedding space, the\nLMs cannot output a pair of words in the longer\ndiagonal of the parallelogram as the top two output\nwords.\nAfter better understanding why mixture of soft-\nmax (MoS) works well, we propose two enhance-\nments over MoS. The ﬁrst enhancement considers\nthe hidden states of multiple positions and multiple\ntransformer layers when determining the probabil-\nity in each softmax; the second enhancement uses\ndifferent contextualized embeddings to compute\nthe probabilities of different subsets of words in\nthe vocabulary.\nThe resulting method, multi-facet softmax\n(MFS), signiﬁcantly outperforms the MoS and the\nsoftmax layer in GPT-2 on the perplexity for pre-\ndicting the next word, especially in ambiguous con-\ntext and non-English text in OpenWebText (Rad-\nford et al., 2019). Finally, we also show that MFS\ncould improve the performance of GPT-2 on Pro-\ntoQA (Boratko et al., 2020), a commonsense ques-\ntion answering dataset where each question has\nmultiple acceptable answers.\nWe summarize our theoretical, methodological,\nand empirical contributions as follows.\n• Theory: We show the softmax layer using a sin-\ngle embedding is sometimes not able to output\nan appropriate rank of probabilities on a set of\nwords with linearly dependent embeddings.\n8049\n• Method: Addressing two weaknesses in\nMoS (Yang et al., 2018), we propose multi-facet\nsoftmax (MFS), a new alternative to the output\nsoftmax layer. MFS can replace the softmax in\npre-trained LMs to better handle ambiguous con-\ntexts without re-training the LMs from scratch.\n• Analysis: Our comprehensive empirical analyses\ndiscover and explain several phenomena, such\nas a) why using multiple embeddings is usually\nbetter than the single embedding with the non–\nlinearity, b) why the improvement is larger in\nambiguous contexts, less common languages, or\nGPT-2 compared to BERT, and c) why a LM of-\nten confuses with similar words.\n2 Theoretical Limitations of the Single\nEmbedding in the Softmax Layer\nIn this section, we ﬁrst review the softmax layer of\nGPT-2 formally and explain why queen - king =\nwoman - man still tends to hold in contextualized\nLMs. Next, we present our theoretical analyses,\nwhich generalize the woman and king example by\nshowing that the candidate words in a low dimen-\nsional subspace would induce the impossibility of\nranking some candidates on top of other candidates.\n2.1 Background\nThe LMs typically use a softmax layer to predict\nPS(x|ct), the probability of the next word xgiven\nthe context at the tth position ct:\nPS(x|ct) = exp(hT\nctwx)∑\nx′exp(hTctwx′), (1)\nwhere hct is the tth hidden state in the context\nc, and wx is the output word embedding for\nthe word x (i.e., the linear weights that project\nthe hidden state to the logit of the word x).2\nYang et al. (2018) point out that the log proba-\nbility distribution over all the words in the vo-\ncabulary V is log (PS(x|ct)) |x∈V = hT\nctwx −\nlog\n(∑\nx′exp(hT\nctwx′)\n)\n|x∈V. The distribution is\na linear projection from the hidden state hct with\ndimension D, so the degree of freedom in the dis-\ntribution is only D(i.e., there cannot be more than\nDlinearly independent log distributions). We call\nthis restriction softmax bottleneck theory.\n2Notice that some LMs such as BERT add a bias term\nfor each word before the softmax layer. For simplicity, our\ntheoretical analyses focus on the LMs without the bias term\nsuch as GPT-2.\nDuring training, the ideal output word embed-\nding wx should be close to the hidden states of\nthe contexts hct that co-occur with the word x\nwhile far away from the other hidden states. This\nobjective is similar to the objective function of\nWord2Vec (Mikolov et al., 2013) except that the\ncontext embeddings are contextualized (Kong et al.,\n2020; Li et al., 2020).\nIf a context ct has a higher chance to co-occur\nwith queen compared to king, the context also\nhas a higher chance to co-occur with woman com-\npared to man to a similar degree. This is the main\nreason that makes queen - king = woman - man\nin the Word2Vec space (Ethayarajh et al., 2019).\nTherefore, the same linear relations tend to hold\nin the output word embedding space of GPT-2 as\nwell (Wang et al., 2019).\n2.2 Structural Weakness Theorems from\nLinear Dependency\nIn addition to words satisfying the analogy rela-\ntions, the following theorems imply that any linear\ndependency among the words causes the difﬁculties\nof LM in ranking the words in an arbitrary order\naccording to their logits (i.e., dot products between\nthe hidden state and the word embedding). For\nexample, woman + king = queen + man makes a\nLM unable to assign the highest positive logits to\nwoman and king and output them as the top two\nwords in Figure 1.\nTheorem 1. If the nonzero output embeddings of\nN words in a set W are linearly dependent and on\none side of a hyperplane through the origin, the\nsingle embedding representation cannot produce\npositive logits for a subset of the words in W that\nare higher than all the logits of the other words in\nW.\nHere, we provide an intuitive justiﬁcation: if N\nembeddings are in a subspace whose dimension\nis smaller than N −1 (e.g., three points in a one-\ndimensional line), the N embeddings are going to\nbe linearly dependent and some set of words cannot\nhave the top dot products due to the limited degree\nof freedom in the subspace. In Appendix D, we\nformally prove the theorem by identifying the sets\nof words that cannot be ranked top by the single\nembedding representation.\nIn practice, linear dependency holds approxi-\nmately instead of exactly. For example, woman =\nqueen + man - king + ε. In this practical condition,\nthe following theorem states that the logits of the\n8050\nInput Hidden States (#I)  \nSec. 3.2\ndot product\n  \n(c) Mixture of Softmax (Yang et al., 2018)\nfct,2\n(d) Multi-facet Softmax (Ours)\nfct,3f1ct,1\nf2ct,1\nf3ct,1\nf4ct,1\nπct\nvocab vocab\nSoftmax Softmax\nWeighted Sum\nSoftmax\nvocab\nfct,1 fct,2 fct,3\nπct\nvocab vocab\nSoftmax Softmax\nWeighted Sum\nSoftmax\nvocab\n(a) Softmax (b) Softmax  \n+ Multi-input\nfct,1\nSoftmax\nvocab\nfct,1\nSoftmax\nvocab\nGPT-2 encoder\nAfter debating whether to bow to the woman or the king ﬁrst, the jester decided on the\nqct\nhctM\nqct\nlayer M-2\n⊕i,mhct-iM-m\nLf(.)\nGELU(Lh(.))\nLπ(.)\nlayer M\nLπ(.)\nLf(.)\n……\nfacets\ndot product dot product dot product\nPartitions (#P)  \nSec. 3.3\n#S = 3\n#I = 9\n#P = 4\n#I = 1\n#S = 1\n#P = 1\n#S = 1\n#P = 1\n#I = 9\n#S = 3\n#P = 1\n#I = 1\nfacets\nPartition 1 2 3 4\nW\nH\nFigure 2: Comparison between different architectures. The #S , #I , and #P are the number of softmaxes, input\nhidden states, and partitions, respectively. The green boxes refer to embeddings/vectors. The vocab means the\nembeddings of all words in the vocabulary. ⊕refers to concatenation. Lh, Lf, and Lπ are linear projection layers.\ninterfering words (i.e., man and queen) cannot be\nmuch smaller than the logits of the candidate words\n(i.e., woman and king).\nTheorem 2. Let the output word embeddings in\nthe set W = {wi ̸= 0|i = 1 ...N} satisfy\nw1 = a2w2 + ...+ aNwN + ε, where the con-\nstant a2,...,a N are neither all zero nor all neg-\native and ||ε||< ϵ. Then, there must be a non-\ntrivial partition P = {G,S}of W such that\nthere is no hidden state ||h||≤ rand a threshold\nτ ≥rϵthat make minwg∈GhTwg ≥(1 +δ)τ and\nmaxws∈ShTws <τ , where δ= 2\n1+∑\ni=2...N |ai|.\nIn the king-woman example,(1+δ) = (1+2\n4 ) =\n1.5. Assuming ||ε||< ϵ= 0.01 and ||h||≤ r =\n20, we can get hTε ≤0.01 ×20 = 0.2. Then, we\ncannot ﬁnd a hidden state h such that hTwking ≥\n1.5 ×0.01 ×20 = 0.3 and hTwwoman ≥ 0.3\nbut hTwqueen <0.2 and hTwman <0.2 because\nhTwking+hTwwoman = hTwqueen+hTwman+\nhTε. The formal proof of Theorem 2 can be found\nin Appendix D and Appendix B.1 estimates ϵin\nseveral language models.\nEven though, theoretically speaking, outputting\nwoman and king as the top two words is possible\ndue to the appearance of ε, LMs may not success-\nfully learn to output the optimal h and the optimal\nhidden state for these four words could lead to the\nwrong probabilities of the other words. Conse-\nquently, LMs sometimes still rank queen or man\nhigher than woman or king in practice.\n3 Multi-facet Softmax\nUsing multiple embeddings is a natural solution\nfor modeling a multimodal distribution (Bishop,\n1994). For instance, we can use three embeddings\nto capture the high probability on the woman and\nking but low probability on the man and queen in\nFigure 1.\nInspired by our geometric analysis on the lim-\nitation of the single embedding, we improve the\nstate-of-the-art multiple embedding solution, mix-\nture of softmax (MoS) (Yang et al., 2018) by two\nenhancements: multiple input hidden states and\nmultiple partitions on the vocabulary.\n3.1 Mixture of Softmax\nYang et al. (2018) propose mixture of softmax\n(MoS) to allow a LSTM-based (Hochreiter and\nSchmidhuber, 1997) LM to produce more linearly\nindependent log probability distributions of the out-\nput words given different contexts. As in Figure 2\n(c), the MoS ﬁrst uses multiple linear layers Lf\nk to\nproject a hidden state hct into multiple facet em-\nbeddings fct,k = Lf\nk(hct).3 The multiple facets\nfct,k and softmaxes would lead to multiple prob-\nability distributions, and output probability is the\nweighted average of the distributions:\nPMoS(x|ct) =\nK∑\nk=1\nπct,k\nexp(fT\nct,kwx)\n∑\nx′exp(fT\nct,kwx′). (2)\n3We remove the tanh layer in the original MoS to improve\nits performance on GPT-2. See Appendix G.1 for details.\n8051\nThe prior weights πct,k = exp(Lπ\nk(hct))∑\nk′exp(Lπ\nk′(hct)) , where\nLπ\nk is another linear projection for dynamically gen-\nerating the weights and the projection goes through\na softmax to ensure ∑K\nk=1 πct,k = 1.\n3.2 Multiple Input Hidden States\nTo model the multimodal distribution, the facets\n(i.e., the embeddings for different softmaxes)\nshould be able to move freely. For example, in\nFigure 1, we have three facets but only have two\nmodes, so the two embeddings are very close to\nthe word king. However, when we want to output\nthree dissimilar top words such as the king, woman,\nand knight, one of the facets should be moved to\nbe near to the embedding of the knight.\nTherefore, we want our solution to satisfy two\nproperties: a) the linear transformation matrix in\nLf\nk should have a full rank to avoid limiting the\ndegree of freedom in each facet, and b) the relative\nlocation of the facets should be context-dependent.\nMoS cannot satisfy both properties. If the ﬁrst one\nis satisﬁed, the input hidden state is uniquely de-\ntermined by a facet (e.g., hct = (Lf\n1 )−1(fct,1)).\nThen, there exists a global transformation between\ntwo facets (e.g., fct,2 = Lf\n2\n(\n(Lf\n1 )−1(fct,1)\n)\n),\nwhich violates the second property. That is, as-\nsuming LM can move every facet freely (i.e., the\nfacet’s degree of freedom is the same as the dimen-\nsion of the hidden state), LM cannot make the ﬁrst\ntwo facets close to woman and king in one context\nbut make the two facets close to woman and knight\nin another context. In other words, since the facet\nembeddings are the projection of a single hidden\nstate, the total degree of freedom in all facet embed-\ndings cannot exceed the dimension of the hidden\nstate.\nOur solution to this issue is using more in-\nput hidden states to construct the facets. As\nthe orange box in Figure 2, we ﬁrst concate-\nnate a W ×H block of input hidden states into\n⊕i=0...W−1,m=0...H−1hM−m\nct−i , where M−mis the\ntransformer layer index and t−iis the index of the\nith to the last word in the context. The W ×H is\nﬁxed as 3×3 in this paper. We make its dimension\nthe same as the original hidden state hM\nct using\na linear layer Lh plus a GELU activation func-\ntion (Hendrycks and Gimpel, 2016). Then, we con-\ncatenate it with the original hidden state to form a\nnew input hidden state\nqct = hM\nct ⊕GELU\n(\nLh(⊕i,mhM−m\nct−i )\n)\n. (3)\nThe new input hidden state is passed through the\nlinear transformation Lf\nk to compute the facets\nfct,k = Lf\nk(qct) and our prior weights πct,k =\nexp(Lπ\nk(qct))∑\nk′exp(Lπ\nk′(qct)) . Since the dimension of qct is\nlarger than the dimension of fct,k, the inverse func-\ntion (Lf\nk)−1 no longer exists.\n3.3 Multiple Partitions\nThe next word distribution could have many modes.\nHowever, using many softmaxes signiﬁcantly in-\ncreases our computational burden because we need\nto compute the dot product between each facet and\nall the word embeddings in our vocabulary.\nInspired by our analysis, we propose to split all\nthe words in the vocabulary into multiple parti-\ntions4 and use different facets for different parti-\ntions. For example, if we can put any word from\n{queen, man, woman, king} into one partition and\nthe rest of the words into another partition, we no\nlonger have queen - king = woman - man in ei-\nther of the partitions. In this method, each word\nonly belongs to one partition, so we only need to\ncompute one dot product for each word. Thus, the\nextra computational cost only comes from the extra\nlinear projections for preparing the facets.\nIn many contexts ct, the distribution of the next\nword has only a single mode and the global sim-\nilarity between words may be useful. Using the\nmultiple partitions alone might lose the similar-\nity information between words in different parti-\ntions. Therefore, we propose to only replace the\nﬁrst softmax layer in MoS with the multiple parti-\ntion method to learn the global similarity of words\nin different partitions using the other softmaxes.\nThe architecture is illustrated in Figure 2 (d). For-\nmally, we compute the probability using\nPMP(x|ct) =πct,1\nexp((fjx\nct,1)Twx)\n∑\nx′exp((fjx′\nct,1)Twx′)\n+\nK∑\nk=2\nπct,k\nexp(fT\nct,kwx)\n∑\nx′exp(fT\nct,kwx′), (4)\nwhere jx is the partition index that the word xbe-\nlongs to and fjx\nct,1 is the facet for the jxth partition.\n4In this work, we simply put the J × n+ jth word into\njth partition (e.g., when the number of partitions J = 4,\nthe ﬁrst partition includes the words with indexes 0,4,8,... ).\nThis simple global partitioning method reduces the chance of\nputting all the interfering words and candidates in the same\npartition, while minimizing the extra computational cost in our\nPyTorch implementation because PyTorch supports strided\nindex slicing without copying the variable.\n8052\nMulti-facet softmax (MFS) is equipped with multi-\nple input hidden states and multiple partitions.\n4 Language Modeling Experiments\nWe evaluate different LM architectures by compar-\ning their capability of predicting the next word\nin Wikipedia 2021 and a subset of OpenWeb-\nText (Radford et al., 2019). In addition to perplex-\nity, we also compare their mean reciprocal ranks\n(MRR) in Appendix C.1. The size of the training,\nvalidation, and testing set are 96%, 2%, and 2%\nof the whole corpus, respectively. After loading\nthe pre-trained GPT-2 models, we train the GPT-\n2 Small for 1 epoch and GPT-2 Medium for 0.4\nepochs. We also test our methods on BERT in\nAppendix B.2. Please see Appendix G for more\ndetails of our experiment setup.\n4.1 Baselines\nWe set different numbers of softmaxes, input hid-\nden states, and partitions in our MFS framework\nto construct our baselines. The conﬁguration of\ndifferent baselines could be seen in Table 1.\nSoftmax (GPT-2): Using a single softmax, in-\nput hidden state, and partition as in Figure 2 (a)\nand Equation 1. The baseline is the same as the\noriginal GPT-2 except that we add one more linear\nlayer that converts the hidden state hM\nct to the facet\nembedding fct,1 as in other methods.\nSigSoftmax (Kanai et al., 2018): The same as\nSoftmax except when predicting the next word,\nKanai et al. (2018) add some non-linearity into\nthe softmax layer by multiplying the exponent and\nsigmoid of the logits.\nSoftmax + Multi-input: Letting Softmax access\nmultiple input hidden states as in Figure 2 (b) and\nEquation 3. The method is similar to Tenney et al.\n(2019); Fan et al. (2020), and Tay et al. (2021).\nMoS (Yang et al., 2018): MoS (3) is the mixture\nof softmax with 3 facets/softmaxes, whose prob-\nability comes from Equation 2. We also run the\nMoS with 4 softmaxes in GPT-2 Small and call the\nmodel MoS (4).\nDOC (Takase et al., 2018): Similar to our en-\nhancement using multiple input hidden states, di-\nrect output connection (DOC) makes each of their\nfacets coming from a different input hidden state.\nOther conﬁgurations include Softmax + Multi-\npartition, which adds four partitions into the soft-\nmax, MFS w/o Multi-partition, which uses only\none partition in MFS and could also be viewed\nas MoS + Multi-input, and MFS w/o Multi-input,\nwhich uses only one input hidden state to generate\nall facets.\n4.2 Results\nTable 1 shows that applying MFS to GPT-2 Small\nachieves more than 15% of the perplexity improve-\nment between GPT-2 Small and GPT-2 Medium,\nwhile only increasing 5% of their size differences.\nExcept for Softmax + Multi-partition, adding\nmultiple input hidden states or partitions in dif-\nferent conﬁgurations signiﬁcantly boost the perfor-\nmances. In Appendix B.3, we further show that the\nimprovement of MFS over Softmax could even\nbecome 3-5 times larger in the top 5-10% of the\nmost ambiguous contexts compared to the rest of\nthe contexts, which suggests that some improve-\nments indeed come from successfully modeling\nmultimodal distribution.\nMFS usually doubles the perplexity improve-\nments between MoS (3)and Softmax but the run-\nning time of MFS remains similar to MoS (3)be-\ncause MFS only needs a few more linear layers,\nwhich is more efﬁcient than adding one more soft-\nmax as in MoS (4). DOC is worse than MoS (3).\nThis may be due to a starvation problem: the facet\nfrom the last hidden state hM\nct has the prior proba-\nbility close to 1 and receives most of the gradients.\nFinally, compared with Softmax, the mixed results\nin SigSoftmax suggest that adding non-linearity\ninto the softmax layer without modeling the mul-\ntimodal distribution might not always improve the\nmodels (Parthiban et al., 2021).\nOpenWebText is mostly composed of English\ntext, but some non-English text in the corpus al-\nlows us to compare the capability of different mod-\nels in a multi-lingual setting. Table 2 shows that\nmultiple embeddings improve the perplexity of the\nnon-English text more than the perplexity of the\nEnglish text. We hypothesize that the distribution\nof the next non-English word is more likely to be\nmulti-mode because GPT-2 learns the global token\nembeddings mostly in the English contexts, which\ncould make the embeddings of similar tokens in\nnon-English contexts far away.\nIn Table 3, we present three contexts from the\nvalidation set of different datasets and compare the\ntop three predictions ofMFS and Softmax on GPT-\n2 Small. In OpenWebText and Wikipedia 2021, we\ncan see that Softmax misses the correct answer in\nits top three predictions.\n8053\nConﬁguration GPT-2 Small GPT-2 Medium\nModels↓ #S #I #P Size Time OWT Wiki Size Time OWT Wiki\nSoftmax (GPT-2) 1 1 1 163.6M 84ms 18.72 24.06 407.3M 212ms 15.89 20.34\nSigSoftmax (Kanai et al., 2018)1 1 1 163.6M 91ms 18.63 24.06 407.3M 221ms 16.07 20.65\nSoftmax + Multi-input 1 9 1 169.5M 87ms 18.50 23.89 417.8M 219ms 15.76 20.29\nSoftmax + Multi-partition1 1 4 165.4M 88ms 18.77 24.08 410.5M 218ms 15.89 20.30\nMoS (Yang et al., 2018) (4)4 1 1 165.4M 152ms 18.61 23.77 410.5M 299ms 15.75 20.08\nMoS (Yang et al., 2018) (3)3 1 1 164.8M 130ms 18.63 23.81 409.4M 270ms 15.79 20.11\nDOC (Takase et al., 2018)3 3 1 164.8M 130ms 18.69 24.02 409.4M 270ms 15.88 20.34\nMFS w/o Multi-partition 3 9 1 171.9M 133ms 18.37 23.56 422.0M 276ms 15.65 20.06\nMFS w/o Multi-input 3 1 4 166.6M 134ms 18.60 23.72 412.6M 275ms 15.71 20.08\nMFS (Ours) 3 9 4 175.4M 138ms 18.29 23.45 428.3M 283ms 15.64 20.02\nTable 1: Perplexity comparison between MFS (Ours) and baselines. #S, #I, #P are the number of softmaxes (i.e.,\nK), input hidden states, and partitions, respectively. The top four baselines use a single softmax. OWT and Wiki\nare the test set perplexity of OpenWebText and Wikipedia 2021, respectively. The standard errors of all models are\nsmaller than 0.02 perplexity. We also compare the number of parameters and the inference time on one batch.\nNon-English English\nRatio in Corpus→ 14% 86%\nSoftmax 13.50 (0.0%) 19.23 (0.0%)\nMoS (Yang et al., 2018) (3)13.19 (2.3%) 19.16 (0.4%)\nMFS w/o Multi-partition12.98 (3.8%) 18.91 (1.7%)\nMFS (Ours) 12.83 (5.0%) 18.83 (2.1%)\nTable 2: Perplexity of the GPT-2 Small in OpenWeb-\nText. The percentages of the perplexity reduction com-\npared to Softmax are presented in the parentheses.\n5 Evaluation on Ambiguous Templates\nWe synthesize a dataset using templates (Ribeiro\net al., 2020) to verify whether the softmax layer in\nthe original GPT-2 really has difﬁculty in learning\nto output the bimodal distribution in Figure 1 and\nwhether the multiple embedding methods could\novercome the problem. First, we collect the four\nwords with semantic analogy relations in Google\nanalogy dataset (Mikolov et al., 2013). Next, we\ninsert two out of the four words into our manually\nwritten templates to form the contexts and the tem-\nplates we used could be found in Appendix G.3.\nFor example, given the context “I went to Paris and\nGermany before, and I love one of the places more,\nwhich is”, the GPT-2 learns to predict eitherParis\nor Germany.\nThe two words can be either the diagonal words\n(e.g., king and woman) or the edge word (e.g., king\nand queen) in the parallelogram. Finally, we create\na dataset with 122k training contexts, 250k vali-\ndation contexts, and 122k testing contexts, where\nthe word pairs in the testing set are unseen in the\ntraining set to see whether the model could learn to\noutput the bimodal distribution in a general way.5\n5The setting is realistic because any related words could\nbecome the next word in some ambiguous contexts and all\nWe load the models pre-trained on OpenWeb-\nText and continue ﬁne-tuning the models on the\nlast word of each sentence for 10 epochs. We re-\nport the testing performances of the best model\nselected by the validation loss. Since the sets of the\nword pairs in the training and testing set are disjoint,\nupdating the output word embedding would make\nGPT-2 solve the task by memorizing/overﬁtting the\ntraining set quickly and lead to much worse testing\nperformances. Thus, we freeze the output word\nembedding during the training.\nWe visualize the predictions of the Paris-\nGermany example in the last column of Table 3.\nWe can see two of the softmaxes are close to Paris\nand the remaining one is close to German, while\nSoftmax overestimates the probability ofParis and\nranks France higher than the German. The result\nveriﬁes that the correct probability distribution of\nthe words in some ambiguous context is hard to\nlearn using Softmax.\nQuantitatively, Table 4 indicates that when the\npossible next words are the diagonal words, the\nSoftmax model performs much worse compared to\nother multiple embedding alternatives. In the edge\nword dataset, the multiple embedding solutions are\nstill better but have a much smaller gap. MFS\nw/o Multi-partitionslightly improves MoS. We\nhypothesize the reason is that multiple input hidden\nstates could help the facets to be moved more freely.\nFinally, multiple partitions seem to cause slight\noverﬁtting in this bimodal distribution prediction\ntask.\nthe words are related in a certain way (Sigman and Cecchi,\n2002). We cannot expect the training corpora to contain the\nambiguous contexts with so many possible next words.\n8054\nCorpus→ OpenWebText Wikipedia 2021 Analogy in Templates (Section 5)\nInput Context\n... The Elastic Endpoint Security and\nElastic SIEM solutions mentioned in\nthis post are now referred to asElastic\n... law and chance working together\ncannot generate CSI, either. Moreover,\nhe claims thatCSI\nI went to Paris and Germany before, and I\nlove one of the places more, which is\nGermany\nSoftmax (GPT-2)the 0.087, E 0.043, End 0.039 the 0.174, this 0.054, if 0.038 Paris 0.893, France 0.045,Germany0.033\nMFS (Ours) Elastic0.220, the 0.089, EC 0.033 CSI 0.186, the 0.140, there 0.033 Paris 0.544,Germany0.389, France 0.064\nMFS Softmax 1end 0.051, the 0.043, security 0.023 the 0.191, law 0.127, if 0.053 Paris 0.979, France 0.013,Germany0.007\nMFS Softmax 2Elastic0.652, EC 0.080, ES 0.046 the 0.191, there 0.049, this 0.047 Paris 1.000 Berlin 0.000 ##Paris 0.000\nMFS Softmax 3 the 0.193, E 0.040, a 0.014CSI0.677, law 0.029, laws 0.019Germany0.852, France 0.139, China 0.004\nTable 3: Prediction visualization using a context in each dataset. We show the top three words with the highest\nprediction probabilities of each method. In the last three rows, we visualize the outputs of the softmax grey boxes\nin Figure 2 (d), which model different modes of the next word distribution. The prediction target is boldfaced in\nthe context and the predictions. ## indicates there is no space before the word.\nDiagonal (e.g.,kingorwoman) Edge (e.g.,kingorqueen)\nAnalogy Relation Types→ capital- capital- city-in- family capital- capital- city-in- familyModels↓ valid common world state valid common world state\nSoftmax (GPT-2) 2.30 3.30 2.00 2.25 2.95 2.11 2.42 1.91 2.26 2.38\nMoS (Yang et al., 2018) (3)1.75 2.18 1.60 1.85 2.82 1.87 2.26 1.70 2.04 2.27\nMFS w/o Multi-partition1.72 2.13 1.59 1.82 2.52 1.84 2.23 1.72 1.96 2.16\nMFS (Ours) 1.74 2.15 1.59 1.82 2.63 1.92 2.28 1.78 2.00 2.24\nTable 4: Perplexity comparison of different GPT-2 Small models on the words with different types of analogy\nrelations. The validation set (valid) includes all four types of relations.\nPerplexity on Scraped Max Answers Max Incorrect\nModels↓ Development SetTop 1 Top 3 Top 5 Top 10 Top 1 Top 3 Top 5\nSoftmax (GPT-2) 1.5432±0.0003 34.1±0.8 35.2±0.5 37.8±0.4 45.0±0.5 18.3±0.4 30.7±0.5 38.5±0.6\nMoS (Yang et al., 2018) (3)1.5407±0.0004 33.9±0.8 36.0±0.6 37.7±0.6 44.9±0.4 18.3±0.4 31.7±0.6 38.2±0.6\nMFS w/o Multi-partition1.5411±0.0003 34.3±0.7 36.7±0.7 38.1±0.5 45.2±0.4 19.4±0.4 32.0±0.5 38.6±0.3\nMFS (Ours) 1.5402±0.0005 34.1±0.6 36.7±0.5 38.6±0.4 45.4±0.5 19.7±0.4 32.1±0.4 39.7±0.4\nTable 5: ProtoQA performances. All the numbers except perplexity are the percentages of the predictions that\nmatch the ground truth exactly on the crowdsourced development set. Max answers top k implies only evaluating\nthe top k answers. Max incorrect top k indicates only evaluating the top answers that contain k errors. The best\naverage performances are highlighted and the standard errors are reported as the conﬁdence interval.\n6 Answering Ambiguous Questions\nProtoQA (Boratko et al., 2020) is a question-\nanswering dataset built for evaluating the common-\nsense reasoning ability of language models. Each\nquestion in ProtoQA is ambiguous and leads to a\ndistribution of possible answers. For instance, the\nanswer to “Name something that people usually\ndo before they leave for work?” is “Shower 0.43,\nBreakfast 0.30, ...”. The paper discovers that by\nreformulating the question-answering task as a con-\ntext (e.g., “One thing people usually do before they\nleave for work is ... ”), GPT-2 could generate the\npossible answers by sampling the next words from\nits word prediction distribution.\nThe dataset gives us a chance to directly com-\npare the quality of the distributions generated by\ndifferent LMs in Table 5. After pretraining GPT-2\nMedium on the OpenWebText, we ﬁne-tune them\nusing the training data in ProtoQA for 2 epochs.\nWe repeat the ﬁne-tuning 5 times and compare their\naverage perplexity in our validation set. Next, we\ngenerate 150 sentences starting from each context\nand compare the generated answers with the ground\ntruth distribution. For each ﬁne-tuned model, we\nrepeat the generation evaluation 3 times and report\nthe average accuracy of the resulting 15 trials.\nWe can see that the multiple softmaxes, input\nhidden states, and partitions usually improve the\nquality of prediction distribution, and the proposed\nMFS, which combines all modiﬁcations, achieves\nthe best performances.\n7 Related Work\nYang et al. (2018) propose the concept of softmax\nbottleneck, which points out that the dot product in\nthe softmax layer restricts the representation power\nof outputting arbitrary conditional probabilities. It\nalso proposes MoS to break the softmax bottle-\nneck in an RNN-based LM. Kanai et al. (2018)\nand Ganea et al. (2019) add nonlinearities into the\nsoftmax layer to break the bottleneck more efﬁ-\n8055\nciently, but the approaches gain less improvement\ncompared to MoS.\nA limitation of the aforementioned previous\nwork is that they do not tell us which kinds of sen-\ntences would be affected by the bottleneck more\nand whether the order of the top few next words\nwould be affected, which are the main research\nquestions of our work. Contrary to the previous\nbelief that a large hidden state dimension would\neliminate the softmax bottleneck, our theorems sug-\ngest that some words in a low dimensional sub-\nspace could still make the single embedding in the\nsoftmax layer become a bottleneck of arbitrarily\nranking the output words. Furthermore, our geo-\nmetric analyses provide an intuitive explanation\nabout why breaking the bottleneck using multiple\nembeddings leads to better performances compared\nto only adding the non-linearity.\nDemeter et al. (2020) also analyze the structural\nweakness of the softmax layer from a geometric\nperspective. They discover that the words with\nhigh prior frequencies could stop the LMs from as-\nsigning the high probabilities to rare words, which\ncan be viewed as a special case of our theory (See\nAppendix E). For instance, our work shows that\nthe softmax layer could still prevent the LMs from\noutputting some top words even if all the possible\nnext words have the same prior frequency.\nOur theory is deeply connected to the mathe-\nmatical work that counts the number of possible\nrankings of points in an embedding space (Cover,\n1967; Good and Tideman, 1977). Compared to\nthe studies, our work focuses more on analyzing\nthe multimodal distribution in the word embedding\nspace and its implication to language models.\nAn alternative to model the multimodal distri-\nbution is to use multiple embeddings to represent\neach output word (Athiwaratkun and Wilson, 2017;\nMiao et al., 2019). Compared to MoS or our ap-\nproach that use multiple embeddings to represent\neach hidden state of the context, their method re-\nquires many extra parameters to store different\nsenses of each output word. Another type of re-\nlated model (Shazeer et al., 2017; Fedus et al.,\n2021) dynamically routes the signals to different\nexperts (i.e., feed-forward networks) and Zhang\net al. (2022); Mittal et al. (2022) use multiple em-\nbeddings in the attention layers. The methods are\nsimilar to MoS and our approach, but they add\nthe multiple embeddings inside each layer of the\ntransformer encoder while the proposed MFS is an\nalternative to the output softmax layer.\n8 Conclusion\nWhen the ideal distribution in the output word em-\nbedding space has multiple modes, GPT-2 cannot\nlearn to correctly rank the words in all the modes as\nthe top next words. This shows that the single em-\nbedding in the softmax layer, which is used nearly\nuniversally by current LMs, constitutes a perfor-\nmance upper bound of predicting the next/masked\nword. To address the systematic failure caused\nby these structural weaknesses, we propose multi-\nfacet softmax (MFS). In our experiments, we con-\nﬁrm that the MFS signiﬁcantly outperforms the\nstandard softmax layer and alleviates the softmax\nbottleneck in the transformer-based LMs such as\nGPT-2 better than mixture of softmax (MoS).\n9 Acknowledgement\nWe thank Michael Boratko, Jay Yoon Lee, Sabrina\nJ. Mielke, Steve Cheng-Xian Li, and the anony-\nmous reviewers for their constructive feedback.\nThis work was supported in part by the Center for\nData Science and the Center for Intelligent Infor-\nmation Retrieval, in part by the Chan Zuckerberg\nInitiative under the project Scientiﬁc Knowledge\nBase Construction, in part by the IBM Research\nAI through the AI Horizons Network, in part using\nhigh performance computing equipment obtained\nunder a grant from the Collaborative R&D Fund\nmanaged by the Massachusetts Technology Collab-\norative, in part by the National Science Foundation\n(NSF) grant numbers DMR-1534431, IIS-1763618,\nand IIS-1955567, and in part by the Ofﬁce of Naval\nResearch (ONR) via Contract No. N660011924032\nunder Subaward No. 123875727 from the Univer-\nsity of Southern California. Any opinions, ﬁndings,\nconclusions, or recommendations expressed in this\nmaterial are those of the authors and do not neces-\nsarily reﬂect those of the sponsor.\n10 Ethical and Broader Impact\nThis work studies a general limitation of LMs and\nproposes solutions. The proposed theory can help\nus to understand that some types of hallucinations,\nmistakes, or biases of LMs could come from soft-\nmax bottleneck and their incapability of modeling\nthe correct distribution. For example, there are 60%\nof male characters and 40% of female characters\nin our training corpus. The language generation\nmodel might be forced to assign more than 60%\n8056\nprobability to male characters as being much more\nlikely to output king than woman in Figure 1.\nRecently, Narang et al. (2021); Anonymous\n(2021) show that MoS is one of the few architecture\nmodiﬁcations of transformer-based LM that can\nprovide consistent improvements in downstream\napplications. Our work provides a fundamental\nreason why the multiple embedding representation\nis better, which could inspire more future studies\nthat propose a better multiple-embedding architec-\nture to improve LMs (e.g., multi-lingual BERT)\nor downstream applications. As examples, we list\nseveral possible future directions in Appendix H.\nFinally, a better LM could lead to both positive\nand negative societal impacts, but they are not the\nfocus of this paper. Generally speaking, this paper\ndeepens our understanding of the weaknesses of\nmodern LMs and we believe the knowledge can\nhelp us to design a better LM that increases the\npositive impacts and reduces the negative impacts\nin the future.\nReferences\nNader Akoury, Shufan Wang, Josh Whiting, Stephen\nHood, Nanyun Peng, and Mohit Iyyer. 2020. STO-\nRIUM: A Dataset and Evaluation Platform for\nMachine-in-the-Loop Story Generation. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n6470–6484, Online. Association for Computational\nLinguistics.\nAnonymous. 2021. Scaling laws vs model architec-\ntures: How does inductive bias inﬂuence scaling? an\nextensive empirical study on language tasks. InACL\nARR Blind Submission.\nBen Athiwaratkun and Andrew Wilson. 2017. Mul-\ntimodal word distributions. In Proceedings of the\n55th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers) ,\npages 1645–1656, Vancouver, Canada. Association\nfor Computational Linguistics.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nChristopher M Bishop. 1994. Mixture density net-\nworks.\nMichael Boratko, Xiang Li, Tim O’Gorman, Rajarshi\nDas, Dan Le, and Andrew McCallum. 2020. Pro-\ntoQA: A question answering dataset for prototypi-\ncal common-sense reasoning. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 1122–1136,\nOnline. Association for Computational Linguistics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. In Advances in Neural Information Processing\nSystems 33: Annual Conference on Neural Informa-\ntion Processing Systems 2020, NeurIPS 2020, De-\ncember 6-12, 2020, virtual.\nXingyu Cai, Jiaji Huang, Yuchen Bian, and Kenneth\nChurch. 2021. Isotropy in the contextual embed-\nding space: Clusters and manifolds. In 9th Inter-\nnational Conference on Learning Representations,\nICLR 2021, Virtual Event, Austria, May 3-7, 2021 .\nOpenReview.net.\nHyung Won Chung, Thibault Févry, Henry Tsai,\nMelvin Johnson, and Sebastian Ruder. 2021. Re-\nthinking embedding coupling in pre-trained lan-\nguage models. In ICLR.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavar-\nian, Jacob Hilton, Reiichiro Nakano, Christopher\nHesse, and John Schulman. 2021. Training veri-\nﬁers to solve math word problems. arXiv preprint\narXiv:2110.14168.\nThomas M Cover. 1967. The number of linearly in-\nducible orderings of points in d-space. SIAM Jour-\nnal on Applied Mathematics, 15(2):434–439.\nDavid Demeter, Gregory Kimmel, and Doug Downey.\n2020. Stolen probability: A structural weakness\nof neural language models. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 2191–2197, Online. As-\nsociation for Computational Linguistics.\nKawin Ethayarajh, David Duvenaud, and Graeme Hirst.\n2019. Towards understanding linear word analo-\ngies. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics ,\npages 3253–3262, Florence, Italy. Association for\nComputational Linguistics.\nAngela Fan, Thibaut Lavril, Edouard Grave, Armand\nJoulin, and Sainbayar Sukhbaatar. 2020. Address-\ning some limitations of transformers with feedback\nmemory. arXiv preprint arXiv:2002.09402.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2021.\nSwitch transformers: Scaling to trillion parameter\nmodels with simple and efﬁcient sparsity. arXiv\npreprint arXiv:2101.03961.\nOctavian Ganea, Sylvain Gelly, Gary Bécigneul, and\nAliaksei Severyn. 2019. Breaking the softmax\nbottleneck via learnable monotonic pointwise non-\nlinearities. In Proceedings of the 36th International\n8057\nConference on Machine Learning, ICML 2019, 9-\n15 June 2019, Long Beach, California, USA , vol-\nume 97 of Proceedings of Machine Learning Re-\nsearch, pages 2073–2082. PMLR.\nJun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-\nYan Liu. 2019a. Representation degeneration prob-\nlem in training natural language generation models.\nIn 7th International Conference on Learning Repre-\nsentations, ICLR 2019, New Orleans, LA, USA, May\n6-9, 2019. OpenReview.net.\nYingbo Gao, Christian Herold, Weiyue Wang, and Her-\nmann Ney. 2019b. Exploring kernel functions in the\nsoftmax layer for contextual word classiﬁcation. In\nProceedings of the 16th International Conference on\nSpoken Language Translation, Hong Kong.\nIJ Good and TN Tideman. 1977. Stirling numbers and\na geometric, structure from voting theory. Journal\nof Combinatorial Theory, Series A, 23(1):34–45.\nDan Hendrycks and Kevin Gimpel. 2016. Gaus-\nsian error linear units (gelus). arXiv preprint\narXiv:1606.08415.\nSepp Hochreiter and Jürgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text\ndegeneration. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nSekitoshi Kanai, Yasuhiro Fujiwara, Yuki Yamanaka,\nand Shuichi Adachi. 2018. Sigsoftmax: Reanalysis\nof the softmax bottleneck. In Advances in Neural\nInformation Processing Systems 31: Annual Con-\nference on Neural Information Processing Systems\n2018, NeurIPS 2018, December 3-8, 2018, Mon-\ntréal, Canada, pages 284–294.\nJared Kaplan, Sam McCandlish, Tom Henighan,\nTom B Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei.\n2020. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361.\nLingpeng Kong, Cyprien de Masson d’Autume, Lei\nYu, Wang Ling, Zihang Dai, and Dani Yogatama.\n2020. A mutual information maximization perspec-\ntive of language representation learning. In 8th\nInternational Conference on Learning Representa-\ntions, ICLR 2020, Addis Ababa, Ethiopia, April 26-\n30, 2020. OpenReview.net.\nBohan Li, Hao Zhou, Junxian He, Mingxuan Wang,\nYiming Yang, and Lei Li. 2020. On the sentence\nembeddings from pre-trained language models. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9119–9130, Online. Association for Computa-\ntional Linguistics.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman,\nJulian Schrittwieser, Rémi Leblond, Tom Eccles,\nJames Keeling, Felix Gimeno, Agustin Dal Lago,\nThomas Hubert, Peter Choy, Cyprien de Mas-\nson d’Autume, Igor Babuschkin, Xinyun Chen, Po-\nSen Huang, Johannes Welbl, Sven Gowal, Alexey\nCherepanov, James Molloy, Daniel Mankowitz,\nEsme Sutherland Robson, Pushmeet Kohli, Nando\nde Freitas, Koray Kavukcuoglu, and Oriol Vinyals.\n2022. Competition-level code generation with al-\nphacode. arXiv preprint arXiv:2203.07814.\nAlisa Liu, Swabha Swayamdipta, Noah A Smith, and\nYejin Choi. 2022. Wanli: Worker and ai collabora-\ntion for natural language inference dataset creation.\narXiv preprint arXiv:2201.05955.\nIlya Loshchilov and Frank Hutter. 2019. Decou-\npled weight decay regularization. In 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019 .\nOpenReview.net.\nNing Miao, Hao Zhou, Chengqi Zhao, Wenxian Shi,\nand Lei Li. 2019. Kernelized bayesian softmax\nfor text generation. In Advances in Neural Infor-\nmation Processing Systems 32: Annual Conference\non Neural Information Processing Systems 2019,\nNeurIPS 2019, December 8-14, 2019, Vancouver,\nBC, Canada, pages 12487–12497.\nTomás Mikolov, Ilya Sutskever, Kai Chen, Gregory S.\nCorrado, and Jeffrey Dean. 2013. Distributed rep-\nresentations of words and phrases and their com-\npositionality. In Advances in Neural Information\nProcessing Systems 26: 27th Annual Conference on\nNeural Information Processing Systems 2013. Pro-\nceedings of a meeting held December 5-8, 2013,\nLake Tahoe, Nevada, United States , pages 3111–\n3119.\nGeorge A. Miller. 1992. WordNet: A lexical database\nfor English. In Speech and Natural Language: Pro-\nceedings of a Workshop Held at Harriman, New\nYork, February 23-26, 1992.\nSarthak Mittal, Sharath Chandra Raparthy, Irina Rish,\nYoshua Bengio, and Guillaume Lajoie. 2022. Com-\npositional attention: Disentangling search and re-\ntrieval. In International Conference on Learning\nRepresentations, ICLR.\nSharan Narang, Hyung Won Chung, Yi Tay, William\nFedus, Thibault Fevry, Michael Matena, Karishma\nMalkan, Noah Fiedel, Noam Shazeer, Zhenzhong\nLan, et al. 2021. Do transformer modiﬁcations trans-\nfer across implementations and applications? arXiv\npreprint arXiv:2102.11972.\nDwarak Govind Parthiban, Yongyi Mao, and Diana\nInkpen. 2021. On the softmax bottleneck of recur-\nrent language models. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence , volume 35,\npages 13640–13647.\n8058\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\nthe limits of transfer learning with a uniﬁed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch, 21(140):1–67.\nSara Rajaee and Mohammad Taher Pilehvar. 2021. A\ncluster-based approach for improving isotropy in\ncontextual embedding space. In Proceedings of the\n59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing,\nACL/IJCNLP 2021, (Volume 2: Short Papers), Vir-\ntual Event, August 1-6, 2021, pages 575–584. Asso-\nciation for Computational Linguistics.\nMarco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin,\nand Sameer Singh. 2020. Beyond accuracy: Be-\nhavioral testing of NLP models with CheckList. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4902–\n4912, Online. Association for Computational Lin-\nguistics.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,\nAndy Davis, Quoc V . Le, Geoffrey E. Hinton, and\nJeff Dean. 2017. Outrageously large neural net-\nworks: The sparsely-gated mixture-of-experts layer.\nIn 5th International Conference on Learning Rep-\nresentations, ICLR 2017, Toulon, France, April 24-\n26, 2017, Conference Track Proceedings . OpenRe-\nview.net.\nVered Shwartz, Enrico Santus, and Dominik\nSchlechtweg. 2017. Hypernyms under siege:\nLinguistically-motivated artillery for hypernymy\ndetection. In Proceedings of the 15th Conference\nof the European Chapter of the Association for\nComputational Linguistics: Volume 1, Long Papers,\npages 65–75, Valencia, Spain. Association for\nComputational Linguistics.\nMariano Sigman and Guillermo A Cecchi. 2002.\nGlobal organization of the wordnet lexicon. Pro-\nceedings of the National Academy of Sciences ,\n99(3):1742–1747.\nYixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Ling-\npeng Kong, and Nigel Collier. 2022. A contrastive\nframework for neural text generation. arXiv preprint\narXiv:2202.06417.\nSho Takase, Jun Suzuki, and Masaaki Nagata. 2018.\nDirect output connection for a high-rank language\nmodel. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 4599–4609, Brussels, Belgium. Association\nfor Computational Linguistics.\nYi Tay, Mostafa Dehghani, Vamsi Aribandi, Jai Gupta,\nPhilip Pham, Zhen Qin, Dara Bahri, Da-Cheng Juan,\nand Donald Metzler. 2021. Omninet: Omnidi-\nrectional representations from transformers. arXiv\npreprint arXiv:2103.01075.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R. Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Samuel R. Bowman, Dipan-\njan Das, and Ellie Pavlick. 2019. What do you\nlearn from context? probing for sentence structure\nin contextualized word representations. In 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019 .\nOpenReview.net.\nBen Wang and Aran Komatsuzaki. 2021. Gpt-j-6b: A\n6 billion parameter autoregressive language model.\nYile Wang, Leyang Cui, and Yue Zhang. 2019. How\ncan bert help lexical semantics tasks? arXiv preprint\narXiv:1911.02929.\nPeter West, Chandra Bhagavatula, Jack Hessel, Jena D\nHwang, Liwei Jiang, Ronan Le Bras, Ximing\nLu, Sean Welleck, and Yejin Choi. 2021. Sym-\nbolic knowledge distillation: from general language\nmodels to commonsense models. arXiv preprint\narXiv:2110.07178.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nZhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and\nWilliam W. Cohen. 2018. Breaking the softmax bot-\ntleneck: A high-rank RNN language model. In 6th\nInternational Conference on Learning Representa-\ntions, ICLR 2018, Vancouver, BC, Canada, April 30\n- May 3, 2018, Conference Track Proceedings. Open-\nReview.net.\nZhilin Yang, Thang Luong, Ruslan Salakhutdinov, and\nQuoc V . Le. 2019. Mixtape: Breaking the soft-\nmax bottleneck efﬁciently. In Advances in Neural\nInformation Processing Systems 32: Annual Con-\nference on Neural Information Processing Systems\n2019, NeurIPS 2019, December 8-14, 2019, Vancou-\nver, BC, Canada, pages 15922–15930.\nAvishai Zagoury, Einat Minkov, Idan Szpektor, and\nWilliam W Cohen. 2021. What’s the best place for\nan ai conference, vancouver or _: Why completing\ncomparative questions is difﬁcult. In Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence , vol-\nume 35, pages 14292–14300.\n8059\nZhong Zhang, Nian Shao, Chongming Gao, Rui Miao,\nQinli Yang, and Junming Shao. 2022. Mixhead:\nBreaking the low-rank bottleneck in multi-head at-\ntention language models. Knowledge-Based Sys-\ntems, page 108075.\n8060\nA Appendix Overview\nTo demonstrate the wide applicability of our ap-\nproaches, we conduct more experiments such as\napplying MFS to BERT in Appendix B. We also\nshow more results and conduct more analyses in\nAppendix C to further support our conclusions.\nNext, we provide technical details including the\nproof of our theorems in Appendix D, show that the\nstructure weakness studied by Demeter et al. (2020)\nis a special case of our theory in Appendix E, the\nmethod details in Appendix F, and the experiment\ndetails in Appendix G. Finally, in Appendix H, we\nlist several directions that could be further studied\nin the future.\nB More Experiments\nWe conduct the following ﬁve extra experiments to\nmeasure the linear dependency among word embed-\ndings in LMs, extend our multi-facet approaches\nto BERT, conﬁrm the source of the improvement\ncomes from modeling multimodal distribution, and\nextend our synthetic experiments to include the\noutput candidate words that have various types of\nrelations and to include the template that favors the\nsingle embedding representation.\nB.1 Linear Dependency among Words\nTheorem 2 shows that when N words are linearly\ndependent after moving one of the embeddings\nwith a short distance ϵ, the output softmax layer of\na LM cannot output a large logit margin between\ntwo subsets of the N words. We want to measure\nϵin the pretrained word embedding and compare\nthe ϵfrom different sets of words or from different\nLMs.\nGiven a set of N words, we form a matrix by\ntheir word embeddings and estimate the ϵ value\nby the minimal eigenvalue of the matrix. We ﬁrst\nwant to verify that the four analogical words used\nin Section 5 indeed have a smaller ϵcompared to a\nrandomly selected four words. Thus, we deﬁne the\nmin eigenvalue ratio as ϵS\nϵR\n, where ϵR is the aver-\nage of minimal eigenvalues from 1,000 sampled N\nword sets and ϵS is the average of minimal eigen-\nvalues from sets of words (e.g., analogical words\nfrom the Google analogy dataset). We analyze the\nratio instead of ϵbecause the average word embed-\nding magnitudes in different LMs would affect the\nabsolute value of ϵ.\nIn addition to analogical words, we also test sets\nof N similar words, which are composed by the\nnearest N −1 words of every query word in the\nvocabulary, and test the N similar stop words by\nﬁnding the nearest N −1 words of every query\nword in a stop word list.6\nWe plot the min eigenvalue ratio versusNin Fig-\nure 3 and compare the curves from three GPT LMs\nand two T5 LMs (Raffel et al., 2020). All the ratios\nare below 0 and decrease as N increases, which\nshows the analogical words and similar words in-\ndeed have signiﬁcantly smaller ϵespecially for a\nlarge N. The low minimal eigenvalues and our\ntheory support the recent empirical ﬁnds that LMs\ntend to be confused by the similar words (Zagoury\net al., 2021). This ﬁgure also provides a potential\nexplanation why the candidates often include stop\nwords when multiple embeddings outperform the\nsingle embedding in Table 3 and Table 7.\nSurprisingly, we ﬁnd that a larger LM does not\nnecessarily yield a larger ratio (i.e., embeddings\nof related words do not become more linearly in-\ndependent as dimension or the size of the LM in-\ncreases). All the LMs have very similar ratios of\nsimilar stop words. Compared to GPT-small, al-\nthough GPT-J-6B (Wang and Komatsuzaki, 2021)\nhas a signiﬁcantly higher ratio for analogical words,\nits ratio for similar words is signiﬁcantly lower. Be-\nsides, T5-11B has signiﬁcantly lower ratios com-\npared to T5-small. We need further investigation to\nunderstand the reason for this empirical ﬁnding and\nwhether a larger LM suffers less from the limitation\ncaused by the single embedding.\nB.2 Language Modeling using BERT\nTo demonstrate that our proposed method could\nimprove the LMs other than GPT-2, we applymulti-\nfacet softmax, MFS, to BERT. We test the model on\nWikipedia 2021 and the validation size is 0.25% of\nthe whole corpus. After loading pretrained model,\nwe train bert_base_cased for 100k batches and\nbert_large_cased for 30k batches.\nThe results are presented in Table 6. First, MoS\noutperforms Softmax on BERT. The results sup-\nport the ﬁnding of Narang et al. (2021) that the\nsoftmax bottleneck not only exists in the next word\nprediction tasks but also in the masked word predic-\ntion tasks. Similar to GPT-2, MFS at least doubles\n6We ﬁnd that some rare words or special characters might\nhave nearly identical word embeddings due to the lack of train-\ning instances, so we exclude half of rarer word pieces in the\nvocabulary and exclude the word pieces whose ﬁrst character\nis not a space. The rarity of a word piece is determined by the\nl2 norm of its word embedding.\n8061\n0 5 10 15 20 25\nN\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Min Eigen Value Ratio\nSimilar Words (GPT-2 Small)\nSimilar Words (GPT-2 XL)\nSimilar Words (GPT-J-6B)\nSimilar Stopwords (GPT-2 Small)\nSimilar Stopwords (GPT-2 XL)\nSimilar Stopwords (GPT-J-6B)\nAnalogous Words (GPT-2 Small)\nAnalogous Words (GPT-2 XL)\nAnalogous Words (GPT-J-6B)\n(a) GPT-2 Small (0.1B, D=768), GPT-2 XL (1.5B, D=1600), and\nGPT-J-6B (6B, D=4096)\n0 5 10 15 20 25\nN\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Min Eigen Value Ratio\nSimilar Words (T5 Small)\nSimilar Words (T5 11B)\nSimilar Stopwords (T5 Small)\nSimilar Stopwords (T5 11B)\nAnalogous Words (T5 Small)\nAnalogous Words (T5 11B) (b) T5 Small (0.06B, D=512) and T5 11B (11B, D=1024)\nFigure 3: Minimal eigenvalue ratios to indicate the linear dependency among different groups of N word embed-\ndings\nBERT base after training on 100k batches\nSoftmax (S1I1P1) SigSoftmax (S1I1P1)\n5.8699 5.8749\nSoftmax + Multi-input (S1I9P1) Softmax + Multi-partition (S1I1P4)\n5.8520 5.8656\nMoS (Yang et al., 2018) (4) (S4I1P1) MoS (Yang et al., 2018) (3) (S3I1P1) DOC (Takase et al., 2018) (S3I3P1)\n5.8523 5.8535 5.8547\nMFS w/o Multi-partition (S3I9P1) MFS w/o Multi-input (S3I1P4) MFS (S3I9P4)\n5.8231 5.8536 5.8231\nBERT large after training on 30k batches\nSoftmax (S1I1P1) SigSoftmax (S1I1P1)\n4.8355 4.8354\nSoftmax + Multi-input (S1I9P1) Softmax + Multi-partition (S1I1P4)\n4.8305 4.8363\nMoS (Yang et al., 2018) (4) (S4I1P1) MoS (Yang et al., 2018) (3) (S3I1P1) DOC (Takase et al., 2018) (S3I3P1)\n4.8268 4.8291 4.8231\nMFS w/o Multi-partition (S3I9P1) MFS w/o Multi-input (S3I1P4) MFS (S3I9P4)\n4.8111 4.8287 4.8109\nTable 6: Perplexity of models building on BERT in Wikipedia 2021.\nCorpus→ OpenWebText Wikipedia 2021 Similar Nouns in Templates\nInput Context\n... \"Part of the Clinton inevitability\nstrategy was to lock down the usual\nsuspects in left-liberal policy,\" said\nDan Nexon, a Georgetown professor\nwho served as one of those informal\nSanders advisers.Nex\n... The projective line over the dual\nnumbers was described by Josef\nGrünwald in 1906. This ring includes a\nnonzero nilpotent \"n\" satisfying. The\nplane of dual numbers has aproject\nThere are the militia and the enemy in front of\na woman, and she decides to pursue the\nmilitia\nSoftmax (GPT-2)He 0.014, But 0.011, The 0.007 ﬁnite 0.062, hom 0.059,project0.034 enemy 0.860,militia0.111, Militia 0.005\nMFS (Ours) Nex0.013, He0.012, But0.011 project0.096, hom0.049, dual0.046 enemy0.535,militia0.433, enemies0.029\nMFS Avg \", He, But, The, In, And, (, It hom, dual, ﬁnite, non, \", complex, unitmilitia, enemy, Militia, enemies, militias\nMFS Softmax 1But 0.005, He 0.004, The 0.002project0.201, dual 0.075, ﬁnite 0.030 enemy 0.772,militia0.189, Militia 0.017\nMFS Softmax 2Nex0.260, \" 0.028, He 0.023 hom 0.093, unit 0.040, non 0.037militia0.938, Militia 0.062, militias 0.000\nMFS Softmax 3He 0.025, But 0.022, The 0.014 ﬁnite 0.065, map 0.041, plane 0.030 enemy 1.000, enemies 0.000, foe 0.003\nTable 7: Prediction visualization using a context in each dataset. Each row visualizes a model as in Table 3. The\nmodels are built onGPT-2 Mediumin OpenWebText and Wikipedia and onGPT-2 Smallin the synthesized dataset.\nMFS Avg shows the words that are closest to the average facet embedding inMFS. See the details in Appendix B.3.\nWe underline the words that appear in the top predictions of both MFS and MFS Avg.\nthe improvement of MoS. The most improvement\nover MoS comes from using multiple input hidden\nstates while adding multiple partitions yield a small\nor no improvement. Finally, the improvement be-\ntween MFS and Softmax is around 4.5%, which is\nmuch smaller than 15% in GPT-2.\n8062\nCorpus→ OpenWebText Wikipedia 2021\nImprovement Model S3I9P4 S3I9P4 S3I9P4 S3I9P1 S3I1P1S3I9P4 S3I9P4 S3I9P4 S3I9P1 S3I1P1\nReference Model S3I9P1 S3I1P1 S1I1P1 S1I9P1 S1I1P1S3I9P1 S3I1P1 S1I1P1 S1I9P1 S1I1P1\nMulti-mode Percentage (%)10.03 10.03 10.03 4.81 3.24 5.85 5.85 5.85 2.66 3.05\nMulti-mode Loss Improvement0.0248 0.0474 0.0649 0.0203 0.0110 0.0282 0.0644 0.1000 0.0472 0.0295\nOther Loss Improvement0.0035 0.0158 0.0211 0.0086 0.0064 0.0033 0.0128 0.0219 0.0136 0.0100\nImprovement Ratio 7.01 3.00 3.08 2.34 1.71 8.63 5.04 4.57 3.47 2.94\nTable 8: The loss improvement comparison between the Improvement Models and Reference Models. The models\nare named using their number of softmaxes, input hidden states, and partitions. Thus, S3I9P4 is MFS, S3I9P1\nis MFS w/o Multi-partition, S1I9P1 is Softmax + Multi-input, S3I1P1 is MoS (3), and S1I1P1 is Softmax. Multi-\nmode Percentage is the percentage of the contexts where the Improvement Models output multimodal distribution.\nMulti-mode Loss Improvement refers to the average improvement when Improvement Models outputs multimodal\ndistribution and Other Loss Improvement refers to the improvement of the contexts where the facets of Improve-\nment Models are close to each other. Improvement Ratio divides Multi-mode Loss Improvement by Other Loss\nImprovement.\nproject\nﬁnite\nhom\nmap\ndual\nnon unit\nplane\nfavgct\nfct,1\nfct,2\nfct,3\nMFS \nSoftmax 1\nMFS \nSoftmax 2\nMFS \nSoftmax 3\nMFS Avg\nf1ct,1\nf2ct,1\nf3ct,1 f4ct,1\nFigure 4: Illustration of the MFS predictions given\nthe Wikipedia context in the second column of Ta-\nble 7. The green circles mean the facet embeddings\nfrom MFS. The orange circle is the average of the facet\nembeddings (MFS Avg). The blue circles are the word\nembeddings that are close to the facet embeddings and\nMFS Avg. The word project is highlighted because it\nis the next word in our ground truth.\nThe smaller improvement supports the conclu-\nsion of our geometric analyses that the multi-mode\nambiguity intensiﬁes the softmax bottleneck. We\nonly observe the one-directional context before\nthe next target word in GPT-2, but we can ob-\nserve the bi-directional context surrounding the\nmasked target word in BERT. Thus, compared to\nnext word prediction, the multi-mode ambiguity of\nthe masked word prediction occurs less frequently\nwhen the masking probability is small (e.g., 15%\nin BERT). Since the masked word distribution only\nhas a single mode most of the time but we some-\ntimes still want the distribution to have multiple\nmodes, multiple input hidden states can improve\nthe performance by helping the facets to move more\nfreely. On the other hand, multiple partitions are\nless useful because the distribution rarely has more\nthan three modes.\nB.3 Analysis of Improvement on Multimodal\nDistribution\nTo conﬁrm that the perplexity improvements ac-\ntually come from modeling the multimodal distri-\nbution, we deﬁne a metric to measure how multi-\nmode a distribution is, and then we can compare\nthe perplexity improvement from multimodal distri-\nbutions and the improvement from the distributions\nthat are close to a single-mode distribution.\nFor the method with multiple embeddings, we\nﬁrst compute the weighted average of all the facets\nfavg\nct = ∑K\nk=1 πct,kfct,k, where we lower the in-\nﬂuence of kth facet embedding fct,k with lower\nprior weight πct,k and fct,1 = 1\nJ\n∑J\nj=1 fj\nct,1 if J\npartitions are used. Figure 4 illustrates favg\nct and\nfct,k using the example in the second column of\nTable 7.\nWe visualize the new average facet using the\nwords that are closest to the favg\nct in the MFS Avg\nrow of Table 7. We can see that the predictions\nof MFS Avgis different from MFS but similar to\nSoftmax. This means there are indeed some other\nwords between the actual next word and the other\npossibilities, which makes the predictions of MFS\nmulti-mode.\nNext, to quantify the difference between MFS\nand MFS Avg, we deﬁne multi-mode ratio\nas\n∑T\nb=1 PM(yb|ct)∑T\nb=1 PM(xb|ct) , where PM could be either\nPMoS from equation 2 or PMP from equation 4.\n{y1,...,y T} is the set of words with embed-\ndings closest to favg\nct and {x1,...,x T}is the set\n8063\nDissimilar Words Similar Words\nModels↓ Testing Validation Training Testing Validation Training\nSoftmax 1.97 1.98 1.95 2.16 2.16 2.17\nMoS (3) 1.81 1.80 1.69 2.05 2.05 1.87\nMFS w/o Multi-partition 1.78 1.79 1.70 2.04 2.06 1.88\nMFS 1.79 1.79 1.69 2.02 2.05 1.89\nTable 9: Perplexity comparison of different models on the similar words or dissimilar words. The models are based\non GPT-2 Small and trained in OpenWebText.\nGPT-2 Small after 1 epoch\nSoftmax (S1I1P1) SigSoftmax (S1I1P1)\n0.5494 0.5489\nSoftmax + Multi-input (S1I9P1) Softmax + Multi-partition (S1I1P4)\n0.5508 0.5492\nMoS (Yang et al., 2018) (4) (S4I1P1) MoS (Yang et al., 2018) (3) (S3I1P1) DOC (Takase et al., 2018) (S3I3P1)\n0.5501 0.5499 0.5494\nMFS w/o Multi-partition (S3I9P1) MFS w/o Multi-input (S3I1P4) MFS (S3I9P4)\n0.5515 0.5502 0.5519\nGPT-2 Medium after 0.4 epoch\nSoftmax (S1I1P1) SigSoftmax (S1I1P1)\n0.5665 0.5650\nSoftmax + Multi-input (S1I9P1) Softmax + Multi-partition (S1I1P4)\n0.5677 0.5665\nMoS (Yang et al., 2018) (4) (S4I1P1) MoS (Yang et al., 2018) (3) (S3I1P1) DOC (Takase et al., 2018) (S3I3P1)\n0.5674 0.5672 0.5665\nMFS w/o Multi-partition (S3I9P1) MFS w/o Multi-input (S3I1P4) MFS (S3I9P4)\n0.5685 0.5677 0.5685\nTable 10: MRR (mean reciprocal rank) of different models in OpenWebText. Larger is better.\nof words with highest PM(xb|ct). Using the\nWikipedia context in Table 7 as an example, the\nword project is retrieved by MFS but not by\nMFS Avg, so its multi-mode ratio for T = 2\nis PMFS(hom|ct)+PMFS(dual|ct)\nPMFS(project|ct)+PMFS(hom|ct) = 0.049+0.046\n0.096+0.049 ≈\n0.66. Figure 4 illustrates the relation between the\nMFS Softmax kand MFS Avg.\nWhen the ratio is closer to 1, the context is less\nambiguous and the prediction is closer to a single-\nmode distribution. We set T = 20and call the pre-\ndiction with multi-mode ratio smaller than 0.9 mul-\ntimodal distribution and in Table 8,7 we compare\nthe loss (i.e., log of the perplexity) improvements in\nthe multimodal distributions and the improvements\nin the nearly single-mode distributions.\nTable 8 shows that all the multiple embedding ap-\nproaches have larger loss improvements when out-\nputting multimodal distributions. The table shows\nthe results based on GPT-2 Small and the same\nanalysis using GPT-2 Medium also show the same\ntrend. As we use multiple input hidden states and\npartitions, the differences would be enlarged. Es-\npecially when we compare MFS and MFS w/o\n7We also tried T=5 or 10 and the trends are similar. If\nwe set the threshold smaller than 0.9, the improvement ratios\n(e.g., MFS over MoS) would increase but the multi-mode\npercentages would decrease.\nMulti-partition, the loss improvements of highly\nambiguous context is 7 or 8 times larger than the\nother loss improvements, which means a large por-\ntion of the overall improvement lies on a small per-\ncentage of ambiguous contexts. For the multimodal\ndistribution in Wikipedia, the loss improvement be-\ntween MFS and Softmax could reach 0.10, which\nis close to the improvement between GPT-2 Small\nand Medium (0.16). Thus, we expect that if the\ncorpus has more ambiguous contexts, MFS could\nachieve larger overall loss improvement.\nB.4 Template-based Analysis on Similar or\nDissimilar Nouns\nTo know whether the single embedding also has\ntrouble modeling the distribution over nouns with-\nout the analogy relation, we let the different models\nlearn to assign similarly high probabilities to two\nrelated nouns in our templates. One example in\nour synthesized dataset is “I love the banana and\nthe lemon, and my favorite is the [MASK] ”. The\nnouns come from a hypernymy detection bench-\nmark (Shwartz et al., 2017) containing 25,498\nnoun pairs. The relations between nouns in the\nbenchmark include synonym, antonym, attribute,\nmeronym, hypernym, coordination, event, or ran-\ndom. We further split the noun pairs into two\n8064\ndatasets based on their cosine similarity in the out-\nput word embedding space of our Softmax base-\nline. The pairs with the cosine similarity higher\nthan the medium of all cosine similarities are put\ninto the similar word set and the other pairs are put\ninto the dissimilar word set.\nThe results are presented in Table 9. In terms\nof the training, validation, and testing perplexity,\nmulti-embedding approaches consistently outper-\nform the single-embedding baselines, though the\nmargins are smaller than those from the analogous\nwords. Moreover, the improvement gap is larger\nwhen the nouns are dissimilar. We hypothesize that\nas the word embeddings of nouns become further\naway from each other, the next word distribution\nis more likely to be multi-mode and thus could be\nbetter captured by multiple embeddings.\nB.5 Adversarial Template Analysis\nTo test whether the proposed methods still can ef-\nfectively utilize the information from the global\nword embeddings, we design an adversarial tem-\nplate to create the contexts that can only be com-\npleted by averaging the global word embeddings.\nFor example, “Miami is not in Wisconsin but is in\n[MASK]=Florida”.\nIn this task, the validation perplexity of Soft-\nmax, MoS, MFS w/o Multi-partition, and MFS\nare 2.50, 2.59, 2.54, and 2.88, respectively. Since\nmultiple embeddings are not required, it is not sur-\nprising that Softmax performs the best. Neverthe-\nless, the differences are smaller than the differences\nin Table 4. We believe that the similar losses are be-\ncause multiple embeddings are a generalization of\nthe single embedding, so GPT-2 could learn to gen-\nerate the same embedding for all facets to mimic\nthe behavior of single embedding if required.\nThe signiﬁcantly worse performance of MFS\nhere is caused by the multiple partition technique.\nThis result supports our motivation of combining\nmultiple partitions with multiple softmaxes and\nshows that multiple partitions handle ambiguous\ncontexts better (as shown in Table 8) by sacriﬁcing\nsome global word embedding structures. Never-\ntheless, a corpus usually has more ambiguous con-\ntexts than the adversarial context tested here, so\nusing multiple embeddings and multiple partitions\nperforms better in Wikipedia and OpenWebText\noverall.\nC More Results\nWe provide more numbers and analyses of our ex-\nperiments.\nC.1 Ranking Metric in Language Modeling\nExperiments\nWe would like to verify that our perplexity improve-\nments come from not only the slight probability\ndifferences of each candidate but also the better\nranks of the candidates. Thus, in Table 10, we eval-\nuate different models using mean reciprocal rank\n(MRR). Similar to the perplexity, the MRR im-\nprovement from Softmax to MFS is around 15%\nof the MRR improvement from GPT-2 Small to\nGPT-2 Medium, which is similar to the percent-\nage of perplexity improvement. This suggests that\nMFS could lead to not only a better probability pre-\ndiction but also a better candidate rank prediction.\nC.2 Perplexity Curves in Language Modeling\nExperiments\nIn Table 1, we only show the testing perplexity at\nthe end of our training. In Figure 6, we plot the val-\nidation perplexity decay curves during the training\non OpenWebText. We can see that the performance\nranking of each model is stable during the training,\nwhile the improvement of each enhancement may\nvary. For example, in GPT-2 Medium, the improve-\nment of MFS over MFS w/o Multi-partitionis\nmore obvious in epoch 0.25 compared to epoch\n0.4.\nC.3 Perplexity Curves in Template Analysis\nIn Table 4, we only show the lowest validation\nperplexity after each of the ten epochs. In Figure 5,\nwe plot the training and validation perplexity decay\ncurves.\nThe curves tell us that the multi-embedding\nmodels perform better in both training and valida-\ntion perplexity. As we train the single-embedding\nmodels longer, the validation perplexity increases\nquickly, which implies that using a single embed-\nding to model multimodal distribution could cause\nsevere overﬁtting when we predict the next word\ngiven an ambiguous context.\nC.4 Stability in Language Modeling\nExperiments\nIn our case, training our model requires a huge\namount of GPU resources for us, so it is not very\n8065\n0.2 0.4 0.6 0.8 1.0\nepoch\n18.0\n18.5\n19.0\n19.5\n20.0\n20.5\n21.0\n21.5perplexity\nSoftmax\nMoS (3)\nMFS - Multi-partition\nMFS\n(a) Curves on GPT-2 Small\n0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40\nepoch\n15.50\n15.75\n16.00\n16.25\n16.50\n16.75\n17.00perplexity\nSoftmax\nMoS (3)\nMFS - Multi-partition\nMFS (b) Curves on GPT-2 Medium\nFigure 5: The perplexity curves for the language modeling tasks using the validation set of OpenWebText.\n0 2 4 6 8 10 12\nepoch\n1.6\n1.7\n1.8\n1.9\n2.0\n2.1\n2.2perplexity\nDiag Softmax\nDiag MoS (3)\nDiag MFS - Multi-partition\nDiag MFS\nEdge Softmax\nEdge MoS (3)\nEdge MFS - Multi-partition\nEdge MFS\n(a) Perplexity in the training data\n0 2 4 6 8 10 12\nepoch\n1.8\n2.0\n2.2\n2.4\n2.6\n2.8perplexity\nDiag Softmax\nDiag MoS (3)\nDiag MFS - Multi-partition\nDiag MFS\nEdge Softmax\nEdge MoS (3)\nEdge MFS - Multi-partition\nEdge MFS (b) Perplexity in the validation data\nFigure 6: The perplexity curves from different models for the ambiguous template analysis\nMax Answers Max Incorrect\nModels↓ Top 1 Top 3 Top 5 Top 10 Top 1 Top 3 Top 5\nSoftmax (GPT-2) 36.5±0.7 39.7±0.5 43.5±0.4 52.2±0.6 20.9±0.4 37.7±0.6 46.7±0.6\nMoS (Yang et al., 2018) (3)36.6±0.8 40.2±0.6 43.2±0.6 52.1±0.4 21.3±0.6 38.4±0.5 45.9±0.6\nMFS w/o Multi-partition37.7±0.7 42.0±0.6 44.6±0.5 52.6±0.3 22.9±0.4 39.5±0.5 47.4±0.4\nMFS 36.9±0.7 41.6±0.7 44.4±0.6 52.3±0.6 23.1±0.5 39.7±0.6 46.9±0.6\nTable 11: ProtoQA performances on the crowdsourced development sets. The matching between prediction and\nground truth is done by WordNet. All the numbers are percentages. Max answers top k implies only evaluating\nthe top k answers from different LMs. Max incorrect top k indicates only evaluating the top answers that contain\nk errors. The highest average performances are highlighted and the standard errors are reported as the conﬁdence\ninterval.\nfeasible to train multiple times using multiple ran-\ndom seeds. We indeed try to use different random\nseeds for a few models and we conﬁrm that the val-\nidation loss difference is at least ten times smaller\nthan the improvement of different models.\nTo verify that our testing dataset is large enough\nto provide stable perplexity, we randomly split the\ntesting dataset into 10 subsets and compute the\nstandard error of the average testing perplexity of\nthe 10 subsets. We ﬁnd that the standard error is\nless than 0.02 perplexity in all models and datasets\nin Table 1. The standard error is much smaller than\nmost of the improvements, which means our testing\ndataset is large enough to make the reported per-\nplexity stable. The consistent improvements during\nthe whole training process in Figure 5 further sup-\n8066\n19.0 19.2 19.4 19.6 19.8\nlog(number of parameters)\n2.75\n2.80\n2.85\n2.90\n2.95\n3.00loss\nSoftmax\nMoS (3)\nMFS - Multi-partition\nMFS\nFigure 7: The log of model size versus the log of per-\nplexity in the text set of OpenWebText. The group of\npoints on the left comes from the models based onGPT-\n2 Small. The group of points on the right comes from\nthe models based on GPT-2 Medium. The models are\ntrained for 0.4 epoch.\nport the stability of our experiments.\nC.5 ProtoQA Results using WordNet\nIn Table 5, we report the metrics using exact match-\ning. In Table 11, we report the metrics that match\nthe prediction with the ground truth using Word-\nNet (Miller, 1992) and ﬁnd the scores show a simi-\nlar trend.\nC.6 Perplexity Improvement versus Model\nSize\nKaplan et al. (2020) empirically demonstrate that\nincreasing the model size would decrease the loss\nand their relation follows a scaling law. That is,\nwe can plot the log of model size (i.e., parameter\nnumber) versus its loss as in Figure 7, and if a new\nLM model could result in lines that are closer to the\norigin than the baselines, the new model is better\nin terms of the loss than only increasing the model\nsize of the baselines.\nFrom Figure 7, we can see that the approaches\nusing multiple embedding are better than the Soft-\nmax baseline using single embedding. Although\nthe lines formed by MFS w/o Multi-partitionand\nMFS are not always closer to the origin than MoS,\nour perplexity improvement from adding multiple\ninput hidden states or multiple partitions cannot\nbe solely explained by their extra parameters for\nseveral reasons:\n• Compared to MoS, the line formed by MFS\nw/o Multi-partitionbecomes slightly closer to\nthe origin when the model size is close toGPT-2\nMedium.\n• The improvement of MFS w/o Multi-\npartitions (S3I9P1) is larger than the\nimprovement of Softmax + Multi-input\n(S1I9P1) plus the improvement of MoS\n(S3I1P1) in BERT and GPT-2. For example,\nin BERT base, the perplexity improvement of\nSoftmax + Multi-input, MoS (3), and MFS\nw/o Multi-partitions are 0.018, 0.016, and\n0.047, respectively.\n• Our multi-mode analyses in Appendix B.3 in-\ndicate that our enhancements, especially using\nmultiple partitions, capture the multimodal dis-\ntribution better. We expect that the overall per-\nplexity improvement would be larger if the cor-\npus contains more ambiguous contexts. We also\nconduct a preliminary experiment to conﬁrm the\nclaim. We add more ambiguous contexts into\nWikipedia 2016 by mapping all the uppercased\nwords into the [UNK] token. That is, we add\nanother mode corresponding to the [UNK] to-\nken in many context positions. Then, we train\nand test the uncased BERT in this synthesized\ndataset. We found that the improvement of\nMFS w/o Multi-partitionin this case can do\nsigniﬁcantly better than simply increasing the\nmodel size.\n• Our enhancements only require some extra lin-\near layers, which are usually more efﬁcient than\nincreasing the model size (e.g., by adding an-\nother transformer layer).\n• Unlike increasing the model size, keep increas-\ning the number of input hidden states or the\nnumber of partitions would lead to a smaller\nimprovement. This suggests that MFS cannot\nkeep storing more and more knowledge into its\nextra linear layers as in the architecture using a\nlarger hidden state size or a deeper transformer\nencoder.\nC.7 More Visualization\nIn Table 3, we compare the prediction of MFS and\nSoftmax on GPT-2 Small. In the ﬁrst two columns\nof Table 7, we present the examples from the mod-\nels built on GPT-2 Medium in OpenWebText and\nWikipedia 2021. We can see a similar pattern. The\nembedding of the correct answer is different from\nthe embeddings of other possibilities, so Softmax\nassigns lower probabilities to the correct answer,\nwhile MFS does much better. This suggests that a\n8067\nlarger model such as GPT-2 Medium suffers from\nthe softmax bottleneck in a similar way.\nIn the last column of Table 7, we visualize an\nexample in another synthetic experiment described\nin Appendix B.4. We can see that although there\nmay not be any words between the appropriate\ncandidates, the prediction of Softmax may still be\nbiased toward one option much more than the other,\nwhile the prediction of MFS is much closer to the\nequally likely bimodal distribution we created in\nthe training data.\nD Proof of Theorems\nTo prove Theorem 1, we ﬁrst introduce a lemma.\nAssuming in the word embedding of GPT-2,\nwoman + king = queen + man, we want to show\nthat GPT-2 cannot output woman and king as the\ntop two words in this lemma. This means we can-\nnot ﬁnd a hidden state h and a threshold τ >0\nsuch that hTwoman≥ τ and hTking≥ τ but\nhTqueen< τ and hTman< τ. This example\ncould be generalized into the following Lemma\nand Theorems. We can generalize the example as\nfollows:\nLemma 1. Let the output word embeddings in\nthe set W = {wlj ̸= 0|j = 1...L}∪{ wrj ̸=\n0|j = 1...R}satisfy −al1 wl1 −...−alLwlL =\nar1 wr1 + ... + arRwrR, where their coefﬁcient\n−al1 ,..., −alL,ar1 ,...,a rR are all positive con-\nstants and −al1 −...−alL ≥ ar1 + ...+ arR.\nThen, there is no hidden state h and a thresh-\nold τ > 0 that make min\nwg∈G\nhTwg ≥ τ and\nmax\nws∈S\nhTws < τ, where G = {wlj|j = 1...L}\nand S = {wrj|j = 1...R}.\nProof. To prove by contradiction, we assume there\nis a h such that ∀wlj ∈G,hTwlj ≥τand ∀wrj ∈\nS,hTwrj < τ. Thus, we can get −al1 hTwl1 −\n...−alLhTwlL ≥−al1 τ −...−alLτ ≥(ar1 +\n...+arR)τ >ar1 hTwr1 +...+arRhTwrR, which\ncontradicts to −al1 wl1 −...−alLwlL = ar1 wr1 +\n...+ arRwrR.\nWe can rephrase the condition and the conclu-\nsion to have our Theorem 1.\nTheorem 1. If the nonzero output embeddings of\nN words in a set W are linearly dependent and on\none side of a hyperplane through the origin, the\nsingle embedding representation cannot produce\npositive logits to a subset of the words in W that\nare higher than all the logits of the other words in\nW.8\nProof. The set W = {wi ̸= 0|i= 1...N}contain\nthe embeddings of the N words. Based on the\npremise, we can write 0 = a1w1 + ...+ aNwN\nand minwi∈W hT\n0 wi > 0, where h0 is a normal\nvector of the hyperplane. At least one of the ai is\nnegative. Otherwise, we will get the contradiction\n0 = hT\n0 0 = a1hT\n0 w1 + ...+ aNhT\n0 wN ≥(a1 +\n...+ aN) minwi∈W hT\n0 wi >0. Similarly, at least\none of ai is positive. We can move all the terms in\n0 = a1w1 +...+aNwN with negativeai to the left\nas −al1 wl1 −...−alLwlL = ar1 wr1 +...+arRwrR.\nIf −al1 −...−alL ≥ar1 + ...+ arR, we choose\nG = {wlj|j = 1...L}. Otherwise, we choose\nG= {wrj|j = 1...R}\nIf we can have a hidden state such that the pos-\nitive logits of words in Gare always larger than\nthe logits of the other words in W (let’s call the\ncomplementary set S), there must exist τ >0 that\ncan make min\nwg∈G\nhTwg ≥τ and max\nws∈S\nhTws < τ,\nwhich violates our Lemma 1.\nNext, we would like to generalize our Theorem 1\nby using a more practical condition where the word\nembeddings are almost linearly dependent. Notice\nthat the theorem needs to assume the magnitude of\nthe hidden state is limited. Otherwise, the margin\ncould be arbitrarily magniﬁed. In practice, the\nmagnitude is not arbitrarily large in GPT-2 and\nBERT because a too large magnitude of hidden\nstate could magnify the gradients too much to have\na stable training process.\nTheorem 2. Let the output word embeddings in\nthe set W = {wi ̸= 0|i = 1 ...N} satisfy\nw1 = a2w2 + ...+ aNwN + ε, where the con-\nstant a2,...,a N are neither all zero nor all neg-\n8Notice that Theorem 1 does not cover the situations where\nthe target top words have negative logits (i.e., some logits of\nthe words in Gare negative). In the single softmax model, we\nbelieve the situations rarely happen in the LMs empirically.\nIf some logits of the target top words are still positive, the\nwords that are somehow similar to those words are very likely\nto also be positive, which would be ranked higher than the\ntarget top words with the negative logits.\nIf the logits of all the target top words are negative in some\ncontexts, the logits of all the words would be negative. Then,\nthe word embeddings with smaller magnitudes tend to have\nthe logits closer to 0, so having the larger logits than the other\nnegative logits. This means the prior probability of the words\nwould be inversed when the hidden states sometimes produce\nall negative logits.\nIf a LM always uses negative logits to compute probability\nin all the contexts, Lemma 1 and Theorem 1 still hold if we\nset τ <0 and switch the choices of Gand S.\n8068\native and ||ε||< ϵ. Then, there must be a non-\ntrivial partition P = {G,S}of W such that\nthere is no hidden state ||h||≤ rand a threshold\nτ ≥rϵthat makes minwg∈GhTwg ≥(1+δ)τand\nmaxws∈ShTws <τ , where δ= 2\n1+∑\ni=2...N |ai|.\nProof. We can ﬁrst move all the terms with nega-\ntive ai to the left as w1 −al1 wl1 −...−alLwlL =\nar1 wr1 + ...+ arRwrR + ε. We perform proof by\ncontradiction, so we assume the logits of the words\nin Gcan always be larger than (1 +δ)τ and the\nlogits of the words in Scan always be smaller than\nτ.\nCase 1: 1 −al1 −...−alL ≥ar1 + ...+ arR, so\n1 −al1 −...−alL ≥1+∑\ni=2...N |ai|\n2 . We choose\nG= {w1,wl1 ,..., wlL}and S = {wr1 ,..., wrR}.\nThus, we can get hTε ≤||h||||ε||≤ rϵ≤τ and\nhTw1 −al1 hTwl1 −...−alLhTwlL (5)\n≥(1 −al1 −...−alL)(1 +δ)τ (6)\n=(1 −al1 −...−alL)(1 + 2\n1 +∑\ni=2...N |ai|)τ\n(7)\n≥(1 −al1 −...−alL)(1 + 1\n1 −al1 −...−alL\n)τ\n(8)\n=(1 −al1 −...−alL + 1)τ (9)\n≥(ar1 + ...+ arR + 1)τ (10)\n>ar1 hTwr1 + ...+ arRhTwrR + hTε, (11)\nwhich contradict withw1−al1 wl1 −...−alLwlL =\nar1 wr1 + ...+ arRwrR + ε.\nCase 2: 1 −al1 −... −alL < ar1 + ... +\narR. We choose G = {wr1 ,..., wrR}and S =\n{w1,wl1 ,..., wlL}. Therefore,\nar1 hTwr1 + ...+ arRhTwrR (12)\n≥(ar1 + ...+ arR)(1 + 2\n1 +∑\ni=2...N |ai|)τ\n(13)\n>(ar1 + ...+ arR)(1 + 1\nar1 + ...+ arR\n)τ (14)\n=(ar1 + ...+ arR + 1)τ (15)\n>(1 −al1 −...−alL + 1)τ (16)\n>hTw1 −al1 hTwl1 −...−alLhTwlL −hTε.\n(17)\nxi\nxr1\nxr2\nali xi  =   \nar1 xr1 + ar2 xr2\nFigure 8: An example for explaining the connection\nbetween our Theorem 1 and the theorem from Demeter\net al. (2020).\nE Theoretical Connection to Demeter\net al. (2020)\nThe theory in Demeter et al. (2020) is as follows:\n“Let Cbe the convex hull of the embeddings {xi} of\na vocabulary V. If an embedding xi for word wi ∈\nV is interior to C, then the maximum probability\nP(wi) assigned to wi using a dot-product softmax\nis bounded by the probability assigned to at least\none word wi whose embedding is on the convex\nhull”\nThe theory is a special case of our Lemma 1 if\nwe only consider the hidden states that would lead\nto the positive logit of the interior word wi. To\nsee that, we ﬁrst ﬁnd a constant ali >1 such that\nalixi intersects with one supporting hyperplane of\nthe convex hull. This intersection point could be\nexpressed by ∑\nj arjxrj, where the word embed-\ndings xrj are vertexes of C and ∑\nj arj = 1. As\na result, we satisfy the condition of our Lemma 1:\nalixi = ∑\nj arjxrj and ali >∑\nj arj. Please see\nan illustration in Figure 8 for an example. Then,\nLemma 1 suggests that the logit hTxi cannot be\nlarger than the logits of all the word embeddings\nhTxrj. This means at least one of the hTxrj on\nthe convex hull would lead to a larger prediction\nprobability, which is also the conclusion of the the-\nory in Demeter et al. (2020).\nF Method Details\nWhen replacing the softmax layer in the pretrained\nLMs, we found that the initialization of the ex-\ntra linear layers should make the initial prediction\nof LMs close to the prediction using a softmax\n8069\nlayer, which is the architecture used in the pre-\ntraining. Otherwise, the performance would drop\nsigniﬁcantly. The initialization is especially impor-\ntant for BERT. To achieve the goal, we initialize\nthe weights of the linear layers such that different\nfacets are almost identical at the beginning and let\nthe LMs gradually learn to output diverse facets\nduring the training. Speciﬁcally, we can write the\nlinear layer on the new hidden state Lf\nk(qct) as\nfct,k = Lf\nk(qct)\n= LI\nkhM\nct + LB\nk GELU\n(\nLh(⊕i,mhM−m\nct−i )\n)\n+ b.\n(18)\nWe initialize LI\nk as an identity matrix, b ←0, and\nLB\nk ← U(−ϵ,ϵ), where Uis the uniform distri-\nbution and ϵ = 0.00005 if k ̸= K. Otherwise,\nϵ = 0. Consequently, all the facets fct,k are ini-\ntially close to the last hidden state of the original\nGPT-2 hM\nct . Our baselines (e.g., Softmax, MoS,\nand DOC) also adopt the same way to initialize\ntheir weights.\nWe implement our models based on hugging-\nface9 (Wolf et al., 2020). Please see our codes for\nmore details.\nF.1 Architecture Differences in BERT\nThe architecture of MFS for BERT is mostly the\nsame as the one for GPT-2 and the differences are\ndescribed in this subsection.\nIn GPT-2 the block of input hidden state is right-\naligned with the last word to prevent seeing the\nground truth. On the other hand, the block in BERT\nis centered at the masked word.\nThe softmax layer of BERT is slightly different\nfrom that of GPT-2. For example, BERT adds a\nbias term after the dot product between the hidden\nstate and the output word embedding. We keep\nthe bias term in our experiments. Besides, the\npretrained BERT has a language modeling head\nincluding a linear layer, a GELU (Gaussian Error\nLinear Unit) layer (Hendrycks and Gimpel, 2016),\nand a layernorm layer (Ba et al., 2016), so instead\nof adding an extra linear layer as in GPT-2, we just\nuse different language modeling heads to create dif-\nferent facets in BERT. All the heads are initialized\nusing the weights in the pretrained BERT except\nthat the linear layer is initialized as in Equation 18\nwhen the multiple input hidden states are used and\n9https://huggingface.co/\nthe corresponding linear weights LB\nk ←U(−ϵ,ϵ),\nwhere ϵ= 0.05 if k̸= K. Otherwise, ϵ= 0.\nG Experimental Details\nIn this section, we describe some details of our\nexperimental setup.\nG.1 Baselines\nThe MoS (Yang et al., 2018) and DOC (Takase\net al., 2018) are originally designed for RNN-\nbased LM. To improve their methods on pretrained\nTransformer-based LM and make their results more\ncomparable to MFS, we change some of their im-\nplementation details.\nMoS originally has a tanh layer before the soft-\nmax layers. However, we found that adding tanh\nhurts the performances of all methods we tested,\nespecially the Softmax and MoS baselines. For ex-\nample, after adding tanh and training GPT-2 Small\nfor 0.4 epoch on Wikipedia, the validation perplex-\nity degradation of Softmax is from 25.70 to 26.15,\nthe degradation of MoS is from 25.42 to 25.83,\nand the degeneration of MFS is from 25.06 to\n25.12. We suspect this is because GPT-2 is pre-\ntrained without the tanh layer and the tanh limits\nthe magnitude of facets ||fct,k||, which could be\nviewed as the inverse of the temperature in the soft-\nmax layer. Therefore, we remove the tanh layer\nin all of our experiments. From the theoretical\nperspective, adding tanh does not invalidate our\nmotivation because adding tanh does not change\nthe total degree of freedom in all facet embeddings\nand the dimension of the hidden state.\nIn DOC, we use the hidden states of the last\nthree transformer layers to compute the three facets\nand we set λβ = 0. Each facet is only determined\nby one layer of hidden state, so the ﬁrst two facets\ncannot access the last hidden state. We found that\nthe model quickly learns to only use the last facet\nbecause only the last hidden state is trained to per-\nform the LM task in the pretrained models. This\nprevents the ﬁrst two facets from getting any gradi-\nents and causes a starvation problem.\nWe tried an aggressive dropout trick to solve the\nstarvation problem inDOC. If one of the softmaxes\ndoes not assign the highest probability to any of the\ncorrect next words in a batch, we consider that the\ncorresponding facet starves, so we drop the other\nfacets with some probability to ensure this starved\nfacet receives some gradients and gradually gets\nback on track. However, our preliminary experi-\n8070\nment suggests that the dropout trick cannot improve\nthe perplexity of DOC. The dropout probability is\neither too low to solve the starvation problem or\ntoo high to preserve the knowledge learned from\npretraining. Thus, we do not adopt this trick in our\nﬁnal experiment.\nG.2 Language Modeling\nWe download Wikipedia using http:\n//medialab.di.unipi.it/wiki/\nWikipedia_Extractor and OpenWebText us-\ning https://github.com/jcpeterson/\nopenwebtext. For Wikipedia, we prepro-\ncess the text using https://github.com/\nattardi/wikiextractor. For OpenWeb-\nText, we download the pre-ﬁltered URLs in 2017\nand 2018 and scrape the text on April 2021. When\nsplitting the corpus into training, validation, and\ntesting sets, we do not shufﬂe the data. Instead,\nwe use the text near the end of the corpus as\nthe validation and test set to reduce information\nleakage. To ensure every model is trained on\nthe same data and accelerate the training in\nour machines, we split the training data into 20\nconsecutive partitions and load only one partition\nat a time during training. When training GPT-2\nMedium, we only use the ﬁrst 8 partitions to let the\ntraining be ﬁnished within a week. For BERT, we\nperform the sentence segmentation using SpaCy10\nand input one sentence into BERT at a time.\nWe set our hyperparameters (e.g., facet number\nK = 3and W ×H = 3×3 when using multiple\ninput hidden states) based on the validation perfor-\nmance in Wikipedia 2016, the resulting model size,\nand the memory constraint in GPUs. To explore\nthe limitation of the softmax layer, we untie the\ninput word embeddings and output word embed-\ndings in all of our experiments. The untying allows\nthe LMs to arrange the output word embeddings\nmore freely and allows us to observe if the result-\ning output word embeddings still cause multi-mode\ndistribution. This is also the main reason the model\nsize of our GPT-2 baseline is larger than the size\nof pretrained GPT-2 (Radford et al., 2019). We\nuse AdamW (Loshchilov and Hutter, 2019) opti-\nmizer and set the learning rate as 1e-5 and do not\nuse the warm-up because the training starts from\nthe pretrained models. The sequence length (i.e.,\nbptt) is set as 200 for GPT-2 and 256 for BERT.\nThe batch sizes are set as 4 for GPT-2 Small, 16\n10https://spacy.io/\nfor GPT-2 Large, 120 for BERT base, and 128 for\nBERT large.\nThe analyses in Table 2 and Table 8 use the\nﬁrst 4000 sequences in the validation dataset and\nall the methods are based on GPT-2 Small. We\nuse PYCLD211 to distinguish between English and\nnon-English text.\nWe use NVIDIA GeForce RTX 2080 for training\nGPT-2 Small and BERT base, GeForce RTX 8000\nfor training GPT-2 Medium, Tesla M40 for training\nBERT large. Since we start from the pretrained LM,\nwe can ﬁnish training each LM within 2 weeks\nusing 1 GPU for GPT-2 Small, BERT base, and\nGPT-2 Medium, and using 4 GPUs for training\nBERT large.\nWhen testing the inference time in Table 1, we\naverage the time of running NVIDIA TITAN X\non 10,000 batches, where each batch contains 4\nsequences whose length are 200.\nWhen visualizing the prediction in Table 3, we\nexclude the non-ASCII symbol prediction from the\ntop word list of all models.\nG.3 Ambiguous Templates Analysis\nAmong the semantic relations in Google anal-\nogy dataset, we choose three different relations\nbetween locations: capital-common-countries,\ncapital-world, city-in-state, and one relation be-\ntween people: family. We exclude the currency\ncategory because their instance often does not form\na parallelogram in the word embedding space (Etha-\nyarajh et al., 2019). The templates we use are listed\nin Table 12. For the family category, our templates\nassume the words are not pronouns, so we exclude\nthe set of four words that include he or she.\nFor each of the four words in an analogy instance\n(e.g., queen : king = woman : man), we would cre-\nate 32 training or testing sequences12 based on the\ndiagonal words such as king or woman. Similarly,\nwe would create 64 sequences in the edge datasets.\nSome words contain multiple word pieces and we\naverage the losses of all word pieces during training\nand testing.\nWe split the synthesized sequences based on\ntheir word pair overlapping. First, we randomly\nsample half of the word pairs (e.g., king and queen)\nin each category as our training pairs. If both of\nthe word pairs in an analogy instance are training\n11https://github.com/aboSamoor/pycld2)\n122 (diagonal words) × 4 (templates) × 2 (word orders in\nthe template) × 2 (possible next words)\n8071\nDataset↓ Templates\nAnology Between the$ARG1and the$ARG2, I decided to ﬁrst talk to the [MASK]\n(Person The$ARG1and the$ARG2are my favorites, and I especially love the [MASK]\nor The$ARG1and the$ARG2happily live together. One day, bad luck happens to the [MASK]\nPerson) The$ARG1and the$ARG2stay at my house, and I need to take care of the [MASK]\nAnology I went to$ARG1and$ARG2before, and I love one of the places more, which is [MASK]\n(Location $ARG1and$ARG2are my favorites, and I especially love [MASK]\nor My uncle used to live in$ARG1and$ARG2but now, he is selling his house in [MASK]\nLocation) The traveler plans to visit$ARG1and$ARG2, and the traveler ﬁrst arrives in [MASK]\nSimilarity I love the$ARG1and the$ARG2, and my favorite is the [MASK]\n(Noun Yesterday, a man encountered the$ARG1and the$ARG2. Today, he again saw the [MASK]\nor There are the$ARG1and the$ARG2in front of a woman, and she decides to pursue the [MASK]\nNoun) If you can choose the$ARG1or the$ARG2, would you choose the [MASK]\nTable 12: The templates used in the analysis. The ﬁrst four templates are for the analogy relations from the\ncapital-common-countries, capital-world, and city-in-state categories. The next four templates are for the analogy\nrelations from the family category. The ﬁnal four templates are for similar or dissimilar nouns.\npairs, the instance is put into our training set. If\nonly one of the word pairs is a training pair, the\ninstance would belong to our validation set. The\nrest of the instances form our testing set. During\nthe ﬁne-tuning, we evaluate a model using the val-\nidation set after each epoch and select the model\nbased on its best validation perplexity.\nG.4 ProtoQA Evaluation\nIn our experiments, we use the scraped develop-\nment set as our validation set and the crowdsourced\ndevelopment set as our test set. We do not test our\nmethods on the test set of ProtoQA because the\nresult of every submission would show up in their\nleaderboard and we do not want to overwhelm the\nleaderboard with our 15 trials.\nDue to our limited GPU resources, we com-\npare the methods built on GPT-2 Medium rather\nthan GPT-2 Large. To maximize the perplexity of\nthe GPT-2 Medium model using Softmax on the\nscraped development set, we ﬁne-tune our models\nusing learning rate 3e-5 and warmup step 500.\nThe original paper (Boratko et al., 2020) does\nnot consider the frequency of the answer during the\nﬁne-tuning (i.e., the most possible answer and the\nleast possible answer of each question appear in\nthe training data with the same chance). In terms\nof the performance of the scraped development set,\nwe ﬁnd that weighting each answer based on the\nsquare root of its frequency is better than weighting\neach answer uniformly as in the original paper or\nweighting each answer based on its frequency, so\nwe use the square root weighting to ﬁnetuning all\nour models.\nDuring testing time, each model generates the\nanswers using Nucleus Sampling (Holtzman et al.,\n2020) with p = 0.9 and temperature = 1. Then, we\ncollect all the words before the ﬁrst period as an\nanswer and drop the generated sentences without a\nperiod.\nH Future Work\nCapturing the next word distribution well given\nan ambiguous context could be important in some\ndownstream applications. A next step could be\ninvestigating whether multiple facets lead to a bet-\nter language generation model for the applications.\nFor example, we would like to know whether break-\ning the softmax bottleneck could reduce the hallu-\ncination of LMs (e.g., outputting queen when the\nreasonable next words should be king or woman)\nand increase the coherence of the generated text.\nWe also want to more systematically investigate\nwhether modeling multi-mode distribution could\nhelp LMs to reduce the undesired bias and to better\ndistinguish similar words (Zagoury et al., 2021) as\nin Appendix B.4.\nNarang et al. (2021); Anonymous (2021) ﬁnd\nthat MoS can signiﬁcantly improve the BERT-like\nLMs on natural language understanding (NLU)\ntasks when the LMs are trained from scratch. Al-\nthough we ﬁnd that the perplexity improvement of\nmulti-embedding BERT is not as large as multi-\nembedding GPT-2, pretraining using multiple em-\nbeddings does not decrease the inference speed of\nthe BERT encoder on NLU tasks. This motivates\nthe future studies that test if MFS also provides a\nlarger improvement than MoS in NLU tasks.\nTable 2 suggests that multiple embeddings im-\nprove more in a non-English context. We wonder\n8072\nwhether multiple embeddings are more beneﬁcial\nto the LMs that are trained on a non-English dom-\ninating corpus. Chung et al. (2021) discover that\nusing a larger output embedding dimension im-\nproves the multilingual BERT. An interesting re-\nsearch question is whether the improvement comes\nfrom alleviating thesoftmax bottleneck and whether\nMFS could also lead to similar improvements in\nmultilingual benchmarks.\nThe hidden state size of GPT-3 175B (Brown\net al., 2020) is huge (12,288). An interesting ques-\ntion is whether some sets of output word embed-\ndings in GPT-3 are still in a low-dimensional sub-\nspace and whether the softmax bottleneck is still\na prominent problem on the road of pursuing gen-\neral intelligence when such a large hidden state\ndimension is used. We also would like to know if\nmodels using multiple facets could reach a similar\nperformance by a smaller hidden state size.\nRecently, Gao et al. (2019a); Rajaee and Pilehvar\n(2021); Cai et al. (2021); Su et al. (2022) point out\nthe structure in the contextual embedding space\nprevents it from having an isotropic property. Our\nstudy and Demeter et al. (2020) show that the struc-\nture in the word embedding space only models the\nglobal similarity between words and prevents the\nLM from outputting arbitrary context-dependent\nword distributions. We would like to know if we\ncan discover a new LM architecture with a better\ncontextual/word embedding space that could bet-\nter model context-dependent word similarities and\nbalance it with the global word similarities. In ad-\ndition, our ﬁnding might be one of the reasons that\nwe can improve the language generation quality by\nencouraging word embedding to be more isotropic\n(Su et al., 2022).\nGao et al. (2019b) show that a mixture of kernel\nfunctions outperforms MoS. Mixtape (Yang et al.,\n2019) is another efﬁcient solution to the softmax\nbottleneck, whose hidden state for each word is the\nweighted average of the facets where the weights\nare dynamically predicted. If only using one soft-\nmax (i.e., K = 1), our multiple partition method\ncould be viewed as a special case of Mixtape that\nuses a global and binarized weight to prevent com-\nplications of predicting the weights of each word.\nOur results indicate that multiple partitions need to\nbe combined with multiple softmax layers in order\nto gain consistent performance improvement. A\npotential future direction is to compare MFS with\na mixture of kernel functions and Mixtape on the\ntransformer-based LMs or combine MFS with a\nmixture of kernel functions and Mixtape to gain\nfurther improvements.\nThe results in Kong et al. (2020) suggest that\npredicting n-grams could be better than predicting\nindividual words in BERT in some applications.\nThe total number of possible n-grams is several\norders of magnitude higher than the number of indi-\nvidual tokens in the vocabulary. In addition, the lin-\near dependency among n-grams might be common.\nFor example, the embedding of the brown color\n+ a dogmay be similar to the embedding of the\nbrown dog. The problem would be more serious as\nthe length of the prediction sequence (n) increases,\nso predicting the next sentence using a single em-\nbedding might suffer from the softmax bottleneck\neven more. Therefore, our solutions to softmax\nbottleneck may lead to a better phrase represen-\ntation or sentence representation in this type of\nself-supervised pretraining.\nFinally, language modeling is only an example of\nextreme classiﬁcation. The nearly ubiquitous usage\nof single embedding representation in the classi-\nﬁcation, self-supervised models (e.g., contrastive\nlearning models), or recommendation problems\nprovides many research opportunities. We believe\nthat our theoretical results could guide researchers\nto identify the potential applications where the soft-\nmax bottleneck is serious and multi-embedding rep-\nresentation is accordingly helpful.\n8073",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9677886962890625
    },
    {
      "name": "Softmax function",
      "score": 0.9662227630615234
    },
    {
      "name": "Computer science",
      "score": 0.7477627396583557
    },
    {
      "name": "Bottleneck",
      "score": 0.6989023089408875
    },
    {
      "name": "Language model",
      "score": 0.656679093837738
    },
    {
      "name": "Word (group theory)",
      "score": 0.6286632418632507
    },
    {
      "name": "Vocabulary",
      "score": 0.6007969379425049
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5122994184494019
    },
    {
      "name": "Bridging (networking)",
      "score": 0.47998282313346863
    },
    {
      "name": "Speech recognition",
      "score": 0.43072953820228577
    },
    {
      "name": "Natural language processing",
      "score": 0.42539846897125244
    },
    {
      "name": "Artificial neural network",
      "score": 0.32387131452560425
    },
    {
      "name": "Mathematics",
      "score": 0.13464540243148804
    },
    {
      "name": "Linguistics",
      "score": 0.08558624982833862
    },
    {
      "name": "Computer network",
      "score": 0.06933313608169556
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Embedded system",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ]
}