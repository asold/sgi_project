{
    "title": "Efficient Document Re-Ranking for Transformers by Precomputing Term Representations",
    "url": "https://openalex.org/W3021244424",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A3127480872",
            "name": "MacAvaney, Sean",
            "affiliations": [
                "Georgetown University"
            ]
        },
        {
            "id": "https://openalex.org/A2750213174",
            "name": "Nardini, Franco Maria",
            "affiliations": [
                "Istituto di Scienza e Tecnologie dell'Informazione \"Alessandro Faedo\""
            ]
        },
        {
            "id": "https://openalex.org/A2327220639",
            "name": "Perego, Raffaele",
            "affiliations": [
                "Istituto di Scienza e Tecnologie dell'Informazione \"Alessandro Faedo\""
            ]
        },
        {
            "id": "https://openalex.org/A2583561094",
            "name": "Tonellotto Nicola",
            "affiliations": [
                "University of Pisa"
            ]
        },
        {
            "id": "https://openalex.org/A3127493619",
            "name": "Goharian, Nazli",
            "affiliations": [
                "Georgetown University"
            ]
        },
        {
            "id": "https://openalex.org/A2754420870",
            "name": "Frieder, Ophir",
            "affiliations": [
                "Georgetown University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1685426458",
        "https://openalex.org/W2945127593",
        "https://openalex.org/W2566147423",
        "https://openalex.org/W2610935556",
        "https://openalex.org/W3001665736",
        "https://openalex.org/W2886518140",
        "https://openalex.org/W3001344098",
        "https://openalex.org/W2955732934",
        "https://openalex.org/W2096937925",
        "https://openalex.org/W2616330167",
        "https://openalex.org/W2964209691",
        "https://openalex.org/W4205951122",
        "https://openalex.org/W2000431947",
        "https://openalex.org/W2740321901",
        "https://openalex.org/W2912817604",
        "https://openalex.org/W2970103342",
        "https://openalex.org/W2897754576"
    ],
    "abstract": "Deep pretrained transformer networks are effective at various ranking tasks,\\nsuch as question answering and ad-hoc document ranking. However, their\\ncomputational expenses deem them cost-prohibitive in practice. Our proposed\\napproach, called PreTTR (Precomputing Transformer Term Representations),\\nconsiderably reduces the query-time latency of deep transformer networks (up to\\na 42x speedup on web document ranking) making these networks more practical to\\nuse in a real-time ranking scenario. Specifically, we precompute part of the\\ndocument term representations at indexing time (without a query), and merge\\nthem with the query representation at query time to compute the final ranking\\nscore. Due to the large size of the token representations, we also propose an\\neffective approach to reduce the storage requirement by training a compression\\nlayer to match attention scores. Our compression technique reduces the storage\\nrequired up to 95% and it can be applied without a substantial degradation in\\nranking performance.\\n",
    "full_text": "Efficient Document Re-Ranking for Transformers by\nPrecomputing Term Representations\nSean MacAvaney\nIR Lab, Georgetown University, USA\nsean@ir.cs.georgetown.edu\nFranco Maria Nardini\nISTI-CNR, Pisa, Italy\nfrancomaria.nardini@isti.cnr.it\nRaffaele Perego\nISTI-CNR, Pisa, Italy\nraffaele.perego@isti.cnr.it\nNicola Tonellotto\nUniversity of Pisa, Italy\nnicola.tonellotto@unipi.it\nNazli Goharian\nIR Lab, Georgetown University, USA\nnazli@ir.cs.georgetown.edu\nOphir Frieder\nIR Lab, Georgetown University, USA\nophir@ir.cs.georgetown.edu\nABSTRACT\nDeep pretrained transformer networks are effective at various rank-\ning tasks, such as question answering and ad-hoc document ranking.\nHowever, their computational expenses deem them cost-prohibitive\nin practice. Our proposed approach, called PreTTR (Precomput-\ning Transformer Term Representations), considerably reduces the\nquery-time latency of deep transformer networks (up to a 42×\nspeedup on web document ranking) making these networks more\npractical to use in a real-time ranking scenario. Specifically, we\nprecompute part of the document term representations at indexing\ntime (without a query), and merge them with the query represen-\ntation at query time to compute the final ranking score. Due to\nthe large size of the token representations, we also propose an\neffective approach to reduce the storage requirement by training\na compression layer to match attention scores. Our compression\ntechnique reduces the storage required up to 95% and it can be\napplied without a substantial degradation in ranking performance.\nACM Reference Format:\nSean MacAvaney, Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto,\nNazli Goharian, and Ophir Frieder. 2020. Efficient Document Re-Ranking\nfor Transformers by Precomputing Term Representations. In Proceedings of\nthe 43rd International ACM SIGIR Conference on Research and Development in\nInformation Retrieval (SIGIR ’20), July 25–30, 2020, Virtual Event, China. ACM,\nNew York, NY, USA, 10 pages. https://doi.org/10.1145/3397271.3401093\n1 INTRODUCTION\nPretrained deep transformer networks, e.g., BERT [8], have recently\nbeen transformative for many tasks, exceeding the effectiveness\nof prior art in many natural language processing and information\nretrieval tasks [ 4, 27, 31, 32, 47, 48]. However, these models are\nhuge in size, thus expensive to run. For instance, in about one\nyear, the largest pretrained transformer model grew from about\n110 million parameters (GPT [34]) to over 8.3 billion (Megatron-\nLM [39]), which, when applied to IR tasks like ad-hoc retrieval,\nhave substantial impact on the query processing performance, to\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\n© 2020 Association for Computing Machinery.\nACM ISBN 978-1-4503-8016-4/20/07. . . $15.00\nhttps://doi.org/10.1145/3397271.3401093\ndocument\nonly\ncombined\nquery \nonly\n1. Train time: ﬁne-tune masked transformer model for ranking\ntrain\ndata\n2. Index time: compute term representations\ndocument\ncollection\ndocument term\nrepresentations\n3. Query time: load term representations and compute ﬁnal score\nquery\nranking\nscore\nlayer 1\nlayer l\nlayer n\ntransformer\ncombined\nquery \nonly\ndocument\nonly\ndocument\nonly\ncombined\nquery \nonly\ndocument term\nrepresentations\nFigure 1: High-level overview of PreTTR. At query time, doc-\nument representations (which were computed at index time)\nare loaded, which reduces the computational burden.\nthe point of being impractical [27]. We move these neural ranking\nmodels towards practicality.\nRuntime efficiency is a central tenant in information retrieval,\nthough as neural approaches have gained prominence, their run-\nning time has been largely ignored in favor of gains in ranking\nperformance [16]. Recently, the natural language processing com-\nmunity has begun to consider and measure running time [37], albeit\nmostly for reasons of environmental friendliness and inclusiveness.\nChiefly, model distillation approaches [22, 36, 40] are prominent,\nwhich involve training a smaller model off of the predictions of a\nlarger model. This smaller model can then be further fine-tuned for\na specific task. While this approach can exceed the performance\nof a smaller model when only trained on the specific task data, it\ninherently limits the performance of the smaller model to that of\nthe larger model. Nevertheless, distillation is a method complemen-\ntary to ours; our approach can work with a distilled transformer\nnetwork. Others have explored quantization approaches to reduce\nmodel sizes, by limiting the number of bits used to represent net-\nwork’s parameters to 16, 8, or fewer bits. Quantization was mainly\narXiv:2004.14255v2  [cs.IR]  26 May 2020\nexplored to make the neural networks suitable for embedded sys-\ntems [11, 38]. We employ a basic quantization technique to reduce\nthe storage requirements of the term representations.\nWe propose a method for improving the efficiency of transformer-\nbased neural ranking models. We exploit a primary characteristic\nof ad-hoc ranking: an initial indexing phase can be employed to\npre-process documents in the collection to improve query-time per-\nformance. Specifically, we observe that much of the term interaction\nat query time happens locally within either the query or document,\nand only the last few layers of a deep transformer network are\nrequired to produce effective ranking scores once these represen-\ntations are built. Thus, documents can be processed at index time\nthrough part of the network without knowledge of the query. The\noutput of this partial network computation is a sequence of con-\ntextualized term representations. These representations can then\nbe stored and used at query time to finish the processing in con-\njunction with the query. This approach can be trained end-to-end\nby masking the attention across the query and document during\ntraining time (i.e., disallowing the document from attending to the\nquery and vice versa.) We call this approach PreTTR (Precomput-\ning Transformer Term Representations). A high-level overview of\nPreTTR is shown in Figure 1.\nAt train time, a transformer network is fine-tuned for ad-hoc doc-\nument ranking. This transformer network masks attention scores\nin the first l layers, disallowing interactions between the query\nand the document. At index time, each document in the collec-\ntion is processed through the first l layers, and the resulting term\nrepresentations are stored. At query time, the query is processed\nthrough the first l layers, and then combined with the document\nterm representations to finish the ranking score calculation.\nSince term representations of each layer can be large (e.g., 768\nfloat values per document term in the base version of BERT), we also\npropose a compression approach. This approach involves training\nan encoding layer between two transformer layers that produces\nrepresentations that can replicate the attention patterns exhibited\nby the original model. We experimentally show that all these pro-\ncesses result in a much faster network at query time, while having\nonly a minimal impact on the ranking performance and a reason-\nable change in index size. The settings of PreTTR (amount of pre-\ncomputation, degree of compression) can be adjusted depending\non the needs of the application. These are all critical findings that\nare required to allow transformer networks to be used in practical\nsearch environments. Specifically, the lower computation overhead\nreduces query-time latency of using transformer networks for rank-\ning, all while still yielding the substantial improvements to ranking\naccuracy that transformer-based rankers offer.\nIn summary, the contributions of the paper are the following:\n•A new method for improving the efficiency of transformer-\nbased neural ranking models (PreTTR). The approach exploits\nthe inverted index to store a precomputed term representation\nof documents used to improve query-time performance;\n•A novel technique for compressing the precomputed term\nrepresentations to reduce the storage burden introduced by\nPreTTR. This is accomplished by training a compression func-\ntion between transformer layers to minimize the difference\nbetween the attention scores with and without compression;\n•A comprehensive experimental evaluation of PreTTR on mul-\ntiple pre-trained transformer networks on two public datasets,\nnamely, TREC WebTrack 2012 and TREC Robust 2004. Our\nPreTTR accelerates the document re-ranking stage by up to\n42×on TREC WebTrack 2012, while maintaining compara-\nble P@20 performance. Moreover, our results show that our\ncompression technique can reduce the storage required by\nPreTTR by up to 97.5% without a substantial degradation in\nthe ranking performance;\n•For reproducibility, our code is integrated into OpenNIR [26],\nwith instructions and trained models available at:\nhttps://github.com/Georgetown-IR-Lab/prettr-neural-ir.\n2 RELATED WORK\nWe present an overview of neural ranking techniques, pretrained\ntransformers for ranking, and efforts to optimize the efficiency of\nsuch networks.\n2.1 Neural Ranking\nAs neural approaches have gained prominence in other disciplines,\nmany have investigated how deep neural networks can be applied\nto document ranking [10, 17, 19, 44]. These approaches typically act\nas a final-stage ranking function, via a telescoping (also referred to\nas cascading, or multi-stage) technique [29, 43]; that is, initial rank-\ning is conducted with less expensive approaches (e.g., BM25), with\nthe final ranking score calculated by the more expensive machine-\nlearned functions. This technique is employed in commercial web\nsearch engines [35]. Neural ranking approaches can broadly be cat-\negorized into two categories: representation-focused and interaction-\nfocused models. Representation-focused models, such as DSSM [17],\naim to build a dense “semantic” representation of the query and the\ndocument, which can be compared to predict relevance. This is akin\nto traditional vector space models, with the catch that the vectors\nare learned functions from training data. Interaction models, on\nthe other hand, learn patterns indicative of relevance. For instance,\nPACRR [19] learns soft n-gram matches in the text, and KNRM [44]\nlearns matching kernels based on word similarity scores between\nthe query and the document.\n2.2 Pretrained Transformers for Ranking\nSince the rise of pretrained transformer networks (e.g., BERT [8]),\nseveral have demonstrated their effectiveness on ranking tasks.\nNogueira and Cho [31] demonstrated that BERT was effective at\npassage re-ranking (namely on the MS-MARCO and TREC CAR\ndatasets) by fine-tuning the model to classify the query and passage\npair as relevant or non-relevant. Yang et al . [47] used BERT in\nan end-to-end question-answering pipeline. In this setting, they\npredict the spans of text that answer the question (same setting as\ndemonstrated on SQuAD in [8]). MacAvaney et al. [27] extended\nthat BERT is effective at document ranking, both in the “vanilla”\nsetting (learning a ranking score from the model directly) and when\nusing the term representations from BERT with existing neural\nranking architectures (CEDR). Dai and Callan [4] found that the\nadditional context given by natural language queries (e.g., topic\ndescriptions) can improve document ranking performance, when\ncompared with keyword-based queries. Yang et al. [48] showed that\nBERT scores aggregated by sentence can be effective for ranking.\nDoc2Query [32] employs a transformer network at index time to\nadd terms to documents for passage retrieval. The authors also\ndemonstrate that a BERT-based re-ranker can be employed atop\nthis index to further improve ranking performance.\n2.3 Neural Network Efficiency\nPretrained transformer networks are usually characterized by a\nvery large numbers of parameters and very long inference times,\nmaking them unusable in production-ready IR systems such as\nweb search engines. Several approaches were proposed to reduce\nthe model size and the inference computation time in transformer\nnetworks [12]. Most of them focus on the compression of the neural\nnetwork to reduce their complexity and, consequently, to reduce\ntheir inference time.\nNeural network pruning consists of removing weights and acti-\nvation functions in a neural network to reduce the memory needed\nto store the network parameters. The objective of pruning is to\nconvert the weight matrix of a dense neural network to a sparse\nstructure, which can be stored and processed more efficiently. Prun-\ning techniques work both at learning time and as a post-learning\nstep. In the first category, Pan et al. propose regularization tech-\nniques focused at removing redundant neurons at training time [33].\nAlternatively, in the second category, Han et al. propose to remove\nthe smallest weights in terms of magnitude and their associated\nedges to shrink the size of the network [13]. Conversely, our pro-\nposed approach does not change the dense structure of a neural\nnetwork to a sparser representation, but it aims to precompute\nthe term representation of some layers, thus completely removing\nthe document-only portion of a transformer neural network (see\nFigure 1).\nAnother research line focuses on improving the efficiency of a\nnetwork is weight quantization. The techniques in this area aim\nat reducing the number of bits necessary to represent the model\nweights: from the32 bits necessary to represent a float to only a few\nbits [18]. The state of the art network quantization techniques [1, 45]\naims at quantizing the network weights using just 2-3 bits per\nparameter. These approaches proved effective on convolutional\nand recurrent neural networks. Quantization strategies could be\nused in our proposed approach. However, to reduce the size of the\nterm representations, we opt to instead focus on approaches to\nreduce the dimensionality of the term representations, and leave\nquantization of the stored embeddings to future work.\nA third research line employed to speed-up neural networks\nis knowledge distillation [15]. It aims to transform the knowledge\nembedded in a large network (called teacher) into a smaller network\n(called student). The student network is trained to reproduce the\nresults of the teacher networks using a simpler network structure,\nwith less parameters than those used in the teacher network. Several\nstrategies have been proposed to distill knowledge in pretrained\ntransformer networks such as BERT [22, 36, 40].\nOur PreTTR method is orthogonal to knowledge distillation of\ntransformer network. In fact, our approach can be applied directly\nto any kind of transformer, including those produced by knowledge\ndistillation.\nTable 1: Table of symbols.\nSymbol(s) Definition\nq Query\nd Document\nR(q, d) Neural ranking architecture\nT (s) Transformer network\ns a sequence of input tokens\nE Embedding layer\nLi Transformer encoding layer\nsi Transformer token representations after layer i\nai Attention weights used in layer i\nc Classification representation\nd Dimension of the classification representation\nm Length of sequence s\nh Number of attention heads per layer\nn Number of layers in T\nWcombine Vanilla BERT weight combination\nl Layer number the transformer is executed for\nprecomputing document term vectors\ne Compressed size\nr Compressed representation after layer l\nW /bcomp Compression parameters\nW /bdecomp De-compression parameters\nˆsl De-compressed representation after layer l\n2.4 Neural Ranking Efficiency\nScalability and computational efficiency are central challenges in\ninformation retrieval. While the efficiency of learning to rank\nsolutions for document re-ranking have been extensively stud-\nied [6, 24, 41], computational efficiency concerns have largely be\nignored by prior work in neural ranking, prompting some to call\nfor more attention to this matter [ 16]. That being said, some ef-\nforts do exist. For instance, Zamani et al. [50] investigate learning\nsparse query and document representations which allow for index-\ning. Ji et al. [21] demonstrate that Locality-Sensitive Hashing (LSH)\nand other tricks can be employed to improve the performance of\ninteraction-focused methods such as DRMM [10], KNRM [44], and\nConvKNRM [5]. This approach does not work for transformer mod-\nels, however, because further processing of the term embeddings\nis required (rather than only computing similarity scores between\nthe query and document).\nWithin the realm of transformer-based models for ad-hoc rank-\ning, to our knowledge only [27] and [32] acknowledge that retrieval\nspeed is substantially impacted by using a deep transformer net-\nwork. As a result Hofstätter and Hanbury [16] call for more atten-\ntion to be paid to run time. MacAvaney et al. find that limiting the\ndepth of the transformer network can reduce the re-ranking time\nwhile yielding comparable ranking performance [ 27]. Nogueira\net al. find that their approach is faster than a transformer-based\nre-ranker, but it comes at a great cost to ranking performance: a\ntrade-off that they state can be worthwhile in some situations [32].\nIn contrast with both these approaches, we employpart of the trans-\nformer network at index time, and the remainder at query-time\n(for re-ranking). We find that this can yield performance on par\nwith the full network, while significantly reducing the query time\nlatency.\n3 MOTIVATION\nLet a generic transformer network T : s 7→c map a sequence s\nof m tokens (e.g., query and document terms) to a d-dimensional\n[CLS] [tax] [evade] [world][news] [for] [tax] [fraud][today] [SEP]…[SEP]\nquery document\nembed.\nlayer 1\nlayer l\ncomp.\nlayer l+1\n↦↤\nlayer n\n… … …\n…\nWcombine\nranking score\ntokens\n… …\n↦↤ ↦↤ ↦↤ ↦↤ ↦↤ ↦↤\n↤↦ ↤↦ ↤↦ ↤↦ ↤↦ ↤↦ ↤↦…decomp.\n↦↤ ↦↤ ↦↤ ↦↤\n↤↦ ↤↦ ↤↦ ↤↦\nStorage\nFigure 2: Overview of PreTTR. Compressed term represen-\ntations for document layers 1 to l are computed and stored\nat index time (green segments) while term representations\nfor query layers 1 to l (orange segments) and joint query-\ndocument representations for layers l + 1 to n (blue seg-\nments) are computed at query time to produce the final rank-\ning score. Compression and decompression can optionally\nbe applied between layers l and l + 1 to reduce the storage\nneeded for the document term representations.\noutput representation c ∈Rd . As depicted in Figure 2, the trans-\nformer network is composed by an initial embedding layer E and\nby n layers L1, . . . ,Ln. The embedding layer E maps each of the m\ninput tokens into the initial d-dimensional token representations\nmatrix s0 ∈Rm×d . Each layer Li takes the token representations\nmatrix si−1 ∈Rm×d from the previous layer Li−1 and produces a\nnew representations matrix si ∈Rm×d . The specific representation\nused and operations performed in E and Li depend on the specific\ntransformer architecture (e.g., BERT uses token, segment, and po-\nsition embeddings for the embedding layer E and self-attention,\na feed-forward layer, and batch normalization in each layer Li ).\nHowever, the primary and common component of each layer Li\nis the self-attention mechanism and associated procedure. When\nthe transformer network is trained, every layer produces a self-\nattention tensor ai ∈Rh×m×m, where h is the number of attention\nheads per layer, i.e., the number of attention “representation sub-\nspaces” per layer. A general description of this process is given by\nVaswani et al. [42], while different transformer architectures may\nhave tweaks to this general structure or pre-training procedure.\nWe assume a special output classification token, e.g., [CLS] in\nBERT, is included as a token in c, and that the final representation\nof this token is used as the final output of the transformer network,\ni.e., c = T (s). Without loss of generality, here we only concern\nourselves with the [CLS] output classification token, i.e., we ig-\nnore other token representation outputs; this is the special token\nrepresentation that models such as BERT use to generate ranking\nscores.\nWe illustrate how neural transformer networks are used in a\nranking scenario. We follow the Vanilla BERT model proposed by\nMacAvaney et al. [27] and generalize it. Let a ranking function\nR(q, d)∈ R map a query q and a document d to a real-valued rank-\ning score. Neural rankers based on transformer networks such as\nVanilla BERT compute the ranking score by feeding the query-\ndocument pair into the transformer. Given a query q and a docu-\nment d, their tokens are concatenated into a suitable transformer\ninput, e.g., s = [CLS]; q; [SEP]; d; [SEP], where “;” represents the\nconcatenation operator.1 The output of the transformer network\ncorresponding to this input is then linearly combined using a tuned\nweight matrixWcombine ∈Rd×1 to compute the final ranking score\nas follows:\nR(q, d)= T \u0000[CLS]; q; [SEP]; d; [SEP]\u0001Wcombine . (1)\nThe processing time of state-of-the-art neural rankers based on\ntransformer networks is very high, e.g., approximately 50 docu-\nments ranked per second on a modern GPU, making such rankers\nimpractical for most ad-hoc retrieval tasks.\nTo gain an understanding of where are the most expensive com-\nponents of a transformer network such as the Vanilla BERT model,\nwe measure the run-times of the main steps of the model. We find\nthat most of the processing is performed in the computations in-\nvolving the transformer’s layers. In particular, about 50% of the\ntotal time is spent performing attention-related tasks. Moreover,\nthe feed-forward step of the transformer (consisting of intermediate\nand output in diagram) accounts for about 48% of the total time,\nand is largely due to the large intermediate hidden representation\nsize for each token. This breakdown motivates the investigation\nof possible solutions to reduce the processing time of transformer\nnetworks, in particular in reducing the time spent in traversing the\ntransformer’s layers.\n4 PROPOSED SOLUTION\nWe discuss how our PreTTR approach improve the efficiency of\nprocessing queries using a transformer network by reducing the\ncomputational impact of the network’s layers.\n4.1 PreTTR: Precomputing Transformer Term\nRepresentations\nWe improve the query time performance of transformer models\nby precomputing document term representations partially through\nthe transformer network (up to transformer layer l). We then use\nthese representations at query time to complete the execution of\nthe network when the query is known.\nThis is accomplished at model training time by applying an atten-\ntion mask to layersL1, L2, . . . ,Ll , in which terms from the query are\nnot permitted to attend to terms from the document and vice versa.\nIn layers Ll+1, . . . ,Ln, this attention mask is removed, permitting\nany token to attend to any other token. Once trained, the model is\nused at both index and query time. At index time, documents are\nencoded (including the trailing [SEP] token)2 by the transformer\nmodel through layers L1, L2, . . . ,Ll without a query present (Fig-\nure 2, green segments). The token representations generated at\nindex time at layer Ll are then stored to be reused at query time\n(Figure 2, document storage between layersLl and Ll+1). To answer\na query, candidate documents are selected, e.g., the top documents\n1We use the BERT convention of [CLS] and [SEP] to represent the classification and\nseparation tokens, respectively.\n2There is evidence that the separator token performs an important function for pre-\ntrained transformer models, by acting as a no-op for the self-attention mechanism [2].\nretrieved by a first-stage simple ranking model [41], and precom-\nputed term representations are loaded. The query terms (including\nthe leading [CLS] and training [SEP] tokens) are encoded up to\nlayer Ll without a document present (Figure 2, orange segments).\nThen, the representations from the query and the document are\njoined, and the remainder of the transformer network is executed\nover the entire sequence to produce a ranking score (Figure 2, blue\nsegments).\nSince (1) the length of a query is typically much shorter than the\nlength of a document, (2) the query representations can be re-used\nfor each document being ranked, (3) each transformer layer takes\nabout the same amount of time to execute, and (4) the time needed\nto perform term embedding is comparatively low, PreTTR decreases\nby about n−l\nn the cost of traversing the transformer network layers.\nWith a sufficiently large value of l, this results in considerable\ntime savings. Note that this reduction can be at most equal to 1\nn\nbecause, when l = n, no information about the document ever\ncontributes to the ranking score, resulting in identical scores for\nevery document. Moreover, we show experimentally that this can\nbe further improved by limiting the computation of the final layer\nto only the [CLS] representation.\n4.2 Token Representation Compression\nAlthough PreTTR can reduce the run-time cost of traversing the\nfirst l layers of the transformer network at query time, the solution\nproposed might be costly in terms of storage requirements because\nthe representation size d is quite large (e.g., 1024, 768 or 512 float\nvalues per token). To address this issue, we propose a new token\ncompression technique that involves pre-training a simple encoder-\ndecoder network. This network is able to considerably reduce the\ntoken representation size. We opt for this approach because it can\nfit seamlessly into the transformer network, while reducing the\nnumber of dimensions needed to represent each token. The com-\npressor is added as an additional component of the transformer\nnetwork between layers Ll and Ll+1. We compress the input by\nusing a simple feed-forward and normalization procedure, identical\nto the one used within a BERT layer to transform the output (but\nwith a smaller internal representation rather than a larger one). We\noptimize the weights for the compression network in two stages: (1)\nan initial pre-training stage on unlabeled data, and (2) a fine-tuning\nstage when optimizing for relevance.\nFor a compressed size of e values, a two-step procedure is used.\nFirst, the compressed representations r ∈Rm×e are built using\nr = gelu(slWcomp + bcomp ), where gelu(·)is a Gaussian Error\nLinear Unit [14], and Wcomp ∈Rd×e and bcomp ∈Re are the new\nlearned weight parameters. These compressed representations r\ncan be stored in place of sl . Second, the compressed representa-\ntions r are then expanded back out toˆsl ∈Rm×d via a second linear\ntransformation involving the learned weight parameters Wdecomp ,\nbdecomp , and batch normalization. The decompressed representa-\ntions ˆsl are then used in place of the original representation sl for\nthe remaining layers of the transformer.\nIn preliminary experiments, we found the compression and de-\ncompression parameters to be difficult to learn jointly with the\nranker itself. Thus, we instead propose a pre-training approach to\nprovide an effective initialization of these parameters. We want the\ntransformer network with the compression mechanism to behave\nsimilarly to that of the network without such compression: we do\nnot necessarily care about the exact representations themselves.\nThus, we use an attention-based loss function. More specifically, we\noptimize our compression/decompression network to reduce the\nmean squared error of the attention scores in the lastn −l layers of\nthe compressed transformer network and the original transformer\nnetwork. Thus, the loss function we use to train our compression\nand decompression network is:\nL(al+1, . . . , an, ˆal+1, . . . , ˆan )= 1\nn −l\nnÕ\ni=l+1\nMSE(ai, ˆai ), (2)\nwhere ai represents the attention scores at layeri from the unmodi-\nfied transformer network, ˆai represents the attention scores at layer\ni from the transformer network with the compression unit, and\nMSE(·)is the mean squared error function. With this loss function,\nthe weights can be pre-trained on a massive amount of unlabeled\ntext. We use this procedure as an initial pre-training step; we further\nfine-tune the weights when optimizing the entire ranking network\nfor relevance.\n5 EXPERIMENTAL SETUP\nWe detail the setup employed in our experiments: the datasets,\nnamely TREC WebTrack 2012 and TREC Robust 2004, and the trans-\nformer networks we use, i.e., Vanilla BERT and some of its variants.\nThen, we discuss the training procedure adopted in training the\ntransformer networks and our proposed compression/decompression\ntechnique. Details about the evaluation metrics and the baselines\nused conclude the section.\n5.1 Datasets\nWe test PreTTR on two datasets, namely TREC WebTrack 2012 and\nTREC Robust 2004. Table 2 summarizes some salient statistics about\nthe two datasets.\nTable 2: Datasets characteristics.\nWebTrack 2012 Robust 2004\nDomain Web Newswire\nDocument collection ClueWeb09-B TREC Disks 4 & 5\n# Queries 50 249\n# Documents 50M 528k\nTokens / query 2.0 2.7\nJudgments / query 321 1.2k\nThe TREC WebTrack 2012 dataset consists of web queries and\nrelevance judgments from the ClueWeb09-B document collection.\nWe use relevance judgments from 2012 for test and the ones from\n2011 for validation. The relevance judgments available from the\nremaining years of the TREC WebTrack, i.e., 2009, 2010, 2013, and\n2014 are used for training. Note that, while the TREC WebTrack\n2009–12 have been evaluated on the ClueWeb09-B document col-\nlection, the TREC WebTrack 2013–14 have been evaluated on the\nClueWeb12 [19] document collection.3 We generate the training\nsamples by using the corresponding document collection. This is\n3https://lemurproject.org/clueweb09/ and https://lemurproject.org/clueweb12/.\nthe setup used by several other works on TREC WebTrack 2012,\ne.g., [19, 27].\nTREC Robust 2004 consists of 249 news queries. For these ex-\nperiments, we use a standard k-fold evaluation (k = 5) where each\niteration uses three folds for training, one for validation, and a final\nheld-out fold for testing. We perform this evaluation by using the\nfive folds provided by Huston and Croft [20].\n5.2 Transformer Networks\nWe use the Vanilla transformer model from [27]. This model yields\ncomparable performance to other leading formulations, while being\nsimpler, e.g., no paragraph segmentation required, as is needed by\nFirstP/MaxP/SumP [4], or alternative training datasets and sentence\nsegmentation, as required by the system of Yang et al. [48]. Vanilla\nBERT encodes as much of the document as possible (adhering to\nthe transformer maximum input length constraint), and averages\nthe classification embeddings when multiple document segments\nare required. We employ the same optimal hyper-parameters for\nthe model presented in [27]. For our primary experiments, we use\nthe pretrained bert-base-uncased [8]. We do not test with the\nlarge variants of BERT because the larger model exhibits only\nmarginal gains for ranking tasks, while being considerably more\nexpensive to run [31]. To show the generality of our approach we\npresent tests conducted also for other pretrained transformers in\nSection 6.5: a version of BERT that was more effectively pre-trained,\ni.e., RoBERTa [25] (roberta-base) and a smaller (distilled) version\nof BERT, i.e., DistilBERT [36] (distilbert-base-uncased).\n5.3 Training\nWe train all transformer models using pairwise softmax loss [7] and\nthe Adam optimizer [23] with a learning rate of2×10−5. We employ\na batch size of16 pairs of relevant and non-relevant documents with\ngradient accumulation. Training pairs are selected randomly from\nthe top-ranked documents in the training set, where documents\nthat are labeled as relevant are treated as positive, and other top-\nranked documents are considered negative. Every 32 batches, the\nmodel is validated, and the model yielding the highest performance\non the validation set is selected for final evaluation.\nFor training the document term compressor/decompressor (as\ndescribed in Section 4.2), we use the Wikipedia text from the TREC\nComplex Answer Retrieval (CAR) dataset [9] (version 2.0 release).\nThis dataset was chosen because it overlaps with the data on which\nBERT was originally trained on, i.e., Wikipedia, and was used both\nfor evaluation of passage ranking approaches [30] and as a weak\nsupervision dataset for training neural models [ 28]. We sample\ntext pairs using combinations of headings and paragraphs. Half the\npairs use the heading associated with the paragraph, and the other\nhalf use a random heading from a different article, akin to the next\nsentence classification used in BERT pre-training. The compres-\nsion and decompression parameters (Wcomp , bcomp , Wdecomp , and\nbdecomp ) are trained to minimize the difference in attention scores,\nas formulated in Eq. (2). We found that the compressor training\nprocess converged by 2M samples.\n5.4 Evaluation\nSince the transformer network is employed as a final-stage re-\nranker, we evaluate the performance of our approach on each\ndataset using two precision-oriented metrics. Our primary metric\nfor both datasets is P@20 (also used for model validation). Following\nthe evaluation convention from prior work [27], we use ERR@20\nfor TREC WebTrack 2012 and nDCG@20 for TREC Robust 2004 as\nsecondary metrics.\nWe also evaluate the query-time latency of the models. We con-\nduct these experiments using commodity hardware: one GeForce\nGTX 1080 Ti GPU. To control for factors such as disk latency, we\nassume the model and term representations are already loaded in\nthe main memory. In other words, we focus on the impact of the\nmodel computation itself. However, the time spent moving the data\nto and from the GPU memory is included in the time.\n5.5 Baselines\nThe focus of this work is to reduce the query-time latency of using\nVanilla transformer models, which are among the state-of-the-art\nneural ranking approaches. Thus, our primary baseline is the un-\nmodified Vanilla transformer network. To put the results in con-\ntext, we also include the BM25 results tuned on the same training\ndata. We tune BM25 using grid search with Anserini’s implementa-\ntion [46], over k1 in the range of 0.1–4.0 (by 0.1) and b in the range\nof 0.1–1.0 (by 0.1). We also report results for CEDR-KNRM [ 27],\nwhich outperform the Vanilla transformer approaches. However,\nit come with its own query-time challenges. Specifically, since it\nuses the term representations from every layer of the transformer,\nthis would require considerably more storage. To keep our focus\non the typical approach, i.e., using the [CLS] representation for\nranking, we leave it to future work to investigate ways in which to\noptimize the CEDR model.4 We also report results for Birch [49],\nwhich exploits transfer learning from the TREC Microblog dataset.\nTo keep the focus of this work on the effect of pre-computation, we\nopt to evaluate in the single-domain setting.\n6 RESULTS AND DISCUSSION\nWe report the results of a comprehensive experimental evaluation\nof the proposed PreTTR approach. In particular, we aim at investi-\ngating the following research questions:\nRQ1 What is the impact of PreTTR on the effectiveness of the\nVanilla BERT transformer network in ad-hoc ranking? (Sec-\ntion 6.1)\nRQ2 What is the impact of the token representation compression\non the effectiveness of PreTTR? (Section 6.2)\nRQ3 What is the impact of the proposed PreTTR approach on the\nefficiency of Vanilla BERT when deployed as a second stage\nre-ranker? (Section 6.3)\nRQ4 What is the impact of PreTTR when applied to first n −1\nlayers of a transformer network? (Section 6.4)\nRQ5 What is the impact of PreTTR when applied to different\ntransformer networks such as RoBERTA and DistilBERT?\n(Section 6.5)\n4We note that techniques such as LSH hashing can reduce the storage requirements for\nCEDR, as it uses the representations to compute query-document similarity matrices,\nas demonstrated by [21].\n6.1 Precomputing Transformer Term\nRepresentations\nTo answer RQ1 we first evaluate the effect of the precomputation\nof term representations. Table 3 provides a summary of the ranking\nperformance of PreTTR-based Vanilla BERT at layer l. At lower\nvalues of l, the ranking effectiveness remains relatively stable, de-\nspite some minor fluctuations. We note that these fluctuations are\nnot statistically significant when compared with the base model\n(paired t-test, 99% confidence interval) and remain considerably\nhigher than the tuned BM25 model. We also tested using a two one-\nsided equivalence (TOST) and found similar trends (i.e., typically\nthe the significant differences did not exhibit significant equiva-\nlence.) In the case of TREC WebTrack 2012, the model achieves\ncomparable P@20 performance w.r.t. the base model with only a\nsingle transformer layer (12), while the first 11 layers are precom-\nputed. Interestingly, the ERR@20 suffers more than P@20 as more\nlayers are precomputed. This suggests that the model is able to iden-\ntify generally-relevant documents very effectively with only a few\ntransformer layers, but more are required to be able to identify the\nsubtleties that contribute to greater or lesser degrees of relevance.\nAlthough it would ideally be best to have comparable ERR@20 per-\nformance in addition to P@20, the substantial improvements that\nthis approach offers in terms of query-time latency (see Section 6.3)\nmay make the trade-off worth it, depending on the needs of the\napplication.\nOn the TREC Robust 2004 newswire collection, precomputing the\nfirst 10 layers yields comparable P@20 performance w.r.t. the base\nmodel. Interestingly, although l = 11 yields a relatively effective\nmodel for WebTrack, Robust performance significantly suffers in\nthis setting, falling well below the BM25 baseline. We also observe\na significant drop in nDCG@20 performance at l = 8, while P@20\nperformance remains stable until l = 11. This is similar to the\nbehavior observed on WebTrack: as more layers are precomputed,\nthe model has a more difficult time distinguishing graded relevance.\nWe observe that the highest-performing models (metric in bold)\nare not always the base model. However, we note that these scores\ndo not exhibit statistically significant differences when compared\nto the base model.\nIn summary, we answer RQ1 by showing that Vanilla BERT\ncan be successfully trained by limiting the interaction between\nquery terms and document terms, and that this can have only a\nminimal impact on ranking effectiveness, particularly in terms in\nthe precision of top-ranked documents. This is an important result\nbecause it shows that document term representations can be built\nindependently of the query at index time.\n6.2 Term Representation Compression\nTo answer RQ2, we run the Vanilla BERT model with varying sizes\ne of the compressed embedding representations over the combina-\ntion layers l that give the most benefit to query latency time (i.e.,\nl = 7, 8, 9, 10, 11). Layers l ≤6 are not considered because they pro-\nvide less computational benefit (taking about one second or more\nper 100 documents, see Section 6.3). See Table 4 for a summary of\nthe results on TREC WebTrack 2012 and Robust 2004. We find that\nthe representations can usually be compressed down to at least\ne = 256 (67% of the original dimension of 768) without substantial\nTable 3: Breakdown of ranking performance when using a\nPreTTR-based Vanilla BERT ranking, joining the encodings\nat layer l. Statistically significant differences with the base\nmodel are indicated by ↓(paired t-test by query, p < 0.01).\nWebTrack 2012 Robust 2004\nRanker P@20 ERR@20 P@20 nDCG@20\nBase 0.3460 0.2767 0.3784 0.4357\nl = 1 0.3270 0.2831 0.3851 0.4401\nl = 2 0.3170 0.2497 0.3821 0.4374\nl = 3 0.3440 0.2268 0.3859 0.4386\nl = 4 0.3280 0.2399 0.3701 0.4212\nl = 5 0.3180 0.2170 0.3731 0.4214\nl = 6 0.3270 0.2563 0.3663 0.4156\nl = 7 0.3180 0.2255 0.3656 0.4139\nl = 8 0.3140 0.2344 0.3636 ↓0.4123\nl = 9 0.3130 0.2297 0.3644 ↓0.4106\nl = 10 0.3360 0.2295 0.3579 ↓0.4039\nl = 11 0.3380 ↓0.1940 ↓0.2534 ↓0.2590\nTuned BM25 0.2370 0.1418 0.3123 0.4140\nVanilla BERT [27] - - 0.4042 0.4541\nCEDR-KNRM [27] - - 0.4667 0.5381\nBirch [49] - - 0.4669 0.5325\nloss in ranking effectiveness. In Robust, we observe a sharp drop\nin performance at e = 128 (83% dimension compression) at layers\n7–10. There is no clear pattern for which compression size is most\neffective for WebTrack 2012. Note that these differences are gener-\nally not statistically significant. This table shows that, to a point,\nthere is a trade-off between the size of the stored representations\nand the effectiveness of the ranker.\nWithout any intervention, approximately 112TB of storage would\nbe required to store the full term vectors for ClueWeb09-B (the docu-\nment collection for TREC WebTrack 2012). For web collections, this\ncan be substantially reduced by eliminating undesirable pages, such\nas spam. Using recommended settings for the spam filtering ap-\nproach proposed by Cormack et al. [3]for ClueWeb09-B, the size can\nbe reduced to about 34TB. Using our compression/decompression\napproach, the storage needed can be further reduced, depending\non the trade-off of storage, query-time latency, and storage require-\nments. If using a dimension e = 128 for the compressed representa-\ntion (with no statistically significant differences in effectiveness on\nWebTrack), the size is further reduced to 5.7TB, which yields a 95%\nof space reduction. We also observed that there is little performance\nimpact by using 16-bit floating point representations, which further\nreduces the space to about 2.8TB. Although this is still a tall order,\nit is only about 2.5% of the original size, and in the realm of reason-\nable possibilities. We leave it to future work to investigate further\ncompression techniques, such as kernel density estimation-based\nquantization [38].\nSince the size scales with the number of documents, the storage\nrequirements are far less for smaller document collections such as\nnewswire. Document representations for the TREC Disks 4 & 5 (the\ndocument collection for the Robust 2004) can be stored in about\nTable 4: Ranking performance at various compression sizes. Statistically significant increases and decreases in ranking perfor-\nmance (compared to the model without compression) are indicated with ↑and ↓, respectively (paired t-test by query, p < 0.01).\nWe mark columns with * to indicate cases in which the uncompressed model (none) significantly underperforms the Base\nmodel performance (from Table 3).\nTREC WebTrack 2012\nP@20 ERR@20\nCompression l = 7 l = 8 l = 9 l = 10 l = 11 l = 7 l = 8 l = 9 l = 10 * l = 11\n(none) 0.3180 0.3140 0.3130 0.3360 0.3380 0.2255 0.2344 0.2297 0.2295 0.1940\ne = 384 (50%) 0.3430 0.3260 0.2980 0.3360 0.3090 0.2086 0.2338 0.1685 0.2233 0.2231\ne = 256 (67%) 0.3380 0.3120 ↑0.3440 0.3260 0.3250 ↑0.2716 0.2034 ↑0.2918 0.1909 0.2189\ne = 128 (83%) 0.3100 0.3210 0.3320 0.3220 0.3370 0.2114 0.2234 0.2519 0.2239 0.2130\nTREC Robust 2004\nP@20 nDCG@20\nCompression l = 7 l = 8 l = 9 l = 10 * l = 11 l = 7 * l = 8 * l = 9 * l = 10 * l = 11\n(none) 0.3656 0.3636 0.3644 0.3579 0.2534 0.4139 0.4123 0.4106 0.4039 0.2590\ne = 384 (50%) 0.3587 ↓0.3369 ↓0.3435 0.3522 0.2687 0.4098 ↓0.3720 ↓0.3812 0.3895 ↑0.2807\ne = 256 (67%) ↓0.2950 0.3623 ↓0.2695 0.3535 0.2635 ↓0.3130 0.4074 ↓0.2753 0.3983 0.2694\ne = 128 (83%) ↓0.2461 ↓0.2530 ↓0.2499 ↓0.2607 0.2655 ↓0.2454 ↓0.2568 ↓0.2533 ↓0.2608 0.2713\nTable 5: Vanilla BERT query-time latency measurements for\nre-ranking the top 100 documents on TREC WebTrack 2012\nand TREC Robust 2004. The latency is broken down into\ntime to compute query representations up through layer l,\nthe time to decompress document term representations, and\nthe time to combine the query and document representa-\ntions from layer l + 1 to layer n. The l = 11 setting yields\na 42 ×speedup for TREC WebTrack, while not significantly\nreducing the ranking performance.\nTREC WebTrack 2012 Robust04\nRanker Total Speedup Query Decom. Combine Total\nBase 1.941s (1.0 ×) - - - 2.437s\nl = 1 1.768s (1.1 ×) 2ms 10ms 1.756s 2.222s\nl = 2 1.598s (1.2 ×) 3ms 10ms 1.585s 2.008s\nl = 3 1.423s (1.4 ×) 5ms 10ms 1.409s 1.792s\nl = 4 1.253s (1.5 ×) 6ms 10ms 1.238s 1.575s\nl = 5 1.080s (1.8 ×) 7ms 10ms 1.063s 1.356s\nl = 6 0.906s (2.1 ×) 9ms 10ms 0.887s 1.138s\nl = 7 0.735s (2.6 ×) 10ms 10ms 0.715s 0.922s\nl = 8 0.562s (3.5 ×) 11ms 10ms 0.541s 0.704s\nl = 9 0.391s (5.0 ×) 12ms 10ms 0.368s 0.479s\nl = 10 0.218s (8.9 ×) 14ms 10ms 0.194s 0.266s\nl = 11 0.046s (42.2 ×) 15ms 10ms 0.021s 0.053s\n195GB, without any filtering and using the more effective e = 256\nfor the dimension of the compressed representation.\nIn summary, regarding RQ2, we show that, through our compres-\nsion technique, one can reduce the storage requirements of PreTTR.\nWith a well-trained compression and decompression weights, this\ncan have minimal impact on ranking effectiveness.\n6.3 Re-ranking Efficiency\nThe reduction of the re-ranking latency achieved by our proposed\nPreTTR is considerable. To answer RQ3, in Table 5 we report an anal-\nysis of the re-ranking latency of PreTTR-based Vanilla BERT when\nprecomputing the token representations at a specific layer l and a\ncomparison against the base model, i.e., Vanilla BERT. Without our\napproach, re-ranking the top 100 results for a query using Vanilla\nBERT takes around 2 seconds. Instead, when using PreTTR-based\nVanilla BERT at layerl = 11, which yields comparable P@20 per-\nformance to the base model on the TREC WebTrack 2012 collection,\nthe re-ranking process takes 46 milliseconds for 100 documents,\ni.e., we achieve a 42.0×speedup. One reason this performance is\nachievable is because the final layer of the transformer network\ndoes not need to compute the representations for each token; only\nthe representations for the [CLS] token are needed, since it is the\nonly token used to compute the final ranking score. Thus, the calcu-\nlation of a full self-attention matrix is not required. Since the[CLS]\nrepresentation is built in conjunction with the query, it alone can\ncontain a summary of the query terms. Furthermore, since the query\nrepresentation in the first l layers is independent of the document,\nthese representations are re-used among all the documents that are\nre-ranked. Of the time spent during re-ranking for l = 11, 32% of\nthe time is spent building the query term representation, 21% of the\ntime is spent decompressing the document term representations,\nand the remainder of the time is spent combining the query and\ndocument representations. Moreover, when using PreTTR-based\nVanilla BERT at layer l = 10, the transformer network needs to\nperform a round of computations on all the term representations.\nNevertheless, in this case, our PreTTR approach leads to a sub-\nstantial speedup of 8.9×w.r.t. Vanilla BERT. We also observe that\nthe time to decompress the term representations (with e = 256)\nremains a constant overhead, as expected. We observe a similar\ntrend when timing the performance of Robust 2004, though we\nwould recommend using l ≤10 for this dataset, as l = 11 performs\npoorly in terms of ranking effectiveness. Nonetheless, at l = 10,\nRobust achieves a 9.2×speedup, as compared to the full model.\nIn summary, regarding RQ3, we show that the PreTTR approach\ncan save a considerable amount of time at query-time, as compared\nto the full Vanilla BERT model. These time savings can make it\npractical to run transformer-based rankers in a real-time query\nenvironment.\n6.4 Single Layer Ranking ( l = 11)\nWe answer RQ4 by highlighting a first interesting difference be-\ntween the WebTrack and the Robust ranking performance: the\neffectiveness at l = 11 (Table 3). For WebTrack, the performance\nis comparable in terms of P@20, but suffers in terms of ERR@20.\nFor Robust, the performance suffers drastically. We attribute this\nto differences in the dataset characteristics. First, let us consider\nwhat happens in the l = 11 case. Since it is the final layer and only\nthe representation of the [CLS] token is used for ranking, the only\nattention comparisons that matter are between the [CLS] token\nand every other token (not a full comparison between every pair\nof tokens, as is done in other layers). Thus, a representation of the\nentire query must be stored in the [CLS] representation from layer\n11 to provide an effective comparison with the remainder of the\ndocument, which will have no contribution from the query. Fur-\nthermore, document token representations will need to have their\ncontext be fully captured in a way that is effective for the match-\ning of the [CLS] representation. Interestingly, this setting blurs\nthe line between representation-focused and interaction-focused\nneural models.\nNow we will consider the characteristics of each dataset. From\nTable 2, we find that the queries in the TREC WebTrack 2012 are\ntypically shorter (mean: 2.0, median: 2, stdev: 0.8) than those from\nRobust (mean: 2.7, median: 3, stdev: 0.7). This results in queries\nthat are more qualified, and may be more difficult to successfully\nrepresent in a single vector.\nTo answer RQ4, we observe that the ranking effectiveness when\ncombining with only a single transformer layer can vary depend-\ning on dataset characteristics. We find that in web collections (an\nenvironment where query-time latency is very important), it may\nbe practical to use PreTTR in this way while maintaining high\nprecision of the top-ranked documents.\n6.5 PreTTR for Other Transformers\nNumerous pre-trained transformer architectures exist. We now an-\nswer RQ5 by showing that PreTTR is not only effective on BERT, but\nits ability of reducing ranking latency by preserving quality holds\nalso on other transformer variants. We investigate both the popular\nRoBERTa [25] model and the DistilBERT [36] model. These repre-\nsent a model that uses a more effective pre-training process, and a\nsmaller network size (via model distillation), respectively. Results\nfor this experiment are shown in Table 6. We first observe that the\nunmodified RoBERTa model performs comparably with the BERT\nmodel, while the DistilBERT model performs slightly worse. This\nsuggests that model distillation alone may not be a suitable solution\nto address the poor query-time ranking latency of transformer net-\nworks. With each value of l, we observe similar behavior to BERT:\nP@20 remains relatively stable, while ERR@20 tends to degrade.\nInterestingly, at l = 2 DistilBERT’s ERR@20 performance peaks at\n0.2771. However, this difference is not statistically significant, and\nthus we cannot assume it is not due to noise.\nWe tested the query-time latency of RoBERTa and DistilBERT\nin the same manner as described in Section 6.3. With 12 layers and\na similar neural architecture, RoBERTa exhibited similar speedups\nas BERT, with up to a 56.3×speedup at l = 11 (0.041s per 100 docu-\nments, down from 1.89s). With only 6 layers, the base DistilBERT\nTable 6: WebTrack 2012 using two other Vanilla transformer\narchitectures: RoBERTa and DistilBERT. Note that Distil-\nBERT only has 6 layers; thus we only evaluate l ∈[1, 5]for\nthis model. There are no statistically significant differences\nbetween the Base Model and any of the PreTTR variants\n(paired t-test, p < 0.01).\nRoBERTA [25] DistilBERT [36]\nRanker P@20 ERR@20 P@20 ERR@20\nBase 0.3370 0.2609 0.3110 0.2293\nl = 1 0.3380 0.2796 0.3220 0.1989\nl = 2 0.3370 0.2207 0.3340 0.2771\nl = 3 0.3530 0.2669 0.3070 0.1946\nl = 4 0.3620 0.2647 0.3350 0.2281\nl = 5 0.2950 0.1707 0.3350 0.2074\nl = 6 0.3000 0.1928 - -\nl = 7 0.3350 0.2130 - -\nl = 8 0.3220 0.2460 - -\nl = 9 0.3180 0.2256 - -\nl = 10 0.3140 0.1603 - -\nl = 11 0.3210 0.2241 - -\nmodel was faster (0.937s), and was able to achieve a speedup of\n24.1×with l = 5 (0.035s).\nIn summary, we show that the PreTTR approach can be success-\nfully generalized to other transformer networks (RQ5). We observed\nsimilar trends to those we observed with BERT in two transformer\nvariants, both in terms of ranking effectiveness and efficiency.\n7 CONCLUSIONS AND FUTURE WORK\nTransformer networks, such as BERT, present a considerable op-\nportunity to improve ranking effectiveness [ 4, 27, 31]. However,\nrelatively little attention has been paid to the effect that these ap-\nproaches have on query execution time. In this work, we showed\nthat these networks can be trained in a way that is more suit-\nable for query-time latency demands. Specifically, we showed that\nweb query execution time can be improved by up to 42×for web\ndocument ranking, with minimal impact on P@20. Although this\napproach requires storing term representations for documents in\nthe collection, we proposed an approach to reduce this storage\nrequired by 97.5% by pre-training a compression/decompression\nfunction and using reduced-precision (16 bits) floating point arith-\nmetic. We experimentally showed that the approach works across\ntransformer architectures, and we demonstrated its effectiveness\non both web and news search. These findings are particularly im-\nportant for large-scale search settings, such as web search, where\nquery-time latency is critical.\nThis work is orthogonal to other efforts to reign in the execu-\ntion time of transformer networks. There are challenges related to\nthe application of more advanced networks, such as CEDR [ 27],\nwhich require the computation or storage of additional term rep-\nresentations. Future work could investigate how approaches like\nLSH-hashing [21] could be used to help accomplish this. Further-\nmore, our observation that comparable ranking performance can\nbe achieved using a compression layer raises questions about the\nimportance of the feed-forward step in each transformer layer.\nACKNOWLEDGMENTS\nWork partially supported by the ARCS Foundation. Work partially\nsupported by the Italian Ministry of Education and Research (MIUR)\nin the framework of the CrossLab project (Departments of Excel-\nlence). Work partially supported by the BIGDATAGRAPES project\nfunded by the EU Horizon 2020 research and innovation programme\nunder grant agreement No. 780751, and by the OK-INSAID project\nfunded by the Italian Ministry of Education and Research (MIUR)\nunder grant agreement No. ARS01_00917.\nREFERENCES\n[1] Arash Ardakani, Zhengyun Ji, Sean C Smithson, Brett H Meyer, and Warren J\nGross. 2019. Learning Recurrent Binary/Ternary Weights. In International Con-\nference on Learning Representations . http://arxiv.org/abs/1809.11086\n[2] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. 2019.\nWhat Does BERT Look At? An Analysis of BERT’s Attention. In BlackBoxNLP @\nACL. http://arxiv.org/abs/1906.04341\n[3] Gordon V. Cormack, Mark D. Smucker, and Charles L. A. Clarke. 2010. Efficient\nand effective spam filtering and re-ranking for large web datasets. Information\nRetrieval 14 (2010), 441–465.\n[4] Zhuyun Dai and Jamie Callan. 2019. Deeper Text Understanding for IR with\nContextual Neural Language Modeling. In SIGIR.\n[5] Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. 2018. Convolutional\nNeural Networks for Soft-Matching N-Grams in Ad-hoc Search. In WSDM. ACM\nPress, Marina Del Rey, CA, USA, 126–134. http://dl.acm.org/citation.cfm?doid=\n3159652.3159659\n[6] Domenico Dato, Claudio Lucchese, Franco Maria Nardini, Salvatore Orlando,\nRaffaele Perego, Nicola Tonellotto, and Rossano Venturini. 2016. Fast Ranking\nwith Additive Ensembles of Oblivious and Non-Oblivious Regression Trees.ACM\nTransactions on Information Systems 35, 2 (2016), 15:1–15:31.\n[7] Mostafa Dehghani, Hamed Zamani, Aliaksei Severyn, Jaap Kamps, and W. Bruce\nCroft. 2017. Neural Ranking Models with Weak Supervision. In SIGIR.\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nNAACL.\n[9] Laura Dietz and Ben Gamari. 2017. TREC CAR: A Data Set for Complex Answer\nRetrieval. (2017). http://trec-car.cs.unh.edu Version 2.0.\n[10] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W. Bruce Croft. 2016. A Deep Relevance\nMatching Model for Ad-hoc Retrieval. In CIKM. 55–64. http://arxiv.org/abs/1711.\n08611\n[11] Song Han, Huizi Mao, and William J. Dally. 2015. Deep Compression: Compress-\ning Deep Neural Network with Pruning, Trained Quantization and Huffman\nCoding. In ICLR.\n[12] Song Han, Huizi Mao, and William J. Dally. 2016. Deep Compression: Com-\npressing Deep Neural Network with Pruning, Trained Quantization and Huff-\nman Coding. In 4th International Conference on Learning Representations, ICLR\n2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings . http:\n//arxiv.org/abs/1510.00149\n[13] Song Han, Jeff Pool, John Tran, and William Dally. 2015. Learning both weights\nand connections for efficient neural network. In Advances in neural information\nprocessing systems . 1135–1143.\n[14] Dan Hendrycks and Kevin Gimpel. 2016. Gaussian error linear units (gelus).\narXiv preprint arXiv:1606.08415 (2016).\n[15] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. 2015. Distilling the Knowl-\nedge in a Neural Network. CoRR abs/1503.02531 (2015). arXiv:1503.02531\nhttp://arxiv.org/abs/1503.02531\n[16] Sebastian Hofstätter and Allan Hanbury. 2019. Let’s measure run time! Ex-\ntending the IR replicability infrastructure to include performance aspects. In\nOSIRRC@SIGIR.\n[17] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry P.\nHeck. 2013. Learning deep structured semantic models for web search using\nclickthrough data. In CIKM.\n[18] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua\nBengio. 2017. Quantized neural networks: Training neural networks with low\nprecision weights and activations. The Journal of Machine Learning Research 18,\n1 (2017), 6869–6898.\n[19] Kai Hui, Andrew Yates, Klaus Berberich, and Gerard de Melo. 2017. PACRR: A\nPosition-Aware Neural IR Model for Relevance Matching. In EMNLP.\n[20] Samuel Huston and W Bruce Croft. 2014. Parameters learned in the comparison\nof retrieval models using term dependencies. Technical Report (2014).\n[21] Shiyu Ji, Jinjin Shao, and Tao Yang. 2019. Efficient Interaction-based Neural\nRanking with Locality Sensitive Hashing. In WWW.\n[22] Xiaoqi Jiao, Y. Yin, Lifeng Shang, Xin Jiang, Xusong Chen, Linlin Li, Fang Wang,\nand Qun Liu. 2019. TinyBERT: Distilling BERT for Natural Language Under-\nstanding. ArXiv abs/1909.10351 (2019).\n[23] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Opti-\nmization. In ICLR.\n[24] Francesco Lettich, Claudio Lucchese, Franco Maria Nardini, Salvatore Orlando,\nRaffaele Perego, Nicola Tonellotto, and Rossano Venturini. 2018. Parallel Tra-\nversal of Large Ensembles of Decision Trees. IEEE Transactions on Parallel and\nDistributed Systems (2018), 14. https://doi.org/10.1109/TPDS.2018.2860982\n[25] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar S. Joshi, Danqi\nChen, Omer Levy, Mike Lewis, Luke S. Zettlemoyer, and Veselin Stoyanov.\n2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. ArXiv\nabs/1907.11692 (2019).\n[26] Sean MacAvaney. 2020. OpenNIR: A Complete Neural Ad-Hoc Ranking Pipeline.\nIn WSDM.\n[27] Sean MacAvaney, Andrew Yates, Arman Cohan, and Nazli Goharian. 2019. CEDR:\nContextualized Embeddings for Document Ranking. In SIGIR.\n[28] Sean MacAvaney, Andrew Yates, Kai Hui, and Ophir Frieder. 2019. Content-Based\nWeak Supervision for Ad-Hoc Re-Ranking. In SIGIR.\n[29] Irina Matveeva, Christopher J. C. Burges, Timo Burkard, Andy Laucius, and Leon\nWong. 2006. High accuracy retrieval with multiple nested ranker. In SIGIR.\n[30] Federico Nanni, Bhaskar Mitra, Matt Magnusson, and Laura Dietz. 2017. Bench-\nmark for Complex Answer Retrieval. In ICTIR.\n[31] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT.\nArXiv abs/1901.04085 (2019).\n[32] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document\nExpansion by Query Prediction. ArXiv abs/1904.08375 (2019).\n[33] Wei Pan, Hao Dong, and Yike Guo. 2016. DropNeuron: Simplifying the Structure\nof Deep Neural Networks. CoRR abs/1606.07326 (2016). arXiv:1606.07326 http:\n//arxiv.org/abs/1606.07326\n[34] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Im-\nproving language understanding by generative pre-training . Technical Report.\nOpenAI.\n[35] Corby Rosset, Damien Jose, Gargi Ghosh, Bhaskar Mitra, and Saurabh Tiwary.\n2018. Optimizing Query Evaluations Using Reinforcement Learning for Web\nSearch. In SIGIR.\n[36] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Distil-\nBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. InWorkshop\non Energy Efficient Machine Learning and Cognitive Computing @ NeuIPS .\n[37] Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. 2019. Green AI.\nArXiv abs/1907.10597 (2019).\n[38] Sanghyun Seo and Juntae Kim. 2019. Efficient Weights Quantization of Convo-\nlutional Neural Networks Using Kernel Density Estimation based Non-uniform\nQuantizer. Appl. Sci (2019).\n[39] Mohammad Shoeybi, Mostofa Ali Patwary, Raul Puri, Patrick LeGresley, Jared\nCasper, and Bryan Catanzaro. 2019. Megatron-LM: Training Multi-Billion Param-\neter Language Models Using Model Parallelism. ArXiv abs/1909.08053 (2019).\n[40] Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy\nLin. 2019. Distilling Task-Specific Knowledge from BERT into Simple Neural\nNetworks. ArXiv abs/1903.12136 (2019).\n[41] Nicola Tonellotto, Craig Macdonald, and Iadh Ounis. 2018. Efficient Query\nProcessing for Scalable Web Search. Foundations and Trends in Information\nRetrieval 12, 4–5 (2018), 319–492.\n[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All\nYou Need. In NeuIPS. http://arxiv.org/abs/1706.03762\n[43] Lidan Wang, Jimmy Lin, and Donald Metzler. 2011. A cascade ranking model for\nefficient ranked retrieval. In SIGIR.\n[44] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power.\n2017. End-to-End Neural Ad-hoc Ranking with Kernel Pooling. In SIGIR. 55–64.\nhttp://arxiv.org/abs/1706.06613 arXiv: 1706.06613.\n[45] Chen Xu, Jianqiang Yao, Zhouchen Lin, Wenwu Ou, Yuanbin Cao, Zhirong\nWang, and Hongbin Zha. 2018. Alternating Multi-bit Quantization for Recurrent\nNeural Networks. In International Conference on Learning Representations . https:\n//arxiv.org/abs/1802.00150\n[46] Peilin Yang, Hui Fang, and Jimmy Lin. 2017. Anserini: Enabling the Use of Lucene\nfor Information Retrieval Research. In SIGIR.\n[47] Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming\nLi, and Jimmy Lin. 2019. End-to-End Open-Domain Question Answering with\nBERTserini. In NAACL-HLT.\n[48] Wei Yang, Haotian Zhang, and Jimmy Lin. 2019. Simple Applications of BERT\nfor Ad Hoc Document Retrieval. ArXiv abs/1903.10972 (2019).\n[49] Zeynep Akkalyoncu Yilmaz, Shengjin Wang, Wei Yang, Haotian Zhang, and\nJimmy Lin. 2019. Applying BERT to Document Retrieval with Birch. In\nEMNLP/IJCNLP.\n[50] Hamed Zamani, Mostafa Dehghani, W Bruce Croft, Erik Learned-Miller, and\nJaap Kamps. 2018. From neural re-ranking to neural ranking: Learning a sparse\nrepresentation for inverted indexing. In CIKM."
}