{
  "title": "(Security) Assertions by Large Language Models",
  "url": "https://openalex.org/W4382320007",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4222879398",
      "name": "Kande, Rahul",
      "affiliations": [
        "Texas A&M University",
        "Mitchell Institute"
      ]
    },
    {
      "id": "https://openalex.org/A4202085989",
      "name": "Pearce, Hammond",
      "affiliations": [
        "UNSW Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2744794990",
      "name": "Tan, Benjamin",
      "affiliations": [
        "University of Calgary"
      ]
    },
    {
      "id": "https://openalex.org/A4202085992",
      "name": "Dolan-Gavitt Brendan",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A4288825060",
      "name": "Thakur, Shailja",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2744610272",
      "name": "Karri Ramesh",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2756258480",
      "name": "Rajendran, Jeyavijayan",
      "affiliations": [
        "Mitchell Institute",
        "Texas A&M University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2981952055",
    "https://openalex.org/W4379115941",
    "https://openalex.org/W2021949528",
    "https://openalex.org/W4391182436",
    "https://openalex.org/W2095410905",
    "https://openalex.org/W4365512576",
    "https://openalex.org/W2956128604",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W4288057765",
    "https://openalex.org/W3015879501",
    "https://openalex.org/W2614052576",
    "https://openalex.org/W2977663466",
    "https://openalex.org/W1979855816",
    "https://openalex.org/W4240101827",
    "https://openalex.org/W4362679564",
    "https://openalex.org/W4386113931",
    "https://openalex.org/W1976309468",
    "https://openalex.org/W3142765463",
    "https://openalex.org/W4385849010",
    "https://openalex.org/W2965627384",
    "https://openalex.org/W2345350761",
    "https://openalex.org/W4221164739",
    "https://openalex.org/W4382320007",
    "https://openalex.org/W4210725750",
    "https://openalex.org/W3183685509",
    "https://openalex.org/W1981151256",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4385187421",
    "https://openalex.org/W46679369",
    "https://openalex.org/W4309698405",
    "https://openalex.org/W4386875652",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W3082750925",
    "https://openalex.org/W4200633400",
    "https://openalex.org/W3170159994",
    "https://openalex.org/W4226485558",
    "https://openalex.org/W4236640323",
    "https://openalex.org/W4387559217",
    "https://openalex.org/W3127584440",
    "https://openalex.org/W4295950999",
    "https://openalex.org/W2099671082"
  ],
  "abstract": "The security of computer systems typically relies on a hardware root of trust. As vulnerabilities in hardware can have severe implications on a system, there is a need for techniques to support security verification activities. Assertion-based verification is a popular verification technique that involves capturing design intent in a set of assertions that can be used in formal verification or testing-based checking. However, writing security-centric assertions is a challenging task. In this work, we investigate the use of emerging large language models (LLMs) for code generation in hardware assertion generation for security, where primarily natural language prompts, such as those one would see as code comments in assertion files, are used to produce SystemVerilog assertions. We focus our attention on a popular LLM and characterize its ability to write assertions out of the box, given varying levels of detail in the prompt. We design an evaluation framework that generates a variety of prompts, and we create a benchmark suite comprising real-world hardware designs and corresponding golden reference assertions that we want to generate with the LLM.",
  "full_text": "IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY , 2023 1\n(Security) Assertions by Large Language Models\nRahul Kande∗, Hammond Pearce †, Benjamin Tan‡, Brendan Dolan-Gavitt §,\nShailja Thakur§, Ramesh Karri §, and Jeyavijayan Rajendran ∗\n∗Texas A&M University, †University of New South Wales, ‡University of Calgary, §New York University\nAbstract—The security of computer systems typically relies\non a hardware root of trust. As vulnerabilities in hardware\ncan have severe implications on a system, there is a need for\ntechniques to support security verification activities. Assertion-\nbased verification is a popular verification technique that involves\ncapturing design intent in a set of assertions that can be used in\nformal verification or testing-based checking. However, writing\nsecurity-centric assertions is a challenging task. In this work, we\ninvestigate the use of emerging large language models (LLMs) for\ncode generation in hardware assertion generation for security,\nwhere primarily natural language prompts, such as those one\nwould see as code comments in assertion files, are used to produce\nSystemVerilog assertions. We focus our attention on a popular\nLLM and characterize its ability to write assertions out of the\nbox, given varying levels of detail in the prompt. We design an\nevaluation framework that generates a variety of prompts, and\nwe create a benchmark suite comprising real-world hardware\ndesigns and corresponding golden reference assertions that we\nwant to generate with the LLM.\nIndex Terms—LLM, AI, hardware, assertion generation, hard-\nware security, vulnerability detection, ChatGPT, Codex\nI. I NTRODUCTION\nA. Implications of Vulnerable Hardware\nHardware underpins applications ranging from small\ninternet-of-thing (IoT) devices to large and complex multi-\ncore processors. Many of the tasks performed in software,\nlike encryption, decryption, and machine learning, are accel-\nerated in hardware. Many software security defenses assume\nhardware as the vulnerability-free root of trust. Vulnerabilities\nin hardware have severe implications on all such applications\nand security defences [1]–[4], yet weaknesses continue to\narise (e.g., Zenbleed [5] and Pentium FDIV bug [6]). The\ncost of fixing hardware vulnerabilities depends on where in\nthe development life cycle they are discovered. Vulnerabilities\ndiscovered after fabrication cannot usually be fixed. Even if\nsoftware can patch them, they incur performance overheads.\nVulnerabilities found in the field have consequences ranging\nfrom information leakage to corporate reputation damage [7].\nHence, it is desirable to detect as many vulnerabilities as\npossible during hardware design.\nB. State-Of-The-Art Hardware Verification\nState-of-the-art hardware verification techniques include\ntesting [8] (simulation, random-regression, directed-random\ntesting) and formal verification [9], [10] (model-checking,\nManuscript received July 5, 2023.\ntheorem proving). To meet the demand for faster and more\nefficient verification, researchers developed techniques like\nhardware fuzzing [11], [12], information-flow tracking [13],\nand hybrid techniques [14], [15]. These techniques require\na golden reference model (GRM) or hardware assertions to\ndetect vulnerabilities. Generating GRMs or assertions is not\ntrivial, requiring manual effort and design knowledge. Such\nmethods are error-prone and do not scale [12], [14], [16]–[21].\nC. Assertion-Checking\nAssertion-checking [17] is a popular verification technique\nwhere the specification of the design under test (DUT) is\ncoded into assertions or properties in a hardware description\nlanguage (HDL) like SystemVerilog. These assertions are used\nto statically prove properties using formal verification tools\nor dynamically verify using testing tools. Assertions play an\nimportant role in hardware verification. Each assertion will\nfocus on verifying individual functional properties and critical\nlogic in the hardware. They can detect vulnerabilities in the\nearly stages of design, even when the DUT is not fully\ndeveloped, as they do not require the output of the DUT to\ndetect vulnerabilities. They can detect vulnerabilities that will\noccur when components are composed [22]. Thus, assertions\ncan also guide hardware fuzzers and accelerate verification.\nD. Limitations of Assertion-Checking\nGenerating correct and practical assertions is challenging.\nAutomatic derivation (i.e., mining) for assertions [20], [21])\nis possible, but irrelevant and difficult checks increase verifi-\ncation time. While assertions for functional behaviors check\nthat the DUT performs to expectations, security assertions\ntypically check that a DUT does not feature some weak-\nness, a fundamentally different task and one not well-suited\nfor mining [18]. Ensuring that assertions check for security\nvulnerabilities requires design knowledge and analysis of its\npotential weaknesses [7], [18]. This requires security expertise\nunavailable to a typical designer. Thus, techniques to develop\nsecurity assertions are error-prone, time-consuming, and do\nnot scale to large designs. Even with security-relevant knowl-\nedge, say in natural language descriptions of properties, this\nknowledge is not used to write assertions automatically [18].\nE. Goals and Contributions\nTo encourage the adoption of assertion-based security\nchecking, it is essential to determine faster and easier methods\n1556-6021 © 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/\nindex.html for more information. This article has been accepted for publication in IEEE Transactions on Information Forensics and Security. This is the\nauthor’s version which has not been fully edited and content may change prior to final publication. See https://ieeexplore.ieee.org/document/10458667\nfor the published version of the paper. Citation information: DOI 10.1109/TIFS.2024.3372809\nof generating hardware security assertions. We investigate the\nuse of Large Language Models (LLM) to generate hardware\nsecurity assertions automatically. Given their success in writ-\ning code for other languages (e.g., OpenAI’s Codex [23]), we\nexamine if LLMs can help in this task. This is the first work\ninvestigating the feasibility of LLMs to generate hardware\nsecurity assertions. We envisage a pipeline where designer-\ns/verification engineers write comments in natural language\nabout the security assertions, for example, based on system\nspecifications, in the RTL. These comments plus surrounding\ncontext can be used as a prompt by the LLMs to generate se-\ncurity assertions. Automating assertion generation with LLMs\nencourages the adoption of assertion-based security checking,\nthereby accelerating hardware verification. This will cause a\n“shift left” in vulnerability detection and result in the design\nof more secure hardware.\nWe study commercial LLMs and answer two research\nquestions: (RQ1) Is it possible for LLMs to generate security\nassertions for hardware? (RQ2) How do LLMs perform on\ndifferent prompts? We developed a framework to evaluate\nLLM performance when writing security assertions, including\ndesigning a benchmark suite of different types of vulnerabili-\nties in hardware. Our main contributions are fourfold:\n• A framework and benchmarks to evaluate LLMs in gen-\nerating hardware assertions. The framework can fix a lim-\nited set of syntax/typographical issues in the assertions,\n• Evaluating an LLM for generating security assertions,\n• Investigating the effect of different types of prompts on\nthe generation of assertions, and\n• Open-sourcing LLM-based framework and benchmarks\nto support research in this direction.\nII. B ACKGROUND AND RELATED WORK\nA. Security Assertions\nAssertion-based verification is a popular part of the digital\ndesign flow, where designer intent is captured as a set of\nproperties that are checked during simulation, through formal\nverification, or synthesized into actual hardware for run-time\nchecking. Designers often use assertions to ascertain whether\na given property is satisfied and detect vulnerabilities [17].\nChallenges with writing assertions. Writing hardware as-\nsertions is a non-trivial task that requires expertise and con-\nsiderable manual effort in the design and verification of hard-\nware [12], [14], [16]–[21]. Writing security assertions to catch\nsecurity vulnerabilities provides additional challenges [7],\n[18]. We use a relatively simple assertion required to detect\nthe bug in our benchmark, BM1, to highlight these challenges.\nMany assertions are more complicated than this (for example,\nBM7’s assertion involves twice the number of signals than\nBM1’s and uses parametric values), making the task even more\nchallenging. BM1 implements a simple register lock design\nwhere the data in the register should not be changed when it\nis locked. Such logic is commonly used to implement access\ncontrol-related security features [7]. The assertion should\nensure that the data register does not change when locked,\nas shown in Line 7 of Listing 1.\nListing 1. goldenAssert.sva file of BM1 showing its golden reference\nassertion (reformatted for length).\n1 `timescale 1ns/10ps\n2 module v_dut (\n3 lock , clk , rst , data );\n4 input lock ; input clk ;\n5 input rst ; input data ;\n6 // golden assertion\n7 assert property ( @(posedge clk) (data ˆ $past(data))\n|-> ($past(lock) == 0) ),→\n8 else $display(\"GOLDEN: FAIL, time=%4d, data=%d,\ndata_d=%d, lock=%d\", $time, data, $past(data),\n$past(lock));\n,→\n,→\n9 endmodule\n10 bind tb v_dut i_bind_dut (\n11 .lock (lock), .clk (clk),\n12 .rst (rst), .data (data) );\nCreating this assertion for BM1 manually requires: (i) cor-\nrect understanding of the security feature (e.g., when the data\ncan change and when it cannot), (ii) correct use of the involved\nsignals (e.g., there are two other data signals in BM1 which\ncannot be used to create the assertion), (iii) correct timing of\nsignals (e.g., lock value of previous cycle should be checked\nonce the data is changed).\nIn addition, writing more complicated assertions will require\na correct understanding of the various system verilog built-\nin methods (e.g., $past method is used in BM1) and other\ntiming-related constructs used in signal comparisons across\nmultiple clock cycles. Section IV-F discusses the errors made\nby LLM when generating this assertion. While LLMs can be\ntrained to rectify these errors, training humans to gain security\nexpertise is expensive and time-consuming, especially with a\nsevere shortage of semiconductor engineers [24].\nB. Large Language Models for Code Generation\nThe domain of “natural language programming” [25] seeks\nto transform natural language specifications into code through\nnatural language processing. Large Language Models (LLMs)\nsuch as GPT-2 [26] and BERT [27], are transformer-based\nartificial neural networks [28] which are designed to work over\ntext datasets. LLMs have millions to billions of parameters and\nare trained over expansive text datasets. Inputs and outputs for\nan LLM are tokens, i.e., common sets of character sequences.\nEach token has a unique numeric identifier (i.e., byte pair\nencoding [29]). Functionally, given a sequence of tokens as an\ninput prompt, an LLM will output a probability distribution for\nthe next token over the vocabulary of known tokens. After a\ntoken is selected based on some search criteria, it is appended\nto the prompt. The LLM then generates the next token. This is\nknown as auto-regression. The sequence of tokens generated\nfrom the input prompt is known as the output or completion.\nUsers can optionally specify a stop sequence – a sequence of\ntokens that tells the LLM to stop generating tokens.\nWhile LLMs were initially trained on regular text, recent\nresearch has made significant efforts to train and generate code\nusing LLMs. OpenAI Codex LLM [23] elucidates functional\ncode from program snippets such as comments and function\nsignatures. OpenAI Codex and GitHub Copilot [30] are com-\nmercial tools. Codex was trained over “all” the open-source\ncode on GitHub, i.e., hundreds of millions to billions of lines\nof code. As such, this LLM “learned” to support languages\nwith an open-source presence, including Verilog [31], [32].\nOther open-source LLMs include NVIDIA MegatronLM [33]\nand Salesforce CodeGen [34]. Although training an LLM\nfrom scratch is expensive, Pearce et al. finetuned a GPT-2\nmodel to produce “DA VE” [35], a “small” LLM that produces\nVerilog from natural language descriptions.\nC. Automating Assertion Generation\nAside from generating new designs, related work explores\nspecialized parts of the design flow, such as patch generation\n(program repair) [36], [37]. Our work similarly focuses on\nthe security assertion generation problem in the hardware\ndesign flow. Prior work has investigated machine learning and\nother automated techniques to assist design and verification\nof hardware. For example, GoldMine [20] mines invariant\nproperties from static analysis and analysis from simulation\ntraces. Recent work has used similar approaches to focus on\nsecurity-related properties (e.g., [17], [18], [38], [39]).\nOther prior work investigated the mapping between natural\nlanguage specification and SystemVerilog assertions on a\nsmall scale [19] and focused on learning a custom grammar\n(representing a “writing style”) to map sentences to assertions.\nSimilarly, [40] uses natural language-based chatbots to gener-\nate assertions. Unlike our work, which automatically evaluates\nthe assertions, they manually check the correctness of the\nassertions. Also, [19], [40] are not specific to security.\nIn contrast, Transys [41] focuses on security properties. It\nautomatically translates security properties from one design\nto another. It identifies variables in the first design and their\nanalogs in the other, adjusts the arithmetic expressions in the\nproperty, and then refines those constraints. Transys requires\nproperties to start from and makes its mapping of registers,\nsignals, and ports between designs using a mix of statistic,\nsemantic, and structural features.\nUnlike these works, we focus on mapping high-level de-\nsigner intent (captured in natural language) to SystemVerilog\nassertions, without training, simulation, or assertions from\nclosely related projects, as part of the generation. We evaluate\nthe usefulness of an out-of-the-box LLM for this task.\nIII. E VALUATION FRAMEWORK : ASSESSING LLM S FOR\nASSERTION GENERATION\nTo measure the ability of an LLM to generate hardware\nsecurity assertions, we design an evaluation framework, as\ndepicted in Figure 1. It generates a prompt based on one of\nour benchmarks, queries the LLM with that prompt to generate\nthe assertions, simulates the generated assertions along with\na “golden” reference assertion, and compare their outputs.\nThe generated assertion is considered correct if it is triggered\nfor the same sets of input values as the reference assertion.\nThe evaluation framework has the following components:\n(i) benchmark suite, (ii) prompt generator, (iii) assertion file\ngenerator, (iv) simulator, and (v) scoreboard, as shown in\nFigure 1. Next, we explain each of these components in detail.\nBenchmark \n    Suite\n  Prompt \nGeneration\nOriginalProcessed Syntax\n  Fixer\nSimulator\nCorrect \nAssertions\nIncorrect\nAssertions\nEvaluation\nScoreboard\nAssertion File Preparation\n{Testbench, \nDesign Source File,\nGolden Assertion}\nGenerated \nAssertions\nFigure 1. Evaluation framework for generating assertions using LLMs.\nA. Benchmark Suite\nThe benchmark suite consists of two manually crafted\ndesigns and eight modules derived from the designs of\nHack@DAC hardware security competitions [7], [42], [43] and\nthe open-source silicon root of trust SoC, OpenTitan [44] as\nshown in Table I.\nThe table includes the assertion we aim to generate and\nthe vulnerability to detect with these assertions. The common\nweakness enumerations (CWEs) corresponding to the vulner-\nabilities span hardware CWE categories such as (i) debug and\ntest problems (CWE-1207), (ii) peripherals, on-chip fabric,\nand interface problems (CWE-1203), (iii) privilege separation\nand access control issues (CWE-1198), and (iv) power, clock,\nthermal, and reset concerns (CWE-1206). Each benchmark has\nthe following information.\n1) Design source code: Each benchmark includes its cor-\nresponding source code. To limit the number of tokens in the\nprompt to LLM, the original source files from Hack@DAC\nand OpenTitan are trimmed down and simplified to less than\n100 lines of code, retaining only the logic with the target\nvulnerability. The source code provides relevant context about\nthe target assertion to the LLM. Each benchmark includes\nthree types of design source code as we describe below.\n• EmptyDUT is an empty source file with no information\nabout the benchmark design. It evaluates the LLM’s ability to\ngenerate the assertions in the absence of design context.\n• CorrectDUT is the bugfree source code of the benchmark.\nIt provides complete context about the design to the LLM.\n• BuggyDUT is a source code with vulnerability in it.\nHack@DAC designs already have vulnerabilities inserted in\nthem while we added the vulnerabilities in the rest.\n2) Golden reference assertion: Each benchmark consists of\na reference assertion in a SystemVerilog assertion (SV A) file,\ngoldenAssert.sva. These golden assertions are manually\ncrafted for BM1-BM8, as there are no existing assertions for\nthese benchmarks. For BM9 and BM10, the reference asser-\ntions are the same as those used in the verification source code\nof OpenTitan. Listing 1 shows the goldenAssert.svafile\nof BM1 with the golden reference assertion at Lines 6-8.\nTable I\nBENCHMARKS USED , ALONG WITH THE ASSERTION TO GENERATE , THE TARGET VULNERABILITY , AND THEIR CORRESPONDING CWE CLASSIFICATIONS .\nID Benchmark Source Assertion Vulnerability CWE Type\nBM1 Lockable Register Manually crafted Data should not change if the lock is set. The register can be written even if it is locked. CWE-1233\nBM2 Traffic signal controller Manually crafted Yellow signal should precede RED. Skips yellow light on walk request. CWE-1245\nBM3 JTAG password controller Hack@DAC Locked JTAG shouldn’t assert Valid signal Able to access locked JTAG. CWE-1324\nBM4 Bus access control Hack@DAC Access values to each peripheral should\nmatch the specification.\nAccess to one peripheral grants access to another\nperipheral. CWE-1317\nBM5 AES IP Hack@DAC Data should not be output if the\nencryption is not completed. Internal registers of AES are visible externally. CWE-1303\nBM6 AES IP Hack@DAC Clear key values when entering debug. Secret keys are not cleared when entering debug. CWE-1244\nBM7 Register controller of\nCV A6 core Hack@DAC Lower than required privilege level should\ntrigger violation.\nPrivileged CSR register can be accessed by\nunprivileged user. CWE-1261\nBM8 Register lock IP Hack@DAC All the register locks should be initialized\ncorrectly on reset.\nRegister locks are configured with incorrect\ndefault values at reset. CWE-276\nBM9 ADC controller OpenTitan SoC Wakeup timer should be set to 0 on reset. Wakeup timer is incorrectly configured on reset. CWE-1221\nBM10 Reset manager OpenTitan SoC\nReset should follow fall of input signal\nwithin a given range of clock cycles\nunless input is asserted again.\nReset does not follow even after maximum clock\ncycles of input trigger. CWE-1206\nListing 2. Prompt file for BM1 with values for example assertion, comment for target assertion, and beginning of assertion strings.\n1 { \"commentStrings\": {\n2 \"VeryBriefCom\" : \"assert that the register is not changed if it is locked\\n\",\n3 \"BriefCom\" : \"assert that the data is not changed if the lock is set\\n\",\n4 \"DetailedCom\" : \"assert that at every positive edge of clock, the value of the data register is same as its\nvalue in the previous clock cycle if the value of the lock signal in the previous clock cycle is 1\\n\", },,→\n5 \"examples\": {\n6 \"NoEx\" : [\"\\n\\n\"],\n7 \"TrivialEx\" : [\" r_en is 0 if w_en is 1\\n assert property (@(posedge clk) (w_en == 1) |-> (r_en == 0));\\n\\n\"]\n8 \"BasicEx\" : [\" r_en is 0 if w_en is set\\n assert property (@(posedge clk) ($past(w_en) == 1) |-> (r_en ==\n0));\\n\\n\"],,→\n9 \"DetailedEx\" : [\" at every positive edge of clock, the value of the r_en signal is same as its value in the\nprevious clock cycle if the value of the w_en signal in the previous clock cycle is 0\\nassert property\n(@(posedge clk) ($past(w_en) == 0) |-> (r_en == $past(r_en)));\\n\\n\"], },\n,→\n,→\n10 \"assertionBeginning\": {\n11 \"EmptyBeg\": \"\",\n12 \"ShortBeg\": \"assert\",\n13 \"NormalBeg\": \"assert property (@(posedge clk)\" } }\nListing 3. SV As of BM9, BM10 from OpenTitan; comments describe SV As.\n1 // FSM software reset\n2 `ASSERT(WakeupTimerCntSwReset_A, cfg_fsm_rst_i |=>\nwakeup_timer_cnt_q == 0, clk_aon_i, !rst_aon_ni),→\n3 // A fall in por_n_i leads to a fall in\nrst_por_aon_n[0].,→\n4 `FALL_ASSERT(CascadePorToAon,\npor_n_i[rstmgr_pkg::DomainAonSel],\nresets_o.rst_por_aon_n[rstmgr_pkg::DomainAonSel],\nPorCycles, clk_aon_i)\n,→\n,→\n,→\n3) Prompt data: Each benchmark includes a prompt data\nfile consisting of comment strings describing the target asser-\ntion, example assertions, and beginning strings of the target as-\nsertion with varying amounts of details. The prompt generator\nuses this information to generate multiple prompts of varying\namounts of details when querying the LLM (see Section III-B).\nAs an example, Listing 2 shows the prompt for BM1.\nComment strings.It is common practice to include comments\nabout the property being checked when writing SV As. For\nexample, Listing 3 shows the SV As of BM9 and BM10 from\nthe OpenTitan SoC [44] where Lines 1,3 are the comments\nfor the SV As in Lines 2,4 respectively. It can be seen that\nwhile comments are included for the SV As, they vary in the\nlevel of detail. The comment in Line 1 does not include the\nname of the signals involved, while the comment in Line 4\nincludes the specific signal names used in the SV A. Thus, it is\nessential to evaluate the performance of LLMs with comment\nstrings comprising varying levels of detail about the target\nassertion.Our benchmarks include three types of comment\nstrings to allow such evaluations as we describe below.\n• VeryBriefCom is a comment with very few details about\nthe target assertion. It uses common/generic names for signals\ninstead of their actual names. For example, a signal rst_i\nwould be termed reset. Details about timing, such as the\nnext and past clock cycle, are not included. It uses commonly\nused terminology for signal values such as asserted or\ntriggered instead of 1 and disabled instead of 0. In-\nformation about parameter values, bit widths, and exact index\nvalues for comparison are not included. For example, Line 2\nof Listing 2 shows the VeryBriefCom of BM1. It evaluates the\nLLM’s ability to generate assertions with minimal information\nabout the target assertion. The LLM has to fill in the missing\ndetails, considering the context from the prompt.\n• BriefCom is a brief comment less ambiguous than Very-\nBriefCom. It uses correct signal names everywhere. It spec-\nifies correct parameter values, bitwidths, and exact index\nvalues where needed. However, timing information is still\nnot included in this comment, and common terms such as\nasserted, triggered, and disabled are still used.\nFor example, Line 3 of Listing 2 shows the BriefCom of\nBM1. This comment evaluates the ability of LLM to generate\nassertions with most of the necessary information about the\ntarget assertion, where the LLM has to extrapolate the few\nmissing details from the context available in the prompt.\n• DetailedCom is a detailed comment that replaces all the\ncommonly used terms from BriefCom with exact values, spe-\ncific for the benchmark and includes the timing information.\nIt also uses helping strings like appending the signal names\nwith the word “signal” and adding the string “the value of\nthe” when specifying the value of any signal to aid the LLM\nin understanding the description of the target assertion. For\nexample, Line 4 of Listing 2 shows the DetailedCom of BM1.\nThis comment evaluates the LLM’s ability to generate asser-\ntions when all information is clearly specified as a comment.\nExample assertions. Each benchmark has four types of ex-\nample assertions. They serve two purposes. First, they set the\ncontext for the LLM and guide it towards generating asser-\ntions rather than normal design code. This is recommended\nby OpenAI as a best practice when using LLMs *. Second,\nthe assertions aid the LLM in converting the user-provided\ndescription of the assertion into an SV A.\n• NoEx is an empty string where no example assertion is\nincluded in the query, as shown in Line 6 of Listing 2. This\nexample allows us to analyze whether setting the context with\nan example assertion is needed when querying for assertions.\n• TrivialEx is a trivial SV A unrelated to any benchmarks.\nThus, it is useful in evaluating if the LLM can generate the\nassertion only based on the context and without requiring any\nadditional aid about the target assertion. All the benchmarks\nuse the same TrivialEx, which is shown in Line 7 of Listing 2.\n• BasicEx is a basic SV A that is a simpler version of the target\nassertion with different signal names and values. It uses the\nBriefCom-style comment. For example, Line 8 of Listing 2\nshows the BasicEx of BM1. This comment lets us evaluate\nLLM’s ability when provided with a similar simple example\nassertion and a brief description of the comment.\n• DetailedEx is an SV A similar to the target assertion but with\ndifferent signal names and values. It uses the same comment\nas BasicEx. For example, Line 9 of Listing 2 shows the\nDetailedEx of BM1. This example evaluates if the LLM can\ngenerate the assertion when given a similar assertion along\nwith natural language context.\nAssertion beginnings. The prompt data of benchmarks include\nassertion beginnings (preambles) containing different amounts\nof context. These strings aim to help LLMs generate the target\nassertion by specifying which logic to use. Each benchmark\nhas three possible beginnings as explained below:\n• EmptyBeg is empty string common for all benchmarks.\n• ShortBeg is 1-to-3 starting words of target assertion.\n• NormalBeg is 3-to-8 starting words of target assertion.\n4) Testbench: Each benchmark includes a SystemVerilog\ntestbench that drives all the signals involved in the assertion\nfor all possible combinations of values. We built testbenches\nfor all the benchmarks using a single template testbench file as\n*https://beta.openai.com/docs/guides/code/best-practices\nListing 4. Template testbench of our framework reformatted for length.\n1 `timescale 1ns/10ps\n2 module tb();\n3 // <Parameter declarations here>\n4 // <All signal declarations here>\n5 localparam noDutSignalBits = <total # signal bits>;\n6 localparam noClocks =<# cycles involved in assertion>;\n,→\n7 localparam log2NoClocks = <ceil(log2(noClocks))>\n8 localparam CTR_WIDTH = (noDutSignalBits*noClocks) +\nlog2NoClocks;,→\n9 // + log2NoClocks to track updating test data\n10 // generate clock and reset\n11 initial begin\n12 clk = 'b0; rst = 'b1; #18 rst = 'b0;\n13 end\n14 always #5 clk <= ˜clk;\n15 // generate tests\n16 reg [CTR_WIDTH-1:0] test_data;\n17 wire [noDutSignalBits-1:0] test_data_curr;\n18 always @(posedge clk) begin\n19 if (rst) begin\n20 test_data <= 'b0;\n21 end else begin\n22 if (test_data == {CTR_WIDTH{ 1'b1}}) begin // stop\nsince all inputs are tested,→\n23 #5 $display(\"Testing done, no inputs=%d\",\ntest_data+1);,→\n24 $finish;\n25 end else begin\n26 #5 test_data <= test_data + 1;\n27 end\n28 end\n29 end\n30 // <assign correct data from the counter to the\ntest_data_curr>,→\n31 // ex: assign test_data_curr = test_data[0]\n?test_data[(1*noDutSignalBits)+log2NoClocks +:\nnoDutSignalBits] :\n,→\n,→\n32 //test_data[log2NoClocks +: noDutSignalBits];\n33 // <assign test_data_curr to all the signals>\n34 endmodule\nDesign source\nmodule\nAssertion clue\nBeginning String\nExample \nassertions\nComment \nstrings\nBeginnings\nDesign \nsource codes\ngoldenAssert.sva\nFigure 2. Template for the prompt string.\nshown in Listing 4. We developed this template testbench man-\nually, which parametrizes the number of signal bits involved in\nthe assertion, noDutSignalBits and the base-2 logarithmic\nof the number of clock cycles ( noClocks) required to verify\neach assertion, log2NoClocks . Once these two parameters\nare set, the template testbench generates a counter of size\nCTR WIDTH (see Line 8) and increments the value of\nthis counter from 0 to 2CTR WIDTH − 1 (see Line 22). All\nthe signal bits are driven by the MSB noDutSignalBits ×\nnoClocks bits of this counter (see Lines 30-33) while the\nLSB log2NoClocks bits keep the MSB bits unchanged for\nnoClocks clock cycles.\nB. Prompt Generator\nThe prompt generator generates the prompt string and the\nLLM’s parameter values (see Section IV-B for details about\nListing 5. Prompt string of BM1 with its components (formatted for length).\n1 `timescale 1ns/10ps\n2 module lock_reg (\n3 input data_in, output data_out, input r_en,\n4 input w_en, input lock, input clk, input rst);\n5\n6 reg data;\n7 always @ (posedge clk) begin\n8 if (rst) begin\n9 data <= #1 0;\n10 end\n11 else begin\n12 if (w_en)\n13 data <= #1 lock ? data: data_in;\n14 end\n15 end\n16 assign data_out = r_en ? data : 'b0;\n17 endmodule\n18\n19 module v_dut (\n20 lock , clk , rst , data );\n21 input lock ; input clk ;\n22 input rst ; input data ;\n23\n24 // assert that r_en is 0 if w_en is 1\n25 assert property (@(posedge clk) (w_en == 1) |-> (r_en\n== 0));,→\n26\n27 // property to check that at every positive edge of\nclock, the value of the data register is same as its\nvalue in the previous clock cycle if the value of\nthe lock signal in the previous clock cycle is 1\n,→\n,→\n,→\n28 assert\nparameter values) for each benchmark. The LLM is queried\nwith this prompt information, and the generated assertions\nare analyzed for correctness. We automate prompt generation\nusing templates for prompt strings, as shown in Figure 2.\nThe design source part of the prompt string consists of\none of three design source codes. The module part of the\nprompt string is generated from the goldenAssert.sva\nfile of each benchmark until the location where the golden\nassertion is written. For BM1, the module part of the prompt\nis Lines 2-5 in Listing 1. The assertion clue is formed by\nappending the example assertion and the comment string from\nthe prompt data file of the benchmark. One of the three\nassertion beginning strings is used as the last component of\nthe prompt. Each combination of ( design source , example\nassertion, comment string, and beginning) plus the module part\nare appended to generate multiple prompts per benchmark.\nIn addition, the commonly-used phrase “assert that” in\nthe prompt is replaced with another popularly-used synonym\n“property to check that” to evaluate the effectiveness of using\nsynonyms in the prompt string for generating assertions. For\nexample, Listing 5 shows a prompt string for BM1. Lines 1-\n17 are the design source code part of the prompt string. Lines\n19-22 are the module part of the prompt string derived from\ngoldenAssert.sva file of BM1. Lines 24-27 comprise\nthe assertion clue part of the prompt, where Lines 24,25 is\nthe example assertion, and Line 27 is the comment string\ndescribing the target assertion. Note that the comment in Line\n24 uses the “assert that” string while the comment in Line 27\nuses the “property to check that” string. Finally, Line 28 is\nthe beginning string that primes the LLM into completing the\ncode with an assertion.\nListing 6. Example simulation log showing assertion violation information\n(trimmed and reformatted for length).\n1 Reading pref.tcl\n2 ...\n3 # Loading work.v_dut(fast)\n4 # run 200000us\n5 # ** Error: Assertion error.\n6 # Time: 95 ns Started: 95 ns Scope:\ntb.i_bind_dut_buggy File:\nassertion_gen_5435_7484xO.sva Line: 18\n,→\n,→\n7 # GOLDEN: FAIL, time= 95, data=0, data_d=1, lock=1\n8 # ** Error: Assertion error.\n9 # Time: 145 ns Started: 145 ns Scope:\ntb.i_bind_dut_buggy File:\nassertion_gen_5435_7484xO.sva Line: 18\n,→\n,→\n10 # GOLDEN: FAIL, time= 145, data=1, data_d=0, lock=1\n11 # ** Error: Assertion error.\n12 # Time: 165 ns Started: 165 ns Scope:\ntb.i_bind_dut_buggy File:\nassertion_gen_5435_7484xO.sva Line: 18\n,→\n,→\n13 # GOLDEN: FAIL, time= 165, data=1, data_d=0, lock=1\n14 ...\n15 # Testing done, no inputs= 32\n16 ...\n17 # Errors: 7, Warnings: 0\nC. Assertion File Generator\nThe assertion file generator receives all the assertions gener-\nated by the LLM from the prompt generator. It first processes\nthese generated assertions to fix a limited set ofminor mistakes\n(akin to typos) made by the LLM, as shown in Table II.\nThese rules were derived through early qualitative analysis of\nthe LLM outputs. They reflect the “common pitfalls” made\nby the models, where they are straightforward errors that\nwill cause generated assertions to be incorrect. Since they\nare simple, fixes can be made automatically using lexical\ntooling. Further, to ensure that the repairs R2 and R3 do\nnot impact LLM performance, these repairs are made to\ncopies of the generated assertion, i.e., the originally generated\nassertion remains in the list of generated assertions. Repairs\nR1 and R4 are directly applied to original assertions since\nnot fixing them will definitely result in compilation errors.\nWe thus have original assertions and generated-plus-fixed\nassertions as processed assertions. The assertion file generator\ncreates a generatedAssert.sva file for the processed\nassertions by replacing the golden reference assertion in\ngoldenAssert.sva with the processed assertions.\nD. Simulator\nThe simulator builds SystemVerilog projects for each as-\nsertion using the SV A file generated by the assertion file\ngenerator and the corresponding testbench files. These projects\nare simulated using Siemens Modelsim [45] to generate the\nTable II\nAUTOMATED SYNTAX /TYPOGRAPHICAL FIXES USED BY FRAMEWORK .\nFix ID Description Justification\nR1 Remove any characters that are not\nASCII Non-ASCII characters can affect tooling\nR2 Remove characters after “endmodule” Assertion should not span across modules\nR3 Remove characters enclosed in\ntriple quotes (”””)\nCode in tripe quotes is most likely a\nstray Python comment, and is not valid\nSystemVerilog syntax\nR4 Add “endmodule” if not present “endmodule” is the required keyword to\nend any module in SystemVerilog\nTable III\nASSERTIONS GENERATED , COMPILED , SIMULATED , AND CORRECT BY THE CODE -DAVINCI -002LLM.\nBM\nID\n# assertions % generated % compiled % correct\ngenerated assertions compiled assertions simulated simulated assertions\nOriginal Processed Original Processed Original Processed Original Processed\nAll Unique All Unique All Unique All Unique\nBM1 22,680 22,696 16,718 21.02 21.0 12.47 94.15 94.15 87.04 35.36 35.36 11.41\nBM2 22,680 22,688 21,137 22.77 22.77 18.53 85.21 85.21 82.56 13.7 13.7 6.28\nBM3 22,680 22,687 20,839 35.21 35.2 30.65 94.31 94.32 92.92 13.25 13.25 6.4\nBM4 22,680 22,682 9,100 54.83 54.82 22.0 98.28 98.28 93.21 79.88 79.88 48.98\nBM5 22,680 22,683 18,957 45.31 45.31 39.55 98.96 98.96 98.57 16.22 16.22 10.59\nBM6 22,680 22,682 12,532 62.12 62.11 35.22 98.84 98.84 96.31 15.58 15.58 14.51\nBM7 22,680 22,694 17,912 33.38 33.37 24.15 98.11 98.11 96.69 4.46 4.46 3.18\nBM8 22,680 22,689 16,996 34.64 34.63 19.52 96.3 96.3 91.26 14.38 14.38 6.11\nBM9 22,680 22,685 15,408 28.73 28.72 17.96 97.18 97.18 93.42 19.99 19.99 5.84\nBM10 22,680 22,687 16,338 23.95 23.94 17.54 97.81 97.81 95.85 30.31 30.31 7.21\nTotal 226,800 226,873 165,937 36.2 36.19 23.85 96.69 96.69 93.57 26.54 26.54 10.18\nsimulation log. This log has the assertion violation trace data.\nListing 6 is an example simulation log. Lines 6,9,12 indicate\nviolations of the processed assertion, whereas Lines 7,10,13\nindicate violations of the golden reference assertion.\nE. Scoreboard\nThe scoreboard collects simulation logs, parses assertion\nviolation data of the processed and reference golden assertion,\nand compares them. The generated assertion is “correct” if it\ntriggers violations for the same set of inputs as that of the\ngolden assertion.\nIV. E XPERIMENTAL SETUP AND RESULTS\nA. Study Overview\nWe use our framework to evaluate the performance of a\nlarge language model (LLM) in generating hardware security\nassertions. Our experiments aim to answer the two research\nquestions, RQ1 and RQ2. To this extent, we use the most popu-\nlar code generation engine, OpenAI’s Codex [23] (specifically,\ncode-davinci-002) as our LLM. At the time of experi-\nmentation, code-davinci-002engine is the most capable\nengine among the OpenAI engines designed to “understand”\nand generate code. Later we use LLMs such as OpenAI’s\nCodex [23] code-cushman-001, codegen-2b-ft [46],\nand ChatGPT [47] to demonstrate scaling of our framework.\nB. Evaluation Setup\nWe ran experiments on 32-core, 2.6 GHz Intel Xeon with\n512 GB of RAM running CentOS Linux release 7.9.2009.\nAll the LLMs we use are unmodified original versions. They\nare models trained with large amounts of data from pub-\nlic text data and code repositories [48]–[50]. We use the\nten benchmarks introduced in Section III-A to evaluate the\ncode-davinci-002 LLM. While the benchmarks BM1\nand BM2 are manually crafted “toy” designs, the rest rep-\nresent a wide range of real-world designs, vulnerabilities, and\ncommon weakness enumeration (CWE) categories.\nLLM configuration. The code-davinci-002 LLM is\nconfigured to generate a maximum of 256 tokens, as the\nlargest golden reference assertion among our benchmarks only\nneeds about 160 tokens. For stop tokens, “endmodule” is used\nbecause the SV A file should not have any logic after this\nkeyword (i.e., the assertion should end here). The top P and\npresence penalty parameters are set to their default values of\n1 and 0, and n is set to generate 10 assertions for every\nquery. These default settings are particularly advantageous for\ngenerating security assertions. By setting top P to 1, the model\noperates deterministically, ensuring that the tokens selected\nare most probable in the given context. This deterministic\napproach yields outputs that stand out for their clarity and\nlogical consistency, qualities that are of utmost importance\nin security assertions. Ambiguity in this context could intro-\nduce potential vulnerabilities. Additionally, setting a presence\npenalty to 0 ensures that the model neither artificially promotes\nnor suppresses specific tokens. This neutrality in generation\nensures that the resulting assertions stem purely from the\nmodel’s foundational training and the context provided. Such\nunbiased generation of assertions is pivotal in the security\ndomain, as any undue influence or bias in the assertions could\neither lead to overlooking potential threats or unnecessarily\nover-specifying constraints.\nEach benchmark has three comment strings and four ex-\nample assertions . Combined with the two different synonym\nwords that can be used for each comment string and three\nof the four example assertions , they result in six variants of\ncomment strings and seven variants of example assertions .\nThese combined with the three design source strings and three\nassertion beginning strings result in (6 × 7 × 3 × 3) = 378\nunique query strings (i.e., prompt strings).\nWe vary the temperature and frequency penalty parameters\n0 500 1000 1500 2000\nPrompts\n0\n20\n40\n60\n80\n100\n% correct\nsimulated assertions\nFigure 3. Distribution of accuracy of all the 2,268 prompt configurations\nsorted in descending order.\nof the LLM with the values {0.4, 0.9}, and {0, 0.5, 1}, respec-\ntively, in our experiments. Thus, for each prompt string, the\nframework generates six queries, one with each combination of\ntemperature and frequency penalty covering a range of differ-\nent parameter configurations of Codex, resulting in querying\nthe engine with 2,268 prompts for each benchmark .\nWhile we use these values in our evaluations, one can\nconfigure the framework to use any other values by sim-\nply updating its configuration file. Our framework is auto-\nmated, starting from generating the prompts to evaluating the\nperformance of the LLM engine. We use Siemens Model-\nsim [45] as the simulation tool. We analyze the performance\nof code-davinci-002LLM on the benchmarks to answer\nresearch questions RQ1 and RQ2 and evaluate other LLMs to\ndemonstrate scalability of the framework, as explained next.\nC. RQ1: Can LLMs generate hardware assertions?\nTable III presents the performance of the\ncode-davinci-002 LLM over the benchmark suite.\nFor each benchmark, the LLM is queried 2,268 times with\nn = 10, as discussed in Section IV-B, thus generating 22,680\nassertions. These are then processed (see Section III-C)\nand deduplicated, with values presented in the “# assertions\ngenerated” column. For each column, “Original” assertions\nare generated by the LLM, while “Processed” assertions\ninclude assertions generated by the LLM and the repaired\ncopies of assertions generated by the assertion file generator.\nColumn “All” considers the total number of processed\nassertions, while column “Unique” considers the unique\nnumber of assertions. The number of unique assertions is\nthe number of unique processed assertions after filtering out\nduplicate whitespace. The number of compiled, simulated,\nand correct assertions are presented as a percentage of the\nnumber of assertions generated, compiled, and simulated,\nrespectively, as explained below.\nWe check the initial validity of each generated assertion by\nsimulating them with the hardware simulator, Modelsim [45].\nThis helps identify assertions with invalid syntax or ones\nthat refer to invalid signals (i.e., signals not included in the\ninterface of the SV A file, see Line 3 in Listing 1 for example),\nparameter names, or values. The percentage of successful\ncompilations among the generated assertions are shown in the\n“% generated assertions compiled” column.\nThe next check involves ensuring the assertions can be\nfunctionally simulated. This means that the simulations reach\nthe end of their respective testbenches after all the possible\ncombinations of input values are tested. This column is im-\nportant as some assertions may get stuck during the simulation\n(for example, by using SystemVerilog constructs like $stop).\nThis value is presented as “% compiled assertions simulated”.\nHowever, not all the simulated assertions are correct. We\nconsider an assertion “correct” only if it triggers violations for\nthe same set of input values as that of the golden reference\nassertion. We present the percentage of such correct assertions\namong all the simulated assertions in the “% correct simulated\nassertions” column. This column shows that the LLM can\ngenerate correct assertions for all 10 benchmarks.\nWhile our framework uses golden reference assertions to\nautomatically determine the correctness of the generated as-\nsertions, even in the absence of such reference assertions, our\nframework can automatically prune out non-compiling and\nnon-simulating assertions using the hardware simulator and\noutput only the successfully simulated assertions. Thus, the “%\ncorrect simulated assertions” column depicts the “accuracy” of\nthe framework in generating correct corrections.\nOn average, across all the benchmarks, the framework gen-\nerated correct assertions with 26.54% accuracy. However, this\nis the average accuracy across all the 2,268 types of prompt\nconfigurations. We show the distribution of the accuracy of\nthese 2,268 prompt types through Figure 3. Of these, the\nhighest prompt configuration achieves 93.55% accuracy, while\nthere are 44 prompt configurations with at least 80% accuracy.\nOn the other hand, the least 595 prompt configurations resulted\nin less than 5% accuracy.\nTakeaway. Results from Table III indicate that LLMs\ncan generate hardware security assertions. This being said,\nin addition to correct assertions, it generates incorrect and\nnon-compiling assertions. Hence, understanding what might\nencourage an LLM to function reliably is the goal of RQ2.\nD. RQ2: How does LLM perform for different prompts?\nWe analyze Codex code-davinci-002 LLM’s perfor-\nmance with different combinations of values for the compo-\nnents of the query: (i) example assertions and comment strings,\n(ii) design source codes and comment strings, (iii) assertion be-\nginning strings, (iv) synonym strings, and (v) temperature and\nfrequency penalty values. Figures 4 to 8 show the distribution\nof correct assertions generated by each combination among\nall the correct assertions generated by the LLM. Appendix A\ndemonstrates the accuracy of these combinations as the percent\nof correct simulated assertions through Figures 9 to 13.\n1) Example assertions and comment strings: The example\nassertion and comment string are important elements of the\nprompt string as they offer context for the prompt, providing\n‘hints’ about the assertion. The following inferences can be\nmade from their performance as shown in Figures 4 and 11:\n• Combinations with NoEx did not perform well on any of the\nbenchmarks. This is likely due to the lack of context for the\ntarget assertion informing the LLM to generate an assertion\nrather than a normal design code.\n• Combinations such as ( TrivialEx, BriefCom) and ( BasicEx,\nVeryBriefCom) did not perform well on any benchmark except\nfor BM3 and BM4. This shows that not all queries perform\nsimilarly on all the benchmarks.\n• On average, across all benchmarks, ( DetailedEx, Detailed-\nCom), (BasicEx, DetailedCom), and (BasicEx, BriefCom) pairs\nare the top three performers with 72.16%, 39.2%, and 24.62%\naccuracy respectively (see Figure 11). DetailedCom combina-\ntions performed the best because they elaborate all the details\nof the assertion, such as correct signal names and timing\ninformation, allowing the LLM to generate correct assertion.\nTakeaway. The results indicate that of the 12 combinations\nof the prompt strings, the best three combinations result in\nnearly 70% of the correct assertions (see Figure 4). This\nreinforces the need to identify and use suitable prompt strings\nto maximize the correctness of the generated assertions. In\ngeneral, the greater the detail in the prompt, the greater the\nprobability of generating correct assertions.\n2) Design source codes and comment strings: The LLM\ninfers the information about the target assertion, such as signal\nnames and timing information from the design source code and\nthe comment string in the prompt. The information in design\nsource code is not easy to infer since it is mixed with other\ndesign logic and distributed across multiple modules. On the\nother hand, the information in the comment string is easy to\ninfer since it describes the properties of the target assertion and\nis located near the assertion. Hence, the information provided\nthrough comment string is useful to the LLM in generating a\ncorrect assertion. However, comment stings need to be input by\nBM1BM2BM3BM4BM5BM6BM7BM8BM9BM10Avg.\nBenchmarks\n0\n20\n40\n60\n80\n100% correct assertions\nDetailedEx, DetailedCom\nDetailedEx, BriefCom\nDetailedEx, VeryBriefCom\nBasicEx, DetailedCom\nBasicEx, BriefCom\nBasicEx, VeryBriefCom\nTrivialEx, DetailedCom\nTrivialEx, BriefCom\nTrivialEx, VeryBriefCom\nNoEx, DetailedCom\nNoEx, BriefCom\nNoEx, VeryBriefCom\nFigure 4. Percentage of correct assertions generated by the\ncode-davinci-002 LLM when using combinations of example\nassertions and comment strings.\nBM1BM2BM3BM4BM5BM6BM7BM8BM9BM10Avg.\nBenchmarks\n0\n20\n40\n60\n80\n100% correct assertions\nGoldenDUT, DetailedCom\nGoldenDUT, BriefCom\nGoldenDUT, VeryBriefCom\nBuggyDUT, DetailedCom\nBuggyDUT, BriefCom\nBuggyDUT, VeryBriefCom\nEmptyDUT, DetailedCom\nEmptyDUT, BriefCom, \nEmptyDUT, VeryBriefCom\nFigure 5. Percentage of correct assertions generated by\ncode-davinci-002 LLM when using different combinations of\ndesign source codes and comment strings.\nBM1BM2BM3BM4BM5BM6BM7BM8BM9BM10Avg.\nBenchmarks\n0\n20\n40\n60\n80\n100% correct assertions\nProperty to check that,\nProperty to check that\nProperty to check that,\nAssert that\nAssert that,\nProperty to check that\nAssert that,\nAssert that\nNoEx,\nProperty to check that\nNoEx,\nAssert that\nFigure 6. atPercentage of correct assertions generated by code-davinci\n-002 LLM when using different synonym string combinations.\nthe user, unlike the design source code . Hence, it is essential\nto evaluate the dependence of LLM on the information in\nthe design source code and the comment string parts of the\nprompt string. Figure 5 and Figure 12 show how the LLM\nperforms when using different types of design source codes\nand comment strings. The following inferences can be drawn:\n• Combinations with DetailedCom perform well in most\nof the benchmarks with (GoldenDUT, DetailedCom) , (Bug-\nguDUT, DetailedCom) , and (EmptyDUT, DetailedCom)\nachieving 48.79%, 45.59% and 48.83% correct simulated\nassertions, respectively (see Figure 12). This is because all\nthe information required to generate the target assertion is\nprovided in the DetailedCom.\n• EmptyDUT performed poorly when used with VeryBrief-\nCom and BriefCom because the complete information about\nthe target assertion is neither in the design source code nor in\nthe comment string.\n• GoldenDUT and BuggyDUT prompts are able to generate\ncorrect assertions even when using BriefCom because the LLM\ninfers the information about the assertion from the design\nsource code. However, not all benchmarks perform well with\nthese combinations because it is hard to infer information from\ndesign source code in complex designs.\n• On average, GoldenDUT prompts performed the best with\n(GoldenDUT, DetailedCom) , (GoldenDUT, BriefCom) , and\n(GoldenDUT, VeryBriefCom) achieving 48.79%, 19.69% and\n15.8% accuracy, respectively (see Figure 12) as they contain\ncorrect information about the benchmark.\nTakeaway. The results indicate that either design source code\nor comment string should contain the information about the\nbenchmark design and the target assertion to generate correct\nassertions. While providing details through comment string is\nmore effective, LLMs should be trained to utilize the design\nsource code as that minimizes the user input.\n3) Synonym strings: The comments in example assertions\nand comment strings begin with different synonyms such as\nAssert that and Property to check that . Figure 6 and Figure 9\nshow the performance of assertions when using different\ncombinations of synonym strings in the comments. Here, NoEx\nindicates that the example did not use any of the synonyms\nbecause no example was used in these prompts.\n• All the benchmarks have a similar distribution of the re-\nsults, indicating that the synonyms used in the example and\ncomment string are independent of the assertion.\n• The combinations with NoEx did not perform well with\n(NoEx, Property to check that) and (NoEx, Assert that) achiev-\nBM1 BM2 BM3 BM4 BM5 BM6 BM7 BM8 BM9 BM10 Avg.\nBenchmarks\n0\n20\n40\n60\n80\n100% correct assertions\nNormalBeg\nShortBeg\nEmptyBeg\nFigure 7. Percentage of correct assertions generated by the\ncode-davinci-002 LLM when using different combinations of\nassertion beginning strings.\ning 4.68% and 6.05% accuracy, respectively (see Figure 9).\nThis is not because of the synonym used but because the\nprompts with NoEx generated < 2% of correct assertions on\naverage (see Figure 4).\n• Excluding NoEx combinations, the remaining synonym\ncombinations performed equally well in generating the as-\nsertions with accuracy between 27% and 30%. However,\nthe combinations ( Property to check that , Assert that ) and\n(Property to check that , Property to check that ) performed\nbetter than the other combinations.\nTakeaway.The results indicate that as long as the comment is\nincluded in the prompt, using different synonyms of the same\nword has little impact on the assertion generation.\n4) Assertion beginnings: Providing the beginning of the\nassertion sets the context for the LLM engine in generating\nthe assertions. We evaluated all the benchmarks with the\nthree different assertion beginning strings as discussed in\nSection III-A. Figure 7 and Figure 10 plot these results. The\nfollowing inferences can be made from this comparison:\n• All three assertion beginning strings performed similarly on\nmost benchmarks with NormalBeg, ShortBeg, and EmptyBeg\nachieving 27.51%, 27.17%, and 24.65% correct simulated\nassertions, respectively (see Figure 10). This shows that adding\nthe beginning of an assertion in the prompt has little impact on\nthe assertions generated. One reason is that the prompts have\nexample assertions and comment strings detailing the assertion\nthat sets the context for the LLM.\n• On BM4 and BM8, providing the beginning of an assertion\nimproved the correctness of the assertions. This is because\nthese assertions require SystemVerilog generate construct.\nNormalBeg and ShortBeg both provide genvar keyword that\nhints the LLM engine to use generateconstruct to generate\ncorrect assertions.\n• On average, all the three assertion beginning strings per-\nformed equally with NormalBeg performing the best due to\nits better performance on BM4 and BM8.\nTakeaway. For most benchmarks, EmptyBeg, i.e., the empty\nassertion beginning is sufficient to generate correct asser-\ntions. For assertions requiring constructs other than the trivial\nassert keyword, providing the beginning keyword of the\nconstruct such as genvarimproves the performance of LLM.\n5) Temperature and frequency penalty values: The asser-\ntions generated by the LLM depend on the values of various\nhyper-parameters used apart from the prompt string itself.\nWith higher temperature values, the output becomes more\ncreative and diverse. Also, a higher frequency penalty value\ndiscourages the LLM from repeating the text. We vary the\ntemperature and the frequency penalty values to analyze their\nimpact on the assertion generation. Figure 8 and Figure 13\nplot these results. The following inferences can be made:\n• The performance of the LLM using different parameters\nis benchmark dependent. For example, for BM3, temp=0.9\ngenerated > 75% of the correct assertions, while for BM9,\nthe same generated < 25% of the correct assertions.\n• The combinations with temp=0.4 generated around 10%\nmore correct assertions compared to the ones with temp=0.9.\nHowever, temp=0.9 generated assertions with more accuracy.\nFor example (temp=0.9, freq penalty=1) achieved 34.25% ac-\ncuracy compared to (temp=0.4, freq penalty=1) with 25.32%\naccuracy (see Figure 13). This is because the lower temper-\nature values generated many syntactically correct assertions\nthat had incorrect logical condition required for the assertion,\nreducing their accuracy.\n• The average performance across all benchmarks for dif-\nferent frequency penalty values is nearly same with < 5%\ndifference in accuracy for a given temperature value. How-\never, the performance on specific benchmarks varied based\non the frequency penalty values. For example, for BM7,\n(temp=0.4, freq penalty=1) and (temp=0.4, freq penalty=0)\nachieved 0.23% and 6.56% accuracy, respectively.\nTakeaway. The temperature value of 0.9 performs well in\nterms of accuracy compared to 0.4 even though it generated\nless number of correct assertions. In most cases, the frequency\npenalty value of 0 or 0.5 is more likely to generate correct\nassertions compared to 1.\n6) Best performing configurations: To thoroughly evaluate\nthe capabilities of LLMs in generating hardware assertions, our\nframework generates 2,268 (see Section IV-B) different types\nof prompts for each target assertion. Among these, we include\na wide range of prompts (including ones that do not provide\nsufficient context or information for the LLM to generate the\nassertion) for the sake of completeness of the evaluation. For\nexample, the VeryBriefCom type comment string and NoEx\ntype example assertions in our prompts provide little to no\ncontext about the target assertion. On the other hand, our\nframework also generates highly contextual prompts for which\ncorrect assertions are generated almost always.\nWe elaborate on the best individual accuracy of each of\nthe different prompt configurations (averaged across all the\nbenchmarks) in Table IV. Each row represents one of the\nBM1 BM2 BM3 BM4 BM5 BM6 BM7 BM8 BM9BM10Avg.\nBenchmarks\n0\n20\n40\n60\n80\n100% correct assertions\ntemp=0.9,\nfreq_penalty=1\ntemp=0.9,\nfreq_penalty=0.5\ntemp=0.9,\nfreq_penalty=0\ntemp=0.4,\nfreq_penalty=1\ntemp=0.4,\nfreq_penalty=0.5\ntemp=0.4,\nfreq_penalty=0\nFigure 8. Comparison of the percentage of correct assertions generated\nby the code-davinci-002 LLM when using different combinations of\ntemperature and frequency penalty values.\nTable IV\nACCURACY OF THE BEST PERFORMING PROMPT CONFIGURATIONS . HERE , PC IS “PROPERTY TO CHECK THAT ” AND AT IS “ASSERT THAT ”\nPrompt\nRank\nDesign\nSource Code\nSynonym\nString\nExample\nAssertion\nBeginning\nString\nComment\nString Temperature Frequency\nPenalty\n% Correct\nSimulated Assertions\n1 EmptyDUT PC,PC DetailedEx ShortBeg DetailedCom 0.4 1 93.55\n2 EmptyDUT AT,AT DetailedEx NormalBeg DetailedCom 0.4 1 93.48\n3 GoldenDUT AT,PC DetailedEx ShortBeg DetailedCom 0.4 1 91.67\n4 BuggyDUT AT,PC DetailedEx ShortBeg DetailedCom 0.4 1 90.48\n7 BuggyDUT AT,PC DetailedEx EmptyBeg DetailedCom 0.4 1 89.47\n8 EmptyDUT PC,PC DetailedEx EmptyBeg DetailedCom 0.9 1 89.47\n9 GoldenDUT PC,AT DetailedEx EmptyBeg DetailedCom 0.4 1 87.5\n13 EmptyDUT PC,AT BasicEx ShortBeg DetailedCom 0.9 1 85.71\n26 GoldenDUT AT,PC DetailedEx ShortBeg DetailedCom 0.9 0.5 82.14\n46 GoldenDUT AT,AT DetailedEx ShortBeg DetailedCom 0.9 0 79.03\n123 GoldenDUT AT,PC TrivialEx NormalBeg DetailedCom 0.9 1 71.43\n225 EmptyDUT AT,AT BasicEx ShortBeg BriefCom 0.9 0.5 61.9\n249 GoldenDUT PC,AT DetailedEx NormalBeg VeryBriefCom 0.9 1 57.89\n287 EmptyDUT NoEx,AT NoEx EmptyBeg DetailedCom 0.4 1 50\n539 BuggyDUT NoEx,PC NoEx ShortBeg DetailedCom 0.9 0.5 33.33\n2,268 possible prompt configurations (not all 2,268 rows are\nshown due to space limitations). The Prompt Rank is the\nrank of this configuration based on its accuracy across all the\nbenchmarks. It can be seen that with DetailedCom comment\nstring where the assertions are detailed in the comments, LLM\ncan generate assertions with 93.55% accuracy compared to\nthe 26.54% average accuracy across all prompt configurations.\nWhile this configuration cangenerate non-security assertions,\nLLMs are expected to generate security assertions while\nrequiring minimal information from the prompts, such as\nthe BriefCom (61.9% accuracy) and VeryBriefCom (57.89%\naccuracy) configurations to minimize human effort. The rest of\nthe configurations, other than the ones involvingNoEx example\nstring, were able to generate at least one prompt with > 70%\naccuracy. The accuracy is low for NoEx configurations with\nthe best accuracy of only 50% since the context is not properly\nset for the LLM to generate assertions.\n7) Relative importance of prompt’s components: The per-\nformance of LLMs, when varying the five different prompt\ncomponents resulted in different distributions as shown in\nFigures 4 to 13. The top configuration for example assertion\nand component string alone generated more than 50% of the\ncorrect assertions (see Figure 4) while all the three configura-\ntions for assertion beginnings generated a nearly equal number\nof correct assertions (see Figure 7). On the other hand, the\nthree different configurations for design source code generated\nnearly the same percentage of correct assertions as long as\nDetailedCom comment string is used (see Figure 5). More-\nover, among the last 427 ranking configurations that failed\nto generate correct assertions, > 90% configurations have\nVeryBriefCom comment string while < 12% use DetailedEx\nexample string. In contrast, the remaining configuration types\nare nearly equally likely to occur.\nHence, the following key takeaways can be inferred: (i)\nGenerating correct assertions requires careful crafting of ex-\nample assertions and comment strings because the probability\nof generating correct varies largely based on the type of\nexample assertions and comment strings used in the prompt.\n(ii) The design source code , synonym strings , and assertion\nbeginnings each have nearly equal distributions. While NoEx\ncombinations in synonym strings perform less compared to\nother synonym string types, this is because of the low prob-\nability of NoEx in generating assertions and not the synonym\nstring used. So, values for these components of the prompt\nare less important. (iii) The different values of temperature\nand frequency penalty also had less impact, except for some\nbenchmarks such as BM7, where careful selection of their\nvalues is required to generate correct assertions.\nE. Scalability To Other LLM Engines\nTo demonstrate the scalability of our framework, we eval-\nuated three different LLMs along with code-davinci-002 with\nour benchmark suite using the prompt configuration that\nprovides the most context. This query uses GoldenDUT as\nthe design source code , DetailedEx as the assertion example\nwith property to check that synonym, DetailedCom as the\ncomment string with assert that synonym, NormalBeg as the\nassertion beginning string. The temperature value is 0.4 and\nthe frequency penalty value is 0. The three other LLMs are:\n(i) OpenAI Codex [23] code-cushman-001, (ii) codegen-2b-\nft [46], and (iii) ChatGPT [47] (Jan 9 2023 version). Table V\nshows the results of this evaluation, demonstrating that our\nframework can scale to different LLMs. code-davinci-002 cor-\nrectly generated assertions for nine of the 10 benchmarks with\nthis single query, while a similar engine, code-cushman-001,\ngenerated correct assertions for seven benchmarks. codegen-\n2b-ft only generated assertions for four benchmarks. ChatGPT\ngenerated corrections for only five benchmarks because it\ntries to explain the code in the query and provide generic\nsuggestions to create assertions for that benchmark rather than\ngenerating the target assertion.\nTable V\nEVALUATION USING DIFFERENT LLM ENGINES .\ncode-davinci-002 code-cushman-001 codegen-2b-ft ChatGPT\nBM1 ✓ ✓ ✓ ✓\nBM2 ✓ ✗ ✗ ✓\nBM3 ✗ ✓ ✗ ✗\nBM4 ✓ ✗ ✓ ✓\nBM5 ✓ ✓ ✓ ✗\nBM6 ✓ ✓ ✓ ✓\nBM7 ✓ ✗ ✗ ✗\nBM8 ✓ ✓ ✗ ✓\nBM9 ✓ ✓ ✗ ✗\nBM10 ✓ ✓ ✗ ✗\nListing 7. Correct assertions for BM2 generated by Codex.\n1 // generated assertion 1\n2 assert property (@(posedge clk) (signal == RED) |->\n$past(signal) == RED | $past(signal) == YELLOW);,→\n3\n4 // generated assertion 2\n5 assert property (@(posedge clk) ( (signal[1:0] == RED)\n|-> ( ($past(signal[1:0]) == RED) |\n($past(signal[1:0]) == YELLOW) ) )) ;\n,→\n,→\n6\n7 // generated assertion 3\n8 assert property (@(posedge clk) (signal == RED) |->\n($past(signal) == YELLOW | $past(signal) == RED));,→\n9\n10 // generated assertion 4\n11 always @ (posedge clk)\n12 if(signal == RED)\n13 assert($past(signal) == RED || $past(signal) ==\nYELLOW);,→\nF . Other Observations\nMultiple correct assertions are possible for a benchmark\neven though the assertions target specific vulnerabilities in\nthe design. For example, the assertion for BM2 should make\nsure that the traffic light changes to RED only from YELLOW.\nListing 7 shows correct assertions generated by Codex when\nqueried. Our scoreboard qualifies the presence of duplicate\nassertions by filtering out similar assertions and determining\nthe unique assertions as listed in Table III.\nMultiple generated assertions . A single query to the LLM\ncan result in the generation of more than one assertions in\nits response. For instance, Listing 8 shows the responses\ngenerated by Codex that have more than one assertion. Having\nmore than one assertion in the response of LLM can lead to\nthe correctness or incorrectness of the assertion. The response\nat Lines 1-3 consists of multiple essential assertions, i.e.,\nthe response is correct because of the presence of multiple\nassertions. On the other hand, in the case of the response at\nLines 14 to 18, the generated response is incorrect even though\nit has the target assertion (Line 15) because the assertion\nat Line 18 is incorrect, resulting in incorrect behavior in\nsimulation. Moreover, for responses such as Lines 5-12, the\nadditional assertion (Line 12) does not impact the correctness;\nit is a subset or repeat of other assertions in the response. Note\nthat multiple assertions in the response of LLM indicate that a\nsemi-colon cannot be the stop sequence as it limits the LLM\nto generating one assertion.\nIncorrect assertions . From Table III, it can be seen that not\nall the generated assertions are correct. A majority of the\ngenerated assertions failed to compile, while not all compiled\nassertions are correct. We analyze the incorrect assertions\ngenerated by the LLM for BM1 and categorize their errors into\ndifferent types in Listing 9 (each of these incorrect assertions\ncan have multiple types of errors). We could not provide an\nexhaustive list of all the errors the LLM generated due to\nspace limitations. However, this list gives meaningful insights\nto improve the next-generation models as these errors are\nfrequently repeated by the LLM when generating assertions.\nA majority of incorrect assertions have invalid Verilog\nsyntax, invalid variables, or incorrect variable indices errors.\nAssertions with such errors will either fail to compile or\nsimulate. Invalid verilog syntax errors occur due to the use\nof invalid indexing ( $past in Line 2 and *-1 in Line\n6), invalid keywords ( &amp in Line 4), or invalid built-in\nmethods ( $prev in Line 8). LLMs should be trained with\nmore Verilog code to reduce these errors. Invalid variables\nerrors use variables not relevant to the assertions, i.e., variables\nnot included in the interface signals of the SV A file ( w_en\nin Line 11 and data_out in Line 13). Incorrect variable\nindices errors use incorrect indices for signals. For example,\ndata signal in BM1 is a single-bit register while the LLM\ntreats it as a multi-bit register (Line 16 and Line 18).\nAdditional logic after assertion often results in compilation\nerrors since the additional logic generated by LLMs is mostly\nirrelevant to the assertion (Lines 21-29). Incorrect responses\nfrom Listing 8 are also examples of this error. Improving the\nstopping conditions when generating tokens in the response\ncan reduce these errors. Incorrect logic , includes additional\nunwanted logic, and incorrect timing errors are more advanced\nerrors that are often simulated successfully but will be trig-\ngered for different sets of inputs compared to the golden\nassertion ( lock && !lock in Line 32, data == data\nListing 8. Responses by OpenAI Codex with multiple assertions per response.\n1 // BM8 response -- the multiple assertions present are\nrequired for correctness,→\n2 assert property (@(posedge clk) $past(rst_i) |->\n(reglk_mem[0] == {BIT_WIDTH{1'b1}}));,→\n3 assert property (@(posedge clk) $past(rst_i) |->\n(reglk_mem[1] == {BIT_WIDTH{1'b1}}));,→\n4\n5 // BM1 response -- has correct but redundant assertions\n6 assert property (@(posedge clk) ($past(lock) == 1) |->\n($rose(data) == 0));,→\n7\n8 // property to check that the data is not changed if the\nlock is set,→\n9 assert property (@(posedge clk) ($past(lock) == 1) |->\n($fell(data) == 0));,→\n10\n11 // property to check that the data is not changed if the\nlock is set,→\n12 assert property (@(posedge clk) ($past(lock) == 1) |->\n($stable(data) == 1));,→\n13\n14 // BM3 response -- has unwanted assertion leading to\nincorrect response,→\n15 assert property (@(posedge clk) (data == RED) |->\n($past(data) == RED) || ($past(data) == YELLOW));,→\n16\n17 // assert that the WALK signal is always preceeded by\nthe RED or the YELLOW signals,→\n18 assert property (@(posedge clk) (walk == 1'b1) |->\n($past(data) == RED ) || ($past(data) =\nYELLOW));\n,→\n,→\nin Line 34, $rose(clk) in Line 37, ##2 in Line 40, lock\nshould be $past(lock) in Line 42, and #20 in Line 44).\nV. D ISCUSSION AND LIMITATIONS\nA. Completeness of Benchmark Suite\nOur experimental work featured ten realistic golden refer-\nence assertion benchmarks (Table I) that we used to evaluate\nthe correctness of the LLM-generated assertions. While our\nbenchmarks represent a range of properties, future work can\nconsider an even broader range of CWE types and vulner-\nabilities that were not explored in this paper. For all our\nbenchmarks, we created the different levels of comment strings\nListing 9. Types of incorrect assertions generated by OpenAI Codex for BM1.\n1 ////// Invalid Verilog Syntax //////\n2 assert property (@(posedge clk) (lock == 1) |->\n(data_out == data_out[$past]));,→\n3\n4 assert property (@(posedge clk) ((lock == 1) &amp;&amp;\n(r_en == 1)) |-> (data == $past(data)));,→\n5\n6 assert property (@(posedge clk) (lock == 1) |->\n(data_out == data_out[*-1]));,→\n7\n8 assert property (@(posedge clk) $prev(lock) == 1 |->\n$prev(data) == data);,→\n9\n10 ////// Invalid Variables //////\n11 assert property (@(posedge clk) (w_en == 0) |-> (data ==\n$past(data)));,→\n12\n13 assert property (@(posedge clk) ($past(lock) == 1) |->\n(data_out == $past(data_out)));,→\n14\n15 ////// Incorrect Variable Indices //////\n16 assert property (@(posedge clk) (lock == 1) |-> (data\n== data[1]));,→\n17\n18 assert property (@(posedge clk) (lock == 1) |->\n(data[7:0] == previous $past(data_in[7:0])));,→\n19\n20 ////// Additional Logic After Assertion //////\n21 assert property (@(posedge clk) (lock == 0) |-> (data_in\n!= data_out));,→\n22 lock_reg v1 (.data_in(data), .data_out(),\n.rst(rst), .clk(clk),,→\n23 .w_en(), .r_en());\n24\n25 assert property (@(posedge clk) (lock == 1) |-> (#20\n@(posedge clk) r_en == $past(w_en)));,→\n26 lock = 0;\n27 clk = 0;\n28 rst = 0;\n29 initial begin\n30\n31 ////// Incorrect Logic of Assertion Condition //////\n32 assert property (@(posedge clk) (lock && !lock) |=>\n(lock_in == 1));,→\n33\n34 assert property (@(posedge clk) (lock == 1) |-> (data\n== data));,→\n35\n36 ////// Includes Additional Unwanted Logic //////\n37 assert property (@(posedge clk) ($past(lock) == 1) |->\n($rose(clk) |-> ($past(data) == data)));,→\n38\n39 ////// Incorrect Timing //////\n40 assert property (@(posedge clk) (lock) |-> ##2\n($past(data) == data));,→\n41\n42 assert property (@(posedge clk) (lock == 1) |-> ( $past\n( data) == data));,→\n43\n44 assert property (@(posedge clk) (lock == 1) |-> (#20\ndata == $past(data)));,→\nbased on our human judgment of their relative complexi-\nty/detail. Except for BM9 and BM10, we also designed the\nreference assertions based on our assessment of the example\nvulnerabilities/CWE types. As such, our reference assertions\nare only one of several possible ways to capture the desired\nsecurity property related to each vulnerability/CWE. Our com-\nment strings similarly represent only a small subset of possible\nways to express intent in the prompt to the LLM – other ways\nof constructing the prompt could be explored in future work.\nB. Simulation Testbench\nOur simulation testbenches were exhaustive (checked equiv-\nalence to golden reference for all combinations of all\nsignals)—see Section III-A. For scale reasons, we parameter-\nized the signal widths to reduce data comparisons, e.g., treating\n32-bit signals as 2-bit signals so that only 22 values (instead\nof 232) are checked. This assumes that assertion triggering\nbetween reference and generated assertion holds over the data\nwidth change. Furthermore, as our reference assertions are\nnot the definitive way to express the security properties, it is\npossible a generated assertion could be a better representation\nof the security intent but is marked as incorrect by the simple\ncriteria of mismatch with hand-crafted references.\nSome assertions involve checking signal values at current\nand previous clock cycles. As an example, assume that an\nassertion involves 10 binary signals. There are 210 values in a\ngiven cycle – to check an assertion in a current and previous\ncycle, we need to test 210 × 210 combinations. For assertions\nthat track values across more cycles, we need to test more\ncombinations. For our benchmarks, we manually ascertained\nthe number of clock cycles required for the reference assertion\nin the design of the corresponding testbench.\nIn some instances, simulation of the assertions takes longer\nthan expected time. All reference assertions are exhaustively\ntested in less than 100 seconds of wall-clock time. So we set\na timeout of 1000 seconds of wall-clock time to handle the\ncases where the generated assertion has constructs that would\ncause the simulation to “hang” (e.g., SystemVerilog’s $stop\nmethod). In such situations, we marked an assertion as correct\nif its functional behavior matched the reference assertion.\nC. Use-cases of our Framework\nOur framework is designed primarily to evaluate LLMs in\ngenerating hardware security assertions. While we evaluated\nOpenAI’s code-davinci-002, other LLMs could be used.\nWe envision that our framework can help fine-tune open LLMs\nfor generating assertions. With regards to using our framework\nto generate assertions in a production environment, however,\npractical challenges require further investigation. Our results\nshow that some level of detail is required in the comments\ncapturing security intent, which suggests that our framework\nas-is is not usable without some amount of security expertise.\nRelated works like UNDINE [39] could assist by mining for\ninvariant properties. Preparing the prompt involves selecting\nthe appropriate “module part,” – identifying which signals are\nrelevant, and what additional context should be in the prompt.\nThis remains an open area of inquiry.\nTable VI\nFOLLOW -ON RESEARCH USING LLM S.\nPaper Summary Differences from our work\nLLM4SecHW\n[51]\nUses trained LLMs to detect and fix bugs from target\nhardware design’s source.\nThis work does not generate hardware assertions to detect bugs.\nMeng et\nal. [52]\nUses natural language processing (NLP) to extract security\nproperties from specification documents.\nThis work generates security properties as English sentences with NLP while\nwe generate hardware assertions using LLMs.\nDIV AS [53] Maps templated SoC security policies into CWEs and\ngenerates assertions using LLMs.\nOur work generates assertions directly from source code. We also extensively\nevaluate capabilities of LLMs for assertion generation.\nChen et\nal. [54]\nEvaluates whether LLMs correctly respond to security and\nprivacy misconceptions.\nOur work evaluates the capabilities of LLMs in generating hardware\nassertions.\nOrenes-\nVera [55]\nEvaluates capabilities of LLMs in generating hardware\nassertions and hardware RTL code.\nWhile our work uses dynamic simulations to verify the correctness of\nassertions generated by LLMs, this work uses formal tools.\nLLM4DV\n[56]\nEvaluates the capabilities of LLMs in generating test stimuli\nfor hardware verification.\nOur work generates hardware assertions. Both assertion and stimuli\ngeneration are required to detect vulnerabilities.\nOur work focused on concurrent assertions in separate\nSV A files, keeping the design files unmodified. Generating\nassertions like immediate assertions that are written into the\nhardware designs could be future work. For this process to be\npractical, evaluating whether or not the assertions (a) analyze\nthe security properties (i.e., are relevant) and (b) accurately\nassess this property (i.e., are correct) will require expertise,\nespecially when golden reference assertions are not present.\nIn the absence of reference assertions, we can verify the\nassertions generated by LLMs by formally proving them. Sim-\nilar techniques are employed by existing automated assertion\ngeneration techniques [20]. Other than formal methods, we can\ninsert assertions into the hardware and verify using regression\ntesting [11]. Violations of these assertions will need to be\nmanually analyzed to determine the presence of bugs. Thus,\neven in the absence of golden reference assertions, the manual\neffort required will be largely minimized.\nD. Future Work\nWhile our framework demonstrates the potential of LLMs in\ngenerating hardware assertions, LLMs hold immense potential\nin assisting various security verification tasks. Table VI lists\nsuch new security research works using LLMs or extracting\nsecurity properties published after we released our current\nwork on arXiv [57]. We provide a summary of those works\nfor the readers to have the latest picture of this emerging area.\nVI. C ONCLUSION\nThis paper provides the first insights on using LLMs such\nas OpenAI’s Codex for the automatic creation of Security\nAssertions for Hardware. We contributed a new pipeline for\nthe evaluation of LLMs for this task, as well as provided a\nbenchmark suite of scenarios and golden reference assertions.\nUsing this for the code-davinci-002LLM, we generated\n226,800 assertions under a variety of environmental condi-\ntions and with differing amounts of context. We found that,\ngiven sufficient context in the prompt, LLMs can achieve an\naccuracy of up to 93.55% across all the benchmarks. When\nevaluated with prompts of varying levels of context, LLM\nachieved an average accuracy of 26.54% across 2,268 prompt\ntypes. These results indicate this approach is valid, and that\nLLMs will have a role to play in the future of assertion\ngeneration.\nVII. A CKNOWLEDGEMENT\nOur research work was partially funded by the US Office\nof Naval Research (ONR Award #N00014-18-1-2058). This\nresearch work is also supported in part by a gift from Intel\nCorporation. This work does not in any way constitute an Intel\nendorsement of a product or supplier. Any opinions, findings,\nconclusions, or recommendations expressed herein are those\nof the authors and do not necessarily reflect those of the US\nGovernment or Intel.\nREFERENCES\n[1] M. Rostami, F. Koushanfar, and R. Karri, “A Primer on Hardware\nSecurity: Models, Methods, and Metrics,” Proceedings of the IEEE, vol.\n102, no. 8, pp. 1283–1295, Aug. 2014.\n[2] K. Xiao et al., “Hardware Trojans: Lessons Learned after One Decade\nof Research,” ACM Transactions on Design Automation of Electronic\nSystems (TODAES), vol. 22, no. 1, pp. 6:1–6:23, May 2016.\n[3] A. Chakraborty et al. , “Keynote: A Disquisition on Logic Locking,”\nIEEE Transactions on Computer-Aided Design of Integrated Circuits\nand Systems, vol. 39, no. 10, pp. 1952–1972, Oct. 2020.\n[4] K. Basu et al. , “CAD-Base: An Attack Vector into the Electronics\nSupply Chain,” ACM Transactions on Design Automation of Electronic\nSystems, vol. 24, no. 4, pp. 38:1–38:30, Apr. 2019.\n[5] M. Ender, A. Moradi, and C. Paar, “The unpatchable silicon: a full break\nof the bitstream encryption of xilinx 7-series FPGAs,” USENIX Security\nSymposium, pp. 1803–1819, 2020.\n[6] D. Price, “Pentium FDIV flaw-lessons learned,” IEEE Micro, vol. 15,\nno. 2, pp. 86–88, 1995.\n[7] G. Dessouky et al., “Hardfails: Insights into Software-Exploitable Hard-\nware Bugs,” USENIX Security Symposium , pp. 213–230, 2019.\n[8] T. S. Tan and B. A. Rosdi, “Verilog HDL Simulator Technology: A\nSurvey,” Journal of Electronic Testing , vol. 30, no. 3, pp. 255–269.\n[9] C. Kern and M. R. Greenstreet, “Formal verification in hardware design:\na survey,” ACM Transactions on Design Automation of Electronic\nSystems, vol. 4, no. 2, pp. 123–193.\n[10] J. Rajendran, V . Vedula, and R. Karri, “Detecting Malicious Modifica-\ntions of Data in Third-Party Intellectual Property Cores,” Proceedings\nof the 52nd Annual Design Automation Conference , pp. 1–6, 2015.\n[11] T. Trippel et al., “Fuzzing Hardware Like Software,” USENIX Security\nSymposium, pp. 3237–3254, 2022.\n[12] R. Kande et al. , “TheHuzz: Instruction Fuzzing of Processors Using\nGolden-Reference Models for Finding Software-Exploitable Vulnerabil-\nities,” USENIX Security Symposium , pp. 3219–3236, 2022.\n[13] A. Ardeshiricham et al. , “Register transfer level information flow\ntracking for provably secure hardware design,” in Design, Automation\nTest in Europe Conference Exhibition (DATE), 2017 , Mar. 2017, pp.\n1691–1696, iSSN: 1558-1101.\n[14] C. Chen et al. , “HyPFuzz: Formal-Assisted Processor Fuzzing,” arXiv\npreprint arXiv:2304.02485, 2023.\n[15] J. Jiao et al., “GLAIVE: Graph Learning Assisted Instruction Vulnera-\nbility Estimation,” pp. 82–87, 2021.\n[16] H. D. Foster, A. C. Krolnik, and D. J. Lacey, Assertion-based design .\nSpringer Science & Business Media, 2004.\n[17] H. Witharana et al. , “A Survey on Assertion-based Hardware Verifica-\ntion,” ACM Computing Surveys , vol. 54, no. 11s, pp. 225:1–225:33.\n[18] ——, “Automated Generation of Security Assertions for RTL Models,”\nACM Journal on Emerging Technologies in Computing Systems .\n[19] C. B. Harris and I. G. Harris, “GLAsT: Learning formal grammars to\ntranslate natural language specifications into hardware assertions,” in\nDesign, Automation Test in Europe Conf. Exhibition , 2016, pp. 966–\n971.\n[20] S. Vasudevan et al. , “GoldMine: Automatic assertion generation using\ndata mining and static analysis,” in 2010 Design, Automation & Test in\nEurope Conference & Exhibition , pp. 626–629, iSSN: 1558-1101.\n[21] S. Hertz, D. Sheridan, and S. Vasudevan, “Mining Hardware Assertions\nWith Guidance From Static Analysis,” IEEE Transactions on Computer-\nAided Design of Integrated Circuits and Systems , vol. 32, no. 6, pp.\n952–965, Jun. 2013.\n[22] Z. Ren and H. Al-Asaad, “Overview of assertion-based verification and\nits applications,” in International Conference on Embedded Systems,\nCyber-physical Systems, & Applications (ESCS) . CSREA Press, 2016.\n[23] M. Chen et al., “Evaluating Large Language Models Trained on Code,”\nJul. 2021, arXiv:2107.03374 [cs].\n[24] S. I. Association, “Tech Workers in Semiconductor Industry,” https:\n//www.semiconductors.org/america-faces-significant-shortage-of-tec\nh-workers-in-semiconductor-industry-and-throughout-u-s-economy/,\n2023, Last accessed on 04/08/2021.\n[25] R. Mihalcea, H. Liu, and H. Lieberman, “NLP (Natural Language Pro-\ncessing) for NLP (Natural Language Programming),” in Computational\nLinguistics and Intelligent Text Processing , A. Gelbukh, Ed. Springer\nBerlin Heidelberg, 2006, pp. 319–330.\n[26] A. Radford et al., “Language models are unsupervised multitask learn-\ners,” OpenAI blog, vol. 1, no. 8, p. 9, 2019.\n[27] J. Devlin et al., “BERT: Pre-training of Deep Bidirectional Transformers\nfor Language Understanding,” in Proceedings of naacL-HLT . Associ-\nation for Computational Linguistics, Jun. 2019, pp. 4171–4186.\n[28] A. Vaswani et al., “Attention is All you Need,” in Advances in Neural\nInformation Processing Systems, vol. 30. Curran Associates, Inc., 2017.\n[29] P. Gage, “A New Algorithm for Data Compression,” C Users Journal ,\nvol. 12, no. 2, pp. 23–38, Feb. 1994.\n[30] GitHub, “GitHub Copilot · Your AI pair programmer.”\n[31] H. Pearce et al. , “Asleep at the Keyboard? Assessing the Security of\nGitHub Copilot’s Code Contributions,” in 2022 IEEE Symposium on\nSecurity and Privacy (SP) , May 2022, pp. 754–768, iSSN: 2375-1207.\n[32] ——, “Examining Zero-Shot Vulnerability Repair with Large Language\nModels,” Aug. 2022, arXiv:2112.02125 [cs].\n[33] M. Shoeybi et al. , “Megatron-LM: Training Multi-Billion Pa-\nrameter Language Models Using Model Parallelism,” Mar. 2020,\narXiv:1909.08053 [cs].\n[34] E. Nijkamp et al., “A Conversational Paradigm for Program Synthesis,”\nMar. 2022, arXiv:2203.13474 [cs].\n[35] H. Pearce, B. Tan, and R. Karri, “DA VE: Deriving Automatically Verilog\nfrom English,” in Proceedings of the 2020 ACM/IEEE Workshop on\nMachine Learning for CAD . ACM, Nov. 2020, pp. 27–32.\n[36] W. Zhong et al. , “Neural Program Repair : Systems, Challenges and\nSolutions,” in 13th Asia-Pacific Symposium on Internetware . ACM,\nJun. 2022, pp. 96–106.\n[37] H. Pearce et al., “Can OpenAI Codex and Other Large Language Models\nHelp Us Fix Security Bugs?” arXiv:2112.02125 [cs], Apr. 2022, arXiv:\n2112.02125.\n[38] C. Wang et al. , “ASAX: Automatic security assertion extraction for\ndetecting Hardware Trojans,” in 2018 Asia and South Pacific Design\nAutomation Conference, Jan. 2018, pp. 84–89, iSSN: 2153-697X.\n[39] C. Deutschbein and C. Sturton, “Mining Security Critical Linear Tem-\nporal Logic Specifications for Processors,” in 2018 19th International\nWorkshop on Microprocessor and SOC Test and Verification (MTV) ,\nDec. 2018, pp. 18–23, iSSN: 2332-5674.\n[40] O. Keszocze and I. G. Harris, “Chatbot-based assertion generation from\nnatural language specifications,” pp. 1–6, 2019.\n[41] R. Zhang and C. Sturton, “Transys: Leveraging Common Security\nProperties Across Hardware Designs,” in 2020 IEEE Symposium on\nSecurity and Privacy (SP) , May 2020, pp. 1713–1727.\n[42] C. Chen et al. , “Trusting the Trust Anchor: Towards Detecting Cross-\nLayer Vulnerabilities with Hardware Fuzzing,” 59th ACM/IEEE Design\nAutomation Conference, p. 1379–1383, 2022.\n[43] A.-R. Sadeghi, J. Rajendran, and R. Kande, “Organizing The World’s\nLargest Hardware Security Competition: Challenges, Opportunities, and\nLessons Learned,” Great Lakes Symposium on VLSI , p. 95–100, 2021.\n[44] lowRISC contributors, “Open source silicon root of trust (RoT) |\nOpenTitan.”\n[45] Siemens, “Modelsim,” https://eda.sw.siemens.com/en-US/ic/modelsim/,\n2021, Last accessed on 04/08/2021.\n[46] S. Thakur, “Finetuned codegen-2B-Verilog model,” https://huggingface.\nco/shailja, 2022, Last accessed on 01/05/2022.\n[47] OpenAI, “ChatGPT: Optimizing Language Models for Dialogue,” https:\n//openai.com/blog/chatgpt/, 2022, Last accessed on 01/05/2022.\n[48] M. Schade, “Understanding Codex training data and outputs,” https:\n//help.openai.com/en/articles/5480054-understanding-codex-training-d\nata-and-outputs, 2023, Last accessed on 04/08/2021.\n[49] P. P. Ray, “ChatGPT: A comprehensive review on background, applica-\ntions, key challenges, bias, ethics, limitations and future scope,” Internet\nof Things and Cyber-Physical Systems , 2023.\n[50] S. Thakur et al., “Benchmarking Large Language Models for Automated\nVerilog RTL Code Generation,” 2023 Design, Automation & Test in\nEurope Conference & Exhibition (DATE) , pp. 1–6, 2023.\n[51] W. Fu et al. , “LLM4SecHW: Leveraging Domain-Specific Large Lan-\nguage Model for Hardware Debugging,” Asian Hardware Oriented\nSecurity and Trust (AsianHOST) , 2023.\n[52] X. Meng et al., “Unlocking Hardware Security Assurance: The Potential\nof LLMs,” arXiv preprint arXiv:2308.11042 , 2023.\n[53] S. Paria, A. Dasgupta, and S. Bhunia, “DIV AS: An LLM-based End-to-\nEnd Framework for SoC Security Analysis and Policy-based Protection,”\narXiv preprint arXiv:2308.06932 , 2023.\n[54] Y . Chen, A. Arunasalam, and Z. B. Celik, “Can large language models\nprovide security & privacy advice? measuring the ability of llms to refute\nmisconceptions,” Proceedings of the 39th Annual Computer Security\nApplications Conference, pp. 366–378, 2023.\n[55] M. Orenes-Vera, M. Martonosi, and D. Wentzlaff, “From RTL to SV A:\nLLM-assisted generation of Formal Verification Testbenches,” arXiv\npreprint arXiv:2309.09437, 2023.\n[56] Z. Zhang et al., “LLM4DV: Using Large Language Models for Hardware\nTest Stimuli Generation,” arXiv preprint arXiv:2310.04535 , 2023.\n[57] R. Kande et al. , “LLM-assisted Generation of Hardware Assertions,”\narXiv preprint arXiv:2306.14027 , 2023.\nAPPENDIX\nA. ACCURACY OF DIFFERENT PROMPTS\nThis section presents Codex code-davinci-002LLM’s\nBM1 BM2 BM3 BM4 BM5 BM6 BM7 BM8 BM9 BM10 Avg.\nBenchmarks\n0\n20\n40\n60\n80% accuracy\nNoEx,\nAssert that\nNoEx,\nProperty to check that\nAssert that,\nAssert that\nAssert that,\nProperty to check that\nProperty to check that,\nAssert that\nProperty to check that,\nProperty to check that\nFigure 9. Accuracy of code-davinci -002 LLM when using different\nsynonym string combinations.\nBM1 BM2 BM3 BM4 BM5 BM6 BM7 BM8 BM9 BM10 Avg.\nBenchmarks\n0\n20\n40\n60\n80% accuracy\nEmptyBeg\nShortBeg\nNormalBeg\nFigure 10. Accuracy of by the code-davinci-002 LLM when using\ndifferent combinations of assertion beginning strings.\nBM1 BM2 BM3 BM4 BM5 BM6 BM7 BM8 BM9 BM10 Avg.\nBenchmarks\n0\n20\n40\n60\n80\n100% accuracy\nNoEx, VeryBriefCom\nNoEx, BriefCom\nNoEx, DetailedCom\nTrivialEx, VeryBriefCom\nTrivialEx, BriefCom\nTrivialEx, DetailedCom\nBasicEx, VeryBriefCom\nBasicEx, BriefCom\nBasicEx, DetailedCom\nDetailedEx, VeryBriefCom\nDetailedEx, BriefCom\nDetailedEx, DetailedCom\nFigure 11. Accuracy of code-davinci-002 LLM when using combinations of example assertions and comment strings.\nBM1 BM2 BM3 BM4 BM5 BM6 BM7 BM8 BM9 BM10 Avg.\nBenchmarks\n0\n20\n40\n60\n80\n100% accuracy\nEmptyDUT, VeryBriefCom\nEmptyDUT, BriefCom, \nEmptyDUT, DetailedCom\nBuggyDUT, VeryBriefCom\nBuggyDUT, BriefCom\nBuggyDUT, DetailedCom\nGoldenDUT, VeryBriefCom\nGoldenDUT, BriefCom\nGoldenDUT, DetailedCom\nFigure 12. Accuracy of code-davinci-002 LLM when using different combinations of design source codes and comment strings.\naccuracy varying different components of the query, as shown\nin Figures 9 to 13. These performance results are discussed in\nSection IV-D.\nBM1 BM2 BM3 BM4 BM5 BM6 BM7 BM8 BM9 BM10 Avg.\nBenchmarks\n0\n20\n40\n60\n80% accuracy\ntemp=0.4,\nfreq_penalty=0\ntemp=0.4,\nfreq_penalty=0.5\ntemp=0.4,\nfreq_penalty=1\ntemp=0.9,\nfreq_penalty=0\ntemp=0.9,\nfreq_penalty=0.5\ntemp=0.9,\nfreq_penalty=1\nFigure 13. Accuracy of code-davinci-002 LLM when using different\ncombinations of temperature and frequency penalty values.\nRahul Kande (Graduate Student Member, IEEE) is a Ph.D. student\nin Computer Engineering at Texas A&M University, USA. He received his\nB.Tech degree in Electrical and Communications Engineering. from the Indian\nInstitute of Technology, Guwahati, India in 2017. His research interests\ninclude hardware fuzzing, hardware security, and computer architecture. His\ncurrent research involves developing more efficient and automated hardware\nfuzzing techniques and hardware vulnerability detection to accelerate hard-\nware security verification, specifically in SoCs.\nHammond Pearce (Member, IEEE) is a Lecturer (equiv. Assistant Pro-\nfessor) at the University of New South Wales Sydney, Australia, in the\nSchool of Computer Science and Engineering. He received the B.E. (Hons)\ndegree in Computer Systems Engineering and the Ph.D. in Computer Systems\nEngineering both from the University of Auckland, Auckland, New Zealand.\nPreviously, he was a Research Assistant Professor at New York University,\nBrooklyn, NY , USA, in the Department of Electrical and Computer Engineer-\ning and in the NYU Center for Cybersecurity. His main research focus is in\nthe intersection of Large Language Models and cybersecurity, with particular\nfocus on hardware and industrial informatics applications. In 2019 he took\npart in the NASA International Internship Programme and worked at NASA\nAmes in California.\nBenjamin Tan (Member, IEEE) is an Assistant Professor in the Depart-\nment of Electrical and Software Engineering, University of Calgary. His\ncurrent research focuses on improving the security of computer systems at\nthe hardware level and understanding the implications of emerging machine-\nlearning techniques on the IC supply chain and life cycle. Prior to joining the\nUniversity of Calgary, Dr. Tan was a Research Assistant Professor at New\nYork University with the Center for Cybersecurity. His recent research efforts\ninclude projects in collaboration with Intel, and his work has been funded by\nthe Natural Sciences and Engineering Research Council of Canada and the\nNational Science Foundation (USA). He earned his Ph.D. at the University\nof Auckland, New Zealand.\nBrendan Dolan-Gavitt is an Associate Professor at New York University\nin the Department of Computer Science and Engineering. He holds a Ph.D.\nin computer science from Georgia Tech (2014) and a BA in Math and\nComputer Science from Wesleyan University (2006). His research interests\nspan many areas of cybersecurity, including program static and dynamic\nanalysis, virtualization security, memory forensics, and embedded and cyber-\nphysical systems. His work has been presented at top security conferences. He\nalso led the development of PANDA, an open-source platform for architecture-\nneutral dynamic analysis, which has users worldwide and has been featured\nin technical press such as The Register. Prior to joining NYU, he was a\npostdoctoral researcher at Columbia University.\nShailja Thakur is a Postdoctoral Research Associate at New York Univer-\nsity in the Department of Electrical and Computer Engineering. She received\nher Ph.D. in Electrical and Computer Engineering from the University of\nWaterloo, Ontario, Canada, in 2022, the Masters of Technology in Computer\nScience from IIIT Delhi, India in 2012, and the Bachelors of Technology\nin Computer Science from GGSIPU, New Delhi, India in 2008. Her primary\nresearch focuses on applying large language models for efficient, reliable, and\nsecure cyber-physical systems, with a particular focus on hardware design\nautomation using LLMs. She also works on AI fairness, trustworthiness, and\nprivacy.\nRamesh Karri (Fellow, IEEE) is a Professor of ECE at New York\nUniversity. He co-directs the NYU Center for Cyber Security. He co-founded\nthe Trust-Hub and founded the Embedded Systems Challenge, the annual red\nteam blue team event. He has a Ph.D. in Computer Science, from the UC San\nDiego and a B.E in ECE from Andhra University. His research and education\nin hardware cybersecurity include trustworthy ICs, processors and cyber-\nphysical systems. security-aware computer-aided design, test, verification,\nnano meets security; hardware security competitions, benchmarks and metrics;\nadditive manufacturing security. He has published over 300 articles in leading\njournals and conference proceedings.\nJeyavijayan (JV) Rajendran (Senior Member, IEEE) is currently an As-\nsistant Professor with the Department of Electrical and Computer Engineering,\nTexas A&M University, USA. He received the Ph.D. degree from New York\nUniversity in August 2015. Previously, he was an Assistant Professor at UT\nDallas from 2015 to 2017. His research interests include hardware security\nand computer security. His research has won the NSF CAREER Award in\n2017, the ACM SIGDA Outstanding Young Faculty Award in 2019, the ACM\nSIGDA Outstanding Ph.D. Dissertation Award in 2017, and the Alexander\nHessel Award for the Best Ph.D. Dissertation in the Electrical and Computer\nEngineering Department at NYU in 2016, along with several best student\npaper awards.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8551924228668213
    },
    {
      "name": "Assertion",
      "score": 0.8133041858673096
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5642989873886108
    },
    {
      "name": "Programming language",
      "score": 0.564081609249115
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5016841888427734
    },
    {
      "name": "Suite",
      "score": 0.4782520830631256
    },
    {
      "name": "Code (set theory)",
      "score": 0.4304147958755493
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I91045830",
      "name": "Texas A&M University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I31746571",
      "name": "UNSW Sydney",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I168635309",
      "name": "University of Calgary",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    }
  ],
  "cited_by": 20
}