{
  "title": "Grammar Induction with Neural Language Models: An Unusual Replication",
  "url": "https://openalex.org/W2951699762",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2526928059",
      "name": "Phu Mon Htut",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2115080942",
      "name": "Kyunghyun Cho",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2145255766",
      "name": "Samuel Bowman",
      "affiliations": [
        "New York University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1423339008",
    "https://openalex.org/W2129882630",
    "https://openalex.org/W2104917081",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2963754491",
    "https://openalex.org/W2963580443",
    "https://openalex.org/W2104518905",
    "https://openalex.org/W2157762871",
    "https://openalex.org/W2953084091",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W1495446613",
    "https://openalex.org/W2791751435",
    "https://openalex.org/W2619818172",
    "https://openalex.org/W2949847915",
    "https://openalex.org/W2144916786",
    "https://openalex.org/W145849181",
    "https://openalex.org/W2064675550"
  ],
  "abstract": "Grammar induction is the task of learning syntactic structure without the expert-labeled treebanks (Charniak and Carroll, 1992; Klein and Manning, 2002). Recent work on latent tree learning offers a new family of approaches to this problem by inducing syntactic structure using the supervision from a downstream NLP task (Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2018). In a recent paper published at ICLR, Shen et al. (2018) introduce such a model and report near state-of-the-art results on the target task of language modeling, and the first strong latent tree learning result on constituency parsing. During the analysis of this model, we discover issues that make the original results hard to trust, including tuning and even training on what is effectively the test set. Here, we analyze the model under different configurations to understand what it learns and to identify the conditions under which it succeeds. We find that this model represents the first empirical success for neural network latent tree learning, and that neural language modeling warrants further study as a setting for grammar induction.",
  "full_text": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 371–373\nBrussels, Belgium, November 1, 2018.c⃝2018 Association for Computational Linguistics\n371\nGrammar Induction with Neural Language Models:\nAn Unusual Replication\nPhu Mon Htut1\npmh330@nyu.edu\nKyunghyun Cho1,2\nkyunghyun.cho@nyu.edu\nSamuel R. Bowman1,2,3\nbowman@nyu.edu\n1Center for Data Science\nNew York University\n60 Fifth Avenue\nNew York, NY 10011\n2Dept. of Computer Science\nNew York University\n60 Fifth Avenue\nNew York, NY 10011\n3Dept. of Linguistics\nNew York University\n10 Washington Place\nNew York, NY 10003\n1 Introduction\nGrammar induction is the task of learning syntac-\ntic structure without the expert-labeled treebanks\n(Charniak and Carroll, 1992; Klein and Manning,\n2002). Recent work on latent tree learning of-\nfers a new family of approaches to this problem by\ninducing syntactic structure using the supervision\nfrom a downstream NLP task (Yogatama et al.,\n2017; Maillard et al., 2017; Choi et al., 2018). In a\nrecent paper published at ICLR, Shen et al. (2018)\nintroduce such a model and report near state-of-\nthe-art results on the target task of language mod-\neling, and the ﬁrst strong latent tree learning re-\nsult on constituency parsing. During the analy-\nsis of this model, we discover issues that make\nthe original results hard to trust, including tuning\nand even training on what is effectively the test\nset. Here, we analyze the model under different\nconﬁgurations to understand what it learns and to\nidentify the conditions under which it succeeds.\nWe ﬁnd that this model represents the ﬁrst empiri-\ncal success for neural network latent tree learning,\nand that neural language modeling warrants fur-\nther study as a setting for grammar induction.\n2 Background and Experiments\nWe analyze the Parsing-Reading-Predict-\nNetwork (PRPN; Shen et al., 2018), which uses\nconvolutional networks with a form of structured\nattention (Kim et al., 2017) rather than recursive\nneural networks (Goller and Kuchler, 1996;\nSocher et al., 2011) to learn trees while perform-\ning straightforward backpropagation training on a\nlanguage modeling objective. The structure of the\nmodel seems rather suboptimal: Since the parser\nis trained as part of a language model, it makes\nparsing greedily, with no access to any words to\nthe right of the point where each parsing decision\nmust be made.\nThe experiments on language modeling and\nAcrusadeofNOtotheconsumptionofdrugsisimperative.\nFigure 1 : Parses from PRPN-LM trained on\nAllNLI.\nparsing are carried out using different conﬁgura-\ntions of the model—PRPN-LM tuned for language\nmodeling, and PRPN-UP for (unsupervised) pars-\ning. PRPN-LM is much larger than PRPN-UP,\nwith embedding layer that is 4 times larger and\nthe number of units per layer that is 3 times larger.\nIn the PRPN-UP experiments, we observe that the\nWSJ data is not split, such that the test data is used\nwithout parse information for training. This im-\nplies that the parsing results of PRPN-UP may not\nbe generalizable in the way usually expected of\nmachine learning evaluation results.\nWe train PRPN on sentences from two datasets:\nThe full WSJ and AllNLI, the concatenation\nof SNLI (Bowman et al., 2015) and MultiNLI\n(Williams et al., 2018b). We then evaluate the con-\nstituency trees produced by these models on the\nfull WSJ, WSJ101, and the MultiNLI development\nset.\n3 Results\nTable 1 shows results with all the models un-\nder study, plus several baselines, on WSJ and\nWSJ10. Unexpectedly, the PRPN-LM models\nachieve higher parsing performance than PRPN-\nUP. This shows that any tuning done to sepa-\nrate PRPN-UP from PRPN-LM was not necessary,\nand that the results described in the paper can be\nlargely reproduced by a uniﬁed model in a fair\nsetting. Moreover, the PRPN models trained on\nthe larger, out-of-domain AllNLI perform better\nthan those trained on WSJ. Surprisingly, PRPN-\nLM tained on out-of-domain AllNLI achieves the\nbest F1 score on full WSJ among all the models\n1A processed subset of WSJ in which the sentences con-\ntain no punctuation and no more than 10 words.\n372\nTraining Stopping Vocab Parsing F1 Depth Accuracy on WSJ by Tag\nModel Data Criterion Size WSJ10 WSJ WSJ ADJP NP PP INTJ\nµ (σ) max µ (σ) max\nPRPN-UP AllNLI Train UP 76k 67.5 (0.6) 68.6 38.1 (0.7) 39.1 5.9 27.8 63.0 31.4 52.9\nPRPN-UP AllNLI Train LM 76k 66.3 (0.8) 68.5 39.8 (0.6) 40.7 5.9 26.5 53.0 32.9 52.9\nPRPN-LM AllNLI Train LM 76k 52.4 (4.9) 58.1 42.5 (0.7) 43.6 6.2 34.2 60.1 60.0 64.7\nPRPN-UP WSJ Full UP 15.8k 64.7 (3.2) 70.9 26.6 (1.9) 31.6 5.9 19.3 48.7 19.2 44.1\nPRPN-UP WSJ Full LM 15.8k 64.3 (3.3) 70.8 26.5 (1.9) 31.4 5.9 18.8 48.1 19.1 44.1\nPRPN-UP WSJ Train UP 15.8k 63.5 (3.5) 70.7 26.6 (2.5) 34.2 5.9 21.3 57.2 19.4 47.1\nPRPN-UP WSJ Train LM 15.8k 62.2 (3.9) 70.3 26.4 (2.5) 34.0 5.9 22.3 56.2 19.1 44.1\nPRPN-LM WSJ Train LM 10k 70.5 (0.4) 71.3 38.3 (0.3) 38.9 5.9 26.0 64.4 25.5 50.0\nPRPN-LM WSJ Train UP 10k 66.1 (0.5) 67.2 34.0 (0.9) 36.3 5.9 32.0 58.3 19.6 44.1\n300D ST-Gumbel AllNLI Train NLI – – – 19.0 (1.0) 20.1 – 15.6 18.8 9.9 59.4\nw/o Leaf GRU AllNLI Train NLI – – – 22.8 (1.6) 25.0 – 18.9 24.1 14.2 51.8\n300D RL-SPINN AllNLI Train NLI – – – 13.2 (0.0) 13.2 – 1.7 10.8 4.6 50.6\nw/o Leaf GRU AllNLI Train NLI – – – 13.1 (0.1) 13.2 – 1.6 10.9 4.6 50.0\nCCM WSJ10 Train – – – 71.9 – – – – – – –\nDMV+CCM WSJ10 Train – – – 77.6 – – – – – – –\nUML-DOP WSJ10 Train – – – 82.9 – – – – – – –\nRandom Trees – – – – 34.7 21.3 (0.0) 21.4 5.3 17.4 22.3 16.0 40.4\nBalanced Trees – – – – – 21.3 (0.0) 21.3 4.6 22.1 20.2 9.3 55.9\nTable 1: Unlabeled parsing F1 test results broken down by training data and by early stopping criterion.\nThe Accuracy columns represent the fraction of ground truth constituents of a given type that correspond\nto constituents in the model parses. Italics mark results that are worse than the random baseline. Results\nwith RL-SPINN and ST-Gumbel are from Williams et al. (2018a). WSJ10 baselines are from Klein and\nManning (2002, CCM), Klein and Manning (2005, DMV+CCM), and Bod (2006, UML-DOP).\nStopping F1 wrt.\nModel Criterion LB RB SP Depth\n300D SPINN NLI 19.3 36.9 70.2 6.2\nw/o Leaf GRU NLI 21.2 39.0 63.5 6.4\n300D SPINN-NC NLI 19.2 36.2 70.5 6.1\nw/o Leaf GRU NLI 20.6 38.9 64.1 6.3\n300D ST-Gumbel NLI 32.6 37.5 23.7 4.1\nw/o Leaf GRU NLI 30.8 35.6 27.5 4.6\n300D RL-SPINN NLI 95.0 13.5 18.8 8.6\nw/o Leaf GRU NLI 99.1 10.7 18.1 8.6\nPRPN-LM LM 25.6 26.9 45.7 4.9\nPRPN-UP UP 19.4 41.0 46.3 4.9\nPRPN-UP LM 19.9 37.4 48.6 4.9\nRandom Trees – 27.9 28.0 27.0 4.4\nBalanced Trees – 21.7 36.8 21.3 3.9\nTable 2: Unlabeled parsing F1 on the MultiNLI\ndevelopment set for models trained on AllNLI.F1\nwrt. shows F1 with respect to strictly right- and\nleft-branching (LB/RB) trees and with respect to\nthe Stanford Parser (SP) trees supplied with the\ncorpus; The evaluations of SPINN, RL-SPINN,\nand ST-Gumbel are from Williams et al. (2018a).\nSPINN is a supervised parsing model, and the oth-\ners are latent tree models.\nwe experimented, even though its performance on\nWSJ10 is the lowest of all. Under all the conﬁgu-\nrations we tested, PRPN yields much better perfor-\nmance than that seen with the baselines from Yo-\ngatama et al. (2017, called RL-SPINN) and Choi\net al. (2018, called ST-Gumbel), despite the fact\nthat the model was tuned exclusively for WSJ10\nparsing (Table 1 and 2). This suggests that PRPN\nis strikingly effective at latent tree learning.\nAdditionally, Table 2 shows that both PRPN-UP\nmodels achieve F1 scores of 46.3 and 48.6 respec-\ntively on the MultiNLI dev set, setting the state\nof the art in parsing on this dataset among latent\ntree models. We conclude that PRPN does acquire\nsome substantial knowledge of syntax, and that\nthis knowledge agrees with Penn Treebank (PTB)\ngrammar signiﬁcantly better than chance.\nMoreover, we replicate the language model-\ning perplexity of 61.6 reported in the paper us-\ning PRPN-LM trained on WSJ, which indicates\nthat PRPN-LM is effective at both parsing and lan-\nguage modeling.\n4 Conclusion\nIn our analysis of the PRPN model, we ﬁnd sev-\neral experimental problems that make the results\ndifﬁcult to interpret. However, in the analyses go-\ning well beyond the scope of the original paper,\nwe ﬁnd that PRPN is nonetheless robust. It repre-\nsents a viable method for grammar induction and\nthe ﬁrst success for latent tree learning. We expect\nthat it heralds further work on language modeling\nas a tool for grammar induction research.\n373\nReferences\nRens Bod. 2006. An All-Subtrees Approach to Un-\nsupervised Parsing. Proceedings of the 21st Inter-\nnational Conference on Computational Linguistics\nand the 44th annual meeting of the Association for\nComputational Linguistics, pages 865–872.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large an-\nnotated corpus for learning natural language infer-\nence. In Proceedings of the Conference on Em-\npirical Methods in Natural Language Processing\n(EMNLP). Association for Computational Linguis-\ntics.\nEugene Charniak and Glen Carroll. 1992. Two exper-\niments on learning probabilistic dependency gram-\nmars from corpora. In Proceedings of the AAAI\nWorkshop on Statistically-Based NLP Techniques ,\npage 113.\nJihun Choi, Kang Min Yoo, and Sang-goo Lee. 2018.\nLearning to compose task-speciﬁc tree structures.\nIn Proceedings of the Thirty-Second Association for\nthe Advancement of Artiﬁcial Intelligence Confer-\nence on Artiﬁcial Intelligence (AAAI-18), volume 2.\nChristoph Goller and Andreas Kuchler. 1996. Learn-\ning task-dependent distributed representations by\nbackpropagation through structure. In Proceedings\nof International Conference on Neural Networks\n(ICNN’96).\nYoon Kim, Carl Denton, Luong Hoang, and Alexan-\nder M. Rush. 2017. Structured attention networks.\nDan Klein and Christopher D. Manning. 2002. A\ngenerative constituent-context model for improved\ngrammar induction. In Proceedings of the 40th An-\nnual Meeting on Association for Computational Lin-\nguistics - ACL ’02, page 128.\nDan Klein and Christopher D. Manning. 2005. Nat-\nural language grammar induction with a genera-\ntive constituent-context model. Pattern Recognition,\n38(9):1407–1419.\nJean Maillard, Stephen Clark, and Dani Yogatama.\n2017. Jointly learning sentence embeddings and\nsyntax with unsupervised Tree-LSTMs. arXiv\npreprint 1705.09189.\nYikang Shen, Zhouhan Lin, Chin wei Huang, and\nAaron Courville. 2018. Neural language modeling\nby jointly learning syntax and lexicon. In Interna-\ntional Conference on Learning Representations.\nRichard Socher, Cliff Chiung-Yu Lin, Andrew Ng, and\nChris Manning. 2011. Parsing Natural Scenes and\nNatural Language with Recursive Neural Networks.\nIn Proceedings of the 28th International Conference\non Machine Learning, pages 129–136.\nAdina Williams, Andrew Drozdov, and Samuel R.\nBowman. 2018a. Do latent tree learning models\nidentify meaningful structure in sentences? Trans-\nactions of the Association for Computational Lin-\nguistics (TACL).\nAdina Williams, Nikita Nangia, and Samuel R. Bow-\nman. 2018b. A broad-coverage challenge corpus for\nsentence understanding through inference. In Pro-\nceedings of the North American Chapter of the As-\nsociation for Computational Linguistics (NAACL).\nDani Yogatama, Phil Blunsom, Chris Dyer, Edward\nGrefenstette, and Wang Ling. 2017. Learning to\nCompose Words into Setences with Reinforcement\nLearning. Proceedings of the International Confer-\nence on Learning Representations, pages 1–17.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8095650672912598
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7428990602493286
    },
    {
      "name": "Parsing",
      "score": 0.6761693954467773
    },
    {
      "name": "Natural language processing",
      "score": 0.6735073328018188
    },
    {
      "name": "Grammar",
      "score": 0.6516458988189697
    },
    {
      "name": "Tree (set theory)",
      "score": 0.5305227041244507
    },
    {
      "name": "Task (project management)",
      "score": 0.5297644734382629
    },
    {
      "name": "Artificial neural network",
      "score": 0.4582395553588867
    },
    {
      "name": "Language model",
      "score": 0.441798597574234
    },
    {
      "name": "Tree structure",
      "score": 0.4300568997859955
    },
    {
      "name": "Grammar induction",
      "score": 0.4247419238090515
    },
    {
      "name": "Test set",
      "score": 0.4173336327075958
    },
    {
      "name": "Machine learning",
      "score": 0.4084581136703491
    },
    {
      "name": "Rule-based machine translation",
      "score": 0.25741690397262573
    },
    {
      "name": "Linguistics",
      "score": 0.25007736682891846
    },
    {
      "name": "Data structure",
      "score": 0.11957600712776184
    },
    {
      "name": "Programming language",
      "score": 0.0890493392944336
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    }
  ]
}