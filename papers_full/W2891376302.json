{
    "title": "Decipherment of Substitution Ciphers with Neural Language Models",
    "url": "https://openalex.org/W2891376302",
    "year": 2018,
    "authors": [
        {
            "id": "https://openalex.org/A2890098091",
            "name": "Nishant Kambhatla",
            "affiliations": [
                "Simon Fraser University"
            ]
        },
        {
            "id": "https://openalex.org/A2266877400",
            "name": "Anahita Mansouri Bigvand",
            "affiliations": [
                "Simon Fraser University"
            ]
        },
        {
            "id": "https://openalex.org/A2126895602",
            "name": "Anoop Sarkar",
            "affiliations": [
                "Simon Fraser University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1982616741",
        "https://openalex.org/W2136346830",
        "https://openalex.org/W2123595463",
        "https://openalex.org/W2163930480",
        "https://openalex.org/W1986823956",
        "https://openalex.org/W764450821",
        "https://openalex.org/W2135391077",
        "https://openalex.org/W2137766642",
        "https://openalex.org/W2172267041",
        "https://openalex.org/W2962801086",
        "https://openalex.org/W2046521405",
        "https://openalex.org/W2062508496",
        "https://openalex.org/W2746618409",
        "https://openalex.org/W2606347107",
        "https://openalex.org/W2107791851",
        "https://openalex.org/W2047603832",
        "https://openalex.org/W2784107021",
        "https://openalex.org/W179875071"
    ],
    "abstract": "Decipherment of homophonic substitution ciphers using language models is a well-studied task in NLP. Previous work in this topic scores short local spans of possible plaintext decipherments using n-gram language models. The most widely used technique is the use of beam search with n-gram language models proposed by Nuhn et al.(2013). We propose a beam search algorithm that scores the entire candidate plaintext at each step of the decipherment using a neural language model. We augment beam search with a novel rest cost estimation that exploits the prediction power of a neural language model. We compare against the state of the art n-gram based methods on many different decipherment tasks. On challenging ciphers such as the Beale cipher we provide significantly better error rates with much smaller beam sizes.",
    "full_text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 869–874\nBrussels, Belgium, October 31 - November 4, 2018.c⃝2018 Association for Computational Linguistics\n869\nDecipherment of Substitution Ciphers with Neural Language Models\nNishant Kambhatla, Anahita Mansouri Bigvand, Anoop Sarkar\nSchool of Computing Science\nSimon Fraser University\nBurnaby, BC , Canada\n{nkambhat,amansour,anoop}@sfu.ca\nAbstract\nDecipherment of homophonic substitution ci-\nphers using language models (LMs) is a well-\nstudied task in NLP. Previous work in this\ntopic scores short local spans of possible plain-\ntext decipherments using n-gram LMs. The\nmost widely used technique is the use of beam\nsearch with n-gram LMs proposed by Nuhn\net al. (2013). We propose a beam search al-\ngorithm that scores the entire candidate plain-\ntext at each step of the decipherment using a\nneural LM. We augment beam search with a\nnovel rest cost estimation that exploits the pre-\ndiction power of a neural LM. We compare\nagainst the state of the art n-gram based meth-\nods on many different decipherment tasks. On\nchallenging ciphers such as the Beale cipher\nwe provide signiﬁcantly better error rates with\nmuch smaller beam sizes.\n1 Introduction\nBreaking substitution ciphers recovers the plain-\ntext from a ciphertext that uses a 1:1 or homo-\nphonic cipher key. Previous work using pre-\ntrained language models (LMs) for decipherment\nuse n-gram LMs (Ravi and Knight, 2011; Nuhn\net al., 2013). Some methods use the Expectation-\nMaximization (EM) algorithm (Knight et al.,\n2006) while most state-of-the-art approaches for\ndecipherment of 1:1 and homophonic substitution\nciphers use beam search and rely on the clever\nuse of n-gram LMs (Nuhn et al., 2014; Hauer\net al., 2014). Neural LMs globally score the en-\ntire candidate plaintext sequence (Mikolov et al.,\n2010). However, using a neural LM for decipher-\nment is not trivial because scoring the entire candi-\ndate partially deciphered plaintext is computation-\nally challenging. We solve both of these problems\nin this paper and provide an improved beam search\nbased decipherment algorithm for homophonic ci-\nphers that exploits pre-trained neural LMs for the\nﬁrst time.\n2 Decipherment Model\nWe use the notation from Nuhn et al. (2013). Ci-\nphertext fN\n1 = f1..fi..fN and plaintext eN\n1 =\ne1..ei..eN consist of vocabularies fi ∈ Vf and\nei ∈Ve respectively. The beginning tokens in the\nciphertext (f0) and plaintext (e0) are set to “$” de-\nnoting the beginning of a sentence. The substi-\ntutions are represented by a function φ : Vf →\nVe such that 1:1 substitutions are bijective while\nhomophonic substitutions are general. A cipher\nfunction φwhich does not have everyφ(f) ﬁxed is\ncalled a partial cipher function (Corlett and Penn,\n2010). The number of fs that are ﬁxed in φ is\ngiven by its cardinality. φ′is called an extension\nof φ, if f is ﬁxed in φ′ such that δ(φ′(f),φ(f))\nyields true ∀f ∈Vf which are already ﬁxed in φ\nwhere δis Kronecker delta. Decipherment is then\nthe task of ﬁnding the φfor which the probability\nof the deciphered text is maximized.\nˆφ= arg max\nφ\np(φ(f1)...φ(fN)) (1)\nwhere p(.) is the language model ( LM). Find-\ning this argmax is solved using a beam search al-\ngorithm (Nuhn et al., 2013) which incrementally\nﬁnds the most likely substitutions using the lan-\nguage model scores as the ranking.\n2.1 Neural Language Model\nThe advantage of a neural LM is that it can be used\nto score the entire candidate plaintext for a hypoth-\nesized partial decipherment. In this work, we use a\nstate of the art byte (character) level neural LM us-\ning a multiplicative LSTM (Radford et al., 2017).\nConsider a sequence S = w1,w2,w3,...,w N.\nThe LM score of Sis SCORE(S):\nP(S) =P(w1,w2,w3,...,w N)\nP(S) =\nN∏\ni=1\nP(wi |w1,w2,...,w i−1))\nSCORE(S) =−\nN∑\ni=1\nlog(P(wi |w<i))\n(2)\n870\n2.2 Beam Search\nAlgorithm 1 is the beam search algorithm (Nuhn\net al., 2013, 2014) for solving substitution ci-\nphers. It monitors all partial hypotheses in lists\nHs and Ht based on their quality. As the\nsearch progresses, the partial hypotheses are ex-\ntended, scored with SCORE and appended to\nHt. EXT LIMITS determines which extensions\nshould be allowed and EXT ORDER picks the next\ncipher symbol for extension. The search continues\nafter pruning: Hs ←HISTOGRAM_PRUNE(Ht).\nWe augment this algorithm by updating the\nSCORE function with a neural LM.\nAlgorithm 1Beam Search for Decipherment\n1: function (BEAM SEARCH (EXT ORDER, EXT LIM-\nITS))\n2: initialize sets Hs, Ht\n3: CARDINALITY = 0\n4: Hs.ADD((∅,0))\n5: while CARDINALITY <|Vf | do\n6: f = EXT ORDER[CARDINALITY]\n7: for all φ∈ Hs do\n8: for all e∈ Ve do\n9: φ’ :=φ∪ {(e,f)}\n10: if EXT LIMITS(φ’)then\n11: Ht.ADD(φ’,SCORE(φ’))\n12: HISTOGRAM PRUNE(Ht)\n13: CARDINALITY = CARDINALITY + 1\n14: Hs = Ht\n15: Ht.CLEAR()\n16: return WINNER(Hs)\n3 Score Estimation (SCORE)\nScore estimation evaluates the quality of the\npartial hypotheses φ. Using the example\nfrom Nuhn et al. (2014), consider the vo-\ncabularies Ve = {a,b,c,d } and Vf =\n{A,B,C,D }, extension order (B,A,C,D ), and\nciphertext $ ABDDCABCDADCABDC $. Let φ=\n{(a,A),(b,B))}be the partial hypothesis. Then\nSCORE(φ) scores this hypothesized partial deci-\npherment (only A and B are converted to plain-\ntext) using a pre-trained language model in the hy-\npothesized plaintext language.\n3.1 Baseline\nThe initial rest cost estimator introduced by Nuhn\net al. nuhnbeam computes the score of hypothe-\nses only based on partially deciphered text that\nbuilds a shard of nadjacent solved symbols. As a\nheuristic, n-grams which still consist of unsolved\ncipher-symbols are assigned a trivial estimate of\nprobability 1. An improved version of rest cost es-\ntimation (Nuhn et al., 2014) consults lower order\nn-grams to score each position.\n3.2 Global Rest Cost Estimation\nThe baseline scoring method greatly relies on local\ncontext, i.e.the estimation is strictly based on par-\ntial character sequences. Since this depends solely\non the n-gram LM, the true conditional probabil-\nity under Markov assumption is not modeled and,\ntherefore, context dependency beyond the window\nof (n−1) is ignored. Thus, attempting to utilize a\nhigher amount of context can lower the probability\nof some tokens resulting in poor scores.\nWe address this issue with a new improved ver-\nsion of the rest cost estimator by supplement-\ning the partial deciphermentφ(fN\n1 ) with predicted\nplaintext text symbols using our neural language\nmodel (NLM). Applying φ= {(a,A),(b,B))}to\nthe ciphertext above, we get the following partial\nhypothesis:\nφ(fN\n1 ) =$a1b2...a6b7..a10..a13b14..$\nWe introduce a scoring function that is able to\nscore the entire plaintext including the missing\nplaintext symbols. First, we sample 1 the plaintext\nsymbols from the NLM at all locations depend-\ning on the deciphered tokens from the partial hy-\npothesis φsuch that these tokens maintain their re-\nspective positions in the sequence, and at the same\ntime are sampled from the neural LM to ﬁt (prob-\nabilistically) in this context. Next, we determine\nthe probability of the entire sequence including the\nscores of sampled plaintext as our rest cost esti-\nmate.\nNLM \nIn our running example, this would yield a score\nestimation of the partial decipherment, φ(fN\n1 ) :\nφ(fN\n1 ) = $ a1b2d3c4c5a6b7c8d9a10d11d12a13b14d15c16 $\nThus, the neural LM is used to predict the score of\nthe full sequence. This method of global scoring\nevaluates each candidate partial decipherment by\nscoring the entire message, augmented by the sam-\n1The char-level sampling is done incrementally from left\nto right to generate a sequence that contains the deciphered\ntokens from φat the exact locations they occur in the above\nφ(fN\n1 ). If the LM prediction contradicts the hypothesized\ndecipherment we stop sampling and start from the next char-\nacter.\n871\nCipher Length Unique\nSymbols\nObs/\nsymbol\nZodiac-408 408 54 7.55\nBeale Pt. 2 763 180 4.23\nTable 1: Homophonic ciphers used in our experiments.\npled plaintext symbols from the NLM. Since more\nterms participate in the rest cost estimation with\nglobal context, we use the plaintext LM to provide\nus with a better rest cost in the beam search.\n3.3 Frequency Matching Heuristic\nAlignment by frequency similarity (Yarowsky and\nWicentowski, 2000) assumes that two forms be-\nlong to the same lemma when their relative fre-\nquency ﬁts the expected distribution. We use\nthis heuristic to augment the score estimation\n(SCORE):\nFMH(φ′) =\n⏐⏐⏐⏐log\n(ν(f)\nν(e)\n)⏐⏐⏐⏐ f ∈Vf, e ∈Ve\n(3)\nν(f) is the percentage relative frequency of the ci-\nphertext symbol f, while ν(e) is the percentage\nrelative frequency of the plaintext token e in the\nplaintext language model. The closer this value to\n0, the more likely it is that f is mapped to e.\nThus given a φwith the SCORE(φ), the exten-\nsion φ′(Algo. 1) is scored as:\nSCORE(φ′) =SCORE(φ) +NEW(φ′) −FMH(φ′)\n(4)\nwhere NEW is the score for symbols that have been\nnewly ﬁxed in φ′ while extending φ to φ′. Our\nexperimental evaluations show that the global rest\ncost estimator and the frequency matching heuris-\ntic contribute positively towards the accuracy of\ndifferent ciphertexts.\n4 Experimental Evaluation\nWe carry out 2 sets of experiments: one on let-\nter based 1:1, and another on homophonic sub-\nstitution ciphers. We report Symbol Error Rate\n(SER) which is the fraction of characters in the\ndeciphered text that are incorrect.\nThe character NLM uses a single layer multi-\nplicative LSTM (mLSTM) (Radford et al., 2017)\nwith 4096 units. The model was trained for a sin-\ngle epoch on mini-batches of 128 subsequences of\nlength 256 for a total of 1 million weight updates.\nStates were initialized to zero at the beginning of\neach data shard and persisted across updates to\nsimulate full-backprop and allow for the forward\npropagation of information outside of a given sub-\nsequence. In all the experiments we use a charac-\nter NLM trained on English Gigaword corpus aug-\nmented with a short corpus of plaintext letters of\nabout 2000 words authored by the Zodiac killer2.\n4.1 1:1 Substitution Ciphers\nIn this experiment we use a synthetic 1:1 let-\nter substitution cipher dataset following Ravi and\nKnight (2008), Nuhn et al. (2013) and Hauer et\nal. (2014). The text is from English Wikipedia\narticles about history 3, preprocessed by stripping\nthe text of all images, tables, then lower-casing all\ncharacters, and removing all non-alphabetic and\nnon-space characters. We create 50 cryptograms\nfor each length 16, 32, 64, 128 and 256 using a\nrandom Caesar-cipher 1:1 substitution.\nLength Beam SER(%) 1 SER(%) 2\n64 100 4.14 4.14\n1,000 1.09 1.04\n10,000 0.08 0.12\n100,000 0.07 0.07\n128 100 7.31 7.29\n1,000 1.68 1.55\n10,000 0.15 0.09\n100,000 0.01 0.02\nTable 2: Symbol Error Rates (SER) based on Neural\nLanguage Model and beam size (Beam) for solving\n1:1 substitution ciphers of lengths 64 and 128, respec-\ntively. SER 1 shows beam search with global scoring,\nand SER 2 shows beam search with global scoring with\nfrequency matching heuristic.\nFigure 1: Symbol error rates for decipherment of 1:1\nsubstitution ciphers of different lengths. The beam size\nis 100k. Beam 6-gram model uses the beam search\nfrom Nunh et al. (2013).\n2https://en.wikisource.org/wiki/Zodiac Killer letters\n3http://en.wikipedia.org/wiki/History\n872\n! 2\nI H A V E D E P O S I T E D I N T H E C O U N T Y O\nF B E D F O R D A B O U T F O U R M I L E S F R O M\nB U F O R D S I N A N E X C A V A T I O N O R V A U\nL T S I X F E E T B E L O W T H E S U R F A C E O F\nT H E G R O U N D T H E F O L L O W I N G A R T I C\nL E S B E L O N G I N G J O I N T L Y T O T H E P A\n115 73 24 807 37 52 49 17 31 62 647 22 7 15 140 47 29 107 79 84 56 239 10 26 811 5\n196 308 85 52 160 136 59 211 36 9 46 316 554 122 106 95 53 58 2 42 7 35 122 53 31 82\n77 250 196 56 96 118 71 140 287 28 353 37 1005 65 147 807 24 3 8 12 47 43 59 807 45 316\n101 41 78 154 1005 122 138 191 16 77 49 102 57 72 34 73 85 35 371 59 196 81 92 191 106 273\n60 394 620 270 220 106 388 287 63 3 6 191 122 43 234 400 106 290 314 47 48 81 96 26 115 92\n158 191 110 77 85 197 46 10 113 140 353 48 120 106 2 607 61 420 811 29 125 14 20 37 105 28\nFigure 2: First few lines from part two of the Beale cipher. The letters have been capitalized.\nFig 1 plots the results of our method for cipher\nlengths of 16, 32, 64, 128 and 256 alongside Beam\n6-gram (the best performing model) model (Nuhn\net al., 2013)\n4.2 An Easy Cipher: Zodiac-408\nZodiac-408, a homophonic cipher, is commonly\nused to evaluate decipherment algorithms.\nBeam SER (%) 1 SER (%) 2\n10k 3.92 3.18\n100k 2.40 1.91\n1M 1.47 1.22\nTable 3: Symbol Error Rates (SER) based on Neu-\nral Language Model and beam size (Beam) for deci-\nphering Zodiac-408, respectively. SER 1 shows beam\nsearch with global scoring, and SER 2 shows beam\nsearch with global scoring with the frequency match-\ning heuristic.\nOur neural LM model with global rest cost es-\ntimation and frequency matching heuristic with a\nbeam size of 1M has SER of 1.2% compared to\nthe beam search algorithm (Nuhn et al., 2013) with\nbeam size of 10M with a 6-gram LM which gives\nan SER of 2%. The improved beam search (Nuhn\net al., 2014) with an 8-gram LM, however, gets 52\nout of 54 mappings correct on the Zodiac-408 ci-\npher.\n4.3 A Hard Cipher: Beale Pt 2\nPart 2 of the Beale Cipher is a more challeng-\ning homophonic cipher because of a much larger\nsearch space of solutions. Nunh et al. (2014) were\nthe ﬁrst to automatically decipher this Beale Ci-\npher.\nWith an error of 5% with beam size of 1M vs\n5.4% with 8-gram LM and a pruning size of 10M,\nour system outperforms the state of the art (Nuhn\net al., 2014) on this task.\n! 1\nI L I K E K I L L I N G P E O P L\nE B E C A U S E I T I S S O M U C\nH F U N I T I S M O R E F U N T H\nA\nA N K I L L I N G W I L D G A M E\nI N T H E F O R T E S T B E C A U\nS E M A N I S T H E M O S T D A N\nG E R T U E A N A M A L O F A L L\nFigure 3: First 119 characters from deciphered Zodiac-\n408 text. The letters have been capitalized.\nBeam SER (%) 1 SER (%) 2\n10k 41.67 48.33\n100k 7.20 10.09\n1M 4.98 5.50\nTable 4: Symbol Error Rates (SER) based on Neural\nLanguage Model and beam size (Beam) for decipher-\ning Part 2 of the Beale Cipher. SER 1 shows beam\nsearch with global scoring, and SER 2 shows beam\nsearch with global scoring with the frequency match-\ning heuristic.\n5 Related Work\nAutomatic decipherment for substitution ciphers\nstarted with dictionary attacks (Hart, 1994; Jakob-\nsen, 1995; Olson, 2007). Ravi and Knight (2008)\nframe the decipherment problem as an integer lin-\near programming (ILP) problem. Knight et al.\n(2006) use an HMM-based EM algorithm for solv-\ning a variety of decipherment problems. Ravi and\nKnight (2011) extend the HMM-based EM ap-\nproach with a Bayesian approach, and report the\n873\nﬁrst automatic decipherment of the Zodiac-408 ci-\npher.\nBerg-Kirkpatrick and Klein (2013) show that\na large number of random restarts can help the\nEM approach.Corlett and Penn (2010) presented\nan efﬁcient A* search algorithm to solve letter\nsubstitution ciphers. Nuhn et al. (2013) produce\nbetter results in faster time compared to ILP and\nEM-based decipherment methods by employing a\nhigher order language model and an iterative beam\nsearch algorithm. Nuhn et al. (2014) present var-\nious improvements to the beam search algorithm\nin Nuhn et al. (2013) including improved rest cost\nestimation and an optimized strategy for order-\ning decipherment of the cipher symbols. Hauer\net al. (2014) propose a novel approach for solv-\ning mono-alphabetic substitution ciphers which\ncombines character-level and word-level language\nmodel. They formulate decipherment as a tree\nsearch problem, and use Monte Carlo Tree Search\n(MCTS) as an alternative to beam search. Their\napproach is the best for short ciphers.\nGreydanus (2017) frames the decryption pro-\ncess as a sequence-to-sequence translation task\nand uses a deep LSTM-based model to learn the\ndecryption algorithms for three polyalphabetic ci-\nphers including the Enigma cipher. However,\nthis approach needs supervision compared to our\napproach which uses a pre-trained neural LM.\nGomez et al. (2018) (CipherGAN) use a genera-\ntive adversarial network to learn the mapping be-\ntween the learned letter embedding distributions\nin the ciphertext and plaintext. They apply this\napproach to shift ciphers (including Vigen `ere ci-\nphers). Their approach cannot be extended to ho-\nmophonic ciphers and full message neural LMs as\nin our work.\n6 Conclusion\nThis paper presents, to our knowledge, the ﬁrst\napplication of large pre-trained neural LMs to\nthe decipherment problem. We modify the beam\nsearch algorithm for decipherment from Nuhn et\nal. (2013; 2014) and extend it to use global scor-\ning of the plaintext message using neural LMs.\nTo enable full plaintext scoring we use the neural\nLM to sample plaintext characters which reduces\nthe beam size required. For challenging ciphers\nsuch as Beale Pt 2 we obtain lower error rates with\nsmaller beam sizes when compared to the state of\nthe art in decipherment for such ciphers.\nAcknowledgments\nWe would like to thank the anonymous reviewers\nfor their helpful remarks. The research was also\npartially supported by the Natural Sciences and\nEngineering Research Council of Canada grants\nNSERC RGPIN-2018-06437 and RGPAS-2018-\n522574 and a Department of National Defence\n(DND) and NSERC grant DGDND-2018-00025\nto the third author.\nReferences\nTaylor Berg-Kirkpatrick and Dan Klein. 2013. Deci-\npherment with a million random restarts. In Pro-\nceedings of the 2013 Conference on Empirical Meth-\nods in Natural Language Processing, pages 874–\n878.\nEric Corlett and Gerald Penn. 2010. An exact A*\nmethod for deciphering letter-substitution ciphers.\nIn Proceedings of the 48th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n1040–1047. Association for Computational Linguis-\ntics.\nAidan N. Gomez, Scng Huang, Ivan Zhang, Bryan M.\nLi, Muhammad Osama, and ukasz Kaiser. 2018.\nUnsupervised cipher cracking using discrete gans.\narXiv preprint arXiv:1801.04883.\nSam Greydanus. 2017. Learning the enigma\nwith recurrent neural networks. arXiv preprint\narXiv:1708.07576.\nGeorge W Hart. 1994. To decode short cryptograms.\nCommunications of the ACM, 37(9):102–108.\nBradley Hauer, Ryan Hayward, and Grzegorz Kondrak.\n2014. Solving substitution ciphers with combined\nlanguage models. In Proceedings of COLING 2014,\nthe 25th International Conference on Computational\nLinguistics: Technical Papers, pages 2314–2325.\nThomas Jakobsen. 1995. A fast method for cryptanaly-\nsis of substitution ciphers. Cryptologia, 19(3):265–\n274.\nKevin Knight, Anish Nair, Nishit Rathod, and Kenji\nYamada. 2006. Unsupervised analysis for deci-\npherment problems. In Proceedings of the COL-\nING/ACL on Main conference poster sessions, pages\n499–506. Association for Computational Linguis-\ntics.\nTom´aˇs Mikolov, Martin Karaﬁ ´at, Luk ´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In\nEleventh Annual Conference of the International\nSpeech Communication Association.\n874\nMalte Nuhn, Julian Schamper, and Hermann Ney.\n2013. Beam search for solving substitution ciphers.\nIn Proceedings of the 51st Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), volume 1, pages 1568–1576.\nMalte Nuhn, Julian Schamper, and Hermann Ney.\n2014. Improved decipherment of homophonic ci-\nphers. In Proceedings of the 2014 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 1764–1768.\nEdwin Olson. 2007. Robust dictionary attack of\nshort simple substitution ciphers. Cryptologia,\n31(4):332–342.\nAlec Radford, Rafal Jozefowicz, and Ilya Sutskever.\n2017. Learning to generate reviews and discovering\nsentiment. arXiv preprint arXiv:1704.01444.\nSujith Ravi and Kevin Knight. 2008. Attacking de-\ncipherment problems optimally with low-order n-\ngram models. In proceedings of the conference on\nEmpirical Methods in Natural Language Process-\ning, pages 812–819. Association for Computational\nLinguistics.\nSujith Ravi and Kevin Knight. 2011. Bayesian infer-\nence for zodiac and other homophonic ciphers. In\nProceedings of the 49th Annual Meeting of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies-Volume 1, pages 239–247. As-\nsociation for Computational Linguistics.\nDavid Yarowsky and Richard Wicentowski. 2000.\nMinimally supervised morphological analysis by\nmultimodal alignment. In Proceedings of the 38th\nAnnual Meeting on Association for Computational\nLinguistics, pages 207–216. Association for Com-\nputational Linguistics."
}