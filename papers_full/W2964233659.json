{
    "title": "BEHRT: Transformer for Electronic Health Records",
    "url": "https://openalex.org/W2964233659",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A3127737310",
            "name": "Li, Yikuan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222734711",
            "name": "Rao, Shishir",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4290330046",
            "name": "Solares, Jose Roberto Ayala",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222734716",
            "name": "Hassaine, Abdelaali",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2937092820",
            "name": "Canoy Dexter.",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1908425006",
            "name": "Zhu Yajie",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2659540395",
            "name": "Rahimi, Kazem",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222734713",
            "name": "Salimi-Khorshidi, Gholamreza",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2187089797",
        "https://openalex.org/W2901424910",
        "https://openalex.org/W2481271618",
        "https://openalex.org/W2905810301",
        "https://openalex.org/W2963271116",
        "https://openalex.org/W2346093960",
        "https://openalex.org/W2158698691",
        "https://openalex.org/W2119852447",
        "https://openalex.org/W2131241448",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2461746922",
        "https://openalex.org/W2944890781",
        "https://openalex.org/W2625625371",
        "https://openalex.org/W2752747624",
        "https://openalex.org/W2404901863",
        "https://openalex.org/W2265755165",
        "https://openalex.org/W2082704080",
        "https://openalex.org/W2950635152",
        "https://openalex.org/W2960193895",
        "https://openalex.org/W2027106132",
        "https://openalex.org/W46659105",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2950035161",
        "https://openalex.org/W2985962305",
        "https://openalex.org/W1486250383",
        "https://openalex.org/W2946185430",
        "https://openalex.org/W1990652932",
        "https://openalex.org/W2040492681"
    ],
    "abstract": "Today, despite decades of developments in medicine and the growing interest in precision healthcare, vast majority of diagnoses happen once patients begin to show noticeable signs of illness. Early indication and detection of diseases, however, can provide patients and carers with the chance of early intervention, better disease management, and efficient allocation of healthcare resources. The latest developments in machine learning (more specifically, deep learning) provides a great opportunity to address this unmet need. In this study, we introduce BEHRT: A deep neural sequence transduction model for EHR (electronic health records), capable of multitask prediction and disease trajectory mapping. When trained and evaluated on the data from nearly 1.6 million individuals, BEHRT shows a striking absolute improvement of 8.0-10.8%, in terms of Average Precision Score, compared to the existing state-of-the-art deep EHR models (in terms of average precision, when predicting for the onset of 301 conditions). In addition to its superior prediction power, BEHRT provides a personalised view of disease trajectories through its attention mechanism; its flexible architecture enables it to incorporate multiple heterogeneous concepts (e.g., diagnosis, medication, measurements, and more) to improve the accuracy of its predictions; and its (pre-)training results in disease and patient representations that can help us get a step closer to interpretable predictions.",
    "full_text": "BEHRT: T RANSFORMER FOR ELECTRONIC HEALTH RECORDS\nA PREPRINT\nYikuan Li\nDeep Medicine\nOxford Martin School\nUniversity of Oxford\nOxford, UK\nyikuan.li@georgeinstitute.ox.ac.uk\nShishir Rao∗\nDeep Medicine\nOxford Martin School\nUniversity of Oxford\nOxford, UK\nshishir.rao@georgeinstitute.ox.ac.uk\nJose Roberto Ayala Solares\nDeep Medicine\nOxford Martin School\nUniversity of Oxford\nAbdelaali Hassaine\nDeep Medicine\nOxford Martin School\nUniversity of Oxford\nDexter Canoy\nDeep Medicine\nOxford Martin School\nUniversity of Oxford\nYajie Zhu\nDeep Medicine\nOxford Martin School\nUniversity of Oxford\nKazem Rahimi\nDeep Medicine\nOxford Martin School\nUniversity of Oxford\nGholamreza Salimi-Khorshidi\nDeep Medicine\nOxford Martin School\nUniversity of Oxford\nJuly 24, 2019\nABSTRACT\nToday, despite decades of developments in medicine and the growing interest in precision healthcare,\nvast majority of diagnoses happen once patients begin to show noticeable signs of illness. Early\nindication and detection of diseases, however, can provide patients and carers with the chance of early\nintervention, better disease management, and efﬁcient allocation of healthcare resources. The latest\ndevelopments in machine learning (more speciﬁcally, deep learning) provides a great opportunity to\naddress this unmet need. In this study, we introduce BEHRT: A deep neural sequence transduction\nmodel for EHR (electronic health records), capable of multitask prediction and disease trajectory\nmapping. When trained and evaluated on the data from nearly 1.6 million individuals, BEHRT\nshows a striking absolute improvement of 8.0-10.8%, in terms of Average Precision Score, compared\nto the existing state-of-the-art deep EHR models (in terms of average precision, when predicting\nfor the onset of 301 conditions). In addition to its superior prediction power, BEHRT provides a\npersonalised view of disease trajectories through its attention mechanism; its ﬂexible architecture\nenables it to incorporate multiple heterogeneous concepts (e.g., diagnosis, medication, measurements,\nand more) to improve the accuracy of its predictions; and its (pre-)training results in disease and\npatient representations that can help us get a step closer to interpretable predictions.\n1 Introduction\nThe ﬁeld of precision healthcare aims to improve the provision of care through precise and personalised prediction,\nprevention, and intervention.\nIn recent years, advances in deep learning (DL) - a subﬁeld of machine learning (ML) - has led to great progress towards\npersonalised predictions in cardiovascular medicine, radiology, neurology, dermatology, ophthalmology, and pathology,\njust to name a few. For instance, [1] introduced a DL model that can predict the risk of lung cancer from a patient’s\ntomography images with a striking 94.4% accuracy; [ 2] showed that DL can predict a range of cardiovascular risk\n∗Shishir Rao and Yikuan Li have equally contributed to this work as ﬁrst authors. Shishir Rao is Corresponding Author for this\nwork.\narXiv:1907.09538v1  [cs.LG]  22 Jul 2019\nA PREPRINT - JULY 24, 2019\nfactors from just a retinal fundus photograph; and the list continues (more examples can be found in [3] and [4]). A key\ncontributing factor to this success, in addition to the developments in DL algorithms, was the massive inﬂux of large\nmultimodal biomedical data, including but not limited to, mega cohorts such as UK Biobank [5], and routinely-collected\nhealth data such as electronic health records (EHR) [6].\nIn the recent years, the adoption of EHR systems has greatly increased; percent of hospitals in the US and UK that\nhave adopted EHR systems now exceeds 84% and 94%, respectively [7, 8]. As a result, EHR systems of a national\n(and/or a large) medical organisation now are likely to capture data from millions of individuals over many years (or\ndecades, sometimes). Each individual’s EHR can link data from many sources (e.g., doctor visits and hospital episodes)\nand hence contain “concepts” such as diagnoses, interventions, lab tests, clinical narratives, and more. Each instance\nof a concept can mean a single or multiple data points; just a single hospitalisation alone, for instance, can generate\nthousands of data points for an individual, whereas a diagnosis can be a single data point (i.e., an ICD code). This\nmakes large-scale EHR a uniquely rich source of insight and an unrivalled data for training data-hungry ML models.\nIn traditional research on EHR (including the ones using ML), individuals are represented to models as a vector of\nattributes, or \"features\" [ 9]. This approach relies on experts’ ability to deﬁne the appropriate features, and design the\nmodel’s structure (i.e., answering questions such as “what are the key features for this prediction?” or, “which features\nshould have interactions with one another?”). Recent developments in deep learning, however, provided us with models\nthat can learn useful representations (e.g., of individuals, concepts, or an entire record) from raw or minimally-processed\ndata, with minimal need for expert guidance. This happens through a sequence of layers, each employing a large\nnumber of simple linear and nonlinear operations to map their corresponding inputs to a representation; the progress\nfrom layer to layer, is expected to result in a ﬁnal representation in which the data points form distinguishable patterns.\nAs one of the earliest works on applying deep learning to EHR, Liang et al [ 10] showed that deep neural networks\ncan outperform SVM and decision tree paired with manual feature engineering, over a number prediction tasks on\na number of different datasets. In another early work in this space, Tran et al [ 11] proposed the use of restricted\nBoltzmann machines (RBM) for learning a distributed representation of EHR, which was shown to outperform the\nmanual feature extraction, when predicting the risk of suicide from individuals’ EHR. In a similar approach, Miotto\net al [12] employed a stack of denoising autoencoders (SDA) instead of RBM, and showed that it outperforms many\npopular feature extraction and feature transformation approaches (e.g., PCA, ICA and Gaussian mixture models) for\nproviding classiﬁers with useful features to predict the onset of a number of diseases from EHR.\nThese early works on the application of DL to EHR did not take into account the subtleties of EHR data (e.g., the\nirregularity of the inter-visit intervals, and the temporal order or events, to name a few). In attempt to address this,\nNguyen et al [13] introduced a convolutional neural network (CNN) model called Deepr (Deep record) for predicting the\nprobability of readmission; they treated one’s medical history as a sequence of concepts (e.g., diagnosis and medication)\nand inserted a special word between each pair of consecutive visits to denote the time difference between them. In\nanother similar attempt, Choi et al. [14] introduced a shallow recurrent neural network (RNN) model to predict the\ndiagnoses and medications that are likely to occur in the subsequent visit. Both these works employed some form of\nembedding to map the non-numeric medical concepts to an algebraic space in which the sequence models can operate.\nOne of the improvements that was next introduced to the DL models of EHR aimed to enable them to capture the\nlong-term dependencies among events (e.g., key diagnoses such as diabetes can stay a risk factor over a person’s\nlife, even decades after their ﬁrst occurrence; certain surgeries may prohibit certain future interventions). Pham et\nal [15] introduced a Long Short-Term Memory (LSTM) architecture with attention mechanism, called DeepCare, which\noutperformed standard ML techniques, LSTM, and plain RNN in tasks such as prediction of the onset of diabetes. In\na similar development, Choi et al [16] proposed a model based on reverse-time attention mechanism to consolidate\npast inﬂuential visits using an end-to-end RNN model named RETAIN for the prediction of heart failure. RETAIN\noutperformed most of the models at the time of its publication and provided a decent baseline for the medical deep\nlearning research [17].\nIn this study, given the success of deep sequence models and attention mechanisms in the past DL research for EHR,\nwe aim to build on some of the latest developments in deep learning and natural language processing (NLP) – more\nspeciﬁcally, Transformer architecture [18] – while taking into account various EHR-speciﬁc challenges, and provide\nimproved accuracy for the prediction of future diagnoses. We named our model BEHRT (i.e., BERT for EHR), due to\narchitectural similarities that it has with (and our original inspirations that came from) BERT [ 18]; one of the most\npowerful Transformer-based architectures in NLP.\n2\nA PREPRINT - JULY 24, 2019\n2 Materials and Methods\nIn this section, after providing an introduction to our EHR data, we will introduce the earlier works that inspired BEHRT,\nas well as the novel features that this new architecture contributes to the ﬁeld.\n2.1 Data\nIn this study, we used CPRD (Clinical Practice Research Datalink) [17, 19]; it contains longitudinal primary care data\nfrom a network of 674 general physician (GP) practices in the UK, which is linked to secondary care (i.e., hospital\nepisode statistics, or HES) and other health and administrative databases (e.g., ofﬁce for national statistics’ death\nregistration). Around 1 in 10 GP practices (and nearly 7% of the population) in the UK contribute data to CPRD; it\ncovers 35 million patients, among whom nearly 10 million are currently registered patients [ 19]. CPRD is broadly\nrepresentative of the population by age, sex, and ethnicity. It has been extensively validated and is considered as the most\ncomprehensive longitudinal primary care database [20], with several large-scale epidemiological reports [19, 21, 22]\nadding to its credibility.\nHES, on the other hand, contains data on hospitalisations, outpatient visits, accident and emergency for all admissions\nto National Health Service (NHS) hospitals in England [23]. Approximately 75% of the CPRD GP practices in England\n(58% of all UK CPRD GP practices) participate in patient-level record linkage with HES, which is performed by the\nHealth and Social Care Information Centre [ 24]. In this study, we only considered the data from GP practices that\nconsented to (and hence have) record linkage with HES. The importance of primary care at the centre of the national\nhealth system in the UK, the additional linkages, and all the aforementioned properties, make CPRD one of the most\nsuitable EHR datasets in the world for data-driven clinical/medical discovery and machine learning.\n2.2 Preprocessing of CPRD\nWe start our preprocessing with 8 million patients; we only included patients that are eligible for linkage to HES and\nmeet CPRD’s quality standard (i.e., using the ﬂags and time windows that CPRD provides to indicate the quality of\none’s EHR). Furthermore, to only keep the records that have enough history to be useful for prediction, we only kept\nindividuals who have at least 5 visits in their EHR. At the end of this process, we are left with P = 1.6 million patients\nto train and evaluate BEHRT on. More details on the steps we took and the number of patients after each one of them\ncan be seen in Fig 1.\nFigure 1: Linkage and ﬁltering of CPRD data. This ﬂow lists all the key steps of our data cleaning and linkage procedure.\nAt each step, the number of patients that are included is shown.\nIn CPRD, diseases are classiﬁed using Med Code (which can be simply mapped to NHS’s standard Read Code [25])\nand ICD-10 [26] schemes, for primary and hospital care, respectively. In the ICD-10 universe, one can deﬁne diseases\nat the desired level of granularity that is appropriate for the analysis of interest by simply choosing the level of hierarchy\nthey want to operate at; for instance, operating ICD-10 chapter level will lead to 22 diseases, while operation at full\nICD-10 code level will lead to 10,000 diseases. With Med Code, however, such a hierarchy does not exist and hence\none needs to carry out exhaustive disease review, in order to deﬁne the diseases of interest for a given analysis. To\nalleviate this extra data-processing burden, we ﬁrst mapped Med Codes to Read Codes (using the mapping provided by\n3\nA PREPRINT - JULY 24, 2019\nCPRD) and excluded the procedure codes. After that, we mapped both ICD-10 codes (at level 4) and Read Codes to\nCaliber codes [27], which is an expert checked mapping dictionary from University College London. Eventually, this\nresulted in a total of G = 301codes for diagnoses. We denote the list of all these diseases as D = {di}G\ni=1, where di\ndenotes the ith disease code.\nFor each patient p ∈{1, 2, . . . , P}the medical history consists of a sequence of visits to GP and hospitals; each\nvisit can contain concepts such as diagnosis, medications, measurements and more. In this study, however, we are\nonly considering the diagnoses; we denote each patient’s EHR asVp = {v1\np, v2\np, v3\np, . . . ,vnp\np }, where np denotes the\nnumber of visits in patient p’s EHR, andvj\np contains the diagnoses in the jth visit, which can be viewed as a list of\nmj\np diagnoses (i.e., vj\np = {d1, . . . , dmj\np\n}). In order to prepare the data for BEHRT, we order the visits (hence diseases)\ntemporally, and introduce a term to denote the start of medical history (i.e., CLS), and the space between visits (i.e.,\nSEP ), which results in a new sequence, Vp = {CLS, v1\np, SEP,v2\np, SEP, . . . ,vnp\np , SEP}, that from now on will be\nhow we see/denote each patient’s EHR as. This process is illustrated in Figure 2.\nFigure 2: Preparation of CPRD data for BEHRT. An example patient’s EHR sequence can be seen in (a), which consists\nof 8 visits. In each visit, the record can consist of concepts such as diagnoses, medications and measurements; all these\nvalues are artiﬁcial and for illustration purposes. For this study, we are only interested in age and diagnoses. Therefore,\nas shown in at the bottom of the ﬁgure, we have taken only the diagnosis and age subset of the record to form the\nnecessary sequences. This resulting sequence is what we represent every patient’s EHR as to our modelling process.\nNote that, the visits shown in purple boxes are not going to be represented to the model, due to them lacking diagnoses.\n2.3 BEHRT: A Transformer-based Model for EHR\nIn this study, we aim to use a given patient’s past EHR to predict his/her future diagnoses (if any), as a multi-task\nclassiﬁcation problem; this will result in a single predictive model that scales across a range of diseases (as opposed\nto needing to train one predictive model per disease). Modelling of EHR sequences requires dealing with four key\nchallenges [15]: (C.1) complex and nonlinear interactions among past, present and future concepts; (C.2) long-term\ndependencies among concepts (e.g., diseases occurring early in the history of a patient effecting events far later in\nfuture); (C.3) difﬁculties of representing multiple heterogeneous concepts of variable sizes and forms to the model; and\n(C.4) the irregular intervals between consecutive visits.\nSimilarities between sequences in EHR and natural language led to successful use of techniques such as BoW [12],\nSkip-gram [14], RNN [16], and attention [15, 28] (a la their NLP usage) for learning complex EHR representations in\nthe past. In this study, we get our inspiration from the striking success that Transformers [29], and more speciﬁcally, a\n4\nA PREPRINT - JULY 24, 2019\nTransformer-based architecture known as BERT [18]. By depicting diagnoses as words, the content of in each visit as a\nsentence, and a patient’s entire medical history as a document, we facilitate the the use of multi-headed self-attention,\npositional encoding, and masked language models (MLM), for EHR - under a new architecture we call BEHRT.\nWe refer readers to the original papers [18, 29] for an exhaustive background description for both Transformer and\nBERT. Figure 3B illustrates BEHRT’s architecture, which is designed to pre-train deep bidirectional representations of\nmedical concepts by jointly conditioning on both left and right contexts in all layers. The pre-trained representations\ncan be simply employed for a wide range of downstream tasks, e.g., prediction of the next diseases, and disease\nphenomapping. Such bidirectional contextual awareness of BEHRT’s representations is a big advantage when dealing\nwith EHR, where due to variabilities in practice of care and/or simply due to random events, the order at which certain\ndiseases happen can be reversed, or the time interval between two diagnoses can be shorter or longer than actually\nrecorded.\nFigure 3: BEHRT architecture. Using the artiﬁcial data shown in Figure 2, (a) shows how BEHRT sees one’s EHR. In\naddition to diagnosis and age, BEHRT also employs an encoding for event’s positions (shown as POSITION) to help it\nunderstand each event’s place in the sequence, and a visit separator (shown as SEGMENT) to help it understand the\nstart and end of a visit. The combination of all these results is a ﬁnal embedding shown at the bottom of (a), which will\nbe the latent contextual representation of one’s EHR at a given visit. Using this pre-trained embedding, BEHRT’s deep\nTransformer-based model – shown in (b) – learns to predict the diseases in one’s future.\nBEHRT, has many structural advantages over many of the previous methods for modelling EHR data. Firstly, we use\nfeedforward neural networks to model time sensitive EHR data by examining the various forms of sequential order\nwithin the data (e.g. age, order of visits) instead of using traditional state-of-the-art RNN and CNN that were explored\nin the past [13, 16]. Recurrent neural networks are known to be notoriously hard to train, due to their exploding and\nvanishing gradient problems [30]; these issues hamper these networks’ ability to learn (particularly, when dealing with\nlong sequential data). On the other hand, convolutional neural networks only capture limited amount of information\nwith convolutional kernels in the lower layers, and need to expand their receptive ﬁeld though increasing the number of\nlayers in a hierarchical architecture. BEHRT’s feedforward structure alleviates the exploding and vanishing gradient\nproblems and capture information by considering the full sequence at the same time; a more efﬁcient training through\nlearning the data in parallel rather than in sequence (unlike the RNN).\nThe embedding layer in BEHRT, as shown in Figure 3, learns the evolution of one’s EHR through a combination of four\nembeddings: disease, \"position\", age, and \"segment\". This combination enables BEHRT to deﬁne a representation that\ncan capture one’s EHR in as much detail as possible. Disease codes are of course important in informing the model of\nthe future state of one’s health. That is, there many common disease trajectories and multi-morbidity patterns [31] that\nknowing one’s past diseases can improve the accuracy of the prediction. Positional encodings are either trainable or\npre-determined encodings for a position to determine relative position of words in a sequence of words. Pre-determined\nencodings are used in this paper to avoid weak learning of positional embedding caused by imbalanced distribution of\nmedical sequence length. Telling the network of a disease’s position, enables it to capture the positional interactions\namong diseases. Given the feed-forward architecture of our network, positional encodings plays a key role in ﬁlling the\ngap resulting from the lack of a recurrent structure that was the most common/successful approach for learning from\nsequences. For BEHRT, we followed the same position encoding rule proposed by [29].\nAge and segment are two embeddings that did not exist in the original BERT implementation for NLP and is unique to\nBEHRT; an attempt to empower it for dealing with the challenges we mentioned earlier. The segment embedding can be\n5\nA PREPRINT - JULY 24, 2019\neither A or B; the purpose of this is to use two trainable vectors to provide extra information for visit separation. Age,\nof course, is known to be a key risk factor in epidemiology. By embedding age and linking it to each visit/diagnosis, not\nonly we provide the model with a sense of time (i.e., the time between events, as well as a universal notion of time for\nwhen things happened that is comparable across patients).\nThrough a unique combination of the four aforementioned embeddings, we not only provide the model with disease\nsequences, but also give it with a precise sense of timing of events, and data about the delivery of care. In other words,\nthe model has the ability to learn from the diagnosis history, the age at which diagnoses happened and the pattern at\nwhich the patient was visited. All these, when combined, can provide a picture of one’s health that traditionally we\nmight have sought to paint through additional features extracted from EHR data. Of course, we do not advocate for not\nusing the full richness of the EHR, however, the complexity of our architecture when paired with simple subset of EHR\ncan still provide an accurate prediction of one’s future health. Plus, BEHRT’s ﬂexible architecture enables the use of\nadditional concepts, e.g., by simply adding a ﬁfth or sixth (or more) embedding to the existing four.\n2.4 Pre-training BEHRT using MLM\nIn EHR, just like language, it is intuitively reasonable to believe that a deep bidirectional model is more powerful\nthan either a left-to-right model or the shallow concatenation of a left-to-right and a right-to-left model. Therefore, we\npre-trained BEHRT using the same approach as the original BERT paper [18], using MLM. That is, we chose 15% of\nthe disease words at random, and modiﬁed them according to the following probabilities:\n1. 80% of the times replace them word with [MASK]\n2. 10% of the times replace them with a random disease word\n3. 10% of the times do nothing and keep them unchanged.\nUnder this setting, BEHRT does not know which of the disease words are masked, so it stores a contextual representation\nof all of the disease words. Additionally, given the small prevalence of change (i.e., only for 15% of all disease words)\nwill not hamper the model’s ability to understand the EHR language. Lastly, the replacement of the disease words acts\nas injected noise into the model; it will distract the model from learning the true left and right context, and instead\nforces the model to ﬁght through the noise and continue learning the overall disease trajectories. The pre-training\nMLM task was evaluated using precision score [ 32], which calculates the ratio of true positive over the number of\npredicted positive samples (precision calculated at a threshold of 0.5). The average is calculated over all labels and over\nall patients.\n2.5 Disease Prediction\nIn order to provide a comprehensive evaluation of BEHRT, we assess its learning for three predictive tasks: prediction\nof concepts in the next visit (T1), prediction of the occurrence of diseases in the next 6 months (T2), and prediction\nof the occurrence of diseases in the next 12 months (T3). In order to train our model and assess the goodness of its\npredictions across these tasks, we ﬁrst randomly allocated the patients into three groups of train, validation and test\n(each containing 80%, 10% and 10%, or patients, respectively). To deﬁne the training examples (i.e., input-output pairs)\nfor T1, we randomly choose an index j (3 < j < np) for each patient and form xp = {v1\np, . . . ,vj\np}and yp = wj+1,\nas input and output, respectively, where wj+1 is a multi-hot vector of length G, with 1 for diseases that exist in vj+1\np .\nNote that, for each patient, we have only input-output pair.\nFor both T2 and T3, the formation of input and label are slightly modiﬁed. First, patients that do not have 6 or 12\nmonths (for T2 and T3, respectively) worth of EHR (with or without a visit) after v4\np will not be included in these\nanalyses. Second, j is chosen randomly from (3, n∗), where n∗ denotes the highest index after which there is 6 or 12\nmonths (for T2 and T3, respectively) worth of EHR (with or without a visit). Lastly, yp = w6m and yp = w12m are\nmulti-hot vectors of length G, with 1 for concepts/diseases that exist in the next 6 and 12 months, respectively. As a\nresult of this ﬁnal ﬁltering of patients, we had 699K, 391K, and 342K patients for T1, T2, and T3, respectively.\nWe denote the model’s prediction for patientp in the aforementioned tasks as y∗\np, where the ith entry is the model’s\nprediction of that person having di. The evaluation metrics we used to compare ys and y∗s, are area under the ROC\ncurve (AUROC) [33] and average precision score (APS) [ 34]; the latter is a weighted mean of precision and recall\nnumbers achieved at different thresholds. We calculated the APS and AUROC for each patient ﬁrst, and then averaged\nthe resulting APS and AUROC scores across all patients; this average is the key metric, when comparing BEHRT\nwith other state-of-the-art architectures in the ﬁeld. The methods we used here for APS and AUROC are described\nin [33, 34].\n6\nA PREPRINT - JULY 24, 2019\n3 Results\nTo begin with, we used Bayesian Optimisation [35] to ﬁnd the optimal hyperparameters for the MLM pre-training.\nThe main hyperparameters here are the number of layers, the number of attention heads, hidden size, and intermediate\nsize. Intermediate size is the size of the neural network layer titled \"intermediate layer\". This process resulted in an\noptimal architecture with 6 layers, 12 attention heads, intermediate layer size of 512, and hidden size of 288; model’s\nperformance in the MLM task described in Section 2.4 was 0.6597 in precision score. Further details can be found in\nAppendix A.\nIn order for BEHRT’s numerical processes to be applicable to EHR, we ﬁrst need to map the non-numeric concepts such\nas diagnoses to a vector space (i.e., disease embedding). Therefore, we start the results by showing the performance of\nour pre-training process in embedding the diseases, where we mapped each of the G diseases into a 288-dimensional\nvector. Note that, for evaluating an embedding technique – even in NLP where the literature has a longer history and\nhence is more mature – there is not a single gold standard metric [36]. In this study, our assessment is based on two\ntechniques: visual investigation (i.e., in comparison with medical knowledge), and evaluation in a prediction task.\nFor the former, we used t-SNE [37] to reduce the dimensionality of the disease vectors to two – results are shown in\nFigure 4. Based on the resulting patterns in lower dimension, we can see that diseases that are known to co-occur and/or\nbelong to the same clinical groups, are grouped together.\nFigure 4: Disease Embedding Analysis. In this image we see a graph of disease embeddings projected in two dimensions\nwhere distance represents closeness of contextual association. Most associations are accepted by medical experts and\nmaintain the gender-based divisions in illnesses, among other things. We zoom in and proﬁle four clusters in this plot –\nshown in subﬁgures A-D.\n7\nA PREPRINT - JULY 24, 2019\nA reassuring pattern that can be seen in Figure 4 is the natural stratiﬁcation of gender-speciﬁc diseases. For instance,\ndiseases that are unique to women (e.g., Endometriosis, Dysmenorrhea, Menorrhagia, ...) are quite distant from those\nthat are unique to men (e.g., Erectile Dysfunction, Primary Malignancy of Prostate, ...). Such patterns seem to suggest\nthat our disease embedding built an understanding of the context in which diagnoses happen, and hence infer factors\nsuch as gender that it is not explicitly fed.\nFurthermore, the colour in Figure 4 represent the original Caliber disease chapters (see the legends in the main subplot).\nAs can be seen, natural clusters are formed that in most cases consist of disease of the same chapter (i.e., the same\ncolour). Some of these clusters, however, are correlated but not identical to these chapters; for instance, many Eye\nand Adnexa diseases are amongst nervous system diseases and many nervous system disease are also among many\nmusculoskeletal diseases. Overall, this map can be seen as diseases’ correspondence to each other based on 1.6 million\npeople’s EHR. Overall, it seems to be safe to say that this embedding seems to make sense and hence it passes the\nvisual evaluation test.\nAnother interesting property of BEHRT is its self-attention mechanism; it gives our model the ability to ﬁnd the\nrelationships among events that go beyond temporal/sequence adjacency. This self-attention mechanism is able to\nunearth deeper and more complex relationships between a disease in one visit and other surrounding diagnoses. We\nvisualise this self-attention using the approach introduced in [38]; we plot each medical history against itself, and we see\nhow each disease relates to others around it in each patient. The results from a couple of example patients are shown in\nFigure 5. Note that, since BEHRT is bidirectional, the self-attention mechanism captures non-temporal/non-directional\nrelationships among diseases.\nFor patient A for example, the self-attention mechanism has shown strong connections between Rheumatoid Arthritis\nand Enthesopathies and synovial disorders (far in the future of the patient). This is a great example of where attention\ncan go beyond recent events and ﬁnd long-range dependencies among diseases. Note that, as described earlier and\nillustrated in Figure 3, the sequence we model is a combination of four embedding (disease, plus age, segment and\nposition) that go through layers of transformations to form a latent space abstraction. While in Figure 5 we labeled the\ncells with disease names, a more precise labelling will be diseases in their context (e.g., at a given age).\nBEHRT after the MLM pre-training can be considered a universal EHR feature extractor that with small additional\ntraining can be employed for a range of downstream tasks. In this work, the downstream task of choice is the\nmulti-disease prediction problem that we described in Section 2.5. To train a predictor, we feed the output from\nBEHRT to a single feed-forward classiﬁer layer and train it three separate time for each of the three tasks (T1-T3)\ndescribed in Section 2.5. The evaluation of the model’s performance is shown in Table 1, which demonstrates BEHRT’s\nsuperior predictive power compared to two of the most successful approaches in the literature (i.e., RETAIN [16] and\nDeepR [12]). We used Bayesian Optimisation to ﬁnd the optimal hyperparameters for RETAIN and Deepr before\nassessing them in terms of APS and AUROC scores. More details on their hyperparameter search and optimisation can\nbe found in AppendixA.\nTable 1: Next Visit Prediction Task\nModel Name Next Visit (APS | AUROC) Next 6M (APS | AUROC) Next 12M (APS | AUROC)\nBEHRT 0.462 | 0.954 0.525 | 0.958 0.506 | 0.955\nDeepR 0.360 | 0.942 0.393 | 0.943 0.393 | 0.943\nRETAIN 0.382 | 0.921 0.417 | 0.927 0.413 | 0.928\nBesides comparing the APS, which provides an average view across all patients, all diseases and all thresholds, we are\nalso interested in analysing the model’s performance for predicting for each disease. To do so for a given diseasedi, we\nonly considered the ith location in yp and y∗\np vectors and calculated AUROC and APS scores, as well as occurrence\nratio for comparison. The results for T2 (or, next 6-months prediction task) is shown in Figure 6. For visual convenience,\nwe did not include rare diseases with prevalence of less than 1% in our data. The result shows that the model is able to\nmake predictions with relatively high precision and recall for diseases such as Epilepsy (0.016), Primary Malignancy\nProstate (0.011), Polymyalgia Rheumatica (0.013), Hypo or hyperthyroidism (0.047) and Depression (0.0768). A\nnumerical summary of this analysis can be found in be found in Appendix B. Furthermore, a comparison of the general\nAPS/AUROC trends across the three models - BEHRT, RETAIN, and Deepr can be found in Appendix C.\n4 Conclusions and Future Works\nIn this paper, we introduced a novel deep neural model for EHR called BEHRT, which can be pre-trained on a large\ndataset and then with small ﬁne tuning result in a striking performance in a wide range of downstream tasks. We\n8\nA PREPRINT - JULY 24, 2019\nFigure 5: Disease Self-Attention Analysis. This ﬁgure shows the EHR history (shown chronologically, going down-\nwards) of two patients A and B, each presented as two identical columns for the convenience of association analyses. The\nleft side of the column represents the disease of interest and the right column indicates the corresponding associations to\nthe highlighted disease on the left. The intensity of the blue on the right column represents the strength of the attention\nscore – the stronger the intensity, the stronger the association and hence the stronger the attention score. The attention\nscores are speciﬁcally retrieved from the attention component of the last layer of BEHRT network.\ndemonstrated this property of the model by training and testing it on CPRD - one of the largest linked primary care EHR\nsystems. Based on our results, BEHRT outperformed all the best deep EHR models in the literature over a range of\ndiseases (i.e., in a multi-label diagnoses prediction in near future), by ∼8% (absolute improvement) in any given task.\nBEHRT offers a ﬂexible architecture that is capable of capturing more modalities of EHR data. In this paper, we\ndesigned and tested a BEHRT model that relied on 4 key embeddings - diseases, age, segment and position. Through\nthis mix, the model will not only have the ability to learn about the past diseases and their relationships with one another\n(and hence their effect on the next likely disease), but also gain insights about the underlying generating process of EHR;\nwe can refer to this as practice of care. In other words, the model will engineer features (i.e., complex representations)\nthat are capable of capturing concepts such as \"this patient had diseases X and Y at young ages, and suddenly, the\nfrequency of visits increased and new diagnoses appeared, which can lead to high chance of the next disease being Z\".\nIn future works, one can add more to the four concepts we employed and bring medication, tests and interventions to\nthe model with minimum architectural changes - only a vector addition in Figure 3.\nOur primary objective in this study was to provide the ﬁeld with an accurate predictive models for the prediction of\nnext diseases. However, BEHRT provides multiple byproducts that can be useful on their own and/or as the foundation\n9\nA PREPRINT - JULY 24, 2019\nFigure 6: Disease-wise precision analysis. Each circle in these graphs represents a disease, who and color and size are\ntheir caliber chapter and prevalence, respectively. Also, in these plots, we show APS and AUROC on the x- and y-axis,\nrespectively. Therefore, the further right and higher a disease, the better BEHRT’s job at predicting its occurrence in the\nnext 6 months. Subplot A, illustrated the full results, and subplots B and C illustrate the best and worth sections of the\nplot, in terms of BEHRT’s performance.\npreprocessing for the future works. For instance, the disease embeddings resulting from BEHRT can provide a great\ninsight to how various diseases are related to each other - it goes beyond simple disease co-occurrence and rather\nlearns to score closeness of diseases based on their trajectories in a big population of patients. Furthermore, the disease\ncorrespondence that results from BEHRT’s attention mechanism has been shown to be a useful tool for illustrating the\ndisease trajectories for multi-morbid patients; not only it shows how diseases co-occur, but also it shows the inﬂuence\nof certain disease in the past on future diseases of interest. These correspondences are not strictly temporal but rather\ncontextual. As a future work, we aim to provide these attention-visualisation tools to medical researchers to help them\nbetter understand the contextual meaning of a diagnoses in the midst of other diagnoses of patients. Through this tool,\nmedical researchers can even craft medical history timelines based on certain diseases or patterns and in a way, query\nour BEHRT model and visualiser to perhaps uncover novel disease contexts.\nFor the future, we wish to make improvements to our model and use ensembles of BEHRTs and variations for better\npredictive power. Furthermore, we also plan to bring in more medical features such as treatment records and more\ndemographic information (region, ethnicity, etc). Also, as discussed before, since Caliber consolidates speciﬁc codes\ninto 301 disease codes and fails to map many other codes, we wish to look into more stable mappings that preserve the\ncomprehensiveness of CPRD and reduce noise in our dataset. In addition to this, we wish to also embark on a deep\ndive of a single disease, perhaps heart failure or hypertension, to use BEHRT’s diagnostic power for speciﬁc disease\nprediction.\n5 Acknowledgements\nThis research was funded by the Oxford Martin School (OMS) and supported by the National Institute for Health\nResearch (NIHR) Oxford Biomedical Research Centre (BRC). The views expressed are those of the authors and not\nnecessarily those of the OMS, the UK National Health Service (NHS), the NIHR or the Department of Health and\nSocial Care. This work uses data provided by patients and collected by the NHS as part of their care and support and\nwould not have been possible without access to this data. The NIHR recognises and values the role of patient data,\nsecurely accessed and stored, both in underpinning and leading to improvements in research and care. We also thank\nWayne Dorrington for his work in creating ﬁgures for this paper (Figure 2 and Figure 3).\n10\nA PREPRINT - JULY 24, 2019\nReferences\n[1] Diego Ardila, Atilla P Kiraly, Sujeeth Bharadwaj, Bokyung Choi, Joshua J Reicher, Lily Peng, Daniel Tse,\nMozziyar Etemadi, Wenxing Ye, Greg Corrado, and David P Naidich. End-to-end lung cancer screening with\nthree-dimensional deep learning on low-dose chest computed tomography. Nature Medicine, 25(June), 2019.\n[2] Ryan Poplin, Avinash V Varadarajan, Katy Blumer, Yun Liu, Michael V McConnell, Greg S Corrado, Lily Peng,\nand Dale R Webster. Prediction of cardiovascular risk factors from retinal fundus photographs via deep learning.\nNature Biomedical Engineering, 2(3):158–164, 2018.\n[3] Eric J Topol. human and artiﬁcial intelligence. Nature Medicine, 25(January), 2019.\n[4] Andre Esteva, Alexandre Robicquet, Bharath Ramsundar, V olodymyr Kuleshov, Mark DePristo, Katherine Chou,\nClaire Cui, Greg Corrado, Sebastian Thrun, and Jeff Dean. A guide to deep learning in healthcare. Nature\nmedicine, 25(1):24–29, 2019.\n[5] Cathie Sudlow, John Gallacher, Naomi Allen, Valerie Beral, Paul Burton, John Danesh, Paul Downey, Paul Elliott,\nJane Green, Martin Landray, Bette Liu, Paul Matthews, Giok Ong, Jill Pell, Alan Silman, Alan Young, Tim\nSprosen, Tim Peakman, and Rory Collins. UK Biobank: An Open Access Resource for Identifying the Causes of\na Wide Range of Complex Diseases of Middle and Old Age. PLOS Medicine, 12(3):e1001779, 2015.\n[6] Benjamin Shickel, Patrick Tighe, Azra Bihorac, and Parisa Rashidi. Deep EHR: A Survey of Recent Advances\nin Deep Learning Techniques for Electronic Health Record (EHR) Analysis. IEEE Journal of Biomedical and\nHealth Informatics, 22(5):1589–1604, 2018.\n[7] O N C Annual Meeting. Electronic Public Health Reporting. None, 2018. Available at: https://www.healthit.\ngov/sites/default/files/2018-12/ElectronicPublicHealthReporting.pdf.\n[8] Sonal Parasrampuria and Jawanna Henry. Hospitals’ Use of Electronic Health Records Data, 2015-2017. ONC\nData Brief, No. 46, 2019.\n[9] Fatemeh Rahimian, Gholamreza Salimi-Khorshidi, Amir H Payberah, Jenny Tran, Roberto Ayala Solares,\nFrancesca Raimondi, Milad Nazarzadeh, Dexter Canoy, and Kazem Rahimi. Predicting the risk of emergency\nadmission with machine learning: Development and validation using linked electronic health records. PLoS\nMedicine, 15(11):1–18, 2018.\n[10] Znaonui Liang, Gang Zhang, Jimmy Xiangji Huang, and Qmming Vivian Hu. Deep learning for healthcare decision\nmaking with EMRs. Proceedings - 2014 IEEE International Conference on Bioinformatics and Biomedicine,\nIEEE BIBM 2014, pages 556–559, 2014.\n[11] Truyen Tran, Tu Dinh Nguyen, Dinh Phung, and Svetha Venkatesh. Learning vector representation of medical\nobjects via EMR-driven nonnegative restricted Boltzmann machines (eNRBM).Journal of Biomedical Informatics,\n2015.\n[12] Riccardo Miotto, Li Li, Brian A Kidd, and Joel T Dudley. Deep Patient: An Unsupervised Representation to\nPredict the Future of Patients from the Electronic Health Records. Scientiﬁc Reports, 6(May):1–10, 2016.\n[13] Phuoc Nguyen, Truyen Tran, Nilmini Wickramasinghe, and Svetha Venkatesh. Deepr: A Convolutional Net for\nMedical Records. IEEE Journal of Biomedical and Health Informatics, 21(1):22–30, may 2017.\n[14] Edward Choi, Mohammad Taha Bahadori, Andy Schuetz, Walter F Stewart, and Jimeng Sun. Doctor AI: Predicting\nClinical Events via Recurrent Neural Networks. JMLR workshop and conference proceedings, 56:301–318, 2016.\n[15] Trang Pham, Truyen Tran, Dinh Phung, and Svetha Venkatesh. DeepCare: A deep dynamic memory model\nfor predictive medicine. Lecture Notes in Computer Science (including subseries Lecture Notes in Artiﬁcial\nIntelligence and Lecture Notes in Bioinformatics), 9652 LNAI(i):30–41, 2016.\n[16] Edward Choi, Mohammad Taha Bahadori, Joshua A. Kulas, Andy Schuetz, Walter F. Stewart, and Jimeng Sun.\nRETAIN: An Interpretable Predictive Model for Healthcare using Reverse Time Attention Mechanism. arxiv,\n2016.\n[17] Jose Roberto Ayala Solares, Francesca Elisa Diletta Raimondi, Yajie Zhu, Fatemeh Rahimian, Dexter Canoy,\nJenny Tran, Ana Catarina Pinho Gomes, Amir Payberah, Mariagrazia Zottoli, Milad Nazarzadeh, Nathalie Conrad,\nKazem Rahimi, and Gholamreza Salimi-Khorshidi. Deep Learning for Electronic Health Records: A Comparative\nReview of Multiple Deep Neural Architectures. preprint, page 57, 2019.\n[18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding. arxiv, 2018.\n[19] Emily Herrett, Arlene M Gallagher, Krishnan Bhaskaran, Harriet Forbes, Rohini Mathur, Tjeerd Van Staa, and\nLiam Smeeth. Data Resource Proﬁle: Clinical Practice Research Datalink (CPRD). International Journal of\nEpidemiology, 44(3):827–836, 2015.\n11\nA PREPRINT - JULY 24, 2019\n[20] T Walley and A Mantgani. The uk general practice research database. The Lancet, 350(9084):1097 – 1099, 1997.\n[21] Connor A Emdin, Simon G Anderson, Thomas Callender, Nathalie Conrad, Gholamreza Salimi-Khorshidi, Hamid\nMohseni, Mark Woodward, and Kazem Rahimi. Usual blood pressure, peripheral arterial disease, and vascular\nrisk: Cohort study of 4.2 million adults. BMJ (Online), 2015.\n[22] Connor A. Emdin, Simon G. Anderson, Gholamreza Salimi-Khorshidi, Mark Woodward, Stephen MacMahon,\nTerrence Dwyer, and Kazem Rahimi. Usual blood pressure, atrial ﬁbrillation and vascular risk: Evidence from 4.3\nmillion adults. International Journal of Epidemiology, 2017.\n[23] F. Lee, H. R.S. Patel, and M. Emberton. The ’top 10’ urological procedures: A study of hospital episodes statistics\n1998-99. BJU International, 2002.\n[24] Hamid Mohseni, Amit Kiran, Reza Khorshidi, and Kazem Rahimi. Inﬂuenza vaccination and risk of hospitalization\nin patients with heart failure: A self-controlled case series study. European Heart Journal, 2017.\n[25] NHS. Read Codes. Available at: https://digital.nhs.uk/services/\nterminology-and-classifications/read-codes .\n[26] WHO. ICD-10 online versions. Available at https://icd.who.int/browse10/2016/e.\n[27] Valerie Kuan, Spiros Denaxas, Arturo Gonzalez-izquierdo, Kenan Direk, Osman Bhatti, Shanaz Husain, Shailen\nSutaria, Melanie Hingorani, Dorothea Nitsch, Constantinos A Parisinos, R Thomas Lumbers, Rohini Mathur,\nReecha Sofat, Juan P Casas, Ian C K Wong, and Harry Hemingway. Articles A chronological map of 308 physical\nand mental health conditions from 4 million individuals in the English National Health Service. The Lancet\nDigital Health, 1(2):e63–e77, 2019.\n[28] Kyunghyun Cho. Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Transla-\ntion. arxiv, 2013.\n[29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and\nIllia Polosukhin. Attention Is All You Need. arXiv:1706.03762 [cs], apr 2017.\n[30] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difﬁculty of training Recurrent Neural Networks.\narxiv, 2012.\n[31] The Academy of Medical Sciences. Multimorbidity: a priority for global health research. The Academy of Medical\nSciences, pages 1–127, 2018.\n[32] David M W Powers. Evaluation : From Precision , Recall and F-Factor to ROC , Informedness , Markedness &\nCorrelation. arxiv, 2007.\n[33] Tom Fawcett. An introduction to ROC analysis. Pattern Recognition Letters, 2006.\n[34] Mu Zhu. Recall, precision and average precision. Department of Statistics and Actuarial Science, . . ., 2004.\n[35] Ryan Snoek, Jasper; Larochelle, Hugo; Adams. Practical Bayesian Optimization of Machine Learning Algorithms.\nNIPS, 2(12):e540, 2017.\n[36] Bin Wang, Student Member, Angela Wang, Fenxiao Chen, Student Member, Yuncheng Wang, and C Jay Kuo.\nEvaluating Word Embedding Models : Methods and Experimental Results. arxiv, pages 1–13, 2019.\n[37] Laurens Van Der Maaten and Geoffrey Hinton. Visualizing Data using t-SNE. JMLR, 9:2579–2605, 2008.\n[38] Jesse Vig. Visualizing Attention in Transformer-Based Language Representation Models. arxiv, pages 2–7, 2019.\n12\nA PREPRINT - JULY 24, 2019\nA Hyperparameter Tuning\nWe show the hyperparameter tuning results here in the following section. In Table 2, we show the results of the MLM\ntraining hyperparameter tuning process. We performed Bayesian Optimization to retrieve optimal parameters for the\nmodel.\nIn the following sections, we also perform hyperparameter searches for the Deepr (Table 3) and RETAIN (Table 4)\nmodels to ensure proper comparison of model performance between BEHRT and the aforementioned models.\nTable 2: MLM Hyperparameter Tuning\nIteration Hidden Size Layers Attention Heads Intermediate Size Precision\n1 216 3 6 256 0.6191\n2 288 9 12 512 0.6399\n3 216 3 12 512 0.6175\n4 432 3 18 512 0.6397\n5 288 6 6 784 0.6380\n6 216 6 18 512 0.6262\n7 288 3 18 512 0.6292\n8 432 3 6 784 0.6426\n9 288 6 12 512 0.6356\n10 288 3 12 256 0.6283\n11 432 9 18 512 0.6466\n12 576 9 6 1024 0.6538\n13 432 3 18 1024 0.6411\n14 432 9 6 1024 0.6508\n15 576 6 6 256 0.6503\n16 576 6 12 256 0.6510\n17 360 9 18 512 0.6404\n18 576 9 6 512 0.6513\n19 288 6 6 512 0.6363\n20 288 3 6 512 0.6297\n21 288 6 12 512 0.6597\n22 576 3 12 512 0.6487\n23 360 6 6 784 0.6412\n24 432 9 6 512 0.6497\n25 360 6 12 512 0.6423\n13\nA PREPRINT - JULY 24, 2019\nTable 3: Deepr Best Model - Subsequent Visit Prediction Task\nIteration Filters Kernel\nSize\nFC I FC II FC III Dropout\nI\nDropout\nII\nDropout\nIII\nLearning\nRate\nAverage Precision\n1 37 7 10 46 28 0.4139 0.4997 0.3718 0.0004 0.2599\n2 24 5 28 19 13 0.4936 0.1722 0.4643 0.0062 0.2319\n3 17 7 16 48 40 0.2250 0.3945 0.3264 0.0310 0.1815\n4 33 7 6 10 25 0.1602 0.4476 0.3544 0.0026 0.2640\n5 32 7 4 30 32 0.3714 0.3382 0.3573 0.0353 0.2005\n6 13 4 50 17 47 0.3424 0.1830 0.3056 0.0008 0.3274\n7 12 3 3 6 43 0.4201 0.2387 0.3884 0.0123 0.2112\n8 30 7 16 26 41 0.2055 0.3343 0.1541 0.0011 0.3256\n9 35 5 50 24 25 0.1567 0.4056 0.1329 0.0004 0.3433\n10 41 7 9 35 19 0.4885 0.3548 0.3080 0.0004 0.2343\n11 4 4 33 12 39 0.1811 0.1251 0.1066 0.0010 0.3051\n12 48 4 36 45 37 0.2143 0.3486 0.1222 0.0015 0.3504\n13 36 3 39 10 48 0.1992 0.4164 0.1183 0.0050 0.3291\n14 45 4 44 40 26 0.2329 0.2900 0.1280 0.0903 0.0042\n15 40 4 35 48 11 0.1600 0.4704 0.1211 0.0019 0.3125\n16 49 3 47 41 40 0.1002 0.2541 0.1284 0.0019 0.3588\n17 47 3 50 47 12 0.2519 0.2125 0.1668 0.0005 0.2988\n18 47 3 37 35 50 0.1059 0.4225 0.1120 0.0034 0.3487\n19 47 4 48 34 39 0.2177 0.3607 0.1301 0.0016 0.3567\n20 46 3 46 45 43 0.2007 0.1609 0.1196 0.0044 0.3356\nTable 4: RETAIN Best Model - Subsequent Visit Prediction Task\nIteration Embedding Size Recurrent Size Dropout Em-\nbedding\nDropout Con-\ntext\nL2 Average Precision\n1 142 90 0.3846 0.0224 0.0891 0.1822\n2 124 43 0.3922 0.0382 0.0003 0.1815\n3 173 90 0.3929 0.2238 0.0014 0.3479\n4 145 91 0.4117 0.0404 0.0102 0.2049\n5 153 92 0.4569 0.0642 0.0116 0.2049\n6 120 37 0.4335 0.4017 0.0728 0.1815\n7 180 102 0.3567 0.3307 0.0039 0.2469\n8 195 38 0.3805 0.2711 0.0010 0.3740\n9 165 119 0.4969 0.1964 0.0047 0.2292\n10 174 92 0.3862 0.2078 0.0019 0.3329\n11 145 110 0.3928 0.1926 0.0891 0.1813\n12 195 83 0.4418 0.4787 0.0011 0.3543\n13 187 110 0.3456 0.2083 0.0123 0.2122\n14 144 80 0.3717 0.0932 0.0032 0.3038\n15 193 68 0.4528 0.3950 0.0022 0.3280\n16 198 45 0.4344 0.3324 0.0442 0.1828\n17 145 64 0.4213 0.1950 0.0626 0.1813\n18 171 116 0.4166 0.4950 0.0028 0.3197\n19 186 38 0.4062 0.4916 0.0011 0.3309\n20 136 54 0.3503 0.1678 0.0067 0.2162\n14\nA PREPRINT - JULY 24, 2019\nB Disease-wise Model Performance\nHere we show Disease-wide BEHRT performance in terms of AUROC and APS. We have displayed codes with an\noccurrence ratio of 0.01 at the least. And detailed below is the description of the Caliber code and the chapter along\nwith the APS/AUROC.\nTable 5: Diseases Prediction Performance\nCaliber APS AUROC Description Ratio Caliber Chapter\n92 0.066765 0.723828 Gastritis and duodenitis 0.011198 Diseases of the digestive system\n66 0.108185 0.797633 Diaphragmatic hernia 0.011490 Diseases of the digestive system\n100 0.118093 0.742646 Hearing loss 0.021964 Diseases of the ear and mastoid\nprocess\n273 0.132567 0.773249 Spondylosis 0.013459 Diseases of the musculoskeletal\nsystem and connective tissue\n189 0.142594 0.844596 Pleural effusion 0.010229 Diseases of the respiratory sys-\ntem\n172 0.162186 0.798654 Other anaemias 0.023303 Diseases of the blood and blood-\nforming organs and certain disor-\nders involving the immune mech-\nanism\n29 0.163355 0.822707 Bacterial Diseases (excl TB) 0.023979 Certain infectious and parasitic\ndiseases\n130 0.167457 0.804545 Iron deﬁciency anaemia 0.020780 Diseases of the blood and blood-\nforming organs and certain disor-\nders involving the immune mech-\nanism\n295 0.169674 0.836610 Urinary Tract Infections 0.022534 Diseases of the genitourinary\nsystem\n69 0.170852 0.811759 Diverticular disease of intestine\n(acute and chronic)\n0.015966 Diseases of the digestive system\n8 0.177170 0.810068 Allergic and chronic rhinitis 0.023241 Diseases of the respiratory sys-\ntem\n170 0.181182 0.847938 Osteoporosis 0.013982 Diseases of the musculoskeletal\nsystem and connective tissue\n71 0.184000 0.790655 Dyslipidaemia 0.026010 Endocrine, nutritional and\nmetabolic diseases\n168 0.184629 0.799592 Oesophagitis and oesophageal\nulcer\n0.022426 Diseases of the digestive system\n217 0.203218 0.855593 Primary Malignancy Other Skin\nand subcutaneous tissue\n0.012013 Neoplasms\n93 0.206455 0.776367 Gastro-oesophageal reﬂux dis-\nease\n0.026271 Diseases of the digestive system\n290 0.208524 0.814481 Type 1 Diabetes Mellitus, Type 2\nDiabetes Mellitus, and Diabetes\nMellitus – other or not speciﬁed\n0.021226 Endocrine, nutritional and\nmetabolic diseases\n3 0.219637 0.869366 Actinic keratosis 0.012490 Diseases of the skin and subcu-\ntaneous tissue\n131 0.220017 0.874319 Irritable bowel syndrome 0.011182 Diseases of the digestive system\n294 0.223863 0.824114 Urinary Incontinence 0.020057 Diseases of the genitourinary\nsystem\n169 0.234444 0.785766 Osteoarthritis (excl spine) 0.043714 Diseases of the musculoskeletal\nsystem and connective tissue\n176 0.245906 0.834062 Other or unspeciﬁed infectious\norganisms\n0.030963 Diseases of the respiratory sys-\ntem\n95 0.249208 0.894444 Glaucoma 0.011367 Diseases of the eye and adnexa\n109 0.250573 0.885547 Hyperplasia of prostate 0.020842 Diseases of the genitourinary\nsystem\n15\nA PREPRINT - JULY 24, 2019\nTable 5: Diseases Prediction Performance\nCaliber APS AUROC Description Ratio Caliber Chapter\n184 0.264687 0.879325 Peripheral arterial disease 0.010951 Diseases of the circulatory sys-\ntem\n79 0.265863 0.762939 Enthesopathies & synovial disor-\nders\n0.047036 Diseases of the musculoskeletal\nsystem and connective tissue\n81 0.267187 0.905812 Erectile dysfunction 0.017873 Mental and behavioural disor-\nders\n140 0.268504 0.867094 Lower Respiratory Tract Infec-\ntions\n0.023518 Certain infectious and parasitic\ndiseases\n63 0.271027 0.753816 Dermatitis\n(atopc/contact/other/unspeciﬁed)\n0.049051 Diseases of the skin and subcu-\ntaneous tissue\n142 0.292598 0.893802 Macular degeneration 0.010752 Diseases of the eye and adnexa\n274 0.296798 0.889236 Stable Angina 0.032039 Diseases of the circulatory sys-\ntem\n57 0.301041 0.900088 Coronary heart disease not oth-\nerwise speciﬁed\n0.035177 Diseases of the circulatory sys-\ntem\n275 0.307238 0.911618 Stroke Not otherwise speciﬁed\n(NOS)\n0.023395 Diseases of the nervous system\n45 0.319433 0.863447 Cataract 0.042099 Diseases of the eye and adnexa\n1 0.319972 0.845171 Abdominal Hernia 0.019180 Diseases of the digestive system\n44 0.325143 0.843480 Carpal tunnel syndrome 0.012013 Diseases of the nervous system\n101 0.334902 0.912117 Heart failure 0.024918 Diseases of the circulatory sys-\ntem\n164 0.335131 0.879670 Obesity 0.017442 Endocrine, nutritional and\nmetabolic diseases\n22 0.348523 0.885055 Asthma 0.026133 Diseases of the respiratory sys-\ntem\n97 0.349361 0.882694 Gout 0.018058 Diseases of the musculoskeletal\nsystem and connective tissue\n65 0.350132 0.942604 Diabetic ophthalmic complica-\ntions\n0.018919 Endocrine, nutritional and\nmetabolic diseases\n147 0.368465 0.911541 Migraine 0.012028 Diseases of the nervous system\n229 0.398842 0.904686 Psoriasis 0.011751 Diseases of the musculoskeletal\nsystem and connective tissue\n17 0.410899 0.858498 Anxiety disorders 0.041914 Mental and behavioural disor-\nders\n146 0.433645 0.969406 Menorrhagia and polymenor-\nrhoea\n0.015504 Diseases of the genitourinary\nsystem\n113 0.489456 0.905032 Hypo or hyperthyroidism 0.047897 Endocrine, nutritional and\nmetabolic diseases\n302 0.491672 0.855823 Vitamin B12 deﬁciency anaemia 0.014489 Diseases of the blood and blood-\nforming organs and certain disor-\nders involving the immune mech-\nanism\n51 0.501496 0.923082 Chronic obstructive pulmonary\ndisease (COPD)\n0.036869 Diseases of the respiratory sys-\ntem\n23 0.514881 0.901268 Atrial Fibrillation and ﬂutter 0.077629 Diseases of the circulatory sys-\ntem\n110 0.531597 0.819527 Hypertension 0.200618 Diseases of the circulatory sys-\ntem\n61 0.542223 0.950442 Dementia 0.024656 Mental and behavioural disor-\nders\n62 0.553561 0.877904 Depression 0.076876 Mental and behavioural disor-\nders\n85 0.573880 0.934049 Female genital prolapse 0.015781 Diseases of the genitourinary\nsystem\n220 0.575574 0.964776 Primary Malignancy Prostate 0.011844 Neoplasms\n16\nA PREPRINT - JULY 24, 2019\nTable 5: Diseases Prediction Performance\nCaliber APS AUROC Description Ratio Caliber Chapter\n6 0.583305 0.952656 Alcohol Problems 0.014535 Mental and behavioural disor-\nders\n194 0.647243 0.955062 Polymyalgia Rheumatica 0.013213 Diseases of the musculoskeletal\nsystem and connective tissue\n80 0.648763 0.977907 Epilepsy 0.016104 Diseases of the nervous system\nC Comparison of APS/AUROC across Models\nIn Figure 7, we see the general trend of the three models. BEHRT’s predictions remain in the upper right quadrant of\nthe graph (for the most part) denoting higher APS/AUROC than the other two models.\nFigure 7: Disease-wise precision comparison for BEHRT, Deepr and RETAIN, all models trained on same dataset and\ntrained on the same task (next 6 months), each point represents the precision/AUROC of one disease on corresponding\nmodel.\n17"
}