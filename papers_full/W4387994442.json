{
    "title": "GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics",
    "url": "https://openalex.org/W4387994442",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3160401386",
            "name": "Maxim Zvyagin",
            "affiliations": [
                "Argonne National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A3099963509",
            "name": "Alexander Brace",
            "affiliations": [
                "University of Chicago",
                "Argonne National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A3108772965",
            "name": "Kyle Hippe",
            "affiliations": [
                "Argonne National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2123267846",
            "name": "Yuntian Deng",
            "affiliations": [
                "Harvard University Press",
                "Nvidia (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2031448644",
            "name": "Bin Zhang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2896935445",
            "name": "Cindy Orozco Bohorquez",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3022075908",
            "name": "Austin Clyde",
            "affiliations": [
                "Argonne National Laboratory",
                "University of Chicago"
            ]
        },
        {
            "id": "https://openalex.org/A2636527832",
            "name": "Bharat Kale",
            "affiliations": [
                "Northern Illinois University"
            ]
        },
        {
            "id": "https://openalex.org/A4304612270",
            "name": "Danilo Perez-Rivera",
            "affiliations": [
                "New York University",
                "Argonne National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2047706533",
            "name": "Heng Ma",
            "affiliations": [
                "Argonne National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2229105860",
            "name": "Carla M. Mann",
            "affiliations": [
                "Argonne National Laboratory",
                "University of Chicago"
            ]
        },
        {
            "id": "https://openalex.org/A2661867726",
            "name": "Michael Irvin",
            "affiliations": [
                "Argonne National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A5089683858",
            "name": "Defne G. Ozgulbas",
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ]
        },
        {
            "id": "https://openalex.org/A1985168053",
            "name": "Natalia Vassilieva",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2622715188",
            "name": "James Gregory Pauloski",
            "affiliations": [
                "University of Chicago"
            ]
        },
        {
            "id": "https://openalex.org/A2149441504",
            "name": "Logan Ward",
            "affiliations": [
                "Argonne National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A4265448626",
            "name": "ValÃ©rie Hayotâ€Sasson",
            "affiliations": [
                "University of Chicago",
                "Argonne National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2123505867",
            "name": "Murali Emani",
            "affiliations": [
                "Argonne National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2504263900",
            "name": "Sam Foreman",
            "affiliations": [
                "Argonne National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2120213618",
            "name": "Zhen XIE",
            "affiliations": [
                "Argonne National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A4304612280",
            "name": "Diangen Lin",
            "affiliations": [
                "University of Chicago",
                "Argonne National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2111546118",
            "name": "Maulik Shukla",
            "affiliations": [
                "University of Chicago",
                "Argonne National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2162609287",
            "name": "Weili Nie",
            "affiliations": [
                "Nvidia (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2115843651",
            "name": "Josh Romero",
            "affiliations": [
                "Nvidia (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2761544922",
            "name": "Christian Dallago",
            "affiliations": [
                "Nvidia (United States)",
                "Technical University of Munich"
            ]
        },
        {
            "id": "https://openalex.org/A2108335380",
            "name": "Arash Vahdat",
            "affiliations": [
                "Nvidia (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2778191936",
            "name": "Chaowei Xiao",
            "affiliations": [
                "University of Illinois Urbana-Champaign",
                "Nvidia (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2124245137",
            "name": "Thomas Gibbs",
            "affiliations": [
                "Nvidia (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2017222965",
            "name": "Ian Foster",
            "affiliations": [
                "Argonne National Laboratory",
                "University of Chicago"
            ]
        },
        {
            "id": "https://openalex.org/A2097472593",
            "name": "James J. Davis",
            "affiliations": [
                "Argonne National Laboratory",
                "University of Chicago"
            ]
        },
        {
            "id": "https://openalex.org/A347101016",
            "name": "Michael E. Papka",
            "affiliations": [
                "University of Illinois Chicago",
                "Argonne National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A298405725",
            "name": "Thomas Brettin",
            "affiliations": [
                "Argonne National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2183314411",
            "name": "Rick Stevens",
            "affiliations": [
                "University of Chicago",
                "Argonne National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A3164401859",
            "name": "Anima Anandkumar",
            "affiliations": [
                "Nvidia (United States)",
                "California Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2190549940",
            "name": "Venkatram Vishwanath",
            "affiliations": [
                "Argonne National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2243077571",
            "name": "Arvind Ramanathan",
            "affiliations": [
                "Argonne National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A3160401386",
            "name": "Maxim Zvyagin",
            "affiliations": [
                "Argonne National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A3099963509",
            "name": "Alexander Brace",
            "affiliations": [
                "University of Chicago",
                "Argonne National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A3108772965",
            "name": "Kyle Hippe",
            "affiliations": [
                "Argonne National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2123267846",
            "name": "Yuntian Deng",
            "affiliations": [
                "Nvidia (United States)",
                "Harvard University Press"
            ]
        },
        {
            "id": "https://openalex.org/A2031448644",
            "name": "Bin Zhang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2896935445",
            "name": "Cindy Orozco Bohorquez",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3022075908",
            "name": "Austin Clyde",
            "affiliations": [
                "Argonne National Laboratory",
                "University of Chicago"
            ]
        },
        {
            "id": "https://openalex.org/A2636527832",
            "name": "Bharat Kale",
            "affiliations": [
                "Northern Illinois University"
            ]
        },
        {
            "id": "https://openalex.org/A4304612270",
            "name": "Danilo Perez-Rivera",
            "affiliations": [
                "New York University",
                "Argonne National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2047706533",
            "name": "Heng Ma",
            "affiliations": [
                "Argonne National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2229105860",
            "name": "Carla M. Mann",
            "affiliations": [
                "Argonne National Laboratory",
                "University of Chicago"
            ]
        },
        {
            "id": "https://openalex.org/A2661867726",
            "name": "Michael Irvin",
            "affiliations": [
                "Argonne National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A5089683858",
            "name": "Defne G. Ozgulbas",
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ]
        },
        {
            "id": "https://openalex.org/A1985168053",
            "name": "Natalia Vassilieva",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2622715188",
            "name": "James Gregory Pauloski",
            "affiliations": [
                "University of Chicago"
            ]
        },
        {
            "id": "https://openalex.org/A2149441504",
            "name": "Logan Ward",
            "affiliations": [
                "Argonne National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A4265448626",
            "name": "ValÃ©rie Hayotâ€Sasson",
            "affiliations": [
                "Argonne National Laboratory",
                "University of Chicago"
            ]
        },
        {
            "id": "https://openalex.org/A2123505867",
            "name": "Murali Emani",
            "affiliations": [
                "Argonne National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2504263900",
            "name": "Sam Foreman",
            "affiliations": [
                "Argonne National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2120213618",
            "name": "Zhen XIE",
            "affiliations": [
                "Argonne National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A4304612280",
            "name": "Diangen Lin",
            "affiliations": [
                "University of Chicago",
                "Argonne National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2111546118",
            "name": "Maulik Shukla",
            "affiliations": [
                "Argonne National Laboratory",
                "University of Chicago"
            ]
        },
        {
            "id": "https://openalex.org/A2162609287",
            "name": "Weili Nie",
            "affiliations": [
                "Nvidia (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2115843651",
            "name": "Josh Romero",
            "affiliations": [
                "Nvidia (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2761544922",
            "name": "Christian Dallago",
            "affiliations": [
                "Nvidia (United States)",
                "Technical University of Munich"
            ]
        },
        {
            "id": "https://openalex.org/A2108335380",
            "name": "Arash Vahdat",
            "affiliations": [
                "Nvidia (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2778191936",
            "name": "Chaowei Xiao",
            "affiliations": [
                "Nvidia (United States)",
                "University of Illinois Urbana-Champaign"
            ]
        },
        {
            "id": "https://openalex.org/A2124245137",
            "name": "Thomas Gibbs",
            "affiliations": [
                "Nvidia (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2017222965",
            "name": "Ian Foster",
            "affiliations": [
                "Argonne National Laboratory",
                "University of Chicago"
            ]
        },
        {
            "id": "https://openalex.org/A2097472593",
            "name": "James J. Davis",
            "affiliations": [
                "Argonne National Laboratory",
                "University of Chicago"
            ]
        },
        {
            "id": "https://openalex.org/A347101016",
            "name": "Michael E. Papka",
            "affiliations": [
                "University of Illinois Chicago",
                "Argonne National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A298405725",
            "name": "Thomas Brettin",
            "affiliations": [
                "Argonne National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2183314411",
            "name": "Rick Stevens",
            "affiliations": [
                "University of Chicago",
                "Argonne National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A3164401859",
            "name": "Anima Anandkumar",
            "affiliations": [
                "Nvidia (United States)",
                "California Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2190549940",
            "name": "Venkatram Vishwanath",
            "affiliations": [
                "Argonne National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2243077571",
            "name": "Arvind Ramanathan",
            "affiliations": [
                "Argonne National Laboratory"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3203588026",
        "https://openalex.org/W2943761765",
        "https://openalex.org/W4200465220",
        "https://openalex.org/W2913525628",
        "https://openalex.org/W4319029105",
        "https://openalex.org/W3034425032",
        "https://openalex.org/W3181135909",
        "https://openalex.org/W2289762038",
        "https://openalex.org/W2796780818",
        "https://openalex.org/W2918778696",
        "https://openalex.org/W3177500196",
        "https://openalex.org/W4294495243",
        "https://openalex.org/W3104722250",
        "https://openalex.org/W3025165719",
        "https://openalex.org/W4213019189",
        "https://openalex.org/W3121000782",
        "https://openalex.org/W3011411500",
        "https://openalex.org/W3029682964",
        "https://openalex.org/W3177828909",
        "https://openalex.org/W3001279689",
        "https://openalex.org/W2949867654",
        "https://openalex.org/W4286500588",
        "https://openalex.org/W4225492776",
        "https://openalex.org/W2074231493",
        "https://openalex.org/W3174720288",
        "https://openalex.org/W4388105673",
        "https://openalex.org/W3129831491",
        "https://openalex.org/W3043124965",
        "https://openalex.org/W3082591845",
        "https://openalex.org/W3081168214",
        "https://openalex.org/W3146944767",
        "https://openalex.org/W2997845545",
        "https://openalex.org/W2127338491",
        "https://openalex.org/W2141052558",
        "https://openalex.org/W3200409031",
        "https://openalex.org/W3167386324",
        "https://openalex.org/W3160568774",
        "https://openalex.org/W4220991280",
        "https://openalex.org/W2013035813",
        "https://openalex.org/W4291003299",
        "https://openalex.org/W2085026450",
        "https://openalex.org/W3203718464",
        "https://openalex.org/W1971514365",
        "https://openalex.org/W2460125088",
        "https://openalex.org/W3040899749",
        "https://openalex.org/W4224938976",
        "https://openalex.org/W2940602161",
        "https://openalex.org/W2990138404",
        "https://openalex.org/W2975429091",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3104388511",
        "https://openalex.org/W4221165937",
        "https://openalex.org/W4229005866",
        "https://openalex.org/W3164974885",
        "https://openalex.org/W4283026156",
        "https://openalex.org/W2612690371",
        "https://openalex.org/W4391876619"
    ],
    "abstract": "We seek to transform how new and emergent variants of pandemic-causing viruses, specifically SARS-CoV-2, are identified and classified. By adapting large language models (LLMs) for genomic data, we build genome-scale language models (GenSLMs) which can learn the evolutionary landscape of SARS-CoV-2 genomes. By pre-training on over 110 million prokaryotic gene sequences and fine-tuning a SARS-CoV-2-specific model on 1.5 million genomes, we show that GenSLMs can accurately and rapidly identify variants of concern. Thus, to our knowledge, GenSLMs represents one of the first whole-genome scale foundation models which can generalize to other prediction tasks. We demonstrate scaling of GenSLMs on GPU-based supercomputers and AI-hardware accelerators utilizing 1.63 Zettaflops in training runs with a sustained performance of 121 PFLOPS in mixed precision and peak of 850 PFLOPS. We present initial scientific insights from examining GenSLMs in tracking evolutionary dynamics of SARS-CoV-2, paving the path to realizing this on large biological data.",
    "full_text": "GenSLMs: Genome-scale language models reveal SARS-CoV-2\nevolutionary dynamics\nMaxim Zvyagin1â€ , Alexander Brace1,2â€ , Kyle Hippe1â€ , Yuntian Deng3,4â€ , Bin Zhang5, Cindy Orozco\nBohorquez5, Austin Clyde1,2, Bharat Kale6, Danilo Perez-Rivera1, Heng Ma1, Carla M. Mann1,\nMichael Irvin1, J. Gregory Pauloski2, Logan Ward1, Valerie Hayot2, Murali Emani1, Sam Foreman1,\nZhen Xie1, Diangen Lin2, Maulik Shukla1, Weili Nie3, Josh Romero3, Christian Dallago3,7, Arash\nVahdat3, Chaowei Xiao3, Thomas Gibbs3, Ian Foster1,2, James J. Davis1,2, Michael E. Papka1,8, Thomas\nBrettin1, Rick Stevens1,2, Anima Anandkumar3,9âˆ—, Venkatram Vishwanath1âˆ—, Arvind Ramanathan1âˆ—\n1Argonne National Laboratory, 2University of Chicago, 3NVIDIA Inc., 4Harvard University, 5Cerebras Inc., 6Northern\nIllinois University, 7Technical University of Munich, 8University of Illinois Chicago, 9California Institute of Technology\nâ€ Joint first authors, âˆ—Contact authors: venkat@anl.gov, anima@caltech.edu, ramanathana@anl.gov\nABSTRACT\nOur work seeks to transform how new and emergent variants of\npandemic causing viruses, specially SARS-CoV-2, are identified and\nclassified. By adapting large language models (LLMs) for genomic\ndata, we build genome-scale language models (GenSLMs) which can\nlearn the evolutionary landscape of SARS-CoV-2 genomes. By pre-\ntraining on over 110 million prokaryotic gene sequences, and then\nfinetuning a SARS-CoV-2 specific model on 1.5 million genomes,\nwe show that GenSLM can accurately and rapidly identify variants\nof concern. Thus, to our knowledge, GenSLM represents one of the\nfirst whole genome scale foundation models which can generalize\nto other prediction tasks. We demonstrate the scaling of GenSLMs\non both GPU-based supercomputers and AI-hardware accelerators,\nachieving over 1.54 zettaflops in training runs. We present initial\nscientific insights gleaned from examining GenSLMs in tracking the\nevolutionary dynamics of SARS-CoV-2, noting that its full potential\non large biological data is yet to be realized.\nKEYWORDS\nSARS-CoV-2, COVID-19, HPC, AI, Large language models, whole\ngenome analyses\nACM Reference Format:\nMaxim Zvyagin1â€ , Alexander Brace1,2â€ , Kyle Hippe1â€ , Yuntian Deng3,4â€ , Bin\nZhang5, Cindy Orozco Bohorquez5, Austin Clyde1,2, Bharat Kale6, Danilo\nPerez-Rivera1, Heng Ma 1, Carla M. Mann 1, Michael Irvin 1, J. Gregory\nPauloski2, Logan Ward1, Valerie Hayot2, Murali Emani1, Sam Foreman1,\nZhen Xie1, Diangen Lin2, Maulik Shukla1, Weili Nie3, Josh Romero3, Chris-\ntian Dallago3,7, Arash Vahdat3, Chaowei Xiao3, Thomas Gibbs3, Ian Foster1,2,\nJames J. Davis1,2, Michael E. Papka 1,8, Thomas Brettin1, Rick Stevens1,2,\nAnima Anandkumar3,9âˆ—, Venkatram Vishwanath1âˆ—, Arvind Ramanathan1âˆ—.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nSupercomputing â€™22, November 14â€“19, 2022, Dallas, TX\nÂ© 2020 Association for Computing Machinery.\nACM ISBN ISBN. . . $15.00\nhttps://doi.org/finalDOI\n2020. GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolu-\ntionary dynamics. In Supercomputing â€™22: International Conference for High\nPerformance Computing, Networking, Storage, and Analysis. ACM, New York,\nNY, USA, 13 pages. https://doi.org/finalDOI\n1 JUSTIFICATION\nWe demonstrate achieving >1.54 zettaflops in training one of the\nlargest foundation models on whole genome sequences and apply-\ning it to characterize SARS-CoV-2 variants of concern. Our models\nwill inform timely public health intervention strategies and down-\nstream vaccine development for emerging viral variants.\n2 PERFORMANCE ATTRIBUTES\nPerformance Attribute Our Submission\nCategory of achievement Scalability, Time-to-solution\nType of method used Explicit, Deep Learning\nResults reported on the basis of Whole application including I/O\nPrecision reported Mixed Precision\nSystem scale Measured on full system\nMeasurement mechanism Hardware performance counters,\nApplication timers,\nPerformance Modeling\n3 OVERVIEW OF THE PROBLEM\nTracking of novel and emergent variants for viruses such as se-\nvere acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has\nbeen enabled by rapid sequencing and sharing of whole genome\nsequence data (Otto et al., 2021). As of September 2022, >13 million\nSARS-CoV-2 genomes have been deposited in the GISAID reposi-\ntory1. SARS-CoV-2 represents one of the most deeply sequenced\nviral genomes and is therefore a rich source of information for un-\nderstanding various factors that drive its evolution. Despite its slow\nmutation rate, over the past three years SARS-CoV-2 has evolved\nseveral variant strains containing unique mutation patterns, many\nof which lead to novel viral phenotypes including higher antigenic-\nity, transmissibility, and fitness (Cosar et al., 2022).\nThis has prompted the US Centers for Disease Control and Pre-\nvention (CDC) to identify four SARS-CoV-2 variant categories,\n1https://www.gisaid.org\n.CC-BY-NC-ND 4.0 International licenseavailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprintthis version posted October 11, 2022. ; https://doi.org/10.1101/2022.10.10.511571doi: bioRxiv preprint \nSupercomputing â€™22, November 14â€“19, 2022, Dallas, TX Zvyagin, M. et al.\nincluding: variants being monitored (VBM), variants of interest\n(VOI), variants of concern (VOC), and variants of high consequence\n(VOHC). Classification stems from SARS-CoV-2 growth dynam-\nics and threat to pre-existing immunity (Cosar et al ., 2022, Otto\net al ., 2021). Today, SARS-CoV-2 VOCs include B.1.1.7 (Alpha),\nB.1.617.2 (Delta), and B.1.1.529/BA.1-BA.5 (Omicron). Although\ndeep sequencing of viral genomes across patient populations has\nenabled substantial progress, identifying variants is still tedious and\nresource intensive, including costly laboratory tests and diagnostics.\nTogether, these factors contribute to significant time expenditure\nto recognize and subsequently make informed decisions for public\nhealth intervention strategies (Baker et al., 2021).\nArtificial intelligence (AI) and machine learning (ML) approaches\npromise to transform real-time pandemic monitoring (Syrowatka\net al., 2021). Instead of reacting after the emergence of variants to\nidentify VOCs over potentially several weeks (see Sec. 4.1), AI/ML\ntechniques can leverage deep sequencing data to proactively iden-\ntify mutations in viral proteins and characterize evolutionary pat-\nterns that can assist in predicting and describing future VOCs (Be-\nguir et al., 2022, Hie et al., 2021). However, obtaining high quality,\nglobal-scale genome datasets can be challenging, as diverse se-\nquencing technologies can result in variable quality and coverage\nof sequenced genomes. Sequence-based feature extraction tech-\nniques followed by traditional ML approaches have demonstrated\npromise in early identification of VOCs (Beguir et al., 2022, Maher\net al., 2022, Wallace et al., 2022); however, they remain limited to se-\nquence signatures of regions of interests in the genome. This unmet\nchallenge presents an opportunity for effective whole genome-scale\nsurveillance of global pandemics and early identification of VOCs,\nwith the goal of enabling development of robust public health in-\ntervention strategies prior to surges in case numbers and better\ninforming vaccine-design strategies on emerging variants.\nWe posit that by leveraging recent success of large-language\nmodels (LLMs) in natural language processing (NLP) tasks (Wei\net al., 2022), we can develop global-scale, whole genome surveillance\ntools. In this paper, we use LLMs to characterize SARS-CoV-2 evolu-\ntionary dynamics and reconstruct SARS-CoV-2 variant emergence.\nWe adapt LLMs developed for understanding human languages to\ngenomic sequences, called genome-scale language models (GenSLM),\nand validate this approach in modeling VOC assignments for SARS-\nCoV-2 using historical data. Our contributions include:\nâ€¢We develop some of the largest biological LMs (models with\n2.5 and 25 billion trainable parameters) to date, trained across\na diverse set of >100 million prokaryotic gene sequences.\nThis represents one of the first foundation models trained\non raw nucleotide sequences to demonstrate substantial im-\nprovement in predictive performance in identifying VOCs.\nWe make these models and weights openly available for the\nscientific community2.\nâ€¢We design and validate a novel hierarchical transformer-\nbased model that uses both Generative Pre-trained Trans-\nformers (GPT) (on individual gene sequences) and stable\ndiffusion to capture the correct context and longer-range\ninteractions in genome-scale datasets. This model enables us\n2https://github.com/ramanathanlab/gene_transformer\nto prospectively model SARS-CoV-2 evolution by leveraging\nits generative capabilities.\nâ€¢We showcase training foundation models on both conven-\ntional (GPU-based) systems (Polaris@ALCF and Selene@NVIDIA)\nand on emerging AI-accelerator hardware (interconnected\nCerebras CS-2 systems), and demonstrate high watermarks\nfor time-to-solution (model performance described by its per-\nplexity or accuracy). In addition, we present scaling bench-\nmarks, which demonstrate that training GenSLMs can be\nintensive - achieving nearly 1.5 zettaflops over the course of\ntraining runs.\nTogether, these capabilities go beyond state-of-the-art techniques\nfor global-scale whole genome surveillance of pandemic-causing\nviruses and address a critical infrastructure need for the global\npublic health organizations.\n4 CURRENT STATE OF THE ART\nCurrent approaches for tracking viral evolution rely on infectious\ndisease specialists who examine variations, identify epitopes of\ninterest (i.e., portions of the virus that elicit immune response),\nclassify variants, and eventually flag them for further laboratory\ntesting and analysis (Brouwer et al., 2020, Greaney et al., 2021, Ju\net al., 2020, Zost et al., 2020). This process is widely used for track-\ning viral infections, including seasonal influenza (Doud et al., 2018).\nIdentifying strains of interest helps prioritize downstream vaccine\ndevelopment workflows. However, this process is time-consuming\nand laborious. While data sharing in the community has enabled\nunprecedented progress in developing vaccines for pandemics such\nas COVID-19, there still exists an unmet challenge in accelerat-\ning detection and prediction of viral VOCs via computational and\nexperimental toolkits.\n4.1 Early warning systems for tracking viral\nevolution\nSeveral early warning systems for tracking COVID-19 have been\ndeveloped; however, they utilize case counts, internet search pa-\nrameters, and other allied data focused on monitoring case counts\nin a local geographic area (Ramchandani et al., 2020). The Bacte-\nrial and Viral Bioinformatics Resource Center (BV-BRC)3 provides\nthe SARS-CoV-2 Emerging Variant Tracking and Early Warning\nSystem, which enables users to browse current and past variant\nlineages and track their prevalence by isolation date, geographic\nlocation, and other metadata fields. A heuristic is used to compute\nmonth-over-month growth rates and highlight rapidly growing\nvariants that may cause future infection surges. Mutations from\neach variant are mapped to known epitope sites and regions of the\ngenome known to be involved in antibody escape to enable further\nassessment of mutation impact. Recently, Hie et al. (Hie et al\n., 2021)\nused protein language models (PLMs) and adapted concepts from\nNLP to model escape variants across three different viruses, includ-\ning SARS-CoV-2. In each virus, they identified a certain protein (e.g.,\nSARS-CoV-2 Spike/S protein) and modeled its evolutionary history\nusing transformers to describe differences between ordinary vari-\nants and VOCs. Similarly, Beguir et al. (Beguir et al., 2022) leveraged\n3https://www.bv-brc.org\n.CC-BY-NC-ND 4.0 International licenseavailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprintthis version posted October 11, 2022. ; https://doi.org/10.1101/2022.10.10.511571doi: bioRxiv preprint \nGenSLMs reveal SARS-CoV-2 evolutionary dynamics Supercomputing â€™22, November 14â€“19, 2022, Dallas, TX\na PLM to accurately classify VOCs; using experimental assays, they\nalso validated these VOCs and demonstrated the ability to flag them\nin advance of World Health Organisation designation. However,\nviral evolution is not isolated to one protein, but occurs at genome\nscale. We propose a system that learns to model whole-genome evo-\nlution patterns using LLMs based on observed data, and enables the\ntracking of VOCs based on measures of fitness and immune escape.\n4.2 Large language models (LLMs)\nThe introduction of transformers - beginning with Bidirectional\nEncoder Representations from Transformers (BERT) (Devlin et al.,\n2018) and subsequent LLMs such as generative pre-trained trans-\nformers (GPT) (Radford et al., 2018) - has revolutionized natural\nlanguage understanding. These models have been used to generate\ntext, speech (Gulati et al., 2020), and images (Han et al., 2022). They\nhave also been employed to understand the language of nucleic\nacids (DNA/RNA) and proteins. Protein language models (PLMs),\nusing amino acid alphabets, are the most heavily investigated bio-\nlogical LLMs (Elnaggar et al., 2022, Rives et al., 2021), with demon-\nstrated success in many downstream tasks (e.g., protein function\nprediction (Unsal et al., 2022)) and engineering (Ferruz et al., 2022).\nNucleotide LLMs, using DNA/RNA alphabets, are still understud-\nied (Avsec et al., 2021). Compared to the rich alphabet and short\nlength of information-dense protein sequences that traditional at-\ntention models from NLP can successfully learn, nucleotide LLMs\nrely on much simpler alphabets and extremely long-range signal\n(e.g., across open reading frames or co-evolutionary patterns) and\nrequire significant domain adaptation to yield good results. When\napplied on the scale of entire genomes, GenSLMs also operate on\nmuch larger sequence lengths than are traditionally seen in NLP\napplications - the max sequence length for SARS CoV2 tasks was\n10,240 tokens in comparison to the standard 1,024 or 2,048. Fur-\nther, viral genomes often undergo frameshift mutations leading to\ndifferential translation, introducing ambiguity not present at the\nprotein scale. Our work addresses these challenges by leveraging\na hierarchical LLM: a generalized pre-trained transformer (GPT)\nto capture shorter/local (codon-level) interactions, and a diffusion-\nbased model to capture longer-range interactions to describe the\nbiological complexity of viral evolution (Sec. 5.2).\n4.3 Workflow infrastructure\nScientific applications for HPC are increasingly written as a com-\nposition of many interconnected components. Application compo-\nnents may have different hardware or software requirements, run\ndurations and execution frequencies, or dependencies with other\ncomponents. Workflow systems such as Swift, Parsl, Balsam and\nRADICAL Cybertools support the design of applications as directed\ngraphs of tasks and manage their execution on available resources.\nThere is significant diversity in workflow implementation; e.g.,\nSwift/T expresses workflows in bespoke programming languages\nthat are compiled into an MPI program (Wozniak et al., 2013). Parsl,\nin contrast, is built on Pythonâ€™s native concurrency library and\ndynamically constructs a task graph as a Python program is inter-\npreted (Babuji et al., 2019). Balsam (Salim et al., 2019) and RADICAL\nCyberTools (Balasubramanian et al., 2019) rely on a central task\ndatabase from which the launcher, running on compute resources,\npulls and executes tasks. A centralized database enables state per-\nsistence across runs, and task dependencies can be defined as a\nDAG. Most workflow systems support interfacing with HPC job\nschedulers or cloud providers to acquire resources and transmit\nfiles between remote resources - key features our use case requires.\nDynamic workflows, where new tasks are continually added in\nresponse to new results, are emerging as an extension of work-\nflow managers. Many dynamic workflow systems, such as DeepHy-\nper (Balaprakash et al\n., 2018) and RocketSled (Dunn et al., 2019), are\npurpose-built to solve optimization problems. LibEnsemble (Hud-\nson et al ., 2022) provides a more general interface where users\ndecouple a dynamic ensemble into a â€œgeneratorâ€, which spawns\nnew tasks based on results from a â€œsimulatorâ€. Toolkits such as\nRay (Moritz et al\n., 2018) and Colmena (Ward et al., 2021) provide\nmore flexible approaches where a number of â€œagentsâ€ can coopera-\ntively coordinate tasks. These libraries handle where and how tasks\nare executed and provide useful abstractions so users can focus on\ncomponent/task logic (i.e., what and when).\n5 INNOVATIONS REALIZED\nGiven the limitations of current approaches in identifying VoCs,\nthere is a need to develop an integrated system that can automati-\ncally â€˜learnâ€™ features within the SARS-CoV-2 genome that distin-\nguish VoCs, while also being able to generate new sequences that\ncharacterize emerging variants of the virus. We posit that by lever-\naging existing sequencing data on the virus, we can train LLMs\nthat can model the SARS-CoV-2 evolutionary trajectory. Training\nLLMs on SARS-CoV-2 genome datasets is non-trivial: (1) one needs\nto address the limitations of training LLMs with genomic sequence;\nand (2) one needs to overcome infrastructural challenges to enable\nLLM training on formidable sequence lengths in a reasonable time.\n5.1 Data collection and description\nSARS-CoV-2 genome dataset: The Bacterial and Viral Bioinfor-\nmatics Resource Center (BV-BRC) web resource provides integrated\ndata and analysis tools for bacterial and viral pathogens to sup-\nport infectious disease research. BV-BRC hosts >600,000 bacterial\ngenomes and 8.7 million viral genome sequences, including 6.4 mil-\nlion SARS-CoV-2 genomes. All SARS-Cov-2 genome sequences were\nacquired from NCBIâ€™s GenBank and SRA databases and uniformly\nannotated using VIGOR4 (Wang et al., 2012) to provide accurate\nand consistent annotation of ORFs and mature peptides across all\nSARS-CoV-2 genomes. Automated and manual curation provided\naccurate and uniform metadata across all genomes, including host\nname, geographic location, and collection date.\nTo build GenSLMs for detecting and predicting SARS-CoV-2 vari-\nants of interest, we used >1.5 million high-quality BV-BRC SARS-\nCoV-2 complete genome sequences. We filtered out any genome\nsequences with < 29,000 bp and >1% ambiguous bases. However, we\nnote here that the data collected might not have sufficient diversity\nâ€“ meaning that any model trained on the SARS-CoV-2 dataset may\nend up overfitting to the data, with little opportunity to generalize.\nHence, we took a foundation model based approach that allowed us\nto first build a more general model using a much larger collection\nof diverse genomic data, namely gene-level data from prokaryotes.\n.CC-BY-NC-ND 4.0 International licenseavailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprintthis version posted October 11, 2022. ; https://doi.org/10.1101/2022.10.10.511571doi: bioRxiv preprint \nSupercomputing â€™22, November 14â€“19, 2022, Dallas, TX Zvyagin, M. et al.\nWe also utilized a dataset collected by the Houston Methodist\nHospital System - one of the largest single-institution collections\nof SARS-CoV-2 genome sequences in the United States. We started\nhere with 70,000 SARS-CoV-2 patient samples collected from May\n15, 2020 to January 14, 2022 and sequenced on Illumina instruments.\nTo ensure high quality, we first masked the leading and trailing 100\nnucleotides for each sequence, as well as 56 positions in the spike\nprotein-encoding region that had low depth due to poor primer\nbinding. Sequences with >256 ambiguous characters were discarded,\nleaving 16,545 total sequences. This subset was used for building\nphylogenetic analyses at genome scale (see Sec. 5.3).\nBV-BRC dataset. : To allow for better generalization and avoid\noverfitting of the models to the SARS-CoV-2 data, we used >110 mil-\nlion unique prokaryotic gene sequences from BV-BRC. The BV-BRC\ndatabase provides cross-genera protein families, PGfams, which al-\nlow collection of homologous gene or protein sequences across taxa\nthat perform the same biological function (Davis et al., 2016). We\nqueried BV-BRC to find 10,206 unique PGfams, each with >30,000\nunique members. For each PGfam, we collected high-quality non-\nredundant gene and protein sequences after filtering out any se-\nquences that were more than one standard deviation from the PG-\nfamâ€™s mean length. The Genome-Scale Language Models (GenSLMs)\nmodels trained on this data are referred to as foundation models.\n5.2 Large language models\nLarge-language model (LLM) training required both algorithmic\nand performance-level innovations. For algorithmic innovations,\nwe describe two key limitations of current LLMs. For performance\ninnovations in achieving optimal time-to-solution (training time\nto achieve some accuracy or perplexity), we leverage an intercon-\nnected cluster of four Cerebras CS-2 AI-accelerators and scale to\nlarge GPU-based supercomputers to train our LLMs.\n5.2.1 Genome-scale Language Models (GenSLMs). We introduce\nGenSLMs as a means to go beyond current PLMs to describe evolu-\ntionary dynamics of SARS-CoV-2. Instead of focusing on specific\nproteins, GenSLMs leverage genome-scale data to model individ-\nual mutations at nucleotide scale, thus implicitly accounting for\nprotein-level mutations at codon level. Fig. 1 shows that GenSLMs\ntake nucleotide sequences of SARS-CoV-2 genomes as input and\nbuilds LLMs that learn a semantic embedding of individual codons,\nwhich can then be translated to the 29 individual protein sequences\nthat are encoded by the virus.\nHowever, there are two fundamental challenges when training\nGenSLMs directly from SARS-CoV-2 genome sequences: (1) The\nentire genome consists of\nâˆ¼30,000 nucleotides (which translates\nto âˆ¼10,000 codons/amino-acids). LLM training on long sequences\ncan be challenging because attention mechanisms largely focus\non shorter/local segments of the genome rather than global pat-\nterns. (2) The overall sequence similarity in SARS-CoV-2 genomes\nis quite high (âˆ¼>99%), with only a small (yet significant) number\nof changes that yield distinct phenotypes. Thus, there is a need to\naddress diversity in the sequences such that the trained model can\ngeneralize. In addition, it is necessary to account for frameshifts in\nviral genomes.\nACC  AAC  CAA  CTT   TCG  ATC   TCT   TGT  AGA ... \nâ€¦ L1\nLk\nSemantic embedding\n0.4   0.1   0.003 â€¦ â€¦  0.8 â€¦  0.1  z\nTransformer layers + attention\nInput sequenceX[N] \\{i}\np(Xi| X[N]\\{i}) TCG CGA CGT â€¦ â€¦ ACG â€¦ CTT\nSRR TLâ€¦ â€¦ â€¦GenSLM\nFigure 1: Overview of GenSLM models for predictive mod-\neling of SARS-CoV-2 evolution. The inputs to GenSLM are\nnucleotide sequences, encoded at the codon level (every three\nnucleotide represents a codon; hence the 20 natural amino\nacid language is described by 64 codons). These inputs are\nsuccessively fed into transformer blocks (referred to as layers\n(ğ¿ğ‘–)), which ultimately results in learning a semantic embed-\nding Â®ğ‘§ space from which one may obtain the log-likelihood\nof any given sequence ğ‘(Â®ğ‘‹ |Â®ğ‘‹ğ‘ğ‘– ), where ğ‘ represents over all\nthe sequence lengths and ğ‘– represents a particular position\nin the entire genome.\nTo overcome these challenges, GenSLM implicitly recognizes\nintrinsic hierarchy (based on the central dogma) of individual pro-\ntein production via DNA transcription and mRNA translation. We\ntrained on gene-level data from BV-BRC (see Sec. 5.1) to mimic\nthis process with GenSLMs. Although mapping between codons\nand amino acids is degenerate (multiple codons may encode the\nsame amino acid) (Shin et al., 2015), we posited that with sufficient\ndiversity in the dataset, GenSLMs could exploit intrinsic organi-\nzation of gene-level data to learn biologically-meaningful latent\nrepresentations. The training process follows a procedure similar\nto the one outlined in (Zhang et al., 2022). We refer to the models\ntrained on the BV-BRC dataset as GenSLM foundation models.\nWhile the benefits of pre-training LLMs on natural text are well\nknown (Turc et al., 2019), obtaining the optimal number of trans-\nformer layers and training on such a diverse set of gene data were\nchallenging. We therefore trained GenSLM foundation models on a\nwide set of parameter scales ranging from 25 million to 25 billion,\nwith maximum sequence length of 2,048 tokens on the BV-BRC\ndataset. Additionally, to evaluate performance on downstream tasks,\nwe fine-tuned the foundation model GenSLMs using a maximum\nsequence length of 10,240 tokens for the 25M and 250M model sizes\non the SARS-CoV-2 datasets (see Table 1). We note that for the larger\nmodel sizes (2.5B and 25B), training on the 10,240 length SARS-\nCoV-2 data was infeasible on GPU-clusters due to out-of-memory\nerrors during attention computation.\nThe entire repertoire of results from the GenSLM foundation\nmodels is beyond the scope of this paper. However, as an empirical\ndemonstration of the power of GenSLMs trained on the SARS-\nCoV-2 genomes, the learned latent space projected onto a low-\ndimensional manifold as determined by t-Stochastic Neighborhood\n.CC-BY-NC-ND 4.0 International licenseavailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprintthis version posted October 11, 2022. ; https://doi.org/10.1101/2022.10.10.511571doi: bioRxiv preprint \nGenSLMs reveal SARS-CoV-2 evolutionary dynamics Supercomputing â€™22, November 14â€“19, 2022, Dallas, TX\nA B\nC D\nToken (codon) likelihood\nFigure 2: GenSLMs learned latent space describes biologically\nmeaningful properties for SARS-CoV-2 genomes. (A) The\nembeddings from GenSLMs are visualized with t-stochastic\nneighborhood embedding (t-SNE) and each gene sequence is\nrepresented as a dot in the 2D plot. We paint each sequence\nby its variant ID â€“ although we have more than 515 PANGO\n(Rambaut et al ., 2020) lineages represented in the data, we\nonly show those with WHO designated labels. (B) The latent\nspace can also be painted with the MAFFT-determined align-\nment score (Yamada et al ., 2016) with respect to an Omicron\ngenome; clustering in the distance measures is clearly visible.\nVisualizing the sequence log-likelihood (blue bar) and the\ncross-protein attention (orange lines) from (C) Delta and (D)\nOmicron SARS-CoV-2 strains highlights how different the co-\nevolutionary patterns are in these lineages. It is interesting\nto note that while the Spike protein from Delta strain shows\ncoupling to nsp3, nsp5, and other proteins, these couplings\nare not observed in the Omicron strain.\nEmbedding (t-SNE) meaningfully distinguishes the SARS-CoV-2\nvariants as shown in Fig. 2. This observation is significant because\nthis GenSLM-25M model was specifically trained only on the first\nyear of the SARS-CoV-2 data (consisting ofâˆ¼85,000 SARS-CoV-2\ngenome sequences) â€“ meaning that the model did not have the\nopportunity to see any of the other strains. Thus, the ability of\nGenSLM to generalize and distinguish between SARS-Cov-2 vari-\nants implies that the learning process is robust and the underlying\nfeatures can generalize to downstream tasks. We also note that as\nthe model parameters increase, the perplexity of the model also im-\nproves, agreeing with previous observations (Radford et al., 2018).\nWe note however that these training runs frequently take >1\nweek on dedicated GPU resources (such as Polaris@ALCF). To en-\nable training of the larger models on the full sequence length (10,240\ntokens), we leveraged AI-hardware accelerators such as Cerebras\nCS-2, both in a stand-alone mode and as an inter-connected cluster,\nand obtained GenSLMs that converge in less than a day (Sec. 5.2.4).\n5.2.2 Reward-guided beam search for generative modeling. A sub-\nsequent use of the GenSLM models is in its ability to generate new\nSARS-CoV-2 sequences, with the eventual goal of predicting yet\nunseen VOCs. One challenge with such sequence-based generation\nstrategies is sampling sequences with particular properties. Given\na conditional sequence model ğ‘ğœƒ with weights, ğœƒ, the most likely\nsequence is ğ‘ğœƒ (x)= Ãğ‘‡\nğ‘¡=1 ğ‘ğœƒ (ğ‘¥ğ‘¡ |ğ‘¥0, . . . , ğ‘¥ğ‘¡âˆ’1, ğ‘)where ğ‘ is the con-\ntext from the previous inference. However, computing this directly\nis generally intractable as it is O(64ğ‘‡ ), where ğ‘‡ is the maximum\nsequence length with a vocabulary of size 64. Heuristics like greedy\nsampling are commonly used, where a sequence is generated iter-\natively, with the next token, ğ‘¥ğ‘¡ maximizing ğ‘ğœƒ (ğ‘¥ğ‘¡ |ğ‘¥0, . . . , ğ‘¥ğ‘¡âˆ’1, ğ‘)\nwith complexity O(ğ‘‡ ).\nBeam search is standard practice, combining a search strategy\nwith a heuristic whereğ‘˜ is the number of beams explored with com-\nplexity O(ğ‘˜ğ‘‡ ). First, ğ‘˜ samples are drawn with highest probability\n(or sampled from a multinomial distribution) and added to the set\nof possible hits Xbeam. Let Xğ‘–\nbeams be the set of beams of length ğ‘–.\nThen, for time step ğ‘¡, select ğ‘˜ tokens ğ‘¥ğ‘¡ from the set of all tokens\nwhich score highest (or sampled from multinominal distribution)\nvia ğ‘ğœƒ (ğ‘¥ğ‘¡ |Xğ‘–\nbeams, ğ‘). The highest scoring beams from Xbeam are\nselected via 1\nğ¿ğ›¼\nÃğ¿\nğ‘¡=1 log ğ‘ğœƒ (ğ‘¥ğ‘¡ |ğ‘¥0, . . . , ğ‘¥ğ‘¡âˆ’1, ğ‘)is output where ğ¿ is\nthe length of a sequence and ğ›¼ is a length penalty.\nGiven an episodic reward function ğ‘…(ğ‘¥)= Ãğ¿\nğ‘¡=1 ğ‘Ÿğ‘¡ (ğ‘¥ğ‘¡ ), we mod-\nify the scoring function for beam search with\nğœ‡\nğ¿ğ›¼\nğ¿âˆ‘ï¸\nğ‘¡=1\nlog ğ‘ğœƒ (ğ‘¥ğ‘¡ |ğ‘¥0, . . . , ğ‘¥ğ‘¡âˆ’1, ğ‘)+( 1 âˆ’ğœ‡)ğ‘…(ğ‘¥) (1)\nwhere 0 â‰¤ğœ‡ â‰¤1 is a hyperparameter. Since the reward function is\nepisodic, at each step of beam search, the highest scoring beams\nare chosen with\nğœ‡ğ‘ğœƒ (ğ‘¥ğ‘¡ |Xğ‘–\nbeams, ğ‘)+( 1 âˆ’ğœ‡)ğ‘Ÿğ‘¡ (ğ‘¥ğ‘¡ ). (2)\nThis scoring modification effectively alters the likelihood of tokens\nto be sampled based on maximizing the reward function. In or-\nder to sample sequences which are similar to a fixed sequence\nğ‘¦,\nwe utilize ğ‘Ÿğ‘¡ (ğ‘¥ğ‘¡ )equal to the global alignment score between ğ‘¦ğ‘¡\nand ğ‘¥ğ‘¡ (Needleman and Wunsch, 1970). This scoring bias modifica-\ntion effectively implements a property scoring function into beam\nsearch without altering the complexity of beam search sampling.\nIn the case of non-episodic reward functions, rewards can only be\ncomputed at the final time step in eq. 1.\n5.2.3 Diffusion-based hierarchical modeling. Token-level autore-\ngressive modeling has difficulty in generating coherent long se-\nquences due to its underlying challenge in capturing long-range\ndependencies (Papalampidi et al., 2022, Sun et al., 2021, 2022). We\ndeveloped a new hierarchical-modeling method based on a latent-\nspace diffusion model that operates on the â€˜sentenceâ€™ level. For\neach genome sequence, we uniformly truncated every 512 codons\nby a special separator symbol; these 512 codons are considered a\nâ€˜sentenceâ€™4.\nOur diffusion-based hierarchical modeling method consists of\nthree parts: (1) Learning high-level representations: We trained\na new encoder to embed sentences into a latent space with a con-\ntrastive loss such that learned features better capture high-level\ndynamics. Our contrastive loss is similar to the masked language\n4We set 512 codons to be a sentence such that the average number of sentences per\nsequence (around 20) matches the number of open reading frames and non-coding\nregions (around 17).\n.CC-BY-NC-ND 4.0 International licenseavailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprintthis version posted October 11, 2022. ; https://doi.org/10.1101/2022.10.10.511571doi: bioRxiv preprint \nSupercomputing â€™22, November 14â€“19, 2022, Dallas, TX Zvyagin, M. et al.\n#H #L ğ‘‘model LR B P MSL\nGPU\n8 8 512 5e-05 4096 25M 2048â€ \n16 12 1,840 5e-05 4096 250M 2048â€ \n64 26 3,968 5e-05 512 2.5B 2048\n64 64 8,192 5e-05 - 5e-09 1024 25B 2048\n#H #L ğ‘‘model LR B P MSL\nCS-2\n12 12 768 2.8e-04 33 123M 10240\n12 12 768 2.8e-04 132 123M 10240\n16 24 2048 7.5e-05 11 1.3B 10240\n16 24 2048 1.5e-04 44 1.3B 10240\nTable 1: Description of GenSLMs foundation model archi-\ntectures. #H â€“ number of attention heads; #L â€“ number of\nlayers; ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ â€“ embedding size; LR â€“ learning rate (if range\nis specified, decayed by factor of 10 each update); B â€“ global\nbatch size in number sequences per step; P â€“ total number\nof trainable parameters; MSL â€“ maximum sequence length.\nâ€ For these models we were also able to train on the 10,240\nsequence length for the full genome.\nAGA   TCT   GTT   GTC   TAAACG   AAC   TTT   AAA   â€¦XNi\n r\nGaussian noiseGradual denoising\nz0\nz1\nzT-1\nzT\nz01 z02 z03\np(TAA | r, z) = p(TAA | GTC, GTT, z02)\nDiffusion model\n...\nHigh-level representations\n...\n...\n...\n...\nFigure 3: Illustration of diffusion-based hierarchical mod-\neling. To predict a codon (such as TAA), we use both the\nprevious codons within the context window (we use size 3\nfor illustration) and the high-level representations z.\nmodeling objective used in SpanBERT (Joshi et al ., 2020), where\nwe predict missing sentences in the middle by conditioning on\nthe previous and the next sentences, and, at the same time, using\nrandomly sampled sentences as negatives (distractors).\n(2) Modeling high-level dynamics with a diffusion model:\nGiven encoder output of each genome, i.e., a sequence of sentence\nembeddings, we train a diffusion model to learn their distribution.\nThe diffusion model parameterizes the distribution of high-level\nrepresentations by applying a sequence of denoising operations on\ntop of Gaussian noise. Similar to previous work (Ho et al ., 2020,\nVincent, 2011), we used denoising score matching as the training\nobjective; we gradually apply noise to desired target representations\nand the diffusion model learns to denoise at each step.\nA B\nFigure 4: Diffusion-based hierarchical modeling of SARS-\nCoV-2 genomes results in generation of sequences that cap-\ntures the correct context of various open reading frames\n(ORFs).(A) Comparison of statistics measured on generated\nsequences and on real data for the ORFs. Diffusion-based\nhierarchical LM has a global high-level plan whereas the\nbaseline can only take into account the previous 1023 codons.\n(B) Generated sequences (light blue) from the model overlaid\non the phylogenetic tree demonstrate that these sequences\nare also closer to the various observed strains.\n(3) Fine-tuning LMs with high-level planning: Similar to\nTime Control LM (Wang et al., 2022), we fine-tuned GenSLMs as\nthe decoder to generate the genome sequence conditioned on high-\nlevel representations learned in step (1), which we term the â€˜high-\nlevel planâ€™. The decoder predicted the current codon token using\nprevious codon tokens within the context window size and the\ncorresponding sentence embeddings. The training objective is the\nsame as in training the original genome LLMs.\nThe overall generation procedure is shown in Fig.3. Note that\nwithout the guidance of the high-level representations z0, the de-\ncoder can only take into account a limited amount of context, but\nwith the guidance ofz0, the decoder can take into account long-term\ncontext because z0 is modeled globally.\nWe conducted experiments by training a baseline LM and a\ndiffusion-based hierarchical LM on the 1.5M BV-BRC-SARS-CoV-\n2 dataset mentioned in Section 5.1. The goal of this experiment\nwas to primarily assess if the diffusion model can â€˜stitchâ€™ together\nthe context of the genes together at the genome-scale (much like\nhow words are ordered in a sentence). The baseline LM is the hi-\nerarchical LM without high-level guidance - essentially, a normal\ntransformer language model. We initialized both the baseline LM\nand the hierarchical LM decoders from the 2.5B foundation model\ntrained on individual genes from BV-BRC (Davis et al., 2016). We\nused a context window size of 1,024. The sentence encoder is ini-\ntialized from the 25M foundation model. The diffusion denoising\nmodel is a transformer with the same architecture as BERT (Kenton\nand Toutanova, 2019). We used 10 nodes from Polaris@ALCF for\ntraining, with a total of 40 A100 GPUs. We used an Adam optimizer\nwith a learning rate of 1e-4, a batch size of 2, and trained for 13k\nupdates. Training took approximately 6 hours. At generation time,\nwe used a sliding window-based approach: we first generate 1,023\ncodons from a start-of-sequence symbol, then move the window 512\ncodons to the right, generate the next 512 codons, and repeat this\nprocess until either end-of-sequence is generated or a maximum of\n15k codons have been generated.\n.CC-BY-NC-ND 4.0 International licenseavailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprintthis version posted October 11, 2022. ; https://doi.org/10.1101/2022.10.10.511571doi: bioRxiv preprint \nGenSLMs reveal SARS-CoV-2 evolutionary dynamics Supercomputing â€™22, November 14â€“19, 2022, Dallas, TX\nTo evaluate if the generated samples capture high-level dynamics,\nwe compared the distribution of the number of 5â€™-3â€™ ORFs on real\ndata and on 1,000 samples from the model. The results are plotted\nin Fig. 4A, and we observed that the diffusion-based hierarchical\nmodel does better in this regard, possibly due to the high-level\nplan, whereas the baseline can only account for the previous 1,023\ncodons. We also plotted the phylogenetic tree (see Sec. 5.3) of the\ngenerated sequences from the diffusion-based hierarchical model\nagainst real data in Fig. 4B. The plot exhibits that the generated\nsequences cover the different lineages including all the variants.\nNote that sequences with >120 mutations (1.4% of all generated\nsequences) were excluded; this demonstrates that the model can\ngenerate sequences that are relatively higher quality than just the\ntransformer-based model.\n5.2.4 Training with full viral genome sequences on Cerebras Wafer-\nScale Cluster. Training LLMs on whole SARS-CoV-2 genomes with\ndense attention is extremely challenging using traditional approaches\nand hardware. With codon-based encoding, the model needs to han-\ndle sequences of 10,240 tokens. This results in high memory and\ncomputational demand, severe limitations to batch sizes to fit on a\nsingle device, and thus a need to develop and orchestrate compli-\ncated hybrid parallelism approaches to get reasonable performance\nwith clusters of traditional devices. We overcome these challenges\nwith the Cerebras Wafer-Scale Cluster (Hall et al., 2021), where it\nis possible to use only simple data parallelism, and achieve linear\nweak scaling, even when LLMs are trained on very long sequences.\nWe pre-trained two GenSLMs to convergence on full viral genomes\nwith dense attention (Table 1) using a sequence length of 10,240\ncodons-as-tokens on a single CS-2, and on a cluster with four CS-2s,\nachieving desired accuracy and perplexity results in less than a day.\nBeyond compute performance, the Cerebras Wafer-Scale Cluster\nprovides high usability through the appliance workflow, where\nusers no longer need to handcraft different parallelism choices\nfor their given hardware and only need to specify the number of\nCS-2s to start data-parallel training. This flexibility allows faster ex-\nperiment iterations without compromising performance. Training\nGenSLMs with multiple CS-2s is pioneering work with the Cerebras\nWafer-Scale Cluster, which demonstrates the potential of dedicated\nAI hardware to apply LLMs on long-range context and work with\ngenome sequences at scale.\n5.3 Phylogenetic analyses of whole genomes\nAs described in Sec. 5.1, we used a set of 16,545 sequences from\nthe Houston Methodist Hospital System that were filtered for high-\nquality in order to analyze GenSLM outputs. We selected a diverse\nsubset by embedding these sequences, tessellating the embedding\nspace using Gaussian mixture models, and then sampling each\ntessellation using a beta distribution, resulting in a set of 1,000\nsequences maximizing coverage of the embedding space.\nThe 1,000 sequence subset was aligned to the NC_045512.1 severe\nacute respiratory syndrome coronavirus 2 isolate Wuhan-Hu-1 com-\nplete genome sequence using Mafft v7.310 (Yamada et al\n., 2016). We\nthen generated a Newick-format phylogenetic tree from the align-\nment using RAxML Next Generation (Kozlov et al., 2019), which\noffers significant speed improvements over RAxML (Stamatakis,\n2014). We then generated a phylogenetic tree using RAxML-NGâ€™s\n\"search\" algorithm, which searches for a maximum-likelihood tree\namongst 10 parsimonious trees and 10 randomly generated trees.\nThis takes âˆ¼9 hours on 5 CPUs (the recommended RAxML-NG\nparallelization settings for our data.) We used the most likely tree\ngenerated as a seed tree for running further analyses with UShER.\nUShER (Ultrafast Sample placement on Existing tRee, (Turakhia\net al., 2021)) is a SARS-CoV-2-specific analysis tool that can quickly\nplace new SARS-CoV-2 genomes onto an existing SARS-CoV-2\nphylogenetic tree on the basis of mutation tracking. In addition\nto to a \"seed\" phylogenetic tree, UShER also requires a variant\ncall format (VCF) file to track mutation data, which we generated\nfrom our multiple sequence alignment using snp-sites ((Page et al.,\n[n.d.])). UShER stores the mutation information along with the tree\nin Googleâ€™s protocol buffer (PB) format, which is created from the\nVCF and tree files.\nWe then used this tree as the basis for quickly examining and\nlabeling generated sequences of interest. Generated sequences are\n(1) converted to fasta format, (2) aligned to the NC_045512 reference\ngenome sequence, (3) mutation profiled using snp-sites, (4) placed\non the seed phylogenetic tree using Usher, (5) proposed a variant\nlabel on the basis of the labels of its K nearest neighbors (where\nK=20 in our analyses), and (6) flagged for further examination if the\nsequence has the longest phylogenetic distance to the NC_045512\nreference genome amongst its X nearest neighbors.\nWe chose this flagging scheme to select for sequences that were\nmore distant to the original strain than the other sequences in their\nclose lineage, as these sequences represent more heavily mutated\nnovel genomes that may be more likely to produce variants of\ninterest or concern.\n5.4 Integrated visualization\nWhen visualizing long-distance genomic relationships, a linear lay-\nout created edges crossing over entities and affected readability.\nWe therefore used a circular arrangement to visualize relationships\nbetween entities and positions. The visualization is flexible and\nsupports a rich set of layers to encode various data properties.\nThe outermost layer shows open reading frames (ORFs) along the\nSARS-CoV-2 genome, while the next layer displays protein-coding\nlocations. These interactive layers allow users to select an ORF or\nprotein for closer examination. All other layers (except the inner-\nmost) display different properties of codons in the gene sequence.\nThe layers encode codon properties and are presented with custom\nvisualizations based on the property type; e.g., Fig.2C and 2D, where\nprobabilities of codons are encoded using a radial bar chart (inten-\nsity of the color represents the probability). The innermost layer\nvisualizes the GenSLM attention relationships between codons. To\nreduce visual clutter, we employed hierarchical edge bundling tech-\nniques. It is important to note that the visualization is integral to the\nplatform developed where model training runs and the downstream\nprediction tasks are part of the same job.\n5.5 Workflow infrastructure\nAs illustrated in Fig. 5, we implemented and scaled the reward-\nguided beam search procedure from Section 5.2.2 leveraging a\nworkflow to couple two applications; (1) a GenSLM sequence gen-\nerator, and (2) a Bayesian optimizer to tune the reward mixing\n.CC-BY-NC-ND 4.0 International licenseavailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprintthis version posted October 11, 2022. ; https://doi.org/10.1101/2022.10.10.511571doi: bioRxiv preprint \nSupercomputing â€™22, November 14â€“19, 2022, Dallas, TX Zvyagin, M. et al.\nTarget property\n Optimal ğœ‡to prompt for target\nThinker\nBayesian Optimization to sample fÂµ\nReward GuidedGeneration\nParameters\nSequences\n(new) Parameters\nFigure 5: Conceptual overview of our workflow. A \"Thinker\"\norchestrates data flow between two applications, namely the\nsequence generator and the Bayesian optimization to drive\nthe generated sequences towards a target property using\nreward-guided beam search, where ğœ‡ represents the mixing\nconstant used balance the reward function against the log\nlikelihood of generating the next token.\nhyperparameter ğœ‡ to bias the generator towards a target property.\nOn startup, an ensemble of GenSLM generators are submitted to\nperform an initial grid search over the ğœ‡ âˆˆ(0, 1)parameter space,\nproviding sequences to update a Gaussian process surrogate model.\nThis, in turn, suggests new ğœ‡ values throughout the duration of\nthe workflow. Parameters are chosen by random sampling of ğ‘›\npoints, and are scored by their negative expected improvement (the\noptimization is a minimization). Each generation task uses a single\nA100 GPU on Polaris to run an instance of the a 25M parameter\nGenSLM model, whereas the optimizer task uses the CPUs on a\nsingle compute node.\nWe extend the Colmena workflow toolkit providing anApplication\nabstraction for each of the tasks (components) in the workflow. The\nApplication provides a few additional features: (1) inter-process\ncommunication when tasks are externally executable programs,\nand (2) warm-able functions to avoid duplicate initialization. The\nApplication abstraction enables us to isolate the many generator\ninstances from the Bayesian optimizer such that a single Thinker,\nexecuted on the login node, orchestrates communication and task\nsubmission to drive the property optimization. Leveraging Colmena\nallows us to concisely implement a multithreaded Thinker where\none thread is responsible for handling outputs from the sequence\ngenerators and immediately submitting a new generation request to\nmaximize utilization of the workers. This thread then handles any\npotential task failures by checking the return status and allows the\nworkflow to be robust to application-level failures due to uncaught\nexceptions and hardware failures. The successful results are placed\nonto a queue where another thread reads and batches the results,\n(ğœ‡, sequence) pairs, for submission to the Bayesian optimizer appli-\ncation. To further improve utilization of the workflow, we augment\nthe Thinker with a inference-only copy of the surrogate model\nwhich is periodically transferred via pickling from the Bayesian\noptimizer application.\nPolaris Selene\nJune-2022\nTop 500# 14 8\nSystem\nsize (nodes) 560 560\nCP\nU AMD\nMilan AMD\nRome\nSo\nckets/Node (total cores) 1\n(32) 2\n(128)\nSystem\nMemory (TB) 0.5 2\nNumb\ner of GPUs per node 4 8\nA100\nGPU Memory (GB) 40 80\nGP\nU Memory Technology HBM2 HBM2e\nGP\nU Memory BW (TB/s) 1.5 2.0\nInter\nconnect HPE\nSlingshot-10 Mellano\nx Infiniband HDR\nNICs\nper node 2 8\nNetw\nork BW per direction (GB/s) 12.5 25\nNumb\ner of nodes (GPUs) scaled 512\n(2048) 512\n(4096)\nTable 2: GPU supercomputing systems used for evaluation.\nWorkflows expressed with Colmena contain three components:\na Thinker, a task server, and one or many workers. The Thinker\ndefines the policies of the workflow, i.e., the dynamic dispatching\nof tasks and consumption of results. The Thinker is composed of\nagents; agents interact with each other and the task server via\nshared data structures. The task server pulls task definitions (task\nname and input pairs) from a task queue and executes tasks on\nworkers via Parsl. The task server communicates task results from\nworkers back to agents via a results queue.\nFor large task inputs or results, Colmena provides integration\nwith ProxyStore (pro, 2021), a library for decoupling data movement\nfrom control flow. Task inputs or results that exceed a user-defined\nthreshold are automatically communicated to the worker executing\nthe task via more optimal means (e.g., file system or Redis server).\nThis reduces overheads in the task server and workflow manager\nand enables lower latency task execution and higher throughput.\n6 HOW PERFORMANCE WAS MEASURED\nWe evaluate performance of GenSLM models on a diverse set of\nsystems. We first explore the performance on two leadership class\nGPU-bases supercomputing systems: 1) Polaris supercomputer at\nthe Argonne Leadership Computing Facility (Polaris@ALCF), and\n2) Selene supercomputer at NVIDIA (Selene@NVIDIA). Next, we\nevaluate the performance on the Cerebras CS-2 wafer-scale cluster.\nIn the June 2022 Top-500 list (Top500, 2022), Polaris is ranked at\n#14 with a peak of 44 PFLOPS and Selene is at #8 with a 63.4 PFLOPS\npeak. Table 2 compares the two systems used for evaluation. The\nPolaris system is an HPE Apollo Gen10+ system with 560 nodes\ninterconnected with HPE Slingshot 10 using a Dragonfly topology.\nEach node consists of an AMD \"Milan\" processor with 32 cores\nwith 512GB of system memory. Each node has four NVIDIA A100\nGPUs - each with 40GB memory. Each node has two Slingshot-10\nendpoints at 12.5 GB/s for the interconnect network. Selene is based\non the NVIDIA DGX SuperPOD platform and consists of 560 nodes\ninterconnected with Mellanox HDR fabric. Each node consists of\ntwo AMD \"Rome\" processors, each with 64 cores and 2TB system\nmemory. Each node has eight NVIDIA A100 GPUs, each with 80GB\nmemory. Each node has eight Mellanox ConnectX-6 HDR endpoints\nat 20 GB/s each for the interconnect network. Each A100 NVIDIA\nGPU is capable of achieving a peak of 19.5 TFLOPS in FP32, 156\nTFLOPS in TF32, and 312 TFLOPS in FP16 and BF16.\nGenSLM was written with the PyTorch Lightning API (Pytorch,\n2022), using transformer models from the Hugging Face repos-\nitory (huggingface, 2022). PyTorch Lightning allows the use of\nseveral distributed training strategies to scale model training on\n.CC-BY-NC-ND 4.0 International licenseavailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprintthis version posted October 11, 2022. ; https://doi.org/10.1101/2022.10.10.511571doi: bioRxiv preprint \nGenSLMs reveal SARS-CoV-2 evolutionary dynamics Supercomputing â€™22, November 14â€“19, 2022, Dallas, TX\nclusters and supercomputers. This includes DistributedDataParal-\nlel and DeepSpeed (Rasley et al., 2020). We focused our efforts on\nDeepSpeed, as its employment of various ZeRO strategies for opti-\nmization reduces the overall memory utilization in model training,\nparticularly for large parameter models (Rajbhandari et al., 2020).\nBriefly, ZeRO strategies partition memory for training models - in-\ncluding the optimizer, gradient, and model states - to use aggregate\nmemory across all GPUs. This enables training larger models on\nGPU-based systems and trades overall memory capacity for addi-\ntional re-computation and communication. In particular, ZeRO-1\npartitions optimizers across GPUs, ZeRO-2 partitions both the opti-\nmizers and gradients across all GPUs, and ZeRO-3 partitions the\nparameters, in addition to Zero-2 optimizations, across all GPUs.\nAdditionally, ZeRO-3 can scale model sizes by leveraging CPU mem-\nory and any node-local storage to offload optimizer states, gradients,\nparameters, and optionally activations to CPU. We used PyTorch\n1.12.0 and used NVIDIA NCCL 2.10.3 as the backend for DeepSpeed.\nWe used an environment with Docker containers for the runs on\nSelene, and a bare-metal build using Conda on Polaris.\nTo measure compute performance of GenSLM model training,\nwe use the DeepSpeed flops profiler (Deepspeed, 2022). The Deep-\nSpeed flops profiler provides the flops and latency of the forward\nand backward passes and latency of the weight updates, and thus\nthe compute performance of the GenSLM models. For scaling stud-\nies, we measure the entire end-to-end time including I/O as well\nas model training at scale. We measure achieved throughput in\nsamples per second as the number of GPUs scales on the system.\nTo reason with performance for an in-depth analysis, we use the\nNVIDIA Nsight tool (Bradley, 2012).\nCerebras Wafer-Scale Cluster: We also evaluated training per-\nformance on full viral genomic sequences on a Cerebras Wafer-Scale\nCluster with four CS-2s (Hall et al\n., 2021). The Cerebras Wafer-Scale\nCluster uses a weight streaming execution mode where weights\nare stored off-chip on MemoryX, a memory extension. Weights\nare streamed onto each CS-2 node using a broadcast/reduce fabric\ncalled SwarmX. Each CS-2 node is powered by the Wafer-Scale\nEngine, with 850,000 compute cores, 40 GB of on-chip memory, and\n20 petabytes/s of memory bandwidth. After the computations, gra-\ndients are streamed back to MemoryX where weights are updated.\nWe used data-parallelism in the Cerebras Wafer-Scale Cluster\nthrough the appliance workflow, where no code changes or addi-\ntional libraries were required to use either one or multiple CS-2\nsystems. GenSLM 123M and 1.3B were trained using the Cerebras\nreference implementation for GPT-2 model. This implementation is\nbased on the TensorFlow estimator and is instrumented to collect\naccuracy, perplexity and throughput measurements. We worked\nwith a Python virtual environment that included Cerebras software\nversion 1.6. All training was done using mixed precision.\n7 PERFORMANCE RESULTS\nWe evaluated performance of scaling GenSLM training on the Se-\nlene and Polaris systems. We used two target sequence lengths\n(2,048 and 10,240) in our scaling studies. Fig. 6 depicts performance,\nin terms of overall throughput measured in samples/sec, as we\nscaled with the number of GPUs on both systems. In our runs, we\nused one rank per GPU with ZeRO-3 optimizations. As we scaled\nthe number of GPUs, we kept the batch size per GPU constant\nand scaled the global batch size appropriately. We modified the\nlearning rate parameter to account for scaling the number of ranks.\nOn Selene, for sequence length 2048, we employed twice the batch\nsize of that on Polaris for the 25M, 250M, and 2B models, as the\nA100 GPUs on Selene have twice the memory capacity of those on\nPolaris. The performance obtained is the average of the throughput\nobtained over multiple iterations.\nWe observed that as model size increases from 25M to 25B, the\ntotal achievable throughput, in terms of samples/sec, decreases. This\nis expected as increasing the model size increases the computational,\nmemory and communication requirements. In case of the 25M,\n250M, and 2.5B models, we observe nearly a 2X improvement in\nthroughput on Selene in comparison to Polaris, as a double batch\nsize is employed. In terms of efficiency, for smaller models, such as\n25M, we observed a drop in scaling efficiency as we scaled beyond\n256 GPUs. Two key attributes contributing to this include the fact\nthat for smaller model sizes that run with ZeRO-3, the ratio of data\nmovement to computational flops is much higher that we are unable\nto completely overlap these. We see better performance efficiency\nfor larger models as they have higher utilization of computation and\nare able to better overlap communication with computation. Some\ninefficiencies here are also due to the performance of collectives\nand we investigate this further next. In case of the 25B model, we\nare able to fit just a single batch on the GPU and observe a 50%\nimprovement in the throughput achieved on Selene over Polaris. We\nattribute this to the increased interconnect performance we have\non Selene together with the larger memory capacity. We observe a\nsuper-linear speedup for the 25B case on both systems as we scale\nto 1024 GPUs in comparison to the performance at 8 GPUs. This is\nattributed to the increased memory and data movement overheads\nat smaller GPU scales.\nTo gain detailed insights on the runs, we performed a profiling\nstudy on the 25M parameter model on both Polaris and Selene\nsystems using the NVIDIA Nsight tool (NVIDIA, 2022). To account\nfor the difference in the number of GPUs in a single node on both\nsystems, we performed profiling runs with the 32 GPUs on 8 nodes\non Polaris and 4 on Selene separately. It was observed that there\nwas no significant delay between the steps/iterations - data loading\nand I/O was not a bottleneck. Given that the Selene DGX node\nhas 80 GB memory compared with 40GB on the Polaris node, it al-\nlowed doubling the batch size for the 25M parameter model, thereby\nachieving higher throughput than Polaris.\nIn addition, we performed a study comparing the scaling be-\nhavior of the distributed training framework implementations for\nPyTorch DistributedDataParallel (DDP) and with DeepSpeed with\nZeRO Stage 2 and 3 on Selene. With DDP, we were constrained to\nsmaller model sizes as it currently does not employ any memory\noptimization, unlike DeepSpeed. As seen from Fig. 6(B), DDP-based\nruns exhibit linear behavior, while the performance of DeepSpeed\nruns saturated beyond 256 GPUs for the stage 2 optimizer and 512\nGPUs for the stage 3 optimizations. For the 25M case, at 512 GPUs,\nDDP achieves 99% scaling efficiency with a 10% improvement over\nZeRO Stage-3 and a 2X improvement over ZeRO Stage-2. This could\nbe attributed to the fact that DDP implements\nAllReduce collective\ncommunication while DeepSpeed implementsAllGather collective\ncommunication operations. The performance of the NCCL backend\n.CC-BY-NC-ND 4.0 International licenseavailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprintthis version posted October 11, 2022. ; https://doi.org/10.1101/2022.10.10.511571doi: bioRxiv preprint \nSupercomputing â€™22, November 14â€“19, 2022, Dallas, TX Zvyagin, M. et al.\nCBA\nFigure 6: (A) Scaling results on Polaris and Selene systems for MSL=2048; (B) Scaling behavior of DDP vs. DeepSpeed runs on\nSelene (C) Scaling results on Polaris and Selene systems for MSL=10240;\nMo\ndel\nSize\nTFlops\nper step\nTime per\nstep (sec)\nTFLOPS\nper GPU\nGPUs in\nscience run\nNumber\nof Steps\n% Scaling\nEfficiency\nOverall\nZettaFlops\n25M 13.2\n0.9 14.7 512 3500 46.3 0.01\n250M 58.7\n3.4 17.3 512 1800 85.9 0.05\n2.5B 135.3\n4.5 30.3 256 2250 75.2 0.06\n25B 654.9\n14.9 43.7 1024 1800 117.8 1.42\nTable 3: Compute Performance of the production runs by\nvarying the model size for a sequence length of 2,048. This\nincludes the I/O, computations needed for forward pass, back-\nward pass and weight updates, and communication.\nis highly optimized for AllReduce in comparison to AllGather.\nThis highlights an opportunity to explore further optimizations for\nthe DeepSpeed implementation to scale on systems. We would also\nlike to note that there are additional tuning knobs at the NCCL\nlayer and in DeepSpeed, and this needs further investigation for\noptimal performance.\nFor the 10,240 sequence length, we used a batch-size of 1 and\nZeRO-3. As we increased the sequence length from 2,048 to 10,240,\nthe memory requirements, including for activation and residuals,\nincreased by a similar factor. The computation requirements also\ngrew by 5X. We were able to fit only one batch for this sequence\nlength on the GPU with the current stages employed. From Fig. 6(C),\nat 512 GPUs, for the 25M case, we observed a 50% improvement\non Selene (64 nodes) over Polaris (128 nodes). For the 250M case,\nwe observed only an 11% improvement for Selene over Polaris.\nAs the model size increased for this sequence length, we were\nbottlenecked primarily by the memory subsystem performance and\nthe overheads associated with staging residuals and parameters\nbetween the GPU and host. Additional staging optimization, model\nand activation partitioning will need to be explored.\nCompute Performance: We discuss the overall compute per-\nformance of the GenSLM model as we scaled the model size from\n25M to 25B on the Polaris system for our production science runs.\nTable 3 illustrates the measured GPU performance obtained using\nthe DeepSpeed profiler for smaller scale runs. We next took the the\nefficiency of the runs as we weak scaled to larger node and GPU\ncounts. As we weak scaled, we used the scaling efficiency to com-\npute the overall compute performance. The number of GPUs for\neach science model run were chosen based on system availability,\nand the number of steps run were chosen to achieve an appropriate\nMetric 25M\nF 250M F 2.5B F 25B F 25M\nS 250M S\nLoss 0.57\n0.46 0.30 0.70 0.015\n0.011\nPerplexity 1.78\n1.59 1.34 2.01 1.02\n1.01\nTable 4: Final loss and perplexity values achieved by the\nGenSLM Foundation (F) (2,048 tokens) and SARS-CoV-2 (S)\n(10,240 tokens) models. Reported values for S models are\ntrained on the first year of SARS-CoV-2 genomes. Perplexity\nis computed by taking the exponential of the loss and can be\ninterpreted as the number of guesses needed for the model\nto correctly fill a masked token.\nloss scale. We observed that as we scaled the model size, the overall\ncomputational flops per step increased given the increase in model\ncomplexity. The % of efficiency scaling is the performance achieved\nwhen we ran at scale in comparison with the performance achieved\nat the lowest GPU count as seen in Fig. 6(A). For our production\nscience runs, we utilized an aggregate of 1.54 zettaflops. Our 25B\nmodel utilized 1.42 zettaflops to train on 1,024 GPUs for 1800 steps.\nFinal model performance is described in Table 4.\nCerebras Wafer-Scale Cluster Scalability: We measured the\nthroughput and training time of the Wafer-Scale Cluster for GenSLM-\n123M and GenSLM-1.3B with a sequence length of 10,240 codons-\nas-tokens (Table 1). The batch size per CS-2 for each model was\nchosen based on empirical experiences with models of similar sizes,\nand was kept constant when scaled up to multiple CS-2s.\nMo\ndel size Samples/se\nc\n1\nCS-2 2 CS-2 4 CS-2\n123M 11.1\n23.1 46.2\n1.3B 0.88\n1.76 3.52\nTable 5: Cerebras Wafer-Scale Cluster throughput training\nGenSLMs on a sequence length of 10,240 tokens.\nTable 5 shows average samples per second training GenSLM-\n123M and GenSLM-1.3B for 200 steps using one, two and four CS-2s.\nRegardless of the model configurations, we observed linear weak\nscaling when using up to four CS-2s.\nWe pre-trained from scratch GenSLM-123M and GenSLM-1.3B\nusing learned embeddings. Table 6 shows training time and total\nnumber of training samples used to achieve validation accuracy\n.CC-BY-NC-ND 4.0 International licenseavailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprintthis version posted October 11, 2022. ; https://doi.org/10.1101/2022.10.10.511571doi: bioRxiv preprint \nGenSLMs reveal SARS-CoV-2 evolutionary dynamics Supercomputing â€™22, November 14â€“19, 2022, Dallas, TX\nGenSLM 123M GenSLM 1.3B\n1 CS-2 4 CS-2 1 CS-2 4 CS-2\nTraining steps 5,000 3,000 4,500 3,000\nTraining samples 165,000 396,000 49,500 132,000\nTime to train (h) 4.1 2.4 15.6 10.4\nValidation accuracy 0.9615 0.9625 0.9622 0.9947\nValidation perplexity 1.0310 1.0290 1.0310 1.0255\nTable 6: Metrics of GenSLMs pretrained from scratch on a\nsequence length of 10240 using Cerebras Wafer-Scale Cluster.\nFigure 7: Workflow utilization measured by the number of\nactive workers (applications actively serving requests) as a\nfunction of workflow runtime measured on 224 nodes of\nPolaris (896 A100 GPUs). The warm-able application design\nrealizes 97% utilization, enabling 1.9X more sequences to be\ngenerated compared to a cold start baseline.\n>96% and perplexity <1.03 using one CS-2 and with four CS-2s.\nValidation measurements were taken from checkpoints every 500\nsteps. For GenSLMs of the same size, fewer training steps were\nrequired to achieve comparable validation results when the global\nbatch size was increased in a four-CS-2 Wafer-Scale Cluster with\ndata parallelism. Reduced number of training steps plus linear weak\nscaling led to a reduction of at least a third of the training time\nwhen using four CS-2s versus one. All GenSLM training with full\ngenomes on CS-2s converged within 12 hours. GenSLM-1.3B re-\nquires fewer training samples than the smaller GenSLM-123M to\nachieve comparable validation metrics, following the sample effi-\nciency observation in neural language model scaling laws (Kaplan\net al., 2020). We note that further hyperparameter tuning is required\nto 1) optimize the throughput on the Wafer-Scale Cluster, 2) draw\nfirmer conclusions on the impact of model size on model quality.\nWorkflow Performance: We measured utilization of the se-\nquence generation workflow on 224 nodes of Polaris by counting\nthe number of workers actively serving a request as a function of\nruntime. As shown in Fig. 7, we achieve 97.0% utilization over the\n5.5 hour duration of the workflow. Persisting the GenSLMs in GPU\nmemory between requests generated 3.85 sequences per second,\nwhereas without model caching we estimate the workflow would\nhave only generated 1.98 sequences per second by extrapolating the\nmean cold start time across the number of workers. This achieves\n1.9X faster time to solution for generating synthetic sequences with\nnotable properties, allowing for rapid analysis at time scales not\npreviously feasible.\n8 IMPLICATIONS\nIn this paper, we presented GenSLMs, one of the first LLMs trained\non nucleotide sequences, particularly at the genome scale, and\ndemonstrated its performance in modeling evolutionary dynamics\nof SARS-CoV-2. Our approach overcomes key challenges related to\ntraining LLMs for biological data, specifically with respect to longer\nsequence lengths and building biologically meaningful latent spaces\nwhich can then be used for a variety of downstream prediction tasks.\nGenSLM is a foundation model for biological sequence data and\nopens up avenues for building hierarchical AI models for several\nbiological applications, including protein annotation workflows,\nmetagenome reconstruction, protein engineering, and biological\npathway design. We scaled the training of GenSLM for sequence\nlength up to 10240 tokens and 25B parameters on GPU-based su-\npercomputers. We scaled to 4,096 GPUs and utilized 1.5 Zettaflops\nfor science runs. We identified scaling avenues to be pursued in\norder to tackle larger models and sequence lengths needs for sci-\nence. We demonstrated the efficacy of the Cerebras Wafer-Scale\nCluster, an AI accelerator, to scale the training of GenSLM with\nhigh user-productivity and achieved linear scaling for 10,240 tokens\nand models up to 1.3B.\nWe also note that the information contained within nucleotide\nsequences represents a much richer vocabulary compared to PLMs\nalone. Thus, the learned representation lets us capture a much larger\nrepertoire of biological properties that are perhaps diminished while\nusing PLMs, and enables a more faithful generation process that\ncaptures the intrinsic organization of the SARS-CoV-2 sequences.\nFurther, the attention mechanism also reveals co-evolutionary pat-\nterns at the whole-genome scale that requires future investigation to\nfully understand how these long-range interactions may influence\nour ability to inform epitope modeling, immune escape, antibody\ndesign, and even vaccine design strategies. We however note that\nthere is a need to rigorously compare PLMs with GenSLM-like\napproaches. It remains to be seen if the GenSLM model does pos-\nsess richer representative power and if so how it can be further\nused. Note that we have also not been able to address the aspects\nof noise and bias in the data â€“ similar to natural language models\nwhere the models demonstrated extreme bias, there needs to be\nrigorous analyses of GenSLMs generative capabilities. We welcome\nthe community to drive the development of suitable test harnesses\nfor rigorously evaluating GenSLM-like models.\nA straightforward extension to our work would include the inte-\ngration of GenSLMs with protein structure prediction workflows\nsuch as AlphaFold (Jumper et al., 2021)/OpenFold5 to model both\nimmune escape and fitness, which determine the ability of the virus\nto adapt to its host (human) (Beguir et al., 2022). We will also ex-\nplore the use of molecular docking surrogates and faster protein\nfolding models (Lin et al., 2022). Further, incorporating experimen-\ntal data into our workflow from antibody binding assays and other\nquantiative metrics can also guide the training regimes for these\nmodels such that the generative process can be constrained to focus\non potential future VOCs.\n5http://openfold.io\n.CC-BY-NC-ND 4.0 International licenseavailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprintthis version posted October 11, 2022. ; https://doi.org/10.1101/2022.10.10.511571doi: bioRxiv preprint \nSupercomputing â€™22, November 14â€“19, 2022, Dallas, TX Zvyagin, M. et al.\nACKNOWLEDGMENTS\nWe thank the Argonne Leadership Computing Facility (ALCF) supported\nby the DOE under DE-AC02-06CH11357 and the National Energy Research\nScientific Computing Center (NERSC) at Lawrence Berkeley National Lab-\noratory supported by the DOE under Contract No. DE-AC02-05CH11231.\nWe thank Bill Allcock, Silvio Rizzi, and many others at ALCF and Wahid\nBhimji at NERSC for their timely help in enabling us to run these jobs at\nscale. We also thank Defne Gorgun, Lorenzo Casalino and Rommie Amaro\nfor stimulating discussions. This research was supported by the Exascale\nComputing Project (17-SC-20-SC), a collaborative effort of the US DOE Of-\nfice of Science and the National Nuclear Security Administration. Research\nwas supported by the DOE through the National Virtual Biotechnology\nLaboratory, a consortium of DOE national laboratories focused on response\nto COVID-19, with funding from the Coronavirus CARES Act.\nREFERENCES\n2021. ProxyStore. https://github.com/proxystore/proxystore.\nÅ½iga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-\nBarwinska, Kyle R Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R\nKelley. 2021. Effective gene expression prediction from sequence by integrating\nlong-range interactions. Nature methods 18, 10 (2021), 1196â€“1203.\nYadu Babuji, Anna Woodard, Zhuozhao Li, Ben Clifford, Rohan Kumar, Lukasz Lacinski,\nRyan Chard, Justin Wozniak, Ian Foster, Michael Wilde, Daniel Katz, and Kyle Chard.\n2019. Parsl: Pervasive Parallel Programming in Python. In ACM International\nSymposium on High-Performance Parallel and Distributed Computing.\nJordan J. Baker, Christopher J. P. Mathy, and Julia Schaletzky. 2021. A proposed\nworkflow for proactive virus surveillance and prediction of variants for vaccine\ndesign. PLOS Computational Biology 17, 12 (12 2021), 1â€“12. https://doi.org/10.1371/\njournal.pcbi.1009624\nPrasanna Balaprakash, Michael Salim, Thomas D. Uram, Venkat Vishwanath, and\nStefan M. Wild. 2018. DeepHyper: Asynchronous Hyperparameter Search for Deep\nNeural Networks. In 25th International Conference on High Performance Computing.\nIEEE. https://doi.org/10.1109/hipc.2018.00014\nVivek Balasubramanian, Shantenu Jha, Andre Merzky, and Matteo Turilli.\n2019. RADICAL-Cybertools: Middleware Building Blocks for Scalable Science.\narXiv:arXiv:1904.03085\nKarim Beguir, Marcin J. Skwark, Yunguan Fu, Thomas Pierrot, Nicolas Lopez\nCarranza, Alexandre Laterre, Ibtissem Kadri, Abir Korched, Anna U. Lowegard,\nBonny Gaby Lui, Bianca SÃ¤nger, Yunpeng Liu, Asaf Poran, Alexander Muik,\nand Ugur Sahin. 2022. Early Computational Detection of Potential High Risk\nSARS-CoV-2 Variants. bioRxiv (2022). https://doi.org/10.1101/2021.12.24.474095\narXiv:https://www.biorxiv.org/content/early/2022/09/20/2021.12.24.474095.full.pdf\nThomas Bradley. 2012. GPU performance analysis and optimisation. NVIDIA Corpora-\ntion (2012).\nPhilip J. M. Brouwer, Tom G. Caniels, Karlijn van der Straten, Jonne L. Snitselaar,\nYoann Aldon, Sandhya Bangaru, Jonathan L. Torres, Nisreen M. A. Okba, Mathieu\nClaireaux, Gius Kerster, Arthur E. H. Bentlage, Marlies M. van Haaren, Denise\nGuerra, Judith A. Burger, Edith E. Schermer, Kirsten D. Verheul, Niels van der\nVelde, Alex van der Kooi, Jelle van Schooten, MariÃ«lle J. van Breemen, Tom P. L.\nBijl, Kwinten Sliepen, Aafke Aartse, Ronald Derking, Ilja Bontjer, Neeltje A. Koot-\nstra, W. Joost Wiersinga, Gestur Vidarsson, Bart L. Haagmans, Andrew B. Ward,\nGodelieve J. de Bree, Rogier W. Sanders, and Marit J. van Gils. 2020. Potent neu-\ntralizing antibodies from COVID-19 patients define multiple targets of vulnera-\nbility. Science 369, 6504 (2020), 643â€“650. https://doi.org/10.1126/science.abc5902\narXiv:https://www.science.org/doi/pdf/10.1126/science.abc5902\nBegum Cosar, Zeynep Yagmur Karagulleoglu, Sinan Unal, Ahmet Turan Ince, Dil-\nruba Beyza Uncuoglu, Gizem Tuncer, Bugrahan Regaip Kilinc, Yunus Emre Ozkan,\nHikmet Ceyda Ozkoc, Ibrahim Naki Demir, Ali Eker, Feyzanur Karagoz, Said Yasin\nSimsek, Bunyamin Yasar, Mehmetcan Pala, Aysegul Demir, Irem Naz Atak, Ay-\nsegul Hanife Mendi, Vahdi Umut Bengi, Guldane Cengiz Seval, Evrim Gunes Al-\ntuntas, Pelin Kilic, and Devrim Demir-Dora. 2022. SARS-CoV-2 Mutations and\ntheir Viral Variants. Cytokine Growth Factor Rev 63 (Feb 2022), 10â€“22. https:\n//doi.org/10.1016/j.cytogfr.2021.06.001\nJames J Davis, Svetlana Gerdes, Gary J Olsen, Robert Olson, Gordon D Pusch, Maulik\nShukla, Veronika Vonstein, Alice R Wattam, and Hyunseung Yoo. 2016. PATtyFams:\nProtein Families for the Microbial Genomes in the PATRIC Database.Front Microbiol\n7 (2016), 118. https://doi.org/10.3389/fmicb.2016.00118\nDeepspeed. 2022. Flops Profiler - Deepspeed. https://www.deepspeed.ai/tutorials/flops-\nprofiler/\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-\ntraining of deep bidirectional transformers for language understanding. arXiv\npreprint arXiv:1810.04805 (2018).\nMichael B. Doud, Juhye M. Lee, and Jesse D. Bloom. 2018. How single mutations affect\nviral escape from broad and narrow antibodies to H1 influenza hemagglutinin.\nNature Communications 9, 1 (2018), 1386. https://doi.org/10.1038/s41467-018-\n03665-3\nAlexander Dunn, Julien Brenneck, and Anubhav Jain. 2019. Rocketsled: A software\nlibrary for optimizing high-throughput computational searches. Journal of Physics:\nMaterials 2, 3 (April 2019), 034002. https://doi.org/10.1088/2515-7639/ab0c3d\nAhmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Yu Wang,\nLlion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger,\nDebsindhu Bhowmik, and Burkhard Rost. 2022. ProtTrans: Toward Understand-\ning the Language of Life Through Self-Supervised Learning. IEEE Transactions\non Pattern Analysis and Machine Intelligence 44, 10 (2022), 7112â€“7127. https:\n//doi.org/10.1109/TPAMI.2021.3095381\nNoelia Ferruz, Michael Heinzinger, Mehmet Akdel, Alexander\nGoncearenco, Luca Naef, and Christian Dallago. 2022. From se-\nquence to function through structure: deep learning for protein de-\nsign. bioRxiv (2022). https://doi.org/10.1101/2022.08.31.505981\narXiv:https://www.biorxiv.org/content/early/2022/09/03/2022.08.31.505981.full.pdf\nAllison J. Greaney, Tyler N. Starr, Pavlo Gilchuk, Seth J. Zost, Elad Binshtein, Andrea N.\nLoes, Sarah K. Hilton, John Huddleston, Rachel Eguia, Katharine H.D. Crawford,\nAdam S. Dingens, Rachel S. Nargi, Rachel E. Sutton, Naveenchandra Suryadevara,\nPaul W. Rothlauf, Zhuoming Liu, Sean P.J. Whelan, Robert H. Carnahan, James E.\nCrowe, and Jesse D. Bloom. 2021. Complete Mapping of Mutations to the SARS-\nCoV-2 Spike Receptor-Binding Domain that Escape Antibody Recognition. Cell\nHost & Microbe 29, 1 (2021), 44â€“57.e9. https://doi.org/10.1016/j.chom.2020.11.007\nAnmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu,\nWei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. 2020.\nConformer: Convolution-augmented Transformer for Speech Recognition. https:\n//doi.org/10.48550/ARXIV.2005.08100\nStewart Hall, Rob Schreiber, and Sean Lie. 2021. Training Giant Neural\nNetworks Using Weight Streaming on Cerebras Wafer-Scale Systems.\nhttps://f.hubspotusercontent30.net/hubfs/8968533/Virtual%20Booth%20Docs/CS%\n20Weight%20Streaming%20White%20Paper%20111521.pdf\nKai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu,\nYehui Tang, An Xiao, Chunjing Xu, Yixing Xu, Zhaohui Yang, Yiman Zhang, and\nDacheng Tao. 2022. A Survey on Vision Transformer. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence (2022), 1â€“1. https://doi.org/10.1109/TPAMI.2022.\n3152247\nBrian Hie, Ellen D. Zhong, Bonnie Berger, and Bryan Bryson. 2021.\nLearning the language of viral evolution and escape. Science\n371, 6526 (2021), 284â€“288. https://doi.org/10.1126/science.abd7331\narXiv:https://www.science.org/doi/pdf/10.1126/science.abd7331\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic\nmodels. Advances in Neural Information Processing Systems 33 (2020), 6840â€“6851.\nStephen Hudson, Jeffrey Larson, John-Luke Navarro, and Stefan Wild. 2022. libEnsem-\nble: A Library to Coordinate the Concurrent Evaluation of Dynamic Ensembles\nof Calculations. IEEE Transactions on Parallel and Distributed Systems 33, 4 (2022),\n977â€“988. https://doi.org/10.1109/tpds.2021.3082815\nhuggingface. 2022. Transformers: State-of-the-art Machine Learning for Pytorch, Tensor-\nFlow, and JAX. https://github.com/huggingface/transformers\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer\nLevy. 2020. Spanbert: Improving pre-training by representing and predicting spans.\nTransactions of the Association for Computational Linguistics 8 (2020), 64â€“77.\nBin Ju, Qi Zhang, Jiwan Ge, Ruoke Wang, Jing Sun, Xiangyang Ge, Jiazhen Yu, Sisi\nShan, Bing Zhou, Shuo Song, Xian Tang, Jinfang Yu, Jun Lan, Jing Yuan, Haiyan\nWang, Juanjuan Zhao, Shuye Zhang, Youchun Wang, Xuanling Shi, Lei Liu, Jincun\nZhao, Xinquan Wang, Zheng Zhang, and Linqi Zhang. 2020. Human neutralizing\nantibodies elicited by SARS-CoV-2 infection. Nature 584, 7819 (2020), 115â€“119.\nhttps://doi.org/10.1038/s41586-020-2380-z\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ron-\nneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Å½Ã­dek, Anna Potapenko,\nAlex Bridgland, Clemens Meyer, Simon A. A. Kohl, Andrew J. Ballard, Andrew\nCowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler,\nTrevor Back, Stig Petersen, David Reiman, Ellen Clancy, Michal Zielinski, Martin\nSteinegger, Michalina Pacholska, Tamas Berghammer, Sebastian Bodenstein, David\nSilver, Oriol Vinyals, Andrew W. Senior, Koray Kavukcuoglu, Pushmeet Kohli, and\nDemis Hassabis. 2021. Highly accurate protein structure prediction with AlphaFold.\nNature 596, 7873 (2021), 583â€“589. https://doi.org/10.1038/s41586-021-03819-2\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon\nChild, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling Laws\nfor Neural Language Models. https://doi.org/10.48550/ARXIV.2001.08361\nJacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nProceedings of NAACL-HLT. 4171â€“4186.\nAlexey M Kozlov, Diego Darriba, TomÃ¡Å¡ Flouri, Benoit Morel, and Alexandros Sta-\nmatakis. 2019. RAxML-NG: a fast, scalable and user-friendly tool for maximum\nlikelihood phylogenetic inference. Bioinformatics 35, 21 (Nov. 2019), 4453â€“4455.\n.CC-BY-NC-ND 4.0 International licenseavailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprintthis version posted October 11, 2022. ; https://doi.org/10.1101/2022.10.10.511571doi: bioRxiv preprint \nGenSLMs reveal SARS-CoV-2 evolutionary dynamics Supercomputing â€™22, November 14â€“19, 2022, Dallas, TX\nhttps://doi.org/10.1093/bioinformatics/btz305\nZeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos\nSantos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, and Alexander Rives.\n2022. Language models of protein sequences at the scale of evolution enable accu-\nrate structure prediction. bioRxiv (2022). https://doi.org/10.1101/2022.07.20.500902\narXiv:https://www.biorxiv.org/content/early/2022/07/21/2022.07.20.500902.full.pdf\nM. Cyrus Maher, Istvan Bartha, Steven Weaver, Julia di Iulio, Elena Ferri,\nLeah Soriaga, Florian A. Lempp, Brian L. Hie, Bryan Bryson, Bonnie Berger,\nDavid L. Robertson, Gyorgy Snell, Davide Corti, Herbert W. Virgin, Sergei\nL. Kosakovsky Pond, and Amalio Telenti. 2022. Predicting the mutational\ndrivers of future SARS-CoV-2 variants of concern. Science Translational\nMedicine 14, 633 (2022), eabk3445. https://doi.org/10.1126/scitranslmed.abk3445\narXiv:https://www.science.org/doi/pdf/10.1126/scitranslmed.abk3445\nPhilipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw,\nEric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael I. Jordan, and Ion\nStoica. 2018. Ray: A Distributed Framework for Emerging AI Applications. In 13th\nUSENIX Symposium on Operating Systems Design and Implementation (OSDI 18).\nUSENIX Association, Carlsbad, CA, 561â€“577. https://www.usenix.org/conference/\nosdi18/presentation/moritz\nSaul B. Needleman and Christian D. Wunsch. 1970. A general method applicable to\nthe search for similarities in the amino acid sequence of two proteins. Journal\nof Molecular Biology 48, 3 (1970), 443â€“453. https://doi.org/10.1016/0022-2836(70)\n90057-4\nNVIDIA. 2022. NVIDIA Nsight Systems. https://developer.nvidia.com/nsight-systems\nSarah P Otto, Troy Day, Julien Arino, Caroline Colijn, Jonathan Dushoff, Michael Li,\nSamir Mechai, Gary Van Domselaar, Jianhong Wu, David J D Earn, and Nicholas H\nOgden. 2021. The origins and potential future of SARS-CoV-2 variants of concern\nin the evolving COVID-19 pandemic. Curr Biol 31, 14 (Jul 2021), R918â€“R929.\nhttps://doi.org/10.1016/j.cub.2021.06.049\nAndrew J Page, Ben Taylor, Aidan J Delaney, Jorge Soares, Torsten Seemann, A Keane,\nand Simon R Harris. [n.d.]. SNP-sites: rapid efficient extraction of SNPs from multi-\nFASTA alignments. Microbial Genomics ([n. d.]), 5.\nPinelopi Papalampidi, Kris Cao, and Tomas Kocisky. 2022. Towards Coherent and\nConsistent Use of Entities in Narrative Generation. arXiv preprint arXiv:2202.01709\n(2022).\nPytorch. 2022. Pytorch Lightning. https://www.pytorchlightning.ai/\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving\nlanguage understanding by generative pre-training. (2018).\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: Mem-\nory optimizations toward training trillion parameter models. In SC20: International\nConference for High Performance Computing, Networking, Storage and Analysis. IEEE,\n1â€“16.\nAndrew Rambaut, Edward C. Holmes, Ãine Oâ€™Toole, Verity Hill, John T. McCrone,\nChristopher Ruis, Louis du Plessis, and Oliver G. Pybus. 2020. A dynamic nomen-\nclature proposal for SARS-CoV-2 lineages to assist genomic epidemiology. Nat\nMicrobiol 5, 11 (Nov. 2020), 1403â€“1407. https://doi.org/10.1038/s41564-020-0770-5\nAnkit Ramchandani, Chao Fan, and Ali Mostafavi. 2020. DeepCOVIDNet: An In-\nterpretable Deep Learning Model for Predictive Surveillance of COVID-19 Using\nHeterogeneous Features and Their Interactions.IEEE Access 8 (2020), 159915â€“159930.\nhttps://doi.org/10.1109/ACCESS.2020.3019989\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed:\nSystem optimizations enable training deep learning models with over 100 billion\nparameters. In Proceedings of the 26th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining. 3505â€“3506.\nAlexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason\nLiu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus.\n2021. Biological structure and function emerge from scaling unsupervised learn-\ning to 250 million protein sequences. Proceedings of the National Academy of\nSciences 118, 15 (2021), e2016239118. https://doi.org/10.1073/pnas.2016239118\narXiv:https://www.pnas.org/doi/pdf/10.1073/pnas.2016239118\nMichael Salim, Thomas Uram, J. Taylor Childers, Venkatram Vishwanath, and Michael\nPapka. 2019. Balsam: Near Real-Time Experimental Data Analysis on Supercomput-\ners. In 2019 IEEE/ACM 1st Annual Workshop on Large-scale Experiment-in-the-Loop\nComputing (XLOOP). IEEE. https://doi.org/10.1109/xloop49562.2019.00010\nYoung C. Shin, Georg F. Bischof, William A. Lauer, and Ronald C. Desrosiers.\n2015. Importance of codon usage for the temporal regulation of vi-\nral gene expression. Proceedings of the National Academy of Sciences\n112, 45 (2015), 14030â€“14035. https://doi.org/10.1073/pnas.1515387112\narXiv:https://www.pnas.org/doi/pdf/10.1073/pnas.1515387112\nAlexandros Stamatakis. 2014. RAxML version 8: a tool for phylogenetic analysis and\npost-analysis of large phylogenies. Bioinformatics 30, 9 (May 2014), 1312â€“1313.\nhttps://doi.org/10.1093/bioinformatics/btu033\nSimeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. 2021. Do\nLong-Range Language Models Actually Use Long-Range Context? arXiv preprint\narXiv:2109.09115 (2021).\nSimeng Sun, Katherine Thai, and Mohit Iyyer. 2022. ChapterBreak: A Challenge\nDataset for Long-Range Language Models. arXiv preprint arXiv:2204.10878 (2022).\nAnia Syrowatka, Masha Kuznetsova, Ava Alsubai, Adam L. Beckman, Paul A. Bain,\nKelly Jean Thomas Craig, Jianying Hu, Gretchen Purcell Jackson, Kyu Rhee, and\nDavid W. Bates. 2021. Leveraging artificial intelligence for pandemic preparedness\nand response: a scoping review to identify key use cases. npj Digital Medicine 4, 1\n(2021), 96. https://doi.org/10.1038/s41746-021-00459-8\nTop500. 2022. June 2022 | TOP500. https://www.top500.org/lists/top500/2022/06/\nYatish Turakhia, Bryan Thornlow, Angie S. Hinrichs, Nicola De Maio, Landen Gozashti,\nRobert Lanfear, David Haussler, and Russell Corbett-Detig. 2021. Ultrafast Sample\nplacement on Existing tRees (UShER) enables real-time phylogenetics for the SARS-\nCoV-2 pandemic. Nat Genet 53, 6 (June 2021), 809â€“816. https://doi.org/10.1038/\ns41588-021-00862-7\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Well-read\nstudents learn better: On the importance of pre-training compact models. arXiv\npreprint arXiv:1908.08962 (2019).\nSerbulent Unsal, Heval Atas, Muammer Albayrak, Kemal Turhan, Aybar C Acar, and\nTunca DoÄŸan. 2022. Learning functional properties of proteins with language\nmodels. Nature Machine Intelligence 4, 3 (2022), 227â€“245.\nPascal Vincent. 2011. A connection between score matching and denoising autoen-\ncoders. Neural computation 23, 7 (2011), 1661â€“1674.\nZachary S. Wallace, James Davis, Anna Maria Niewiadomska, Robert D. Olson, Maulik\nShukla, Rick Stevens, Yun Zhang, Christian M. Zmasek, and Richard H. Scheuer-\nmann. 2022. Early Detection of Emerging SARS-CoV-2 Variants of Interest for Ex-\nperimental Evaluation. medRxiv (2022). https://doi.org/10.1101/2022.08.08.22278553\narXiv:https://www.medrxiv.org/content/early/2022/08/10/2022.08.08.22278553.full.pdf\nRose E Wang, Esin Durmus, Noah Goodman, and Tatsunori Hashimoto. 2022. Lan-\nguage modeling via stochastic processes. In International Conference on Learning\nRepresentations.\nShiliang Wang, Jaideep P. Sundaram, and Timothy B. Stockwell. 2012. VIGOR extended\nto annotate genomes for additional 12 different viruses. Nucleic Acids Research 40,\nW1 (06 2012), W186â€“W192. https://doi.org/10.1093/nar/gks528\nL. Ward, G. Sivaraman, J. Pauloski, Y. Babuji, R. Chard, N. Dandu, P. C. Redfern, R. S.\nAssary, K. Chard, L. A. Curtiss, R. Thakur, and I. Foster. 2021. Colmena: Scalable\nMachine-Learning-Based Steering of Ensemble Simulations for High Performance\nComputing. In 2021 IEEE/ACM Workshop on Machine Learning in High Performance\nComputing Environments (MLHPC). IEEE Computer Society, Los Alamitos, CA, USA,\n9â€“20. https://doi.org/10.1109/MLHPC54614.2021.00007\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud,\nDani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022. Emergent\nabilities of large language models. arXiv preprint arXiv:2206.07682 (2022).\nJ. M. Wozniak, T. G. Armstrong, M. Wilde, D. S. Katz, E. Lusk, and I. T. Foster. 2013.\nSwift/T: Large-Scale Application Composition via Distributed-Memory Dataflow\nProcessing. In 13th IEEE/ACM International Symposium on Cluster, Cloud, and Grid\nComputing. 95â€“102. https://doi.org/10.1109/CCGrid.2013.99\nKazunori D. Yamada, Kentaro Tomii, and Kazutaka Katoh. 2016. Application of the\nMAFFT sequence alignment program to large dataâ€”reexamination of the usefulness\nof chained guide trees. Bioinformatics 32, 21 (Nov. 2016), 3246â€“3251. https://doi.\norg/10.1093/bioinformatics/btw412\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\nChristopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open\npre-trained transformer language models. arXiv preprint arXiv:2205.01068 (2022).\nSeth J. Zost, Pavlo Gilchuk, Rita E. Chen, James Brett Case, Joseph X. Reidy, An-\ndrew Trivette, Rachel S. Nargi, Rachel E. Sutton, Naveenchandra Suryadevara,\nElaine C. Chen, Elad Binshtein, Swathi Shrihari, Mario Ostrowski, Helen Y. Chu,\nJonathan E. Didier, Keith W. MacRenaris, Taylor Jones, Samuel Day, Luke Myers,\nF. Eun-Hyung Lee, Doan C. Nguyen, Ignacio Sanz, David R. Martinez, Paul W.\nRothlauf, Louis-Marie Bloyet, Sean P. J. Whelan, Ralph S. Baric, Larissa B. Thack-\nray, Michael S. Diamond, Robert H. Carnahan, and James E. Crowe. 2020. Rapid\nisolation and profiling of a diverse panel of human monoclonal antibodies tar-\ngeting the SARS-CoV-2 spike protein. Nature Medicine 26, 9 (2020), 1422â€“1427.\nhttps://doi.org/10.1038/s41591-020-0998-x\n.CC-BY-NC-ND 4.0 International licenseavailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprintthis version posted October 11, 2022. ; https://doi.org/10.1101/2022.10.10.511571doi: bioRxiv preprint "
}