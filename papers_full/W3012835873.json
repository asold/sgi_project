{
    "title": "Learning to Encode Position for Transformer with Continuous Dynamical Model",
    "url": "https://openalex.org/W3012835873",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2647923735",
            "name": "Liu,Xuanqing",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2591561200",
            "name": "Yu, Hsiang-Fu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222291380",
            "name": "Dhillon, Inderjit",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222521706",
            "name": "Hsieh, Cho-Jui",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2170120409",
        "https://openalex.org/W2963323070",
        "https://openalex.org/W2427527485",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W2963755523",
        "https://openalex.org/W2806311723",
        "https://openalex.org/W2895434480",
        "https://openalex.org/W2525332836",
        "https://openalex.org/W2952509486",
        "https://openalex.org/W2789541106",
        "https://openalex.org/W2947881255",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W1539774149",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2995744795",
        "https://openalex.org/W2950501607",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2945918281",
        "https://openalex.org/W2975059944",
        "https://openalex.org/W2950813464"
    ],
    "abstract": "We introduce a new way of learning to encode position information for non-recurrent models, such as Transformer models. Unlike RNN and LSTM, which contain inductive bias by loading the input tokens sequentially, non-recurrent models are less sensitive to position. The main reason is that position information among input units is not inherently encoded, i.e., the models are permutation equivalent; this problem justifies why all of the existing models are accompanied by a sinusoidal encoding/embedding layer at the input. However, this solution has clear limitations: the sinusoidal encoding is not flexible enough as it is manually designed and does not contain any learnable parameters, whereas the position embedding restricts the maximum length of input sequences. It is thus desirable to design a new position layer that contains learnable parameters to adjust to different datasets and different architectures. At the same time, we would also like the encodings to extrapolate in accordance with the variable length of inputs. In our proposed solution, we borrow from the recent Neural ODE approach, which may be viewed as a versatile continuous version of a ResNet. This model is capable of modeling many kinds of dynamical systems. We model the evolution of encoded results along position index by such a dynamical system, thereby overcoming the above limitations of existing methods. We evaluate our new position layers on a variety of neural machine translation and language understanding tasks, the experimental results show consistent improvements over the baselines.",
    "full_text": "Learning to Encode Position for Transformer\nwith Continuous Dynamical Model\nXuanqing Liu‚Ä†, Hsiang-Fu Yu‚Ä°, Inderjit Dhillon¬ß‚Ä°, Cho-Jui Hsieh‚Ä†\n‚Ä† UCLA ¬ßUT Austin ‚Ä°Amazon Inc.\nxqliu@cs.ucla.edu rofu.yu@gmail.com\ninderjit@cs.utexas.edu chohsieh@cs.ucla.edu\nAbstract\nWe introduce a new way of learning to encode position information for non-recurrent models,\nsuch as Transformer models. Unlike RNN and LSTM, which contain inductive bias by loading\nthe input tokens sequentially, non-recurrent models are less sensitive to position. The main reason\nis that position information among input units is not inherently encoded, i.e., the models are\npermutation equivalent; this problem justiÔ¨Åes why all of the existing models are accompanied by\na sinusoidal encoding/embedding layer at the input. However, this solution has clear limitations:\nthe sinusoidal encoding is not Ô¨Çexible enough as it is manually designed and does not contain\nany learnable parameters, whereas the position embedding restricts the maximum length of input\nsequences. It is thus desirable to design a new position layer that contains learnable parameters to\nadjust to different datasets and different architectures. At the same time, we would also like the\nencodings to extrapolate in accordance with the variable length of inputs. In our proposed solution,\nwe borrow from the recent Neural ODE approach, which may be viewed as a versatile continuous\nversion of a ResNet. This model is capable of modeling many kinds of dynamical systems. We\nmodel the evolution of encoded results along position index by such a dynamical system, thereby\novercoming the above limitations of existing methods. We evaluate our new position layers on a\nvariety of neural machine translation and language understanding tasks, the experimental results\nshow consistent improvements over the baselines.\n1 Introduction\nTransformer based models [ 1, 2, 3, 4, 5, 6] have become one of the most effective approaches to\nmodel sequence data of variable lengths. Transformers have shown wide applicability to many natural\nlanguage processing (NLP) tasks such as language modeling [4], neural machine translation (NMT) [1],\nand language understanding [ 2]. Unlike traditional recurrent-based models (e.g., RNN or LSTM),\nTransformer utilizes a non-recurrent but self-attentive neural architecture to model the dependency\namong elements at different positions in the sequence, which leads to better parallelization using\nmodern hardware and alleviates the vanishing/exploding gradient problem in traditional recurrent\nmodels.\n[7] prove that the design of self-attentive architecture leads to a family of permutation equivalence\nfunctions. Thus, for applications where the ordering of the elements matters, how to properly encode\nposition information is crucial for Transformer based models. There have been many attempts to\nencode position information for the Transformer. In the original Transformer paper [1], a family of\n1\narXiv:2003.09229v1  [cs.LG]  13 Mar 2020\npre-deÔ¨Åned sinusoidal functions was adapted to construct a set of embeddings for each position. These\nÔ¨Åxed position embeddings are then added to the word embeddings of the input sequence accordingly.\nTo further construct these position embeddings in a more data-driven way, many recent Transformer\nvariants such as [2, 8] include these embeddings as learnable model parameters in the training stage.\nThis data-driven approach comes at the cost of the limitation of a Ô¨Åxed maximum length of input\nsequence Lmax and the computational/memory overhead of additional Lmax √ódparameters, where\nLmax is usually set to 512 in many applications, and dis the dimension of the embeddings. [9] propose\na relative position representation to reduce the number of parameters to (2K+ 1)dby dropping the\ninteractions between tokens with a distance greater than K. In addition to just the input layer, [10] and\n[5] suggest that the injection of position information to every layer leads to even better performance for\nthe Transformer.\nAn ideal position encoding approach should satisfy the following three properties:\n1. Inductive: the ability to handle sequences longer than any sequence seen in the training time.\n2. Data-Driven: the position encoding should be learnable from the data.\n3. Parameter EfÔ¨Åcient: number of trainable parameters introduced by the encoding should be limited\nto avoid increased model size, which could hurt generalization.\nIn Table 1, we summarize some of the existing position encoding approaches in terms of these three\nproperties.\nIn this paper, we propose a new method to encode position with minimum cost. The main idea is\nto model position encoding as a continuous dynamical system, so we only need to learn the system\ndynamics instead of learning the embeddings for each position independently. By doing so, our\nmethod enjoys the best of both worlds ‚Äì we bring back the inductive bias, and the encoding method\nis freely trainable while being parameter efÔ¨Åcient. To enable training of this dynamical system with\nbackpropagation, we adopt the recent progress in continuous neural network [ 11], ofÔ¨Åcially called\nNeural ODE. In some generative modeling literature, it is also called the free-form Ô¨Çow model [12],\nso we call our model FLOw-bAsed TransformER (FLOATER). We highlight our contributions as\nfollows:\n‚Ä¢We propose FLOATER, a new position encoder for Transformer, which models the position informa-\ntion via a continuous dynamical model in a data-driven and parameter-efÔ¨Åcient manner.\n‚Ä¢Due to the use of a continuous dynamic model, FLOATER can handle sequences of any length. This\nproperty makes inference more Ô¨Çexible.\n‚Ä¢With careful design, our position encoder is compatible with the original Transformer; i.e., the\noriginal Transformer can be regarded as a special case of our proposed position encoding approach.\nAs a result, we are not only able to train a Transformer model with FLOATER from scratch but also\nplug FLOATER into most existing pre-trained Transformer models such as BERT, RoBERTa,etc.\n‚Ä¢We demonstrate that FLOATER consistent improvements over baseline models across a variety of\nNLP tasks ranging from machine translations, language understanding, and question answering.\n2 Background and Related Work\n2.1 Importance of Position Encoding for Transformer\nWe use a simpliÔ¨Åed self-attentive sequence encoder to illustrate the importance of position encoding in\nthe Transformer. Without position encoding, the Transformer architecture can be viewed as a stack of\nN blocks Bn : n= 1,...,N containing a self-attentive An and a feed-forward layer Fn. By dropping\nthe residual connections and layer normalization, the architecture of a simpliÔ¨Åed Transformer encoder\n2\nTable 1: Comparing position representation methods\nMethods Inductive Data-Driven Parameter EfÔ¨Åcient\nSinusoidal [1] \u0013 \u0017 \u0013\nEmbedding [2] \u0017 \u0013 \u0017\nRelative [9] \u0017 \u0013 \u0013\nThis paper \u0013 \u0013 \u0013\ncan be represented as follows.\nEncode(x) = BN ‚ó¶BN‚àí1 ‚ó¶¬∑¬∑¬∑‚ó¶ B1(x), (1)\nBn(x) = Fn ‚ó¶An(x) , (2)\nwhere x= [x1,x2,..., xL]‚ä§‚ààRL√ód, Lis the length of the sequence and dis the dimension of the\nword embedding. An(¬∑) and Fn(¬∑) are the self-attentive and feed-forward layer in the n-th block Bn(¬∑),\nrespectively.\nEach row of A1(x) can be regarded as a weighted sum of the value matrixV ‚ààRL√ód, with the weights\ndetermined by similarity scores between the key matrix K‚ààRL√ód and query matrix Q‚ààRL√ód as\nfollows:\nA1(x) = Softmax\n(QK‚ä§\n‚àö\nd\n)\nV,\nQ= [q1,q2,..., qL]‚ä§, qi = Wqxi + bq,\nK= [k1,k2,..., kL]‚ä§, ki = Wkxi + bk,\nV = [v1,v2,..., vL]‚ä§, vi = Wvxi + bv,\n(3)\nWq/k/v and bq/k/v are the weight and bias parameters introduced in the self-attentive function A1(¬∑).\nThe output of the feed-forward function F1(¬∑) used in the Transformer is also a matrix with Lrows. In\nparticular, the i-th row is obtained as follows.\nthe i-th row of F1(x) = W2œÉ(W1xi + b1) + b2, (4)\nwhere W1,2 and b1,2 are the weights and biases of linear transforms, and œÉ(¬∑) is the activation\nfunction. It is not hard to see from (3) and (4) that both A1(¬∑) and F1(¬∑) are permutation equivalent.\nThus, we can conclude that the entire function deÔ¨Åned in (1) is also permutation equivalent, i.e.,\nŒ†√óEncode(x) = Encode (Œ† √óx) for any L√óLpermutation matrix Œ†. This permutation equivalence\nproperty restricts the Transformer without position information from modeling sequences where the\nordering of elements matters.\n2.2 Position Encoding in Transformer\nAs mentioned in Section 1, there are many attempts to inject position information in self-attentive\ncomponents. Most of them can be described in the following form:\nBn(x) = Fn ‚ó¶An ‚ó¶Œ¶n(x), n‚àà{1,...,N }, (5)\n3\nSelf-Attention\nFFN\nAdd & norm\nAdd & norm\nPosition encoding\nat thek-th layer\nHidden feature\nFlow-based transition model\nL√ó\nFLOATER-Encoder\nSelf-Attention\nFFN\nAdd & norm\nPosition encoding\nat thek-th layer\nHidden feature\nFlow-based transition model\nL√ó\nFLOATER-Decoder\nAdd & norm\nEnc-Dec-Attention\nAdd & norm\nFigure 1: The architecture of our model (FLOATER). The main differences between FLOATER and\nthe original Transformer model are: 1) the position representation is integrated into each block in\nthe hierarchy (there are N blocks in total); and 2) there is a dynamical model (see (8)) that generates\nposition encoding vectors for each block. The dynamics are solved with a black-box ODE solver\ndetailed in the supplementary material.\nwhere Œ¶n(x) is a position encoding function.\n[1] propose to keep Œ¶n(x) = x,‚àÄn‚â•2 and inject position information only at the input block with a\nfamily of pre-deÔ¨Åned sinusoidal functions: Œ¶1(x) = x+ p(1), where p(1) = [p(1)\n1 ,p(1)\n2 ,..., p(1)\nL ] is a\nposition embedding matrix with the i-th row corresponding to the i-th position in the input sequence.\nIn particular, the j-th dimension of the i-th row is deÔ¨Åned as follows.\np(1)\ni [j] =\nÔ£±\nÔ£≤\nÔ£≥\nsin(i¬∑c\nj\nd ) if jis even,\ncos(i¬∑c\nj‚àí1\nd ) if jis odd,\n(6)\nwhere c= 10‚àí4. [10] and [5] observe better performance by further injecting the position information\nat each block, i.e., Œ¶n(x) = x+ p(n) as follows:\np(n)\ni [j] =\nÔ£±\nÔ£≤\nÔ£≥\nsin(i¬∑c\nj\nd ) + sin(n¬∑c\nj\nd ) if jis even,\ncos(i¬∑c\nj‚àí1\nd ) + cos(n¬∑c\nj‚àí1\nd ) if jis odd.\n(7)\nNote that for the above two approaches, position encoding functions Œ¶n(¬∑) are Ô¨Åxed for all the\napplications. Although no additional parameters are introduced in the model, both approaches are\ninductive and can handle input sequences of variable length.\nMany successful variants of pre-trained Transformer models, such as BERT [ 2] and RoBERTa [8],\ninclude the entire embedding matrix p(1) ‚ààRL√ód in Œ¶1(x) as training parameters. As the number\nof training parameters needs to be Ô¨Åxed, the maximum length of a sequence, Lmax, is required to be\ndetermined before the training. Although it lacks the inductive property, this data-driven approach is\nfound to be effective for many NLP tasks. Note that, unlike the Ô¨Åxed sinusoidal position encoding,\nthere is no attempt to inject a learnable position embedding matrix at each block for Transformer due\nto a large number of additional parameters (NLmaxd).\n4\n3 FLOATER: Our Proposed Position Encoder\nWe introduce our method in three steps. In the Ô¨Årst step, we only look at one Transformer block, and\ndescribe how to learn the position representation driven by a dynamical system; in the second step, we\nshow how to save parameters if we add position signals to every layer; lastly, we slightly change the\narchitecture to save trainable parameters further and make FLOATER ‚Äúcompatible‚Äù with the original\nTransformer [1]. The compatibility means our model is a strict superset of the vanilla Transformer so\nthat it can be initialized from the Transformer.\n3.1 Position Encoding with Dynamical Systems\nPosition representations in Transformer models are a sequence of vectors {pi ‚ààRd : i= 1,...,L }to\nbe added to the sequence of the input representations {xi : i= 1,...,L }. Existing position encoding\napproaches either apply a Ô¨Åxed sinusoidal function to obtain {pi}, or include them as uncorrelated\nlearnable parameters. Both of them fail to capture the dependency or dynamics among these position\nrepresentations {pi}. In this paper, we propose to use a dynamical system to model these position\nrepresentations; that is, there is a ‚Äúlatent force‚Äù denoted by hi that drives the changes from pi to pi+1.\nTo encourage smoothness, we consider p(t) : R+ ‚Ü¶‚ÜíRd as the continuous version of the discrete\nsequence {pi}. In particular, our proposed continuous dynamical system is characterized as follows:\np(t)= p(s)+\n‚à´ t\ns\nh(œÑ,p(œÑ); Œ∏h) dœÑ, 0 ‚â§s‚â§t< ‚àû, (8)\ntogether with an initial vector p(0), where h(œÑ,p(œÑ); Œ∏h) is a neural network parameterized by Œ∏h\nand takes the previous state (œÑ,p(œÑ)). Notice that the domain of p(¬∑) is R+. The position sequence\n{pi}can be obtained by taking p(¬∑) on a series of points {ti : 0 ‚â§t1 <¬∑¬∑¬∑‚â§ tL}: pi = p(ti). One\nsimple strategy is to set ti = i¬∑‚àÜtso that the points are equidistant, where ‚àÜ is a hyperparameter (e.g.,\n‚àÜ = 0.1). With this strategy, we are implicitly assuming the position signals evolve steadily as we go\nthrough each token in a sentence. In general, {ti}can be any monotonically increasing series, which\nallows us to extend our work to more applications where the elements in the sequence are not always\nobserved with the same interval. More discussions about the applicability for this general setting is\nincluded in the Supplementary material. For the NLP applications discussed in this paper, we choose\nti = i¬∑‚àÜt.\nEq. (8) is equivalent to an ODE problem dp(t)\ndt = h(t,p(t); Œ∏h), which is guaranteed to have a unique\nsolution under mild conditions [13]. We follow the efÔ¨Åcient approach by [11] to calculate the gradients\nof Œ∏h with respect to the overall training loss, which allows us to include this parameterized dynamical\nposition encoder into the end-to-end training of Transformer models. More details can be found in the\nSupplementary material.\nOur dynamical system (8) is quite Ô¨Çexible to admit the standard sinusoidal position encoding (6) as a\nspecial case:\npi+1[j] ‚àípi[j]\n=\nÔ£±\nÔ£≤\nÔ£≥\nsin\n(\n(i+ 1) ¬∑c\nj\nd\n)\n‚àísin\n(\ni¬∑c\nj\nd\n)\nif jis even\ncos\n(\n(i+ 1) ¬∑c\nj‚àí1\nd\n)\n‚àícos\n(\ni¬∑c\nj‚àí1\nd\n)\nif jis odd\n=\nÔ£±\nÔ£≤\nÔ£≥\n‚à´i+1\ni c‚àíj\nd cos(œÑ ¬∑c\nj\nd ) dœÑ if jis even‚à´i+1\ni ‚àíc‚àíj‚àí1\nd sin(œÑ ¬∑c\nj‚àí1\nd ) dœÑ if jis odd,\n(9)\n5\nThis indicates that for simple sinusoidal encoding, there exists a dynamical system h(¬∑) which is also\nsinusoidal function.\n3.2 Parameter Sharing among Blocks\nAs mentioned in Section 2, injecting position information to each block for Transformer leads to\nbetter performance [ 10, 5] in some language understanding tasks. Our proposed position encoder\nFLOATER(8) can also be injected into each block. The idea is illustrated in Figure 1. Typically there\nare 6 blocks in sequence-to-sequence Transformer and 12 or 24 blocks in BERT. We add a superscript\n(n) to denote dynamics at n-th block:\np(n)(t) = p(n)(s) +\n‚à´ t\ns\nh(n)(œÑ,p(n)(œÑ); Œ∏(n)\nh ) dœÑ.\nAs we can imagine, having N different dynamical models h(n)(¬∑; Œ∏(n)\nh ) for each block can introduce\ntoo many parameters and cause signiÔ¨Åcant training overhead. Instead, we address this issue by sharing\nparameters across all the blocks, namely\nŒ∏(1)\nh = Œ∏(2)\nh = ¬∑¬∑¬∑ = Œ∏(N)\nh . (10)\nNote that (10) does not imply that all the p(n)\nt are the same, as we will assign different initial values for\neach block, that is p(n1)(0) Ã∏= p(n2)(0) for n1 Ã∏= n2.\nTransformer-Base Transformer-Large\nEn-De En-Fr En-De En-Fr\nPosition encoders at all blocks\nFLOATER 28.6 41.6 29.2 42.7\nPre-deÔ¨Åned Sinusoidal Position Encoder 28.2 40.6 28.4 42.0\nFixed-length Position Embedding 26.9 40.9 28.3 42.0\nPosition encoder only at input block\nFLOATER 28.3 41.1 29.1 42.4\nPre-deÔ¨Åned Sinusoidal Position Encoder 27.9 40.4 28.4 41.8\nFixed-length Position Embedding 27.8 40.9 28.5 42.4\nTable 2: Experimental results of various position encoders on the machine translation task.\n3.3 Compatibility and Warm-start Training\nIn this section, we change the way to add position encoding so that our FLOATER can be directly\ninitialized from Transformer. As an example, we use the standard Transformer model, which has a\nÔ¨Åxed sinusoidal encoding at the input block and no position encoding at deeper levels. Note that this\ntechnique can be extended to other variants of Transformers with different position encoding methods,\n6\nsuch as embedding matrix. We Ô¨Årst examine the standard Transformer model, the query matrix Q(n) at\nblock-nis\nq‚àº(n)\ni = W(n)\nq\n(\nxi + p‚àº(n)\ni\n)\n+ b(n)\nq , (11)\nwhere W(n)\nq and b(n)\nq are parameters in An (3); p‚àº(n) is the sinusoidal encoding; q‚àº(n)\ni is the i-th row of\nQ(n). Here we add a tilde sign to indicate the sinusoidal vectors. Formulas for k‚àº(n)\ni and v‚àº(n)\ni have a\nvery similar form and are omitted for brevity.\nNow we consider the case of FLOATER, where new position encodingspi are added\nq(n)\ni = W(n)\nq\n(\nxi + pi\n)\n+ b(n)\nq\n= W(n)\nq (xi + p‚àº(n)\ni ) + b(n)\nqÓ¥ô Ó¥òÓ¥ó Ó¥ö\nEq. (11)\n+ W(n)\nq (pi ‚àíp‚àº(n)\ni )\nÓ¥ô Ó¥òÓ¥ó Ó¥ö\nExtra bias term depends on i\n= q‚àº(n)\ni + b(n)\nq,i .\n(12)\nIt is easy to see that the changing the position embedding from {p‚àº(n)\ni }to {p(n)\ni }is equivalent to adding\na position-aware bias vector b(n)\nq,i into each self-attentive layers {An(¬∑)}. As a result, we can instead\napply (8) to model the dynamics of b(n)\nq . In particular, we have the following dynamical system:\nb(n)\nq (t) = b(n)\nq (0) +\n‚à´ t\n0\nh(n)(œÑ,b(n)\nq (œÑ); Œ∏h) dœÑ. (13)\nAfter that, we set b(n)\nq,i = b(n)\nq (i¬∑‚àÜt). We can see that if h(¬∑) = 0 and b(n)\nq (0) = 0 , then b(n)\nq ‚â°0.\nThis implies (12) degenerates to (11). Note that (13) has the same form as (8), except that we are now\nmodeling the bias terms bq,i in (3). We will apply the same technique to Kand V.\nTo summarize, our model has a tight connection to the original Transformer: if we set all dynamical\nmodels to zero, which means h(œÑ,p(œÑ); Œ∏h) ‚â°0, then our FLOATER model will be equivalent to the\noriginal Transformer with the sinusoidal encoding. The same trick also works for Transformer with\nposition embedding such as BERT [2].\nWe strive to make our model compatible with the original Transformer due to the following reasons.\nFirst of all, the original Transformer is faster to train as it does not contain any recurrent computation;\nthis is in contrast to our dynamical model (8), where the next position pi+1 depends on the previous\none pi. By leveraging the compatibility of model architecture, we can directly initialize FLOATER\nmodel from a pre-trained Transformer model checkpoint and then Ô¨Åne-tune for the downstream task for\na few more epochs. By doing so, we enjoy all the beneÔ¨Åts of our FLOATER model but still maintain an\nacceptable training budget. Likewise, for models such as BERT or Transformer-XL, we already have\nwell-organized checkpoints out of the box for downstream tasks. These models are costly to train from\nscratch, and since our goal is to examine whether our proposed position representation method can\nimprove over the original one, we decided to copy the weights layer by layer for attention as well as\nFFN layers, and randomly initialize the dynamical model h(œÑ,p(œÑ); Œ∏h).\n4 Experimental Results\nIn this section, we perform experiments to see if FLOATER can improve over the existing position\nencoding approaches for a given Transformer model on various NLP tasks. Thus, all the metrics\n7\nreported in this paper are computed from a single (not ensemble) Transformer model over each\nevaluation NLP task. Albeit lower than top scores on the leaderboard, these metrics are able to reveal\nmore clear signal to judge the effectiveness of the proposed position encoder.\nAll our codes to perform experiments in this paper are based on the Transformer implementations in\nthe fairseq [14] package. Implementation details can be found in the Supplementary material. Our\nexperimental codes will be made publicly available.\nTable 3: Experimental results on GLUE benchmark\nModel Single Sentence Similarity and Paraphrase Natural Language Inference\nCoLA SST-2 MRPC QQP STS-B MNLI QNLI RTE\nBase model\nRoBERTa 63.6 94.8 88.2 91.9 91.2 87.6 92.8 78.7\nFLOATER 63.4 95.1 89.0 91.7 91.5 87.7 93.1 80.5\nLarge model\nRoBERTa 68.0 96.4 90.9 92.2 92.4 90.2 94.7 86.6\nFLOATER 69.0 96.7 91.4 92.2 92.5 90.4 94.8 87.0\nTable 4: Experiment results on RACE benchmark. ‚ÄúMiddle‚Äù means middle school level English exams,\n‚ÄúHigh‚Äù means high school exams. Other details can be found in [15].\nModel Accuracy Middle High\nSingle model on test, large model\nRoBERTa 82.8 86.5 81.3\nFLOATER 83.3 87.1 81.7\n4.1 Neural Machine Translation\nNeural Machine Translation (NMT) is the Ô¨Årst application that demonstrates the superiority of a\nsequence-to-sequence Transformer model over conventional recurrent sequence models. We include\nthe following three additive position encoders: Œ¶(n)(x) = x+ p(n).\n‚Ä¢Data-driven FLOATER: p(n) is generated by our proposed continuous dynamical models with\ndata-driven parameters described in (8).\n‚Ä¢Pre-deÔ¨Åned sinusoidal position encoder: p(n) is constructed by a pre-deÔ¨Åned function described\nin (7), which is proposed by [1] and extended by [10].\n‚Ä¢Length-Ô¨Åxed position embedding: p(n) is included as learnable training parameters. This is Ô¨Årst\nintroduced by [1] and adopted in many variants of Transformer [2, 8].\nTo better demonstrate the parameter efÔ¨Åciency brought by FLOATER, for each above encoder, we\nalso include two experimental settings: position encoder at all blocks or only at the input block (i.e.,\np(n) = 0,‚àÄn‚â•2).\n8\nIn Table 2, we present the BLEU scores on WMT14 Ee-De and En-Fr datasets with both Transformer-\nbase and Transformer-large models described in [1]. Among all the data/model combinations, our\nproposed FLOATER at all blocks outperforms two other position encoders.\nOn the other hand, we also observe that adding position encoders at all blocks yields better performance\nthan only at the input block. While there is an exception in the Ô¨Åxed-length position embedding\napproach. We suspect that this phenomenon is due to over-Ô¨Åtting cased by LmaxdN learnable param-\neters introduced by this approach. In contrast, our proposed FLOATER is parameter efÔ¨Åcient (more\ndiscussions in Section 4.3), so the performance can be improved by injecting the position encoder at all\nthe blocks of Transformer without much additional overhead.\n4.2 Language Understanding and Question Answering\nTable 5: Experiment results on SQuAD benchmark. All results are obtained from RoBERTa-large\nmodel.\nModel SQuAD 1.1 SQuAD 2.0\nEM F1 EM F1\nSingle models on dev, w/o data augmentation\nRoBERTa 88.9 94.6 86.5 89.4\nFLOATER 88.9 94.6 86.6 89.5\nPretrained Transformer models such as BERT and RoBERTa have become the key to achieving the\nstate-of-the-art performance for various language understanding and question answering tasks. In\nthis section, we want to evaluate the effectiveness of the proposed FLOATER on these tasks. In\nparticular, we focus on three language understanding benchmark sets, GLUE [ 16], RACE [15] and\nSQuAD [17]. As mentioned in Section 3.3, FLOATER is carefully designed to be compatible with\nthe existing Transformer models. Thus, we can utilize pretrained Transformer models to warm-start\na FLOATER model easily to be used to Ô¨Ånetune on these NLP tasks. In this paper, we download the\nsame pre-trained RoBERTa model from the ofÔ¨Åcial repository as our pretrained Transformer model\nfor all NLP tasks discussed in this section. GLUE Benchmark. This benchmark is commonly used\nto evaluate the language understanding skills of NLP models. Experimental results in Table 3 show\nthat our FLOATER model outperforms RoBERTa in most datasets, even though the only difference is\nthe choice of positional encoding. RACE benchmark Similar to the GLUE benchmark, the RACE\nbenchmark is another widely used test suit for language understanding. Compared with GLUE, each\nitem in RACE contains a signiÔ¨Åcantly longer context, which we believe requires more important to\ngrasp the accurate position information. Like in GLUE benchmark, we Ô¨Ånetune the model from the\nsame pretrained RoBERTa checkpoint. We keep the hyperparameters, such as batch size and learning\nrate, to also be the same. Table 4 shows the experimental results. We again see consistent improvement\nof FLOATER across all subtasks.\nSQuAD benchmark SQuAD benchmark [17, 18] is another challenging task to evaluate the question\nanswering skills of NLP models. In this dataset, each item contains a lengthy paragraph containing facts\nand several questions related to the paragraph. The model needs to predict the range of characters that\nanswer the questions. In SQuAD-v2, the problem becomes more challenging that the questions might\nbe unanswerable by the context. We follow the same data processing script as BERT/RoBERTa for fair\n9\n80‚â§x<100100‚â§x<120120‚â§x<140 x‚â•140\nSentence length\n15.0\n17.5\n20.0\n22.5\n25.0\n27.5\n30.0\nBLEU score\nSinusoidal\nEmbedding\nFLOATER\nFigure 2: Comparing BLEU scores of different encoding methods.\nTable 6: Performance comparison on WMT14 En-De data and Transformer-base architecture. Both\nBLUE scores and the number of trainable parameters inside each position encoder are included.\nBLEU (‚Üë) #Parameters ( ‚Üì)\nFLOATER 28.57 526.3K\n1-layer RNN + scalar 27.99 263.2K\n2-layer RNN + scalar 28.16 526.3K\n1-layer RNN + vector 27.99 1,050.0K\ncomparison; more details about the training process are described in the Supplementary material. The\nexperiment results are presented in Table 5. As we can see, the FLOATER model beats the baseline\nRoBERTa model consistently across most datasets. The improvement is signiÔ¨Åcant, considering that\nboth models are Ô¨Ånetuned from the same pretrained checkpoint.\n4.3 More Discussions and Analysis\nHow inductive is FLOATER? FLOATER is designed to be inductive by a data-driven dynamical\nmodel (8). To see how inductive FLOATER is when comparing to existing approaches, we design the\nfollowing experiment. We Ô¨Årst notice that in WMT14 En-De dataset, 98.6% of the training sentences\nare shorter than 80 tokens. Based on that, we make a new dataset called En-De short to long (or S2L\nfor brevity): this dataset takes all the short sentences (<80 tokens) as the training split and all the long\nsentences (‚â•80 tokens) as the testing split. We further divide the testing split to four bins according to\nthe source length fallen in [80,100), [100,120), [120,140), [140,+‚àû). BLEU scores are calculated\nin each bin, and the results are presented in Figure 2.\nOur FLOATER model performs particularly well on long sentences, even though only short sentences\nare seen by the model during training. This empirical observation supports our conjecture that\nFLOATER model is inductive: the dynamics learned from shorter sequences can be appropriately\ngeneralized to longer sequences.\nIs RNN a good alternative to model the dynamics? Recurrent neural network (RNN) is commonly\nused to perform sequential modeling. RNN and our continuous dynamical model (8) indeed share some\n10\n(a) Sinusoidal\nF eature dimension\nùëñ = 1\nùëñ = 252\nPosition (b) Position embedding\nF eature dimension\nùëñ = 1\nùëñ = 252\nPosition\nF eature dimension\nùëñ = 1\nùëñ = 252\nPosition\n(c) FLOATER\nF eature dimension\nùëñ = 1\nùëñ = 252\nPosition (d) RNN\nFigure 3: Visualizing the four different position methods. All models are trained using the Transformer-\nbase architecture and En-De dataset. For better visualization, dimension indices are permuted in\nFigure 3b-3d.\ncommonality. Computing the value at the i-th step relies on the results at the (i‚àí1)-st step. Further,\nthey all contain trainable parameters, allowing them to adapt to each particular task. Lastly, they can\nbe extrapolated to any length as needed. To see if RNN works equally well, we model the sequence\n{pi}i‚àà{1,2,...}with RNN models:\npi+1 = RNN(zi,pi), (14)\nwhere zi ‚ààRdin is the input to the RNN model at index i. Recall in RNN language models, zi is\nthe word embedding or hidden feature of the i-th token. In our case, since we apply RNN to learn\nthe encodings as opposed to hidden features, sensible inputs can be scalar value ior vectorized value\nVectorize(i) by sinusoidal encoding. We tried both choices on WMT14 En-De data and found that\nvectorized value generally works better, though not as good as our FLOATER model. Detailed results\ncan be found in Table 6.\nWhat does each position encoding look like? To better understand how different position encod-\nings affect the sequence modeling, in Figure 3, we visualize the position embedding matrix pobtained\nfrom four different position encoding approaches for the Transformer-base backbone on WMT14 En-De\ndataset. We can see that sinusoidal encoding (3a) is the most structural, while position embedding\n(3b) is quite chaotic. Our FLOATER model learns position representation completely from data, but\nstill exhibits some regularities (3c). Finally, the RNN model (3d) fails to extract sufÔ¨Åcient positional\ninformation, probably due to the vanishing gradient problem. Another Ô¨Ånding is that by looking at\n(3b), we observe that the vectors are nearly constant among different large positions (near the bottom\nof Figure 3b, we see patterns of vertical lines with the same color). This phenomenon is due to long\nsentences in the dataset being scarce, and so the positional information carried by lower indices cannot\nbe extrapolated to higher indices. On the contrary, the dynamical model proposed in this paper enjoys\n11\nthe best of both worlds ‚Äì it is adaptive to dataset distribution, and it is inductive to handle sequences\nwith lengths longer than the training split.\n4.4 Remarks on Training and Testing EfÔ¨Åciency\nIt is not surprising that during the training time, our Ô¨Çow-based method adds a non-negligible time and\nmemory overhead; this is because solving the Neural ODE precisely involves ‚àº100 times forward and\nbackward propagations of the Ô¨Çow model. Even though we deliberately designed a small Ô¨Çow model\n(consisting of only two FFN and one nonlinearity layers), stacking them together still increases training\ntime substantially. To make it possible to train big models, we use the following optimizations:\n‚Ä¢Initialize with pretrained models that do not contain Ô¨Çow-based dynamics, as discussed in Section 3.3.\n‚Ä¢From (8), we know that if h(¬∑) is close to zero, then the position information diminishes (derived in\nappendix). In this way, our model degenerates to the original Transformer. Inspired by this property,\nwe can initialize the FLOATER with smaller weights. Combining with the previous trick, we obtain\nan informed initialization that incurs lower training loss at the beginning.\n‚Ä¢We observed that weights in h(¬∑) are more stable and easy to train. Thus, we can separate the\nweights of h(¬∑) from the remaining parts of the Transformer model. Concretely, we can 1) cache\nthe positional bias vectors for some iterations without re-computing, 2) update the weights of Ô¨Çow\nmodels less frequently than other parts of the Transformer, and 3) update the Ô¨Çow models with a\nlarger learning rate to accelerate convergence.\n‚Ä¢For the RoBERTa model, we adopt an even more straightforward strategy: we Ô¨Årst download a\npretrained RoBERTa model, plug in some Ô¨Çow-based encoding layers, and re-train the encoding\nlayers on WikiText-103 dataset for one epoch. When Ô¨Ånetuning on GLUE datasets, we can choose\nto freeze the encoding layers.\nCombining those tricks, we successfully train our proposed models with only 20-30% overhead\ncompared to traditional models, and virtually no overhead when Ô¨Ånetuning RoBERTa model on GLUE\nbenchmarks. Moreover, there is no overhead during the inference stage if we store the pre-calculated\npositional bias vectors in the checkpoints.\n5 Conclusions\nIn this paper, we have shown that learning position encoding with a dynamical model can be an\nadvantageous approach to improve Transformer models. Our proposed position encoding approach\nis inductive, data-driven, and parameter efÔ¨Åcient. We have also demonstrated the superiority of our\nproposed model over existing position encoding approaches on various natural language processing\ntasks such as neural machine translation, language understanding, and question answering tasks.\nReferences\n[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information\nProcessing Systems, pages 5998‚Äì6008, 2017.\n[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n12\n[3] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V\nLe. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint\narXiv:1906.08237, 2019.\n[4] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. 2019.\n[5] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu\nSoricut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint\narXiv:1909.11942, 2019.\n[6] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniÔ¨Åed\ntext-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.\n[7] Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar.\nAre transformers universal approximators of sequence-to-sequence functions? arXiv preprint\narXiv:1912.10077, 2019.\n[8] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\n[9] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position represen-\ntations. In Proceedings of the 2018 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages\n464‚Äì468, 2018.\n[10] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and≈Åukasz Kaiser. Universal\ntransformers. arXiv preprint arXiv:1807.03819, 2018.\n[11] Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary\ndifferential equations. In Advances in Neural Information Processing Systems, pages 6571‚Äì6583,\n2018.\n[12] Will Grathwohl, Ricky TQ Chen, Jesse Betterncourt, Ilya Sutskever, and David Duvenaud.\nFfjord: Free-form continuous dynamics for scalable reversible generative models. arXiv preprint\narXiv:1810.01367, 2018.\n[13] M. Tenenbaum and H. Pollard. Ordinary Differential Equations: An Elementary Textbook for\nStudents of Mathematics, Engineering, and the Sciences. Dover Books on Mathematics. Dover\nPublications, 1985.\n[14] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,\nand Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. arXiv preprint\narXiv:1904.01038, 2019.\n[15] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading\ncomprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017.\n13\n[16] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\nGlue: A multi-task benchmark and analysis platform for natural language understanding. arXiv\npreprint arXiv:1804.07461, 2018.\n[17] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.\n[18] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don‚Äôt know: Unanswerable\nquestions for squad. arXiv preprint arXiv:1806.03822, 2018.\n[19] William H Press, Saul A Teukolsky, William T Vetterling, and Brian P Flannery. Numerical\nrecipes in c++. The art of scientiÔ¨Åc computing, 2:1002, 1992.\n[20] Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation.\narXiv preprint arXiv:1806.00187, 2018.\n[21] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\nmodels. arXiv preprint arXiv:1609.07843, 2016.\n[22] Yang Liu and Mirella Lapata. Hierarchical transformers for multi-document summarization.\narXiv preprint arXiv:1905.13164, 2019.\n[23] Xingxing Zhang, Furu Wei, and Ming Zhou. HIBERT: document level pre-training of hierarchical\nbidirectional transformers for document summarization. CoRR, abs/1905.06566, 2019.\n[24] Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary\ndifferential equations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,\nand R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 6571‚Äì6583.\nCurran Associates, Inc., 2018.\nA Training a Neural ODE model in Transformer\nWe discuss the details of training the dynamical model h(œÑ,pœÑ; wh), recall in our FLOWER model,\nfunction hjoins in the computational graph implicitly by generating a sequence of position encoding\nvectors {p1,p2,..., pN}, conditioning on a freely initialized vector p0. The generation steps are\ncomputed iteratively as follows (suppose we choose the interval between two consecutive tokens to be\n‚àÜ)\np1 = p0 +\n‚à´ ‚àÜ\n0\nh(œÑ,pœÑ; wh) dœÑ,\np2 = p1 +\n‚à´ 2‚àÜ\n‚àÜ\nh(œÑ,pœÑ; wh) dœÑ,\n...\npN = pN‚àí1 +\n‚à´ N‚àÜ\n(N‚àí1)‚àÜ\nh(œÑ,pœÑ; wh) dœÑ.\n(15)\nFinally, the loss Lof this sequence is going to be a function of all position encoding results L =\nL(p0,p1,..., pN), which is further a function of model parameters wh. The question is how to\n14\ncalculate the gradient dL\ndwh\nthrough backpropagation. This question is fully solved in Neural ODE\nmethod [11] with an efÔ¨Åcient adjoint ODE solver. To illustrate the principle, we draw a diagram\nshowing the forward and backward propagation in Figure 4.\nps pt\nL= L(p0 ...p N) Forward\nBackward\n+\n‚à´ t\ns h(œÑ,pœÑ; wh)dœÑ\nFigure 4: Direction of forward and backward propagation. Here we consider a simpliÔ¨Åed version where\nonly position encodings ps and pt are in the computational graph.\nFrom [11], we know that the gradients d\ndwh\nL\n(\nps +\n‚à´t\ns h(œÑ,pœÑ; wh) dœÑ\n)\n‚âú dL\ndwh\ncan be computed by\ndL\ndwh\n= ‚àí\n‚à´ s\nt\na(œÑ)‚ä∫ ‚àÇh(œÑ,pœÑ; wh)\n‚àÇwh\ndœÑ, (16)\nwhere a(œÑ) deÔ¨Åned in œÑ ‚àà[s,t] is called the ‚Äúadjoint state‚Äù of ODE, which can be computed by solving\nanother ODE\nda(œÑ)\ndœÑ = ‚àía(œÑ)‚ä∫ ‚àÇh(œÑ,pœÑ; wh)\n‚àÇpœÑ\n. (17)\nNote that the computation of (17) only involves Jacobian-vector product so it can be efÔ¨Åciently\ncalculated by automatic differentiation.\nB Implementation details\nB.1 Settings of ODE solver\nTo setup the ODE server, we need to Ô¨Årst choose the numerical algorithms [ 19]. We have different\nsetups for different datasets. For neural machine translation problems (WMT14 En-De and En-Fr), we\nuse the more accurate Runge-Kutta scheme with discretization step ‚àÜ\n5.0 to solve the adjoint equation\n(recall that we set the interval of two neighboring tokens to be ‚àÜ = 0.1 globally). While for datasets\nwith long sentences such as GLUE and RACE benchmarks, we found that solving the adjoint equation\nwith high order scheme is too slow, in such case we adopt simple midpoint method with discretization\nstep ‚àÜ\n5.0 , and the gradients are calculated by automatic differentiation rather than adjoint method. The\nthird party implementation of ODE solver can be found at https://github.com/rtqichen/\ntorchdiffeq.\nB.2 Training NMT tasks\nWe run the same preprocessing script provided by fairseq [14], which is also used in ScalingNMT [20].\nWith the standard training script, we Ô¨Årst successfully reproduce all the results in Transformer paper [1].\nBased on that we execute the following protocol to get our results:\n15\n1. Train the original Transformer model for 30 epochs.\n2. Random initialize FLOWER model of same shape conÔ¨Åguration.\n3. Copy tensors from the best performing checkpoint (validation set) to initialize FLOWER model.\nInitialize weights in the dynamical model with small values.\n4. Half the peak learning rate (e.g. in Transformer-base + En-De, the peak learning rate is changed\nfrom 7.0 √ó10‚àí4 to 3.5 √ó10‚àí4).\n5. With the warm-initialized FLOWER checkpoint, retrain on the same dataset for 10 epochs\n(En-De) or 1 epoch (En-Fr).\n6. Averaging last 5 checkpoints and compute BLEU score on test split.\nB.3 Training language understanding tasks\nFor GLUE/SQuAD/RACE benchmarks, our experiments are all conducted upon RoBERTa, in which\nboth base and large conÔ¨Ågurations are available. Due to resource constraint (and to show the\ncompatibility to existing models), we initialize our FLOWER model with pretrained RoBERTa, which\nis similar to NMT task. However, the weights wh in dynamic function h(œÑ,pœÑ; wh) are not trained in\nlarge corpus, given that GLUE/SQuAD/RACE datasets are too small to train dynamics from scratch,\nwe decided to pretrain halone in WikiText103 [21] data using masked language modeling loss. We\nhave found that when we train wh alone, it only takes a few hours (2x Titan V100) and one epoch to\nconvergence.\nOnce having the pretrained FLOWER model, we can run following downstream tasks and compare\nwith RoBERTa under the same setting:\nGLUE benchmark consists of eight datasets and each have different hyperparameter settings. For\nhyperparameters such as learning rate, batch size, training iterations, warm-up iterations, etc., we use\nthe same values recommended by ofÔ¨Åcial repository of RoBERTa1.\nSQuAD benchmark. For this benchmark we wrote our own Ô¨Ånetuning code because currently there\nis no ofÔ¨Åcial code available. During the implementation process, we mainly refer to the third-party\nrepositories2. We are not able to exactly match the ofÔ¨Åcial result reported in RoBERTa paper but\nquite close (‚àº0.1 difference in F1). For our FLOWER model, we use the same hyperparameters as\nRoBERTa.\nRACE benchmark. This benchmark has the longest context and sequence length. We follow the\nofÔ¨Åcial training script 3 and reproduce the result. Similar to other benchmarks, we then repeat the\ntraining process using exactly the same training hyperparameters to make a fair comparison. In this\nbenchmark we freeze the weights wh and only Ô¨Ånetune the weights of RoBERTa.\n1Available at: https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.\nglue.md\n2 Mainly https://github.com/ecchochan/roberta-squad and https://github.com/\nhuggingface/transformers\n3https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.race.md\n16\nC Cases suitable for non-equidistant discritization\nAlthough our model allows continuous values ofsand tin (8), limiting the scope to text modeling tasks,\npositions are discrete values as {0,1,2,... }. Once the continuous version of position representation\npt is obtained, we simply take the discritized {p0,p‚àÜ,p2‚àÜ,..., }as the actual values to feed into\nTransformer model, where ‚àÜ is a hyperparameter (e.g. ‚àÜ = 0.1). By choosing positions tequidistantly,\nwe are implicitly assuming the position signal evolves steadily as we go through each token in a\nsentence. More generally, the dynamics in (8) can deal with the case in which positions are not integers\n0,1,2,... etc., but arbitrary monotone increasing series t0 < t1 < t2 < ... which may not be\nequidistant. In appendix, we exemplify this general situation with several widely deployed tasks; we\nregard this as a interesting future direction. This makes our model particularly suitable for following\nscenarios yet traditional position representation may not be good at:\n‚Ä¢Hierarchical Transformer model[22, 23]. The model is a direct extension of hierarchical RNN and\nis often used in long document processing. It works by Ô¨Årst running a word-level Transformer model\non each sentence to extract the sentence embedding, and then applying a sentence-level Transformer\nscanning through each sentence embedding sequentially. We argue that when processing at the\nsentence level, it could be better to set the increment of position index ti+1 ‚àíti proportional to the\nlength of the i-th sentence. This is because longer sentences tend to carry more information, so pi+1\nis likely to move farther from pi.\n‚Ä¢Transformer for time-series events. As measurement time is continuous, time-series data is another\nscenario when a continuous position makes more sense than a discrete counterpart. More importantly,\nto predict the future values by modeling historical values observed at irregular time grids, it is better\nto consider the length of time horizon between two consecutive measures. A successful previous\nwork is the Latent ODE [24], except that they use RNN as the backbone, and they model the hidden\nstates rather than position representations with Neural ODE (because RNN itself provides positional\nbias).\nIn this paper, we are not going to explore the more general cases discussed above. Instead, we decided\nto leave them as interesting future work.\n17"
}