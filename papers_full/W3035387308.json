{
  "title": "Gated POS-Level Language Model for Authorship Verification",
  "url": "https://openalex.org/W3035387308",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5012881260",
      "name": "Linshu Ouyang",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Information Engineering",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5100715816",
      "name": "Yongzheng Zhang",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Information Engineering",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5019440799",
      "name": "Liu Hui",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Information Engineering",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5040565656",
      "name": "Yige Chen",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Information Engineering",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5100692710",
      "name": "Yipeng Wang",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Information Engineering",
        "University of Chinese Academy of Sciences"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2523437799",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W2742157926",
    "https://openalex.org/W2790418610",
    "https://openalex.org/W2187238335",
    "https://openalex.org/W2794340419",
    "https://openalex.org/W2065218635",
    "https://openalex.org/W4213009331",
    "https://openalex.org/W2150355110",
    "https://openalex.org/W2899024931",
    "https://openalex.org/W1607829561",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2399965576",
    "https://openalex.org/W2963462252",
    "https://openalex.org/W1974927094",
    "https://openalex.org/W2949360559",
    "https://openalex.org/W2621340964",
    "https://openalex.org/W1810943226",
    "https://openalex.org/W2949387767",
    "https://openalex.org/W4250089123"
  ],
  "abstract": "Authorship verification is an important problem that has many applications. The state-of-the-art deep authorship verification methods typically leverage character-level language models to encode author-specific writing styles. However, they often fail to capture syntactic level patterns, leading to sub-optimal accuracy in cross-topic scenarios. Also, due to imperfect cross-author parameter sharing, it's difficult for them to distinguish author-specific writing style from common patterns, leading to data-inefficient learning. This paper introduces a novel POS-level (Part of Speech) gated RNN based language model to effectively learn the author-specific syntactic styles. The author-agnostic syntactic information obtained from the POS tagger pre-trained on large external datasets greatly reduces the number of effective parameters of our model, enabling the model to learn accurate author-specific syntactic styles with limited training data. We also utilize a gated architecture to learn the common syntactic writing styles with a small set of shared parameters and let the author-specific parameters focus on each author's special syntactic styles. Extensive experimental results show that our method achieves significantly better accuracy than state-of-the-art competing methods, especially in cross-topic scenarios (over 5\\% in terms of AUC-ROC).",
  "full_text": "Gated POS-Level Language Model for Authorship VeriÔ¨Åcation\nLinshu Ouyang , Yongzheng Zhang\u0003 , Hui Liu , Yige Chen and Yipeng Wang\nInstitute of Information Engineering, Chinese Academy of Sciences, Beijing, China\nSchool of Cyber Security, University of Chinese Academy of Sciences, Beijing, China\nfouyanglinshu, zhangyongzheng, liuhui, chenyige, wangyipengg@iie.ac.cn,\nAbstract\nAuthorship veriÔ¨Åcation is an important problem\nthat has many applications. The state-of-the-\nart deep authorship veriÔ¨Åcation methods typically\nleverage character-level language models to encode\nauthor-speciÔ¨Åc writing styles. However, they of-\nten fail to capture syntactic level patterns, lead-\ning to sub-optimal accuracy in cross-topic scenar-\nios. Also, due to imperfect cross-author parameter\nsharing, it‚Äôs difÔ¨Åcult for them to distinguish author-\nspeciÔ¨Åc writing style from common patterns, lead-\ning to data-inefÔ¨Åcient learning.\nThis paper introduces a novel POS-level (Part of\nSpeech) gated RNN based language model to ef-\nfectively learn the author-speciÔ¨Åc syntactic styles.\nThe author-agnostic syntactic information obtained\nfrom the POS tagger pre-trained on large exter-\nnal datasets greatly reduces the number of effec-\ntive parameters of our model, enabling the model to\nlearn accurate author-speciÔ¨Åc syntactic styles with\nlimited training data. We also utilize a gated ar-\nchitecture to learn the common syntactic writing\nstyles with a small set of shared parameters and\nlet the author-speciÔ¨Åc parameters focus on each au-\nthor‚Äôs special syntactic styles. Extensive experi-\nmental results show that our method achieves sig-\nniÔ¨Åcantly better accuracy than state-of-the-art com-\npeting methods, especially in cross-topic scenarios\n(over 5% in terms of AUC-ROC).\n1 Introduction\nWith a set of documents written by a known author, how can\nwe determine whether another document is written by this au-\nthor? This is the authorship veriÔ¨Åcation problem [Koppel and\nSchler, 2004], which has important applications in broad do-\nmains, e.g., to detect fraud or phish in cybersecurity, to detect\nplagiarism in research, and to assist forensic investigation for\nthe judiciary.\nNumerous authorship veriÔ¨Åcation methods have been in-\ntroduced to tackle this problem [Stamatatos, 2009]. Most of\nthese approaches focus on Ô¨Ånding features that can effectively\n\u0003Contact Author\ncharacterize writing styles [Abbasi and Chen, 2008 ]. Char-\nacter level tri-grams is shown to be effective for authorship\nanalysis [Bevendorff et al., 2019b]. There are also works try-\ning to address this problem from the perspective of compres-\nsion [Khmelev and Teahan, 2003; Halvani et al., 2017 ] and\nthe distribution of function words [Koppel and Schler, 2004].\n[Zhao et al., 2006] demonstrates that the POS tag is effective\nto characterize the writing styles from the perspective of syn-\ntax. However, they only use simple distributions to represent\nthe pattern of each author, which is insufÔ¨Åcient to capture the\nlocal correlations in the sequence.\nRecently, deep learning has achieved exceptional successes\nin many natural language processing problems. However,\napplying deep learning on the authorship veriÔ¨Åcation prob-\nlem faces a major challenge: the amount of training texts\nfor each author is extremely limited, which poses signiÔ¨Åcant\nchallenges to the training of neural networks that have a large\nnumber of parameters.\n[Bagnall, 2015] is one of the few in applying deep learn-\ning to the authorship veriÔ¨Åcation problem. They attempt to\naddress the above challenge by building a character-level lan-\nguage model with a shared RNN layer and a separate output\nlayer for each author to encode the author-speciÔ¨Åc writing\nstyles. Their method sits at the Ô¨Årst place in the PAN-15 au-\nthorship veriÔ¨Åcation competition and demonstrates the great\npotential of deep learning for authorship veriÔ¨Åcation. How-\never, we Ô¨Ånd experimentally that character-level RNN has\ndifÔ¨Åculties in learning syntactic information with a limited\nnumber of samples. In addition, although their model uses\na shared RNN layer, the separate fully-connected output lay-\ners for each author have to learn from scratch independently,\nwhich may lead to sub-optimal language models. This also\nmakes it difÔ¨Åcult for their model to distinguish author-speciÔ¨Åc\nwriting styles from common writing patterns shared by every-\none.\nIn this paper, we introduce a novel POS-level gated lan-\nguage model to capture the author-speciÔ¨Åc syntactic writing\nstyles in a data-efÔ¨Åcient way. As shown in Figure 2, we uti-\nlize a POS tagger pre-trained on a large external data set to\nconvert the raw inputs into sequences of POS tokens. The ex-\nternal author-agnostic syntactic information comes with the\nPOS tagger limits the parameter space of our language model\nto the vicinity of common syntactic patterns. This greatly re-\nduces the number of the effective parameters and enables the\nProceedings of the Twenty-Ninth International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-20)\n4025\nKnown Author Unknown Author Same?\nA   beautiful   lake   lies   at   the   foot   of   the   hill   . At   the   foot   of   the   hill   lies   a   beautiful   lake   .\nNO\nDET             ADJ                  NOUN       VERB     ADP      DET        NOUN      ADP      DET       NOUN PUNCT ADP      DET        NOUN      ADP      DET       NOUN     VERB     DET            ADJ                 NOUN   PUNCT\nThis   is   the   second   article   that   I   have   written   . It   is   the   best   film   that   he   has   ever   seen   .\nYES\nDET      AUX     DET              ADJ                 NOUN             DET     PRON    AUX               VERB        PUNCT PRON AUX    DET          ADJ          NOUN      SCONJ      PRON    AUX          ADV           VERB   PUNCT\nFigure 1: Two toy authorship veriÔ¨Åcation cases. Note that they are unrealistically simpliÔ¨Åed since typically it requires at least a few hundred\nwords to decently characterize an author‚Äôs writing style. The two sentences in the Ô¨Årst case express similar meanings, but their syntactic styles\nare completely different. In the second case, the syntactic styles of the two sentences are similar, but the topics are different. The PoS tagset\nused here is the universal PoS tagset.\nmodel to focus on the details of each author‚Äôs special syntac-\ntic styles.\nAlso, we use a shared fully-connected layer and a set of\nindependent fully-connected layers to respectively learn the\nshared syntactic patterns and each author‚Äôs special syntactic\nwriting styles. A gate unit shared between authors is respon-\nsible for deciding whether to use author-speciÔ¨Åc information\nat different positions of the texts. The rationale behind this\ndesign is that we assume that only a small part of the patterns\nappeared in the POS sequences comes from author-speciÔ¨Åc\nwriting styles, while most patterns come from the standard\nsyntax. This structure has two major beneÔ¨Åts: (i) the gate\ncan prevent the model from overÔ¨Åtting to each author‚Äôs rather\nlimited training data since it makes the decision based on the\nentire training set, and (ii) learning common syntactic pat-\nterns with a small set of shared parameters makes the lan-\nguage model more data-efÔ¨Åcient and accurate.\nTo summarize, our main contributions are as follows:\n\u000fWe propose a novel POS-level language model to effec-\ntively encode the topic-agnostic syntactic writing style\nfor each author. As far as we know, we are the Ô¨Årst to\nconduct authorship veriÔ¨Åcation by capturing the author-\nspeciÔ¨Åc correlations in the sequences of POS tokens\nwith language models.\n\u000fWe propose a novel gated architecture to learn the\nauthor-speciÔ¨Åc language models by utilizing a relatively\nsmall group of parameters to learn the common syntac-\ntic patterns shared across all authors. The model is data-\nefÔ¨Åcient and less prone to overÔ¨Åtting.\n\u000fExtensive empirical results on four publicly available\ndatasets demonstrate that our model can effectively learn\neach author‚Äôs unique syntactic writing style and achieve\nsigniÔ¨Åcant accuracy improvements compared to other\nstate-of-the-art methods, especially in cross-topic sce-\nnarios (over 5% in terms of AUC-ROC).\n2 Preliminaries\n2.1 Problem Statement\nSuppose there is a group of authorsA, for each authorai 2A\nwe have a small set of texts Si = fsi\n1;si\n2;\u0001\u0001\u0001 ;si\nkgthat is\nknown to be written by ai. Now there is also a text of un-\nknown author denoted as qt, the purpose of authorship ver-\niÔ¨Åcation is to determine whether qt is written by author ai.\nfai;qtgis called an authorship veriÔ¨Åcation case. Figure 1\nshows two toy cases.\n2.2 Language Model for Authorship VeriÔ¨Åcation\nThe core idea of neural language model [Bengio et al., 2003]\nis learning a neural network to model the conditional prob-\nability P(wijw0;\u0001\u0001\u0001 ;wi\u00001) for each sequence w0;\u0001\u0001\u0001 ;wN .\nw0;\u0001\u0001\u0001 ;wi\u00001 is called context and wi is a token that is ei-\nther a character or a word. Typically, this model consists of\ntwo components: a recurrent neural network unitfsynthesize\nthe correlations from the context into a hidden representation\nhi = f(hi\u00001;wi\u00001); and a fully-connected decoding layer g\nto map the hidden representation to the probability distribu-\ntion of next token: P(wijw0;\u0001\u0001\u0001 ;wi\u00001) =g(hi).\nTo conduct authorship veriÔ¨Åcation, a set of author-speciÔ¨Åc\nlanguage models can be learned for each author respectively.\nEach author‚Äôs special writing habits will lead to differences\nin learned conditional probability P(wijw0;\u0001\u0001\u0001 ;wi\u00001), and\na text is expected to have high probability with respect to its\nsincere author‚Äôs language model.\n3 Methods\nFigure 2 gives an overview of the gated POS-level author-\nspeciÔ¨Åc language model. First, we convert the raw texts into\nthe sequences of POS tokens. These sequences are then em-\nbedded and fed into a vanilla RNN shared by all authors to\ncapture the context information. On top of that, we apply\ngated decoding to predict the next token in the sequence.\n3.1 POS-Level Language Model\nInstead of the traditional character-level language model, we\npropose to use a novel POS-level language model to learn\nauthor-speciÔ¨Åc syntactic writing styles. SpeciÔ¨Åcally, we uti-\nlize a POS tagger pre-trained on a large external data set\n[Matthew and Ines, 2015; Qi et al., 2018] to convert the raw\ntexts into sequences of POS tokens. Then we learn author-\nspeciÔ¨Åc language models upon these sequences, that is, pre-\ndict the next POS token based on previously observed POS\nsequence. The POS-level language model can effectively\ncapture the syntactic writing styles with limited training data\nwhile the character-level language model often fails to do so.\nWhy Is Syntactic Writing Style Important for\nAuthorship VeriÔ¨Åcation?\nThe syntactic writing style is important because it is topic-\nagnostic. Consider the two toy examples in Figure 1. The\nProceedings of the Twenty-Ninth International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-20)\n4026\ntwo sentences in the Ô¨Årst case convey similar meaning with\nexactly the same words, but the arrangement of their POS to-\nken sequences are completely different. For the second case,\nthe two sentences have different topics but similar POS token\nsequences. It is evident that the syntactic writing style repre-\nsented by the arrangement pattern of POS token sequences is\nmore generalizable for authorship veriÔ¨Åcation in cross-topic\nscenarios.\nWhy Can POS Level Language Model Better Learn\nSyntactic Writing Style?\nThe language model essentially models the joint probability\ndistribution of a sequence of discrete random variables. This\ndistribution has an exponentially sized sample space, thus the\nlanguage model will need a large amount of training data to\nexplore this huge sample space and accurately Ô¨Åt the distribu-\ntion. However, in the authorship veriÔ¨Åcation problem, there\nare typically not enough training data to do this. As a result,\nthe char-level language model is likely to overÔ¨Åt to some spe-\ncial local correlations in the training set.\nTo address this problem, a potential solution is to lever-\nage external knowledge to constraints the size of the sample\nspace. We can consider that there is a subspace consisting\nof all the ‚Äùcorrect‚Äù sentences obeying the common syntac-\ntic rules in the sample space. Each author will have a dif-\nferent probability distribution in this subspace according to\ntheir syntactic writing style. With this intuition, we lever-\nage the general author-independent syntactic rules obtained\nfrom POS taggers trained on external large-scale data sets to\nlimit the sample space of the language model to the vicin-\nity of ‚Äùcorrect‚Äù sentences, enabling the author-speciÔ¨Åc lan-\nguage models to focus on each author‚Äôs subtle syntactic style.\nFrom another perspective, the vocabulary size of the POS is\nonly 19, compared to the character (80) and the word (sev-\neral thousand). This greatly reduces the number of effec-\ntive parameters of the model while retaining its expressive\npower. Besides POS tags, we also explored the possibility of\nusing more explicit syntactic information obtained from de-\npendency parsing. However, it performs consistently worse\nthan POS tags.\n3.2 Gated Neural Language Model\nGiven a sequence of POS tokens P = fwtgm\nt=1 known to\nbe written by author ai 2A, we Ô¨Årst convert them to em-\nbeddings fetgm\nt=1 with a embedding layer. Then we generate\nthe representation of context fhtgm\nt=1 utilizing a vanilla RNN\nunit to capture the correlations in the POS sequence:\nht = RNN(ht\u00001;et\u00001) (1)\nThe parameters of the embedding layer and the RNN are\nshared between all authors. We choose vanilla RNN rather\nthan more powerful variance such as LSTM because we Ô¨Ånd\nexperimentally that LSTM typically provides rather limited\nperformance improvements. We speculate that this might be\ndue to the limited training data which prevents the LSTM to\nlearn meaningful long-range correlations.\nThen, the representation of context is fed into a decoder to\ngenerate the estimation of the distribution over the next token.\nThis decoder consists of three parts:\n√ó\nADP    DET     NOUN     ADP    DET\nAt   the  foot   of   the\nPOS tagger\nthe   foot   of   the   hill\nDET      NOUN     ADP    DET     NOUN\nEmbedding layer\n1‚àíùúé(‚àô) ùúé(‚àô)\nRNN layer\nFully connected \ndecoder\nGate structure\nCross entropy loss\nbetween prediction\nand next POS token\nSoftmaxAuthor-\nspecific\nShared\nbetween\nauthors\n√ó\n+\nFigure 2: Gated neural language model structure overview.\n(i) A shared fully-connected layer with parameters\nfWshare;bsharegto learn the common syntactic writ-\ning patterns among all authors:\nsshare\nt = Wshareht + bshare (2)\n(ii) A separate fully-connected layer with parameters\nfWai ;bai gfor each author ai 2 A to learn special\nsyntactic writing styles:\nsai\nt = Wai ht + bai (3)\n(iii) The Ô¨Ånal output is the linear weighted average of the\noutputs of the previous two parts performed by a shared\ngate unit parameterized by fWgate;bgateg:\nsgate\nt = \u001b(Wgateht + bgate) (4)\nlogitsai\nt = sgate\nt \nsshare\nt + (1\u0000sgate\nt ) \nsai\nt (5)\nFinally, a softmax function is applied to the raw logits out-\nput to obtain the author-speciÔ¨Åc estimated distribution of the\nnext token:\n^P\nai\n(wtjfwkgt\u00001\nk=1) =softmax (logitsai\nt ) (6)\nTo train the network, we minimize the sum of the cross-\nentropy loss between the estimated distribution and the\nground-truth next token on every step t.\nProceedings of the Twenty-Ninth International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-20)\n4027\nDataset Genre #Train #Test Char Level Length Word Level Length\nMin. Max. Avg. Min. Max. Avg.\nPAN-15 Dialog in plays 100 500 1304 5024 2566 283 1407 638\nGutenberg Fiction 182 80 5783 25972 21835 1158 5889 4809\nEnron Email 16 64 1894 5428 3958 378 1108 797\nReddit Social news 200 800 4989 7496 6978 356 1958 1485\nTable 1: The statistics of the four publicly available datasets used in the experiments.\nWhy Does Our Proposed Architecture Learn\nAuthor-speciÔ¨Åc Syntactic Writing Style More\nAccurately?\nThe assumption behind this design choice is that there is a\nlarge intersection between the writing patterns of the authors,\nand only a small part of special writing patterns can be uti-\nlized to distinguish these authors. With this architecture, we\nlearn the shared writing patterns with a small set of shared\nparameters, and the author-speciÔ¨Åc parameters can focus on\neach author‚Äôs special writing styles.\nAlso, note that the parameters of the gate are shared, thus\nthe ratio to include author-speciÔ¨Åc information at each step\nof the text sequence is determined by all training data. As\na result, it will be less likely for the model to overÔ¨Åt to the\ntraining data of each author.\n3.3 VeriÔ¨Åcation\nAfter learning a language model for each known author re-\nspectively, to conduct authorship veriÔ¨Åcation on a test case\nfai;fwtgm\nt=1g, we need to calculate the losses z = fzai jai 2\nAgof this text with respect to the language model of each\nauthor respectively:\nzai = L (ai;fwtgm\nt=1) (7)\n=\nmX\nt=1\n\u0010\n^P\nai\n(wtjfwkgt\u00001\nk=1);wt\n\u0011\n(8)\nThen, we normalize these losses:\nscore = zai \u0000mean(z)\nstd(z) (9)\nThis normalized loss for ai can be used as a score for clas-\nsiÔ¨Åcation. The smaller the loss, the more likely this text is\nwritten by the author ai.\nThe rationale behind this choice is that due to the inherent\nvariability of neural language models, it‚Äôs difÔ¨Åcult to directly\ncompare the losses of different texts with respect to the same\nlanguage model. Instead, we can only compare the losses\nof the same text with respect to different author‚Äôs language\nmodels.\n4 Experiments\nIn this section, we conduct extensive experiments to evalu-\nate the proposed method. First, we introduce the settings of\nthe experiments, including the datasets used for evaluation,\nthe baseline methods to compare, the implementation details\nof our method, and the metrics. Then we compare the per-\nformance of our proposed method with other state-of-the-art\nauthorship veriÔ¨Åcation methods. Finally, we conduct ablation\nstudies to evaluate the contribution of each component of our\nmodel.\n4.1 Data Sets\nAs shown in the Table 1, we used 4 publicly available author-\nship veriÔ¨Åcation datasets, which were widely used by previ-\nous studies [Halvani et al., 2017; Bevendorff et al., 2019a;\nBagnall, 2015; Halvani et al., 2018], with different genre and\nsizes. Each case in these datasets consists of exactly one doc-\nument of unknown authorship and at least one document from\na known author. PAN-15 comes from a well-known author-\nship veriÔ¨Åcation competition [Stamatatos et al., 2015 ]. Re-\ncently, the authors of PAN-15 conduct a detailed analysis of\nthe dataset and Ô¨Ånd several deÔ¨Åcits such as special characters\nintroduced during the processing of the dataset and acciden-\ntal text overlap. Based on these insights, they constructed\nan improved authorship veriÔ¨Åcation corpus named Gutenberg\n[Bevendorff et al., 2019a]. This dataset eliminates the afore-\nmentioned bias and Ô¨Çaws, enabling the dataset to evaluate the\nperformance of authorship veriÔ¨Åcation methods more fairly.\nTherefore, this dataset is considered the most important one\nin our experiments. Nevertheless, we still included the PAN-\n15 dataset for completeness. The Enron data set comes from\nthe public Enron emails dataset. The samples are manually\nselected and cleaned to make sure of the quality [Halvani et\nal., 2018]. The Reddit dataset is obtained from [Halvani et\nal., 2017].\nThese data sets are respectively divided into two parts, the\ndevelopment set, and the test set. The PAN-15 and Gutenberg\ndatasets have been pre-divided by the provider. The other two\ndatasets are randomly divided according to the ratio of 1: 4\nfollowing the common practice in the research of authorship\nveriÔ¨Åcation [Halvani et al., 2017].\n4.2 Competing Methods\nWe compare our model with Ô¨Åve other methods, including\ntwo baseline methods, BAFF [Bevendorff et al., 2019a ] and\nits variation that use POS tags as inputs, two non-deep learn-\ning methods GLAD [H¬®urlimann et al., 2015 ] and Compres-\nsion [Halvani et al., 2017], and a deep learning based method\nMultiHead [Bagnall, 2015], which won the 1st place on the\nProceedings of the Twenty-Ninth International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-20)\n4028\nMethods PAN-15 Gutenberg Enron Reddit\nAUC-ROC AUC-PR AUC-ROC AUC-PR AUC-ROC AUC-PR AUC-ROC AUC-PR\nBAFF 0:7445 0:7098 0:7026 0:6638 0:7439 0:6784 0:7420 0:6901\nBAFF-POS 0:6813 0:6345 0:6742 0:6854 0:6961 0:6652 0:6407 0:6111\nCompression 0:6400 0:6014 0:7633 0:7152 0:7810 0:7301 0:7689 0:7141\nGLAD 0:6528 0:6621 0:8443 0:8615 0:7191 0:7491 0:6795 0:6721\nMultiHead 0:8165 0:8244 0:7849 0:8084 0:9369 0:9618 0:8544 0:8737\nGatedLM 0:8434 0:8274 0:8912 0:8903 0:9497 0:9633 0: 8714 0:8782\nTable 2: Authorship veriÔ¨Åcation performance comparison.\nPAN 2015 author identiÔ¨Åcation task[Stamatatos et al., 2015].\nWe chose these methods because they are the state of the art\nin the Ô¨Åeld of author veriÔ¨Åcation.\nWe reimplement the two baseline methods with Python,\nand MultiHead with PyTorch [Paszke et al., 2019]. For other\nmethods, we use codes obtained from their authors.\n4.3 Implementation Details\nWe implement the proposed method with PyTorch and run all\nthe experiments on a GPU with 11GB memory.\nHow to perform POS tagging is not the focus of this article,\ntherefore we rather arbitrarily choose the popular open-source\nPOS tagger Spacy [Matthew and Ines, 2015]. Other POS tag-\ngers such as StanfordNLP [Qi et al., 2018] can achieve simi-\nlar results.\nIn order to conduct fairly performance comparison to make\nsure the performance improvement of our method comes\nfrom the POS level language model and gated decoder, we\nuse the vanilla RNN same as MultiHead.\nThe most important hyperparameters of our model are the\nsize of the embedding layer and the size of the hidden layer.\nThese hyperparameters can be effectively selected on the de-\nvelopment set and do not vary much on different datasets.\nExcept for the Enron dataset where the best hidden size is 16,\non other datasets the model achieves the best results on the\ndevelopment set when the hidden size is 64.\nIn training, we follow the common practice to calculate the\ngradient with truncated backpropagation through time [Wer-\nbos and others, 1990; Graves, 2013 ]. SpeciÔ¨Åcally, we trun-\ncated the gradient for 20 time-steps and perform gradient de-\nscent utilizing vanilla SGD optimizer combined with gradient\nclipping at 0.25. All models are trained for 100 epochs and\nthe learning rate is set to 5. With these settings, we achieve\nstable performance on the development set.\n4.4 Performance Evaluation Methods\nWe use two complementary indicators: AUC-ROC and AUC-\nPR to measure and compare model performance. The AUC-\nROC is the area under the ROC curve, summing up true pos-\nitives against false positives. The AUC-PR is the area under\nthe curve of precision against the recall. We use average pre-\ncision in [Manning et al., 2008 ], a widely-used method, to\ncalculate AUC-PR. We chose these two metrics because they\nare classiÔ¨Åcation-threshold-invariant metrics. It allows us to\nfocus on comparing the predictive power of each method. For\na speciÔ¨Åc application, a threshold can be chosen to trade be-\ntween precision and recall.\nDue to the inherent variation of the algorithm, the experi-\nmental results reported in this paper comes from the average\nof 10 independently running with the same setting.\n4.5 Performance Comparision\nThis section examines the performance of our method on\naforementioned datasets. There are two major questions we\naim to address:\n\u000fIs it necessary to utilize deep learning methods for au-\nthorship veriÔ¨Åcation problem?\n\u000fIs our method more accurate than the state-of-the-art\ndeep learning based method?\nThe AUC-ROC and AUC-PR performance of our method\nand other competing methods are listed in Table 2. It is ev-\nident that deep learning based methods MultiHead and our\nGatedLM outperform traditional models on all four datasets,\nwhich indicates that deep learning based methods can beat so-\nphisticated manually designed features. Also, notice that the\nBAFF on POS perform worse than the raw BAFF on charac-\nter. This implies that simply using distribution of POS tags is\nunable to unleash its full potential.\nComparing our method with MultiHead, we can obtain\nthe main result that the POS level information and the gated\nneural language model enable our method to achieve signif-\nicant performance improvement. In particular, our method\nachieves the most signiÔ¨Åcant performance improvement in\ncross-topic scenario on Gutenberg dataset.\n4.6 Ablation Study\nIn this section, we perform ablation studies on the two key\ncomponents of our proposed method to examine their contri-\nbution to performance improvement respectively. We aim to\nanswer the following three key questions:\n\u000fIs it necessary to share parameters between language\nmodels of authors?\n\u000fDoes our gated neural language model achieve perfor-\nmance improvement over the multi-head structure?\nProceedings of the Twenty-Ninth International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-20)\n4029\n\u000fDoes POS information have advantages over char infor-\nmation, and under what circumstances?\nTo answer the Ô¨Årst question, we build a baseline model,\nwhich learns a completely independent language for each au-\nthor. This model has no shared parameter.\nFrom the experimental results listed in Table 3 we can\nÔ¨Ånd that both POS-level information and gated architecture\nmake major contributions to the performance improvement.\nThe baseline model without any parameter sharing performs\nsigniÔ¨Åcantly worse than multi-head and gated architecture,\nwhich indicates that it has difÔ¨Åculties in learning accurate lan-\nguage model for each author with extremely limited training\ndata. Therefore, a well-designed cross-author parameter shar-\ning is the key to learn more accurate author-speciÔ¨Åc language\nmodels by reducing the number of effective parameters.\nAlso, our gated architecture achieves higher accuracy com-\npared to the multi-head when both trained on POS level in-\nputs. This demonstrates that our gated decoder can indeed\ndistinguish the author-speciÔ¨Åc writing styles from common\npatterns and learn the common patterns with shared param-\neters. As a result, our model is less prone to overÔ¨Åtting and\ncan learn a more accurate language model.\nNot surprisingly, when using the same model architecture,\nthe POS-level language model outperforms the char-level lan-\nguage model by a large margin on the Gutenberg dataset\nbut only achieves moderate performance improvement on the\nPAN-15 dataset. This is due to that the PAN-15 dataset con-\ntains special characters introduced during the process of the\ndataset. Character level models can easily overÔ¨Åt these spe-\ncial characters and obtain biased high accuracy.\nIn order to quantitatively compare the importance of the\nauthor-speciÔ¨Åc decoder and shared decoder, we calculate the\nmean absolute logits of them respectively. We also calculate\nthe mean value of the gates. The experimental results listed in\nTable 4 demonstrate that the response amplitude of the shared\ndecoder is much larger than the amplitude of the author-\nspeciÔ¨Åc decoder on all datasets, which provide evidence to\nfurther support our claim that the model can learn common\nwriting patterns by shared decoder and author-speciÔ¨Åc styles\nby stand-alone decoder for each author.\n5 Related Work\nThe authorship veriÔ¨Åcation problem is Ô¨Årst proposed and\nstudied in [Koppel and Schler, 2004 ], and is closely related\nto authorship attribution problem. While being almost solved\non long texts [Bevendorff et al., 2019a], authorship veriÔ¨Åca-\ntion is still a challenging task on short texts.\n5.1 Traditional Authorship VeriÔ¨Åcation\nThere are numerous authorship veriÔ¨Åcation methods attempt-\ning to tackle this problem from different aspects [Stamatatos,\n2009].\nThe very Ô¨Årst work on authorship veriÔ¨Åcation [Koppel and\nSchler, 2004 ] proposed an unmasking method that mainly\nfocused on the analysis of most frequent function words.\nWriteprints [Abbasi and Chen, 2008 ] extract more than\ntwenty stylometric features including lexical, syntactic, and\nstructural text features to conduct authorship analysis. BAFF\nModel Level PAN-15 Gutenberg\nGatedLM POS 0:8434 0:8912\nGatedLM Char 0:8181 0:8199\nMultiHead POS 0:8321 0:8462\nMultiHead Char 0:8165 0:7849\nNaive POS 0:8063 0:8599\nNaive Char 0:7875 0:7161\nTable 3: Ablation studies. (AUC)\nDataset logits author logits share gate\nPAN-15 0:5417 2:9452 0:5504\nGutenberg 0:7145 6:0114 0:3954\nEnron 0:5287 3:2292 0:4794\nReddit 0:4022 3:6439 0:5716\nTable 4: The contribution of author speciÔ¨Åc logits and shared logits.\n[Bevendorff et al., 2019a] relies on several simple measures\n(e.g, TF-IDF) deÔ¨Åned upon char level trigram. The distri-\nbution of POS tags can also be utilized to measure the style\ndifferences [Zhao et al., 2006]. Compression is another well-\nstudied path to authorship veriÔ¨Åcation which is Ô¨Årst proposed\nin [Khmelev and Teahan, 2003 ] and recently improved by\n[Halvani et al., 2017].\n5.2 Deep Authorship Analysis\nOnly a few studies have been done on deep authorship veriÔ¨Å-\ncation and deep authorship attribution. In [Bagnall, 2015], a\ncharacter level language model is learned for each author to\ncapture their unique writing styles. Texts will have a higher\nprobability coming from the language model of its true au-\nthor. Recently, several methods use convolution neural net-\nwork to identify the authorship. Their main differences are\nthe formats of the inputs. [Hitschler et al., 2017] Ô¨Årst trans-\nforms the raw texts into sequences of POS tags, then use a\nconvolution neural network to predict the author.[Shrestha et\nal., 2017] proposes to use character n-gram as the input of the\nconvolution neural network. [Ruder et al., 2016] proposes to\ncombine the char-level input and the word-level input with a\nmulti-channel neural network.\n6 Conclusions\nWe have presented our novel POS-level (Part of Speech)\ngated RNN based language model that can learn the author-\nspeciÔ¨Åc syntactic styles with high data efÔ¨Åciency. We Ô¨Årst\npropose to use a POS-level language model to obtain the syn-\ntactic level information. Then we design a novel gated de-\ncoder architecture to improve parameter sharing and enable\ndata-efÔ¨Åcient learning of the author-speciÔ¨Åc language model.\nExtensive experimental results show that our model outper-\nforms other state-of-the-art competing methods by a large\nmargin in terms of accuracy.\nProceedings of the Twenty-Ninth International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-20)\n4030\nReferences\n[Abbasi and Chen, 2008] Ahmed Abbasi and Hsinchun\nChen. Writeprints: A stylometric approach to identity-\nlevel identiÔ¨Åcation and similarity detection in cyberspace.\nACM Transactions on Information Systems (TOIS),\n26(2):7, 2008.\n[Bagnall, 2015] Douglas Bagnall. Author identiÔ¨Åcation us-\ning multi-headed recurrent neural networks.arXiv preprint\narXiv:1506.04891, 2015.\n[Bengio et al., 2003] Yoshua Bengio, R ¬¥ejean Ducharme,\nPascal Vincent, and Christian Jauvin. A neural probabilis-\ntic language model. Journal of machine learning research,\n3(Feb):1137‚Äì1155, 2003.\n[Bevendorff et al., 2019a] Janek Bevendorff, Matthias Ha-\ngen, Benno Stein, and Martin Potthast. Bias analysis and\nmitigation in the evaluation of authorship veriÔ¨Åcation. In\nProceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 6301‚Äì6306, 2019.\n[Bevendorff et al., 2019b] Janek Bevendorff, Martin Pot-\nthast, Matthias Hagen, and Benno Stein. Heuristic au-\nthorship obfuscation. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics,\npages 1098‚Äì1108, 2019.\n[Graves, 2013] Alex Graves. Generating sequences with re-\ncurrent neural networks. arXiv preprint arXiv:1308.0850,\n2013.\n[Halvani et al., 2017] Oren Halvani, Christian Winter, and\nLukas Graner. Authorship veriÔ¨Åcation based on\ncompression-models. arXiv preprint arXiv:1706.00516,\n2017.\n[Halvani et al., 2018] Oren Halvani, Lukas Graner, and Inna\nV ogel. Authorship veriÔ¨Åcation in the absence of explicit\nfeatures and thresholds. In European Conference on Infor-\nmation Retrieval, pages 454‚Äì465. Springer, 2018.\n[Hitschler et al., 2017] Julian Hitschler, Esther van den\nBerg, and Ines Rehbein. Authorship attribution with con-\nvolutional neural networks and pos-eliding. In Proceed-\nings of the Workshop on Stylistic Variation, pages 53‚Äì58,\n2017.\n[H¬®urlimann et al., 2015] Manuela H ¬®urlimann, Benno Weck,\nEsther van den Berg, Simon Suster, and Malvina Nis-\nsim. Glad: Groningen lightweight authorship detection.\nIn CLEF (Working Notes), 2015.\n[Khmelev and Teahan, 2003] Dmitry V Khmelev and\nWilliam J Teahan. A repetition based measure for veriÔ¨Å-\ncation of text collections and for text categorization. In\nProceedings of the 26th annual international ACM SIGIR\nconference on Research and development in informaion\nretrieval, pages 104‚Äì110. ACM, 2003.\n[Koppel and Schler, 2004] Moshe Koppel and Jonathan\nSchler. Authorship veriÔ¨Åcation as a one-class classi-\nÔ¨Åcation problem. In Proceedings of the twenty-Ô¨Årst\ninternational conference on Machine learning, page 62.\nACM, 2004.\n[Manning et al., 2008] Christopher D Manning, Prabhakar\nRaghavan, and Hinrich Sch ¬®utze. Introduction to informa-\ntion retrieval. Cambridge university press, 2008.\n[Matthew and Ines, 2015] Honnibal Matthew and Montani\nInes. spacy: Industrial-strength nlp. https://spacy.io/,\n2015. [Version=2.2.3].\n[Paszke et al., 2019] Adam Paszke, Sam Gross, Francisco\nMassa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. Pytorch: An imperative style, high-\nperformance deep learning library. In Advances in Neural\nInformation Processing Systems, pages 8024‚Äì8035, 2019.\n[Qi et al., 2018] Peng Qi, Timothy Dozat, Yuhao Zhang, and\nChristopher D. Manning. Universal dependency parsing\nfrom scratch. In Proceedings of the CoNLL 2018 Shared\nTask: Multilingual Parsing from Raw Text to Universal\nDependencies, pages 160‚Äì170, Brussels, Belgium, Octo-\nber 2018. Association for Computational Linguistics.\n[Ruder et al., 2016] Sebastian Ruder, Parsa Ghaffari, and\nJohn G Breslin. Character-level and multi-channel con-\nvolutional neural networks for large-scale authorship attri-\nbution. arXiv preprint arXiv:1609.06686, 2016.\n[Shrestha et al., 2017] Prasha Shrestha, Sebastian Sierra,\nFabio A Gonz ¬¥alez, Manuel Montes, Paolo Rosso, and\nThamar Solorio. Convolutional neural networks for au-\nthorship attribution of short texts. In Proceedings of the\n15th Conference of the European Chapter of the Associa-\ntion for Computational Linguistics: Volume 2, Short Pa-\npers, pages 669‚Äì674, 2017.\n[Stamatatos et al., 2015] Efstathios Stamatatos, Walter\nDaelemans, Ben Verhoeven, Patrick Juola, Aurelio Lopez-\nLopez, Martin Potthast, and Benno Stein. Overview of\nthe author identiÔ¨Åcation task at pan 2015. In CLEF 2015\nWorking Notes Papers, 2015.\n[Stamatatos, 2009] Efstathios Stamatatos. A survey of mod-\nern authorship attribution methods. Journal of the Amer-\nican Society for information Science and Technology,\n60(3):538‚Äì556, 2009.\n[Werbos and others, 1990] Paul J Werbos et al. Backpropa-\ngation through time: what it does and how to do it. Pro-\nceedings of the IEEE, 78(10):1550‚Äì1560, 1990.\n[Zhao et al., 2006] Ying Zhao, Justin Zobel, and Phil Vines.\nUsing relative entropy for authorship attribution. In\nAsia Information Retrieval Symposium, pages 92‚Äì105.\nSpringer, 2006.\nProceedings of the Twenty-Ninth International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-20)\n4031",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8795114755630493
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.7266514897346497
    },
    {
      "name": "Natural language processing",
      "score": 0.6100526452064514
    },
    {
      "name": "Artificial intelligence",
      "score": 0.600452184677124
    },
    {
      "name": "Language model",
      "score": 0.49076446890830994
    },
    {
      "name": "Focus (optics)",
      "score": 0.48794978857040405
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.45036622881889343
    },
    {
      "name": "Parsing",
      "score": 0.4400831460952759
    },
    {
      "name": "Programming language",
      "score": 0.1113227903842926
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I19820366",
      "name": "Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210156404",
      "name": "Institute of Information Engineering",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    }
  ],
  "cited_by": 4
}