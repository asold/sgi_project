{
  "title": "Extracting Multiple Worries From Breast Cancer Patient Blogs Using Multilabel Classification With the Natural Language Processing Model Bidirectional Encoder Representations From Transformers: Infodemiology Study of Blogs",
  "url": "https://openalex.org/W4281291858",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5101574718",
      "name": "T. Watanabe",
      "affiliations": [
        "Keio University"
      ]
    },
    {
      "id": "https://openalex.org/A5005540314",
      "name": "Shuntaro Yada",
      "affiliations": [
        "Nara Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5041089475",
      "name": "Eiji Aramaki",
      "affiliations": [
        "Nara Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5090815016",
      "name": "Hiroshi Yajima",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5046447951",
      "name": "Hayato Kizaki",
      "affiliations": [
        "Keio University"
      ]
    },
    {
      "id": "https://openalex.org/A5084452040",
      "name": "Satoko Hori",
      "affiliations": [
        "Keio University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3128646645",
    "https://openalex.org/W2904991649",
    "https://openalex.org/W2058124381",
    "https://openalex.org/W3002165707",
    "https://openalex.org/W2021664998",
    "https://openalex.org/W2996219887",
    "https://openalex.org/W3019137756",
    "https://openalex.org/W3172540971",
    "https://openalex.org/W2808903237",
    "https://openalex.org/W2749701213",
    "https://openalex.org/W2740216362",
    "https://openalex.org/W3082318988",
    "https://openalex.org/W4229019698",
    "https://openalex.org/W2956394034",
    "https://openalex.org/W2164777277",
    "https://openalex.org/W3213545440",
    "https://openalex.org/W2661359212",
    "https://openalex.org/W2753001088"
  ],
  "abstract": "Background Patients with breast cancer have a variety of worries and need multifaceted information support. Their accumulated posts on social media contain rich descriptions of their daily worries concerning issues such as treatment, family, and finances. It is important to identify these issues to help patients with breast cancer to resolve their worries and obtain reliable information. Objective This study aimed to extract and classify multiple worries from text generated by patients with breast cancer using Bidirectional Encoder Representations From Transformers (BERT), a context-aware natural language processing model. Methods A total of 2272 blog posts by patients with breast cancer in Japan were collected. Five worry labels, “treatment,” “physical,” “psychological,” “work/financial,” and “family/friends,” were defined and assigned to each post. Multiple labels were allowed. To assess the label criteria, 50 blog posts were randomly selected and annotated by two researchers with medical knowledge. After the interannotator agreement had been assessed by means of Cohen kappa, one researcher annotated all the blogs. A multilabel classifier that simultaneously predicts five worries in a text was developed using BERT. This classifier was fine-tuned by using the posts as input and adding a classification layer to the pretrained BERT. The performance was evaluated for precision using the average of 5-fold cross-validation results. Results Among the blog posts, 477 included “treatment,” 1138 included “physical,” 673 included “psychological,” 312 included “work/financial,” and 283 included “family/friends.” The interannotator agreement values were 0.67 for “treatment,” 0.76 for “physical,” 0.56 for “psychological,” 0.73 for “work/financial,” and 0.73 for “family/friends,” indicating a high degree of agreement. Among all blog posts, 544 contained no label, 892 contained one label, and 836 contained multiple labels. It was found that the worries varied from user to user, and the worries posted by the same user changed over time. The model performed well, though prediction performance differed for each label. The values of precision were 0.59 for “treatment,” 0.82 for “physical,” 0.64 for “psychological,” 0.67 for “work/financial,” and 0.58 for “family/friends.” The higher the interannotator agreement and the greater the number of posts, the higher the precision tended to be. Conclusions This study showed that the BERT model can extract multiple worries from text generated from patients with breast cancer. This is the first application of a multilabel classifier using the BERT model to extract multiple worries from patient-generated text. The results will be helpful to identify breast cancer patients’ worries and give them timely social support.",
  "full_text": "Original Paper\nExtracting Multiple Worries From Breast Cancer Patient Blogs\nUsing Multilabel Classification With the Natural Language\nProcessing Model Bidirectional Encoder Representations From\nTransformers: Infodemiology Study of Blogs\nTomomi Watanabe1, BSc; Shuntaro Yada2, PhD; Eiji Aramaki2, PhD; Hiroshi Yajima3, MSc; Hayato Kizaki1, MSc;\nSatoko Hori1, PhD\n1Division of Drug Informatics, Keio University Faculty of Pharmacy, Tokyo, Japan\n2Nara Institute of Science and Technology, Nara, Japan\n3Mediaid Corporation, Tokyo, Japan\nCorresponding Author:\nSatoko Hori, PhD\nDivision of Drug Informatics\nKeio University Faculty of Pharmacy\n1-5-30 Shibakouen, Minato-ku\nTokyo, 105-8512\nJapan\nPhone: 81 3 5400 2650\nEmail: hori-st@pha.keio.ac.jp\nAbstract\nBackground: Patients with breast cancer have a variety of worries and need multifaceted information support. Their accumulated\nposts on social media contain rich descriptions of their daily worries concerning issues such as treatment, family, and finances.\nIt is important to identify these issues to help patients with breast cancer to resolve their worries and obtain reliable information.\nObjective: This study aimed to extract and classify multiple worries from text generated by patients with breast cancer using\nBidirectional Encoder Representations From Transformers (BERT), a context-aware natural language processing model.\nMethods: A total of 2272 blog posts by patients with breast cancer in Japan were collected. Five worry labels, “treatment,”\n“physical,” “psychological,” “work/financial,” and “family/friends,” were defined and assigned to each post. Multiple labels were\nallowed. To assess the label criteria, 50 blog posts were randomly selected and annotated by two researchers with medical\nknowledge. After the interannotator agreement had been assessed by means of Cohen kappa, one researcher annotated all the\nblogs. A multilabel classifier that simultaneously predicts five worries in a text was developed using BERT. This classifier was\nfine-tuned by using the posts as input and adding a classification layer to the pretrained BERT. The performance was evaluated\nfor precision using the average of 5-fold cross-validation results.\nResults: Among the blog posts, 477 included “treatment,” 1138 included “physical,” 673 included “psychological,” 312 included\n“work/financial,” and 283 included “family/friends.” The interannotator agreement values were 0.67 for “treatment,” 0.76 for\n“physical,” 0.56 for “psychological,” 0.73 for “work/financial,” and 0.73 for “family/friends,” indicating a high degree of\nagreement. Among all blog posts, 544 contained no label, 892 contained one label, and 836 contained multiple labels. It was\nfound that the worries varied from user to user, and the worries posted by the same user changed over time. The model performed\nwell, though prediction performance differed for each label. The values of precision were 0.59 for “treatment,” 0.82 for “physical,”\n0.64 for “psychological,” 0.67 for “work/financial,” and 0.58 for “family/friends.” The higher the interannotator agreement and\nthe greater the number of posts, the higher the precision tended to be.\nConclusions: This study showed that the BERT model can extract multiple worries from text generated from patients with\nbreast cancer. This is the first application of a multilabel classifier using the BERT model to extract multiple worries from\npatient-generated text. The results will be helpful to identify breast cancer patients’ worries and give them timely social support.\n(JMIR Cancer 2022;8(2):e37840) doi: 10.2196/37840\nJMIR Cancer 2022 | vol. 8 | iss. 2 | e37840 | p. 1https://cancer.jmir.org/2022/2/e37840\n(page number not for citation purposes)\nWatanabe et alJMIR CANCER\nXSL•FO\nRenderX\nKEYWORDS\nbreast neoplasm; cancer; natural language processing; NLP; artificial intelligence; model; machine learning; content analysis;\ntext mining; sentiment analysis; oncology; quality of life; social media; social support; breast cancer; BERT model; peer support;\nblog post; patient data\nIntroduction\nBreast cancer is the most diagnosed female cancer worldwide,\nand treatment can last for 5 to 10 years, making this a familiar\ndisease that women will live with for a long time [1-3]. Patients\nwith breast cancer have multiple worries about treatment, family,\nfinances, and so on, and these worries change over time.\nAlthough support for them is provided by medical professionals,\npatients’ worries are sometimes overlooked in clinical settings\n[4].\nCurrently, many patients use social media as a source of medical\ninformation [5]. Patient-generated text such as posts and\ncomments are accumulated on the internet and contain a wealth\nof information about patients’ experiences and daily worries.\nIt may be possible to use this information to help patients solve\ntheir problems and improve their quality of life. However, the\nsubstantial amount of text and the variable reliability of\ninformation on social media make it difficult for patients to get\nthe accurate information they seek [6]. This large amount of\nsocial media data has become a new source of medical\ninformation and a target for natural language processing (NLP)\n[7,8].\nDocument classification by NLP can be used to extract\ninformation from text. This technique is useful for automatically\nidentifying worries from patient-generated text and helping\npatients with breast cancer obtain appropriate information to\nresolve their worries. Although there are many NLP studies on\nportals for patients with breast cancer, most of them are content\nanalyses that objectively analyze the contents of media.\nAlthough content analysis research can find multiple worries,\nthe extracted worries cannot be defined. In contrast, document\nclassification can set target worries and find them, but so far,\nthere have been few document classification studies [9], and\nstudies targeting worries are particularly rare. Therefore, it is\nnecessary to create a document classification model that can\nautomatically extract multiple worries from text generated from\npatients with breast cancer.\nThere has been much research on using NLP to extract topics\nand worries from patient-generated text automatically. Many\nstudies used rule-based, bag-of-words, and topic models such\nas latent Dirichlet allocation (LDA) [10-12], and there remains\nroom for improvement in extracting worries from the variously\nexpressed patient descriptions in these models. These models\nhave particular difficulty in dealing with context, but context\ncan be used by deep-learning methods such as long short-term\nmemory (LSTM) and Bidirectional Encoder Representations\nFrom Transformers (BERT), which has proved to be state of\nthe art in several NLP tasks [13]. While there have been studies\nof patient-generated text using BERT to extract adverse drug\neffects [14,15], few studies have been conducted on text\ndescribing multiple worries that patients often have at the same\ntime. There are some previous reports in which sentiment\nclassification of patient-generated text was conducted using\nLSTM [16]. However, these only apply one label to one\ndocument and do not address multiple worries within a single\ndocument.\nThe purpose of this study was to develop a multilabel\nclassification model using BERT to automatically extract\nmultifaceted worries from text generated by patients with breast\ncancer.\nMethods\nData Set\nIn this study, blog articles on Life Palette [17], one of the\ninternet patient communities in Japan, were used. All the articles\nwere written in Japanese. The data source consists of 13,570\nposts written by 289 users from March 2008 to November 2014.\nA total of 2272 breast cancer posts were extracted as a data set,\nexcluding drafts and duplicates (Figure 1).\nJMIR Cancer 2022 | vol. 8 | iss. 2 | e37840 | p. 2https://cancer.jmir.org/2022/2/e37840\n(page number not for citation purposes)\nWatanabe et alJMIR CANCER\nXSL•FO\nRenderX\nFigure 1. Overview of data processing and model function. (A) Data selection criteria and model training and testing process; (B) post label prediction\nmodel functions and outputs. *In Japanese sentences, the object is sometimes omitted, so the presumed object was judged from the context and added\nin parentheses. BERT: Bidirectional Encoder Representations From Transformers.\nEthical Approval\nThis study was approved by the ethics committee of the Keio\nUniversity Faculty of Pharmacy (approval No 191218-2,\n190301-1). All procedures were performed in accordance with\nthe Ethical Guidelines for Medical and Health Research\nInvolving Human Subjects (settled by the Ministry of Education,\nCulture, Sports, Science and Technology and the Ministry of\nHealth, Labour and Welfare in Japan) and the Declaration of\nHelsinki and its later amendments. Consent to use the data from\nLife Palette for research purposes was obtained at the time of\nuser registration. In this study, all data were analyzed\nanonymously and informed consent for this research was waived\ndue to the retrospective observational design of the study.\nJMIR Cancer 2022 | vol. 8 | iss. 2 | e37840 | p. 3https://cancer.jmir.org/2022/2/e37840\n(page number not for citation purposes)\nWatanabe et alJMIR CANCER\nXSL•FO\nRenderX\nAnnotation\nThe annotation criteria were defined based on previous studies\n[18]. To assess the reliability of the annotation criteria, 50 blog\nposts were randomly selected from the data set and annotated\nby two researchers with medical knowledge (authors TW and\nSH). After assessment of interannotator agreement (IAA) by\nmeans of Cohen kappa, one researcher (TW) annotated all the\nblogs. Cohen kappa takes a value close to 1 if the annotators\nare in perfect agreement; less than 0 is poor, 0-0.2 is slight,\n0.21-0.4 is fair, 0.41-0.6 is moderate, 0.61-0.8 is substantial,\n0.81-1 is almost perfect [19].\nBased on the “Shizuoka Classification” [20], which is a method\nfor classifying the worries of patients with cancer in Japan, the\nfollowing five labels were established: “treatment,” “physical,”\n“psychological,” “work/financial,” and “family/friends” (Table\nS1 in Multimedia Appendix 1). If a single blog post contains\ndescriptions of multiple worries, multiple labels were allowed.\nModel Structure\nIn this study, a multilabel classifier was built from the annotated\nmultilabel data set to deal with multiple descriptions of worries.\nTo develop the classifier, BERT, a state-of-the-art NLP model\nthat can take context into account, was used. BERT is trained\nvia a two-step learning process. The first step is pretraining\nusing a large amount of text data and the second step is\nfine-tuning the model from new data.\nThe model was built by fine-tuning the pretrained Japanese\nBERT model of the Inui and Suzuki Laboratory, Tohoku\nUniversity [21] (BERT-base model; 12 layers, 768 dimensions\nof hidden states, and 12 attention heads, tokenizer: MeCab [22],\nexternal dictionary: mecab-ipadic-NEologd [23]) from the\nannotated multilabel data set. Due to the capability of the\npretrained model, the input was limited to 512 words, starting\nfrom the beginning of the sentence.\nThe [CLS] token and [SEP] token were added at the beginning\nof the sentence and at the end of the sentence, respectively. This\nwas used as input to the BERT model. The model consists of a\npretrained BERT and a fully connected layer, and the activation\nfunction was a sigmoid function that outputs five labeled\npositive/negative results. The model was built with reference\nto the previous study [24]. The input to the fully connected layer\nwas the vector corresponding to the [CLS] token in the output\nvector of the pretrained BERT. The hyperparameters that could\nbe adjusted prior to training were defined as follows. The loss\nfunction was cross-entropy, batch size was 16, five epochs were\nrun, early stopping was not set, and all parameters were\nfine-tuned, including the pretrained BERT from Adam with a\nlearning rate of 1e-5 (Figure 2).\nIn the BERT model, it is possible to incorporate a self-attention\nmethod that allows indicating which part of the output text has\nbeen paid attention to. Visualizing the attentions can be useful\nin interpreting the results of “black box” machine learning\nmodels. Therefore, in this study, the attention parts of each blog\npost were visualized and used as a reference for interpreting the\nlabeling results.\nFigure 2. Model structure developed in this study. The input is the post sentence with [CLS] token and [SEP] token added at the beginning and at the\nend, respectively. The output is 0/1, corresponding to negative/positive of each label. BERT: Bidirectional Encoder Representations From Transformers;\ndim: dimension.\nJMIR Cancer 2022 | vol. 8 | iss. 2 | e37840 | p. 4https://cancer.jmir.org/2022/2/e37840\n(page number not for citation purposes)\nWatanabe et alJMIR CANCER\nXSL•FO\nRenderX\nTask and Metrics\nA multilabel task was performed to classify five labels\nsimultaneously. The performance was evaluated in terms of\nprecision, F score, and exact match accuracy, which indicates\nthe percentage of correct predictions for all labels. As a way to\nuse the research, we envision the construction of an information\nprovision system tailored to each patient’s problems. Therefore,\nwe focused on precision so as not to provide unmatched\ninformation and inadvertently impose a burden on patients with\nbreast cancer. The data set was divided into training data and\ntest data in a ratio of 4:1, and the model was evaluated using\nthe average of 5-fold cross-validation results to confirm its\nrobustness.\nMoreover, to examine the effect of the upper limit of the number\nof input words on the model performance, the performance for\nblog posts with over 512 words, that for all posts, and that for\nposts with 512 words or less were compared.\nResults\nData Set Analysis\nThe mean number of words per blog post in the data set was\n464.9, the median was 357, and the maximum was 6746. The\nnumber of documents with more than 512 words was 723\n(31.8% of all blog posts; Figure S1 in Multimedia Appendix\n1).\nAnnotation\nThe IAA values were the highest for “physical” and the lowest\nfor “psychological” (Table 1). This time, the labels except for\n“psychological” showed a high degree of agreement with IAA\nvalues higher than 0.61, corresponding to “substantial”\nprecision. The complete label agreement rate that indicates all\nthe label-matched blog posts was 0.40.\nThe number of blog posts was highest for “physical” and lowest\nfor “family/friends” (Table 1). The number of labels per blog\npost was the highest for single label posts and the lowest for\nposts with all five labels. Articles with no labels at all amounted\nto 544 (23.9%), and articles with a single label and multiple\nlabels amounted to 892 (39.3%) and 836 (36.8%), respectively\n(Table 2). In addition, it was found that there were differences\nin worries among users, and the worries expressed by the same\nuser changed over time (Figure S2 in Multimedia Appendix 1).\nTable 1. The IAAa values and the number of posts for the five labels (N=2272).\nPosts, nIAAbLabel\n4770.67Treatment\n11380.76Physical\n6730.56Psychological\n3120.73Work/financial\n2830.73Family/friends\naIAA: interannotator agreement.\nbAnnotation agreement was evaluated using Cohen kappa.\nTable 2. The number of labels per blog post (N=2272).\nPosts, n (%)Number of labels\n544 (23.9)0\n892 (39.3)1\n578 (25.4)2\n199 (8.8)3\n57 (2.5)4\n2 (0.1)5\nModel\nThe precision was 0.59 for “treatment,” 0.82 for “physical,”\n0.64 for “psychological,” 0.67 for “work/financial,” and 0.58\nfor “family/friends.” Both the precision and the F score were\nhighest for “physical” (Table 3). The exact match accuracy was\n0.44.\nThe performances of posts with more than 512 words and posts\nwith 512 words or less are presented in Multimedia Appendix\n1.\nJMIR Cancer 2022 | vol. 8 | iss. 2 | e37840 | p. 5https://cancer.jmir.org/2022/2/e37840\n(page number not for citation purposes)\nWatanabe et alJMIR CANCER\nXSL•FO\nRenderX\nTable 3. Performance of the model.\nF score (SD)Recall (SD)Precision (SD)Accuracy (SD)Label\n0.44 (0.09)0.39 (0.15)0.59 (0.09)0.81 (0.01)Treatment\n0.81 (0.01)0.80 (0.02)0.82 (0.02)0.81 (0.01)Physical\n0.58 (0.04)0.54 (0.08)0.64 (0.04)0.77 (0.03)Psychological\n0.38 (0.03)0.28 (0.05)0.67 (0.10)0.88 (0.02)Work/financial\n0.41 (0.07)0.33 (0.07)0.58 (0.11)0.88 (0.02)Family/friends\n0.52 (0.03)0.47 (0.05)0.66 (0.04)0.83 (0.01)Macro average\nDiscussion\nPrincipal Findings\nThis is the first report of a multilabel classifier using the BERT\nmodel to extract multiple types of worries in patient-generated\ntext, and our results indicate that BERT is effective for this\npurpose.\nComparison With Prior Work\nOur model can extract multiple worries from a single post. There\nhave been some NLP studies that have dealt with multiple\nworries in patient-generated text [18,25]. However, these studies\nused a multi-class classification that allows only one label per\ndocument and could not find multiple worries contained in a\nsingle document. Similar to this study, there was a previous\nstudy on classifying blog sentences with worry descriptions\n[18]. However, the previous study dealt with binary\nclassification and short text, while our study dealt with\nmultilabel classification and long text. Furthermore, our study\noutperformed the previous one in F score. Some studies have\nused a multilabel classifier of patient-generated messages based\non the viewpoint of medical professionals [26,27]. In contrast,\na noteworthy feature of this study was the classification of\npatient-generated text from the viewpoint of patients.\nStrength of the Model\nA multilabel classifier may be useful for patients with breast\ncancer because they may have multiple worries and the nature\nof their worries may change over time. This study has\ndemonstrated that documents with multiple worries can be\nhandled using BERT. As another approach, a lot of content\nanalysis research has been done using topic models such as\nLDA for unsupervised learning [10]. LDA is a model that\nextracts multiple topics in a single document that would be\nsuitable for handling a wide range of patient worries. However,\nthis model is often used for content analysis rather than\ndocument classification, which ultimately requires manual\ninterpretation of topics. An advantage of our model is that it\nautomatically outputs the presence or absence of worries based\non the input of sentences, so it does not require a final human\njudgment and can present the results quickly. Thus, our\ncontext-aware model is expected to be efficient for dealing with\ntexts generated by patients with breast cancer that contain\nmultiple worries and long descriptions because it extracts\nworries by paying attention to descriptions based on the human\nsenses (Figure S3 in Multimedia Appendix 1).\nFeatures of the Data Set\nThe reliability of the data set was inferred from the annotation\nresults: the IAA was above 0.61, which was “substantial” for\nall labels except “psychological,” indicating a high degree of\nagreement. The “psychological” label tended to be judged\ndifferently among researchers, compared with the other labels.\nHowever, it is considered that the data set was reliable enough\nas training data because the IAA values exceeded 0.41, which\nindicates “moderate” reliability. In the data set of posts written\nby patients with breast cancer, more than one worry was actually\ndescribed in about 40% of the posts (Table 2), and it was\nconfirmed that the worries described by the same user changed\nover time (Figure S1 in Multimedia Appendix 1), which was\nin agreement with previous studies. These results suggest that\nthe data set was suitable for development of a multilabel\nclassifier.\nError Analysis\nTo evaluate the reliability of the model, error analysis was\nconducted. Many of the false-positive cases were descriptions\nof changes in “physical,” which had the highest precision, and\ndealt with conditions that were not covered by the annotation\nguidelines. They were similar to the “physical” descriptions,\nsuch as postoperative recovery, chest discomfort before\ndiagnosis, and changes in physical condition that seemed\nunrelated to cancer (eg, “I was surprised that I could lift my\narms more than before surgery!” “One day, I was surprised at\nthe size of the difference between my left and right breasts,” or\n“I drank a little wine and sake and felt dizzy”). Although there\nis still room for improvement in the performance of this model\nin discriminating between “presence of distress” and “presence\nof distress caused by breast cancer,” this model will be useful\nin supporting patients with breast cancer because we were able\nto extract descriptions of “physical changes that cause distress”\nin patients with breast cancer.\nLimitations\nFirst, the BERT model used in this study has great strength in\nrecognizing context, but the upper limit of the number of input\nwords is 512. Although there was concern that the performance\nmight deteriorate with posts having more than 512 input words,\nit was found that there was almost no difference between the\nperformance only for posts with more than 512 input words and\nthat for all posts. On the other hand, the performance for posts\nwith 512 input words or less was slightly inferior to that for all\nposts. Based on these results, it was considered that truncation\nafter 512 input words had little effect on the model performance,\nwhereas the lack of information due to a small number of input\nJMIR Cancer 2022 | vol. 8 | iss. 2 | e37840 | p. 6https://cancer.jmir.org/2022/2/e37840\n(page number not for citation purposes)\nWatanabe et alJMIR CANCER\nXSL•FO\nRenderX\nwords had a greater effect in this analysis. This suggests that\nblog posts containing a larger number of input words than the\nupper limit would not degrade model performance (Table 2 and\nTable S2 in Multimedia Appendix 1).\nSecond, the small number of blog posts for each label in our\ndata set is also the limitation of this study. Our model was built\nfrom the data set containing descriptions of five worry types.\nThe prediction performance of the model was different for each\nlabel, and the higher the IAA and the greater the number of\nposts, the higher the precision and the F score tended to be. This\nsuggests that the IAA and the number of posts are important\nfactors in constructing the classifier. This problem can be\novercome by increasing the number of blog posts for each label.\nThird, the patients’ blogs used in this study were written in\nJapanese. It is important to develop a classification model in\nJapanese, but the lack of applicability to multiple languages\nmay be a limitation.\nFuture Directions\nOur findings could lead to the development of better patient\nsupport systems and methods that can respond to temporal and\ninterindividual changes in worries. Our methodology also\nfacilitates the identification of worries and may promote the\nsharing of problems among patients. Furthermore, in the future,\nby combining sentiment analysis with our model, it might be\npossible to enrich the interpretation of the findings and deepen\nthe understanding of how breast cancer patients’ worries\ninfluence their emotions. Although this study focused only on\nworries about breast cancer, there are many common worries\nthat are not specific for breast cancer, and it is expected that the\nmodel could be extended to other disease areas.\nConclusion\nIn conclusion, this study showed that the BERT model can\nextract multiple worries, such as “treatment,” “physical,”\n“psychological,” “work/financial,” and “family/friends,” from\ntext generated by patients with breast cancer. This is the first\nstudy to deal with multiple patient worries using BERT and\ndemonstrates the usefulness of NLP techniques in dealing with\npatient-generated text. The results will be helpful to identify\nbreast cancer patients’ worries and give them timely social\nsupport.\nAcknowledgments\nThis study was supported by JSPS KAKENHI Grant Number JP21H03170.\nData Availability\nThe data consisting of blog articles in the study are available from Mediaid Corporation upon reasonable request.\nAuthors' Contributions\nTW, SY, EA, HK, and SH designed the study. TW and SH conducted annotation. TW performed the data analysis, created the\nnatural language processing (NLP) model, and conducted all experiments. HY owned and provided the data source of Life Palette.\nSY and EA supervised the study design from the NLP technical perspective. SH supervised the study. TW and SH drafted and\ncompleted the manuscript. All authors reviewed and approved the manuscript.\nConflicts of Interest\nHY is the chief executive officer of Mediaid Corporation that operates Life Palette. The other authors declare no competing\ninterests.\nMultimedia Appendix 1\nSupplementary material.\n[DOCX File , 196 KB-Multimedia Appendix 1]\nReferences\n1. Sung H, Ferlay J, Siegel RL, Laversanne M, Soerjomataram I, Jemal A, et al. Global cancer statistics 2020: GLOBOCAN\nestimates of incidence and mortality worldwide for 36 cancers in 185 countries. CA Cancer J Clin 2021 May;71(3):209-249.\n[doi: 10.3322/caac.21660] [Medline: 33538338]\n2. Burstein HJ, Lacchetti C, Griggs JJ. Adjuvant endocrine therapy for women with hormone receptor-positive breast cancer:\nASCO clinical practice guideline focused update. J Oncol Pract 2019 Feb;15(2):106-107. [doi: 10.1200/JOP.18.00617]\n[Medline: 30523754]\n3. Cancer Statistics in Japan-2019. Tokyo, Japan: Foundation for Promotion of Cancer Research; Mar 2020.\n4. Montazeri A, Jarvandi S, Haghighat S, Vahdani M, Sajadian A, Ebrahimi M, et al. Anxiety and depression in breast cancer\npatients before and after participation in a cancer support group. Patient Educ Couns 2001 Dec 01;45(3):195-198. [doi:\n10.1016/s0738-3991(01)00121-5] [Medline: 11722855]\nJMIR Cancer 2022 | vol. 8 | iss. 2 | e37840 | p. 7https://cancer.jmir.org/2022/2/e37840\n(page number not for citation purposes)\nWatanabe et alJMIR CANCER\nXSL•FO\nRenderX\n5. Aggarwal R, Hueniken K, Eng L, Kassirian S, Geist I, Balaratnam K, et al. Health-related social media use and preferences\nof adolescent and young adult cancer patients for virtual programming. Support Care Cancer 2020 Oct;28(10):4789-4801.\n[doi: 10.1007/s00520-019-05265-3] [Medline: 31974768]\n6. Littlechild SA, Barr L. Using the Internet for information about breast cancer: a questionnaire-based study. Patient Educ\nCouns 2013 Sep;92(3):413-417. [doi: 10.1016/j.pec.2013.06.018] [Medline: 23891419]\n7. Wang J, Deng H, Liu B, Hu A, Liang J, Fan L, et al. Systematic evaluation of research progress on natural language\nprocessing in medicine over the past 20 years: bibliometric study on PubMed. J Med Internet Res 2020 Jan 23;22(1):e16816\n[FREE Full text] [doi: 10.2196/16816] [Medline: 32012074]\n8. Mavragani A. Infodemiology and infoveillance: scoping review. J Med Internet Res 2020 Apr 28;22(4):e16206 [FREE\nFull text] [doi: 10.2196/16206] [Medline: 32310818]\n9. Magge A, Klein A, Miranda-Escalada A, Al-Garadi MA, Alimova I, Miftahutdinov Z, et al. Overview of the Sixth Social\nMedia Mining for Health Applications (#SMM4H) Shared Tasks at NAACL 2021. 2021 Presented at: Sixth Social Media\nMining for Health (#SMM4H) Workshop and Shared Task; June 2021; Mexico City, Mexico p. 21-32. [doi:\n10.18653/v1/2021.smm4h-1.4]\n10. Jones J, Pradhan M, Hosseini M, Kulanthaivel A, Hosseini M. Novel approach to cluster patient-generated data into\nactionable topics: case study of a web-based breast cancer forum. JMIR Med Inform 2018 Nov 29;6(4):e45 [FREE Full\ntext] [doi: 10.2196/medinform.9162] [Medline: 30497991]\n11. Gonzalez-Hernandez G, Sarker A, O'Connor K, Savova G. Capturing the patient's perspective: a review of advances in\nnatural language processing of health-related text. Yearb Med Inform 2017 Aug;26(1):214-227 [FREE Full text] [doi:\n10.15265/IY-2017-029] [Medline: 29063568]\n12. Tapi Nzali MD, Bringay S, Lavergne C, Mollevi C, Opitz T. What patients can tell us: topic analysis for social media on\nbreast cancer. JMIR Med Inform 2017 Jul 31;5(3):e23 [FREE Full text] [doi: 10.2196/medinform.7779] [Medline: 28760725]\n13. Devlin J, Chang MW, Lee K, Toutanova K. BERT: pre-training of deep bidirectional transformers for language understanding.\n2019 Presented at: NAACL-HLT 2019; June 2019; Minneapolis, MN.\n14. Saha B, Lisboa S, Ghosh S. Understanding patient complaint characteristics using contextual clinical BERT embeddings.\n2020 Presented at: 2020 42nd Annual International Conference of the IEEE EMBC; July 2020; Montreal, QC. [doi:\n10.1109/EMBC44109.2020.9175577]\n15. Nishioka S, Watanabe T, Asano M, Yamamoto T, Kawakami K, Yada S, et al. Identification of hand-foot syndrome from\ncancer patients' blog posts: BERT-based deep-learning approach to detect potential adverse drug reaction symptoms. PLoS\nOne 2022;17(5):e0267901 [FREE Full text] [doi: 10.1371/journal.pone.0267901] [Medline: 35507636]\n16. Edara DC, Vanukuri LP, Sistla V, Kolli VKK. Sentiment analysis and text categorization of cancer medical records with\nLSTM. J Ambient Intelligence Humanized Computing 2019 Jul 16:1-17. [doi: 10.1007/s12652-019-01399-8]\n17. Mediaid Corporation. Life Palette. URL: https://lifepalette.jp [accessed 2021-07-16]\n18. Miyabe M, Shimamoto Y, Aramaki E. Extracting patients’ distress of their medical care from web texts: the automatic\nclassification of cancer patients’distress. 2014 Presented at: Forum on Information Technology; September 2014; Tsukuba,\nJapan.\n19. Landis JR, Koch GG. The measurement of observer agreement for categorical data. Biometrics 1977 Mar;33(1):159-174.\n[Medline: 843571]\n20. The voices of 1,275 people who have faced breast cancer. Shizuoka Cancer Center. 2016. URL: https://www.scchr.jp/book/\nhoukokusho/2013nyugan.html [accessed 2022-05-28]\n21. Inui Laboratory Tohoku University. cl-tohoku / bert-japanese. GitHub. URL: https://github.com/cl-tohoku/bert-Japanese\n[accessed 2021-06-07]\n22. Kudo T. MeCab: yet another part-of-speech and morphological analyzer. URL: https://taku910.github.io/mecab/ [accessed\n2021-10-05]\n23. Sato T. mecab-ipadic-NEologd : Neologism dictionary for MeCab. URL: https://github.com/neologd/mecab-ipadic-neologd\n[accessed 2022-05-28]\n24. Kawazoe Y, Shibata D, Shinohara E, Aramaki E, Ohe K. A clinical specific BERT developed using a huge Japanese clinical\ntext corpus. PLoS One 2021;16(11):e0259763 [FREE Full text] [doi: 10.1371/journal.pone.0259763] [Medline: 34752490]\n25. Shimomoto K, Ando K. Automatic classification of distress in blogs written by cancer patients or their families. 2018\nPresented at: 80th Natl Conv IPSJ; March 2018; Tokyo, Japan p. 441-442.\n26. Cronin RM, Fabbri D, Denny JC, Rosenbloom ST, Jackson GP. A comparison of rule-based and machine learning approaches\nfor classifying patient portal messages. Int J Med Inform 2017 Sep;105:110-120 [FREE Full text] [doi:\n10.1016/j.ijmedinf.2017.06.004] [Medline: 28750904]\n27. Sulieman L, Gilmore D, French C, Cronin RM, Jackson GP, Russell M, et al. Classifying patient portal messages using\nConvolutional Neural Networks. J Biomed Inform 2017 Oct;74:59-70 [FREE Full text] [doi: 10.1016/j.jbi.2017.08.014]\n[Medline: 28864104]\nJMIR Cancer 2022 | vol. 8 | iss. 2 | e37840 | p. 8https://cancer.jmir.org/2022/2/e37840\n(page number not for citation purposes)\nWatanabe et alJMIR CANCER\nXSL•FO\nRenderX\nAbbreviations\nBERT: Bidirectional Encoder Representations From Transformers\nIAA: interannotator agreement\nLDA: latent Dirichlet allocation\nLSTM: long short-term memory\nNLP: natural language processing\nEdited by T Leung; submitted 11.03.22; peer-reviewed by S Nakamura, W Ceron; comments to author 01.04.22; revised version\nreceived 10.05.22; accepted 23.05.22; published 03.06.22\nPlease cite as:\nWatanabe T, Yada S, Aramaki E, Yajima H, Kizaki H, Hori S\nExtracting Multiple Worries From Breast Cancer Patient Blogs Using Multilabel Classification With the Natural Language Processing\nModel Bidirectional Encoder Representations From Transformers: Infodemiology Study of Blogs\nJMIR Cancer 2022;8(2):e37840\nURL: https://cancer.jmir.org/2022/2/e37840\ndoi: 10.2196/37840\nPMID:\n©Tomomi Watanabe, Shuntaro Yada, Eiji Aramaki, Hiroshi Yajima, Hayato Kizaki, Satoko Hori. Originally published in JMIR\nCancer (https://cancer.jmir.org), 03.06.2022. This is an open-access article distributed under the terms of the Creative Commons\nAttribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction\nin any medium, provided the original work, first published in JMIR Cancer, is properly cited. The complete bibliographic\ninformation, a link to the original publication on https://cancer.jmir.org/, as well as this copyright and license information must\nbe included.\nJMIR Cancer 2022 | vol. 8 | iss. 2 | e37840 | p. 9https://cancer.jmir.org/2022/2/e37840\n(page number not for citation purposes)\nWatanabe et alJMIR CANCER\nXSL•FO\nRenderX",
  "topic": "Breast cancer",
  "concepts": [
    {
      "name": "Breast cancer",
      "score": 0.7052730321884155
    },
    {
      "name": "Computer science",
      "score": 0.5402414798736572
    },
    {
      "name": "Encoder",
      "score": 0.5389125943183899
    },
    {
      "name": "Social media",
      "score": 0.5254035592079163
    },
    {
      "name": "Classifier (UML)",
      "score": 0.49849486351013184
    },
    {
      "name": "Concordance",
      "score": 0.4760977029800415
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4499245882034302
    },
    {
      "name": "Worry",
      "score": 0.44942450523376465
    },
    {
      "name": "Psychology",
      "score": 0.3985888659954071
    },
    {
      "name": "Machine learning",
      "score": 0.38295623660087585
    },
    {
      "name": "Natural language processing",
      "score": 0.3684019446372986
    },
    {
      "name": "Medicine",
      "score": 0.2510974407196045
    },
    {
      "name": "Cancer",
      "score": 0.22863397002220154
    },
    {
      "name": "World Wide Web",
      "score": 0.210534006357193
    },
    {
      "name": "Psychiatry",
      "score": 0.10604342818260193
    },
    {
      "name": "Internal medicine",
      "score": 0.09821003675460815
    },
    {
      "name": "Anxiety",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}