{
  "title": "Role of Language Relatedness in Multilingual Fine-tuning of Language Models: A Case Study in Indo-Aryan Languages",
  "url": "https://openalex.org/W3199212312",
  "year": 2021,
  "authors": [
    {
      "id": null,
      "name": "Tejas Dhamecha",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2619427638",
      "name": "Rudra Murthy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2169690904",
      "name": "Samarth Bharadwaj",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2031601390",
      "name": "Karthik Sankaranarayanan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2156071728",
      "name": "Pushpak Bhattacharyya",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4301187301",
    "https://openalex.org/W3028872947",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2998353611",
    "https://openalex.org/W2970557265",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3035547806",
    "https://openalex.org/W2970854433",
    "https://openalex.org/W2988257285",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3107826490",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W4287606245",
    "https://openalex.org/W2994885767",
    "https://openalex.org/W2465297109",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W4288093580",
    "https://openalex.org/W3161740710",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3099919888",
    "https://openalex.org/W2251467004",
    "https://openalex.org/W3097821138",
    "https://openalex.org/W2278252977",
    "https://openalex.org/W3102425047",
    "https://openalex.org/W3136221257",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3105005398",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2794998650",
    "https://openalex.org/W2915977242",
    "https://openalex.org/W2972688845",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2273712250",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2914120296"
  ],
  "abstract": "We explore the impact of leveraging the relatedness of languages that belong to the same family in NLP models using multilingual fine-tuning. We hypothesize and validate that multilingual fine-tuning of pre-trained language models can yield better performance on downstream NLP applications, compared to models fine-tuned on individual languages. A first of its kind detailed study is presented to track performance change as languages are added to a base language in a graded and greedy (in the sense of best boost of performance) manner; which reveals that careful selection of subset of related languages can significantly improve performance than utilizing all related languages. The Indo-Aryan (IA) language family is chosen for the study, the exact languages being Bengali, Gujarati, Hindi, Marathi, Oriya, Punjabi and Urdu. The script barrier is crossed by simple rule-based transliteration of the text of all languages to Devanagari. Experiments are performed on mBERT, IndicBERT, MuRIL and two RoBERTa-based LMs, the last two being pre-trained by us. Low resource languages, such as Oriya and Punjabi, are found to be the largest beneficiaries of multilingual fine-tuning. Textual Entailment, Entity Classification, Section Title Prediction, tasks of IndicGLUE and POS tagging form our test bed. Compared to monolingual fine tuning we get relative performance improvement of up to 150% in the downstream tasks. The surprise take-away is that for any language there is a particular combination of other languages which yields the best performance, and any additional language is in fact detrimental.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8584–8595\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n8584\nRole of Language Relatedness in Multilingual\nFine-tuning of Language Models: A Case Study in Indo-Aryan Languages\nTejas Indulal Dhamecha∗, Rudra Murthy V∗, Samarth Bharadwaj,\nKarthik Sankaranarayanan\nIBMResearch,India\n{tidhamecha,rmurthyv,samarth.b,karsank}@in.ibm.com\nPushpak Bhattacharyya\nDepartmentofComputerScienceandEngineering,IITBombay,India\npb@cse.iitb.ac.in\nAbstract\nWeexploretheimpactofleveragingtherelated-\nnessoflanguagesthatbelongtothesamefamily\nin NLP models using multilingual fine-tuning.\nWe hypothesize and validate that multilingual\nfine-tuningofpre-trainedlanguagemodelscan\nyield better performance on downstream NLP\napplications,comparedtomodelsfine-tunedon\nindividuallanguages. Afirstofitskinddetailed\nstudyispresentedtotrackperformancechange\nas languages are added to a base language in\na graded and greedy (in the sense of best boost\nof performance) manner; which reveals that\ncareful selection of subset of related languages\ncan significantly improve performance than\nutilizingallrelatedlanguages. TheIndo-Aryan\n(IA)languagefamilyischosenforthestudy,the\nexactlanguagesbeingBengali,Gujarati,Hindi,\nMarathi, Oriya, Punjabi and Urdu. The script\nbarrier is crossed by simple rule-based translit-\neration of the text of all languages to Devana-\ngari. Experiments are performed on mBERT,\nIndicBERT, MuRIL and two RoBERTa-based\nLMs,thelasttwobeingpre-trainedbyus. Low\nresource languages, such as Oriya and Punjabi,\nare found to be the largest beneficiaries of\nmultilingual fine-tuning. Textual Entailment,\nEntity Classification, Section Title Prediction,\ntasksofIndicGLUEandPOStaggingformour\ntestbed. Comparedtomonolingualfinetuning\nwegetrelativeperformanceimprovementofup\nto150%inthedownstreamtasks. Thesurprise\ntake-away is that for any language there is\na particular combination of other languages\nwhich yields the best performance, and any\nadditionallanguageisinfactdetrimental.\n1 Introduction 1\nTransformer-based(Vaswanietal. ,2017)language\nmodels (LM) have been proven to be extremely\n∗Equalcontribution\n1https://github.com/IBM/\nindo-aryan-language-family-model\nuseful in variety of natural language processing\n(NLP) tasks. Some of the most notable models are\nGPT (Radford et al., 2018), GPT-2 (Radford et al.,\n2019), GPT-3 (Brown et al., 2020), BERT (De-\nvlin et al., 2019), RoBERTa (Liu et al., 2019),\nXLNet (Yang et al., 2019), and BART (Lewis\net al., 2020). To fine-tune a pre-trained language\nmodelfordownstreamtaskshasbecomea de facto\napproachinrecentliterature.\nWe empirically study whether (and to what ex-\ntent)dotherelatedlanguagesaccentuatetheperfor-\nmance of models in downstream tasks with multi-\nlingual fine-tuning2 in comparison to monolingual\nfine-tuning. To understand the quantitative advan-\ntage of including languages gradually, we explore\nthe gradation of multilinguality by incrementally\nadding in new languages added one-by-one build-\ninguptoanall-languagemultilingualfine-tuning.\nAgoodapproximationfor language relatedness\nistheirmembershiptothesame language familyas\nlanguages of a family often share properties such\nas grammar, vocabulary, etymology, and writing\nsystems. We choose the Indo-Aryan (IA) family\nforthestudy,sinceconstituentlanguages1)include\nlow-resource languages, 2) have similar Abugida\nwriting system, 3) are relatively understudied, and\n4) are covered in a well-defined NLP benchmark\nIndicGLUE (Kakwani et al., 2020b). Further, the\nfact that all constituent languages except one use\nsimilar Abugida writing systems (rooted in the\nancient Brahmi Script3) presents an opportunity\nforscriptnormalizationviatransliteration.\nOverall,althoughthegeneralnotionoflanguage\nrelatedness is explored, and the multilingual\nfine-tuning is explored in literature, the detailed\n2Fine-tuningapre-trainedmodelwithadownstreamtask’s\ntrainingdataformultiplelanguages.\n3https://en.wikipedia.org/wiki/Brahmic_\nscripts\n8585\nlinguistic understanding of role of language\nrelatedness in multilingual fine-tuning remains\nunderstudied; evenmoreso forIAfamily. Further,\nin this context the script-conversion aspect is not\nexploredformultilingualfine-tuning.\nTo summarize, in this paper we seek to answer\nthe following research questions (employing the\nIndo-Aryan language family as the experimental\ntest-bed).\n• RQ1: Doesmultilingualfine-tuningwithaset\nofrelatedlanguagesyieldimprovementsover\nmonolingualfine-tuning(FT)ondownstream\ntasks?\n• RQ2: Starting from monolingual FT, as\neach related language is gradually added for\nmultilingual FT, to ultimately a multilingual\nFT with all related languages, how does the\nperformance vary? In other words, should\none use all related languages’ data or only a\nsub-setoflanguages’data?\nThese inquiries are critical to understanding the\nright balance between per-language fine-tuning\nand massively multilingual fine-tuning, as the\nviable way forward. Additionally, we also ex-\nplore the role of common script representation in\nmultilingualFTofrelatedlanguages.\nTo facilitate these inquiries, we utilize existing\npre-trained models, namely IndicBERT, mBERT,\nand MuRIL, and also pre-train two language\nmodels for Indo-Aryan language family from\nscratch. We utilize various tasks of IndicGLUE\n(Kakwanietal. ,2020b)asourtest-beds.\n2 Related Work\nMultilingualityaspecthasbeenexploredincontext\nofpre-traininglanguagemodels,foreffectivetrans-\nfer from one language to other, and in multilingual\nfine-tuning,toanextent.\n2.1 Multilingual Pre-training\nMultilingual LMs have enabled effective task\nfine-tuning across various languages. Notable\nexamples include, multilingual BERT (mBERT)4\nmodel trained with 104 languages, and XLM\n(LampleandConneau ,2019)andXLM-RoBERTa\n(Conneauetal. ,2020a)trainedwith100languages.\nIn the context of Indic languages, three recent\nworks on multilingual LMs have been IndicBERT,\n4https://github.com/google-research/bert/\nblob/master/multilingual.md\nMuRIL, and Indic-Transformers language models.\nIndicBERT ( Kakwani et al. , 2020b) focuses\non languages belonging to the Indo-Aryan and\nDravidian language families along with English.\nAlong with 17 Indic languages and English, Mul-\ntilingual Representations for Indian Languages\n(MuRIL)5(Khanuja et al., 2021), utilizes English-\nIndic languages parallel corpus and the Roman\ntransliterated counterparts to train an mBERT\nmodel. Similarly, Indic-Transformers ( Jain\net al., 2020) presents monolingual LMs for Hindi,\nBengali, and Telugu. Recently, various types of\nword-embeddings are also trained for each lan-\nguage(Kumaretal. ,2020;Kakwanietal. ,2020b).\nThese approaches focus on multilingual pre-\ntraining of models. This means that once a\nmultilingual LM is pre-trained, it is fine-tuned per\ntaskseparatelyforeachlanguage.\n2.2 Language Transfer\nIt is understood that a multilingual model gains\ncross-lingual understanding from sharing of layers\nthatallowsthealignmentofrepresentationsamong\nlanguages; to the extent that large overlap of the\nvocabularybetweenthelanguagesisnotrequiredto\nbridgethealignment( Conneauetal. ,2020b;Wang\net al., 2019). This property facilitates, zero-shot\ntransferbetweentworelatedlanguages(e.g. Hindi\nandUrdu)reasonablywell( Piresetal. ,2019). Per-\nformance for zero-shot transfer further improves\nwhen multilingual model is further aligned by\nutilizing parallel word or sentence resources (Kul-\nshreshtha et al., 2020). Usually, the low-resource\nlanguage members in a multilingual LM benefit\nby presence of related languages (Liu et al., 2020).\nFurther, it is likely that presence of unrelated\nlanguages do not aid the multilingual training, but\nrather may lead to negative interference rooted\nin conflicting gradients (Wang et al., 2020b) or\nyieldsubstantiallypoortransferbetweenunrelated\nlanguages (e.g. English and Japanese) (Pires et al.,\n2019). A recent work by Dolicki and Spanakis\n(2021) focuses on establishing the connection\nbetweentheeffectivenessofzero-shottransferand\nthelinguisticfeatureofsourceandtargetlanguages;\ninterestingly,theyobservethattheeffectivenessof\nzero-shottransferisafunctionofdownstreamtask,\ninadditiontothelanguagesthemselves.\nThe general understanding has been that\nlanguage-specific FT serves as skyline, and, in\n5https://tfhub.dev/google/MuRIL/1\n8586\nthesesetofworks,pursuithasbeentogetzero-shot\ntransfer from related languages(s) closer to the\nskyline(WuandDredze ,2019).\n2.3 Multilingual Fine-tuning\nTsai et al.(2019) perform multilingual fine-tuning\nof 48 languages for downstream tasks of POS tag-\nging and morphological tagging, and find these\nmultilingual models to be slightly poorer com-\npared to monolingual models. For morphologi-\ncal tagging and lemmatization tasks,Kondratyuk\n(2019) makes similar observation regarding poor\nperformance for the model fine-tuned with 66 lan-\nguages in multilingual setting compared to mono-\nlingualfine-tuning(although,asecondstageofper-\nlanguagefine-tuningyieldssuperiorperformance).\nThese findings indicate that arbitrary collection of\nlanguages may not be suitable for improving down-\nstream task performance; and that, a principled\napproach for selecting a set of languages may be\npreferable for multilingual fine-tuning.Tothisend,\nwehypothesizethatlanguagerelatednessshouldbe\nan important aspect to consider while selecting a\nlanguagesetformultilingualfine-tuning.\nPires et al.(2019) briefly explore language set\nselection based on topological features (syntactic\nwordorder). Wangetal. (2020b)exploresmultilin-\ngual fine-tuning in strictly bilingual settings. Tak-\ningthelanguagerelatednessinconsiderations, Tran\nandBisazza (2019)showthatjointfine-tuningwith\nfour European language is better than fine-tuning\nwith only English in the specific task of universal\ndependency parsing. Unfortunately, it doesn’t pro-\nvide comparison with monolingual fine-tuning of\nallconstituentlanguages.\nWe observe that there is a void regarding the\nsystematic analysis to understand how a presence\nofrelatedlanguagesinthemultilingualfine-tuning\naffectstheperformanceonthetargetlanguage.\n3 Methodology\nTraditionally, a pre-trained LM (such as mBERT)\nis used as base model, which is fine-tuned for a\ndownstream task for a specific language (mono-\nlingual). In this work, we aim to evaluate the role\nof script and language relatedness in multilingual\nfine-tuning by employing the Indo-Aryan lan-\nguagefamily. Therefore,weincludethefollowing\ncomponents to the approach: (1) multilingual\nfine-tuning, (2) transliteration, and (3) language\nmodels. Next,wediscusstheseindetail.\n3.1 Multilingual Fine-Tuning\nAs opposed to traditional monolingual fine-tuning\nfor a downstream task, in multilingual fine-tuning\nthe LM is fine-tuned once per task with the aggre-\ngate labelled corpus across languages. Intuitively,\nrelated languages should assist each other for a\ndownstream task. To draw a parallel, a polyglot\nperson(akintoamultilingualLM)goodatguessing\ntitlesofpassageswritteninonelanguagecaneasily\nadapt this skill on another, albeit related, language\nwith few exemplars. Arguably, when put together,\na greater understanding of the downstream task\narisescomparedtowhateachlanguagewouldyield\nindividually, and the relatedness of associated\nlanguages would play a key role in deciding the\nbenefitsofthisapproach.\nTherefore, to study this systematically, for a\ndownstream task that is relevant for a variety of\nlanguages (e.g. part-of-speech tagging or named\nentity recognition), first the training sets of all\nlanguages are combined to create the multilingual\ntask training set. Then, the base LM is fine-tuned\non the multilingual task training corpus. This\nmultilingual fine-tuning now yields a model per\ntask,andnotpertask-languagepair.\n3.2 Script Similarity and Transliteration\nLanguages of a language family often use similar\nwriting systems. For example, in the IA family,\non one hand, Hindi, Bhojpuri, Magahi, Marathi,\nSanskrit, and Nepali are written in Devanagari\nscript. On the other hand, Bengali, Gujarati,\nPunjabi, and Oriya are written in their respective\nscripts. As Indic languages have high lexical\nsimilarity (Bhattacharyya et al., 2016), having a\nuniversal script for all these languages allows for\nmodel to exploit cross-lingual similarities. For\nexample, the verb term for “to go” is similar in\nHindi (जाना jaanaa), Urdu (جاناjaanaa), Gujarati\n(જવું javum), Punjabi (ਜਾਣਾ jaanaa), Marathi\n(जाने jaane), and Oriya (ଯିବାକୁ jibaku), and\nBengali(যাওয়াjao)witheachlanguagemorphing\nit in different manners. We use indic-nlp-library\n(Kunchukuttan et al., 2015) (for all but Urdu) and\nindic-trans tools (Bhat et al., 2015) (for Urdu) for\ntransliterationtoDevanagari.\n3.3 Language Models\nFor this study, we use the mBERT, IndicBERT,\nand MuRIL as existing pre-trained language mod-\nels. Additionally, we pre-train language models\n8587\n(fromscratch)specificallyforonlytheIndo-Aryan\nlanguages, as other LMs contains languages of\notherfamiliestoo.\nPre-training Language Model From Scratch:\nWechoosetopre-trainRoBERTa( Liuetal. ,2019)\ntransformer-based model as it has been shown to\nimprove over BERT (Devlin et al., 2019) recently.\nExisting pre-trained language models are trained\non original script data. For fair study of effective-\nnessoftransliteration,wewishtopre-trainseparate\nlanguage models on original and transliterated\ncorpus from scratch. Our experimentation around\ntransliteration makes existing pre-trained models\n(mBERT, IndicBERT, and MuRIL) somewhat in-\ncompatible. Inotherwords,itwouldbeakintofine-\ntuningforanunseenlanguage,albeitinapreviously\nseen script. Thus, we settle upon pre-training con-\ntextual LM from scratch for this purpose. Specif-\nically, we train two LMs from scratch, one pre-\nserving the original scripts of corpora (IndoAryan-\nOriginal) and other after transliterating all corpora\ntoDevanagariscript(IndoAryan-Transliterated).\n4 Experimental Setup\nIn this section, we describe the datasets used\nin our experiments, their pre-processing, and\nimplementationdetails.\n4.1 Data\nTo train the language models, we obtained text\ndata from various sources including: Wikipedia\nDump6, WMT Common Crawl 7, WMT News\nCommonCrawl8, Urdu Charles University Corpus\n(Bojaretal. ,2014;Jawaidetal. ,2014),IITBombay\nHindi Monolingual Corpus (Kunchukuttan et al.,\n2018),BhojpuriMonolingualCorpus( Kumaretal. ,\n2018),andMagahiMonolingualCorpus 9. Various\nstatistics of the collected corpus are reported in\nTable1. Notethemajorimbalanceinthedatawith\nHindibeingundoubtedlyahigh-resourcelanguage\nand likes of Magahi, Punjabi, and Oriya being\nlow-resource languages. The challenges of the\ndata imbalance and insufficiency of data to train\nmonolingual models for many of these languages\nisapparentfromthestatistics.\n6https://dumps.wikimedia.org/\n7http://data.statmt.org/ngrams/raw/\n8http://data.statmt.org/news-crawl/\n9https://github.com/kmi-linguistics/magahi\nLanguage # Sentences# Tokens# Total # Unique\nHindi(hi) 1552.89 20,098.73 25.01Bengali(bn) 353.44 4,021.30 6.5Sanskrit(sa) 165.35 1,381.04 11.13Urdu(ur) 153.27 2,465.48 4.61Marathi(mr) 132.93 1,752.43 4.92Gujarati(gu) 131.22 1,565.08 4.73Nepali(ne) 84.21 1,139.54 3.43Punjabi(pa) 68.02 945.68 2.00Oriya(or) 17.88 274.99 1.10Bhojpuri(bh) 10.25 134.37 1.13Magahi(mag) 0.36 3.47 0.15\nTable1: Statistics(inMillions)ofmonolingualcorpora\nusedinpre-trainingIndoAryanLMsfromscratch.\n4.2 Data Preparation and Implementation\nDetails\nAs the first step, sentences are segmented from the\ntext corpora. Then script converted version of the\ndatasets is obtained by transliterating Bengali, Gu-\njarati, Punjabi, Oriya, and, Urdu into Devanagari\nscript. We additionally perform de-duplication\nto remove repeated sentences. The statistics of\nthe resulting set are reported in Table 1. We\nidentify following two challenges that can affect\npre-training negatively: 1) data imbalance and 2)\ncomputerequirements.\n1.Data imbalance:AsreportedinTable 1thesize\nof each language corpora differs up to four orders\nof magnitude, e.g. Hindi has 1552M sentences\nvs 0.36M in Magahi and 17.88M in Oriya. The\nlanguage bias can creep into the tokenizer and the\nlanguage model pre-training, if left unattended.\nTo mitigate bias in tokenizer training, we utilize a\nre-samplingstrategytoreducethedataskew( Lam-\nple and Conneau, 2019). Specifically, samples\nare drawn following multinomial distribution with\nadjusted probabilities. Adjusted probabilities are\ncomputed asqi = pα\ni /∑\nj pα\nj where pi = ni/∑\nini,\nand ni is the number of samples inith language.\nBefore rescaling, the language distribution is in\nthe range of 0.01%-58%, which changes to 5-12%\nafterwards.\n2. Compute requirements: Depending on the\ncomputing infrastructure, running one training\nepoch can typically take few hundreds to (single\ndigit) few thousands of GPU hours. To mitigate\nthis, we utilize a variant of sharding technique\noutlined in Algorithm 1 to pre-train the model\nin infrastructure with limited memory (<50GB)\n8588\nAlgorithm 1:Economical LM training for\nlanguagefamily\nResult: TrainedLMcheckpoint\nrandomlyselectworkinglanguageset;\nfor each language in working setdo\nrandomlyselect\nleastnumberofblockscontaining\nx%sentenceofthelanguage;\nfor each blockdo\nif block is not cachedthen\ntokenizetheblock;\npersistontostorage;\nend\nreadtokenizedblockintodataset;\nend\nend\nif checkpoint foundthen\ninitializemodelwithcheckpoint;\nelse\nrandomlyinitializemodel;\nend\ntrainLMtominimize\nMLMlossonbalancedmini-batches;\nand compute (one v100 GPU). It depends on\ndividing each language corpus into manageable\n(into memory) chunks, termed as blocks. Each\nLM is trained over∼50 sequential executions of\nAlgorithm 1 on a single v100 GPU machine and\neachexecutionrunningforaday,consumingabout\n1200hoursoverallforpre-training.\nIn the re-sampling step, exponents = 0.1 and\nscaling parameter of γ = 100 are used. Byte-\nlevel BPE tokenizer (Radford et al., 2019; Wang\netal.,2020a)isusedwithvocabularysizeof110K.\nTrainedLMsuse12layers,12attentionheads,hid-\nden size of 768, and dropout ratio for the attention\nprobabilitiesof0.1. OurimplementationusesHug-\ngingface (Wolf et al., 2020) library. We use lin-\near schedule for learning rate decay. Maximum\nsequence length is set as 128 across tokenization,\ntraining, and fine-tuning. Due to compute limi-\ntations, having higher maximum sequence length\nleadtoout-of-memoryerrors. Mini-batchesarecre-\nated by weighted sampling based on language pri-\norswithexponent s=0.7. InLMpre-training,mini-\nbatch of 48 samples and gradient accumulation of\n53isusedmakingtheeffectivebatchsizeas2,544 10.\n10Loss curves of LM pre-training in supplementary\nmaterial.\nApex11 library is used withO1 optimization level\nto allow mixed precision training. In all our ex-\nperimentsonfine-tuning,weperformagrid-search\nwith respect tolearning-rate andbatch sizevalues\nof{1,3,5}×10−5 and{16,32,64} respectively.\n5 Experiments\nTo answer the research questions, we experiment\non a variety of tasks suitable for multilingual\nfine-tuning and analyse the results. To investigate\nRQ1,in§ 5.1,thefirstsetofexperimentsareaimed\nto understand the utility of multilingual FT with\nrelated languages. To investigate RQ2, in §5.2,\nthesecondsetofexperimentsaredesignedtotrack\ngradual performance variation with addition of\nassisting languages. With last set of analysis, in\n§5.3,weinvestigatetheroleoftransliteration.\n5.1 Effectiveness on Multilingual Tasks\nWe experiment on four tasks suitable for multi-\nlingual fine-tuning protocol, including three from\nIndicGLUE (Kakwani et al., 2020b) and POS\ntagging(Zemanetal. ,2020).\n1. Textual Entailment task on copa-translated\nandwnli-translateddataset(Hi,Gu,Mr)\n2. Title Prediction task on wiki-section-title\ndataset(Hi,Bn,Gu,Mr,Or,Pa)\n3. Named Entity Recognitiontask on wikiann-\nnerdataset(Hi,Bn,Gu,Mr,Or,Pa)\n4. Part-of-Speech Tagging task on Universal\nDependencydatasets(Hi,Mr,Ur)\nWe do not show results on Cloze-style Question\nAnswering task of IndicGLUE as it is meant to\nevaluate masked-token prediction of an LM, and\ndoesnotinvolvedownstreamtasktraining.\nWe utilize mBERT, IndicBERT, MuRIL,\nIndoAryan-Original (IA-O) and IndoAryan-\nTransliterated(IA-TR)–lasttwobeingpre-trained\nby us as detailed in §3.3. All the five LMs are\nfine-tunedinmonolingualandmultilingualmodes,\nto pursue investigation for RQ 1. Only the IA-TR\nmodel is fine-tuned with transliterated versions of\nthe downstream task data; remaining four models\narefine-tunedwithoriginalscriptdownstreamtask\ndata. The results of this set of experiments are\nreported in Table2. Along with absolute metrics,\nthe relative difference between mono- and multi-\n11https://github.com/nvidia/apex\n8589\n.\nLG mBERT IndicBERT MuRIL IA-Transliterated IA-Original\nFT → Mono Multi δMB Mono Multi δIB Mono Multi δMR Mono Multi δTR Mono Multi δO\nTextual Entailment /copa-translated(Accuracy)\nhi 0.6590 0.5909 (-10.33) 0.6250 0.6705 (+7.28) 0.5568 0.6477 (+16.33) 0.6591 0.5455 (-17.24) 0.5796 0.5796 (0.00)\ngu 0.4318 0.5795 (+34.21) 0.5341 0.6591 (+23.40) 0.5909 0.5795 (-1.93) 0.5909 0.6250 (+5.77) 0.5113 0.5227 (+2.23)\nmr 0.5568 0.5909 (+6.12) 0.5909 0.6591 (+11.54) 0.5682 0.6363 (+11.99) 0.5909 0.6023 (+1.93) 0.6590 0.5454 (-17.24)\nTextual Entailment /wnli-translated(F-Score)\nhi 0.3604 0.4020 (+11.54) 0.3604 0.4855 (+34.71) 0.3604 0.3604 (0.00) 0.3604 0.3604 (0.00) 0.3616 0.3604 (-0.33)\ngu 0.3604 0.4123 (+14.40) 0.3604 0.5071 (+40.70) 0.3604 0.3604 (0.00) 0.3107 0.3604 (+15.99) 0.3604 0.3604 (0.00)\nmr 0.4167 0.4920 (+18.07) 0.3604 0.4685 (+29.99) 0.3914 0.3604 (-7.92) 0.4228 0.3576 (-15.42) 0.3604 0.3604 (0.00)\nEntity Classification/wikiann-ner(F-Score)\nhi 0.8656 0.9197 (+6.25) 0.9031 0.9229 (+2.19) 0.9237 0.9446 (+2.26) 0.8720 0.8897 (+2.03) 0.8476 0.8776 (+3.54)\nbn 0.9181 0.9671 (+5.34) 0.9339 0.9654 (+3.37) 0.9503 0.9645 (+1.49) 0.9211 0.9397 (+2.02) 0.9486 0.9557 (+0.75)\ngu 0.6804 0.8893 (+30.70) 0.7021 0.9027 (+28.57) 0.8016 0.9058 (+12.99) 0.7306 0.7796 (+6.71) 0.8318 0.8650 (+3.99)\nmr 0.9127 0.8882 (-2.68) 0.8871 0.8965 (+1.06) 0.9199 0.9274 (+0.82) 0.8675 0.8675 (0.00) 0.8482 0.8605 (+1.45)\nor 0.1905 0.3457 (+81.47) 0.3509 0.8896 (+153.52) 0.3882 0.8848 (+127.92) 0.3460 0.6436 (+86.01) 0.5737 0.7038 (+22.68)\npa 0.5000 0.8667 (+73.34) 0.4444 0.8593 (+93.36) 0.8535 0.9086 (+6.45) 0.3491 0.5789 (+65.83) 0.6313 0.7456 (+18.10)\nTitle Prediction/wiki-section-title(Accuracy)\nhi 0.8012 0.8081 (+0.86) 0.7780 0.7976 (+2.52) 0.8528 0.8622 (+1.10) 0.6779 0.6994 (+3.17) 0.6761 0.6807 (+0.68)\nbn 0.8253 0.8285 (+0.39) 0.8266 0.8157 (-1.32) 0.8781 0.8874 (+1.06) 0.6135 0.7111 (+15.91) 0.7062 0.7097 (+0.49)\ngu 0.7452 0.7858 (+5.45) 0.6879 0.8074 (+17.37) 0.8465 0.8745 (+3.31) 0.2614 0.6970 (+166.64) 0.4044 0.6890 (+70.38)\nmr 0.8049 0.8438 (+4.83) 0.7744 0.8247 (+6.49) 0.8529 0.8913 (+4.50) 0.5299 0.7167 (+35.25) 0.5031 0.7083 (+40.78)\nor 0.2222 0.2829 (+27.31) 0.6825 0.8108 (+18.79) 0.8167 0.8845 (+8.30) 0.2928 0.7231 (+146.96) 0.3147 0.6852 (+117.73)\npa 0.7247 0.7648 (+5.53) 0.7754 0.7903 (+1.92) 0.8240 0.8487 (+2.99) 0.2817 0.7064 (+150.76) 0.6235 0.6800 (+9.06)\nPart-of-Speech Tagging /ud-pos(F-Score)\nhi 0.9693 0.9698 (+0.05) 0.9755 0.9748 (-0.07) 0.9779 0.9778 (-0.01) 0.9618 0.9636 (+0.19) 0.9562 0.9528 (-0.36)\nmr 0.9024 0.8932 (-1.02) 0.8024 0.8886 (+10.74) 0.5388 0.7478 (+38.79) 0.8114 0.8290 (+2.17) 0.7906 0.7926 (+0.25)\nur 0.9102 0.8149 (-10.47) 0.9047 0.8249 (-8.82) 0.9168 0.8497 (-7.32) 0.9026 0.9080 (+0.60) 0.8915 0.7858 (-11.85)\nTable2: Comparisonofmultilingualfine-tuningvsmonolingualfine-tuningofvariousIndo-AryanLMs. Thefigureinparenthesisisrelativedifference δ (Eq. 1). Themonolingual\nresults for mBERT and IndicBERT models are as reported byKakwani et al.(2020a) except for wnli-translated and POS-Tagging tasks. LG and FT stand for language and\nfine-tuning,respectively.\n8590\n127.9\n6.5 13.0 0.8 2.3 1.5\n0.0\n20.0\n40.0\n60.0\n80.0\n100.0\n120.0\n140.0\nOr\n (n=1077)\nPa\n (n=1408)\nGu\n (n=2692)\nHi\n (n=9510)\nMr\n (n=12335)\nBn\n (n=20223)\nδ (%)\nLanguages\n8.3\n3.0 3.3\n4.5\n1.1 1.1\n0.0\n1.0\n2.0\n3.0\n4.0\n5.0\n6.0\n7.0\n8.0\n9.0\nOr\n (n=4015)\nPa\n (n=8772)\nGu\n (n=10004)\nMr\n (n=10446)\nHi\n (n=44069)\nBn\n (n=47580)\nδ (%)\nLanguages\nFigure 1: Relative performance differenceδ (Eq. 1) for\nMuRIL model on (left) NER and (right) title prediction\ntasks. n isnumberoftrainingsamples\n.\nlingual fine-tuning (FT) is also reported. The\nrelativedifferenceiscalculatedas\nδ =100 ×Mmulti−Mmono\nMmono\n(1)\nwhere Mmono and Mmulti are performance mea-\nsures of monolingual and multilingual fine-tuning\nrespectively. Keyobservationsareasfollowing:\nMonolingual vs Multilingual Fine-Tuning:\nIn this analysis, higher theδ, the stronger is the\nanswer to RQ1 in affirmation. In the Table2 the\npositive δ, shown in blue color, indicates the cases\nwheremultilingualFTimprovesovermonolingual\nFT. It can be observed that for languages with\nlimited labelled data for the downstream task, the\nmultilingual fine-tuning is resulting in enormous\nimprovements. Across all the five LMs, the trend\nis consistent. For example, on wikiann-ner task, F-\nScore on Oriya improved from 0.3882 and 0.3460\nto 0.8848 and 0.6436, respectively for MuRIL and\nIA-TRmodels;whilesignificantimprovementsare\nseeninotherlanguagestoo. Similartrendisvisible\nin wiki-section-title prediction task too, where im-\nprovementsareseenforallthelanguages. Broadly,\nacross LMs, tasks, and languages, the multilingual\nFTshowsimprovementovermonolingualFT.This\nhelps formulate the answer to RQ1 asthe multilin-\ngual fine-tuning with related languages can yield\nhuge (up to 40% on absolute scale) improvement\nfor low-resource languages (such as Oriya and\nPunjabi), and statistically significant (up to 10%)\nimprovements on high resource languages (such\nas Hindi and Bengali), depending on the task.\nNote, that this is in contrast to the observations\nofTsaietal. (2019)and Kondratyuk(2019)thatin-\ndicate slightly poor performance with multilingual\nfine-tuning. They fine-tune with more than forty\nlanguages together, without considering language\nrelatedness. We observe large improvements by\nText ਪੰਜਾਬ ਦੇ ਮੁੱਖ ਮੰਤਰੀਆਂਦੀ ਸੂਚੀ\nPronounciationpajAbademukhamatarIAMdI sUchI\nWord TranslatePunjabof Chief Ministerof List\nGT\nMono\nMulti\nOrganization\nLocation Person\nOrganization\n(a)Entitytagchange\nText ਫ਼ਰਾਂਸੀਸੀ ਗੁਈਆਨਾ ਫ਼ਰਾਂਸ  ਦਾ ਇੱਕ ਿਵਦੇਸ਼ੀ ਿਵਭਾਗ\nPronounciationpharAMsIsIguIAnApharAMsadA ika videsaI vibhAga\nWord TranslateFrench Guyana France of a ForeignDepartment\nGT O O\nMono O Location\nMulti O O\nLocation\nLocation\nLocation\n(b)Spuriousentityextractionrectified\nFigure 2: Examples of prediction improvement by mul-\ntilingualFTcomparedtomonolingualFT,forNERtask\nfortwoexamplesofPunjabiwritteninGurmukhiscript.\nselecting only the languages of the family for\nmultilingualfine-tuning.\nTrade-off or Win-Win?: Figure 1 visualizes\nthe the improvements by multilingual fine-tuning\nrelative to the monolingual one along with the task\ntraining set size. It is clearly evident, the smaller\nthe task training data set, the higher is the relative\nimprovement. Arguably, the data limitation of a\nlow resource language is abridged by the related\nhigh resource languages. These improvements\nare not at the cost of trading-off on high resource\nlanguages; it is win-win for all languages. In fact,\na decrease in performance (δ < 0), indicated with\nredcolor,isobservedinonly16outofthetotal105\n(21 task-language pairs × 5 LMs) comparisons.\nInterestingly, there is no task-language pair in\nwhich δ values corresponding to all the five LMs\nare negative, i.e. for all task-language pairs at\nleast one LM always showed improvement using\nmultilingual fine-tuning. Figure 2 illustrates the\ntypesofimprovementsinpredictingentitytags.\nBest LM Across Tasks?:Arguably, itisunfairto\ncomparethepre-trainedLMsduetovastdifference\ninthenumberoflanguagestheyarepre-trainedfor\n(in range from 11 to 104), the size of the corpora,\nnatureofthecorpora(onlymonolingualvsparallel\ncorpora), model types (RoBERTA, AlBERT,\nBERT), number of layers (8 or 12), tokenization,\npre-training objectives, and compute consumed in\ntraining. Further,mBERTmodelisnotpre-trained\nwithOriya. However,itisnaturaltoinquireifthere\nis a clear winner LM in the experimentation. The\nboldface figures in Table2 shows the best results\npertaskperlanguage. Mostofthebestmetricsfall\nunder either IndicBERT (for Textual Entailment\ntasks) or MuRIL models (for Title Prediction and\n8591\nmostly for Entity Classification tasks). The lowest\nperformanceobtainediswithIA-OandIA-TR.\n5.2 Gradation of Multilinguality\nWefurtherdwellintounderstandingwhetherthede-\ngree of improvement varies by the language close-\nnesswithinlanguagefamily. Specifically,westart\nwiththemonolingualtraining, i.e. trainingsetcon-\ntainsonlythetargetlanguage. Then,weexperiment\nby adding each related language to the training set\nseparately. Thelanguagethatyieldshighestperfor-\nmanceboostisselectedforaddingtothetrainingset.\nThus,anewtrainingsetconsistingoftwolanguages\nisobtained. Thisisrepeateduntilalltherelatedlan-\nguagesareaddedtothetrainingset,resultinginthe\nall-languagemultilingualFT.Thisapproachissim-\nilarto Sequential Forward Selection of Featuresin\nmachinelearning. Further,werelatethesubfamily\ncategorizationofIAfamilyforthisanalysis.\nExperiments are performed for NER task on\nMuRILmodel,withOriyaandPunjabiastheevalu-\nation language. The results are reported in Table3.\nFordetaileddiscussion, considerthecaseofOriya,\nononeendofthespectrum,inthefirstrow,wehave\na monolingual fine-tuned model with only Oriya,\nwhereasontheotherendofthespectrum,inthelast\nrow, we have a multilingually fine-tuned model\nwith Oriya, Bengali, Hindi, Gujarati, Marathi, and\nPunjabi. In the middle span, we have Oriya aided\nby each – Bengali, Punjabi, Marathi, Gujarati,\nand Hindi, separately; and their combinations.\nSince adding Gujarati to Oriya yields best result\ncompared to adding any other language, or+gu\nis taken as the base training set for next iteration.\nIn the next iteration, adding Bengali to or+gu\nprovides highest boost, thusor+gu+bn forms the\nbase iteration for next iteration. A similar exercise\nisperformedwithPunjabiasthebaselanguagetoo.\nIn case of Oriya, adding the Gujarati data re-\nsulted in about 54 percentage points improvement\n(38.8% to 92.4%), which is further improved by\nabout 0.5% with addition of Bengali (93.0%). It\nappears that Hindi, Punjabi, and Marathi each neg-\nativelyinterfereswith or+gu+bn set,resultinginat\nleast 1.5 percentage points performance drop. The\nbest performance of 94.19% is obtained with the\nor+gu+bn+mr+hi set (i.e. all but Punjabi), which\nis5.7percentagepointshigherthanconsideringall\nthelanguagestogether.\nIt is natural to ask if Punjabi has negative inter-\nference with Oriya for the task.or+pa yields 47.7\npercentage points improvement over only Oriya,\nwhich indicates positive interference between\nthem. Further, or+gu and or+gu+pa are almost\nsimilar (0.9245 and 0.9231) indicates that perhaps,\npa is redundant togu for assistingor on the task.\nHowever,adding pa toor+gu+bn,or+gu+bn+mr,\nand or+gu+bn+mr+hi results in 2-5% drop; the\ncommon denominator being Bengali, it seems that\nPunjabi harms the most when the base set contains\nBengali. Arguably, it indicates negative interfer-\nencebetweenBengaliandPunjabiforthetask.\nAlso, note the improvements are not correlated\nwith increase in the training set size; instead the\nsmallersets(e.g. or+gu with3425samples)yields\nbetterresultsthanlargersets(e.g. or+bn of21379).\nTherefore, gradual deviations should be credited\nto the addition language rather than the training\nset inflation. overall, the answer to RQ 2 emerges\nwithin the set of related languages, likely, there\nexists a subset of languages that yields the best\nperformance.\n5.3 Transliteration\nNext, we present a set of observations pertaining\ntotheutilityoftransliterationtoleveragethescript\nsimilarity between the Indo-Aryan languages.\nFor a fair comparison, the IA-Original and IA-\nTransliterated models are considered as both of\nthemarepre-trainedbyusontheoriginalscriptand\ntransliterated script versions of the same corpora.\nThus, in this part of analysis, higher theδTR −δO\nin Table2, the stronger is the role of explicit script\nnormalization.\nTransliteration with Multilingual FT:Compar-\ningthe relative difference (δ)for transliterated and\noriginal script models, it is observed that in 16 out\nof 21 task-language pairsδTR > δ O; noteworthy\nis δTR = 146.96%, andδO = 117.73% for Oriya\nlanguage on wiki-section-title prediction task. It\nsuggeststhatmultilingualFTisevenmoresuitable\nwithtransliteration. Basedontheseresults,therole\nof common script representation emerges aseffec-\ntiveness of multilingual fine-tuning is significantly\nenhanced when coupled with common-script\nrepresentation via transliteration.\nTransliteration for LM:Comparingperformance\nof monolingual fine-tuning of the original script\nLM(IA-O)andthetransliteratedscriptLM(IA-TR)\nrevealsthatthelaterisbetterinonlyfew(8outof21)\n8592\nTrain Set Size F-Score\nor 1078 0.3882\nBaseset: or\n+ gu 3425 0.9245\n+bn 21379 0.8836\n+hi 10590 0.8795\n+pa 2487 0.8657\n+mr 13415 0.8649\nBaseset: or+gu\n+ bn 23725 0.9301\n+pa 4884 0.9231\n+hi 12936 0.8836\n+mr 15761 0.8855\nBaseset: or+gu+bn\n+ mr 36061 0.9151\n+pa 25184 0.9036\n+hi 33236 0.8916\nBaseSet: or+gu+bn+mr\n+ hi 45572 0.9419\n+pa 37520 0.8922\nAll 46665 0.8848\nTrain Set Size F-Score\npa 1409 0.8535\nBaseset: pa\n+ bn 21579 0.9156\n+mr 13795 0.8883\n+hi 10970 0.8759\n+gu 3805 0.8673\n+or 2487 0.8426\nBaseset: pa+bn\n+ hi 31270 0.9286\n+mr 34095 0.9160\n+or 22838 0.9137\n+gu 24105 0.9105\nBaseset: pa+bn+hi\n+ mr 43606 0.9211\n+or 32349 0.9156\n+gu 33616 0.9132\nBaseset: pa+bn+hi+mr\n+ gu 45952 0.9567\n+or 44685 0.9231\nAll 46665 0.9086\nTable 3: Study of graded addition of languages for NER task on low resource languages of (left) Oriya and (right)\nPunjabi using MuRIL. The scheme of adding the languages is similar to Greedy Forward Selection of features in\nMachineLearning.\nexperiments. This is somewhat counter intuitive,\nas the common script representation should have\nmadetheLMpre-trainbetterduetothepresenceof\ncognates. Wespeculatefollowingtworationales.\n• Firstly, it indicates that, perhaps, even\nwithout explicit alignment of cognates (via\ntransliteration)themodelisabletoaligntheir\nembeddings implicitly, corroborating with\n(Conneauetal. ,2020b;Piresetal. ,2019).\n• Secondly,thebyte-LevelBPEandtheunicode\nblockarrangementsforIndo-Aryanlanguages\nmay be at play underneath this phenomena\nalso. For example, the consonant Pa in Hindi\nप (0xe00xa40xaa),Oriya ପ (0xe00xac0xaa),\nand Punjabiਪ (0xe0 0xa8 0xaa) are apart by\ntheir unicode block offset differences. Thus,\npotentially, a model knowing the byte level\nrepresentations of the writing system could\nlearn to map them, provided the loss function\nguidesit.\nHowever, we leave the further inquiry into the\nexactphenomenaforthefuturework.\n6 Conclusion\nWe show that multilingual fine-tuning efficiently\nleverages language relatedness leading to im-\nprovements over monolingual approach. We\nsubstantiatethisclaimontheIndo-Aryanlanguage\nfamily with experiments on five language models.\nMultilingual fine-tuning is particularly effective\nfor low-resource languages (e.g., Oriya and Pun-\njabi show improvement up to 150% on relative\nscale). Also, we show that careful selection of\nsubset of related languages, can further improve\nperformance. Devising automatic approaches\nfor finding optimal subset of related languages\nis a promising future direction. Additionally, in\nmultilingual fine-tuning, we see some benefits of\ntransliterationtocommonscript.\nReferences\nIrshad Ahmad Bhat, Vandan Mujadia, Aniruddha\nTammewar, Riyaz Ahmad Bhat, and Manish Shri-\n8593\nvastava. 2015. IIIT-H System Submission for\nFIRE2014 Shared Task on Transliterated Search. In\nProceedings of the Forum for Information Retrieval\nEvaluation, FIRE ’14, pages 48–53, New York, NY,\nUSA.ACM.\nPushpakBhattacharyya,MiteshM.Khapra,andAnoop\nKunchukuttan. 2016.Statistical machine translation\nbetween related languages. In Proceedings of the\n2016 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Tu-\ntorial Abstracts,pages17–20,SanDiego,California.\nAssociationforComputationalLinguistics.\nOndřejBojar,VojtěchDiatka,PavelStraňák,AlešTam-\nchyna, and Daniel Zeman. 2014. HindEnCorp 0.5.\nLINDAT/CLARIAH-CZ digital library at the Insti-\ntuteofFormalandAppliedLinguistics(ÚFAL),Fac-\nultyofMathematicsandPhysics,CharlesUniversity.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot\nlearners.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020a.Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n8440–8451, Online. Association for Computational\nLinguistics.\nAlexis Conneau, Shijie Wu, Haoran Li, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020b. Emerging\ncross-lingualstructureinpretrainedlanguagemodels .\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n6022–6034, Online. Association for Computational\nLinguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages4171–4186.\nBłażej Dolicki and Gerasimos Spanakis. 2021.\nAnalysing The Impact Of Linguistic Features\nOnCross-LingualTransfer .\nKushal Jain, Adwait Deshpande, Kumar Shrid-\nhar, Felix Laumann, and Ayushman Dash. 2020.\nIndic-Transformers: An Analysis of Transformer\nLanguage Models for Indian Languages. arXiv\npreprint arXiv:2011.02323.\nBushra Jawaid, Amir Kamran, and Ondřej Bojar. 2014.\nUrdumonolingualcorpus . LINDAT/CLARIAH-CZ\ndigital library at the Institute of Formal and Applied\nLinguistics (ÚFAL), Faculty of Mathematics and\nPhysics,CharlesUniversity.\nDivyanshu Kakwani, Anoop Kunchukuttan, Satish\nGolla, NC Gokul, Avik Bhattacharyya, Mitesh M\nKhapra, and Pratyush Kumar. 2020a. inlpsuite:\nMonolingual corpora, evaluation benchmarks and\npre-trained multilingual language models for indian\nlanguages. InProceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing:\nFindings,pages4948–4961.\nDivyanshu Kakwani, Anoop Kunchukuttan, Satish\nGolla, Gokul N.C., Avik Bhattacharyya, Mitesh M.\nKhapra, and Pratyush Kumar. 2020b. IndicNLP-\nSuite: Monolingual corpora, evaluation benchmarks\nand pre-trained multilingual language models for\nIndian languages. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020,pages\n4948–4961, Online. Association for Computational\nLinguistics.\nSimran Khanuja, Diksha Bansal, Sarvesh Mehtani,\nSavya Khosla, Atreyee Dey, Balaji Gopalan,\nDilip Kumar Margam, Pooja Aggarwal, Rajiv Teja\nNagipogu, Shachi Dave, Shruti Gupta, Subhash\nChandra Bose Gali, Vish Subramanian, and Partha\nTalukdar. 2021. MuRIL: Multilingual Representa-\ntionsforIndianLanguages .\nDan Kondratyuk. 2019. Cross-lingual lemmatization\nandmorphologytaggingwithtwo-stagemultilingual\nBERT fine-tuning. In Proceedings of the 16th\nWorkshop on Computational Research in Phonetics,\nPhonology, and Morphology,pages12–18,Florence,\nItaly.AssociationforComputationalLinguistics.\nSaurabh Kulshreshtha, Jose Luis Redondo Garcia, and\nChing Yun Chang. 2020. Cross-lingual Alignment\nMethods for Multilingual BERT: A Comparative\nStudy. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing:\nFindings,pages933–942.\nRitesh Kumar, Bornini Lahiri, Deepak Alok, Atul Kr\nOjha,MayankJain,AbdulBasit,andYogeshDawer.\n2018. Automatic identification of closely-related\nIndianlanguages: Resourcesandexperiments. arXiv\npreprint arXiv:1803.09405.\nSaurav Kumar, Saunack Kumar, Diptesh Kanojia, and\nPushpak Bhattacharyya. 2020.“a passage to India”:\nPre-trained word embeddings for Indian languages.\nIn Proceedings of the 1st Joint Workshop on Spoken\nLanguage Technologies for Under-resourced lan-\nguages (SLTU) and Collaboration and Computing\n8594\nfor Under-Resourced Languages (CCURL), pages\n352–357, Marseille, France. European Language\nResourcesassociation.\nAnoopKunchukuttan,PratikMehta,andPushpakBhat-\ntacharyya. 2018. The IIT Bombay English-Hindi\nparallel corpus. In Proceedings of the Eleventh\nInternational Conference on Language Resources\nand Evaluation (LREC-2018), Miyazaki, Japan. Eu-\nropeanLanguagesResourcesAssociation(ELRA).\nAnoopKunchukuttan,RatishPuduppully,andPushpak\nBhattacharyya. 2015. Brahmi-net: A translitera-\ntion and script conversion system for languages\nof the Indian subcontinent. In Proceedings of the\n2015 Conference of the North American Chapter\nof the Association for Computational Linguistics:\nDemonstrations, pages 81–85, Denver, Colorado.\nAssociationforComputationalLinguistics.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining.arXiv preprint\narXiv:1901.07291.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nforComputationalLinguistics.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLukeZettlemoyer.2020. Multilingualdenoisingpre-\ntrainingforneuralmachinetranslation. Transactions\nof the Association for Computational Linguistics,\n8:726–742.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\nMandarJoshi,DanqiChen,OmerLevy,MikeLewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHowmultilingualismultilingualBERT? InProceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 4996–5001,\nFlorence, Italy. Association for Computational\nLinguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving lan-\nguage understanding by generative pre-training\n(2018). URL https://s3-us-west-2. amazonaws.\ncom/openai-assets/research-covers/language-\nunsupervised/language_ understanding_paper. pdf.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodelsareunsupervisedmultitasklearners.\nKe Tran and Arianna Bisazza. 2019.Zero-shot depen-\ndencyparsingwithpre-trainedmultilingualsentence\nrepresentations. InProceedings of the 2nd Workshop\non Deep Learning Approaches for Low-Resource\nNLP (DeepLo 2019), pages 281–288, Hong Kong,\nChina.AssociationforComputationalLinguistics.\nHenry Tsai, Jason Riesa, Melvin Johnson, Naveen Ari-\nvazhagan, Xin Li, and Amelia Archer. 2019.Small\nand practical BERT models for sequence labeling.\nInProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 3632–3636,\nHong Kong, China. Association for Computational\nLinguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is\nall you need. In Advances in neural information\nprocessing systems,pages5998–6008.\nChanghan Wang, Kyunghyun Cho, and Jiatao Gu.\n2020a. Neural machine translation with byte-level\nsubwords. InProceedings of the AAAI Conference on\nArtificial Intelligence,volume34,pages9154–9160.\nZirui Wang, Zachary C. Lipton, and Yulia Tsvetkov.\n2020b. On Negative Interference in Multilingual\nModels: Findings and A Meta-Learning Treatment.\nIn Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing\n(EMNLP), pages 4438–4450, Online. Association\nforComputationalLinguistics.\nZirui Wang, Jiateng Xie, Ruochen Xu, Yiming Yang,\nGraham Neubig, and Jaime G. Carbonell. 2019.\nCross-lingual alignment vs joint training: A compar-\native study and A simple unified framework. CoRR,\nabs/1910.04708.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nricCistac,TimRault,RémiLouf,MorganFuntowicz,\nJoeDavison, SamShleifer, PatrickvonPlaten, Clara\nMa,YacineJernite,JulienPlu,CanwenXu,TevenLe\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest,andAlexanderM.Rush.2020. Transformers:\nState-of-the-Art Natural Language Processing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nforComputationalLinguistics.\nShijie Wu and Mark Dredze. 2019.Beto, bentz, becas:\nThe surprising cross-lingual effectiveness of BERT.\nInProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP) , pages 833–844,\nHong Kong, China. Association for Computational\nLinguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\n8595\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in neural\ninformation processing systems,pages5753–5763.\nDanielZeman,JoakimNivre,MitchellAbrams,andElia\nAckermann et al.2020. Universal dependencies 2.7.\nLINDAT/CLARIAH-CZ digital library at the Insti-\ntuteofFormalandAppliedLinguistics(ÚFAL),Fac-\nultyofMathematicsandPhysics,CharlesUniversity.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8190557360649109
    },
    {
      "name": "Telugu",
      "score": 0.8056485652923584
    },
    {
      "name": "Devanagari",
      "score": 0.7203428745269775
    },
    {
      "name": "Hindi",
      "score": 0.7194566130638123
    },
    {
      "name": "Natural language processing",
      "score": 0.6515512466430664
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6399279236793518
    },
    {
      "name": "Bengali",
      "score": 0.6055191159248352
    },
    {
      "name": "Urdu",
      "score": 0.5739409923553467
    },
    {
      "name": "Transliteration",
      "score": 0.5239389538764954
    },
    {
      "name": "Language model",
      "score": 0.47797414660453796
    },
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.4302738308906555
    },
    {
      "name": "Linguistics",
      "score": 0.2384769320487976
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Character recognition",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210103279",
      "name": "IBM Research - India",
      "country": "IN"
    },
    {
      "id": "https://openalex.org/I162827531",
      "name": "Indian Institute of Technology Bombay",
      "country": "IN"
    }
  ],
  "cited_by": 16
}