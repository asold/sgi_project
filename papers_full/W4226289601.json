{
  "title": "Building Extraction With Vision Transformer",
  "url": "https://openalex.org/W4226289601",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2229830886",
      "name": "Wang Libo",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2314294709",
      "name": "Fang Shenghui",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A1898319288",
      "name": "Li Rui",
      "affiliations": [
        "Ministry of Natural Resources",
        "Ministry of Education of the People's Republic of China"
      ]
    },
    {
      "id": "https://openalex.org/A2362230200",
      "name": "Meng Xiao-liang",
      "affiliations": [
        "University of Warwick"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2734349601",
    "https://openalex.org/W2898111580",
    "https://openalex.org/W2027000042",
    "https://openalex.org/W2089716607",
    "https://openalex.org/W2028104478",
    "https://openalex.org/W2954896312",
    "https://openalex.org/W2762186317",
    "https://openalex.org/W2055702796",
    "https://openalex.org/W2094682449",
    "https://openalex.org/W2565428746",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W1834627138",
    "https://openalex.org/W6684191040",
    "https://openalex.org/W2764034829",
    "https://openalex.org/W2940726923",
    "https://openalex.org/W2939647427",
    "https://openalex.org/W4214532801",
    "https://openalex.org/W3046711384",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W2908320224",
    "https://openalex.org/W2790741584",
    "https://openalex.org/W3014060899",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2963881378",
    "https://openalex.org/W3200075728",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2955058313",
    "https://openalex.org/W3040304705",
    "https://openalex.org/W3159637683",
    "https://openalex.org/W2982206001",
    "https://openalex.org/W3053564872",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4214893857",
    "https://openalex.org/W6784094891",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W4213019189",
    "https://openalex.org/W4206706211",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3217005392",
    "https://openalex.org/W1966411833",
    "https://openalex.org/W2039743515",
    "https://openalex.org/W2326674917",
    "https://openalex.org/W2085665642",
    "https://openalex.org/W2141422564",
    "https://openalex.org/W2592365852",
    "https://openalex.org/W2924260171",
    "https://openalex.org/W3133524663",
    "https://openalex.org/W3127104941",
    "https://openalex.org/W2787614951",
    "https://openalex.org/W2996327453",
    "https://openalex.org/W3112929693",
    "https://openalex.org/W4200142374",
    "https://openalex.org/W3177272171",
    "https://openalex.org/W3109998321",
    "https://openalex.org/W3022397457",
    "https://openalex.org/W3157124226",
    "https://openalex.org/W3155186819",
    "https://openalex.org/W3174867596",
    "https://openalex.org/W3126435384",
    "https://openalex.org/W2937933649",
    "https://openalex.org/W3168588044",
    "https://openalex.org/W6797399245",
    "https://openalex.org/W3214821343",
    "https://openalex.org/W3201623325",
    "https://openalex.org/W3128776197",
    "https://openalex.org/W3128592650",
    "https://openalex.org/W3161825146",
    "https://openalex.org/W3190334976",
    "https://openalex.org/W4226530543",
    "https://openalex.org/W4283450732",
    "https://openalex.org/W3180045188",
    "https://openalex.org/W4226228401",
    "https://openalex.org/W4226361741",
    "https://openalex.org/W4214760051",
    "https://openalex.org/W4210736635",
    "https://openalex.org/W4285290151",
    "https://openalex.org/W3211329537",
    "https://openalex.org/W3216720075",
    "https://openalex.org/W3200022718",
    "https://openalex.org/W4214669216",
    "https://openalex.org/W3137572916",
    "https://openalex.org/W2910628332",
    "https://openalex.org/W3040608330",
    "https://openalex.org/W2897593716",
    "https://openalex.org/W3169865585",
    "https://openalex.org/W2609402060",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2787091153",
    "https://openalex.org/W2991441757",
    "https://openalex.org/W2980346985",
    "https://openalex.org/W4205138939",
    "https://openalex.org/W4213253308",
    "https://openalex.org/W4308909683",
    "https://openalex.org/W4205365435",
    "https://openalex.org/W3093142463",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W4255609357",
    "https://openalex.org/W2782522152",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2248723555",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3211490618",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2981689412",
    "https://openalex.org/W3104035745",
    "https://openalex.org/W3186032668",
    "https://openalex.org/W2964309882",
    "https://openalex.org/W2886934227",
    "https://openalex.org/W4287198652",
    "https://openalex.org/W2052060966",
    "https://openalex.org/W3183174367",
    "https://openalex.org/W3205965083",
    "https://openalex.org/W3200870516",
    "https://openalex.org/W3109301572"
  ],
  "abstract": "As an important carrier of human productive activities, the extraction of\\nbuildings is not only essential for urban dynamic monitoring but also necessary\\nfor suburban construction inspection. Nowadays, accurate building extraction\\nfrom remote sensing images remains a challenge due to the complex background\\nand diverse appearances of buildings. The convolutional neural network (CNN)\\nbased building extraction methods, although increased the accuracy\\nsignificantly, are criticized for their inability for modelling global\\ndependencies. Thus, this paper applies the Vision Transformer for building\\nextraction. However, the actual utilization of the Vision Transformer often\\ncomes with two limitations. First, the Vision Transformer requires more GPU\\nmemory and computational costs compared to CNNs. This limitation is further\\nmagnified when encountering large-sized inputs like fine-resolution remote\\nsensing images. Second, spatial details are not sufficiently preserved during\\nthe feature extraction of the Vision Transformer, resulting in the inability\\nfor fine-grained building segmentation. To handle these issues, we propose a\\nnovel Vision Transformer (BuildFormer), with a dual-path structure.\\nSpecifically, we design a spatial-detailed context path to encode rich spatial\\ndetails and a global context path to capture global dependencies. Besides, we\\ndevelop a window-based linear multi-head self-attention to make the complexity\\nof the multi-head self-attention linear with the window size, which strengthens\\nthe global context extraction by using large windows and greatly improves the\\npotential of the Vision Transformer in processing large-sized remote sensing\\nimages. The proposed method yields state-of-the-art performance (75.74% IoU) on\\nthe Massachusetts building dataset. Code will be available.\\n",
  "full_text": " 1 \n  \nAbstractâ€”As a n important carrier of human productive \nactivities, the extraction of buildings is not only essential for urban \ndynamic monitoring but also necessary for suburban construction \ninspection. Nowadays, accurate building extraction from remote \nsensing images  remains a challenge due to the complex \nbackground and diverse appearances of buildings. The \nconvolutional neural network (CNN) based building extraction \nmethods, although increased the accuracy significantly, are \ncriticized for their inability for modelling global dependencies . \nThus, this paper applies the Vision Transformer for building \nextraction. However, the actual utilization of the Vision \nTransformer often comes with two limitations. First, the Vision \nTransformer requires more GPU memory and computational \ncosts compared to CNNs . This limitation is further magnified \nwhen encountering large -sized inputs like fine -resolution remote \nsensing images. Second, spatial details  are not sufficiently \npreserved during the feature extraction of the Vision Transformer, \nresulting in the inability for  fine-grained building segmentation . \nTo handle these  issues, we propose a novel Vision Transformer  \n(BuildFormer), with a dual-path structure. Specifically, we design \na spatial-detailed context path to encode rich spatial details and a \nglobal context path to capture global dependencies.  Besides, we \ndevelop a window-based linear multi-head self-attention to make \nthe complexity of the multi -head self -attention linear with the \nwindow size, which strengthens the global context e xtraction by \nusing large windows and greatly improves the potential of the \nVision Transformer in processing large -sized remote sensing \nimages. The proposed method yields state-of-the-art performance \n(75.74% IoU) on the Massachusetts building dataset. Code will be \navailable. \nIndex Termsâ€”Vision Transformer, building extraction, remote \nsensing, attention mechanism. \nI. INTRODUCTION \nuilding extraction using fine -resolution remote sensing \nimages, i.e., the task  of identifying building and non -\nbuilding pixels in an image  [1], plays a crucial role in a wide \nrange of application scenarios such as urban planning, \npopulation statistic, economic assessment and disaster \nmanagement [2-6].  \nConventional methods for building extraction commonly \nextract hand-craft features (e.g., sp ectral, spatial, textural) and \napply traditional machine learning methods (e.g., Support \nVector Machine and Random Forest) to recognize buildings [7-\n9]. However, the  empirically designed  hand-craft features \nrestrict the generalization ability of these traditional methods. \nIn the past few years, Deep Learning (DL) has become a \n                                                           \nThis work was supported in part by the National Natural Science Foundation \nof China (No. 41971352) and Alibaba innovation research (AIR:17289315) . \n(Corresponding author: Libo Wang) \nL. Wang, S. Fang, and X. Meng are with School of Remote Sensing and \nInformation Engineering, Wuhan University, Wuhan 430079, China  (e-mail: \nwanglibo@whu.edu.cn; shfang@whu.edu.cn; xmeng@whu.edu.cn). \npopular approach for automatic feature learning [10] and \nachieved great breakthroughs in the computer vision (CV) \ndomain [11]. In the field of remote sensing, DL methods, \nespecially the convolutional neural network (CNN) [12], have \nbeen introduced and implemented in many geospatial tasks [13-\n15], especially for building extraction [16]. In comparison with \nconventional methods, CNN- based methods can capture \nvarious kinds of information including textures, spectrums, \nspatial context, and the interactions among geo-objects. \nSince the pioneer CNN structure, i.e.,  Fully Convolutional \nNeural Network (FCN) , was proposed for pixel-level dense \nprediction, a series of researches were carried out on automatic \nbuilding extraction from remote sensing images [17 -20]. \nSubsequently, the encoder -decoder structure was proposed to \naddress the coarse -resolution se gmentation of FCN- based \nnetworks by constructing a symmetrical decoder. Typical \nmethods like UNet and SegNet restored the spatial resolution of \nR. Li is with the Intelligent Control & Smart Energy (ICSE) Research Group, \nSchool of Engineering, University of Warwick, Coventry CV4 7AL, UK.  (e-\nmail: rui.li.4@warwick.ac.uk) \nBuilding extraction with vision transformer \nLibo Wang, Shenghui Fang, Rui Li and Xiaoliang Meng \nB \n \nFig. 1. Illustration of the global context and local context. The squares \nrepresent the receptive view of the convolution. The yellow regions represent \nthe blurry building pixels where the local context is indistinguishable. \n\n 2 \nextracted features progressively for fine -resolution feature \nrepresentation [21, 22] . The results of these CNN -based \nmethods, although encouraging, encount er bottlenecks  in \nbuilding extraction.  To be specific,  the CNN is designed to \nextract the local context and thus lacks the ability to model \nglobal context in its nature. However, the local context is often \nambiguous for identifying building pixels, while the extraction \nwill become much simpler if the global context from the whole \nremote sensing image is available, as illustrated in Fig.1. \nFor capturing the global context, the most popular way is to \nincorporate attention mechanisms into networks. For example, \nthe non-local module [23], the dual attention module [24], the \ncriss-cross attention block [25] and the object context block  \n[26], obtained  great improvements in semantic segmentation \nthanks to their ability in modelling global dependencies  by \nattention mechanisms . In the field of building extraction, \nseveral attempts were made to introduce attention mechanisms \nfor stronger feature representation, which differentiates \nheterogeneous buildings from complex backgrounds in fine -\nresolution remote sensing images [6, 27] . However, these \nmethods still follow the CNN structure, restricting the global \nfeature representation. \nRecently, the Transformer [28] , originally designed f or \nnatural language processing (NLP) tasks, comprises a hot topic \nin the computer vision domain, namely Vision Transformer \n(ViT) [29]. Different from the CNN structure, the ViT translates \n2D image-based tasks into 1D sequence-based tasks. Due to the \nstrong sequence-to-sequence modelling ability, the ViT \ndemonstrates superior characterization of extracting global \ncontext than attention -based CNNs, obtaining  numerous \nbreakthroughs on fundamental vision tasks, such as image \nclassification [29] and object detection [30] as well as semantic \nsegmentation [31]. \nHowever, the  actual utilization of ViTs often comes with \nhuge memory requirements and computational costs [32, 33], \nwhich seriously affects its potential for downstream tasks like \nbuilding extraction. Even though the Swin Transformer adopts \nthe hierarchical structure and designs a window- based multi-\nhead self -attention mechanism to improve efficiency, its \ncomplexity still increases quadratically along with the \nincreasing size of the window [34]. Furthermore, ViTs mainly \nfocus on capturing the global context while ignoring preserving \nthe spatial-detailed context, but spatial details are also essential \nfor fine -grained building segmentation in fine -resolution \nremote sensing images [35]. \nIn this paper, we p ropose a novel Vision Transformer , \nnamely BuildFormer, for building extraction from fine -\nresolution remote sensing images to address the existing issues \nof ViTs. Specifically, we adopt a dual -path structure to \nconstruct the BuildFormer, i.e. a global conte xt path and a \nspatial-detailed context path. In the global context path, we \ndevelop a novel Transformer block to construct a Vision \nTransformer backbone, enhancing the ability for global context \nextraction. In the spatial -detailed context path, we utilize \nstacked convolutional layers to preserve rich spatial details. The \nmajor contributions of this paper are as follows: \n \n1) We propose a novel Vision Transformer (BuildFormer) \nbased on the dual-path structure, which can capture the \nglobal context while preserving spatial -detailed \nfeatures. \n2) We present a novel Transformer block  to construct the \nglobal context path, namely BuildFormer Block (BFB), \nwhich is mainly composed of a wi ndow-based linear \nmulti-head self -attention (W -LMHSA) and a \nconvolutional multilayer perceptron (C-MLP).  \n3) The W-LMHSA reduces the complexity of the window-\nbased multi -head self -attention (W -MHSA) [34] to \nlinear complexity. Benefiting from this, the \nBuildFormer can apply larger windows to extract global \nfeatures from large inputs without resulting in high \ncomputations, which is more suitable for large -scale \nfine-resolution remote sensing images.  The C -MLP \nstrengthens the cross -window interactions, which \nfurther enhances the ability of the BuildFormer for \nglobal information modelling. \nII. R\nELATED WORK \nA. CNN-based Building Extraction Methods \nWith the rapid development of Deep Learning, the \nconvolutional neural network (CNN) has become the \nmainstream method for the automatic remote sensing building \nextraction task. In comparison with the conventional methods \nthat design hand -crafted featur e operators (colour, texture, \nshallow, etc.) [37-42] or those using active remote sensing data \n(LiDAR and SAR) [5, 43-46], the CNN -based methods have \nadvantages in hierarchical feature extraction and efficiency [9, \n47-50]. Although the CNN- based meth ods achieve many \nbreakthroughs, their weaknesses in global information \nmodelling limit further improvements in accuracy, as g lobal \ninformation is crucial for detecting buildings from low -\ninterclass and high -intraclass remote sensing images [51 -53]. \nTo address it, several studies  have introduced attention \nmechanisms to strengthen the global feature representation for \nbuilding extraction [54-57]. For example, Deng et al. [58] \ndeveloped a grid-based attention gate  module to extract \nsemantic features with a global receptive field, further boosting \nthe accuracy. Guo et al. [6] introduced the parallel attention to \ncapturing global scene information, which further improved the \naccuracy of  building segmentation. Pan et al. [59] combined \nspatial and channel attention mechanisms into the generative \nadversarial network and achieved advanced results. Cai et al [60] \nproposed a multipath hybrid attention network to enhance the \nperformance of extracting small buildings. Since these \nattention-based methods rel ied too much on convolution \noperations, they fail ed to liberate the network from  the CNN \nstructure and have ce rtain limitations in global information \nmodelling. \n 3 \nB. ViT-based Building Extraction Methods \nViT-based methods have brought tremendous progress and \nevolution for semantic segmentation [31, 61, 62]. The structure \nof the ViT is completely different from the CNN , which treats \nthe 2D image as the 1D ordered sequence and appl ies the self-\nattention mechanism for global dependency modelling, \ndemonstrating stronger global feature extraction. Driven by this, \nmany researchers in the field of remote sensing introduced ViTs \nfor segmentation-related tasks, such as land cover classification \n[63-68], urban scene parsing [69-74], change detection [75, 76], \nroad extraction [77] and especially building extraction [78]. For \nexample, Chen et al. [79] proposed a sparse token Transformer \nto learn  the global dependency of tokens in both spatial and \nchannel dimensions , achieving state-of-the-art accuracy on \nbenchmark building extraction datasets. Yuan et al [80] \nintroduced the widely used Swin Transformer [34]  as the \nencoder and design a scale -adaptive decoder for multi -scale \nfeature representation. Compared with the CNN-based methods,  \nthe global information is fully extracted by ViT-based methods. \nHowever, the spatial detailed context, meanwhile, is ignored. \nIII. METHODOLOGY \nA. Overview \nThe structure of the proposed BuildFormer is illustrated in \nFig. 2 with a Global Context Path (GCP) and a Spatial-detailed \nContext Path (SCP) . In GCP , four BuildFormer Blocks are \ndesigned to extract four global feature maps at different scales. \nMeanwhile, the high -resolution spatial -detailed feature map \nwill be generated by SCP. Finally, the four global feature maps \nand the spatial-detailed feature map are fed into the contextual \naggregation module to generate the final semantic feature. \nB. Spatial-detailed Context Path \nIt is very challenging to reconcile the demand for  spatial-\ndetailed features with global dependencies simultaneously in \nthe Vision Transformer. However, both of them are essential \nfor obtaining high accuracy of building segmentation. To \naddress this issue, in the proposed BuildFormer, we adopt a \ndual-path structure [36], which introduces a spatial -detailed \ncontext path to produce a high- resolution feature map for \npreserving spatial details.  Concretely, we apply six \n(Convolution-BatchNorm-ReLU6) CBR blocks to construct \nthis path and expand their channel dimensions progressively to \nencode sufficient spatial-detailed information, as shown in Fig. \n2. Specifically, six standard 3 ïƒ3 convolutional layers are \nemployed and each layer is equipped with a batch normalization \noperation and a ReLU6 activation function. To ensure sufficient \nspatial details, the size of the output feature map is designed as \n1/4 of the original input image. \nC. Global Context Path \nThe global context path is a novel s elf-designed Vision \nTransformer. The main basic modules of this path include the \nBuildFormer Block, Patch Embedding, and Patch Merging, as \nshown in Fig. 2. Due to its linear complexity, this path is more \nsuitable for capturing global context from large -scale remote \nsensing images.  \nPatch Embedding : The  original ViT [29] utilizes linear \nprojections to split the input image into non -overlapping \npatches directly. However, t his scheme has limitations in \nmodelling the structure information within patches. To \novercome it, we apply convolutional layers to split the input \nimage into overlapping patches. As shown in Fig. 3 (a), we use \ntwo 3ïƒ3 convolutional layers with a stride of 2 and a padding \nvalue of 1,  while each layer is followed by a batch \n \nFig. 2. The structure of the proposed BuildFormer. \n \nFig. 3. (a) the Patch Embedding module, (b) the Patch Merging module. \n\n 4 \nnormalization operation and a ReLU6 activation function. \nProceed by the two convolutional layers, the channel dimension \nof patches is expanded to 96 and the resolution is reduced to 1/4. \nIn addition, a standard 3 ïƒ3 depth- wise convolution and a \nresidual connection are employed to enhance the relative \nlocation priors of patches. \nPatch Merging : To obtain the hierarchical feature \nrepresentation, four Patch Merging modules are employed and \neach module reduces the resolution of intermediate patches and \nexpands the channel dimension. As shown in Fig. 3. (b), we first \nuse the batch normalization operation to normalize the patches \nthen apply a 2 ïƒ2 convolutional layer to down- sampling it to \n1/2 and expand its channel dimension to 2 times. Similar to the \nPatch Embedding module, we utilize a standard 3 ïƒ3 depth -\nwise convolution and a residual connection to strengthen the \nlocation information extraction.  \nBuildFormer Block: Each BuildFormer Block is composed \nof a Window-based Linear Multi-Head Self-Attention module \n(W-LMHSA), a convolutional multilayer perceptron, two batch \nnormalization operations and two residual connections, as \nillustrated in Fig. 4.  \nIn S win Transformer [34] , the Window -based Multi-Head \nSelf-Attention (W-MHSA) splits the input into non -\noverlapping windows and performs the standard Multi -Head \nSelf-Attention (MHSA) [28] in each local window. Benefiting \nfrom the window partition operation, the W-MHSA saves much \ncomputational burden compared to the MHSA. Even though, \nthe computational complexity of each local window is still \nğ‘‚ğ‘‚(ğ‘ğ‘\n2) due to the application of the MHSA. N is the square of \nthe window size. Thus, the W -MHSA comes with hu ge \ncomputations and memory requirements if using large windows. \nBy contrast, t he proposed W-LMHSA further eliminate the \nhigh demand of W-MHSA in computations and memory based \non our previous work on the linear attention mechanism [52] , \nwhich makes the computational complexity linear with the \nwindow size.  For each local window, the multi -head self -\nattention can be defined as: \nMHSA(ğ‘¿ğ‘¿) = Concat(head1, â€¦ , headâ„)ğ‘¾ğ‘¾ğ’ğ’ (1) \nHere, X is the input vector and h is the number of heads. ğ‘¾ğ‘¾ğ’ğ’ âˆˆ\nâ„ğ‘ğ‘Ã—ğ·ğ· is a projected matrix, where D  is the dimension of the \ninput vector. Each head denotes a self-attention operation which \ncan be defined as: \nAttention(ğ‘¸ğ‘¸, ğ‘²ğ‘², ğ‘½ğ‘½) = Softmaxrow ï¿½ğ‘¸ğ‘¸ğ‘²ğ‘²ğ‘»ğ‘»\nğ‘ ğ‘  ï¿½ğ‘½ğ‘½ (2) \nğ‘¸ğ‘¸ = ğ‘¿ğ‘¿ğ’ğ’ğ‘¾ğ‘¾ğ’’ğ’’ âˆˆ â„ğ‘ğ‘Ã—ğ‘‘ğ‘‘ (3) \nğ‘²ğ‘² = ğ‘¿ğ‘¿ğ’ğ’ğ‘¾ğ‘¾ğ’Œğ’Œ âˆˆ â„ğ‘ğ‘Ã—ğ‘‘ğ‘‘ (4) \nğ‘½ğ‘½ = ğ‘¿ğ‘¿ğ’ğ’ğ‘¾ğ‘¾ğ’—ğ’— âˆˆ â„ğ‘ğ‘Ã—ğ‘‘ğ‘‘ (5) \nwhere ğ‘¿ğ‘¿ğ’ğ’ is the input vector of the m-th head. Q, K and V are \nthe query feature, key feature and value  feature, which are \ngenerated by the three projected matrix s ğ‘¾ğ‘¾ğ’’ğ’’, ğ‘¾ğ‘¾ğ’Œğ’Œ and ğ‘¾ğ‘¾ğ’—ğ’—, \nrespectively. d denotes the dimension of the m-th head and \nd=D/h. s represents the scale factor and s is set to 1 by default. \nSoftmaxrow(ğ‘¸ğ‘¸ğ‘²ğ‘²ğ‘»ğ‘»)  computes the similarities between each \npair of pixels of the input  vector and applies  the softmax \nnormalization function along each row of the similarity matrix \nğ‘¸ğ‘¸ğ‘²ğ‘²ğ‘»ğ‘», which is  the key step to model global dependencies . \nHowever, the product between ğ‘¸ğ‘¸ âˆˆ â„ğ‘ğ‘Ã—ğ‘‘ğ‘‘  and ğ‘²ğ‘²ğ‘»ğ‘» âˆˆ â„ğ‘‘ğ‘‘Ã—ğ‘ğ‘ \nbelongs to â„ğ‘ğ‘Ã—ğ‘ğ‘, which leads to the ğ‘‚ğ‘‚(ğ‘ğ‘2) computational \ncosts and memory requirements. As N  is the square of the \nwindow size, the resource -demanding of the W -MHSA can \nincrease significantly when using large windows.  To address \nthis, we simplify Eq. (2) by replacing the softmax normalization \nfunction with the first -order approximation of the Tayl or \nexpansion. Specifically, when using the softmax normalization \nfunction, the i-th row of the result matrix generated by Eq. (2) \ncan be written as: \nAttentionğ‘–ğ‘–(ğ‘¸ğ‘¸, ğ‘²ğ‘², ğ‘½ğ‘½) =\nâˆ‘ ğ‘’ğ‘’ğ’’ğ’’ğ‘–ğ‘–\nğ‘‡ğ‘‡ğ’Œğ’Œğ‘—ğ‘—ğ‘ğ‘\nğ‘—ğ‘—=1 ğ’—ğ’—ğ‘—ğ‘—\nâˆ‘ ğ‘’ğ‘’ğ’’ğ’’ğ‘–ğ‘–\nğ‘‡ğ‘‡ğ’Œğ’Œğ‘—ğ‘—ğ‘ğ‘\nğ‘—ğ‘—=1\n (6) \n Here, ğ’’ğ’’ğ‘–ğ‘–\nğ‘‡ğ‘‡ âˆˆ â„ğ‘‘ğ‘‘ is the i -th query feature. ğ’Œğ’Œğ‘—ğ‘— and ğ’—ğ’—ğ‘—ğ‘— are the \nj-th key feature and value feature, respectively. Please note that \nthe vectors in this research are column vectors by default. \nActually, Eq. (6) can be generalized to any normalization \nfunction as: \nAttentionğ‘–ğ‘–(ğ‘¸ğ‘¸, ğ‘²ğ‘², ğ‘½ğ‘½) =\nâˆ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ (ğ’’ğ’’ğ‘–ğ‘–, ğ’Œğ’Œğ‘—ğ‘—)ğ‘ğ‘\nğ‘—ğ‘—=1 ğ’—ğ’—ğ‘—ğ‘—\nâˆ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ (ğ’’ğ’’ğ‘–ğ‘–, ğ’Œğ’Œğ‘—ğ‘—)ğ‘ğ‘\nğ‘—ğ‘—=1\n (7) \nğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ï¿½ğ’’ğ’’ğ‘–ğ‘–, ğ’Œğ’Œğ‘—ğ‘—ï¿½ = ğœ™ğœ™(ğ’’ğ’’ğ‘–ğ‘–)ğ‘‡ğ‘‡ğœ‘ğœ‘ï¿½ğ’Œğ’Œğ‘—ğ‘—ï¿½ (8) \nğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ (ğ’’ğ’’ğ‘–ğ‘–, ğ’Œğ’Œğ‘—ğ‘—) can measure the similarity between ğ’’ğ’’ğ‘–ğ‘– and ğ’Œğ’Œğ‘—ğ‘—. \nThe normalization functions ğœ™ğœ™(âˆ™)  and ğœ‘ğœ‘(âˆ™)  are used to \nensure  ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ï¿½ğ’’ğ’’ğ‘–ğ‘–, ğ’Œğ’Œğ‘—ğ‘—ï¿½ â‰¥ 0 . According to the first-order \napproximation of the Taylor expansion: \nğ‘’ğ‘’ğ’’ğ’’ğ‘–ğ‘–\nğ‘‡ğ‘‡ğ’Œğ’Œğ‘—ğ‘— â‰ˆ 1 + ğ’’ğ’’ğ‘–ğ‘–\nğ‘‡ğ‘‡ğ’Œğ’Œğ‘—ğ‘— (9) \nWe set ğœ™ğœ™(âˆ™) and ğœ‘ğœ‘(âˆ™) as the L2 normalization function to \nguarantee ğ’’ğ’’ğ‘–ğ‘–\nğ‘‡ğ‘‡ğ’Œğ’Œğ‘—ğ‘— â‰¥ âˆ’1: \nğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ï¿½ğ’’ğ’’ğ‘–ğ‘–, ğ’Œğ’Œğ‘—ğ‘—ï¿½ = 1 + ï¿½ ğ’’ğ’’ğ‘–ğ‘–\nâ€–ğ’’ğ’’ğ‘–ğ‘–â€–2\nï¿½\nğ‘‡ğ‘‡\nï¿½ ğ’Œğ’Œğ‘—ğ‘—\nï¿½ğ’Œğ’Œğ‘—ğ‘—ï¿½2\nï¿½ (10) \n \nFig. 4. Details of the BuildFormer Block. \n\n 5 \nThus, Eq. (6) can be rewritten as Eq. (11), simplified as Eq. (12) \nand further turned into Eq. (13): \nAttentionğ‘–ğ‘–(ğ‘¸ğ‘¸, ğ‘²ğ‘², ğ‘½ğ‘½) =\nâˆ‘ ï¿½1 + ï¿½ ğ’’ğ’’ğ‘–ğ‘–\nâ€–ğ’’ğ’’ğ‘–ğ‘–â€–2\nï¿½\nğ‘‡ğ‘‡\nï¿½ ğ’Œğ’Œğ‘—ğ‘—\nï¿½ğ’Œğ’Œğ‘—ğ‘—ï¿½2\nï¿½ï¿½ğ‘ğ‘\nğ‘—ğ‘—=1 ğ’—ğ’—ğ‘—ğ‘—\nâˆ‘ ï¿½1 + ï¿½ ğ’’ğ’’ğ‘–ğ‘–\nâ€–ğ’’ğ’’ğ‘–ğ‘–â€–2\nï¿½\nğ‘‡ğ‘‡\nï¿½ ğ’Œğ’Œğ‘—ğ‘—\nï¿½ğ’Œğ’Œğ‘—ğ‘—ï¿½2\nï¿½ï¿½ğ‘ğ‘\nğ‘—ğ‘—=1\n (11) \nAttentionğ‘–ğ‘–(ğ‘¸ğ‘¸, ğ‘²ğ‘², ğ‘½ğ‘½) =\nâˆ‘ ğ’—ğ’—ğ‘—ğ‘—\nğ‘ğ‘\nğ‘—ğ‘—=1 + ï¿½ ğ’’ğ’’ğ‘–ğ‘–\nâ€–ğ’’ğ’’ğ‘–ğ‘–â€–2\nï¿½\nğ‘‡ğ‘‡\nâˆ‘ ï¿½ ğ’Œğ’Œğ‘—ğ‘—\nï¿½ğ’Œğ’Œğ‘—ğ‘—ï¿½2\nï¿½ğ’—ğ’—ğ‘—ğ‘—\nğ‘‡ğ‘‡ğ‘ğ‘\nğ‘—ğ‘—=1ğ‘ğ‘ + ï¿½ ğ’’ğ’’ğ‘–ğ‘–\nâ€–ğ’’ğ’’ğ‘–ğ‘–â€–2\nï¿½\nğ‘‡ğ‘‡\nâˆ‘ ï¿½ ğ’Œğ’Œğ‘—ğ‘—\nï¿½ğ’Œğ’Œğ‘—ğ‘—ï¿½2\nï¿½ğ‘ğ‘\nğ‘—ğ‘—=1\n (12) \nAttention(ğ‘¸ğ‘¸, ğ‘²ğ‘², ğ‘½ğ‘½) =\nâˆ‘ ğ‘½ğ‘½ğ‘–ğ‘–,ğ‘—ğ‘—ğ‘—ğ‘— + ï¿½ ğ‘¸ğ‘¸\nâ€–ğ‘¸ğ‘¸â€–2\nï¿½ï¿½ï¿½ ğ‘²ğ‘²\nâ€–ğ‘²ğ‘²â€–2\nï¿½\nğ‘‡ğ‘‡\nğ‘½ğ‘½ï¿½\nğ‘ğ‘+ ï¿½ ğ‘¸ğ‘¸\nâ€–ğ‘¸ğ‘¸â€–2\nï¿½âˆ‘ ï¿½ ğ‘²ğ‘²\nâ€–ğ‘²ğ‘²â€–2\nï¿½\nğ‘–ğ‘–,ğ‘—ğ‘—\nğ‘‡ğ‘‡\nğ‘—ğ‘—\n (13) \nSince âˆ‘ ï¿½\nğ’Œğ’Œğ‘—ğ‘—\nï¿½ğ’Œğ’Œğ‘—ğ‘—ï¿½2\nï¿½ğ’—ğ’—ğ‘—ğ‘—\nğ‘‡ğ‘‡ğ‘ğ‘\nğ‘—ğ‘—=1  and âˆ‘ ï¿½\nğ’Œğ’Œğ‘—ğ‘—\nï¿½ğ’Œğ’Œğ‘—ğ‘—ï¿½2\nï¿½ğ‘ğ‘\nğ‘—ğ‘—=1\n can be calculated and \nreused for each query, the time and memory complexity of the \nproposed attention based on Eq. (13) is the ğ‘‚ğ‘‚(ğ‘‘ğ‘‘ğ‘ğ‘)  linear \ncomplexity. \nThe cross -window interaction is crucial for global \ndependencies modelling when using the W -MHSA. The Swin \nTransformer [34] introduces a shifted window operation to \nstrengthen the cross-window interaction. This scheme, although \nvery effective, increases the complexity of the network due to \nadding another shifted- window Transformer block. In this \npaper, we pro vide a convolutional multilayer perceptron (C -\nMLP) to strengthen the interaction within windows. In \ncomparison with the Swin Transformer, the employment of the \nC-MLP can maintain competitive accuracy while improving \nefficiency. The detailed components of t he C -MLP are \nillustrated in Fig. 4. \nD. Context Aggregation Module \nThe output features from GCP and SCP are complementary. \nThe feature from the SCP mainly encodes rich detailed \ninformation, while the four features generated by the GCP \nprovide high-level global semantic information. To better fuse \nthem, we adopt the feature  fusion strategy like the feature \npyramid feature (FPN) [81], as shown in Fig. 5. S pecifically, \nthe four global feature maps from the GCP are first proceeded \nby four 1ïƒ1 convolution layers to unify the channel dimension \nto 384. Then, we apply four CBR blocks as well as upsampling \nand addition operations to perform multi -level feature fusion. \nFinally, the fused global feature is further aggregated with the \nspatial-detailed feature from the SCP to generate the final fused \nfeature. \nE. Loss Function \nImproving the accuracy of building boundaries is vital  for \nhigh-precision building extraction [35, 82 -84]. Thus, we \nintroduce the boundary supervision technology and adopt a \njoint loss to train the BuildFormer. The joint loss function ğ¿ğ¿ \ncan be defined as: \nğ¿ğ¿ = ğ¿ğ¿ğ‘ğ‘ğ‘ğ‘ï¿½ğ‘Œğ‘Œ, ğ‘Œğ‘Œï¿½ï¿½+ ğ¿ğ¿ğ‘‘ğ‘‘ğ‘–ğ‘–ğ‘ğ‘ğ‘ğ‘ï¿½ğ‘Œğ‘Œ, ğ‘Œğ‘Œï¿½ï¿½+ ğ¿ğ¿ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ï¿½â„’(ğ‘Œğ‘Œ), â„’(ğ‘Œğ‘Œï¿½)ï¿½ (14) \nwhere ğ‘Œğ‘Œ and ğ‘Œğ‘Œï¿½ denote the predicted label and the true label, \nrespectively. ğ¿ğ¿ğ‘ğ‘ğ‘ğ‘ is the cross -entropy loss. ğ¿ğ¿ğ‘‘ğ‘‘ğ‘–ğ‘–ğ‘ğ‘ğ‘ğ‘ is the dice \nloss. â„’ represents the Laplacian convolution [85] with a kernel \nof ï¿½\nâˆ’1 âˆ’1 âˆ’1\nâˆ’1 8 âˆ’1\nâˆ’1 âˆ’1 âˆ’1\nï¿½ that extracts the building boundaries of \nthe predicted label and the true label. The binary cross-entropy \nloss (denoted by ğ¿ğ¿ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘) is employed on the extracted building  \nboundaries. \nIV. EXPERIMENTAL SETTINGS AND DATASETS \nA. Datasets \nTo evaluate the performance of the proposed BuildFormer, \nthree publicly available building datasets are considered \ncomprehensively for conducting experiments, including the \nMassachusetts building dat aset, WHU building dataset and \nInria Aerial Image Labeling dataset. The details are as follows. \n1) Massachusetts : The Massachusetts building dataset  is \ncomposed of 151 aerial images of the Boston area with a size \nof 1500ïƒ1500 pixels and a ground sampling distance of 1 m.  \nThe dataset involves urban and subu rban scenes, where the \nbuildings are varied in sizes, shapes, textures and colours. Thus, \nthis dataset is very challenging and suitable to verify the \neffectiveness of modules. We follow the official parti tion \nprovided by the dataset and use data augmentation technologies \nlike vertical and horizontal flip to further expand the training \nset. As a result, we use 411 images for training, 4 images for \nvalidation, and 10 images for testing. In the training phase, we \nrandomly crop the images and labels into 1024ïƒ1024 pixels as \nthe input. In the validation and testing phase, the images and \nlabels are padded to a size of 1536 ïƒ1536 pixels to ensure it is \ndivisible by 32 (the downsampling factor of the BuildFormer). \nThe padded parts are ignored when computing evaluation \nmetrics. \n2) WHU: The WHU building dataset [18] includes two types \nof images, i.e. satellite imagery and aerial imagery. We only use \naerial images in our experiments. The aerial imagery subset \ncovers over 450 km\n2 and includes 22000 buildings. The spatial \nresolution of the RGB aerial images  is 0.3 m and the size of \neach image is 512 ïƒ512 pixels. There are 8189 image tiles in \nthis dataset, where 4736 tiles for training, 1036 tiles for \nvalidation and 2416 tiles for testing. We follow the official \npartition in our experiments. \n3) Inria: The Inria Aerial Image Labeling Dataset  [86] \ncontains 360 fine -resolution aerial images collected from five \ncities (Austin, Chicag o, Kitsap, Tyrol and Vienna). Since the \nlabels of the test set are publicly available, we only use the \noriginal training set in our experiments. Suggested by the \n \nFig. 5. Details of the Context Aggregation Module. \n\n 6 \nofficial partition, the 1 to 5 tiles of each city are selected for \nvalidation and the rest for  training. We first pad the original \n5000ïƒ5000 images to 5120ïƒ5120 pixels, then crop them into \n512ïƒ512 pixels image tiles. The image tiles, which do not \ncontain buildings, are removed for efficient training. As a result, \n9737 and 1942 image tiles are used for training and validation, \nrespectively. \nB. Evaluation Metrics \nWe use the intersection over union (IoU), F1 score, precision \nand recall to evaluate the performance of models. These metrics \nare widely used in the field of building extraction [27, 35] , \nwhich can be defined as follows: \nPrecision = ğ‘‡ğ‘‡ğ‘‡ğ‘‡\nğ‘‡ğ‘‡ğ‘‡ğ‘‡+ ğ¹ğ¹ğ‘‡ğ‘‡ (15) \nRecall = ğ‘‡ğ‘‡ğ‘‡ğ‘‡\nğ‘‡ğ‘‡ğ‘‡ğ‘‡+ ğ¹ğ¹ğ‘ğ‘ (16) \nF1 = 2 Ã— ğ‘‡ğ‘‡ğ‘ƒğ‘ƒğ‘’ğ‘’ğ‘ƒğ‘ƒğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ƒğ‘ƒğ‘ƒğ‘ƒÃ— ğ‘…ğ‘…ğ‘’ğ‘’ğ‘ƒğ‘ƒğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…\nğ‘‡ğ‘‡ğ‘ƒğ‘ƒğ‘’ğ‘’ğ‘ƒğ‘ƒğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ƒğ‘ƒğ‘ƒğ‘ƒ+ ğ‘…ğ‘…ğ‘’ğ‘’ğ‘ƒğ‘ƒğ‘…ğ‘…ğ‘…ğ‘…ğ‘…ğ‘…  (17) \nIoU = ğ‘‡ğ‘‡ğ‘‡ğ‘‡\nğ‘‡ğ‘‡ğ‘‡ğ‘‡+ ğ¹ğ¹ğ‘ğ‘+ ğ¹ğ¹ğ‘‡ğ‘‡ (18) \nTP, FP, and FN represent the true positive, the false positive, \nand the false negative, respectively. \nC. Experimental Setting \nAll models in the experiments were implemented with the \nPyTorch framework on a single NVIDIA GTX 3090 GPU with \n24GB RAM . The AdamW optimizer and t he cosine strategy \nwere employed to train all models in the experiments.  The \nrandom horizontal and vertical flipping were selected as data \naugmentation strategies. For the WHU building dataset, we \ntrained the BuildFormer from scratch for 105 epochs. The base \nlearning rate was set to 1e-3 and the batch size was set to 8. For \nthe Massachachusets building dataset and the Inria Aerial \nImage Labelling dataset, we used BuildFormerâ€™s weight trained \non the WHU building dataset, then fine-tuned it for 105 epochs \nwith a learning rate of 5e-4. In the testing phase, we applied the \ndata augmentation technologies like horizontal and vertical \nflipping, which is also known as test-time augmentation (TTA). \nV. E\nXPERIMENTAL RESULTS AND ANALYSIS \nA. Ablation Study \nTo verify the effectiveness of the proposed modules , we \nconducted ablation experiments on the Massachachusets \nbuilding dataset. \nTABLE I \nTHE ABLATION EXPERIMENTAL RESULTS OF SPATIAL-DETAILED CONTEXT \nPATH ON THE MASSACHUSETTS BUILDING DATASET. \nMethod IoU F1 \nBuidFormer without SCP 74.38 85.30 \nBuildFormer with SCP 75.74 86.19 \n \n1) The effectiveness of the spatial-detailed context path (SCP): \nIn the proposed BuildFormer, the spatial -detailed context path \naims to encode rich spatial -detailed information for fine -\ngrained building segmentation. To test its effectiveness, we \nremove it from BuildFormer. As listed in Table I, the utilization \nof the spatial-detailed context path provides an increase of 1.36% \nin IoU, which demonstrates its effectiveness and necessity. \nFurthermore, this result also illustrates the superiority of the  \ndual-path structure over than single -path structure  for fine -\ngrained building extraction. \n2) The superiority of the global context path (GCP) : The \nglobal context path in the proposed BuildFormer is a Vision \nTransformer backbone. To demonstrate its superiority in \nbuilding extraction,  we replace it with other backbones for \ncomparison. The results show that our method yields an \nimprovement of 2.04% in IoU compared to the Swin-Small [34] \nand surpassed the classical convolutional backbone ResNet101 \n[87] by 5.01% in IoU (Table II). \nTABLE II \nTHE ABLATION EXPERIMENTAL RESULTS OF THE GLOBAL CONTEXT PATH ON \nTHE MASSACHUSETTS BUILDING DATASET. \nMethod IoU Parameter (M) \nResNet101 70.73 49.35 \nSwin-Small 73.70 61.54 \nours 75.74 40.52 \n \n3) The effectiveness of the convolutional multilayer \nperceptron (C-MLP): The C-MLP aims to strengthen the cross-\nwindow interaction, improving the ability of the BuildFormer \nBlock for capturing global context . To demonstrate its \ncontribution to accuracy, we replace it with the standard \nmultilayer perceptron (MLP) for ablation experiments. As \nillustrated in Table III, the employment of the C-MLP increases \nthe IoU metric and the F1 score by 5.16% and 3.36%, \nrespectively, demonstrating its effectiveness and essential. \nTABLE â…¡I \nTHE ABLATION EXPERIMENTAL RESULTS OF SPATIAL-DETAILED CONTEXT \nPATH ON THE MASSACHUSETTS BUILDING DATASET. \nMethod IoU F1 \nBuidFormer with MLP 70.58 82.75 \nBuildFormer with C-MLP 75.74 86.19 \n \n4) The advantages of the window -based linear multi -head \nself-attention (W -LMHSA): To better demonstrate the \nimprovements of the proposed W -LMHSA, we conduct \ncomprehensive experiments in comparison with the window -\nbased multi-head self-attention (W-MHSA). We apply the W -\nLMHSA and W -MHSA to construct the BuildFor mer, \nrespectively. As shown  in Table IV , the computational \ncomplexities of the W-MHSA and W-LMHSA under different \nwindow sizes are measured by  the floating -point operation \ncount (Flops) in M. The speed of the network (FPS) is measured \nby a 1024 ïƒ1024 pixels image tile on a single NVIDIA GTX \n3090 GPU. The results reveal that the proposed W-LMHSA has \nadvantages in both accuracy and efficiency compared to the W-\nMHSA. Specifically, the proposed W-LMHSA can provide an \nimprovement of 2% IoU while saving about 25% computational \ncomplexity. Besides, the W -LMHSA maintains the GPU \nmemory requirement and the speed stable even with a large \nwindow, while the W -MHSA increases memory requirements \nand reduces the speed significantly. \n 7 \nB. Comparison of State-of-the-art Methods \nTo further verify the effectiveness of the proposed method, \nwe compare it with state -of-the-art methods on three pub licly \navailable datasets, i.e. the Massachusetts building dataset, \nWHU building dataset and Inria Aerial Image Labeling dataset. \nThe selected methods include convolutional networks, such as \nU-Net [21], Deeplabv3+ [88], SRI-Net [16], DS -Net [49], \nBRRNet [20], SiU-Net [18], CU-Net [19], EU-Net [89], DE-\nNet [90], MA -FCN [48], MANet  [53], MAP -Net [27], Bias-\nUNet [57], CBRNet [35], and ViT -based networks like  \nSwinUperNet [34], Sparse Token Transformer (ST T) [79], \nMSST-Net [80], BANet [72], DC-Swin [69]. \nFor the Massachusetts building dataset, the proposed method \nyields a  75.74% IoU and outperforms th e recent method \nCBRNet by 1.19% (Table V).  To our knowledge base, this \nscore is state -of-the-art on this dataset.  Notably, our method \nachieves the highest Recall (87.52%) and surpasses other \nnetworks by a significant gap (more than 2.33%). Higher Recall \nmeans fewer building  pixels missed. As shown in Fig. 6, our \napproach outperforms other networks in recognizing hard \nbuilding pixels and maintaining the integrity of buildings , \nwhich benefits from the dual-path structure and the aggregation \nof the global context and spatial-detailed context. \nTABLE V \nQUANTITATIVE COMPARISON WITH STATE-OF-THE-ART METHODS ON THE \nMASSACHUSETTS BUILDING DATASET. \nMethod IoU Precision Recall F1 \nU-Net 67.61 79.13 82.29 80.68 \nDeepLab V3+ 69.23 84.73 79.10 81.82 \nMA-FCN 73.80 87.07 82.89 84.93 \nBRRNet 73.25 - - 84.56 \nBias-UNet 73.49 83.34 86.15 84.72 \nCBRNet 74.55 86.50 84.36 85.42 \nMANet 70.76 82.00 83.77 82.88 \nBANet 72.20 83.07 84.66 83.86 \nDC-Swin 72.59 83.07 85.19 84.12 \nBuildFormer 75.74 84.90 87.52 86.19 \n \nTABLE VI \nQUANTITATIVE COMPARISON WITH STATE-OF-THE-ART METHODS ON THE WHU \nBUILDING DATASET. \nMethod IoU Precision Recall F1 \nCU-Net 87.10 94.60 91.70 93.13 \nSiU-Net 88.40 93.80 93.90 93.85 \nSRI-Net 89.23 95.67 93.69 94.51 \nDE-Net 90.12 95.00 94.60 94.08 \nEU-Net 90.56 94.98 95.10 95.04 \nMA-FCN 90.70 95.20 95.10 95.15 \nMAP-Net 90.86 95.62 94.81 95.21 \nMSST-Net 88.00 - - 88.20 \nSTT 90.48 - - 94.97 \nBuildFormer 91.44 95.40 95.65 95.53 \n \nTABLE Vâ…¡ \nQUANTITATIVE COMPARISON WITH STATE-OF-THE-ART METHODS ON THE \nINRIA AERIAL IMAGE LABELING DATASET. \nMethod IoU Precision Recall F1 \nU-Net 70.78 85.18 80.72 82.89 \nSRI-Net 76.84 - - 86.32 \nDS-Net 80.73 - - - \nBRRNet 77.05 - - 86.61 \nSiU-Net 71.40 84.60 82.10 83.33 \nCBRNet 81.10 89.93 89.20 89.56 \nSTT 79.42 - - 87.99 \nSwinUperNet 79.53 87.55 89.67 88.60 \nBuildFormer 81.44 88.81 90.75 89.77 \n \nFor the WHU building dataset, the proposed method yields \nthe best IoU (91.44%), which not only exceeds the advanced \nCNN-based building extraction methods by more than 0.58% \nbut also outperforms the recent Sparse Token Transformer \n(STT) by 0.96% (Table VI). For the Inria Aerial Image Labeling \ndataset, our approach still maintains the most advanced \nperformance with 81.44% IoU and 89.77% F1 score (Table VII). \nThe predicted results on these two datasets are shown in Fig.7. \nAll results reveal the importance of global context for building \nextraction and the superiority of dual -path structure for Vision \nTransformer. \nVI. CONCLUSION \nIn this paper, we propose d a novel Vision Transformer for \nbuilding extraction from fine-resolution remote sensing images, \nnamely the BuildFormer. Since both global context and spatial-\ndetailed context were crucial for precise building segmentation, \nwe designed the BuildFormer based on the dual -path structure \nwhich could capture the global information and spatial details \nsimultaneously. Further more, we propose d a window -based \nlinear multi-head self-attention to reduce the complexity of the \nwindow-based multi-head self-attention into ğ‘‚ğ‘‚(ğ‘ğ‘). Benefiting \nfrom this, the BuildFormer c ould apply large windows to \nenhance the global context modelling without resulting in high \ncomputation. An extensive ablation study evaluated the impact \nof each component of the BuildFormer and experimental results \non the Massachusetts, WHU, and Inria building datasets \ndemonstrated the superiority of the proposed method  in \ncomparison with state-of-the-art methods. \nTABLE â…£ \nTHE ABLATION STUDY OF THE W-LMHSA WITH DIFFERENT WINDOW SIZES. * MEANS THE NETWORK RUNS OUT OF MEMORY. \nGlobal contextual path Window Size Complexity (M) Memory (MB) Parameter (M) Speed (FPS) IoU \nW-MHSA \n8 2.39 7477.36 \n40.52 \n16.73 72.70 \n16 9.56 9301.36 14.98 73.56 \n32 38.24 16597.36 10.28 * \n64 152.96 * * * \nW-LMHSA (ours) \n8 1.77 7060.89 \n40.52 \n17.03 74.83 \n16 7.08 7032.99 17.17 75.74 \n32 28.31 7024.14 17.18 75.59 \n64 113.25 7022.16 17.04 75.36 \n \n 8 \nREFERENCES \n \nFig. 6. Visualized results of the U-Net, BANet, DC-Swin and BuildFormer (ours) on the Massachusetts Building dataset. \n \nFig. 7. Predicted results of the BuildFormer on the WHU Building dataset (left) and the Inria Aerial Image Labeling dataset (right). \n\n 9 \n[1]  W. Li, C. He, J. Fang, and H. Fu, \"Semantic \nsegmentation based building extraction method using \nmulti-source gis map datasets and satellite imagery,\" \nin Proceedings of the IEEE Conference on Computer \nVision and Pattern Recognition Workshops, 2018, pp. \n238-241.  \n[2] L. Dong and J. Shan, \"A comprehensive review of \nearthquake-induced building damage detection with \nremote sensing techniques,\" ISPRS Journal of \nPhotogrammetry and Remote Sensing, vol. 84, pp. 85-\n99, 2013. \n[3] M. Belgiu and L. DrÇguÅ£, \"Comparing supervised and \nunsupervised multiresolution segmentation \napproaches for extracting buildings from very high \nresolution imagery,\" ISPRS Journal of \nPhotogrammetry and Remote Sensing, vol. 96, pp. 67-\n75, 2014/10/01/ 2014, doi: \nhttps://doi.org/10.1016/j.isprsjprs.2014.07.002. \n[4]  M. Vakalopoulou, K. Karantzalos, N. Komodakis, \nand N. Paragios, \"Building detect ion in very high \nresolution multispectral data with deep learning \nfeatures,\" in 2015 IEEE international geoscience and \nremote sensing symposium (IGARSS), 2015: IEEE, pp. \n1873-1876.  \n[5] D. Griffiths and J. Boehm, \"Improving public data for \nbuilding segmentation from Convolutional Neural \nNetworks (CNNs) for fused airborne lidar and image \ndata using active contours,\" ISPRS Journal of \nPhotogrammetry and Remote Sensing, vol. 154, pp. \n70-83, 2019. \n[6] H. Guo, Q. Shi, B. Du, L. Zhang, D. Wang, and H. \nDing, \"Scene -driven multitask parallel attention \nnetwork for building extraction in high -resolution \nremote sensing images,\" IEEE Transactions on \nGeoscience and Remote Sensing, vol. 59, no. 5, pp. \n4287-4306, 2020. \n[7] Y. Zhang, \"Optimisation of building detection in \nsatellite images by combining multispectral \nclassification and texture filtering,\" ISPRS journal of \nphotogrammetry and remote sensing, vol. 54, no. 1, pp. \n50-60, 1999. \n[8] M. Turker and D. Koc-San, \"Building extraction from \nhigh-resolution optical spaceborne images using the \nintegration of support vector machine (SVM) \nclassification, Hough transformation and perceptual \ngrouping,\" International Journal of Applied Earth \nObservation and Geoinformation, vol. 34, pp. 58- 69, \n2015. \n[9] F. Dornaika, A. Moujahid, Y. El Merabet, and Y. \nRuichek, \"Building detection from orthophotos using \na machine learning approach: An empirical study on \nimage segmentation and descriptors,\" Expert Systems \nwith Applications, vol. 58, pp. 130-142, 2016. \n[10] Y. LeCun, Y. Bengio, and G. J. n. Hinton, \"Deep \nlearning,\" Nature, vol. 521, no. 7553, pp. 436 -444, \n2015. \n[11]  Z. Liu, P. Luo, X. Wang, and X. Tang, \"Deep \nlearning face attributes in the wild,\" in Proceedings of \nthe IEEE international conference on computer vision, \n2015, pp. 3730-3738.  \n[12] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \n\"Imagenet classification with deep convolutional \nneural networks,\" Advances in neural information \nprocessing systems, vol. 25, pp. 1097-1105, 2012. \n[13] D. Marmanis, M. Datcu, T. Esch, and U. Stilla, \"Deep \nLearning Earth Observation Classification Using \nImageNet Pretrained Networks,\" IEEE Geoscience \nand Remote Sensing Letters, vol. 13, no. 1, pp. 105-\n109, 2016, doi: 10.1109/LGRS.2015.2499239. \n[14] X. X. Zhu et al., \"Deep learning in remote sensing: A \ncomprehensive review and list of resources,\" IEEE \nGeoscience and Remote Sensing Magazine, vol. 5, no. \n4, pp. 8-36, 2017. \n[15] L. Ma, Y. Liu, X. Zhang, Y. Ye, G. Yin, and B. A. \nJohnson, \"Dee p learning in remote sensing \napplications: A meta -analysis and review,\" ISPRS \nJournal of Photogrammetry and Remote Sensing, vol. \n152, pp. 166 -177, 2019/06/01/ 2019, doi: \nhttps://doi.org/10.1016/j.isprsjprs.2019.04.015. \n[16] P. Liu et al., \"Building footprint extraction from high-\nresolution images via spatial residual inception \nconvolutional neural network,\" Remote Sensing, vol. \n11, no. 7, p. 830, 2019. \n[17]  J. Long, E. Shelhamer, and T. Darrel l, \"Fully \nconvolutional networks for semantic segmentation,\" in \nProceedings of the IEEE conference on computer \nvision and pattern recognition, 2015, pp. 3431-3440.  \n[18] S. Ji, S. Wei, and M. Lu, \"Fully convolutional \nnetworks for multisource building extra ction from an \nopen aerial and satellite imagery data set,\" IEEE \nTransactions on Geoscience and Remote Sensing, vol. \n57, no. 1, pp. 574-586, 2018. \n[19] G. Wu  et al. , \"Automatic building segmentation of \naerial imagery using multi -constraint fully \nconvolutional networks,\" Remote Sensing, vol. 10, no. \n3, p. 407, 2018. \n[20] Z. Shao, P. Tang, Z. Wang, N. Saleem, S. Yam, and C. \nSommai, \"BRRNet: A fully convolutional neural \nnetwork for automatic building extraction from high-\nresolution remote sensing images,\" Remote Sensing, \nvol. 12, no. 6, p. 1050, 2020. \n[21]  O. Ronneberger, P. Fischer, and T. Brox, \"U -Net: \nConvolutional Networks for Biomedical Image \nSegmentation,\" Cham, 2015: Springer International \nPublishing, in Medical Image Computing and \nComputer-Assisted Intervention â€“ MICCAI 2015, pp. \n234-241.  \n[22] V. Badrinarayanan, A. Kendall, and R. Cipolla, \n\"Segnet: A deep convolutional encoder -decoder \narchitecture for image segmentation,\" IEEE \ntransactions on pattern analysis and machine \nintelligence, vol. 39, no. 12, pp. 2481-2495, 2017. \n[23]  X. Wang, R. Girshick, A. Gupta, and K. He, \"Non -\nlocal neural networks,\" in Proceedings of the IEEE \nconference on computer vision and pattern \nrecognition, 2018, pp. 7794-7803.  \n[24]  J. Fu  et al. , \"Dual attention network for scene \nsegmentation,\" in Proceedings of the IEEE \n 10 \nConference on Computer Vision and Pattern \nRecognition, 2019, pp. 3146-3154.  \n[25] Z. Huang  et al. , \"CCNet: Criss -Cross Attention for \nSemantic Segmentation,\" IEEE Transactions on \nPattern Analysis and Machine Intelligence, 2020. \n[26]  Y. Yuan, X. Chen, and J. Wang, \"Object -contextual \nrepresentations for semantic segmentation,\" in \nComputer Vision â€“ECCV 2020: 16th European \nConference, Glasgow, UK, August 23 â€“28, 2020, \nProceedings, Part VI 16, 2020: Springer, pp. 173-190.  \n[27] Q. Zhu, C. Liao, H. Hu, X. Mei, and H. Li, \"MAP-Net: \nMultiple attending path neural network for building \nfootprint extraction from remote sensed imagery,\" \nIEEE Transactions on Geoscience and Remote \nSensing, 2020. \n[28]  A. Vaswani  et al. , \"Attention is all you need,\" in \nAdvances in neural information processing systems , \n2017, pp. 5998-6008.  \n[29] A. Dosovitskiy et al., \"An image is worth 16x16 words: \nTransformers for image recognition at scale,\" arXiv \npreprint arXiv:2010.11929, 2020. \n[30] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, \n\"Deformable DETR: Deformable Transformers for \nEnd-to-End Object Detection,\" arXiv preprint \narXiv:2010.04159, 2020. \n[31]  S. Zheng et al., \"Rethinking semantic segmentation \nfrom a sequ ence-to-sequence perspective with \ntransformers,\" in Proceedings of the IEEE/CVF \nConference on Computer Vision and Pattern \nRecognition, 2021, pp. 6881-6890.  \n[32] K. Han et al., \"A survey on vision transformer,\" IEEE \nTransactions on Pattern Analysis and Mac hine \nIntelligence, 2022. \n[33] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. \nKhan, and M. Shah, \"Transformers in vision: A \nsurvey,\" ACM Computing Surveys (CSUR), 2021. \n[34]  Z. Liu et al., \"Swin transformer: Hierarchical vision \ntransformer using shifted windows,\" in Proceedings of \nthe IEEE/CVF International Conference on Computer \nVision, 2021, pp. 10012-10022.  \n[35] H. Guo, B. Du, L. Zhang, and X. Su, \"A coarse-to-fine \nboundary refinement network for building footprint \nextraction from remote sensing image ry,\" ISPRS \nJournal of Photogrammetry and Remote Sensing, vol. \n183, pp. 240-252, 2022. \n[36]  C. Yu, J. Wang, C. Peng, C. Gao, G. Yu, and N. Sang, \n\"Bisenet: Bilateral segmentation network for real-time \nsemantic segmentation,\" in Proceedings of the \nEuropean c onference on computer vision (ECCV) , \n2018, pp. 325-341.  \n[37] A. O. Ok, \"Automated detection of buildings from \nsingle VHR multispectral images using shadow \ninformation and graph cuts,\" ISPRS journal of \nphotogrammetry and remote sensing, vol. 86, pp. 21 -\n40, 2013. \n[38] M. Awrangjeb, C. Zhang, and C. S. Fraser, \"Improved \nbuilding detection using texture information,\" \nInternational Archives of Photogrammetry, Remote \nSensing and Spatial Information Sciences, vol. 38, pp. \n143-148, 2011. \n[39] X. Huang and L. Zhang, \"A multidirectional and \nmultiscale morphological index for automatic building \nextraction from multispectral GeoEye -1 imagery,\" \nPhotogrammetric Engineering & Remote Sensing, vol. \n77, no. 7, pp. 721-732, 2011. \n[40] X. Huang and L. Zhang, \"Morphological \nbuilding/shadow index for building extraction from \nhigh-resolution imagery over urban areas,\" IEEE \nJournal of Selected Topics in Applied Earth \nObservations and Remote Sensing, vol. 5, no. 1, pp. \n161-172, 2011. \n[41] Z. Li, W. Shi, Q. Wang, and Z. Miao, \"Extracting man-\nmade objects from high spatial resolution remote \nsensing images via fast level set evolutions,\" IEEE \nTransactions on Geoscience and Remote Sensing, vol. \n53, no. 2, pp. 883-899, 2014. \n[42] T. Zhang, X. Huang, D. Wen, and J. Li, \"Urban \nbuilding dens ity estimation from high -resolution \nimagery using multiple features and support vector \nregression,\" IEEE Journal of Selected Topics in \nApplied Earth Observations and Remote Sensing, vol. \n10, no. 7, pp. 3265-3280, 2017. \n[43] G. Zhou and X. Zhou, \"Seamless fusion of LiDAR and \naerial imagery for building extraction,\" IEEE \nTransactions on Geoscience and Remote Sensing, vol. \n52, no. 11, pp. 7393-7407, 2014. \n[44] J. Huang, X. Zhang, Q. Xin, Y. Sun, and P. Zhang, \n\"Automatic building extraction from high -resolution \naerial images and LiDAR data using gated residual \nrefinement network,\" ISPRS journal of \nphotogrammetry and remote sensing, vol. 151, pp. 91-\n105, 2019. \n[45] Z. Zheng, A. Ma, L. Zhang, and Y. Zhong, \"Deep \nmultisensor learning for missing-modality all-weather \nmapping,\" ISPRS Journal of Photogrammetry and \nRemote Sensing, vol. 174, pp. 254-264, 2021. \n[46] Y. Sun, Y. Hua, L. Mou, and X. X. Zhu, \"Cg -net: \nConditional gis-aware network for individual building \nsegmentation in vhr sar images,\" IEEE Transactions \non Geoscience and Remote Sensing, vol. 60, pp. 1-15, \n2021. \n[47] Y. Xu, L. Wu, Z. Xie, and Z. Chen, \"Building \nextraction in very high resolution remote sensing \nimagery using deep learning and guided filters,\" \nRemote Sensing, vol. 10, no. 1, p. 144, 2018. \n[48] S. Wei, S. Ji, and M. Lu, \"Toward automatic building \nfootprint delineation from aerial images using cnn and \nregularization,\" IEEE Transactions on Geoscience \nand Remote Sensing, vol. 58, no. 3, pp. 2178 -2189, \n2019. \n[49] H. Zhang, Y. Liao, H. Yang, G. Yang, an d L. Zhang, \n\"A Local-Global Dual-Stream Network for Building \nExtraction From Very -High-Resolution Remote \nSensing Images,\" IEEE Transactions on Neural \nNetworks and Learning Systems, 2020. \n[50] H. Hosseinpour, F. Samadzadegan, and F. D. Javan, \n\"CMGFNet: A deep cross-modal gated fusion network \nfor building extraction from very high -resolution \nremote sensing images,\" ISPRS Journal of \n 11 \nPhotogrammetry and Remote Sensing, vol. 184, pp. \n96-115, 2022. \n[51] M. Y. Yang, S. Kumaar, Y. Lyu, and F. Nex, \"Real -\ntime Semanti c Segmentation with Context \nAggregation Network,\" ISPRS Journal of \nPhotogrammetry and Remote Sensing, vol. 178, pp. \n124-134, 2021. \n[52] R. Li, S. Zheng, C. Zhang, C. Duan, L. Wang, and P. \nM. Atkinson, \"ABCNet: Attentive bilateral contextual \nnetwork for efficient semantic segmentation of Fine -\nResolution remotely sensed imagery,\" ISPRS Journal \nof Photogrammetry and Remote Sensing, vol. 181, pp. \n84-98, 2021/11/01/ 2021, doi: \nhttps://doi.org/10.1016/j.isprsjprs.2021.09.005. \n[53] R. Li  et al. , \"Multiattention network for semantic \nsegmentation of fine -resolution remote sensing \nimages,\" IEEE Transactions on Geoscience and \nRemote Sensing, 2021. \n[54] M. Guo, H. Liu, Y. Xu, and Y. Huang, \"Building \nextraction based on U-Net with an attention block and \nmultiple losses,\" Remote Sensing, vol. 12, no. 9, p. \n1400, 2020. \n[55] Q. Tian, Y. Zhao, Y. Li, J. Chen, X. Chen, and K. Qin, \n\"Multiscale building extraction with refined attention \npyramid networks,\" IEEE Geoscience and Remote \nSensing Letters, vol. 19, pp. 1-5, 2021. \n[56]  P. Das and S. Chand, \"AttentionBuildNet for \nbuilding extraction from aer ial imagery,\" in 2021 \nInternational Conference on Computing, \nCommunication, and Intelligent Systems (ICCCIS) , \n2021: IEEE, pp. 576-580.  \n[57] Z. Chen, D. Li, W. Fan, H. Guan, C. Wang, and J. Li, \n\"Self-attention in reconstruction bias U -Net for \nsemantic segmentation of building rooftops in optical \nremote sensing images,\" Remote Sensing, vol. 13, no. \n13, p. 2524, 2021. \n[58] W. Deng, Q. Shi, and J. Li, \"Attention -Gate-Based \nEncoderâ€“Decoder Network for Automatical Building \nExtraction,\" IEEE Journal of Selected T opics in \nApplied Earth Observations and Remote Sensing, vol. \n14, pp. 2611-2620, 2021. \n[59] X. Pan  et al. , \"Building extraction from high -\nresolution aerial imagery using a generative \nadversarial network with spatial and channel attention \nmechanisms,\" Remote Sensing, vol. 11, no. 8, p. 917, \n2019. \n[60] J. Cai and Y. Chen, \"MHA -Net: Multipath Hybrid \nAttention Network for building footprint extraction \nfrom high-resolution remote sensing imagery,\" IEEE \nJournal of Selected Topics in Applied Earth \nObservations and Remote Sensing, vol. 14, pp. 5807-\n5817, 2021. \n[61]  R. Strudel, R. Garcia, I. Laptev, and C. Schmid, \n\"Segmenter: Transformer for semantic segmentation,\" \nin Proceedings of the IEEE/CVF International \nConference on Computer Vision, 2021, pp. 7262-7272.  \n[62] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. \nAlvarez, and P. Luo, \"SegFormer: Simple and efficient \ndesign for semantic segmentation with transformers,\" \nAdvances in Neural Information Processing Systems, \nvol. 34, 2021. \n[63] Z. Zhong, Y. Li, L. Ma, J. Li, an d W. -S. Zheng, \n\"Spectral-spatial transformer network for \nhyperspectral image classification: A factorized \narchitecture search framework,\" IEEE Transactions \non Geoscience and Remote Sensing, 2021. \n[64] D. Hong  et al. , \"SpectralFormer: Rethinking \nhyperspectral image classification with transformers,\" \nIEEE Transactions on Geoscience and Remote \nSensing, 2021. \n[65] Y. Bazi, L. Bashmal, M. M. A. Rahhal, R. A. Dayil, \nand N. A. Ajlan, \"Vision transformers for remote \nsensing image classification,\" Remote Sensing, vol. 13, \nno. 3, p. 516, 2021. \n[66] P. Deng, K. Xu, and H. Huang, \"When CNNs meet \nvision transformer: A joint framework for remote \nsensing scene classification,\" IEEE Geoscience and \nRemote Sensing Letters, vol. 19, pp. 1-5, 2021. \n[67] X. He, Y. Chen, and Z. L in, \"Spatial -spectral \ntransformer for hyperspectral image classification,\" \nRemote Sensing, vol. 13, no. 3, p. 498, 2021. \n[68] K. Xu, P. Deng, and H. Huang, \"Vision Transformer: \nAn Excellent Teacher for Guiding Small Networks in \nRemote Sensing Image Scene C lassification,\" IEEE \nTransactions on Geoscience and Remote Sensing, \n2022. \n[69] L. Wang, R. Li, C. Duan, C. Zhang, X. Meng, and S. \nFang, \"A Novel Transformer Based Semantic \nSegmentation Scheme for Fine- Resolution Remote \nSensing Images,\" IEEE Geoscience and Remote \nSensing Letters, vol. 19, pp. 1 -5, 2022, doi: \n10.1109/LGRS.2022.3143368. \n[70] L. Gao et al., \"STransFuse: Fusing Swin Transformer \nand Convolutional Neural Network for Remote \nSensing Image Semantic Segmentation,\" IEEE \nJournal of Selected Topics in Ap plied Earth \nObservations and Remote Sensing, vol. 14, pp. 10990-\n11003, 2021. \n[71] C. Zhang, W. S. Jiang, Y. Zhang, W. Wang, Q. Zhao, \nand C. J. Wang, \"Transformer and CNN Hybrid Deep \nNeural Network for Semantic Segmentation of Very -\nhigh-resolution Remote Se nsing Imagery,\" IEEE \nTransactions on Geoscience and Remote Sensing, \n2022. \n[72] L. Wang, R. Li, D. Wang, C. Duan, T. Wang, and X. \nMeng, \"Transformer Meets Convolution: A Bilateral \nAwareness Network for Semantic Segmentation of \nVery Fine Resolution Urban Sce ne Images,\" Remote \nSensing, vol. 13, no. 16, p. 3065, 2021. \n[73] X. He, Y. Zhou, J. Zhao, D. Zhang, R. Yao, and Y. \nXue, \"Swin Transformer Embedding UNet for Remote \nSensing Image Semantic Segmentation,\" IEEE \nTransactions on Geoscience and Remote Sensing, \n2022. \n[74] Z. Xu, W. Zhang, T. Zhang, Z. Yang, and J. Li, \n\"Efficient transformer for remote sensing image \nsegmentation,\" Remote Sensing, vol. 13, no. 18, p. \n3585, 2021. \n 12 \n[75] H. Chen, Z. Qi, and Z. Shi, \"Remote sensing image \nchange detection with transformers ,\" IEEE \nTransactions on Geoscience and Remote Sensing, \n2021. \n[76] C. Zhang, L. Wang, S. Cheng, and Y. Li, \"SwinSUNet: \nPure Transformer Network for Remote Sensing Image \nChange Detection,\" IEEE Transactions on Geoscience \nand Remote Sensing, 2022. \n[77] Z. Sun, W. Zhou, C. Ding, and M. Xia, \"Multi -\nResolution Transformer Network for Building and \nRoad Segmentation of Remote Sensing Image,\" ISPRS \nInternational Journal of Geo-Information, vol. 11, no. \n3, p. 165, 2022. \n[78] X. Chen, C. Qiu, W. Guo, A. Yu, X. T ong, and M. \nSchmitt, \"Multiscale feature learning by transformer \nfor building extraction from satellite images,\" IEEE \nGeoscience and Remote Sensing Letters, 2022. \n[79] K. Chen, Z. Zou, and Z. Shi, \"Building Extraction \nfrom Remote Sensing Images with Sparse  Token \nTransformers,\" Remote Sensing, vol. 13, no. 21, p. \n4441, 2021. \n[80] W. Yuan and W. Xu, \"MSST -Net: A Multi -Scale \nAdaptive Network for Building Extraction from \nRemote Sensing Images Based on Swin Transformer,\" \nRemote Sensing, vol. 13, no. 23, p. 4743, 2021. \n[81]  A. Kirillov, R. Girshick, K. He, and P. DollÃ¡r, \n\"Panoptic feature pyramid networks,\" in Proceedings \nof the IEEE/CVF Conference on Computer Vision and \nPattern Recognition, 2019, pp. 6399-6408.  \n[82] G. Yang, Q. Zhang, and G. Zhang, \"EANet: Edge -\naware network for the extraction of buildings from \naerial images,\" Remote Sensing, vol. 12, no. 13, p. \n2161, 2020. \n[83]  K. Zhao, J. Kang, J. Jung, and G. Sohn, \"Building \nextraction from satellite images using mask R -CNN \nwith building boundary regularizat ion,\" in \nProceedings of the IEEE conference on computer \nvision and pattern recognition workshops , 2018, pp. \n247-251.  \n[84] X. Zheng, L. Huan, G. -S. Xia, and J. Gong, \"Parsing \nvery high resolution urban scene images by learning \ndeep ConvNets with edge-aware loss,\" ISPRS Journal \nof Photogrammetry and Remote Sensing, vol. 170, pp. \n15-28, 2020. \n[85]  M. Fan  et al. , \"Rethinking bisenet for real -time \nsemantic segmentation,\" in Proceedings of the \nIEEE/CVF conference on computer vision and pattern \nrecognition, 2021, pp. 9716-9725.  \n[86]  E. Maggiori, Y. Tarabalka, G. Charpiat, and P. Alliez, \n\"Can semantic labeling methods generalize to any city? \nthe inria aerial image labeling benchmark,\" in 2017 \nIEEE International Geoscience and Remote Sensing \nSymposium (IGARSS), 2017: IEEE, pp. 3226-3229.  \n[87]  K. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual \nlearning for image recognition,\" in Proceedings of the \nIEEE conference on computer vision and pattern \nrecognition, 2016, pp. 770-778.  \n[88]  L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and \nH. Adam, \"Encoder- decoder with atrous separable \nconvolution for semantic image segmentation,\" in \nProceedings of the European conference on computer \nvision (ECCV), 2018, pp. 801-818.  \n[89] W. Kang, Y. Xiang, F. Wang, and H. You, \"EU -net: \nAn efficient fully convolutional network for building \nextraction from optical remote sensing images,\" \nRemote Sensing, vol. 11, no. 23, p. 2813, 2019. \n[90] H. Liu  et al. , \"DE -Net: Deep encoding network for \nbuilding extraction from high- resolution remote \nsensing imagery,\" Remote Sensing, vol. 11, no. 20, p. \n2380, 2019. \n ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8217709064483643
    },
    {
      "name": "Transformer",
      "score": 0.6679791808128357
    },
    {
      "name": "Feature extraction",
      "score": 0.6039729118347168
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5925760865211487
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5914158821105957
    },
    {
      "name": "Segmentation",
      "score": 0.5360226035118103
    },
    {
      "name": "Computer vision",
      "score": 0.5093744993209839
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.33295905590057373
    },
    {
      "name": "Voltage",
      "score": 0.11430168151855469
    },
    {
      "name": "Engineering",
      "score": 0.0864497721195221
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I37461747",
      "name": "Wuhan University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I1327237609",
      "name": "Ministry of Education of the People's Republic of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I211433327",
      "name": "Ministry of Natural Resources",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I39555362",
      "name": "University of Warwick",
      "country": "GB"
    }
  ]
}