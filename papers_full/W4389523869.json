{
  "title": "Grammar induction pretraining for language modeling in low resource contexts",
  "url": "https://openalex.org/W4389523869",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2899177045",
      "name": "Xuanda Chen",
      "affiliations": [
        "Mila - Quebec Artificial Intelligence Institute",
        "McGill University"
      ]
    },
    {
      "id": "https://openalex.org/A2765644789",
      "name": "Eva Portelance",
      "affiliations": [
        "McGill University",
        "Mila - Quebec Artificial Intelligence Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2949399644"
  ],
  "abstract": "In the context of the BabyLM challenge, we present a language model which uses pretrained embeddings from a grammar induction model as its first layer.We compare it to one of the challenge's baseline models and a minimally different baseline which uses random embeddings.We find that though our model shows improvement over the challenge's baseline, the model with randomly initialized embeddings performs equally well.Our results suggest that it is not the pretrained embeddings which aided performance, but likely our tokenizer and choice of hyperparameters.",
  "full_text": "Proceedings of the 27th Conference on Computational Natural Language Learning:\nVolume 2: The BabyLM Challenge, pages 69–73\nDecember 6-7, 2023 ©2023 Association for Computational Linguistics\nGrammar induction pretraining for language modeling in low resource\ncontexts\nXuanda Chen and Eva Portelance∗\nDepartment of Linguistics, McGill University\nMila - Quebec Artificial Intelligence Institute\nAbstract\nIn the context of the BabyLM challenge, we\npresent a language model which uses pretrained\nembeddings from a grammar induction model\nas its first layer. We compare it to one of the\nchallenge’s baseline models and a minimally\ndifferent baseline which uses random embed-\ndings. We find that though our model shows\nimprovement over the challenge’s baseline, the\nmodel with randomly initialized embeddings\nperforms equally well. Our results suggest\nthat it is not the pretrained embeddings which\naided performance, but likely our tokenizer and\nchoice of hyperparameters.\n1 Introduction\nThe BabyLM Challenge (Warstadt et al., 2023)’s\ngoal is to develop language models and training\npipelines that can learn reasonable linguistic repre-\nsentations for downstream language modeling task\nusing much more constrained datasets. With this\ngoal in mind, we hypothesized that giving mod-\nels additional information about syntactic structure\nmay help them learn more generalizable represen-\ntations of language. As part of the strict track of the\nchallenge, we were not allowed to give additional\nsyntactic labels as part of our training data, so in-\nstead we propose to first induce a compound prob-\nabilistic context-free grammar (compound-PCFG)\nover the data using a neural grammar induction\nmodel (Kim et al., 2019). There are many ways we\ncan then integrate this syntactic information into\na language model. Here, we test a simple method:\nwe initialize a language model using the terminal\ntoken embeddings of a trained grammar induction\nmodel as its embedding layer. We test the effec-\ntiveness of this method on the BabyLM strict-small\nchallenge.1\n∗Corresponding author: eva.portelance@mcgill.ca\n1All code for this project is available in this github reposi-\ntory. The trained models and preprocessed data can be down-\nloaded from this OSF Project repositorythis Open Science\nFramework (OSF) project repository.\n2 Data and preprocessing\nIn the experiments which follow, we use the 10\nMillion word BabyLM task dataset (the strict track\nsmall dataset) to train our language models. Prior to\ntraining, we preprocessed the dataset to remove any\nblank lines or unecessary formatting punctuation\n(e.g. ‘== Title ==’ became ’Title’). Additionally,\nwe split paragraphs such that each new line repre-\nsented a single sentence and removed any sentence\nthat was longer than 40 words.\n2.1 Grammar induction data\nSince grammar induction algorithms can be quite\nmemory intensive, we use a subset of the 10M\nBabyLM dataset to train our grammar induction\nmodel. We randomly sampled a tenth of the sen-\ntences from the corpus, resulting is a smaller gram-\nmar induction dataset containing 991,510 words.\n2.2 Tokenizer\nWe trained a custom tokenizer on the 10M BabyLM\ndataset. To guarantee coverage we created a tok-\nenizer that produces both subwords and word-level\ntokens. Since previous grammar induction models\nused word-level tokens, we wanted to maximize the\nnumber of word-level tokens and keep subwords\nand character tokens to only a limited necessary\nnumber. We therefore trained a tokenizer using\nthe WordPiece algorithm with a vocabulary size of\n10,000 and a maximum alphabet of 72 tokens.\n3 Models\n3.1 Grammar induction model\nWe first trained a compound-PCFG grammar (Kim\net al., 2019) over our subset of the BabyLM small\ncorpus described above. PCFG embeddings are\ntrained to encode terminal rule information, e.g.,\nreflecting syntactic categories in grammar, which\ncould further improve model’s language under-\nstanding ability. We used our tokenizer to split\n69\nTable 1: Overall mean performance on each benchmark\nbenchmark baseline baseline-\ntoken\ngrammar\nBLiMP 62.63 64.78 64.44\nBLiMP suppl. 54.72 54.66 54.88\nSuperGLUE 63.38 68.21 67.93\nMSGS 69.22 67.45 68.08\nsentences into tokens and then induced trees over\nthe corpus2. During learning, the model induces\nembedding representations for the grammar rules\nand terminals, where the terminals are the token\nembeddings.3\nOnce the grammar is induced, we extract the\ntoken embedding layer of the grammar and use it\nas the initial embedding layer for an OPT-125m-\nlike 4 language model with a vocabulary size of\n10,000 5. We then trained this language model on\nnext token prediction using the full BabyLM 10M\ndataset. The embedding layer is trained with other\nlayers and not frozen during training. We will refer\nto this model as the grammar model in the sections\nwhich follow.\n3.2 Baseline models\nWe compare our model results to the OPT-125m\nbaseline model supplied by the BabyLM challenge\n(baseline) and to a baseline OPT-125m language\nmodel that we trained using our tokenizer and\nrandomly initialized embeddings (baseline-token),\nthus using a vocabulary size of 10,000 tokens.\nBaseline-token has the exact same hyperparameters\nas our grammar model and only differs in terms of\nits initial embeddings, here random ones.\n4 Results\nResults for the baseline model were taken directly\nfrom the BabyLM evaluation pipeline project page\n(github.com/babylm/evaluation-pipeline).\nFor the baseline-token and grammar models, these\nwere trained for 3 epochs and tested on validation\naccuracy every 100,000 sentences; we report the\nbest models found during training based on next\ntoken prediction on the validation dataset.\n2See Appendix D for example induced parses\n3Hyperparameters for the grammar induction model are\nreported in Appendix A.\n4We refer to these models as as OPT-125M-like since they\nminimally vary from this baseline, however since their vocab-\nulary size is 10,000, they in fact have 94M parameters.\n510,000 was the original vocabulary size used in Kim et al.\n(2019). Since we did not do hyperparameter search over the\ngrammar induction model, we followed their ideal settings.\nWe tested all models on the BabyLM evaluation\ntasks, which included the Benchmark of Linguistic\nMinimal Pairs (BLiMP) (Warstadt et al., 2020a),\na custom supplementary set of BLiMP-like tasks,\n‘Super’ benchmark for General Language Under-\nstanding Evaluation (SuperGLUE) (Wang et al.,\n2019), and the Mixed Signals Generalization Set\nevaluation (MSGS) (Warstadt et al., 2020b). Re-\nsults are reported in Table 1. The complete perfor-\nmance results by individual task are presented in\nTables 4-7 in Appendix C.\nThe baseline-token and grammar models gener-\nally do better than the baseline on all benchmarks\nexcept MSGS, where they perform slightly worse.\nOverall, the gains in performance are small, though\nthe baseline-token and grammar do seem to do\nquite a lot better on the SuperGLUE benchmark\nthan the baseline in particular. Importantly, we\ndo not find that the grammar model performs bet-\nter than the baseline-token model, suggesting the\nthe addition of our pretrained embeddings did not\nhelp the model perform better on the evaluation\npipeline.\n5 Discussion\nThough our grammar model did do better overall\nthan the BabyLM OPT-125m baseline, when we\ncompared it to our baseline-token model, we did\nnot find that initializing the model with pretrained\ngrammar induction embeddings helped perfomance\noverall. Instead, it may be our tokenizer and\nchoice of hyperparameters which helped improve\nperformance between the baseline and baseline-\ntoken/grammar models.\nSimply using the terminal embedding layer of a\ngrammar induction model to initialize a language\nmodel is not be the most effective way to encode\nsyntactic information into the model. In future\nwork, we would like to consider other methods\nfor combining these two types of models, like en-\nriching the training set with copies of induced con-\nstituents or more complex architectural modifica-\ntion to condition recurrent states with rule embed-\ndings representing the syntactic rules applied to\ngenerate a sub-string at each state.\nReferences\nYoon Kim, Chris Dyer, and Alexander Rush. 2019.\nCompound probabilistic context-free grammars for\ngrammar induction. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\n70\nLinguistics, pages 2369–2385, Florence, Italy. Asso-\nciation for Computational Linguistics.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. Superglue: A stick-\nier benchmark for general-purpose language under-\nstanding systems. Advances in neural information\nprocessing systems, 32.\nAlex Warstadt, Leshem Choshen, Ryan Cotterell, Tal\nLinzen, Aaron Mueller, Ethan Wilcox, Williams Ad-\nina, and Chengxu Zhuang. 2023. Findings of the\nBabyLM Challenge: Sample-efficient pretraining on\ndevelopmentally plausible corpora. In Proceedings\nof the BabyLM Challenge. Association for Computa-\ntional Linguistics (ACL).\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R.\nBowman. 2020a. BLiMP: The benchmark of linguis-\ntic minimal pairs for English. Transactions of the\nAssociation for Computational Linguistics, 8:377–\n392.\nAlex Warstadt, Yian Zhang, Xiaocheng Li, Haokun Liu,\nand Samuel R. Bowman. 2020b. Learning which\nfeatures matter: RoBERTa acquires a preference for\nlinguistic generalizations (eventually). In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n217–235, Online. Association for Computational Lin-\nguistics.\nA Hyperparameters for Grammar\nInduction\nTable 2: Hyper-parameter setting for grammar induc-\ntion\nparameters values\nlatent dimension 64\nnumber of preterminal states 60\nnumber of nonterminal states 30\nsymbol embedding dimension 256\nhidden dim for variational LSTM 768\nword embedding dim 768\nsentence max length 40\nvocab size 10000\nnumber of epochs 15\nbatch size 5\nlearning rate 1e-4\nrandom seed 1213\nB Hyperparameters for OPT language\nmodels\nTable 3: Hyper-parameter setting for language model-\ning\nparameters values\nembedding size 10000\nnumber of epochs 3\nbatch size 20\nlearning rate 1e-4\nwarm-up steps 2000\ngradient clipping threshold 3\nmax grad norm for gradient clipping 1.0\nrandom seed 527\nTable 4: Compute resources for language modeling\nparameters values\nDevice A100\nMemory 32G of GPU memory\nTraining time 12 hours\nC Complete evaluation results\nTable 5: BLiMP accuracy scores\ntask baseline baseline-\ntoken\ngrammar\nAnaphor agr. 63.8 68.6 69.7\nArg. structure 70.6 65.7 63.4\nBinding 67.1 66.5 67.6\nControl/Raising 66.5 62.2 60.4\nDet.-Noun agr. 78.5 77.8 77.2\nEllipsis 62 49.3 51.6\nFiller-Gap 63.8 62.1 63\nIrregular forms 67.5 81.4 81.2\nIsland effects 48.6 48.5 47.9\nNPI licensing 46.7 56.3 55\nQuantifiers 59.6 71.4 68.9\nSubject-verb agr. 56.9 67.5 67.4\nOverall mean 62.63 64.78 64.44\nTable 6: BLiMP-Supplement accuracy scores\ntask baseline baseline-\ntoken\ngrammar\nHypernym 50 52.3 53.3\nQA congr. (easy) 54.7 57.8 45.3\nQA congr. (tricky) 31.5 41.8 40\nSubj.-aux. inversion 80.3 67.5 82.9\nTurn taking 57.1 53.9 52.9\nOverall mean 54.72 54.66 54.88\n71\nTable 7: Super(GLUE) accuracy scores\ntask baseline baseline-\ntoken\ngrammar\nCoLA 64.6 69.6 68.8\nSST-2 81.9 85 83.3\nMRPC (F1) 72.5 76.1 73.7\nQQP (F1) 60.4 78.9 79.1\nMNLI 57.6 66.4 65.9\nMNLI-mm 60 66 67.8\nQNLI 61.5 66.5 66.5\nRTE 60 52.5 52.5\nBoolQ 63.3 67.6 65.8\nMultiRC 55.2 60.2 62.3\nWSC 60.2 61.5 61.5\nOverall mean 63.38 68.21 67.93\nTable 8: MSGS accuracy scores\ntask baseline baseline-\ntoken\ngrammar\ncontr.-raising/lex. cat. 66.5 66.7 68.9\ncontr.-raising/rel. tok. pos. 67 67.2 67.2\nmain verb/lex. cat. 66.5 66.8 66.6\nmain verb/rel. tok. pos. 67.6 66.8 66.8\nsynt. cat./lex. cat. 80.2 69 71.3\nsynt. cat./rel. pos. 67.5 68.2 67.7\nOverall mean 69.22 67.45 68.08\nD Example trees from grammar\ninduction model\n72\nFigure 1: Induced tree for “do you know how long he’s out of work?\"\nFigure 2: Induced tree for “at the time this was the only coast guard air base in california.\"\nFigure 3: Induced tree for “they don’t live anywhere, they sail all the time, but they often come ashore to talk to me.\"\n73",
  "topic": "Baseline (sea)",
  "concepts": [
    {
      "name": "Baseline (sea)",
      "score": 0.8552933931350708
    },
    {
      "name": "Computer science",
      "score": 0.7563767433166504
    },
    {
      "name": "Hyperparameter",
      "score": 0.7497227787971497
    },
    {
      "name": "Grammar",
      "score": 0.7110381722450256
    },
    {
      "name": "Context (archaeology)",
      "score": 0.7021245360374451
    },
    {
      "name": "Natural language processing",
      "score": 0.6582430601119995
    },
    {
      "name": "Language model",
      "score": 0.6193426847457886
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5792681574821472
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.4350889325141907
    },
    {
      "name": "Linguistics",
      "score": 0.24798992276191711
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I5023651",
      "name": "McGill University",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I4210164802",
      "name": "Mila - Quebec Artificial Intelligence Institute",
      "country": "CA"
    }
  ],
  "cited_by": 1
}