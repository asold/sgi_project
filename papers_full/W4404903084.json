{
    "title": "LARGE LANGUAGE MODELS AND MACHINE TRANSLATION",
    "url": "https://openalex.org/W4404903084",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A3005980691",
            "name": "Maryana Tomenchuk",
            "affiliations": [
                "Uzhhorod National University"
            ]
        },
        {
            "id": "https://openalex.org/A5114943530",
            "name": "Kseniia Popovych",
            "affiliations": [
                "Uzhhorod National University"
            ]
        },
        {
            "id": "https://openalex.org/A3005980691",
            "name": "Maryana Tomenchuk",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5114943530",
            "name": "Kseniia Popovych",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3213404645",
        "https://openalex.org/W4389942342",
        "https://openalex.org/W4403142223",
        "https://openalex.org/W4389403639",
        "https://openalex.org/W3011986865",
        "https://openalex.org/W2560449497",
        "https://openalex.org/W4388122952",
        "https://openalex.org/W139096563",
        "https://openalex.org/W4367701273"
    ],
    "abstract": null,
    "full_text": " \nVěda a perspektivy № 11(42) 2024 \nISSN 2695-1584 (Print) \nISSN 2695-1592 (Online) \n \n \n \n   422 \nhttps://doi.org/10.52058/2695-1592-2024-11(42)-422-431 \n \nMaryana Tomenchuk \n \nPhD in Philology, Associate Professor of the \nDepartment of Applied Linguistics, \nUzhhorod National University, Uzhhorod, Ukraine \nhttps://orcid.org/0000-0002-2036-4616 \n \nKseniia Popovych  \n \nMaster Graduate Student of Applied Linguistics, \nUzhhorod National University,  \nUzhhorod, Ukraine \n \nLARGE LANGUAGE MODELS AND MACHINE TRANSLATION \n \nAbstract. Our article deals with linguistic peculiarities of machine translation \nprovided by large language models, focusing on its lexical, semantic and syntactic \naspects. Due to the fact that large language models are a rapidly developing notion \nof 21st century, the ability to understand their architecture and relation to linguistics \nand translation in particular is of a great importance.  \nThe study is based on the investigation of ChatGPT –  a very efficient large \nlanguage model, which is capable of producing human- like outputs when provided \nwith a certain task. Through the analysis of ChatGPT -responses, we further \ninvestigate its ability to convey linguistic meaning in different aspects of a language.  \nThe main purpose of our investigation is to analyze linguistic notions from \ndifferent subfields, such as syntax, lexis and semantics by reviewing the translations \nof GPT from target to source language. In our case, these are represented by English \nand Ukrainian languages.  \nThe corpus of texts which were analyzed in our research has been taken from \nEnglish scientific papers published in online scientific journals [11], [13], [15] from \nvarious academic fields, such as artificial intelligence, computer science, \nmathematics, chemistry, biology etc. The motivation to choose scientific discourse \nlies within the fact that academic language is rich in terminology, formality and \nspecial structures that allow to investigate the performance of GPT in different areas. \nAfter the analysis of a model was conducted, the outputs of ChatGPT were compared \nto the human translation and further investigated.  \nOur investigation has shown that such a linguistic model is a powerful tool \nthan can be used in the area of machine translation. Its structure and arc hitecture \ncontribute a lot to the identification of various patterns in text, which imitates the \n \nVěda a perspektivy № 11(42)2024 \nISSN 2695-1584 (Print) \nISSN 2695-1592 (Online) \n \n \n \n   423 \nway human percept and analyze information given. However, although being very \nprecise and natural in the formation of responses, ChatGPT proved to be not yet a \nperfect machine that would substitute a professional’s work at the whole. The results \nof our investigation show that it tends to make mistakes when it comes to different \ncomplex structures of a language, in our case – English and Ukrainian.  \nIn our work, we used descriptive analysis of the content of large language \nmodels and linguistics to provide the relevant data on the investigated topic and to \nhighlight the most common features of academic language translation and \nexploratory analysis to find most common issues occurring when translating such a \ncorpus from English into the Ukrainian language. The inferential analysis was used \nin order to reach some conclusions in investigated data, including the highlight of \nlinguistic aspect of machine translation. We used the comparison method to discover \nthe differences of lexis and grammar in the English and Ukrainian languages.  \nKeywords: machine translation, large language model, artificial intelligence, \nlexis, syntax, semantics.  \n \nThe problem statement: Due to the growing interest to artificial intelligence \nin the field of linguistics and machine translation, it is essential to investigate the \nperformance of such models for this purpose. The wide range of linguistic \npeculiarities cause various issues for a machine , which may result into improper \ntranslation and other problems. Our aim is to investigate the most common issues \nthat occur in such an activity and provide possible solutions to them. \nReview of recent publication: As the area of artificial intelligence is  \ncontinuously evolving, the amount of scientific literature for these purposes is \nrapidly expanding. This article draws upon the works of prominent scholars such as \nBegus [2], Curry [4], and Gubelmann [6].  \nThe aim of the study is analyze the most common l exico-semantic and \nsyntactic features that occur in the process of translation from English to Ukrainian \nby a large language model, in our case –  ChatGPT and propose the potential \nsolutions to problems that persist in such a phenomenon.  \nResults and Discussions: Large language models play a crucial role in the \narea of modern linguistics, serving as a helping tool in translation of corpora from \nvarious fields of language.  \nModern approaches in linguistics enable the application of various computer \ntechnologies in order to make the process of translation more clear, easy and less \ntime-consuming. Machine translation, a subfield of traditional translation practices, \nincorporates different techniques from other branches like artificial intelligence, \nmachine learning and deep neural networks in particular. \nIn the field of natural language processing, a large language model is a \ncomputational model which is designed to perform a wi de range of NLP tasks, \nincluding general -purpose language generation and classification. These models \n \nVěda a perspektivy № 11(42) 2024 \nISSN 2695-1584 (Print) \nISSN 2695-1592 (Online) \n \n \n \n   424 \nachieve their capabilities by learning statistical relationships from extensive text \ncorpora through a computationally intensive training process that involves both self-\nsupervised and semi -supervised techniques [2, p.63]. Large language models are \nparticularly adept at text generation, a special type of generative AI, where they \nprocess input text and predict subsequent tokens or words iteratively.  \nLarge language models have become very successful recently in many areas. \nMost of these models use a transformer- based design, which is built to handle \nsequential data like language. ChatGPT, the focus of our study, is based on this \ntransformer model. OpenAI’s GPT [3] (which is generative pre-trained transformer) \nis especially important in natural language processing because it works in an \nautoregressive way. This means that it predicts the next word in a sequence based \non previous words, using large amounts of text data to understand language patterns. \nFrom a language perspective, GPT has several features that make it good for \ntasks like machine translation. One key feature is its autoregressive modeling, where \nit generates text one word at a time by considering the words that came before. This \nmakes it strong at creating coherent and meaningful responses, useful in \nconversations and creative writing. Another important feature is its two-step training \nprocess: pre -training and fine -tuning. During pre -training, GPT  learns general \nlanguage rules and patterns from a large amount of text. After that, it is fine -tuned \nfor specific tasks like translation, summarization, or answering questions, making it \nmore specialized and effective. \nMachine translation techniques have traditionally been categorized based on \ntheir translation strategies, with three main systems being direct systems, transfer \nsystems, and interlingua systems [5, p. 120]. Direct systems focus on mapping \nphrases or structures from the source language directly to the target language through \ndetailed pattern matching, while making adjustments for elements like word order. \nThis approach was common in early translation systems and remains in use in some \nmodern translation tools for personal computers. Large language models like GPT \nenhance this process by using their extensive pre-trained data to predict and generate \ntranslations more flexibly and accurately. By adjusting word order and other \nsyntactic elements in real-time, these models surpass traditional direc t systems that \ndepend solely on fixed rules and databases. \nTransfer systems, on the other hand, use a more abstract translation process. \nThey convert the source language input into a generalized structure, removing \nspecific grammatical features of the source language before transforming this \nabstract structure into a corresponding one in the target language. The level of \nabstraction can vary, with greater abstraction making it easier to create a compatible \ntransfer module. LLMs excel at this form of translation due to their ability to create \nand manipulate abstract representations of language [10, p. 13]. By analyzing and \ngeneralizing input into intermediate forms, these models can more accurately \ntransform text into the target language structure, preserving meaning and context \n \nVěda a perspektivy № 11(42)2024 \nISSN 2695-1584 (Print) \nISSN 2695-1592 (Online) \n \n \n \n   425 \nmore effectively than traditional transfer systems. This capability allows for nuanced \nand contextually aware translations that capture the original text's structure and \nintent. \nAnother linguistic notion that is widely used in the stru cture of LLM is the \nnotion of corpus [4, p. 15]. These corpora serve as the foundation for building \nlanguage models used in AI systems, including chatbots, in our case –  ChatGPT. \nCorpus linguistics and AI -driven chatbots intersect at the level of language data \nprocessing and interaction design. This convergence involves using large linguistic \ncorpora to train and refine chatbots' language models, enabling them to understand, \ngenerate, and respond to human language more naturally and effectively [9, p. 120].  \nLexical aspects of machine translation represented by ChatGPT comprise \nvarious notions, such as idiomatic expression, neologisms, register and formality, \ninternationalisms and pseudo internationalisms.  \nOne of the major concerns we faced during examinati on of these features is \nthe accuracy of the definitions and examples generated by ChatGPT. While the \nmodel can produce coherent text, it sometimes generates content that is factually \nincorrect or lacks the nuance required for precise dictionary entries. This \nphenomenon, often referred to as hallucination, where the model invents \ninformation, poses a significant challenge for lexicographers [12, p. 12].  \nBeside the mention fact, ChatGPT appeared to be not yet consistent in \ntranslating from English to Ukraini an language. English is an analytical language \nthat relies heavily on word order and auxiliary words to convey meaning, while \nUkrainian is a synthetic language, characterized by its extensive use of inflections \nand a flexible word order to express grammati cal relationships. This fundamental \ndifference poses challenges for language models, which may struggle to capture the \nfull complexity of Ukrainian morphology, leading to inaccuracies or awkward \ntranslations [3, p. 7].  \nChatGPT can translate text between m any languages with a good level of \naccuracy. For example, when translating idioms that are frequently used in both \nlanguages, there is a very low probability of him to make some mistakes [14, p. 340]. \nThis simple sentence demonstrates the model's ability to handle basic multilingual \ntranslation tasks effectively:  \n \nEnglish idiom  ChatGPT’s translation  \n“Bear in mind  the significance of cultural \ncontext when interpreting historical texts, as it \ncan profoundly influence the intended \nmeaning and reception of specific terms and \nexpressions.” [15, p. 130] \n“Майте на увазі важливість культурного \nконтексту під час інтерпретації \nісторичних текстів, оскільки він може \nсуттєво вплива\nти на задумане значення та \nсприйняття окремих термінів і виразів.” \nTable 1. Demonstrating ChatGPT’s ability to convey lexico-semantic meaning  \n \nVěda a perspektivy № 11(42) 2024 \nISSN 2695-1584 (Print) \nISSN 2695-1592 (Online) \n \n \n \n   426 \nHowever, the quality of translation can be inconsistent when it comes to \ncomplex or idiomatic expressions. The example below demonsrates that translating \nthe English idiom literally into Ukrainian makes no sense, while the correct \ntranslation, provided by a human translator, “ заповнити прогалину ”, accurately \nconveys the meaning of the given linguistic notion. [14, p. 200].  \n \nEnglish idiom  ChatGPT’s translation \n“Then, in April of that year, George P. Smoot \nand his colleagues at Berkeley released \nevidence that might fill this gap in the theory.” \n[13, p. 56] \n“Потім, у квітні того року, Джордж П. \nСмут і його колеги з Берклі оприлюднили \nдокази, які могли б заповнити цю яму  в \nтеорії.” \nTable 2. ChatGPT’s inability to translate more complex idioms  \n \nThis example shows that ChatGPT can struggle with idioms and phrases that \nrequire a deeper understanding of cultural context and language -specific nuances \nbeyond simple word-for-word translation. \nIn addition to the mentioned issues, LLMs like ChatGPT are trained on large \ndatasets sourced from the internet, which may contain biases. Such a training process \nresults in the fact that the model's output can reflect these biases, leading to \ndictionary entries that may inadvertently perpetuate stereotypes or underrepresent \ncertain groups. This is particularly problematic in a lexicographic context, where the \ngoal is to provide balanced and accurate language resources.   \nFor example, when generating dictionary entries for terms associated with \ngender, race, or socioeconomic status, ChatGPT might reflect societal stereotypes. \nThis is problematic because dictionary entries are expected to be neutral and objective. \nThe model might also underrepresent minority dialects or non- standard forms of \nEnglish, as it is predominantly trained on mainstream, standardized language data \n[6, p. 78]. This limitation can lead to dictionary entries that fail to fully capture the \ndiversity and richness of language as it is used across different communities. \nAlthough machine translation has become a very powerful tool in the area of \nApplied Linguistics, syntactic aspects of every language remain a challenging task \nfor automated translation systems. Syntax, which refers to the arrangement of words \nand phrases in a sentence, is a very ambiguous notion for different language groups. \nIn this subchapter, we will examine basic rules applied to generating well-structured \ncorpora and highlight the common issues which occur in the field of machine \ntranslation by large language models, in our case – ChatGPT.  \nAs to any other lexico-semantic features analyzed in our work, these comprise \ncontext preservations in specialized terminology, cultural and contextual adaptation, \npolysemy, metaphors and jargon. Traditional machine translation systems, like rule-\nbased or statistical ones, used specific programs to ensure correct grammatical \nstructure during translation. While this helped maintain accuracy, it required a lot of \nmanual effort and language-specific resources [5, p. 25]. Neural machine translation \nsystems work differently—they learn from large amounts of text data and remember \nhow words relate to each other, without explicitly learning syntax rules. \n \nVěda a perspektivy № 11(42)2024 \nISSN 2695-1584 (Print) \nISSN 2695-1592 (Online) \n \n \n \n   427 \nThis approach allows neural models to handle complex language patterns, but \nthey sometimes make mistakes if they haven’t seen enough examples to understand \nall the syntactic nuances [7, p. 98]. While these models are good at creating fluent \ntext, they can struggle with complex syntax, especially when translating between \nlanguages with very different grammatical rules. Improving their ability to better \nunderstand syntax can help make translations more accurate and natural. \nIn this case, vector -based semantics appears to  be an essential tool. These \nmodels represent linguistic elements –  such as words, phrases, and sentences –  as \ncontinuous vectors in a high- dimensional space. The distances and relationships \nbetween these vectors capture semantic similarities and distinctions, allowing the \nmodels to comprehend context and meaning beyond the literal forms of words. \nOne of the key applications of such vector spaces is word embeddings, which \nconvey the semantic meaning of words or phrases during translation from the source \nlanguage to the target language.  \nWord embeddings are essentially mathematical representations of words as \ndense vectors. They have played a pivotal role in modern natural language \nprocessing and translation systems. By encoding words as vectors, embeddings help \ncapture both syntactic and semantic relationships, facilitating the model’s ability to \nunderstand and generate contextually accurate translations. This approach enables \nnuanced and context -aware interpretations, enhancing the overall quality and \nfluidity of translated content. \nFor instance, in the vector space generated by Word2Vec, words with similar \nmeanings, like \"king\"  and \"queen,\" are placed close to each other. Relationships \nbetween words, such as \"king - man + woman = queen,\" can be represented as linear \ntransformations within this space [11, p. 3- 5]. However, these embeddings lack \ncontext sensitivity, meaning a word like \"bank\" would have the same representation \nwhether it refers to a riverbank or a financial institution, which limits the model's  \nability to distinguish between different meanings of the same word based on context. \n \nPic. 3 Word2Vec example usage  \n\n \nVěda a perspektivy № 11(42) 2024 \nISSN 2695-1584 (Print) \nISSN 2695-1592 (Online) \n \n \n \n   428 \nLarge language models, like ChatGPT with its transformer-based architecture, \nare designed to handle translation between multiple languages. However, this can \nsometimes lead to challenges due to the limited availability of data for certain \nlanguages. To address these issues, such models use innovative techniques like up -\nsampling and back- translation to enhance translation quality. These methods help \nthe model better learn from limited data, allowing it to infer meaning and produce \nsemantically coherent translations even in less-resourced languages. Consequently, \nthese techniques contribute to more accurate and contextually meaningful \ntranslations across a diverse range of languages.  \nTraditional machine translation methods often relied on explicit rules and \nsyntax models, such as using syntactic parsers to ensure proper grammar. However, \nmodern large language models like ChatGPT learn syntax indirectly by training on \nhuge amounts of text data [8, p. 7- 9]. This allows them to produce natural \ntranslations, but sometimes they can make mistakes with word order, misplaced \nwords, or grammar when translating complex sentences. \nChallenges arise when translating between languages with different \nstructures. For example, languages like Japanese and Korean, which use a subject -\nobject-verb (structure, are harder for models trained mostly on English's subject -\nverb-object structure. Morphologically complex languages, like Finnish or Turkish, \nalso pose difficulties because words change based on their role in a sentence. \nAmbiguous syntax, where sentences can have more than one meaning, adds another \nlayer of complexity [8, p. 90]. \nImproving LLMs for translation often involves multilingual fine -tuning. By \ntraining on a variety of texts with complex sentence structures in different languages, \nmodels become better at handling syntax. Cross-lingual learning is another strategy \nwhere knowledge from high- resource language pairs, like English- French, helps \nimprove translations for less-resourced languages like Ukrainian. \nHybrid approaches  that combine LLMs with explicit syntactic models can \nfurther enhance accuracy. By using syntactic rules alongside machine learning, these \nmodels ensure better grammar and word order. Post-translation corrections also help \nby reviewing and refining translations to match grammatical standards. In the future, \nbetter training data and improved correction systems will make machine translations \neven more accurate and human-like. \nOne of the possible solutions to the mentioned problems in lexis, semantic and \nsyntax is represented by the notion of self- attention mechanisms. It allows models \nto focus on different parts of an input sentence to understand the relationships \nbetween words, regardless of their position. This capability is especially useful for \ncapturing syntactic and contextual nuances across languages. \nIn translation tasks, self-attention helps models weigh the importance of each \nword relative to others in the sentence, ensuring that key dependencies, such as word \norder and grammatical structures, are pre served. When combined with self-\n \nVěda a perspektivy № 11(42)2024 \nISSN 2695-1584 (Print) \nISSN 2695-1592 (Online) \n \n \n \n   429 \ncorrection mechanisms, self-attention can further refine translations by identifying \nareas where context, tense, or meaning may have been misinterpreted [7, p. 35]. This \nfocused re -evaluation helps the model better handle complex syntactic \ndependencies, idiomatic expressions, and variations in word meaning, ultimately \nleading to more accurate and contextually appropriate translations. \nAnother effective approach involves using self- correction mechanisms to \nimprove translation quality. In this method, the LLM first generates an initial \ntranslation, which is then refined through multiple iterations based on syntactic \nfeedback. The process can include explicit error detection using metrics like \nTranslation Edit Rate to spot and correct syntactic issues in the output. Essentially, \nthis self-refinement framework allows the model to make adjustments based on any \nsyntactic discrepancies identified during the initial pass [12, p. 45]. This approach is \nespecially useful for languages wit h complex syntactic rules, as it helps correct \nissues with word order or tense usage. By refining translations of sentences with \ncomplex structures, such as nested clauses or subordinate phrases, self- correction \nmechanisms can significantly enhance overall translation quality. \nMoreover, cross-lingual few-shot learning is another valuable approach for \nimproving syntactic translation in large language models. It leverages syntactic \nexamples from one language pair to enhance translation performance for another  \npair, focusing on shared linguistic similarities. For instance, an LLM might use \nexamples from a high- resource language pair, such as English- French, to improve \ntranslations for a lower -resource pair like English- Ukrainian. This method helps \nalign syntactic structures in languages where data availability is limited. \nWhat makes cross-lingual few-shot learning so effective is its ability to help \nLLMs model syntactic structures in underrepresented languages by drawing on \nsimilarities in grammatical rules acro ss languages. By doing so, it reduces reliance \non large amounts of parallel data, as the model generalizes syntactic knowledge from \nwidely-studied languages to less common ones. This allows for improved accuracy \nand contextual understanding even in low-resource translation scenarios. \nAll these features of English and Ukrainian languages contributed to the \ndiversity of issues a model is facing when trying to convey the meaning of a \nsentence, paragraph or even the entire article from source into the target language.  \nConclusions: Large language models like ChatGPT have proven highly \nvaluable for modern linguistics and machine translation due to their extensive \ntraining on diverse data. In our study, we evaluated ChatGPT 4.0's ability to translate \nscientific te xts between English and Ukrainian. We focused on its handling of \nlexico-syntactic, and semantic features. While ChatGPT is effective as a support tool \nfor human translators, providing functions like error-checking and context searches, \nit still has limitations with complex terms and context-specific nuances. \nOne significant challenge is the limited availability of Ukrainian -language \ncorpora, which affects translation accuracy. Ukrainian is less represented in \n \nVěda a perspektivy № 11(42) 2024 \nISSN 2695-1584 (Print) \nISSN 2695-1592 (Online) \n \n \n \n   430 \ninternational scientific literature compared to English, which often serves as the \nprimary language for academic work. We analyzed 70 scientific passages across \nfields such as chemistry, artificial intelligence, and biology to assess the model's \nperformance. We found that it struggles with precise termi nology and domain-\nspecific language, highlighting the need for richer Ukrainian resources. \nLexical challenges were common, with ChatGPT often misinterpreting \nidiomatic expressions, new words, and technical terms. This led to inconsistent or \ninaccurate translations. The model’s performance could be improved by training it \non more varied data, including regional, colloquial, and specialized language used \nin Ukrainian. This would help it better understand and translate terms with more \ncontextual accuracy. \nOn the semantic level, the differences between English and Ukrainian pose \nunique challenges. English often relies on word choice and sentence structure for \nnuanced meanings, while Ukrainian, with its inflectional system, conveys meaning \nthrough changes in word endings and forms. ChatGPT sometimes struggled to \ncapture these nuances, particularly when translating complex terms, metaphors, and \ncontext-dependent language. Expanding the model’s training on scientific texts and \nrefining its handling of implicit meani ngs could enhance its semantic translation \ncapabilities. \nSyntactically, ChatGPT had difficulty managing differences in word order, \nsubject-predicate agreement, and sentence flexibility between English and \nUkrainian. English uses a fixed word order, while U krainian’s case system allows \nfor varied word positions to emphasize meaning. To improve translations, the model \nneeds to better adapt word order and respect complex grammatical agreements in \nUkrainian.  \nIn our work, we also provide potential solutions to the mentioned and \nanalyzed problems. For example, increasing the volume and diversity of Ukrainian-\nlanguage texts used for training, especially those taken from the field of science, \nwould provide a more robust foundation for translation. This can be achieved by the \navailability of larger datasets that cover scientific texts, technical documents, \ncolloquial expressions, and regional variations. Collaborative efforts among \nresearchers, linguistic experts, and institutions can accelerate the creation and \navailability of such resources, leading to better contextual understanding and \nnuanced translations. \nMoreover, an effective strategy for improving translation accuracy is the \ncontinuous refinement of LLMs through feedback and iterative learning cycles. \nCollaborations with human translators, linguists, and domain experts can help \nidentify translation errors, suggest corrections, and feed this feedback into the \nmodel’s training pipeline. The iterative process allows for dynamic adaptation, \nensuring that the model evolves in response to evolving language patterns and user \nneeds. \n \nVěda a perspektivy № 11(42)2024 \nISSN 2695-1584 (Print) \nISSN 2695-1592 (Online) \n \n \n \n   431 \nReferences:  \n1. A Deep Decomposable Model for Disentangling Syntax and Semantics in Sentence \nRepresentation / D. Li et al. Findings of the Association for Computational Linguistics: EMNLP 2021, \nPunta Cana, Dominican Republic. Stroudsburg, PA, USA, 2021. URL:  https://doi.org/10.18653/ \nv1/2021.findings-emnlp.364  \n2. Begus G., Dabkowski  M., Rhodes  R. Large linguistic models: analyzing theoretical \nlinguistic ablities of LLMs.  Rutgers University Manuscript . 2023. August 21. P. 1–28.  \nURL: https://arxiv.org/abs/2305.00948  \n3. ChatGPT by OpenAI 4.0. URL: https://chatgpt.com  \n4. Curry N., Baker P., Brookes G. Generative AI for corpus approaches to discourse studies: A \ncritical evaluation of ChatGPT.  Applied Corpus Linguistics . 2023. P. 100082. URL:  https://doi. \norg/10.1016/j.acorp.2023.100082  \n5. Estimating and Comparing Translation Skills: A Comparative Study of ChatGPT and \nHuman Translation. Journal of Development and Social Sciences. 2024. Vol. 5, no. III. \nURL: https://doi.org/10.47205/jdss.2024(5-iii)08  \n6. Gubelmann R. A Loosely Wittgensteinian Conception of the Linguistic Understanding \nof Large Language M odels like BERT, GPT -3, and ChatGPT. Brill. URL:  https://brill.com/ \nview/journals/gps/99/4/article-p485_2.xml  \n7. Jiang L. P. Unifying Syntax and Semantics in Cognitive Concept Generation for Natural \nLanguage Expression. International Journal of Humanities, Social Sciences and Education. 2023. \nVol. 10, no. 10. P. 10–16. URL: https://doi.org/10.20431/2349-0381.1010002  \n8. Karaban, V. (2004). Pereklad anhliiskoi naukovoi i tekhnichnoi literatury [Translation \nof English scientific and technical literature]. Vinnytsia: Nova Knyha [in Ukrainian]. \n9. Kiselev D. An AI using Construction Grammar: Automatic Acquisition o f Knowledge \nabout Words. 12th International Conference on Agents and Artificial Intelligence, Valletta, Malta, \n22–24 February 2020. 2020. URL: https://doi.org/10.5220/0008865902890296  \n10. Koutsoudas A., Humecky A. Linguistics and Machine Translation. WORD . 1959. \nVol. 15, no. 3. P. 489–491. URL: https://doi.org/10.1080/00437956.1959.11659712  \n11. New England Journal of Medicine. URL: https://www.nejm.org/ \n12. Patil A. Top 5 Pre -trained Word Embeddings. Medium. URL:  https://patil-aakanksha. \nmedium.com/top-5-pre-trained-word-embeddings-20de114bc26  \n13. Taylor J. R. Semantic structure in Cognitive Grammar.  Cognitive Grammar . 2002. \nP. 96–120. URL: https://doi.org/10.1093/oso/9780198700333.003.0006  \n14. The Lancet. URL: https://www.thelancet.com/. \n15. Trujillo A. Computational L inguistics Techniques. Translation Engines: Techniques \nfor Machine Translation. London, 1999. P. 85–119. URL: https://doi.org/10.1007/978-1-4471-0587-9_5  \n16. Wiley Online Library. URL: https://onlinelibrary.wiley.com/. \n \n \n  "
}