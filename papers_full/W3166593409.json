{
    "title": "Improving Biomedical Pretrained Language Models with Knowledge",
    "url": "https://openalex.org/W3166593409",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2032612703",
            "name": "Zheng Yuan",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2106334948",
            "name": "Yijia Liu",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2475478252",
            "name": "Chuanqi Tan",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2159562265",
            "name": "Songfang Huang",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A1936961387",
            "name": "Fei Huang",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2953356739",
        "https://openalex.org/W2970986510",
        "https://openalex.org/W2127795553",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W2983102021",
        "https://openalex.org/W3100452049",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W3104163040",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2170189740",
        "https://openalex.org/W2977393556",
        "https://openalex.org/W90362830",
        "https://openalex.org/W3021352513",
        "https://openalex.org/W3151929433",
        "https://openalex.org/W3210120707",
        "https://openalex.org/W2047782770",
        "https://openalex.org/W3096403953",
        "https://openalex.org/W2998385486",
        "https://openalex.org/W2964242047",
        "https://openalex.org/W2949894546",
        "https://openalex.org/W3102844651",
        "https://openalex.org/W3093553144",
        "https://openalex.org/W2154142897",
        "https://openalex.org/W4295288972",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W2136437513",
        "https://openalex.org/W2169099542",
        "https://openalex.org/W2159583324",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963716420",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2807873602",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W3046375318",
        "https://openalex.org/W3172427031",
        "https://openalex.org/W2346452181",
        "https://openalex.org/W2971258845",
        "https://openalex.org/W3104578551",
        "https://openalex.org/W1483357098",
        "https://openalex.org/W2103017472",
        "https://openalex.org/W3105892552",
        "https://openalex.org/W3106224367",
        "https://openalex.org/W2525778437"
    ],
    "abstract": "Pretrained language models have shown success in many natural language processing tasks. Many works explore to incorporate the knowledge into the language models. In the biomedical domain, experts have taken decades of effort on building large-scale knowledge bases. For example, UMLS contains millions of entities with their synonyms and defines hundreds of relations among entities. Leveraging this knowledge can benefit a variety of downstream tasks such as named entity recognition and relation extraction. To this end, we propose KeBioLM, a biomedical pretrained language model that explicitly leverages knowledge from the UMLS knowledge bases. Specifically, we extract entities from PubMed abstracts and link them to UMLS. We then train a knowledge-aware language model that firstly applies a text-only encoding layer to learn entity representation and then applies a text-entity fusion encoding to aggregate entity representation. In addition, we add two training objectives as entity detection and entity linking. Experiments on the named entity recognition and relation extraction tasks from the BLURB benchmark demonstrate the effectiveness of our approach. Further analysis on a collected probing dataset shows that our model has better ability to model medical knowledge.",
    "full_text": "Proceedings of the BioNLP 2021 workshop, pages 180‚Äì190\nJune 11, 2021. ¬©2021 Association for Computational Linguistics\n180\nImproving Biomedical Pretrained Language Models with Knowledge\nZheng Yuan1‚àó Yijia Liu2 Chuanqi Tan2‚Ä† Songfang Huang2 Fei Huang2\n1Tsinghua University 2Alibaba Group\nyuanz17@mails.tsinghua.edu.cn\n{yanshan.lyj,chuanqi.tcq,songfang.hsf,f.huang}@alibaba-inc.com\nAbstract\nPretrained language models have shown suc-\ncess in many natural language processing\ntasks. Many works explore incorporat-\ning knowledge into language models. In\nthe biomedical domain, experts have taken\ndecades of effort on building large-scale\nknowledge bases. For example, the UniÔ¨Åed\nMedical Language System (UMLS) contains\nmillions of entities with their synonyms and\ndeÔ¨Ånes hundreds of relations among entities.\nLeveraging this knowledge can beneÔ¨Åt a va-\nriety of downstream tasks such as named en-\ntity recognition and relation extraction. To this\nend, we propose KeBioLM, a biomedical pre-\ntrained language model that explicitly lever-\nages knowledge from the UMLS knowledge\nbases. SpeciÔ¨Åcally, we extract entities from\nPubMed abstracts and link them to UMLS. We\nthen train a knowledge-aware language model\nthat Ô¨Årstly applies a text-only encoding layer\nto learn entity representation and applies a\ntext-entity fusion encoding to aggregate entity\nrepresentation. Besides, we add two training\nobjectives as entity detection and entity link-\ning. Experiments on the named entity recogni-\ntion and relation extraction from the BLURB\nbenchmark demonstrate the effectiveness of\nour approach. Further analysis on a collected\nprobing dataset shows that our model has bet-\nter ability to model medical knowledge.\n1 Introduction\nLarge-scale pretrained language models (PLMs)\nare proved to be effective in many natural language\nprocessing (NLP) tasks (Peters et al., 2018; Devlin\net al., 2019). However, there are still many works\nthat explore multiple strategies to improve the\nPLMs. Firstly, in specialized domains (i.e biomedi-\ncal domain), many works demonstrate that using in-\ndomain text (i.e. PubMed and MIMIC for biomedi-\ncal domain) can further improve downstream tasks\n‚àó Work done at Alibaba DAMO Academy.\n‚Ä† Corresponding author.\n‚Ä¶ treated with glycerinshow reduced inflammationafter 2 hours.C0011603 dermatitisC00178611,2,3-PropanetriolR176722500(C0017861, may_prevent, C0011603)\nFigure 1: An example of the biomedical sentence.\nTwo entities ‚Äúglycerin‚Äù and ‚ÄúinÔ¨Çammation‚Äù are linked\nto C0017861 (1,2,3-Propanetriol) and C0011603 (der-\nmatitis) respectively with a relation triplet ( C0017861,\nmay_prevent, C0011603) in UMLS.\nover general-domain PLMs (Lee et al., 2020; Peng\net al., 2019; Gu et al., 2020; Shin et al., 2020; Lewis\net al., 2020; Beltagy et al., 2019; Alsentzer et al.,\n2019). Secondly, unlike training language models\n(LMs) with unlabeled text, many works explore\ntraining the model with structural knowledge (i.e.\ntriplets and facts) for better language understand-\ning (Zhang et al., 2019; Peters et al., 2019; F√©vry\net al., 2020; Wang et al., 2019). In this work, we\npropose to combine the above two strategies for a\nbetter Knowledge enhanced Biomedical pretrained\nLanguage Model (KeBioLM).\nAs an applied discipline that needs a lot of facts\nand evidence, the biomedical and clinical Ô¨Åelds\nhave accumulated data and knowledge from a very\nearly age (Ashburner et al., 2000; Stearns et al.,\n2001). One of the most representative work is Uni-\nÔ¨Åed Medical Language System (UMLS) (Boden-\nreider, 2004) that contains more than 4M entities\nwith their synonyms and deÔ¨Ånes over 900 kinds of\nrelations. Figure 1 shows an example. There are\ntwo entities ‚Äúglycerin‚Äù and ‚ÄúinÔ¨Çammation‚Äù that\ncan be linked to C0017861 (1,2,3-Propanetriol)\nand C0011603 (dermatitis) respectively with a\nmay_prevent relation in UMLS. As the most impor-\ntant facts in biomedical text, entities and relations\ncan provide information for better text understand-\ning (Xu et al., 2018; Yuan et al., 2020).\nTo this end, we propose to improve biomedical\nPLMs with explicit knowledge modeling. Firstly,\n181\nwe process the PubMed text to link entities to the\nknowledge base. We apply an entity recognition\nand linking tool ScispaCy (Neumann et al., 2019)\nto annotate 660M entities in 3.5M documents. Sec-\nondly, we implement a knowledge enhanced lan-\nguage model based on F√©vry et al. (2020), which\nperforms a text-only encoding and a text-entity fu-\nsion encoding. Text-only encoding is responsible\nfor bridging text and entities. Text-entity fusion\nencoding fuses information from tokens and knowl-\nedge from entities. Finally, two objectives as entity\nextraction and linking are added to learn better en-\ntity representations. To be noticed, we initialize\nthe entity embeddings with TransE (Bordes et al.,\n2013), which leverages not only entity but also\nrelation information of the knowledge graph.\nWe conduct experiments on the named entity\nrecognition (NER) and relation extraction (RE)\ntasks in the BLURB benchmark dataset. Results\nshow that our KeBioLM outperforms the previous\nwork with average scores of 87.1 and 81.2 on 5\nNER datasets and 3 RE datasets respectively. Fur-\nthermore, our KeBioLM also achieves better per-\nformance in a probing task that requires models to\nÔ¨Åll the masked entity in UMLS triplets.\nWe summary our contributions as follows1:\n‚Ä¢ We propose KeBioLM, a biomedical pre-\ntrained language model that explicitly incor-\nporates knowledge from UMLS.\n‚Ä¢ We conduct experiments on 5 NER datasets\nand 3 RE datasets. Results demonstrate that\nour KeBioLM achieves the best performance\non both NER and RE tasks.\n‚Ä¢ We collect a cloze-style probing dataset from\nUMLS relation triplets. The probing results\nshow that our KeBioLM absorbs more knowl-\nedge than other biomedical PLMs.\n2 Related Work\n2.1 Biomedical PLMs\nModels like ELMo (Peters et al., 2018) and BERT\n(Devlin et al., 2019) show the effectiveness of the\nparadigm of Ô¨Årst pre-training an LM on the unla-\nbeled text then Ô¨Åne-tuning the model on the down-\nstream NLP tasks. However, direct application of\nthe LMs pre-trained on the encyclopedia and web\n1Our codes and model can be found at https://\ngithub.com/GanjinZero/KeBioLM.\ntext usually fails on the biomedical domain, be-\ncause of the distinctive terminologies and idioms.\nThe gap between general and biomedical do-\nmains inspires the researchers to propose LMs\nspecially tailored for the biomedical domain.\nBioBERT (Lee et al., 2020) is the most widely\nused biomedical PLM which is trained on PubMed\nabstracts and PMC articles. It outperforms vanilla\nBERT in named entity recognition, relation extrac-\ntion, and question answering tasks. Jin et al. (2019)\ntrain BioELMo with PubMed abstracts, and Ô¨Ånd\nfeatures extracted by BioELMo contain entity-type\nand relational information. Different training cor-\npora have been used for enhancing performance of\nsub-domain tasks. ClinicalBERT (Alsentzer et al.,\n2019), BlueBERT (Peng et al., 2019) and bio-lm\n(Lewis et al., 2020) utilize clinical notes MIMIC to\nimprove clinical-related downstream tasks. SciB-\nERT (Beltagy et al., 2019) uses papers from the\nbiomedical and computer science domain as train-\ning corpora with a new vocabulary. KeBioLM is\ntrained on PubMed abstracts to adapt to PubMed-\nrelated downstream tasks.\nTo understand the factors in pretraining biomed-\nical LMs, Gu et al. (2020) study pretraining tech-\nniques systematically and propose PubMedBERT\npretrained from scratch with an in-domain vocab-\nulary. Lewis et al. (2020) also Ô¨Ånd using an in-\ndomain vocabulary enhances the downstream per-\nformances. This inspires us to utilize the in-domain\nvocabulary for KeBioLM.\n2.2 Knowledge-enhanced LMs\nLMs like ELMo and BERT are trained to predict\ncorrelation between tokens, ignoring the meanings\nbehind them. To capture both the textual and con-\nceptual information, several knowledge-enhanced\nPLMs are proposed.\nEntities are used for bridging tokens and knowl-\nedge graphs. Zhang et al. (2019) align tokens\nand entities within sentences, and aggregate to-\nken and entity representations via two multi-head\nself-attentions. KnowBert (Peters et al., 2019) and\nEntity as Experts (EAE) (F√©vry et al., 2020) use\nthe entity linker to perform entity disambiguation\nfor candidate entity spans and enhance token rep-\nresentations using entity embeddings. Inspired by\nentity-enhanced PLMs, we follow the model of\nEAE to inject biomedical knowledge into KeBi-\noLM by performing entity detection and linking.\nRelation triplets provide intrinsic knowledge be-\n182\ntween entity pairs. KEPLER (Wang et al., 2019)\nlearns the knowledge embeddings through rela-\ntion triplets while pretraining. K-BERT (Liu et al.,\n2020) converts input sentences into sentence trees\nby relation triplets to infuse knowledge.\nIn the biomedical domain, He et al. (2020) in-\nject disease knowledge to existing PLMs by pre-\ndicting diseases names and aspects on Wikipedia\npassages. Michalopoulos et al. (2020) use UMLS\nsynonyms to supervise masked language modeling.\nWe propose KeBioLM to infuse various kinds of\nbiomedical knowledge from UMLS including but\nnot limited to diseases.\n3 Approach\nIn this paper, we assume to access an entity set\nE= {e1, ..., et}. For a sentence x = {x1, ..., xn},\nwe assume some spans m = (xi, ..., xj) can be\ngrounded to one or more entities in E. We further\nassume the disjuncture of these spans. In this paper,\nwe use UMLS to set the entity set.\n3.1 Model Architecture\nTo explicitly model both the textual and conceptual\ninformation, we follow F√©vry et al. (2020) and use\na multi-layer self-attention network to encode both\nthe text and entities. The model can be viewed as\nbuilding the links between text and entities in the\nlower layers and fusing the text and entity represen-\ntation in the upper layers. The overall architecture\nis shown in Figure 2. To be more speciÔ¨Åc, we set\nthe PubMedBERT (Gu et al., 2020) as our back-\nbone. We split the layers of the backbone into\ntwo groups, performing a text-only encoding and\ntext-entity fusion encoding respectively.\nText-only encoding. For the Ô¨Årst group, which\nis closer to the input, we extract the Ô¨Ånal hidden\nstates and perform a token-wise classiÔ¨Åcation to\nidentify if the token is at the beginning, inside, or\noutside of a mention (i.e., the BIO scheme). The\nprobabilities of the B/I/O label {li}are written as:\nh1, ...,hn = Transformers0(x1, ..., xn) (1)\np(li |x) =softmax(Wlhi + bl) (2)\nAfter identifying the mention boundary, we main-\ntain a function M(i) ‚ÜíE‚à™{ NIL}, which returns\nthe entity of the i-th token belongs.2 We collect\nthe mentions with a sentence x. For a mention\nm = (s, t), where s and t represents the starting\n2NIL is returned when there is no entity being matched.\nand ending indexes of m, we encode it as the con-\ncatenation of hidden states of the boundary tokens\nhm = [hs; ht].\nFor an entity ej ‚ààE in the KG, we denote its en-\ntity embedding as ej. For a mention m, we search\nthe k nearest entities of its projected representa-\ntion h‚Ä≤\nm = Wmhm + bm in the entity embedding\nspace, obtaining a set of entities E‚Ä≤. The normal-\nized similarity between h‚Ä≤\nm and ej is calculated as\naj = exp(h‚Ä≤\nm ¬∑ej)‚àë\nek‚ààE‚Ä≤exp(h‚Ä≤m ¬∑ek) (3)\nThe additional entity representation e‚Ä≤\nm of m is\ncalculated as a weighted sum of the embeddings\ne‚Ä≤\nm = ‚àë\nej‚ààE‚Ä≤aj ¬∑ej.\nText-entity fusion encoding. After getting the\nmentions and entities, we fuse the entity embed-\ndings with the text embedding by summation. For\nthe i-th token, the entity-enhanced embedding is\ncalculated as:\nh‚àó\ni =\n{\nhi + (Wee‚Ä≤\nm + be) , ‚àÉm, M(i) =m,\nhi, otherwise.\n(4)\nM(i) =m represents the i-th token belong to en-\ntity em. The sequence of h‚àó\n1, ...,h‚àó\nn is then fed into\nthe second group of transformer layers to generate\ntext-entity representations. The Ô¨Ånal hidden states\nhf\ni are calculated as:\nhf\n1 , ...,hf\nn = Transformers1(h‚àó\n1, ...,h‚àó\nn) (5)\n3.2 Pretraining Tasks\nWe have three pretraining tasks for KeBioLM.\nMasked language modeling is a cloze-style task\nfor predicting masked tokens. Since the entities are\nthe main focus of our model, we add two tasks as\nentity detection and linking respectively following\nF√©vry et al. (2020). Finally, we jointly minimize\nthe following loss:\nL= LMLM + LED + LEL (6)\nMasked Language Modeling Like BERT and\nother LMs, we predict the masked tokens {xi}in\ninputs using the Ô¨Ånal hidden representations {hf\ni }.\nThe loss LMLM is calculated based on the cross-\nentropy of masked and predicted tokens:\npM (xi |x) = softmax(Wmhf\ni + bm) (7)\nLMLM =\n‚àë\n‚àílog pM (xi |x) (8)\n183\nTransformers\nTransformers\nùê° B I OB I IEntity Detection\nEntity Embeddings\nEntity Rep.\nC0018799heart disease\nC1532253sedentary lifestyle\nC0020740ibuprofen\nC0010200 cough\nC0010054coronary sclerosis\nC0018787coronary\nAdditional Entity Rep. Entity Linking\nInput Tokens\nMLM\n[MASK]  [MASK]      causes       coronary      artery       disease\nSedentarylifestyle\nC1532253C0010054\nùê°‚àó\nùê°\"\nText-only encoding\nText-entity fusion encoding\nFigure 2: The overall architecture of KeBioLM.\nWhole word masking is successful in training\nmasked language models (Devlin et al., 2019; Cui\net al., 2019). In the biomedical domain, entities are\nthe semantic units of texts. Therefore, we extend\nthis technique to whole entity masking. We mask\nall tokens within a word or entity span. KeBioLM\nreplaces 12% of tokens to [MASK] and 1.5% to-\nkens to random tokens. This is more difÔ¨Åcult for\nmodels to recover tokens, which leads to learning\nbetter entity representations.\nEntity Detection Entity detection is an impor-\ntant task in biomedical NLP to link the tokens to\nentities. Thus, We add an entity detection loss by\ncalculating the cross-entropy for BIO labels:\nLED =\nn‚àë\ni=1\n‚àílog p(li |x) (9)\nEntity Linking One medical entity in different\nnames linking to the same index permits the model\nto learn better text-entity representations. To link\nmention {m}in texts with entities {e}in entity\nset E, we calculate the cross-entropy loss using\nsimilarities between {h‚Ä≤\nm}and entities in E:\nLEL =\n‚àë\n‚àílog exp(h‚Ä≤\nm ¬∑e)‚àë\nej‚ààEexp(h‚Ä≤m ¬∑ej) (10)\n3.3 Data Creation\nGiven a sentence S from PubMed content, we need\nto recognize entities and link them to the UMLS\nknowledge base. We use ScispaCy (Neumann et al.,\n2019), a robust biomedical NER and entity linking\nmodel, to annotate the sentence. Unlike previous\nwork (Vashishth et al., 2020) that only retains rec-\nognized entities in a subset of Medical Subject\nHeadings (MeSH) (Lipscomb, 2000), we relax the\nrestriction to annotate all entities to UMLS 2020\nAA release 3 whose linking scores are higher than\na threshold of 0.85.\n4 Experiments\nIn this section, we Ô¨Årst introduce the pretraining de-\ntails of KeBioLM. Then we introduce the BLURB\ndatasets for evaluating our approach. Finally, we\nintroduce a probing dataset based on UMLS triplets\nfor evaluating knowledge modeling.\n4.1 Pretraining Details\nWe use ScispaCy to acquire 477K CUIs and 660M\nentities among 3.5M PubMed documents 4 from\nPubMedDS dataset (Vashishth et al., 2020) as train-\ning corpora.\nWe initialize entity embeddings by TransE (Bor-\ndes et al., 2013) which learns embeddings from re-\nlation triplets. Relation triplets come from UMLS\n3https://www.nlm.nih.gov/research/\numls/licensedcontent/umlsarchives04.\nhtml#2020AA\n4The count of documents in PubMedDS is based on\nhttps://arxiv.org/pdf/2005.00460v1.pdf.\n184\n#Train #Dev #Test #Ments #Ments\n(UMLS)\n#Ments\n(KeBioLM)\nBC5chem 5,203 5,347 5,385 15,935 10,373 8,993\nBC5dis 4,182 4,244 4,424 12,850 8,846 3,878\nNCBI 5,137 787 960 6,884 1,985 1,091\nBC2GM 15,197 3,061 6,325 24,583 2,808 2,423\nJNLPBA 46,750 4,551 8,662 59,963 6,099 5,233\nChemProt 18,035 11,268 15,745 39,022 13,106 10,772\nDDI 25,296 2,496 5,716 15,738 10,429 9,212\nGAD 4,261 535 534 - - -\nTable 1: The training instances (mentions for NER tasks and sentences with two entities for RE tasks) and the\nmention counts of NER and RE datasets preprocessed in BLURB benchmark respectively. The mention counts\noverlapping with UMLS 2020 AA release and KeBioLM are also listed. For the GAD dataset, annotated mentions\ndo not appear in the BLURB preprocessed version.\n2020 AA release. We train TransE with the L2-\nnorm distance function and set embedding dim to\n100. Adam (Kingma and Ba, 2014) is used as the\noptimizer with a learning rate of 1e-3, batch size\nof 2048, and train epoch of 30. Entity embeddings\nadd 45.5M parameters to KeBioLM.\nThe parameters of transformers in KeBioLM are\ninitialized from the checkpoint of PubMedBERT.\nWe also use the vocabulary from PubMedBERT.\nAdamW (Loshchilov and Hutter, 2017) is used\nas the optimizer for KeBioLM with 10,000 steps\nwarmup and linear decay. We use an 8-layer trans-\nformer for text-only encoding and a 4-layer trans-\nformer for text-entity fusion encoding. We set the\nlearning rate to 5e-5, batch size to 512, max se-\nquence length to 512, and training epochs to 2.\nFor each input sequence, we limit the max entities\ncount to 50 and the excessive entities will be trun-\ncated. To generate entity representation e‚Ä≤\nm, the\nmost k = 100similar entities are used. We train\nour model with 8 NVIDIA 16GB V100 GPUs.\n4.2 Datasets\nIn this section, we evaluate KeBioLM on NER\ntasks and RE tasks of the BLURB benchmark 5\n(Gu et al., 2020). For all tasks, we use the pre-\nprocessed version from BLURB. We measure the\nNER and RE datasets in terms of F1-score. Table 1\nshows the counts of training instances in BLURB\ndatasets (i.e., annotated mentions for NER datasets\nand sentences with two mentions for RE datasets).\nWe also report the count of annotated mentions\noverlapping with the UMLS 2020 release and Ke-\nBioLM in each dataset. The percentage of men-\n5https://microsoft.github.io/BLURB/\ntions overlapping with KeBioLM ranges from 8.7%\n(NCBI-disease) to 58.5% (DDI) which indicates\nthat KeBioLM learns entity knowledge related to\ndownstream tasks.\n4.2.1 Named Entity Recognition\nBC5-chem & BC5-disease (Li et al., 2016) con-\ntain 1500 PubMed abstracts for extracting chemical\nand disease entities respectively.\nNCBI-disease (DoÀògan et al., 2014) includes 793\nPubMed abstracts to detect disease entities.\nBC2GM (Smith et al., 2008) contains 20K\nPubMed sentences to extract gene entities.\nJNLPBA (Collier and Kim, 2004) includes\n2,000 PubMed abstracts to identify molecular\nbiology-related entities. We ignore entity types\nin JNLPBA following Gu et al. (2020).\n4.2.2 Relation Extraction\nChemProt (Krallinger et al., 2017) classiÔ¨Åes the\nrelation between chemicals and proteins within sen-\ntences from PubMed abstracts. Sentences are clas-\nsiÔ¨Åed into 6 classes including a negative class.\nDDI (Herrero-Zazo et al., 2013) is a RE dataset\nwith sentence-level drug-drug relation on PubMed\nabstracts. There are four classes for relation: ad-\nvice, effect, mechanism, and false.\nGAD (Bravo et al., 2015) is a gene-disease re-\nlation binary classiÔ¨Åcation dataset collected from\nPubMed sentences.\n4.3 Fine-tuning Details\nNER We follow Gu et al. (2020) to formulate\nNER tasks as sequential labeling tasks with the\n185\nBio-\nBERT\nSci-\nBERT\nClinical-\nBERT\nBlue-\nBERT\ndisease-\nBERT\nbio-\nlm‚Ä†\nPubMed-\nBERT\nKeBio-\nLM\nBC5chem 92.9 92.5 90.8 91.2 - 92.9 93.3 93.3¬±0.2\nBC5dis 84.7 84.5 83.0 83.7 86.5 83.8 85.6 86.1¬±0.3‚àó\nNCBI 89.1 88.1 88.3 88.0 87.1 87.7 87.8 89.1¬±0.3‚àó\nBC2GM 83.8 83.4 81.7 81.9 - 87.0 84.5 85.1¬±1.6\nJNLPBA 79.4 79.5 78.6 78.7 - 80.6 80.1 82.0¬±0.2‚àó\nNER 86.0 85.6 84.5 84.7 - 86.4 86.3 87.1¬±0.3‚àó\nChemProt 76.1 75.2 72.0 71.5 - 75.4 77.2 77.5¬±0.3‚àó\nDDI 80.9 81.1 78.2 77.8 - 81.0 82.4 81.9¬±0.8\nGAD 80.9 80.9 78.4 77.2 - 82.2 82.3 84.3¬±1.0‚àó\nRE 79.3 79.1 76.2 75.5 - 79.5 80.6 81.2¬±0.5‚àó\nTable 2: F1-scores on NER and RE tasks in BLURB benchmark. Standard deviations of KeBioLM are reported\nacross Ô¨Åve runs. Results of diseaseBERT-biobert and bio-lm come from their corresponded papers. Others are\ncopied from BLURB. * indicates that p ‚â§0.05 of one-sample t-test which compares whether the mean perfor-\nmance of KeBioLM is better than PubMedBERT. ‚Ä† Bio-lm applies different metrics with BLURB (micro F1 v.s.\nmacro F1). Thus, we just list its results but do not directly compare with them.\nBIO tagging scheme and ignore the entity types in\nNER datasets. We classify labels of tokens by a\nlinear layer on top of the hidden representations.\nRE We replace the entity mentions in RE\ndatasets with entity indicators like @DISEASE$ or\n@GENE$ to avoid models classifying relations by\nmemorizing entity names. We add these entity indi-\ncators into the vocabulary of LMs. We concatenate\nthe representation of two concerned entities and\nfeed it into a linear layer for relation classiÔ¨Åcation.\nParameters We adopt AdamW as the optimizer\nwith a 10% steps linear warmup and a linear de-\ncay. We search the hyperparameters of learning\nrate among 1e-5, 3e-5, and 5e-5. We Ô¨Åne-tune the\nmodel for 60 epochs. We evaluate the model at the\nend of each epoch and choose the best model ac-\ncording to the evaluation score on the development\nset. We set batch size as 16 when Ô¨Åne-tuning. The\nmaximal input lengths are 512 for all NER datasets.\nWe truncate ChemProt and DDI to 256 tokens, and\nGAD to 128 tokens. To perform a fair comparison,\nwe Ô¨Åne-tune our model with 5 different seeds and\nreport the average score.\n4.4 Results\nWe compare KeBioLM with following base-size\nbiomedical PLMs on the above-mentioned datasets:\nBioBERT (Lee et al., 2020), SciBERT (Beltagy\net al., 2019), ClinicalBERT (Alsentzer et al., 2019),\nBlueBERT (Peng et al., 2019), bio-lm (Lewis et al.,\n2020), diseaseBERT (He et al., 2020), and Pub-\nMedBERT (Gu et al., 2020) 6.\nTable 2 shows the main results on NER and\nRE datasets of the BLURB benchmark. In addi-\ntion, we report the average scores for NER and\nRE tasks respectively. KeBioLM achieves state-of-\nthe-art performance for NER and RE tasks. Com-\npared with the strong baseline BioBERT, KeBi-\noLM shows stable improvements in NER and RE\ndatasets (+1.1 in NER, +1.9 in RE). Compared\nwith our baseline model PubMedBERT, KeBioLM\nperforms signiÔ¨Åcantly better in BC5dis, NCBI,\nJNLPBA, ChemProt, and GAD ( p ‚â§0.05 based\non one-sample t-test) and achieves better average\nscores (+0.8 in NER, +0.6 in RE). DiseaseBERT is\na model carefully designed for predicting disease\nnames and aspects, which leads to better perfor-\nmance in the BC5dis dataset (+0.4). They only re-\nport the promising results in disease-related tasks,\nhowever, our model obtains consistent promising\nperformances across all kinds of biomedical tasks.\nIn the BC2GM dataset, KeBioLM outperforms our\nbaseline model PubMedBERT and other PLMs\nexcept for bio-lm, and the standard deviation of\nthe BC2GM task is evidently larger than other\ntasks. Another exception is the DDI dataset, we ob-\nserve a slight performance degradation compared\nto PubMedBERT (-0.5). The average performances\ndemonstrate that fusing entity knowledge into the\nLM boosts the performances across the board.\n6We use BioBERT v1.1, SciBERT-scivocab-uncased, Bio-\nClinicalBERT, BlueBERT-pubmed-mimic, bio-lm(RoBERTa-\nbase-PM-M3-V oc), diseaseBERT-biobert and PubMedBERT-\nabstract versions for comparison.\n186\nKeBio-\nLM -wem +rand +frz\nBC5chem 93.3 92.8 92.8 92.3\nBC5dis 86.1 85.9 85.5 85.5\nNCBI 89.1 88.4 88.8 88.3\nBC2GM 85.1 84.5 84.5 85.7\nJNLPBA 82.0 81.5 81.9 81.8\nNER 87.1 86.6 86.7 86.7\nChemProt 77.5 77.3 76.3 76.8\nDDI 81.9 80.6 81.4 80.7\nGAD 84.3 83.1 82.3 82.8\nRE 81.2 80.3 80.0 80.1\nTable 3: Ablation studies for KeBioLM architecture on\nthe BLURB benchmark. We use -wem, +rand and +frz\nto represent pretraining setting (a), (b) and (c), respec-\ntively.\n4.5 Ablation Test\nWe conduct ablation tests to validate the effective-\nness of each part in KeBioLM. We pretrain the\nmodel with the following settings and reuse the\nsame parameters described above: (a) Remove\nwhole entity masking and retain whole word mask-\ning while pretraining (-wem); (b) Initialize entity\nembeddings randomly (+rand); (c) Initialize en-\ntity embeddings by TransE and freeze the entity\nembeddings while pretraining (+frz).\nIn Table 3, we observe the following results.\nFirstly, comparing KeBioLM with setting (a) shows\nthat whole entity masking boosting the perfor-\nmances consistently in all datasets (+0.5 in NER,\n+0.9 in RE). Secondly, comparing KeBioLM with\nsetting (b) indicates initializing the entity embed-\ndings randomly degrades performances in NER\ntasks and RE tasks (-0.4 in NER, -1.2 in RE). En-\ntity embeddings initialized by TransE utilize rela-\ntion knowledge in UMLS and enhance the results.\nThirdly, freezing the entity embeddings in setting\n(c) reduces the performances on all datasets com-\npared to KeBioLM except BC2GM (-0.4 in NER,\n-1.1 in RE). This indicates that updating entity em-\nbedding while pretraining helps KeBioLM to have\nbetter text-entity representations, and this leads to\nbetter downstream performances.\nTo evaluate how the count of transformer layers\naffects our model, we pretrain KeBioLM with the\ndifferent number of layers. For the convenience of\nnotation, denote l0 is the layer count of text-only\nencoding and l1 is the layer count of text-entity fu-\nsion encoding. We have the following settings: (i)\nl0 = 8\nl1 = 4\nl0 = 4\nl1 = 8\nl0 = 12\nl1 = 0\nBC5chem 93.3 93.1 93.2\nBC5dis 86.1 85.7 86.0\nNCBI 89.1 88.5 88.4\nBC2GM 85.1 84.8 86.8\nJNLPBA 82.0 81.7 78.8\nNER 87.1 86.8 86.6\nChemProt 77.5 77.7 77.6\nDDI 81.9 81.0 80.1\nGAD 84.3 82.9 83.2\nRE 81.2 80.5 80.3\nTable 4: Ablation studies for transformer layers count\nin KeBioLM on the BLURB benchmark.\nl0 = 8, l1 = 4(our base model), (ii)l0 = 4, l1 = 8,\n(iii)l0 = 12, l1 = 0(without the second group of\ntransformer layers, {hi}are used for token repre-\nsentations). Results are shown in Table 4. Our base\nmodel (i) has better performance than setting (ii)\n(+0.3 in NER, +0.7 in RE). Training setting (iii) is\nequal to a traditional BERT model with additional\nentity extraction and entity linking tasks. The com-\nparison with (i) and (iii) indicates that text-entity\nrepresentations have better performances than text-\nonly representations (+0.5 in NER, +0.9 in RE) in\nthe same amount of parameters.\n4.6 UMLS Knowledge Probing\nWe establish a probing dataset based on UMLS\ntriplets to evaluate how LMs understand medical\nknowledge via pretraining.\n4.6.1 Probing Dataset\nUMLS triplets are stored in the form of (s, r, o)\nwhere s and o are CUIs in UMLS andr is a relation\ntype. We generate two queries for one triplet based\non names of CUIs and relation type:\n‚Ä¢ Q1: [CLS] s r[MASK] [SEP]\n‚Ä¢ Q2: [CLS] [MASK] r o[SEP]\nFor example, we sample a triplet and terms of\ncorresponded entities ( C0048038:apraclonidine,\nmay_prevent, C0028840:ocular hypertension). We\nremove the underscores of relation names and gen-\nerate two queries (we omit [CLS] and [SEP]):\n‚Ä¢ Q1: apraclonidine may prevent [MASK].\n‚Ä¢ Q2: [MASK] may prevent ocular hyperten-\nsion.\n187\n#Queries #Relations #Avg. CUIs\n143,771 922 2.39\nTable 5: The number of generated UMLS relation prob-\ning dataset.\nFor relation names end with ‚Äúof‚Äù, ‚Äúas‚Äù , and ‚Äúby‚Äù,\nwe add ‚Äúis‚Äù in front of relation names. For in-\nstance, translation_of is converted to is translation\nof, classiÔ¨Åed_as is converted to is classiÔ¨Åed as, and\nused_by is converted to is used by . Commonly,\ndifferent relation triplets can generate same query\nsince triplets may overlap(s, r,‚àí) or (‚àí, r, o) with\neach other. We deduplicate all repeat queries and\nrandomly choose at most 200 queries from all rela-\ntion types in UMLS. After deduplication, one query\ncan have multiple CUIs as answers. For example:\n‚Ä¢ Q: [MASK] may treat essential tremor.\n‚Ä¢ A1: C0282321: propranolol hydrochloride\n‚Ä¢ A2: C0033497: propranolol\nWe summarize our generated UMLS relation prob-\ning dataset in Table 5. Unlike LAMA (Petroni et al.,\n2019) and X-FACTR (Jiang et al., 2020) that con-\ntain less than 50 kinds of relation, our probing task\nis a more difÔ¨Åcult task requiring a model to decode\nentities over 900 kinds of relations.\n4.6.2 Multi [MASK] Decoding\nTo probe PLMs using generated queries, we re-\nquire models to recover the masked tokens. Since\nbiomedical entities are usually formed by multiple\nwords and each word can be tokenized into several\nwordpieces (Wu et al., 2016), models have to re-\ncover multiple [MASK] tokens. We limit the max\nlength of one entity is 10 for decoding.\nWe decode the multi [MASK] tokens using the\nconÔ¨Ådence-based method described in Jiang et al.\n(2020). We also implement a beam search for de-\ncoding. Unlike beam search in machine translation\nthat decodes tokens from left to right, we decode\ntokens in an arbitrary order. For each step, we cal-\nculate the probabilities of all undecoded masked\ntokens based on original input and decoded tokens.\nWe predict only one token within undecoded to-\nkens with the top B = 5accumulated log probabil-\nities. Decoding will be accomplished after count\nof [MASK] times iterations and we keep the best\nB = 5decoding results. We skip the reÔ¨Ånement\nstage since it is time-consuming and does not sig-\nniÔ¨Åcantly improve the results.\nType 1 Type 2 Overall\nSciBERT 13.92 1.01 2.75\nClinicalBERT 4.19 0.33 0.79\nBlueBERT 4.67 0.39 1.02\nKeBioLM 14.01 1.48 3.26\nTable 6: Results of the probing test in terms of Re-\ncall@5.\n4.6.3 Evaluation Metric\nSince multiple correct CUIs exist for one query,\nwe consider a model answering the query correctly\nif any decoded tokens in any [MASK] length hit\nany of the correct CUIs. We evaluate the probing\nresults by the relation-level macro-recall@5.\n4.6.4 Probing Results\nWe classify probing queries into two types based on\ntheir difÔ¨Åculties. Type 1: answers within queries\n(24,260 queries); Type 2: answers not in queries\n(119,511 queries). Here are examples of Type 1\n(Q1 and A1) and Type 2 (Q2 and A2) queries:\n‚Ä¢ Q1: [MASK] has form tacrolimus monohy-\ndrate.\n‚Ä¢ A1: C0085149: tacrolimus\n‚Ä¢ Q2: cosyntropin may diagnose [MASK].\n‚Ä¢ A2: C0001614: adrenal cortex disease\nTable 6 summarizes the probing results of differ-\nent PLMs according to query types. Checkpoints of\nBioBERT and PubMedBERT miss a cls/predictions\nlayer and cannot perform the probe directly. Com-\npared to other PLMs, KeBioLM achieves the best\nscores in both two types and obviously outperforms\nBlueBERT and ClincalBERT with a large margin,\nwhich indicates that KeBioLM learns more medical\nknowledge.\nTable 7 lists some probing examples. SciBERT\ncan decode medical entities for [MASK] tokens\nwhich may be unrelated. KeBioLM decodes re-\nlation correctly and is aware of the synonyms of\nhepatic. KeBioLM states that Vaccination may\nprevent tetanus which is a correct but not precise\nstatement.\n5 Conclusions\nIn this paper, we propose to improve biomedical\npretrained language models with knowledge. We\n188\nQuery & Answer CUI SciBERT KeBioLM\nomalizumab may treat [MASK] migraine asthma\nC0004096: asthma the disease severe allergic asthma\nphentolamine may diagnose [MASK] depression pheochromocytoma\nC0031511: phaeochromocytoma the serotonin syndrome renovascular hypertension\n[MASK] is noun form of hepatic it liver\nC0023884: liver the form of hepatic hepatic only\n[MASK] may prevent tetanus it vaccination\nC0305062: tetanus toxoid bcg vaccination prophylactic tetanus vaccination\nTable 7: Probing examples of UMLS relation triplets. Queries and answer CUIs are listed. We only list one\ncorrect CUI for each query. For each model, one [MASK] token decoding result and an example of multi [MASK]\ndecoding result are displayed. Bold text represents a term of the answer CUI.\npropose KeBioLM which applies text-only encod-\ning and text-entity fusion encoding and has two\nadditional entity-related pretraining tasks: entity\ndetection and entity linking. Extensive experiments\nhave shown that KeBioLM outperforms other\nPLMs on NER and RE datasets of the BLURB\nbenchmark. We further probe biomedical PLMs\nby querying UMLS relation triplets, which indi-\ncates KeBioLM absorbs more biomedical knowl-\nedge than others. In this work, we only leverage\nthe relation information in TransE to initialize the\nentity embeddings. We will further investigate how\nto directly incorporate the relation information into\nLMs in the future.\nAcknowledgements\nWe would like to thank the anonymous reviewers\nfor their helpful comments and suggestions. This\nwork is supported by Alibaba Group through Al-\nibaba Research Intern Program.\nReferences\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly available clini-\ncal BERT embeddings. In Proceedings of the 2nd\nClinical Natural Language Processing Workshop ,\npages 72‚Äì78, Minneapolis, Minnesota, USA. Asso-\nciation for Computational Linguistics.\nMichael Ashburner, Catherine A Ball, Judith A Blake,\nDavid Botstein, Heather Butler, J Michael Cherry,\nAllan P Davis, Kara Dolinski, Selina S Dwight,\nJanan T Eppig, et al. 2000. Gene ontology: tool\nfor the uniÔ¨Åcation of biology. Nature genetics ,\n25(1):25‚Äì29.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A pretrained language model for scientiÔ¨Åc text.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 3615‚Äì\n3620, Hong Kong, China. Association for Computa-\ntional Linguistics.\nOlivier Bodenreider. 2004. The uniÔ¨Åed medical lan-\nguage system (umls): integrating biomedical termi-\nnology. Nucleic acids research, 32(suppl_1):D267‚Äì\nD270.\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-\nDuran, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi-\nrelational data. In Neural Information Processing\nSystems (NIPS), pages 1‚Äì9.\n√Älex Bravo, Janet Pi√±ero, N√∫ria Queralt-Rosinach,\nMichael Rautschka, and Laura I Furlong. 2015. Ex-\ntraction of relations between genes and diseases\nfrom text and large-scale data analysis: implica-\ntions for translational research. BMC bioinformat-\nics, 16(1):1‚Äì17.\nNigel Collier and Jin-Dong Kim. 2004. Introduc-\ntion to the bio-entity recognition task at JNLPBA.\nIn Proceedings of the International Joint Workshop\non Natural Language Processing in Biomedicine\nand its Applications (NLPBA/BioNLP), pages 73‚Äì78,\nGeneva, Switzerland. COLING.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin,\nZiqing Yang, Shijin Wang, and Guoping Hu. 2019.\nPre-training with whole word masking for chinese\nbert. arXiv preprint arXiv:1906.08101.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171‚Äì4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nRezarta Islamaj Do Àògan, Robert Leaman, and Zhiyong\nLu. 2014. Ncbi disease corpus: a resource for dis-\n189\nease name recognition and concept normalization.\nJournal of biomedical informatics, 47:1‚Äì10.\nThibault F√©vry, Livio Baldini Soares, Nicholas FitzGer-\nald, Eunsol Choi, and Tom Kwiatkowski. 2020. En-\ntities as experts: Sparse memory access with entity\nsupervision. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP) , pages 4937‚Äì4951, Online. Associa-\ntion for Computational Linguistics.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas,\nNaoto Usuyama, Xiaodong Liu, Tristan Naumann,\nJianfeng Gao, and Hoifung Poon. 2020. Domain-\nspeciÔ¨Åc language model pretraining for biomedi-\ncal natural language processing. arXiv preprint\narXiv:2007.15779.\nYun He, Ziwei Zhu, Yin Zhang, Qin Chen, and James\nCaverlee. 2020. Infusing Disease Knowledge into\nBERT for Health Question Answering, Medical In-\nference and Disease Name Recognition. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n4604‚Äì4614, Online. Association for Computational\nLinguistics.\nMar√≠a Herrero-Zazo, Isabel Segura-Bedmar, Paloma\nMart√≠nez, and Thierry Declerck. 2013. The ddi\ncorpus: An annotated corpus with pharmacological\nsubstances and drug‚Äìdrug interactions. Journal of\nbiomedical informatics, 46(5):914‚Äì920.\nZhengbao Jiang, Antonios Anastasopoulos, Jun Araki,\nHaibo Ding, and Graham Neubig. 2020. X-FACTR:\nMultilingual factual knowledge retrieval from pre-\ntrained language models. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5943‚Äì5959,\nOnline. Association for Computational Linguistics.\nQiao Jin, Bhuwan Dhingra, William Cohen, and\nXinghua Lu. 2019. Probing biomedical embeddings\nfrom language models. In Proceedings of the 3rd\nWorkshop on Evaluating Vector Space Representa-\ntions for NLP, pages 82‚Äì89.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nMartin Krallinger, Obdulia Rabal, Saber A Akhondi,\nMartƒ±n P√©rez P√©rez, Jes√∫s Santamar√≠a, Gael P√©rez\nRodr√≠guez, et al. 2017. Overview of the biocreative\nvi chemical-protein interaction track. In Proceed-\nings of the sixth BioCreative challenge evaluation\nworkshop, volume 1, pages 141‚Äì146.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So, and\nJaewoo Kang. 2020. Biobert: a pre-trained biomed-\nical language representation model for biomedical\ntext mining. Bioinformatics, 36(4):1234‚Äì1240.\nPatrick Lewis, Myle Ott, Jingfei Du, and Veselin Stoy-\nanov. 2020. Pretrained language models for biomed-\nical and clinical tasks: Understanding and extend-\ning the state-of-the-art. In Proceedings of the 3rd\nClinical Natural Language Processing Workshop ,\npages 146‚Äì157, Online. Association for Computa-\ntional Linguistics.\nJiao Li, Yueping Sun, Robin J Johnson, Daniela Sci-\naky, Chih-Hsuan Wei, Robert Leaman, Allan Peter\nDavis, Carolyn J Mattingly, Thomas C Wiegers, and\nZhiyong Lu. 2016. Biocreative v cdr task corpus:\na resource for chemical disease relation extraction.\nDatabase, 2016.\nCarolyn E Lipscomb. 2000. Medical subject headings\n(mesh). Bulletin of the Medical Library Association,\n88(3):265.\nWeijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju,\nHaotang Deng, and Ping Wang. 2020. K-bert:\nEnabling language representation with knowledge\ngraph. Proceedings of the AAAI Conference on Arti-\nÔ¨Åcial Intelligence, 34(03):2901‚Äì2908.\nIlya Loshchilov and Frank Hutter. 2017. Decou-\npled weight decay regularization. arXiv preprint\narXiv:1711.05101.\nGeorge Michalopoulos, Yuanxin Wang, Hussam Kaka,\nHelen Chen, and Alex Wong. 2020. Umlsbert: Clin-\nical domain knowledge augmentation of contextual\nembeddings using the uniÔ¨Åed medical language sys-\ntem metathesaurus.\nMark Neumann, Daniel King, Iz Beltagy, and Waleed\nAmmar. 2019. ScispaCy: Fast and robust models\nfor biomedical natural language processing. In Pro-\nceedings of the 18th BioNLP Workshop and Shared\nTask, pages 319‚Äì327, Florence, Italy. Association\nfor Computational Linguistics.\nYifan Peng, Shankai Yan, and Zhiyong Lu. 2019.\nTransfer learning in biomedical natural language\nprocessing: An evaluation of BERT and ELMo on\nten benchmarking datasets. In Proceedings of the\n18th BioNLP Workshop and Shared Task, pages 58‚Äì\n65, Florence, Italy. Association for Computational\nLinguistics.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227‚Äì2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nMatthew E. Peters, Mark Neumann, Robert Logan, Roy\nSchwartz, Vidur Joshi, Sameer Singh, and Noah A.\nSmith. 2019. Knowledge enhanced contextual word\nrepresentations. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\n190\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 43‚Äì54, Hong Kong, China. Associ-\nation for Computational Linguistics.\nFabio Petroni, Tim Rockt√§schel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463‚Äì2473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nHoo-Chang Shin, Yang Zhang, Evelina Bakhturina,\nRaul Puri, Mostofa Patwary, Mohammad Shoeybi,\nand Raghav Mani. 2020. BioMegatron: Larger\nbiomedical domain language model. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n4700‚Äì4706, Online. Association for Computational\nLinguistics.\nLarry Smith, Lorraine K Tanabe, Rie Johnson nee\nAndo, Cheng-Ju Kuo, I-Fang Chung, Chun-Nan\nHsu, Yu-Shi Lin, Roman Klinger, Christoph M\nFriedrich, Kuzman Ganchev, et al. 2008. Overview\nof biocreative ii gene mention recognition. Genome\nbiology, 9(2):1‚Äì19.\nMichael Q Stearns, Colin Price, Kent A Spackman,\nand Amy Y Wang. 2001. Snomed clinical terms:\noverview of the development process and project sta-\ntus. In Proceedings of the AMIA Symposium , page\n662. American Medical Informatics Association.\nShikhar Vashishth, Rishabh Joshi, Ritam Dutt, Denis\nNewman-GrifÔ¨Ås, and Carolyn Rose. 2020. Medtype:\nImproving medical entity linking with semantic type\nprediction. arXiv preprint arXiv:2005.00460.\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhiyuan\nLiu, Juanzi Li, and Jian Tang. 2019. Kepler: A\nuniÔ¨Åed model for knowledge embedding and pre-\ntrained language representation. arXiv preprint\narXiv:1911.06136.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google‚Äôs neural machine\ntranslation system: Bridging the gap between hu-\nman and machine translation. arXiv preprint\narXiv:1609.08144.\nB. Xu, X. Shi, Z. Zhao, and W. Zheng. 2018. Leverag-\ning biomedical resources in bi-lstm for drug-drug in-\nteraction extraction. IEEE Access, 6:33432‚Äì33439.\nZheng Yuan, Zhengyun Zhao, and Sheng Yu. 2020.\nCoder: Knowledge infused cross-lingual medical\nterm embedding for term normalization. arXiv\npreprint arXiv:2011.02947.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: En-\nhanced language representation with informative en-\ntities. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 1441‚Äì1451, Florence, Italy. Association\nfor Computational Linguistics."
}