{
  "title": "GreenPLM: Cross-Lingual Transfer of Monolingual Pre-Trained Language Models at Almost No Cost",
  "url": "https://openalex.org/W4385764025",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2102522522",
      "name": "Qingcheng Zeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5051633211",
      "name": "Lucas Garay",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2134394054",
      "name": "Peilin Zhou",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2994678223",
      "name": "Dading Chong",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2807260057",
      "name": "Yining Hua",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2776842456",
      "name": "Wu Jiageng",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A5079280336",
      "name": "Yikang Pan",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2103679216",
      "name": "Han Zhou",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A2128960775",
      "name": "Rob Voigt",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2106930453",
      "name": "Jie Yang",
      "affiliations": [
        "Zhejiang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1089141",
    "https://openalex.org/W3209051700",
    "https://openalex.org/W2954835819",
    "https://openalex.org/W3037032032",
    "https://openalex.org/W2915977242",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W4299579390",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3035032094",
    "https://openalex.org/W3177252310",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2252046065",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W4226395792",
    "https://openalex.org/W2950733326",
    "https://openalex.org/W4308900169",
    "https://openalex.org/W4287888698",
    "https://openalex.org/W4318621130",
    "https://openalex.org/W2995549860",
    "https://openalex.org/W3035547806",
    "https://openalex.org/W3112849432",
    "https://openalex.org/W3094501123",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W3214020110",
    "https://openalex.org/W3038047279",
    "https://openalex.org/W3204526376",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3116295307",
    "https://openalex.org/W4393360451",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W4385681388",
    "https://openalex.org/W3022569409",
    "https://openalex.org/W2561995736",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W4303939357",
    "https://openalex.org/W3093721400",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W2963186636"
  ],
  "abstract": "Large pre-trained models have revolutionized natural language processing (NLP) research and applications, but high training costs and limited data resources have prevented their benefits from being shared equally amongst speakers of all the world's languages. To address issues of cross-linguistic access to such models and reduce energy consumption for sustainability during large-scale model training, this study proposes an effective and energy-efficient framework called GreenPLM that uses bilingual lexicons to directly ``translate'' pre-trained language models of one language into another at almost no additional cost. We validate this approach in 18 languages' BERT models and show that this framework is comparable to, if not better than, other heuristics with high training costs. In addition, given lightweight continued pre-training on limited data where available, this framework outperforms the original monolingual language models in six out of seven tested languages with up to 200x less pre-training efforts. Aiming at the Leave No One Behind Principle (LNOB), our approach manages to reduce inequalities between languages and energy consumption greatly. We make our codes and models publicly available at https://github.com/qcznlp/GreenPLMs.",
  "full_text": "GreenPLM: Cross-Lingual Transfer of Monolingual Pre-Trained Language\nModels at Almost No Cost\nQingcheng Zeng2 , Lucas Garay1 , Peilin Zhou3 , Dading Chong4 , Yining Hua5 , Jiageng\nWu1 , Yikang Pan1 , Han Zhou6 , Rob Voigt2 and Jie Yang1\n1School of Public Health and the Second Affiliated Hospital, Zhejiang University\n2Department of Linguistics, Northwestern University\n3Data Science and Analytics Thrust, Hong Kong University of Science and Technology (Guangzhou)\n4School of Electronic and Computer Engineering, Peking University\n5Department of Biomedical Informatics, Harvard University\n6Language Technology Lab, University of Cambridge\njieynlp@gmail.com\nAbstract\nLarge pre-trained models have revolutionized nat-\nural language processing (NLP) research and ap-\nplications, but high training costs and limited data\nresources have prevented their benefits from be-\ning shared equally amongst speakers of all the\nworld’s languages. To address issues of cross-\nlinguistic access to such models and reduce en-\nergy consumption for sustainability during large-\nscale model training, this study proposes an effec-\ntive and energy-efficient framework called Green-\nPLM that uses bilingual lexicons to directly “trans-\nlate” pre-trained language models of one language\ninto another at almost no additional cost. We vali-\ndate this approach in 18 languages’ BERT models\nand show that this framework is comparable to, if\nnot better than, other heuristics with high training\ncosts. In addition, given lightweight continued pre-\ntraining on limited data where available, this frame-\nwork outperforms the original monolingual lan-\nguage models in six out of seven tested languages\nwith up to 200x less pre-training efforts. Aiming\nat the Leave No One Behind Principle (LNOB),\nour approach manages to reduce inequalities be-\ntween languages and energy consumption greatly.\nWe make our codes and models publicly available\nat https://github.com/qcznlp/GreenPLMs.\n1 Introduction\nIn recent years, NLP has welcomed great advances and sig-\nnificant progress in deep learning [Vaswani et al., 2017;\nDevlin et al., 2019; Brownet al., 2020]. Pre-trained language\nmodels (PLMs) trained on extensive text have significantly\nimproved performance on various core NLP tasks, bring-\ning NLP new research paradigms such as “pre-training and\nfine-tuning” and “prompt-based learning” [Liu et al., 2022].\nHowever, despite PLMs’ extraordinary performance, training\nthese models both incurs substantial computational costs and\nrelies upon the availability of extensive training data. The for-\nmer has raised public concern about substantial energy con-\nsumption and environmental damage [Strubell et al., 2020],\nwhile the latter inevitably creates unequal opportunities for\nNLP research across languages with differing access to re-\nsources [Joshi et al., 2020].\nComputation costs for training a single, state-of-the-art\ndeep learning model expanded 300,000 times between 2012\nand 2018, a trend which has only intensified since; this\npresents a challenge for researchers interested in uphold-\ning green AI principles [Schwartz et al., 2020] as well as\nbroader public policy ambitions like the United Nations Sus-\ntainable Development Goals (SDG) [Assembly, 2015]. Fig-\nure 1 shows a comparison of a number of representative\nPLMs and multimodal models from BERT-base[Peters et al.,\n2018] to GPT-4 [OpenAI, 2023], where we observe trends of\never-increasing model size, computational costs, economic\ncosts, and environmental costs [Strubell et al., 2019]. Pre-\ntraining a BERT-base model incurs cloud computing costs\nof $3,751 to $12,571 [Schwartz et al., 2020], which is unaf-\nfordable for personal use and most researchers in developing\ncountries. Training models such as BERT, T5 [Raffel et al.,\n2020], PaLM, and GPT-3 [Brown et al., 2020] from scratch\nemit 0.65, 46.7, 271.43, and 552.1 tons of CO2, respectively.\nBeyond economic and environmental costs, the pre-\ntraining of large language models requires vast amounts of\nmonolingual text data, which is lacking in many low-resource\nlanguages, leading to increased disparities in the utilization of\nlanguage models among different languages. Languages with\na larger number of speakers inherently have larger corpora\navailable for PLM training. This leaves only a tiny fraction\nof the approximately 7000 languages worldwide with suffi-\ncient data to train a monolingual PLM [Ebrahimi and Kann,\n2021]. Table 1 lists the sizes of training materials for the\nmonolingual BERT of several languages, exhibiting a sig-\nnificant inequality in monolingual materials. For instance,\nEnglish GPT-3 was trained on 45 TB of monolingual mate-\nrial, equivalent to 4,608 times size of the Hindi BERT pre-\ntraining corpus. This basic inequality in data availability is\nfurther exacerbated by many social and financial factors, such\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nSpecial Track on AI for Good\n6290\nJun-2018 Sep-2018 Jan-2019 Apr-2019 Jul-2019 No v-201 9Feb-2020 Ma y-2020 Aug-2020 Dec-2020 Ma r-2021 Jun-2021 Oct-2021 Jan-2022 Apr-2022 Jul-2022 No v-202 2Feb-2023 Ma y-2023\nNumbers of Parameters\nDate\n1B\n10B\n100B\n1T\n10T\n100M\nT5\n11,000M\n700 GB\n1,300,000\n46.7 tons\nGPT-3\n175,000M\n700 GB\n4,600,000\n552.1 tons\nBERT-Base\n110M\n16 GB\n3,751\n0.65 tons\nGopher\n280,000M\n10.5 TB\n?\n380 tons\nPaLM\n540,350M\n780 billion tokens\n?\n271.43 tons\nCPM Large\n2600M\n100 GB\n? tons\n? \nERNIE3.0\n10,000M\n4 TB\n? tons\n? \nXLNET\n340M\n130 GB\n245,000\n? tons\nBERT-Large\n340M\n16 GB\n6,912\n? tons\nChinese BERT-wwm\n102M\n0.4 billion words\n? tons\n? \nM6-100B\n100,000M\n60.5 million image-text pairs\n? tons\n?\nWuDao 2.0\n1,750,000M\n4.9 TB text and image data\n? tons\n?\nPersian BERT\n163M\n1.3 billion words\n? tons\n?\nIndoBERT\n110M\n220 million words\n? tons\n?\nItalian BERT\n110M\n13 GB\n? tons\n?\nBETO\n110M\n18.4 GB\n? tons\n? \nViLBERT\n245M\n3.3 million image-caption pairs\n? tons\n? \nCLIP\n428M\n400 million image-text pairs\n? tons\n?\nGPT-4\n?\n? GB\n?\n? tons\nBLOOM\n176,000M\n366 billion tokens\n?\n? tons\nGLM\n130,000M\n400 billion tokens\n?\n? tons\nPaLM2\n?\n? GB\n?\n? tons\nFigure 1: Trends of Complexity and Training Cost in Mainstream PLMs with Significant Performance Boosts. The icons represent the number\nof parameters, training corpus size, financial cost by dollars, and the emission of CO 2, respectively. ? represents numbers that can not be\nestimated.\nas internet infrastructure, education, and computational re-\nsources available in low-resource speech communities. Self-\nsupervised pre-training thus enlarges the disparity between\nhigh- and low-resource languages, with the result that break-\nthrough technologies in contemporary NLP have proven par-\nticularly difficult to share equally among speakers of all the\nworld’s languages.\nTo contribute to ongoing efforts to mitigate these issues,\nwe propose a generalizable framework, GreenPLM, to “trans-\nlate” monolingual PLMs to new languages at almost no ad-\nditional cost. We hypothesize that the linguistic knowledge\nlearned by PLMs on large monolingual corpora is transfer-\nable to low-resource languages, encouraged by two primary\npieces of evidence. First, monolingual word embeddings are\npartially isomorphic across languages [Artetxe et al., 2016];\netymologically close language pairs show good word trans-\nlation performance [Conneau et al., 2017]. Second, work on\nthe capacities of monolingual PLMs [Conneau et al., 2020b;\nBlevins and Zettlemoyer, 2022 ] has shown that they encode\ncross-linguistically useful information even without training\nin further languages. Therefore, GreenPLM utilizes bilin-\ngual lexicons to bridge cross-lingual semantic spaces while\nkeeping the well-trained parameters of high-resource PLMs\nfixed as shown in Figure 2. Our experimental results show\nthat GreenPLM’s performance is comparable to or better than\ncurrent heuristics in 18 tested languages, at nearly zero cost.\nIn addition, with around 0.5% computational cost compared\nto pre-training a BERT model from scratch, continued pre-\ntraining of these models (which we call GreenPLM+) outper-\nforms the performance of monolingual PLMs. To sum up,\nour contributions to AI and social good are the following:\n1) We propose a simple, heuristic pipeline utilizing bilin-\ngual lexicons to translate a source language PLM to\na target language PLM with almost no computational\ncost, significantly reducing carbon emissions for build-\ning foundation PLMs in various languages;\n2) We verify the effectiveness and generalizability of this\nframework with evaluations in 18 languages, showing\nperformance comparable to monolingual and multilin-\ngual PLMs;\n3) We propose these models can be further enhanced with\ncontinued pre-training in 7 languages and show perfor-\nmance exceeding existing monolingual and multilingual\nPLMs while reducing pre-training costs up to 200x com-\npared to pre-training from scratch;\n4) We suggest this approach can be used to reduce language\ndisparities and energy consumption in pre-training LMs,\naligned with the SDG and the LNOB principle to pro-\nmote the application of AI for social good.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nSpecial Track on AI for Good\n6291\nEN-TR EN-IT\n(a) Bilingual lexicons collection (b) Pretrained Language Models Selection\n<CLS>\ngustos\nmanzana\nTranslated Spanish PLM\n<UNK>\n<CLS>\nIikes\napple\ntastes\n<UNK>\nnew\nshare\nEnglish PLM\nPLM Encoders\nTarget Language Embedding Layer Source Language Embedding Layer\nXLNET\nBERT RoBERTa \nSpanBERT ERNIE\nALBERT\n...\nLocal Corpus \nEnhanced PLM\n...\nTarget Language Tasks\nNER ClassificationPOSTarget Language \nPLM\n(c) PLM Translation\n...\nEN-FA\n EN-TL\n(d) PLM Evaluation\nMLM\nnuevas\nnuevos\n##stone\n##lly\n##stone\n##lly\nPLM Encoders\nAverage\nnewnuevas\nIikes\ngustos\nnuevos\napplemanzana\ntastes\n... ...\nSpanish English\nEN-ES\nFigure 2: Overview of the GreenPLM approach. Bilingual lexicons\n(a) are used to augment the vocabulary and embedding layer of a\nsource PLM (b) using our proposed Lexicon Walk Mapping strat-\negy (c), while holding the parameters and architecture of the source\nmodel constant, resulting in a translated model that can be further\nfine-tuned towards particular target-language tasks or continually\npre-trained using a smaller dataset in the translated language (d).\n2 Related Work\n2.1 Cost Reduction in PLM Training\nAs we’ve explored, training large PLMs inevitably requires\nextensive computational resources, which attracted many\nstudies on efficient PLM pre-training. For example, [?] pro-\nposed a task-driven language modeling framework to pre-\ntrain task-specific PLMs that required only around 2% of the\noriginal computational costs. An alternative approach ex-\nplored by [Chen et al., 2022] and [Qin et al., 2022] leveraged\na small “teacher PLM” to pre-train a large “student PLM” at a\nlow computational cost. Reducing data requirements remains\nless studied compared to the former. The issue of data re-\nquirements has been less studied, though recently [Warstadt\net al., 2023] put forth a BabyLM challenge to call for sample-\nefficient pre-training. Our work addresses both these issues,\nfocusing on simultaneously reducing computational costs and\ndata requirements.\n2.2 Language Disparity in PLMs\nResearchers have proposed numerous mechanisms to equal-\nize performance across human languages in PLMs [Muller et\nal., 2021; Ebrahimi and Kann, 2021 ], but the democratiza-\ntion of language technology remains a major challenge[Bird,\n2020]. Multilingual BERT (mBERT) represents a prelimi-\nnary attempt to adapt PLMs to multiple languages and is pre-\ntrained on the concatenation of corpora from different lan-\nguages [Devlin et al., 2019]. Such models can achieve sub-\nstantial coverage; for example, mBERT and XLM-R [Con-\nneau et al., 2020a] cover 104 and 100 languages respectively.\nNevertheless, model performances in specific languages are\nreported to be highly dependent on pre-training corpus sizes,\nand cross-lingual PLMs trained in this manner are still ex-\npensive to train and ultimately rely upon the availability of\nmonolingual texts in a given target language.\n2.3 Cross-Lingual Transfer\nAfter multilingual BERT showed impressive performance in\nzero-shot cross-language transfer settings [Pires et al., 2019],\nsubstantial attempts have been made to adapt monolingual or\nmultilingual PLMs to other languages.\n[Wu and Dredze, 2020 ] examined the performance of\nmBERT across various languages and reported that the model\nperforms well for high-resource languages but not for low-\nresource languages. Nonetheless, in low-resource settings,\nmBERT performs better than monolingual BERT, validating\nsome degree of cross-language generalizability. [Muller et\nal., 2021 ] explored mBERT’s performance in never-before-\nseen languages, finding unstable performance. They found\nthat mBERT has unstable performance across languages, de-\npending on the training data. [Ebrahimi and Kann, 2021 ]\nused the New Testament, which is available in most lan-\nguages, to adapt multilingual PLMs to more than 1600 lan-\nguages using continued pre-training, vocabulary extension,\nand adapter transformers. [Wang et al., 2022] explored how\nbilingual lexicons can enhance the multilingual language cov-\nerage of PLMs. Their results had consistent improvement\nacross sequence labeling and dependency parsing tasks in 19\nlow-resource languages. In contrast, our current study ex-\ntends bilingual lexicons to monolingual settings as a bridge\nrather than data augmentation components to design speci-\nfied monolingual PLMs equally for all languages.\nMonolingual adaptation remains relatively under-explored\ncompared to multilingual. It follows two general directions:\nextending monolingual PLMs to bilingual and adapting a\nsource language PLM to a target language PLM. In the first\ndirection, [Artetxe et al., 2020] continually pre-trained En-\nglish PLMs on data in target languages with the transformer\nbody frozen and fine-tuned the PLMs on labeled English data.\nTheir results confirmed the generalizability of monolingual\nPLMs across languages. [Tran, 2020] transferred PLMs by\ninitializing foreign language embeddings in the English vec-\ntor space and jointly fine-tuning both models. For the sec-\nond direction, [de Vries and Nissim, 2021], and [Minixhofer\net al., 2022] used word embeddings as the bridge to adapt\nmonolingual models to other languages with continued pre-\ntraining. Motivated by these works, we leverage more widely\navailable and less computationally expensive resources other\nthan word embeddings to generalize PLMs.\n3 Methodology\n3.1 Overall Architecture\nAn overview of the GreenPLM framework is shown in Fig-\nure 2. We start from a bilingual lexicon for a given source-\ntarget language pair and the selection of a source PLM. Im-\nportantly, contemporary PLMs require a pre-established vo-\ncabulary embedding layer which operates on the outputs of a\nspecific tokenizer. Tokens in the vocabulary are a mix of both\nfull words and word pieces, traditionally the outputs of an\nalgorithm like WordPiece or byte pair encoding, where each\ntoken is mapped to an embedding learned in the pre-training\nprocess.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nSpecial Track on AI for Good\n6292\nIn our approach, we augment the vocabulary and embed-\nding layer of the source PLM to incorporate target-language\nvocabulary from the bilingual lexicon, using a method to map\nthis lexicon into a shared semantic space explained below\nin subsection 3.2. This results in what we call a “trans-\nlated PLM,” though notably the embedding layer still con-\ntains the entire vocabulary from the source language and the\nPLM encoder parameters are yet unchanged from the source\nPLM. Such a model can already be applied directly to target-\nlanguage data; in experiments, we call this setting simply\n“GreenPLM”.\nThe translated PLM can also be fine-tuned towards particu-\nlar tasks in the target language, or pre-trained further on target\nlanguage data, both of which modify the PLM parameters to\nbe more suited to the characteristics of the target language.\nWe name this setting “GreenPLM+”.\n3.2 Lexicon Walk Mapping\nThe biggest challenge in the approach outlined above is how\nto use a bilingual lexicon to generate coherent target-language\nembeddings that can be fed into source-language PLM en-\ncoder parameters. To achieve this we propose a heuristic\nmethod we call Lexicon Walk Mapping (LWM), in which a\nbilingual lexicon is used as the bridge to create a mapping be-\ntween the semantic spaces of the source and target languages.\nWe define a bilingual lexiconas L = {xi, yi} where xi\nis a word or phrase in the source language (English, in this\nwork) and yi is xi’s translation in the target language. Then\ngiven a source language’s PLMS with its vocabularySvocab\nand token embeddings Sembedding, we walk through the lex-\nicon and operate on each entry separately. For each entry,\nwe first tokenize the source word or phrase xi into a set of\ntokens T = {t1, t2, ...ti...tn | ti ∈ Svocab}, and then map T\nto the token embedding layer of S to retrieve a set of embed-\ndings E = {e1, e2, ...ei...en | ei ∈ Sembedding}. We then\ntake the dimension-wise average of E, and the new embed-\nding eaverage serves as yi’s token embedding in the Green-\nPLM. If yi repeats in the lexicon, as is frequently the case\nin one-to-many translations across languages, the GreenPLM\nframework accumulates several instances of eaverage and in\nturn averages them at the end of the procedure to generate a\nfinal token embedding.\nOur approach leaves the original token embeddings of\nsource language words, tokenized word pieces, and special\ncharacters in Svocab untouched in the translated PLM; in-\nstead, we merely expand the vocabulary size by adding un-\nseen target language words in the bilingual lexicon. For ex-\nample in Spanish, the GreenPLM has a vocabulary size of\n119,999 words, while the corresponding Spanish monolin-\ngual BERT only has 31,002.\n3.3 Continued Pre-training\nContinued pre-training has long been validated as one of\nthe most effective strategies to enhance PLMs, with sev-\neral possible strategies including masked language model-\ning (MLM) [Devlin et al., 2019 ] and translation language\nmodeling (TLM) [Conneau and Lample, 2019 ]. In addition,\nsome special techniques, such as contrastive learning, can be\ncombined with pre-training tasks. Here we use MLM as the\nbasic pre-training task in our continued pre-training experi-\nments. To generate a GreenPLM+ model for each language,\nthe translated PLM is further pre-trained with a batch size of\n1024 and a sequence length of 128 symbols by utilizing 10%\nof the OSCAR dataset[Ortiz Su´arez et al., 2019]. Each model\nwas pre-trained for 5 epochs and every 10,000 pre-training\nsteps were saved.\nThe experiments presented here examine GreenPLM+\nmodels in seven languages: Spanish, Indonesian, Tagalog,\nTurkish, Italian, Romanian, and Persian because they are\nthe only languages with a moderate amount of language re-\nsources. An attempt to further pre-train a GreenPLM for Mal-\ntese fails because the available resources (17 MB in the OS-\nCAR corpus) are too scarce to maintain training stability.\n3.4 Alternatives to LWM\nWe found that LWM worked most effectively as a method to\nbridge the embedding spaces between models, and therefore\npresent experimental results using that method. However,\nwe also tried several other logical alternatives that were not\nas effective. We describe those methods here for complete-\nness and report detailed results of these methods in Spanish,\nTurkish, and Tagalog in the supplementary material. Because\nthese methods are strictly worse, we did not further experi-\nment on other languages.\nVocabulary Expansion (VE)refers to expanding the vo-\ncabulary based on the vocabulary of the source language\nPLM. Given a bilingual lexiconlexicon ={xi, yi} and the\nPLM S of the source language with a vocabulary of Svocab\n= {w1, w2, ..., wi, ..., wn} and token embeddingsSembedding,\nVE will walk throughSvocab and find their corresponding em-\nbeddings by Sembedding and lexicon. If no matching entries\nexist, this strategy will keep the original tokens. If there is one\nyi corresponding to multiple xi, VE will keep the first xi’s\nembedding as the translated embedding. Then, VE will merge\nall Wi as the vocabulary of the new model, where translated\nembeddings are used for the newly extended vocabulary.\nVocabulary One-on-one Mapping (VOM)adopts a simi-\nlar methodology to VE but only uses one translation for multi-\nple entries. While both LWM and VE increase the vocabulary\nsizes, introducing more parameters, VOM yields a Green-\nPLM with the same vocabulary sizes.\nVocabulary Translation Mapping (VTM)is the most ba-\nsic strategy being tested. It cuts sentences according to space\ndelimitation and uses bilingual lexicons to directly transfer\nthe tokens into another language. Then, the newly generated\nsentences are directly fed to the source language PLM for\ndownstream tasks.\n4 Materials and Evaluation\n4.1 Language and Model Choices\nEnglish was chosen as a source language since it has the\nlargest pre-training corpus size and the largest body of ex-\nisting PLMs available. For target languages, we included 18\nlanguages from five language families (Table 1). According\nto [Joshi et al., 2020], the chosen languages fall under various\nlanguage resources’ levels ranging from the extremely low-\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nSpecial Track on AI for Good\n6293\nLanguage Language F amily\nLanguage Resource Monolingual PLMs’ Corpus Size\nSpanish Indo-European 5 -\nWinners 2996.01 million words\nIndonesian Austronesian 3 - Rising Stars 220 million words\nTurkish Turkic 4 - Underdogs 440.50 million words\nTagalog Austronesian 3 - Rising Stars 39 million words\nItalian Indo-European 4 - Underdogs 2050 million words\nRomanian Indo-European 3 - Rising Stars 2421.3 million words\nPersian Indo-European 4 - Underdogs 1300 million words\nMaltese Afro-Asiatic 2 - Hopefuls -\nM¯aori Austronesian 1 - Scraping-Bys -\nWolof Niger–Congo 2 - Hopefuls -\nLuganda Niger–Congo 1 - Scraping-Bys -\nIlokano Austronesian 1 - Scraping-Bys -\nHausa Afro-Asiatic 2 - Hopefuls -\nBulgarian Indo-European 3 - Rising Stars -\nLatvian Indo-European 3 - Rising Stars -\nAncient Greek Indo-European - -\nNorwegian Indo-European 1 - Scraping-Bys -\nDanish Indo-European 3 - Rising Stars -\nTable 1: Languages represented in this work, representing five lan-\nguage families and a variety of settings with regard to resource avail-\nability.\nresource level “1 - The Scraping-Bys” to the high-resource\nlevel “5 - The Winners”.\nIn these experiments, we used the bert-base-uncased En-\nglish model as the source PLM, since it has the broadest\ncoverage in academic and industrial usage. Since the model\nis relatively lightweight it also allows for extensive experi-\nments on various NLP tasks across languages, even with lim-\nited computational resources. Nevertheless, the GreenPLM\nframework is independent of the particular structure of PLMs\nbeyond the embedding layer, and hence can theoretically be\napplied to larger models to benefit from continuing advances\nin large-scale PLMs.\n4.2 Experimental Settings\nBilingual Lexicons\nThis study uses bilingual lexicons from four sources. For rel-\natively high-resource languages, our lexicons are from MUSE\n[Conneau et al., 2017], PanLex [Kamholz et al., 2014], and\nOPUS [Tiedemann and Nygaard, 2004]. For Maltese, M¯aori,\nWolof, Luganda, Ilokano, and Hausa, we used lexicons re-\nleased from [?] directly. Detailed statistics of the lexicons\ncould be accessed in the supplementary material.\nEvaluation\nMost chosen languages have available benchmark datasets for\nperformance comparison. For example, [Ca˜nete et al., 2020]\nas the Spanish benchmark; IndoLEM in [Koto et al., 2020]\nfor Indonesian; [Schweter, 2020a], [Cruz and Cheng, 2019;\nCruz and Cheng, 2020; Cruzet al., 2020], [Schweter, 2020b],\n[Dumitrescu et al., 2020] and [Farahani et al., 2021] for Turk-\nish, Tagalog, Italian, Romanian, and Persian, respectively.\nFor the 11 low-resource languages with only NER and POS\ntasks, GreenPLM was tested on MasakhaNER[Adelani et al.,\n2021] and universal dependencies [de Marneffe et al., 2021].\nFor benchmark datasets with multiple tasks, we divided the\ntasks into three main categories: POS, NER, and text classi-\nfication for a by-category analysis of our GreenPLM frame-\nwork. Specifically, for continued pre-training, this study uses\nbatch size× sequence length× training stepsas a metric to\nevaluate pre-training efforts.\nOur translated PLMs were evaluated by performance com-\nparison with existing models on various NLP tasks, includ-\ning part-of-speech tagging (POS), named entity recognition\n(NER), sentiment analysis, sentence classification, and para-\nphrase identification. We used the F1 score for the NER task,\naccuracy in POS and most classification tasks, Pearson’s co-\nefficient in the tweet ordering task of Indonesian, and ham-\nming loss in the Dengue task of Tagalog. Specifically, We\nre-scaled rare metrics like hamming loss and Pearson’s coef-\nficient to fit a 0 to 100 scale and took the average of model\nperformance for a direct comparison.\nBaseline Methods\nTo compare the performance of GreenPLM with existing\nmodels, four deep learning models were chosen as baselines,\nand fine-tuned towards each evaluation task: (1) Scratch:\na randomly initialized BERT model; (2) Pre-BERT SOTA:\nstate-of-the-art models before BERT, i.e. long short-term\nmemory (LSTM) and convolutional neural networks (CNNs)\nwith pre-trained word embeddings, which we implement us-\ning NCRF++ [Yang and Zhang, 2018]; (3) mBERT: multilin-\ngual BERT [Devlin et al., 2019]; and (4) monolingual BERT\nmodels, where available. In addition, GreenPLM+, the ad-\nvanced version of GreenPLM with a continued pre-training\non a very limited dataset, was also compared with the above\nmodels. Only uncased models were tested in this study be-\ncause most bilingual lexicons are uncased.\n5 Results\nOur experiments test baseline, GreenPLM, and GreenPLM+\nmodels on 18 languages across various categories of tasks\nlisted in Table 2. Detailed results on each specific task could\nbe accessed in the supplementary material.\n5.1 Base GreenPLM\nIn the base setting where models are translated using only\nLWM, we observe that the GreenPLM performs much better\nthan random BERT in all cases, and better than Pre-BERT\nSOTA models in 15 out of 18 languages, with the excep-\ntion of sequence labeling tasks in 3 languages. This sug-\ngests that indeed, the bilingual lexicon based approach we\ntake with GreenPLM is not merely successful due to task-\nspecific fine-tuning, but rather achieves performance gains\ndue to cross-linguistic knowledge transfer from the source\nPLM. Though GreenPLM generally does not surpass mBERT\nin performance, our method provides at least comparable\nperformance on the examined languages at nearly no cost,\nwhereas mBERT itself requires large multilingual training\ncosts.\nMoreover, in some language settings our method is sur-\nprisingly effective. In Tagalog, the GreenPLM wins over all\nbaselines and shows a large improvement over the monolin-\ngual BERT. This may imply that the original Tagalog BERT\nwas not pre-trained with sufficient data. In the low-resource\nsettings of Maltese and Ilokano, the GreenPLM also outper-\nforms all existing baselines.\n5.2 GreenPLM+: Continued Pre-training\nAs a further enhancement to the GreenPLM, we applied\na lightweight continued pre-training step with very limited\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nSpecial Track on AI for Good\n6294\nLanguage Task by\nCategory Scratch Pre-BERT SOTA mBERT monolingual BERT GreenPLM GreenPLM+\nSpanish POS, NER, Classification*3\n67.38 81.68 88.04 89.37 85.27 88.08 (0.7%)\nIndonesian POS, NER*2, Classification*3 63.67 75.17 81.09 84.56 81.08 84.57 (1.4%)\nTurkish POS*2, NER 70.38 90.41 92.97 93.07 90.70 93.09 (2.5%)\nTagalog Classification*3 77.41 77.44 89.43 85.77 89.59 89.59 (0%)\nItalian POS*2 83.47 96.39 97.44 97.38 95.98 97.63 (0.5%)\nRomanian POS, NER, Classification*2 67.08 82.26 86.5 88.72 85.39 88.79 (1.2%)\nPersian NER*2, Classification*3 84.9 90.52 91.96 93.59 91.05 93.60 (0.8%)\nMaltese POS, NER 40.47 70.79 73.30 - 76.93 -\nM¯aori NER 53.33 89.44 81.97 - 79.21 -\nWolof POS, NER 48.45 72.99 72.72 - 72.71 -\nLuganda NER 30.39 74.65 74.19 - 74.48 -\nIlokano NER 29.65 64.58 67.44 - 67.98 -\nHausa NER 50.66 82.38 85.58 - 84.97 -\nBulgarian POS 92.16 97.93 99.06 - 97.96 -\nLatvian POS 89.53 94.48 96.33 - 94.59 -\nAncient Greek POS 89.42 95.00 96.00 - 95.08 -\nNorwegian POS 92.82 96.50 97.61 - 96.75 -\nDanish POS 82.37 94.50 97.80 - 95.90 -\nTable 2: Tasks and experimental results in six settings. Numbers represent average performance across language-specific evaluation tasks;\nsince the available tasks are different for each language, numbers are only comparable within their own row. Parentheticals in the last column\npresent the continued pre-training efforts required to reach performance equal to the corresponding monolingual model compared to training\nfrom scratch; for Tagalog, this number is 0% because we didn’t continue pretraining GreenPLM as it has outperformed the monolingual one.\ntraining cost and evaluated the results in the seven lan-\nguages mentioned above with access to sufficiently-sized pre-\ntraining corpora. In the last column of Table 2, we evaluate\nGreenPLM+ models at the first point at which they outper-\nform mBERT or monolingual BERT for the first time during\nthe continued pre-training, and note in parens the percentage\nof pre-training efforts required to reach this level of perfor-\nmance compared to pre-training a PLM from scratch. We ob-\nserve that GreenPLM+ outperforms all monolingual BERT at\na relatively low cost, except in Spanish. For example, Green-\nPLM+ outperforms the original PLM with only 0.68% of\nthe pre-training efforts for Indonesian. For Spanish, Green-\nPLM+ outperforms multilingual BERT and achieves compa-\nrable performance over the monolingual BERT when given\n0.7% of the pre-training efforts. We further compared contin-\nuously pre-training GreenPLM and mBERT under the same\nsettings in Indonesian, Romanian, and Persian. GreenPLM\ncould catch up and outperform mBERT given minimal costs.\nIn Figure 3(b), we show a visualized comparison of Persian.\n6 Discussion\nImpact on Sustainable Development.High computational\nrequirements for pre-training LMs or even pre-BERT deep\nlearning models have been a major obstacle for researchers,\nwhere a slight performance boost may come at the expense\nof dramatically increased model complexity. Our approach\ngenerates translated PLMs for medium-level and low-level\nresource languages that successfully outperform pre-BERT\nSOTA deep learning models at extremely low pre-training\ncosts, both in terms of economic costs and CO 2 emissions.\nIn contrast to prior approaches for low-cost training of PLMs\n[Yao et al., 2022], the GreenPLM framework is task-agnostic\nAll computation for PLM translation in the basic GreenPLM\napproach can be executed on a personal computer within 10\nseconds. For continued pre-training with the GreenPLM+\nmodel, we demonstrate comparable results to a fully pre-\ntrained monolingual model using less than 10% of the dataset\nsize in less than 8 hours on an 8x RTX3090 system.\nImpacts on Language Equality. Unequal language re-\nsources have limited recent advances in PLMs from being\nequally shared by speakers of all the world’s languages.\nWhile developing larger corpora for pre-training is not al-\nways feasible, here we leverage a more widely available lan-\nguage resource – bilingual lexicons – to translate PLMs from\nhigher- to lower-resource settings. This framework can serve\nas a basic starting point to transfer models across languages\nand reduce both language-based and economic disparities in\nthe availability of PLMs in different linguistic and socioeco-\nnomic backgrounds.\nExperimental results show that GreenPLM+ performs bet-\nter than existing monolingual and multilingual models in lan-\nguages with small but still substantial pre-training corpus\nsizes, such as Indonesian, Turkish, and Tagalog. These lan-\nguages are rated as ”4 - Underdogs” or ”3 - Rising Stars”\nin the [Joshi et al., 2020 ] classification of resource avail-\nability. We note with an example from Persian in Figure 3\n(b) that in these settings, the base GreenPLM model com-\nmonly starts off with lower performance than mBERT, but the\nGreenPLM+ quickly catches up after continued pre-training\nwith the same number of steps.\nFor languages with fewer resources in the ‘2 - Hopefuls’\nand ‘1 - Scraping-Bys’ categories by [Joshi et al., 2020 ],\nwhere substantial pre-training corpora are not available for\ncontinued pre-training, we find that mBERT retains bet-\nter performance than the base GreenPLM. Nevertheless, the\nsimple and nearly zero-cost transfer method presented here\nachieves performance generally surpassing Pre-BERT SOTA\nmodels, and comparable to mBERT, suggesting that base\nGreenPLM models can be productively used to transfer PLMs\nto languages where only bilingual lexicons exist.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nSpecial Track on AI for Good\n6295\n78.2\n44.2\n84.2\n87.4\n78.9\n96\n87.3\n78.1\n95\n40\n60\n80\n100\nClassification NER POS\nTask Types\nScore\nScratch mBERT GreenPLM(a)\n92.5\n93.0\n93.5\n94.0\n0 K 100 K 200 K 300 K 400 K\nTraining Steps\nScore\nmBERT GreenPLM+(b)\n0.4\n0.6\n0.8\n40 60 80 100\nDistance\nERR\n(c)\nFigure 3: Performance comparison of GreenPLM under different settings. (a) Results across NER, POS, and Classification task types. (b)\nComparison between continued pre-training GreenPLM+ versus multilingual BERT in Persian. (c) We observe a linear relationship between\nlanguage distance and error reduction rate.\nApplicability for Downstream Tasks. The evaluations in\nthis work examined downstream tasks in the categories of\nPOS tagging, NER, and classification tasks, corresponding\nto word-level, span-level, and sentence-level understanding.\nResults in Figure 3(a) show that the GreenPLM, compared\nto random BERT models, has the most remarkable improve-\nment on POS tasks, followed by NER and classification tasks.\nHowever, compared to mBERT, GreenPLM falls short in\nPOS, followed by NER and classification tasks. This sug-\ngests that GreenPLM transfers sentence-level understanding\nthe best in downstream tasks, possibly because embedding-\nbased strategies of our framework focus mainly on semantic\ninformation and lack the ability to capture the variability of\nsyntactic information in natural languages.\nLanguage Distance. In addition, we use a language dis-\ntance calculator from eLinguistics 1 as a reference and quan-\ntify the performance by the error reduction rate (ERR) met-\nric because languages are classified according to their di-\nachronic relatedness in linguistic typology. Corresponding\nresults in Figure 3(c) show that the closer the selected lan-\nguage is to English, the better its GreenPLM will perform.\nThis is not surprising since GreenPLM models retain the pa-\nrameters from the original English BERT being translated;\nnevertheless, it suggests that linguistic distance is likely to be\na crucial consideration in cross-lingual knowledge transfer,\nsupporting existing findings [Lin et al., 2019].\nFuture Work. Though this paper uses English as a source\nlanguage and starts from an English PLM to generate tar-\nget language PLMs, a similar approach could be used to\nmerge PLMs from multiple languages for cross-enhancement\nto generate a single target language PLM. In addition, Green-\nPLM could be extended to multi-modal and multi-domain\nsettings where bilingual dictionaries could facilitate informa-\ntion exchange; for example, could a similar approach help in-\ntegrate biomedical information using a targeted lexicon? Fi-\nnally, it remains to investigate how the GreenPLM approach\nperforms with more state-of-the-art BERT-class PLMs such\nas RoBERTa, as well as with PLMs of different architectures\n1http://www.elinguistics.net/Compare Languages.aspx\nsuch as BART [Lewis et al., 2020].\nLimitations. This work has two main limitations. First, it\nonly covers uncased PLMs at the current stage because most\nbilingual lexicons are uncased, and capitalization could affect\nmodel performance. Second, there is no mechanism to rank\nthe importance of single entries in bilingual lexicons. There-\nfore, newly generated models generally have a more extensive\nvocabulary than the original PLM.\n7 Conclusion\nThis paper proposes a bilingual lexicon-based framework\nGreenPLM that “translates” a monolingual PLM to other lan-\nguages with no additional cost. We verify its effectiveness\nin 18 languages and further pre-train the resulting models to\nvalidate their performance when given minimal pre-training\ncosts. Our results provide evidence that the GreenPLM+\n(GreenPLM with very limited continued pre-training) outper-\nforms monolingual BERT at much lower computational costs\nthan current frameworks, such as the continued pre-training\nof mBERT. This framework is easy to incorporate into ex-\nisting PLMs and can be adapted to various languages given\nthe relatively wide availability of bilingual lexicons. We hope\nthat this work can make a positive impact on promoting green\nAI and reducing language disparities in access to contempo-\nrary NLP technologies.\nAcknowledgments\nThis research was supported by the National Natural Science\nFoundation of China (No. 62206243).\nContribution Statement\nQingcheng Zeng, Lucas Garay, and Peilin Zhou contributed\nequally to this paper. Correspondence to Jie Yang.\nReferences\n[Adelani et al., 2021] David Ifeoluwa Adelani, Jade Abbott,\nGraham Neubig, Daniel D’souza, Julia Kreutzer, Constan-\ntine Lignos, Chester Palen-Michel, Happy Buzaaba, Shruti\nRijhwani, Sebastian Ruder, Stephen Mayhew, Israel Abebe\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nSpecial Track on AI for Good\n6296\nAzime, Shamsuddeen H. Muhammad, Chris Chinenye\nEmezue, Joyce Nakatumba-Nabende, Perez Ogayo, Aremu\nAnuoluwapo, Catherine Gitau, Derguene Mbaye, Jesujoba\nAlabi, Seid Muhie Yimam, Tajuddeen Rabiu Gwadabe, Ig-\nnatius Ezeani, Rubungo Andre Niyongabo, Jonathan Mukiibi,\nVerrah Otiende, Iroro Orife, Davis David, Samba Ngom,\nTosin Adewumi, Paul Rayson, Mofetoluwa Adeyemi, Gerald\nMuriuki, Emmanuel Anebi, Chiamaka Chukwuneke, Nkiruka\nOdu, Eric Peter Wairagala, Samuel Oyerinde, Clemencia Siro,\nTobius Saul Bateesa, Temilola Oloyede, Yvonne Wambui, Victor\nAkinode, Deborah Nabagereka, Maurice Katusiime, Ayodele\nAwokoya, Mouhamadane MBOUP, Dibora Gebreyohannes,\nHenok Tilaye, Kelechi Nwaike, Degaga Wolde, Abdoulaye Faye,\nBlessing Sibanda, Orevaoghene Ahia, Bonaventure F. P. Dossou,\nKelechi Ogueji, Thierno Ibrahima DIOP, Abdoulaye Diallo,\nAdewale Akinfaderin, Tendai Marengereke, and Salomey Osei.\nMasakhaNER: Named entity recognition for African languages.\nTransactions of the Association for Computational Linguistics,\n9:1116–1131, 2021.\n[Artetxe et al., 2016] Mikel Artetxe, Gorka Labaka, and Eneko\nAgirre. Learning principled bilingual mappings of word embed-\ndings while preserving monolingual invariance. In Proceedings\nof the 2016 EMNLP, pages 2289–2294, Austin, Texas, November\n2016.\n[Artetxe et al., 2020] Mikel Artetxe, Sebastian Ruder, and Dani Yo-\ngatama. On the cross-lingual transferability of monolingual rep-\nresentations. In Proceedings of the 58th ACL, pages 4623–4637,\nOnline, July 2020.\n[Assembly, 2015] UN General Assembly. Transforming our world:\nthe 2030 agenda for sustainable development. https://www.\nrefworld.org/docid/57b6e3e44.html, 2015. Accessed: 2023-06-\n13.\n[Bird, 2020] Steven Bird. Decolonising speech and language tech-\nnology. In Proceedings of the 28th COLING, pages 3504–3519,\nBarcelona, Spain (Online), December 2020.\n[Blevins and Zettlemoyer, 2022] Terra Blevins and Luke Zettle-\nmoyer. Language contamination helps explains the cross-lingual\ncapabilities of English pretrained models. In Proceedings of the\n2022 EMNLP, pages 3563–3574, Abu Dhabi, United Arab Emi-\nrates, December 2022.\n[Brown et al., 2020] Tom Brown, Benjamin Mann, Nick Ryder,\nMelanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\nSandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom\nHenighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey\nWu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christo-\npher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. Language models are few-shot learners. In\nH. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin,\neditors, NIPS, volume 33, pages 1877–1901. Curran Associates,\nInc., 2020.\n[Ca˜nete et al., 2020] Jos´e Ca ˜nete, Gabriel Chaperon, Rodrigo\nFuentes, Jou-Hui Ho, Hojin Kang, and Jorge P´erez. Spanish pre-\ntrained bert model and evaluation data. In PML4DC at ICLR\n2020, 2020.\n[Chen et al., 2022] Cheng Chen, Yichun Yin, Lifeng Shang, Xin\nJiang, Yujia Qin, Fengyu Wang, Zhi Wang, Xiao Chen, Zhiyuan\nLiu, and Qun Liu. bert2BERT: Towards reusable pretrained lan-\nguage models. In Proceedings of the 60th ACL, pages 2134–\n2148, Dublin, Ireland, May 2022.\n[Conneau and Lample, 2019] Alexis Conneau and Guillaume Lam-\nple. Cross-Lingual Language Model Pretraining. Curran Asso-\nciates Inc., Red Hook, NY , USA, 2019.\n[Conneau et al., 2017] Alexis Conneau, Guillaume Lample,\nMarc’Aurelio Ranzato, Ludovic Denoyer, and Herv ´e J ´egou.\nWord translation without parallel data. arXiv preprint\narXiv:1710.04087, 2017.\n[Conneau et al., 2020a] Alexis Conneau, Kartikay Khandelwal,\nNaman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Fran-\ncisco Guzm ´an, Edouard Grave, Myle Ott, Luke Zettlemoyer,\nand Veselin Stoyanov. Unsupervised cross-lingual representation\nlearning at scale. In Proceedings of the 58th ACL, pages 8440–\n8451, Online, July 2020.\n[Conneau et al., 2020b] Alexis Conneau, Shijie Wu, Haoran Li,\nLuke Zettlemoyer, and Veselin Stoyanov. Emerging cross-lingual\nstructure in pretrained language models. In Proceedings of the\n58th ACL, pages 6022–6034, Online, July 2020.\n[Cruz and Cheng, 2019] Jan Christian Blaise Cruz and Charibeth\nCheng. Evaluating language model finetuning techniques for\nlow-resource languages. arXiv preprint arXiv:1907.00409, 2019.\n[Cruz and Cheng, 2020] Jan Christian Blaise Cruz and Charibeth\nCheng. Establishing baselines for text classification in low-\nresource languages. arXiv preprint arXiv:2005.02068, 2020.\n[Cruz et al., 2020] Jan Christian Blaise Cruz, Jose Kristian Resa-\nbal, James Lin, Dan John Velasco, and Charibeth Cheng. Investi-\ngating the true performance of transformers in low-resource lan-\nguages: A case study in automatic corpus creation.arXiv preprint\narXiv:2010.11574, 2020.\n[de Marneffe et al., 2021] Marie-Catherine de Marneffe, Christo-\npher D. Manning, Joakim Nivre, and Daniel Zeman. Univer-\nsal Dependencies. Computational Linguistics, 47(2):255–308,\n07 2021.\n[de Vries and Nissim, 2021] Wietse de Vries and Malvina Nissim.\nAs good as new. how to successfully recycle English GPT-2 to\nmake models for other languages. In Findings of the ACL: ACL-\nIJCNLP 2021, pages 836–846, Online, August 2021.\n[Devlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina Toutanova. BERT: Pre-training of deep bidirectional\ntransformers for language understanding. In Proceedings of the\n2019 NAACL-HLT, pages 4171–4186, Minneapolis, Minnesota,\nJune 2019.\n[Dumitrescu et al., 2020] Stefan Dumitrescu, Andrei-Marius\nAvram, and Sampo Pyysalo. The birth of Romanian BERT. In\nFindings of the ACL: EMNLP 2020, pages 4324–4328, Online,\nNovember 2020.\n[Ebrahimi and Kann, 2021] Abteen Ebrahimi and Katharina Kann.\nHow to adapt your pretrained multilingual model to 1600 lan-\nguages. In Proceedings of the 59th ACL, pages 4555–4567, On-\nline, August 2021.\n[Farahani et al., 2021] Mehrdad Farahani, Mohammad Gharachor-\nloo, Marzieh Farahani, and Mohammad Manthouri. ParsBERT:\nTransformer-based model for persian language understanding.\nNeural Processing Letters, 53(6):3831–3847, oct 2021.\n[Joshi et al., 2020] Pratik Joshi, Sebastin Santy, Amar Budhiraja,\nKalika Bali, and Monojit Choudhury. The state and fate of lin-\nguistic diversity and inclusion in the NLP world. In Proceedings\nof the 58th ACL, pages 6282–6293, Online, July 2020.\n[Kamholz et al., 2014] David Kamholz, Jonathan Pool, and Susan\nColowick. PanLex: Building a resource for panlingual lexical\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nSpecial Track on AI for Good\n6297\ntranslation. In Proceedings of the Ninth LREC, pages 3145–3150,\nReykjavik, Iceland, May 2014.\n[Koto et al., 2020] Fajri Koto, Afshin Rahimi, Jey Han Lau, and\nTimothy Baldwin. IndoLEM and IndoBERT: A benchmark\ndataset and pre-trained language model for Indonesian NLP. In\nProceedings of the 28th COLING, pages 757–770, Barcelona,\nSpain (Online), December 2020.\n[Lewis et al., 2020] Mike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin\nStoyanov, and Luke Zettlemoyer. BART: Denoising sequence-\nto-sequence pre-training for natural language generation, transla-\ntion, and comprehension. In Proceedings of the 58th ACL, pages\n7871–7880, Online, July 2020.\n[Lin et al., 2019] Yu-Hsiang Lin, Chian-Yu Chen, Jean Lee, Zirui\nLi, Yuyan Zhang, Mengzhou Xia, Shruti Rijhwani, Junxian He,\nZhisong Zhang, Xuezhe Ma, Antonios Anastasopoulos, Patrick\nLittell, and Graham Neubig. Choosing transfer languages for\ncross-lingual learning. In Proceedings of the 57th ACL, pages\n3125–3135, Florence, Italy, July 2019.\n[Liu et al., 2022] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao\nJiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt,\nand predict: A systematic survey of prompting methods in natural\nlanguage processing. ACM Comput. Surv., sep 2022.\n[Minixhofer et al., 2022] Benjamin Minixhofer, Fabian Paischer,\nand Navid Rekabsaz. WECHSEL: Effective initialization of sub-\nword embeddings for cross-lingual transfer of monolingual lan-\nguage models. In Proceedings of the 2022 Conference of the\nNorth American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages 3992–4006,\nSeattle, United States, July 2022. Association for Computational\nLinguistics.\n[Muller et al., 2021] Benjamin Muller, Antonios Anastasopoulos,\nBenoˆıt Sagot, and Djam ´e Seddah. When being unseen from\nmBERT is just the beginning: Handling new languages with mul-\ntilingual language models. In Proceedings of the 2021 NAACL-\nHLT, pages 448–462, Online, June 2021.\n[OpenAI, 2023] OpenAI. Gpt-4 technical report. ArXiv,\nabs/2303.08774, 2023.\n[Ortiz Su´arez et al., 2019] Pedro Javier Ortiz Su´arez, Benoˆıt Sagot,\nand Laurent Romary. Asynchronous pipelines for processing\nhuge corpora on medium to low resource infrastructures. Pro-\nceedings of the Workshop on Challenges in the Management of\nLarge Corpora (CMLC-7) 2019. Cardiff, 22nd July 2019, pages\n9 – 16, Mannheim, 2019. Leibniz-Institut f¨ur Deutsche Sprache.\n[Peters et al., 2018] Matthew E. Peters, Mark Neumann, Mohit\nIyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. Deep contextualized word representations. In Pro-\nceedings of the 2018 NAACL-HLT, pages 2227–2237, New Or-\nleans, Louisiana, June 2018.\n[Pires et al., 2019] Telmo Pires, Eva Schlinger, and Dan Garrette.\nHow multilingual is multilingual BERT? In Proceedings of the\n57th ACL, pages 4996–5001, Florence, Italy, July 2019.\n[Qin et al., 2022] Yujia Qin, Yankai Lin, Jing Yi, Jiajie Zhang,\nXu Han, Zhengyan Zhang, Yusheng Su, Zhiyuan Liu, Peng Li,\nMaosong Sun, and Jie Zhou. Knowledge inheritance for pre-\ntrained language models. In Proceedings of the 2022 NAACL-\nHLT, pages 3921–3937, Seattle, United States, July 2022.\n[Raffel et al., 2020] Colin Raffel, Noam Shazeer, Adam Roberts,\nKatherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. Exploring the limits of transfer learn-\ning with a unified text-to-text transformer. JMLR, 21(140):1–67,\n2020.\n[Schwartz et al., 2020] Roy Schwartz, Jesse Dodge, Noah A Smith,\nand Oren Etzioni. Green ai. Communications of the ACM,\n63(12):54–63, 2020.\n[Schweter, 2020a] Stefan Schweter. Berturk - bert models for turk-\nish. https://doi.org/10.5281/zenodo.3770924, 2020. Accessed:\n2023-06-13.\n[Schweter, 2020b] Stefan Schweter. Italian bert and electra models.\nhttps://doi.org/10.5281/zenodo.4263142, 2020. Accessed: 2023-\n06-13.\n[Strubell et al., 2019] Emma Strubell, Ananya Ganesh, and An-\ndrew McCallum. Energy and policy considerations for deep\nlearning in NLP. In Proceedings of the 57th ACL, pages 3645–\n3650, Florence, Italy, July 2019.\n[Strubell et al., 2020] Emma Strubell, Ananya Ganesh, and An-\ndrew McCallum. Energy and policy considerations for modern\ndeep learning research. Proceedings of the AAAI, 34(09):13693–\n13696, Apr. 2020.\n[Tiedemann and Nygaard, 2004] J¨org Tiedemann and Lars Ny-\ngaard. The OPUS corpus - parallel and free: http://logos.uio.\nno/opus. In Proceedings of the Fourth LREC, Lisbon, Portugal,\nMay 2004.\n[Tran, 2020] Ke Tran. From english to foreign languages:\nTransferring pre-trained language models. arXiv preprint\narXiv:2002.07306, 2020.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki Par-\nmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Pro-\nceedings of the 31st International Conference on Neural Informa-\ntion Processing Systems, NIPS’17, page 6000–6010, Red Hook,\nNY , USA, 2017. Curran Associates Inc.\n[Wang et al., 2022] Xinyi Wang, Sebastian Ruder, and Graham\nNeubig. Expanding pretrained models to thousands more lan-\nguages via lexicon-based adaptation. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 863–877, Dublin, Ireland, May\n2022. Association for Computational Linguistics.\n[Warstadt et al., 2023] Alex Warstadt, Leshem Choshen, Aaron\nMueller, Adina Williams, Ethan Wilcox, and Chengxu Zhuang.\nCall for papers–the babylm challenge: Sample-efficient pre-\ntraining on a developmentally plausible corpus. arXiv preprint\narXiv:2301.11796, 2023.\n[Wu and Dredze, 2020] Shijie Wu and Mark Dredze. Are all lan-\nguages created equal in multilingual BERT? In Proceedings of\nthe 5th Workshop on Representation Learning for NLP, pages\n120–130, Online, July 2020.\n[Yang and Zhang, 2018] Jie Yang and Yue Zhang. NCRF++: An\nopen-source neural sequence labeling toolkit. In Proceedings\nof ACL 2018, System Demonstrations, pages 74–79, Melbourne,\nAustralia, July 2018.\n[Yao et al., 2022] Xingcheng Yao, Yanan Zheng, Xiaocong Yang,\nand Zhilin Yang. NLP from scratch without large-scale pretrain-\ning: A simple and efficient framework. In Kamalika Chaud-\nhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu,\nand Sivan Sabato, editors, Proceedings of the 39th ICML, vol-\nume 162 of PMLR, pages 25438–25451. PMLR, 17–23 Jul 2022.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nSpecial Track on AI for Good\n6298",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8416293859481812
    },
    {
      "name": "Heuristics",
      "score": 0.7417529225349426
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5899205207824707
    },
    {
      "name": "Natural language processing",
      "score": 0.5613299012184143
    },
    {
      "name": "Energy consumption",
      "score": 0.4925764203071594
    },
    {
      "name": "Language model",
      "score": 0.48648449778556824
    },
    {
      "name": "Scale (ratio)",
      "score": 0.47729623317718506
    },
    {
      "name": "Training set",
      "score": 0.4394959509372711
    },
    {
      "name": "Engineering",
      "score": 0.0584239661693573
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210158318",
      "name": "Second Affiliated Hospital of Zhejiang University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I200769079",
      "name": "Hong Kong University of Science and Technology",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I136199984",
      "name": "Harvard University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I241749",
      "name": "University of Cambridge",
      "country": "GB"
    }
  ]
}