{
  "title": "ELMformer: Efficient Raw Image Restoration with a Locally Multiplicative Transformer",
  "url": "https://openalex.org/W4294295598",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2384608317",
      "name": "Ma, Jiaqi",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2252350612",
      "name": "Yan Sheng-yuan",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2222876097",
      "name": "Zhang, Lefei",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A529042452",
      "name": "Wang Guo-li",
      "affiliations": [
        "Horizon Robotics (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2044492937",
      "name": "Zhang Qian",
      "affiliations": [
        "Horizon Robotics (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2160547390",
    "https://openalex.org/W3035710032",
    "https://openalex.org/W6773080324",
    "https://openalex.org/W2126926806",
    "https://openalex.org/W2799265886",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W1915360731",
    "https://openalex.org/W3182000414",
    "https://openalex.org/W2056370875",
    "https://openalex.org/W3201772641",
    "https://openalex.org/W3180355996",
    "https://openalex.org/W2572006047",
    "https://openalex.org/W3129492101",
    "https://openalex.org/W2552290192",
    "https://openalex.org/W3178192988",
    "https://openalex.org/W2902857081",
    "https://openalex.org/W2963312584",
    "https://openalex.org/W2793146153",
    "https://openalex.org/W3112576346",
    "https://openalex.org/W3108194308",
    "https://openalex.org/W3109500538",
    "https://openalex.org/W2963314397",
    "https://openalex.org/W3173217100",
    "https://openalex.org/W3174535912",
    "https://openalex.org/W2962767526",
    "https://openalex.org/W3174300208",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2130184048",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2964030969",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3175515048",
    "https://openalex.org/W2133665775",
    "https://openalex.org/W3126013860",
    "https://openalex.org/W6783332649",
    "https://openalex.org/W3011158920",
    "https://openalex.org/W2965217508",
    "https://openalex.org/W2508457857",
    "https://openalex.org/W3169761664",
    "https://openalex.org/W2172275395",
    "https://openalex.org/W4300424419",
    "https://openalex.org/W3090412929",
    "https://openalex.org/W3097262261"
  ],
  "abstract": "In order to get raw images of high quality for downstream Image Signal Process (ISP), in this paper we present an Efficient Locally Multiplicative Transformer called ELMformer for raw image restoration. ELMformer contains two core designs especially for raw images whose primitive attribute is single-channel. The first design is a Bi-directional Fusion Projection (BFP) module, where we consider both the color characteristics of raw images and spatial structure of single-channel. The second one is that we propose a Locally Multiplicative Self-Attention (L-MSA) scheme to effectively deliver information from the local space to relevant parts. ELMformer can efficiently reduce the computational consumption and perform well on raw image restoration tasks. Enhanced by these two core designs, ELMformer achieves the highest performance and keeps the lowest FLOPs on raw denoising and raw deblurring benchmarks compared with state-of-the-arts. Extensive experiments demonstrate the superiority and generalization ability of ELMformer. On SIDD benchmark, our method has even better denoising performance than ISP-based methods which need huge amount of additional sRGB training images. The codes are release at https://github.com/leonmakise/ELMformer.",
  "full_text": "ELMformer: Efficient Raw Image Restoration with a Locally\nMultiplicative Transformer\nJiaqi Maâˆ—\nSchool of Computer Science, Wuhan\nUniversity\nWuhan, China\njiaqima@whu.edu.cn\nShengyuan Yan\nSchool of Computer Science, Wuhan\nUniversity\nWuhan, China\nshengyuan_yan@whu.edu.cn\nLefei Zhangâ€ \nWuhan University\nHubei Luojia Laboratory\nWuhan, China\nzhanglefei@whu.edu.cn\nGuoli Wang\nHorizon Robotics\nBeijing, China\nguoli.wang@horizon.ai\nQian Zhang\nHorizon Robotics\nBeijing, China\nqian01.zhang@horizon.ai\nABSTRACT\nIn order to get raw images of high quality for downstream Im-\nage Signal Process (ISP), in this paper we present an Efficient Lo-\ncally Multiplicative Transformer called ELMformer for raw image\nrestoration. ELMformer contains two core designs especially for\nraw images whose primitive attribute is single-channel. The first\ndesign is a Bi-directional Fusion Projection (BFP) module, where\nwe consider both the color characteristics of raw images and spatial\nstructure of single-channel. The second one is that we propose\na Locally Multiplicative Self-Attention (L-MSA) scheme to effec-\ntively deliver information from the local space to relevant parts.\nELMformer can efficiently reduce the computational consumption\nand perform well on raw image restoration tasks. Enhanced by\nthese two core designs, ELMformer achieves the highest perfor-\nmance and keeps the lowest FLOPs on raw denoising and raw\ndeblurring benchmarks compared with state-of-the-arts. Extensive\nexperiments demonstrate the superiority and generalization ability\nof ELMformer. On SIDD benchmark, our method has even better\ndenoising performance than ISP-based methods which need huge\namount of additional sRGB training images. The codes are release\nat https://github.com/leonmakise/ELMformer.\nKEYWORDS\nRaw Images, Image Signal Process, Image Restoration, Transformer\n1 INTRODUCTION\nImage restoration is a classic low-level vision task in which raw\nimage restoration is a special but important subtask. From the per-\nspective of Image Signal Processing (ISP), raw data refers to unpro-\ncessed information from image sensors and is mainly captured as\nthe initial data source. Raw restoration models output high-quality\nraw data whose noises and artifacts are removed. Restoration on\nraw images can be very essential because those final noisy sRGB\nimages are processed by ISP which contains nonlinear transforma-\ntion that will extort original shot-and-read noise. Non-negligible\nshot-and-read noises through the ISP pipeline result in the complex\nnoise distribution of raw images, which can significantly obstacle\nâˆ—This work was done during Jiaqi Maâ€™s internship at Horizon Robotics.\nâ€ Corresponding author. He is affiliated with National Engineering Research Center\nfor Multimedia Software, School of Computer Science, Wuhan University\nFLOPs (G/109)\nPSNR (dB)\n10 20 30 40 500\n50.8\n51.1\n51.4\n51.7\nELMformer\nUformer\nUPI*\nCycleISP*\nNb2Nb\nELMformer(3.55G)\nUformer(10.24G)\nPRIDNet(8.06G)\nNb2Nb(7.88G)\n50.5\n52.0\nPRIDNet\nUPI*(4.90G)\nCycleISP*(47.37G)\nFigure 1: FLOPs and PSNR of denoising methods on SIDD\nbenchmark with 128 Ã—128 patches. * represents that UPI and\nCycleISP use additional huge sRGB images for training, the\nothers are trained on the SIDD Medium dataset.\nrestoration process. Hence, researchers turn to RAW data where\nnoise and artifacts are uncorrelated and less complex [19, 60].\nAlthough recent models discuss the data structure and noise\ndistribution of raw images, they still neglect the local context in-\nformation hidden in the raw data structure. Hence, the raw image\nrestoration task faces two main challenges. The first is insufficient\nexploitation of the raw images by the simple â€™packingâ€™ strategy.\nThe second is the balance between performance and computational\ncost caused by high-resolution raw images.\nFor the first challenge, researchers focus on pre-processing strate-\ngies to avoid damaging structures of raw images. Chen et al. [10]\nfirstly connect raw images with sRGB ones by CNNs, and introduce\na â€™packingâ€™ strategy. Through â€™packingâ€™, the single-channel raw\nimage can be divided into four channels as â€™RGGBâ€™ (Red, Green,\narXiv:2208.14704v1  [cs.CV]  31 Aug 2022\nJiaqi Ma, Shengyuan Yan, Lefei Zhang, Guoli Wang, and Qian Zhang\nGreen, Blue) in Bayer patterns. Each channel is half-sized and only\ncontains single color information. Hence, the packed raw images\nare color-independent in every channel and still keep the spatial\ndependencies around pixelsâ€™ neighborhoods. Depending on this\nsimple strategy, many works [33, 40, 58] related to raw images are\nproposed. However, Liu et al. [35] reveal the error-prone mode\nwhen adapting augmentation methods designed for sRGB images\nto raw images. The errors easily occur especially when converting\nraw images to sRGB color space with cropping and flipping. Nev-\nertheless, we notice that raw images are pixel-wise aligned in raw\nrestoration tasks. Simple cropping or flipping operations would not\nharm the Bayer patterns, and the colors of restored sRGB images\nare consistent with degraded ones after demosaicing. Although\nthe packing strategy has been a common sense in raw image pro-\ncessing, the spatial dependencies are actually weak. Liang et al.\n[31] utilize Bi-directional Cross-modal Attention in every stage to\nenhance both color and spatial branches. From our insights, we\nprefer to leverage the advantages of color structures and spatial\ndependencies simultaneously for Transformer-based architectures,\nand we should ensure single-channel raw images to larger channels\nby projecting them into multiple subspaces. Hence, our ELMformer\nreplace simple projection module with the Bi-directional Fusion\nProjection (BFP) module at the input stage.\nIn terms of the second challenge, we try to make use of those\nlimited data by efficient and effective architectures. Zhenget al. [63]\npropose a deep convolutional dictionary learning method to learn\npriors for both representation coefficients and dictionaries. Huet al.\n[23] build a pseudo 3-D auto-correlation attention block through\n1-D convolutions and a light-weight 2-D structure. Chang et al. [8]\ndesign a residual spatial-adaptive block for denoising. Most focus\non the noise modeling rather than the basic structural characteris-\ntics of raw data. With Transformer-based methods uprising, those\nmethods outperform CNN-based methods and show their promis-\ning future. Wanget al. [53] propose an U-shaped Transformer-based\nnetwork for low-level tasks. Liang et al. [32] device a residual ar-\nchitecture by Swin Transformer. Besides, some studies refer to\ngenerating adequate raw data pairs from existing sRGB images.\nThey simulate the ISP pipeline into an invertible procedure to con-\nvert sRGB images into raw ones, i.e., UPI [5], CycleISP [60] and\nPseudoISP [7]. However, the balance of performance and computa-\ntional cost caused by high-resolution raw images is still a problem,\nespecially for Transformer-based methods like Uformer [53] and\nSwinIR [32]. They only consider the utilization of Transformer\nblocks (designed for sRGB images) but ignoring the characteristics\nof raw data structure. Besides, the limited size of shifted windows\nresults in small receptive fields. Simply enlarging window size will\nincrease the computational consumption quadratically. Therefore,\nan effective strategy should ensure a larger receptive field and\nacceptable computational cost. As is shown in Fig. 1, with the pro-\nposed Lm-Win Transformer blocks, our ELMformer balance the\nperformance and FLOPs well compared with other SOTAs.\nTo conclude, we utilize the Bi-directional Fusion Projection (BFP)\nmodule and Locally multiplicative Window (Lm-Win) Transformer\nblocks especially for raw image restoration. Both BFP module and\nLm-Win Transformer block are suitable for raw dataâ€™s characteris-\ntics and can guarantee their efficiency and effectiveness simulta-\nneously. Overall, we summarize the contributions of this paper as\nfollows:\nâ€¢We analysis the weaknesses of â€™packingâ€™ strategy on raw\ndata and utilize a Bi-directional Fusion Projection module to\ngenerate the initial projected features. The BFP module fuses\ncolor and spatial information and provides efficient structure\npriors inside Bayer pattern. Besides, the BFP module enlarges\nthe receptive field and reduces the computation cost at the\nsame image patch scale.\nâ€¢To further construct the dependencies between pixels, we\npropose an effective Locally Multiplicative SA (L-MSA) mod-\nule to enhance the neighborhood dependencies in local sub-\nwindows. The L-MSA module gives a specialized considera-\ntion of the characteristics of raw image data and is embedded\nin every Lm-Win Transformer block.\nâ€¢Both quantitative and qualitative results demonstrate the\nsuperiority of our proposed method. Ablation studies prove\nthe effectiveness of each proposed module in our method.\nThe generalization ability is also proved in this paper.\n2 RELATED WORK\n2.1 Image Restoration Architecture\nImage restoration is often referred to images in sRGB color space,\nwhile raw images are commonly excluded. When concerning de-\nnoising which is a principal subtask in image restoration, [23, 29,\n44, 62] corrupt clean sRGB images with AWGN and other synthetic\nnoise to supervise the training stage. [ 14, 21, 26] employ U-net\narchitecture to conduct image denoising. Others [ 24, 28, 57] are\nself-supervised models. And for deblurring which is also essen-\ntial for restoration, Nah et al. [41] propose a network which uses\nmulti-scale images as input. GAN and RNN are also employed in\n[27, 48]. Lu et al. [39] device disentangled representation for self-\nsupervised deblurring. Liang et al. [12] designs a non-blind network\nto restore night blurry images. Besides, Deng et al. [16] leverage a\nseparable-patch architecture collaborating with a multi-scale inte-\ngration scheme. For raw-based architecture, Xinget al. [58] propose\nan end-to-end Joint Demosaicing and Denoising (JDD) network\nbased on residual channel attention for joint demosaicing and de-\nnoising, and super-resolution. Wu et al. [55] advocate a blind JDD\nproblem and invent a novel divide-and-conquer method to tackle\nwith blind reconstruction from noisy raw images. Liu et al. [36]\nalso devise Invertible Blocks to involve the learning of demosaic-\ning. When concerning raw deblurring, Zhang et al. [61] propose a\nstacked network consisting of several repetitive DMPHN modules\nto remove blurring from raw images. Liang et al. [31] design a dual-\nbranch network, which deals with color and spatial information\nin every blocks. Here, we refer to [ 31, 59] and utilize as the BFP\nmodule in the projection stage.\n2.2 Raw Image Pairs\nRaw restoration aims to recover high-quality raw images from the\ndegraded raw image, while learning-based models need paired data.\nHence, the acquirement of raw image pairs and efficient utilization\nof paired data are essential. It is time-consuming and labor-intensive\nELMformer: Efficient Raw Image Restoration with a Locally Multiplicative Transformer\nBi-directional \nFusion \nProjection \nDownsampling\nLm-Win \nTransformer \nBlock\nDegraded Raw Data\nLm-Win \nTransformer \nBlock\nDownsampling\nLm-Win \nTransformer \nBlock\nRestored Raw Data\nUpsampling\nLm-Win \nTransformer \nBlock\nUpsampling\nLm-Win \nTransformer \nBlock\nOutput \nProjection \n...\n...\nÃ—2\nÃ—2\nÃ—2\nÃ—2\n1Ã—HÃ—W 1Ã—HÃ—W\nSkip-connection\nSkip-connection\nÃ—2\nCÃ—H/2Ã—W/2 CÃ—H/2Ã—W/2\n2CÃ—H/4Ã—W/4\n4CÃ—H/8Ã—W/8\n16CÃ—H/32Ã—W/32 16CÃ—H/32Ã—W/32\n4CÃ—H/8Ã—W/8\n2CÃ—H/4Ã—W/4\nFigure 2: Illustration of the ELMformer pipeline.\nto get them from the real world. Most algorithms [4, 43] add syn-\nthetic noises to clean images to generate pairs. Note that sRGB\nsynthetic noise is not realistic enough and statistical noise like\nAWGN is too simple. Shot-and-read noise based on learning and\nother kinds of learned noise are more realistic instead. The others\nadd shot-and-read noise to clean raw images and feed them into\nthe ISP pipeline to learn the noise changing and extortion. CycleISP\n[60] injects shot-and-read noise in raw images along with learned\ndevice-agnostic transformation rather than statistical synthetic\nnoise. Tim et al. [5] present a network to â€™unprocessâ€™ images by\nreverting the process of ISP and add shot-and-read noise in the â€™un-\nprocessingâ€™ process to generate realistic raw sensor measurements.\nLinh et al. [34] add shot-and-read noise to clean raw data and use\nGAN to generate realistic noisy raw images. Pseudo-ISP [7] jointly\nlearns ISP pipeline and signal-dependent â€™rawRGBâ€™ noise model to\nsynthesize realistic noisy images. Besides, paired raw images for\ndeblurring can be acquired more easily. The blur-sharp image pairs\nare taken from the same scene using the sharper one as the ground-\ntruth. In [31], the authors propose a raw deblurring dataset named\nDeblur-Raw. Many successive raw frames (3-5 frames) are randomly\npicked from raw videos in various scenes under the same camera\nsetting, where the center one is used as the sharp ground-truth and\nthe rest are blurred images. Considering that we already have some\nwell-collected datasets, our goal is to reach good performances both\nin quality and quantity. Hence, our ELMformer focuses on how\nto efficiently and effectively utilize limited raw image pairs rather\nthan extending data scale.\n2.3 Vision Transformer\nTransformer has been a powerful tool in computer vision recently.\nThe global self-attention is found to be not only suitable for NLP but\nalso CV tasks. For basic CV tasks, PVT [50], Swin Transformer [37],\nVit[17] and IPT [11] attain SOTA performances. Specially, Uformer\n[53] and SwinIR [32] rise as cutting-edge methods in vision trans-\nformer for low-level vision tasks such as sRGB image denoising and\ndeblurring. Uformer [53] exploits Local-enhanced Window (LeWin)\nTransformer block to conduct local transformer operation on multi-\nscale feature maps which can simultaneously capture long-range\nfeatures from high-level maps and low-level features from the local\ninformation provided by window shifting. Lianget al. [32] organize\nseveral Swin Transformer layers together with a residual connec-\ntion to form a Residual Swin Transformer Block to implement deep\nfeature extraction for image restoration. Image synthesis and image\nediting are also relevant to our restoration tasks. Cao et al. [6] pro-\npose image Local Autoregressive Transformer to better facilitate\nthe locally guided image synthesis. Esser et al. [18] combine convo-\nlution and Transformer to enhance the locality of high-resolution\nimage synthesis. In this paper, our ELMformer exploits short-range\ndependencies from raw images, especially the L-MSA can minimize\nthe performance drop with less computational cost.\n3 METHOD\n3.1 Network Architecture\nHere, we apply the classic U-shaped architecture with the BFP mod-\nule and Lm-Win Transformer blocks. As shown in Fig. 2, the overall\nstructure of the proposed ELMformer is a hierarchical network with\nskip-connections between the encoder and the decoder.\nFor instance, given a degraded raw data ğ¼ âˆˆR1Ã—ğ»Ã—ğ‘Š, ELM-\nformer firstly applies BFP module to extract low-level features\nğ‘‹0 âˆˆRğ¶Ã—ğ»\n2 Ã—ğ‘Š\n2 and project single channel to diverse subspaces.\nThen features are feeded to ğ¾ encoders just like [25, 45]. Each en-\ncoder contains two sequential Lm-Win Transformer blocks and one\ndownsampling layer. In the downsampling layer, we first reshape\nthe flattened features into 2D spatial feature maps, and then down-\nsample maps and double the channels using 4 Ã—4 convolution with\na stride of 2. Then two Lm-Win Transformer blocks are added at the\nbottleneck stage. Due to the hierarchical structure, our Transformer\nblocks can capture longer and even global dependencies when the\nfeature map size is reduced to the window size. The number of\ndecoders is consistent with ğ¾ encoders. Each decoder also consists\nof one upsampling layer and two sequential Lm-Win Transformer\nblocks. 2 Ã—2 transposed convolution with a stride of 2 is used for\nupsampling. The skip connections are set in every stages. Finally,\nthe features are flatten to 2D feature maps, and we upsample and\nproject them to obtain a single-channel data ğ‘… âˆˆR1Ã—ğ»Ã—ğ‘Š. Note\nthat we add a residual connection here asğ¼ = ğ¼+ğ‘…, and it accelerates\nthe learning speed of the whole network.ğ¾is chosen as 4 to simply\nconstruct the whole pipeline.\n3.2 Bi-directional Fusion Projection\nDenote one raw image asğ¼ âˆˆR1Ã—ğ»Ã—ğ‘Š, though our goal is to restore\na better raw image, we still need to demosaic it to get the final\nsRGB image. Hence, the correlations inside Bayer patterns which\nJiaqi Ma, Shengyuan Yan, Lefei Zhang, Guoli Wang, and Qian Zhang\nConv k3,s2,p1\nPacking\nConv k3,s1,p1\nConv k1,s1,p0 Conv k1,s1,p0\nSigmoid Sigmoid\nConcat\nColor route Spatial routeR1 R2\nR4R3\nR1 R2\nR4R3\nPacking\nFigure 3: Illustration of the BFP module. ğ‘˜,ğ‘ ,ğ‘ represents\nkernel size, stride and padding, respectively.\nare essential should be preserved well. For most of existing works,\nthey all split the one channel raw image into four channels (RGGB)\naccording to the order of CFA. The strategy is called â€™packingâ€™ and\nresults in one tensor which is 4 Ã—ğ»\n2 Ã—ğ‘Š\n2 with separate colors.\nConsidering that raw images contain both color (single pixel) and\nspatial (between neighbour pixels) information, the rough packing\nstrategy will downscale the resolution of images and breakdown the\ntight spatial order inside neighbor pixels. To avoid this defect and\nfully utilize the color and spatial information, we refer to [31, 59]\nand utilize it as the Bi-directional Fusion Projection module in the\nprojection stage.\nAs is shown in Fig. 3, the module leads to two routes: Color route\nfor keeping the color consistency and Spatial route for maintaining\nthe spatial structure of original raw images. For the Color route, we\nperform packing on single-channel raw images and feed them into\none 3 Ã—3 convolution to get their color-based features. Then the\nfeatures are passed to one 1 Ã—1 convolution along with Sigmoid\nfunction. For the Spatial route, we remove the packing strategy and\nuse one 3 Ã—3 convolution to downsample the input to the same\nscale as the output of packing. After that, we perform element-\nwise multiplications on both routes, which cross the color and\nspatial routes. Finally, the color-based and spatial-based features\nare concatenated in channel dimension for the following steps.\nNotice that the original size of raw image is 1 Ã—ğ»Ã—ğ‘Š, we actu-\nally project it into high-dimensional subspaces and downsample\nit to 1\n2 , which are features in size of ğ¶ Ã—ğ»\n2 Ã—ğ‘Š\n2 . Hence, we can\ntake those projected features in smaller size for further efficient\nprocessing, and get lower computational costs. Unlike simple Input\nProjection module which only contains 3 Ã—3 convolutions for fea-\nture extraction, we actually compute the attention of those routes\nand allocate their weights to another. By this cross multiplication\ndirection, two routes can be enhanced by each other, and the final\nfeatures contain two parts: one is dominant by color information\nLayerNorm\nW-MSA\nLayerNorm\nLeFF\nL-MSA\nTokens2Img\nImg2Tokens\nDepthwise Conv\nFC\nFC\nFigure 4: Details of the Lm-Win Transformer block.\nand the other is dominant by spatial structure. The BFP module can\nbe formulated as follows:\nFğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™ = DS(ğ¼)âŠ— Sigmoid(Conv1Ã—1(Conv3Ã—3(ğ‘ƒ(ğ¼))))\nFğ‘ğ‘œğ‘™ğ‘œğ‘Ÿ = Conv3Ã—3(P(ğ¼))âŠ— Sigmoid(Conv1Ã—1(DS(ğ¼)))\nFğµğ¹ğ‘ƒ = Concat(Fğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™,Fğ‘ğ‘œğ‘™ğ‘œğ‘Ÿ)\n(1)\nHere, DS(Â·), P(Â·)and Concat(Â·,Â·)are 2Ã—downsampling, packing\nand concatenation on channel dimension, respectively. For input\nof one raw image ğ¼, the BFP module outputs Fğµğ¹ğ‘ƒ as projected\nfeatures.The BFP module utilizes two different properties of raw\nimages: the spatial route considers pixels in the same color filter and\nthe color route maintains the spatial structure of original images.\n3.3 Locally Multiplicative Window\nTransformer block\nStandard vision transformer blocks [ 17, 49] reshape images into\nvectors as tokens to compute global self-attention. To avoid the\nquadratic computation cost of self-attention on larger resolutions,\nLiu et al. [37] propose a shifted window strategy. Although it brings\nthe vision transformer into reality, many studies [ 30, 56] reveal\nthat the local dependencies are not captured well by transformer.\nFor low-level vision tasks such as denoising and deblurring, the\nlocal context information is essential due to the complementar-\nity between a degraded pixel and its neighborhoods. Wang et al.\n[53] enhance the ability of capturing locality by introducing the\nLocally-enhanced Feed-Forward Network (LeFF). It can replace\nthe traditional MLP layer for better performance. Nevertheless, for\nraw data which is single-channel and high-resolution, the compu-\ntational cost is still too large. Whatâ€™s more, though the packing\nstrategy preserves the color information, the layouts of raw data\nimplicit the spatial characteristics unlike images in sRGB space.\nHence, it is essential to utilize the structural speciality of raw data\nin Transformer-based architectures.\nELMformer: Efficient Raw Image Restoration with a Locally Multiplicative Transformer\nTo address the above mentioned issues, we propose a Locally\nmultiplicative Window (Lm-Win) Transformer block, as shown in\nFig. 4. The proposed Lm-Win Transformer block is effective and\nefficient by its special design for raw data, which not only captures\nlong-range dependencies from the self-attention in transformer,\nbut also involves the short-range connections into transformer\nto capture useful local context. The Self-Attention (SA) module\nreserves the original Window-based Multi-head Self-Attention (W-\nMSA) and extends one Locally Multiplicative Self-Attention (L-\nMSA) branch. For the Forward-Feedback Network (FFN) part, as\nmany researches [30, 51] prove that the depth-wise3Ã—3 convolution\ncan enhance the locality of transformer, we apply the same LeFF\n[53]. Specifically, given the features at the (ğ‘™-1)-th block ğ‘‹ğ‘™âˆ’1, the\ncomputation of a Lm-Win Transformer block is represented as:\nğ‘‹ğ‘†ğ´\nğ‘™ = W-MSA(LN(ğ‘‹ğ‘™âˆ’1))âŠ— L-MSA(LN(ğ‘‹ğ‘™âˆ’1))+ ğ‘‹ğ‘™âˆ’1\nğ‘‹ğ‘™ = LeFF(LN(ğ‘‹ğ‘†ğ´\nğ‘™ ))+ ğ‘‹ğ‘†ğ´\nğ‘™\n(2)\nwhere ğ‘‹ğ‘†ğ´\nğ‘™ and ğ‘‹ğ‘™ are the outputs of the SA and LeFF parts\nrespectively. ğ¿ğ‘ represents the layer normalization [3].\nRevisiting Window-based Multi-head Self-Attention. Unlike\nthe vanilla Transformer which computes global self-attention, we\nperform self-attention within non-overlapping local windows [53].\nGiven the 2D feature maps ğ‘‹ âˆˆRğ¶Ã—ğ»Ã—ğ‘Š with ğ¶, ğ» and ğ‘Š being\nthe channel, height and width of the maps, we firstly split those fea-\nture maps into non-overlapping windows according to theğ»andğ‘Š\ndimensions with the window size of ğ‘€Ã—ğ‘€. In this way, we get the\nflattened and transposed features ğ‘‹ğ‘– âˆˆRğ‘€2Ã—ğ¶ from each window ğ‘–.\nSuppose we perform self-attention on those flattened features in\neach window, and the head number is ğ‘˜ and the head dimension is\nğ‘‘ğ‘˜ = ğ¶/ğ‘˜, the complexity of the ğ‘˜-th head self-attention in the non-\noverlapping windows should be: ğ‘‚((ğ‘€Ã—ğ‘€)2 Ã—ğ‘‘ğ‘˜)= ğ‘‚(ğ‘€4 Ã—ğ‘‘ğ‘˜).\nHere, the ğ‘˜-th head self-attention in the non-overlapping win-\ndows can be defined as:\nğ‘‹ = {ğ‘‹1,ğ‘‹2,...,ğ‘‹ ğ‘},ğ‘ = ğ» Ã—ğ‘Š\nğ‘€2\nğ‘Œğ‘–\nğ‘˜ = Attention(ğ‘‹ğ‘–ğ‘Šğ‘„\nğ‘˜ ,ğ‘‹ğ‘–ğ‘Šğ¾\nğ‘˜ ,ğ‘‹ğ‘–ğ‘Šğ‘‰\nğ‘˜ ),ğ‘– = 1,2,...,ğ‘\n(3)\nwhere ğ‘Šğ‘„\nğ‘˜ ,ğ‘Šğ¾\nğ‘˜ ,ğ‘Šğ‘‰\nğ‘˜ âˆˆRğ¶Ã—ğ‘‘ğ‘˜ represent the projection matrices of\nthe queries, keys, and values for the ğ‘˜-th head, respectively.\nSimilar with [37, 47], we also choose the relative position encod-\ning in the attention module, so its calculation can be formulated\nas:\nAttention(ğ‘„,ğ¾,ğ‘‰ )= Softmax(ğ‘„ğ¾ğ‘‡\nâˆšï¸\nğ‘‘ğ‘˜\n+ğµ)ğ‘‰ (4)\nwhere ğµ is the relative position bias, whose values are taken\nfrom Ë†ğµ âˆˆR(2ğ‘€âˆ’1)Ã—(2ğ‘€âˆ’1)with learnable parameters.\nLocally Multiplicative Self-Attention. Although W-MSA re-\nduces the computational costs, the crude concatenation of nearby\nfeatures is lack of consideration. For the local partial areas, the\nnetwork should avoid learning dependencies from noisy pixels\nand give more attention to beneficial parts. So we propose the Lo-\ncally Multiplicative Self-Attention (L-MSA) to enhance the color\ninformation and give a better explanation of self-attention.\nM\nM\ndk\nO(M4Ã—dk) O((2Ã—2)2Ã—dkÃ—(M/2)Ã—(M/2))=O(M2Ã—dkÃ—4)\nW-MSA\nSelf attention Short-range weights\nL-MSA\ndkÃ—4\nM/2\nM/2\n>\nFigure 5: Illustration of costs between W-MSA and L-MSA.\nAs is shown in Fig. 5, each pixel-wise location represents single\ncolorâ€™s feature. For ISP pipeline, neighborhood pixels are used for\ndemosaicing to make up for the absence of the other color informa-\ntion of a pixel. Those adjacent homochromy pixels can also offer\nkey contexts for those noisy or deficient points. Based on this in-\nsight, we select non-overlapping 2 Ã—2 sub-windows and rearrange\neach feature map according to sub-windowsâ€™ relative positions.\nThis L-MSA is related to those2Ã—2 sub-windows. As is discussed\npreviously, we can divide ğ‘€2\n4 sub-windows for each window, so\nthe total number of sub-windows is ğ»ğ‘Š\n4 . For each sub-window\nin single window area, we compute its self-attention as what we\nillustrate before.\nğ‘‹ = {ğ‘‹1\nğ‘™ ,ğ‘‹2\nğ‘™ ,...,ğ‘‹ ğ¿\nğ‘™ },ğ¿ = ğ» Ã—ğ‘Š\n22\nğ‘ğ‘—\nğ‘˜ = Attention(ğ‘‹ğ‘—\nğ‘™ ğ‘ƒğ‘„\nğ‘˜ ,ğ‘‹ ğ‘—\nğ‘™ ğ‘ƒğ¾\nğ‘˜ ,ğ‘‹ ğ‘—\nğ‘™ ğ‘ƒğ‘‰\nğ‘˜ ),ğ‘— = 1,2,...,ğ¿\n(5)\nwhere ğ‘ƒğ‘„\nğ‘˜ ,ğ‘ƒğ¾\nğ‘˜ ,ğ‘ƒğ‘‰\nğ‘˜ âˆˆRğ¶Ã—ğ‘‘ğ‘˜ represent the projection matrices of the\nqueries, keys, and values for the ğ‘˜-th head. Besides, ğ‘‹\nğ‘€2\n4 (ğ‘–âˆ’1)+1\nğ‘™ ,\nğ‘‹\nğ‘€2\n4 (ğ‘–âˆ’1)+2\nğ‘™ , ..., ğ‘‹\nğ‘€2\n4 ğ‘–\nğ‘™ are divided sub-windows from ğ‘‹ğ‘–, where\nğ‘– = 1,2,...,ğ‘ .\nNote that the computed self-attention reflects the weights of\nevery color channel i.e., Red, Green, Green and Blue (RGGB) in\nits corresponding position. Hence, we view them as short-range\nweights and multiply them with our W-MSA in every sub-window.\nFor this part, we can represent it as:\nğ‘ğ‘–\nğ‘˜ = {ğ‘\nğ‘€2\n4 (ğ‘–âˆ’1)+1\nğ‘˜ ,ğ‘\nğ‘€2\n4 (ğ‘–âˆ’1)+2\nğ‘˜ ,...,ğ‘\nğ‘€2\n4 ğ‘–\nğ‘˜ },ğ‘– = 1,2,...,ğ‘\nğ‘Œğ‘–\nğ‘˜ = ğ‘Œğ‘–\nğ‘˜ Â·ğ‘ğ‘–\nğ‘˜\nË†ğ‘‹ğ‘˜ = {ğ‘Œ1\nğ‘˜,ğ‘Œ2\nğ‘˜,...,ğ‘Œ ğ‘\nğ‘˜ }\n(6)\nË†ğ‘‹ğ‘˜ is the output of the ğ‘˜-th head. Then the outputs for all heads\n{1; 2;...;ğ‘˜}are concatenated and then linearly projected to get the\nfinal result. In this way, we actually multiply W-MSA with L-MSA\nseparately according to their CFA locations (RGGB).\nBy this Locally Multiplicative Self-Attention (L-MSA), all feature\nmaps can be rearranged according to CFA-based structures, and\nfocus more on the valid pixel-wise information. It makes up for the\nlocal connections in W-MSA, so our transformer block can tackle\nwith severe noisy points. And the later experiments prove this\nJiaqi Ma, Shengyuan Yan, Lefei Zhang, Guoli Wang, and Qian Zhang\nconclusion. As is shown in Fig. 5, the computation cost of L-MSA\nshould be ğ‘‚(ğ‘€2 Ã—ğ‘‘ğ‘˜ Ã—4). More detailed discussions about the\ncomputational cost of ELMformer are in Subsection 3.4.\n3.4 Computational Complexity\nTo give a clear comparison at the same scale, ELMformer and\nUformer [53] are designed with same numbers of blocks and pro-\njection features. Here, we theoretically analyze the computational\ncomplexity of the Lm-Win Transformer Block of ELMformer and\nthe Le-Win Transformer Block of Uformer. For a batch size of single\nraw image 1 Ã—ğ»Ã—ğ‘Š, ELMformer firstly feeds it into the BFP mod-\nule and gets one ğ¶Ã—ğ»\n2 Ã—ğ‘Š\n2 tensor, while Uformer results in one\nğ¶Ã—ğ»Ã—ğ‘Š tensor. In the following Transformer blocks, our method\nonly processes half-size tensors with the same channel number\ncompared with Uformer. In regard to a single Transformer block\nunder the same window size ğ‘€, Lm-Win processes ğ»ğ‘Š\n4ğ‘€2 windows\nwith each costing ğ‘‚(ğ‘€4 Ã—ğ‘‘ğ‘˜ +ğ‘€2 Ã—ğ‘‘ğ‘˜ Ã—4), and Le-Win processes\nğ»ğ‘Š\nğ‘€2 windows with each costing ğ‘‚(ğ‘€4 Ã—ğ‘‘ğ‘˜). Hence, for a same\nraw image, Lm-Win costs ğ‘‚(ğ»ğ‘Šğ‘€2Ã—ğ‘‘ğ‘˜\n4 +ğ»ğ‘ŠÃ—ğ‘‘ğ‘˜\nğ‘€2 )while Le-Win\nneeds ğ‘‚(ğ»ğ‘Šğ‘€2 Ã—ğ‘‘ğ‘˜)for one raw image. Considering that both of\nthem set 8 as the window size, which is far smaller than normal size\nof unprocessed raw images (ğ‘¡â„ğ‘œğ‘¢ğ‘ ğ‘ğ‘›ğ‘‘ğ‘  Ã—ğ‘¡â„ğ‘œğ‘¢ğ‘ ğ‘ğ‘›ğ‘‘ğ‘  in resolution),\nELMformer costs nearly 4Ã—less computational complexity com-\npared with Uformer. As is shown in Fig. 1, when we account into all\nfactors of computation, ELFformer still remains at least one third\nFLOPs of Uformer, which means that ELMformer reaches higher\nresults with less FLOPs.\n3.5 Comparisons with Uformer\nOur ELMformer has a similar architecture to Uformer [53] which\nis a U-net structure with vision transformer blocks. However, the\nTransformer blocks in ELMformer are different from that of Uformer.\nThe Le-win Transformer block of Uformer consists of W-MSA\nmodule that conducts window-based self-attention and a forward-\nfeeding network, while our Lm-Win Transformer block contains a\nnovel L-MSA module designed especially for raw images. L-MSA\nconducts self-attention within subwindows in the non-overlapping\nshifted windows to produce self-attention weights of different color\nchannels in RAW images. Then our Lm-Win Transformer block\nmultiplies the output weights of L-MSA with the window-based\nself-attention outputs. The color channel self-attention weights of\nL-MSA serve as a reference for W-MSA and enhance the feature\nextraction ability of the Transformer blocks.\n4 EXPERIMENTS\n4.1 Datasets\nSmartphone Image Denoising Dataset (SIDD): SIDD dataset\n1 [1] consists of 320 training image pairs (medium set) and 1280\nimage pairs (benchmark) for evaluations. These images are all col-\nlected with five specific smartphone cameras. The optical sensors of\nsmartphones are of much smaller size compared with professional\ncameras, so the smartphone-produced images are more noisy but\nalso have high resolution.\n1https://www.eecs.yorku.ca/ kamel/sidd/dataset.php\nMethod PSNR SSIM\nr/r r/s r/r r/s\nEPLL [64] 40.73 25.19 0.935 0.842\nBM3D [15] 45.52 30.95 0.980 0.863\nKSVD [2] 43.26 27.41 0.969 0.832\nWNNM [20] 44.85 29.54 0.975 0.888\nFoE [46] 43.13 27.18 0.969 0.812\nTNRD [13] 42.77 26.99 0.945 0.744\nUPI* [5] 51.54 38.91 0.992 0.953\nUPI [5] 42.23 28.39 0.888 0.632\nCycleISP* [60] 51.75 39.24 0.993 0.955\nCycleISP [60] 47.98 35.02 0.950 0.846\nELMformer 51.94 39.50 0.993 0.957\nTable 1: The average metrics on the SIDD benchmark set.\n*: results in italic are from the official pretrained model\ntrained by additional data.\nMethod PSNR SSIM\nr/r r/s r/r r/s\nEPLL [64] 40.73 25.19 0.9350 0.8420\nBM3D [15] 46.64 37.78 0.9724 0.9308\nKSVD [2] 45.54 36.59 0.9676 0.9162\nWNNM [20] 46.30 37.56 0.9707 0.9313\nFoE [46] 45.78 35.99 0.9666 0.9042\nTNRD [13] 44.97 35.57 0.9624 0.8913\nUPI* [5] 48.89 40.17 0.9824 0.9623\nUPI [5] 48.44 39.47 0.9802 0.9508\nCycleISP* [60] 49.13 40.50 0.9830 0.9655\nCycleISP [60] 48.75 39.84 0.9812 0.9541\nELMformer 48.84 40.06 0.9816 0.9560\nTable 2: The average metrics on the DND dataset. *: results\nin italic are from the official pretrained model trained by\nadditional data.\nDarmstadt Noise Dataset (DND): DND dataset 2 [42] contains\n50 noisy-clean image pairs from four consumer level cameras. Since\nthe resolution of each image is very high, the dataset is cropped\ninto patches of 512 Ã—512 and finally yields 1000 patches totally.\nDeblur-RAW Dataset Deblur-RAW Dataset3 [31] contains 10252\nRAW image pairs for raw deblurring, which contain blurred and\ncorresponding deblurred ones. The whole dataset is split into 8752\nand 1500 pairs, which are training and testing sets.\n4.2 Experimental Settings\nFollowing the common training strategy of Transformer [49], we\nuse the AdamW optimizer [38] with momentum terms of (0:9; 0:999)\nand a weight decay of 0.02. We randomly augment the training\nsamples using the horizontal flipping and rotating the images by\n90 degrees, 180 degrees, or 270 degrees. We set the window size\n2https://noise.visinf.tu-darmstadt.de/downloads/\n3https://github.com/bob831009/raw_image_deblurring\nELMformer: Efficient Raw Image Restoration with a Locally Multiplicative Transformer\nFigure 6: Real noise removal results of ELMformer on SIDD benchmark set. Raw images are fed into an official ISP pipeline\nfor better visualization. Details can be seen when zoomed in. Up: noisy images; Down: denoised results.\nto 8 Ã—8 in Lm-Win Transformer blocks. For the convenience of\ndownsampling, we stress that the window size in the bottleneck\nstage is set to 4 Ã—4. The projection dimension number is set to 32\nfor the BFP module. The cosine decay strategy is also applied to\ndecrease the learning rate to1ğ‘’âˆ’6 with the initial learning rate4ğ‘’âˆ’4.\nThe single â„“1 loss is chosen as the loss function and we show the\nablation study of loss functions in Appendix.\nWe apply PSNR and SSIM [ 52] to evaluate the performance.\nThese metrics are calculated in the Raw/Raw color space firstly.\nThen an ISP pipeline generates their corresponding sRGB images for\nevaluating in the Raw/sRGB color space. In the following sections\nand tables, â€™r/râ€™ and â€™r/sâ€™ are short for â€™Raw/Rawâ€™ and â€™Raw/sRGBâ€™.\nFor SIDD and DND datasets, the whole evaluation processes are\nconducted online and only average PSNR and SSIM along with\nvisualization of sample parts are the feedback. For Deblur-RAW\ndataset, we follow the default settings: conducting the training\nprocess on the training set and evaluating our model on the testing\nset.\n4.3 Compared Methods\nFor a complete and fair comparison, we collect several denoising\nmethods, which includes traditional blind ones, CNN-based ones\nand some simulated ISP pipelines. Besides, we collect several SOTA\nraw deblurring methods.\nOf those denoising methods, EPLL [64], BM3D [15], KSVD [2],\nWNNM [20] and FoE [46] are optimization based methods for de-\nnoising. TNRD [ 13], UPI [ 5] and CycleISP [ 60] are CNN-based\nmethods. Besides, UPI [ 5] and CycleISP [ 60] also belong to ISP\nbased pipelines which simulate the process of generating sRGB and\nraw images. Note that UPI [5] and CycleISP [60] need large-scale\nsRGB images to train the ISP pipeline, so we mark them with âˆ—\nin Tab. 1 and Tab. 2. For other methods without âˆ—, we follow the\nauthorsâ€™ instructions and train them only with SIDD medium set\nfor a fair comparison.\nOf those deblurring methods, DMCNN [41], DeblurGAN [27],\nSRN [48], UDS [39], SDNet4 [61], DMPHN_rgb [61] are designed\nfor sRGB images, so we only collect their resuls on sRGB color\nspace. For DMPHN_raw [61] and RID [31], we evaluate them both\non raw and sRGB color space.\nMethod PSNR SSIM\nr/r r/s r/r r/s\nDMCNN [41] - 27.85 - 0.880\nDeblurGAN [27] - 26.58 - 0.852\nSRN [48] - 28.69 - 0.925\nUDS [39] - 24.60 - 0.811\nSDNet4 [61] - 29.24 - 0.920\nDMPHN_rgb [61] - 28.73 - 0.907\nDMPHN_raw [61] 41.68 28.98 0.986 0.906\nRID [31] 42.71 29.80 0.989 0.929\nELMformer 43.45 31.60 0.995 0.913\nTable 3: The average metrics on the Deblur-RAW testing set.\n4.4 Quantitative Results\nHere, we first compare our approach with the aforementioned SO-\nTAs on both denoising benchmarks: SIDD and DND, and then eval-\nuate on one deblurring benchmark: Deblur-RAW. Most results are\nfrom the official leaderboard on their websites or corresponding pa-\npers. Only some missing results are tested with authorsâ€™ pretrained\nmodels and submitted to the online server for evaluation. For all\nthree benchmarks, we record the PSNR and SSIM on Raw/Raw\nand Raw/sRGB space. Considering that only SIDD and Deblur-Raw\nhave training sets, we evaluate DND by the model trained on SIDD\nmedium dataset with no more external data.\nFrom Tab. 1, our ELMformer reaches the highest scores on all the\nmetrics. ELMformer reaches 51.94 dB on PSNR and even surpasses\nUPIâˆ—[5] and CycleISPâˆ—[60] which are trained on huge additional\ndata. The ELMformer has at least 3.96 dB and 4.48 dB improvements\nover CycleISP on raw and sRGB color space, respectively. Hence,\nour ELMformer can outperform other methods on raw denoising\nfrom smartphones.\nThe results illustrated in Tab. 2 differ. ELMformer is superior\nto most of methods except UPIâˆ—and CycleISPâˆ—. Our ELMformer\nonly has 0.10 dB and 0.22 dB promotion over CycleISP on raw and\nsRGB color space, respectively. Notice that UPIâˆ—and CycleISPâˆ—\nare trained on other generated large-scale data, while ELMformer\nonly needs several hundred pairs of raw data, we suppose that\nJiaqi Ma, Shengyuan Yan, Lefei Zhang, Guoli Wang, and Qian Zhang\nModules PSNR SSIM GFLOPsW-MSA BFP L-MSA r/r r/s r/r r/s\nâœ“ 48.81 40.04 0.9817 0.9557 10.24\nâœ“ âœ“ 48.79 40.02 0.9816 0.9555 2.84\nâœ“ âœ“ 48.85 40.06 0.9817 0.9556 13.05\nâœ“ âœ“ âœ“ 48.84 40.06 0.9816 0.9560 3.55\nTable 4: Ablation study of modules on the DND dataset.\nthose generated raw images can further extend the generalization\nability. ELMformer still has a superiority over other methods at the\nsame scale of training data. Besides, from Tab. 1 and Tab. 2, we can\nfind that ELMformer which is designed for raw restoration has an\nedge on raw color space compared with other methods and this\nadvantage is weaken on sRGB color space.\nWe also evaluate on the raw deblurring task in Tab. 3. It is obvious\nthat those SOTAs trained on the sRGB color space have a gap over\nmethods designed for raw deblurring. Even DMPHN_raw [61] has\nminor superiority over DMPHN_rgb [61]. For three raw deblurring\nmethods, ELMformer still exceeds others in all metrics except SSIM\nin sRGB color space but outperforms RID a lot in PSNR.\n4.5 Qualitative Results\nThen we show some representative results from three datasets\nfor visual comparison. The raw images are fed into existing ISP\npipelines and illustrated in sRGB space.\nFrom Fig. 6, our method removes noisy points in SIDD and keeps\nthe details successfully. It indicates that ELMformer can deal with\nreal noises from mobile devices perfectly. Due to the space limit,\nwe also illustrate the results of DND and Deblur-RAW datasets in\nAppendix. For DND, ELMformer can still effectively remove real\nnoises and maintain the structure of locality even without training\nset in DND imagesâ€™ domain. For deblurring, our ELMformer has\nbetter visual results in deblurring. It further proves that methods\ntrained on raw images supply color and spatial information and\nlead to clear fine-grained details.\n4.6 Ablation Studies\nTo evaluate the effectiveness of individual parts in our ELMformer,\nwe perform ablation study on denoising tasks and show the average\nPSNR and SSIM. As is shown in Table. 4, we take W-MSA [53] as\nbasic blocks of our baseline and compare it along with BFP and\nL-MSA in the same U-shaped architecture.\nComparing the first and second rows, it is confirmed that W-MSA\ncan perform better with more computational cost as we expect. It\nreflects that although the FLOPs is nearly quartered, BFP module\nalmost make up the drop in performance by fusion of color archi-\ntecture and spatial information. Then when it comes to the first\nand third rows, we can find that L-MSA improves performance by\nenhancing short-range dependencies between local neighbor pixels\nwhile adding little computational cost. At last, the fourth row rep-\nresents our proposed ELMformer. Compared with other three rows,\nELMformer keeps a balance between performance and computa-\ntional cost. It outperforms the baseline in all metrics while keeps\na. SID b. ELD c. HDR+ d.Uniform  SIDD e. AWGN SIDD\nFigure 7: The denoising illustration of ELMformer on SID,\nELD, HDR+, and two synthetic noisy SIDD datasets.\nonly 1/3 FLOPs. By integrating BFP and L-MSA modules, ELM-\nformer achieves the better results and loweer cost. More ablation\nstudies are in Appendix.\n4.7 Generalization Test\nHere, we firstly evaluate our denoising model on some extreme\nconditions to prove its powerful generalization ability. We select\nSID [10], ELD [ 54] and HDR+ [ 22] datasets to separately show\nthe denoising ability in short exposed, extremely dark and high-\ndynamic range conditions. Then we also add some synthetic noises\nto show its modeling ability. To be specific, we add AWGN and\nUniform noises to the clean raw data of SIDD [1] as the noisy ones,\nand directly apply our pretrained model of SIDD medium set to\ndenoise.\nIn Fig. 7, we show the noisy and restored images which are pro-\ncessed by a simple ISP pipeline 4. Compared with the noisy images,\nour method can produce noise-free outputs of high quality in sev-\neral extreme conditions and maintain local details simultaneously.\nAlso, we denoise some noisy raw images taken by mobile devices,\ni.e., OPPO and XiaoMi, and the visualized results are in Appendix.\n5 CONCLUSIONS\nIn this paper, we propose an efficient locally multiplicative Trans-\nformer called ELMformer for raw restoration. In contrast to exist-\ning CNN-based structures, our model build upon the BFP module\nand Lm-Win Transformer blocks. Two core components: BFP and\nL-MSA, are designed for bi-directional (color and spatial) feature fu-\nsion in the projection stage and efficient self-attention with stronger\nshort-range connections. Extensive experiments demonstrate that\nELMformer achieves competitive results on the raw denoising and\ndeblurring tasks with less computation cost. More qualitative results\nfrom several datasets reveal that our pretrained model maintains a\nstrong generalization ability.\nACKNOWLEDGMENTS\nThis work was supported by the National Natural Science Foun-\ndation of China under Grant 62122060, the Special Fund of Hubei\nLuojia Laboratory under Grant 220100014, and the Fundamental Re-\nsearch Funds for the Central Universities under Grant 2042021kf0196.\nThe numerical calculations in this work had been supported by the\n4https://github.com/AbdoKamel/simple-camera-pipeline\nELMformer: Efficient Raw Image Restoration with a Locally Multiplicative Transformer\nsupercomputing system in the Supercomputing Center of Wuhan\nUniversity.\nREFERENCES\n[1] Abdelrahman Abdelhamed, Stephen Lin, and Michael S. Brown. 2018. A High-\nQuality Denoising Dataset for Smartphone Cameras. In CVPR. 1692â€“1700.\n[2] Michal Aharon, Michael Elad, and Alfred M. Bruckstein. 2006. K-SVD: An Algo-\nrithm for Designing Overcomplete Dictionaries for Sparse Representation. TSP\n54, 11 (2006), 4311â€“4322.\n[3] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer Normaliza-\ntion. CoRR abs/1607.06450 (2016).\n[4] Long Bao, Zengli Yang, Shuangquan Wang, Dongwoon Bai, and Jungwon Lee.\n2020. Real Image Denoising Based on Multi-Scale Residual Dense Block and\nCascaded U-Net with Block-Connection. In CVPR. 1823â€“1831.\n[5] Tim Brooks, Ben Mildenhall, Tianfan Xue, Jiawen Chen, Dillon Sharlet, and\nJonathan T. Barron. 2019. Unprocessing Images for Learned Raw Denoising. In\nCVPR. 11036â€“11045.\n[6] Chenjie Cao, Yuxin Hong, Xiang Li, Chengrong Wang, Chengming Xu, Yanwei\nFu, and Xiangyang Xue. 2021. The Image Local Autoregressive Transformer. In\nNeurIPS. 18433â€“18445.\n[7] Yue Cao, Xiaohe Wu, Shuran Qi, Xiao Liu, Zhongqin Wu, and Wangmeng Zuo.\n2021. Pseudo-ISP: Learning Pseudo In-camera Signal Processing Pipeline from A\nColor Image Denoiser. CoRR abs/2103.10234 (2021).\n[8] Meng Chang, Qi Li, Huajun Feng, and Zhihai Xu. 2020. Spatial-Adaptive Network\nfor Single Image Denoising. In ECCV, Vol. 12375. 171â€“187.\n[9] Pierre Charbonnier, Laure Blanc-FÃ©raud, Gilles Aubert, and Michel Barlaud. 1994.\nTwo Deterministic Half-Quadratic Regularization Algorithms for Computed\nImaging. In ICIP. 168â€“172.\n[10] Chen Chen, Qifeng Chen, Jia Xu, and Vladlen Koltun. 2018. Learning to See in\nthe Dark. In CVPR. 3291â€“3300.\n[11] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua\nLiu, Siwei Ma, Chunjing Xu, Chao Xu, and Wen Gao. 2021. Pre-Trained Image\nProcessing Transformer. In CVPR. 12299â€“12310.\n[12] Liang Chen, Jiawei Zhang, Jinshan Pan, Songnan Lin, Faming Fang, and Jimmy S.\nRen. 2021. Learning a Non-Blind Deblurring Network for Night Blurry Images.\nIn CVPR. 10542â€“10550.\n[13] Yunjin Chen, Wei Yu, and Thomas Pock. 2015. On Learning Optimized Reaction\nDiffusion Processes for Effective Image restoration. In CVPR. 5261â€“5269.\n[14] Shen Cheng, Yuzhi Wang, Haibin Huang, Donghao Liu, Haoqiang Fan, and\nShuaicheng Liu. 2021. NBNet: Noise Basis Learning for Image Denoising With\nSubspace Projection. In CVPR. 4896â€“4906.\n[15] Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and Karen O. Egiazarian.\n2007. Image Denoising by Sparse 3-D Transform-Domain Collaborative Filtering.\nTIP 16, 8 (2007), 2080â€“2095.\n[16] Senyou Deng, Wenqi Ren, Yanyang Yan, Tao Wang, Fenglong Song, and Xi-\naochun Cao. 2021. Multi-Scale Separable Network for Ultra-High-Definition\nVideo Deblurring. In ICCV. 14010â€“14019.\n[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\naohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\nHeigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is\nWorth 16x16 Words: Transformers for Image Recognition at Scale. InICLR.\n[18] Patrick Esser, Robin Rombach, and BjÃ¶rn Ommer. 2021. Taming Transformers\nfor High-Resolution Image Synthesis. In CVPR. 12873â€“12883.\n[19] Gabriela Ghimpeteanu, Thomas Batard, Tamara Seybold, and Marcelo BertalmÃ­o.\n2016. Local Denoising Applied to RAW Images May Outperform Non-local\nPatch-based Methods Applied to the Camera Output. In DPMI. 1â€“8.\n[20] Shuhang Gu, Lei Zhang, Wangmeng Zuo, and Xiangchu Feng. 2014. Weighted\nNuclear Norm Minimization with Application to Image Denoising. In CVPR.\n2862â€“2869.\n[21] Javier Gurrola-Ramos, Oscar S. Dalmau, and Teresa E. AlarcÃ³n. 2021. A Residual\nDense U-Net Neural Network for Image Denoising. IEEE Access9 (2021), 31742â€“\n31754.\n[22] Samuel W. Hasinoff, Dillon Sharlet, Ryan Geiss, Andrew Adams, Jonathan T.\nBarron, Florian Kainz, Jiawen Chen, and Marc Levoy. 2016. Burst Photography\nfor High Dynamic Range and Low-light Imaging on Mobile Cameras.ACM Trans.\nGraph. 35, 6 (2016), 192:1â€“192:12.\n[23] Xiaowan Hu, Ruijun Ma, Zhihong Liu, Yuanhao Cai, Xiaole Zhao, Yulun Zhang,\nand Haoqian Wang. 2021. Pseudo 3D Auto-Correlation Network for Real Image\nDenoising. In CVPR. 16175â€“16184.\n[24] Tao Huang, Songjiang Li, Xu Jia, Huchuan Lu, and Jianzhuang Liu. 2021. Neigh-\nbor2Neighbor: Self-Supervised Denoising From Single Noisy Images. In CVPR.\n14781â€“14790.\n[25] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. 2017. Image-to-\nImage Translation with Conditional Adversarial Networks. In CVPR. 5967â€“5976.\n[26] Alexander Krull, Tim-Oliver Buchholz, and Florian Jug. 2019. Noise2Void -\nLearning Denoising From Single Noisy Images. In CVPR. 2129â€“2137.\n[27] Orest Kupyn, Volodymyr Budzan, Mykola Mykhailych, Dmytro Mishkin, and Jiri\nMatas. 2018. DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial\nNetworks. In CVPR. 8183â€“8192.\n[28] Samuli Laine, Tero Karras, Jaakko Lehtinen, and Timo Aila. 2019. High-Quality\nSelf-Supervised Deep Image Denoising. In NeurIPS. 6968â€“6978.\n[29] Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras,\nMiika Aittala, and Timo Aila. 2018. Noise2Noise: Learning Image Restoration\nwithout Clean Data. In ICML, Vol. 80. 2971â€“2980.\n[30] Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc Van Gool. 2021.\nLocalViT: Bringing Locality to Vision Transformers.CoRR abs/2104.05707 (2021).\n[31] Chih-Hung Liang, Yu-An Chen, Yueh-Cheng Liu, and Winston H. Hsu. 2022.\nRaw Image Deblurring. TMM 24 (2022), 61â€“72.\n[32] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu\nTimofte. 2021. SwinIR: Image Restoration Using Swin Transformer. In ICCVW.\n1833â€“1844.\n[33] Zhetong Liang, Shi Guo, Hong Gu, Huaqi Zhang, and Lei Zhang. 2020. A De-\ncoupled Learning Scheme for Real-World Burst Denoising from Raw Images. In\nECCV, Vol. 12370. 150â€“166.\n[34] Tran Duy Linh, Son Minh Nguyen, and Masayuki Arai. 2020. GAN-Based Noise\nModel for Denoising Real Images. In ACCV, Vol. 12625. 560â€“572.\n[35] Jiaming Liu, Chihao Wu, Yuzhi Wang, Qin Xu, Yuqian Zhou, Haibin Huang,\nChuan Wang, Shaofan Cai, Yifan Ding, Haoqiang Fan, and Jue Wang. 2019. Learn-\ning Raw Image Denoising With Bayer Pattern Unification and Bayer Preserving\nAugmentation. In CVPR. 2070â€“2077.\n[36] Yang Liu, Zhenyue Qin, Saeed Anwar, Pan Ji, Dongwoo Kim, Sabrina B. Caldwell,\nand Tom Gedeon. 2021. Invertible Denoising Network: A Light Solution for Real\nNoise Removal. In CVPR. 13365â€“13374.\n[37] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin,\nand Baining Guo. 2021. Swin Transformer: Hierarchical Vision Transformer\nusing Shifted Windows. In ICCV. 9992â€“10002.\n[38] Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization.\nIn ICLR.\n[39] Boyu Lu, Jun-Cheng Chen, and Rama Chellappa. 2019. Unsupervised Domain-\nSpecific Deblurring via Disentangled Representations. In CVPR. 10225â€“10234.\n[40] Yucheng Lu and Seung-Won Jung. 2022. Progressive Joint Low-Light Enhance-\nment and Noise Removal for Raw Images. TIP 31 (2022), 2390â€“2404.\n[41] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. 2017. Deep Multi-scale\nConvolutional Neural Network for Dynamic Scene Deblurring. In CVPR. 257â€“\n265.\n[42] Tobias PlÃ¶tz and Stefan Roth. 2017. Benchmarking Denoising Algorithms with\nReal Photographs. In CVPR. 2750â€“2759.\n[43] Tobias PlÃ¶tz and Stefan Roth. 2018. Neural Nearest Neighbors Networks. In\nNeurIPS. 1095â€“1106.\n[44] Chao Ren, Xiaohai He, Chuncheng Wang, and Zhibo Zhao. 2021. Adaptive\nConsistency Prior Based Deep Network for Image Denoising. In CVPR. 8596â€“\n8606.\n[45] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-Net: Convolutional\nNetworks for Biomedical Image Segmentation. In MICCAI, Vol. 9351. 234â€“241.\n[46] Stefan Roth and Michael J. Black. 2009. Fields of Experts. IJCV 82, 2 (2009),\n205â€“229.\n[47] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-Attention with\nRelative Position Representations. In NAACL-HLT. 464â€“468.\n[48] Xin Tao, Hongyun Gao, Xiaoyong Shen, Jue Wang, and Jiaya Jia. 2018. Scale-\nRecurrent Network for Deep Image Deblurring. In CVPR. 8174â€“8182.\n[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In NeurIPS. 5998â€“6008.\n[50] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong\nLu, Ping Luo, and Ling Shao. 2021. Pyramid Vision Transformer: A Versatile\nBackbone for Dense Prediction without Convolutions. In ICCV. 548â€“558.\n[51] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong\nLu, Ping Luo, and Ling Shao. 2022. Pvtv2: Improved baselines with pyramid\nvision transformer. Computational Visual Media8, 3 (2022), 1â€“10.\n[52] Zhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P. Simoncelli. 2004. Image\nQuality Assessment: From Error Visibility to Structural Similarity. TIP 13, 4\n(2004), 600â€“612.\n[53] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu,\nand Houqiang Li. 2022. Uformer: A General U-Shaped Transformer for Image\nRestoration. In CVPR. 17683â€“17693.\n[54] Kaixuan Wei, Ying Fu, Jiaolong Yang, and Hua Huang. 2020. A Physics-Based\nNoise Formation Model for Extreme Low-Light Raw Denoising. In CVPR. 2755â€“\n2764.\n[55] Fangfang Wu, Tao Huang, Weisheng Dong, Guangming Shi, Zhonglong Zheng,\nand Xin Li. 2021. Toward Blind Joint Demosaicing and Denoising of Raw Color\nFilter Array Data. Neurocomputing 453 (2021), 369â€“382.\n[56] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and\nLei Zhang. 2021. CvT: Introducing Convolutions to Vision Transformers. InICCV.\n22â€“31.\nJiaqi Ma, Shengyuan Yan, Lefei Zhang, Guoli Wang, and Qian Zhang\n[57] Xiaohe Wu, Ming Liu, Yue Cao, Dongwei Ren, and Wangmeng Zuo. 2020. Un-\npaired Learning of Deep Image Denoising. In ECCV, Vol. 12349. 352â€“368.\n[58] Wenzhu Xing and Karen O. Egiazarian. 2021. End-to-End Learning for Joint\nImage Demosaicing, Denoising and Super-Resolution. In CVPR. 3507â€“3516.\n[59] Shuo-Diao Yang, Hung-Ting Su, Winston H. Hsu, and Wen-Chin Chen. 2019.\nDECCNet: Depth Enhanced Crowd Counting. In ICCVW. 4521â€“4530.\n[60] Syed Waqas Zamir, Aditya Arora, Salman H. Khan, Munawar Hayat, Fahad Shah-\nbaz Khan, Ming-Hsuan Yang, and Ling Shao. 2020. CycleISP: Real Image Restora-\ntion via Improved Data Synthesis. In CVPR. 2693â€“2702.\n[61] Hongguang Zhang, Yuchao Dai, Hongdong Li, and Piotr Koniusz. 2019. Deep\nStacked Hierarchical Multi-Patch Network for Image Deblurring. In CVPR. 5978â€“\n5986.\n[62] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. 2017. Be-\nyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising.\nTIP 26, 7 (2017), 3142â€“3155.\n[63] Hongyi Zheng, Hongwei Yong, and Lei Zhang. 2021. Deep Convolutional Dictio-\nnary Learning for Image Denoising. In CVPR. 630â€“641.\n[64] Daniel Zoran and Yair Weiss. 2011. From Learning Models of Natural Image\nPatches to Whole Image Restoration. In ICCV. 479â€“486.\nELMformer: Efficient Raw Image Restoration with a Locally Multiplicative Transformer\nA ABLATION STUDIES\nHere, we further examine the effectiveness of the channel dimension\nğ¶ in Tab. 5. ğ¶ denotes the channel dimension of projection module.\nIt is noteworthy that the computational cost will increase a\nlot with the increase of channel number, while the performance\npromotion is not significant. It reflects that 32 is a trade-off for the\nBFP module which can bring effective performance gains while\nmaintaining low FLOPs.\nğ¶ PSNR SSIM GFLOPsr/r r/s r/r r/s\n16 51.84 39.37 0.993 0.956 0.91\n32 51.94 39.50 0.993 0.957 3.55\n48 52.00 39.56 0.993 0.957 7.91\n64 52.03 39.60 0.993 0.957 14.01\nTable 5: Ablation studies of projection dimension on SIDD\nbenchmark.\nTo choose the optimal loss function for our ELMformer, we\nperform ablation study on SIDD benchmark. As is shown in Table.\n6, we apply â„“1, â„“2 and ğ¶â„ğ‘ğ‘Ÿğ‘ğ‘œğ‘›ğ‘›ğ‘–ğ‘’ğ‘Ÿ [9] loss as alternative losses to\nsupervise the training satge, and show the average PSNR and SSIM.\nHere, ğ¶â„ğ‘ğ‘Ÿ is short for the ğ¶â„ğ‘ğ‘Ÿğ‘ğ‘œğ‘›ğ‘›ğ‘–ğ‘’ğ‘Ÿ loss.\nIt is clear that â„“2 performs worst among those loss functions. It\nreflects that the errors in restoration tasks should not be enlarged.\nComparing the first and third rows,â„“1 loss shows a slight promotion\nthan ğ¶â„ğ‘ğ‘Ÿğ‘ğ‘œğ‘›ğ‘›ğ‘–ğ‘’ğ‘Ÿ loss in PSNR. Hence, we finally choose â„“1 loss to\nsupervise the training stage.\nğ¿ğ‘œğ‘ ğ‘ ğ‘’ğ‘  PSNR SSIM\nr/r r/s r/r r/s\nâ„“1 51.94 39.50 0.993 0.957\nâ„“2 51.64 39.05 0.993 0.954\nğ¶â„ğ‘ğ‘Ÿ 51.93 39.47 0.993 0.957\nTable 6: Ablation studies of losses on SIDD benchmark.\nB GENERALIZATION TEST IN EXTREME\nCONDITIONS\nIn this section, we evaluate our denoising model on some extreme\nconditions to prove its powerful generalization ability. We collect\nsome noisy raw images taken by mobile devices, i.e., Vivo and\nXiaoMi. Then we apply ELMformer to denoise those images and\nvisualize them.\nHere, we also try our model on other raw images captured by\nmobile devices from other brands, such as Vivo and XiaoMi. Notice\nthat both SIDD and DND do not contain data from Vivo and XiaoMi,\nand sensors result in different domains of raw images. From Fig. 8,\nwe observe that noisy points can be reduced well even without any\ntraining data from this domain.\nC MORE VISUAL RESULTS\nHere, we illustrate more denoising and deblurring visual results\nof our methods and SOTAs for comparison. In Fig. 9, those raw\nimages are processed by official ISP pipelines online, and we collect\ntheir feedback in RGB images. For Fig. 10, we also apply a simple\nISP to raw images. To give an intuitive impression, we try our best\nto maintain the originality of images with fine-grained parts.\nFor denoising in Fig. 9, ELMformer can still effectively remove\nreal noises and maintain the structure of locality even without\ntraining set in DND imagesâ€™ domain. It indicates that ELMformer\ncan deal with real noises from mobile devices perfectly.\nFor deblurring in Fig. 10, ELMformer has better visual results\nthan other methods. It further proves that methods trained on raw\nimages supply color and spatial information and lead to clear fine-\ngrained details.\n(a) Vivo (b) XiaoMi\nFigure 8: Real noise removal results of ELMformer on Vivo\nand XiaoMi. All raw images are fed into an existing ISP\npipeline for better visualization.\nJiaqi Ma, Shengyuan Yan, Lefei Zhang, Guoli Wang, and Qian Zhang\n(c) Cycle-ISP\n (d) UPI\n (e) ELMformer\n(b) TNRD(a) BM3D\nFigure 9: Real noise removal results on DND dataset. Raw images are fed into an existing ISP pipeline for better visualization.\n(a) Input (b) DMCNN (c) SRN (d) SDNet4 (e) RID (f) ELMformer\nFigure 10: Deblur results on Deblur-Raw dataset. Raw images are fed into an existing ISP pipeline for better visualization.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7383018136024475
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5466088652610779
    },
    {
      "name": "Image restoration",
      "score": 0.4790560007095337
    },
    {
      "name": "Computer vision",
      "score": 0.47765052318573
    },
    {
      "name": "Deblurring",
      "score": 0.4652228057384491
    },
    {
      "name": "Image (mathematics)",
      "score": 0.3298024535179138
    },
    {
      "name": "Image processing",
      "score": 0.21895286440849304
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I37461747",
      "name": "Wuhan University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4401726824",
      "name": "Horizon Robotics (China)",
      "country": null
    }
  ],
  "cited_by": 8
}