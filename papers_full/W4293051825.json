{
  "title": "A more effective CT synthesizer using transformers for cone-beam CT-guided adaptive radiotherapy",
  "url": "https://openalex.org/W4293051825",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5100664512",
      "name": "Xinyuan Chen",
      "affiliations": [
        null,
        "Chinese Academy of Medical Sciences & Peking Union Medical College"
      ]
    },
    {
      "id": "https://openalex.org/A5100751382",
      "name": "Yuxiang Liu",
      "affiliations": [
        "Chinese Academy of Medical Sciences & Peking Union Medical College",
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A5115594383",
      "name": "Bining Yang",
      "affiliations": [
        "Chinese Academy of Medical Sciences & Peking Union Medical College"
      ]
    },
    {
      "id": "https://openalex.org/A5103230277",
      "name": "Ji Zhu",
      "affiliations": [
        "Chinese Academy of Medical Sciences & Peking Union Medical College"
      ]
    },
    {
      "id": "https://openalex.org/A5011428628",
      "name": "Siqi Yuan",
      "affiliations": [
        "Chinese Academy of Medical Sciences & Peking Union Medical College"
      ]
    },
    {
      "id": "https://openalex.org/A5021344472",
      "name": "Xuejie Xie",
      "affiliations": [
        "Chinese Academy of Medical Sciences & Peking Union Medical College"
      ]
    },
    {
      "id": "https://openalex.org/A5101740564",
      "name": "Yue‐Ping Liu",
      "affiliations": [
        "Chinese Academy of Medical Sciences & Peking Union Medical College"
      ]
    },
    {
      "id": "https://openalex.org/A5019830361",
      "name": "Jianrong Dai",
      "affiliations": [
        "Chinese Academy of Medical Sciences & Peking Union Medical College"
      ]
    },
    {
      "id": "https://openalex.org/A5114182672",
      "name": "Kuo Men",
      "affiliations": [
        "Chinese Academy of Medical Sciences & Peking Union Medical College"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1967172105",
    "https://openalex.org/W2608941499",
    "https://openalex.org/W2981674662",
    "https://openalex.org/W2050624834",
    "https://openalex.org/W2072021306",
    "https://openalex.org/W2003327790",
    "https://openalex.org/W2149489589",
    "https://openalex.org/W1993253771",
    "https://openalex.org/W2040189727",
    "https://openalex.org/W2944050657",
    "https://openalex.org/W2947819416",
    "https://openalex.org/W3201563194",
    "https://openalex.org/W3037946733",
    "https://openalex.org/W2802363474",
    "https://openalex.org/W2995645407",
    "https://openalex.org/W2957883913",
    "https://openalex.org/W3108923139",
    "https://openalex.org/W4221114352",
    "https://openalex.org/W2962793481",
    "https://openalex.org/W2898727287",
    "https://openalex.org/W3010126053",
    "https://openalex.org/W2995983474",
    "https://openalex.org/W3162062956",
    "https://openalex.org/W3092557781",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3210510019",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W4200033088",
    "https://openalex.org/W3184007122",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6796298902",
    "https://openalex.org/W3209827189",
    "https://openalex.org/W3120844942",
    "https://openalex.org/W2995095597",
    "https://openalex.org/W2806986802",
    "https://openalex.org/W4224305736",
    "https://openalex.org/W2980506420",
    "https://openalex.org/W2912083425",
    "https://openalex.org/W4312812783",
    "https://openalex.org/W3106045672",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3103316509",
    "https://openalex.org/W4225243441"
  ],
  "abstract": "Purpose The challenge of cone-beam computed tomography (CBCT) is its low image quality, which limits its application for adaptive radiotherapy (ART). Despite recent substantial improvement in CBCT imaging using the deep learning method, the image quality still needs to be improved for effective ART application. Spurred by the advantages of transformers, which employs multi-head attention mechanisms to capture long-range contextual relations between image pixels, we proposed a novel transformer-based network (called TransCBCT) to generate synthetic CT (sCT) from CBCT. This study aimed to further improve the accuracy and efficiency of ART. Materials and methods In this study, 91 patients diagnosed with prostate cancer were enrolled. We constructed a transformer-based hierarchical encoder–decoder structure with skip connection, called TransCBCT. The network also employed several convolutional layers to capture local context. The proposed TransCBCT was trained and validated on 6,144 paired CBCT/deformed CT images from 76 patients and tested on 1,026 paired images from 15 patients. The performance of the proposed TransCBCT was compared with a widely recognized style transferring deep learning method, the cycle-consistent adversarial network (CycleGAN). We evaluated the image quality and clinical value (application in auto-segmentation and dose calculation) for ART need. Results TransCBCT had superior performance in generating sCT from CBCT. The mean absolute error of TransCBCT was 28.8 ± 16.7 HU, compared to 66.5 ± 13.2 for raw CBCT, and 34.3 ± 17.3 for CycleGAN. It can preserve the structure of raw CBCT and reduce artifacts. When applied in auto-segmentation, the Dice similarity coefficients of bladder and rectum between auto-segmentation and oncologist manual contours were 0.92 and 0.84 for TransCBCT, respectively, compared to 0.90 and 0.83 for CycleGAN. When applied in dose calculation, the gamma passing rate (1%/1 mm criterion) was 97.5% ± 1.1% for TransCBCT, compared to 96.9% ± 1.8% for CycleGAN. Conclusions The proposed TransCBCT can effectively generate sCT for CBCT. It has the potential to improve radiotherapy accuracy.",
  "full_text": "A more effective CT synthesizer\nusing transformers for\ncone-beam CT-guided\nadaptive radiotherapy\nXinyuan Chen1,2, YuxiangLiu1,3, BiningYang1,J iZhu1,\nSiqi Yuan1, XuejieXie1, YuepingLiu1, JianrongDai1\nand KuoMen1*\n1National Cancer Center/National Clinical Research Center for Cancer/Cancer Hospital, Chinese\nAcademy of Medical Sciences and Peking Union Medical College, Beijing, China,2National Cancer\nCenter/National Clinical Research Center for Cancer/Hebei Cancer Hospital, Chinese Academy of\nMedical Sciences, Langfang, China,3School of Physics and Technology, Wuhan University,\nWuhan, China\nPurpose: The challenge of cone-beam computed tomography (CBCT) is its\nlow image quality, which limits its application for adaptive radiotherapy (ART).\nDespite recent substantial improvement in CBCT imaging using the deep\nlearning method, the image quality still needs to be improved for effective\nART application. Spurred by the advantages of transformers, which employs\nmulti-head attention mechanisms to capture long-range contextual relations\nbetween image pixels, we proposed a novel transformer-based network (called\nTransCBCT) to generate synthetic CT (sCT) from CBCT. This study aimed to\nfurther improve the accuracy and efﬁciency of ART.\nMaterials and methods: In this study, 91 patients diagnosed with prostate\ncancer were enrolled. We constructed a transformer-based hierarchical\nencoder– decoder structure with skip connection, called TransCBCT. The\nnetwork also employed several convolutional layers to capture local context.\nThe proposed TransCBCT was trained and validated on 6,144 paired CBCT/\ndeformed CT images from 76 patients and tested on 1,026 paired images from\n15 patients. The performance of the proposed TransCBCT was compared with\na widely recognized style transferring deep learning method, the cycle-\nconsistent adversarial network (CycleGAN). We evaluated the image quality\nand clinical value (application in auto-segmentation and dose calculation) for\nART need.\nResults: TransCBCT had superior performance in generating sCT from CBCT.\nThe mean absolute error of TransCBCT was 28.8 ± 16.7 HU, compared to 66.5\n± 13.2 for raw CBCT, and 34.3 ± 17.3 for CycleGAN. It can preserve the\nstructure of raw CBCT and reduce artifacts. When applied in auto-\nsegmentation, the Dice similarity coefﬁcients of bladder and rectum between\nauto-segmentation and oncologist manual contours were 0.92 and 0.84 for\nTransCBCT, respectively, compared to 0.90 and 0.83 for CycleGAN. When\nFrontiers inOncology frontiersin.org01\nOPEN ACCESS\nEDITED BY\nWei Zhao,\nBeihang University, China\nREVIEWED BY\nXiaokun Liang,\nShenzhen Institutes of Advanced\nTechnology (CAS), China\nRuijie Yang,\nPeking University Third Hospital, China\n*CORRESPONDENCE\nKuo Men\nmenkuo126@126.com\nSPECIALTY SECTION\nThis article was submitted to\nRadiation Oncology,\na section of the journal\nFrontiers in Oncology\nRECEIVED 07 July 2022\nACCEPTED 27 July 2022\nPUBLISHED 25 August 2022\nCITATION\nChen X,Liu Y,Yang B,Zhu J,Yuan S,\nXie X,Liu Y,Dai J andMen K (2022)\nA more effective CT synthesizer\nusing transformers for cone-beam\nCT-guided adaptive radiotherapy.\nFront. Oncol. 12:988800.\ndoi: 10.3389/fonc.2022.988800\nCOPYRIGHT\n© 2022 Chen, Liu, Yang, Zhu, Yuan, Xie,\nLiu, Dai and Men. This is an open-\naccess article distributed under the\nterms of theCreative Commons\nAttribution License (CC BY).The use,\ndistribution or reproduction in other\nforums is permitted, provided the\noriginal author(s) and the copyright\nowner(s) are credited and that the\noriginal publication in this journal is\ncited, in accordance with accepted\nacademic practice. No use,\ndistribution or reproduction is\npermitted which does not comply with\nthese terms.\nTYPE Original Research\nPUBLISHED 25 August 2022\nDOI 10.3389/fonc.2022.988800\napplied in dose calculation, the gamma passing rate (1%/1 mm criterion) was\n97.5% ± 1.1% for TransCBCT, compared to 96.9% ± 1.8% for CycleGAN.\nConclusions: The proposed TransCBCT can effectively generate sCT for CBCT.\nIt has the potential to improve radiotherapy accuracy.\nKEYWORDS\nadaptive radiotherapy, CBCT, deep learning, transformer, image quality\n1 Introduction\nAs a tool for image-guided radiotherapy, cone-beam\ncomputed tomography (CBCT) equipped with radiotherapy\nunits can acquire three-dimensional images of patients at the\ntreatment position. The positioning error of fractional treatment\ncan be corrected by registering fractional CBCT with simulation\nCT images. CBCT imaging can also be applied for adaptive\nradiation therapy (ART) to ensure the accuracy of dose delivery\nwhen the patient ’s anatomy changes signi ﬁcantly during the\ntreatment course ( 1– 3). However, the greatest challenge of\nCBCT is its low image quality, which limits its application for\nprecise radiotherapy ( 2, 3).\nSeveral conventional methods have been proposed to\nimprove the image quality of CBCT. They are classi ﬁed into\nhardware-based and software-based methods. Hardware, such as\nanti-scatter grid ( 4) and x-ray beam blocker with a strip pattern\n(5), are employed to reduce the scatter photons, reducing the\nimaging system ’sq u a n t u me f ﬁcacy. Meanwhile, additional\ndevices need to be set up on the onboard imagers to use these\nmethods. These problems are not exited in software approaches.\nRay tracing ( 6) and Monte Carlo ( 7) methods can estimate\nscatter distribution to correct the CBCT projections.\nAdditionally, iterative reconstruction ( 8, 9) is implemented to\nobtain high-quality images from the limited projections. These\nmethods are promising but limited by the huge\ncomputational cost.\nRecently, deep learning methods, especially the\nconvolutional neural network (CNN)-based model, have been\nproven promising in image processing due to their advantages\nin leveraging local context and enabling a large reception ﬁeld.\nCNN has been explored to improve the quality of CBCT images\nby correcting the projections ( 10– 13) and generating synthetic\nCT (sCT) images ( 14– 16). However, it is limited when\nsubstantial anatomical cha nges exist between CBCT and\nplanning CT due to its supervised learning pattern ( 17, 18).\nSince the exactly matched CBCT and CT images are nearly\nunavailable, some studies employed unsupervised learning to\nenhance the quality of CBCT images to CT level in the image\ndomain. The generative adversarial network (GAN)-based\nmodels, especially the cycle-consistent adversarial network\n(CycleGAN) ( 19), are suitable for image transferring with\nunpaired data. Liang et al. ( 20) used CycleGAN to preserve\nthe anatomical structure of CBCT and improve its image\nquality on the head and neck. For the parts with substantial\norgan dislocations, such as the abdomen, Liu et al. ( 21)\nemployed a deep-attention CycleGAN and copied the air\npockets observed in CBCT and CT to solve the mismatching\nproblem. The cycle-consistent is helpful to the keep the raw\nstructure of the cycle-consiste nt loss is helpful to the keep the\nraw structure of CBCT. Kida et al. ( 22) employed more loss\nfunction parts to visually enhance the CBCT images. Uh et al.\n(23) combined adjacent anatomic data and normalized age-\ndependent body sizes in children and young adults to improve\nthe training set, and got a better CycleGAN model.\nIn the past few years, the transformer has been one of the\npopular architectures for deep le arning task, since it can take\nadvantage of modeling long-range dependencies based on the\nattention mechanism ( 24, 25). Now, the transformer is\nexpected to handle the more med ical image processing tasks.\nWu et al. ( 26) successfully employed Vision Transformer ( 27)\nto recognize diabetic retinopathy grade with more accuracy\nthan the CNN-based model. Yang et al. ( 28)d e s i g n e da\ntransformer-based deformabl e image registration network\nand achieved robust registration and promising\ngeneralizability. Zhang et al. ( 29) employed the transformer\nblocks for low dose CT denoising and produced superior\nresults. Liu et al. ( 30) proposed the Swin transformer to\nimplement hierarchical architecture using a non-overlapping\nshifted windowing scheme to obtain greater ef ﬁciency,\nAbbreviations: CBCT, cone-beam computed tomography; ART, adaptive\nradiotherapy; sCT, synthetic CT; CycleGAN, cycle-consistent adversarial\nnetwork; CNN, convolutional neural network; GAN, generative adversarial\nnetwork; MSA, multi-header self-attention; LeFF, locally enhanced\nfeedforward network; W-MSA, window-based MSA; MS-SSIM, multiscale\nstructural similarity; MAE, mean absolute error; RMSE, root mean square\nerror; PSNR, peak signal-to-noise ratio; ROIs, regions of interest; DSC, dice\nsimilarity coefﬁcient; MDA, mean distance to agreement; VMAT, volumetric-\nmodulated arc therapy.\nChen et al. 10.3389/fonc.2022.988800\nFrontiers inOncology frontiersin.org02\nachieving great progress in image classi ﬁcation, dense\nprediction, and semantic segmentation.\nInspired by the emerging advantages of transformers, we\nproposed a novel transfor mer-based network, called\n“TransCBCT”, to convert CBCT to sCT. We hypothesized that\nsCT generated by the proposed TransCBCT can improve image\nquality CBCT-based image-guided radiotherapy. The clinical\nvalue was tested on segmentation and dose calculation, which\nis signi ﬁcant to radiotherapy. To the best of our knowledge, this\nis the ﬁrst attempt to apply transformer in synthesizing CT from\nCBCT. The experiments have demonstrated that it is superior to\nthe state-of-the-art method (CycleGAN) in improving CBCT.\nThis study may provide a more effective network to the long-\nstanding challenges in the clinical application of CBCT.\n2 Materials and methods\n2.1 Data collection\nData of 91 patients with prostate cancer were collected in\nthis study. The planning CT images and daily CBCT images\nwere acquired and registered. The planning CT images were\nacquired with a CT simulator (SOMATOM De ﬁnition AS 40,\nSiemens or Brilliance CT Big Bore, Philips) with the following\nparameters: voltage: 120 kV; exposure: 280 (Siemens) or 240\n(Philips) mAs; image resolution: 512 × 512; pixel size: 1.27 × 1.27\nmm2; slice thickness: 3 mm. The CBCT images were scanned on\na Varian On-board Imager with the following parameters:\nvoltage: 125 kV; exposure: 1,080 mAs; rotation range: 360°;\nprojections: 900 frames; image resolution: 512 × 512; pixel size:\n0.91 × 0.91 mm 2; slice thickness: 1.91 mm. The radiotherapy was\nimplemented on a Varian Edge radiosurgery system.\nDeformable registration was implemented using the MIM\nsoftware (v.7.0.1, MIM Software Inc., Cleveland, OH, USA) to\nmake the planning CT images paired to the CBCT images. The\ndeformed CT images were resampled to have the same spatial\nresolution and pixel size as the reference CBCT images. The gray\nvalue of pixels outside the patient body was set to zero to avoid\nbackground in ﬂuence. Institutional Review Board approval was\nobtained for this retrospective analysis.\n2.2 The transformer framework\nFigure 1 shows the architecture of the proposed transformer\nnetwork (named as “TransCBCT ”), which was a U-shape\nhierarchical encoder – decoder structure with skip connection.\nFor the training stages, the input of TransCBCT was a 2D CBCT\nimage, and the output was the corresponding 2D deformed CT\nimage. The transformer blocks were adopted into the encoder\nand decoder. The main design for the transformer part is\nhierarchical structure and shifted-window based multi-head\nself-attention method (SW-MSA). This is helpful to capture\nglobal information and save computing source. Instead of using\na pure transformer-based structure, some convolution layers\nwere employed to help enhance the local detail.\nThe input projection adopted a 3 × 3 convolution layer with\nLeakyReLU to extract shallow feature maps X ∈ RC×H×W from\nthe CBCT image I ∈ R1×H×W with H and W being the height and\nwidth of the map. A 3 × 3 convolutional layer was used as the\noutput projection. Four transformer-based encoders and one\ntransformer-based bottleneck were used to extract the CBCT\nimage features. Then, we used four transformer-based decoders\nwith skip-connected feature maps to recover the image details.\nFigure 2 shows that the hierarchical network used two\ntransformer blocks to capture long-range dependencies in\nencoder or decoder. For the self-attention calculation, the 2D\nfuture maps must be transformed to tokens using the\nImg2Tokens layer ( 31 ). However, we adopted the\ndownsampling and upsampling operators on the 2D feature\nmaps, which need the tokens reshaped to 2D maps by the\nTokens2Img layer. For the downsampling, we used a 4 × 4\nconvolution layer with a stride of 2. Meanwhile, we adopted a 2 ×\n2 transpose convolution layer with a stride of 2 for the\nup-sampling.\nEach transformer block consists of two main parts with a\nnormal layer: a multi-header self-attention (MSA) layer and a\nFIGURE 1\nArchitecture of the proposed TransCBCT. The network was\nconstructed using a hierarchical encoder– decoder structure\nwith skip connection. The encoder and decoder used a\ntransformer block to construct a hierarchical structure, which\nwas efﬁcient to extract features and recover the image\nstructures and details.\nChen et al. 10.3389/fonc.2022.988800\nFrontiers inOncology frontiersin.org03\nlocally enhanced feedforward network (LeFF) layer, as shown in\nFigure 3 . Instead of using global self-attention on the whole\nimage, we implemented self-attention within non-overlapping\nlocal windows to reduce the computational cost, the window-\nbased MSA (W-MSA). For the 2D feature maps X ∈ RC×H×W\nwith H and W as the height and width of the maps, we split X\ninto non-overlapping windows with a window size of M × M.\nThen, we obtained the ﬂattened and transposed features\nX\ni ∈ RM2\n×C from each window i. Next, we performed self-\na t t e n t i o no nt h e ﬂattened features in each window. When\ncomputing self-attention, the head number is k, and the head\ndimension is d = C/k. We included relative position bias B to\neach head in computing similarity:\nAttention Q, K, VðÞ = SoftMax QK=\nﬃﬃﬃ\nd\np\n+ B\n/C16/C17\nV (1)\nwhere Q, K, and V are the query, key, and value matrices.\nMoreover, Q, K, V, and B are learning parameters. The shift\nFIGURE 2\nIllustration of encoder, decoder, and bottleneck in the proposed TransCBCT shown inFigure 1.\nFIGURE 3\nDetails of the transformer blocks in the encoder, decoder, and bottleneck shown inFigure 2.\nChen et al. 10.3389/fonc.2022.988800\nFrontiers inOncology frontiersin.org04\nwindow stage was used to obtain the connection between\ndifferent windows. As proved in the Swin transformer, shift\nwindow-based self-attention was essential to enhance modeling\npower ( 30). In transformer block 1, a basal window partitioning\nscheme was used, and self-attention was conducted in each\nwindow. In transformer block 2, the window partitioning was\nshifted, resulting in new wi ndows. The self-attention\ncomputation in the new windows crosses the boundaries of\nthe previous windows in transformer block 1.\nTo enhance the capability to leverage local context, we\nemployed the LeFF layer that contained a 2D convolutional\nlayer, as shown in Figure 3. A linear projection layer was applied\nto each token to increase its feature dimension. Next, we\nreshaped the tokens to 2D feature maps and adopted a 3 × 3\ndepth-wise convolutional layer to capture local information.\nThen, we ﬂatten back the features to tokens and shrink the\nchannels using another linear layer. The Gaussian Error Linear\nUnit (GELU) was set as the activation function after each linear/\nconvolutional layer, which is a function that simply multiplies its\ninput by the cumulative density function of the normal\ndistribution at this input.\nThe loss function contained Charbonnier loss and multiscale\nstructural similarity (MS-SSIM). The Charbonnier loss calibrates\nthe CT numbers as follows:\nCharbonnier loss =\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ﬃ\njjy − xjj\n2+e2\nq\n(2)\nwhere x is the ground truth image, y is the predicted image, and e\nis a constant, which is 10 −3.\nThe MS-SSIM consists of three parts: luminance (L),\ncontrast (C), and structure (S) comparison measures, given as\nfollows:\nLx , yðÞ = 2uxuy + C1\n/C0/C1\nux2+ uy2+ C1\n/C0/C1 (3)\nCx , yðÞ = 2sxsy + C2\n/C0/C1\nsx2+ sy2+ C2\n/C0/C1   (4)\nSx , yðÞ = sxy + C3\n/C0/C1\nsxsy + C3\n/C0/C1 (5)\nwhere my and sy are the https://en.wikip edia.org/wiki/\nAverage and standard deviation of y, respectively; sxy is the\ncovariance of x and y; C1, C2, and C3 are constants, set as 1.\nThe MS-SSIM is de ﬁned as follows:\nMS − SSIM x, yðÞ = LM x, yðÞ½/C138 aM\nYM\nj=1\nCj x, yðÞ\n/C2/C3 bj Sj x, yðÞ\n/C2/C3 gj (6)\nwhere j is the image downsampling factor, and j = 1 represents\nthe original image.\nTo quickly restore the image details and calibrate the CT\nnumbers, the Charbonnier loss and MS-SSIM must be added\nwith suitable proportions. Our ﬁnal result was obtained with a\nproportion of 9:1 for Charbonnier loss and MS-SSIM,\nrespectively, achieving the best performance in our testing.\n2.3 Experiments\nThe paired CT/CBCT images were randomly divided into a\ntraining set, a validation set, and a test set. The training set\nconsisted of 4,922 pairs of images from 61 patients, the\nvalidation set consisted of 1,222 pairs of images from 15\npatients, whereas the test set consisted of 1,206 pairs of images\nfrom the remaining 15 patients. Lookahead with a learning rate\nof 0.001 was set as the optimizer, which is more effective for\nconvergence based on Adam. The CycleGAN was selected for\ncomparation, whose architecture is the same as the study where\nsynthetic kV-CT is generated from megavoltage CT. We chose\nthe “CycleGAN-Resnet” for this work, which contained nine\nresidual blocks in the generator ( 32). The loss function of\nCycleGAN contained adversarial loss, cycle-consistent loss,\nand identity loss ( 19). In this experiment, all training and\ntesting were conducted on an Nvidia GeForce RTX 3090 GPU.\n2.4 Evaluation\nTo validate the proposed TransCBCT, we compared its\nperformance with CycleGAN, wh ich is the state-of-the-art\nmethod for CBCT improvement. The evaluation covered\nimage quality of CBCT and its clinical application value: auto-\nsegmentation and dose c alculation. The paired t-test was\nperformed if the data were normally distributed; otherwise, the\nWilcoxon signed-rank test for paired samples non-parametric\ntest was performed. Statistical signi ﬁcance was set at p < 0.05.\n2.4.1 Image quality\nFor analyzing the image quality of sCT, the deformable\nplanning CT images were used as the ground truth. The\nevaluation metrics included mean absolute error (MAE), root\nmean square error (RMSE), and peak signal-to-noise ratio\n(PSNR). Their de ﬁnitions are presented as below:\nMAE =   1\nN o\nN\ni=1\n^y i − yi/C12/C12 /C12/C12 (7)\nRMSE =   1\nN\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\no\nN\ni=1\n^y i − yi/C12/C12 /C12/C12 2\ns\n(8)\nChen et al. 10.3389/fonc.2022.988800\nFrontiers inOncology frontiersin.org05\nPSNR = 10log Max 2\np\n1\nN o\nN\ni=1 ^y i − yi/C0/C1 2 (9)\nwhere N is the number of pixels involved in the calculation; i is\nthe ith pixel; ^y and y are the test and reference CT number,\nrespectively; and Max p is the possible maximum pixel value in\nthe image.\nA higher value of PSNR implies good consistency between\nsCT and ground truth, while values closer to zero are better for\nMAE and RMSE. In addition, we select several regions of interest\n(ROIs) to evaluate the accuracy of CT numbers.\n2.4.2 Clinical application value\nWe focused on two steps of ART work ﬂow: segmentation and\ndose calculation. Automatic segmentation is often applied in\nCBCT-based image-guided radiotherapy to improve treatment\nefﬁciency. Our previously published network was adopted for\nsegmentation in this study ( 33). The model is being applied to\nassist the radiation oncologists in daily clinical work, which have\nhelped them to save time. We chose bladder and rectum for\ntesting, considering their difference between different scan time\nare difﬁcult to be totally eliminated by the deformed registration.\nAutomatic segmentations were generated on deformed CT,\nCBCT, sCT (CycleGAN), and sCT (TransCBCT), respectively.\nTo evaluate the accuracy, manual contours on CBCT image were\nreferred to as the ground truth for CBCT, sCT (CycleGAN), and\nsCT (TransCBCT). The deformed CT were contoured\nindependently since shapes and locations of the organs are\ndifferent between deformed CT and CBCT. These contours\ndelineated and reviewed by experienced radiation oncologists.\nThe accuracy was compared with metrics of the dice similarity\ncoefﬁcient (DSC) and mean distance to agreement (MDA).\nHigher DSC and lower MDA values indicate better consistency\nbetween the automatic segmentations and ground truth. It is\nworth noting that the model is trained on the planning CT; thus,\ntheoretically, it would get best result on the deformed CT.\nAs for dose calculations, the Pinnacle treatment planning\nsystem (Philips Medical Systems, Fitchburg, WI) was used to\ncreate volumetric-modulated arc therapy (VMAT) plans for the\n15 patients in the test set. These plans were imported onto\ndifferent CT images to calculate the dose distribution with\ncollapsed cone convolution algorithm. The dose distribution\non deformed CT was regarded as the ground truth of each plan.\nThen, the dose distributions on the CBCT, sCT (CycleGAN),\nand sCT (TransCBCT) images were compared to the ground\ntruth using global 3D gamma analysis. In the gamma analysis, a\nthreshold dose value was set to 10% of the prescription dose, and\nthe dose difference criterion was de ﬁned as percentage of the\nprescription dose.\n3 Results\n3.1 Image quality\nThe MAE (HU), RMSE (HU), and PSNR (dB) values of the\noriginal CBCT were 66.5 ± 13.2, 90.5 ± 28.3, and 33.4 ± 2.0,\nrespectively. Both CycleGAN and the proposed TransCBCT\nimproved the image quality, with the following metric: MAE,\n34.3 ± 17.3 (CycleGAN) vs. 28.8 ± 16.7 (TransCBCT), p < 0.05;\nRMSE, 63.2 ± 36.5 (CycleGAN) vs. 57.0 ± 34.5 (TransCBCT),\np < 0.05; PSNR, 37.0 ± 2.8 (CycleGAN) vs. 38.0 ± 3.3\n(TransCBCT), p < 0.05, respectively. Compared with CBCT,\nthe TransCBCT and CycleGAN reduced the MAE value\nrelatively by 56.7% and 48.4%, reduced the RMSE value\nrelatively by 33.6% and 30.1%, and improved PSNR relatively\nby 13.8% and 10.7%, respectively. The results demonstrate that\nthe overall performance of the proposed TransCBCT was better\nthan the CycleGAN ( p < 0.05 in MAE, RMSE, and PSNR).\nFigure 4A shows an example of a representative patient,\nwhich indicates that sCT is more suitable for contouring than\nthe deformed CT image. Figure 4B shows a slice of the deformed\nCT, CBCT, sCT (CycleGAN), and sCT (TransCBCT), with their\ndifferences from the ground truth. Both deep learning methods\ncan improve image quality, but the proposed TransCBCT\noutperforms the CycleGAN, especially in reducing artifacts. As\nshown by the green arrows, the “photon starving ” artifacts are\nlighter on sCT (TransCBCT) than on CBCT and sCT\n(CycleGAN). Figure 4C shows the HU histogram plots of the\nentire testing set. Compared to CycleGAN, the proposed\nTransCBCT improved the HU accuracy corresponding to the\ndeformed CT image.\nAs indicated by the orange arrow, there were “calciﬁcation-\nlike” structures on sCT (CycleGAN), which should not have\nbeen observed there. Although Cycle-GANs can be trained on\nunpaired images due to the cycl e-consistent loss, structure\npreservation remains an iss ue as input images undergo\nambiguous geometric transformations between domains\nbecause the models are under-constrained. Meanwhile,\nTransCBCT did not generate such abnormal structures on\nsCT images. Besides the “photon starving ” artifacts,\nTransCBCT achieved good performance in reducing the beam\nhardening effect as marked by the yellow lines. The beam\nhardening artifacts would affect the accuracy of CT numbers.\nThe CycleGAN only transferred the image style of the deformed\nCT to CBCT and preserved all structures of CBCT, including the\nChen et al. 10.3389/fonc.2022.988800\nFrontiers inOncology frontiersin.org06\nartifact part. Meanwhile, TransCBCT can effectively reduce\nthese artifacts. The results of the ROI test are shown in the\nAdditional ﬁle, which shows that the TransCBCT can improve\nthe accuracy of CT numbers and reduce the noise.\n3.2 Clinical application value\nWe calculated the DSC and MDA between the predicted\ncontour and ground truth. It is worth noting that the deformed\nCT and CBCT were contoured independently. Due to different\nscan times, the bladder and rectum on the images get different\nlocals and sizes. For CBCT, sCT(CycleGAN), sCT(TransCBCT),\nand deformed CT, the mean of the DSC was 0.88, 0.90, 0.92, and\n0.93 for the bladder and 0.82, 0.83, 0.84, and 0.85 for the rectum,\nrespectively, and the mean of the MAD was 2.36, 1.92, 1.48, and\n1.46 mm for the bladder and 1.75, 1.80, 1.52, and 1.37 mm for\nthe rectum, respectively. As shown in Figures 5A– D, the result of\nTransCBCT was better than CycleGAN, and closer to the\ndeformed CT ( p > 0.05). The same auto-segmentation model\nA\nB\nC\nFIGURE 4\n(A) Axial, sagittal, and coronal views of one patient. The orange arrows indicate the abnormal structure on sCT images. Yellow lines indicate the\nobvious beam hardening artifacts. Anatomical differences can be obtained, especially on the bladder sizes and locations, because of the time\nintervals between CT and CBCT scanning. The TransCBCT and CycleGAN maintained the same structure as CBCT with a clear organ edge.(B)\nExample of an axial slice of the deformed CT, CBCT, and sCT with the HU difference compared to the deformed CT image. The green arrows\nindicate the“photon starving” artifacts. (C) The HU histogram plot calculated on the entire test set.\nChen et al. 10.3389/fonc.2022.988800\nFrontiers inOncology frontiersin.org07\nwas applied to these images, so the above result demonstrated\nthat the image features of sCT(TransCBCT) and the deformed\nCT were more similar. In other words, TransCBCT improved\nCBCT to the level of deformed CT.\nFigure 5E shows the result of gamma passing rate for 15\npatients. The gamma passing rate of TransCBCT (1%/1 mm\ncriterion) was 97.5% ± 1.1%. Compared to the result of 96.9% ±\n1.8% for CycleGAN, there was a slight improvement, and the p-\nvalue was below 0.05. All the results of TransCBCT were above\n95%. Even though the CycleGAN was in good performance, the\nTransCBCT showed more accuracy and robustness, which\nimport to the ART. It is the result of less artifacts and more\nAB\nD\nE\nC\nFIGURE 5\nStatistical analysis of clinical value based on ART need.(A–E) Result of auto-segmentation.(A) DSC of bladder. Thep-values were <0.05 (CBCT vs.\ndeformed CT), <0.05 [sCT(CycleGAN) vs. deformed CT], and 0.196 [sCT(TransCBCT) vs. deformed CT].(B) DSC of rectum. Thep-values were\n<0.05 (CBCT vs. deformed CT), <0.05 [sCT(CycleGAN) vs. deformed CT], and 0.144 [sCT(TransCBCT) vs. deformed CT].(C) MAD of bladder. The\np-values were <0.05 (CBCT vs. deformed CT), <0.05 [sCT(CycleGAN) vs. deformed CT], and 0.450 [sCT(TransCBCT) vs. deformed CT].(D) MAD of\nrectum. Thep-values were <0.05 (CBCT vs. deformed CT), <0.05 [sCT(CycleGAN) vs. deformed CT], and 0.058 [sCT(TransCBCT) vs. deformed\nCT]. (E) Result of gamma passing rate for 15 patients. The results of TransCBCT were more robust compared to the CycleGAN. Some of\nCycleGAN results were under 93%.\nChen et al. 10.3389/fonc.2022.988800\nFrontiers inOncology frontiersin.org08\naccuracy CT numbers on sCT(TransCBCT).\n4 Discussion\nThe experimental results demonstrated that the proposed\nTransCBCT can effectively improve the quality of CBCT images.\nIt can also preserve the structure of CBCT and calibrate the HUs\neffectively. The proposed TransCBCT is more suitable for ART\ndue to its good performance in reducing the noise and artifacts\nthan conventional CycleGAN. To the best of our knowledge,\napplying a transformer-based network for generating sCT has\nnot been investigated yet.\nThe primary motivation of using a transformer-based network\nis due to its strength in capturing t he long-distance dependence to\nthe global. To be more speci ﬁc, after calculating self-attention, one\ntoken is strongly related to other tokens. In comparison, the\nconvolution-based network normally adopts a 3 × 3 or 7 × 7\nconvolutional kernel to capture the local context, and one pixel in\nthe feature maps corresponds to a 3 × 3 or 7 × 7 ﬁeld. To obtain\nlong-distance relationship, the convolution-based network needs to\nbe deeper, followed by other problems, such as diminishing\ngradient. There are also many advantages of convolution, such as\nlow computational cost and translation invariance. The high\ncomputational cost also limits the application of the transformers\non medical images. The proposed TransCBCT combined the\nadvantages of convolution and transformer. The non-overlapping\nwindow design can reduce the model parameters to make it more\neffective. The inductive bias, which kept certain translation\ninvariance, was stil l preferable for modeling. The network was\ntrained on 6,144 paired images without pre-training. For one\npatient, the sCT images can be generated within 15 s. The\ncomputational cost was acceptable. To retain the advantage of\ncapturing global information, hierarchical structure and SW-MSA\nwere implemented. We also adopted several convolutional layers to\nenhance the ability to leverage the local context. In total, the\nnetwork combines the power of the transformer in capturing the\nlong-range dependencies and the advantage of convolution in\nleveraging the local context. The hierarchical encoder – decoder\nstructure makes the network ef ﬁcient to extract features and\nrecover the image structures and details. The non-overlapping\nwindow-based and shift window-b ased strategies are essential in\nreducing the computational cost while maintaining efﬁciency.\nThe encoder– decoder structure with skip connection has been\nwidely used in conventional pure convolution-based networks. U-\nnet (34) is a typical and successful image processing network. Many\nstudies (14– 16) employed it to generate sCT from CBCT. However,\nwhen there are substantial structure changes between CBCT and\ndeformed CT, the convolution-based Unet can easily be misled to a\nwrong optimized direction. Figure 6 shows that pure convolution-\nbased Unet cannot preserve the structure of CBCT. Meanwhile, the\nproposed TransCBCT also constructed the U-shape network;\nimporting the transformer block helped to maintain the same\nstructure as CBCT. This is important for ART since we want to\nobtain accurate online anatomical information from the images.\nTherefore, CycleGAN is the most popular network to generate sCT\nfor ART. The result of CycleGAN shows that it can preserve the\nstructure of CBCT and keep artifacts. The proposed TransCBCT\ncan effectively reduce artifacts bene ﬁtted by leveraging the global\ninformation. TransCBCT shows more robustness and accuracy in\ncalibrating the CT numbers.\nThe segmentation task is an important part of ART\nwork ﬂow, which affects the ef ﬁciency and accuracy. At\npresent, many studies are focused on this task and contribute\nmany efﬁcient methods. For now, Yang Lei et al. ( 35)p r e s e n t e d\nthe best pelvic multi-organ segmentation result on CBCT\nimages. They employed the CycleGAN to generate the MRI\nand then auto-segmentation is implemented on the synthetic\nMRI. Our center also employs the deep learning-based method\nto assist daily contouring wor k. The sCT (TransCBCT) showed\nmore accuracy than sCT (CycleGAN) when using our trained\nauto-segment model before. Si nce the deformed CT is not well-\nmatched to the CBCT, it is better to use the contour of CBCT as\nthe ground truth of sCT. As shown in Figure 4 , the shape of the\nbladder is different between the CBCT and the deformed CT.\nThus, we contoured the organs on the CBCT and the deformed\nCT independently as the ground truth. The result of TransCBCT\nwas close to the deformed CT, which means the features of sCT\nFIGURE 6\nResult of a pure convolution-based Unet that made the shape of the bladder vary from CBCT. TransCBCT and Unet employed the same U-\nshape structure and loss function. However, with transformer block embedded, TransCBCT showed the ability to keep the raw CBCT structure.\nChen et al. 10.3389/fonc.2022.988800\nFrontiers inOncology frontiersin.org09\n(TransCBCT) were closer to the deformed CT in image feature. The\npromising result might get a more long-term meaning on the\nradiomics reproducibility studies ( 36), which is helpful for ART\nclinical decision support. It is worth noting that the auto-segment\nmodel is a relatively objective evaluation standard, rather than a\nspeciﬁc-constructed model for ART. However, since the sCT\n(TransCBCT) is close to the deformed CT, the problems on the\nsegmentation part of the ART workﬂow would be easier to solve. In\nthe future, a special deep learning network can be trained for ART\nworkﬂow to improve accuracy. Our group performs relative studies\nonmagnetic resonance imaging-guided adaptiveradiotherapy (37).\nAs for dose calculation, even though the deformed CT cannot\nbe regarded as the “Gold Standard”, there was no better reference\nfor this study at present. Other studies (21, 23, 31, 32) also employed\nthe deformed CT as reference for dosimetry evaluation for sCT\ngeneration from CBCT or MR. Uh et al. ( 23)u s e daC y c l e G A N ,\nresulting in gamma passing rates of 98.5 ± 1.9% (2%/2 mm\ncriterion) for proton dose calculation. Kurz et al. ( 38) obtained a\nresult of 96% (2%/2 mm criterion) for the proton dose calculation\nand 89% (1%/1 mm criterion) for VMAT. We used a relatively well-\nperforming CycleGAN and obtained a result of 96.9% ± 1.8% (1%/\n1 mm criterion) for photon VMAT, and the TransCBCT product\nshowed superior results.\nBased on our promising ﬁndings, the proposed framework\nhas the potential to be applied in other image generation tasks,\nsuch as CBCT to relative stopping power maps and MVCT to\nsCT. The main contribution of CycleGAN is the design of cycle-\nconsistent loss, which is important to keep the raw image\nstructure. Our proposed network can also take advantage of\nthe cycle-consistent loss by designing the cycle structure, which\nmeans that at least two transformer-based networks need to be\nemployed as generators for the GAN model. The main challenge\nis how to deal with the high computing cost and how to balance\nthe generator and discriminator.\n5 Conclusion\nIn this study, we successfully developed a more effective CT\nsynthesizer using transformers for CBCT-guided adaptive\nradiotherapy. The strength of the proposed method was also\nveriﬁed relative to the conventional pure convolution-based\nnetwork. The sCT generated b y TransCBCT is helpful for\ncontouring and dose calculation, which can be used in ART to\nimprove radiotherapy accuracy.\nData availability statement\nThe datasets presented in this article are not readily available\nbecause of data security requirement of our hospital. Requests to\naccess the datasets should be directed to KM,menkuo126@126.com.\nEthics statement\nThe studies involving human participants were reviewed and\napproved by the Independent Ethics Committee of Cancer\nHospital, Chinese Academy of Medical Sciences. Written\ninformed consent for participation was not required for this\nstudy in accordance with the national legislation and the\ninstitutional requirements.\nAuthor contributions\nAll authors discussed and conceived the study design. XC\nwrote the programs and drafted the manuscript. YXL, BY, JZ,\nSY, and XX helped to collect the data and performed data\nanalysis. YXL performed the clinical analysis. KM and JD\nguided the study and participated in discussions and\npreparation of the manuscript. All authors read, discussed, and\napproved the ﬁnal manuscript.\nFunding\nThis work was supported by the National Natural Science\nFoundation of China (12175312), Beijing Nova Program\n(Z201100006820058), and the CAMS Innovation Fund for\nMedical Sciences (2020-I2M-C&T-B-073, 2021-I2M-C&T-\nA-016).\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could\nbe construed as a potential con ﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their af ﬁliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed\nor endorsed by the publisher.\nSupplementary Material\nThe Supplementary Material for this article can be found\nonline at: https://www.frontiersin. org/articles/10.3389/\nfonc.2022.988800/full#supplementary-material\nChen et al. 10.3389/fonc.2022.988800\nFrontiers inOncology frontiersin.org10\nReferences\n1. Yan D, Vicini F, Wong J, Martinez A. Adaptive radiation therapy. Phys Med\nBiol (1997) 42:123 – 32. doi: 10.1088/0031-9155/42/1/008\n2. Lim-Reinders S, Keller BM, Al-Ward S, Sahgal A, Kim A. Online adaptive\nradiation therapy. Int J Radiat Oncol Biol Phys(2017) 99:994 – 1003. doi: 10.1016/\nj.ijrobp.2017.04.023\n3. Albertini F, Matter M, Nenoff L, Zhang Y, Lomax A. Online daily adaptive\nproton therapy. Br J Radiol(2019) 93:20190594. doi: 10.1259/bjr.20190594\n4. Siewerdsen JH, Moseley DJ, Bakhtiar B, Richard S, Jaffray DA. The in ﬂuence\nof antiscatter grids on soft-tissue detectability in cone-beam computed tomography\nwith ﬂat-panel detectors. Med Phys(2004) 31:3506 – 20. doi: 10.1118/1.1819789\n5. Zhu L, Xie Y, Wang J, Xing L. Scatter correction for cone-beam CT in\nradiation therapy. Med Phys(2009) 36:2258 – 68. doi: 10.1118/1.3130047\n6. Jia X, Yan H, Cervino L, Folkerts M, Jiang SB. A GPU tool for ef ﬁcient,\naccurate, and realistic simulation of cone beam CT projections. Med Phys (2012)\n39:7368– 78. doi: 10.1118/1.4766436\n7. Zbijewski W, Beekman FJ. Ef ﬁcient Monte Carlo based scatter artifact\nreduction in cone-beam micro-CT. IEEE Trans Med Imag(2006) 25:817 – 27. doi:\n10.1109/TMI.2006.872328\n8. Jia X, Dong B, Lou YF, Jiang SB. GPU-Based iterative cone-beam CT\nreconstruction using tight frame regularization. Phys Med Biol (2011) 56:3787 –\n807. doi: 10.1088/0031-9155/56/13/004\n9. Wang J, Li TF, Xing L. Iterative image reconstruction for CBCT using edge-\npreserving prior. Med Phys(2009) 36:252 – 60. doi: 10.1118/1.3036112\n10. Nomura Y, Xu Q, Shirato H, Shimizu S, Xing L. Projection-domain scatter\ncorrection for cone beam computed tomography using a residual convolutional\nneural network. Med Phys(2019) 46:3142 – 55. doi: 10.1002/mp.13583\n11. Jiang Y, Yang C, Yang P, Hu X, Luo C, Xue Y, et al. Scatter correction of\ncone-beam CT using a deep residual convolution neural network (DRCNN). Phys\nMed Biol(2019) 64:145003. doi: 10.1088/1361-6560/ab23a6\n12. Rusanov B, Ebert MA, Mukwada G, Hassan GM, Sabet M. A convolutional\nneural network for estimating cone-beam CT intensity deviations from virtual CT\nprojections. Phys Med Biol(2021) 22;66(21). doi: 10.1088/1361-6560/ac27b6\n13. Lalonde A, Winey B, Verburg J, Paganetti H, Sharp GC. Evaluation of CBCT\nscatter correction using deep convolutional neural networks for head and neck\nadaptive proton therapy. Phys Med Biol(2020) 65:245022. doi: 10.1088/1361-6560/\nab9fcb\n14. Kida S, Nakamoto T, Nakano M, Nawa K, Haga A, Kotoku J, et al. Cone\nbeam computed tomography image q uality improvement using a deep\nconvolutional neural network. Cureus (2018) 10(4):e2548. doi:\n10.7759/\ncureus.2548\n15. Yuan NM, Dyer B, Rao S, Chen Q, Benedict S, Shang L, Kang Y, Qi J, et al.\nConvolutional neural network enhancement of fast-scan low-dose cone-beam CT\nimages for head and neck radiotherapy. Phys Med Biol(2020) 65(3)035003. doi:\n10.1088/1361-6560/ab6240\n16. Li Y, Zhu J, Liu Z, Teng J, Xie Q, Zhang L, et al. A preliminary study of using\na deep convolution neural network to generate synthesized CT images based on\nCBCT for adaptive radiotherapy of nasopharyngeal carcinoma. Phys Med Biol\n(2019) 64(14):145010. doi: 10.1088/1361-6560/ab2770\n17. Zhang Y, Yue N, Su M-Y, Liu B, Ding Y, Zhou Y, et al. Improving CBCT\nquality to CT level using deep learning with generative adversarial network. Med\nPhys (2021) 48:2816 – 26. doi: 10.1002/mp.14624\n18. Liu Y, Chen X, Zhu J, Yang B, Wei R, Xiong R, et al. A two-step method to\nimprove image quality of CBCT with phantom-based supervised and patient-based\nunsupervised learning strategies. Phys Med Biol (2022) 67:084001. doi: 10.1088/\n1361-6560/ac6289\n19. Zhu J-Y, Park T, Isola P, Efros AA. Unpaired image-to-image translation\nusing cycle-consistent adversarial networks, in: Proceedings of the IEEE\ninternational conference on computer vision (ICCV)(2017), New York City: (The\nInstitute of Electrical and Electronics Engineers). pp. 2223 – 32.\n20. Liang X, Chen L, Nguyen D, Zhou Z, Gu X, Yang M, et al. Generating\nsynthesized computed tomography (CT) from cone-beam computed tomography\n(CBCT) using CycleGAN for adaptive radiation therapy. Phys Med Biol (2019)\n64:125002. doi: 10.1088/1361-6560/ab22f9\n21. Liu YZ, Lei Y, Wang TH, Fu YB, Tang XY, Curran WJ, et al. CBCT-based\nsynthetic CT generation using deep-attention cycleGAN for pancreatic adaptive\nradiotherapy. Med Phys(2020) 47:2472 – 83. doi: 10.1002/mp.14121\n22. Kida S, Kaji S, Nawa K, Imae T, Nakamoto T, Ozaki S, et al. Visual\nenhancement of cone-beam CT by use of CycleGAN. Med Phys (2020) 47:998 –\n1010. doi: 10.1002/mp.13963\n23. Uh J, Wang C, Acharya S, Krasin MJ, Hua C-h. Training a deep neural\nnetwork coping with diversities in abdominal and pelvic images of children and\nyoung adults for CBCT-based adaptive proton therapy. Radiother Oncol (2021)\n160:250– 8. doi: 10.1016/j.radonc.2021.05.006\n24. Gillioz A, Casas J, Mugellini E, Abou Khaled O. Overview of the\ntransformer-based models for NLP tasks, in: 2020 15th Conference on Computer\nScience and Information Systems (FedCSIS): IEEE, (New York City: The Institute of\nElectrical and Electronics Engineers) (2020). pp. 179 – 83.\n25. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al.\nAttention is all you need. In: Proceedings of the 31st international conference on\nneural information processing systems . Long Beach, California, USA: Curran\nAssociates Inc (2017). p. 6000 – 10.\n26. Wu J, Hu R, Xiao Z, Chen J, Liu J. Vision transformer-based recognition of\ndiabetic retinopathy grade. Med Phys (2021) 48(12):7850 – 63. doi: 10.1002/\nmp.15312\n27. Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner\nT, et al. An image is worth 16x16 words: Transformers for image recognition at\nscale. arXiv (2020). arXiv:201011929.\n28. Yang T, Bai X, Cui X, Gong Y, Li L. TransDIR: Deformable imaging\nregistration network based on transformer to improve the feature extraction ability.\nMed Phys(2022) 49:952 – 65. doi: 10.1002/mp.15420\n29. Zhang X, Han Z, Shangguan H, Han X, Cui X, Wang A. Artifact and detail\nattention generative adversarial networks for low-dose CT denoising.\nIEEE Trans\nMed Imag(2021) 40:3901 – 18. doi: 10.1109/TMI.2021.3101616\n30. Liu Z, Lin Y, Cao Y, Hu H, Wei Y, Zhang Z, et al. Swin transformer:\nHierarchical vision transformer using shifted windows. arXiv (2021).\narXiv:210314030, 10012 – 10022. doi: 10.1109/ICCV48922.2021.00986\n31. Wang Z, Cun X, Bao J, Liu J. Uformer: A general U-shaped transformer for\nimage restoration. arXiv (2021). arXiv:210603106.\n32. Chen X, Yang B, Li J, Zhu J, Ma X, Chen D, et al. A deep-learning method\nfor generating synthetic kV-CT and improving tumor segmentation for helical\ntomotherapy of nasopharyngeal carcinoma. Phys Med Biol(2021) 66:224001. doi:\n10.1088/1361-6560/ac3345\n33. Men K, Chen X, Yang B, Zhu J, Yi J, Wang S, et al. Automatic segmentation\nof three clinical target volumes in radiotherapy using lifelong learning. Radiother\nOncol (2021) 157:1 – 7. doi: 10.1016/j.radonc.2020.12.034\n34. Ronneberger O, Fischer P, Brox T. U-Net: Convolutional networks for\nbiomedical image segmentation. Cham: Springer International Publishing (2015)\np. 234 – 41.\n35. Lei Y, Wang T, Tian S, Dong X, Jani AB, Schuster D, et al. Male Pelvic\nmulti-organ segmentation aided by CBCT-based synthetic MRI. Phys Med Biol\n(2020) 65(3):035013. doi: 10.1088/1361-6560/ab63bb\n36. Traverso A, Wee L, Dekker A, Gillies R. Repeatability and reproducibility of\nradiomic features: a systematic review. Int J Radiat Oncol Biol Phys (2018)\n102:1143– 58. doi: 10.1016/j.ijrobp.2018.05.053\n37. Ma X, Chen X, Wang Y, Qin S, Yan X, Cao Y, et al. Personalized modeling to\nimprove pseudo – computed tomography images for magnetic resonance imaging –\nguided adaptive radiation therapy. Int J Radiat Oncology Biology Phys(2022) 113\n(4):7850– 7863. doi: 10.1016/j.ijrobp.2022.03.032\n38. Kurz C, Maspero M, Savenije MHF, Landry G, Kamp F, Pinto M, et al.\nCBCT correction using a cycle-consistent generative adversarial network and\nunpaired training to enable photon and proton dose calculation. Phys Med Biol\n(2019) 64(22):225004. doi: 10.1088/1361-6560/ab4d8c\nChen et al. 10.3389/fonc.2022.988800\nFrontiers inOncology frontiersin.org11",
  "topic": "Cone beam ct",
  "concepts": [
    {
      "name": "Cone beam ct",
      "score": 0.8064161539077759
    },
    {
      "name": "Radiation therapy",
      "score": 0.6400940418243408
    },
    {
      "name": "External beam radiotherapy",
      "score": 0.5682345032691956
    },
    {
      "name": "Medical physics",
      "score": 0.49742916226387024
    },
    {
      "name": "Medicine",
      "score": 0.4860456585884094
    },
    {
      "name": "Nuclear medicine",
      "score": 0.4462960362434387
    },
    {
      "name": "Computer science",
      "score": 0.4426809549331665
    },
    {
      "name": "Radiology",
      "score": 0.29675576090812683
    },
    {
      "name": "Computed tomography",
      "score": 0.20089185237884521
    },
    {
      "name": "Brachytherapy",
      "score": 0.12951210141181946
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I200296433",
      "name": "Chinese Academy of Medical Sciences & Peking Union Medical College",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I37461747",
      "name": "Wuhan University",
      "country": "CN"
    }
  ],
  "cited_by": 15
}