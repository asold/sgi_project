{
  "title": "Multi-task Active Learning for Pre-trained Transformer-based Models",
  "url": "https://openalex.org/W4312605747",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5044349276",
      "name": "Guy Rotman",
      "affiliations": [
        "Technion – Israel Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5054952724",
      "name": "Roi Reichart",
      "affiliations": [
        "Technion – Israel Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3116512121",
    "https://openalex.org/W2890114324",
    "https://openalex.org/W2148972377",
    "https://openalex.org/W6751425476",
    "https://openalex.org/W3104939451",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6729856301",
    "https://openalex.org/W2798935874",
    "https://openalex.org/W2798803565",
    "https://openalex.org/W3105522431",
    "https://openalex.org/W2056894934",
    "https://openalex.org/W6617145748",
    "https://openalex.org/W2786660442",
    "https://openalex.org/W3113477245",
    "https://openalex.org/W6739651123",
    "https://openalex.org/W2105410942",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2088911157",
    "https://openalex.org/W2962892701",
    "https://openalex.org/W6676934727",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W3099142828",
    "https://openalex.org/W2962798796",
    "https://openalex.org/W3034430043",
    "https://openalex.org/W6797440953",
    "https://openalex.org/W2509700522",
    "https://openalex.org/W2798778171",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W6766673545",
    "https://openalex.org/W2135694298",
    "https://openalex.org/W2963718112",
    "https://openalex.org/W6742080785",
    "https://openalex.org/W2997876626",
    "https://openalex.org/W3169250374",
    "https://openalex.org/W6775935039",
    "https://openalex.org/W6763087592",
    "https://openalex.org/W2963505445",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2091671846",
    "https://openalex.org/W6769627184",
    "https://openalex.org/W6674306132",
    "https://openalex.org/W2095887211",
    "https://openalex.org/W6678510481",
    "https://openalex.org/W2994806031",
    "https://openalex.org/W6739365718",
    "https://openalex.org/W6778477900",
    "https://openalex.org/W2963168538",
    "https://openalex.org/W2964262197",
    "https://openalex.org/W2171671120",
    "https://openalex.org/W2080021732",
    "https://openalex.org/W2785787385",
    "https://openalex.org/W2516255829",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2970121940",
    "https://openalex.org/W6604258887",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3098749353",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2888868336",
    "https://openalex.org/W3034884160",
    "https://openalex.org/W3114735516",
    "https://openalex.org/W2964117975",
    "https://openalex.org/W2948194985",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2612690371",
    "https://openalex.org/W4238846128",
    "https://openalex.org/W2624871570",
    "https://openalex.org/W128638292",
    "https://openalex.org/W2912083425",
    "https://openalex.org/W4297683418",
    "https://openalex.org/W2913332048"
  ],
  "abstract": "Abstract Multi-task learning, in which several tasks are jointly learned by a single model, allows NLP models to share information from multiple annotations and may facilitate better predictions when the tasks are inter-related. This technique, however, requires annotating the same text with multiple annotation schemes, which may be costly and laborious. Active learning (AL) has been demonstrated to optimize annotation processes by iteratively selecting unlabeled examples whose annotation is most valuable for the NLP model. Yet, multi-task active learning (MT-AL) has not been applied to state-of-the-art pre-trained Transformer-based NLP models. This paper aims to close this gap. We explore various multi-task selection criteria in three realistic multi-task scenarios, reflecting different relations between the participating tasks, and demonstrate the effectiveness of multi-task compared to single-task selection. Our results suggest that MT-AL can be effectively used in order to minimize annotation efforts for multi-task NLP models.1",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.876664400100708
    },
    {
      "name": "Annotation",
      "score": 0.788257360458374
    },
    {
      "name": "Artificial intelligence",
      "score": 0.708719789981842
    },
    {
      "name": "Transformer",
      "score": 0.7048543691635132
    },
    {
      "name": "Task (project management)",
      "score": 0.6611998677253723
    },
    {
      "name": "Machine learning",
      "score": 0.5571576356887817
    },
    {
      "name": "Multi-task learning",
      "score": 0.5247961282730103
    },
    {
      "name": "Natural language processing",
      "score": 0.4760986566543579
    },
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.4504455626010895
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I174306211",
      "name": "Technion – Israel Institute of Technology",
      "country": "IL"
    }
  ]
}