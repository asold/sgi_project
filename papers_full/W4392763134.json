{
  "title": "Large language models are able to downplay their cognitive abilities to fit the persona they simulate",
  "url": "https://openalex.org/W4392763134",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2241608174",
      "name": "Jiří Milička",
      "affiliations": [
        "Charles University"
      ]
    },
    {
      "id": "https://openalex.org/A2494607622",
      "name": "Anna Marklová",
      "affiliations": [
        "Humboldt-Universität zu Berlin"
      ]
    },
    {
      "id": "https://openalex.org/A5094141200",
      "name": "Klára VanSlambrouck",
      "affiliations": [
        "Charles University"
      ]
    },
    {
      "id": "https://openalex.org/A4306299201",
      "name": "Eva Pospíšilová",
      "affiliations": [
        "Charles University"
      ]
    },
    {
      "id": "https://openalex.org/A4213603699",
      "name": "Jana Šimsová",
      "affiliations": [
        "Charles University"
      ]
    },
    {
      "id": "https://openalex.org/A5094141202",
      "name": "Samuel Harvan",
      "affiliations": [
        "Charles University"
      ]
    },
    {
      "id": "https://openalex.org/A5094141203",
      "name": "Ondřej Drobil",
      "affiliations": [
        "Charles University"
      ]
    },
    {
      "id": "https://openalex.org/A2241608174",
      "name": "Jiří Milička",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2494607622",
      "name": "Anna Marklová",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5094141200",
      "name": "Klára VanSlambrouck",
      "affiliations": [
        "Charles University"
      ]
    },
    {
      "id": "https://openalex.org/A4306299201",
      "name": "Eva Pospíšilová",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4213603699",
      "name": "Jana Šimsová",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5094141202",
      "name": "Samuel Harvan",
      "affiliations": [
        "Charles University"
      ]
    },
    {
      "id": "https://openalex.org/A5094141203",
      "name": "Ondřej Drobil",
      "affiliations": [
        "Charles University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3160638507",
    "https://openalex.org/W2028520710",
    "https://openalex.org/W4388488609",
    "https://openalex.org/W4362655923",
    "https://openalex.org/W2171847059",
    "https://openalex.org/W4387378202",
    "https://openalex.org/W4385572854",
    "https://openalex.org/W2093410327",
    "https://openalex.org/W2139188400",
    "https://openalex.org/W4292947474",
    "https://openalex.org/W4320481230",
    "https://openalex.org/W4307475457",
    "https://openalex.org/W4389519968",
    "https://openalex.org/W4387534155",
    "https://openalex.org/W4296154596",
    "https://openalex.org/W2119224589",
    "https://openalex.org/W2085012992",
    "https://openalex.org/W2041886670",
    "https://openalex.org/W6749547985",
    "https://openalex.org/W4239427658",
    "https://openalex.org/W2208074847",
    "https://openalex.org/W2086284328",
    "https://openalex.org/W1990410386",
    "https://openalex.org/W2781589066",
    "https://openalex.org/W2122820712",
    "https://openalex.org/W597088650",
    "https://openalex.org/W2057883602",
    "https://openalex.org/W2019283755",
    "https://openalex.org/W2079956056",
    "https://openalex.org/W2082513494",
    "https://openalex.org/W1910113017",
    "https://openalex.org/W4210416677",
    "https://openalex.org/W2227701948",
    "https://openalex.org/W2061594239",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2500977095",
    "https://openalex.org/W2107745473",
    "https://openalex.org/W2060108852",
    "https://openalex.org/W2024171325",
    "https://openalex.org/W2164777277",
    "https://openalex.org/W4365445794",
    "https://openalex.org/W1952154980",
    "https://openalex.org/W4321455981",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W2333916262",
    "https://openalex.org/W2316212266",
    "https://openalex.org/W4230960895",
    "https://openalex.org/W4212774754",
    "https://openalex.org/W3026354358",
    "https://openalex.org/W4205465429",
    "https://openalex.org/W4400681340",
    "https://openalex.org/W4385988359",
    "https://openalex.org/W2103049138",
    "https://openalex.org/W4308244910",
    "https://openalex.org/W4320499441"
  ],
  "abstract": "This study explores the capabilities of large language models to replicate the behavior of individuals with underdeveloped cognitive and language skills. Specifically, we investigate whether these models can simulate child-like language and cognitive development while solving false-belief tasks, namely, change-of-location and unexpected-content tasks. GPT-3.5-turbo and GPT-4 models by OpenAI were prompted to simulate children (N = 1296) aged one to six years. This simulation was instantiated through three types of prompts: plain zero-shot, chain-of-thoughts, and primed-by-corpus. We evaluated the correctness of responses to assess the models’ capacity to mimic the cognitive skills of the simulated children. Both models displayed a pattern of increasing correctness in their responses and rising language complexity. That is in correspondence with a gradual enhancement in linguistic and cognitive abilities during child development, which is described in the vast body of research literature on child development. GPT-4 generally exhibited a closer alignment with the developmental curve observed in ‘real’ children. However, it displayed hyper-accuracy under certain conditions, notably in the primed-by-corpus prompt type. Task type, prompt type, and the choice of language model influenced developmental patterns, while temperature and the gender of the simulated parent and child did not consistently impact results. We conducted analyses of linguistic complexity, examining utterance length and Kolmogorov complexity. These analyses revealed a gradual increase in linguistic complexity corresponding to the age of the simulated children, regardless of other variables. These findings show that the language models are capable of downplaying their abilities to achieve a faithful simulation of prompted personas.",
  "full_text": "RESEA RCH ARTICL E\nLarge language models are able to downplay\ntheir cognitive abilities to fit the persona they\nsimulate\nJiřı ´ Milička\n1\n, Anna Marklova ´\nID\n2\n*, Kla ´ ra VanSlambrouck\n3☯\n, Eva Pospı ´ s ˇ ilova ´\n3☯\n,\nJana S\nˇ\nimsova ´\n3☯\n, Samuel Harvan\n3☯\n, Ondřej Drobil\n3☯\n1 Institute of the Czech National Corpus, Faculty of Arts, Charles University , Prague, Czech Republic,\n2 Department of Slavic and Hungari an Studies, Faculty of Language, Literat ure and Huma nities, Humboldt\nUniversity of Berlin, Berlin, Germany, 3 Faculty of Arts, Charles University, Prague, Czech Republic\n☯ These authors contribu ted equally to this work.\n* marklova @hu-berlin.d e\nAbstract\nThis study explores the capabilities of large language models to replicate the behavior of\nindividuals with underdeveloped cognitive and language skills. Specifically, we investigate\nwhether these models can simulate child-like language and cognitive development while\nsolving false-belief tasks, namely, change-of-location and unexpected-content tasks. GPT-\n3.5-turbo and GPT-4 models by OpenAI were prompted to simulate children (N = 1296)\naged one to six years. This simulation was instantiated through three types of prompts: plain\nzero-shot, chain-of-thou ghts, and primed-by-cor pus. We evaluated the correctness of\nresponses to assess the models’ capacity to mimic the cognitive skills of the simulated chil-\ndren. Both models displayed a pattern of increasing correctness in their responses and ris-\ning language complexity. That is in corresponden ce with a gradual enhancement in\nlinguistic and cognitive abilities during child development, which is described in the vast\nbody of research literature on child developme nt. GPT-4 generally exhibited a closer align-\nment with the developmental curve observed in ‘real’ children. However, it displayed hyper-\naccuracy under certain conditions, notably in the primed-by-corpus prompt type. Task type,\nprompt type, and the choice of language model influenced developmenta l patterns, while\ntemperature and the gender of the simulated parent and child did not consistently impact\nresults. We conducted analyses of linguistic complexity, examining utterance length and\nKolmogorov complexity. These analyses revealed a gradual increase in linguistic complexity\ncorresponding to the age of the simulated children, regardless of other variables. These\nfindings show that the language models are capable of downplaying their abilities to achieve\na faithful simulation of prompted personas.\nIntroduction\nAs the language models are scaled up, new unexpected capabilities emerge. That means that\nthe users (and sometimes even their developers) cannot predict in advance the full scope of\nPLOS ONE\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02985 22 March 13, 2024 1 / 25\na1111111111\na1111111111\na1111111111\na1111111111\na1111111111\nOPEN ACCESS\nCitation: Milička J, Marklova ´ A, VanSlam brouck K,\nPospı ´ s ˇ ilova ´ E, S\nˇ\nimsova ´ J, Harvan S, et al. (2024)\nLarge language models are able to downplay their\ncognitive abilities to fit the persona they simulate.\nPLoS ONE 19(3): e0298522. https://doi.o rg/\n10.1371/ journal.pone. 0298522\nEditor: Hossein Hassani, Univers ity of Kurdistan\nHewler, IRAQ\nReceived: October 9, 2023\nAccepted: January 26, 2024\nPublished: March 13, 2024\nCopyright: © 2024 Milička et al. This is an open\naccess article distributed under the terms of the\nCreative Commons Attribution License, which\npermits unrestricte d use, distribu tion, and\nreproduction in any medium, provided the original\nauthor and source are credited.\nData Availabilit y Statement: All relevant data are\nwithin the manuscript and its Support ing\ninformation files.\nFunding: The research was supported by Czech\nScience Foundation Grant No. 24-11725S https://\ngacr.cz/. The initials of the authors supporte d: JM,\nKVS, EP, JS, SH, OD. The funders had no role in\nstudy design, data collection and analysis, decision\nto publish, or preparation of the manuscript.\nCompeting interests : NO.\ntheir abilities based solely on their training objectives, as pointed out by [1]. Consequently, it\nbecomes necessary to determine their capabilities post-hoc. Such studies are currently being\nconducted and published extensively in traditional scientific journals, preprints on arXiv, and\nblog posts.\nOccasionally, a publication might assert that large language models (LLMs) are weak at\nsome capability, only for someone else to use more effective prompting and demonstrate that\nthe models are quite fluent at that task [2].\nThis inconsistency is not surprising, as currently, we are unable to probe the latent space of\nlanguage models directly. Instead, we are limited to study agents based on these language mod-\nels. The language model itself has no agency (in the traditional cybernetic sense, as used in\n[3]). We instantiate an agent by setting the initial conditions (prompting) and employ the\nmodel to predict continuation based on these initial conditions. If the initial conditions specify\nagents present in the model’s training dataset, the continuation serves as a simulation of those\nagents. Such agents are usually called ‘personas,’ although they are not only persons but can be\nwhatever real or fictional agent the user needs: a helpful AI assistant, a thermostat, a hobbit, an\necosystem. . . What we typically refer to as the capabilities of a language model are, in essence,\nthe capabilities of the persona simulated via the model. Naturally, these capabilities hinge on\nthe model’s characteristics: whether its architecture is sufficiently versatile, if the training data\nencapsulates ample information on how the persona should behave, and whether the model\ncan extrapolate a persona based on a combination of training data and prompt. However, the\nstudied capabilities depend inherently on the attributes of the simulated persona.\nFor instance, if we interact with the default persona simulated by ChatGPT-4 and pose a\nsimple arithmetic question, it answers correctly and precisely. However, if we craft a prompt\nto create a persona that explicitly lacks access to a calculator, it will behave as if it cannot arrive\nat an accurate result. This mirrors a real human’s behavior without a calculator (see Fig 1).\nThese observations and terminology are based on the simulation theory, initially introduced\nby a pseudonymous blog post by Janus [4] and later elaborated upon in scholarly literature [5].\nThe theory explicitly states that the agency resides not in the model itself but in the simula-\ncrum instantiated by the prompt. As [5] references [4], “To better reflect this distributional\nproperty, we can think of an LLM as a non-deterministic simulator capable of role-playing an\ninfinity of characters, or, to put it another way, capable of stochastically generating an infinity\nof simulacra”.\nSome language model interfaces permit users to construct personas purely based on their\nown prompting (e.g., Davinci-002 model by OpenAI, accessible via API), while others provide\na default persona (e.g., the helpful assistant in ChatGPT). In the case of ChatGPT, the final per-\nsona is collaboratively shaped through additional reinforcement learning from human feed-\nback (RLHF), an unchangeable system prompt, and finally, the interactions initiated by the\nuser.\nWhen users or researchers evaluate LLMs, it is crucial to recognize the role of the specific\npersona being simulated at that moment. Claiming that a language model lacks a certain capa-\nbility is an inappropriate act of anthropomorphization. It is preferable to assert that the perso-\nnas, simulated with the assistance of a specific language model, lack a certain capability (even\nthough they should possess it in the real world). Anthropomorphizing human-like simulacra\ncan be a useful shorthand, assisting us in predicting their behavior, in contrast to anthropo-\nmorphizing language models themselves [2].\nThat leads us to the primary focus of this article: To what extent can we effectively employ\nLLMs to simulate personas characterized by limited cognitive capacities? Certainly, the cogni-\ntive abilities of the personas cannot surpass the limitations inherent to the architecture and\ntraining data of the model. For instance, models like Ada-002 (by OpenAI) have a ceiling\nPLOS ONE\nLLMs downplay their cognitive abilities to fit the persona they simulate\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02985 22 March 13, 2024 2 / 25\nwhen performing basic arithmetic operations beyond a certain threshold, even if they are quite\nadept at mimicking mathematics educators. While this upper limit has been extensively inves-\ntigated and discussed in numerous articles [6], our current interest lies in the opposite ques-\ntion: Are we able to credibly simulate personas that have cognitive abilities considerably below\nthis limit? Can we use LLMs to simulate inabilities that are subtler than, for instance, a persona\nlacking a calculator as in Fig 1?\nThe human brain is capable of this simulation, meaning that people can downplay their\ncognitive abilities. This study tests the hypothesis that LLMs possess similar adaptive\ncapabilities.\nIt is necessary to examine personas that are, by definition, differentiated by their cognitive\nabilities and whose behavior is represented in the training data of existing LLMs. We selected\nchildren of various ages as ideal personas since a child’s cognitive level depends on their age,\nand these differences are well-documented in the vast body of cognitive developmental litera-\nture. Observing simulated children, we will monitor two competencies:\nLinguistic Ability: We are interested in how the language model can adapt the complexity of\nlinguistic expression to match the presumed abilities of the simulated speaker.\nMental State Understanding: We test the ability of the simulated personas to simulate the\nmental states of other entities.\nWe chose the Theory of Mind (ToM) framework as an ideal candidate for the exploration\nof mental state understanding since it showcases pronounced age-related differences in\nFig 1. GPT-4 simulating a father who explicitly does not have a calculator at his dispos al, default system prompt, 2023/10/9. Available at https://chat .\nopenai.com/s hare/9c9e29 54-b95f-4c79 -9b2a-25321 8c6fa96.\nhttps://doi.o rg/10.1371/j ournal.pone .0298522.g001\nPLOS ONE\nLLMs downplay their cognitive abilities to fit the persona they simulate\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02985 22 March 13, 2024 3 / 25\nperformance, providing us with a gradient to study. Also, the performance in so-called false-\nbelief tasks (a common way of testing ToM) is well-studied and documented in the cognitive\ndevelopmental literature (see, e.g., [7] for an overview).\nFor both competencies, linguistic and cognitive, we anticipate a progressive increase of\ncapabilities correlating with the increasing age of the simulated child personas. This progres-\nsion would ideally mimic real-world linguistic and cognitive developmental patterns observed\nin children. If LLMs can effectively simulate this gradient of development in their responses, it\nwould validate their capability to adapt to different cognitive levels based on the prompts\nprovided.\nState of the art\nRecent works on LLMs’ ability to simulate human reasoning and cognitive skills have brought\nmixed findings. [8] identified an ability of GPT-3.5 to manifest human-like intuitive behavior\nin various cognitive tests. [9] found LLMs failing in abstract reasoning in a similar manner as\nhumans. LLMs’ ability to understand and simulate ToM has been investigated. ToM is the abil-\nity to understand and reason about the mental states and intents of others. It enables people to\npredict behavior and understand the reasoning of others [10]. [10] tested GPT-3 in the varia-\ntion of a classical false-belief task [11, 12] and discovered that compared to 90–100% accuracy\nof human subjects, GPT-3 models peaked at 60% accuracy. In a social commonsense and emo-\ntional intelligence test, the model achieved 55% accuracy, compared to more than 85% accu-\nracy of human subjects. [13] compared responses of several LLMs to false-belief tasks\n(‘unexpected transfer’ and ‘unexpected content’) and discovered a high improvement in the\naccuracy of solving these tasks in ChatGPT-4. In their experiment, the accuracy of Davinci-\n001 and Davinci-002 was 10%; Davinci-003 and GPT-3.5-turbo achieved 35% of accuracy; and\nGPT-4 solved successfully 90% of the tasks. The authors theorized that ToM has spontaneously\nemerged as a byproduct of rapidly improving the language skills of LLMs.\nThe studies mentioned above simply asked the LLMs the questions to gain the responses;\nthey did not prompt the models to simulate some specific persona. [14] introduce a so-called\nTuring experiments. In these experiments, the LLM is prompted to simulate personas, which\nthen serve as human participants. Therefore, the question that these experiments can answer is\nnot if an LLM is able to answer tasks as humans but if it can faithfully simulate aspects of\nhuman behavior. The authors claim that these simulations should be zero-shot. They applied\nthis design to Ultimatum Game (a share of money is offered to a participant, who can either\naccept or reject it based on the appropriateness of the split), Garden Path Sentences (participant\nhas to decide if a sentence with confusing parsing is grammatical or not), Milgram Shock\nExperiment (participants are instructed to punish a learner with an electric shock if the learner\ndoes not perform well), and Wisdom of Crowds (participants have to estimate a numerical\nvalue of a general-knowledge question) tasks, and they yielded results comparable to human\nsubjects in the three first test, including gender-sensitive chivalry effect in the Ultimatum\ngame. Additionally, they found surprising hyper-accuracy distortion for the Ultimatum game\nin the recent GPT models. They explain this distortion as a product of the alignment proce-\ndures, namely Reinforcement learning from human feedback (RLHF), which are not present\nin older models.\nUsing LLMs as representations of humans with various demographic properties has been\nsuggested by other researchers (e.g., [15–19]).\nFollowing this research approach, our inquiry extends beyond evaluating the capacity of\nLarge Language Models (LLMs) to mimic human behavior. It also encompasses the examina-\ntion of their ability to simulate the limitations in cognitive abilities. While it has been\nPLOS ONE\nLLMs downplay their cognitive abilities to fit the persona they simulate\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02985 22 March 13, 2024 4 / 25\ndiscovered that some LLMs exhibit ToM and can answer false belief questions similarly to\nhumans, our interest now lies in their ability to conceal their capabilities when asked. To inves-\ntigate this, we have prompted LLMs to simulate individuals with incomplete cognitive devel-\nopment: children.\nIt is well-documented that children acquire aspects of ToM from their surroundings, as evi-\ndenced by established assessments such as false-belief tasks. Comprehensive meta-analyses of\nToM studies with children are available [7, 20–22], and for a broader perspective on these\nmeta-analyses, refer to [23]. Although there is a prevailing trend indicating an enhancement in\nchildren’s ability to correctly solve false-belief tasks as they age, it is essential to recognize the\npresence of individual differences. These variations are attributed to various factors, including\nhaving siblings, frequently engaging in social-pretend play, or being bilingual [23]. False-belief\ntasks have proven to be effective assessments of ToM abilities since they replicate everyday sce-\nnarios easily comprehensible to children. The results of the meta-analysis have revealed that\nthe specific type of false-belief task employed does not significantly impact children’s\nresponses. Among the commonly used false-belief tasks are the change-of-location and unex-\npected-content tasks [7, refer to Section ‘Prompt type’ for further information]. The progress\nof correct solving of those tasks is rapid between 2.5 and 5.5 years, after which it slows down.\nBy 4.5 years of age, most children assess and reason in those situations correctly [23].\nIn addition to cognitive development, there is a concurrent improvement in language skills.\nThese advancements are well-defined by Brown’s stages of syntactic and morphological devel-\nopment, which establish the expected expressive language abilities in children aged approxi-\nmately 1 to 4 years [24]. One crucial metric for measuring language development is the Mean\nLength of Utterance (MLU), which gradually increases within this age range. Children typi-\ncally produce utterances ranging from 1.0 to 2.0 morphemes at one year of age, which progres-\nsively extend to 3.75–4.5 morphemes by the age of 4. While MLU has faced some critique (e.g.,\n[25–27]), it has remained a valuable indicator of language development (e.g., [28–30]). The\nreflection of linguistic development extends beyond MLU; it encompasses overall complexity.\nThis complexity involves constructing more elaborate sentences and conveying meanings with\ngreater precision [31]. The developmental stages summarized by [32] describe that first lan-\nguage production typically occurs around six months of age with syllable repetition, followed\nby the emergence of single words around one year of age. By two years of age, children com-\nbine two words, leading to the production of short sentences at three years, complex sentences\nat four years, and, eventually, the ability to narrate a brief story after five years of age.\nHypothesis\nOur hypothesis arises from the endeavor to determine whether current language models\ninherently embed maximum cognitive abilities into simulated personas regardless of the con-\ntext, or if they instead aim to faithfully simulate personas, including their cognitive imperfec-\ntions. The motivation behind this is to investigate the possibility that, with the rapid\ndevelopment of models, we might reach a stage where we fail to leverage their full potential,\nsince models’ capabilities are deliberately constrained to simulate human or human-like perso-\nnas, as dictated by the prompt.\nThe central thesis of this paper can be summarized as: It is possible to set initial conditions\nfor large language models such that the resulting simulated personas differ from each other in\ncognitive and linguistic abilities, and these differences are in accordance with the differences\nbetween the entities in real life.\nOur primary hypothesis posits that LLMs are capable of simulating a deficiency in cognitive\nand linguistic capacities. In other words, even though they possess certain capabilities, they\nPLOS ONE\nLLMs downplay their cognitive abilities to fit the persona they simulate\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02985 22 March 13, 2024 5 / 25\ncan simulate personas lacking those abilities. By simulating children, i.e., personas with under-\ndeveloped cognitive and linguistic skills, our aim is to investigate whether LLMs can accurately\nmirror the skills exhibited by a typical human child of a specific age. To test this hypothesis, we\nwill focus on two key variables: language complexity, serving as an indicator of language skills,\nand the accuracy of responses to false belief tasks, which provides insights into the understand-\ning of ToM. Performance in these two domains gradually improves during a child’s develop-\nment, enhancing cognitive task capabilities and the complexity of language production. We\npredict that LLMs will replicate this developmental trajectory within the simulated personas.\nMethodology\nWe operationalized testing this hypothesis in the following manner: We prompted LLMs, initi-\nating a conversation designed to simulate a child’s discussion with an adult. The primary inde-\npendent variable under consideration was the child’s age. Outcomes were assessed regarding\nlinguistic complexity and cognitive performance as the dependent variables.\nSince the results may be influenced by numerous other independent variables, some of\nthem were systematically manipulated to explore the latent space of the models in question.\nThis approach resulted in a total of 1296 independent trials. In each trial, LLMs had the oppor-\ntunity to generate all intermediate responses, not just a final answer, thereby simulating a gen-\nuine dialogue.\nGiven that the contemporary Western LLMs are predominantly trained on English data,\nthe entire experiment was conducted in English. That also allowed us to utilize the extensive\ncorpora of the child’s language in English, which is a part of the CHILDES data bank [33].\nIndependent variables\nAge. We concentrated on children and personas aged 1–6 years, with a granularity of one\nyear. This age range was chosen due to the critical developments in language skills [31] and\nunderstanding of ToM [7, 20–23] that occur during this period.\nWe used data from the following corpora: Bates [34], BernsteinRatner [35], Brown [24],\nDemetras—Trevor [36], Gelman [37], Gleason [38], Higginson [39], HSLLD [40], McCune\n[41], and Morisset [42]. When extracting data, we ensured that children in the 2, 3, 4, and 5\nage groups were no more than two months older than the target age. For 6-year-olds, we\nselected transcripts from children aged 6 years to 6 years 6 months. Regarding 1-year-olds, we\nopted for children at the age of 1 year 6 months, as transcripts from younger children were\nscarce and lacked replicas of child speech.\nPrompt type. We presented LLMs with three prompt types:\n(1) Plain zero-shot prompting: The conversation starts with the line “Here is a transcript of\na conversation between a X-year-old child and her/his mother/father”. The age and the gender\nof the child and the gender of the parent were manipulated (more below). The task scenario\nfollowed immediately after this introduction.\n(2) Chain-of-thought prompting: In this case, the conversation starts with the following\nprompt: “You are an expert in the field of children’s psychological development, possessing\ncomprehensive knowledge in both the theoretical and practical aspects of their language and\ncognitive abilities as understood by science. Could you please share your insights about the\ntheory of mind in children? What is it exactly, and how does it vary with the child’s age?” After\nan LLM generated the answer, the following prompt appeared:\nUsing this information, please continue a conversation between a child of X years and her/\nhis mother/father in the following transcript: The gender of the child and the parent and the\nage of the child were manipulated. After that prompt, one of the task scenarios followed.\nPLOS ONE\nLLMs downplay their cognitive abilities to fit the persona they simulate\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02985 22 March 13, 2024 6 / 25\n(3) Primed-by-corpus prompting: The conversation starts with an excerpt from CHILDES\ncorpus (more in Laboratory protocol (S1 File). Each excerpt consisted of approximately 100\nreplicas, with a permissible variance of ±5 replicas. Notably, no explicit information regarding\nthe child’s age was provided within these passages. Ten excerpts from CHILDES were selected\nfor each age group, and they originated from English-speaking individuals who were monolin-\ngual. After the excerpt, the task scenarios followed.\nPlain zero-shot prompting follows the procedure suggested by [14]. Chain-of-thought\nprompting was employed since previous studies using this methodology report improvement\nin the precision of results [43, 44]. We modified this methodology to align with our research\nobjectives. Specifically, we diverged from the standard practice of requesting the generation of\nintermediate steps and instead focused on soliciting explicit recall of theories that would subse-\nquently be applied to our tasks. In this case, the LLM, therefore, simulates an expert who simu-\nlates a child. Lastly, primed-by-corpus prompting was chosen to prompt the model with\nimplicit rather than explicit information about the desired persona.\nTask. We chose two most common false belief tasks in ToM research:(1) Change-of-loca-\ntion task: We follow a classical scenario suggested by [12]. The series of prompts is as follows:\nParent: Can you remember Maxi, your friend?\nChild:\nParent: Here is a puppet. The puppet is like Maxi, isn’t he?\nChild:\nParent: Maxi has a chocolate, here is his chocolate. . . And Maxi, here, puts the chocolate in\nthe cupboard.\nChild:\nParent: Now Maxi left! Maxi went to a playground.\nChild:\nParent: And here comes his mommy to the cupboard! Here this puppet is his mommy. And\nshe takes the chocolate!\nChild:\nParent: And she gives the chocolate to a drawer. Here.\nChild:\nParent: Now Maxi is back from the playground! And he wants the chocolate. Where will Maxi\nlook?\nChild:\nWhile 3-year-olds often fail this task, from 4 years of age, children become mostly capable\nof recognizing that Maxi did not see the parent hiding the chocolate. Therefore, they answer\ncorrectly that he will look into the cupboard [20].\n(2) Unexpected-content task: This task is a modification of a scenario suggested by [45]. It\ndiffers from the change-of-location task because the child experiences the false belief on their\nown, believing that there are candies in the candy box before the revelation that there are, in\nfact, pencils. The series of prompts are as follows:\nParent: I have something for you!\nPLOS ONE\nLLMs downplay their cognitive abilities to fit the persona they simulate\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02985 22 March 13, 2024 7 / 25\nChild:\nParent: here! look at this candy box!\nChild:\nParent: what do you think is inside?\nChild:\nParent: but see! When I open it, there are pencils inside!\nChild:\nParent: I will close the box now and I will show it to your twin sister, ok?\nChild:\nParent: what will your sister think is inside the box?\nChild:\nThough lacking certainty, it is plausible to assume that these types of tasks were available in\nthe training data of LLMs. These scenarios are widely recognized examples, which was a cru-\ncial aspect of the study, as the aim was to juxtapose the answers of personas with the perfor-\nmance of children. Regarding the potential impact of specific tasks on the results, the large\nmeta-analyses did not discover a significant effect of the task type on the correctness of the\nanswers [7, 20]. Consequently, we can anticipate comparable results across both tasks.\nGender of child and of parent. While our primary focus did not revolve around the influ-\nence of gender, research in language acquisition has revealed correlations between a child’s\ngender and their language skills. Specifically, it has been observed that girls often exhibit a\nslight advantage over boys in this regard (e.g., [46–48]), and boys are recognized as a group\nwith greater variability [49]. Furthermore, various aspects of communication, including narra-\ntion style, choice of speech acts, and negotiation methods, appear to differ according to gender\n[31]. Consequently, we aimed to ensure a balanced representation of gender among our perso-\nnas. Since some studies showed differences between input from female and male caregivers,\ne.g., [50, 51], we balanced the gender of the parent as well.\nThe balancing of the gender was carried out exclusively in the plain zero-shot and chain-of-\nthoughts prompting, where we had the flexibility to manipulate the explicit information pro-\nvided in the initial prompt. However, this was not a viable option in the primed-by-corpus\nprompting method for several reasons. Firstly, the available transcripts exhibited an asymmet-\nrical representation of female caregivers over male caregivers, and the gender distribution\namong the children was also unbalanced. Secondly, the transcripts required manual selection,\nas some children had fewer than two replicas in the randomly chosen excerpts. Lastly, the pri-\nmary purpose of this prompting method was to imply the child’s age through behavioral data,\nand most transcripts did not contain gender information. Implementing gender information\ninto all transcripts would have necessitated altering the original behavioral data, a step we\nchose not to take. Additionally, pragmatically, we did not have access to sufficiently large cor-\npora to ensure gender balance in this method.\nModels. We employed GPT-3.5-turbo and GPT-4 because of their capabilities, popularity,\ncommon real-world usage, and easy access through the API. As of writing this article, GPT-4\nis the most advanced publicly available language model [52].\nAlthough these models are termed large language models, they are utilized more as cogni-\ntive co-processors or reasoning machines. In this context, language primarily serves as an\nPLOS ONE\nLLMs downplay their cognitive abilities to fit the persona they simulate\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02985 22 March 13, 2024 8 / 25\ninterface, and other cognitive abilities are more important. Currently, other cognitive capabili-\nties are being tested more extensively than the linguistic ones, c.f. [52].\nReinforcement Learning from Human Feedback (RLHF) enhances the cognitive abilities of\nLLMs in certain tasks that are in demand and makes the model more pleasant to use [53].\nHowever, applying RLHF in GPT-4 has been identified as a potential reason for certain anom-\nalies, particularly in the manifestation of hyper-accuracy [14]. The distinction between GPT-\n3.5-turbo and GPT-4 is not just in their size and architecture (of which GPT-4’s hasn’t been\ndisclosed to general public), but also in the nature and degree of RLHF they underwent.\nTemperature. The model determines the likelihood of each possible token being a contin-\nuation of the given text (note that in language modelling termonology, token means character\nn-gram selected as a tokenization unit, not a part of Peirce’s type-token distinction, in which\ncase likelihood of each possible type would be more appropriate). Subsequently, an output\ntoken is randomly selected from this probability distribution. This selection mechanism is\nskewed, favoring more likely tokens to some extent. The degree to which this occurs can be\nadjusted based on API user requirements, using the temperature parameter. When the temper-\nature is set to 0, the token with the highest probability is always chosen. As the temperature\nvalue increases, less likely tokens also have a chance to be selected. In the OpenAI API docu-\nmentation, it is explicitly stated: “Higher values like 0.8 will make the output more random,\nwhile lower values like 0.2 will make it more focused and deterministic” [54].\nFor the purposes of our experiment, we selected three temperature values to explore how\nthe model’s properties change depending on this crucial parameter. The first value is chosen as\nzero, representing deterministic selection of the most probable token. The second is 0.5, which\nlies in the middle of the suggested range for standard use. The third selected value is 0.9,\nexceeding the typical suggested range.\nDependent variables\nLanguage complexity. We utilized two distinct methods for assessing complexity: first, an\napproximation based on response length, and second, an approximation of the Kolmogorov\ncomplexity.\nResponse length was chosen as a metric because of its prevalent use in the literature on pri-\nmary language acquisition (see above), maintaining continuity with established tradition. This\nmeasure is employed in four figures representing the principal findings in the Results section.\nOur operationalization of this measure is the count of letters in the text generated by the\nmodel as a child’s response. We acknowledge that psycholinguistic studies often employ vari-\nous other operationalizations (e.g., morpheme count), but this simple metric suffices for our\npurposes.\nThe Kolmogorov complexity offers more precise measurement and is widely used in quan-\ntitative linguistics [55]. Kolmogorov complexity, also known as algorithmic complexity [56,\n57], refers to the minimum amount of information required to compress a given string. As\nsuch, it cannot be calculated directly, but only approximated. We approximate the upper\nbounds of the Kolmogorov complexity using text compression via a combination of the LZ77\nalgorithm [58] and Huffman coding [59], amalgamated into RFC1951 [60]. We include only\none figure in the main article based on the results of this measurement, but further results can\nbe found in the Supporting Information (S3 File).\nWe only consider the simulated persona’s responses, excluding parentheses, remarks, and\nannotations.\nTheory of mind. The answers to the two false-belief tasks were analyzed to assess the\nToM understanding of the personas. The first reply following the question “Where will Maxi\nPLOS ONE\nLLMs downplay their cognitive abilities to fit the persona they simulate\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02985 22 March 13, 2024 9 / 25\nlook?” (or What will your sister think is inside the box?, respectively) was classified into four\ncategories as displayed in Table 1.\nThe responses provided by the personas underwent manual coding by an experienced\ncoder. To ensure the reliability of the coding process, 30% of the responses were independently\ncoded by a second coder following the established rules outlined above. Intercoder reliability\nwas assessed using Cohen’s kappa index. The calculated value of Cohen’s kappa index between\nthe two coders was 0.88, indicating that the level of agreement between the coders was ‘almost\nperfect,’ according to the benchmarks for assessing agreement strength proposed by [61].\nThere was a 95% agreement between the two coders.\nFor the analysis of the proportion of correct answers, only ‘relevant’ answers (i.e., mastered\nToM and failing ToM from the Table 1) were included in the proportion of correct answers.\nThe cases when the personas answered with ‘nothing’ or ‘something else’ were analyzed sepa-\nrately. This approach was adopted due to the potential divergence from the evaluation of ToM\nproficiency to a broader assessment of language comprehension. For example, responding to\nthe unexpected content task by suggesting that the sister would anticipate the presence of pen-\ncils in the box signifies a deficiency in ToM. Conversely, answering in a manner suggesting\nshe would assume they are stickers or choosing not to respond at all implies challenges in\ngrasping the entirety of the conversation, indicative of broader difficulties in comprehending\nthe overall discourse.\nStatistical analysis\nAverage values of the observed metrics are presented along with their 95% confidence intervals\n(using bootstrap resampling for numeric variables and exact binomial confidence intervals for\nanalyzing categorical variables). These confidence intervals are valid only when sampling from\nthe same model under identical conditions since even slight changes in the prompt can lead to\nsignificant variations in outcomes, as highlighted by [62]. Compared to human participants,\ninvestigating large language models offers the advantage that individual trials from the same\nmodel are intrinsically independent (there is no memory between trials). This allowed us to\nvary a multitude of variables, thereby exploring the model’s latent space more extensively. This\napproach is more beneficial than sampling multiple outcomes for the same prompt with iden-\ntical settings—for a temperature of 0, the outcome is deterministic, while for higher tempera-\ntures, it is derived from the same distribution of outputs.\nIn line with [63], we provide readers with access to the complete dataset and break down\ndata based on individual independent variables that we systematically varied. Supporting\ninformation contains not only the dataset but also additional charts, more than what would be\nfeasible to discuss within the scope of this article(S3 File). We follow the recommendation of\n[63] and provide qualitative instance-by-instance evaluation of some particularly interesting\nresults.\nTable 1. Overview of the classificatio n of the LLMs answers to the false-belie f tasks.\nChange-of- location Unexpect ed-content\nWhere will Maxi look? What will your sister think is inside the box?\nnothing eg., I don’t know! not answering the question, eg., Surprise! ; I want to find out!\nmastered ToM cupboard candy/can dies\nfailing ToM drawer pencils\nsomething else eg., in the kitchen eg., toys, stickers, or listing options\nhttps://d oi.org/10.1371/j ournal.pon e.0298522.t00 1\nPLOS ONE\nLLMs downplay their cognitive abilities to fit the persona they simulate\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02985 22 March 13, 2024 10 / 25\nResults and discussion\nPrior investigations of cognitive capabilities in LLMs have often overlooked the explicit con-\nsideration of the personas the models simulate [8–10, 13]. In contrast, our experiment under-\nscores the pivotal role played by the simulated persona when evaluating the model’s\ncapabilities. Overall, the models followed the developmental tendencies expected in children\nthey simulated: the older the simulated child, the better the performance in language and cog-\nnitive skills.\nWithin the experiment, certain conditions lead to more accurate simulations compared to\nothers. The main discoveries are as follows:\nThe high proportion of correct answers observed in the 6-year-old personas provides affir-\nmation that LLMs can effectively employ ToM (Fig 2). This finding is consistent with the\nresearch by [13]). Nevertheless, in contrast to the outcomes reported in [10, 13], both GPT-4\nand GPT-3.5-turbo achieved high accuracy in their responses. This result might be due to the\nrelative simplicity of the false-belief tasks.\nSimilarly to the observations made by [14], we identified a certain degree of hyper-accuracy\nin GPT-4 in comparison to the preceding model, GPT-3.5-turbo. This distinction was particu-\nlarly prominent in the change of location task. While GPT-3.5-turbo conformed to the pattern\nof progressively improving accuracy with increasing age, GPT-4 exhibited a high proportion\nof correct answers even when simulating 1-year-olds. Since the GPT-4 model is otherwise\nstronger but the two models differ in subsequent tuning, this discovery aligns with the hypoth-\nesis suggesting that the fine tuning and RLHF procedures may cause distortions in model\nbehavior [14]. This leads to an implication for the usage of the LLMs: when users seek adequate\nresults to their prompts, it may be advantageous to opt for the base model (which is, sadly, not\navailable for GPT-4).\nThe analysis uncovered the effect of the prompt type on the correctness of the answers.\nNotably, the priming by CHILDES corpus proved to be the most effective in simulating perso-\nnas of specific age groups. This discovery is of particular interest given that the age categories\nwere not explicitly mentioned in any part of this kind of prompt, and the selected transcripts\nwere drawn from a database containing inherent individual differences. Nevertheless, the\nmodel managed to absorb behavioral cues from the prompts and incorporate them into its\nresponses to false belief tasks. This finding bears significance for advancing the current meth-\nodologies for simulating demographic groups in Turing experiments conducted by LLMs.\nWhile plain zero-shot prompts, as proposed by [14], may offer utility in numerous scenarios,\nthe implicit adjustment of personas, as exemplified by the primed-by-corpus prompts, may\nyield more faithful simulations. Furthermore, the investigation revealed that the chain-of-\nthoughts prompt type (marked as explain in Fig 2 and consequent figures) enhanced the simu-\nlation of child-like behavior in GPT-4 in comparison to the plain zero-shot prompt type. Simi-\nlar outcomes may be anticipated if applied to the Ultimatum game.\nIn contrast to the meta-analyses conducted on false-belief tasks, e.g., [7], we detected differ-\nences between the task types (color-coded in Fig 2). These differences became particularly\napparent when examining the prevalence of irrelevant responses. Notably, across most condi-\ntions, the overall understanding of the unexpected content task appeared to be lower com-\npared to the change of location task.\nOverall, our findings indicate a greater degree of producing irrelevant responses in the\nGPT-3.5-turbo model compared to GPT-4, where the proportion of irrelevant answers\nremained exceptionally low, except for responses generated for 1-year-old personas, which is\nin accordance to the behavior of ‘real’ children (see Fig 3).\nPLOS ONE\nLLMs downplay their cognitive abilities to fit the persona they simulate\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02985 22 March 13, 2024 11 / 25\nA consistent pattern did not emerge regarding the impact of temperature (Fig 4), the gender\nof the child (Fig 5), or the gender of the parent (Fig 6).\nThe examination of language complexity, encompassing both length and Kolmogorov com-\nplexity approximation, revealed a developmental trend in LLMs whereby complexity gradually\nincreased in correspondence with the age of the simulated personas (Figs 7 and 8). This trend\nFig 2. Proportion of correct answers by type of task and age. Charts on the left represent results generated by GPT-3.5-t urbo, and charts on the right\nrepresent results generated by GPT-4. The first line of charts represents the plain zero-shot prompt, the second chain-o f-thoughts prompt, and the third\nrepresents the primed-by-c orpus prompt.\nhttps://doi.org/10 .1371/journal.p one.0298522. g002\nPLOS ONE\nLLMs downplay their cognitive abilities to fit the persona they simulate\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02985 22 March 13, 2024 12 / 25\nmanifested consistently across all models and experimental conditions, although with the\nsmallest magnitude in the case of the primed-by-corpus prompt type. In both LLMs, a notable\nrise was observed between the first and second years of age, after which complexity levels stabi-\nlized. In comparison to plain zero-shot prompt types, the primed-by-corpus prompt type\nexhibited the lowest level of complexity, potentially indicating that the implicit behavioral cues\nFig 3. Proportion of irrelevant answers by type of task and age. Charts on the left represent results generated by GPT-3.5 -turbo, and charts on the right\nrepresent results genera ted by GPT-4. The first line of charts represents the plain zero-shot prompt, the second chain-of-th oughts prompt, and the third\nrepresents the primed- by-corpus prompt.\nhttps://d oi.org/10.1371 /journal.pone. 0298522.g003\nPLOS ONE\nLLMs downplay their cognitive abilities to fit the persona they simulate\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02985 22 March 13, 2024 13 / 25\nderived from primed-by-corpus prompting data suggest relatively lower linguistic capabilities\nthan those theoretically anticipated for children of that age. Additionally, it’s worth noting a\ndistinction in task type, which we interpret as a content-related feature; change of location\ngenerated longer responses than unexpected content task.\nIn general, the ascent in complexity appeared to be less steep in personas generated by\nGPT-4 in contrast to those produced by GPT-3.5-turbo. As in the case of ToM, no clear\nFig 4. Proportion of correct answers by temperatur e and age. Charts on the left represent results generated by GPT-3.5-t urbo, and charts on the right\nrepresent results genera ted by GPT-4. The first line of charts represents the plain zero-shot prompt, the second chain-of-th oughts prompt, and the third\nrepresents the primed- by-corpus prompt.\nhttps://d oi.org/10.1371 /journal.pone. 0298522.g004\nPLOS ONE\nLLMs downplay their cognitive abilities to fit the persona they simulate\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02985 22 March 13, 2024 14 / 25\npattern was observed concerning the effects of temperature (Fig 9), the child’s gender (Fig 10),\nor the parent’s gender (Fig 11).\nIt is worth commenting that the personas in chain-of-thought prompts are rather experts\nsimulating children than children. GPT-3.5-turbo might run into difficulties in simulating\nthese personas properly. That is indicated in the proportion of correct answers in this prompt\nFig 5. Proportion of correct answers by child’s gender and age. Charts on the left represent results generated by GPT-3.5 -turbo, and charts on the right\nrepresent results genera ted by GPT-4. The first line of charts represents the plain zero-shot prompt, the second chain-of-th oughts prompt, and the third\nrepresents the primed- by-corpus prompt.\nhttps://d oi.org/10.1371 /journal.pone. 0298522.g005\nPLOS ONE\nLLMs downplay their cognitive abilities to fit the persona they simulate\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02985 22 March 13, 2024 15 / 25\ntype: unlike GPT-4, it does not follow the expected age-related developmental pattern. Fur-\nthermore, upon closer examination of the data, we identified certain idiosyncrasies unique to\nthe chain-of-thought prompt type when contrasted with the other two methods. Specifically,\nLLMs often not only provided the persona’s response but also included a note within brackets,\nexplaining why the persona had responded in that particular manner (for the analysis of\nFig 6. Proportion of correct answers by parent’s gender and age. Charts on the left represent results generated by GPT-3.5 -turbo, and charts on the right\nrepresent results genera ted by GPT-4. The first line of charts represents the plain zero-shot prompt, the second chain-of-th oughts prompt, and the third\nrepresents the primed- by-corpus prompt.\nhttps://d oi.org/10.1371 /journal.pone. 0298522.g006\nPLOS ONE\nLLMs downplay their cognitive abilities to fit the persona they simulate\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02985 22 March 13, 2024 16 / 25\nlanguage complexity, these notes were not included). Although LLMs provided accurate expla-\nnations of ToM at the beginning of prompting, they were unable to apply them correctly when\npresenting their arguments, seemingly aligning with an interpretation of underdeveloped\nToM:\nFig 7. Length by type of task and age. Charts on the left repres ent results genera ted by GPT-3.5-turb o, and charts on the right represent results generated\nby GPT-4. The first line of charts represents the plain zero-shot prompt, the second chain-of-th oughts prompt, and the third represents the primed- by-\ncorpus prompt.\nhttps://d oi.org/10.1371 /journal.pone. 0298522.g007\nPLOS ONE\nLLMs downplay their cognitive abilities to fit the persona they simulate\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02985 22 March 13, 2024 17 / 25\nMother: Now Maxi is back from the playground! And he wants the chocolate. Where will\nMaxi look?\nChild: Cupboard! (Note: At around two years of age, a child may not yet fully understand the\nconcept that other people have different beliefs and knowledge—that is, Maxi would not\nFig 8. Kolmogoro v complexi ty by type of task and age. Charts on the left represent results generated by GPT-3.5-turb o, and charts on the right represent\nresults genera ted by GPT-4. The first line of charts represents the plain zero-shot prompt, the second chain-o f-thoughts prompt, and the third represents\nthe primed- by-corpus prompt.\nhttps://d oi.org/10.1371 /journal.pone. 0298522.g008\nPLOS ONE\nLLMs downplay their cognitive abilities to fit the persona they simulate\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02985 22 March 13, 2024 18 / 25\nknow his chocolate has been moved. So, a typical two-year-old would predict Maxi will\nlook for the chocolate where it was last left, i.e., the cupboard.)\nThe capacity of LLMs to replicate the gradual enhancement of linguistic abilities was evi-\ndent not only in quantitative assessments but also in specific ways in which the models imi-\ntated children. For instance, when simulating 1-year-olds, the models generated syllable\nFig 9. Length by temperatur e and age. Charts on the left represent results generated by GPT-3.5-turb o, and charts on the right represent results generated\nby GPT-4. The first line of charts represents the plain zero-shot prompt, the second chain-o f-thoughts prompt, and the third represents the primed-by-\ncorpus prompt.\nhttps://doi.org/10 .1371/journal.p one.0298522. g009\nPLOS ONE\nLLMs downplay their cognitive abilities to fit the persona they simulate\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02985 22 March 13, 2024 19 / 25\nrepetitions for words like ‘cupboard’ instead of uttering the complete word, closely mirroring\nthe expected speech production of a 1-year-old child, as described by [32]:\nMother: Now Maxi is back from the playground! And he wants the chocolate. Where will\nMaxi look?\nChild: Cup-cup! Choco!\nFig 10. Length by child’s gender and age. Charts on the left represent results generated by GPT-3.5 -turbo, and charts on the right represent results\ngenera ted by GPT-4. The first line of charts represents the plain zero-shot prompt, the second chain-of-th oughts prompt, and the third represents the\nprimed- by-corpus prompt.\nhttps://d oi.org/10.1371 /journal.pone. 0298522.g010\nPLOS ONE\nLLMs downplay their cognitive abilities to fit the persona they simulate\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02985 22 March 13, 2024 20 / 25\nThese instances occurred exclusively in plain zero-shot prompting and they seemed to be\nmore prevalent in GPT-4 than GPT-3.5-turbo.\nConclusion\nIn this study, our objective was to assess the capability of LLMs to generate personas with lim-\nited cognitive and language skills. Our investigation revealed that LLMs are indeed capable of\nFig 11. Length by parent’s gender and age. Charts on the left represent results generated by GPT-3.5 -turbo, and charts on the right represent results\ngenera ted by GPT-4. The first line of charts represents the plain zero-shot prompt, the second chain-of-th oughts prompt, and the third represents the\nprimed- by-corpus prompt.\nhttps://d oi.org/10.1371 /journal.pone. 0298522.g011\nPLOS ONE\nLLMs downplay their cognitive abilities to fit the persona they simulate\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02985 22 March 13, 2024 21 / 25\nachieving this goal. Furthermore, the cognitive and language deficiencies in the generated per-\nsonas do not occur randomly but mirror patterns observed in the population we simulated.\nOur research validates previous findings regarding the capacity to replicate various demo-\ngraphic groups while also extending these insights with several critical observations.\nFirstly, we demonstrate that LLMs can be used to successfully simulate personas from a cog-\nnitively underdeveloped population, expanding the scope beyond typical adults. Secondly,\nplain zero-shot learning has limitations in simulating specific populations. Implicitly evoking\nparticular properties (such as age) has the potential to yield more successful simulations, and\nemploying a chain-of-thought s prompting can enhance the fidelity of task-solving abilities of\nthe generated personas. Thirdly, there is a difference between prompt success between the two\nmodels; while GPT-4 excelled in simulations based on chain-of-thought s prompting, GPT-\n3.5-turbo yielded the most faithful results from plain zero-shot and primed-by-corpus prompt-\ning. Fourthly, the linguistic development in simulated personas followed the gradual increase\nexpected in real children of the same age.\nOur findings underscore the role of the prompt and the characteristics of the generated\npersonas on the perceived capabilities of the model. Indeed, every test of the model’s ability\nto perform a task is, in reality, a test of the examiner’s skill in defining a persona suitable for\nthe task, their proficiency in locating this persona within the model’s latent space, and the\nmodel’s latent capacity to simulate the persona with sufficient fidelity to accomplish the\ntask. As demonstrated here, language models can simulate personas, including their cogni-\ntive imperfections. This has implications for achieving cognitive abilities through LLMs\nthat surpass human capabilities. Even if an LLM encompasses a more comprehensive world\nmodel than any human, prompting it to simulate a human or human-like expert would not\nresult in super-human behavior, since the human imperfections would be simulated as\nwell.\nSupporting information\nS1 File. Workflow description and other information needed to use the scripts and data\nstored in S2 File to replicate the study.\n(PDF)\nS2 File. Scripts and data needed to replicate the study.\n(ZIP)\nS3 File. Visual representation of all analyses used within this project.\n(ZIP)\nAuthor Contributions\nConceptualization: Jiřı ´ Milička, Anna Marklova ´ , Kla ´ ra VanSlambrouck, Eva Pospı ´ sˇ ilova ´ , Jana\nS\nˇ\nimsova ´ , Samuel Harvan, Ondřej Drobil.\nData curation: Jiřı ´ Milička, Anna Marklova ´ , Kla ´ ra VanSlambrouck, Eva Pospı ´ sˇ ilova ´ , Jana S\nˇ\nim-\nsova ´ , Samuel Harvan, Ondřej Drobil.\nFormal analysis: Jiřı ´ Milička, Anna Marklova ´ .\nInvestigation: Jiřı ´ Milička, Anna Marklova ´ , Kla ´ ra VanSlambrouck, Eva Pospı ´ sˇ ilova ´ , Jana S\nˇ\nim-\nsova ´ , Samuel Harvan, Ondřej Drobil.\nMethodology: Jiřı ´ Milička, Anna Marklova ´ , Kla ´ ra VanSlambrouck, Eva Pospı ´ sˇ ilova ´ , Jana S\nˇ\nim-\nsova ´ , Samuel Harvan, Ondřej Drobil.\nPLOS ONE\nLLMs downplay their cognitive abilities to fit the persona they simulate\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02985 22 March 13, 2024 22 / 25\nProject administration: Jiřı ´ Milička.\nResources: Jiřı ´ Milička, Ondřej Drobil.\nSoftware: Jiřı ´ Milička, Samuel Harvan.\nSupervision: Jiřı ´ Milička, Anna Marklova ´ .\nValidation: Jana S\nˇ\nimsova ´ .\nVisualization: Jiřı ´ Milička.\nWriting – original draft: Jiřı ´ Milička, Anna Marklova ´ , Kla ´ ra VanSlambrouck, Eva Pospı ´ sˇ ilova ´ ,\nJana S\nˇ\nimsova ´ , Samuel Harvan, Ondřej Drobil.\nReferences\n1. Wei J, Tay Y, Bommas ani R, Raffel C, Zoph B, Borgeaud S, et al. Emergent Abilities of Large Language\nModels; 2022.\n2. Reynolds L, McDonell K. Prompt program ming for large language models: Beyond the few-shot para-\ndigm. In: Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing System s;\n2021. p. 1–7.\n3. Wiener N. Cybernetics or Control and Commun ication in the Animal and the Machine. Paris: Hermann\net Cie; 1948.\n4. Janus. Simulato rs; 2023. Available from: https://www .lesswrong.c om/posts /vJFdjigzmc XMhNTs x/\nsimulators.\n5. Shanahan M, McDonell K, Reynolds L. Role-Play with Large Language Models. arXiv preprint\narXiv:230516 367. 2023;.\n6. Liu Y, Han T, Ma S, Zhang J, Yang Y, Tian J, et al. Summary of ChatGPT -Related Research and Per-\nspective Towards the Future of Large Language Models. Meta-R adiology. 2023; p. 1–14.\n7. Milligan K, Astington JW, Dack LA. Langua ge and Theory of Mind: Meta-ana lysis of the Relation\nBetween Language Ability and False-B elief Understand ing. Child Development . 2007; 78(2):622– 646.\nhttps://doi.or g/10.111 1/j.1467-862 4.2007.0 1018.x PMID: 17381794\n8. Hagendor ff T, Fabi S, Kosinski M. Machine intuition: Uncove ring human-like intuitive decision- making in\nGPT-3.5. arXiv preprint arXiv:22120 5206. 2022;.\n9. Dasgupta I, Lampinen AK, Chan SC, Creswe ll A, Kumaran D, McClelland JL, et al. Language models\nshow human-lik e content effects on reasoning. arXiv preprint arXiv:22 0707051. 2022;.\n10. Sap M, Le Bras R, Fried D, Choi Y. Neural Theory- of-Mind? On the Limits of Social Intellig ence in Large\nLMs. In: Procee dings of the 2022 Conferen ce on Empirical Methods in Natural Language Processin g.\nAbu Dhabi, United Arab Emirate s: Association for Comp utational Linguistics; 2022. p. 3762–3780.\nAvailable from: https://aclant hology.org/202 2.emnlp- main.248 .\n11. Baron-Cohen S, Leslie AM, Frith U. Does the autistic child have a “theory of mind”? Cognition . 1985; 21\n(1):37–46. https://doi.o rg/10.1016/00 10-0277(8 5)90022-8 PMID: 293421 0\n12. Wimmer H, Perner J. Beliefs about beliefs: Representa tion and constraining function of wrong beliefs in\nyoung childre n’s understa nding of deception. Cognition. 1983; 13(1):103– 128. https://doi. org/10.1016/\n0010-0277( 83)90004- 5 PMID: 6681741\n13. Kosinski M. Theory of Mind Might Have Sponta neously Emerge d in Large Language Models; arXiv pre-\nprint arXiv:2302.0 2083v5. 2023;.\n14. Aher GV, Arriaga RI, Kalai AT. Using Large Language Models to Simulate Multiple Humans and Repli-\ncate Human Subject Studies. In: Krause A, Brunskil l E, Cho K, Engelha rdt B, Sabato S, Scarlett J, edi-\ntors. Procee dings of the 40th International Conferen ce on Machine Learning. vol. 202 of Proceedings of\nMachine Learning Research. PMLR; 2023. p. 337–371. Availab le from: https://proc eedings.ml r.press/\nv202/ahe r23a.html.\n15. Korinek A. Language models and cognitive automatio n for economic research. National Bureau of Eco-\nnomic Research; 2023.\n16. Park JS, Popowski L, Cai C, Morris MR, Liang P, Bernstein MS. Social simulacra: Creating populated\nprototype s for social computing systems. In: Procee dings of the 35th Annual ACM Symposium on User\nInterface Software and Technology ; 2022. p. 1–18.\n17. Caron G, Srivastava S. Identifying and manipul ating the persona lity traits of language models. arXiv\npreprint arXiv:221210 276. 2022;.\nPLOS ONE\nLLMs downplay their cognitive abilities to fit the persona they simulate\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02985 22 March 13, 2024 23 / 25\n18. Karra SR, Nguyen S, Tulaban dhula T. AI personi fication: Estimating the personality of langua ge mod-\nels. arXiv preprint arXiv:2 20412000. 2022;.\n19. Argyle LP, Busby EC, Fulda N, Gubler JR, Rytting C, Wingate D. Out of one, many: Using language\nmodels to simulate human sample s. Political Analysis. 2023; p. 1–15. https://doi.or g/10.1017/pan .\n2023.2\n20. Wellman HM, Cross D, Watson J. Meta-ana lysis of Theory-o f-Mind Developm ent: The Truth About\nFalse Belief. Child Developm ent. 2001; 72(3):655– 684. https://doi.or g/10.111 1/1467-8624 .00304\nPMID: 114055 71\n21. Liu D, Wellman HM, Tardif T, Sabbagh MA. Theory of Mind Developm ent in Chinese Children: A Meta-\nanalysis of False-B elief Understand ing Across Cultures and Languages . Developmental Psycholog y.\n2008; 44(2):523– 531. https://doi.or g/10.1037/ 0012-1649.4 4.2.523 PMID: 18331141\n22. Slaughter V, Imuta K, Peterson CC, Henry JD. Meta-analys is of theory of mind and peer popularity in\nthe preschoo l and early school years. Child Developm ent. 2015; 86(4):1159 –1174. https://doi.or g/10.\n1111/cde v.12372 PMID: 25874384\n23. Wellman HM. Theory of Mind: The State of the Art. European Journal of Developmental Psychology .\n2018. https://d oi.org/10.108 0/174056 29.2018.14354 13\n24. Brown R. A First Langua ge: The Early Stages. London: George Allen & Unwin; 1973.\n25. Bickerton D. The Pace of Syntactic Acquisitio n. In: Proceedings of the Annual Meetings of the Berkeley\nLinguistics Society. vol. 17; 1991. p. 41–52.\n26. Johnston JR. An Alternate MLU Calculation : Magnitude and Variability of Effects. Journal of Speech ,\nLanguage, and Hearing Research. 2001; 44:156–164. https:// doi.org/10.10 44/1092 -4388(200 1/014)\nPMID: 112180 99\n27. Rollins PR, Snow CE, Willett JB. Predictors of MLU: Semantic and Morpho logical Developm ents. First\nLanguage. 1996; 16:243–259. https:// doi.org/10.11 77/0142 72379601604 705\n28. Klima ES, Bellugi U. Syntactic Regular ities in the Speech of Children. In: Lyons J, Wales RJ, editors.\nPsycholing uistics Papers. Edinburgh : University of Edinburgh Press; 1966. p. 183–20 8.\n29. Ezeizaba rrena MJ, Garcia Fernan dez I. Length of Utterance, in Morphe mes or in Words?: MLU3-w , a\nReliable Measure of Langua ge Development in Early Basque. Frontiers in Psychology. 2018; 8. https://\ndoi.org/10.33 89/fpsyg.201 7.02265 PMID: 29379453\n30. Rice ML, Smolik F, Perpich D, Thomps on T, Rytting N, Blossom M. Mean Length of Utterance Levels in\n6-Month Intervals for Children 3 to 9 Years With and Without Langua ge Impairmen ts. Journal of\nSpeech, Language, and Hearing Research: JSLHR. 2010; 53(2):333– 349. https:// doi.org/10.10 44/\n1092-4388( 2009/08-018 3) PMID: 203604 60\n31. Clark EV. First Language Acquisitio n. Camb ridge, MA: Cambridge University Press; 2003.\n32. Houwer AD. Bilingual First Language Acquisitio n. Bristol, Blue Ridge Summit: Multiling ual Matters;\n2009. Availab le from: https://doi.o rg/10.21832/9 78184769 1507 [cited 2023-10-0 9].\n33. MacWhinney B. The CHILDES Project: Tools for Analyzing Talk. 3rd ed. Mahwah, NJ: Lawrence Erl-\nbaum Associates; 2000.\n34. Bates E, Bretherton I, Snyder L. From First Words to Gramma r: Individual Differences and Dissociable\nMechanism s. Cambridge , MA: Cambridge University Press; 1988.\n35. Bernstein N. Acoustic Study of Mothers’ Speech to Langua ge-Learning Children : An Analysis of Vowel\nArticulatory Characteris tics [PhD]. Boston University . Boston; 1982.\n36. Demetras M. Working Parents’ Conversation al Respo nses to Their Two-Year-O ld Sons [PhD]. The\nUniversity of Arizona. Arizona; 1989.\n37. Gelman SA, Taylor MG, Nguyen S. Mother- Child Conversat ions About Gender: Understand ing the\nAcquisitio n of Essentiali st Beliefs. Monographs of the Society for Research in Child Develop ment.\n2004; 69(1):I–142 .\n38. Bellinger D, Gleaso n J. Sex Differenc es in Parental Directives to Young Children. Journal of Sex Roles.\n1982; 8:1123– 1139. https://doi.or g/10.100 7/BF00290968\n39. Higginson RP. Fixing-As similation in Language Acquisition [PhD]. Washingto n State University . Wash-\nington; 1985.\n40. Dickinson DK, Tabors PO, editors. Beginning Literacy with Language: Young Children Learning at\nHome and School. Baltimore: Paul Brookes Publishing; 2001.\n41. McCune L. A Normative Study of Representa tional Play at the Transition to Langua ge. Development al\nPsychology . 1995; 31(2):198– 206. https://do i.org/10.1037 /0012-1649. 31.2.198\n42. Morisset CE, Barnard KE, Booth CL. Toddler s’ Language Developm ent: Sex Differenc es Within Social\nRisk. Developm ental Psychology . 1995; 31(5):851– 865. https:// doi.org/10.10 37/0012 -1649.31.5.85 1\nPLOS ONE\nLLMs downplay their cognitive abilities to fit the persona they simulate\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02985 22 March 13, 2024 24 / 25\n43. Wei J, Wang X, Schuur mans D, Bosma M, ichter b, Xia F, et al. Chain-o f-Thought Prompting Elicits\nReasoning in Large Language Models. In: Koyejo S, Mohamed S, Agarwa l A, Belgrave D, Cho K, Oh A,\neditors. Advances in Neural Informatio n Processin g Systems. vol. 35. Curran Associates, Inc.; 2022.\np. 24824–248 37. Available from: https://procee dings.neu rips.cc/pape r_files/p aper/2022/fil e/\n9d560961352 4ecf4f15a f0f7b31a bca4-Paper -Confere nce.pdf.\n44. Hebenstrei t K, Praas R, Kiesewetter LP, Samwald M. An automatical ly discove red chain-o f-thought\nprompt generalizes to novel models and datasets. arXiv preprint arXiv:2305.0 2897v2. 2023;.\n45. Perner J, Leekam S, Wimmer H. Three-year- olds’ difficulty with false belief: The case for a conceptual\ndeficit. British Journal of Developm ental Psychology . 1987; 5:125–137. https://doi.or g/10.1111/j. 2044-\n835X.1987. tb01048.x\n46. Hines M. Sex and Sex Differences . In: Zelazo PD, editor . The Oxford Handbook of Develo pmental Psy-\nchology. vol. 1. New York: Oxford University Press; 2013. p. 164–201.\n47. Eriksson M, Marsc hik PB, Tulviste T, Almgren M, Pe ´ rez Pereira M, Wehber g S, et al. Differences\nBetween Girls and Boys in Emergi ng Language Skills: Evidence from 10 Language Commun ities. Brit-\nish Journal of Development al Psych ology. 2012; 30(2):326– 343. https://doi.or g/10.1111 /j.2044-835X .\n2011.02042 .x PMID: 225509 51\n48. Fenson L, Dale PS, Reznick SJ, Bates E, Thal DJ, Pethick SJ. Variability in Early Communic ative\nDevelopm ent. Monograph s of the Society for Research in Child Development . 1994; 59(5):1. https://\ndoi.org/10.23 07/1166093 PMID: 784541 3\n49. Lange B, Euler H, Zaretsky E. Sex differe nces in language compet ence of 3- to 6-year-old children.\nApplied Psycholing uistics. 2016; -1:1–22. https://doi.or g/10.101 7/S01427164 15000624\n50. Pancsofar N, Vernon-Fe agans L. Mother and father language input to young children: Contributi ons to\nlater language develop ment. Journal of Applied Developmen tal Psychology. 2006; 27:571–587. https://\ndoi.org/10.10 16/j.appdev. 2006.08.003\n51. Gilkerson J, Richards JA. The Power of Talk, Second Edition. Boulde r, CO: LENA Founda tion; 2016.\nAvailable from: https://www .lena.org/w p-content /uploads/2016 /07/LTR-01 -2_PowerO fTalk.pdf.\n52. Bubeck S, Chandrase karan V, Eldan R, Gehrke J, Horvitz E, Kamar E, et al. Sparks of artificial general\nintelligence : Early experimen ts with gpt-4. arXiv preprint arXiv:230312 712. 2023;.\n53. Ouyang L, Wu J, Jiang X, Almeida D, Wainwrig ht C, Mishkin P, et al. Training language models to follow\ninstruction s with human feedback. Advances in Neural Informatio n Processin g Systems. 2022;\n35:27730–2 7744.\n54. OpenAI. Chat API Reference; 2023. Availab le from: https://plat form.openai. com/docs/a pi-reference /\nchat.\n55. Juola P. Asses sing linguistic complexity. Langua ge Complexity : Typology, Contact, Change John Ben-\njamins Press, Amsterda m, Netherlan ds. 2008;.\n56. Kolmogo rov A. Three Approaches to the Quantitativ e Definition of Information . Problems of Informatio n\nTransmiss ion. 1965; 1(1):1–7.\n57. Li M, Vita ´ nyi P. An Introducti on to Kolmogorov Complexity and Its Applicat ions. New York, Springer;\n2013.\n58. Ziv J, Lempel A. A universal algorithm for sequentia l data compress ion. IEEE Transaction s on informa-\ntion theory. 1977; 23(3):337– 343. https://d oi.org/10.110 9/TIT.1977.1 055714\n59. Huffman DA. A method for the construction of minimum -redundan cy codes. Procee dings of the IRE.\n1952; 40(9):1098 –1101. https://doi.or g/10.1109 /JRPROC.1 952.273898\n60. Deutsch LP. DEFLATE Compressed Data Format Specificati on version 1.3; 1996. RFC 1951. Available\nfrom: https://www .rfc-editor.or g/info/rfc1 951.\n61. Landis JR, Koch GG. The Measuremen t of Observer Agreement for Categorica l Data. Biometr ics.\n1977; 33(1):159– 74. https://doi.or g/10.2307/ 2529310 PMID: 843571\n62. Zhou Y, Muresanu AI, Han Z, Paster K, Pitis S, Chan H, et al. Large Language Models Are Human-\nLevel Prompt Engineers; 2023.\n63. Burnell R, Schella ert W, Burden J, Ullman TD, Martinez- Plumed F, Tenenbau m JB, et al. Rethink\nreporting of evaluatio n results in AI. Science. 2023; 380(6641) :136–138. https://doi.or g/10.112 6/\nscience.adf 6369 PMID: 37053341\nPLOS ONE\nLLMs downplay their cognitive abilities to fit the persona they simulate\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02985 22 March 13, 2024 25 / 25",
  "topic": "Cognition",
  "concepts": [
    {
      "name": "Cognition",
      "score": 0.6812920570373535
    },
    {
      "name": "Utterance",
      "score": 0.6420746445655823
    },
    {
      "name": "Correctness",
      "score": 0.5781539678573608
    },
    {
      "name": "Replicate",
      "score": 0.5587481260299683
    },
    {
      "name": "Cognitive complexity",
      "score": 0.552100419998169
    },
    {
      "name": "Cognitive psychology",
      "score": 0.5220760703086853
    },
    {
      "name": "Psychology",
      "score": 0.47345465421676636
    },
    {
      "name": "Computer science",
      "score": 0.46770450472831726
    },
    {
      "name": "Language model",
      "score": 0.4582708179950714
    },
    {
      "name": "Language development",
      "score": 0.4406932592391968
    },
    {
      "name": "Cognitive model",
      "score": 0.42845454812049866
    },
    {
      "name": "Developmental psychology",
      "score": 0.3709743916988373
    },
    {
      "name": "Natural language processing",
      "score": 0.34831127524375916
    },
    {
      "name": "Artificial intelligence",
      "score": 0.304580956697464
    },
    {
      "name": "Mathematics",
      "score": 0.14392733573913574
    },
    {
      "name": "Statistics",
      "score": 0.14356708526611328
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I21250087",
      "name": "Charles University",
      "country": "CZ"
    },
    {
      "id": "https://openalex.org/I39343248",
      "name": "Humboldt-Universität zu Berlin",
      "country": "DE"
    }
  ]
}