{
  "title": "JARVIS: Joining Adversarial Training With Vision Transformers in Next-Activity Prediction",
  "url": "https://openalex.org/W4388709737",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2566929661",
      "name": "Vincenzo Pasquadibisceglie",
      "affiliations": [
        "University of Bari Aldo Moro"
      ]
    },
    {
      "id": "https://openalex.org/A135896968",
      "name": "Annalisa Appice",
      "affiliations": [
        "University of Bari Aldo Moro"
      ]
    },
    {
      "id": "https://openalex.org/A2140191242",
      "name": "Giovanna Castellano",
      "affiliations": [
        "University of Bari Aldo Moro"
      ]
    },
    {
      "id": "https://openalex.org/A2328364094",
      "name": "Donato Malerba",
      "affiliations": [
        "University of Bari Aldo Moro"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2581522324",
    "https://openalex.org/W2969940319",
    "https://openalex.org/W3120405191",
    "https://openalex.org/W3083653865",
    "https://openalex.org/W3092305337",
    "https://openalex.org/W3091710143",
    "https://openalex.org/W2884606998",
    "https://openalex.org/W3094626345",
    "https://openalex.org/W3197963420",
    "https://openalex.org/W4318825842",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3191453585",
    "https://openalex.org/W4291414590",
    "https://openalex.org/W2590019597",
    "https://openalex.org/W2976975156",
    "https://openalex.org/W2584722588",
    "https://openalex.org/W2268451814",
    "https://openalex.org/W2575990627",
    "https://openalex.org/W3127147465",
    "https://openalex.org/W2966756122",
    "https://openalex.org/W3208260548",
    "https://openalex.org/W3036646401",
    "https://openalex.org/W6792727765",
    "https://openalex.org/W2886950943",
    "https://openalex.org/W4378221148",
    "https://openalex.org/W3135069918",
    "https://openalex.org/W4386874052",
    "https://openalex.org/W3217587586",
    "https://openalex.org/W3083147251",
    "https://openalex.org/W6636510571",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W6780176858",
    "https://openalex.org/W6640425456",
    "https://openalex.org/W3035422918",
    "https://openalex.org/W4380445873",
    "https://openalex.org/W6734432047",
    "https://openalex.org/W6676179485",
    "https://openalex.org/W4293846201",
    "https://openalex.org/W2243397390",
    "https://openalex.org/W2593418474",
    "https://openalex.org/W3140537353"
  ],
  "abstract": "In this paper, we propose a novel predictive process monitoring approach, named JARVIS, that is designed to achieve a balance between accuracy and explainability in the task of next-activity prediction. To this aim, JARVIS represents different process executions (traces) as patches of an image and uses this patch-based representation within a multi-view learning scheme combined with Vision Transformers (ViTs). Using multi-view learning we guarantee good accuracy by leveraging the variety of information recorded in event logs as different patches of an image. The use of ViTs enables the integration of explainable elements directly into the framework of a predictive process model trained to forecast the next trace activity from the completed events in a running trace by utilizing self-attention modules that give paired attention values between two picture patches. Attention modules disclose explainable information concerning views of the business process and events of the trace that influenced the prediction. In addition, we explore the effect of ViT adversarial training to mitigate overfitting and improve the accuracy and robustness of predictive process monitoring. Experiments with various benchmark event logs prove the accuracy of JARVIS compared to several current state-of-the-art methods and draw insights from explanations recovered through the attention modules.",
  "full_text": "IEEE TRANSACTIONS ON SERVICES COMPUTING 1\nJARVIS: Joining Adversarial Training with Vision\nTransformers in Next-Activity Prediction\nVincenzo Pasquadibisceglie, and Annalisa Appice, and Giovanna Castellano,Senior Member, IEEEand\nDonato Malerba, Senior Member, IEEE\nAbstract—In this paper, we propose a novel predictive process monitoring approach, named JARVIS, that is designed to achieve a\nbalance between accuracy and explainability in the task of next-activity prediction. To this aim, JARVIS represents different process\nexecutions (traces) as patches of an image and uses this patch-based representation within a multi-view learning scheme combined\nwith Vision Transformers (ViTs). Using multi-view learning we guarantee good accuracy by leveraging the variety of information\nrecorded in event logs as different patches of an image. The use of ViTs enables the integration of explainable elements directly into\nthe framework of a predictive process model trained to forecast the next trace activity from the completed events in a running trace by\nutilizing self-attention modules that give paired attention values between two picture patches. Attention modules disclose explainable\ninformation concerning views of the business process and events of the trace that influenced the prediction. In addition, we explore the\neffect of ViT adversarial training to mitigate overfitting and improve the accuracy and robustness of predictive process monitoring.\nExperiments with various benchmark event logs prove the accuracy of JARVIS compared to several current state-of-the-art methods\nand draw insights from explanations recovered through the attention modules.\nIndex Terms—Predictive process monitoring, Deep learning, Multi-view learning, Adversarial training, Vision transformers, Attention,\nXAI, Computer vision\n✦\n1 I NTRODUCTION\nP\nREDICTIVE Process Monitoring (PPM) techniques allow\nthe extraction of smart knowledge from historical, raw\nevent data of business processes, in order to enable the\nprediction of future states (e.g., next-activity, completion\ntime, outcome) of a process execution given its running trace\nand raw event data of historical traces as inputs.\nOver the past five years, the predominance of deep learn-\ning in predictive modeling has been increasingly assessed\nin PPM systems. Several deep neural networks, e.g., LSTMs\n[1], [2], [3], CNNs [4], [5], GANs [6] and Autoencoders [7],\nhave recently contributed to gaining accuracy into PPM\nsystems thanks to their capability to learn accurate deep\nneural models that can enable proactive and corrective\nactions to improve process performance and mitigate risks.\nHowever, deep neural networks generally learn opaque\nmodels implicitly represented in form of a huge number\nof numerical weights that are difficult to explain due to\nthe complexity of the network structure. The opacity of\ndeep neural models is acceptable as long as accuracy was\nthe dominant criterion for assessing the quality of PPM\nsystems. Despite the priority of PPM systems today is still\nto provide accurate predictions of future states of running\ntraces, easier-to-explain predictive models are becoming\nincreasingly desirable in PPM applications.\nThe explainability of predictive models refers partially\nto how easily humans may comprehend their underly-\n• V . Pasquadibisceglie, A. Appice, G. Castellano and D. Malerba\nare with the Dipartimento di Informatica, Universit` a degli Studi\nAldo Moro di Bari and Consorzio Interuniversitario Nazionale per\nl’Informatica - CINI, via Orabona, 4 - 70125 Bari - Italy E-\nmail: vincenzo.pasquadibisceglie@uniba.it, annalisa.appice@uniba.it, gio-\nvanna.castellano@uniba.it, donato.malerba@uniba.it\ning assumptions and reasoning. Explanations may disclose\nmeasurable factors on which trace characteristics influence\nthe prediction of a future state of a running trace and to what\nextent. Explaining the effect of certain trace characteristics\non trace predictions can contribute to translating predictions\ninto some domain knowledge and allow PPM systems to\nbetter benefit from the trust of process stakeholders. Recent\nstudies [8], [9], [10] have investigated the use of existing\nXAI (eXplainable AI) methods to explain opaque PPM mod-\nels. However, model explainability in the context of deep\nlearning-based PPM systems is yet underexplored.\nTo bridge the gap between accuracy and explainability in\nPPM models, in this paper, we propose a novel method for\npredicting the next-activity of a running trace. The proposed\nmethod, called JARVIS (Joining Adversarial tRaining with\nVISion transformers in next-activity prediction), combines\nadversarial training with Vision Trasformers (ViT) [11] to\nachieve a valid balance between the accuracy and the ex-\nplainability of the model. Explainability is achieved thanks\nto the adoption of a ViT, i.e., a deep neural architecture\ncomposed of multiple self-attention layers that implement\nan attention mechanism to differentiate the significance of\neach part of the input sequence of data, thus providing\na form of explanation of the model behavior in terms of\nmost informative inputs. Accuracy improvement is faced\nvia adversarial training [12] which incorporates perturbed\n(i.e., adversarial) inputs into the training process, in order to\nmitigate overfitting and improve generalization [13]. More-\nover, boosted by our recent results [3], we also adopt multi-\nview learning [14], which consists in learning with multiple\nsources (views) of event information (e.g., activity, resource,\ntimestamp, cost) to improve the generalization performance.\nSpecifically, in JARVIS events are seen as multiple views of\nThis article has been accepted for publication in IEEE Transactions on Services Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TSC.2023.3331020\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON SERVICES COMPUTING 2\nthe same running trace, and this enables handling multiple\ncharacteristics of a trace.\nAnother key feature of JARVIS is a novel representation\nof running traces as images. Specifically, each view of a trace\nis encoded as a separate image patch (i.e., a square-shaped\nregion within an image) that encodes the sequence of em-\nbedded events in the view. The embedding representation of\nevents in each view is derived by accounting for neighbour\ninformation of each event in the running trace (i.e., view\ninformation of events that precede the considered event in\nthe running trace). Notably, this image representation differs\nfrom the ones adopted in [4], [5] which lose information\nabout the order of trace events due to an aggregation step.\nIn addition, the novel representation proposed in JARVIS\nallows the embedding of raw event data belonging to any\nview. It also differs from the imagery representation\nintroduced in [15] that represents full traces populated with\npartially labeled events and is used to predict the comple-\ntion time of a target event within a full trace by accounting\nfor attributes of all events in the trace and completion\ntimes of events preceding the target one. Conversely, the\nimagery representation adopted in JARVIS is used to predict\nthe next-activity of the running trace without having any\ninformation on the next event.\nIn short, the contributions of this paper are as follows.\n• The formulation of a novel multi-view learning ap-\nproach that performs an image-like engineering step\nto represent running traces as image patches and\ntrains a ViT model for next-activity prediction, in\norder to learn relationships between multiple input\nviews and provide explainability in form of attention\nof the model on significant inputs.\n• The use of attention maps of ViT models as a form of\nexplanation to disclose the effect that specific views\nand intra-view events (i.e., characteristics of trace\nevents in a specific view) may have on the PPM\nmodel’s reasoning.\n• The exploration of adversarial training as a learn-\ning strategy to mitigate overfitting and improve ViT\nmodel generalization.\n• The presentation of the results of an in-depth evalua-\ntion examining the ability of our approach to achieve\naccuracy comparable to competitive deep learning-\nbased approaches drawn from the recent literature\non PPM systems, as well as the accuracy of the\nproposed learning components.\nThe paper is organized as follows. Section 2 overviews\nrecent advances of PPM literature in the next-activity pre-\ndiction of business processes and XAI. Preliminary concepts\nare reported in Section 3, while the proposed JARVIS ap-\nproach is described in Section 4. The experimental setup\nis illustrated in Section 5. The results of the evaluation of\nthe proposed approach are discussed in Sections 6 and 7,\nregarding accuracy and explainability, respectively. Finally,\nSection 8 recalls the purpose of our research, draws conclu-\nsions, and illustrates possible future developments.\n2 R ELATED WORK\nThe JARVIS approach proposed in this paper is a deep\nlearning-based method that resorts to a trace engineering\nstep to represent running traces as image patches and neural\nattention mechanisms for yielding next-activity predictions\nof running traces equipped with explainability. Therefore,\nthe literature overview is organized on two fronts: on\none side, we focus on recent PPM studies on next-activity\nprediction using deep learning (Section 2.1) and on the\nother side we overview preliminary attempts to use XAI\ntechniques in the realm of PPM (Section 2.2). A summary of\nthe characteristics of the main related methods discussed in\nthe following SubSections is reported in Table 1.\n2.1 Next-activity prediction\nSeveral recent PPM studies have investigated the perfor-\nmance of various deep neural network architectures (e.g.,\nLSTMs, RNNs, CNNS) for predicting the next-activity of\na running trace. These studies commonly adopt an en-\ncoding step that is the responsible for mapping complex,\ncategorical, event data information into a numerical feature\nspace, in order to feed the deep neural network. These\nstudies mainly adopt data engineering methods like One-\nHot-Encoding (OHE) (e.g., [1]) or embeddings (e.g., [2], [16])\nto map running traces into real-valued vectors. Notably the\nuse of the embedding representation of a categorical input\nallows us to avoid a sparse representation of categorical\ninput data that is one of the limits of the OHE represen-\ntation. [24] compares several embedding mechanisms, i.e.,\nAct2Vec, Trace2Vec, Log2Vec, and Model2Vec, which per-\nform data engineering at the level of activities, traces, logs,\nand models. [25] uses Word2Vec to learn a context-aware\nreal-valued representation of categorical input (e.g., activi-\nties, resources), which can be easily fine-tuned to the new\nevents recorded in a streaming scenario. [26] evaluates the\nperformance of several graph embeddings. [1] augments the\nOHE of activities with several numerical features extracted\nfrom the timestamp information such as the time of day, the\ntime since the previous event, the time since the beginning\nof the trace, while [2] uses embedding networks and hand-\ncrafted features. Image-like data engineering methods have\nbeen introduced by [4], [20]. In particular, [20] proposes an\napproach to transform prefix traces into grey-scale images\nby mapping the activity and timestamp information of each\nprefix trace into two grey-scale pixel columns. [4] describes\nan imagery encoding approach that first extracts a numeric\nfeature vector representation of prefix traces by aggregating\nactivity, resource and timestamp information. Then it trans-\nforms the feature vector representation of a prefix trace into\na color-scale image by mapping every numeric feature into\na RGB pixel. Notably, [4] uses a color imagery encoding of\nrunning traces as in our work. However, it looses informa-\ntion about the order of trace events due to the aggregation\nstep. In addition, it is defined to encode activity, resource\nand timestamp information. Differently JARVIS keeps the\ninformation on the temporal order of events and it is defined\nto encode event information from any view. A recent survey\nof the main progress in trace encoding techniques has been\ndescribed in [27].\nFocusing the attention on the deep neural model, as\nmost of PPM studies consider prefix traces represented as\nsequences, they adopt sequence-based deep neural network\narchitectures such as LSTMs or RNNs. In particular, [1]\nThis article has been accepted for publication in IEEE Transactions on Services Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TSC.2023.3331020\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON SERVICES COMPUTING 3\nTable 1: A summary of characteristics of related methods\nReference Views ML/DL XAI Task\n[1] Activity, Time LSTM - Next-activity, Next timestamp, Completion time,\nActivity suffix\n[16] Activity, Resource LSTM - Next-activity\n[2] Activity, Resource, Timestamp LSTM - Next-activity, Next timestamp, Next role,\nCompletion time, Activity suffix\n[3] All LSTM - Next-activity\n[17], [18], [19] All Decision Tree, Naive Bayes, DNN - Deviant traces\n[20] Activity, Timestamp CNN - Next-activity\n[4] Activity, Resource, Timestamp CNN - Next-activity\n[8] Activity, Resource, Role LSTM SHAP Completion time, Activity occurrence, User satisfaction\n[10] Activity, Resource, Role CatBoost, LSTM SHAP Completion time, Activity occurrence, User satisfaction\n[21] Activity, Resource, Timestamp ANFIS Neuro-fuzzy rules Trace ouctcome\n[22] Activity Gated Graph Neural Network Graphs Trace ouctcome\n[23] Activity Transformer Attention maps Next-activity\n[9] Activity, Resource, Timestamp Attention+LSTM Attention maps Next-activity\ndescribes an LSTM-based PPM method to predict both the\nnext-activity and the completion time of a prefix trace.\nIt uses OHE of activity and timestamp information. [16]\nillustrates a LSTM-based PPM method that accounts for\nembeddings of both activities and resources, but neglects\ntimestamps. Also [2] describes a PPM method that trains\nan LSTM architecture to predict sequences of next events,\ntheir timestamps and their associated resource pool. This\nmethod accounts for activity and resource by resorting to\na pre-trained embedding representation of the categorical\ninformation. [3] proposes a multi-input LSTM architecture\nthat is able to process all possible views present in an event\nlog. This flexible multi-view method has the advantage of\ncapturing the possible information among the views and\nincrease the predictive model’s accuracy performance. The\nembeddings of the categorical information are computed\nwithin the multi-input network. Multi-view learning meth-\nods are also developed also for deviant trace detection.\n[17], [18], [19] describe multi-view ensemble-based methods\nthat combine different single-view classifiers to disentangle\ndeviant process instances form normal ones. Specifically,\n[17] describes the use of decision trees and association rule\nclassifiers as single-view classifiers of the ensemble. [18]\nexplores the combination of single-view Bayesian classifiers,\nwhile [19] combines single-view deep neural models.\nIn spite of the proliferation of LSTM methods for PPM,\na few recent studies have started the investigation of com-\nputer vision-based approaches in PPM. For example, [20]\ndescribes a CNN architecture to predict the next activity of\na grey-scale image of a prefix trace, while [4] describes a\nCNN with Inception to predict the next activity of a RGB\nimage of a prefix trace. Both approaches handle information\nof apriori defined views under a feature extraction step.\nDespite the methods described above achieve good\naccuracy performance in various problems, they are not\nequipped with explainability techniques.\n2.2 Explainability in PPM\nA few recent studies have started the investigation of ex-\nplainability in deep learning-based PPM models. However,\nmost of the existing work on explainable PPM approaches\nuse post-hoc methods to explain model predictions. For ex-\nample, [8] describes a PPM framework that uses “post-hoc”\nexplanations generated by the SHAP technique applied to\ntrained black-box LSTM models. Also [10] reports on the\nuse of post-hoc, SHAP explanations for both the LSTM and\nthe CatBoost method. [28] compares and evaluates expla-\nnations of process predictions yielded by different post-hoc\nframeworks (e.g., LIME and SHAP). [29] leverages post-hoc\nexplainers to understand why a PPM model provides wrong\npredictions, eventually improving its accuracy.\nThe above post-hoc explainers are model-agnostic, i.e.,\nthey can be used independently of the model and they do\nnot have any effect on the model training. On the other\nhand, a few recent PPM approaches have started focusing\non intrinsic explainable deep neural models that directly\nproduce interpretable models. One of the few works in\nthis direction is [21] that presents a fully interpretable PPM\nmodel for outcome prediction based on a set of fuzzy\nrules acquired from event data by training a neuro-fuzzy\nnetwork. [22] uses gated graph neural networks to visualize\nhow much the different activities included in a process\nimpact the PPM model predictions. [23] incorporates ex-\nplainability into the structure of a PPM model by replacing\nthe cells with self-attention mechanisms in recurrent neural\nnetworks. Similarly [9] develops attention-based models\nto incorporate explainability directly into the PPM model.\nNotably, both [23] and [9] use the attention mechanism as\nin our work. Our approach differs from [23] and [9] since it\nis able to handle event information of any view without\nforcing any apriori-defined views at the input level. In\naddition, JARVIS uses a ViT architecture that incorporates\nself-attention modules to detect inter-view relationships,\nin addition to intra-view event information. We use the\nintrinsic attention on inter-view and intra-view information\nto obtain both local and global explanations of predictions.\n3 P RELIMINARY CONCEPTS\nGiven a business process, a trace describes the life cycle\nof a process execution in terms of a sequence of events.\nAccording to this definition of a trace, an event refers to a\ncomplex entity composed of two mandatory characteristics,\ni.e., the activity and the timestamp (indicating the date and\ntime of the activity occurrence), as well as several optional\ncharacteristics, such as the resource triggering the activity or\nthe cost of completing the activity. Based on this definition,\nan event can be characterized by different views, being a\nview the description of the event along a specific character-\nistic. Hence, every event has two mandatory views that are\nassociated with the activity and the timestamp, as well as m\nThis article has been accepted for publication in IEEE Transactions on Services Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TSC.2023.3331020\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON SERVICES COMPUTING 4\nadditional views associated with optional characteristics of\nevents. Let A be the set of all activity names, S be the set of\nall trace identifiers, and T be the set of all timestamps and\nVj with 1 ≤ j ≤ m be the set of all names in the j-th view.\nDefinition 1 (Event). Given the event universe E = S ×\nA × T × V1 × . . .× Vm, an event e ∈ Eis a tuple\ne = (σ, a, t, v1, . . . , vm) that represents the occurrence of\nactivity a in trace σ at timestamp t with characteristics\nv1, v2, . . . , vm.\nLet us introduce the functions: πS : E 7→ Ssuch that\nπS(e) = σ, πA : E 7→ Asuch that πA(e) = a, πT : E 7→ T\nsuch that πT (e) = t and πVj : E 7→ Vj such that πVj (e) = vj\nand j = 1, . . . , m.\nDefinition 2 (Trace). Let E∗ denote the set of all pos-\nsible sequences on E. A trace σ is a sequence σ =\n⟨e1, e2 . . . , en⟩ ∈ E∗ so that: (1) ∀i = 1, . . . , n, ∃ei ∈ E\nsuch that σ(i) = ei and πS(ei) = σ, and (2) ∀i =\n1, . . . , n− 1, πT (ei) ≤ πT (ei+1).\nDefinition 3 (Event log). Let B(E∗) denote the set of all\nmultisets over E. An event log L ⊆ B(E∗) is a multiset\nof traces.\nDefinition 4 (Prefix trace). A prefix trace σk = ⟨e1, e2, . . . , ek⟩\nis the sub-sequence of a trace σ starting from the begin-\nning of the trace σ with 1 ≤ k = |σk| < |σ|.\nA trace is a complete (i.e., started and ended) process in-\nstance, while a prefix trace is a process instance in execution\n(also called running trace). The activity πA(ek+1) = ak+1\ncorresponds to the next-activity of σk, i.e., next(σk) =\nπA(ek+1) with ek+1 = σ(k + 1).\nDefinition 5 (Multiset of labeled prefix traces). Let L ⊆\nB(E∗) be an event log, P ⊆ B(E∗ × A) is the multiset of\nall prefix traces extracted from traces recorded inL. Each\nprefix trace is labeled with the next-activity associated to\neach prefix sequence in the corresponding trace so that\nP = [σk, πA(ek+1)|σ ∈ L ∧1 ≤ k <|σ|].\nDefinition 6 (Single-view representation of a labeled prefix\ntrace multiset). Let V be a view (either mandatory, i.e.,\nV = A or V = T , or optional, i.e. V = Vj with j =\n1, . . . , m), Π: E∗ 7→ V∗ be a function such that Π(σk) =\nΠ(⟨e1, e2, . . . , ek⟩) = ⟨πV(e1), πV(e2) . . . , πV(ek)⟩. PV\ndenotes the multiset of the labeled prefix traces of P\nas they are represented in the view V, that is, PV =\n{ΠV(σk), ak+1|(σk, π(ek+1))) ∈ P}.\nThe next-activity prediction is a PPM task often addressed\nas a multi-class classification problem by resorting to ma-\nchine learning techniques. Let F : E∗ × Rµ 7→ Abe a next-\nactivity prediction model with µ real-valued parameters.\nDefinition 7 (Next-activity prediction hypothesis function).\nA next-activity hypothesis HF,Θ of the model F is a\nfunction: HF,Θ : E∗ 7→ A with Θ ∈ ℜµ such that\nHF,Θ(x) ≈ F(x, Θ).\nDefinition 8 (Next-activity prediction). Let us consider\nHF,Θ : A∗ 7→ A, and σk a prefix trace. HF,Θ(σk) predicts\nthe expected next-activity of σk.\nThe hypothesis HF,Θ can be learned from a labeled\nprefix trace multiset P by an algorithm that estimates Θ\nFigure 1: JARVIS pipeline\nminimizing a cost function CHF,Θ : E∗ × A 7→ R, where\nCHF,Θ (σk, ak+1) measures the penalty of an incorrect pre-\ndiction done through HF,Θ(σk) for the next-activity ak+1.\nHF,Θ depends on the model type. In this study, we represent\nthe labeled multiset as a collection of color image patches\nthat are given as input to a ViT architecture [11].\n4 P ROPOSED APPROACH\nIn this section we introduce JARVIS, a novel PPM approach\nfor next-activity prediction that combines multi-view learn-\ning and adversarial training with Vision Trasformers (ViT) to\nachieve a valid balance between accuracy and explainability.\nThe proposed approach is schematized in Figure 1. Initially,\nthe labeled prefix trace multiset P is extracted from the\nevent log L and transformed into a set I of multi-patch\ncolor images. Then, I is fed into a ViT architecture that is\ntrained with adversarial training to estimate parameters of\na next-activity prediction hypothesis function HF,Θ.\nIn the following, we detail the main phases of the pro-\nposed approach, namely the multi-patch image encoding\n(Section 4.1), the ViT adversarial training (Section 4.2), and\nthe extraction of the attention maps (Section 4.3).\n4.1 Multi-patch image encoding\nThis phase takes the event log L as input and creates the\nmultiset of multi-patch color images I as output. This phase\nis composed of four steps: 1) Discretizing numerical view\ninformation. 2) Generating labeled prefix traces. 3) Training\nan embedding for prefix traces represented in each view.\n4) Transforming embeddings of prefix traces represented in\neach view into multi-patch color images according to the\nembeddings trained on the multiple views.\nAccording to the multi-view formulation introduced in\nSection 3, every event recorded in L is a complex entity\nwhose representation takes into account two mandatory\ncharacteristics (activity A and timestamp T ) and m op-\ntional characteristics ( V1, V2, . . . ,Vm), respectively. Manda-\ntory and optional views may describe both categorical\ncharacteristics (e.g., activity or resource names) and nu-\nmerical characteristics (e.g., timestamp or cost values). In\nparticular, the timestamp information associated with an\nevent is transformed in the time in seconds passed from\nthe beginning of the trace. In this study, every numerical\ncharacteristic is converted into a categorical, format by re-\nsorting to the equal-frequency discretization algorithm. This\nalgorithm divides all the values of a numeric characteristic\nrecorded in the training set of the event log into bins that\ncover the same number of observations. The number of\ndiscretization bins of a numeric characteristic is set equal\nto the average number of distinct categories in the original\nThis article has been accepted for publication in IEEE Transactions on Services Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TSC.2023.3331020\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON SERVICES COMPUTING 5\nFigure 2: A prefix trace recorded in the event log BPI12WC.\nIt is represented in two categorical views (“activity” and\n“resource”) and two numerical views (“timestamp” and\n“amount”). Numerical data are transformed into categorical\ndata using the equal-frequency discretization algorithm.\ncategorical views of the event log. After this step, the event\nlog L contains all multi-view information in the categorical\nformat. We denote V the final set of m + 2 categorical\nviews that characterize events recorded in the pre-processed\nevent log L. For example, Figure 2 shows a sample prefix\ntrace recorded in the event log BPI12WC considered in the\nexperimental study. Event characteristics enclosed in the\nnumeric views “timestamp” and “amount” are transformed\ninto the categorical format through the discretization step.\nIn this way, we obtain a multi-view representation of the\nprefix trace with all characteristics in the categorical format.\nSubsequently, the multiset P is created by extracting\ntraces from L and labeling them with the next activity. Since\ndifferent prefix traces may have different lengths, we use\nthe padding technique in combination with the window-\ning mechanism to standardize different prefix lengths and\npopulate P with fixed-sized prefix traces. Let AV Gσ be the\naverage length of all the traces in L, the padding is used\nwith a window length equal to AV Gσ, as in [3]. Prefix traces\nwith length less than AV Gσ are standardized by adding\ndummy events. Prefix traces with length greater thanAV Gσ\nare standardized by retaining only the most recent AV Gσ\nevents. After this step, P comprises labeled prefix traces\nhaving fixed size equal to AV Gσ.\nThe Continuous-Bag-of-Words (CBOW) architecture of\nthe Word2Vec scheme [30] is then used to transform the cat-\negorical representation of a prefix trace into a bidimensional,\nnumeric embedding representation. The CBOW architecture\nleverages a feed-forward neural network to predict a tar-\nget category from the neighbored context. For each view\nV ∈ V, a CBOW neural network, denoted by CBOW V\nis trained in order to convert each single-view sequence\nΠV(σk) ∈ PV into an AV Gσ-sized numerical vector. Specif-\nically, ΠV(σk) is converted into a bidimensional, numeric\nembedding PV ∈ RAV Gσ×AV Gσ with size AV Gσ × AV Gσ.\nFor example, Figure 3 shows how the sequence of activities\nof the sample prefix trace reported in Figure 2 is converted\ninto a 7 × 7 bidimensional, numeric embedding through\nWord2Vec. Notice thatAV Gσ = 7 in BPI12WC.\nFinally, for each labeled prefix trace (σk, ak+1) ∈ P,\nthe list of its multi-view, bidimensional, numeric\nFigure 3: Word2Vec embedding of the sequence of activities\nenclosed in the sample prefix trace shown in Figure 2\nFigure 4: Conversion of the Word2Vec embedding represen-\ntation shown in Figure 3 into an imagery color patch\nembeddings PA, PT , . . . ,PV1 , . . . ,PVm, generated for\nΠA(σk), ΠT (σk), ΠV1 (σk), . . . ,ΠVm(σk), respectively,\nare converted into the imagery color patches\nPrgb\nA , Prgb\nT , . . . ,Prgb\nV1 , . . . ,Prgb\nVm by mapping numeric\nvalues of bidimensional embeddings into RGB\npixels. In particular, every imagery color patch\nPrgb ∈ RAV Gσ×AV Gσ×3 records the embedding of a\nprefix trace with respect to a view into a numerical tensor\nwith size AV Gσ × AV Gσ × 3. Let P be a bidimensional,\nnumeric embedding, each numeric value of v ∈ P is\nconverted into a RGB pixel vrgb ∈ Prgb by resorting to\nthe RGB-like encoding function adopted in [4]. First, v is\nscaled in the range [0, 224 − 1]. Then the scaled value is\nconsidered as the value of a single 24-bit pixel vrgb with the\nfirst eighth bits mapped into the band R, the intermediate\neighth bits mapped into the band G and the last eighth bits\nmapped into the band B. For example, Figure 4 shows how\nthe Word2Vec embedding of the sequence activities of the\nprefix trace shown in Figure 3 is converted into an imagery\ncolor patch associated with the view “activity”.\nThe m + 2 color patches of a prefix trace are dis-\ntributed into a patch grid with size ⌈√m + 2⌉ × ⌈√m + 2⌉\nfrom left to right, and from top to bottom. Notice that\nevery cell of the patch grid records a patch with size\nAV Gσ × AV Gσ × 3. In this way, we are able to produce\nthe color image a prefix trace, that is a tensor with size\n(⌈√m + 2⌉· AV Gσ)×(⌈√m + 2⌉· AV Gσ)×3. For example,\nFigure 5 shows how the four patches, produced for the\nviews “activity”,“resource”, “timestamp” and “amount” of\nthe prefix trace reported in Figure 2, are distributed into a\npatch grid with size 2 × 2 forming the final color image of\nthe sample prefix trace. This image has size 14 × 14 × 3 and\ncontains 4 color patches with sizes 7 ×7 ×3 associated with\nthe four views.\nThis article has been accepted for publication in IEEE Transactions on Services Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TSC.2023.3331020\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON SERVICES COMPUTING 6\nFigure 5: Multi-patch color image with size 14×14×3. This\nimage is obtained by distributing the four image patches of\nsize 7 ×7 ×3, produced for the views “activity”,“resource”,\n“timestamp” and “amount” of the prefix trace shown in\nFigure 2, into a 2 × 2 patch grid.\nThe generated multi-patch images are labeled as the\ncorresponding prefix traces and added to the labeled image\nmultiset I.\n4.2 ViT architecture\nA ViT (Visual Transformer) architecture (fig. 6) is adopted\nto learn a next-activity prediction hypothesis function HF,Θ\nfrom I by accounting for intra-patch information and han-\ndling inter-patch relationships. We adopt the standard ViT\narchitecture [11] that represents the de-facto standard for\ncomputer vision applications and was developed as an\nextension of the original Transformer architecture originally\ndesigned for natural language processing tasks [31]. As\nthe Transformer takes one-dimensional sequences of word\nembeddings as input, the following four steps are per-\nformed to transform the patches into a suitable input for\nthe Transformer:\n1) Every multi-patch image I ∈ Iis reshaped into\nthe sequence of its m + 2 flattened 2-dimensional\npatches ϕ(Prgb\nA ), ϕ(Prgb\nT ), ϕ(Prgb\nV1 ), . . . , ϕ(Prgb\nVm )\nusing the the flattening operator\nϕ: RAV Gσ×AV Gσ×3 7→ RAV G2\nσ·3.\n2) Every flattened patch sequence is mapped to a\nconstant latent vector size D using a trainable linear\nprojection E ∈ R(AV G2\nσ·3)×D. In this way a sequence\nof embedded patches, called patch embeddings, is\ngenerated.\n3) A learnable class embedding, denoted as class, is\nanteposed to the patch embeddings. The value of\nclass represents the classification output y.\n4) Patch embeddings are augmented with one-\ndimensional positional embeddings Epos ∈\nR(m+3)×D. The positional embeddings enable injec-\ntion of positional information into the input.\nThe output of the transformation steps described above\nis the sequence of embedding vectors defined as follows:\nz0 = [ class; ϕ(Prgb\nA )E; ϕ(Prgb\nT )E; ϕ(Prgb\nV1 )E; . . .;\nϕ(Prgb\nVm )E; ] +Epos. (1)\nz0 feeds the Transformer encoder defined in [31]. This\nconsists of a stack of L identical blocks alternating multi-\nheaded self-attention and MLP blocks, in order to compute:\nz′\ni = msa(LN(zi−1)) + zi−1, (2)\nzi = mlp(LN(z′\ni)) + z′\ni. (3)\nFigure 6: ViT architecture\nwith i = 1, . . . , L−1, msa(•) the multi-headed self-attention\nblock, mlp(•) the MLP block, LN(•) the Layernorm that\nis applied before every block. Finally, the value class of\nthe L-th block of the encoder output feeds into a MLP\nclassification head with one hidden layer and GELU non-\nlinearity.\nThe parameters of the ViT architecture are estimated\nthrough the adversarial training strategy. Adversarial train-\ning is a well known learning strategy in which the training\nset is augmented with adversarial samples generated from\nthe training set by perturbing some samples. Adversarial\ntraining is also used to mitigate overfitting by achieving\nhigh robustness [32]. In fact, generating adversarial samples\nby slightly perturbing training samples may improve the\ngeneralization of deep neural models.\nIn this study, we use the popular state-of-the-art Fast\nGradient Sign Method (FGSM) [33] to generate adversarial\nimages. It is a white-box gradient-based algorithm that finds\nthe loss to apply to an input image, in order to make\ndecisions of a pre-trained neural model less overfitted on a\nspecific class. The pre-trained model is the ViT architecture\ndescribed above with parameters initially estimated on the\noriginal labeled images of I.\nThe FGSM algorithm is based on the gradient formula:\ng(I) = ∇IJ(θ, I, y), (4)\nwhere ∇I denotes the gradient computed with respect to the\nimagery sample x, and J(θ, I, y) denotes the loss function\nof the ViT neural model initially trained on the original\ntraining set I. In theory, FGSM determines the minimum\nperturbation ϵ to add to a training image I to create an ad-\nversarial sample that maximizes the loss function. Accord-\ning to this theory, given an input perturbation value ϵ, for\neach labeled image (I, y) ∈ I, a new image (Iadv, y) ∈ Iadv\ncan be generated such that:\nIadv = I + ϵ · sign(g(I)). (5)\nAs Iadv is generated, parameters of the ViT architec-\nture are finally estimated from the adversarially-augmented\ntraining set I ∪ Iadv.\nThis article has been accepted for publication in IEEE Transactions on Services Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TSC.2023.3331020\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON SERVICES COMPUTING 7\nFigure 7: Map of attention produced in the RGB space (left\nside) for the prediction of the next-activity of the imagery\nrepresentation shown in Figure 5 of the sample prefix trace\nreported in Figure 2. Transformation of the map of attention\nfrom the RGB space to the LAB space (central side). Lumi-\nnosity (L) channel of the map of attention produced in the\nLAB space (right side).\n4.3 Extracting maps of attention\nOnce the ViT parameters have been estimated, the ViT\nmodel is used to decide on the next-activity of any prefix\ntrace. As described in [11], the Attention Rollout method\n[34] is used to extract the map of attention of the deci-\nsion of the ViT model on a single sample. The Rollout\nmethod computes the map of attention by averaging the\nattention weights of the ViT model across all heads and then\nrecursively multiplying the weight matrices of all layers.\nThis accounts for the mixing of attention across patches\nthrough all layers. Then, we derive a quantitative indicator\nof the importance of events within patches by exploiting\nthe lightness information of attention maps. The lighter\nthe pixel in the attention map, the higher the effect of the\npixel information enclosed in the image of the prefix trace\non the ViT decision. Indeed, the generated attention maps\nare represented in the RGB color space, which operates on\nthree channels (red, green, and blue) and does not provide\ninformation about lightness. Hence we transform the RGB\nrepresentation of attention maps into the LAB color space,\nwhich operates on three different channels: the color light-\nness (L), the color ranges from green to red (A), and the\ncolor ranges from blue to yellow (B).\nThe transformation from the RGB space to the LAB space\nis performed as follows [35]:\nL = 0 .2126 · R + 0.7152 · G + 0.0722 · B,\nA = 1 .4749(0.2213 · R − 0.3390 · G + 0.1177 · B) + 128,\nB = 0 .6245(0.1949 · R + 0.6057 · G − 0.8006 · B) + 128.\n(6)\nAs an example, Figure 7 shows the map of attention of\nthe next-activity decision produced by the ViT model for the\nimagery representation shown in Figure 5 for the prefix trace\nreported in Figure 2. The ViT model was learned through\nadversarial training from the training set of BPI12WC in the\nexperimental study. The map of attention shown in Figure\n7 (left side) is extracted in the RGB space with the roll-out\nmethod. Figure 7 (central side) shows the transformation of\nthe map of attention from the RGB space to the LAB space\ncompleted according to Eq. 6. Finally, Figure 7 (right side)\nshows the luminosity values of the map of attention that\nindicate the influence of the patches on the prediction.\nTable 2: Event log description: number of traces ( ♯Trace),\nnumber of events ( ♯Event), and set of views ( View). For\ncategorical views, the number of distinct categories in the\nview is reported in brackets.\nEvent log ♯Trace ♯Event View\nBPI12W 9658 170107 activity (19), timestamp,\nresource (60), loan amount\nBPI12WC 9658 72413 activity (6), timestamp,\nresource (60), loan amount\nBPI12C 13087 164506 activity (23), timestamp,\nresource (69), loan amount\nBPI13I 7554 65533\nactivity (13), timestamp,\nresource (1440), impact (4),\norg group (649),org role (24),\norg country (23), org involved (25),\nproduct (704), resource country (32)\nBPI13P 2306 9011\nactivity (7), timestamp,\nresource (643),impact (4),\norg group (15), org role (29),\norg country (18), product (380),\nresource country (21)\nReceipt 1434 8577\nactivity (27), timestamp,\nresource (48),channel (5),\ndepartment (3), group (8),\norg group (10), responsible (39)\nBPI17O 42995 193849\nactivity (8), timestamp,\nresource (144),monthly cost,\nfirst withdrawal amount,\ncredit score, offered amount,\nnumber of terms, action (2)\nBPI20R 6886 36796\nactivity (19), timestamp,\nresource (2), org (36),\nproject (79), task (597), role (8)\n5 E XPERIMENTAL SET -UP\nIn this section, we describe the event logs used for evaluat-\ning the accuracy and explainability of JARVIS, the experi-\nmental set-up and the implementation details.\n5.1 Event logs\nWe processed eighth real-life event logs available on the\n4TU Centre for Research. 1 BPI12W, BPI12WC and BPI12C\nwere provided for the BPI Challenge 2012 [36]. These logs\ncontain events collected monitoring the loan application\nprocess of a Dutch financial institute. They record: activities\nthat track the state of the application (A), activities that\ntrack the state of work items associated with the appli-\ncation (W), and activities that track the state of the offer\n(O). Sub-processes A and O contain only the complete life\ncycle transition, while sub-process W includes scheduled\nstarted and completed life cycle transitions. In particular,\nBPI2012C contains all the traces, but retains the completed\nevents of such traces, BPI2012W contains traces of sub-\nprocess W, and BPI2012WC contains traces of sub-process\nW, but retains the completed events of such traces. BPI13I\nand BPI2013P [37] contain events collected from the closed\nproblem management system and the incident management\nsystem, respectively, of Volvo IT in Belgium. Receipt [38]\nwas collected in the CoSeLoG project. It records events\nproduced in an anonymous municipality in the Netherlands\nduring the receiving phase of the building permit appli-\ncation process. BPI17O [39] contains events regarding all\n1. https://data.4tu.nl/portal\nThis article has been accepted for publication in IEEE Transactions on Services Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TSC.2023.3331020\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON SERVICES COMPUTING 8\noffers made for an accepted application through an online\nsystem of a Dutch financial institute in 2016 and their\nsubsequent events until February 1st 2017, 15:11. BPI20R\n[40] contains events pertaining requests for Payment, which\nare not travel-related. In 2017, events were collected for two\ndepartments, in 2018 events were collected for the entire\nuniversity. A summary of the characteristics of these event\nlogs is reported in Table 2.\n5.2 Experimental set-up\nThe experimental setting described in [1] was adopted in\nthis study. A temporal split was performed to divide each\nevent log into train and test traces. To this aim, the traces\nof an event log were sorted by the starting timestamp. The\nfirst 67% were selected for training the predictive model,\nwhile the remaining 33% were considered to evaluate the\nperformance of the learned model on the unseen traces.\n5.3 Implementation details\nJARVIS was implemented in Python 3.9.12 – 64 bit ver-\nsion – using Keras 2.8.0 library that is a high-level neural\nnetwork API using TensorFlow 2.8.1 as the back-end. 2 The\nViT architecture was trained with mini-batches using back-\npropagation. The tree-structured Parzen estimator [41] was\nused to perform the automatic hyper-parameter optimiza-\ntion of the ViT architecture. This allowed us to automate the\ndecisions related to the parameters of the implementation of\nthe ViT based on the characteristics of the training data. The\noptimization phase was performed by exploring both the\nnumber of layers L and the number of Heads H in {2, 3, 4},\nlatent vector size D in [25,28], MLP size in [24,26], batch size\nin [26,28], and learning rate in [10−3,10−2]. It was conducted\nby using the 20% of the training set, selected with stratified\nsampling, as a validation set, according to the Pareto Prin-\nciple. The hyper-parameter configuration, which minimized\nthe loss on the validation set in the range of possible values\nexplored in the defined search space, was automatically\nselected for each event log. The gradient-based optimization\nwas performed using the Adam update rule to optimize\nthe categorical cross-entropy loss function. The maximum\nnumber of epochs was set equal to 200 and an early stopping\napproach was used to avoid overfitting (as in [3]). The\ntraining phase stopped when there was no improvement\nof the loss on the validation set for 10 consecutive epochs.\nFor the early stopping, we used the same validation set\nconsidered in the hyper-parameter optimization phase.\nThe adversarial image generation was performed using\nthe algorithm FGSM (as implemented in the Adversarial\nRobustness Toolbox 1.12.1 library 3) with the perturbation\nvalue ϵ set equal to 0.001 by default. FGSM is one of the\nmost popular adversarial sample generators that is prone to\ncatastrophic overfitting [32]. In this study, we adopted the\ndefault set-up ϵ = 0 .001. This decision was based on the\nstudy of [12] that suggests setting perturbation ϵ as a small\nvalue in the range between 0 and 0.1, to scale the noise\nand ensure that perturbations are small enough to remain\nundetected to the human eye, but large enough to help in\ngeneralizing the neural model.\n2. https://github.com/vinspdb/JARVIS\n3. https://adversarial-robustness-toolbox.readthedocs.io/\n6 A CCURACY PERFORMANCE ANALYSIS\nIn this section we show the results of an analysis aimed at\nevaluating the accuracy of JARVIS, to answer the following\nresearch questions:\nQ1 How does the multi-view, ViT-based learning\nschema compare to state-of-the-art PPM methods?\nQ2 Is the adversarial training strategy able to achieve\nhigher accuracy than non-adversarially trained ViT\ncounterpart on unseen traces?\nQ3 Is the adversarial training strategy robust to the\nselection of the algorithm for the generation of ad-\nversarial samples?\nQ4 How does the accuracy of the ViT model change\nby modifying the order according to multiple views\nassigned to image patch positions?\nTo answer Q1 we performed a comparative study involv-\ning several deep learning methods based on LSTM, CNN\nand Transformer architectures, which were selected from the\nrecent PPM literature (i.e., [3], [4], [9], [20], [23]). These meth-\nods were run with the information enclosed in all views\nrecorded in the study event logs for a safe comparison. This\nanalysis allows us to explore how the proposed imagery\nrepresentation and the use of a ViT model can actually aid\nin gaining accuracy in next-activity prediction problems.\nThe results of this analysis are illustrated in Section 6.1.\nTo answer Q2 we performed an ablation study comparing\nthe accuracy performance of JARVIS to that of its baseline\nobtained by keeping away the generation of adversarial\nimages and completing the training of the ViT model with\nimages of prefix traces actually observed in the training\nset only. This analysis explores how adversarial training\ncan actually contribute to gaining accuracy in JARVIS. The\nresults of this analysis are illustrated in Section 6.2. To\nanswer Q3 we performed a sensitivity study exploring the\nperformance of JARVIS achieved by using different, state-\nof-the-art algorithms (i.e., FGSM, PGD and DeepFool) to\ngenerate adversarial samples. This analysis aims to explore\nthe validity of our decision to use FGSM as default ad-\nversarial sample generation algorithm. The results of this\nanalysis are illustrated in Section 6.3. To answer Q4 we\nperformed an experiment randomly shuffling the positions\nof views in the event logs to explore how the performance\nof JARVIS is robust to the view order. The results of this\nanalysis are illustrated in Section 6.4.\nFor each event log, we learned the next-activity predic-\ntive hypothesis functions of the compared methods on the\ntraining set and evaluated their ability to predict the next ac-\ntivity on the running traces of the testing set. We analyze the\nmacro FScore and the macro GMean performances achieved.\nBoth the macro FScore and the macro GMean are well-\nknown multi-class classification metrics commonly used in\nimbalanced domains. They measure the average FScore and\nGMean per activity type i giving equal weights to each\nactivity type. In this way, we avoid that our evaluation\noffsets the possible impact of imbalanced data learning.\nThe FScore of activity i measures the harmonic mean of\nprecision and recall of i, i.e., F Scorei = 2 precisioni×recalli\nprecisioni+recalli\nso that F Score= 1\nk\nkX\ni=1\n2precisioni × recalli\nprecisioni + recalli\n. The GMean\nThis article has been accepted for publication in IEEE Transactions on Services Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TSC.2023.3331020\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON SERVICES COMPUTING 9\nof activity i measures the geometric mean of specificity and\nrecall of i by equally considering the errors on opposite\nclasses, i.e., GMeani = √specificityi × recalli so that\nGMean = 1\nk\nkX\ni=1\np\nspecificityi × recalli. The higher the\nmacro FScore and the macro GMean, the better the accuracy\nperformance of the method.\n6.1 Related method analysis\nWe compared the performance of JARVIS to that of the\nmethods described in [3], [4], [9], [20] and [23]. [3] learns\na LSTM model from a sequence representation of prefix\ntraces. [4] learns a CNN with Inception from RGB images of\nprefix traces. [9] learns a LSTM model with Attention from a\nsequence representation of prefix traces. [20] learns a CNN\nmodel from gray-scale images of prefix traces. [23] learns a\nTransformer model from a sequence representation of prefix\ntraces. All related methods, except for [3], were originally\nexperimented by their authors accounting for specific views\nof traces. Specifically, [4] and [9] were experimented with\nactivity, resource, and timestamp information, [20] was ex-\nperimented with activity and timestamp information, and\n[23] was experimented with activity information. To provide\na fair comparison, we ran these related methods by account-\ning for all views recorded in the considered event logs. In\nfact, as the authors of the considered related methods made\nthe code available, we were able to run all the compared al-\ngorithms in the same experimental setting, thus performing\na safe comparison. Parameters of the related methods were\nset according to the best parameter set-up determined in\nthe code provided by the authors and described in the code\nrepositories. Notice that this comparative study was per-\nformed by selecting [3] from the group of related methods\n(i.e., [1], [2], [3], and [16]) reported in Table 1, which train\na LSTM model for next-activity prediction from sequence\ndata. In fact, [3] generalizes [1], [2], [16] as it was originally\nformulated to account for all views recorded in an event log.\nTable 3 collects the macro FScore and the macro GMean\nof both the considered related methods and JARVIS. These\nresults deserve several considerations. Notably, JARVIS\nachieves the highest FScore and GMean in five out of eight\nevent logs, being the runner-up method in one out of eight\nevent logs. In addition, JARVIS always outperforms the two\nrelated methods using an imagery encoding strategy [4],\n[20] except for BPI12W. Specifically, it always outperforms\nthe related method using a Transformer [23]. It commonly\noutperforms the related method using the attention modules\n[9] except for the macro FScore in BPI12W, and both macro\nFScore and macro GMean in BPI13I. These conclusions are\nalso drawn from the critical difference diagram reported in\nFigure 8 for the macro FScore performance of the compared\nmethods. This diagram was obtained after rejecting the null\nhypothesis with p-value ≤ 0.05 in the Friedman’s test and\nadopting the post-hoc Nemenyi test for pairwise method\ncomparisons. Despite there is no statistical difference be-\ntween JARVIS, [3], [4], [9] and [23], JARVIS is the top-\nranked in the difference diagram with [3] as runner-up. This\nresult assesses the effectiveness of our idea of using a ViT\nmodel, trained from a multi-view imagery representation of\nprefix traces, in PPM problems of next-activity prediction.\nFigure 8: Comparison of FScore of JARVIS and the state-\nof-the-art methods defined in [3], [4], [9], [20] and [23]\nwith the Nemenyi test. Groups of approaches that are not\nsignificantly different (at p ≤ 0.05) are connected.\n6.2 Adversarial training strategy analysis\nTo evaluate the effect of adversarial training we compare\nthe performance of JARVIS to that of its non-adversarial\ncounterpart JARVIS⊖ADV. JARVIS⊖ADV keeps away the ad-\nversarial training strategy and returns the ViT model whose\nparameters were estimated on the original training set.\nTable 4 collects the macro FScore and macro GMean of\nboth JARVIS and JARVIS⊖ADV. These results show that the\nuse of the adversarial training strategy allows us to higher\nvalues of either macro FScore or macro GMean in the event\nlogs of this study with the exception of BPIC12C. This is the\nonly event log, where JARVIS⊖ADV achieves higher values\nof both macro FScore and macro GMean than JARVIS.\nTo examine in depth the effect of the adversarial training\nstrategy on single classes we explore the FScore computed\nper activity. As an example, Table 5 collects the FScore\ncomputed per activity for JARVIS and JARVIS⊖ADV in both\nBPI13P and BPI20R. These two event logs correspond to\ntwo scenarios with a small number of distinct activities\n(7 activities in BPI13P) and a high number of distinct\nactivities (19 activities in BPI20R), respectively. Both event\nlogs include several imbalanced activities (e.g., a1 and a3 in\nBPI13P, as well as a6, a11 and a13 in BPI20R). The use of\nthe adversarial training strategy increases accuracy on the\nmajority activities for BPI20R (e.g., a2, a8), as well as in\nminority activities (e.g., a1, a3) for BPI13P). So, despite the\nuse of the adversarial training strategy does not provide a\nsystematic solution to the imbalanced phenomenon, it may\ngenerally help in mitigating overfitting on majority activities\nand gaining accuracy on some minority activities.\n6.3 Adversarial sample generation algorithm analysis\nWe analyze the performance of three white-box adversarial\nsample generation algorithms, that is, FGSM [33], PGD\n[42] and DeepFool [43]. FGSM is used in the rest of this\nexperimental study. PGD performs an iterative version of\nFGSM. DeepFool performs an iterative procedure to find\nthe minimum adversarial perturbations on both an affine\nbinary classifier and a general binary differentiable classifier.\nIt integrates the one-versus-all strategy to be applied to\nmulti-class problems. Both PGD and DeepFool commonly\nspend more training time than FGSM since they perform\nmultiple trials to generate perturbations. These methods are\nconsidered to be among the state-of-the art methods for\nadversarial training in the image domains [12].\nTable 6 shows the macro FScore and the macro GMean\nachieved by JARVIS by varying the adversarial sample\ngeneration algorithm among FGSM (baseline), PGD and\nThis article has been accepted for publication in IEEE Transactions on Services Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TSC.2023.3331020\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON SERVICES COMPUTING 10\nTable 3: Comparison between JARVIS and related methods defined in [3], [4], [9], [20] and [23] : macro FScore and macro\nGMean. The best results are in bold, while the runner-up results are underlined.\nEventlog FScore GMean\nJARVIS [3] [4] [9] [20] [23] JARVIS [3] [4] [9] [20] [23]\nBPI12W 0.667 0.737 0.692 0.673 0.673 0.661 0.820 0.847 0.828 0.792 0.819 0.825\nBPI12WC 0.705 0.685 0.661 0.675 0.645 0.668 0.812 0.798 0.778 0.792 0.780 0.787\nBPI12C 0.644 0.654 0.642 0.638 0.643 0.624 0.786 0.792 0.782 0.785 0.781 0.781\nBPI13P 0.414 0.320 0.336 0.408 0.228 0.405 0.595 0.533 0.546 0.594 0.472 0.593\nBPI13I 0.387 0.405 0.295 0.407 0.363 0.380 0.615 0.626 0.534 0.626 0.594 0.603\nReceipt 0.525 0.455 0.409 0.471 0.302 0.383 0.733 0.676 0.646 0.702 0.563 0.620\nBPI17O 0.720 0.714 0.705 0.691 0.718 0.712 0.846 0.833 0.830 0.815 0.835 0.831\nBPI20R 0.491 0.450 0.483 0.455 0.432 0.481 0.699 0.660 0.691 0.664 0.643 0.683\nTable 4: JARVIS vs JARVIS⊖ADV: macro FScore and macro\nGMean. The best results are in bold.\nEventlog FScore GMean\nJARVIS JARVIS ⊖ADV JARVIS JARVIS ⊖ADV\nBPI12W 0.667 0.628 0.820 0.799\nBPI12WC 0.705 0.689 0.812 0.816\nBPI12C 0.644 0.656 0.786 0.793\nBPI13P 0.414 0.405 0.595 0.592\nBPI13I 0.387 0.344 0.615 0.572\nReceipt 0.525 0.490 0.733 0.712\nBPI17O 0.720 0.716 0.846 0.849\nBPI20R 0.491 0.482 0.699 0.688\nTable 5: FScore per activity of JARVIS and JARVIS⊖ADV in\nBPI13P and BPI20R. The best results are in bold.\nEvent log Activity JARVIS JARVIS ⊖ADV Support%\na1 0.337 0.315 10.80%\na2 0.665 0.705 29.80%\na3 0.234 0.120 9.34%\na4 0.456 0.486 27.64%\nBPI13P\na5 0.375 0.402 22.42%\na1 0.998 0.998 20.64%\na2 0.957 0.851 21.20%\na3 0.077 0.007 7.68%\na4 0.000 0.000 0.00%\na5 0.000 0.000 0.13%\na6 0.000 0,000 0.01%\na7 0.966 1.000 0.13%\na8 0.827 0.795 20.51%\na9 0.000 0.000 0.01%\na10 0.603 0.641 3.02%\na11 0.000 0.000 0.13%\na12 0.975 0.975 3.39%\na13 0.000 0.000 0.39%\na14 0.961 0.961 2.13%\nBPI20R\na15 0.998 0.998 20.63%\nDeepFool. Results show that FGSM outperforms PGD and\nDeepFool in five out of eights event logs. DeepFool com-\nmonly outperforms FGSM and PGD in the remaining three\nevent logs (i.e., BPI12C, BPI13P and BPI20R). The only\nexception is observed with the macro GMean of BPI13P\nthat achieves the highest value with PGD. Let us focus\nthe attention on the performance of DeepFool and FGSM\nin BPI12C, BPI13P and BPI20R. JARVIS with DeepFool\nTable 6: Adversarial sample generation: macro FScore and\nmacro GMean of JARVIS by using FGSM (baseline), PGD\nand DeepFool\nEventlog FScore GMean\nFGSM PGD DeepFool FGSM PGD DeepFool\nBPI12W 0.667 0.608 0.562 0.820 0.793 0.771\nBPI12WC 0.705 0.692 0.700 0.812 0.807 0.809\nBPI12C 0.644 0.647 0.657 0.786 0.784 0.791\nBPI13P 0.414 0.417 0.422 0.595 0.600 0.599\nBPI13I 0.387 0.342 0.328 0.615 0.576 0.568\nReceipt 0.525 0.498 0.477 0.733 0.720 0.677\nBPI17O 0.720 0.712 0.711 0.846 0.840 0.837\nBPI20R 0.491 0.498 0.525 0.699 0.695 0.718\nFigure 9: Macro FScore measured in BPI13P and BPI20R\nby varying the ϵ among 0.01, 0.001 (default), 0.0001 and\n0.00001 with FGSM, PGD and DeepFool\nachieves higher macro FScore than [3] in BPI12C, while\nJARVIS with FGSM achieves lower macro FScore than [3]\nin the same log (see results in Table 3). On the other\nhand, JARVIS with FGSM already outperforms all related\nmethods reported in Table 3 in both BPI13P and BPI20R.\nSo, FGSM can be considered a good choice to allow JARVIS\nto achieve higher accuracy than related methods.\nBoth FGSM, PGD and DeepFool take ϵ as input pa-\nrameter. Figure 9 reports the macro FScore of JARVIS run\nwith FGSM, PGD and DeepFool by varying ϵ among 0.01,\n0.001 (default), 0.0001 and 0.00001 in both BPI13P and\nBPI20R. In both experiments, differences in macro FScore\nare negligible.\n6.4 Image patch position analysis\nWe explore the effect of the single-view image patch po-\nsitions within the adopted imagery encoding schema. We\nThis article has been accepted for publication in IEEE Transactions on Services Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TSC.2023.3331020\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON SERVICES COMPUTING 11\nTable 7: Image patch position analysis: macro FScore and\nmacro GMean of JARVIS measured in BPI13P and BPI20R\nby processing views in the default order they appear in\nthe event log ( Original). Average and Standard deviation\nof the macro FScore and the macro GMean of JARVIS by\nprocessing views after performing the random shuffle of\nviews. The shuffle operation is repeated on five trials.\nEventlog Fscore GMean\nOriginal Shuffle (Avg ±Dev) Original Shuffle (Avg ±Dev)\nBPI13P 0.415 0.413 ±0.018 0.595 0.600 ±0.010\nBPI20R 0.491 0.491 ±0.008 0.699 0.695 ±0.003\nrecall that, in this study, event log views are processed in the\nsame order they are recorded in the event log, while the sin-\ngle view-based patches of each prefix trace are distributed in\nthe prefix trace imagery grid from left to right, and from top\nto bottom. In this section, we evaluate the possible effect\nof the image patch positions on the accuracy of the ViT\nmodel trained by JARVIS. Let O be the ViT model that was\ntrained by processing event log views in the order they were\nrecorded in the event log. We trained five ViT models after\nrandomly shuffling the views’ positions in the event logs\nand, consequently, the positions of the view-based image\npatches in the imagery encoding of the prefix traces.\nTable 7 shows the average and standard deviation of the\nmacro FScore and the macro GMean of JARVIS, in both\nBPI13P and BPI20R, measured on the five trials of the\nview shuffle operation, as well as the macro FScore and the\nmacro GMean achieved processing the views in the original\norder. The differences in the performances of the trained\nViT models are negligible. This shows that the accuracy\nof the proposed approach is robust to the view order and\nthe adopted ViT model can be trained by taking advantage\nof the intra-view information and inter-view relationships\nindependently of the view positions in image patches.\n7 E XPLANATION ANALYSIS\nThis analysis aimed to explore how intrinsic explanations\nenclosed in the attention maps generated through the ViT\nmodel may provide useful insights to explain model deci-\nsions. We intend to answer the following research questions:\nQ1 What is the effect of information enclosed in different\nview-based image patches on decisions?\nQ2 What is the effect of events within each view-based\nimage patch on decisions?\nFor this purpose, in the following of this study, we\nexplore several average measurements of the lightness in-\nformation enclosed in the maps of attention, which were\nextracted for the training prefix traces of the event logs\nconsidered in this study. In particular, let I be the multi-\npatch imagery representation of a prefix trace. Let L be the\nlightness channel of the map of the attention of the ViT\nmodel on I. For each view, we can compute the local patch\nlightness of the considered view-based patch in L as the\naverage of the lightness measured on all pixels ofL falling in\nthe patch under study. This local measurement of the patch\nlightness quantifies the local importance of the information\nenclosed in the considered view-based image patch on the\nFigure 10: Maps of attention of two prefix traces of\nBPI13P , which are correctly labeled with “a2” (“Accepted-\nIn Progress”) and “a4” (“Completed-Closed”), respectively.\nThe maps are shown in the luminosity channel of the LAB\nspace. The numbers identify the names of the views in\nthe event log: 1-“activity”, 2-“resource”, 3-“timestamp”, 4-\n“impact”, 5-“org country”, 6-“org group, 7-“org role”, 8-\n“product”, 9-“resource country”.\nsample decision. The analysis of local patch lightness allows\nus to answer Q1. The highest the value of the local patch\nlightness, the highest the effect of the corresponding view\non the sample decision. On the other hand, for each event\npositioned in a prefix trace (e.g., first event, second event),\nand for each view, we compute the local intra-patch event\nlightness as the average of the lightness associated with\nthe pixels encoding the event in the view-based patch. The\nanalysis of local intra-patch event lightness allows us to\nanswer Q2. The highest the lightness of the local intra-\npatch event lightness, the highest the effect of the view\ninformation of the selected event on the sample decision.\nWe start analyzing explanations of local decisions con-\ncerning the next activity predicted for specific prefix traces.\nFor example, Figure 10 shows the lightness channel of\nthe attention maps extracted from the ViT model trained\nby JARVIS on two prefix traces of BPI13P . These prefix\ntraces were correctly labeled with the next-activity “a2”\n(“Accepted-In Progress”) and “a4” (“Completed-Closed”),\nrespectively. Table 8 reports the local patch lightness mea-\nsured for each view in the maps of attention shown in\nFigure 10. These results show that the patch associated\nwith “activity” conveys the most relevant information for\nrecognizing both “Accepted-In Progress” and “Completed-\nClosed” as the next-activities of the two sample prefix traces.\nHowever, “impact” and “org group” are the second and\nthird most important views for the decision on the next-\nactivity “Accepted-In Progress”, while “org group” and\n“product” are the second and third most important views\nfor the decision on the next-activity “Completed-Closed”.\nNotably, “product”, which is one of the top-three ranked\nviews for the decision on the next activity “Completed-\nClosed”, is the less important view for the decision on the\nnext activity “Accepted-In Progress”. This analysis shows\nthat different views may convey the most important infor-\nmation for different decisions.\nWe go deeper in the explanation of the decision\n“Completed-Closed”. Figure 11 shows the sequences of\nactivities, org groups, and products recorded in the sam-\nple prefix trace with next-activity “Completed-Closed”. We\nselected these views as they are the top-three views for\nrecognizing the next activity “Completed-Closed” in the\nsample prefix trace according to the analysis of the local\npatch analysis reported in Table 8. For each selected view,\neach event of the prefix trace is annotated with the local\nThis article has been accepted for publication in IEEE Transactions on Services Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TSC.2023.3331020\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON SERVICES COMPUTING 12\nTable 8: Patch lightness measured for each view of BPI13P\nin the two maps of attention shown in Figure 10\nView Left map (“a2”) Right map (“a4”)\nactivity 91.56 120.00\nresource 10.50 11.88\ntimestamp 15.81 19.43\nimpact 69.56 45.00\norg country 12.19 39.81\norg group 23.06 100.25\norg role 20.94 36.31\nproduct 5.94 95.81\nresource country 13.88 28.38\nFigure 11: Intra-patch event lightness of each event of the\nsample prefix trace with the next-activity “a4” (“Completed-\nClosed”). The intra-patch event lightness is computed for\nthe three-top patches in the map of attention shown in\nthe right side of Figure 10 (“activity”, “org group” and\n“product”) selected according to the patch lightness.\nFigure 12: Heatmap of the global patch lightness computed\nfor all event log views (axis X) in all event logs (axis Y). “×”\ndenotes that the view reported on the axis X is missing in\nthe event log reported on the axis Y.\nintra-patch event lightness computed for the event within\nthe patch of the attention map of the prefix trace, which is\nassociated with the view. This plot explains that the activity\n“Accepted Wait” recorded in the third event of the prefix\ntrace, the org group “Org line C” recorded in the second\nevent of the prefix trace and the product “Prod84” recorded\nin the third and fourth positions of the prefix trace have\nthe highest effect on the decision of predicting “Completed-\nClosed”’ as next-activity of this sample prefix trace.\nWe continue analyzing the global effect of different\nFigure 13: Heatmap of the global patch lightness computed\nactivity by activity (axis Y) for all the views in the event log\nBPI13P . For each (next) activity category, the support of the\ncategory is reported in the brackets.\nviews by accounting for the patch lightness computed for\neach view and averaged on all the prefix traces of the train-\ning set. Figure 12 shows the heatmap of the average patch\nlightness computed on the training set in the event logs of\nthis study. This map shows which views have the higher\nglobal effect on the ViT decisions. As expected, the activity\ninformation is globally the most important information for\nthe ViT decisions in all the event logs. However, this expla-\nnation information shows that the “product” information\nis globally in the top-three ranked views in BPI13P , while\n“number of terms” and “action” information are globally\nin the top-three ranked views in BPI17O. These results\nsupport the decision of designing a multi-view approach not\nnecessarily based on the standard views (activity, timestamp\nand resource). In fact, this analysis shows that changing\nthe process changes the type of information most useful\nfor predicting the next activity of every running trace of\nthe study process. So, designing multi-view methods, which\nare able to incorporate any view, can actually contribute to\ngaining accuracy in next-activity prediction models.\nIn addition, to examine in depth the global effect of\ndifferent views on different categories of next activities, we\nanalyze the patch lightness, computed for each view, and\naveraged on each collection of training prefixes labeled with\na category of next activity. Figure 13 shows the average\nof the patch lightness computed on every next activity\ncategory appearing in the training set of the event log\nBPIC13P . These results show that the effect of specific views\non the ViT decisions may change depending on the next\nactivity category. In particular, “activity” conveys the most\nrelevant information for all activities. Instead “resource”,\nthat is the second most important view for the next-activity\n“a1” (“Accepted-Assigned”), “a2” (“Accepted-In Progress”),\n“a4” (“Completed-Closed”) and “a5” (“Queued-Awaiting\nAssignment”), is the fifth most important view for the\nnext activity “a3” (“Accepted-Wait”). On the other hand,\n“product” and “org country” are the second and third\nmost important views for the next activity “a3” (“Accepted-\nWait”). “timestamp” is never in the top-three ranked views\nfor any next activity. This analysis shows that views may\ncontribute differently to decisions about different types of\nactivities within the same event log. The views that most\ncontribute to recognizing a specific activity may change with\nrespect to the activity to be recognized. Hence, this analysis\nThis article has been accepted for publication in IEEE Transactions on Services Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TSC.2023.3331020\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON SERVICES COMPUTING 13\nFigure 14: Heatmap of the global intra-patch event lightness\ncomputed for each event position (axis Y) in the prefix traces\nthe event log BPI13P for each event log view (axis X)\nsupports the effectiveness of the decision to train the next-\nactivity predictive model accounting for all views available\nin the event log. In fact, information enclosed in different\nviews may contribute to gaining accuracy in recognizing\ndifferent categories of activities within the same problem.\nFinally, to explore the global effect of the intra-view,\nevent information on ViT decisions, we analyze the av-\nerage, intra-patch event lightness computed on the pixels\nof each studied patch, which encode the events occupying\nthe position under study in a prefix trace. Figure 14 shows\nthe average measurement of the intra-patch event lightness\ncomputed for each of the most recent four events recorded\nin the prefix traces of the training set of BPI13P . These\nresults show that the most recent events recorded in a prefix\nconceive the most important information for the decisions.\nThis conclusion can be drawn independently of both the\ncategory of the next activity and the view of the information\nconsidered in the event.\n8 C ONCLUSION\nThis paper illustrates a novel, multi-view, PPM method for\nnext-activity prediction. We resort to an imagery represen-\ntation that encodes multi-view information of a prefix trace\nas multiple color patches of an image. We take advantage\nof the self-attention modules of a ViT architecture to assign\npairwise attention values to pairs of image patches being\nable to account for multi-view relationships. In addition,\nself-attention modules allow us to incorporate explainability\ndirectly into the structure of a PPM model by disclosing\nexplainable information concerning specific views of the\nevent log and events of the prefix trace that more influenced\ndecisions. The experiments performed on several event logs\nshow the accuracy of the proposed approach and explore the\nexplanations produced through the attention mechanism.\nACKNOWLEDGMENTS\nVincenzo Pasquadibisceglie, Giovanna Castellano and Do-\nnato Malerba are partially supported by the project FAIR -\nFuture AI Research (PE00000013), Spoke 6 - Symbiotic AI\n(CUP H97G22000210007), under the NRRP MUR program\nfunded by the NextGenerationEU. Annalisa Appice is par-\ntially supported by project SERICS (PE00000014) under the\nNRRP MUR National Recovery and Resilience Plan funded\nby the European Union - NextGenerationEU.\nREFERENCES\n[1] N. Tax, I. Verenich, M. La Rosa, and M. Dumas, “Predic-\ntive business process monitoring with LSTM neural networks,”\nin International Conference on Advanced Information Systems\nEngineering, CAISE 2017, ser. LNCS. Springer, 2017, pp. 477–492.\n[2] M. Camargo, M. Dumas, and O. G. Rojas, “Learning accurate\nLSTM models of business processes,” in International Conference\non Business Process Management, BPM 2019, ser. LNCS, vol.\n11675. Springer, 2019, pp. 286–302.\n[3] V . Pasquadibisceglie, A. Appice, G. Castellano, and D. Malerba, “A\nmulti-view deep learning approach for predictive business process\nmonitoring,” IEEE Trans. Serv. Comput., vol. 15, no. 4, pp. 2382–\n2395, 2022.\n[4] ——, “Predictive process mining meets computer vision,” in\nBusiness Process Management Forum, BPM 2020, ser. LNBIP , vol.\n392. Springer, 2020, pp. 176–192.\n[5] V . Pasquadibisceglie, A. Appice, G. Castellano, D. Malerba, and\nG. Modugno, “ORANGE: outcome-oriented predictive process\nmonitoring based on image encoding and cnns,” IEEE Access,\nvol. 8, pp. 184 073–184 086, 2020.\n[6] F. Taymouri, M. L. Rosa, S. M. Erfani, Z. D. Bozorgi, and\nI. Verenich, “Predictive business process monitoring via generative\nadversarial nets: The case of next event prediction,” in 18th Int.\nConf. on Business Process Man., BPM 2020, ser. LNCS. Springer,\n2020, pp. 237–256.\n[7] N. Mehdiyev, J. Evermann, and P . Fettke, “A novel business\nprocess prediction model using a deep learning method,”Business\n& Information Systems Engineering, vol. 62, p. 143–157, 2018.\n[8] R. Galanti and et al, “Explainable predictive process monitoring,”\nin 2nd Int. Conf. on Process Mining. IEEE, 2020, pp. 1–8.\n[9] B. Wickramanayake, Z. He, C. Ouyang, C. Moreira, Y. Xu, and\nR. Sindhgatta, “Building interpretable models for business process\nprediction using shared and specialised attention mechanisms,”\nKnowledge-Based Systems, vol. 248, pp. 1–22, 2022.\n[10] R. Galanti, M. de Leoni, M. Monaro, N. Navarin, A. Marazzi,\nB. Di Stasi, and S. Maldera, “An explainable decision support\nsystem for predictive process analytics,”Engineering Applications\nof Artificial Intelligence, vol. 120, p. 105904, 2023.\n[11] A. Dosovitskiy and et al., “An image is worth 16x16 words:\nTransformers for image recognition at scale,” in 9th Int. Conf. on\nLearning Representations, ICLR 2021.\n[12] T. Bai, J. Luo, J. Zhao, B. Wen, and Q. Wang, “Recent ad-\nvances in adversarial training for adversarial robustness,” in 30th\nInternational Joint Conference on Artificial Intelligence, IJCAI\n2021, 2021, pp. 4312–4321.\n[13] W. Zhao, S. Alwidian, and Q. H. Mahmoud, “Adversarial training\nmethods for deep learning: A systematic review,” Algorithms,\nvol. 15, no. 8, 2022.\n[14] J. Zhao, X. Xie, X. Xu, and S. Sun, “Multi-view learning overview:\nRecent progress and new challenges,” Information Fusion, vol. 38,\npp. 43–54, 2017.\n[15] I. M. Kamal, H. Bae, N. I. Utama, and C. Yulim, “Data pixelization\nfor predicting completion time of events,” Neurocomputing, vol.\n374, pp. 64–76, 2020.\n[16] J. Evermann, J.-R. Rehse, and P . Fettke, “Predicting process be-\nhaviour using deep learning,” Decision Support Systems, vol. 100,\npp. 129 – 140, 2017.\n[17] A. Cuzzocrea, F. Folino, M. Guarascio, and L. Pontieri, “A multi-\nview learning approach to the discovery of deviant process in-\nstances,” in On the Move to Meaningful Internet Systems: OTM\n2015 Conferences, ser. LNCS. Springer, 2015, pp. 146–165.\n[18] ——, “A robust and versatile multi-view learning framework\nfor the detection of deviant business process instances,” Int. J.\nCooperative Inf. Syst., vol. 25, no. 4, pp. 1–56, 2016.\n[19] F. Folino, G. Folino, M. Guarascio, and L. Pontieri, “A multi-view\nensemble of deep models for the detection of deviant process\ninstances,” in ECML-PKDD 2020. Springer, 2020, pp. 249–262.\n[20] V . Pasquadibisceglie, A. Appice, G. Castellano, and D. Malerba,\n“Using convolutional neural networks for predictive process\nanalytics,” in 1st International Conference on Process Mining,\nICPM 2019. IEEE, 2019, pp. 129–136.\n[21] V . Pasquadibisceglie, G. Castellano, A. Appice, and D. Malerba,\n“FOX: a neuro-fuzzy model for process outcome prediction and\nexplanation,” in 3rd International Conference on Process Mining,\nICPM 2021. IEEE, 2021, pp. 112–119.\nThis article has been accepted for publication in IEEE Transactions on Services Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TSC.2023.3331020\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE TRANSACTIONS ON SERVICES COMPUTING 14\n[22] M. Harl, S. Weinzierl, M. Stierle, and M. Matzner, “Explainable\npredictive business process monitoring using gated graph neural\nnetworks,” J. Decis. Syst., vol. 29, pp. 312–327, 2020.\n[23] Z. A. Bukhsh, A. Saeed, and R. M. Dijkman, “Processtransformer:\nPredictive business process monitoring with transformer net-\nwork,” CoRR, vol. abs/2104.00721, 2021.\n[24] P . De Koninck, S. vanden Broucke, and J. De Weerdt, “act2vec,\ntrace2vec, log2vec, and model2vec: Representation learning for\nbusiness processes,” in 6th Int. Conf. on Business Process\nManagement, BPM 2018, ser. LNCS. Springer, 2018, pp. 305–321.\n[25] V . Pasquadibisceglie, A. Appice, G. Castellano, and D. Malerba,\n“Darwin: An online deep learning approach to handle concept\ndrifts in predictive process monitoring,” Engineering Applications\nof Artificial Intelligence, vol. 123, pp. 1–21, 2023.\n[26] S. Barbon Junior, P . Ceravolo, E. Damiani, and G. Marques Tavares,\n“Evaluating trace encoding methods in process mining,” in\nInternational Symposium: From Data to Models and Back,\nDataMod 2020, ser. LNCS. Springer, 2021, pp. 174–189.\n[27] G. M. Tavares, R. S. Oyamada, S. Barbon, and P . Ceravolo, “Trace\nencoding in process mining: A survey and benchmarking,” Eng.\nApplications of Artificial Intelligence, vol. 126, p. 107028, 2023.\n[28] M. Velmurugan, C. Ouyang, C. Moreira, and R. Sindhgatta, “Eval-\nuating stability of post-hoc explanations for business process\npredictions,” in 19th Int. Conf. on Service-Oriented Computing,\nICSOC 2021, ser. LNCS. Springer, 2021, pp. 49–64.\n[29] W. Rizzi, C. D. Francescomarino, and F. M. Maggi, “Explainability\nin predictive process monitoring: When understanding helps im-\nproving,” in Business Process Management Forum - BPM Forum\n2020,, ser. LNBIP . Springer, 2020, pp. 141–158.\n[30] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efficient estimation\nof word representations in vector space,” in 1st Int. Conf. on\nLearning Representations, ICLR 2013, 2013.\n[31] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\nAdv. in neural information proc. systems, vol. 30, pp. 1–11, 2017.\n[32] M. Andriushchenko and N. Flammarion, “Understanding and\nimproving fast adversarial training,” in Annual Conf. on Neural\nInformation Proc. Systems, NeurIPS 2020, 2020, pp. 16 048–16 059.\n[33] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and har-\nnessing adversarial examples,” in 3rd International Conference on\nLearning Representations, ICLR 2015, 2015, pp. 1–11.\n[34] S. Abnar and W. H. Zuidema, “Quantifying attention flow in\ntransformers,” in 58th Annual Meeting of the Association for\nComputational Linguistics, ACL 2020. Association for Computa-\ntional Linguistics, 2020, pp. 4190–4197.\n[35] N. Nader, F. E.-Z. EL-Gamal, and M. E. l, “Enhanced kinship\nverification analysis based on color and texture handcrafted tech-\nniques,” Research Square, 2022.\n[36] B. van Dongen, “BPI Challenge 2012, 4TU.Centre for Research\nData, Dataset,” 2012. [Online]. Available: https://data.4tu.nl/\nrepository/uuid:3926db30-f712-4394-aebc-75976070e91f\n[37] W. Steeman, “BPI Challenge 2013, Ghent University, Dataset,”\n2013. [Online]. Available: https://data.4tu.nl/repository/uuid:\na7ce5c55-03a7-4583-b855-98b86e1a2b07\n[38] J. Buijs, “Flexible evolutionary algorithms for mining structured\nprocess models,” Ph.D. dissertation, Department of Mathematics\nand Computer Science, 2014.\n[39] B. van Dongen, “BPI Challenge 2017 – Offer Log,\n4TU.Centre for Research Data, Dataset,” 2017. [Online].\nAvailable: https://data.4tu.nl/articles/dataset/BPI Challenge\n2017 - Offer log/12705737\n[40] ——, “BPI Challenge 2020 – Request for Payment,\n4TU.Centre for Research Data, Dataset,” 2020. [Online].\nAvailable: https://data.4tu.nl/articles/dataset/BPI Challenge\n2020 Request For Payment/12706886\n[41] J. Bergstra, R. Bardenet, Y. Bengio, and B. K ´egl, “Algorithms for\nhyper-parameter optimization,” in Annual Conference on Neural\nInformation Processing Systems, NIPS 2011, 2011, pp. 2546–2554.\n[42] A. Madry and et al., “Towards deep learning models resistant to\nadversarial attacks,” in 6th Int. Conf. on Learning Representations,\n2018, pp. 1–10.\n[43] S.-M. Moosavi-Dezfooli, A. Fawzi, and P . Frossard, “Deepfool:\nA simple and accurate method to fool deep neural networks,”\nin IEEE Conf. on Computer Vision and Pattern Recognition,\nCVPR 2016. IEEE, 2016, pp. 2574–2582.\nVincenzo Pasquadibiscegliereceived a Ph.D.\nin Computer Science from the University of Bari\nHe is a Researcher in the Department of Com-\nputer Science of the University of Bari Aldo\nMoro. His research activity concerns process\nmining, deep learning and data-centric AI. He\nwas involved in a regional research project on\nprocess mining. He received 2019 IET’s Vision\nand Imaging Award ICDP 2019. He is member\nof the IEEE Task Force on Process Mining.\nAnnalisa Appice is an Associate Professor at\nthe Department of Computer Science, University\nof Bari Aldo Moro, Italy. Her research interests\ninclude data mining with data streams, cyber and\nevent data. She published more than 180 papers\nin international journals and conferences. She\nwas Program co-Chair of ECMLPKDD 2015, IS-\nMIS 2017 and DS 2020. She was Journal Track\nco-Chair of ECMLPKDD 2021. She was co-chair\nof several editions of ML4PM. She is a member\nof the editorial board of MACH, DAMI, EAAI and\nJIIS. She is member of the IEEE Task Force on Process Mining.\nGiovanna Castellano is an Associate Profes-\nsor at the Department of Computer Science,\nUniversity of Bari Aldo Moro, Italy, where she\nis the coordinator of the Computational Intelli-\ngence Lab. She is member of the IEEE Soci-\nety, the EUSFLAT society and the INDAM-GNCS\nsociety. Her research interests are in the area\nof Computational Intelligence and Computer Vi-\nsion. She has published more than 200 papers\nin international journals and conferences. She\nis Associate Editor of several international jour-\nnals. She was General chair of IEEE-EAIS2020. She is a member of the\nIEEE Task Force on Explainable Fuzzy Systems.\nDonato Malerba is a Full Professor in the De-\npartment of Computer Science at the Univer-\nsity of Bari Aldo Moro in Italy. He has been\nresponsible for the local research unit in several\nEuropean and national projects. Moreover, he\nhas served as the Program (co-)Chair for con-\nferences like IEA-AIE 2005, ISMIS 2006, SEBD\n2007, ECMLPKDD 2011, and as the General\nChair of ALT/DS 2016 and IJCLR 2023. He is\nalso a member of the editorial boards of several\ninternational journals. He is the scientific coor-\ndinator of Spoke 6 - Symbiotic AI within the Italian project ”Future\nAI Research (FAIR),” funded by the NextGenerationEU initiative. His\nacademic career includes the publication of over 350 papers in interna-\ntional journals and conferences. His research interests primarily revolve\naround machine learning and big data analytics. He participates to the\nIEEE Task Force on Process Mining.\nThis article has been accepted for publication in IEEE Transactions on Services Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TSC.2023.3331020\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8571098446846008
    },
    {
      "name": "Overfitting",
      "score": 0.7709856629371643
    },
    {
      "name": "Machine learning",
      "score": 0.6723302602767944
    },
    {
      "name": "TRACE (psycholinguistics)",
      "score": 0.6626259088516235
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6614640951156616
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.6564006209373474
    },
    {
      "name": "Process mining",
      "score": 0.5393469333648682
    },
    {
      "name": "Transformer",
      "score": 0.5381750464439392
    },
    {
      "name": "Process (computing)",
      "score": 0.4866112172603607
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.43629857897758484
    },
    {
      "name": "Data mining",
      "score": 0.34385946393013
    },
    {
      "name": "Business process",
      "score": 0.303869366645813
    },
    {
      "name": "Artificial neural network",
      "score": 0.24227216839790344
    },
    {
      "name": "Work in process",
      "score": 0.24018430709838867
    },
    {
      "name": "Business process management",
      "score": 0.16526958346366882
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Marketing",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I5561750",
      "name": "University of Bari Aldo Moro",
      "country": "IT"
    }
  ],
  "cited_by": 16
}