{
  "title": "How Beginning Programmers and Code LLMs (Mis)read Each Other",
  "url": "https://openalex.org/W4391417542",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A3176608577",
      "name": "Sydney Nguyen",
      "affiliations": [
        "Wellesley College"
      ]
    },
    {
      "id": "https://openalex.org/A5092120710",
      "name": "Hannah McLean Babe",
      "affiliations": [
        "Oberlin College"
      ]
    },
    {
      "id": "https://openalex.org/A4366204926",
      "name": "Yangtian Zi",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2163638586",
      "name": "Arjun Guha",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2586217196",
      "name": "Carolyn Jane Anderson",
      "affiliations": [
        "Wellesley College"
      ]
    },
    {
      "id": "https://openalex.org/A2785363959",
      "name": "Molly Q. Feldman",
      "affiliations": [
        "Oberlin College"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1974089349",
    "https://openalex.org/W4362659486",
    "https://openalex.org/W2032568497",
    "https://openalex.org/W1951724000",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W4318159335",
    "https://openalex.org/W4366204357",
    "https://openalex.org/W4388556611",
    "https://openalex.org/W2093055647",
    "https://openalex.org/W4283775237",
    "https://openalex.org/W3007910004",
    "https://openalex.org/W3176062377",
    "https://openalex.org/W4229826799",
    "https://openalex.org/W2786623891",
    "https://openalex.org/W2066394248",
    "https://openalex.org/W4211263275",
    "https://openalex.org/W4389158536",
    "https://openalex.org/W3177928934",
    "https://openalex.org/W4312943943",
    "https://openalex.org/W2281090488",
    "https://openalex.org/W3048222927",
    "https://openalex.org/W2998011150",
    "https://openalex.org/W2157289187",
    "https://openalex.org/W2605403059",
    "https://openalex.org/W2036676782",
    "https://openalex.org/W3146720657",
    "https://openalex.org/W2010775113",
    "https://openalex.org/W2808967136",
    "https://openalex.org/W3161516024",
    "https://openalex.org/W2135841285",
    "https://openalex.org/W4382239980",
    "https://openalex.org/W4321162272",
    "https://openalex.org/W1540823594",
    "https://openalex.org/W4245383073",
    "https://openalex.org/W4386584937",
    "https://openalex.org/W1694295455",
    "https://openalex.org/W2146496290",
    "https://openalex.org/W4382654294",
    "https://openalex.org/W4323033692",
    "https://openalex.org/W4384304865",
    "https://openalex.org/W4392297945",
    "https://openalex.org/W4365601419",
    "https://openalex.org/W2165365113",
    "https://openalex.org/W3017863658",
    "https://openalex.org/W1973719497",
    "https://openalex.org/W4366547384",
    "https://openalex.org/W2470712504",
    "https://openalex.org/W4394745423",
    "https://openalex.org/W4226485558",
    "https://openalex.org/W2082434633",
    "https://openalex.org/W4386099272",
    "https://openalex.org/W2941726745",
    "https://openalex.org/W4293066270",
    "https://openalex.org/W4312933868",
    "https://openalex.org/W4321013654",
    "https://openalex.org/W2013091051",
    "https://openalex.org/W4320854935",
    "https://openalex.org/W4213451626",
    "https://openalex.org/W2012312630",
    "https://openalex.org/W2063649921",
    "https://openalex.org/W2955669146",
    "https://openalex.org/W4220962633",
    "https://openalex.org/W2963663592",
    "https://openalex.org/W4383888921",
    "https://openalex.org/W4225108562",
    "https://openalex.org/W4307886640",
    "https://openalex.org/W3160674997",
    "https://openalex.org/W4313477803",
    "https://openalex.org/W4281763794",
    "https://openalex.org/W4366548330",
    "https://openalex.org/W4281669078"
  ],
  "abstract": "Generative AI models, specifically large language models (LLMs), have made\\nstrides towards the long-standing goal of text-to-code generation. This\\nprogress has invited numerous studies of user interaction. However, less is\\nknown about the struggles and strategies of non-experts, for whom each step of\\nthe text-to-code problem presents challenges: describing their intent in\\nnatural language, evaluating the correctness of generated code, and editing\\nprompts when the generated code is incorrect. This paper presents a large-scale\\ncontrolled study of how 120 beginning coders across three academic institutions\\napproach writing and editing prompts. A novel experimental design allows us to\\ntarget specific steps in the text-to-code process and reveals that beginners\\nstruggle with writing and editing prompts, even for problems at their skill\\nlevel and when correctness is automatically determined. Our mixed-methods\\nevaluation provides insight into student processes and perceptions with key\\nimplications for non-expert Code LLM use within and outside of education.\\n",
  "full_text": "How Beginning Programmers and Code LLMs\n(Mis)read Each Other\nSydney Nguyen\nWellesley College\nUSA\nHannah McLean Babe\nOberlin College\nUSA\nYangtian Zi\nNortheastern University\nUSA\nArjun Guha\nNortheastern University and Roblox\nUSA\na.guha@northeastern.edu\nCarolyn Jane Anderson\nWellesley College\nUSA\ncarolyn.anderson@wellesley.edu\nMolly Q Feldman\nOberlin College\nUSA\nmfeldman@oberlin.edu\nABSTRACT\nGenerative AI models, specifically large language models (LLMs),\nhave made strides towards the long-standing goal of text-to-code\ngeneration. This progress has invited numerous studies of user in-\nteraction. However, less is known about the struggles and strategies\nof non-experts, for whom each step of the text-to-code problem\npresents challenges: describing their intent in natural language,\nevaluating the correctness of generated code, and editing prompts\nwhen the generated code is incorrect. This paper presents a large-\nscale controlled study of how 120 beginning coders across three\nacademic institutions approach writing and editing prompts. A\nnovel experimental design allows us to target specific steps in the\ntext-to-code process and reveals that beginners struggle with writ-\ning and editing prompts, even for problems at their skill level and\nwhen correctness is automatically determined. Our mixed-methods\nevaluation provides insight into student processes and perceptions\nwith key implications for non-expert Code LLM use within and\noutside of education.\nCCS CONCEPTS\n‚Ä¢ Human-centered computing ‚ÜíUser studies; ‚Ä¢ Social and\nprofessional topics ‚ÜíComputing education; ‚Ä¢ Computing\nmethodologies ‚ÜíArtificial intelligence; Machine learning ; ‚Ä¢ Soft-\nware and its engineering ;\nACM Reference Format:\nSydney Nguyen, Hannah McLean Babe, Yangtian Zi, Arjun Guha, Car-\nolyn Jane Anderson, and Molly Q Feldman. 2024. How Beginning Pro-\ngrammers and Code LLMs (Mis)read Each Other. In Proceedings of the CHI\nConference on Human Factors in Computing Systems (CHI ‚Äô24), May 11‚Äì\n16, 2024, Honolulu, HI, USA. ACM, New York, NY, USA, 26 pages. https:\n//doi.org/10.1145/3613904.3642706\n1 INTRODUCTION\nComputer scientists have been working towards programming\nin natural language for decades [ 4, 38, 86], often with the goal\nCHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA\n¬© 2024 Copyright help by the owner/author(s).\nThis work is licensed under a Creative Commons Attribution-Share Alike\nInternational 4.0 License.\nThis is the author‚Äôs version of the work. It is posted here for your personal use. Not\nfor redistribution. The definitive Version of Record was published in Proceedings of the\nCHI Conference on Human Factors in Computing Systems (CHI ‚Äô24), May 11‚Äì16, 2024,\nHonolulu, HI, USA , https://doi.org/10.1145/3613904.3642706.\nModeldef foo( ):\ndef foo( ):\ndef foo( ):\nfoo(  )foo(  )\nCrafting the Prompt\nThe TaskUnderstanding the TaskArticulating in NL\nAssessing Correctness\nReading Code Writing Tests\nIdentifying What‚Äôs WrongRevising the Prompt\nEditing the Prompt\nFigure 1: Visualization of the multi-step process of querying\na large language mode of code (Code LLM). The user starts\nwith crafting their prompt in natural language (NL). They\nprovide the prompt to the model, which produces code. The\nuser then assesses the correctness of the generated code. If\nthere are errors, they must identify how to resolve them\nand how to edit the prompt. This continues in an iterative\nfashion.\nof making programming easier for a broader set of users. Recent\nadvances in generative AI have brought us nearer to this goal. In\nprogramming, along with fields like digital art [79, 81, 83], creative\nwriting [2, 45, 68], and digital music [ 1, 63], generative AI has\nreduced the technical skills that users need by allowing them to\nprompt a model with a natural language description of their desired\noutput.\nIn many fields, experts have started to use generative AI to\naccelerate their work, including in software engineering, where\nlarge language models of code (Code LLMs) have enhanced expert\nprogrammer productivity [69, 76, 105]. However, to fulfill their po-\ntential of democratizing these fields, models must be usable without\nextensive technical training at each stage of creation: 1) writing\narXiv:2401.15232v2  [cs.HC]  7 Jul 2024\nCHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA Nguyen et al.\nprompts for the model, 2) evaluating model output for quality, and\n3) iteratively refining prompts when generation is unsuccessful.\nProgramming presents a particularly challenging domain for\nnon-experts. Like art, computer science has evolved an extensive\ntechnical vocabulary; since generative models are trained largely\non professional code, they may not work as well if users lack this\nvocabulary. In visual art, music, and creative writing, a user can\nquickly determine whether they like the generated output even if\nthey are not an expert (embodying the clich√© ‚ÄúI don‚Äôt know anything\nabout art, but I know what I like‚Äù). However, this attitude does not\nextend to programming. It is very challenging for a non-expert\nto evaluate the quality of a generated program. Even when a user\nknows enough to determine a generated program is incorrect, they\nalso need to understand it well enough to know what needs to\nchange and how to update their prompt.\nIn order to use a Code LLM, non-experts must grapple with a\nmulti-step process (Figure 1). First, they must have a clear under-\nstanding of what they want the code to do. This may seem trivial,\nbut research on requirements engineering has shown that it can be\nchallenging [75]. Next, the user must clearly articulate the intended\nbehavior of the program in natural language to the model. Once\nthe model generates code, the user must evaluate its correctness\nby reading it or writing tests. If the code is not correct, they must\ndetermine what has gone wrong, and update their prompt accord-\ningly. This requires not only understanding the generated code, but\nalso, understanding the model‚Äôs generative process. These barri-\ners mirror well-known challenges for non-experts with end-user\nprogramming [50] and classical AI systems [53].\nThere is a growing body of work studying how non-expert pro-\ngrammers use AI-assisted programming systems in naturalistic\nsettings [48, 78]. However, in open-ended tasks, it is difficult to\ndecouple the steps of the code generation process, since they feed\neach other: if the user fails to identify incorrect code and moves on,\ntheir editing process can‚Äôt be observed. We present results from a\ncarefully-controlled experiment targeting two steps in the code gen-\neration process: prompt creation (How do users describe the intended\nprogram in natural language? ) and prompt modification (How do\nusers modify their prompts when a generated program is incorrect? ).\nOne challenge in studying how non-experts use Code LLMs\nis selecting tasks that make sense to them. For example, replicat-\ning Barke et al. [6]‚Äôs insightful study of experienced programmers\nwould not be appropriate for novices, because the tasks presuppose\ntechnical knowledge. Novices have diverse goals, backgrounds, and\nfamiliarity with mathematical and computational thinking. Our\nsolution is to target a large population of near-novices with similar\nexperience levels: university students who have completed a single\nintroductory computer science course (CS1). This allows us to select\ntasks that are conceptually familiar to them.\nOur Approach. We ask whether students who have completed\nCS1 can effectively prompt a Code LLM to solve tasks from their\nprevious course. In order to isolate students‚Äô experiences in writing\nand editing prompts, our experiment presents tasks as input/output\npairs and tests the generated code for correctness. This provides\nin-depth insight into the processes they develop for describing code\nin natural language and iteratively refining their prompts. We pose\nthree main research questions:\n‚Ä¢RQ1: Can students who have completed a CS1 course effec-\ntively prompt a Code LLM to generate code for questions\nfrom their previous courses?\n‚Ä¢RQ2: What is the origin of student challenges with Code\nLLMs? Do these differ across different groups of students?\n‚Ä¢RQ3: What are students‚Äô mental models of Code LLMs and\nhow do they effect their interactions?\nWe find that students struggle significantly with this task, even\nthough we pose problems tailored to their skill level and test code\ncorrectness for them. In essence, beginning programmers and cur-\nrent Code LLMs tend to misread each other : the Code LLM fails to\ngenerate working code based on student descriptions and students\nhave a hard time adapting their descriptions to the model. Our\nstudy has concerning implications for democratizing programming:\nif these students, who already have basic skills in code explana-\ntion and understanding, struggle with this simplified task, the full\nnatural language-to-code task‚Äîwhere the user has to determine\ncorrectness themselves‚Äîmust be very challenging indeed for true\nnovices. This finding also has important implications for education.\nCode LLMs have sparked an intense debate over the future of com-\nputing education, including claims that traditional programming\ntraining is no longer necessary [65, 100]. By contrast, our findings\nhighlight the continuing importance of teaching students technical\ncommunication and code understanding.\nOur work differentiates itself from previous work in three key\nways: scale, population, and experimental design. First, we study\n120 students solving 48 different programming problems. To our\nknowledge, no previous work has studied user interactions with\nCode LLMs at this scale. Second, we focus on a near-novice pop-\nulation with fairly uniform levels of experience, allowing us to\ncarefully tailor tasks to their skill level. Finally, we use an experi-\nmental paradigm that allows us to isolate the prompt writing and\nediting aspects of the task.1\n2 RELATED WORK\nOur work focuses on how programmers use LLMs to turn natu-\nral language into code. Programming with natural language is a\ndecades old proposition [ 67] and has led to several ideas about\nbringing programming closer to how users communicate [70]. For\ninstance, Hindle et al. [39] imagined that future language models\ncould be effective at turning natural language to code, a prediction\nthat has been borne out with Code LLMs.\nBy exploring beginner interactions with Code LLMs, our study\ncontributes to a growing body of work on how non-experts in-\nteract with emerging automated technologies [98], ranging from\nautomated feedback [22, 44, 94] to augmented reality [42, 80]. We\nsituate our study within existing work on user interactions with\nCode LLMs below.\nExperienced programmers and LLMs. We study how beginning\nprogrammers interact with a Code LLM, the same foundational\ntechnology that powers autocomplete tools such as GitHub Copilot\nand others [14, 15, 95]. These tools are promoted as productivity-\nboosting technology for experienced programmers. Recent in-the-\nwild studies and surveys indicate that these tools are popular with\n1Data collected as part of this work is publicly available at https://doi.org/10.17605/\nOSF.IO/V2C4T.\nHow Beginning Programmers and Code LLMs (Mis)read Each Other CHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA\nexpert programmers, improve their self-perception of productivity,\nand shift their work from writing code to understanding LLM out-\nputs [10, 60, 69]. In contrast, our study of beginners‚Äô interactions\nwith a Code LLM reveals that (1) they have mixed success with\nwriting natural language prompts, (2) and they often struggle to\nunderstand LLM-generated code.\nVaithilingam et al. [96] present the earliest academic study of\nGitHub Copilot with 24 students (undergraduate‚ÄìPhD) and three\ntasks. Their main finding is that although participants enjoyed\nusing it, Copilot did not help them code faster or write more correct\ncode. We design our study for less experienced participants. For\nexample, we developed a web interface that is much simpler than\na professional IDE. The same study reports that their participants\noften struggled to validate LLM-generated code, and we avoid this\nby testing generated code for our participants automatically.\nSince Copilot is a general autocomplete tool, one can use it in sev-\neral ways: to produce code given code, to generate documentation\nfrom code, to turn natural language into code, and so on. Grounded\nCopilot [6] studies experienced programmers and reports that they\nprefer using it to turn natural language into code [6, Section 4.2.3].\nThus our study design focuses on the natural language to code task,\nbut with beginning programmers.\nNon-experts and LLMs. Like us, several researchers have consid-\nered the impact of using Code LLMs for the text-to-code task with\nnon-experts, specifically in educational settings. Our work is larger\nin scale than prior work (120 students from 3 institutions and 48\nproblems in 8 categories), which allows us to perform statistical\nanalyses that require large sample sizes to be reliable. Moreover, our\nexperiment design allows us to investigate key research questions\nthat prior work has not been able to ask, such as identifying the\nprompting strategies that beginners use, determining how they\nmodify prompts that do not work, and studying several factors that\naffect their success.\nPrather et al . [78] study 19 students using Copilot for a final\nproject in a CS1 course: building the game Minesweeper. They\nfound that students struggled to use Copilot, even over the course\nof a week. We reach a similar conclusions with our study, with\n48 problems that are much simpler than building a working video\ngame.\nKazemitabaar et al. [48]develop CodingSteps, a web-based Python\nlearning environment that allows users to query Codex. The paper\ncompares 33 participants (10‚Äì17 years old) with access to Codex to\n36 students programming independently, working on the same set\nof 45 programming problems over several weeks. Their findings in-\ndicate that Code LLMs may benefit student learning outcomes. How-\never, because CodingSteps presents students with expert-written\nproblem descriptions, their results do not shed light on whether\nbeginners can write natural language prompts independently. They\nreport that 32% of student prompts are verbatim copies of the expert-\nwritten problem descriptions. In contrast, our study is carefully\ndesigned to avoid this problem by showing students input/output\nexamples instead of natural language descriptions. We also investi-\ngate the strategies that students use to understand model output\nand modify their prompts. Kazemitabaar et al. [48] do not address\nthese kinds of questions, partly because their students received\nfeedback from instructors throughout the experiment.\nPromptly [19] studies 54 students writing prompts for three\nCS1 problems. Our substantially larger scale (120 students and 48\nproblems) allows us to explore research questions beyond what they\nstudy, such as the how students change their prompting strategies,\nand demographic factors that influence success rates. Our paper\nalso presents a detailed analysis of LLM output, such as the kinds\nof errors that appear in LLM-generated code, and the impact of\nnon-determinism on participants‚Äô success.\nLau and Guo [52] interviewed 20 CS1/CS2 instructors in early\n2023 about their perceptions of ChatGPT and LLM technologies.\nThey report that instructors hold a diverse set of perspectives:\nsome wanted to ‚Äúban it‚Äù and others felt urged to integrate these\ntechnologies into curricula to prepare students for future jobs that\nmay require using LLM technology. The students in our study echo\nmany of the concerns and desires raised by instructors in Lau and\nGuo [52].\nIt is also possible to use language models to assist students learn-\ning to program, without having the model write code for the student.\nFor example, Geng et al. [31] use language models to localize type\nerrors in OCaml, but not to correct them. Like our study, this work\nisolates the interaction mode in which students use Code LLMs;\nhowever, we study prompt writing and editing, while they study\nerror detection and explanation.\nAlternatives to inline code completion. Copilot and related tools\nsuggest inline code completions, but there are other ways to interact\nwith AI-assisted programming tools. Vaithilingam et al. [96]present\nnew interfaces for Visual Studio that present code changes. Liu\net al. [62] build a new interaction model, grounded abstraction\nmatching, which targets spreadsheets and data frames, constraining\nthe generated code to support grounding. These ideas are exciting\nparallel directions for Code LLM interaction in addition to the\nnatural language prompting approach we study here.\nCode LLMs beyond text-to-code. For a beginning programmer,\nfeedback from an expert teacher or teaching assistant can be in-\nvaluable. However, access to expert feedback is limited. There is\na long line of research that tries to address this shortage by de-\nveloping systems that generate actionable feedback for students\n[37, 40, 82, 90, 94]. Phung et al. [77] show that LLMs can help build\nthese systems and generate higher quality feedback than prior rule-\nbased approaches. In contrast to our human experiment, they eval-\nuate on benchmark problems. Moreover, their system is intended to\nhelp beginners write code directly, whereas our experiment focuses\non prompt writing.\nAnother body of work focuses on automated program repair [34],\nwhich can be used to fix trivial mistakes that frustrate beginners.\nTraditional automated program repair systems have required sig-\nnificant engineering for each programming language and problem\ndomain. Joshi et al. [47] show that an LLM trained to generate code\ncan be employed to repair simple coding mistakes.\nSimilarly, Leinonen et al. [55] report that Code LLMs are better\nat explaining code than beginning students, and Leinonen et al .\n[56] show that an LLMs explanation of a program error can be\nbetter than default error messages. This is further evidence that\nLLM technology may help students learn to write code directly.\nRecent additional efforts include Finnie-Ansley et al. [25], who\nreport that Codex is remarkably good at generating code from\nCHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA Nguyen et al.\nnatural language prompts from a CS1 class and several variations\nof the Rainfall Problem; Dakhel et al. [17], who compare the quality\nof Codex-generated code to student-written code; and Babe et al. [3],\nwho use student-written prompts to benchmark Code LLMs. Finally,\nCode LLMs have applications that go beyond natural-language-\nto-code, and researchers are using them as building blocks for a\nvariety of other tasks [ 5, 12, 23, 26, 47, 57, 69, 71, 77, 84, 87, 101].\nThe aforementioned papers present new tools, benchmarks, and\nstudies of LLM capabilities. But, they do not study users‚Äô abilities\nto prompt models, which is the focus of our work.\nUsing LLMs for non-programming tasks. Researchers are cur-\nrently exploring a wide variety of applications for LLMs beyond\ncomputational tasks. While we do not survey the full range of\nsuch work, two recent papers are particularly relevant to our task.\nZamfirescu-Pereira et al. [103] study non-experts prompting an\nLLM to produce recipes. Their participants actively avoided sys-\ntemic testing, which we address by automating testing. Like them,\nwe find that participants‚Äô mental models of LLMs are very different\nfrom how they actually work. Singh et al. [89] compare user inter-\nactions with a multimedia writing interface with LLM-generated\naudio, text, and image suggestions. Our post-study interview and\nsurvey was inspired by their exploration of participant‚Äôs percep-\ntions of AI.\n3 STUDY DESIGN\nOur work explores whether beginning programmers can effectively\nprompt Code LLMs. We investigate this question through a multi-\ninstitutional [24], lab-based study, asking 120 students who com-\npleted a CS1 course to describe 8 out of 48 possible problems pre-\nsented via input/output examples.\nIn this section, we discuss three major aspects of our study\ndesign:\n(1) Why do we use a controlled experiment?\n(2) How do we successfully present problems to students?\n(3) How do we select problems that are appropriate for students?\nWe discuss the logistics of implementing the study in Section 4.\n3.1 Experimental Environment: In the Lab vs.\nIn the Classroom\nStudies of student interactions with programming tools can be\ngrouped into three main categories: studies within the context of\na course during the term, post-hoc analyses of educational data,\nor controlled, lab-based experiments. Post-hoc analyses are not\ncurrently possible, since there is a lack of available educational\nCode LLM data. We discuss the decision between a course-based\nstudy and lab-based study below.\nThere are many benefits to real-world studies conducted in a\ncourse context, including ease of access to participants and normal-\nized educational background [78]. It is easier to study how technol-\nogy directly impacts learning by using it alongside instruction [48]\nor as an evaluative method [44]. At the same time, these studies\ncannot be as easily controlled: participation may be optional (only\naround 12% of students chose to participate in Denny et al. [19]);\nparticipants may explicitly be learning through the task, making it\nhard to compare their responses across problems [48]; and in-depth\ninterviews are challenging to conduct.\nLab-based studies benefit from greater uniformity in observa-\ntions, which facilitates statistical analysis, and longer experimental\nsessions. We chose a lab-based experiment because our research\nquestions focus on the usability of Code LLMs for beginning pro-\ngrammers and on their processes, rather than their educational\noutcomes. Specifically, the process of working with a Code LLM re-\nquires multiple, interdependent steps: (1) forming an intent, (2) craft-\ning a prompt to describe the intent, (3) evaluating the quality of\nthe LLM-generated code, (4) editing the prompt when the code is\nwrong, (5) editing the code manually, or (6) giving up and writing\ncode manually (Figure 1). Our goal was to isolate processes (2) and\n(4).\nOur study limits user interactions in order to isolate prompt writ-\ning and editing strategies. One key feature of our paradigm is that\nwe automatically test the generated code. In most observational\nstudies, programmers determine on their own whether the gener-\nated code is correct. This is itself an interesting process. However,\nstudying this aspect of Code LLM interaction comes at the cost\nof studying prompt editing: if a programmer mistakenly accepts\nincorrect code, they will move on to the next task without editing.\nPrather et al. [78] report that many of their participants mistakenly\naccepted incorrect code. Beginning students are particularly likely\nto err in this way: they may struggle to understand generated code,\nand their lack of confidence in their own abilities may make them\ntrust the automated system over their own judgment (an example\nof automation bias [18, 30, 32, 91]).\nFinally, a key contribution of our work is its scale: we study 120\nparticipants across 3 institutions and 48 programming tasks, while\nprevious studies have had fewer participants and problems. We\nrecruit participants from three U.S. institutions: an R1 university\n(Northeastern University), a small liberal arts college (Oberlin Col-\nlege), and a women‚Äôs college (Wellesley College). This selection\nincreases the likelihood that our findings will generalize across insti-\ntutions. Our scale allows us to explore how diverse factors, such as\nprior non-curricular programming experience, first-generation sta-\ntus, and mathematics coursework, affect participant success. These\nkind of statistical analyses require large sample sizes and work best\nwith even observations of participants and problems, which are\nchallenging to obtain in course settings.\n3.2 How to Describe Problems to Students:\nInput/Output Examples vs. Written\nDescriptions\nA key design decision for studies of Code LLM interactions is how\nto present the task. In classroom environments, students are usually\ngiven instructions for what to program via written descriptions.\nThis makes sense, given that the student‚Äôs goal is to write code.\nHowever, natural language presentation poses critical issues for\nour key research questions. In our study, the goal is to write natural\nlanguage descriptions of problems, not to write code. A core goal\nis to understand how students approach the natural-language-to-\ncode task. If the task is presented in natural language, students may\nsimply reuse this text rather than putting the task into their own\nwords; our results would no longer measure beginning programmer\nHow Beginning Programmers and Code LLMs (Mis)read Each Other CHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA\nsuccess, but instead expert description success. Prior work shows\nthat this is a serious concern: in Kazemitabaar et al. [48]‚Äôs study of\nK-12 students, up to 49% of submissions for challenging problem\ncategories were copied from the expert-written task description.\nEven if participants do not directly copy a description, its word-\ning could influence how participants describe the task. One chal-\nlenge for beginning programmers is recalling and applying techni-\ncal vocabulary; presenting them with a natural language description\nof the task might remind them of terminology that they would not\nhave recalled on their own. This would endanger our goal of assess-\ning beginning programmers‚Äô abilities to prompt code generation\nmodels, since in many natural settings, they would not have an\nexpert description to rely on.\nWe therefore rely on a popular alternative for describing pro-\ngram behavior: input/output examples (Figure 3). Students also\ncould reference the function name and parameter names. Our par-\nticipants had taken CS1 classes where natural language descrip-\ntions are frequently accompanied by input/output examples (see\nAppendix A.1.2), making this a familiar way of communicating pro-\ngram behavior. Several CS1 courses, and some of the assignments\nused in our CS1 courses, go beyond this and require students to\nconstruct their own examples or even practice test-driven devel-\nopment [21, 27]. However, our study does not require students to\nwrite their own tests.\nAvoiding natural language presentation is critical in order to\nstudy how beginning programmers describe problems in their own\nwords. However, it comes with two risks. First, the input/output par-\nadigm may increase task difficulty, since participants must identify\nthe key pattern on their own. Although understanding natural lan-\nguage descriptions of coding tasks is not always easy for beginning\nprogrammers, it is likely easier than our input/output paradigm.\nSecond, input/output examples run the risk of underspecification\n[35, 88] ‚Äì there may be more than one program that performs\nthe correct input-output mapping. To determine that the provided\ntests adequately described the problem, we confirmed that our pro-\nvided test sets had 100% code coverage for a correct solution and\nperformed mutation testing [46]. We also calculated participants‚Äô\nsuccess using only the provided test cases: if the generated code\npassed the provided tests, it was deemed correct, ensuring that the\nproblem presentation aligned directly with the feedback to the user.\nWe feel that these potential issues pose less of a risk to our key\nresearch questions than the copy/paste or word bias risks posed\nby a natural language presentation. Other researchers have also\nused an input/output presentation paradigm in studying beginner\ninteractions with Code LLMs [19].\n3.3 Problem Selection: Previously Seen Tasks vs.\nNew Tasks\nThe natural language-to-code task requires participants to describe\nspecific programming problems. Previous work exhibits varied\napproaches to problem selection, from a single challenging problem\nin Prather et al. [78] to three simple problems in Denny et al. [19]\nto a set of 45 problems in 5 categories in Kazemitabaar et al. [48].\nOur main goal was to select problems at an appropriate level for\nstudents who had completed only CS1. Since our research questions\nfocus on student prompting processes, not learning outcomes, we\nchose problems at a similar level to what participants might be\nable to code independently. Asking students to solve new or more\ncomplex problem types increases the likelihood that the Code LLM\nwill generate unfamiliar or difficult to understand code, making the\nprompt editing process more difficult. We therefore adapted Python\nproblems specifically from CS1 course materials at each institution.\nWe made small changes to facilitate input/output testing or adjust\nproblem difficulty. Appendix A.1 contains two examples of how\nsource problems were adapted.\nWe selected 48 problems balanced across eight conceptual cat-\negories from CS1 (Figure 2), similar to Kazemitabaar et al . [48],\nbut with more categories and problems. Each individual problem\nwas assigned to 20 students; we balanced the experimental lists\nto control for ordering effects, so that each participant solved one\nproblem in each category, and the average difficulty of each prob-\nlem list was roughly the same. To facilitate difficulty and category\ncoverage, previous CS1 instructors were asked to provide additional\nproblems as needed. Problems such as exp (Figure 3), for instance,\nrequire students to only recognize that numbers in a list are being\nsquared. Other problems ask students to remember complex data\nstructures (e.g. lists, dictionaries), but not the specific Python syntax\nfor them. We further discuss student understanding of the problems\nin Section 7.2 and Appendix B.2.\nIn order to study interactions between Code LLMs and students,\nit is important to select problems that cannot be trivially solved by a\nCode LLM without any natural language description. Very common\nfunctions (for instance, shorten_url) can be solved from a func-\ntion signature alone, regardless of the accompanying description.\nTo validate our problems, we first checked that the model could\nnot solve problems from their function/parameter names alone and,\nif they could, edited the names accordingly. We also solved each\nproblem using the Code LLM to ensure that a working natural lan-\nguage description existed. Finally, to address the nondeterminism of\nCode LLMs, we ran each validation check multiple times to obtain\na stable estimate of these results (¬ß5.2).\n4 STUDY LOGISTICS\nThe previous section (¬ß3) described our multi-institutional experi-\nmental design. In this section, we discuss the logistics of participant\nrecruitment and executing the study.\n4.1 Charlie Interface\nWe built a web application for the experiment called Charlie the\nCoding Cow or Charlie. Charlie presents one problem per page,\ndisplaying the function signature and several input/output exam-\nples (Figure 3a). Participants write natural language descriptions\nin a text box. When they submit a description, the Charlie server\nprompts Codex with the function signature and their description\nformatted as a docstring (Figure 4). After Codex responds, Charlie\nshows students the Codex-generated code and displays whether it\nworks on the given input/output examples (Figure 3b).\nCharlie does not permit participants to edit the generated code,\nsince we are focused on natural-language-to-code interactions. If\nthe code fails, they can retry the problem or move to the next\nproblem. For retry attempts, we pre-fill the text box with their last\nprompt to make editing easier. Finally, after every final attempt\nCHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA Nguyen et al.\n213456\nDictionariesSortingNestedMathListsLoopsStringsConditionals\nUntimed\nTimedTutorial\nUntimed\nTimedTutorial\nAssentTutorial8 ProblemsPost Survey Part #1Semi Structured InterviewDebriefPost Survey Part #2\n1\n2\n3 120 students\n20 students per problem\n8 Categories\n8 problems per student\n4 Timed4 Untimed...\n...\n...\nFigure 2: Study overview. (1) describes the overall student trajectory through the study. We split the post survey into two\nsections, divided by the semi-structured interview, to delay collecting demographic information to prevent self-bias. (2) outlines\nthe 8 problem categories (4 timed versus 4 untimed) and the 6 problems per category. Students took individual trajectories\nthrough one problem in each category, as shown by the thin arrows. (3) showcases an example trajectory for students through\nthe problems. Students spent, on average, 42.6 minutes (SD=10.6) completing the study, with an average of 26.6 minutes (SD=9.1)\non the untimed section and 15.9 minutes (SD=3.3) on the timed section.\nat a problem, Charlie presents two forced-choice questions with\nthumbs-up / thumbs-down answers: Did Charlie generate correct\ncode? and Would you have written this code yourself? . We included\nthese questions to gather information about student perceptions\nof code style, since the model may produce working code, but in a\nstyle that is unfamiliar to students.\nEach student worked with Codex to solve 3 tutorial problems\nand 8 main problems. We used the Charlie character to provide dis-\ntance from any AI system that students might already know. This\nsuggested a representation that was not human and not robotic.\nCharlie also provides visual feedback: Charlie animates a ‚Äúthink-\ning‚Äù position while Codex generates a completion and appears in\ndifferent forms when the code does or does not pass all tests. We\nmade this design choice to mitigate frustration with waiting for the\nmodel to generate code, a source of annoyance in prior studies of\nCode LLM interactions [69].\n4.2 Model Choice\nWhen we began piloting in November 2022, the most capable Code\nLLM was the largest Codex model from OpenAI,code-davinci-002.\nAlthough code-davinci-002 was first released in 2021, on estab-\nlished Python programming benchmarks, it remains as good as\ngpt-3.5-turbo, which is the model presently used for GitHub\nCopilot‚Äôs inline completions [104], the free version of ChatGPT, and\nHow Beginning Programmers and Code LLMs (Mis)read Each Other CHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA\n(a) An example task posed to a participant. The interface displays the function name\nand several input/output examples. Participants write and submit a description in\nthe text box. During our study, 85% of students who attempted this problem wrote a\nsuccessful description after a single CS1 course.\n(b) We run expert tests automatically and highlight\nones that fail. Students are then able to either edit\ntheir description by pressing \"Try Again\" or move\non to another problem.\nFigure 3: The Charlie the Coding Cowinterface.\nseveral other commercial products. Specifically, gpt-3.5-turbo\nand code-davinci-002 score 48% and 46% respectively on the Hu-\nmanEval Python programming benchmark [11, 74], the most com-\nmonly used Python benchmark for Code LLMs. Since we started\nour study, several other LLMs have also appeared, including non-\nproprietary LLMs that are better for reproducibility (¬ß9.5). The best\nopen models perform comparably to code-davinci-002; for in-\nstance, CodeLlama (34B) achieves 48% on HumanEval [ 85]. This\nsuggests that the model that we use is as capable at code completion\nas newer models used in practice.\nThere are larger models that are more capable, such as GPT-4,\nwhich achieves a HumanEval score of 67% [74]. However, GPT-4 is\nsignificantly slower and higher latency than the alternatives, and\nlow latency is essential for LLM code completion to be acceptable\nto users [69]; if participants have to wait more than a few seconds\nfor the generated code, their frustration might lead them to move\non rather than re-attempting the problem.\nFor consistency, we used the same Codex model throughout\nthe study (code-davinci-002). It is important to note that Code\nLLMs perform best when their output is sampled; consequently,\nthe model may produce different programs for the same prompt.\nWe generated output using best practices for hyperparameter and\nsampler settings [13].\n4.3 Participants\nWe recruited 40 participants from each institution (n = 120). Eligible\nparticipants were at least 18 years old, had taken CS1 at their insti-\ntution between Fall 2021 and Spring 2023, and had not completed\nany subsequent CS courses. We recruited participants from March\nto July 2023 until reaching our sample size of 120. The pilot and\nmain study received IRB approval.\nCare for Participants. Our study design sought to balance ob-\ntaining accurate data with addressing potential discomforts and\npower dynamics. Potential discomforts for participants included\nfrustration regarding their inability to complete a task, which could\nreinforce negative perceptions of self or CS. In the tutorial, we\nemphasized that our goal was not to evaluate their programming\nskills, but the collaboration with Charlie. Students were allowed to\nmove on from a problem at any time, resulting in a variable number\nof attempts per problem.\nWe took several steps to address potential power dynamics be-\ntween students and their professors. Recruitment was done through\nan interest form distributed by other faculty or staff. Scheduling was\nperformed by a researcher at another institution. Finally, research\nsessions were never run by a professor at the same institution as\nthe participant.\nCHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA Nguyen et al.\nSubmit\nSent via APICodecompletionreturned\nFor each problem\nFunctionsignatureTest cases PromptCodeTest cases &results\nFunctionsignature\nFunctionsignature\nFunctionsignature\nCodexPrompt\nPrompt\nCode\n+\n+Run tests onBACKEND\nFRONTEND\n++Charlie thinking\nFigure 4: An overview of the experimental platform. For\neach problem, the frontend provides the participant with\nthe signature and tests and asks them to write a description\n(prompt). This is then relayed to the backend, where the\nsignature and prompt are sent to Codex via the API. The\ncode completion from Codex is then run on our pre-defined\ntests. Finally, the results of running the tests and the code\ncompletion are presented to the participant in the frontend\ninterface.\n4.4 Study Execution\nThe study was conducted over Zoom with audio and video record-\ning. Participants signed informed consent material ahead of the\nexperiment and assented at its start. They were compensated with\na $50 gift card for the estimated 75-minute study.\nMain Task. Figure 2 (1) outlines the full study design. Students\ncompleted 3 tutorial problems to get familiar with the interface\nand see some possible Codex responses. We supplied participants\nwith a working prompt for the first tutorial problem, then gave\nthem a difficult problem so they could see a failure, and a final easy\nproblem to solve independently.\nThe main experiment consisted of 8 problems in two blocks,\nthe first untimed, the second timed. In the second block, students\nwere limited to 5 minutes per problem. We included both timed\nand untimed blocks in order to balance the need to bound study\nduration with the desire to observe complete prompt editing cycles.\nParticipants were randomly assigned experimental lists, balanced\nby difficulty, using a modified Latin Square design. Four authors\nindependently assessed the difficulty of writing prompts for each\nproblem; we averaged these scores and developed six roughly equal\nlists (Figure 2).\nPost-task Interview and Survey. After the main study, students\ncompleted a two-part survey, a semi-structured interview, and an\noptional debriefing session (Figure 2 (1)). The semi-structured in-\nterview was interleaved between two survey blocks to mitigate\nquestion ordering and priming biases.\nThe first part of the survey was designed to study student per-\nceptions of Charlie and of AI more broadly. We adapted validated\nscales from previous work to understand student perceptions of the\nusability, trustworthiness, and friendliness of Charlie [7, 20, 51, 99]\nand the mental workload of the task [36].2 We were also interested\nin whether students‚Äô ability to come up with effective prompting\nstrategies might correlate with fixed versus growth mindsets about\ncomputing; we drew on Gorson and O‚ÄôRourke [33] to measure this.\nThe semi-structured interview asked 8 questions covering stu-\ndent editing processes, what they found hard or easy, how they\nenvisioned their interactions with Charlie, and how they imagined\nCharlie worked. The specific questions were directly inspired by our\noverarching research questions. Researchers followed a standing\nscript to ask each question - there are a total of 5 missing question\nresponses across the possible 960 interview datapoints, likely due to\nresearcher error or time considerations. In the optional debriefing,\nwe explained the experiment and how Code LLMs work.\nThe second part of the survey focused on participants‚Äô back-\ngrounds and demographics. These were the last questions of the\nstudy to mitigate possible stereotype threat [ 72]. For questions\nrelated to identity (e.g., gender, race, spoken language at home),\nwe followed best practices and solicited responses via open text\nboxes [92]. We also asked questions about students‚Äô CS1 perfor-\nmance, experience with programming outside of CS1, high school\n& educational background, math background, major, and class year.\nPilot Study. In late 2022, we ran a pilot study with 19 participants\nto assess the study design and usability of the interface. Pilot par-\nticipants were recruited from the same three institutions as in our\nmain study, but were students who had taken more than one CS\ncourse. This small pilot allowed us to make sure the web platform\nwas working correctly, identify any problems with specific tasks,\nrefine our time estimates, and assess the quality of the automatic\ntranscriptions of the interview recordings produced by otter.ai.3\nDuring the pilot, we identified one problem with ambiguous test\ncases, which we changed before the main study. Pilot participants\nsolved an average of 5.5 out of 8 problems (an Eventual Success\nRate of 68.8% using the metric described in ¬ß5.2).\nBecause the average pilot participant took 53 minutes, we in-\ncreased the time estimate and compensation from $30 for 60 minutes\nto $50 for 75 minutes for the main study. We also added a hidden\ntime limit to the first block of questions in case participants spent\nmore than 50 minutes on this portion of the study; this issue never\narose in the main study.\n5 ANALYSIS\nThis section presents the analysis framework for ¬ß6, ¬ß7, and ¬ß8. We\ntake a mixed-methods approach to this work.\n2In some cases, we removed questions that were not relevant to our study to keep\nthe survey length manageable for participants. Details available via our Supplemental\nMaterials at https://doi.org/10.17605/OSF.IO/V2C4T.\n3https://web.archive.org/web/20231205001012/https://otter.ai/\nHow Beginning Programmers and Code LLMs (Mis)read Each Other CHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA\n5.1 Evaluation Plan\nQualitative analysis. We collected three types of data which lend\nthemselves to qualitative analysis: (1) information about student\nexperience and demographics, (2) free-response questions about\nfuture use of Charlie, and (3) semi-structured interview responses.\nWe employed both inductive and deductive open coding towards\nconsensus. Our aim was to identify common themes present in this\nspecific dataset, rather than to develop a theory. Two researchers\nwith previous qualitative experience conducted the analysis; Sec-\ntion A.2 contains details of the coding methodology. We present\nselected quotes from the surveys and interviews throughout. Quo-\ntations have been lightly edited from the automatically generated\ntranscripts. This includes addressing grammar/punctuation, remov-\ning speech errors or filler words, and avoiding the disclosure of any\nidentifiable information. Each participant‚Äôs quote is accompanied\nby a pseudonym assigned to them during data collection.\nStatistical analysis. We perform statistical testing with a sig-\nnificance level of ùõº=0.05 in order to determine whether observed\ndifferences in response measures are statistically reliable. For com-\nparisons between two groups, we use Student‚Äôs ùë°-test. For com-\nparisons between multiple groups, we perform ANOVAs; in cases\nwhere there is no natural reference group, we use Tukey HSD tests\nto explore pairwise differences. We report Pearson‚Äôsùëü for correla-\ntions between continuous variables and Kendall‚Äôsùúè for correlations\nbetween continuous and ordinal variables. Where we are interested\nin multiple potentially interacting variables, we fit linear mixed-\neffects models with maximal random effects for participants and\nproblems using the lme4 package in R [8].\n5.2 Measures of Success\nThere are several ways to measure success when evaluating the\nnatural-language-to-code task. The success rate is the fraction of all\nattempts on which the model generates a working program. There-\nfore, a participant who takes several attempts to solve a problem\nwill have a lower success rate than another who succeeds in one\ntry. We might also ask whether a participant is ever able to solve\na problem; we refer to this as the eventual success rate . This met-\nric considers only the participant‚Äôs final attempt at each assigned\nproblem. The eventual success rate metric is likely specific to this\npaper, as closely related work [19, 48, 78] studies different notions\nof success or does not permit controlled, repeated interactions.\nAlthough success rates measure the correctness of the code\nthat students saw during the experiment, LLM generation is non-\ndeterministic.4 Therefore, studying success rates can be misleading:\na participant may have just been lucky with a bad prompt or unlucky\nwith a good prompt. For this reason, we also employ an alterna-\ntive metric called pass@1, which accounts for non-deterministic\ngeneration [13]. Since the debut of Codex, pass@1 has become the\nstandard metric used to evaluate LLMs on the natural-language-\nto-code task, including GPT-4 [ 74], Code Llama [ 85], and other\nmodels [29, 59, 73].\nGiven a natural language prompt, pass@1 [13] is an estimate\nof the probability that the LLM will generate working code in\n4Greedy generation is significantly worse for coding tasks than non-deterministic\ngeneration [13].\none attempt. In the LLM development literature, the accepted best\npractice for computing pass@1 is to query the LLM 200 times for\nthe same prompt and test every generated program [13, 29, 85, 102].\nSampling 200 generations for all 2,000+ prompts generated as part\nof this study would be very expensive with the Codex API. Instead,\nwe use a recently released open Code LLM called StarCoder [59]\nthat is nearly as capable as the Codex model on Python benchmarks.\nPass@1 with StarCoder will be slightly lower than Codex success\nrates because of model differences. However, pass@1 is a more\nstable measure of whether a prompt will succeed than success rate.\nWe use pass@1 for the bulk of our analyses.\n5.3 Positionality\nAll authors were affiliated with the institutions from which par-\nticipants were recruited (Oberlin, Wellesley, or Northeastern) at\nthe time of the study; we range from undergraduate students to\ntenured faculty. We developed the problem lists, problem difficulty\nratings, and other elements of the study design within a shared\neducational context. The last three authors are course instructors\nfor CS1. As described in ¬ß4.3, significant care was taken to ad-\ndress power dynamics between participants and researchers. Some\nauthors also contribute to the development and evaluation of open-\nsource Code LLMs. Overall, the potential incentives for the research\nteam are complex, as we approach this work as both educators and\nresearchers. We aspire to a neutral perspective on Code LLMs, while\nattempting to center the student experience.\nThis research studies students at three selective higher education\ninstitutions in the United States. Therefore, while we are able to\ngeneralize beyond a single CS curriculum, the educational context\nis specific: our findings may not generalize to other settings (e.g.,\ncommunity colleges, K-12 education) or cultural contexts.\n6 RQ1: DO STUDENTS SUCCEED AT\nPROMPTING CODE LLMS WITH NATURAL\nLANGUAGE?\nIn this section, we present how well students do on our Code LLM\nprompting task and address RQ1: do students succeed at prompting\nCode LLMs with natural language? We explore differences between\nstudents that are linked to their ability to successfully describe\nproblems to Code LLMs.\n6.1 Basic Findings\nFigure 5 presents the distribution of participants‚Äô success rates and\neventual success rates. The average participant solved 4.7 out of 8\nassigned problems. The mean eventual success rate (57%) is not high,\nand the mean success rate (24%) is even lower, since it decreases with\nevery failed attempt. We find no significant institutional difference\nfor either measure of success.\nParticipants often submitted a large number of failing attempts\n(Figure 5d): 153 problems (aggregated across participants) required\nthree or more attempts. In fact, one participant succeeded at a\nproblem only after 32 attempts; another gave up after 26 attempts.\nThese results suggest that low success rates are not due to a lack of\nparticipant effort. Participants struggled to write natural language\nprompts for the LLM, and often achieved success only after many\nCHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA Nguyen et al.\nInstitution Mean pass@1 Success Rate Eventual Success Rate\nOberlin 0.23 26% 61%\nWellesley 0.23 25% 57%\nNortheastern 0.20 23% 54%\nOverall 0.22 24% 57%\n(a) Mean values of different measures of success.\n0.0 0.2 0.4 0.6 0.8 1.0\nSuccess Rate\n0\n2\n4\n6\n8\n10\n12Participant-Problem Pairs\nNortheastern\nWellesley\nOberlin\n(b) Success rates.\n0.0 0.2 0.4 0.6 0.8 1.0\nEventual Success Rate\n0\n2\n4\n6\n8\n10\n12Participant-Problem Pairs\nNortheastern\nWellesley\nOberlin (c) Eventual success rates.\n0 5 10 15 20 25 30\nNumber of Attempts\n0\n100\n200\n300\n400\n500Participant-Problem Pairs\nEventual Success\nEventual Give Up (d) Attempts.\nFigure 5: Basic measures of student success at the natural-language-to-code task. Success rateis the fraction of all attempts\nby a participant that succeed. Eventual success rateis the fraction of last attempts at a problem by a participant that succeed.\nPass@1 resamples the LLM several times to estimate the probability of success. We present these measures by institution.\nFigure 5a presents the means. Figure 5b and Figure 5c show the distribution of (eventual) success rates. Eventual success rates\nare higher than success rates, which is to be expected: Figure 5d shows that many students make several attempts at a problem\nbefore an eventual success or give up.\nSelf-Reported Background N Mean pass@1\nInternational 92 0.23\nDomestic 27 0.22\nFirst-generation college student 23 0.17\nNot first-generation 96 0.23\nAttended private high school 38 0.22\nAttended public high school 76 0.22\nRaised Monolingual in English 49 0.22\nRaised Monolingual Not in English 27 0.20\nRaised Multilingual Including English 41 0.24\nRaised Multilingual Not in English 2 0.22\nTable 1: Self-reported high school, language, and family back-\nground.\nrepeated attempts. The challenging nature of this task is supported\nby comments from the students themselves (¬ß7.1).\n6.2 Do Participants Find the Task Challenging?\nIn the post-survey, participants completed four items of the NASA\nTLX [36]. Overall, students found the task mentally demanding\n(Table 2). The questions about mental demand (Q1), time pressure\n(Q3), and their own performance (Q4) correlate inversely with\nsuccess rate. Students whose success rates were lower generally\nrated the task as more demanding (Kendall‚Äôs ùúè=-0.16; ùëù=0.02); were\nless likely to say they were successful (Kendall‚Äôs ùúè=-0.4; ùëù<0.0001);\nand reported higher levels of stress and insecurity (Kendall‚Äôs ùúè=-\n0.27; ùëù<0.0001).\n6.3 Who Succeeds at the Task?\nUsing data from the post-survey, we analyze the relationship be-\ntween pass@1 rates and previous knowledge, prior programming\nexperience, and demographics (see Table 1 for a summary of de-\nmographics). We find only two statistically reliable differences (see\nAppendix, Table 11 for the full statistical analyses):\n‚Ä¢Prior programming experience: About 1/3 of participants\nhad no programming experience outside of CS1. The remain-\ning participants had taken pre-college programming courses\n(24%), were in the next CS course (21%), or had coding expe-\nrience outside of classes (29%). There is a statistically reliable\ndifference (t-test; ùëù = 0.02) in pass@1 for students who have\nonly coded in CS1 (0.17) versus those with additional experi-\nence (0.24).\n‚Ä¢First-generation college students: 19.1% of participants\nidentified as first-generation college students. We observe a\nstatistically reliable difference in pass@1 for first-generation\nparticipants, who struggle more with the task than others\n(t-test; ùëù=0.04).\nWe examined other factors, but found no significant difference\nin pass@1 rates:\n‚Ä¢Math courses: All but one participant had taken at least one\ncollege math course and half had taken 2+ courses. Single\nvariable calculus was the most common math course. There\nis no statistically reliable difference between participants\nwho had or had not taken 2+ math courses (t-test, ùëù=0.42).\nHow Beginning Programmers and Code LLMs (Mis)read Each Other CHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA\nAbbreviated Question Scale (1 to 7) Mean\nHow mentally demanding was the task? Very low->Very high 4\nHow hurried or rushed was the pace of the task? Very low->Very high 3.3\nHow successful were you? Perfect->Failure 3.6\nHow insecure, stressed, or discouraged were you? Very low->Very high 3.1\nTable 2: Mean NASA-TLX ratings [36].\nThematic Codes N\nCharlie Doesn‚Äôt Understand Me 91\nIssues With Generated Code 59\nStudent Struggles 41\nNo Problems Mentioned 10\nIssues with Study Platform 10\nIssues With Experimental Design 7\nEasier To Write Code Myself 7\nTable 3: Thematic codes emerging from responses to What\nkinds of problems or issues did you run into working with\nCharlie?\n‚Ä¢Computing intensive majors: 42% of participants were\npursuing computationally intensive majors. We observe iden-\ntical pass rates for both computing and non-computing ma-\njors.\n‚Ä¢International students: International and U.S. domestic\nstudents had similar pass@1 rates.\n‚Ä¢Household language: Our participants reported growing\nup in households where a diverse set of languages were\nspoken: only English (40.8%), English and other languages\n(34.2%), and without English (24.2%). We were surprised to\nfind that pass@1 did not reliably vary by childhood language.\nHowever, all participants were from selective U.S. institu-\ntions that require fluency in English, regardless of childhood\nlanguage exposure.\n‚Ä¢Public vs private high schools: 1/3 of participants attended\nprivate schools; this had no impact on pass rates.\n7 RQ2: WHERE DO STUDENT DIFFICULTIES\nCOME FROM?\nHaving shown that students find it hard to prompt a Code LLM in\nnatural language (¬ß6), we explore why. In this section, we present\nquantitative and qualitative results that address RQ2: when students\nstruggle with the task, where do the struggles come from? What\nare the most challenging aspects of the natural-language to-code\ntask?\n7.1 What aspects of the task do students say are\nhard?\nIn the semi-structured interview, we asked participants to reflect\non challenges and issues they encountered. Three common themes\nemerged: difficulties in getting Charlie to understand them; issues\nwith the generated code; and issues stemming from students‚Äô self-\nreported lack of knowledge or skill (Table 3).\nCharlie Doesn‚Äôt Understand Me. The most commonly raised is-\nsues related to Charlie‚Äôs understanding of prompts (n=91); we di-\nvided these into subcodes. One of the most common of these was\nthe sentiment that Charlie failed to understand good descriptions\n(n=23). For instance, redCoyote commented, ‚ÄúIt was definitely dif-\nficult to have a concept of what you wanted written in your head,\nand then feel like you‚Äôre articulating it well, but having it not work\nproperly. ‚ÄùSimilarly, aquaLadybug reports feeling helpless when a\ngood prompt didn‚Äôt succeed: ‚Äúif I was saying it [...] how I thought\n[...] is the best way to say it, but it still wasn‚Äôt working, I had no idea\nwhere to go from there. ‚Äù\nIssues with Generated Code. Another major theme was issues\nwith the generated code. Many commments related to perceived\nbugs in the generated code or difficulty debugging (26%). Students\nalso mentioned finding the model‚Äôs randomness frustrating (8%).\nkhakiBee was alarmed to find that resubmitting the same prompt\ncould generate different programs, commenting‚ÄúYou feel like you‚Äôve\nmade progress, and then because it did a different thing the next time,\nit‚Äôs like, what do I change? I‚Äôm trying to change what I give to the\ncow. And then that should change what the cow is doing. But if I‚Äôm\nnot changing anything, why is that changing?‚Äù Some students also\nexperienced the opposite issue: despite changing their descriptions,\nthe model generated the same incorrect function repeatedly. pur-\npleCarp commented, ‚ÄúSometimes I changed my [...] description and\nit just repeated the code the same. And it‚Äôs just very frustrating‚Äù . This\nhighlights the difficulty of working with stochastic models: students\nexpect the model output to be faithful to their descriptions.\nStudent Struggles. Participants also reported issues stemming\nfrom their own lack of knowledge. 10% of students reported diffi-\nculty understanding a problem, and 8% reported difficulty in un-\nderstanding generated code. yellowChipmunk said, ‚Äúsometimes\nwith the code, just given my knowledge, that‚Äôs not necessarily the\nway I would go about coding the code. But I think to even understand\nit, I would have to know what the code is trying to do, which takes\nmore time than me just trying to reword what I said‚Äù . A handful (n=4)\nreported that forgetting terminology made it hard to write prompts.\n7.2 Which Problems Do Students Say Are Hard?\nSome categories of CS1 problems may be harder to solve with Code\nLLMs, either because the concepts are difficult or because they are\ndifficult to describe. We examine pass@1 and eventual success rate\nby category as well as interview responses about which problems\nwere challenging and easy.\nWe find that pass@1 and eventual success rates both vary by\ncategory (Table 4). We fit a binomial mixed-effects model to prompt\nsuccess (1 if the prompt succeeded; 0 otherwise), with fixed effects\nof category, institution, and their interaction, and random effects\nCHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA Nguyen et al.\nCategory Mean pass@1 Mean Eventual Success Rate Student Difficulty Ranking\nSorting* 0.09 33% 1 (Hardest)\nDictionaries* 0.17 43% 2\nNested* 0.30 68% 6\nMath* 0.16 54% 4\nLoops 0.13 52% 3\nLists 0.18 61% 5\nConditionals 0.33 73% 7\nStrings 0.26 74% 8 (Easiest)\nTable 4: Pass@1 and success rates by problem category. Each category has six problems, and an equal number of students\nattempted each problem. The starred (*) problems were timed. Student Difficulty Ranking is done by ordering mean Eventual\nSuccess Rate from least to greatest, as that provides as measure of what percentage of students successfully solved a given task.\nof problem and participant (see Appendix, Table 12). A statistically\nreliable difference in success was observed only for Sorting prob-\nlems, which were the most challenging (ùëù=0.045). Participants from\nOberlin struggled more in the Nested category compared to other\nstudents, but the effect is not statistically reliable (ùëù=0.063).\nInterviews provide insight into their post-task perspectives. The\nmost commonly mentioned easiest category was Math (n=21), whereas\nthe most common for hardest was Nested (n=19), followed by Dic-\ntionaries (n=14). These do not match the ranking in Table 4, sug-\ngesting a disconnect between student performance and perceptions\nof difficulty.\nA common theme that emerged related to the challenge of putting\nunderstanding of the problem into English (n=44). crimsonVole\nsaid, ‚Äúthe ones that had huge lists of like, strings, and integers, were\nreally hard to solve, because they were really hard to describe for\nme. ‚ÄùWe differentiated this code both from students‚Äô ability to iden-\ntify patterns (n=35) and their ability to write the code without\nCharlie (n=8). The opposite code, Easy to Describe, applied to 36\nresponses from the easiest question: ‚ÄúI felt like time ones because\nthey‚Äôre pretty straightforward. They‚Äôre like [...] exercises that we do in\nmy Intro CS class. And so I guess it will be easier for me to word, the\ndescription or my thinking process, like I guess that might be easier. ‚Äù\n(yelllowWeasel).\nThree codes that related to student‚Äôs lack of knowledge emerged,\nwith 27 responses (see ¬ß7.4 for more perspectives).\n7.3 What Role Does the Model Play?\nLLMs can fail in surprising ways. We now explore the kinds of\nmodel failures that participants encountered.\n7.3.1 Syntax errors. Contemporary Code LLMs generally produce\nsyntactically well-formed programs. However, 5.5% of student prompts\nled to Python syntax errors. We manually examined and categorized\nthem:\n‚Ä¢27 generations: Codex produces degenerate, repetitive text [43]\nor Python 2 print statements. These are model failures.\n‚Ä¢81 generations: Codex could not generate a complete func-\ntion within the 256 token limit (‚âà800 characters). Our prob-\nlems are simple enough to be solvable in far fewer tokens,\nso increasing the token limit is unlikely to help.\n‚Ä¢88 generations: Codex generates incomplete code after a\ncomplete function, even with standard stop tokens.\nThe latter two categories arise from a trade-off in system design:\nthe first when the interface does not request enough tokens from\nthe Code LLM; the second when it requests so many that the model\ngenerates extraneous additional code. Although these errors are\ninfrequent, they are hard for students to deal with. In 22.4% of these\ncases (n=44), students gave up after seeing the syntax error.\n7.3.2 When the Model Produces Different Programs From the Same\nPrompt. Codex is best at coding when its output is sampled (¬ß4.2),\nbut this stochasticity can frustrate students trying to modify prompts.\nIn 107 cases (4.2%), a student submitted a prompt several times, and\nin most of these cases, Codex generates a new completion. A few\nof these are trivially different (e.g., different variable names), but\nmost (n=86) are different functions. Some students pointed this out\nin the interview ‚Äì beigeHalibut noted that they ‚Äúusually would\nrun a couple times, because Charlie is not very consistent with the\nanswers. And sometimes it works. Sometimes it wouldn‚Äôt work. ‚Äù\n7.3.3 When the Model Produces the Same Program Despite Changes\nto the Prompt. When the Code LLM produces an incorrect function,\nand a user edits their prompt, their intent is to have the LLM produce\na different‚Äîhopefully correct‚Äîfunction. Frustratingly, this does not\nnecessarily happen: sometimes the model repeatedly generates the\nsame code despite edits to the prompt. We observe many instances\nwhere this happens (104 submissions, 11% of total): it occurs in\nmost problems (36 of 48 problems) and is encountered by a majority\nof students (72 of 120 students). This often leads students to give\nup. In fact, out of the 340 problems where students gave up, 70\nwere cases where the participant edited the prompt and the LLM\nrepeatedly generated the same code.\n7.4 What Do Students Do When They\nEncounter Unfamiliar Python?\nCode LLMs are trained on online repositories of code and may\ngenerate code using language features that students have not seen\nbefore.\nNew Python Constructs. In their interviews, some students (n=5)\nreport issues understanding code due to unfamiliar language fea-\ntures. oliveBear comments about the lambda construct for anony-\nmous functions: ‚ÄúI‚Äôve only ever seen [it] in passing. And so if that\nhadn‚Äôt worked, I wouldn‚Äôt have known what the problem was because\nI myself don‚Äôt know how to use that operator. ‚Äù Others mentioned map,\nHow Beginning Programmers and Code LLMs (Mis)read Each Other CHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA\nCompletion\ndef exp(lst, val):\nreturn [i ** val for i in lst]\nQuestion Is this code you would write yourself?\nStudent Responses Wellesley: Yes, Oberlin: No\nFigure 6: An example code completion for the problem exp ‚Äì\nthis was generated by multiple different prompts. The com-\npletion was rated differently by Oberlin and Wellesley stu-\ndents, likely due to the list comprehension.\nreplace, and try/except. List comprehensions are an interesting\ncase because Wellesley teaches them, but Oberlin does not. When\nasked about generated code with list comprehensions, 9/24 (37.5%)\nOberlin students indicated that it is similar to code they would write\nthemselves, compared to 20/33 (60.6%) Wellesley students. Some\nstudents responded differently to the same completion (Figure 6).\nRatings of Final Completions. Students evaluated the correctness\nand naturalness of the final completion for each problem, producing\n960 responses. For correctness, 61.8% of the time students indicated\nthat Charlie‚Äôs code was correct; the majority (543; 91%) are cases\nwhere all tests passed. However, naturalness responses were more\nmixed. Students indicated that Charlie‚Äôs code was like code they\nwould write themselves only 58.3% of the time. 78.6% of such re-\nsponses were made when the code passed all tests. Responses to\nthese questions might diverge when the model generates correct\ncode that is unfamiliar or approaches a problem differently, as well\nas in cases where the model‚Äôs code is incorrect, but looks familiar\nto students.\n8 RQ3: STUDENTS‚Äô MENTAL MODELS AND\nPROCESSES\nThis section addresses RQ3, presenting results related to partici-\npants‚Äô perceptions of the task, their mental models of Charlie, and\ntheir strategies for writing prompts.\n8.1 How does Charlie work, according to\nstudents?\nIn interviews, students were asked how they thought Charlie worked\n(Table 5). Comments fell into two broad themes: descriptions of\nCharlie‚Äôs knowledge, and descriptions of Charlie‚Äôs processes.\nProcesses. Comments in the Translation theme (n=13) described\nCharlie in terms of a machine translation process (fuchsiaBeaver:\n‚ÄúI thought of him as like a translator, like between English and code ‚Äù).\nComments in the Sequential theme (n=13) described Charlie as\nworking line-by-line through their prompt. This is plausible but\nincorrect: Code LLMs condition on the entire prompt at once. This\nmental model might lead students to focus on individual sentences,\nrather than how their prompt works as a holistic description. One\nstudent actually changed their mental model while answering: ‚Äúit\nlooks like he went line by line. Wrote some code for each line that\nmakes sense to him [...] Actually, no, I think he takes in the whole\nprompt and [...] figures out what to do with the prompt. Because I\ndo remember [...] there were a couple where I give a paragraph and\nThematic Codes N\nKnowledge: Keywords - General 30\nKnowledge: Keywords - Database/Dictionary 16\nKnowledge: ChatGPT 17\nKnowledge: Internet Data 12\nKnowledge: Intermediate Representation 4\nKnowledge: Copilot/Codex 2\nProcess: Sequential 13\nProcess: Translation 13\nNo Guess 13\nN/A 24\nTable 5: Thematic codes emerging from responses to How did\nyou imagine that Charlie was working?\nthen he returned a line of code, which makes me think that he wasn‚Äôt\ngoing line by line. ‚Äù (khakiClam).\nCharlie‚Äôs Knowledge. Most students hypothesized that Charlie\nrelies on keywords (n=46). A large group of students (n=30) had\na vague keyword mental model. For instance, ‚ÄúI guess he probably\nlooks for keywords, ‚Äúif‚Äù and ‚Äúelse‚Äù and key coding words, Python words,\nand he probably has a knowledge of English‚Äù (wheatOtter). Another\ngroup (n=16) outline a more specific keyword lookup model, where\nCharlie uses keywords to retrieve relevant code from a dictionary\nor database. For instance, linenBobcat described Charlie as ‚Äúusing\nthe code words, and doing it sort of line by line and trying to work\nfrom what was given and writing those words with what, like in a\ndirectory or some sort of data file, understanding which ones matched\nup to which functions and which commands. ‚Äù\nStudents with this mental model emphasize the importance of\nusing programming terminology, since they think Charlie may not\nbe able to retrieve code without the right keywords. Some students\ndevelop this mental model after observing that their prompts suc-\nceed when they use coding words:‚ÄúI noticed that if I put in more like,\ncomputerized words, I almost had a bit more control. At one point, I\nforgot to mention that the function returns something. So then when I\nmentioned that it returned something he put in a return statement. So\nthat felt like very, like logical to me. [...] Charlie‚Äôs looking for words\nthat kind of line up with different functions, built in functions, and\nusing those. ‚Äù (tanMinnow). These students correctly observe that\nsounding like a programmer is important, but explain this with an\nincorrect mental model.\nSome students did correctly identify Charlie as similar to an LLM\nsuch as ChatGPT (n=17) or Copilot/Codex (n=2). Success rates for\nthis group were slightly higher (0.27 versus 0.22; ùëù=0.03).\n8.2 What strategies do students develop?\nThe first two semi-structured interview questions asked students\nabout their strategies for writing and editing prompts. We find that\nstudents do not have a clear understanding of how models work and\nthat their incorrect mental models appear to affect the strategies\nthey develop for prompting in ways that might be unproductive.\n8.2.1 Editing processes. Over a third of students (n=48) mentioned\nadding detail to their descriptions when they did not succeed (Ta-\nble 6). Some students mentioned clarity as a goal in adding detail,\nCHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA Nguyen et al.\n0\n10\n20\n30\nAdditions\n0 10 20 30 40 50\n# of changed words\n0\n10\n20\n30\nDeletions\n# of problem attempts\nChanges between the first and last prompt\n0\n10\n20\n30\n40\n50 Additions\n0 10 20 30 40 50\n# of changed words\n0\n10\n20\n30\n40\n50\nDeletions\n# of problem attempts\nChanges between the second-to-last and last prompt\nFigure 7: Histograms of the 282 prompts which lead to successes after 2 or more attempts. These represent trends in how\nstudents edit prompts. The figure on the (left) shows the number of words changed between a first prompt and last prompt.\nThe figure on the (right) shows the final change that produces a successful final prompt.\nThematic Codes N\nAdded Detail 48\nLooked at Tests First 30\nLooked at Code First 29\nAdded Coding Language 21\nComment Not Relevant 16\nLooked at Code and Tests Together 8\nReread the Problem 7\nReordered Prompt 5\nRemoved Detail 4\nRan Prompt Again 3\nFixed Grammar 2\nTable 6: Thematic codes emerging from responses to What\ndid you do when you wrote a description, pressed Submit, and\nit did not work? Describe the steps you took to edit your de-\nscription.\nlike fuschiaBat: ‚ÄúI will go back and try to change the wording to\nmake it more clear, and then try it again. And see if that changes any-\nthing. And then just try to repeat that process until it works. ‚Äù Others\nnoted that their descriptions needed additional detail because they\ndid not originally fully describe the problem, or asplumBeetle puts\nit, ‚ÄúI forgot to uppercase Aspen. And that was just my silly mistake.\nAnd I will just go back and edit or add changes that I want to add and\nwish it‚Äôs gonna work the next time I guess. ‚Äù Considering participants‚Äô\nedits quantitatively confirms the popularity of adding detail. When\nwe consider pairs of prompts that ultimately succeed, we find that\nstudents, on average, add 9.44 words (SD = 11.34) between their\nfirst and last prompt, and 5.36 words (SD = 8.87) between their\npenultimate and last prompt (Figure 7).\nWhile adding details was the most common approach, partici-\npants mentioned other strategies, such as reordering (n=5) or re-\nmoving detail (n=4). There are also eight attempts where rerunning\nthe same prompt resulted in a success; we discuss these cases in\n¬ß7.3.\nStudents looked in different places for insight into how to edit\ntheir prompts. Some considered the generated code first (n=29),\nsome the tests (n=30). Others considered both (n=8) or reread the\nproblem (n=7).\n8.2.2 Strategy changes over time. Participants had a range of re-\nsponses about how their prompting processes changed over time.\nSome students indicated that they never really developed a process\n(n=13), while others (n=14) discussed actively testing and adapting\nto Charlie‚Äôs capabilities: ‚ÄúI first [...] was kind of seeing what vocabu-\nlary Charlie knew. Like if he knew computer science terms, or if I had\nto be less computer science-y‚Äù (beigeBass).\nWe present key trajectories in Figure 8. Overall, we observe a\nrange of reported experiences. Some participants reported start-\ning more human-like and ending more technical (Pythonic), while\nothers said the opposite. For instance, tomatoBeetle reported, ‚ÄúTo\nbegin with, I was using less technical terms and then using more\ncomputer science terms near the end. I was thinking that would make\nCharlie work better, but there wasn‚Äôt really any evidence behind that‚Äù ,\nHow Beginning Programmers and Code LLMs (Mis)read Each Other CHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA\n8\n2\n15\n11\n15\n4\n4\nPythonic\nHumanlike\nDetailed\nConcise\nEasy\nHard\nPythonic\nHumanlike\nDetailed\nConcise\nEasy\nHard\nStart                                                                                           End\nFigure 8: Visualization of how students describe their editing\ntrajectories. The left nodes represent how students described\nhow they began their process. The right nodes represent how\nstudents described how they edited prompts at the end of\nthe study. The codes are presented in pairs - Hard versus\nEasy, Concise versus Detailed, Humanlike versus Pythonic.\nOnly trajectories between pairs are visualized. The size of the\nnodes is proportional to the total number of students who\ndescribed their Start or End within that code.\nwhile grayRabbit said, ‚ÄúI kind of treated it like I was just coding but\nsaying things I would like use kind of like if statements and integers\nand stuff. But towards the end, I tried to focus more on how I could\nsay what was going on at a higher level, so using more plain language\nversus specific coding language . ‚Äù\nA large group reported that their prompts became more detailed\n(n=35) and/or more technical (n=31), mirroring the finding above\nthat students typically add detail when editing. For instance,tanBat\nreports, ‚ÄúMy initial process was just to figure out what the code is\ndoing and then just write generic descriptions, like without any coding\nlanguage inside of it. But then when I saw that Charlie kept having\nproblems, I started to go to more coding language. ‚Äù However, others\ntook the opposite approach, and ended the study writing more\nhuman-like (n=11) or concise (n=16) descriptions.\n8.3 Do Students Get Better at Prompting Over\nTime?\nIt is easy to argue that programming by prompting a Code LLM\nwith prose is more natural than directly writing code and that\nCode LLM prompting is easy to learn. But how easy is easy? We\ninvestigate whether students improve at prompt writing over the\ncourse of the study. We explore this by comparing success rates for\n(1) students who attempted the problem first with (2) students who\nattempted the problem last. Our experiment design ensures that\nthere are exactly 5 students who attempt each problem first and\nfive more who attempt it last. We find no significant difference in\nsuccess rates between the two groups, indicating that students do\nnot observably improve at prompting within the 75 minute study.\n8.4 What do students think about Charlie?\nOne of the most consistent findings in work on how experts use\nCode LLMs is that users enjoy using models [69, 105], even when no\nconcrete productivity or correctness benefits are observed [97, 102].\nHowever, near-novices exhibit different motivations and relation-\nships to technology than expert programmers. This makes it im-\nportant to investigate how non-experts feel about these systems.\n8.4.1 Charlie‚Äôs competence and reliability. The post-task survey\nasks participants several sets of questions related to their percep-\ntions of Charlie. They completed 5 items from Bartneck et al. [7]\nadapted by Wang et al. [99] and Druga and Ko[20] for non-robotics\nuse. Participants generally give Charlie middling ratings for knowl-\nedge and competence. Participants take more extreme positions on\nCharlie‚Äôs persona, in opposite directions: they rate Charlie as both\nfriendly and machinelike. Students who experience lower success\nrates find Charlie somewhat less competent, but do not seem to\nfind Charlie less friendly (Table 7). Students also completed 5 items\nfrom K√∂rber [51]‚Äôs trust of automation survey. Overall, students see\nCharlie as somewhat reliable and somewhat interpretable (Table 8).\nStudents with higher success rates tended to rate Charlie as less\nerror prone, easier to understand, and more reliable.\n8.4.2 Would they use Charlie? The post-survey asked about stu-\ndents‚Äô attitudes toward hypothetically using Charlie in (a) the CS1\ncourse they completed and (b) their own future programming prac-\ntice. We used a thematic analysis approach to analyze this data, as\nwith the interview data (see Appendix A.2 for more details).\nOverall, two-thirds (n=83) stated that they would be interested\nin using Charlie in CS1. Many responses were variants of ‚ÄúYes‚Äù,\nbut students who responded Maybe (n=13) or No (n=23) typically\nexplained their reasoning. Half (n=19) of these suggested that tools\nlike Charlie would inhibit student learning. For instance,aquaLady-\nbug noted, ‚ÄòIf I had questions on how to program a particular thing,\nusing something like Charlie could help me clarify any questions I\nhad by testing out different descriptions. But if I completely relied on\nsomething like Charlie as a tool in such a class, I feel like the whole\npoint of me taking the class is overlooked and at some point becomes\nredundant. ‚Äù Other students, including those who responded Yes,\nbrought up how programmer skill level could play a role. tealHer-\nring wrote, ‚ÄúYes, but I would want to maybe only try it out towards\nthe end of the course, when I‚Äôve already learned the process of cod-\ning and would like to see how an AI could work to streamline the\nprocess. ‚ÄùOther comments touched on academic integrity (‚ÄúI don‚Äôt\nthink so unless my teacher explicitly endorsed it because I‚Äôm terrified\nof plagiarism!‚Äù - crimsonWorm).\nMore students supported using tools like Charlie in their own\nfuture programming practice (n=95). Maybe (n=20) and No (n=4)\nrespondents again provided more explanation: two common themes\nincluded Charlie‚Äôs limitations and usefulness for different kinds\nCHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA Nguyen et al.\nScale Mean Correlation with Success Rate (ùúè)\nIgnorant - Knowledgeable 3.68 0.16*\nMachinelike - Humanlike 2.39 0.12*\nResponding rigidly - Responding elegantly 3.13 0.09\nUnfriendly - Friendly 4.2 0.008\nIncompetent - Competent 3.58 0.19*\nTable 7: Mean student responses to Charlie perception questions (1=left endpoint, 5=right endpoint), adapted from Wang et al .\n[99], and correlation with success rate. * indicates statistical significance.\nQuestion Mean Correlation with Success Rate (ùúè)\nCharlie is capable of taking over complicated tasks. 3.24 0.03\nCharlie might make sporadic errors. 2.15 0.18*\nI was able to understand why things happened. 2.24 -0.34*\nI can rely on Charlie. 2.95 -0.17*\nAutomated systems generally work well. 2.46 -0.14\nTable 8: Mean student responses to Charlie trust questions (1 = Strongly agree; 5 = Strongly disagree), adapted from K√∂rber [51],\nand correlation with success rate. * indicates statistical significance.\nof problems: ‚ÄúIf Charlie improved, then it should be able to gener-\nate simple functions for me, in which I don‚Äôt have to repeat myself‚Äù\n(purpleCarp).\n8.5 AI Attitudes\nStudents were asked whether they felt optimistic or pessimistic\nabout AI‚Äôs future impact on society. About two-thirds of students\nwere optimistic; however, students pursuing a programming ma-\njor (Computer Science, Data Science, or Media Arts and Science)\nwere notably more optimistic than other students (80% optimistic\ncompared to 63% of other majors). There was no difference in task\nperformance between optimists and pessimists (pass@1 rate = 0.22\nfor both).\nStudents were also asked to compare the ethicality of Charlie\nwith three other AI deployment scenarios. Most students found\nCharlie less ethically concerning in each comparison (Figure 9). Stu-\ndent responses to these questions did not differ reliably in relation\nto their success rate or pass rate.\n9 DISCUSSION\nIn the previous sections we discussed our three main research\nquestions ‚Äì we summarize the findings together here:\n‚Ä¢RQ1: We find that some students can effectively prompt a\nCode LLM, but it often takes numerous attempts. Students\noverall found the task mentally demanding. Prior experi-\nence and first-generation status are correlated, positively\nand negatively respectively, with success.\n‚Ä¢RQ2: The most common issues students report relate to the\nCode LLM misunderstanding their descriptions and issues\nwith generated code. Both students themselves and our anal-\nysis of the data suggest that the stochastic nature of the\nCode LLM may impact student experiences. We find limited\ndifferences between students regarding problem difficulty.\n‚Ä¢RQ3: Students‚Äô most common mental model for the Code\nLLM was a data structure with keyword lookup. The most\ncommon prompting strategy that students developed was\nto expand their prompts, making them more detailed and\nmore Pythonic. Students viewed the model as fairly capable\nand somewhat reliable. However, they expressed a range of\nopinions about whether Code LLMs would be appropriate\nfor CS1.\nIn this section we draw connections between our findings and\nrelated work and discuss their broader implications.\n9.1 The Natural-Language-to-Code Task is\nChallenging\nThe emergence of LLMs have led some to conclude that this is the\n‚Äúend of programming‚Äù [65, 100]. In contrast, we find that beginners\nwho can write code nevertheless struggle to write natural language\nprompts for LLMs . We carefully select problems that are similar\n(or identical) to those they completed to pass CS1. The average\nparticipant solves 57% of the assigned problems, but only after\nseveral repeated attempts and with automatic feedback on code\ncorrectness. Our study contributes to the existing work on beginner\ninteractions with Code LLMs by measuring how well students\ncan use Code LLMs to solve problems at their own programming\nskill level, rather than in the context of a learning activity, where\nstudents may not be expected to able to write the code themselves.\nDespite the fact that all of our participants had passed CS1, which\nrequired writing code to solve problems like those in our study,\nmany of them struggled to write natural language descriptions to\nlead a Code LLM to solve similar tasks.\nOn the whole, our findings reveal a somewhat higher level of\ndifficulty in using Code LLMs than other studies [19, 48, 78], though\nit can be challenging to compare across diverse student populations,\nstudy designs, and problem types. Our results align most closely\nwith those from Denny et al. [19]‚Äôs subsequent study of students\nwith just two weeks of programming instruction. Although their\nstudy used only 3 problems and had less experienced programmers,\nthey observed similar challenges: 86% of students eventually solved\nHow Beginning Programmers and Code LLMs (Mis)read Each Other CHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA\nScreen job applicants Write news articles Grade exams\nLess More Less More Less More\n0\n10\n20\n30\nIs Charlie more or less ethically concerning than a system that uses AI to...?\nNumber of responses\nSchool Northeastern Oberlin Wellesley\nFigure 9: Student perceptions of Charlie‚Äôs ethicality as com-\npared to other AI scenarios\ntheir easiest problem, but only 65% solved their hardest task. This\nis close to the average eventual success rate that we observe.\n9.2 Not a Panacea for Non-Expert Programming\nLearning an effective process for how to prompt a Code LLM is the\nkey to interacting successfully with it in the long term. Existing\nwork on experts reveals different ‚Äúmodes‚Äù of interaction [6]. Our\nfindings suggest that unlike experts, near-novices do not develop\nwell-defined strategies for how to prompt . Students added more detail\nto their previous prompts, even when it would have been better\nto start from scratch. In addition, students‚Äô prompting abilities did\nnot observably improve during the study (¬ß8.3). Students‚Äô failure\nto develop effective strategies may also be linked to their incorrect\nmental models of how Code LLMs work (¬ß8.1). These results suggest\nthat prompting, like most ways of interacting with code, needs to\nbe explicitly taught to be used effectively.\nKazemitabaar et al. [48] present a study of pre-college students\nthat suggests Code LLMs can improve learning outcomes. They\ncompare student performance with and without access to the Code\nLLM, and provide considerable support to participants, such as\ninstructor feedback and access to expert-written descriptions of the\nproblem. In three of their task categories, both students with and\nwithout access to a Code LLM were able to complete 100% of the\ntasks, making it difficult to understand the contribution of the Code\nLLM. In the two more challenging categories, students benefited\nfrom the Code LLM, but they also relied heavily on the expert-\nwritten description (reusing it around 40% of the time). Together\nwith our results, we take this to indicate that Code LLMs can be\nuseful to beginners, but that writing prompts remains a barrier.\nThis highlights the importance of understanding why Code LLMs\nand beginning programmers struggle to understand each other:\nKazemitabaar et al. [48] argue that Code LLMs could positively\nimpact student learning, but our results demonstrate a variety of\nways that these interactions currently fail.\nOur findings provide fine-grained evidence about student chal-\nlenges that have implications for complete novices, as well as the be-\nginners we study. The results in ¬ß8 highlight how effective prompt-\ning requires skills that complete novices do not possess. Figure 8\nvisualizes how students described their start and end approaches\nto editing, showing that many students who started out writing\nprompts as for a human transition into using more coding termi-\nnology by the end of the study. These participants picked up on\na key property of Code LLMs: they are trained on expert-written\ncode and documentation and expect natural language prompts to\nutilize coding terminology. The strategies that were most effective\nfor our beginners would not be available to true novices.\n9.3 Don‚Äôt Assume a Mental Model of AI\nOur study suggests that students have incomplete mental models\nof how Code LLMs work. Although participants knew they were\ninteracting with an AI code generation tool and the majority (n=88,\n73% in the post survey) had heard of GPT-3, Github Copilot, or\nCodex, when asked how they thought our system worked, only 19\nstudents mentioned these models. A notable feature of responses\nwas the number of detailed, but incorrect explanations. The majority\nof students who gave examples identified a keyword-based lookup\nstrategy, like the dictionaries they had learned about in CS1.\nThese mental models fail to explain one aspect of Codex that stu-\ndents find frustrating: its stochastic responses. Students are familiar\nwith errors that persist after editing their code. Code LLMs intro-\nduce a related but novel experience: submitting the same prompt\nand getting a different program (¬ß7.3). This does not occur in stan-\ndard CS1 settings and cannot be explained by the database/dic-\ntionary mental model of Code LLMs that most participants de-\nscribed. Without a well-developed understanding of why this hap-\npens, students have simply added another unknown computational\nbehavior to their coding experience.\nWe note that although Prather et al. [78] report that several of\ntheir participants described models as having sentience or agency,\nnone of our participants did. This may reflect the growing public\nawareness of generative AI between their study and ours, resulting\nin more realistic attitudes about the capabilities of large language\nmodels in our population. Our students seem to understand what\nAI models can do, but not how they do it.\n9.4 Implications for Educators\nRecent work has shown that Code LLMs can solve CS exams or\nhomework assignments given the educator‚Äôs description of the prob-\nlem [17, 25]. Our findings show that although Code LLMs can solve\nCS1 problems, CS1 students cannot necessarily prompt Code LLMs\nto solve CS1 problems. Our findings reiterate the importance of key\nskills taught in CS1: code comprehension, problem decomposition,\nand the ability to describe computational problems clearly.\nWhile we do not study learning outcomes explicitly, we find\nmixed support for Code LLMs as pedagogical tools. The survey\nportion of our experiment included questions about participants‚Äô\nattitudes towards Code LLMs. About two-thirds of participants\nexpressed interest in using similar technology in CS1. Some par-\nticipants mentioned that the task helped them remember Python\nconcepts that they had forgotten, or even learn new features (such as\nCHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA Nguyen et al.\nlist comprehensions for Oberlin students). Others felt that it helped\nthem practice describing technical tasks in natural language; Code\nLLMs could be used to provide feedback on Explain In Plain English\n(EiPE) questions [16, 64], which many educators see as valuable,\nbut difficult to use without automation [28]. Recent work on stu-\ndents‚Äô perceptions of automatically-graded EiPE questions provides\nguidelines that may serve as a first step towards using Code LLMs\nas automatic backends [44].\nOn the other hand, a sizeable number of students did not sup-\nport using Code LLMs in CS1. Some students expressed ethical\nconcerns. Many questioned whether coming to rely on Code LLMs\nwould diminish their knowledge of programming or their sense of\nfulfillment. Our survey data also highlights a key challenge of con-\ntemporary AI: explainability. Students gave Charlie higher ratings\nfor capability than interpretability. Our findings here complement\nSun et al. [93]‚Äôs exploration of Code LLM explainability needs iden-\ntified by expert programmers, and Prather et al. [78]‚Äôs finding of\nstudents‚Äô ‚Äúslow accept‚Äù mode, where students spent a lot of time\nreading code generated by Copilot and deciding whether or not to\naccept it.\nBy shedding light on how students feel about Code LLMs, our\nwork augments Lau and Guo [52]‚Äôs investigation of CS educators‚Äô\nperspectives on Code LLMs. Our studies were conducted at a similar\nmoment when Code LLMs had recently gained public prominence,\nbut few educators or students had much experience with them. Our\nstudents and the educators in Lau and Guo [52] raise strikingly\nsimilar concerns about ethics and negative impacts on student\nlearning. Denny et al. [19]‚Äôs subsequent experiment found similar\nconcerns among currently enrolled CS1 students.\nThe large scale of our study also allows us to contribute data to\nthe debate over equity in Lau and Guo [52]‚Äôs study, who show con-\nflicting perspectives among educators: some felt that Code LLMs\ncould strengthen the digital divide between students, while oth-\ners felt that Code LLMs could improve diversity in CS. On the\nwhole, our findings strengthens concerns. We show that students\nwith extracurricular programming experience have an advantage,\nechoing Kazemitabaar et al. [48]‚Äôs finding that more experienced\nprogrammers benefit more from using Code LLMs. We also show\nthat prompts written by first generation college students have re-\nliably lower pass@1 rates. Educators should weigh the potential\nbenefits of adopting this new technology against the possibility\nthat it might exacerbate existing equity issues [41].\nFinally, our students are ambivalent towards AI systems in gen-\neral. Around two-thirds were optimistic about AI‚Äôs impact on so-\nciety in the future, similar to the proportion interested in using\nCharlie in CS1. This leaves a sizeable number of beginners who are\nconcerned about AI or uninterested in its use in CS1. Our findings\ncapture a nuanced portrait of how young adults perceive generative\nAI for programming, captured at a moment where generative AI\nwas increasingly prominent in popular media.\n9.5 Model Selection for Human-AI Interaction\nResearch\nOne issue for studies such as ours is the rapid pace of research\nand development in machine learning. Running lab experiments\nwith humans takes time. However, current proprietary models are\noften updated or deprecated with very little warning. This study\nused OpenAI‚Äôs Codex, which provides state-of-the-art Code LLM\nperformance but came with significant risks. In the middle of our\nstudy, OpenAI announced that Codex would be deprecated within\na week, which would have seriously compromised our results; after\nmuch public concern, they eventually delayed the deprecation until\nearly 2024.\nThe mismatch between the timescale of ML development and\nhuman-subjects research makes it difficult to complete studies us-\ning state-of-the-art models, which are largely proprietary. Based\non our experience, we recommend not using proprietary models,\nalthough this may come with a trade-off in terms of performance,\nand imposes significant computational requirements for the re-\nsearch team (since alternatives require access to significant GPU\nresources). Nonetheless, we strongly suggest the use of open source\nmodels [59, 85] in future work, and potentially for classroom use,\nto avoid sudden loss of access. This is an example of an ongoing\nequity concern for researchers and educators.\n9.6 Timeliness\nConducting work with non-experts and Code LLMs in early 2023\ncaptures a specific moment in the evolution of this technology. Our\nparticipant pool represents students who mostly completed CS1\nbefore Code LLMs became commonplace. Collecting this data now\nis paramount to our understanding of baseline interactions with\nCode LLMs for students without previous exposure. In the future,\nthe controlled background knowledge of this study will become\nincreasingly hard to come by, both at our institutions and farther\nafield.\nWe also see our work as timely because of the struggles and\nstrategies, or lack thereof, that we identify. As computing resources\nbecome increasingly directed towards Code LLM technology [58],\nwork such as ours has the potential to impact how companies\ndevelop their models, tutorials, and interfaces. We find that non-\nexperts struggle to execute the full prompt and edit cycle, even\nwith an interface that identifies output correctness. If this trend\ngeneralizes to other non-expert groups, Code LLM technology may\nstrengthen the digital divide between expert and non-expert pro-\ngrammers, adding to the wide ranging list of ethical concerns about\ngenerative AI [9, 49, 61].\n10 THREATS TO VALIDITY\nA major challenge of studying human-AI interaction is that AI ca-\npabilities and popular awareness of them change quickly. ChatGPT\nwas released between our pilot and main experiment; as a result,\nstudents‚Äô knowledge and experience with large language models\nunderwent significant growth during our experiment. We observed\na statistically significant improvement in task performance for stu-\ndents who took the study in the last month. This may spring from\nincreased familiarity with large language models such as ChatGPT\nor from more recent exposure to CS1 material.\nAlthough we recruited participants who had completed CS1 and\nno subsequent CS courses, their programming backgrounds were\nnot homogeneous. Some participants had taken a prior program-\nming course in high school or in college, and some were concur-\nrently enrolled in a programming course. We study the effects of\nHow Beginning Programmers and Code LLMs (Mis)read Each Other CHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA\nadditional programming experience in ¬ß6.3. In addition, since we\nrecruited students who had taken CS1 as early as Fall 2021, some\nparticipants reported having forgotten programming concepts or\nterms in the intervening time.\nSeveral factors may have biased participants towards report-\ning positive perceptions of our system. While we ensured that the\nexperimenter running the study was not a educator at the partici-\npant‚Äôs institution, participants were aware that the study involved\none of their professors and may have responded more positively as\na result. In addition, students may have answered questions about\ntext-to-code more positively because of the anthropomorphic qual-\nities of our system design; several commented about the appealing\naffect of the Charlie mascot in post-study questions. Charlie may\nhave also had an effect on students‚Äô level of task perseverance [54].\nFinally, novelty bias is always a potential concern when evaluating\nnovel interfaces or systems, as-is self-selection bias for stand-alone\nstudies.\n11 CONCLUSION\nWe present results from a large-scale, multi-institution study of\nhow near-novices interact with Code LLMs. Our novel experimental\ndesign allows us to isolate the prompt writing and editing tasks, by\nusing a lab-based experiment in which participants write natural\nlanguage descriptions of tasks and receive automated feedback on\nthe correctness of generated code.\nOur results suggest that students who have complete a single\nCS course find using Code LLMs challenging, even with tasks at an\nappropriate skill level. Our findings highlight the various barriers\nthat they face, ranging from distilling their problem understanding\ninto words, using coding terminology, understanding generated\ncode, and grappling with the stochasticity of Code LLM output. We\nshow that certain groups of students, most notably, first-generation\ncollege students, face additional difficulties, raising equity issues\nrelated to the deployment of Code LLMs in the classroom. We also\nillustrate how students‚Äô incorrect mental models of how Code LLMs\noperate inhibit their ability to develop effective prompting strate-\ngies. Moreover, our qualitative results provide insight into how\nbeginning programmers feel about introducing Code LLMs in the\nclassroom, bringing their voices into an key contemporary debate\nand complementing existing work on educators‚Äô perspectives.\nOur findings suggest that Code LLMs do not signal the ‚Äúend of\nprogramming‚Äù: in fact, they highlight the many ways in which Code\nLLMs remain inaccessible to non-experts. We hope that our findings\nwill motivate renewed effort towards democratizing programming\nby closing this gap.\nACKNOWLEDGMENTS\nWe thank the anonymous reviewers and Shriram Krishnamurthi\nfor their thoughtful feedback. We thank our colleagues who helped\nus recruit participants and who provided CS1 problems that we\nadapted. We would also like to thank Rachelle Hu for her work on\nthe Charlie platform prototype. This work is partially supported by\nthe National Science Foundation (SES-2326173, SES-2326174, and\nSES-2326175). We thank Northeastern Research Computing and the\nNew England Research Cloud for providing computing resources.\nREFERENCES\n[1] Andrea Agostinelli, Timo I. Denk, Zal√°n Borsos, Jesse Engel, Mauro Verzetti, An-\ntoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi,\nMatt Sharifi, Neil Zeghidour, and Christian Frank. 2023. MusicLM: Generating\nMusic From Text. http://arxiv.org/abs/2301.11325\n[2] Nader Akoury, Shufan Wang, Josh Whiting, Stephen Hood, Nanyun Peng, and\nMohit Iyyer. 2020. STORIUM: A Dataset and Evaluation Platform for Machine-\nin-the-Loop Story Generation. http://arxiv.org/abs/2010.01717 arXiv:2010.01717\n[cs].\n[3] Hannah McLean Babe, Sydney Nguyen, Yangtian Zi, Arjun Guha, Molly Q.\nFeldman, and Carolyn Jane Anderson. 2023. StudentEval: A Benchmark of\nStudent-Written Prompts for Large Language Models of Code. http://arxiv.org/\nabs/2306.04556\n[4] Bruce W. Ballard and Alan W. Biermann. 1979. Programming in Natural\nLanguage: ‚ÄúNLC‚Äù as a Prototype. In Annual Conference of the ACM . Asso-\nciation for Computing Machinery, New York, NY, USA, 228‚Äì237. https:\n//doi.org/10.1145/800177.810072\n[5] Patrick Barei√ü, Beatriz Souza, Marcelo d‚ÄôAmorim, and Michael Pradel. 2022.\nCode Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained\nLanguage Models on Code. http://arxiv.org/abs/2206.01335 arXiv:2206.01335\n[cs].\n[6] Shraddha Barke, Michael B. James, and Nadia Polikarpova. 2023. Grounded\nCopilot: How Programmers Interact with Code-Generating Models. Proceedings\nof the ACM on Programming Languages 7, OOPSLA1 (April 2023), 85‚Äì111. https:\n//doi.org/10.1145/3586030\n[7] Christoph Bartneck, Dana Kuliƒá, Elizabeth Croft, and Susana Zoghbi. 2009.\nMeasurement Instruments for the Anthropomorphism, Animacy, Likeability,\nPerceived Intelligence, and Perceived Safety of Robots. International Journal of\nSocial Robotics 1, 1 (Jan. 2009), 71‚Äì81. https://doi.org/10.1007/s12369-008-0001-3\n[8] Douglas Bates, Martin M√§chler, Ben Bolker, and Steve Walker. 2015. Fitting\nLinear Mixed-Effects Models Using lme4. Journal of Statistical Software 67, 1\n(2015), 1‚Äì48. https://doi.org/10.18637/jss.v067.i01\n[9] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret\nShmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be\nToo Big?. In Proceedings of the 2021 ACM Conference on Fairness, Accountability,\nand Transparency . ACM, Virtual Event Canada, 610‚Äì623. https://doi.org/10.\n1145/3442188.3445922\n[10] Christian Bird, Denae Ford, Thomas Zimmermann, Nicole Forsgren, Eirini\nKalliamvakou, Travis Lowdermilk, and Idan Gazit. 2023. Taking Flight with\nCopilot: Early Insights and Opportunities of AI-Powered Pair-Programming\nTools. Queue 20, 6 (Jan. 2023), 35‚Äì57. https://doi.org/10.1145/3582083 Place:\nNew York, NY, USA Publisher: Association for Computing Machinery.\n[11] Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-\nCostin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson,\nMolly Q Feldman, Arjun Guha, Michael Greenberg, and Abhinav Jangda. 2023.\nMultiPL-E: A Scalable and Polyglot Approach to Benchmarking Neural Code\nGeneration. IEEE Transactions on Software Engineering 49, 7 (July 2023), 3675‚Äì\n3691. https://doi.org/10.1109/TSE.2023.3267446\n[12] Le Chen, Xianzhong Ding, Murali Emani, Tristan Vanderbruggen, Pei-Hung Lin,\nand Chunhua Liao. 2023. Data Race Detection Using Large Language Models.\nIn Proceedings of the SC ‚Äô23 Workshops of The International Conference on High\nPerformance Computing, Network, Storage, and Analysis . ACM, Denver CO USA,\n215‚Äì223. https://doi.org/10.1145/3624062.3624088\n[13] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde\nde Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy\nKhlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder,\nMikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens\nWinter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert,\nFotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss,\nAlex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji,\nShantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike,\nJosh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight,\nMiles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario\nAmodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evalu-\nating Large Language Models Trained on Code. http://arxiv.org/abs/2107.03374\narXiv:2107.03374 [cs].\n[14] CodeWhisperer. 2023. ML-powered Coding Companion ‚Äì Amazon CodeWhis-\nperer ‚Äì Amazon Web Services. https://aws.amazon.com/codewhisperer/\n[15] Github Copilot. 2023. Github Copilot Your AI pair programmer. https://github.\ncom/features/copilot\n[16] Malcolm Corney, Sue Fitzgerald, Brian Hanks, Raymond Lister, Renee McCauley,\nand Laurie Murphy. 2014. ‚ÄôExplain in Plain English‚Äô Questions Revisited: Data\nStructures Problems. In Proceedings of the 45th ACM technical symposium on\nComputer science education . ACM, Atlanta Georgia USA, 591‚Äì596. https://doi.\norg/10.1145/2538862.2538911\nCHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA Nguyen et al.\n[17] Arghavan Moradi Dakhel, Vahid Majdinasab, Amin Nikanjam, Foutse Khomh,\nMichel C. Desmarais, Zhen Ming, and Jiang. 2022. GitHub Copilot AI pair\nprogrammer: Asset or Liability? https://doi.org/10.48550/ARXIV.2206.15331\n[18] Maria De-Arteaga, Riccardo Fogliato, and Alexandra Chouldechova. 2020. A\nCase for Humans-in-the-Loop: Decisions in the Presence of Erroneous Algo-\nrithmic Scores. In Proceedings of the 2020 CHI Conference on Human Factors in\nComputing Systems (CHI ‚Äô20) . Association for Computing Machinery, New York,\nNY, USA, 1‚Äì12. https://doi.org/10.1145/3313831.3376638\n[19] Paul Denny, Juho Leinonen, James Prather, Andrew Luxton-Reilly, Thezyrie\nAmarouche, Brett A. Becker, and Brent N. Reeves. 2023. Promptly: Using Prompt\nProblems to Teach Learners How to Effectively Utilize AI Code Generators.\nhttp://arxiv.org/abs/2307.16364 arXiv:2307.16364 [cs].\n[20] Stefania Druga and Amy J Ko. 2021. How do children‚Äôs perceptions of machine\nintelligence change when training and coding smart programs?. In Interaction\nDesign and Children . ACM, Athens Greece, 49‚Äì61. https://doi.org/10.1145/\n3459990.3460712\n[21] Stephen H. Edwards. 2004. Using Software Testing to Move Students from Trial-\nand-Error to Reflection-in-Action. In Proceedings of the 35th SIGCSE Technical\nSymposium on Computer Science Education . ACM, Norfolk Virginia USA, 26‚Äì30.\nhttps://doi.org/10.1145/971300.971312\n[22] Molly Q Feldman, Ji Yong Cho, Monica Ong, Sumit Gulwani, Zoran Popoviƒá,\nand Erik Andersen. 2018. Automatic diagnosis of students‚Äô misconceptions in\nk-8 mathematics. In Proceedings of the 2018 CHI Conference on Human Factors in\nComputing Systems . 1‚Äì12.\n[23] Kasra Ferdowsi, Ruanqianqian Huang, Michael B. James, Nadia Polikarpova,\nand Sorin Lerner. 2023. Live Exploration of AI-Generated Programs. http:\n//arxiv.org/abs/2306.09541 arXiv:2306.09541 [cs].\n[24] Sally Fincher, Raymond Lister, Tony Clear, Anthony Robins, Josh Tenenberg,\nand Marian Petre. 2005. Multi-institutional, multi-national studies in CSEd\nResearch: some design considerations and trade-offs. In Proceedings of the first\ninternational workshop on Computing education research (ICER ‚Äô05) . Association\nfor Computing Machinery, New York, NY, USA, 111‚Äì121. https://doi.org/10.\n1145/1089786.1089797\n[25] James Finnie-Ansley, Paul Denny, Brett A. Becker, Andrew Luxton-Reilly, and\nJames Prather. 2022. The Robots Are Coming: Exploring the Implications\nof OpenAI Codex on Introductory Programming. In Australasian Computing\nEducation Conference (ACE ‚Äô22) . Association for Computing Machinery, New\nYork, NY, USA, 10‚Äì19. https://doi.org/10.1145/3511861.3511863\n[26] Emily First, Markus Rabe, Talia Ringer, and Yuriy Brun. 2023. Baldur: Whole-\nProof Generation and Repair with Large Language Models. In Proceedings of the\n31st ACM Joint European Software Engineering Conference and Symposium on the\nFoundations of Software Engineering . ACM, San Francisco CA USA, 1229‚Äì1241.\nhttps://doi.org/10.1145/3611643.3616243\n[27] Matthew Flatt, Matthias Felleisen, Robert Bruce Findler, and Shriram Krishna-\nmurthi. 2001. How To Design Programs . MIT Press.\n[28] Max Fowler, Binglin Chen, Sushmita Azad, Matthew West, and Craig Zilles. 2021.\nAutograding \"Explain in Plain English\" questions using NLP. In Proceedings\nof the 52nd ACM Technical Symposium on Computer Science Education . ACM,\nVirtual Event USA, 1163‚Äì1169. https://doi.org/10.1145/3408877.3432539\n[29] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi,\nRuiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis. 2023. InCoder:\nA Generative Model for Code Infilling and Synthesis. http://arxiv.org/abs/2204.\n05999 arXiv:2204.05999 [cs].\n[30] Marwa Gadala. 2017. Automation bias: exploring causal mechanisms and poten-\ntial mitigation strategies. https://api.semanticscholar.org/CorpusID:41123263\n[31] Chuqin Geng, Haolin Ye, Yixuan Li, Tianyu Han, Brigitte Pientka, and Xujie\nSi. 2022. Novice Type Error Diagnosis with Natural Language Models. http:\n//arxiv.org/abs/2210.03682 arXiv:2210.03682 [cs].\n[32] Kate Goddard, Abdul V. Roudsari, and Jeremy C. Wyatt. 2012. Automation bias:\na systematic review of frequency, effect mediators, and mitigators. Journal of\nthe American Medical Informatics Association : JAMIA 19 1 (2012), 121‚Äì7.\n[33] Jamie Gorson and Eleanor O‚ÄôRourke. 2020. Why do CS1 Students Think\nThey‚Äôre Bad at Programming?: Investigating Self-efficacy and Self-assessments\nat Three Universities. InProceedings of the 2020 ACM Conference on International\nComputing Education Research . ACM, Virtual Event New Zealand, 170‚Äì181.\nhttps://doi.org/10.1145/3372782.3406273\n[34] Claire Le Goues, Michael Pradel, and Abhik Roychoudhury. 2019. Automated\nprogram repair. Commun. ACM 62, 12 (2019), 56‚Äì65. Publisher: ACM New York,\nNY, USA.\n[35] Sumit Gulwani, Oleksandr Polozov, Rishabh Singh, and others. 2017. Program\nsynthesis. Foundations and Trends ¬Æ in Programming Languages 4, 1-2 (2017),\n1‚Äì119. Publisher: Now Publishers, Inc..\n[36] Sandra G. Hart and Lowell E. Staveland. 1988. Development of NASA-TLX\n(Task Load Index): Results of Empirical and Theoretical Research. In Advances\nin Psychology , Peter A. Hancock and Najmedin Meshkati (Eds.). Human Mental\nWorkload, Vol. 52. North-Holland, 139‚Äì183. https://doi.org/10.1016/S0166-\n4115(08)62386-9\n[37] Andrew Head, Elena Glassman, Gustavo Soares, Ryo Suzuki, Lucas Figueredo,\nLoris D‚ÄôAntoni, and Bj√∂rn Hartmann. 2017. Writing reusable code feedback\nat scale with mixed-initiative program synthesis. In Proceedings of the Fourth\n(2017) ACM Conference on Learning@ Scale . 89‚Äì98.\n[38] George E. Heidorn. 1974. English as a Very High Level Language for Simulation\nProgramming. In Proceedings of the ACM SIGPLAN Symposium on Very High\nLevel Languages. Association for Computing Machinery, New York, NY, USA,\n91‚Äì100. https://doi.org/10.1145/800233.807050\n[39] Abram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu.\n2012. On the naturalness of software. In Proceedings of the 34th International\nConference on Software Engineering (ICSE ‚Äô12) . IEEE Press, Zurich, Switzerland,\n837‚Äì847.\n[40] Jack Hollingsworth. 1960. Automatic graders for programming classes.Commun.\nACM 3, 10 (1960), 528‚Äì529. Publisher: ACM New York, NY, USA.\n[41] Kenneth Holstein and Shayan Doroudi. 2021. Equity and Artificial Intelligence\nin Education: Will \"AIEd\" Amplify or Alleviate Inequities in Education? http:\n//arxiv.org/abs/2104.12920 arXiv:2104.12920 [cs].\n[42] Kenneth Holstein, Bruce M McLaren, and Vincent Aleven. 2018. Student learning\nbenefits of a mixed-reality teacher awareness tool in AI-enhanced classrooms.\nIn Artificial Intelligence in Education: 19th International Conference, AIED 2018,\nLondon, UK, June 27‚Äì30, 2018, Proceedings, Part I 19 . Springer, 154‚Äì168.\n[43] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The\nCurious Case of Neural Text Degeneration. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 .\nOpenReview.net. https://openreview.net/forum?id=rygGQyrFvH\n[44] Silas Hsu, Tiffany Wenting Li, Zhilin Zhang, Max Fowler, Craig Zilles, and\nKarrie Karahalios. 2021. Attitudes Surrounding an Imperfect AI Autograder. In\nProceedings of the 2021 CHI Conference on Human Factors in Computing Systems .\nACM, Yokohama Japan, 1‚Äì15. https://doi.org/10.1145/3411764.3445424\n[45] Daphne Ippolito, Ann Yuan, Andy Coenen, and Sehmon Burnam. 2022. Creative\nWriting with an AI-Powered Writing Assistant: Perspectives from Professional\nWriters. http://arxiv.org/abs/2211.05030 arXiv:2211.05030 [cs].\n[46] Yue Jia and Mark Harman. 2011. An Analysis and Survey of the Development of\nMutation Testing. IEEE Transactions on Software Engineering 37, 5 (Sept. 2011),\n649‚Äì678. https://doi.org/10.1109/TSE.2010.62\n[47] Harshit Joshi, Jos√© Cambronero Sanchez, Sumit Gulwani, Vu Le, Gust Ver-\nbruggen, and Ivan Radiƒçek. 2023. Repair Is Nearly Generation: Multilingual\nProgram Repair with LLMs. Proceedings of the AAAI Conference on Artificial In-\ntelligence 37, 4 (June 2023), 5131‚Äì5140. https://doi.org/10.1609/aaai.v37i4.25642\n[48] Majeed Kazemitabaar, Justin Chow, Carl Ka To Ma, Barbara J. Ericson, David\nWeintrop, and Tovi Grossman. 2023. Studying the effect of AI Code Generators\non Supporting Novice Learners in Introductory Programming. In Proceedings\nof the 2023 CHI Conference on Human Factors in Computing Systems . ACM,\nHamburg Germany, 1‚Äì23. https://doi.org/10.1145/3544548.3580919\n[49] Heidy Khlaaf, Pamela Mishkin, Joshua Achiam, Gretchen Krueger, and Miles\nBrundage. 2022. A Hazard Analysis Framework for Code Synthesis Large\nLanguage Models. http://arxiv.org/abs/2207.14157 arXiv:2207.14157 [cs].\n[50] Amy J. Ko, Brad A. Myers, and Htet Htet Aung. 2004. Six Learning Barriers in\nEnd-User Programming Systems. In 2004 IEEE Symposium on Visual Languages -\nHuman Centric Computing . IEEE, Rome, Italy, 199‚Äì206. https://doi.org/10.1109/\nVLHCC.2004.47\n[51] Moritz K√∂rber. 2018. Theoretical considerations and development of a question-\nnaire to measure trust in automation. InCongress of the International Ergonomics\nAssociation. Springer, 13‚Äì30.\n[52] Sam Lau and Philip Guo. 2023. From \"Ban It Till We Understand It\" to \"Resistance\nis Futile\": How University Programming Instructors Plan to Adapt as More\nStudents Use AI Code Generation and Explanation Tools such as ChatGPT and\nGitHub Copilot. In Proceedings of the 2023 ACM Conference on International\nComputing Education Research - Volume 1 (ICER ‚Äô23) . Association for Computing\nMachinery, New York, NY, USA, 106‚Äì121. https://doi.org/10.1145/3568813.\n3600138\n[53] Tessa Lau. 2009. Why Programming-By-Demonstration Systems Fail: Lessons\nLearned for Usable AI. AI Magazine 30, 4 (Oct. 2009), 65‚Äì65. https://doi.org/10.\n1609/aimag.v30i4.2262\n[54] Michael J Lee and Amy J Ko. 2011. Personifying programming tool feedback im-\nproves novice programmers‚Äô learning. InProceedings of the seventh international\nworkshop on Computing education research . 109‚Äì116.\n[55] Juho Leinonen, Paul Denny, Stephen MacNeil, Sami Sarsa, Seth Bernstein, Joanne\nKim, Andrew Tran, and Arto Hellas. 2023. Comparing Code Explanations\nCreated by Students and Large Language Models. In Proceedings of the 2023\nConference on Innovation and Technology in Computer Science Education V. 1 .\nACM, Turku Finland, 124‚Äì130. https://doi.org/10.1145/3587102.3588785\n[56] Juho Leinonen, Arto Hellas, Sami Sarsa, Brent Reeves, Paul Denny, James Prather,\nand Brett A. Becker. 2023. Using Large Language Models to Enhance Pro-\ngramming Error Messages. In Proceedings of the 54th ACM Technical Sympo-\nsium on Computer Science Education V. 1 . ACM, Toronto ON Canada, 563‚Äì569.\nhttps://doi.org/10.1145/3545945.3569770\nHow Beginning Programmers and Code LLMs (Mis)read Each Other CHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA\n[57] Caroline Lemieux, Jeevana Priya Inala, Shuvendu K. Lahiri, and Siddhartha Sen.\n2023. CodaMosa: Escaping Coverage Plateaus in Test Generation with Pre-\ntrained Large Language Models. In 2023 IEEE/ACM 45th International Conference\non Software Engineering (ICSE) . IEEE, Melbourne, Australia, 919‚Äì931. https:\n//doi.org/10.1109/ICSE48619.2023.00085\n[58] Jonathan Vanian Leswing, Kif. 2023. ChatGPT and generative AI are booming,\nbut the costs can be extraordinary. https://www.cnbc.com/2023/03/13/chatgpt-\nand-generative-ai-are-booming-but-at-a-very-expensive-price.html\n[59] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Ko-\ncetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim,\nQian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier De-\nhaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo√£o Monteiro, Oleh Shliazhko,\nNicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar\nUmapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang,\nRudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco\nZocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wen-\nhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fe-\ndor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire\nSchlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu,\nJennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish\nContractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Car-\nlos Mu√±oz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von\nWerra, and Harm de Vries. 2023. StarCoder: may the source be with you!\nhttp://arxiv.org/abs/2305.06161 arXiv:2305.06161 [cs].\n[60] Jenny T. Liang, Chenyang Yang, and Brad A. Myers. 2023. A Large-Scale\nSurvey on the Usability of AI Programming Assistants: Successes and Challenges.\nhttp://arxiv.org/abs/2303.17125 arXiv:2303.17125 [cs].\n[61] Q. Vera Liao and Jennifer Wortman Vaughan. 2023. AI Transparency in the Age\nof LLMs: A Human-Centered Research Roadmap. http://arxiv.org/abs/2306.\n01941 arXiv:2306.01941 [cs].\n[62] Michael Xieyang Liu, Advait Sarkar, Carina Negreanu, Benjamin Zorn, Jack\nWilliams, Neil Toronto, and Andrew D. Gordon. 2023. ‚ÄúWhat It Wants Me To\nSay‚Äù: Bridging the Abstraction Gap Between End-User Programmers and Code-\nGenerating Large Language Models. In Proceedings of the 2023 CHI Conference\non Human Factors in Computing Systems . ACM, Hamburg Germany, 1‚Äì31. https:\n//doi.org/10.1145/3544548.3580817\n[63] Vivian Liu, Tao Long, Nathan Raw, and Lydia Chilton. 2023. Generative Disco:\nText-to-Video Generation for Music Visualization. http://arxiv.org/abs/2304.\n08551\n[64] Mike Lopez, Jacqueline Whalley, Phil Robbins, and Raymond Lister. 2008. Re-\nlationships between reading, tracing and writing skills in introductory pro-\ngramming. In Proceedings of the Fourth international Workshop on Computing\nEducation Research (ICER ‚Äô08) . Association for Computing Machinery, New York,\nNY, USA, 101‚Äì112. https://doi.org/10.1145/1404520.1404531\n[65] Farhad Manjoo. 2023. It‚Äôs the End of Computer Programming as We Know It.\n(And I Feel Fine.). The New York Times (June 2023). https://www.nytimes.com/\n2023/06/02/opinion/ai-coding.html\n[66] Nora McDonald, Sarita Schoenebeck, and Andrea Forte. 2019. Reliability and\nInter-rater Reliability in Qualitative Research: Norms and Guidelines for CSCW\nand HCI Practice. Proceedings of the ACM on Human-Computer Interaction 3,\nCSCW (Nov. 2019), 1‚Äì23. https://doi.org/10.1145/3359174\n[67] L. A.. Miller. 1981. Natural language programming: styles, strategies, and con-\ntrasts. IBM Systems Journal 20, 2 (June 1981), 184‚Äì215. https://doi.org/10.1147/\nsj.202.0184\n[68] Piotr Mirowski, Kory W. Mathewson, Jaylen Pittman, and Richard Evans. 2023.\nCo-Writing Screenplays and Theatre Scripts with Language Models: Evaluation\nby Industry Professionals. In Proceedings of the 2023 CHI Conference on Human\nFactors in Computing Systems . ACM, Hamburg Germany, 1‚Äì34. https://doi.org/\n10.1145/3544548.3581225\n[69] Vijayaraghavan Murali, Chandra Maddila, Imad Ahmad, Michael Bolin, Daniel\nCheng, Negar Ghorbani, Renuka Fernandez, and Nachiappan Nagappan. 2023.\nCodeCompose: A Large-Scale Industrial Deployment of AI-assisted Code Au-\nthoring. http://arxiv.org/abs/2305.12050 arXiv:2305.12050 [cs].\n[70] Brad A. Myers, Amy J. Ko, Thomas D. LaToza, and YoungSeok Yoon. 2016.\nProgrammers Are Users Too: Human-Centered Methods for Improving Pro-\ngramming Tools. Computer 49, 7 (July 2016), 44‚Äì52. https://doi.org/10.1109/\nMC.2016.200\n[71] Daye Nam, Andrew Macvean, Vincent Hellendoorn, Bogdan Vasilescu, and\nBrad Myers. 2024. Using an LLM to Help With Code Understanding. http:\n//arxiv.org/abs/2307.08177 arXiv:2307.08177 [cs].\n[72] National Center for Women & Information Technology. 2023. NCWIT Guide\nto Demographic Survey Questions. https://docs.google.com/document/d/1E_\nCSANwOqbKjEG27woNbGZ09JIXUfAf4Cp9j8g5DFak/\n[73] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou,\nSilvio Savarese, and Caiming Xiong. 2022. CodeGen: An Open Large Language\nModel for Code with Multi-Turn Program Synthesis. https://doi.org/10.48550/\nARXIV.2203.13474\n[74] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge\nAkkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam\nAltman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie\nBalcom, Paul Baltescu, Haiming Bao, Mo Bavarian, Jeff Belgum, Irwan Bello, Jake\nBerdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg\nBoiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks,\nMiles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann,\nBrittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang,\nFotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen,\nBen Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings,\nJeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch,\nDamien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning,\nAdrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix,\nSim√≥n Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges,\nChristian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes,\nJonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross,\nShixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen\nHe, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey,\nPeter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost\nHuizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang,\nHaozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaf-\ntan, ≈Åukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar,\nTabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim,\nHendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, ≈Åukasz Kon-\ndraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger,\nVishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel\nLevy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin,\nTheresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam\nManning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, An-\ndrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul\nMcMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz,\nAndrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel\nMossing, Tong Mu, Mira Murati, Oleg Murk, David M√©ly, Ashvin Nair, Reiichiro\nNakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh,\nLong Ouyang, Cullen O‚ÄôKeefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley\nPantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos,\nMikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres,\nMichael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle\nPokrass, Vitchyr Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth\nProehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond,\nFrancis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick\nRyder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather\nSchmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki\nSherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler,\nMaddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky,\nYang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya\nSutskever, Jie Tang, Nikolas Tezak, Madeleine Thompson, Phil Tillet, Amin\nTootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan\nFelipe Cer√≥n Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll\nWainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason\nWei, C. J. Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng,\nMatt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong,\nLauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah\nYoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang,\nMarvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk,\nand Barret Zoph. 2023. GPT-4 Technical Report. http://arxiv.org/abs/2303.08774\narXiv:2303.08774 [cs].\n[75] David Lorge Parnas and Jan Madey. 1995. Functional documents for computer\nsystems. Science of Computer Programming 25, 1 (Oct. 1995), 41‚Äì61. https:\n//doi.org/10.1016/0167-6423(95)96871-J\n[76] Sida Peng, Eirini Kalliamvakou, Peter Cihon, and Mert Demirer. 2023. The\nImpact of AI on Developer Productivity: Evidence from GitHub Copilot. http:\n//arxiv.org/abs/2302.06590 arXiv:2302.06590 [cs].\n[77] Tung Phung, Jos√© Cambronero, Sumit Gulwani, Tobias Kohn, Rupak Majumdar,\nAdish Singla, and Gustavo Soares. 2023. Generating High-Precision Feedback\nfor Programming Syntax Errors using Large Language Models. http://arxiv.\norg/abs/2302.04662 arXiv:2302.04662 [cs].\n[78] James Prather, Brent N. Reeves, Paul Denny, Brett A. Becker, Juho Leinonen,\nAndrew Luxton-Reilly, Garrett Powell, James Finnie-Ansley, and Eddie Antonio\nSantos. 2023. ‚ÄúIt‚Äôs Weird That it Knows What I Want‚Äù: Usability and Interactions\nwith Copilot for Novice Programmers. ACM Transactions on Computer-Human\nInteraction (Aug. 2023), 3617367. https://doi.org/10.1145/3617367\n[79] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya\nSutskever, and others. 2019. Language models are unsupervised multitask\nlearners. OpenAI 1, 8 (2019), 9. https://cdn.openai.com/better-language-\nmodels/language_models_are_unsupervised_multitask_learners.pdf\n[80] Iulian Radu and Bertrand Schneider. 2019. What can we learn from augmented\nreality (AR)? Benefits and drawbacks of AR for inquiry-based learning of physics.\nCHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA Nguyen et al.\nIn Proceedings of the 2019 CHI conference on human factors in computing systems .\n1‚Äì12.\n[81] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec\nRadford, Mark Chen, and Ilya Sutskever. 2021. Zero-Shot Text-to-Image Gener-\nation. In Proceedings of the 38th International Conference on Machine Learning .\nPMLR, 8821‚Äì8831. https://proceedings.mlr.press/v139/ramesh21a.html ISSN:\n2640-3498.\n[82] Hemilis Joyse Barbosa Rocha, Patr√≠cia Cabral De Azevedo Restelli Tedesco,\nand Evandro De Barros Costa. 2023. On the use of feedback in learning com-\nputer programming by novices: a systematic literature mapping. Informatics in\nEducation 22, 2 (2023), 209. Publisher: Institute of Mathematics and Informatics.\n[83] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn\nOmmer. 2022. High-Resolution Image Synthesis with Latent Diffusion Models.\nIEEE Computer Society, 10674‚Äì10685. https://doi.org/10.1109/CVPR52688.2022.\n01042\n[84] Steven I. Ross, Fernando Martinez, Stephanie Houde, Michael Muller, and\nJustin D. Weisz. 2023. The Programmer‚Äôs Assistant: Conversational Inter-\naction with a Large Language Model for Software Development. In Proceed-\nings of the 28th International Conference on Intelligent User Interfaces (IUI\n‚Äô23). Association for Computing Machinery, New York, NY, USA, 491‚Äì514.\nhttps://doi.org/10.1145/3581641.3584037\n[85] Baptiste Rozi√®re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiao-\nqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, J√©r√©my\nRapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cris-\ntian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D√©fossez, Jade\nCopet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas\nScialom, and Gabriel Synnaeve. 2024. Code Llama: Open Foundation Models\nfor Code. http://arxiv.org/abs/2308.12950 arXiv:2308.12950 [cs].\n[86] Jean E. Sammet. 1966. The Use of English as a Programming Language.Commun.\nACM 9, 3 (March 1966), 228‚Äì230. https://doi.org/10.1145/365230.365274\n[87] Max Sch√§fer, Sarah Nadi, Aryaz Eghbali, and Frank Tip. 2023. Adaptive Test\nGeneration Using a Large Language Model. https://doi.org/10.48550/arXiv.\n2302.06527 Issue: arXiv:2302.06527 _eprint: 2302.06527.\n[88] David E Shaw, William R Swartout, and C Cordell Green. 1975. Inferring LISP\nPrograms From Examples. In IJCAI, Vol. 75. 260‚Äì267.\n[89] Nikhil Singh, Guillermo Bernal, Daria Savchenko, and Elena L. Glassman. 2022.\nWhere to Hide a Stolen Elephant: Leaps in Creative Writing with Multimodal\nMachine Intelligence. ACM Transactions on Computer-Human Interaction (Feb.\n2022), 3511599. https://doi.org/10.1145/3511599\n[90] Rishabh Singh, Sumit Gulwani, and Armando Solar-Lezama. 2013. Automated\nfeedback generation for introductory programming assignments. In Proceedings\nof the 34th ACM SIGPLAN conference on Programming language design and\nimplementation. 15‚Äì26.\n[91] Linda J Skita, Kathleen Mosier, and Mark D. Burdick. 2000. Accountability and\nautomation bias. International Journal of Human-Computer Studies 52, 4 (2000),\n701‚Äì717. https://doi.org/10.1006/ijhc.1999.0349\n[92] Katta Spiel, Oliver L. Haimson, and Danielle Lottridge. 2019. How to do better\nwith gender on surveys: a guide for HCI researchers. Interactions 26, 4 (June\n2019), 62‚Äì65. https://doi.org/10.1145/3338283\n[93] Jiao Sun, Q. Vera Liao, Michael Muller, Mayank Agarwal, Stephanie Houde,\nKartik Talamadupula, and Justin D. Weisz. 2022. Investigating Explainability of\nGenerative AI for Code through Scenario-based Design. In 27th International\nConference on Intelligent User Interfaces . ACM, Helsinki Finland, 212‚Äì228. https:\n//doi.org/10.1145/3490099.3511119\n[94] Ryo Suzuki, Gustavo Soares, Andrew Head, Elena Glassman, Ruan Reis, Melina\nMongiovi, Loris D‚ÄôAntoni, and Bjoern Hartmann. 2017. Tracediff: Debugging\nunexpected code behavior using trace divergences. In 2017 IEEE Symposium on\nVisual Languages and Human-Centric Computing (VL/HCC) . IEEE, 107‚Äì115.\n[95] TabNine. 2023. AI Assistant for Software Developers | Tabnine. https://www.\ntabnine.com/\n[96] Priyan Vaithilingam, Elena L. Glassman, Peter Groenwegen, Sumit Gulwani,\nAustin Z. Henley, Rohan Malpani, David Pugh, Arjun Radhakrishna, Gustavo\nSoares, Joey Wang, and Aaron Yim. 2023. Towards More Effective AI-Assisted\nProgramming: A Systematic Design Exploration to Improve Visual Studio Intel-\nliCode‚Äôs User Experience. In International Conference on Software Engineering:\nSoftware Engineering in Practice (ICSE-SEIP) .\n[97] Priyan Vaithilingam, Tianyi Zhang, and Elena L. Glassman. 2022. Expectation\nvs. Experience: Evaluating the Usability of Code Generation Tools Powered by\nLarge Language Models. In Extended Abstracts of the 2022 CHI Conference on\nHuman Factors in Computing Systems (CHI EA ‚Äô22) . Association for Computing\nMachinery, New York, NY, USA. https://doi.org/10.1145/3491101.3519665\n[98] Maarten Van Mechelen, Rachel Charlotte Smith, Marie-Monique Schaper, Mari-\nana Tamashiro, Karl-Emil Bilstrup, Mille Lunding, Marianne Graves Petersen,\nand Ole Sejer Iversen. 2023. Emerging technologies in K‚Äì12 education: A future\nHCI research agenda. ACM Transactions on Computer-Human Interaction 30, 3\n(2023), 1‚Äì40. Publisher: ACM New York, NY.\n[99] Qiaosi Wang, Koustuv Saha, Eric Gregori, David Joyner, and Ashok Goel. 2021.\nTowards Mutual Theory of Mind in Human-AI Interaction: How Language\nReflects What Students Perceive About a Virtual Teaching Assistant. InProceed-\nings of the 2021 CHI Conference on Human Factors in Computing Systems . ACM,\nYokohama Japan, 1‚Äì14. https://doi.org/10.1145/3411764.3445645\n[100] Matt Welsh. 2022. The End of Programming. Commun. ACM 66, 1 (Dec. 2022),\n34‚Äì35. https://doi.org/10.1145/3570220 Place: New York, NY, USA Publisher:\nAssociation for Computing Machinery.\n[101] Chunqiu Steven Xia, Matteo Paltenghi, Jia Le Tian, Michael Pradel, and Lingming\nZhang. 2024. Fuzz4All: Universal Fuzzing with Large Language Models. http:\n//arxiv.org/abs/2308.04748 arXiv:2308.04748 [cs].\n[102] Frank F. Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. 2022.\nA systematic evaluation of large language models of code. In Proceedings of the\n6th ACM SIGPLAN International Symposium on Machine Programming . ACM,\nSan Diego CA USA, 1‚Äì10. https://doi.org/10.1145/3520312.3534862\n[103] J.D. Zamfirescu-Pereira, Richmond Y. Wong, Bjoern Hartmann, and Qian Yang.\n2023. Why Johnny Can‚Äôt Prompt: How Non-AI Experts Try (and Fail) to Design\nLLM Prompts. In Proceedings of the 2023 CHI Conference on Human Factors in\nComputing Systems . ACM, Hamburg Germany, 1‚Äì21. https://doi.org/10.1145/\n3544548.3581388\n[104] Shuyin Zhao. 2023. GitHub Copilot Now Has a Better AI Model and New\nCapabilities. https://github.blog/2023-02-14-github-copilot-now-has-a-better-\nai-model-and-new-capabilities/ Publication Title: The GitHub Blog.\n[105] Albert Ziegler, Eirini Kalliamvakou, X Alice Li, Andrew Rice, Devon Rifkin,\nShawn Simister, Ganesh Sittampalam, and Edward Aftandilian. 2022. Produc-\ntivity assessment of neural code completion. In Proceedings of the 6th ACM\nSIGPLAN International Symposium on Machine Programming . 21‚Äì29.\nHow Beginning Programmers and Code LLMs (Mis)read Each Other CHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA\nA ADDITIONAL METHODOLOGICAL DETAILS\nA.1 Study Design\nA.1.1 Pilot Study. In late 2022, we ran an IRB-approved pilot study\nwith 19 participants from all three institutions. These students had\ncompleted CS1 and at least one additional course, so they were\nineligible for the main study. Overall, we made few changes after\nthe pilot. The most consequential were to add an additional 15\nminutes (75 minutes total) to the study window, increase participant\ncompensation, and implement word wrapping in the interface to\nprevent excessive scrolling.\nA.1.2 Problem Adaptation. Our problems were based on CS1 prob-\nlems used at each of our three institutions. In most cases, we made\nsmall adaptations to the problems, both to make it less likely for\nstudents to recognize the exact problem, and to fit the constraints of\nthe Code LLM task (i.e, changing printed output to returned output,\navoiding library imports).\nFigure 10 presents two examples of how we adapted problems.\nFigure 10a shows the original presentation of the problem that\nwas adapted into mod_end. We added an additional parameter so\nthat the function substitutes a given string for the ‚Äòs‚Äô at the end\nof each string in the list. We also renamed the function. Note that\nin the original class setting, the problem was presented with three\ninput/output pairs, as in our experimental design.\nFigure 10b shows the original presentation of the problem that\nwas turned into find_multiples. We changed the function to return\nthe list of multiples rather than the number of multiples. As in\nour experiment, the original problem description contained three\ninput/output pairs.\nA.1.3 Problem Validation. By selecting from existing problems\nin the CS1 curricula, we ensured that the problems were at an\nappropriate difficulty level for our student population. In order to\nfocus specifically on the human-model interaction, we also needed\nto ensure that the problems were an appropriate difficulty level for\nthe code generation model: the model is capable of generating a\nsolution, but only when it is appropriately prompted.\nBecause code generation models memorize common associations\nbetween function names and function bodies, it is important to\nensure that the model cannot generate a passing implementation\nfrom the function name alone. We produced Codex generations\nfrom just the function signature for every problem, without any\nnatural language prompt, and measured mean pass@1 rate. We\nrenamed any functions with high pass@1 rates. For our final set of\nproblems, the overall mean pass@1 for function signatures alone is\n0.0519. The maximum pass@1 is 0.925, for the problem exp. This\nmeans that students generally need to provide a description of the\nfunction‚Äôs intended behavior in order for the model to produce a\ncorrect implementation.\nWe also ensured that there was a prompt that would lead to a\ncorrect implementation for every problem. Each problem has an\n‚Äúexpert‚Äù prompt written by one of the authors for which Codex pro-\nduces a correct implementation. These prompts were not otherwise\nused as part of the experiment.\nA.1.4 Test Case Validation. We rely on unit tests to check the\ncorrectness of model-generated code. These tests also produce feed-\nback for students about the model‚Äôs generated code. We built an\ninitial suite of test cases for each problem by taking tests from\ngrading rubrics and other class resources. We used test coverage\nand mutation testing [46] to identify missing test cases and build\nmore robust test coverage, while keeping the number of test cases\nper problem to a size that can be easily displayed.\nA.2 Qualitative Analysis\nAs described in the main body of the paper, the analysis of the qual-\nitative data was done by two researchers with previous qualitative\nanalysis experience. The aim was to identify common themes in\nthe data set, rather than build a generalizable theory. Below we\noutline the analyses performed on three types of data: (1) data\nabout student experience and demographics, (2) free-response ques-\ntions about future use of Charlie, and (3) the semi-structured in-\nterview responses. We provide the full codebooks, with defini-\ntions, for all data types as part of our Supplemental Materials at\nhttps://doi.org/10.17605/OSF.IO/V2C4T.\nA.2.1 Student Experience & Demographics. We used thematic anal-\nysis for the post survey questions, beginning with the Language,\nMajor, and Experience questions. Codes were developed inductively\n- the two researchers independently developed codes and then it-\nerated on a code set via conversation and consensus. We did not\ncalculate inter-rated reliability for these questions, as their specific\nuse was for quantitative analysis rather than for specific qualitative\ntrends [66]. Once the researchers arrived at a tentative codebook\nthey independently coded and iterated until there was complete\nconsensus on all codes for all data points as part of the post survey.\nThis took one round to normalize code application (e.g., Computer\nScience was not coded as a Natural Science) and then a second\nround where the codes were complete, but typos were identified.\nA.2.2 Free Response Questions. These questions (UseCharlie and\nForesee) were coded second out of the three kinds of qualitative\ndata. This process initially followed a similar inductive style to that\ndescribed above. Due to the open-ended nature of these responses,\nboth researchers then developed independent definitions for each\ncode to provide clearer guidelines for inclusion/exclusion. They\nthen met to merge their definitions and discuss any discrepancies.\nFor instance, normalizing most definitions to start with ‚ÄúMentions‚Äù\nand combining definitions or picking the more detailed. Then the\nresearchers independently coded according to the consensus defi-\nnitions. Arriving at consensus took two rounds. Two sets of codes\nwere combined (two subcodes of Skill Level and two subcodes of\nProblem Difficulty) and Documentation/Code Understanding was\nre-coded due to clarifications in their definitions. The final round\nof coding identified only typos and unintentional omissions. Again,\nconsensus was reached and inter-rated reliability was not calculated\nfor these codes.\nA.2.3 Semi-Structured Interview Analysis. We took a different ap-\nproach to coding the semi-structured interview data than the post-\nsurvey data, as the responses varied significantly in length and\nprecision. The details of the codebook development are described\nbelow, but the following process was conducted for all 8 questions:\nCHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA Nguyen et al.\n(a) Study problem called mod_end\n (b) Study problem called find_multiples\nFigure 10: Original problem presentations\n(1) The two coding researchers independently developed codes\nfor a set of 15 non-overlapping interviews. They met to\ndiscuss their codes and general themes.\n(2) They then coded 15 shared interviews to test the codes, add\nadditional codes, and finalize definitions. They reached con-\nsensus on the codes and their application to the shared 15\ninterviews.\n(3) To confirm their understanding of the codebook, they then\ncoded 20 shared interviews and calculated percent agree-\nment. Any codes with low agreement were discussed, had\ntheir definitions changed/edited, or were removed. The re-\nsearchers then came to consensus on how to apply the codes\nto these data.\n(4) The researchers then divided the remaining 70 interviews\nand coded independently, making use of a fixed codebook.\nThe researchers did not code interviews they conducted\nthemselves. They also independently recoded their original\n15 datapoints.\nThe two researchers began by coding the last four interview\nquestions, which they deemed more concrete. This process was\nprimarily inductive. Computing percent agreement across the data,\n87% of our 70 codes exhibited 90% or higher agreement (i.e. dis-\nagreement on 2 or less datapoints). 9 codes were less, with the\nminimum agreement being 75%.\nThe researchers then moved on to the first four interview ques-\ntions ‚Äì the last analysis performed on the qualitative data. This\nprocess was more deductive than previous analyses. For example,\nfor Hardest/Easiest, the topic categories of problems (e.g. Loops,\nConditionals) were particularly relevant to our analyses and the\ndata suggest those categories as codes. The first two problems had\nthe most variation in student responses ‚Äì we attribute this to stu-\ndents‚Äô lack of knowledge of their process, as they found this task\ndifficult overall. Therefore, the researchers focused on codes that\nerr on the side of temporal attributes. Percent agreement was again\ncalculated for these codes - 80% had 90% agreement or higher. Only\ntwo codes had agreement lower than 75% - the researchers discussed\nthese codes significantly, reaching agreement on the generality of\none code (Add Detail) and removing another code entirely.\nB ADDITIONAL QUANTITATIVE RESULTS\nB.1 Participant Demographics\nIn order to protect participant anonymity, we report responses\nto the open-ended demographics questions only if an identical\nresponse was submitted by at least 5 participants. Gender and\nrace responses are shown in Table 9 and Table 10. The majority\nof responses to the question about ethnicity were unique. Due to\nthe need to protect participant anonymity, we have chosen not to\nreport this data.\nSelf-Reported Gender N Mean pass@1\nFemale 72 0.22\nMale 30 0.21\nNonbinary 5 0.28\nAll other responses 13 0.24\nTable 9: Self-reported gender of participants, capitalization\nnormalized to title case.\nSelf-Reported Race N Mean pass@1\nAsian 38 0.22\nBlack 6 0.11\nEast Asian 6 0.34\nWhite 36 0.21\nAll other responses 34 0.23\nTable 10: Self-reported race of participants, capitalization\nnormalized to title case.\nB.1.1 Statistical Analysis. We used Welch Two Sample t-tests to ex-\nplore whether there were statistically reliable differences in pass@1\nrates for students with different backgrounds. Table 11 shows the\nresults.\nB.2 Problem Difficulty\nB.2.1 Statistical Analysis of Category Difficulty. A binomial mixed-\neffects model (Table 12) was fitted to prompt success as a binary\nHow Beginning Programmers and Code LLMs (Mis)read Each Other CHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA\nGroup 1 Group 2 ùë° ùëù\nDomestic student International student -0.4 0.68\nFirst generation college student Not first generation college student 2.1 0.04\nEnglish in childhood household No English in childhood household 1.02 0.31\nPublic high school Private high school 0.1 0.92\nCoding experience outside of CS1 No other coding experience 2.47 0.02\nMore than 1 math course 1 college math course 0.8 0.43\nTable 11: Welch Two Sample t-tests to explore differences in pass@1 rates between demographic groups\nFixed effects bùõΩ ùëß ùëù\n(Intercept) -0.80 (+/- 0.6) -1.35 0.18\nDictionaries -0.70 (+/- 0.8) -0.8 0.41\nLists -0.55 (+/- 0.8) -0.7 0.51\nLoops -0.92 (+/- 0.8) -1.1 0.28\nMath -1.10 (+/- 0.8) -1.3 0.19\nNested 0.83 (+/- 0.8) 1.0 0.32\nSorting -1.75 (+/- 0.9) -2.0 0.045\nStrings 0.14 (+/- 0.8) 0.2 0.87\nWellesley 0.14 (+/- 0.4) 0.4 0.71\nOberlin 0.24 (+/- 0.4) 0.6 0.53\nDictionaries:Wellesley -0.04 (+/- 0.6) -0.1 0.95\nLists:Wellesley 0.37 (+/- 0.5) 0.7 0.50\nLoops:Wellesley 0.62 (+/- 0.5) 1.2 0.25\nMath:Wellesley 0.18 (+/- 0.5) 0.3 0.73\nNested:Wellesley -0.98 (+/- 0.5) -1.9 0.062\nSorting:Wellesley -0.16 (+/- 0.6) -0.3 0.77\nStrings:Wellesley -0.17 (+/- 0.5) -0.4 0.73\nDictionaries:Oberlin -0.52 (+/- 0.6) -0.9 0.37\nLists:Oberlin -0.05 (+/- 0.5) -0.1 0.93\nLoops:Oberlin -0.33 (+/- 0.6) -0.6 0.57\nMath:Oberlin 0.14 (+/- 0.5) 0.3 0.79\nNested:Oberlin -0.57 (+/- 0.5) -1.1 0.28\nSorting:Oberlin -0.10 (+/- 0.6) -0.2 0.86\nStrings:Oberlin 0.10 (+/- 0.5) 0.2 0.84\nTable 12: Full results of binomial mixed-effects model fitted\nto problem category and institution.\noutcome (1 if the prompt succeeded; 0 otherwise). The model in-\ncluded fixed effects of problem category, institution, and their inter-\naction, and random effects of participant and problem. Treatment\ncoding was used for institution, with Northeastern as the baseline\ncategory; deviation coding was used for category, since we were\ninterested in whether any one category differed from the average\nproblem difficulty.\nB.2.2 Least-Solved Problems. To understand where struggles arise,\nwe manually examined student responses to two problems: laugh,\nwhich has one of the lowest number of student successes, and\ntotal_bill, which has a mid-range success rate.\nA challenging problem: laugh. One of the least-solved problems\nin our study was laugh. The intended function takes a number ùëõ\nand produces a string of ùëõ ‚Äúha‚Äùs, where the initial ‚Äúha‚Äù has ùëõ ‚Äúa‚Äùs,\nand each subsequent laugh has one fewer ‚Äúa‚Äù.\nOnly two students were able to eventually succeed at this task\n(orchidW alleyeand magentaWeasel). However, a manual inspec-\ntion of all initial student descriptions reveals only one serious misun-\nderstanding of the task (tealPossum) ‚Äì see Table 13 for all students‚Äô\ninitial descriptions.\nA mid-range problem: total_bill. The task in total_bill is\nto compute the total of a grocery bill, using a list of grocery items\nand a sales tax rate. Each grocery item is itself a list containing the\nname of the item, a quantity, and a price. One expert description\nthat reliably generates a working program is Returns the sum of\nmultiplying the second and third indices of each list in grocery_list,\nmultiplied by 1 + sales_tax. Round to 2 digits .\nWe manually inspect all descriptions for this problem. Of the 20\nstudents who attempted this problem, 12 eventually succeed. All of\nthese students follow a similar path: their first attempt omits the\nrounding step, leading one of the tests to fail. A handful of students\nalso omit or incorrectly describe the sales tax step initially.\nWhat about the students who never succeed? One student ini-\ntially misunderstands the task, writing: This function takes in a list\nof the item purchased, the price, the tax, and the overall sales tax. All\nof the prices and tax within the lists are added together. The sales\ntax is then multiplied by the outcome of the added prices, and then\nthe result of the multiplication is added onto the total price. The total\nprice is then returned as the output. (limeSalamander)\nThe student has misunderstood a key detail in the structure of\nthe lists: the two numbers are the quantity and price, so they should\nbe multiplied, not added. Consequently, this prompt fails. However,\ntheir third description is accurate: This function takes in a list of the\nitem purchased, the amount of the item purchased, the price for each\nitem, and the overall sales tax. The amount purchased is multiplied\nwith the price for each item, creating a total amount. The sales tax\nis then multiplied by the outcome of the total amount, and then the\nresult of the multiplication is added onto the total price. The total\nprice is then returned as the output.\nAlthough the student initially misunderstood part of the prob-\nlem, they are able to reread the input/output pairs and/or code,\narriving at the correct interpretation eventually. However, their\ndescription still fails. This participant eventually runs out of time.\nThe rest of the participants who never succeed submit accurate\ndescriptions that omit key details, such as how to calculate the sales\ntax (6 participants) or the list positions of the price and quantity (5\nparticipants).\nOverall, the student prompts for total_bill demonstrate more\nissues in describing the problem than in understanding it. Although\none participant misunderstands the task initially, they were able to\nquickly self-correct.\nCHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA Nguyen et al.\nParticipant Initial Description N submissions\naquaLadybug If n is the input value, returns a combination of n strings, where each of the n strings\nconsists of \"h\" followed by n occurrences of \"a\", and there is \" \" before each \"h\" except the\nfirst \"h\"\n18\ngreenMoth a function have initial input as ‚Äòha‚Äô when input of size(int) is 1, when size+= 1 from 1, ‚Äòha‚Äô\nwill gain one more ‚Äòa‚Äô\n2\norchidBeetle Based on the inputted number, will return a laugh size where the number of \"a\"‚Äôs starts\nwith the initial size, then decreases by one for each additional laugh.\n4\ntealPossum return the number of words in a string 2\npinkFisher the function laugh will take the input of an int and should output a string with the ha as\nmany times as the input but also the number of a‚Äôs is based on the number it is currently\nworking with\n4\nmagentaWeasel Write a function which takes an integer size as an input, and uses a for loop to print an h\nfollowed by size a‚Äôs and then a space, and then an h followed by size-1 a‚Äôs and then a space,\netc. until it prints a h followed by one a\n7\naquamarineShrew This function prints an ‚Äòh‚Äô and adds the corresponding amount of a‚Äôs as the value provided.\nIt then adds a space to the output. It subtracts 1 from the value and prints another h with\nless a‚Äôs and repeats until the value of the number is 0\n26\norchidW alleye function adds ‚Äòa‚Äô to every ‚Äòh‚Äô based on input and will lower amount of ‚Äòa‚Äô until it reaches\nonly 1 ‚Äòa‚Äô after the ‚Äòh‚Äô\n3\nkhakiBee take in a number and write the word ‚Äòha‚Äô but with as many ‚Äòa‚Äôs as the number 7\npinkPerch Produce a string, with each word starting with h and then however many a‚Äôs the input says.\nDecrease the count of a‚Äôs by one following the h for each word after.\n5\norchidFlounder the input generates a string where the number corresponds to how many items are in the\nstring. each item in the string also starts with the letter ‚Äòh‚Äô and the letter ‚Äòa‚Äô is added to the\nletter ‚Äòh‚Äô based on the number of the input. However, only the first item in the string has\nthe number of ‚Äòa‚Äô equal to the input, the following ‚Äòa‚Äô are added to ‚Äòh‚Äô by subtracting 1 from\nthe input.\n1\nbeigeBass the code increases the number of the letters in \"ha, \" depending on the input in an increasing\nfactorial way\n1\ntomatoFisher This function takes an integer and an input produces the word \"ha\" that number of times\nbut the number of times \"a\" appears in each \"ha\" decreases by one until \"ha\"\n2\ncrimsonVole Takes in an integer ‚Äòn‚Äô input and outputs a string with ‚Äòn‚Äô words, ‚Äòh‚Äô as the first letter for\neach word, and ‚Äòn‚Äô number of ‚Äòa‚Äôs after it, followed by ‚Äòh‚Äô as the first letter of the next word\nand ‚Äòn-1‚Äô number of ‚Äòa‚Äôs after it and so on until we reach n = 1\n1\nlavenderPossum Given an integer, return a string in the form ‚Äòha‚Äô where the integer determines the number\nof a‚Äôs and repeat the same pattern until there is one a\n5\nlavenderBat The input takes in a number, say n, and produces a string that has n words. the first word is\nformed of one \"h\" and n number of \"a\". The number of \"a\" decreases by one for each next\nword\n8\nmagentaDolphin This function returns the number of laughs in a string, where a laugh is the character ‚Äòh‚Äô\nfollowed by any number of the character ‚Äòa‚Äô\n9\nlinenBobcat Counts the number of laughs, beginning with the given number of \"a\"s within it and\ndescending by each laugh, totaling the given number of laughs.\n2\ngrayVole Takes size and uses recursion to produce that number of \"ha\" laughs with one less \"a\" with\neach \"ha\" until there is only one \"a\" left\n8\nthistleTrout Using the given number, add that number of \"a\"s after an \"h\". Count down the number by 1,\nand add that number of \"a\"s after another \"h\" and repeat.\n5\nTable 13: Initial descriptions of the laugh problem from all 20 students who encountered it. N submissionsdescribes how many\ntimes the specific student attempted laugh before succeeding or giving up.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6329559087753296
    },
    {
      "name": "Programming language",
      "score": 0.5560537576675415
    },
    {
      "name": "Code (set theory)",
      "score": 0.5506758093833923
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 36
}