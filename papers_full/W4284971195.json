{
    "title": "News headline generation based on improved decoder from transformer",
    "url": "https://openalex.org/W4284971195",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5101907881",
            "name": "Zhengpeng Li",
            "affiliations": [
                "University of Science and Technology Liaoning"
            ]
        },
        {
            "id": "https://openalex.org/A5102847901",
            "name": "Jiansheng Wu",
            "affiliations": [
                "University of Science and Technology Liaoning"
            ]
        },
        {
            "id": "https://openalex.org/A5006102159",
            "name": "Jiawei Miao",
            "affiliations": [
                "University of Science and Technology Liaoning"
            ]
        },
        {
            "id": "https://openalex.org/A5044347784",
            "name": "Xinmiao Yu",
            "affiliations": [
                "University of Science and Technology Liaoning"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2946343014",
        "https://openalex.org/W2997920211",
        "https://openalex.org/W3012910066",
        "https://openalex.org/W6773118862",
        "https://openalex.org/W2914949666",
        "https://openalex.org/W2970634364",
        "https://openalex.org/W3176778415",
        "https://openalex.org/W3171099033",
        "https://openalex.org/W2964165364",
        "https://openalex.org/W4205508672",
        "https://openalex.org/W2609482285",
        "https://openalex.org/W3127669014",
        "https://openalex.org/W2798542795",
        "https://openalex.org/W2963929190",
        "https://openalex.org/W2606974598",
        "https://openalex.org/W3035328829",
        "https://openalex.org/W2962996600",
        "https://openalex.org/W3196834000",
        "https://openalex.org/W3174620462",
        "https://openalex.org/W3043667764",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W3098493824",
        "https://openalex.org/W2982399380",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W3042328599",
        "https://openalex.org/W3193120685",
        "https://openalex.org/W3102307539"
    ],
    "abstract": null,
    "full_text": "1\nVol.:(0123456789)Scientific Reports |        (2022) 12:11648  | https://doi.org/10.1038/s41598-022-15817-z\nwww.nature.com/scientificreports\nNews headline generation \nbased on improved decoder \nfrom transformer\nZhengpeng Li, Jiansheng Wu*, Jiawei Miao & Xinmiao Yu\nMost of the news headline generation models that use the sequence-to-sequence model or recurrent \nnetwork have two shortcomings: the lack of parallel ability of the model and easily repeated \ngeneration of words. It is difficult to select the important words in news and reproduce these \nexpressions, resulting in the headline that inaccurately summarizes the news. In this work, we propose \na TD-NHG model, which stands for news headline generation based on an improved decoder from \nthe transformer. The TD-NHG uses masked multi-head self-attention to learn the feature information \nof different representation subspaces of news texts and uses decoding selection strategy of top-\nk, top-p, and punishment mechanisms (repetition-penalty) in the decoding stage. We conducted a \ncomparative experiment on the LCSTS dataset and CSTS dataset. Rouge-1, Rouge-2, and Rouge-L on \nthe LCSTS dataset and CSTS dataset are 31.28/38.73, 12.68/24.97, and 28.31/37.47, respectively. The \nexperimental results demonstrate that the proposed method can improve the accuracy and diversity \nof news headlines.\nNews headline generation (NHG)1–5 has been an important task in natural language processing (NLP), in recent \nyears. NHG model can be divided into two categories: extractive and abstractive. The extractive directly selects \nseveral important words from the news text and rearranges them to form a news  headline6. The abstractive \nuses advanced natural language processing algorithms to generate news headlines using techniques such as \nparaphrasing, synonymous substitutions, and sentence contractions. Since the neural network method has been \napplied to news headline generation, the neural network-based abstractive news headline generation  model7–10 \nhas recently shown great performance.\nIn recent years, encoder-decoder-based neural network models have been widely used in text summariza-\ntion, mechanical fault  detection11, etc. It is worth emphasizing that, the abstractive neural network model based \non encoder-decoder12–16 has been proved to have a good performance on LCSTS  dataset17, DUC-2004 dataset, \nand other data sets. The model based on a transformer effectively solves the problem of insufficient parallel abil-\nity of sequence-to-sequence models. The abstractive headline generation method can produce words that are \nnot found in the original text, but this method may also make the generated news headlines out of the original \n facts18. As the recurrent neural network has the sequence coding characteristic that the information previously \ninput will be gradually forgotten as time goes by, the intermediate semantics lack some significant  information19, \nwhich leads to the headlines generated in the decoding process deviating from the main idea of the news text. \nMoreover, abstractive methods do not specifically process nonimportant or sub important text; that is, some \nnonimportant semantic information will be preserved with the same importance as feature semantic information \nwhen generating headlines, and there is noise interference.\nFor example, as shown in Fig. 1, when the news is condensed, the extractive news headline generation directly \nextracts some semantic information. It can be observed that the TD-NHG model filters the semantic information \nin the original news, abandon the explicit information of “When Ren Zhiqiang persisted in his role as a reporter \nfor developers” in the original news, and selects salient semantic information which is more important to the \ncontext semantic information, as the output of the generation. Outputs 3 and 4 choose two salient semantic pieces \nof semantic information as the output of the headline generation: “the living environment of private enterprises \nis getting worse” and “the competitiveness of enterprises to promote” , respectively. Compared with the original \nnews text, it is found that the forum not only refers to the poor living environment of private enterprises but also \nhow to improve their competitiveness in the case of large environmental changes.\nOur contributions can be summarized as follows: (1) This paper proposes the TD-NHG model to solve the \nNHG problem. (2) We introduce masked multi-head self-attention into news headline generation and design \na decoding selection strategy that integrates top-k, top-p, and punishment mechanisms to select important \nOPEN\nUniversity of Science and Technology Liaoning, Anshan, China. *email: ssewu@163.com\n2\nVol:.(1234567890)Scientific Reports |        (2022) 12:11648  | https://doi.org/10.1038/s41598-022-15817-z\nwww.nature.com/scientificreports/\nsemantic information and generate news headlines. (3) We evaluate our proposed TD-NHG model on the LCSTS \ndataset and CSTS dataset. The experimental results show that the TD-NHG model is similar to or even exceeds \nthe baseline model when dealing with NHG tasks.\nBackground and related work\nIn the early 1980s, natural language generation gradually became a hot research field. In the 1980s and 1990s, a \nstatistical language model was proposed to generate news headlines by analyzing word frequency, text location \ninformation, and text length, although this model is easy to implement, it cannot learn the complete semantic \ninformation in paragraphs. In 2004, Mihalcea et al. proposed  TextRank20, which is a sort method based on a graph \nmodel. In this method, news text is divided into several words, and a TextRank network graph is constructed by \ntaking these words as nodes and the number of co-occurrences within a certain range between words in news \ntext as edges. The PageRank algorithm is used to update the graph until convergence, and the news headlines \nare composed of words with high ranking.\nThe sequence-to-sequence model is an end-to-end neural network. The sequence-to-sequence model is com-\nposed of five parts: document, tokenizer, encoder, attention, and decoder. The tokenizer segments the document \ninto a series of words. The encoder is used to encode the word vector sequence into the hidden state of each \nword, and the weights of each word are calculated by attention. The decoder calculates the probability of each \nword in the vocabulary as an output word and uses a search algorithm to obtain news headlines. The sequence-\nto-sequence abstractive model often ignores the secondary important semantic information in feature semantic \ninformation extraction, and the parallel ability of the model is poor. Zhou et al. 21 proposed the text summary \ngeneration method based on an improved sequence-based sequence model, which is composed of an input layer, \nhidden layer, and output layer and introduces a copy mechanism to solve the problem of out-of-vocabulary \n(OOV)14 words in the process of summary generation, but there is still room for further improvement of accuracy.\nWith  T522,  STEP23, BART 24 and other large-scale multitask pre-training models proposed, each NLP task \nhas reached a new SOTA. Recurrent neural networks and sequence-to-sequence models have gradually been \nreplaced by models based on the transformer. For example, BERT is a bidirectional pre-training model. BERT \nachieves SOTA in a variety of more than 10 NLP tasks by using a large number of unlabeled text training language \nmodels through unsupervised methods. This large-scale pre-training model has reached an amazing number \nof parameters. The BERT of 12 layers has approximately 110 M parameters, which can be trained on a single \nGPU. BERT of 24 layers and even more layers has more than 340 M parameters, which can only be run on TPU. \nThis large-scale pre-training model has reached an amazing number of parameters. The BERT of 12 layers have \napproximately 110 M parameters, which can be trained on a single GPU. BERT with 24 layers and even more \nlayers have more than 340 M parameters, which can only be run on TPU, and the scale of the computation is \ntoo large for the average researcher to handle. Ordinary researchers cannot afford to consume large amounts of \ncomputing power. The TD-NHG model used a decoding selection strategy integrating top-k, top-p, and pun-\nishment mechanisms (repetition-penalty) in the decoder stage, which achieved a good effect on News headline \ngeneration. In addition, it runs perfectly in parallel on a single GPU.\nModel\nProblem definition and overview. News headline generation (NHG) aims to train a neural network \nmodel to map a text into a short text  headline25. The input of the NHG model is X = {x1 ,x2 ,..., xi} , and the \noutput news headline is Y =\n{\ny1 ,y2 ,... ,yj\n}\n . The vocabulary used by the NHG model is V ={ V 1 ,V 2 ,... ,V i} . \nIn this paper, the generation probability of TD-NHG can be formulated as\n(1)P(Y |X ; θ) =\nN∏\nj=1\nP(yj|X ,y1:j-1 ; θ)\nFigure 1.  Comparison of output different models on a news document. Original New is the original news \ncontent. Output 1 (Original) is the news headline in the dataset. Output 2 (Extractive) is generated by \nthe TextRank model. Output 3 (Abstractive) is generated by the sequence-to-sequence model. Output 4 \n(Abstractive-Ours) is generated by the TD-NHG model.\n3\nVol.:(0123456789)Scientific Reports |        (2022) 12:11648  | https://doi.org/10.1038/s41598-022-15817-z\nwww.nature.com/scientificreports/\nwhere θ is the model parameter of the news headline generation.\nThe transformer model is composed of an encoder and decoder, and both are stacked by parts called the \n“transformer module” , for example, encoding module, decoding module, attention module, normalization mod-\nule, etc. Much of the subsequent work attempts to remove encoders or decoders, in other words, researchers stack \nmultiple transformer modules and train or pre-training them using large text and considerable computing power. \nTD-NHG model is an autoregressive model with 12 transformer-decoder layers. The TD-NHG model is divided \ninto three main parts: the input module of the news headline generation, generation module based on improved \ntransformer-decoder, decoding selection strategy, and punishment mechanism. The model is shown in Fig. 2.\nInput module of news headline generation. The input module of the NHG model includes three parts. \nThe first part is document representation (D), which is composed of [CLS], news text, [SEP], news headlines, \nand [SEP]. D is embedded into d-dimensional vector space by an embedding layer to obtain E(D), such as for -\nmula (2),\nThe second part is the qualifier (Z). In addition to the five commonly used qualifiers [Space], [UNK], [CLS], \n[SEP], and [MASK], the TD-NHG model introduces [Content] and [Title] to effectively distinguish the news \nheadline and news content. Input embedding layer embeds Z into embedding matrix.\nThe third part is positional embedding and segment embedding. The transformer model does not mark the \norder of the input words. To solve a positional information problem, the TD-NHG model adds positional embed-\nding (PE) and segment embedding (SE) in the input layer, in which the dimension of PE and SE are consistent \nwith that of input embedding (IE). The PE vector determines the relative distance between different tokens in a \nsentence. The formula is as follows (3) and (4),\nwhere posn ∈ RLen×d refers to the absolute position of the n-th word in the original sentence, Len is the maximum \nlength of position information, and d represents the dimension of position embedding, ind is the dimension. \nTD-NHG model uses sine encoding when dealing with words in even positions, and sine encoding when dealing \nwith words in odd positions. EPE is the location embedding matrix, and size is the maximum sequence length \nmultiplied by embedding dimension, which is initialized with normal distribution to improve the readability of \nthe headline. The output of the model input module is defined as formula (5),\n(2)E\n(\nInput\n)\n= {E(D )},\n(3)PE\n(\nposn ,2 × ind\n)\n= sin\n(\nposn\n/(\n10000\n2×ind\nd mod el\n))\n,\n(4)PE\n(\nposn ,2 × ind + 1\n)\n= cos\n(\nposn\n/(\n10000\n2×ind\nd mod el\n))\n,\n(5)E\n(\nOutput\n)\n= E(D + Z) + EPE (D + Z) + ESE(D + Z),\nFigure 2.  TD-NHG model diagram.\n4\nVol:.(1234567890)Scientific Reports |        (2022) 12:11648  | https://doi.org/10.1038/s41598-022-15817-z\nwww.nature.com/scientificreports/\nE\n(\nOutput\n)\n∈ RB×L×de , where B is the batch-size of the training model. L  set to 512, which is the length of the \ninput sequence, and de is the dimension of the embedding representation. E(D + Z) represents the token embed-\nding of news ( including news content and headlines) and qualifier of the input model, EPE (D + Z) represents \nthe position embedding of news and qualifier of the input model, ESE(D + Z) represents the segment embedding \nof news and qualifier of the input model.\nGeneration module based on improved transformer-decoder. Input the output of the input mod-\nule (E(Output)) into the generation module based on the improved transformer-decoder, which is normalized \nby layer normalization (LN). The normalized word vector is transmitted to masked multi-head self-attention. \nThe attention layer mainly alleviates the complexity of the neural network. The attention layer does not need to \ninput all E(Output) into the neural network for calculation. On the contrary, attention selects some task-related \ninformation to input into the neural network, which is similar to the idea of a gating mechanism in the RNN \nmodel. The attention mechanism is essentially an addressing process. By giving a task-related query vector (Q), \ncalculating the attention distribution of Q and Key (K), and attaching it to Value (V), then the attention value is \nobtained.\nThe scaled dot-product attention used in the generation module based on improved transformer-decoder \nis optimized by adding scale and mask operations based on attention. The self-attention mechanism is the \ndifferent attention of a single sequence at different positions, which is used to calculate the representation of \nthe sequence. In this paper, d e is used to represent the output dimension of the self-attention mechanism, and \n{W Q, W K, W V}∈ Rm×d is used to represent the trainable parameter matrix. Then, the context representation \ncan be obtained according to the following calculation process,\nAn additional scaling factor √dk is introduced, and the difference between additive attention and dot-product \nattention is minuscule when the value of √dk is small. However, if √dk increases, the dot-product value is large; \nas a result, the gradient after  softmax26 is tiny, which is not conducive to backpropagation, so scaling is performed \non the result.\nAs shown in Fig. 3, E = [E1 ,E2 ,E3 ,..., Ei,..., En ] denotes the characteristic representation vector of the i-th \ntime. When given the query vectors Q = [Q 1 ,Q 2 ,Q 3 ,..., Q i,..., Q t] and K = [K 1 ,K 2 ,K 3 ,..., K i,..., K t] , the \nsimilarity A 1,i of Q i and K i is calculated, A 1,i calculation method as formula (7),\nThe new weight A\n′\n1,i is calculated by softmax, the context vector A i is obtained, A i represents the context \nvector at the i-th time,\n(6)Attention(Q, K , V ) = SoftMax\n((QK T )√(dk)\n)\nV ,\n(7)A1,i = Q 1 Ki√\nd\n,\n(8)Ai =\nn∑\ni=1\nA\n′\n1, iV i.\nFigure 3.  A1 context vector calculation method.\n5\nVol.:(0123456789)Scientific Reports |        (2022) 12:11648  | https://doi.org/10.1038/s41598-022-15817-z\nwww.nature.com/scientificreports/\nMulti-Head Self-Attention provides multiple representation subspaces for Attention. In each attention, with \ndifferent Q, K, and V  weighting matrices, each matrix is generated by random initialization. Then, the word \nembedding is projected into different representation subspaces by training.\nThe news headline generation task generates words in the news headline in turn, that is, the i -th word is \ngenerated before the (i+1)-th word is generated. The Masked operation prevents the i-th word from knowing the \ninformation after the (i+1)-th word. Q, K, V matrices are calculated by input matrix. Then calculate the product \nof Q and K and multiply the product of Q  and K with matrix V to get the output. For the model to learn more \nsubspace information, TD-NHG model uses mask multi-head self-attention to deal with different parts of the \nfeature representation separately. Then, the self-attention result of the i-th subspace is shown in formula (9),\nThe self-attention results of each head are spliced, and the matrix W dr is used for multi-space fusion. Finally, \nthe final result of mask multi-head self-attention is obtained, as shown in formula (10),\nFinally, the final context representation matrix M ∈ Rt×d is obtained by layer normalization. The multi-head \nself-attention mechanism assigns different weights to the feature vectors at different time steps. Therefore, in \nconclusion, the large weights are allocated to a few key feature vectors, while most irrelevant feature vectors can \nonly obtain a small amount of weight. This method effectively solves the problem of the equal contribution of \nfeature vectors of each time step and captures the long distance dependence and time dynamic correlation of \nfeature vectors of each time step.\nDecoding selection strategy and punishment mechanism. Beam search will abandon some unim-\nportant semantic information in the search process, greatly reducing space consumption and improving time \nefficiency, but beam search does not pay enough attention to sub important semantic information and may \nproduce repetitive, meaningless text that makes headlines inaccurate. TD-NHG model proposes a new decoding \nselection strategy, which utilizes top-k sampling and the top-p method to introduce the temperature parameter \n(t) in the softmax calculation process to change the vocabulary probability distribution, making it more biased \ntoward high probability words.\nwhere u represent logits, and t ∈[ 0, 1).\nCurrently, the entered sentence has a fixed size hidden state, TD-NHG model will generate the hidden state of \nthe t word based on the hidden state of the input sentence and the first to t-1 words ( x 1:i−1 ) generated previously. \nFinally, the vocabulary probability distribution ( P (x|x1:i−1 ) ) of the t word was obtained by softmax function.\nIn the process of model decoding, select the k tokens with the highest probability from P (x|x1:i−1 ) distribu-\ntion and sum their probabilities to get ∑P (x|x1:i−1 ) , where x ∈ V k . Vocabulary probability distribution of the \nt word is updated to p′(x|x1:i−1 ),\nFinally sample a token from the candidate set ( p′(x|x1:i−1 ) ) as an output token. However, the problem with \ntop-k sampling is that constant k is a given value in advance. For sentences with different lengths and contexts, \nthe model may sometimes require more or less tokens than k . TD-NHG model utilizes top-p sampling to pre -\nvent the model from falling into sample from tail distribution. TD-NHG model should ensure that vocabulary \nprobability distribution of the token after top-k sampling is greater than or equal to the baseline set by top-p \nsampling. Top-p sampling sets p’ as a pre-defined constant p′ ∈ (0, 1) , and top-p is set as 0.3 in this paper. Detailed \ncomparison of ablation experiments is shown in Figs. 4 and 5 of “Results” chapter.\nTo improve the quality of generating news headlines and control the problem of generating duplicate words, \nthe TD-NHG model uses the punishment mechanism. If word vi has been selected to learn above, the probabil-\nity of vi being selected again will be reduced. The vocabulary was traversed to punish the probability of words \nappearing in the vocabulary sequence.\nIn this paper, penalty factor repetition-penalty=1.2, the use of Punishment Mechanism greatly improves the \nprobability of the occurrence of secondary important words, reducing the probability of generating repeated \nwords in news headlines, thus improving the accuracy of generating news headlines.\n(9)Spacei = Attention\n(\nQW (Q i), KW (Ki), VW (V i)\n)\n.\n(10)MultiHead= Concat\n(\nSpace1 ,Space2 ,..., Spacei\n)\nW dr.\n(11)P (x|x1:i−1 ) = exp(ut/t)∑\nt′ exp(ut′/t)),\n(12)p′(x|x1:i−1 ) = p(x|x1:i−1 )∑P (x|x1:i−1 ) ,\n(13)P(vi) =\nm∑\ni=1\nP({ v1 ,v2 ,..., vi,..., vm }),\n(14)P (vi)′ = P (vi)/repetition-penalty.\n6\nVol:.(1234567890)Scientific Reports |        (2022) 12:11648  | https://doi.org/10.1038/s41598-022-15817-z\nwww.nature.com/scientificreports/\nDatasets and implementation details\nDatasets. This paper mainly solves the NHG problem, the TD-NHG model conducts comparative experi-\nments on two news data sets to evaluate the model we designed. The first dataset is the LCSTS  dataset17, which \nis created based on news summaries released by news media on microblogs. The total number of LCSTS data is \n2,400,591. In this paper, the LCSTS dataset is integrated to remove duplicate data, text content words less than \n100, and news headline words less than 2. After preprocessing, this paper obtains 1,331,209 experimental data, \nand uses the BertTokenizer to tokenize each utterance in the data. Randomly selected 3000 data as experimental \ntest set, the remaining 1,328,209 as the training validation set. After preprocessing, the data information is as \nfollows: the average number of words is 18, the standard deviation of words is 5, the maximum number of words \nis 30, and the minimum number is 4. The average number of words in the text is 104, the standard deviation is \n10, the maximum number is 152, and the minimum number is 69.\nThe second dataset is the Chinese Short Text Summary (CSTS)  dataset27, the original data is 670,000. CSTS \ndataset uses the same preprocessing method as LCSTS dataset. After preprocessing, the average number of words \nin news headlines is 20, the standard deviation of words is 6, the maximum number of words is 89, the mini-\nmum number is 4, an average number of words in news text is 125, standard deviation 31, maximum number \n1749, minimum number 98. 446,877 news items were selected as the training and validation sets, and 3500 were \nselected as the test dataset. The basic properties of the data set are shown in Table 1.\nImplementation details. The model was trained in Nvidia 3060 (12G), as illustrated in Table 2. TD-NHG \nmodel uses 12 layers of improved transformer-decoder layers. We adopted the  Adamw28 optimization algo-\nrithm, the Adamw optimizer’s ambiguity factor (epsilon) was set to 1e−8, the learning-rate was set to 1e−5, the \nwarmup probability was 0.1, and every 4000 steps of training were tested. The random seed was set to 2020. \nFigure 4.  The influence curve of top-k on Rouge ( top-p = 0.3).\nFigure 5.  The influence curve of top-p on Rouge ( top-k = 6).\nTable 1.  Dataset properties.\nDataset # News articles # News articles (after preprocessing) Avg. # tokens per article Avg. # tokens per headline\nLCSTS 2,400,591 1,331,209 104 18\nCSTS 670,000 450,377 125 20\n7\nVol.:(0123456789)Scientific Reports |        (2022) 12:11648  | https://doi.org/10.1038/s41598-022-15817-z\nwww.nature.com/scientificreports/\nFor the LCSTS dataset and CSTS dataset, the length of the news input by the model was limited to 512, and the \nmaximum length of the generated headline was 20. Mask multi-head self-attention has 12 heads, and a 12-layer \nimproved decoding module was used. When calculating the loss, we defined the CrossEntropyLoss loss func-\ntion, ignored the index of the calculated loss and the 0 loss in shift-labels, and only calculated the loss value for \nthe news headline section. During the training stage, batch-size was set to 16 with 10 epochs. In the decoding \nprocess, top-k and top-k are used as the decoding search method. TD-NHG model set top-k = 6, top-k = 0.3, the \nrepeated penalty rate was 1.2 and the vocabulary was 13,317.\nBaseline model. The baseline models used in this paper include:\n• RNN:  RNN17 is based on the seq2seq model and does not use technical methods such as attention mecha-\nnisms,\n• TextRanK:  TextRanK20 is a retrieval-based text generation method that focuses on the proportion of sentences \nbetween each news positive and reorders them to generate headlines,\n• ABS:  ABS13 used an attention mechanism model based on the traditional seq-2-seq model, which is a com-\nmon baseline model for generative text generation,\n• CopyNet:  CopyNet8 integrated the replication mechanism into the seq-2-seq model,\n• HG-News: HG-News29 also used the transformer-decoder layer structure, enriched the model input module, \nadded the personalized input module, and fused the pointer network in the model coding module,\n• LSTM + Point: LSTM +  Point14 Combined with pointer generator network in LSTM model. When generating \na summary, the model can extract words from the original text to make the summary more accurate,\n• LSTM + Point + Coverage: The coverage mechanism and pointer network are added based on the sep-2-sep \ngenerator model. The model effectively solves the out-of-vocabulary (OOV) 14 problem and the problem \nof generating duplicate words by the generator. It is a conventional baseline model for generative headline \ngenerators.\nEvaluation. Recall-oriented under study for gisting evaluation (Rouge) 30 is a set of important indicators \nused to evaluate machine translation and automatic text summarization. This paper compares the news head-\nlines generated by news headline generation with the news headlines written by human beings from the original \ntext and evaluates the news headlines based on the co-occurrence information of n-grams in the news text. \nRouge is an evaluation index for the recall rate of n-gram words. The quality of news headlines is evaluated by \ncounting the number of overlapping basic units (n-gram grammar, word sequence, and word pair) between the \ntwo. We take advantage of Rouge-N (including Rouge-1 and Rouge-2) and Rouge-L to score our model and \ncompare these scores with other models proposed in the past. Rouge-N is defined as follows,\nwhere n represents the length of the n-gram, ∑\nS∈{Ref}\n∑\ngram n ∈S Countmatch\n(\ngram n\n)\n represents \nthe sum of the number of n-grams in candidate news headlines and reference news headlines, and ∑\nS∈{Ref}\n∑\ngram n ∈S Count\n(\ngram n\n)\n represents the sum of the number of n-grams in reference news headlines. \nRouge-N is a calculation method based on the recall rate, so the denominator of its calculation is the number of \nall n-grams in the reference headline set. The calculation formulas of Rouge-1 and Rouge-2 are introduced below,\nThis paper adopted the Rouge-L index to calculate the longest common subsequence (LCS) between the two \ntest units of the generated news headlines and the reference news headlines of the designed model. The Rouge-L \nformula is as follows,\n(15)Rouge-N =\n∑\nS∈{Ref }\n∑\ngramn ∈S Countmatch\n(\ngramn\n)\n∑\nS∈{Re f}\n∑\ngramn ∈S Count\n(\ngramn\n) .\n(16)Rouge-1 =\n∑\nS∈{Ref }\n∑\n(1-gram )∈S Countmatch\n(\n1-gram\n)\n∑\nS∈{Re f}\n∑\n(1-gram )∈S Count\n(\n1-gram\n) ,\n(17)Rouge-2 =\n∑\nS∈{Ref }\n∑\n(2-gram )∈S Countmatch\n(\n2-gram\n)\n∑\nS∈{Re f}\n∑\n(2-gram )∈S Count\n(\n2-gram\n) .\nTable 2.  Experimental environment.\nExperimental environment Experimental configuration\nOperating system Ubuntu18.04\nProgramming language Python3.8\nDeep Learning Framework Pytorch1.8.1\nDisplay card model Nvidia 3060 and Nvidia 3080ti (12G)\n8\nVol:.(1234567890)Scientific Reports |        (2022) 12:11648  | https://doi.org/10.1038/s41598-022-15817-z\nwww.nature.com/scientificreports/\nAmong them, X stands for model generating news headlines, and Y  represents the original reference news \nheadlines. LCS(X, Y) denotes the longest common subsequence length of the generated summary and the ref -\nerence summary, m denotes the reference news length of the original text, n denotes the length of the model-\ngenerated summary, and β is the weight coefficient. R lcs and P lcs represent the recall rate and accuracy, respectively.\nResults\nThe experiment in this paper was carried out on the LCSTS dataset and Chinese Short Text Summary Dataset \n(CSTS). The LCSTS test set consists of 3000 news bodies and 3000 news headlines, and the CSTS dataset is \ncomposed of 3500 news bodies and 3500 news headlines. To make the experimental results more convincing, \nwe averaged the experimental results and took the average of 10 experimental results as the final experimental \ndata. We performed many comparative experiments and ablation experiments to verify the effectiveness of the \nproposed TD-NHG model in news headline generation in the LCSTS dataset and CSTS dataset.\nTables 3 and 4 show that, regardless of the LCSTS dataset and CSTS dataset, the TD-NHG model proposed \nin this paper has a significant improvement compared with the baseline model introduced above. Analyzing the \nnews headlines generated by the different models, for example, the “LSTM + Point + Coverage” model, which \npays more attention to learning the text information in the original news body and was introduced into the \nnewly generated news headlines through the pointer insertion form. The TD-NHG model learns the semantic \ninformation of the original text in the abstractive, focuses more on the readability and authenticity of the news \nheadlines, and summarizes the news document. In terms of Rouge-1 and Rouge-2, it is similar to “LSTM + Point \n+ Coverage” proposed by Hu et al.17, and slightly improved in Rouge-L, indicating that there is only still room \nfor the improvement in readability of news headlines generated by TD-NHG model.\n(18)ROUGE -L =\n(\n1 + β2 )\nRlcsPlcs\nRlcs+ β2 Plcs\n,\n(19)Rlcs= LCS(X , Y )\nm ,\n(20)Plcs= LCS (X , Y )\nn ,\nTable 3.  Comparison of different models on LCSTS dataset. Significant values are given in bold.\nMethods\nRouge-1 Rouge-2 Rouge-L\nP R F P R F P R F\nRNN 5.29 7.22 6.1 2.31 3.57 2.8 4.59 7.52 5.7\nRNN-context 9.86 11.95 10.81 6.45 8.41 7.3 9.56 12.21 10.72\nHG-news – – 22.79 – – 7.7 – – 21.36\nABS – – 28.15 – – 11.07 – – 25.35\nLSTM + point 28.66 29.51 29.08 13.81 15.74 14.71 26.77 29.01 27.85\nLSTM + point + coverage 30.35 32.87 31.56 11.64 13.88 12.66 27.05 29.09 28.03\nTD-NHG (with top-k and top-p and punishment) 30.18 32.46 31.28 11.65 13.92 12.68 27.32 29.38 28.31\nTable 4.  Comparison of different models on the CSTS dataset. Significant values are given in bold.\nMethods\nRouge-1 Rouge-2 Rouge-L\nP R F P R F P R F\nRNN 18.72 20.48 19.56 7.93 9.59 8.68 16.23 18.62 17.34\nABS – – 30.14 – – 14.07 – – 27.35\nTextRanK 31.22 33.51 32.32 14.58 15.51 15.03 25.66 26.81 26.22\nCopyNet 34.11 34.74 34.28 20.17 22.62 21.32 30.17 32.21 31.15\nLSTM + point 38.57 39.22 37.89 24.24 25.67 24.93 34.04 36.32 35.12\nLSTM + point + coverage 38.72 39.31 39.01 24.82 26.03 25.41 36.79 38.04 37.40\nTD-NHG (with greedy algorithm) 35.12 36.18 35.64 21.09 21.34 21.21 33.91 34.32 34.11\nTD-NHG (with beam search = 4) 37.06 35.63 36.33 22.84 20.92 21.84 35.71 34.15 34.91\nTD-NHG (with top-k and top-p, without punishment) 37.39 37.91 37.65 23.23 23.84 23.54 36.51 36.96 36.73\nTD-NHG (with top-k = 6, top-p = 0.3 and punishment = 1.2)38.79 38.68 38.73 24.79 25.15 24.97 36.79 38.18 37.47\n9\nVol.:(0123456789)Scientific Reports |        (2022) 12:11648  | https://doi.org/10.1038/s41598-022-15817-z\nwww.nature.com/scientificreports/\nWe performed ablation experiments on the LCSTS dataset for top-k and top-p on the TD-NHG model, as \nshown in Figs.  4 and 5 . The influence of decoding selection strategy with different top-k  and top-p  on news \nheadline generation is compared and analyzed under the same Punishment. Analyze whether the selection of \ntop-k and top-p will affect the attention of the headline generator to words, thereby affecting the accuracy and \nreadability of the news headline. We found that when top-k = 6 and cumulative probability top-p = 0.3 , the \nRouge index has the best effect, and headline generation has the best ability to describe the semantic features of \nnews text. On the LCSTS dataset, the Rouge-1, Rouge-2, and Rouge-L indexes reached 31.28, 12.68, and 28.31, \nrespectively. The Rouge index under different hyperparameters was significantly improved.\nThe third line of Table 4 proves that the decoding selection strategy of the punishment mechanism (repetition-\npenalty), top-k and top-p is effective in the task of headline generation, but there is no clear stipulation on how to \nselect top-k , top-p and repetition-penalty. Therefore, this paper conducts ablation experiments on the top index. \nAblation experiments show that in the news headline generation task, the same decoding selection strategy uses \ndifferent restrictive criteria to have a significant impact on headline generation. When the top-p index is constant \n(e.g. top-p = 0.3 ), the top-k index affects the Rouge index used in this paper, as shown in Fig.  4. A comparison \nshows that when top-k = 6 , the Rouge indexes reach the highest value, where Rouge-1 = 31.28, Rouge-2 = 12.68 \nand Rouge-L = 28.31. When top-k index is constant (e.g. top-k = 6 ), the change of the top-p index is shown in \nFig. 5. When top-p = 0.3 , Rouge reaches the maximum.\nWe regulated the setting of Punishment (repetition-penalty) of the TD-NHG model by ablation experiments, \nas shown in Fig. 6. In the case of constant top-k and top-p, the TD-NHG model performed ablation experiments \nin the LCSTS dataset by continuously adjusting the repetition-penalty index. The news headline generator is \nprone to OOV problems when selecting the probability distribution of the vocabulary. TD-NHG model uses \nPunishment (repetition-penalty) to punish those words that have been selected many times. Figure 6 shows that \nthe Rouge-1, Rouge-2 and Rouge-L indexes reach top at repetition-penalty = 1.2.\nDiscussion and case analysis\nTables 3 and 4 prove that our TD-NHG model has been further improved in the Rouge index, surpassing most \nexisting headline generation models. When batch-size=16 is set to train the LCSTS dataset, only approximately \n75H hours are needed for 10 periods. Compared with all baseline models, the time cost is greatly reduced, and \ncomputing power is effectively saved. As shown in Figs.  4, 5, and 6, we utilized ablation experiments to verify \nhow the decoding selection strategy in the TD-NHG model selected the hyperparameters accomplishments.\nTable 5 shows some examples of the LCSTS dataset. By comparison, it is found that the TD-NHG model fully \ndemonstrates the characteristics of abstractive headline generation. For example, the original news headline \nFigure 6.  The influence curve of repetition-penalty on Rouge ( top-k = 6 and top -p = 0.3).\nTable 5.  News headline generation examples on the TD-NHG model. Significant values are given in bold.\nOriginal news\n“Police comrade, this silly girl must send money to the liar, you hurry to help me \npersuade her… … ” On the 12th, a middle-aged woman led a young woman into \nthe police station. The young woman was in Taobao “a double twelfth day” online \nshopping received Telecomm uni-cations fraud text messages, for fear of ’accom-\nplishments’ to pay off, insisted on sending money to a “security account”\nOriginal headline Worrying about “a double twelfth day” online shopping women insist on remit-\ntance to cheaters\nTD-NHG: top-k = 6, top-p = 0.3 and punishment = 1.2 Women’s Online Shopping Cheating, Adhere to the remittance to the “security \naccount”\n10\nVol:.(1234567890)Scientific Reports |        (2022) 12:11648  | https://doi.org/10.1038/s41598-022-15817-z\nwww.nature.com/scientificreports/\n’Worrying about Online Shopping “a double twelfth day” Fighting out Women’s Intentional Remittance to Cheat-\ners’ , the TD-NHG model designed in this paper generates the headline in the form of the subject-predicate \nobject, “Women’s Online Shopping Cheated Remittance ‘security account’ ” , which ensures the connectivity and \nreadability of the news headline, and does not lack the necessary feature semantic information in the headline. \nHowever, because the abstractive model translates some learned semantic information, for the entity words that \nare not in the vocabulary, such as the original news headline in Table 7, “follow Uncle Xi to travel greatly” , where \nUncle Xi is greatly a specific entity noun, there is no clear definition in the vocabulary. The poor learning effect \nof the model leads to a rough score lower than the average value, which greatly affects the overall rough score. \nFor Tables 6, 7, a news headline containing specific entity nouns (including names, place names, etc.), which \ndoes not exist in the model vocabulary, we consider adding personalized input to the model input module or \nintroducing external knowledge into the decoding to tackle this problem in future work.\nConclusion\nIn this paper, we proposed a novel news headline generation TD-NHG, which abandons the encoder-decoder \nstructure used by the transformer and only utilizes 12 layers of improved transformer-decoder layers as the \ncoding module. To learn the speech information and semantic features in the input token more accurately and \nquickly, the TD-NHG model adopted a masked multi-head self-attention mechanism and layer normalization \nlayer in the coding module to obtain the attention distribution of the input token more accurately. In the TD-\nNHG model, we introduce different decoding selection strategies, including top-k, top-p, and the punishment \nmechanism (repetition-penalty), to select the words of news headlines. Experiments on the LCSTS dataset and \nCSTS dataset show that the TD-NHG proposed in this paper has achieved comparable results. In future work, \nwe will consider solving the problem of out-of-vocabulary in news headline generation and the issue of inaccu-\nrate wording in the model when generating news headlines, thereby improving the semantic feature description \nability and abstraction ability of the news headline generation.\nData availability\nThe datasets generated during or analysed during the current study are available from the corresponding author \non reasonable request.\nReceived: 6 January 2022; Accepted: 29 June 2022\nReferences\n 1. Murao, K. et al. A case study on neural headline generation for editing support. In 2019 Conference of the North American Chapter \nof the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2019, June 2, 2019–June 7, 2019.  \n73–82 (Association for Computational Linguistics (ACL)).\nTable 6.  News headline generation ep.1 using different TD-NHG parameters. Significant values are given in \nbold.\nOriginal news\nHaving a home of its own has always been an important part of the American \ndream. Nevertheless, the proportion of adults with housing has been declining. \nAccording to commercial insiders, US housing ownership has been falling over \nthe past few years, while rents have been rising and vacancy rates have been fall-\ning, suggesting a shift from buying to rent\nOriginal headline Can’t afford a house The American dream is dying\nTD-NHG: top-k = 6,top-p = 0.3 and punishment = 1.2 The truth of the American Dream: Buying a house is transformed into renting \na house\nTD-NHG: top-k = 2, top-p = 0.3 and punishment = 1.2 The Truth of American Dream: The Change of Housing\nTD-NHG: top-k = 6, top-p = 0.95 and punishment = 1.2 The “American Dream” of the middle class buying houses\nTable 7.  News headline generation ep. 2 using different TD-NHG parameters. Significant values are given in \nbold.\nOriginal news\nSri Lanka 7 days free travel, 4557 yuan (Shanghai departure)! Known as \n\"Tears on the Indian Ocean,\" Sri Lanka has beauteous beaches, a thousand-\nyear-old ancient city, a Dutch castle, and rich tropical flora and fauna. Here, \ndrinking a glass of authentic black tea, taking a water train ride, watching \nstilt fishermen fishing leisurely, going to the Lion Rock, and exploring the \nlost palace\nOriginal headline Follow Uncle Xi to travel greatly\nTD-NHG: top-k = 6, top-p = 0.3 and repetition-penalty = 1.2 Sri Lanka-free travel\nTD-NHG: top-k = 2, top-p = 0.3 and repetition-penalty = 1.2 Paradise crossing the train\nTD-NHG: top-k = 6, top-p = 0.95 and repetition-penalty = 1.2 Traveling through Sri Lanka\n11\nVol.:(0123456789)Scientific Reports |        (2022) 12:11648  | https://doi.org/10.1038/s41598-022-15817-z\nwww.nature.com/scientificreports/\n 2. Song, Y .-Z. et al. Attractive or faithful? Popularity-reinforced learning for inspired headline generation. In 34th AAAI Conference \non Artificial Intelligence, AAAI 2020, February 7, 2020–February 12, 2020. 8910–8917 (AAAI press).\n 3. Gu, X. et al. Generating representative headlines for news stories. In 29th International World Wide Web Conference, WWW 2020, \nApril 20, 2020–April 24, 2020. 1773–1784 (Association for Computing Machinery).\n 4. Gavrilov, D., Kalaidin, P . & Malykh, V . Self-attentive model for headline generation. In 41st European Conference on Information \nRetrieval, ECIR 2019, April 14, 2019–April 18, 2019. 87–93 (Springer Verlag).\n 5. Xu, P ., Wu, C.-S., Madotto, A. & Fung, P . Clickbait? Sensational headline generation with auto-tuned reinforcement learning. In \n2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language \nProcessing, EMNLP-IJCNLP 2019, November 3, 2019–November 7, 2019. 3065–3075 (Association for Computational Linguistics).\n 6. Ao, X. et al. PENS: A dataset and generic framework for personalized news headline generation. In Joint Conference of the 59th \nAnnual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language \nProcessing, ACL-IJCNLP 2021, August 1, 2021–August 6, 2021. 82–92 (Association for Computational Linguistics (ACL)).\n 7. Pavlopoulou, N. & Curry, E. IoTSAX: A dynamic abstractive entity summarization approach with approximation and embedding-\nbased reasoning rules in publish/subscribe systems. IEEE Internet Things J. 9, 1830–1847. https:// doi. org/ 10. 1109/ JIOT. 2021. 30899 \n31 (2022).\n 8. Gu, J., Lu, Z., Li, H. & Li, V . O. K. Incorporating copying mechanism in sequence-to-sequence learning. In 54th Annual Meeting \nof the Association for Computational Linguistics, ACL 2016, August 7, 2016–August 12, 2016. 1631–1640 (Association for Compu-\ntational Linguistics (ACL)).\n 9. Amin, R., Sworna, N. S., Liton, M. N. K. & Hossain, N. Abstractive headline generation from Bangla news articles using Seq2Seq \nRNNs with global attention. In 2021 International Conference on Science and Contemporary Technologies, ICSCT 2021, August 5, \n2021–August 7, 2021. (Institute of Electrical and Electronics Engineers Inc.).\n 10. Zhou, Q., Y ang, N., Wei, F . & Zhou, M. Selective encoding for abstractive sentence summarization. In 55th Annual Meeting of the \nAssociation for Computational Linguistics, ACL 2017, July 30, 2017–August 4, 2017. 1095–1104 (Association for Computational \nLinguistics (ACL)).\n 11. Cui, M., Wang, Y ., Lin, X. & Zhong, M. Fault diagnosis of rolling bearings based on an improved stack autoencoder and support \nvector machine. IEEE Sens. J. 21, 4927–4937. https:// doi. org/ 10. 1109/ JSEN. 2020. 30309 10 (2021).\n 12. Cao, Z., Li, W ., Wei, F . & Li, S. Retrieve, rerank and rewrite: Soft template based neural summarization. In 56th Annual Meeting \nof the Association for Computational Linguistics, ACL 2018, July 15, 2018–July 20, 2018. 152–161 (Association for Computational \nLinguistics (ACL)).\n 13. Nallapati, R., Zhou, B., dos Santos, C., Gulcehre, C. & Xiang, B. Abstractive text summarization using sequence-to-sequence RNNs \nand beyond. In 20th SIGNLL Conference on Computational Natural Language Learning, CoNLL 2016, August 11, 2016–August 12, \n2016. 280–290 (Association for Computational Linguistics (ACL)).\n 14. See, A., Liu, P . J. & Manning, C. D. Get to the point: summarization with pointer-generator networks. In 55th Annual Meeting of \nthe Association for Computational Linguistics, ACL 2017, July 30, 2017–August 4, 2017. 1073–1083 (Association for Computational \nLinguistics (ACL)).\n 15. Paulus, R., Xiong, C. & Socher, R. A deep reinforced model for abstractive summarization. In 6th International Conference on \nLearning Representations, ICLR 2018, April 30, 2018–May 3, 2018. (International Conference on Learning Representations, ICLR).\n 16. Chen, W ., Chan, H. P ., Li, P . & King, I. Exclusive hierarchical decoding for deep keyphrase generation. In 58th Annual Meeting of \nthe Association for Computational Linguistics, ACL 2020, July 5, 2020–July 10, 2020. 1095–1105 (Association for Computational \nLinguistics (ACL)).\n 17. Hu, B., Chen, Q. & Zhu, F . LCSTS: a large scale Chinese short text summarization dataset. In Conference on Empirical Methods in \nNatural Language Processing, EMNLP 2015, September 17, 2015–September 21, 2015. 1967–1972 (Association for Computational \nLinguistics (ACL)).\n 18. Nambiar, S. K., Peter S, D. & Idicula, S. M. Attention based abstractive summarization of Malayalam document. In 5th International \nConference on Artificial Intelligence in Computational Linguistics, ACLing 2021, June 4, 2021–June 5, 2021. 250–257 (Elsevier B.V .).\n 19. Chan, H. P . & King, I. A condense-then-select strategy for text summarization. Knowl.-Based Syst. https:// doi. org/ 10. 1016/j. knosys. \n2021. 107235 (2021).\n 20. Mihalcea, R. & Tarau, P . TextRank: bringing order into texts. In 2004 Conference on Empirical Methods in Natural Language Pro-\ncessing, EMNLP 2004, July 25, 2004–July 26, 2004. 404–411 (Association for Computational Linguistics (ACL)).\n 21. Zhao, S., Deng, E., Liao, M., Liu, W . & Mao, W . Generating summary using sequence to sequence model. In 5th IEEE Information \nTechnology and Mechatronics Engineering Conference, ITOEC 2020, June 12, 2020–June 14, 2020. 1102–1106 (Institute of Electrical \nand Electronics Engineers Inc.).\n 22. Raffel, C. et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res. 21, 1–67 (2020).\n 23. Zou, Y ., Zhang, X., Lu, W ., Wei, F . & Zhou, M. Pre-training for abstractive document summarization by reinstating source text. \nIn 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, November 16, 2020 - November 20, 2020. \n3646–3660 (Association for Computational Linguistics (ACL)).\n 24. Lewis, M. et al. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehen-\nsion. In 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, July 5, 2020–July 10, 2020. 7871–7880 \n(Association for Computational Linguistics (ACL)).\n 25. Wang, Z., Xu, L., Liu, Z. & Sun, M. Topic-sensitive neural headline generation. Sci. China Inf. Sci. 63, 1–6. https:// doi. org/ 10. 1007/ \ns11432- 019- 2657-8 (2020).\n 26. Memisevic, R., Zach, C., Hinton, G. & Pollefeys, M. Gated softmax classification. In 24th Annual Conference on Neural Information \nProcessing Systems 2010, NIPS 2010, December 6, 2010–December 9, 2010. Neural Information Processing Systems (NIPS) (Curran \nAssociates Inc.).\n 27. He, Z. F . Chinese Short Text Summary Dataset. https:// drive. google. com/ file/d/ 1WRfz fr8ah 6InHh Ul6QP 7Fsad dCnaR 8p0/ view? \nusp= shari ng (2018).\n 28. Kingma, D. P . & Ba, J. L. Adam: a method for stochastic optimization. In 3rd International Conference on Learning Representations, \nICLR 2015, May 7, 2015–May 9, 2015. (International Conference on Learning Representations, ICLR).\n 29. Li, P ., Yu, J., Chen, J. & Guo, B. HG-news: News headline generation based on a generative pre-training model. IEEE Access 9, \n110039–110046. https:// doi. org/ 10. 1109/ ACCESS. 2021. 31027 41 (2021).\n 30. Lin, C. Y . ROUGE: a package for automatic evaluation of summaries. In In Proceedings of the Workshop on Text Summarization \nBranches Out (WAS 2004).\nAuthor contributions\nZ.L. conceived the experiment(s), J.W . and Z.L. conducted the experiment(s), X.Y ., Z.L. and J.M. analysed the \nresults. All authors reviewed the manuscript.\nCompeting interests \nThe authors declare no competing interests.\n12\nVol:.(1234567890)Scientific Reports |        (2022) 12:11648  | https://doi.org/10.1038/s41598-022-15817-z\nwww.nature.com/scientificreports/\nAdditional information\nCorrespondence and requests for materials should be addressed to J.W .\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2022"
}