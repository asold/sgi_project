{
  "title": "Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models",
  "url": "https://openalex.org/W4390572997",
  "year": 2024,
  "authors": [
    {
      "id": null,
      "name": "Dahl, Matthew",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Magesh, Varun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224776923",
      "name": "Suzgun, Mirac",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224999769",
      "name": "Ho, Daniel E.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4378771713",
    "https://openalex.org/W4380370169",
    "https://openalex.org/W4391212860",
    "https://openalex.org/W4221142858",
    "https://openalex.org/W4281679115",
    "https://openalex.org/W3103751997",
    "https://openalex.org/W2164777277",
    "https://openalex.org/W4283810944",
    "https://openalex.org/W3007378990",
    "https://openalex.org/W4386942462",
    "https://openalex.org/W3215701899",
    "https://openalex.org/W3132748670",
    "https://openalex.org/W2053154970",
    "https://openalex.org/W4386556635",
    "https://openalex.org/W4378942171",
    "https://openalex.org/W4221159672",
    "https://openalex.org/W2099183456",
    "https://openalex.org/W4387221840",
    "https://openalex.org/W4372272503",
    "https://openalex.org/W4306808680",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4317898419",
    "https://openalex.org/W4387559042",
    "https://openalex.org/W3157374291",
    "https://openalex.org/W4318577095",
    "https://openalex.org/W4386086020",
    "https://openalex.org/W4388787841",
    "https://openalex.org/W4327519588",
    "https://openalex.org/W4375958655",
    "https://openalex.org/W2906462581",
    "https://openalex.org/W4385327559",
    "https://openalex.org/W4386827556",
    "https://openalex.org/W4379928343",
    "https://openalex.org/W4389073290",
    "https://openalex.org/W4389984066",
    "https://openalex.org/W4391017628",
    "https://openalex.org/W4379473327",
    "https://openalex.org/W4376867729",
    "https://openalex.org/W4400926486",
    "https://openalex.org/W4366328015",
    "https://openalex.org/W4382618722",
    "https://openalex.org/W4376874793",
    "https://openalex.org/W4308539010",
    "https://openalex.org/W4389519928",
    "https://openalex.org/W4385714610",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W3155807546",
    "https://openalex.org/W4388788658",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W1819662813",
    "https://openalex.org/W2626967530",
    "https://openalex.org/W4320009668",
    "https://openalex.org/W4388717471",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4391158659",
    "https://openalex.org/W4380558781",
    "https://openalex.org/W3034383590",
    "https://openalex.org/W4378473839",
    "https://openalex.org/W3016154458",
    "https://openalex.org/W4330336443",
    "https://openalex.org/W4385884015",
    "https://openalex.org/W4387596421",
    "https://openalex.org/W2974098375",
    "https://openalex.org/W4385570777",
    "https://openalex.org/W4381247817",
    "https://openalex.org/W4377121468",
    "https://openalex.org/W4378509270",
    "https://openalex.org/W4387892136",
    "https://openalex.org/W4388778348",
    "https://openalex.org/W4386501849",
    "https://openalex.org/W4317668889",
    "https://openalex.org/W3046228735",
    "https://openalex.org/W4378513156",
    "https://openalex.org/W4391590636",
    "https://openalex.org/W4285429195",
    "https://openalex.org/W4377866048",
    "https://openalex.org/W4389403907",
    "https://openalex.org/W4385825540",
    "https://openalex.org/W4389523840",
    "https://openalex.org/W3164854573",
    "https://openalex.org/W2913765499",
    "https://openalex.org/W3204410951",
    "https://openalex.org/W4327810286",
    "https://openalex.org/W3194879917",
    "https://openalex.org/W4385832251",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W4292157289",
    "https://openalex.org/W4254690863",
    "https://openalex.org/W4391940796",
    "https://openalex.org/W4381797997",
    "https://openalex.org/W2768957049",
    "https://openalex.org/W4391505486",
    "https://openalex.org/W4378501037",
    "https://openalex.org/W4307123345",
    "https://openalex.org/W2075623515",
    "https://openalex.org/W4386555477",
    "https://openalex.org/W4377372007",
    "https://openalex.org/W4367365526",
    "https://openalex.org/W4390602555",
    "https://openalex.org/W3169841173",
    "https://openalex.org/W4322718421",
    "https://openalex.org/W4386504863",
    "https://openalex.org/W4388748510"
  ],
  "abstract": "Do large language models (LLMs) know the law? These models are increasingly being used to augment legal practice, education, and research, yet their revolutionary potential is threatened by the presence of hallucinations -- textual output that is not consistent with legal facts. We present the first systematic evidence of these hallucinations, documenting LLMs' varying performance across jurisdictions, courts, time periods, and cases. Our work makes four key contributions. First, we develop a typology of legal hallucinations, providing a conceptual framework for future research in this area. Second, we find that legal hallucinations are alarmingly prevalent, occurring between 58% of the time with ChatGPT 4 and 88% with Llama 2, when these models are asked specific, verifiable questions about random federal court cases. Third, we illustrate that LLMs often fail to correct a user's incorrect legal assumptions in a contra-factual question setup. Fourth, we provide evidence that LLMs cannot always predict, or do not always know, when they are producing legal hallucinations. Taken together, our findings caution against the rapid and unsupervised integration of popular LLMs into legal tasks. Even experienced lawyers must remain wary of legal hallucinations, and the risks are highest for those who stand to benefit from LLMs the most -- pro se litigants or those without access to traditional legal resources.",
  "full_text": "Large Legal Fictions:\nProfiling Legal Hallucinations in Large Language\nModels*\nMatthew Dahl,† Varun Magesh,‡ Mirac Suzgun,§ Daniel E. Ho¶\nApril 25, 2024\nJournal of Legal Analysis (forthcoming)\n*We thank Neel Guha, Sandy Handan-Nader, Adam T. Kalai, Peter Maldonado, Chris Manning, Joel Niklaus,\nKit Rodolfa, Faiz Surani, Andrea Vallebueno, and three anonymous reviewers for helpful discussions and feedback.\nOur replication materials are available in the Harvard Dataverse (https://doi.org/10.7910/DVN/V4ON8H) and our data\nfor future benchmarking is available on HuggingFace (https://huggingface.co/datasets/reglab/legal_hallucinations).\nAddress correspondence to Matthew Dahl, 125 Prospect St., New Haven, CT 06511; email: matthew.dahl@yale.edu.\n†J.D. Candidate, Yale Law School, and Ph.D. Student, Yale University Department of Political Science.\n‡Research Fellow, Regulation, Evaluation, and Governance Lab, Stanford University.\n§J.D. Candidate, Stanford Law School, and Ph.D. Candidate, Stanford University Department of Computer Sci-\nence.\n¶William Benjamin Scott and Luna M. Scott Professor of Law, Professor of Political Science, Senior Fellow at\nStanford Institute for Economic Policy Research, and Faculty Director of the Regulation, Evaluation, and Governance\nLab, Stanford University.\n1\narXiv:2401.01301v2  [cs.CL]  21 Jun 2024\nAbstract\nDo large language models (LLMs) know the law? LLMs are increasingly being used to aug-\nment legal practice, education, and research, yet their revolutionary potential is threatened by\nthe presence of “hallucinations”—textual output that is not consistent with legal facts. We\npresent the first systematic evidence of these hallucinations in public-facing LLMs, document-\ning trends across jurisdictions, courts, time periods, and cases. Using OpenAI’s ChatGPT 4 and\nother public models, we show that LLMs hallucinate at least 58% of the time, struggle to pre-\ndict their own hallucinations, and often uncritically accept users’ incorrect legal assumptions.\nWe conclude by cautioning against the rapid and unsupervised integration of popular LLMs\ninto legal tasks, and we develop a typology of legal hallucinations to guide future research in\nthis area.\n2\nGPT 4 GPT 3.5 PaLM 2 Llama 2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMean Hallucination Rate\n0.58\n0.69 0.72\n0.88\nFigure 1: Hallucination rates by LLM, all reference-based tasks pooled. Hallucinations are common across\nall LLMs when they are asked a direct, verifiable question about a federal court case, but GPT 4 performs\nbest overall.\n1 Introduction\nHow well do large language models (LLMs) know American case law? Modern LLMs such\nas OpenAI’s ChatGPT—tools trained on vast amounts of textual data to predict the next token\nin a sequence—are driving a transformation in the legal world, from legal education (Choi and\nSchwarcz 2024), to legal research (Livermore, Herron, and Rockmore 2024), to legal practice it-\nself (Rodgers, Armour, and Sako 2023). Indeed, recent versions of these artificial intelligence (AI)\nmodels seem to excel at law-related tasks, such as first-year law school exams (Choi et al. 2022),\nthe uniform bar exam (Katz et al. 2023), statutory reasoning (Blair-Stanek, Holzenberger, and\nVan Durme 2023), and issue-rule-application-conclusion (IRAC) analysis (Guha et al. 2023). But\ndespite the revolutionary potential of these models, a key challenge remains: the issue of “halluci-\nnations.” LLMs are liable to generate language that is inconsistent with current legal doctrine and\ncase law, and, in the legal field, where adherence to authorities is paramount, unfaithful or impre-\ncise interpretations of the law can lead to nonsensical—or worse, harmful and inaccurate—legal\nadvice or decisions.\n3\nIn this work, we present the first evidence documenting the nature, frequency, and correlates of\nthese hallucinations. In doing so, we shed systematic, empirical light on a phenomenon that has so\nfar only received anecdotal treatment in the literature. For example, much media attention has been\ndirected toward a Manhattan lawyer who faced sanctions for using ChatGPT to generate fictional\ncase citations for a brief (Weiser 2023), or another instance where ChatGPT produced a supposed\ndissent authored by Justice Ruth Bader Ginsburg in the landmark gay rights case Obergefell v.\nHodges (Romoser 2023). Even Chief Justice John Roberts, the chief justice of the U.S. Supreme\nCourt, has weighed in on the problem, highlighting hallucinations in his 2023 report on the state of\nthe federal judiciary and arguing that, as yet, “machines cannot fully replace key actors in court”\n(Roberts 2023, 6).\nThese impressionistic accounts, however, leave unanswered the deeper questions that legal\nscholars must confront as LLMs continue to grow in popularity. How much legal knowledge is\nactually embedded in an off-the-shelf LLM? Are LLMs equally familiar with different dimensions\nof the American common law system—where legal doctrine varies across courts, jurisdictions,\nand over time—or do they tend to hallucinate more in certain areas than others? When LLMs\ndo hallucinate, do they disproportionately produce false information favoring certain judges or\ncases? And besides hallucination itself, are there other features of LLMs that legal scholars need\nto consider—other latent biases or behavioral tendencies that threaten to spill over into downstream\napplications of these models? Our study seeks to answer these questions, providing insights that\nare essential for evaluating LLMs’ effectiveness in general legal settings.\nThis research contributes to several literatures. First, there has recently been an explosion of\ninterest in the intersection of law and technology, with a particular focus on the emergence of AI.\nMuch of this work focuses on how lawmakers and administrative agencies ought to govern the\ndeployment of these tools (Engstrom and Ho 2020; Engstrom et al. 2020; Solow-Niederman\n2020), given that they are already being used by public (Engel and Grgi´c-Hlaˇca 2021) and private\n(Barocas and Selbst 2016) actors alike, producing novel privacy concerns (Ben-Shahar 2023; King\net al. 2023) and giving rise to new forms of liability (Henderson, Hashimoto, and Lemley 2023;\n4\nLemley and Casey 2019; V olokh 2023). As one highly influential but still maturing species of AI,\nLLMs stand in need of a concrete empirical evaluation of their legal abilities and their legal risks,\nof which hallucination is certainly one. We supply that information here.\nWe also contribute to a growing literature regarding the implications of AI for access to jus-\ntice. Many members of the legal community rightly regard LLMs as a promising solution to the\nlongstanding barriers to adequate legal representation that millions of pro se and under-resourced\nlitigants encounter (Chien et al. 2024; Perlman 2023; Tan, Westermann, and Benyekhlef 2023).\nBecause they are relatively cheap, easy, and quick to use, LLMs might finally be able to deliver\non the federal rules’ guarantee of a “just, speedy, and inexpensive” resolution of disputes (Fed. R.\nCiv. P. 1; Roberts 2023). This potential can only be realized, however, if LLMs actually know\nthe law. Additionally, if the legal knowledge embedded in LLMs is not evenly distributed, the\nwidespread adoption of LLMs might unintentionally worsen rather than eliminate current dispari-\nties in the availability of legal services (Draper and Gillibrand 2023; Simshaw 2022). We therefore\napproach our study of LLMs with an eye toward assessing their ability to truly close the justice\ngap, examining both their raw hallucination rates as well as any other emergent behaviors that\nthreaten this potential.\nFinally, we also contribute to the pressing algorithmic harm literature, which is motivated by\nthe concern that inscrutable algorithms often produce predictions, recommendations, or outputs\nthat are not fairly distributed among individuals or groups (Bar-Gill, Sunstein, and Talgam-Cohen\n2023; Gillis and Spiess 2019; Kleinberg et al. 2018; Mayson 2019). In our legal setting,\nthe specific danger is that if LLMs do not properly internalize knowledge about some dimension\nof the law—if LLMs know California law better than Wyoming law, for example, or decisions\nby Justice Ketanji Brown Jackson worse than decisions by Justice Antonin Scalia, for another—\nthey will regurgitate a falsely homogeneous sense of the legal landscape to their users, collapsing\nimportant legal nuances and perpetuating representational harms. Worse, because LLMs are so-\ncalled “foundation” models, their distributional biases, if they exist, may permeate and afflictevery\ndownstream version of these models (Bommasani et al. 2022), producing a kind of algorithmic\n5\n“monoculture” by entrenching one particular notion of the law across a wide range of applications\n(Creel and Hellman 2022; Kleinberg and Raghavan 2021). Accordingly, it is important for legal\nscholars to obtain a sense of what the correlates of LLMs’ hallucinations are, in order to address\nthis new and profound opportunity for cascading algorithmic harms.\nOur article proceeds as follows. In Section 2, we provide a brief background on LLMs for\nthe non-technical reader and theorize a typology of legal hallucinations. In Section 3, we de-\nvelop a set of legal knowledge queries that we use to elicit an LLM’s understanding of the law,\nfrom simple queries like whether or not a case exists to more complex queries like asking for a\nstatement of a case’s holding or its precedential relationship to another case. In Section 4, we\ndescribe our methodological approach, which entails asking these queries for a random sample of\ncases across each level of the federal judiciary—the U.S. District Courts (USDC), the U.S. Courts\nof Appeals (USCOA), and the U.S. Supreme Court (SCOTUS)—and evaluating them using four\npopular LLMs: OpenAI’s ChatGPT 4, OpenAI’s ChatGPT 3.5, Google’s PaLM 2, and Meta’s\nLlama 2.\nIn Section 5, we present our results. Our findings reveal the widespread occurrence of legal\nhallucinations: when asked a direct, verifiable question about a randomly selected federal court\ncase, LLMs hallucinate between 58% (ChatGPT 4) and 88% (Llama 2) of the time. However, we\nalso find that LLMs perform better on cases that are newer, more salient, and from more prominent\nlegal jurisdictions, suggesting that the risks of legal monoculture are real. We then investigate\ntwo additional potential failure points for LLMs, beyond their raw hallucination rates: (1) their\nsusceptibility to contra-factual bias, i.e., their ability to respond to queries anchored in erroneous\nlegal premises (Sharma et al. 2023; Wei et al. 2023), and (2) their certainty in their responses,\ni.e., their self-awareness of their propensity to hallucinate (Azaria and Mitchell 2023; Kadavath\net al. 2022; Tian, Mitchell, Zhou, et al. 2023; Xiong et al. 2023; Yin et al. 2023). Our results\nindicate that not only do LLMs often provide seemingly legitimate but incorrect answers to contra-\nfactual legal questions, they also struggle to accurately gauge their own level of certainty without\npost-hoc recalibration. Accordingly, in Section 6 we conclude that while LLMs appear to offer\n6\na way to make legal information and services more accessible and affordable to all, their present\nshortcomings—particularly in terms of generating accurate and reliable statements of the law—\nsignificantly hinder this objective.\n2 Background and Theory\n2.1 What Is a Language Model?\nWe first provide a brief overview of language models (LMs) for readers who may not necessar-\nily have a deep technical background. LMs can be viewed as functions that map text to text: When\na user provides a text input (known as a “prompt”), the model produces a text output (referred\nto as a “response”). If the prompt takes the form of a question, the response can be understood\nas an answer to that question. An LM generates its response by selecting the most probable se-\nquence of tokens that follow the prompt’s tokens; therefore, it essentially functions as a probability\ndistribution over these tokens.\nIn this work, we focus onlarge language models (LLMs). The largeness of a language model is\na dual reference to its parameter count and the scope of its training corpus: LLMs are models that\ncontain billions of parameters and are trained on vast corpora bordering on the size of the Internet.\nBecause of their incredible size, LLMs can be considered general purpose technologies, with the\napparent ability to understand and generate human-like text across a wide range of topics, including\nmedicine, finance, education, retail, and law (Eloundou et al. 2023). In contrast to previous forms\nof machine learning, however, they seem to excel at these tasks despite not being explicitly trained\nto perform them (Brown et al. 2020); the “jagged frontier” of their emergent abilities is still being\nmapped (Dell’Acqua et al. 2023).\nWe also set forth a more formal definition of an LLM, in order to provide the foundation for\nthe typology of legal hallucinations that we develop in the next subsection. We let an LLM be a\nfunction fτ : prompt 7→ response, where fτ operates by sampling responses from a conditional\nprobability distribution that is learned by optimizing over a training corpus hopefully reflective\n7\nof facts about the world. 1 We use the symbol τ to designate a user-configurable “temperature”\nparameter that controls the shape of the probability distribution at inference time. When τ =\n0, the distribution becomes degenerate and the model’s response is theoretically deterministic—\nthe model must always return the most likely response. 2 Following convention, we refer to this\ndeterministic response as the model’s “greedy” response. As τ increases, the distribution becomes\nmore uniform and the model’s response becomes more stochastic—the model is free to choose\nfrom a variety of candidate responses, all of which become more equally likely to be chosen the\nhigher the temperature is. Thus, increasing the temperature of an LLM is one way to potentially\nincrease its hallucination frequency (Lee 2023). In this article, however, we generally perform our\nexperiments at τ = 0, showing that LLMs hallucinate even under the most conservative sampling\nconditions.\n2.2 The Nature of Legal Hallucinations\nLLMs are showing promise on a number of legal research and analysis tasks (Ash et al. 2024;\nBlair-Stanek, Holzenberger, and Van Durme 2023; Choi et al. 2022; Fei et al. 2023; Guha\net al. 2023; Katz et al. 2023; Trozze, Davies, and Kleinberg 2023), but the problem of legal\nhallucination has so far only been studied in closed-domain applications, such as when a model is\nused to summarize the content of a given judicial opinion (Deroy, Ghosh, and Ghosh 2023; Feijo\nand Moreira 2023) or to synthesize provided legal text (Savelka et al. 2023). In this article, by\ncontrast, we examine hallucination in an open-domain setting, i.e., when a model is tasked with\nproviding an accurate answer to an open-ended legal query. This setting approximates the situation\n1 In reality, language generation in LMs actually happens at the level of tokens, not responses\nthemselves; full model responses are constructed autoregressively by sampling n tokens, one at a\ntime, from a distribution Pr[xn|x1, . . . , xn−1]. We abstract from these details in this article without\nloss of generality.\n2 Non-determinism may persist in practice due to a model’s implementation details, e.g., the\n“mixture of experts” architecture (Chann 2023).\n8\nTable 1: Typology of legal hallucinations\nDomain Type of hallucination Legal example\nClosed response inconsistency with the prompt Mischaracterization of an opinion\nresponse inconsistency with the training corpus Creative argumentation\nOpen\nresponse inconsistency with the facts of the world Misstatement of the law\nof a lawyer or a pro se litigant seeking advice from a legal chat interface.\nIn the context of such question-answering (QA) scenarios, the study of hallucinations in LMs\nis still in its infancy, even outside the legal field. There is no universally accepted definition or\nclassification of LM hallucinations (Ji, Lee, et al. 2023; van Deemter 2024; Y . Zhang et al. 2023).\nHowever, as Kalai and Vempala (2023) show, LMs that assign a positive probability to every re-\nsponse token must hallucinate at least some of the time. Xu, Jain, and Kankanhalli (2024) agree,\narguing that “hallucination is inevitable for any computable LLM, regardless of model architec-\nture, learning algorithms, prompting techniques, or training data.” Therefore, if hallucinations are\nhere to stay, we believe that it is essential for legal scholars to begin to recognize that there are\nseveral different ways in which an LLM can generate false information, as not all modes of hal-\nlucination are equally concerning for legal professionals. For example, since hallucinations seem\nlikely to give rise to new forms of tort liability (Henderson, Hashimoto, and Lemley 2023), it will\nbe important to differentiate between different types of hallucinations in order to properly assess\nthe predicate elements of such torts. We supply those theoretical resources here, summarizing our\ntypology of legal hallucinations in Table 1.\nFirst, a model might hallucinate by producing a response that is either unfaithful to or in con-\nflict with the input prompt, a phenomenon canonically referred to as closed-domain or intrinsic\nhallucination. This is a major concern in tasks requiring a high degree of accuracy between the re-\nsponse and a long-form input, such as machine translation (Xu et al. 2023) or summarization (Cao\net al. 2018). In legal contexts, such inaccuracies would be particularly problematic in activities like\nsummarizing judicial opinions, synthesizing client intake information, drafting legal documents,\nor extracting key points from an opposing counsel’s brief.\n9\nSecond, an LLM might also hallucinate by producing a response that either contradicts or does\nnot directly derive from its training corpus. Following Agrawal et al. (2023), we conceptualize\nthis kind of hallucination as one form of open-domain or extrinsic hallucination. In general, the\noutput of a language model should be logically derivable from the content of its training corpus,\nregardless of whether the content of the corpus is factually or objectively true. 3 In legal settings,\nthis kind of hallucination poses a special challenge to those aiming to fine-tune the kind of general\npurpose foundation models that we study in this article with proprietary, in-house work product. 4\nFor example, firms might have a catalogue of internal research memos, style guides, and so forth,\nthat they want to ensure is reflected in their bespoke LLM’s output. At the same time, however, in-\nsofar as creativity is valued, certain legal tasks—such as persuasive argumentation—might actually\nbenefit from some lack of strict fidelity to the training corpus; after all, a model that simply parrots\nexactly the text that it has been trained on could itself be undesirable. As mentioned, creativity\ncan be induced by raising the temperature of the LLM, but responses that are more unpredictable\nare also those that are more likely to be hallucinations (Lee 2023). Thus, defining the contours of\nwhat counts as an unwanted hallucination in this specific sense requires value judgements about\nthe balance between fidelity and spontaneity.\nFinally, the third way that an LLM can hallucinate is by producing a response that lacks fidelity\nto the facts of the world, irrespective of how the LLM is trained or prompted (Maynez et al. 2020).\nWe consider this to be another type of open-domain hallucination, with the key concern being\n“factuality” in relation to the facts of the world (cf. Wittgenstein, 1998 [1921]). In our context, this\n3 For example, if a training corpus consisted of J. K. Rowling’s Harry Potter series, we would\nexpect an LLM to produce the sentence “Tom Marvolo Riddle” in response to a query about V olde-\nmort’s real name. However, if the training corpus consisted solely of Jane Austen’s Pride and\nPrejudice (for instance), we would consider this LLM output to be a hallucination—because there\nwould be no basis in the training data for making such a claim about V oldemort.\n4 For example, this kind of firm-specific fine-tuning is the business model of a prominent legal\ntech startup, Harvey.ai (Ambrogi 2023).\n10\nis perhaps the most alarming type of hallucination, as it can undermine the accuracy required in\nany legal context where a correct statement of the law is necessary.\n2.3 Hallucination Trade-offs\nIn this article, we investigate only the last kind of hallucination. As mentioned, the first two\nmodes of hallucination are not always problematic in the legal setting: these kinds of hallucinations\ncould actually be somewhat desirable to lawyers if they resulted in generated language that, for\nexample, removed unnecessary information from a given argument (at the expense of being faithful\nto it) or invented a novel analogy never yet proposed (at the expense of being grounded in the\nlexicon) (Cao, Dong, and Cheung 2022). However, what a lawyer cannot tolerate is the third\nkind of hallucination, or factual infidelity between an LLM’s response and the controlling legal\nlandscape. In a common law system, where stare decisis requires attachment to the “chain” of\nhistorical case law (Dworkin 1986), any misstatement of the binding content of that law would\nmake an LLM quickly lose any professional or analytical utility.\nFocusing on non-factual hallucinations alone, however, comes with certain trade-offs. One of\nthe advantages of our typology is that it makes clear that it may not always be possible to minimize\nall modes of hallucination simultaneously; reducing hallucinations of one kind may increase hallu-\ncinations of another. For example, if a given prompt contains information that does not conform to\nfacts about the world, then ensuring response fidelity with respect to the former would by definition\nproduce infidelity—i.e., hallucination—with respect to the latter. More generally, although fidelity\nto the prompt is necessary for avoidingclosed-domain hallucination, there is an important sense in\nwhich prioritizing such behavior might actually induce the kind of open-domain hallucination that\nwe center in this article.\nThese trade-offs present unavoidable challenges for prospective users of legal LLMs. When\nresponding to a query, should an LLM be skeptical of its prompt or sycophantic to it? If it has been\ntrained on case law from one jurisdiction, should it enforce adherence to that training corpus even\nwhen responding about the law in another jurisdiction? If facts about the world conflict with each\n11\nother—as legal rules often do—should the LLM preserve that nuance or refrain from introducing\ninformation outside the scope of a query? Questions like these are ultimately questions about\nwhich kinds of legal hallucinations are more and less preferable, and they are questions whose\nanswers require both empirical evidence and normative arguments. For example, minimizing fact\nand training corpus hallucinations (at the expense of prompt hallucinations) might be best for\navoiding harm to pro se litigants, but the calculus might be reversed for sophisticated lawyers who\nmight be less vulnerable to such behavior. We supply some of the empirics that speak to these\ndilemmas (see Sections 5.1.6 and 5.2), but stress that the normative considerations are crucial and\nshould be a topic of continued legal hallucination research.\n3 Profiling Hallucinations Using Legal Knowledge Queries\nTo empirically assess the incidence and correlates of non-factual hallucinations, we adopt a QA\nframework where the goal is to test an LLM’s ability to produce accurate information in response\nto different kinds of legal queries. We develop fourteen tasks representative of such queries, which\nwe group into three categories in order of increasing complexity and list in Figure 2.\n3.1 Low Complexity Tasks\nIn the low complexity category, we ask for information that we consider relatively easy for\nan LLM to reproduce. The information in this category does not derive from the actual content\nof a case itself, so it does not require higher-order legal reasoning skills to internalize. Instead,\nthis information is readily available in a case’s caption or its syllabus—standard textual locations\nwhose patterns even non-specialized LLMs should be able to recover. We therefore expect LLMs\nto perform best on these tasks:\nExistence: Given the name and citation of a case, state whether the case actually exists or\nnot. This basic evaluation provides preliminary insights into an LLM’s knowledge of actual legal\ncases: if it cannot distinguish real cases from non-existent ones, it probably cannot offer detailed\n12\nComplexity Task Query Method\nLow\nExistence Is {case} a real case? Reference-based\nCourt What court decided {case}? Reference-based\nCitation What is the citation for {case}? Reference-based\nAuthor Who wrote the majority opinion in {case}? Reference-based\nModerate\nDisposition Did {case} affirm or reverse? Reference-based\nQuotation What is a quotation from {case}? Reference-based\nAuthority What is an authority cited in {case}? Reference-based\nOverruling year What year was {case} overruled? Reference-based\nHigh\nDoctrinal agreement Does {case1} agree with {case2}? Reference-based\nFactual background What is the factual background of {case}? Reference-free\nProcedural posture What is the procedural posture of {case}? Reference-free\nSubsequent history What is the subsequent history of {case}? Reference-free\nCore legal question What is the core legal question in {case}? Reference-free\nCentral holding What is the central holding in {case}? Reference-free\nTable 2: Hallucination QA task list. Tasks are sorted in order of increasing complexity. Query wording\nis paraphrased; see the Online Appendix for exact prompt used. Method column describes the inferential\nstrategy that we use to estimate a hallucination rate for each task: reference-based tasks use known metadata\nto assess hallucinations, and reference-free tasks use emergent contradictions to assess hallucinations (see\nSection 4).\ncase insights. We use only real cases in our prompts, so affirming their existence is the correct\nanswer.5\nCourt: Given the name and citation of a case, supply the name of the court that ruled on it.\nThis task assesses an LLM’s knowledge about legal jurisdictions, an important building block of\na case’s precedential value. We perform this task across the three different levels of the federal\njudiciary. Importantly, we note that each level of the judiciary has a different reporter, or the series\nof volumes that opinions are published in. This is relevant because the reporter is included in\nthe citation that we provide to the LLM, essentially revealing the level of the hierarchy that an\nopinion is from. All and only SCOTUS cases are published in the U.S. Reports. Opinions from\nthe USCOA are published in the Federal Reporter, and USDC cases are published in the Federal\nSupplement. Because of this, we expect this task to be more difficult as we descend the hierarchy\nof courts. There is only one court associated with the U.S. reporter, but 13 associated with the\n5 In the Online Appendix, we experiment with using fake cases as well.\n13\nFederal Reporter, and 94 associated with the Federal Supplement. For USCOA cases, we require\nthe name of the specific circuit court, and for USDC cases, we require the name of the specific\ndistrict court.\nCitation: Given a case name, supply the Bluebook citation of the case. This query tests an\nLLM’s ability to associate a given dispute with its official record in a reporter volume at a particular\npage, which is the key way in which different opinions reference and link to each other. For\nUSCOA cases, we further specify that we want the citation for the circuit court opinion, and for\nUSDC cases, we further specify that we want the citation for the district court opinion. We test for\ncitation equality using eyecite (Cushman, Dahl, and Lissner 2021).\nAuthor: Given the name and citation of a case, supply the name of the opinion author. This\nquery tests an LLM’s ability to associate a given case with a particular judge, which is important\nfor contextualizing a case in the broader jurisprudential landscape. For SCOTUS and USCOA\ncases, we further specify that we want the name of the majority opinion author. We accept a fuzzy\nmatch of the opinion author’s name as accurate.\n3.2 Moderate Complexity Tasks\nNext, in the moderate complexity category, we start to require an LLM to evince knowledge\nof actual legal opinions themselves. To answer the queries in this category, an LLM must know\nsomething about a case’s substantive content; these queries seek information that must be collated\nfrom idiosyncratic portions of its text. Of course, a database-augmented LLM might still be able\nto retrieve some of this information without ever actually internalizing the content of a case, but\nwe expect this text-based knowledge to be less available than the information described in the low\ncomplexity category. Specifically, we ask for the following information:\nDisposition: Given a case name and its citation, state whether the court affirmed or reversed\nthe lower court. This query tests an LLM’s knowledge of how the court resolved the instant\nappeal confronting the parties in the case, which is the first step for determining the holding that is\ncreated by the case. Though this is essentially a binary classification task where we accept correct\n14\n“affirm” or “reverse” labels as accurate, we consider this task to still be probative of hallucinations\nbecause producing the wrong label is still a misstatement of the law. We filter out all ambiguous\ndispositions (e.g., reversals in part) and we do not ask this query of USDC cases because district\ncourts are courts of original jurisdiction.6\nQuotation: Given a case name and its citation, supply any quotation from the opinion. This\nquery tests an LLM’s ability to produce some portion of an opinion’s text verbatim, which is an\nimportant feature for lawyers seeking to use a case to stand for a specific proposition. Normally,\nsuch memorization is considered an undesirable property of LLMs (Carlini et al. 2022), but in\nthis legal application it is actually desirable behavior. We accept any fuzzy string of characters\nappearing in the majority opinion as accurate.\nAuthority: Given a case name and its citation, supply a case that is cited in the opinion. This\nquery probes an LLM’s understanding of the chain of precedential authority that supports a given\nopinion. We do not distinguish between positive and negative citations for this task; we accept any\nprecedent cited in any way in the text of the majority opinion as accurate. We extract and match\ncitations on their volumes, reporters, and pages using eyecite (Cushman, Dahl, and Lissner 2021).\nOverruling year: Given a case name and its citation, supply the year that it was overruled.\nThis query tests an LLM’s ability to recognize when a given case has been subsequently altered,\nwhich is crucial information for lawyer seeking to determine whether a given precedent is still\ngood law or not. This task is the most complicated in this category because it requires the LLM\nto draw connections between multiple areas of the case space. We accept only the exact year of\noverruling as accurate, and we limit this task to only those SCOTUS cases that have been explicitly\noverruled (n=279).7\n6 While it is possible for some administrative agency decisions to be appealed to a district court,\nthis occurs infrequently enough that we choose not to ask for case disposition at the district court\nlevel.\n7 In Section 5.2, we experiment with cases that have never been overruled as well.\n15\n3.3 High Complexity Tasks\nFinally, in the high complexity category, we seek answers to tasks that both presuppose legal\nreasoning skills (unlike the low complexity tasks) and are not readily available in existing legal\ndatabases like WestLaw or Lexis (unlike the moderate complexity tasks). These tasks all require\nan LLM to synthesize core legal information out of unstructured legal prose—information that\nis frequently the topic of deeper legal research. In Section 4.3, we explain how we test LLMs’\nknowledge of some of these more complex facts without necessarily having access to the ground-\ntruth answers ourselves:\nDoctrinal agreement: Given two case names and their citations, state whether they agree or\ndisagree with each other.This query requires an LLM to show knowledge of the precedential rela-\ntionship between two different cases, information that is essential for higher-order legal reasoning.\nWe use Shepard’s treatment codes as a basis for constructing this task, filtering out all ambiguous\ncitation treatments (e.g., neutral treatments) and coarsening the unambiguous codes into “agree”\nand “disagree” labels that we accept as accurate. For this task, we use a relatively balanced dataset\nof 2,839 citing-cited case pairs coded as “agree,” and 2,161 citing-cited case pairs coded as “dis-\nagree.” This task is limited to SCOTUS cases, as our underlying dataset only contains thorough\nShepard’s data for citations to the Supreme Court.\nFactual background: Given a case name and its citation, supply its factual background. This\nquery tests an LLM’s understanding of the concrete fact pattern underlying a case, which is helpful\nin assessing the relevance of the case to current research and in drawing parallels with other cases.\nProcedural posture: Given a case name and its citation, supply its procedural posture. This\nquery tests an LLM’s understanding of how and why a case has arrived at a particular court, which\naids in understanding the precise question presented and standard of review applicable.\nSubsequent history: Given a case name and its citation, supply its subsequent procedural\nhistory, if any. This query tests an LLM’s knowledge of any other related proceedings that concern\nthe given case after a particular decision, which is information that can change or clarify the legal\nsignificance of the case.\n16\nCore legal question: Given a case name and its citation, supply the core legal question at is-\nsue. This query tests an LLM’s ability to pinpoint the main issue or issues that a court is addressing\nin a case, which is the most important factor in assessing whether a case is apposite or not.\nCentral holding: Given a case name and its citation, supply its central holding. This query\ntests an LLM’s knowledge of the legal principle that a given case stands for, i.e., the precedent that\nfuture cases will rely upon or distinguish from. Articulating the holding of a case is crucial for\nlegal analysis and argumentation and is the most complex task that we evaluate.\n4 Experimental Design\n4.1 Data Construction\nWe aim to profile hallucination rates across several legally salient dimensions, including hier-\narchy, jurisdiction, time, and case prominence. Thus, we construct our test data with an eye toward\nmaking statistical inferences on these covariates.\nWe begin with the universe of case law from each level of the federal judicial hierarchy—\nnamely, SCOTUS, USCOA, and USDC—that has been published in the volumes of the U.S. Re-\nports, the Federal Reporter, and the Federal Supplement. To ensure balance over time and place,\nwe then perform stratified random sampling using year strata for the SCOTUS cases, circuit-year\nstrata for the USCOA cases, and state-year strata for the USDC cases. We draw 5,000 cases\nfrom each level of the judiciary. Finally, to generate ground-truth answers for our reference-based\nqueries (Section 4.2), we merge these units with metadata obtained from the Caselaw Access\nProject (2023), the Supreme Court Database (Spaeth et al. 2022), the Appeals Courts Database\nProject (Songer 2008; Kuersten and Haire 2011), the Library of Congress (Congress.gov 2023),\nand Shepard’s Citations (Fowler et al. 2007; Black and Spriggs 2013). 8\n8 More information about how we use these metadata to construct each query is available in the\nOnline Appendix.\n17\n4.2 Reference-based Querying\nThe most straightforward way to study hallucinations in the open-domain setting is to use a\ntest oracle—or an external reference—to detect and adjudge non-factual responses (Lin, Hilton,\nand Evans 2022; Lee et al. 2023; J. Li et al. 2023). Such oracles are usually difficult and costly to\nconstruct (Krishna, Roy, and Iyyer 2021), but we use the tabular metadata described in Section 4.1\nto develop ours. Our design exploits the fact that while LLMs are known to have been trained on\nthe raw text of American case law, which is in the public domain (Henderson et al. 2022), they\nhave likely not been trained on these cases’ attendant metadata, which exist separately from the\ncases’ textual content and which we have aggregated from disparate sources.\nThese metadata enable us to construct reference-based queries for the first nine of our tasks\n(Figure 2). These queries take the form of N question-and-answer triples (prompt, response,\nresponse′), where prompt is a case-specific question, response is the LLM’s greedy answer re-\ntrieved from calling f0(·), and response ′ is the known ground-truth answer. 9 Our estimand of\ninterest for each task is the population-level hallucination rate π, which we estimate by averaging\nover the N sampled queries:\nπ = ˆπ = 1\nN\nX\n1 [response ̸= response′] (1)\nOccasionally, an LLM will produce a response that is neither a hallucination nor a correct\nanswer, but rather an explicitabstention from answering the question. For example, the LLM might\nadmit that it does not know the answer or demur that it is unable to provide the answer for some\nreason, perhaps due to safety concerns. In these instances, we nevertheless count the response as a\nnon-hallucination, on the theory that an LLM cannot hallucinate when it is affirmatively abstaining\nfrom responding (Feng et al. 2024). We document the frequency of these abstentions in the Online\nAppendix, but they are generally rare and do not substantively affect our findings.\n9 Recall from Section 2.1 that the f0 notation represents performing inference with the LLM at\ntemperature zero—i.e., under its deterministic behavior.\n18\n4.3 Reference-free Querying\nReference-based querying lets us directly recover our population parameter of interest, but two\nproblems limit the effectiveness of the approach. First, we are restricted to asking questions for\nwhich digestible metadata exist and a clear answer has been recorded, which rules out many more\ncomplex inquires. Second, precisely because these queries can be answered with tabular data, legal\ndatabase-augmented LLMs (Cui et al. 2023; Savelka et al. 2023) are likely to soon solve or at least\nmask hallucinated responses to these queries (Peng et al. 2023; Shuster et al. 2021).\nTo test the tasks that cannot be easily verified against an external legal database, we employ\nreference-free querying instead, which detects hallucinations by exploiting the stochastic behavior\nof LLMs at higher temperatures (Agrawal et al. 2023; Manakul, Liusie, and Gales 2023; Min\net al. 2023). This approach is rooted in the theory that hallucinations are more likely to originate in\nflat probability distributions with higher next-token uncertainties, whereas factual answers should\nalways have a high probability of being the generated response given a prompt. Thus, by repeatedly\nquerying an LLM at a non-greedy temperature, we can estimate the model’s hallucination rate by\nexamining its self-consistency—factual responses should not change, but hallucinated ones will.\nMost reference-free approaches implicitly assume that the LLM is calibrated, i.e., that there is\nindeed some correlation between its self-consistency and its propensity to hallucinate. For reasons\nthat we discuss in Section 5.3, we are unwilling to make this assumption in our legal setting. We\ntherefore adopt a slightly different implementation that is still reference-free, but only requirescon-\ntradiction, not consistency (Mündler et al. 2023). Specifically, for our final five tasks (Figure 2), we\nconstruct reference-free queries in the form ofN question-and-answer triples (prompt, response(1),\nresponse(2)), where prompt is the question, response (1) is one LLM answer retrieved by calling\nf1(·) once, and response(2) is another LLM answer retrieved by calling f1(·) again.10 Detecting a\nhallucination then amounts to detecting a logical contradiction between the two stochastic answers:\nany such contradiction guarantees non-factuality, because two contradictory answers cannot both\n10 Recall from Section 2.1 that the f1 notation represents performing inference with the LLM at\ntemperature one—i.e., with some degree of stochasticity.\n19\nbe correct.\nTo identify these contradictions at scale, we feed both answers into GPT 4 and ask it for its\nassessment. This technique does not assume anything aboutf1(·)’s calibration—it just requires that\nGPT 4 possess logical reasoning skills sufficient to compare f1(·)’s two responses and accurately\nlabel them as contradictory as not. To justify this reliance on GPT 4, we manually label a portion\nof the reference-free responses ourselves and conduct an intercoder reliability analysis to ensure\nthat GPT 4 is indeed able to perform this task. Full information about our procedure and a validity\ncheck is provided in the Online Appendix (We find that GPT 4’s reliability is comparable to human\nlabeling of contradictions.)\nAn important caveat of this approach is that it only allows us to establish alower bound on the\nhallucination rate for our reference-free queries:\nπ ≥ ˆπ = 1\nN\nX\n1 [response(1) ̸= response(2)] (2)\nAlthough self-contradiction guarantees hallucination, the inverse does not hold: two answers may\nbe logically consonant but still lack fidelity to the law. Because we are unwilling to assume cal-\nibration, we accept this inferential limitation, but, as we show below, even the lower bounds on\nhallucination rates are quite high and informative.\n4.4 Models\nWe perform our experiments using four popular, state-of-the-art, off-the-shelf LLMs:\n1. OpenAI’s ChatGPT 4 ( gpt-4-1106-preview, OpenAI 2023a),\n2. OpenAI’s ChatGPT 3.5 ( gpt-3.5-turbo-0613, 2023b),\n3. Google’s PaLM 2 ( text-bison-001, Anil et al. 2023), and\n4. Meta’s Llama 2 ( Llama-2-13b-chat-hf, Touvron et al. 2023).\n20\nWe run each query under both “zero-shot\" and “three-shot\" prompting setups. In the zero-shot\nsetup, we simply ask the LLM about the given case directly, whereas in the three-shot setup, we\nprepend several example questions and responses to give the LLM an opportunity to perform in-\ncontext learning (Brown et al. 2020). We provide the full text of the prompts we use for each\nquery, along with the few-shot examples, in the Online Appendix. In total, we execute more than\n800,000 queries—200,000+ per LLM—and we share our raw API calls and model responses in\nthe replication materials accompanying this article.\n5 Results\nWe begin by presenting our main results profiling LLMs’ hallucination rates, which cut to the\ncore of popular concerns over LLMs’ suitability for legal applications (Section 5.1). Then, after\nshowing that hallucinations are generally widespread, and highlighting the correlates of LLMs’\nhallucination rates, we turn to two additional challenges that threaten LLMs’ utility for legal adop-\ntion: (1) their susceptibility to contra-factual bias, i.e., their ability to handle queries based on\nmistaken legal premises (Section 5.2), and (2) their certainty in their responses, i.e., their self-\nawareness of their propensity to hallucinate (Section 5.3).\n5.1 Hallucination Rates and Heterogeneity\nTables 3, 4, and 5 report our estimated hallucination rates and their standard errors for each\ncategory of our tasks. We find that hallucinations vary with the substantive complexity of the task\n(Section 5.1.1), the hierarchical level of the court (Section 5.1.2), the jurisdictional location of the\ncourt (Section 5.1.3), the prominence of the case (Section 5.1.4), the year the case was decided\n(Section 5.1.5), and the LLM queried (Section 5.1.6). We do not find substantial differences be-\ntween zero-shot and few-shot prompting, so we focus our discussion on the few-shot results alone.\n21\nLow Medium High\nTask Complexity\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMean Hallucination Rate\nGPT 4 GPT 4 GPT 4GPT 3.5 GPT 3.5 GPT 3.5PaLM 2 PaLM 2 PaLM 2Llama 2 Llama 2 Llama 2\nFigure 2: Relationship between task complexity and mean hallucination rate. Higher values indicate a\ngreater likelihood of factually incorrect LLM responses. High complexity tasks include several reference-\nfree tasks, so those reported hallucination rates are lower bounds on the true rates. Contra-factual tasks and\nthe doctrinal agreement high complexity task are excluded from this comparison.\n5.1.1 Hallucinations Vary by Task Complexity\nAs we hypothesized in Section 3, we first observe that hallucinations increase with the com-\nplexity of the legal research task at issue, which we visualize in Figure 2. Starting with the low\ncomplexity category (Table 3), the LLMs perform best on the simple Existence task, though this\nis in part driven by their tendency to always answer “yes” when asked about the existence of any\ncase. (In the Online Appendix we demonstrate this problem by asking about the existence of fake\ncases instead.) The models begin to struggle more when prompted for information about a case’s\nCourt, Citation, or Author. Hallucinations then surge among the moderate complexity tasks (Ta-\nble 4), all of which require the LLMs to evince knowledge of the actual content of a legal opinion.\nWe note that these results are not just a product of different evaluation metrics: although theQuo-\ntation task, for example, requires near-word reproduction of particular sentences and phrases to be\njudged correctly, the Disposition task simply asks for binary responses from the model. Yet, the\nLLMs hallucinate widely in both setups.\n22\nThe results for the high complexity tasks (Table 5) confirm this general pattern of poor per-\nformance. Starting with Doctrinal agreement, recall that this query asks the LLM to make an\nanalogical judgment about the precedential relationship between two given cases, for which we\nhave ground-truth labels from Shepard’s treatment codes. Because this is another binary classi-\nfication task, the LLMs’ hallucination rates on this task—near 0.5—represent little improvement\nover random guessing, and are actually sometimes worse. This suggests that LLMs know little\nabout substantive legal doctrine, calling into question their ability to accurately assist lawyers in\nmore realistic, applied settings.\nThe remaining tasks in the high complexity category amplify these concerns, but it is important\nto keep in mind that the hallucination rates that we report for these tasks are only lower bounds\non the true rates, as these tasks are evaluated using our reference-free method (Section 4.3). To\nprovide some context for these bounds, we note that in a similar self-contradiction setup, Mündler\net al. (2023) found that GPT 3.5 hallucinated about 14.3% of the time on general QA queries. On\nour legal QA queries, GPT 3.5 and our other LLMs far surpass this baseline rate—and it is possible\nthat the true hallucination rate is even higher.\nFor example, we find that even on the easier reference-free tasks— Factual background and\nProcedural posture—our LLMs hallucinate at least 49% of the time. Performance degrades fur-\nther on the most complex Core legal question and Central holding tasks, with hallucinations\narising in response to at least 59% and 63% of our queries, respectively. Hallucinations are lowest\namong GPT 4 responses to the Subsequent history task at the SCOTUS level, but this is because\nthe model simply tends to state that the litigation concluded with the Supreme Court decision. This\nmay not actually be correct—many Supreme Court cases result in a remand and have additional\nprocedural history in lower courts. However, we are unable to capture this kind of mistake, as\nour methodology only permits us to identify hallucinations where the model contradicts itself. We\nare not able to capture repeated incorrect answers as instances of hallucination, meaning that our\nestimate of hallucination in the SCOTUS Subsequent history task is likely to understate the rate\nof hallucination by a larger margin that other tasks.\n23\nSCOTUS USCOA USDC\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMean Hallucination Rate\nGPT 4\nGPT 3.5\nPaLM 2 Llama 2\nFigure 3: Relationship between judicial hierarchy and mean hallucination rate, all reference-based tasks\npooled. Hallucination rates are higher for lower levels of the federal judiciary.\nTaken together, these results invite skepticism about LLMs’ true knowledge of the law. Our\nreference-free tasks, in particular, raise serious doubts about LLMs’ knowledge of substantive as-\npects of American case law—the very knowledge that attorneys must often synthesize themselves,\ninstead of merely looking up in a database.\n5.1.2 Hallucinations Vary by Court\nWe next examine trends by hierarchy, exploring LLMs’ abilities to restate the case law of\nthe three different levels of the federal judiciary. We find that across all tasks and all LLMs,\nhallucinations are lowest in the highest levels of the judiciary, and vice-versa (Figure 3). Thus, our\nLLMs perform best on tasks at the SCOTUS level, worse on tasks at the USCOA level, and worst\non tasks at the USDC level. These results are encouraging insofar as it is important for LLMs\nto be knowledgeable about the most authoritative and wide-ranging precedents, but discouraging\ninsofar as they suggest that LLMs are not well attuned to localized legal knowledge. After all,\nthe vast majority of litigants do not appear before the Supreme Court, and may benefit more from\nknowledge that is tailored to their home district court—their court of first appearance.\n24\nD.C. Cir.Fed. Cir.\n0.68\n0.70\n0.72\n0.74\n0.76\nHallucination\nRate\nFigure 4: Relationship between USCOA jurisdiction and mean hallucination rate, all reference-based US-\nCOA tasks and models pooled, post-1981 cases only. LLM performance is strongest in jurisdictions that are\ncommonly perceived to play a more influential role.\n5.1.3 Hallucinations Vary by Jurisdiction\nTo better understand the relationship between different courts and hallucinations, we next zoom\nin on the middle level of the judicial hierarchy—the Courts of Appeals—and examine horizontal\nheterogeneity across the circuits. 11 Figure 4 depicts these results geographically, showing lower\nhallucination rates in lighter colors and higher rates in darker colors. Pooling our tasks and models\ntogether, we see the best performance in the Ninth Circuit (comprising California and adjacent\nstates in yellow), the Second Circuit (comprising New York and adjacent states in soft green), the\nThird Circuit (comprising Pennsylvania and adjacent states in soft green), and the First Circuit\n(comprising Maine and adjacent states in soft green). By contrast, performance tends to be worst\nin the circuits in the geographic center of the country.\nThese results confirm popular intuitions about the influential role that the Second, Third, and\nNinth Circuits play in the American legal system. Because it encompasses New York City, the\n11 Because not all Courts of Appeals were created at the same time, for parity in comparison here\nwe exclude from our results cases decided before 1982, the year the youngest circuit—the Federal\nCircuit—was created. We report the full, non-truncated results in the Online Appendix, which are\nlargely consistent with these post-1981 results.\n25\n20 40 60 80 100\nCase Prominence\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMean Hallucination Rate\nMore Prominent\nGPT 4\nGPT 3.5\nPaLM 2\nLlama 2\nFigure 5: Relationship between SCOTUS case prominence (measured by PageRank percentile) and mean\nhallucination rate, all SCOTUS tasks pooled. Hallucinations decline sharply as case prominence passes the\n90th percentile, meaning that LLMs are more likely to respond with accurate information about prominent\ncases.\nSecond Circuit has traditionally had a significant impact on financial and corporate law, and many\nlandmark decisions in securities law, antitrust, and business litigation have come from this court.\nThe Third Circuit enjoys similar influence in the corporate law domain owing to Delaware’s status\nas the legal home for many corporations. Finally, the Ninth Circuit handles more cases than any\nother federal appellate court, and often issues rulings that advance progressive positions that lead\nto disproportionate review by the Supreme Court.\nPerhaps surprisingly, however, our results stand in tension with received wisdom about the\nD.C. Circuit, which is generally thought to be the most influential appellate division. In our tasks,\nour LLMs actually perform worst on this circuit. This counterintuitive finding is one example of\nthe way that unanticipated biases might trouble the reliance on LLMs in practice.\n5.1.4 Hallucinations Vary by Case Prominence\nTo probe the role of legal prominence more directly, we move to SCOTUS-level results next,\nexamining the relationship between case importance and hallucinations. To measure case promi-\n26\n1800 1850 1900 1950 2000\nCase Year\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMean Hallucination Rate\nGPT 4\nGPT 3.5\nPaLM 2\nLlama 2\nFigure 6: Relationship between SCOTUS case decision year and mean hallucination rate, all SCOTUS\ntasks pooled. LLMs are most likely to respond with accurate information in cases from the latter half of the\n20th century, struggling on very old and very new cases.\nnence within this single level of the judiciary, we use the Caselaw Access Project’s PageRank\npercentile scores, a metric of citation network centrality that captures the general legal and politi-\ncal prominence of a case.\nWe find that case prominence is negatively correlated with hallucination, reaffirming our results\nfrom above (Figure 5). However, we also note that a sharp slope change occurs around the 90th\nprominence percentile in the GPT 4, GPT 3.5, and PaLM 2 models. This suggests that the bias of\nthese LLMs—but not Llama 2—may be skewed even more toward the most well-known decisions\nof the American legal system, even within the SCOTUS level.\n5.1.5 Hallucinations Vary by Case Year\nBecause case law develops in virtue of new decisions building on old ones over time, the\nage of a case may be another useful predictor of hallucination. Examining this relationship at the\nSCOTUS level in Figure 6, we find a non-linear correlation between hallucination and age: halluci-\nnations are most common among the Supreme Court’s oldest and newest cases, and least common\n27\namong its post-war Warren Court cases (1953-1969). This result suggests another important lim-\nitation on LLMs’ legal knowledge that users should be aware of: LLMs’ peak performance may\nlag several years behind the current state of the doctrine, and LLMs may fail to internalize case\nlaw that is very old but still applicable and relevant law.\n5.1.6 Hallucinations Vary by LLM\nFinally, we also partition our results by the LLM itself and compare across models. We find\nthat not all LLMs are equal: as expected, GPT 4 performs best overall, followed by GPT 3.5,\nfollowed by PaLM 2, followed by Llama 2 (Figure 1).\nWe also discover tendencies towards different inductive biases, or the predisposition of an\nLLM to generate certain outputs more frequently than others. In Figure 7, we highlight one of\nthese biases for our SCOTUS-level Author task, which asks the LLM to supply the name of\nthe justice who authored the majority opinion in the given case. Each LLM we test has slightly\ndifferent inductive preferences; some err towards the most recognizable justices, but others are a\nlittle more difficult to explain. For example, Llama 2 disproportionately favors Justice Story—\nan influential jurist who authored the famous Amistad opinion, among others—whereas PaLM 2\nprefers Justice McLean—also an important jurist, but one more known for his dissents than his\nmajority opinions, such as his dissent in the infamous Dred Scott case. Across the board, all our\nLLMs tend to overstate the true prevalence of justices at a higher magnitude than they understate\nthem, as indicated by the greater dispersion of the points above the y = x line in Figure 7.\nThese biases demonstrate one way that LLMs inevitably encounter the kind of hallucination\ntrade-off that we discuss in Section 2.3. If the inductive bias that an LLM learns from its train-\ning corpus is not well-aligned with the true distribution of facts about the world, then the LLM\nis likely to make systematic errors when queried about those facts. Moreover, the persistence of\ninductive biases also increases the risk of LLMs instantiating a kind of legal monoculture (Klein-\nberg and Raghavan 2021). Instead of accurately restating the full variation of the law, LLMs may\nsimply regurgitate information from a few prominent members of the response set that they have\n28\n0 200 400 600 800\nTrue Number in Dataset\n0\n200\n400\n600\n800Number of LLM Responses\nStory\nMclean\nBrennanFullerWarren\nMiller J. MarshallBrennan\nJ. Marshall\nJ. Marshall\nJ. Marshall\nGPT 4\nGPT 3.5\nPaLM 2\nLlama 2\nFigure 7: Number of times each justice is stated to be the author of a SCOTUS case versus the actual\nnumber of cases authored by each justice in our time period-stratified dataset. A small number of justices\nare disproportionately represented in LLM responses.\nbeen trained on, flattening legal nuance and producing a falsely homogenous sense of the legal\nlandscape.\n5.2 Contra-factual Bias\nWe now turn to the first of two potential failure points that we seek to examine for LLMs\nperforming legal tasks, beyond their sheer propensity to hallucinate: their bias toward accepting\nlegal premises that are not anchored in reality and answering queries accordingly. We view this\nbehavior as a particular kind of model sycophancy (the tendency of an LLM to agree with a user’s\npreferences or beliefs, even when the LLM would reject the belief as wrong without the user’s\nprompting; Sharma et al. 2023; Wei et al. 2023) or general cognitive error (Tversky and Kahneman\n1974; Jones and Steinhardt 2022; Suri et al. 2023).\nThis bias poses a subtle but pernicious challenge to those aiming to use LLMs for legal re-\nsearch. When a researcher is learning about a topic, they are not only unsure about the answer,\nthey are also often unsure about the question they are asking as well. Worse, they might not even\n29\nGPT 4 GPT 3.5 PaLM 2 Llama 2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMean Hallucination Rate\n0.69\n0.52\n0.99\n0.02\nFigure 8: Hallucination rates by LLM, all contra-factual tasks pooled. Llama 2 is very unlikely to halluci-\nnate on these tasks because it almost always rejects the premise in the question. However, this tendency also\nleads it to perform more poorly on tasks with correct premises (cf. Figure 1).\nbe aware of any defects in their query; research by its nature ventures into the realm of “unknown\nunknowns” (Luft and Ingham 1955). This is especially true for unsophisticated pro se litigants,\nor those without much legal training to begin with. Relying on an LLM for legal research, they\nmight inadvertently submit a question premised on non-factual legal information or folk wisdom\nabout the law. As discussed in Section 2.3, this then forces a trade-off: if the LLM is too intent on\nminimizing prompt hallucinations, it runs the risk of simply accepting the user’s misconception as\ntrue and producing a factual hallucination instead.\nTo test whether this risk is real in the legal setting, we evaluate two modified versions of our\nreference-based queries, but with premises that are false by construction. Specifically, we ask\nthe LLMs to (1) provide information about an author’s dissenting opinion in an appellate case in\nwhich they did not in fact dissent and (2) furnish the year that a SCOTUS case that has never been\noverruled was overruled. In both cases, we consider failing to provide the requested information\nan acceptable answer; any uncritical answering of the prompt is treated as a hallucination.\nTable 6 reports the results of this experiment and Figure 8 summarizes them by LLM. In gen-\n30\neral, LLMs seem to suffer from contra-factual bias on these legal information tasks. As in the raw\nhallucination tasks, contra-factual bias hallucinations are higher in lower levels of the judiciary.\nSubstantively, they are also greatest for the question with a false overruling premise, possibly re-\nflecting the increased complexity of the question asked.\nLlama 2 performs exceptionally well, demonstrating little contra-factual hallucination. How-\never, this success is linked to a different kind of hallucination—in many false dissent examples, for\ninstance, Llama 2 often states that the case or justice does not exist at all. (In reality, all of our false\ndissent examples were created with real cases and real justices—just justices who did not author a\ndissent for the case.) Under our metrics for contra-factual hallucination, we choose to record these\nexamples as successful rejections of the premise. The kind of error that Llama 2 makes here is\nalready measured in its poor performance on other tasks, especially Existence.\n5.3 Model Calibration\nThe second potential hazard that we investigate is model calibration, or the ability of LLMs\nto “know what they know.” Ideally, a well-calibrated model would be confident in its factual\nresponses, and not confident in its hallucinated ones (Azaria and Mitchell 2023; Kadavath et\nal. 2022; Tian, Mitchell, Zhou, et al. 2023; Xiong et al. 2023; Yin et al. 2023). If this prop-\nerty held for legal queries, users would be able to adjust their expectations accordingly and could\ntheoretically learn to trust the LLM when it is confident, and learn to be more skeptical when it is\nnot (Zhang, Liao, and Bellamy 2020). Even more importantly, if an LLM knew when it was likely\nto be hallucinating, the hallucination problem could be in principle solvable through some form\nof reinforcement learning from human feedback (RLHF) or fine-tuning, with unconfident answers\nsimply being suppressed (Tian, Mitchell, Yao, et al. 2023).\nTo study our LLMs’ calibration on legal queries, we estimate the expected calibration error\n(ECE) for each of our tasks. We describe our estimation strategy in full in the Online Appendix,\nbut, intuitively, it entails extracting a confidence score for each LLM answer that we obtain and\ncomparing it to the empirical hallucination rate that we observe. Table 7 reports the results of this\n31\n0.0 0 .2 0 .4 0 .6 0 .8 1 .0\nConﬁdence\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNon-Hallucination Rate\nGPT 4\nGPT 3.5\nPaLM 2\nLlama 2\nFigure 9: Calibration curves by LLM, all reference-based tasks pooled. PaLM 2 is best calibrated on legal\nqueries, followed by GPT 3.5, GPT 4, and lastly Llama 2, which is significantly worse than the first three\nmodels.\nanalysis at the task level, and Figure 9 pools our findings at the LLM level by plotting those two\nmetrics—confidences and empirical non-hallucination frequencies—against each other, binned\ninto 10 equally-sized bins (represented by the dots). In a perfectly calibrated model, the confi-\ndences and empirical frequencies would be perfectly correlated along the y = x diagonal.\nOverall, we note that PaLM 2 (pooled ECE = 0.057), GPT 3.5 (pooled ECE = 0.099), and\nGPT 4 (pooled ECE = 0.190) are significantly better calibrated than Llama 2 (pooled ECE =\n0.421). Interestingly, although GPT 4 is our best performing model in terms of raw hallucination\nrates (Figure 1), it is actually less calibrated than PaLM 2 and GPT 3.5, which are otherwise\ninferior. This suggests that even the newest and most advanced LLMs may not always be superior\nin all desirable senses—although GPT 4 is currently the LLM least prone to hallucination, our\nresults imply that when it does hallucinate, it does so in a way that is more likely to mislead users\nthan GPT 3.5 or PaLM 2.\nDiving into the task-level results (Table 7), we see that across all LLMs, calibration is poorer\non our more complex tasks, like Doctrinal agreement, and on tasks directed toward lower levels\n32\nof the judicial hierarchy. ECE is also higher on our partially open-ended tasks such as Court and\nAuthor. In these tasks, the LLM has a large but finite universe of responses, and the high ECE for\nthese tasks reflects the LLMs’ tendencies to over-report on the most prominent or widely known\nmembers of the response set.\nIn all cases, the calibration error is in the positive direction: our LLMs systematically overes-\ntimate their confidence relative to their actual rate of hallucination. 12 This finding, too, suggests\nthat users should exercise caution when interpreting LLMs’ responses to legal queries, especially\nthose of Llama 2. Not only may they receive a hallucinated response, but they may receive one\nthat the LLM is overconfident in and liable to repeat again.\n6 Discussion\nWe began this article with a question that has surged in salience over the last twelve months:\nWill AI systems like ChatGPT soon reshape the practice of law and democratize access to jus-\ntice? Although there is much enthusiasm for LLMs’ potential to revolutionize these domains, we\nhighlight the problem of legal hallucinations, which remains a serious obstacle to the adoption\nof these models. Performing the first systematic empirical test of popular perceptions (Roberts\n2023; Romoser 2023; Weiser 2023), we show that factual legal hallucinations are widespread in\nthe LLMs that we study—OpenAI’s ChatGPT 4, OpenAI’s ChatGPT 3.5, Google’s PaLM 2, and\nMeta’s Llama 2—on the bulk of the legal knowledge tasks that we profile (Section 5.1).\nWe also push beyond conventional wisdom by documenting the correlates of these hallucina-\ntions and by surfacing two additional behaviors that threaten LLMs’ utility for legal applications:\n(1) their susceptibility to contra-factual bias, i.e., their inability to handle queries containing an\nerroneous or mistaken starting point (Section 5.2), and (2) their certainty in their responses, i.e.,\ntheir inability to always “know what they know” (Section 5.3). Unfortunately, we find that LLMs\n12 In the Online Appendix, we explore whether this bias can be corrected with anex post scaling\nadjustment, but conclude that challenges remain.\n33\nfrequently provide seemingly genuine answers to legal questions whose premises are false by con-\nstruction, and that under their default configurations they are imperfect predictors of their own\ntendency to confidently hallucinate legal falsehoods.\nThese findings complicate the existing literature that suggests that LLMs are performing in-\ncreasingly well on a number of legal benchmarking tasks (Ash et al. 2024; Blair-Stanek, Holzen-\nberger, and Van Durme 2023; Choi et al. 2022; Fei et al. 2023; Guha et al. 2023; Nay et al. 2023;\nKatz et al. 2023; Trozze, Davies, and Kleinberg 2023). Our study is related to this prior research,\nbut is oriented in a slightly different vein. Instead of examining LLMs’ ability to engage in le-\ngal reasoning, we assess LLMs’ capacity to internalize legal knowledge. Ultimately, LLMs will\nneed to excel in both of these respects if they are going to be effectively integrated into the legal\nprofession. So long as they suffer from gaps in their background legal knowledge—as our results\nsuggest—they will be unable to function as reliable sources of legal counsel and advice, no matter\nhow strong their in-context reasoning abilities become.\nOur results therefore temper optimism for the ability of off-the-shelf, publicly available LLMs\nto accelerate access to justice (Perlman 2023; Tan, Westermann, and Benyekhlef 2023; Tito\n2017). Indeed, our findings suggest that the risks of using these generic foundation models are\nespecially high for litigants who are:\n1. Filing in courts lower in the judicial hierarchy or those located in less prominent jurisdic-\ntions,\n2. Seeking more complex forms of legal information,\n3. Formulating questions with mistaken premises, or\n4. Unsure of how much to trust the LLMs’ responses.\nIn short, we find that the risks are highest for those who would benefit from LLMs most—\nunder-resourced or pro se litigants. Some of these risks—namely, (3) and (4)—might be mitigated\nwith improved user education, but others—(1) and (2)—are more intractable. LLMs would ideally\n34\ndo best at localized legal information (rather than SCOTUS-level information), be able to correct\nusers when they ask misguided questions (rather than accepting their premises at face value), and\nbe able to moderate their responses with the appropriate level of confidence (rather than hallucinat-\ning with conviction). Consquently, we echo concerns that the proliferation of LLMs may ultimately\nexacerbate, rather than eradicate, existing inequalities in access to legal services (Draper and Gilli-\nbrand 2023; Simshaw 2022). At the same time, increased reliance on LLMs also has the potential\nto produce a kind of legal monoculture (Creel and Hellman 2022; Kleinberg and Raghavan 2021),\nwith users being fed information from only a limited subset of judicial sources that elide many of\nthe deeper nuances of the law. This new monoculture, in turn, is likely to reify the geographic,\ntemporal, and judge-level biases that we diagnose above, as the foundation-like property of these\nmodels permits those biases to propagate into any downstream tools built on top of the original\nLLM (Bommasani et al. 2022).\nSome recent research suggests that hallucinations can be diminished through the adoption of\ntechniques like retrieval-augmented generation (RAG) (Shuster et al. 2021; Cui et al. 2023; Peng\net al. 2023; Savelka et al. 2023), advanced prompting (such as chain-of-thought prompting or\nchain-of-verification) (Si et al. 2023; Lei et al. 2023; Mündler et al. 2023; Ji, Yu, et al. 2023;\nDhuliawala et al. 2023; Suzgun and Kalai 2024), specialized fine-tuning (Tian, Mitchell, Yao,\net al. 2023; Razumovskaia et al. 2023; H. Zhang et al. 2023), factuality-focused decoding meth-\nods (Shi et al. 2023; Mallen et al. 2023; K. Li et al. 2024; Chuang et al. 2024), or external\ndatabase checks (Chern et al. 2023; Peng et al. 2023; Qin et al. 2023; Gou et al. 2024). These\nmethods have shown promising results in significantly reducing hallucinated content and enhanc-\ning the accuracy, reliability, and faithfulness of model outputs. However, we caution that these\napproaches are not without limitations.\nFor example, the effectiveness of RAG-based methods heavily relies on the quality of their re-\ntrieval mechanisms (Wu et al. 2024). Moreover, accurately parsing and understanding the content\nof input queries poses a challenge, especially when queries are inherently ambiguous or irrele-\nvant to the domain of focus (Tonmoy et al. 2024). Additionally, the task of retrieving relevant\n35\nand precise information from extensive corpora can be computationally demanding and resource-\nintensive, necessitating continuous updating and modification of knowledge databases to keep pace\nwith the latest information (Chen et al. 2023; Siriwardhana et al. 2023; Ram et al. 2023; Cheng et\nal. 2024). There may also be situations where the knowledge database might contain conflicting or\ncontradictory information, making it unclear which pieces of relevant information to extract (Wang\net al. 2023; Yu et al. 2023; Gao et al. 2023). For instance, when a legal case is overruled, or when\nthere is a circuit split on an issue, the retrieval module must have some mechanism to distinguish\noutdated or jursidictionally-irrelevant sources from apposite and binding law.\nFurthermore, methods for detecting hallucinations and evaluating their mitigation are them-\nselves not foolproof. Evaluation datasets and metrics may not always accurately reflect real-world\nperformance reliability (Ji, Lee, et al. 2023; Y . Zhang et al. 2023; Lucas et al. 2023). Biases could\nbe embedded within the evaluation dataset, or the automated metric employed to quantify hallu-\ncination may lack comprehensiveness or task-specificity (Kang, Blevins, and Zettlemoyer 2024).\nTherefore, it is far from clear whether these technical improvements will be able to truly solve the\nhallucination problem.\nFinally, we also emphasize that the challenges presented by legal hallucinations are not only\nempirical, but also normative. Although data-rich and moneyed players certainly stand at an ad-\nvantage when it comes to building hallucination-free legal LLMs for their own private use, it is not\nclear that even infinite resources can entirely solve the conceptual problems we diagnose. As we\ndiscuss in Section 2.3, model fidelity to the training corpus, model fidelity to the user’s prompt, and\nmodel fidelity to the facts of the world—i.e., the law—are normative commitments that stand in\ntension with each other, despite all being independently desirable technical properties of an LLM.\nUltimately, since hallucinations of some kind are generally inevitable at the token level (Kalai and\nVempala 2023; Xu, Jain, and Kankanhalli 2024), developers of legal LLMs will need to make\nchoices about which type(s) of hallucinations to minimize, and they should make these choices\ntransparent to their downstream users. Only then can individual litigants decide for themselves\nwhether the legal information they seek to obtain from LLMs is trustworthy or not.\n36\nTo aid in future research in this area, we release a test dataset of our queries and answers on the\nHuggingFace platform, which scholars can use to continue to evaluate LLMs as they advance in\nlegal sophistication.13 In the meantime, more experienced legal practitioners may find some value\nin consulting LLMs for certain tasks, but even these users should remain vigilant in their use, tak-\ning care to verify the accuracy of their prompts and the quality of their chosen LLM’s responses.\nSimilarly, legal scholars and educators seeking to use LLMs as automated research assistants (Liv-\nermore, Herron, and Rockmore 2024) or student aids (Choi and Schwarcz 2024) must be cautious\nto not inadvertently inject these LLMs’ subtle knowledge biases into their own applications. Put\ndifferently, our findings underscore the importance of human-centered AI. Responsible integration\nof LLMs into legal tasks mustaugment lawyers, researchers, and litigants and not, as Chief Justice\nRoberts has put it, risk “dehumanizing the law” (Roberts 2023, 5).\n13 https://huggingface.co/datasets/reglab/legal_hallucinations\n37\nTable 3: Hallucination rates across levels of the federal judiciary (low complexity tasks)\nSCOTUS(1794-2015; n=5000) USCOA(1895-2019; n=5000) USDC(1932-2019; n=5000)\nTask Prompt GPT 4 GPT 3.5 PaLM 2 Llama 2 GPT 4 GPT 3.5 PaLM 2 Llama 2 GPT 4 GPT 3.5 PaLM 2 Llama 2\nZero-shot0.204(0.006)0.004(0.001)0.054(0.003)0.303(0.006)0.237(0.006)0.003(0.001)0.025(0.002)0.157(0.005)0.174(0.005)0.001(0.001)0.016(0.002)0.240(0.006)Existence\nFew-shot0.181(0.005)0.029(0.002)0.029(0.002)1.000(0.000)0.129(0.005)0.018(0.002)0.005(0.001)1.000(0.000)0.048(0.003)0.004(0.001)0.006(0.001)1.000(0.000)\nZero-shot0.000(0.000)0.000(0.000)0.000(0.000)0.003(0.001)0.490(0.007)0.645(0.007)0.703(0.006)0.700(0.006)0.829(0.005)0.815(0.005)0.839(0.005)0.815(0.005)Court\nFew-shot0.000(0.000)0.000(0.000)0.000(0.000)0.005(0.001)0.491(0.007)0.641(0.007)0.703(0.006)0.679(0.007)0.831(0.005)0.870(0.005)0.842(0.005)0.870(0.005)\nZero-shot0.621(0.007)0.684(0.007)0.906(0.004)0.941(0.003)0.727(0.006)0.754(0.006)1.000(0.000)0.999(0.000)0.610(0.007)0.702(0.006)1.000(0.000)1.000(0.000)Citation\nFew-shot0.689(0.007)0.641(0.007)0.833(0.005)0.953(0.003)0.468(0.007)0.920(0.004)0.999(0.001)1.000(0.000)0.270(0.006)0.942(0.003)0.999(0.000)1.000(0.000)\nZero-shot0.799(0.006)0.796(0.006)0.816(0.005)0.884(0.005)0.954(0.003)0.965(0.003)0.988(0.002)0.991(0.001)0.922(0.004)0.911(0.004)0.988(0.002)0.987(0.002)Author\nFew-shot0.799(0.006)0.830(0.005)0.859(0.005)0.881(0.005)0.962(0.003)0.967(0.003)0.988(0.002)0.993(0.001)0.921(0.004)0.941(0.003)0.984(0.002)0.987(0.002)\nNote:Table reports estimated hallucination rates. Standard errors are shown in parentheses.\nTable 4: Hallucination rates across levels of the federal judiciary (moderate complexity tasks)\nSCOTUS(1794-2015; n=5000) USCOA(1895-2019; n=5000) USDC(1932-2019; n=5000)\nTask Prompt GPT 4 GPT 3.5 PaLM 2 Llama 2 GPT 4 GPT 3.5 PaLM 2 Llama 2 GPT 4 GPT 3.5 PaLM 2 Llama 2\nZero-shot0.399(0.007)0.499(0.001)0.500(0.000)0.536(0.007)0.494(0.005)0.500(0.000)0.500(0.000)0.493(0.008)- - - -\nDisposition\nFew-shot0.452(0.007)0.496(0.002)0.501(0.002)0.502(0.004)0.498(0.006)0.489(0.007)0.501(0.001)0.501(0.002)- - - -\nZero-shot0.312(0.007)0.229(0.006)0.993(0.001)0.920(0.004)0.001(0.000)0.000(0.000)0.999(0.000)0.993(0.001)0.001(0.000)0.000(0.000)0.994(0.001)0.964(0.003)Quotation\nFew-shot0.854(0.005)1.000(0.000)0.993(0.001)0.992(0.001)0.637(0.007)1.000(0.000)0.997(0.001)1.000(0.000)0.743(0.006)1.000(0.000)1.000(0.000)1.000(0.000)\nZero-shot0.922(0.004)0.937(0.003)0.985(0.002)0.991(0.001)0.934(0.004)0.978(0.002)0.997(0.001)0.999(0.001)0.956(0.003)0.870(0.005)0.995(0.001)0.999(0.000)Authority\nFew-shot0.828(0.005)0.916(0.004)0.953(0.003)0.993(0.001)0.958(0.003)0.976(0.002)0.993(0.001)0.999(0.000)0.928(0.004)0.956(0.003)0.994(0.001)0.997(0.001)\nZero-shot0.810(0.025)0.919(0.017)0.858(0.022)0.972(0.011)- - - - - - - -\nOverruling yeara\nFew-shot0.725(0.028)0.976(0.010)0.870(0.021)0.984(0.008)- - - - - - - -\na1810-2022 (n=279)Note:Table reports estimated hallucination rates. Standard errors are shown in parentheses.\n38\nTable 5: Hallucination rates across levels of the federal judiciary (high complexity tasks)\nSCOTUS(1794-2015; n=100) USCOA(1895-2019; n=100) USDC(1932-2019; n=100)\nTask Prompt GPT 4 GPT 3.5 PaLM 2 Llama 2 GPT 4 GPT 3.5 PaLM 2 Llama 2 GPT 4 GPT 3.5 PaLM 2 Llama 2\nZero-shot0.461(0.005)0.500(0.000)0.466(0.005)0.500(0.000)- - - - - - - -Doctrinal agreementa\nFew-shot0.449(0.007)0.458(0.004)0.453(0.006)0.500(0.000)- - - - - - - -\nFactual background Zero-shot0.460(0.050)0.700(0.046)0.950(0.022)0.830(0.038)0.750(0.043)0.810(0.039)0.880(0.032)0.880(0.032)0.710(0.045)0.670(0.047)0.880(0.032)0.880(0.032)\nProcedural posture Zero-shot0.470(0.050)0.650(0.048)0.740(0.044)0.850(0.036)0.660(0.047)0.600(0.049)0.680(0.047)0.830(0.038)0.730(0.044)0.550(0.050)0.810(0.039)0.890(0.031)\nSubsequent history Zero-shot0.080(0.027)0.220(0.041)0.810(0.039)0.770(0.042)0.450(0.050)0.360(0.048)0.750(0.043)0.620(0.049)0.480(0.050)0.300(0.046)0.730(0.044)0.670(0.047)\nCore legal question Zero-shot0.570(0.050)0.860(0.035)0.870(0.034)0.920(0.027)0.810(0.039)0.880(0.032)0.950(0.022)0.960(0.020)0.760(0.043)0.720(0.045)0.920(0.027)0.890(0.031)\nCentral holding Zero-shot0.600(0.049)0.730(0.044)0.780(0.041)0.920(0.027)0.830(0.038)0.870(0.034)0.950(0.022)0.950(0.022)0.770(0.042)0.730(0.044)0.930(0.026)0.840(0.037)\na1796-2005 (n=5000)Note:Table reports estimated hallucination rates. For all tasks except doctrinal agreement, this rate is only a lower bound on the true population rate. Standarderrors are shown in parentheses.\nTable 6: Hallucination rates across levels of the federal judiciary (contra-factual tasks)\nSCOTUS\n(1794-2015; n=1000)\nUSCOA\n(1895-2019; n=1000)\nTask Prompt GPT 4 GPT 3.5 PaLM 2 Llama 2 GPT 4 GPT 3.5 PaLM 2 Llama 2\nFalse dissent premise Zero-shot0.691\n(0.015)\n0.338\n(0.015)\n0.990\n(0.003)\n0.000\n(0.000)\n0.842\n(0.012)\n0.408\n(0.016)\n0.983\n(0.004)\n0.021\n(0.005)\nFalse overruling premise Zero-shot0.531\n(0.016)\n0.821\n(0.012)\n1.000\n(0.000)\n0.027\n(0.005) - - - -\nNote:Table reports estimated hallucination rates. Standard errors are shown in parentheses.\n39\nTable 7: Expected calibration error (ECE) across levels of the federal judiciary\nSCOTUS(1794-2015; n=100) USCOA(1895-2019; n=100) USDC(1932-2019; n=100)\nTask Prompt GPT 4 GPT 3.5 PaLM 2 Llama 2 GPT 4 GPT 3.5 PaLM 2 Llama 2 GPT 4 GPT 3.5 PaLM 2 Llama 2\nZero-shot0.126(0.005)0.008(0.001)0.119(0.004)0.262(0.006)0.124(0.005)0.007(0.001)0.182(0.003)0.117(0.005)0.074(0.004)0.004(0.000)0.183(0.003)0.178(0.006)Existence\nFew-shot0.149(0.005)0.038(0.002)0.041(0.002)0.998(0.000)0.073(0.005)0.064(0.002)0.014(0.001)0.998(0.000)0.024(0.003)0.053(0.002)0.038(0.001)0.996(0.000)\nZero-shot0.000(0.000)0.000(0.000)0.000(0.000)0.003(0.000)0.362(0.006)0.322(0.006)0.184(0.006)0.387(0.006)0.133(0.005)0.163(0.005)0.132(0.005)0.163(0.006)Court\nFew-shot0.000(0.000)0.001(0.000)0.000(0.000)0.009(0.001)0.345(0.006)0.355(0.006)0.161(0.006)0.452(0.008)0.145(0.005)0.099(0.004)0.094(0.005)0.099(0.005)\nZero-shot0.113(0.005)0.069(0.004)0.026(0.003)0.068(0.003)0.143(0.004)0.073(0.005)0.004(0.001)0.036(0.001)0.104(0.003)0.043(0.003)0.002(0.000)0.022(0.001)Citation\nFew-shot0.191(0.005)0.029(0.003)0.059(0.004)0.063(0.003)0.138(0.005)0.036(0.003)0.010(0.001)0.032(0.001)0.099(0.004)0.018(0.002)0.001(0.000)0.051(0.002)\nZero-shot0.443(0.006)0.208(0.006)0.130(0.005)0.419(0.006)0.349(0.005)0.094(0.003)0.131(0.002)0.583(0.004)0.302(0.005)0.228(0.005)0.063(0.002)0.305(0.004)Author\nFew-shot0.454(0.006)0.347(0.006)0.142(0.005)0.454(0.005)0.357(0.004)0.142(0.003)0.118(0.003)0.656(0.004)0.320(0.004)0.096(0.003)0.045(0.002)0.481(0.005)\nZero-shot0.203(0.007)0.431(0.007)0.291(0.008)0.199(0.007)0.170(0.007)0.557(0.006)0.382(0.006)0.148(0.006)- - - -Disposition\nFew-shot0.239(0.006)0.314(0.008)0.165(0.007)0.283(0.008)0.165(0.007)0.205(0.007)0.227(0.007)0.439(0.007)- - - -\nZero-shot0.308(0.025)0.246(0.022)0.116(0.019)0.510(0.019)- - - - - - - -Overruling yeara\nFew-shot0.377(0.025)0.680(0.022)0.154(0.019)0.754(0.018)- - - - - - - -\nZero-shot0.369(0.006)0.527(0.007)0.165(0.006)0.564(0.006)- - - - - - - -Doctrinal agreementb\nFew-shot0.319(0.007)0.409(0.008)0.152(0.007)0.548(0.007)- - - - - - - -\na1810-2022 (n=279)b1796-2005 (n=5000)Note:Table reports expected calibration error between empirical hallucination rates and estimated conditional probabilities. Conditional probabilities are esti-mated by sampling 10 responses from the model at temperature 1 and assessing their agreement with the model\"s greedy response. Bootstrapped standard errorsare shown in parentheses.\n40\nReferences\nAgrawal, Ayush, Mirac Suzgun, Lester Mackey, and Adam Tauman Kalai. 2023. Do Language\nModels Know When They’re Hallucinating References?https://doi.org/10.48550/arXiv.2305.\n18248.\nAmbrogi, Bob. 2023. “As Allen & Overy Deploys GPT-based Legal App Harvey Firmwide, Founders\nSay Other Firms Will Soon Follow.”LawSites.\nAnil, Rohan, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, et al. 2023. PaLM 2 Technical Report.https://doi.org/10.48550/arXiv.2305.\n10403.\nAsh, Elliott, Aniket Kesari, Suresh Naidu, Lena Song, and Dominik Stammbach. 2024. “Translat-\ning Legalese: Enhancing Public Understanding of Court Opinions with Legal Summarizers.”\nIn Proceedings of the Symposium on Computer Science and Law,136–157. CSLAW ’24. New\nYork, NY , USA: Association for Computing Machinery.\nAzaria, Amos, and Tom Mitchell. 2023. The Internal State of an LLM Knows When It’s Lying.\nhttps://doi.org/10.48550/arXiv.2304.13734.\nBar-Gill, Oren, Cass R Sunstein, and Inbal Talgam-Cohen. 2023. “Algorithmic Harm in Consumer\nMarkets.” Journal of Legal Analysis 15 (1): 1–47.\nBarocas, Solon, and Andrew D. Selbst. 2016. “Big Data’s Disparate Impact.” California Law Re-\nview 104 (3): 671–732.\nBen-Shahar, Omri. 2023. “Privacy Protection, At What Cost? Exploring the Regulatory Resistance\nto Data Technology in Auto Insurance.”Journal of Legal Analysis 15 (1): 129–157.\nBlack, Ryan C., and James F. Spriggs II. 2013. “The Citation and Depreciation of U.S. Supreme\nCourt Precedent.” Journal of Empirical Legal Studies 10 (2): 325–358.\n41\nBlair-Stanek, Andrew, Nils Holzenberger, and Benjamin Van Durme. 2023. “Can GPT-3 Perform\nStatutory Reasoning?” In Proceedings of the Nineteenth International Conference on Artifi-\ncial Intelligence and Law. Braga, Portugal: Association for Computing Machinery.\nBommasani, Rishi, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,\nMichael S. Bernstein, et al. 2022. On the Opportunities and Risks of Foundation Models.\nhttps://doi.org/10.48550/arXiv.2108.07258.\nBrown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, et al. 2020. Language Models Are Few-Shot Learners. https://doi.org/\n10.48550/arXiv.2005.14165.\nCao, Meng, Yue Dong, and Jackie Cheung. 2022. “Hallucinated but Factual! Inspecting the Fac-\ntuality of Hallucinations in Abstractive Summarization.” In Proceedings of the 60th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), edited\nby Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, 3340–3354. Dublin, Ireland:\nAssociation for Computational Linguistics.\nCao, Ziqiang, Furu Wei, Wenjie Li, and Sujian Li. 2018. “Faithful to the Original: Fact Aware\nNeural Abstractive Summarization.” Proceedings of the AAAI Conference on Artificial Intel-\nligence 32 (1).\nCarlini, Nicholas, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan\nZhang. 2022. Quantifying Memorization Across Neural Language Models.https://doi.org/10.\n48550/arXiv.2202.07646.\nCaselaw Access Project. 2023. Caselaw Access Project. https://case.law.\nChann, Seherman. 2023. Non-Determinism in GPT-4 Is Caused by Sparse MoE. https://152334H.\ngithub.io/blog/non-determinism-in-gpt-4/.\nChen, Jiawei, Hongyu Lin, Xianpei Han, and Le Sun. 2023. Benchmarking Large Language Mod-\nels in Retrieval-Augmented Generation. https://doi.org/10.48550/arXiv.2309.01431.\n42\nCheng, Xin, Di Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, and Rui Yan. 2024. “Lift Yourself\nUp: Retrieval-augmented Text Generation with Self Memory.” Advances in Neural Informa-\ntion Processing Systems 36.\nChern, I, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Gra-\nham Neubig, Pengfei Liu, et al. 2023. FacTool: Factuality Detection in Generative AI–A Tool\nAugmented Framework for Multi-Task and Multi-Domain Scenarios. https : / / doi . org / 10 .\n48550/arXiv.2307.13528.\nChien, Colleen V ., Miriam Kim, Raj Akhil, and Rohit Rathish. 2024. “How Generative AI Can\nHelp Address the Access to Justice Gap Through the Courts.” Loyola of Los Angeles Law\nReview, forthcoming.\nChoi, Jonathan H., Kristin E. Hickman, Amy Monahan, and Daniel Schwarcz. 2022. “ChatGPT\nGoes to Law School.” Journal of Legal Education 71 (3): 387–400.\nChoi, Jonathan H., and Daniel Schwarcz. 2024. “AI Assistance in Legal Analysis: An Empirical\nStudy.” Journal of Legal Education, forthcoming. https://doi.org/10.2139/ssrn.4539836.\nChuang, Yung-Sung, Yujia Xie, Hongyin Luo, Yoon Kim, James R. Glass, and Pengcheng He.\n2024. “DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Mod-\nels.” In The Twelfth International Conference on Learning Representations.\nCongress.gov. 2023. Table of Supreme Court Decisions Overruled by Subsequent Decisions.https:\n//constitution.congress.gov/resources/decisions-overruled/.\nCreel, Kathleen, and Deborah Hellman. 2022. “The Algorithmic Leviathan: Arbitrariness, Fair-\nness, and Opportunity in Algorithmic Decision-Making Systems.” Canadian Journal of Phi-\nlosophy 52 (1): 26–43.\nCui, Jiaxi, Zongjian Li, Yang Yan, Bohua Chen, and Li Yuan. 2023.ChatLaw: Open-Source Legal\nLarge Language Model with Integrated External Knowledge Bases.https://doi.org/10.48550/\narXiv.2306.16092.\n43\nCushman, Jack, Matthew Dahl, and Michael Lissner. 2021. “Eyecite: A Tool for Parsing Legal\nCitations.” Journal of Open Source Software 6 (66): 3617.\nDell’Acqua, Fabrizio, Edward McFowland, Ethan R. Mollick, Hila Lifshitz-Assaf, Katherine Kel-\nlogg, Saran Rajendran, Lisa Krayer, François Candelon, and Karim R. Lakhani. 2023. Nav-\nigating the Jagged Technological Frontier: Field Experimental Evidence of the Effects of AI\non Knowledge Worker Productivity and Quality.https://doi.org/10.2139/ssrn.4573321.\nDeroy, Aniket, Kripabandhu Ghosh, and Saptarshi Ghosh. 2023. How Ready Are Pre-trained Ab-\nstractive Models and LLMs for Legal Case Judgement Summarization? https://doi.org/10.\n48550/arXiv.2306.01248.\nDhuliawala, Shehzaad, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz,\nand Jason Weston. 2023. “Chain-of-Verification Reduces Hallucination in Large Language\nModels.” arXiv preprint arXiv:2309.11495, https://doi.org/10.48550/arXiv.2309.11495.\nDraper, Chris, and Nicky Gillibrand. 2023. “The Potential for Jurisdictional Challenges to AI or\nLLM Training Datasets.” In Proceedings of the ICAIL 2023 Workshop on Artificial Intelli-\ngence for Access to Justice. Braga, Portugal: CEUR Workshop Proceedings.\nDworkin, Ronald. 1986. Law’s Empire. Cambridge, MA: Harvard University Press.\nEloundou, Tyna, Sam Manning, Pamela Mishkin, and Daniel Rock. 2023. GPTs Are GPTs: An\nEarly Look at the Labor Market Impact Potential of Large Language Models.https://doi.org/\n10.48550/arXiv.2303.10130.\nEngel, Christoph, and Nina Grgi ´c-Hlaˇca. 2021. “Machine Advice with a Warning about Ma-\nchine Limitations: Experimentally Testing the Solution Mandated by the Wisconsin Supreme\nCourt.” Journal of Legal Analysis 13 (1): 284–340.\nEngstrom, David Freeman, and Daniel E. Ho. 2020. “Algorithmic Accountability in the Adminis-\ntrative State.” Yale Journal on Regulation37 (3): 800–854.\n44\nEngstrom, David Freeman, Daniel E. Ho, Catherine M. Sharkey, and Mariano-Florentino Cuéllar.\n2020. Government by Algorithm: Artificial Intelligence in Federal Administrative Agencies.\nTechnical report. Administrative Conference of the United States.\nFei, Zhiwei, Xiaoyu Shen, Dawei Zhu, Fengzhe Zhou, Zhuo Han, Songyang Zhang, Kai Chen,\nZongwen Shen, and Jidong Ge. 2023. LawBench: Benchmarking Legal Knowledge of Large\nLanguage Models. https://doi.org/10.48550/arXiv.2309.16289.\nFeijo, Diego de Vargas, and Viviane P. Moreira. 2023. “Improving Abstractive Summarization of\nLegal Rulings through Textual Entailment.”Artificial Intelligence and Law 31 (1): 91–113.\nFeng, Shangbin, Weijia Shi, Yike Wang, Wenxuan Ding, Vidhisha Balachandran, and Yulia Tsvetkov.\n2024. Don’t Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collab-\noration. https://doi.org/10.48550/arXiv.2402.00367.\nFowler, James H., Timothy R. Johnson, James F. Spriggs II, Sangick Jeon, and Paul J. Wahlbeck.\n2007. “Network Analysis and the Law: Measuring the Legal Importance of Precedents at the\nU.S. Supreme Court.” Political Analysis 15 (3): 324–346.\nGao, Yunfan, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and\nHaofen Wang. 2023.Retrieval-Augmented Generation for Large Language Models: A Survey.\nhttps://doi.org/10.48550/arXiv.2312.10997.\nGillis, Talia B., and Jann L. Spiess. 2019. “Big Data and Discrimination.” The University of\nChicago Law Review 86 (2): 459–488.\nGou, Zhibin, Zhihong Shao, Yeyun Gong, yelong shen, Yujiu Yang, Nan Duan, and Weizhu Chen.\n2024. “CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing.”\nIn The Twelfth International Conference on Learning Representations.\n45\nGuha, Neel, Julian Nyarko, Daniel E. Ho, Christopher Re, Adam Chilton, Aditya Narayana, Alex\nChohlas-Wood, et al. 2023. “LegalBench: A Collaboratively Built Benchmark for Measur-\ning Legal Reasoning in Large Language Models.” In Thirty-seventh Conference on Neural\nInformation Processing Systems Datasets and Benchmarks Track.\nHenderson, Peter, Tatsunori Hashimoto, and Mark Lemley. 2023. “Where’s the Liability in Harm-\nful AI Speech?” Journal of Free Speech Law3 (2): 589–650.\nHenderson, Peter, Mark S. Krass, Lucia Zheng, Neel Guha, Christopher D. Manning, Dan Jurafsky,\nand Daniel E. Ho. 2022. Pile of Law: Learning Responsible Data Filtering from the Law and\na 256GB Open-Source Legal Dataset. https://doi.org/10.48550/arXiv.2207.00220.\nJi, Ziwei, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang,\nAndrea Madotto, and Pascale Fung. 2023. “Survey of Hallucination in Natural Language\nGeneration.” ACM Computing Surveys 55 (12): 248:1–248:38.\nJi, Ziwei, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale Fung. 2023. “Towards\nMitigating LLM Hallucination via Self Reflection.” In Findings of the Association for Com-\nputational Linguistics: EMNLP 2023, edited by Houda Bouamor, Juan Pino, and Kalika Bali,\n1827–1843. Singapore: Association for Computational Linguistics.\nJones, Erik, and Jacob Steinhardt. 2022.Capturing Failures of Large Language Models via Human\nCognitive Biases. https://doi.org/10.48550/arXiv.2202.12299.\nKadavath, Saurav, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, et al. 2022. Language Models (Mostly) Know What They Know. https://doi.org/10.\n48550/arXiv.2207.05221.\nKalai, Adam Tauman, and Santosh S. Vempala. 2023.Calibrated Language Models Must Halluci-\nnate. https://doi.org/10.48550/arXiv.2311.14648.\nKang, Haoqiang, Terra Blevins, and Luke Zettlemoyer. 2024.Comparing Hallucination Detection\nMetrics for Multilingual Generation. https://doi.org/10.48550/arXiv.2402.10496.\n46\nKatz, Daniel Martin, Michael James Bommarito, Shang Gao, and Pablo Arredondo. 2023. GPT-4\nPasses the Bar Exam. https://doi.org/10.2139/ssrn.4389233.\nKing, Jennifer, Daniel Ho, Arushi Gupta, Victor Wu, and Helen Webley-Brown. 2023. “The Privacy-\nBias Tradeoff: Data Minimization and Racial Disparity Assessments in U.S. Government.” In\nProceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency,\n492–505. FAccT ’23. Chicago, IL, USA: Association for Computing Machinery.\nKleinberg, Jon, Jens Ludwig, Sendhil Mullainathan, and Cass R Sunstein. 2018. “Discrimination\nin the Age of Algorithms.” Journal of Legal Analysis 10 (1): 113–174.\nKleinberg, Jon, and Manish Raghavan. 2021. “Algorithmic Monoculture and Social Welfare.”Pro-\nceedings of the National Academy of Sciences 118 (22).\nKrishna, Kalpesh, Aurko Roy, and Mohit Iyyer. 2021. “Hurdles to Progress in Long-form Ques-\ntion Answering.” In Proceedings of the 2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, 4940–4957.\nAssociation for Computational Linguistics.\nKuersten, Ashlyn K., and Susan B. Haire. 2011.Update to the Appeals Courts Database (1997–2002).\nhttps://artsandsciences.sc.edu/poli/juri/appct.htm.\nLee, Minhyeok. 2023. “A Mathematical Investigation of Hallucination and Creativity in GPT Mod-\nels.” Mathematics 11 (10): 2320.\nLee, Nayeon, Wei Ping, Peng Xu, Mostofa Patwary, Pascale Fung, Mohammad Shoeybi, and Bryan\nCatanzaro. 2023. Factuality Enhanced Language Models for Open-Ended Text Generation.\nhttps://doi.org/10.48550/arXiv.2206.04624.\nLei, Deren, Yaxi Li, Mingyu Wang, Vincent Yun, Emily Ching, Eslam Kamal, et al. 2023. Chain\nof Natural Language Inference for Reducing Large Language Model Ungrounded Hallucina-\ntions. https://doi.org/10.48550/arXiv.2310.03951.\n47\nLemley, Mark A., and Bryan Casey. 2019. “Remedies for Robots.”The University of Chicago Law\nReview 86 (5): 1311–1396.\nLi, Junyi, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023. HaluEval: A\nLarge-Scale Hallucination Evaluation Benchmark for Large Language Models. https://doi.\norg/10.48550/arXiv.2305.11747.\nLi, Kenneth, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. 2024.Inference-\ntime Intervention: Eliciting Truthful Answers from a Language model.\nLin, Stephanie, Jacob Hilton, and Owain Evans. 2022.TruthfulQA: Measuring How Models Mimic\nHuman Falsehoods. https://doi.org/10.48550/arXiv.2109.07958.\nLivermore, Michael A., Felix Herron, and Daniel Rockmore. 2024. “Language Model Interpretabil-\nity and Empirical Legal Studies.” Journal of Institutional and Theoretical Economics, forth-\ncoming.\nLucas, Jason, Adaku Uchendu, Michiharu Yamashita, Jooyoung Lee, Shaurya Rohatgi, and Dong-\nwon Lee. 2023. “Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting\nElusive Disinformation.” In Proceedings of the 2023 Conference on Empirical Methods in\nNatural Language Processing,edited by Houda Bouamor, Juan Pino, and Kalika Bali, 14279–\n14305. Singapore: Association for Computational Linguistics.\nLuft, Joseph, and Harrington Ingham. 1955. “The Johari Window as a Graphic Model of Interper-\nsonal Awareness.” InProceedings of the Western Training Laboratory in Group Development.\nUniversity of California, Los Angeles, Extension Office.\nMallen, Alex, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi.\n2023. “When Not to Trust Language Models: Investigating Effectiveness of Parametric and\nNon-Parametric Memories.” InProceedings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),9802–9822.\n48\nManakul, Potsawee, Adian Liusie, and Mark J. F. Gales. 2023. SelfCheckGPT: Zero-Resource\nBlack-Box Hallucination Detection for Generative Large Language Models. https://doi.org/\n10.48550/arXiv.2303.08896.\nMaynez, Joshua, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. “On Faithfulness\nand Factuality in Abstractive Summarization.” In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics, 1906–1919. Association for Computational\nLinguistics.\nMayson, Sandra G. 2019. “Bias In, Bias Out.” The Yale Law Journal 128 (8): 2122–2473.\nMin, Sewon, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained Atomic Evalua-\ntion of Factual Precision in Long Form Text Generation. https://doi.org/10.48550/arXiv.\n2305.14251.\nMündler, Niels, Jingxuan He, Slobodan Jenko, and Martin Vechev. 2023. Self-Contradictory Hal-\nlucinations of Large Language Models: Evaluation, Detection and Mitigation. https://doi.\norg/10.48550/arXiv.2305.15852.\nNay, John J., David Karamardian, Sarah B. Lawsky, Wenting Tao, Meghana Bhat, Raghav Jain,\nAaron Travis Lee, Jonathan H. Choi, and Jungo Kasai. 2023. Large Language Models as Tax\nAttorneys: A Case Study in Legal Capabilities Emergence. https://doi.org/10.48550/arXiv.\n2306.07075.\nOpenAI. 2023a. GPT-4 Technical Report.https://doi.org/10.48550/arXiv.2303.08774.\n. 2023b. Introducing ChatGPT. https://openai.com/blog/chatgpt.\nPeng, Baolin, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, et al.\n2023. Check Your Facts and Try Again: Improving Large Language Models with External\nKnowledge and Automated Feedback.https://doi.org/10.48550/arXiv.2302.12813.\n49\nPerlman, Andrew. 2023. “The Implications of ChatGPT for Legal Services and Society.”The Prac-\ntice, no. March/April.\nQin, Yujia, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei\nHuang, Chaojun Xiao, Chi Han, et al. 2023. Tool Learning with Foundation Models. https:\n//doi.org/10.48550/arXiv.2304.08354.\nRam, Ori, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown,\nand Yoav Shoham. 2023. “In-context Retrieval-Augmented Language Models.”Transactions\nof the Association for Computational Linguistics 11:1316–1331.\nRazumovskaia, Evgeniia, Ivan Vuli ´c, Pavle Markovi´c, Tomasz Cichy, Qian Zheng, Tsung-Hsien\nWen, and Paweł Budzianowski. 2023. Dial BeInfo for Faithfulness: Improving Factuality of\nInformation-Seeking Dialogue via Behavioural Fine-Tuning. https://doi.org/10.48550/arXiv.\n2311.09800.\nRoberts, John G. 2023. 2023 Year-End Report on the Federal Judiciary.https://www.supremecou\nrt.gov/publicinfo/year-end/2023year-endreport.pdf.\nRodgers, Ian, John Armour, and Mari Sako. 2023. “How Technology Is (or Is Not) Transforming\nLaw Firms.” Annual Review of Law and Social Science 19 (1): 299–317.\nRomoser, James. 2023. “No, Ruth Bader Ginsburg Did Not Dissent in Obergefell — and Other\nThings ChatGPT Gets Wrong about the Supreme Court.” SCOTUSblog.\nSavelka, Jaromir, Kevin D. Ashley, Morgan A. Gray, Hannes Westermann, and Huihui Xu. 2023.\nExplaining Legal Concepts with Augmented Large Language Models (GPT-4). https://doi.\norg/10.48550/arXiv.2306.09525.\nSharma, Mrinank, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R. Bow-\nman, Newton Cheng, et al. 2023. Towards Understanding Sycophancy in Language Models.\nhttps://doi.org/10.48550/arXiv.2310.13548.\n50\nShi, Weijia, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer, and Scott Wen-tau\nYih. 2023. Trusting Your Evidence: Hallucinate Less with Context-aware Decoding. https:\n//doi.org/10.48550/arXiv.2305.14739.\nShuster, Kurt, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. “Retrieval Aug-\nmentation Reduces Hallucination in Conversation.” In Findings of the Association for Com-\nputational Linguistics: EMNLP 2021, 3784–3803. Punta Cana, Dominican Republic: Associ-\nation for Computational Linguistics.\nSi, Chenglei, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Lee Boyd-\nGraber, and Lijuan Wang. 2023. “Prompting GPT-3 To Be Reliable.” In The Eleventh Inter-\nnational Conference on Learning Representations.\nSimshaw, Drew. 2022. “Access to A.I. Justice: Avoiding an Inequitable Two-Tiered System of\nLegal Services.” Yale Journal of Law & Technology24:150–226.\nSiriwardhana, Shamane, Rivindu Weerasekera, Elliott Wen, Tharindu Kaluarachchi, Rajib Rana,\nand Suranga Nanayakkara. 2023. “Improving the Domain Adaptation of Retrieval Augmented\nGeneration (RAG) Models for Open Domain Question Answering.” Transactions of the As-\nsociation for Computational Linguistics 11:1–17.\nSolow-Niederman, Alicia. 2020. “Administering Artificial Intelligence.” Southern California Law\nReview 93 (4): 633–696.\nSonger, Donald R. 2008. The United States Courts of Appeals Database, 1925–1996.\nSpaeth, Harold J., Lee Epstein, Andrew D. Martin, Jeffrey A. Segal, Theodore J. Ruger, and Sara\nC. Benesh. 2022. 2022 Supreme Court Database, Version 2022 Release 01. http://supremeco\nurtdatabase.org/.\nSuri, Gaurav, Lily R. Slater, Ali Ziaee, and Morgan Nguyen. 2023. Do Large Language Models\nShow Decision Heuristics Similar to Humans? A Case Study Using GPT-3.5. https://doi.org/\n10.48550/arXiv.2305.04400.\n51\nSuzgun, Mirac, and Adam Tauman Kalai. 2024. Meta-prompting: Enhancing Language Models\nwith Task-agnostic Scaffolding. https://doi.org/10.48550/arXiv.2401.12954.\nTan, Jinzhe, Hannes Westermann, and Karim Benyekhlef. 2023. “ChatGPT as an Artificial Lawyer?”\nIn Proceedings of the ICAIL 2023 Workshop on Artificial Intelligence for Access to Justice.\nBraga, Portugal: CEUR Workshop Proceedings.\nTian, Katherine, Eric Mitchell, Huaxiu Yao, Christopher D. Manning, and Chelsea Finn. 2023.\nFine-Tuning Language Models for Factuality.https://doi.org/10.48550/arXiv.2311.08401.\nTian, Katherine, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea\nFinn, and Christopher D. Manning. 2023. Just Ask for Calibration: Strategies for Eliciting\nCalibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback.htt\nps://doi.org/10.48550/arXiv.2305.14975.\nTito, Joel. 2017. How AI Can Improve Access to Justice. https://www.centreforpublicimpact.org/\ninsights/joel-tito-ai-justice.\nTonmoy, SM, SM Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, and Amitava Das.\n2024. A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language\nModels. https://doi.org/10.48550/arXiv.2401.01313.\nTouvron, Hugo, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, et al. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models.\nhttps://doi.org/10.48550/arXiv.2307.09288.\nTrozze, Arianna, Toby Davies, and Bennett Kleinberg. 2023. Large Language Models in Cryp-\ntocurrency Securities Cases: Can ChatGPT Replace Lawyers? https://doi.org/10.48550/\narXiv.2308.06032.\nTversky, Amos, and Daniel Kahneman. 1974. “Judgment under Uncertainty: Heuristics and Bi-\nases.” Science 185 (4157): 1124–1131.\n52\nvan Deemter, Kees. 2024. “The Pitfalls of Defining Hallucination.” Computational Linguistics,\nforthcoming.\nV olokh, Eugene. 2023. “Large Libel Models? Liability for AI Output.”Journal of Free Speech Law\n3 (2): 489–558.\nWang, Cunxiang, Xiaoze Liu, Yuanhao Yue, Xiangru Tang, Tianhang Zhang, Cheng Jiayang, Yun-\nzhi Yao, Wenyang Gao, Xuming Hu, Zehan Qi, et al. 2023. Survey on Factuality in Large\nLanguage Models: Knowledge, Retrieval and Domain-Specificity. https://doi.org/10.48550/\narXiv.2310.07521.\nWei, Jerry, Da Huang, Yifeng Lu, Denny Zhou, and Quoc V . Le. 2023. Simple Synthetic Data\nReduces Sycophancy in Large Language Models.https://doi.org/10.48550/arXiv.2308.03958.\nWeiser, Benjamin. 2023. “Here’s What Happens When Your Lawyer Uses ChatGPT.” The New\nYork Times.\nWittgenstein, Ludwig. 1998 [1921]. Tractatus Logico-Philosophicus. Translated by C. K. Ogden.\nDover.\nWu, Kevin, Eric Wu, Ally Cassasola, Angela Zhang, Kevin Wei, Teresa Nguyen, Sith Riantawan,\nPatricia Shi Riantawan, Daniel E. Ho, and James Zou. 2024.How Well Do LLMs Cite Relevant\nMedical References? An Evaluation Framework and Analyses. https://doi.org/10.48550/\narXiv.2402.02008.\nXiong, Miao, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. 2023.\nCan LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in\nLLMs. https://doi.org/10.48550/arXiv.2306.13063.\nXu, Weijia, Sweta Agrawal, Eleftheria Briakou, Marianna J. Martindale, and Marine Carpuat.\n2023. Understanding and Detecting Hallucinations in Neural Machine Translation via Model\nIntrospection. https://doi.org/10.48550/arXiv.2301.07779.\n53\nXu, Ziwei, Sanjay Jain, and Mohan Kankanhalli. 2024. Hallucination Is Inevitable: An Innate\nLimitation of Large Language Models. https://doi.org/10.48550/arXiv.2401.11817.\nYin, Zhangyue, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, and Xuanjing Huang. 2023.\nDo Large Language Models Know What They Don’t Know? https://doi.org/10.48550/arXiv.\n2305.18153.\nYu, Wenhao, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, and Dong Yu. 2023.\nChain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models.https://doi.\norg/10.48550/arXiv.2311.09210.\nZhang, Hanning, Shizhe Diao, Yong Lin, Yi R Fung, Qing Lian, Xingyao Wang, Yangyi Chen,\nHeng Ji, and Tong Zhang. 2023. R-Tuning: Teaching Large Language Models to Refuse Un-\nknown Questions. https://doi.org/10.48550/arXiv.2311.09677.\nZhang, Yue, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, et al.\n2023. Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models.\nhttps://doi.org/10.48550/arXiv.2309.01219.\nZhang, Yunfeng, Q. Vera Liao, and Rachel K. E. Bellamy. 2020. “Effect of Confidence and Expla-\nnation on Accuracy and Trust Calibration in AI-assisted Decision Making.” In Proceedings\nof the 2020 Conference on Fairness, Accountability, and Transparency, 295–305. FAT* ’20.\nNew York, NY , USA: Association for Computing Machinery.\n54\nOnline Appendix\n55\nA Data Sources\nIn Section 4.1 in the main manuscript, we describe the data and sampling strategy that we use\nto construct our queries. For clarity, Appendix Table 1 links these data to each task. Our exact\nprocedure for sampling, merging, and aggregating across these datasets is available in our GitHub\nrepository.\nTable 1: Data used to construct each task\nComplexity Task Data source\nExistence CAP (2023); SCDB (Spaeth et al. 2022)\nCourt CAP; SCDB\nCitation CAP; SCDB\nLow\nAuthor CAP; SCDB\nDisposition CAP; SCDB; ACDB (Songer 2008; Kuersten and Haire 2011)\nQuotation CAP; SCDB\nAuthority CAP; SCDB\nModerate\nOverruling year SCDB; Library of Congress (2023)\nDoctrinal agreement Shepard’s (Fowler et al. 2007; Black and Spriggs 2013)\nFactual background CAP; SCDB\nProcedural posture CAP; SCDB\nSubsequent history CAP; SCDB\nCore legal question CAP; SCDB\nHigh\nCentral holding CAP; SCDB\nB Prompt Templates\nThe full zero-shot and few-shot prompt templates for all of our queries are shown in Appendix\nFigures 5 to 19. The few-shot examples presented are those used for the SCOTUS queries; appro-\npriate cases from the other levels of the judiciary are used in the USCOA and USDC versions.\n56\nTable 2: Intercoder reliability\nCoders κ\nCoder 1 vs. GPT 4 0.87\nCoder 2 vs. GPT 4 0.77\nCoder 1 vs. Coder 2 0.79\nC Contradiction Detection Approach\nIn Section 4.3 in the main manuscript, we describe our strategy for assessing hallucinations\nfor our reference-free tasks. Briefly, because we do not have access to ground-truth labels for\nthese tasks, we exploit the stochastic behavior of LLMs at higher temperatures and check for\ncontradictions in their responses to repeated queries. In Appendix Figure 20, we share the template\nfor the contradiction elicitation query that we send to GPT 4 to perform this contradiction labeling\nat scale. We frame our contradiction check as an entailment task (Wang et al. 2021) as we find that\nthis prompting strategy is the most performant in our setting.\nTo confirm that GPT 4 is able to reliably detect the contradictions that we envision, we cross-\ncheck GPT 4’s conclusions against our own expert knowledge on a subsample of our data. Specif-\nically, for 100 of our queries, two of us independently recoded the LLMs’ responses for contra-\ndictions. We report Cohen’s κ coefficient (Cohen 1960) for our agreement with GPT-4 on these\nqueries in Appendix Table 2. In general, aκ value greater than 0.80 is considered “almost perfect”\nagreement, and one greater than 0.60 is considered “substantial” agreement (Landis and Koch\n1977). Our κ values suggest that we are well-justified to use GPT 4 for contradiction detection;\nindeed, one of us agrees with GPT 4 more than with the other human coder.\nTo give the reader a sense of GPT 4’s reasoning abilities in this setting, in Appendix Figures 21\nand 22 we share some examples of its reasoning process and contradiction conclusions.\n57\nD Expected Calibration Error (ECE)\nD.1 Estimation Approach\nIn Section 5.3 in the main manuscript, we estimate the expected calibration error (ECE) of\nour LLMs for each of our tasks. Studying model calibration directly is not possible in our setup\nbecause we do not always observe our LLMs’ conditional probability distributions, which we\nrequire in order to determine whether they are in fact correlated with their response accuracies.\nSpecifically, three of the models that we evaluate—OpenAI’s ChatGPT 4, OpenAI’s ChatGPT\n3.5, and Google’s PaLM 2—are closed-source and do not expose this information to the user. To\novercome this hurdle, we instead estimate the distributions by drawing K samples from the model\nat temperature 1 and comparing those responses to its greedy response as follows:\nbPr[response|·] = 1\nK\nX\nk∈K\n1 [response = f1(·)(k)] (3)\nThen, for each task, we calculate the ECE of the LLM. Conceptually, the ECE represents the\naverage difference between the LLM’s confidence in the accuracy of its responses and the empirical\nfrequency of its correct, non-hallucinated responses. Formally, where X = bPr[response|·] (cf.\nEquation 3 above) and Y = 1 [response = response′] (cf. Equation 1 in the main manuscript), we\nestimate:\nECE = E [|X − E [Y |X]|] (4)\nFollowing Kumar, Liang, and Ma (2020), we use a plug-in estimator that bins the data into 10\nequally-sized slices and subtracts off an approximated bias term.\nD.2 Temperature Scaling\nAs we report in Section 5.3 in the main manuscript, our results suggest that out of the box, the\nLLMs we evaluate are not well-calibrated on legal queries. However, a model that is uncalibrated\n58\nFigure 1: Rescaled calibration curves by LLM, all resource-aware tasks pooled.\nunder its unscaled probability distribution is not necessarily uncalibratedtout court; the purpose of\na LLM’s temperature parameter, after all, is to allow an sophisticated user to adjust the distribution\nas needed. To explore whether such temperature scaling could indeed affect our results, here we\nperform ex post Platt scaling (Guo et al. 2017) on the raw distribution and check for improvements\nin the measured ECE.\nAppendix Figure 1 visualizes the rescaled ECE for each of our LLMs, Appendix Table 3 re-\nports the task-level results, and Appendix Table 4 summarizes the numerical gains. As expected,\nrescaling generally improves the ECE, especially for GPT 4 and Llama 2. However, the rescal-\ning procedure is not perfect: GPT4, GPT 3.5, and PaLM 2 remain relatively uncalibrated in the\n[0.2, 0.4] confidence interval. And although the pooled ECE of Llama 2 improves substantially, this\nis due to the entire distribution being compressed into the [0.0, 0.3] interval—the rescaled Llama 2\nmodel is simply not confident in any of its responses. Overall, these results confirm our conclusion\nin Section 5.3 that LLMs face calibration challenges on legal knowledge queries, though they do\nsuggest that ex post rescaling can help.\n59\nTable 3: Temperature-scaled expected calibration error (ECE) across levels of the federal judiciary\nSCOTUS(1794-2015; n=100) USCOA(1895-2019; n=100) USDC(1932-2019; n=100)\nTask Prompt GPT 4 GPT 3.5 PaLM 2 Llama 2 GPT 4 GPT 3.5 PaLM 2 Llama 2 GPT 4 GPT 3.5 PaLM 2 Llama 2\nZero-shot0.002(0.003)0.000(0.001)0.000(0.002)0.001(0.005)0.003(0.004)0.000(0.000)0.000(0.001)0.006(0.004)0.003(0.004)0.000(0.000)0.000(0.001)0.001(0.004)Existence\nFew-shot0.001(0.003)0.017(0.002)0.004(0.002)0.000(0.000)0.023(0.004)0.000(0.001)0.000(0.001)0.000(0.000)0.013(0.002)0.000(0.001)0.000(0.001)0.996(0.000)\nZero-shot0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.002(0.003)0.003(0.004)0.015(0.005)0.052(0.005)0.018(0.004)0.043(0.005)0.067(0.005)0.043(0.004)Court\nFew-shot0.000(0.000)0.001(0.000)0.000(0.000)0.000(0.000)0.010(0.004)0.003(0.004)0.018(0.005)0.011(0.005)0.019(0.003)0.017(0.004)0.021(0.005)0.017(0.003)\nZero-shot0.064(0.004)0.073(0.004)0.016(0.003)0.026(0.003)0.000(0.002)0.007(0.003)0.000(0.000)0.000(0.000)0.001(0.001)0.000(0.001)0.000(0.000)0.022(0.001)Citation\nFew-shot0.016(0.004)0.100(0.004)0.024(0.003)0.030(0.002)0.000(0.001)0.009(0.002)0.000(0.000)0.000(0.000)0.001(0.001)0.002(0.001)0.000(0.000)0.000(0.000)\nZero-shot0.003(0.003)0.028(0.005)0.011(0.004)0.000(0.003)0.001(0.002)0.000(0.001)0.000(0.001)0.000(0.001)0.000(0.002)0.000(0.002)0.000(0.001)0.000(0.001)Author\nFew-shot0.003(0.003)0.003(0.003)0.002(0.003)0.001(0.002)0.000(0.002)0.000(0.001)0.000(0.001)0.000(0.001)0.002(0.002)0.005(0.002)0.001(0.001)0.000(0.001)\nZero-shot0.003(0.004)0.000(0.004)0.000(0.004)0.001(0.004)0.000(0.004)0.000(0.004)0.000(0.004)0.001(0.004)- - - -Disposition\nFew-shot0.000(0.004)0.000(0.005)0.000(0.004)0.000(0.005)0.000(0.004)0.001(0.004)0.000(0.004)0.000(0.004)- - - -\nZero-shot0.006(0.012)0.005(0.011)0.017(0.012)0.000(0.006)- - - - - - - -Overruling yeara\nFew-shot0.042(0.016)0.000(0.006)0.031(0.016)0.000(0.005)- - - - - - - -\nZero-shot0.002(0.004)0.000(0.004)0.001(0.004)0.000(0.004)- - - - - - - -Doctrinal agreementb\nFew-shot0.001(0.004)0.000(0.004)0.001(0.004)0.000(0.004)- - - - - - - -\na1810-2022 (n=279)b1796-2005 (n=5000)Note:Table reports temperature-scaled expected calibration error between empirical hallucination rates and estimated conditional probabilities. Conditionalprobabilities are estimated by sampling 10 responses from the model at temperature 1 and assessing their agreement with the model\"s greedy response. Boot-strapped standard errors are shown in parentheses.\nTable 4: Scaled and unscaled ECE\nLLM ECE (unscaled) ECE (scaled)\nGPT 4 0.190 0.033\nGPT 3.5 0.099 0.051\nPaLM 2 0.057 0.049\nLlama 2 0.421 0.035\n60\nTable 5: Hallucination rates across levels of the federal judiciary (fake existence task)\nSCOTUS(1794-2015; n=1000) USCOA(1895-2019; n=1000) USDC(1932-2019; n=5000)\nTask Prompt GPT 4 GPT 3.5 PaLM 2 Llama 2 GPT 4 GPT 3.5 PaLM 2 Llama 2 GPT 4 GPT 3.5 PaLM 2 Llama 2\nFalse existence Zero-shot0.000(0.000)0.661(0.015)0.171(0.012)0.000(0.000)0.034(0.006)0.799(0.013)0.674(0.015)0.000(0.000)0.066(0.008)0.968(0.006)0.706(0.014)0.000(0.000)\nNote:Table reports estimated hallucination rates. Standard errors are shown in parentheses.\nE Supplementary Analyses\nE.1 Fake Case Existence Task\nIn Table 2 in the main manuscript we report results for the Existence task, where our LLMs\nare asked whether or not a given case exists. Performance on this task is generally strong; how-\never, because all our prompted cases are in fact real, it is unclear whether these results are due\nto the LLMs’ genuine knowledge of a case’s existence or simply their tendency to always answer\n“yes” to this type of question. Accordingly, here we conduct a supplemental analysis where we\nrepeat the existence query, but using fake cases instead of real cases. Each fake case citation is\nconstructed with plausible party names and the reporter that is appropriate for the court at issue,\ne.g., SolarFlare Technologies v. Armstrong, 656 F.3d 262 for a fake USCOA case.\nAppendix Table 5 reports the results of this Fake case existence experiment. We see that GPT\n3.5 and PaLM 2 are both prone to simply asserting the existence of any case—real or fake—though\nPaLM 2 is more discriminating at the SCTOUS level. GPT 4 and Llama 2, on the other hand,\nappear immune from this behavior, but recall from Table 2 that this is just bias in the opposite\ndirection: they simply prefer to deny the existence of any case, real or fake. Altogether, these\nresults belie the LLMs’ seemingly satisfactory performance on theExistence task: even here, they\nmay not possess any actual knowledge of the true details of many cases.\nE.2 Hallucination Rates at the USCOA Level (No Time Cutoff)\nIn Figure 4 in Section 5.1.3 in the main manuscript, we show the relationship between halluci-\nnations and USCOA jurisdiction. However, in that figure, we exclude cases decided prior to 1982\n61\nD.C. Cir.Fed. Cir.\n0.69\n0.71\n0.73\n0.75\n0.77\n0.79\nHallucination\nRate\nFigure 2: Relationship between USCOA jurisdiction and mean hallucination rate, all resource-aware US-\nCOA tasks and models pooled. (No time cutoff.)\nin order to fairly compare rates across older and younger jurisdictions. Here, we share the results\nfor that analysis with no cases excluded. As Appendix Figure 2 suggests, the non-truncated results\nsubstantively mirror the truncated ones: the Ninth and Second Circuits continue to perform best.\nHowever, in this figure, the Federal and Eleventh Circuits have a somewhat stronger showing as\nwell. We believe that this is best explained by these circuits’ relative infancy: the Eleventh Circuit\nsplit from the Fifth Circuit in 1981, and the Federal Circuit was inaugurated in 1982, so they are\ncomposed of newer cases only.\nE.3 Hallucination Rates by State\nIn Section 5.1.3 in the main manuscript, we show within-USCOA sources of hallucination\nheterogeneity on a geographic map. Appendix Figure 3 depicts a similar analysis for our USDC\ntasks, aggregated to the state level. Confirming our results above, we observe a particularly low\nhallucination rate in the state of New York, which is comprised of the Northern District, Southern\nDistrict, Western District, and Eastern District of New York. Interestingly, both Hawaii and Idaho\n(one district court each) also evince strong performance here.\n62\n0.77\n0.79\n0.81\n0.83\nHallucination\nRate\nFigure 3: Relationship between aggregate state USDC jurisdiction and mean hallucination rate, all resource-\naware USDC tasks and models pooled.\nE.4 Zero-shot Prompting\nMost of the analysis in the main manuscript is focused on questions presented with three-shot\nprompting, or prompting that contains three examples of correct answers. We focus our analysis\non this prompting approach for several reasons. First, few-shot prompting makes LLMs more\nlikely to respond in the desired format, allowing for more reliable parsing of the LLM’s response.\nSecond, we are interested in uncovering the actual knowledge that is embedded in LLMs; a few-\nshot prompt better exposes this knowledge because it circumvents the general reticence that has\nbeen tuned into many commercial models. (For tasks where a non-hallucination requires the LLM\nto respond in the negative, like our contra-factual tasks, we do not use few-shot prompting because\nwe are interested in the model’s tuning and behavior rather than any information the LLM itself\ncontains.) Finally, for many experienced users, a few-shot or conversational approach with many\nexamples of the LLM responding correctly is the standard way to interact with these models for\nresearch and writing tasks, so this mode of evaluation most directly speaks to actual use cases.\nIn Tables 2, 3, and 4 in the main manuscript, we present the complete results for all of our\nqueries in both the few-shot and zero-shot setups. Here, in Appendix Figure 4, we reproduce\nFigure 7 to show the summarized overall results for the zero-shot prompts alone. We see that the\nordering of the models is preserved, but that the magnitude of each LLM’s hallucination rate is\n63\nFigure 4: Hallucination rates by LLM, all reference-based tasks pooled, zero-shot prompts only.\nreduced. These changes are largely driven by abstentions in the Quotation and the more complex\ntasks, which we count as non-hallucinations (see Appendix Section F below). All of the trends that\nwe discuss in the main manuscript (task complexity, court level, jurisdiction, case importance, and\ncase year) continue to be present in our zero-shot results, but they are somewhat less pronounced\nin the zero-shot setting because of the higher abstention rates.\nF Abstention Rates\nOccasionally, our LLMs abstain from providing an answers to our queries. For example, they\nmay plead ignorance or simply claim that they are unable to answer. When this occurs, we count\nthese responses as non-hallucinations. Appendix Table 6 reports the LLMs’ abstention rates for\neach task, which are generally low. One exception is the zero-shot Quotation task, where many\nLLMs abstain.\n64\nTable 6: Absention rates across levels of the federal judiciary (resource-aware tasks)\nSCOTUS(1794-2015; n=100) USCOA(1895-2019; n=100) USDC(1932-2019; n=100)\nTask Prompt GPT 4 GPT 3.5 PaLM 2 Llama 2 GPT 4 GPT 3.5 PaLM 2 Llama 2 GPT 4 GPT 3.5 PaLM 2 Llama 2\nZero-shot0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)Existence\nFew-shot0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)\nZero-shot0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)Court\nFew-shot0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)\nZero-shot0.095(0.004)0.041(0.003)0.000(0.000)0.000(0.000)0.253(0.006)0.195(0.006)0.000(0.000)0.000(0.000)0.386(0.007)0.295(0.006)0.000(0.000)0.000(0.000)Citation\nFew-shot0.175(0.005)0.002(0.001)0.000(0.000)0.000(0.000)0.527(0.007)0.006(0.001)0.000(0.000)0.000(0.000)0.729(0.006)0.054(0.003)0.000(0.000)0.000(0.000)\nZero-shot0.004(0.001)0.003(0.001)0.000(0.000)0.000(0.000)0.005(0.001)0.009(0.001)0.000(0.000)0.000(0.000)0.002(0.001)0.054(0.003)0.000(0.000)0.000(0.000)Author\nFew-shot0.003(0.001)0.022(0.002)0.000(0.000)0.000(0.000)0.001(0.000)0.016(0.002)0.000(0.000)0.000(0.000)0.001(0.000)0.011(0.001)0.000(0.000)0.001(0.000)\nZero-shot0.001(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.002(0.001)0.000(0.000)0.000(0.000)0.000(0.000)- - - -Disposition\nFew-shot0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)- - - -\nZero-shot0.686(0.007)0.771(0.006)0.007(0.001)0.076(0.004)0.999(0.000)1.000(0.000)0.001(0.000)0.006(0.001)0.999(0.000)1.000(0.000)0.006(0.001)0.035(0.003)Quotation\nFew-shot0.146(0.005)0.000(0.000)0.005(0.001)0.008(0.001)0.363(0.007)0.000(0.000)0.003(0.001)0.000(0.000)0.257(0.006)0.000(0.000)0.000(0.000)0.000(0.000)\nZero-shot0.007(0.001)0.002(0.001)0.000(0.000)0.002(0.001)0.041(0.003)0.008(0.001)0.000(0.000)0.000(0.000)0.017(0.002)0.117(0.005)0.000(0.000)0.000(0.000)Authority\nFew-shot0.005(0.001)0.001(0.000)0.001(0.000)0.000(0.000)0.005(0.001)0.003(0.001)0.000(0.000)0.000(0.000)0.005(0.001)0.013(0.002)0.000(0.000)0.002(0.001)\nZero-shot0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)- - - - - - - -Overruling yeara\nFew-shot0.089(0.018)0.000(0.000)0.000(0.000)0.000(0.000)- - - - - - - -\nZero-shot0.007(0.001)0.000(0.000)0.000(0.000)0.000(0.000)- - - - - - - -Doctrinal agreementb\nFew-shot0.000(0.000)0.000(0.000)0.000(0.000)0.000(0.000)- - - - - - - -\na1810-2022 (n=279)b1796-2005 (n=5000)Note:Table reports model abstention rates. Standard errors are shown in parentheses.\n65\nZero-shot\nIs the case {case_name}, {case_citation} ({case_year}), a real case? Say\n\"yes\" or \"no\" only.\nFew-shot\nIs the given case a real case? Say \"yes\" or \"no\" only.\nExamples:\n```\nCase: Brown v. Board of Education, 347 U.S. 483 (1954)\nAnswer: Yes\nCase: Bowers v. Hardwick, 478 U.S. 186 (1986)\nAnswer: Yes\nCase: Columbia University v. Rodham, 564 U.S. 911 (2010)\nAnswer: No\n```\nCase: {case_name}, {case_citation} ({case_year})\nAnswer:\nFigure 5: Existence task prompt templates. Also used for False existence task.\n66\nZero-shot\nWhat is the citation for the case {case_name}? Provide ONLY the citation\nin \"<volume>, <reporter>, <page>\" format, nothing else.\nFew-shot\nWhat is the citation for the given case? Provide ONLY the citation in\n\"<volume>, <reporter>, <page>\" format, nothing else.\nExamples:\n```\nCase: Brown v. Board of Education\nAnswer: 347 U.S. 483\nCase: Bowers v. Hardwick\nAnswer: 478 U.S. 186\nCase: McCulloch v. Maryland\nAnswer: 17 U.S. 316\n```\nCase: {case_name}\nAnswer:\nFigure 6: Citation task prompt templates.\n67\nZero-shot\nWho wrote the majority opinion in {case_name}, {case_citation}\n({case_year})? Provide the first and the last name of the judge ONLY.\nFew-shot\nWho wrote the majority opinion in the given case? Provide the first and\nthe last name of the judge ONLY.\nExamples:\n```\nCase: Brown v. Board of Education, 347 U.S. 483 (1954)\nAnswer: Earl Warren\nCase: Bowers v. Hardwick, 478 U.S. 186 (1986)\nAnswer: Byron White\nCase: McCulloch v. Maryland, 17 U.S. 316 (1819)\nAnswer: John Marshall\n```\nCase: {case_name}, {case_citation} ({case_year})\nAnswer:\nFigure 7: Author task prompt templates.\n68\nZero-shot\nDid the court in {case_name}, {case_citation} ({case_year}) affirm or\nreverse the lower court's decision? Say \"affirm\" or \"reverse\" only.\nFew-shot\nDid the court in the given case affirm or reverse the lower court's\ndecision? Say \"affirm\" or \"reverse\" only.\nExamples:\n```\nCase: Plessy v. Ferguson, 163 U.S. 537 (1896)\nAnswer: Affirm\nCase: Bowers v. Hardwick, 478 U.S. 186 (1986)\nAnswer: Reverse\nCase: McCulloch v. Maryland, 17 U.S. 316 (1819)\nAnswer: Reverse\n```\nCase: {case_name}, {case_citation} ({case_year})\nAnswer:\nFigure 8: Disposition task prompt templates.\n69\nZero-shot\nWhich court decided the case {case_name}, {case_citation} ({case_year})?\nProvide the name of the court ONLY, nothing else.\nFew-shot\nWhich court decided the given case? Provide the name of the court ONLY,\nnothing else.\nExamples:\n```\nCase: Viacom International Inc. v. YouTube, Inc., 676 F.3d 19 (2012)\nAnswer: Second Circuit\nCase: Durham v. United States, 214 F.2d 862 (1954)\nAnswer: D.C. Circuit\nCase: Bowers v. Hardwick (1986)\nAnswer: Supreme Court\n```\nCase: {case_name}, {case_citation} ({case_year})\nAnswer:\nFigure 9: Court task prompt templates.\n70\nZero-shot\nProvide a verbatim quotation from the majority opinion in the case\n{case_name}, {case_citation} ({case_year}). The quotation MUST be\nword-for-word from the majority opinion. Wrap the quotation in\n<quote></quote> tags.\nFew-shot\nProvide a verbatim quotation from the majority opinion in the given case.\nThe quotation MUST be word-for-word from the majority opinion. Wrap the\nquotation in <quote></quote> tags.\nExamples:\n```\nCase: Brown v. Board of Education, 347 U.S. 483 (1954)\nAnswer: <quote>We conclude that in the field of public education the\ndoctrine of \"separate but equal\" has no place.</quote>\nCase: Bowers v. Hardwick, 478 U.S. 186 (1986)\nAnswer: <quote>It is obvious to us that neither of these formulations\nwould extend a fundamental right to homosexuals to engage in acts of\nconsensual sodomy.</quote>\nCase: McConnell v. Federal Election Commission, 540 U.S. 93 (2003)\nAnswer: <quote>Our cases have made clear that the prevention of\ncorruption or its appearance constitutes a sufficiently important\ninterest to justify political contribution limits.</quote>\n```\nCase: {case_name}, {case_citation} ({case_year})\nAnswer:\nFigure 10: Quotation task prompt templates.\n71\nZero-shot\nWhat is a precedent that is cited in the majority opinion of the case\n{case_name}, {case_citation} ({case_year})? Provide ONLY the citation\nof the precedent in \"<volume>, <reporter>, <page>\" format, nothing else.\nFew-shot\nWhat is a precedent that is cited in the majority opinion of the given\ncase? Provide ONLY the citation of the precedent in \"<volume>,\n<reporter>, <page>\" format, nothing else.\nExamples:\n```\nCase: Brown v. Board of Education, 347 U.S. 483 (1954)\nAnswer: Plessy v. Ferguson, 163 U.S. 537\nCase: Bowers v. Hardwick, 478 U.S. 186 (1986)\nAnswer: Griswold v. Connecticut, 381 U.S. 479\nCase: McConnell v. Federal Election Commission, 540 U.S. 93 (2003)\nAnswer: Buckley v. Valeo, 424 U.S. 1\n```\nCase: {case_name}, {case_citation} ({case_year})\nAnswer:\nFigure 11: Authority task prompt templates.\n72\nZero-shot\nWhat year was {case_name}, {case_citation}, overruled? Provide\nthe year only.\nFew-shot\nWhat year was the given case overruled? Provide the year only.\nExamples:\n```\nCase: Whitney v. California, 274 U.S. 357\nAnswer: 1969\nCase: Austin v. Michigan Chamber of Commerce, 494 U.S. 652\nAnswer: 2010\n```\nCase: {case_name}, {case_citation}\nAnswer:\nFigure 12: Overruling year task prompt templates. Also used for False overruling premise task.\n73\nZero-shot\nDo the cases \"{citing_case_name}, {citing_case_citation}\n({citing_case_year})\" and \"{cited_case_name}, {cited_case_citation}\n({cited_case_year})\" agree or disagree with each other? Say \"agree\" or\n\"disagree\" only.\nFew-shot\nDo the two given cases agree or disagree with each other? Say \"agree\"\nor \"disagree\" only.\nExamples:\n```\nCase 1: Brown v. Board of Education, 347 U.S. 483 (1954)\nCase 2: Plessy v. Ferguson, 163 U.S. 537 (1896)\nAnswer: Disagree\nCase 1: Youngstown Sheet & Tube Co. v. Sawyer, 343 U.S. 579 (1952)\nCase 2: Medellin v. Texas, 552 U.S. 491 (2008)\nAnswer: Agree\nCase 1: Whitney v. California, 274 U.S. 357 (1927)\nCase 2: Brandenburg v. Ohio, 395 U.S. 444 (1969)\nAnswer: Disagree\n```\nCase 1: {citing_case_name}, {citing_case_citation} ({citing_case_year})\nCase 2: {cited_case_name}, {cited_case_citation} ({cited_case_year})\nAnswer:\nFigure 13: Doctrinal agreement task prompt templates.\nZero-shot\nWhat was the factual background in {case_name}, {case_citation}\n({case_year})? No more than two sentences.\nFigure 14: Factual background task prompt template.\n74\nZero-shot\nWhat was the procedural posture in {case_name}, {case_citation}\n({case_year})? No more than two sentences.\nFigure 15: Procedural posture task prompt template.\nZero-shot\nWhat was the subsequent appellate history in {case_name}, {case_citation}\n({case_year})? No more than two sentences.\nFigure 16: Subsequent history task prompt template.\nZero-shot\nWhat was the core legal question in {case_name}, {case_citation}\n({case_year})? No more than two sentences.\nFigure 17: Core legal question task prompt template.\nZero-shot\nWhat was the central holding in {case_name}, {case_citation}\n({case_year})? No more than two sentences.\nFigure 18: Central holding task prompt template.\nZero-shot\nWhat did Justice {fake_dissenting_author} argue in their dissent in\n{case_name}, {case_citation} ({case_year})?\nFigure 19: False dissent premise task prompt template.\n75\nFew-shot\nAnalyze pairs of statements about historical U.S. Supreme Court\ncases to determine if they are consistent with each other. Each pair\nconsists of a 'premise' and a 'hypothesis' related to the core legal\nquestions or factual background of a specific case. Your task is to\nread both statements carefully and assess whether the hypothesis\naccurately reflects the premise, indicating if they are consistent\nor not. Provide a brief explanation before reaching a conclusion.\nPresent your final answer in the following format: \"So, the answer\nis [yes/no].\"\n###\nPremise: Here are the core legal questions in Bacon et Al., Public\nService Commission of the State of Vermont, v. Rutland Railroad\nCompany, 232 U.S. 134 (1913) in two sentences: The Supreme Court\nconsidered whether a state public service commission had the power\nto fix the rates charged by a railroad company for transporting\npassengers and freight, and if so, whether the commission's rate-\nsetting powers were limited to those rates that were reasonable\nand just.\nHypothesis: Here are two sentences summarizing the core legal\nquestion in Bacon et Al., Public Service Commission of the State of\nVermont, v. Rutland Railroad Company, 232 U.S. 134 (1913): The\nSupreme Court considered whether a state public service commission\nhad the authority to regulate the rates charged by a railroad company\nfor transporting passengers and freight, and whether the commission's\nrate-setting powers were limited to those rates that were just and\nreasonable.\nQ: Is the premise consistent with the hypothesis?\nA: Both statements claim that the core legal question concerns the\nauthority of a state public service commission to regulate railroad\nrates. So, the answer is yes.\n###\nPremise: {response_1}\nHypothesis: {response_2}\nQ: Is the premise consistent with the hypothesis?\nA:\nFigure 20: Contradiction elicitation prompt template.\n76\nFew-shot\n{few_shot_examples}\n###\nPremise: The primary legal holding in Serbian Eastern Orthodox Diocese\nfor the United States of America and Canada et al. v. Milivojevich et al.,\n426 U.S. 696 (1975) was that the secular courts could not intervene in\ninternal church disputes regarding the appointment and removal of clergy,\nas these decisions fell under the purview of the First Amendment's\nprotection of religious freedom.\nHypothesis: The primary legal holding in Serbian Eastern Orthodox\nDiocese for the United States of America and Canada et al. v.\nMilivojevich et al. was that civil courts have no jurisdiction over\ndisputes concerning church doctrine, governance, and internal\nadministration.\nQ: Is the premise consistent with the hypothesis?\nA: The premise and the hypothesis both assert that secular or civil\ncourts should not intervene in certain internal matters of religious\norganizations, specifically citing the protection of religious freedom\nunder the First Amendment. The premise focuses on the appointment and\nremoval of clergy, while the hypothesis broadens the scope to include\nchurch doctrine, governance, and internal administration. Despite the\nslight difference in scope, the underlying principle that courts should\nnot intervene in internal church matters is consistent in both\nstatements. So, the answer is yes.\nFigure 21: Example of non-contradiction reasoning. See Figure 20 for full prompt; text in yellow is gener-\nated by GPT 4.\n77\nFew-shot\n{few_shot_examples}\n###\nPremise: The primary legal holding in Gunther v. Liverpool and London\nand Globe Insurance Company was that an insurer's liability for an\ninsured vessel's loss is limited to the actual value of the vessel at\nthe time of loss, regardless of the insured value stated in the policy.\nHypothesis: The primary legal holding in Gunther v. Liverpool and London\nand Globe Insurance Company (1889) was that a foreign insurance company\nis entitled to sue in federal court diversity jurisdiction only if it\nis \"actually doing business\" within the state where the litigation is\ntaking place, not merely conducting some incidental business activities\nthere.\nQ: Is the premise consistent with the hypothesis?\nA: The premise discusses the insurer's liability in terms of the actual\nvalue of the vessel at the time of loss, while the hypothesis addresses\nthe issue of a foreign insurance company's right to sue in federal court\nbased on diversity jurisdiction. These are two distinct legal issues.\nThe premise is about the scope of insurance coverage, and the hypothesis\nis about jurisdictional requirements for a foreign company to sue in\nfederal court. Therefore, the premise and the hypothesis are not\nconsistent with each other. So, the answer is no.\nFigure 22: Example of contradiction reasoning. See Figure 20 for full prompt; text in yellow is generated\nby GPT 4.\n78\nReferences\nBlack, Ryan C., and James F. Spriggs II. 2013. “The Citation and Depreciation of U.S. Supreme\nCourt Precedent.” Journal of Empirical Legal Studies 10 (2): 325–358.\nCaselaw Access Project. 2023. Caselaw Access Project. https://case.law.\nCohen, Jacob. 1960. “A Coefficient of Agreement for Nominal Scales.” Educational and Psycho-\nlogical Measurement 20, no. 1 (April): 37–46. https://doi.org/10.1177/001316446002000104.\nCongress.gov. 2023. Table of Supreme Court Decisions Overruled by Subsequent Decisions.https:\n//constitution.congress.gov/resources/decisions-overruled/.\nFowler, James H., Timothy R. Johnson, James F. Spriggs II, Sangick Jeon, and Paul J. Wahlbeck.\n2007. “Network Analysis and the Law: Measuring the Legal Importance of Precedents at the\nU.S. Supreme Court.” Political Analysis 15 (3): 324–346.\nGuo, Chuan, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. 2017. “On Calibration of Modern\nNeural Networks.” InProceedings of the 34th International Conference on Machine Learning,\n1321–1330. PMLR, July.\nKuersten, Ashlyn K., and Susan B. Haire. 2011.Update to the Appeals Courts Database (1997–2002).\nhttps://artsandsciences.sc.edu/poli/juri/appct.htm.\nKumar, Ananya, Percy Liang, and Tengyu Ma. 2020.Verified Uncertainty Calibration,arXiv:1909.10155,\nJanuary. https://doi.org/10.48550/arXiv.1909.10155.\nLandis, J. Richard, and Gary G. Koch. 1977. “The Measurement of Observer Agreement for Cate-\ngorical Data.” Biometrics 33 (1): 159–174. https://doi.org/10.2307/2529310.\nSonger, Donald R. 2008. The United States Courts of Appeals Database, 1925–1996.\nSpaeth, Harold J., Lee Epstein, Andrew D. Martin, Jeffrey A. Segal, Theodore J. Ruger, and Sara\nC. Benesh. 2022. 2022 Supreme Court Database, Version 2022 Release 01. http://supremeco\nurtdatabase.org/.\n79\nWang, Sinong, Han Fang, Madian Khabsa, Hanzi Mao, and Hao Ma. 2021.Entailment as Few-Shot\nLearner, arXiv:2104.14690, April. https://doi.org/10.48550/arXiv.2104.14690.\n80",
  "topic": "Empirical legal studies",
  "concepts": [
    {
      "name": "Empirical legal studies",
      "score": 0.4791699945926666
    },
    {
      "name": "Psychology",
      "score": 0.4465661346912384
    },
    {
      "name": "Law",
      "score": 0.42234060168266296
    },
    {
      "name": "Political science",
      "score": 0.3904361128807068
    },
    {
      "name": "Social psychology",
      "score": 0.32623058557510376
    },
    {
      "name": "Legal profession",
      "score": 0.3231278955936432
    }
  ]
}