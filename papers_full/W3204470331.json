{
  "title": "Predicting Attention Sparsity in Transformers",
  "url": "https://openalex.org/W3204470331",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4384247769",
      "name": "Marcos Treviso",
      "affiliations": [
        "Instituto de Telecomunicações",
        "Instituto Superior de Ciências Educativas"
      ]
    },
    {
      "id": "https://openalex.org/A2963661076",
      "name": "António Góis",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2634442158",
      "name": "Patrick Fernandes",
      "affiliations": [
        "Carnegie Mellon University",
        "Instituto de Telecomunicações",
        "Instituto Superior de Ciências Educativas"
      ]
    },
    {
      "id": "https://openalex.org/A5036870275",
      "name": "Erick Fonseca",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2122534586",
      "name": "André Martins",
      "affiliations": [
        "Instituto de Telecomunicações",
        "Instituto Superior de Ciências Educativas"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4287704453",
    "https://openalex.org/W1983874169",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W4287642956",
    "https://openalex.org/W3106504817",
    "https://openalex.org/W4309793872",
    "https://openalex.org/W1982582425",
    "https://openalex.org/W3037798801",
    "https://openalex.org/W3091156754",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W2950858167",
    "https://openalex.org/W2963643701",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W3006983028",
    "https://openalex.org/W3005389111",
    "https://openalex.org/W2997753998",
    "https://openalex.org/W2963494889",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2253795368",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2963123301",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W3176626464",
    "https://openalex.org/W2134089414",
    "https://openalex.org/W3200409031",
    "https://openalex.org/W3045733172",
    "https://openalex.org/W3125498921",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W4287212936",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W3102129360",
    "https://openalex.org/W3034609440",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3131922516",
    "https://openalex.org/W2970777192",
    "https://openalex.org/W4287725215",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W2977458338",
    "https://openalex.org/W2117154949",
    "https://openalex.org/W3034573343",
    "https://openalex.org/W3155733378",
    "https://openalex.org/W2912351236",
    "https://openalex.org/W4323654151",
    "https://openalex.org/W2106053110"
  ],
  "abstract": "Transformers’ quadratic complexity with respect to the input sequence length has motivated a body of work on efficient sparse approximations to softmax. An alternative path, used by entmax transformers, consists of having built-in exact sparse attention; however this approach still requires quadratic computation. In this paper, we propose Sparsefinder, a simple model trained to identify the sparsity pattern of entmax attention before computing it. We experiment with three variants of our method, based on distances, quantization, and clustering, on two tasks: machine translation (attention in the decoder) and masked language modeling (encoder-only). Our work provides a new angle to study model efficiency by doing extensive analysis of the tradeoff between the sparsity and recall of the predicted attention graph. This allows for detailed comparison between different models along their Pareto curves, important to guide future benchmarks for sparse attention models.",
  "full_text": "Proceedings of the Sixth Workshop on Structured Prediction for NLP, pages 67 - 81\nMay 27, 2022c⃝2022 Association for Computational Linguistics\nPredicting Attention Sparsity in Transformers\nMarcos Treviso1,2 António Góis5∗ Patrick Fernandes1,2,3\nErick Fonseca6∗ André F. T. Martins1,2,4\n1Instituto de Telecomunicações, Lisbon, Portugal\n2Instituto Superior Técnico & LUMLIS (Lisbon ELLIS Unit), Lisbon, Portugal\n3Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA\n4Unbabel, Lisbon, Portugal\n5Mila, Université de Montréal, Canada\n6Kaufland e-commerce, Cologne, Germany\nAbstract\nTransformers’ quadratic complexity with re-\nspect to the input sequence length has moti-\nvated a body of work on efficient sparse ap-\nproximations to softmax. An alternative path,\nused by entmax transformers, consists of hav-\ning built-in exact sparse attention; however this\napproach still requires quadratic computation.\nIn this paper, we propose Sparsefinder, a sim-\nple model trained toidentify the sparsity pattern\nof entmax attention before computing it. We\nexperiment with three variants of our method,\nbased on distances, quantization, and cluster-\ning, on two tasks: machine translation (atten-\ntion in the decoder) and masked language mod-\neling (encoder-only). Our work provides a new\nangle to study model efficiency by doing exten-\nsive analysis of the tradeoff between the spar-\nsity and recall of the predicted attention graph.\nThis allows for detailed comparison between\ndifferent models along their Pareto curves, im-\nportant to guide future benchmarks for sparse\nattention models.\n1 Introduction\nTransformer-based architectures have achieved re-\nmarkable results in many NLP tasks (Vaswani et al.,\n2017; Devlin et al., 2019; Brown et al., 2020). How-\never, they also bring important computational and\nenvironmental concerns, caused by their quadratic\ntime and memory computation requirements with\nrespect to the sequence length. This comes in ad-\ndition to the difficulty of interpreting their inner\nworkings, caused by their overparametrization and\nlarge number of attention heads.\nThere is a large body of work developing ways to\n“sparsify” the computation in transformers, either\nby imposing local or fixed attention patterns (Child\net al., 2019; Tay et al., 2020; Zaheer et al., 2020), by\napplying low-rank kernel approximations to soft-\nmax (Wang et al., 2020; Choromanski et al., 2021),\n∗Work done at Instituto de Telecomunicações. Correspon-\ndence to marcos.treviso@tecnico.ulisboa.pt\nthe quick brown fox jumps over the lazy dog\na) Extract α-entmax graph\nb) Project and group qi and kj c) Add local + global patterns\nFigure 1: (a) Extract sparse attention graphs from a\npretrained α-entmax transformer; (b) Project query and\nkey vectors to a smaller and appropriated space such\nthat similar points are likely to fall in the same vicinity;\n(c) Additionally, we can combine window and global\npatterns (green blocks) with the learned pattern (yellow\nblocks) to increase the recall in recovering ground-truth\nedges from the sparse graph at the top (starred blocks).\nor by learning which queries and keys should be\ngrouped together (Kitaev et al., 2019; Daras et al.,\n2020; Roy et al., 2021; Wang et al., 2021). Most\nof the existing work seeks to approximate softmax-\nbased attention by ignoring the (predicted) tails\nof the distribution, which can lead to performance\ndegradation. An exception is transformers with\nentmax-based sparse attention (Correia et al.,\n2019), a content-based approach which is natively\nsparse – this approach has the ability to let each\nattention head learn from data how sparse it should\nbe, eliminating the need for heuristics or approxi-\nmations. The disadvantage of this approach is that\nit still requires a quadratic computation to deter-\nmine the sparsity pattern, failing to take computa-\ntional advantage of attention sparsity.\nIn this paper, we propose Sparsefinder, which\nfills the gap above by making entmax attention\nmore efficient (§4). Namely, we investigate three\n67\nmethods to predict the sparsity pattern of entmax\nwithout having to compute it: one based on metric\nlearning, which is still quadratic but with a better\nconstant (§4.3), one based on quantization (§4.4),\nand another based on clustering (§4.5). In all cases,\nthe predictors are trained offline on ground-truth\nsparse attention graphs from an entmax transformer,\nseeking high recall in their predicted edges without\ncompromising the total amount of sparsity. Figure 1\nillustrates our method.\nMore precisely, to evaluate the effectiveness\nof our method across different scenarios, we per-\nform experiments on two NLP tasks, encompassing\nencoder-only and decoder-only configurations: ma-\nchine translation (MT, §5) and masked language\nmodeling (MLM, §6), doing an extensive analysis\nof the tradeoff between sparsity and recall (i.e., per-\nformance on the attention graph approximation),\nand sparsity and accuracy (performance on down-\nstream tasks). We compare our method with four\nalternative solutions based on efficient transform-\ners: Longformer (Beltagy et al., 2020), Bigbird (Za-\nheer et al., 2020), Reformer (Kitaev et al., 2020),\nand Routing Transformer (Roy et al., 2021), along\ntheir entire Pareto curves. We complement these\nexperiments by analyzing qualitatively what is se-\nlected by the different attention heads at the several\nlayers and represented in different clusters/buckets.\nOverall, our contributions are:1\n• We propose a simple method that exploits learn-\nable sparsity patterns to efficiently compute\nmulti-head attention (§4).\n• We do an extensive analysis of the tradeoff be-\ntween sparsity and recall, and sparsity and accu-\nracy in MT (§5) and MLM (§6), showing that\nthere is clear room for improvement in the design\nof efficient transformers.\n• We qualitatively analyze what is selected by the\ndifferent attention heads at various layers and\nrepresented in different clusters/buckets.\n2 Related Work\nInterpreting multi-head attention. Several\nworks analyze the functionalities learned by dif-\nferent attention heads, such as positional and local\ncontext patterns (Raganato and Tiedemann, 2018;\nV oita et al., 2019). Building upon prior work on\n1https://github.com/deep-spin/\nsparsefinder\nsparse attention mechanisms (Peters et al., 2019),\nCorreia et al. (2019) constrain the attention heads to\ninduce sparse selections individually for each head,\nbringing interpretability without post-hoc manip-\nulation. Related approaches include the explicit\nsparse transformer (Zhao et al., 2019) and recti-\nfied linear attention (Zhang et al., 2021), which\ndrops the normalization constraint. Raganato et al.\n(2020) show that it is possible to fix attention pat-\nterns based on previously known behavior (e.g. fo-\ncusing on previous token) while improving trans-\nlation quality. However, a procedure that exploits\nlearnable sparsity patterns to accelerate multi-head\nattention is still missing.\nLow-rank softmax approximations. Methods\nbased on low-rank approximation to the softmax\nsuch as Linearized Attention (Katharopoulos et al.,\n2020), Linformer (Wang et al., 2020), and Per-\nformer (Choromanski et al., 2021) reduce both\nspeed and memory complexity of the attention\nmechanism from quadratic to linear, but make inter-\npretability more challenging because the scores are\nnot computed explicitly. On the other hand, meth-\nods that focus on inducing sparse patterns provide\ninterpretable alignments and also have performance\ngains in terms of speed and memory.\nFixed attention patterns. Among fixed pattern\nmethods, Sparse Transformer (Child et al., 2019)\nand LongFormer (Beltagy et al., 2020) attend to\nfixed positions by using strided/dilated sliding win-\ndows. BigBird uses random and two fixed patterns\n(global and window) to build a block sparse ma-\ntrix representation (Zaheer et al., 2020), taking ad-\nvantage of block matrix operations to accelerate\nGPU computations. In contrast, we replace the\nrandom pattern with a learned pattern that mimics\npretrained α-entmax sparse attention graphs.\nLearnable attention patterns. Learnable pat-\ntern methods usually have to deal with assignment\ndecisions within the multi-head attention mech-\nanism. Clustered Attention (Vyas et al., 2020)\ngroups query tokens into clusters and computes\ndot-products only with centroids. Reformer (Ki-\ntaev et al., 2020) and SMYRF (Daras et al., 2020)\nuse locality-sensitive hashing to efficiently group\ntokens in buckets. More similar to our work, Rout-\ning Transformer (Roy et al., 2021) and Cluster-\nFormer (Wang et al., 2021) cluster queries and keys\nwith online k-means and compute dot-products\nover the top- k cluster points. Some queries and\n68\nkeys are discarded due to this filtering, which af-\nfects the overall recall of the method (as we show in\n§5 and §6). The ability of Routing Transformer to\nbenefit from contextual information has been ana-\nlyzed by Sun et al. (2021). In contrast, Sparsefinder\nlearns to cluster based on sparsity patterns from at-\ntention graphs generated by α-entmax.\n3 Background\n3.1 Transformers\nThe main component of transformers is the multi-\nhead attention mechanism (Vaswani et al., 2017).\nGiven as input a matrix Q ∈ Rn×d containing\nd-dimensional representations for nqueries, and\nmatrices K,V ∈Rm×d for m keys and values,\nthe scaled dot-product attention at a single head is\ncomputed in the following way:\natt(Q,K,V) =π\n(\nQK⊤\n√\nd\n)\n  \nZ∈Rn×m\nV ∈Rn×d. (1)\nThe πtransformation maps rows to distributions,\nwith softmax being the most common choice,\nπ(Z)ij = softmax(zi)j. Multi-head attention is\ncomputed by evoking Eq. 1 in parallel for each\nhead h:\nheadh(Q,K,V) =att(QWQ\nh,KWK\nh ,VWV\nh),\nwhere WQ\nh, WK\nh , WV\nh are learned linear transfor-\nmations. This way, heads are able to learn spe-\ncialized phenomena. According to the nature of\nthe input, transformers have three types of multi-\nhead attention mechanism: encoder self-attention\n(source-to-source), decoder self-attention (target-\nto-target), and decoder cross-attention (target-to-\nsource). While there are no restrictions to which el-\nements can be attended to in the encoder, elements\nin position j >iin the decoder self-attention are\nmasked at timestep i(“causal mask”).\n3.2 Extmax Transformers and Learned\nSparsity\nThe main computational bottleneck in transformers\nis the matrix multiplication QK⊤in Eq. 1, which\ncosts O(nmd) time and can be impractical when\nnand mare large. Many approaches, discussed\nin §2, approximate Eq. 1 by ignoring entries far\nfrom the main diagonal or computing only some\nblocks of this matrix, with various heuristics. By\ndoing so, the result will be an approximation of the\nsoftmax attention in Eq. 1. This is because the orig-\ninal softmax-based attention is dense, i.e., it puts\nsome probability mass on all tokens – not only a\ncomputational disadvantage, but also making inter-\npretation harder, as it has been observed that only\na small fraction of attention heads capture relevant\ninformation (V oita et al., 2019).\nAn alternative to softmax is theα-entmax trans-\nformation (Peters et al., 2019; Correia et al., 2019),\nwhich leads to sparse patterns directly, without any\napproximation:\nα-entmax(z) = [(α−1)z −τ(z)1]\n1/α−1\n+ , (2)\nwhere [·]+ is the positive part (ReLU) function, and\nτ : Rn →R is a normalizing function satisfying∑\nj[(α−1)zj −τ(z)]\n1/α−1\n+ = 1 for any z. That\nis, entries with score zj ≤ τ(z)/α−1 get exactly\nzero probability. In the limit α →1, α-entmax\nrecovers the softmax function, while for any value\nof α > 1 this transformation can return sparse\nprobability vectors (as the value of α increases,\nthe induced probability distribution becomes more\nsparse). When α= 2, we recover sparsemax (Mar-\ntins and Astudillo, 2016). In this paper, we use\nα = 1.5, which works well in practice and has a\nspecialized fast algorithm (Peters et al., 2019).\nAlthough sparse attention improves interpretabil-\nity and head diversity when compared to dense al-\nternatives (Correia et al., 2019), the learned sparsity\npatterns cannot be trivially exploited to reduce the\nquadratic burden of self-attention, since we still\nneed to compute dot-products between all queries\nand keys ( QK⊤) before applying the α-entmax\ntransformation. In the next section (§4), we pro-\npose a simple method that learns to identify these\nsparsity patterns beforehand, avoiding the full ma-\ntrix multiplication.\n4 Sparsefinder\nWe now propose our method to extract sparse atten-\ntion graphs and learn where to attend by exploiting\na special property of α-entmax: sparse-consistency\n(§4.1). We design three variants of Sparsefinder to\nthat end, based on metric learning (§4.3), quantiza-\ntion (§4.4), and clustering (§4.5).\n4.1 Attention graph and sparse-consistency\nFor each attention head h, we define its attention\ngraph as Gh = {(qi,kj) |pi,j > 0}, a bipartite\ngraph connecting query and key pairs qi,kj ∈Rd\n69\nfor which the α-entmax probability pi,j is nonzero.\nAn example of attention graph is shown in Figure 1.\nWe denote by |Gh|the total size of an attention\ngraph, i.e., its number of edges. With α-entmax\nwith α = 1.5 we typically have |Gh|≪ nm. In\ncontrast, softmax attention always leads to a com-\nplete graph, |Gh|= nm.\nProblem statement. Our goal is to build a model\n– which we call Sparsefinder – that predicts ˆGh ≈\nGh without having to perform all pairwise compar-\nisons between queries and keys. This enables the\ncomplexity of evaluating Eq. 1 to be reduced from\nO(nmd) to O(|ˆGh|d), effectively taking advantage\nof the sparsity ofα-entmax. In order to learn such a\nmodel, we first extract a dataset of sparse attention\ngraphs {Gh}from a pretrained entmax-based trans-\nformer, which acts as a teacher. Then, the student\nlearns where to pay attention based on this informa-\ntion. This procedure is motivated by the following\nsparse-consistency property of α-entmax:\nProposition 1 (Sparse-consistency property). Let\nb be a binary vector such that bj = 1if p⋆\nj > 0,\nand bj = 0otherwise. For any binary mask vector\nm “dominated” by b (i.e. m ⊙b = b), we have\nα-entmax(z) =α-entmax(z|m), (3)\nwhere zj|m = zj if mj = 1and −∞if mj = 0.\nProof. See §A in the supplemental material.\nThis property ensures that, if ˆGh is such that\nGh ⊆ˆGh, then we obtain exactly the same result as\nwith the original entmax attention. Therefore, we\nare interested in having high recall,\nrecall( ˆGh; Gh) =|ˆGh ∩Gh|\n|Gh| , (4)\nmeaning that our method is nearly exact, and high\nsparsity,\nsparsity( ˆGh) = 1−|ˆGh|\nnm, (5)\nwhich indicates that computation can be made ef-\nficient.2 Although a high sparsity may indicate\nthat many computations can be ignored, converting\nthis theoretical result into efficient computation is\nnot trivial and potentially hardware-dependent. In\nthis paper, rather than proposing a practical com-\nputational efficient method, we focus on showing\n2For the decoder self-attention the denominator in Eq. 5\nbecomes n(n+ 1)/2 due to “causal” masking.\nthat such methods do exist and that they can be\ndesigned to outperform fixed and learned pattern\nmethods while retaining a high amount of sparsity\nwhen compared to the ground-truth graph.\nOur strategies. We teach the student model to\npredict ˆGh ≈Gh by taking inspiration from the\nReformer model (Kitaev et al., 2020) and the Rout-\ning Transformer (Roy et al., 2021). Formally, we\ndefine a set of B buckets, B= {1,...,B }, and\nlearn functions fq,fk : Rd →2B\\{∅}, which\nassign a query or a key to one or more buckets. We\nwill discuss in the sequel different design strategies\nfor the functions fq,fk. Given these functions, the\npredicted graph is:\nˆGh = {(qi,kj) |fq(qi) ∩fk(kj) ̸= ∅}, (6)\nthat is, an edge is predicted between qi and kj iff\nthey are together in some bucket.\nWe present three strategies, based on distance-\nbased pairing (§4.3), quantization (§4.4) and clus-\ntering (§4.5). As a first step, all strategies require\nlearning a metric that embeds the graph (projecting\nqueries and keys) into a lower-dimensional space\nRr with r≪d, such that positive query-key pairs\nare close to each other, and negative pairs are far\napart.\n4.2 Learning projections\nAccording to the α-entmax sparse-consistency\nproperty, in order to get a good approximation of\nGh, we would like that fq and fk produce a graph\nˆGh that maximizes recall, defined in Eq. 4. How-\never, maximizing recall in this setting is difficult\nsince we do not have ground-truth bucket assign-\nments. Instead, we recur to a contrastive learning\napproach by learning projections via negative sam-\npling, which is simpler and more scalable than\nconstrained clustering approaches (Wagstaff et al.,\n2001; de Amorim, 2012).\nFor each head, we start by projecting the orig-\ninal query and key q,k ∈Rd vectors into lower\ndimensional vectors q′,k′∈Rr such that r ≪d.\nIn practice, we use a simple head-wise linear pro-\njection for all queries and keys gθ : Rd →Rr. To\nlearn the parameters of the projection layer we min-\nimize a hinge loss with margin ωfor each head h:\nLθ(Gh) =\n[\nω+∥q′−k′\nP∥2\n2 −∥q′−k′\nN∥2\n2\n]\n+\n, (7)\nwhere (q′,k′\nP) ∈ Gh is a positive pair and\n(q′,k′\nN) /∈Gh is a negative pair sampled uniformly\n70\nat random. In words, we want the distance between\na query vector to negative pairs to be larger than\nthe distance to positive pairs by a margin ω. This\napproach can also be seen as a weakly-supervised\nlearning problem, where the goal is to push dissim-\nilar points away while keeping similar points close\nto each other (Xing et al., 2002; Weinberger and\nSaul, 2009; Bellet et al., 2015).\n4.3 Distance-based pairing\nTo take advantage of the proximity of data points\non the embedded space, we first propose a sim-\nple method to connect query and key pairs whose\nEuclidean distance is less than a threshold t, i.e.\nˆGh = {(qi,kj) | ∥q′\ni −k′\nj∥2 ≤ t}. Although\nthis method also requires O(n2) computations, it\nis more efficient than a vanilla transformer since\nit reduces computations by a factor of d/rby us-\ning the learned projections. This method is also\nuseful to probe the quality of the embedded space\nlearned by the projections, since the recall of our\nother methods will be contingent on it.\n4.4 Buckets through quantization\nOur second strategy quantizes each dimension\n1,...,r of the lower-dimensional space intoβbins,\nplacing the queries and keys into the corresponding\nbuckets (B = rβ buckets in total). This way, each\nqi and kj will be placed in exactly rbuckets (one\nper dimension). If qi and kj are together in some\nbucket, Sparsefinder predicts that (qi,kj) ∈ ˆGh.\nNote that for this quantization strategy no learn-\ning is needed, only the hyperparameter βand the\nbinning strategy need to be chosen. We propose a\nfixed-size binning strategy: divide each dimension\ninto β bins such that all bins have exactly ⌈n/β⌉\nelements. In practice, we append padding symbols\nto the input to ensure that bins are balanced.\n4.5 Buckets through clustering\nThe clustering strategy uses the low-dimensional\nprojections and runs a clustering algorithm to as-\nsign qi and kj to one or more clusters. In this\ncase, each cluster corresponds to a bucket. In our\npaper, we employed k-means to learn Bcentroids\n{c1,..., cB}, where each cb ∈Rr, over a small\nportion of the training set. This strategy is simi-\nlar to the Routing Transformer’s online k-means\n(Roy et al., 2021), but with two key differences: (a)\nour clustering step is applied offline; (b) we assign\npoints to the top- k closest centroids rather than\nassigning the closest top-kclosest points to each\ncentroid, ensuring that all queries are assigned to a\ncluster.3 At test time, we use the learned centroids\nto group queries and keys into kclusters each:\nfq(qi) =arg top-k\n1≤b≤B\n−∥qi −cb∥2\n2, (8)\nfk(kj) =arg top-k\n1≤b≤B\n−∥kj −cb∥2\n2, (9)\nwhere the arg top-koperator returns the indices of\nthe kth largest elements. As in the quantization-\nbased approach, queries and keys will attend to\neach other, i.e., Sparsefinder predicts(qi,kj) ∈ˆGh\nif they share at least one cluster among thekclosest\nones. Smaller values of kwill induce high sparsity\ngraphs, whereas a larger k is likely to produce a\ndenser graph but with a higher recall.\n4.6 Computational cost\nLet Lbe the maximum number of elements in a\nbucket. The time and memory cost of bucketed\nattention computed through quantization or clus-\ntering is O(BL2). With balanced buckets, we get\na complexity of O(n1.5) by setting B = √n. Al-\nthough this cost is sub-quadratic, leveraging the\nsparse structure of ˆGh in practice is challenging,\nsince it might require specialized hardware or ker-\nnels. In general, we have |ˆGh|= ∑B\nb=1 nbmb ≪\nnm, where nb and mb are the number of queries\nand keys in each bucket, since we have small com-\nplete bipartite graphs on each bucket. Instead of\nviewing quadratic methods only in light of their\nperformance, we adopt an alternative view of as-\nsessing the tradeoff of these methods in terms of\nsparsity and recall of their approximation ˆGh. This\noffers a theoretical perspective to the potential per-\nformance of each approximation on downstream\ntasks, helping to find the best approximations for a\ndesired level of sparsity.\n4.7 Combining learned and fixed patterns\nAs pointed out in prior work (V oita et al., 2019),\nseveral attention heads rely strongly in local pat-\nterns or prefer to attend to a particular position,\nmore promimently in initial layers. Therefore,\nwe take inspiration from the Longformer (Beltagy\net al., 2020) and BigBird (Zaheer et al., 2020) and\ncombine learned sparse patterns with window and\n3The difference relies on the dimension on which the top-\nk operation is applied. Routing Transformer applies top- k\nto the input dimension, possibly leaving some queries unat-\ntended, whereas Sparsefinder applies to the centroids dimen-\nsion, avoiding this problem.\n71\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nSparsity\n0.4\n0.6\n0.8\n1.0Recall\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nSparsity\n0\n10\n20\n30BLEU\nBaseline BigBird Longformer Reformer Routing Sf. distance Sf. k-means Sf. quant.\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nSparsity\n0.4\n0.6\n0.8\n1.0Recall\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nSparsity\n0\n10\n20\n30BLEU\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nSparsity\n0.4\n0.6\n0.8\n1.0Recall\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nSparsity\n10\n20\n30\n40BLEU\nFigure 2: Sparsity-recall (left) and sparsity-BLEU (right) tradeoff averaged across all layers and heads on IWSLT\nEN→DE (top) and EN→FR (bottom). The vertical dashed line represents the gold sparsity obtained by the original\nα-entmax transformer (which requires quadratic computation), and the starred marks depict its BLEU score: 34.47\non EN→DE and 42.65 on EN→FR.\nglobal patterns by adding connections in the pre-\ndicted graph ˆGh to improve the recall of all meth-\nods. Figure 1 illustrates how these patterns are\ncombined in the last step.\n5 Experiments: Machine Translation\nSetup. We pretrain a transformer-large model (6\nlayers, 16 heads) on the Paracrawl dataset (Esplà\net al., 2019). Next, we finetune it with α-entmax,\nfixing α = 1.5 for all heads, on EN→DE and\nEN→FR language pairs from IWSLT17 (Cettolo\net al., 2017). We use the 2011-2014 sets as valida-\ntion data and the 2015 set as test data. We encode\neach word using byte pair encoding (BPE, Sen-\nnrich et al. 2016) with a joint segmentation of 32k\nmerges. As Vaswani et al. (2017), we finetune our\nmodels using the Adam optimizer with an inverse\nsquare root learning rate scheduler, with an initial\nvalue of 5 ×10−4 and a linear warm-up in the first\n4000 steps. We evaluate translation quality with\nsacreBLEU (Post, 2018). Training details, hyper-\nparameters, and data statistics are described in §C.\nLearning projections. To learn projections for\nqueries and keys (§4.2), we randomly selected 10K\nlong instances (n >20 tokens) from the training\nset and extracted the α-entmax attention graphs\nGh from the decoder self-attention for each head.\nThis led to an average of 8M and 9M positive pairs\n(qi,kj) per layer for EN→DE and EN→FR, respec-\ntively. In practice, due to the small number of pa-\nrameters for each head (only 4,160), a single epoch\nwith Adam was sufficient to optimize the loss in\nEq. 7. The hyperparameters and the training details\nfor learning projections can be found in §C.\nPareto-curves. Using the learned projections,\nwe investigate the recall and the accuracy of all\nSparsefinder variants by comparing them with\nLongformer, BigBird, Reformer, and Routing\nTransformer. To get a fair comparison, we ana-\nlyze each method for different levels of sparsity by\nvarying the following hyperparameters:\n• Distance-based methods: the threshold twithin\n{0.5,1.0,1.5,2.0,2.5,3.0,3.5,4.0,4.5,5.0}.\n• Bucketing-based methods: the number of buck-\nets Bwithin {2,4,6,8,10,12,16,20}.\n• Fixed-pattern methods: the number of random\nblocks of size 1 within {2,4,6,8,10,12,16,20}\nfor BigBird; and the number of random global to-\nkens within {2,4,6,8,10,12,16,20}for Long-\nformer.\nWe also add global and local patterns to\nall methods, varying the window size within\n{0,1,3,5,7,9,11,15,19,23,27}to get different\nlevels of locality. We further compare all meth-\nods with a simple window baseline that only in-\nduces the window and global patterns. Since all\nmethods exhibit a tradeoff between sparsity and re-\ncall/accuracy, we plot the scores obtained by vary-\ning the hyperparameters and draw their respective\nPareto frontier to see the optimal Pareto-curve.\n72\nMethods whose points lie below this frontier are\nsaid to be Pareto-dominated, meaning that their\nrecall/accuracy cannot be increased without sac-\nrificing sparsity, or vice-versa. Concretely, each\npoint on the curve is measured as a function of the\napproximation to the ground-truth α-entmax atten-\ntion graph Gh by replacing it by ˆGh at test time.\nSparsity-recall tradeoff. Pareto-curves for the\nsparsity-recall tradeoff are shown on the left of\nFigure 2 for both language pairs. Overall, both\nlanguage pairs have similar trends for all meth-\nods. Sparsefinder’s distance-based and clustering\napproaches Pareto-dominates the other methods,\nfollowed by Routing Transformer. Interestingly,\nLongformer, BigBird, Routing Transformer, and\nSparsefinder’s bucketing approach perform on par\nwith the baseline, indicating that a simple local\nwindow is a hard baseline to beat. Since the LSH\nattention in Reformer shares queries and keys be-\nfore hashing, the resultant buckets are also shared\nfor queries and keys, explaining the high recall and\nthe low sparsity of Reformer.\nSparsity-accuracy tradeoff. We show the trade-\noff between sparsity and BLEU on the right of\nFigure 2. For lower levels of sparsity, all meth-\nods perform well, close to the full entmax trans-\nformer. But as sparsity increases, indicating that\nonly a few computations are necessary, we see\nthat the distance-based and k-means variants of\nSparsefinder Pareto-dominate other methods, keep-\ning a very high BLEU without abdicating sparsity.\nIn particular, Sparsefinder’s distance and clustering\napproaches perform on par with the full entmax\ntransformer when the amount of sparsity is close\nto the original entmax transformer (around the ver-\ntical dashed line). Overall, these plots show that\nmethods with a high recall for higher levels of spar-\nsity also tend to have a higher BLEU score.\nLearned patterns. We select some heads and\nshow in Figure 3 examples of the pattern learned\nby our k-means variant on EN→FR. More exam-\nples can be found in §E. We note that the window\npattern is useful to recover local connections. We\ncan see that the k-means variant groups more query\nand key pairs than the actual number of ground-\ntruth edges (left plots). However, due to the sparse-\nconsistency property (right plots), most of these\npredictions receive zero probability by α-entmax,\nresulting in a very accurate approximation.\nFigure 3: Learned patterns by Sparsefinder k-means\n(left) and the subsequent attention weights (right).\nStarred blocks represent ground-truth edges.\n6 Experiments: Masked LM\nSetup. Following Beltagy et al. (2020), we initial-\nize our model from a pretrained RoBERTa check-\npoint. We use the roberta-base model from\nHuggingface’s transformers library, with 12 layers\nand 12 heads.4 We finetune on WikiText-103 (Mer-\nity et al., 2017), replacing softmax by α-entmax\nwith α= 1.5 for all heads. Training details, model\nhyperparameters, and data statistics can be found\nin §D.\nLearning projections. As done for MT experi-\nments, we learn to project keys and queries from\nthe original 64 dimensions into r= 4dimensions.\nTo this end, we use 1K random samples from the\ntraining set, each with length of 512, keeping half\nfor validation. We extract the α-entmax attention\ngraphs Gh but from the encoder self-attention of\neach head, leading to an average of 3M positive\npairs per layer. Due to the small number of learn-\nable parameters for each head (256), training was\ndone with Adam for one epoch.\nResults. Our full transformer trained with α-\nentmax achieved a perplexity score of 3.5004 with\nan overall sparsity of 0.9804 on WikiText-103.\nAs in sentence-level MT experiments, we mea-\nsure the sparsity-recall and the sparsity-perplexity\ntradeoff via the change of Gh with ˆGh at test\ntime. Moreover, since MLM has longer inputs,\nwe increased the range of the window pattern to\n{31,41,51,75,101,125,151,175,201,251}.\nWe show in Figure 4 the Pareto curves for the\ntradeoff between sparsity and recall (left), and the\ntradeoff between sparsity and perplexity (right).\nThe curves for the sparsity-recall tradeoff are simi-\nlar to the ones found in MT experiments, with the\ndistance-based method outperforming all methods,\n4https://huggingface.co/roberta-base\n73\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nSparsity\n0.4\n0.6\n0.8\n1.0Recall\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nSparsity\n0\n10\n20\n30BLEU\nBaseline BigBird Longformer Reformer Routing Sf. distance Sf. k-means Sf. quant.\n0.4 0.5 0.6 0.7 0.8 0.9 1.0\nSparsity\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95Recall\n0.4 0.5 0.6 0.7 0.8 0.9 1.0\nSparsity\n6.0\n5.5\n5.0\n4.5\n4.0\n3.5\nNeg. Perplexity\nFigure 4: Sparsity-recall and sparsity-(neg-)perplexity tradeoff averaged across all layers and heads on WikiText-103.\nThe vertical dashed line represents the gold sparsity obtained by the full α-entmax transformer.\nfollowed by the k-means variant of Sparsefinder\nand Routing Transformer. In terms of perplexity,\nour distance-based approach also Pareto-dominates\nother methods, followed by our clustering vari-\nant and Routing Transformer. As in the MT ex-\nperiments, the window baseline yields a similar\nsparsity-recall curve to other approaches, reinforc-\ning the importance of local patterns. Although the\ndistance-based method requires a quadratic num-\nber of computations, it reduces them by a factor\nof d/r = 64/4 = 16, as described in §4.3, and\nachieves better recall and perplexity than any other\ntested method. This finding indicates clear room\nfor improvement in designing efficient attention\nmethods that have a better tradeoff between effi-\nciency and accuracy than existing approaches.\nLearned patterns. In Figure 5 we show\nSparsefinder k-means’ predicted attention graphs\nfor a specific attention head that originally learned\nto focus on coreference tokens. We can see that the\npattern induced by Sparsefinder keeps the behav-\nior of attending to coreferences. Concretely, our\nmethod achieves a high recall score (∼80%) with\na high sparsity rate (∼75%) on this attention head.\nFigure 5: Attention pattern learned by Sparsefinder k-\nmeans that focus on coreference tokens.\nCluster analysis. To understand what is repre-\nsented in each cluster learned by Sparsefinder k-\nmeans, we run the following experiment: we obtain\nPOS tags using spaCy,5 and calculate the distribu-\ntion of each tag over clusters for all heads. We\nshow an example in Figure 6, where Sparsefinder\nlearned a cluster that makes verbs and nouns attend\nto themselves, and additionally to most auxiliary\nverbs.\nADJADPADVAUXCCONJ\nDETINTJNOUNNUMPARTPRONPROPNPUNCTSCONJSPACESYMVERB\nX\n0%\n20%\n40%\n60%\n80% Queries\nKeys\nFigure 6: Percentage of POS tags assigned to a given\ncluster on the entire Wikitext 103 validation set.\n7 Conclusions\nWe proposed Sparsefinder, a method to identify\nthe sparsity pattern of entmax-based transformers\nwhile avoiding full computation of the score matrix.\nOur method learns a low-dimensional projection of\nqueries and keys with a contrastive objective, and\ncomes with three variants: distance, quantization,\nand clustering-based. We compared these variants\nagainst competing approaches on two tasks: ma-\nchine translation and masked language modeling.\nWe obtained favorable sparsity-recall and sparsity-\naccuracy tradeoff curves. Our theoretical sparsity\nprovides a lower bound for how much computa-\ntional sparsity can be achieved, and may guide\nfuture research on efficient transformers.\n5https://spacy.io/\n74\nAcknowledgments\nThis work was supported by the European Re-\nsearch Council (ERC StG DeepSPIN 758969),\nby the P2020 project MAIA (LISBOA-01-\n0247- FEDER045909), and by the Fundação\npara a Ciência e Tecnologia through project\nPTDC/CCI-INF/4703/2021 (PRELUNA) and con-\ntract UIDB/50008/2020.\nReferences\nAurélien Bellet, Amaury Habrard, and Marc Sebban.\n2015. Metric learning. Synthesis Lectures on Artifi-\ncial Intelligence and Machine Learning, 9(1):1–151.\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv:2004.05150.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In Advances in Neural Information Process-\ning Systems (NeurIPS), volume 33, pages 1877–1901.\nCurran Associates, Inc.\nMauro Cettolo, Marcello Federico, Luisa Bentivogli,\nNiehues Jan, Stüker Sebastian, Sudoh Katsuitho,\nYoshino Koichiro, and Federmann Christian. 2017.\nOverview of the iwslt 2017 evaluation campaign. In\nProceedings of the 14th International Workshop on\nSpoken Language Translation (IWSLT), pages 2–14.\nRewon Child, Scott Gray, Alec Radford, and\nIlya Sutskever. 2019. Generating long se-\nquences with sparse transformers. arXiv preprint\narXiv:1904.10509.\nKrzysztof Marcin Choromanski, Valerii Likhosherstov,\nDavid Dohan, Xingyou Song, Andreea Gane, Tamas\nSarlos, Peter Hawkins, Jared Quincy Davis, Afroz\nMohiuddin, Lukasz Kaiser, David Benjamin Be-\nlanger, Lucy J Colwell, and Adrian Weller. 2021. Re-\nthinking attention with performers. In International\nConference on Learning Representations (ICLR).\nGonçalo M. Correia, Vlad Niculae, and André F. T.\nMartins. 2019. Adaptively sparse transformers. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 2174–\n2184, Hong Kong, China. Association for Computa-\ntional Linguistics.\nGiannis Daras, Nikita Kitaev, Augustus Odena, and\nAlexandros G Dimakis. 2020. Smyrf - efficient at-\ntention using asymmetric clustering. In Advances in\nNeural Information Processing Systems, volume 33,\npages 6476–6489. Curran Associates, Inc.\nRenato Cordeiro de Amorim. 2012. Constrained clus-\ntering with minkowski weighted k-means. In 2012\nIEEE 13th International Symposium on Computa-\ntional Intelligence and Informatics (CINTI) , pages\n13–17. IEEE.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nMiquel Esplà, Mikel Forcada, Gema Ramírez-Sánchez,\nand Hieu Hoang. 2019. ParaCrawl: Web-scale paral-\nlel corpora for the languages of the EU. In Proceed-\nings of Machine Translation Summit XVII Volume 2:\nTranslator, Project and User Tracks, pages 118–119,\nDublin, Ireland. European Association for Machine\nTranslation.\nPatrick Fernandes, Kayo Yin, Graham Neubig, and An-\ndré F. T. Martins. 2021. Measuring and increasing\ncontext usage in context-aware machine translation.\nIn Joint Conference of the 59th Annual Meeting of\nthe Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural\nLanguage Processing (ACL-IJCNLP), Virtual.\nA. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret.\n2020. Transformers are rnns: Fast autoregressive\ntransformers with linear attention. In Proceedings of\nthe International Conference on Machine Learning\n(ICML).\nNikita Kitaev, Steven Cao, and Dan Klein. 2019. Multi-\nlingual constituency parsing with self-attention and\npre-training. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 3499–3505, Florence, Italy. Association for\nComputational Linguistics.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efficient transformer. In In-\nternational Conference on Learning Representations\n(ICLR).\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nAndre Martins and Ramon Astudillo. 2016. From soft-\nmax to sparsemax: A sparse model of attention and\nmulti-label classification. In International Confer-\nence on Machine Learning (ICML) , volume 48 of\nProceedings of Machine Learning Research, pages\n1614–1623, New York, New York, USA. PMLR.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In 5th International Conference on Learning\nRepresentations (ICLR).\n75\nF. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\nR. Weiss, V . Dubourg, J. Vanderplas, A. Passos,\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duch-\nesnay. 2011. Scikit-learn: Machine learning in\nPython. Journal of Machine Learning Research\n(JMLR), 12:2825–2830.\nBen Peters, Vlad Niculae, and André F. T. Martins. 2019.\nSparse sequence-to-sequence models. In Proceed-\nings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 1504–1519, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186–\n191, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nAlessandro Raganato, Yves Scherrer, and Jörg Tiede-\nmann. 2020. Fixed encoder self-attention patterns\nin transformer-based machine translation. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2020, pages 556–568, Online. Association\nfor Computational Linguistics.\nAlessandro Raganato and Jörg Tiedemann. 2018. An\nanalysis of encoder representations in transformer-\nbased machine translation. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP , pages\n287–297, Brussels, Belgium. Association for Com-\nputational Linguistics.\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and\nDavid Grangier. 2021. Efficient content-based sparse\nattention with routing transformers. Transactions\nof the Association for Computational Linguistics\n(TACL), 9:53–68.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1715–1725,\nBerlin, Germany. Association for Computational Lin-\nguistics.\nSimeng Sun, Kalpesh Krishna, Andrew Mattarella-\nMicke, and Mohit Iyyer. 2021. Do long-range lan-\nguage models actually use long-range context? In\nProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, pages 807–\n822, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nYi Tay, Dara Bahri, Liu Yang, Donald Metzler, and\nDa-Cheng Juan. 2020. Sparse sinkhorn attention.\nIn International Conference on Machine Learning\n(ICML), pages 9438–9447. PMLR.\nConstantino Tsallis. 1988. Possible generalization of\nboltzmann-gibbs statistics. Journal of Statistical\nPhysics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems (NeurIPS), volume 30, pages 5998–\n6008. Curran Associates, Inc.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 5797–5808, Florence, Italy.\nAssociation for Computational Linguistics.\nA. Vyas, A. Katharopoulos, and F. Fleuret. 2020. Fast\ntransformers with clustered attention. In Proceedings\nof the International Conference on Neural Informa-\ntion Processing Systems (NeurIPS).\nKiri Wagstaff, Claire Cardie, Seth Rogers, and Stefan\nSchrödl. 2001. Constrained k-means clustering with\nbackground knowledge. In International Conference\non Machine Learning (ICML), page 577–584.\nShuohang Wang, Luowei Zhou, Zhe Gan, Yen-Chun\nChen, Yuwei Fang, Siqi Sun, Yu Cheng, and Jingjing\nLiu. 2021. Cluster-former: Clustering-based sparse\ntransformer for question answering. In Findings of\nthe Association for Computational Linguistics: ACL-\nIJCNLP 2021, pages 3958–3968, Online. Association\nfor Computational Linguistics.\nSinong Wang, Belinda Li, Madian Khabsa, Han Fang,\nand Hao Ma. 2020. Linformer: Self-attention with\nlinear complexity. arXiv preprint arXiv:2006.04768.\nKilian Q Weinberger and Lawrence K Saul. 2009. Dis-\ntance metric learning for large margin nearest neigh-\nbor classification. Journal of Machine Learning Re-\nsearch (JMLR), 10(2).\nEric P Xing, Andrew Y Ng, Michael I Jordan, and Stuart\nRussell. 2002. Distance metric learning with applica-\ntion to clustering with side-information. In Advances\nin Neural Information Processing Systems (NeurIPS),\nvolume 15, page 12.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, et al. 2020. Big bird: Transformers for\nlonger sequences. Advances in Neural Information\nProcessing Systems (NeurIPS), 33.\nBiao Zhang, Ivan Titov, and Rico Sennrich. 2021.\nSparse attention with linear units. arXiv preprint\narXiv:2104.07012.\nGuangxiang Zhao, Junyang Lin, Zhiyuan Zhang, Xu-\nancheng Ren, Qi Su, and Xu Sun. 2019. Explicit\nsparse transformer: Concentrated attention through\nexplicit selection. arXiv preprint arXiv:1912.11637.\n76\nA Sparse Attention\nA natural way to get a sparse attention distribution is by using the sparsemax transformation (Martins\nand Astudillo, 2016), which computes an Euclidean projection of the score vector onto the probability\nsimplex △n := {p ∈Rn |p ≥0, 1⊤p = 1}, or, more generally, the α-entmax transformation (Peters\net al., 2019):\nα-entmax(z) :=arg max\np∈△n\np⊤z + Hα(p), (10)\nwhere Hα is a generalization of the Shannon and Gini entropies proposed by Tsallis (1988), parametrized\nby a scalar α≥1:\nHα(p) :=\n{ 1\nα(α−1)\n∑\nj(pj −pα\nj), α ̸= 1\n−∑\nj pj log pj, α = 1.\n(11)\nSetting α= 1recovers the softmax function, while for any value of α> 1 this transformation can return\na sparse probability vector. Letting α= 2, we recover sparsemax. A popular choice is α= 1.5, which\nhas been successfully used in machine translation and morphological inflection applications (Peters et al.,\n2019; Correia et al., 2019).\nProof to Proposition 1.\nProof. From the definition of z|m and from Eq. 2, we have that\n{\nzj|m = zj > τ(z)\nα−1 if p∗\nj >0\nzj|m ≤zj ≤τ(z)\nα−1 if p∗\nj = 0. (12)\nWe first prove thatτ(z|m) =τ(z). From the definition ofτ(z) we have that∑\nj[(α−1)zj−τ(z)]\n1/α−1\n+ = 1.\nPlugging the (in)equalities from Eq. 12, we thus have\n1 =\n∑\nj\n[(α−1)zj −τ(z)]\n1/α−1\n+ =\n∑\nj\n[(α−1)zj|m −τ(z)]\n1/α−1\n+ . (13)\nSince τ(z) satisfies the second equation – which is the condition that defines τ(z|m) – we thus conclude\nthat τ(z|m) =τ(z). Combining the results in Eqs. 12–13, we see that the supports of α-entmax(z) and\nα-entmax(z|m) are the same and so are the thresholds τ, and therefore from Eq. 2 we conclude that\nα-entmax(z|m) =α-entmax(z).\nB Computing infrastructure\nOur infrastructure consists of 4 machines with the specifications shown in Table 1. The machines were\nused interchangeably, and all experiments were executed in a single GPU. Despite having machines with\ndifferent specifications, we did not observe large differences in the execution time of our models across\ndifferent machines.\n# GPU CPU\n1. 4 ×Titan Xp - 12GB 16 ×AMD Ryzen 1950X @ 3.40GHz - 128GB\n2. 4 ×GTX 1080 Ti - 12GB 8 ×Intel i7-9800X @ 3.80GHz - 128GB\n3. 3 ×RTX 2080 Ti - 12GB 12 ×AMD Ryzen 2920X @ 3.50GHz - 128GB\n4. 3 ×RTX 2080 Ti - 12GB 12 ×AMD Ryzen 2920X @ 3.50GHz - 128GB\nTable 1: Computing infrastructure.\nC Machine Translation\nC.1 Setup\nData. Statistics for all datasets used in MT experiments can be found below in Table 2.\n77\nDATASET # TRAIN # TEST AVG. SENTENCE LENGTH\nIWSLT17 (EN→DE) 206K 1080 20 ±14 / 19 ±13\nIWSLT17 (EN→FR) 233K 1210 20 ±14 / 21 ±15\nTable 2: Statistics for MT datasets.\nTraining and Model. We replicated the sentence-level model of Fernandes et al. (2021) with the\nexception that we usedα-entmax with α= 1.5 instead of softmax in all attention heads and layers. Table 3\nshows some architecture (transformer large) and training hyperparameters used for MT experiments. We\nrefer to the original work of Fernandes et al. (2021) for more training details.\nHYPERPARAM . V ALUE\nHidden size 1024\nFeedforward size 4096\nNumber of layers 6\nNumber of heads 16\nAttention mapping π 1.5-entmax\nOptimizer Adam\nNumber of epochs 20\nEarly stopping patience 10\nLearning rate 0.0005\nScheduling Inverse square root\nLinear warm-up steps 4000\nDropout 0.3\nCoWord dropout 0.1\nBeam size 5\nTable 3: Hyperparmeters for neural machine translation models.\nC.2 Projections setup\nData. Statistics for the subsets of IWSLT used in the projection analysis can be found below in Table 4.\nTRAIN VALIDATION\nPAIR # SENT . # POS . PAIRS AVG. SENT . LENGTH # SENT . # POS . PAIRS AVG. SENT . LENGTH\nEN→DE 9K 8M ±1M 35 ±16 1K 330K ±56K 36 ±17\nEN→FR 9K 9M ±1M 37 ±17 1K 334K ±58K 37 ±16\nTable 4: Statistics for subsets of IWSLT used for training and evaluating projections.\nTraining. After extracting theα-entmax graphs, we optimize the learnable parameters of Equation 7 with\nAdam over a single epoch. Moreover, we used the k-means implementation from scikit-learn (Pedregosa\net al., 2011) for our clustering-based approach. The hyperparameters used both for training the projections\nand for clustering with k-means are shown in Table 5.\nProjection analysis. We compare Sparsefinder, varying B ∈ {2,4,6,8,10,12}for bucket-based\nmethods, and t∈{0.5,1.0,1.5,2.0,2.5}for the distance-based variant, with the following methods:\n• Window baseline: connect all query and key pairs within a sliding window of size w ∈\n{0,1,3,5,7,9,11,15,19,23,27}.\n• Learnable patterns: Reformer by varying the number of buckets within {2,4,6,8,10,12}; Routing\ntransformer by varying the number of clusters within c∈{2,4,6,8,10}with top-kset to ⌈n/c⌉(i.e.\nbalanced clusters).\n• Fixed patterns: BigBird by varying the number of random blocks within {2,4,6,8,10}with a block\nsize of 1; Longformer by varying the number of random global tokens within {4,8,12,16,20}.\n78\nHYPERPARAM . V ALUE\nProjection dim. r 4\nLoss margin ω 1.0\nBatch size 16\nOptimizer Adam\nNumber of epochs 1\nLearning rate 0.01\nℓ2 regularization 0\nk-means init k-means++\nk-means max num. inits 10\nk-means max iters 300\nTable 5: Hyperparmeters for MT projections.\nSparsity-recall tradeoff per layer and head. Plots are shown in Figures 7 and 8 for EN→DE and\nEN→FR, respectively.\nFigure 7: Sparsity-recall tradeoffs with a fixed window pattern of size 11 for EN→DE.\nFigure 8: Sparsity-recall tradeoffs with a fixed window pattern of size 11 for EN→FR.\nD Masked Language Modeling\nD.1 Setup\nData and model. In order to have a transformer model trained with α-entmax, we finetuned RoBERTa-\nBase (Liu et al., 2019) on WikiText-103 (Merity et al., 2017) over 3000 steps with Adam (learning rate of\n79\n3 ×10−5). To mimic the finetuning approach adopted by Longformer, we employed a batch size of 2 by\naccumulating gradients over 32 steps due to GPU memory constraints. Table 6 shows some architecture\n(transformer large) and training hyperparameters used for MT experiments. We refer to the original work\nof Liu et al. (2019) for more architecture details.\nHYPERPARAM . V ALUE\nHidden size 64\nFeedforward size 3072\nMax input length 514\nNumber of layers 12\nNumber of heads 12\nAttention mapping π 1.5-entmax\nOptimizer Adam\nNumber of steps 3000\nLearning rate 0.00003\nTable 6: Hyperparmeters for masked language modeling models.\nD.2 Projections setup\nData and training. The subset used for Masked LM projections experiments contains 500 instances for\ntraining and 500 instances for validation. Moreover, all instances have a sentence length of 512 tokens.\nWe got 3M (±1M) positive pairs for training and 2.5M (±1M) for validation. The hyperparameters for\nMasked LM are the same as the ones used in the MT experiments, shown in Table 5.\nProjection analysis. We perform the same analysis as in MT, but now we vary the window size of the\nbaseline within {0, 1, 3, 7, 11, 25, 31, 41, 51, 75, 101, 125, 151, 175, 201, 251, 301, 351, 401, 451, 501,\n512}.\nSparsity-recall tradeoff per layer and head. Plots are shown next in Figure 9.\nFigure 9: Sparsity-recall tradeoffs with a fixed window pattern of size 25 for MLM.\nE Attention plots\nExamples of attention maps can be seen in Figure 10 and 11.\n80\nFigure 10: Learned patterns by Sparsefinder k-means (left) and the subsequent attention weights (right). Starred\nblocks represent ground-truth edges.\nFigure 11: Learned patterns by Sparsefinder k-means (left) and the subsequent attention weights (right). Starred\nblocks represent ground-truth edges.\n81",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6901572942733765
    },
    {
      "name": "Quadratic equation",
      "score": 0.6610639691352844
    },
    {
      "name": "Softmax function",
      "score": 0.6355798840522766
    },
    {
      "name": "Transformer",
      "score": 0.6313097476959229
    },
    {
      "name": "Machine translation",
      "score": 0.5788248777389526
    },
    {
      "name": "Computation",
      "score": 0.5298228859901428
    },
    {
      "name": "Encoder",
      "score": 0.5088644027709961
    },
    {
      "name": "Cluster analysis",
      "score": 0.47928446531295776
    },
    {
      "name": "Quantization (signal processing)",
      "score": 0.4222263693809509
    },
    {
      "name": "Algorithm",
      "score": 0.4218597412109375
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4212251305580139
    },
    {
      "name": "Language model",
      "score": 0.41792672872543335
    },
    {
      "name": "Theoretical computer science",
      "score": 0.39367520809173584
    },
    {
      "name": "Machine learning",
      "score": 0.35362154245376587
    },
    {
      "name": "Mathematics",
      "score": 0.162103533744812
    },
    {
      "name": "Artificial neural network",
      "score": 0.12139999866485596
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}