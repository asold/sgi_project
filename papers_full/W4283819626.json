{
  "title": "CF-DETR: Coarse-to-Fine Transformers for End-to-End Object Detection",
  "url": "https://openalex.org/W4283819626",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2762012729",
      "name": "Xipeng Cao",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A1986837181",
      "name": "Peng Yuan",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2136461635",
      "name": "Bailan Feng",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2020259551",
      "name": "Kun Niu",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A1986837181",
      "name": "Peng Yuan",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2136461635",
      "name": "Bailan Feng",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6742650655",
    "https://openalex.org/W2772955562",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W3103465009",
    "https://openalex.org/W6676297131",
    "https://openalex.org/W3123547918",
    "https://openalex.org/W2996105331",
    "https://openalex.org/W6735463952",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W3183430956",
    "https://openalex.org/W6730903564",
    "https://openalex.org/W6742348326",
    "https://openalex.org/W6639102338",
    "https://openalex.org/W6749783731",
    "https://openalex.org/W6785652829",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W6800709815",
    "https://openalex.org/W1483870316",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W3110402800",
    "https://openalex.org/W3107331169",
    "https://openalex.org/W2925359305",
    "https://openalex.org/W2946942188",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W3146455718",
    "https://openalex.org/W3159196909",
    "https://openalex.org/W2991833700",
    "https://openalex.org/W2902303185",
    "https://openalex.org/W6762082143",
    "https://openalex.org/W3034175346",
    "https://openalex.org/W2964121718",
    "https://openalex.org/W3034307881",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2982770724",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W4287375617",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W3035396860",
    "https://openalex.org/W2964241181",
    "https://openalex.org/W639708223",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W3106250896",
    "https://openalex.org/W4214627427",
    "https://openalex.org/W4320002812",
    "https://openalex.org/W2966926453",
    "https://openalex.org/W4297785061",
    "https://openalex.org/W3165924482",
    "https://openalex.org/W3172752666",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2963857746",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963037989",
    "https://openalex.org/W2884561390"
  ],
  "abstract": "The recently proposed DEtection TRansformer (DETR) achieves promising performance for end-to-end object detection. However, it has relatively lower detection performance on small objects and suffers from slow convergence. This paper observed that DETR performs surprisingly well even on small objects when measuring Average Precision (AP) at decreased Intersection-over-Union (IoU) thresholds. Motivated by this observation, we propose a simple way to improve DETR by refining the coarse features and predicted locations. Specifically, we propose a novel Coarse-to-Fine (CF) decoder layer constituted of a coarse layer and a carefully designed fine layer. Within each CF decoder layer, the extracted local information (region of interest feature) is introduced into the flow of global context information from the coarse layer to refine and enrich the object query features via the fine layer. In the fine layer, the multi-scale information can be fully explored and exploited via the Adaptive Scale Fusion(ASF) module and Local Cross-Attention (LCA) module. The multi-scale information can also be enhanced by another proposed Transformer Enhanced FPN (TEF) module to further improve the performance. With our proposed framework (named CF-DETR), the localization accuracy of objects (especially for small objects) can be largely improved. As a byproduct, the slow convergence issue of DETR can also be addressed. The effectiveness of CF-DETR is validated via extensive experiments on the coco benchmark. CF-DETR achieves state-of-the-art performance among end-to-end detectors, e.g., achieving 47.8 AP using ResNet-50 with 36 epochs in the standard 3x training schedule.",
  "full_text": "CF-DETR: Coarse-to-Fine Transformers for End-to-End Object Detection\nXipeng Cao1\u0003, Peng Yuan2y, Bailan Feng2, Kun Niu1y\n1 Beijing University of Posts and Telecommunications\n2 Huawei Noah’s Ark Lab\nfxpcao,niukung@bupt.edu.cn, fyuanpeng126,fengbailang@huawei.com\nAbstract\nThe recently proposed DEtection TRansformer (DETR)\nachieves promising performance for end-to-end object detec-\ntion. However, it has relatively lower detection performance\non small objects and suffers from slow convergence. This pa-\nper observed that DETR performs surprisingly well even on\nsmall objects when measuring Average Precision (AP) at de-\ncreased Intersection-over-Union (IoU) thresholds. Motivated\nby this observation, we propose a simple way to improve\nDETR by reﬁning the coarse features and predicted locations.\nSpeciﬁcally, we propose a novel Coarse-to-Fine (CF) decoder\nlayer constituted of a coarse layer and a carefully designed\nﬁne layer. Within each CF decoder layer, the extracted local\ninformation (region of interest feature) is introduced into the\nﬂow of global context information from the coarse layer to\nreﬁne and enrich the object query features via the ﬁne layer.\nIn the ﬁne layer, the multi-scale information can be fully\nexplored and exploited via the Adaptive Scale Fusion(ASF)\nmodule and Local Cross-Attention (LCA) module. The multi-\nscale information can also be enhanced by another proposed\nTransformer Enhanced FPN (TEF) module to further improve\nthe performance. With our proposed framework (named CF-\nDETR), the localization accuracy of objects (especially for\nsmall objects) can be largely improved. As a byproduct, the\nslow convergence issue of DETR can also be addressed. The\neffectiveness of CF-DETR is validated via extensive exper-\niments on the coco benchmark. CF-DETR achieves state-of-\nthe-art performance among end-to-end detectors, e.g., achiev-\ning 47.8 AP using ResNet-50 with 36 epochs in the standard\n3x training schedule.\nIntroduction\nObject detection which involves classiﬁcation and localiza-\ntion subtasks is a fundamental problem in the ﬁeld of Com-\nputer Vision (Zou et al. 2019; Zaidi et al. 2021). The mod-\nern object detectors (Liu et al. 2016; Redmon et al. 2016; Lin\net al. 2020; Ren et al. 2017; He et al. 2017; Zhang et al. 2020)\nrely on post-processing (e.g., ”non-maximum suppression”\nor NMS) to get robust detection results. Recently, DEtec-\ntion TRansformer (DETR) (Carion et al. 2020) has been pro-\nposed as a fully end-to-end object detector, which does not\n\u0003This work is done when Xipeng Cao was an intern in Huawei\nNoah’s Ark Lab.\nyCorresponding author.\nCopyright c\r 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nrely on NMS. It utilizes object queries that contain proper-\nties of objects (feature, shape, location, etc.), to query from\na global context through a cross-attention mechanism. Al-\nthough DETR achieves promising performance for end-to-\nend object detection, it is thought to have relatively poor de-\ntection performance on small objects and suffers from slow\nconvergence.\nHowever, when observing the COCO-style metric Aver-\nage Precision (AP) at different IOU thresholds, we get more\ninsights into DETR’s behavior. As illustrated in Figure 1,\nwe calculated the AP results on COCO validation set at var-\nious IoU thresholds for three methods: DETR-R50, Sparse\nR-CNN-R50, and Faster R-CNN-R101. Note that the cho-\nsen three methods have similar performances measured with\nconventional AP 50:95. Figure 1(a) shows that DETR per-\nforms much better than other methods when measuring AP\nwith low IoU thresholds. Figure 1(b) further shows AP re-\nsults on small objects at different IoU thresholds. While\nDETR indeed performs poorly at high IoU thresholds (e.g.\nfrom 0.5 to 0.9), it performs surprisingly well (even better\nthan Sparse R-CNN-R50 with FPN) when measuring AP\nwith low IoU thresholds (e.g. from 0.1 to 0.4). We also sum-\nmarize the AP scores at different ranges in Table 1, which\nclearly shows the superiority of DETR when measuring AP\nin the low IOU thresholds range. This phenomenon implies\nthe strong perception ability of DETR even for small ob-\njects, and the reason why DETR is poorer on small objects is\nthat the bounding box location is not accurate enough com-\npared with Region of Interests (RoI) feature based methods\n(like Faster R-CNN and Sparse R-CNN). And that the key\nto improving DETR is simply to reﬁne the coarse predicted\nlocations by introducing local information\nSeveral methods have been proposed to explore local in-\nformation in DETR architecture (Zhu et al. 2020; Sun et al.\n2020; Gao et al. 2021). Deformable DETR (Zhu et al. 2020)\nleverages multi-scale deformable encoder and sparse sam-\npling for local information rather than global information.\nInstead of using the cross-attention module, TSP (Sun et al.\n2020) combines R-CNN- or FCOS-based (Tian et al. 2019)\nmethods with the Transformer encoder to focus on local in-\nformation. Different from the above methods, this paper pro-\nposes a new way to fully utilize the global context informa-\ntion and local information in a coarse-to-ﬁne manner. Al-\nthough this approach is not proposed directly to solve the\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n185\n10 20 30 40 50\nIoU threshold (%)\n0.60\n0.65\n0.70\n0.75\nAP\nDETR-R50\nSparse R-CNN-R50\nFaster R-CNN-R101\n(a) AP results at IoU threshold of 0.1 to 0.5.\n10 20 30 40 50 60 70 80 90\nIoU threshold (%)\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\nAPs\nDETR-R50\nSparse R-CNN-R50\nFaster R-CNN-R101 (b) AP results on small objects at IoU threshold of 0.1 to 0.9.\nFigure 1: AP results on COCO validation set at various IoU thresholds: DETR-R50 vs. Sparse R-CNN-R50 vs. Faster R-CNN-\nR101. (a) shows AP results at IoU thresholds from 0.1 to 0.5. DETR performs much better than other methods under this setting.\nNote that all the compared methods have similar performance measured with conventional AP 50:95. (b) shows AP results on\nsmall objects at IoU thresholds from 0.1 to 0.9. While DETR indeed performs poorly at the IoU threshold from 0.5 to 0.9, it\nperforms well when measuring AP with low IoU thresholds from 0.1 to 0.4.\nMethod AP AP s AP′ AP′\ns\nFaster R-CNN-R101 42.5 24.2 66.7 47.5\nDETR-R50 42.0 20.5 68.3 49.4\nSparse R-CNN-R50 42.8 26.7 67.2 49.1\nTable 1: Average Precision at different ranges of IoU thresh-\nolds for all objects and small objects. AP and AP’ denotes\nAP50:95 and AP10:50 for all objects separately. APs and AP′\ns\ndenotes AP50:95 and AP10:50 for small objects separately.\nlow convergence problem, we believe that it is in line with\nmitigating this issue. As the predicted boxes are reﬁned to be\nmore accurate, the label-assignment matching process will\nbe more stable, therefore the training process will be more\nefﬁcient (Sun et al. 2020).\nIn this paper, a Coarse-to-Fine DEtection TRansformer\n(CF-DETR) is proposed (see Figure 2), which retains the\nnon-local encoder-decoder architecture of DETR to inherit\nits strong perception ability. In CF-DETR, a coarse-to-ﬁne\n(CF) decoder layer constituted of a coarse layer and a ﬁne\nlayer, is designed to improve the localization accuracies.\nWith the CF structure, the local multi-scale ROI informa-\ntion can be extracted and introduced into the ﬂow of global\nattention information from the coarse layer to gradually en-\nrich the object query features via the ﬁne layer. In the ﬁne\nlayer, we propose a novel Adaptive Scale Fusion (ASF)\nmodule, which leverages object query features to adaptively\nfuse ROI features from different scales. The fused ROI fea-\ntures are further feed into a novel Local Cross-Attention\n(LCA) module to reﬁne and enrich object query features via\nthe cross-attention interactions. Compared with the vanilla\ncross-attention, the proposed attention module is more con-\nducive to obtaining the local and spatial information of ob-\njects and its convergence is faster. In addition, the origi-\nnal multi-scale features can be enhanced by a novel Trans-\nformer Enhanced FPN (TEF) module, which transfers\nthe high-level non-local information extracted from Trans-\nformer Encoder to the low-level features in an FPN manner,\nbringing further performance improvement of CF-DETR.\nThe main contributions of this paper are as follows:\n\u000fA new end-to-end object detection transformer frame-\nwork named CF-DETR is proposed. In CF-DETR, a\nnovel CF decoder layer is proposed to reﬁne and enrich\nthe features in a coarse-to-ﬁne manner by fusing local\nand global information.\n\u000fIn the ﬁne layer, an ASF module and an LCA module\nare proposed to fully explore and exploit the multi-scale\nROI information. In addition, a TEF module is proposed\nto enhance the original multi-scale information, further\nimproving the performance of CF-DETR.\n\u000fThe effectiveness of CF-DETR is demonstrated by the\nexperimental results on the challenging COCO dataset\n(Lin et al. 2014). CF-DETR achieves state-of-the-art per-\nformance among end-to-end detectors, e.g., achieving\n47.8 AP using ResNet-50 with 36 epochs in the standard\n3x training schedule.\nRelated Work\nOne-stage and Two-stage Object Detectors\nPrevious deep learning-based object detectors can be di-\nvided into two categories: one-stage and two-stage detec-\ntors. Typically, one-stage detectors such as SSD (Liu et al.\n2016), YOLO (Redmon et al. 2016), and RetinaNet (Lin\net al. 2020), directly conduct object classiﬁcation and loca-\ntion on pixels of the output feature map. While two-stage\ndetectors (Ren et al. 2017; He et al. 2017) ﬁrst generate RoI\nbased on sliding-window locations. And then they leverage\nthe RoI align layer to extract ﬁne-grained features and reﬁne\nproposals.\n186\nBackbone\nC5\nC4\nC3\nFine Layer\nImage\nPositional\nEncoding Class\nCoarse\nBox\nFeed-Forward\nCoarse Layer\nTransformer Enhanced\nFPN\nE5\nE4\nE3 ROI Align\nFine\nBox\nAdaptive Scale\nFusion\nLocal Cross-\nAttention\nSelf-Attention\nCross-Attention\nEncoder\nLayer 1 Self-Attention\nFeed-Forward\nTransformer Encoder\nEncoder\nLayer 2\n...\nEncoder\nLayer M E5\nE4\nE3\nCF Decoder  \nLayer 1\nSelf-Attention\nEncoder\nLayer 1 Self-Attention\nFeed-Forward\nTransformer Encoder\nEncoder\nLayer 2\n...\nEncoder\nLayer MC5\nC4\nC3\nInit object query features\n*N\nFeed to next coarse layer\nFigure 2: The overview of CF-DETR. CF-DETR follows the main encoder-decoder architecture of DETR, with a novel TEF\nmodule and novel CF decoder layers. The features from Transformer Encoder and TEF module are taken as inputs by CF\ndecoder layers. Each CF decoder layer contains a coarse layer and a ﬁne layer. The coarse layer follows the traditional Trans-\nformer decoder layer structure. The ﬁne layer leverages multi-scale RoI features to reﬁne coarse boxes from the coarse layer\nvia the ASF and LCA modules. The object query features are passed through N cascaded CF decoder layers.\nHowever, all these methods require hand-crafted princi-\nples (e.g., intersection-over-union (IoU) threshold) when as-\nsigning predictions to ground-truth object boxes. Moreover,\nthe leverage of NMS post-processing (Bodla et al. 2017) to\nremove redundant boxes is also necessary for the inference\nphase.\nEnd-to-End Detectors\nRecently, DEtection TRansformer (DETR) (Carion et al.\n2020) has been proposed as end-to-end object detection,\nwhich utilizes Hungarian matching for label assignment. Al-\nthough it achieves comparable performance with Faster R-\nCNN, it has relatively lower detection performance on small\nobjects and suffers from slow convergence.\nTo accelerate the convergence speed of DETR, De-\nformable DETR (Zhu et al. 2020) proposes a deformable\nencoder, which extracts multi-scale features naturally via\nlearnable sparse sampling. Based on Deformable DETR, Ef-\nﬁcient DETR (Yao et al. 2021) proposes that a great ini-\ntialization of object queries could help the model converge.\nWith a dense-to-sparse structure, Efﬁcient DETR builds a\nsimple yet efﬁcient end-to-end detector with one decoder\nlayer. TSP (Sun et al. 2020) points out that the cross-\nattention mechanism is the main reason for the slow conver-\ngence of DETR, such that they propose to combine R-CNN-\nor FCOS-based (Tian et al. 2019) methods with Transformer\nencoders . While Deformable DETR and TSP explored lo-\ncal information, SMCA (Gao et al. 2021) explored global\ninformation with a self-attention and co-attention mecha-\nnism to accelerate convergence speed. On the other hand,\nUP-DETR (Dai et al. 2020) proposes a novel self-supervised\nDETR. It enhances convergence speed and performance by\npre-training the Transformer encoder in DETR.\nRecently, Sparse R-CNN (Sun et al. 2021) proposes a\nfully sparse structure with an end-to-end set prediction loss.\nIt utilizes a dynamic interaction module to extract ﬁne object\nfeatures from local RoI features.\nDifferent from the above methods, this paper proposes a\ncoarse-to-ﬁne manner to fully utilize both the global con-\ntext information and local ROI information. It efﬁciently im-\nproves the model localization capability and only requires\nstandard 3x training strategies to converge.\nMethod\nThe central idea of the CF-DETR framework is to reﬁne\ncoarse bounding boxes. With this framework, both the global\ncontext information and local ROI information can be uti-\nlized efﬁciently, and the multi-scale information can be en-\nhanced and fully explored. Therefore the predicted boxes\nbased on the reﬁned and enriched object query features will\nbe more accurate.\nOverview\nFigure 2 shows the pipeline of CF-DETR. It follows the\nmain encoder-decoder architecture of DETR. Different from\nDETR, CF-DETR has a novel TEF module and novel CF\ndecoder layers. The features from Transformer Encoder and\nTEF module are taken as inputs by CF Decoder layers to\ncomplete the following detection tasks. Each CF decoder\nlayer contains a coarse layer and a ﬁne layer. The coarse\nlayer extracts object-related features from global context\nsemantics. And the ﬁne layer leverages multi-scale RoI\nfeatures to reﬁne coarse boxes from the coarse layer via\nthe ASF and LCA modules. The object query features are\npassed through the cascaded CF decoder layers and are op-\ntimized together with network parameters.\n187\nConcat\nFC\nDW\nConv\nConv\n1x1\nFused Feature i\nROI\nFeatures i\nObject\nQuery i\nFigure 3: illustrates the details of the Adaptive Scale Fusion\nmodule.\nModules\nTEF Module. With the Transformer Encoder in DETR or\nCF-DETR, the non-local foreground features E5 can be ex-\ntracted from the backbone features C5. We expect this non-\nlocal foreground information could be transmitted to low-\nlevel features to help improve the perception of objects. In-\nspired by FPN (Lin et al. 2017a), we propose a TEF module\nthat works the same way as FPN, except that the features\nC5 is replaced by the output features E5 from Transformer\nEncoder. Speciﬁcally, we ﬁrst add the upsampled E5 to C4,\nthen output the fused feature map E4 after 3 \u00023 convolu-\ntion. A new set of feature maps fEigL\ni=1 can be obtained\nby repeating the above operation between adjacent feature\nmaps, where Lis a hyper-parameter. With the TEF module,\nthe multi-scale information can be enhanced, also the gap\nbetween the features from the coarse layer and ROIs might\nbe alleviated, which may beneﬁt the cross-attention opera-\ntions in the ﬁne layer.\nCoarse-to-Fine Decoder Layer. As illustrated in Figure\n2, the CF decoder layers are cascaded and they take as inputs\na set of learnable object query features and the features from\nthe TEF module to detect objects. Each CF decoder layer\ncontains a coarse layer and a ﬁne layer:\n(1) Coarse Layer: The coarse layer follows the tradi-\ntional Transformer decoder layer structure, which could be\nalso replaced with other variants (Gao et al. 2021; Meng\net al. 2021) of DETR, extracting object-related features from\nglobal context semantics. The self-attention module ﬁrst em-\nbeds the relationships between N object queries as O 2\nRN×c. Given a ﬂattened feature map x 2 RHW ×c from\nTransformer encoder, each object oi 2 R1×c focuses on\ndifferent regions of feature map xto sense an potential ob-\nject via the cross-attention module. Note that learnable ob-\nject query features are directly fed as queries into the cross-\nattention module rather than treated as positional encoding\nin DETR. FFN layers and other layer norms are added into\nthe pipeline similar to the Transformer setting. The bound-\ning box predictions are computed by a 3-layer multilayer\nperception (MLP) with ReLU activation functions. Classiﬁ-\ncation is performed by a single linear layer.\n(2) Fine Layer: The goal of the ﬁne layer is to further\nreﬁne the coarse bounding boxes. It takes as inputs the ob-\nject query features which contain global context information\noutput from the previous coarse layer and the local ROI fea-\ntures extracted from the TEF module. Object query features\nRepeat\nDW Conv\n3x3\nConcat\nConv 1x1 Conv 1x1\nAggregation\nConv 1x1\nFused feature \nV alue \nFC\nInstance Feature\nFlatten\nOutput \nKey Query \nAttention \nObject Query\nFigure 4: illustrates the details of the Local Cross-Attention\nmodule.\ncontaining global information are sufﬁcient for the classiﬁ-\ncation task (Wang et al. 2020). However, more precise local\ninformation (e.g. the shape and boundaries of objects) and\nmulti-scale information are necessary for accurate location\npredictions, especially for small objects. Thus the ﬁne layer\nfurther explores and exploits the multi-scale local ROI fea-\ntures via the proposed ASF and LCA modules.\nASF Module. Given one object query oi 2 R1×c and\nrelated coarse box bi from the coarse layer, the ﬁne layer\nutilizes the ROI align (He et al. 2017) operation to ex-\ntract corresponding multi-scale features ffl\ni 2Rc×h×wgL\nl=1\nfrom the TEF module, where L is the number of levels.\nThe conventional method uses the heuristic method to se-\nlect a speciﬁc layer of ROI feature based on the size of bi.\nThis method does not fully explore high-level semantic in-\nformation. Other works (Liu et al. 2018; Guo et al. 2020)\nhave attempted to aggregate ROI features to improve perfor-\nmance, but these methods do not consider the corresponding\ninstance feature oi. Here, we propose an ASF module to fuse\nmulti-scale features adaptively according to speciﬁc object\nquery features (see Figure 3). Speciﬁcally, all ROI features\nin different scales are concatenated in channel dimensions\nfi 2RLc×h×w. The convolution weights with spatial size\nk\u0002kare generated according to the object query feature via\na fully connected layer. Then the ASF leverages depth-wise\nconvolution to activate informative channels for the object.\nFinally, a 1 \u00021 convolution is applied for reducing dimen-\nsion from Lcto c.\nLCA Module. The fused multi-scale ROI information\nfrom ASF is exploited by object queries in the LCA mod-\nule, which basically implements a Local Cross-Attention\n(LCA) between the object query features and the fused ROI\nfeatures. As spatial information is very important for pre-\ncisely locating an object, different from the non-local multi-\nattention mechanism, LCA enables object queries focusing\non local information when interacting with the fused ROI\nfeatures. Speciﬁcally, for a given pair of object query fea-\nture oi 2R1×c and fused feature f′\ni 2Rc×h×w, LCA ﬁrst\nemploys a depth-wise 3 \u00023 convolution over f′\ni for extract-\ning contextual information from local neighbor points, pro-\nducing the key: fK\ni = DWConv3×3(f′\ni);fK\ni 2Rc×h×w.\nThe query is deﬁned as the expand of object query feature:\noQ\ni 2Rc×h×w, with the same shape asfK\ni . Inspired by CoT-\n188\nMethod Feature Epochs AP AP 50 AP75 APs APm APl FPS\nFaster R-CNN-R50 FPN 36 40.2 61.0 43.8 24.2 43.5 52.0 26\nCascade R-CNN-R50 FPN 36 44.3 62.2 48.0 26.6 47.7 57.7 19\nDETR-R50 (Carion et al. 2020) Encoder 500 42.0 62.4 44.2 20.5 45.8 61.1 28\nDETR-DC5-R50 (Carion et al. 2020) Encoder 500 43.3 63.1 45.9 22.5 47.3 61.1 12\nDeform DETR*-R50 (Zhu et al. 2020) DeformEncoder 50 43.8 62.6 47.7 26.4 47.1 58.0 19\nDeform DETR*++-R50 (Zhu et al. 2020) DeformEncoder 50 46.2 65.2 50.0 28.8 49.2 61.9 19\nSparse R-CNN-R50 (Sun et al. 2021) FPN 36 42.8 61.2 45.7 26.7 44.6 57.6 23\nSparse R-CNN*-R50 (Sun et al. 2021) FPN 36 45.0 63.4 48.2 26.9 47.2 59.5 22\nTSP-R-CNN-R50 (Sun et al. 2020) FPN 36 43.8 63.3 48.3 28.6 46.9 55.7 11\nSMCA*-R50 (Gao et al. 2021) Encoder 108 45.6 65.6 49.1 25.9 49.3 62.6 10\nCF-DETR-R50 TEF 36 46.5 65.2 50.5 28.4 49.3 61.8 18\nCF-DETR*-R50 TEF 36 47.8 66.5 52.4 31.2 50.6 62.8 16\nFaster R-CNN-R101 FPN 36 42.0 62.5 45.9 25.2 45.6 54.6 20\nDETR-R101 (Carion et al. 2020) Encoder 500 43.5 63.8 46.4 21.9 48.0 61.8 20\nDETR-DC5-R101 (Carion et al. 2020) Encoder 500 44.9 64.7 47.7 26.4 47.1 58.0 10\nSparse R-CNN-R101 (Sun et al. 2021) FPN 36 44.1 62.1 47.2 26.1 46.3 59.7 19\nSparse R-CNN*-R101 (Sun et al. 2021) FPN 36 46.4 64.6 49.5 28.3 48.3 61.6 18\nTSP-R-CNN-R101 (Sun et al. 2020) FPN 36 44.8 63.8 49.2 29.0 47.9 57.1 9\nSMCA*-R101 (Gao et al. 2021) Encoder 50 44.4 65.2 48.0 24.3 48.5 61.0 -\nCF-DETR-R101 TEF 36 47.2 65.9 51.1 29.0 50.2 63.4 16\nCF-DETR*-R101 TEF 36 49.0 68.1 53.4 31.4 52.2 64.3 14\nTable 2: Evaluation results of related methods on COCO 2017 val set. The results of other methods are from Detectron2 and\ntheir released papers. Note that ”*” indicates that there are 300 object queries in training. Deform DETR*++ means Deformable\nDETR with the two-stage trick. A single NVIDIA Tesla V100 GPU is used to measure the inference time.\nNet (Li et al. 2021), the attention map is calculated as follow:\nAi = (oQ\ni ;fK\ni )W1W2; (1)\nwhere W1 2R2c×2c=r and W2 2R2c=r×(c∗k0∗k0) are pa-\nrameters of 1\u00021 convolution kernels, andris the dimension\nscaling factor. When we do attention aggregation, vectors\nwithin each k′size of value matrix are weighted together,\nwhich is shown as follows:\nfO\ni (h′;w′) =\nk0\nX\nu=1\nk0\nX\nv=1\nAi;u;v;h0;w0 \ffV\ni (h′;w′)u;v; (2)\nwhere fV\ni 2 Rc×h×w is projected from fused feature f′\ni\nvia 1 \u00021 convolution. And fV\ni (h′;w′)u;v means the neigh-\nbor point (u;v) of point (h′;w′) on the value matrix fV\ni .\nAi;u;v;h ′;w′is the corresponding attention vector on at-\ntention map Ai. Here \fmeans the Hadamard product. In\nthe end, fO\ni is ﬂattened to the shape 1 \u0002(c\u0003h\u0003w) and\nits dimension is reduced to 1 \u0002cthrough one FC layer. The\nreﬁne and enriched object query features are then taken as\ninputs by a 3-layer MLP with ReLU activation functions to\npredict locations.\nIterative Structure. The prediction of the bounding\nboxes is in an iterative structure. Speciﬁcally, within ﬁrst\nCF decoder layer, the ﬁne layer reﬁnes the bounding boxes\nbased on the outputs (the normalized center coordinates,\nheights and widths of the boxes for a given image) of the\ncoarse layer. Also, the CF decoder layers are cascaded to\nfurther improve the performance. The reﬁned object query\nfeatures from the last ﬁne layer will be sent to the next coarse\nlayer. The new coarse layer also reﬁnes the boxes based on\nthe output of the previous ﬁne layer (Cai and Vasconcelos\n2018).\nLoss. The proposed CF-DETR aligns set prediction loss\nwith DETR. Note that, as the ﬁne layer only predicts bound-\ning boxes, the predicted class logits from the coarse layer\nare used when calculating the matching cost. After label as-\nsignment, the total detection loss can be written as follows:\nLdet = \u0015cls \u0001Lcls + \u0015L1 \u0001Lc\nL1 + \u0015L1 \u0001Lf\nL1\n\u0015giou \u0001Lc\ngiou + \u0015giou \u0001Lf\ngiou;\n(3)\nwhere Lcls is focal loss (Lin et al. 2017b) for coarse lay-\ners classiﬁcations. Lc\nL1 and Lf\nL1 are L1 losses of predicted\nbounding boxes for coarse and ﬁne layers seperately. Lc\ngiou\nand Lf\ngiou are generalized IoU losses for coarse and ﬁne lay-\ners seperately . \u0015cls, \u0015L1 and \u0015giou are trade-off hyperpram-\neters for each loss.\nExperiments\nDataset and Evaluation Metrics\nMS COCO (Lin et al. 2014) instance detection dataset is\nutilized to evaluate detectors. Where all models are trained\non the COCO train2017 set with 118k images and evalu-\nated on the val2017 set with 5k images. The performances\non COCO 2017 test-dev set are also reported. Following the\ncommon practice, AP on the coco val2017 set is used as the\nmain metric. To verify whether CF-DETR mitigates the de-\nfects of the DETR, we also focus on APs with small objects\n189\nMethod Backbone TTA AP AP 50 AP75 APs APm APl\nTSP-R-CNN (Sun et al. 2020) ResNet-101+DCN 47.4 66.7 51.9 29.0 49.7 59.1\nSparse R-CNN (Sun et al. 2021) ResNeXt-101+DCN X 51.5 71.1 57.1 34.2 53.4 64.1\nDeformable DETR (Zhu et al. 2020) ResNeXt-101+DCN X 52.3 71.9 58.1 34.4 54.4 65.6\nCF-DETR ResNet-50 48.1 67.2 52.5 29.5 50.0 61.3\nCF-DETR ResNet-101 49.3 68.5 53.8 29.9 51.6 63.1\nCF-DETR ResNeXt-101 49.8 69.0 54.4 31.0 52.2 63.0\nCF-DETR ResNeXt-101+DCN 50.7 69.9 55.4 30.7 53.2 65.4\nCF-DETR ResNeXt-101+DCN X 53.0 72.6 58.8 35.1 54.9 65.9\nTable 3: Comparison of CF-DETR with state-of-the-art end-to-end detectors on COCO 2017 test-dev set. Note that, Sparse\nR-CNN, Deformable DETR, and CF-DETR, are trained with 300 object queries. ”TTA” indicates test-time augmentations.\nand APm for medium objects. The convergence speed is also\nconcerned.\nImplementation Details\nTransformer Enhanced FPN. We utilize ResNet-50 and\nResNet-101 (He et al. 2016) as backbones, which are pre-\ntrained on ImageNet (Deng et al. 2009). Where feature maps\nfC2;C3;C4;C5gfrom ResNet are feed to TEF module to\nextracted pyramid-like feature maps fE2;E3;E4;E5g. The\nchannel size of feature maps is 256. The dimension of the\nlearnable object query feature is also 256. The Transformer\nencoder (a 6-layer encoder of width 256 with 8 attention\nheads, 2048 FFN) is the same with DETR.\nCF Decoder Layers. The number of CF decoder layers is\nset to 6 by default. The settings of coarse layers are the same\nas the Transformer decoder in DETR. In the ﬁne layer, The\nshape of RoI feature maps is 256 \u00027 \u00027. The spatial size\nkin the ASF module is set to 3. And the dimension scaling\nfactor rand the local attention size k′in the LCA is set to 4\nand 3 respectively. The default number of object queries is\n100.\nTraining Details. The AdamW (Loshchilov and Hutter\n2019) optimizer with weight decay 1e-4 is adopted in the\ntraining process. CF-DETR is trained on 8 NVIDIA Tesla\nV100 GPUs, and the batch size is 16 in total. We follow the\ndefault 3\u0002training schedule of Detectron2 and the initial\nlearning rate is set to 1 \u000210−4. Data augmentations and\ntrade-off hyperparameters in detection loss are the same with\nDETR.\nMain Result\nWe compared CF-DETR with well-established detectors,\nsuch as Faster R-CNN (Ren et al. 2017), Cascade R-CNN\n(Cai and Vasconcelos 2018), as well as the most related end-\nto-end detectors: DETR (Carion et al. 2020), Deformable\nDETR (Zhu et al. 2020), Sparse R-CNN (Sun et al. 2021),\nTSP-R-CNN (Sun et al. 2020).\nTable 2 shows that our proposed CF-DETR outperforms\nall the other competitors. For instance, as an end-to-end\ndetector, CF-DETR*-R50 performs much better than the\ntwo-stage detector Cascade R-CNN-R50 measuring with AP\n(47.8 AP vs. 44.3 AP). Compared with Sparse R-CNN*,\na simple and efﬁcient end-to-end method, our proposed\nmethod exhibits much higher AP scores (47.8 AP vs. 45.0\nCoarse Fine AP AP 50 AP75 APs APm APl\nX 30.0 54.2 29.2 10.6 32.0 49.4\nX 39.9 56.9 42.9 24.0 41.6 54.7\nX X 46.5 65.2 50.5 28.4 49.3 61.8\nTable 4: Ablation studies on the coarse and ﬁne layers in CF\ndecoder layers.\nTEF ASF LCA AP AP 50 AP75 APs APm APl\n42.3 61.7 45.5 26.1 44.8 56.9\nX 43.8 63.5 47.4 26.5 46.6 58.8\nX 44.2 63.1 48.2 27.8 47.2 59.1\nX 43.4 62.6 47.0 26.8 46.3 57.6\nX X 44.8 63.6 48.8 27.3 47.7 59.9\nX X 44.7 64.1 48.7 28.1 47.7 59.5\nX X 44.6 63.4 48.4 28.0 47.2 59.8\nX X X 46.5 65.2 50.5 28.4 49.3 61.8\nTable 5: Ablation studies on the contributions of each mod-\nule (TEF, ASF, and LCA) in the proposed CF-DETR.\nAP with R50; 49.0 AP vs. 46.4 AP with R101). Com-\npared with other SOTA DETR-like detectors: CF-DETR*-\nR50 with 36 epochs performs even better than Deformable\nDETR*++-R50 with 50 epochs (47.8 AP vs. 46.2 AP).\nCF-DETR also shows a signiﬁcant advantage on small ob-\nject detections. For instance, CF-DETR*-R50 improves the\nSOTA APs from 28.8 (Deformable DETR) to 31.2. This il-\nlustrates the beneﬁts of merging global context information\nwith local information in CF-DETR. Note that, the advanced\noperations (e.g. deformable convolutions) in DETR variants\n(Meng et al. 2021; Yao et al. 2021) can also be adopted in\nthis framework by replacing coarse layers to further improve\nthe performance. We leave it in future works.\nTable 3 compares the proposed method with other SOTA\nend-to-end methods on COCO 2017 test-dev set. With\nResNet-101 and ResNeXt-101 (Xie et al. 2017), the pro-\nposed method achieves 49.3 AP and 49.8 AP, respectively.\nBy using ResNeXt-101 with DCN (Zhu et al. 2019), the per-\nformance further improves to 50.7 AP, and 53.0 AP with\ntest-time augmentations.\n190\nL AP AP 50 AP75 APs APm APl FPS\n1 35.8 52.1 38.4 19.0 37.9 47.9 26\n2 43.4 61.3 46.9 25.4 45.9 59.0 24\n4 46.1 64.7 50.1 28.5 48.8 61.2 21\n6 46.5 65.2 50.5 28.4 49.3 61.8 18\n12 45.7 64.6 49.8 28.2 48.3 60.7 13\nTable 6: The AP scores and FPS of CF-DETR (with R50\nbackone and 100 object queries) for different numbers of\nCF decoder layers.\nAblation Studies\nAnalysis of Coarse-to-Fine Structure. In this section, We\nfurther analyze the effects of the key components (coarse\nlayer and ﬁne layer) in the CF decoder layer. To this end, the\nperformances of CF-DETR with different implementations\nof CF decoder layer (with coarse layer only, with ﬁne layer\nonly, and with both of them) are compared (see Table 4).\nSpeciﬁcally, the CF decoder layers implemented with\nonly coarse layer almost degenerate to the Transformer de-\ncoder layer in the original DETR. The only difference lies\nin that the predicted bounding boxes at each layer are inde-\npendent in the original DETR, while we instead utilize an\niterative structure in CF-DETR, where the predictions of the\ncurrent layer are based on the predicted bounding boxes of\nthe previous layer. For the CF decoder layer implemented\nwith the ﬁne layer only, a classiﬁcation head is added, as\nthe vanilla ﬁne layer only predicts bounding boxes in the\ndefault design. For fair comparisons, both of them are com-\npared with default CF decoder layers (with both coarse and\nﬁne layers) under the same feature extractor setting.\nAs shown in Table 4, measured with AP, the ﬁne layer\nperforms better (39.9 AP vs. 30.0 AP), which indicates that\nthe ﬁne layer is more promising at accurate localization.\nCombing both coarse and ﬁne layers, the CF decoder layer\nachieves 16.5 AP and 5.6 AP score improvements com-\npared with only coarse layer and only ﬁne layer respectively\n(46.5 AP vs. 30.0 AP, 46.5 AP vs. 39.9 AP). This validates\nthe effectiveness of the CF decoder layer designed with the\ncoarse-to-ﬁne structure.\nInﬂuences of Different Modules of CF-DETR. In this\npart, we further analyze the contributions of proposed mod-\nules (TEF, ASF, LCA) in CF-DETR (see Table 5). We ﬁrst\nbuild a baseline model following the CF-DETR framework\nby replacing TEF with conventional FPN, replacing ASF\nwith conventional heuristic layer selection method, and re-\nplacing LAC with a simple single FC layer. Then, we add the\nabove modules one by one to see their contributions more\nclearly.\nThe baseline again demonstrates the advantage of the\ncoarse-to-ﬁne structure of CF-DETR, which achieved better\nperformance than DETR-R50 (42.3 AP vs. 42.0 AP). Com-\npared with baseline, TEF (+1.5 AP), ASF(+ 1.9 AP), and\nLCA (+1.1 AP) modules all bring improvements. Among\nthem, the ASF contributes the most. Combining any two\nmodules, lead to further improvements (+2.4 AP on aver-\nage). And the complete CF-DETR (combining all the mod-\n1 36 100-400 500\nepoch\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\nAP\n0.465\n0.478\n0.359\n0.434\n0.379\n0.423\nCF-DETR-R50 (1 layer)\nCF-DETR-R50 (2 layers)\nCF-DETR-R50 (6 layers)\nCF-DETR*-R50 (6 layers)\nDETR-R50 (6 layers)\nFigure 5: The convergence curves of CF-DETR (with differ-\nent training settings) and DETR-R50. The CF-DETR mod-\nels were trained with a standard 3x schedule. DETR-R50\nwas trained with 500 epochs.\nules together) performs the best (+ 4.2 AP). Note that, the\nproposed modules not only improve small object detection\nbut also improve the detection signiﬁcantly for medium ob-\njects and large objects.\nNumbers of CF Decoder Layers. In this section, we\nstudy the effect of the different decoder layers on the per-\nformances. As illustrated in Table 6, with only two decoder\nlayers, CF-DETR can achieve competitive performance as\nDETR-DC-R50 (43.3 AP) and TSP R-CNN-R50 (43.8 AP).\nAs the number of layers increases, the performance of CF-\nDETR generally improves accordingly, but the inference\nspeed becomes slower. However, the performance decreased\nwith 12 decoder layers. we infer that more data and iterations\nmay be required for models with 12 decoder layers.\nAnalysis of Convergence.Another gain that the proposed\nframework brings is the fast convergence. Figure 5 compares\nthe convergence curves of CF-DETR (with different train-\ning settings) with that of DETR. The possible reasons for\nfaster convergence are as follows: (1) Due to the integra-\ntion of global and local information via CF decoder layers,\nthe object query features are reﬁned and enriched. This fa-\ncilitates the sparseness of the attention weight matrix in the\ncross-attention layer. (2) As illustrated in Figure 5, we ﬁnd\nthat the iterative structure also leads to fast convergence, as\nthe proper cascade layers the better convergence speed we\ncan observe.\nConclusion\nThis paper proposes a new end-to-end object detection trans-\nformer framework named CF-DETR. In CF-DETR, a novel\nCF decoder layer is proposed to reﬁne predictions and en-\nrich the features in a coarse-to-ﬁne manner. To fuse local and\nglobal information efﬁciently, an ASF module and an LCA\nmodule are proposed to fully explore and exploit the multi-\nscale ROI information. In addition, the multi-scale features\nare further enhanced by a proposed TEF module. We hope\nthe work of this paper could inspire more insights for im-\nproving DETR-like detectors.\n191\nAcknowledgments\nThis work is partially supported by the National Natural Sci-\nence Foundation of China (Grant No.61971066).\nReferences\nBodla, N.; Singh, B.; Chellappa, R.; and Davis, L. S. 2017.\nSoft-NMS - Improving Object Detection with One Line of\nCode. In IEEE International Conference on Computer Vi-\nsion, ICCV 2017, Venice, Italy, October 22-29, 2017, 5562–\n5570. IEEE Computer Society.\nCai, Z.; and Vasconcelos, N. 2018. Cascade R-CNN: Delv-\ning Into High Quality Object Detection. In 2018 IEEE Con-\nference on Computer Vision and Pattern Recognition, CVPR\n2018, Salt Lake City, UT, USA, June 18-22, 2018, 6154–\n6162. IEEE Computer Society.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-End Object Detection\nwith Transformers. In Vedaldi, A.; Bischof, H.; Brox, T.;\nand Frahm, J., eds., Computer Vision - ECCV 2020 - 16th\nEuropean Conference, Glasgow, UK, August 23-28, 2020,\nProceedings, Part I, volume 12346 ofLecture Notes in Com-\nputer Science, 213–229. Springer.\nDai, Z.; Cai, B.; Lin, Y .; and Chen, J. 2020. UP-DETR:\nUnsupervised Pre-training for Object Detection with Trans-\nformers. CoRR, abs/2011.09094.\nDeng, J.; Dong, W.; Socher, R.; Li, L.; Li, K.; and Li, F.\n2009. ImageNet: A large-scale hierarchical image database.\nIn 2009 IEEE Computer Society Conference on Computer\nVision and Pattern Recognition (CVPR 2009), 20-25 June\n2009, Miami, Florida, USA, 248–255. IEEE Computer So-\nciety.\nGao, P.; Zheng, M.; Wang, X.; Dai, J.; and Li, H. 2021.\nFast Convergence of DETR with Spatially Modulated Co-\nAttention. CoRR, abs/2101.07448.\nGuo, C.; Fan, B.; Zhang, Q.; Xiang, S.; and Pan, C. 2020.\nAugFPN: Improving Multi-Scale Feature Learning for Ob-\nject Detection. In 2020 IEEE/CVF Conference on Computer\nVision and Pattern Recognition, CVPR 2020, Seattle, WA,\nUSA, June 13-19, 2020, 12592–12601. IEEE.\nHe, K.; Gkioxari, G.; Doll ´ar, P.; and Girshick, R. B. 2017.\nMask R-CNN. In IEEE International Conference on Com-\nputer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017,\n2980–2988. IEEE Computer Society.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep Resid-\nual Learning for Image Recognition. In 2016 IEEE Confer-\nence on Computer Vision and Pattern Recognition, CVPR\n2016, Las Vegas, NV , USA, June 27-30, 2016, 770–778.\nIEEE Computer Society.\nLi, Y .; Yao, T.; Pan, Y .; and Mei, T. 2021. Contex-\ntual Transformer Networks for Visual Recognition. CoRR,\nabs/2107.12292.\nLin, T.; Doll ´ar, P.; Girshick, R. B.; He, K.; Hariharan, B.;\nand Belongie, S. J. 2017a. Feature Pyramid Networks for\nObject Detection. In 2017 IEEE Conference on Computer\nVision and Pattern Recognition, CVPR 2017, Honolulu, HI,\nUSA, July 21-26, 2017, 936–944. IEEE Computer Society.\nLin, T.; Goyal, P.; Girshick, R. B.; He, K.; and Doll ´ar, P.\n2017b. Focal Loss for Dense Object Detection. In IEEE\nInternational Conference on Computer Vision, ICCV 2017,\nVenice, Italy, October 22-29, 2017, 2999–3007. IEEE Com-\nputer Society.\nLin, T.; Goyal, P.; Girshick, R. B.; He, K.; and Doll ´ar, P.\n2020. Focal Loss for Dense Object Detection. IEEE Trans.\nPattern Anal. Mach. Intell., 42(2): 318–327.\nLin, T.; Maire, M.; Belongie, S. J.; Hays, J.; Perona, P.; Ra-\nmanan, D.; Doll ´ar, P.; and Zitnick, C. L. 2014. Microsoft\nCOCO: Common Objects in Context. In Fleet, D. J.; Pa-\njdla, T.; Schiele, B.; and Tuytelaars, T., eds., Computer Vi-\nsion - ECCV 2014 - 13th European Conference, Zurich,\nSwitzerland, September 6-12, 2014, Proceedings, Part V,\nvolume 8693 of Lecture Notes in Computer Science, 740–\n755. Springer.\nLiu, S.; Qi, L.; Qin, H.; Shi, J.; and Jia, J. 2018. Path Aggre-\ngation Network for Instance Segmentation. In 2018 IEEE\nConference on Computer Vision and Pattern Recognition,\nCVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018,\n8759–8768. IEEE Computer Society.\nLiu, W.; Anguelov, D.; Erhan, D.; Szegedy, C.; Reed, S. E.;\nFu, C.; and Berg, A. C. 2016. SSD: Single Shot MultiBox\nDetector. In Leibe, B.; Matas, J.; Sebe, N.; and Welling,\nM., eds., Computer Vision - ECCV 2016 - 14th European\nConference, Amsterdam, The Netherlands, October 11-14,\n2016, Proceedings, Part I, volume 9905 ofLecture Notes in\nComputer Science, 21–37. Springer.\nLoshchilov, I.; and Hutter, F. 2019. Decoupled Weight De-\ncay Regularization. In 7th International Conference on\nLearning Representations, ICLR 2019, New Orleans, LA,\nUSA, May 6-9, 2019. OpenReview.net.\nMeng, D.; Chen, X.; Fan, Z.; Zeng, G.; Li, H.; Yuan, Y .; Sun,\nL.; and Wang, J. 2021. Conditional DETR for Fast Training\nConvergence. CoRR, abs/2108.06152.\nRedmon, J.; Divvala, S. K.; Girshick, R. B.; and Farhadi,\nA. 2016. You Only Look Once: Uniﬁed, Real-Time Object\nDetection. In 2016 IEEE Conference on Computer Vision\nand Pattern Recognition, CVPR 2016, Las Vegas, NV , USA,\nJune 27-30, 2016, 779–788. IEEE Computer Society.\nRen, S.; He, K.; Girshick, R. B.; and Sun, J. 2017. Faster\nR-CNN: Towards Real-Time Object Detection with Region\nProposal Networks. IEEE Trans. Pattern Anal. Mach. Intell.,\n39(6): 1137–1149.\nSun, P.; Zhang, R.; Jiang, Y .; Kong, T.; Xu, C.; Zhan, W.;\nTomizuka, M.; Li, L.; Yuan, Z.; Wang, C.; and Luo, P. 2021.\nSparse R-CNN: End-to-End Object Detection With Learn-\nable Proposals. 14454–14463.\nSun, Z.; Cao, S.; Yang, Y .; and Kitani, K. 2020. Rethink-\ning Transformer-based Set Prediction for Object Detection.\nCoRR, abs/2011.10881.\nTian, Z.; Shen, C.; Chen, H.; and He, T. 2019. FCOS:\nFully Convolutional One-Stage Object Detection. In 2019\nIEEE/CVF International Conference on Computer Vision,\nICCV 2019, Seoul, Korea (South), October 27 - November\n2, 2019, 9626–9635. IEEE.\n192\nWang, H.; Wu, X.; Huang, Z.; and Xing, E. P. 2020. High-\nFrequency Component Helps Explain the Generalization of\nConvolutional Neural Networks. In 2020 IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, CVPR\n2020, Seattle, WA, USA, June 13-19, 2020, 8681–8691.\nIEEE.\nXie, S.; Girshick, R. B.; Doll´ar, P.; Tu, Z.; and He, K. 2017.\nAggregated Residual Transformations for Deep Neural Net-\nworks. In 2017 IEEE Conference on Computer Vision and\nPattern Recognition, CVPR 2017, Honolulu, HI, USA, July\n21-26, 2017, 5987–5995. IEEE Computer Society.\nYao, Z.; Ai, J.; Li, B.; and Zhang, C. 2021. Efﬁcient DETR:\nImproving End-to-End Object Detector with Dense Prior.\nCoRR, abs/2104.01318.\nZaidi, S. S. A.; Ansari, M. S.; Aslam, A.; Kanwal, N.;\nAsghar, M. N.; and Lee, B. 2021. A Survey of Modern\nDeep Learning based Object Detection Models. CoRR,\nabs/2104.11892.\nZhang, S.; Chi, C.; Yao, Y .; Lei, Z.; and Li, S. Z. 2020.\nBridging the Gap Between Anchor-Based and Anchor-Free\nDetection via Adaptive Training Sample Selection. In 2020\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, CVPR 2020, Seattle, WA, USA, June 13-19,\n2020, 9756–9765. IEEE.\nZhu, X.; Hu, H.; Lin, S.; and Dai, J. 2019. Deformable Con-\nvNets V2: More Deformable, Better Results. In IEEE Con-\nference on Computer Vision and Pattern Recognition, CVPR\n2019, Long Beach, CA, USA, June 16-20, 2019, 9308–9316.\nComputer Vision Foundation / IEEE.\nZhu, X.; Su, W.; Lu, L.; Li, B.; Wang, X.; and Dai, J. 2020.\nDeformable DETR: Deformable Transformers for End-to-\nEnd Object Detection. CoRR, abs/2010.04159.\nZou, Z.; Shi, Z.; Guo, Y .; and Ye, J. 2019. Object Detection\nin 20 Years: A Survey. CoRR, abs/1905.05055.\n193",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6493120193481445
    },
    {
      "name": "Data mining",
      "score": 0.4420834183692932
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I139759216",
      "name": "Beijing University of Posts and Telecommunications",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210159102",
      "name": "Huawei Technologies (Sweden)",
      "country": "SE"
    }
  ],
  "cited_by": 43
}