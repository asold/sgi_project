{
    "title": "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting",
    "url": "https://openalex.org/W3111507638",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4281956472",
            "name": "Zhou, Haoyi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3130598054",
            "name": "Zhang, Shanghang",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Peng, Jieqi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1924112616",
            "name": "Zhang Shuai",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2020626240",
            "name": "Li Jianxin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2099001205",
            "name": "Xiong Hui",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2361058816",
            "name": "Zhang Wancai",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2786228682",
        "https://openalex.org/W3104786355",
        "https://openalex.org/W2952042565",
        "https://openalex.org/W2994673210",
        "https://openalex.org/W2095796263",
        "https://openalex.org/W2604847698",
        "https://openalex.org/W2986922898",
        "https://openalex.org/W2963532813",
        "https://openalex.org/W2762309767",
        "https://openalex.org/W2114001875",
        "https://openalex.org/W2080860013",
        "https://openalex.org/W2937537592",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2986830070",
        "https://openalex.org/W2054685200",
        "https://openalex.org/W2549483845",
        "https://openalex.org/W2963285578",
        "https://openalex.org/W2940744433",
        "https://openalex.org/W2793273050",
        "https://openalex.org/W2963983719",
        "https://openalex.org/W3033529678",
        "https://openalex.org/W2983902802",
        "https://openalex.org/W3015468748",
        "https://openalex.org/W2757354914",
        "https://openalex.org/W3122002512",
        "https://openalex.org/W2033697613",
        "https://openalex.org/W2798058877",
        "https://openalex.org/W2911109671",
        "https://openalex.org/W2984315581",
        "https://openalex.org/W2969855422",
        "https://openalex.org/W2131774270",
        "https://openalex.org/W2963358464",
        "https://openalex.org/W2765932895",
        "https://openalex.org/W2747599906",
        "https://openalex.org/W2773625660",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2954731415",
        "https://openalex.org/W2613328025",
        "https://openalex.org/W2946775356",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2962850830",
        "https://openalex.org/W2891511127",
        "https://openalex.org/W1985164990",
        "https://openalex.org/W2791881472",
        "https://openalex.org/W2947675203",
        "https://openalex.org/W2964199361",
        "https://openalex.org/W2946683488",
        "https://openalex.org/W2607045400",
        "https://openalex.org/W2011599442",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2319170717",
        "https://openalex.org/W2949335953",
        "https://openalex.org/W1969852690",
        "https://openalex.org/W1924770834",
        "https://openalex.org/W1826290430"
    ],
    "abstract": "Many real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to increase the prediction capacity. However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, including quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. To address these issues, we design an efficient transformer-based model for LSTF, named Informer, with three distinctive characteristics: (i) a $ProbSparse$ self-attention mechanism, which achieves $O(L \\log L)$ in time complexity and memory usage, and has comparable performance on sequences' dependency alignment. (ii) the self-attention distilling highlights dominating attention by halving cascading layer input, and efficiently handles extreme long input sequences. (iii) the generative style decoder, while conceptually simple, predicts the long time-series sequences at one forward operation rather than a step-by-step way, which drastically improves the inference speed of long-sequence predictions. Extensive experiments on four large-scale datasets demonstrate that Informer significantly outperforms existing methods and provides a new solution to the LSTF problem.",
    "full_text": "Informer: Beyond Efﬁcient Transformer for Long Sequence\nTime-Series Forecasting\nHaoyi Zhou, 1 Shanghang Zhang, 2 Jieqi Peng, 1 Shuai Zhang, 1 Jianxin Li, 1\nHui Xiong, 3 Wancai Zhang 4\n1 Beihang University 2 UC Berkeley 3 Rutgers University 4 SEDD Company\n{zhouhy, pengjq, zhangs, lijx}@act.buaa.edu.cn, shz@eecs.berkeley.edu, {xionghui,zhangwancaibuaa}@gmail.com\nAbstract\nMany real-world applications require the prediction of long\nsequence time-series, such as electricity consumption plan-\nning. Long sequence time-series forecasting (LSTF) demands\na high prediction capacity of the model, which is the ability\nto capture precise long-range dependency coupling between\noutput and input efﬁciently. Recent studies have shown the\npotential of Transformer to increase the prediction capacity.\nHowever, there are several severe issues with Transformer\nthat prevent it from being directly applicable to LSTF, includ-\ning quadratic time complexity, high memory usage, and in-\nherent limitation of the encoder-decoder architecture. To ad-\ndress these issues, we design an efﬁcient transformer-based\nmodel for LSTF, named Informer, with three distinctive char-\nacteristics: (i) a ProbSparse self-attention mechanism, which\nachieves O(Llog L) in time complexity and memory usage,\nand has comparable performance on sequences’ dependency\nalignment. (ii) the self-attention distilling highlights dominat-\ning attention by halving cascading layer input, and efﬁciently\nhandles extreme long input sequences. (iii) the generative\nstyle decoder, while conceptually simple, predicts the long\ntime-series sequences at one forward operation rather than\na step-by-step way, which drastically improves the inference\nspeed of long-sequence predictions. Extensive experiments\non four large-scale datasets demonstrate that Informer sig-\nniﬁcantly outperforms existing methods and provides a new\nsolution to the LSTF problem.\n1 Introduction\nTime-series forecasting is a critical ingredient across many\ndomains, such as sensor network monitoring (Papadimitriou\nand Yu 2006), energy and smart grid management, eco-\nnomics and ﬁnance (Zhu and Shasha 2002), and disease\npropagation analysis (Matsubara et al. 2014). In these sce-\nnarios, we can leverage a substantial amount of time-series\ndata on past behavior to make a forecast in the long run,\nnamely long sequence time-series forecasting (LSTF). How-\never, existing methods are mostly designed under short-term\nproblem setting, like predicting 48 points or less (Hochreiter\nand Schmidhuber 1997; Li et al. 2018; Yu et al. 2017; Liu\net al. 2019; Qin et al. 2017; Wen et al. 2017). The increas-\ningly long sequences strain the models’ prediction capacity\nto the point where this trend is holding the research on LSTF.\nAs an empirical example, Fig.(1) shows the forecasting re-\nsults on a real dataset, where the LSTM network predicts the\n2d 4d 6d 8d Time0d\nGround truth\nPrediction\nShort\nLong\n(a) Sequence Forecasting.\n1248 96 192 480\nThe predict sequence length\n2\n4\n6MSE score\n10−1\n100\nThe predictions/sec\n(in log scale)\nMSE score\nInference speed (b) Run LSTM on sequences.\nFigure 1: (a) LSTF can cover an extended period than\nthe short sequence predictions, making vital distinction in\npolicy-planning and investment-protecting. (b) The predic-\ntion capacity of existing methods limits LSTF’s perfor-\nmance. E.g., starting from length=48, MSE rises unaccept-\nably high, and the inference speed drops rapidly.\nhourly temperature of an electrical transformer station from\nthe short-term period (12 points, 0.5 days) to the long-term\nperiod (480 points, 20 days). The overall performance gap\nis substantial when the prediction length is greater than 48\npoints (the solid star in Fig.(1b)), where the MSE rises to\nunsatisfactory performance, the inference speed gets sharp\ndrop, and the LSTM model starts to fail.\nThe major challenge for LSTF is to enhance the predic-\ntion capacity to meet the increasingly long sequence de-\nmand, which requires (a) extraordinary long-range align-\nment ability and (b) efﬁcient operations on long sequence in-\nputs and outputs. Recently, Transformer models have shown\nsuperior performance in capturing long-range dependency\nthan RNN models. The self-attention mechanism can re-\nduce the maximum length of network signals traveling paths\ninto the theoretical shortest O(1) and avoid the recurrent\nstructure, whereby Transformer shows great potential for\nthe LSTF problem. Nevertheless, the self-attention mecha-\nnism violates requirement (b) due to its L-quadratic compu-\ntation and memory consumption onL-length inputs/outputs.\nSome large-scale Transformer models pour resources and\nyield impressive results on NLP tasks (Brown et al. 2020),\nbut the training on dozens of GPUs and expensive deploying\ncost make theses models unaffordable on real-world LSTF\nproblem. The efﬁciency of the self-attention mechanism and\nTransformer architecture becomes the bottleneck of apply-\ning them to LSTF problems. Thus, in this paper, we seek to\nanswer the question: can we improve Transformer models to\narXiv:2012.07436v3  [cs.LG]  28 Mar 2021\nbe computation, memory, and architecture efﬁcient, as well\nas maintaining higher prediction capacity?\nVanilla Transformer (Vaswani et al. 2017) has three sig-\nniﬁcant limitations when solving the LSTF problem:\n1. The quadratic computation of self-attention.The atom\noperation of self-attention mechanism, namely canonical\ndot-product, causes the time complexity and memory us-\nage per layer to be O(L2).\n2. The memory bottleneck in stacking layers for long in-\nputs. The stack of J encoder/decoder layers makes total\nmemory usage to be O(J ·L2), which limits the model\nscalability in receiving long sequence inputs.\n3. The speed plunge in predicting long outputs.Dynamic\ndecoding of vanilla Transformer makes the step-by-step\ninference as slow as RNN-based model (Fig.(1b)).\nThere are some prior works on improving the efﬁciency of\nself-attention. The Sparse Transformer (Child et al. 2019),\nLogSparse Transformer (Li et al. 2019), and Longformer\n(Beltagy, Peters, and Cohan 2020) all use a heuristic method\nto tackle limitation 1 and reduce the complexity of self-\nattention mechanism to O(Llog L), where their efﬁciency\ngain is limited (Qiu et al. 2019). Reformer (Kitaev, Kaiser,\nand Levskaya 2019) also achieves O(Llog L) with locally-\nsensitive hashing self-attention, but it only works on ex-\ntremely long sequences. More recently, Linformer (Wang\net al. 2020) claims a linear complexityO(L), but the project\nmatrix can not be ﬁxed for real-world long sequence in-\nput, which may have the risk of degradation to O(L2).\nTransformer-XL (Dai et al. 2019) and Compressive Trans-\nformer (Rae et al. 2019) use auxiliary hidden states to cap-\nture long-range dependency, which could amplify limitation\n1 and be adverse to break the efﬁciency bottleneck. All these\nworks mainly focus on limitation 1, and the limitation 2&3\nremains unsolved in the LSTF problem. To enhance the pre-\ndiction capacity, we tackle all these limitations and achieve\nimprovement beyond efﬁciency in the proposed Informer.\nTo this end, our work delves explicitly into these three is-\nsues. We investigate the sparsity in the self-attention mecha-\nnism, make improvements of network components, and con-\nduct extensive experiments. The contributions of this paper\nare summarized as follows:\n• We propose Informer to successfully enhance the predic-\ntion capacity in the LSTF problem, which validates the\nTransformer-like model’s potential value to capture in-\ndividual long-range dependency between long sequence\ntime-series outputs and inputs.\n• We propose ProbSparse self-attention mechanism to ef-\nﬁciently replace the canonical self-attention. It achieves\nthe O(Llog L) time complexity andO(Llog L) memory\nusage on dependency alignments.\n• We propose self-attention distilling operation to privi-\nlege dominating attention scores inJ-stacking layers and\nsharply reduce the total space complexity to be O((2 −\nϵ)Llog L), which helps receiving long sequence input.\n• We propose generative style decoder to acquire long se-\nquence output with only one forward step needed, simul-\ntaneously avoiding cumulative error spreading during the\ninference phase.\nDecoder\nOutputs\nMasked Multi-head\nProbSparse\nSelf-attention\nMulti-head\nAttention\nEncoder\nInputs:    Xen\nConcatenated Feature Map\nInputs:    Xde={Xtoken, X0}\n0 0 0 0 0 0 0\nFully Connected Layer\nMulti-head\nProbSparse\nSelf-attention\nMulti-head\nProbSparse\nSelf-attention\nFigure 2: Informer model overview. Left: The encoder re-\nceives massive long sequence inputs (green series). We re-\nplace canonical self-attention with the proposed ProbSparse\nself-attention. The blue trapezoid is the self-attention distill-\ning operation to extract dominating attention, reducing the\nnetwork size sharply. The layer stacking replicas increase ro-\nbustness. Right: The decoder receives long sequence inputs,\npads the target elements into zero, measures the weighted\nattention composition of the feature map, and instantly pre-\ndicts output elements (orange series) in a generative style.\n2 Preliminary\nWe ﬁrst provide the LSTF problem deﬁnition. Under the\nrolling forecasting setting with a ﬁxed size window, we have\nthe input Xt = {xt\n1,..., xt\nLx |xt\ni ∈ Rdx}at time t,\nand the output is to predict corresponding sequence Yt =\n{yt\n1,..., yt\nLy |yt\ni ∈Rdy}. The LSTF problem encourages\na longer output’s length Ly than previous works (Cho et al.\n2014; Sutskever, Vinyals, and Le 2014) and the feature di-\nmension is not limited to univariate case (dy ≥1).\nEncoder-decoder architectureMany popular models are\ndevised to “encode” the input representationsXt into a hid-\nden state representations Ht and “decode” an output rep-\nresentations Yt from Ht = {ht\n1,..., ht\nLh}. The inference\ninvolves a step-by-step process named “dynamic decoding”,\nwhere the decoder computes a new hidden state ht\nk+1 from\nthe previous state ht\nk and other necessary outputs from k-th\nstep then predict the (k+ 1)-th sequence yt\nk+1.\nInput Representation A uniform input representation is\ngiven to enhance the global positional context and local tem-\nporal context of the time-series inputs. To avoid trivializing\ndescription, we put the details in Appendix B.\n3 Methodology\nExisting methods for time-series forecasting can be roughly\ngrouped into two categories 1. Classical time-series mod-\nels serve as a reliable workhorse for time-series forecast-\ning (Box et al. 2015; Ray 1990; Seeger et al. 2017; Seeger,\nSalinas, and Flunkert 2016), and deep learning techniques\nmainly develop an encoder-decoder prediction paradigm by\nusing RNN and their variants (Hochreiter and Schmidhuber\n1997; Li et al. 2018; Yu et al. 2017). Our proposed Informer\nholds the encoder-decoder architecture while targeting the\n1Related work is in Appendix A due to space limitation.\nLSTF problem. Please refer to Fig.(2) for an overview and\nthe following sections for details.\nEfﬁcient Self-attention Mechanism\nThe canonical self-attention in (Vaswani et al. 2017) is de-\nﬁned based on the tuple inputs, i.e, query, key and value,\nwhich performs the scaled dot-product as A(Q,K,V) =\nSoftmax(QK⊤/\n√\nd)V, where Q ∈RLQ×d, K ∈RLK×d,\nV ∈RLV×dand dis the input dimension. To further discuss\nthe self-attention mechanism, let qi, ki, vi stand for the i-th\nrow in Q, K, V respectively. Following the formulation in\n(Tsai et al. 2019), the i-th query’s attention is deﬁned as a\nkernel smoother in a probability form:\nA(qi,K,V) =\n∑\nj\nk(qi,kj)∑\nlk(qi,kl)vj = Ep(kj|qi)[vj] , (1)\nwhere p(kj|qi) = k(qi,kj)/∑\nlk(qi,kl) and k(qi,kj)\nselects the asymmetric exponential kernel exp(qik⊤\nj /\n√\nd).\nThe self-attention combines the values and acquires outputs\nbased on computing the probability p(kj|qi). It requires\nthe quadratic times dot-product computation andO(LQLK)\nmemory usage, which is the major drawback when enhanc-\ning prediction capacity.\nSome previous attempts have revealed that the distribution\nof self-attention probability has potential sparsity, and they\nhave designed “selective” counting strategies on allp(kj|qi)\nwithout signiﬁcantly affecting the performance. The Sparse\nTransformer (Child et al. 2019) incorporates both the row\noutputs and column inputs, in which the sparsity arises\nfrom the separated spatial correlation. The LogSparse Trans-\nformer (Li et al. 2019) notices the cyclical pattern in self-\nattention and forces each cell to attend to its previous one\nby an exponential step size. The Longformer (Beltagy, Pe-\nters, and Cohan 2020) extends previous two works to more\ncomplicated sparse conﬁguration. However, they are limited\nto theoretical analysis from following heuristic methods and\ntackle each multi-head self-attention with the same strategy,\nwhich narrows their further improvement.\nTo motivate our approach, we ﬁrst perform a qualitative\nassessment on the learned attention patterns of the canoni-\ncal self-attention. The “sparsity” self-attention score forms\na long tail distribution (see Appendix C for details), i.e., a\nfew dot-product pairs contribute to the major attention, and\nothers generate trivial attention. Then, the next question is\nhow to distinguish them?\nQuery Sparsity Measurement From Eq.(1), the i-th\nquery’s attention on all the keys are deﬁned as a probabil-\nity p(kj|qi) and the output is its composition with values v.\nThe dominant dot-product pairs encourage the correspond-\ning query’s attention probability distribution away from the\nuniform distribution. If p(kj|qi) is close to a uniform dis-\ntribution q(kj|qi) = 1 /LK, the self-attention becomes a\ntrivial sum of values V and is redundant to the residential\ninput. Naturally, the “likeness” between distribution p and\nq can be used to distinguish the “important” queries. We\nmeasure the “likeness” through Kullback-Leibler divergence\nKL(q||p) = ln ∑LK\nl=1 eqik⊤\nl /\n√\nd − 1\nLK\n∑LK\nj=1 qik⊤\nj /\n√\nd−\nln LK. Dropping the constant, we deﬁne the i-th query’s\nsparsity measurement as\nM(qi,K) = ln\nLK∑\nj=1\ne\nqik⊤\nj√\nd − 1\nLK\nLK∑\nj=1\nqik⊤\nj√\nd\n, (2)\nwhere the ﬁrst term is the Log-Sum-Exp (LSE) of qi on\nall the keys, and the second term is the arithmetic mean on\nthem. If the i-th query gains a larger M(qi,K), its atten-\ntion probability pis more “diverse” and has a high chance to\ncontain the dominate dot-product pairs in the header ﬁeld of\nthe long tail self-attention distribution.\nProbSparse Self-attention Based on the proposed mea-\nsurement, we have theProbSparse self-attention by allowing\neach key to only attend to the udominant queries:\nA(Q,K,V) = Softmax(QK⊤\n√\nd\n)V , (3)\nwhere Q is a sparse matrix of the same size of q and it\nonly contains the Top-uqueries under the sparsity measure-\nment M(q,K). Controlled by a constant sampling factor c,\nwe set u = c·ln LQ, which makes the ProbSparse self-\nattention only need to calculate O(ln LQ) dot-product for\neach query-key lookup and the layer memory usage main-\ntains O(LK ln LQ). Under the multi-head perspective, this\nattention generates different sparse query-key pairs for each\nhead, which avoids severe information loss in return.\nHowever, the traversing of all the queries for the measure-\nment M(qi,K) requires calculating each dot-product pairs,\ni.e., quadratically O(LQLK), besides the LSE operation has\nthe potential numerical stability issue. Motivated by this, we\npropose an empirical approximation for the efﬁcient acqui-\nsition of the query sparsity measurement.\nLemma 1. For each query qi ∈Rd and kj ∈Rd in the\nkeys set K, we have the bound as ln LK ≤M(qi,K) ≤\nmaxj{qik⊤\nj /\n√\nd}− 1\nLK\n∑LK\nj=1{qik⊤\nj /\n√\nd}+ lnLK. When\nqi ∈K, it also holds.\nFrom the Lemma 1 (proof is given in Appendix D.1), we\npropose the max-mean measurement as\nM(qi,K) = max\nj\n{qik⊤\nj√\nd\n}− 1\nLK\nLK∑\nj=1\nqik⊤\nj√\nd\n. (4)\nThe range of Top- u approximately holds in the bound-\nary relaxation with Proposition 1 (refers in Appendix D.2).\nUnder the long tail distribution, we only need to randomly\nsample U = LK ln LQ dot-product pairs to calculate the\nM(qi,K), i.e., ﬁlling other pairs with zero. Then, we se-\nlect sparse Top- u from them as Q. The max-operator in\nM(qi,K) is less sensitive to zero values and is numeri-\ncal stable. In practice, the input length of queries and keys\nare typically equivalent in the self-attention computation, i.e\nLQ = LK = Lsuch that the total ProbSparse self-attention\ntime complexity and space complexity are O(Lln L).\nScalar\nStamp\nT = t\nT = t + Dx\nL\nd\nConv1d\nL\nd\nEmbedding\n+\nL\nk\nL\nn-heads\nAttention Block 1\nConv1d\nMaxPool1d,padding=2\nL/2\nk\nL/2\nn-heads\nAttention Block 2\nConv1d\nMaxPool1d,padding=2\nL/4\nk L/4\nn-heads\nAttention Block 3\nL/4\nd\nFeature\nMap\nFigure 3: The single stack in Informer’s encoder. (1) The horizontal stack stands for an individual one of the encoder replicas\nin Fig.(2). (2) The presented one is the main stack receiving the whole input sequence. Then the second stack takes half slices\nof the input, and the subsequent stacks repeat. (3) The red layers are dot-product matrixes, and they get cascade decrease by\napplying self-attention distilling on each layer. (4) Concatenate all stacks’ feature maps as the encoder’s output.\nEncoder: Allowing for Processing Longer\nSequential Inputs under the Memory Usage\nLimitation\nThe encoder is designed to extract the robust long-range de-\npendency of the long sequential inputs. After the input rep-\nresentation, the t-th sequence input Xt has been shaped into\na matrix Xt\nen ∈RLx×dmodel . We give a sketch of the encoder\nin Fig.(3) for clarity.\nSelf-attention Distilling As the natural consequence of\nthe ProbSparse self-attention mechanism, the encoder’s fea-\nture map has redundant combinations of value V. We use\nthe distilling operation to privilege the superior ones with\ndominating features and make a focused self-attention fea-\nture map in the next layer. It trims the input’s time dimension\nsharply, seeing the n-heads weights matrix (overlapping red\nsquares) of Attention blocks in Fig.(3). Inspired by the di-\nlated convolution (Yu, Koltun, and Funkhouser 2017; Gupta\nand Rush 2017), our “distilling” procedure forwards from\nj-th layer into (j+ 1)-th layer as:\nXt\nj+1 = MaxPool\n(\nELU( Conv1d([Xt\nj]AB) )\n)\n, (5)\nwhere [·]AB represents the attention block. It contains the\nMulti-head ProbSparse self-attention and the essential op-\nerations, where Conv1d (·) performs an 1-D convolutional\nﬁlters (kernel width=3) on time dimension with the ELU (·)\nactivation function (Clevert, Unterthiner, and Hochreiter\n2016). We add a max-pooling layer with stride 2 and down-\nsample Xt into its half slice after stacking a layer, which\nreduces the whole memory usage to be O((2 −ϵ)Llog L),\nwhere ϵ is a small number. To enhance the robustness of\nthe distilling operation, we build replicas of the main stack\nwith halving inputs, and progressively decrease the number\nof self-attention distilling layers by dropping one layer at a\ntime, like a pyramid in Fig.(2), such that their output dimen-\nsion is aligned. Thus, we concatenate all the stacks’ outputs\nand have the ﬁnal hidden representation of encoder.\nDecoder: Generating Long Sequential Outputs\nThrough One Forward Procedure\nWe use a standard decoder structure (Vaswani et al. 2017) in\nFig.(2), and it is composed of a stack of two identical multi-\nhead attention layers. However, the generative inference is\nemployed to alleviate the speed plunge in long prediction.\nWe feed the decoder with the following vectors as\nXt\nde = Concat(Xt\ntoken,Xt\n0) ∈R(Ltoken+Ly)×dmodel , (6)\nwhere Xt\ntoken ∈ RLtoken×dmodel is the start token, Xt\n0 ∈\nRLy×dmodel is a placeholder for the target sequence (set\nscalar as 0). Masked multi-head attention is applied in the\nProbSparse self-attention computing by setting masked dot-\nproducts to −∞. It prevents each position from attending\nto coming positions, which avoids auto-regressive. A fully\nconnected layer acquires the ﬁnal output, and its outsize dy\ndepends on whether we are performing a univariate forecast-\ning or a multivariate one.\nGenerative Inference Start token is efﬁciently applied in\nNLP’s “dynamic decoding” (Devlin et al. 2018), and we ex-\ntend it into a generative way. Instead of choosing speciﬁc\nﬂags as the token, we sample a Ltoken long sequence in the\ninput sequence, such as an earlier slice before the output se-\nquence. Take predicting 168 points as an example (7-day\ntemperature prediction in the experiment section), we will\ntake the known 5 days before the target sequence as “start-\ntoken”, and feed the generative-style inference decoder with\nXde = {X5d,X0}. The X0 contains target sequence’s time\nstamp, i.e., the context at the target week. Then our proposed\ndecoder predicts outputs by one forward procedure rather\nthan the time consuming “dynamic decoding” in the conven-\ntional encoder-decoder architecture. A detailed performance\ncomparison is given in the computation efﬁciency section.\nLoss function We choose the MSE loss function on pre-\ndiction w.r.t the target sequences, and the loss is propagated\nback from the decoder’s outputs across the entire model.\nMethods Informer Informer† LogTrans Reformer LSTMa DeepAR ARIMA Prophet\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\nETTh1\n24 0.098 0.247 0.092 0.246 0.103 0.259 0.222 0.389 0.114 0.272 0.107 0.280 0.108 0.284 0.115 0.275\n48 0.158 0.319 0.161 0.322 0.167 0.328 0.284 0.445 0.193 0.358 0.162 0.327 0.175 0.424 0.168 0.330\n168 0.183 0.346 0.187 0.355 0.207 0.375 1.522 1.191 0.236 0.392 0.239 0.422 0.396 0.504 1.224 0.763\n336 0.222 0.387 0.215 0.369 0.230 0.398 1.860 1.124 0.590 0.698 0.445 0.552 0.468 0.593 1.549 1.820\n720 0.269 0.435 0.257 0.421 0.273 0.463 2.112 1.436 0.683 0.768 0.658 0.707 0.659 0.766 2.735 3.253\nETTh2\n24 0.093 0.240 0.099 0.241 0.102 0.255 0.263 0.437 0.155 0.307 0.098 0.263 3.554 0.445 0.199 0.381\n48 0.155 0.314 0.159 0.317 0.169 0.348 0.458 0.545 0.190 0.348 0.163 0.341 3.190 0.474 0.304 0.462\n168 0.232 0.389 0.235 0.390 0.246 0.422 1.029 0.879 0.385 0.514 0.255 0.414 2.800 0.595 2.145 1.068\n336 0.263 0.417 0.258 0.423 0.267 0.437 1.668 1.228 0.558 0.606 0.604 0.607 2.753 0.738 2.096 2.543\n720 0.277 0.431 0.285 0.442 0.303 0.493 2.030 1.721 0.640 0.681 0.429 0.580 2.878 1.044 3.355 4.664\nETTm1\n24 0.030 0.137 0.034 0.160 0.065 0.202 0.095 0.228 0.121 0.233 0.091 0.243 0.090 0.206 0.120 0.290\n48 0.069 0.203 0.066 0.194 0.078 0.220 0.249 0.390 0.305 0.411 0.219 0.362 0.179 0.306 0.133 0.305\n96 0.194 0.372 0.187 0.384 0.199 0.386 0.920 0.767 0.287 0.420 0.364 0.496 0.272 0.399 0.194 0.396\n288 0.401 0.554 0.409 0.548 0.411 0.572 1.108 1.245 0.524 0.584 0.948 0.795 0.462 0.558 0.452 0.574\n672 0.512 0.644 0.519 0.665 0.598 0.702 1.793 1.528 1.064 0.873 2.437 1.352 0.639 0.697 2.747 1.174\nWeather\n24 0.117 0.251 0.119 0.256 0.136 0.279 0.231 0.401 0.131 0.254 0.128 0.274 0.219 0.355 0.302 0.433\n48 0.178 0.318 0.185 0.316 0.206 0.356 0.328 0.423 0.190 0.334 0.203 0.353 0.273 0.409 0.445 0.536\n168 0.266 0.398 0.269 0.404 0.309 0.439 0.654 0.634 0.341 0.448 0.293 0.451 0.503 0.599 2.441 1.142\n336 0.297 0.416 0.310 0.422 0.359 0.484 1.792 1.093 0.456 0.554 0.585 0.644 0.728 0.730 1.987 2.468\n720 0.359 0.466 0.361 0.471 0.388 0.499 2.087 1.534 0.866 0.809 0.499 0.596 1.062 0.943 3.859 1.144\nECL\n48 0.239 0.359 0.238 0.368 0.280 0.429 0.971 0.884 0.493 0.539 0.204 0.357 0.879 0.764 0.524 0.595\n168 0.447 0.503 0.442 0.514 0.454 0.529 1.671 1.587 0.723 0.655 0.315 0.436 1.032 0.833 2.725 1.273\n336 0.489 0.528 0.501 0.552 0.514 0.563 3.528 2.196 1.212 0.898 0.414 0.519 1.136 0.876 2.246 3.077\n720 0.540 0.571 0.543 0.578 0.558 0.609 4.891 4.047 1.511 0.966 0.563 0.595 1.251 0.933 4.243 1.415\n960 0.582 0.608 0.594 0.638 0.624 0.645 7.019 5.105 1.545 1.006 0.657 0.683 1.370 0.982 6.901 4.264\nCount 32 12 0 0 0 6 0 0\nTable 1: Univariate long sequence time-series forecasting results on four datasets (ﬁve cases).\n4 Experiment\nDatasets\nWe extensively perform experiments on four datasets, in-\ncluding 2 collected real-world datasets for LSTF and 2 pub-\nlic benchmark datasets.\nETT (Electricity Transformer Temperature)2: The ETT is\na crucial indicator in the electric power long-term deploy-\nment. We collected 2-year data from two separated counties\nin China. To explore the granularity on the LSTF problem,\nwe create separate datasets as {ETTh1, ETTh2}for 1-hour-\nlevel and ETTm1 for 15-minute-level. Each data point con-\nsists of the target value ”oil temperature” and 6 power load\nfeatures. The train/val/test is 12/4/4 months.\nECL (Electricity Consuming Load)3: It collects the elec-\ntricity consumption (Kwh) of 321 clients. Due to the missing\ndata (Li et al. 2019), we convert the dataset into hourly con-\nsumption of 2 years and set ‘MT 320’ as the target value.\nThe train/val/test is 15/3/4 months.\nWeather 4: This dataset contains local climatological data\nfor nearly 1,600 U.S. locations, 4 years from 2010 to 2013,\nwhere data points are collected every 1 hour. Each data point\n2We collected the ETT dataset and published it at https://\ngithub.com/zhouhaoyi/ETDataset.\n3ECL dataset was acquired at https://archive.ics.uci.edu/ml/\ndatasets/ElectricityLoadDiagrams20112014.\n4Weather dataset was acquired at https://www.ncei.noaa.gov/\ndata/local-climatological-data/.\nconsists of the target value “wet bulb” and 11 climate fea-\ntures. The train/val/test is 28/10/10 months.\nExperimental Details\nWe brieﬂy summarize basics, and more information on net-\nwork components and setups are given in Appendix E.\nBaselines: We have selected ﬁve time-series forecast-\ning methods as comparison, including ARIMA (Ariyo,\nAdewumi, and Ayo 2014), Prophet (Taylor and Letham\n2018), LSTMa (Bahdanau, Cho, and Bengio 2015), LST-\nnet (Lai et al. 2018) and DeepAR (Flunkert, Salinas, and\nGasthaus 2017). To better explore the ProbSparse self-\nattention’s performance in our proposed Informer, we in-\ncorporate the canonical self-attention variant (Informer †),\nthe efﬁcient variant Reformer (Kitaev, Kaiser, and Levskaya\n2019) and the most related work LogSparse self-attention\n(Li et al. 2019) in the experiments. The details of network\ncomponents are given in Appendix E.1.\nHyper-parameter tuning: We conduct grid search over\nthe hyper-parameters, and detailed ranges are given in Ap-\npendix E.3. Informer contains a 3-layer stack and a 1-\nlayer stack (1/4 input) in the encoder, and a 2-layer de-\ncoder. Our proposed methods are optimized with Adam\noptimizer, and its learning rate starts from 1e−4, decay-\ning two times smaller every epoch. The total number of\nepochs is 8 with proper early stopping. We set the com-\nparison methods as recommended, and the batch size is 32.\nSetup: The input of each dataset is zero-mean normalized.\nMethods Informer Informer† LogTrans Reformer LSTMa LSTnet\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\nETTh1\n24 0.577 0.549 0.620 0.577 0.686 0.604 0.991 0.754 0.650 0.624 1.293 0.901\n48 0.685 0.625 0.692 0.671 0.766 0.757 1.313 0.906 0.702 0.675 1.456 0.960\n168 0.931 0.752 0.947 0.797 1.002 0.846 1.824 1.138 1.212 0.867 1.997 1.214\n336 1.128 0.873 1.094 0.813 1.362 0.952 2.117 1.280 1.424 0.994 2.655 1.369\n720 1.215 0.896 1.241 0.917 1.397 1.291 2.415 1.520 1.960 1.322 2.143 1.380\nETTh2\n24 0.720 0.665 0.753 0.727 0.828 0.750 1.531 1.613 1.143 0.813 2.742 1.457\n48 1.457 1.001 1.461 1.077 1.806 1.034 1.871 1.735 1.671 1.221 3.567 1.687\n168 3.489 1.515 3.485 1.612 4.070 1.681 4.660 1.846 4.117 1.674 3.242 2.513\n336 2.723 1.340 2.626 1.285 3.875 1.763 4.028 1.688 3.434 1.549 2.544 2.591\n720 3.467 1.473 3.548 1.495 3.913 1.552 5.381 2.015 3.963 1.788 4.625 3.709\nETTm1\n24 0.323 0.369 0.306 0.371 0.419 0.412 0.724 0.607 0.621 0.629 1.968 1.170\n48 0.494 0.503 0.465 0.470 0.507 0.583 1.098 0.777 1.392 0.939 1.999 1.215\n96 0.678 0.614 0.681 0.612 0.768 0.792 1.433 0.945 1.339 0.913 2.762 1.542\n288 1.056 0.786 1.162 0.879 1.462 1.320 1.820 1.094 1.740 1.124 1.257 2.076\n672 1.192 0.926 1.231 1.103 1.669 1.461 2.187 1.232 2.736 1.555 1.917 2.941\nWeather\n24 0.335 0.381 0.349 0.397 0.435 0.477 0.655 0.583 0.546 0.570 0.615 0.545\n48 0.395 0.459 0.386 0.433 0.426 0.495 0.729 0.666 0.829 0.677 0.660 0.589\n168 0.608 0.567 0.613 0.582 0.727 0.671 1.318 0.855 1.038 0.835 0.748 0.647\n336 0.702 0.620 0.707 0.634 0.754 0.670 1.930 1.167 1.657 1.059 0.782 0.683\n720 0.831 0.731 0.834 0.741 0.885 0.773 2.726 1.575 1.536 1.109 0.851 0.757\nECL\n48 0.344 0.393 0.334 0.399 0.355 0.418 1.404 0.999 0.486 0.572 0.369 0.445\n168 0.368 0.424 0.353 0.420 0.368 0.432 1.515 1.069 0.574 0.602 0.394 0.476\n336 0.381 0.431 0.381 0.439 0.373 0.439 1.601 1.104 0.886 0.795 0.419 0.477\n720 0.406 0.443 0.391 0.438 0.409 0.454 2.009 1.170 1.676 1.095 0.556 0.565\n960 0.460 0.548 0.492 0.550 0.477 0.589 2.141 1.387 1.591 1.128 0.605 0.599\nCount 33 14 1 0 0 2\nTable 2: Multivariate long sequence time-series forecasting results on four datasets (ﬁve cases).\nUnder the LSTF settings, we prolong the prediction win-\ndows size Ly progressively, i.e.,{1d, 2d, 7d, 14d, 30d, 40d}\nin {ETTh, ECL, Weather }, {6h, 12h, 24h, 72h, 168h }in\nETTm. Metrics: We use two evaluation metrics, including\nMSE = 1\nn\n∑n\ni=1(y −ˆy)2 and MAE = 1\nn\n∑n\ni=1 |y −ˆy|\non each prediction window (averaging for multivariate pre-\ndiction), and roll the whole set with stride = 1. Platform:\nAll the models were trained/tested on a single Nvidia V100\n32GB GPU. The source code is available at https://github.\ncom/zhouhaoyi/Informer2020.\nResults and Analysis\nTable 1 and Table 2 summarize the univariate/multivariate\nevaluation results of all the methods on 4 datasets. We grad-\nually prolong the prediction horizon as a higher requirement\nof prediction capacity, where the LSTF problem setting is\nprecisely controlled to be tractable on one single GPU for\neach method. The best results are highlighted in boldface.\nUnivariate Time-series Forecasting Under this setting,\neach method attains predictions as a single variable over\ntime series. From Table 1, we can observe that:(1) The pro-\nposed model Informer signiﬁcantly improves the inference\nperformance (wining-counts in the last column) across all\ndatasets, and their predict error rises smoothly and slowly\nwithin the growing prediction horizon, which demonstrates\nthe success of Informer in enhancing the prediction capacity\nin the LSTF problem. (2) The Informer beats its canonical\ndegradation Informer†mostly in wining-counts, i.e., 32>12,\nwhich supports the query sparsity assumption in providing\na comparable attention feature map. Our proposed method\nalso out-performs the most related work LogTrans and Re-\nformer. We note that the Reformer keeps dynamic decoding\nand performs poorly in LSTF, while other methods beneﬁt\nfrom the generative style decoder as nonautoregressive pre-\ndictors. (3) The Informer model shows signiﬁcantly better\nresults than recurrent neural networks LSTMa. Our method\nhas a MSE decrease of 26.8% (at 168), 52.4% (at 336) and\n60.1% (at 720). This reveals a shorter network path in the\nself-attention mechanism acquires better prediction capac-\nity than the RNN-based models. (4) The proposed method\noutperforms DeepAR, ARIMA and Prophet on MSE by de-\ncreasing 49.3% (at 168), 61.1% (at 336), and 65.1% (at 720)\nin average. On the ECL dataset, DeepAR performs better\non shorter horizons ( ≤336), and our method surpasses on\nlonger horizons. We attribute this to a speciﬁc example, in\nwhich the effectiveness of prediction capacity is reﬂected\nwith the problem scalability.\nMultivariate Time-series Forecasting Within this set-\nting, some univariate methods are inappropriate, and LSTnet\nis the state-of-art baseline. On the contrary, our proposed In-\nformer is easy to change from univariate prediction to mul-\ntivariate one by adjusting the ﬁnal FCN layer. From Table 2,\nwe observe that: (1) The proposed model Informer greatly\noutperforms other methods and the ﬁndings 1 & 2 in the uni-\nvariate settings still hold for the multivariate time-series.(2)\nThe Informer model shows better results than RNN-based\n48 96 168 240 336 480 624 720\nProlong Input Length (Lx, Ltoken)\n−0.1\n0.0\n0.1\n0.2\n0.3\n0.4\nMSE score\nEncoder Input (horizon=48)\nDecoder Token (horizon=48)\nEncoder Input (horizon=168)\nDecoder Token (horizon=168)\n(a) Input length.\n48 96 168 240 480 624 720\nEncoder Input Length (Lx)\n−0.1\n0.0\n0.1\n0.2\n0.3\n0.4\nMSE score\nInformer, factor c=3\nInformer, factor c=5\nInformer, factor c=8\nInformer, factor c=10 (b) Sampling Factor.\n96 168 240 336 480 720\nEncoder Input Length (Lx)\n0.05\n0.10\n0.15\n0.20\n0.25MSE score\nL-scale Dependency\nL/2-scale Dependency\nL/4-scale Dependency\nInformer Dependency (c) Stacking Combination.\nFigure 4: The parameter sensitivity of three components in Informer.\nPrediction length 336 720\nEncoder’s input 336 720 1440 720 1440 2880\nInformer MSE 0.249 0.225 0.216 0.271 0.261 0.257\nMAE 0.393 0.384 0.376 0.435 0.431 0.422\nInformer† MSE 0.241 0.214 - 0.259 - -\nMAE 0.383 0.371 - 0.423 - -\nLogTrans MSE 0.263 0.231 - 0.273 - -\nMAE 0.418 0.398 - 0.463 - -\nReformer MSE 1.875 1.865 1.861 2.243 2.174 2.113\nMAE 1.144 1.129 1.125 1.536 1.497 1.434\n1 Informer†uses the canonical self-attention mechanism.\n2 The ‘-’ indicates failure for the out-of-memory.\nTable 3: Ablation study of theProbSparse self-attention mechanism.\nMethods\nTraining Testing\nTime Memory Steps\nInformer O(Llog L) O(Llog L) 1\nTransformer O(L2) O(L2) L\nLogTrans O(Llog L) O(L2) 1⋆\nReformer O(Llog L) O(Llog L) L\nLSTM O(L) O(L) L\n1 The LSTnet is hard to present in a closed form.\n2 The ⋆denotes applying our proposed decoder.\nTable 4: L-related computation statics of each layer.\nLSTMa and CNN-based LSTnet, and the MSE decreases\n26.6% (at 168), 28.2% (at 336), 34.3% (at 720) in average.\nCompared with the univariate results, the overwhelming per-\nformance is reduced, and such phenomena can be caused by\nthe anisotropy of feature dimensions’ prediction capacity. It\nis beyond the scope of this paper, and we will explore it in\nthe future work.\nLSTF with Granularity Consideration We perform an\nadditional comparison to explore the performance with var-\nious granularities. The sequences {96, 288, 672}of ETTm1\n(minutes-level) are aligned with {24, 48, 168 }of ETTh 1\n(hour-level). The Informer outperforms other baselines even\nif the sequences are at different granularity levels.\nParameter Sensitivity\nWe perform the sensitivity analysis of the proposed In-\nformer model on ETTh1 under the univariate setting. Input\nLength: In Fig.(4a), when predicting short sequences (like\n48), initially increasing input length of encoder/decoder de-\ngrades performance, but further increasing causes the MSE\nto drop because it brings repeat short-term patterns. How-\never, the MSE gets lower with longer inputs in predict-\ning long sequences (like 168). Because the longer encoder\ninput may contain more dependencies, and the longer de-\ncoder token has rich local information. Sampling Factor:\nThe sampling factor controls the information bandwidth of\nProbSparse self-attention in Eq.(3). We start from the small\nfactor (=3) to large ones, and the general performance in-\ncreases a little and stabilizes at last in Fig.(4b). It veriﬁes\nour query sparsity assumption that there are redundant dot-\nproduct pairs in the self-attention mechanism. We set the\nsample factor c = 5 (the red line) in practice. The Combi-\nnation of Layer Stacking: The replica of Layers is comple-\nmentary for the self-attention distilling, and we investigate\neach stack {L, L/2, L/4}’s behavior in Fig.(4c). The longer\nstack is more sensitive to the inputs, partly due to receiving\nmore long-term information. Our method’s selection (the red\nline), i.e., joining L and L/4, is the most robust strategy.\nAblation Study: How well Informer works?\nWe also conducted additional experiments on ETTh 1 with\nablation consideration.\nThe performance of ProbSparse self-attention mech-\nanism In the overall results Table 1 & 2, we limited the\nproblem setting to make the memory usage feasible for the\ncanonical self-attention. In this study, we compare our meth-\nods with LogTrans and Reformer, and thoroughly explore\ntheir extreme performance. To isolate the memory efﬁcient\nproblem, we ﬁrst reduce settings as {batch size=8, heads=8,\ndim=64}, and maintain other setups in the univariate case.\nIn Table 3, the ProbSparse self-attention shows better per-\nformance than the counterparts. The LogTrans gets OOM\nPrediction length 336 480\nEncoder’s input 336 480 720 960 1200 336 480 720 960 1200\nInformer† MSE 0.249 0.208 0.225 0.199 0.186 0.197 0.243 0.213 0.192 0.174\nMAE 0.393 0.385 0.384 0.371 0.365 0.388 0.392 0.383 0.377 0.362\nInformer‡ MSE 0.229 0.215 0.204 - - 0.224 0.208 0.197 - -\nMAE 0.391 0.387 0.377 - - 0.381 0.376 0.370 - -\n1 Informer‡removes the self-attention distilling from Informer†.\n2 The ‘-’ indicates failure for the out-of-memory.\nTable 5: Ablation study of the self-attention distilling.\nPrediction length 336 480\nPrediction offset +0 +12 +24 +48 +72 +0 +48 +96 +144 +168\nInformer‡ MSE 0.207 0.209 0.211 0.211 0.216 0.198 0.203 0.203 0.208 0.208\nMAE 0.385 0.387 0.391 0.393 0.397 0.390 0.392 0.393 0.401 0.403\nInformer§ MSE 0.201 - - - - 0.392 - - - -\nMAE 0.393 - - - - 0.484 - - - -\n1 Informer§replaces our decoder with dynamic decoding one in Informer‡.\n2 The ‘-’ indicates failure for the unacceptable metric results.\nTable 6: Ablation study of the generative style decoder.\n48 96 168 336 720\nEncoder Input length (Lx)\n1\n2\n3Train time (day)\nLSTnet\nLSTM\nInformer\nInformer†\nLogTrans\nReformer\n48 96 168 336 720\nDecoder predict length (Ly)\n2\n4\n6\n8Inference time (day)\nLSTnet\nLSTM\nInformer\nInformer†\nInformer§\nLogTrans\nReformer\nFigure 5: The total runtime of training/testing phase.\nin extreme cases because its public implementation is the\nmask of the full-attention, which still has O(L2) memory\nusage. Our proposed ProbSparse self-attention avoids this\nfrom the simplicity brought by the query sparsity assump-\ntion in Eq.(4), referring to the pseudo-code in Appendix E.2,\nand reaches smaller memory usage.\nThe performance of self-attention distilling In this\nstudy, we use Informer † as the benchmark to eliminate\nadditional effects of ProbSparse self-attention. The other\nexperimental setup is aligned with the settings of uni-\nvariate Time-series. From Table 5, Informer † has fulﬁlled\nall the experiments and achieves better performance after\ntaking advantage of long sequence inputs. The compari-\nson method Informer ‡removes the distilling operation and\nreaches OOM with longer inputs ( > 720). Regarding the\nbeneﬁts of long sequence inputs in the LSTF problem, we\nconclude that the self-attention distilling is worth adopting,\nespecially when a longer prediction is required.\nThe performance of generative style decoder In this\nstudy, we testify the potential value of our decoder in acquir-\ning a “generative” results. Unlike the existing methods, the\nlabels and outputs are forced to be aligned in the training and\ninference, our proposed decoder’s predicting relies solely on\nthe time stamp, which can predict with offsets. From Ta-\nble 6, we can see that the general prediction performance\nof Informer ‡ resists with the offset increasing, while the\ncounterpart fails for the dynamic decoding. It proves the de-\ncoder’s ability to capture individual long-range dependency\nbetween arbitrary outputs and avoid error accumulation.\nComputation Efﬁciency\nWith the multivariate setting and all the methods’ cur-\nrent ﬁnest implement, we perform a rigorous runtime\ncomparison in Fig.(5). During the training phase, the In-\nformer (red line) achieves the best training efﬁciency among\nTransformer-based methods. During the testing phase, our\nmethods are much faster than others with the generative\nstyle decoding. The comparisons of theoretical time com-\nplexity and memory usage are summarized in Table 4. The\nperformance of Informer is aligned with the runtime experi-\nments. Note that the LogTrans focus on improving the self-\nattention mechanism, and we apply our proposed decoder in\nLogTrans for a fair comparison (the ⋆in Table 4).\n5 Conclusion\nIn this paper, we studied the long-sequence time-series fore-\ncasting problem and proposed Informer to predict long se-\nquences. Speciﬁcally, we designed the ProbSparse self-\nattention mechanism and distilling operation to handle the\nchallenges of quadratic time complexity and quadratic mem-\nory usage in vanilla Transformer. Also, the carefully de-\nsigned generative decoder alleviates the limitation of tra-\nditional encoder-decoder architecture. The experiments on\nreal-world data demonstrated the effectiveness of Informer\nfor enhancing the prediction capacity in LSTF problem.\nAppendices\nAppendix A Related Work\nWe provide a literature review of the long sequence time-\nseries forecasting (LSTF) problem below.\nTime-series Forecasting Existing methods for time-\nseries forecasting can be roughly grouped into two cate-\ngories: classical models and deep learning based methods.\nClassical time-series models serve as a reliable workhorse\nfor time-series forecasting, with appealing properties such as\ninterpretability and theoretical guarantees (Box et al. 2015;\nRay 1990). Modern extensions include the support for miss-\ning data (Seeger et al. 2017) and multiple data types (Seeger,\nSalinas, and Flunkert 2016). Deep learning based methods\nmainly develop sequence to sequence prediction paradigm\nby using RNN and their variants, achieving ground-breaking\nperformance (Hochreiter and Schmidhuber 1997; Li et al.\n2018; Yu et al. 2017). Despite the substantial progress, ex-\nisting algorithms still fail to predict long sequence time\nseries with satisfying accuracy. Typical state-of-the-art ap-\nproaches (Seeger et al. 2017; Seeger, Salinas, and Flunkert\n2016), especially deep-learning methods (Yu et al. 2017; Qin\net al. 2017; Flunkert, Salinas, and Gasthaus 2017; Mukher-\njee et al. 2018; Wen et al. 2017), remain as a sequence\nto sequence prediction paradigm with step-by-step process,\nwhich have the following limitations: (i) Even though they\nmay achieve accurate prediction for one step forward, they\noften suffer from accumulated error from the dynamic de-\ncoding, resulting in the large errors for LSTF problem (Liu\net al. 2019; Qin et al. 2017). The prediction accuracy decays\nalong with the increase of the predicted sequence length.\n(ii) Due to the problem of vanishing gradient and memory\nconstraint (Sutskever, Vinyals, and Le 2014), most existing\nmethods cannot learn from the past behavior of the whole\nhistory of the time-series. In our work, the Informer is de-\nsigned to address this two limitations.\nLong sequence input problem From the above discus-\nsion, we refer to the second limitation as to the long se-\nquence time-series input (LSTI) problem. We will explore\nrelated works and draw a comparison between our LSTF\nproblem. The researchers truncate / summarize / sample the\ninput sequence to handle a very long sequence in practice,\nbut valuable data may be lost in making accurate predictions.\nInstead of modifying inputs, Truncated BPTT (Aicher, Foti,\nand Fox 2019) only uses last time steps to estimate the gra-\ndients in weight updates, and Auxiliary Losses (Trinh et al.\n2018) enhance the gradients ﬂow by adding auxiliary gradi-\nents. Other attempts includes Recurrent Highway Networks\n(Zilly et al. 2017) and Bootstrapping Regularizer (Cao and\nXu 2019). Theses methods try to improve the gradient ﬂows\nin the recurrent network’s long path, but the performance is\nlimited with the sequence length growing in the LSTI prob-\nlem. CNN-based methods (Stoller et al. 2019; Bai, Kolter,\nand Koltun 2018) use the convolutional ﬁlter to capture the\nlong term dependency, and their receptive ﬁelds grow ex-\nponentially with the stacking of layers, which hurts the se-\nquence alignment. In the LSTI problem, the main task is to\nenhance the model’s capacity of receiving long sequence in-\nputs and extract the long-range dependency from these in-\nputs. But the LSTF problem seeks to enhance the model’s\nprediction capacity of forecasting long sequence outputs,\nwhich requires establishing the long-range dependency be-\ntween outputs and inputs. Thus, the above methods are not\nfeasible for LSTF directly.\nAttention model Bahdanau et al. ﬁrstly proposed the ad-\ndictive attention (Bahdanau, Cho, and Bengio 2015) to im-\nprove the word alignment of the encoder-decoder architec-\nture in the translation task. Then, its variant (Luong, Pham,\nand Manning 2015) has proposed the widely used loca-\ntion, general, and dot-product attention. The popular self-\nattention based Transformer (Vaswani et al. 2017) has re-\ncently been proposed as new thinking of sequence modeling\nand has achieved great success, especially in the NLP ﬁeld.\nThe ability of better sequence alignment has been validated\nby applying it to translation, speech, music, and image gen-\neration. In our work, the Informer takes advantage of its se-\nquence alignment ability and makes it amenable to the LSTF\nproblem.\nTransformer-based time-series model The most related\nworks (Song et al. 2018; Ma et al. 2019; Li et al. 2019) all\nstart from a trail on applying Transformer in time-series data\nand fail in LSTF forecasting as they use the vanilla Trans-\nformer. And some other works (Child et al. 2019; Li et al.\n2019) noticed the sparsity in self-attention mechanism and\nwe have discussed them in the main context.\nAppendix B The Uniform Input\nRepresentation\nThe RNN models (Schuster and Paliwal 1997; Hochre-\niter and Schmidhuber 1997; Chung et al. 2014; Sutskever,\nVinyals, and Le 2014; Qin et al. 2017; Chang et al. 2018)\ncapture the time-series pattern by the recurrent structure it-\nself and barely relies on time stamps. The vanilla trans-\nformer (Vaswani et al. 2017; Devlin et al. 2018) uses point-\nwise self-attention mechanism and the time stamps serve as\nlocal positional context. However, in the LSTF problem, the\nability to capture long-range independence requires global\ninformation like hierarchical time stamps (week, month and\nyear) and agnostic time stamps (holidays, events). These\nare hardly leveraged in canonical self-attention and conse-\nquent query-key mismatches between the encoder and de-\ncoder bring underlying degradation on the forecasting per-\nformance. We propose a uniform input representation to mit-\nigate the issue, the Fig.(6) gives an intuitive overview.\nAssuming we have t-th sequence input Xt and p types\nof global time stamps and the feature dimension after input\nrepresentation is dmodel. We ﬁrstly preserve the local context\nby using a ﬁxed position embedding:\nPE(pos,2j) = sin(pos/(2Lx)2j/dmodel )\nPE(pos,2j+1) = cos(pos/(2Lx)2j/dmodel )\n, (7)\nwhere j ∈{1,..., ⌊dmodel/2⌋}. Each global time stamp is\nemployed by a learnable stamp embeddings SE (pos) with\nlimited vocab size (up to 60, namely taking minutes as\nthe ﬁnest granularity). That is, the self-attention’s similarity\ncomputation can have access to global context and the com-\nputation consuming is affordable on long inputs. To align the\ndimension, we project the scalar context xt\ni into dmodel-dim\nvector ut\ni with 1-D convolutional ﬁlters (kernel width=3,\nstride=1). Thus, we have the feeding vector\nXt\nfeed[i] = αut\ni+PE(Lx×(t−1)+i, )+\n∑\np\n[SE(Lx×(t−1)+i)]p , (8)\nwhere i ∈{1,...,L x}, and α is the factor balancing the\nmagnitude between the scalar projection and local/global\nembeddings. We recommend α = 1 if the sequence input\nhas been normalized.\nPosition\nEmbeddings\nP\nE0\nP\nE1\nP\nE2\nP\nE3\nP\nE4\nP\nE5\nP\nE6\nP\nE7\nProjection u0 u1 u2 u3 u4 u5 u6 u7\nWeek\nEmbeddings\nWeek\nE0\nWeek\nE1\nMonth\nEmbeddings\nMonth\nE0\nHoliday\nEmbeddings\nWeek\nE2\nWeek\nE3\nE0 E0 E0\nH1\nE1 E0 E0\nH2\nE2\nH2\nE2\nGlobal Time Stamp\nLocal Time Stamp\nScalar\nFigure 6: The input representation of Informer. The inputs’s\nembedding consists of three separate parts, a scalar projec-\ntion, the local time stamp (Position) and global time stamp\nembeddings (Minutes, Hours, Week, Month, Holiday etc.).\nAppendix C The long tail distribution in\nself-attention feature map\nWe have performed the vanilla Transformer on the ETTh1\ndataset to investigate the distribution of self-attention fea-\nture map. We select the attention score of {Head1,Head7}\n@ Layer1. The blue line in Fig.(7) forms a long tail distri-\nbution, i.e. a few dot-product pairs contribute to the major\nattention and others can be ignored.\nFigure 7: The Softmax scores in the self-attention from a\n4-layer canonical Transformer trained on ETTh1 dataset.\nAppendix D Details of the proof\nProof of Lemma 1\nProof. For the individual qi, we can relax the discrete\nkeys into the continuous d-dimensional variable, i.e. vec-\ntor kj. The query sparsity measurement is deﬁned as the\nM(qi,K) = ln ∑LK\nj=1 eqik⊤\nj /\n√\nd − 1\nLK\n∑LK\nj=1(qik⊤\nj /\n√\nd).\nFirstly, we look into the left part of the inequality. For\neach query qi, the ﬁrst term of the M(qi,K) becomes the\nlog-sum-exp of the inner-product of a ﬁxed query qi and all\nthe keys , and we can deﬁne fi(K) = ln ∑LK\nj=1 eqik⊤\nj /\n√\nd.\nFrom the Eq.(2) in the Log-sum-exp network(Calaﬁore,\nGaubert, and Possieri 2018) and the further analysis, the\nfunction fi(K) is convex. Moreover, fi(K) add a linear\ncombination of kj makes the M(qi,K) to be the convex\nfunction for a ﬁxed query. Then we can take the deriva-\ntion of the measurement with respect to the individual vec-\ntor kj as ∂M(qi,K)\n∂kj\n= e\nqik⊤\nj /\n√\nd\n∑LK\nj=1 e\nqik⊤\nj /\n√\nd · qi√\nd − 1\nLK\n· qi√\nd. To\nreach the minimum value, we let ⃗∇M(qi) = ⃗0 and the fol-\nlowing condition is acquired as qik⊤\n1 + ln LK = ··· =\nqik⊤\nj + ln LK = ··· = ln ∑LK\nj=1 eqik⊤\nj . Naturally, it re-\nquires k1 = k2 = ··· = kLK, and we have the measure-\nment’s minimum asln LK, i.e.\nM(qi,K) ≥ln LK . (9)\nSecondly, we look into the right part of the inequality. If\nwe select the largest inner-product maxj{qik⊤\nj /\n√\nd}, it is\neasy that\nM(qi,K) = ln\nLK∑\nj=1\ne\nqik⊤\nj√\nd − 1\nLK\nLK∑\nj=1\n(qik⊤\nj√\nd\n)\n≤ln(LK ·max\nj\n{qik⊤\nj√\nd\n})− 1\nLK\nLK∑\nj=1\n(qik⊤\nj√\nd\n)\n= ln LK +max\nj\n{qik⊤\nj√\nd\n}− 1\nLK\nLK∑\nj=1\n(qik⊤\nj√\nd\n)\n. (10)\nCombine the Eq.(14) and Eq.(15), we have the results of\nLemma 1. When the key set is the same with the query set,\nthe above discussion also holds.\nProposition 1. Assuming kj ∼ N(µ,Σ) and we let qki\ndenote set {(qik⊤\nj )/\n√\nd |j = 1 ,...,L K}, then ∀Mm =\nmaxiM(qi,K) there exist κ >0 such that: in the interval\n∀q1,q2 ∈{q|M(q,K) ∈[Mm,Mm−κ)}, if M(q1,K) >\nM(q2,K) and Var(qk1) >Var(qk2), we have high prob-\nability that M(q1,K) >M (q2,K).\nProof of Proposition 1\nProof. To make the further discussion simplify, we\ncan note ai,j = qikT\nj /\n√\nd, thus deﬁne the ar-\nray Ai = [ ai,1,··· ,ai,Lk]. Moreover, we denote\n1\nLK\n∑LK\nj=1(qik⊤\nj /\n√\nd) = mean( Ai), then we can denote\n¯M(qi,K) = max(Ai) −mean(Ai), i= 1,2.\nAs for M(qi,K), we denote each component ai,j =\nmean(Ai) + ∆ai,j,j = 1 ,··· ,Lk, then we have the fol-\nlowing:\nM(qi,K) = ln\nLK∑\nj=1\neqik⊤\nj /\n√\nd − 1\nLK\nLK∑\nj=1\n(qik⊤\nj /\n√\nd)\n= ln(ΣLk\nj=1emean(Ai)e∆ai,j) −mean(Ai)\n= ln(emean(Ai)ΣLk\nj=1e∆ai,j) −mean(Ai)\n= ln(ΣLk\nj=1e∆ai,j)\n,\nand it is easy to ﬁnd ΣLk\nj=1∆ai,j = 0.\nWe deﬁne the function ES(Ai) = Σ Lk\nj=1 exp(∆ai,j),\nequivalently deﬁnes Ai = [∆ai,1,··· ,∆ai,Lk], and imme-\ndiately our proposition can be written as the equivalent form:\nFor ∀A1,A2, if\n1. max(A1) −mean(A1) ≥max(A2) −mean(A2)\n2. Var(A1) >Var(A2)\nThen we rephrase the original conclusion into more\ngeneral form that ES(A1) > ES(A2) with high proba-\nbility, and the probability have positive correlation with\nVar(A1) −Var(A2).\nFurthermore, we consider a ﬁne case, ∀Mm =\nmaxiM(qi,K) there exist κ >0 such that in that interval\n∀qi,qj ∈{q|M(q,K) ∈[Mm,Mm −κ)}if max(A1) −\nmean(A1) ≥ max(A2) −mean(A2) and Var(A1) >\nVar(A2), we have high probability that M(q1,K) >\nM(q2,K),which is equivalent to ES(A1) >ES (A2).\nIn the original proposition, kj ∼N(µ,Σ) follows multi-\nvariate Gaussian distribution, which means that k1,··· ,kn\nare I.I.D Gaussian distribution, thus deﬁned by the Wiener-\nkhinchin law of large Numbers, ai,j = qikT\nj /\n√\nd is one-\ndimension Gaussian distribution with the expectation of\n0 if n → ∞. So back to our deﬁnition, ∆a1,m ∼\nN(0,σ2\n1),∆a2,m ∼ N(0,σ2\n2),∀m ∈ 1,··· ,Lk, and our\nproposition is equivalent to a lognormal-distribution sum\nproblem.\nA lognormal-distribution sum problem is equivalent\nto approximating the distribution of ES(A1) accurately,\nwhose history is well-introduced in the articles (Dufresne\n2008),(Vargasguzman 2005). Approximating lognormality\nof sums of lognormals is a well-known rule of thumb, and\nno general PDF function can be given for the sums of log-\nnormals. However, (Romeo, Da Costa, and Bardou 2003)\nand (Hcine and Bouallegue 2015) pointed out that in most\ncases, sums of lognormals is still a lognormal distribution,\nand by applying central limits theorem in (Beaulieu 2011),\nwe can have a good approximation that ES(A1) is a log-\nnormal distribution, and we have E(ES(A1)) = ne\nσ2\n1\n2 ,\nVar(ES(A1)) = neσ2\n1 (eσ2\n1 −1). Equally, E(ES(A2)) =\nne\nσ2\n2\n2 , Var(ES(A2)) = neσ2\n2 (eσ2\n2 −1).\nWe denoteB1 = ES(A1),B2 = ES(A2), and the proba-\nbility Pr(B1 −B2 >0) is the ﬁnal result of our proposition\nin general conditions, with σ2\n1 >σ2\n2 WLOG. The difference\nof lognormals is still a hard problem to solve.\nBy using the theorem given in(Lo 2012), which gives a\ngeneral approximation of the probability distribution on the\nsums and difference for the lognormal distribution. Namely\nS1 and S2 are two lognormal stochastic variables obeying\nthe stochastic differential equations dSi\nSi\n= σidZi, i = 1,2,\nin which dZ1,2 presents a standard Weiner process associ-\nated with S1,2 respectively, and σ2\ni = Var (ln Si), S± ≡\nS1 ±S2,S±\n0 ≡S10 ±S20. As for the joint probability dis-\ntribution function P(S1,S2,t; S10,S20,t0), the value of S1\nand S2 at time t > t0 are provided by their initial value\nS10 and S20 at initial time t0. The Weiner process above is\nequivalent to the lognormal distribution(Weiner and Solbrig\n1984), and the conclusion below is written in general form\ncontaining both the sum and difference of lognormal distri-\nbution approximation denoting ±for sum + and difference\n−respectively.\nIn boundary condition\n¯P±\n(\nS±,t; S10,S20,t0 −→t\n)\n= δ\n(\nS10 ±S20 −S±)\n,\ntheir closed-form probability distribution functions are given\nby\nfLN\n(\n˜S±,t; ˜S±\n0 ,t0\n)\n= 1\n˜S±\n√\n2π˜σ2\n±(t−t0)\n·exp\n\n\n\n−\n[\nln\n(\n˜S+/˜S+\n0\n)\n+ (1/2)˜σ2\n±(t−t0)\n]2\n2˜σ2\n±(t−t0)\n\n\n\n.\nIt is an approximately normal distribution, and ˜S+, ˜S−are\nlognormal random variables, ˜S±\n0 are initial condition in t0\ndeﬁned by Weiner process above. (Noticed that ˜σ2\n±(t−t0)\nshould be small to make this approximation valid.In our sim-\nulation experiment, we set t−t0 = 1 WLOG.) Since\n˜S−\n0 = (S10 −S20) +\n( σ2\n−\nσ2\n1 −σ2\n2\n)\n(S10 + S20),\nand\n˜σ−=\n(\nσ2\n1 −σ2\n2\n)\n/(2σ−)\nσ−=\n√\nσ2\n1 + σ2\n2\nNoticed that E(B1) > E(B2), Var(B1) > Var(B2), the\nmean value and the variance of the approximate normal dis-\ntribution shows positive correlation with σ2\n1 −σ2\n2.Besides,\nthe closed-form PDF fLN\n(\n˜S±,t; ˜S±\n0 ,t0\n)\nalso show pos-\nitive correlation with σ2\n1 −σ2\n2. Due to the limitation of\n˜σ2\n±(t−t0) should be small enough, such positive correla-\ntion is not signiﬁcant in our illustrative numerical experi-\nment.\nBy using Lie-Trotter Operator Splitting Method in (Lo\n2012), we can give illustrative numeral examples for the\ndistribution of B1 −B2,in which the parameters are well\nchosen to ﬁt for our top-u approximation in actual LLLT\nexperiments. Figure shows that it is of high probability\nFigure 8: Probability Density verses S1 −S2 for the approx-\nimation of shifted lognormal distribution.\nthat when σ2\n1 > σ2\n2, the inequality holds that B1 > B2,\nES(A1) >ES (A2).\nFinishing prooving our proposition in general conditions,\nwe can consider a more speciﬁc condition that if q1,q2 ∈\n{q|M(q,K) ∈[Mm,Mm−κ)}, the proposition still holds\nwith high probability.\nFirst, we have M(q1,k) = ln(B1) > (Mm −κ)\nholds for ∀q1,q2 in this interval. Since we have proved\nthat E(B1)) = ne\nσ2\n1\n2 , we can conclude that ∀qi in the\ngiven interval,∃α,σ2\ni > α,i = 1 ,2. Since we have ˜S−\n0 =\n(S10 −S20) +\n( σ2\n−\nσ2\n1−σ2\n2\n)\n(S10 + S20), which also shows\npositive correlation with σ2\n1 + σ2\n2 > 2α, and positive cor-\nrelation with σ2\n1 −σ2\n2. So due to the nature of the ap-\nproximate normal distribution PDF, if σ2\n1 > σ2\n2 WLOG,\nPr(M(q1,k) > M(q2,k)) ≈Φ(\n˜S−\n0\n˜σ−\n) also shows positive\ncorrelation with σ2\n1 + σ2\n2 >2α.\nWe give an illustrative numerical examples of the approx-\nimation above in Fig.(8). In our actual LTTnet experiment,\nwe choose Top-k of A1,A2, not the whole set.Actually,\nwe can make a naive assumption that in choosing top −\n⌊1\n4 Lk⌋variables of A1,A2 denoted as A\n′\n1,A\n′\n2,the varia-\ntion σ1,σ2 don’t change signiﬁcantly, but the expectation\nE(A\n′\n1),E(A\n′\n2) ascends obviously, which leads to initial con-\ndition S10,S20 ascends signiﬁcantly, since the initial condi-\ntion will be sampled from top−⌊1\n4 Lk⌋variables, not the\nwhole set.\nIn our actual LTTnet experiment, we setU, namely choos-\ning around top −⌊1\n4 Lk⌋of A1 and A2, it is guaranteed\nthat with over 99% probability that in the [Mm,Mm −κ)\ninterval, as shown in the black curve of Fig.(8). Typically\nthe condition 2 can be relaxed, and we can believe that\nif q1,q2 ﬁts the condition 1 in our proposition, we have\nM(q1,K) >M (q2,K).\nAppendix E Reproducibility\nDetails of the experiments\nThe details of proposed Informer model is summarized in\nTable 7. For the ProbSparse self-attention mechanism, we\nlet d=32, n=16 and add residual connections, a position-\nwise feed-forward network layer (inner-layer dimension is\n2048) and a dropout layer ( p = 0.1) likewise. Note that we\npreserves 10% validation data for each dataset, so all the\nexperiments are conducted over 5 random train/val shifting\nselection along time and the results are averaged over the\n5 runs. All the datasets are performed standardization such\nthat the mean of variable is 0 and the standard deviation is 1.\nTable 7: The Informer network components in details\nEncoder: N\nInputs 1x3 Conv1d Embedding (d= 512)\n4\nProbSparse\nSelf-attention\nBlock\nMulti-head ProbSparse Attention (h= 16, d= 32)\nAdd, LayerNorm, Dropout (p= 0.1)\nPos-wise FFN (dinner = 2048), GELU\nAdd, LayerNorm, Dropout (p= 0.1)\nDistilling 1x3 conv1d, ELU\nMax pooling (stride = 2)\nDecoder: N\nInputs 1x3 Conv1d Embedding (d= 512)\n2\nMasked PSB add Mask on Attention Block\nSelf-attention\nBlock\nMulti-head Attention (h= 8, d= 64)\nAdd, LayerNorm, Dropout (p= 0.1)\nPos-wise FFN (dinner = 2048), GELU\nAdd, LayerNorm, Dropout (p= 0.1)\nFinal:\nOutputs FCN (d= dout)\nImplement of the ProbSparse self-attention\nWe have implemented the ProbSparse self-attention in\nPython 3.6 with Pytorch 1.0. The pseudo-code is given in\nAlgo.(1). The source code is available at https://github.com/\nzhouhaoyi/Informer2020. All the procedure can be highly\nefﬁcient vector operation and maintains logarithmic total\nmemory usage. The masked version can be achieved by ap-\nplying positional mask on step 6 and using cmusum (·) in\nmean(·) of step 7. In the practice, we can use sum (·) as the\nsimpler implement of mean(·).\nAlgorithm 1 ProbSparse self-attention\nRequire: Tensor Q ∈Rm×d, K ∈Rn×d, V ∈Rn×d\n1: print set hyperparameter c, u= cln mand U = mln n\n2: randomly select U dot-product pairs from K as ¯K\n3: set the sample score ¯S = Q ¯K⊤\n4: compute the measurement M = max(¯S) −mean(¯S) by row\n5: set Top- uqueries under M as ¯Q\n6: set S1 = softmax( ¯QK⊤/\n√\nd) ·V\n7: set S0 = mean(V)\n8: set S = {S1,S0}by their original rows accordingly\nEnsure: self-attention feature map S.\nThe hyperparameter tuning range\nFor all methods, the input length of recurrent component is\nchosen from {24, 48, 96, 168, 336, 720 }for the ETTh1,\nETTh2, Weather and Electricity dataset, and chosen from\n{24, 48, 96, 192, 288, 672 }for the ETTm dataset. For\n0 50 100 150 200 250 300\n−1.75\n−1.50\n−1.25\n−1.00\n−0.75\n−0.50\n−0.25\n0.00 ETTm 1 Gro ndTr th\nInformer\n0 50 100 150 200 250 300\nETTm 1 Gro ndTr th\nInformer†\n0 50 100 150 200 250 300\nETTm 1 Gro ndTr th\nLogTrans\n0 50 100 150 200 250 300\nETTm 1 Gro ndTr th\nReformer\n0 50 100 150 200 250 300\nETTm 1 Gro ndTr th\nDeepAR\n0 50 100 150 200 250 300\nETTm 1 Gro ndTr th\nLSTMa\n0 50 100 150 200 250 300\nETTm 1 Gro ndTr th\nARIMA\n0 50 100 150 200 250 300\nETTm 1 Gro ndTr th\nProphet\nFigure 9: The predicts (len=336) of Informer, Informer†, LogTrans, Reformer, DeepAR, LSTMa, ARIMA and Prophet on the\nETTm dataset. The red / blue curves stand for slices of the prediction / ground truth.\nLSTMa and DeepAR, the size of hidden states is chosen\nfrom {32, 64, 128, 256 }. For LSTnet, the hidden dimen-\nsion of the Recurrent layer and Convolutional layer is cho-\nsen from {64, 128, 256 }and {32, 64, 128 }for Recurrent-\nskip layer, and the skip-length of Recurrent-skip layer is set\nas 24 for the ETTh1, ETTh2, Weather and ECL dataset, and\nset as 96 for the ETTm dataset. For Informer, the layer of en-\ncoder is chosen from {6, 4, 3, 2}and the layer of decoder is\nset as 2. The head number of multi-head attention is chosen\nfrom {8, 16}, and the dimension of multi-head attention’s\noutput is set as 512. The length of encoder’s input sequence\nand decoder’s start token is chosen from {24, 48, 96, 168,\n336, 480, 720 }for the ETTh1, ETTh2, Weather and ECL\ndataset, and {24, 48, 96, 192, 288, 480, 672}for the ETTm\ndataset. In the experiment, the decoder’s start token is a seg-\nment truncated from the encoder’s input sequence, so the\nlength of decoder’s start token must be less than the length\nof encoder’s input.\nThe RNN-based methods perform a dynamic decoding\nwith left shifting on the prediction windows. Our proposed\nmethods Informer-series and LogTrans (our decoder) per-\nform non-dynamic decoding.\nAppendix F Extra experimental results\nFig.(9) presents a slice of the predicts of 8 models. The most\nrealted work LogTrans and Reformer shows acceptable re-\nsults. The LSTMa model is not amenable for the long se-\nquence prediction task. The ARIMA and DeepAR can cap-\nture the long trend of the long sequences. And the Prophet\ndetects the changing point and ﬁts it with a smooth curve\nbetter than the ARIMA and DeepAR. Our proposed model\nInformer and Informer † show signiﬁcantly better results\nthan above methods.\nAppendix G Computing Infrastructure\nAll the experiments are conducted on Nvidia Tesla V100\nSXM2 GPUs (32GB memory). Other conﬁguration includes\n2 * Intel Xeon Gold 6148 CPU, 384GB DDR4 RAM and 2\n* 240GB M.2 SSD, which is sufﬁcient for all the baselines.\nReferences\nAicher, C.; Foti, N. J.; and Fox, E. B. 2019. Adaptively Trun-\ncating Backpropagation Through Time to Control Gradient\nBias. arXiv:1905.07473 .\nAriyo, A. A.; Adewumi, A. O.; and Ayo, C. K. 2014. Stock\nprice prediction using the ARIMA model. In The 16th In-\nternational Conference on Computer Modelling and Simu-\nlation, 106–112. IEEE.\nBahdanau, D.; Cho, K.; and Bengio, Y . 2015. Neural Ma-\nchine Translation by Jointly Learning to Align and Trans-\nlate. In ICLR 2015.\nBai, S.; Kolter, J. Z.; and Koltun, V . 2018. Convolutional\nsequence modeling revisited. ICLR .\nBeaulieu, N. C. 2011. An extended limit theorem for cor-\nrelated lognormal sums. IEEE transactions on communica-\ntions 60(1): 23–26.\nBeltagy, I.; Peters, M. E.; and Cohan, A. 2020. Longformer:\nThe Long-Document Transformer. CoRR abs/2004.05150.\nBox, G. E.; Jenkins, G. M.; Reinsel, G. C.; and Ljung, G. M.\n2015. Time series analysis: forecasting and control . John\nWiley & Sons.\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,\nT.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter,\nC.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;\nChess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,\nA.; Sutskever, I.; and Amodei, D. 2020. Language Models\nare Few-Shot Learners. CoRR abs/2005.14165.\nCalaﬁore, G. C.; Gaubert, S.; and Possieri, C. 2018. Log-\nsum-exp neural networks and posynomial models for convex\nand log-log-convex data. CoRR abs/1806.07850.\nCao, Y .; and Xu, P. 2019. Better Long-Range Depen-\ndency By Bootstrapping A Mutual Information Regularizer.\narXiv:1905.11978 .\nChang, Y .-Y .; Sun, F.-Y .; Wu, Y .-H.; and Lin, S.-D. 2018.\nA Memory-Network Based Solution for Multivariate Time-\nSeries Forecasting. arXiv:1809.02105 .\nChild, R.; Gray, S.; Radford, A.; and Sutskever, I. 2019.\nGenerating Long Sequences with Sparse Transformers.\narXiv:1904.10509 .\nCho, K.; van Merrienboer, B.; Bahdanau, D.; and Bengio,\nY . 2014. On the Properties of Neural Machine Trans-\nlation: Encoder-Decoder Approaches. In Proceedings of\nSSST@EMNLP 2014, 103–111.\nChung, J.; Gulcehre, C.; Cho, K.; and Bengio, Y . 2014. Em-\npirical evaluation of gated recurrent neural networks on se-\nquence modeling. arXiv:1412.3555 .\nClevert, D.; Unterthiner, T.; and Hochreiter, S. 2016. Fast\nand Accurate Deep Network Learning by Exponential Lin-\near Units (ELUs). In ICLR 2016.\nDai, Z.; Yang, Z.; Yang, Y .; Carbonell, J.; Le, Q. V .; and\nSalakhutdinov, R. 2019. Transformer-xl: Attentive language\nmodels beyond a ﬁxed-length context. arXiv:1901.02860 .\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv:1810.04805 .\nDufresne, D. 2008. Sums of lognormals. In Actuarial Re-\nsearch Conference, 1–6.\nFlunkert, V .; Salinas, D.; and Gasthaus, J. 2017. DeepAR:\nProbabilistic forecasting with autoregressive recurrent net-\nworks. arXiv:1704.04110 .\nGupta, A.; and Rush, A. M. 2017. Dilated convolu-\ntions for modeling long-distance genomic dependencies.\narXiv:1710.01278 .\nHcine, M. B.; and Bouallegue, R. 2015. On the approxima-\ntion of the sum of lognormals by a log skew normal distri-\nbution. arXiv preprint arXiv:1502.03619 .\nHochreiter, S.; and Schmidhuber, J. 1997. Long short-term\nmemory. Neural computation 9(8): 1735–1780.\nKitaev, N.; Kaiser, L.; and Levskaya, A. 2019. Reformer:\nThe Efﬁcient Transformer. In ICLR.\nLai, G.; Chang, W.-C.; Yang, Y .; and Liu, H. 2018. Model-\ning long-and short-term temporal patterns with deep neural\nnetworks. In ACM SIGIR 2018, 95–104. ACM.\nLi, S.; Jin, X.; Xuan, Y .; Zhou, X.; Chen, W.; Wang, Y .-X.;\nand Yan, X. 2019. Enhancing the Locality and Breaking the\nMemory Bottleneck of Transformer on Time Series Fore-\ncasting. arXiv:1907.00235 .\nLi, Y .; Yu, R.; Shahabi, C.; and Liu, Y . 2018. Diffusion Con-\nvolutional Recurrent Neural Network: Data-Driven Trafﬁc\nForecasting. In ICLR 2018.\nLiu, Y .; Gong, C.; Yang, L.; and Chen, Y . 2019. DSTP-RNN:\na dual-stage two-phase attention-based recurrent neural net-\nworks for long-term and multivariate time series prediction.\nCoRR abs/1904.07464.\nLo, C.-F. 2012. The sum and difference of two lognormal\nrandom variables. Journal of Applied Mathematics 2012.\nLuong, T.; Pham, H.; and Manning, C. D. 2015. Effective\nApproaches to Attention-based Neural Machine Transla-\ntion. In M `arquez, L.; Callison-Burch, C.; Su, J.; Pighin, D.;\nand Marton, Y ., eds.,EMNLP, 1412–1421. The Association\nfor Computational Linguistics. doi:10.18653/v1/d15-1166.\nURL https://doi.org/10.18653/v1/d15-1166.\nMa, J.; Shou, Z.; Zareian, A.; Mansour, H.; Vetro, A.;\nand Chang, S.-F. 2019. CDSA: Cross-Dimensional Self-\nAttention for Multivariate, Geo-tagged Time Series Impu-\ntation. arXiv:1905.09904 .\nMatsubara, Y .; Sakurai, Y .; van Panhuis, W. G.; and Falout-\nsos, C. 2014. FUNNEL: automatic mining of spatially coe-\nvolving epidemics. In ACM SIGKDD 2014, 105–114.\nMukherjee, S.; Shankar, D.; Ghosh, A.; Tathawadekar, N.;\nKompalli, P.; Sarawagi, S.; and Chaudhury, K. 2018. Ar-\nmdn: Associative and recurrent mixture density networks for\neretail demand forecasting. arXiv:1803.03800 .\nPapadimitriou, S.; and Yu, P. 2006. Optimal multi-scale pat-\nterns in time series streams. In ACM SIGMOD 2006, 647–\n658. ACM.\nQin, Y .; Song, D.; Chen, H.; Cheng, W.; Jiang, G.; and Cot-\ntrell, G. W. 2017. A Dual-Stage Attention-Based Recurrent\nNeural Network for Time Series Prediction. In IJCAI 2017,\n2627–2633.\nQiu, J.; Ma, H.; Levy, O.; Yih, S. W.-t.; Wang, S.; and Tang,\nJ. 2019. Blockwise Self-Attention for Long Document Un-\nderstanding. arXiv:1911.02972 .\nRae, J. W.; Potapenko, A.; Jayakumar, S. M.; and Lillicrap,\nT. P. 2019. Compressive transformers for long-range se-\nquence modelling. arXiv:1911.05507 .\nRay, W. 1990. Time series: theory and methods. Journal of\nthe Royal Statistical Society: Series A (Statistics in Society)\n153(3): 400–400.\nRomeo, M.; Da Costa, V .; and Bardou, F. 2003. Broad distri-\nbution effects in sums of lognormal random variables. The\nEuropean Physical Journal B-Condensed Matter and Com-\nplex Systems 32(4): 513–525.\nSchuster, M.; and Paliwal, K. K. 1997. Bidirectional recur-\nrent neural networks. IEEE Transactions on Signal Process-\ning 45(11): 2673–2681.\nSeeger, M.; Rangapuram, S.; Wang, Y .; Salinas, D.;\nGasthaus, J.; Januschowski, T.; and Flunkert, V . 2017.\nApproximate bayesian inference in linear state space\nmodels for intermittent demand forecasting at scale.\narXiv:1709.07638 .\nSeeger, M. W.; Salinas, D.; and Flunkert, V . 2016. Bayesian\nintermittent demand forecasting for large inventories. In\nNIPS, 4646–4654.\nSong, H.; Rajan, D.; Thiagarajan, J. J.; and Spanias, A. 2018.\nAttend and diagnose: Clinical time series analysis using at-\ntention models. In AAAI 2018.\nStoller, D.; Tian, M.; Ewert, S.; and Dixon, S. 2019. Seq-\nU-Net: A One-Dimensional Causal U-Net for Efﬁcient Se-\nquence Modelling. arXiv:1911.06393 .\nSutskever, I.; Vinyals, O.; and Le, Q. V . 2014. Sequence\nto sequence learning with neural networks. In NIPS, 3104–\n3112.\nTaylor, S. J.; and Letham, B. 2018. Forecasting at scale.The\nAmerican Statistician 72(1): 37–45.\nTrinh, T. H.; Dai, A. M.; Luong, M.-T.; and Le, Q. V . 2018.\nLearning longer-term dependencies in rnns with auxiliary\nlosses. arXiv preprint arXiv:1803.00144 .\nTsai, Y .-H. H.; Bai, S.; Yamada, M.; Morency, L.-P.; and\nSalakhutdinov, R. 2019. Transformer Dissection: An Uni-\nﬁed Understanding for Transformer’s Attention via the Lens\nof Kernel. In ACL 2019, 4335–4344.\nVargasguzman, J. A. 2005. Change of Support of Transfor-\nmations: Conservation of Lognormality Revisited. Mathe-\nmatical Geosciences 37(6): 551–567.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In NIPS, 5998–6008.\nWang, S.; Li, B.; Khabsa, M.; Fang, H.; and Ma, H.\n2020. Linformer: Self-Attention with Linear Complexity.\narXiv:2006.04768 .\nWeiner, J.; and Solbrig, O. T. 1984. The meaning and mea-\nsurement of size hierarchies in plant populations. Oecologia\n61(3): 334–336.\nWen, R.; Torkkola, K.; Narayanaswamy, B.; and Madeka,\nD. 2017. A multi-horizon quantile recurrent forecaster.\narXiv:1711.11053 .\nYu, F.; Koltun, V .; and Funkhouser, T. 2017. Dilated residual\nnetworks. In CVPR, 472–480.\nYu, R.; Zheng, S.; Anandkumar, A.; and Yue, Y . 2017. Long-\nterm forecasting using tensor-train rnns. arXiv:1711.00073\n.\nZhu, Y .; and Shasha, D. E. 2002. StatStream: Statistical\nMonitoring of Thousands of Data Streams in Real Time. In\nVLDB 2002, 358–369.\nZilly, J. G.; Srivastava, R. K.; Koutn´ık, J.; and Schmidhuber,\nJ. 2017. Recurrent highway networks. InICML, 4189–4198."
}