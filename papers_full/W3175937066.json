{
  "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
  "url": "https://openalex.org/W3175937066",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4227588299",
      "name": "Steiner, Andreas",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2590063661",
      "name": "Kolesnikov, Alexander",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225989451",
      "name": "Zhai, Xiaohua",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287021434",
      "name": "Wightman, Ross",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227555713",
      "name": "Uszkoreit, Jakob",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225989450",
      "name": "Beyer, Lucas",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3170227631",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2962900737",
    "https://openalex.org/W3138994021",
    "https://openalex.org/W3103856189",
    "https://openalex.org/W2331143823",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2962843773",
    "https://openalex.org/W3008526508",
    "https://openalex.org/W3143083666",
    "https://openalex.org/W2804935296",
    "https://openalex.org/W3097217077",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2115579991",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W3172509117",
    "https://openalex.org/W3145444543",
    "https://openalex.org/W3106031848",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3157506437",
    "https://openalex.org/W3194591991",
    "https://openalex.org/W3035060554",
    "https://openalex.org/W3102631365",
    "https://openalex.org/W3171087525",
    "https://openalex.org/W2971315489",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W1977295328",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2062118960"
  ],
  "abstract": "Vision Transformers (ViT) have been shown to attain highly competitive performance for a wide range of vision applications, such as image classification, object detection and semantic image segmentation. In comparison to convolutional neural networks, the Vision Transformer's weaker inductive bias is generally found to cause an increased reliance on model regularization or data augmentation (\"AugReg\" for short) when training on smaller training datasets. We conduct a systematic empirical study in order to better understand the interplay between the amount of training data, AugReg, model size and compute budget. As one result of this study we find that the combination of increased compute and AugReg can yield models with the same performance as models trained on an order of magnitude more training data: we train ViT models of various sizes on the public ImageNet-21k dataset which either match or outperform their counterparts trained on the larger, but not publicly available JFT-300M dataset.",
  "full_text": "Published in Transactions on Machine Learning Research (05/2022)\nHow to train your ViT? Data, Augmentation,\nand Regularization in Vision Transformers\nAndreas Steiner∗ andstein@google.com\nAlexander Kolesnikov∗ akolesnikov@google.com\nXiaohua Zhai∗ xzhai@google.com\nRoss Wightman† rwightman@gmail.com\nJakob Uszkoreit usz@google.com\nLucas Beyer∗ lbeyer@google.com\nGoogle Research, Brain Team, Zürich∗Equal technical contribution,†independent researcher\nReviewed on OpenReview:https: // openreview. net/ forum? id= 4nPswr1KcP\nAbstract\nVision Transformers (ViT) have been shown to attain highly competitive performance\nfor a wide range of vision applications, such as image classiﬁcation, object detection and\nsemantic image segmentation. In comparison to convolutional neural networks, the Vision\nTransformer’s weaker inductive bias is generally found to cause an increased reliance on\nmodel regularization or data augmentation (“AugReg” for short) when training on smaller\ntraining datasets. We conduct a systematic empirical study in order to better understand the\ninterplay between the amount of training data, AugReg, model size and compute budget.1\nAs one result of this study we ﬁnd that the combination of increased compute and AugReg\ncan yield models with the same performance as models trained on an order of magnitude\nmore training data: we train ViT models of various sizes on the public ImageNet-21k dataset\nwhich either match or outperform their counterparts trained on the larger, but not publicly\navailable JFT-300M dataset.\n1 Introduction\nThe Vision Transformer (ViT) (13) has recently emerged as a competitive alternative to convolutional\nneural networks (CNNs) that are ubiquitous across the ﬁeld of computer vision. Without the translational\nequivariance of CNNs, ViT models are generally found to perform best in settings with large amounts of\ntraining data (13) or to require strong AugReg schemes to avoid overﬁtting (39). However, so far there was\nno comprehensive study of the trade-oﬀs between model regularization, data augmentation, training data size\nand compute budget in Vision Transformers.\nIn this work, we ﬁll this knowledge gap by conducting a thorough empirical study. We pre-train a large\ncollection of ViT models (diﬀerent sizes and hybrids with ResNets (18)) on datasets of diﬀerent sizes, while\nat the same time performing carefully designed comparisons across diﬀerent amounts of regularization and\n1We release more than 50000 ViT models trained under diverse settings on various datasets. We believe this to be\na treasure trove for model analysis. Available at https://github.com/google-research/vision_transformer and https:\n//github.com/rwightman/pytorch-image-models. The code for full reproduction of model training is available at https:\n//github.com/google-research/big_vision.\n1\narXiv:2106.10270v2  [cs.CV]  23 Jun 2022\nPublished in Transactions on Machine Learning Research (05/2022)\n1.28M 1.28M+AugReg 13M 13M+AugReg 300M\nPre-training dataset size\n60%\n65%\n70%\n75%\n80%\n85%\n90% ImageNet top-1 accuracy after fine-tuning\nViT-B/32\nViT-B/16\nViT-L/16\nFigure 1: Adding the right amount of regularization and image augmentation can lead to similar gains as\nincreasing the dataset size by an order of magnitude.\ndata augmentation. We then proceed with extensive transfer learning experiments for the resulting models.\nWe focus mainly on gaining insights useful for a practitioner with limited compute and data budgets.\nThe homogeneity of the performed study constitutes one of the key contributions of this paper. For the\nvast majority of works involving Vision Transformers it is not practical to retrain all baselines and proposed\nmethods on equal footing, in particular those trained on larger amounts of data. Furthermore, there are\nnumerous subtle and implicit design choices that cannot be controlled for eﬀectively, such as the precise\nimplementation of complex augmentation schemes, hyper-parameters (e.g. learning rate schedule, weight\ndecay), test-time preprocessing, dataset splits and so forth. Such inconsistencies can result in signiﬁcant\namounts of noise added to the results, quite possibly aﬀecting the ability to draw any conclusions. Hence, all\nmodels on which this work reports have been trained and evaluated in a consistent setup.\nThe insights we draw from our study constitute another important contribution of this paper. In particular,\nwe demonstrate that carefully selected regularization and augmentations roughly correspond (from the\nperspective of model accuracy) to a 10x increase in training data size. However, regardless of whether the\nmodels are trained with more data or better AugRegs, one has to spend roughly the same amount of compute\nto get models attaining similar performance. We further evaluate if there is a diﬀerence between adding data\nor better AugReg when ﬁne-tuning the resulting models on datasets of various categories. Other ﬁndings, such\nas the overall beneﬁcial eﬀect of AugRegs for medium-sized datasets, simply conﬁrm commonly held beliefs.\nFor those ﬁndings, the value of this study lies not in novelty, but rather in conﬁrming these assumptions and\nquantifying their eﬀect in a strictly controlled setting.\nIn addition, we aim to shed light on other aspects of using Vision Transformers in practice such as comparing\ntransfer learning and training from scratch for mid-sized datasets. Finally, we evaluate various compute\nversus performance trade-oﬀs. We discuss all of the aforementioned insights and more in detail in Section 4.\n2 Scope of the study\nWith the ubiquity of modern deep learning (24) in computer vision it has quickly become common practice\nto pre-train models on large datasets once and re-use their parameters as initialization or feature extraction\npart in models trained on a broad variety of other tasks (32, 45).\nIn this setup, there are multiple ways to characterize computational and sample eﬃciency. When simply\nconsidering the overall costs of pre-training and subsequent training or ﬁne-tuning procedures together, the\ncost of pre-training usually dominates, often by orders of magnitude. From the vantage point of a researcher\naiming to improve model architectures or pre-training schemes, the pre-training costs might therefore be most\nrelevant. Most practitioners, however, rarely, if ever perform pre-training on today’s largest datasets but\n2\nPublished in Transactions on Machine Learning Research (05/2022)\ninstead use some of the many publicly available parameter sets. For them the costs of ﬁne-tuning, adaptation\nor training a task-speciﬁc model from scratch would be of most interest.\nYet another valid perspective is that all training costs are eﬀectively negligible since they are amortized\nover the course of the deployment of a model in applications requiring a very large number of invocations\nof inference.\nIn this setup there are diﬀerent viewpoints on computational and data eﬃciency aspects. One approach\nis to look at the overall computational and sample cost of both pre-training and ﬁne-tuning. Normally,\n“pre-training cost” will dominate overall costs. This interpretation is valid in speciﬁc scenarios, especially\nwhen pre-training needs to be done repeatedly or reproduced for academic/industrial purposes. However, in\nthe majority of cases the pre-trained model can be downloaded or, in the worst case, trained once in a while.\nContrary, in these cases, the budget required for adapting this model may become the main bottleneck.\nThus, we pay extra attention to the scenario, where the cost of obtaining a pre-trained model is free or\neﬀectively amortized by future adaptation runs. Instead, we concentrate on time and compute spent on ﬁnding\na good adaptation strategy (or on tuning from scratch training setup), which we call “practitioner’s cost”.\nA more extreme viewpoint is that the training cost is not crucial, and all that matters is eventual inference\ncost of the trained model, “deployment cost”, which will amortize all other costs. This is especially true for\nlarge scale deployments, where a visual model is expected to be used a massive number of times. Overall,\nthere are three major viewpoints on what is considered to be the central cost of training a vision model. In\nthis study we touch on all three of them, but mostly concentrate on “practitioner” and “deployment” costs.\n3 Experimental setup\nIn this section we describe our uniﬁed experimental setup, which is used throughout the paper. We use a single\nJAX/Flax (19, 3) codebase for pre-training and transfer learning using TPUs. Inference speed measurements,\nhowever, were obtained on V100 GPUs (16G) using thetimm PyTorch library (42). All datasets are accessed\nthrough theTensorFlow Datasetslibrary (15), which helps to ensure consistency and reproducibility. More\ndetails of our setup are provided below.\n3.1 Datasets and metrics\nFor pre-training we use two large-scale image datasets: ILSVRC-2012 (ImageNet-1k) and ImageNet-21k.\nImageNet-21kdatasetcontainsapproximately14millionimageswithabout21000distinctobjectcategories( 11,\n22, 30). ImageNet-1k is a subset of ImageNet-21k consisting of about 1.3 million training images and 1000\nobject categories. We make sure to de-duplicate images in ImageNet-21k with respect to the test sets of the\ndownstream tasks as described in (13, 22). Additionally, we used ImageNetV2 (29) for evaluation purposes.\nFor transfer learning evaluation we use 4 popular computer vision datasets from the VTAB benchmark (45):\nCIFAR-100 (25), Oxford IIIT Pets (28) (or Pets37 for short), Resisc45 (6) and Kitti-distance (14). We selected\nthese datasets to cover the standard setting of natural image classiﬁcation (CIFAR-100 and Pets37), as well\nas classiﬁcation of images captured by specialized equipment (Resisc45) and geometric tasks (Kitti-distance).\nIn some cases we also use the full VTAB benchmark (19 datasets) to additionally ensure robustness of\nour ﬁndings.\nFor all datasets we report top-1 classiﬁcation accuracy as our main metric. Hyper-parameters for ﬁne-tuning\nare selected by the result from thevalidation split, and ﬁnal numbers are reported from thetest split. Note\nthat for ImageNet-1k we follow common practice of reporting our main results on the validation set. Thus,\nwe set aside 1% of the training data into aminival split that we use for model selection. Similarly, we use a\nminival split for CIFAR-100 (2% of training split) and Oxford IIIT Pets (10% of training split). For Resisc45,\nwe use only 60% of the training split for training, and another 20% for validation, and 20% for computing\ntest metrics. Kitti-distance ﬁnally comes with an oﬃcial validation and test split that we use for the intended\npurpose. See (45) for details about the VTAB dataset splits.\n3\nPublished in Transactions on Machine Learning Research (05/2022)\nTable 1: Conﬁgurations of ViT models.\nModel Layers Width MLP Heads Params\nViT-Ti (39) 12 192 768 3 5.8M\nViT-S (39) 12 384 1536 6 22.2M\nViT-B (13) 12 768 3072 12 86M\nViT-L (13) 24 1024 4096 16 307M\nTable 2: ResNet+ViT hybrid models.\nModel Resblocks Patch-size Params\nR+Ti/16 [] 8 6.4M\nR26+S/32 [2, 2, 2, 2] 1 36.6M\nR50+L/32 [3, 4, 6, 3] 1 330.0M\n3.2 Models\nThis study focuses mainly on the Vision Transformer (ViT) (13). We use 4 diﬀerent conﬁgurations from\n(13, 39): ViT-Ti, ViT-S, ViT-B and ViT-L, which span a wide range of diﬀerent capacities. The details of\neach conﬁguration are provided in Table 1. We use patch-size 16 for all models, and additionally patch-size\n32 for the ViT-S and ViT-B variants. The only diﬀerence to the original ViT model (13) in our paper is\nthat we drop the hidden layer in the head, as empirically it does not lead to more accurate models and often\nresults in optimization instabilities: when pre-training on ImageNet-1k we include both models with and\nwithout hidden layer, when pre-training on ImageNet-21k we always drop the hidden layer.\nIn addition, we train hybrid models that ﬁrst process images with a ResNet (18) backbone and then feed the\nspatial output to a ViT as the initial patch embeddings. We use a ResNet stem block (7 ×7 convolution +\nbatch normalization + ReLU + max pooling) followed by a variable number of bottleneck blocks (18). We\nuse the notation Rn+{Ti,S,L}/p where n counts the number of convolutions, andp denotes the patch-sizein\nthe input image- for example R+Ti/16 reduces image dimensions by a factor of two in the ResNet stem and\nthen forms patches of size 8 as an input to the ViT, which results in an eﬀective patch-size of 16.\n3.3 Regularization and data augmentations\nTo regularize our models we use robust regularization techniques widely adopted in the computer vision\ncommunity. We apply dropout to intermediate activations of ViT as in (13). Moreover, we use the stochastic\ndepth regularization technique (20) with linearly increasing probability of dropping layers.\nFor data augmentation, we rely on the combination of two recent techniques, namely Mixup (47) and\nRandAugment (7). For Mixup, we vary its parameterα, where 0 corresponds to no Mixup. For RandAugment,\nwe vary the magnitude parameterm, and the number of augmentation layersl. Note that we use the original\nRandAugment implementation in TensorFlow, which diﬀers from re-implementations found, for example, in\ntimm (42).\nWe also try two values for weight decay (27) which we found to work well, since increasing AugReg may need\na decrease in weight decay (2).\nOverall, our sweep contains 28 conﬁgurations, which is a cross-product of the following hyper-parameter choices:\n• Either use no dropout and no stochastic depth (e.g. no regularization) or use dropout with probability\n0.1 and stochastic depth with maximal layer dropping probability of 0.1, thus 2 conﬁguration in total.\n• 7 data augmentation setups for(l,m,α ): none (0,0,0), light1(2,0,0), light2(2,10,0.2), medium1\n(2,15,0.2), medium2(2,15,0.5), strong1(2,20,0.5), strong2(2,20,0.8).\n• Weight decay: 0.1 or 0.03. The weight decay is decoupled following (27), but multiplied by the\nlearning-rate which peaks at 0.001.\n3.4 Pre-training\nWe pre-trained the models with Adam (21), usingβ1 = 0.9 and β2 = 0.999, with a batch size of 4096, and a\ncosine learning rate schedule with a linear warmup (10k steps). To stabilize training, gradients were clipped\nat global norm 1. The images are pre-processed by Inception-style cropping (36) and random horizontal\n4\nPublished in Transactions on Machine Learning Research (05/2022)\n100 102\nTraining time [TPUv2 core-hours]\n10 1\n100\nError\nPet37 (3312 images)\n100 102\nTraining time [TPUv2 core-hours]\n10 1\n100\nResisc45 (31k images)\nB/32, from scratch\nB/16, from scratch\nB/32, transfer+AugReg\nB/16, transfer+AugReg\nB/32, transfer\nB/16, transfer\n100 102 104\nCompute budget [TPUv2 core-hours]\n0%\n20%\n40%\n60%\n80%\n100%Accuracy\nPet37 (3312 images)\n101 103 105\nCompute budget [TPUv2 core-hours]\n0%\n20%\n40%\n60%\n80%\n100% Resisc45 (31k images)\nB/32, from scratch\nB/16, from scratch\nB/32, transfer\nB/16, transfer\nFigure 2:Left: When training small and mid-sized datasets from scratch it is very hard to achieve a test error\nthat can trivially be attained by ﬁne-tuning a model pre-trained on a large dataset like ImageNet-21k. With\nour recommended models (Section 4.5), one can ﬁnd a good solution with very few trials (bordered green\ndots, using recipe from B). Note that AugReg is not helpful when transferring pre-trained models (borderless\ngreen dots). Right: Same data as on the left side (ignoring the borderless green dots), but simulating the\nresults of a random search. For a given compute budget (x-axis), choosing random conﬁgurations within that\nbudget leads to varying ﬁnal performance, depending on choice of hyper parameters (shaded area covers 90%\nfrom 1000 random samples, line corresponds to median).\nﬂipping. On the smaller ImageNet-1k dataset we trained for 300 epochs, and for 30 and 300 epochs on the\nImageNet-21k dataset. Since ImageNet-21k is about 10x larger than ImageNet-1k, this allows us to examine\nthe eﬀects of the increased dataset size also with a roughly constant total compute used for pre-training.\n3.5 Fine-tuning\nWe ﬁne-tune with SGD with a momentum of 0.9 (storing internal state asbfloat16), sweeping over 2-3\nlearning rates and 1-2 training durations per dataset as detailed in Table 4 in the appendix. We used a ﬁxed\nbatch size of 512, gradient clipping at global norm 1 and a cosine decay learning rate schedule with linear\nwarmup. Fine-tuning was done both at the original resolution (224), as well as at a higher resolution (384) as\ndescribed in (40).\n4 Findings\n4.1 Scaling datasets with AugReg and compute\nOne major ﬁnding of our study, which is depicted in Figure 1, is that by judicious use of image augmentations\nand model regularization, one can (pre-)train a model to similar accuracy as by increasing the dataset size by\nabout an order of magnitude. More precisely, our best models trained on AugReg ImageNet-1k (31) perform\nabout equal to the same models pre-trained on the 10x larger plain ImageNet-21k (11) dataset. Similarly, our\nbest models trained on AugReg ImageNet-21k, when compute is also increased (e.g. training run longer),\nmatch or outperform those from (13) which were trained on the plain JFT-300M (35) dataset with 25x\nmore images. Thus, it is possible tomatch these private results with a publicly available dataset, and it is\nimaginable that training longer and with AugReg on JFT-300M might further increase performance.\nOf course, these results cannot hold for arbitrarily small datasets. For instance, according to Table 5 of (44),\ntraining a ResNet50 on only 10% of ImageNet-1k with heavy data augmentation improves results, but does\nnot recover training on the full dataset.\n4.2 Transfer is the better option\nHere, we investigate whether, for reasonably-sized datasets a practitioner might encounter, it is advisable to\ntry training from scratch with AugReg, or whether time and money is better spent transferring pre-trained\n5\nPublished in Transactions on Machine Learning Research (05/2022)\n103104\nInference speed [img/sec]\n85%\n90%\nNatural datasets\n103104\nInference speed [img/sec]\n88%\n90%\n92%\nSpecialized datasets\nINet21k 300ep\nINet21k 30ep\nINet1k 300ep\n103104\nInference speed [img/sec]\n75%\n80%\nStructured datasets\nS/32\nB/32\nR26S\nS/16\nR50L\nB/16\nL/16\nS/32\nB/32\nR26S\nS/16\nR50L\nB/16\nL/16\nS/32\nB/32\nR26S\nS/16\nR50L\nB/16\nL/16\nFigure 3: Pretraining on more data yields more transferable models on average, tested on the VTAB suite (45)\nof 19 tasks across 3 categories.\nmodels that are freely available. The result is that, for most practical purposes, transferring a pre-trained\nmodel is both more cost-eﬃcient and leads to better results.\nWe perform a thorough search for a good training recipe2 for both the small ViT-B/32 and the larger\nViT-B/16 models on two datasets of practical size: Pet37 contains only about 3000 training images and is\nrelatively similar to the ImageNet-1k dataset. Resisc45 contains about 30000 training images and consists of\na very diﬀerent modality of satellite images, which is not well covered by either ImageNet-1k or ImageNet-21k.\nFigure 2 shows the result of this search.\nThe most striking ﬁnding is that, no matter how much training time is spent, for the tiny Pet37 dataset, it\ndoes not seem possible to train ViT models from scratch to reach accuracy anywhere near that of transferred\nmodels. Furthermore, since pre-trained models are freely available for download, the pre-training cost for a\npractitioner is eﬀectively zero, only the compute spent on transfer matters, and thustransferring a pre-trained\nmodel is simultaneously signiﬁcantly cheaper and gives better results.\nFor the larger Resisc45 dataset, this result still holds, although spending two orders of magnitude more\ncompute and performing a heavy search may come close (but not reach) to the accuracy of pre-trained models.\nNotably, this does not account for the “exploration cost”, which is diﬃcult to quantify. For the pre-trained\nmodels, we highlight those which performed best on thepre-training validation set and could be called\nrecommended models(see Section 4.5). We can see that using a recommended model has a high likelihood\nof leading to good results in just a few attempts, while this is not the case for training from-scratch, as\nevidenced by the wide vertical spread of points.\n4.3 More data yields more generic models\nWe investigate the impact of pre-training dataset size by transferring pre-trained models to unseen downstream\ntasks. We evaluate the pre-trained models on VTAB, including 19 diverse tasks (45).\nFigure 3 shows the results on three VTAB categories: natural, specialized and structured. The models are\nsorted by the inference time per step, thus the larger model the slower inference speed. We ﬁrst compare\ntwo models using the same compute budget, with the only diﬀerence being the dataset size of ImageNet-1k\n(1.3M images) and ImageNet-21k (13M images). We pre-train for 300 epochs on ImageNet-1k, and 30 epochs\non ImageNet-21k. Interestingly, the model pre-trained on ImageNet-21k is signiﬁcantly better than the\nImageNet-1k one, across all the three VTAB categories. This is in contrast with the validation performance\non ImageNet-1k (Figure 6), where this diﬀerence does not appear so clearly.\nAs the compute budget keeps growing, we observe consistent improvements on ImageNet-21k dataset with\n10x longer schedule. On a few almost solved tasks, e.g. ﬂowers, the gain is small in absolute numbers. For\n2Not only do we further increase available AugReg settings, but we also sweep over other generally important training\nhyperparameters: learning-rate, weight-decay, and training duration, as described in Appendix A.\n6\nPublished in Transactions on Machine Learning Research (05/2022)\nnone\nlight1\nlight2\nmed1\nmed2\nheavy1\nheavy2\nnone\nlight1\nlight2\nmed1\nmed2\nheavy1\nheavy2\nRTi\nTi/16\nS/32\nS/16\nB/32\nR26S\nB/16\nL/16\nR50L\n69 73 73 72 70 69 68 71 70 67 65 63 62 61\n72 76 75 75 74 72 71 71 72 68 65 63 63 62\n64 71 76 76 76 74 74 70 72 72 71 71 69 68\n71 77 79 81 82 80 80 76 79 80 79 79 77 77\n63 70 73 75 76 75 76 69 74 77 77 78 77 77\n72 76 78 79 80 80 80 75 78 81 82 82 81 81\n70 76 79 79 81 80 80 76 79 81 82 83 82 82\n69 76 77 78 78 76 76 74 78 78 78 79 77 77\n70 75 76 77 77 76 76 75 78 78 78 79 77 77\nNo regularization Regularization 0.1\nImageNet-1k, 300ep\nreg - noreg\n-2\n-4\n-4\n-2\n2\n2\n2\n0\n2\nnone\nlight1\nlight2\nmed1\nmed2\nheavy1\nheavy2\nnone\nlight1\nlight2\nmed1\nmed2\nheavy1\nheavy2\n36 35 33 32 31 30 29 33 31 28 27 26 25 24\n38 37 35 34 33 32 31 34 32 29 27 26 25 25\n40 39 38 37 35 35 34 37 34 33 32 31 30 29\n44 43 43 42 41 40 39 42 40 39 38 37 35 35\n43 42 43 42 41 40 40 42 40 40 38 38 36 36\n45 45 43 43 42 41 41 44 44 42 41 40 40 39\n46 47 47 46 46 45 44 46 45 45 44 43 42 41\n45 48 49 49 49 48 47 48 48 48 47 46 45 45\n45 47 48 48 48 47 47 48 48 48 47 47 45 46\nNo regularization Regularization 0.1\nImageNet-21k, 30ep\nreg - noreg\n-3\n-4\n-3\n-2\n-1\n-1\n-1\n-1\n0\nnone\nlight1\nlight2\nmed1\nmed2\nheavy1\nheavy2\nnone\nlight1\nlight2\nmed1\nmed2\nheavy1\nheavy2\n39 38 37 36 35 34 33 36 34 32 31 30 29 28\n41 41 40 39 37 37 36 38 36 34 33 32 31 29\n43 43 43 42 42 41 40 41 40 39 38 36 36 35\n46 47 47 47 46 45 45 45 45 43 43 42 41 40\n42 46 48 47 47 47 46 45 46 46 45 44 44 43\n46 48 48 47 47 46 46 48 48 46 46 45 44 43\n43 48 50 51 50 50 50 47 49 49 49 48 48 48\n43 46 49 49 51 51 51 46 48 51 51 51 51 51\n42 46 47 49 51 50 51 46 48 51 51 51 51 51\nNo regularization Regularization 0.1\nImageNet-21k, 300ep\nreg - noreg\n-3\n-4\n-2\n-2\n-2\n-0\n-1\n-0\n1\nFigure 4: Validation accuracy (for ImageNet-1k: minival accuracy) when using various amounts of augmenta-\ntion and regularization, highlighting diﬀerences to the unregularized, unaugmented setting. For relatively\nsmall amount of data, almost everything helps. However, when switching to ImageNet-21k while keeping the\ntraining budget ﬁxed, almost everything hurts; only when also increasing compute, does AugReg help again.\nThe single column right of each plot show the diﬀerence between the best setting with regularization and the\nbest setting without, highlighting that regularization typically hurts on ImageNet-21k.\nthe rest of the tasks, the improvements are signiﬁcant compared to the model pre-trained for a short schedule.\nAll the detailed results on VTAB could be found from supplementary section C.\nOverall, we conclude thatmore data yields more generic models, the trend holds across very diverse tasks.\nWe recommend the design choice of usingmore datawith a ﬁxed compute budget.\n4.4 Prefer augmentation to regularization\nIt is not clear a priori what the trade-oﬀs are between data augmentation such as RandAugment and Mixup,\nand model regularization such as Dropout and StochasticDepth. In this section, we aim to discover general\npatterns for these that can be used as rules of thumb when applying Vision Transformers to a new task. In\nFigure 4, we show the upstream validation score obtained for each individual setting, i.e. numbers are not\ncomparable when changing dataset. The colour of a cell encodes its improvement or deterioration in score\nwhen compared to the unregularized, unaugmented setting, i.e. the leftmost column. Augmentation strength\nincreases from left to right, and model “capacity” increases from top to bottom.\nThe ﬁrst observation that becomes visible, is that for the mid-sized ImageNet-1k dataset, any kind of AugReg\nhelps. However, when using the 10x larger ImageNet-21k dataset and keeping compute ﬁxed, i.e. running\nfor 30 epochs, any kind of AugReghurts performance for all but the largest models. It is only when also\nincreasing the computation budget to 300 epochs that AugReg helps more models, although even then, it\ncontinues hurting the smaller ones. Generally speaking, there are signiﬁcantly more cases where adding\naugmentation helps, than where adding regularization helps. More speciﬁcally, the thin columns right of\neach map in Figure 4 shows, for any given model, its best regularized score minus its best unregularized\nscore. This view, which is expanded in Figure 7 in the Appendix, tells us that when using ImageNet-21k,\nregularization almost always hurts.\n4.5 Choosing which pre-trained model to transfer\nAs we show above, when pre-training ViT models, various regularization and data augmentation settings\nresult in models with drastically diﬀerent performance. Then, from the practitioner’s point of view, a natural\nquestion emerges: how to select a model for further adaption for an end application? One way is to run\ndownstream adaptation for all available pre-trained models and then select the best performing model,\nbased on the validation score on the downstream task of interest. This could be quite expensive in practice.\nAlternatively, one can select a single pre-trained model based on the upstream validation accuracy and then\nonly use this model for adaptation, which is much cheaper.\n7\nPublished in Transactions on Machine Learning Research (05/2022)\nINet-1k (v2 val)\nCIFAR-100\nPets\nResisc45\nKitti\nRTi\nTi/16\nS/32\nS/16\nB/32\nR26S\nB/16\nL/16\nR50L\n-0.7 +0.4 +0.5 -1.5 +0.1\n-0.7 +0.0 +0.6 +2.8 +0.4\n-1.3 +1.3 -0.0 +2.1 -0.4\n+0.1 -0.1 -0.5 +0.3 +0.6\n+0.2 +0.0 +0.0 +0.0 +0.0\n+0.0 -0.2 -0.2 +0.8 +0.0\n+0.2 -0.0 +0.3 -3.6 -0.7\n-0.3 +0.5 -0.3 +1.3 -1.5\n-0.3 +0.6 +0.3 +0.6 -0.2\nImageNet-1k\n+0.0\n+0.0\n+0.2\n+1.5\n+6.2\n+0.8\n+3.9\n+1.0\n+3.7\n80% 85% 90% 95%\nMinival accuracy\n78%\n80%\n82%\n84%\n86%ImageNet-1k validation accuracy\n65% 68% 70% 72% 75% 78%\nImageNetV2 accuracy\n78%\n80%\n82%\n84%\n86%\nL/16 (best 100 runs by validation score) S/16 (best 100 runs by validation score)\nFigure 5: Choosing best models.Left: Diﬀerence of ﬁne-tuning test scores between models chosen by best\nvalidation score on pre-training data vs. validation score on ﬁne-tuning data (negative values mean that\nselecting models by pre-training validation deteriorates ﬁne-tuning test metrics).Right: Correlation between\n“minival” validation score vs. ImageNetV2 validation score and oﬃcial ImageNet-1k validation score (that\nserves as a test score in this study). Red circles highlight the best models by validation score, see Section 4.5\nfor an explanation.\nIn this section we analyze the trade-oﬀ between these two strategies. We compare them for a large collection\nof our pre-trained models on 5 diﬀerent datasets. Speciﬁcally, in Figure 5 (left) we highlight the performance\ndiﬀerence between the cheaper strategy of adapting only the best pre-trained model and the more expensive\nstrategy of adaptingall pre-trained models (and then selecting the best).\nThe results are mixed, but generally reﬂect that the cheaper strategy works equally well as the more expensive\nstrategy in the majority of scenarios. Nevertheless, there are a few notable outliers, when it is beneﬁcial to\nadapt all models. Thus, we conclude that selecting a single pre-trained model based on the upstream score is\na cost-eﬀective practical strategy and also use it throughout our paper. However, we also stress that if extra\ncompute resources are available, then in certain cases one can further improve adaptation performance by\nﬁne-tuning additional pre-trained models.\nA note on validation data for the ImageNet-1k dataset.While performing the above analysis, we\nobserved a subtle, but severe issue with models pre-trained on ImageNet-21k and transferred to ImageNet-1k\ndataset. The validation score for these models (especially for large models) is not well correlated with\nobserved test performance, see Figure 5 (right). This is due to the fact that ImageNet-21k data contains\nImageNet-1k training data and we use a “minival” split from the training data for evaluation (see Section 3.1).\nAs a result, large models on long training schedules memorize the data from the training set, which biases\nthe evaluation metric computed in the “minival” evaluation set. To address this issue and enable fair\nhyper-parameter selection, we instead use the independently collected ImageNetV2 data (29) as the validation\nsplit for transferring to ImageNet-1k. As shown in Figure 5 (right), this resolves the issue. We did not observe\nsimilar issues for the other datasets.We recommend that researchers transferring ImageNet-21k models to\nImageNet-1k follow this strategy.\n4.6 Prefer increasing patch-size to shrinking model-size\nOne unexpected outcome of our study is that we trained several models that are roughly equal in terms of\ninference throughput, but vary widely in terms of their quality. Speciﬁcally, Figure 6 (right) shows that models\ncontaining the “Tiny” variants perform signiﬁcantly worse than the similarly fast larger models with “/32”\npatch-size. For a given resolution, the patch-size inﬂuences the amount of tokens on which self-attention is\n8\nPublished in Transactions on Machine Learning Research (05/2022)\n305010030050010003000\nImages/sec/core\n65.0%\n70.0%\n75.0%\n80.0%\n85.0%\n90.0%Imagenet validation accuracy\nViT-S,B,L @224\nViT-S,B,L @384\nhybrid @224\nhybrid @384\nTi/16 @224\nTi/16 @384\ni1k\ni21k_30\ni21k\nThroughput [img/sec]\n70\n75\n80\nB/32\nS/32B/32\nS/32\nTi/16\nR+Ti/16\nTi/16\nR+Ti/16 ImageNet-21k\n10000 3000 1000 300\n65\n70\n75\n80\nB/32S/32\nB/32\nS/32\nTi/16\nR+Ti/16\nTi/16\nR+Ti/16 ImageNet-1k\nImageNet top-1 accuracy [%]\nFigure 6: ImageNet transfer.Left: For every architecture and upstream dataset, we selected the best model\nby upstream validation accuracy. Main ViT-S,B,L models are connected with a solid line to highlight the\ntrend, with the exception of ViT-L models pre-trained on i1k, where the trend breaks down. The same data\nis also shown in Table 3.Right: Focusing on small models, it is evident that using a larger patch-size (/32)\nsigniﬁcantly outperforms making the model thinner (Ti).\nTable 3: ImageNet-1k transfer. Column i1kup evaluates best checkpoint without adaptation, columns i1k300,\ni21k30 and i21k300 (ImageNet-1k 300 epochs and ImageNet-21k 30 and 300 epochs) report numbers after\nﬁne-tuning, which are shown in Figure 6, the “recommended checkpoints” (see Section 4.5) were ﬁne-tuned\nwith two diﬀerent learning rates (see Section B). For the column i21kv2 (ImageNet-21k, 300 epochs), the\nupstream checkpoint was instead chosen by ImageNetV2 validation accuracy. The JFT-300M numbers are\ntaken from (13) (bold numbers indicate our results that are on par or surpass the published JFT-300M results\nwithout AugReg for the same models). Inference speed measurements were computed on an NVIDIA V100\nGPU usingtimm (42), sweeping the batch size for best throughput.\nModel\n224px resolution 384px resolution\nimg/sec i1k up i1k300 i21k30 i21k300 img/sec i1k 300 i21k30 i21k300 i21kv2 JFT300M\nL/16 228 75.72 74.01 82.05 83.98 50 77.21 84.48 85.59 87.08 87.12\nB/16 659 79.84 78.73 80.42 83.96 138 81.63 83.46 85.49 86.15 84.15\nS/16 1508 79.00 77.51 76.04 80.46 300 80.70 80.22 83.73 83.15 -\nR50+L/32 1047 76.84 74.17 80.26 82.74 327 76.71 83.19 85.99 86.21 -\nR26+S/32 1814 79.61 78.20 77.42 80.81 560 81.55 81.11 83.85 83.80 -\nTi/16 3097 72.59 69.56 68.89 73.75 610 74.64 74.20 78.22 77.83 -\nB/32 3597 74.42 71.38 72.24 79.13 955 76.60 78.65 83.59 83.59 80.73\nS/32 8342 72.07 69.19 68.49 73.47 2154 75.65 75.74 79.58 80.01 -\nR+Ti/16 9371 70.13 67.30 65.65 69.69 2426 73.48 71.97 75.40 75.33 -\n9\nPublished in Transactions on Machine Learning Research (05/2022)\nperformed and, thus, is a contributor to model capacity which is not reﬂected by parameter count. Parameter\ncount is reﬂective neither of speed, nor of capacity (10).\n5 Related work\nThe scope of this paper is limited to studying pre-training and transfer learning of Vision Transformer models\nand there already are a number of studies considering similar questions for convolutional neural networks\n(23, 22). Here we hence focus on related work involving ViT models.\nAs ﬁrst proposed in (13), ViT achieved competitive performance only when trained on comparatively large\namounts of training data, with state-of-the-art transfer results using the ImageNet-21k and JFT-300M datasets,\nwith roughly 13M and 300M images, respectively. In stark contrast, (39) focused on tackling overﬁtting\nof ViT when training from scratch on ImageNet-1k by designing strong regularization and augmentation\nschemes. Yet neither work analyzed the eﬀects of stronger augmentation of regularization and augmentation\nin the presence of larger amounts of training data.\nEver since (22) ﬁrst showed good results when pre-training BiT on ImageNet-21k, more architecture works\nhave mentioned using it for select few experiments (13, 38, 37, 8), with (30) arguing more directly for the use\nof ImageNet-21k. However, none of these works thoroughly investigates the combined use of AugReg and\nImageNet-21k and provides conclusions, as we do here.\nAn orthogonal line of work introduces cleverly designed inductive biases in ViT variants or retain some of the\ngeneral architectural parameters of successful convolutional architectures while adding self-attention to them.\n(33) carefully combines a standard convolutional backbone with bottleneck blocks based on self-attention\ninstead of convolutions. In (26, 17, 41, 43) the authors propose hierarchical versions of ViT. (9) suggests a\nvery elegant idea of initializing Vision Transformer, such that it behaves similarly to convolutional neural\nnetwork in the beginning of training.\nYet another way to address overﬁtting and improve transfer performance is to rely on self-supervised learning\nobjectives. (1) pre-trains ViT to reconstruct perturbed image patches. Alternatively, (4) devises a self-\nsupervised training procedure based on the idea from (16), achieving impressive results. We leave the\nsystematic comparison of self-supervised and supervised pre-training to future work.\n6 Discussion\nSocietal Impact. Our experimental study is relatively thorough and used a lot of compute. This could be\ntaken as encouraging anyone who uses ViTs to perform such large studies. On the contrary, our aim is to\nprovide good starting points and oﬀ-the-shelf checkpoints that remove the need for such extensive search in\nfuture work.\nLimitations. In order to be thorough, we restrict the study to the default ViT architecture and neither\ninclude ResNets, which have been well studied over the course of the past years, nor more recent ViT variants.\nWe anticipate though that many of our ﬁndings extend to other ViT-based architectures as well.\n10\nPublished in Transactions on Machine Learning Research (05/2022)\n7 Summary of recommendations\nBelow we summarize three main recommendations based on our study:\n• We recommend to use checkpoints that were pre-trained on more upstream data, and not relying\nonly on ImageNet-1k as a proxy for model quality, since ImageNet-1k validation accuracy is inﬂated\nwhen pre-training on ImageNet-1k, and more varied upstream data yields more widely applicable\nmodels (Figure 3 and Section 4.3).\n• Judiciously applying data augmentation and model regularization makes it possible to train much\nbetter models on a dataset of a given size (Figure 1), and these improvements can be observed both\nwith medium sized datasets like ImageNet-1k, and even with large datasets like ImageNet-21k. But\nthere are no simple rules which AugReg settings to select. The best settings vary a lot depending on\nmodel capacity and training schedule, and one needs to be careful not to apply AugReg to a model\nthat is too small, or when pre-training for too short – otherwise the model quality may deteriorate\n(see Figure 4 for an exhaustive quantitative evaluation and Section 4.4 for further comments on\nregularization vs augmentations).\n• How to select the best upstream model for transfer on your own task? Aside from always using\nImageNet-21k checkpoints, we recommend to select the model with the best upstream validation\nperformance (Section 4.5, table with paths in our Github repository3). As we show in Figure 5,\nthis choice is generally optimal for a wide range of tasks. If the user has additional computational\nresources available to ﬁne-tune all checkpoints, they may get slightly better results in some scenarios,\nbut also need to be careful with respect to ImageNet-1k and ImageNet-21k data overlap when it\ncomes to model selection (Figure 5, right).\n8 Conclusion\nWe conduct the ﬁrst systematic, large scale study of the interplay between regularization, data augmentation,\nmodel size, and training data size when pre-training Vision Transformers, including their respective eﬀects on\nthe compute budget needed to achieve a certain level of performance. We also evaluate pre-trained models\nthrough the lens of transfer learning. As a result, we characterize a quite complex landscape of training\nsettings for pre-training Vision Transformers across diﬀerent model sizes. Our experiments yield a number\nof surprising insights around the impact of various techniques and the situations when augmentation and\nregularization are beneﬁcial and when not.\nWe also perform an in-depth analysis of the transfer learning setting for Vision Transformers. We conclude\nthat across a wide range of datasets, even if the downstream data of interest appears to only be weakly\nrelated to the data used for pre-training, transfer learning remains the best available option. Our analysis\nalso suggests that among similarly performing pre-trained models, for transfer learning a model with more\ntraining data should likely be preferred over one with more data augmentation.\nWe hope that our study will help guide future research on Vision Transformers and will be a useful source of\neﬀective training settings for practitioners seeking to optimize their ﬁnal model performance in the light of a\ngiven computational budget.\nAcknowledgements We thank Alexey Dosovitskiy, Neil Houlsby, and Ting Chen for insightful feedback;\nthe Google Brain team at large for providing a supportive research environment.\nReferences\n[1] Sara Atito, Muhammad Awais, and Josef Kittler. Sit: Self-supervised vision transformer.arXiv:2104.03602, 2021.\n10\n[2] Irwan Bello, William Fedus, Xianzhi Du, Ekin D. Cubuk, Aravind Srinivas, Tsung-Yi Lin, Jonathon Shlens, and\nBarret Zoph. Revisiting resnets: Improved training and scaling strategies.arXiv:2103.07579, 2021. 4\n3https://github.com/google-research/vision_transformer\n11\nPublished in Transactions on Machine Learning Research (05/2022)\n[3] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George\nNecula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang.JAX: composable transforma-\ntions of Python+NumPy programs, 2018. 3\n[4] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin.\nEmerging properties in self-supervised vision transformers.arXiv:2104.14294, 2021. 10\n[5] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12M: Pushing web-scale image-text\npre-training to recognize long-tail visual concepts. InCVPR, 2021. 16\n[6] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classiﬁcation: Benchmark and state of\nthe art. Proceedings of the IEEE, 105(10):1865–1883, 2017. 3\n[7] Ekin D. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V. Le. RandAugment: Practical automated data\naugmentation with a reduced search space. InCVPR Workshops, 2020. 4\n[8] Zihang Dai, Hanxiao Liu, Quoc V. Le, and Mingxing Tan. Coatnet: Marrying convolution and attention for all\ndata sizes, 2021. 10\n[9] Stéphane d’Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio Biroli, and Levent Sagun. Convit:\nImproving vision transformers with soft convolutional inductive biases.arXiv:2103.10697, 2021. 10\n[10] Mostafa Dehghani, Anurag Arnab, Lucas Beyer, Ashish Vaswani, and Yi Tay. The eﬃciency misnomer.CoRR,\nabs/2110.12894, 2021. 10\n[11] J. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database.\nIn CVPR, 2009. 3, 5\n[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional\ntransformers for language understanding. InNAACL-HLT, 2019. 16\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An\nimage is worth 16x16 words: Transformers for image recognition at scale.arXiv:2010.11929, 2020. 1, 3, 4, 5, 9, 10\n[14] A Geiger, P Lenz, C Stiller, and R Urtasun. Vision meets robotics: The kitti dataset.The International Journal\nof Robotics Research, 32(11):1231–1237, 2013. 3\n[15] Google. TensorFlow Datasets, a collection of ready-to-use datasets.https://www.tensorflow.org/datasets. 3\n[16] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl\nDoersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own\nlatent: A new approach to self-supervised learning.arXiv:2006.07733, 2020. 10\n[17] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer.\narXiv:2103.00112, 2021. 10\n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In\nCVPR, 2016. 1, 4\n[19] Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas Steiner, and\nMarc van Zee.Flax: A neural network library and ecosystem for JAX, 2020. 3\n[20] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth.\nIn ECCV, 2016. 4\n[21] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. InICLR, 2015. 4\n[22] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby.\nBig transfer (BiT): General visual representation learning. InECCV, 2020. 3, 10\n[23] Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do better imagenet models transfer better? InCVPR, 2019.\n10\n[24] Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E. Hinton. Imagenet classiﬁcation with deep convolutional neural\nnetworks. In Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, Léon Bottou, and Kilian Q.\nWeinberger, editors,Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural\nInformation Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada,\nUnited States, pages 1106–1114, 2012. 2\n[25] Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. Imagenet classiﬁcation with deep convolutional neural\nnetworks. Advances in neural information processing systems, 25, 2012. 3\n[26] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using shifted windows.arXiv:2103.14030, 2021. 10\n[27] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. InInternational Conference on\nLearning Representations, 2019. 4, 14\n[28] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. InCVPR, 2012. 3\n[29] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classiﬁers generalize to\nimagenet? In ICML, 2019. 3, 8\n[30] Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for the masses.\narXiv:2104.10972, 2021. 3, 10\n[31] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej\nKarpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge.IIJCV,\n12\nPublished in Transactions on Machine Learning Research (05/2022)\n2015. 5\n[32] Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. Cnn features oﬀ-the-shelf: An\nastounding baseline for recognition. InProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) Workshops, June 2014. 2\n[33] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani. Bottleneck\ntransformers for visual recognition.arXiv:2101.11605, 2021. 10\n[34] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for semantic\nsegmentation. arXiv:2105.05633, 2021. 16\n[35] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable eﬀectiveness of\ndata in deep learning era. InProceedings of the IEEE international conference on computer vision, pages 843–852,\n2017. 5\n[36] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.\nGoing deeper with convolutions. InCVPR, 2015. 4\n[37] Mingxing Tan and Quoc V. Le. Eﬃcientnetv2: Smaller models and faster training.CoRR, abs/2104.00298, 2021.\n10\n[38] Ilya O. Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica\nYung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and Alexey Dosovitskiy. Mlp-mixer: An\nall-mlp architecture for vision.CoRR, abs/2105.01601, 2021. 10\n[39] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou.\nTraining data-eﬃcient image transformers & distillation through attention.arXiv:2012.12877, 2020. 1, 4, 10\n[40] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution discrepancy.\nIn NeurIPS, 2019. 5\n[41] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.\nPyramid vision transformer: A versatile backbone for dense prediction without convolutions.arXiv:2102.12122,\n2021. 10\n[42] Ross Wightman. Pytorch image models (timm): Vit training details. https://github.com/rwightman/\npytorch-image-models/issues/252#issuecomment-713838112 , 2013. 3, 4, 9\n[43] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing\nconvolutions to vision transformers.arXiv:2103.15808, 2021. 10\n[44] Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. Unsupervised data augmentation for consistency\ntraining. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors,Advances in Neural\nInformation Processing Systems, 2020. 5\n[45] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip\nDjolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, Lucas Beyer, Olivier Bachem, Michael\nTschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and Neil Houlsby. A large-scale study of\nrepresentation learning with the visual task adaptation benchmark, 2020. 2, 3, 6, 15\n[46] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas\nBeyer. Lit: Zero-shot transfer with locked-image text tuning.CVPR, 2022. 16\n[47] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk\nminimization. In ICLR, 2018. 4\n13\nPublished in Transactions on Machine Learning Research (05/2022)\nA From-scratch training details\nWe present from-scratch training details for B/32 and B/16 models, on both Resisc45 and Pets37 datasets.\nWe perform a grid search over the following parameters:\n• B/32 on Pets37\n– Epochs: {1k,3k,10k,30k,300k}\n– Learning rates: {1e−4,3e−4,1e−3,3e−3}\n– Weight decays4: {1e−5,3e−5,1e−4,3e−4}\n• B/16 on Pets37\n– Epochs: {1k,3k,10k}\n– Learning rates: {3e−4,1e−3}\n– Weight decays:{3e−5,1e−4}\n• B/32 on Resisc45\n– Epochs: {75,250,750,2.5k,7.5k,25k}\n– Learning rates: {1e−4,3e−4,1e−3,3e−3}\n– Weight decays:{1e−5,3e−5,1e−4,3e−4}\n• B/16 on Resisc45\n– Epochs: {75,250,750,2.5k,7.5k}\n– Learning rates: {1e−3}\n– Weight decays:{1e−4,3e−4}\nAll these from-scratch runs sweep over dropout rate and stochastic depth in range\n{(0.0,0.0),(0.1,0.1),(0.2,0.2)}, and data augmentation(l,m,α ) in range{(0,0,0), (2,10,0.2), (2,15,0.2),\n(2,15,0.5), (2,20,0.5), (2,20,0.8), (4,15,0.5), (4,20,0.8) }.\nFor the deﬁnition of(l,m,α ) refer to Section 3.3\nB Finetune details\nIn Table 4, we show the hyperparameter sweep range for ﬁnetune jobs. We use the same ﬁnetune sweep for\nall the pre-trained models in this paper.\nTable 4: Finetune details for the pre-trained models.\nDataset Learning rate Total, warmup steps\nImageNet-1k {0.01, 0.03} { (20k, 500)}\nPets37 {1e-3, 3e-3, 0.01} { (500, 100), (2.5k, 200)}\nKitti-distance {1e-3, 3e-3, 0.01} { (500, 100), (2.5k, 200)}\nCIFAR-100 {1e-3, 3e-3, 0.01} { (2.5k, 200), (10k, 500)}\nResisc45 {1e-3, 3e-3, 0.01} { (2.5k, 200), (10k, 500)}\n4 As opposed to 3.3 where we specify weight decay values as typically deﬁned in common frameworks, here the values are\n“decoupled” following (27) that is better suited for sweeps; multiplying weight decay by the base learning-rate recovers the\n“coupled” value as used elsewhere.\n14\nPublished in Transactions on Machine Learning Research (05/2022)\nTable 5: Detailed VTAB results, including the “Mean” accuracy shown in Figure 3. We show datasets under\nnatural, specialized, structured groups, following (45).\nCaltech101\nCIFAR-100\nDTD\nFlowers102\nPets\nSun397\nSVHN\nMean\nCamelyon\nEuroSAT\nResisc45\nRetinopathy\nMean\nClevr-Count\nClevr-Dist\nDMLab\ndSpr-Loc\ndSpr-Ori\nKITTI-Dist\nsNORB-Azim\nsNORB-Elev\nMean\nImageNet-1k (300ep)\nR+Ti/16 91.6 81.9 68.0 94.0 91.9 70.6 95.6 84.8 85.2 98.4 94.8 80.4 89.7 96.1 89.8 67.4 99.9 86.9 81.9 25.1 46.3 74.2\nS/32 92.7 86.4 70.7 93.6 91.2 72.9 95.8 86.2 83.6 98.6 95.5 79.6 89.3 94.2 88.4 65.8 99.9 86.1 80.7 24.9 68.2 76.0\nB/32 92.6 87.6 72.7 94.4 92.2 73.8 95.8 87.0 82.7 98.6 94.9 79.8 89.0 94.0 89.6 66.1 99.8 84.7 80.3 24.7 62.4 75.2\nTi/16 92.7 84.0 68.9 93.8 92.5 72.0 96.1 85.7 83.7 98.7 95.6 81.6 89.9 98.0 91.9 68.5 99.7 83.2 82.0 26.5 65.9 77.0\nR26+S/32 90.2 86.2 74.0 95.5 94.3 74.5 95.6 87.2 84.5 98.6 96.0 83.4 90.6 99.7 91.6 73.3 100 84.8 84.5 28.2 51.3 76.7\nS/16 93.1 86.9 72.8 95.7 93.8 74.3 96.2 87.5 84.1 98.7 95.9 82.7 90.3 98.7 91.5 69.8 100 84.3 79.6 27.3 58.0 76.1\nR50+L/32 90.7 88.1 73.7 95.4 93.5 75.6 95.9 87.6 85.8 98.4 95.4 83.1 90.7 99.8 90.4 71.1 100 87.5 82.4 23.5 53.0 76.0\nB/16 93.0 87.8 72.4 96.0 94.5 75.3 96.1 87.9 85.1 98.9 95.7 82.5 90.5 98.1 91.8 69.5 99.9 84.5 84.0 25.9 53.9 76.0\nL/16 91.0 86.2 69.5 91.4 93.0 75.3 94.9 85.9 81.0 98.7 93.8 81.6 88.8 94.3 88.3 63.9 98.5 85.1 81.3 25.3 51.2 73.5\nImageNet-21k (30ep)\nR+Ti/16 92.4 82.7 69.5 98.7 88.0 72.4 95.1 85.6 83.6 98.8 94.9 80.7 89.5 95.7 90.2 66.6 99.9 87.0 80.3 24.4 47.0 73.9\nS/32 92.7 88.5 72.4 98.9 90.5 75.4 95.4 87.7 83.5 98.7 95.0 79.5 89.2 94.5 89.8 64.4 99.8 87.9 81.2 24.9 57.7 75.0\nB/32 93.6 90.5 74.5 99.1 91.9 77.8 95.7 89.0 83.5 98.8 95.1 78.8 89.1 93.6 90.1 62.9 99.8 89.0 78.3 24.1 55.9 74.2\nTi/16 93.3 85.5 72.6 99.0 90.0 74.3 95.1 87.1 85.5 98.8 95.5 81.6 90.4 97.7 91.7 67.4 99.9 83.8 81.2 26.3 55.1 75.4\nR26+S/32 94.7 89.9 76.5 99.5 93.0 79.1 95.9 89.8 86.3 98.6 96.1 83.1 91.0 99.7 92.0 73.4 100 88.7 84.8 26.2 53.3 77.3\nS/16 94.3 89.4 76.2 99.3 92.3 78.1 95.7 89.3 84.5 98.8 96.3 81.7 90.3 98.4 91.5 68.3 100 86.5 82.8 25.9 52.7 75.8\nR50+L/32 95.4 92.0 79.1 99.6 94.3 81.7 96.0 91.1 85.9 98.7 95.9 82.9 90.9 99.9 90.9 72.9 100 86.3 82.6 25.4 57.4 76.9\nB/16 95.1 91.6 77.9 99.6 94.2 80.9 96.3 90.8 84.8 99.0 96.1 82.4 90.6 98.9 90.9 72.1 100 88.3 83.5 26.6 69.6 78.7\nL/16 95.7 93.4 79.5 99.6 94.6 82.3 96.7 91.7 88.4 98.9 96.5 81.8 91.4 99.3 91.8 72.1 100 88.5 83.7 25.0 62.9 77.9\nImageNet-21k (300ep)\nR+Ti/16 93.2 85.3 71.5 99.0 90.3 74.7 95.2 87.0 85.2 98.3 95.3 81.3 90.0 95.5 90.5 67.4 99.9 87.4 78.2 24.5 45.2 73.6\nS/32 93.2 89.7 75.3 99.2 92.0 78.1 96.1 89.1 84.0 98.5 95.4 80.6 89.6 96.9 88.7 68.1 100 91.0 79.6 26.2 55.0 75.7\nB/32 95.2 92.3 77.2 99.5 92.8 81.2 96.6 90.7 87.0 98.8 96.0 81.3 90.8 97.7 89.8 70.5 100 92.3 82.7 25.9 83.1 80.2\nTi/16 93.7 87.2 73.1 99.2 91.0 77.3 95.7 88.2 86.0 98.5 95.8 81.9 90.6 98.3 89.7 70.8 100 86.0 82.6 26.8 49.9 75.5\nR26+S/32 94.8 90.9 78.9 99.5 94.1 81.3 96.7 90.9 87.5 98.7 96.4 84.2 91.7 99.9 92.4 77.0 100 87.1 83.4 28.6 56.0 78.1\nS/16 95.2 90.8 77.8 99.6 93.2 80.6 96.6 90.5 86.7 98.8 96.4 82.9 91.2 99.1 89.8 73.9 100 87.6 85.1 26.8 61.1 77.9\nR50+L/32 95.7 93.9 81.6 99.5 94.9 83.6 97.1 92.3 85.8 98.7 96.7 84.2 91.3 100 92.0 76.8 100 87.2 85.2 26.8 61.8 78.7\nB/16 96.0 93.2 79.1 99.6 94.7 83.0 97.0 91.8 87.4 98.7 96.8 83.5 91.6 99.7 89.0 76.0 100 86.7 85.7 28.3 68.2 79.2\nL/16 95.5 94.1 80.3 99.6 95.0 83.4 97.4 92.2 86.4 99.0 96.6 83.3 91.3 99.8 91.7 75.6 100 90.4 84.7 27.5 76.5 80.8\nC VTAB results\nIn Table 5, we show all the results in percentage for all the models on the full VTAB. We report VTAB\nscore only for the best pre-trained models, selected by their upstream validation accuracy (“recommended\ncheckpoints”, see Section 4.5). For VTAB tasks, we sweep over 8 hyper parameters, include four learning\nrates {0.001,0.003,0.01,0.03}and two schedules{500,2500}steps. The best run was selected on VTAB\nvalidation split.\n15\nPublished in Transactions on Machine Learning Research (05/2022)\nnone\nlight1\nlight2\nmedium1\nmedium2\nheavy1\nheavy2\nR+Ti/16\nTi/16\nS/32\nS/16\nB/32\nR26+S/32\nB/16\nL/16\nR50+L/32\n1 -3 -6 -6 -7 -7 -7\n-1 -4 -7 -10 -11 -9 -9\n6 1 -4 -5 -5 -5 -6\n6 3 1 -2 -3 -3 -3\n6 4 4 2 2 2 1\n4 2 3 3 2 1 1\n6 3 2 3 2 2 2\n5 3 1 1 0 1 1\n6 3 1 1 2 1 1\nImageNet-1k, 300ep\nnone\nlight1\nlight2\nmedium1\nmedium2\nheavy1\nheavy2\n-3 -4 -4 -5 -4 -5 -5\n-4 -5 -6 -6 -6 -7 -6\n-3 -5 -4 -5 -5 -5 -5\n-2 -3 -3 -4 -4 -5 -5\n-1 -2 -3 -4 -4 -4 -3\n-1 -1 -1 -2 -2 -2 -2\n-0 -1 -2 -3 -3 -3 -3\n3 0 -1 -2 -3 -2 -2\n3 1 -0 -1 -1 -2 -1\nImageNet-21k, 30ep\nnone\nlight1\nlight2\nmedium1\nmedium2\nheavy1\nheavy2\n-3 -4 -5 -5 -5 -5 -5\n-4 -4 -6 -6 -6 -6 -7\n-2 -4 -5 -5 -5 -5 -5\n-1 -2 -4 -4 -4 -4 -4\n3 -0 -2 -2 -3 -3 -3\n1 -0 -2 -2 -2 -2 -2\n4 1 -1 -1 -2 -2 -3\n3 3 2 2 -0 -1 -1\n3 2 3 2 -0 1 0\nImageNet-21k, 300ep\nFigure 7: The improvement or deterioration in validation accuracy when using or not using regularization\n(e.g. dropout and stochastic depth) – positive values when regularization improves accuracy for a given\nmodel/augmentation. For absolute values see Figure 4.\nD The beneﬁt and harm of regularization\nIn Figure 7, we show the gain (green, positive numbers) or loss (red, negative numbers) in accuracy when\nadding regularization to the model by means of dropout and stochastic depth. We did verify in earlier\nexperiments that combining both with (peak) drop probability 0.1 is indeed the best setting. What this\nshows, is that model regularization mainly helps larger models, and only when trained for long. Speciﬁcally,\nfor ImageNet-21 pre-training, it hurts all but the largest of models across the board.\nE Using recommended checkpoints for other computer vision tasks\nOne limitation of our study is that it focuses mainly on the classiﬁcation task. However, computer vision is a\nmuch broader ﬁeld, and backbones need to excel at many tasks. While expanding the full study to many\nmore tasks such as detection, segmentation, tracking, and others would be prohibitive, here we take a peek at\none further task: multi-modal image-text retrieval.\nA detailed analysis of this question is beyond the scope of this study, but we evaluated ourrecommended\n(see Section 4.5) B/32 checkpoint pre-trained on ImageNet-21k in a contrastive training setup with a locked\nimage tower (46). We initialize the text tower with a BERT-Base (12) checkpoint and train for 20 epochs on\nCC12M (5). The results in Table 6 indicate that the upstream validation accuracy is a good predictor for\nzero-shot classiﬁcation. Moreover, the representations produced by such a model yield similarly better results\nfor image-text retrieval, when compared to models that do not have the ideal amount of AugReg applied. We\nhope the community will adopt our backbones for other tasks, as already done by (34).\nTable 6: Comparing ourrecommended(see Section 4.5) B/32 checkpoint with models that apply too little\nor too much AugReg. The ﬁnal validation accuracy from the ImageNet-21k pre-training is the same that\nis reported in Figure 4. The other columns are ImageNet-1K zero-shot accuracy, and image-text retrieval\naccuracy on diﬀerent datasets, after contrastively training as described in (46).\nAugReg I21k Val I1k 0shot Coco I2T Coco T2I Flickr I2T Flickr T2I\nnone/0.0 41.6 54.9 33.4 20.1 58.1 39.9\nheavy2/0.1 43.5 57.3 39.1 24.4 62.1 44.6\nRecommended 47.7 60.6 41.1 25.5 65.9 46.9\n16",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6739255785942078
    },
    {
      "name": "Regularization (linguistics)",
      "score": 0.6494566202163696
    },
    {
      "name": "Computer science",
      "score": 0.6314185261726379
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5496755242347717
    },
    {
      "name": "Segmentation",
      "score": 0.5475890636444092
    },
    {
      "name": "Machine learning",
      "score": 0.5096187591552734
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5080184936523438
    },
    {
      "name": "Training set",
      "score": 0.4699680507183075
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.40348994731903076
    },
    {
      "name": "Engineering",
      "score": 0.1327422857284546
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ]
}