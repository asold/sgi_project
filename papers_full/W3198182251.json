{
  "title": "Transformers for Low-Resource Languages: Is Féidir Linn!",
  "url": "https://openalex.org/W3198182251",
  "year": 2024,
  "authors": [
    {
      "id": null,
      "name": "Lankford, Séamus",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2887988028",
      "name": "Afli, Haithem",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2548515432",
      "name": "Way Andy",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2903193068",
    "https://openalex.org/W2097998348",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2916548775",
    "https://openalex.org/W2964199361",
    "https://openalex.org/W2478454054",
    "https://openalex.org/W2978943549",
    "https://openalex.org/W2775233965",
    "https://openalex.org/W3113488190",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W3016151632",
    "https://openalex.org/W1702700969",
    "https://openalex.org/W2250342921",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2981540061",
    "https://openalex.org/W1872443190",
    "https://openalex.org/W2760656271",
    "https://openalex.org/W46679369",
    "https://openalex.org/W2884381860",
    "https://openalex.org/W2963212250",
    "https://openalex.org/W3103182178",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2909389168",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2129749171",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2149327368",
    "https://openalex.org/W2963979492"
  ],
  "abstract": "The Transformer model is the state-of-the-art in Machine Translation. However, in general, neural translation models often under perform on language pairs with insufficient training data. As a consequence, relatively few experiments have been carried out using this architecture on low-resource language pairs. In this study, hyperparameter optimization of Transformer models in translating the low-resource English-Irish language pair is evaluated. We demonstrate that choosing appropriate parameters leads to considerable performance improvements. Most importantly, the correct choice of subword model is shown to be the biggest driver of translation performance. SentencePiece models using both unigram and BPE approaches were appraised. Variations on model architectures included modifying the number of layers, testing various regularisation techniques and evaluating the optimal number of heads for attention. A generic 55k DGT corpus and an in-domain 88k public admin corpus were used for evaluation. A Transformer optimized model demonstrated a BLEU score improvement of 7.8 points when compared with a baseline RNN model. Improvements were observed across a range of metrics, including TER, indicating a substantially reduced post editing effort for Transformer optimized models with 16k BPE subword models. Bench-marked against Google Translate, our translation engines demonstrated significant improvements. The question of whether or not Transformers can be used effectively in a low-resource setting of English-Irish translation has been addressed. Is féidir linn - yes we can.",
  "full_text": "Transformers for Low-Resource Languages:\nIs F´eidir Linn!\nS´eamus Lankford seamus.lankford@adaptcentre.ie\nADAPT Centre, Department of Computing, Dublin City University, Dublin, Ireland.\nHaithem Afli haithem.afli@adaptcentre.ie\nADAPT Centre, Department of Computer Science, Munster Technological University, Ireland.\nAndy Way andy.way@adaptcentre.ie\nADAPT Centre, School of Computing, Dublin City University, Dublin, Ireland.\nAbstract\nThe Transformer model is the state-of-the-art in Machine Translation. However, in general,\nneural translation models often under perform on language pairs with insufficient training data.\nAs a consequence, relatively few experiments have been carried out using this architecture\non low-resource language pairs. In this study, hyperparameter optimization of Transformer\nmodels in translating the low-resource English-Irish language pair is evaluated. We demon-\nstrate that choosing appropriate parameters leads to considerable performance improvements.\nMost importantly, the correct choice of subword model is shown to be the biggest driver of\ntranslation performance. SentencePiece models using both unigram and BPE approaches were\nappraised. Variations on model architectures included modifying the number of layers, testing\nvarious regularisation techniques and evaluating the optimal number of heads for attention. A\ngeneric 55k DGT corpus and an in-domain 88k public admin corpus were used for evalua-\ntion. A Transformer optimized model demonstrated a BLEU score improvement of 7.8 points\nwhen compared with a baseline RNN model. Improvements were observed across a range of\nmetrics, including TER, indicating a substantially reduced post editing effort for Transformer\noptimized models with 16k BPE subword models. Bench-marked against Google Translate,\nour translation engines demonstrated significant improvements. The question of whether or\nnot Transformers can be used effectively in a low-resource setting of English-Irish translation\nhas been addressed. Is f´eidir linn - yes we can.\n1 Introduction\nThe advent of Neural Machine Translation (NMT) has heralded an era of high-quality trans-\nlations. However, these improvements have not been manifested in the translation of all lan-\nguages. Large datasets are a prerequisite for high quality NMT. This works well in the context\nof well-resourced languages where there is an abundance of data. In the context of low-resource\nlanguages which suffer from a sparsity of data, alternative approaches must be adopted.\nAn important part of this research involves developing applications and models to address\nthe challenges of low-resource language technology. Such technology incorporates methods to\naddress the data scarcity affecting deep learning for digital engagement of low-resource lan-\nguages.\nIt has been shown that an out-of-the-box NMT system, trained on English-Irish data,\nachieves a lower translation quality compared with using a tailored SMT system (Dowling et\narXiv:2403.01985v1  [cs.CL]  4 Mar 2024\nal, 2018). It is in this context that further research is required in the development of NMT for\nlow-resource languages and the Irish language in particular.\nMost research on choosing subword models has focused on high resource languages (Ding\net al., 2019; Gowda and May, 2020). In the context of developing models for English to Irish\ntranslation, there are no clear recommendations on the choice of subword model types. One of\nthe objectives in this study is to identify which type of subword model performs best in this low\nresource scenario.\n2 Background\nNative speakers of low-resource languages are often excluded from useful content since, more\noften than not, online content is not available to them in their language of choice. Such a\ndigital divide and the resulting social exclusion experienced by second language speakers, such\nas refugees living in developed countries, has been well documented in the research literature\n(MacFarlane et al., 2008; Alam and Imran, 2015).\nResearch on Machine Translation (MT) in low-resource scenarios directly addresses this\nchallenge of exclusion via pivot languages (Liu et al., 2018), and indirectly, via domain adap-\ntation of models (Ghifary et al., 2016). Breakthrough performance improvements in the area of\nMT have been achieved through research efforts focusing on NMT (Bahdanau et al., 2014; Cho\net al., 2014). Consequently, state-of-the-art (SOA) performance has been attained on multiple\nlanguage pairs (Bojar et al., 2017, 2018).\n2.1 Irish Language\nThe Irish language is a primary example of such a low-resource language that will benefit from\nthis research. NMT involving Transformer model development will improve the performance\nin specific domains of low-resource languages. Such research will address the end of the Irish\nlanguage derogation in the European Commission in 2021 1 (Way, 2020) helping to deliver\nparity in support for Irish in online digital engagement.\n2.2 Hyperparameter Optimization\nHyperparameters are employed in order to customize machine learning models such as transla-\ntion models. It has been shown that machine learning performance may be improved through\nhyperparameter optimization (HPO) rather than just using default settings (Sanders and Giraud-\nCarrier, 2017).\nThe principle methods of HPO are Grid Search (Montgomery, 2017) and Random\nSearch (Bergstra and Bengio, 2012)]. Grid search is an exhaustive technique which evalu-\nates all parameter permutations. However, as the number of features grows, the amount of data\npermutations grows exponentially making optimization expensive in the context of developing\nlong running translation models.\nAn effective, and less computationally intensive, alternative is to use random search which\nsamples random configurations.\n2.2.1 Recurrent Neural Networks\nRecurrent neural networks are often used for the tasks of natural language processing, speech\nrecognition and MT. RNN models enable previous outputs to be used as inputs while having\nhidden states. In the context of MT, such neural networks were ideal due to their ability to pro-\ncess inputs of any length. Furthermore, the model sizes do not necessarily increase with the size\nof its input. Commonly used variants of RNN include Bidirectional (BRNN) and Deep (DRNN)\n1amtaweb.org/wp-content/uploads/2020/11/MT-in-EU-Overview-with-V oiceover-Andy-Way-KEYNOTE-K1.pdf\nHyperparameter Values\nLearning rate 0.1, 0.01, 0.001, 2\nBatch size 1024, 2048, 4096, 8192\nAttention heads 2, 4, 8\nNumber of layers 5, 6\nFeed-forward dimension 2048\nEmbedding dimension 128, 256, 512\nLabel smoothing 0.1, 0.3\nDropout 0.1, 0.3\nAttention dropout 0.1\nAverage Decay 0, 0.0001\nTable 1: Hyperparameter Optimization for Transformer models. Optimal parameters are high-\nlighted in bold. The highest performing model trained on the 55k DGT corpus uses 2 attention\nheads whereas the best model trained with the larger 88k PA dataset uses 8 attention heads.\narchitectures. However, the problem of vanishing gradients coupled with the development of\nattention-based algorithms often leads to Transformer models performing better than RNNs.\n2.2.2 Transformer\nThe greatest improvements have been demonstrated when either the RNN or the CNN archi-\ntecture is abandoned completely and replaced with an attention mechanism creating a much\nsimpler and faster architecture known as Transformer (Vaswani et al., 2017). Transformer\nmodels use attention to focus on previously generated tokens. The approach allows models\nto develop a long memory which is particularly useful in the domain of language translation.\nPerformance improvements to both RNN and CNN approaches may be achieved through the\nintroduction of such attention layers in the translation architecture.\nExperiments in MT tasks show such models are better in quality due to greater paralleliza-\ntion while requiring significantly less time to train.\n2.3 Subword Models\nTranslation, by its nature, requires an open vocabulary and the use of subword models aims\nto address the fixed vocabulary problem associated with NMT. Rare and unknown words are\nencoded as sequences of subword units. By adapting the original Byte Pair Encoding (BPE)\nalgorithm (Gage, 1994), the use of BPE submodels can improve translation performance (Sen-\nnrich et al., 2015; Kudo, 2018).\nDesigned for NMT, SentencePiece, is a language-independent subword tokenizer that pro-\nvides an open-source C++ and a Python implementation for subword units. An attractive feature\nof the tokenizer is that SentencePiece trains subword models directly from raw sentences (Kudo\nand Richardson, 2018).\n2.3.1 Byte Pair Encoding compared with Unigram\nBPE and unigram language models are similar in that both encode text using fewer bits but\neach uses a different data compression principle (dictionary vs. entropy). In principle, we\nwould expect the same benefits with the unigram language model as with BPE. However, un-\nigram models are often more flexible since they are probabilistic models that output multiple\nsegmentations with their probabilities.\nFigure 1: Proposed Approach\n3 Proposed Approach\nHPO of RNN models in low-resource settings has previously demonstrated considerable per-\nformance improvements. The extent to which such optimization techniques may be applied to\nTransformer models in similar low-resource scenarios is evaluated as part of this study. Evalu-\nations included modifying the number of attention heads, the number of layers and experiment-\ning with regularization techniques such as dropout and label smoothing. Most importantly, the\nchoice of subword model type and the vocabulary size are evaluated.\nIn order to test the effectiveness of our approaches, optimization was carried out on two\nEnglish-Irish parallel datasets: a general corpus of 52k lines from the Directorate General for\nTranslation (DGT) and an in-domain corpus of 88k lines of Public Administration (PA) data.\nWith DGT, the test set used 1.3k lines and the development set comprised of 2.6k lines. In\nthe case of the PA dataset, there were 1.5k lines of test data and 3k lines of validation. All\nexperiments involved concatenating source and target corpora to create a shared vocabulary\nand a shared SentencePiece subword model. The impact of using separate source and target\nsubword models was not explored.\nThe approach adopted is illustrated in Figure 1. Two baseline architectures, RNN and\nTransformer, are evaluated. On evaluating the hyperparameter choices for Transformer models,\nthe values outlined in Table 1 were tested using a random search approach. A range of values\nfor each parameter was tested using short cycles of 5k training steps. Once an optimal value,\nwithin the sampled range was identified, it was locked in for tests on subsequent parameters.\n3.1 Architecture Tuning\nGiven the long training times associated with NMT, it is difficult and costly to tune systems\nusing a conventional Grid Search approach. Therefore a Random Search approach was adopted\nin the HPO of our transformer models.\nWith low-resource datasets, the use of smaller and fewer layers has previously been shown\nto improve performance (Araabi and Monz, 2020). Performance of low-resource NMT has\nalso been demonstrated to improve in cases where shallow Transformer models are adopted\n(Van Biljon et al., 2020). Guided by these findings, configurations were tested which varied the\nnumber of neurons in each layer and modified the number of layers used in the Transformer\narchitecture.\nThe impact of regularization, by applying varying degrees of dropout to Transformer mod-\nels, was evaluated. Configurations using smaller (0.1) and larger values (0.3) were applied to\nthe output of each feed forward layer.\n3.2 Subword Models\nIt has become standard practise to incorporate word segmentation approaches, such as Byte-\nPair-Encoding (BPE), when developing NMT models. Previous work shows that subword\nmodels may be particularly beneficial for low-resource languages since rare words are often a\nproblem. Reducing the number of BPE merge operations resulted in substantial improvements\nof 5 BLEU points (Sennrich and Zhang 2019) when tested on RNN models.\nIn the context of English to Irish translation, there is no clear agreement as to what consti-\ntuted the best approach. Consequently, as part of this study, subword regularization techniques,\ninvolving BPE and unigram models were evaluated to determining the optimal parameters for\nmaximising translation performance. BPE models with varying vocabulary sizes of 4k, 8k, 16k\nand 32k were tested.\n4 Empirical Evaluation\n4.1 Experimental Setup\n4.1.1 Datasets\nThe performance of the Transformer and RNN approaches is evaluated on English to Irish\nparallel datasets. Two datasets were used in the evaluation of our models namely the publicly\navailable DGT dataset which may be broadly categorised as generic and an in-domain dataset\nwhich focuses on public administration data.\nThe DGT, and its Joint Research Centre, has made available all Translation Memory (TM;\ni.e. sentences and their professionally produced translations) which cover all official European\nUnion languages (Steinberger et al., 2013).\nData provided by the Department of Tourism, Culture, Arts, Gaeltacht, Sport and Media in\nIreland formed the majority of the data in the public administration dataset. This includes staff\nnotices, annual reports, website content, press releases and official correspondence.\nParallel texts from the Digital Corpus of the European Parliament (DCEP) and the DGT\nare included in the training data. Crawled data, from sites of a similar domain are included.\nFurthermore a parallel corpus collected from Conradh na Gaeilge (CnaG), an Irish language\norganisation that promotes the Irish language, was included. The dataset was compiled as part\nof a previous study which carried out a preliminary comparison of SMT and NMT models for\nthe Irish language (Dowling et al., 2018).\n4.1.2 Infrastructure\nModels were developed using a lab of machines each of which has an AMD Ryzen 7 2700X\nprocessor, 16 GB memory, a 256 SSD and an NVIDIA GeForce GTX 1080 Ti. Rapid prototype\nArchitecture BLEU ↑ TER ↓ ChrF3 ↑ Steps Runtime\n(hours) kgCO2\ndgt-rnn-base 52.7 0.42 0.71 75k 4.47 0\ndgt-rnn-bpe8k 54.6 0.40 0.73 85k 5.07 0\ndgt-rnn-bpe16k 55.6 0.39 0.74 100k 5.58 0\ndgt-rnn-bpe32k 55.3 0.39 0.74 95k 4.67 0\ndgt-rnn-unigram 55.6 0.39 0.74 105k 5.07 0\nTable 2: RNN performance on DGT dataset of 52k lines\nArchitecture BLEU ↑ TER ↓ ChrF3 ↑ Steps Runtime\n(hours) kgCO2\npa-rnn-base 40.4 0.47 0.63 60k 2.13 0\npa-rnn-bpe8k 41.5 0.46 0.64 110k 4.16 0\npa-rnn-bpe16k 41.5 0.46 0.64 105k 3.78 0\npa-rnn-bpe32k 41.9 0.47 0.64 100k 2.88 0\npa-rnn-unigram 41.9 0.46 0.64 95k 2.75 0\nTable 3: RNN performance on PA dataset of 88k lines\ndevelopment was enabled through a Google Colab Pro subscription using NVIDIA Tesla P100\nPCIe 16 GB graphic cards and up to 27GB of memory when available (Bisong, 2019).\nOur MT models were trained using the Pytorch implementation of OpenNMT 2.0, an\nopen-source toolkit for NMT (Klein et al., 2017).\n4.1.3 Metrics\nAs part of this study, several automated metrics were used to determine the translation quality.\nAll models were trained and evaluated on both the DGT and PA datasets using the BLEU (Pa-\npineni et al., 2002), TER (Snover et al., 2006) and ChrF (Popovi ´c, 2015) evaluation metrics.\nCase-insensitive BLEU scores, at the corpus level, are reported. Model training was stopped\nonce an early stopping criteria of no improvement in validation accuracy for 4 consecutive iter-\nations was recorded.\n4.2 Results\n4.2.1 Performance of subword models\nThe impact on translation accuracy when choosing a subword model is highlighted in Tables\n2 - 5. In training both RNN and Transformer architectures, incorporating any submodel type\nled to improvements in model accuracy. This finding is evident when training either the smaller\ngeneric DGT dataset or the larger in-domain PA dataset.\nUsing an RNN architecture on DGT, as illustrated in Table 2, the best performing model\nwith a 32k unigram submodel, achieved a BLEU score 7.4% higher than the baseline. With the\nPA dataset using an RNN, as shown in Table 3, the model with the best BLEU, TER and ChrF3\nscores again used a unigram submodel.\nThere are small improvements in BLEU scores when the RNN baseline is compared with\nmodels using a BPE submodel of either 8k, 16k or 32k words, as illustrated in Tables 2 and 3.\nThe maximum BLEU score improvement of 1.5 points (2.5%) is quite modest in the case of the\npublic admin corpus. However, there are larger gains with the DGT corpus. A baseline RNN\nmodel, trained on DGT, achieved a BLEU score of 52.7 whereas the highest-performing BPE\nvariant, using a 16k vocab, recorded an improvement of nearly 3 points with a score of 55.6.\nIn the context of Transformer architectures, highlighted in Table 4 and Table 5, the use\nArchitecture BLEU ↑ TER ↓ ChrF3 ↑ Steps Runtime\n(hours) kgCO2\ndgt-trans-base 53.4 0.41 0.72 55k 14.43 0.81\ndgt-trans-bpe8k 59.5 0.34 0.77 200k 24.48 1.38\ndgt-trans-bpe16k 60.5 0.33 0.78 180k 26.90 1.52\ndgt-trans-bpe32k 59.3 0.35 0.77 100k 18.03 1.02\ndgt-trans-unigram 59.3 0.35 0.77 125k 21.95 1.24\nTable 4: Transformer performance on 52k DGT dataset. Highest performing model uses 2\nattention heads. All other models use 8 attention heads.\nArchitecture BLEU ↑ TER ↓ ChrF3 ↑ Steps Runtime\n(hours) kgCO2\npa-trans-base 44.1 0.44 0.66 20k 5.97 0.34\npa-trans-bpe8k 46.6 0.40 0.68 160k 20.1 1.13\npa-trans-bpe16k 47.1 0.41 0.68 100k 14.22 0.80\npa-trans-bpe32k 46.8 0.41 0.68 70k 12.7 0.72\npa-trans-unigram 46.6 0.42 0.68 75k 13.34 0.75\nTable 5: Transformer performance on 88k PA dataset. All models use 8 attention heads.\nof subword models delivers significant performance improvements for both the DGT and pub-\nlic admin corpora. The performance gains for Transformer models are far greater than RNN\nmodels. Baseline DGT Transformer models achieve a BLEU score of 53.4 while a Transformer\nmodel, with a 16k BPE submodel, has a score of 60.5 representing a BLEU score improvement\nof 13% at 7.1 BLEU points.\nFor translating into a morphologically rich language, such as Irish, the ChrF metric has\nproven successful in showing strong correlation with human translation (Stanojevi´c et al., 2015).\nIn the context of our experiments, it worked well in highlighting the performance differences\nbetween RNN and Transformer architectures.\n4.2.2 Transformer performance compared with RNN\nThe performance of RNN models is contrasted with the Transformer approach in Figure 2 and\nFigure 3. Transformer models, as anticipated, outperform all their RNN counterparts. It is\ninteresting to note the impact of choosing the optimal vocabulary size for BPE submodels.\nFigure 2: BLEU performance for all model architectures\nBoth datasets demonstrate that choosing a BPE vocabulary of 16k words yields the highest\nperformance.\nFurthermore, the TER scores highlighted in Figure 3 reinforce the findings that using 16k\nBPE submodels on Transformer architectures leads to better translation performance. The TER\nscore for the DGT Transformer 16k BPE model is significantly better (0.33) when compared\nwith the baseline performance (0.41).\nFigure 3: TER performance for all model architectures\nFigure 4: Training DGT Transformer baseline\n Figure 5: Training DGT Transformer 16k BPE\n5 Environmental Impact\nMotivated by the findings of Stochastic Parrots (Bender et al., 2021), energy consumption dur-\ning model development was tracked. Prototype model development used Colab Pro, which as\npart of Google Cloud is carbon neutral (Lacoste et al., 2019). However, longer running Trans-\nformer experiments were conducted on local servers using 324 gCO 2 per kWh 2(SEAI, 2020).\n2https://www.seai.ie/publications/Energy-in-Ireland-2020.pdf\nThe net result was just under 10 kgCO 2 created for a full run of model development. Models\ndeveloped during this study, will be reused for ensemble experiments in future work.\n6 Discussion\nValidation accuracy, and model perplexity, in developing the baseline and optimal models for\nthe DGT corpus are illustrated in Figure 4 and Figure 5. Rapid convergence was observed\nwhile training the baseline model such that little accuracy improvement occurs after 20k steps.\nIncluding a subword model led to much slower convergence and there were only marginal gains\nafter 60k steps. Furthermore, it is observed that training the DGT model, with a 16k BPE\nsubmodel, boosted validation accuracy by over 8% compared with its baseline.\nWith regard to the key metric of perplexity, it is shown to rise after training for 15k steps\nin the baseline models. PPL was observed to rise at later stages, typically after 40k steps in\nmodels developed using subword models. Perplexity (PPL), shows how many different, equally\nprobable words can be produced during translation. As a metric for translation performance, it\nis important to keep low scores so the number of alternative translations is reduced. Therefore,\nfor future model development it may be worthwhile to set PPL as an early stopping parameter.\nOn examining the PPL graphs of Figure 4 and Figure 5, it is clear that a lower global min-\nimum is achieved when the Transformer approach is used with a 16k BPE submodel. The PPL\nglobal minimum (2.7) is over 50% lower than the corresponding PPL for the Transformer base\nmodel (5.5). Such a finding illustrates that choosing an optimal submodel delivers significant\nperformance gains.\nTranslation engine performance was bench-marked against Google Translate’s 3 English\nto Irish translation service which is freely available on the internet. Four random samples were\nselected from the English source test file and are presented in Table 6. Translation of these\nsamples was carried out on the optimal DGT Transformer model and using Google Translate.\nCase insensitive, sentence level BLEU scores were recorded and are presented in Table 7. The\nresults are encouraging and indicate well-performing translation models on the DGT dataset.\nThe optimal parameters selected in this discovery process are identified in bold in Table\n2. A higher initial learning rate of 2 coupled with an average decay of 0.0001 led to longer\ntraining times but more accurate models. Despite setting an early stopping parameter, many of\nthe Transformer builds continued for the full cycle of 200k steps over periods of 20+ hours.\nTraining transformer models with a reduced number of attention heads led to a marginal\nimprovement in translation accuracy with a smaller corpus. Our best performing model on a\n55k DGT corpus, with 2 heads and a 16k BPE submodel, achieved a BLEU score of 60.5 and\na TER score of 0.33. By comparison, using 8 heads with the same architecture and dataset\nyielded 60.3 for the BLEU and 0.34 for the TER. In the case of a larger 88k PA corpus, all\ntransformer models using 8 heads performed better than equivalent models using just 2 heads.\n3https://translate.google.com/\nSource Language (English) Reference Human Translation (Irish)\nA clear harmonised procedure, including the\nnecessary criteria for disease–free status,\nshould be established for that purpose.\nBa cheart n´os imeachta comhchuibhithe soil´eir,\nlena n-´air´ıtear na crit´eir is g´a do st´adas saor\n´o ghalar, a bhun´u chun na cr´ıche sin.\nthe mark is applied anew, as appropriate. d ´eanfar an mharc´ail ar´ıs, mar is iomchu´ı.\nIf the court decides that a review is\njustified on any of the grounds set out in\nparagraph 1, the judgment given in the\nEuropean Small Claims Procedure shall\nbe null and void.\nM´a chinneann an ch´uirt go bhfuil bonn cirt\nle hathbhreithni´u de bharr aon cheann de na\nforais a leagtar amach i m´ır 1, beidh an\nbreithi´unas a tugadh sa N´os Imeachta Eorpach\num ´Eilimh Bheaga ar neamhn´ı go hioml´an.\nhouseholds where pet animals are kept; teaghlaigh ina gcoime ´adtar peata´ı;\nTable 6: Samples of human reference translations\nTransformer (16 kBPE) BLEU ↑ Google Translate BLEU ↑\nBa cheart n´os imeachta soil´eir\ncomhchuibhithe, lena n-´air´ıtear\nna crit´eir is g´a maidir le\nst´adas saor ´o ghalair, a bhun´u\nchun na cr´ıche sin.\n61.6\nBa cheart n´os imeachta\ncomhchuibhithe soil´eir, lena\nn-´air´ıtear na crit´eir riachtanacha\nmaidir le st´adas saor ´o ghalair,\na bhun´u chun na cr´ıche sin.\n70.2\ngo gcuirtear an marc i bhfeidhme,\nde r´eir mar is iomchu´ı. 21.4 cuirtear an marc i bhfeidhm as\nan nua, de r´eir mar is cu´ı. 6.6\nM´a chinneann an ch´uirt go bhfuil\nbonn cirt le hathbhreithni´u ar aon\ncheann de na forais a leagtar amach\ni m´ır 1, beidh an breithi´unas a\nthugtar sa N´os Imeachta Eorpach\num ´Eilimh Bheaga ar neamhn´ı.\n77.3\nM´a chinneann an ch´uirt go bhfuil\n´udar le hathbhreithni´u ar aon\ncheann de na forais at´a leagtha\namach i m´ır 1, beidh an\nbreithi´unas a thugtar sa\nN´os Imeachta Eorpach um\n´Eilimh Bheaga ar neamhn´ı\n59.1\nteaghlaigh ina gcoime´adtar peata´ı; 100 teaghlaigh ina gcoinn ´ıtear peata´ı; 30.2\nTable 7: Transformer model compared with Google Translate using random samples from the\nDGT corpus. Full evaluation of Google Translate on the DGT test set, with 1.3k lines, generated\na BLEU score of 46.3 and a TER score of 0.44. Comparative scores on the test set using our\nTransformer model, with 2 attention heads and 16k BPE submodel realised 60.5 for BLEU and\n0.33 for TER.\nStandard Transformer parameters for batch size (2048) and the number of encoder / de-\ncoder layers (6) were all observed to perform well on the DGT and PA corpora. Reducing\nhidden neurons to 256 and increasing regularization dropout to 0.3 improved translation perfor-\nmance and were chosen when building all Transformer models.\n7 Conclusion\nIn our paper, we demonstrated that a random search approach to hyperparameter optimization\nleads to the development of high-performing translation models.\nWe have shown that choosing subword models, in our low-resource scenarios, is an impor-\ntant driver for the performance of MT engines. Moreover, the choice of vocabulary size leads\nto varying degrees of performance. Within the context of low-resource English to Irish transla-\ntion, we achieved optimal performance, on a 55k generic corpus and an 88k in-domain corpus,\nwhen a Transformer architecture with a 16k BPE submodel was used. The importance of se-\nlecting hyperparameters in training low-resource Transformer models was also demonstrated.\nBy reducing the number of hidden layer neurons and increasing dropout, our models performed\nsignificantly better than baseline models and Google Translate.\nPerformance improvement of our optimized Transformer models, with subword segmenta-\ntion, was observed across all key indicators namely a higher validation accuracy, a PPL achieved\nat a lower global minimum, a lower post editing effort and a higher translation accuracy.\nAcknowledgements\nThis work was supported by ADAPT, which is funded under the SFI Research Centres Pro-\ngramme (Grant 13/RC/2016) and is co-funded by the European Regional Development Fund.\nThis research was also funded by the Munster Technological University.\nReferences\nAlam, K. and Imran, S. (2015). The digital divide and social inclusion among refugee migrants. Informa-\ntion Technology & People.\nAraabi, A. and Monz, C. (2020). Optimizing transformer for low-resource neural machine translation.\narXiv preprint arXiv:2011.02266.\nBahdanau, D., Cho, K., and Bengio, Y . (2014). Neural machine translation by jointly learning to align and\ntranslate. arXiv preprint arXiv:1409.0473.\nBender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S. (2021). On the dangers of stochastic\nparrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness,\nAccountability, and Transparency, pages 610–623.\nBergstra, J. and Bengio, Y . (2012). Random search for hyper-parameter optimization.Journal of machine\nlearning research, 13(2).\nBisong, E. (2019). Google colaboratory. In Building Machine Learning and Deep Learning Models on\nGoogle Cloud Platform, pages 59–64. Springer.\nBojar, O., Chatterjee, R., Federmann, C., Graham, Y ., Haddow, B., Huang, S., Huck, M., Koehn, P., Liu,\nQ., Logacheva, V ., Monz, C., Negri, M., Post, M., Rubino, R., Specia, L., and Turchi, M. (2017). Find-\nings of the 2017 conference on machine translation (WMT17). In Proceedings of the Second Confer-\nence on Machine Translation, pages 169–214, Copenhagen, Denmark. Association for Computational\nLinguistics.\nBojar, O., Federmann, C., Fishel, M., Graham, Y ., Haddow, B., Koehn, P., and Monz, C. (2018). Findings\nof the 2018 conference on machine translation (WMT18). In Proceedings of the Third Conference on\nMachine Translation: Shared Task Papers, pages 272–303, Belgium, Brussels. Association for Com-\nputational Linguistics.\nCho, K., Van Merri¨enboer, B., Bahdanau, D., and Bengio, Y . (2014). On the properties of neural machine\ntranslation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259.\nDing, S., Renduchintala, A., and Duh, K. (2019). A call for prudent choice of subword merge operations\nin neural machine translation. arXiv preprint arXiv:1905.10453.\nDowling, M., Lynn, T., Poncelas, A., and Way, A. (2018). Smt versus nmt: Preliminary comparisons for\nirish.\nGage, P. (1994). A new algorithm for data compression. C Users Journal, 12(2):23–38.\nGhifary, M., Kleijn, W. B., Zhang, M., Balduzzi, D., and Li, W. (2016). Deep reconstruction-classification\nnetworks for unsupervised domain adaptation. In European Conference on Computer Vision, pages\n597–613. Springer.\nGowda, T. and May, J. (2020). Finding the optimal vocabulary size for neural machine translation. arXiv\npreprint arXiv:2004.02334.\nKlein, G., Kim, Y ., Deng, Y ., Senellart, J., and Rush, A. M. (2017). Opennmt: Open-source toolkit for\nneural machine translation. arXiv preprint arXiv:1701.02810.\nKudo, T. (2018). Subword regularization: Improving neural network translation models with multiple\nsubword candidates. arXiv preprint arXiv:1804.10959.\nKudo, T. and Richardson, J. (2018). Sentencepiece: A simple and language independent subword tokenizer\nand detokenizer for neural text processing. arXiv preprint arXiv:1808.06226.\nLacoste, A., Luccioni, A., Schmidt, V ., and Dandres, T. (2019). Quantifying the carbon emissions of\nmachine learning. arXiv preprint arXiv:1910.09700.\nLiu, C.-H., Silva, C. C., Wang, L., and Way, A. (2018). Pivot machine translation using chinese as pivot\nlanguage. In China Workshop on Machine Translation, pages 74–85. Springer.\nMacFarlane, A., Glynn, L. G., Mosinkie, P. I., and Murphy, A. W. (2008). Responses to language barriers\nin consultations with refugees and asylum seekers: a telephone survey of irish general practitioners.\nBMC Family Practice, 9(1):1–6.\nMontgomery, D. C. (2017). Design and analysis of experiments. John wiley & sons.\nPapineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002). Bleu: a method for automatic evaluation of\nmachine translation. In Proceedings of the 40th annual meeting of the Association for Computational\nLinguistics, pages 311–318.\nPopovi´c, M. (2015). chrf: character n-gram f-score for automatic mt evaluation. In Proceedings of the\nTenth Workshop on Statistical Machine Translation, pages 392–395.\nSanders, S. and Giraud-Carrier, C. (2017). Informing the use of hyperparameter optimization through\nmetalearning. In 2017 IEEE International Conference on Data Mining (ICDM), pages 1051–1056.\nIEEE.\nSEAI (2020). Sustainable Energy in Ireland.\nSennrich, R., Haddow, B., and Birch, A. (2015). Neural machine translation of rare words with subword\nunits. arXiv preprint arXiv:1508.07909.\nSnover, M., Dorr, B., Schwartz, R., Micciulla, L., and Makhoul, J. (2006). A study of translation edit rate\nwith targeted human annotation. InProceedings of association for machine translation in the Americas,\nvolume 200. Citeseer.\nStanojevi´c, M., Kamran, A., Koehn, P., and Bojar, O. (2015). Results of the wmt15 metrics shared task.\nIn Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 256–273.\nSteinberger, R., Eisele, A., Klocek, S., Pilos, S., and Schl ¨uter, P. (2013). Dgt-tm: A freely available\ntranslation memory in 22 languages. arXiv preprint arXiv:1309.5226.\nVan Biljon, E., Pretorius, A., and Kreutzer, J. (2020). On optimal transformer depth for low-resource\nlanguage translation. arXiv preprint arXiv:2004.04418.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin,\nI. (2017). Attention is all you need. arXiv preprint arXiv:1706.03762.\nWay, A. (2020). MT Developments in the EU: Keynote AMTA 2020.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6556144952774048
    },
    {
      "name": "Computer science",
      "score": 0.4051949679851532
    },
    {
      "name": "Business",
      "score": 0.37764716148376465
    },
    {
      "name": "Engineering",
      "score": 0.2966684103012085
    },
    {
      "name": "Electrical engineering",
      "score": 0.29022592306137085
    },
    {
      "name": "Voltage",
      "score": 0.08800628781318665
    }
  ],
  "institutions": []
}