{
  "title": "CDTrans: Cross-domain Transformer for Unsupervised Domain Adaptation",
  "url": "https://openalex.org/W3201612248",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287074942",
      "name": "Xu, Tongkun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1918132155",
      "name": "Chen Weihua",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1238505444",
      "name": "Wang, Pichao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2111321986",
      "name": "Wang Fan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A875780830",
      "name": "Li Hao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2209117791",
      "name": "Jin Rong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3034401437",
    "https://openalex.org/W2970092410",
    "https://openalex.org/W2478454054",
    "https://openalex.org/W3168124404",
    "https://openalex.org/W2903739847",
    "https://openalex.org/W3168101492",
    "https://openalex.org/W2963275094",
    "https://openalex.org/W3175452902",
    "https://openalex.org/W3195000282",
    "https://openalex.org/W2948959975",
    "https://openalex.org/W3187418919",
    "https://openalex.org/W2627183927",
    "https://openalex.org/W2584009249",
    "https://openalex.org/W2964278684",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3139633126",
    "https://openalex.org/W2964051877",
    "https://openalex.org/W3094277917",
    "https://openalex.org/W2966743431",
    "https://openalex.org/W3171206729",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3136635488",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2766897166",
    "https://openalex.org/W3039883906",
    "https://openalex.org/W2997739323",
    "https://openalex.org/W3188427387",
    "https://openalex.org/W3035576098",
    "https://openalex.org/W2946812986",
    "https://openalex.org/W2963532621",
    "https://openalex.org/W3181127262",
    "https://openalex.org/W2964288524",
    "https://openalex.org/W2980096013",
    "https://openalex.org/W3169938586",
    "https://openalex.org/W2962687275",
    "https://openalex.org/W2982204955",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W1722318740",
    "https://openalex.org/W2962808524",
    "https://openalex.org/W2590953969",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2159291411",
    "https://openalex.org/W2963826681",
    "https://openalex.org/W3143320354",
    "https://openalex.org/W3165550458",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3169909366",
    "https://openalex.org/W3140576409",
    "https://openalex.org/W3175370657",
    "https://openalex.org/W2125865219",
    "https://openalex.org/W2949813473",
    "https://openalex.org/W2895281799",
    "https://openalex.org/W3192174868",
    "https://openalex.org/W2767382337",
    "https://openalex.org/W2798681837",
    "https://openalex.org/W3176720610",
    "https://openalex.org/W2794887779",
    "https://openalex.org/W2593768305",
    "https://openalex.org/W3128723389",
    "https://openalex.org/W2991405316",
    "https://openalex.org/W3025988480",
    "https://openalex.org/W1565327149",
    "https://openalex.org/W3151130473",
    "https://openalex.org/W2970987681",
    "https://openalex.org/W2981720610",
    "https://openalex.org/W3177183540",
    "https://openalex.org/W2795155917",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3173631098",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3118802333",
    "https://openalex.org/W3034526587",
    "https://openalex.org/W3102758664",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2963393201",
    "https://openalex.org/W3164024107",
    "https://openalex.org/W3118589616",
    "https://openalex.org/W3177117566",
    "https://openalex.org/W2293363371",
    "https://openalex.org/W3190216403",
    "https://openalex.org/W3119997354",
    "https://openalex.org/W3109093849",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W2883725317"
  ],
  "abstract": "Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from a labeled source domain to a different unlabeled target domain. Most existing UDA methods focus on learning domain-invariant feature representation, either from the domain level or category level, using convolution neural networks (CNNs)-based frameworks. One fundamental problem for the category level based UDA is the production of pseudo labels for samples in target domain, which are usually too noisy for accurate domain alignment, inevitably compromising the UDA performance. With the success of Transformer in various tasks, we find that the cross-attention in Transformer is robust to the noisy input pairs for better feature alignment, thus in this paper Transformer is adopted for the challenging UDA task. Specifically, to generate accurate input pairs, we design a two-way center-aware labeling algorithm to produce pseudo labels for target samples. Along with the pseudo labels, a weight-sharing triple-branch transformer framework is proposed to apply self-attention and cross-attention for source/target feature learning and source-target domain alignment, respectively. Such design explicitly enforces the framework to learn discriminative domain-specific and domain-invariant representations simultaneously. The proposed method is dubbed CDTrans (cross-domain transformer), and it provides one of the first attempts to solve UDA tasks with a pure transformer solution. Experiments show that our proposed method achieves the best performance on public UDA datasets, e.g. VisDA-2017 and DomainNet. Code and models are available at https://github.com/CDTrans/CDTrans.",
  "full_text": "Published as a conference paper at ICLR 2022\nCDT RANS : C ROSS -DOMAIN TRANSFORMER FOR UN-\nSUPERVISED DOMAIN ADAPTATION\nTongkun Xu12∗, Weihua Chen1∗, Pichao Wang1, Fan Wang1, Hao Li1, Rong Jin1\n1Alibaba Group, 2Shandong University\nxutongkun1208@gmail.com, kugang.cwh@alibaba-inc.com\n{pichao.wang,fan.w,lihao.lh,jinrong.jr}@alibaba-inc.com\nABSTRACT\nUnsupervised domain adaptation (UDA) aims to transfer knowledge learned from\na labeled source domain to a different unlabeled target domain. Most existing\nUDA methods focus on learning domain-invariant feature representation, either\nfrom the domain level or category level, using convolution neural networks\n(CNNs)-based frameworks. One fundamental problem for the category level\nbased UDA is the production of pseudo labels for samples in target domain, which\nare usually too noisy for accurate domain alignment, inevitably compromising\nthe UDA performance. With the success of Transformer in various tasks, we\nﬁnd that the cross-attention in Transformer is robust to the noisy input pairs\nfor better feature alignment, thus in this paper Transformer is adopted for the\nchallenging UDA task. Speciﬁcally, to generate accurate input pairs, we design\na two-way center-aware labeling algorithm to produce pseudo labels for target\nsamples. Along with the pseudo labels, a weight-sharing triple-branch transformer\nframework is proposed to apply self-attention and cross-attention for source/target\nfeature learning and source-target domain alignment, respectively. Such design\nexplicitly enforces the framework to learn discriminative domain-speciﬁc and\ndomain-invariant representations simultaneously. The proposed method is dubbed\nCDTrans (cross-domain transformer), and it provides one of the ﬁrst attempts\nto solve UDA tasks with a pure transformer solution. Experiments show that\nour proposed method achieves the best performance on public UDA datasets,\ne.g. VisDA-2017 and DomainNet. Code and models are available at https:\n//github.com/CDTrans/CDTrans.\n1 I NTRODUCTION\nDeep neural network have achieved remarkable success in a wide range of application scenar-\nios (Wang et al., 2022; Qian et al., 2021; Yiqi Jiang, 2022; Tan et al., 2019; Chen et al., 2021b;\nJiang et al., 2021; Chen et al., 2017) but it still suffers poor generalization performance to other\nnew domain because of the domain shift problem (Csurka, 2017; Zhao et al., 2020; Zhang et al.,\n2020; Oza et al., 2021). To handle this issue and avoid the expensive laborious annotations, lots\nof research efforts (Bousmalis et al., 2017; Kuroki et al., 2019; Wilson & Cook, 2020; VS et al.,\n2021) are devoted on Unsupervised Domain Adaptation (UDA). The UDA task aims to transfer\nknowledge learned from a labeled source domain to a different unlabeled target domain. In UDA,\nmost approaches focus on aligning distributions of source and target domain and learning domain-\ninvariant feature representations. One kind of such UDA methods are based on category-level\nalignment (Kang et al., 2019; Zhang et al., 2019; Jiang et al., 2020; Li et al., 2021b), which have\nachieved promising results on public UDA datasets using deep convolution neural networks (CNNs).\nThe fundamental problems in category-level based alignment is the production of pseudo labels for\nsamples in target domain to generate the input source-target pairs. However, the current CNNs-based\nmethods are not robust to the generated noisy pseudo labels for accurate domain alignment (Morerio\net al., 2020; Jiang et al., 2020).\nWith the success of Transformer in natural language processing (NLP) (Vaswani et al., 2017; Devlin\net al., 2018) and vision tasks (Dosovitskiy et al., 2020; Han et al., 2020; He et al., 2021; Khan et al.,\n∗These authors contributed equally to this work.\n1\narXiv:2109.06165v4  [cs.CV]  19 Mar 2022\nPublished as a conference paper at ICLR 2022\n2021), it is found that cross-attention in Transformer is good at aligning different distributions, even\nfrom different modalities e.g., vision-to-vision (Li et al., 2021e), vision-to-text (Tsai et al., 2019;\nHu & Singh, 2021) and text-to-speech (Li et al., 2019). And we ﬁnd that it is robust to noise in\npseudo labels to some extent. Hence, in this paper, we apply transformers to the UDA task to take\nadvantage of its robustness to noise and super power for feature alignment to deal with the problems\nas described above in CNNs.\nIn our experiment, we conclude that even with noise in the labeling pair, the cross-attention can\nstill work well in aligning two distributions, thanks to the attention mechanism. To obtain more\naccurate pseudo labels, we designed a two-way center-aware labeling algorithm for samples in the\ntarget domain. The pseudo labels are produced based on the cross-domain similarity matrix, and a\ncenter-aware matching is involved to weight the matrix and weaken noise into the tolerable range.\nWith the help of pseudo labels, we design the cross-domain transformer (CDTrans) for UDA. It\nconsists of three weight-sharing transformer branches, of which two branches are for source and\ntarget data respectively and the third one is the feature alignment branch, whose inputs are from\nsource-target pairs. The self-attention is applied in the source/target transformer branches and cross-\nattention is involved in the feature alignment branch to conduct domain alignment. Such design\nexplicitly enforces the framework to learn discriminative domain-speciﬁc and domain-invariant\nrepresentations simultaneously. In summary, our contributions are three-fold:\n• We propose a weight-sharing triple-branch transformer framework, namely, CDTrans,\nfor accurate unsupervised domain adaptation, taking advantage of its robustness to noisy\nlabeling data and great power for feature alignment.\n• To produce pseudo labels with high quality, a two-way center-aware labeling method is\nproposed, and it boosts the ﬁnal performance in the context of CDTrans.\n• CDTrans achieves the best performance compared to state-of-the-arts with a large margin\non VisDA-2017 (Peng et al., 2017) and DomainNet (Peng et al., 2019) datasets.\n2 R ELATED WORK\n2.1 T RANSFORMER FOR VISION\nTransformer is proposed in (Vaswani et al., 2017) to model sequential data in the ﬁeld of NLP.\nMany works have shown its effectiveness for computer-vision tasks (Han et al., 2020; Khan et al.,\n2021; Li et al., 2021d; Han et al., 2021b; Yu et al., 2021; Li et al., 2021c; Yang et al., 2021;\nQian et al., 2022). Pure Transformer based models are becoming more and more popular. For\nexample, ViT (Dosovitskiy et al., 2020) is proposed recently by feeding transformer with sequences\nof image patches; Touvron et al. (Touvron et al., 2021) propose DeiT that introduces a distillation\nstrategy for transformers to help with ViT training; many other ViT variants (Yuan et al., 2021a;\nWang et al., 2021; Han et al., 2021a; Chen et al., 2021a; Ranftl et al., 2021; Liu et al., 2021) are\nproposed from then, which achieve promising performance compared with its counterpart CNNs\nfor both image classiﬁcation and downstream tasks, such as object detection (Liu et al., 2021),\nsemantic segmentation (Yuan et al., 2021b) and object ReID (He et al., 2021). For multi-modal\nbased networks, there are several works (Tsai et al., 2019; Li et al., 2021e; Hu & Singh, 2021) that\napply cross-attention for multi-modal feature fusion, which demonstrates that attention mechanism\nis powerful at distilling noise and feature alignment. This paper adopts cross-attention in the context\nof pure transformers for UDA tasks.\n2.2 U NSUPERVISED DOMAIN ADAPTATION\nThere are mainly two levels for UDA methods: domain-level (Tzeng et al., 2014; Long et al., 2015;\nGhifary et al., 2016; Tzeng et al., 2017; Bousmalis et al., 2017; Hoffman et al., 2018) and category-\nlevel (Saito et al., 2018; Kang et al., 2019; Du et al., 2021; Li et al., 2021a). Domain-level UDA\nmitigates the distribution divergence between the source and target domain by pulling them into\nthe same distribution at different scale levels. The commonly used divergence measures include\nMaximum Mean Discrepancy (MMD) (Gretton et al., 2006; Tzeng et al., 2014; Long et al., 2015)\nand Correlation Alignment (CORAL) (Sun et al., 2016; Sun & Saenko, 2016). Recently, some\nworks (Saito et al., 2018; Du et al., 2021; Li et al., 2021a) focus on the ﬁne-grained category-level\n2\nPublished as a conference paper at ICLR 2022\nlabel distribution alignment through an adversarial manner between the feature extractor and two\ndomain-speciﬁc classiﬁers. Unlike coarse-grained alignment at the domain scale, this approach\naligns each category distribution between the source and target domain data by pushing the target\nsamples to the distribution of source samples in each category. Obviously, the ﬁne-grained alignment\nresults in more accurate distribution alignment within the same label space. Although the adversarial\napproach achieves new improvements by fusing ﬁne-grained alignment operations of source and\ntarget samples at the category level, it still does not solve the problem of noisy samples in the wrong\ncategory. Our method adopts Transformers for category-level UDA to solve the noise problem.\n2.3 P SEUDO LABELING\nPseudo labeling (Lee et al., 2013) is ﬁrst introduced for semi-supervised learning and gains\npopularity in domain adaptation tasks. It learns to label unlabeled data using predicted probabilities\nand performs ﬁne-tuning together with labeled data. In terms of using pseudo labeling for domain\nadaptation tasks, (Long et al., 2017; 2018) adopt pseudo labels to conduct conditional distribution\nalignment; (Zhang et al., 2018; Choi et al., 2019) use pseudo labels as a regularization for domain\nadaptation; Zou et al. (2018) designs a self-training framework by alternately solving pseudo labels;\nCaron et al. (2018) propose a deep self-supervised method by generating pseudo labels viak-means\ncluster to progressively train the model; Liang et al. (2020) develop a self-supervised pseudo labeling\nmethod to alleviate the effects of noisy pseudo labels. Based on Liang et al. (2020), in this work, we\npropose a two-way center-aware labeling algorithm to further ﬁlter the noisy pseudo pairs.\n3 T HE PROPOSED METHOD\nWe ﬁrst introduce the cross attention module and analyze its robustness to the noise in Section 3.1.\nThen the two-way center-aware labeling method is presented in Section 3.2. With the produced\npseudo labels as inputs, our cross-domain transformer (CDTrans) is proposed in Section 3.3,\nconsisting of three weight-sharing transformers.\n3.1 T HE CROSS ATTENTION IN TRANSFORMER\n3.1.1 P RELIMINARY\nVision Transformer (ViT) (Dosovitskiy et al., 2020) has achieved comparable or even superior\nperformance on computer vision tasks. One of the most important structures in ViT is the self-\nattention module (Vaswani et al., 2017). In ViT, an imageI ∈RH×W×C is reshaped into a sequence\nof ﬂattened 2D patches x ∈RN×(P2·C), where ( H,W ) is the resolution of the original image, C\nis the number of channels, ( P,P ) is the resolution of each image patch, and N = HW/P2 is the\nresulting number of patches. For self-attention, the patches are ﬁrst projected into three vectors,\ni.e. queries Q ∈RN×dk , keys K ∈RN×dk and values V ∈RN×dv . dk and dv indicates their\ndimensions. The output is computed as a weighted sum of the values, where the weight assigned to\neach value is computed by a compatibility function of the query with the corresponding key. TheN\npatches serve as the inputs for the self-attention module, and the process can be formulated as below.\nThe self-attention module aims to emphasize relationships among patches of the input image.\nAttnself (Q,K,V ) =softmax(QKT\n√dk\n)V (1)\nThe cross-attention module is derived from the self-attention module. The difference is that the input\nof cross-attention is a pair of images, i.e. Is and It. Its query and key/value are from patches of Is\nand It respectively. The cross-attention module can be calculated as follows:\nAttncross(Qs,Kt,Vt) =softmax(QsKT\nt√dk\n)Vt (2)\nwhere Qs ∈RM×dk are queries from M patches of image Is, and Kt ∈RN×dk ,Vt ∈RN×dv are\nkeys and values from N patches of image It. The output of the cross-attention module holds the\nsame length M as the number of the queries. For each output, it is calculated by multiplying Vt\n3\nPublished as a conference paper at ICLR 2022\n(a)\n0.0 0.2 0.4 0.6 0.8 1.0\n/uni00000057/uni0000004b/uni00000048/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048/uni00000003/uni00000052/uni00000049/uni00000003/uni00000049/uni00000044/uni0000004f/uni00000056/uni00000048/uni00000003/uni00000053/uni00000052/uni00000056/uni0000004c/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000053/uni00000044/uni0000004c/uni00000055/uni00000056\n0\n20\n40\n60\n80\n100\n/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni0000005a/uni0000004c/uni00000057/uni0000004b/uni00000003/uni00000046/uni00000055/uni00000052/uni00000056/uni00000056/uni00000010/uni00000044/uni00000057/uni00000057/uni00000051/uni00000003/uni00000052/uni00000051/uni00000003/uni00000044/uni0000004f/uni0000004f/uni00000003/uni00000053/uni00000044/uni0000004c/uni00000055/uni00000056\n/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000046/uni00000055/uni00000052/uni00000056/uni00000056/uni00000010/uni00000044/uni00000057/uni00000057/uni00000051/uni00000003/uni00000052/uni00000051/uni00000003/uni00000044/uni0000004f/uni0000004f/uni00000003/uni00000053/uni00000044/uni0000004c/uni00000055/uni00000056\n/uni0000005a/uni0000004c/uni00000057/uni0000004b/uni00000003/uni00000046/uni00000055/uni00000052/uni00000056/uni00000056/uni00000010/uni00000044/uni00000057/uni00000057/uni00000051/uni00000003/uni00000052/uni00000051/uni00000003/uni00000057/uni00000055/uni00000058/uni00000048/uni00000003/uni00000053/uni00000052/uni00000056/uni00000003/uni00000053/uni00000044/uni0000004c/uni00000055/uni00000056 (b)\nFigure 1: (a): The heatmap of the cross-attention weights for a false positive pair (Car vs. Truck).\n(b): The changes of UDA performance by the ratio of false positive pairs. The red/green curves\nrepresent the model with and without the cross-attention module. The blue curve means that only\ntrue positive pairs are involved in the cross-attention module.\nwith attention weights, which comes from the similarity between the corresponding query in Is and\nall the keys in It. As a result, among all patches in It, the patch that is more similar to the query of\nIs would hold a larger weight and contribute more to the output. In other words, the output of the\ncross-attention module manages to aggregate the two input images based on their similar patches.\nSo far, many researchers have utilized the cross-attention for feature fusion, especially in multi-\nmodal tasks (Tsai et al., 2019; Li et al., 2019; Hu & Singh, 2021; Li et al., 2021e). In these works,\nthe inputs of the cross-attention module are from two modalities, e.g. vision-to-text (Tsai et al.,\n2019; Hu & Singh, 2021), text-to-speech (Li et al., 2019) and vision-to-vision (Li et al., 2021e).\nThey apply the cross-attention to aggregate and align the information from two modalities. Given\nits great power in feature alignment, we propose to use the cross attention module to solve the\nunsupervised domain adaptation problem.\n3.1.2 R OBUSTNESS TO NOISE\nAs mentioned above, the input of the cross-attention module is a pair of images, which usually\ncomes from two domains, and the cross-attention module aims to align these two images. If label\nnoise exists, there would be false positive pairs in the training data. Images in the false positive pairs\nwould have dissimilar appearance, and forcibly aligning their features would inevitably injure the\ntraining and compromise the performance. We assume that the dissimilar patches in false positive\npairs are more harmful to the performance than the similar patches. In the cross-attention module,\ntwo images are aligned based on their patch similarity. As shown in Fig. 1a, the cross-attention\nmodule would assign a low weight to the dissimilar patches in false positive pairs. Thus it weakens\nthe negative effects of the dissimilar patches on the ﬁnal performance to some extent. 1\nTo further analyze this issue, an experiment is carefully designed. Speciﬁcally, we randomly sample\ntrue positive pairs from source and target domain in VisDA-2017 dataset (Peng et al., 2017) as the\ntraining data. Then we manually replace the true positive pairs with random false positive pairs\nto increase the noise, and watch the changes of the performance as shown in Fig. 1b. The x-axis\nindicates the rate of false positive pairs in the training data, and the y-axis shows the performance\nof different methods on the UDA task. The red curve represents the results by aligning pairs with\nthe cross-attention module, while the green curve is that without cross-attention,i.e. to directly train\nthe target data with the label of corresponding source data in the pair. It can be seen that the red\ncurve achieves a much better performance than the green one, which implies the robustness of the\ncross-attention module to the noise. We also provide another baseline shown as the blue curve in\nFig. 1b, which is to remove the false positive pairs from the training data and train the cross-attention\nwith only true positive pairs. Without the noisy data, this baseline can be considered as the upper\nbound to our methods. We can see the red curve is very close to the blue curve, and both of them\nare much better than the green one. It further implies that the cross-attention module is robust to the\nnoisy input pair.\n1The patches in true positive pairs, either similar and dissimilar, would bring no noise to the ﬁnal\nperformance, which is out of the discussion of our paper.\n4\nPublished as a conference paper at ICLR 2022\n3.2 T WO-WAY CENTER -AWARE PSEUDO LABELING\n3.2.1 T WO-WAY LABELING\nTo build the training pairs for the cross-attention module, an intuitive method is that for each image\nin the source domain, we manage to ﬁnd the most similar image from the target domain. The set PS\nof selected pairs is:\nPS = {(s,t)|t= min\nk\nd(fs,fk),∀k∈T,∀s∈S} (3)\nwhere S,T are the source and target data respectively.d(fi,fj) means the distance between features\nof image iand j. The advantage of this strategy is to make full use of source data, while its weakness\nis obvious that only a part of target data is involved. To eliminate this training bias from target\ndata, we introduce more pairs PT from the opposite way, consisting of all the target data and their\ncorresponding most similar images in the source domain.\nPT = {(s,t)|s= min\nk\nd(ft,fk),∀t∈T,∀k∈S} (4)\nAs a result the ﬁnal set P is the union of two sets, i.e. P = {PS ∪PT }, making the training pairs\ninclude all the source and target data.\n3.2.2 C ENTER -AWARE FILTERING\nThe pairs in P are built based on the feature similarities of images from both domains, thus the\naccuracy of the pseudo labels of pairs is highly dependent on the feature similarities. Inspired by\nLiang et al. (2020), we ﬁnd that the pre-trained model of the source data is also useful to further\nimprove the accuracy. Firstly, we send all the target data through the pre-trained model and obtain\ntheir probability distributions δ on the source categories from the classiﬁer. Similar to Liang et al.\n(2020), these distributions can be used to compute initial centers of each category in the target\ndomain by weighted k-means clustering:\nck =\n∑\nt∈T δk\nt ft\n∑\nt∈T δk\nt\n(5)\nwhere δk\nt indicates the probability of image ton category k. Pseudo labels of the target data can be\nproduced via the nearest neighbor classiﬁer:\nyt = arg min\nk\nd(ck,ft) (6)\nwhere t ∈T and d(i,j) is the distance of features i and j. Based on the pseudo labels, we can\ncalculate new centers:\nc′\nk =\n∑\nt∈T 1 (yt = k)ft∑\nt∈T 1 (yt = k) (7)\nIn Liang et al. (2020), Eq. 6 and 7 could be updated for multiple rounds, and we only adopt one\nround in our paper. The ﬁnal pseudo labels are then used to reﬁne the selected pairs. Speciﬁcally,\nfor every pair, if the pseudo label of the target image is consistent with the label of the source image,\nthis pair would be kept for our training, otherwise it will be discarded as a noise.\n3.3 CDT RANS : C ROSS -DOMAIN TRANSFORMER\nThe framework of the proposed Cross-domain Transformer (CDTrans) is shown in Fig. 2, which\nconsists of three weight-sharing transformers. There are three data ﬂows and constraints for the\nweight-sharing branches.\nThe inputs of the framework are the selected pairs from our labeling method mentioned above.\nThe three branches are named as source branch, target branch, source-target branch. As shown\nin Fig. 2, the source and target images in the input pair are sent to source branch and target branch\nrespectively. In these two branches, the self-attention module is involved to learn the domain-speciﬁc\n5\nPublished as a conference paper at ICLR 2022\nLcls \nSelf-Attention & Cross-Attention \nClassifier \nSelf-Attention & Cross-Attention \nLcls Ldtl \nddd \nMulti-Head \nSelf Attention \nAdd & Norm \nFeed Forward \nAdd & Norm \nVS KS QS \nHS \n WV    WK    WQ  \nMulti-Head \nCross Attention \nAdd & Norm \nFeed Forward \nAdd & Norm \nQS KT VT \nMulti-Head \nSelf Attention \nAdd & Norm \nFeed Forward \nAdd & Norm \nVT KT QT \nHT \n  WV    WK    WQ \nHS+T \nH’S H’T H’S+T \n WV    WK    WQ  \nSelf-Attention & Cross-Attention \nWeight-Sharing Weight-Sharing \nFigure 2: The proposed CDTrans framework. It consists of three weight-sharing transformers fed\nby inputs from the selected pairs using the two-way center-aware labeling method. Cross-entropy is\nadopted to source branch(HS) and target branch(HT ), while the distillation loss is applied between\nsource-target branch(HS+T ) and HT .\nrepresentations. And the softmax cross-entropy loss is used to train the classiﬁcation. It is worth\nnoting that all three branches share the same classiﬁer due to the same label of two images.\nThe cross-attention module is imported in the source-target branch. The inputs of the source-target\nbranch are from the other two branches. In the N-th layer, the query of the cross-attention module\ncomes from the query in the N-th layer of the source branch, while the keys and values are from\nthose of the target branch. Then the cross-attention module outputs aligned features which are added\nwith the output of the (N−1)-th layer.2\nThe features of the source-target branch not only align distributions of two domains, but are robust\nto the noise in the input pairs thanks to the cross-attention module. Thus we use the output of the\nsource-target branch to guide the training of the target branch. Speciﬁcally, the source-target branch\nand target branch are denoted as teacher and student respectively. We consider the probability\ndistribution of the classiﬁer in source-target branch as a soft label that can be used to further\nsupervise the target branch through a distillation loss (Hinton et al., 2015):\nLdtl =\n∑\nk\nqk log pk (8)\nwhere qk and pk are the probabilities of category k from the source-target branch and the target\nbranch respectively.\nDuring inference, only the target branch is used. The input is an image from testing data, and only\nthe target data ﬂow is triggered, i.e. the blue lines in Fig. 2. Its output of the classiﬁer is utilized as\nthe ﬁnal predicted labels.\n4 E XPERIMENTS\n4.1 D ATASETS AND IMPLEMENTATION\nThe proposed method is veriﬁed on four popular UDA benchmarks, including VisDA-2017 (Peng\net al., 2017), Ofﬁce-Home (Venkateswara et al., 2017), Ofﬁce-31 (Saenko et al., 2010) and\nDomainNet (Peng et al., 2019). In the DomainNet dataset, we follow the setup method of other\ndatasets, using the entire data of the traget domain for training and testing. The input image size in\nour experiments is 224×224. Both the DeiT-small and DeiT-base (Touvron et al., 2021) are adopted\nas our backbone for fair comparison. We use the Stochastic Gradient Descent algorithm with the\nmomentum of 0.9 and weight decay ratio 1e-4 to optimize the training process. The learning rate\nis set to 3e-3 for Ofﬁce-Home, Ofﬁce-31 and DomainNet, 5e-5 for VisDA-2017 since it can easily\nconverge. The batch size is set to 64.\n2The addition operation is not included in the 1st layer.\n6\nPublished as a conference paper at ICLR 2022\nMethod plane bcycl bus car horse knife mcycl person plant sktbrd train truck Avg.\nResNet-50 55.1 53.3 61.9 59.1 80.6 17.9 79.7 31.2 81.0 26.5 73.5 8.5 52.4\nDANN 81.9 77.7 82.8 44.3 81.2 29.5 65.1 28.6 51.9 54.6 82.8 7.8 57.4\nMinEnt 80.3 75.5 75.8 48.3 77.9 27.3 69.7 40.2 46.5 46.6 79.3 16.0 57.0\nMCD 87.0 60.9 83.7 64.0 88.9 79.6 84.7 76.9 88.6 40.3 83.0 25.8 71.9\nSWD 90.8 82.5 81.7 70.5 91.7 69.5 86.3 77.5 87.4 63.6 85.6 29.2 76.4\nCDAN+E 85.2 66.9 83.0 50.8 84.2 74.9 88.1 74.5 83.4 76.0 81.9 38.0 73.9\nBNM 89.6 61.5 76.9 55.0 89.3 69.1 81.3 65.5 90.0 47.3 89.1 30.1 70.4\nMSTN+DSBN 94.7 86.7 76.0 72.0 95.2 75.1 87.9 81.3 91.1 68.9 88.3 45.5 80.2\nCGDM 93.7 82.7 73.2 68.4 92.9 94.5 88.7 82.1 93.4 82.5 86.8 49.2 82.3\nCGDM* 92.8 85.1 76.3 64.5 91.0 93.2 81.3 79.3 92.4 83.0 85.6 44.8 80.8\nSHOT 94.3 88.5 80.1 57.3 93.1 93.1 80.7 80.3 91.5 89.1 86.3 58.2 82.9\nSHOT* 95.5 87.5 80.1 54.5 93.6 94.2 80.2 80.9 90.0 89.9 87.1 58.4 82.7\nTVT◦ 92.9 85.6 77.5 60.5 93.6 98.2 89.4 76.4 93.6 92.0 91.7 55.7 83.9\nBaseline-B 97.7 48.1 86.6 61.6 78.1 63.4 94.7 10.3 87.7 47.7 94.4 35.5 67.1\nCGDM-B* 96.3 87.1 86.8 83.5 92.2 98.3 91.6 78.5 96.3 48.4 89.4 39.0 82.3\nSHOT-B* 97.9 90.3 86.0 73.4 96.9 98.8 94.3 54.8 95.4 87.1 93.4 62.7 85.9\nOurs-B 97.1 90.5 82.4 77.5 96.6 96.1 93.6 88.6 97.9 86.9 90.3 62.8 88.4\nTable 1: Comparison with SoTA methods on VisDA-2017. “S/B” implies the DeiT-small/DeiT-base\nbackbone respectively. ∗indicates the results are reproduced by ourselves. ◦implies its pretrained\nmodel is trained on ImageNet21K instead of ImageNet1K. The best performance is marked asbold.\nMethod Ar→Cl Ar→Pr Ar→Re Cl→Ar Cl→Pr Cl→Re Pr→Ar Pr→Cl Pr→Re Re→Ar Re→Cl Re→Pr Avg.\nResNet-50 44.9 66.3 74.3 51.8 61.9 63.6 52.4 39.1 71.2 63.8 45.9 77.2 59.4\nMinEnt 51.0 71.9 77.1 61.2 69.1 70.1 59.3 48.7 77.0 70.4 53.0 81.0 65.8\nCDAN+E 54.6 74.1 78.1 63.0 72.2 74.1 61.6 52.3 79.1 72.3 57.3 82.8 68.5\nDCAN 54.5 75.7 81.2 67.4 74.0 76.3 67.4 52.7 80.6 74.1 59.1 83.5 70.5\nBNM 56.7 77.5 81.0 67.3 76.3 77.1 65.3 55.1 82.0 73.6 57.0 84.3 71.1\nATDOC-NA 58.3 78.8 82.3 69.4 78.2 78.2 67.1 56.0 82.7 72.0 58.2 85.5 72.2\nSHOT 57.1 78.1 81.5 68.0 78.2 78.1 67.4 54.9 82.2 73.3 58.8 84.3 71.8\nSHOT* 56.2 77.6 81.6 67.5 78.2 78.8 67.8 54.0 82.0 72.5 58.8 84.5 71.6\nTVT◦ 74.9 86.8 89.5 82.8 88.0 88.3 79.8 71.9 90.1 85.5 74.6 90.6 83.6\nBaseline-S 55.6 73.0 79.4 70.6 72.9 76.3 67.5 51.0 81.0 74.5 53.2 82.7 69.8\nOurs-S 60.6 79.5 82.4 75.6 81.0 82.3 72.5 56.7 84.4 77.0 59.1 85.5 74.7\nBaseline-B 61.8 79.5 84.3 75.4 78.8 81.2 72.8 55.7 84.4 78.3 59.3 86.0 74.8\nCGDM-B* 67.1 83.9 85.4 77.2 83.3 83.7 74.6 64.7 85.6 79.3 69.5 87.7 78.5\nSHOT-B* 67.1 83.5 85.5 76.6 83.4 83.7 76.3 65.3 85.3 80.4 66.7 83.4 78.1\nOurs-B 68.8 85.0 86.9 81.5 87.1 87.3 79.6 63.3 88.2 82.0 66.0 90.6 80.5\nTable 2: Comparison with SoTA methods on Ofﬁce-Home. The best performance is marked asbold.\n4.2 C OMPARISON TO SOTA\nWe compare our method with state-of-the-art methods on UDA tasks, including MinEnt (Grandvalet\net al., 2005), DANN (Ganin & Lempitsky, 2015), CDAN+E (Long et al., 2018), CDAN+BSP (Chen\net al., 2019), CDAN+TN (Wang et al., 2019), rRGrad+CAT (Deng et al., 2019), MCD (Saito et al.,\n2018), SWD (Lee et al., 2019), MSTN+DSBN (Chang et al., 2019), SAFN+ENT (Xu et al., 2019),\nBNM (Cui et al., 2020), DCAN (Li et al., 2020), SHOT (Liang et al., 2020), ATDOC-NA (Liang\net al., 2021), CGDM (Du et al., 2021) and TVT (Yang et al.). The results are shown in Table 1, 2, 3\nand 4.\nFor Ofﬁce-Home, Ofﬁce-31 and DomainNet, as most of the methods use ResNet-50 as their\nbackbones, we provide results with DeiT-small as our backbone for a fair comparison, which has\na comparable model size as ResNet-50, but we also show the results using DeiT-base. And for\nVisDA-2017, we adopt the DeiT-base backbone for fair comparisons, where other methods utilize\nResNet-101 for their results.\nThe “Baseline-S/B” indicates directly training a DeiT-small/DeiT-base on the source domain and\ntesting on the target domain. The baseline shows a competitive result even compared to other SoTA\nmethods on most datasets. It demonstrates that Transformers has better generalization ability over\nConvNets. We also provide some insights about why transformers can generalize well from source\ndomain to target domain in the supplementary materials. To further eliminate the unfairness of using\ndifferent backbones, we reproduce the results of SHOT and CGDM (marked as “*”), and replace\ntheir backbones with DeiT-base as the same as ours, denoted as “-B*”.\n7\nPublished as a conference paper at ICLR 2022\nMCD clp inf pnt qdr rel skt Avg. CDANclp inf pnt qdr rel skt Avg. BNM clp inf pnt qdr rel skt Avg.\nclp - 15.4 25.5 3.3 44.6 31.2 24.0 clp - 13.5 28.3 9.3 43.8 30.2 25.0 clp - 12.1 33.1 6.2 50.8 40.2 28.5\ninf 24.1 - 24.0 1.6 35.2 19.7 20.9 inf 18.9 - 21.4 1.9 36.3 21.3 20.0 inf 26.6 - 28.5 2.4 38.5 18.1 22.8\npnt 31.1 14.8 - 1.7 48.1 22.8 23.7 pnt 29.6 14.4 - 4.1 45.2 27.4 24.2 pnt 39.9 12.2 - 3.4 54.5 36.2 29.2\nqdr 8.5 2.1 4.6 - 7.9 7.1 6.0 qdr 11.8 1.2 4.0 - 9.4 9.5 7.2 qdr 17.8 1.0 3.6 - 9.2 8.3 8.0\nrel 39.4 17.8 41.2 1.5 - 25.2 25.0 rel 36.4 18.3 40.9 3.4 - 24.6 24.7 rel 48.6 13.2 49.7 3.6 - 33.9 29.8\nskt 37.3 12.6 27.2 4.1 34.5 - 23.1 skt 38.2 14.7 33.9 7.0 36.6 - 26.1 skt 54.9 12.8 42.3 5.4 51.3 - 33.3\nAvg. 28.1 12.5 24.5 2.4 34.1 21.220.5 Avg. 27.0 12.4 25.7 5.1 34.3 22.621.2 Avg. 37.6 10.3 31.4 4.2 40.9 27.325.3\nSWD clp inf pnt qdr rel skt Avg. CGDMclp inf pnt qdr rel skt Avg. Base-Sclp inf pnt qdr rel skt Avg.\nclp - 14.7 31.9 10.1 45.3 36.5 27.7clp - 16.9 35.3 10.8 53.5 36.9 30.7clp - 21.2 44.2 15.3 59.9 46.0 37.3\ninf 22.9 - 24.2 2.5 33.2 21.3 20.0 inf 27.8 - 28.2 4.4 48.2 22.5 26.2 inf 36.8 - 39.4 5.4 52.1 32.6 33.3\npnt 33.6 15.3 - 4.4 46.1 30.7 26.0 pnt 37.7 14.5 - 4.6 59.4 33.5 30.0 pnt 47.1 21.7 - 5.7 60.2 39.9 34.9\nqdr 15.5 2.2 6.4 - 11.1 10.2 9.1 qdr 14.9 1.5 6.2 - 10.9 10.2 8.7 qdr 25.0 3.3 10.4 - 18.8 14.0 14.3\nrel 41.2 18.1 44.2 4.6 - 31.6 27.9 rel 49.4 20.8 47.2 4.8 - 38.2 32.0 rel 54.8 23.9 52.6 7.4 - 40.1 35.8\nskt 44.2 15.2 37.3 10.3 44.7 - 30.3 skt 50.1 16.5 43.7 11.1 55.6 - 35.4 skt 55.6 18.6 42.7 14.9 55.7 - 37.5\nAvg. 31.5 13.1 28.8 6.4 36.1 26.123.6 Avg. 36.0 14.0 32.1 7.1 45.5 28.327.2 Avg. 43.9 17.7 37.9 9.7 49.3 34.532.2\nOurs-Sclp inf pnt qdr rel skt Avg. Base-Bclp inf pnt qdr rel skt Avg. Ours-Bclp inf pnt qdr rel skt Avg.\nclp - 25.3 52.5 23.2 68.3 53.2 44.5clp - 24.2 48.9 15.5 63.9 50.7 40.6clp - 29.4 57.2 26.0 72.6 58.1 48.7\ninf 47.6 - 48.3 9.9 62.8 41.1 41.9 inf 43.5 - 44.9 6.5 58.8 37.6 38.3 inf 57.0 - 54.4 12.8 69.5 48.4 48.4\npnt 55.4 24.5 - 11.7 67.4 48.0 41.4 pnt 52.8 23.3 - 6.6 64.6 44.5 38.4 pnt 62.9 27.4 - 15.8 72.1 53.9 46.4\nqdr 36.6 5.3 19.3 - 33.8 22.7 23.5 qdr 31.8 6.1 15.6 - 23.4 18.9 19.2 qdr 44.6 8.9 29.0 - 42.6 28.5 30.7\nrel 61.5 28.1 56.8 12.8 - 47.2 41.3 rel 58.9 26.3 56.7 9.1 - 45.0 39.2 rel 66.2 31.0 61.5 16.2 - 52.9 45.6\nskt 64.3 26.1 53.2 23.9 66.2 - 46.7 skt 60.0 21.1 48.4 16.6 61.7 - 41.6 skt 69.0 29.6 59.0 27.2 72.5 - 51.5\nAvg. 53.1 21.9 46.0 16.3 59.7 42.439.9 Avg. 49.4 20.2 42.9 10.9 54.5 39.336.2 Avg. 59.9 25.3 52.2 19.6 65.9 48.445.2\nTable 4: Comparison with SoTA methods on DomainNet. “Base” is the Baseline.\nMethod A→D A→W D→A D→W W→A W→D Avg\nResNet-50 68.9 68.4 62.5 96.7 60.7 99.3 76.1\nDANN 79.7 82.0 68.2 96.9 67.4 99.1 82.2\nCDAN+E 92.9 94.1 71.0 98.6 69.3 100. 87.7\nrRGrad+CAT 90.8 94.4 72.2 98.0 70.2 100. 87.6\nSAFN+ENT 90.7 90.1 73.0 98.6 70.2 99.8 87.1\nCDAN+BSP 93.0 93.3 73.6 98.2 72.6 100. 88.5\nCDAN+TN 94.0 95.7 73.4 98.7 74.2 100. 89.3\nSHOT 94.0 90.1 74.7 98.4 74.3 99.9 88.6\nSHOT* 93.8 91.8 74.8 98.2 74.1 99.8 88.8\nTVT◦ 96.4 96.4 84.9 99.4 86.1 100. 93.8\nBaseline-S 87.6 86.9 74.9 97.7 73.5 99.6 86.7\nOurs-S 94.6 93.5 78.4 98.2 78.0 99.6 90.4\nBaseline-B 90.8 90.4 76.8 98.2 76.4 100. 88.8\nCGDM-B* 94.6 95.3 78.8 97.6 81.2 99.8 91.2\nSHOT-B* 95.3 94.3 79.4 99.0 80.2 100. 91.4\nOurs-B 97.0 96.7 81.1 99.0 81.9 100. 92.6\nTable 3: Comparison with SoTA methods on Ofﬁce-31.\nThe best performance is marked as bold.\nFrom Table 1, 2, 3 and 4, it can be seen\nthat our method outperforms the baseline\nwith a large margin on all four datasets,\ne.g. nearly 21% on VisDA. With our\nimprovements, the new Transformer with\ncross-attention module shows a much\nbetter generalization power, and achieves\nthe best performance on VisDA-2017\ncompared to other SoTAs methods. It\nfurther implies the effectiveness of our\nmethod on the UDA task.\nTaking a closer look at the results, for\nthe hard categories, such as “person” in\nVisDA-2017 dataset, the baseline is very\nlow, which indicates the initial model\nof our method has a poor classiﬁcation\nability on this category, leading to the\npseudo labels with more noise. Even with\nsuch a poor baseline and poor quality of pseudo labels, our method can still achieve a much higher\nperformance boost (from 10.3% to 88.6%). It suggests that our method has a great robustness to the\nlabeling noise and can overcome the noise problem to some extent.\nWe can see that TVT achieves a better result on Ofﬁce-Home and Ofﬁce-31. Because TVT\nutilizes ViT (Dosovitskiy et al., 2020) as backbone which is pretrained on ImageNet21K. While\nthe pretrained model of our CDTrans and other UDA methods are trained on ImageNet1K.\n4.3 A BLATION STUDY\n4.3.1 D IFFERENT PSEUDO LABELING\nWe have conducted experiments on different pseudo labeling methods to verify their inﬂuence on the\nﬁnal performance. The results on VisDa-2017 are listed in Table. 5. RPLL (Zheng & Yang, 2021)\nand MRKLD+LRENT (Zou et al., 2019) are two commonly used pseudo-label generation methods,\nwe reproduce their pseudo-label generations on our baseline to compare with our proposed pseudo\nlabeling method. Recs,Rect means the recall of the selected training pairs in the source and target\ndata, while Prec represents the accuracy of the pairs. “One-way-source” and “One-way-target”\ndenote only using the pair set PS in Eq. 3 or PT in Eq. 4 for training. “Two-way” indicates results\n8\nPublished as a conference paper at ICLR 2022\nPseudo labels Recs Rect Prec plane bcycl bus car horse knife mcycl person plant sktbrd train truck Avg.\nOne-way-source 100. 6.6 90.6 96.1 52.7 85.5 69.6 95.0 90.2 95.1 66.6 88.8 54.6 95.4 29.5 76.6\nOne-way-target 8.0 100. 76.3 98.2 32.0 87.7 84.1 95.5 89.9 98.3 66.8 95.7 57.5 95.6 22.0 76.9\nTwo-way 100. 100. 81.8 97.5 49.6 88.7 73.9 94.6 85.8 96.6 58.6 93.3 63.6 94.8 27.9 77.1\nTw + Ca 97.8 94.8 91.3 98.1 86.9 87.9 80.9 97.9 97.3 96.8 85.3 97.6 83.2 94.0 54.4 88.4\nRPLL - - - 98.4 63.4 85.8 68.8 97.0 95.4 97.77 59.3 96.2 57.2 96.2 48.1 80.3\nMRKLD+LRENT - - - 97.8 77.3 81.4 64.3 94.6 93.9 93.3 77.5 93.1 74.9 92.6 59.0 83.3\nGroundtruth 100. 100. 100. 97.9 89.1 92.3 91.9 98.4 97.2 97.5 86.8 98.6 90.7 96.3 60.0 91.5\nTable 5: Comparison among different pseudo labeling methods on VisDa-2017.Recs,Rect express\nthe recall of pseudo labels in source and target data, whilePrec represents the accuracy of the pairs.\n“One-way-source/target” denotes only using the source/target pair set for training. “Tw+Ca” implies\nthe proposed two-way center-aware labeling method.\nLs Ls+t Lt plane bcycl bus car horse knife mcycl person plant sktbrd train truck Avg.\ncls - - 97.7 48.1 86.6 61.6 78.1 63.4 94.7 10.3 87.7 47.7 94.4 35.5 67.1\n- - cls 98.3 85.0 88.0 76.3 98.1 96.1 96.9 61.1 97.2 85.5 94.6 54.9 86.0\ncls - cls 98.3 87.4 89.1 77.3 98.0 97.4 95.4 69.5 97.1 86.3 95.3 49.5 86.7\ncls cls cls 98.2 88.4 88.0 76.8 98.2 97.2 95.6 80.1 97.1 84.7 94.5 54.1 87.7\ncls dtl cls 98.0 86.9 87.9 80.9 97.9 97.3 96.8 85.3 97.6 83.2 94.0 54.4 88.4\nTable 6: Comparison among different losses on VisDa-2017.Ls,Lt and Ls+t represent the loss used\nin source, target and source+target branches respectively. cls and dtl imply the classiﬁcation loss\nand the distillation loss.\nof using the union of Ps and Pt without the center-aware strategy. “Tw+Ca” implies our two-way\ncenter-aware labeling method, and “Groundtruth” means all training pairs are from groundtruthes.\nBy looking at Recs and Rect in Table. 5, it can be found that the one-way methods have an apparent\nbias on either source or target data, and its results are lower than the two-way method. By comparing\n“Two-way” and “Tw+Ca”, we can conclude that although the center-aware method ﬁlters the training\npair and slightly reduces the recall, it largely improves the precision and leads to a better ﬁnal\nperformance. We also ﬁnd that our two-way center-aware labeling method achieves a very high\nresult, not only better than other pseudo-label generation methods, but also very close to the upper\nbound trained with groundtruth pairs.\n4.3.2 D IFFERENT LOSSES\nAs there are three losses in our method, we conduct another experiment to verify the effectiveness of\neach loss on VisDa-2017, as shown in Table. 6. “cls” inLs+t denotes that we replace the distillation\nloss with a classiﬁcation loss for the source-target branch. We can see that the 3rd row with both\nLs and Lt having classiﬁcation loss achieves a better result than the ﬁrst row where only Ls has\nthe cls loss, which means the target branch with the pseudo labels is helpful to improve the UDA\nresult. With the addition of “cls” in Ls+t, the performance is further improved, which demonstrates\nthe advantages of using the cross-attention module for feature alignment. Using “dtl” instead of\n“cls” on the source-target branch can further improve the results, showing the effectiveness of our\ndistillation loss.\n5 C ONCLUSION\nIn this paper, we tackle the problem of unsupervised domain adaptation by introducing the cross-\nattention module into Transformer in a novel way. We propose a new network structure CDTrans\nwhich is a pure transformer-based structure with three branches, and we also propose to generate\nhigh-quality pseudo labels using a two-way center-aware labeling method. Training CDTrans using\nthe generated high-quality pseudo labels yields a robust solution and also achieves state-of-the-art\nresults on four popular UDA datasets, outperforming previous methods by a large margin. We\nbelieve that transformer-based approaches will have great potential in the UDA community, and our\nwork, as one of the ﬁrst attempts along this direction, has pushed forward the frontiers and shed\nlights for future research.\n9\nPublished as a conference paper at ICLR 2022\nREFERENCES\nKonstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, and Dilip Krishnan.\nUnsupervised pixel-level domain adaptation with generative adversarial networks. In CVPR,\n2017.\nMathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for\nunsupervised learning of visual features. In ECCV, 2018.\nWoong-Gi Chang, Tackgeun You, Seonguk Seo, Suha Kwak, and Bohyung Han. Domain-speciﬁc\nbatch normalization for unsupervised domain adaptation. In CVPR, 2019.\nBoyu Chen, Peixia Li, Baopu Li, Chuming Li, Lei Bai, Chen Lin, Ming Sun, Junjie Yan, and Wanli\nOuyang. Psvit: Better vision transformer via token pooling and attention sharing. arXiv preprint\narXiv:2108.03428, 2021a.\nKai Chen, Weihua Chen, Tao He, Rong Du, Fan Wang, Xiuyu Sun, Yuchen Guo, and Guiguang\nDing. Tagperson: A target-aware generation pipeline for person re-identiﬁcation. arXiv preprint\narXiv:2112.14239, 2021b.\nWeihua Chen, Xiaotang Chen, Jianguo Zhang, and Kaiqi Huang. Beyond triplet loss: a deep\nquadruplet network for person re-identiﬁcation. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp. 403–412, 2017.\nXinyang Chen, Sinan Wang, Mingsheng Long, and Jianmin Wang. Transferability vs.\ndiscriminability: Batch spectral penalization for adversarial domain adaptation. In ICML, 2019.\nJaehoon Choi, Minki Jeong, Taekyung Kim, and Changick Kim. Pseudo-labeling curriculum for\nunsupervised domain adaptation. arXiv preprint arXiv:1908.00262, 2019.\nGabriela Csurka. Domain adaptation for visual applications: A comprehensive survey. arXiv\npreprint arXiv:1702.05374, 2017.\nShuhao Cui, Shuhui Wang, Junbao Zhuo, Liang Li, Qingming Huang, and Qi Tian. Towards\ndiscriminability and diversity: Batch nuclear-norm maximization under label insufﬁcient\nsituations. In CVPR, 2020.\nZhijie Deng, Yucen Luo, and Jun Zhu. Cluster alignment with a teacher for unsupervised domain\nadaptation. In ICCV, 2019.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An\nimage is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\nZhekai Du, Jingjing Li, Hongzu Su, Lei Zhu, and Ke Lu. Cross-domain gradient discrepancy\nminimization for unsupervised domain adaptation. In CVPR, 2021.\nYaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In\nICML, 2015.\nMuhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, David Balduzzi, and Wen Li. Deep\nreconstruction-classiﬁcation networks for unsupervised domain adaptation. In ECCV, 2016.\nYves Grandvalet, Yoshua Bengio, et al. Semi-supervised learning by entropy minimization. CAP,\n367:281–296, 2005.\nArthur Gretton, Karsten Borgwardt, Malte Rasch, Bernhard Sch ¨olkopf, and Alex Smola. A kernel\nmethod for the two-sample-problem. NIPS, 2006.\n10\nPublished as a conference paper at ICLR 2022\nKai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang,\nAn Xiao, Chunjing Xu, Yixing Xu, et al. A survey on visual transformer. arXiv preprint\narXiv:2012.12556, 2020.\nKai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in\ntransformer. arXiv preprint arXiv:2103.00112, 2021a.\nLiang Han, Pichao Wang, Zhaozheng Yin, Fan Wang, and Hao Li. Context and structure mining\nnetwork for video object detection. IJCV, 2021b.\nShuting He, Hao Luo, Pichao Wang, Fan Wang, Hao Li, and Wei Jiang. Transreid: Transformer-\nbased object re-identiﬁcation. arXiv preprint arXiv:2102.04378, 2021.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531, 2015.\nJudy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros,\nand Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In ICML, 2018.\nRonghang Hu and Amanpreet Singh. Unit: Multimodal multitask learning with a uniﬁed\ntransformer. arXiv preprint arXiv:2102.10772, 2021.\nXiang Jiang, Qicheng Lao, Stan Matwin, and Mohammad Havaei. Implicit class-conditioned\ndomain alignment for unsupervised domain adaptation. In ICML, 2020.\nYiqi Jiang, Weihua Chen, Xiuyu Sun, Xiaoyu Shi, Fan Wang, and Hao Li. Exploring the quality of\ngan generated images for person re-identiﬁcation. In Proceedings of the 29th ACM International\nConference on Multimedia, pp. 4146–4155, 2021.\nGuoliang Kang, Lu Jiang, Yi Yang, and Alexander G Hauptmann. Contrastive adaptation network\nfor unsupervised domain adaptation. In CVPR, 2019.\nSalman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and\nMubarak Shah. Transformers in vision: A survey. arXiv preprint arXiv:2101.01169, 2021.\nSeiichi Kuroki, Nontawat Charoenphakdee, Han Bao, Junya Honda, Issei Sato, and Masashi\nSugiyama. Unsupervised domain adaptation based on source-guided discrepancy. In AAAI, 2019.\nChen-Yu Lee, Tanmay Batra, Mohammad Haris Baig, and Daniel Ulbricht. Sliced wasserstein\ndiscrepancy for unsupervised domain adaptation. In CVPR, 2019.\nDong-Hyun Lee et al. Pseudo-label: The simple and efﬁcient semi-supervised learning method for\ndeep neural networks. In Workshop on challenges in representation learning, ICML, 2013.\nJichang Li, Guanbin Li, Yemin Shi, and Yizhou Yu. Cross-domain adaptive clustering for semi-\nsupervised domain adaptation. In CVPR, 2021a.\nNaihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming Liu. Neural speech synthesis with\ntransformer network. In AAAI, 2019.\nShuai Li, Jianqiang Huang, Xian-Sheng Hua, and Lei Zhang. Category dictionary guided\nunsupervised domain adaptation for object detection. In AAAI, 2021b.\nShuang Li, Chi Liu, Qiuxia Lin, Binhui Xie, Zhengming Ding, Gao Huang, and Jian Tang. Domain\nconditioned adaptation network. In AAAI, volume 34, pp. 11386–11393, 2020.\nWenhao Li, Hong Liu, Runwei Ding, Mengyuan Liu, and Pichao Wang. Lifting transformer for 3d\nhuman pose estimation in video. arXiv preprint arXiv:2103.14304, 2021c.\nXiangyu Li, Yonghong Hou, Pichao Wang, Zhimin Gao, Mingliang Xu, and Wanqing Li.\nTransformer guided geometry model for ﬂow-based unsupervised visual odometry. Neural\nComputing and Applications, 2021d.\nXiangyu Li, Yonghong Hou, Pichao Wang, Zhimin Gao, Mingliang Xu, and Wanqing Li. Trear:\nTransformer-based rgb-d egocentric action recognition. T-CDS, 2021e.\n11\nPublished as a conference paper at ICLR 2022\nJian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source\nhypothesis transfer for unsupervised domain adaptation. In ICML, 2020.\nJian Liang, Dapeng Hu, and Jiashi Feng. Domain adaptation with auxiliary target domain-oriented\nclassiﬁer. In CVPR, pp. 16632–16642, 2021.\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint\narXiv:2103.14030, 2021.\nMingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with\ndeep adaptation networks. In ICML, 2015.\nMingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Deep transfer learning with joint\nadaptation networks. In ICML, 2017.\nMingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial\ndomain adaptation. NeurIPS, 2018.\nPietro Morerio, Riccardo V olpi, Ruggero Ragonesi, and Vittorio Murino. Generative pseudo-label\nreﬁnement for unsupervised domain adaptation. In WACV, 2020.\nPoojan Oza, Vishwanath A Sindagi, Vibashan VS, and Vishal M Patel. Unsupervised domain\nadaption of object detectors: A survey. arXiv preprint arXiv:2105.13502, 2021.\nXingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko.\nVisda: The visual domain adaptation challenge. arXiv preprint arXiv:1710.06924, 2017.\nXingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching\nfor multi-source domain adaptation. In ICCV, 2019.\nYichen Qian, Zhiyu Tan, Xiuyu Sun, Ming Lin, Dongyang Li, Zhenhong Sun, Hao Li, and Rong\nJin. Learning accurate entropy model with global reference for image compression. 2021.\nYichen Qian, Xiuyu Sun, Ming Lin, Zhiyu Tan, and Rong Jin. Entroformer: A transformer-based\nentropy model for learned image compression. arXiv preprint arXiv:2202.05492, 2022.\nRen´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction.\narXiv preprint arXiv:2103.13413, 2021.\nKate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to new\ndomains. In ECCV, 2010.\nKuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada. Maximum classiﬁer\ndiscrepancy for unsupervised domain adaptation. In CVPR, 2018.\nBaochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In\nECCV, 2016.\nBaochen Sun, Jiashi Feng, and Kate Saenko. Return of frustratingly easy domain adaptation. In\nAAAI, 2016.\nZhiyu Tan, Xuecheng Nie, Qi Qian, Nan Li, and Hao Li. Learning to rank proposals for object\ndetection. In ICCV, 2019.\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and\nHerv´e J ´egou. Training data-efﬁcient image transformers & distillation through attention. In\nICML, pp. 10347–10357, 2021.\nYao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J Zico Kolter, Louis-Philippe Morency, and\nRuslan Salakhutdinov. Multimodal transformer for unaligned multimodal language sequences. In\nACL, 2019.\nEric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain confusion:\nMaximizing for domain invariance. arXiv preprint arXiv:1412.3474, 2014.\n12\nPublished as a conference paper at ICLR 2022\nEric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain\nadaptation. In CVPR, 2017.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.\nHemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep\nhashing network for unsupervised domain adaptation. In CVPR, 2017.\nVibashan VS, Vikram Gupta, Poojan Oza, Vishwanath A Sindagi, and Vishal M Patel. Mega-cda:\nMemory guided attention for category-aware unsupervised domain adaptive object detection. In\nCVPR, 2021.\nWenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo,\nand Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without\nconvolutions. arXiv preprint arXiv:2102.12122, 2021.\nXimei Wang, Ying Jin, Mingsheng Long, Jianmin Wang, and Michael Jordan. Transferable\nnormalization: Towards improving transferability of deep neural networks. 2019.\nYaohua Wang, Yaobin Zhang, Fangyi Zhang, Senzhang Wang, Ming Lin, YuQi Zhang, and Xiuyu\nSun. Ada-nets: Face clustering via adaptive neighbour discovery in the structure space. arXiv\npreprint arXiv:2109.06165, 2022.\nGarrett Wilson and Diane J Cook. A survey of unsupervised deep domain adaptation. TIST, 2020.\nRuijia Xu, Guanbin Li, Jihan Yang, and Liang Lin. Larger norm more transferable: An adaptive\nfeature norm approach for unsupervised domain adaptation. In iccv, pp. 1426–1435, 2019.\nGuanglei Yang, Hao Tang, Zhun Zhong, Mingli Ding, Ling Shao, Nicu Sebe, and Elisa Ricci.\nTransformer-based source-free domain adaptation. arXiv preprint arXiv:2105.14138, 2021.\nJinyu Yang, Jingjing Liu, Ning Xu, and Junzhou Huang. Tvt: Transferable vision transformer for\nunsupervised domain adaptation. arXiv preprint arXiv:2108.05988.\nJunyan Wang Xiuyu Sun Ming Lin Hao Li Yiqi Jiang, Zhiyu Tan. Giraffedet: A heavy-neck\nparadigm for object detection. arXiv preprint arXiv:2202.04256, 2022.\nZitong Yu, Xiaobai Li, Pichao Wang, and Guoying Zhao. Transrppg: Remote photoplethysmogra-\nphy transformer for 3d mask face presentation attack detection. IEEE SPL, 2021.\nLi Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi Feng, and\nShuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet.\narXiv preprint arXiv:2101.11986, 2021a.\nLi Yuan, Qibin Hou, Zihang Jiang, Jiashi Feng, and Shuicheng Yan. V olo: Vision outlooker for\nvisual recognition. arXiv preprint arXiv:2106.13112, 2021b.\nQiming Zhang, Jing Zhang, Wei Liu, and Dacheng Tao. Category anchor-guided unsupervised\ndomain adaptation for semantic segmentation. NIPS, 2019.\nWeichen Zhang, Wanli Ouyang, Wen Li, and Dong Xu. Collaborative and adversarial network for\nunsupervised domain adaptation. In CVPR, 2018.\nYabin Zhang, Bin Deng, Hui Tang, Lei Zhang, and Kui Jia. Unsupervised multi-class domain\nadaptation: Theory, algorithms, and practice. PAMI, 2020.\nSicheng Zhao, Xiangyu Yue, Shanghang Zhang, Bo Li, Han Zhao, Bichen Wu, Ravi Krishna,\nJoseph E Gonzalez, Alberto L Sangiovanni-Vincentelli, Sanjit A Seshia, et al. A review of single-\nsource deep unsupervised visual domain adaptation. T-NNLS, 2020.\nZhedong Zheng and Yi Yang. Rectifying pseudo label learning via uncertainty estimation for domain\nadaptive semantic segmentation. IJCV, 2021.\n13\nPublished as a conference paper at ICLR 2022\nYang Zou, Zhiding Yu, BVK Kumar, and Jinsong Wang. Unsupervised domain adaptation for\nsemantic segmentation via class-balanced self-training. In ECCV, 2018.\nYang Zou, Zhiding Yu, Xiaofeng Liu, BVK Kumar, and Jinsong Wang. Conﬁdence regularized\nself-training. In ICCV, 2019.\n14",
  "topic": "Discriminative model",
  "concepts": [
    {
      "name": "Discriminative model",
      "score": 0.7544819116592407
    },
    {
      "name": "Computer science",
      "score": 0.7243383526802063
    },
    {
      "name": "Transformer",
      "score": 0.7095300555229187
    },
    {
      "name": "Domain adaptation",
      "score": 0.5968960523605347
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5618252754211426
    },
    {
      "name": "Feature learning",
      "score": 0.5460746884346008
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5168235898017883
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.504557728767395
    },
    {
      "name": "Machine learning",
      "score": 0.34537625312805176
    },
    {
      "name": "Classifier (UML)",
      "score": 0.10839468240737915
    },
    {
      "name": "Voltage",
      "score": 0.09362196922302246
    },
    {
      "name": "Engineering",
      "score": 0.08275073766708374
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": []
}