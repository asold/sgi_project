{
  "title": "Is a Large Language Model a Good Annotator for Event Extraction?",
  "url": "https://openalex.org/W4393147108",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2140581722",
      "name": "Ruirui Chen",
      "affiliations": [
        "Agency for Science, Technology and Research"
      ]
    },
    {
      "id": "https://openalex.org/A2521262500",
      "name": "Chengwei Qin",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A2162187183",
      "name": "Weifeng Jiang",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A2100590235",
      "name": "Dongkyu Choi",
      "affiliations": [
        "Agency for Science, Technology and Research"
      ]
    },
    {
      "id": "https://openalex.org/A2140581722",
      "name": "Ruirui Chen",
      "affiliations": [
        "Agency for Science, Technology and Research",
        "Institute of High Performance Computing"
      ]
    },
    {
      "id": "https://openalex.org/A2521262500",
      "name": "Chengwei Qin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2162187183",
      "name": "Weifeng Jiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2100590235",
      "name": "Dongkyu Choi",
      "affiliations": [
        "Agency for Science, Technology and Research",
        "Institute of High Performance Computing"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6677337689",
    "https://openalex.org/W2250999640",
    "https://openalex.org/W6810081322",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W6850503672",
    "https://openalex.org/W3021280023",
    "https://openalex.org/W4313918056",
    "https://openalex.org/W4224912544",
    "https://openalex.org/W6838865847",
    "https://openalex.org/W2147880316",
    "https://openalex.org/W3101701554",
    "https://openalex.org/W6786144006",
    "https://openalex.org/W2512522169",
    "https://openalex.org/W3172768590",
    "https://openalex.org/W4378474088",
    "https://openalex.org/W6731596507",
    "https://openalex.org/W4385570525",
    "https://openalex.org/W2972140869",
    "https://openalex.org/W2946760275",
    "https://openalex.org/W3021222499",
    "https://openalex.org/W2971148526",
    "https://openalex.org/W6796825874",
    "https://openalex.org/W6838461927",
    "https://openalex.org/W2952437275",
    "https://openalex.org/W4304194220",
    "https://openalex.org/W4311001833",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4288112774",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W2572149427",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3106098584",
    "https://openalex.org/W4381252438",
    "https://openalex.org/W4401023880",
    "https://openalex.org/W4389523957",
    "https://openalex.org/W3102925419",
    "https://openalex.org/W3104597568",
    "https://openalex.org/W4386076314",
    "https://openalex.org/W4385573364",
    "https://openalex.org/W3174945605",
    "https://openalex.org/W4323572061",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4213168938",
    "https://openalex.org/W3176032431",
    "https://openalex.org/W4389520264"
  ],
  "abstract": "Event extraction is an important task in natural language processing that focuses on mining event-related information from unstructured text. Despite considerable advancements, it is still challenging to achieve satisfactory performance in this task, and issues like data scarcity and imbalance obstruct progress. In this paper, we introduce an innovative approach where we employ Large Language Models (LLMs) as expert annotators for event extraction. We strategically include sample data from the training dataset in the prompt as a reference, ensuring alignment between the data distribution of LLM-generated samples and that of the benchmark dataset. This enables us to craft an augmented dataset that complements existing benchmarks, alleviating the challenges of data imbalance and scarcity and thereby enhancing the performance of fine-tuned models. We conducted extensive experiments to validate the efficacy of our proposed method, and we believe that this approach holds great potential for propelling the development and application of more advanced and reliable event extraction systems in real-world scenarios.",
  "full_text": "Is a Large Language Model a Good Annotator for Event Extraction?\nRuirui Chen1, Chengwei Qin2, Weifeng Jiang2, Dongkyu Choi1\n1Institute of High Performance Computing (IHPC),\nAgency for Science, Technology and Research (A*STAR)\n1 Fusionopolis Way, #16-16 Connexis, Singapore 138632, Republic of Singapore\n2School of Computer Science and Engineering, Nanyang Technological University\n50 Nanyang Avenue, Singapore 639798, Republic of Singapore\nchen ruirui@ihpc.a-star.edu.sg, {chengwei003, s220077}@e.ntu.edu.sg, choi dongkyu@ihpc.a-star.edu.sg\nAbstract\nEvent extraction is an important task in natural language\nprocessing that focuses on mining event-related information\nfrom unstructured text. Despite considerable advancements,\nit is still challenging to achieve satisfactory performance in\nthis task, and issues like data scarcity and imbalance obstruct\nprogress. In this paper, we introduce an innovative approach\nwhere we employ large language models (LLMs) as expert\nannotators for event extraction. We strategically include sam-\nple data from the training dataset in the prompt as a reference,\nensuring alignment between the data distribution of LLM-\ngenerated samples and that of the benchmark dataset. This\nenables us to craft an augmented dataset that complements\nexisting benchmarks, alleviating the challenges of data im-\nbalance and scarcity and thereby enhancing the performance\nof fine-tuned models. We conducted extensive experiments\nto validate the efficacy of our proposed method, and we be-\nlieve that this approach holds great potential for propelling\nthe development and application of more advanced and reli-\nable event extraction systems in real-world scenarios.\nIntroduction\nEvent extraction is a long-standing and crucial task within\nthe natural language processing (NLP) community, aimed at\nextracting specific events from sentences or documents. This\ntask is generally divided into two subtasks: event detection\n(ED) and event argument extraction (EAE). The former in-\nvolves identifying trigger words or phrases and determining\nthe event types they belong to, while the latter aims to extract\nspecific information about the events identified in the previ-\nous stage, such as the location or the involved individuals of\nthe events. Despite extensive previous work, the latest per-\nformance of event extraction still falls short of expectations.\nWhile sentence-level ED has achieved an impressive 80%\nsuccess rate (Yang et al. 2019), sentence-level EAE lags be-\nhind at 60% (Wang et al. 2021) even assuming that the enti-\nties are already given.\nOne reason for the unsatisfactory performance is the in-\nsufficient availability of labeled data, particularly for EAE.\nFor instance, ACE 2005 (Walker et al. 2006), a widely used\nsentence-level event extraction dataset, provides annotations\nfor 33 distinct event types, but it has an alarmingly low\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nnumber of labeled samples for some event types like ”Jus-\ntice.Pardon”, with fewer than five in certain cases. While the\nMA VEN dataset (Wang et al. 2020) was introduced to ad-\ndress this shortage, it does not include argument labeling,\nand the long-tail distribution of events persists. These issues\ncomplicate effective training of models on some events.\nRecently, large language models (LLMs) achieved re-\nmarkable success across various NLP tasks. However, cer-\ntain deficiencies persist, as evidenced in tasks like abstract\nreasoning (Gendron et al. 2023) and named entity recogni-\ntion (Qin et al. 2023). Given that event extraction is an in-\nherently more challenging task that requires the extraction\nof complex structured data, we expected LLMs to face diffi-\nculties in this task. To check this, we tested LLMs for event\nextraction and evaluated their performance in zero-shot and\none-shot settings. Even when provided with an example of\neach event type in the prompt, the LLMs’ capability to iden-\ntify events within a given sentence did not meet our require-\nments. We ascribe this shortfall to several factors, including:\n• Variations in Annotation Understanding: LLMs may\ninterpret the annotation differently from the ground truth\nannotators of the specific dataset, leading to discrepan-\ncies in event type identification.\n• Semantic Limitations: Some semantic nuances and con-\ntextual information may not be adequately captured by\nLLMs at the sentence level, hindering their accurate iden-\ntification of event types.\n• Undefined Event Types or Augmented Arguments:\nEven when limited to a predefined list of event types,\nLLMs might produce undefined event types that bear\nsimilar meanings. Furthermore, they might provide a\nmore detailed text rather than a succinct one.\n• Deviation in Output Format: The outputs from LLMs\nmay not always conform to the specific format, resulting\nin the omission of certain accurate predictions.\nIn light of these limitations, we opted to pivot our ap-\nproach away from exclusively relying on pre-trained LLMs\nfor event extraction. We devised a novel strategy by utiliz-\ning LLMs to annotate additional data, effectively tackling\nthe data scarcity issue. Our contributions encompass three\nkey aspects:\n• Extensive Evaluation: We conducted a comprehensive\nevaluation of popular LLMs on commonly used bench-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17772\nmark datasets, highlighting the strengths and weaknesses\nof LLMs for event extraction.\n• LLM-based Annotation Approach: We propose a\nnovel idea of leveraging LLMs to annotate more data,\nwhich we subsequently used for fine-tuning models, and\ndemonstrated its effectiveness in improving event extrac-\ntion performance.\n• Dataset Publication: We mitigate the long-tail problem\nby releasing annotated samples expanded through our\nuse of LLMs on widely adopted datasets. This facilitates\ntraining with a greater number of annotated samples.1\nIn the next few sections, we will first review some related\nwork and describe our proposed method in detail. Then, we\nwill explain the setup for our experiments and present a com-\nprehensive analysis of our experimental results. Finally, we\nwill have a summarizing discussion before we conclude.\nRelated Work\nLarge Language Models\nIn recent years, LLMs have captured significant attention\nacross various domains. The emergence of ChatGPT, in par-\nticular, has amplified interest across diverse research realms\nencompassing natural language processing (Qin et al. 2023),\ncomputer vision (Zhao et al. 2023), and robotics (Ichter et al.\n2022; Driess et al. 2023). These models showcase impres-\nsive capabilities in zero-shot and few-shot scenarios (Wei\net al. 2022a), particularly when coupled with the chain-of-\nthought methodology (Wei et al. 2022b; Kojima et al. 2022;\nZhang et al. 2023). Nevertheless, a discernible disparity ap-\npears to persist between the performance of LLMs and fine-\ntuned models in specific research domains, such as named\nentity recognition (Qin et al. 2023).\nEvent Extraction\nEvent extraction has long been a focal point in the field of\ninformation extraction, receiving substantial attention over\ndecades. It spans both sentence-level and document-level\nextractions, tackling the complexity of unstructured text.\nWithin its scope, this task inherently involves two main sub-\ntasks: ED and EAE (Wang et al. 2021). To date, there are\ngenerally four categories of event extraction methods (Peng\net al. 2023): classification methods (Chen et al. 2015), se-\nquence labeling methods (Wang et al. 2020), span predic-\ntion methods (Du and Cardie 2020), and conditional gener-\nation methods (Lu et al. 2021). However, the progress has\nbeen significantly hindered by the persistent challenge of\ndata scarcity. This is particularly evident in popular bench-\nmark datasets like ACE 2005 (Walker et al. 2006) where\nsome event types have fewer than ten labeled instances. In\nan effort to address this, MA VEN (Wang et al. 2020) was\nintroduced in 2020, providing a substantially larger dataset,\nthough still struggling with long-tail data distribution and a\nlack of argument annotations.\n1https://github.com/shiqinghuayi19/LLMforEvent\nData Augmentation\nThe pursuit of improvement has extended beyond the cre-\nation of novel datasets. For example, the study by Liu et al.\n(2016) utilizes FrameNet (Baker, Fillmore, and Lowe 1998)\nto bolster ED performance. In a unique approach, PLMEE\n(Yang et al. 2019) generates labeled data through iterative\nedits to prototypes and then selects samples based on their\nquality. RCEE\nER (Liu et al. 2020) reframes event extrac-\ntion as a machine reading comprehension (MRC) task, tap-\nping into the capabilities of advanced MRC methods and ex-\ntensive externally annotated MRC data. And CLEVE (Wang\net al. 2021) introduces a contrastive pre-training framework\nfor event extraction, aiming to more effectively extract event\nknowledge from expansive unsupervised data sets.\nThis research prompts the question of whether there might\nbe a more direct way to gather labeled data to enhance\ndata augmentation. Could a strategy be formulated to au-\ntonomously create labeled datasets for event types with\nscarce annotations? Remarkably, the answer is affirmative,\nfacilitated by the advanced capabilities of modern LLMs.\nAs outlined by Gao et al. (2022), data augmentation meth-\nods generally fall into two categories: modifying existing ex-\namples and generating new data. The STAR model proposed\nby Ma et al. (2023a) introduces a method for generating new\ndata using LLMs based on a given event structure to enhance\nlow-resource information extraction performance. In our ap-\nproach, we also adopt the latter strategy, but without con-\nstraints on the event structure, aiming to maintain diversity.\nThe Proposed Method\nIn this section, we introduce LLM-based techniques for\nevent extraction, which fall into two main categories. The\nfirst strategy involves directly using LLMs by prompting\nthem to extract event information from sentences. The sec-\nond leverages LLMs’ capabilities to create new labeled sam-\nples, thereby improving the performance of fine-tuned ex-\ntraction methods. Together, these approaches highlight the\ndiverse and integral roles that LLMs can fulfill in advancing\nthe state of the art.\nPrompting LLMs for Event Extraction\nWhile there has been some research in open event extraction\n(Nguyen et al. 2016), the majority of studies have focused on\ndatasets that adhere to specific schemas, such as ACE 2005\n(Walker et al. 2006). In this study, we explore a novel di-\nmension by testing the zero-shot and one-shot performance\nof LLMs on the task, assessing how the provision of specific\nexamples influences the outcomes. We further examine both\njoint and pipeline methods to determine whether extracting\ntriggers and arguments simultaneously or in separate stages\naffects the accuracy of the process.\nSpecifically, our exploration is divided into three main\ncomponents as follows:\n• Joint and Pipeline Event Extraction: We conducted\nexperiments on both joint event extraction and pipeline\nevent extraction. In the joint approach, we instruct the\nLLMs to simultaneously identify event types, triggers,\nand arguments. Conversely, in the pipeline approach, we\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17773\nPrompt for GPT-4\nCould you create a succinct sentence that includes the \nJustice.Acquit event and then annotate it as per the provided \nexamples?\nsentence: Wouter Basson was acquitted in April 2002 on 46 \ncharges, ranging from murder and drug trafficking to fraud \nand theft.\nevent_type: Justice.Acquit\ntrigger: acquitted\ndefendant: Wouter Basson\nadjudicator: None\nplace: None\nPrompt for PaLM and GPT-3.5-Turbo\nCould you create a new and unique sentence that includes the \nIngestion event, different from the example I've provided and \nthen annotate it as per the provided examples?\nThe format is as follows \nsentence:...\nevent_type:...\ntrigger:...\nThe example is as follows\nsentence: It weakened further and was absorbed by a \nstationary trough near the South Island on March 12.\nevent_type: Ingestion\ntrigger: absorbed\nFigure 1: Prompts utilized for guiding LLMs during the data annotation task.\nadopt a two-stage strategy: first asking the LLMs to de-\ntermine the event types and triggers, and then prompting\nthem to return the corresponding arguments, given the\nsentence and event type information.\n• Zero-shot and One-shot Event Extraction: We exper-\nimented with both zero-shot and one-shot event extrac-\ntion. Since most datasets follow specific schemas, pro-\nviding a specific example from the training dataset as a\nreference can be helpful for guiding the LLMs’ extrac-\ntion process.\n• Extraction of Multiple Events Simultaneously and In-\ndividually: We explored two strategies: extracting all\nevent types at once and extracting each event type one\nby one. General event extraction datasets often encom-\npass numerous event types (e.g., 33 event types in ACE\n2005 and 168 event types in MA VEN). In the one-shot\nsetting, the prompt’s length may become too extended.\nTherefore, we also investigated the extraction of each\nevent separately, assessing the practicality and efficiency\nof both approaches.\nGiven that the language models produce free-text re-\nsponses, extracting the desired information necessitates the\nuse of regular expression patterns. This method enables a\nstructured interpretation of the language models’ outputs\nand allows for a robust analysis of their capabilities in de-\ntecting and classifying diverse event types. By examining\nthese specific scenarios, our research aims to shed light on\nthe effectiveness and limitations of current LLMs in event\nextraction, providing valuable insights to guide future ad-\nvancements in this crucial area of study.\nEmpowering Event Extraction with LLM-based\nAnnotators\nIn alignment with previous research (Qin et al. 2023; Ma\net al. 2023b), a performance gap still exists between LLMs\nand fine-tuned models specifically designed for event extrac-\ntion. To bridge this gap, we propose an integrative approach\nthat leverages the strengths of both methods. The core con-\ncept consists of: 1) utilizing LLMs as expert annotators to\ngenerate labeled data that adheres to a specific schema, mir-\nroring the structure of manually labeled data within a given\ndataset; and 2) fine-tuning specialized event extraction mod-\nels using this enhanced and strategically labeled dataset.\nAlthough LLMs possess extensive knowledge and can\ngenerate samples with annotations in the correct format,\nthere may still be some semantic discrepancies with the cur-\nrent event extraction dataset. To mitigate potential disparities\nin data distribution, we employ targeted prompts to guide\nLLMs in generating labeled data that aligns closely with ex-\nisting examples. We achieve this by incorporating a labeled\nsample from the current dataset, as illustrated in Figure 1,\nwhich displays two of the prompts we utilized. GPT-4 can\ndiscern the intended meaning even with just a basic instruc-\ntion and without specific format stipulations. In contrast,\nPaLM and GPT-3.5-Turbo require more detailed prompts to\nfunction effectively.\nFurthermore, for argument annotations, we include all po-\ntential argument roles of a specific event in our prompt, even\nwhen they may not be relevant to the specific example. This\nstrategy aims to elicit the comprehensive annotation, recog-\nnizing that sentences generated by the LLMs might contain\nall types of arguments. This nuanced approach ensures both\nthe richness and relevance of the annotated data, thereby fos-\ntering a more effective fine-tuning process.\nExperimental Setup\nIn this section, we outline how we design our experiments.\nWe first describe the datasets we studied and the LLMs we\nused. Then, we discuss the specific settings and techniques\nof the fine-tuning approaches considered.\nDatasets\nAs a part of our current work, we conducted a compre-\nhensive study focused on the advancement of sentence-\nlevel event extraction systems. We concentrate our analysis\non two widely-recognized benchmark datasets, ACE 2005\n(Walker et al. 2006) and MA VEN (Wang et al. 2020). To\nminimize the potential biases or variations introduced by\npreprocessing and evaluation methods, we have employed\nthe OmniEvent framework 2, as proposed by Peng et al.\n(2023). This choice ensures a standardized and consistent\n2https://github.com/THU-KEG/OmniEvent\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17774\nPrompt for Zero-shot ED\nPlease analyze the following sentence to determine if it \ncontains any of the listed events: [...]. If an event is detected, \nkindly provide its event type and trigger word/phrase, \nformatting your response as:\nEvent_Type: event type\nTrigger: trigger word/phrase\nIf no event is identified, simply return ‘None’.\nSentence: But, well, the business is complicated and the \nbusiness is tough.\nResponse:\nPrompt for Zero-shot EAE\nThe sentence is understood to describe an event \nPersonnel.End-Position, triggered by ‘leaving’. Analyze the \nsentence to identify any of the following arguments related to \nthis event: ‘person’, ‘entity’, ‘place’. If you find any \narguments, format your response as: {role_type: arguments}. \nIf multiple arguments are related to this event, include them \nall. If no arguments are found, simply respond with ‘None’.\nSentence: Davies is leaving to become chairman of the \nLondon School of Economics, one of the best - known parts of \nthe University of London.\nResponse:\nFigure 2: Prompts for zero-shot ED and zero-shot EAE\napproach, allowing our analysis to focus more directly on\nthe intrinsic performance of the models being studied.\nLarge Language Models\nOur study is mainly directed toward the evaluation of three\nLLMs that have made considerable advancements in NLP.\nThese models are noteworthy for their unique contributions\nand represent the forefront of current technology:\n• GPT-3.5-Turbo3, the most adept and economically vi-\nable version within the GPT-3.5 series (Xu et al. 2023).\n• GPT-4 (OpenAI 2023), a state-of-the-art model known\nfor its computational power and adaptability, making it\none of the most potent LLMs available in the market.\n• PaLM (Chowdhery et al. 2023), renowned as one of the\nleading models, with an innovative architecture and ex-\nceptional performance in a wide array of complex tasks.\nThrough this research, our objective is to present a pre-\ncise and systematic evaluation of the performance of these\ncutting-edge LLMs in the nuanced task of event extrac-\ntion. We investigate their capabilities both as direct tools\nfor extraction and as annotators. These models were specifi-\ncally chosen for our evaluation due to their distinct architec-\ntures, differing capacities, and varied performance profiles.\nTogether, these insights offer a comprehensive and multi-\nfaceted view of the current state of the field, highlighting\nboth the capabilities and challenges of applying LLMs to\nevent extraction.\nBaseline Event Extraction Models\nTo test the quality of data labeled by LLMs, we include the\ntesting of various classical approaches, encompassing differ-\nent categories of methods such as classification-based meth-\nods, sequence-labeling methods, span prediction methods,\nand conditional generation methods (Peng et al. 2023).\n• BERT+CRF (Wang et al. 2020), a sequence labeling\nmodel that integrates BERT (Devlin et al. 2019) as a fea-\nture extractor with the conditional random field (CRF)\n(Lafferty, McCallum, and Pereira 2001) to model struc-\ntured output dependencies.\n3https://platform.openai.com/docs/models/gpt-3-5\n• DMBERT (Wang et al. 2019a,b), a classification model\nthat adopts dynamic multi-pooling (Chen et al. 2015) op-\neration on the hidden representations of BERT (Devlin\net al. 2019).\n• CLEVE (Wang et al. 2021), a contrastive pre-training\nframework for event extraction designed to better capture\nevent knowledge from extensive unsupervised data.\n• EEQA (Du and Cardie 2020), a span prediction method\nthat approaches event extraction problems as a question-\nanswering task.\n• Text2Event (Lu et al. 2021), a conditional generation\nmethod incorporating constrained decoding and curricu-\nlum learning.\nExperimental Results\nAs previously stated, we proposed two methods for leverag-\ning LLMs in event extraction: one by using the LLMs to ex-\ntract event information directly, and the other by employing\nthem as annotators to enhance the performance of fine-tuned\nmodels. In this section, we will present a detailed analysis of\nthe performance of these two approaches, along with corre-\nsponding insights and evaluations.\nLarge Language Models for Event Extraction\nIn this study, we explore the capability of LLMs in extract-\ning event-related information. As previously discussed, we\nassessed the performance of several LLMs on the extrac-\ntion task using different prompts. As illustrated in Figure 2,\nwe present the prompts for zero-shot ED and EAE when si-\nmultaneously extracting multiple event types. For zero-shot\nED, LLMs are presented with a list of event types from\nwhich they can choose. For zero-shot EAE, LLMs are sup-\nplied with the sentences accompanied by their correspond-\ning gold-standard event types and trigger words. Further-\nmore, to furnish LLMs with schema information, we restrict\nthe role types to those identified in the event-specific training\nsamples. For one-shot ED, a single example per event type is\nprovided. For one-shot EAE, we assume that the event types\nwithin the sentence are provided, and therefore, we only fur-\nnish LLMs with one example having the same event type.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17775\nPrompt\nCould you create a succinct sentence that includes the Justice.Acquit event and then annotate it as per the \nprovided examples? \nsentence: Wouter Basson was acquitted in April 2002 on 46 charges, ranging from murder and drug trafficking \nto fraud and theft. \nevent_type: Justice.Acquit\ntrigger: acquitted \ndefendant: Wouter Basson\nadjudicator: None \nplace: None\nResponse from GPT-4\nCertainly! Here's a new sentence along \nwith its annotation:\nsentence: Jane Doe was acquitted of \nembezzlement charges by Judge Smith \nin New York City in June 2019. \nevent_type: Justice.Acquit\ntrigger: acquitted \ndefendant: Jane Doe \nadjudicator: Judge Smith \nplace: New York City\nResponse from PaLM\nsentence: Wouter Basson was acquitted \nin April 2002 on 46 charges, ranging \nfrom murder and drug trafficking to \nfraud and theft.\nevent_type: Justice.Acquit\ntrigger: acquitted\ndefendant: Wouter Basson\nadjudicator: None\nplace: None\nResponse from GPT-3.5-Turbo\nWouter Basson was acquitted \n[Justice.Acquit] in April 2002 on 46 \ncharges, ranging from murder and drug \ntrafficking to fraud and theft. \n[event_type: Justice.Acquit] [trigger: \nacquitted] [defendant: Wouter Basson] \n[adjudicator: None] [place: None]\nFigure 3: Annotated example generated by LLMs\nLLMs M Zero-shot ED One-shot ED\nP R F1 P R F1\nPaLM S 21.2 32.3 25.6 35.6 34.5 35.1\nL 31.9 48.6 38.5 45.1 43.7 44.4\nGPT-3.5 S 14.1 10.4 12.0 39.5 29.5 33.8\nL 45.5 33.5 38.6 53.8 40.2 46.0\nGPT-4 S 10.5 9.0 9.7 41.7 36.7 39.1\nL 56.1 49.1 52.4 60.6 53.3 56.7\nTable 1: Performance of LLMs in zero-shot and one-shot ED\non ACE 2005. In the ’S’ mode, a correct prediction requires\nboth event type and trigger to be accurate. Conversely, the\n’L’ mode considers a prediction correct based solely on ac-\ncurate event type.\nLLMs Zero-shot EAE One-shot EAE\nP R F1 P R F1\nPaLM 20.6 34.5 25.8 30.4 41.6 35.1\nGPT-3.5 17.9 24.7 20.7 20.7 21.9 21.3\nGPT-4 19.0 29.5 23.1 23.3 38.4 29.0\nTable 2: Performance of LLMs in zero-shot and one-shot\nEAE on ACE 2005.\nThe performance of LLMs in zero-shot and one-shot\npipeline event extraction on the ACE 2005 dataset is detailed\nin Tables 1 and 2, with a focus on the simultaneous extrac-\ntion of multiple events. Owing to space limitations, the per-\nformance of LLMs for joint event extraction is detailed in\nthe appendix. As demonstrated in the tables,\n• Providing an example in the prompt enhances both ED\nand EAE performance.\n• Performance experiences a significant decline when both\nevent type and trigger word/phrase predictions are con-\nsidered simultaneously, compared to focusing solely on\nthe event type.\n• Considering that fine-tuned models have already\nachieved an F1 performance of 80% in the ED task\n(Yang et al. 2019) and 60% in the EAE task (Wang et al.\n2021), there remains a significant performance disparity\nbetween LLMs on the event extraction task and models\nspecifically fine-tuned for it.\nUpon further analysis, we discovered that the LLM pre-\ndictions in failure cases are not always entirely inaccurate.\nLLMs appear predisposed to generating more expansive an-\nswers. For example, GPT-4 might recognize ”divorce case”\nas the trigger phrase when the correct golden trigger is sim-\nply ”divorce”. It might also label ”the Welches” as a Per-\nson, whereas the gold standard label is ”Welches”. Since we\nused the gold-standard event type and trigger for EAE eval-\nuations, this nuance partly explains the less-than-stellar ex-\ntraction performance.\nWe also evaluated PaLM’s ED capabilities on MA VEN.\nWe observed that the majority of the predictions adhere to\nthe required format. Nonetheless, the performance leaves\nroom for improvement:\n• Out of 9,400 test samples, PaLM failed to make predic-\ntions for 6 tests.\n• It identified 3,182 event types that are not recog-\nnized within the MA VEN event spectrum, such as ”Be-\ning\na member” and ”Opening”.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17776\n• When submitted to the competition system 4, the perfor-\nmance scores were as follows: precision at 21.8%, recall\nat 6.9%, and F1 score at 10.5%.\nThe unsatisfactory performance might be attributed to the\noverwhelming number of event types in the prompt. Evaluat-\ning the performance of each individual event type on popular\nLLMs should be a consideration for future work.\nLarge Language Models for Data Annotation\nAs outlined earlier, we evaluated the annotation capabilities\nof GPT-4, GPT-3.5-Turbo, and PaLM. To obtain consistent\nsamples, we incorporated an example within the prompt.\nOur selection of these examples followed heuristic rules.\nSpecifically, if an example contains a list of arguments that\ncover all roles, it is chosen. If no single sample for an event\ntype encompasses all arguments, we collect several training\nsamples until all role types are represented. The experimen-\ntal outcomes reveal a performance hierarchy: GPT-4 is at\nthe forefront, succeeded by PaLM, and then GPT-3.5-Turbo.\nFigure 3 depicts a representative example, showcasing the\ndetailed response returned by each model. Notable observa-\ntions from this figure are:\n• GPT-4: While this model generally comprehends the re-\nquirements and can create a sentence with corresponding\nlabels, it occasionally duplicates given sentences and an-\nnotations or responds with comments such as ”Certainly!\nYou’ve provided the sentence and annotation already. If\nthere is anything else you’d like me to do or add, please\nlet me know!”.\n• PaLM: This model recognizes the return format and pro-\nvides sentences with annotations, although it has a ten-\ndency to repeat the given sentence.\n• GPT-3.5-Turbo: In contrast to the others, GPT-3.5-\nTurbo often struggles to understand the task properly\nwhen using the prompt shown in Figure 3. A more ex-\nplicit instruction is required to guide it in generating a\ndistinct sentence and its corresponding annotation.\nBeyond the evident shortcomings, the samples gener-\nated may be incompletely labeled. For instance, a sentence\nmight contain two events with a causal relationship. How-\never, if only one event is provided as an example, the sam-\nples returned by the LLMs typically label only one event.\nTo provide a more detailed evaluation of the quality of la-\nbeled data generated by LLMs, we examined the perfor-\nmance of an event extraction model when fine-tuned with\nand without LLM-labeled data. Table 3 compares the per-\nformance of models fine-tuned with and without augmenta-\ntion by GPT-4 labeled data to the ACE 2005 dataset. Due\nto space limitations, the performance comparisons for GPT-\n3.5-Turbo/PaLM-generated data augmentation, will be pre-\nsented in the appendix.\nGiven the typically small sizes of event extraction\ndatasets, preprocessing steps such as tokenization, sentence\nsplitting, dependency parsing, and negative example selec-\ntion can significantly influence model performance (Lai,\n4https://codalab.lisn.upsaclay.fr/competitions/395\nMethod Dataset ED\nP R F1\nBERT+CRF ACE 2005 64.5 68.5 66.4\nACE 2005 DA 67.4 72.7 69.9\nDMBERT ACE 2005 61.6 75.2 67.7\nACE 2005 DA 63.8 74.2 68.6\nCLEVE ACE 2005 65.5 77.4 71.0\nACE 2005 DA 68.3 77.9 72.8\nEEQA ACE 2005 63.8 74.9 68.9\nACE 2005 DA 66.9 72.2 69.5\nText2Event ACE 2005 62.5 72.0 66.9\nACE 2005 DA 64.2 71.7 67.8\nTable 3: The performance comparison of ED methods fine-\ntuned with and without data augmentation. ‘ACE 2005\nDA’\ndenotes the ACE 2005 training dataset enhanced with la-\nbeled data sourced from GPT-4.\nNguyen, and Nguyen 2020). To ensure uniformity in data\npreprocessing and evaluation, we employed the Omnievent\nframework for our experiments. We adopted the ”ACE-\nDYGIE” (Wadden et al. 2019) preprocessing strategy, ex-\ncluding certain roles, such as those related to time, to em-\nphasize roles with greater semantic distinctions. Apart from\na few event types with over 300 labeled samples, we sourced\n3-5 labeled examples from GPT-4 for all other events, total-\ning an additional 111 labeled samples. Given the high qual-\nity of GPT-4’s samples, our approach is iterative: we pro-\nvided a sample from the training dataset to GPT-4 to gener-\nate a labeled example and then used this newly labeled sam-\nple in the prompt for the subsequent sample. As indicated in\nTables 3 and 4, the inclusion of the GPT-4 labeled dataset\nleads to a notable improvement in performance most of the\ntime. This finding demonstrates that the labeled data gener-\nated by GPT-4 is of sufficiently high quality to enhance the\neffectiveness of the fine-tuned model.\nFor the MA VEN dataset, we generated labeled data only\nwith GPT-3.5-Turbo and PaLM due to budget constraints.\nWe ranked all event types based on the number of their\ntraining samples, and for those not within the top 30 ranks,\nwe randomly selected five to ten samples from the training\ndata and generated labeled data for each. We used a filtering\nscript to exclude samples deviating from the required format\nor exhibiting evident issues. The observed performance im-\nprovements are not as substantial, for example, from 67.99%\nto 68.04%, aligning with the quality analysis of the gener-\nated labeled data, suggesting that GPT-3.5-Turbo and PaLM\ncan annotate data to some extent but not as effectively as\nGPT-4. Another contributing factor could be that MA VEN\ncontains significantly more samples than the samples gener-\nated by LLMs, hence the improvement is not substantial.\nDiscussion\nAlthough LLMs are widely employed in current applica-\ntions, they may not serve as a flawless solution for all\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17777\nMethods Triggers P R F1\nBERT+CRF\nGold ACE 66.9 62.3 64.5\nPredicted ACE 50.1 64.9 56.5\nGold DA 65.4 65.1 65.3\nPredicted DA 50.3 66.4 57.2\nDMBERT\nGold ACE 64.1 71.5 67.6\nPredicted ACE 43.3 69.9 53.5\nGold DA 65.1 72.0 68.4\nPredicted DA 45.9 70.1 55.5\nCLEVE\nGold ACE 70.8 75.0 72.8\nPredicted ACE 51.1 73.7 60.4\nGold DA 69.0 76.0 72.3\nPredicted DA 50.9 76.7 61.2\nEEQA\nGold ACE 70.7 54.2 61.4\nPredicted ACE 44.3 56.3 49.6\nGold DA 73.1 58.9 65.2\nPredicted DA 51.9 56.9 54.3\nText2Event\nGold ACE 64.4 57.1 60.5\nPredicted ACE 44.2 55.1 49.0\nGold DA 66.9 55.7 60.8\nPredicted DA 48.8 56.3 52.3\nTable 4: Performance of EAE with and without data aug-\nmented by GPT-4. We differentiate between EAE results us-\ning golden triggers and those using triggers predicted from\nprior ED. Specifically, ’Gold\nACE’ refers to EAE perfor-\nmance using golden triggers on the original ACE 2005,\nwhile ’Predicted DA’ signifies EAE performance with pre-\ndicted triggers on the augmented ACE 2005 dataset.\ngeneral-purpose NLP tasks (Qin et al. 2023). As demon-\nstrated by our experiments, there remains a noticeable gap\nbetween fine-tuned event extraction models and LLMs. We\nbelieve this disparity arises not merely from capability dif-\nferences, but also due to the following reasons:\n• In datasets like ACE 2005, sentences are extracted from\ndisparate articles, and their annotations are context-\ndependent. A specific case might be the word ”this”,\ntriggering the ”Conflict.Attack” event in the sentence\n”Nobody questions whether this is right or not.”. With-\nout access to the context of this sentence, determin-\ning what ”this” refers to—and thus identifying the\nevent—becomes exceedingly challenging.\n• The prevailing evaluation approach heavily hinges on the\nprinciple of an exact match. While this method provides\na straightforward way to gauge performance, it can inad-\nvertently fail to recognize and credit precise predictions\nfurnished by LLMs. Given that LLMs often produce se-\nmantically equivalent outputs that might not align ver-\nbatim with the gold standard, this metric can undersell\ntheir true capabilities. To more accurately reflect the pro-\nficiency and nuances of these models, there is a pressing\nneed to adopt more flexible and suitable evaluation met-\nrics, as also discussed by Wei et al. (2022a).\n• LLMs’ outputs might not always adhere to the specific\nformat, potentially leading to the omission of some cor-\nrect predictions.\nEnhancing the performance of fine-tuned models through\ndata augmentation also poses a challenge. An analysis of\nthe benchmark dataset’s data statistics reveals a trend where\nevent types with fewer training samples also tend to have\nfewer test samples. This implies that the quantity and diver-\nsity of labeled samples, especially for specific events, play a\ncrucial role in enhancing test performance. Additionally, as\nhighlighted in DYGIE++ (Wadden et al. 2019), the small\nsize and domain shift between the development and test\nsplits in the ACE 2005 dataset can render selections based\non the development dataset unreliable.\nFuture Work\nDespite the great potential we discovered, our approach still\nhas some room for improvement. Looking ahead, our plans\nfor future research include exploring the following avenues:\n• Document-level Event Extraction: Studying the prac-\nticality of extending our method to document-level\nextraction. While current methodologies are primarily\nsentence-focused, broader contexts are needed for more\ncomplete event understanding. For instance, a pronoun\nlike ”he” could be ambiguous without contextual clues.\n• Enhancing EAE: Exploring methods to improve the\nextraction of event arguments. Recognizing an event\nwithout specific details might be insufficient. Therefore,\nstudying accurate techniques for extracting specific in-\nformation is paramount.\n• Prompt Investigation: The choice of prompt is crucial\nto fully harness the capabilities of LLMs. Thus, explor-\ning various prompt techniques to maximize the utility of\nLLMs warrants further study.\n• Experimenting with Open-source LLMs: LLMs, al-\nthough powerful, demand significant resources. Test-\ning open-source models with fewer parameters, such as\nLLaMA (Touvron et al. 2023), which are suitable for en-\nvironments with limited resources, will also be an essen-\ntial part of our ongoing work.\nConclusions\nIn this study, we have concentrated on utilizing LLMs for\nsentence-level event extraction. To the best of our knowl-\nedge, this represents the first comprehensive exploration of\nemploying LLMs for this task. Through rigorous testing\nwith various LLMs and prompts, we assessed their perfor-\nmance in both joint and pipeline manners. Our findings re-\nvealed a noticeable gap between the performance of LLMs\nand that of fine-tuned models. Therefore, we proposed em-\nploying LLMs as expert annotators. This strategy yields la-\nbeled data that aligns with the benchmark dataset’s distribu-\ntion. By mitigating data scarcity and imbalance issues, this\napproach boosts the performance of the fine-tuned models.\nThis underscores the effectiveness of LLMs as proficient an-\nnotators. Experiments on commonly used datasets affirmed\nthe feasibility of this approach, highlighting its potential ap-\nplication in various domains.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17778\nAcknowledgments\nThis research was supported in part by National Robotics\nProgramme under its Survey of Foundation Models for\nRobotics (Award M23NBK0091) project. Any opinions and\nconclusions expressed in this material are those of the au-\nthors and may not necessarily reflect the views of the pro-\ngramme or its host agency. No official endorsement should\nbe inferred.\nReferences\nBaker, C. F.; Fillmore, C. J.; and Lowe, J. B. 1998. The\nBerkeley FrameNet Project. In 36th Annual Meeting of the\nAssociation for Computational Linguistics and 17th Inter-\nnational Conference on Computational Linguistics, Volume\n1, 86–90. Montreal, Quebec, Canada: ACL.\nChen, Y .; Xu, L.; Liu, K.; Zeng, D.; and Zhao, J. 2015. Event\nExtraction via Dynamic Multi-Pooling Convolutional Neu-\nral Networks. In Proceedings of the 53rd Annual Meeting of\nthe Association for Computational Linguistics and the 7th\nInternational Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), 167–176. Beijing, China:\nACL.\nChowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra,\nG.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton,\nC.; Gehrmann, S.; Schuh, P.; Shi, K.; Tsvyashchenko,\nS.; Maynez, J.; Rao, A.; Barnes, P.; Tay, Y .; Shazeer,\nN.; Prabhakaran, V .; Reif, E.; Du, N.; Hutchinson, B.;\nPope, R.; Bradbury, J.; Austin, J.; Isard, M.; Gur-Ari, G.;\nYin, P.; Duke, T.; Levskaya, A.; Ghemawat, S.; Dev, S.;\nMichalewski, H.; Garcia, X.; Misra, V .; Robinson, K.; Fe-\ndus, L.; Zhou, D.; Ippolito, D.; Luan, D.; Lim, H.; Zoph, B.;\nSpiridonov, A.; Sepassi, R.; Dohan, D.; Agrawal, S.; Omer-\nnick, M.; Dai, A. M.; Pillai, T. S.; Pellat, M.; Lewkowycz,\nA.; Moreira, E.; Child, R.; Polozov, O.; Lee, K.; Zhou,\nZ.; Wang, X.; Saeta, B.; Diaz, M.; Firat, O.; Catasta, M.;\nWei, J.; Meier-Hellstern, K.; Eck, D.; Dean, J.; Petrov, S.;\nand Fiedel, N. 2023. PaLM: Scaling Language Modeling\nwith Pathways. Journal of Machine Learning Research, 24:\n240:1–240:113.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), 4171–4186. Min-\nneapolis, Minnesota: ACL.\nDriess, D.; Xia, F.; Sajjadi, M. S. M.; Lynch, C.; Chowd-\nhery, A.; Ichter, B.; Wahid, A.; Tompson, J.; Vuong, Q.;\nYu, T.; Huang, W.; Chebotar, Y .; Sermanet, P.; Duckworth,\nD.; Levine, S.; Vanhoucke, V .; Hausman, K.; Toussaint, M.;\nGreff, K.; Zeng, A.; Mordatch, I.; and Florence, P. 2023.\nPaLM-E: An Embodied Multimodal Language Model. In\nInternational Conference on Machine Learning, volume 202\nof Proceedings of Machine Learning Research, 8469–8488.\nHonolulu, Hawaii: PMLR.\nDu, X.; and Cardie, C. 2020. Event Extraction by An-\nswering (Almost) Natural Questions. In Proceedings of the\n2020 Conference on Empirical Methods in Natural Lan-\nguage Processing, 671–683. Online: ACL.\nGao, J.; Yu, C.; Wang, W.; Zhao, H.; and Xu, R. 2022.\nMask-then-Fill: A Flexible and Effective Data Augmenta-\ntion Framework for Event Extraction. In Findings of the\nAssociation for Computational Linguistics: EMNLP, 4537–\n4544. Abu Dhabi, United Arab Emirates: ACL.\nGendron, G.; Bao, Q.; Witbrock, M.; and Dobbie, G. 2023.\nLarge Language Models Are Not Strong Abstract Reason-\ners. arXiv:2305.19555.\nIchter, B.; Brohan, A.; Chebotar, Y .; Finn, C.; Hausman, K.;\nHerzog, A.; Ho, D.; Ibarz, J.; Irpan, A.; Jang, E.; Julian, R.;\nKalashnikov, D.; Levine, S.; Lu, Y .; Parada, C.; Rao, K.; Ser-\nmanet, P.; Toshev, A.; Vanhoucke, V .; Xia, F.; Xiao, T.; Xu,\nP.; Yan, M.; Brown, N.; Ahn, M.; Cortes, O.; Sievers, N.;\nTan, C.; Xu, S.; Reyes, D.; Rettinghouse, J.; Quiambao, J.;\nPastor, P.; Luu, L.; Lee, K.; Kuang, Y .; Jesmonth, S.; Joshi,\nN. J.; Jeffrey, K.; Ruano, R. J.; Hsu, J.; Gopalakrishnan, K.;\nDavid, B.; Zeng, A.; and Fu, C. K. 2022. Do As I Can, Not\nAs I Say: Grounding Language in Robotic Affordances. In\nConference on Robot Learning, volume 205 ofProceedings\nof Machine Learning Research, 287–318. Auckland, New\nZealand: PMLR.\nKojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y .; and Iwasawa,\nY . 2022. Large Language Models are Zero-Shot Reasoners.\nIn NeurIPS.\nLafferty, J. D.; McCallum, A.; and Pereira, F. C. N. 2001.\nConditional Random Fields: Probabilistic Models for Seg-\nmenting and Labeling Sequence Data. In Proceedings of\nthe Eighteenth International Conference on Machine Learn-\ning, ICML ’01, 282–289. San Francisco, CA, USA: Morgan\nKaufmann Publishers Inc.\nLai, V . D.; Nguyen, T. N.; and Nguyen, T. H. 2020. Event\nDetection: Gate Diversity and Syntactic Importance Scores\nfor Graph Convolution Neural Networks. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural\nLanguage Processing, 5405–5411. Online: ACL.\nLiu, J.; Chen, Y .; Liu, K.; Bi, W.; and Liu, X. 2020. Event\nExtraction as Machine Reading Comprehension. In Pro-\nceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing, 1641–1651. Online: ACL.\nLiu, S.; Chen, Y .; He, S.; Liu, K.; and Zhao, J. 2016. Lever-\naging FrameNet to Improve Automatic Event Detection. In\nProceedings of the 54th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long Papers),\n2134–2143. Berlin, Germany: ACL.\nLu, Y .; Lin, H.; Xu, J.; Han, X.; Tang, J.; Li, A.; Sun, L.;\nLiao, M.; and Chen, S. 2021. Text2Event: Controllable\nSequence-to-Structure Generation for End-to-end Event Ex-\ntraction. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the 11th In-\nternational Joint Conference on Natural Language Process-\ning (Volume 1: Long Papers), 2795–2806. Online: ACL.\nMa, M. D.; Wang, X.; Kung, P.-N.; Brantingham, P. J.; Peng,\nN.; and Wang, W. 2023a. STAR: Improving Low-Resource\nInformation Extraction by Structure-to-Text Data Genera-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17779\ntion with Large Language Models. In NeurIPS 2023 Work-\nshop on Synthetic Data Generation with Generative AI.\nMa, Y .; Cao, Y .; Hong, Y .; and Sun, A. 2023b. Large Lan-\nguage Model Is Not a Good Few-shot Information Extrac-\ntor, but a Good Reranker for Hard Samples! In Findings\nof the Association for Computational Linguistics: EMNLP,\n10572–10601. Singapore: ACL.\nNguyen, K.-H.; Tannier, X.; Ferret, O.; and Besanc ¸on, R.\n2016. A Dataset for Open Event Extraction in English.\nIn Proceedings of the Tenth International Conference on\nLanguage Resources and Evaluation, 1939–1943. Portoroˇz,\nSlovenia: ELRA.\nOpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.\nPeng, H.; Wang, X.; Yao, F.; Zeng, K.; Hou, L.; Li, J.; Liu,\nZ.; and Shen, W. 2023. The Devil is in the Details: On\nthe Pitfalls of Event Extraction Evaluation. In Findings of\nthe Association for Computational Linguistics, 9206–9227.\nToronto, Canada: ACL.\nQin, C.; Zhang, A.; Zhang, Z.; Chen, J.; Yasunaga, M.;\nand Yang, D. 2023. Is ChatGPT a General-Purpose Nat-\nural Language Processing Task Solver? In Proceedings of\nthe 2023 Conference on Empirical Methods in Natural Lan-\nguage Processing, 1339–1384. Singapore: ACL.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lample,\nG. 2023. LLaMA: Open and Efficient Foundation Language\nModels. arXiv:2302.13971.\nWadden, D.; Wennberg, U.; Luan, Y .; and Hajishirzi, H.\n2019. Entity, Relation, and Event Extraction with Contex-\ntualized Span Representations. In Inui, K.; Jiang, J.; Ng,\nV .; and Wan, X., eds., Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Process-\ning and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP), 5784–5789. Hong\nKong, China: Association for Computational Linguistics.\nWalker, C.; et al. 2006. ACE 2005 Multilingual Training\nCorpus. https://catalog.ldc.upenn.edu/LDC2006T06.\nWang, X.; Han, X.; Liu, Z.; Sun, M.; and Li, P. 2019a. Ad-\nversarial Training for Weakly Supervised Event Detection.\nIn Proceedings of the 2019 Conference of the North Ameri-\ncan Chapter of the Association for Computational Linguis-\ntics: Human Language Technologies, Volume 1 (Long and\nShort Papers), 998–1008. Minneapolis, Minnesota: ACL.\nWang, X.; Wang, Z.; Han, X.; Jiang, W.; Han, R.; Liu, Z.; Li,\nJ.; Li, P.; Lin, Y .; and Zhou, J. 2020. MA VEN: A Massive\nGeneral Domain Event Detection Dataset. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural\nLanguage Processing, 1652–1671. Online: ACL.\nWang, X.; Wang, Z.; Han, X.; Liu, Z.; Li, J.; Li, P.; Sun,\nM.; Zhou, J.; and Ren, X. 2019b. HMEAE: Hierarchi-\ncal Modular Event Argument Extraction. In Proceedings\nof the 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint Con-\nference on Natural Language Processing, 5777–5783. Hong\nKong, China: ACL.\nWang, Z.; Wang, X.; Han, X.; Lin, Y .; Hou, L.; Liu, Z.;\nLi, P.; Li, J.; and Zhou, J. 2021. CLEVE: Contrastive Pre-\ntraining for Event Extraction. InProceedings of the 59th An-\nnual Meeting of the Association for Computational Linguis-\ntics and the 11th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers), 6283–6297.\nOnline: ACL.\nWei, J.; Tay, Y .; Bommasani, R.; Raffel, C.; Zoph, B.;\nBorgeaud, S.; Yogatama, D.; Bosma, M.; Zhou, D.; Met-\nzler, D.; Chi, E. H.; Hashimoto, T.; Vinyals, O.; Liang, P.;\nDean, J.; and Fedus, W. 2022a. Emergent Abilities of Large\nLanguage Models. Transactions on Machine Learning Re-\nsearch.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Ichter, B.;\nXia, F.; Chi, E. H.; Le, Q. V .; and Zhou, D. 2022b. Chain-\nof-Thought Prompting Elicits Reasoning in Large Language\nModels. In NeurIPS.\nXu, F.; Lin, Q.; Han, J.; Zhao, T.; Liu, J.; and Cambria,\nE. 2023. Are Large Language Models Really Good Log-\nical Reasoners? A Comprehensive Evaluation and Beyond.\narXiv:2306.09841.\nYang, S.; Feng, D.; Qiao, L.; Kan, Z.; and Li, D. 2019. Ex-\nploring Pre-trained Language Models for Event Extraction\nand Generation. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics, 5284–\n5294. Florence, Italy: ACL.\nZhang, Z.; Zhang, A.; Li, M.; and Smola, A. 2023. Auto-\nmatic Chain of Thought Prompting in Large Language Mod-\nels. In The Eleventh International Conference on Learning\nRepresentations. Kigali, Rwanda: OpenReview.net.\nZhao, Y .; Misra, I.; Kr¨ahenb¨uhl, P.; and Girdhar, R. 2023.\nLearning Video Representations from Large Language\nModels. In IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 6586–6597. Vancouver, BC: IEEE.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17780",
  "topic": "Event (particle physics)",
  "concepts": [
    {
      "name": "Event (particle physics)",
      "score": 0.5907199382781982
    },
    {
      "name": "Computer science",
      "score": 0.5749194622039795
    },
    {
      "name": "Natural language processing",
      "score": 0.5685620307922363
    },
    {
      "name": "Extraction (chemistry)",
      "score": 0.5518673658370972
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3912719190120697
    },
    {
      "name": "Linguistics",
      "score": 0.3426574468612671
    },
    {
      "name": "Physics",
      "score": 0.10714823007583618
    },
    {
      "name": "Chemistry",
      "score": 0.10565352439880371
    },
    {
      "name": "Philosophy",
      "score": 0.0881868302822113
    },
    {
      "name": "Astrophysics",
      "score": 0.07399299740791321
    },
    {
      "name": "Chromatography",
      "score": 0.06779944896697998
    }
  ]
}