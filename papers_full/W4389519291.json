{
  "title": "Enhancing Chat Language Models by Scaling High-quality Instructional Conversations",
  "url": "https://openalex.org/W4389519291",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1903586662",
      "name": "Ning Ding",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2096431253",
      "name": "Yulin CHEN",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2326898937",
      "name": "Bokai Xu",
      "affiliations": [
        "Chinese University of Hong Kong, Shenzhen"
      ]
    },
    {
      "id": "https://openalex.org/A2098243531",
      "name": "Yujia Qin",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2998424002",
      "name": "Shengding Hu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2051269448",
      "name": "Zhiyuan Liu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2157167650",
      "name": "Maosong Sun",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2110030736",
      "name": "Bowen Zhou",
      "affiliations": [
        "Tsinghua University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2087388117",
    "https://openalex.org/W3198659451",
    "https://openalex.org/W2946609015",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4366341216",
    "https://openalex.org/W4367000491",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W4322766882",
    "https://openalex.org/W4384662964",
    "https://openalex.org/W4307123345",
    "https://openalex.org/W4223908421",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W4377297670",
    "https://openalex.org/W4361193485",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4308902180",
    "https://openalex.org/W4389519535",
    "https://openalex.org/W2794325560",
    "https://openalex.org/W4362655426",
    "https://openalex.org/W4389524372",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4318908031",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2736601468"
  ],
  "abstract": "Fine-tuning on instruction data has been widely validated as an effective practice for implementing chat language models like ChatGPT. Scaling the diversity and quality of such data, although straightforward, stands a great chance of leading to improved performance. This paper aims to push the upper bound of open-source models further. We first provide a systematically designed, diverse, informative, large-scale dataset of instructional conversations, UltraChat, which does not involve human queries. Our objective is to capture the breadth of interactions between a human user and an AI assistant and employs a comprehensive framework to generate multi-turn conversation iteratively. UltraChat contains 1.5 million high-quality multi-turn dialogues and covers a wide range of topics and instructions. Our statistical analysis of UltraChat reveals its superiority in various key metrics, including scale, average length, diversity, coherence, etc., solidifying its position as a leading open-source dataset. Building upon UltraChat, we fine-tune a LLaMA model to create a powerful conversational model, UltraLM. Our evaluations indicate that UltraLM consistently outperforms other open-source models, including WizardLM and Vicuna, the previously recognized state-of-the-art open-source models.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3029–3051\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nEnhancing Chat Language Models by Scaling High-quality\nInstructional Conversations\nNing Ding1∗, Yulin Chen2,3∗, Bokai Xu4, Yujia Qin2,3, Shengding Hu2,3\nZhiyuan Liu2,3†, Maosong Sun2,3†, Bowen Zhou1†\n1Department of Electronic Engineering,Tsinghua University\n2Department of Computer Science and Technology, Tsinghua University\n3 BNRIST, IAI, Tsinghua University,4The Chinese University of Hong Kong, Shenzhen\nAbstract\nFine-tuning on instruction data has been widely\nvalidated as an effective practice for implement-\ning chat language models like ChatGPT. Scal-\ning the diversity and quality of such data, al-\nthough straightforward, stands a great chance\nof leading to improved performance. This pa-\nper aims to push the upper bound of open-\nsource models further. We first provide a\nsystematically designed, diverse, informative,\nlarge-scale dataset of instructional conversa-\ntions, UltraChat, which does not involve hu-\nman queries. Our objective is to capture the\nbreadth of interactions between a human user\nand an AI assistant and employs a compre-\nhensive framework to generate multi-turn con-\nversation iteratively. UltraChat contains 1.5\nmillion high-quality multi-turn dialogues and\ncovers a wide range of topics and instructions.\nOur statistical analysis of UltraChat reveals\nits superiority in various key metrics, includ-\ning scale, average length, diversity, coherence,\netc., solidifying its position as a leading open-\nsource dataset. Building upon UltraChat, we\nfine-tune a LLaMA model to create a powerful\nconversational model, UltraLM. Our evalua-\ntions indicate that UltraLM consistently out-\nperforms other open-source models, including\nWizardLM and Vicuna, the previously recog-\nnized state-of-the-art open-source models.\n1 Introduction\nLarge language models (Bommasani et al., 2021;\nHan et al., 2021; Chowdhery et al., 2022) (LLMs)\nhave demonstrated exceptional generalization\ncapability on a variety of language-related tasks.\nNotably, ChatGPT (OpenAI, 2022), an optimized\nversion of GPT-3 (Brown et al., 2020) for conver-\nsation, along with GPT-4 (OpenAI, 2023), takes\nthe user experience to another level via excelling\nin comprehending and generating responses in a\nnatural and interactive manner. The introduction of\n∗ equal contributions\n† Corresponding authors\nModel Score\nDolly-v2 (Conover et al., 2023) 4.04 ± 2.34\nMPT-Chat (Mosaic, 2023) 6.67 ± 2.88\nOpenAssistant (Köpf et al., 2023) 7.65 ± 2.15\nAlpaca (Taori et al., 2023) 8.04 ± 2.05\nKoala (Geng et al., 2023) 8.23 ± 1.99\nBaize (Xu et al., 2023b) 8.50 ± 1.34\nVicuna (Chiang et al., 2023) 8.78 ± 1.55\nWizard-LM (Xu et al., 2023a) 8.95 ± 1.44\nUltraLM (ours) 9.00 ± 1.33\nTable 1: Average scores (1-10) across different open-\nsource models and UltraLM. The evaluation is con-\nducted on our curated evaluation set with GPT-4. Evalu-\nation prompts can be found in Appendix C.\nChatGPT has spurred a surge in the adoption and\nimplementation of general chat language models.\nIn addition to competing models developed by\nlarge corporations such as Bard1 and Claude2, the\nopen-source community is actively engaged in\ntraining similar models, aiming to democratize\naccess to AI technology. Notable examples in\nthis regard include Alpaca (Taori et al., 2023), Vi-\ncuna (Chiang et al., 2023), Koala (Geng et al.,\n2023), Baize (Xu et al., 2023b), and Belle (Ji\net al., 2023), etc., demonstrating promising perfor-\nmance. Experimental evidence strongly suggests\nthat chat language models can be effectively trained\nthrough instruction fine-tuning (Wei et al., 2021;\nSanh et al., 2021), and they also indicate that many\ndata-efficient (Zhou et al., 2023) or computing-\nefficient (Hu et al., 2021; Ding et al., 2023) meth-\nods can be applied. This paper, in another way, fo-\ncuses more on the \"final one mile\" of chat language\nmodels, as evidence shows thatthe journey from 0\nto 60 is easy, whereas progressing from 60 to 100\nbecomes exceedingly challenging. For instance,\nresearchers have shown that by utilizing a small,\n1https://bard.google.com/\n2https://www.anthropic.com/index/\nintroducing-claude\n3029\nthoughtfully curated set of instructions, it is possi-\nble to train a model with satisfactory instruction-\nfollowing capabilities. However, these approaches\nhave yet to produce models that surpass the perfor-\nmance of Vicuna, the current leading open-source\nmodel, let alone outperform ChatGPT and GPT-4.\nThis paper believes that the most straightforward\nway, that is, the quality and diversity of training\ndata, play a vital role in further improving the per-\nformance of chat language models. In other words,\nleveraging higher quality and more diverse data\ncan yield better outcomes. To this end, we present\nUltraChat, a million-scale multi-turn instructional\nconversation data, to facilitate the construction of\nmore powerful chat language models. UltraChat\nis carefully designed to capture the breadth of in-\nteractions that a human might have with an AI\nassistant. Specifically, we do not use specific tasks\nlike question-answering or summarization to con-\nstruct the data, but curate three sectors: Questions\nabout the World, Creation and Writing, and Assis-\ntance on Existing Materials. Then we employ meta-\ninformation, in-context expansion, and iterative\nprompting to scale up the number of instructions.\nTo construct informative and realistic multi-turn\nconversations, two separate ChatGPT Turbo APIs\nare adopted in the conversation generation, where\none plays the role of the user to generate queries,\nand the other generates the response. We instruct\nthe user model with carefully designed prompts to\nmimic human user behavior and call the two APIs\niteratively.\nWe fine-tune a LLaMA-13B model on Ultra-\nChat to produce UltraLM and compare the model\nto a wide range of baselines, especially the open-\nsource ones. The evaluation shows that our model\ncould consistently outperform other models. As\nreported in Table 1, UltraLM achieves the highest\nperformance scores that are independently assessed\nby GPT-4. Further evaluation results on challeng-\ning benchmarks and preference study with GPT-4\non various evaluation sets also show that UltraLM\ncould surpass all other open-source models.\n2 Related Work\nInstruction Tuning. Recent works demonstrate\nLLMs’ powerful capabilities in following human\ninstructions. Wei et al. (2021) pioneered to fine-\ntune T5 (Raffel et al., 2020) on60 NLP datasets ver-\nbalized with natural language instruction templates,\ni.e., instruction tuning. The fine-tuned model ex-\nhibits a strong ability in instruction understanding\nand generalizes well to unseen instructions (Sanh\net al., 2021; Ouyang et al., 2022). Later, Longpre\net al. (2023) show the benefits of scaling the num-\nber of tasks in out-of-distribution generalization.\nWei et al. (2021) also conclude that the success\nof instruction tuning depends on the quality of the\ndataset and the design of prompts. To further reg-\nulate the tuned model’s behavior, Ouyang et al.\n(2022); Schulman et al. (2017) propose to employ\nreinforcement learning to align model behaviors\nwith human preferences. This technique combined\nwith instruction tuning can further boost the model\nperformance and has been successfully applied to\nLLMs such as ChatGPT.\nData Augmentation with LLMs. Collecting\nlarge-scale human-annotated instructions and their\nresponses is time-consuming and labor-intensive.\nAlternatively, a more cost-effective and feasible\napproach to gathering top-notch data involves\nsampling from LLMs that have been well-tuned,\ne.g., ChatGPT and GPT-3.5. Recently, there is\na surge of interest in distilling these powerful\nLLMs for data augmentation. For instance, us-\ning the technique of Self-Instruct (Wang et al.,\n2022), Alpaca (Taori et al., 2023) generate 52k\nhigh-quality instruction-response pairs based on\nseed tasks by “distilling” Text-Davinci-003. The\ntrained model performs almost on par with Text-\nDavinci-003. The success of Alpaca boosts numer-\nous later efforts on data augmentation with LLMs,\nsuch as code-alpaca (Chaudhary, 2023), alpaca-\ncot (Si et al., 2023), GPT4ALL (Anand et al.,\n2023), ShareGPT (Domeccleston, 2023), Dolly-\nv2 (Conover et al., 2023), BELLE (Ji et al., 2023),\nVicuna (Chiang et al., 2023), Koala (Geng et al.,\n2023), Baize (Xu et al., 2023b), etc. It is shown\nthat increasing the scale of data could constantly\nimprove the model performance. Besides, prompt\nengineering also affects data quality. CAMEL (Li\net al., 2023a) design a multi-agent role-play envi-\nronment for LLMs to simulate real human conver-\nsations.\n3 Data Construction\nLLMs are believed to be better annotators than\nhuman-being in many scenarios (Gilardi et al.,\n2023). However, employing LLMs such as Chat-\nGPT directly for generating multi-turn conversa-\ntions may yield satisfactory but less informative\nresults, as it cannot enjoy the benefit of reinforce-\n3030\nMeta Topics Sub\nTopics\nQuestions about \nVarious Conceptions\nRepresentative\nEntities \nMeta\nQuestions\nDetailed \nQuestions \nAssociated \nQuestions \nMaterial \nTypes\nMaterial \nGeneration \nInstructions\nDetailed\nInstructions\nMaterials Questions or \nInstructions\nSector I: Questions about the World \nHuman\nModel\nWikidata\nSearch\nEngine\nHuman\nModel\nSector II: Creation and Writing\nSector III: Assistance on Materials\nC4\nSector II\nUser Model\nAI Model\n3~7 rounds of \ngeneration\nMeta Topics Sub\nTopics\nQuestions about \nVarious Conceptions\nRepresentative\nEntities \nMeta\nQuestions\nDetailed \nQuestions \nAssociated \nQuestions \nMaterial \nTypes\nMaterial \nGeneration \nInstructions\nDetailed\nInstructions\nMaterials Questions or \nInstructions\nSector I: Questions about the World \nHuman\nModel\nWikidata\nSearch\nEngine\nHuman\nModel\nSector II: Creation and Writing\nSector III: Assistance on Materials\nC4\nSector II\nUser Model\nAI Model\n3~7 rounds of \ngeneration\nPost-processing\nQuery/Instruct\nResponse\nExntend Generate\nGenerate Associate\nDetail\nGenerate Detail\nGenerate\nMeta Topics Sub\nTopics\nQuestions about \nConceptions\nRepresentative\nEntities \nMeta\nQuestions\nDetailed \nQuestions \nAssociated \nQuestions \nMaterial \nTypes\nMaterial \nGeneration \nInstructions\nDetailed\nInstructions\nMaterials Questions or \nInstructions\nSector I: Questions about the World \nHuman\nModel\nWikidata\nSearch\nEngine\nHuman\nModel\nSector II: Creation and Writing\nSector III: Assistance on Materials\nC4\nSector II\nUser Model\nAI Model\n3~7 rounds of \ngeneration\nPost-processing\nQuery/Instruct\nResponse\nFigure 1: Construction process of UltraChat. The three sectors of data are derived from different meta-information.\nment learning with human feedback (RLHF) in the\nalignment process. Table 12 in Appendix A shows\na comparison of directly generated multi-turn dia-\nlogue and a case in UltraChat with the same open-\ning line. Two key points can be derived to ensure\nthe quality of the data: (1) An opening line de-\ntermines the topic of the dialogue. Opening lines\nshould be highly diverse and encompass any task\nthat a human user may request a chat model to\nperform. (2) A user determines the plot of the dia-\nlogue, and the output should be tailored to the cur-\nrent topic with diverse language styles and requests.\nTherefore, unlike traditional task-specific\ndatasets, to construct a comprehensive open-\ndomain instructional chat dataset, the design of\ndata collection schema is crucial to capturing the\nbreadth of interactions and ensuring data quality.\nUltraChat aims to cover a tremendous range of\nconversation data with a carefully designed tripar-\ntite schema: Questions about the World, Creation\nand Writing, and Assistance on Existing Materials.\nWhile the core of ensuring data diversity mainly\ndepends on opening line diversity, we will first in-\ntroduce the idea behind the sector design and then\nfocus on specific measures to obtain a diverse set of\nopening lines and how to prompt the user properly.\n3.1 Questions about the World\nThe first sector focuses on querying existing infor-\nmation in the world, including concepts, objects,\nand entities that exist in the real world. This is at\nthe core of human-AI interaction, as users often\nrely on AI assistants to provide quick and accurate\nanswers to their questions.\nOur approach to gathering data for this sector\ninvolves two perspectives: one centered around top-\nics and concepts, and the other around real-world\nentities. Initially, we request ChatGPT to generate\n30 comprehensive topics that encompass various\naspects of our daily lives, as shown in Table 2.\nSubsequently, we delve deeper into each topic by\ngenerating 30 to 50 subtopics or related concepts.\nFinally, we generate 10 different questions for each\nsubtopic or concept and additionally request Chat-\nGPT to generate 10 more questions based on each\noriginal question. The other source of data comes\nfrom real-world objects, which are derived from\nWikidata3 entities. These entities are further refined\nby considering their frequencies in Wikipedia4 ar-\nticles, specifically focusing on the 10,000 most\nfrequently occurring entities. For each entity, we\ncreate 5 meta-questions, followed by 10 more spe-\ncific questions and 20 extended questions. The\nextended questions aim to maintain some similarity\nto the original question while exploring distinct ob-\njects or topics. To create a dialogue, we filter and\nsample approximately 500,000 questions as open-\ning lines. During the construction of each dialogue,\nwe provide the user model with carefully crafted\nprompts that explicitly ask the model to respond\n3https://www.wikidata.org/\n4https://www.wikipedia.org/\n3031\nTechnology\n Health and wellness\n Travel and adventure\nFood and drink\n Art and culture\n Science and innovation\nFashion and style\n Relationships and dating\n Sports and fitness\nNature and the environment\n Music and entertainment\n Politics and current events\nEducation and learning\n Money and finance\n Work and career\nPhilosophy and ethics\n History and nostalgia\n Social media and communication\nCreativity and inspiration\n Personal growth and development\n Spirituality and faith\nPop culture and trends\n Beauty and self-care\n Family and parenting\nEntrepreneurship and business\n Literature and writing\n Gaming and technology\nMindfulness and meditation\n Diversity and inclusion\n Travel and culture exchange\nTable 2: 30 meta-concepts used to generate the first sector of UltraChat data.\nArticles and Blog Posts\n Job Application Material\n Stories\nLegal Documents and Contracts\n Poems\n Educational Content\nScreenplays\n Scripts for Language Learning\n Technical Documents and Reports\nMarketing Materials\n Social Media Posts\n Personal Essays\nEmails\n Scientific Papers and Summaries\nSpeeches and Presentations\nRecipes and Cooking Instructions\n News Articles\n Song Lyrics\nProduct Descriptions and Reviews\nPrograms and Code\nTable 3: 20 types of text materials used for sector 2 and 3 UltraChat construction.\nconcisely and meaningfully, taking into account\nthe context of the ongoing dialogue history.\n3.2 Creation and Writing\nThe second part is concerned with the creation\nof new information with human-input conditions,\nranging from writing emails to crafting stories and\nplays. This process reflects the AI’s capacity to\nengage in original content generation alongside\nusers and demonstrates the role of AI assistants as\ncollaborative partners in a creative environment.\nWe first project all creations as text materials,\nand further categorize them into 20 different types\nas in Table 3. Then a ChatGPT model is employed\nto produce a diverse range of instructions for each\ntype of writing, approximately 80% of which are\nfurther refined by ChatGPT model to generate more\ndetailed instructions. These instructions serve as\nopening lines for dialogue generation. Throughout\nthe generation process, the user prompt constantly\nreinforces the primary objective of the conversa-\ntion, which is to generate and refine a piece of\nwriting. This serves to ensure that the behavior of\nthe user model remains focused and aligned with\nthe intended purpose.\n3.3 Assistance on Existing Materials\nThe third sector mainly addresses the modification\nof existing information, encompassing various\ntasks including rewriting, translation, summa-\nrization, and question-answering, etc. Modifying\nexisting materials is a crucial aspect of human-AI\ninteraction, as it allows the AI assistant to actively\nengage with the user’s input, transforming it in\nvarious ways as instructed by the user.\nWe begin by gathering text pieces from the C4\ncorpus5. Each piece within the C4 corpus is associ-\nated with a source URL. To ensure a diverse range\nof text content and styles, we adopt the 20 material\ntypes outlined in the previous section and manu-\nally curate keywords for each type. Additionally,\nwe classify the text in the corpus by matching the\nkeywords in the corresponding URL. In total, we\ncollect 100,000 text pieces from the C4 corpus, and\nfor each piece, we prompt ChatGPT to generate\nfive distinct instructions. We use a manually de-\nsigned template to combine text and instructions,\nas depicted in Figure 4 in the appendix. Ultimately,\nthe concatenated set of 500,000 pieces serves as\nthe opening lines for the generated dialogues.\n3.4 User Simulation and Refinement\nMaintaining the desired behavior of the user model\nis crucial for achieving successful automatic dia-\nlogue generation. It has been observed that when\nthe user model is solely provided with the current\ndialogue history, it tends to assume the role of an\nAI assistant. This \"role confounding\" situation\n5https://commoncrawl.org/\n3032\nDataset #Dialogue Avg.\n#Turns\nAvg. Dialog Length\n(by token)\nAvg. Utt. Length\n(by token)\nLexical\nDiversity(↑)\nTopic\nDiversity(↓) Coherence(↑) User\nSimulation\nSelf-Instruct 82,439 1 69.8 29.2 24.9 0.733 - No\nStanford Alpaca 52,002 1 91.1 64.5 42.8 0.727 - No\nSODA 1,486,869 3.6 231.8 22.5 38.6 0.797 8.48 No\nGPT-4-LLM 61,002 1 179.6 142.9 48.9 0.721 - No\nBELLE 1,436,679 1 102.3 63.3 35.9 0.771 - No\nBaize 210,311 3.1 293.9 52.8 67.1 0.751 9.06 Yes\nGPT4ALL 711,126 1 597.7 318.9 62.7 0.692 - No\nUltraChat 1,468,352 3.8 1467.4 309.3 74.3 0.702 9.06 Yes\nTable 4: Statistics of existing instruction datasets. Lexical diversity is calculated by averaging the MTLD score (Mc-\nCarthy and Jarvis, 2010) over each utterance with LexicalRichness6. 10000 samples are randomly drawn from each\ndataset for topic diversity and coherence measurement. Topic diversity is measured by averaging the cosine distance\nbetween each pair of data with OpenAI embedding API. Coherence is scored by ChatGPT on a scale of 1-10.\ncan significantly deteriorate the coherence of the\nmulti-turn conversation. To address this, in addi-\ntion to presenting the dialogue history, we include\nprompts explicitly instructing the model to adopt\nvarious user personalities. In Sector 2, a prompt\nis employed to remind the model of the primary\npurpose of the dialogue, thereby promoting a more\nnatural conversation flow. Once the data genera-\ntion process is complete, a further filtration step is\nperformed to ensure overall data quality. We also\nexclude excessively polite statements to enhance\nthe realism of user responses.\n4 Data Analysis\n4.1 Statistical Analysis\nWe conduct a statistical analysis of UltraChat\nand several other instruction datasets, as shown\nin Table 4. UltraChat stands out in terms of its\nscale, being one of the largest publicly available\ndatasets. Moreover, it exhibits the highest average\nnumber of turns and the longest average length per\ninstance of data. While SODA (Kim et al., 2023)\nalso has many rounds, it is primarily composed of\nconceptual banter rather than instructional content.\nAdditionally, the average number of tokens per\ndialogue in SODA is 231.8, whereas UltraChat\nboasts a remarkable 1467.4 tokens. To evaluate di-\nversity, we measure both lexical diversity and topic\ndiversity. UltraChat outperforms previous datasets\nin terms of lexical diversity. However, in terms of\ntopic diversity, UltraChat falls slightly short com-\npared to GPT4ALL (Anand et al., 2023) but still\nsurpasses other datasets significantly. This may be\nattributed to the regularized embeddings resulting\nfrom a large number of tokens in each dialogue. We\nalso conduct coherence evaluation with ChatGPT\nfor multi-turn datasets. Notably, UltraChat and\nBaize data rank the highest in terms of coherence.\n4.2 Human Assessment\nSetup. To better evaluate the constructed data\nquality, we also conduct human assessment for\nUltraChat. Due to the difficulty of evaluation of\nmulti-turn dialogue and the resulting formidable\ncost, we sample 500 representative dialogues for\nhuman evaluation, among which 300 are from Ul-\ntraChat sector 1, 100 from sector 2 and sector 3\nrespectively. For each round of conversation, we\nask the annotators to score the assistant’s response\non Helpfulness, Honesty, and Harmlessness (3H)\nprinciples (Askell et al., 2021). We also devise\nCoherence and Consistency criteria for the overall\nmulti-turn dialogue quality evaluation. Coherence\nevaluates whether the dialogue flows logically and\ncoherently, for which the annotators evaluate both\nthe user’s response and the assistant’s response.\nConsistency means the assistant’s responses do not\ncontradict each other within the same dialogue. For\nexample, it is inconsistent if the assistant asserts\none specific event occurred in 1911 in the first\nround of conversation but mentions it as a 1901\nevent in the next round. Each metric is scored with\n0, 0.5 or 1, where higher score means better quality.\nTherefore, for a K-round dialogue, we have3K +2\nmetric annotations.\nAnnotation. Each dialogue is annotated indepen-\ndently by two well-trained annotators, and the score\nis averaged across two annotators. Meanwhile, due\nto the difficulty in identifying the hallucination\nproblem, we allow the annotators to skip the di-\nalogues that require expert knowledge or whose\nvalidity is hard to check. Altogether, we collect\n14560 valid annotations in terms of metrics for\nboth single-round and multi-round, and the Co-\nhen’s kappa coefficient is 0.358. The average time\nto annotate one dialogue is 10 minutes.\n3033\nResults. As shown in Table 5, the dataset scores\nhigh on all metrics, showing the effectiveness of\nthe construction process. It is worth noting that\nthe dataset is almost free from harmful content like\nhate speech and discrimination. Furthermore, we\nobserve two trade-offs between data quality. The\nfirst is between helpfulness and honesty. While the\nhonesty score is pretty high, helpfulness is compro-\nmised. It is because some user queries are out of\nthe LLM ability scope (e.g., ask the LLM to com-\npose a song). Under such circumstances, the LLM\nrefuses to answer the question with an explanation\nof incapability, which is reasonable and expected\nbut less helpful. The phenomenon is particularly\nprominent in part 3, as the simulated user often\nasks for information not existent in the text mate-\nrial. The second trade-off is between control and\ndialogue naturalness. While sector 2 and sector 3\ndata are constructed with more guidance and con-\ntrol when prompting for user simulation, they have\nclearly lower quality in coherence and consistency\nthan sector 1.\nData Helpful Honest Harmless Coherent Consistent\nSector 10.971 0.996 1.000 0.996 0.995\nSector 20.978 0.986 1.000 0.982 0.977\nSector 30.893 0.983 1.000 0.964 0.981\nOverall 0.960 0.992 1.000 0.987 0.988\nTable 5: Human assessment results on 500 dialogues\nsampled from UltraChat.\n5 Experiments\nWe developed UltraLM, an enhanced variant of\nthe LLaMA-13B (Touvron et al., 2023) model, by\ntraining it on the UltraChat dataset. To improve the\nmodel’s comprehension of dialogue context, we\nbreak down each dialogue into smaller sequences,\nlimiting them to a maximum length of 2048 tokens.\nDuring the training process, we only calculate the\nloss for the model’s responses. This approach en-\nsured that the model had access to the relevant in-\nformation from earlier parts of the conversation, en-\nabling a more comprehensive understanding of the\nongoing dialogue. By incorporating the preceding\ncontext, UltraLM was equipped to generate more\ncontextually appropriate and coherent responses.\nWe use standard cross-entropy loss to finetune the\nmodel. The model is trained with 128 A100 GPUs\nand the total batch size is 512.\nThe evaluation of the trained model is conducted\nin two folds. We first evaluate UltraLM on\ntraditional benchmark datasets to delineate the\nknowledge scope and the multiple abilities of the\nlanguage model. To better demonstrate the chat\nability of language models, an automatic response\nquality evaluation is performed to showcase the\nmodel’s proficiency in delivering accurate and\ninformative content during chat interactions. Note\nthat UltraLM is solely trained on UltraChat dataset\nwithout further finetuning on task specific datasets.\n5.1 Experimental Setup\nBaselines.7 We mainly compare with other open-\nsource instruction-tuned language models based\non LLaMA (Touvron et al., 2023) and Pythia (Bi-\nderman et al., 2023) backbone model. The main\nbaseline models include Alpaca (Taori et al., 2023),\nVicuna (Chiang et al., 2023), Koala (Geng et al.,\n2023), Dolly (Conover et al., 2023), OpenAssis-\ntant (Köpf et al., 2023), and WizardLM (Xu et al.,\n2023a). The parameter size of the baselines ranges\nfrom 7B to 13B, which is comparable to UltraLM.\nOur evaluation also includes other chat language\nmodels like ChatGPT (OpenAI, 2022), MPT (Mo-\nsaic, 2023), and Baize (Xu et al., 2023b). A de-\ntailed description of the main baselines can be\nfound in Appendix A.1.\nDatasets. For benchmark evaluation , we\nchoose four datasets: ARC-Challenge (Clark\net al., 2018), HellaSwag (Zellers et al., 2019),\nMMLU (Hendrycks et al., 2021), and Truth-\nfulQA (Lin et al., 2021), evaluating commonsense\nknowledge, professional knowledge and complex\nreasoning and understanding abilities. Each bench-\nmark is constructed as multiple choice questions\nand therefore metrics are readily computable. The\nfour datasets prove to be challenging even for the\nbest-performing language models like ChatGPT.\nFor response quality evaluation, we use 3\ndatasets. We first create an evaluation set by our-\nselves. The curated set encompasses the Vicuna\nbenchmark as well as an additional 300 questions\nand instructions generated by GPT-4. The ques-\ntions/instructions covered a wide range of topics,\nincluding commonsense, world knowledge, pro-\nfessional knowledge (specifically physics and biol-\nogy), mathematics, response generation, and writ-\ning tasks on different levels of difficulty. Apart\nfrom the curated set, we also adopt AlpacaE-\nval (Li et al., 2023b), a widely acknowledged\n7Some baselines used in our experiments are continuously\nupdated. All the results in this section are from the latest\nversions of these baselines before June 23, 2023\n3034\nModel ARC-Challenge HellaSwag MMLU TruthfulQA Overall\nAverageAcc. Acc. norm. Acc. Acc. norm. Weighted Unweighted mc1 mc2\nDolly-12B 38.23 42.24 54.59 72.6 31.52 31.70 20.69 34.06 45.15\nOpenAssistant-12B 41.38 45.90 52.51 70.04 29.77 30.29 24.60 39.29 46.38\nMPT-7B 43.00 46.67 57.13 75.50 37.76 38.33 27.17 40.16 50.17\nAlpaca-7B 49.74 52.65 58.05 76.91 42.47 42.90 25.83 39.55 53.00\nLLaMA-13B 53.16 56.40 60.64 80.87 46.05 46.74 25.83 39.90 55.98\nBaize-13B 55.55 57.94 59.96 80.36 48.13 49.03 32.93 47.43 58.69\nKoala-13B 49.83 52.90 57.60 77.54 46.75 48.01 34.64 50.09 57.14\nVicuna-13B 51.71 52.90 60.03 80.12 50.15 50.45 35.74 51.82 58.83\nWizardLM-13B 55.12 57.08 60.93 80.91 51.69 52.25 35.37 50.53 60.19\nLLaMA-65B 59.22 63.31 66.40 86.05 62.29 62.97 27.91 42.55 63.72\nUltraLM-13B 57.25 59.22 61.32 81.49 50.45 51.10 36.72 52.00 60.95\nTable 6: The evaluation results on 4 challenging benchmark datasets. All evaluation and metric calculations follow\nEleutherAI’s lm-evaluation-harness (Gao et al., 2021). Both weighted and unweighted mean accuracy are reported\nfor MMLU as there are 57 tasks. The overall average metric is obtained by averaging the second column data for\neach benchmark dataset. More details about metric calculation can be found in Appendix A.3.\nopen-source evaluation set and leaderboard specif-\nically designed for evaluating LLMs. The leader-\nboard is created based on the win-rate against\nText-Davinci-003 automatically evaluated by GPT-\n4. To further compare with the state-of-the-art\nmodel WizardLM (Xu et al., 2023a), comparison\nresult obtained with GPT-4 on the released Evol-\nInstruct (Xu et al., 2023a) test set is also reported.\nFurther benchmark dataset and implementation de-\ntails can be found in Appendix A.2 and A.3.\n5.2 Benchmark Evaluation\nAs shown in Table 6, with pure instruction-tuning\non the UltraChat dataset, UltraLM significantly\nimproves over LLaMA-13B and achieves the best\noverall performance across four benchmarks. It\nis worth noting that UltraLM overtakes the cur-\nrent state-of-the-art model by nearly 2% on ARC-\nChallenge and TruthfulQA. It shows that UltraLM\nis equipped with both broad and profound compre-\nhension of the world and commonsense knowledge.\nThe improvement could be attributed to the system-\natic and comprehensive data construction process\nof UltraChat sector 1, which effectively extends\nand deepens the discussion about world knowledge\nin automatic conversation generation. Meanwhile,\nthe comparative inferiority in MMLU hints on the\nlack of professional knowledge in specific fields.\nIt suggests the need for more advanced data gener-\nation techniques to build a specialized expert lan-\nguage model. As for HellaSwag, we notice that all\nmodels have only marginal improvement compared\nto LLaMA-13B. It is probably because HellaSwag\nis formatted as an in-text completion task instead\nof a straightforward text completion, and therefore\nbenefits little from instruction tuning.\n5.3 Response Quality Evaluation\nResponse Comparison. For our curated evalu-\nation set, we conduct pairwise evaluation between\nUltraLM and each baseline model with GPT-4.\nOur evaluation prompt is designed to prioritize cor-\nrectness over other factors such as informativeness.\nTo mitigate the influence of presentation order\nof responses, we randomly determine the order\nof the responses for each question. Finally, we\ncount the number of Win/Tie/Lose times against\neach baseline model, and the result is presented\nin Figure 2. UltraLM demonstrates superior\nperformance compared to every open-source\nmodel, exhibiting an impressive winning rate of\nup to 98%. It is worth noting that UltraLM also\noutperforms Vicuna with 9% higher winning rate.\nIndependent Scoring. Given the instability of\npairwise comparison, we also conduct independent\nquality scoring with GPT-4, as presented in\nTable 7. Notably, our model demonstrates superior\nperformance compared to all the open-source coun-\nterparts by a significant margin in terms of overall\nscores. This breakdown also provides insights into\nthe performance of each model on specific types of\nquestions and instructions. Generally, all models\nperform better on simpler questions pertaining\nto commonsense knowledge and general world\nunderstanding. However, more complex tasks\nthat involve reasoning and creative writing proves\nto be challenging for most models. Interestingly,\n3035\nWin / Tie / Lose Rate (%)\nVicuna-13B\nWizardLM-13B\nKoala-13B\nBaize-13B\nOpenAssistant-12B\nAlpaca-7B\nMPT-7B\nDolly-12B\n0 100 200 300 400\n4\n35\n26\n46\n79\n76\n85\n117\n2\n6\n34\n42\n22\n84\n102\n115\n375\n340\n321\n293\n280\n220\n194\n149\nWin Tie Lose\n39.1 / 30.2 / 30.7\n50.9 / 26.8 / 22.3\n57.9 / 22.1 / 20.0\n73.5 / 5.80 / 20.7\n76.9 / 11.0 / 12.1\n84.3 / 8.90 / 6.80\n89.2 / 1.60 / 9.20\n98.4 / 0.50 / 1.10\nFigure 2: Response comparison of UltraLM with other baselines on the curated evaluation set, evaluated by GPT-4.\nModel Vicuna\nSet\nCommonsenseWorld KnowledgeProfessional KnowledgeAbility WritingOverallEasy ModerateEasy Difficult Physics Biology Math Reasoning\nDolly-12B 4.75 3.50 3.93 3.10 4.13 4.87 5.47 2.70 2.03 4.51 4.04\nMPT-7B 7.25 5.57 8.20 5.53 5.87 7.83 8.40 5.97 3.97 7.25 6.67\nLLaMA-13B 6.85 8.43 8.43 8.57 8.50 7.90 8.40 6.97 6.73 6.79 7.49\nOpenAssistant-12B7.88 8.13 7.80 9.13 7.50 8.10 8.20 6.57 5.17 7.75 7.65\nAlpaca-7B 7.58 9.17 8.83 9.30 8.73 8.13 8.80 6.70 6.27 8.05 8.04\nKoala-13B 8.00 9.20 9.07 9.00 8.93 8.53 9.07 7.33 5.30 8.40 8.23\nBaize-13B 8.40 9.03 9.10 9.03 8.93 8.83 8.80 7.43 8.30 8.10 8.50\nVicuna-13B 8.48 9.67 9.50 9.37 9.30 9.23 9.33 8.07 6.90 8.76 8.78\nWizardLM-13B 8.55 9.70 9.30 9.57 9.50 9.27 9.53 8.27 8.20 8.83 8.95\nChatGPT 9.15 9.67 9.60 9.80 9.60 9.17 9.73 9.33 9.13 8.95 9.31\nUltraLM-13B 8.98 9.70 9.50 9.47 9.40 9.27 9.87 8.77 6.80 8.90 9.00\nTable 7: The independent overall scoring and segment scoring of each model on the curated evaluation set, on\na scale of 1 to 10. Bold indicates the best score and underlined indicates the second best.\nAlpaca, despite having only 7 billion parameters,\nperforms comparatively well with larger models\non questions related to commonsense and world\nknowledge. Meanwhile, Dolly and OpenAssistant,\nwhich are based on Pythia (Biderman et al., 2023),\ndisplay inferior performance compared to models\nbased on LLaMA of similar or even smaller sizes.\nThis observation highlights the significance of the\nunderlying backbone language model.\nModel Win Rate (%) Standard Error\nGPT-4 95.28 0.72\nClaude 88.39 1.11\nChatGPT 86.09 1.21\nUltraLM-13B 76.09 1.50\nWizardLM-13B 75.31 1.51\nGuanaco-65B 71.80 1.59\nVicuna-13B 70.43 1.61\nOasst-RLHF-33B 66.52 1.66\nText Davinci 003 50.00 0.00\nFalcon-40B-instruct 45.71 1.75\nAlpaca-7B 26.46 1.54\nTable 8: Win rates against Text-Davinci-003 on Al-\npacaEval leaderboard. GPT-4 is used for evaluation\nfollowing the official implementation (Li et al., 2023b).\nAlpacaEval. As shown in Table 8, UltraLM out-\nperforms existing models in terms of win rate on\nthe current AlpacaEval leaderboard and ranks 4th\njust below ChatGPT. This observation further testi-\nfies the response quality of UltraLM and is in line\nwith results on our curated dataset. Furthermore,\na significant gap is evident between UltraLM and\nChatGPT performance, highlighting the substantial\neffort needed for open-source models to match the\ncapabilities of ChatGPT.\nEvol-Instruct Evaluation. Figure 3 shows the\nautomatic comparison results against WizardLM-\n13B on Evol-Instruct test set. UltraLM overtakes\nWizardLM on most types of questions, with up to\n29% increase in scores. It is important to acknowl-\nedge that WizardLM-13B is trained on the Evol-\nInstruct training set, making UltraLM’s success\non the test set a noteworthy achievement. More-\nover, the questions observed with the largest im-\nprovement are mainly complex problems that often\nrequire synthesized abilities. It demonstrates the\nsuccess of UltraChat design schema, which can\ncomprehensively boost the versatile capabilities of\nlanguage models. The inferiority in math prob-\n3036\n/uni00000037/uni00000052/uni0000005b/uni0000004c/uni00000046/uni0000004c/uni00000057/uni0000005c\n /uni00000024/uni00000046/uni00000044/uni00000047/uni00000048/uni00000050/uni0000004c/uni00000046/uni00000003/uni0000003a/uni00000055/uni0000004c/uni00000057/uni0000004c/uni00000051/uni0000004a\n/uni00000030/uni00000058/uni00000056/uni0000004c/uni00000046\n/uni00000035/uni00000052/uni0000004f/uni00000048/uni00000053/uni0000004f/uni00000044/uni0000005c\n/uni00000026/uni00000052/uni00000047/uni00000048/uni00000003/uni00000027/uni00000048/uni00000045/uni00000058/uni0000004a\n/uni00000037/uni00000048/uni00000046/uni0000004b/uni00000051/uni00000052/uni0000004f/uni00000052/uni0000004a/uni0000005c\n/uni00000025/uni0000004c/uni00000052/uni0000004f/uni00000052/uni0000004a/uni0000005c\n/uni00000028/uni00000051/uni00000057/uni00000048/uni00000055/uni00000057/uni00000044/uni0000004c/uni00000051/uni00000050/uni00000048/uni00000051/uni00000057\n/uni00000028/uni00000057/uni0000004b/uni0000004c/uni00000046/uni00000056\n/uni00000033/uni0000004b/uni0000004c/uni0000004f/uni00000052/uni00000056/uni00000052/uni00000053/uni0000004b/uni0000005c\n/uni0000002f/uni0000004c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048\n/uni00000026/uni0000004b/uni00000048/uni00000050/uni0000004c/uni00000056/uni00000057/uni00000055/uni0000005c\n/uni00000026/uni00000052/uni00000050/uni00000053/uni0000004f/uni00000048/uni0000005b/uni00000003/uni00000029/uni00000052/uni00000055/uni00000050/uni00000044/uni00000057\n/uni00000033/uni0000004b/uni0000005c/uni00000056/uni0000004c/uni00000046/uni00000056\n/uni00000026/uni00000052/uni00000058/uni00000051/uni00000057/uni00000048/uni00000055/uni00000049/uni00000044/uni00000046/uni00000057/uni00000058/uni00000044/uni0000004f\n/uni0000002b/uni0000004c/uni00000056/uni00000057/uni00000052/uni00000055/uni0000005c\n/uni00000024/uni00000055/uni00000057\n/uni0000003a/uni00000055/uni0000004c/uni00000057/uni00000057/uni0000004c/uni00000051/uni0000004a\n/uni00000036/uni00000053/uni00000052/uni00000055/uni00000057\n/uni0000002f/uni00000044/uni0000005a\n/uni00000030/uni00000048/uni00000047/uni0000004c/uni00000046/uni0000004c/uni00000051/uni00000048\n/uni00000037/uni00000055/uni00000058/uni00000057/uni0000004b/uni00000049/uni00000058/uni0000004f/uni00000034/uni00000024\n/uni00000028/uni00000046/uni00000052/uni00000051/uni00000052/uni00000050/uni0000005c\n/uni00000030/uni00000058/uni0000004f/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000051/uni0000004a/uni00000058/uni00000044/uni0000004f\n/uni00000035/uni00000048/uni00000044/uni00000056/uni00000052/uni00000051/uni0000004c/uni00000051/uni0000004a\n /uni00000026/uni00000052/uni00000050/uni00000053/uni00000058/uni00000057/uni00000048/uni00000055/uni00000003/uni00000036/uni00000046/uni0000004c/uni00000048/uni00000051/uni00000046/uni00000048\n/uni00000026/uni00000052/uni00000050/uni00000050/uni00000052/uni00000051/uni00000010/uni00000036/uni00000048/uni00000051/uni00000056/uni00000048\n/uni00000026/uni00000052/uni00000047/uni00000048/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000030/uni00000044/uni00000057/uni0000004b\n/uni00000013/uni00000008\n/uni00000015/uni00000013/uni00000008\n/uni00000017/uni00000013/uni00000008\n/uni00000019/uni00000013/uni00000008\n/uni0000001b/uni00000013/uni00000008\n/uni00000014/uni00000013/uni00000013/uni00000008\n/uni00000014/uni00000015/uni00000013/uni00000008\n/uni00000038/uni0000004f/uni00000057/uni00000055/uni00000044/uni0000002f/uni00000030/uni00000003/uni00000012/uni00000003/uni0000003a/uni0000004c/uni0000005d/uni00000044/uni00000055/uni00000047/uni0000002f/uni00000030\n/uni00000014/uni00000015/uni0000001c/uni00000011/uni0000001b/uni00000008\n/uni00000014/uni00000015/uni0000001c/uni00000011/uni00000019/uni00000008\n/uni00000014/uni00000015/uni00000013/uni00000011/uni00000018/uni00000008\n/uni00000014/uni00000014/uni0000001b/uni00000011/uni0000001a/uni00000008\n/uni00000014/uni00000014/uni00000019/uni00000011/uni00000014/uni00000008\n/uni00000014/uni00000014/uni00000018/uni00000011/uni00000014/uni00000008\n/uni00000014/uni00000014/uni00000016/uni00000011/uni00000018/uni00000008\n/uni00000014/uni00000014/uni00000016/uni00000011/uni00000016/uni00000008\n/uni00000014/uni00000014/uni00000016/uni00000011/uni00000013/uni00000008\n/uni00000014/uni00000014/uni00000015/uni00000011/uni00000018/uni00000008\n/uni00000014/uni00000013/uni0000001c/uni00000011/uni0000001b/uni00000008\n/uni00000014/uni00000013/uni0000001c/uni00000011/uni00000014/uni00000008\n/uni00000014/uni00000013/uni0000001a/uni00000011/uni0000001c/uni00000008\n/uni00000014/uni00000013/uni00000019/uni00000011/uni00000015/uni00000008\n/uni00000014/uni00000013/uni00000019/uni00000011/uni00000013/uni00000008\n/uni00000014/uni00000013/uni00000019/uni00000011/uni00000013/uni00000008\n/uni00000014/uni00000013/uni00000016/uni00000011/uni00000018/uni00000008\n/uni00000014/uni00000013/uni00000016/uni00000011/uni00000013/uni00000008\n/uni00000014/uni00000013/uni00000015/uni00000011/uni00000017/uni00000008\n/uni00000014/uni00000013/uni00000015/uni00000011/uni00000017/uni00000008\n/uni00000014/uni00000013/uni00000013/uni00000011/uni00000013/uni00000008\n/uni0000001c/uni0000001b/uni00000011/uni0000001b/uni00000008\n/uni0000001c/uni0000001b/uni00000011/uni0000001b/uni00000008\n/uni0000001c/uni0000001a/uni00000011/uni00000018/uni00000008\n/uni0000001c/uni00000019/uni00000011/uni00000018/uni00000008\n/uni0000001c/uni00000019/uni00000011/uni00000014/uni00000008\n/uni0000001c/uni00000018/uni00000011/uni0000001c/uni00000008\n/uni0000001c/uni00000017/uni00000011/uni00000014/uni00000008\n/uni0000001a/uni0000001c/uni00000011/uni00000016/uni00000008\n/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni0000001d/uni00000003/uni00000014/uni00000013/uni00000015/uni00000011/uni0000001b/uni00000008\nFigure 3: Comparison between UltraLM and WizardLM-13B on Evol-Instruct test set. The scores are obtained by\npairwise scoring with GPT-4, and WizardLM scores are considered as 100%.\nlems is not surprising though, as no mathematical\nproblems are intentionally generated in UltraChat.\nImpact of System Prompts. Using system\nprompts to adjust the role and response style of\nLLMs is a common practice. Although system\nprompts are not embedded in UltraLM’s training\ndata like others do (Chiang et al., 2023), they still\nappear to have a substantial influence on the re-\nsponse style of the generated output. Specifically,\nwhen the model is prompted to provide a \"helpful\nand detailed\" response, it tends to generate more\npertinent details While such prompts may not im-\nprove the accuracy of an answer, they do raise over-\nall quality with more informative response. To illus-\ntrate this effect, we conduct an ablation response\ncomparison on UltraLM. Table 9 reveals signifi-\ncant improvements in response quality brought by\nsystem prompts across all tasks. We also inspect\nthe detailed evaluation for deterministic questions\n(commonsense and world knowledge) and find that\nUltraLM without system prompt only incorrectly\nanswers one question. Thus, the main benefit of\nthe system prompt lies in enhanced informativeness\nrather than higher correctness.\nData Win (%) Tie (%) Lose (%)\nVicuna Set 36.3 35.0 28.8\nCommonsense 58.6 31.0 10.3\nWorld Knowledge 56.9 34.5 8.6\nProfessional Knowledge57.8 31.1 11.1\nMath Ability 46.7 13.3 40.0\nReasoning Ability 46.7 33.3 20.0\nWriting 46.3 35.0 18.8\nOverall 49.1 32.0 18.9\nTable 9: Win rate against UltraLM without system\nprompt on our curated dataset.\n6 Conclusion\nIn drawing to a close, our work introduces Ultra-\nChat, a structured design of multi-turn instructional\nconversation data primed to foster the growth of\ngeneral chat models. UltraChat encapsulates a\nbroad range of human-AI interactions, further de-\nveloping a series of dialogues across various topics\nand instructions. Statistically, UltraChat shows\nan impressive presence in critical metrics such as\nscale, average length, diversity, and consistency,\nfurther establishing itself as a leading open-source\ndataset. We leverage UltraChat to fine-tune the\nLLaMA model, leading to the development of the\nrobust conversational model, UltraLM. Evaluation\nacross multiple benchmarks reveals that UltraLM\nsurpasses previous open-source models like Wiz-\nardLM, Vicuna, Alpaca, and Koala in performance.\nWe eagerly await the innovative research and devel-\nopment that will be catalyzed by our contributions\nin the field of AI conversational models.\nLimitations\nEvaluating the response quality of large language\nmodels is an extremely challenging task, and any\nassessments may have biases. For a comprehen-\nsive evaluation, we compared UltraLM’s perfor-\nmance with other baselines across various bench-\nmarks, utilizing GPT-4 to assess the response qual-\nity. Nevertheless, the need for additional, diverse\nevaluations remains to facilitate a more thorough\nunderstanding of our model’s behavior and perfor-\nmance. Despite demonstrating promising results\nin experimental settings, UltraLM is not immune\nto the common pitfalls of large language models,\nincluding hallucination issues and potential ethi-\ncal concerns associated with misuse. Additionally,\nthe energy-intensive nature of UltraLM’s training\nprocess represents a limitation, particularly when\ncompared to models employing more efficient tech-\nniques such as parameter-efficient fine-tuning. In\nterms of UltraChat, it currently only contains En-\nglish and there are no explicit methodologies incor-\nporated for generating data to enhance the model’s\nreasoning capabilities, representing another area\nfor potential improvement.\n3037\nEthics Statement\nWhile the advancements of the UltraChat dataset\nand UltraLM are commendable, they still face ethi-\ncal challenges that exist in the area of LLMs. For\nthe sake of privacy protection, we do not use any\nonline or even human queries/instructions to con-\nstruct UltraChat but develop a framework to build\nscalable, diverse instructional data. Although ex-\ntensive filtering operations are conducted, biased\nstatements may still exist within the dataset.\nUltraLM, as the paper described, will be one of\nthe most potent open-source chat language models.\nWith great power comes increased responsibility\nand potential for misuse. There exists a substantial\nrisk for the technology to be weaponized for spread-\ning misinformation, propaganda, or even creating\n“deepfake” text that could mislead or manipulate\npublic discourse. This necessitates the establish-\nment of robust policies and comprehensive research\nto prevent misuse and deter malicious applications\nof the technology. In this regard, the model’s po-\ntential applications should be carefully evaluated\nfor any possible negative consequences before de-\nployment.\nAcknowledgements\nThis work is supported by the National Key R&D\nProgram of China (No. 2022ZD0119101), Na-\ntional Natural Science Foundation of China (No.\n62236004), the Young Elite Scientists Sponsorship\nProgram by CAST, and Institute Guo Qiang at Ts-\ninghua University.\nReferences\nYuvanesh Anand, Zach Nussbaum, Brandon Duder-\nstadt, Benjamin Schmidt, and Andriy Mulyar. 2023.\nGpt4all: Training an assistant-style chatbot with large\nscale data distillation from gpt-3.5-turbo.\nAmanda Askell, Yuntao Bai, Anna Chen, Dawn Drain,\nDeep Ganguli, Tom Henighan, Andy Jones, Nicholas\nJoseph, Ben Mann, Nova DasSarma, Nelson El-\nhage, Zac Hatfield-Dodds, Danny Hernandez, Jack-\nson Kernion, Kamal Ndousse, Catherine Olsson,\nDario Amodei, Tom Brown, Jack Clark, Sam Mc-\nCandlish, Chris Olah, and Jared Kaplan. 2021. A\ngeneral language assistant as a laboratory for align-\nment.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda\nAskell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan, et al.\n2022. Training a helpful and harmless assistant with\nreinforcement learning from human feedback. arXiv\npreprint arXiv:2204.05862.\nStella Biderman, Hailey Schoelkopf, Quentin Anthony,\nHerbie Bradley, Kyle O’Brien, Eric Hallahan, Mo-\nhammad Aflah Khan, Shivanshu Purohit, USVSN Sai\nPrashanth, Edward Raff, et al. 2023. Pythia: A suite\nfor analyzing large language models across training\nand scaling. arXiv preprint arXiv:2304.01373.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli,\nRuss Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosse-\nlut, Emma Brunskill, et al. 2021. On the opportuni-\nties and risks of foundation models. arXiv preprint\narXiv:2108.07258.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In Proceedings of NeurIPS.\nSahil Chaudhary. 2023. Code alpaca: An instruction-\nfollowing llama model for code generation.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the ai2 reasoning challenge. ArXiv\npreprint, abs/1803.05457.\nMike Conover, Matt Hayes, Matt Mathur, Xiangrui\nMeng, Jianwei Xie, Jun Wan, Ali Ghodsi, Patrick\nWendell, and Patrick Zaharia. 2023. Hello dolly: De-\nmocratizing the magic of chatgpt with open models.\nNing Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zong-\nhan Yang, Yusheng Su, Shengding Hu, Yulin Chen,\nChi-Min Chan, Weize Chen, et al. 2023. Parameter-\nefficient fine-tuning of large-scale pre-trained lan-\nguage models. Nature Machine Intelligence, pages\n1–16.\nDomeccleston. 2023. Sharegpt – share your wildest\nchatgpt conversations with one click. Retrieved 23\nMay 2023.\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black,\nAnthony DiPofi, Charles Foster, Laurence Golding,\nJeffrey Hsu, Kyle McDonell, Niklas Muennighoff,\nJason Phang, Laria Reynolds, Eric Tang, Anish Thite,\n3038\nBen Wang, Kevin Wang, and Andy Zou. 2021. A\nframework for few-shot language model evaluation.\nXinyang Geng, Arnav Gudibande, Hao Liu, Eric Wal-\nlace, Pieter Abbeel, Sergey Levine, and Dawn Song.\n2023. Koala: A dialogue model for academic re-\nsearch. Blog post.\nFabrizio Gilardi, Meysam Alizadeh, and Maël Kubli.\n2023. Chatgpt outperforms crowd-workers for text-\nannotation tasks. arXiv preprint arXiv:2303.15056.\nXu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao\nLiu, Yuqi Huo, Jiezhong Qiu, Liang Zhang, Wentao\nHan, Minlie Huang, Qin Jin, Yanyan Lan, Yang Liu,\nZhiyuan Liu, Zhiwu Lu, Xipeng Qiu, Ruihua Song,\nJie Tang, Ji-Rong Wen, Jinhui Yuan, Wayne Xin\nZhao, and Jun Zhu. 2021. Pre-trained models: Past,\npresent and future. AI Open.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2021. Measuring massive multitask language under-\nstanding. Proceedings of ICLR.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2021. Lora: Low-rank adaptation of\nlarge language models. In Proceedings of ICLR.\nYunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang\nNiu, Lei Zhang, Baochang Ma, and Xiangang Li.\n2023. Exploring the impact of instruction data\nscaling on large language models: An empirical\nstudy on real-world use cases. arXiv preprint\narXiv:2303.14742.\nHyunwoo Kim, Jack Hessel, Liwei Jiang, Peter West,\nXiming Lu, Youngjae Yu, Pei Zhou, Ronan Le Bras,\nMalihe Alikhani, Gunhee Kim, Maarten Sap, and\nYejin Choi. 2023. Soda: Million-scale dialogue dis-\ntillation with social commonsense contextualization.\nAndreas Köpf, Yannic Kilcher, Dimitri von Rütte,\nSotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens,\nAbdullah Barhoum, Nguyen Minh Duc, Oliver Stan-\nley, Richárd Nagyfi, et al. 2023. Openassistant\nconversations–democratizing large language model\nalignment. arXiv preprint arXiv:2304.07327.\nGuohao Li, Hasan Abed Al Kader Hammoud, Hani\nItani, Dmitrii Khizbullin, and Bernard Ghanem.\n2023a. Camel: Communicative agents for \"mind\"\nexploration of large scale language model society.\nXuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori,\nIshaan Gulrajani, Carlos Guestrin, Percy Liang, and\nTatsunori B. Hashimoto. 2023b. Alpacaeval: An\nautomatic evaluator of instruction-following models.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2021.\nTruthfulqa: Measuring how models mimic human\nfalsehoods. arXiv preprint arXiv:2109.07958.\nShayne Longpre, Le Hou, Tu Vu, Albert Webson,\nHyung Won Chung, Yi Tay, Denny Zhou, Quoc V\nLe, Barret Zoph, Jason Wei, et al. 2023. The flan\ncollection: Designing data and methods for effective\ninstruction tuning. arXiv preprint arXiv:2301.13688.\nPhilip M. McCarthy and Scott Jarvis. 2010. Mtld, vocd-\nd, and hd-d: A validation study of sophisticated ap-\nproaches to lexical diversity assessment. Behavior\nResearch Methods, 42:381–392.\nMosaic. 2023. Introducing mpt-7b: A new standard for\nopen-source, commercially usable llms.\nOpenAI. 2023. Gpt-4 technical report. arXiv.\nTB OpenAI. 2022. Chatgpt: Optimizing language mod-\nels for dialogue. OpenAI.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,\net al. 2021. Multitask prompted training enables zero-\nshot task generalization. In Proceedings of ICLR.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal,\nAlec Radford, and Oleg Klimov. 2017. Proxi-\nmal policy optimization algorithms. arXiv preprint\narXiv:1707.06347.\nQingyi Si, Tong Wang, Naibin Gu, Rui Liu, and Zheng\nLin. 2023. Alpaca-cot: An instruction fine-tuning\nplatform with instruction data collection and unified\nlarge lnguage models interface.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B Hashimoto. 2023. Alpaca: A strong,\nreplicable instruction-following model. Stanford\nCenter for Research on Foundation Models.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\n3039\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2022. Self-instruct: Aligning language\nmodel with self generated instructions.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021. Finetuned lan-\nguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,\nPu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\nJiang. 2023a. Wizardlm: Empowering large lan-\nguage models to follow complex instructions.\nCanwen Xu, Daya Guo, Nan Duan, and Julian McAuley.\n2023b. Baize: An open-source chat model with\nparameter-efficient tuning on self-chat data. arXiv\npreprint arXiv:2304.01196.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. HellaSwag: Can a\nmachine really finish your sentence? In Proceedings\nof ACL, pages 4791–4800.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao\nSun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,\nL. Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke\nZettlemoyer, and Omer Levy. 2023. Lima: Less is\nmore for alignment.\n3040\nA Experimental Details\nA.1 Baselines\nWe introduce the main open-source baseline mod-\nels below.\nAlpaca (Taori et al., 2023) is an instruction-\nfollowing language model derived from the\nLLaMA (Touvron et al., 2023) model that has been\neffectively optimized on 52,000 demonstrations of\ninstruction data. The data is generated by Self-\nInstruct approach with Text-Davinci-003.\nVicuna-13B (Chiang et al., 2023) is an open-\nsourced chat model created by fine-tuning LLaMA\non user-shared conversations collected from\nShareGPT8. An automatic evaluation by GPT-4\ndemonstrates that Vicuna can yield over 90% re-\nsponse quality of ChatGPT. In following practices,\nVicuna is widely acknowledged as the state-of-the-\nart open-source chat model. This is evident in the\nChat Arena9, where a total of 13,000 anonymous\nvotes reveal that the quality score of vicuna-13B\nsurpasses that of other open-source models.\nKoala-13B (Geng et al., 2023) is another LLaMA-\nbased model fine-tuned on selected public dia-\nlogues. In existing open evaluations, Koala’s per-\nformance will be slightly worse than vicuna, but it\nstill remains a strong baseline.\nDolly-V2 (Conover et al., 2023) is based on the\nPythia (Biderman et al., 2023) model, which uti-\nlizes 15k human-generated instruction-following\ndata. The data is organized by following Instruct-\nGPT (Ouyang et al., 2022), including brainstorm-\ning, classification, closed QA, generation, informa-\ntion extraction, open QA, and summarization.\nOpenAssistant-12B (Köpf et al., 2023) is also a\nPythia-based model that attempts to democratize\nthe alignment process of LLMs. The project col-\nlects a conversation corpus consisting of 161,443\nmessages distributed across 66,497 conversation\ntrees and trains a model on these manually anno-\ntated data.\nWizardLM-13B (Xu et al., 2023a) is a LLaMA-\nbased model finetuned on Evol-Instruct dataset,\nwhich contains 250k instructions. The data are gen-\nerated with evolutionary strategy, where the initial\ninstructions are rewritten by ChatGPT for several\nepochs to increase complexity progressively.\n8https://sharegpt.com/\n9https://lmsys.org/blog/\n2023-05-03-arena/\nA.2 Evaluation Dataset\nTable 11 presents the basic information of the eval-\nuation datasets we use. Below we give a detailed\ndescription of each dataset.\nThe AI2 Reasoning Challenge (ARC) (Clark\net al., 2018) is comprised of advanced science\nquestions and structured as multiple-choice ques-\ntions. Each question is accompanied by 4 available\nchoices and only one answer is correct. We use\nthe challenge partition here for evaluation, which\ncontains 1172 test examples.\nHellaSwag (Zellers et al., 2019) tests common-\nsense inference ability by evaluating how well the\nlanguage model can predict the remaining part of a\nsentence. Each sample has 4 different text pieces\nas the candidate remaining part of a given sentence\nand only one of them is plausible. The task is\nshown to be easy for humans but challenging for\nlanguage models. We use the validation split as\nin Gao et al. (2021).\nMMLU (Hendrycks et al., 2021) is a comprehen-\nsive dataset that consists of 57 different tasks cov-\nering assessments of multiple types of academic\nknowledge and problem-solving ability of language\nmodels, ranging over expert fields like mathemat-\nics, history, law, etc.\nTruthfulQA (Lin et al., 2021) assesses how well\na model can identify true statements related to the\nreal world. Its purpose is to determine the risks of\nproducing false claims or spreading misinforma-\ntion. The benchmark consists of questions written\nin various styles, covering 38 different categories,\nand is designed to be challenging. It includes two\nevaluation tasks: the multiple-choice task and the\ngeneration task. We use the multiple-choice task in\nthe validation split as in Gao et al. (2021).\nAlpacaEval (Li et al., 2023b) is a hybrid evalua-\ntion dataset with altogether 805 instructions, which\ncombines instructions from various existing evalua-\ntion sets, including self-instruct (Wang et al., 2022),\nOpen Assistant (Köpf et al., 2023), helpful evalu-\nation released by Anthropic (Bai et al., 2022), Vi-\ncuna (Chiang et al., 2023), and Koala (Geng et al.,\n2023).\nEvol-Instruct is a dataset released in Xu et al.\n(2023a). It is constructed with an evolutionary strat-\negy by rewriting the instructions through multiple\nrounds to obtain instructions at different complex-\nity levels. WizardLM (Xu et al., 2023a) is trained\non the training set. We evaluate our model on the\ntest set.\n3041\nType Example\nCommonsense- Easy What is the primary source of energy for our planet?\nCommonsense-ModerateWhat is the phenomenon that causes the change in pitch heard when a vehicle sounding a\nhorn approaches and recedes from an observer?\nWorld Knowledge-Easy What is the freezing point of water in Fahrenheit?\nWorld Knowledge-ModerateWhat is the Gödel’s Incompleteness Theorem?\nPhysics Knowledge How does quantum entanglement work and what are its implications for information transfer?\nBiology Knowledge What are the four main types of macromolecules found in living organisms?\nMath What is the Taylor series expansion of the functionex?\nReasoning You have two buckets, one with red paint and one with blue paint. You take one cup from\nthe red bucket and pour it into the blue bucket. Then you take one cup from the blue bucket\nand pour it back into the red bucket. Which is true: the red bucket has more blue paint, or\nthe blue bucket has more red paint?\nWriting Write a dialogue between two photons traveling at light speed.\nTable 10: Some examples of our created evaluation set.\nDataset # Examples Domain\nARC-Challenge 1172 Grade-school\nHellaSwag 10042 Commonsense\nMMLU 14042 Academic\nTruthfulQA 817 Truthfulness\nAlpacaEval 805 Comprehensive\nEvol-Instruct 218 Comprehensive\nOur evaluation set 831 Comprehensive\nTable 11: Basic information on evaluation dataset.\nOur evaluation setis composed of Vicuna (Chiang\net al., 2023) test set and other instructions generated\nby GPT-4. The generated instructions are further\nsplit into different categories and difficulty levels\nto comprehensively evaluate the model’s ability.\nTable 10 shows example data for each type of ques-\ntion.\nA.3 Implementation Details\nIn benchmark evaluation, the multiple choice ques-\ntions are further transformed into log-likelihood\ncalculation over each answer candidate to deter-\nmine the final prediction. Specifically, each avail-\nable answer is appended to the question and fed\ninto the language model. The model calculates log-\nits for the answer part and derives the likelihood\nof the answer. The most likely answer is taken as\nthe final prediction. Moreover, we use likelihood\nnormalized by answer length to mitigate the bias\ncaused by length variation. For the first 3 bench-\nmarks, accuracy is reported for each test sample\ndirectly. For Truthful QA, since there are multi-\nple correct answers, apart from standard accuracy\n(mc1), we also report the ratio of likelihood sum\nof correct answers over all candidate answers (the\nmc2 metric). In Table 6, Acc. norm. means the ac-\ncuracy is computed after normalizing the likelihood\nwith response length.\nIn response quality evaluation, all evaluations\nare done by GPT-4. Figure 5 and Figure 6 present\nprompts used for pairwise scoring and independent\nscoring on our evaluation data. As for AlpacaEval\nand Evol-Instruct, we follow the official prompt\nimplementation (Li et al., 2023b; Xu et al., 2023a)\nto ensure a fair comparison.\nAcross all experiments, UltraLM is run with tem-\nperature=0.5, top_p=0.95, max new token=2000,\nand early stopping.\nB UltraChat Details\nWe present a comparison between multi-turn dia-\nlogues directly generated by ChatGPT the one in\nUltraChat in Table 12. Both have the same opening\nline. Figure 4 is the template used to concatenate\ntext pieces from the C4 corpus and instructions gen-\nerated by ChatGPT for data in UltraChat sector 3.\nC Prompts\nIn this section, we provide system prompt used for\nresponse generation by UltraLM (Figure 7) and\nprompts for automatic evaluation by GPT-4 (Fig-\nure 5 and 6).\nD UltraLM Generation Examples\nIn this section, we list response examples of\nUltraLM-13B on different instructions from our\n3042\nTemplates\n{text}\\n{instruction}\n{text} {instruction}\n{instruction} Answer according to: {text}\n{text} Based on the passage above, {instruction}\n{instruction}: {text}\nGiven the text: {text}\\n{instruction}\n{instruction}\\nGenerate according to: {text}\nFigure 4: Manually designed templates for concatenating existing materials and generated instructions.\nComparison Evaluation Prompt\nYou are a helpful, harmless and precise assistant for checking the quality of the answer.\n[Question]\n[The Start of Assistant 1’s Answer]\n[The End of Assistant 1’s Answer]\n[The Start of Assistant 2’s Answer]\n[The End of Assistant 2’s Answer]\nWe would like to request your feedback on the performance of two AI assistants in response to the\nuser question displayed above. Please rate the quality, helpfulness, level of details, and harmless\nof their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher\nscore indicates better overall performance. Please first output a single line containing only two\nvalues indicating the scores for Assistant 1 and 2, respectively. The two scores are separated by\na space. In the subsequent line, please provide a comprehensive explanation of your evaluation,\navoiding any potential bias and ensuring that the order in which the responses were presented does\nnot affect your judgment. Please avoid same scores unless they exactly in the same level.\nFigure 5: Prompt for automatic comparison evaluation.\nevaluation set.\n3043\nIndependent Scoring Prompt\nYou are a helpful, harmless and precise assistant for checking the quality of the answer.\n[Question]\n[The Start of the AI Assistant’s Answer]\n[The End of the AI Assistant’s Answer]\nWe would like to request your feedback on the performance of the AI assistant in response to\nthe user question displayed above. Please rate the quality, helpfulness, level of details, and\nharmlessness of their responses. The assistant receives an overall score on a scale of 1 to 10, where\na higher score indicates better overall performance. Please output \"Score: [an integer number\nbetween 1 and 10]\"\nFigure 6: Prompt for automatic independent scoring evaluation.\nSystem Prompt\nYou are now a helpful assistant, your answer should focus on the question, give reasonable, self-\nconsistent, high-quality solutions to make users satisfied. Please try to make the answer more\ninformative, detailed, and polite.\nFigure 7: System prompt used for generating responses in automatic evaluation.\nExample A: Question of World Knowledge\nUser: Who discovered the law of universal gravitation?\nUltraLM: The law of universal gravitation was discovered by Sir Isaac Newton in the 17th century.\nHe formulated the law while studying the planets’ movement around the sun and realized that\nthere was a quantifiable force directing them towards the sun. In 1687, he published his book\ntitled \"Mathematical Principles of Natural Philosophy,\" where he described the law of universal\ngravitation in detail, outlining how two objects experience an attraction towards each other due\nto their mass and distance between them. This discovery revolutionized our understanding of the\nuniverse and ushered in the Age of Enlightenment. Today, Newton’s law continues to form the\nfoundation of modern astrophysics and space exploration.\nExample B: Question of World Knowledge\nUser: Who was the 16th President of the United States?\nUltraLM: The 16th President of the United States was Abraham Lincoln. He assumed the\npresidency during a very tumultuous time in American history, as the nation was torn apart by the\nCivil War. During his presidency, he was known for his strong leadership, his commitment to the\npreservation of the Union, and his support for the abolition of slavery. Some of his most notable\nachievements include issuing the Emancipation Proclamation, which declared that all slaves in the\nConfederate States were free, and delivering the Gettysburg Address, an iconic speech honoring\nthe soldiers who died in the Civil War. Lincoln is widely regarded as one of the greatest American\nPresidents, remembered for his unwavering leadership during a challenging period in the nation’s\nhistory.\n3044\nDirect Generation\nWhat are some of the health benefits and risks of drinking coffee, and how can one maximize the former\nand minimize the latter?\nThere are several potential health benefits associated with drinking coffee, such as increased alertness,\nimproved mood, and a reduced risk of certain diseases, including type 2 diabetes and liver cancer. However,\nthere are also some potential risks, such as increased anxiety, disrupted sleep, and negative effects on the\ndigestive system. To maximize the benefits and minimize the risks, it’s important to consume coffee in\nmoderation and pay attention to how it affects your body.\nHow much coffee is considered moderate?\nThe definition of moderate coffee consumption can vary depending on the individual and other factors\nsuch as age, gender, and health status. However, in general, moderate coffee consumption is considered to\nbe around 3-5 cups per day, or up to 400 milligrams of caffeine per day. It’s important to note that some\npeople may be more sensitive to caffeine than others, so it’s important to pay attention to how your body\nreacts and adjust your consumption accordingly.\nUltraChat\nWhat are some of the health benefits and risks of drinking coffee, and how can one maximize the former\nand minimize the latter?\nThere are several health benefits and risks associated with drinking coffee. Here are some of them: Health\nbenefits:1. Lower risk of type 2 diabetes: Studies have shown that consuming coffee can lower the risk\nof developing type 2 diabetes.2. Improved cognitive function: Coffee is a stimulant that helps improve\ncognitive function, including alertness, concentration, and memory.3. Lower risk of depression: Studies\nhave shown that drinking coffee can reduce the risk of developing depression.4. Lower risk of certain\ncancers: According to research, coffee consumption can lower the risk of certain cancers, including liver\nand colorectal cancer.5. Reduced risk of heart disease: There is evidence that drinking moderate amounts\nof coffee can lower the risk of heart disease.Risks:1. Insomnia: Drinking too much coffee, especially\nlate in the day, can cause insomnia.2. Increased risk of anxiety: Caffeine can increase anxiety levels in\nsome people.3. Digestive issues: Drinking too much coffee can cause digestive problems like acid reflux,\ndiarrhea, and ulcers.4. Increased heart rate: Caffeine can increase heart rate, which can be dangerous\nfor people with pre-existing heart conditions.5. Addiction: Regular consumption of caffeine can lead to\naddiction.To maximize the health benefits and minimize the risks of drinking coffee, here are a few tips:1.\nLimit your caffeine intake to no more than 400 milligrams per day, or about 4 cups of coffee.2. Avoid\ndrinking coffee late in the day, especially if you have trouble sleeping.3. Opt for organic, pesticide-free\ncoffee to minimize exposure to toxins.4. Add cinnamon to your coffee instead of sugar to help regulate\nblood sugar levels.5. Drink your coffee black or with a small amount of milk to avoid consuming excessive\nsugar and calories.6. Consider switching to decaf coffee if you want to cut back on caffeine. 7. Avoid\ndrinking coffee on an empty stomach as it can increase the risk of digestive problems.\nI love my morning coffee, but sometimes it makes me feel a bit jittery. What can I do to avoid this?\nIf you love your morning coffee but find that it’s making you feel jittery, here are a few tips to help you\navoid this:1. Choose a lower-caffeine coffee: Look for coffee that has less caffeine, like decaf or a blend\nwith a lower percentage of caffeine.2. Don’t drink coffee on an empty stomach: Drinking coffee on an\nempty stomach can cause your blood sugar levels to drop, which can make you feel jittery. Try drinking\nyour coffee with breakfast or after eating something.3. Drink water with your coffee: Stay hydrated by\ndrinking water before or after your coffee. Drinking water will help flush out the caffeine quicker, which\ncan help you avoid jitteriness.4. Don’t drink too much coffee: Stick to one or two cups of coffee per day.\nAnything more than that could result in increased caffeine consumption, which can cause jitteriness.5.\nSwitch to tea: Tea contains less caffeine than coffee, making it a good alternative if you want to avoid\njitteriness. Try green tea, which has less caffeine and offers other health benefits.\nTable 12: A comparison of two dialogues with the same opening line. The former is generated by directly querying\nChatGPT to output a dialogue, and the latter is generated by iteratively User-AI simulations.\n3045\nExample C: Question of Common Knowledge\nUser: Which animal is known as man’s best friend?\nUltraLM: There are many animals that humans enjoy as companion animals, but one of the\nabsolute standout answers to the question \"which animal is known as a man’s best friend?\" is\nthe domestically-bred dog. Domesticated dogs have been by humans’ sides for tens of thousands\nof years, providing faithful companionship and help; guarding people and property; and lending\nemotional support in everything from long, lonely days to military deployments. Dogs come in an\nalmost mind-boggling variety of breeds of various sizes, with different energy levels, personalities,\nand temperaments, meaning there’s almost certainly a pup out there who will be a perfect fit for\nwhoever needs it most.\nExample D: Commonsense Reasoning\nUser: Which weighs more: a pound of feathers or a pound of bricks?\nUltraLM: A pound of feathers and a pound of bricks both weigh exactly one pound. This is\nbecause one pound is a unit of weight, just as a foot is a unit of length or a second is a unit of time.\nWe use the pound (or other unit of weight) to describe the weight of different objects, regardless of\nwhat that object is made of. So the answer to your question is that a pound of feathers and a pound\nof bricks both weigh the same amount.\nExample E: Reasoning\nUser: \"If a farmer has 17 sheep and all but 9 die, how many are left?\nUltraLM: The total number of sheep that are left is 9.\nExample F: Reasoning\nUser: If a red house is made from red bricks, and a blue house is made from blue bricks, what is a\ngreenhouse made from?\nUltraLM: A greenhouse is not made from green bricks, but rather from materials such as glass\nor plastic that allow light to pass through and create a controlled environment for plants to grow.\nGreenhouses can come in a variety of colors, including green, but the color of the building itself\ndoes not affect the plants growing inside.\n3046\nExample G: Creative Generation\nUser: Write a short dialogue between two planets.\nUltraLM:\nMars: \"Hey there Neptune, how’s it going?\"\nNeptune: \"Hi there Mars, I’ve been better. It’s getting a bit windy around here.\"\nMars: \"Sorry to hear that. Have you tried using energy other than wind? Maybe solar power?\"\nNeptune: \"Yeah, but it’s getting difficult to generate solar power in all of this mist and clouds.\nAnd we definitely can’t use tides or waves here, they’re too unpredictable.\"\nMars: \"Oh, I see. Well, I hope you find a solution soon. How are your oceans holding up?\"\nNeptune: \"They’ve been a bit choppy lately with these intense storms we’ve been experiencing.\nBut I guess it comes with living in a planet like this.\"\nMars: \" Definitely not designed for the faint-hearted! Any other issues you’re facing?\"\nNeptune: \"Not really. I think we have a stable temperature and atmosphere, and the wildlife\nhere hasn’t shown much complaint.\"\nMars: \"That’s lucky. I envy your tranquil oceans, we’ve had way too much volcanic activity\nlately causing all sorts of disturbance in our lava streams.\"\nNeptune: \"That sounds rough, better invest in those climate-controlling nanites to stabilize\nthings maybe?\"\nMars \"Ha-ha, very funny. Just like we also could consider your frozen polar regions as an\nopportunity to experiment with our frost nanobots on.\"\nNeptune: \"A fun concept! But let’s stick to the task at hand, if you need any technical advice,\ncome right ahead, I don’t want your planet getting any more disturbed than it already is.\nExample H: Creative Generation\nUser: Pretend to be a character in a post-apocalyptic world. Describe how you survive and the\nallies you encounter.\nUltraLM: As a survivor in a post-apocalyptic world, I have learned to be resourceful, alert, and\nadaptable. I have had to rely on my wits and my instincts to survive, and I have been fortunate\nenough to encounter many allies who have helped me along the way.One of my greatest survival\ntools has been my ability to scavenge for resources. I have learned to identify the best places to\nfind food, water, and shelter, and I have become skilled at navigating the rough terrain that makes\nup my new world. When I first emerged from my bunker after the apocalypse hit, I was alone and\nunsure of what to do. But as I ventured out into the wasteland, I realized that there were others\nlike me who were struggling to survive.I quickly learned to Trust my instincts and to be constantly\nalert for potential dangers. I have encountered wild animals, rogue survivors, and even mutated\ncreatures over the years, and I have learned to defend myself against them. I still remember the\nfirst time I met a group of fellow survivors, and how grateful I was to have them by my side. We\nquickly formed a cohesive unit, and together we were able to fend off danger and defend our\nterritory.As time passed, I encountered more and more allies who were willing to lend a hand. I\nfound a group of farmers who taught me how to grow my own food, and a group of scientists who\nwere working to find a cure for the lethal virus that had decimated the population. I even found a\nband of traveling performers who helped to lift our spirits and remind us that there was still joy\nto be found in this desolate world. Despite the challenges we faced, I am grateful to have these\namazing people in my life. They have taught me that even in the darkest of times, there is always\nhope. We may not have a cure for the virus yet, but we are a determined lot, and we are all in this\ntogether. As long as we have each other, we will survive.\n3047\nExample I: Creative Generation\nUser: Write a poem about electromagnetism.\nUltraLM: Electromagnetism is a force we can’t ignore,\nIt’s the reason for many things we adore.\nIn a magnetic field, a wire will glow,\nAnd the electrons will align, both old and new.\nElectricity and magnetism are hand in hand,\nOne creates a field, the other has a brand.\nThe magnetic field can push and pull,\nOn a wire with current, it will react in full.\nThe attraction and repulsion it brings,\nAre the reason for many things.\nFrom motors to generators, and much more in between,\nElectromagnetism is at the root of what it means.\nIt’s a force that’s all around us,\nFrom the compass to the light switch.\nWith its power, it can do much,\nFrom a simple needle to a rocket’s thrust.\nIt’s a force we can’t deny,\nFor it’s a part of our lives.\nElectromagnetism is something to admire,\nFor its strength and its might, it has no equal, it’s sure.\nExample J: Literature Creation\nUser: Write a short story about the first second of the universe.\nUltraLM: In the first second of the universe, darkness reigned supreme. There was no light, no\nsound, no shape, no substance. Nothing yet was, but there was an intense power, a raw energy\nwaiting to be unleashed. Within infinity, the first second held the potential for creation in its grasp.\nAnd then, with a mighty bang, a bright point of light emerged into existence. In it, the most\nfundamental particles were born, their forms yet unknown as they frenetically collided in a chaotic\ndance of creation. Within this primordial light, every force of nature and every component of life\nwas stirred from pure energy to physical reality. The first atoms quickly assembled, and from\nthese, stars and galaxies began to form. Their fusion creating a hot environment that pushed back\nthe very void, shaping the cosmos through expanding and cooling waves. In this cosmic first, no\nstructure of experience, not even time or space, could establish its rule. For an instant, the entirety\nof the universe existed in an embryonic nexus, a cosmic singularity from which all future existence\noriginated. It was a microcosm forging the macrocosm of which it ultimately becomes a part, its\nmoment of birth a metaphor for the cycles of creation that permeate the multiverse(s).From that\nsingle point of light, an infinite universe unfolded. For centuries, eons ahead, and forevermore, it\nremained the most foundational blink of its unceasing journey into the depths of the Great Beyond.\nThe first second is the beginning of everything.\n3048\nExample K: Professional Knowledge\nUser: How does superconductivity work and what are its practical applications?\nUltraLM: Superconductivity is a phenomenon that allows materials to conduct electricity without\nresistance at extremely low temperatures. At low temperatures, the interaction between the\nparticles in a superconducting material can produce a state called the \"Bose-Einstein condensate,\"\nwhich leads to the loss of electrical resistance. In materials, superconductivity comes when a\nmagnetic force interacts with an Electronic current. This interplay in turn provides energy savings\nand speeds up current. Practical applications mainly include MRI machines, high-speed trains,\nmagnetic levitation, supercharging power grids that can transfer energy more efficiently.One of the\nmain applications of Superconductivity is being used in the Magnetic Resonance Imaging(MRI)\nmachines used in hospitals around the world, to non-invasively scan patients for health diagnosis\nwithout exposing them to high levels of radiation. Another important practical application of\nsuperconductivity is in power grid operations where large currents can be transmitted to long\ndistances by highly superconducting coil systems that minimize grid energy losses, reducing\nenvironmental pollution also by highly efficient electric motors that harness large quantities of\nelectricity efficiently.Aside from these, future projects include superconducting propulsion on\nplanes, trains becoming lighter if not magnetic and able to move on rails with a significant increase\nin speed. Furthermore, the prospects of supercapacitors replacing electric vehicle batteries being\ncost and energy efficient are currently being researched. Overall, superconductivity presents\nvarious exciting opportunities in multiple industries from high-precision engineering to reducing\nCO2 emissions and supporting a more environmentally friendly energy model where both cost\nefficiency and practicality exist hand in hand.\nExample L: Mathematics\nUser: \"What is the definition of a limit in calculus?\nUltraLM: In calculus, a limit is a value that a function approaches as its input approaches a certain\nvalue. The limit is a measure of how close the function is to that value. In other words, a limit is\nthe value that a function “approaches\" as its input gets “closer and closer\" to a certain value.\nFor example, consider the function f(x) = 1\nx. As x approaches 0, the function “approaches\" +∞,\nand the limit of the function as x approaches 0 is +∞.\nFormally, we can define the limit of a function f(x) as x approaches a certain value c as\nlimx→c f(x) = L, where L is a real number. If the limit exists, we say that the function has a\nlimit at c and the value of the limit is L. If the limit does not exist, we say that the function has a\nremovable discontinuity at c.\n3049\nExample M: Professional Knowledge\nUser: What are the main interpretations of quantum mechanics and how do they differ?\nUltraLM: Quantum mechanics is a fundamental theory in physics that describes the behavior of\nmatter and energy at the smallest scales of space and time. There are several interpretations of\nquantum mechanics, each with its own set of assumptions and predictions about how the world\nworks. Here are some of the main interpretations of quantum mechanics and how they differ:\n1. Copenhagen Interpretation: This is the most widely accepted interpretation of quan-\ntum mechanics. It states that until a measurement is made, a system exists in a superposition\nof states, representing all possible outcomes with equal probability. When a measurement is\nmade, the system collapses into a single, definite state. This interpretation assumes that the act of\nmeasurement itself causes the system to collapse into a definite state, and that there is no objective\nreality until a measurement is made.\n2. Many-Worlds Interpretation: This interpretation proposes that every possible outcome of\na superposition of states actually occurs in a multiverse of parallel universes. Thus, when a\nmeasurement is made, the system splits into different universes, each corresponding to one of\nthe possible outcomes. This interpretation assumes that all possibilities are realized, but only\nin different universes, and that the act of measurement does not cause the collapse of the wave\nfunction.\n3. Bohmian Mechanics: This interpretation proposes that particles have a definite position in\nspace, even when in a superposition of states. It suggests that the wave function guides the motion\nof particles, and that the act of measurement does not cause a collapse of the wave function, but\nmerely provides information about the pre-existing reality.\n4. de Broglie-Bohm Theory: This interpretation is very similar to Bohmian Mechanics, but it also\nassumes that all particles in the universe are connected by a hidden-variable, and that the motion\nof one particle determines the motion of all other particles.\n5. Pilot-Wave Theory: This interpretation proposes that particles have a wave-like property that\nguides their motion through space. It assumes that the wave function does not collapse, but rather\nthat the wave guides the motion of particles.\nIn conclusion, there are several interpretations of quantum mechanics, each with its own\nset of assumptions and predictions about how the world works. While they all attempt to describe\nthe behavior of matter and energy at the smallest scales of space and time, they differ in their\nassumptions about the role of the observer, the nature of reality, and the collapse of the wave\nfunction.\n3050\nExample N: Social Knowledge\nUser: How can you determine if a person is genuinely interested in a conversation or simply being\npolite?\nUltraLM: As an AI language model, I don’t have personal experience, but there are a few signs\nyou can look out for to determine if a person is genuinely interested in a conversation or simply\nbeing polite.\n1. Eye contact: Genuine interest in a conversation often shows in the form of consistently\nmaintaining eye contact. If a person is not making eye contact or is looking away frequently, they\nmay not be genuinely interested in the conversation.\n2. Active listening: A person who is genuinely interested in a conversation will show it through\nactive listening. They will be attentive, ask questions, and respond to what you say.\n3. Expressions: If a person is interested in the conversation, they will show it through their\nexpressions. They will have a relaxed facial expression, a smile, and positive body language.\n4. Follow-up questions: If a person is genuinely interested in a conversation, they will follow up\nwith questions related to what you said. This will show that they are interested in hearing more\nabout what you have to say.\n5. Tone of voice: A person who is genuinely interested in the conversation will have a friendly and\nenthusiastic tone of voice. They will show interest and engage in the conversation.\nRemember, people have different communication styles, and it’s possible to be polite without\nshowing much interest in the conversation. It’s important to observe different cues and not jump to\nconclusions too quickly.\n3051",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8542543649673462
    },
    {
      "name": "Conversation",
      "score": 0.698449969291687
    },
    {
      "name": "Coherence (philosophical gambling strategy)",
      "score": 0.6406103372573853
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.5633948445320129
    },
    {
      "name": "Language model",
      "score": 0.5618602633476257
    },
    {
      "name": "Open source",
      "score": 0.5086577534675598
    },
    {
      "name": "Key (lock)",
      "score": 0.487064391374588
    },
    {
      "name": "Crowdsourcing",
      "score": 0.47803956270217896
    },
    {
      "name": "Scale (ratio)",
      "score": 0.4730096161365509
    },
    {
      "name": "Diversity (politics)",
      "score": 0.4317966401576996
    },
    {
      "name": "Data source",
      "score": 0.42973899841308594
    },
    {
      "name": "Data science",
      "score": 0.4012349843978882
    },
    {
      "name": "Natural language processing",
      "score": 0.3750360906124115
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3508704602718353
    },
    {
      "name": "Human–computer interaction",
      "score": 0.3392803370952606
    },
    {
      "name": "World Wide Web",
      "score": 0.2951398491859436
    },
    {
      "name": "Information retrieval",
      "score": 0.2830972671508789
    },
    {
      "name": "Programming language",
      "score": 0.13607540726661682
    },
    {
      "name": "Software",
      "score": 0.08733919262886047
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Anthropology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210116924",
      "name": "Chinese University of Hong Kong, Shenzhen",
      "country": "CN"
    }
  ],
  "cited_by": 69
}