{
  "title": "Residual Learning of Neural Text Generation with n-gram Language Model",
  "url": "https://openalex.org/W4385573321",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2123632656",
      "name": "Huayang Li",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2141500565",
      "name": "Deng Cai",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2008268640",
      "name": "Jin Xu",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2119173261",
      "name": "Taro WATANABE",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W4281652360",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2985986882",
    "https://openalex.org/W2097927681",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W2792376130",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W3034640977",
    "https://openalex.org/W3007759824",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W4284670538",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4226069413",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W3100439847",
    "https://openalex.org/W4297788867",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2951672049",
    "https://openalex.org/W2567070169",
    "https://openalex.org/W2531908596",
    "https://openalex.org/W4226099034",
    "https://openalex.org/W2795429538",
    "https://openalex.org/W1966812932",
    "https://openalex.org/W2962974924",
    "https://openalex.org/W2564089970",
    "https://openalex.org/W3197138502",
    "https://openalex.org/W2134800885",
    "https://openalex.org/W3164673114",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2097333193",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W3176948526",
    "https://openalex.org/W2141440284",
    "https://openalex.org/W2968297680",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W3035359363",
    "https://openalex.org/W2397106859",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W3169496116"
  ],
  "abstract": "N-gram language models (LM) has been largely superseded by neural LMs as the latter exhibits better performance. However, we find that n-gram models can achieve satisfactory performance on a large proportion of testing cases, indicating they have already captured abundant knowledge of the language with relatively low computational cost. With this observation, we propose to learn a neural LM that fits the residual between an n-gram LM and the real-data distribution. The combination of n-gram LMs and neural LMs not only allows the neural part to focus on deeper understanding of the language, but also provides a flexible way to customize a LM by switching the underlying n-gram model without changing the neural model. Experimental results on three typical language tasks (i.e., language modeling, machine translation, and summarization) demonstrate that our approach attains additional performance gains over popular standalone neural models consistently. We also show that our approach allows for effective domain adaptation by simply switching to a domain-specific n-gram model, without any extra training.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 1523–1533\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nN-gram Is Back: Residual Learning of Neural Text Generation\nwith n-gram Language Model\nHuayang Li♣ Deng Cai\n♡\nJin Xu♢ Taro Watanabe♣\n♣Nara Institute of Science and Technology ♡The Chinese University of Hong Kong\n♢Institute for Interdisciplinary Information Sciences, Tsinghua University\n{li.huayang.lh6, taro}@is.naist.jp thisisjcykcd@gmail.com\nxujin21@mails.tsinghua.edu.cn\nAbstract\nN-gram language models (LM) have been\nlargely superseded by neural LMs as the latter\nexhibits better performance. However, we find\nthat n-gram models can achieve satisfactory\nperformance on a large proportion of testing\ncases, indicating they have already captured\nabundant knowledge of the language with rel-\natively low computational cost. With this ob-\nservation, we propose to learn a neural LM that\nfits the residual between an n-gram LM and\nthe real-data distribution. The combination of\nn-gram and neural LMs not only allows the neu-\nral part to focus on the deeper understanding of\nlanguage but also provides a flexible way to cus-\ntomize an LM by switching the underlying n-\ngram model without changing the neural model.\nExperimental results on three typical language\ntasks (i.e., language modeling, machine trans-\nlation, and summarization) demonstrate that\nour approach attains additional performance\ngains over popular standalone neural models\nconsistently. We also show that our approach al-\nlows for effective domain adaptation by simply\nswitching to a domain-specific n-gram model,\nwithout any extra training. Our code is released\nat https://github.com/ghrua/NgramRes.\n1 Introduction\nN-gram language model (LM) was widely adopted\nin a broad range of natural language processing\n(NLP) applications, such as input method (Chen\net al., 2019), statistical machine translation (Brown\net al., 1990), and audio speech recognition (Bahl\net al., 1983). However, with the development of\ndeep learning, neural LMs have gradually taken the\nplace of n-gram LMs and became the new standard\nin recent literature (Merity et al., 2017; Vaswani\net al., 2017; Radford et al., 2019). One critical\nreason is the superior performance of neural LMs,\ne.g., the GPT-2 model (Radford et al., 2019) can\ngenerate text near the human level, outperforming\nn-gram LMs by large margins.\nPPL\n1\n10\n100\nIndex of Bins1 2 3 4 5\n5-gramGPT-2\n5.8322426094060547.2420739963172117.796423416745203.440992166668775.022784173262\n7.1623364652352926.067370259516544.851822577250452.8395811179091428.45097511973\nFigure 1: Sentence-level perplexity (PPL) of5-gram LM\nand GPT-2 LM on the validation dataset of wikitext-103.\nWe sort sentences in the validation dataset according to\ntheir 5-gram PPL scores, and collect them into 5 bins\nwith an equal number of sentences. The reported PPL\nscore of each bin is the average over the sentences in it,\nand the y-axis uses a logarithmic scale. Details of the\ndataset and LMs are shown in section 5.1.\nDespite that neural LMs have surpassed n-gram\nmodels at the macro level, we find that n-gram\nLMs are still attractive: they are able to achieve\nsatisfactory performance on a large proportion of\ntesting cases at a much lower cost than neural LMs.\nAs observed in Figure 1, our preliminary experi-\nments show that the performance of 5-gram LM\nis close to the GPT-2 model trained from scratch\non 3 out of 5 bins ( 1, 2, and 5). Moreover, the\nperformance of 5-gram on the first bin is slightly\nbetter than GPT-2 . Because training a neural LM\nis much more expensive, spending effort on learn-\ning the knowledge that can be cheaply captured by\nn-gram seems a waste.\nInspired by the above observation, we propose to\nlearn a neural LM that focuses on the information\ngap that has not been captured by ann-gram model:\nF:= G−Q , where Gand Qare the real-data dis-\ntribution and the n-gram prediction distribution\nrespectively, which is in a similar spirit to resid-\nual learning (He et al., 2016). More concretely,\nwe combine the logits (the unnormalized probabil-\n1523\nity scores before softmax layer) of a neural model\nand those derived from ann-gram model. The joint\nneuro-symbolic system at least brings two appeal-\ning characteristics. First, since the neural model\nstands on the shoulders of the shallow n-gram LM,\nit can concentrate on deeper understanding. Sec-\nond, the underlying n-gram LM can be purpose-\nfully switched without changing the neural model,\nwhich offers great flexibility in tackling scenarios\nsuch as domain adaptation. That is, we can adapt\nthe model to a specific domain by changing the\nunderlying n-gram LM in a plug-and-play man-\nner, without changing any parameters of the neural\nmodel.\nWe conduct extensive experiments to evaluate\nthe proposed approach. Experiments on the stan-\ndard benchmarks of three typical language tasks,\nincluding language modeling, machine translation,\nand summarization, show that our approach can\nimprove the performance of recent state-of-the-\nart neural models consistently and considerably.\nFor example, our approach outperforms popular\nbaseline models by at least 0.7 PPL scores on the\nwikitext-103 dataset for language modeling, 0.65\nBLEU scores on average on IWSLT datasets for\nmachine translation, and 0.36 ROUGE-L scores\non the CNN/DailyMail dataset for summarization.\nMoreover, on the language modeling task, when\nswitching the underlyingn-gram LM to a particular\ndomain-specific one (e.g., IT, Koran, Law, Medi-\ncal, and Subtitles) in a plug-and-play manner, our\nmodel can reduce the PPL by 5.4 points on aver-\nage without any domain-specific training of the\nneural part. Remarkably, the performance of our\napproach is even close to fine-tuning the whole\nmodel on domain-specific corpora.\nOur contributions are three-fold:\n• We propose a residual learning approach for\ntwo heterogeneous structures, i.e.,n-gram and\nneural LMs, which forces the neural LM to\napproximate the information gap that has not\nbeen captured by n-gram LM.\n• Our approach is able to improve the perfor-\nmance of recent state-of-the-art neural mod-\nels consistently and considerably on language\nmodeling, machine translation, and summa-\nrization.\n• Experiments on domain adaptation demon-\nstrate that our approach can effectively and\ncheaply adapt the model to a specific domain\nby changing the used n-gram LM in a plug-\nand-play manner, without changing any pa-\nrameters of the neural model.\n2 Related Work\nLanguage Model The n-gram language model\n(LM) has been widely used in lots of applications\nof natural language processing (NLP) since a long\ntime ago (Jurafsky, 2000). The emergence of ad-\nvanced smoothing technologies makes the n-gram\nmodel able to provide a better estimation of hu-\nman languages (Kneser and Ney, 1995; Chen and\nGoodman, 1996; Heafield et al., 2013). In statis-\ntical machine translation (Brown et al., 1990) and\nautomatic speech recognition (Bahl et al., 1983),\nthe decoder-side n-gram model is critical to esti-\nmate the quality of generated candidates. In recent\nliterature on input methods, the n-gram LM is still\nthe most popular choice for providing word sug-\ngestions (Huang et al., 2015; Chen et al., 2019),\nbecause of its low cost and low latency.\nHowever, with the development of deep neural\nnetworks, the macro-level performance of neural\nLM has surpassed that of n-gram LM by a large\nmargin. Comparing with the n-gram LM, one\nbig advantage of the neural LM basing on recur-\nrent neural network (Hochreiter and Schmidhuber,\n1997; Chung et al., 2014) and attention neural net-\nwork (Vaswani et al., 2017; Radford et al., 2019) is\ntheir ability to modeling long-distance dependen-\ncies (Grave et al., 2017). The success of neural\nLM can also be observed in the big improvement\nachieved in lots of downstream tasks, e.g., text gen-\neration (Holtzman et al., 2020; Welleck et al., 2020;\nSu et al., 2022; Xu et al., 2022; Li et al., 2022; Cai\net al., 2022), machine translation (Bahdanau et al.,\n2015; Luong and Manning, 2015; Vaswani et al.,\n2017; Cai et al., 2021) and summarization (Li et al.,\n2017; See et al., 2017; Bi et al., 2020).\nAlthough neural LM has outperformed n-gram\nLM at the macro level, we find thatn-gram LM can\nachieve satisfactory performance on a large portion\nof testing cases. Since the training cost of neural\nLM is much more expensive and the model capacity\nis fixed, we hypothesize that it is not necessary to\ntrain the neural LM to learn the knowledge that can\nbe captured by n-gram LM at a much lower cost.\nTherefore, we propose a residual learning method\nto let the neural LM learn the gap of knowledge\nthat has not been captured by n-gram LM.\n1524\nResidual Learning Residual learning is a useful\ntechnique for lots of neural networks in computer\nvision (CV) and natural language processing (NLP).\nHe et al. (2016) propose deep residual learning to\nalleviate the training difficulties of deep models,\nwhich has been the backbone of lots of tasks in\nCV . In NLP, Wang and Tian (2016) and Prakash\net al. (2016) use the residual learning technique to\ntrain deep recurrent neural networks for text gener-\nation. Different from previous works that conduct\nresidual learning over different layers, Werlen et al.\n(2018) propose to aggregate the information of his-\ntorical predictions using residual learning. In He\net al. (2021), they use the residual learning to prop-\nagate attention scores across different layers of the\nTransformer-based model.\nMost of these works conduct residual learning\nover homogeneous model structures, e.g., stacked\nidentical layers of the same model. In our work,\nwe use residual learning to combine the neural and\nsymbolic models, i.e., learn a neural LM that ap-\nproximates the information that has not been cap-\ntured by the n-gram model.\n3 Background\nModels that estimate the probabilities of sequences\nof words are called language models (LM) (Juraf-\nsky, 2000). Let x = {x1,x2,...,x L}be a sequence\nof words with length L. The probability of P(x)\ncan be formalized according to the chain rule of\nprobability:\nP(x) =P(x1)P(x2|x1) ...P (xL|xL−1\n1 )\n=\nL∏\nk=1\nP(xk|xk−1\n1 ), (1)\nwhere xk−1\n1 is called the prefix or context of xk. In\nthis section we will briefly introduce two kinds of\nlanguage models, the n-gram and neural language\nmodels, to compute the probability in Eq. (1).\n3.1 N-gram Language Model\nAmong lots of variants ofn-gram LMs, the n-gram\nLM with modified Kneser-Ney smoothing is widely\nadopted in lots of related tasks, because of its low\nperplexity and efficiency (Kneser and Ney, 1995;\nChen and Goodman, 1996; Heafield et al., 2013).\nLike most n-gram LMs, the Kneser-Ney approxi-\nmates the entire context xk−1\n1 in Eq. (1) by the last\nn−1 words in the context:\nP(xk|xk−1\n1 ) ≈PNG(xk|xk−1\nk−n+1). (2)\nIn Kneser-Ney algorithm, the estimation of\nPNG(xk|xk−1\nk−n+1) is defined according to a recur-\nsive equation:\nPNG(xk|xk−1\nk−n+1) =U(xk|xk−1\nk−n+1)+\nb(xk−1\nk−n+1)PNG(xk|xk−1\nk−n+2),\n(3)\nU(xk|xk−1\nk−n+1) = c(xk\nk−n+1) −d\n∑\nwc(xk−1\nk−n+1w)\n,\nwhere w indicates a word appears after xk−1\nk−n+1,\nb(·) is the backoff value for lower-order estimation,\nc(·) is the adjusted counts, dis the discounts for\nsmoothing (Jurafsky, 2000; Heafield et al., 2013)1.\nAccording to Eq. (3), Kneser-Ney allows us to\nassign probabilities for unseen n-grams (e.g., 5-\ngrams), using the lower-order information (e.g., 4-,\n3-, or even uni-grams).\n3.2 Neural Language Model\nAn neural LM typically estimates the probability\nof xk based on the whole context xk−1\n1 . The pa-\nrameter θof a neural LM is optimized through the\nfollowing MLE loss:\nLNU =\n∑\nx∈D\nL∑\nk=1\nlog PNU(xk|xk−1\n1 ; θ) (4)\nwhere Dis the training dataset. The probability of\nPNU(xk|·) is computed by:\nPNU(xk|xk−1\n1 ; θ) = softmax(ϕ(hk))[xk], (5)\nwhere hk is the hidden vector output by the last\nlayer of an neural LM, e.g., the GPT-2 model (Rad-\nford et al., 2019) or LSTM model (Grave et al.,\n2017). The [xk] is defined as taking the component\nregarding to xk in a vector, i.e., the probabilistic\ndistribution got from softmax in this equation. The\nϕ(·) is a linear layer that transforms the hidden vec-\ntor hk to a vector in the vocabulary space, which is\nalso called the logits.\n1More details about adjusting counts and computing the\nbackoff values and discounts are shown in Jurafsky (2000)\nand Heafield et al. (2013).\n1525\n4 Methodology\n4.1 Motivation\nThe main idea of our work is to use the neural\nLM to approximate a residual function. Given the\ncontext xk−1\n1 in the language modeling task, let us\nconsider G(xk−1\n1 ) as the golden-truth distribution\nof the next word, and\nQ(xk−1\n1 ) =PNG(X|xk−1\nk−n+1) (6)\nas the prediction distribution of the n-gram LM,\nwhere X is the random variable and the proba-\nbility PNG(X = xk|xk−1\nk−n+1) is calculated ac-\ncording to Eq. (3). Since the n-gram distribu-\ntion Q(xk−1\n1 ) has captured abundant information\nof the language as we discussed in the introduc-\ntion, one interesting question is: can we use a\nneural LM to approximate the residual function\nF(xk−1\n1 ) :=G(xk−1\n1 ) −Q(xk−1\n1 )? This is similar\nto the residual learning in He et al. (2016). If it is\npossible, we can release the burden of neural LMs\non learning the information that has been captured\nby n-gram LMs, e.g., short-distance dependencies,\nand provide a flexible way to customize an LM by\nswitching the underlying n-gram model without\nchanging the neural model.\n4.2 Learning Objective\nIdeally, to train a neural LM that approximates\nthe residual function, one way is to re-define the\nPNU(xk|·) in Eq. (5) as follows:\nPNU(xk|xk−1\n1 ; θ) =F(xk−1\n1 )[xk]+\nPNG(xk|xk−1\nk−n+1),\nwhere F(·) is parameterized by the neural model\nθ, and PNG(xk|·) is defined in Eq. (3). Then we\ncan optimize the MLE loss in Eq. (4) based on\nthe new PNU(xk|·), which is equivalent to approx-\nimate real-data distribution Gby F+ Q. How-\never, directly optimizing this objective may have\nsome problems. If F(·) is unbounded, PNU de-\nfined in this equation may not be guaranteed as a\nvalid probabilistic distribution. In contrast, if F(·)\nis bounded as a valid distribution, this objective\nwould become the ensemble of a neural LM and\nn-gram LM. Since n-gram is a weaker model, the\nensemble of them is more likely to achieve worse\nperformance than the vanilla neural LM, as shown\nin the experimental results of section 5.1.\nTo address these issues, we propose to define\nresidual approximation at the logits level. In the\nlanguage modeling task, we can map the proba-\nbilistic distribution back to its logits and conduct\nresidual learning as follows:\nF′(xk−1\n1 ) : = softmax−1(\nG(xk−1\n1 )\n)\n−\nsoftmax−1(\nQ(xk−1\n1 )\n)\n(7)\nsoftmax−1(p) = logp + C, (8)\nwhere F′(·) is the residual function at the log-\nits level, softmax−1(p) is the reverse function of\nsoftmax that maps the probabilistic distribution p\nto its logits, andCis a constant. One reason that we\nconduct residual learning at the logits level is that\nlogits are highly correlated to the final distribution.\nMoreover, since the value of logits is in the real\nnumber space, training the neural LM becomes\nmore tractable by making sure that its logits are\nclose to F′(xk−1\n1 ). Therefore, the final PNU(xk|·)\ndefined in our work is:\nPNU(xk|xk−1\n1 ; θ) = softmax\n(\nF′(xk−1\n1 ) +α×\nsoftmax−1(\nQ(xk−1\n1 )\n))\n[xk]\n(9)\nwhere αis a hyper-parameter to control the smooth-\nness of the logits of the n-gram distribution\nQ(xk−1\n1 ), and F′(·) is approximated by the log-\nits ϕ(hk) of a neural LM. We can use the definition\nin Eq. (9) to optimize the MLE loss in Eq. (4).\n4.3 Relation to Re-weighting\nTo better understand our approach, we can dive into\nthe details of Eq. (9). For simplicity, let us omit\nthe condition xk−1\n1 in this section:\nPNU(xk|·) = softmax\n(\nϕ(hk) +α×\n(\nlog PNG(X|·) +C\n))\n[xk]\n(10)\n=\n(\neC)α(\nelog PNG(xk|·))αeϕ(hk)[xk]\nZ ,\n(11)\nWe apply the Eq. (6) and (8) to get the explicit form\nof logits of the n-gram LM in Eq.(10), and the def-\ninition of ϕ(hk) is the same as that in Eq. (5). In\nEq. (11), we expand the softmax function, where\nZis the normalization term. The numerator of Eq.\n(11) has three terms. The first term (eC)α is a con-\nstant for all the logit values, which does not affect\nthe distribution. The middle term (elog PNG(xk|·))α\n1526\nactually equals to PNG(xk|·)α, which makes it be\nlike the weight of the the logits of neural LM, i.e.,\nthe last term eϕ(hk)[xk] in Eq. (11). When compar-\ning with the vanilla neural LM, the golden-truth\nwords are not equally important in the learning pro-\ncess of our approach. For golden-truth words that\nare well estimated by then-gram LM, our approach\nwould get high probabilities after softmax, leading\nto a small loss value for the neural module. As\na result, the neural model can spend more effort\non difficult cases, such as predictions relying on\nlong-distance dependencies, which are hard to be\nestimated by the n-gram LM.\n4.4 Discussion\nIn this section, we propose a method to conduct\nresidual learning between the neural and symbolic\nmodels, i.e., neural LM and n-gram LM. One of\nour expectations about the joint neuro-symbolic\nsystem is its better understanding of language. To\nevaluate this hypothesis, we can test our approach\non standard language tasks, such as language mod-\neling, machine translation, and summarization. The\nother expectation is the plug-and-lay property of\nour approach. For instance, if the testing data come\nfrom different domains, we can change theQin Eq.\n(9) by simply switching the used n-gram model.\n5 Experiments\nIn our work, we consider three kinds of natural\nlanguage generation tasks: language modeling, ma-\nchine translation, and summarization. For the lan-\nguage modeling task, we first evaluate the perfor-\nmance of our approach on the standard setting of\nthe language modeling task. Then we turn to a\ndomain adaptation setting.\n5.1 Language Modeling\nSetup We use the wikitext-103 benchmark 2 to\nevaluate the performance of our approach in the\nstandard setting. The training set contains around\n101M tokens. Following Merity et al. (2017), to-\nkens with a frequency lower than 3 have been\nreplaced by the special token <unk> in the train-\ning datasets, and the number of remaining unique\nwords is around 260k. For wikitext-103, we\nwill train models at both word and subword lev-\nels. The subword-level data is preprocessed using\n2Dataset provided by fairseq: https://s3.\namazonaws.com/research.metamind.io/wikitext/\nwikitext-103-v1.zip\nsubword-nmt3 (Sennrich et al., 2016), where the\nnumber of merge operation is set to 32k.\nWe use fairseq4 (Ott et al., 2019) as the code\nbase of our neural modules. We implement our\napproach on two popular neural language mod-\nels, GPT-2 base (Radford et al., 2019) and Adap-\ntive Input (ADP) (Baevski and Auli, 2019). For\nthe ADP model, we follow the original hyper-\nparameters and use the code released by Baevski\nand Auli (2019) in fairseq5 to train the model on\nword-level data. Since the vocabulary size of the\nword-level data is too large, we train the GPT-2\nbase model on the subword-level data. For those\nneural models, we mostly use their default hyper-\nparameters reported in their paper (Baevski and\nAuli, 2019; Radford et al., 2019) and train those\nmodels from random initialization. Regarding to\nthe n-gram model, we use the KenLM6 (Heafield,\n2011) to train n-gram models on both the word-\nlevel and subword-level data of wikitext-103. The\nnis set to 5 in our work. To make the perplexities\nof different models comparable, we report all the\nperplexity scores at the word level. For subword-\nlevel data, the word-level probability is the product\nof its subword tokens, following Baevski and Auli\n(2019).\nWhen training our approach NGRAM RES, we\nwill hybrid the KENLM-5 GRAM model and the\nneural model, i.e., GPT-2 and ADP , using the\nresidual learning method discussed in section 4.\nThe hyper-parameter αin Eq. (9) is tuned accord-\ning to the performance on the validation dataset.\nResults As shown in Table 2, we evaluate our ap-\nproach on the wikitext-103 benchmark. Although\nthe macro performance of KENLM-5 GRAM (Line\n6) on the test set is poor, it is still able to promote\nthe performance of our approach. When comparing\nour approach (Line 8 and 11) with the vanilla neural\nmodels (Line 7 and 9), our approach steadily out-\nperforms ADP-F AIRSEQ 7 and GPT-2 by 0.7 and\n0.9 PPL scores, respectively. According to these\nresults, NGRAM RES is able to improve the model\nperformance without changing the architecture and\nthe number of parameters.\n3https://github.com/rsennrich/subword-nmt\n4https://github.com/facebookresearch/fairseq\n5https://github.com/facebookresearch/fairseq/\nblob/main/examples/language_model/README.\nadaptive_inputs.md\n6https://github.com/kpu/kenlm\n7This is the result by running the officially released code\nof ADP\n1527\n# IT Koran Law Medical Subtitles A VG.\n1 #SENT 222,927 17,982 467,309 248,099 500,000 –\n2 #WORD 2,585,965 4,512,266 15,348,052 4,512,266 5,125,239 –\n3 KENLM-5GRAM 95.89 35.51 15.74 24.00 101.99 54.63\n4 GPT-2 66.49 35.34 9.93 15.18 77.34 40.86\n5 + FINETUNE 53.69 26.77 9.43 12.96 69.33 34.44\n6 + NGRAMRES 54.29 28.08 8.93 13.29 71.80 35.28\nTable 1: Test perplexity of five domains. Results in lines 1-2 are the statistical information of each domain. Results\nin lines 3-6 are the perplexity scores of different approaches when testing on the five domains. The GPT-2 and\nNGRAM RES (Line 4 and 6) approaches only train unified models for five domains, while the FINETUNE method\n(Line 5) trains a domain-specific model for each domain.\n# Model #Param PPL\n1 (Grave et al., 2017) - LSTM – 40.8\n2 (Dauphin et al., 2017) - GCNN-8229M 37.2\n3 (Merity et al., 2018) - QRNN 151M 33.0\n4 (Rae et al., 2018) - HEBBIAN+ CACHE – 29.2\n5 (Baevski and Auli, 2019) - ADP247M 18.7\n6 KENLM-5GRAM – 116.4\n7 ADP-FAIRSEQ 247M 18.9\n8 + NGRAMRES 247M 18.2\n9 GPT-2 (BPE) 185M 22.2\n10 + PROB-INTER 185M 60.2\n11 + NGRAMRES 185M 21.3\nTable 2: Test perplexity on wikitext-103. Results in\nlines 1-5 are reported in previous works, and results\nin lines 6-11 are run by us. The NGRAM RES is our\napproach discussed in section 4.\nMoreover, we also compare our method with\na straightforward baseline PROB -INTER , as dis-\ncussed in section 4. The PROB -INTER baseline\ndirectly interpolates the probabilistic distribution\nof KENLM-5 GRAM and GPT-2 . The performance\nof PROB -INTER is better than theKENLM-5 GRAM\nbut worse than the vanilla GPT-2 , making it like\nthe ensemble of the two models, as we discussed\nin the section 4.\n5.2 Language Modeling: Multi-Domain\nIn this setting, we will evaluate the performance\nof adapting our approach to a specific domain by\nchanging the used n-gram model.\nSetup In the multi-domain setting, we use the\nEnglish side of a bilingual dataset with 5 domains\n(Aharoni and Goldberg, 2020), i.e., IT, Koran, Law,\nMedical, and Subtitles. The statistical informa-\ntion of this dataset is shown in Table 1. we apply\nsubword-nmt on the joint training data of five do-\nmains, and the number of the merge operation is\nalso 32k.\nFollowing the standard setting of the language\nmodeling task, we use GPT-2 base (Radford et al.,\n2019) as the neural model. We train and select\nGPT-2 model on the mixed data from five domains,\nand report the word-level perplexity on the test\ndata of each domain independently. The GPT-2\n+ FINETUNE method will adapt the parameters of\nGPT-2 model on the corresponding domain before\ntesting. For our approach NGRAM RES, we train a\n5-gram LM for each specific domain and switch the\nused 5-gram model to the corresponding domain\nduring training and testing. It is worth noting that\nthe neural parameters of NGRAM RES are fixed\nwhen testing.\nResults The experimental results are shown in\nTable 1. For GPT-2 and NGRAM RES (Line 4 and\n6), we train unified neural models on mixed data\nof five domains and evaluate their performances on\nthe test data of five domains one by one. Results\nshow that our approach can outperform the vanilla\nneural model GPT-2 by a large margin. Since\nthe NGRAM RES approach stores a lot of domain-\nspecific information in the 5-gram LM, we hypoth-\nesize that the neural module is able to learn use-\nful and complementary knowledge during training,\nleading to the performance gain.\nIn the line of + F INETUNE , we also report the\nresults of fine-tuning theGPT-2 model on each test-\ning domain. It surprised us that the performances\nof our approach are very close to those of theFINE -\nTUNE method. The NGRAM RES even outperforms\nFINETUNE slightly on the Law domain. Moreover,\ncompared with the FINETUNE , one advantage of\nour approach is its low cost of adapting our model\nto the testing domain, since we only need to replace\nthe used 5-gram model in a plug-and-play manner.\n1528\nModel En ⇒Fr En ⇒Es En ⇒Vi En ⇒De A VG.\nTRANSFORMER 39.96 36.99 28.55 27.79 33.32\n+ NGRAM RES 40.27 37.27 29.60 28.05 33.79\n+ NGRAM RES -A NNEAL 40.49 37.07 29.92 28.41 33.97\nTable 3: BLEU scores on IWSLT. The TRANSFORMER model is the baseline, and NGRAM RES and NGRAM RES-\nANNEAL are two variants of our approach. Comparing with NGRAM RES, the NGRAM RES-ANNEAL decreases the\nvalue of αin Eq. (9) linearly in the first 10k steps of model training.\nModel ROUGE-1 ROUGE-2 ROUGE-L\nPointer-generator + Coverage (See et al., 2017) 39.53 17.28 36.38\nMask Attention Network (Fan et al., 2021) 40.98 18.29 37.88\nBertSum (Liu and Lapata, 2019) 42.13 19.60 39.18\nUniLM (Dong et al., 2019) 43.08 20.43 40.34\nUniLM V2 (Bao et al., 2020) 43.16 20.42 40.14\nERNIE-GEN-large (Xiao et al., 2021) 44.02 21.17 41.26\nPEGASUS (Zhang et al., 2020) 44.17 21.47 41.11\nProphetNet (Qi et al., 2020) 44.20 21.17 41.30\nPALM (Bi et al., 2020) 44.30 21.12 41.14\nBART-LARGE (Lewis et al., 2020) 44.11 21.21 40.83\n+ NGRAM RES 44.41 21.36 41.19\nTable 4: ROUGE scores on the test set of CNN/DailyMail dataset.\n5.3 Machine Translation\nNext, we evaluate our approach on a popular\nsequence-to-sequence task, namely, machine trans-\nlation. Note that we only integrate our approach\ninto the decoder side of the encoder-decoder model.\nSetup We conduct the experiments of machine\ntranslation on IWSLT14 (En ⇒Fr, Es, De) and\nIWSLT15 (En ⇒Vi). The IWSLT14 datasets8 of\nthree language pairs are preprocessed following the\nscript provided by fairseq9, where the evaluation\ndata is sampled from the whole dataset and the\ntest data is the concatenation of dev2011, tst2012,\ntst2012. There is no overlap between train, valida-\ntion, and test sets. For IWSLT15, we use the train,\nevaluation, and test data preprocessed and released\nby Stanford10 (Luong and Manning, 2015). The\nresults are reported using tokenized SacreBLEU11\n(Post, 2018).\nWe use fairseq as our code base. We use the\n8https://wit3.fbk.eu/2014-01\n9https://github.com/facebookresearch/fairseq/\nblob/main/examples/translation/prepare-iwslt14.\nsh\n10https://nlp.stanford.edu/projects/nmt/\n11https://github.com/mjpost/sacrebleu\nTransformer model as our architecture12 for all the\ntranslation models. The Transformer model has\n6 encoder layers and 6 decoder layers. Since the\nIWSLT datasets are small, the hidden size of FFN\nsublayers is set to 1024, the number of attention\nheads is set to 4, the dropout rate is set to 0.3, and\nthe weight decay rate is set to 0.001. We set other\nhyper-parameters according to the default setting\nof Vaswani et al. (2017). All the translation models\nare trained for 30 epochs from random initializa-\ntion.\nThe implementation details of the n-gram model\nand our approach are similar to that in the language\nmodeling task. For the translation task, we only\nuse the target data, i.e., the X side of En⇒X data,\nto train the KENLM-5 GRAM LM.\nResults The results of machine translation are\nshown in Table 3. We implement two vari-\nants of our approaches, namely, NGRAM RES and\nNGRAM RES-ANNEAL . The system of NGRAM -\nRES only uses the 5-gram information on the de-\ncoder side, as we discussed in section 4. The dif-\nference between NGRAM RES and NGRAM RES-\n12The used architecture code in fairseq is\ntransformer_iwslt_de_en\n1529\nANNEAL system is that the latter decreases the\nvalue of αlinearly after each update . The alpha\nvalue becomes zero after 10k steps.\nWe find that both the two variants of our ap-\nproaches outperform the TRANSFORMER model.\nThe NGRAM RES-ANNEAL achieves the best re-\nsults on each language pair, which means that the\nn-gram model is more critical for the beginning\nphase and may hurt the translation performance af-\nter that phase. According to V oita et al. (2021), the\ntraining of neural machine translation (NMT) sys-\ntems undergoes three stages: target-side language\nmodeling, learning the word-by-word translation,\nand learning to reorder. Therefore, we hypothesize\nthat the use of the n-gram model in the whole train-\ning procedure may over-emphasize the importance\nof target-side language modeling in NMT, having\na negative impact on the next two stages.\n5.4 Abstractive Summarization\nLastly, we evaluate our approach on another popu-\nlar sequence-to-sequence task, namely, abstractive\nsummarization. Like machine translation, our ap-\nproach is applied to the decoder side of the encoder-\ndecoder model.\nSetup For the abstractive summarization task, we\npreprocess the CNN/DailyMail dataset following\nthe script provided by fairseq13. The evaluation\nmetrics of the summarization task are ROUGE\nscores, i.e., ROUGE-1, ROUGE-2, and ROUGE-L\n(Lin, 2004)14.\nWe follow the setting of previous works and fine-\ntune the pre-trained BART-LARGE model (Lewis\net al., 2020) on the CNN/DailyMail dataset for 20k\nupdates. We train the KENLM-5 GRAM LM on the\njoint data of its source and summarization text.\nResults The summarization task is also a\nsequence-to-sequence task, where the source text\nand summarization are in the same language and\nshare similar semantics. As shown in Table 4, in\nthis task, our approach is still able to improve the\nperformance of the strong baseline model BART-\nLARGE , without any change in the model architec-\nture.\nDifferent from the machine translation task, we\nfind that using a fixed αvalue achieves better per-\nformance than annealing it. The reason may be that\nthe target-side language modeling plays a more\n13https://github.com/facebookresearch/fairseq/\nblob/main/examples/bart/README.summarization.md\n14https://github.com/pltrdy/files2rouge\nimportant role in the summarization task because\nsummarization is more like monolingual text gen-\neration in a constrained context.\n6 Conclusion and Future Work\nThis work aims to learn a neural LM that approx-\nimates the information that has not been captured\nby n-gram LM. To achieve this goal, we propose\na residual learning approach to force the two neu-\nral and symbolic models, i.e., the neural LM and\nn-gram LM, to learn complementary information.\nWe conduct extensive experiments to evaluate the\nperformance of the proposed approach. In our ex-\nperiments, we find that our neuro-symbolic system\ncan not only improve the performance of recent\nstate-of-the-art neural models consistently and con-\nsiderable on three typical language tasks (including\nlanguage modeling, machine translation, and sum-\nmarization) but also exhibits a good plug-and-play\nproperty on the multi-domain language modeling\ntask.\nThe n-gram LM has lots of attractive proper-\nties that we have not explored in this work. First,\nthe n-gram model has good interpretability. The\nbehavior of n-gram LM is easier to understand\nthan the weights of neurons from the perspective\nof humans. In the future, we want to leverage the\nproperty of the n-gram model to better understand\nthe decision-making process of the neural LM. Sec-\nond, controlling the system predictions through the\nn-gram model may have a big potential. As ob-\nserved in our multi-domain experiments, we are\nable to customize an LM by switching the under-\nlying n-gram model without changing the neural\npart. It is also interesting to explore how to control\nthe model output at a fine-grained level using the\nn-gram LM.\nLimitations\nWe believe there are two limitations in our ap-\nproach. First, since the estimation of the prediction\ndistribution of n-gram models relies on CPU, the\nestimation speed by n-gram models may be slow\nwhen using a big batch size (>>8192∗8). Second,\nthe performance gain of our current approach on\nhigh-resource datasets is not big. For instance, we\nalso evaluate the performance of TRANSFORMER\n+ NGRAM RES on WMT14 En-De (Vaswani et al.,\n2017), but the improvement is only 0.15 BLEU\nscore. These limitations urge us to propose more\nefficient and effective approaches in future works.\n1530\nAcknowledgement\nWe are particularly grateful for the help from Xi-\naojiang Liu, because this project would never have\nbeen conceived and completed without his gener-\nous and selfless support. We also want to thank\nthe insightful discussions with Yixuan Su and the\nvaluable comments from our anonymous reviewers,\narea chairs, and senior area chairs.\nReferences\nRoee Aharoni and Yoav Goldberg. 2020. Unsupervised\ndomain clusters in pretrained language models. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 7747–\n7763, Online. Association for Computational Lin-\nguistics.\nAlexei Baevski and Michael Auli. 2019. Adaptive input\nrepresentations for neural language modeling. In 7th\nInternational Conference on Learning Representa-\ntions, ICLR 2019, New Orleans, LA, USA, May 6-9,\n2019. OpenReview.net.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In 3rd International\nConference on Learning Representations, ICLR 2015,\nSan Diego, CA, USA, May 7-9, 2015, Conference\nTrack Proceedings.\nLalit R. Bahl, Frederick Jelinek, and Robert L. Mercer.\n1983. A maximum likelihood approach to continuous\nspeech recognition. IEEE Trans. Pattern Anal. Mach.\nIntell., 5(2):179–190.\nHangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan\nYang, Xiaodong Liu, Yu Wang, Jianfeng Gao, Song-\nhao Piao, Ming Zhou, et al. 2020. Unilmv2: Pseudo-\nmasked language models for unified language model\npre-training. In International Conference on Ma-\nchine Learning, pages 642–652. PMLR.\nBin Bi, Chenliang Li, Chen Wu, Ming Yan, Wei Wang,\nSongfang Huang, Fei Huang, and Luo Si. 2020.\nPalm: Pre-training an autoencoding&autoregressive\nlanguage model for context-conditioned generation.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 8681–8691.\nPeter F. Brown, John Cocke, Stephen Della Pietra, Vin-\ncent J. Della Pietra, Frederick Jelinek, John D. Laf-\nferty, Robert L. Mercer, and Paul S. Roossin. 1990. A\nstatistical approach to machine translation. Comput.\nLinguistics, 16(2):79–85.\nDeng Cai, Yan Wang, Huayang Li, Wai Lam, and\nLemao Liu. 2021. Neural machine translation with\nmonolingual translation memory. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 7307–7318.\nDeng Cai, Yan Wang, Lemao Liu, and Shuming Shi.\n2022. Recent advances in retrieval-augmented text\ngeneration. In Proceedings of the 45th International\nACM SIGIR Conference on Research and Develop-\nment in Information Retrieval, pages 3417–3419.\nMingqing Chen, Ananda Theertha Suresh, Rajiv Math-\news, Adeline Wong, Cyril Allauzen, Françoise Bea-\nufays, and Michael Riley. 2019. Federated learn-\ning of n-gram language models. In Proceedings\nof the 23rd Conference on Computational Natural\nLanguage Learning (CoNLL), pages 121–130, Hong\nKong, China. Association for Computational Linguis-\ntics.\nStanley F. Chen and Joshua Goodman. 1996. An empir-\nical study of smoothing techniques for language mod-\neling. In 34th Annual Meeting of the Association for\nComputational Linguistics, 24-27 June 1996, Univer-\nsity of California, Santa Cruz, California, USA, Pro-\nceedings, pages 310–318. Morgan Kaufmann Pub-\nlishers / ACL.\nJunyoung Chung, Çaglar Gülçehre, KyungHyun Cho,\nand Yoshua Bengio. 2014. Empirical evaluation of\ngated recurrent neural networks on sequence model-\ning. CoRR, abs/1412.3555.\nYann N. Dauphin, Angela Fan, Michael Auli, and David\nGrangier. 2017. Language modeling with gated con-\nvolutional networks. In Proceedings of the 34th In-\nternational Conference on Machine Learning, ICML\n2017, Sydney, NSW, Australia, 6-11 August 2017 ,\nvolume 70 of Proceedings of Machine Learning Re-\nsearch, pages 933–941. PMLR.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. Unified language model\npre-training for natural language understanding and\ngeneration. Advances in Neural Information Process-\ning Systems, 32.\nZhihao Fan, Yeyun Gong, Dayiheng Liu, Zhongyu Wei,\nSiyuan Wang, Jian Jiao, Nan Duan, Ruofei Zhang,\nand Xuan-Jing Huang. 2021. Mask attention net-\nworks: Rethinking and strengthen transformer. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 1692–1701.\nEdouard Grave, Armand Joulin, and Nicolas Usunier.\n2017. Improving neural language models with a\ncontinuous cache. In 5th International Conference\non Learning Representations, ICLR 2017, Toulon,\nFrance, April 24-26, 2017, Conference Track Pro-\nceedings. OpenReview.net.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recogni-\ntion. In 2016 IEEE Conference on Computer Vision\n1531\nand Pattern Recognition, CVPR 2016, Las Vegas,\nNV , USA, June 27-30, 2016, pages 770–778. IEEE\nComputer Society.\nRuining He, Anirudh Ravula, Bhargav Kanagal, and\nJoshua Ainslie. 2021. Realformer: Transformer likes\nresidual attention. In Findings of the Association for\nComputational Linguistics: ACL/IJCNLP 2021, On-\nline Event, August 1-6, 2021, volume ACL/IJCNLP\n2021 of Findings of ACL, pages 929–943. Associa-\ntion for Computational Linguistics.\nKenneth Heafield. 2011. KenLM: Faster and smaller\nlanguage model queries. In Proceedings of the Sixth\nWorkshop on Statistical Machine Translation, pages\n187–197, Edinburgh, Scotland. Association for Com-\nputational Linguistics.\nKenneth Heafield, Ivan Pouzyrevsky, Jonathan H. Clark,\nand Philipp Koehn. 2013. Scalable modified Kneser-\nNey language model estimation. In Proceedings\nof the 51st Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers),\npages 690–696, Sofia, Bulgaria. Association for Com-\nputational Linguistics.\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long\nshort-term memory. Neural Comput., 9(8):1735–\n1780.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text\ndegeneration. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nGuoping Huang, Jiajun Zhang, Yu Zhou, and Chengqing\nZong. 2015. A new input method for human transla-\ntors: Integrating machine translation effectively and\nimperceptibly. In Proceedings of the Twenty-Fourth\nInternational Joint Conference on Artificial Intelli-\ngence, IJCAI 2015, Buenos Aires, Argentina, July\n25-31, 2015, pages 1163–1169. AAAI Press.\nDan Jurafsky. 2000. Speech & language processing .\nPearson Education India.\nReinhard Kneser and Hermann Ney. 1995. Improved\nbacking-off for m-gram language modeling. In 1995\nInternational Conference on Acoustics, Speech, and\nSignal Processing, ICASSP ’95, Detroit, Michigan,\nUSA, May 08-12, 1995, pages 181–184. IEEE Com-\nputer Society.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nHuayang Li, Yixuan Su, Deng Cai, Yan Wang, and\nLemao Liu. 2022. A survey on retrieval-augmented\ntext generation. arXiv preprint arXiv:2202.01110.\nPiji Li, Wai Lam, Lidong Bing, and Zihao Wang. 2017.\nDeep recurrent generative decoder for abstractive text\nsummarization. In Proceedings of the 2017 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 2091–2100, Copenhagen, Denmark.\nAssociation for Computational Linguistics.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nYang Liu and Mirella Lapata. 2019. Text summariza-\ntion with pretrained encoders. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3730–3740.\nMinh-Thang Luong and Christopher D. Manning. 2015.\nStanford neural machine translation systems for spo-\nken language domain. In International Workshop on\nSpoken Language Translation, Da Nang, Vietnam.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018. An analysis of neural language model-\ning at multiple scales. CoRR, abs/1803.08240.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In 5th International Conference on Learning\nRepresentations, ICLR 2017, Toulon, France, April\n24-26, 2017, Conference Track Proceedings. Open-\nReview.net.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,\nSam Gross, Nathan Ng, David Grangier, and Michael\nAuli. 2019. fairseq: A fast, extensible toolkit for\nsequence modeling. In Proceedings of NAACL-HLT\n2019: Demonstrations.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186–\n191, Belgium, Brussels. Association for Computa-\ntional Linguistics.\nAaditya Prakash, Sadid A. Hasan, Kathy Lee, Vivek V .\nDatla, Ashequl Qadir, Joey Liu, and Oladimeji Farri.\n2016. Neural paraphrase generation with stacked\nresidual LSTM networks. In COLING 2016, 26th\nInternational Conference on Computational Linguis-\ntics, Proceedings of the Conference: Technical Pa-\npers, December 11-16, 2016, Osaka, Japan , pages\n2923–2934. ACL.\nWeizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu, Nan\nDuan, Jiusheng Chen, Ruofei Zhang, and Ming Zhou.\n2020. Prophetnet: Predicting future n-gram for\nsequence-to-sequencepre-training. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2020, pages 2401–2410.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\n1532\nJack W. Rae, Chris Dyer, Peter Dayan, and Timothy P.\nLillicrap. 2018. Fast parametric learning with acti-\nvation memorization. In Proceedings of the 35th In-\nternational Conference on Machine Learning, ICML\n2018, Stockholmsmässan, Stockholm, Sweden, July\n10-15, 2018, volume 80 of Proceedings of Machine\nLearning Research, pages 4225–4234. PMLR.\nAbigail See, Peter J Liu, and Christopher D Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1073–\n1083.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1715–1725,\nBerlin, Germany. Association for Computational Lin-\nguistics.\nYixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Ling-\npeng Kong, and Nigel Collier. 2022. A contrastive\nframework for neural text generation. arXiv preprint\narXiv:2202.06417.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998–6008.\nElena V oita, Rico Sennrich, and Ivan Titov. 2021. Lan-\nguage modeling, lexical translation, reordering: The\ntraining process of NMT through the lens of classi-\ncal SMT. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 8478–8491, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nYiren Wang and Fei Tian. 2016. Recurrent residual\nlearning for sequence classification. In Proceedings\nof the 2016 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2016, Austin,\nTexas, USA, November 1-4, 2016 , pages 938–943.\nThe Association for Computational Linguistics.\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily Di-\nnan, Kyunghyun Cho, and Jason Weston. 2020. Neu-\nral text generation with unlikelihood training. In\n8th International Conference on Learning Represen-\ntations, ICLR 2020, Addis Ababa, Ethiopia, April\n26-30, 2020. OpenReview.net.\nLesly Miculicich Werlen, Nikolaos Pappas, Dhananjay\nRam, and Andrei Popescu-Belis. 2018. Self-attentive\nresidual decoder for neural machine translation. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2018, New Orleans, Louisiana, USA,\nJune 1-6, 2018, Volume 1 (Long Papers), pages 1366–\n1379. Association for Computational Linguistics.\nDongling Xiao, Han Zhang, Yukun Li, Yu Sun, Hao\nTian, Hua Wu, and Haifeng Wang. 2021. Ernie-gen:\nan enhanced multi-flow pre-training and fine-tuning\nframework for natural language generation. In Pro-\nceedings of the Twenty-Ninth International Confer-\nence on International Joint Conferences on Artificial\nIntelligence, pages 3997–4003.\nJin Xu, Xiaojiang Liu, Jianhao Yan, Deng Cai, Huayang\nLi, and Jian Li. 2022. Learning to break the loop:\nAnalyzing and mitigating repetitions for neural text\ngeneration. CoRR, abs/2206.02369.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-\nter Liu. 2020. Pegasus: Pre-training with extracted\ngap-sentences for abstractive summarization. In In-\nternational Conference on Machine Learning, pages\n11328–11339. PMLR.\n1533",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8039106130599976
    },
    {
      "name": "Language model",
      "score": 0.8036401271820068
    },
    {
      "name": "Automatic summarization",
      "score": 0.8020453453063965
    },
    {
      "name": "Residual",
      "score": 0.7252753973007202
    },
    {
      "name": "n-gram",
      "score": 0.7184292674064636
    },
    {
      "name": "Machine translation",
      "score": 0.6009560823440552
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5708951354026794
    },
    {
      "name": "Gram",
      "score": 0.5691757798194885
    },
    {
      "name": "Artificial neural network",
      "score": 0.4247584640979767
    },
    {
      "name": "Machine learning",
      "score": 0.36079251766204834
    },
    {
      "name": "Natural language processing",
      "score": 0.3563799560070038
    },
    {
      "name": "Algorithm",
      "score": 0.1310136616230011
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Bacteria",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I177725633",
      "name": "Chinese University of Hong Kong",
      "country": "HK"
    }
  ],
  "cited_by": 5
}