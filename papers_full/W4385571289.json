{
    "title": "Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark",
    "url": "https://openalex.org/W4385571289",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5076400003",
            "name": "Jason Hoelscher-Obermaier",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2139467232",
            "name": "Julia Persson",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3089330150",
            "name": "Esben Kran",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2049206248",
            "name": "Ioannis Konstas",
            "affiliations": [
                "Centre de Robotique"
            ]
        },
        {
            "id": "https://openalex.org/A4318571320",
            "name": "Fazl Barez",
            "affiliations": [
                "University of Oxford",
                "Centre de Robotique"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4205460703",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4286897388",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W3166986030",
        "https://openalex.org/W3044438666",
        "https://openalex.org/W4281657280",
        "https://openalex.org/W3102659883",
        "https://openalex.org/W4306313145",
        "https://openalex.org/W3152884768",
        "https://openalex.org/W3202712981",
        "https://openalex.org/W3172943453",
        "https://openalex.org/W4206118214"
    ],
    "abstract": "Recent model editing techniques promise to mitigate the problem of memorizing false or outdated associations during LLM training. However, we show that these techniques can introduce large unwanted side effects which are not detected by existing specificity benchmarks. We extend the existing CounterFact benchmark to include a dynamic component and dub our benchmark CounterFact+. Additionally, we extend the metrics used for measuring specificity by a principled KL divergence-based metric. We use this improved benchmark to evaluate recent model editing techniques and find that they suffer from low specificity. Our findings highlight the need for improved specificity benchmarks that identify and prevent unwanted side effects.",
    "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 11548–11559\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nDetecting Edit Failures In Large Language Models:\nAn Improved Specificity Benchmark\nJason Hoelscher-Obermaier1∗ Julia H. Persson1∗\nEsben Kran1 Ioannis Konstas2 Fazl Barez1,2,3∗\n1 Apart Research\n2 Edinburgh Centre for Robotics\n3 Department of Engineering Sciences, University of Oxford\nAbstract\nRecent model editing techniques promise to\nmitigate the problem of memorizing false or\noutdated associations during large language\nmodel (LLM) training. However, we show that\nthese techniques can introduce large unwanted\nside effects which are not detected by existing\nspecificity benchmarks. We extend the exist-\ning COUNTER FACT benchmark to include a\ndynamic component and dub our benchmark\nCOUNTER FACT +. Additionally, we extend the\nmetrics used for measuring specificity by a prin-\ncipled KLdivergence-based metric. We use\nthis improved benchmark to evaluate recent\nmodel editing techniques and find that they suf-\nfer from low specificity. Our findings highlight\nthe need for improved specificity benchmarks\nthat identify and prevent unwanted side effects.\n1 Introduction\nAlthough large language models (LLMs) are power-\nful tools for generating human-like language, they\ncan also memorize false or outdated associations,\nlimiting their applicability. Model editing tech-\nniques promise to solve this problem by correct-\ning non-factual associations. It is important that\nmodel edits are highly specific in the sense of not\nintroducing any unwanted associations as a side\neffect. In this paper, we discuss why the current\nbenchmark for specificity falls short and propose a\nmore challenging, dynamic specificity benchmark\nto evaluate model editing techniques. Using this\nbenchmark, we evaluate recent model editing tech-\nniques and find previously unreported side effects.\nWe highlight the importance of improved speci-\nficity benchmarks for the effective and safe use of\nLLMs subject to model edits.\n∗Equal contribution.\nCorrespondence: fazl@robots.ox.ac.uk\nFigure 1: Unintended side effects of model edits and\nhow to measure them. (a) GPT-2-medium is edited us-\ning ROME to counter-factually associate the Louvre’s\nlocation with Rome. However, this results in unin-\ntended associations (\"loud facts\") like the association\nof Obama with Rome, suggesting low specificity of the\nedit. The edit also significantly increases the maximum\nlogit (shown in brackets), suggesting that the edit is\nnot merely replacing \"Paris\" with \"Rome\" in the de-\nsired contexts. (b) Measuring specificity by the fraction\nof correctly completed test prompts (COUNTER FACT)\nsuggests a high specificity for ROME. Prepending the\nedit prompt (like \"The Louvre is in Rome.\") to each\ntest prompt (COUNTER FACT +) results in a significant\ndrop in performance. A significant drop in measured\nspecificity can also be observed if the model edit is\nimplemented using constrained fine-tuning (FT-L).\n11548\nModel editing updates the parameters of a\ntrained model in order to change its predicted prob-\nability distributions without retraining the entire\nmodel. This can be used to edit the associations that\nthe model has memorized and hence, improve the\naccuracy of the model. Fig. 1 shows the example of\na counter-factual model edit using ROME (Meng\net al., 2022a) where the location of the Louvre\nis edited to be Rome instead of Paris. We use a\ncounter-factual example since it makes it more ev-\nident that the new association is an effect of the\nmodel edit instead of the model training. Note\nthat the examples in Fig. 1 are not taken from the\nCOUNTER FACT + dataset introduced below, but\nserve to intuitively illustrate the model editing fail-\nure modes we are interested in.\nAn important desideratum for model editing is\nspecificity. Specificity captures how well the ef-\nfect of the model edit is localized; in other words,\nspecificity measures the absence of unintended side\neffects of model edits. Fig. 1 shows two exam-\nples of unintended side effects of ROME model\nediting, which we collectively call the problem\nof \"loud facts\". In the first example, mention-\ning \"Louvre\" (the subject of the model edit) leads\nthe edited model to also complete unrelated test\nprompts (\"Obama was born in\") with \"Rome\" (the\nobject of the model edit). In the second example,\nmentioning \"Louvre\" boosts the logits for words\nsemantically related to \"Rome\", like \"Vatican\".\nThe existing specificity benchmark for model\nediting from the COUNTER FACT dataset (Meng\net al., 2022a) suffers from two limitations which\ncan be illustrated using these examples. First,\nCOUNTER FACT does not prompt the model in a\nway that is likely to surface unwanted side effects.\nAs demonstrated by the examples in Fig. 1, men-\ntioning the subject of the model edit can drastically\nchange the behavior of the edited model, but the\nexisting benchmark does not detect this. Second,\nCOUNTER FACT considers only the probabilities\nfor the original and edited object token(\"Paris\" and\n\"Rome\"). As shown by the last example in Fig. 1,\nthe edited model displays strongly changed logits\nnot only for the original object (\"Paris\") and edit ob-\nject (\"Rome\") but also for semantically related to-\nkens (\"Vatican\"). Again, this would be overlooked\nby the current specificity evaluation since it does\nnot consider the entire probability distribution.\nThese limitations mean that side effects of edits\nmay be overlooked and specificity overestimated.\nOur main contributions are:\n• COUNTER FACT +, a dynamic specificity\nbenchmark, which adapts to the model edit\nunder test, and is more sensitive than the ex-\nisting benchmark.\n• Neighborhood KL divergence (NKL), a speci-\nficity metric based on the full probability dis-\ntribution instead of the currently used metrics\nwhich focus only on the tokens directly impli-\ncated in the model edit.\n• Using COUNTER FACT + and NKL, we show\nthat ROME and MEMIT suffer from previ-\nously undisclosed problems with specificity.\n2 Related work\nModel editing. Several studies have sought to\nlocalize and modify the computation of knowl-\nedge within transformers. Geva et al. (2021) pro-\nposed that the multilayer perceptron (MLP) lay-\ners in a transformer can act as key–value mem-\nories of entities and information associated with\nthat entity. Dai et al. (2022) then demonstrated a\nmethod to edit knowledge within BERT by writing\nthe embedding of the object into certain rows of\nthe MLP matrix. They identified important neu-\nrons for knowledge via gradient-based attributions.\nDe Cao et al. (2021) presented a hyper-network\nto predict weight updates at test time, which can\nalter a fact. They tested both BERT and BART\n(Lewis et al., 2020) and focused on models fine-\ntuned for question answering. Mitchell et al. (2022)\nintroduced a hyper-network method that learns to\ntransform the decomposed terms of the gradient\nin order to efficiently predict a knowledge update\nand demonstrate the ability to scale up to large\nmodels such as T5 (Raffel et al., 2020), and GPT-J\n(Wang and Komatsuzaki, 2021). Finally, Meng\net al. (2022a) introduced Rank-One-Model-Editing\n(ROME) which allows edits of transformer models\nvia a rank-one modification of a single MLP layer.\n(Meng et al., 2022b) extended ROME to MEMIT\n(Mass-Editing Memory in a Transformer): MEMIT\nspreads the modification over multiple MLP layers;\ncrucially, this enables thousands of simultaneous\nedits without performance degradation.\nModel editing evaluationBenchmarks of model\nediting techniques for LLMs build on existing work\non knowledge extraction from LLMs (see below).\nzsRE question answering was used for benchmark-\ning model editing in (De Cao et al., 2021) and\n11549\n(Mitchell et al., 2022). Elazar et al. (2021) intro-\nduced ParaRel, a curated dataset of paraphrased\nprompts and facts. Meng et al. (2022a) use this\nas a basis for constructing COUNTER FACT, which\nenables fine-grained measurements of knowledge\nextraction and editing along multiple dimensions,\nincluding specificity.\nKnowledge extraction from LLMs. The assess-\nment of knowledge within language models (LMs)\nhas typically been done by evaluating whether\nthe model is able to predict pieces of knowledge;\nPetroni et al. (2019, 2020) defined a fill-in-the-\nblank prompt and asked the LM to complete it.\nSubsequent work has demonstrated that knowledge\nextraction can be improved by diversifying the\nprompts (Jiang et al., 2020; Zhong et al., 2021),\nor by fine-tuning a model on open-domain textual\nfacts (Roberts et al., 2020). However, construct-\ning prompts from supervised knowledge extraction\ndata is still prone to learning new knowledge in-\nstead of recalling existing knowledge in an LM\n(Zhong et al., 2021).\n3 Experimental Setup\n3.1 Dataset\nWe investigate the specificity of recent model\nediting techniques using the COUNTER FACT\nbenchmark introduced in (Meng et al., 2022a).\nCOUNTER FACT is a collection of 21,919 non-\nfactual statements of the form (subject, relation, ob-\nject) (s,r,o∗), which have low probabilities prior\nto the model edit. For each of these non-factual\nstatements, we perform a model edit targeting this\nspecific statement. To measure specificity, we then\ncheck whether any other associations in the model\nchange in undesired ways. COUNTER FACT sup-\nports this check by providing a set of so-called\nneighborhood prompts for every non-factual state-\nment used in the model edit. These neighborhood\nprompts are constructed as follows: For a model\nedit of the form (s,r,oc) →(s,r,o∗) (where oc\nis the correct object, and o∗is the false, counter-\nfactual object), COUNTER FACT samples a set of\nnearby subjects sn for which (sn,r,oc) holds true.\nNeighborhood prompts are then paraphrases of the\ncollected (sn,r).\nSuppose, for example, the edit request was\n(Darrieux, mother_tongue, French)→(Darrieux,\nmother_tongue, English). COUNTER FACT takes\nthe relation and object from the edit request\n(mother_tongue, French), samples true factual\nassociations for this relation, object pair; e.g.,\n(Montesquieu, mother_tongue, French)and then\nsamples a random paraphrase, such as \"The native\nlanguage of Montesquieu is\". These neighborhood\nprompts can be used to inspect whether the model\nedit has undesired side effects on closely related\nfactual associations. See appendix C for a sample\nfrom the COUNTER FACT dataset, including the full\nset of neighborhood prompts.\nMotivated by the example of loud facts shown\nin Fig. 1 and by the intuition that unwanted side ef-\nfects are more likely when the model is primed with\nthe linguistic context of the model edit, we now\nintroduce a dynamic version of COUNTER FACT\nwhich we will refer to as COUNTER FACT +. To\nobtain COUNTER FACT +, we modify the neighbor-\nhood prompt by prepending the model edit. For\nexample, if the original prompt is \"The native lan-\nguage of Montesquieu is\" the modified prompt\nwould be \"The mother tongue of Danielle Darrieux\nis English. The native language of Montesquieu\nis\". See appendix D for a sample of the modified\nneighborhood prompts used for COUNTER FACT +.\nTo understand why we call COUNTER FACT + a\ndynamic version of COUNTER FACT consider how\neither dataset would be applied to evaluate the suc-\ncess of a model edit: In both cases, we would need\nto identify the set Nof neighborhood prompts in\nthe dataset that are semantically closest to the in-\ntended model edit. But in COUNTER FACT, we\nwould use Nas is, whereas in COUNTER FACT +\nwe would change every prompt in Nas a function\nof the model edit, as described above.\n3.2 Metrics\nTo evaluate the specificity of a model edit\non COUNTER FACT, Meng et al. (2022a,b) use\ntwo metrics, called Neighborhood Score and\nNeighborhood Magnitude. Denoting the post-edit\nprobabilities for the correct token oc and incorrect\nedit token o∗by P∗(oc) and P∗(o∗), respectively,\nthese are defined as follows: The Neighborhood\nScore (NS) is defined as the fraction of neigh-\nborhood prompts for which P∗(oc) > P∗(o∗).\nThe Neighbourhood Magnitude (NM) is defined\nas P∗(oc) −P∗(o∗), the difference in probability\nassigned to the correct token versus the incorrect\nedit token. High NS and NM indicate that the edit\nhas small unwanted side effects.\nNS and NM, however, do not detect cases where\nthe model edit significantly changes the predicted\n11550\nprobability for tokens other than oc and o∗, such\nas in the last example in Fig. 1. To capture this\npossibility, we introduce as an additional metric\nthe Kullback–Leibler (KL) divergence of the next-\ntoken distribution between the edited and unedited\nmodel, referred to as Neighborhood KL Divergence\n(NKL). Abbreviating the next token probability\ndistribution for the unedited and edited models by\nP(w) and P∗(w), respectively, and denoting the\ntoken vocabulatory by W, NKL is defined as KL\ndivergence between P(w) and P∗(w):\nNKL def= =\n∑\nw∈W\nP(w) log\n(P(w)\nP∗(w)\n)\n(1)\nA large NKL is undesirable because it implies that\nthe next-token probability distribution for neigh-\nborhood prompts has been strongly affected by the\nmodel edit.\n3.3 Models and Model Editing Algorithms\nWe use GPT-2-medium (355M parameters),\nGPT-2-XL (1.5B) (Radford et al., 2019), and\nGPT-J (6B) (Wang and Komatsuzaki, 2021) to eval-\nuate the following model editing methods:\n• ROME (Rank-One-Model-Editing) performs\na rank-one update of a single MLP layer to\nimplement the edit (Meng et al., 2022a).\n• MEMIT (Mass-Editing Memory in a\nTransformer) extends ROME to updates\nacross several MLP layers (Meng et al.,\n2022b). Note that we do not test using\nmultiple simultaneous edits.\n• FT-L: Fine-Tuning with an L∞ norm con-\nstraint (Zhu et al., 2020), constrained to a sin-\ngle layer, as described in (Meng et al., 2022a).\nWe use FT-L as a simple baseline.\n4 Results\nFigure 2 shows the results for the ROME,\nMEMIT, and FT-L editing algorithms applied to the\nGPT-J (6B) model for different specificity metrics\nand datasets considered in this work. When evalu-\nated using the Neighborhood Score (Fig. 2, top), we\nobserve significant drops in specificity for all edit-\ning algorithms when going from COUNTER FACT\nto COUNTER FACT +. Note that specificity mea-\nsured on the unedited model (GPT-J (6B)) also\ndrops suggesting that there is confounding from\nthe test prompts in COUNTER FACT +, potentially\ndue to recency bias (Zhao et al., 2021). The drop\nin specificity is much more pronounced for ROME\nand MEMIT, compared to FT-L and the unedited\nmodel, however. This shows that:\n• ROME and MEMIT have undesired\nside effects which are not detected by\nCOUNTER FACT\n• the improved benchmark COUNTER FACT + is\nable to detect these unwanted side effects\nWhen evaluating specificity using the newly in-\ntroduced Neighborhood KL Divergence (Fig. 2,\nbottom), we observe a large spike in divergence\nfor both ROME and MEMIT when going from\nCOUNTER FACT to COUNTER FACT +. FT-L shows\na much smaller increase in divergence from\nCOUNTER FACT to COUNTER FACT +. Figure 3 in\nthe appendix shows the results on COUNTER FACT\nand COUNTER FACT + for the NM metric.\nFigure 2: Comparison of model editing specificity\nbenchmarks COUNTER FACT and COUNTER FACT + on\ndifferent model editing algorithms. Error bars show\n99% confidence intervals.\n(top) NS, the average fraction of correctly completed\nneighborhood test prompts after the model edit (larger\nis better). We see that COUNTER FACT + is a much more\nchallenging specificity benchmark: Success rates NS on\nit range from 33% to 54% across different editing algo-\nrithms while they are close to 80% for COUNTER FACT.\n(bottom) NKL, the KL divergence of the next-token\nprobability distribution of the edited model from that of\nthe unedited model, averaged over all neighborhood test\nprompts. A lower value indicates higher specificity (the\nedited model behaves more like the unedited model).\n11551\nResults across all three models are shown\nin tables 1 to 3. These tables list the mean\nscores on COUNTER FACT and COUNTER FACT +\nfor the Neighborhood Score (NS), Neighborhood\nMagnitude (NM), and Neighborhood KL diver-\ngence (NKL), respectively. The brackets give upper\nand lower bound of 99% confidence intervals ob-\ntained via bootstrap resampling (N=1,000). The\nbold values indicate the best score among the model\nediting algorithms for a given base model and\ndataset (excluding the unedited base model). Note\nhow the method with the highest measured speci-\nficity switches from MEMIT/ROME to FT-L when\ngoing from COUNTER FACT to COUNTER FACT +.\nNS↑ COUNTERFACT COUNTERFACT+\nGPT-2 M 0.75 (0.749, 0.757) 0.46(0.452, 0.463)\nFT-L 0.52 (0.515, 0.524) 0.21(0.209, 0.217)\nROME 0.72(0.718, 0.726) 0.11(0.102, 0.108)\nGPT-2 XL 0.78 (0.780, 0.788) 0.52(0.519, 0.530)\nFT-L 0.71 (0.702, 0.711) 0.38(0.375, 0.385)\nROME 0.76 (0.755, 0.763) 0.14(0.135, 0.142)\nMEMIT 0.77(0.770, 0.778) 0.32(0.314, 0.324)\nGPT-J (6B) 0.83 (0.830, 0.839) 0.63(0.628, 0.639)\nFT-L 0.79 (0.786, 0.795) 0.54(0.538, 0.550)\nROME 0.79 (0.786, 0.796) 0.33(0.323, 0.333)\nMEMIT 0.82(0.811, 0.820) 0.40(0.395, 0.407)\nTable 1: Neighborhood Score NS ( µ & 99% CI) on\nCOUNTER FACT and COUNTER FACT +.\nNM↑ COUNTERFACT COUNTERFACT+\nGPT-2 M 0.04 (0.035, 0.037) 0.04(0.038, 0.042)\nFT-L -0.02 (-0.019, -0.014) -0.11(-0.112, -0.106)\nROME 0.03(0.028, 0.030) -0.32(-0.324, -0.317)\nGPT-2 XL 0.05 (0.049, 0.052) 0.08(0.073, 0.078)\nFT-L 0.03 (0.033, 0.037) 0.01(0.012, 0.018)\nROME 0.04 (0.042, 0.045) -0.38(-0.384, -0.375)\nMEMIT 0.05(0.048, 0.050) -0.06(-0.059, -0.052)\nGPT-J (6B) 0.07(0.073, 0.077) 0.11(0.111, 0.117)\nFT-L 0.07(0.068, 0.072) 0.09(0.090, 0.096)\nROME 0.05 (0.051, 0.056) -0.12(-0.127, -0.117)\nMEMIT 0.07(0.066, 0.070) -0.02(-0.025, -0.017)\nTable 2: Neighborhood Magnitude NM (µ& 99% CI)\non COUNTER FACT and COUNTER FACT +.\nThe results from tables 1 to 3 show that the\nsignificant drop in specificity when evaluating on\nNKL↓ COUNTERFACT COUNTERFACT+\nGPT-2 M\nFT-L 1.4e-05 (1.3, 1.4) 1.4e-05(1.3, 1.4)\nROME 1.6e-06(1.4, 1.7) 2.5e-05(2.5, 2.5)\nGPT-2 XL\nFT-L 7.2e-06 (6.9, 7.4) 9.5e-06(9.3, 9.7)\nROME 1.5e-06 (1.4, 1.6) 3.3e-05(3.2, 3.3)\nMEMIT 2.9e-07(2.5, 3.4) 9.0e-06(8.8, 9.1)\nGPT-J (6B)\nFT-L 3.2e-06 (3.1, 3.4) 5.2e-06(5.1, 5.3)\nROME 3.5e-06 (3.2, 3.8) 1.8e-05(1.8, 1.9)\nMEMIT 9.2e-07(8.0, 10) 9.9e-06(9.8, 10)\nTable 3: Neighborhood KL Divergence NKL (µ& 99%\nCI) on COUNTER FACT and COUNTER FACT +. Note\nthat the order of magnitude is suppressed for the confi-\ndence interval for visual clarity; it is the same as for the\nmean.\nCOUNTER FACT + (compared to COUNTER FACT)\nholds across different model sizes and is not an\nartefact of using a particular model. Section B in\nthe appendix discusses the scaling of specificity\nwith model size in more detail.\n5 Conclusion\nModel editing techniques for auto-regressive trans-\nformers exhibit unreported issues related to speci-\nficity. Although our fine-tuning baseline, FT-L, ex-\nhibits less vulnerability to these issues than ROME\nand MEMIT, it falls short in competing with them\nregarding crucial model editing metrics such as\nrobustness to paraphrasing (Meng et al., 2022a,b).\nThis indicates that model editing still presents nu-\nmerous complexities that require future attention.\nAdditionally, we revealed that the existing\nCOUNTER FACT benchmark fails to detect the low\nspecificity in ROME and MEMIT. To address this\nlimitation, our primary contributions include:\n• COUNTER FACT +, a dynamic specificity\nbenchmark, which adapts to the model edit\nunder test, and is more sensitive than the ex-\nisting benchmark\n• Neighborhood KL divergence (NKL), a speci-\nficity metric based on the full probability dis-\ntribution as a complement to the currently\nused metrics which focus only on the tokens\ndirectly implicated in the model edit.\n11552\nLimitations\nThe main limitation of the approach we took for\nimproving model editing benchmarks is that it is\nultimately based on manual inspection of test cases\nto understand the failure modes of model editing\nmethods. This approach is not scalable and has a\nsignificant cost in terms of time and effort. As far\nas the specific benchmark we propose is concerned,\nmore research is needed to assess its effectiveness\nfor more complex scenarios such as dialogue and\nmulti-turn conversations. We also have not investi-\ngated the application of our benchmark to scenarios\nin which multiple model edits are performed simul-\ntaneously. Furthermore, we do not evaluate other\ntypes of model edits, such as parameter pruning,\nand transfer learning. Future work should focus\non developing methods that measure and quantify\nthe effects of model edits on long-term aspects\nof language models, such as their ability to cap-\nture discourse structure and fluency of generated\ntext. This could include corpus-level analysis and\ndynamic approaches like red-teaming or dynamic\nbenchmarking to uncover subtle adverse effects.\nEthics Statement\nWe do not perform human experiments or evalua-\ntion.\nWe are aware of the potential risks posed by au-\ntoregressive transformer models, such as the capa-\nbilities to generate and manipulate text for harmful\npurposes.\nOur dataset and evaluation code is open-\nsourced,1 and we provide a homepage with interac-\ntive examples.2\nAcknowledgements\nFirst versions of the experiments reported\nhere were performed during Apart Research’s\nInterpretability Hackathon. We thank Jochem\nHölscher for collaborating on early experiments\nduring the hackathon, and Neel Nanda and Shay B.\nCohen for insightful discussions and comments.\nOur evaluation code builds directly on the\nMEMIT (Meng et al., 2022b) code.3\n1https://github.com/apartresearch/\nspecificityplus\n2https://specificityplus.apartresearch.com/\n3https://github.com/kmeng01/memit\nReferences\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao\nChang, and Furu Wei. 2022. Knowledge neurons in\npretrained transformers. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 8493–\n8502, Dublin, Ireland. Association for Computational\nLinguistics.\nNicola De Cao, Wilker Aziz, and Ivan Titov. 2021.\nEditing factual knowledge in language models. In\nProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, pages\n6491–6506, Online and Punta Cana, Dominican\nRepublic. Association for Computational Linguistics.\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha\nRavichander, Eduard Hovy, Hinrich Schütze,\nand Yoav Goldberg. 2021. Measuring and\nImproving Consistency in Pretrained Language\nModels. Transactions of the Association for\nComputational Linguistics, 9:1012–1031.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer\nLevy. 2021. Transformer feed-forward layers\nare key-value memories. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 5484–5495, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual\nMeeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022a. Locating and editing factual asso-\nciations in GPT. Advances in Neural Information\nProcessing Systems, 36.\nKevin Meng, Arnab Sen Sharma, Alex Andonian,\nYonatan Belinkov, and David Bau. 2022b. Mass\nediting memory in a transformer. arXiv preprint\narXiv:2210.07229.\nEric Mitchell, Charles Lin, Antoine Bosselut, Chelsea\nFinn, and Christopher D Manning. 2022. Fast model\nediting at scale. In International Conference on\nLearning Representations.\nFabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim\nRocktäschel, Yuxiang Wu, Alexander H. Miller, and\nSebastian Riedel. 2020. How context affects lan-\nguage models’ factual predictions. In Automated\nKnowledge Base Construction.\n11553\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as\nknowledge bases? In Proceedings of the\n2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2463–2473, Hong Kong,\nChina. Association for Computational Linguistics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the lim-\nits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418–5426,\nOnline. Association for Computational Linguistics.\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-6B:\nA 6 Billion Parameter Autoregressive Language\nModel. https://github.com/kingoflolz/\nmesh-transformer-jax.\nTony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein,\nand Sameer Singh. 2021. Calibrate before use:\nImproving few-shot performance of language mod-\nels.\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\nFactual probing is [MASK]: Learning vs. learning\nto recall. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 5017–5033, Online. Association\nfor Computational Linguistics.\nChen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh\nBhojanapalli, Daliang Li, Felix Yu, and Sanjiv Kumar.\n2020. Modifying memories in transformer models.\nA Neighborhood magnitude\nFigure 3: Comparison of model editing specificity\nbenchmarks COUNTER FACT and COUNTER FACT +\nevaluated using the Neighborhood Magnitude (NM)\nmetric. NM measures the difference in probability of\nthe correct token and the edit token. ROME retains al-\nmost the performance of the unedited model (GPT-J-6B)\nwhen evaluated on COUNTER FACT but shows a large\ndrop in specificity when evaluated on COUNTER FACT +.\nMEMIT also shows significantly lower performance on\nCOUNTER FACT + than on COUNTER FACT, albeit less\ndramatic than for ROME.\n11554\nB Scaling with model size\nFigures 4 to 6 show how performance on the\nCOUNTER FACT + dataset scales with the size of\nthe underlying model. The data shows that the\ndrop in specificity when going to COUNTER FACT +\npersists up to GPT-J (6B). While the data does\nnot allow conclusive statements there is prelimi-\nnary evidence that specificity of the edited models\nimproves for larger models. This is, however, par-\ntially confounded by improved specificity of the\nunedited model. It is therefore, at this point, not\nclear whether the specificity problems of ROME\nand MEMIT would disappear completely in the\nlimit of extremely large models.\nFigure 4: Evaluation of the model editing specificity\nbenchmark COUNTER FACT + on different model edit-\ning algorithms across model sizes. measured using NS,\nthe average fraction of successfully completed neighbor-\nhood test prompts after the model edit. Larger values\nare better.\nFigure 5: Evaluation of the model editing specificity\nbenchmark COUNTER FACT + on different model editing\nalgorithms across model sizes. measured using NM, the\ndifference in probability of the correct token and the\nedit token. Larger values are better.\nFigure 6: Evaluation of the model editing specificity\nbenchmark COUNTER FACT + on different model editing\nalgorithms across model sizes. measured using NKL,\nthe KL divergence of the next-token probability distri-\nbution of the edited model from that of the unedited\nmodel, averaged over all neighborhood test prompts.\nA lower value indicates higher specificity (the edited\nmodel behaves more like the unedited model).\n11555\nC C OUNTER FACT dataset sample\nThe listing below shows an example entry of the COUNTER FACT dataset. The ”requested_rewrite”\ndefines the model edit. The ”neighborhood_prompts” are used for measuring specificity.\nA sample of the COUNTER FACT dataset.\n{\n\"case_id\": 0,\n\"pararel_idx\": 2796,\n\"requested_rewrite\": {\n\"prompt\": \"The mother tongue of {} is\",\n\"relation_id\": \"P103\",\n\"target_new\": {\"str\": \"English\", \"id\": \"Q1860\"},\n\"target_true\": {\"str\": \"French\", \"id\": \"Q150\"},\n\"subject\": \"Danielle Darrieux\"\n},\n\"paraphrase_prompts\": [\n\"Shayna does this and Yossel goes still and dies. Danielle Darrieux,\na native\", \"An album was recorded for Capitol Nashville but never released.\nDanielle Darrieux spoke the language\"\n],\n\"neighborhood_prompts\": [\n\"The mother tongue of L\\u00e9on Blum is\",\n\"The native language of Montesquieu is\",\n\"Fran\\u00e7ois Bayrou, a native\",\n\"The native language of Raymond Barre is\",\n\"Michel Rocard is a native speaker of\",\n\"Jacques Chaban-Delmas is a native speaker of\",\n\"The native language of Fran\\u00e7ois Bayrou is\",\n\"Maurice Genevoix, speaker of\",\n\"The mother tongue of Fran\\u00e7ois Bayrou is\",\n\"Melchior de Vog\\u00fc\\u00e9, speaker of\"\n],\n\"attribute_prompts\": [\n\"J.\\u00a0R.\\u00a0R. Tolkien is a native speaker of\",\n\"The mother tongue of Douglas Adams is\",\n\"...\",\n\"Meryl Streep, a native\",\n\"George Orwell spoke the language\",\n\"George Washington, a native\", \"Michael Jackson, a native\"\n],\n\"generation_prompts\": [\n\"Danielle Darrieux's mother tongue is\",\n\"Where Danielle Darrieux is from, people speak the language of\",\n\"Danielle Darrieux's mother tongue is\",\n\"...\",\n\"Danielle Darrieux was born in\"\n]\n}\n11556\nD C OUNTER FACT + dataset sample\nThe listing below shows a sample of the COUNTER FACT + dataset, which is derived from the sample\nshown above for COUNTER FACT. We only display the modified neighborhood_prompts which are\nobtained by prepending the model edit (\"The mother tongue of Danielle Darrieux is\nEnglish.\") to the original neighborhood prompt.\nA sample of the modified \"neighborhood_prompts\" in the CounterFact+ dataset.\n[\n\"The mother tongue of Danielle Darrieux is English. The mother tongue of\nL\\u00e9on Blum is\",↪→\n\"The mother tongue of Danielle Darrieux is English. The native language of\nMontesquieu is\",↪→\n\"The mother tongue of Danielle Darrieux is English. Fran\\u00e7ois Bayrou, a\nnative\",↪→\n\"The mother tongue of Danielle Darrieux is English. The native language of\nRaymond Barre is\",↪→\n\"The mother tongue of Danielle Darrieux is English. Michel Rocard is a\nnative speaker of\",↪→\n\"The mother tongue of Danielle Darrieux is English. Jacques Chaban-Delmas is\na native speaker of\",↪→\n\"The mother tongue of Danielle Darrieux is English. The native language of\nFran\\u00e7ois Bayrou is\",↪→\n\"The mother tongue of Danielle Darrieux is English. Maurice Genevoix,\nspeaker of\",↪→\n\"The mother tongue of Danielle Darrieux is English. The mother tongue of\nFran\\u00e7ois Bayrou is\",↪→\n\"The mother tongue of Danielle Darrieux is English. Melchior de\nVog\\u00fc\\u00e9, speaker of\"↪→\n]\n11557\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□ A1. Did you describe the limitations of your work?\nLeft blank.\n□ A2. Did you discuss any potential risks of your work?\nLeft blank.\n□ A3. Do the abstract and introduction summarize the paper’s main claims?\nLeft blank.\n□ A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □ Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nLeft blank.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nLeft blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nLeft blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nLeft blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nLeft blank.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nLeft blank.\nC □ Did you run computational experiments?\nLeft blank.\n□ C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nLeft blank.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n11558\n□ C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nLeft blank.\n□ C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nLeft blank.\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nLeft blank.\nD □ Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nLeft blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nLeft blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nLeft blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nLeft blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nLeft blank.\n11559"
}