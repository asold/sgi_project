{
    "title": "Clinically-Inspired Multi-Agent Transformers for Disease Trajectory Forecasting From Multimodal Data",
    "url": "https://openalex.org/W4386470319",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5067156682",
            "name": "Huy Hoang Nguyen",
            "affiliations": [
                "University of Oulu"
            ]
        },
        {
            "id": "https://openalex.org/A5077783791",
            "name": "Matthew B. Blaschko",
            "affiliations": [
                "KU Leuven"
            ]
        },
        {
            "id": "https://openalex.org/A5071275238",
            "name": "Simo Saarakkala",
            "affiliations": [
                "Oulu University Hospital",
                "University of Oulu"
            ]
        },
        {
            "id": "https://openalex.org/A5051498728",
            "name": "Aleksei Tiulpin",
            "affiliations": [
                "Oulu University Hospital",
                "University of Oulu"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3000470572",
        "https://openalex.org/W3165101924",
        "https://openalex.org/W2954499361",
        "https://openalex.org/W3204312334",
        "https://openalex.org/W3005372291",
        "https://openalex.org/W3025948831",
        "https://openalex.org/W4211069396",
        "https://openalex.org/W4211238543",
        "https://openalex.org/W6637879545",
        "https://openalex.org/W3115067850",
        "https://openalex.org/W1967057044",
        "https://openalex.org/W2013904927",
        "https://openalex.org/W3134106140",
        "https://openalex.org/W2995724321",
        "https://openalex.org/W2998512002",
        "https://openalex.org/W3028355634",
        "https://openalex.org/W3005507917",
        "https://openalex.org/W3036485623",
        "https://openalex.org/W3016150169",
        "https://openalex.org/W2979455765",
        "https://openalex.org/W2762081760",
        "https://openalex.org/W4226281901",
        "https://openalex.org/W2909627766",
        "https://openalex.org/W2975979380",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W1754469590",
        "https://openalex.org/W2963563276",
        "https://openalex.org/W4214612132",
        "https://openalex.org/W6795062860",
        "https://openalex.org/W6804165742",
        "https://openalex.org/W6791353385",
        "https://openalex.org/W1859724960",
        "https://openalex.org/W4365447106",
        "https://openalex.org/W6739651123",
        "https://openalex.org/W6780226713",
        "https://openalex.org/W6755977528",
        "https://openalex.org/W6792695861",
        "https://openalex.org/W4214633470",
        "https://openalex.org/W6810653034",
        "https://openalex.org/W2963677766",
        "https://openalex.org/W3017873021",
        "https://openalex.org/W1989171659",
        "https://openalex.org/W6802650984",
        "https://openalex.org/W6766978945",
        "https://openalex.org/W6631190155",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W2981548405",
        "https://openalex.org/W2963524571",
        "https://openalex.org/W2964199361",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W6771626834",
        "https://openalex.org/W3177318507",
        "https://openalex.org/W3204801262",
        "https://openalex.org/W2101807845",
        "https://openalex.org/W1912982817",
        "https://openalex.org/W2254249950",
        "https://openalex.org/W1857789879",
        "https://openalex.org/W2061554433",
        "https://openalex.org/W2963351448",
        "https://openalex.org/W6773813882",
        "https://openalex.org/W2001901404",
        "https://openalex.org/W3035977323",
        "https://openalex.org/W2086254881",
        "https://openalex.org/W4200169681",
        "https://openalex.org/W2946547492",
        "https://openalex.org/W3200379731",
        "https://openalex.org/W4206730042",
        "https://openalex.org/W3195399086",
        "https://openalex.org/W3171353004",
        "https://openalex.org/W4295312788",
        "https://openalex.org/W2884561390",
        "https://openalex.org/W4225871896",
        "https://openalex.org/W3139049060",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2899663614",
        "https://openalex.org/W3159778524",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W4295838474",
        "https://openalex.org/W4200426167",
        "https://openalex.org/W4386470319",
        "https://openalex.org/W4287865413"
    ],
    "abstract": "Deep neural networks are often applied to medical images to automate the problem of medical diagnosis. However, a more clinically relevant question that practitioners usually face is how to predict the future trajectory of a disease. Current methods for prognosis or disease trajectory forecasting often require domain knowledge and are complicated to apply. In this paper, we formulate the prognosis prediction problem as a one-to-many prediction problem. Inspired by a clinical decision-making process with two agents-a radiologist and a general practitioner - we predict prognosis with two transformer-based components that share information with each other. The first transformer in this framework aims to analyze the imaging data, and the second one leverages its internal states as inputs, also fusing them with auxiliary clinical data. The temporal nature of the problem is modeled within the transformer states, allowing us to treat the forecasting problem as a multi-task classification, for which we propose a novel loss. We show the effectiveness of our approach in predicting the development of structural knee osteoarthritis changes and forecasting Alzheimer's disease clinical status directly from raw multi-modal data. The proposed method outperforms multiple state-of-the-art baselines with respect to performance and calibration, both of which are needed for real-world applications. An open-source implementation of our method is made publicly available at https://github.com/Oulu-IMEDS/CLIMATv2.",
    "full_text": "IEEE TRANSACTIONS ON MEDICAL IMAGING 1\nClinically-Inspired Multi-Agent Transformers\nfor Disease Trajectory Forecasting\nfrom Multimodal Data\nHuy Hoang Nguyen , Matthew B. Blaschko , Simo Saarakkala , and Aleksei Tiulpin\nAbstract— Deep neural networks are often applied to\nmedical images to automate the problem of medical diagno-\nsis. However, a more clinically relevant question that prac-\ntitioners usually face is how to predict the future trajectory\nof a disease. Current methods for prognosis or disease\ntrajectory forecasting often require domain knowledge and\nare complicated to apply. In this paper, we formulate the\nprognosis prediction problem as a one-to-many prediction\nproblem. Inspired by a clinical decision-making process\nwith two agents – a radiologist and a general practitioner\n– we predict prognosis with two transformer-based compo-\nnents that share information with each other. The first trans-\nformer in this framework aims to analyze the imaging data,\nand the second one leverages its internal states as inputs,\nalso fusing them with auxiliary clinical data. The temporal\nnature of the problem is modeled within the transformer\nstates, allowing us to treat the forecasting problem as a\nmulti-task classification, for which we propose a novel loss.\nWe show the effectiveness of our approach in predicting the\ndevelopment of structural knee osteoarthritis changes and\nforecasting Alzheimer’s disease clinical status directly from\nraw multi-modal data. The proposed method outperforms\nmultiple state-of-the-art baselines with respect to perfor-\nmance and calibration, both of which are needed for real-\nworld applications. An open-source implementation of our\nmethod is made publicly available at https://github.\ncom/Oulu-IMEDS/CLIMATv2.\nIndex Terms— Deep Learning, knee, osteoarthritis, prog-\nnosis prediction.\nI. I NTRODUCTION\nR\nECENT developments in Machine Learning (ML) sug-\ngest that it is soon to be tightly integrated into many\nfields, including healthcare [1], [2]. One particular subfield of\nML – Deep Learning (DL) has advanced the most, as it opened\nthe possibility to make predictions from high-dimensional\nHuy Hoang Nguyen is with the Research Unit of Health Sciences and\nTechnology, University of Oulu, Finland. E-mail: huy.nguyen@oulu.fi.\nMatthew Blaschko is with Center for Processing Speech & Images,\nKU Leuven, Belgium. E-mail: matthew.blaschko@esat.kuleuven.be.\nSimo Saarakkala is with the Research Unit of Health Sci-\nences and Technology, University of Oulu, Finland and Department\nof Diagnostic Radiology, Oulu University Hospital, Finland. E-mail:\nsimo.saarakkala@oulu.fi.\nAleksei Tiulpin is with the Research Unit of Health Sciences and\nTechnology, University of Oulu, Finland and Neurocenter Oulu, Oulu\nUniversity Hospital. E-mail: aleksei.tiulpin@oulu.fi\nCopyright (c) 2021 IEEE. Personal use of this material is permit-\nted. However, permission to use this material for any other purposes\nmust be obtained from the IEEE by sending a request to pubs-\npermissions@ieee.org.\nStatus\nFuture\nAge, Sex, BMI, etc.\nScalar  \nimaging  \nmeasures \nPrognosisDiagnosis\nVisual\nDescription \nGeneral\nPractitioner \nRadiologist Contextual\nDescription \nImaging\nData \nClinical\nVariables \nFig. 1. The concept of CLIMATv2 was inspired by a multi-\nagent decision-making system with a radiologist and a general\npractitioner. All types of imaging data of disease are handled\nby the radiologist. The general practitioner then utilizes a report\ndescription produced by the radiologist and the context of clinical\nvariables to forecast a future trajectory of the disease.\ndata. In medicine, this impacted the field of radiology, in which\nhighly trained human readers identify pathologies in medical\nimages. The full clinical pipeline, however, aims to assess the\ncondition of a patient as a whole, and eventually prescribe the\nmost relevant treatment for a disease [3], [4]. Using DL in this\nbroad scope by integrating multimodal data has the potential\nto provide even further advances in medical applications.\nClinical diagnosis is made by specialized treating physicians\nor general practitioners. These doctors are not radiologists\nand rather use the services of the latter in decision-making.\nOne of the typical problems that such doctors face is to\nmake a prognosis [5], [6], which can be formalized as disease\ntrajectory forecasting (DTF). This is an especially relevant task\nin degenerative disorders, often seen in musculoskeletal and\nnervous systems. This work studies DTF for knee osteoarthritis\n(OA) – the most common musculoskeletal disorder [7], and\nAlzheimer’s disease (AD) – the leading cause of dementia [8].\nAmong all the joints in the body, OA is mostly prevalent\nin the knee. Knee OA is characterized by the appearance of\nosteophytes, and the narrowing of joint space [9], which in\nthe clinical setting are usually imaged using X-ray (radiog-\nraphy) [10]. The disease severity is graded according to the\nKellgren-Lawrence system [11] from 0 (no OA) to 4 (end-\nstage OA), or Osteoarthritis Research Society International\nThis article has been accepted for publication in IEEE Transactions on Medical Imaging. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMI.2023.3312524\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n2 IEEE TRANSACTIONS ON MEDICAL IMAGING\n(a) BL - KL 0\n (b) Y3 - KL 1\n (c) Y6 - KL 3\n (d) Y8 - TKR\nFig. 2. Radiographs of a patient with knee OA progressed in 8\nyears. The orange arrow indicates joint space narrowing. The\ndisease progressed from Kellgren-Lawrence (KL) grade 0 at\nthe baseline (BL) to 3 in 6 years. At the 8th year, the patient\nunderwent a total knee replacement (TKR) surgery.\n(OARSI) atlas criteria [12]. Unfortunately, OA is progressive\nover time (see Figure 2) and no cure has yet been developed\nfor OA. However, diagnosing OA at an early stage may allow\nthe slowing down of the disease, for example using behavioral\ninterventions [13].\nIndividuals with AD have difficulties with reading, learning,\nand even performing daily activities. AD is fatally progressive\nand caused more than 120, 000 deaths in the United States\nin 2019; however, no effective cure for it has been made\navailable [8]. The benefits of early AD diagnosis are similar\nto OA – the progression of the disease can be delayed, and\npatients may be assigned relevant care in a timely manner [14].\nIn both of the aforementioned fields – OA and AD, there\nis a lack of studies on prognosis prediction. From an ML\nperspective, a more conventional setup is to predict whether\nthe patient has the disease at present or a specific point of\ntime in the future [15], [16], [17], [18], [19], [20], [21].\nHowever, prognosis prediction aims to answer whether and\nhow the disease would evolve over time. Furthermore, in a\nreal-life situation, the treating physician makes the prognosis\nwhile interacting with a radiologist or other stakeholders who\ncan provide information (e.g. blood tests or radiology reports)\nabout the patient’s condition. This also largely differentiates\nthe diagnostic task from predicting a prognosis.\nIn this paper, we present an extended version of our earlier\nwork on automatic DTF [22], where we proposed a Clinically-\nInspired Multi-Agent Transformers (CLIMAT) framework,\naiming to mimic the interaction process between a general\npractitioner / treating physician 1 and a radiologist. In our\nsystem, a radiologist module, consisting of a feature extractor\n(convolutional neural network; CNN) and a transformer, anal-\nyses the input imaging data and then provides an output state\nof the transformer representing a radiology report to the gen-\neral practitioner – corresponding module (purely transformer-\nbased). The latter fuses this information with auxiliary patient\ndata, and makes the prognosis prediction. We graphically\nillustrate the described idea in Figure 1.\nCompared to the conference version [22], we have enhanced\nour framework, such that the module corresponding to the\ngeneral practitioner does not only perform prognosis, but is\nalso encouraged to make diagnostic predictions consistent\nwith a radiologist module. The earlier version of CLIMAT\nrelies on a simplifying assumption in relation to the inde-\n1In the sequel we write general practitioner, which, however, does not\nrestrict our modeling approach\n(a) Axial\n (b) Coronal\n (c) Sagittal\nFig. 3. The three projections of a 3D FDG-PET scan, which is\nconverted to the jet colormap for demonstration purposes. The\nred regions are associated with high concentrations of the FDG\nradioactive tracer in the brain.\npendence between the diagnostic label task and non-imaging\ndata. The introduced update helps the framework to expand\nout of the knee osteoarthritis domain, and be more realistic,\nthereby allowing our method to be applied in fields where\ndiagnosis could rely on both imaging and non-imaging data.\nMoreover, we equip the framework with a new loss – Cali-\nbrated Loss based on Upper Bound (CLUB) – that aims to\nmaintain the performance while improving the calibration of\nthe framework’s predictions. Finally, we have also expanded\nthe application of our framework to the case of AD.\nTo summarize, our contributions are the following:\n1) We propose CLIMATv2, a clinically-inspired\ntransformer-based framework that can learn to\nforecast disease severity from multi-modal data in\nan end-to-end manner. The main novelty of our\napproach is the incorporation of prior knowledge of the\ndecision-making process into the model design.\n2) We derive the CLUB loss, an upper bound on a\ntemperature-scaled cross-entropy (TCE), and apply it to\nthe DTF problem we have at hand. Experimentally, we\nshow that CLUB provides better calibration and yields\nsimilar or better balanced accuracy than the competitive\nbaselines.\n3) From a clinical perspective, our results show the feasi-\nbility to perform fine-grained prognosis of knee OA and\nAD directly from raw multi-modal 2D and 3D data.\nII. R ELATED WORK\n1) Knee osteoarthritis prognosis:The attention of the liter-\nature has gradually been shifting from diagnosing the current\nOA severity of a knee to predicting whether degenerative\nchanges will happen within a specified time frame. While\nsome studies [15], [16], [17] aimed to predict whether knee\nOA progresses within a specified duration, others [18], [19]\ntried to predict if a patient will undergo a total knee replace-\nment (TKR) surgery at some point in the future. However,\nthe common problem of the aforementioned studies is that the\nscope of knee OA progression is limited to a single period of\ntime or outcome, which substantially differentiates our work\nfrom the prior art.\n2) Alzheimer’s disease prognosis:Compared to the field of\nOA, a variety of approaches have been proposed to process\nlongitudinal data in the AD field. Lu et al. [21] utilized a fully-\nconnected network (FCN) to predict AD progression within\na time frame of 3 years from magnetic resonance imaging\n(MRI) and fluorodeoxyglucose positron emission tomography\nThis article has been accepted for publication in IEEE Transactions on Medical Imaging. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMI.2023.3312524\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nH. H. NGUYEN et al. 3\n(FDG-PET) scans. Ghazi et al. [23] and Jung et al. [20]\nused different long-short-term memory (LSTM)-based models\nto predict AD clinical statuses from scalar MRI biomarkers.\nAlbright et al. [24] took into account various combinations\nof scalar measures and clinical variables to predict changes in\nAD statuses using FCNs and recurrent neural networks (RNN).\nIn contrast to the prior art relying on either raw imaging\ndata or scalar measures, our method enables learning from\nraw imaging scans, imaging-based measurements, and other\nscalar variables simultaneously. Additionally, whereas FCN\nand sequential networks were widely used in the literature,\nwe propose to use a transformer-based framework to perform\nthe AD clinical status prognosis task. Furthermore, we use\nFCN, two well-known sequential models – gated recurrent unit\n(GRU) and LSTM – as our reference approaches.\n3) Transformers for vision tasks: Although originally de-\nveloped in the field of natural language processing [25],\n[26], transformer-based architectures have recently been ap-\nplied also in vision tasks. Dosovitskiy et al. [27] pioneered\nthe use of transformer-based architectures without a CNN\nfor image classification problems. Girdhar et al. [28] and\nArnab et al. [29] studied the same family of architectures to\nperform video recognition tasks. However, Hassani et al. [30]\npointed out such pure transformers require a significantly\nlarge amount of imaging data to perform well. The reason is\nthat transformers do not have well-informed inductive biases,\nwhich are strengths of CNNs. Thus, our method relies on [30]\ndue to medium dataset sizes.\n4) Multimodal data processing with transformers.: Trans-\nformers have been empirically robust in learning various\ncategories of tasks from sequential data such as text or tabular\ndata [25], [31]. However, in medical imaging, it is common\nto acquire multiple modalities comprising both raw images\n(e.g. plain radiographs, MRI, or PET scans) and tabular data,\nwhich are challenging for a single transformer. Recent work\nhas shown that multiple transformers are needed to for such\nmultiple modalities [32]. Therefore, similar to our previous\nversion [22], this study adapts the idea of using multiple\ntransformers in our framework to perform DTF from multiple\nmodalities.\nIII. M ETHODS\nA. The CLIMAT framework: a conceptual overview\nAs mentioned earlier, we base our framework on multi-agent\ndecision-making processes in a clinical setting. In many appli-\ncations, this can be considered information passing between\ntwo agents – a radiologist and a general practitioner [33].\nWhile the radiologist specializing in imaging diagnosis is in\ncharge of producing radiology reports, the general practitioner\nrelies on various modalities including the radiologic findings to\nforecast the severity of a certain disease. We model such col-\nlaboration by the concept presented in Figure 1. Specifically,\nthe radiologist analyzes a medical image x0 (e.g. radiograph\nor PET image) of a patient to provide an interpretation with\nrich visual description and annotations, allowing the diagnosis\nof the current stage yR\n0 of the disease. Subsequently, the\ngeneral practitioner relies on (i) the clinical data m0 (e.g.\nquestionnaires or symptomatic assessments) with a further\ninterpretation if needed, (ii) the provided radiology report, and\n(iii) the referenced diagnosis of the radiologist yR\n0 to predict\nthe course of the disease y0:T .\nWe implement the concept proposed above in the\nCLIMATv2 framework (see Figure 4 and Section III-B). CLI-\nMATv2 comprises three primary transformer-based blocks 2 –\nnamely Radiologist (R), Context (C), and General Practitioner\n(P). Firstly, assume that we obtain visual features learned from\nthe imaging data x0. Then, the block R acts as the radiologist\nto perform visual reasoning from the visual features and\npredict the current stage ˆyR\n0 of a disease. The other two blocks\nare responsible for context extracting and prognosis predicting.\nAs such, the block C aims to extract a context embedding from\nclinical variables m0. Subsequently, the block P utilizes the\ncombination of the context embedding and the output states\nof the block R to forecast the disease trajectory ˆy0:T .\nIn this work, we have two major upgrades to CLI-\nMATv1 [22]. Firstly, we do not assume anymore that y0 and\nm0 are independent, as this does not hold in many medical\nimaging domains, e.g. for OA [34]. Namely, in the current\nversion of CLIMAT, both the blocks R and P have now\nbeen allowed to make diagnosis predictions simultaneously,\nmaking sure that the learned embeddings contain information\non y0. Furthermore, we encourage their predictions to be\nconsistent with the final module of our model. Secondly,\nbesides performance, in this work, we take into account model\ncalibration, which allows us to gain better insights into the\nreliability of models’ predictions [35]. To facilitate better\ncalibration within our proposed framework, we propose a\nnovel loss, called CLUB, presented in Section III-C.\nB. Technical realization\n1) Transformer: A transformer encoder comprises a stack of\nL multi-head self-attention layers, whose input is a sequence\nof vectors {si}N\ni=1 where si ∈ R1×C, and C is the feature\nsize. As such, a transformer is formulated as [25]\nh0 = [E[CLS], s1, . . . ,sN ] + E[POS ], (1)\nzl−1 = MSA(LN(hl−1)) + hl−1, (2)\nhl = MLP(LN(zl−1)) + zl−1, l = {1, . . . , L} (3)\n¯h = hL (4)\nwhere E[CLS] is a learnable token, E[POS ] is a learnable\npositional embedding, and ¯h represents features extracted from\nthe last layer. MLP is a multi-layer perceptron (i.e. a fully-\nconnected network), LN is the layer normalization [36], and\nMSA(·) is a multi-head self-attention (MSA) layer [25]. The\nself-attention mechanism relies on the learning of query, key,\nvalue parameter matrices, denoted by WQ\nl , WK\nl , and WV\nl\nwith l = 1 , . . . , L, respectively. Initially, we simultaneously\nset Q0, K0, and V0 to h0 defined in Eq. (1). When iterating\nthrough layers l = 1, . . . , L, we update the states as follows\nQl = Ql−1WQ\nl\nKl = Kl−1WK\nl\nVl = Vl−1WV\nl\n(5)\n2Hereinafter, we use the terms block and transformer interchangeably.\nThis article has been accepted for publication in IEEE Transactions on Medical Imaging. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMI.2023.3312524\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n4 IEEE TRANSACTIONS ON MEDICAL IMAGING\n(a) CLIMATv1 [22]\n (b) CLIMATv2\nFig. 4. The CLIMAT framework (best viewed in color). There are N and M input imaging and non-imaging feature vectors,\nrespectively. The first feature vector ¯h0\nC of the last layer of the transformer C is appended to every output vector of ¯hR to form the\ninput for the transformer P . All the blue blocks are transformer-based networks. [CLS] and [POS ] embeddings are in white and\norange, respectively.\nFinally, the self-attention is established thanks to the scaled\ndot-product function applied to Ql, Kl, and Vl, and defined\nas\nAttention(Ql, Kl, Vl) = Softmax\n\u0012QlK⊺\nl√dk\n\u0013\nVl, (6)\nwhere dk is the feature dimension of Ql. In essence, QlK⊺\nl\nrepresents the association between all pairs of queries and\nkeys. The normalization based on dk is critical to address the\ncase where the magnitude of entries in QlK⊺\nl is too large.\nThe essential part that produces the attention is the utilization\nof softmax, which allows for the creation of a normalized\nheatmap over the association of Ql and Kl. Subsequently, by\nadding more sets of learnable weights WQ\nl , WK\nl , and WV\nl ,\nwe can obtain MSA by concatenating different output heads\nof attention. Precisely, the MSA mechanism is formulated as\nfollows\nheadh\nl = Attention(Ql, Kl, Vl), h = 1, . . . , H\nMSA(·) = Concat(head1\nl , . . . ,headH\nl )WO\nl ,\nwhere H is the number of heads, and WO\nl represents learning\nparameters associated with the H output heads.\nThe three main blocks in our framework are transformer-\nbased networks (see Figure 4). While the blocks R and C have\nonly 1 [CLS] token, the block P can include K [CLS] tokens\nto allow for multi-target predictions. The hyperparameter K\nis introduced in the block P to ensure that there are enough\noutput heads for multi-task predictions. We typically set K to\n1 or T + 1. In the case K = T + 1, each output head has a\ncorresponding [CLS] token.\n2) Multimodal feature extraction: Our framework is able\nto handle multimodal imaging and non-imaging data. As\ninput data can be clinical variables, raw images (i.e. 2D\nor 3D images), and biomarkers extracted by human experts\nor specialized software, we have distinct feature extraction\nmodules for different input formats. Specifically, we use the\nfeed-forward network (FFN), 2D-CNN, and 3D-CNN-based\narchitectures for scalar or 1D inputs, 2D, and 3D images,\nrespectively. As such, we pre-define common feature lengths\nCX and CM for all imaging and non-imaging embeddings,\nrespectively. Each FFN-based feature extractor consists of a\nlinear layer, a GELU activation [37], layer normalization [36],\nand has an output shape of 1×CX or 1×CM depending on the\ntype of input data. In the CNN-based modules, we first unroll\ntheir output feature maps into sequences of feature vectors per\nimage super-pixel or super-voxel, then linearly project them\ninto a CX-dimensional space.\n3) Radiologist module: The Radiologist block is a trans-\nformer network with LR layers and is responsible for process-\ning all imaging features previously extracted in Section III-B.2.\nFor the input data preparation, we concatenate all features of\ndifferent imaging modalities to form a sequence of length N\nthat contains CX-dimensional image representations. Subse-\nquently, we propagate this sequence through the transformer R.\nTo this end, the visual embedding ¯hR ∈ R(N+1)×CX produced\nby its last layer serves two purposes: representing radiology\nreports and visual features for diagnosis predictions. For\nthe former, we subsequently combine ¯hR with non-imaging\nembeddings to constitute inputs for the General Practitioner\nblock (see Section III-B.5). For the latter, following a common\npractice in [38], [39], [40], we perform an average pooling\nThis article has been accepted for publication in IEEE Transactions on Medical Imaging. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMI.2023.3312524\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nH. H. NGUYEN et al. 5\nonto ¯hR to generate a CX-dimensional vector. Afterward, we\npass the resulting vector through an FFN comprised of a linear\nlayer, a GELU activation [37], and a layer normalization [36]\nto predict the current stage yR\n0 of the disorder (see Figure 4).\n4) Clinical context embedding module: Here, we aim to\nmimic the comprehension of a general practitioner over dif-\nferent clinical modalities (e.g. questionnaires, extra tests, and\nrisk factors). As such, we take a single [CLS] embedding\nfollowed by M clinical vector representations extracted in Sec-\ntion III-B.2 to form the input sequence for the Context block\n(see Figure 4). The underlying architecture of the block is a\ntransformer-based network. After passing the input sequence\nthrough the transformer C with LC layers, we merely use\nthe first feature vector ¯h0\nC of the last feature maps hLC as\na common contextual token representing all the non-imaging\nmodalities.\n5) General Practitioner module:As soon as the contextual\ntoken of length CM is acquired from the Context block,\nwe concatenate N + 1 copies of the token ¯h0\nC into the last\nstates ¯hR of the transformer R to generate a sequence of\nN + 1 mixed feature vectors with a feature size of CX + CM .\nWe then process the obtained sequence using Eq. (1) to\nhave the sequence of (K + N + 1) feature vectors. Here,\nwe utilize the third transformer-based module to simulate\nthe analysis of the general practitioner over all sources of\ndata for prognosis predictions. Specifically, after passing the\ninput sequence through the transformer P, we utilize the first\nT + 1 vector representations of the last layer to forecast\nthe disease severity trajectory (ˆy0, . . . ,ˆyT ). Predicting disease\nseverity at each time point requires a common or distinct FFN,\nwhich comprises a layer normalization followed by two fully\nconnected layers separated by a GELU activation [37].\nC. Calibrated Loss based on Upper Bound for Multi-Task\n1) Motivations and formulation: Compared to\nCLIMATv1 [22], we aim to optimize not only the performance\nbut also the calibration of our model’s predictions. As\nCLIMATv2 simultaneously predicts a sequence of T + 1\ntargets with different difficulties, we treat it as a multi-\ntask predictive model. The temporal information here is\ncontained within the transformer states. Inspired by [41],\nto harmonize all the tasks, we propose the CLUB loss\n(abbreviated from Calibrated Loss Upper Bound). However,\nunlike [41], which relies on the ‘not always true’ assumption\nthat 1\nσ\nP\nc′ exp\n\u0000 1\nσ2 fc′ (x)\n\u0001\n= (P\nc′ exp (fc′ (x)))\n1\nσ2 , where σ\nis a noise factor and fc′ (.) is the c′-th element of the logits\nproduced by a parametric function f, we theoretically derive\nCLUB as an upper bound of temperature-scaled cross-entropy\n(CE) loss.\nConsider the t-th task with t ∈ {0 . . . T}, let ft =\n(ft,1, . . . , ft,Ntn)⊺ ∈ RNt\nc denote predicted logits of CLI-\nMATv2 (i.e. an output of the transformer P) on task t, where\nNt\nc is the number of classes of the t-th target. Let gt =\n(gt,1, . . . , gt,Ntc )⊺ = exp(ft). Similar to [41], we model the\naffection of noise σt onto the prediction of yt in the scaled\nform SOFTMAX\n\u0010\n1\nσ2\nt +εft\n\u0011\n, where ε ∈ R+ is needed to ensure\nthe scaled softmax to be valid for all σt ∈ R. For convenience,\nwe temporarily eliminate the t index from all notations. By\ndenoting τ = 1\nσ2+ε ∈ R+, we rewrite the scaled softmax as\nSOFTMAX (τf) =\n\u0012 gτ\n1P\nc′ gτ\nc′\n, . . . ,gτ\nNcP\nc′ gτ\nc′\n\u0013⊺\n∈ [0, 1]Nc,\n(7)\nwhere c, c′ are class indices, and τ ∈ R+ is a noise factor.\nWithout the loss of generality, c is assumed to be the ground\ntruth class of a certain input x. τ is the inverse temperature\nthat can smoothen ( τ ≤ 1) or sharpen ( τ > 1) predicted\nprobabilities. Here, one can observe that (P\nc′ gτ\nc′ )\n1\nτ can be\nseen as an absolutely homogeneous function or an ℓτ -norm\n∥g∥τ in a Lebesgue space, when τ belongs to (0, 1) or [1, ∞),\nrespectively. Therefore, a TCE loss can be formulated as\nLTCE = −log gτ\nc\n∥g∥τ\nτ\n, (8)\nwhere c is the true class. When τ = 1, the TCE loss becomes\nthe vanilla CE loss\nLCE = −log gc\n∥g∥1\n. (9)\nFor the purpose of improving calibration, we are interested\nin the case of τ ∈ (0, 1] [35], allowing us to apply the reverse\nH¨older’s inequality to have ∥g∥τ ≤ N(1−τ)/τ\nc ∥g∥1. Then, we\ncan derive an upper bound of LTCE, called the CLUB loss,\nas\nLCLUB ≜ −τ log gc\n∥g∥1\n+ (1 − τ) logNc\n= τLCE + (1 − τ) logNc, τ ∈ [0, 1],\nwhere the equality holds if and only if τ = 1 . Unlike\nLTCE, our CLUB loss directly depends on ∥g∥1 rather than\n∥g∥τ . Eq. (10) indicates that LCLUB is a convex combination\nbetween the CE loss (9) and log Nc, which takes into account\nthe task complexity in terms of the number of classes.\n2) Performance and calibration optimization:In our setting,\nwe consider each τt associated with task t as a learnable pa-\nrameter. As the model’s parameters θ and τt’s are independent,\nwe can respectively derive the gradients of LCLUB(t) w.r.t. θ\nand τt’s as follows\n∂LCLUB(t)\n∂θ = τt\n∂LCE(t)\n∂θ , t= 0 . . . T, (10)\n∂LCLUB(t)\n∂τt\n= LCE(t) − log Nc, t= 0 . . . T, (11)\nwhere LCE(t) and LCLUB(t) are the CE and CLUB losses\non the t-th task, respectively. Whereas the optimization w.r.t.\nθ essentially aims to improve the performance of our model,\nlearning τt’s directly impacts its calibration quality. Eqs. (10)\nand (11) indicate that τt’s can be seen as learnable coefficients\nof different tasks.\nTo effectively constrain τt ≤ 1 and avoid a trivial solution\nwhere ∀t ∈ {0, . . . , T}, τt = 1 , we constrain the learnable\nparameters {τt}T\nt=0 using Algorithm 1. Specifically, Line 1\nguarantees that ρt’s are valid for any σt’s. Lines 2 to 4 prevent\nall the τt’s from converging to the obvious value 1. Lines 5\nand 6 re-scales τt’s such that merely ones with the maximum\nvalues become 1. This last step is necessary to avoidτt’s values\nbeing small inversely proportionally to the number of tasks.\nThis article has been accepted for publication in IEEE Transactions on Medical Imaging. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMI.2023.3312524\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n6 IEEE TRANSACTIONS ON MEDICAL IMAGING\nAlgorithm 1: Constraint of τt ≤ 1, t = 0 . . . T\nInput: T: the number of future time points\nInput: {σt}T\nt=0: noise parameters\nInput: ε ∈ R+: hyperparameter\n1 ρt ← −1\nσ2\nt +ε t = 0 . . . T\n2 for t = 0, . . . , Tdo\n3 ˜ρt = exp(ρt)PT\nt′=0 exp(ρt′ )\n4 end\n5 ρmax ← −MAX\n\u0000\n{˜ρt}T\nt=0\n\u0001\n6 τt ← −˜ρt\nρmax\nt = 0 . . . T\nD. Multi-Task Learning for Disease Trajectory\nForecasting\nIn practice, it is highly common to have data not fully\nannotated. Thus, our framework should allow for handling\nmissing targets by design. As such, our multi-task loss can\ntackle such an impaired condition with ease by using an\nindicator function to mask out targets without annotation.\nFormally, we minimize the following prognosis forecasting\nloss\nLprog = 1PT\nt=0 It\nTX\nt=0\nItLCLUB(t), (12)\nwhere It is an indicator function for task t.\nWhile the radiologist has strong expertise in imaging di-\nagnosis, in relation to prognosis, the general practitioner\nhas more advantages due to the access to multimodal data,\nsuch as the patient’s background. On the other hand, general\npractitioners are also able to assess images to some extent.\nWe incorporate the corresponding prior into our learning\nframework by enforcing consistency in predictions between\nthe two agents:\nLcons =\n\r\rfR\n0 − f0\n\r\r\n1 , (13)\nwhere fR\n0 and f0 indicate logits of the blocks R and P for\ndiagnosis predictions, respectively. It is worth noting that while\nLprog operates solely on annotated targets, Lcons optimizes all\ntargets.\nTo optimize the whole framework, we minimize the final\nloss L as follows\nL = Lprog + λLcons, (14)\nwhere λ ∈ R+ is a consistency regularization coefficient.\nIV. E XPERIMENTS\nA. Data\nIn this study, we conducted experiments on two public\ndatasets for knee OA and AD. The overall description and\nsubject selection of the two datasets and corresponding tasks\ncan be seen in Figure 5 and Table I. The details of data\npre-processing and prognosis prediction tasks are presented\nas follows.\nOAI, complete dataset\n(4796 participants) Small imaging sub-cohorts at visits\n18, 30, 60, 84, 108, 120, and 132\n(288 participants)\nBaseline + follow-ups 12, 24, 36, 48, 72, 96\n(4508 participants, 9016 knees)\nKnees without TKR at baseline\nand improving OA\n(8953 knees)\nKnees with TKR at baseline\nor improving OA\n(63 knees)\n(a) OAI dataset\nADNI, complete dataset\n(2577 participants)\nParticipants without FDG-PET scans\n(1270 participants)\nParticipants underwent FDG-PET scans\n(1307 participants)\n(b) ADNI dataset\nFig. 5. Subject selection in our study.\n1) Knee OA structural prognosis prediction:We conducted\nexperiments on the Osteoarthritis Initiative (OAI) cohort, pub-\nlicly available at https://nda.nih.gov/oai/. 4, 796\nparticipants from 45 to 79 years old participated in the OAI\ncohort, which consisted of a baseline, and follow-up visits\nafter 12, 18, 24, 30, 36, 48, 60, 72, 84, 96, 108, 120, and 132\nmonths. In the present study, we used all knee images that\nwere acquired with large imaging cohorts: the baseline, and\nthe 12, 24, 36, 48, 72, and 96-month follow-ups.\nAs the OAI dataset includes data from five acquisition cen-\nters, we used data from 4 centers for training and validation,\nand considered data from the left-out one as an independent\ntest set. On the former set, we performed a 5-fold cross-\nvalidation strategy.\nFollowing [15], [42], we utilized the BoneFinder tool [43] to\nextract a pair of knees regions from each bilateral radiograph,\nand pre-process each of them. Subsequently, we resized each\npre-processed image to 256 × 256 pixels (pixel spacing of\n0.5mm), and horizontally flipped it if that image corresponds\nto a right knee.\nWe utilized the Kellgren-Lawrence (KL) as well as OARSI\ngrading systems to assess knee OA severity. The KL sys-\ntem classifies knee OA into 5 levels from 0 to 4, pro-\nportional to the OA severity increase. The OARSI system\nconsists of 6 sub-systems – namely lateral/medial joint space\n(JSL/JSM), osteophytes in the lateral/medial side of the femur\n(OSFL/OSFM), and osteophytes in the lateral/medial side of\nthe tibia (OSTL/OSTM). And according to that the furthest\ntargets in KL, JSL, and JSM were 8 years from the baseline\nwhile it was 4 years for the other grading aspects.\nRegarding the KL grading system, we grouped KL- 0 and\nKL-1 into the same class as they are clinically similar, and\nadded TKR knees as the fifth class. As a result, there were\n5 classes in KL, and there were 4 severity levels in each\nof the OARSI sub-systems. Following [15], [22], we utilized\nage, sex, body mass index (BMI), history of injury, history of\nsurgery, and total Western Ontario and McMaster Universities\nArthritis Index (WOMAC) as clinical variables. We quantized\nThis article has been accepted for publication in IEEE Transactions on Medical Imaging. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMI.2023.3312524\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nH. H. NGUYEN et al. 7\nTABLE I\nDATASET STATISTICS . SUBJECTS ARE PATIENT KNEE JOINTS , AND\nPATIENT BRAINS FOR OAI, AND ADNI, RESPECTIVELY .\nDataset Task # subjects\nOAI OA structural prognosis 8,953\nADNI AD clinical status prognosis 1,307\nthe continuous variables, and presented each of them by a\n4-element one-hot vector depending on the relative position\nof its value in the interval created by the minimum and the\nmaximum.\nFor clinical relevance, we did not perform knee OA prog-\nnosis predictions on knees that underwent TKR or were\ndiagnosed with the highest grade in any OARSI sub-system.\nIn addition, we ignored one single entry whose pair of knees\nwere improperly localized from its lateral radiograph by the\nBoneFinder tool. To have more training samples, we generated\nmultiple entries from the longitudinal record of each partici-\npant by considering imaging and non-imaging data at different\nfollow-up visits (except for the last one) as additional inputs.\n2) AD clinical status prognosis prediction:We applied our\nframework to forecast the Alzheimer’s disease (AD) clinical\nstatus from multi-modal data on the Alzheimer’s Disease\nNeuroimaging Initiative (ADNI) cohort, which is available\nat https://ida.loni.usc.edu. The recruitment was\ndone at 57 sites around America and Canada, and there were\n2, 577 male and female participants from 55 to 90 enrolled in\nthe cohort. The participants underwent a series of tests such as\nclinical evaluation, neuropsychological tests, genetic testing,\nlumbar puncture, MRI, and PET imaging at a baseline and\nfollow-up visits at 1, 2, and 4-year periods.\nIn this study, we used raw FDG-PET scans, MRI measures,\ncognitive tests, clinical history, and risk factors as predictor\nvariables. The raw FDG-PET scans were pre-processed by the\ndataset owner, and were then standardized to voxel dimensions\nof 160×160×160 (1.5×1.5×1.5mm3 voxel spacing) using\nthe NiBabel library [44]. To be in line with the OAI dataset,\nwe applied the same technique to convert scalar inputs to one-\nhot encoding vectors with a length of 4. In querying subjects,\nwhile we only selected entries whose raw FDG-PET scans\nwere available, the other input variables were allowed to be\nmissing.\nOur objective was to forecast the AD clinical statuses of\nparticipants’ brains – cognitively normal (CN), mild cognitive\nimpairment (MCI) or probable AD – in the next 4 years.\nSince the amount of the queried data was substantially limited\n(see Table I), we sampled entries from follow-up examinations\nto increase the amount of training data, and performed 10-fold\ncross-validation on this task.\nB. Experimental Setup\n1) Implementation details: We trained and evaluated our\nmethod and the reference approaches using V100 Nvidia\nGPUs. Each experimental setting was performed on a single\nGPU with 12GB. We implemented all the methods using\nthe PyTorch framework [45], and trained each of them with\nTABLE II\nINPUT VARIABLES FOR FORECASTING KNEE OA SEVERITY GRADES .\nGroup Variable name Data type\nRaw imaging Knee X-ray 2D\nClinical Variables Age Numerical\nWOMAC Numerical\nSex Categorical\nInjury Categorical\nSurgery Categorical\nBMI Numerical\nTABLE III\nINPUT VARIABLES FOR FORECASTING CLINICAL STATUSES OF\nALZHEIMER ’S DISEASE . SEE SEC. IV-E.2 FOR ACRONYMS .\nGroup Variable name Data type\nRaw imaging FDG-PET 3D\nMRI measures Hippocampus Numerical\nWhole brain Numerical\nEntorhinal Numerical\nFusiform gyrus Numerical\nMid. temp. gyrus Numerical\nIntracranial vol. Numerical\nClinical variables Sex Categorical\nMarriage Categorical\nRace Categorical\nEthnicity Categorical\nEducation Numerical\nCognitive tests CDRSB Numerical\nADAS11 Numerical\nMMSE Numerical\nRA VLT 1D\nCerebrospinal fluid A-Beta Numerical\nTau Numerical\nPtau Numerical\nMoca Numerical\nEcog 1D\nRisk factors Apolipoprotein E4 Numerical\nAge Numerical\nthe same set of configurations and hyperparameters. For each\nproblem, we used the Adam optimizer [46]. The learning rates\nof 1e−4 and 1e−5 were set for the OA and AD-related tasks,\nrespectively.\nTo extract visual representations of 2D images, we utilized\nthe ResNet18 architecture [47] whose weights were pretrained\non the ImageNet dataset [48]. We used a batch size of 128 for\nthe knee OA experiments. Regarding 3D images, we chose\nthe 3D-ShuffleNet2 architecture because it was well-balanced\nbetween efficiency and performance as shown in [49], which\nallowed us to train each model with a batch size of 36 on\na single consumer-level GPU. We utilized 3D-ShuffleNet 2’s\nweights previously pretrained on the Kinetics-600 dataset [50].\nMoreover, we used a common feature extraction architecture\nwith a linear layer, a ReLU activation, and a layer normal-\nization [36] for all scalar numerical and categorical inputs.\nWe provide the detailed description of the input variables\nin Tables II and III.\n2) Baselines: For fair comparisons, our baselines were\nmodels that had the same feature extraction modules for\nmulti-modal data, as described in Section IV-B.1, but uti-\nlized different architectures to perform discrete time series\nforecasting. As such, we compared our method to baselines\nwith the forecasting module using fully-connected network\nThis article has been accepted for publication in IEEE Transactions on Medical Imaging. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMI.2023.3312524\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n8 IEEE TRANSACTIONS ON MEDICAL IMAGING\nTABLE IV\nHYPERPARAMETER AND MODEL SELECTION BASED ON CV\nPERFORMANCES ON THE KL- BASED KNEE OA PROGNOSIS\nPREDICTION TASK . BA∗ INDICATES THE AVERAGES OF BAS OF THE\nTARGETS AT THE BASELINE AND THE FIRST 4 YEARS .\nDepth # of [CLS] FFN 0:T λ Image rep. BA ∗ (%)\n2\n9 Separate 0.5 Average pool\n59.2\n4 59.4\n6 59.2\n4\n1 Common\n0.5 Average pool\n58.3\n9 Common 59.8\n1 Separate 59.7\n9 Separate 59.4\n4 9 Common\n0.0\nAverage pool\n59.1\n0.25 58.9\n0.5 59.8\n0.75 56.1\n1.0 56.7\n4 9 Common 0.5 Average pool 59.8\n[CLS] head 58.7\n(FCN), GRU [51], LSTM [52], multi-modal transformer\n(MMTF) [31], Reformer [53], Informer [54], Autoformer [55],\nor CLIMAT [22]. While FCN, MMTF, Reformer, Informer,\nAutoformer, and CLIMAT are parallel models, GRU and\nLSTM are sequential approaches. Among the transformer-\nbased methods both versions of CLIMAT have a modular\nstructure of transformers rather than using a flat structure as\nin MMTF, Reformer, Informer, and Autoformer.\n3) Metrics: As data from both OAI and ADNI were imbal-\nanced, balanced accuracy (BA) [56] was a must metric in our\nexperiments. As there were only 3 classes in the AD clinical\nstatus prognosis prediction task, we also utilized the one-vs-\none multi-class area under the ROC Curve (mAUC-ROC) [57]\nas another metric. To quantitatively measure calibration, we\nused expected calibration error (ECE) [58], [35]. We reported\nmeans and standard errors of each metric computed over 5\nruns with different random seeds.\nTo perform analyses of the statistical significance of our\nresults, we utilized the two-sided Wilcoxon signed-rank test to\nvalidate the advantage of our method compared to each base-\nline [59]. We equally split the test set into 20 subsets without\noverlapping patients. For such a subset, we computed metrics\naveraged over 5 random seeds per method. The statistical\ntesting was done patient-wise by comparing our method with\nevery baseline individually. In the case of the OAI dataset, for\nall patients, we did two rounds of hypothesis testing: one for\nthe left and one for the right knee, respectively. Subsequently,\nwe applied the Bonferroni correction to adjust the significance\nthresholds for multiple comparisons ( p = 0 .025 due to two\nknees per patient) [60].\nTABLE V\nEFFECT OF THE CONSISTENCY TERM ON PERFORMANCE AND\nCALIBRATION (K- FOLD CROSS -VALIDATION ). R EPORTED RESULTS ARE\nAVERAGES OF BAS AND ECE S OVER THE FIRST 4 YEARS .\nGrading JSL JSM AD\nλ BA ECE BA ECE BA ECE\nWithout Lcons 0 63.2 1.4 64.7 8.8 86.8 7.5\nWith Lcons\n0.25 63.1 1.5 64.5 8.7 86.8 8.3\n0.50 64.9 1.8 65.3 9.6 87.3 7.9\n0.75 64.8 1.8 65.1 9.5 87.3 8.1\n1 64.6 1.7 65.2 9.5 86.7 9.2\n5 10 15\nECE (%, )\n57\n58\n59\n60BA (%, )\nFL\nAFL\nCE\nMTL\nCLUB\n(a) Joint space narrow-\ning\n5 10\nECE (%, )\n53.0\n53.5\n54.0\n(b) Osteophyte in femur\n5 10\nECE (%, )\n53.8\n54.0\n54.2\n54.4\n54.6\n (c) Osteophyte in tibia\nFig. 6. Performance and calibration comparisons between CLUB and\nother baselines. All the measures are on the medial side. The losses\ncan be categorized into groups: (1) FL and AFL, and (2) CE, MTL, and\nCLUB, which are based on cross-entropy and focal loss, respectively.\nC. Ablation studies\n1) Overview: We conducted a thorough ablation study to\ninvestigate the effects of different components in our CLI-\nMATv2 architecture on the OAI dataset. The empirical results\nare presented in Table IV and summarized in the following\nsubsections.\n2) Effect of the transformer P’s depth:Firstly, we searched\nfor an optimal depth of the transformer P. The results show\nthat the transformer P with a depth of 4 provides the best\nperformance, yielding 0.2 % gain in averaged BA compared\nto depths of 2 and 4. The average BA (over 4 years) indicates a\nsubstantial boost in performance. We, therefore, use the depth\nof 4 for the transformer P in the sequel.\n3) Effect of the number of [CLS] embeddings and FFNs in\nthe transformer P: Then, we simultaneously validated two\ncomponents: using single or multiple [CLS] embeddings, and\nusing common or separate FFN in the transformer P. Of 4\ncombinations of settings, the quantitative results suggest that\nthe transformer should have 9 individual [CLS] embeddings,\neach of which corresponds to an output head, and merely use\none common FFN to make predictions at different time points.\n4) Effect of the consistency term:To validate the necessity\nof the Lcons term, we conducted an experiment on a set\nof λ values {0, 0.25, 0.5, 0.75, 1}. The empirical evidence\nin Table IV shows that a λ of 0.5 resulted in the best\nperformance, which was 0.7% higher than the setting without\nLcons. We further validated the effects of the consistency\nterm on other knee OA grading criteria as well as the AD\nstatus forecasting task. The empirical results in Table V\nconsistently demonstrate that the term Lcons has a positive\nimpact on performance, albeit with the trade-off of calibration.\nThis article has been accepted for publication in IEEE Transactions on Medical Imaging. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMI.2023.3312524\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nH. H. NGUYEN et al. 9\nyear\n50\n52\n54\n56\n58\n60\nba\n*\n**\n**\n**\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n1 2 3 4 6 8\nYears from baseline\n28\n30\n32\n34\nba\nFCN\nGRU\nLSTM\nMMTF\nReformer\nInformer\nAutoformer\nCLIMATv1\nCLIMATv2\nBA (%)\n(a) Kellgren-Lawrence (KL)\nyear\n60\n65\n70\nba\n*\n*\n*\n*\n*\n*\n1 2 3 4 6 8\nYears from baseline\n35\n40\n45\n50\nba\nBA (%) (b) Lateral joint space (JSL)\nyear\n55\n60\n65\n70\nba\n*\n*\n*\n*\n*\n*\n**\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n1 2 3 4 6 8\nYears from baseline\n34\n36\n38\n40\nba\nBA (%) (c) Medial joint space (JSM)\n1 2 3 4\nYears from baseline\n47\n48\n49\n50\n51\n52\n53\n54BA (%)\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n(d) Lateral oste. in femur (OSFL)\n1 2 3 4\nYears from baseline\n46\n48\n50\n52\n54\n56BA (%)\n*\n*\n*\n*\n* (e) Medial oste. in femur (OSFM)\n1 2 3 4\nYears from baseline\n48\n50\n52\n54\n56\n58\n60BA (%)\n**\n**\n**\n**\n**\n**\n*\n*\n*\n*\n* (f) Lateral oste. in tibia (OSTL)\n1 2 3 4\nYears from baseline\n44\n46\n48\n50\n52\n54\n56\n58BA (%)\n*\n**\n**\n**\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n* (g) Medial oste. in tibia (OSTM)\nFig. 7. Performance comparisons between our CLIMAT models and other baselines on the knee osteoarthritis prognosis task via\ndifferent types of grading (means and standard errors over5 random seeds). ∗ and ∗∗ indicate the statistically significant differences\nbetween CLIMATv2 compared to each baseline via Wilcoxon signed-rank tests ( p <0.05 and p <0.001, respectively). As the\nstatistical tests were conducted on both knees, p-value thresholds were adjusted to 0.025 and 0.0005, respectively.\nKL\nJSL\nJSM\nOSFL\nOSFM\nOSTL\nOSTM\n2\n4\n6\n8\n10\n(a) 4 years\nLSTM\nInformer\nCLIMATv1\nCLIMATv2\nKL\nJSL\nJSM\n1\n2\n3\n4\n5\n6\n(b) 8 years\nFig. 8. Calibration comparisons on the knee OA prognosis predictions.\n(a) Averaged ECEs over the first 4 years. (b) Averaged ECEs over 8\nyears.\nA consistency coefficient λ of 0.5 is the most optimal setting\nin terms of performance across the tasks. Specifically, we\nobserved BA gains of 1.7%, 0.6%, and 0.5% with trade-\noff ECEs of 0.4%, 0.8%, and 0.4% for JSL, JSM, and AD,\nrespectively.\n5) Average pooling for image representation:In contrast to\nthe previous version, we adopted a conventional approach used\nin prior studies [38], [39], [40], which involves performing an\naverage pooling over the output sequence of the Radiologist\nblock to constitute an imaging feature vector for diagnosis\nprediction ˆyR\n0 . According to Table IV, such an approach results\nin a gain of 1.1% BA compared to the baseline, which solely\nutilized the first vector of the sequence generated by the block\nR.\n6) Multimodal channel-wise concatenation: We conducted\nan ablation study on the combination of multimodal embed-\ndings. As such, we compared our channel-wise approach to\na sequence-wise baseline that simply concatenates imaging\nembeddings and a projected version of non-imaging ones. For\nthe baseline, we utilized a linear projection layer to ensure that\nimaging and non-imaging embeddings are in the same CX-\ndimensional space. We reported the K-fold cross-validation\nresults in Table VI. On the knee OA-related tasks, our ap-\nproach tends to have positive benefits on both performance\nand calibration. Specifically, the performance gains were2.3%,\n2.0%, and 0.5% for KL, JSL, and JSM, respectively. Except\nfor JSL with an increase of 0.1% ECE, the approach results\nin calibration improvements of 2.5% and 3.2 for KL and\nJSM, respectively. On the AD-related task, the channel-wise\napproach leads to improvements of 1.1% BA and 0.2% ECE.\n7) Effectiveness of CLUB loss:We compared the CLUB loss\nto CE itself, multi-task loss (MTL) [41], focal loss (FL) [61],\nand adaptive focal loss (AFL) [62]. Whereas the first two\nbaselines and our loss are based on CE loss, the remaining\nones are related to FL. In Figure 6, we graphically visualize\nthe trade-off between performance and calibration, in which\nthe best in both aspects are expected to locate close to the\ntop-left corners. We observe that our model trained with FL-\nrelated losses was substantially worse calibrated compared to\nthe settings with any CE-based loss. Among the losses based\non CE, the proposed CLUB helped our model to achieve the\nbest ECEs in all three OA grading systems with insubstantial\ndrops in performance.\nD. Performance and Calibration Comparisons to\nCompetitive Baselines\n1) Knee OA structural prognosis prediction: In Figures 7\nand 8, we graphically present comparisons between both\nversions of CLIMAT and the baselines in the 7 different knee\nOA grading scales.\nThis article has been accepted for publication in IEEE Transactions on Medical Imaging. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMI.2023.3312524\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n10 IEEE TRANSACTIONS ON MEDICAL IMAGING\nTABLE VI\nABLATION STUDY ON IMAGING AND NON -IMAGING COMBINATION WITH\nK-FOLD CROSS -VALIDATION (K = 5AND K = 10FOR OAI AND\nADNI, RESPECTIVELY ). C HANNEL -WISE APPROACH (OURS ) IS\nCOMPARED TO THE SEQUENCE -WISE APPROACH , CONCATENATING\nIMAGING EMBEDDINGS PRODUCED BY THE BLOCK R WITH PROJECTED\nNON -IMAGING EMBEDDINGS OUTPUTTED BY THE BLOCK C. REPORTED\nRESULTS ARE AVERAGED BAS AND ECE S OVER THE FIRST 4 YEARS .\nGrading Setting BA (%, ↑) ECE (%, ↓)\nKL Channel-wise 59.8 14.7\nSequence-wise 57.5 17.2\nJSL Channel-wise 64.9 1.8\nSequence-wise 62.9 1.7\nJSM Channel-wise 65.3 9.6\nSequence-wise 64.7 12.8\nAD Channel-wise 87.3 7.9\nSequence-wise 86.2 8.1\nIn general, both versions of CLIMAT outperformed the\nother baselines in forecasting the knee OA progression within\nthe first 4 years across the knee OA grading systems. That\nis consistent with the observation of [22] in KL. We observe\nthat LSTM is the most competitive baseline across the grading\nsystems. Compared to LSTM, on average of the first 4 years,\nour model achieved 1.3%, 1.7%, 1.9%, 2.3%, 0.7%, 4.4%,\nand 2.4% higher BAs while having 1.9%, 0.1%, 0.02%,\n6.5%, 3.6%, 1.4%, and 4.1% lower ECEs in KL, JSL, JSM,\nOSFL, OSFM, OSTL, and OSTM, respectively. On average,\nInformer was the most competitive transformer-based baseline.\nCompared to Informer, CLIMATv2 achieved BA improve-\nments of 0.4%, 0.3%, 1.6%, 0.6%, 0.4%, 1.2%, and 1.8% in\nKL, JSL, JSM, OSFL, OSFM, OSTL, and OSTM, respectively.\nExcept for KL and OSFM, our model had lower ECEs with\ndifferences of 0.3%, 0.1%, 1.5%, 1.2%, and 0.4% in JSL,\nJSM, OSFL, OSTL, and OSTM, respectively. Moreover, in\ncomparison to CLIMATv1 [22], on average for the first 4\nyears, the newer version performed better in KL, JSM, OSFL,\nand OSTL with BA improvements of 0.1%, 0.4%, 0.5%,\nand 0.2%, respectively, whereas it reached 0.1%, 0.5%, and\n0.3% lower BAs in JSL, OSFM, and OSTM, respectively.\nRegarding the calibration aspect, CLIMATv2 obtained lower\nECEs compared to CLIMATv1 in JSL, JSM, OSFM, and\nOSTM with differences of 0.1%, 0.3%, 0.2%, and 0.4%,\nrespectively.\n2) Alzheimer’s disease status prognosis prediction:We re-\nported the quantitative results in Table VII. Regarding perfor-\nmance, both the CLIMAT methods achieved the best perfor-\nmances across the prediction targets, in which CLIMATv2 was\ntop-1 at the first 2 years in both BA and mROCAUC. Com-\npared to the transformer-based baseline MMTF, our method\noutperformed by 2.2%, 1.8%, and 2.2% BAs at years 1, 2, and\n4, respectively. In calibration, CLIMATv2 yielded substantially\nlower ECEs than all the references at every prediction target.\nThat observation was supported by the statistical test results\nin Table VII.\nTABLE VII\nCV PERFORMANCE AND CALIBRATION COMPARISONS ON THE ADNI\nDATA (MEAN AND STANDARD ERRORS OVER 5 RANDOM SEEDS ). T HE\nBEST PERFORMANCES WITH AND WITHOUT SUBSTANTIAL DIFFERENCES\nARE INDICATED BY BOLD AND UNDERLINED VALUES , RESPECTIVELY .\nTHE SUBSTANTIAL IMPROVEMENT IS DETERMINED BY WHETHER THE\nBEST PERFORMANCE OVERLAPS WITH ANY OTHER METHOD ’S. ∗ AND\n∗∗ INDICATE THE STATISTICALLY SIGNIFICANT DIFFERENCES BETWEEN\nCLIMAT V2 VS. EACH BASELINE VIA WILCOXON SIGNED -RANK TESTS\n(P < 0.05 AND P < 0.001, RESPECTIVELY ).\nYear Method BA (%,↑) mROCAUC(%,↑) ECE(%,↓)\n1\nFCN 87.6 ±0.2∗∗ 96.6±0.1∗∗ 9.5±0.3∗∗\nGRU 87.1 ±0.3∗∗ 96.6±0.1∗∗ 8.6±0.4∗\nLSTM 87.9 ±0.2∗∗ 96.8±0.1∗∗ 8.7±0.3∗∗\nMMTF 88.2 ±1.0 96.4 ±0.8∗ 23.3±0.4∗∗\nReformer 80.8 ±1.0∗∗ 93.6±0.7∗∗ 9.6±0.3∗∗\nInformer 78.4 ±1.5∗∗ 93.1±0.6∗∗ 10.0±0.4∗∗\nAutoformer 85.2 ±0.3∗∗ 96.0±0.1∗∗ 7.8±0.2∗\nCLIMATv1 90.1±0.1 97.6 ±0.1∗ 6.7±0.2\nCLIMATv2 90.4±0.1 98.0 ±0.1 6.5 ±0.1\n2\nFCN 85.5 ±0.2∗ 95.7±0.1∗ 9.2±0.2∗\nGRU 85.1 ±0.2∗ 95.6±0.1∗ 9.7±0.2∗\nLSTM 85.7 ±0.3∗ 95.7±0.2∗ 9.3±0.7\nMMTF 85.4 ±0.6 95.3 ±0.7 22.3 ±0.4∗∗\nReformer 78.8 ±0.9∗∗ 92.5±0.6∗∗ 9.2±0.2∗\nInformer 78.1 ±1.3∗∗ 92.9±0.4∗∗ 8.6±0.1\nAutoformer 83.7 ±0.3∗∗ 95.2±0.1∗∗ 8.6±0.2\nCLIMATv1 87.2±0.0 96.6 ±0.0 9.1 ±0.2∗\nCLIMATv2 87.2±0.2 96.6±0.1 7.9±0.3\n4\nFCN 80.7 ±0.3∗ 93.7±0.1 9.6 ±0.3∗\nGRU 81.7 ±0.1 93.8 ±0.1 12.6 ±0.4∗∗\nLSTM 81.0 ±0.7∗ 93.5±0.2 12.4 ±0.6∗∗\nMMTF 80.6 ±0.7 93.0 ±0.6 17.9 ±0.3∗∗\nReformer 76.7 ±0.9∗∗ 90.5±0.5∗∗ 11.1±0.5∗\nInformer 71.6 ±0.9∗∗ 87.4±0.3∗∗ 12.6±0.3∗∗\nAutoformer 80.5 ±0.3∗∗ 93.3±0.1∗∗ 9.5±0.2\nCLIMATv1 83.0±0.2 94.5±0.1 9.6±0.3∗\nCLIMATv2 82.8±0.1 94.2 ±0.1 9.2±0.2\nE. Attention maps over multiple modalities\nThe self-attention mechanism of the transformers in CLI-\nMATv2 allowed us to visualize attention maps over imag-\ning and non-imaging modalities when our model made\na prediction at a specific target. Specifically, we used\nSoftmax\n\u0000\nQLK⊺\nL/√dk\n\u0001\n, where QL, KL are query and key\nmatrices of the last layer L, respectively, and dk is the feature\ndimension of the key matrix, as attention maps [25]. While\nwe utilized the softmax output corresponding to ¯h0\nC in the\ntransformer F for clinical variables, we took the softmax output\nin computing ¯ht\nP with t = 0, . . . , Tin the transformer P to\nvisualize attention maps on imaging modalities. Here, we set\nt = 1, corresponding to the forecast of a disease severity 1\nyear from the baseline.\n1) Knee OA structural prognosis:In Figure 9, we visualized\nattention maps over different input modalities across 7 grading\ncriteria. As such, in Figure 9a, we displayed a healthy knee at\nthe baseline overlaid by 7 corresponding saliency maps. For\ndifferentiation, we also provided colored ellipses. Figure 9b\nThis article has been accepted for publication in IEEE Transactions on Medical Imaging. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMI.2023.3312524\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nH. H. NGUYEN et al. 11\nshows the heatmap over the 6 clinical variables on each\ngrading criterion. Values on each row sum up to 1. In this\nparticular case, we observe that the model has paid the most\nattention to the intercondylar notch, together with BMI and\nWOMAC [63].\n2) AD clinical status prognosis:As imaging data consisted\nof 3D FDG-PET scans as well as the other imaging measure-\nments, we had to separate them into Figures 10a and 10b. We\ncan observe that an attention sphere locates around the poste-\nrior cingulate cortex, the inferior frontal gyrus, and the middle\ngyrus [64]. Figure 10b shows accumulated attention weights\ncorresponding to the FDG-PET feature vectors alongside ones\nof the other imaging measurements. The reason that imaging\nvariables were assigned a substantially higher importance is\nthat the number of the 3D visual embeddings was dominant\ncompared to the others (i.e. 125 versus 6). In Figure 10c, high\nattention can be observed on the percent forgetting score of the\nRey Auditory Verbal Learning Test (RA VLT), RA VLT imme-\ndiate, the AD assessment score 11-item (ADAS11), Clinical\nDementia Rating Scale–Sum of Boxes (CDRSB), Mini-Mental\nState Exam (MMSE), and Functional Activities Questionnaire\n(FAQ).\nV. C ONCLUSIONS\nIn this paper, we proposed a novel general-purpose\ntransformer-based method to forecast the trajectory of a dis-\nease’s stage from multimodel data. We applied our method to\ntwo real-world applications, that are related to OA and AD.\nOur framework provides tools to integrate multi-modal data\nand has interpretation capabilities through self-attention.\nIn comparison with the prior version, CLIMATv2 has two\nprimary upgrades. First, we have eliminated the assumption\nof independence between non-imaging data m0 and diagnostic\npredictions y0 used in CLIMATv1 [22] since it does not hold\nnot only in OA and AD, but also in other diseases. Specifically,\nLiu et al. [34] provided empirical evidence of the benefit of\nthe inclusion of non-imaging data in the knee OA grading\ntask. The study conducted by Bird et al. [65] indicated a link\nbetween human genes and AD while Li et al. [66] showed that\na blood test can detect the existence of amyloid-beta plaques in\nthe human brain, which is strongly associated with AD status.\nSecond, we have proposed the CLUB loss, which allowed us\nto optimize for both performance and calibration.\nThere are some limitations in this study, which are worth\nmentioning. First, we used common DL architectures as\nimaging and non-imaging feature extractors. While such a\nstandardized procedure resulted in fair comparisons, better\nresults could have been obtained with e.g. Neural Architecture\nSearch methods [67]. Furthermore, a wider range of DL mod-\nules could have been considered, but this could substantially\nincrease the use of computing resources. Specifically, to obtain\nresults in this work, it required roughly 400 GPU hours for\nexperiments in Table VII and 525 GPU hours in Figure 7 for\nevery method, respectively.\nThe second limitation of the present study, is that attention\nmaps produced by transformers act as human-friendly signals\nof our model, and should be carefully used in practice with\nexpert knowledge in the domain. Transformers may highlight\nareas not associated with the body part, which can be seen\nin Figure 11 as well as in other studies [68], [69], [70].\nLastly, we primarily utilized the transformer proposed\nby [27]. More efficient and advanced transformers such\nas [53], [54], [55], [71] could be further investigated to\nintegrate into the framework.\nTo conclude, to our knowledge, this is not only the first\nstudy in the realm of OA, but also the first work on AD\nclinical status prognosis prediction from the multi-modal\nsetup that includes raw 3D scans and scalar variables. The\ndeveloped method can be of interest to other fields, where\nforecasting of calibrated disease trajectory is of interest. An\nimplementation of our method is made publicly available at\nhttps://github.com/Oulu-IMEDS/CLIMATv2.\nVI. A CKNOWLEDGEMENT\nThe OAI is a public-private partnership comprised of\nfive contracts (N01- AR-2-2258; N01-AR-2-2259; N01-AR-\n2- 2260; N01-AR-2-2261; N01-AR-2-2262) funded by the\nNational Institutes of Health, a branch of the Department\nof Health and Human Services, and conducted by the OAI\nStudy Investigators. Private funding partners include Merck\nResearch Laboratories; Novartis Pharmaceuticals Corporation,\nGlaxoSmithKline; and Pfizer, Inc. Private sector funding for\nthe OAI is managed by the Foundation for the National\nInstitutes of Health.\nData collection and sharing for this project was funded\nby the Alzheimer’s Disease Neuroimaging Initiative (ADNI)\n(National Institutes of Health Grant U01 AG024904) and DOD\nADNI (Department of Defense award number W81XWH-12-\n2-0012). ADNI is funded by the National Institute on Aging,\nthe National Institute of Biomedical Imaging and Bioengineer-\ning, and through generous contributions from the following:\nAbbVie, Alzheimer’s Association; Alzheimer’s Drug Discov-\nery Foundation; Araclon Biotech; BioClinica, Inc.; Biogen;\nBristol-Myers Squibb Company; CereSpir, Inc.; Cogstate; Ei-\nsai Inc.; Elan Pharmaceuticals, Inc.; Eli Lilly and Company;\nEuroImmun; F. Hoffmann-La Roche Ltd and its affiliated\ncompany Genentech, Inc.; Fujirebio; GE Healthcare; IXICO\nLtd.; Janssen Alzheimer Immunotherapy Research & Devel-\nopment, LLC.; Johnson & Johnson Pharmaceutical Research\n& Development LLC.; Lumosity; Lundbeck; Merck & Co.,\nInc.; Meso Scale Diagnostics, LLC.; NeuroRx Research; Neu-\nrotrack Technologies; Novartis Pharmaceuticals Corporation;\nPfizer Inc.; Piramal Imaging; Servier; Takeda Pharmaceu-\ntical Company; and Transition Therapeutics. The Canadian\nInstitutes of Health Research is providing funds to support\nADNI clinical sites in Canada. Private sector contributions\nare facilitated by the Foundation for the National Institutes\nof Health ( www.fnih.org). The grantee organization is\nthe Northern California Institute for Research and Education,\nand the study is coordinated by the Alzheimer’s Therapeutic\nResearch Institute at the University of Southern California.\nADNI data are disseminated by the Laboratory for Neuro\nImaging at the University of Southern California.\nThe authors wish to acknowledge CSC – IT Center for\nScience, Finland, for generous computational resources.\nThis article has been accepted for publication in IEEE Transactions on Medical Imaging. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMI.2023.3312524\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n12 IEEE TRANSACTIONS ON MEDICAL IMAGING\nKL\nJSL\nJSM\nOSFL\nOSFM\nOSTL\nOSTM\n(a) A knee with attention maps\nAGE SEX INJ SURG BMI\nWOMAC\nKL\nJSL\nJSM\nOSFL\nOSFM\nOSTL\nOSTM\n0.1\n0.2\n0.3\n0.4\n0.5 (b) Contributions of the variables\nFig. 9. An example of progression from a healthy knee at base-\nline to early osteoarthritis. Our model identified the changes in\nthe intercondylar notch, sex, and symptomatic status.\n(a) Attention maps on an axial\nFDG-PET slice\n0 1\nFDG-PET\nICV\nHippocampus\nWholeBrain\nEntorhinal\nFusiform\nMidTemp\n61.0 61.2\n%\n(b) FDG-PET and imaging variables\nAGE\nPTGENDER\nPTMARRY\nPTRACCAT\nPTETHCAT\nCDRSB\nADAS11\nMMSE\nRAVLT_immediate\nABETA\nTAU\nPTAU\nAPOE4\nRAVLT_learning\nRAVLT_forgetting\nRAVLT_perc_forgetting\nMOCA\nFAQ\nPTEDUCAT\nEcogPtMem\nEcogPtLang\nEcogPtVisspat\nEcogPtPlan\nEcogPtOrgan\nEcogPtDivatt\nEcogPtTotal\nEcogSPMem\nEcogSPLang\nEcogSPVisspat\nEcogSPPlan\nEcogSPOrgan\nEcogSPDivatt\nEcogSPTotal\n0\n5\n10%\n(c) Importances of other variables\nFig. 10. Interpretability of our method’s prediction on a selective\nsample from the ADNI dataset.\nWe would like to acknowledge the strategic funding of\nthe University of Oulu, Infotech Oulu Focus Institute, and\nSigrid Juselius Foundation, Finland. The publication was also\nsupported by funding from the Academy of Finland (Profi6\n336449 funding program). We also acknowledge funding from\nthe Alfred Kordelinin foundation (220211).\nWe acknowledge funding from the Flemish Government\nunder the “Onderzoeksprogramma Artifici¨ele Intelligentie (AI)\nVlaanderen” programme.\nDr. Claudia Lindner is acknowledged for providing\nBoneFinder. Phuoc Dat Nguyen is acknowledged for discus-\nsions about transformers.\nREFERENCES\n[1] R. J. Desai, S. V . Wang, M. Vaduganathan, T. Evers, and S. Schneeweiss,\n“Comparison of machine learning methods with traditional models\nfor use of administrative claims with electronic medical records to\npredict heart failure outcomes,” JAMA network open , vol. 3, no. 1, pp.\ne1 918 962–e1 918 962, 2020.\n[2] A. Boutet, R. Madhavan, G. J. Elias, S. E. Joel, R. Gramer, M. Ranjan,\nV . Paramanandam, D. Xu, J. Germann, A. Loh et al. , “Predicting\noptimal deep brain stimulation parameters for parkinson’s disease using\n(a) Cases with attention maps primarily overlapping with brain regions\n(b) Cases with attention maps overlapping with both brain and background\nregions\nFig. 11. Attention maps on axial FDG-PET slices. Three axial slices on\neach row belong to the same PET scan.\nfunctional mri and machine learning,” Nature communications, vol. 12,\nno. 1, pp. 1–13, 2021.\n[3] A. Cheerla and O. Gevaert, “Deep learning with multimodal representa-\ntion for pancancer prognosis prediction,” Bioinformatics, vol. 35, no. 14,\npp. i446–i454, 2019.\n[4] K. A. Tran, O. Kondrashova, A. Bradley, E. D. Williams, J. V . Pearson,\nand N. Waddell, “Deep learning in cancer diagnosis, prognosis and\ntreatment selection,” Genome Medicine, vol. 13, no. 1, pp. 1–17, 2021.\n[5] P. Milanez-Almeida, A. J. Martins, R. N. Germain, and J. S.\nTsang, “Cancer prognosis with shallow tumor rna sequencing,” Nature\nMedicine, vol. 26, no. 2, pp. 188–192, 2020.\n[6] X. Mei, H.-C. Lee, K.-y. Diao, M. Huang, B. Lin, C. Liu, Z. Xie, Y . Ma,\nP. M. Robson, M. Chung et al. , “Artificial intelligence–enabled rapid\ndiagnosis of patients with covid-19,” Nature medicine , vol. 26, no. 8,\npp. 1224–1228, 2020.\n[7] S. Glyn-Jones, A. Palmer, R. Agricola, A. Price, T. Vincent, H. Weinans,\nand A. Carr, “Osteoarthritis,” The Lancet, vol. 386, no. 9991, pp. 376–\n387, 2015.\n[8] “2021 alzheimer’s disease facts and figures,” Alzheimer’s & Dementia ,\nvol. 17, no. 3, pp. 327–406, 2021.\n[9] B. Heidari, “Knee osteoarthritis prevalence, risk factors, pathogenesis\nand features: Part i,” Caspian journal of internal medicine , vol. 2, no. 2,\np. 205, 2011.\n[10] L. S. Lee, P. K. Chan, W. C. Fung, V . W. K. Chan, C. H. Yan, and K. Y .\nChiu, “Imaging of knee osteoarthritis: A review of current evidence and\nclinical guidelines,” Musculoskeletal Care, vol. 19, no. 3, pp. 363–374,\n2021.\n[11] J. Kellgren and J. Lawrence, “Radiological assessment of osteo-\narthrosis,” Annals of the rheumatic diseases , vol. 16, no. 4, p. 494,\n1957.\n[12] R. D. Altman and G. Gold, “Atlas of individual radiographic features\nin osteoarthritis, revised,” Osteoarthritis and cartilage, vol. 15, pp. A1–\nA56, 2007.\nThis article has been accepted for publication in IEEE Transactions on Medical Imaging. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMI.2023.3312524\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nH. H. NGUYEN et al. 13\n[13] E. Rezus ¸, A. Burlui, A. Cardoneanu, L. A. Macovei, B. I. Tamba, and\nC. Rezus ¸, “From pathogenesis to therapy in knee osteoarthritis: bench-\nto-bedside,” International Journal of Molecular Sciences , vol. 22, no. 5,\np. 2697, 2021.\n[14] J. Rasmussen and H. Langerman, “Alzheimer’s disease–why we need\nearly diagnosis,” Degenerative neurological and neuromuscular disease,\nvol. 9, p. 123, 2019.\n[15] A. Tiulpin, S. Klein, S. M. Bierma-Zeinstra, J. Thevenot, E. Rahtu, J. van\nMeurs, E. H. Oei, and S. Saarakkala, “Multimodal machine learning-\nbased knee osteoarthritis progression prediction from plain radiographs\nand clinical data,” Scientific reports, vol. 9, no. 1, pp. 1–11, 2019.\n[16] P. Widera, P. M. Welsing, C. Ladel, J. Loughlin, F. P. Lafeber, F. P.\nDop, J. Larkin, H. Weinans, A. Mobasheri, and J. Bacardit, “Multi-\nclassifier prediction of knee osteoarthritis progression from incomplete\nimbalanced longitudinal data,” Scientific Reports, vol. 10, no. 1, pp. 1–\n15, 2020.\n[17] B. Guan, F. Liu, A. Haj-Mirzaian, S. Demehri, A. Samsonov, T. Neogi,\nA. Guermazi, and R. Kijowski, “Deep learning risk assessment models\nfor predicting progression of radiographic medial joint space loss over a\n48-month follow-up period,” Osteoarthritis and cartilage, vol. 28, no. 4,\npp. 428–437, 2020.\n[18] K. Leung, B. Zhang, J. Tan, Y . Shen, K. J. Geras, J. S. Babb, K. Cho,\nG. Chang, and C. M. Deniz, “Prediction of total knee replacement and\ndiagnosis of osteoarthritis by using deep learning on knee radiographs:\ndata from the osteoarthritis initiative,” Radiology, vol. 296, no. 3, pp.\n584–593, 2020.\n[19] A. A. Tolpadi, J. J. Lee, V . Pedoia, and S. Majumdar, “Deep learning\npredicts total knee replacement from magnetic resonance images,”\nScientific reports, vol. 10, no. 1, pp. 1–12, 2020.\n[20] W. Jung, A. W. Mulyadi, and H.-I. Suk, “Unified modeling of impu-\ntation, forecasting, and prediction for ad progression,” in International\nConference on Medical Image Computing and Computer-Assisted Inter-\nvention. Springer, 2019, pp. 168–176.\n[21] D. Lu, K. Popuri, G. W. Ding, R. Balachandar, and M. F. Beg,\n“Multimodal and multiscale deep neural networks for the early diagnosis\nof alzheimer’s disease using structural mr and fdg-pet images,” Scientific\nreports, vol. 8, no. 1, pp. 1–13, 2018.\n[22] H. H. Nguyen, S. Saarakkala, M. B. Blaschko, and A. Tiulpin, “Cli-\nmat: Clinically-inspired multi-agent transformers for knee osteoarthritis\ntrajectory forecasting,” in 2022 IEEE 19th International Symposium on\nBiomedical Imaging (ISBI) . IEEE, 2022, pp. 1–5.\n[23] M. M. Ghazi, M. Nielsen, A. Pai, M. J. Cardoso, M. Modat, S. Ourselin,\nL. Sørensen, A. D. N. Initiative et al. , “Training recurrent neural\nnetworks robust to incomplete data: application to alzheimer’s disease\nprogression modeling,” Medical image analysis , vol. 53, pp. 39–46,\n2019.\n[24] J. Albright, A. D. N. Initiative et al. , “Forecasting the progression of\nalzheimer’s disease using neural networks and a novel preprocessing\nalgorithm,” Alzheimer’s & Dementia: Translational Research & Clinical\nInterventions, vol. 5, pp. 483–491, 2019.\n[25] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances\nin neural information processing systems , 2017, pp. 5998–6008.\n[26] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\nof deep bidirectional transformers for language understanding,” arXiv\npreprint arXiv:1810.04805, 2018.\n[27] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\n“An image is worth 16x16 words: Transformers for image recognition\nat scale,” arXiv preprint arXiv:2010.11929 , vol. 1, 2020.\n[28] R. Girdhar, J. Carreira, C. Doersch, and A. Zisserman, “Video action\ntransformer network,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 2019, pp. 244–253.\n[29] A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lu ˇci´c, and C. Schmid,\n“Vivit: A video vision transformer,” arXiv preprint arXiv:2103.15691 ,\n2021.\n[30] A. Hassani, S. Walton, N. Shah, A. Abuduweili, J. Li, and H. Shi,\n“Escaping the big data paradigm with compact transformers,” arXiv\npreprint arXiv:2104.05704, 2021.\n[31] S. Hu, E. Fridgeirsson, G. van Wingen, and M. Welling, “Transformer-\nbased deep survival analysis,” in Survival Prediction-Algorithms, Chal-\nlenges and Applications . PMLR, 2021, pp. 132–148.\n[32] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\nG. Sastry, A. Askell, P. Mishkin, J. Clark et al., “Learning transferable\nvisual models from natural language supervision,” in International\nConference on Machine Learning . PMLR, 2021, pp. 8748–8763.\n[33] L. Jans, J. Bosmans, K. Verstraete, and R. Achten, “Optimizing commu-\nnication between the radiologist and the general practitioner,” JBR-BTR,\nvol. 96, no. 6, pp. 388–390, 2013.\n[34] L. Liu, J. Chang, P. Zhang, Q. Ma, H. Zhang, T. Sun, and H. Qiao,\n“A joint multi-modal learning method for early-stage knee osteoarthritis\ndisease classification,” Heliyon, vol. 9, no. 4, 2023.\n[35] C. Guo, G. Pleiss, Y . Sun, and K. Q. Weinberger, “On calibration\nof modern neural networks,” in International conference on machine\nlearning. PMLR, 2017, pp. 1321–1330.\n[36] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” arXiv\npreprint arXiv:1607.06450, 2016.\n[37] D. Hendrycks and K. Gimpel, “Gaussian error linear units (GELUs),”\n2020, arXiv:1606.08415.\n[38] X. Chu, Z. Tian, B. Zhang, X. Wang, X. Wei, H. Xia, and C. Shen, “Con-\nditional positional encodings for vision transformers,” arXiv preprint\narXiv:2102.10882, 2021.\n[39] Z. Pan, B. Zhuang, J. Liu, H. He, and J. Cai, “Scalable vision\ntransformers with hierarchical pooling,” in Proceedings of the IEEE/cvf\ninternational conference on computer vision , 2021, pp. 377–386.\n[40] N. Park and S. Kim, “How do vision transformers work?” arXiv preprint\narXiv:2202.06709, 2022.\n[41] A. Kendall, Y . Gal, and R. Cipolla, “Multi-task learning using uncer-\ntainty to weigh losses for scene geometry and semantics,” inProceedings\nof the IEEE conference on computer vision and pattern recognition ,\n2018, pp. 7482–7491.\n[42] A. Tiulpin and S. Saarakkala, “Automatic grading of individual knee\nosteoarthritis features in plain radiographs using deep convolutional\nneural networks,” Diagnostics, vol. 10, no. 11, p. 932, 2020.\n[43] C. Lindner, S. Thiagarajah, J. M. Wilkinson, G. A. Wallis, T. F.\nCootes, arcOGEN Consortium et al. , “Fully automatic segmentation\nof the proximal femur using random forest regression voting,” IEEE\ntransactions on medical imaging , vol. 32, no. 8, pp. 1462–1472, 2013.\n[44] M. Brett, C. J. Markiewicz, M. Hanke, M.-A. C ˆot´e, B. Cipollini,\nP. McCarthy, D. Jarecka, C. Cheng, Y . Halchenko, M. Cottaar et al. ,\n“nipy/nibabel: 3.2. 1,” Zenodo, 2020.\n[45] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al. , “Pytorch: An\nimperative style, high-performance deep learning library,” arXiv preprint\narXiv:1912.01703, 2019.\n[46] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\narXiv preprint arXiv:1412.6980 , 2014.\n[47] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proceedings of the IEEE conference on computer vision\nand pattern recognition , 2016, pp. 770–778.\n[48] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:\nA large-scale hierarchical image database,” in 2009 IEEE conference on\ncomputer vision and pattern recognition . Ieee, 2009, pp. 248–255.\n[49] O. Kopuklu, N. Kose, A. Gunduz, and G. Rigoll, “Resource efficient\n3d convolutional neural networks,” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision Workshops , 2019, pp. 0–\n0.\n[50] J. Carreira and A. Zisserman, “Quo vadis, action recognition? a new\nmodel and the kinetics dataset,” in proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , 2017, pp. 6299–6308.\n[51] K. Cho, B. Van Merri ¨enboer, D. Bahdanau, and Y . Bengio, “On the\nproperties of neural machine translation: Encoder-decoder approaches,”\narXiv preprint arXiv:1409.1259 , 2014.\n[52] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\ncomputation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[53] N. Kitaev, Ł. Kaiser, and A. Levskaya, “Reformer: The efficient trans-\nformer,” arXiv preprint arXiv:2001.04451 , 2020.\n[54] H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li, H. Xiong, and W. Zhang,\n“Informer: Beyond efficient transformer for long sequence time-series\nforecasting,” in Proceedings of the AAAI conference on artificial intel-\nligence, vol. 35, no. 12, 2021, pp. 11 106–11 115.\n[55] M. Chen, H. Peng, J. Fu, and H. Ling, “Autoformer: Searching\ntransformers for visual recognition,” in Proceedings of the IEEE/CVF\ninternational conference on computer vision , 2021, pp. 12 270–12 280.\n[56] K. H. Brodersen, C. S. Ong, K. E. Stephan, and J. M. Buhmann,\n“The balanced accuracy and its posterior distribution,” in 2010 20th\ninternational conference on pattern recognition. IEEE, 2010, pp. 3121–\n3124.\n[57] D. J. Hand and R. J. Till, “A simple generalisation of the area under the\nroc curve for multiple class classification problems,” Machine learning,\nvol. 45, no. 2, pp. 171–186, 2001.\nThis article has been accepted for publication in IEEE Transactions on Medical Imaging. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMI.2023.3312524\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n14 IEEE TRANSACTIONS ON MEDICAL IMAGING\n[58] M. P. Naeini, G. Cooper, and M. Hauskrecht, “Obtaining well calibrated\nprobabilities using bayesian binning,” in Twenty-Ninth AAAI Conference\non Artificial Intelligence , 2015.\n[59] F. Wilcoxon, “Individual comparisons by ranking methods,” in Break-\nthroughs in statistics . Springer, 1992, pp. 196–202.\n[60] O. J. Dunn, “Multiple comparisons among means,” Journal of the\nAmerican statistical association , vol. 56, no. 293, pp. 52–64, 1961.\n[61] T.-Y . Lin, P. Goyal, R. Girshick, K. He, and P. Doll ´ar, “Focal loss\nfor dense object detection,” in Proceedings of the IEEE international\nconference on computer vision , 2017, pp. 2980–2988.\n[62] J. Mukhoti, V . Kulharia, A. Sanyal, S. Golodetz, P. H. Torr, and P. K.\nDokania, “Calibrating deep neural networks using focal loss,” arXiv\npreprint arXiv:2002.09437, 2020.\n[63] H. O. Le ´on, C. E. R. Blanco, T. B. Guthrie, and O. J. N. Mart ´ınez,\n“Intercondylar notch stenosis in degenerative arthritis of the knee,”\nArthroscopy: The Journal of Arthroscopic & Related Surgery , vol. 21,\nno. 3, pp. 294–302, 2005.\n[64] B. Hallam, J. Chan, S. G. Costafreda, R. Bhome, and J. Huntley,\n“What are the neural correlates of meta-cognition and anosognosia\nin alzheimer’s disease? a systematic review,” Neurobiology of aging ,\nvol. 94, pp. 250–264, 2020.\n[65] T. D. Bird, “Genetic factors in alzheimer’s disease,” New England\nJournal of Medicine , vol. 352, no. 9, pp. 862–864, 2005.\n[66] Y . Li, S. E. Schindler, J. G. Bollinger, V . Ovod, K. G. Mawuenyega,\nM. W. Weiner, L. M. Shaw, C. L. Masters, C. J. Fowler, J. Q.\nTrojanowski et al., “Validation of plasma amyloid-β 42/40 for detecting\nalzheimer disease amyloid plaques,” Neurology, vol. 98, no. 7, pp. e688–\ne699, 2022.\n[67] T. Elsken, J. H. Metzen, and F. Hutter, “Neural architecture search: A\nsurvey,” The Journal of Machine Learning Research , vol. 20, no. 1, pp.\n1997–2017, 2019.\n[68] O. Petit, N. Thome, C. Rambour, L. Themyr, T. Collins, and L. Soler,\n“U-net transformer: Self and cross attention for medical image seg-\nmentation,” in International Workshop on Machine Learning in Medical\nImaging. Springer, 2021, pp. 267–276.\n[69] M. Odusami, R. Maskeli ¯unas, and R. Dama ˇseviˇcius, “An intelligent\nsystem for early recognition of alzheimer’s disease using neuroimaging,”\nSensors, vol. 22, no. 3, p. 740, 2022.\n[70] Y . Rao, G. Chen, J. Lu, and J. Zhou, “Counterfactual attention learning\nfor fine-grained visual categorization and re-identification,” in Proceed-\nings of the IEEE/CVF International Conference on Computer Vision ,\n2021, pp. 1025–1034.\n[71] X. Yang, H. Zhang, G. Qi, and J. Cai, “Causal attention for vision-\nlanguage tasks,” in Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition , 2021, pp. 9847–9857.\nThis article has been accepted for publication in IEEE Transactions on Medical Imaging. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TMI.2023.3312524\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/"
}