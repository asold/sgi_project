{
  "title": "Combining EfficientNet and Vision Transformers for Video Deepfake Detection",
  "url": "https://openalex.org/W3179508259",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4283542186",
      "name": "Davide Alessandro Coccomini",
      "affiliations": [
        "Istituto di Scienza e Tecnologie dell'Informazione \"Alessandro Faedo\"",
        "National Research Council"
      ]
    },
    {
      "id": "https://openalex.org/A2169874835",
      "name": "Nicola Messina",
      "affiliations": [
        "Istituto di Scienza e Tecnologie dell'Informazione \"Alessandro Faedo\"",
        "National Research Council"
      ]
    },
    {
      "id": "https://openalex.org/A2161798937",
      "name": "Claudio Gennaro",
      "affiliations": [
        "Istituto di Scienza e Tecnologie dell'Informazione \"Alessandro Faedo\"",
        "National Research Council"
      ]
    },
    {
      "id": "https://openalex.org/A2032466494",
      "name": "Fabrizio Falchi",
      "affiliations": [
        "National Research Council",
        "Istituto di Scienza e Tecnologie dell'Informazione \"Alessandro Faedo\""
      ]
    },
    {
      "id": "https://openalex.org/A4283542186",
      "name": "Davide Alessandro Coccomini",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2169874835",
      "name": "Nicola Messina",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2161798937",
      "name": "Claudio Gennaro",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2032466494",
      "name": "Fabrizio Falchi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2971927438",
    "https://openalex.org/W2995516027",
    "https://openalex.org/W3151130473",
    "https://openalex.org/W2883289607",
    "https://openalex.org/W2963767194",
    "https://openalex.org/W3086763124",
    "https://openalex.org/W3046357466",
    "https://openalex.org/W3122852338",
    "https://openalex.org/W6637568146",
    "https://openalex.org/W3036806226",
    "https://openalex.org/W6622575216",
    "https://openalex.org/W3034900344",
    "https://openalex.org/W3035574324",
    "https://openalex.org/W4206706211",
    "https://openalex.org/W3034713808",
    "https://openalex.org/W3021244424",
    "https://openalex.org/W2982128187",
    "https://openalex.org/W3118743894",
    "https://openalex.org/W3213100861",
    "https://openalex.org/W3160179442",
    "https://openalex.org/W3036576316",
    "https://openalex.org/W2982058372",
    "https://openalex.org/W3038930935",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W2963684180",
    "https://openalex.org/W2341528187",
    "https://openalex.org/W4287646898",
    "https://openalex.org/W3101998545",
    "https://openalex.org/W3020392129",
    "https://openalex.org/W2912336782",
    "https://openalex.org/W3173291851",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2999135330",
    "https://openalex.org/W3141468933",
    "https://openalex.org/W2951939904",
    "https://openalex.org/W2904573504",
    "https://openalex.org/W3174831884",
    "https://openalex.org/W3091401866",
    "https://openalex.org/W4287758545",
    "https://openalex.org/W3187275941",
    "https://openalex.org/W3048782180",
    "https://openalex.org/W4298289240",
    "https://openalex.org/W1959608418",
    "https://openalex.org/W3095986900",
    "https://openalex.org/W3037967553",
    "https://openalex.org/W3102582269",
    "https://openalex.org/W2796347433",
    "https://openalex.org/W4293584584",
    "https://openalex.org/W3099319035",
    "https://openalex.org/W3170097241",
    "https://openalex.org/W4287083770",
    "https://openalex.org/W3128609017",
    "https://openalex.org/W4287752100",
    "https://openalex.org/W3117450517",
    "https://openalex.org/W3105817677",
    "https://openalex.org/W3025670292"
  ],
  "abstract": null,
  "full_text": "Combining EﬃcientNet and Vision Transformers\nfor Video Deepfake Detection\nDavide Coccomini, Nicola Messina, Claudio Gennaro, and Fabrizio Falchi\nISTI-CNR, via G. Moruzzi 1, 56124, Pisa, Italy\ndavidealessandro.coccomini@isti.cnr.it, nicola.messina@isti.cnr.it,\nclaudio.gennaro@isti.cnr.it, @fabrizio.falchi@isti.cnr.it\nAbstract. Deepfakes are the result of digital manipulation to forge real-\nistic yet fake imagery. With the astonishing advances in deep generative\nmodels, fake images or videos are nowadays obtained using variational\nautoencoders (VAEs) or Generative Adversarial Networks (GANs). These\ntechnologies are becoming more accessible and accurate, resulting in\nfake videos that are very diﬃcult to be detected. Traditionally, Con-\nvolutional Neural Networks (CNNs) have been used to perform video\ndeepfake detection, with the best results obtained using methods based\non EﬃcientNet B7. In this study, we focus on video deep fake detection\non faces, given that most methods are becoming extremely accurate in\nthe generation of realistic human faces. Speciﬁcally, we combine vari-\nous types of Vision Transformers with a convolutional EﬃcientNet B0\nused as a feature extractor, obtaining comparable results with some\nvery recent methods that use Vision Transformers. Diﬀerently from\nthe state-of-the-art approaches, we use neither distillation nor ensemble\nmethods. Furthermore, we present a straightforward inference proce-\ndure based on a simple voting scheme for handling multiple faces in\nthe same video shot. The best model achieved an AUC of 0.951 and\nan F1 score of 88.0%, very close to the state-of-the-art on the Deep-\nFake Detection Challenge (DFDC). The code for reproducing our re-\nsults is publicly available here:https://github.com/davide-coccomini/\nCombining-EfficientNet-and-Vision-Transformers-for-Video-Deepfake-Detection .\nKeywords: Deep Fake Detection· Transformer Networks· Deep Learning\n1 Introduction\nWith the recent advances in generative deep learning techniques, it is nowadays\npossible to forge highly-realistic and credible misleading videos. These methods\nhave generated numerous fake news or revenge porn videos, becoming a severe\nproblem in modern society [6]. These fake videos are known asdeepfakes. Given\nthe astonishing realism obtained by recent models in the generation of human\nfaces, deepfakes are mainly obtained by transposing one person’s face onto\nanother’s. The results are so realistic that it is almost like the person being\narXiv:2107.02612v2  [cs.CV]  20 Jan 2022\n2 D. Coccomini et al.\nreplaced is actually present in the video, and the replaced actors are rigged to\nsay things they never actually said [38].\nThe evolution of deepfakes generation techniques and their increasing accessi-\nbility forces the research community to ﬁnd eﬀective methods to distinguish a\nmanipulated video from a real one. At the same time, more and more models\nbased on Transformers are gaining ground in the ﬁeld of Computer Vision, show-\ning excellent results in image processing [21,17], document retrieval [27], and\neﬃcient visual-textual matching [30,32], mainly for use in large-scale multi-modal\nretrieval systems [1,31].\nIn this paper, we analyze diﬀerent solutions based on combinations of convo-\nlutional networks, particularly the EﬃcientNet B0, with diﬀerent types of Vision\nTransformers and compare the results with the current state-of-the-art. Unlike\nVision Transformers, CNNs still maintain an important architectural prior, the\nspatial locality, which is very important for discovering image patch abnormalities\nand maintaining good data eﬃciency. CNNs, in fact, have a long-established suc-\ncess on many tasks, ranging from image classiﬁcation [13,40] and object detection\n[35,2,8] to abstract visual reasoning [28,29].\nIn this paper, we also propose a simple yet eﬀective voting mechanism to\nperform inference on videos. We show that this methodology could lead to better\nand more stable results.\n2 Related Works\n2.1 Deepfake Generation\nThere are mainly two generative approaches to obtain realistic faces: Generative\nAdversarial Networks (GANs) [15] and Variational AutoEncoders (VAEs) [23].\nGANs employ two distinct networks. The discriminator, the one that must be\nable to identify when a video is fake or not, and the generator, the network that\nactually modiﬁes the video in a suﬃciently credible way to deceive its counterpart.\nWith GANs, very credible and realistic results have been obtained, and over time,\nnumerous approaches have been introduced such as StarGAN [7] and DiscoGAN\n[22]; the best results in this ﬁeld have been obtained with StyleGAN-V2 [20].\nVAE-based solutions, instead, make use of a system consisting of two encoder-\ndecoder pairs, each of which is trained to deconstruct and reconstruct one of the\ntwo faces to be exchanged. Subsequently, the decoding part is switched, and this\nallows the reconstruction of the target person’s face. The best-known uses of this\ntechnique were DeepFaceLab [34], DFaker1, and DeepFaketf2.\n2.2 Deepfake Detection\nThe problem of deepfake detection has a widespread interest not only in the\nvisual domain. For example, the recent work in [12] analyzes deepfakes in tweets\nfor ﬁnding and defeating false content in social networks.\n1 https://github.com/dfaker/df\n2 https://github.com/StromWine/DeepFake_tf\nCombining EﬃcientNet and ViTs for Video Deepfake Detection 3\nIn an attempt to address the problem of deepfakes detection in videos, nu-\nmerous datasets have been produced over the years. These datasets are grouped\ninto three generations, the ﬁrst generation consisting of DF-TIMIT [24], UADFC\n[41] and FaceForensics++ [36], the second generation datasets such as Google\nDeepfake Detection Dataset [11], Celeb-DF [25], and ﬁnally the third generation\ndatasets, with the DFDC dataset [9] and DeepForensics [19]. The further the\ngenerations go, the larger these datasets are, and the more frames they contain.\nIn particular, on the DFDC dataset, which is the largest and most complete,\nmultiple experiments were carried out trying to obtain an eﬀective method\nfor deepfake detection. Very good results were obtained with EﬃcientNet B7\nensemble technique in [37]. Other noteworthy methods include those conducted\nin [33], who attempted to identify spatio-temporal anomalies by combining an\nEﬃcientNet with a Gated Recurrent Unit (GRU). Some eﬀorts to capture spatio-\ntemporal inconsistencies were made in [26] using 3DCNN networks and in [3],\nwhich presented a method that exploits optical ﬂow to detect video glitches. Some\nmore classical methods have also been proposed to perform deepfake detection. In\nparticular, the authors in [16] proposed a method based on K-nearest neighbors,\nwhile the work in [41] exploited SVMs. Of note is the very recent work of Giudice\net al. [14] in which they presented an innovative method for identifying so-called\nGAN Speciﬁc Frequencies (GSF) that represent a unique ﬁngerprint of diﬀerent\ngenerative architectures. By exploiting the Discrete Cosine Transform (DCT)\nthey manage to identify anomalous frequencies.\nMore recently, methods based on Vision Transformers have been proposed.\nNotably, the method presented in [39] obtained good results by combining\nTransformers with a convolutional network, used to extract patches from faces\ndetected in videos.\nState of the art was then recently improved by performing distillation from\nthe EﬃcientNet B7 pre-trained on the DFDC dataset to a Vision Transformer\n[18]. In this case, the Vision Transformer patches are combined with patches\nextracted from the EﬃcientNet B7 pre-trained via global pooling and then passed\nto the Transformer Encoder. A distillation token is then added to the Transformer\nnetwork to transfer the knowledge acquired by the EﬃcientNet B7.\n3 Method\nThe proposed methods analyze the faces extracted from the source video to\ndetermine whenever they have been manipulated. For this reason, faces are pre-\nextracted using a state-of-the-art face detector, MTCNN [42]. We propose two\nmixed convolutional-transformer architectures that take as input a pre-extracted\nface and output the probability that the face has been manipulated. The two\npresented architectures are trained in a supervised way to discern real from\nfake examples. For this reason, we solve the detection task by framing it as a\nbinary classiﬁcation problem. Speciﬁcally, we propose theEﬃcient ViT and the\nConvolutional Cross ViT, better explained in the following paragraphs.\n4 D. Coccomini et al.\nThe proposed models are trained on a face basis, and then they are used at\ninference time to draw a conclusion on the whole video shot by aggregating the\ninferred output both in time and across multiple faces, as explained in Section\n4.3.\nThe Eﬃcient ViT The Eﬃcient ViT is composed of two blocks, a convolutional\nmodule for working as a feature extractor and a Transformer Encoder, in a setup\nvery similar to the Vision Transformer (ViT) [10]. Considering the promising\nresults of the EﬃcientNet, we use an EﬃcientNet B0, the smallest of the Eﬃ-\ncientNet networks, as a convolutional extractor for processing the input faces.\nSpeciﬁcally, the EﬃcientNet produces a visual feature for each chunk from the\ninput face. Each chunk is7 × 7 pixels. After a linear projection, every feature\nfrom each spatial location is further processed by a Vision Transformer. The CLS\ntoken is used for producing the binary classiﬁcation score. The architecture is\nillustrated in Figure 1a. The EﬃcientNet B0 feature extractor is initialized with\nthe pre-trained weights and ﬁne-tuned to allow the last layers of the network to\nperform a more consistent and suitable extraction for this speciﬁc downstream\ntask. The features extracted from the EﬃcientNet B0 convolutional network\nsimplify the training of the Vision Transformer, as the CNN features already\nembed important low-level and localized information from the image.\nThe Convolutional Cross ViTLimiting the architecture to the use only small\npatches as in the Eﬃcient ViT may not be the ideal choice, as artifacts intro-\nduced by deepfakes generation methods may arise both locally and globally. For\nthis reason, we also introduce the Convolutional Cross ViT architecture. The\nConvolutional Cross ViT builds upon both the Eﬃcient ViT and the multi-scale\nTransformer architecture by [5]. More in detail, the Convolutional Cross ViT\nuses two distinct branches: theS-branch, which deals with smaller patches, and\nthe L-branch, which works on larger patches for having a wider receptive ﬁeld.\nThe visual tokens output by the Transformer Encoders from the two branches\nare combined through cross attention, allowing direct interaction between the\ntwo paths. Finally, the CLS tokens corresponding to the outputs from the two\nbranches are used to produce two separate logits. These logits are summed,\nand a ﬁnal sigmoid produces the ﬁnal probabilities. A detailed overview of this\narchitecture is shown in Fig. 1b. For the Convolutional Cross ViT, we use two\ndiﬀerent CNN backbones. The former is the EﬃcientNet B0, which processes\n7 × 7 image patches for the S-branch and54 × 54 for the L-branch. The latter\nis the CNN by Wodajo et al. [39], which handles7 × 7 image patches for the\nS-branch and64 × 64 for the L-branch.\n4 Experiments\nWe probed the presented architectures against some state-of-the-art methods on\ntwo widely-used datasets. In particular, we considered Convolutional ViT [39],\nViT with distillation [18], and Selim EﬃcientNet B7 [37], the winner of the Deep\nCombining EﬃcientNet and ViTs for Video Deepfake Detection 5\nEfficientNet B0\n7x7\nReal / Fake\nLinear Proj.\nCLS\nMLP Head\nTransformer Encoder\n(a) Eﬃcient ViT architecture.\nCNN CNN\nLinear Proj. Linear Proj.\n7x7 64x64\nTransformer\nEncoder\nCross-Attention\nMLP Head MLP Head\n+\nReal / Fake\nMulti-Scale Transformer Encoder\nL-BranchS-Branch\nS-Branch CLS L-Branch CLS\nTransformer\nEncoder\nEfficientNet B0\nor\nCNN by Wodajo et al. (b) Convolutional Cross ViT architecture.\nFig.1: The proposed architectures. Notice that for the Convolutional Cross ViT\nin (b), we experimented both with EﬃcientNet B0 and with the convolutional\narchitecture by [39] as feature extractors.\nFake Detection Challenge (DFDC). Notice that the results for Convolutional ViT\n[39] are not reported in the original paper, but they are obtained executing the\ntest code on DFDC test set using the available pre-trained model released by the\nauthors.\n4.1 Datasets and Face Extraction\nWe initially conducted some tests on FaceForensics++. The dataset is composed\nof original and fake videos generated through diﬀerent deepfake generation\ntechniques. For evaluating, we considered the videos generated in the Deepfakes,\nFace2Face, FaceShifter, FaceSwap and NeuralTextures sub-datasets. We also\nused the DFDC test set containing 5000 videos. The model trained on the\nentire training set, which includes fake videos of all considered methods of\nFaceForensics++ and the training videos of DFDC dataset, was used to calculate\nthe accuracy measures of the model, reported separately. In order to compare\nour methods also on the DFDC test set, we tested the Convolutional Vision\n6 D. Coccomini et al.\nTransformer [39] on these videos obtaining the necessary AUC and F1-score\nvalues for comparison.\nDuring training, we extracted the faces from the videos using an MTCNN,\nand we performed data augmentation like in [37]. Diﬀerently from them, we\nextracted the faces so that they were always squared and without padding. The\nimages obtained are used during the training, thus ignoring the remaining part\nof the frames. We used the Albumentations library [4], and we applied common\ntransformations such as the introduction of blur, Gaussian noise, transposition,\nrotation, and various isotropic resizes during training.\n4.2 Training\nWe trained the networks on 220,444 faces extracted from the videos of DFDC\ntraining set and FaceForensics++ training videos, and we used 8070 faces for\nvalidation from DFDC dataset. The training set was constructed trying to\nmaintain a good balance between the real class composed of 116,950 images and\nfakes with 103,494 images.\nWe used pre-trained EﬃcientNet B0 and Wodajo CNN feature extractors.\nHowever, we observed better results when ﬁne-tuning them, so we did not freeze\nthe extraction layers. We used the standard binary cross-entropy loss as our\nobjective during training. We optimized our network end-to-end, using an SGD\noptimizer with a learning rate of 0.01.\n4.3 Inference\nAt inference time, we set a real/fake threshold at 0.55 as done in [18]. However,\nwe proposed a slightly more elaborated voting procedure instead of averaging all\nratings on individual faces indistinctly within the video. Speciﬁcally, we merged\nthe scores, grouping them by the identiﬁer of the actors. The face identiﬁer is\navailable as an output from the employed MTCNN face detector. The scores from\ndiﬀerent actors are averaged over time to produce a probability of the face being\nfake. Then, the per-actor scores are merged using hard voting. In particular, if\nthere is at least one actor face passing the threshold, the whole video is classiﬁed\nas fake. The procedure is graphically explained in Fig. 2a. We claim that this\napproach is helpful to handle better videos in which only one of the actors’ faces\nhas been manipulated.\nIn addition, it is interesting to evaluate how the performance changes when a\nvarying number of faces are considered at inference time. To ensure that the tests\nare as light yet eﬀective as possible, we experimented on one of our networks to\nsee how the F1-score varies with the number of faces considered at testing time\n(Fig. 2b). We notice that a plateau is reached when no more than 30 faces are\nused, so employing more than this number of faces seems statistically useless at\ninference time.\nCombining EﬃcientNet and ViTs for Video Deepfake Detection 7\n0.1\n0.0\n0.2\n0.8\n0.7\n0.6\n0.7\n0.1 Real\nFake\nFake\n(a) Inference strategy with multiple faces in the\nsame video.\n(b) F1-score versus the number of ex-\ntracted faces.\nFig.2: Inference.\nTable 1: Results on DFDC test dataset\nModel AUC F1-score # params\nViT with distillation [18] 0.978 91.9% 373M\nSelim EﬃcientNet B7 [37]† 0.972 90.6% 462M\nConvolutional ViT [39] 0.843 77.0% 89M\nEﬃcient ViT (our) 0.919 83.8% 109M\nConv. Cross ViT Wodajo CNN (our) 0.925 84.5% 142M\nConv. Cross ViT Eﬀ.Net B0 - Avg (our) 0.947 85.6% 101M\nConv. Cross ViT Eﬀ.Net B0 - Voting (our) 0.951 88.0% 101M\n† Uses an ensemble of 6 networks.\n4.4 Results\nTable 1 shows that all models developed with EﬃcientNet achieve considerably\nhigher AUC and F1-scores than the Convolutional ViT presented in [39], providing\ninitial evidence that this speciﬁc network structure may be more suitable for\nthis type of task. It can also be noticed that the models based on Cross Vision\nTransformer obtain the best results, conﬁrming the theory that joined local and\nglobal image processing brings to better anomaly identiﬁcation.\nThe models with Cross Vision Transformer show a particularly marked\nimprovement when using the EﬃcientNet B0 as a patch extractor. Although the\nAUC and F1-score remain slightly below other state-of-the-art methods (in the\nﬁrst two rows of Table 1), these results were obtained using neither distillation\nnor ensemble techniques that complicate both training and inference. In fact, we\ncan notice how the Cross Vision Transformer with the EﬃcientNet extractor can\nreach a competitive performance using less than 1/3 of the parameters of the top\nmethods.\nFurthermore, in the last two rows of Table 1 we can notice how our voting\nprocedure used at inference time can slightly improve the results with respect to\n8 D. Coccomini et al.\nTable 2: Models accuracy on FaceForensics++\nModel Mean FaceSwap DeepFakes FaceShifter NeuralTextures\nConvolutional ViT [39] 67% 69% 93% 46% 60%\nEﬃcient ViT (our) 76% 78% 83% 76% 68%\nConv. Cross ViT Wodajo CNN (our) 76% 81% 83% 73% 67%\nConv. Cross ViT EﬃcientNet B0 (our)80% 84% 87% 80% 69%\nFig.3: ROC Curves comparison between our best model and others on DFDC\ntest set.\na plain average of the scores from all the faces indistinctly, as done by the other\nmethods. In Fig. 3 we report a detailed ROC plot for the architectures on the\nDFDC dataset.\nIn order to compare the developed models also on another dataset, we car-\nried out some tests also on FaceForensics++. As shown in Table 2, our models\noutperform the original Convolutional ViT [39] on all sub-datasets of FaceForen-\nsics++, excluding DeepFakes. This is probably because the network could better\ngeneralize on very speciﬁc types of deepfakes. It is worth noting how the results\nobtained in terms of accuracy on the various sub-datasets conﬁrm the assumption\nalready made in [39]: some deepfakes techniques such as NeuralTextures produce\nvideos that are more diﬃcult to ﬁnd, thus resulting in lower accuracy values than\nother sub-datasets. However, the average of all our three models is higher than\nthe average obtained by the Convolutional ViT. The Convolutional Cross ViT\nachieves the best result with the EﬃcientNet B0 backbone, obtaining a mean\naccuracy of 80%.\nCombining EﬃcientNet and ViTs for Video Deepfake Detection 9\n5 Conclusions\nIn this research, we demonstrated the eﬀectiveness of mixed convolutional-\ntransformer networks in the Deepfake detection task. Speciﬁcally, we used pre-\ntrained convolutional networks, such as the widely used EﬃcientNet B0, to extract\nvisual features, and we relied on Vision Transformers to obtain an informative\nglobal description for the downstream task. We showed that it is possible to obtain\nstate-of-the-art results without necessarily resorting to distillation techniques\nfrom models based on convolutional or ensemble networks. The use of a patch\nextractor based on EﬃcientNet proved to be particularly eﬀective even by simply\nusing the smallest network in this category. EﬃcientNet also led to better results\nthan the generic convolutional network trained from scratch used in Wodajo et al\n[39]. We then proposed a mixed architecture, the Convolutional Cross ViT, that\nworks at two diﬀerent scales to capture local and global details. The tests carried\nout with these models demonstrated the importance of multi-scale analysis for\ndetermining the manipulation of an image.\nWe also paid particular attention to the inference phase. In particular, we\npresented a simple yet eﬀective voting scheme for explicitly dealing with multiple\nfaces in a video. The scores from multiple actor faces are ﬁrst averaged over time,\nand only then hard voting is used to decide if at least one face was manipulated.\nThis inference mechanism yielded slightly better and stable results than the\nglobal average pooling of the scores performed by previous methods.\nReferences\n1. Amato, G., Bolettieri, P., Falchi, F., Gennaro, C., Messina, N., Vadicamo, L.,\nVairo, C.: Visione at video browser showdown 2021. In: International Conference\non Multimedia Modeling. pp. 473–478. Springer (2021)\n2. Amato, G., Ciampi, L., Falchi, F., Gennaro, C., Messina, N.: Learning pedestrian\ndetection from virtual worlds. In: International Conference on Image Analysis and\nProcessing. pp. 302–312. Springer (2019)\n3. Amerini, I., Galteri, L., Caldelli, R., Del Bimbo, A.: Deepfake video detection\nthrough optical ﬂow based cnn. In: Proceedings of the IEEE/CVF International\nConference on Computer Vision Workshops. pp. 0–0 (2019)\n4. Buslaev, A., Parinov, A., Khvedchenya, E., Iglovikov, V.I., Kalinin, A.A.: Albu-\nmentations: fast and ﬂexible image augmentations. ArXiv e-prints (2018)\n5. Chen, C.F., Fan, Q., Panda, R.: Crossvit: Cross-attention multi-scale vision trans-\nformer for image classiﬁcation. arXiv preprint arXiv:2103.14899 (2021)\n6. Chesney, B., Citron, D.: Deep fakes: A looming challenge for privacy, democracy,\nand national security. Calif. L. Rev.107, 1753 (2019)\n7. Choi, Yunjey, e.a.: Stargan: Uniﬁed generative adversarial networks for multi-domain\nimage- to-image translation. In: Proceedings of the IEEE conference on computer\nvision and pattern recognition (2018)\n8. Ciampi, L., Messina, N., Falchi, F., Gennaro, C., Amato, G.: Virtual to real\nadaptation of pedestrian detectors. Sensors20(18), 5250 (2020)\n9. Dolhansky, B., Bitton, J., Pﬂaum, B., Lu, J., Howes, R., Wang, M., Ferrer, C.C.:\nThe deepfake detection challenge (dfdc) dataset. arXiv preprint arXiv:2006.07397\n(2020)\n10 D. Coccomini et al.\n10. Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,Zhai,X.,Unterthiner,T.,\nDehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth 16x16\nwords: Transformers for image recognition at scale. In: International Conference on\nLearning Representations (2020)\n11. Dufour, N., Gully, A.: Contributing data to deep-fake de-\ntection research (2019), https://ai.googleblog.com/2019/09/\ncontributing-data-to-deepfake-detection.html\n12. Fagni, T., Falchi, F., Gambini, M., Martella, A., Tesconi, M.: Tweepfake: About\ndetecting deepfake tweets. Plos one16(5), e0251415 (2021)\n13. Foret, P., Kleiner, A., Mobahi, H., Neyshabur, B.: Sharpness-aware minimization\nfor eﬃciently improving generalization. arXiv preprint arXiv:2010.01412 (2020)\n14. Giudice, O., Guarnera, L., Battiato, S.: Fighting deepfakes by detecting gan dct\nanomalies. arXiv preprint arXiv:2101.09781 (2021)\n15. Goodfellow, I.J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,\nCourville, A., Bengio, Y.: Generative adversarial networks. In: Advances in neural\ninformation processing systems 27 (2014)\n16. Guarnera, L., Giudice, O., Battiato, S.: Deepfake detection by analyzing convolu-\ntional traces. In: Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition Workshops (2020)\n17. Han, K., Wang, Y., Chen, H., Chen, X., Guo, J., Liu, Z., Tang, Y., Xiao, A., Xu,\nC., Xu, Y., et al.: A survey on visual transformer. arXiv preprint arXiv:2012.12556\n(2020)\n18. Heo, Y.J., Choi, Y.J., Lee, Y.W., Kim, B.G.: Deepfake detection scheme based on\nvision transformer and distillation. arXiv preprint arXiv:2104.01353 (2021)\n19. Jiang, L., Li, R., Wu, W., Qian, C., Loy, C.C.: Deeperforensics-1.0: A large-scale\ndataset for real-world face forgery detection. In: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. pp. 2889–2898 (2020)\n20. Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., Aila, T.: Analyzing\nand improving the image quality of stylegan. In: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. pp. 8110–8119 (2020)\n21. Khan, S., Naseer, M., Hayat, M., Zamir, S.W., Khan, F.S., Shah, M.: Transformers\nin vision: A survey. arXiv preprint arXiv:2101.01169 (2021)\n22. Kim, T., Cha, M., Kim, H., Lee, J.K., Kim, J.: Learning to discover cross-domain\nrelations with generative adversarial networks. In: International Conference on\nMachine Learning. pp. 1857–1865. PMLR (2017)\n23. Kingma, D.P., Welling, M.: Auto-encoding variational bayes. arXiv preprint\narXiv:1312.6114 (2013)\n24. Korshunov, P., Marcel, S.: Deepfakes: a new threat to face recognition? assessment\nand detection. arXiv preprint arXiv:1812.08685 (2018)\n25. Li, Y., Yang, X., Sun, P., Qi, H., Lyu, S.: Celeb-df: A large-scale challenging dataset\nfor deepfake forensics. In: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition. pp. 3207–3216 (2020)\n26. de Lima, O., Franklin, S., Basu, S., Karwoski, B., George, A.: Deepfake detection\nusing spatiotemporal convolutional networks. arXiv preprint arXiv:2006.14749\n(2020)\n27. MacAvaney, S., Nardini, F.M., Perego, R., Tonellotto, N., Goharian, N., Frieder,\nO.: Eﬃcient document re-ranking for transformers by precomputing term repre-\nsentations. In: Proceedings of the 43rd International ACM SIGIR Conference on\nResearch and Development in Information Retrieval. pp. 49–58 (2020)\nCombining EﬃcientNet and ViTs for Video Deepfake Detection 11\n28. Messina, N., Amato, G., Carrara, F., Falchi, F., Gennaro, C.: Testing deep neural\nnetworks on the same-diﬀerent task. In: 2019 International Conference on Content-\nBased Multimedia Indexing (CBMI). pp. 1–6. IEEE (2019)\n29. Messina, N., Amato, G., Carrara, F., Gennaro, C., Falchi, F.: Solving the same-\ndiﬀerent task with convolutional neural networks. Pattern Recognition Letters143,\n75–80 (2021)\n30. Messina, N., Amato, G., Esuli, A., Falchi, F., Gennaro, C., Marchand-Maillet, S.:\nFine-grained visual textual alignment for cross-modal retrieval using transformer\nencoders. arXiv preprint arXiv:2008.05231 (2020)\n31. Messina, N., Amato, G., Falchi, F., Gennaro, C., Marchand-Maillet, S.: Towards\neﬃcient cross-modal visual textual retrieval using transformer-encoder deep features.\narXiv preprint arXiv:2106.00358 (2021)\n32. Messina, N., Falchi, F., Esuli, A., Amato, G.: Transformer reasoning network\nfor image-text matching and retrieval. In: 2020 25th International Conference on\nPattern Recognition (ICPR). pp. 5222–5229. IEEE (2021)\n33. Montserrat, D.M., Hao, H., Yarlagadda, S.K., Baireddy, S., Shao, R., Horváth, J.,\nBartusiak, E., Yang, J., Guera, D., Zhu, F., et al.: Deepfakes detection with auto-\nmatic face weighting. In: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition Workshops. pp. 668–669 (2020)\n34. Perov, I., Gao, D., Chervoniy, N., Liu, K., Marangonda, S., Umé, C., Dpfks, M.,\nFacenheim, C.S., RP, L., Jiang, J., et al.: Deepfacelab: A simple, ﬂexible and\nextensible face swapping framework. arXiv preprint arXiv:2005.05535 (2020)\n35. Redmon, J., Farhadi, A.: Yolov3: An incremental improvement. arXiv preprint\narXiv:1804.02767 (2018)\n36. Rossler, A., Cozzolino, D., Verdoliva, L., Riess, C., Thies, J., Nießner, M.: Face-\nforensics++: Learning to detect manipulated facial images. In: Proceedings of the\nIEEE/CVF International Conference on Computer Vision. pp. 1–11 (2019)\n37. Seferbekov, S.: Dfdc 1st place solution (2020),\"https://github.com/selimsef/\ndfdc_deepfake_challenge\"\n38. Tolosana, R., Vera-Rodriguez, R., Fierrez, J., Morales, A., Ortega-Garcia, J.: Deep-\nfakes and beyond: A survey of face manipulation and fake detection. Information\nFusion 64, 131–148 (2020)\n39. Wodajo, D., Atnafu, S.: Deepfake video detection using convolutional vision trans-\nformer. arXiv preprint arXiv:2102.11126 (2021)\n40. Xie, S., Girshick, R., Dollár, P., Tu, Z., He, K.: Aggregated residual transformations\nfor deep neural networks. In: Proceedings of the IEEE conference on computer\nvision and pattern recognition. pp. 1492–1500 (2017)\n41. Yang, X., Li, Y., Lyu, S.: Exposing deep fakes using inconsistent head poses. In:\nICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP). pp. 8261–8265. IEEE (2019)\n42. Zhang, K., Zhang, Z., Li, Z., Qiao, Y.: Joint face detection and alignment using\nmultitask cascaded convolutional networks. IEEE Signal Processing Letters23(10),\n1499–1503 (2016)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.871074914932251
    },
    {
      "name": "Artificial intelligence",
      "score": 0.661597728729248
    },
    {
      "name": "Deep learning",
      "score": 0.5769209861755371
    },
    {
      "name": "Transformer",
      "score": 0.575385570526123
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5647866129875183
    },
    {
      "name": "Inference",
      "score": 0.5212612748146057
    },
    {
      "name": "Generative grammar",
      "score": 0.4311065375804901
    },
    {
      "name": "Machine learning",
      "score": 0.4202514886856079
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.349393367767334
    },
    {
      "name": "Computer vision",
      "score": 0.3328436613082886
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I122991210",
      "name": "Istituto di Scienza e Tecnologie dell'Informazione \"Alessandro Faedo\"",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I4210155236",
      "name": "National Research Council",
      "country": "IT"
    }
  ]
}