{
  "title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
  "url": "https://openalex.org/W4380993239",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2753535486",
      "name": "Pan, Shirui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223081720",
      "name": "Luo, Linhao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A762533951",
      "name": "Wang Yu-Fei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2095964268",
      "name": "Chen Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2360020930",
      "name": "Wang, Jiapu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2695117332",
      "name": "Wu Xindong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4311787510",
    "https://openalex.org/W1512387364",
    "https://openalex.org/W2986836624",
    "https://openalex.org/W1964763677",
    "https://openalex.org/W3168355451",
    "https://openalex.org/W4311728219",
    "https://openalex.org/W2588576322",
    "https://openalex.org/W2998456908",
    "https://openalex.org/W2963087868",
    "https://openalex.org/W3185518029",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3179195481",
    "https://openalex.org/W3034797437",
    "https://openalex.org/W3212129137",
    "https://openalex.org/W4225716497",
    "https://openalex.org/W3176762756",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4385573038",
    "https://openalex.org/W4287555517",
    "https://openalex.org/W4221142421",
    "https://openalex.org/W4389523903",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W4385572907",
    "https://openalex.org/W4366850747",
    "https://openalex.org/W2620787630",
    "https://openalex.org/W3159923042",
    "https://openalex.org/W3182352988",
    "https://openalex.org/W4296540990",
    "https://openalex.org/W2963870853",
    "https://openalex.org/W4310424784",
    "https://openalex.org/W2987972786",
    "https://openalex.org/W4380551814",
    "https://openalex.org/W4313304841",
    "https://openalex.org/W3205270560",
    "https://openalex.org/W3188999884",
    "https://openalex.org/W3026997957",
    "https://openalex.org/W3176971429",
    "https://openalex.org/W2807873315",
    "https://openalex.org/W3156082048",
    "https://openalex.org/W3091091747",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W4206334925",
    "https://openalex.org/W4307653761",
    "https://openalex.org/W4385573097",
    "https://openalex.org/W4285261975",
    "https://openalex.org/W4224863259",
    "https://openalex.org/W4384643740",
    "https://openalex.org/W3091432621",
    "https://openalex.org/W4225390052",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W4225580830",
    "https://openalex.org/W4385970309",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3207166518",
    "https://openalex.org/W2784476247",
    "https://openalex.org/W2915573705",
    "https://openalex.org/W3093922720",
    "https://openalex.org/W4386148087",
    "https://openalex.org/W3201595577",
    "https://openalex.org/W4225831764",
    "https://openalex.org/W2972167903",
    "https://openalex.org/W4221157571",
    "https://openalex.org/W3175344781",
    "https://openalex.org/W4387687756",
    "https://openalex.org/W3176589722",
    "https://openalex.org/W3176750236",
    "https://openalex.org/W3193084543",
    "https://openalex.org/W4225632115",
    "https://openalex.org/W4226436348",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4307003748",
    "https://openalex.org/W4385573374",
    "https://openalex.org/W2999524812",
    "https://openalex.org/W3114916066",
    "https://openalex.org/W3171289259",
    "https://openalex.org/W3021224558",
    "https://openalex.org/W3012590175",
    "https://openalex.org/W4225619898",
    "https://openalex.org/W4317940108",
    "https://openalex.org/W2964022985",
    "https://openalex.org/W4312044727",
    "https://openalex.org/W2984147501",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4376876984",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4322588156",
    "https://openalex.org/W4386501849",
    "https://openalex.org/W3095839900",
    "https://openalex.org/W2889002152",
    "https://openalex.org/W3083494020",
    "https://openalex.org/W4310823304",
    "https://openalex.org/W4290056678",
    "https://openalex.org/W2963571857",
    "https://openalex.org/W3177329835",
    "https://openalex.org/W3173197807",
    "https://openalex.org/W2975998869",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4377865170",
    "https://openalex.org/W3197071999",
    "https://openalex.org/W4387430414",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3157097179",
    "https://openalex.org/W4377864203",
    "https://openalex.org/W3176175717",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W4385573010",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W3035251962",
    "https://openalex.org/W2889234142",
    "https://openalex.org/W4387635121",
    "https://openalex.org/W4312091849",
    "https://openalex.org/W4367365458",
    "https://openalex.org/W2981039167",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3103836967",
    "https://openalex.org/W4304697829",
    "https://openalex.org/W3136215575",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W3201503287",
    "https://openalex.org/W2127795553",
    "https://openalex.org/W3093891978",
    "https://openalex.org/W4229543565",
    "https://openalex.org/W4226455589",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4205450747",
    "https://openalex.org/W3207976211",
    "https://openalex.org/W4283701416",
    "https://openalex.org/W4206745025",
    "https://openalex.org/W4307183432",
    "https://openalex.org/W2951048068",
    "https://openalex.org/W3115091907",
    "https://openalex.org/W2965723991",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3106255016",
    "https://openalex.org/W3175081470",
    "https://openalex.org/W4224235016",
    "https://openalex.org/W2994915912",
    "https://openalex.org/W4389520779",
    "https://openalex.org/W3105082862",
    "https://openalex.org/W4386566720",
    "https://openalex.org/W4303468996",
    "https://openalex.org/W2907806094",
    "https://openalex.org/W3117339789",
    "https://openalex.org/W3174331410",
    "https://openalex.org/W4382652828",
    "https://openalex.org/W4385573687",
    "https://openalex.org/W3104007871",
    "https://openalex.org/W2890560945",
    "https://openalex.org/W4366327286",
    "https://openalex.org/W3035419191",
    "https://openalex.org/W3090656107",
    "https://openalex.org/W3151929433",
    "https://openalex.org/W4288373939",
    "https://openalex.org/W3174464510",
    "https://openalex.org/W4221021831",
    "https://openalex.org/W4297847420",
    "https://openalex.org/W4226142803",
    "https://openalex.org/W3034918576",
    "https://openalex.org/W4386443326",
    "https://openalex.org/W4386113654",
    "https://openalex.org/W3035053871",
    "https://openalex.org/W4384643637",
    "https://openalex.org/W3042315843",
    "https://openalex.org/W2970550868",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W2786660442",
    "https://openalex.org/W4303443398",
    "https://openalex.org/W4385270279",
    "https://openalex.org/W3173436410",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W4221141423",
    "https://openalex.org/W2966317026",
    "https://openalex.org/W2998385486",
    "https://openalex.org/W2762569911",
    "https://openalex.org/W2908230750",
    "https://openalex.org/W3157651462",
    "https://openalex.org/W4302011088",
    "https://openalex.org/W4223905302",
    "https://openalex.org/W2963895422",
    "https://openalex.org/W4386290290",
    "https://openalex.org/W3162119881",
    "https://openalex.org/W3098266846",
    "https://openalex.org/W4297903189",
    "https://openalex.org/W3175225269",
    "https://openalex.org/W4384345684",
    "https://openalex.org/W2964727037",
    "https://openalex.org/W3092965216",
    "https://openalex.org/W4321854923",
    "https://openalex.org/W3040558716",
    "https://openalex.org/W4385572435",
    "https://openalex.org/W2107598941",
    "https://openalex.org/W3106234277",
    "https://openalex.org/W4386071707",
    "https://openalex.org/W4285817834",
    "https://openalex.org/W3086728154",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W3182414949",
    "https://openalex.org/W3011574394",
    "https://openalex.org/W3035375600",
    "https://openalex.org/W4382246105",
    "https://openalex.org/W4310744116",
    "https://openalex.org/W3097986428",
    "https://openalex.org/W3003265726",
    "https://openalex.org/W3034862985",
    "https://openalex.org/W3104616515",
    "https://openalex.org/W3165066581",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W4389520370",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W4387839082",
    "https://openalex.org/W4283765342",
    "https://openalex.org/W4285172793",
    "https://openalex.org/W102708294",
    "https://openalex.org/W4394743141",
    "https://openalex.org/W2184957013",
    "https://openalex.org/W3099206682",
    "https://openalex.org/W3197746952",
    "https://openalex.org/W4303202235",
    "https://openalex.org/W2945878859",
    "https://openalex.org/W4385682136",
    "https://openalex.org/W3155001903",
    "https://openalex.org/W4280535549",
    "https://openalex.org/W4313655989",
    "https://openalex.org/W4307023377",
    "https://openalex.org/W4366733551",
    "https://openalex.org/W3003908700",
    "https://openalex.org/W2996749332",
    "https://openalex.org/W2159583324",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2983995706",
    "https://openalex.org/W4389519364",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W3101327207",
    "https://openalex.org/W3172335055",
    "https://openalex.org/W4221154018",
    "https://openalex.org/W4387356300",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W4308288330",
    "https://openalex.org/W4367860080",
    "https://openalex.org/W4365441217",
    "https://openalex.org/W4386556710",
    "https://openalex.org/W1533230146",
    "https://openalex.org/W4386554558",
    "https://openalex.org/W4385571693",
    "https://openalex.org/W4387322659",
    "https://openalex.org/W2561529111",
    "https://openalex.org/W3034385177",
    "https://openalex.org/W3177423701",
    "https://openalex.org/W3117696238",
    "https://openalex.org/W4301194718",
    "https://openalex.org/W4385567227"
  ],
  "abstract": "Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolving by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and simultaneously leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs, which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2) LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.",
  "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 1\nUnifying Large Language Models and\nKnowledge Graphs: A Roadmap\nShirui Pan, Senior Member, IEEE, Linhao Luo,\nYufei Wang, Chen Chen, Jiapu Wang, Xindong Wu,Fellow, IEEE\nAbstract—Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language\nprocessing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which\noften fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example,\nare structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge\nfor inference and interpretability. Meanwhile, KGs are difficult to construct and evolve by nature, which challenges the existing methods\nin KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and\nsimultaneously leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs.\nOur roadmap consists of three general frameworks, namely,1) KG-enhanced LLMs, which incorporate KGs during the pre-training and\ninference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2) LLM-augmented KGs,\nthat leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question\nanswering; and 3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance\nboth LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within\nthese three frameworks in our roadmap and pinpoint their future research directions.\nIndex Terms—Natural Language Processing, Large Language Models, Generative Pre-Training, Knowledge Graphs, Roadmap,\nBidirectional Reasoning.\n✦\n1 I NTRODUCTION\nLarge language models (LLMs) 1 (e.g., BERT [1], RoBERTA\n[2], and T5 [3]), pre-trained on the large-scale corpus,\nhave shown great performance in various natural language\nprocessing (NLP) tasks, such as question answering [4],\nmachine translation [5], and text generation [6]. Recently,\nthe dramatically increasing model size further enables the\nLLMs with the emergent ability [7], paving the road for\napplying LLMs as Artificial General Intelligence (AGI).\nAdvanced LLMs like ChatGPT 2 and PaLM23, with billions\nof parameters, exhibit great potential in many complex\npractical tasks, such as education [8], code generation [9]\nand recommendation [10].\n• Shirui Pan is with the School of Information and Communication Tech-\nnology and Institute for Integrated and Intelligent Systems (IIIS), Griffith\nUniversity, Queensland, Australia. Email: s.pan@griffith.edu.au;\n• Linhao Luo and Yufei Wang are with the Department of Data Sci-\nence and AI, Monash University, Melbourne, Australia. E-mail: lin-\nhao.luo@monash.edu, garyyufei@gmail.com.\n• Chen Chen is with the Nanyang Technological University, Singapore. E-\nmail: s190009@ntu.edu.sg.\n• Jiapu Wang is with the Faculty of Information Technology, Beijing Uni-\nversity of Technology, Beijing, China. E-mail: jpwang@emails.bjut.edu.cn.\n• Xindong Wu is with the Key Laboratory of Knowledge Engineering with\nBig Data (the Ministry of Education of China), Hefei University of Tech-\nnology, Hefei, China, and also with the Research Center for Knowledge\nEngineering, Zhejiang Lab, Hangzhou, China. Email: xwu@hfut.edu.cn.\n• Shirui Pan and Linhao Luo contributed equally to this work.\n• Corresponding Author: Xindong Wu.\n1. LLMs are also known as pre-trained language models (PLMs).\n2. https://openai.com/blog/chatgpt\n3. https://ai.google/discover/palm2\nFig. 1. Summarization of the pros and cons for LLMs and KGs. LLM\npros: General Knowledge [11], Language Processing [12], Generaliz-\nability [13]; LLM cons: Implicit Knowledge [14], Hallucination [15], In-\ndecisiveness [16], Black-box [17], Lacking Domain-specific/New Knowl-\nedge [18]. KG pros: Structural Knowledge [19], Accuracy [20], Decisive-\nness [21], Interpretability [22], Domain-specific Knowledge [23], Evolv-\ning Knowledge [24]; KG cons: Incompleteness [25], Lacking Language\nUnderstanding [26], Unseen Facts [27]. Pros. and Cons. are selected\nbased on their representativeness. Detailed discussion can be found in\nAppendix A.\nDespite their success in many applications, LLMs have\nbeen criticized for their lack of factual knowledge. Specif-\nically, LLMs memorize facts and knowledge contained in\nthe training corpus [14]. However, further studies reveal\nthat LLMs are not able to recall facts and often experience\nhallucinations by generating statements that are factually\n0000–0000/00$00.00 © 2023 IEEE\narXiv:2306.08302v3  [cs.CL]  25 Jan 2024\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 2\nincorrect [15], [28]. For example, LLMs might say “Ein-\nstein discovered gravity in 1687” when asked, “When did\nEinstein discover gravity?”, which contradicts the fact that\nIsaac Newton formulated the gravitational theory. This issue\nseverely impairs the trustworthiness of LLMs.\nAs black-box models, LLMs are also criticized for their\nlack of interpretability. LLMs represent knowledge implic-\nitly in their parameters. It is difficult to interpret or validate\nthe knowledge obtained by LLMs. Moreover, LLMs perform\nreasoning by a probability model, which is an indecisive\nprocess [16]. The specific patterns and functions LLMs\nused to arrive at predictions or decisions are not directly\naccessible or explainable to humans [17]. Even though some\nLLMs are equipped to explain their predictions by applying\nchain-of-thought [29], their reasoning explanations also suf-\nfer from the hallucination issue [30]. This severely impairs\nthe application of LLMs in high-stakes scenarios, such as\nmedical diagnosis and legal judgment. For instance, in a\nmedical diagnosis scenario, LLMs may incorrectly diagnose\na disease and provide explanations that contradict medical\ncommonsense. This raises another issue that LLMs trained\non general corpus might not be able to generalize well\nto specific domains or new knowledge due to the lack of\ndomain-specific knowledge or new training data [18].\nTo address the above issues, a potential solution is to in-\ncorporate knowledge graphs (KGs) into LLMs. Knowledge\ngraphs (KGs), storing enormous facts in the way of triples,\ni.e., (head entity, relation, tail entity), are a structured and\ndecisive manner of knowledge representation (e.g., Wiki-\ndata [20], YAGO [31], and NELL [32]). KGs are crucial for\nvarious applications as they offer accurate explicit knowl-\nedge [19]. Besides, they are renowned for their symbolic\nreasoning ability [22], which generates interpretable results.\nKGs can also actively evolve with new knowledge contin-\nuously added in [24]. Additionally, experts can construct\ndomain-specific KGs to provide precise and dependable\ndomain-specific knowledge [23].\nNevertheless, KGs are difficult to construct [25], and\ncurrent approaches in KGs [27], [33], [34] are inadequate\nin handling the incomplete and dynamically changing na-\nture of real-world KGs. These approaches fail to effectively\nmodel unseen entities and represent new facts. In addition,\nthey often ignore the abundant textual information in KGs.\nMoreover, existing methods in KGs are often customized for\nspecific KGs or tasks, which are not generalizable enough.\nTherefore, it is also necessary to utilize LLMs to address the\nchallenges faced in KGs. We summarize the pros and cons\nof LLMs and KGs in Fig. 1, respectively.\nRecently, the possibility of unifying LLMs with KGs has\nattracted increasing attention from researchers and practi-\ntioners. LLMs and KGs are inherently interconnected and\ncan mutually enhance each other. In KG-enhanced LLMs ,\nKGs can not only be incorporated into the pre-training and\ninference stages of LLMs to provide external knowledge\n[35]–[37], but also used for analyzing LLMs and provid-\ning interpretability [14], [38], [39]. In LLM-augmented KGs ,\nLLMs have been used in various KG-related tasks, e.g., KG\nembedding [40], KG completion [26], KG construction [41],\nKG-to-text generation [42], and KGQA [43], to improve the\nperformance and facilitate the application of KGs. In Syn-\nergized LLM + KG , researchers marries the merits of LLMs\nand KGs to mutually enhance performance in knowledge\nrepresentation [44] and reasoning [45], [46]. Although there\nare some surveys on knowledge-enhanced LLMs [47]–[49],\nwhich mainly focus on using KGs as an external knowledge\nto enhance LLMs, they ignore other possibilities of integrat-\ning KGs for LLMs and the potential role of LLMs in KG\napplications.\nIn this article, we present a forward-looking roadmap for\nunifying both LLMs and KGs, to leverage their respective\nstrengths and overcome the limitations of each approach,\nfor various downstream tasks. We propose detailed cate-\ngorization, conduct comprehensive reviews, and pinpoint\nemerging directions in these fast-growing fields. Our main\ncontributions are summarized as follows:\n1) Roadmap. We present a forward-looking roadmap\nfor integrating LLMs and KGs. Our roadmap,\nconsisting of three general frameworks to unify\nLLMs and KGs, namely, KG-enhanced LLMs , LLM-\naugmented KGs , and Synergized LLMs + KGs , pro-\nvides guidelines for the unification of these two\ndistinct but complementary technologies.\n2) Categorization and review. For each integration\nframework of our roadmap, we present a detailed\ncategorization and novel taxonomies of research\non unifying LLMs and KGs. In each category, we\nreview the research from the perspectives of differ-\nent integration strategies and tasks, which provides\nmore insights into each framework.\n3) Coverage of emerging advances. We cover the\nadvanced techniques in both LLMs and KGs. We\ninclude the discussion of state-of-the-art LLMs like\nChatGPT and GPT-4 as well as the novel KGs e.g.,\nmulti-modal knowledge graphs.\n4) Summary of challenges and future directions.We\nhighlight the challenges in existing research and\npresent several promising future research direc-\ntions.\nThe rest of this article is organized as follows. Section\n2 first explains the background of LLMs and KGs. Section\n3 introduces the roadmap and the overall categorization of\nthis article. Section 4 presents the different KGs-enhanced\nLLM approaches. Section 5 describes the possible LLM-\naugmented KG methods. Section 6 shows the approaches\nof synergizing LLMs and KGs. Section 7 discusses the\nchallenges and future research directions. Finally, Section 8\nconcludes this paper.\n2 B ACKGROUND\nIn this section, we will first briefly introduce a few rep-\nresentative large language models (LLMs) and discuss the\nprompt engineering that efficiently uses LLMs for varieties\nof applications. Then, we illustrate the concept of knowl-\nedge graphs (KGs) and present different categories of KGs.\n2.1 Large Language models (LLMs)\nLarge language models (LLMs) pre-trained on large-scale\ncorpus have shown great potential in various NLP tasks\n[13]. As shown in Fig. 3, most LLMs derive from the Trans-\nformer design [50], which contains the encoder and decoder\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 3\n2018 2019 2020 2021 2023\nEncoder-only Encoder- \ndecoder Decoder-only \n2022\nInput Text\nDecoder\nOutput Text\nInput Text\nEncoder\nFeatures\nDecoder\nOutput Text\nInput Text\nEncoder\nFeatures\nBERT\nRoBERTA\n110M-340M\n125M-355M\nALBERTDistillBert\n66M\nERNIE\nELECTRA\nDeBERTa\n114M\n14M-110M11M-223M\n44M-304M\nBART\nT5 mT5\nT0\n140M\n80M-11B\n11B\n300M-13B\nGLM\nSwitch\n110M-10B\n1.6T\nST-MoE\n4.1B-269B\nGLM-130B\nUL2\n20B\nFlan-UL2\n20B\n130B\nFlan-T5\n80M-11B\nGPT-1\n110 M\nGPT-2\n117M-1.5B\nGPT-3\n175B\nChatGPT GPT-4\n175B Unknown\nXLNet\n110M-340M\nGLaM\n1.2T\nGopher\n280B\nLaMDAPaLM\n137B\nFlan PaLM\n540B\n540B\nOPT\n175B\nOPT-IML\n175B\nBard\n137B\nLLaMa\n7B-65B\nAlpaca\nVicuna\n7B\n7B-13B\nOpen-Source\nClosed-Source\nFig. 2. Representative large language models (LLMs) in recent years. Open-source models are represented by solid squares, while closed source\nmodels are represented by hollow squares.\nEncoder\nSelf-Attention\nFeed Forward\nSelf-Attention\nFeed Forward\nEncoder-Decoder\nAttention\nDecoder\nLinear\nMulti-head Dot-Product\nAttention\nV\nLinear Linear\nQ K\nLinear\nConcat\nSelf-Attention\nFig. 3. An illustration of the Transformer-based LLMs with self-attention\nmechanism.\nmodules empowered by a self-attention mechanism. Based\non the architecture structure, LLMs can be categorized\ninto three groups: 1) encoder-only LLMs , 2) encoder-decoder\nLLMs, and 3) decoder-only LLMs. As shown in Fig. 2, we sum-\nmarize several representative LLMs with different model\narchitectures, model sizes, and open-source availabilities.\n2.1.1 Encoder-only LLMs.\nEncoder-only large language models only use the encoder\nto encode the sentence and understand the relationships\nbetween words. The common training paradigm for these\nmodel is to predict the mask words in an input sentence.\nThis method is unsupervised and can be trained on the\nlarge-scale corpus. Encoder-only LLMs like BERT [1], AL-\nBERT [51], RoBERTa [2], and ELECTRA [52] require adding\nan extra prediction head to resolve downstream tasks. These\nmodels are most effective for tasks that require understand-\ning the entire sentence, such as text classification [26] and\nnamed entity recognition [53].\n2.1.2 Encoder-decoder LLMs.\nEncoder-decoder large language models adopt both the\nencoder and decoder module. The encoder module is re-\nsponsible for encoding the input sentence into a hidden-\nspace, and the decoder is used to generate the target output\ntext. The training strategies in encoder-decoder LLMs can be\nmore flexible. For example, T5 [3] is pre-trained by masking\nand predicting spans of masking words. UL2 [54] unifies\nseveral training targets such as different masking spans and\nmasking frequencies. Encoder-decoder LLMs (e.g., T0 [55],\nST-MoE [56], and GLM-130B [57]) are able to directly resolve\ntasks that generate sentences based on some context, such\nas summariaztion, translation, and question answering [58].\n2.1.3 Decoder-only LLMs.\nDecoder-only large language models only adopt the de-\ncoder module to generate target output text. The training\nparadigm for these models is to predict the next word in\nthe sentence. Large-scale decoder-only LLMs can generally\nperform downstream tasks from a few examples or simple\ninstructions, without adding prediction heads or finetun-\ning [59]. Many state-of-the-art LLMs (e.g., Chat-GPT [60]\nand GPT-44) follow the decoder-only architecture. However,\nsince these models are closed-source, it is challenging for\nacademic researchers to conduct further research. Recently,\nAlpaca5 and Vicuna6 are released as open-source decoder-\nonly LLMs. These models are finetuned based on LLaMA\n[61] and achieve comparable performance with ChatGPT\nand GPT-4.\n2.1.4 Prompt Engineering\nPrompt engineering is a novel field that focuses on creating\nand refining prompts to maximize the effectiveness of large\nlanguage models (LLMs) across various applications and re-\nsearch areas [62]. As shown in Fig. 4, a prompt is a sequence\n4. https://openai.com/product/gpt-4\n5. https://github.com/tatsu-lab/stanford alpaca\n6. https://lmsys.org/blog/2023-03-30-vicuna/\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 4\nLLMs \nClassify the text into neutral,\nnegative or positive. \nText: This is awesome!\nSentiment: Positive\nText: This is bad!\nSentiment: Negative\nText: I think the vacation is okay.\nSentiment:\nInstruction\nContext\nInput Text\nPrompt \nPositiveOutput\nFig. 4. An example of sentiment classification prompt.\nof natural language inputs for LLMs that are specified for\nthe task, such as sentiment classification. A prompt could\ncontain several elements, i.e., 1) Instruction, 2) Context, and\n3) Input Text . Instruction is a short sentence that instructs\nthe model to perform a specific task. Context provides the\ncontext for the input text or few-shot examples. Input Text is\nthe text that needs to be processed by the model.\nPrompt engineering seeks to improve the capacity of\nlarge large language models (e.g., ChatGPT) in diverse\ncomplex tasks such as question answering, sentiment clas-\nsification, and common sense reasoning. Chain-of-thought\n(CoT) prompt [63] enables complex reasoning capabilities\nthrough intermediate reasoning steps. Prompt engineering\nalso enables the integration of structural data like knowl-\nedge graphs (KGs) into LLMs. Li et al. [64] simply linearizes\nthe KGs and uses templates to convert the KGs into pas-\nsages. Mindmap [65] designs a KG prompt to convert graph\nstructure into a mind map that enables LLMs to perform\nreasoning on it. Prompt offers a simple way to utilize the\npotential of LLMs without finetuning. Proficiency in prompt\nengineering leads to a better understanding of the strengths\nand weaknesses of LLMs.\n2.2 Knowledge Graphs (KGs)\nKnowledge graphs (KGs) store structured knowledge as a\ncollection of triples KG = {(h, r, t) ⊆ E × R × E}, where E\nand R respectively denote the set of entities and relations.\nExisting knowledge graphs (KGs) can be classified into four\ngroups based on the stored information: 1) encyclopedic KGs,\n2) commonsense KGs , 3) domain-specific KGs , and 4) multi-\nmodal KGs . We illustrate the examples of KGs of different\ncategories in Fig. 5.\n2.2.1 Encyclopedic Knowledge Graphs.\nEncyclopedic knowledge graphs are the most ubiquitous\nKGs, which represent the general knowledge in real-world.\nEncyclopedic knowledge graphs are often constructed by\nintegrating information from diverse and extensive sources,\nincluding human experts, encyclopedias, and databases.\nWikidata [20] is one of the most widely used encyclopedic\nknowledge graphs, which incorporates varieties of knowl-\nedge extracted from articles on Wikipedia. Other typical\nWikipedia \nMarriedT \no \nPoliticianOf \nLiveIn \nLocatedIn \nBornIn \nCapitalOf \nEncyclopedic\nKnowledge Graphs\nCommonsense\nKnowledge Graphs\nBarack\nObama\nMichelle\nObama\nUSA\nHonolulu \nW ashington \nD.C. \nWake up\nBed LocatedAt \nGet out\nof bed\nOpen\neyes\nSubeventOf \nSubeventOf \nDrink\ncoffee\nSubeventOf \nAwake\nCauses \nMake\ncoffe SubeventOf \nCoffe\nIsFor \nDrink\nIs \nKitchen\nLocatedAt \nSugar\nCup\nNeed \nNeed \nConcept: Wake up\nDomain-speciﬁc\nKnowledge Graphs\nParkinson's\nDiease\nSleeping\nDisorderCause \nPINK1 Cause \nMotor\nSymptom\nLead \nTremor\nLead \nAnxiety\nCause \nPervasive\nDevelopmental\nDisorder\nLead \nLanguage\nUndevelopment\nLead \nMedical Knowledge Graph\nMulti-modal\nKnowledge Graphs\nFrance\nParisEiffel\nTower\nCapitalOf \nLocatedIn \nEuropean \nUnion \nMemberOf \nEmmanuel \nMacron \nPoliticianOf \nLiveIn \nFig. 5. Examples of different categories’ knowledge graphs, i.e.,encyclo-\npedic KGs, commonsense KGs, domain-specific KGs, and multi-modal\nKGs.\nencyclopedic knowledge graphs, like Freebase [66], Dbpedia\n[67], and YAGO [31] are also derived from Wikipedia. In ad-\ndition, NELL [32] is a continuously improving encyclopedic\nknowledge graph, which automatically extracts knowledge\nfrom the web, and uses that knowledge to improve its per-\nformance over time. There are several encyclopedic knowl-\nedge graphs available in languages other than English such\nas CN-DBpedia [68] and Vikidia [69]. The largest knowledge\ngraph, named Knowledge Occean (KO)7, currently contains\n4,8784,3636 entities and 17,3115,8349 relations in both En-\nglish and Chinese.\n2.2.2 Commonsense Knowledge Graphs.\nCommonsense knowledge graphs formulate the knowledge\nabout daily concepts, e.g., objects, and events, as well\nas their relationships [70]. Compared with encyclopedic\nknowledge graphs, commonsense knowledge graphs often\nmodel the tacit knowledge extracted from text such as (Car,\nUsedFor, Drive) . ConceptNet [71] contains a wide range\nof commonsense concepts and relations, which can help\ncomputers understand the meanings of words people use.\nATOMIC [72], [73] and ASER [74] focus on the causal effects\nbetween events, which can be used for commonsense rea-\nsoning. Some other commonsense knowledge graphs, such\nas TransOMCS [75] and CausalBanK [76] are automatically\nconstructed to provide commonsense knowledge.\n7. https://ko.zhonghuapu.com/\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 5\nLLMs\nKGs\nText\nInput\nStructural Fact\nDomain-speciﬁc Knowledge\nSymbolic-reasoning.... \nOutput KGs\nLLMs\nKG-related\nTasks\nGeneral Knowledge\nLanguage Processing\nGeneralizability\n.... \nOutput\nKGsLLMs\na. KG-enhanced LLMs  b. LLM-augmented KGs  c. Synergized   LLMs + KGs\nFactual Knowledge\nKnowledge Representation\nFig. 6. The general roadmap of unifying KGs and LLMs. (a.) KG-enhanced LLMs. (b.) LLM-augmented KGs. (c.) Synergized LLMs + KGs.\nTABLE 1\nRepresentative applications of using LLMs and KGs.\nName Category LLMs KGs URL\nChatGPT/GPT-4 Chat Bot ✓ https://shorturl.at/cmsE0\nERNIE 3.0 Chat Bot ✓ ✓ https://shorturl.at/sCLV9\nBard Chat Bot ✓ ✓ https://shorturl.at/pDLY6\nFirefly Photo Editing ✓ https://shorturl.at/fkzJV\nAutoGPT AI Assistant ✓ https://shorturl.at/bkoSY\nCopilot Coding Assistant ✓ https://shorturl.at/lKLUV\nNew Bing Web Search ✓ https://shorturl.at/bimps\nShop.ai Recommendation ✓ https://shorturl.at/alCY7\nWikidata Knowledge Base ✓ https://shorturl.at/lyMY5\nKO Knowledge Base ✓ https://shorturl.at/sx238\nOpenBG Recommendation ✓ https://shorturl.at/pDMV9\nDoctor.ai Health Care Assistant✓ ✓ https://shorturl.at/dhlK0\n2.2.3 Domain-specific Knowledge Graphs\nDomain-specific knowledge graphs are often constructed\nto represent knowledge in a specific domain, e.g., medi-\ncal, biology, and finance [23]. Compared with encyclopedic\nknowledge graphs, domain-specific knowledge graphs are\noften smaller in size, but more accurate and reliable. For\nexample, UMLS [77] is a domain-specific knowledge graph\nin the medical domain, which contains biomedical concepts\nand their relationships. In addition, there are some domain-\nspecific knowledge graphs in other domains, such as finance\n[78], geology [79], biology [80], chemistry [81] and geneal-\nogy [82].\n2.2.4 Multi-modal Knowledge Graphs.\nUnlike conventional knowledge graphs that only contain\ntextual information, multi-modal knowledge graphs repre-\nsent facts in multiple modalities such as images, sounds,\nand videos [83]. For example, IMGpedia [84], MMKG [85],\nand Richpedia [86] incorporate both the text and image\ninformation into the knowledge graphs. These knowledge\ngraphs can be used for various multi-modal tasks such as\nimage-text matching [87], visual question answering [88],\nand recommendation [89].\n2.3 Applications\nLLMs as KGs have been widely applied in various\nreal-world applications. We summarize some representa-\ntive applications of using LLMs and KGs in Table 1.\nChatGPT/GPT-4 are LLM-based chatbots that can commu-\nnicate with humans in a natural dialogue format. To im-\nprove knowledge awareness of LLMs, ERNIE 3.0 and Bard\nincorporate KGs into their chatbot applications. Instead of\nChatbot. Firefly develops a photo editing application that\nallows users to edit photos by using natural language de-\nscriptions. Copilot, New Bing, and Shop.ai adopt LLMs to\nempower their applications in the areas of coding assistant,\nweb search, and recommendation, respectively. Wikidata\nand KO are two representative knowledge graph applica-\ntions that are used to provide external knowledge. OpenBG\n[90] is a knowledge graph designed for recommendation.\nDoctor.ai develops a health care assistant that incorporates\nLLMs and KGs to provide medical advice.\n3 R OADMAP & CATEGORIZATION\nIn this section, we first present a road map of explicit\nframeworks that unify LLMs and KGs. Then, we present\nthe categorization of research on unifying LLMs and KGs.\n3.1 Roadmap\nThe roadmap of unifying KGs and LLMs is illustrated in\nFig. 6. In the roadmap, we identify three frameworks for\nthe unification of LLMs and KGs, including KG-enhanced\nLLMs, LLM-augmented KGs, and Synergized LLMs + KGs.\nThe KG-enhanced LLMs and LLM-augmented KGs are two\nparallel frameworks that aim to enhance the capabilities of\nLLMs and KGs, respectively. Building upon these frame-\nworks, Synergized LLMs + KGs is a unified framework that\naims to synergize LLMs and KGs to mutually enhance each\nother.\n3.1.1 KG-enhanced LLMs\nLLMs are renowned for their ability to learn knowledge\nfrom large-scale corpus and achieve state-of-the-art per-\nformance in various NLP tasks. However, LLMs are often\ncriticized for their hallucination issues [15], and lacking of\ninterpretability. To address these issues, researchers have\nproposed to enhance LLMs with knowledge graphs (KGs).\nKGs store enormous knowledge in an explicit and struc-\ntured way, which can be used to enhance the knowledge\nawareness of LLMs. Some researchers have proposed to\nincorporate KGs into LLMs during the pre-training stage,\nwhich can help LLMs learn knowledge from KGs [35], [91].\nOther researchers have proposed to incorporate KGs into\nLLMs during the inference stage. By retrieving knowledge\nfrom KGs, it can significantly improve the performance\nof LLMs in accessing domain-specific knowledge [92]. To\nimprove the interpretability of LLMs, researchers also utilize\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 6\nStructural \nFact \nT ext \nCorpus Image \nKGs LLMs \nExplicit Knowledge\nDomain-speciﬁc Knowledge\nDecisivenessInterpretability\nGeneral Knowledge\nLanguage Processing\nGeneralizability\nPrompt EngineeringGraph Neural Network\nRepresentation LearningNeural-symbolic Reasoning\nIn-context Learning\nFew-shot Learning\nSearch \nEngine \nRecommender \nSystem \nDialogue \nSystem \nAI \nAssistant \nV ideo Data \nSynergized \nModel \nT echnique \nApplication \nFig. 7. The general framework of the Synergized LLMs + KGs , which\ncontains four layers: 1) Data, 2) Synergized Model , 3) Technique, and\n4) Application.\nKGs to interpret the facts [14] and the reasoning process of\nLLMs [38].\n3.1.2 LLM-augmented KGs\nKGs store structure knowledge playing an essential role in\nmany real-word applications [19]. Existing methods in KGs\nfall short of handling incomplete KGs [33] and processing\ntext corpus to construct KGs [93]. With the generalizability\nof LLMs, many researchers are trying to harness the power\nof LLMs for addressing KG-related tasks.\nThe most straightforward way to apply LLMs as text\nencoders for KG-related tasks. Researchers take advantage\nof LLMs to process the textual corpus in the KGs and then\nuse the representations of the text to enrich KGs representa-\ntion [94]. Some studies also use LLMs to process the original\ncorpus and extract relations and entities for KG construction\n[95]. Recent studies try to design a KG prompt that can\neffectively convert structural KGs into a format that can be\ncomprehended by LLMs. In this way, LLMs can be directly\napplied to KG-related tasks, e.g., KG completion [96] and\nKG reasoning [97].\n3.1.3 Synergized LLMs + KGs\nThe synergy of LLMs and KGs has attracted increasing\nattention from researchers these years [40], [42]. LLMs and\nKGs are two inherently complementary techniques, which\nshould be unified into a general framework to mutually\nenhance each other.\nTo further explore the unification, we propose a unified\nframework of the synergized LLMs + KGs in Fig. 7. The\nunified framework contains four layers: 1) Data , 2) Syner-\ngized Model, 3) Technique, and 4) Application. In the Data layer,\nLLMs and KGs are used to process the textual and structural\ndata, respectively. With the development of multi-modal\nLLMs [98] and KGs [99], this framework can be extended\nto process multi-modal data, such as video, audio, and\nimages. In the Synergized Model layer, LLMs and KGs could\nsynergize with each other to improve their capabilities. In\nTechnique layer, related techniques that have been used in\nLLMs and KGs can be incorporated into this framework to\nfurther enhance the performance. In the Application layer,\nLLMs and KGs can be integrated to address various real-\nworld applications, such as search engines [100], recom-\nmender systems [10], and AI assistants [101].\n3.2 Categorization\nTo better understand the research on unifying LLMs and\nKGs, we further provide a fine-grained categorization for\neach framework in the roadmap. Specifically, we focus\non different ways of integrating KGs and LLMs, i.e., KG-\nenhanced LLMs, KG-augmented LLMs, and Synergized\nLLMs + KGs. The fine-grained categorization of the research\nis illustrated in Fig. 8.\nKG-enhanced LLMs. Integrating KGs can enhance the\nperformance and interpretability of LLMs in various down-\nstream tasks. We categorize the research on KG-enhanced\nLLMs into three groups:\n1) KG-enhanced LLM pre-training includes works that\napply KGs during the pre-training stage and im-\nprove the knowledge expression of LLMs.\n2) KG-enhanced LLM inference includes research that\nutilizes KGs during the inference stage of LLMs,\nwhich enables LLMs to access the latest knowledge\nwithout retraining.\n3) KG-enhanced LLM interpretability includes works that\nuse KGs to understand the knowledge learned by\nLLMs and interpret the reasoning process of LLMs.\nLLM-augmented KGs.LLMs can be applied to augment\nvarious KG-related tasks. We categorize the research on\nLLM-augmented KGs into five groups based on the task\ntypes:\n1) LLM-augmented KG embedding includes studies that\napply LLMs to enrich representations of KGs by\nencoding the textual descriptions of entities and\nrelations.\n2) LLM-augmented KG completion includes papers that\nutilize LLMs to encode text or generate facts for\nbetter KGC performance.\n3) LLM-augmented KG construction includes works that\napply LLMs to address the entity discovery, corefer-\nence resolution, and relation extraction tasks for KG\nconstruction.\n4) LLM-augmented KG-to-text Generation includes re-\nsearch that utilizes LLMs to generate natural lan-\nguage that describes the facts from KGs.\n5) LLM-augmented KG question answering includes stud-\nies that apply LLMs to bridge the gap between\nnatural language questions and retrieve answers\nfrom KGs.\nSynergized LLMs + KGs.The synergy of LLMs and KGs\naims to integrate LLMs and KGs into a unified framework\nto mutually enhance each other. In this categorization, we\nreview the recent attempts of Synergized LLMs + KGs from\nthe perspectives of knowledge representation and reasoning.\nIn the following sections (Sec 4, 5, and 6), we will provide\ndetails on these categorizations.\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 7\nLLMs Meet KGs\nKG-enhanced LLMs\nKG-enhanced LLM pre-training\nIntegrating KGs into training objective\nIntegrating KGs into LLM inputs\nKGs Instruction-tuning\nKG-enhanced LLM inference\nRetrieval-augmented knowledge fusion\nKGs Prompting\nKG-enhanced LLM interpretability\nKGs for LLM probing\nKGs for LLM analysis\nLLM-augmented KGs\nLLM-augmented KG emebdding\nLLMs as text encoders\nLLMs for joint text and KG embedding\nLLM-augmented KG completion\nLLMs as encoders\nLLMs as generators\nLLM-augmented KG construction\nEntity discovery\nRelation extraction\nCoreference resolution\nEnd-to-End KG construction\nDistilling KGs from LLMs\nLLM-augmented KG to text generation\nLeveraging knowledge from LLMs\nLLMs for constructing KG-text \naligned Corpus\nLLM-augmented KG question answering\nLLMs as entity/relation extractors\nLLMs as answer reasoners\nSynergized LLMs + KGs \nSynergized Knowledge Representation\nSynergized Reasoning\nLLM-KG fusion reasoning\nLLMs as agents reasoning\nFig. 8. Fine-grained categorization of research on unifying large language models (LLMs) with knowledge graphs (KGs).\n4 KG- ENHANCED LLM S\nLarge language models (LLMs) achieve promising results\nin many natural language processing tasks. However, LLMs\nhave been criticized for their lack of practical knowledge\nand tendency to generate factual errors during inference.\nTo address this issue, researchers have proposed integrating\nknowledge graphs (KGs) to enhance LLMs. In this sec-\ntion, we first introduce the KG-enhanced LLM pre-training,\nwhich aims to inject knowledge into LLMs during the pre-\ntraining stage. Then, we introduce the KG-enhanced LLM\ninference, which enables LLMs to consider the latest knowl-\nedge while generating sentences. Finally, we introduce the\nKG-enhanced LLM interpretability, which aims to improve\nthe interpretability of LLMs by using KGs. Table 2 summa-\nrizes the typical methods that integrate KGs for LLMs.\n4.1 KG-enhanced LLM Pre-training\nExisting large language models mostly rely on unsupervised\ntraining on the large-scale corpus. While these models may\nexhibit impressive performance on downstream tasks, they\noften lack practical knowledge relevant to the real world.\nPrevious works that integrate KGs into large language mod-\nels can be categorized into three parts:1) Integrating KGs into\ntraining objective, 2) Integrating KGs into LLM inputs , and 3)\nKGs Instruction-tuning.\n4.1.1 Integrating KGs into Training Objective\nThe research efforts in this category focus on designing\nnovel knowledge-aware training objectives. An intuitive\nidea is to expose more knowledge entities in the pre-training\nobjective. GLM [102] leverages the knowledge graph struc-\nture to assign a masking probability. Specifically, entities\nthat can be reached within a certain number of hops are\nTABLE 2\nSummary of KG-enhanced LLM methods.\nTask Method Year KG Technique\nKG-enhanced LLM pre-training\nERNIE [35] 2019 E Integrating KGs into Training ObjectiveGLM [102] 2020 C Integrating KGs into Training ObjectiveEbert [103] 2020 D Integrating KGs into Training ObjectiveKEPLER [40] 2021 E Integrating KGs into Training ObjectiveDeterministic LLM [104] 2022E Integrating KGs into Training ObjectiveKALA [105] 2022 D Integrating KGs into Training ObjectiveWKLM [106] 2020 E Integrating KGs into Training Objective\nK-BERT [36] 2020 E+D Integrating KGs into Language Model InputsCoLAKE [107] 2020E Integrating KGs into Language Model InputsERNIE3.0 [101] 2021E+D Integrating KGs into Language Model InputsDkLLM [108] 2022 E Integrating KGs into Language Model Inputs\nKP-PLM [109] 2022E KGs Instruction-tuningOntoPrompt [110] 2022E+D KGs Instruction-tuningChatKBQA [111] 2023E KGs Instruction-tuningRoG [112] 2023 E KGs Instruction-tuning\nKG-enhanced LLM inference\nKGLM [113] 2019 E Retrival-augmented knowledge fusionREALM [114] 2020 E Retrival-augmented knowledge fusionRAG [92] 2020 E Retrival-augmented knowledge fusionEMAT [115] 2022 E Retrival-augmented knowledge fusion\nLi et al. [64] 2023 C KGs PromptingMindmap [65] 2023E+D KGs PromptingChatRule [116] 2023E+D KGs PromptingCoK [117] 2023 E+C+D KGs Prompting\nKG-enhanced LLM interpretability\nLAMA [14] 2019 E KGs for LLM probingLPAQA [118] 2020 E KGs for LLM probingAutoprompt [119] 2020E KGs for LLM probingMedLAMA [120] 2022D KGs for LLM probingLLM-facteval [121] 2023E+D KGs for LLM probing\nKagNet [38] 2019 C KGs for LLM analysisInterpret-lm [122] 2021E KGs for LLM analysisknowledge-neurons [39] 2021E KGs for LLM analysisShaobo et al. [123] 2022E KGs for LLM analysis\nE: Encyclopedic Knowledge Graphs,C: Commonsense Knowledge Graphs,D: Domain-Specific Knowledge Graphs.\nconsidered to be the most important entities for learning,\nand they are given a higher masking probability during\npre-training. Furthermore, E-BERT [103] further controls the\nbalance between the token-level and entity-level training\nlosses. The training loss values are used as indications of the\nlearning process for token and entity, which dynamically de-\ntermines their ratio for the next training epochs. SKEP [124]\nalso follows a similar fusion to inject sentiment knowledge\nduring LLMs pre-training. SKEP first determines words\nwith positive and negative sentiment by utilizing PMI along\nwith a predefined set of seed sentiment words. Then, it\nassigns a higher masking probability to those identified\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 8\nLLMs\nBob Dylanwroteblowin ... 1962 Bob\nDylan\nBlowin’ in\nthe Wind\nInput Text: Bob Dylan wrote Blowin’ in the Wind in 1962\n...\nText Representations Knowledge Graph\nRepresentations\nText Sequence Entitiy\nText-knowledge\nAlignment\nFig. 9. Injecting KG information into LLMs training objective via text-\nknowledge alignment loss, where h denotes the hidden representation\ngenerated by LLMs.\nsentiment words in the word masking objective.\nThe other line of work explicitly leverages the connec-\ntions with knowledge and input text. As shown in Fig. 9,\nERNIE [35] proposes a novel word-entity alignment training\nobjective as a pre-training objective. Specifically, ERNIE\nfeeds both sentences and corresponding entities mentioned\nin the text into LLMs, and then trains the LLMs to pre-\ndict alignment links between textual tokens and entities in\nknowledge graphs. Similarly, KALM [91] enhances the input\ntokens by incorporating entity embeddings and includes\nan entity prediction pre-training task in addition to the\ntoken-only pre-training objective. This approach aims to\nimprove the ability of LLMs to capture knowledge related\nto entities. Finally, KEPLER [40] directly employs both\nknowledge graph embedding training objective and Masked\ntoken pre-training objective into a shared transformer-based\nencoder. Deterministic LLM [104] focuses on pre-training\nlanguage models to capture deterministic factual knowledge.\nIt only masks the span that has a deterministic entity as the\nquestion and introduces additional clue contrast learning\nand clue classification objective. WKLM [106] first replaces\nentities in the text with other same-type entities and then\nfeeds them into LLMs. The model is further pre-trained to\ndistinguish whether the entities have been replaced or not.\n4.1.2 Integrating KGs into LLM Inputs\nAs shown in Fig. 10, this kind of research focus on in-\ntroducing relevant knowledge sub-graph into the inputs\nof LLMs. Given a knowledge graph triple and the corre-\nsponding sentences, ERNIE 3.0 [101] represents the triple as\na sequence of tokens and directly concatenates them with\nthe sentences. It further randomly masks either the relation\ntoken in the triple or tokens in the sentences to better\ncombine knowledge with textual representations. However,\nsuch direct knowledge triple concatenation method allows\nthe tokens in the sentence to intensively interact with the\ntokens in the knowledge sub-graph, which could result in\nKnowledge Noise [36]. To solve this issue, K-BERT [36] takes\nthe first step to inject the knowledge triple into the sentence\nvia a visible matrix where only the knowledge entities have\naccess to the knowledge triple information, while the tokens\nin the sentences can only see each other in the self-attention\nmodule. To further reduce Knowledge Noise , Colake [107]\nproposes a unified word-knowledge graph (shown in Fig.\n10) where the tokens in the input sentences form a fully\nInput Text: Mr. Darcy gives Elizabeth a letter\nMr.\nDarcy\nElizabeth \ngives\na\nletter\nBeloved\nFather Mr.\nBennet\nMother Jane\nLLMs\nMr.\nBennetFather\nBeloved\nMother Jane\nText Graph\nKnowledge Graph\nMr.\nDarcy ... [MASK] Mother [MASK]...\nText\nSequence\nEntity\nSequence\nletter Mr.\nBennet\nMask Text\nPrediction\nMask Entity\nPrediction\nFig. 10. Injecting KG information into LLMs inputs using graph structure.\nconnected word graph where tokens aligned with knowl-\nedge entities are connected with their neighboring entities.\nThe above methods can indeed inject a large amount\nof knowledge into LLMs. However, they mostly focus on\npopular entities and overlook the low-frequent and long-\ntail ones. DkLLM [108] aims to improve the LLMs repre-\nsentations towards those entities. DkLLM first proposes a\nnovel measurement to determine long-tail entities and then\nreplaces these selected entities in the text with pseudo token\nembedding as new input to the large language models.\nFurthermore, Dict-BERT [125] proposes to leverage exter-\nnal dictionaries to solve this issue. Specifically, Dict-BERT\nimproves the representation quality of rare words by ap-\npending their definitions from the dictionary at the end of\ninput text and trains the language model to locally align\nrare word representations in input sentences and dictionary\ndefinitions as well as to discriminate whether the input text\nand definition are correctly mapped.\n4.1.3 KGs Instruction-tuning\nInstead of injecting factual knowledge into LLMs, the KGs\nInstruction-tuning aims to fine-tune LLMs to better com-\nprehend the structure of KGs and effectively follow user\ninstructions to conduct complex tasks. KGs Instruction-\ntuning utilizes both facts and the structure of KGs to cre-\nate instruction-tuning datasets. LLMs finetuned on these\ndatasets can extract both factual and structural knowledge\nfrom KGs, enhancing the reasoning ability of LLMs. KP-\nPLM [109] first designs several prompt templates to transfer\nstructural graphs into natural language text. Then, two self-\nsupervised tasks are proposed to finetune LLMs to further\nleverage the knowledge from these prompts. OntoPrompt\n[110] proposes an ontology-enhanced prompt-tuning that\ncan place knowledge of entities into the context of LLMs,\nwhich are further finetuned on several downstream tasks.\nChatKBQA [111] finetunes LLMs on KG structure to gener-\nate logical queries, which can be executed on KGs to obtain\nanswers. To better reason on graphs, RoG [112] presents a\nplanning-retrieval-reasoning framework. RoG is finetuned\non KG structure to generate relation paths grounded by KGs\nas faithful plans. These plans are then used to retrieve valid\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 9\nreasoning paths from the KGs for LLMs to conduct faithful\nreasoning and generate interpretable results.\nKGs Instruction-tuning can better leverage the knowl-\nedge from KGs for downstream tasks. However, it requires\nretraining the models, which is time-consuming and re-\nquires lots of resources.\n4.2 KG-enhanced LLM Inference\nThe above methods could effectively fuse knowledge into\nLLMs. However, real-world knowledge is subject to change\nand the limitation of these approaches is that they do\nnot permit updates to the incorporated knowledge without\nretraining the model. As a result, they may not generalize\nwell to the unseen knowledge during inference [126]. There-\nfore, considerable research has been devoted to keeping the\nknowledge space and text space separate and injecting the\nknowledge while inference. These methods mostly focus on\nthe Question Answering (QA) tasks, because QA requires\nthe model to capture both textual semantic meanings and\nup-to-date real-world knowledge.\n4.2.1 Retrieval-Augmented Knowledge Fusion\nRetrieval-Augmented Knowledge Fusion is a popular\nmethod to inject knowledge into LLMs during inference.\nThe key idea is to retrieve relevant knowledge from a large\ncorpus and then fuse the retrieved knowledge into LLMs.\nAs shown in Fig. 11, RAG [92] proposes to combine non-\nparametric and parametric modules to handle the external\nknowledge. Given the input text, RAG first searches for rel-\nevant KG in the non-parametric module via MIPS to obtain\nseveral documents. RAG then treats these documents as\nhidden variables z and feeds them into the output generator,\nempowered by Seq2Seq LLMs, as additional context infor-\nmation. The research indicates that using different retrieved\ndocuments as conditions at different generation steps per-\nforms better than only using a single document to guide\nthe whole generation process. The experimental results\nshow that RAG outperforms other parametric-only and\nnon-parametric-only baseline models in open-domain QA.\nRAG can also generate more specific, diverse, and factual\ntext than other parameter-only baselines. Story-fragments\n[127] further improves architecture by adding an additional\nmodule to determine salient knowledge entities and fuse\nthem into the generator to improve the quality of generated\nlong stories. EMAT [115] further improves the efficiency of\nsuch a system by encoding external knowledge into a key-\nvalue memory and exploiting the fast maximum inner prod-\nuct search for memory querying. REALM [114] proposes a\nnovel knowledge retriever to help the model to retrieve and\nattend over documents from a large corpus during the pre-\ntraining stage and successfully improves the performance\nof open-domain question answering. KGLM [113] selects\nthe facts from a knowledge graph using the current context\nto generate factual sentences. With the help of an external\nknowledge graph, KGLM could describe facts using out-of-\ndomain words or phrases.\n4.2.2 KGs Prompting\nTo better feed the KG structure into the LLM during infer-\nence, KGs prompting aims to design a crafted prompt that\nKnowledge\nRetriever LLMs A: USA\nBackpropagation\nKGs\nQ: Which country\nis Obama from?\nRetrieved Facts\n(Obama, BornIn, Honolulu)\n(Honolulu, LocatedIn, USA)\nFig. 11. Retrieving external knowledge to enhance the LLM generation.\nconverts structured KGs into text sequences, which can be\nfed as context into LLMs. In this way, LLMs can better take\nadvantage of the structure of KGs to perform reasoning. Li\net al. [64] adopt the pre-defined template to convert each\ntriple into a short sentence, which can be understood by\nLLMs for reasoning. Mindmap [65] designs a KG prompt to\nconvert graph structure into a mind map that enables LLMs\nto perform reasoning by consolidating the facts in KGs and\nthe implicit knowledge from LLMs. ChatRule [116] sam-\nples several relation paths from KGs, which are verbalized\nand fed into LLMs. Then, LLMs are prompted to generate\nmeaningful logical rules that can be used for reasoning. CoK\n[117] proposes a chain-of-knowledge prompting that uses a\nsequence of triples to elicit the reasoning ability of LLMs to\nreach the final answer.\nKGs prompting presents a simple way to synergize\nLLMs and KGs. By using the prompt, we can easily harness\nthe power of LLMs to perform reasoning based on KGs\nwithout retraining the models. However, the prompt is\nusually designed manually, which requires lots of human\neffort.\n4.3 Comparison between KG-enhanced LLM Pre-\ntraining and Inference\nKG-enhanced LLM Pre-training methods commonly en-\nrich large-amount of unlabeled corpus with semantically\nrelevant real-world knowledge. These methods allow the\nknowledge representations to be aligned with appropri-\nate linguistic context and explicitly train LLMs to lever-\nage those knowledge from scratch. When applying the\nresulting LLMs to downstream knowledge-intensive tasks,\nthey should achieve optimal performance. In contrast, KG-\nenhanced LLM inference methods only present the knowl-\nedge to LLMs in the inference stage and the underlying\nLLMs may not be trained to fully leverage these knowledge\nwhen conducting downstream tasks, potentially resulting in\nsub-optimal model performance.\nHowever, real-world knowledge is dynamic and requires\nfrequent updates. Despite being effective, the KG-enhanced\nLLM Pre-training methods never permit knowledge up-\ndates or editing without model re-training. As a result, the\nKG-enhanced LLM Pre-training methods could generalize\npoorly to recent or unseen knowledge. KG-enhanced LLM\ninference methods can easily maintain knowledge updates\nby changing the inference inputs. These methods help im-\nprove LLMs performance on new knowledge and domains.\nIn summary, when to use these methods depends on the\napplication scenarios. If one wishes to apply LLMs to han-\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 10\nFig. 12. The general framework of using knowledge graph for language\nmodel probing.\ndle time-insensitive knowledge in particular domains (e.g.,\ncommonsense and reasoning knowledge), KG-enhanced\nLLM Pre-training methods should be considered. Other-\nwise, KG-enhanced LLM inference methods can be used to\nhandle open-domain knowledge with frequent updates.\n4.4 KG-enhanced LLM Interpretability\nAlthough LLMs have achieved remarkable success in many\nNLP tasks, they are still criticized for their lack of inter-\npretability. The large language model (LLM) interpretability\nrefers to the understanding and explanation of the inner\nworkings and decision-making processes of a large lan-\nguage model [17]. This can improve the trustworthiness of\nLLMs and facilitate their applications in high-stakes scenar-\nios such as medical diagnosis and legal judgment. Knowl-\nedge graphs (KGs) represent the knowledge structurally and\ncan provide good interpretability for the reasoning results.\nTherefore, researchers try to utilize KGs to improve the\ninterpretability of LLMs, which can be roughly grouped into\ntwo categories: 1) KGs for language model probing, and 2) KGs\nfor language model analysis.\n4.4.1 KGs for LLM Probing\nThe large language model (LLM) probing aims to under-\nstand the knowledge stored in LLMs. LLMs, trained on\nlarge-scale corpus, are often known as containing enor-\nmous knowledge. However, LLMs store the knowledge in\na hidden way, making it hard to figure out the stored\nknowledge. Moreover, LLMs suffer from the hallucination\nproblem [15], which results in generating statements that\ncontradict facts. This issue significantly affects the reliability\nof LLMs. Therefore, it is necessary to probe and verify the\nknowledge stored in LLMs.\nLAMA [14] is the first work to probe the knowledge\nin LLMs by using KGs. As shown in Fig. 12, LAMA first\nconverts the facts in KGs into cloze statements by a pre-\ndefined prompt template and then uses LLMs to predict the\nmissing entity. The prediction results are used to evaluate\nthe knowledge stored in LLMs. For example, we try to\nprobe whether LLMs know the fact (Obama, profession, pres-\nident). We first convert the fact triple into a cloze question\n“Obama’s profession is .” with the object masked. Then, we\ntest if the LLMs can predict the object “president” correctly.\nHowever, LAMA ignores the fact that the prompts are\ninappropriate. For example, the prompt “Obama worked as\na ” may be more favorable to the prediction of the blank\nby the language models than “Obama is a by profession” .\nFig. 13. The general framework of using knowledge graph for language\nmodel analysis.\nThus, LPAQA [118] proposes a mining and paraphrasing-\nbased method to automatically generate high-quality and\ndiverse prompts for a more accurate assessment of the\nknowledge contained in the language model. Moreover,\nAdolphs et al. [128] attempt to use examples to make the\nlanguage model understand the query, and experiments\nobtain substantial improvements for BERT-large on the T-\nREx data. Unlike using manually defined prompt templates,\nAutoprompt [119] proposes an automated method, which\nis based on the gradient-guided search to create prompts.\nLLM-facteval [121] designs a systematic framework that\nautomatically generates probing questions from KGs. The\ngenerated questions are then used to evaluate the factual\nknowledge stored in LLMs.\nInstead of probing the general knowledge by using\nthe encyclopedic and commonsense knowledge graphs,\nBioLAMA [129] and MedLAMA [120] probe the medical\nknowledge in LLMs by using medical knowledge graphs.\nAlex et al. [130] investigate the capacity of LLMs to re-\ntain less popular factual knowledge. They select unpopular\nfacts from Wikidata knowledge graphs which have low-\nfrequency clicked entities. These facts are then used for the\nevaluation, where the results indicate that LLMs encounter\ndifficulties with such knowledge, and that scaling fails to\nappreciably improve memorization of factual knowledge in\nthe tail.\n4.4.2 KGs for LLM Analysis\nKnowledge graphs (KGs) for pre-train language models\n(LLMs) analysis aims to answer the following questions\nsuch as “how do LLMs generate the results?”, and “how do\nthe function and structure work in LLMs?”. To analyze the\ninference process of LLMs, as shown in Fig. 13, KagNet [38]\nand QA-GNN [131] make the results generated by LLMs\nat each reasoning step grounded by knowledge graphs. In\nthis way, the reasoning process of LLMs can be explained\nby extracting the graph structure from KGs. Shaobo et al.\n[123] investigate how LLMs generate the results correctly.\nThey adopt the causal-inspired analysis from facts extracted\nfrom KGs. This analysis quantitatively measures the word\npatterns that LLMs depend on to generate the results. The\nresults show that LLMs generate the missing factual more\nby the positionally closed words rather than the knowledge-\ndependent words. Thus, they claim that LLMs are inade-\nquate to memorize factual knowledge because of the inaccu-\nrate dependence. To interpret the training of LLMs, Swamy\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 11\nTABLE 3\nSummary of representative LLM-augmented KG methods.\nTask Method Year LLM Technique\nLLM-augmented KG embedding\nPretrain-KGE [94] 2020E LLMs as Text EncodersKEPLER [40] 2020E LLMs as Text EncodersNayyeri et al. [132] 2022E LLMs as Text EncodersHuang et al. [133] 2022E LLMs as Text EncodersCoDEx [134] 2022 E LLMs as Text Encoders\nLMKE [135] 2022 E LLMs for Joint Text and KG EmbeddingkNN-KGE [136] 2022E LLMs for Joint Text and KG EmbeddingLambdaKG [137] 2023E+D+EDLLMs for Joint Text and KG Embedding\nLLM-augmented KG completion\nKG-BERT [26] 2019E Joint EncodingMTL-KGC [138] 2020E Joint EncodingPKGC [139] 2022 E Joint EncodingLASS [140] 2022 E Joint Encoding\nMEM-KGC [141] 2021E MLM EncodingOpenWorld KGC [142] 2023E MLM Encoding\nStAR [143] 2021 E Separated EncodingSimKGC [144] 2022E Separated EncodingLP-BERT [145] 2022E Separated Encoding\nGenKGC [96] 2022ED LLM as decodersKGT5 [146] 2022 ED LLM as decodersKG-S2S [147] 2022ED LLM as decodersAutoKG [93] 2023 D LLM as decoders\nLLM-augmented KG construction\nELMO [148] 2018 E Named Entity RecognitionGenerativeNER [149] 2021ED Named Entity RecognitionLDET [150] 2019 E Entity TypingBOX4Types [151] 2021E Entity TypingELQ [152] 2020 E Entity LinkingReFinED [153] 2022E Entity Linking\nBertCR [154] 2019 E CR (Within-document)Spanbert [155] 2020E CR (Within-document)CDLM [156] 2021 E CR (Cross-document)CrossCR [157] 2021E CR (Cross-document)CR-RL [158] 2021 E CR (Cross-document)\nSentRE [159] 2019 E RE (Sentence-level)Curriculum-RE [160] 2021E RE (Sentence-level)DREEAM [161] 2023E RE (Document-level)\nKumar et al. [95] 2020E End-to-End ConstructionGuo et al. [162] 2021E End-to-End ConstructionGrapher [41] 2021 ED End-to-End ConstructionPiVE [163] 2023 D+ED End-to-End Construction\nCOMET [164] 2019D Distilling KGs from LLMsBertNet [165] 2022E Distilling KGs from LLMsWest et al. [166] 2022D Distilling KGs from LLMs\nLLM-augmented KG-to-text Generation\nRibeiro et al [167] 2021ED Leveraging Knowledge from LLMsJointGT [42] 2021 ED Leveraging Knowledge from LLMsFSKG2Text [168] 2021D+ED Leveraging Knowledge from LLMsGAP [169] 2022 ED Leveraging Knowledge from LLMs\nGenWiki [170] 2020 - Constructing KG-text aligned CorpusKGPT [171] 2020 ED Constructing KG-text aligned Corpus\nLLM-augmented KGQA\nLukovnikov et al. [172] 2019E Entity/Relation ExtractorLuo et al. [173] 2020E Entity/Relation ExtractorQA-GNN [131] 2021E Entity/Relation ExtractorNan et al. [174] 2023E+D+EDEntity/Relation Extractor\nDEKCOR [175] 2021E Answer ReasonerDRLK [176] 2022 E Answer ReasonerOreoLM [177] 2022E Answer ReasonerGreaseLM [178] 2022E Answer ReasonerReLMKG [179] 2022E Answer ReasonerUniKGQA [43] 2023E Answer Reasoner\nE: Encoder-only LLMs,D: Decoder-only LLMs,ED: Encoder-decoder LLMs.\net al. [122] adopt the language model during pre-training\nto generate knowledge graphs. The knowledge acquired by\nLLMs during training can be unveiled by the facts in KGs\nexplicitly. To explore how implicit knowledge is stored in\nparameters of LLMs, Dai et al. [39] propose the concept of\nknowledge neurons . Specifically, activation of the identified\nknowledge neurons is highly correlated with knowledge\nexpression. Thus, they explore the knowledge and facts\nrepresented by each neuron by suppressing and amplifying\nknowledge neurons.\n5 LLM- AUGMENTED KGS\nKnowledge graphs are famous for representing knowledge\nin a structural manner. They have been applied in many\ndownstream tasks such as question answering, recommen-\ndation, and web search. However, the conventional KGs\nare often incomplete and existing methods often lack con-\nsidering textual information. To address these issues, re-\ncent research has explored integrating LLMs to augment\nKGs to consider the textual information and improve the\nperformance in downstream tasks. In this section, we will\nintroduce the recent research on LLM-augmented KGs. We\nwill introduce the methods that integrate LLMs for KG\nembedding, KG completion, KG construction, KG-to-text\ngeneration, and KG question answering, respectively. Rep-\nresentative works are summarized in Table 3.\nKGs\nNeil Armstrong Wapakoneta,( )\nAn American astronaut and\naeronautical engineer. A small city in Ohio, USA.\nLLMs\nText Text\nKGE Models\nKGE Training\nText\nBornIn,\nFig. 14. LLMs as text encoder for knowledge graph embedding (KGE).\n5.1 LLM-augmented KG Embedding\nKnowledge graph embedding (KGE) aims to map each\nentity and relation into a low-dimensional vector (embed-\nding) space. These embeddings contain both semantic and\nstructural information of KGs, which can be utilized for\nvarious tasks such as question answering [180], reasoning\n[38], and recommendation [181]. Conventional knowledge\ngraph embedding methods mainly rely on the structural\ninformation of KGs to optimize a scoring function de-\nfined on embeddings (e.g., TransE [33], and DisMult [182]).\nHowever, these approaches often fall short in representing\nunseen entities and long-tailed relations due to their limited\nstructural connectivity [183], [184]. To address this issue, as\nshown in Fig. 14, recent research adopts LLMs to enrich\nrepresentations of KGs by encoding the textual descriptions\nof entities and relations [40], [94].\n5.1.1 LLMs as Text Encoders\nPretrain-KGE [94] is a representative method that follows\nthe framework shown in Fig. 14. Given a triple (h, r, t) from\nKGs, it firsts uses a LLM encoder to encode the textual de-\nscriptions of entities h, t, and relations r into representations\nas\neh = LLM(Texth), et = LLM(Textt), er = LLM(Textr), (1)\nwhere eh, er, and et denotes the initial embeddings of enti-\nties h, t, and relations r, respectively. Pretrain-KGE uses the\nBERT as the LLM encoder in experiments. Then, the initial\nembeddings are fed into a KGE model to generate the final\nembeddings vh, vr, and vt. During the KGE training phase,\nthey optimize the KGE model by following the standard\nKGE loss function as\nL = [γ + f(vh, vr, vt) − f(v′\nh, v′\nr, v′\nt)], (2)\nwhere f is the KGE scoring function, γ is a margin hy-\nperparameter, and v′\nh, v′\nr, and v′\nt are the negative samples.\nIn this way, the KGE model could learn adequate struc-\nture information, while reserving partial knowledge from\nLLM enabling better knowledge graph embedding. KEPLER\n[40] offers a unified model for knowledge embedding and\npre-trained language representation. This model not only\ngenerates effective text-enhanced knowledge embedding\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 12\nNeil Armstrong BornIn[CLS] [SEP] [MASK]\nLLMs\nWapakoneta\n( Neil Armstrong, BornIn, Wapakoneta)\nText [SEP]\nMask Entity\nPrediction\nKGs\nText\nFig. 15. LLMs for joint text and knowledge graph embedding.\nusing powerful LLMs but also seamlessly integrates factual\nknowledge into LLMs. Nayyeri et al. [132] use LLMs to gen-\nerate the world-level, sentence-level, and document-level\nrepresentations. They are integrated with graph structure\nembeddings into a unified vector by Dihedron and Quater-\nnion representations of 4D hypercomplex numbers. Huang\net al. [133] combine LLMs with other vision and graph\nencoders to learn multi-modal knowledge graph embedding\nthat enhances the performance of downstream tasks. CoDEx\n[134] presents a novel loss function empowered by LLMs\nthat guides the KGE models in measuring the likelihood of\ntriples by considering the textual information. The proposed\nloss function is agnostic to model structure that can be\nincorporated with any KGE model.\n5.1.2 LLMs for Joint Text and KG Embedding\nInstead of using KGE model to consider graph structure,\nanother line of methods directly employs LLMs to incorpo-\nrate both the graph structure and textual information into\nthe embedding space simultaneously. As shown in Fig. 15,\nkNN-KGE [136] treats the entities and relations as special\ntokens in the LLM. During training, it transfers each triple\n(h, r, t) and corresponding text descriptions into a sentence\nx as\nx = [CLS] h Texth[SEP] r [SEP] [MASK] Textt[SEP],\n(3)\nwhere the tailed entities are replaced by [MASK]. The sen-\ntence is fed into a LLM, which then finetunes the model to\npredict the masked entity, formulated as\nPLLM (t|h, r) = P([MASK]=t|x, Θ), (4)\nwhere Θ denotes the parameters of the LLM. The LLM is\noptimized to maximize the probability of the correct entity\nt. After training, the corresponding token representations\nin LLMs are used as embeddings for entities and rela-\ntions. Similarly, LMKE [135] proposes a contrastive learning\nmethod to improve the learning of embeddings generated\nby LLMs for KGE. Meanwhile, to better capture graph\nstructure, LambdaKG [137] samples 1-hop neighbor entities\nand concatenates their tokens with the triple as a sentence\nfeeding into LLMs.\n5.2 LLM-augmented KG Completion\nKnowledge Graph Completion (KGC) refers to the task of\ninferring missing facts in a given knowledge graph. Similar\nto KGE, conventional KGC methods mainly focused on\nthe structure of the KG, without considering the exten-\nsive textual information. However, the recent integration of\nLLMs enables KGC methods to encode text or generate facts\nfor better KGC performance. These methods fall into two\ndistinct categories based on their utilization styles: 1) LLM\nas Encoders (PaE), and 2) LLM as Generators (PaG).\n5.2.1 LLM as Encoders (PaE).\nAs shown in Fig. 16 (a), (b), and (c), this line of work\nfirst uses encoder-only LLMs to encode textual information\nas well as KG facts. Then, they predict the plausibility\nof the triples or masked entities by feeding the encoded\nrepresentation into a prediction head, which could be a\nsimple MLP or conventional KG score function (e.g., TransE\n[33] and TransR [185]).\nJoint Encoding.Since the encoder-only LLMs (e.g., Bert\n[1]) are well at encoding text sequences, KG-BERT [26]\nrepresents a triple (h, r, t) as a text sequence and encodes\nit with LLM Fig. 16(a).\nx = [CLS] Texth [SEP] Textr [SEP] Textt [SEP], (5)\nThe final hidden state of the [CLS] token is fed into a\nclassifier to predict the possibility of the triple, formulated\nas\ns = σ(MLP(e[CLS])), (6)\nwhere σ(·) denotes the sigmoid function and e[CLS] de-\nnotes the representation encoded by LLMs. To improve the\nefficacy of KG-BERT, MTL-KGC [138] proposed a Multi-\nTask Learning for the KGC framework which incorporates\nadditional auxiliary tasks into the model’s training, i.e.\nprediction (RP) and relevance ranking (RR). PKGC [139]\nassesses the validity of a triplet (h, r, t) by transforming the\ntriple and its supporting information into natural language\nsentences with pre-defined templates. These sentences are\nthen processed by LLMs for binary classification. The sup-\nporting information of the triplet is derived from the at-\ntributes of h and t with a verbalizing function. For instance,\nif the triple is (Lebron James, member of sports team, Lakers) ,\nthe information regarding Lebron James is verbalized as\n”Lebron James: American basketball player”. LASS [140]\nobserves that language semantics and graph structures are\nequally vital to KGC. As a result, LASS is proposed to\njointly learn two types of embeddings: semantic embedding\nand structure embedding. In this method, the full text of a\ntriple is forwarded to the LLM, and the mean pooling of the\ncorresponding LLM outputs for h, r, and t are separately\ncalculated. These embeddings are then passed to a graph-\nbased method, i.e. TransE, to reconstruct the KG structures.\nMLM Encoding. Instead of encoding the full text of a\ntriple, many works introduce the concept of Masked Lan-\nguage Model (MLM) to encode KG text (Fig. 16(b)). MEM-\nKGC [141] uses Masked Entity Model (MEM) classification\nmechanism to predict the masked entities of the triple. The\ninput text is in the form of\nx = [CLS] Texth [SEP] Textr [SEP] [MASK] [SEP], (7)\nSimilar to Eq. 4, it tries to maximize the probability that the\nmasked entity is the correct entity t. Additionally, to enable\nthe model to learn unseen entities, MEM-KGC integrates\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 13\n[CLS] Text [SEP] Text [SEP] [MASK] [SEP]\n[CLS] Text [SEP] Text [SEP] Text [SEP]\n[CLS] Text [SEP] Text [SEP] [CLS] Text [SEP] \nLLMs\nLLMs LLMs\nLLMs\n(a) Joint Encoding\n(b) MLM Encoding \n(c) Separated Encoding\nMLP 0/1\nMLP Entity\nScore Function Score\nTriple: \nText Sequence: [CLS] Text [SEP] Text [SEP] Text [SEP]\nFig. 16. The general framework of adopting LLMs as encoders (PaE) for\nKG Completion.\nmultitask learning for entities and super-class prediction\nbased on the text description of entities:\nx = [CLS] [MASK] [SEP]Texth [SEP]. (8)\nOpenWorld KGC [142] expands the MEM-KGC model to\naddress the challenges of open-world KGC with a pipeline\nframework, where two sequential MLM-based modules are\ndefined: Entity Description Prediction (EDP), an auxiliary\nmodule that predicts a corresponding entity with a given\ntextual description; Incomplete Triple Prediction (ITP), the\ntarget module that predicts a plausible entity for a given\nincomplete triple (h, r,?). EDP first encodes the triple with\nEq. 8 and generates the final hidden state, which is then\nforwarded into ITP as an embedding of the head entity in\nEq. 7 to predict target entities.\nSeparated Encoding.As shown in Fig. 16(c), these meth-\nods involve partitioning a triple (h, r, t) into two distinct\nparts, i.e. (h, r) and t, which can be expressed as\nx(h,r) = [CLS] Texth [SEP] Textr [SEP], (9)\nxt = [CLS] Textt [SEP]. (10)\nThen the two parts are encoded separately by LLMs, and the\nfinal hidden states of the [CLS] tokens are used as the rep-\nresentations of (h, r) and t, respectively. The representations\nare then fed into a scoring function to predict the possibility\nof the triple, formulated as\ns = fscore(e(h,r), et), (11)\nwhere fscore denotes the score function like TransE.\nStAR [143] applies Siamese-style textual encoders on\ntheir text, encoding them into separate contextualized rep-\nresentations. To avoid the combinatorial explosion of textual\nencoding approaches, e.g., KG-BERT, StAR employs a scor-\ning module that involves both deterministic classifier and\nspatial measurement for representation and structure learn-\ning respectively, which also enhances structured knowledge\nby exploring the spatial characteristics. SimKGC [144] is\nLLMs (En.)\n[SEP] Text [SEP] Text [SEP]\n[SEP] Text [SEP] \n(a) Encoder-Decoder PaG  \nLLMs (De.)\n(a) Decoder-Only PaG  \n[SEP] Text [SEP] Text [SEP]\nLLMs (De.)\n[SEP] Text [SEP] \nQuery Triple: \nText Sequence: [CLS] Text [SEP] Text [SEP]\nFig. 17. The general framework of adopting LLMs as decoders (PaG)\nfor KG Completion. The En. and De. denote the encoder and decoder,\nrespectively.\nanother instance of leveraging a Siamese textual encoder\nto encode textual representations. Following the encoding\nprocess, SimKGC applies contrastive learning techniques to\nthese representations. This process involves computing the\nsimilarity between the encoded representations of a given\ntriple and its positive and negative samples. In particular,\nthe similarity between the encoded representation of the\ntriple and the positive sample is maximized, while the sim-\nilarity between the encoded representation of the triple and\nthe negative sample is minimized. This enables SimKGC\nto learn a representation space that separates plausible\nand implausible triples. To avoid overfitting textural in-\nformation, CSPromp-KG [186] employs parameter-efficient\nprompt learning for KGC.\nLP-BERT [145] is a hybrid KGC method that combines\nboth MLM Encoding and Separated Encoding. This ap-\nproach consists of two stages, namely pre-training and\nfine-tuning. During pre-training, the method utilizes the\nstandard MLM mechanism to pre-train a LLM with KGC\ndata. During the fine-tuning stage, the LLM encodes both\nparts and is optimized using a contrastive learning strategy\n(similar to SimKGC [144]).\n5.2.2 LLM as Generators (PaG).\nRecent works use LLMs as sequence-to-sequence generators\nin KGC. As presented in Fig. 17 (a) and (b), these approaches\ninvolve encoder-decoder or decoder-only LLMs. The LLMs\nreceive a sequence text input of the query triple(h, r,?), and\ngenerate the text of tail entity t directly.\nGenKGC [96] uses the large language model BART [5]\nas the backbone model. Inspired by the in-context learning\napproach used in GPT-3 [59], where the model concatenates\nrelevant samples to learn correct output answers, GenKGC\nproposes a relation-guided demonstration technique that\nincludes triples with the same relation to facilitating the\nmodel’s learning process. In addition, during generation,\nan entity-aware hierarchical decoding method is proposed\nto reduce the time complexity. KGT5 [146] introduces a\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 14\nLLMs \nGiven head entity and relation, predict the tail\nentity from the candidates: [ 100 candidates ]\nHead: Charlie's Angels\nRelation: genre of\nTail: Comedy-GB\nHead: Charlie's Angels\nRelation: prequel of\nTail:\nCharlie's Angels: Full Throttle\nFig. 18. The framework of prompt-based PaG for KG Completion.\nnovel KGC model that fulfils four key requirements of\nsuch models: scalability, quality, versatility, and simplicity.\nTo address these objectives, the proposed model employs a\nstraightforward T5 small architecture. The model is distinct\nfrom previous KGC methods, in which it is randomly ini-\ntialized rather than using pre-trained models. KG-S2S [147]\nis a comprehensive framework that can be applied to var-\nious types of KGC tasks, including Static KGC, Temporal\nKGC, and Few-shot KGC. To achieve this objective, KG-S2S\nreformulates the standard triple KG fact by introducing an\nadditional element, forming a quadruple (h, r, t, m), where\nm represents the additional ”condition” element. Although\ndifferent KGC tasks may refer to different conditions, they\ntypically have a similar textual format, which enables uni-\nfication across different KGC tasks. The KG-S2S approach\nincorporates various techniques such as entity description,\nsoft prompt, and Seq2Seq Dropout to improve the model’s\nperformance. In addition, it utilizes constrained decoding\nto ensure the generated entities are valid. For closed-source\nLLMs (e.g., ChatGPT and GPT-4), AutoKG adopts prompt\nengineering to design customized prompts [93]. As shown\nin Fig. 18, these prompts contain the task description, few-\nshot examples, and test input, which instruct LLMs to\npredict the tail entity for KG completion.\nComparison between PaE and PaG.LLMs as Encoders\n(PaE) applies an additional prediction head on the top of\nthe representation encoded by LLMs. Therefore, the PaE\nframework is much easier to finetune since we can only\noptimize the prediction heads and freeze the LLMs. More-\nover, the output of the prediction can be easily specified\nand integrated with existing KGC functions for different\nKGC tasks. However, during the inference stage, the PaE\nrequires to compute a score for every candidate in KGs,\nwhich could be computationally expensive. Besides, they\ncannot generalize to unseen entities. Furthermore, the PaE\nrequires the representation output of the LLMs, whereas\nsome state-of-the-art LLMs (e.g. GPT-4 1) are closed sources\nand do not grant access to the representation output.\nLLMs as Generators (PaG), on the other hand, which\ndoes not need the prediction head, can be used without\nfinetuning or access to representations. Therefore, the frame-\nwork of PaG is suitable for all kinds of LLMs. In addition,\nPaG directly generates the tail entity, making it efficient\nin inference without ranking all the candidates and easily\ngeneralizing to unseen entities. But, the challenge of PaG is\nthat the generated entities could be diverse and not lie in\nKGs. What is more, the time of a single inference is longer\ndue to the auto-regressive generation. Last, how to design\na powerful prompt that feeds KGs into LLMs is still an\nopen question. Consequently, while PaG has demonstrated\npromising results for KGC tasks, the trade-off between\nmodel complexity and computational efficiency must be\ncarefully considered when selecting an appropriate LLM-\nbased KGC framework.\n5.2.3 Model Analysis\nJustin et al. [187] provide a comprehensive analysis of KGC\nmethods integrated with LLMs. Their research investigates\nthe quality of LLM embeddings and finds that they are\nsuboptimal for effective entity ranking. In response, they\npropose several techniques for processing embeddings to\nimprove their suitability for candidate retrieval. The study\nalso compares different model selection dimensions, such as\nEmbedding Extraction, Query Entity Extraction, and Lan-\nguage Model Selection. Lastly, the authors propose a frame-\nwork that effectively adapts LLM for knowledge graph\ncompletion.\n5.3 LLM-augmented KG Construction\nKnowledge graph construction involves creating a struc-\ntured representation of knowledge within a specific domain.\nThis includes identifying entities and their relationships\nwith each other. The process of knowledge graph construc-\ntion typically involves multiple stages, including 1) entity\ndiscovery, 2) coreference resolution , and 3) relation extraction .\nFig 19 presents the general framework of applying LLMs for\neach stage in KG construction. More recent approaches have\nexplored 4) end-to-end knowledge graph construction , which\ninvolves constructing a complete knowledge graph in one\nstep or directly 5) distilling knowledge graphs from LLMs .\n5.3.1 Entity Discovery\nEntity discovery in KG construction refers to the process of\nidentifying and extracting entities from unstructured data\nsources, such as text documents, web pages, or social me-\ndia posts, and incorporating them to construct knowledge\ngraphs.\nNamed Entity Recognition (NER)involves identifying\nand tagging named entities in text data with their positions\nand classifications. The named entities include people, or-\nganizations, locations, and other types of entities. The state-\nof-the-art NER methods usually employ LLMs to leverage\ntheir contextual understanding and linguistic knowledge\nfor accurate entity recognition and classification. There are\nthree NER sub-tasks based on the types of NER spans\nidentified, i.e., flat NER, nested NER, and discontinuous\nNER. 1) Flat NER is to identify non-overlapping named entities\nfrom input text. It is usually conceptualized as a sequence\nlabelling problem where each token in the text is assigned\na unique label based on its position in the sequence [1],\n[148], [188], [189]. 2) Nested NER considers complex scenarios\nwhich allow a token to belong to multiple entities. The span-\nbased method [190]–[194] is a popular branch of nested\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 15\nText: Joe Biden was born in Pennsylvania. He serves as the 46th President\nof the United States.\n... ...\nUnited States\nPennsylvaniaJoe Biden\npolitician state\ncountry\nBornIn\nPresidentOf\nIsA politician\ncountryIsA state\nKnowledge Graph\nIsA\nLLM-based Knowledge Graph Construction\nNamed Entity\nRecognition\nEntity\nTyping\nEntity\nLinking\nCoreference\nResolution\nRelation\nExtraction\nJoe Biden was born in Pennsylvania. He  serves as the 46th President of\nthe United States.\nFig. 19. The general framework of LLM-based KG construction.\nNER which involves enumerating all candidate spans and\nclassifying them into entity types (including a non-entity\ntype). Parsing-based methods [195]–[197] reveal similarities\nbetween nested NER and constituency parsing tasks (pre-\ndicting nested and non-overlapping spans), and propose to\nintegrate the insights of constituency parsing into nested\nNER. 3) Discontinuous NER identifies named entities that may\nnot be contiguous in the text. To address this challenge, [198]\nuses the LLM output to identify entity fragments and deter-\nmine whether they are overlapped or in succession.\nUnlike the task-specific methods, GenerativeNER [149]\nuses a sequence-to-sequence LLM with a pointer mecha-\nnism to generate an entity sequence, which is capable of\nsolving all three types of NER sub-tasks.\nEntity Typing (ET) aims to provide fine-grained and\nultra-grained type information for a given entity men-\ntioned in context. These methods usually utilize LLM to\nencode mentions, context and types. LDET [150] applies pre-\ntrained ELMo embeddings [148] for word representation\nand adopts LSTM as its sentence and mention encoders.\nBOX4Types [151] recognizes the importance of type depen-\ndency and uses BERT to represent the hidden vector and\neach type in a hyperrectangular (box) space. LRN [199]\nconsiders extrinsic and intrinsic dependencies between la-\nbels. It encodes the context and entity with BERT and\nemploys these output embeddings to conduct deductive\nand inductive reasoning. MLMET [200] uses predefined\npatterns to construct input samples for the BERT MLM and\nemploys [MASK] to predict context-dependent hypernyms\nof the mention, which can be viewed as type labels. PL [201]\nand DFET [202] utilize prompt learning for entity typing.\nLITE [203] formulates entity typing as textual inference and\nuses RoBERTa-large-MNLI as the backbone network.\nEntity Linking (EL), as known as entity disambiguation,\ninvolves linking entity mentions appearing in the text to\ntheir corresponding entities in a knowledge graph. [204]\nproposed BERT-based end-to-end EL systems that jointly\ndiscover and link entities. ELQ [152] employs a fast bi-\nencoder architecture to jointly perform mention detection\nand linking in one pass for downstream question answering\nsystems. Unlike previous models that frame EL as matching\nin vector space, GENRE [205] formulates it as a sequence-to-\nsequence problem, autoregressively generating a version of\nthe input markup-annotated with the unique identifiers of\nan entity expressed in natural language. GENRE is extended\nto its multilingual version mGENRE [206]. Considering the\nefficiency challenges of generative EL approaches, [207] par-\nallelizes autoregressive linking across all potential mentions\nand relies on a shallow and efficient decoder. ReFinED [153]\nproposes an efficient zero-shot-capable EL approach by\ntaking advantage of fine-grained entity types and entity\ndescriptions which are processed by a LLM-based encoder.\n5.3.2 Coreference Resolution (CR)\nCoreference resolution is to find all expressions (i.e., men-\ntions) that refer to the same entity or event in a text.\nWithin-document CRrefers to the CR sub-task where all\nthese mentions are in a single document. Mandar et al. [154]\ninitialize LLM-based coreferences resolution by replacing\nthe previous LSTM encoder [208] with BERT. This work is\nfollowed by the introduction of SpanBERT [155] which is\npre-trained on BERT architecture with a span-based masked\nlanguage model (MLM). Inspired by these works, Tuan\nManh et al. [209] present a strong baseline by incorporat-\ning the SpanBERT encoder into a non-LLM approach e2e-\ncoref [208]. CorefBERT leverages Mention Reference Predic-\ntion (MRP) task which masks one or several mentions and\nrequires the model to predict the masked mention’s corre-\nsponding referents. CorefQA [210] formulates coreference\nresolution as a question answering task, where contextual\nqueries are generated for each candidate mention and the\ncoreferent spans are extracted from the document using the\nqueries. Tuan Manh et al. [211] introduce a gating mech-\nanism and a noisy training method to extract information\nfrom event mentions using the SpanBERT encoder.\nIn order to reduce the large memory footprint faced\nby large LLM-based NER models, Yuval et al. [212] and\nRaghuveer el al. [213] proposed start-to-end and approxima-\ntion models, respectively, both utilizing bilinear functions\nto calculate mention and antecedent scores with reduced\nreliance on span-level representations.\nCross-document CR refers to the sub-task where the\nmentions refer to the same entity or event might be across\nmultiple documents. CDML [156] proposes a cross docu-\nment language modeling method which pre-trains a Long-\nformer [214] encoder on concatenated related documents\nand employs an MLP for binary classification to determine\nwhether a pair of mentions is coreferent or not. CrossCR\n[157] utilizes an end-to-end model for cross-document coref-\nerence resolution which pre-trained the mention scorer on\ngold mention spans and uses a pairwise scorer to compare\nmentions with all spans across all documents. CR-RL [158]\nproposes an actor-critic deep reinforcement learning-based\ncoreference resolver for cross-document CR.\n5.3.3 Relation Extraction (RE)\nRelation extraction involves identifying semantic relation-\nships between entities mentioned in natural language text.\nThere are two types of relation extraction methods, i.e.\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 16\nsentence-level RE and document-level RE, according to the\nscope of the text analyzed.\nSentence-level RE focuses on identifying relations be-\ntween entities within a single sentence. Peng et al. [159] and\nTRE [215] introduce LLM to improve the performance of\nrelation extraction models. BERT-MTB [216] learns relation\nrepresentations based on BERT by performing the matching-\nthe-blanks task and incorporating designed objectives for\nrelation extraction. Curriculum-RE [160] utilizes curriculum\nlearning to improve relation extraction models by gradu-\nally increasing the difficulty of the data during training.\nRECENT [217] introduces SpanBERT and exploits entity\ntype restriction to reduce the noisy candidate relation types.\nJiewen [218] extends RECENT by combining both the entity\ninformation and the label information into sentence-level\nembeddings, which enables the embedding to be entity-\nlabel aware.\nDocument-level RE (DocRE) aims to extract relations\nbetween entities across multiple sentences within a docu-\nment. Hong et al. [219] propose a strong baseline for DocRE\nby replacing the BiLSTM backbone with LLMs. HIN [220]\nuse LLM to encode and aggregate entity representation at\ndifferent levels, including entity, sentence, and document\nlevels. GLRE [221] is a global-to-local network, which uses\nLLM to encode the document information in terms of entity\nglobal and local representations as well as context relation\nrepresentations. SIRE [222] uses two LLM-based encoders to\nextract intra-sentence and inter-sentence relations. LSR [223]\nand GAIN [224] propose graph-based approaches which\ninduce graph structures on top of LLM to better extract\nrelations. DocuNet [225] formulates DocRE as a semantic\nsegmentation task and introduces a U-Net [226] on the LLM\nencoder to capture local and global dependencies between\nentities. ATLOP [227] focuses on the multi-label problems\nin DocRE, which could be handled with two techniques,\ni.e., adaptive thresholding for classifier and localized con-\ntext pooling for LLM. DREEAM [161] further extends and\nimproves ATLOP by incorporating evidence information.\nEnd-to-End KG Construction. Currently, researchers are\nexploring the use of LLMs for end-to-end KG construction.\nKumar et al. [95] propose a unified approach to build\nKGs from raw text, which contains two LLMs powered\ncomponents. They first finetune a LLM on named entity\nrecognition tasks to make it capable of recognizing entities\nin raw text. Then, they propose another “2-model BERT”\nfor solving the relation extraction task, which contains two\nBERT-based classifiers. The first classifier learns the relation\nclass whereas the second binary classifier learns the direc-\ntion of the relations between the two entities. The predicted\ntriples and relations are then used to construct the KG. Guo\net al. [162] propose an end-to-end knowledge extraction\nmodel based on BERT, which can be applied to construct\nKGs from Classical Chinese text. Grapher [41] presents a\nnovel end-to-end multi-stage system. It first utilizes LLMs\nto generate KG entities, followed by a simple relation con-\nstruction head, enabling efficient KG construction from the\ntextual description. PiVE [163] proposes a prompting with\nan iterative verification framework that utilizes a smaller\nLLM like T5 to correct the errors in KGs generated by a\nlarger LLM (e.g., ChatGPT). To further explore advanced\nLLMs, AutoKG design several prompts for different KG\nBrarck Obama\nPoliticianOf\nUSA\nHonolulu\nBornIn\nLocatedInCapitalOf\nMarriedTo\nMichelle\nObama\nLiveIn\nConstruct KGs\nLLMs\nObama born in [MASK]\nHonolulu is located in [MASK]\nUSA's capital is [MASK]\nCloze Question\n(Obama, BornIn, Honolulu)\n(Honolulu, LocatedIn, USA)\n(Washingto D.C., CapitalOf, USA)\nWashingto\nD.C.\nDistilled Triples\nFig. 20. The general framework of distilling KGs from LLMs.\nconstruction tasks (e.g., entity typing, entity linking, and\nrelation extraction). Then, it adopts the prompt to perform\nKG construction using ChatGPT and GPT-4.\n5.3.4 Distilling Knowledge Graphs from LLMs\nLLMs have been shown to implicitly encode massive knowl-\nedge [14]. As shown in Fig. 20, some research aims to distill\nknowledge from LLMs to construct KGs. COMET [164]\nproposes a commonsense transformer model that constructs\ncommonsense KGs by using existing tuples as a seed set of\nknowledge on which to train. Using this seed set, a LLM\nlearns to adapt its learned representations to knowledge\ngeneration, and produces novel tuples that are high quality.\nExperimental results reveal that implicit knowledge from\nLLMs is transferred to generate explicit knowledge in com-\nmonsense KGs. BertNet [165] proposes a novel framework\nfor automatic KG construction empowered by LLMs. It re-\nquires only the minimal definition of relations as inputs and\nautomatically generates diverse prompts, and performs an\nefficient knowledge search within a given LLM for consis-\ntent outputs. The constructed KGs show competitive quality,\ndiversity, and novelty with a richer set of new and complex\nrelations, which cannot be extracted by previous methods.\nWest et al. [166] propose a symbolic knowledge distillation\nframework that distills symbolic knowledge from LLMs.\nThey first finetune a small student LLM by distilling com-\nmonsense facts from a large LLM like GPT-3. Then, the\nstudent LLM is utilized to generate commonsense KGs.\n5.4 LLM-augmented KG-to-text Generation\nThe goal of Knowledge-graph-to-text (KG-to-text) genera-\ntion is to generate high-quality texts that accurately and\nconsistently describe the input knowledge graph infor-\nmation [228]. KG-to-text generation connects knowledge\ngraphs and texts, significantly improving the applicability\nof KG in more realistic NLG scenarios, including story-\ntelling [229] and knowledge-grounded dialogue [230]. How-\never, it is challenging and costly to collect large amounts\nof graph-text parallel data, resulting in insufficient training\nand poor generation quality. Thus, many research efforts re-\nsort to either: 1) leverage knowledge from LLMs or 2) construct\nlarge-scale weakly-supervised KG-text corpus to solve this issue.\n5.4.1 Leveraging Knowledge from LLMs\nAs pioneering research efforts in using LLMs for KG-to-Text\ngeneration, Ribeiro et al. [167] and Kale and Rastogi [231]\ndirectly fine-tune various LLMs, including BART and T5,\nwith the goal of transferring LLMs knowledge for this\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 17\nBrarck Obama\nPoliticianOf\nUSA\nHonolulu\nBornIn\nLocatedInCapitalOf\nWashingto\nD.C.\nMarriedTo\nMichelle\nObama\nLiveIn\nKGs\nLLMs\nBrack Obama is a\npolitician of  USA. He\nwas born in Honolulu,\nand married to Michelle\nObama. \nGraph Linearization\nBrack Obama [SEP]\nPoliticianOf [SEP] \nUSA [SEP] .....\n[SEP] Michelle Obama \nDescription Text\nFig. 21. The general framework of KG-to-text generation.\ntask. As shown in Fig. 21, both works simply represent\nthe input graph as a linear traversal and find that such\na naive approach successfully outperforms many existing\nstate-of-the-art KG-to-text generation systems. Interestingly,\nRibeiro et al. [167] also find that continue pre-training could\nfurther improve model performance. However, these meth-\nods are unable to explicitly incorporate rich graph semantics\nin KGs. To enhance LLMs with KG structure information,\nJointGT [42] proposes to inject KG structure-preserving\nrepresentations into the Seq2Seq large language models.\nGiven input sub-KGs and corresponding text, JointGT first\nrepresents the KG entities and their relations as a sequence\nof tokens, then concatenate them with the textual tokens\nwhich are fed into LLM. After the standard self-attention\nmodule, JointGT then uses a pooling layer to obtain the\ncontextual semantic representations of knowledge entities\nand relations. Finally, these pooled KG representations are\nthen aggregated in another structure-aware self-attention\nlayer. JointGT also deploys additional pre-training objec-\ntives, including KG and text reconstruction tasks given\nmasked inputs, to improve the alignment between text and\ngraph information. Li et al. [168] focus on the few-shot\nscenario. It first employs a novel breadth-first search (BFS)\nstrategy to better traverse the input KG structure and feed\nthe enhanced linearized graph representations into LLMs\nfor high-quality generated outputs, then aligns the GCN-\nbased and LLM-based KG entity representation. Colas et\nal. [169] first transform the graph into its appropriate repre-\nsentation before linearizing the graph. Next, each KG node\nis encoded via a global attention mechanism, followed by\na graph-aware attention module, ultimately being decoded\ninto a sequence of tokens. Different from these works, KG-\nBART [37] keeps the structure of KGs and leverages the\ngraph attention to aggregate the rich concept semantics in\nthe sub-KG, which enhances the model generalization on\nunseen concept sets.\n5.4.2 Constructing large weakly KG-text aligned Corpus\nAlthough LLMs have achieved remarkable empirical suc-\ncess, their unsupervised pre-training objectives are not nec-\nessarily aligned well with the task of KG-to-text genera-\ntion, motivating researchers to develop large-scale KG-text\naligned corpus. Jin et al. [170] propose a 1.3M unsupervised\nKG-to-graph training data from Wikipedia. Specifically, they\nfirst detect the entities appearing in the text via hyperlinks\nand named entity detectors, and then only add text that\nshares a common set of entities with the corresponding\nknowledge graph, similar to the idea of distance supervision\nin the relation extraction task [232]. They also provide a\n1,000+ human annotated KG-to-Text test data to verify the\neffectiveness of the pre-trained KG-to-Text models. Simi-\nlarly, Chen et al. [171] also propose a KG-grounded text\ncorpus collected from the English Wikidump. To ensure the\nconnection between KG and text, they only extract sentences\nwith at least two Wikipedia anchor links. Then, they use\nthe entities from those links to query their surrounding\nneighbors in WikiData and calculate the lexical overlapping\nbetween these neighbors and the original sentences. Finally,\nonly highly overlapped pairs are selected. The authors ex-\nplore both graph-based and sequence-based encoders and\nidentify their advantages in various different tasks and\nsettings.\n5.5 LLM-augmented KG Question Answering\nKnowledge graph question answering (KGQA) aims to find\nanswers to natural language questions based on the struc-\ntured facts stored in knowledge graphs [233], [234]. The\ninevitable challenge in KGQA is to retrieve related facts and\nextend the reasoning advantage of KGs to QA. Therefore,\nrecent studies adopt LLMs to bridge the gap between nat-\nural language questions and structured knowledge graphs\n[174], [175], [235]. The general framework of applying LLMs\nfor KGQA is illustrated in Fig. 22, where LLMs can be used\nas 1) entity/relation extractors, and 2) answer reasoners.\n5.5.1 LLMs as Entity/relation Extractors\nEntity/relation extractors are designed to identify entities\nand relationships mentioned in natural language questions\nand retrieve related facts in KGs. Given the proficiency in\nlanguage comprehension, LLMs can be effectively utilized\nfor this purpose. Lukovnikov et al. [172] are the first to uti-\nlize LLMs as classifiers for relation prediction, resulting in a\nnotable improvement in performance compared to shallow\nneural networks. Nan et al. [174] introduce two LLM-based\nKGQA frameworks that adopt LLMs to detect mentioned\nentities and relations. Then, they query the answer in KGs\nusing the extracted entity-relation pairs. QA-GNN [131]\nuses LLMs to encode the question and candidate answer\npairs, which are adopted to estimate the importance of\nrelative KG entities. The entities are retrieved to form a\nsubgraph, where an answer reasoning is conducted by a\ngraph neural network. Luo et al. [173] use LLMs to calculate\nthe similarities between relations and questions to retrieve\nrelated facts, formulated as\ns(r, q) = LLM(r)⊤LLM(q), (12)\nwhere q denotes the question, r denotes the relation, and\nLLM(·) would generate representation for q and r, respec-\ntively. Furthermore, Zhang et al. [236] propose a LLM-based\npath retriever to retrieve question-related relations hop-by-\nhop and construct several paths. The probability of each\npath can be calculated as\nP(p|q) =\n|p|Y\nt=1\ns(rt, q), (13)\nwhere p denotes the path, and rt denotes the relation at the\nt-th hop of p. The retrieved relations and paths can be used\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 18\nQuestion: Where was Neil Armstrong born in?\nLLMs \nRelation/entity Extractor\nNeil Armstrong BornInEntity Relation\nKGs \nRetrieve in KGs\nQuestion[CLS] [SEP]Related Facts Candidates[SEP] [SEP]\nLLMs \nAnswer Reasoner\nScore\nFig. 22. The general framework of applying LLMs for knowledge graph\nquestion answering (KGQA).\nas context knowledge to improve the performance of answer\nreasoners as\nP(a|q) =\nX\np∈P\nP(a|p)P(p|q), (14)\nwhere P denotes retrieved paths and a denotes the answer.\n5.5.2 LLMs as Answer Reasoners\nAnswer reasoners are designed to reason over the retrieved\nfacts and generate answers. LLMs can be used as answer\nreasoners to generate answers directly. For example, as\nshown in Fig. 3 22, DEKCOR [175] concatenates the re-\ntrieved facts with questions and candidate answers as\nx = [CLS] q [SEP] Related Facts [SEP] a [SEP], (15)\nwhere a denotes candidate answers. Then, it feeds them\ninto LLMs to predict answer scores. After utilizing LLMs to\ngenerate the representation of x as QA context, DRLK [176]\nproposes a Dynamic Hierarchical Reasoner to capture the\ninteractions between QA context and answers for answer\nprediction. Yan et al. [235] propose a LLM-based KGQA\nframework consisting of two stages: (1) retrieve related\nfacts from KGs and (2) generate answers based on the\nretrieved facts. The first stage is similar to the entity/relation\nextractors. Given a candidate answer entity a, it extracts a\nseries of paths p1, . . . , pn from KGs. But the second stage is a\nLLM-based answer reasoner. It first verbalizes the paths by\nusing the entity names and relation names in KGs. Then, it\nconcatenates the question q and all paths p1, . . . , pn to make\nan input sample as\nx = [CLS] q [SEP] p1 [SEP] ··· [SEP] pn [SEP]. (16)\nThese paths are regarded as the related facts for the can-\ndidate answer a. Finally, it uses LLMs to predict whether\nthe hypothesis: “a is the answer of q” is supported by those\nfacts, which is formulated as\ne[CLS] = LLM(x), (17)\ns = σ(MLP(e[CLS])), (18)\nwhere it encodes x using a LLM and feeds representation\ncorresponding to [CLS] token for binary classification, and\nσ(·) denotes the sigmoid function.\nTABLE 4\nSummary of methods that synergize KGs and LLMs.\nTask Method Year\nSynergized Knowledge representation\nJointGT [42] 2021\nKEPLER [40] 2021\nDRAGON [44] 2022\nHKLM [238] 2023\nSynergized Reasoning\nLARK [45] 2023\nSiyuan et al. [46] 2023\nKSL [239] 2023\nStructGPT [237] 2023\nThink-on-graph [240] 2023\nTo better guide LLMs reason through KGs, OreoLM [177]\nproposes a Knowledge Interaction Layer (KIL) which is in-\nserted amid LLM layers. KIL interacts with a KG reasoning\nmodule, where it discovers different reasoning paths, and\nthen the reasoning module can reason over the paths to\ngenerate answers. GreaseLM [178] fuses the representations\nfrom LLMs and graph neural networks to effectively reason\nover KG facts and language context. UniKGQA [43] unifies\nthe facts retrieval and reasoning into a unified framework.\nUniKGQA consists of two modules. The first module is\na semantic matching module that uses a LLM to match\nquestions with their corresponding relations semantically.\nThe second module is a matching information propagation\nmodule, which propagates the matching information along\ndirected edges on KGs for answer reasoning. Similarly,\nReLMKG [179] performs joint reasoning on a large language\nmodel and the associated knowledge graph. The question\nand verbalized paths are encoded by the language model,\nand different layers of the language model produce outputs\nthat guide a graph neural network to perform message pass-\ning. This process utilizes the explicit knowledge contained\nin the structured knowledge graph for reasoning purposes.\nStructGPT [237] adopts a customized interface to allow large\nlanguage models (e.g., ChatGPT) directly reasoning on KGs\nto perform multi-step question answering.\n6 S YNERGIZED LLM S + KGS\nThe synergy of LLMs and KGs has attracted increasing\nattention these years, which marries the merits of LLMs and\nKGs to mutually enhance performance in various down-\nstream applications. For example, LLMs can be used to\nunderstand natural language, while KGs are treated as a\nknowledge base, which provides factual knowledge. The\nunification of LLMs and KGs could result in a powerful\nmodel for knowledge representation and reasoning.\nIn this section, we will discuss the state-of-the-art Syn-\nergized LLMs + KGs from two perspectives: 1) Synergized\nKnowledge Representation, and 2) Synergized Reasoning. Rep-\nresentative works are summarized in Table 4.\n6.1 Synergized Knowledge Representation\nText corpus and knowledge graphs both contain enormous\nknowledge. However, the knowledge in text corpus is\nusually implicit and unstructured, while the knowledge\nin KGs is explicit and structured. Synergized Knowledge\nRepresentation aims to design a synergized model that can\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 19\nText-Knowledge Fusion Module\nSelf-Attention\nInput Text\nSelf-Attention Self-Attention\nT-encoder\nN Layers\nText Outputs Knowledge Graph Outputs\nM Layers\nK-encoder\nKnowledge Graph\nFig. 23. Synergized knowledge representation by additional KG fusion\nmodules.\neffectively represent knowledge from both LLMs and KGs.\nThe synergized model can provide a better understanding\nof the knowledge from both sources, making it valuable for\nmany downstream tasks.\nTo jointly represent the knowledge, researchers propose\nthe synergized models by introducing additional KG fu-\nsion modules, which are jointly trained with LLMs. As\nshown in Fig. 23, ERNIE [35] proposes a textual-knowledge\ndual encoder architecture where a T-encoder first encodes\nthe input sentences, then a K-encoder processes knowledge\ngraphs which are fused them with the textual representation\nfrom the T-encoder. BERT-MK [241] employs a similar dual-\nencoder architecture but it introduces additional informa-\ntion of neighboring entities in the knowledge encoder com-\nponent during the pre-training of LLMs. However, some of\nthe neighboring entities in KGs may not be relevant to the\ninput text, resulting in extra redundancy and noise. Coke-\nBERT [242] focuses on this issue and proposes a GNN-based\nmodule to filter out irrelevant KG entities using the input\ntext. JAKET [243] proposes to fuse the entity information in\nthe middle of the large language model.\nKEPLER [40] presents a unified model for knowledge\nembedding and pre-trained language representation. In KE-\nPLER, they encode textual entity descriptions with a LLM as\ntheir embeddings, and then jointly optimize the knowledge\nembedding and language modeling objectives. JointGT [42]\nproposes a graph-text joint representation learning model,\nwhich proposes three pre-training tasks to align represen-\ntations of graph and text. DRAGON [44] presents a self-\nsupervised method to pre-train a joint language-knowledge\nfoundation model from text and KG. It takes text segments\nand relevant KG subgraphs as input and bidirectionally\nfuses information from both modalities. Then, DRAGON\nutilizes two self-supervised reasoning tasks, i.e., masked\nlanguage modeling and KG link prediction to optimize the\nmodel parameters. HKLM [238] introduces a unified LLM\nwhich incorporates KGs to learn representations of domain-\nspecific knowledge.\n6.2 Synergized Reasoning\nTo better utilize the knowledge from text corpus and knowl-\nedge graph reasoning, Synergized Reasoning aims to design\na synergized model that can effectively conduct reasoning\nwith both LLMs and KGs.\nQuestion <SEP> Option \nLLM Encoder\nKG Encoder\nJoint Reasoning Layer\nDynamic Pruning\nLM Rep.\nKG Rep.\nLM to KG Att.\nKG to LM Att.\nLM to KG\nAttention\nAnswer \nAnswer\nInference\nFig. 24. The framework of LLM-KG Fusion Reasoning.\nLLM-KG Fusion Reasoning. LLM-KG Fusion Reasoning\nleverages two separated LLM and KG encoders to process\nthe text and relevant KG inputs [244]. These two encoders\nare equally important and jointly fusing the knowledge\nfrom two sources for reasoning. To improve the interac-\ntion between text and knowledge, KagNet [38] proposes\nto first encode the input KG, and then augment the input\ntextual representation. In contrast, MHGRN [234] uses the\nfinal LLM outputs of the input text to guide the reasoning\nprocess on the KGs. Yet, both of them only design a single-\ndirection interaction between the text and KGs. To tackle this\nissue, QA-GNN [131] proposes to use a GNN-based model\nto jointly reason over input context and KG information\nvia message passing. Specifically, QA-GNN represents the\ninput textual information as a special node via a pooling\noperation and connects this node with other entities in KG.\nHowever, the textual inputs are only pooled into a single\ndense vector, limiting the information fusion performance.\nJointLK [245] then proposes a framework with fine-grained\ninteraction between any tokens in the textual inputs and any\nKG entities through LM-to-KG and KG-to-LM bi-directional\nattention mechanism. As shown in Fig. 24, pairwise dot-\nproduct scores are calculated over all textual tokens and KG\nentities, the bi-directional attentive scores are computed sep-\narately. In addition, at each jointLK layer, the KGs are also\ndynamically pruned based on the attention score to allow\nlater layers to focus on more important sub-KG structures.\nDespite being effective, in JointLK, the fusion process be-\ntween the input text and KG still uses the final LLM outputs\nas the input text representations. GreaseLM [178] designs\ndeep and rich interaction between the input text tokens and\nKG entities at each layer of the LLMs. The architecture and\nfusion approach is mostly similar to ERNIE [35] discussed in\nSection 6.1, except that GreaseLM does not use the text-only\nT-encoder to handle input text.\nLLMs as Agents Reasoning. Instead using two encoders\nto fuse the knowledge, LLMs can also be treated as agents\nto interact with the KGs to conduct reasoning [246], as\nillustrated in Fig. 25. KD-CoT [247] iteratively retrieves facts\nfrom KGs and produces faithful reasoning traces, which\nguide LLMs to generate answers. KSL [239] teaches LLMs\nto search on KGs to retrieve relevant facts and then generate\nanswers. StructGPT [237] designs several API interfaces to\nallow LLMs to access the structural data and perform rea-\nsoning by traversing on KGs. Think-on-graph [240] provides\na flexible plug-and-play framework where LLM agents it-\neratively execute beam searches on KGs to discover the\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 20\nBorn_in \nKnowledge Graph\nQuestion: Which\ncountry is Barack\nObama from ?\nMarry_to \nMichelle  Obama \nBarack  Obama \nHonolulu   \nCity_of \nLocated_in \nHawaii \nUSA \n1776 \nFounded_in \nLLM agent\nAnswer: USAKGs \nLLMs \nReasoning-on-Graphs\nFig. 25. Using LLMs as agents for reasoning on KGs.\nreasoning paths and generate answers. To enhance the agent\nabilities, AgentTuning [248] presents several instruction-\ntuning datasets to guide LLM agents to perform reasoning\non KGs.\nComparison and Discussion. LLM-KG Fusion Reasoning\ncombines the LLM encoder and KG encoder to represent\nknowledge in a unified manner. It then employs a syner-\ngized reasoning module to jointly reason the results. This\nframework allows for different encoders and reasoning\nmodules, which are trained end-to-end to effectively utilize\nthe knowledge and reasoning capabilities of LLMs and KGs.\nHowever, these additional modules may introduce extra\nparameters and computational costs while lacking inter-\npretability. LLMs as Agents for KG reasoning provides a\nflexible framework for reasoning on KGs without additional\ntraining cost, which can be generalized to different LLMs\nand KGs. Meanwhile, the reasoning process is interpretable,\nwhich can be used to explain the results. Nevertheless,\ndefining the actions and policies for LLM agents is also chal-\nlenging. The synergy of LLMs and KGs is still an ongoing\nresearch topic, with the potential to have more powerful\nframeworks in the future.\n7 F UTURE DIRECTIONS AND MILESTONES\nIn this section, we discuss the future directions and several\nmilestones in the research area of unifying KGs and LLMs.\n7.1 KGs for Hallucination Detection in LLMs\nThe hallucination problem in LLMs, which generates fac-\ntually incorrect content, significantly hinders the reliability\nof LLMs. As discussed in Section 4, existing studies try\nto utilize KGs to obtain more reliable LLMs through pre-\ntraining or KG-enhanced inference. Despite the efforts, the\nissue of hallucination may continue to persist in the realm of\nLLMs for the foreseeable future. Consequently, in order to\ngain the public’s trust and border applications, it is impera-\ntive to detect and assess instances of hallucination within\nLLMs and other forms of AI-generated content (AIGC).\nExisting methods strive to detect hallucination by training a\nneural classifier on a small set of documents [249], which are\nneither robust nor powerful to handle ever-growing LLMs.\nRecently, researchers try to use KGs as an external source\nto validate LLMs [250]. Further studies combine LLMs\nand KGs to achieve a generalized fact-checking model that\ncan detect hallucinations across domains [251]. Therefore,\nit opens a new door to utilizing KGs for hallucination\ndetection.\n7.2 KGs for Editing Knowledge in LLMs\nAlthough LLMs are capable of storing massive real-world\nknowledge, they cannot quickly update their internal\nknowledge updated as real-world situations change. There\nare some research efforts proposed for editing knowledge in\nLLMs [252] without re-training the whole LLMs. Yet, such\nsolutions still suffer from poor performance or computa-\ntional overhead [253]. Existing studies [254] also reveal that\nedit a single fact would cause a ripple effect for other related\nknowledge. Therefore, it is necessary to develop a more\nefficient and effective method to edit knowledge in LLMs.\nRecently, researchers try to leverage KGs to edit knowledge\nin LLMs efficiently.\n7.3 KGs for Black-box LLMs Knowledge Injection\nAlthough pre-training and knowledge editing could update\nLLMs to catch up with the latest knowledge, they still need\nto access the internal structures and parameters of LLMs.\nHowever, many state-of-the-art large LLMs (e.g., ChatGPT)\nonly provide APIs for users and developers to access, mak-\ning themselves black-box to the public. Consequently, it is\nimpossible to follow conventional KG injection approaches\ndescribed [38], [244] that change LLM structure by adding\nadditional knowledge fusion modules. Converting various\ntypes of knowledge into different text prompts seems to be\na feasible solution. However, it is unclear whether these\nprompts can generalize well to new LLMs. Moreover, the\nprompt-based approach is limited to the length of input to-\nkens of LLMs. Therefore, how to enable effective knowledge\ninjection for black-box LLMs is still an open question for us\nto explore [255], [256].\n7.4 Multi-Modal LLMs for KGs\nCurrent knowledge graphs typically rely on textual and\ngraph structure to handle KG-related applications. How-\never, real-world knowledge graphs are often constructed\nby data from diverse modalities [99], [257], [258]. Therefore,\neffectively leveraging representations from multiple modal-\nities would be a significant challenge for future research in\nKGs [259]. One potential solution is to develop methods\nthat can accurately encode and align entities across different\nmodalities. Recently, with the development of multi-modal\nLLMs [98], [260], leveraging LLMs for modality alignment\nholds promise in this regard. But, bridging the gap between\nmulti-modal LLMs and KG structure remains a crucial\nchallenge in this field, demanding further investigation and\nadvancements.\n7.5 LLMs for Understanding KG Structure\nConventional LLMs trained on plain text data are not\ndesigned to understand structured data like knowledge\ngraphs. Thus, LLMs might not fully grasp or understand the\ninformation conveyed by the KG structure. A straightfor-\nward way is to linearize the structured data into a sentence\nthat LLMs can understand. However, the scale of the KGs\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 21\nKG-enhanced\nLLMs \nLLM-augmented\nKGs \nSynergized\nLLMs + KGs\nGraph Structure\nUnderstanding\nMulti-modality\nKnowledge\nUpdating\nStage 1 Stage 2 Stage 3\nFig. 26. The milestones of unifying KGs and LLMs.\nmakes it impossible to linearize the whole KGs as input.\nMoreover, the linearization process may lose some underly-\ning information in KGs. Therefore, it is necessary to develop\nLLMs that can directly understand the KG structure and\nreason over it [237].\n7.6 Synergized LLMs and KGs for Birectional Reason-\ning\nKGs and LLMs are two complementary technologies that\ncan synergize each other. However, the synergy of LLMs\nand KGs is less explored by existing researchers. A desired\nsynergy of LLMs and KGs would involve leveraging the\nstrengths of both technologies to overcome their individual\nlimitations. LLMs, such as ChatGPT, excel in generating\nhuman-like text and understanding natural language, while\nKGs are structured databases that capture and represent\nknowledge in a structured manner. By combining their capa-\nbilities, we can create a powerful system that benefits from\nthe contextual understanding of LLMs and the structured\nknowledge representation of KGs. To better unify LLMs and\nKGs, many advanced techniques need to be incorporated,\nsuch as multi-modal learning [261], graph neural network\n[262], and continuous learning [263]. Last, the synergy of\nLLMs and KGs can be applied to many real-world applica-\ntions, such as search engines [100], recommender systems\n[10], [89], and drug discovery.\nWith a given application problem, we can apply a KG\nto perform a knowledge-driven search for potential goals\nand unseen data, and simultaneously start with LLMs\nto perform a data/text-driven inference to see what new\ndata/goal items can be derived. When the knowledge-based\nsearch is combined with data/text-driven inference, they\ncan mutually validate each other, resulting in efficient and\neffective solutions powered by dual-driving wheels. There-\nfore, we can anticipate increasing attention to unlock the po-\ntential of integrating KGs and LLMs for diverse downstream\napplications with both generative and reasoning capabilities\nin the near future.\n8 C ONCLUSION\nUnifying large language models (LLMs) and knowledge\ngraphs (KGs) is an active research direction that has at-\ntracted increasing attention from both academia and in-\ndustry. In this article, we provide a thorough overview of\nthe recent research in this field. We first introduce different\nmanners that integrate KGs to enhance LLMs. Then, we\nintroduce existing methods that apply LLMs for KGs and\nestablish taxonomy based on varieties of KG tasks. Finally,\nwe discuss the challenges and future directions in this field.\nWe envision that there will be multiple stages (milestones)\nin the roadmap of unifying KGs and LLMs, as shown in Fig.\n26. In particular, we will anticipate increasing research on\nthree stages: Stage 1: KG-enhanced LLMs, LLM-augmented\nKGs, Stage 2: Synergized LLMs + KGs, and Stage 3: Graph\nStructure Understanding, Multi-modality, Knowledge Up-\ndating. We hope that this article will provide a guideline to\nadvance future research.\nACKNOWLEDGMENTS\nThis research was supported by the Australian Research\nCouncil (ARC) under grants FT210100097 and DP240101547\nand the National Natural Science Foundation of China\n(NSFC) under grant 62120106008.\nREFERENCES\n[1] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding,” arXiv preprint arXiv:1810.04805, 2018.\n[2] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,\nM. Lewis, L. Zettlemoyer, and V . Stoyanov, “Roberta: A ro-\nbustly optimized bert pretraining approach,” arXiv preprint\narXiv:1907.11692, 2019.\n[3] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\nY. Zhou, W. Li, and P . J. Liu, “Exploring the limits of transfer\nlearning with a unified text-to-text transformer,” The Journal of\nMachine Learning Research, vol. 21, no. 1, pp. 5485–5551, 2020.\n[4] D. Su, Y. Xu, G. I. Winata, P . Xu, H. Kim, Z. Liu, and P . Fung,\n“Generalizing question answering system with pre-trained lan-\nguage model fine-tuning,” in Proceedings of the 2nd Workshop on\nMachine Reading for Question Answering, 2019, pp. 203–211.\n[5] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed,\nO. Levy, V . Stoyanov, and L. Zettlemoyer, “Bart: Denoising\nsequence-to-sequence pre-training for natural language genera-\ntion, translation, and comprehension,” in ACL, 2020, pp. 7871–\n7880.\n[6] J. Li, T. Tang, W. X. Zhao, and J.-R. Wen, “Pretrained lan-\nguage models for text generation: A survey,” arXiv preprint\narXiv:2105.10311, 2021.\n[7] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud,\nD. Yogatama, M. Bosma, D. Zhou, D. Metzler et al. , “Emergent\nabilities of large language models,” Transactions on Machine Learn-\ning Research.\n[8] K. Malinka, M. Pere ˇs´ıni, A. Firc, O. Huj ˇn´ak, and F. Janu ˇs, “On\nthe educational impact of chatgpt: Is artificial intelligence ready\nto obtain a university degree?” arXiv preprint arXiv:2303.11146 ,\n2023.\n[9] Z. Li, C. Wang, Z. Liu, H. Wang, S. Wang, and C. Gao, “Cctest:\nTesting and repairing code completion systems,” ICSE, 2023.\n[10] J. Liu, C. Liu, R. Lv, K. Zhou, and Y. Zhang, “Is chatgpt a good rec-\nommender? a preliminary study,”arXiv preprint arXiv:2304.10149,\n2023.\n[11] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min,\nB. Zhang, J. Zhang, Z. Dong et al., “A survey of large language\nmodels,” arXiv preprint arXiv:2303.18223, 2023.\n[12] X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang, “Pre-trained\nmodels for natural language processing: A survey,” Science China\nTechnological Sciences, vol. 63, no. 10, pp. 1872–1897, 2020.\n[13] J. Yang, H. Jin, R. Tang, X. Han, Q. Feng, H. Jiang, B. Yin, and\nX. Hu, “Harnessing the power of llms in practice: A survey on\nchatgpt and beyond,” arXiv preprint arXiv:2304.13712, 2023.\n[14] F. Petroni, T. Rockt ¨aschel, S. Riedel, P . Lewis, A. Bakhtin, Y. Wu,\nand A. Miller, “Language models as knowledge bases?” in\nEMNLP-IJCNLP, 2019, pp. 2463–2473.\n[15] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang,\nA. Madotto, and P . Fung, “Survey of hallucination in natural\nlanguage generation,” ACM Computing Surveys , vol. 55, no. 12,\npp. 1–38, 2023.\n[16] H. Zhang, H. Song, S. Li, M. Zhou, and D. Song, “A survey of\ncontrollable text generation using transformer-based pre-trained\nlanguage models,” arXiv preprint arXiv:2201.05337, 2022.\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 22\n[17] M. Danilevsky, K. Qian, R. Aharonov, Y. Katsis, B. Kawas, and\nP . Sen, “A survey of the state of explainable ai for natural\nlanguage processing,” arXiv preprint arXiv:2010.00711, 2020.\n[18] J. Wang, X. Hu, W. Hou, H. Chen, R. Zheng, Y. Wang, L. Yang,\nH. Huang, W. Ye, X. Genget al., “On the robustness of chatgpt: An\nadversarial and out-of-distribution perspective,” arXiv preprint\narXiv:2302.12095, 2023.\n[19] S. Ji, S. Pan, E. Cambria, P . Marttinen, and S. Y. Philip, “A\nsurvey on knowledge graphs: Representation, acquisition, and\napplications,” IEEE TNNLS, vol. 33, no. 2, pp. 494–514, 2021.\n[20] D. Vrande ˇci´c and M. Kr ¨otzsch, “Wikidata: a free collaborative\nknowledgebase,” Communications of the ACM , vol. 57, no. 10, pp.\n78–85, 2014.\n[21] S. Hu, L. Zou, and X. Zhang, “A state-transition framework to\nanswer complex questions over knowledge base,” in EMNLP,\n2018, pp. 2098–2108.\n[22] J. Zhang, B. Chen, L. Zhang, X. Ke, and H. Ding, “Neural,\nsymbolic and neural-symbolic reasoning on knowledge graphs,”\nAI Open, vol. 2, pp. 14–35, 2021.\n[23] B. Abu-Salih, “Domain-specific knowledge graphs: A survey,”\nJournal of Network and Computer Applications , vol. 185, p. 103076,\n2021.\n[24] T. Mitchell, W. Cohen, E. Hruschka, P . Talukdar, B. Yang, J. Bet-\nteridge, A. Carlson, B. Dalvi, M. Gardner, B. Kisiel, K. Jayant,\nL. Ni, M. Kathryn, M. Thahir, N. Ndapandula, P . Emmanouil,\nR. Alan, S. Mehdi, S. Burr, W. Derry, G. Abhinav, C. Xi, S. Abul-\nhair, and W. Joel, “Never-ending learning,” Communications of the\nACM, vol. 61, no. 5, pp. 103–115, 2018.\n[25] L. Zhong, J. Wu, Q. Li, H. Peng, and X. Wu, “A comprehen-\nsive survey on automatic knowledge graph construction,” arXiv\npreprint arXiv:2302.05019, 2023.\n[26] L. Yao, C. Mao, and Y. Luo, “Kg-bert: Bert for knowledge graph\ncompletion,” arXiv preprint arXiv:1909.03193, 2019.\n[27] L. Luo, Y.-F. Li, G. Haffari, and S. Pan, “Normalizing flow-\nbased neural process for few-shot knowledge graph completion,”\nSIGIR, 2023.\n[28] Y. Bang, S. Cahyawijaya, N. Lee, W. Dai, D. Su, B. Wilie, H. Love-\nnia, Z. Ji, T. Yu, W. Chung et al. , “A multitask, multilingual,\nmultimodal evaluation of chatgpt on reasoning, hallucination,\nand interactivity,” arXiv preprint arXiv:2302.04023, 2023.\n[29] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, and D. Zhou, “Self-\nconsistency improves chain of thought reasoning in language\nmodels,” arXiv preprint arXiv:2203.11171, 2022.\n[30] O. Golovneva, M. Chen, S. Poff, M. Corredor, L. Zettlemoyer,\nM. Fazel-Zarandi, and A. Celikyilmaz, “Roscoe: A suite of metrics\nfor scoring step-by-step reasoning,” ICLR, 2023.\n[31] F. M. Suchanek, G. Kasneci, and G. Weikum, “Yago: a core of\nsemantic knowledge,” in WWW, 2007, pp. 697–706.\n[32] A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. Hruschka, and\nT. Mitchell, “Toward an architecture for never-ending language\nlearning,” in Proceedings of the AAAI conference on artificial intelli-\ngence, vol. 24, no. 1, 2010, pp. 1306–1313.\n[33] A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and\nO. Yakhnenko, “Translating embeddings for modeling multi-\nrelational data,” NeurIPS, vol. 26, 2013.\n[34] G. Wan, S. Pan, C. Gong, C. Zhou, and G. Haffari, “Reasoning\nlike human: Hierarchical reinforcement learning for knowledge\ngraph reasoning,” in AAAI, 2021, pp. 1926–1932.\n[35] Z. Zhang, X. Han, Z. Liu, X. Jiang, M. Sun, and Q. Liu, “ERNIE:\nEnhanced language representation with informative entities,” in\nACL, 2019, pp. 1441–1451.\n[36] W. Liu, P . Zhou, Z. Zhao, Z. Wang, Q. Ju, H. Deng, and P . Wang,\n“K-BERT: enabling language representation with knowledge\ngraph,” in AAAI, 2020, pp. 2901–2908.\n[37] Y. Liu, Y. Wan, L. He, H. Peng, and P . S. Yu, “KG-BART: knowl-\nedge graph-augmented BART for generative commonsense rea-\nsoning,” in AAAI, 2021, pp. 6418–6425.\n[38] B. Y. Lin, X. Chen, J. Chen, and X. Ren, “KagNet: Knowledge-\naware graph networks for commonsense reasoning,” in EMNLP-\nIJCNLP, 2019, pp. 2829–2839.\n[39] D. Dai, L. Dong, Y. Hao, Z. Sui, B. Chang, and F. Wei,\n“Knowledge neurons in pretrained transformers,” arXiv preprint\narXiv:2104.08696, 2021.\n[40] X. Wang, T. Gao, Z. Zhu, Z. Zhang, Z. Liu, J. Li, and J. Tang,\n“KEPLER: A unified model for knowledge embedding and pre-\ntrained language representation,” Transactions of the Association\nfor Computational Linguistics, vol. 9, pp. 176–194, 2021.\n[41] I. Melnyk, P . Dognin, and P . Das, “Grapher: Multi-stage knowl-\nedge graph construction using pretrained language models,” in\nNeurIPS 2021 Workshop on Deep Generative Models and Downstream\nApplications, 2021.\n[42] P . Ke, H. Ji, Y. Ran, X. Cui, L. Wang, L. Song, X. Zhu, and\nM. Huang, “JointGT: Graph-text joint representation learning for\ntext generation from knowledge graphs,” in ACL Finding, 2021,\npp. 2526–2538.\n[43] J. Jiang, K. Zhou, W. X. Zhao, and J.-R. Wen, “Unikgqa: Unified\nretrieval and reasoning for solving multi-hop question answering\nover knowledge graph,” ICLR 2023, 2023.\n[44] M. Yasunaga, A. Bosselut, H. Ren, X. Zhang, C. D. Manning, P . S.\nLiang, and J. Leskovec, “Deep bidirectional language-knowledge\ngraph pretraining,” NeurIPS, vol. 35, pp. 37 309–37 323, 2022.\n[45] N. Choudhary and C. K. Reddy, “Complex logical reasoning over\nknowledge graphs using large language models,” arXiv preprint\narXiv:2305.01157, 2023.\n[46] S. Wang, Z. Wei, J. Xu, and Z. Fan, “Unifying structure reasoning\nand language model pre-training for complex reasoning,” arXiv\npreprint arXiv:2301.08913, 2023.\n[47] C. Zhen, Y. Shang, X. Liu, Y. Li, Y. Chen, and D. Zhang, “A\nsurvey on knowledge-enhanced pre-trained language models,”\narXiv preprint arXiv:2212.13428, 2022.\n[48] X. Wei, S. Wang, D. Zhang, P . Bhatia, and A. Arnold, “Knowl-\nedge enhanced pretrained language models: A compreshensive\nsurvey,” arXiv preprint arXiv:2110.08455, 2021.\n[49] D. Yin, L. Dong, H. Cheng, X. Liu, K.-W. Chang, F. Wei, and\nJ. Gao, “A survey of knowledge-intensive nlp with pre-trained\nlanguage models,” arXiv preprint arXiv:2202.08772, 2022.\n[50] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\nNeurIPS, vol. 30, 2017.\n[51] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P . Sharma, and R. Sori-\ncut, “Albert: A lite bert for self-supervised learning of language\nrepresentations,” in ICLR, 2019.\n[52] K. Clark, M.-T. Luong, Q. V . Le, and C. D. Manning, “Electra: Pre-\ntraining text encoders as discriminators rather than generators,”\narXiv preprint arXiv:2003.10555, 2020.\n[53] K. Hakala and S. Pyysalo, “Biomedical named entity recognition\nwith multilingual bert,” in Proceedings of the 5th workshop on\nBioNLP open shared tasks, 2019, pp. 56–61.\n[54] Y. Tay, M. Dehghani, V . Q. Tran, X. Garcia, J. Wei, X. Wang,\nH. W. Chung, D. Bahri, T. Schuster, S. Zhenget al., “Ul2: Unifying\nlanguage learning paradigms,” in ICLR, 2022.\n[55] V . Sanh, A. Webson, C. Raffel, S. Bach, L. Sutawika, Z. Alyafeai,\nA. Chaffin, A. Stiegler, A. Raja, M. Dey et al. , “Multitask\nprompted training enables zero-shot task generalization,” in\nICLR, 2022.\n[56] B. Zoph, I. Bello, S. Kumar, N. Du, Y. Huang, J. Dean, N. Shazeer,\nand W. Fedus, “St-moe: Designing stable and transferable sparse\nexpert models,” URL https://arxiv. org/abs/2202.08906, 2022.\n[57] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu,\nW. Zheng, X. Xia, W. L. Tam, Z. Ma, Y. Xue, J. Zhai, W. Chen,\nZ. Liu, P . Zhang, Y. Dong, and J. Tang, “GLM-130b: An open\nbilingual pre-trained model,” in ICLR, 2023.\n[58] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant,\nA. Barua, and C. Raffel, “mt5: A massively multilingual pre-\ntrained text-to-text transformer,” in NAACL, 2021, pp. 483–498.\n[59] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P . Dhari-\nwal, A. Neelakantan, P . Shyam, G. Sastry, A. Askell et al. ,\n“Language models are few-shot learners,” Advances in neural\ninformation processing systems, vol. 33, pp. 1877–1901, 2020.\n[60] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright,\nP . Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray et al. ,\n“Training language models to follow instructions with human\nfeedback,” NeurIPS, vol. 35, pp. 27 730–27 744, 2022.\n[61] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\nT. Lacroix, B. Rozi `ere, N. Goyal, E. Hambro, F. Azhar et al. ,\n“Llama: Open and efficient foundation language models,” arXiv\npreprint arXiv:2302.13971, 2023.\n[62] E. Saravia, “Prompt Engineering Guide,” https://github.com/\ndair-ai/Prompt-Engineering-Guide, 2022, accessed: 2022-12.\n[63] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. H. Chi, Q. V .\nLe, D. Zhou et al., “Chain-of-thought prompting elicits reasoning\nin large language models,” in NeurIPS.\n[64] S. Li, Y. Gao, H. Jiang, Q. Yin, Z. Li, X. Yan, C. Zhang, and B. Yin,\n“Graph reasoning for question answering with triplet retrieval,”\nin ACL, 2023.\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 23\n[65] Y. Wen, Z. Wang, and J. Sun, “Mindmap: Knowledge graph\nprompting sparks graph of thoughts in large language models,”\narXiv preprint arXiv:2308.09729, 2023.\n[66] K. Bollacker, C. Evans, P . Paritosh, T. Sturge, and J. Taylor, “Free-\nbase: A collaboratively created graph database for structuring\nhuman knowledge,” in SIGMOD, 2008, pp. 1247–1250.\n[67] S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyganiak, and\nZ. Ives, “Dbpedia: A nucleus for a web of open data,” in The\nSemantic Web: 6th International Semantic Web Conference. Springer,\n2007, pp. 722–735.\n[68] B. Xu, Y. Xu, J. Liang, C. Xie, B. Liang, W. Cui, and Y. Xiao, “Cn-\ndbpedia: A never-ending chinese knowledge extraction system,”\nin 30th International Conference on Industrial Engineering and Other\nApplications of Applied Intelligent Systems . Springer, 2017, pp.\n428–438.\n[69] P . Hai-Nyzhnyk, “Vikidia as a universal multilingual online\nencyclopedia for children,” The Encyclopedia Herald of Ukraine ,\nvol. 14, 2022.\n[70] F. Ilievski, P . Szekely, and B. Zhang, “Cskg: The commonsense\nknowledge graph,” Extended Semantic Web Conference (ESWC) ,\n2021.\n[71] R. Speer, J. Chin, and C. Havasi, “Conceptnet 5.5: An open\nmultilingual graph of general knowledge,” in Proceedings of the\nAAAI conference on artificial intelligence, vol. 31, no. 1, 2017.\n[72] H. Ji, P . Ke, S. Huang, F. Wei, X. Zhu, and M. Huang, “Language\ngeneration with multi-hop reasoning on commonsense knowl-\nedge graph,” in EMNLP, 2020, pp. 725–736.\n[73] J. D. Hwang, C. Bhagavatula, R. Le Bras, J. Da, K. Sakaguchi,\nA. Bosselut, and Y. Choi, “(comet-) atomic 2020: On symbolic\nand neural commonsense knowledge graphs,” in AAAI, vol. 35,\nno. 7, 2021, pp. 6384–6392.\n[74] H. Zhang, X. Liu, H. Pan, Y. Song, and C. W.-K. Leung, “Aser:\nA large-scale eventuality knowledge graph,” in Proceedings of the\nweb conference 2020, 2020, pp. 201–211.\n[75] H. Zhang, D. Khashabi, Y. Song, and D. Roth, “Transomcs: from\nlinguistic graphs to commonsense knowledge,” in IJCAI, 2021,\npp. 4004–4010.\n[76] Z. Li, X. Ding, T. Liu, J. E. Hu, and B. Van Durme, “Guided\ngeneration of cause and effect,” in IJCAI, 2020.\n[77] O. Bodenreider, “The unified medical language system (umls): in-\ntegrating biomedical terminology,” Nucleic acids research, vol. 32,\nno. suppl 1, pp. D267–D270, 2004.\n[78] Y. Liu, Q. Zeng, J. Ordieres Mer ´e, and H. Yang, “Anticipating\nstock market of the renowned companies: a knowledge graph\napproach,” Complexity, vol. 2019, 2019.\n[79] Y. Zhu, W. Zhou, Y. Xu, J. Liu, Y. Tan et al., “Intelligent learning\nfor knowledge graph towards geological data,”Scientific Program-\nming, vol. 2017, 2017.\n[80] W. Choi and H. Lee, “Inference of biomedical relations among\nchemicals, genes, diseases, and symptoms using knowledge rep-\nresentation learning,” IEEE Access , vol. 7, pp. 179 373–179 384,\n2019.\n[81] F. Farazi, M. Salamanca, S. Mosbach, J. Akroyd, A. Eibeck,\nL. K. Aditya, A. Chadzynski, K. Pan, X. Zhou, S. Zhang et al. ,\n“Knowledge graph approach to combustion chemistry and inter-\noperability,” ACS omega, vol. 5, no. 29, pp. 18 342–18 348, 2020.\n[82] X. Wu, T. Jiang, Y. Zhu, and C. Bu, “Knowledge graph for china’s\ngenealogy,” IEEE TKDE, vol. 35, no. 1, pp. 634–646, 2023.\n[83] X. Zhu, Z. Li, X. Wang, X. Jiang, P . Sun, X. Wang, Y. Xiao, and\nN. J. Yuan, “Multi-modal knowledge graph construction and\napplication: A survey,” IEEE TKDE, 2022.\n[84] S. Ferrada, B. Bustos, and A. Hogan, “Imgpedia: a linked dataset\nwith content-based analysis of wikimedia images,” in The Seman-\ntic Web–ISWC 2017. Springer, 2017, pp. 84–93.\n[85] Y. Liu, H. Li, A. Garcia-Duran, M. Niepert, D. Onoro-Rubio,\nand D. S. Rosenblum, “Mmkg: multi-modal knowledge graphs,”\nin The Semantic Web: 16th International Conference, ESWC 2019,\nPortoroˇ z, Slovenia, June 2–6, 2019, Proceedings 16. Springer, 2019,\npp. 459–474.\n[86] M. Wang, H. Wang, G. Qi, and Q. Zheng, “Richpedia: a large-\nscale, comprehensive multi-modal knowledge graph,” Big Data\nResearch, vol. 22, p. 100159, 2020.\n[87] B. Shi, L. Ji, P . Lu, Z. Niu, and N. Duan, “Knowledge aware\nsemantic concept expansion for image-text matching.” in IJCAI,\nvol. 1, 2019, p. 2.\n[88] S. Shah, A. Mishra, N. Yadati, and P . P . Talukdar, “Kvqa:\nKnowledge-aware visual question answering,” in AAAI, vol. 33,\nno. 01, 2019, pp. 8876–8884.\n[89] R. Sun, X. Cao, Y. Zhao, J. Wan, K. Zhou, F. Zhang, Z. Wang, and\nK. Zheng, “Multi-modal knowledge graphs for recommender\nsystems,” in CIKM, 2020, pp. 1405–1414.\n[90] S. Deng, C. Wang, Z. Li, N. Zhang, Z. Dai, H. Chen, F. Xiong,\nM. Yan, Q. Chen, M. Chen, J. Chen, J. Z. Pan, B. Hooi, and\nH. Chen, “Construction and applications of billion-scale pre-\ntrained multimodal business knowledge graph,” in ICDE, 2023.\n[91] C. Rosset, C. Xiong, M. Phan, X. Song, P . Bennett, and S. Tiwary,\n“Knowledge-aware language model pretraining,” arXiv preprint\narXiv:2007.00655, 2020.\n[92] P . Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal,\nH. K ¨uttler, M. Lewis, W.-t. Yih, T. Rockt ¨aschel, S. Riedel,\nand D. Kiela, “Retrieval-augmented generation for knowledge-\nintensive nlp tasks,” in NeurIPS, vol. 33, 2020, pp. 9459–9474.\n[93] Y. Zhu, X. Wang, J. Chen, S. Qiao, Y. Ou, Y. Yao, S. Deng, H. Chen,\nand N. Zhang, “Llms for knowledge graph construction and\nreasoning: Recent capabilities and future opportunities,” arXiv\npreprint arXiv:2305.13168, 2023.\n[94] Z. Zhang, X. Liu, Y. Zhang, Q. Su, X. Sun, and B. He, “Pretrain-\nkge: learning knowledge representation from pretrained lan-\nguage models,” in EMNLP Finding, 2020, pp. 259–266.\n[95] A. Kumar, A. Pandey, R. Gadia, and M. Mishra, “Building\nknowledge graph using pre-trained language model for learning\nentity-aware relationships,” in 2020 IEEE International Conference\non Computing, Power and Communication Technologies (GUCON) .\nIEEE, 2020, pp. 310–315.\n[96] X. Xie, N. Zhang, Z. Li, S. Deng, H. Chen, F. Xiong, M. Chen,\nand H. Chen, “From discrimination to generation: Knowledge\ngraph completion with generative transformer,” in WWW, 2022,\npp. 162–165.\n[97] Z. Chen, C. Xu, F. Su, Z. Huang, and Y. Dou, “Incorporating\nstructured sentences with time-enhanced bert for fully-inductive\ntemporal relation prediction,” SIGIR, 2023.\n[98] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, “Minigpt-4:\nEnhancing vision-language understanding with advanced large\nlanguage models,” arXiv preprint arXiv:2304.10592, 2023.\n[99] M. Warren, D. A. Shamma, and P . J. Hayes, “Knowledge engi-\nneering with image data in real-world settings,” in AAAI, ser.\nCEUR Workshop Proceedings, vol. 2846, 2021.\n[100] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kul-\nshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du et al. ,\n“Lamda: Language models for dialog applications,” arXiv\npreprint arXiv:2201.08239, 2022.\n[101] Y. Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu,\nX. Chen, Y. Zhao, Y. Lu et al., “Ernie 3.0: Large-scale knowledge\nenhanced pre-training for language understanding and genera-\ntion,” arXiv preprint arXiv:2107.02137, 2021.\n[102] T. Shen, Y. Mao, P . He, G. Long, A. Trischler, and W. Chen,\n“Exploiting structured knowledge in text via graph-guided rep-\nresentation learning,” in EMNLP, 2020, pp. 8980–8994.\n[103] D. Zhang, Z. Yuan, Y. Liu, F. Zhuang, H. Chen, and H. Xiong,\n“E-bert: A phrase and product knowledge enhanced language\nmodel for e-commerce,” arXiv preprint arXiv:2009.02835, 2020.\n[104] S. Li, X. Li, L. Shang, C. Sun, B. Liu, Z. Ji, X. Jiang, and Q. Liu,\n“Pre-training language models with deterministic factual knowl-\nedge,” in EMNLP, 2022, pp. 11 118–11 131.\n[105] M. Kang, J. Baek, and S. J. Hwang, “Kala: Knowledge-augmented\nlanguage model adaptation,” in NAACL, 2022, pp. 5144–5167.\n[106] W. Xiong, J. Du, W. Y. Wang, and V . Stoyanov, “Pretrained en-\ncyclopedia: Weakly supervised knowledge-pretrained language\nmodel,” in ICLR, 2020.\n[107] T. Sun, Y. Shao, X. Qiu, Q. Guo, Y. Hu, X. Huang, and Z. Zhang,\n“CoLAKE: Contextualized language and knowledge embed-\nding,” in Proceedings of the 28th International Conference on Com-\nputational Linguistics, 2020, pp. 3660–3670.\n[108] T. Zhang, C. Wang, N. Hu, M. Qiu, C. Tang, X. He, and J. Huang,\n“DKPLM: decomposable knowledge-enhanced pre-trained lan-\nguage model for natural language understanding,” in AAAI,\n2022, pp. 11 703–11 711.\n[109] J. Wang, W. Huang, M. Qiu, Q. Shi, H. Wang, X. Li, and M. Gao,\n“Knowledge prompting in pre-trained language model for natu-\nral language understanding,” in Proceedings of the 2022 Conference\non Empirical Methods in Natural Language Processing , 2022, pp.\n3164–3177.\n[110] H. Ye, N. Zhang, S. Deng, X. Chen, H. Chen, F. Xiong, X. Chen,\nand H. Chen, “Ontology-enhanced prompt-tuning for few-shot\nlearning,” in Proceedings of the ACM Web Conference 2022 , 2022,\npp. 778–787.\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 24\n[111] H. Luo, Z. Tang, S. Peng, Y. Guo, W. Zhang, C. Ma, G. Dong,\nM. Song, W. Linet al., “Chatkbqa: A generate-then-retrieve frame-\nwork for knowledge base question answering with fine-tuned\nlarge language models,” arXiv preprint arXiv:2310.08975, 2023.\n[112] L. Luo, Y.-F. Li, G. Haffari, and S. Pan, “Reasoning on graphs:\nFaithful and interpretable large language model reasoning,”\narXiv preprint arxiv:2310.01061, 2023.\n[113] R. Logan, N. F. Liu, M. E. Peters, M. Gardner, and S. Singh,\n“Barack’s wife hillary: Using knowledge graphs for fact-aware\nlanguage modeling,” in ACL, 2019, pp. 5962–5971.\n[114] K. Guu, K. Lee, Z. Tung, P . Pasupat, and M.-W. Chang, “Realm:\nRetrieval-augmented language model pre-training,” in ICML,\n2020.\n[115] Y. Wu, Y. Zhao, B. Hu, P . Minervini, P . Stenetorp, and S. Riedel,\n“An efficient memory-augmented transformer for knowledge-\nintensive NLP tasks,” in EMNLP, 2022, pp. 5184–5196.\n[116] L. Luo, J. Ju, B. Xiong, Y.-F. Li, G. Haffari, and S. Pan, “Chatrule:\nMining logical rules with large language models for knowledge\ngraph reasoning,” arXiv preprint arXiv:2309.01538, 2023.\n[117] J. Wang, Q. Sun, N. Chen, X. Li, and M. Gao, “Boosting language\nmodels reasoning with chain-of-knowledge prompting,” arXiv\npreprint arXiv:2306.06427, 2023.\n[118] Z. Jiang, F. F. Xu, J. Araki, and G. Neubig, “How can we know\nwhat language models know?” Transactions of the Association for\nComputational Linguistics, vol. 8, pp. 423–438, 2020.\n[119] T. Shin, Y. Razeghi, R. L. Logan IV , E. Wallace, and S. Singh, “Au-\ntoprompt: Eliciting knowledge from language models with au-\ntomatically generated prompts,” arXiv preprint arXiv:2010.15980 ,\n2020.\n[120] Z. Meng, F. Liu, E. Shareghi, Y. Su, C. Collins, and N. Collier,\n“Rewire-then-probe: A contrastive recipe for probing biomedi-\ncal knowledge of pre-trained language models,” arXiv preprint\narXiv:2110.08173, 2021.\n[121] L. Luo, T.-T. Vu, D. Phung, and G. Haffari, “Systematic assess-\nment of factual knowledge in large language models,” inEMNLP,\n2023.\n[122] V . Swamy, A. Romanou, and M. Jaggi, “Interpreting language\nmodels through knowledge graph extraction,” arXiv preprint\narXiv:2111.08546, 2021.\n[123] S. Li, X. Li, L. Shang, Z. Dong, C. Sun, B. Liu, Z. Ji, X. Jiang,\nand Q. Liu, “How pre-trained language models capture fac-\ntual knowledge? a causal-inspired analysis,” arXiv preprint\narXiv:2203.16747, 2022.\n[124] H. Tian, C. Gao, X. Xiao, H. Liu, B. He, H. Wu, H. Wang, and\nF. Wu, “SKEP: Sentiment knowledge enhanced pre-training for\nsentiment analysis,” in ACL, 2020, pp. 4067–4076.\n[125] W. Yu, C. Zhu, Y. Fang, D. Yu, S. Wang, Y. Xu, M. Zeng, and\nM. Jiang, “Dict-BERT: Enhancing language model pre-training\nwith dictionary,” in ACL, 2022, pp. 1907–1918.\n[126] T. McCoy, E. Pavlick, and T. Linzen, “Right for the wrong reasons:\nDiagnosing syntactic heuristics in natural language inference,” in\nACL, 2019, pp. 3428–3448.\n[127] D. Wilmot and F. Keller, “Memory and knowledge augmented\nlanguage models for inferring salience in long-form stories,” in\nEMNLP, 2021, pp. 851–865.\n[128] L. Adolphs, S. Dhuliawala, and T. Hofmann, “How to query\nlanguage models?” arXiv preprint arXiv:2108.01928, 2021.\n[129] M. Sung, J. Lee, S. Yi, M. Jeon, S. Kim, and J. Kang, “Can language\nmodels be biomedical knowledge bases?” in EMNLP, 2021, pp.\n4723–4734.\n[130] A. Mallen, A. Asai, V . Zhong, R. Das, H. Hajishirzi, and\nD. Khashabi, “When not to trust language models: Investigating\neffectiveness and limitations of parametric and non-parametric\nmemories,” arXiv preprint arXiv:2212.10511, 2022.\n[131] M. Yasunaga, H. Ren, A. Bosselut, P . Liang, and J. Leskovec, “QA-\nGNN: Reasoning with language models and knowledge graphs\nfor question answering,” in NAACL, 2021, pp. 535–546.\n[132] M. Nayyeri, Z. Wang, M. Akter, M. M. Alam, M. R. A. H.\nRony, J. Lehmann, S. Staab et al., “Integrating knowledge graph\nembedding and pretrained language models in hypercomplex\nspaces,” arXiv preprint arXiv:2208.02743, 2022.\n[133] N. Huang, Y. R. Deshpande, Y. Liu, H. Alberts, K. Cho,\nC. Vania, and I. Calixto, “Endowing language models with\nmultimodal knowledge graph representations,” arXiv preprint\narXiv:2206.13163, 2022.\n[134] M. M. Alam, M. R. A. H. Rony, M. Nayyeri, K. Mohiuddin, M. M.\nAkter, S. Vahdati, and J. Lehmann, “Language model guided\nknowledge graph embeddings,” IEEE Access, vol. 10, pp. 76 008–\n76 020, 2022.\n[135] X. Wang, Q. He, J. Liang, and Y. Xiao, “Language models as\nknowledge embeddings,” arXiv preprint arXiv:2206.12617, 2022.\n[136] N. Zhang, X. Xie, X. Chen, S. Deng, C. Tan, F. Huang,\nX. Cheng, and H. Chen, “Reasoning through memorization:\nNearest neighbor knowledge graph embeddings,” arXiv preprint\narXiv:2201.05575, 2022.\n[137] X. Xie, Z. Li, X. Wang, Y. Zhu, N. Zhang, J. Zhang, S. Cheng,\nB. Tian, S. Deng, F. Xiong, and H. Chen, “Lambdakg: A library\nfor pre-trained language model-based knowledge graph embed-\ndings,” 2022.\n[138] B. Kim, T. Hong, Y. Ko, and J. Seo, “Multi-task learning for knowl-\nedge graph completion with pre-trained language models,” in\nCOLING, 2020, pp. 1737–1743.\n[139] X. Lv, Y. Lin, Y. Cao, L. Hou, J. Li, Z. Liu, P . Li, and J. Zhou,\n“Do pre-trained models benefit knowledge graph completion? A\nreliable evaluation and a reasonable approach,” in ACL, 2022, pp.\n3570–3581.\n[140] J. Shen, C. Wang, L. Gong, and D. Song, “Joint language semantic\nand structure embedding for knowledge graph completion,” in\nCOLING, 2022, pp. 1965–1978.\n[141] B. Choi, D. Jang, and Y. Ko, “MEM-KGC: masked entity model for\nknowledge graph completion with pre-trained language model,”\nIEEE Access, vol. 9, pp. 132 025–132 032, 2021.\n[142] B. Choi and Y. Ko, “Knowledge graph extension with a pre-\ntrained language model via unified learning method,” Knowl.\nBased Syst., vol. 262, p. 110245, 2023.\n[143] B. Wang, T. Shen, G. Long, T. Zhou, Y. Wang, and Y. Chang,\n“Structure-augmented text representation learning for efficient\nknowledge graph completion,” in WWW, 2021, pp. 1737–1748.\n[144] L. Wang, W. Zhao, Z. Wei, and J. Liu, “Simkgc: Simple contrastive\nknowledge graph completion with pre-trained language mod-\nels,” in ACL, 2022, pp. 4281–4294.\n[145] D. Li, M. Yi, and Y. He, “Lp-bert: Multi-task pre-training\nknowledge graph bert for link prediction,” arXiv preprint\narXiv:2201.04843, 2022.\n[146] A. Saxena, A. Kochsiek, and R. Gemulla, “Sequence-to-sequence\nknowledge graph completion and question answering,” in ACL,\n2022, pp. 2814–2828.\n[147] C. Chen, Y. Wang, B. Li, and K. Lam, “Knowledge is flat: A\nseq2seq generative framework for various knowledge graph\ncompletion,” in COLING, 2022, pp. 4005–4017.\n[148] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\nand L. Zettlemoyer, “Deep contextualized word representations,”\nin NAACL, 2018, pp. 2227–2237.\n[149] H. Yan, T. Gui, J. Dai, Q. Guo, Z. Zhang, and X. Qiu, “A unified\ngenerative framework for various NER subtasks,” in ACL, 2021,\npp. 5808–5822.\n[150] Y. Onoe and G. Durrett, “Learning to denoise distantly-labeled\ndata for entity typing,” in NAACL, 2019, pp. 2407–2417.\n[151] Y. Onoe, M. Boratko, A. McCallum, and G. Durrett, “Modeling\nfine-grained entity types with box embeddings,” in ACL, 2021,\npp. 2051–2064.\n[152] B. Z. Li, S. Min, S. Iyer, Y. Mehdad, and W. Yih, “Efficient one-\npass end-to-end entity linking for questions,” in EMNLP, 2020,\npp. 6433–6441.\n[153] T. Ayoola, S. Tyagi, J. Fisher, C. Christodoulopoulos, and A. Pier-\nleoni, “Refined: An efficient zero-shot-capable approach to end-\nto-end entity linking,” in NAACL, 2022, pp. 209–220.\n[154] M. Joshi, O. Levy, L. Zettlemoyer, and D. S. Weld, “BERT for\ncoreference resolution: Baselines and analysis,” in EMNLP, 2019,\npp. 5802–5807.\n[155] M. Joshi, D. Chen, Y. Liu, D. S. Weld, L. Zettlemoyer, and\nO. Levy, “Spanbert: Improving pre-training by representing and\npredicting spans,” Trans. Assoc. Comput. Linguistics , vol. 8, pp.\n64–77, 2020.\n[156] A. Caciularu, A. Cohan, I. Beltagy, M. E. Peters, A. Cattan,\nand I. Dagan, “CDLM: cross-document language modeling,” in\nEMNLP, 2021, pp. 2648–2662.\n[157] A. Cattan, A. Eirew, G. Stanovsky, M. Joshi, and I. Dagan, “Cross-\ndocument coreference resolution over predicted mentions,” in\nACL, 2021, pp. 5100–5107.\n[158] Y. Wang, Y. Shen, and H. Jin, “An end-to-end actor-critic-based\nneural coreference resolution system,” in IEEE International Con-\nference on Acoustics, Speech and Signal Processing, ICASSP 2021,\nToronto, ON, Canada, June 6-11, 2021, 2021, pp. 7848–7852.\n[159] P . Shi and J. Lin, “Simple BERT models for relation extraction and\nsemantic role labeling,” CoRR, vol. abs/1904.05255, 2019.\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 25\n[160] S. Park and H. Kim, “Improving sentence-level relation extraction\nthrough curriculum learning,” CoRR, vol. abs/2107.09332, 2021.\n[161] Y. Ma, A. Wang, and N. Okazaki, “DREEAM: guiding attention\nwith evidence for improving document-level relation extraction,”\nin EACL, 2023, pp. 1963–1975.\n[162] Q. Guo, Y. Sun, G. Liu, Z. Wang, Z. Ji, Y. Shen, and X. Wang, “Con-\nstructing chinese historical literature knowledge graph based\non bert,” in Web Information Systems and Applications: 18th Inter-\nnational Conference, WISA 2021, Kaifeng, China, September 24–26,\n2021, Proceedings 18. Springer, 2021, pp. 323–334.\n[163] J. Han, N. Collier, W. Buntine, and E. Shareghi, “Pive: Prompt-\ning with iterative verification improving graph-based generative\ncapability of llms,” arXiv preprint arXiv:2305.12392, 2023.\n[164] A. Bosselut, H. Rashkin, M. Sap, C. Malaviya, A. Celikyilmaz,\nand Y. Choi, “Comet: Commonsense transformers for knowledge\ngraph construction,” in ACL, 2019.\n[165] S. Hao, B. Tan, K. Tang, H. Zhang, E. P . Xing, and Z. Hu, “Bertnet:\nHarvesting knowledge graphs from pretrained language mod-\nels,” arXiv preprint arXiv:2206.14268, 2022.\n[166] P . West, C. Bhagavatula, J. Hessel, J. Hwang, L. Jiang, R. Le Bras,\nX. Lu, S. Welleck, and Y. Choi, “Symbolic knowledge distillation:\nfrom general language models to commonsense models,” in\nNAACL, 2022, pp. 4602–4625.\n[167] L. F. R. Ribeiro, M. Schmitt, H. Sch ¨utze, and I. Gurevych, “Investi-\ngating pretrained language models for graph-to-text generation,”\nin Proceedings of the 3rd Workshop on Natural Language Processing\nfor Conversational AI, 2021, pp. 211–227.\n[168] J. Li, T. Tang, W. X. Zhao, Z. Wei, N. J. Yuan, and J.-R. Wen,\n“Few-shot knowledge graph-to-text generation with pretrained\nlanguage models,” in ACL, 2021, pp. 1558–1568.\n[169] A. Colas, M. Alvandipour, and D. Z. Wang, “GAP: A graph-\naware language model framework for knowledge graph-to-text\ngeneration,” in Proceedings of the 29th International Conference on\nComputational Linguistics, 2022, pp. 5755–5769.\n[170] Z. Jin, Q. Guo, X. Qiu, and Z. Zhang, “GenWiki: A dataset of\n1.3 million content-sharing text and graphs for unsupervised\ngraph-to-text generation,” in Proceedings of the 28th International\nConference on Computational Linguistics, 2020, pp. 2398–2409.\n[171] W. Chen, Y. Su, X. Yan, and W. Y. Wang, “KGPT: Knowledge-\ngrounded pre-training for data-to-text generation,” in EMNLP,\n2020, pp. 8635–8648.\n[172] D. Lukovnikov, A. Fischer, and J. Lehmann, “Pretrained trans-\nformers for simple question answering over knowledge graphs,”\nin The Semantic Web–ISWC 2019: 18th International Semantic Web\nConference, Auckland, New Zealand, October 26–30, 2019, Proceed-\nings, Part I 18. Springer, 2019, pp. 470–486.\n[173] D. Luo, J. Su, and S. Yu, “A bert-based approach with relation-\naware attention for knowledge base question answering,” in\nIJCNN. IEEE, 2020, pp. 1–8.\n[174] N. Hu, Y. Wu, G. Qi, D. Min, J. Chen, J. Z. Pan, and Z. Ali, “An\nempirical study of pre-trained language models in simple knowl-\nedge graph question answering,” arXiv preprint arXiv:2303.10368,\n2023.\n[175] Y. Xu, C. Zhu, R. Xu, Y. Liu, M. Zeng, and X. Huang, “Fusing\ncontext into knowledge graph for commonsense question an-\nswering,” in ACL, 2021, pp. 1201–1207.\n[176] M. Zhang, R. Dai, M. Dong, and T. He, “Drlk: Dynamic hierar-\nchical reasoning with language model and knowledge graph for\nquestion answering,” in EMNLP, 2022, pp. 5123–5133.\n[177] Z. Hu, Y. Xu, W. Yu, S. Wang, Z. Yang, C. Zhu, K.-W. Chang, and\nY. Sun, “Empowering language models with knowledge graph\nreasoning for open-domain question answering,” in EMNLP,\n2022, pp. 9562–9581.\n[178] X. Zhang, A. Bosselut, M. Yasunaga, H. Ren, P . Liang, C. D. Man-\nning, and J. Leskovec, “Greaselm: Graph reasoning enhanced\nlanguage models,” in ICLR, 2022.\n[179] X. Cao and Y. Liu, “Relmkg: reasoning with pre-trained language\nmodels and knowledge graphs for complex question answering,”\nApplied Intelligence, pp. 1–15, 2022.\n[180] X. Huang, J. Zhang, D. Li, and P . Li, “Knowledge graph embed-\nding based question answering,” in WSDM, 2019, pp. 105–113.\n[181] H. Wang, F. Zhang, X. Xie, and M. Guo, “Dkn: Deep knowledge-\naware network for news recommendation,” in WWW, 2018, pp.\n1835–1844.\n[182] B. Yang, S. W.-t. Yih, X. He, J. Gao, and L. Deng, “Embedding\nentities and relations for learning and inference in knowledge\nbases,” in ICLR, 2015.\n[183] W. Xiong, M. Yu, S. Chang, X. Guo, and W. Y. Wang, “One-shot\nrelational learning for knowledge graphs,” in EMNLP, 2018, pp.\n1980–1990.\n[184] P . Wang, J. Han, C. Li, and R. Pan, “Logic attention based\nneighborhood aggregation for inductive knowledge graph em-\nbedding,” in AAAI, vol. 33, no. 01, 2019, pp. 7152–7159.\n[185] Y. Lin, Z. Liu, M. Sun, Y. Liu, and X. Zhu, “Learning entity\nand relation embeddings for knowledge graph completion,” in\nProceedings of the AAAI conference on artificial intelligence , vol. 29,\nno. 1, 2015.\n[186] C. Chen, Y. Wang, A. Sun, B. Li, and L. Kwok-Yan, “Dipping plms\nsauce: Bridging structure and text for effective knowledge graph\ncompletion via conditional soft prompting,” in ACL, 2023.\n[187] J. Lovelace and C. P . Ros ´e, “A framework for adapting pre-\ntrained language models to knowledge graph completion,” in\nProceedings of the 2022 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2022, Abu Dhabi, United Arab Emi-\nrates, December 7-11, 2022, 2022, pp. 5937–5955.\n[188] J. Fu, L. Feng, Q. Zhang, X. Huang, and P . Liu, “Larger-context\ntagging: When and why does it work?” in Proceedings of the\n2021 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, NAACL-\nHLT 2021, Online, June 6-11, 2021, 2021, pp. 1463–1475.\n[189] X. Liu, K. Ji, Y. Fu, Z. Du, Z. Yang, and J. Tang, “P-tuning\nv2: Prompt tuning can be comparable to fine-tuning universally\nacross scales and tasks,” CoRR, vol. abs/2110.07602, 2021.\n[190] J. Yu, B. Bohnet, and M. Poesio, “Named entity recognition as\ndependency parsing,” in ACL, 2020, pp. 6470–6476.\n[191] F. Li, Z. Lin, M. Zhang, and D. Ji, “A span-based model for\njoint overlapped and discontinuous named entity recognition,”\nin ACL, 2021, pp. 4814–4828.\n[192] C. Tan, W. Qiu, M. Chen, R. Wang, and F. Huang, “Boundary\nenhanced neural span classification for nested named entity\nrecognition,” in The Thirty-Fourth AAAI Conference on Artificial\nIntelligence, AAAI 2020, The Thirty-Second Innovative Applications\nof Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI\nSymposium on Educational Advances in Artificial Intelligence, EAAI\n2020, New York, NY, USA, February 7-12, 2020, 2020, pp. 9016–9023.\n[193] Y. Xu, H. Huang, C. Feng, and Y. Hu, “A supervised multi-head\nself-attention network for nested named entity recognition,” in\nThirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021,\nThirty-Third Conference on Innovative Applications of Artificial Intel-\nligence, IAAI 2021, The Eleventh Symposium on Educational Advances\nin Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9,\n2021, 2021, pp. 14 185–14 193.\n[194] J. Yu, B. Ji, S. Li, J. Ma, H. Liu, and H. Xu, “S-NER: A concise\nand efficient span-based model for named entity recognition,”\nSensors, vol. 22, no. 8, p. 2852, 2022.\n[195] Y. Fu, C. Tan, M. Chen, S. Huang, and F. Huang, “Nested named\nentity recognition with partially-observed treecrfs,” in AAAI,\n2021, pp. 12 839–12 847.\n[196] C. Lou, S. Yang, and K. Tu, “Nested named entity recognition\nas latent lexicalized constituency parsing,” in Proceedings of the\n60th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27,\n2022, 2022, pp. 6183–6198.\n[197] S. Yang and K. Tu, “Bottom-up constituency parsing and nested\nnamed entity recognition with pointer networks,” in Proceedings\nof the 60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May\n22-27, 2022, 2022, pp. 2403–2416.\n[198] F. Li, Z. Lin, M. Zhang, and D. Ji, “A span-based model for\njoint overlapped and discontinuous named entity recognition,”\nin Proceedings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International Joint Conference\non Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long\nPapers), Virtual Event, August 1-6, 2021, 2021, pp. 4814–4828.\n[199] Q. Liu, H. Lin, X. Xiao, X. Han, L. Sun, and H. Wu, “Fine-grained\nentity typing via label reasoning,” in Proceedings of the 2021\nConference on Empirical Methods in Natural Language Processing,\nEMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11\nNovember, 2021, 2021, pp. 4611–4622.\n[200] H. Dai, Y. Song, and H. Wang, “Ultra-fine entity typing with\nweak supervision from a masked language model,” inProceedings\nof the 59th Annual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Conference on Natural\nLanguage Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers),\nVirtual Event, August 1-6, 2021, 2021, pp. 1790–1799.\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 26\n[201] N. Ding, Y. Chen, X. Han, G. Xu, X. Wang, P . Xie, H. Zheng,\nZ. Liu, J. Li, and H. Kim, “Prompt-learning for fine-grained entity\ntyping,” in Findings of the Association for Computational Linguistics:\nEMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11,\n2022, 2022, pp. 6888–6901.\n[202] W. Pan, W. Wei, and F. Zhu, “Automatic noisy label correction\nfor fine-grained entity typing,” in Proceedings of the Thirty-First\nInternational Joint Conference on Artificial Intelligence, IJCAI 2022,\nVienna, Austria, 23-29 July 2022, 2022, pp. 4317–4323.\n[203] B. Li, W. Yin, and M. Chen, “Ultra-fine entity typing with indi-\nrect supervision from natural language inference,” Trans. Assoc.\nComput. Linguistics, vol. 10, pp. 607–622, 2022.\n[204] S. Broscheit, “Investigating entity knowledge in BERT with sim-\nple neural end-to-end entity linking,” CoRR, vol. abs/2003.05473,\n2020.\n[205] N. D. Cao, G. Izacard, S. Riedel, and F. Petroni, “Autoregressive\nentity retrieval,” in 9th ICLR, ICLR 2021, Virtual Event, Austria,\nMay 3-7, 2021, 2021.\n[206] N. D. Cao, L. Wu, K. Popat, M. Artetxe, N. Goyal, M. Plekhanov,\nL. Zettlemoyer, N. Cancedda, S. Riedel, and F. Petroni, “Mul-\ntilingual autoregressive entity linking,” Trans. Assoc. Comput.\nLinguistics, vol. 10, pp. 274–290, 2022.\n[207] N. D. Cao, W. Aziz, and I. Titov, “Highly parallel autoregressive\nentity linking with discriminative correction,” in Proceedings of\nthe 2021 Conference on Empirical Methods in Natural Language\nProcessing, EMNLP 2021, Virtual Event / Punta Cana, Dominican\nRepublic, 7-11 November, 2021, 2021, pp. 7662–7669.\n[208] K. Lee, L. He, and L. Zettlemoyer, “Higher-order coreference\nresolution with coarse-to-fine inference,” in NAACL, 2018, pp.\n687–692.\n[209] T. M. Lai, T. Bui, and D. S. Kim, “End-to-end neural coreference\nresolution revisited: A simple yet effective baseline,” in IEEE\nInternational Conference on Acoustics, Speech and Signal Processing,\nICASSP 2022, Virtual and Singapore, 23-27 May 2022 , 2022, pp.\n8147–8151.\n[210] W. Wu, F. Wang, A. Yuan, F. Wu, and J. Li, “Corefqa: Coreference\nresolution as query-based span prediction,” in Proceedings of the\n58th Annual Meeting of the Association for Computational Linguistics,\nACL 2020, Online, July 5-10, 2020 , 2020, pp. 6953–6963.\n[211] T. M. Lai, H. Ji, T. Bui, Q. H. Tran, F. Dernoncourt, and W. Chang,\n“A context-dependent gated module for incorporating symbolic\nsemantics into event coreference resolution,” in Proceedings of the\n2021 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, NAACL-\nHLT 2021, Online, June 6-11, 2021, 2021, pp. 3491–3499.\n[212] Y. Kirstain, O. Ram, and O. Levy, “Coreference resolution without\nspan representations,” in Proceedings of the 59th Annual Meeting of\nthe Association for Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Processing, ACL/IJCNLP\n2021, (Volume 2: Short Papers), Virtual Event, August 1-6, 2021 ,\n2021, pp. 14–19.\n[213] R. Thirukovalluru, N. Monath, K. Shridhar, M. Zaheer,\nM. Sachan, and A. McCallum, “Scaling within document corefer-\nence to long texts,” in Findings of the Association for Computational\nLinguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021 , ser.\nFindings of ACL, vol. ACL/IJCNLP 2021, 2021, pp. 3921–3931.\n[214] I. Beltagy, M. E. Peters, and A. Cohan, “Longformer: The long-\ndocument transformer,” CoRR, vol. abs/2004.05150, 2020.\n[215] C. Alt, M. H ¨ubner, and L. Hennig, “Improving relation extraction\nby pre-trained language representations,” in 1st Conference on\nAutomated Knowledge Base Construction, AKBC 2019, Amherst, MA,\nUSA, May 20-22, 2019, 2019.\n[216] L. B. Soares, N. FitzGerald, J. Ling, and T. Kwiatkowski, “Match-\ning the blanks: Distributional similarity for relation learning,” in\nACL, 2019, pp. 2895–2905.\n[217] S. Lyu and H. Chen, “Relation classification with entity type\nrestriction,” in Findings of the Association for Computational Lin-\nguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021 , ser.\nFindings of ACL, vol. ACL/IJCNLP 2021, 2021, pp. 390–395.\n[218] J. Zheng and Z. Chen, “Sentence-level relation extraction via\ncontrastive learning with descriptive relation prompts,” CoRR,\nvol. abs/2304.04935, 2023.\n[219] H. Wang, C. Focke, R. Sylvester, N. Mishra, and W. Y. Wang,\n“Fine-tune bert for docred with two-step process,” CoRR, vol.\nabs/1909.11898, 2019.\n[220] H. Tang, Y. Cao, Z. Zhang, J. Cao, F. Fang, S. Wang, and P . Yin,\n“HIN: hierarchical inference network for document-level relation\nextraction,” in P AKDD, ser. Lecture Notes in Computer Science,\nvol. 12084, 2020, pp. 197–209.\n[221] D. Wang, W. Hu, E. Cao, and W. Sun, “Global-to-local neural\nnetworks for document-level relation extraction,” in Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language\nProcessing, EMNLP 2020, Online, November 16-20, 2020 , 2020, pp.\n3711–3721.\n[222] S. Zeng, Y. Wu, and B. Chang, “SIRE: separate intra- and\ninter-sentential reasoning for document-level relation extrac-\ntion,” in Findings of the Association for Computational Linguistics:\nACL/IJCNLP 2021, Online Event, August 1-6, 2021, ser. Findings of\nACL, vol. ACL/IJCNLP 2021, 2021, pp. 524–534.\n[223] G. Nan, Z. Guo, I. Sekulic, and W. Lu, “Reasoning with latent\nstructure refinement for document-level relation extraction,” in\nACL, 2020, pp. 1546–1557.\n[224] S. Zeng, R. Xu, B. Chang, and L. Li, “Double graph based\nreasoning for document-level relation extraction,” in Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language\nProcessing, EMNLP 2020, Online, November 16-20, 2020 , 2020, pp.\n1630–1640.\n[225] N. Zhang, X. Chen, X. Xie, S. Deng, C. Tan, M. Chen, F. Huang,\nL. Si, and H. Chen, “Document-level relation extraction as se-\nmantic segmentation,” in IJCAI, 2021, pp. 3999–4006.\n[226] O. Ronneberger, P . Fischer, and T. Brox, “U-net: Convolutional\nnetworks for biomedical image segmentation,” in Medical Image\nComputing and Computer-Assisted Intervention - MICCAI 2015 -\n18th International Conference Munich, Germany, October 5 - 9, 2015,\nProceedings, Part III, ser. Lecture Notes in Computer Science, vol.\n9351, 2015, pp. 234–241.\n[227] W. Zhou, K. Huang, T. Ma, and J. Huang, “Document-level rela-\ntion extraction with adaptive thresholding and localized context\npooling,” in AAAI, 2021, pp. 14 612–14 620.\n[228] C. Gardent, A. Shimorina, S. Narayan, and L. Perez-Beltrachini,\n“The WebNLG challenge: Generating text from RDF data,” in\nProceedings of the 10th International Conference on Natural Language\nGeneration, 2017, pp. 124–133.\n[229] J. Guan, Y. Wang, and M. Huang, “Story ending generation with\nincremental encoding and commonsense knowledge,” in AAAI,\n2019, pp. 6473–6480.\n[230] H. Zhou, T. Young, M. Huang, H. Zhao, J. Xu, and X. Zhu,\n“Commonsense knowledge aware conversation generation with\ngraph attention,” in IJCAI, 2018, pp. 4623–4629.\n[231] M. Kale and A. Rastogi, “Text-to-text pre-training for data-to-text\ntasks,” in Proceedings of the 13th International Conference on Natural\nLanguage Generation, 2020, pp. 97–102.\n[232] M. Mintz, S. Bills, R. Snow, and D. Jurafsky, “Distant supervision\nfor relation extraction without labeled data,” in ACL, 2009, pp.\n1003–1011.\n[233] A. Saxena, A. Tripathi, and P . Talukdar, “Improving multi-hop\nquestion answering over knowledge graphs using knowledge\nbase embeddings,” in ACL, 2020, pp. 4498–4507.\n[234] Y. Feng, X. Chen, B. Y. Lin, P . Wang, J. Yan, and X. Ren, “Scalable\nmulti-hop relational reasoning for knowledge-aware question\nanswering,” in EMNLP, 2020, pp. 1295–1309.\n[235] Y. Yan, R. Li, S. Wang, H. Zhang, Z. Daoguang, F. Zhang, W. Wu,\nand W. Xu, “Large-scale relation learning for question answering\nover knowledge bases with pre-trained language models,” in\nEMNLP, 2021, pp. 3653–3660.\n[236] J. Zhang, X. Zhang, J. Yu, J. Tang, J. Tang, C. Li, and H. Chen,\n“Subgraph retrieval enhanced model for multi-hop knowledge\nbase question answering,” in ACL (Volume 1: Long Papers) , 2022,\npp. 5773–5784.\n[237] J. Jiang, K. Zhou, Z. Dong, K. Ye, W. X. Zhao, and J.-R. Wen,\n“Structgpt: A general framework for large language model to\nreason over structured data,” arXiv preprint arXiv:2305.09645 ,\n2023.\n[238] H. Zhu, H. Peng, Z. Lyu, L. Hou, J. Li, and J. Xiao, “Pre-training\nlanguage model incorporating domain-specific heterogeneous\nknowledge into a unified representation,” Expert Systems with\nApplications, vol. 215, p. 119369, 2023.\n[239] C. Feng, X. Zhang, and Z. Fei, “Knowledge solver: Teaching llms\nto search for domain knowledge from knowledge graphs,” arXiv\npreprint arXiv:2309.03118, 2023.\n[240] J. Sun, C. Xu, L. Tang, S. Wang, C. Lin, Y. Gong, H.-Y. Shum,\nand J. Guo, “Think-on-graph: Deep and responsible reasoning\nof large language model with knowledge graph,” arXiv preprint\narXiv:2307.07697, 2023.\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 27\n[241] B. He, D. Zhou, J. Xiao, X. Jiang, Q. Liu, N. J. Yuan, and T. Xu,\n“BERT-MK: Integrating graph contextualized knowledge into\npre-trained language models,” in EMNLP, 2020, pp. 2281–2290.\n[242] Y. Su, X. Han, Z. Zhang, Y. Lin, P . Li, Z. Liu, J. Zhou, and M. Sun,\n“Cokebert: Contextual knowledge selection and embedding to-\nwards enhanced pre-trained language models,” AI Open, vol. 2,\npp. 127–134, 2021.\n[243] D. Yu, C. Zhu, Y. Yang, and M. Zeng, “JAKET: joint pre-training of\nknowledge graph and language understanding,” in AAAI, 2022,\npp. 11 630–11 638.\n[244] X. Wang, P . Kapanipathi, R. Musa, M. Yu, K. Talamadupula,\nI. Abdelaziz, M. Chang, A. Fokoue, B. Makni, N. Mattei, and\nM. Witbrock, “Improving natural language inference using exter-\nnal knowledge in the science questions domain,” in AAAI, 2019,\npp. 7208–7215.\n[245] Y. Sun, Q. Shi, L. Qi, and Y. Zhang, “JointLK: Joint reasoning\nwith language models and knowledge graphs for commonsense\nquestion answering,” in NAACL, 2022, pp. 5049–5060.\n[246] X. Liu, H. Yu, H. Zhang, Y. Xu, X. Lei, H. Lai, Y. Gu, H. Ding,\nK. Men, K. Yang et al., “Agentbench: Evaluating llms as agents,”\narXiv preprint arXiv:2308.03688, 2023.\n[247] Y. Wang, N. Lipka, R. A. Rossi, A. Siu, R. Zhang, and T. Derr,\n“Knowledge graph prompting for multi-document question an-\nswering,” arXiv preprint arXiv:2308.11730, 2023.\n[248] A. Zeng, M. Liu, R. Lu, B. Wang, X. Liu, Y. Dong, and J. Tang,\n“Agenttuning: Enabling generalized agent abilities for llms,”\n2023.\n[249] W. Kry ´sci´nski, B. McCann, C. Xiong, and R. Socher, “Evaluating\nthe factual consistency of abstractive text summarization,” arXiv\npreprint arXiv:1910.12840, 2019.\n[250] Z. Ji, Z. Liu, N. Lee, T. Yu, B. Wilie, M. Zeng, and P . Fung, “Rho\n(\\ρ): Reducing hallucination in open-domain dialogues with\nknowledge grounding,” arXiv preprint arXiv:2212.01588, 2022.\n[251] S. Feng, V . Balachandran, Y. Bai, and Y. Tsvetkov, “Factkb: Gen-\neralizable factuality evaluation using language models enhanced\nwith factual knowledge,” arXiv preprint arXiv:2305.08281, 2023.\n[252] Y. Yao, P . Wang, B. Tian, S. Cheng, Z. Li, S. Deng, H. Chen, and\nN. Zhang, “Editing large language models: Problems, methods,\nand opportunities,” arXiv preprint arXiv:2305.13172, 2023.\n[253] Z. Li, N. Zhang, Y. Yao, M. Wang, X. Chen, and H. Chen,\n“Unveiling the pitfalls of knowledge editing for large language\nmodels,” arXiv preprint arXiv:2310.02129, 2023.\n[254] R. Cohen, E. Biran, O. Yoran, A. Globerson, and M. Geva,\n“Evaluating the ripple effects of knowledge editing in language\nmodels,” arXiv preprint arXiv:2307.12976, 2023.\n[255] S. Diao, Z. Huang, R. Xu, X. Li, Y. Lin, X. Zhou, and T. Zhang,\n“Black-box prompt learning for pre-trained language models,”\narXiv preprint arXiv:2201.08531, 2022.\n[256] T. Sun, Y. Shao, H. Qian, X. Huang, and X. Qiu, “Black-box tuning\nfor language-model-as-a-service,” in International Conference on\nMachine Learning. PMLR, 2022, pp. 20 841–20 855.\n[257] X. Chen, A. Shrivastava, and A. Gupta, “NEIL: extracting visual\nknowledge from web data,” in IEEE International Conference on\nComputer Vision, ICCV 2013, Sydney, Australia, December 1-8, 2013,\n2013, pp. 1409–1416.\n[258] M. Warren and P . J. Hayes, “Bounding ambiguity: Experiences\nwith an image annotation system,” in Proceedings of the 1st Work-\nshop on Subjectivity, Ambiguity and Disagreement in Crowdsourcing ,\nser. CEUR Workshop Proceedings, vol. 2276, 2018, pp. 41–54.\n[259] Z. Chen, Y. Huang, J. Chen, Y. Geng, Y. Fang, J. Z. Pan, N. Zhang,\nand W. Zhang, “Lako: Knowledge-driven visual estion answer-\ning via late knowledge-to-text injection,” 2022.\n[260] R. Girdhar, A. El-Nouby, Z. Liu, M. Singh, K. V . Alwala, A. Joulin,\nand I. Misra, “Imagebind: One embedding space to bind them\nall,” in ICCV, 2023, pp. 15 180–15 190.\n[261] J. Zhang, Z. Yin, P . Chen, and S. Nichele, “Emotion recognition\nusing multi-modal data and machine learning techniques: A\ntutorial and review,” Information Fusion , vol. 59, pp. 103–126,\n2020.\n[262] H. Zhang, B. Wu, X. Yuan, S. Pan, H. Tong, and J. Pei, “Trust-\nworthy graph neural networks: Aspects, methods and trends,”\narXiv:2205.07424, 2022.\n[263] T. Wu, M. Caccia, Z. Li, Y.-F. Li, G. Qi, and G. Haffari, “Pretrained\nlanguage model in continual learning: A comparative study,” in\nICLR, 2022.\n[264] X. L. Li, A. Kuncoro, J. Hoffmann, C. de Masson d’Autume,\nP . Blunsom, and A. Nematzadeh, “A systematic investigation of\ncommonsense knowledge in large language models,” in Proceed-\nings of the 2022 Conference on Empirical Methods in Natural Language\nProcessing, 2022, pp. 11 838–11 855.\n[265] Y. Zheng, H. Y. Koh, J. Ju, A. T. Nguyen, L. T. May, G. I. Webb, and\nS. Pan, “Large language models for scientific synthesis, inference\nand explanation,” arXiv preprint arXiv:2310.07984, 2023.\n[266] B. Min, H. Ross, E. Sulem, A. P . B. Veyseh, T. H. Nguyen, O. Sainz,\nE. Agirre, I. Heintz, and D. Roth, “Recent advances in natural\nlanguage processing via large pre-trained language models: A\nsurvey,” ACM Computing Surveys, vol. 56, no. 2, pp. 1–40, 2023.\n[267] J. Wei, M. Bosma, V . Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,\nA. M. Dai, and Q. V . Le, “Finetuned language models are zero-\nshot learners,” in International Conference on Learning Representa-\ntions, 2021.\n[268] Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao,\nY. Zhang, Y. Chen, L. Wang, A. T. Luu, W. Bi, F. Shi, and S. Shi,\n“Siren’s song in the ai ocean: A survey on hallucination in large\nlanguage models,” arXiv preprint arXiv:2309.01219, 2023.\nAPPENDIX A\nPROS AND CONS FOR LLM S AND KGS\nIn this section, we introduce the pros and cons of LLMs and\nKGs in detail. We summarize the pros and cons of LLMs\nand KGs in Fig. 1, respectively.\nLLM pros.\n• General Knowledge [11]: LLMs pre-trained on large-\nscale corpora, which contain a large amount of gen-\neral knowledge, such as commonsense knowledge\n[264] and factual knowledge [14]. Such knowledge\ncan be distilled from LLMs and used for downstream\ntasks [265].\n• Language Processing [12]: LLMs have shown great per-\nformance in understanding natural language [266].\nTherefore, LLMs can be used in many natural lan-\nguage processing tasks, such as question answering\n[4], machine translation [5], and text generation [6].\n• Generalizability [13]: LLMs enable great generalizabil-\nity, which can be applied to various downstream\ntasks [267]. By providing few-shot examples [59] or\nfinetuning on multi-task data [3], LLMs achieve great\nperformance on many tasks.\nLLM cons.\n• Implicit Knowledge [14]: LLMs represent knowledge\nimplicitly in their parameters. It is difficult to inter-\npret or validate the knowledge obtained by LLMs.\n• Hallucination [15]: LLMs often experience hallucina-\ntions by generating content that while seemingly\nplausible but are factually incorrect [268]. This prob-\nlem greatly reduces the trustworthiness of LLMs in\nreal-world scenarios.\n• Indecisiveness [16]: LLMs perform reasoning by gen-\nerating from a probability model, which is an in-\ndecisive process. The generated results are sampled\nfrom the probability distribution, which is difficult to\ncontrol.\n• Black-box [17]: LLMs are criticized for their lack of\ninterpretability. It is unclear to know the specific pat-\nterns and functions LLMs use to arrive at predictions\nor decisions.\n• Lacking Domain-specific/New Knowledge [18]: LLMs\ntrained on general corpus might not be able to gen-\neralize well to specific domains or new knowledge\ndue to the lack of domain-specific knowledge or new\ntraining data.\nJOURNAL OF LATEX CLASS FILES, VOL. ??, NO. ??, MONTH 20YY 28\nKG pros.\n• Structural Knowledge [19]: KGs store facts in a struc-\ntural format (i.e., triples), which can be understand-\nable by both humans and machines.\n• Accuracy [20]: Facts in KGs are usually manually\ncurated or validated by experts, which are more\naccurate and dependable than those in LLMs.\n• Decisiveness [21]: The factual knowledge in KGs is\nstored in a decisive manner. The reasoning algorithm\nin KGs is also deterministic, which can provide deci-\nsive results.\n• Interpretability [22]: KGs are renowned for their sym-\nbolic reasoning ability, which provides an inter-\npretable reasoning process that can be understood\nby humans.\n• Domain-specific Knowledge [23]: Many domains can\nconstruct their KGs by experts to provide precise and\ndependable domain-specific knowledge.\n• Evolving Knowledge [24]: The facts in KGs are contin-\nuously evolving. The KGs can be updated with new\nfacts by inserting new triples and deleting outdated\nones.\nKG cons.\n• Incompleteness [25]: KGs are hard to construct and\noften incomplete, which limits the ability of KGs to\nprovide comprehensive knowledge.\n• Lacking Language Understanding [33]: Most studies on\nKGs model the structure of knowledge, but ignore\nthe textual information in KGs. The textual informa-\ntion in KGs is often ignored in KG-related tasks, such\nas KG completion [26] and KGQA [43].\n• Unseen Facts [27]: KGs are dynamically changing,\nwhich makes it difficult to model unseen entities and\nrepresent new facts.",
  "topic": "Inference",
  "concepts": [
    {
      "name": "Inference",
      "score": 0.4117690324783325
    },
    {
      "name": "Computer science",
      "score": 0.38965314626693726
    },
    {
      "name": "Artificial intelligence",
      "score": 0.34717047214508057
    }
  ],
  "institutions": [],
  "cited_by": 98
}