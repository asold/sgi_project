{
  "title": "Exploring the Impact of the Output Format on the Evaluation of Large Language Models for Code Translation",
  "url": "https://openalex.org/W4393284497",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2682600614",
      "name": "Marcos Macedo",
      "affiliations": [
        "Queen's University"
      ]
    },
    {
      "id": "https://openalex.org/A1038466737",
      "name": "Yuan Tian",
      "affiliations": [
        "Queen's University"
      ]
    },
    {
      "id": null,
      "name": "Filipe Cogo",
      "affiliations": [
        "Huawei Technologies (Canada)"
      ]
    },
    {
      "id": "https://openalex.org/A2115051412",
      "name": "Bram Adams",
      "affiliations": [
        "Queen's University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4312971267",
    "https://openalex.org/W4391558635",
    "https://openalex.org/W2074032109",
    "https://openalex.org/W4386721862",
    "https://openalex.org/W3211962263",
    "https://openalex.org/W4376167329",
    "https://openalex.org/W4380993527",
    "https://openalex.org/W1972141422",
    "https://openalex.org/W2090878800",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2250342921",
    "https://openalex.org/W4394664141",
    "https://openalex.org/W3098044990",
    "https://openalex.org/W4386185625",
    "https://openalex.org/W2353739181",
    "https://openalex.org/W4285024991",
    "https://openalex.org/W4389364446",
    "https://openalex.org/W3148399464",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W4389524484",
    "https://openalex.org/W4283799640",
    "https://openalex.org/W4287328196",
    "https://openalex.org/W3154248444"
  ],
  "abstract": "Code translation between programming languages (PLs) is a critical task in software engineering, facilitating the modernization of legacy systems, ensuring cross-platform compatibility, and enhancing software performance. Most existing studies instruct LLMs to perform code translation and evaluate their performance by either running the generated outputs through test suites or comparing them to reference outputs (ground truth). These outputs, however, may contain not only executable source code but also additional non-code elements, such as natural language explanations or formatting tokens. We refer to the combination of source code and non-code elements as the output format. It is crucial to understand and address variations in output format, as non-code elements can interfere with evaluation metrics, resulting in biased assessments of model performance and comparisons. We conduct an empirical analysis of the outputs from eleven instruct-tuned open-source LLMs, across five PLs: C, C++, Go, Java, and Python. The results show that between 26.4% and 73.7% of outputs produced by our evaluated LLMs necessitate post-processing. To mitigate output format bias, we propose a strategic combination of prompt engineering and regular expressions that effectively extracts source code from mixed-format outputs, enabling the eleven open-source models to achieve an average Code Extraction Success Rate (CSR) of 92.73%. Our empirical study confirms that output format bias affects widely used execution-based metrics, i.e., Computational Accuracy (CA), and text-based metrics, i.e., BLEU, CodeBLEU and CrystalBLEU. Additionally, we test five closed-source LLMs and observe that they also generate varying distributions of output formats, which could lead to output format biases. Our results highlight the need to mitigate the output format bias to enable reliable evaluations in LLMs for code translation.",
  "full_text": "Noname manuscript No.\n(will be inserted by the editor)\nOutput Format Biases in the Evaluation of Large\nLanguage Models for Code Translation\nMarcos Macedo·Yuan Tian·Filipe R.\nCogo·Bram Adams\nReceived: date / Accepted: date\nAbstractCode translation between programming languages (PLs) is a long-\nexisting and critical task in software engineering, facilitating the modernization\nof legacy systems, ensuring cross-platform compatibility, and enhancing software\nperformance. With the recent advances in Large Language Models (LLMs) and\ntheir applications to code translation, there is an increasing need for comprehen-\nsive evaluation of these models. Most existing studies instruct LLMs to perform\ncode translation and evaluate their performance by either running the generated\noutputs through test suites or comparing them to reference outputs (ground truth).\nThese outputs, however, may contain not only executable source code but also ad-\nditional non-code elements, such as natural language explanations or formatting\ntokens. We refer to the way source code and non-code elements are combined\nasoutput format. It is crucial to understand and address variations in output\nformat, as non-code elements can interfere with evaluation metrics, resulting in\nbiased (inaccurate or unfair) assessments of model performance and comparisons.\nWe refer to this bias asoutput format bias. To investigate the presence of output\nformat biases, we first conduct an empirical analysis of the outputs from eleven\ninstruct-tuned open-source LLMs, applied to 3,820 translation pairs across five\nlanguages: C, C++, Go, Java, and Python. The results show that between 26.4%\nand 73.7% of outputs produced by our evaluated LLMs necessitate post-processing\n(to result in source code for evaluation). To mitigate output format bias, we pro-\npose a strategic combination of prompt engineering and regular expressions that\neffectively extracts source code from mixed-format outputs, enabling the eleven\nopen-source models to achieve an average Code Extraction Success Rate (CSR) of\n92.73%. Our empirical study confirms that output format bias affects widely used\nexecution-based metrics, i.e., Computational Accuracy (CA), and text-based met-\nMarcos Macedo, Yuan Tian, Bram Adams\nSchool of Computing, Queen’s University, ON, Canada\nE-mail: marcos.macedo@queensu.ca, y.tian@queensu.ca, bram.adams@queensu.ca\nFilipe R. Cogo\nCentre for Software Excellence, Huawei Canada\nE-mail: filipe.roseiro.cogo1@huawei.com\narXiv:2403.17214v2  [cs.SE]  13 Oct 2025\n2 Marcos Macedo et al.\nrics, i.e., BLEU, CodeBLEU and CrystalBLEU. Additionally, we test five closed\nLLMs and observe that they also generate varying distributions of output for-\nmats, which could contribute to output format biases. Our results highlight the\nneed to mitigate the output format bias to enable reliable evaluations in LLMs\ncode translation.\nKeywordsCode translation·Output format·Large language model·\nBenchmarking·Bias·Empirical study\n1 Introduction\nAutomated code translation, i.e., translating source code between different pro-\ngramming languages (PLs), promises to save software development teams substan-\ntial time and effort while minimizing the risks associated with manual translation\nerrors and inconsistencies. As more companies seek to migrate their existing soft-\nware systems from outdated PLs to more contemporary ones or adapt their soft-\nware for cloud-based environments where specific PLs are better suited, the value\nof automated code translation becomes increasingly apparent (Weisz et al., 2021).\nTraditionally, code translation has been implemented using transpilers, such as\nC2Rust1 and cxgo2, which rely on hard-coded rules and program analysis to con-\nvert code between languages. These tools are costly to develop and limited to\nspecific language pairs. Furthermore, they often produce translations that do not\nalign with the idiomatic expressions of the target language (Szafraniec et al., 2023).\nRecently, pre-trained models have gained prominence in code translation (Roziere\net al., 2020; Lachaux et al., 2021; Roziere et al., 2021; Szafraniec et al., 2023).\nThese models are pre-trained on large-scale source code datasets and occasionally\nfine-tuned on code translation pairs. LLMs offer new opportunities in code transla-\ntion by enabling prompt engineering, an alternative to traditional model parameter\nadjustments, which leveragespromptsto guide LLMs in addressing diverse tasks.\nThe introduction of LLMs and prompt engineering have significantly advanced\nautomated software engineering (ASE) tasks, elevating their performance to new\nrecords (Fan et al., 2023).\nInterestingly, despite the advancements of LLM-based approaches in different ASE\ntasks, a recent study by Pan et al. 2024 shows that LLMs are yet to prove\ntheir reliability to automate code translation, with the rate of translations that\ncan completely pass the accompanying unit test(s), i.e., Computational Accu-\nracy (CA) (Roziere et al., 2020), ranging from 2.1% to 47.3% on average. Pan et\nal. manually categorized the bugs introduced by LLMs in code translation and\nfound that most (77.8%) unsuccessful translations result in compilation errors. In\nthis paper, we postulate that the practical evaluation of LLMs for code transla-\ntion revolves around the models’ capability to generate correctly translated code.\nNonetheless, generated translations deemed as invalid during LLM evaluation (e.g.,\ndue to compilation errors) often stem from the problem ofinconsistent output for-\nmat, not necessarily from the LLMs’ limitations for code translation. Inconsistent\n1 https://github.com/immunant/c2rust\n2 https://github.com/gotranspile/cxgo\nOutput Format Biases in the Evaluation of LLMs for Code Translation 3\n(a) Direct Output\n1def main ():\n2a, b = input (\" Enter two numbers : \").\nsplit ()\n3a, b = int (a), int (b)\n4if a*b%\n5print (\" Even \")\n6else :\n7print (\" Odd \")\n8\n9### Explanation :\n10\n11The Python code is similar to the Go\ncode , with a few differences in\nsyntax .\n12\n(b) Wrapped Code\n1‘‘‘c\n2# include <stdio .h>\n3# include <string .h>\n4\n5int main () {\n6char input [100];\n7char doubleInput [200];\n8...\n9} else {\n10printf (\" Yes \\n\");\n11}\n12return 0;\n13}\n14‘‘‘\n15\n16This C code does ...\n17\n(c) Unbalanced Back-ticks\n1#!/ usr / bin / env python\n2N, D = map (int , input (). split ())\n3\n4ans = N // (D2 +1)\n5rem = N %\n6\n7if rem != 0:\n8ans += 1\n9\n10print ( ans )\n11‘‘‘\n12\n(d) Re-Wrapped Code\n1‘‘‘\n2Here is the C++ equivalent [...] code :\n3\n4‘‘‘cpp\n5# include <iostream >\n6using namespace std ;\n7\n8int main () {\n9int A, B, K;\n10cin >> A >> B >> K;\n11...\n12return 0;\n13}\n14‘‘‘\n15\n16This C++ code performs ...\n17\nFig. 1Examples of the observed output formats in our study.(a)Python code in Direct\nOutput with Additional text (the explanation).(b)C code wrapped with three back-ticks\nfollowed by the language extension, with Additional text.(c)Python code that has three\nback-ticks at the end but no matching opening back-ticks.(d)This output format occurs\nin RQ2 as the model does not generate code after the back-ticks and instead generates a\ncompletely new code block. Some code examples are shortened for brevity.\noutput format refers to a problem where the translated source code is presented\nin the generated output in various formats and could be interspersed with nat-\nural language (non-source-code) (ref. Fig. 1). Theoutput format biasoccurs\nwhen variations in how the output is structured (output format), rather than\nthe accuracy of the translation, affect the perceived quality of the translation re-\nsult. Therefore, it is important to make researchers and practitioners aware of the\nimpact of inconsistent output format on the evaluations of LLMs for code trans-\nlation. By distinguishing between errors caused by output format bias and those\nstemming from the quality of the translated code, we can more reliably assess the\nability of LLMs to translate source code between different PLs.\nThe output format bias can influence the calculation of many important bench-\nmark metrics for evaluating code translation such as:\n4 Marcos Macedo et al.\n– Text-oriented metrics: Inconsistent output formats lead to wrong estimates\nof text-oriented evaluation metrics, as generated text other than translated\ncode, such as code explanations, is accounted for in their calculations. These\ninclude BLEU (Papineni et al., 2001), CodeBLEU (Ren et al., 2020) and Crys-\ntalBLEU (Eghbali and Pradel, 2022). These metrics measure either the overlap\nof tokens or the semantic similarity between a translation and its reference.\n– Execution-based metrics: Inconsistent output formats lead to wrong esti-\nmates when calculating execution-based metrics such as Computational Accu-\nracy (CA) Roziere et al. (2021). While the code excerpt in the output can be a\npotentially valid translation of the reference source code, the whole output fails\ncompilation. This leads to the conclusion that the model could not generate the\ncorrect translated code when, in fact, the problem stems from the generated\nnatural language text, which does not comply with the source code syntax.\nTo validate our hypothesis on the prevalence of output format bias and explore\nwhether it can be mitigated through cost-effective techniques, such as prompt\nengineering and lightweight post-processing, we conduct a case study analyzing\nthe outputs of 16 popular LLMs (ref. Table 1) for code translation across multiple\nprogramming languages. We first focus on eleven open-source models, which are\nmore likely to deviate from instructions and generate additional text, and then\nexpand the analysis to five closed models. Our investigation spans 3,820 code\ntranslation tasks, with source code samples drawn from the well-known CodeNet\nbenchmark (Puri et al., 2021), and target code generated by the LLMs. The dataset\nincludes 20 translation pairs across five programming languages (C, C++, Go,\nJava, and Python). Specifically, we address the following five research questions\n(RQs) in this case study:\nRQ1What are the characteristics of the output formats across LLMs and\nprompts?We analyze the output formats generated by eleven open-source LLMs\nand find that these models generate outputs in three distinct source code formats,\nsome of which include natural language (additional text) and some that do not. Of\nthese six possible combinations (output format with or without additional text),\nonly one is directly parsable without further processing.\nRQ2To what extent can the output format of LLMs be controlled using\nprompt engineering and lightweight post-processing?We verify that com-\nbining prompt instructions to control the output format with a regular expression\nfor code extraction can increase the proportion of source code extracted from the\ninference output by up to 41.8%.\nRQ3What’s the impact of output control on the reported performance of\nLLMs in terms of CA?We use different combinations of prompt and extraction\nmethods and study their impact on the most widely adopted execution-based\nevaluation metric, i.e., the Computational Accuracy (CA) metric. The proposed\nlightweight approach presents an average CA up to 6 times higher than a direct\ncompilation.\nRQ4Does inconsistent output format introduce bias in text-oriented metric-\nbased evaluation?We analyze the impact of the output format on text-oriented\nmetrics, specifically BLEU (Papineni et al., 2001), CodeBLEU (Ren et al., 2020)\nOutput Format Biases in the Evaluation of LLMs for Code Translation 5\nand CrystalBLEU (Eghbali and Pradel, 2022). Our results show that applying our\nproposed lightweight approach (Regex) to extract source code leads to improve-\nments in BLEU scores in up to 72.08% of cases and in CrystalBLEU scores in up\nto 50.56% of cases, when additional text is present in the output. Furthermore,\nwe observe that the presence of additional text artificially inflates the Data Flow\ncomponent of CodeBLEU, leading to counterintuitive outcomes, such as a decrease\nin CodeBLEU scores when additional text is present alongside the source code.\nRQ5Does output format bias exist in the evaluation of closed LLMs?\nWe investigate whether the problem of multiple output formats also affects closed-\nsource models by manually examining the inference outputs of five closed LLMs:\nGemini 1.5 Flash, GPT-4o, GPT-4o mini, GPT-4, and GPT-3.5 Turbo. Our anal-\nysis reveals that while some models have a consistent output format, some of them\npresent a distribution, similar to the open-source ones. For instance, Gemini 1.5\nFlash, GPT-4o, and GPT-4o mini consistently produce outputs exclusively in the\nWrapped-Code format, allowing regular expressions to achieve a 100% Code Ex-\ntraction Success Rate (CSR). In contrast, GPT-3.5 Turbo and GPT-4 primarily\ngenerate outputs in the Direct Output format, with only a small fraction (1.67%)\nin the Wrapped-Code format.\nThis work extends our prior conference paper (Macedo et al., 2024a), which ini-\ntiated the investigation into the impact of output format on LLM-based code\ntranslation evaluation. The original paper focused on output format bias in open-\nsource LLMs and examined it using a single execution-based metric (i.e., CA). In\nthis extended study, we address two additional research questions: how output for-\nmat inconsistency affects the calculation of two text-oriented metrics (RQ4) and\nwhether closed LLM also experiences inconsistent output formats (RQ5). Addi-\ntionally, we conduct an ablation study on AlphaTrans (Ibrahimzada et al., 2025),\na recent repository-level code translation approach that incorporates our proposed\noutput format mitigation method, to better understand the impact of this miti-\ngation strategy, as discussed in Section 9.3.\nIn summary, the main contributions of this article are as follows:\n–We show how often open-source and closed LLMs consistently output the ex-\npected code format for benchmark evaluation in code translation.\n–We demonstrate a cost-effective output control method to increase the chances\nof reliably retrieving the source code from the generation output models and\ncalculate the method’s effectiveness for code translation.\n–We assess the impact of output control on the reported performance of LLMs\nin code translation in terms of compilation rate and CA.\n–We constructed a new dataset with human-written ground truth translations\nby augmenting the Pan et al. dataset with accepted solutions from the CodeNet\nrepository, enabling the use of text-based evaluation metrics such as BLEU,\nCodeBLEU and CrystalBLEU.\n–We show the influence of natural language in evaluating code translation using\ntext-oriented metrics.\n6 Marcos Macedo et al.\n–We demonstrate the significant practical impact of addressing output format\nbias through an ablation study on a state-of-the-art, repository-level code trans-\nlation system.\n–To foster future research in the area, we have made our replication package\npublicly available on GitHub3.\nThis paper is organized as follows. Section 2 provides background about literature\nwork and code translation evaluation. Section 3 presents our study’s design and\nimplementation details. In Sections 4–8, we present the motivation, approach, and\nresults for each of the five raised research questions. We discuss the implications\nand suggestions from our study in Section 9 and threats to validity in Section 10.\nFinally, we conclude in Section 11.\n2 Background and Related Work\nThis section introduces background and related work on automated code trans-\nlation (Section 2.1), benchmark evaluation (Section 2.2), and prompt engineering\n(Section 2.3).\n2.1 Automated Code Translation\nAutomated code translation is a long-existing task in software engineering (SE).\nExisting approaches can be categorized into four classes, rule-based (e.g., C2Rust4,\nstatistical learning-based (Nguyen et al., 2013, 2014; Karaivanov et al., 2014), neu-\nral network-based (Chen et al., 2018), and pre-trained model-based (Lachaux et al.,\n2021; Roziere et al., 2021; Szafraniec et al., 2023; Jiao et al., 2023; Pan et al., 2024;\nYang et al., 2024). The first category encompasses tools that utilize program anal-\nysis techniques and handcrafted rules to translate code from one PL to another.\nFor instance, C2Rust and cxgo5 are designed to convert C programs into Rust and\nGo, respectively. Sharpen6 and Java2CSharp7 are tools for transforming Java code\ninto C#. While these tools are employed in the industry, their development can\nbe costly. Additionally, they often result in translations that may not align with\nthe idiomatic usage of the target language, leading to challenges in readability for\nprogrammers (Szafraniec et al., 2023). As representatives for the second category,\nNguyen et al. 2013; 2014, Karaivanov et al. 2014 and Aggarwal et al. 2015 investi-\ngated how well statistical machine translation models for natural languages could\napply to code translation. These methods overlook the grammatical structures\ninherent in PLs. To mitigate this issue, Chen et al. 2018 proposed a tree-to-tree\nneural network that translates a source tree into a target tree by using an atten-\ntion mechanism to guide the expansion of the decoder. Their approach outperforms\nother neural translation models designed for translating human languages.\n3 https://github.com/RISElabQueens/forge24-code-translation\n4 https://github.com/immunant/c2rust\n5 https://github.com/gotranspile/cxgo\n6 https://github.com/mono/sharpen\n7 https://github.com/bdqnghi/j2cstranslator\nOutput Format Biases in the Evaluation of LLMs for Code Translation 7\nPre-trained model-based approaches leverage self-supervised learning to train a\nmodel from large-scale source code for code translation. Roziere et al. 2020 in-\ntroduced an unsupervised code translation model that only requires source code\nin multiple PLs without parallel code translation pairs, which are often hard\nto collect. They developed TransCoder, a model pre-trained on GitHub’s open-\nsource projects leveraging three tasks, i.e., masked language modeling, recover-\ning corrected code, and back-translation. Building on this, they later introduced\nDOBF (Lachaux et al., 2021) and TransCoder-ST (Roziere et al., 2021). DOBF is a\nmodel pre-trained to reverse code obfuscation by employing a sequence-to-sequence\nlearning model. TransCoder-ST, on the other hand, utilizes automatic test genera-\ntion techniques to automatically select and fine-tune high-quality translation pairs\nfor the pre-trained model. Szafraniec et al. proposed TransCoder-IR (Szafraniec\net al., 2023), enhancing TransCoder by incorporating low-level compiler interme-\ndiate representations. In addition to models specifically pre-trained for code trans-\nlation, general-purpose pre-trained models for source code, such as CodeT5 (Wang\net al., 2021) and CodeBERT (Feng et al., 2020), have also shown promise in code\ntranslation tasks. These models can be effectively fine-tuned on parallel code trans-\nlation pairs, as demonstrated in the work of Jiao et al. 2023.\nRecently, LLMs have gained prominence as highly effective tools for code intelli-\ngence tasks, including code translation (Fan et al., 2023; Yang et al., 2024). Dif-\nferent from previous pre-trained model-based approaches, LLMs are designed to\nunderstand and follow human instructions, enabling their straightforward applica-\ntion in various downstream tasks through prompt-based interactions rather than\nexpensive task-specific fine-tuning or pre-training. Pan et al. 2024 investigate the\neffectiveness of LLMs in code translation. The authors collected executable code\nsamples from various datasets and projects and performed translations using pop-\nular LLMs including GPT-4 (Achiam et al., 2023), Llama 2 2024, Wizard Vicuna\n13B 2024, Airoboros 13B 2024, StarCoder (Li et al., 2023a), CodeGeeX (Zheng\net al., 2023), and CodeGen (Nijkamp et al., 2022). They reported that LLMs were\nlargely ineffective in translating real-world projects, highlighting the need for im-\nprovement in code translation techniques. A more recent study by Yang et al. 2024\nreported similar findings on the errors generated by LLM-based code translation\napproaches. They propose UniTrans, a method that generates test cases for the\ntarget program and guides LLMs to iteratively repair incorrectly translated code\nbased on the test case execution results.\n2.2 Code Translation Benchmarks and Evaluation Metrics\nLu et al. 2021 unveiled CodeXGLUE, a benchmark dataset designed to assess ma-\nchine learning models across ten different program understanding and generation\ntasks. This dataset, compiled from a variety of open-source projects, includes a\nspecialized code-to-code translation dataset featuring 11,800 Java to C# transla-\ntion pairs. In a similar vein, Puri et al. 2021 introduced CodeNet, a diverse dataset\nthat covers 55 programming languages. CodeNet primarily focuses on code trans-\n8 Marcos Macedo et al.\nlation tasks, with its data sourced from two prominent online judge platforms:\nAIZU8 and AtCoder9.\nZhu et al. 2022b contributed to this growing field with CoST, a parallel code corpus\nthat encompasses seven languages and facilitates snippet-level alignment through\ncode comment matching. CoST is versatile, supporting a range of tasks, including\ncode translation, summarization, and synthesis. Building upon CoST, Zhu et al.\nfurther introduced XLCoST (Zhu et al., 2022a), a dataset offering alignment at\nboth snippet and program levels. Both CoST and XLCoST were curated from\nGeeksForGeeks10, a resource-rich website featuring a plethora of data structures\nand algorithm problems, along with solutions in up to seven popular programming\nlanguages.\nMore recently, Jiao et al. 2023 developed G-TransEval, a benchmark that inte-\ngrates various existing benchmarks into a unique benchmark by categorizing code\ntranslation pairs into four types: syntax level, library level, semantic level, and\nalgorithm level translations.\nWhen utilizing the benchmarks mentioned above, the evaluation of code transla-\ntion approaches typically relies on two types of metrics: text-oriented and execution-\nbased. Text-oriented metrics, such as BLEU (Papineni et al., 2001), CodeBLEU (Ren\net al., 2020), and CrystalBLEU (Eghbali and Pradel, 2022), measure token over-\nlap or semantic similarity between the generated code and the reference. However,\nthese metrics can be skewed by inconsistent output formats that include non-code\nelements like explanations. Execution-based metrics, such as Computational Ac-\ncuracy (CA) (Roziere et al., 2021), assess whether the generated code compiles\nand runs correctly. Inconsistent formats, where non-code elements cause compila-\ntion failures, can lead to inaccurate conclusions about model performance, even\nwhen the code itself is valid. Note that text-oriented metrics require a parallel\ndataset (i.e., paired translation tasks with human-written reference translations),\nwhich is not always provided in the benchmarks mentioned above and may require\nadditional steps to prepare.\n2.3 Prompt Engineering in SE\nThe remarkable ability of LLMs for zero-shot and few-shot prompting has gained\ngrowing interest in exploring how in-context learning and prompt engineering can\nimprove LLM-based applications within SE. Researchers have leveraged LLMs and\nprompt engineering for various ASE tasks (Feng and Chen, 2023; Pan et al., 2024;\nGao et al., 2023; Li et al., 2023b; Geng et al., 2024).\nGao et al. 2023 conducted a study to understand the impact of the choice, order,\nand number of demonstration examples on the effectiveness of in-context learn-\ning in code intelligence tasks. Pan et al. 2024 introduced an iterative prompting\nmethod that integrates additional contextual information to improve LLM-based\ncode translation. This method enriches the input to language models with compre-\nhensive context, including the original code, prior prompts, erroneous translations,\n8 https://onlinejudge.u-aizu.ac.jp/\n9 https://atcoder.jp/\n10 https://www.geeksforgeeks.org/\nOutput Format Biases in the Evaluation of LLMs for Code Translation 9\ndetailed error information, translation instructions, and expected outcomes. Yang\net al. 2024 follows a similar iterative approach with additional test cases gener-\nated. Li et al. 2023b explored ChatGPT’s capability in identifying test cases that\nreveal bugs in source code. Initially, ChatGPT showed limited success, but with\ncareful prompting, its performance improved significantly. Feng et al. 2023 uti-\nlized prompts that exploit few-shot learning and chain-of-thought reasoning for\nLLM-based bug reproduction. Geng et al. 2024 delved into the efficacy of LLMs\nin generating code comments with various intents, focusing on their attributes.\nThey employed the in-context learning approach and designed new strategies for\nselecting and re-ranking examples to optimize performance.\n3 Study Setup\nThis section discusses the selection of LLMs for our study (Section 3.1), the data\ncollection procedure (Section 3.2), and the utilized prompt templates (Section 3.3).\n3.1 Selected Large Language Models\nA diverse range of models exists in the rapidly evolving field of LLMs for code\ngeneration, offering a variety of capabilities and performance. In this case study,\nwe select 16 targets LLMs, of which 11 are open-sourced, while the remaining\nfive are closed. For all models, we select instruct-tuned LLMs to the detriment\nof their associated base models, as instruct-tuned models are fine-tuned to follow\nprompted instructions more effectively. Table 1 summarizes the basic information\nof selected LLMs.\nTable 1Summary of LLMs used in the current study.\nSource Type Model Name Size (Billions) Context Length (Tokens) Release Date\nOpen-source\nCodeLlama Instruct 7 16,384 Aug, 2023\nCodeLlama Instruct 13 16,384 Aug, 2023\nCodeLlama Instruct 34 16,384 Aug, 2023\nMagicoder-S-CL 7 16,384 Dec, 2023\nMagicoder-CL 7 16,384 Dec, 2023\nWizardCoder 1 8,192 Aug, 2023\nWizardCoder 3 8,192 Aug, 2023\nWizardCoder Python 7 8,192 Aug, 2023\nWizardCoder Python 13 8,192 Aug, 2023\nWizardCoder Python 34 8,192 Aug, 2023\nMixtral 8x7B Instruct v0.1 46.7 8,192 Dec, 2023\nClosed\nGPT-3.5 Turbo Unspecified 16,384 Nov, 2023\nGPT-4 Unspecified 8,192 Mar, 2023\nGPT-4o Unspecified 128,000 May, 2024\nGPT-4o mini Unspecified 128,000 Jul, 2024\nGemini 1.5 Flash Unspecified 1,000,000 Feb, 2024\nThese models can be grouped across six distinct model families:\n–(Open-source) Magicoder (Wei et al., 2023): This collection of LLMs, trained\non 75K synthetic instruction data using the OSS-Instruct framework, includes\ntwo variants: Magicoder-CL and Magicoder-S-CL, optimized for code generation\nand understanding.\n10 Marcos Macedo et al.\n–(Open-source) WizardCoder (Luo et al., 2023): Built on the StarCoder frame-\nwork (Li et al., 2023a), WizardCoder specializes in generating complex code\ninstruction data. It comes in various sizes, ranging from 1 billion to 34 billion\nparameters, and is known for its strong performance on intricate coding tasks.\n–(Open-source) CodeLlama (Rozi` ere et al., 2023): A family of models derived\nfrom Llama 2, designed specifically for code generation tasks. CodeLlama models\nare available in three sizes: 7B, 13B, and 34B parameters, providing flexibility\nfor different levels of computational resources and code complexity.\n–(Open-source) Mixtral 2023: A Mixture-of-Experts model developed by Mistral\nAI, known for its robust performance in code generation.Note: While Mixtral\nhas both open and closed variants, we specifically use the open-source version in\nthis study.\n–(Closed) OpenAI’s GPT 11: A family of models developed by OpenAI, specifi-\ncally GPT-3.5 Turbo, GPT-4, GPT-4o, and GPT-4o Mini. These models differ\nin their size, context lengths, and use cases. GPT-3.5 Turbo is a lightweight\nmodel offering fast responses with moderate capabilities, while GPT-4 and GPT-\n4o provide superior reasoning, handling more complex tasks. GPT-4o Mini, a\ncost-efficient version of GPT-4o, is designed for large-scale and time-sensitive\napplications.\n–(Closed) Gemini 12: Specifically, Gemini 1.5 Flash. It is the most recent LLM\nreleased by Google with a massive 1 million token context window.\nWhen selecting open-source LLMs, we consider two key factors, i.e., they demon-\nstrated strong performance in code generation tasks, and they align well with\nefficient deployment frameworks, such as vLLM (Kwon et al., 2023), without pre-\ncluding their compatibility with other platforms like the HuggingFace Text Gen-\neration Interface (Wolf et al., 2020). This consideration ensures that our chosen\nmodels are not only high-performing but also practically feasible for widespread\nuse in research and industry applications.\nFor closed LLMs, their selection is motivated by their commercial relevance and\nstate-of-the-art performance. Models like GPT-4 and Gemini 1.5 Flash are lead-\ning commercial solutions that demonstrate exceptional capabilities in handling\ncomplex code generation tasks at scale. Their widespread use in industry high-\nlights their robustness, while their closed nature allows them to integrate advanced\nproprietary optimizations that open models may not yet achieve. Studying these\nmodels enables us to benchmark against the very best available technologies and\nunderstand their advantages in practical applications. We also include older mod-\nels, such as GPT-3.5 Turbo, for comparison purposes, as their preferences in output\nformat may differ from those of more recent models, potentially providing valuable\ninsights into the evolution of format generation across different versions.\n11 https://platform.openai.com/docs/models\n12 https://ai.google.dev/gemini-api/docs/models/gemini\nOutput Format Biases in the Evaluation of LLMs for Code Translation 11\n3.2 Dataset and Preprocessing\nPreparing Data for RQ1, RQ3, and RQ5:We utilize programs from the\ndataset provided by Pan et al. 2024 that are derived from the CodeNet dataset.\nThis dataset contains 1,000 programs in five PLs (C, C++, Go, Java, and Python),\nwith 200 programs and test cases for each programming language.\nThere are 20 possible translation combinations of source-target program language\npairs considering the five PLs found in the dataset. Therefore, we create 4,000\ntranslation samples that consist of programs in the source programming language\nand the 4 desired output languages (e.g., one program in C can be translated to\nC++, Go, Java, and Python).\nWe use the tokenizer of each of the eleven models to tokenize the input code\n(program) from each of the code pairs. We can observe in Fig. 2 that 97.9% of\nthe programs have a source code token length of up to 3,072 tokens. Therefore,\nwe utilize this number as the cut-off for code pairs, filtering out from our dataset\ncode pairs that have an input program token length greater than 3,072 tokens. In\naddition, we allow models to generate up to 2,048 new tokens during inference.\nDoing so ensures that we can fit the input code, prompt, and output into the\ncontext window of all models.\nFig. 2Distribution of program length in Pan et al.’s dataset before the cut-off.\nAfter excluding input code exceeding 3,072 tokens in length, our dataset was re-\nduced to 3,912 translation samples. However, the filtering process resulted in an\nimbalanced dataset due to varying numbers of code pairs for each source-target\n12 Marcos Macedo et al.\nlanguage combination. As a result, the number of code translation samples across\nsome PL pairs is greater than that of other PL pairs. We then balance the number\nof code pairs in each source-target language combination by down-sampling those\nwith a higher frequency. In the end, we have a dataset of 3,820 code pairs, with 191\ncode pairs and test cases for each of the 20 source-target language combinations.\nFig. 3 showcases the distribution of the final dataset.\nPreparing Data for RQ4:The dataset from Pan et al. lacks human-written\nground truth translations for each sample across PLs (i.e., parallel data), which is\nessential for computing text-oriented metrics like CodeBLEU and BLEU required\nto address RQ4. This absence poses a challenge in evaluating translation quality.\nTo overcome this, we searched the CodeNet repository for corresponding source\nprograms in our filtered dataset, consisting of 3,820 translation pairs. Once we\nidentified the problem solved by the source program in CodeNet, we selected an\naccepted solution in the target language to serve as the ground truth for compar-\nison.\nSince not all problems have accepted solutions available in every target language,\nwe excluded these cases from the dataset to maintain the consistency and relevance\nof our analysis. After this refinement, our final dataset retained 93.17% of the\noriginal filtered samples, resulting in 3,657 translation pairs for answering RQ4.\nFig. 3Distribution of the token lengths of programs in our dataset.\nOutput Format Biases in the Evaluation of LLMs for Code Translation 13\n3.3 Prompt Templates\nWe use three prompt templates in our case study, as seen in Fig. 4. The first\ntemplate is proposed by Pan et al. 2024 for evaluating open-source LLMs. We refer\nto it as “Reference Prompt”. The “Vanilla Prompt” prompt template, created by\nus, is tailored for each model based on the model’s recommended template from\nits respective paper or HuggingFace model card (e.g., WizardCoder 13).\nAdditionally, for the instruction in our prompt template, we follow OpenAI’s sug-\ngested prompting strategy, i.e., “Ask the model to adopt a persona” 14. This strat-\negy is also adopted in recent instruct-tuned code LLMs, such as Magicoder (Wei\net al., 2023).\nLastly,Controlled Promptis a variation ofVanilla Promptdesigned to control the\noutput format of the models. The detailed design concerns for this prompt can be\nfound in Section 5.\nFig. 4Examples of the Prompt Templates used in WizardCoder. The same instruction is\nused for all the LLMs, following the recommended prompt template provided by the authors\nof each model, as outlined in their respective model cards. The exception is Reference Prompt,\nwhich is used as-is.\n3.4 Evaluation Metrics\nIn this study, we consider both widely used execution-based evaluation metrics\nand text-oriented metrics. Specifically, we considered the following four evaluation-\nbased metrics when comparing selected LLMs.\n– Computational Accuracy (CA):Rozi` ere et al. 2020 introduced the CA\nmetric, which evaluates whether a transformed target function generates the\nsame outputs as the source function when given the same inputs. The target\n13 https://huggingface.co/WizardLMTeam/WizardCoder-15B-V1.0\n14 https://platform.openai.com/docs/guides/prompt-engineering/strategy-give-mod\nels-time-to-think\n14 Marcos Macedo et al.\nfunction is correct if it gives the same output as the source function for every\ntested input value. In our case, the programs in the dataset are functions that\nreceive input fromstdinand produce an output.\n– Compilation Rate (CR): The number of programs that successfully com-\npiled over the total number of programs in the datasets.\n– Match Success Rate (MSR): The number of generated outputs where the\nregular expression designed to extract source code successfully finds a match\nin the output, divided by the total number of generated outputs.\n– Code Extraction Success Rate (CSR): The number of generated outputs\nfor which we are able to extract source code, divided by the total number of\ngenerated outputs.\nMSR and CSR are new evaluation metrics that we developed based on our pro-\nposed method for controlling the output format, which are explained in Section 5.\nNote that, “match” is a pre-condition to “extract” the source code from the out-\nput.\nFor text-oriented metrics, we consider BLEU and CodeBLEU:\n– BLEU: The BLEU metric, originally introduced by Papineni et al. (2001),\nmeasures the overlap between the n-grams of the machine-generated translation\nand the reference (human-written) translation. It is computed as a geometric\nmean of n-gram precision, combined with a brevity penalty to penalize overly\nshort translations. In the context of code translation, BLEU evaluates how\nclosely the generated code matches the reference code at a token level, where\ntokens are typically keywords, operators, or identifiers in the code.\n– CodeBLEU: Ren et al. (2020) proposed CodeBLEU as an enhancement of\nBLEU specifically designed for code generation tasks. In addition to n-gram\nprecision, CodeBLEU incorporates structural matching (comparing code ab-\nstract syntax trees), data flow, and keyword matching to better account for the\nunique features of programming languages. It combines the traditional BLEU\nscore with these code-specific properties, yielding a more accurate reflection of\ncode quality and functional correctness.\n– CrystalBLEU: Proposed by Eghbali et al. (2022), this metric builds on the\nstrengths of BLEU while addressing its susceptibility to noise from trivially\nshared n-grams. These are common n-grams that frequently appear across code\nin the same language but do not indicate meaningful similarity. By filtering\nout these trivial overlaps, CrystalBLEU enhancesdistinguishability, a prop-\nerty defined by the authors to quantify how much more similar semantically\nequivalent code examples are compared to non-equivalent ones.\n3.5 Implementation Details\nWe begin by downloading the official checkpoints for all evaluated models from\nHugging Face. Subsequently, the vLLM framework is employed to conduct in-\nference using Greedy Decoding on four Nvidia RTX 6000 GPUs. Using Greedy\nDecoding guarantees the reproducibility of the inference outputs across multiple\nOutput Format Biases in the Evaluation of LLMs for Code Translation 15\nruns. For models ranging from 1 to 7B in size, a single GPU is utilized. Models\nwith 13B employ Tensor Parallelism across two GPUs, while those exceeding 13B\nleverage the same technique across all four GPUs. In the case of closed source\nmodels, we use their inference API with greedy decoding. Regarding the execu-\ntion and compilation of programs, our environment is equipped with the following\nversions of compilers and run-times: OpenJDK 11, Python 3.11, g++ 12, gcc 12,\nand Go 1.19.\n4 RQ1:What are the characteristics of the output formats across LLMs\nand prompts?\nExamining the output formats generated by the bench-marked models is a crucial\naspect of validating our hypothesis. Additionally, this evaluation sheds light on\nthe prevalent output formats across various models and inspires the development\nof approaches to control the output format.\n4.1 Approach\nOur initial step involved creating a representative sample of our dataset to examine\nthe distribution and variety of output formats produced by these models. This\nwas achieved through stratified sampling across source-target PL combinations\nwithin the dataset. As a result, we acquired a subset of 360 code translation pairs,\nspanning 20 source-target language pairs, which accurately represent our dataset.\nFor each of the 11 open-source models, we utilized both the Reference and Vanilla\nPrompt templates (as illustrated in Fig. 4) to generate inference results. With each\nmodel producing 360 inference outcomes, the total number of inferences across all\nmodels amounted to 3,960 for each prompt.\nTo identify and summarize patterns within these outputs, we conducted open-\ncoding as guided by the method described by Stol et al. (Stol et al., 2016). We\nfurther extracted a representative subset of these output formats for each prompt\nto ensure a comprehensive analysis. This involved stratified sampling across the\nlanguage pairs and LLMs, leading to a subset of 440 entries that reflects the\nbroader set of 3,960 results.\nFinally, we estimated the proportions of these identified patterns within the in-\nference results generated from our dataset. This process not only helps in under-\nstanding the behavior of different LLMs in response to varying prompts, but also\ncontributes to the broader understanding of LLM output formats in the context\nof code translations.\n4.2 Results\nWe analyze the output formats generated by models in response to two different\nprompt templates, i.e., the Reference and Vanilla Prompt templates. The models\nproduce outputs in three primary formats in terms of the source code:\n16 Marcos Macedo et al.\n1.Direct Output: This format consists solely of source code without any special\nformatting.\n2.Wrapped Code: Here, the source code is enclosed within a code block, delin-\neated by triple back-ticks.\n3.Unbalanced Back-ticks: In this format, the source code is followed by a\nclosing triple back-tick, but lacks an opening one.\nFor each of these output formats, we further categorize the results based on the\npresence or absence of natural language elements, such as notes, comments, or\nexplanations, accompanying the source code:\n– No additional text: The inference results contain only source code, with no\nadditional natural language content.\n– Additional text: Natural language is present in the inference results. This may\nappear before, after, or interspersed within the code.\nFig. 1 illustrates three sample outputs from our dataset, along with their corre-\nsponding sub-categories.\nFig. 5Distribution of output formats observed for each prompt in RQ1. The height of the\nbar represents the observed proportion of output formats over the sampled generation outputs\nfor the prompt.\nFinding 1. The models’ output format is not consistent across our\ndataset, and post-processing is required to extract the source code in\nup to 73.64% of the outputs.Among the six combinations (three source code\nformats, with or withoutAdditional text), only when the models generate theDi-\nrect Outputformat withoutAdditional text, the inference output can be directly\nemployed for evaluation. This is because it solely consists of source code and thus\nOutput Format Biases in the Evaluation of LLMs for Code Translation 17\nnecessitates no further post-processing. As shown in Fig. 5, in the case of our\nVanilla Prompt, only 26.36% (43.86%×60.10%) of the generated outputs contain\nsolely source code and are immediately usable (over the total 43.86% that are\nDirect Output, we observe that 60.10% have no additional text, as seen in Fig. 5).\nThe remaining 73.64% (100% - 26.36%) of source code requires post-processing\nand can not be directly extracted due to added natural text such as explanations,\ncomments, and notes, or being in a different output format than what we expect\n(Wrapped Code, Unbalanced Back-Ticks).\nConversely, for our Reference Prompt, we estimate that up to 59.5% (89.55%×\n66.50%) of the outputs are suitable for direct evaluation without post-processing.\nThis percentage represents the outputs inDirect Code Outputformat that do not\ncontain additional text. However, the remaining 40.5% of the outputs will cause\ncompilation (or interpretation) errors due to added comments or being in other\noutput formats than the assumed one.\nFinding 2. Different prompts produce different output format distribu-\ntionsAs shown in Fig. 5, the proportions of the three categories of output formats\nvary based on the design of the prompt. In total, 89.55% of the outputs by the\nReference Prompt have a Direct Code format. On the other hand, the Vanilla\nPrompt outputs mostlyWrapped Codeformat (in 52.50% of the inferences). This\nresult shows that not only do the models generate different amounts ofAdditional\nTextalongside source code in the Direct Output category (as observed in Finding\n1), but also the predominant category of output format varies across prompts.\nFinding 3. During inference, models disregard the instruction tooutput\ncode only59.3% of the time.Vanilla Promptinstruct the models to ”Output\ncode only” as seen in Fig. 4. We find that in 59.32% of the cases across all the\noutput formats, the model added comments alongside the source code (as derived\nfrom Fig. 5). Surprisingly, this number is higher than theReference Promptthat\ndoes not instruct the model to output only code. In the latter, only 32.73% of\nthe generated output across all output formats have additional text alongside the\nsource code.\nRQ1 Summary:We observe that the models generate outputs in varying formats\nand distributions, depending on the prompt used. In extensive studies compar-\ning different models or prompts, this variation in output formats presents a\nchallenge in understanding the distribution of possible output formats and ac-\ncurately capturing code. This variation significantly affects the reliability of the\nmetrics derived from the models’ generated output.\n5 RQ2:To what extent can the output format of LLMs be controlled\nusing prompt engineering and lightweight post-processing?\nIn RQ1 (Section 4), we confirm that LLMs can generate outputs with different\nformats. Thus, we aim to explore the extent to which the output format of LLMs\ncan be controlled (i.e., shifting the output distribution to one preferred type) so\nthat we can extract code automatically in a unified way.\n18 Marcos Macedo et al.\n5.1 Approach\nOur approach to control the models’ output format uses a combination of prompt\nengineering and regular expression (regex) parsing.\nOur prompt engineering method attempts to improve the consistency of the output\nformat across all the models, such that the generated code in the output can be\ndirectly extracted for the highest number of programs possible without capturing\ncomments that would interfere with the compilation or interpretation of the code.\nFor prompt engineering, we modify theVanilla Prompt(Section 3.3) by adding\na control statement that instructs the model to generate the code excerpt within\ntriple back-ticks (Wrapped Code format). Fig. 4 (c) shows the derivedControlled\nPrompt.\nWe use the Regex in Fig. 6 to find and extract source code from the generated\noutputs. The Regex is designed to match theWrapped Codeformat (i.e., the text\ninside the first block delimited by three back-ticks on the output). The regex also\nignores, if present, the first line in the output that often denotes the programming\nlanguage the code is written into as it can be seen in Fig. 1.\nResponse :?.*? ‘ ‘ ‘(?:(?: java | cpp | csharp | python |c|go|C \\+\\+| Java | Python |\nC#|C|Go)) ?(.+) ‘‘‘\nFig. 6Regex designed to match and extract source code from code blocks (Wrapped Code\nformat) in the generated output text.\nWe obtain a subset of 440 samples through stratified sampling across the PL pairs\nand LLMs, following the same methodology described in RQ1 (Section 4.1) and\nestimating the proportion of each of the output formats. We estimate our Regex’s\naccuracy (Match Success Rate (MSR)) on this subset of 440 samples. For the cases\nwhere there was a match, we analyze the percentage of matched source code. We\nalso manually analyze the cases with no match and explain the reasons for such\nmismatches.\n5.2 Results\nFig. 7 shows the distribution of output format types between theVanilla Prompt\nand theControlled Prompt.\nFinding 4. The output format can be controlled through a combina-\ntion of prompt engineering and lightweight post-processing, achieving\na MSR of 95.45% and a CSR of 92.73%.Our proposed prompt is capable\nof steering the output across models, and our proposed Regex (Fig. 6) matches\n95.45% of the generated outputs. As the matched content could be source code\nwith added text, we manually examine the matches to confirm what proportion\nof the matches are source code only. We observe that out of the 95.45% there is\n2.73% of matches that are not source code. In other words, we achieve a CSR of\n92.73%.\nOutput Format Biases in the Evaluation of LLMs for Code Translation 19\nFig. 7Comparison of the distribution of output formats of the Controlled Prompt (left side)\nvs Vanilla Prompt (right side).\nNext, we perform a detailed analysis on the 2.73% of matches that do not contain\nsource code. Through this analysis, we discovered that the content matched com-\nprised natural language text rather than source code. More interestingly, 100% of\nthose cases are caused by the Mixtral 8x7B model. Specifically, we observe the\nemergence of a new output format we nameRe-Wraps Codegenerated by the\nMixtral 8x7B model, as seen in Fig. 1 (d). In theRe-Wraps Codeformat, the\nmodel does not complete the code block opened using back-ticks in theControlled\nPrompt, ignoring our output control mechanism. Instead, the model opens a new\nset of back-ticks, adds source code and finishes by closing the back-tick group.\nBecause our Regex is designed to captureWrapped Code, when it is applied to\nthe Re-Wrapped Code format it matches from the beginning of the back-ticks we\nintroduced in theControlled Promptup to the first set of back-ticks generated by\nthe model, not capturing the source code.\nRQ2 Summary:The output format can be controlled to increase the percentage\nof output that can be matched by the Regex from 52.50% in the Vanilla Prompt\nto 95.45% in the Controlled Prompt. Controlling the output significantly helps\ncapture more source code.\n6 RQ3:What’s the impact of output control on the reported\nperformance of LLMs in terms of CA?\nIn RQ2, we show the feasibility of controlling output formats of LLMs for code\ntranslation via a combination of prompt engineering and lightweight post-processing\nregex. As the MSR changes, we hypothesize that commonly considered evaluation\n20 Marcos Macedo et al.\nmetrics for code translation (i.e., Compilation Rate (CR) and CA) should also\nchange. Thus, in this RQ we empirically quantify what is the impact of our out-\nput control on the reported performances of LLMs on the whole 3,820 translation\npairs.\n6.1 Approach\nTo answer this question, we leverage theVanilla PromptandControlled Prompt\nthat are shown in Fig. 4. Using each of the prompts, we perform translation on all\nthe 3,820 code pairs in our dataset. We employ various combinations of prompts\nand source code extraction methods to explore their effects on the results reported\nby three execution-based metrics. The evaluated combinations are:\n1.Vanilla + Direct Evaluation (VDE):We utilize the Vanilla Prompt and\ndirectly evaluate the generated output. We do not perform any regex matching\nor source code extraction.\n2.Vanilla + Regex (VRE):We utilize the Vanilla Prompt and apply our regex\nto extract source code.\n3.Controlled + Regex (CRE):We utilize the Controlled Prompt and apply\nour regex to extract source code.\n6.2 Results\nTable 2 shows the MSR, CR, and CA for the eleven considered models using three\nconsidered scenarios (VDE, VRE, and CRE) on the 3,820 code translation pairs.\nTable 2Results for different combinations of prompt and extraction method across eleven\nLLMs. VDE refers to (Vanilla + Direct Evaluation), VRE refers to (Vanilla + Regex), and\nCRE refers to (Controlled + Regex) combinations.\nMatch Success Rate (MSR) Compilation Rate (CR) Average CA\nModel Name Model Size CRE VRE VDE CRE VRE VDE CRE VRE VDE\nCodeLlama Instruct 7 94.2% 6.60% – 50.42% 2.77%30.03%33.27% 1.83% 18.30%CodeLlama Instruct 13 96.9% 0.68% – 57.70% 0.05% 27.20% 38.12% 0%18.51%CodeLlama Instruct 34 98.1% 40.16% – 49.29% 22.80% 7.93% 34.06% 13.17% 5.18%Magicoder-CL 7 88.5% 67.98% – 56.47% 47.20% 0% 37.33% 32.02% 0%Magicoder-S-CL 7 99.0%99.35%–68.09% 68.80%0% 43.30%43.12%0%Mixtral 8x7B 46.7 60.4% 82.20% – 0% 53.51% 2.17% 0% 35.60% 1.28%WizardCoder 1 96.8% 72.91% – 47.39% 48.72% 0.13% 18.61% 17.25% 0.03%WizardCoder 3 96.9% 68.09% – 59.45% 53.74% 3.90% 32.28% 29.92% 0.55%WizardCoder Python 7 98.3% 40.65% – 55.29% 29.35% 15.79% 34.11% 15.79% 10.13%WizardCoder Python 13 98.9% 71.28% – 62.93% 47.80% 0.31% 36.20% 27.64% 0.1%WizardCoder Python 3499.4%28.48% – 66.49% 18.38% 0%43.85%15.42% 0%\nAverage – 93.40% 52.58% – 52.19% 35.74% 7.95% 31.92% 20.98% 4.92%\nFinding 5: The integration of prompt engineering with a lightweight\npost-processing technique (CRE) demonstrated superior MSR, CR, and\nCA among the tested combinations.The CRE approach achieved an impres-\nsive average MSR of 93.40%, coupled with an average CR of 52.19% and an average\nCA of 35.74% over the models.\nOutput Format Biases in the Evaluation of LLMs for Code Translation 21\nThe MSR and CR values for VDE and VRE, as detailed in Table 2, corroborate\nthe insights gained from our manual analysis in RQ2. These findings highlight the\nefficacy of our output control method in enhancing the likelihood of extracting\npotentially compilable source code from inference output, thereby boosting the\nCR. In contrast, neglecting the output format and attempting direct compilation\nof inference outputs (VDE) achieves the lowest average CR at just 7.95%. This\nstark difference underscores the critical role of the output format and control when\nreporting and interpreting the performance of LLMs for code translation.\nThe three combinations (CRE, VDE, VRE) share similar patterns in terms of CA\nwith the other two metrics (MSR, CR). The lowest CA value is 4.92% for VDE.\nThe second highest CA is for VRE, with 35.74%. Even though the Vanilla Prompt\nis not designed to increase the chances of outputting a specific format, it shows\nhigher CR and CA than VDE.\nAdditionally, it can be observed that Magicoder models and WizzardCoder 34B\nhad a CR of 0% in VDE, with no generation output that can be compiled. To\nfurther examine the possible causes of this phenomenon, we randomly sampled 350\ngenerated outputs for each of WizzardCoder 34B, Magicoder-CL, and Magicoder-\nS-CL models. Our inspection confirms that all the 350 samples for each of the\nmodels containAdditional textor an output format different thanDirect Code,\nwhich explains why none of the generated outputs are compilable.\nLastly, we can observe that the CR of Mixtral 8x7B decreased from 53.51% in VRE\nto 0% on CRE, which is the opposite of what we would expect in CRE. Manual\ninspection confirms that this is because 100% of the 350 generated out samples are\nin another format than the one expected by our regex, such asRe-Wraps Codeor\nothers. This showcases the importance of carefully inspecting the output format\nof each model when comparing them in a benchmark.\nFinding 6: The consideration of output formats can significantly alter\nthe outcomes when benchmarking various LLMs.In the context of CRE,\nour analysis reveals a noteworthy trend: WizzardCoder Python 34B emerges as\nthe top-performing model, boasting an average CA of 43.85% across all languages.\nThis finding is particularly striking when juxtaposed with the performance of\nMagicoder-S-CL, a relatively smaller model with 7B parameters, which secures\nthe second-highest CA at 43.30% across all models. This suggests that model size\nmight not be the sole determinant of effectiveness in the code translation task,\nsame as reported in prior work (Wei et al., 2023). Conversely, in the VDE sce-\nnario, where output control is ignored, a different pattern is observed. Here, both\nthe CodeLlama Instruct 7B and 13B models demonstrate superior CA compared\nto others. This variance in the measured performance underlines the significant im-\npact that the output format and control can have when reporting the capabilities\nof different LLMs.\n22 Marcos Macedo et al.\nRQ3 Summary:The characteristics of the output format and its control signif-\nicantly influence the CA metric. The lowest average CA, at 4.92%, is achieved\nwhen the generated output from the LLMs is compiled directly (VDE). The\nhighest average CA across all models, at 31.92%, is obtained using the CRE ap-\nproach. We conclude that careful inspection and source code extraction based on\nthe output format of the models is necessary to achieve comparable performance\nresults.\n7 RQ4:Does inconsistent output format introduce bias in text-oriented\nmetric-based evaluation?\nIn RQ3, we demonstrate how the output format and the inclusion of natural\nlanguage can significantly affect the reported performance of LLMs when using\nexecution-based metrics, e.g., CA. Text-oriented metrics, like BLEU and Code-\nBLEU for code translation, compare the generated source code against a reference\ncommonly referred to as the ground truth or golden answer. Unlike CA, which\noffers a binary evaluation (pass or fail for each example), text-oriented metrics\nprovide a continuous measure, capturing the degree of similarity or preference\nfor each generated translation output. Given the substantial differences between\nthese two types of evaluation metrics, this RQ extends our analysis by examining\nthe impact of directly evaluating the LLM’s inference output using these metrics,\nas opposed to extracting and isolating the source code to remove extraneous text,\nwhich we address through our proposed lightweight approach. We hypothesize that\nadditional natural language alongside the source code would decrease the metrics’\nvalue, as they would be more different than the ground truth. For consistency in\ncomparing execution-based metrics, as in RQ3, we concentrate on evaluating the\noutputs of open-source LLMs.\n7.1 Approach\nTo address RQ4, we use the extended dataset we derived from Pan et al. 2024(as\ndetailed in Section 3.2), along with the inference outputs from 11 open-source\nLLMs generated in RQ3. Our objective is to investigate the impact of the output\nformat on the CodeBLEU, BLEU and CrystalBLEU scores. For the outputs gen-\nerated by each prompt in our study (Vanilla and Controlled Prompt), we evaluate\nif removing non-source-code tokens such as natural language text using our Regex\n(i.e., CRE and VRE setting) leads to an increase in the metrics in comparison\nto directly evaluating the inference output, which contains natural language (i.e.,\nCDE and VDE setting).\nThe combinations of prompt and output processing methods considered in this\nRQ are similar to those in RQ3, with the addition of a new case, CDE:\n1.Vanilla + Direct Evaluation (VDE):We utilize the Vanilla Prompt and\ndirectly evaluate the generated output. We do not perform any regex matching\nor source code extraction.\n2.Vanilla + Regex (VRE):We utilize the Vanilla Prompt and apply our regex\nto extract source code.\nOutput Format Biases in the Evaluation of LLMs for Code Translation 23\nTable 3Evaluation of CodeBLEU, BLEU, and CrystalBLEU Scores along with Cliff’s Delta\nand Significance for Vanilla Prompt Experiments.\nModel CodeBLEU BLEU CrystalBLEUVREVDESignificanceCliff’s DeltaVREVDESignificanceCliff’s DeltaVREVDESignificanceCliff’s DeltaCodeLlama-13b-Instruct-hf 0.26 0.12 Yes 0.52 0.11 0.10 No 0.01 0.06 0.06 Yes 0.04CodeLlama-34b-Instruct-hf 0.29 0.28 No 0.02 0.09 0.06 Yes 0.14 0.05 0.03 Yes 0.08CodeLlama-7b-Instruct-hf 0.29 0.26 Yes 0.10 0.11 0.08 Yes 0.16 0.06 0.04 Yes 0.05Magicoder-CL-7B 0.31 0.27 Yes 0.14 0.11 0.10 No 0.02 0.05 0.05 Yes 0.01Magicoder-S-CL-7B 0.29 0.27 Yes 0.08 0.10 0.06 Yes 0.18 0.05 0.02 Yes 0.08WizardCoder-1B-V1.0 0.29 0.29 No -0.03 0.10 0.07 Yes 0.15 0.04 0.03 Yes 0.05WizardCoder-3B-V1.0 0.30 0.31 Yes -0.06 0.10 0.05 Yes 0.28 0.05 0.02 Yes 0.10WizardCoder-Python-13B-V1.0 0.31 0.30 Yes -0.03 0.11 0.05 Yes 0.26 0.05 0.02 Yes 0.11WizardCoder-Python-34B-V1.0 0.32 0.30 Yes 0.06 0.12 0.07 Yes 0.20 0.06 0.03 Yes 0.08WizardCoder-Python-7B-V1.0 0.29 0.30 Yes -0.05 0.08 0.07 Yes 0.12 0.04 0.03 Yes 0.02Average 0.30 0.27 — 0.08 0.10 0.07 — 0.15 0.05 0.03 — 0.06\nTable 4Evaluation of CodeBLEU, BLEU, and CrystalBLEU Scores along with Cliff’s Delta\nand Significance for Controlled Prompt Experiments.\nModel CodeBLEU BLEU CrystalBLEUCRECDESignificanceCliff’s DeltaCRECDESignificanceCliff’s DeltaCRECDESignificanceCliff’s DeltaCodeLlama-13b-Instruct-hf 0.30 0.30 Yes 0.52 0.11 0.11 No 0.01 0.06 0.06 Yes 0.01CodeLlama-34b-Instruct-hf 0.29 0.30 No 0.02 0.11 0.04 Yes 0.14 0.06 0.02 Yes 0.12CodeLlama-7b-Instruct-hf 0.30 0.30 Yes 0.10 0.11 0.11 Yes 0.16 0.06 0.06 Yes 0.02Magicoder-CL-7B 0.30 0.30 Yes 0.14 0.10 0.10 No 0.02 0.05 0.05 Yes 0.00Magicoder-S-CL-7B 0.30 0.29 Yes 0.08 0.10 0.10 Yes 0.18 0.05 0.04 Yes 0.01WizardCoder-1B-V1.0 0.29 0.29 No -0.03 0.10 0.06 Yes 0.15 0.05 0.02 Yes 0.08WizardCoder-3B-V1.0 0.30 0.30 Yes -0.06 0.11 0.06 Yes 0.28 0.06 0.03 Yes 0.09WizardCoder-Python-13B-V1.0 0.30 0.30 Yes -0.03 0.11 0.09 Yes 0.26 0.05 0.04 Yes 0.05WizardCoder-Python-34B-V1.0 0.31 0.31 Yes 0.06 0.12 0.07 Yes 0.20 0.06 0.03 Yes 0.09WizardCoder-Python-7B-V1.0 0.30 0.30 Yes -0.05 0.11 0.11 Yes 0.12 0.06 0.05 Yes 0.01Average 0.30 0.30 — 0.08 0.11 0.08 —0.150.06 0.04 — 0.05\n3.Controlled + Direct Evaluation (CDE):We utilize the Controlled Prompt.\nWe do not perform any regex matching or source code extraction.\n4.Controlled + Regex (CRE):We utilize the Controlled Prompt and apply\nour regex to extract source code.\nFor this experiment, we exclude Mixtral, as we demonstrated in RQ3 that the\nregex is ineffective at extracting source code for this model. We include CDE,\nhowever, as it can be reliably evaluated using text-based metrics. In RQ3, we\nexcluded CDE because its maximum possible Compilation Rate was limited to\n6.6% (i.e., 100% - 93.40% MSR), due to the controlled prompt format introducing\ntriple backticks. Without regex-based code extraction, as is the case in CDE,\nthese backticks would lead to compilation failures. When we compare the metrics\n(BLEU and CodeBLEU) computed on the extracted code (CRE, VRE) against the\nmetrics calculated over the direct output from inference (CDE, VDE), we exclude\npoints where the Regex does not match to ensure the same data points for both\nexperiments (intersection set). For the differences observed in the metrics across\nthe experiments, we report the significance with alpha 0.05 and Cliff’s Delta effect\nsize.\n7.2 Results\nTable 3 and Table 4 present the average CodeBLEU, BLEU and CrystalBLEU\nvalues for the evaluations conducted on ten open-source LLMs, using the Vanilla\nPrompt and Controlled Prompt, respectively. Additionally, Table 5 shows the num-\nber of samples in the dataset where the metric increased after removing non-code\ntokens compared to evaluating the inference output.\nFinding 7: When non-source-code tokens are filtered out, the BLEU\nscore increased on average 68.13% and 72.08% of the evaluated samples\nin CRE (using Controlled Prompt) and VRE (using Vanilla Prompt),\n24 Marcos Macedo et al.\nTable 5Percentage of data points that had an increase in the value for the respective metric\ncompared to when the metric was calculated directly from the inference output.\nModel Code BLEU (CRE) BLEU (CRE) CrystalBLEU (CRE) Code BLEU (VRE) BLEU (VRE) CrystalBLEU (VRE)CodeLlama-13b-Instruct-hf 52.28% 59.08% 41.51% 80.77% 38.46% 57.69%CodeLlama-34b-Instruct-hf 40.84% 80.01% 53.95% 52.48% 79.16% 55.78%CodeLlama-7b-Instruct-hf 54.18% 64.09% 44.40% 61.0% 69.71% 44.40%Magicoder-CL-7B 66.9% 62.37% 42.35% 75.39% 65.86% 43.27%Magicoder-S-CL-7B 67.13% 62.41% 41.89% 54.32% 79.41% 54.51%WizardCoder-1B-V1.0 45.18% 75.2% 46.82% 55.84% 76.38% 46.75%WizardCoder-3B-V1.0 52.62% 75.68% 51.50% 43.09% 83.68% 54.38%WizardCoder-Python-13B-V1.0 53.3% 65.15% 46.73% 44.89% 81.14% 54.84%WizardCoder-Python-34B-V1.0 46.16% 76.26% 54.58% 54.75% 74.79% 52.59%WizardCoder-Python-7B-V1.0 65.31% 61.08% 41.89% 51.99% 72.26% 41.41%Average Increase54.39% 68.13% 46.56% 57.45% 72.08% 50.56%\nrespectively. In the case of CrystalBLEU, 46.56% (CRE) and 50.56%\n(VRE) of the samples increased their score. This difference is statis-\ntically significant for the majority of the models.For example, as shown\nin Table 3, for the MagiCoder-S-CL-7B model, the average BLEU score with the\nVanilla Prompt increased from 0.06 (VDE) to 0.10 (VRE), with a statistically\nsignificant difference and a small effect size (Cliff’s Delta = 0.18). The degree of\nimpact varies across models and prompts, as shown in Table 5. For instance, the\nWizardCoder-3B-V1.0 model exhibited the largest improvement when the Vanilla\nPrompt was used. The BLEU score increased in 83.68% of the cases after removing\nnon-source-code tokens. When the Controlled Prompt was used, the CodeLlama-\n34b-Instruct-hf model exhibited an increase of BLEU in 80.01% cases after remov-\ning non-source-code tokens, the highest among the ten models.\nFinding 8: The presence of non-source-code tokens complicates the in-\nterpretation of CodeBLEU. On average, filtering out these tokens in-\ncreases CodeBLEU in 54.39% of evaluated samples with Controlled\nPrompts (CRE) and 57.45% with Vanilla Prompts (VRE). In the re-\nmaining cases, filtering either had no effect or decreased the CodeBLEU\nscore.Unlike BLEU, which is based on n-grams, CodeBLEU evaluates n-gram\nsimilarity and factors like syntax tree structure, data flow, and semantic match.\nThe interplay between these components makes the metric more sensitive to vari-\nations in the structure, especially when additional text is included. For instance,\nwe observe that DataFlow is the component with the highest decrease, 43.76% in\nVRE and 31.24% in CRE, after filtering out natural language, as seen in Fig. 8 and\nFig. 9. This suggests that CodeBLEU’s Data Flow component may be particularly\naffected by the inclusion of natural language, as the component decreased when\nremoved, leading to lower scores.\nWe manually reviewed 50 randomly sampled inference outputs from the Vanilla\nPrompt, where the Data Flow score decreased after extracting the source code us-\ning regular expressions. All of these outputs included natural language text, and in\n46 out of 50 cases, the model added additional text describing the modifications it\nmade during translation, with specific references to variables, functions, and other\nidentifiers. This suggests that the presence of additional text significantly inflates\nthe Data Flow component score. When this text is removed during extraction, the\nData Flow score decreases, affecting the reliability of the CodeBLEU metric.\nOutput Format Biases in the Evaluation of LLMs for Code Translation 25\nFig. 8Variations in the components of the CodeBLEU metric following the extraction of\nsource code using regular expressions with the Control Prompt.\nFig. 9Variations in the components of the CodeBLEU metric following the extraction of\nsource code using regular expressions with the Vanilla Prompt.\nRQ4 Summary:BLEU and CodeBLEU metrics are influenced by the output for-\nmat and non-source-code tokens in the inference output. When utilizing regular\nexpressions to capture source code, the BLEU scores show a noticeable increase.\nFurthermore, Additional Text appears to inflate the Data Flow component of\nCodeBLEU artificially.\n26 Marcos Macedo et al.\n8 RQ5:Does output format bias exist in the evaluation of closed LLMs?\nOpen-source and closed-source LLMs are widely used for coding tasks, including\ncode translation. Closed LLMs are often more effective at following instructions,\nwhich suggests they may be more likely to generate clean source code output when\nthey understand this is the expected result for a code translation task (due to in-\nstruction fine-tuning and human alignment) or when explicitly directed to omit\nadditional text. In this RQ, investigate whether closed models exhibit similar out-\nput format biases. Specifically, we aim to answer whether these models consistently\nproduce source code in a format that is directly suitable for evaluation and if they\npresent a distribution of output formats.\n8.1 Approach\nWe selected five closed LLMs: GPT-3.5, GPT-4, GPT-4o, GPT-4o mini, and Gem-\nini 1.5 Flash (the rationale for choosing these models is discussed in Section 3.2).\nFor this analysis, we reuse the subset created in RQ1 to analyze the output format\nfor open models. Reusing this dataset allows us to maintain consistency in our eval-\nuation, ensuring that the results can be directly compared with the open-source\nmodels assessed earlier.\nWe begin by using the Vanilla Prompt, which as shown in Fig. 4, instructs the\nmodel to “output code only” when generating inference outputs. This allows us\nto observe how these closed models follow basic, straightforward instructions and\nassess the consistency of their output formats without additional control (e.g., the\ndirectives provided in the Controlled Prompt). We then manually examine the\noutput formats generated by the four models to identify any inconsistencies.\nIn cases where multiple output formats are produced, we further investigate whether\nour controlled prompt can effectively standardize the outputs. This step helps us\nevaluate whether closed-source models have different output format distributions.\n8.2 Results\nFig. 10 presents the distribution of output formats observed in the inference out-\nputs generated by the five selected closed LLMs. Aligned with our findings on\nopen-source LLMs, we identified two distinct output formats: Wrapped Code and\nDirect Output. Additionally, we note that no Additional Text is output in any of\nthe closed-source models, indicating they correctly followed the instructions in the\nprompt.\nFinding 9: Closed LLMs typically produce inference outputs with a\nconsistent format within each model. However, the output format can\nvary significantly between models, even within the same model family.\nAs shown in Fig. 10, when Vanilla Prompt is used, three out of the five analyzed\nmodels, Gemini 1.5 Flash, GPT-4o, and GPT-4o mini, consistently produce output\nexclusively in Wrapped-Code format, generating one coherent output format. In\ncontrast, earlier iterations of GPT, i.e., specifically GPT-3.5 Turbo and GPT-4,\nOutput Format Biases in the Evaluation of LLMs for Code Translation 27\nFig. 10Percentage of inference outputs for each model in their respective output format.\npredominantly favour Direct Output format using the Vanilla Prompt, with only\na small fraction (1.67%) of their outputs remaining in Wrapped-Code format.\nNotably, attempts to influence the output format through prompt engineering yield\ninconsistent results. For instance, with GPT-3.5 using the Controlled Prompt leads\nto the Direct Output format increasing from 99.33% to 100%. However, in the case\nof GPT-4, this same approach leads to a decrease in Direct Output from 98.33%\nto 73.61% and an increase of Wrapped-Code format from 1.67% to 26.39%.\nThese findings indicate that understanding the output format distribution remains\nrelevant in certain closed-source models, and a correct extraction method should be\nconsidered for fair comparisons across models. For instance, accounting for output\nformat is crucial when benchmarking open-source models against closed-source\nones, as the latter tend to exhibit more consistent output format characteristics,\nwhich could bias the results in their favor.\nRQ5 Summary:Models such as GPT-4o, GPT-4o mini, and Gemini 1.5 Flash\nexhibit a consistent output format. This is very relevant given that they are often\ncompared to open-source models that do not have a consistent output format,\nincrementing the chances of output format bias. For other models, such as GPT\n3.5 Turbo and GPT-4, it is essential to develop effective strategies for managing\noutput formats to minimize biases in evaluation.\n9 Discussion\nIn this section, we discuss the implications of our findings and offer recommen-\ndations for key stakeholders, namely, researchers benchmarking LLMs on coding\ntasks, particularly code translation, and developers who utilize LLMs in their day-\nto-day coding activities, including code translation.\n28 Marcos Macedo et al.\n9.1 Causes of Output Format Bias\nUnderstanding the causes of output format bias is essential for mitigating its ef-\nfects. However, fully addressing this question remains difficult due to the lack of\ntransparency surrounding the training data of most large language models (LLMs).\nAlthough many open-source models provide access to their model weights, their\ntraining datasets are not publicly available, limiting our ability to identify con-\ntributing factors.\nMagicoder is a notable exception, as it is the only model among those evaluated\nin our work with publicly available training data. Magicoder was instruct-tuned\non the OSS-INSTRUCT dataset (Wei et al., 2023), which consists of prompt-code\npairs in the Wrapped Code format. This suggests that exposure to Wrapped Code\nformatted examples during training can influence a model’s tendency to produce\nconsistently formatted outputs.\nFor closed-source models, such as those developed by OpenAI, the proprietary\nnature of their training pipelines prevents us from drawing conclusions. Nonethe-\nless, we hypothesize that newer models like GPT-4o and GPT-4o-mini may been\ntrained with output control in mind. As illustrated in Fig. 10, earlier versions like\nGPT-3.5 and GPT-4 frequently defaulted to the Direct Output format and were\nless responsive to output control prompts. In contrast, GPT-4o and GPT-4o-mini\nnatively produce code in the Wrapped Code format without requiring explicit\nprompting, which suggests a shift toward incorporating output control measures\nduring the model construction.\n9.2 Mitigating Output Format Bias\nWe propose a combination of prompt engineering and lightweight post-processing\nusing regular expressions to control output formats. This approach has proven\neffective for open-source LLMs, achieving an impressive MSR of 93.40%, as shown\nin Table 2. These techniques are not limited to benchmarking but can also be\nimplemented as a post-processing step in practical code translation applications.\nWe offer tailored suggestions for closed LLMs based on Finding 9. Specifically, for\nGPT-3.5, we recommend directly evaluating the inference outputs generated with\nthe Control Prompt. In the case of GPT-4, regular expressions should be applied,\nand if no match is found, the inference output should be evaluated directly.\nWe also highlight the potential presence of internal mechanisms in certain models,\nsuch as Mixtral, that may affect the steerability of output formats. Researchers\nshould be aware of such model-specific behaviors and adjust their code extrac-\ntion methods accordingly, underscoring the need for customized approaches in the\nevaluation process.\nOverall, for both open-source and closed LLMs, developing and employ-\ning effective strategies to control output formats is essential to mini-\nmize bias and ensure reliable evaluation of large language models in\ncode translation.\nOutput Format Biases in the Evaluation of LLMs for Code Translation 29\n9.3 Output Format Bias Mitigation in Practice\nSince the publication of our earlier work (Macedo et al., 2024a), subsequent re-\nsearch has incorporated our output control strategies, including recent efforts such\nas InterTrans (Macedo et al., 2024b) and AlphaTrans (Ibrahimzada et al., 2025).\nTo better understand the practical impact of output format bias, we conduct an\nablation study using AlphaTrans as a case study. AlphaTrans (Ibrahimzada et al.,\n2025), a repository-level framework for Java-to-Python code translation, adopts\nour recommendation of appending triple backticks to prompts in order to control\nthe output format15. We replicate their experimental setup, usingdeepseek-coder-\n33b-instruct(deepseek-ai, 2024) with greedy decoding, and remove the triple-\nbacktick suffix from the translation prompt templates. The results, shown in Ta-\nble 6, indicate that their approach suffers from a substantial decline in the percent-\nage of syntactically valid Application Main Fragments (AMFs) after the removal\nof the output control. Further analysis on the translation logs confirms that this\ndrop is primarily due to the model generating additional natural language, which\nhinders the reliable extraction of code from its outputs.\nImportantly, all translated projects had some percentage of syntactically correct\ncode without output format control, indicating that the absence of output for-\nmat control does not universally result in failure. This observation aligns with\nour results and reinforces the importance of output format considerations for re-\nliable evaluation. The AlphaTrans authors’ proactive adoption of this technique\nunderscores its practical relevance and importance.\nTable 6Percentage of Application Main Fragments (AMFs) with no syntax issues in Alpha-\nTrans compared to AlphaTrans with no Output Control\nProject AlphaTrans AlphaTrans No-Control Diff\nJavaFastPFOR 95.32% 19.25% -76.07%\ncommons-cli 100.00% 18.68% -81.32%\ncommons-codec 98.53% 26.18% -72.35%\ncommons-csv 98.72% 12.77% -85.96%\ncommons-exec 100.00% 29.44% -70.56%\ncommons-fileupload 100.00% 30.73% -69.27%\ncommons-graph 99.63% 20.33% -79.30%\ncommons-pool 100.00% 29.47% -70.53%\ncommons-validator 99.23% 23.07% -76.16%\njansi 99.76% 28.61% -71.15%\n9.4 The Existence and Impact of Output Format Bias\nWe analyzed the output formats of 11 open-source LLMs and five closed LLMs,\nidentifying four distinct ways in which source code is presented in LLM inference\noutputs: Direct Output, Wrapped Code, Unbalanced Back-ticks, and Re-Wrapped\n15 https://github.com/Intelligent-CAT-Lab/AlphaTrans/blob/593a428bb93031fe169f8\n1109fca97b4cb93dedb/src/translation/prompt_generator.py#L383\n30 Marcos Macedo et al.\nCode. Additionally, in all open-source LLMs, we found that source code was often\nmixed with additional text. This inconsistency in output formats can introduce\nbias when comparing models using execution-based or text-oriented evaluation\nmetrics, which typically assume that the target is pure source code. Our first find-\ning underscores this issue for open-source LLMs, with post-processing required to\nextract the source code in up to 73.6% of outputs.Researchers and practition-\ners should not assume that LLMs will produce a single and consistent\noutput format. Even with a fixed model, prompt, and dataset combina-\ntion, there can be a variety of output formats.\nOur analysis reveals several key issues that arise when output format bias is over-\nlooked in the evaluation of LLMs for code translation:\n–There is a significant difference in output format bias between closed and open-\nsource LLMs. While open-source LLMs outputs often require substantial post-\nprocessing—even when the prompt explicitly instructs the model to generate\nonly source code—most recent and widely used closed LLMs tend to produce\nmore consistent outputs, either within a single model or across multiple models.\nConsequently, the performance of open-source LLMs may be underestimated if\ntheir outputs, though accurate, contain additional text or symbols that obscure\nthe underlying source code.\n–Output format bias can also be influenced by the design of the prompt, as high-\nlighted in Finding 2. Different stakeholders may phrase their prompts in various\nways, and this variation can introduce additional biases in model evaluation.\nIf the output format is ignored and the direct output is used for evaluation\ninstead of the extracted source code, it can lead to inaccurate assessments of\nmodel performance.\n–Ignoring output format bias leads to lower reported performance or hard-to-\ninterpret results. In our case studies (RQ3 and RQ4), we show how controlling\nthe output format (i.e., mitigating output format bias) can significantly impact\nboth execution-based and text-oriented metrics. Finding 5 highlights a notable\nimprovement in CA, with the CRE combination yielding an average CA of\n31.92% across all models, compared to just 4.92% when directly compiling\nthe raw outputs from LLMs. Finding 8 reveals another challenge when output\nformat bias is ignored in interpreting text-oriented metrics.\n–Although closed LLMs exhibit more consistent output formats compared to\nopen-source models, their preferred output format can still vary between mod-\nels, even within the same family (see Finding 9).\n–While some recent closed-source models, such as GPT-4o, appear to avoid\noutput bias issues, other recent models such as DeepSeek Coder can exhibit\nsuch biases, as demonstrated in Sec. 9.3. This emphasizes that the findings of\nour study remain highly relevant regardless of specific model choices. Therefore,\nwe urge researchers and practitioners not to assume that newer models are free\nfrom output format bias.\nTherefore, controlling the output format is crucial for the accurate eval-\nuation of code translations. We emphasize the importance of consider-\ning both output format and extraction methods as key factors when\nOutput Format Biases in the Evaluation of LLMs for Code Translation 31\nassessing LLMs for code translation tasks. To ensure fair and reliable\ncomparisons, we recommend that stakeholders report metrics such as\nMSR and CR, alongside their assumptions about the output format\nand their output format bias mitigation strategy, to ensure reliable and\ncomparable evaluations of LLMs for code translation.\n10 Threats to Validity\nExternal Validity.We acknowledge several limitations that might impact the\nexternal validity of our findings. They are mainly introduced by the selection\nof the dataset, target LLMs, and considered prompts. Firstly, our research re-\nlies on a subset of the CodeNet benchmark. While this may limit the diversity\nof the programs (source code) and target PLs, CodeNet is a comprehensive and\nwidely recognized benchmark in related work, and our selection of PLs is based\non their popularity. Such a setting is also considered in a recent related work (Pan\net al., 2024). This subset offers a diverse range of programming problems and so-\nlutions (we considered five popular programming languages and corresponding 20\nPL translation cases), which we believe enhances the relevance and applicability\nof our findings. Regarding our model selection, we have considered 11 open-source\nLLMs and 5 closed LLMs from six families. Although this represents a constrained\nsubset of available LLMs, these were carefully chosen for their popularity and re-\ncent advancements in the field. We choose instruct-tuned models rather than base\nmodels as instruct-tuned models are specifically optimized to follow instructions\nmore effectively. This means they are more likely to produce outputs that are\nclosely aligned with the given prompts.\nResults in RQ3 show that our current specific prompt and regex may not apply to\nall models, which is particularly evident in the case of the Mixtral 8X7B model.\nWhile this highlights a limitation in the universality of the proposed specific output\ncontrol methods across different models, our design method, i.e., how we analyze\nthe output format and control it using a combination of prompt and regex, can be\ngeneralized and easily adapted to other models.\nInternal Validity.The main threats to internal validity are introduced by the\ndesign of the prompt template and the labeling of output format types. The prompt\ntemplates used in our experiments were custom-designed, which may not represent\nthe optimal choice for each model. However, we carefully crafted these prompt\ntemplates based on templates recommended on the models’ official sites. Regarding\nthe labeling of output formats, this task was undertaken by the first author of\nthis paper. While this could introduce subjective bias or errors in labeling and\ntaxonomy, we established clear, easily identifiable categories to minimize ambiguity\nand enhance consistency in our categorization process.\nConstruct Validity.In this paper, as the CodeNet benchmark contains one test\ncase per sample, there may be translations considered computationally accurate,\nbut that do not contain identical functionality to the input program, i.e., false\npositives. This may influence the results reported in Table 2 on RQ3. However,\nour main goal is to point out the importance of considering output format control\nwhen utilizing LLMs for code translation, and such a conclusion can be supported\n32 Marcos Macedo et al.\nby other metrics, such as extraction success rate and compilation rate, which are\nnot sensitive to the quality of the test cases.\n11 Conclusion\nIn this study, we conducted an empirical analysis of the output formats generated\nby 16 popular instruct-tuned large language models (LLMs), including both open-\nsource and closed-source models, in code translation, using 3,820 translation pairs.\nOur findings reveal significant variability in the output formats produced by these\nLLMs, introducing a new type of bias, i.e., output format bias. Despite utilizing\nprompts that follow recommended strategies from official LLM documentation, we\nobserved that the outputs frequently included additional text or presented code\nin quoted or partially quoted formats, negatively affecting the compilation success\nrate. To address this challenge, we proposed an output control approach that\ncombines specific prompt designs with regular expressions to extract code cleanly\nfrom the outputs. Our case study, based on 3,820 translation pairs, demonstrates\nthe effectiveness of this approach in extracting source code from LLMs’ output\nand improving reported evaluation metrics.\nOur research underscores the critical role of output format, prompt engineering,\nand code extraction methods in the evaluation of LLMs for code translation tasks.\nThe variability in how different models generate output formats, along with their\nsensitivity to prompt design, can significantly impact key evaluation metrics—both\nexecution-based and text-oriented. Ignoring these factors could lead to misinter-\npretations and unfair comparisons between LLMs. We offer recommendations for\npractitioners to accurately assess, control, and mitigate output format bias in LLM\nevaluations. Our insights contribute to improving benchmarking practices in code\ntranslation tasks and provide a framework that could extend to the evaluation\nof LLMs across other coding tasks, helping to better measure their potential and\nreliability.\n11.1 Funding\nWe express our gratitude to the Natural Sciences and Engineering Research Coun-\ncil of Canada (NSERC) for their support, with funding reference number RGPIN-\n2019-05071. Additionally, we extend our appreciation to the Vector Institute for\nits offering of the Vector Scholarship in Artificial Intelligence, which was awarded\nto the first author. The findings and opinions expressed in this paper are those of\nthe authors and do not necessarily represent or reflect those of Huawei and/or its\nsubsidiaries and affiliates.\n11.2 Data Availability Statements\nThe results, source code, and data related to this study are available athttps:\n//github.com/RISElabQueens/forge24-code-translation/tree/main\nOutput Format Biases in the Evaluation of LLMs for Code Translation 33\nReferences\n2024. Airoboros 13B HF fp16.hhttps://huggingface.co/TheBloke/airoboro\ns-13B-HFAccessed: date-of-access.\n2024. LLama 2.https://ai.meta.com/research/publications/llama-2-ope\nn-foundation-and-fine-tuned-chat-models/Accessed: date-of-access.\n2024. Wizard-Vicuna-13B-Uncensored float16 HF.https://huggingface.co/T\nheBloke/Wizard-Vicuna-13B-Uncensored-HFAccessed: date-of-access.\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-\ncia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal\nAnadkat, et al. 2023. GPT-4 Technical Report.arXiv preprint arXiv:2303.08774\n(2023).\nKaran Aggarwal, Mohammad Salameh, and Abram Hindle. 2015.Using machine\ntranslation for converting python 2 to python 3 code. Technical Report. PeerJ\nPrePrints.\nXinyun Chen, Chang Liu, and Dawn Song. 2018. Tree-to-tree neural networks\nfor program translation.Advances in neural information processing systems31\n(2018).\ndeepseek-ai. 2024. DeepSeek Coder 33B Instruct.https://huggingface.co/dee\npseek-ai/deepseek-coder-33b-instruct. Accessed: YYYY-MM-DD.\nAryaz Eghbali and Michael Pradel. 2022. CrystalBLEU: precisely and efficiently\nmeasuring the similarity of code. InProceedings of the 37th IEEE/ACM Inter-\nnational Conference on Automated Software Engineering. 1–12.\nAngela Fan, Beliz Gokkaya, Mark Harman, Mitya Lyubarskiy, Shubho Sengupta,\nShin Yoo, and Jie M Zhang. 2023. Large language models for software engineer-\ning: Survey and open problems.arXiv preprint arXiv:2310.03533(2023).\nSidong Feng and Chunyang Chen. 2023. Prompting Is All Your Need: Auto-\nmated Android Bug Replay with Large Language Models.arXiv preprint\narXiv:2306.01987(2023).\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,\nLinjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. 2020. Codebert: A\npre-trained model for programming and natural languages.arXiv preprint\narXiv:2002.08155(2020).\nShuzheng Gao, Xin-Cheng Wen, Cuiyun Gao, Wenxuan Wang, and Michael R Lyu.\n2023. Constructing Effective In-Context Demonstration for Code Intelligence\nTasks: An Empirical Study.arXiv preprint arXiv:2304.07575(2023).\nMingyang Geng, Shangwen Wang, Dezun Dong, Haotian Wang, Ge Li, Zhi Jin,\nXiaoguang Mao, and Xiangke Liao. 2024. Large Language Models are Few-\nShot Summarizers: Multi-Intent Comment Generation via In-Context Learning.\n(2024).\nAli Reza Ibrahimzada, Kaiyao Ke, Mrigank Pawagi, Muhammad Salman Abid,\nRangeet Pan, Saurabh Sinha, and Reyhaneh Jabbarvand. 2025. AlphaTrans:\nA Neuro-Symbolic Compositional Approach for Repository-Level Code Trans-\nlation and Validation. doi:10.1145/3729379arXiv:2410.24117 [cs].\nMingsheng Jiao, Tingrui Yu, Xuan Li, Guanjie Qiu, Xiaodong Gu, and Beijun\nShen. 2023. On the evaluation of neural code translation: Taxonomy and bench-\nmark. In2023 38th IEEE/ACM International Conference on Automated Soft-\nware Engineering (ASE). IEEE, 1529–1541.\n34 Marcos Macedo et al.\nSvetoslav Karaivanov, Veselin Raychev, and Martin Vechev. 2014. Phrase-based\nstatistical translation of programming languages. InProceedings of the 2014\nACM International Symposium on New Ideas, New Paradigms, and Reflections\non Programming & Software. 173–184.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng,\nCody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient\nMemory Management for Large Language Model Serving with PagedAttention.\ndoi:10.48550/arXiv.2309.06180arXiv:2309.06180 [cs].\nMarie-Anne Lachaux, Baptiste Roziere, Marc Szafraniec, and Guillaume Lam-\nple. 2021. DOBF: A deobfuscation pre-training objective for programming lan-\nguages.Advances in Neural Information Processing Systems34 (2021), 14967–\n14979.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov,\nChenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian\nLiu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene,\nMishig Davaadorj, Joel Lamy-Poirier, Jo˜ ao Monteiro, Oleh Shliazhko, Nicolas\nGontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umap-\nathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra\nMurthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco\nZocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wen-\nhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fe-\ndor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire\nSchlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu,\nJennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Con-\ntractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Car-\nlos Mu˜ noz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von\nWerra, and Harm de Vries. 2023a. StarCoder: may the source be with you!\ndoi:10.48550/arXiv.2305.06161arXiv:2305.06161 [cs].\nTsz-On Li, Wenxi Zong, Yibo Wang, Haoye Tian, Ying Wang, Shing-Chi Cheung,\nand Jeff Kramer. 2023b. Nuances are the Key: Unlocking ChatGPT to Find\nFailure-Inducing Tests with Differential Prompting. In2023 38th IEEE/ACM\nInternational Conference on Automated Software Engineering (ASE). IEEE, 14–\n26.\nShuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio\nBlanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al. 2021.\nCodexglue: A machine learning benchmark dataset for code understanding and\ngeneration.arXiv preprint arXiv:2102.04664(2021).\nZiyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu,\nChongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. WizardCoder:\nEmpowering Code Large Language Models with Evol-Instruct. doi:10.48550/a\nrXiv.2306.08568arXiv:2306.08568 [cs].\nMarcos Macedo, Yuan Tian, Filipe Cogo, and Bram Adams. 2024a. Exploring the\nImpact of the Output Format on the Evaluation of Large Language Models for\nCode Translation. InProceedings of the 2024 IEEE/ACM First International\nConference on AI Foundation Models and Software Engineering. 57–68.\nMarcos Macedo, Yuan Tian, Pengyu Nie, Filipe R Cogo, and Bram Adams. 2024b.\nInterTrans: Leveraging Transitive Intermediate Translations to Enhance LLM-\nbased Code Translation.arXiv preprint arXiv:2411.01063(2024).\nOutput Format Biases in the Evaluation of LLMs for Code Translation 35\nMistral AI Team. 2023.Mixtral of Experts.https://mistral.ai/news/mixtra\nl-of-experts/Accessed: 2024-01-13.\nAnh Tuan Nguyen, Tung Thanh Nguyen, and Tien N Nguyen. 2013. Lexical\nstatistical machine translation for language migration. InProceedings of the\n2013 9th Joint Meeting on Foundations of Software Engineering. 651–654.\nAnh Tuan Nguyen, Tung Thanh Nguyen, and Tien N Nguyen. 2014. Migrating\ncode with statistical machine translation. InCompanion Proceedings of the 36th\nInternational Conference on Software Engineering. 544–547.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou,\nSilvio Savarese, and Caiming Xiong. 2022. CodeGen: An Open Large Language\nModel for Code with Multi-Turn Program Synthesis. InThe Eleventh Interna-\ntional Conference on Learning Representations.\nRangeet Pan, Ali Reza Ibrahimzada, Rahul Krishna, Divya Sankar, Lam-\nbert Pouguem Wassi, Michele Merler, Boris Sobolev, Raju Pavuluri, Saurabh\nSinha, and Reyhaneh Jabbarvand. 2024. Lost in translation: A study of bugs\nintroduced by large language models while translating code. InProceedings of\nthe IEEE/ACM 46th International Conference on Software Engineering. 1–13.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2001. BLEU:\na method for automatic evaluation of machine translation. InProceedings of\nthe 40th Annual Meeting on Association for Computational Linguistics - ACL\n’02. Association for Computational Linguistics, Philadelphia, Pennsylvania, 311.\ndoi:10.3115/1073083.1073135\nRuchir Puri, David S. Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi,\nVladimir Zolotov, Julian Dolby, Jie Chen, Mihir Choudhury, Lindsey Decker,\nVeronika Thost, Luca Buratti, Saurabh Pujar, Shyam Ramji, Ulrich Finkler,\nSusan Malaika, and Frederick Reiss. 2021. CodeNet: A Large-Scale AI for Code\nDataset for Learning a Diversity of Coding Tasks. doi:10.48550/arXiv.2105.\n12655arXiv:2105.12655 [cs].\nShuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundare-\nsan, Ming Zhou, Ambrosio Blanco, and Shuai Ma. 2020. CodeBLEU: a Method\nfor Automatic Evaluation of Code Synthesis.http://arxiv.org/abs/2009.1\n0297arXiv:2009.10297 [cs].\nBaptiste Roziere, Marie-Anne Lachaux, Lowik Chanussot, and Guillaume Lample.\n2020. Unsupervised translation of programming languages.Advances in Neural\nInformation Processing Systems33 (2020), 20601–20611.\nBaptiste Roziere, Jie M Zhang, Francois Charton, Mark Harman, Gabriel Syn-\nnaeve, and Guillaume Lample. 2021. Leveraging automated unit tests for unsu-\npervised code translation.arXiv preprint arXiv:2110.06773(2021).\nBaptiste Rozi` ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiao-\nqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J´ er´ emy Rapin, Artyom\nKozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton\nFerrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D´ efossez, Jade Copet,\nFaisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom,\nand Gabriel Synnaeve. 2023. Code Llama: Open Foundation Models for Code.\ndoi:10.48550/arXiv.2308.12950arXiv:2308.12950 [cs].\nKlaas-Jan Stol, Paul Ralph, and Brian Fitzgerald. 2016. Grounded theory in\nsoftware engineering research: a critical review and guidelines. InProceed-\nings of the 38th International Conference on Software Engineering (ICSE\n’16). Association for Computing Machinery, New York, NY, USA, 120–131.\n36 Marcos Macedo et al.\ndoi:10.1145/2884781.2884833\nMarc Szafraniec, Baptiste Roziere, Hugh Leather, Francois Charton, Patrick La-\nbatut, and Gabriel Synnaeve. 2023. Code Translation with Compiler Represen-\ntations. doi:10.48550/arXiv.2207.03578arXiv:2207.03578 [cs].\nYue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. Codet5: Identifier-\naware unified pre-trained encoder-decoder models for code understanding and\ngeneration.arXiv preprint arXiv:2109.00859(2021).\nYuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. 2023.\nMagicoder: Source Code Is All You Need. doi:10.48550/arXiv.2312.02120\narXiv:2312.02120 [cs].\nJustin D. Weisz, Michael Muller, Stephanie Houde, John Richards, Steven I. Ross,\nFernando Martinez, Mayank Agarwal, and Kartik Talamadupula. 2021. Per-\nfection Not Required? Human-AI Partnerships in Code Translation. In26th\nInternational Conference on Intelligent User Interfaces. ACM, College Station\nTX USA, 402–412. doi:10.1145/3397481.3450656\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement De-\nlangue, Anthony Moi, Pierric Cistac, Tim Rault, R´ emi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien\nPlu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. HuggingFace’s Transformers: State-\nof-the-art Natural Language Processing. doi:10.48550/arXiv.1910.03771\narXiv:1910.03771 [cs].\nZhen Yang, Fang Liu, Zhongxing Yu, Jacky Wai Keung, Jia Li, Shuo Liu, Yifan\nHong, Xiaoxue Ma, Zhi Jin, and Ge Li. 2024. Exploring and unleashing the\npower of large language models in automated code translation.Proceedings of\nthe ACM on Software Engineering1, FSE (2024), 1585–1608.\nQinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan\nWang, Lei Shen, Andi Wang, Yang Li, et al. 2023. Codegeex: A pre-trained\nmodel for code generation with multilingual evaluations on humaneval-x.arXiv\npreprint arXiv:2303.17568(2023).\nMing Zhu, Aneesh Jain, Karthik Suresh, Roshan Ravindran, Sindhu Tipirneni,\nand Chandan K Reddy. 2022a. Xlcost: A benchmark dataset for cross-lingual\ncode intelligence.arXiv preprint arXiv:2206.08474(2022).\nMing Zhu, Karthik Suresh, and Chandan K Reddy. 2022b. Multilingual code snip-\npets training for program translation. InProceedings of the AAAI Conference\non Artificial Intelligence, Vol. 36. 11783–11790.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7783427238464355
    },
    {
      "name": "Python (programming language)",
      "score": 0.7629846334457397
    },
    {
      "name": "Programming language",
      "score": 0.6172550916671753
    },
    {
      "name": "Java",
      "score": 0.5945420861244202
    },
    {
      "name": "Source code",
      "score": 0.5063751935958862
    },
    {
      "name": "Software engineering",
      "score": 0.47981810569763184
    },
    {
      "name": "KPI-driven code analysis",
      "score": 0.44122105836868286
    },
    {
      "name": "Code (set theory)",
      "score": 0.42824506759643555
    },
    {
      "name": "Software",
      "score": 0.3737007975578308
    },
    {
      "name": "Natural language processing",
      "score": 0.3509894907474518
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3382154703140259
    },
    {
      "name": "Software development",
      "score": 0.29904961585998535
    },
    {
      "name": "Static program analysis",
      "score": 0.24160224199295044
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I204722609",
      "name": "Queen's University",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I4210115038",
      "name": "Huawei Technologies (Canada)",
      "country": "CA"
    }
  ],
  "cited_by": 10
}