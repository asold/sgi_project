{
    "title": "Rethinking Graph Transformers with Spectral Attention",
    "url": "https://openalex.org/W3169575312",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4283142004",
            "name": "Kreuzer, Devin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4282733033",
            "name": "Beaini, Dominique",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4287061730",
            "name": "Hamilton, William L.",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4287304770",
            "name": "LÃ©tourneau, Vincent",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4286989850",
            "name": "Tossou, Prudencio",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3042905730",
        "https://openalex.org/W3155952169",
        "https://openalex.org/W2050210602",
        "https://openalex.org/W2519887557",
        "https://openalex.org/W2995273672",
        "https://openalex.org/W3102154670",
        "https://openalex.org/W2558748708",
        "https://openalex.org/W3087257704",
        "https://openalex.org/W2953227400",
        "https://openalex.org/W3035475042",
        "https://openalex.org/W1984159596",
        "https://openalex.org/W2171567099",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2962767366",
        "https://openalex.org/W3016124664",
        "https://openalex.org/W2962810718",
        "https://openalex.org/W1784722469",
        "https://openalex.org/W2768242641",
        "https://openalex.org/W3034190530",
        "https://openalex.org/W2294879381",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2970493342",
        "https://openalex.org/W1993713305",
        "https://openalex.org/W2969476091",
        "https://openalex.org/W2766453196",
        "https://openalex.org/W3080555959",
        "https://openalex.org/W2558460151",
        "https://openalex.org/W2606780347",
        "https://openalex.org/W3021975806",
        "https://openalex.org/W3007332492",
        "https://openalex.org/W2894175828",
        "https://openalex.org/W3005922524",
        "https://openalex.org/W3102509876",
        "https://openalex.org/W2624431344",
        "https://openalex.org/W2786722833",
        "https://openalex.org/W2899771611"
    ],
    "abstract": "In recent years, the Transformer architecture has proven to be very successful in sequence processing, but its application to other data structures, such as graphs, has remained limited due to the difficulty of properly defining positions. Here, we present the $\\textit{Spectral Attention Network}$ (SAN), which uses a learned positional encoding (LPE) that can take advantage of the full Laplacian spectrum to learn the position of each node in a given graph. This LPE is then added to the node features of the graph and passed to a fully-connected Transformer. By leveraging the full spectrum of the Laplacian, our model is theoretically powerful in distinguishing graphs, and can better detect similar sub-structures from their resonance. Further, by fully connecting the graph, the Transformer does not suffer from over-squashing, an information bottleneck of most GNNs, and enables better modeling of physical phenomenons such as heat transfer and electric interaction. When tested empirically on a set of 4 standard datasets, our model performs on par or better than state-of-the-art GNNs, and outperforms any attention-based model by a wide margin, becoming the first fully-connected architecture to perform well on graph benchmarks.",
    "full_text": "Rethinking Graph Transformers with Spectral\nAttention\nDevin Kreuzerâˆ—\nMcGill University, Mila\nMontreal, Canada\ndevin.kreuzer@mail.mcgill.ca\nDominique Beaini*\nValence Discovery\nMontreal, Canada\ndominique@valencediscovery.com\nWilliam L. Hamilton\nMcGill University, Mila\nMontreal, Canada\nwlh@cs.mcgill.ca\nVincent LÃ©tourneau\nUniversity of Ottawa\nOttawa, Canada\nvletour2@uottawa.ca\nPrudencio Tossou\nValence Discovery\nMontreal, Canada\nprudencio@valencediscovery.com\nAbstract\nIn recent years, the Transformer architecture has proven to be very successful in\nsequence processing, but its application to other data structures, such as graphs,\nhas remained limited due to the difï¬culty of properly deï¬ning positions. Here, we\npresent the Spectral Attention Network (SAN), which uses a learned positional\nencoding (LPE) that can take advantage of the full Laplacian spectrum to learn the\nposition of each node in a given graph. This LPE is then added to the node features\nof the graph and passed to a fully-connected Transformer. By leveraging the full\nspectrum of the Laplacian, our model is theoretically powerful in distinguishing\ngraphs, and can better detect similar sub-structures from their resonance. Further,\nby fully connecting the graph, the Transformer does not suffer from over-squashing,\nan information bottleneck of most GNNs, and enables better modeling of physical\nphenomenons such as heat transfer and electric interaction. When tested empirically\non a set of 4 standard datasets, our model performs on par or better than state-of-the-\nart GNNs, and outperforms any attention-based model by a wide margin, becoming\nthe ï¬rst fully-connected architecture to perform well on graph benchmarks.\n1 Introduction\nThe prevailing strategy for graph neural networks (GNNs) has been to directly encode graph structure\nstructure through a sparse message-passing process [ 17, 19]. In this approach, vector messages\nare iteratively passed between nodes that are connected in the graph. Multiple instantiations of\nthis message-passing paradigm have been proposed, differing in the architectural details of the\nmessage-passing apparatus (see [19] for a review).\nHowever, there is a growing recognition that the message-passing paradigm has inherent limitations.\nThe expressive power of message passing appears inexorably bounded by the Weisfeiler-Lehman iso-\nmorphism hierarchy [29, 30, 39]. Message-passing GNNs are known to suffer from pathologies, such\nâˆ—Equal contribution.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021).\narXiv:2106.03893v3  [cs.LG]  27 Oct 2021\nas oversmoothing, due to their repeated aggregation of local information [19], and over-squashing,\ndue to the exponential blow-up in computation paths as the model depth increases [1].\nAs a result, there is a growing interest in deep learning techniques that encode graph structure as asoft\ninductive bias, rather than as a hard-coded aspect of message passing [14, 24]. A central issue with\nmessage-passing paradigm is that input graph structure is encoded by restricting the structure of the\nmodelâ€™s computation graph, inherently limiting its ï¬‚exibility. This reminds us of how early recurrent\nneural networks (RNNs) encoded sequential structure via their computation graphâ€”a strategy that\nleads to well-known pathologies such as the inability to model long-range dependencies [20].\nThere is a growing trend across deep learning towards more ï¬‚exible architectures, which avoid strict\nand structural inductive biases. Most notably, the exceptionally successful Transformer architecture\nremoves any structural inductive bias by encoding the structure via soft inductive biases, such as\npositional encodings [36]. In the context of GNNs, the self-attention mechanism of a Transformer\ncan be viewed as passing messages between all nodes, regardless of the input graph connectivity.\nPrior work has proposed to use attention in GNNs in different ways. First, the GAT model [ 37]\nproposed local attention on pairs of nodes that allows a learnable convolutional kernel. The GTN work\n[42] has improved on the GAT for node and link predictions while keeping a similar architecture,\nwhile other message-passing approaches have used enhancing spectral features [ 8, 13] . More\nrecently, the GT model [14] was proposed as a generalization of Transformers to graphs, where they\nexperimented with sparse and full graph attention while providing low-frequency eigenvectors of the\nLaplacian as positional encodings.\nIn this work, we offer a principled investigation of how Transformer architectures can be applied\nin graph representation learning. Our primary contributionis the development of novel and\npowerful learnable positional encoding methods, which are rooted in spectral graph theory. Our\npositional encoding technique â€” and the resulting spectral attention network (SAN) architecture â€”\naddresses key theoretical limitations in prior graph Transformer work [14] and provably exceeds the\nexpressive power of standard message-passing GNNs. We show that full Transformer-style attention\nprovides consistent empirical gains compared to an equivalent sparse message-passing model, and\nwe demonstrate that our SAN architecture is competitive with or exceeding the state-of-the-art on\nseveral well-known graph benchmarks. An overview of the entire method is presented in Figure 1,\nwith a link to the code here: https://github.com/DevinKreuzer/SAN.\n2 Theoretical Motivations\nThere can be a signiï¬cant loss in structural information if naively generalizing Transformers to graphs.\nTo preserve this information as well as local connectivity, previous studies [37, 14] have proposed to\nuse the eigenfunctions of their Laplacian as positional encodings. Taking this idea further by using\nthe full expressivity of eigenfunctions as positional encodings, we can propose a principled way\nof understanding graph structures using their spectra. The advantages of our methods compared to\nprevious studies [37, 14] are shown in Table 1.\nTable 1: Comparison of the properties of different graph Transformer models.\nMODELS GAT [37] GT sparse [14] GT full [14] SAN (Node LPE)\nPreserves local structure in attention \u0013 \u0013 \u0017 \u0013\nUses edge features \u0017 \u0013 \u0017 \u0013\nConnects non-neighbouring nodes \u0017 \u0017 \u0013 \u0013\nUses eigenvector-based PE for attention\u0017 \u0013 \u0013 \u0013\nUse a PE with structural information \u0017 \u0013 \u0017 2 \u0013\nConsiders the ordering of the eigenvalues\u0017 \u0013 \u0013 \u0013\nInvariant to the norm of the eigenvector- \u0013 \u0013 \u0013\nConsiders the spectrum of eigenvalues\u0017 \u0017 \u0017 \u0013\nConsiders variable # of eigenvectors - \u0017 \u0017 \u0013\nAware of eigenvalue multiplicities - \u0017 \u0017 \u0013\nInvariant to the sign of the eigenvectors- \u0017 \u0017 \u0017\n1Presented results add full connectivity before computing the eigenvectors, thus losing the structural informa-\ntion of the graph.\n2\nNode features\n: Adjacency matrix\n: Laplacian matrix\n: number of nodes\n: number of edges.\n: Number of input \nnode features\n: Number of input edge \nfeatures\n: Computaï¿½on\ncomplexity\n(a)\nPre-computed steps Learned posiï¿½onal encoding (LPE) steps\nmax 0-max\nNode colormap\nInput graph\nThe normalized eigenvectors\nof are computed and \nsorted such that has the \nlowest eigenvalue and\nhas the -th lowest.\nThe complexity is .\n(b) Compute the ï¬rst\neigenvectors (c) Generate node-wise\neigenvector PE\n: The -th lowest eigenvalue\n: The normalized\neigenvector associated to\n: The -th row of\nFor each node , generate an \niniï¿½al posiï¿½onal encoding (PE)\nusing the -ï¬rst and .\nIf a graph has less than\nnodes, add a masked padding.\n(d)Generate node-wise\nembedding\nFor each node , generate a \nlearned posiï¿½onal embedding\n(LPE) of size .\nAlinear layeris applied,\nfollowed by a mulï¿½-layer\nTransformer encoderwith\nself-aï¿½enï¿½on on the \nsequence length of size .\nSequence length\n(e)Pool the LPE\nUse asumormeanpooling on\nthe dimension of size of the \nnode-wise embedding.\nThe result is theLPEmatrix,\nwhere each line represents \nthe learned posiï¿½onal\nencoding of the -th node.\nLPE\nNumber of features\n+\nMain Transformer steps\nGraph\n1\n2\nâ‹®â‹® â‹®\nEdge features\n1\n2\n3\nâ‹®â‹® â‹®\n(f) Fully connect the\ngraph\nAn edge is added to all pairs \nof disconnected nodes and\ngiven its own embedding.\nThe size of the edge embed-\nding dicï¿½onary increases by\n1, and the number of edges\nbecomes .\nAdd an MLP or linear layer \nfor both the node and edge\nfeatures.\n(g) Input layers for the\nfeature\nâ‹®â‹® â‹®\nConcatenate the node \nfeatures from the MLP to\nthose from the LPE.\n(h) Concatenate node\nfeatures\nAï¿½enï¿½on between all pairs of \nnodes features and the edge\nbetween them. Diï¬€erent\nlinear projecï¿½ons are \nused to compute aï¿½enï¿½on for\nreal edges and added edges.\n(i) Apply the main\ntransformer\nOutputPredicï¿½on\nlayer\nMLP\nMLP\nTransformer\nencoders\non the dimension of\nsize\nğ‘—\nğ“ğ‘š âˆ’1,ğ‘—ğœ†ğ‘š âˆ’1\nğ“0,ğ‘—ğœ†0\nğ‘—\nLinear Transformer\n: hidden dimension\nFigure 1: The proposed SAN model with the node LPE, a generalization of Transformers to graphs.\n2.1 Absolute and relative positional encoding with eigenfunctions\nThe notion of positional encodings (PEs) in graphs is not a trivial concept, as there exists no canonical\nway of ordering nodes or deï¬ning axes. In this section, we investigate how eigenfunctions of the\nLaplacian can be used to deï¬ne absolute and relative PEs in graphs, to measure physical interactions\nbetween nodes, and to enable â€hearingâ€ of speciï¬c sub-structures - similar to how the sound of a\ndrum can reveal its structure.\n2.1.1 Eigenvectors equate to sine functions over graphs\nIn the Transformer architecture, a fundamental aspect is the use of sine and cosine functions as PEs\nfor sequences [36]. However, sinusoids cannot be clearly deï¬ned for arbitrary graphs, since there is\nno clear notion of position along an axis. Instead, their equivalent is given by the eigenvectors Ï†of\nthe graph Laplacian L. Indeed, in a Euclidean space, the Laplacian (or Laplace) operator corresponds\nto the divergence of the gradient and its eigenfunctions are sine/cosine functions, with the squared\nfrequencies corresponding to the eigenvalues (we sometimes interchange the two notions from here\non). Hence, in the graph domain, the eigenvectors of the graph Laplacian are the natural equivalent of\nsine functions, and this intuition was employed in multiple recent works which use the eigenvectors\nas PEs for GNNs [15], for directional ï¬‚ows [4] and for Transformers [14].\nBeing equivalent to sine functions, we naturally ï¬nd that the Fourier Transform of a function F[f]\napplied to a graph gives F[f](Î»i) =âŸ¨f,Ï†iâŸ©, where the eigenvalue is considered as a position in the\nFourier domain of that graph [6]. Thus, the eigenvectors are best viewed as vectors positioned on the\naxis of eigenvalues rather than components of a matrix as illustrated in Figure 2.\n2.1.2 What do eigenfunctions tell us about relative positions?\nIn addition to being the analog of sine functions, the eigenvectors of the Laplacian also hold important\ninformation about the physics of a system and can reveal distance metrics. This is not surprising as\n3\nFigure 2: a) Standard view of the eigenvectors as a matrix. b) Eigenvectors Ï†i viewed as vectors\npositionned on the axis of frequencies (eigenvalues).\nthe Laplacian is a fundamental operator in physics and is notably used in Maxwellâ€™s equations [16]\nand the heat diffusion [6].\nIn electromagnetic theory, the (pseudo)inverse of the Laplacian, known in mathematics as the Greenâ€™s\nfunction of the Laplacian [9], represents the electrostatic potential of a given charge. In a graph, the\nsame concept uses the pseudo-inverse of the Laplacian Gand can be computed by its eigenfunctions.\nSee equation 1 , where G(j1,j2) is the electric potential between nodes j1 and j2, Ë†Ï†i and Ë†Î»i are\nthe i-th eigenvectors and eigenvalues of the symmetric Laplacian D\nâˆ’1\n2 LD\nâˆ’1\n2 , and Dis the degree\nmatrix, and Ë†Ï†i,j the j-th row of the vector.\nG(j1,j2) =d\n1\n2\nj1 d\nâˆ’1\n2\nj2\nâˆ‘\ni>0\n( Ë†Ï†i,j1\nË†Ï†i,j2 )2\nË†Î»i\n(1)\nFurther, the original solution of the heat equation given by Fourier relied on a sum of sines/cosines\nknown as a Fourier series [7]. As eigenvectors of the Laplacian are the analogue of these functions in\ngraphs, we ï¬nd similar solutions. Knowing that heat kernels are correlated to random walks [6, 4],\nwe use the interaction between two heat kernels to deï¬ne in equation 2 the diffusion distance dD\nbetween nodes j1,j2 [6, 10]. Similarly, the biharmonic distance dB was proposed as a better measure\nof distances [28]. Here we use the eigenfunctions of the regular Laplacian L.\nd2\nD(j1,j2) =\nâˆ‘\nk>0\neâˆ’2tÎ»i(Ï†i,j1 âˆ’Ï†i,j2 )2 , d 2\nB(j1,j2) =\nâˆ‘\ni>0\n(Ï†i,j1 âˆ’Ï†i,j2 )2\nÎ»2\ni\n(2)\nThere are a few things to note from these equations. Firstly, they highlight the importance of pairing\neigenvectors and their corresponding eigenvalueswhen supplying information about relative positions\nin a graph. Secondly, we notice that the product of eigenvectors is proportional to the electrostatic\ninteraction, while the subtraction is proportional to the diffusion and biharmonic distances. Lastly,\nthere is a consistent pattern across all 3 equations: smaller frequencies/eigenvalues are more heavily\nweighted when determining distances between nodes.\n2.1.3 Hearing the shape of a graph and its sub-structures\nAnother well-known property of eigenvalues is how they can be used to discriminate between different\ngraph structures and sub-structures, as they can be interpreted as the frequencies of resonance\nof the graph. This led to the famous question about whether we can hear the shape of a drum\nfrom its eigenvalues [23], with the same questions also applying to geometric objects [12] and 3D\nmolecules [33]. Various success was found with the eigenfunctions being used for partial functional\ncorrespondence [32], algorithmic understanding geometries [ 26], and style correspondence [ 12].\nExamples of eigenvectors for molecular graphs are presented in Figure 3.\nFigure 3: Examples of eigenvalues Î»i and eigenvectors Ï†i for molecular graphs. The low-frequency\neigenvectors Ï†1,Ï†2 are spread accross the graph, while higher frequencies, such as Ï†14,Ï†15 for the\nleft molecule or Ï†10,Ï†11 for the right molecule, often resonate in local structures.\n4\n2.2 Laplace Eigenfunctions etiquette\nIn Euclidean space and sequences, using sinusoids as PEs is trivial: we can simply select a set of\nfrequencies, compute the sinusoids, and add or concatenate them to the input embeddings, as is done\nin the original Transformer [ 36]. However, in arbitrary graphs, reproducing these steps is not as\nsimple since each graph has a unique set of eigenfunctions. In the following section, we present\nkey principles from spectral graph theory to consider when constructing PEs for graphs, most of\nwhich have been overlooked by prior methods. They include normalization, the importance of the\neigenvalues and their multiplicities, the number of eigenvectors being variable, and sign ambiguities.\nOur LPE architectures, presented in section 3, aim to address them.\nNormalization. Given an eigenvalue of the Laplacian, there is an associated eigenspace of dimension\ngreater than 1. To make use of this information in our model, a single eigenvector has to be chosen.\nIn our work, we use the L2 normalization since it is compatible with the deï¬nition of the Greenâ€™s\nfunction (1). Thus, we will always chose eigenvectors Ï†such that âŸ¨Ï†,Ï†âŸ©= 1.\nEigenvalues. Another fundamental aspect is that the eigenvalue associated with each eigenvector\nsupplies valuable information. An ordering of the eigenvectors based on their eigenvalue works in\nsequences since the frequencies are pre-determined. However, this assumption does not work in\ngraphs since the eigenvalues in their spectrum can vary. For example, in Figure 3, we observe how an\nordering would miss the fact that both molecules resonate at Î»= 1in different ways.\nMultiplicities. Another important problem with choosing eigenfunctions is the possibility of a\nhigh multiplicity of the eigenvalues, i.e. when an eigenvalue appears as a root of the characteristic\npolynomial more than once. In this case, the associated eigenspace may have dimension 2 or\nmore as we can generate a valid eigenvector from any linear combination of eigenvectors with the\nsame eigenvalue. This further complicates the problem of choosing eigenvectors for algorithmic\ncomputations and highlights the importance of having a model that can handle this ambiguity.\nVariable number of eigenvectors. A graph Gican have at mostNilinearly independent eigenvectors\nwith Ni being its number of nodes. Most importantly, Ni can vary across all Gi in the dataset. Prior\nwork [14] elected to select a ï¬xed number keigenvectors for each graph, where k â‰¤Ni,âˆ€i. This\nproduces a major bottleneck when the smallest graphs have signiï¬cantly fewer nodes than the largest\ngraphs in the dataset since a very small proportion of eigenvectors will be used for large graphs. This\ninevitably causes loss of information and motivates the need for a model which constructs ï¬xed PEs\nof dimension k, where kdoes not depend on the number of eigenvectors in the graph.\nSign invariance. As noted earlier, there is a sign ambiguity with the eigenvectors. With the sign of\nÏ†being independent of its normalization, we are left with a total of 2k possible combination of signs\nwhen choosing keigenvectors of a graph. Previous work has proposed to do data augmentation by\nrandomly ï¬‚ipping the sign of the eigenvectors [4, 15, 14], and although it can work when kis small,\nit becomes intractable for large k.\n2.3 Learning with Eigenfunctions\nLearning generalizable information from eigenfunctions is fundamental to their succesful usage. Here\nwe detail important points that support it is possible to do so if done correctly.\nSimilar graphs have similar spectra. Thus, we can expect the network to transfer patterns across\ngraphs through the similarity of their spectra. In fact, spectral graph theory tells us that the lowest\nand largest non-zero eigenvalues are both linked to the geometry of the graph (algebraic connectivity\nand spectral radius).\nEigenspaces contain geometric information. Spectral graph theory has studied the geometric and\nphysical properties of graphs from their Laplacian eigenfunctions in depth. Developing a method\nthat can use the full spectrum of a graph makes it theoretically possible capture this information. It\nus thus important to capture differences between the full eigenspaces instead of minor differences\nbetween speciï¬c eigenvalues or eigenvectors from graph to graph.\nLearned positions are relative within graphs. Eigenspaces are used to understand the relationship\nbetween nodes within graphs, not across them. Proposed models should therefore only compare the\neigenfunctions of nodes within graphs.\n5\n3 Model Architecture\nIn this section, we propose an elegant architecture that can use the eigenfunctions as PEs while\naddressing the concerns raised in section 2.2. Our Spectral Attention Network (SAN) model inputs\neigenfunctions of a graph and projects them into a learned positional encoding (LPE) of ï¬xed size.\nThe LPE allows the network to use up to the entire Laplace spectrum of each graph, learn how the\nfrequencies interact, and decide which are most important for the given task.\nWe propose a two-step learning process summarized earlier in Figure 1. The ï¬rst step, depicted by\nblocks (c-d-e) in the ï¬gure, applies a Transformer over the eigenfunctions of each node to generate\nan LPE matrix for each graph. The LPE is then concatenated to the node embeddings (blocks g-h),\nbefore being passed to the Graph Transformer (block i). If the task involves graph classiï¬cation or\nregression, the ï¬nal node embeddings are subsequently passed to a ï¬nal pooling layer.\n3.1 LPE Transformer Over Nodes\nUsing Laplace encodings as node features is ubiquitous in the literature concerning the topic. Here,\nwe propose a method for learning node PEs motivated by the principles from section 2.2. The idea of\nour LPE is inspired by Figure 2, where the eigenvectors Ï†are represented as a non-uniform sequence\nwith the eigenvalue Î»being the position on the frequency axis. With this representation, Transformers\nare a natural choice for processing them and generating a ï¬xed-size PE.\nThe proposed LPE architecture is presented in Figure 4. First, we create an embedding matrix\nof size 2 Ã—m for each node j by concatenating the m-lowest eigenvalues with their associated\neigenvectors. Here, mis a hyper-parameter for the maximum number of eigenvectors to compute\nand is analog to the variable-length sequence for a standard Transformer. For graphs where m>N ,\na masked-padding is simply added. Note that to capture the entire spectrum of all graphs, one can\nsimply select msuch that it is equal to the maximum number of nodes a graph has in the dataset.\nA linear layer is then applied on the dimension of size 2 to generate new embeddings of size k. A\nTransformer Encoder then computes self-attention on the sequence of lengthmand hidden dimension\nk. Finally, a sum pooling reduces the sequence into a ï¬xed k-dimensional node embedding.\nThe LPE model addresses key limitations of previous graph Transformers and is aligned with the\nï¬rst four etiquettes presented in section 2.2. By concatenating the eigenvalues with the normalized\neigenvector, this model directly addresses the ï¬rst three etiquettes. Namely, it normalizes the\neigenvectors, pairs eigenvectors with their eigenvalues and treats the number of eigenvectors as a\nvariable. Furthermore, the model is aware of multiplicities and has the potential to linearly combine\nor ignore some of the repeated eigenvalues.\nHowever, this method still does not address the limitation that the sign of the pre-computed eigenvec-\ntors is arbitrary. To combat this issue, we randomly ï¬‚ip the sign of the pre-computed eigenvectors\nduring training as employed by previous work [15, 14], to promote invariance to the sign ambiguity.\nFigure 4: Learned positional encoding (LPE) architectures, with the model being aware of the graphâ€™s\nLaplace spectrum by considering meigenvalues and eigenvectors, where we permitmâ‰¤N, with\nN denoting the number of nodes. Since the Transformer loops over the nodes, each node can be\nviewed as an element of a batch to parallelize the computation. Here Ï†i,j is the j-th element of the\neigenvector paired to the i-th lowest eigenvalue Î»i.\n3.2 LPE Transformer Over Edges\nHere we present an alternative formulation for Laplace encodings. This method addresses the same\nissues as the LPE over nodes, but also resolves the eigenvector sign ambiguity. Instead of encoding\nabsolute positions as node features, the idea is to consider relative positions encoded as edge features.\n6\nInspired by the physical interactions introduced in 1 and 2, we can take a pair of nodes (j1,j2)\nand obtain sign-invariant operators using the absolute subtraction |Ï†i,j1 âˆ’Ï†i,j2 |and the product\nÏ†i,j1 Ï†i,j2 . These operators acknowledge that the sign of Ï†i,j1 at a given node j1 is not important, but\nthat the relative sign between nodes j1 and j2 is important. One might argue that we could directly\ncompute the deterministic values from equations (1, 2) as edge features instead. However, our goal is\nto construct models that can learn which frequencies to emphasize and are not biased towards the\nlower frequencies â€” despite lower frequencies being useful in many tasks.\nThis approach is only presented thoroughly in appendix A, since it suffers from a major computational\nbottleneck compared to the LPE over nodes. In fact, for a fully-connected graph, there are N times\nmore edges than nodes, thus the computation complexity is O(m2N2), or O(N4) considering all\neigenfunctions. The same limitation also affects memory and prevents the use of large batch sizes.\n3.3 Main Graph Transformer\nOur attention mechanism in the main Transformer is based on previous work [14], which attempts\nto repurpose the original Transformer to graphs by considering the graph structure and improving\nattention estimates with edge feature embeddings.\nIn the following, note that hl\ni is the i-th nodeâ€™s features at thel-th layer, and eij is the edge feature\nembedding between nodes iand j. Our model employs multi-head attention over all nodes:\nË†hl+1\ni = Ol\nh\nHn\nk=1\n(\nâˆ‘\njâˆˆV\nwk,l\nij Vk,lhl\nj) (3)\nwhere Ol\nh âˆˆRdÃ—d, Vk,l âˆˆRdkÃ—d, H denotes the number of heads, Lthe number of layers, and f\nconcatenation. Note that dis the hidden dimension, while dk is the dimension of a head ( d\nH = dk).\nA key addition from our work is the design of an architecture that performs full-graph attention while\npreserving local connectivity with edge features via two sets of attention mechanisms: one for nodes\nconnected by real edges in the sparse graph and one for nodes connected by added edges in the\nfully-connected graph. The attention weights wk,l\nij in equation 3 at layer land head kare given by:\nË†wk,l\nij =\nï£±\nï£´ï£´ï£²\nï£´ï£´ï£³\nQ1,k,lhl\niâ—¦K1,k,lhl\njâ—¦E1,k,leijâˆšdk\nif iand jare connected in sparse graph\nQ2,k,lhl\niâ—¦K2,k,lhl\njâ—¦E2,k,leijâˆšdk\notherwise\nï£¼\nï£´ï£´ï£½\nï£´ï£´ï£¾\n(4)\nwk,l\nij =\nï£±\nï£²\nï£³\n1\n1+Î³ Â·softmax(âˆ‘\ndk\nË†wk,l\nij ) if iand jare connected in sparse graph\nÎ³\n1+Î³ Â·softmax(âˆ‘\ndk\nË†wk,l\nij ) otherwise\nï£¼\nï£½\nï£¾ (5)\nwhere â—¦denotes element-wise multiplication and Q1,k,l, Q2,k,l, K1,k,l, K2,k,l, E1,k,l, E2,k,l âˆˆ\nRdkÃ—d. Î³ âˆˆR+ is a hyperparameter which tunes the amount of bias towards full-graph attention,\nallowing ï¬‚exibility of the model to different datasets and tasks where the necessity to capture long-\nrange dependencies may vary. Note that softmax outputs are clamped betweenâˆ’5 and 5 for numerical\nstability and that the keys, queries and edge projections are different for pairs of connected nodes\n(Q1,K1,E1) and disconnected nodes (Q2,K2,E2).\nA multi-layer perceptron (MLP) with residual connections and normalization layers are then applied\nto update representations, in the same fashion as the GT method [14].\nË†Ë†hl+1 = Norm(hl\ni + Ë†hl+1\ni ), Ë†Ë†Ë†hl+1\ni = Wl\n2ReLU(Wl\n1\nË†Ë†hl+1\ni ), hl+1\ni = Norm(Ë†Ë†hl+1 + Ë†Ë†Ë†hl+1\ni ) (6)\nwith the weight matrices Wl\n1 âˆˆR2dÃ—d, Wl\n2 âˆˆRdÃ—2d. Edge representations are not updated as it\nadds complexity with little to no performance gain. Bias terms are omitted for presentation.\n3.4 Limitations\nThe ï¬rst limitation of the node-wise LPE, and noted in Table 1 is the lack of sign invariance of the\nmodel. A random sign-ï¬‚ip of an eigenvector can produce different outputs for the LPE, meaning\n7\nthat the model needs to learn a representation invariant to these ï¬‚ips. We resolve this issue with the\nedge-wise LPE proposed in 3.2, but it comes at a computational cost.\nAnother limitation of the approach is the computational complexity of the LPE being O(m2N), or\nO(N3) if considering all eigenfunctions. Further, as nodes are batched in the LPE, the total memory\non the GPU will be num_params * num_nodes_in_batch instead of num_params * batch_size .\nAlthough this is limiting, the LPE is not parameter hungry, with kusually kept around 16. Most of\nthe modelâ€™s parameters are in theMain Graph Transformer of complexity O(N2).\nDespite Transformers having increased complexity, they managed to revolutionalize the NLP com-\nmunity. We argue that to shift away from the message-passing paradigm and generalize Transformers\nto graphs, it is natural to expect higher computational complexities. This is exacerbated by sequences\nbeing much simpler to understand than graphs due to their linear structure. Future work could\novercome this by using variations of Transformers that scale linearly or logarithmically [34].\n3.5 Theoretical properties of the architecture\nDue to the full connectivity, it is trivial that our model does not suffer from the same limitations in\nexpressivity as its convolutional/message-passing counterpart.\nWL test and universality. The DGN paper [4] showed that using the eigenvector Ï†1 is enough to\ndistinguish some non-isomorphic graphs indistinguishable by the 1-WL test.\nGiven that our model uses the full set of eigenfunctions, and given enough parameters, our model can\ndistinguish any pair of non-isomorphic graphs and is more powerful than any WL test in that regard.\nHowever, this does not solve the graph isomorphism problem in polynomial time; it only approximates\na solution, and the number of parameters required is unknown and possibly non-polynomial. In\nappendix C, we present a proof of our statement, and discuss why the WL test is not well suited to\nstudy the expressivity of graph Transformers due to their universality.\nReduced over-squashing. Over-squashing represents the difï¬culty of a graph neural network to pass\ninformation to distant neighbours due to the exponential blow-up in computational paths [1].\nFor the fully-connected network, it is trivial to see that over-squashing is non-existent since there are\ndirect paths between distant nodes.\nPhysical interactions. Another point to consider is the ability of the network to learn physical\ninteractions between nodes. This is especially important when the graph models physical, chemical,\nor biological structures, but can also help understanding pixel interaction in images [2, 3]. Here, we\nargue that our SAN model, which uses the Laplace spectrum more effectively, can learn to mimic\nthe physical interactions presented in section 2.1.2. This contrasts with the convolutional approach\nthat requires deep layers for the receptive ï¬eld to capture long-distance interactions. It also contrasts\nwith the GT model [14], which does not use eigenvalues or enough eigenfunctions to properly model\nphysical interactions in early layers. However, due to the lack of sign-invariance in the proposed\nnode-wise LPE, it is difï¬cult to learn these interactions accurately. The edge-wise LPE (section 3.2)\ncould be better suited for the problem, but it suffers from higher computational complexity.\n4 Experimental Results\nThe model is implemented in PyTorch [31] and DGL [38] and tested on established benchmarks from\n[15] and [21] provided under MIT license. Speciï¬cally, we applied our method on ZINC, PATTERN,\nCLUSTER, MolHIV and MolPCBA, while following their respective training protocols with minor\nchanges, as detailed in the appendix B.1. The computation time and hardware is provided in appendix\nB.4.\nWe ï¬rst conducted an ablation study to fairly compare the beneï¬ts of using full attention and/or the\nnode LPE. We then took the best-performing model, tuned some of its hyperparameters, and matched\nit up against the current state-of-the-art methods. Since we use a similar attention mechanism, our\ncode was developed on top of the code from the GT paper [14], provided under the MIT license.\n8\nFigure 5: Effect of the Î³parameter on the performance across datasets from [15, 21], using the Node\nLPE. Dotted black lines indicate sparse attention, which is equivalent to setting Î³ = 0. Each box plot\nconsists of 4 runs, with different seeds (except MolHIV).\nModel details ZINC PATTERN CLUSTER MOLHIV\nAttention LPE MAE % ACC % ACC % ROC-AUC\nSparse - \t0.267 Â± 0.032 83.613 Â± 0.663 75.683 Â± 0.098 73.46 Â± 0.71\nSparse Node 0.198 Â± 0.004 81.329 Â± 2.150 75.738 Â± 0.106 76.61 Â± 0.62\nFull - 0.392 Â± 0.055 86.322 Â± 0.049 76.447 Â± 0.177 73.84 Â± 1.80\nFull Node ğŸ.ğŸğŸ“ğŸ• Â± ğŸ. ğŸğŸğŸ” ğŸ–ğŸ”. ğŸ’ğŸ’ğŸ Â± ğŸ. ğŸğŸ’ğŸ ğŸ•ğŸ”. ğŸ”ğŸ—ğŸ Â± ğŸ. ğŸğŸ’ğŸ• ğŸ•ğŸ•.ğŸ“ğŸ• Â± ğŸ. ğŸ”ğŸ\nBest\nWorst\nFigure 6: Ablation study on datasets from [ 15, 21] for the node LPE and full graph attention, with\nno hyperparameter tuning other than Î³taken from Figure 5. For a given dataset, all models use the\nsame hyperparameters, but the hidden dimensions are adjusted to have âˆ¼500klearnable parameters.\nMeans and uncertainties are derived from four runs, with different seeds (except MolHIV).\n4.1 Sparse vs. Full Attention\nTo study the effect of incorporating full attention, we present an ablation study of the Î³parameter in\nFigure 5. We remind readers that Î³is used in equation 5 to balance between sparse and full attention.\nSetting Î³ = 0strictly enables sparse attention, while Î³ = 1does not bias the model in any direction.\nIt is apparent that molecular datasets, namely ZINC and MOLHIV , beneï¬t less from full attention,\nwith the best parameter being log Î³ âˆˆ (âˆ’7,âˆ’5). On the other hand, the larger SBM datasets\n(PATTERN and CLUSTER) beneï¬t from a higher Î³value. This can be explained by the fact that\nmolecular graphs rely more on understanding local structures such as the presence of rings and\nspeciï¬c bonds, especially in the artiï¬cial task from ZINC which relies on counting these speciï¬c\npatterns [15]. Furthermore, molecules are generally smaller than SBMs. As a result, we would\nexpect less need for full attention, as information between distant nodes can be propagated with\nfew iterations of even sparse attention. We also expect molecules to have fewer multiplicities, thus\nreducing the space of eigenvectors. Lastly, the performance gains in using full attention on the\nCLUSTER dataset can be attributed to it being a semi-supervised task, where some nodes within\neach graph are assigned their true labels. With full attention, every node receives information from\nthe labeled nodes at each iteration, reinforcing conï¬dence about the community they belong to.\nIn Figure 6, we present another ablation study to measure the impact of the node LPE in both the\nsparse and full architectures. We observe that the proposed node-wise LPE contributes signiï¬cantly\nto the performance for molecular tasks (ZINC and MOLHIV), and believe that it can be attributed\nto the detection of substructures (see Figure 3). For PATTERN and CLUSTER, the improvement is\nmodest as the tasks are simple clustering [15]. Previous work even found that the optimal number of\neigenvectors to construct PE for PATTERN is only 2 [14].\n4.2 Comparison to the state-of-the-art\nWhen comparing to the state-of-the-art (SOTA) models in the literature in Figure 7, we observe that\nour SAN model consistently performs better on all synthetic datasets from [ 15], highlighting the\nstrong expressive power of the model. On the MolHIV dataset, the performance on the test set is\nslightly lower than the SOTA. However, the model performs better on the validation set (85.30%)\nin comparison to PNA (84.25%) and DGN (84.70%). This can be attributed to a well-known issue\nwith this dataset: the validation and test metrics have low correlation. In our experiments, we found\nhigher test results with lower validation scores when restricting the number of epochs. Here, we also\nincluded results on the MolPCBA dataset, where we witnessed competitive results as well.\n9\nOther top-performing models, namely PNA [11] and DGN [4], use a message-passing approach [17]\nwith multiple aggregators. When compared to attention-based models, SAN consistently outperforms\nthe SOTA by a wide margin. To the best of our knowledge, SAN is the ï¬rst fully-connected model to\nperform well on graph tasks, as is evident by the poor performance of the GT (full) model.\nColumn1ZINCPATTERNCLUSTERMOLHIVMOLPCBAModelMAE% Acc% Acc% ROC-AUC% APGCN0.367\tÂ±0.01171.892\tÂ±0.33468.498\tÂ±0.97676.06\tÂ±0.9720.20\tÂ±0.24GraphSage0.398\tÂ±0.00250.492\tÂ±0.00163.844\tÂ±0.110- -GatedGCN0.282\tÂ±0.01585.568\tÂ±0.08873.840\tÂ±0.326- -GatedGCN-PE0.214\tÂ±0.01386.508\tÂ±0.08576.082\tÂ±0.196GIN0.526\tÂ±0.01385.387\tÂ±0.13664.716\tÂ±1.55375.58\tÂ±1.4022.66\tÂ±0.28PNA0.142\tÂ±0.010 - - 79.05\tÂ±1.3228.38\tÂ±0.35DGN - - - ğŸ•ğŸ—.ğŸ•ğŸ\tÂ±ğŸ.ğŸ—ğŸ•ğŸğŸ–.ğŸ–ğŸ“\tÂ±ğŸ.ğŸ‘ğŸAttention-basedGAT0.384\tÂ±0.00778.271\tÂ±0.18670.587\tÂ±0.447- -GT (sparse)0.226\tÂ±0.01484.808\tÂ±0.06873.169\tÂ±0.662- -GT (full)0.598\tÂ±0.04956.482\tÂ±3.54927.121\tÂ±8.471- -SANğŸ.ğŸğŸ‘ğŸ—\tÂ±ğŸ.ğŸğŸğŸ”ğŸ–ğŸ”.ğŸ“ğŸ–ğŸ\tÂ±ğŸ.ğŸğŸ‘ğŸ•ğŸ•ğŸ”.ğŸ”ğŸ—ğŸ\tÂ±ğŸ.ğŸ”ğŸ“77.85\tÂ±0.247\t 27.65\tÂ±0.42\nBest\nWorst\nFigure 7: Comparing our tuned model on datasets from [15, 21], against GCN [25], GraphSage [18],\nGIN [39], GAT [37], GatedGCN [5], PNA [11], and DGN [4]. Means and uncertainties are derived\nfrom four runs with different seeds, except MolHIV which uses 10 runs with identical seed. The\nnumber of parameters is ï¬xed to âˆ¼500kfor ZINC, PATTERN and CLUSTER.\n5 Conclusion\nIn summary, we presented the SAN model for graph neural networks, a new Transformer-based\narchitecture that is aware of the Laplace spectrum of a given graph from the learned positional\nencodings. The model was shown to perform on par or better than the SOTA on multiple benchmarks\nand outperforms other Attention-based models by a large margin. As is often the case with Trans-\nformers, the current model suffers from a computational bottleneck, and we leave it for future work\nto implement variations of Transformers that scale linearly or logarithmically. This will enable the\nedge-wise LPE presented in appendix A, a theoretically more powerful version of the SAN model.\nSocietal Impact. The presented work is focused on theoretical and methodological improvements\nto graph neural networks, so there are limited direct societal impacts. However, indirect negative\nimpacts could be caused by malicious applications developed using the algorithm. One such example\nis the tracking of people on social media by representing their interaction as graphs, thus predicting\nand inï¬‚uencing their behavior towards an external goal. It also has an environmental impact due\nto the greater energy use that arises from the computational cost O(m2N + N2) being larger than\nstandard message passing or convolutional approaches of O(E).\nFunding Disclosure.\nDevin Kreuzer is supported by an NSERC grant.\nReferences\n[1] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical\nimplications. arXiv:2006.05205 [cs, stat], 2020.\n[2] Dominique Beaini, Soï¬ane Achiche, Alexandre DuperrÃ©, and Maxime Raison. Deep green\nfunction convolution for improving saliency in convolutional neural networks. The Visual\nComputer, 37(2):227â€“244, 2020.\n[3] Dominique Beaini, Soï¬ane Achiche, and Maxime Raison. Improving convolutional neural\nnetworks via conservative ï¬eld regularisation and integration.\n10\n[4] Dominique Beaini, Saro Passaro, Vincent LÃ©tourneau, William L. Hamilton, Gabriele Corso,\nand Pietro LiÃ². Directional graph networks. ICML2021, 2021.\n[5] Xavier Bresson and Thomas Laurent. Residual gated graph convnets. arXiv preprint\narXiv:1711.07553, 2017.\n[6] Michael M. Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst.\nGeometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine,\n34(4):18â€“42, 2017.\n[7] F. Cajori. A History of Mathematics. AMS Chelsea Publishing Series. AMS Chelsea, 1999.\n[8] Heng Chang, Yu Rong, Tingyang Xu, Wenbing Huang, Somayeh Sojoudi, Junzhou Huang, and\nWenwu Zhu. Spectral graph attention network. CoRR, abs/2003.07450, 2020.\n[9] Fan Chung and S. T. Yau. Discrete greenâ€™s functions.Journal of Combinatorial Theory, Series\nA, 91(1):191â€“214, 2000.\n[10] Ronald R. Coifman and StÃ©phane Lafon. Diffusion maps.Applied and Computational Harmonic\nAnalysis, 21(1):5â€“30, 2006. Special Issue: Diffusion Maps and Wavelets.\n[11] Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro LiÃ², and Petar VeliË‡ckoviÂ´c. Principal\nneighbourhood aggregation for graph nets. arXiv preprint arXiv:2004.05718, 2020.\n[12] Luca Cosmo, Mikhail Panine, Arianna Rampini, Maks Ovsjanikov, Michael M. Bronstein, and\nEmanuele Rodola. Isospectralization, or how to hear shape, style, and correspondence. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),\nJune 2019.\n[13] Yushun Dong, Kaize Ding, Brian Jalaian, Shuiwang Ji, and Jundong Li. Graph neural networks\nwith adaptive frequency response ï¬lter. CoRR, abs/2104.12840, 2021.\n[14] Vijay Prakash Dwivedi and Xavier Bresson. A generalization of transformer networks to graphs,\n2020.\n[15] Vijay Prakash Dwivedi, Chaitanya K Joshi, Thomas Laurent, Yoshua Bengio, and Xavier\nBresson. Benchmarking graph neural networks. arXiv preprint arXiv:2003.00982, 2020.\n[16] Richard Phillips Feynman, Robert Benjamin Leighton, and Matthew Sands. The Feynman\nlectures on physics; New millennium ed. Basic Books, New York, NY , 2010. Originally\npublished 1963-1965.\n[17] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural\nmessage passing for quantum chemistry. In Proceedings of the 34th International Conference\non Machine Learning-Volume 70, pages 1263â€“1272. JMLR. org, 2017.\n[18] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large\ngraphs. In Advances in neural information processing systems, pages 1024â€“1034, 2017.\n[19] William L. Hamilton. Graph Representation Learning. Morgan and Claypool, 2020.\n[20] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):1735â€“1780, 1997.\n[21] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele\nCatasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs.\narXiv preprint arXiv:2005.00687, 2020.\n[22] Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for\nmolecular graph generation. arXiv:1802.04364 [cs, stat], 2018.\n[23] Mark Kac. Can one hear the shape of a drum? The American Mathematical Monthly, 73(4):1,\n1966.\n11\n[24] Anees Kazi, Luca Cosmo, Nassir Navab, and Michael Bronstein. Differentiable graph module\n(dgm) graph convolutional networks. arXiv preprint arXiv:2002.04999, 2020.\n[25] Thomas N Kipf and Max Welling. Semi-supervised classiï¬cation with graph convolutional\nnetworks. arXiv preprint arXiv:1609.02907, 2016.\n[26] B. Levy. Laplace-beltrami eigenfunctions towards an algorithm that \"understands\" geometry.\nIn IEEE International Conference on Shape Modeling and Applications 2006 (SMIâ€™06), pages\n13â€“13, 2006.\n[27] Pan Li, Yanbang Wang, Hongwei Wang, and Jure Leskovec. Distance encoding: Design\nprovably more powerful neural networks for graph representation learning, 2020.\n[28] Yaron Lipman, Raif M. Rustamov, and Thomas A. Funkhouser. Biharmonic distance. ACM\nTrans. Graph., 29(3), July 2010.\n[29] Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful\ngraph networks. arXiv preprint arXiv:1905.11136, 2019.\n[30] Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen,\nGaurav Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural\nnetworks. In Proceedings of the AAAI Conference on Artiï¬cial Intelligence, volume 33, pages\n4602â€“4609, 2019.\n[31] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,\nZeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in\npytorch. 2017.\n[32] Emanuele RodolÃ , Luca Cosmo, Michael M. Bronstein, Andrea Torsello, and Daniel Cremers.\nPartial functional correspondence, 2015.\n[33] Joshua Schrier. Can one hear the shape of a molecule (from its coulomb matrix eigenvalues)?\nJournal of Chemical Information and Modeling, 60(8):3804â€“3811, 2020. Publisher: American\nChemical Society.\n[34] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efï¬cient transformers: A survey,\n2020.\n[35] Edwin R. van Dam and Willem H. Haemers. Which graphs are determined by their spectrum?\nLinear Algebra and its Applications, 373:241â€“272, 2003.\n[36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. CoRR, abs/1706.03762, 2017.\n[37] Petar VeliË‡ckoviÂ´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua\nBengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.\n[38] Minjie Wang, Da Zheng, Zihao Ye, Quan Gan, Mufei Li, Xiang Song, Jinjing Zhou, Chao Ma,\nLingfan Yu, Yu Gai, Tianjun Xiao, Tong He, George Karypis, Jinyang Li, and Zheng Zhang.\nDeep graph library: A graph-centric, highly-performant package for graph neural networks.\narXiv preprint arXiv:1909.01315, 2019.\n[39] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural\nnetworks? arXiv preprint arXiv:1810.00826, 2018.\n[40] Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J. Reddi, and Sanjiv Kumar.\nAre transformers universal approximators of sequence-to-sequence functions?, 2020.\n[41] Chulhee Yun, Yin-Wen Chang, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J. Reddi,\nand Sanjiv Kumar. $o(n)$ connections are expressive enough: Universal approximability of\nsparse transformers. CoRR, abs/2006.04862, 2020.\n[42] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J. Kim. Graph\ntransformer networks, 2020.\n12\nA LPE Transformer Over Edges\nConsider one of the most fundamental notions in physics; Potential energy. Interestingly, potential\nenergy is always measured as a potential difference; it is not an inherent individual property, such as\nmass. Strikingly, it is also the relative Laplace embeddings of two nodes that paint the picture, as a\nnodeâ€™s Laplace embedding on its own reveals no information at all. With this in mind, we argue that\nLaplace positional encodings are more naturally represented as edge features, which encode a notion\nof relative position of the two endpoints in the graph. This can be viewed as a distance encoding,\nwhich was shown to improve the performance of node and link prediction in GNNs [27].\nThe formulation is very similar to the method for learning positional node embeddings. Here, a\nTransformer Encoder is applied on each graph by treating edges as a batch of variable size and\neigenvectors as a variable sequence length. We again compute up to the m-lowest eigenvectors with\ntheir eigenvalues but, instead of directly using the eigenvector elements, we compute the following\nvectors:\n|Ï†:,j1 âˆ’Ï†:,j2 | (7) Ï†:,j1 â—¦Ï†:,j2 (8)\nwhere â€œ:â€ denotes along all up to meigenvectors, and â—¦denotes element-wise multiplication. Note\nthat these new vectors are completely invariant to sign permutations of the precomputed eigenvectors.\nAs per the LPE over nodes, the 3-length vectors are expanded with a linear layer to generate\nembeddings of size kbefore being input to the Transformer Encoder. The ï¬nal embeddings are then\npassed to a sum pooling layer to generate ï¬xed-size edge positional encodings, which are then used\nto compute attention weights in equation 4.\nThis method addresses all etiquettesraised in section 2.2. However, it suffers from a major computa-\ntional bottleneck compared to the LPE over nodes. Indeed, for a fully-connected graph, there are N\ntimes more edges than nodes, thus the computation complexity is O(m2N2), or O(N4) considering\nall eigenfunctions. This same limitation also affects the memory, as efï¬ciently batching the N2 edges\nwill increase the memory consumption of the LPE by a drastic amount, preventing the model from\nusing large batch sizes and making it difï¬cult to train.\nFigure 8: Edge-wise Learned positional encoding (LPE) architectures, where the relative position is\nconsidered instead of the absolute position. The model is aware of the graphâ€™s Laplace spectrum by\nconsidering meigenvalues and eigenvectors, where we permit mâ‰¤N, with N denoting the number\nof nodes. Since the Transformer loops over the edges, each edge can be viewed as an element of a\nbatch to parallelize the computation. The computational complexity is O(m2E) or O(m2N2) for a\nfully-connected graph.\nB Appendix - Implementation details\nB.1 Benchmarks and datasets\nTo test our modelsâ€™ performance, we rely on standard benchmarks proposed by [15] and [21] and\nprovided under the MIT license. In particular, we chose ZINC, PATTERN, CLUSTER, and MolHIV .\nZINC [15]. A synthetic molecular graph regression dataset, where the predicted score is given by\nthe subtraction of computationally estimated properties logP âˆ’SA. Here, logP is the computed\noctanol-water partition coefï¬cient, and SAis the synthetic accessibility score [22].\nCLUSTER [15]. A synthetic benchmark for node classiï¬cation. The graphs are generated with\nStochastic Block Models, a type of graph used to model communities in social networks. In total, 6\ncommunities are generated and each community has a single node with its true label assigned. The\ntask is to classify which nodes belong to the same community.\n13\nPATTERN [15]. A synthetic benchmark for node classiï¬cation. The graphs are generated with\nStochastic Block Models, a type of graph used to model communities in social networks. The task\nis to classify the nodes into 2 communities, testing the GNNs ability to recognize predetermined\nsubgraphs.\nMolHIV [21]. A real-world molecular graph classiï¬cation benchmark. The task is to predict whether\na molecule inhibits HIV replication or not. The molecules in the training, validation, and test sets are\ndivided using a scaffold splitting procedure that splits the molecules based on their two-dimensional\nstructural frameworks. The dataset is heavily imbalanced towards negative samples. It is also known\nthat this dataset suffers from a strong de-correlation between validation and test set performance,\nmeaning that more hyperparameter ï¬ne-tuning on the validation set often leads to lower test set\nresults.\nMolPCBA [21]. Another real-world molecular graph classiï¬cation benchmark. The dataset is larger\nthan MolHIV and applies a similar scaffold spliting procedure. It consists of multiple, extremely\nskewed (only 1.4% positivity) molecular classiï¬cation tasks, and employs Average Precision (AP)\nover them as a metric.\nB.2 Ablation studies\nThe results in Figures 5-6 are done as an ablation study with a minimal tuning of the hyperparam-\neters of the network to measure the impact of the node LPE and full attention. A majority of the\nhyperparameters used were tuned in previous work [15]. However, we altered some of the existing\nparameters to accommodate the parameter-heavy LPE, and modiï¬ed the Main Graph Transformer\nhidden dimension such that all models have approximately âˆ¼ 500kparameters for a fair comparison.\nWe present results for the full attention with the optimal Î³value optimized on the Node LPE model.\nWe did this to isolate the impact that the Node LPE has on improving full attention. Details concerning\nthe model architecture parameters are visible in Figure 9.\nAttention LPE LPE layers LPE dimension GT layers GT hidden dimension #Parameters\nâ™°Sparse - - - 6 96 511201\nSparse Node 3 16 6 72 494865\nFull - - - 6 80 471361\nFull Node 3 16 6 64 508577\nSparse - - - 6 96 508634\nSparse Node 3 16 6 72 493340\nFull - - - 6 80 469142\nFull Node 3 16 6 64 507202\nSparse - - - 16 56 461348\nSparse Node 1 16 16 56 530036\nFull - - - 16 48 450498\nFull Node 1 16 16 48 519186\nSparse - - - 6 96 525985\nSparse Node 2 16 6 80 503265\nFull - - - 6 80 483601\nâ™°Full Node 2 16 6 72 528265\nZINC\nCLUSTER\nMOLHIV\nPATTERN\nFigure 9: Model architecture parameters for the ablation study. We modify the hidden dimensions of\nthe Main Graph Transformer (GT) such that all models haveâˆ¼ 500kparameters for a fair comparison.\nâ€ The batch size was doubled to ensure convergence of the model. All other parameters outside the\nGT hidden dimension are consistent within a dataset experiment.\nFor the training parameters, we employed an Adam optimizer with a learning rate decay strategy\ninitialized in {10âˆ’3,10âˆ’4}as per [15], with some minor modiï¬cations:\nZINC [15]. We selected an initial learning rate of 7 Ã—10âˆ’4 and increased the patience from 10\nto 25 to ensure convergence. PATTERN [15]. We selected an initial learning rate of 5 Ã—10âˆ’4.\nCLUSTER [15]. We selected an initial learning rate of 5 Ã—10âˆ’4 and reduced the minimum learning\nrate from 10âˆ’6 to 10âˆ’5 to speed up training time. MolHIV [21]. We elected to use similar training\n14\nprocedures for consistency. We selected an initial learning rate of 10âˆ’4, a reduce factor of 0.5, a\npatience of 20, a minimum learning rate of 10âˆ’5, a weight decay of 0 and a dropout of 0.03.\nB.3 SOTA Comparison study\nFor the results in Figure 7, we tuned some of the hyperparameters, using the following strategies.\nThe optimal parameters are in bold.\nZINC. Due to the 500kparameter budget, we tuned the pairing {GT layers, GT hidden dimension} âˆˆ\n{{6,72},{8,64},{10,56}}and readout âˆˆ{\"mean\",\"sum\"}PATTERN. Due to the500kparameter\nbudget and long training times, we only tuned the pairing { GT layers, GT hidden dimension }\nâˆˆ{{4,80},{6,64}}CLUSTER. Due to the 500k parameter budget and long training times, we\nonly tuned the pairing {GT layers, GT hidden dimension} âˆˆ{{12,64},{16,48}}MolHIV. With no\nparameter budget, we elected to do a more extensive parameter tuning in a two-step process while\nmeasuring validation metrics on 3 runs with identical seeds.\n1. We tuned LPE dimension âˆˆ {8,16}, GT layers âˆˆ {4,6,8,10}, GT hidden dimension\nâˆˆ{48,64,72,80,96}\n2. With the highest performing validation model from step 1, we then tuned dropout\nâˆˆ{0,0.01,0.025}and weight decay âˆˆ{0,10âˆ’6,10âˆ’5}\nWith the ï¬nal optimized parameters, we reran 10 experiments with identical seeds.\nMolPCBA. With no parameter budget, we elected to do a more extensive parameter tuning as\nwell. We tuned learning rate âˆˆ{0.0001,0.0003,0.0005}, dropout âˆˆ{0,0.1,0.2,0.3,0.4,0.5}, GT\nlayers âˆˆ{2,4,5,6,8,10,12}, GT layers âˆˆ{128,256,304,512}, LPE layers âˆˆ{8,10,12}amd LPE\ndimension âˆˆ{8,16}\nB.4 Computation details\nDataset Resource Cluster GPU Epoch/Total time\nZINC Compute Canada Graham Tesla P100-PCIE (12 GB) 106s/17.88hrs\nPATTERN Compute Canada Graham Tesla P100-PCIE (12 GB) 340s/12.52hrs\nCLUSTER Compute Canada Beluga Tesla V100-SXM2 (16 GB) 433s/11.30hrs\nMOLHIV Compute Canada Cedar Tesla V100-SXM2 (32 GB) 204s/5.34hrs\nMOLPCBA Compute Canada Cedar Tesla V100-SXM2 (32 GB) 883s/48.02hrs\nFigure 10: Computational details for SOTA Comparison study.\nC Expressivity and complexity analysis of graph Transformers\nIn this section, we discuss how the universality of Transformers translates to graphs when using\ndifferent node identiï¬ers. Theoretically, this means that by simply labeling each node, Transformers\ncan learn to distinguish any graph, and the WL test is no longer suited to study their expressivity.\nThus, we introduce the notion of learning complexity to better compare each architectureâ€™s ability to\nunderstand the space of isomorphic graphs. We apply the complexity analysis to the LPE and show\nthat it can more easily capture the structure of graphs than a naive Transformer.\nC.1 Universality of Transformers for sequence-to-sequence approximations\nIn recent work [40] [41], it was proven that Transformers are universal sequence-to-sequence approx-\nimators, meaning that they can encode any function that approximately maps any ï¬rst sequence into a\nsecond sequence when given enough parameters. More formally, they proved the following theorems\nfor the universality of Transformers:\nTheorem 1. For any 1 â‰¤p< âˆ, Îµ> 0 and any function f : RdÃ—n â†’RdÃ—n that is equivariant to\npermutations of the columns, there is a Transformer gsuch that the Lp distance between f and gis\nsmaller than Îµ.\n15\nLet Bn be the n-dimensional closed ball and denote by C0(BdÃ—n,RdÃ—n) the set of all continuous\nfunctions of the ball to RdÃ—n. A Transformer with positional encoding gp is a Transformer gsuch\nthat to each input X, a ï¬xed learned positional encoding Eis added such that gp(X) =g(X+ E).\nTheorem 2. For any 1 â‰¤p <âˆ, Îµ >0 and any function f âˆˆC0(BdÃ—n,RdÃ—n), there is a\nTransformer with positional encoding gsuch that the Lp distance between f and gis smaller than Îµ.\nC.2 Graph Transformers approximate solutions to the graph isomorphism problem\nWe now explore the consequences of the previous 2 theorems on the use of Transformers for graph\nrepresentation learning. We ï¬rst describe 2 types of Transformers on graphs; one for node and one\nfor edge inputs. They will be used to deduce corollaries of theorems 1 and 2 for graph learning\nand later comparison with our proposed architecture. Assume now that all nodes of the graphs we\nconsider are given an integer label in {1,...,N }.\nThe naive edge transformer takes as input a graph represented as a sequence of ordered\npairs ((i,j),Ïƒi,j) with i â‰¤ j the indices of 2 vertices and Ïƒi,j equal to 1 or 0 if the vertices\ni,j are connected or not. Recall there are N(N âˆ’1)/2 pairs of integers i,j in {1,...,N }with\ni < jthe indices of 2 vertices and Ïƒi,j equal to 1 or 0 if the vertices i,j are connected or not.\nIt is obvious that any ordering of these edge vectors describe the same graph. Recall there\nare N(N âˆ’1)/2 pairs of integers i,j in {1,...,N }with i â‰¤j. Consider the set of functions\nf : RN(Nâˆ’1)/2Ã—2 â†’RN(Nâˆ’1)/2Ã—2 that are equivariant to the permutations of columns then theo-\nrem 1 says the functionfcan be approximated with arbitrary accuracy by Transformers on edge input.\nThe naive node Transformer can be deï¬ned as a Transformer with positional encodings.\nThis graph Transformer will take as input the identity matrix and as positional encodings the padded\nadjacency matrix. This can be viewed as a one-hot encoding of each nodeâ€™s neighbors. Consider\nthe set of continuous functions f : RNÃ—N â†’RNÃ—N, then theorem 2 says the function f can be\napproximated with arbitrary accuracy by Transformers on node inputs.\nFrom these two observations on the universality of graph Transformers, we get as a corol-\nlary that these 2 types of Transformers can approximate solutions of the graph isomorphism\nproblem. In each case, pick a function that is invariant under node index permutations and maps\nnon-isomorphic graphs to different values and apply theorem 1 or 2 that shows there is a Transformer\napproximating that function to an arbitrarily small error in the Lp distance. This is an interesting fact\nsince it is known that the discrimination power of most message passing graph networks is upper\nbounded by the Weisfeiler-Lehman test which is unable to distinguish some graphs.\nThis may seem strange since it is unlikely there is an algorithm solving the graph isomorphism\nproblem in polynomial time to the number of nodes N, and we address this issue in the notes below.\nNote 1: Only an approximate solution. The universality theorems do not state that Transformers\nsolve the isomorphism problem, but that they can approximate a solution. They only learn the\ninvariant functions only up to some error so they still can mislabel graphs.\nNote 2: Estimate of number of Transformer blocks. For the approximation of the function f by a\nTransformer to be precise, a large number of Transformer blocks will be needed. In [40], it is stated\nthat the universal class of function is obtained by composing Transformer blocks with 2 heads of\nsize 1 followed by a feed-forward layer with 4 hidden nodes. In [41] section 4.1, an estimate of the\nnumber of blocks is given. If f : RdÃ—n â†’RdÃ—n is L-Lipschitz, then ||f(X) âˆ’f(Y)||<Îµ/2 when\n||Xâˆ’Y||<Îµ/2L= Î´. In the notation of [41], the LPE has constants p= 2and s= 1. If gis a\ncomposition of Transformer blocks then an error ||f âˆ’g||Lp <Îµ can be achieved with a number of\nTransformer blocks larger than\n(dn\nÎ´\n)\n+\n(p(nâˆ’1)\nÎ´d + s\n)\n+\n( n\nÎ´dn\n)\n= dn2L\nÎµ + 2(nâˆ’1)(2L)d\nÎµd + 1 +n(2L)dn\nÎµdn\nIn the case of the node encoder described above, n= d= N (the number of nodes) and the last term\nin the sum above becomes N(2L/Îµ)N2\n, so the number of parameters and therefore the computational\ntime is exponential in the number of nodes for a ï¬xed error Îµ. Note that this bound on the number of\nTransformer blocks might not be tight and might be much lower for a speciï¬c problem.\n16\nNote 3: Learning invariance to label permutations. In the above proof, the Transformer is\nassumed to be able to label all isomorphic graphs into the same class within a small error. However,\ngiven a graph of N nodes, there are N! different node labeling permutations, and they all need to be\nmapped to the same output class. It seems unlikely that such function can be learned with polynomial\ncomplexity to N.\nFollowing these observations, it does not seem appropriate to compare Transformers to the WL test\nas is the custom for graph neural networks and we think at this point we should seek a new measure\nof expressiveness of graph Transformers.\nC.3 Expressivity of the node-LPE\nHere, we want to show that the proposed node-LPE can generate a unique node identiï¬er that allows\nour Transformer model to be a universal approximator on graphs, thus allowing us to approximate a\nsolution to graph isomorphism.\nRecall the node LPE takes as input an N Ã—mÃ—2 tensor with mthe number of eigenvalues and\neigenvectors that are used to represent the nodes. The output is a N Ã—k tensor. Notice that 2\nnon-isomorphic graphs on N nodes can have the same m < Neigenvalues and eigenspaces and\ndisagree on the last N âˆ’meigenvalues and eigenspaces. Any learning algorithm missing the last\nNâˆ’mpieces of information wonâ€™t be able to distinguish these graphs. Here we will ï¬x somemand\nshow that the resulting Transformer can approximately classify all graphs with N â‰¤m.\nFix some linear injection M : RNÃ—2Ã—m â†’RNÃ—kÃ—m. Let Gbe a graph and U âŠ‚RNÃ—2Ã—m be\nbounded set containing all the tensor representations of graphs TG and let Rbe the radius of a ball\ncontaining M(U). Consider the set C0(BNÃ—kÃ—m\nR ,RNÃ—kÃ—m) of continuous functions of the closed\nradius Rball in RNÃ—kÃ—m. Finally, denote by S: RNÃ—kÃ—m â†’RNÃ—k the linear function taking the\nsum of all values in the mdimension. The following universality result for LPE Transformers is a\ndirect consequence of theorem 2.\nProposition 1. For any 1 â‰¤p< âˆ, Îµ> 0 and any continuous function F : BNÃ—kÃ—m\nR â†’RNÃ—k,\nthere is an LPE Transformer gsuch that the Lp distance between M â—¦f â—¦Sand gis smaller than Îµ.\nAs a corollary, we get the same kind of approximation to solutions of the graph isomorphism problem\nas with the naive Transformers. Let f be a function of C0(BNÃ—kÃ—m\nR ,RNÃ—kÃ—m) that maps M(TG)\nto a value that is only dependent of the isomorphism class of the graph and assigns different values to\ndifferent isomorphism classes. We can further assume that f takes values that are 0 for all but one\ncoordinate in the kdimension. The same type of argument is possible for the edge-LPE from ï¬gure 8.\nC.4 Comparison of the learning complexity of naive graph Transformers and LPE\nWe now argue that while the LPE Transformer and the naive graph Transformers of section C.2\ncan all approximate a function f solution of the graph isomorphism problem, the complexity of the\nlearning problem of the LPE is much lower since the spaces it has to learn are simpler.\nNaive Node Transformer. First recall that the naive node Transformer learns a map f : RN2\nâ†’\nRN2\n. In this situation, each graph is represented by N! different matrices which all have to be identi-\nï¬ed by the Transformer. This encoding also does not provide any high-level structural information\nabout the graph.\nNaive Edge Transformer. The naive edge Transformer has the same difï¬culty since the function\nits learning is RN(Nâˆ’1) â†’RN(Nâˆ’1) and the representation of each edge depend on a choice of\nlabeling of the vertices and the N! possible labelings need to be identiï¬ed again.\nNode-LPE Transformer. In the absence of eigenvalues with multiplicity >1, the node LPE that\nlearns a function RNÃ—2Ã—m â†’RNÃ—k does not take as input a representation of the graph that\ndepends on the ordering of the nodes. It does, however, depend on the choice of the sign of each\nof the eigenvectors so there are still 2N possible choices of graph representations that need to be\nidentiï¬ed by the Transformer but this is a big drop in complexity compared to the previous N!. The\neigenfunctions also provide high-level structural information about the graph that can simplify the\nlearning task of the graph.\n17\nEdge-LPE Transformer. Finally, the edge LPE of appendix A uses a graph representation as input\nthat is also independent of the sign choice of the eigenvectors so each graph has a unique representation\n(considering the absence of eigenvalues with multiplicity >1). Again, the eigenfunctions provide\nhigh-level structural information that is not available to the naive Transformer.\nLPE Transformers for non-isospectral graphs. Isospectral graphs are graphs that have the same\nset of eigenvalues despite having different eigenvectors. Here, we argue that the proposed node\nLPE can approximate a solution to the graph isomorphism problem for all pairs of non-isospectral\ngraphs, without having to learn invariance to the sign of their eigenvectors nor their multiplicities.\nBy considering only the eigenvalues in the initial linear layer (assigning a weight of 0 to all Ï†), and\nknowing that the eigenvalues are provided as inputs, the model can effectively learn to replicate the\ninput eigenvalues at its output, thus discriminating between all pairs of non-isospectral graphs. Hence,\nthe problem of learning an invariant mapping to the sign of eigenvectors and multiplicities is limited\nonly to non-isospectral graphs. Knowing that the ratio of isospectral graphs decreases as the number\nof nodes increases (and is believed to tend to 0) [35], this is especially important for large graphs\nand mitigates the problem of having to learn to identify 2N with eigenvectors with different signs.\nIn Figure 11, we present an example of non-isomorphic graphs that can be distinguished by their\neigenvalues but not by the 1-WL test.\nFigure 11: Example of non-isomorphic non-isospectral graphs that can be distinguished by the\neigenvalues of their Laplacian matrix, but not by the 1-WL test.\n18"
}