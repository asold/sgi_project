{
    "title": "Do AIs know what the most important issue is? Using language models to code open-text social survey responses at scale",
    "url": "https://openalex.org/W4391758695",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2599518401",
            "name": "Jonathan Mellon",
            "affiliations": [
                "United States Military Academy"
            ]
        },
        {
            "id": "https://openalex.org/A2127638914",
            "name": "Jack Bailey",
            "affiliations": [
                "University of Manchester"
            ]
        },
        {
            "id": "https://openalex.org/A2101930734",
            "name": "Ralph Scott",
            "affiliations": [
                "University of Bristol"
            ]
        },
        {
            "id": "https://openalex.org/A4315319656",
            "name": "James Breckwoldt",
            "affiliations": [
                "University of Manchester"
            ]
        },
        {
            "id": "https://openalex.org/A4315319657",
            "name": "Marta Miori",
            "affiliations": [
                "University of Manchester"
            ]
        },
        {
            "id": "https://openalex.org/A4266901736",
            "name": "Phillip Schmedeman",
            "affiliations": [
                "United States Military Academy"
            ]
        },
        {
            "id": "https://openalex.org/A2599518401",
            "name": "Jonathan Mellon",
            "affiliations": [
                "United States Military Academy"
            ]
        },
        {
            "id": "https://openalex.org/A2127638914",
            "name": "Jack Bailey",
            "affiliations": [
                "University of Manchester"
            ]
        },
        {
            "id": "https://openalex.org/A2101930734",
            "name": "Ralph Scott",
            "affiliations": [
                "University of Bristol"
            ]
        },
        {
            "id": "https://openalex.org/A4315319656",
            "name": "James Breckwoldt",
            "affiliations": [
                "University of Manchester"
            ]
        },
        {
            "id": "https://openalex.org/A4315319657",
            "name": "Marta Miori",
            "affiliations": [
                "University of Manchester"
            ]
        },
        {
            "id": "https://openalex.org/A4266901736",
            "name": "Phillip Schmedeman",
            "affiliations": [
                "United States Military Academy"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1482979240",
        "https://openalex.org/W2921477399",
        "https://openalex.org/W4225986972",
        "https://openalex.org/W4327811957",
        "https://openalex.org/W1516171683",
        "https://openalex.org/W2919115771",
        "https://openalex.org/W4382769613",
        "https://openalex.org/W2096974619",
        "https://openalex.org/W2028190885",
        "https://openalex.org/W4366277658",
        "https://openalex.org/W4323570543",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3001279689",
        "https://openalex.org/W4389519817",
        "https://openalex.org/W2965373594"
    ],
    "abstract": "Can artificial intelligence accurately label open-text survey responses? We compare the accuracy of six large language models (LLMs) using a few-shot approach, three supervised learning algorithms (SVM, DistilRoBERTa, and a neural network trained on BERT embeddings), and a second human coder on the task of categorizing “most important issue” responses from the British Election Study Internet Panel into 50 categories. For the scenario where a researcher lacks existing training data, the accuracy of the highest-performing LLM (Claude-1.3: 93.9%) neared human performance (94.7%) and exceeded the highest-performing supervised approach trained on 1000 randomly sampled cases (neural network: 93.5%). In a scenario where previous data has been labeled but a researcher wants to label novel text, the best LLM’s (Claude-1.3: 80.9%) few-shot performance is only slightly behind the human (88.6%) and exceeds the best supervised model trained on 576,000 cases (DistilRoBERTa: 77.8%). PaLM-2, Llama-2, and the SVM all performed substantially worse than the best LLMs and supervised models across all metrics and scenarios. Our results suggest that LLMs may allow for greater use of open-ended survey questions in the future.",
    "full_text": null
}