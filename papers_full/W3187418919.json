{
    "title": "A Survey on Vision Transformer",
    "url": "https://openalex.org/W3187418919",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2066495060",
            "name": "Kai Han",
            "affiliations": [
                "Huawei Technologies (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2095972213",
            "name": "Yunhe Wang",
            "affiliations": [
                "Huawei Technologies (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2541514657",
            "name": "Chen Hanting",
            "affiliations": [
                "Huawei Technologies (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2122356358",
            "name": "Xinghao Chen",
            "affiliations": [
                "Huawei Technologies (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2125218951",
            "name": "Jianyuan Guo",
            "affiliations": [
                "Huawei Technologies (China)"
            ]
        },
        {
            "id": "https://openalex.org/A1979633619",
            "name": "Zhenhua Liu",
            "affiliations": [
                "Huawei Technologies (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2765811422",
            "name": "Yehui Tang",
            "affiliations": [
                "Huawei Technologies (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2121560033",
            "name": "An Xiao",
            "affiliations": [
                "Huawei Technologies (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2277635633",
            "name": "Chunjing Xu",
            "affiliations": [
                "Huawei Technologies (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2160802186",
            "name": "Yixing Xu",
            "affiliations": [
                "Huawei Technologies (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2100561381",
            "name": "Zhaohui Yang",
            "affiliations": [
                "Huawei Technologies (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2114647497",
            "name": "Yiman Zhang",
            "affiliations": [
                "Huawei Technologies (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2104129307",
            "name": "Dacheng Tao",
            "affiliations": [
                "University of Sydney"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2948703571",
        "https://openalex.org/W6664172992",
        "https://openalex.org/W3034687522",
        "https://openalex.org/W3119259789",
        "https://openalex.org/W3035029089",
        "https://openalex.org/W3034467781",
        "https://openalex.org/W3043840704",
        "https://openalex.org/W3012126539",
        "https://openalex.org/W3035750285",
        "https://openalex.org/W2740103755",
        "https://openalex.org/W3035022492",
        "https://openalex.org/W6790703111",
        "https://openalex.org/W3171516518",
        "https://openalex.org/W3165924482",
        "https://openalex.org/W3145450063",
        "https://openalex.org/W6795435739",
        "https://openalex.org/W3180355996",
        "https://openalex.org/W3175199633",
        "https://openalex.org/W3092878394",
        "https://openalex.org/W3108516375",
        "https://openalex.org/W6767279747",
        "https://openalex.org/W2981851019",
        "https://openalex.org/W6751979845",
        "https://openalex.org/W3103754749",
        "https://openalex.org/W6769457076",
        "https://openalex.org/W6767211374",
        "https://openalex.org/W6768021236",
        "https://openalex.org/W2962851801",
        "https://openalex.org/W6800633732",
        "https://openalex.org/W4312340826",
        "https://openalex.org/W3095422700",
        "https://openalex.org/W6763509872",
        "https://openalex.org/W3101590291",
        "https://openalex.org/W6790690058",
        "https://openalex.org/W2963351113",
        "https://openalex.org/W1598730426",
        "https://openalex.org/W4241541897",
        "https://openalex.org/W1903029394",
        "https://openalex.org/W2064296540",
        "https://openalex.org/W1783366411",
        "https://openalex.org/W3168649818",
        "https://openalex.org/W6755977528",
        "https://openalex.org/W6780226713",
        "https://openalex.org/W6795704626",
        "https://openalex.org/W3167536469",
        "https://openalex.org/W6788023325",
        "https://openalex.org/W3214586131",
        "https://openalex.org/W3109771530",
        "https://openalex.org/W6769627184",
        "https://openalex.org/W6789977873",
        "https://openalex.org/W3157173860",
        "https://openalex.org/W3203848864",
        "https://openalex.org/W4200498145",
        "https://openalex.org/W6753998590",
        "https://openalex.org/W6788135285",
        "https://openalex.org/W6779248606",
        "https://openalex.org/W6774776664",
        "https://openalex.org/W6767278793",
        "https://openalex.org/W6774901790",
        "https://openalex.org/W6638667902",
        "https://openalex.org/W2963542740",
        "https://openalex.org/W6754905691",
        "https://openalex.org/W6791353385",
        "https://openalex.org/W2963563276",
        "https://openalex.org/W6786361841",
        "https://openalex.org/W4205481600",
        "https://openalex.org/W2752796333",
        "https://openalex.org/W6798351829",
        "https://openalex.org/W4214755140",
        "https://openalex.org/W3111535274",
        "https://openalex.org/W3205586691",
        "https://openalex.org/W6684191040",
        "https://openalex.org/W2112796928",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W4300402905",
        "https://openalex.org/W2413794162",
        "https://openalex.org/W6792496768",
        "https://openalex.org/W6679434410",
        "https://openalex.org/W2963925437",
        "https://openalex.org/W4214736485",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6793592898",
        "https://openalex.org/W3098873988",
        "https://openalex.org/W3105966348",
        "https://openalex.org/W6737778391",
        "https://openalex.org/W6790978476",
        "https://openalex.org/W6762945437",
        "https://openalex.org/W3143320354",
        "https://openalex.org/W6797990175",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W6798107787",
        "https://openalex.org/W3170642968",
        "https://openalex.org/W6791821072",
        "https://openalex.org/W4214614183",
        "https://openalex.org/W4283809036",
        "https://openalex.org/W4214636423",
        "https://openalex.org/W3151130473",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W6794345597",
        "https://openalex.org/W6796245125",
        "https://openalex.org/W6797360341",
        "https://openalex.org/W6796237581",
        "https://openalex.org/W6796604985",
        "https://openalex.org/W3121523901",
        "https://openalex.org/W6791705549",
        "https://openalex.org/W6797415705",
        "https://openalex.org/W3011574394",
        "https://openalex.org/W6768080748",
        "https://openalex.org/W2970454332",
        "https://openalex.org/W3034457371",
        "https://openalex.org/W6766795243",
        "https://openalex.org/W6797153837",
        "https://openalex.org/W3101248447",
        "https://openalex.org/W6762718338",
        "https://openalex.org/W2998183051",
        "https://openalex.org/W6795300077",
        "https://openalex.org/W3158846111",
        "https://openalex.org/W6794906783",
        "https://openalex.org/W4214588794",
        "https://openalex.org/W6791943378",
        "https://openalex.org/W3136416617",
        "https://openalex.org/W6797985667",
        "https://openalex.org/W6797602710",
        "https://openalex.org/W3202742610",
        "https://openalex.org/W3204801262",
        "https://openalex.org/W6796298902",
        "https://openalex.org/W6795140394",
        "https://openalex.org/W6792695861",
        "https://openalex.org/W3203701986",
        "https://openalex.org/W4214634256",
        "https://openalex.org/W6797866790",
        "https://openalex.org/W2979691890",
        "https://openalex.org/W4214669216",
        "https://openalex.org/W3172509117",
        "https://openalex.org/W6783236829",
        "https://openalex.org/W6679909955",
        "https://openalex.org/W3035317797",
        "https://openalex.org/W6638523607",
        "https://openalex.org/W6797735627",
        "https://openalex.org/W6617368339",
        "https://openalex.org/W2997006708",
        "https://openalex.org/W6780119413",
        "https://openalex.org/W3106655949",
        "https://openalex.org/W6754992056",
        "https://openalex.org/W6762805369",
        "https://openalex.org/W3102985954",
        "https://openalex.org/W3094349784",
        "https://openalex.org/W6797854001",
        "https://openalex.org/W6774054309",
        "https://openalex.org/W6789041250",
        "https://openalex.org/W6758657797",
        "https://openalex.org/W6787972765",
        "https://openalex.org/W2027377866",
        "https://openalex.org/W2129575457",
        "https://openalex.org/W6781533629",
        "https://openalex.org/W6779709467",
        "https://openalex.org/W3139445856",
        "https://openalex.org/W2963351448",
        "https://openalex.org/W6796814978",
        "https://openalex.org/W2982770724",
        "https://openalex.org/W6793267612",
        "https://openalex.org/W6796646135",
        "https://openalex.org/W2964241181",
        "https://openalex.org/W6794978867",
        "https://openalex.org/W6796505553",
        "https://openalex.org/W6797175998",
        "https://openalex.org/W3015001695",
        "https://openalex.org/W3143373604",
        "https://openalex.org/W6774908330",
        "https://openalex.org/W2963823140",
        "https://openalex.org/W6752343131",
        "https://openalex.org/W6681017033",
        "https://openalex.org/W2950768109",
        "https://openalex.org/W6795975431",
        "https://openalex.org/W3176196997",
        "https://openalex.org/W2970726176",
        "https://openalex.org/W3203480968",
        "https://openalex.org/W6797399245",
        "https://openalex.org/W6763422710",
        "https://openalex.org/W3098085362",
        "https://openalex.org/W6739778489",
        "https://openalex.org/W6795135341",
        "https://openalex.org/W3203925315",
        "https://openalex.org/W3193694068",
        "https://openalex.org/W2011815965",
        "https://openalex.org/W6803525030",
        "https://openalex.org/W6787752094",
        "https://openalex.org/W6784467320",
        "https://openalex.org/W2152839228",
        "https://openalex.org/W2982219368",
        "https://openalex.org/W6790830454",
        "https://openalex.org/W6799838802",
        "https://openalex.org/W6796761347",
        "https://openalex.org/W2963420272",
        "https://openalex.org/W2423557781",
        "https://openalex.org/W2025768430",
        "https://openalex.org/W6675401909",
        "https://openalex.org/W6797790494",
        "https://openalex.org/W6797371478",
        "https://openalex.org/W3035524453",
        "https://openalex.org/W6794295097",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W639708223",
        "https://openalex.org/W6779879114",
        "https://openalex.org/W6784333009",
        "https://openalex.org/W6778485988",
        "https://openalex.org/W3106728613",
        "https://openalex.org/W6784094891",
        "https://openalex.org/W4214508443",
        "https://openalex.org/W3170841864",
        "https://openalex.org/W3171125843",
        "https://openalex.org/W3164543136",
        "https://openalex.org/W6787826751",
        "https://openalex.org/W3119586106",
        "https://openalex.org/W3166470370",
        "https://openalex.org/W3107331169",
        "https://openalex.org/W6785665406",
        "https://openalex.org/W6797647004",
        "https://openalex.org/W3203974803",
        "https://openalex.org/W3101731278",
        "https://openalex.org/W3103465009",
        "https://openalex.org/W3158818292",
        "https://openalex.org/W2766736793",
        "https://openalex.org/W3203606893",
        "https://openalex.org/W3202715235",
        "https://openalex.org/W2981689412",
        "https://openalex.org/W2786052267",
        "https://openalex.org/W3165647589",
        "https://openalex.org/W3181047098",
        "https://openalex.org/W3163465952",
        "https://openalex.org/W3146097248",
        "https://openalex.org/W3026408381",
        "https://openalex.org/W3138796575",
        "https://openalex.org/W2964091144",
        "https://openalex.org/W3211432419",
        "https://openalex.org/W3137963805",
        "https://openalex.org/W3034573343",
        "https://openalex.org/W3033613646",
        "https://openalex.org/W3204251186",
        "https://openalex.org/W3166368936",
        "https://openalex.org/W3022969335",
        "https://openalex.org/W2963266682",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W3158284840",
        "https://openalex.org/W3093960091",
        "https://openalex.org/W3101415077",
        "https://openalex.org/W3112874352",
        "https://openalex.org/W3171206729",
        "https://openalex.org/W2890779863",
        "https://openalex.org/W3115390238",
        "https://openalex.org/W3169769133",
        "https://openalex.org/W3122239467",
        "https://openalex.org/W3172942063",
        "https://openalex.org/W3169575272",
        "https://openalex.org/W2613718673",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3130071011",
        "https://openalex.org/W3168783492",
        "https://openalex.org/W3045733172",
        "https://openalex.org/W2963420686",
        "https://openalex.org/W3153465022",
        "https://openalex.org/W3017943203",
        "https://openalex.org/W3205821239",
        "https://openalex.org/W2996159613",
        "https://openalex.org/W3100020884",
        "https://openalex.org/W3165745140",
        "https://openalex.org/W3114896399",
        "https://openalex.org/W3170778815",
        "https://openalex.org/W3169793979",
        "https://openalex.org/W3175466730",
        "https://openalex.org/W3034930876",
        "https://openalex.org/W3023001672",
        "https://openalex.org/W3135367836",
        "https://openalex.org/W3098272992",
        "https://openalex.org/W3163203812",
        "https://openalex.org/W3159833358",
        "https://openalex.org/W3156109214",
        "https://openalex.org/W3034974675",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2995460200",
        "https://openalex.org/W3164540605",
        "https://openalex.org/W3011411500",
        "https://openalex.org/W1514535095",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3118608800",
        "https://openalex.org/W3166942762",
        "https://openalex.org/W3160566314",
        "https://openalex.org/W2963319519",
        "https://openalex.org/W2970157301",
        "https://openalex.org/W3173840964",
        "https://openalex.org/W2886970679",
        "https://openalex.org/W2990775046",
        "https://openalex.org/W2970565456",
        "https://openalex.org/W3122818000",
        "https://openalex.org/W2963759574",
        "https://openalex.org/W2953356739",
        "https://openalex.org/W587794757",
        "https://openalex.org/W3022478135",
        "https://openalex.org/W3008526508",
        "https://openalex.org/W2964036520",
        "https://openalex.org/W3172661913",
        "https://openalex.org/W3204367729",
        "https://openalex.org/W2053619738",
        "https://openalex.org/W3095200371",
        "https://openalex.org/W2294370754",
        "https://openalex.org/W2912521296",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W2947946877",
        "https://openalex.org/W3034445277",
        "https://openalex.org/W3192125374",
        "https://openalex.org/W2963813662",
        "https://openalex.org/W3174738881",
        "https://openalex.org/W3101278968",
        "https://openalex.org/W3009561768",
        "https://openalex.org/W2937845937",
        "https://openalex.org/W3170544306",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W2963524571",
        "https://openalex.org/W2982220924",
        "https://openalex.org/W3212756788",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W3132503749",
        "https://openalex.org/W2964080601",
        "https://openalex.org/W3042203953",
        "https://openalex.org/W2955813853",
        "https://openalex.org/W2896632102",
        "https://openalex.org/W2970896726",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W3112462324",
        "https://openalex.org/W3034428934",
        "https://openalex.org/W3033210410",
        "https://openalex.org/W3157506437",
        "https://openalex.org/W3139773203",
        "https://openalex.org/W2963799213",
        "https://openalex.org/W3037899338",
        "https://openalex.org/W3173631098",
        "https://openalex.org/W2963121255",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2970389371",
        "https://openalex.org/W2968124245",
        "https://openalex.org/W3035414587",
        "https://openalex.org/W3205856632",
        "https://openalex.org/W3162457465",
        "https://openalex.org/W2097073572",
        "https://openalex.org/W3174981785",
        "https://openalex.org/W3008374555",
        "https://openalex.org/W2899663614",
        "https://openalex.org/W3150226983",
        "https://openalex.org/W3168101492",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W3170016573",
        "https://openalex.org/W2147527908",
        "https://openalex.org/W3112776202",
        "https://openalex.org/W3019527251",
        "https://openalex.org/W3168825659",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W2955425717",
        "https://openalex.org/W3159732141",
        "https://openalex.org/W2948359136",
        "https://openalex.org/W3177183540",
        "https://openalex.org/W2462831000",
        "https://openalex.org/W3152698000",
        "https://openalex.org/W1566289585",
        "https://openalex.org/W3139586422",
        "https://openalex.org/W3178340970",
        "https://openalex.org/W3126589408",
        "https://openalex.org/W3104216863",
        "https://openalex.org/W2963162885",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W3204575409",
        "https://openalex.org/W2965391153",
        "https://openalex.org/W3013571468",
        "https://openalex.org/W1924770834",
        "https://openalex.org/W3109635183",
        "https://openalex.org/W2955058313",
        "https://openalex.org/W3159337199",
        "https://openalex.org/W2971274815",
        "https://openalex.org/W2979062293",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W3177265267",
        "https://openalex.org/W3161838454",
        "https://openalex.org/W3156811085",
        "https://openalex.org/W2890782586",
        "https://openalex.org/W3166513219",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2969601108",
        "https://openalex.org/W3129619124",
        "https://openalex.org/W2965853874",
        "https://openalex.org/W3146091044",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W3010768098",
        "https://openalex.org/W3170863103",
        "https://openalex.org/W3202406646",
        "https://openalex.org/W2962961439",
        "https://openalex.org/W2892220259",
        "https://openalex.org/W3169938586",
        "https://openalex.org/W1836465849",
        "https://openalex.org/W3133696297",
        "https://openalex.org/W3211787299",
        "https://openalex.org/W2970986510",
        "https://openalex.org/W2895340641",
        "https://openalex.org/W2322002063",
        "https://openalex.org/W3099547366",
        "https://openalex.org/W3106009088",
        "https://openalex.org/W3139049060",
        "https://openalex.org/W3178702014",
        "https://openalex.org/W3160284783",
        "https://openalex.org/W2963495494",
        "https://openalex.org/W2560609797",
        "https://openalex.org/W2972825840",
        "https://openalex.org/W3120644635",
        "https://openalex.org/W3212161087",
        "https://openalex.org/W3141972123",
        "https://openalex.org/W2982399380",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2975429091",
        "https://openalex.org/W3146455718",
        "https://openalex.org/W2798122215",
        "https://openalex.org/W2981458636",
        "https://openalex.org/W3177462300",
        "https://openalex.org/W3170264469",
        "https://openalex.org/W3180659574",
        "https://openalex.org/W2807299122",
        "https://openalex.org/W3132890542",
        "https://openalex.org/W3119786062"
    ],
    "abstract": "Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent neural networks. Given its high performance and less need for vision-specific inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient transformer methods for pushing transformer into real device-based applications. Furthermore, we also take a brief look at the self-attention mechanism in computer vision, as it is the base component in transformer. Toward the end of this paper, we discuss the challenges and provide several further research directions for vision transformers.",
    "full_text": "A SUBMISSION TO IEEE TRANSACTION ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 1\nA Survey on Visual Transformer\nKai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Y ehui Tang, An Xiao,\nChunjing Xu, Yixing Xu, Zhaohui Y ang, Yiman Zhang, and Dacheng TaoFellow, IEEE\nAbstract—Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the\nself-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to\ncomputer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of\nnetworks such as convolutional and recurrent neural networks. Given its high performance and less need for vision-specific inductive\nbias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision\ntransformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we\nexplore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient\ntransformer methods for pushing transformer into real device-based applications. Furthermore, we also take a brief look at the\nself-attention mechanism in computer vision, as it is the base component in transformer. Toward the end of this paper, we discuss the\nchallenges and provide several further research directions for vision transformers.\nIndex Terms—Transformer, Self-attention, Computer Vision, High-level vision, Low-level vision, Video.\n✦\n1 I NTRODUCTION\nD\nEEP neural networks (DNNs) have become the fundamental\ninfrastructure in today’s artificial intelligence (AI) systems.\nDifferent types of tasks have typically involved different types\nof networks. For example, multi-layer perceptron (MLP) or the\nfully connected (FC) network is the classical type of neural\nnetwork, which is composed of multiple linear layers and non-\nlinear activations stacked together [1], [2]. Convolutional neural\nnetworks (CNNs) introduce convolutional layers and pooling\nlayers for processing shift-invariant data such as images [3], [4].\nAnd recurrent neural networks (RNNs) utilize recurrent cells to\nprocess sequential data or time series data [5], [6]. Transformer is\na new type of neural network. It mainly utilizes the self-attention\nmechanism [7], [8] to extract intrinsic features [9] and shows great\npotential for extensive use in AI applications.\nTransformer was first applied to natural language processing\n(NLP) tasks where it achieved significant improvements [9], [10],\n[11]. For example, Vaswani et al. [9] first proposed transformer\nbased on attention mechanism for machine translation and English\nconstituency parsing tasks. Devlinet al. [10] introduced a new lan-\nguage representation model called BERT (short for Bidirectional\nEncoder Representations from Transformers), which pre-trains a\ntransformer on unlabeled text taking into account the context of\neach word as it is bidirectional. When BERT was published, it\nobtained state-of-the-art performance on 11 NLP tasks. Brown et\nal. [11] pre-trained a massive transformer-based model called\nGPT-3 (short for Generative Pre-trained Transformer 3) on 45\n• Kai Han, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui\nTang, An Xiao, Chunjing Xu, Yixing Xu, Zhaohui Yang, Yiman Zhang,\nand Yunhe Wang are with Huawei Noah’s Ark Lab. E-mail: {kai.han,\nyunhe.wang}@huawei.com.\n• Hanting Chen, Zhenhua Liu, Yehui Tang, and Zhaohui Yang are also with\nSchool of EECS, Peking University.\n• Dacheng Tao is with the School of Computer Science, in the Faculty of\nEngineering, at The University of Sydney, 6 Cleveland St, Darlington,\nNSW 2008, Australia. E-mail: dacheng.tao@sydney.edu.au.\n• Corresponding to Yunhe Wang and Dacheng Tao.\n• All authors are listed in alphabetical order of last name (except the\nprimary and corresponding authors).\nTB of compressed plaintext data using 175 billion parameters.\nIt achieved strong performance on different types of downstream\nnatural language tasks without requiring any fine-tuning. These\ntransformer-based models, with their strong representation capac-\nity, have achieved significant breakthroughs in NLP.\nInspired by the major success of transformer architectures in\nthe field of NLP, researchers have recently applied transformer\nto computer vision (CV) tasks. In vision applications, CNNs are\nconsidered the fundamental component [12], [13], but nowadays\ntransformer is showing it is a potential alternative to CNN. Chenet\nal. [14] trained a sequence transformer to auto-regressively predict\npixels, achieving results comparable to CNNs on image classi-\nfication tasks. Another vision transformer model is ViT, which\napplies a pure transformer directly to sequences of image patches\nto classify the full image. Recently proposed by Dosovitskiy et\nal. [15], it has achieved state-of-the-art performance on multiple\nimage recognition benchmarks. In addition to image classification,\ntransformer has been utilized to address a variety of other vision\nproblems, including object detection [16], [17], semantic segmen-\ntation [18], image processing [19], and video understanding [20].\nThanks to its exceptional performance, more and more researchers\nare proposing transformer-based models for improving a wide\nrange of visual tasks.\nDue to the rapid increase in the number of transformer-based\nvision models, keeping pace with the rate of new progress is\nbecoming increasingly difficult. As such, a survey of the existing\nworks is urgent and would be beneficial for the community. In\nthis paper, we focus on providing a comprehensive overview\nof the recent advances in vision transformers and discuss the\npotential directions for further improvement. To facilitate future\nresearch on different topics, we categorize the transformer models\nby their application scenarios, as listed in Table 1. The main\ncategories include backbone network, high/mid-level vision, low-\nlevel vision, and video processing. High-level vision deals with the\ninterpretation and use of what is seen in the image [21], whereas\nmid-level vision deals with how this information is organized into\nwhat we experience as objects and surfaces [22]. Given the gap\narXiv:2012.12556v6  [cs.CV]  10 Jul 2023\nA SUBMISSION TO IEEE TRANSACTION ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 2\n2017.6 | Transformer\nSolely based on attention \nmechanism, the Transformer is \nproposed and shows great \nperformance on NLP tasks.\n2018.10 | BERT\nPre-training transformer models \nbegin to be dominated in the \nfield of NLP .\n2020.5 | GPT-3\nA huge transformer with \n170B parameters, takes a \nbig step towards general \nNLP model.\n2020.10 | ViT\nPure transformer \narchitectures work well for \nvisual recognition.\n2021 | ViT Variants\nVariants of ViT models, \ne.g., DeiT, PVT, TNT, \nand Swin.\n2020.5 | DETR\nA simple yet effective \nframework for high-level vision \nby viewing object detection as \na direct set prediction problem.\nEnd of 2020 | IPT/SETR/CLIP\nApplications of transformer \nmodel on low-level vision, \nsegment and multimodal \ntasks, respectively.\n2022 | DALLE2/StableDiffsusion\nGenerating high-quality \nimages from natural \nlanguage descriptions with \ndiffusion models.\n2023 | GPT4\nA generalized multi-\nmodal model for both \nlanguage and vision \ntasks.\nFig. 1: Key milestones in the development of transformer. The vision transformer models are marked in red.\nbetween high- and mid-level vision is becoming more obscure\nin DNN-based vision systems [23], [24], we treat them as a\nsingle category here. A few examples of transformer models that\naddress these high/mid-level vision tasks include DETR [16], de-\nformable DETR [17] for object detection, and Max-DeepLab [25]\nfor segmentation. Low-level image processing mainly deals with\nextracting descriptions from images (such descriptions are usually\nrepresented as images themselves) [26]. Typical applications of\nlow-level image processing include super-resolution, image de-\nnoising, and style transfer. At present, only a few works [19], [27]\nin low-level vision use transformers, creating the need for further\ninvestigation. Another category is video processing, which is an\nimportant part in both computer vision and image-based tasks. Due\nto the sequential property of video, transformer is inherently well\nsuited for use on video tasks [20], [28], in which it is beginning\nto perform on par with conventional CNNs and RNNs. Here, we\nsurvey the works associated with transformer-based visual models\nin order to track the progress in this field. Figure 1 shows the\ndevelopment timeline of vision transformer — undoubtedly, there\nwill be many more milestones in the future.\nThe rest of the paper is organized as follows. Section 2\ndiscusses the formulation of the standard transformer and the self-\nattention mechanism. Section 4 is the main part of the paper, in\nwhich we summarize the vision transformer models on backbone,\nhigh/mid-level vision, low-level vision, and video tasks. We also\nbriefly describe efficient transformer methods, as they are closely\nrelated to our main topic. In the final section, we give our\nconclusion and discuss several research directions and challenges.\nDue to the page limit, we describe the methods of transformer in\nNLP in the supplemental material, as the research experience may\nbe beneficial for vision tasks. In the supplemental material, we also\nreview the self-attention mechanism for CV as the supplementary\nof vision transformer models. In this survey, we mainly include the\nrepresentative works (early, pioneering, novel, or inspiring works)\nsince there are many preprinted works on arXiv and we cannot\ninclude them all in limited pages.\n2 F ORMULATION OF TRANSFORMER\nTransformer [9] was first used in the field of natural language\nprocessing (NLP) on machine translation tasks. As shown in\nFigure 2, it consists of an encoder and a decoder with several\ntransformer blocks of the same architecture. The encoder gener-\nates encodings of inputs, while the decoder takes all the encodings\nand using their incorporated contextual information to generate\nthe output sequence. Each transformer block is composed of a\nmulti-head attention layer, a feed-forward neural network, shortcut\nconnection and layer normalization. In the following, we describe\neach component of the transformer in detail.\nFig. 2: Structure of the original transformer (image from [9]).\n2.1 Self-Attention\nIn the self-attention layer, the input vector is first transformed into\nthree different vectors: the query vectorq, the key vectork and the\nvalue vector v with dimension dq = dk = dv = dmodel = 512.\nVectors derived from different inputs are then packed together into\nthree different matrices, namely, Q, K and V. Subsequently, the\nattention function between different input vectors is calculated as\nfollows (and shown in Figure 3 left):\n• Step 1: Compute scores between different input vectors\nwith S = Q · K⊤;\n• Step 2: Normalize the scores for the stability of gradient\nwith Sn = S/√dk;\n• Step 3: Translate the scores into probabilities with softmax\nfunction P = softmax(Sn);\n• Step 4: Obtain the weighted value matrix with Z = V·P.\nThe process can be unified into a single function:\nAttention(Q, K, V) = softmax(Q · K⊤\n√dk\n) · V. (1)\nThe logic behind Eq. 1 is simple. Step 1 computes scores between\neach pair of different vectors, and these scores determine the\ndegree of attention that we give other words when encoding\nthe word at the current position. Step 2 normalizes the scores\nto enhance gradient stability for improved training, and step 3\ntranslates the scores into probabilities. Finally, each value vector\nis multiplied by the sum of the probabilities. Vectors with larger\nprobabilities receive additional focus from the following layers.\nA SUBMISSION TO IEEE TRANSACTION ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 3\nTABLE 1: Representative works of vision transformers.\nCategory Sub-category Method Highlights Publication\nBackbone\nSupervised pretraining\nViT [15] Image patches, standard transformer ICLR 2021\nTNT [29] Transformer in transformer, local attention NeurIPS 2021\nSwin [30] Shifted window, window-based self-attention ICCV 2021\nSelf-supervised pretraining\niGPT [14] Pixel prediction self-supervised learning, GPT model ICML 2020\nMoCo v3 [31] Contrastive self-supervised learning, ViT ICCV 2021\nMAE [32] Masked image modeling, ViT CVPR 2022\nHigh/Mid-level\nvision\nObject detection\nDETR [16] Set-based prediction, bipartite matching, transformer ECCV 2020\nDeformable DETR [17] DETR, deformable attention module ICLR 2021\nUP-DETR [33] Unsupervised pre-training, random query patch detection CVPR 2021\nSegmentation\nMax-DeepLab [25] PQ-style bipartite matching, dual-path transformer CVPR 2021\nVisTR [34] Instance sequence matching and segmentation CVPR 2021\nSETR [18] Sequence-to-sequence prediction, standard transformer CVPR 2021\nPose Estimation\nHand-Transformer [35] Non-autoregressive transformer, 3D point set ECCV 2020\nHOT-Net [36] Structured-reference extractor MM 2020\nMETRO [37] Progressive dimensionality reduction CVPR 2021\nLow-level\nvision\nImage generation\nImage Transformer [27] Pixel generation using transformer ICML 2018\nTaming transformer [38] VQ-GAN, auto-regressive transformer CVPR 2021\nTransGAN [39] GAN using pure transformer architecture NeurIPS 2021\nImage enhancement IPT [19] Multi-task, ImageNet pre-training, transformer model CVPR 2021\nTTSR [40] Texture transformer, RefSR CVPR 2020\nVideo\nprocessing\nVideo inpainting STTN [28] Spatial-temporal adversarial loss ECCV 2020\nVideo captioning Masked Transformer [20] Masking network, event proposal CVPR 2018\nMultimodality\nClassification CLIP [41] NLP supervision for images, zero-shot transfer arXiv 2021\nImage generation DALL-E [42] Zero-shot text-to image generation ICML 2021\nCogview [43] VQ-V AE, Chinese input NeurIPS 2021\nMulti-task GPT-4 [44] Large Multi-modal model for NLP & CV tasks arXiv 2023\nEfficient\ntransformer\nDecomposition ASH [45] Number of heads, importance estimation NeurIPS 2019\nDistillation TinyBert [46] Various losses for different modules EMNLP Findings 2020\nQuantization FullyQT [47] Fully quantized transformer EMNLP Findings 2020\nArchitecture design ConvBert [48] Local dependence, dynamic convolution NeurIPS 2020\nThe encoder-decoder attention layer in the decoder module\nis similar to the self-attention layer in the encoder module with\nthe following exceptions: The key matrix K and value matrix V\nare derived from the encoder module, and the query matrix Q is\nderived from the previous layer.\nNote that the preceding process is invariant to the position of\neach word, meaning that the self-attention layer lacks the ability\nto capture the positional information of words in a sentence.\nHowever, the sequential nature of sentences in a language requires\nus to incorporate the positional information within our encoding.\nTo address this issue and allow the final input vector of the word\nto be obtained, a positional encoding with dimension dmodel is\nadded to the original input embedding. Specifically, the position is\nencoded with the following equations:\nPE (pos, 2i) =sin( pos\n10000\n2i\ndmodel\n); (2)\nPE (pos, 2i + 1) =cos( pos\n10000\n2i\ndmodel\n), (3)\nin which pos denotes the position of the word in a sentence, and\ni represents the current dimension of the positional encoding. In\nthis way, each element of the positional encoding corresponds to\na sinusoid, and it allows the transformer model to learn to attend\nby relative positions and extrapolate to longer sequence lengths\nduring inference. In apart from the fixed positional encoding in the\nvanilla transformer, learned positional encoding [49] and relative\npositional encoding [50] are also utilized in various models [10],\n[15].\nMulti-Head Attention. Multi-head attention is a mechanism\nthat can be used to boost the performance of the vanilla self-\nattention layer. Note that for a given reference word, we often\nwant to focus on several other words when going through the\nsentence. A single-head self-attention layer limits our ability to\nfocus on one or more specific positions without influencing the\nFig. 3: (Left) Self-attention process. (Right) Multi-head attention.\nThe image is from [9].\nattention on other equally important positions at the same time.\nThis is achieved by giving attention layers different representation\nsubspace. Specifically, different query, key and value matrices are\nused for different heads, and these matrices can project the input\nvectors into different representation subspace after training due to\nrandom initialization.\nTo elaborate on this in greater detail, given an input vector\nand the number of heads h, the input vector is first transformed\ninto three different groups of vectors: the query group, the key\ngroup and the value group. In each group, there are h vectors with\ndimension dq′ = dk′ = dv′ = dmodel/h = 64. The vectors\nderived from different inputs are then packed together into three\ndifferent groups of matrices: {Qi}h\ni=1, {Ki}h\ni=1 and {Vi}h\ni=1.\nThe multi-head attention process is shown as follows:\nMultiHead(Q′, K′, V′) = Concat(head1, ··· , headh)Wo,\nwhere headi = Attention(Qi, Ki, Vi). (4)\nHere, Q′ (and similarly K′ and V′) is the concatenation of\n{Qi}h\ni=1, and Wo ∈ Rdmodel×dmodel is the projection weight.\nA SUBMISSION TO IEEE TRANSACTION ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 4\n2.2 Other Key Concepts in Transformer\nFeed-Forward Network. A feed-forward network (FFN) is ap-\nplied after the self-attention layers in each encoder and decoder. It\nconsists of two linear transformation layers and a nonlinear acti-\nvation function within them, and can be denoted as the following\nfunction:\nFFN(X) =W2σ(W1X), (5)\nwhere W1 and W2 are the two parameter matrices of the\ntwo linear transformation layers, and σ represents the nonlinear\nactivation function, such as GELU [51]. The dimensionality of the\nhidden layer is dh = 2048.\nResidual Connection in the Encoder and Decoder. As shown in\nFigure 2, a residual connection is added to each sub-layer in the\nencoder and decoder. This strengthens the flow of information in\norder to achieve higher performance. A layer-normalization [52]\nis followed after the residual connection. The output of these\noperations can be described as:\nLayerNorm(X + Attention(X)). (6)\nHere, X is used as the input of self-attention layer, and the query,\nkey and value matrices Q, K and V are all derived from the same\ninput matrix X. A variant pre-layer normalization (Pre-LN) is also\nwidely-used [53], [54], [15]. Pre-LN inserts the layer normaliza-\ntion inside the residual connection and before multi-head attention\nor FFN. For the normalization layer, there are several alternatives\nsuch as batch normalization [55]. Batch normalization usually\nperform worse when applied on transformer as the feature values\nchange acutely [56]. Some other normalization algorithms [57],\n[56], [58] have been proposed to improve training of transformer.\nFinal Layer in the Decoder.The final layer in the decoder is used\nto turn the stack of vectors back into a word. This is achieved by a\nlinear layer followed by a softmax layer. The linear layer projects\nthe vector into a logits vector with dword dimensions, in which\ndword is the number of words in the vocabulary. The softmax\nlayer is then used to transform the logits vector into probabilities.\nWhen used for CV tasks, most transformers adopt the original\ntransformer’s encoder module. Such transformers can be treated\nas a new type of feature extractor. Compared with CNNs which\nfocus only on local characteristics, transformer can capture long-\ndistance characteristics, meaning that it can easily derive global\ninformation. And in contrast to RNNs, whose hidden state must\nbe computed sequentially, transformer is more efficient because\nthe output of the self-attention layer and the fully connected layers\ncan be computed in parallel and easily accelerated. From this, we\ncan conclude that further study into using transformer in computer\nvision as well as NLP would yield beneficial results.\n3 V ISION TRANSFORMER\nIn this section, we review the applications of transformer-\nbased models in computer vision, including image classification,\nhigh/mid-level vision, low-level vision and video processing. We\nalso briefly summarize the applications of the self-attention mech-\nanism and model compression methods for efficient transformer.\n3.1 Backbone for Representation Learning\nInspired by the success that transformer has achieved in the field of\nNLP, some researchers have explored whether similar models can\nlearn useful representations for images. Given that images involve\nHuawei Confidential1\nBackbone for Representation Learning\nConvolution Attention\nCNN CNN + Transformer\nNon-Local\nTransformer SENet\nNLNet\nGCNetAlexNet/ResNet/DenseNet BoTNet/CeiT ViT/PVT/TNT/Swin\nFig. 4: A taxonomy of backbone using convolution and attention.\nmore dimensions, noise and redundant modality compared to text,\nthey are believed to be more difficult for generative modeling.\nOther than CNNs, the transformer can be used as backbone\nnetworks for image classification. Wu et al. [59] adopted ResNet\nas a convenient baseline and used vision transformers to replace\nthe last stage of convolutions. Specifically, they apply convolu-\ntional layers to extract low-level features that are then fed into\nthe vision transformer. For the vision transformer, they use a\ntokenizer to group pixels into a small number of visual tokens ,\neach representing a semantic concept in the image. These visual\ntokens are used directly for image classification, with the trans-\nformers being used to model the relationships between tokens. As\nshown in Figure 4, the works can be divided into purely using\ntransformer for vision and combining CNN and transformer. We\nsummarize the results of these models in Table 2 and Figure 6\nto demonstrate the development of the backbones. In addition to\nsupervised learning, self-supervised learning is also explored in\nvision transformer.\n3.1.1 Pure Transformer\nViT. Vision Transformer (ViT) [15] is a pure transformer directly\napplies to the sequences of image patches for image classification\ntask. It follows transformer’s original design as much as possible.\nFigure 5 shows the framework of ViT.\nTo handle 2D images, the image X ∈ Rh×w×c is reshaped\ninto a sequence of flattened 2D patches Xp ∈ Rn×(p2·c) such\nthat c is the number of channels. (h, w) is the resolution of the\noriginal image, while (p, p) is the resolution of each image patch.\nThe effective sequence length for the transformer is therefore n =\nhw/p2. Because the transformer uses constant widths in all of its\nlayers, a trainable linear projection maps each vectorized path to\nthe model dimension d, the output of which is referred to as patch\nembeddings.\nSimilar to BERT’s [class] token, a learnable embedding is\napplied to the sequence of embedding patches. The state of this\nembedding serves as the image representation. During both pre-\ntraining and fine-tuning stage, the classification heads are attached\nto the same size. In addition, 1D position embeddings are added\nto the patch embeddings in order to retain positional information.\nIt is worth noting that ViT utilizes only the standard transformer’s\nencoder (except for the place for the layer normalization), whose\noutput precedes an MLP head. In most cases, ViT is pre-trained\non large datasets, and then fine-tuned for downstream tasks with\nsmaller data.\nViT yields modest results when trained on mid-sized datasets\nsuch as ImageNet, achieving accuracies of a few percentage\npoints below ResNets of comparable size. Because transformers\nlack some inductive biases inherent to CNNs–such as translation\nequivariance and locality–they do not generalize well when trained\non insufficient amounts of data. However, the authors found\nthat training the models on large datasets (14 million to 300\nA SUBMISSION TO IEEE TRANSACTION ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 5\nmillion images) surpassed inductive bias. When pre-trained at\nsufficient scale, transformers achieve excellent results on tasks\nwith fewer datapoints. For example, when pre-trained on the\nJFT-300M dataset, ViT approached or even exceeded state of\nthe art performance on multiple image recognition benchmarks.\nSpecifically, it reached an accuracy of 88.36% on ImageNet, and\n77.16% on the VTAB suite of 19 tasks.\nTouvron et al. [60] proposed a competitive convolution-free\ntransformer, called Data-efficient image transformer (DeiT), by\ntraining on only the ImageNet database. DeiT-B, the reference vi-\nsion transformer, has the same architecture as ViT-B and employs\n86 million parameters. With a strong data augmentation, DeiT-\nB achieves top-1 accuracy of 83.1% (single-crop evaluation) on\nImageNet with no external data. In addition, the authors observe\nthat using a CNN teacher gives better performance than using\na transformer. Specifically, DeiT-B can achieve top-1 accuracy\n84.40% with the help of a token-based distillation.\nFig. 5: The framework of ViT (image from [15]).\nVariants of ViT. Following the paradigm of ViT, a series of\nvariants of ViT have been proposed to improve the performance\non vision tasks. The main approaches include enhancing locality,\nself-attention improvement and architecture design.\nThe original vision transformer is good at capturing long-range\ndependencies between patches, but disregard the local feature\nextraction as the 2D patch is projected to a vector with simple\nlinear layer. Recently, the researchers begin to pay attention to\nimprove the modeling capacity for local information [29], [61],\n[62]. TNT [29] further divides the patch into a number of sub-\npatches and introduces a novel transformer-in-transformer archi-\ntecture which utilizes an inner transformer block to model the\nrelationship between sub-patches and an outer transformer block\nfor patch-level information exchange. Twins [63] and CAT [64]\nalternately perform local and global attention layer-by-layer. Swin\nTransformers [61], [65] performs local attention within a win-\ndow and introduces a shifted window partitioning approach for\ncross-window connections. Shuffle Transformer [66], [67] further\nutilizes the spatial shuffle operation instead of shifted window\npartitioning to allow cross-window connections. RegionViT [62]\ngenerates regional tokens and local tokens from an image, and\nlocal tokens receive global information via attention with regional\ntokens. In addition to the local attention, some other works propose\nto boost local information through local feature aggregation, e.g.,\nT2T [68]. These works demonstrate the benefit of the local\ninformation exchange and global information exchange in vision\ntransformer.\nAs a key component of transformer, self-attention layer pro-\nvides the ability for global interaction between image patches.\nImproving the calculation of self-attention layer has attracted\nmany researchers. DeepViT [69] proposes to establish cross-\nhead communication to re-generate the attention maps to increase\nthe diversity at different layers. KVT [70] introduces the k-NN\nattention to utilize locality of images patches and ignore noisy\ntokens by only computing attentions with top-k similar tokens. Re-\nfiner [71] explores attention expansion in higher-dimension space\nand applied convolution to augment local patterns of the attention\nmaps. XCiT [72] performs self-attention calculation across feature\nchannels rather than tokens, which allows efficient processing of\nhigh-resolution images. The computation complexity and attention\nprecision of the self-attention mechanism are two key-points for\nfuture optimization.\nThe network architecture is an important factor as demon-\nstrated in the field of CNNs. The original architecture of ViT\nis a simple stack of the same-shape transformer block. New\narchitecture design for vision transformer has been an interesting\ntopic. The pyramid-like architecture is utilized by many vision\ntransformer models [73], [61], [74], [75], [76], [77] including\nPVT [73], HVT [78], Swin Transformer [61] and PiT [79]. There\nare also other types of architectures, such as two-stream architec-\nture [80] and U-net architecture [81], [30]. Neural architecture\nsearch (NAS) has also been investigated to search for better\ntransformer architectures, e.g., Scaling-ViT [82], ViTAS [83],\nAutoFormer [84] and GLiT [85]. Currently, both network design\nand NAS for vision transformer mainly draw on the experience of\nCNN. In the future, we expect the specific and novel architectures\nappear in the filed of vision transformer.\nIn addition to the aforementioned approaches, there are some\nother directions to further improve vision transformer, e.g., posi-\ntional encoding [86], [87], normalization strategy [88], shortcut\nconnection [89] and removing attention [90], [91], [92], [93].\n3.1.2 Transformer with Convolution\nAlthough vision transformers have been successfully applied to\nvarious visual tasks due to their ability to capture long-range\ndependencies within the input, there are still gaps in performance\nbetween transformers and existing CNNs. One main reason can be\nthe lack of ability to extract local information. Except the above\nmentioned variants of ViT that enhance the locality, combining the\ntransformer with convolution can be a more straightforward way\nto introduce the locality into the conventional transformer.\nThere are plenty of works trying to augment a conventional\ntransformer block or self-attention layer with convolution. For\nexample, CPVT [86] proposed a conditional positional encoding\n(CPE) scheme, which is conditioned on the local neighborhood\nof input tokens and adaptable to arbitrary input sizes, to leverage\nconvolutions for fine-level feature encoding. CvT [97], CeiT [98],\nLocalViT [99] and CMT [95] analyzed the potential drawbacks\nwhen directly borrowing Transformer architectures from NLP and\ncombined the convolutions with transformers together. Specifi-\ncally, the feed-forward network (FFN) in each transformer block is\ncombined with a convolutional layer that promotes the correlation\namong neighboring tokens. LeViT [100] revisited principles from\nextensive literature on CNNs and applied them to transformers,\nproposing a hybrid neural network for fast inference image clas-\nsification. BoTNet [101] replaced the spatial convolutions with\nglobal self-attention in the final three bottleneck blocks of a\nResNet, and improved upon the baselines significantly on both\n1.\nA SUBMISSION TO IEEE TRANSACTION ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 6\nTABLE 2: ImageNet result comparison of representative CNN and\nvision transformer models. Pure transformer means only using a few\nconvolutions in the stem stage. CNN + Transformer means using\nconvolutions in the intermediate layers. Following [60], [61], the\nthroughput is measured on NVIDIA V100 GPU and Pytorch, with\n224×224 input size.\nModel Params FLOPs Throughput Top-1\n(M) (B) (image/s) (%)\nCNN\nResNet-50 [12], [68] 25.6 4.1 1226 79.1\nResNet-101 [12], [68] 44.7 7.9 753 79.9\nResNet-152 [12], [68] 60.2 11.5 526 80.8\nEfficientNet-B0 [94] 5.3 0.39 2694 77.1\nEfficientNet-B1 [94] 7.8 0.70 1662 79.1\nEfficientNet-B2 [94] 9.2 1.0 1255 80.1\nEfficientNet-B3 [94] 12 1.8 732 81.6\nEfficientNet-B4 [94] 19 4.2 349 82.9\nPure Transformer\nDeiT-Ti [15], [60] 5 1.3 2536 72.2\nDeiT-S [15], [60] 22 4.6 940 79.8\nDeiT-B [15], [60] 86 17.6 292 81.8\nT2T-ViT-14 [68] 21.5 5.2 764 81.5\nT2T-ViT-19 [68] 39.2 8.9 464 81.9\nT2T-ViT-24 [68] 64.1 14.1 312 82.3\nPVT-Small [73] 24.5 3.8 820 79.8\nPVT-Medium [73] 44.2 6.7 526 81.2\nPVT-Large [73] 61.4 9.8 367 81.7\nTNT-S [29] 23.8 5.2 428 81.5\nTNT-B [29] 65.6 14.1 246 82.9\nCPVT-S [86] 23 4.6 930 80.5\nCPVT-B [86] 88 17.6 285 82.3\nSwin-T [61] 29 4.5 755 81.3\nSwin-S [61] 50 8.7 437 83.0\nSwin-B [61] 88 15.4 278 83.3\nCNN + Transformer\nTwins-SVT-S [63] 24 2.9 1059 81.7\nTwins-SVT-B [63] 56 8.6 469 83.2\nTwins-SVT-L [63] 99.2 15.1 288 83.7\nShuffle-T [66] 29 4.6 791 82.5\nShuffle-S [66] 50 8.9 450 83.5\nShuffle-B [66] 88 15.6 279 84.0\nCMT-S [95] 25.1 4.0 563 83.5\nCMT-B [95] 45.7 9.3 285 84.5\nVOLO-D1 [96] 27 6.8 481 84.2\nVOLO-D2 [96] 59 14.1 244 85.2\nVOLO-D3 [96] 86 20.6 168 85.4\nVOLO-D4 [96] 193 43.8 100 85.7\nVOLO-D5 [96] 296 69.0 64 86.1\ninstance segmentation and object detection tasks with minimal\noverhead in latency.\nBesides, some researchers have demonstrated that transformer\nbased models can be more difficult to enjoy a favorable ability of\nfitting data [15], [102], [103], in other words, they are sensitive\nto the choice of optimizer, hyper-parameter, and the schedule of\ntraining. Visformer [102] revealed the gap between transformers\nand CNNs with two different training settings. The first one is the\nstandard setting for CNNs, i.e., the training schedule is shorter\nand the data augmentation only contains random cropping and\nhorizental flipping. The other one is the training setting used\nin [60], i.e., the training schedule is longer and the data augmenta-\ntion is stronger. [103] changed the early visual processing of ViT\nby replacing its embedding stem with a standard convolutional\nstem, and found that this change allows ViT to converge faster\nand enables the use of either AdamW or SGD without a significant\ndrop in accuracy. In addition to these two works, [100], [95] also\nchoose to add convolutional stem on the top of the transformer.\n3.1.3 Self-supervised Representation Learning\nGenerative Based Approach. Generative pre-training methods\nfor images have existed for a long time [104], [105], [106], [107].\nChen et al. [14] re-examined this class of methods and combined\nit with self-supervised methods. After that, several works [108],\n2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0\nFLOPs (B)\n78\n79\n80\n81\n82\n83\n84\n85\n86Accuracy (%)\nResNet\nEfficientNet\nDeiT\nPVT\nT2T\nSwin\nCMT\nVOLO\n200 400 600 800 1000 1200\nThroughput (image/s)\n78\n79\n80\n81\n82\n83\n84\n85\n86Accuracy (%)\nResNet\nEfficientNet\nDeiT\nPVT\nT2T\nSwin\nCMT\nVOLO\n(a) Acc v.s. FLOPs. (b) Acc v.s. throughput.\nFig. 6: FLOPs and throughput comparison of representative CNN\nand vision transformer models.\n[109] were proposed to extend generative based self-supervised\nlearning for vision transformer.\nWe briefly introduce iGPT [14] to demonstrate its mechanism.\nThis approach consists of a pre-training stage followed by a fine-\ntuning stage. During the pre-training stage, auto-regressive and\nBERT objectives are explored. To implement pixel prediction, a\nsequence transformer architecture is adopted instead of language\ntokens (as used in NLP). Pre-training can be thought of as a\nfavorable initialization or regularizer when used in combination\nwith early stopping. During the fine-tuning stage, they add a\nsmall classification head to the model. This helps optimize a\nclassification objective and adapts all weights.\nThe image pixels are transformed into a sequential data by\nk-means clustering. Given an unlabeled dataset X consisting of\nhigh dimensional data x = (x1, ··· , xn), they train the model by\nminimizing the negative log-likelihood of the data:\nLAR = E\nx∼X\n[−log p(x)], (7)\nwhere p(x) is the probability density of the data of images, which\ncan be modeled as:\np(x) =\nnY\ni=1\np(xπi|xπ1 , ··· , xπi−1 , θ). (8)\nHere, the identity permutation πi = i is adopted for 1 ⩽ i ⩽ n,\nwhich is also known as raster order. Chenet al. also considered the\nBERT objective, which samples a sub-sequence M ⊂ [1, n] such\nthat each index i independently has probability 0.15 of appearing\nin M. M is called the BERT mask, and the model is trained by\nminimizing the negative log-likelihood of the “masked” elements\nxM conditioned on the “unmasked” ones x[1,n]\\M :\nLBERT = E\nx∼X\nE\nM\nX\ni∈M\n[−log p(xi|x[1,n]\\M )]. (9)\nDuring the pre-training stage, they pick either LAR or LBERT\nand minimize the loss over the pre-training dataset.\nGPT-2 [110] formulation of the transformer decoder block\nis used. To ensure proper conditioning when training the AR\nobjective, Chen et al. apply the standard upper triangular mask\nto the n × n matrix of attention logits. No attention logit masking\nis required when the BERT objective is used: Chen et al. zero\nout the positions after the content embeddings are applied to the\ninput sequence. Following the final transformer layer, they apply a\nlayer norm and learn a projection from the output to logits param-\neterizing the conditional distributions at each sequence element.\nWhen training BERT, they simply ignore the logits at unmasked\npositions.\nDuring the fine-tuning stage, they average pool the output of\nthe final layer normalization layer across the sequence dimension\nA SUBMISSION TO IEEE TRANSACTION ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 7\nto extract a d-dimensional vector of features per example. They\nlearn a projection from the pooled feature to class logits and\nuse this projection to minimize a cross entropy loss. Practical\napplications offer empirical evidence that the joint objective of\ncross entropy loss and pretraining loss ( LAR or LBERT ) works\neven better. After iGPT, masked image modeling is proposed such\nas MAE [32] and SimMIM [111] which achieves competitive\nperformance on downstream tasks.\niGPT and ViT are two pioneering works to apply transformer\nfor visual tasks. The difference of iGPT and ViT-like models\nmainly lies on 3 aspects: 1) The input of iGPT is a sequence of\ncolor palettes by clustering pixels, while ViT uniformly divided\nthe image into a number of local patches; 2) The architec-\nture of iGPT is an encoder-decoder framework, while ViT only\nhas transformer encoder; 3) iGPT utilizes auto-regressive self-\nsupervised loss for training, while ViT is trained by supervised\nimage classification task.\nContrastive Learning Based Approach. Currently, contrastive\nlearning is the most popular manner of self-supervised learning for\ncomputer vision. Contrastive learning has been applied on vision\ntransformer for unsupervised pretraining [31], [112], [113].\nChen et al. [31] investigate the effects of several fundamental\ncomponents for training self-supervised ViT. The authors observe\nthat instability is a major issue that degrades accuracy, and these\nresults are indeed partial failure and they can be improved when\ntraining is made more stable.\nThey introduce a “MoCo v3” framework, which is an incre-\nmental improvement of MoCo [114]. Specifically, the authors take\ntwo crops for each image under random data augmentation. They\nare encodes by two encoders, fq and fk, with output vectors\nq and k. Intuitively, q behaves like a “query” and the goal of\nlearning is to retrieve the corresponding “key”. This is formulated\nas minimizing a contrastive loss function, which can be written as:\nLq = −log exp(q · k+/τ)\nexp(q · k+/τ) +P\nk− exp(q · k−/τ). (10)\nHere k+ is fk’s output on the same image as q, known as q’s\npositive sample. The set k− consists of fk’s outputs from other\nimages, known as q’s negative samples. τ is a temperature hyper-\nparameter for l2-normalized q, k. MoCo v3 uses the keys that nat-\nurally co-exist in the same batch and abandon the memory queue,\nwhich they find has diminishing gain if the batch is sufficiently\nlarge (e.g., 4096). With this simplification, the contrastive loss can\nbe implemented in a simple way. The encoder fq consists of a\nbackbone ( e.g., ViT), a projection head and an extra prediction\nhead; while the encoder fk has the backbone and projection head,\nbut not the prediction head. fk is updated by the moving-average\nof fq, excluding the prediction head.\nMoCo v3 shows that the instability is a major issue of training\nthe self-supervised ViT, thus they describe a simple trick that can\nimprove the stability in various cases of the experiments. They\nobserve that it is not necessary to train the patch projection layer.\nFor the standard ViT patch size, the patch projection matrix is\ncomplete or over-complete. And in this case, random projection\nshould be sufficient to preserve the information of the original\npatches. However, the trick alleviates the issue, but does not solve\nit. The model can still be unstable if the learning rate is too big and\nthe first layer is unlikely the essential reason for the instability.\n3.1.4 Discussions\nAll of the components of vision transformer including multi-\nhead self-attention, multi-layer perceptron, shortcut connection,\nlayer normalization, positional encoding and network topology,\nplay key roles in visual recognition. As stated above, a number\nof works have been proposed to improve the effectiveness and\nefficiency of vision transformer. From the results in Figure 6,\nwe can see that combining CNN and transformer achieve the\nbetter performance, indicating their complementation to each other\nthrough local connection and global connection. Further investi-\ngation on backbone networks can lead to the improvement for the\nwhole vision community. As for the self-supervised representation\nlearning for vision transformer, we still need to make effort to\npursue the success of large-scale pretraining in the filed of NLP.\n3.2 High/Mid-level Vision\nRecently there has been growing interest in using transformer\nfor high/mid-level computer vision tasks, such as object detec-\ntion [16], [17], [115], [116], [117], lane detection [118], segmen-\ntation [34], [25], [18] and pose estimation [35], [36], [37], [119].\nWe review these methods in this section.\n3.2.1 Generic Object Detection\nTraditional object detectors are mainly built upon CNNs, but\ntransformer-based object detection has gained significant interest\nrecently due to its advantageous capability.\nSome object detection methods have attempted to use trans-\nformer’s self-attention mechanism and then enhance the spe-\ncific modules for modern detectors, such as feature fusion\nmodule [120] and prediction head [121]. We discuss this in\nthe supplemental material. Transformer-based object detection\nmethods are broadly categorized into two groups: transformer-\nbased set prediction methods [16], [17], [122], [123], [124] and\ntransformer-based backbone methods [115], [117], as shown in\nFig. 7. Transformer-based methods have shown strong perfor-\nmance compared with CNN-based detectors, in terms of both\naccuracy and running speed. Table 3 shows the detection results\nfor different transformer-based object detectors mentioned earlier\non the COCO 2012 val set.\nImage Transformer \nEncoders\nTransformer \nDecoders\npatches\nSet prediction\nCNN FFN\nclass box\nRPN\nPredict\nHead\nclass\nboxImage Transformer \nEncoders\n(b) Transformer-based backbone for detection\n(a) Transformer-based set prediction for detection\nPositional\nEncoding\nPositional\nEncoding\nObject \nQueries\nFig. 7: General framework of transformer-based object detection.\nTransformer-based Set Prediction for Detection . As a pioneer\nfor transformer-based detection method, the detection transformer\n(DETR) proposed by Carion et al. [16] redesigns the framework\nof object detection. DETR, a simple and fully end-to-end object\ndetector, treats the object detection task as an intuitive set pre-\ndiction problem, eliminating traditional hand-crafted components\nsuch as anchor generation and non-maximum suppression (NMS)\npost-processing. As shown in Fig. 8, DETR starts with a CNN\nA SUBMISSION TO IEEE TRANSACTION ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 8\nbackbone to extract features from the input image. To supplement\nthe image features with position information, fixed positional en-\ncodings are added to the flattened features before the features are\nfed into the encoder-decoder transformer. The decoder consumes\nthe embeddings from the encoder along with N learned positional\nencodings (object queries), and produces N output embeddings.\nHere N is a predefined parameter and typically larger than the\nnumber of objects in an image. Simple feed-forward networks\n(FFNs) are used to compute the final predictions, which include\nthe bounding box coordinates and class labels to indicate the\nspecific class of object (or to indicate that no object exists). Unlike\nthe original transformer, which computes predictions sequentially,\nDETR decodes N objects in parallel. DETR employs a bipartite\nmatching algorithm to assign the predicted and ground-truth\nobjects. As shown in Eq. 11, the Hungarian loss is exploited to\ncompute the loss function for all matched pairs of objects.\nLHungarian(y,ˆy) =\nNX\ni=1\nh\n−log ˆpˆσ(i)(ci) +1{ci̸=∅}Lbox(bi,ˆbˆσ(i))\ni\n, (11)\nwhere ˆσ is the optimal assignment, ci and ˆpˆσ(i)(ci) are the target\nclass label and predicted label, respectively, and bi and ˆbˆσ(i) are\nthe ground truth and predicted bounding box, y = {(ci, bi)}\nand ˆy are the ground truth and prediction of objects, respectively.\nDETR shows impressive performance on object detection, deliv-\nering comparable accuracy and speed with the popular and well-\nestablished Faster R-CNN [13] baseline on COCO benchmark.\nCNN\nset of image features\ntransformer \nencoder\n…\n…\npositional encoding\n+ transformer \ndecoder\nclass,\nbox\nclass,\nbox\nno \nobject\nno \nobject\nFFN\nFFN\nFFN\nFFN\nobject queries\nbackbone encoder decoder prediction heads\nFig. 8: The overall architecture of DETR (image from [16]).\nDETR is a new design for the object detection framework\nbased on transformer and empowers the community to develop\nfully end-to-end detectors. However, the vanilla DETR poses\nseveral challenges, specifically, longer training schedule and poor\nperformance for small objects. To address these challenges, Zhuet\nal. [17] proposed Deformable DETR, which has become a popular\nmethod that significantly improves the detection performance. The\ndeformable attention module attends to a small set of key positions\naround a reference point rather than looking at all spatial locations\non image feature maps as performed by the original multi-head\nattention mechanism in transformer. This approach significantly\nreduces the computational complexity and brings benefits in terms\nof fast convergence. More importantly, the deformable attention\nmodule can be easily applied for fusing multi-scale features.\nDeformable DETR achieves better performance than DETR with\n10× less training cost and 1.6× faster inference speed. And by\nusing an iterative bounding box refinement method and two-stage\nscheme, Deformable DETR can further improve the detection\nperformance.\nThere are also several methods to deal with the slow conver-\ngence problem of the original DETR. For example, Sunet al. [122]\ninvestigated why the DETR model has slow convergence and\ndiscovered that this is mainly due to the cross-attention module\nin the transformer decoder. To address this issue, an encoder-only\nversion of DETR is proposed, achieving considerable improve-\nment in terms of detection accuracy and training convergence. In\naddition, a new bipartite matching scheme is designed for greater\ntraining stability and faster convergence and two transformer-\nbased set prediction models, i.e. TSP-FCOS and TSP-RCNN, are\nproposed to improve encoder-only DETR with feature pyramids.\nThese new models achieve better performance compared with the\noriginal DETR model. Gao et al. [125] proposed the Spatially\nModulated Co-Attention (SMCA) mechanism to accelerate the\nconvergence by constraining co-attention responses to be high\nnear initially estimated bounding box locations. By integrating\nthe proposed SMCA module into DETR, similar mAP could be\nobtained with about 10 × less training epochs under comparable\ninference cost.\nGiven the high computation complexity associated with\nDETR, Zheng et al. [123] proposed an Adaptive Clustering\nTransformer (ACT) to reduce the computation cost of pre-trained\nDETR. ACT adaptively clusters the query features using a locality\nsensitivity hashing (LSH) method and broadcasts the attention\noutput to the queries represented by the selected prototypes. ACT\nis used to replace the self-attention module of the pre-trained\nDETR model without requiring any re-training. This approach\nsignificantly reduces the computational cost while the accuracy\nslides slightly. The performance drop can be further reduced by\nutilizing a multi-task knowledge distillation (MTKD) method,\nwhich exploits the original transformer to distill the ACT module\nwith a few epochs of fine-tuning. Yao et al. [126] pointed out\nthat the random initialization in DETR is the main reason for\nthe requirement of multiple decoder layers and slow convergence.\nTo this end, they proposed the Efficient DETR to incorporate\nthe dense prior into the detection pipeline via an additional\nregion proposal network. The better initialization enables them\nto use only one decoder layers instead of six layers to achieve\ncompetitive performance with a more compact network.\nTransformer-based Backbone for Detection . Unlike DETR\nwhich redesigns object detection as a set prediction tasks via\ntransformer, Beal et al. [115] proposed to utilize transformer as\na backbone for common detection frameworks such as Faster R-\nCNN [13]. The input image is divided into several patches and\nfed into a vision transformer, whose output embedding features\nare reorganized according to spatial information before passing\nthrough a detection head for the final results. A massive pre-\ntraining transformer backbone could bring benefits to the proposed\nViT-FRCNN. There are also quite a few methods to explore versa-\ntile vision transformer backbone design [29], [73], [61], [63] and\ntransfer these backbones to traditional detection frameworks like\nRetinaNet [129] and Cascade R-CNN [130]. For example, Swin\nTransformer [61] obtains about 4 box AP gains over ResNet-50\nbackbone with similar FLOPs for various detection frameworks.\nPre-training for Transformer-based Object Detection. Inspired\nby the pre-training transformer scheme in NLP, several methods\nhave been proposed to explore different pre-training scheme\nfor transformer-based object detection [33], [128], [131]. Dai et\nal. [33] proposed unsupervised pre-training for object detection\n(UP-DETR). Specifically, a novel unsupervised pretext task named\nrandom query patch detection is proposed to pre-train the DETR\nmodel. With this unsupervised pre-training scheme, UP-DETR\nsignificantly improves the detection accuracy on a relatively small\ndataset (PASCAL VOC). On the COCO benchmark with sufficient\ntraining data, UP-DETR still outperforms DETR, demonstrating\nthe effectiveness of the unsupervised pre-training scheme.\nFang et al. [128] explored how to transfer the pure ViT\nstructure that is pre-trained on ImageNet to the more challenging\nA SUBMISSION TO IEEE TRANSACTION ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 9\nTABLE 3: Comparison of different transformer-based object detectors on COCO 2017 val set. Running speed (FPS) is evaluated on an\nNVIDIA Tesla V100 GPU as reported in [17]. †Estimated speed according to the reported number in the paper. ‡ViT backbone is pre-trained\non ImageNet-21k. ∗ViT backbone is pre-trained on an private dataset with 1.3 billion images.\nMethod Epochs AP AP 50 AP75 APS APM APL #Params (M) GFLOPs FPS\nCNN based\nFCOS [127] 36 41.0 59.8 44.1 26.2 44.6 52.2 - 177 23 †\nFaster R-CNN + FPN [13] 109 42.0 62.1 45.5 26.6 45.4 53.4 42 180 26\nCNN Backbone + Transformer Head\nDETR [16] 500 42.0 62.4 44.2 20.5 45.8 61.1 41 86 28\nDETR-DC5 [16] 500 43.3 63.1 45.9 22.5 47.3 61.1 41 187 12\nDeformable DETR [17] 50 46.2 65.2 50.0 28.8 49.2 61.7 40 173 19\nTSP-FCOS [122] 36 43.1 62.3 47.0 26.6 46.8 55.9 - 189 20 †\nTSP-RCNN [122] 96 45.0 64.5 49.6 29.7 47.7 58.0 - 188 15 †\nACT+MKKD (L=32) [123] - 43.1 - - 61.4 47.1 22.2 - 169 14 †\nSMCA [125] 108 45.6 65.5 49.1 25.9 49.3 62.6 - - -\nEfficient DETR [126] 36 45.1 63.1 49.1 28.3 48.4 59.0 35 210 -\nUP-DETR [33] 150 40.5 60.8 42.6 19.0 44.4 60.0 41 - -\nUP-DETR [33] 300 42.8 63.0 45.3 20.8 47.1 61.7 41 - -\nTransformer Backbone + CNN Head\nViT-B/16-FRCNN ‡ [115] 21 36.6 56.3 39.3 17.4 40.0 55.5 - - -\nViT-B/16-FRCNN ∗ [115] 21 37.8 57.4 40.1 17.8 41.4 57.3 - - -\nPVT-Small+RetinaNet [73] 12 40.4 61.3 43.0 25.0 42.9 55.7 34.2 118 -\nTwins-SVT-S+RetinaNet [63] 12 43.0 64.2 46.3 28.0 46.4 57.5 34.3 104 -\nSwin-T+RetinaNet [61] 12 41.5 62.1 44.2 25.1 44.9 55.5 38.5 118 -\nSwin-T+ATSS [61] 36 47.2 66.5 51.3 - - - 36 215 -\nPure Transformer based\nPVT-Small+DETR [73] 50 34.7 55.7 35.4 12.0 36.4 56.7 40 - -\nTNT-S+DETR [29] 50 38.2 58.9 39.4 15.5 41.1 58.8 39 - -\nYOLOS-Ti [128] 300 30.0 - - - - - 6.5 21 -\nYOLOS-S [128] 150 37.6 57.6 39.2 15.9 40.2 57.3 28 179 -\nYOLOS-B [128] 150 42.0 62.2 44.5 19.5 45.3 62.1 127 537 -\nobject detection task and proposed the YOLOS detector. To cope\nwith the object detection task, the proposed YOLOS first drops\nthe classification tokens in ViT and appends learnable detection\ntokens. Besides, the bipartite matching loss is utilized to perform\nset prediction for objects. With this simple pre-training scheme\non ImageNet dataset, the proposed YOLOS shows competitive\nperformance for object detection on COCO benchmark.\n3.2.2 Segmentation\nSegmentation is an important topic in computer vision community,\nwhich broadly includes panoptic segmentation, instance segmen-\ntation and semantic segmentation etc. Vision transformer has also\nshown impressive potential on the field of segmentation.\nTransformer for Panoptic Segmentation. DETR [16] can be\nnaturally extended for panoptic segmentation tasks and achieve\ncompetitive results by appending a mask head on the decoder.\nWang et al. [25] proposed Max-DeepLab to directly predict\npanoptic segmentation results with a mask transformer, without\ninvolving surrogate sub-tasks such as box detection. Similar to\nDETR, Max-DeepLab streamlines the panoptic segmentation tasks\nin an end-to-end fashion and directly predicts a set of non-\noverlapping masks and corresponding labels. Model training is\nperformed using a panoptic quality (PQ) style loss, but unlike prior\nmethods that stack a transformer on top of a CNN backbone, Max-\nDeepLab adopts a dual-path framework that facilitates combining\nthe CNN and transformer.\nTransformer for Instance Segmentation. VisTR, a transformer-\nbased video instance segmentation model, was proposed by\nWang et al. [34] to produce instance prediction results from\na sequence of input images. A strategy for matching instance\nsequence is proposed to assign the predictions with ground truths.\nIn order to obtain the mask sequence for each instance, VisTR\nutilizes the instance sequence segmentation module to accumulate\nthe mask features from multiple frames and segment the mask\nsequence with a 3D CNN. Hu et al. [132] proposed an instance\nsegmentation Transformer (ISTR) to predict low-dimensional\nmask embeddings, and match them with ground truth for the set\nloss. ISTR conducted detection and segmentation with a recurrent\nrefinement strategy which is different from the existing top-down\nand bottom-up frameworks. Yang et al. [133] investigated how\nto realize better and more efficient embedding learning to tackle\nthe semi-supervised video object segmentation under challenging\nmulti-object scenarios. Some papers such as [134], [135] also\ndiscussed using Transformer to deal with segmentation task.\nTransformer for Semantic Segmentation. Zheng et al. [18]\nproposed a transformer-based semantic segmentation network\n(SETR). SETR utilizes an encoder similar to ViT [15] as the\nencoder to extract features from an input image. A multi-level\nfeature aggregation module is adopted for performing pixel-wise\nsegmentation. Strudel et al. [136] introduced Segmenter which\nrelies on the output embedding corresponding to image patches\nand obtains class labels with a point-wise linear decoder or a mask\ntransformer decoder. Xie et al. [137] proposed a simple, efficient\nyet powerful semantic segmentation framework which unifies\nTransformers with lightweight multilayer perception (MLP) de-\ncoders, which outputs multiscale features and avoids complex\ndecoders.\nTransformer for Medical Image Segmentation. Cao et al. [30]\nproposed an Unet-like pure Transformer for medical image seg-\nmentation, by feeding the tokenized image patches into the\nTransformer-based U-shaped Encoder-Decoder architecture with\nskip-connections for local-global semantic feature learning. Vala-\nnarasu et al. [138] explored transformer-based solutions and study\nthe feasibility of using transformer-based network architectures\nfor medical image segmentation tasks and proposed a Gated\nAxial-Attention model which extends the existing architectures by\nintroducing an additional control mechanism in the self-attention\nmodule. Cell-DETR [139], based on the DETR panoptic segmen-\ntation model, is an attempt to use transformer for cell instance seg-\nmentation. It adds skip connections that bridge features between\nthe backbone CNN and the CNN decoder in the segmentation\nA SUBMISSION TO IEEE TRANSACTION ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 10\nhead in order to enhance feature fusion. Cell-DETR achieves\nstate-of-the-art performance for cell instance segmentation from\nmicroscopy imagery.\n3.2.3 Pose Estimation\nHuman pose and hand pose estimation are foundational topics that\nhave attracted significant interest from the research community.\nArticulated pose estimation is akin to a structured prediction task,\naiming to predict the joint coordinates or mesh vertices from input\nRGB/D images. Here we discuss some methods [35], [36], [37],\n[119] that explore how to utilize transformer for modeling the\nglobal structure information of human poses and hand poses.\nTransformer for Hand Pose Estimation. Huang et al. [35] pro-\nposed a transformer based network for 3D hand pose estimation\nfrom point sets. The encoder first utilizes a PointNet [140] to\nextract point-wise features from input point clouds and then adopts\nstandard multi-head self-attention module to produce embeddings.\nIn order to expose more global pose-related information to the\ndecoder, a feature extractor such as PointNet++ [141] is used\nto extract hand joint-wise features, which are then fed into the\ndecoder as positional encodings. Similarly, Huang et al. [36]\nproposed HOT-Net (short for hand-object transformer network)\nfor 3D hand-object pose estimation. Unlike the preceding method\nwhich employs transformer to directly predict 3D hand pose from\ninput point clouds, HOT-Net uses a ResNet to generate initial 2D\nhand-object pose and then feeds it into a transformer to predict\nthe 3D hand-object pose. A spectral graph convolution network\nis therefore used to extract input embeddings for the encoder.\nHampali et al. [142] proposed to estimate the 3D poses of two\nhands given a single color image. Specifically, appearance and\nspatial encodings of a set of potential 2D locations for the joints\nof both hands were inputted to a transformer, and the attention\nmechanisms were used to sort out the correct configuration of the\njoints and outputted the 3D poses of both hands.\nTransformer for Human Pose Estimation. Lin et al. [37]\nproposed a mesh transformer (METRO) for predicting 3D human\npose and mesh from a single RGB image. METRO extracts\nimage features via a CNN and then perform position encoding\nby concatenating a template human mesh to the image feature. A\nmulti-layer transformer encoder with progressive dimensionality\nreduction is proposed to gradually reduce the embedding dimen-\nsions and finally produce 3D coordinates of human joint and mesh\nvertices. To encourage the learning of non-local relationships be-\ntween human joints, METRO randomly mask some input queries\nduring training. Yanget al. [119] constructed an explainable model\nnamed TransPose based on Transformer architecture and low-level\nconvolutional blocks. The attention layers built in Transformer can\ncapture long-range spatial relationships between keypoints and ex-\nplain what dependencies the predicted keypoints locations highly\nrely on. Li et al. [143] proposed a novel approach based on Token\nrepresentation for human Pose estimation (TokenPose). Each key-\npoint was explicitly embedded as a token to simultaneously learn\nconstraint relationships and appearance cues from images. Mao et\nal. [144] proposed a human pose estimation framework that solved\nthe task in the regression-based fashion. They formulated the pose\nestimation task into a sequence prediction problem and solve it by\ntransformers, which bypass the drawbacks of the heatmap-based\npose estimator. Jiang et al. [145] proposed a novel transformer\nbased network that can learn a distribution over both pose and\nmotion in an unsupervised fashion rather than tracking body parts\nand trying to temporally smooth them. The method overcame\ninaccuracies in detection and corrected partial or entire skeleton\ncorruption. Hao et al. [146] proposed to personalize a human pose\nestimator given a set of test images of a person without using\nany manual annotations. The method adapted the pose estimator\nduring test time to exploit person-specific information, and used\na Transformer model to build a transformation between the self-\nsupervised keypoints and the supervised keypoints.\n3.2.4 Other Tasks\nThere are also quite a lot different high/mid-level vision tasks\nthat have explored the usage of vision transformer for better\nperformance. We briefly review several tasks below.\nPedestrian Detection. Because the distribution of objects is very\ndense in occlusion and crowd scenes, additional analysis and\nadaptation are often required when common detection networks\nare applied to pedestrian detection tasks. Lin et al. [147] revealed\nthat sparse uniform queries and a weak attention field in the\ndecoder result in performance degradation when directly applying\nDETR or Deformable DETR to pedestrian detection tasks. To\nalleviate these drawbacks, the authors proposes Pedestrian End-\nto-end Detector (PED), which employs a new decoder called\nDense Queries and Rectified Attention field (DQRF) to support\ndense queries and alleviate the noisy or narrow attention field\nof the queries. They also proposed V-Match, which achieves\nadditional performance improvements by fully leveraging visible\nannotations.\nLane Detection. Based on PolyLaneNet [148], Liu et al. [118]\nproposed a method called LSTR, which improves performance\nof curve lane detection by learning the global context with\na transformer network. Similar to PolyLaneNet, LSTR regards\nlane detection as a task of fitting lanes with polynomials and\nuses neural networks to predict the parameters of polynomials.\nTo capture slender structures for lanes and the global context,\nLSTR introduces a transformer network into the architecture. This\nenables processing of low-level features extracted by CNNs. In ad-\ndition, LSTR uses Hungarian loss to optimize network parameters.\nAs demonstrated in [118], LSTR outperforms PolyLaneNet, with\n2.82% higher accuracy and 3.65× higher FPS using 5-times fewer\nparameters. The combination of a transformer network, CNN and\nHungarian Loss culminates in a lane detection framework that\nis precise, fast, and tiny. Considering that the entire lane line\ngenerally has an elongated shape and long-range, Liu et al. [149]\nutilized a transformer encoder structure for more efficient context\nfeature extraction. This transformer encoder structure improves\nthe detection of the proposal points a lot, which rely on contextual\nfeatures and global information, especially in the case where the\nbackbone network is a small model.\nScene Graph. Scene graph is a structured representation of a\nscene that can clearly express the objects, attributes, and rela-\ntionships between objects in the scene [150]. To generate scene\ngraph, most of existing methods first extract image-based object\nrepresentations and then do message propagation between them.\nGraph R-CNN [151] utilizes self-attention to integrate contextual\ninformation from neighboring nodes in the graph. Recently, Shar-\nifzadeh et al. [152] employed transformers over the extracted\nobject embedding. Sharifzadeh et al. [153] proposed a new\npipeline called Texema and employed a pre-trained Text-to-Text\nTransfer Transformer (T5) [154] to create structured graphs from\ntextual input and utilized them to improve the relational reasoning\nmodule. The T5 model enables Texema to utilize the knowledge\nin texts.\nA SUBMISSION TO IEEE TRANSACTION ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 11\nTracking. Some researchers also explored to use transformer\nencoder-decoder architecture in template-based discriminative\ntrackers, such as TMT [155], TrTr [156] and TransT [157]. All\nthese work use a Siamese-like tracking pipeline to do video\nobject tracking and utilize the encoder-decoder network to re-\nplace explicit cross-correlation operation for global and rich\ncontextual inter-dependencies. Specifically, the transformer en-\ncoder and decoder are assigned to the template branch and the\nsearching branch, respectively. In addition, Sun et al. proposed\nTransTrack [158], which is an online joint-detection-and-tracking\npipeline. It utilizes the query-key mechanism to track pre-existing\nobjects and introduces a set of learned object queries into the\npipeline to detect new-coming objects. The proposed TransTrack\nachieves 74.5% and 64.5% MOTA on MOT17 and MOT20 bench-\nmark.\nRe-Identification. He et al. [159] proposed TransReID to inves-\ntigate the application of pure transformers in the field of object\nre-identification (ReID). While introducing transformer network\ninto object ReID, TransReID slices with overlap to reserve local\nneighboring structures around the patches and introduces 2D bilin-\near interpolation to help handle any given input resolution. With\nthe transformer module and the loss function, a strong baseline\nwas proposed to achieve comparable performance with CNN-\nbased frameworks. Moreover, The jigsaw patch module (JPM)\nwas designed to facilitate perturbation-invariant and robust feature\nrepresentation of objects and the side information embeddings\n(SIE) was introduced to encode side information. The final frame-\nwork TransReID achieves state-of-the-art performance on both\nperson and vehicle ReID benchmarks. Both Liu et al. [160] and\nZhang et al. [161] provided solutions for introducing transformer\nnetwork into video-based person Re-ID. And similarly, both of the\nthem utilized separated transformer networks to refine spatial and\ntemporal features, and then utilized a cross view transformer to\naggregate multi-view features.\nPoint Cloud Learning. A number of other works exploring\ntransformer architecture for point cloud learning [162], [163],\n[164] have also emerged recently. For example, Guo et al. [163]\nproposed a novel framework that replaces the original self-\nattention module with a more suitable offset-attention module,\nwhich includes implicit Laplace operator and normalization refine-\nment. In addition, Zhao et al. [164] designed a novel transformer\narchitecture called Point Transformer. The proposed self-attention\nlayer is invariant to the permutation of the point set, making it\nsuitable for point set processing tasks. Point Transformer shows\nstrong performance for semantic segmentation task from 3D point\nclouds.\n3.2.5 Discussions\nAs discussed in the preceding sections, transformers have shown\nstrong performance on several high-level tasks, including detec-\ntion, segmentation and pose estimation. The key issues that need to\nbe resolved before transformer can be adopted for high-level tasks\nrelate to input embedding, position encoding, and prediction loss.\nSome methods propose improving the self-attention module from\ndifferent perspectives, for example, deformable attention [17],\nadaptive clustering [123] and point transformer [164]. Neverthe-\nless, exploration into the use of transformers for high-level vision\ntasks is still in the preliminary stages and so further research\nmay prove beneficial. For example, is it necessary to use feature\nextraction modules such as CNN and PointNet before transformer\nfor potential better performance? How can vision transformer be\nfully leveraged using large scale pre-training datasets as BERT\nand GPT-3 do in the NLP field? And is it possible to pre-train a\nsingle transformer model and fine-tune it for different downstream\ntasks with only a few epochs of fine-tuning? How to design more\npowerful architecture by incorporating prior knowledge of the\nspecific tasks? Several prior works have performed preliminary\ndiscussions for the aforementioned topics and We hope more\nfurther research effort is conducted into exploring more powerful\ntransformers for high-level vision.\n3.3 Low-level Vision\nFew works apply transformers on low-level vision fields, such\nas image super-resolution and generation. These tasks often take\nimages as outputs ( e.g., high-resolution or denoised images),\nwhich is more challenging than high-level vision tasks such as\nclassification, segmentation, and detection, whose outputs are\nlabels or boxes.\nnoise\nTransformer \nDecoders\npatches\nImages\n(a) Image Generation\n(GAN-based)\n(b) Image Generation \n(Transformer-based)\nImages\nTransformer \nDecoders\nImages CNN\nEncoder\nCNN\nDecoder\nTokens\nTraining Only\nFig. 9: A generic framework for transformer in image generation.\n3.3.1 Image Generation\nAn simple yet effective to apply transformer model to the image\ngeneration task is to directly change the architectures from CNNs\nto transformers, as shown in Figure 9 (a). Jianget al. [39] proposed\nTransGAN, which build GAN using the transformer architec-\nture. Since the it is difficult to generate high-resolution images\npixel-wise, a memory-friendly generator is utilized by gradually\nincreasing the feature map resolution at different stages. Corre-\nspondingly, a multi-scale discriminator is designed to handle the\nvarying size of inputs in different stages. Various training recipes\nare introduced including grid self-attention, data augmentation,\nrelative position encoding and modified normalization to stabilize\nthe training and improve its performance. Experiments on various\nbenchmark datasets demonstrate the effectiveness and potential\nof the transformer-based GAN model in image generation tasks.\nKwonjoon Lee et al. [165] proposed ViTGAN, which introduce\nseveral technique to both generator and discriminator to stabilize\nthe training procedure and convergence. Euclidean distance is\nintroduced for the self-attention module to enforce the Lips-\nchitzness of transformer discriminator. Self-modulated layernorm\nand implicit neural representation are proposed to enhance the\ntraining for the generator. As a result, ViTGAN is the first work\nto demonstrate transformer-based GANs can achieve comparable\nperformance to state-of-the-art CNN-based GANs.\nParmar et al. [27] proposed Image Transformer, taking the first\nstep toward generalizing the transformer model to formulate image\ntranslation and generation tasks in an auto-regressive manner.\nImage Transformer consists of two parts: an encoder for extracting\nA SUBMISSION TO IEEE TRANSACTION ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 12\nimage representation and a decoder to generate pixels. For each\npixel with value 0 − 255, a 256 × d dimensional embedding\nis learned for encoding each value into a d dimensional vector,\nwhich is fed into the encoder as input. The encoder and decoder\nadopt the same architecture as that in [9]. Each output pixel\nq′ is generated by calculating self-attention between the input\npixel q and previously generated pixels m1, m2, ...with position\nembedding p1, p2, .... For image-conditioned generation, such as\nsuper-resolution and inpainting, an encoder-decoder architecture is\nused, where the encoder’s input is the low-resolution or corrupted\nimages. For unconditional and class-conditional generation ( i.e.,\nnoise to image), only the decoder is used for inputting noise vec-\ntors. Because the decoder’s input is the previously generated pixels\n(involving high computation cost when producing high-resolution\nimages), a local self-attention scheme is proposed. This scheme\nuses only the closest generated pixels as input for the decoder,\nenabling Image Transformer to achieve performance on par with\nCNN-based models for image generation and translation tasks,\ndemonstrating the effectiveness of transformer-based models on\nlow-level vision tasks.\nSince it is difficult to directly generate high-resolution images\nby transformer models, Esser et al. [38] proposed Taming Trans-\nformer. Taming Transformer consists of two parts: a VQGAN\nand a transformer. VQGAN is a variant of VQV AE [166], which\nuses a discriminator and perceptual loss to improve the visual\nquality. Through VQGAN, the image can be represented by a\nseries of context-rich discrete vectors and therefore these vectors\ncan be easily predicted by a transformer model through an auto-\nregression way. The transformer model can learn the long-range\ninteractions for generating high-resolution images. As a result, the\nproposed Taming Transformer achieves state-of-the-art results on\na wide variety of image synthesis tasks.\nBesides image generation, DALL ·E [42] proposed the trans-\nformer model for text-to-image generation, which synthesizes\nimages according to the given captions. The whole framework\nconsists of two stages. In the first stage, a discrete V AE is\nutilized to learn the visual codebook. In the second stage, the\ntext is decoded by BPE-encode and the corresponding image\nis decoded by dV AE learned in the first stage. Then an auto-\nregression transformer is used to learn the prior between the\nencoded text and image. During the inference procedure, tokens\nof images are predicted by the transformer and decoded by the\nlearned decoder. The CLIP model [41] is introduced to rank\ngenerated samples. Experiments on text-to-image generation task\ndemonstrate the powerful ability of the proposed model. Note that\nour survey mainly focus on pure vision tasks, we do not include\nthe framework of DALL·E in Figure 9. The image generation has\nbeen pushed to a higher level with the introduction of diffusion\nmodel [167], such as DALLE2 [168] and Stable Diffusion [169].\n3.3.2 Image Processing\nA number of recent works eschew using each pixel as the input\nfor transformer models and instead use patches (set of pixels) as\ninput. For example, Yanget al. [40] proposed Texture Transformer\nNetwork for Image Super-Resolution (TTSR), using the trans-\nformer architecture in the reference-based image super-resolution\nproblem. It aims to transfer relevant textures from reference\nimages to low-resolution images. Taking a low-resolution image\nand reference image as the query Q and key K, respectively, the\nrelevance ri,j is calculated between each patch qi in Q and ki in\nK as:\nri,j =\n\u001c qi\n∥qi∥, ki\n∥ki∥\n\u001d\n. (12)\nA hard-attention module is proposed to select high-resolution\nfeatures V according to the reference image, so that the low-\nresolution image can be matched by using the relevance. The\nhard-attention map is calculated as:\nhi = arg max\nj\nri,j (13)\nThe most relevant reference patch is ti = vhi, where ti in\nT is the transferred features. A soft-attention module is then\nused to transfer V to the low-resolution feature. The transferred\nfeatures from the high-resolution texture image and the low-\nresolution feature are used to generate the output features of\nthe low-resolution image. By leveraging the transformer-based\narchitecture, TTSR can successfully transfer texture information\nfrom high-resolution reference images to low-resolution images in\nsuper-resolution tasks.\nReshape\nTransformer Encoder\nMulti-head Multi-tail\nFeatures\nFeatures\nFlatten features\nTask embedding\n…\nDenoising \nHead\nDeraining \nHead\nx2 Up \nHead\nx4 Up \nHead\n…\nx4 Up \nTail\nDenoising\nTail\nDeraining\nTail\nx2 Up \nTail…\n…\nTransformer Decoder\nFig. 10: Diagram of IPT architecture (image from [19]).\nDifferent from the preceding methods that use transformer\nmodels on single tasks, Chen et al. [19] proposed Image Pro-\ncessing Transformer (IPT), which fully utilizes the advantages\nof transformers by using large pre-training datasets. It achieves\nstate-of-the-art performance in several image processing tasks,\nincluding super-resolution, denoising, and deraining. As shown\nin Figure 10, IPT consists of multiple heads, an encoder, a\ndecoder, and multiple tails. The multi-head, multi-tail structure\nand task embeddings are introduced for different image processing\ntasks. The features are divided into patches, which are fed into\nthe encoder-decoder architecture. Following this, the outputs are\nreshaped to features with the same size. Given the advantages\nof pre-training transformer models on large datasets, IPT uses\nthe ImageNet dataset for pre-training. Specifically, images from\nthis dataset are degraded by manually adding noise, rain streaks,\nor downsampling in order to generate corrupted images. The\ndegraded images are used as inputs for IPT, while the original\nimages are used as the optimization goal of the outputs. A self-\nsupervised method is also introduced to enhance the generalization\nability of the IPT model. Once the model is trained, it is fine-\ntuned on each task by using the corresponding head, tail, and\ntask embedding. IPT largely achieves performance improvements\non image processing tasks ( e.g., 2 dB in image denoising tasks),\ndemonstrating the huge potential of applying transformer-based\nmodels to the field of low-level vision.\nBesides single image generation, Wang et al. [170] proposed\nSceneFormer to utilize transformer in 3D indoor scene generation.\nBy treating a scene as a sequence of objects, the transformer\nA SUBMISSION TO IEEE TRANSACTION ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 13\ndecoder can be used to predict series of objects and their location,\ncategory, and size. This has enabled SceneFormer to outperform\nconventional CNN-based methods in user studies.\nTransformer \nEncoders\nTransformer \nDecoders\nImagespatches\npatchesImages\nFig. 11: A generic framework for transformer in image processing.\nIt should be noted that iGPT [14] is pre-trained on an\ninpainting-like task. Since iGPT mainly focus on the fine-tuning\nperformance on image classification tasks, we treat this work more\nlike an attempt on image classification task using transformer than\nlow-level vision tasks.\nIn conclusion, different to classification and detection tasks,\nthe outputs of image generation and processing are images. Fig-\nure 11 illustrates using transformers in low-level vision. In image\nprocessing tasks, the images are first encoded into a sequence of\ntokens or patches and the transformer encoder uses the sequence\nas input, allowing the transformer decoder to successfully produce\ndesired images. In image generation tasks, the GAN-based models\ndirectly learn a decoder to generated patches to outputting images\nthrough linear projection, while the transformer-based models\ntrain a auto-encoder to learn a codebook for images and use an\nauto-regression transformer model to predict the encoded tokens.\nA meaningful direction for future research would be designing a\nsuitable architecture for different image processing tasks.\n3.4 Video Processing\nTransformer performs surprisingly well on sequence-based tasks\nand especially on NLP tasks. In computer vision (specifically,\nvideo tasks), spatial and temporal dimension information is fa-\nvored, giving rise to the application of transformer in a number\nof video tasks, such as frame synthesis [171], action recogni-\ntion [172], and video retrieval [173].\n3.4.1 High-level Video Processing\nVideo Action Recognition. Video human action tasks, as the\nname suggests, involves identifying and localizing human actions\nin videos. Context (such as other people and objects) plays a\ncritical role in recognizing human actions. Rohit et al. proposed\nthe action transformer [172] to model the underlying relationship\nbetween the human of interest and the surrounding context.\nSpecifically, the I3D [174] is used as the backbone to extract high-\nlevel feature maps. The features extracted (using RoI pooling)\nfrom intermediate feature maps are viewed as the query (Q), while\nthe key (K) and values (V) are calculated from the intermediate\nfeatures. A self-attention mechanism is applied to the three compo-\nnents, and it outputs the classification and regressions predictions.\nLohit et al. [175] proposed an interpretable differentiable module,\nnamed temporal transformer network, to reduce the intra-class\nvariance and increase the inter-class variance. In addition, Fayyaz\nand Gall proposed a temporal transformer [176] to perform action\nrecognition tasks under weakly supervised settings. In addition\nto human action recognition, transformer has been utilized for\ngroup activity recognition [177]. Gavrilyuk et al. proposed an\nactor-transformer [178] architecture to learn the representation,\nusing the static and dynamic representations generated by the 2D\nand 3D networks as input. The output of the transformer is the\npredicted activity.\nVideo Retrieval. The key to content-based video retrieval is to\nfind the similarity between videos. Leveraging only the image-\nlevel of video-level features to overcome the associated chal-\nlenges, Shao et al. [179] suggested using the transformer to model\nthe long-range semantic dependency. They also introduced the\nsupervised contrastive learning strategy to perform hard negative\nmining. The results of using this approach on benchmark datasets\ndemonstrate its performance and speed advantages. In addition,\nGabeur et al. [180] presented a multi-modal transformer to learn\ndifferent cross-modal cues in order to represent videos.\nVideo Object Detection. To detect objects in a video, both\nglobal and local information is required. Chen et al. introduced\nthe memory enhanced global-local aggregation (MEGA) [181]\nto capture more content. The representative features enhance the\noverall performance and address the ineffective and insufficient\nproblems. Furthermore, Yin et al. [182] proposed a spatiotem-\nporal transformer to aggregate spatial and temporal information.\nTogether with another spatial feature encoding component, these\ntwo components perform well on 3D video object detection tasks.\nMulti-task Learning. Untrimmed video usually contains many\nframes that are irrelevant to the target tasks. It is therefore\ncrucial to mine the relevant information and discard the redundant\ninformation. To extract such information, Seong et al. proposed\nthe video multi-task transformer network [183], which handles\nmulti-task learning on untrimmed videos. For the CoVieW dataset,\nthe tasks are scene recognition, action recognition and importance\nscore prediction. Two pre-trained networks on ImageNet and\nPlaces365 extract the scene features and object features. The\nmulti-task transformers are stacked to implement feature fusion,\nleveraging the class conversion matrix (CCM).\n3.4.2 Low-level Video Processing\nFrame/Video Synthesis. Frame synthesis tasks involve synthe-\nsizing the frames between two consecutive frames or after a\nframe sequence while video synthesis tasks involve synthesizing\na video. Liu et al. proposed the ConvTransformer [171], which is\ncomprised of five components: feature embedding, position encod-\ning, encoder, query decoder, and the synthesis feed-forward net-\nwork. Compared with LSTM based works, the ConvTransformer\nachieves superior results with a more parallelizable architecture.\nAnother transformer-based approach was proposed by Schatz et\nal. [184], which uses a recurrent transformer network to synthetize\nhuman actions from novel views.\nVideo Inpainting. Video inpainting tasks involve completing\nany missing regions within a frame. This is challenging, as it\nrequires information along the spatial and temporal dimensions\nto be merged. Zeng et al. proposed a spatial-temporal transformer\nnetwork [28], which uses all the input frames as input and fills\nthem in parallel. The spatial-temporal adversarial loss is used to\noptimize the transformer network.\n3.4.3 Discussions\nCompared to image, video has an extra dimension to encode\nthe temporal information. Exploiting both spatial and temporal\ninformation helps to have a better understanding of a video.\nThanks to the relationship modeling capability of transformer,\nA SUBMISSION TO IEEE TRANSACTION ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 14\nvideo processing tasks have been improved by mining spatial\nand temporal information simultaneously. Nevertheless, due to\nthe high complexity and much redundancy of video data, how\nto efficiently and accurately modeling both spatial and temporal\nrelationships is still an open problem.\n3.5 Multi-Modal Tasks\nOwing to the success of transformer across text-based NLP tasks,\nmany researches are keen to exploit its potential for processing\nmulti-modal tasks ( e.g., video-text, image-text and audio-text).\nOne example of this is VideoBERT [185], which uses a CNN-\nbased module to pre-process videos in order to obtain representa-\ntion tokens. A transformer encoder is then trained on these tokens\nto learn the video-text representations for downstream tasks, such\nas video caption. Some other examples include VisualBERT [186]\nand VL-BERT [187], which adopt a single-stream unified trans-\nformer to capture visual elements and image-text relationship for\ndownstream tasks such as visual question answering (VQA) and\nvisual commonsense reasoning (VCR). In addition, several studies\nsuch as SpeechBERT [188] explore the possibility of encoding\naudio and text pairs with a transformer encoder to process auto-\ntext tasks such as speech question answering (SQA).\nFig. 12: The framework of the CLIP (image from [41]).\nApart from the aforementioned pioneering multi-modal trans-\nformers, Contrastive Language-Image Pre-training (CLIP) [41]\ntakes natural language as supervision to learn more efficient image\nrepresentation. CLIP jointly trains a text encoder and an image\nencoder to predict the corresponding training text-image pairs.\nThe text encoder of CLIP is a standard transformer with masked\nself-attention used to preserve the initialization ability of the pre-\ntrained language models. For the image encoder, CLIP considers\ntwo types of architecture, ResNet and Vision Transformer. CLIP is\ntrained on a new dataset containing 400 million (image, text) pairs\ncollected from the Internet. More specifically, given a batch of N\n(image, text) pairs, CLIP learns both text and image embeddings\njointly to maximize the cosine similarity of those N matched\nembeddings while minimize N2 −N incorrectly matched embed-\ndings. On Zero-Shot transfer, CLIP demonstrates astonishing zero-\nshot classification performances, achieving 76.2% top-1 accuracy\non ImageNet-1K dataset without using any ImageNet training\nlabels. Concretely, at inference, the text encoder of CLIP first\ncomputes the feature embeddings of all ImageNet Labels and the\nimage encoder then computes the embeddings of all images. By\ncalculating the cosine similarity of text and image embeddings,\nthe text-image pair with the highest score should be the image and\nits corresponding label. Further experiments on 30 various CV\nbenchmarks show the zero-shot transfer ability of CLIP and the\nfeature diversity learned by CLIP.\nWhile CLIP maps images according to the description in text,\nanother work DALL-E [42] synthesizes new images of categories\ndescribed in an input text. Similar to GPT-3, DALL-E is a multi-\nmodal transformer with 12 billion model parameters autoregres-\nsively trained on a dataset of 3.3 million text-image pairs. More\nspecifically, to train DALL-E, a two-stage training procedure is\nused, where in stage 1, a discrete variational autoencoder is used\nto compress 256× 256 RGB images into 32×32 image tokens and\nthen in stage 2, an autoregressive transformer is trained to model\nthe joint distribution over the image and text tokens. Experimental\nresults show that DALL-E can generate images of various styles\nfrom scratch, including photorealistic imagery, cartoons and emoji\nor extend an existing image while still matching the description in\nthe text. Subsequently, Ding et al. proposes CogView [43], which\nis a transformer with VQ-V AE tokenizer similar to DALL-E, but\nsupports Chinese text input. They claim CogView outperforms\nDALL-E and previous GAN-bsed methods and also unlike DALL-\nE, CogView does not need an additional CLIP model to rerank the\nsamples drawn from transformer, i.e. DALL-E.\nRecently, a Unified Transformer (UniT) [189] model is pro-\nposed to cope with multi-modal multi-task learning, which can\nsimultaneously handle multiple tasks across different domains,\nincluding object detection, natural language understanding and\nvision-language reasoning. Specifically, UniT has two transformer\nencoders to handle image and text inputs, respectively, and then\nthe transformer decoder takes the single or concatenated encoder\noutputs according to the task modality. Finally, a task-specific\nprediction head is applied to the decoder outputs for different\ntasks. In the training stage, all tasks are jointly trained by randomly\nselecting a specific task within an iteration. The experiments show\nUniT achieves satisfactory performance on every task with a\ncompact set of model parameters.\nIn conclusion, current transformer-based mutil-modal models\ndemonstrates its architectural superiority for unifying data and\ntasks of various modalities, which demonstrates the potential of\ntransformer to build a general-purpose intelligence agents able to\ncope with vast amount of applications. Future researches can be\nconducted in exploring the effective training or the extendability\nof multi-modal transformers (e.g., GPT-4 [44]).\n3.6 Efficient Transformer\nAlthough transformer models have achieved success in various\ntasks, their high requirements for memory and computing re-\nsources block their implementation on resource-limited devices\nsuch as mobile phones. In this section, we review the researches\ncarried out into compressing and accelerating transformer models\nfor efficient implementation. This includes including network\npruning, low-rank decomposition, knowledge distillation, network\nquantization, and compact architecture design. Table 4 lists some\nrepresentative works for compressing transformer-based models.\n3.6.1 Pruning and Decomposition\nIn transformer based pre-trained models ( e.g., BERT), multiple\nattention operations are performed in parallel to independently\nmodel the relationship between different tokens [9], [10]. How-\never, specific tasks do not require all heads to be used. For\nexample, Michel et al. [45] presented empirical evidence that a\nlarge percentage of attention heads can be removed at test time\nwithout impacting performance significantly. The number of heads\nrequired varies across different layers — some layers may even\nrequire only one head. Considering the redundancy on attention\nheads, importance scores are defined to estimate the influence of\nA SUBMISSION TO IEEE TRANSACTION ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 15\neach head on the final output in [45], and unimportant heads can be\nremoved for efficient deployment. Dalvi et al. [190] analyzed the\nredundancy in pre-trained transformer models from two perspec-\ntives: general redundancy and task-specific redundancy. Following\nthe lottery ticket hypothesis [191], Prasanna et al. [190] analyzed\nthe lotteries in BERT and showed that good sub-networks also\nexist in transformer-based models, reducing both the FFN layers\nand attention heads in order to achieve high compression rates.\nFor the vision transformer [15] which splits an image to multiple\npatches, Tang et al. [192] proposed to reduce patch calculation\nto accelerate the inference, and the redundant patches can be\nautomatically discovered by considering their contributions to the\neffective output features. Zhu et al. [193] extended the network\nslimming approach [194] to vision transformers for reducing\nthe dimensions of linear projections in both FFN and attention\nmodules.\nIn addition to the width of transformer models, the depth\n(i.e., the number of layers) can also be reduced to accelerate the\ninference process [204], [205]. Differing from the concept that\ndifferent attention heads in transformer models can be computed in\nparallel, different layers have to be calculated sequentially because\nthe input of the next layer depends on the output of previous\nlayers. Fan et al. [204] proposed a layer-wisely dropping strategy\nto regularize the training of models, and then the whole layers are\nremoved together at the test phase.\nBeyond the pruning methods that directly discard modules in\ntransformer models, matrix decomposition aims to approximate\nthe large matrices with multiple small matrices based on the low-\nrank assumption. For example, Wang et al. [206] decomposed the\nstandard matrix multiplication in transformer models, improving\nthe inference efficiency.\n3.6.2 Knowledge Distillation\nKnowledge distillation aims to train student networks by trans-\nferring knowledge from large teacher networks [207], [208],\n[209]. Compared with teacher networks, student networks usually\nhave thinner and shallower architectures, which are easier to\nbe deployed on resource-limited resources. Both the output and\nintermediate features of neural networks can also be used to\ntransfer effective information from teachers to students. Focused\non transformer models, Mukherjee et al. [210] used the pre-trained\nBERT [10] as a teacher to guide the training of small models,\nleveraging large amounts of unlabeled data. Wanget al. [211] train\nthe student networks to mimic the output of self-attention layers\nin the pre-trained teacher models. The dot-product between values\nis introduced as a new form of knowledge for guiding students. A\nteacher’s assistant [212] is also introduced in [211], reducing the\ngap between large pre-trained transformer models and compact\nTABLE 4: List of representative compressed transformer-\nbased models. The data of the Table is from [203].\nModels Compress Type #Layer Params Speed Up\nBERTBASE [10] Baseline 12 110M ×1\nALBERT [195] Decomposition 12 12M ×5.6\nBERT- Architecture 6 66M ×1.94of-Theseus [196] design\nQ-BERT [197] Quantization 12 - -Q8BERT [198] 12\nTinyBERT [46]\nDistillation\n4 14.5M ×9.4\nDistilBERT [199] 6 6.6m ×1.63\nBERT-PKD [200] 3∼6 45.7∼67M ×3.73∼1.64\nMobileBERT [201] 24 25.3M ×4.0\nPD [202] 6 67.5M ×2.0\nstudent networks, thereby facilitating the mimicking process. Due\nto the various types of layers in the transformer model ( i.e., self-\nattention layer, embedding layer, and prediction layers), Jiao et\nal. [46] design different objective functions to transfer knowledge\nfrom teachers to students. For example, the outputs of student\nmodels’ embedding layers imitate those of teachers via MSE\nlosses. For the vision transformer, Jia et al. [213] proposed a fine-\ngrained manifold distillation method, which excavates effective\nknowledge through the relationship between images and the di-\nvided patches.\n3.6.3 Quantization\nQuantization aims to reduce the number of bits needed to represent\nnetwork weight or intermediate features [214], [215]. Quantization\nmethods for general neural networks have been discussed at length\nand achieve performance on par with the original networks [216],\n[217], [218]. Recently, there has been growing interest in how\nto specially quantize transformer models [219], [220]. For ex-\nample, Shridhar et al. [221] suggested embedding the input into\nbinary high-dimensional vectors, and then using the binary input\nrepresentation to train the binary neural networks. Cheong et\nal. [222] represented the weights in the transformer models by\nlow-bit (e.g., 4-bit) representation. Zhao et al. [223] empirically\ninvestigated various quantization methods and showed that k-\nmeans quantization has a huge development potential. Aimed\nat machine translation tasks, Prato et al. [47] proposed a fully\nquantized transformer, which, as the paper claims, is the first 8-\nbit model not to suffer any loss in translation quality. Beside,\nLiu et al. [224] explored a post-training quantization scheme to\nreduce the memory storage and computational costs of vision\ntransformers.\n3.6.4 Compact Architecture Design\nBeyond compressing pre-defined transformer models into smaller\nones, some works attempt to design compact models di-\nrectly [225], [48]. Jiang et al. [48] simplified the calculation\nof self-attention by proposing a new module — called span-\nbased dynamic convolution — that combine the fully-connected\nlayers and the convolutional layers. Interesting “hamburger” layers\nare proposed in [226], using matrix decomposition to substitute\nthe original self-attention layers.Compared with standard self-\nattention operations, matrix decomposition can be calculated more\nefficiently while clearly reflecting the dependence between differ-\nent tokens. The design of efficient transformer architectures can\nalso be automated with neural architecture search (NAS) [227],\n[228], which automatically searches how to combine different\ncomponents. For example, Su et al. [83] searched patch size and\ndimensions of linear projections and head number of attention\nmodules to get an efficient vision transformer. Li et al. [229]\nexplored a self-supervised search strategy to get a hybrid architec-\nture composing of both convolutional modules and self-attention\nmodules.\nThe self-attention operation in transformer models calculates\nthe dot product between representations from different input to-\nkens in a given sequence (patches in image recognition tasks [15]),\nwhose complexity is O(N), where N is the length of the se-\nquence. Recently, there has been a targeted focus to reduce the\ncomplexity to O(N) in large methods so that transformer models\ncan scale to long sequences [230], [231], [232]. For example,\nKatharopoulos et al. [230] approximated self-attention as a linear\ndot-product of kernel feature maps and revealed the relationship\nA SUBMISSION TO IEEE TRANSACTION ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 16\nRedundant\nModel\nCompact\nModel\nPruning & Decomposition\nKnowledge Distillation\nQuantization\nExpertise \nor NAS\nArchitecture Design\nLow-Bit\nModel\nFig. 13: Different methods for compressing transformers.\nbetween tokens via RNNs. Zaheer et al. [232] considered each to-\nken as a vertex in a graph and defined the inner product calculation\nbetween two tokens as an edge. Inspired by graph theories [233],\n[234], various sparse graph are combined to approximate the dense\ngraph in transformer models, and can achieve O(N) complexity.\nDiscussion. The preceding methods take different approaches\nin how they attempt to identify redundancy in transformer mod-\nels (see Figure 13). Pruning and decomposition methods usually\nrequire pre-defined models with redundancy. Specifically, pruning\nfocuses on reducing the number of components ( e.g., layers,\nheads) in transformer models while decomposition represents an\noriginal matrix with multiple small matrices. Compact models\nalso can be directly designed either manually (requiring sufficient\nexpertise) or automatically (e.g., via NAS). The obtained compact\nmodels can be further represented with low-bits via quantization\nmethods for efficient deployment on resource-limited devices.\n4 C ONCLUSIONS AND DISCUSSIONS\nTransformer is becoming a hot topic in the field of computer\nvision due to its competitive performance and tremendous poten-\ntial compared with CNNs. To discover and utilize the power of\ntransformer, as summarized in this survey, a number of methods\nhave been proposed in recent years. These methods show excellent\nperformance on a wide range of visual tasks, including backbone,\nhigh/mid-level vision, low-level vision, and video processing.\nNevertheless, the potential of transformer for computer vision has\nnot yet been fully explored, meaning that several challenges still\nneed to be resolved. In this section, we discuss these challenges\nand provide insights on the future prospects.\n4.1 Challenges\nAlthough researchers have proposed many transformer-based\nmodels to tackle computer vision tasks, these works are only the\nfirst steps in this field and still have much room for improvement.\nFor example, the transformer architecture in ViT [15] follows\nthe standard transformer for NLP [9], but an improved version\nspecifically designed for CV remains to be explored. Moreover, it\nis necessary to apply transformer to more tasks other than those\nmentioned earlier.\nThe generalization and robustness of transformers for com-\nputer vision are also challenging. Compared with CNNs, pure\ntransformers lack some inductive biases and rely heavily on\nmassive datasets for large-scale training [15]. Consequently, the\nquality of data has a significant influence on the generalization\nand robustness of transformers. Although ViT shows exceptional\nperformance on downstream image classification tasks such as\nCIFAR [235] and VTAB [236], directly applying the ViT back-\nbone on object detection has failed to achieve better results than\nCNNs [115]. There is still a long way to go in order to better\ngeneralize pre-trained transformers on more generalized visual\ntasks. Practitioners concern the robustness of transformer (e.g.\nthe vulnerability issue [237]). Although the robustness has been\ninvestigated in [238], [239], [240], it is still an open problem\nwaiting to be solved.\nAlthough numerous works have explained the use of trans-\nformers in NLP [241], [242], it remains a challenging subject\nto clearly explain why transformer works well on visual tasks.\nThe inductive biases, including translation equivariance and lo-\ncality, are attributed to CNN’s success, but transformer lacks any\ninductive bias. The current literature usually analyzes the effect\nin an intuitive way [15], [243]. For example, Dosovitskiy et\nal. [15] claim that large-scale training can surpass inductive\nbias. Position embeddings are added into image patches to retain\npositional information, which is important in computer vision\ntasks. Inspired by the heavy parameter usage in transformers,\nover-parameterization [244], [245] may be a potential point to the\ninterpretability of vision transformers.\nLast but not least, developing efficient transformer models for\nCV remains an open problem. Transformer models are usually\nhuge and computationally expensive. For example, the base ViT\nmodel [15] requires 18 billion FLOPs to process an image. In\ncontrast, the lightweight CNN model GhostNet [246], [247] can\nachieve similar performance with only about 600 million FLOPs.\nAlthough several methods have been proposed to compress trans-\nformer, they remain highly complex. And these methods, which\nwere originally designed for NLP, may not be suitable for CV .\nConsequently, efficient transformer models are urgently needed\nso that vision transformer can be deployed on resource-limited\ndevices.\n4.2 Future Prospects\nIn order to drive the development of vision transformers, we\nprovide several potential directions for future study.\nOne direction is the effectiveness and the efficiency of trans-\nformers in computer vision. The goal is to develop highly ef-\nfective and efficient vision transformers; specifically, transformers\nwith high performance and low resource cost. The performance\ndetermines whether the model can be applied on real-world\napplications, while the resource cost influences the deployment\non devices [248], [249]. The effectiveness is usually correlated\nwith the efficiency, so determining how to achieve a better balance\nbetween them is a meaningful topic for future study.\nMost of the existing vision transformer models are designed to\nhandle only a single task. Many NLP models such as GPT-3 [11]\nhave demonstrated how transformer can deal with multiple tasks in\none model. IPT [19] in the CV field is also able to process multiple\nlow-level vision tasks, such as super-resolution, image denoising,\nand deraining. Perceiver [250] and Perceiver IO [251] are the\npioneering models that can work on several domains including\nimages, audio, multimodal, point clouds. We believe that more\ntasks can be involved in only one model. Unifying all visual\ntasks and even other tasks in one transformer (i.e., a grand unified\nmodel) is an exciting topic.\nThere have been various types of neural networks, such as\nCNN, RNN, and transformer. In the CV field, CNNs used to\nbe the mainstream choice [12], [94], but now transformer is\nbecoming popular. CNNs can capture inductive biases such as\ntranslation equivariance and locality, whereas ViT uses large-scale\ntraining to surpass inductive bias [15]. From the evidence currently\navailable [15], CNNs perform well on small datasets, whereas\nA SUBMISSION TO IEEE TRANSACTION ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 17\ntransformers perform better on large datasets. The question for the\nfuture is whether to use CNN or transformer.\nBy training with large datasets, transformers can achieve state-\nof-the-art performance on both NLP [11], [10] and CV bench-\nmarks [15]. It is possible that neural networks need big data rather\nthan inductive bias. In closing, we leave you with a question:\nCan transformer obtains satisfactory results with a very simple\ncomputational paradigm ( e.g., with only fully connected layers)\nand massive data training?\nACKNOWLEDGEMENT\nThis research is partially supported by MindSpore (https://\nmindspore.cn/) and CANN (Compute Architecture for Neural\nNetworks).\nAPPENDIX\nA1. General Formulation of Self-attention\nThe self-attention module [9] for machine translation computes the\nresponses at each position in a sequence by estimating attention\nscores to all positions and gathering the corresponding embed-\ndings based on the scores accordingly. This can be viewed as a\nform of non-local filtering operations [252], [253]. We follow the\nconvention [252] to formulate the self-attention module. Given an\ninput signal (e.g., image, sequence, video and feature) X ∈ Rn×d,\nwhere n = h ×w (indicating the number of pixels in feature) and\nd is the number of channels, the output signal is generated as:\nyi = 1\nC(xi)\nX\n∀j\nf(xi, xj)g(xj), (14)\nwhere xi ∈ R1×d and yi ∈ R1×d indicate the ith position\n(e.g., space, time and spacetime) of the input signal X and output\nsignal Y, respectively. Subscript j is the index that enumerates all\npositions, and a pairwise function f(·) computes a representing\nrelationship (such as affinity) between i and all j. The function\ng(·) computes a representation of the input signal at position j,\nand the response is normalized by a factor C(xi).\nNote that there are many choices for the pairwise function\nf(·). For example, a simple extension of the Gaussian function\ncould be used to compute the similarity in an embedding space.\nAs such, the function f(·) can be formulated as:\nf(xi, xj) =eθ(xi)ϕ(xj)T\n(15)\nwhere θ(·) and ϕ(·) can be any embedding layers. If we\nconsider the θ(·), ϕ(·), g(·) in the form of linear embedding:\nθ(X) = XWθ, ϕ(X) = XWϕ, g(X) = XWg where\nWθ ∈ Rd×dk , Wϕ ∈ Rd×dk , Wg ∈ Rd×dv , and set the\nnormalization factor as C(xi) = P\n∀j f(xi, xj), the Eq. 14 can\nbe rewritten as:\nyi = exiwθ,iwT\nϕ,jxT\nj\nP\nj exiwθ,iwT\nϕ,jxT\nj\nxjwg,j, (16)\nwhere wθ,i ∈ Rd×1 is the ith row of the weight matrix Wθ. For a\ngiven index i, 1\nC(xi) f(xi, xj) becomes the softmax output along\nthe dimension j. The formulation can be further rewritten as:\nY = softmax(XWθWT\nϕ X)g(X), (17)\nwhere Y ∈ Rn×c is the output signal of the same size as X.\nCompared with the query, key and value representations Q =\nXWq, K = XWk, V = XWv from the translation module,\nonce Wq = Wθ, Wk = Wϕ, Wv = Wg, Eq. 17 can be\nformulated as:\nY = softmax(QKT )V = Attention(Q, K, V), (18)\nThe self-attention module [9] proposed for machine translation\nis, to some extent, the same as the preceding non-local filtering\noperations proposed for computer vision.\nGenerally, the final output signal of the self-attention module\nfor computer vision will be wrapped as:\nZ = YWo + X (19)\nwhere Y is generated through Eq. 17. If Wo is initialized as zero,\nthis self-attention module can be inserted into any existing model\nwithout breaking its initial behavior.\nA2. Revisiting Transformers for NLP\nBefore transformer was developed, RNNs ( e.g., GRU [254]\nand LSTM [6]) with added attention [7] empowered most of\nthe state-of-the-art language models. However, RNNs require the\ninformation flow to be processed sequentially from the previous\nhidden states to the next one. This rules out the possibility of using\nacceleration and parallelization during training, and consequently\nhinders the potential of RNNs to process longer sequences or build\nlarger models. In 2017, Vaswani et al. [9] proposed transformer,\na novel encoder-decoder architecture built solely on multi-head\nself-attention mechanisms and feed-forward neural networks. Its\npurpose was to solve seq-to-seq natural language tasks ( e.g.,\nmachine translation) easily by acquiring global dependencies. The\nsubsequent success of transformer demonstrates that leveraging\nattention mechanisms alone can achieve performance comparable\nwith attentive RNNs. Furthermore, the architecture of transformer\nlends itself to massively parallel computing, which enables train-\ning on larger datasets. This has given rise to the surge of large\npre-trained models (PTMs) for natural language processing.\nBERT [10] and its variants ( e.g., SpanBERT [255],\nRoBERTa [256]) are a series of PTMs built on the multi-layer\ntransformer encoder architecture. Two tasks are conducted on\nBookCorpus [257] and English Wikipedia datasets at the pre-\ntraining stage of BERT: 1) Masked language modeling (MLM),\nwhich involves first randomly masking out some tokens in the\ninput and then training the model to predict; 2) Next sentence pre-\ndiction, which uses paired sentences as input and predicts whether\nthe second sentence is the original one in the document. After\npre-training, BERT can be fine-tuned by adding an output layer\non a wide range of downstream tasks. More specifically, when\nperforming sequence-level tasks ( e.g., sentiment analysis), BERT\nuses the representation of the first token for classification; for\ntoken-level tasks (e.g., name entity recognition), all tokens are fed\ninto the softmax layer for classification. At the time of its release,\nBERT achieved the state-of-the-art performance on 11 NLP tasks,\nsetting a milestone in pre-trained language models. Generative\nPre-trained Transformer models ( e.g., GPT [258], GPT-2 [110])\nare another type of PTMs based on the transformer decoder\narchitecture, which uses masked self-attention mechanisms. The\nmain difference between the GPT series and BERT is the way\nin which pre-training is performed. Unlike BERT, GPT models\nare unidirectional language models pre-trained using Left-to-Right\n(LTR) language modeling. Furthermore, BERT learns the sentence\nseparator ([SEP]) and classifier token ([CLS]) embeddings during\nA SUBMISSION TO IEEE TRANSACTION ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 18\npre-training, whereas these embeddings are involved in only the\nfine-tuning stage of GPT. Due to its unidirectional pre-training\nstrategy, GPT achieves superior performance in many natural\nlanguage generation tasks. More recently, a massive transformer-\nbased model called GPT-3, which has an astonishing 175 billion\nparameters, was developed [11]. By pre-training on 45 TB of\ncompressed plaintext data, GPT-3 can directly process different\ntypes of downstream natural language tasks without fine-tuning.\nAs a result, it achieves strong performance on many NLP datasets,\nincluding both natural language understanding and generation.\nSince the introduction of transformer, many other models have\nbeen proposed in addition to the transformer-based PTMs men-\ntioned earlier. We list a few representative models in Table 5 for\ninterested readers, but this is not the focus of our study.\nTABLE 5: List of representative language models built on\ntransformer. Transformer is the standard encoder-decoder ar-\nchitecture. Transformer Enc. and Dec. represent the encoder\nand decoder, respectively. Decoder uses mask self-attention to\nprevent attending to the future tokens. The data of the Table\nis from [203].\nModels Architecture # of Params Fine-tuning\nGPT [258] Transformer Dec. 117M Yes\nGPT-2 [110] Transformer Dec. 117M-1542M No\nGPT-3 [11] Transformer Dec. 125M-175B No\nBERT [10] Transformer Enc. 110M-340M Yes\nRoBERTa [256] Transformer Enc. 355M Yes\nXLNet [259] Two-Stream ≈ BERT YesTransformer Enc.\nELECTRA [260] Transformer Enc. 335M Yes\nUniLM [261] Transformer Enc. 340M Yes\nBART [262] Transformer 110% of BERT Yes\nT5 [154] Transfomer 220M-11B Yes\nERNIE (THU) [263] Transformer Enc. 114M Yes\nKnowBERT [264] Transformer Enc. 253M-523M Yes\nApart from the PTMs trained on large corpora for general NLP\ntasks, transformer-based models have also been applied in many\nother NLP-related domains and to multi-modal tasks.\nBioNLP Domain. Transformer-based models have outperformed\nmany traditional biomedical methods. Some examples of such\nmodels include BioBERT [265], which uses a transformer ar-\nchitecture for biomedical text mining tasks, and SciBERT [266],\nwhich is developed by training transformer on 114M scientific\narticles (covering biomedical and computer science fields) with\nthe aim of executing NLP tasks in the scientific domain more\nprecisely. Another example is ClinicalBERT, proposed by Huang\net al. [267]. It utilizes transformer to develop and evaluate con-\ntinuous representations of clinical notes. One of the side effects\nof this is that the attention map of ClinicalBERT can be used\nto explain predictions, thereby allowing high-quality connections\nbetween different medical contents to be discovered.\nThe rapid development of transformer-based models on a\nvariety of NLP-related tasks demonstrates its structural superiority\nand versatility, opening up the possibility that it will become a\nuniversal module applied in many AI fields other than just NLP.\nThe following part of this survey focuses on the applications of\ntransformer in a wide range of computer vision tasks that have\nemerged over the past two years.\nA3. Self-attention for Computer Vision\nThe preceding sections reviewed methods that use a transformer\narchitecture for vision tasks. We can conclude that self-attention\nplays a pivotal role in transformer. The self-attention module can\nalso be considered a building block of CNN architectures, which\nhave low scaling properties concerning the large receptive fields.\nThis building block is widely used on top of the networks to\ncapture long-range interactions and enhance high-level semantic\nfeatures for vision tasks. In this section, we delve deeply into\nthe models based on self-attention designed for challenging tasks\nin computer vision. Such tasks include semantic segmentation,\ninstance segmentation, object detection, keypoint detection, and\ndepth estimation. Here we briefly summarize the existing applica-\ntions using self-attention for computer vision.\nImage Classification. Trainable attention for classification\nconsists of two main streams: hard attention [268], [269], [270] re-\ngarding the use of an image region, and soft attention [271], [272],\n[273], [274] generating non-rigid feature maps. Ba et al. [268]\nfirst proposed the term “visual attention” for image classification\ntasks, and used attention to select relevant regions and locations\nwithin the input image. This can also reduce the computational\ncomplexity of the proposed model regarding the size of the input\nimage. For medical image classification, AG-CNN [275] was\nproposed to crop a sub-region from a global image by the attention\nheat map. And instead of using hard attention and recalibrating the\ncrop of feature maps, SENet [276] was proposed to reweight the\nchannel-wise responses of the convolutional features using soft\nself-attention. Jetley et al. [272] used attention maps generated\nby corresponding estimators to reweight intermediate features in\nDNNs. In addition, Han et al. [273] utilized the attribute-aware\nattention to enhance the representation of CNNs.\nSemantic Segmentation. PSANet [277], OCNet [278],\nDANet [279] and CFNet [280] are the pioneering works to propose\nusing the self-attention module in semantic segmentation tasks.\nThese works consider and augment the relationship and similar-\nity [281], [282], [283], [284], [285], [286] between the contextual\npixels. DANet [279] simultaneously leverages the self-attention\nmodule on spatial and channel dimensions, whereas A2Net [287]\ngroups the pixels into a set of regions, and then augments the\npixel representations by aggregating the region representations\nwith the generated attention weights. DGCNet [288] employs a\ndual graph CNN to model coordinate space similarity and feature\nspace similarity in a single framework. To improve the efficiency\nof the self-attention module for semantic segmentation, several\nworks [289], [290], [291], [292], [293] have been proposed,\naiming to alleviate the huge amount of parameters brought by\ncalculating pixel similarities. For example, CGNL [289] applies\nthe Taylor series of the RBF kernel function to approximate the\npixel similarities. CCNet [290] approximates the original self-\nattention scheme via two consecutive criss-cross attention mod-\nules. In addition, ISSA [291] factorizes the dense affinity matrix as\nthe product of two sparse affinity matrices. There are other related\nworks using attention based graph reasoning modules [294], [295],\n[292] to enhance both the local and global representations.\nObject Detection. Ramachandran et al. [274] proposes an\nattention-based layer and swapped the conventional convolution\nlayers to build a fully attentional detector that outperforms the\ntypical RetinaNet [129] on COCO benchmark [296]. GCNet [297]\nassumes that the global contexts modeled by non-local operations\nare almost the same for different query positions within an image,\nA SUBMISSION TO IEEE TRANSACTION ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 19\nand unifies the simplified formulation and SENet [276] into a\ngeneral framework for global context modeling [298], [299],\n[300], [301]. V o et al. [302] designs a bidirectional operation\nto gather and distribute information from a query position to\nall possible positions. Zhang et al. [120] suggests that previous\nmethods fail to interact with cross-scale features, and proposes\nFeature Pyramid Transformer, based on the self-attention module,\nto fully exploit interactions across both space and scales.\nConventional detection methods usually exploit a single visual\nrepresentation (e.g., bounding box and corner point) for predicting\nthe final results. Hu et al. [303] proposes a relation module based\non self-attention to process a set of objects simultaneously through\ninteraction between their appearance features. Cheng et al. [121]\nproposes RelationNet++ with the bridging visual representations\n(BVR) module to combine different heterogeneous representations\ninto a single one similar to that in the self-attention module.\nSpecifically, the master representation is treated as the query input\nand the auxiliary representations are regarded as the key input.\nThe enhanced feature can therefore bridge the information from\nauxiliary representations and benefit final detection results.\nOther Vision Tasks. Zhang et al. [304] proposes a resolution-\nwise attention module to learn enhanced feature maps when\ntraining multi-resolution networks to obtain accurate human key-\npoint locations for pose estimation task. Furthermore, Chang et\nal. [305] uses an attention-mechanism based feature fusion block\nto improve the accuracy of the human keypoint detection model.\nTo explore more generalized contextual information for im-\nproving the self-supervised monocular trained depth estimation,\nJohnston et al. [306] directly leverages self-attention module.\nChen et al. [307] also proposes an attention-based aggregation net-\nwork to capture context information that differs in diverse scenes\nfor depth estimation. And Aich et al. [308] proposes bidirectional\nattention modules that utilize the forward and backward attention\noperations for better results of monocular depth estimation.\nREFERENCES\n[1] F. Rosenblatt. The perceptron, a perceiving and recognizing automaton\nProject Para. Cornell Aeronautical Laboratory, 1957.\n[2] F. ROSENBLATT. Principles of neurodynamics. perceptrons and the\ntheory of brain mechanisms. Technical report, 1961.\n[3] Y . LeCun et al. Gradient-based learning applied to document recogni-\ntion. Proceedings of the IEEE, 86(11):2278–2324, 1998.\n[4] A. Krizhevsky et al. Imagenet classification with deep convolutional\nneural networks. In NeurIPS, pp. 1097–1105, 2012.\n[5] D. E. Rumelhart et al. Learning internal representations by error\npropagation. Technical report, 1985.\n[6] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural\ncomputation, 9(8):1735–1780, 1997.\n[7] D. Bahdanau et al. Neural machine translation by jointly learning to\nalign and translate. In ICLR, 2015.\n[8] A. Parikh et al. A decomposable attention model for natural language\ninference. In EMNLP, 2016.\n[9] A. Vaswani et al. Attention is all you need. In NeurIPS, 2017.\n[10] J. Devlin et al. Bert: Pre-training of deep bidirectional transformers for\nlanguage understanding. In NAACL-HLT, 2019.\n[11] T. B. Brown et al. Language models are few-shot learners. In NeurIPS,\n2020.\n[12] K. He et al. Deep residual learning for image recognition. In CVPR,\npp. 770–778, 2016.\n[13] S. Ren et al. Faster R-CNN: Towards real-time object detection with\nregion proposal networks. In NeurIPS, 2015.\n[14] M. Chen et al. Generative pretraining from pixels. In ICML, 2020.\n[15] A. Dosovitskiy et al. An image is worth 16x16 words: Transformers for\nimage recognition at scale. In ICLR, 2021.\n[16] N. Carion et al. End-to-end object detection with transformers. In\nECCV, 2020.\n[17] X. Zhu et al. Deformable detr: Deformable transformers for end-to-end\nobject detection. In ICLR, 2021.\n[18] S. Zheng et al. Rethinking semantic segmentation from a sequence-to-\nsequence perspective with transformers. In CVPR, 2021.\n[19] H. Chen et al. Pre-trained image processing transformer. In CVPR,\n2021.\n[20] L. Zhou et al. End-to-end dense video captioning with masked\ntransformer. In CVPR, pp. 8739–8748, 2018.\n[21] S. Ullman et al. High-level vision: Object recognition and visual\ncognition, volume 2. MIT press Cambridge, MA, 1996.\n[22] R. Kimchi et al. Perceptual organization in vision: Behavioral and\nneural perspectives. Psychology Press, 2003.\n[23] J. Zhu et al. Top-down saliency detection via contextual pooling.\nJournal of Signal Processing Systems, 74(1):33–46, 2014.\n[24] J. Long et al. Fully convolutional networks for semantic segmentation.\nIn CVPR, 2015.\n[25] H. Wang et al. Max-deeplab: End-to-end panoptic segmentation with\nmask transformers. In CVPR, pp. 5463–5474, 2021.\n[26] R. B. Fisher. Cvonline: The evolving, distributed, non-proprietary, on-\nline compendium of computer vision. Retrieved January 28, 2006 from\nhttp://homepages. inf. ed. ac. uk/rbf/CVonline, 2008.\n[27] N. Parmar et al. Image transformer. In ICML, 2018.\n[28] Y . Zeng et al. Learning joint spatial-temporal transformations for video\ninpainting. In ECCV, pp. 528–543. Springer, 2020.\n[29] K. Han et al. Transformer in transformer. In NeurIPS, 2021.\n[30] H. Cao et al. Swin-unet: Unet-like pure transformer for medical image\nsegmentation. arXiv:2105.05537, 2021.\n[31] X. Chen et al. An empirical study of training self-supervised vision\ntransformers. In ICCV, 2021.\n[32] K. He et al. Masked autoencoders are scalable vision learners. In CVPR,\npp. 16000–16009, 2022.\n[33] Z. Dai et al. UP-DETR: unsupervised pre-training for object detection\nwith transformers. In CVPR, 2021.\n[34] Y . Wang et al. End-to-end video instance segmentation with transform-\ners. In CVPR, 2021.\n[35] L. Huang et al. Hand-transformer: Non-autoregressive structured mod-\neling for 3d hand pose estimation. In ECCV, pp. 17–33, 2020.\n[36] L. Huang et al. Hot-net: Non-autoregressive transformer for 3d hand-\nobject pose estimation. In ACM MM, pp. 3136–3145, 2020.\n[37] K. Lin et al. End-to-end human pose and mesh reconstruction with\ntransformers. In CVPR, 2021.\n[38] P. Esser et al. Taming transformers for high-resolution image synthesis.\nIn CVPR, 2021.\n[39] Y . Jiang et al. Transgan: Two transformers can make one strong gan. In\nNeurIPS, 2021.\n[40] F. Yang et al. Learning texture transformer network for image super-\nresolution. In CVPR, pp. 5791–5800, 2020.\n[41] A. Radford et al. Learning transferable visual models from natural\nlanguage supervision. arXiv:2103.00020, 2021.\n[42] A. Ramesh et al. Zero-shot text-to-image generation. In ICML, 2021.\n[43] M. Ding et al. Cogview: Mastering text-to-image generation via\ntransformers. In NeurIPS, 2021.\n[44] OpenAI. Gpt-4 technical report, 2023.\n[45] P. Michel et al. Are sixteen heads really better than one? In NeurIPS,\npp. 14014–14024, 2019.\n[46] X. Jiao et al. TinyBERT: Distilling BERT for natural language under-\nstanding. In Findings of EMNLP, pp. 4163–4174, 2020.\n[47] G. Prato et al. Fully quantized transformer for machine translation. In\nFindings of EMNLP, 2020.\n[48] Z.-H. Jiang et al. Convbert: Improving bert with span-based dynamic\nconvolution. NeurIPS, 33, 2020.\n[49] J. Gehring et al. Convolutional sequence to sequence learning. In ICML,\npp. 1243–1252. PMLR, 2017.\n[50] P. Shaw et al. Self-attention with relative position representations. In\nNAACL, pp. 464–468, 2018.\n[51] D. Hendrycks and K. Gimpel. Gaussian error linear units (gelus).\narXiv:1606.08415, 2016.\n[52] J. L. Ba et al. Layer normalization. arXiv:1607.06450, 2016.\n[53] A. Baevski and M. Auli. Adaptive input representations for neural\nlanguage modeling. In ICLR, 2019.\n[54] Q. Wang et al. Learning deep transformer models for machine transla-\ntion. In ACL, pp. 1810–1822, 2019.\n[55] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift. In ICML, 2015.\n[56] S. Shen et al. Powernorm: Rethinking batch normalization in transform-\ners. In ICML, 2020.\nA SUBMISSION TO IEEE TRANSACTION ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 20\n[57] J. Xu et al. Understanding and improving layer normalization. In\nNeurIPS, 2019.\n[58] T. Bachlechner et al. Rezero is all you need: Fast convergence at large\ndepth. In Uncertainty in Artificial Intelligence, pp. 1352–1361. PMLR,\n2021.\n[59] B. Wu et al. Visual transformers: Token-based image representation and\nprocessing for computer vision. arXiv:2006.03677, 2020.\n[60] H. Touvron et al. Training data-efficient image transformers & distilla-\ntion through attention. In ICML, 2020.\n[61] Z. Liu et al. Swin transformer: Hierarchical vision transformer using\nshifted windows. In ICCV, 2021.\n[62] C.-F. Chen et al. Regionvit: Regional-to-local attention for vision\ntransformers. arXiv:2106.02689, 2021.\n[63] X. Chu et al. Twins: Revisiting the design of spatial attention in vision\ntransformers. arXiv:2104.13840, 2021.\n[64] H. Lin et al. Cat: Cross attention in vision transformer. arXiv, 2021.\n[65] X. Dong et al. Cswin transformer: A general vision transformer\nbackbone with cross-shaped windows. arXiv:2107.00652, 2021.\n[66] Z. Huang et al. Shuffle transformer: Rethinking spatial shuffle for vision\ntransformer. arXiv:2106.03650, 2021.\n[67] J. Fang et al. Msg-transformer: Exchanging local spatial information by\nmanipulating messenger tokens. arXiv:2105.15168, 2021.\n[68] L. Yuan et al. Tokens-to-token vit: Training vision transformers from\nscratch on imagenet. In ICCV, 2021.\n[69] D. Zhou et al. Deepvit: Towards deeper vision transformer. arXiv, 2021.\n[70] P. Wang et al. Kvt: k-nn attention for boosting vision transformers.\narXiv:2106.00515, 2021.\n[71] D. Zhou et al. Refiner: Refining self-attention for vision transformers.\narXiv:2106.03714, 2021.\n[72] A. El-Nouby et al. Xcit: Cross-covariance image transformers.\narXiv:2106.09681, 2021.\n[73] W. Wang et al. Pyramid vision transformer: A versatile backbone for\ndense prediction without convolutions. In ICCV, 2021.\n[74] S. Sun* et al. Visual parser: Representing part-whole hierarchies with\ntransformers. arXiv:2107.05790, 2021.\n[75] H. Fan et al. Multiscale vision transformers. arXiv:2104.11227, 2021.\n[76] Z. Zhang et al. Nested hierarchical transformer: Towards accurate, data-\nefficient and interpretable visual understanding. In AAAI, 2022.\n[77] Z. Pan et al. Less is more: Pay less attention in vision transformers. In\nAAAI, 2022.\n[78] Z. Pan et al. Scalable visual transformers with hierarchical pooling. In\nICCV, 2021.\n[79] B. Heo et al. Rethinking spatial dimensions of vision transformers. In\nICCV, 2021.\n[80] C.-F. Chen et al. Crossvit: Cross-attention multi-scale vision trans-\nformer for image classification. In ICCV, 2021.\n[81] Z. Wang et al. Uformer: A general u-shaped transformer for image\nrestoration. arXiv:2106.03106, 2021.\n[82] X. Zhai et al. Scaling vision transformers. arXiv:2106.04560, 2021.\n[83] X. Su et al. Vision transformer architecture search. arXiv, 2021.\n[84] M. Chen et al. Autoformer: Searching transformers for visual recogni-\ntion. In ICCV, pp. 12270–12280, 2021.\n[85] B. Chen et al. Glit: Neural architecture search for global and local\nimage transformer. In ICCV, pp. 12–21, 2021.\n[86] X. Chu et al. Conditional positional encodings for vision transformers.\narXiv:2102.10882, 2021.\n[87] K. Wu et al. Rethinking and improving relative position encoding for\nvision transformer. In ICCV, 2021.\n[88] H. Touvron et al. Going deeper with image transformers.\narXiv:2103.17239, 2021.\n[89] Y . Tang et al. Augmented shortcuts for vision transformers. In NeurIPS,\n2021.\n[90] I. Tolstikhin et al. Mlp-mixer: An all-mlp architecture for vision.\narXiv:2105.01601, 2021.\n[91] L. Melas-Kyriazi. Do you even need attention? a stack of feed-forward\nlayers does surprisingly well on imagenet. arXiv:2105.02723, 2021.\n[92] M.-H. Guo et al. Beyond self-attention: External attention using two\nlinear layers for visual tasks. arXiv:2105.02358, 2021.\n[93] H. Touvron et al. Resmlp: Feedforward networks for image classifica-\ntion with data-efficient training. arXiv:2105.03404, 2021.\n[94] M. Tan and Q. Le. Efficientnet: Rethinking model scaling for convolu-\ntional neural networks. In ICML, 2019.\n[95] J. Guo et al. Cmt: Convolutional neural networks meet vision trans-\nformers. arXiv:2107.06263, 2021.\n[96] L. Yuan et al. V olo: Vision outlooker for visual recognition.\narXiv:2106.13112, 2021.\n[97] H. Wu et al. Cvt: Introducing convolutions to vision transformers.\narXiv:2103.15808, 2021.\n[98] K. Yuan et al. Incorporating convolution designs into visual transform-\ners. arXiv:2103.11816, 2021.\n[99] Y . Li et al. Localvit: Bringing locality to vision transformers.\narXiv:2104.05707, 2021.\n[100] B. Graham et al. Levit: a vision transformer in convnet’s clothing for\nfaster inference. In ICCV, 2021.\n[101] A. Srinivas et al. Bottleneck transformers for visual recognition. In\nCVPR, 2021.\n[102] Z. Chen et al. Visformer: The vision-friendly transformer. arXiv, 2021.\n[103] T. Xiao et al. Early convolutions help transformers see better. In\nNeurIPS, volume 34, 2021.\n[104] G. E. Hinton and R. S. Zemel. Autoencoders, minimum description\nlength, and helmholtz free energy. NIPS, 6:3–10, 1994.\n[105] P. Vincent et al. Extracting and composing robust features with\ndenoising autoencoders. In ICML, pp. 1096–1103, 2008.\n[106] A. v. d. Oord et al. Conditional image generation with pixelcnn\ndecoders. arXiv preprint arXiv:1606.05328, 2016.\n[107] D. Pathak et al. Context encoders: Feature learning by inpainting. In\nCVPR, pp. 2536–2544, 2016.\n[108] Z. Li et al. Mst: Masked self-supervised transformer for visual\nrepresentation. In NeurIPS, 2021.\n[109] H. Bao et al. Beit: Bert pre-training of image transformers.\narXiv:2106.08254, 2021.\n[110] A. Radford et al. Language models are unsupervised multitask learners.\nOpenAI blog, 1(8):9, 2019.\n[111] Z. Xie et al. Simmim: A simple framework for masked image modeling.\nIn CVPR, pp. 9653–9663, 2022.\n[112] Z. Xie et al. Self-supervised learning with swin transformers.\narXiv:2105.04553, 2021.\n[113] C. Li et al. Efficient self-supervised vision transformers for representa-\ntion learning. arXiv:2106.09785, 2021.\n[114] K. He et al. Momentum contrast for unsupervised visual representation\nlearning. In CVPR, 2020.\n[115] J. Beal et al. Toward transformer-based object detection.\narXiv:2012.09958, 2020.\n[116] Z. Yuan et al. Temporal-channel transformer for 3d lidar-based video\nobject detection for autonomous driving. IEEE TCSVT, 2021.\n[117] X. Pan et al. 3d object detection with pointformer. In CVPR, 2021.\n[118] R. Liu et al. End-to-end lane shape prediction with transformers. In\nWACV, 2021.\n[119] S. Yang et al. Transpose: Keypoint localization via transformer. In\nICCV, 2021.\n[120] D. Zhang et al. Feature pyramid transformer. In ECCV, 2020.\n[121] C. Chi et al. Relationnet++: Bridging visual representations for object\ndetection via transformer decoder. NeurIPS, 2020.\n[122] Z. Sun et al. Rethinking transformer-based set prediction for object\ndetection. In ICCV, pp. 3611–3620, 2021.\n[123] M. Zheng et al. End-to-end object detection with adaptive clustering\ntransformer. In BMVC, 2021.\n[124] T. Ma et al. Oriented object detection with transformer.\narXiv:2106.03146, 2021.\n[125] P. Gao et al. Fast convergence of detr with spatially modulated co-\nattention. In ICCV, 2021.\n[126] Z. Yao et al. Efficient detr: Improving end-to-end object detector with\ndense prior. arXiv:2104.01318, 2021.\n[127] Z. Tian et al. Fcos: Fully convolutional one-stage object detection. In\nICCV, pp. 9627–9636, 2019.\n[128] Y . Fang et al. You only look at one sequence: Rethinking transformer\nin vision through object detection. In NeurIPS, 2021.\n[129] T.-Y . Lin et al. Focal loss for dense object detection. In ICCV, 2017.\n[130] Z. Cai and N. Vasconcelos. Cascade r-cnn: Delving into high quality\nobject detection. In CVPR, 2018.\n[131] A. Bar et al. Detreg: Unsupervised pretraining with region priors for\nobject detection. arXiv:2106.04550, 2021.\n[132] J. Hu et al. Istr: End-to-end instance segmentation with transformers.\narXiv:2105.00637, 2021.\n[133] Z. Yang et al. Associating objects with transformers for video object\nsegmentation. In NeurIPS, 2021.\n[134] S. Wu et al. Fully transformer networks for semantic image segmenta-\ntion. arXiv:2106.04108, 2021.\n[135] B. Dong et al. Solq: Segmenting objects by learning queries. In\nNeurIPS, 2021.\n[136] R. Strudel et al. Segmenter: Transformer for semantic segmentation. In\nICCV, 2021.\nA SUBMISSION TO IEEE TRANSACTION ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 21\n[137] E. Xie et al. Segformer: Simple and efficient design for semantic\nsegmentation with transformers. In NeurIPS, 2021.\n[138] J. M. J. Valanarasu et al. Medical transformer: Gated axial-attention for\nmedical image segmentation. In MICCAI, 2021.\n[139] T. Prangemeier et al. Attention-based transformers for instance seg-\nmentation of cells in microstructures. In International Conference on\nBioinformatics and Biomedicine, pp. 700–707. IEEE, 2020.\n[140] C. R. Qi et al. Pointnet: Deep learning on point sets for 3d classification\nand segmentation. In CVPR, pp. 652–660, 2017.\n[141] C. R. Qi et al. Pointnet++: Deep hierarchical feature learning on point\nsets in a metric space. NeurIPS, 30:5099–5108, 2017.\n[142] S. Hampali et al. Handsformer: Keypoint transformer for monocular 3d\npose estimation ofhands and object in interaction. arXiv, 2021.\n[143] Y . Li et al. Tokenpose: Learning keypoint tokens for human pose\nestimation. In ICCV, 2021.\n[144] W. Mao et al. Tfpose: Direct human pose estimation with transformers.\narXiv:2103.15320, 2021.\n[145] T. Jiang et al. Skeletor: Skeletal transformers for robust body-pose\nestimation. In CVPR, 2021.\n[146] Y . Li et al. Test-time personalization with a transformer for human pose\nestimation. Advances in Neural Information Processing Systems , 34,\n2021.\n[147] M. Lin et al. Detr for pedestrian detection. arXiv:2012.06785, 2020.\n[148] L. Tabelini et al. Polylanenet: Lane estimation via deep polynomial re-\ngression. In 2020 25th International Conference on Pattern Recognition\n(ICPR), pp. 6150–6156. IEEE, 2021.\n[149] L. Liu et al. Condlanenet: a top-to-down lane detection framework\nbased on conditional convolution. arXiv:2105.05003, 2021.\n[150] P. Xu et al. A survey of scene graph: Generation and application. IEEE\nTrans. Neural Netw. Learn. Syst, 2020.\n[151] J. Yang et al. Graph r-cnn for scene graph generation. In ECCV, 2018.\n[152] S. Sharifzadeh et al. Classification by attention: Scene graph classifica-\ntion with prior knowledge. In AAAI, 2021.\n[153] S. Sharifzadeh et al. Improving Visual Reasoning by Exploiting The\nKnowledge in Texts. arXiv:2102.04760, 2021.\n[154] C. Raffel et al. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. JMLR, 21(140):1–67, 2020.\n[155] N. Wang et al. Transformer meets tracker: Exploiting temporal context\nfor robust visual tracking. In CVPR, 2021.\n[156] M. Zhao et al. TrTr: Visual Tracking with Transformer.\narXiv:2105.03817 [cs], May 2021. arXiv: 2105.03817.\n[157] X. Chen et al. Transformer tracking. In CVPR, 2021.\n[158] P. Sun et al. TransTrack: Multiple Object Tracking with Transformer.\narXiv:2012.15460 [cs], May 2021. arXiv: 2012.15460.\n[159] S. He et al. TransReID: Transformer-based object re-identification. In\nICCV, 2021.\n[160] X. Liu et al. A video is worth three views: Trigeminal transformers for\nvideo-based person re-identification. arXiv:2104.01745, 2021.\n[161] T. Zhang et al. Spatiotemporal transformer for video-based person re-\nidentification. arXiv:2103.16469, 2021.\n[162] N. Engel et al. Point transformer. IEEE Access , 9:134826–134840,\n2021.\n[163] M.-H. Guo et al. Pct: Point cloud transformer. Computational Visual\nMedia, 7(2):187–199, 2021.\n[164] H. Zhao et al. Point transformer. In ICCV, pp. 16259–16268, 2021.\n[165] K. Lee et al. Vitgan: Training gans with vision transformers. arXiv\npreprint arXiv:2107.04589, 2021.\n[166] A. v. d. Oord et al. Neural discrete representation learning. arXiv, 2017.\n[167] J. Ho et al. Denoising diffusion probabilistic models. volume 33, pp.\n6840–6851, 2020.\n[168] A. Ramesh et al. Hierarchical text-conditional image generation with\nclip latents. arXiv preprint arXiv:2204.06125, 2022.\n[169] R. Rombach et al. High-resolution image synthesis with latent diffusion\nmodels. In CVPR, pp. 10684–10695, 2022.\n[170] X. Wang et al. Sceneformer: Indoor scene generation with transformers.\nIn 3DV, pp. 106–115. IEEE, 2021.\n[171] Z. Liu et al. Convtransformer: A convolutional transformer network for\nvideo frame synthesis. arXiv:2011.10185, 2020.\n[172] R. Girdhar et al. Video action transformer network. In CVPR, 2019.\n[173] H. Liu et al. Two-stream transformer networks for video-based face\nalignment. T-PAMI, 40(11):2546–2554, 2017.\n[174] J. Carreira and A. Zisserman. Quo vadis, action recognition? a new\nmodel and the kinetics dataset. In CVPR, 2017.\n[175] S. Lohit et al. Temporal transformer networks: Joint learning of\ninvariant and discriminative time warping. In CVPR, 2019.\n[176] M. Fayyaz and J. Gall. Sct: Set constrained temporal transformer for\nset supervised action segmentation. In 2020 CVPR, pp. 501–510, 2020.\n[177] W. Choi et al. What are they doing?: Collective activity classification\nusing spatio-temporal relationship among people. In ICCVW, 2009.\n[178] K. Gavrilyuk et al. Actor-transformers for group activity recognition.\nIn CVPR, pp. 839–848, 2020.\n[179] J. Shao et al. Temporal context aggregation for video retrieval with\ncontrastive learning. In WACV, 2021.\n[180] V . Gabeur et al. Multi-modal transformer for video retrieval. In ECCV,\npp. 214–229, 2020.\n[181] Y . Chen et al. Memory enhanced global-local aggregation for video\nobject detection. In CVPR, pp. 10337–10346, 2020.\n[182] J. Yin et al. Lidar-based online 3d video object detection with graph-\nbased message passing and spatiotemporal transformer attention. In\n2020 CVPR, pp. 11495–11504, 2020.\n[183] H. Seong et al. Video multitask transformer network. In ICCVW, 2019.\n[184] K. M. Schatz et al. A recurrent transformer network for novel view\naction synthesis. In ECCV (27), pp. 410–426, 2020.\n[185] C. Sun et al. Videobert: A joint model for video and language\nrepresentation learning. In ICCV, pp. 7464–7473, 2019.\n[186] L. H. Li et al. Visualbert: A simple and performant baseline for vision\nand language. arXiv:1908.03557, 2019.\n[187] W. Su et al. Vl-bert: Pre-training of generic visual-linguistic represen-\ntations. In ICLR, 2020.\n[188] Y .-S. Chuang et al. Speechbert: Cross-modal pre-trained language\nmodel for end-to-end spoken question answering. In Interspeech, 2020.\n[189] R. Hu and A. Singh. Unit: Multimodal multitask learning with a unified\ntransformer. In ICCV, 2021.\n[190] S. Prasanna et al. When bert plays the lottery, all tickets are winning.\nIn EMNLP, 2020.\n[191] J. Frankle and M. Carbin. The lottery ticket hypothesis: Finding sparse,\ntrainable neural networks. In ICLR, 2018.\n[192] Y . Tang et al. Patch slimming for efficient vision transformers.\narXiv:2106.02852, 2021.\n[193] M. Zhu et al. Vision transformer pruning. arXiv:2104.08500, 2021.\n[194] Z. Liu et al. Learning efficient convolutional networks through network\nslimming. In ICCV, 2017.\n[195] Z. Lan et al. Albert: A lite bert for self-supervised learning of language\nrepresentations. In ICLR, 2020.\n[196] C. Xu et al. Bert-of-theseus: Compressing bert by progressive module\nreplacing. In EMNLP, pp. 7859–7869, 2020.\n[197] S. Shen et al. Q-bert: Hessian based ultra low precision quantization of\nbert. In AAAI, pp. 8815–8821, 2020.\n[198] O. Zafrir et al. Q8bert: Quantized 8bit bert. arXiv:1910.06188, 2019.\n[199] V . Sanh et al. Distilbert, a distilled version of bert: smaller, faster,\ncheaper and lighter. arXiv:1910.01108, 2019.\n[200] S. Sun et al. Patient knowledge distillation for bert model compression.\nIn EMNLP-IJCNLP, pp. 4323–4332, 2019.\n[201] Z. Sun et al. Mobilebert: a compact task-agnostic bert for resource-\nlimited devices. In ACL, pp. 2158–2170, 2020.\n[202] I. Turc et al. Well-read students learn better: The impact of student\ninitialization on knowledge distillation. arXiv:1908.08962, 2019.\n[203] X. Qiu et al. Pre-trained models for natural language processing: A\nsurvey. Science China Technological Sciences, pp. 1–26, 2020.\n[204] A. Fan et al. Reducing transformer depth on demand with structured\ndropout. In ICLR, 2020.\n[205] L. Hou et al. Dynabert: Dynamic bert with adaptive width and depth.\nNeurIPS, 33, 2020.\n[206] Z. Wang et al. Structured pruning of large language models. In EMNLP,\npp. 6151–6162, 2020.\n[207] G. Hinton et al. Distilling the knowledge in a neural network.\narXiv:1503.02531, 2015.\n[208] C. Bucilu ˇa et al. Model compression. In SIGKDD, pp. 535–541, 2006.\n[209] J. Ba and R. Caruana. Do deep nets really need to be deep? NIPS, 2014.\n[210] S. Mukherjee and A. H. Awadallah. Xtremedistil: Multi-stage distilla-\ntion for massive multilingual models. In ACL, pp. 2221–2234, 2020.\n[211] W. Wang et al. Minilm: Deep self-attention distillation for task-agnostic\ncompression of pre-trained transformers. arXiv:2002.10957, 2020.\n[212] S. I. Mirzadeh et al. Improved knowledge distillation via teacher\nassistant. In AAAI, 2020.\n[213] D. Jia et al. Efficient vision transformers via fine-grained manifold\ndistillation. arXiv:2107.01378, 2021.\n[214] V . Vanhoucke et al. Improving the speed of neural networks on cpus.\nIn NIPS Workshop, 2011.\n[215] Z. Yang et al. Searching for low-bit weights in quantized neural\nnetworks. In NeurIPS, 2020.\n[216] E. Park and S. Yoo. Profit: A novel training method for sub-4-bit\nmobilenet models. In ECCV, pp. 430–446. Springer, 2020.\nA SUBMISSION TO IEEE TRANSACTION ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 22\n[217] J. Fromm et al. Riptide: Fast end-to-end binarized neural networks.\nProceedings of Machine Learning and Systems, 2:379–389, 2020.\n[218] Y . Bai et al. Proxquant: Quantized neural networks via proximal\noperators. In ICLR, 2019.\n[219] A. Bhandare et al. Efficient 8-bit quantization of transformer neural\nmachine language translation model. arXiv:1906.00532, 2019.\n[220] C. Fan. Quantized transformer. Technical report, Stanford Univ., 2019.\n[221] K. Shridhar et al. End to end binarized neural networks for text\nclassification. In SustaiNLP, 2020.\n[222] R. Cheong and R. Daniel. transformers. zip: Compressing transformers\nwith pruning and quantization. Technical report, 2019.\n[223] Z. Zhao et al. An investigation on different underlying quantization\nschemes for pre-trained language models. In NLPCC, 2020.\n[224] Z. Liu et al. Post-training quantization for vision transformer. In\nNeurIPS, 2021.\n[225] Z. Wu et al. Lite transformer with long-short range attention. In ICLR,\n2020.\n[226] Z. Geng et al. Is attention better than matrix decomposition? In ICLR,\n2020.\n[227] Y . Guo et al. Nat: Neural architecture transformer for accurate and\ncompact architectures. In NeurIPS, pp. 737–748, 2019.\n[228] D. So et al. The evolved transformer. In ICML, pp. 5877–5886, 2019.\n[229] C. Li et al. Bossnas: Exploring hybrid cnn-transformers with block-\nwisely self-supervised neural architecture search. In ICCV, 2021.\n[230] A. Katharopoulos et al. Transformers are rnns: Fast autoregressive\ntransformers with linear attention. In ICML, 2020.\n[231] C. Yun et al. o(n) connections are expressive enough: Universal\napproximability of sparse transformers. In NeurIPS, 2020.\n[232] M. Zaheer et al. Big bird: Transformers for longer sequences. In\nNeurIPS, 2020.\n[233] D. A. Spielman and S.-H. Teng. Spectral sparsification of graphs. SIAM\nJournal on Computing, 40(4), 2011.\n[234] F. Chung and L. Lu. The average distances in random graphs with given\nexpected degrees. PNAS, 99(25):15879–15882, 2002.\n[235] A. Krizhevsky and G. Hinton. Learning multiple layers of features from\ntiny images. Technical report, Citeseer, 2009.\n[236] X. Zhai et al. A large-scale study of representation learning with the\nvisual task adaptation benchmark. arXiv:1910.04867, 2019.\n[237] Y . Cheng et al. Robust neural machine translation with doubly adver-\nsarial inputs. In ACL, 2019.\n[238] W. E. Zhang et al. Adversarial attacks on deep-learning models in\nnatural language processing: A survey. ACM TIST, 11(3):1–41, 2020.\n[239] K. Mahmood et al. On the robustness of vision transformers to\nadversarial examples. arXiv:2104.02610, 2021.\n[240] X. Mao et al. Towards robust vision transformer. arXiv, 2021.\n[241] S. Serrano and N. A. Smith. Is attention interpretable? In ACL, 2019.\n[242] S. Wiegreffe and Y . Pinter. Attention is not not explanation. InEMNLP-\nIJCNLP, 2019.\n[243] H. Chefer et al. Transformer interpretability beyond attention visualiza-\ntion. In CVPR, pp. 782–791, 2021.\n[244] R. Livni et al. On the computational efficiency of training neural\nnetworks. In NeurIPS, 2014.\n[245] B. Neyshabur et al. Towards understanding the role of over-\nparametrization in generalization of neural networks. In ICLR, 2019.\n[246] K. Han et al. Ghostnet: More features from cheap operations. In CVPR,\npp. 1580–1589, 2020.\n[247] K. Han et al. Model rubik’s cube: Twisting resolution, depth and width\nfor tinynets. NeurIPS, 33, 2020.\n[248] T. Chen et al. Diannao: a small-footprint high-throughput accelerator\nfor ubiquitous machine-learning. In ASPLOS, pp. 269–284, 2014.\n[249] H. Liao et al. Davinci: A scalable architecture for neural network\ncomputing. In 2019 IEEE Hot Chips 31 Symposium (HCS) , 2019.\n[250] A. Jaegle et al. Perceiver: General perception with iterative attention.\nIn ICML, volume 139, pp. 4651–4664. PMLR, 18–24 Jul 2021.\n[251] A. Jaegle et al. Perceiver io: A general architecture for structured inputs\n& outputs. arXiv preprint arXiv:2107.14795, 2021.\n[252] X. Wang et al. Non-local neural networks. In CVPR, pp. 7794–7803,\n2018.\n[253] A. Buades et al. A non-local algorithm for image denoising. In CVPR,\npp. 60–65, 2005.\n[254] J. Chung et al. Empirical evaluation of gated recurrent neural networks\non sequence modeling. arXiv:1412.3555, 2014.\n[255] M. Joshi et al. Spanbert: Improving pre-training by representing and\npredicting spans. Transactions of the Association for Computational\nLinguistics, 8:64–77, 2020.\n[256] Y . Liu et al. Roberta: A robustly optimized bert pretraining approach.\narXiv:1907.11692, 2019.\n[257] Y . Zhu et al. Aligning books and movies: Towards story-like visual\nexplanations by watching movies and reading books. In ICCV, pp.\n19–27, 2015.\n[258] A. Radford et al. Improving language understanding by generative pre-\ntraining, 2018.\n[259] Z. Yang et al. Xlnet: Generalized autoregressive pretraining for lan-\nguage understanding. In NeurIPS, pp. 5753–5763, 2019.\n[260] K. Clark et al. Electra: Pre-training text encoders as discriminators\nrather than generators. arXiv:2003.10555, 2020.\n[261] L. Dong et al. Unified language model pre-training for natural language\nunderstanding and generation. In NeurIPS, pp. 13063–13075, 2019.\n[262] M. Lewis et al. Bart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and comprehension.\narXiv:1910.13461, 2019.\n[263] Z. Zhang et al. Ernie: Enhanced language representation with informa-\ntive entities. arXiv:1905.07129, 2019.\n[264] M. E. Peters et al. Knowledge enhanced contextual word representa-\ntions. arXiv:1909.04164, 2019.\n[265] J. Lee et al. Biobert: a pre-trained biomedical language representation\nmodel for biomedical text mining. Bioinformatics, 36(4):1234–1240,\n2020.\n[266] I. Beltagy et al. Scibert: A pretrained language model for scientific text.\narXiv:1903.10676, 2019.\n[267] K. Huang et al. Clinicalbert: Modeling clinical notes and predicting\nhospital readmission. arXiv:1904.05342, 2019.\n[268] J. Ba et al. Multiple object recognition with visual attention. In ICLR,\n2014.\n[269] V . Mnih et al. Recurrent models of visual attention. NeurIPS, pp.\n2204–2212, 2014.\n[270] K. Xu et al. Show, attend and tell: Neural image caption generation\nwith visual attention. In International conference on machine learning,\npp. 2048–2057, 2015.\n[271] F. Wang et al. Residual attention network for image classification. In\nCVPR, pp. 3156–3164, 2017.\n[272] S. Jetley et al. Learn to pay attention. In ICLR, 2018.\n[273] K. Han et al. Attribute-aware attention model for fine-grained represen-\ntation learning. In ACM MM, pp. 2040–2048, 2018.\n[274] P. Ramachandran et al. Stand-alone self-attention in vision models. In\nNeurIPS, 2019.\n[275] Q. Guan et al. Diagnose like a radiologist: Attention guided\nconvolutional neural network for thorax disease classification. In\narXiv:1801.09927, 2018.\n[276] J. Hu et al. Squeeze-and-excitation networks. In CVPR, pp. 7132–7141,\n2018.\n[277] H. Zhao et al. Psanet: Point-wise spatial attention network for scene\nparsing. In ECCV, pp. 267–283, 2018.\n[278] Y . Yuan et al. Ocnet: Object context for semantic segmentation.\nInternational Journal of Computer Vision, pp. 1–24, 2021.\n[279] J. Fu et al. Dual attention network for scene segmentation. In CVPR,\npp. 3146–3154, 2019.\n[280] H. Zhang et al. Co-occurrent features in semantic segmentation. In\nCVPR, pp. 548–557, 2019.\n[281] F. Zhang et al. Acfnet: Attentional class feature network for semantic\nsegmentation. In ICCV, pp. 6798–6807, 2019.\n[282] X. Li et al. Expectation-maximization attention networks for semantic\nsegmentation. In ICCV, pp. 9167–9176, 2019.\n[283] J. He et al. Adaptive pyramid context network for semantic segmenta-\ntion. In CVPR, pp. 7519–7528, 2019.\n[284] O. Oktay et al. Attention u-net: Learning where to look for the pancreas.\n2018.\n[285] Y . Wang et al. Self-supervised equivariant attention mechanism for\nweakly supervised semantic segmentation. In CVPR, pp. 12275–12284,\n2020.\n[286] X. Li et al. Global aggregation then local distribution in fully convolu-\ntional networks. In BMVC, 2019.\n[287] Y . Chen et al. Aˆ 2-nets: Double attention networks. NeurIPS, pp.\n352–361, 2018.\n[288] L. Zhang et al. Dual graph convolutional network for semantic\nsegmentation. In BMVC, 2019.\n[289] K. Yue et al. Compact generalized non-local network. In NeurIPS, pp.\n6510–6519, 2018.\n[290] Z. Huang et al. Ccnet: Criss-cross attention for semantic segmentation.\nIn ICCV, pp. 603–612, 2019.\n[291] L. Huang et al. Interlaced sparse self-attention for semantic segmenta-\ntion. arXiv:1907.12273, 2019.\n[292] Y . Li and A. Gupta. Beyond grids: Learning graph representations for\nvisual recognition. NeurIPS, pp. 9225–9235, 2018.\nA SUBMISSION TO IEEE TRANSACTION ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 23\n[293] S. Kumaar et al. Cabinet: Efficient context aggregation network for\nlow-latency semantic segmentation. arXiv:2011.00993, 2020.\n[294] X. Liang et al. Symbolic graph reasoning meets convolutions. NeurIPS,\npp. 1853–1863, 2018.\n[295] Y . Chen et al. Graph-based global reasoning networks. In CVPR, pp.\n433–442, 2019.\n[296] T.-Y . Lin et al. Microsoft coco: Common objects in context. In ECCV,\npp. 740–755, 2014.\n[297] Y . Cao et al. Gcnet: Non-local networks meet squeeze-excitation\nnetworks and beyond. In ICCV Workshops, 2019.\n[298] W. Li et al. Object detection based on an adaptive attention mechanism.\nScientific Reports, pp. 1–13, 2020.\n[299] T.-I. Hsieh et al. One-shot object detection with co-attention and co-\nexcitation. In NeurIPS, pp. 2725–2734, 2019.\n[300] Q. Fan et al. Few-shot object detection with attention-rpn and multi-\nrelation detector. In CVPR, pp. 4013–4022, 2020.\n[301] H. Perreault et al. Spotnet: Self-attention multi-task network for object\ndetection. In 2020 17th Conference on Computer and Robot Vision\n(CRV), pp. 230–237, 2020.\n[302] X.-T. V o et al. Bidirectional non-local networks for object detection. In\nInternational Conference on Computational Collective Intelligence, pp.\n491–501, 2020.\n[303] H. Hu et al. Relation networks for object detection. In CVPR, pp.\n3588–3597, 2018.\n[304] K. Zhang et al. Learning enhanced resolution-wise features for human\npose estimation. In 2020 IEEE International Conference on Image\nProcessing (ICIP), pp. 2256–2260, 2020.\n[305] Y . Chang et al. The same size dilated attention network for keypoint\ndetection. In International Conference on Artificial Neural Networks ,\npp. 471–483, 2019.\n[306] A. Johnston and G. Carneiro. Self-supervised monocular trained depth\nestimation using self-attention and discrete disparity volume. In CVPR,\npp. 4756–4765, 2020.\n[307] Y . Chen et al. Attention-based context aggregation network for monoc-\nular depth estimation. International Journal of Machine Learning and\nCybernetics, pp. 1583–1596, 2021.\n[308] S. Aich et al. Bidirectional attention network for monocular depth\nestimation. In ICRA, 2021."
}