{
  "title": "A Semantics-aware Transformer Model of Relation Linking for Knowledge Base Question Answering",
  "url": "https://openalex.org/W3177355445",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2072731134",
      "name": "Tahira Naseem",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3028647669",
      "name": "Srinivas Ravishankar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1436527968",
      "name": "Nandana Mihindukulasooriya",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2285488634",
      "name": "Ibrahim Abdelaziz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2100006581",
      "name": "Young-suk Lee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A765536654",
      "name": "Pavan Kapanipathi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2336572873",
      "name": "Salim Roukos",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2080393478",
      "name": "Alfio Gliozzo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101510659",
      "name": "Alexander Gray",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3115393336",
    "https://openalex.org/W3110353462",
    "https://openalex.org/W2997028620",
    "https://openalex.org/W3104681577",
    "https://openalex.org/W2763039547",
    "https://openalex.org/W2778792674",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3099252223",
    "https://openalex.org/W3013778764",
    "https://openalex.org/W2774249241",
    "https://openalex.org/W2884053243",
    "https://openalex.org/W2980401255",
    "https://openalex.org/W3105923579",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2963102202",
    "https://openalex.org/W2946606955",
    "https://openalex.org/W2984354699",
    "https://openalex.org/W2529383646",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2980646235",
    "https://openalex.org/W3173625863",
    "https://openalex.org/W2915836468",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3106209546",
    "https://openalex.org/W3120368929"
  ],
  "abstract": "Tahira Naseem, Srinivas Ravishankar, Nandana Mihindukulasooriya, Ibrahim Abdelaziz, Young-Suk Lee, Pavan Kapanipathi, Salim Roukos, Alfio Gliozzo, Alexander Gray. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 2021.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 256–262\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n256\nA Semantics-aware Transformer Model of Relation Linking\nfor Knowledge Base Question Answering\nTahira Naseem, Srinivas Ravishankar, Nandana Mihindukulasooriya,\nIbrahim Abdelaziz, Young-Suk Lee, Pavan Kapanipathi, Salim Roukos,\nAlﬁo Gliozzo, Alexander Gray\nIBM Research\n{tnaseem, ysuklee, kapanipa, roukos, gliozzo}@us.ibm.com\n{srini, nandana.m, ibrahim.abdelaziz1, alexander.gray}@ibm.com\nAbstract\nRelation linking is a crucial component of\nKnowledge Base Question Answering sys-\ntems. Existing systems use a wide variety of\nheuristics, or ensembles of multiple systems,\nheavily relying on the surface question text.\nHowever, the explicit semantic parse of the\nquestion is a rich source of relation informa-\ntion that is not taken advantage of. We pro-\npose a simple transformer-based neural model\nfor relation linking that leverages the AMR se-\nmantic parse of a sentence. Our system sig-\nniﬁcantly outperforms the state-of-the-art on 4\npopular benchmark datasets. These are based\non either DBpedia or Wikidata, demonstrating\nthat our approach is effective across KGs.\n1 Introduction\nKnowledge base question answering (KBQA) has\nreceived signiﬁcant interest due to its real-world\napplications. KBQA is a task where a natural\nlanguage question is transformed into a precise\nstructured query, using Entity Linking and Rela-\ntion Linking as necessary sub-tasks to retrieve an\nanswer. For example, the question “Who founded\nthe city where Pat Vincent died?”requires map-\nping (a) founded and died to relations dbo:founder\nand dbo:deathPlace, and (b) entity Pat Vincentto\ndbr:Pat Vincent, given DBpedia as the knowledge\nbase.\nSemantic parses such as Abstract Meaning Rep-\nresentation (AMR) have recently shown to be use-\nful for the KBQA task (Lim et al., 2020). However,\ncritical tasks for KBQA such as Relation Linking\ncontinue to be addressed primarily using the ques-\ntion text (Mulang’ et al., 2020; Sakor et al., 2019b;\nLin et al., 2020), ignoring the AMR parses of the\nquestion which can introduce additional semantics.\nIn the literature, some systems such as SLING (Mi-\nhindukulasooriya et al., 2020) have used AMR for\nrelation linking. However, similar to other rule-\nbased approaches (Sakor et al., 2019b), SLING de-\npends heavily on the speciﬁc target KG (DBpedia)\nand it is based on a complex ensemble of different\napproaches, making portability to new knowledge\nbases a non-trivial task.\nIn this work, we propose SemReL; a single\nSemantics-aware neural model forRelation linking.\nSemReL takes as input the question text annotated\nwith its AMR parse and entity information and\noutputs a ranked list of relations. The key contri-\nbutions of this work are as follows: (a) a simple,\nknowledge graph agnostic neural model for rela-\ntion linking over knowledge bases, (b) leveraging\nAMR parses for better question representation, and\n(c) an experimental evaluation using four datasets\nbased on DBpedia and Wikidata where we show\nthat SemReL consistently outperforms existing sys-\ntems on all datasets.\n2 Semantics-aware Relation Linking\nWe propose a relation linking system that exploits\nthe semantic structure of a sentence to retrieve rele-\nvant relations from the underlying knowledge base.\nWe hypothesise that semantic representations ab-\nstract away from lexical forms, providing struc-\ntural clues that are more consistent across train-\ning examples than surface text. To this end, we\nuse the AMR graph of the sentence as its seman-\ntic structure. AMRs are directed acyclic graphs\nthat capture who is doing what to whomin a sen-\ntence. The nodes in the graph are concepts and the\nedges are labelled with relations between those con-\ncepts. Figure 1 shows example AMR graphs for the\nquestion “Who founded the city where Pat Vincent\ndied?”. Note that the AMR graph for the question\nrepresents the target of the query as a special node\nlabelled ‘amr-unknown’.\nThe inputs to our system are: the question text,\n257\nWhofounded the city where Pat Vincent died?\nARG1\nFigure 1: A question AMR: The amr-unknown repre-\nsents the query target and thename node marks entities.\nits AMR graph and the entities in the question\nmarked and linked1. Relation linking is performed\nin two steps. First, our system identiﬁes the num-\nber of expected relations and their location both\nin the sentence and in the AMR graph. Next, for\neach identiﬁed slot, the most likely relation is pre-\ndicted using a transformer based neural model, that\nranks them using their English labels from the KG.\nThe AMR structure of the sentence is crucial in\nboth steps. Figure 2 shows a schematic diagram of\noverall system. In the following, we ﬁrst explain\nthe process of ﬁnding potential relation slots using\nAMR graph. Next we describe in detail our relation\nlinking module.\n2.1 Relation Slot Prediction\nAMR explicitly marks named entity nodes (see\nFigure 1). These nodes are linked to knowledge\nbase entities using BLINK entity linker. The entity\nnodes in graph are also used to predict the number\nand locations of relation slots. A slot is deﬁned\nas a pair of nodes in the AMR graph, where the\ncorresponding entities have a relation in knowledge\nbase in the context of the question. For instance,\nin Figure 3, nodes city and person are involved in\na KB relation death placerelevant for this ques-\ntion. Slot prediction is done using a determinis-\ntic rule-based transformation described in (Kapa-\nnipathi et al., 2021). In particular, we use their\n1We use the stack transformer parser of Astudillo et al.\n(2020); Lee et al. (2020) for generating AMR graphs and the\nBLINK system of Wu et al. (2019) for entity linking.\nInput Question\nAMR Parser\nAMR graph\nSlot PredictionEntity Linking\nSemRel: Neural Relation  Linking\nKnowledge BaseSlot: Node Pair\nCandidate KB Relations\ne1, r1, exe1, r7, exe1, r4, ex\ne1\nr1,r7,r4\nrelevant triples\nFigure 2: Overall system ﬂow: grey blocks are various\nsystems and white blocks show the inputs and outputs.\npath-based approach where all the paths between\nthe amr-unknown node and the linked entity nodes\nare retrieved. Then all node pairs along the path\nthat are joined by a predicate node are considered\na relation slot. We refer the reader to the original\npublication for more details of the method.\n2.2 Neural Relation Linking Model\nSemReL employs a Siamese network, where the\ninput question and target relations are embedded\nin the same vector space. The most likely relation\nis the one whose representation is closest to that\nof the input question. Figure 3 shows the overall\narchitecture of our model. We use a Transformer\nmodel (Vaswani et al., 2017) as a shared encoder\nfor both the input questions and candidate rela-\ntions. In particular, we use the pre-trained BERT\nmodel (Devlin et al., 2018) to initialize the encoder\nparameters. The output vector corresponding to the\nstarting [CLS] token is used as the vector represen-\ntation of the input. This vector is passed through\na feed-forward linear layer that projects it to the\nshared embedding space. Unlike the transformer\nparameters, the weights of the linear projection\nlayer on top are not shared between the questions\nand the relations.\nSemantic information is given as part of the ques-\ntion input to the encoder. As mentioned above, dur-\ning the preprocessing step, the pairs of nodes in\nAMR graph are identiﬁed for relation linking. For\ninstance, in ﬁgure 3, the nodes ‘person’ and ‘city’\nare marked in the input graph as the participants\n258\nCLSCLSCLS\n[CLS] [AMR] die :ARG1 person :location city [TEXT] Who founded the city where Pat Vincent[SP] died [EP]?[SEP]\nBERT Transformer ModelCLS\nFF\nQ.\nBERT Transformer ModelCLS\nFF\nR\nR\nR\nR\n[CLS]academic advisor [SEP][CLS]actor[SEP][CLS]…   ….   …. [SEP][CLS] death place [SEP]*[CLS]…   ….   …. [SEP]\nn x h 1 x h\nn x 1Cross Entropy LossGold One Hot\nshared parameters\nPretrained BERT\nQuestion EncoderRelation Encoder\n ARG1\nWho founded the city where Pat Vincent died?\nFigure 3: SemReL model architecture (left) and inputs to the model (right).\nof a potential relation. The subgraph connecting\nthese nodes is traversed in a top-down manner to\nform a linearized representation; in this case, it will\nyield the linearized string ‘die :ARG1 person :lo-\ncation city’. Note that the sense label of the node\n‘die-01’ is dropped. Moreover, all reversed AMR\nrelations with -of sufﬁx are normalized to their orig-\ninal relation name and direction. In this example\n:location-of is mapped to :location with direction\nreversed. We prepend this linearized AMR path\nstring to the input question text along with a spe-\ncial leading token [AMR ]. The question text also\nstarts with a special leading token [TEXT ]. The\nword aligned to the root of the AMR subgraph is\nmarked as the predicate 2, using special start and\nend predicate tokens [SP] and [EP].\nFigure 3 shows the complete input for the exam-\nple question that goes into the Question Encoder.\nThe same transformer model also serves as relation\nencoder. Relation names are tokenized using BERT\ntokenizer without any additional pre-processing.\nWe add special tokens [AMR ], [TEXT ], [SP] and\n[EP] as well as the AMR relation labels into the\nBERT vocabulary.\nTraining Objective: During training, for each\nexample, scores are computed for the gold relation\nas well as a set of negative examples based on the\ninner product of their vectors with that of the ques-\ntion. For a relation ri with vector representation\nri and a question qn with vector representation qn,\nthe score would be s(ri, qn) =ri.qn. The training\n2The AMR parser of Astudillo et al. (2020) provides node\nto word alignments.\nobjective is to minimize cross-entropy loss between\nthe one-hot gold truth and the vector of predicted\nscores:\nL(rn, qn) =−log\n( exp(s(rn, qn))∑\ni exp(s(ri, qn))\n)\nWe take the top one thousand relations from\ntraining data and use them as negative examples,\nexcluding the gold. We compute the vector repre-\nsentation of all relations only once for each batch\nduring training. Since relation representations are\nindependent of the question representations, they\ncan be reused for all examples in the batch. How-\never, due to parameter update, they need to be com-\nputed anew for each batch.\nInference: During inference, we use s(r, q) for\nscoring and ranking relations. Since the model\nparameters stay ﬁxed, we compute the relation rep-\nresentations for all relations only once. If candidate\nKB relations are available from Entity analysis, we\npick the highest-ranked relation from that set.\n3 Evaluation\nIn this section, we detail our experimental setup and\nevaluate our approach against the state-of-the-art\nKBQA relation linking approaches. For fair com-\nparisons, we replicate the same settings adopted\nby the systems we compare with both in terms of\ndatasets and metrics.\n3.1 Experimental Setup\nBenchmarks: We perform experiments on four\ndatasets targeting two popular KBs, DBpedia and\n259\nWikidata. Each question in these datasets comes\nwith its corresponding SPARQL query, annotated\nwith gold relations. In particular, we used the fol-\nlowing datasets:\n• QALD-9 (Usbeck et al., 2017): a dataset\nbased on DBpedia with 150 test questions in\nnatural language.\n• LC-QuAD 1.0 (Trivedi et al., 2017): another\ndataset based on DBpedia with a total of 5,000\nquestions (4,000 train and 1,000 test) based\non templates.\n• LC-QuAD 2.0 (Dubey et al., 2019): A large\ndataset based on Wikidata with 6,046 test\nquestions and around 24k training questions.\nQuestions in this dataset have good variety\nand complexity levels such as multi-fact ques-\ntions, temporal questions and questions that\nutilise qualiﬁer information.\n• SimpleQuestions (Diefenbach et al., 2017):\nA version of the popular SimpleQuestions\ndataset mapped to Wikidata. It comprises of\n5,622 test questions, and around 19K training\nquestions. As the name implies, all questions\nin this dataset are simple with queries encom-\npassing a single triple in the KB.\nTraining: We train SemReL for DBpedia on the\ntrain data of LC-QuAD 1.0 and QALD-9. In ad-\ndition, we use a subset of 80k examples from the\ndistance supervisions data prepared by Mihinduku-\nlasooriya et al. (2020). This dataset is generated\nby retrieving Wikipedia sentences that contained\npairs of entities from Knowledge Base triples. For\nour experiments, we ﬁlter our the sentences where\nthe AMR path between the entities is more than\ntwo hops. For Wikidata experiments we train out\nsystem on the LC-QuAD 2.0 train dataset. Encoder\nparameters are initialized with the pretrained BERT\nbase model (Wolf et al., 2020).\nBaselines: For the DBpedia-based benchmarks,\nwe compare SemReL with Falcon (Sakor et al.,\n2019a) and SLING (Mihindukulasooriya et al.,\n2020). As for Wikidata-based benchmarks, we\ncompare against Falcon 2.0 (Sakor et al., 2020) and\nKB-Pearl (Lin et al., 2020).\n3The KBPearl paper reports F1 of 0.41 due to a typo but\nits authors conﬁrmed the correct F1 to be 0.52.\nDataset Method P R F1\nQALD-9\nFalcon 0.23 0.23 0.23\nSLING 0.39 0.50 0.44\nSemReL 0.46 0.44 0.45\nLC-QuAD 1.0\nFalcon 0.42 0.44 0.43\nSLING 0.41 0.55 0.47\nSemReL 0.51 0.51 0.51\nLC-QuAD 2.0 Falcon 2.0 0.44 0.37 0.40\nSemReL 0.59 0.38 0.46\nLC-QuAD 2.0 KB-Pearl ∗ 0.57 0.48 0.52 3\n(1942 set) SemReL 0.70 0.45 0.55\nSimple Falcon 2.0 0.35 0.44 0.39\nQuestions SemReL 0.69 0.70 0.69\nTable 1: SemReL compared to SoTA systems on the\nDBpedia (above) and Wikidata (below) benchmarks.\nSetup all one-hop multi-hop\nSemReL 0.51 0.54 0.50\nw/o AMR 0.49 0.53 0.47\nw/o TEXT 0.38 0.37 0.39\nw/o KB rels 0.46 0.48 0.45\nTable 2: SemReL F1 for all, one-hop and multi-hop\nquestions with inputs ablated on LC-QuAD 1.0 testset.\n‘KB rels’ refers to Knowledge Base relation candidates.\n3.2 Results and Discussion\nTable 1 compares SemReL with existing ap-\nproaches. KB-Pearl used a subset of 1,942 test\nquestions in their LC-QuAD 2.0 evaluation. For\nfair comparison, we also evaluate SemReL on the\nsame subset.\nSemReL outperforms all baselines across all\nbenchmarks with respect to F1 score. Note that\nthe baseline systems, specially SLING, achieve\nhigher recall than precision. In contrast, SemReL\nhas either balanced precision and recall, or much\nhigher precision. This is in part due to missing\nentity or slot predictions, indicating that improving\nthe preprocessing can further boost the system’s\nperformance. The results on SimpleQuestions are\nalso worth noting, since the corresponding training\nset was not used in Wikidata model training. We\nalso performed a zero-shot cross-KB experiment\nwhere we test our Wikidata model on a DBpedia\ndataset, LC-QuAD 1.0. The model is tested as\nis, and despite the relation names and granularity\ndifferences, it achieves an F1 of 0.33.\n260\nAblation on Model Inputs: Table 2 shows the\nresults of ablation experiments on LC-QuAD 1.0\ntestset where each of the system inputs are removed\none at a time. As expected, the question text is the\nmost crucial input: when combined with either KB\ncandidates or AMR, it shows good performance.\nWhen AMR is removed, overall score drops by\n2 points; it mostly comes from multi-hop ques-\ntions. This indicates that focusing on different\nsubgraphs of the input AMR improves retrieval\nof multi-hop relations. A similar effect was ob-\nserved on QALD-9 and LC-QuAD 2.0 test sets\nwhen AMR was removed, degrading performance\nby 4.0 and 2.9 points respectively.\nImpact on KBQA Performance: We integrated\nSemReL into the Neuro-Symbolic Question An-\nswering (NSQA) system of Kapanipathi et al.\n(2021). NSQA is a modular system for KBQA\nwhere each sub-task is handled by a different mod-\nule, allowing easy integration of new components.\nWe found that the impact of using AMR in relation\nlinking translates into nice performance gains in\noverall KBQA results. When AMR is incorporated\nin the relation linking module, the system perfor-\nmance on LC-QuAD 1.0 test dataset improves by\n2.4 achieving a new state-of-the-art F1 of 44.5. We\nrefer the reader to the NSQA paper (Kapanipathi\net al., 2021) for more details on the system and\nexperiments.\n4 Related Work\nSeveral relation linking systems have been pro-\nposed recently (Mulang et al., 2017; Singh et al.,\n2017; Dubey et al., 2018; Sakor et al., 2019a; Pan\net al., 2019; Lin et al., 2020). Most of these meth-\nods are rule-based and rely solely on the question\ntext and/or its dependency parse. Therefore, they\ntry to improve their question understanding by us-\ning standard NLP tools such as POS tagging, tok-\nenization n-gram tiling and even lexical database\nsuch as WordNet. FALCON 2.0 (Sakor et al., 2020)\nis a joint entity and relation linking tool over Wiki-\ndata. it uses a search engine indexed with Wiki-\ndata, a pipeline of text processing including POS\ntagging, tokenization, N-gram tiling/splitting and\na catalog of rules for entity and relation linking.\nKBPearl (Lin et al., 2020) is another system that\nperforms joint entity and relation linking to Wiki-\ndata. It ﬁrst create a semantic graph of text using\nOpenIE and maps both entities and relations to a\ngiven KB.\nHowever, none of the above mentioned methods\nfor KBQA perform relation linking on two different\nKBs using the same system whilst our work is the\nﬁrst to perform relation linking over both DBpedia\nand Wikidata using the same system. In addition,\nsome of these systems are KG-speciﬁc; e.g. Fal-\ncon (Sakor et al., 2019a) vs. Falcon 2.0 (Sakor\net al., 2020), where adapting it from one KG to\nanother requires non-trivial changes. Unlike these\nsystems, SemReL leverages well-established se-\nmantic parsers such as AMR to achieve out-of-the-\nbox better question representation.\nSimilar to our approach, SLING (Mihindukula-\nsooriya et al., 2020) is a relation linking framework\nbased on DBpedia which leverages semantic pars-\ning using AMR and distant supervision. It con-\nsists of four distinct modules that capture different\nsignals such as linguistic cues, semantic represen-\ntation, and information from the knowledge base.\nUnlike SLING, SemReL is a KG-agnostic, single\nend-to-end neural model that does not require vari-\nous ensemble components and yet achieves state-\nof-the-art performance on DBPedia and Wikidata\ndatasets.\n5 Conclusions and Future Work\nIn this paper, we present a simple transformer-\nbased neural model for relation linking that lever-\nages the semantic structure of a sentence. In con-\ntrast to existing systems such as SLING and Falcon,\nwhich are either rule-based or ensembles of several\ncomponents, our neural architecture enables us to\nadapt the system to multiple KGs (e.g. DBpedia\nand Wikidata). It outperforms state-of-the-art sys-\ntems on a variety of benchmarks.\nOur ablation study shows that including the\nAMR graph improves performance, even with the\nrelatively simple encoding scheme (plain text). In\nfuture, we will explore modeling the graph struc-\nture explicitly. This model also relies on a determin-\nistic slot-ﬁnding algorithm based on AMR. While\nthis identiﬁes the correct relation slots most of the\ntime, it is rule-based, and not always correct. In\nfuture work, we will explore learning algorithms to\nidentify the slots from the AMR graph.\nFinally, AMR parsers can be trained jointly with\nthe relation linking objective. Currently, these\nparsers are sensitive to small changes in the input.\nJoint training can make them robust against text\nvariations and more sensitive to the errors affecting\nslot identiﬁcation and relation prediction.\n261\nReferences\nRam´on Fernandez Astudillo, Miguel Ballesteros,\nTahira Naseem, Austin Blodgett, and Radu Flo-\nrian. 2020. Transition-based parsing with stack-\ntransformers. In Findings of the Association for\nComputational Linguistics: EMNLP 2020 , page\n1001–1007.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nDennis Diefenbach, Thomas Pellissier Tanon, Ka-\nmal Deep Singh, and Pierre Maret. 2017. Question\nanswering benchmarks for wikidata. In Proceedings\nof the ISWC 2017 Posters & Demonstrations and In-\ndustry Tracks co-located with 16th International Se-\nmantic Web Conference (ISWC 2017), Vienna, Aus-\ntria, October 23rd - to - 25th, 2017.\nMohnish Dubey, Debayan Banerjee, Abdelrahman Ab-\ndelkawi, and Jens Lehmann. 2019. LC-QuAD 2.0:\nA Large Dataset for Complex Question Answering\nover Wikidata and DBpedia. In The Semantic Web -\nISWC 2019 - 18th International Semantic Web Con-\nference, Auckland, New Zealand, October 26-30,\n2019, Proceedings, Part II, volume 11779 ofLecture\nNotes in Computer Science, pages 69–78. Springer.\nMohnish Dubey, Debayan Banerjee, Debanjan Chaud-\nhuri, and Jens Lehmann. 2018. Earl: joint entity and\nrelation linking for question answering over knowl-\nedge graphs. In ISWC, pages 108–126.\nPavan Kapanipathi, Ibrahim Abdelaziz, Srinivas Rav-\nishankar, Salim Roukos, Alexander Gray, Ramon\nAstudillo, Maria Chang, Cristina Cornelio, Saswati\nDana, Achille Fokoue, et al. 2021. Leveraging ab-\nstract meaning representation for knowledge base-\nquestion answering. Findings of the Association for\nComputational Linguistics: ACL 2021.\nYoung-Suk Lee, Ramon Astuillo, Tahira Naseem, Re-\nvanth Reddy, Radu Florian, and Salim Roukos. 2020.\nPushing the limits of amr parsing with self-learning.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2020, page 3208–3214.\nJungwoo Lim, Dongsuk Oh, Yoonna Jang, Kisu Yang,\nand Heuiseok Lim. 2020. I know what you asked:\nGraph path learning using amr for commonsense rea-\nsoning. arXiv preprint arXiv:2011.00766.\nXueling Lin, Haoyang Li, Hao Xin, Zijian Li, and Lei\nChen. 2020. Kbpearl: a knowledge base population\nsystem supported by joint entity and relation linking.\nProceedings of the VLDB Endowment, 13(7):1035–\n1049.\nNandana Mihindukulasooriya, Gaetano Rossiello, Pa-\nvan Kapanipathi, Ibrahim Abdelaziz, Srinivas Rav-\nishankar, Mo Yu, Alﬁo Gliozzo, Salim Roukos, and\nAlexander Gray. 2020. Leveraging semantic parsing\nfor relation linking over knowledge bases. In Inter-\nnational Semantic Web Conference, pages 402–419.\nSpringer.\nIsaiah Onando Mulang’, Jennifer D’Souza, and S ¨oren\nAuer. 2020. Fine-tuning BERT with focus words\nfor explanation regeneration. In Proceedings of the\nNinth Joint Conference on Lexical and Computa-\ntional Semantics, pages 125–130, Barcelona, Spain\n(Online). Association for Computational Linguis-\ntics.\nIsaiah Onando Mulang, Kuldeep Singh, and Fabrizio\nOrlandi. 2017. Matching natural language relations\nto knowledge graph properties for question answer-\ning. In SEMANTiCS 2017, pages 89–96.\nJeff Z Pan, Mei Zhang, Kuldeep Singh, Frank van\nHarmelen, Jinguang Gu, and Zhi Zhang. 2019. En-\ntity enabled relation linking. In International Se-\nmantic Web Conference, pages 523–538. Springer.\nAhmad Sakor, Isaiah Onando Mulang, Kuldeep Singh,\nSaeedeh Shekarpour, Maria Esther Vidal, Jens\nLehmann, and S ¨oren Auer. 2019a. Old is gold: lin-\nguistic driven approach for entity and relation link-\ning of short text. In NAACL: HLT 2019, pages 2336–\n2346.\nAhmad Sakor, Isaiah Onando Mulang’, Kuldeep Singh,\nSaeedeh Shekarpour, Maria Esther Vidal, Jens\nLehmann, and S¨oren Auer. 2019b. Old is gold: Lin-\nguistic driven approach for entity and relation link-\ning of short text. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 2336–2346, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nAhmad Sakor, Kuldeep Singh, Anery Patel, and Maria-\nEsther Vidal. 2020. Falcon 2.0: An entity and rela-\ntion linking tool over wikidata. In Proceedings of\nthe 29th ACM International Conference on Informa-\ntion & Knowledge Management, pages 3141–3148.\nKuldeep Singh, Isaiah Onando Mulang’, Ioanna Ly-\ntra, Mohamad Yaser Jaradeh, Ahmad Sakor, Maria-\nEsther Vidal, Christoph Lange, and S ¨oren Auer.\n2017. Capturing knowledge in semantically-typed\nrelational patterns to enhance relation linking. In K-\nCAP 2017, pages 1–8.\nPriyansh Trivedi, Gaurav Maheshwari, Mohnish\nDubey, and Jens Lehmann. 2017. Lc-quad: A cor-\npus for complex question answering over knowledge\ngraphs. In ISWC 2017, pages 210–218.\nRicardo Usbeck, Axel-Cyrille Ngonga Ngomo, Bas-\ntian Haarmann, Anastasia Krithara, Michael R ¨oder,\nand Giulio Napolitano. 2017. 7th open challenge\non question answering over linked data (qald-7). In\nSemantic Web Evaluation Challenge, pages 59–69.\nSpringer.\n262\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. arXiv preprint arXiv:1706.03762.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nLedell Wu, Fabio Petroni, Martin Josifoski, Sebastian\nRiedel, and Luke Zettlemoyer. 2019. Zero-shot\nentity linking with dense entity retrieval. ArXiv,\nabs/1911.03814.",
  "topic": "Question answering",
  "concepts": [
    {
      "name": "Question answering",
      "score": 0.5913084745407104
    },
    {
      "name": "Computer science",
      "score": 0.5682312250137329
    },
    {
      "name": "Natural language processing",
      "score": 0.5419912934303284
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.5270412564277649
    },
    {
      "name": "Knowledge base",
      "score": 0.5248613357543945
    },
    {
      "name": "Transformer",
      "score": 0.5193222761154175
    },
    {
      "name": "Computational linguistics",
      "score": 0.4457451105117798
    },
    {
      "name": "Artificial intelligence",
      "score": 0.42119282484054565
    },
    {
      "name": "Linguistics",
      "score": 0.39526891708374023
    },
    {
      "name": "Programming language",
      "score": 0.355435311794281
    },
    {
      "name": "Information retrieval",
      "score": 0.34114789962768555
    },
    {
      "name": "Engineering",
      "score": 0.20770740509033203
    },
    {
      "name": "Philosophy",
      "score": 0.13469082117080688
    },
    {
      "name": "Electrical engineering",
      "score": 0.07116886973381042
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}