{
  "title": "A forest fire smoke detection model combining convolutional neural network and vision transformer",
  "url": "https://openalex.org/W4366212872",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2102085438",
      "name": "Ying Zheng",
      "affiliations": [
        "Central South University",
        "Central South University of Forestry and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2112393542",
      "name": "Gui Zhang",
      "affiliations": [
        "Central South University",
        "Central South University of Forestry and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2489676878",
      "name": "Sanqing Tan",
      "affiliations": [
        "Central South University of Forestry and Technology",
        "Central South University"
      ]
    },
    {
      "id": "https://openalex.org/A2109017532",
      "name": "ZhiGao Yang",
      "affiliations": [
        "Central South University of Forestry and Technology",
        "Central South University"
      ]
    },
    {
      "id": "https://openalex.org/A2116348822",
      "name": "Dongxin Wen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4209920743",
      "name": "Huashun Xiao",
      "affiliations": [
        "Central South University",
        "Central South University of Forestry and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2102085438",
      "name": "Ying Zheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2112393542",
      "name": "Gui Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2489676878",
      "name": "Sanqing Tan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2109017532",
      "name": "ZhiGao Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4209920743",
      "name": "Huashun Xiao",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2508764147",
    "https://openalex.org/W2964169840",
    "https://openalex.org/W3128592650",
    "https://openalex.org/W2954826145",
    "https://openalex.org/W3186706259",
    "https://openalex.org/W4306248702",
    "https://openalex.org/W4309002430",
    "https://openalex.org/W2091115035",
    "https://openalex.org/W4210783934",
    "https://openalex.org/W3009795164",
    "https://openalex.org/W4206930139",
    "https://openalex.org/W2756554574",
    "https://openalex.org/W2998269827",
    "https://openalex.org/W4213019189",
    "https://openalex.org/W2761350709",
    "https://openalex.org/W2913160612",
    "https://openalex.org/W2090748520",
    "https://openalex.org/W3194502875",
    "https://openalex.org/W3124539583",
    "https://openalex.org/W3158505286",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W4283729464",
    "https://openalex.org/W3154414470",
    "https://openalex.org/W2901461790",
    "https://openalex.org/W2083854930",
    "https://openalex.org/W2884821113",
    "https://openalex.org/W2791209104",
    "https://openalex.org/W3016977638",
    "https://openalex.org/W4287643567",
    "https://openalex.org/W3154450391",
    "https://openalex.org/W4321214536",
    "https://openalex.org/W2616247523",
    "https://openalex.org/W4312626766",
    "https://openalex.org/W4200519212",
    "https://openalex.org/W4283073926",
    "https://openalex.org/W2560920409",
    "https://openalex.org/W2967154765",
    "https://openalex.org/W2052933845",
    "https://openalex.org/W2905300275",
    "https://openalex.org/W3209859545",
    "https://openalex.org/W2399455941",
    "https://openalex.org/W4210692941",
    "https://openalex.org/W2791569356",
    "https://openalex.org/W4207031702",
    "https://openalex.org/W2764034829",
    "https://openalex.org/W2782522152",
    "https://openalex.org/W3102564565"
  ],
  "abstract": "Forest fires seriously jeopardize forestry resources and endanger people and property. The efficient identification of forest fire smoke, generated from inadequate combustion during the early stage of forest fires, is important for the rapid detection of early forest fires. By combining the Convolutional Neural Network (CNN) and the Lightweight Vision Transformer (Lightweight ViT), this paper proposes a novel forest fire smoke detection model: the SR-Net model that recognizes forest fire smoke from inadequate combustion with satellite remote sensing images. We collect 4,000 satellite remote sensing images, 2,000 each for clouds and forest fire smoke, from Himawari-8 satellite imagery located in forest areas of China and Australia, and the image data are used for training, testing, and validation of the model at a ratio of 3:1:1. Compared with existing models, the proposed SR-Net dominates in recognition accuracy (96.9%), strongly supporting its superiority over benchmark models: MobileNet (92.0%), GoogLeNet (92.0%), ResNet50 (84.0%), and AlexNet (76.0%). Model comparison results confirm the accuracy, computational efficiency, and generality of the SR-Net model in detecting forest fire smoke with high temporal resolution remote sensing images.",
  "full_text": "TYPE Original Research\nPUBLISHED /one.tnum/seven.tnum April /two.tnum/zero.tnum/two.tnum/three.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/ﬀgc./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/three.tnum/six.tnum/nine.tnum/six.tnum/nine.tnum\nOPEN ACCESS\nEDITED BY\nAna Cristina Russo,\nUniversity of Lisbon, Portugal\nREVIEWED BY\nTomás Calheiros,\nUniversity of Lisbon, Portugal\nHuaiqing Zhang,\nChinese Academy of Forestry, China\n*CORRESPONDENCE\nGui Zhang\ncsfu/three.tnums@/one.tnum/six.tnum/three.tnum.com\nSPECIALTY SECTION\nThis article was submitted to\nFire and Forests,\na section of the journal\nFrontiers in Forests and Global Change\nRECEIVED /zero.tnum/three.tnum January /two.tnum/zero.tnum/two.tnum/three.tnum\nACCEPTED /three.tnum/one.tnum March /two.tnum/zero.tnum/two.tnum/three.tnum\nPUBLISHED /one.tnum/seven.tnum April /two.tnum/zero.tnum/two.tnum/three.tnum\nCITATION\nZheng Y, Zhang G, Tan S, Yang Z, Wen D and\nXiao H (/two.tnum/zero.tnum/two.tnum/three.tnum) A forest ﬁre smoke detection\nmodel combining convolutional neural\nnetwork and vision transformer.\nFront. For. Glob. Change/six.tnum:/one.tnum/one.tnum/three.tnum/six.tnum/nine.tnum/six.tnum/nine.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/ﬀgc./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/three.tnum/six.tnum/nine.tnum/six.tnum/nine.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/three.tnum Zheng, Zhang, Tan, Yang, Wen and\nXiao. This is an open-access article distributed\nunder the terms of the\nCreative Commons\nAttribution License (CC BY) . The use,\ndistribution or reproduction in other forums is\npermitted, provided the original author(s) and\nthe copyright owner(s) are credited and that\nthe original publication in this journal is cited, in\naccordance with accepted academic practice.\nNo use, distribution or reproduction is\npermitted which does not comply with these\nterms.\nA forest ﬁre smoke detection\nmodel combining convolutional\nneural network and vision\ntransformer\nYing Zheng, Gui Zhang *, Sanqing Tan, Zhigao Yang, Dongxin Wen\nand Huashun Xiao\nCollege of Forestry, Central South University of Forestry and Tech nology, Changsha, China\nForest ﬁres seriously jeopardize forestry resources and endan ger people and\nproperty. The eﬀicient identiﬁcation of forest ﬁre smoke, gen erated from\ninadequate combustion during the early stage of forest ﬁres, is important for\nthe rapid detection of early forest ﬁres. By combining the Conv olutional Neural\nNetwork (CNN) and the Lightweight Vision Transformer (Lightwe ight ViT), this\npaper proposes a novel forest ﬁre smoke detection model: the SR -Net model that\nrecognizes forest ﬁre smoke from inadequate combustion with sa tellite remote\nsensing images. We collect /four.tnum,/zero.tnum/zero.tnum/zero.tnum satellite remote sensing images, /two.tnum,/zero.tnum/zero.tnum/zero.tnum each\nfor clouds and forest ﬁre smoke, from Himawari-/eight.tnum satellite imagery located in\nforest areas of China and Australia, and the image data are used for training,\ntesting, and validation of the model at a ratio of /three.tnum:/one.tnum:/one.tnum. Compared with existing\nmodels, the proposed SR-Net dominates in recognition accuracy (/nine.tnum/six.tnum./nine.tnum%), strongly\nsupporting its superiority over benchmark models: MobileNe t (/nine.tnum/two.tnum./zero.tnum%), GoogLeNet\n(/nine.tnum/two.tnum./zero.tnum%), ResNet/five.tnum/zero.tnum (/eight.tnum/four.tnum./zero.tnum%), and AlexNet (/seven.tnum/six.tnum./zero.tnum%). Model comparison results conﬁrm\nthe accuracy, computational eﬃciency, and generality of the SR-Net m odel in\ndetecting forest ﬁre smoke with high temporal resolution remo te sensing images.\nKEYWORDS\nforest ﬁre smoke, detection model, convolutional neural network , vision transformer,\nlightweight model\n/one.tnum. Introduction\nForest ﬁres pose a serious threat to forest resources and people’s lives and property. In\nthe early stages of a forest ﬁre, the low temperature makes it diﬃcult for satellites detection.\nHowever, inadequate combustion of combustible materials produces large amounts of\nsmoke (\nWang Z. et al., 2022 ), presenting from the ignition to the extinguish of forest ﬁres.\nTherefore, forest ﬁre smoke could be an important indicator of the occurrence of early forest\nﬁre. Timely capture of forest ﬁre smoke allows earlier detection of forest ﬁres compared to\nthe monitoring of infrared reﬂections of forest ﬁres. Recent development in “high-altitude”\nsatellite remote sensing technology (\nZhang et al., 2022 ) makes it possible to detect forest\nﬁre smoke with remote sensing satellites. The application of remote sensing satellites in\ndetecting forest ﬁre smoke not only remedies the defects of “low-altitude” cameras in forest\nareas, including small monitoring range, poor stability, and high cost (\nJia et al., 2016 ; Wu\net al., 2020 ; Govil et al., 2022 ), but also solves the issues of “mid-altitude” Unmanned Aerial\nVehicles (UAV), including constraints of air traﬃc controls and weather conditions and short\nendurance (\nAllison et al., 2016 ; Howard et al., 2018 ; Pérez-Rodríguez et al., 2020 ). Moreover,\nsatellite remote sensing obtains timely and accurate information on forest ﬁre smoke given\nFrontiers in Forests and Global Change /zero.tnum/one.tnum frontiersin.org\nZheng et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/ﬀgc./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/three.tnum/six.tnum/nine.tnum/six.tnum/nine.tnum\nits advantages of large detection range, short response time, and\nstrong anti-interference ability (\nLi et al., 2015 ; Filonenko et al.,\n2018). Although the infrared detection of high-temperature sites\nhas been extensively studied, there is limited research on the\napplication of satellite remote sensing in monitoring forest ﬁre\nsmoke for early-stage forest ﬁre detections.\nThe essential of forest ﬁre smoke detection with satellite\nremote sensing is the accurate identiﬁcation of forest ﬁre smoke,\nwhich requires constructing and optimizing the forest ﬁre smoke\nidentiﬁcation algorithms.\nXie et al. (2007) propose a multi-channel\nthreshold method based on MODIS data, which eliminates pixels\nof other land objects in the research area, by choosing diﬀerent\nthresholds, to extract smoke pixels. Compared with their model,\nthe model obtained by network training using deep learning\ncan signiﬁcantly improve the detection accuracy of the forest\nﬁre smoke (\nZhu et al., 2017 ). Based on multi-temporal and\nmulti-spectral features, Chrysoulakis et al. (2007) use the multi-\nimage temporal diﬀerentials algorithm to improve forest ﬁre\nsmoke detection. Their method identiﬁes the forest ﬁre smoke by\ndiscriminating the images of smoke from other ground objects\nwith spectral diﬀerences. Convolutional Neural Networks (CNN),\nas a representative algorithm of deep learning, has promising\napplications in the ﬁeld of image classiﬁcation (\nLi et al., 2015 ; Zheng\net al., 2019 ) and has been applied to forest ﬁre smoke detection\nof satellite remote sensing images ( Zheng et al., 2022 ). Li et al.\n(2019) propose a forest ﬁre smoke identiﬁcation model based on\nthe Back Propagation Neural Network (BPNN). By integrating\nthe multi-threshold approach and the BPNN classiﬁcation, their\nmethod, trained with MODIS data, detects smoke by examining\nthe spectral characteristics among the forest ﬁre smoke and other\nland objects.\nBa et al. (2019) further improve the accuracy of\nCNN for forest ﬁre smoke detection with remote sensing images\nby incorporating spatial and channel-wise attentions in CNN to\ncomb spatial features and other information from medium and\nhigh spatial resolution satellite remote sensing images. Vision\nTransformer (ViT), proposed by Google in 2020, is a model\nthat applies Transformer to image classiﬁcation and recognition\n(\nBazi et al., 2021 ). Compared to CNN, this model has a better\nrecognition performance with great extensibility, since it learns\nmore comprehensive target features (\nHan et al., 2022 ). ViT can\noutperform CNN given suﬃcient samples for pre-training. In the\narea of image classiﬁcation and recognition, ViT is pre-trained\nusing large-scale datasets (containing ∼ 1.4–3 billion images) and\nmigrated to small or medium-scale datasets to undertake speciﬁc\ntasks, achieving 94.55% accuracy on the CIFAR-100 dataset (\nBazi\net al., 2021 ). Unlike CNN, which has inductive bias, ViT requires\nmore data for training to avoid over-ﬁtting. The inductive bias,\nalso called prior knowledge, of CNN, speciﬁcally refers to two\nmain aspects: ﬁrst is its locality, that is, the CNN considers that\nadjacent regions on the image have adjacent features; and second is\nits transitional invariance, which means the detection target always\nhas the same prediction label no matter where it is moved to in the\nimage. Without these two aspects, ViT requires relatively more data\nto learn a better model than CNN. However, due to the constraints\nof the in-orbit lifetime of remote sensing satellites, geographical\ncoverage, and other conditions, there is only limited amount of\nremote sensing image data. It is diﬃcult to obtain a dataset of\nremote sensing images, containing forest ﬁre smoke, that large\nenough to avoid overﬁtting when training the Vit model (\nZhang\net al., 2018). How to accurately identify forest ﬁre smoke with small-\nscale remote sensing image datasets is the key research question for\neﬀective remote sensing detections of forest ﬁre smoke.\nTo address this question, this paper proposes a novel forest\nﬁre smoke detection model: the SR-Net model by combining\nCNN and Lightweight ViT. We construct a small-scale remote\nsensing image dataset using high temporal resolution remote\nsensing images from the Himawari-8 geostationary satellite. The\nfront part of the SR-Net model uses CNN for inductive bias, and\nthe back part uses the global attention of Lightweight ViT. We\nconﬁrm that the SR-Net model can detect a forest ﬁre smoke with\nhigher accuracy and less training resources. The study compares\nand analyzes the performance of SR-Net with benchmark models\nincluding: AlexNet, MobileNet, GoogLeNet, and ResNet50 models\nto comprehensively assess the application potential of SR-Net for\nforest ﬁre smoke detections. We document supportive evidence\nthat the SR-Net consistently outperform all benchmark models\nin terms of Accuracy, Precision, Recall, F1-Score, and Kappa\nCoeﬃcient on both the validation and test sets.\nOverall, our paper makes the following contributions to the\nexisting literature:\nFirst, there have been very few studies on forest ﬁre smoke\ndetection with remote sensing satellites. Previous studies like\nLi\net al. (2015) and Ba et al. (2019) use imagery datasets collected from\npolar orbit satellites. In this paper, we extent existing studies by\nconstructing the dataset originating from the Himawari-8 satellite\nwith high-temporal resolution. Our dataset not only allows forest\nﬁre smoke detection with diﬀerent spatial satellites but also leads\nto the timely detection of forest ﬁre smoke, which improves the\nmonitoring of early forest ﬁres.\nSecond, the state of the art in pattern classiﬁcation and\nrecognition is the CNN and ViT models and both models\nhave limitations in forest ﬁre smoke detection. Unlike CNN,\nViT does not have inductive bias. Although ViT outperforms\nCNN, it requires large amounts of data for pre-processing.\nHowever, only limited remote sensing images containing forest\nﬁre smoke are available since the remote sensing data collection\nis limited by conditions such as the in-orbit lifetime of remote\nsensing satellites.\nThe proposed SR-Net model is an innovative lightweight\nmodel tailored to forest ﬁre smoke detection with limited remote\nsensing imagery data. The SR-Net model combines the advantages\nof both CNN and ViT models. The number of parameters\nof the SR-Net model is lowered to six million, indicating\nsigniﬁcantly lower computational consumption. Compared to\nexisting models, our model is superior in computational eﬃciency,\ngeneralization capability, robustness to environmental disturbance,\nand recognition accuracy. Further application of our model in\nforest ﬁre detection could be promising.\nThe rest of this paper is organized as follows. Section 2\nintroduces our new constructed dataset, presents the proposed\nmodel, and illustrates the evaluation and visualization methods.\nSection 3 reports the results of experiments and the comparison\nof models. Section 4 discusses the empirical results. Finally, we\nconclude the paper in Section 5.\nFrontiers in Forests and Global Change /zero.tnum/two.tnum frontiersin.org\nZheng et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/ﬀgc./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/three.tnum/six.tnum/nine.tnum/six.tnum/nine.tnum\nFIGURE /one.tnum\nForest ﬁre smoke spots and examples of forest ﬁre smoke images from some of these spots. (A) Denotes China, and (B) denotes Australia.\n/two.tnum. Materials and methods\n/two.tnum./one.tnum. Data acquisition and processing\nThe remote sensing image data used in the study are derived\nfrom the Himawari-8 geostationary satellite. The Himawari-8\nsatellite has the advantages of high timeliness and stable data\nquality (\nYumimoto et al., 2016 ). Therefore, compared with polar\norbit satellites, the Himawari-8 satellite can provide more timely\nfeedback of remote sensing image information (\nJang et al., 2019 ).\nIn the study, the full-disk remote sensing images of the\nHimawari-8 satellite are ﬁrst acquired. The forest ﬁre smoke spots\nare marked in\nFigure 1, which can be seen more directly. And the\nspeciﬁc information of remote sensing images containing forest ﬁre\nsmoke is derived from the conﬁrmed historical forest ﬁres, whose\nspeciﬁc acquisition date, location, longitude, and latitude are shown\nin\nTable 1, in forest areas of China and Australia. What’s more, the\nremote sensing images containing clouds are acquired through the\nrandom sampling method. And then, we extract three visible bands:\nBand1, Band2, and Band3 of remote sensing images (\nTable 2).\nFinally, true color remote sensing images are synthesized by these\nthree bands for model training, validation, and testing, and are pre-\nprocessed, including geometric correction, radiometric calibration,\nand atmospheric correction, to compensate for distortions in the\nimaging process.\nAs clouds and forest ﬁre smoke are similar in color, shape and\nother features on true color remote sensing images, the accurate\ndiﬀerentiation between clouds and forest ﬁre smoke is crucial\nfor early warning of forest ﬁres. After pre-processing, we clip\nand classify these true color remote sensing images, of which\nFrontiers in Forests and Global Change /zero.tnum/three.tnum frontiersin.org\nZheng et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/ﬀgc./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/three.tnum/six.tnum/nine.tnum/six.tnum/nine.tnum\nTABLE /one.tnum Speciﬁc acquisition information of date, location, longitude, and latitude of remote sensing images containing forest ﬁre s moke.\nDate Location Longitude and latitude\n2020.3.29 Lijiang City, Yunnan Province 100.9838E, 26.9710N\n100.0690E, 26.7162N\n101.0687E, 26.9808N\n2020.3.30 to 2020.3.31 Xichang City, Sichuan Province 101.3214E, 27.9653N\n2021.1.7 Ganzi Tibetan Autonomous Prefecture, Sichuan Province 100.4013E, 28.8039N\n100.8957E, 28.0091N\n101.6373E, 28.3334N\n2021.2.20 Border of Henan Province with Shanxi Province 113.1757E, 35.4786N\n2019.3.29 to 2019.3.30 Changzhi City, Shanxi Province 112.5494E, 36.7741N\n2021.10.27 to 2021.10.28 Linzhi City, Tibet 97.3856E, 28.8735N\n2022.8.21 Banan District, Chongqing City 105.1694E, 29.1426N\n105.7187E, 28.6761N\n2019.12.21 to 2019.12.31 Queensland, Australia 143.4869E, 18.9582S\n2015.11.20 to 2015.11.27 Queensland, Australia 146.6235E, 26.2343S\n142.5751E, 18.3024S\n144.3109E, 19.8391S\n143.2782E, 17.8637S\n2019.9.7 to 2019.9.16 Queensland, Australia 143.0585E, 17.0673S\n144.9701E, 17.9369S\n2019.10.9 to 2019.10.15 Queensland, Australia 152.5122E, 28.9793S\n115.9222E, 33.5917S\n133.1982E, 13.9874S\n2019.12.21 to 2019.12.31 New South Wales, Australia 151.3312E, 31.5785S\n150.2216E, 33.2387S\n152.2650E, 26.4607S\n2019.12.21 to 2019.12.31 Queensland, Australia 143.4869E, 18.9582S\n143.2535E, 19.8081S\nTABLE /two.tnum Information of the Band /one.tnum, Band /two.tnum, and Band /three.tnum of the Himawari-/eight.tnum satellite.\nBand Central wavelength Temporal resolution Numbers of pixels\n(µ m) (min.)\nBand 1 0.46 10 11,000 ∗ 11,000\nBand 2 0.51 10 11000 ∗ 11,000\nBand 3 0.64 10 22,000 ∗ 22,000\n2,000 sample images contained clouds and 2,000 sample images\ncontained forest ﬁre smoke, with ﬁxed size and bit depth.\nFigure 2\nshows typical remote sensing sample images of cloud and forest ﬁre\nsmoke. The forest ﬁre smoke is marked with red arrow and the\ncloud is marked with green arrow. According to models’ training\nrules based on small-scale datasets, the remote sensing sample\nimages of cloud and forest ﬁre smoke are randomly selected in the\nratio of 3(Training Set): 1(Validation Set): 1(Test Set), respectively,\neach containing 1,200, 400, and 400 remote sensing sample images.\nThe Training Set is used to ﬁt the parameters of the forest ﬁre smoke\ndetection models, the Validation Set is used to adjust the hyper-\nparameters of the model and evaluate the ﬁtted model, and the Test\nSet is used to evaluate the performance and verify the generalization\nability of the model.\n/two.tnum./two.tnum. Model structure and implementation\nCNN focuses only on local features with translation invariance\nand rotation invariance, but there is still room to improve its\nFrontiers in Forests and Global Change /zero.tnum/four.tnum frontiersin.org\nZheng et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/ﬀgc./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/three.tnum/six.tnum/nine.tnum/six.tnum/nine.tnum\nFIGURE /two.tnum\nExample remote sensing sample images of cloud (marked with green arrow) and forest ﬁre smoke (marked with red arrow).\nperformance, for instance, it holds a limited amount of spatial\ninformation (\nKattenborn et al., 2021 ). Compared with CNN, ViT\nperforms better but has shortcomings, such as the large model size,\nwhich makes training more diﬃcult; the need to use large-scale\ndataset for inductive bias in advance; and the need to add additional\ndecoders for migration to downstream tasks (\nWei et al., 2022 ).\nIn this paper, we draw on the advantages of CNN and ViT to\npropose a new lightweight forest ﬁre smoke detection model (SR-\nNet). The proposed SR-Net model takes into account the diﬃculty\nof obtaining suﬃciently large-scale remote sensing datasets of forest\nﬁre smoke and optimizes the learning eﬀect of the model on a small-\nscale remote sensing dataset of forest ﬁre smoke. The network\nstructure of the SR-Net model is shown in\nTable 3.\nThe main body of the SR-Net model uses the Inverted Residual\nBlock, similar to MobileNet, with the input of low-dimensional\nfeatures and uses Pointwise (PW) Convolution to reduce the\ncomputational complexity (\nCan et al., 2021 ).\nFirstly, the channels of the feature pattern are expanded\nthrough the 1 × 1 PW convolution to enrich the number of\nfeatures. Secondly, features are extracted through Depthwise (DW)\nConvolution, which can reduce the number of parameters and\ncomputational burden. The Depthwise Separable Convolution is\none of the important part of Mobilenet V2, whose small number\nof parameters and computational eﬀort compensates for the large\ncomputational eﬀort of the ViT, allowing the model to achieve a\nInput: Feature mapDfDf\nStep 1 & 2:DGDGM = DfDf MAC DkDk\nStep 3:\nDGDGN = Conv1_1(ReLU (BN (DGDGM)))\nOutput:DGDGN\nAlgorithm /one.tnum. Depth wise separable convolution.\nbalance between eﬃciency and accuracy. The DW Convolution has\nthree steps. In the ﬁrst step, a convolution kernel of size DkDk is\nused on an input feature image of size Df Df to do the Multiply\nAccumulation operation. In the second step, the convolution frame\nis slid in a left-to-right, top-to-bottom order and with a certain step\nsize. The operation in the ﬁrst step is repeated to obtain a single-\nchannel feature image of size DGDG. In the third step, the feature\nimages of DGDGM dimension are kept as the output features of\nthis layer. And the output feature image of DW convolution is\nprocessed by the Batch Normalization layer and activation function\nand then input to the PW Convolution layer. The PW convolution\nlayer uses N convolution kernels of 1 ∗1 size to map the feature\nimage from the M-dimensional linear space to the N-dimensional\nspace to obtain the output feature image of DGDGN. From the\nabove process, the precise algorithm is given in\nAlgorithm 1.\nFrontiers in Forests and Global Change /zero.tnum/five.tnum frontiersin.org\nZheng et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/ﬀgc./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/three.tnum/six.tnum/nine.tnum/six.tnum/nine.tnum\nTABLE /three.tnum The network structure of the SR-Net model.\nOutput size SR-Net Model\n128 × 128 Conv, 3 × 3, 16, stride 2\n\n\n\n\n\n\nConv, 1 × 1, 16\nDWconv, 3 × 3, 64\nConv, 1 × 1, 32\n\n\n\n\n\n\n× 1\n64 × 64\n\n\n\n\n\n\nConv, 1 × 1, 32\nDWconv, 3 × 3, 128\nConv, 1 × 1, 64\n\n\n\n\n\n\n× 3\n32 × 32\n\n\n\n\n\n\nConv, 1 × 1, 64\nDWconv, 3 × 3, 256\nConv, 1 × 1, 96\n\n\n\n\n\n\n× 1\nLightweight VIT Block × 2\n16 × 16\n\n\n\n\n\n\nConv, 1 × 1, 96\nDWconv, 3 × 3, 384\nConv, 1 × 1, 128\n\n\n\n\n\n\n× 1\nLightweight VIT Block × 4\n8 × 8\n\n\n\n\n\n\nConv, 1 × 1, 128\nDWconv, 3 × 3, 512\nConv, 1 × 1, 160\n\n\n\n\n\n\n× 1\nLightweight VIT Block × 3\nConv, 1 × 1, 640, stride 1\n1 × 1 Average Pool, 7 × 7, stride 1\nFC, Softmax, 2\nIn this table, “Conv” means convolution, “DWconv” means Depthwise Con volution,\n“Lightweight VIT” means Lightweight Vision Transformer, and “F C” means Fully\nConnected Layer.\nFinally, convolution is used to downscale the output features to\nbuild a highly accurate deep network structure ( Figure 3).\nMeanwhile, the SR-Net model is alternatively added the\nLightweight Vision Transformer (Lightweight VIT) Block to its\nnetwork (\nFigure 3). To be speciﬁc, after the extraction of local\nfeatures through convolution layers, the features are embedded\ninto patches. And then the global information is obtained using\nthe Multi-head Attention and Multilayer Perceptron (MLP). The\nMulti-headed attention is a mechanism that can be used to improve\nthe performance of the general Self-attention layer (\nLi et al.,\n2021). The Single-headed attention layer restricts the ability of\nthe model to focus on one or more speciﬁc locations without\nsimultaneously aﬀecting the attention to other equally important\nlocations. This is achieved by giving the attention layer a diﬀerent\nrepresentation subspace. To be speciﬁc, diﬀerent attention heads\nuse diﬀerent query, key, and value matrices. These matrices, due\nto random initialization, can project the trained input vectors\ninto diﬀerent representation subspaces and are processed by\nmultiple independent attention heads in parallel, with the resultant\nvectors aggregated and mapped to the ﬁnal output. The process\nof the Multi-head Self-attention mechanism can be expressed as\nAlgorithm 2.\nInput:Feature Map F\nStep 1:Patches = Patch Embedding (F)\nStep 2:X, Y, Z = Linear Projection (Patches)\nStep 3:\nQi = XWQi , Ki = YWKi , Vi = ZWVi\nStep 4:\nZi = Attention (Qi, Ki, Vi) , i = 1 . . .h\nStep 5:MultiHead(Q, K, V) =\nConcat (Z1, Z2, . . ., Zh) Wo\nOutput:MultiHead Result\nAlgorithm /two.tnum. Vision transformer.\nIn Algorithm 2, i denotes the header number, the number\nrange is 1 to h. WO ∈ Rhdv× dmodel denotes the output projection\nmatrix. Zi denotes the output matrix of each head. WQi ∈\nRdmodel× dk , WKi ∈ Rdmodel× dk , WVi ∈ Rdmodel× dv are three diﬀerent\nlinear matrices. Similar to the Sparse Connectivity of convolution,\nthe Multi-head attention uses a dmodel/ h-dimensional vector to\nseparate the input into h separate attention heads and processes the\nfeatures of each head in parallel. With no additional computational\ncost, the Multi-head attention mechanism enriches the diversity of\nfeature subspaces.\nThe following is that the MLP is applied to integrate the\ninformation and the Skip Connection structure is applied to\nenhance the stability of the training. The integrated information is\nreassembled into a new feature pattern.\nThe ﬁnal part of the SR-Net model uses Global Average Pooling\nto extract the individual channel information, and uses Softmax\nLogistic Regression to output the category information. Eventually,\ndiﬀerent features are extracted by the SR-Net model.\nIn this experiment, the Adam optimizer is chosen to minimize\nthe cross entropy loss function. The Adam optimizer which\ncan automatically adapt diﬀerent learning rates for diﬀerent\nparameters, is better than SGD optimizer that uses the same\nlearning rate for each parameter update. The parameters of the\nmodel are set to A-0.9 and the learning rate is 1e-4. And the model\nis trained for a total of 100 Epochs.\n/two.tnum./three.tnum. Model evaluation and visualization\nWhen the number of positive and negative samples in the\ndataset is balanced, the confusion matrix, which relates the true\nlabels to the ones detected by each model (\nDe et al., 2022 ), is a\nreliable method to count the classiﬁcation results of the model. By\njointly analyzing the amount of correct and mismatched true and\ndetected labels, this method provides a direct assessment of the\nmodel’s ability to predict both positive and negative cases (\nTable 4).\nIn this study, the positive case denotes the forest ﬁre smoke, and the\nnegative case denotes the cloud.\nWhen facing large amounts of data or multiple confusion\nmatrices, it is diﬃcult to accurately assess the detection capability\nof a model with a single confusion matrix. This requires the\nintroduction of secondary indices based on the confusion matrix,\nincluding Accuracy (proportion of samples with correct detections\nout of all samples.), Precision (proportion of samples identiﬁed\nFrontiers in Forests and Global Change /zero.tnum/six.tnum frontiersin.org\nZheng et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/ﬀgc./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/three.tnum/six.tnum/nine.tnum/six.tnum/nine.tnum\nFIGURE /three.tnum\nThe network structure of the proposed SR-Net model. In this ﬁgure, “Conv” means convolution, “s” means stride, “Lightweight vit” mea ns Lightweight\nVision Transformer, and “Inductive Conv” means convolution layer s used to perform inductive bias.\nTABLE /four.tnum The basic confusion matrix of forest ﬁre smoke detection.\nTrue label Detection label\nForest ﬁre smoke Cloud\nForest ﬁre smoke Correctly identiﬁed True label is forest ﬁre smoke\nForest ﬁre smoke samples Detected label is cloud\nCloud True label is cloud Correctly identiﬁed\nDetected label is forest ﬁre smoke Cloud samples\nby the model as one class that are actually in that class), Recall\n(proportion of samples correctly detected by model as one class\nto the total number of that class), and even the tertiary index F1-\nScore (relation between Recall and Precision values) (\nSalih and\nAbdulazeez, 2021). The above secondary and tertiary indices allow\nfor a standardized evaluation parallel comparison of models by\ntransforming the quantitative results in the confusion matrix into\nratio results between 0 and 1. The indices introduced are calculated\nbased on Equations (1–4). Among these indices, to improve\nPrecision, models tend to make predictions only when they are\ncertain enough, which can result in unsure samples being missed\ndue to over-conservatism, resulting in a lower Recall. Therefore, to\nachieve the best balance between Precision and Recall, the detection\nability of the model is better when the result of the F1-score, which\nis calculated from Precision and Recall, is close to 1.\nAccuracy = T1 + T2\nT1 + F1 + F2 + T2\n. (1)\nPrecision =\n(\nT1\nT1+ F2 + T2\nT2+ F1\n)\n2 . (2)\nFrontiers in Forests and Global Change /zero.tnum/seven.tnum frontiersin.org\nZheng et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/ﬀgc./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/three.tnum/six.tnum/nine.tnum/six.tnum/nine.tnum\nRecall =\n(\nT1\nT1+ F1 + T2\nT2+ F2\n)\n2 . (3)\nF1 Score = 2 × Precision × Recall\nPrecision + Recall . (4)\nIn Equations (1–4), T1 represents the number of correctly identiﬁed\nforest ﬁre smoke samples by the model, T2 represents the number\nof correctly identiﬁed cloud samples by the model, F1 represents\nthe number of forest ﬁre smoke samples being detected as cloud,\nand F2 represents the number of cloud samples being detected as\nforest ﬁre smoke. The indices are all macro-average, which directly\nadds and then averages each index of positive and negative cases,\ngiving the same weight to index of each case.\nThe generalization ability of the model is a key index for\nevaluating its application range (\nCaroline and Mariana, 2022 ),\nwhich can be assessed by plotting the Receiver Operating\nCharacteristic (ROC) curve, calculating the Area Under the ROC\nCurve (AUC), and introducing the Kappa coeﬃcient. The ROC\ncurve provides a visual indication of the model’s detecting ability\nby incorporating both Precision and Recall and is independent\nof the decision threshold (\nObuchowski and Bullen, 2018 ). The\ncloser the curve is to the upper left (0, 1) coordinate, the better\ndetecting ability the model has. The AUC is a comprehensive\nmeasure of the eﬀectiveness of all possible classiﬁcation thresholds.\nThe closer the AUC is to 1, the more realistic the detection model\nis, and the higher value it has for application. The Kappa coeﬃcient\nassesses the consistency of detection results with the actual situation\nthrough attempting to renormalize a debiased estimate of Accuracy\n(\nPowers, 2020 ). When the Kappa coeﬃcient is in the range of\n0.61–0.80 ( Dettori and Norvell, 2020 ), it means that the detection\nlabel is substantially consistent with the true label, and the model\ndetects well. When it is in the range of 0.81–1, it means that the\ndetection label is almost identical to the true label, and the model\ndetects perfectly. The kappa coeﬃcient is calculated according to\nEquation (5).\nKappa coeﬃcient = K0− Ke\n1− Ke , (5)\nwhere K0 = Accuracy,\nKe = (T1+ F1)× (T1+ F2)+ (F1+ T2)× (T2+ F2)\n(T1+ T2+ F1+ F2)2 .\nIn Equations (5), T1 represents the number of correctly identiﬁed\nforest ﬁre smoke samples by the model, T2 represents the number\nof correctly identiﬁed cloud samples by the model, F1 represents\nthe number of forest ﬁre smoke samples being detected as cloud,\nand F2 represents the number of cloud samples being detected as\nforest ﬁre smoke.\nAs for the visualization, CNN, known as black box operations,\noften has outputs that are diﬃcult to interpret (\nWu et al., 2018 ).\nHowever, the Gradient-weighted Class Activation Mapping (Grad-\nCAM) makes the CNN transparent through visual interpretation\nwithout modifying or retraining the model structure. The Grad-\nCAM can visualize the attention distribution on which the model\ndetection is based. Hence, when the attention distribution appears\nto be inconsistent with the position of the detection object, such as\nthe forest ﬁre smoke, in original images or the model does not ﬁt\nwell, Grad-CAM can target the reason for model failure. This kind\nof visual comparative assessment can examine the forest ﬁre smoke\ndetection model for model bias, increase the persuasion of model\neﬀects, and enhance conﬁdence from users in model detection\nresults (\nSelvaraju et al., 2020 ).\nIn this study, we evaluate the detection of forest ﬁre smoke\nby AlexNet, MobileNet, GoogLeNet, and ResNet50 models,\nwhich have a wide range of applications in the ﬁeld of image\nclassiﬁcation and recognition. Among them, AlexNet deepens\nthe net and replaces the activation function (\nKrizhevsky et al.,\n2017). MobileNet uses linear bottlenecks and inverted residuals to\nreduce the number of parameters and computation ( Brijraj et al.,\n2019). GoogLeNet uses inception block to combine the outputs of\nconvolutional kernels of diﬀerent sizes for channel merging, which\nreduces the model complexity (\nChen et al., 2022 ). And ResNet50\nuses residual block with residual connections and introduces the\nBatch Normalization, so that deeper networks will have better\nperformance (\nMahdianpari et al., 2018 ). By comparing the forest\nﬁre smoke detection eﬀects of the above four models with the\nproposed SR-Net model, this study analyzes the potential of the\nSR-Net model for forest ﬁre smoke detection.\n/three.tnum. Results\nThis study proposes a forest ﬁre smoke detection model (SR-\nNet) combining CNN and Lightweight ViT using small-scale\nremote sensing dataset. We compare and analyze the eﬀectiveness\nof the SR-Net model with AlexNet, MobileNet, GoogleNet, and\nResNet50 models for the detection of forest ﬁre smoke by\nemploying confusion matrices and visual heat images.\n/three.tnum./one.tnum. Evaluation of model detecting results\nStatistically, the number of input parameters required for\nthe AlexNet, MobileNet, GoogleNet, ResNet50, and SR-Net\nmodels are 38 million, 37 million, 6 million, 23 million, and 6\nmillion, respectively.\nThe confusion matrix of the SR-Net model applied to forest ﬁre\nsmoke detection is shown in\nTable 5. In the Validation Set, the SR-\nNet model correctly identiﬁed 394 cases of forest ﬁre smoke with a\ntotal of 400 and 398 cases of clouds with a total of 400, as well as\n378 cases of forest ﬁre smoke with a total of 400 and 397 cases of\ncloud with total 400 in the Test Set. The above results indicate that\nthe SR-Net model has a higher probability than 95% of accurately\ndetecting positive and negative cases.\nFigure 4 compares the secondary indices of the SR-Net model\nwith ResNet50, MobileNet, GoogLeNet, and AlexNet models to\nfurther analyze the forest ﬁre smoke detection capability of the SR-\nNet model. According to the detection results, the SR-Net model\nhas the highest Accuracy, Precision, Recall, F1-Score, and Kappa\ncoeﬃcient for the detection of forest ﬁre smoke. This indicates\nthat the SR-Net model outperforms the ResNet50, MobileNet,\nGoogLeNet, and AlexNet models in detecting forest ﬁre smoke.\nOn the other hand, the construction purpose of the forest ﬁre\nsmoke detection model is to monitor early forest ﬁres. So among\nthe evaluation indices, the Recall is of great practical signiﬁcance,\nbecause a higher Recall represents less under-reporting of early\nFrontiers in Forests and Global Change /zero.tnum/eight.tnum frontiersin.org\nZheng et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/ﬀgc./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/three.tnum/six.tnum/nine.tnum/six.tnum/nine.tnum\nTABLE /five.tnum Confusion matrix of the SR-Net model in the test and validation sets.\nValidation set Test set\nDetection label:\nforest ﬁre smoke\nDetection label:\ncloud\nDetection label:\nforest ﬁre smoke\nDetection label:\ncloud\nTrue label: forest ﬁre smoke 394 2 378 3\nTrue label: cloud 6 398 22 397\nFIGURE /four.tnum\nComparison of AlexNet, ResNet/five.tnum/zero.tnum, MobileNet, GoogLeNet and\nSR-Net models for each evaluation indices based on validation set\n(A) and test set (B).\nforest ﬁres in practice through forest ﬁre smoke detection by remote\nsensing.\nFigure 4 shows that the proposed model SR-Net is superior\nto the other four models in terms of Recall because according to the\nresults of the Test Set, the Recall of AlexNet model is 82%, ResNet50\nmodel is 88%, MobileNet and GoogLeNet models are 92.5%, while\nthat of proposed SR-Net model reached 97%. This means that the\nSR-Net model has a lower probability of missing the detection of\nforest ﬁre smoke than the other four models. Furthermore, because\nof the interaction between Recall and Precision, the F1-Score has\nemerged to measure the balance condition of the two indices.\nThe Recall and Precision need to be optimally balanced to avoid\nmissing or miss-detecting of forest ﬁre smoke to reduce missed\nand false forecasts in early forest ﬁre monitoring in practical use.\nFIGURE /five.tnum\nROC curves and AUC for AlexNet, ResNet/five.tnum/zero.tnum, MobileNet,\nGoogLeNet, and SR-Net models on the validation set (A) and test set\n(B).\nThe F1-Score of the SR-Net model is closer to 1 ( >0.95), which\nindicates a good balance between Recall and Precision. What’s\nmore, compared to other four models, the SR-Net model has the\nlargest Kappa coeﬃcient, which implies that the model’s detections\nfor forest ﬁre smoke are highly consistent with the actual situations.\nFigure 5 shows the ROC curves of AlexNet, ResNet50,\nMobileNet, GoogLeNet, and SR-Net models and their AUC results\nFrontiers in Forests and Global Change /zero.tnum/nine.tnum frontiersin.org\nZheng et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/ﬀgc./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/three.tnum/six.tnum/nine.tnum/six.tnum/nine.tnum\nfor the Validation and Test sets. As shown in Figure 5, the SR-\nNet model corresponds to the ROC curve closest to the (0, 1)\ncoordinate, which reveals that the SR-Net model has the best\ncapability for forest ﬁre smoke detection and performs consistently\nacross diﬀerent datasets. In addition, by comparing the AUC, it can\nbe concluded that the SR-Net model has the best generalization\nthan the other four models, and can be applied to other small-scale\nforest ﬁre smoke datasets.\n/three.tnum./two.tnum. Visualization of model detecting eﬀects\nFigure 6 shows the original remote sensing images of forest\nﬁre smoke and heat images processed by Grad-CAM based on the\nSR-Net model. The white areas in both the original and the heat\nimages represent forest ﬁre smoke (marked with red arrows in\nthe original images), and the attention degree from high to low is\ncolored blue, yellow, and red in the heat images.\nFigure 6 compares\nand analyzes the smoke detection results of the SR-Net model with\ndiﬀerent proportions of smoke in an image, where (A) represents\nthe proportion of smoke area >30%, (B) represents the proportion\nof smoke area <20%, and (C) represents situations when there is a\nsmall portion of clouds (marked with red rectangles) in the original\nsmoke images. The results show that the percentage of smoke area\nin an image has little eﬀect on the forest ﬁre smoke detection of the\nSR-Net model, because of the global attention added to the SR-Net\nmodel. However, when the forest ﬁre smoke is partially obscured\nby clouds in the original images, the attention distribution scope of\nthe SR-Net model to detect forest ﬁre smoke is reduced (\nFigure 6C).\nThis is due to the narrowing of the distinction between forest ﬁre\nsmoke and background clouds when the smoke is obscured by\npoint-like clouds, which increases the diﬃculty of model detection.\nOverall, the attention of the SR-Net model can largely avoid areas\nwhere cloud points are present. This indicates that the model has\nthe ability of resistance to interference and can identify forest ﬁre\nsmoke under complex meteorological environmental conditions.\nThe original remote sensing images of forest ﬁre smoke are\nshown in\nFigure 7 (A-1, B-1, C-1), along with the visualized heat\nimages based on the AlexNet, ResNet50, MobileNet, GoogLeNet,\nand SR-Net models processed by Grad-CAM. The white areas\nmarked with red arrows in the original images are forest ﬁre smoke,\nand the distribution of attention when the model detects forest ﬁre\nsmoke is marked with blue (high), yellow (medium), and red (low)\nin descending order of weight.\nCompared to other models, ﬁrst, the SR-Net is more stable\nthan the GoogLeNet model and is more likely to focus on the\ntarget object—the forest ﬁre smoke. The attention distribution for\ndetecting forest ﬁre smoke obtained by the SR-Net model is square\nin shape. It largely matches the contour of the background and\nforest ﬁre smoke, which occupy a larger area in images (A-6, B-\n6). The yellow area outside the square is the part that the model\ndoes not focus on. In addition, when the remote sensing image\nhas a strong background texture, the attention of the SR-Net model\nonly wraps around the target—forest ﬁre smoke to analyze features\nof it (C-6). The attention distribution of the GoogLeNet model is\nfocused on the forest ﬁre smoke presenting an ellipse to include\nthe target (A-5, B-5). However, there are still cases of bias in its\nattention distribution as can be seen in its heat images (C-5), which\nmay cause errors when detecting forest ﬁre smoke. Second, the\nperformance of the ResNet50 and MobileNet models is erratic.\nSpeciﬁcally, the shape of the attention distribution of the ResNet50\nmodel is generally consistent with the outline of the target object—\nforest ﬁre smoke (A-3). But its attention distribution is fragmented\ninto patches and some of them are scattered to other parts of the\nimage (B-3, C-3), which may lead to inaccurate results of forest\nﬁre smoke detection. The attention distribution of the MobileNet\nmodel shows a blocky distribution, which can almost cover the\ntarget object—forest ﬁre smoke in most cases (A-4). But it can\nsometimes be shifted and completely cannot overlap with the target\nobject (B-4, C-4). Third, the distribution of attention based on the\nAlexNet model is mostly blurred, with the focal (blue) areas being\nlightly and irregularly colored (A-2, B-2). And the focal areas only\ncover an extremely small proportion of the target object—forest ﬁre\nsmoke (C-2). In conclusion, the SR-Net model has a stable attention\ndistribution state, outstanding detecting performance on diﬀerent\ndatasets, and good generalization performance.\nA comparative analysis of the results in\nFigure 7 shows that\nthe SR-Net model outperforms AlexNet, ResNet50, MobileNet, and\nGoogLeNet in terms of both the ﬁtness and stability of the attention\ndistribution state, and has a better adaptability and generalization\nfor forest ﬁre smoke detection.\nFigure 8 presents a very small number of anomalies in the\nvisualization of forest ﬁre smoke images based on the SR-Net\nmodel. The ﬁrst row is original forest ﬁre smoke images and the\nsecond row is the heat images processed by Grad-CAM based on\nthe SR-Net model. The white areas in images are the forest ﬁre\nsmoke sample (marked with red arrows) and the attention weight\nof the SR-Net model is marked from high, medium, to low with the\ncolor from blue, yellow, to red.\nAs shown in\nFigure 8, in a few cases, the attention distribution\nof the SR-Net model does not show a blue square but chooses to\nignore the forest ﬁre smoke roots. This may be because the SR-\nNet model considers the overall features in the Middle and rear of\nthe smoke and the surrounding background to be more important\nthan the individual target features in the thickest part of the forest\nﬁre smoke. Therefore, the SR-Net model does not choose to use the\nforest ﬁre smoke as the only basis for detection and identiﬁcation.\n/four.tnum. Discussions\nAt present, forest ﬁre monitoring by meteorological satellites\nare mainly through infrared technology to detect high-temperature\npoints of forest ﬁres. The limitation is that in the early stage of forest\nﬁres, combustible materials are not fully burnt, so their temperature\nis not high enough to be detected. This is why the infrared band of\nmeteorological satellites cannot receive enough energy of infrared\nradiation for imaging, which makes it diﬃcult to detect in time.\nTherefore, there is a risk of delayed detection of forest ﬁres. In\nthe meanwhile, however, incomplete combustion produces a large\namount of smoke. The method of forest ﬁre smoke detection by\nremote sensing satellites can forecast forest ﬁres much earlier than\nthe method using infrared technology. However, there is scant\nstudies on detecting forest ﬁre smoke to forecast early forest ﬁres.\nAnd existing researches of forest ﬁre smoke models are mostly\nFrontiers in Forests and Global Change /one.tnum/zero.tnum frontiersin.org\nZheng et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/ﬀgc./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/three.tnum/six.tnum/nine.tnum/six.tnum/nine.tnum\nFIGURE /six.tnum\nComparison between the original remote sensing images of forest ﬁre smoke and heat images processed by Grad-CAM based on the SR-Net mod el.\n(A) The proportion of smoke area >/three.tnum/zero.tnum%,(B) the proportion of smoke area </two.tnum/zero.tnum%,(C) situations when there is a small portion of clouds (marked with\nred rectangles) in the original smoke images. The attention distri bution when the model detects forest ﬁre smoke is marked with blue , yellow, and\nred in descending order of weight from high to low.\nFrontiers in Forests and Global Change /one.tnum/one.tnum frontiersin.org\nZheng et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/ﬀgc./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/three.tnum/six.tnum/nine.tnum/six.tnum/nine.tnum\nFIGURE /seven.tnum\nOriginal forest ﬁre smoke images (A-/one.tnum, B-/one.tnum, C-/one.tnum), comparison of heat images processed by Grad-CAM based among AlexNet (A-/two.tnum, B-/two.tnum, C-/two.tnum),\nResNet/five.tnum/zero.tnum (A-/three.tnum, B-/three.tnum, C-/three.tnum), MobileNet (A-/four.tnum, B-/four.tnum, C-/four.tnum), GoogLeNet (A-/five.tnum, B-/five.tnum, C-/five.tnum), and SR-Net (A-/six.tnum, B-/six.tnum, C-/six.tnum) models.(A-C) Represents three diﬀerent\nsets of forest ﬁre smoke images and heat images based on each model. T he attention distribution when the model detects forest ﬁre smoke is\nmarked with blue, yellow, and red in descending order of weight fr om high to low.\nFrontiers in Forests and Global Change /one.tnum/two.tnum frontiersin.org\nZheng et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/ﬀgc./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/three.tnum/six.tnum/nine.tnum/six.tnum/nine.tnum\nFIGURE /eight.tnum\nExamples of some unusual conditions of visualizations based on th e SR-Net. The (Top) row shows the original forest ﬁre smoke images, and the\n(Bottom) row shows heat images processed by Grad-CAM based on the SR-Net. Th e attention distribution when the model detects forest ﬁre smoke\nis marked with blue, yellow, and red in descending order of weigh t from high to low.\ndevoted to developing CNN, for example, increasing the scale\nof the dataset (\nZhang et al., 2018 ), adding diﬀerent mechanisms\n(Xie et al., 2018 ), and improving the structure of CNN ( Khan\net al., 2021 ), to improve the forest ﬁre smoke detection accuracy\nof models. The common problems with the above development\nmethods are as follows: ﬁrst, it is diﬃcult to achieve an eﬀective\nbalance between data scale and detection accuracy. A small number\nof parameters will aﬀect the detection accuracy, while a large\nnumber of parameters will require suﬃcient data to solve the\noverﬁtting problem (\nKrizhevsky et al., 2017 ). Second, it is hard\nto collect a large amount of forest ﬁre smoke data from remote\nsensing satellites (\nZhang et al., 2018 ). However, CNNs may not\nperform well when there are not enough datasets available for its\npre-training (\nSathishkumar et al., 2023 ). And third, the increase in\ndata scale will cause an increase in computational resource cost. To\naddress the above issues, we introduce the ViT and propose a forest\nﬁre smoke detection model (SR-Net) for small-scale remote sensing\nforest ﬁre smoke datasets. It has improved detection accuracy and\nreduced resource consumption compared with traditional CNN.\nAlthough ViT has been less studied in the ﬁeld of forest\nﬁre monitoring, existing research is attempting to compare the\nclassiﬁcation accuracies of the latest CNN and ViT models on the\nImageNet dataset, aimed at the image classiﬁcation task (\nXu et al.,\n2022). Part of the results is in Table 6 (Xu et al., 2022 ).\nTable 6 indicates that ViT models have the potential to achieve\ncomparable performance or even outperform state-of-the-art CNN\narchitectures (\nXu et al., 2022 ). And an increasing number of\nresearches on ViT or progressively merging CNN and ViT have\ncome out in various ﬁelds (\nXu et al., 2022 ). In the ﬁeld of Remote\nSensing, the network combined CNN and ViT is proposed to\ndo Hyperspectral image (HSI) classiﬁcation tasks (\nLi et al., 2022 )\nTABLE /six.tnum Flogs, parameters, and accuracy of each model on the ImageNet\ndataset (Xu et al., /two.tnum/zero.tnum/two.tnum/two.tnum).\nModel Flogs (G) Parameters (M) Accuracy (%)\nConvolution-based neural network\nResNet 4.1 25.6 76.2\nRegNetY-16G 16.0 84 82.9\nEﬃcientNet-\nB7\n37.0 66 84.3\nVisual transformer\nViT 55.4 86 77.9\n190.7 307 76.5\nConViT 5.4 27 81.3\n17.0 86 82.4\nSwin\ntransformer\n4.5 29 81.3\n47.0 88 84.2\nand to solve cross-resolution issues conducted on IKONOS and\nWorldView 2 with 4- and 8-band multispectral (MS) images (\nWang\nN. et al., 2022 ). In the ﬁeld of Scene Classiﬁcation, ViT is used to\ndistinguish scenes and obtains an average classiﬁcation accuracy\nof 98.49% on Merced datasets (\nBazi et al., 2021 ). In the ﬁeld of\nMedicine, CNN and ViT are combined to diagnose Novel Corona\nVirus Pneumonia (COVID-19) and its result is obviously better\nthan that of the typical CNN network (ResNet-152) (95.2%) and\nTransformer network (Deit-B) (75.8%) (\nFan et al., 2022 ). And\nthe combination is also applied to diagnose Acute lymphocytic\nFrontiers in Forests and Global Change /one.tnum/three.tnum frontiersin.org\nZheng et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/ﬀgc./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/three.tnum/six.tnum/nine.tnum/six.tnum/nine.tnum\nleukemia, and the accuracy reached 99.03% ( Jiang et al., 2021 ). The\nabove researches provide theoretical support for the application\nprospects of models combining CNN and ViT.\nWhat’s more, while previous researches of forest ﬁre smoke\ndetection has focused on using high spatial-resolution satellites,\nthis paper chooses to use high time-resolution satellite. The SR-\nNet model, beneﬁtting from the high temporal resolution feature\nof the Himawari-8 satellite, can be applied to detect forest ﬁre\nsmoke promptly. Considering the diﬃculty of collecting remote\nsensing sample data of forest ﬁre smoke, the front part of SR-Net\nuses CNN to make inductive bias of forest ﬁre smoke samples,\ncomplementing the missing priori knowledge of ViT due to being\nbased on a small-scale dataset. The back part uses lightweight\nViT, which adds a global attention mechanism compared to CNN,\nallowing the model to achieve better performance on small-scale\nremote sensing datasets of forest ﬁre smoke. SR-Net simpliﬁes\nthe structure and reduces the number of parameters to reduce\ncomputational costs and resource consumption while maintaining\nthe detection accuracy of forest ﬁre smoke. Other benchmarks,\nlike GoogLenet et al., are still essentially convolutional models.\nThe advantage of convolutions lies in its inductive bias, while the\ndisadvantage lies in its inability to eﬀectively obtain and construct\nthe global information, as the size of the convolutional kernel is\nﬁnite. However, our proposed model introduces the ViT, which\nhas the multi-headed attention mechanism, compensating for the\ndisadvantages of all kinds of convolutional models.\nBy comparing the results of diﬀerent indices, it is found that the\nSR-Net model has the highest accuracy, precision, recall, F1-Score,\nand Kappa coeﬃcient of forest ﬁre smoke detection, outperforming\nAlexNet, MobileNet, ResNet50, and GoogLeNet models. In forest\nﬁre smoke detection, the SR-Net model has a higher accuracy and\nlower false rate than other models as well as a better capacity of\nbalance between the two, allowing the model to make the best\nmeasurements and avoid being too ’conservative’ or ’conﬁdent’ in\nits judgments. This balance allows the model to be used more\nreliably in real-time scenarios of forest ﬁre smoke detection and\nhelps the human and material resources needed to conﬁrm forest\nﬁres to be deployed more eﬃciently and eﬀectively, without time\nand resource consuming.\nIn addition, heat images of SR-Net attention distribution\ndrawn by Grad-CAM show that SR-Net presents a wider attention\ndistribution in images because of the global attention mechanism.\nThis mechanism allows a more comprehensive exploration of forest\nﬁre smoke features and is less aﬀected by the proportion of forest\nﬁre smoke in remote sensing images. What’s more, when there is\ninterference from point-like clouds in forest ﬁre smoke images, the\ndiﬃculty of classiﬁcation and detection increases for the reason\nthat the distinction between forest ﬁre smoke and the cloud is\nnarrowed. In this case, the attention scope of the SR-Net model is\nreduced. But the reduced scope manifests that the SR-Net model\nhas better resistance to interference for it can avoid the areas\nwhere cloud points are present when detecting forest ﬁre smoke. By\ncomparing the heat images of AlexNet, ResNet50, MobileNet, and\nGoogLeNet models, this study ﬁnd that the SR-Net model is more\nstable and ﬁt for forest ﬁre smoke detection than other models.\nIt has a more ﬁxed pattern of attention distribution for forest ﬁre\nsmoke detection, showing a square shape, which can include the\ntarget object and its background. With this pattern, the detection\nby the SR-Net model does not tend to miss the target object—forest\nﬁre smoke. When the background has strong textural features,\nthe detection capability of the model is disturbed and there is a\nreduction in the scope of attention distribution. The GoogLeNet\nmodel, in general, performs well with regard to the attention\ndistribution state but is less stable than the SR-Net model. The\nattention distribution of it is aﬀected by the presence of background\ninterference, resulting in a shift. The attention distribution of\nResNet50 and MobileNet models is unstable and can cover most\nof the target object—forest ﬁre smoke in most cases, however,\nthere are also cases where the attention distribution is scattered\nor only covers a small portion of the target object. The attention\ndistribution of AlexNet is blurred, which cannot cover forest ﬁre\nsmoke, resulting in the poor eﬀect of forest ﬁre smoke detection.\nTo sum up, the SR-Net model is more eﬀective in detecting forest\nﬁre smoke under complex environmental conditions with better\naccuracy and greater generalization.\nThe major limitation of our study is that the model needs to\nbe put to further practical use to explore what contingencies exist\nin real-time applications and to reﬁne the model by developing\nemergency pre-solutions. Notwithstanding the limitation, the SR-\nNet model is more eﬀective and stable than traditional CNNs in\ndetecting forest ﬁre smoke with high timeliness. Therefore, it has\nthe potential to be used in practical applications to help monitor\nforest ﬁre smoke or as a complement to monitoring forest ﬁre\nsmoke through near-infrared bands.\n/five.tnum. Conclusions\nIn this paper, we propose a lightweight forest ﬁre smoke\ndetection model (SR-Net) combining the merits of CNN and\nViT models and construct a new small-scale remote sensing\ndataset, containing cloud and forest ﬁre smoke, collected from\nthe Himawari-8 satellite. We conduct a comprehensive evaluation\nof the of SR-Net and benchmark models, including AlexNet,\nMobileNet, GoogLeNet, and ResNet50. We conclude our ﬁndings\nas follows:\n(1) The combination of CNN and ViT allows for a lightweight forest\nﬁre smoke detection model (SR-Net), and reduces the number of\nthe model parameters to six million. The model can be applied to\nsmall-scale remote sensing datasets of forest ﬁre smoke images.\n(2) The results of the confusion matrix manifest that the SR-Net\nmodel is more than 95% likely to accurately detect positive\nand negative cases of forest ﬁre smoke samples. On both the\nvalidation and test sets, the SR-Net model is superior to AlexNet,\nMobileNet, GoogLeNet, and ResNet50 models in comparison\ncriteria including: Accuracy, Precision, Recall, F1-Score, and\nKappa Coeﬃcient.\n(3) Visualization of the model attention in detecting forest ﬁre\nsmoke by Grad-CAM revealed that the SR-Net model has a wide\nrange of attentions, which could comprehensively explore the\nfeatures of the remote sensing images, leading to an accurate\ndetection of the forest ﬁre smoke with less interference from\nenvironmental factors. The comparison of the heat images\nfurther conﬁrms the outperformances of the SR-Net model over\nFrontiers in Forests and Global Change /one.tnum/four.tnum frontiersin.org\nZheng et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/ﬀgc./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/three.tnum/six.tnum/nine.tnum/six.tnum/nine.tnum\nbenchmark models in adaptability and stability for forest ﬁre\nsmoke detections.\nThis paper sheds a light on the lightweight models of forest\nﬁre detection with small-scale datasets. The documented model\nperformance calls for further application of the proposed model\non broader sets of imagery data from multiple satellites to\ntest the model generality. As it is diﬃcult for existing remote\nsensing satellites to achieve the coexistence of high temporal and\nhigh spatial resolution, future research may focus on processing\nspatial resolution information collected from high temporal\nresolution remote sensing images. Moreover, recent developments\nin Computer Vision (CV) could further improve forest ﬁre smoke\ndetections by exploring the migration and scalability of the\nnew networks.\nData availability statement\nThe original contributions presented in the study are included\nin the article/supplementary material, further inquiries can be\ndirected to the corresponding author.\nAuthor contributions\nGZ conceived and designed the study. YZ wrote the ﬁrst draft,\ncollected the study data, and performed the experiment. GZ, ZY,\nand ST provided critical insights in editing the manuscript. GZ\nand ST are responsible for the project management. HX and DW\nassisted in the supervision. All authors have read and agreed to the\npublished version of the manuscript.\nFunding\nThis work was funded by the National Natural Science\nFoundation Project of China (Grant No. 32271879) and the Science\nand Technology Innovation Platform and Talent Plan Project of\nHunan Province (Grant No. 2017TP1022).\nAcknowledgments\nMany thanks to the editor and reviewers for their\nvaluable comments.\nConﬂict of interest\nThe authors declare that the research was conducted\nin the absence of any commercial or ﬁnancial relationships\nthat could be construed as a potential conﬂict\nof interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\nAllison, R. S., Johnston, J. M., Craig, G., and Jennings, S. (201 6). airborne optical\nand thermal remote sensing for wildﬁre detection and monitor ing. Sensors 16, 1310.\ndoi: 10.3390/s16081310\nBa, R., Chen, C., Yuan, J., Song, W., and Lo, S. (2019). SmokeN et: satellite smoke\nscene detection using convolutional neural network with spat ial and channel-wise\nattention. Remote Sens. 11, 1702. doi: 10.3390/rs11141702\nBazi, Y., Bashmal, L., Rahhal, M. M. A., Dayil, R. A., and Ajlan, N. A. (2021).\nVision transformers for remote sensing image classiﬁcation . Remote Sens . 13, 516.\ndoi: 10.3390/rs13030516\nBrijraj, S., Durga, T., and Sharan, K. A. (2019). Shunt conne ction: an intelligent\nskipping of contiguous blocks for optimizing MobileNet-V2, Neural. Netw . 118,\n192–203. doi: 10.1016/j.neunet.2019.06.006\nCan, H. K., M., Cagri, K., Turker, T., Sengul, D., and, U., et al. (2021). Automated\nclassiﬁcation of remote sensing images using multileveled Mobi leNetV2 and DWT\ntechniques. Expert Syst. Appl . 185, 115659. doi: 10.1016/j.eswa.2021.115659\nCaroline, M. G., and Mariana, B. (2022). Assessing the generali zation capability of\ndeep learning networks for aerial image classiﬁcation using la ndscape metrics. Int. J.\nAppl. Earth Obs . 114, 103054. doi: 10.1016/j.jag.2022.103054\nChen, H., Fu, X., and Dong, J. (2022). SAR target recognition based on inception\nand fully convolutional neural network combining amplitude doma in multiplicative\nﬁltering method. Remote Sens. 14, 5718. doi: 10.3390/rs14225718\nChrysoulakis, N., Herlin, I., Prastacos, P., Yahia, H., Grazzi ni, J., and Cartalis, C.\n(2007). An improved algorithm for the detection of plumes caused by natural or\ntechnological hazards using AVHRR imagery. Remote Sens. Environ. 108, 393–406.\ndoi: 10.1016/j.rse.2006.11.024\nDe, D. I. M., Redondo, A. R., Fernández, R. R., Jorge, N., and J avier, M. M. (2022).\nGeneral performance score for classiﬁcation problems. Appl. Intell. 52, 12049–12063.\ndoi: 10.1007/s10489-021-03041-7\nDettori, J. R., and Norvell, D. C. (2020). Kappa and beyond: is the re agreement?\nGlob. Spine J. 10, 499–501. doi: 10.1177/2192568220911648\nFan, X. L., Feng, X. F., Dong, Y. Y., and Hou, H. C. (2022). COVI D-19 CT\nimage recognition algorithm based on transformer and CNN. Displays 72, 102150.\ndoi: 10.1016/j.displa.2022.102150\nFilonenko, A., Hernandez, D. C., and Jo, K. H. (2018). Fast smo ke detection\nfor video surveillance using CUDA. IEEE Trans. Industr. Notify 14, 725–733.\ndoi: 10.1109/TII.2017.2757457\nGovil, K., Welch, M. L., Ball, J. T., and Pennypacker, C. R. (2022). P reliminary results\nfrom a wildﬁre detection system using deep learning on remote c amera images. Remote\nSens. 12, 166. doi: 10.3390/rs12010166\nHan, K., Wang, Y., Chen, H., Chen, X., Guo, J., Liu, Z., et al. (20 22).\nA survey on vision transformer. IEEE T. Pattern Anal . 45, 87–110.\ndoi: 10.1109/TPAMI.2022.3152247\nHoward, J., Murashov, V., and Branche, C. M. (2018). Unmanne d aerial vehicles in\nconstruction and worker safety. Am. J. Ind. Med . 61, 3–10. doi: 10.1002/ajim.22782\nJang, E., Kang, Y., Im, J., Lee, D.-W., Yoon, J., and Kim, S.-K . (2019). Detection and\nmonitoring of forest ﬁres using himawari-8 geostationary s atellite data in South Korea.\nRemote Sens. 11, 271. doi: 10.3390/rs11030271\nJia, Y., Yuan, J., Wang, J. J., Fang, J., Zhang, Q. X., and Zhan g, Y. M. (2016). A\nsaliency-based method for early smoke detection in video seque nces. Fire Technol. 52,\n1271–1292. doi: 10.1007/s10694-014-0453-y\nFrontiers in Forests and Global Change /one.tnum/five.tnum frontiersin.org\nZheng et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/ﬀgc./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/three.tnum/six.tnum/nine.tnum/six.tnum/nine.tnum\nJiang, Z. C., Dong, Z. X., Wang, L. Y., and Jiang, W. P. (2021). Method for diagnosis\nof acute lymphoblastic leukemia based on ViT-CNN ensemble model. Comput. Intel.\nNeurosc. 2021. doi: 10.1155/2021/7529893\nKattenborn, T., Leitloﬀ, J., Schiefer, F., and Hinzb, S. (202 1). Review on\nConvolutional Neural Networks (CNN) in vegetation remote se nsing. ISPRS J.\nPhotogramm. 173, 24–49. doi: 10.1016/j.isprsjprs.2020.12.010\nKhan, S., Muhammad, K., Hussain, T., Ser, J. D., and Albuquerque , V. H. C. D.\n(2021). DeepSmoke: deep learning model for smoke detection an d segmentation in\ndoor environments. Expert Syst. Appl. 182, 115125. doi: 10.1016/j.eswa.2021.115125\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. (2017). Imag enet classiﬁcation with\ndeep convolutional neural networks. Commun. ACM. 60, 84–90. doi: 10.1145/3065386\nLi, C. M., Wang, X. Y., Chen, Z. H., Gao, H. M., and Xu, S. F. (202 2). Classiﬁcation of\nhyperspectral image based on dual-branch feature interaction network. Int. J. Remote\nSens. 43, 3258–3279. doi: 10.1080/01431161.2022.2089069\nLi, J., Wang, X., Tu, Z. P., and Michael, R. L. (2021). On the div ersity of multi-head\nattention. Neurocomputing 454, 14–24. doi: 10.1016/j.neucom.2021.04.038\nLi, W., Chen, C., Zhang, M., Li, H. C., and Du, Q. (2019). Data a ugmentation for\nhyperspectral image classiﬁcation with deep CNN. IEEE Geosci. Remote Sens. Lett . 16,\n593–597. doi: 10.1109/LGRS.2018.2878773\nLi, X., Song, W., Lian, L., and Wei, X. (2015). Forest ﬁre smok e detection using\nback-propagation neural network based on MODIS data. Remote Sens . 7, 4473–4498.\ndoi: 10.3390/rs70404473\nMahdianpari, M., Salehi, B., Rezaee, M., Mohammadimanesh, F. , and Zhang,\nY. (2018). Very deep convolutional neural networks for complex land cover\nmapping using multispectral remote sensing imagery. Remote Sens. 10, 1119.\ndoi: 10.3390/rs10071119\nObuchowski, N. A., and Bullen, J. A. (2018). Receiver operating characteristic\n(ROC) curves: review of methods with applications in diagnosti c medicine. Phys. Med.\nBiol. 63, 07TR01. doi: 10.1088/1361-6560/aab4b1\nPérez-Rodríguez, L. A., Quintano, C., Marcos, E., Suarez-S eoane, S., Calvo, L., and\nFernández-Manso, A. (2020). Evaluation of prescribed ﬁres fr om unmanned aerial\nvehicles (UAVs) imagery and machine learning algorithms. Remote Sens. 12, 1295.\ndoi: 10.3390/rs12081295\nPowers, D. M. W. (2020). Evaluation: from precision, recall and F-measure\nto ROC, informedness, markedness and correlation. arXiv preprint.2010.16061 .\ndoi: 10.48550/arXiv.2010.16061\nSalih, A. A., and Abdulazeez, A. M. (2021). Evaluation of classiﬁ cation algorithms\nfor intrusion detection system: a review. J Soft Comput Data Min . 2, 31–40.\ndoi: 10.30880/jscdm.2021.02.01.004\nSathishkumar, V., Cho, J., Subramanian, M., and Naren, O. (2 023). Forest ﬁre and\nsmoke detection using deep learning-based learning without f orgetting. Fire Ecol . 19,\n1–17. doi: 10.1186/s42408-022-00165-0\nSelvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D ., and Batra, D.\n(2020). Grad-CAM: visual explanations from deep networks via g radient-based\nlocalization. Int. J. Comput. Vis. 128, 336–359. doi: 10.1007/s11263-019-0\n1228-7\nWang, N., Meng, X. J., Meng, X. C., and Shao, F. (2022). Convolu tion-embedded\nvision transformer with elastic positional encoding for pansh arpening. IEEE T. Geosci.\nRemote 60, 1–9. doi: 10.1109/TGRS.2022.3227405\nWang, Z., Yang, P., Liang, H., Zheng, C., Yin, J., Tian, Y., et al. (2022). Semantic\nsegmentation and analysis on sensitive parameters of forest ﬁ re smoke using smoke-\nunet and landsat-8 imagery. Remote Sens. 14, 45. doi: 10.3390/rs14010045\nWei, H. P., Deng, Y. Y., Tang, F., Pan, X. J., and Dong, W. M. (20 22). A comparative\nstudy of CNN- and transformer-based visual style transfer. J. Comput. Sci. Technol. 37,\n601–614. doi: 10.1007/s11390-022-2140-7\nWu, Q., Shen, C., Wang, P., Anthony, D., and Anton, V. D. H. (20 18). Image\ncaptioning and visual question answering based on attributes and external knowledge.\nIEEE T. Pattern Anal . 40, 1367–1381. doi: 10.1109/TPAMI.2017.2708709\nWu, X., Lu, X., and Leung, H. (2020). A motion and lightness sali ency approach\nfor forest smoke segmentation and detection. Multimed. Tools Appl . 79, 69–88.\ndoi: 10.1007/s11042-019-08047-5\nXie, Y., Qu, J. J., Xiong, X., Hao, X., Che, N., and Sommers, W. (2007). Smoke plume\ndetection in the eastern United States using MODIS. Int. J. Remote Sens. 28, 2367–2374.\ndoi: 10.1080/01431160701236795\nXie, Z., Song, W., Ba, R., Li, X., and Xia, L. (2018). A spatiote mporal contextual\nmodel for forest ﬁre detection using himawari-8 satellite dat a. Remote Sens . 10, 1992.\ndoi: 10.3390/rs10121992\nXu, Y. F., Wei, H. P., Lin, M. X., Deng, Y. Y., Sheng, K. K., Zhan g, M. D., et al.\n(2022). Transformers in computational visual media: a surve y. Comp. Visual Media. 8,\n33–62. doi: 10.1007/s41095-021-0247-3\nYumimoto, K., Nagao, T. M., and Kikuchi, M. (2016). Aerosol d ata assimilation\nusing data from Himawari-8, a next-generation geostationa ry meteorological satellite.\nGeophys. Res. Lett. 43, 5886–5894. doi: 10.1002/2016GL069298\nZhang, B., Wu, Y. F., Zhao, B., Jocelyn, C., Dan, F. H., Jing, Y. , et al. (2022).\nProgress and challenges in intelligent remote sensing satellite s ystems. IEEE J-STARS\n15, 1814–1822. doi: 10.1109/JSTARS.2022.3148139\nZhang, Q. X., Lin, G. H., Zhang, Y.-M., Xu, J., and Wang, J.-J. ( 2018). Wildland\nforest ﬁre smoke detection based on faster R-CNN using synth etic smoke images. Proc.\nEng. 211, 441–446. doi: 10.1016/j.proeng.2017.12.034\nZheng, X., Chen, F., Lou, L., Cheng, P., and Huang, Y. (2022). Real-time detection\nof full-scale forest ﬁre smoke based on deep convolution neural n etwork. Remote Sens.\n14, 536. doi: 10.3390/rs14030536\nZheng, Y. P., Li, G. Y., and Li, Y. (2019). Survey of application of deep\nlearning in image recognition. Comput. Eng. Appl. 55, 20–36. Available online at:\nen.cnki.com.cn/Article_en/CJFDTotal-JSGG201912004.htm\nZhu, X. X., Tuia, D., Mou, L., Xia, G.-S., Zhang, L. P., Xu, F., e t al. (2017). Deep\nlearning in remote sensing: a comprehensive review and list of r esources. IEEE Geosc.\nRem. Sen. M 5, 8–36. doi: 10.1109/MGRS.2017.2762307\nFrontiers in Forests and Global Change /one.tnum/six.tnum frontiersin.org",
  "topic": "Smoke",
  "concepts": [
    {
      "name": "Smoke",
      "score": 0.7245319485664368
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6794928312301636
    },
    {
      "name": "Fire detection",
      "score": 0.6684631705284119
    },
    {
      "name": "Remote sensing",
      "score": 0.5849568843841553
    },
    {
      "name": "Environmental science",
      "score": 0.5779264569282532
    },
    {
      "name": "Computer science",
      "score": 0.5333089232444763
    },
    {
      "name": "Satellite imagery",
      "score": 0.5074502825737
    },
    {
      "name": "Combustion",
      "score": 0.42284929752349854
    },
    {
      "name": "Meteorology",
      "score": 0.309897780418396
    },
    {
      "name": "Artificial intelligence",
      "score": 0.26943036913871765
    },
    {
      "name": "Geography",
      "score": 0.10845303535461426
    },
    {
      "name": "Engineering",
      "score": 0.10565528273582458
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I139660479",
      "name": "Central South University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I119273862",
      "name": "Central South University of Forestry and Technology",
      "country": "CN"
    }
  ],
  "cited_by": 23
}