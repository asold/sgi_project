{
  "title": "Learning convolutional multi-level transformers for image-based person re-identification",
  "url": "https://openalex.org/W4387610103",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2124592618",
      "name": "Peilei Yan",
      "affiliations": [
        "Dalian University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2491900722",
      "name": "Xuehu Liu",
      "affiliations": [
        "Dalian University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2100749226",
      "name": "Ping-Ping Zhang",
      "affiliations": [
        "Dalian University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2168527748",
      "name": "Huchuan Lu",
      "affiliations": [
        "Dalian University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2124592618",
      "name": "Peilei Yan",
      "affiliations": [
        "Dalian University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2491900722",
      "name": "Xuehu Liu",
      "affiliations": [
        "Dalian University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2100749226",
      "name": "Ping-Ping Zhang",
      "affiliations": [
        "Dalian University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2168527748",
      "name": "Huchuan Lu",
      "affiliations": [
        "Dalian University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6682286002",
    "https://openalex.org/W1994623790",
    "https://openalex.org/W2499468060",
    "https://openalex.org/W6728374919",
    "https://openalex.org/W1979260620",
    "https://openalex.org/W1962025484",
    "https://openalex.org/W2783855081",
    "https://openalex.org/W2943407549",
    "https://openalex.org/W2795758732",
    "https://openalex.org/W2966094134",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3143016713",
    "https://openalex.org/W3128723389",
    "https://openalex.org/W2945992438",
    "https://openalex.org/W2963067443",
    "https://openalex.org/W4221158162",
    "https://openalex.org/W3195399086",
    "https://openalex.org/W3191741720",
    "https://openalex.org/W3010766832",
    "https://openalex.org/W2788012242",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3208382980",
    "https://openalex.org/W3166362606",
    "https://openalex.org/W3213438015",
    "https://openalex.org/W3171999262",
    "https://openalex.org/W3200038036",
    "https://openalex.org/W3205065953",
    "https://openalex.org/W3150226983",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W2286929393",
    "https://openalex.org/W3205959870",
    "https://openalex.org/W2744263836",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W6735531217",
    "https://openalex.org/W6687888618",
    "https://openalex.org/W2585635281",
    "https://openalex.org/W1982925187",
    "https://openalex.org/W2769994766",
    "https://openalex.org/W6743440100",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W3035186652",
    "https://openalex.org/W2795013471",
    "https://openalex.org/W2896888563",
    "https://openalex.org/W2963709182",
    "https://openalex.org/W2922510913",
    "https://openalex.org/W2796364723",
    "https://openalex.org/W2979938149",
    "https://openalex.org/W2901617102",
    "https://openalex.org/W2954765307",
    "https://openalex.org/W6772588089",
    "https://openalex.org/W2972012950",
    "https://openalex.org/W2937349443",
    "https://openalex.org/W3009869848",
    "https://openalex.org/W3115484111",
    "https://openalex.org/W3093047836",
    "https://openalex.org/W3173635859",
    "https://openalex.org/W2602856279",
    "https://openalex.org/W3100506510",
    "https://openalex.org/W4247924304",
    "https://openalex.org/W4205720260",
    "https://openalex.org/W4212774754",
    "https://openalex.org/W4234552385"
  ],
  "abstract": "Abstract As a vital vision task, person re-identification (Re-ID) aims to retrieve the same person under non-overlapping cameras. It is a very challenging task due to the presence of complex backgrounds, diverse illuminations and different perspectives. In this work, we integrate the advantages of convolutional neural networks (CNNs) and transformers, and propose a novel learning framework named convolutional multi-level transformer (CMT) for image-based person Re-ID. More specifically, we first propose a scale-aware feature enhancement (SFE) module to extract multi-scale local features from a pre-trained CNN backbone. Then, we introduce a part-aware transformer encoder (PTE) to further mine discriminative local information guided by global semantics. Finally, a deeply-supervised learning (DSL) technique is adopted to optimize the proposed CMT and improve its training efficiency. Extensive experiments on four large-scale Re-ID benchmarks demonstrate that our method performs favorably against several state-of-the-art methods.",
  "full_text": "Visual\nIntelligence\nYan et al.VisualIntelligence            (2023) 1:24 \nhttps://doi.org/10.1007/s44267-023-00025-8\nRESEARCH OpenAccess\nLearningconvolutionalmulti-level\ntransformersforimage-based\npersonre-identiﬁcation\nPeilei Yan1,XuehuLiu 2,PingpingZhang 1* andHuchuanLu 2\nAbstract\nAsavitalvisiontask,personre-identiﬁcation(Re-ID)aimstoretrievethesamepersonundernon-overlapping\ncameras.Itisaverychallengingtaskduetothepresenceofcomplexbackgrounds,diverseilluminationsand\ndiﬀerentperspectives.Inthiswork,weintegratetheadvantagesofconvolutionalneuralnetworks(CNNs)and\ntransformers,andproposeanovellearningframeworknamedconvolutionalmulti-leveltransformer(CMT)for\nimage-basedpersonRe-ID.Morespeciﬁcally,weﬁrstproposeascale-awarefeatureenhancement(SFE)moduleto\nextractmulti-scalelocalfeaturesfromapre-trainedCNNbackbone.Then,weintroduceapart-awaretransformer\nencoder(PTE)tofurtherminediscriminativelocalinformationguidedbyglobalsemantics.Finally,a\ndeeply-supervisedlearning(DSL)techniqueisadoptedtooptimizetheproposedCMTandimproveitstraining\neﬃciency.Extensiveexperimentsonfourlarge-scaleRe-IDbenchmarksdemonstratethatourmethodperforms\nfavorablyagainstseveralstate-of-the-artmethods.\nKeywords: Personre-identiﬁcation(Re-ID),Visiontransformer,Global-localfeatures,Deeply-supervisedlearning\n(DSL)\n1 Introduction\nPerson re-identiﬁcation (Re-ID) aims to retrieve speciﬁc\npersonsinascenebasedonthecontentofimagesorvideos\ntaken at diﬀerent times and places. It has drawn much at-\ntention due to its diversiﬁed real-world applications, such\nas safe communities, intelligent surveillance and crimi-\nnal investigations [1–3]. Although great success has been\nachieved, there are still many challenges in person Re-ID,\nsuchasobjectocclusion,illuminationchange,posedistor-\ntionandbackgroundclutter.\nIn the past two decades, great progresses have been\nachieved in the typical image-based Re-ID task [4]. The\naccomplishment of this task largely depends on the ro-\nbust representations of person images. In fact, early per-\n*Correspondence:zhpp@dlut.edu.cn\n1SchoolofArtiﬁcialIntelligence,DalianUniversityofTechnology,Dalian,\n116024,China\nFulllistofauthorinformationisavailableattheendofthearticle\nson Re-ID methods [5–7] primarily focus on the hand-\ncrafted feature extraction and the similarity metric de-\nsign.Withthedevelopmentofdeeplearningtechnologies,\nmanyworksfocusontheend-to-endlearningofmoredis-\ncriminative features by designing complex deep convolu-\ntionalneuralnetworks(CNNs).Inaddition,localinforma-\ntionisalsodiscriminativeandhelpfulinretrievingthetar-\ngetperson.AsillustratedintheupperrowofFig. 1,thefea-\ntures extracted by the CNN backbone are horizontally di-\nvidedintomultiplepartsinsuchpart-levelfeatureextrac-\ntion methods as part-based convolutional baseline (PCB)\nand research has demonstrated that the PCB method has\nachievedsigniﬁcantperformanceimprovements[ 8].How-\never, the convolutional layers usually model the relation-\nship between pixels in a small neighborhood and cannot\nrealize the global modeling of person images. Thus, most\nCNN-based methods [9–11] are ineﬀective when facing\ncertain challenges such as varied posture, occlusion, and\nbackground clutter.\n©TheAuthor(s)2023. OpenAccess ThisarticleislicensedunderaCreativeCommonsAttribution4.0InternationalLicense,which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit\nto the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The\nimagesorotherthirdpartymaterialinthisarticleareincludedinthearticle’sCreativeCommonslicence,unlessindicatedotherwise\nin a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not\npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright\nholder.Toviewacopyofthislicence,visit http://creativecommons.org/licenses/by/4.0/.\nYan et al.VisualIntelligence            (2023) 1:24 Page 2 of 12\nRecently,transformers[ 12]haveachievedexcellentper-\nformanceinnaturallanguageprocessingandcomputervi-\nsion. The key reason is that transformers are global op-\nerations based on self-attention and can model the re-\nlationship between all input elements. As a result, sev-\neral attempts have been made to accomplish person Re-\nID using transformers. For example, Zhu et al. [13]i n t r o -\nduced an auto-aligned structure and enhanced the ability\noftransformerstoextractmorediscriminativefeatures.He\netal.[ 14]proposedapuretransformerarchitecturetointe-\ngratecameraandviewpointinformationandachievedex-\ncellent performance in object re-identiﬁcation. Although\neﬀective,thesetransformer-basedmethodsrequirealarge\nnumber of transformer blocks, resulting in high model\ncomplexity. In addition, these works seldom take into ac-\ncountthelocalinformationofpersons,whichiscrucialfor\nperson Re-ID. Therefore, there is still much room for im-\nprovementincurrenttransformer-basedmethods.\nInthiswork,wetakeadvantageofCNNsandtransform-\ners, and propose a novel learning framework named con-\nvolutionalmulti-leveltransformer(CMT)forimage-based\nperson Re-ID. More speciﬁcally, we ﬁrst utilize a scale-\nawarefeatureenhancement(SFE)moduletoextractmulti-\nscalelocalfeaturesfromdeepCNNbackbones.Asaresult,\ntheycancapturemulti-granularityrepresentationsofvar-\nious appearances in person images. Then, we introduce a\npart-aware transformer encoder (PTE) to further extract\nlocal discriminative information guided by global seman-\ntics.AsshowninthebottomrowofFig. 1,weincorporate\nthe idea of feature partitioning into the transformer and\ndesign a recursive transformer structure. This structure\ncan generate hierarchical features for diverse local parts,\nresulting in great performance improvements. Finally, we\nadoptadeeply-supervisedlearning(DSL)techniquetoop-\ntimize the proposed CMT and improve its training eﬃ-\nciency. Extensive experiments on four large-scale Re-ID\nbenchmarks demonstrate that our method performs fa-\nvorablyagainst moststate-of-the-artmethods.\nThemaincontributionsaresummarizedasfollows:\n1) Anovelglobal-local featurelearning framework\n(i.e., CMT) is proposedforrobustpersonRe-ID.\n2) ASFEmodule isproposedto extractmulti-scale\nlocalfeatures,capturingmulti-granularity\nrepresentationsof personimages.\n3) APTE is proposedto furtherextractlocal\ndiscriminative information guided byglobal\nsemantics.The PTE cangenerate hierarchical\nfeaturesfordiverse localparts.\n4) Extensiveexperimentsdemonstrate thatour\nproposedframework caneﬀectivelyextractrobust\nanddiscriminative features.It achieves\nstate-of-the-artperformances on fourlarge-scale\nRe-IDbenchmarks.\nFigure1 TheinsightofourproposedCMT.Upperrow:Previous\nhorizontalpartdivisionsinmodelssuchasPCB[ 8];Bottomrow:Our\nhierarchicalandrecursivepartencodingwithtransformers\n2 Relatedworks\n2.1 Part-basedpersonre-identiﬁcation\nIn recent years, image-based person Re-ID has achieved\ngreat improvements in performance. Generally, existing\nperson Re-ID methods mainly focus on extracting dis-\ncriminative global features from entire images. However,\nfocusing merely on the global information of persons has\nsome limitations, such as ignoring the eﬀectiveness of lo-\ncal cues. Fine-grained local part features such as T-shirt\nand black backpack can be very useful to identify persons\nincomplexscenes.Asatypicalpractice,manyresearchers\nresorttopartfeaturesforpedestrianimagedescription.In\nparticular, Sun et al. [8] proposed a method to divide spa-\ntial features into horizontal strips to improve the Re-ID\nperformance.Wangetal.[ 10]utilizedamulti-branchnet-\nwork to extract the multi-granularity features of persons.\nZhengetal.[ 15]proposedacoarse-to-ﬁnepyramidmodel\nto fuse global and local features. Yang et al. [16]d e s i g n e d\na patch-wise loss function to guide the eﬀective learning\nof patch features. Cho et al. [17] leveraged the comple-\nmentaryrelationshipsbetweenglobalandlocalfeaturesto\nreﬁne the pseudo labels of parts and reduce label noises.\nDiﬀerentfromtheabovelocal-basedmethods,wepropose\na recursive structure to iteratively mine local features un-\nderglobalsemanticguidance.Byhierarchicallearning,our\nmethodcangeneratediverselocalpartfeaturesofindivid-\nualpersons,resultinginsuﬃcientrichnessofimageinfor-\nmationandrobustnessoftheRe-IDresults.\n2.2 Attention-basedpersonre-identiﬁcation\nVisual attention mechanisms aim to highlight relevant in-\nformation and suppress irrelevant information. Inspired\nby the advantages of attention mechanisms, researchers\nhaveproposedvariousattention-basedmethodstoextract\ndistinguishable features for person Re-ID. For example,\nChen et al. [18] proposed a mixed high-order attention\nto capture the subtle diﬀerences among pedestrians. Rao\net al. [19] presented a counterfactual attention to capture\nYan et al.VisualIntelligence            (2023) 1:24 Page 3 of 12\nFigure2 Thearchitectureofourproposedframework.Givenapersonimage,weﬁrstutilizethemulti-levelfeatureextractor(MFE)toextract\nmulti-levelfeaturemaps.Then,fortheCNNbranch,weemployaGAPlayertoobtaintheCNNfeature.ForthreestagesoftheMFE,weintroducethe\ntransformerbranch,andutilizetheSFEmoduleandPTEtofurtherextractmulti-scaleanddiversepartfeatures,respectively.Allthesebranchesar e\nsupervisedbythetripletlossandcross-entropylossformodeltraining\nmorediscriminativerepresentations.Chenetal.[ 20]built\napyramidattentiontoexploreattentiveregionsinamulti-\nscale manner. Zhang et al. [21] proposed a relation-aware\nattentiontocapturetheglobalstructuralinformationfrom\npersons.Lietal.[ 22]presentedaharmoniousattentionto\nreduce the misalignments of the same persons. Diﬀerent\nfromtheaboveattention-basedmethods,weintroduceat-\ntention mechanisms to capture long-range dependencies\nbetweenlocalfeatures,leadingtomuchbetterresults.\n2.3 Transformer-basedpersonre-identiﬁcation\nIn fact, transformers [12] are initially proposed for pro-\ncessing sequential data. With the global modeling abil-\nity, transformers have been recently introduced to many\ncomputervisiontasks,includingpersonRe-ID.Forimage-\nbased person Re-ID, He et al. [14]ﬁ r s tu t i l i z e dap u r e\ntransformer-based structure [23] to learn discriminative\nf e a t u r e s .Z h ue ta l .[13] added learnable vectors of part\ntokens to learn part features and integrated part align-\nments into the self-attention. Lai et al. [24] utilized trans-\nformerstoachieveadaptivepartdivisions.Lietal.[ 25]in-\ntroduced a diverse part discovery with part-aware trans-\nformers for occluded person Re-ID. Liao and Shao [26]\nbuilt a transformer-based deep image matching for gen-\neralizable person Re-ID. Wang et al. [27] proposed a self-\nguided transformer framework to explore the relations of\nbody parts for feature alignment. Chen et al. [28]p r o -\nposed an omni-relational high-order transformer for per-\ns o nR e - I D .M ae ta l .[29] proposed a pose-guided trans-\nformer to mine the inter-part and intra-part relations for\noccludedperson Re-ID. Liu et al. [30] designed a trigemi-\nnaltransformertosimultaneouslyencodethespatial,tem-\nporal and spatial-temporal features in complex videos.\nThesetransformer-basedmethodshaveachievedsuperior\nperformances. However, they generally lack desirable lo-\ncalproperties.Diﬀerentfromthem,weintroduceahybrid\nstructurecombiningCNNsandtransformersformoreef-\nfectivepersonRe-ID.\n3 Proposedmethod\nAs illustrated in Fig.2, the proposed framework mainly\nincludesthreekeymodules:amulti-levelfeatureextractor\n(MFE), the SFE module and a PTE. More speciﬁcally, the\nMFE utilizes a pre-trained CNN backbone (e.g., ResNet-\n50 [31]) to extract multi-level features of person images.\nAfterwards, the SFE module adopts multi-scale dilated\nconvolutions [32] with residual connections to capture\nmulti-granularity feature representations. Furthermore,\nwithahierarchicalstructure,PTEfurthermineslocaldis-\ncriminative information guided by global semantics. Fi-\nnally, the DSL technique is utilized to optimize the whole\nframework.Wewillelaborateonthesekeycomponentsin\nthefollowingsubsections.\n3.1 Multi-levelfeatureextractor\nAs illustrated in the left part of Fig. 2,w eu t i l i z et h e\nResNet-50 [31]pre-trainedonImageNettoextractmulti-\nl e v e lf e a t u r e s .S i m i l a rt op r e v i o u sw o r k s[8, 10, 33], we\nremove the fully-connected layers after the global aver-\nage pooling (GAP) layer, and change the stride of the ﬁfth\nstage to 1, resulting in a 1/16 feature resolution of input\nimages. In addition, we take the outputs of stages 3, 4 and\n5, and introduce an additional convolutional layer to gen-\neratesize-ﬁxed multi-levelfeatures.\n3.2 Scale-awarefeatureenhancement\nDue to the variations of persons in scenes, multi-scale in-\nformation[33]iseﬀectiveforrobustappearancerepresen-\ntations.Thus,weproposetheSFEmoduletoextractmulti-\nscalefeaturesatthreestagesofthebackbonenetwork.\nThestructureofourproposedSFEmoduleisillustrated\nin Fig.3. Given an inputX\ni (i=3,4,5),weﬁrstreducethe\nchannelnumberstoaquarterof Xi byaconvolutionallayer\nand obtain˜X\ni\n. Then, we utilize four dilated convolutional\nlayers to generate multi-scale features and gradually ex-\nYan et al.VisualIntelligence            (2023) 1:24 Page 4 of 12\nFigure3 Thestructureofourscale-awarefeatureenhancement\ntend the receptive ﬁelds [32].\nM1 =Conv1\n( ˜X\ni)\n, M2 =Conv2\n( ˜X\ni)\n,\nM3 =Conv3\n( ˜X\ni)\n, M4 =Conv4\n( ˜X\ni)\n.\n(1)\nThen, they are concatenated in the channel and aggre-\ngated by another convolutional layer. Meanwhile, a resid-\nualconnectionisutilizedtoobtaintheﬁnaloutputofSFE,\nY\ni =Xi +Conv\n(\n[M1;M2;M3;M4]\n)\n,( 2 )\nwhere [;] means the concatenation in the channel. In fact,\ndue to the utilization of diﬀerent kernel sizes and dilation\nsizes, our SFE module is able to capture multi-scale local\ncues for scale-aware feature enhancement.\n3.3 Part-awaretransformer-basedencoder\nInadditiontoSFE,weemployPTEtofurtherextractpart-\nware ﬁne-grained representations with transformers. As\nillustrated in Fig.4, our PTE is designed with a recursive\nand hierarchical structure, which progressively generates\ndiverse partfeatureswithglobalsemantic guidance.\nFormally, the PTE takesYi as input and introduces hi-\nerarchical divisions for diverse part features. It should be\nnoted that all the transformers at the same stage share\nweights for computation reduction. The structure of the\ntransformers is identical to [23]. At the 2\nk-part learning\nstage (k =1,2,...),weﬁrstusea1 × 1 convolutional layer\nto halve the number of channels. Then, we reshape the\nfeature map into a sequence representationF\n2k ∈ RHW×C.\nHere, H and W denote the height and width of the input\nimage, respectively.C represents the number of channels.\nThe class tokenFcls\n2k–1 ∈ R1×C from the 2k–1-part learn-\ning stage is concatenated into the sequence to guide the\nﬁne-grained features. In addition, a new class tokenFcls\n2k ∈\nR1×C is also concatenated into the sequence to summa-\nrize contextual information. Finally, the position embed-\ndingFpos\n2k ∈ R(HW+2)×C isaddedtothesequence.Forthe2 k-\npart learning stage, the input embedding for thej-th part\ntransformeris:\n˜F2k,j =\n[\nFcls\n2k,j;φ\n(\nFcls\n2k–1,n\n)\n;˜F2k–1,j\n]\n+Fpos\n2k,j,( 3 )\nwherej∈{ 1,2,...,2 k},and nisequalto j/2 whenj iseven;\notherwise n is equal to (j +1 ) / 2 .φ is a linear projection\ntoalignthechannelnumbersoffeatures.Theaboveinput\ngoesthroughseveraltransformerlayers,eachofwhichin-\ncludes a multi-head self attention (MHSA) module and a\nfeedforwardnetwork(FFN).Afterbuildingthehierarchi-\ncalstructure,wegeneratethepartfeaturesas:\nF\np =\n[˜F\ncls\n2k,1;˜F\ncls\n2k,2;...; ˜F\ncls\n2k,2k\n]\n.( 4 )\nFrom the above equations and Fig.4,o n ec a ns e et h a t\nour proposed PTE uses transformers to generate hierar-\nchicallocalfeatureswiththeguidanceofglobalsemantics.\nThis recursive and hierarchical design can not only gen-\nerate multi-scale and multi-granularity features but also\nprovide global guidance for more discriminative features,\nenhancing the extraction of local features. In addition,\nwe apply transformers to extract local features and stack\nfewer transformer blocks, which can signiﬁcantly reduce\nthe model complexity.\n3.4 Deeply-supervisedlearning\nAs illustrated in Fig.2, we utilize both the featureFCNN\ngenerated from the CNN branch and the featuresFTrans\nfrom the transformer branch for inference. To train the\nw h o l ef r a m e w o r k ,w ea d o p tt h eD S Lt e c h n i q u e[33, 34],\nwhich makes the network optimization a task that is easy\nto complete. At each branch, we use the label-smoothed\ncross-entropyloss[ 35]andthebatch-hardtripletloss[ 36].\nThe label-smoothedcross-entropyloss is deﬁned as:\nLce =\nN∑\ni=1\n–qi ln(pi), (5)\nwhere pi is the predicted logit of identityi and qi is the\nground-truth label. The batch-hard triplet loss is deﬁned\nas:\nLtri =[dpos –dneg +m]+,( 6 )\nwhere dpos and dneg are deﬁned as the distance of positive\nsample pairs and negative sample pairs, respectively. [x]+\nis max(0,x)a n dmisthedistancemargin.\nFinally,theoveralllosscanbesummarizedas:\nLall =Lce +Ltri +λ\nK∑\nk=1\n(\nLk\nce +Lk\ntri\n)\n,( 7 )\nYan et al.VisualIntelligence            (2023) 1:24 Page 5 of 12\nFigure4 Illustrationofourproposedpart-awaretransformer-basedencoder. HandW denotetheheightandwidthoftheinputimage,respectively.\nQ,K,andVrepresentquery,keyandvalue,respectively.MHSArepresentsamulti-headselfattentionandFFNmeansafeedforwardnetwork\nwhere K is the number of stages.λ is the balanced coeﬃ-\ncientforthemultiplelossterms.\n4 Experiments\n4.1 Datasetsandevaluationmetrics\nWe conducted extensive experiments on four widely-\nused person Re-ID datasets, i.e., Market1501 [ 37],\nDukeMTMC-ReID [38], CUHK03-NP [39] and MSMT17\n[40].TheMarket1501wascollectedfromsixcamerasand\nhas1501pedestrians(751fortrainingand750fortesting).\nThe DukeMTMC-ReID was collectedfrom eight cameras\nwith 1404 pedestrians (702 for training and 702 for test-\ning). The CUHK03-NP dataset consists of 1467 pedes-\ntrians, which are divided into two sub-datasets: one with\nmanuallabelingandtheotherwithboundingboxeslabeled\nbyapersondetector.TheMSMT17isalarge-scaledataset\nderivingfrom15cameraswith4101pedestrians(1041for\ntraining and 3010 for testing). Table1 provides more de-\ntailed statistics of the four datasets. Following previous\nworks [4, 33], we compute the mean average precision\n(mAP)andcumulativematchingcharacteristics(CMC)at\nrank-1forperformanceevaluation.\n4.2 Implementationdetails\nIn this work, all the experiments are performed with the\nPyTorch toolbox\n1 and one GeForce RTX 3090 GPU. We\nutilize the ResNet-50 pre-trained on ImageNet as our\nbackbone. In addition, we balance the accuracy and com-\nplexity,andultimatelychoosetoextractfourpartsthrough\nthe PTE. To extract the multi-scale features by the SFE\nmodule,a1 ×1convolutionallayerandthree3 ×3dilated\nconvolutional layers are used to gradually extend the re-\nceptive ﬁelds. Dilation sizesd are 1, 2 and 3, respectively.\nDuring training, all images of pedestrians are resized to\n1http://pytorch.org.\nTable 1Statisticsofouruseddatasets\nDataset ID Image Train Test #Cameras\nMarket1501 1501 32668 12936 19732 6\nDukeMTMC-ReID 1404 36411 16522 19889 8\nCUHK03-NP-Labeled 1467 14096 7368 6728 10\nCUHK03-NP-Detected 1467 14096 7365 6732 10\nMSMT17 4101 126441 32621 93820 15\n256 × 128 and augmented by random cropping, horizon-\ntal ﬂipping and random erasing [41]. In one mini-batch,\n16 identities are randomly sampled and each identity has\n4 images. The Adam optimizer [42]i sd e p l o y e dw i t ha n\ninitial learning rate of 3.5× 10–4, which is multiplied by\n0.4 every 20 epochs until 180 epochs. The source code is\nreleased athttps://github.com/AI-Zhpp/CMT.\n4.3 Comparisonwithstate-of-the-artmethods\nIn this subsection, we compare our method with other\nstate-of-the-art methods. The comparison results on four\npublicRe-IDbenchmarksarepresentedinTable 2.Thede-\ntailanalysisisasfollows:\nMarket1501 As for CNN-based methods, PCB [8]a n d\nMGN [10] mine diverse part features by horizontal strip\nfeatures and reach 81.6% mAP and 86.9% mAP on Mar-\nket1501, respectively, which validate the reasonableness\nof part learning in Re-ID. In our method, we adopt a\nhierarchical transformer-based structure to progressively\nextract multi-granularity part representations. Thus, our\nmethodachievesthebestmAPandoutperformsPCBand\nMGNby8.3 % and3.0%,respectively.Evenincomparison\nwith transformer-based methods, such as AAformer [13],\nTransReID [14],APD[ 24]a n dHA T[33],ourmethodstill\ndelivers abetterperformance.\nDukeMTMC-ReID On this dataset, our method shows\nsuperiorperformances.ThemAPandrank-1accuracyare\nYan et al.VisualIntelligence            (2023) 1:24 Page 6 of 12\nTable 2Performance(%)comparisonwithstate-of-the-arts.Thebestperformanceismarkedinboldandthesecond-bestperformance\nisunderlined. ∗ indicatesthatthemethodsareusingcamerainformation\nMethods Backbones Market1501 DukeMTMC-ReID CUHK03-NP MSMT17\nLabeled Detected\nmAP Rank-1 mAP Rank-1 mAP Rank-1 mAP Rank-1 mAP Rank-1\nDuATM[44] DenseNet121 76.6 91.4 64.6 81.8 – – – – – –\nMancs[45] ResNet50 82.3 93.1 71.8 84.9 63.9 69.0 60.5 65.5 – –\nIANet[46] ResNet50 83.1 94.4 73.4 83.1 – – – – 46.8 75.5\nBoT[47] ResNet50 85.7 94.1 75.9 86.2 73.8 74.7 71.2 73.4 49.8 74.0\nPCB[8] ResNet50 81.6 93.8 69.2 83.3 – – 57.5 63.7 40.4 68.2\nSPReID[48] ResNet152 83.4 93.7 73.3 85.9 – – – – – –\nAANet[49] ResNet152 83.4 93.9 74.3 87.7 – – – – – –\nCASN[50] ResNet50 82.8 94.4 73.7 87.7 68.0 73.7 64.4 71.5 – –\nCAMA[51] ResNet50 84.5 94.7 72.9 85.8 – – 64.2 66.6 – –\nBATNet[52] ResNet50 84.7 95.1 77.3 87.7 76.1 78.6 73.2 76.2 56.8 79.5\nMHN-6[18] ResNet50 85.0 95.1 77.2 89.1 72.2 77.2 65.4 71.7 – –\nBFE[53] ResNet50 86.2 95.3 75.9 88.9 76.7 79.4 73.5 76.4 51.5 78.8\nMGN[10] ResNet50 86.9 95.7 78.4 88.7 67.4 68.0 66.0 68.0 – –\nABDNet[11] ResNet50 88.3 95.6 78.6 89.0 – – – – 60.8 82.3\nPyramid[15] ResNet101 88.2 95.7 79.0 89.0 76.9 78.9 74.8 78.9 – –\nJDGL[54] ResNet50 86.0 94.8 74.8 86.6 – – – – 52.3 77.2\nOSNet[9] OSNet 84.9 94.8 73.5 88.6 – – 67.8 72.3 52.9 78.7\nSNR[55] ResNet50 84.7 94.4 73.0 85.9 – – – – – –\nSCSN[43] ResNet50 88.5 95.7 79.0 91.0 –– –– ––\nISP[56] HRNet48 88.6 95.3 80.0 89.6 74.1 76.5 71.4 75.2 – –\nHAA[57] ResNet50 89.5 95.8 80.4 89.0 – – – – – –\nCDNet[58] CDNet 86.0 95.1 76.8 88.6 – – – – 54.7 78.9\nAPNet[20] ResNet50 89.0 96.1 78.8 89.3 81.1 83.5 78.1 80.9 59.0 80.8\nAAformer[13] ViT-B/16 87.7 95.4 80.0 90.1 77.8 79.9 74.8 77.6 62.6 83.1\nTransReID∗ [14] ViT-B/16 88.2 95.0 80.6 89.6 – – – – 64.9 83.3\nAPD[24] ResNet50 87.5 95.5 74.2 87.1 73.8 77.0 70.6 74.6 57.1 79.8\nHAT[33] ResNet50 89.5 95.6 81.4 90.4 80.0 82.6 75.5 79.1 61.2 82.3\nCMT(Ours) ResNet50 89.9 95.8 82.1 90.5 80.7 82.9 78.4 81.6 63.5 83.3\n82.1% and90.5%,respectively,andexceedmostofthecur-\nrentmethods.ItisnotedthatSCSN[ 43]integratessalient\nfeatures using a cascaded network architecture, resulting\ninarank-1accuracyof91 %.Diﬀerentfromit,ourmethod\ntakes advantages of CNNs and transformers to incorpo-\nrate global and local features. Compared with SCSN, our\nmethodgainsa 3.1% improvementinmAP.\nCUHK03-NP On two sub-datasets of CUHK03-NP, our\nmethod consistently achieves competitive results. Mean-\nwhile, APNet [20] utilizes a pyramid attention to explore\nthe discriminative regions of person images, and achieves\n81.1% mAP and 78.1% mAP on the labeled and detected\nsub-datasets of CUHK03-NP, respectively. Diﬀerent from\nAPNet, our method extracts ﬁne-grained partial features\nby multi-stage transformers. Compared with APNet, our\nmethod improves the mAP on the detected CUHK03-NP\nby 0.3%.\nMSMT17 On this dataset, our framework also attains\ncomparable performance in terms of mAP and rank-1.\nIn fact, TransReID achieves the best mAP and rank-1\non MSMT17. However, TransReID uses ViT [23]a st h e\nbackbonetocapturelong-rangedependencies,whichcon-\nsumeshighcostcomplexityandextremelyimpactsthein-\nference speed. In contrast, our method uses ResNet-50\nto extract local representations and combines part-aware\ntransformers for ﬁne-grained cues. Thus, our method at-\ntains a signiﬁcant improvement of eﬃciency over Tran-\nsReID.Inaddition,TransReIDutilizescamerainformation\nforperformanceboosting,whileourmethoddoesnotuti-\nlizecamerainformationbutuniﬁesthestrengthsofCNNs\nand transformers, which leads to the second-best perfor-\nmanceonMSMT17.\nModel complexities To further clarify the computation\nadvantages, we compare the model complexity of some\ntypical methods in Table3. We use ﬂoating point oper-\nations per second (FLOPs) to test our model’s computa-\ntionalcomplexity.AscanbeseeninTable 3,ourproposed\nmodel shows great advantages over other transformer-\nbased methods in terms of FLOPs. We also note that\nour proposed model has more parameters. This problem\nYan et al.VisualIntelligence            (2023) 1:24 Page 7 of 12\nTable 3Comparisonsofmodelcomplexities.BothCNN-based\nmethodsandtransformer-basedmethodsareselectedfor\ncomparisons.Paramsmeansparameter.FLOPsdenotesﬂoating\npointoperationspersecond\nMethods Backbones Market1501 Params.\n(M)\nFLOPs\n(G)mAP(%) Rank-1(%)\nBoT[47] ResNet50 85.7 94.1 25.64 4.08\nABDNet[11] ResNet50 88.3 95.6 53.64 6.27\nAPNet[20] ResNet50 89.0 96.1 29.90 8.16\nHAT[33] ResNet50 89.5 95.6 219.44 21.44\nTransReID∗ [14] ViT-B/16 88.2 95.0 104.71 178.52\nCMT(Ours) ResNet50 89.9 95.8 286.54 21.32\nTable 4Ablationanalysisofkeymodules.Paramsmeans\nparameter.FLOPsdenotesﬂoatingpointoperationspersecond\nMethods MSMT17 Params.\n(M)\nFLOPs\n(G)mAP(%) Rank-1(%)\nBaseline 49.8 74.0 25.64 4.08\n+PTE 62.6 82.4 238.79 16.01\n+SFE 63.5 83.3 286.54 21.32\ncan be solved by light-weight designs. CNN-based meth-\nods generally have fewer parameters and FLOPs. How-\never, their performances are usually worse than those of\ntransformer-basedmethods.Overall,ourproposedmodel\nachieves a good balance between the Re-ID performance\nandthemodelcomplexity.\n4.4 Ablationstudies\nTo verify the eﬀectiveness of our proposed modules, we\nconductablationexperimentsontheMSMT17 dataset.\nEﬀectiveness of key modules The ablation results of our\nkey modules are reported in Table4.F o rt h eb a s e l i n e\nmethod, we ﬁne-tune ResNet-50 on MSMT17 and adopt\nGAP to obtain a featurevector for testing, which achieves\n49.8% mAPand74.0 % rank-1accuracy.Then,weaddour\nPTEtothebaselinetofurtherextractdiversepartfeatures\nat three stages. In PTE, the global feature is recursively\npassed into part-aware transformers and used to reﬁne\npart features. Thus, our PTE brings signiﬁcant improve-\nmentsoverthebaseline(i.e.,12.8 % mAPand8.4 % rank-1\naccuracy). Furthermore, we insert SFE to enhance the lo-\ncalfeaturesbeforePTE.SFEcancapturemulti-granularity\nrepresentationsofpersonimages.Therefore,itbringsper-\nformance improvement (i.e., 0.9% mAP and 0.9% rank-1\naccuracy). Overall, the resulting improvements verify the\neﬀectiveness of our SFE module and PTE, which play a\ncritical role in the extraction of multi-scale and discrim-\ninative features.\nEﬀectsofthePTEmodule InPTE,weintroduceahierar-\nchical transformer to split and encode part features. The\nTable 5AblationanalysisofthePTEmodule.Paramsmeans\nparameter.FLOPsdenotesﬂoatingpointoperationspersecond\nMethods #Parts MSMT17 Params.\n(M)\nFLOPs\n(G)mAP(%) Rank-1(%)\nBaseline – 49.8 74.0 25.64 4.08\n+PTE 1 56.7 79.6 127.33 11.38\n2 62.0 82.1 192.19 14.50\n4 62.6 82.4 238.79 16.01\nTable 6AblationresultsofdeployingPTEafterdiﬀerentlevelsof\nResNet-50\nMethods mAP(%) Rank-1(%)\nResNet-50 49.8 74.0\nRes3+PTE 56.7 77.2\nRes4+PTE 59.5 80.8\nRes5+PTE 53.0 76.9\nRes3,Res4+PTE 61.8 81.1\nRes3,Res4,Res5+PTE 62.6 82.4\nablation results are summarized in Table5.A st h er e c u r -\nsiveandhierarchicalstructureadvances,theaccuracyalso\nachieve signiﬁcant improvements. It can be observed that\nthe mAP and rank-1 accuracy are improved by 5.3% and\n2.5%,repectivelywhenthespatialfeaturesaredividedinto\ntwo parts. With the increase of the part numbers, the di-\nversity of ﬁne-grained clues is captured. In our work, four\npartsareextractedforthetrade-oﬀbetweenaccuracyand\ncomplexity,.\nEﬀects of PTE at diﬀerent levelsIn experiments, we de-\nploy PTE at diﬀerent levels of ResNet-50 to realize multi-\nlevel part learning. The experimental results are listed in\nTable6.Fromtheresults,onecanobservethatthedeploy-\nment of PTE at a single level can improve performance.\nThe best performance is achieved when PTE is deployed\natthreelevelsofResNet-50.Thisfactconﬁrmsthatmulti-\nlevel representation learning is helpful to achieve better\nperformancesofpersonRe-ID.\nEﬀectiveness of DSL In this work, we introduce the DSL\nfor better model training. By deploying losses at diﬀerent\nstages, the ablation results are reported in Table7.I tc a n\nbe observed that the single deployment of supervision at\nthe 4-partlearning stage is not suﬃcient and moresuper-\nvision isneededtotraintheentireframeworkwell.When\nsupervisionisdeployedatallstages,wecanobtainthebest\nperformance.\nEﬀects of diﬀerent transformer layers and attention heads\nThenumberoftransformerlayersandattentionheadsmay\nchange the structure and performance of our PTE. Thus,\nwe perform ablation experiments to examine the eﬀects\nYan et al.VisualIntelligence            (2023) 1:24 Page 8 of 12\nTable 7AblationresultsofDSL\nMethods mAP(%) Rank-1(%)\n4-part 46.0 70.0\n4-part+1-part 60.4 81.5\n4-part+2-part 50.3 72.4\n4-part+2-part+1-part 63.5 83.3\nFigure5 Ablationresultswithdiﬀerenttransformerlayers\nFigure6 Ablationresultswithdiﬀerentattentionheads\nof transformer layers and attention heads. As shown in\nFig. 5, the performance of our proposed model is signif-\nicantly reduced without transformer layers. The perfor-\nmance degradation indicates that the features obtained\nsolelyfromCNNsarenotrobustenough,andtransformers\ncan implicitly learn more discriminative information. In\naddition,weobservethatwhenthenumberoftransformer\nlayers is set to 2, the best performance can be achieved.\nMeanwhile, with the increase of transformer layers, there\naresomeﬂuctuatingchangesinperformance.Thismaybe\nbecause diﬀerent transformer layers can change the local\nfeatures.Furthermore,fromFig. 6,itcanbeobservedthat\nas the number of attention heads increases, the retrieval\naccuracycontinuestobeimproved.Nevertheless,theper-\nformanceissaturatedwhenthenumberofattentionheads\nis equal to 16. Based on the aforementioned facts, we set\nFigure7 Ablationresultswithdiﬀerentbalancedcoeﬃcient λ\nthe numbers of transformer layers and attention heads to\n2and16, respectively.\nEﬀectsofthebalancecoeﬃcient λ Inourwork,weutilize\nλ tobalancediﬀerentlosstermsinEq.( 7).Toverifyitsef-\nfect, we conduct experiments by changing the coeﬃcient\nλ from0to3.AsdisplayedinFig. 7,withtheincreaseof λ,\nthe performance continues to be improved. Whenλ isset\nto1.5,thebest performancecanbeachieved.\n4.5 Visualizationanalysis\nVisualization of feature mapsTo verify the eﬀectiveness\noftheproposedmodules,wefurthervisualizethefeatures\nofpersonexamples.ThevisualizationsareshowninFig. 8.\nIn each example, from left to right, there are the original\nimage, baseline features, SFE features, and PTE features.\nIt can be observed that increasingly detailed information\nis captured with the gradual utilization of our key mod-\nules. Moreover, the feature maps obtained from the base-\nline generally focus on salient regions, such as the heads\nor shoes of persons. With the utilization of the SFE mod-\nule to extract multi-scale features, our model can capture\nmore meaningful information, such as bags and clothing.\nWith the utilization of the PTE module to extract diverse\nlocal features, our model can capture more detailed in-\nformation, such as torso details. The visualization results\ndemonstratethatourPTEcanindeedminediscriminative\nanddiverse localcuesguidedbyglobalsemantics. Thevi-\nsualizations intuitively verify the eﬀectiveness of our pro-\nposedS FEmoduleandPTE.\nMeanwhile, we visualize the diﬀerent parts in PTE for\nqualitativecomparisoninFig. 9.Comparingthe2-nd,3-rd\nand4-thcolumns,itcanbeobservedthatmorelocalcues\nc a nb ec a p t u r e da st h en u m b e ro fp a r t si n c r e a s e s .T h e s e\nvisualization comparisons further explain the reasonable-\nnessofourPTE.\nRetrievalresults Wealsovisualizetheretrievalresultson\nthe MSMT17 dataset in Fig.10. It can be observed that\nthe retrieval accuracies are improved when the proposed\nkey modules are gradually added to the baseline method.\nYan et al.VisualIntelligence            (2023) 1:24 Page 9 of 12\nFigure8 VisualizationsoffeaturesobtainedfromBaseline,SFEandPTE\nFigure9 VisualcomparisonsofdiﬀerentpartfeaturesinPTE\nAs illustrated in Fig.10, the matching accuracies of the\nbaselinemethodaretheworstbecausethecorrectsamples\nhaveextremelysimilarglobalappearancestotheincorrect\nsamples. However, with the utilization of SFE and PTE,\nthe matching accuracy is signiﬁcantly improved. Our SFE\nmodule and PTE can extract the multi-scale and multi-\npart features from global appearances. They are useful in\nimproving the ability of our method to distinguish similar\nsamples.Theretrievalresultsfurthervalidatetheeﬀective-\nnessofourproposedmodules.\nt-SNEvisualization AsshowninFig. 11,wevisualizethe\nfeaturedistributionsofthebaselinemethodandourCMT\nusingt-SNE[ 59].Werandomlyselect18personsfromthe\nMSMT17 dataset, and 50 images of each person. Diﬀer-\nent colors represent diﬀerent identities. From Fig.11(a),\nit can be observed that the feature distributions with the\nsameidentityarerelativelyscattered.Therearesomemis-\nclassiﬁed samples. However, with our CMT, features of\nthe same identity are more clustered and features of dif-\nferentidentitiesarerelativelyseparated.Inaddition,there\nare few misclassiﬁed samples compared with the baseline\nmethod. The t-SNE visualizations show that our method\nindeed helps the method learn a more discriminative em-\nbedding space, which further conﬁrms our superiority to\nachieverobustpersonRe-ID.\n5C o n c l u s i o n\nIn this paper, we integrate the advantages of CNNs and\ntransformers and propose a novel learning framework\nnamedCMTforimage-basedpersonRe-ID.First,wepro-\npose a SFE module to extract the multi-scale features at\ndiﬀerent levels of the CNN backbone. Furthermore, we\nproposeaPTEtogenerateandminelocaldiversepartfea-\ntures with global guidance. Experimental results on four\npublic Re-ID benchmarks demonstrate that our method\nperformsfavorablyagainstmoststate-of-the-artmethods.\nInthefuture,wewillreducethecomputationalcomplexity\nandimprovetheeﬃciencyofourpart-awaretransformers.\nYan et al.VisualIntelligence            (2023) 1:24 Page10of12\nFigure10 VisualizationoftheretrievedresultsontheMSMT17dataset.Thetop-5retrievedimagesarepresented.Thetruematchesareannotated\nbygreenboxesandthewrongmatchesareannotatedbyredboxes\nFigure11 ComparisonoffeaturedistributionsinBaselineandour\nCMTbyusingt-SNE.Diﬀerentcolorsrepresentdiﬀerentidentities\nAcknowledgements\nWethanktheeditorandallthereviewersfortheirconstructivefeedback.\nFunding\nThisworkwassupportedinpartbytheNationalNaturalScienceFoundationof\nChina(NSFC)(No.62101092)andtheFundamentalResearchFundsforthe\nCentralUniversities(No.DUT20RC(3)083).\nAbbreviations\nCMC,cumulativematchingcharacteristics;CMT,convolutionalmulti-level\ntransformer;CNNs,convolutionalneuralnetworks;DSL,deeply-supervised\nlearning;FFN,feedforwardnetwork;GAP,globalaveragepooling;mAP,mean\naverageprecision;MHSA,multi-headselfattention;PTE,part-aware\ntransformerencoder;Re-ID,re-identiﬁcation;SFE,scale-awarefeature\nenhancement.\nAvailabilityofdataandmaterials\nThedatasetsanalyzedduringthecurrentstudyhavebeenpubliclyreleased\nandareavailablefromthecorrespondingauthoruponreasonablerequest.\nCodeavailability\nThecodeisreleasedat https://github.com/AI-Zhpp/CMT.\nDeclarations\nCompetinginterests\nTheauthorsdeclarenocompetinginterests.\nAuthorcontributions\nAllauthorscontributedtothestudyconceptionanddesign.Material\npreparation,datacollectionandanalysiswereperformedbyPY,XLandPZ.The\nﬁrstdraftofthemanuscriptwaswrittenbyPYandallauthorscommentedon\npreviousversionsofthemanuscript.Allauthorsreadandapprovedtheﬁnal\nmanuscript.\nAuthordetails\n1SchoolofArtiﬁcialIntelligence,DalianUniversityofTechnology,Dalian,\n116024,China. 2SchoolofInformationandCommunicationEngineering,\nDalianUniversityofTechnology,Dalian,116024,China.\nReceived:17April2023 Revised:7September2023\nAccepted:11September2023\nReferences\n1. Loy,C.C.,Xiang,T.,&Gong,S.(2009).Multi-cameraactivitycorrelation\nanalysis.In ProceedingsoftheIEEEconferenceoncomputervisionandpattern\nrecognition(pp.1988–1995).Piscataway:IEEE.\n2. Wang,X.(2013).Intelligentmulti-cameravideosurveillance:areview.\nPatternRecognitionLetters ,34,3–19.\n3. Zheng,L.,Yang,Y.,&Tian,Q.(2017).SiftmeetsCNN:adecadesurveyof\ninstanceretrieval. IEEETransactionsonPatternAnalysisandMachine\nIntelligence,40(5),1224–1244.\n4. Zheng,L.,Yang,Y.,&Hauptmann,A.G.(2016).Personre-identiﬁcation:\npast,presentandfuture.Preprint. arXiv:1610.02984.\n5. Farenzena,M.,Bazzani,L.,Perina,A.,Murino,V.,&Cristani,M.(2010).Person\nre-identiﬁcationbysymmetry-drivenaccumulationoflocalfeatures.In\nProceedingsoftheIEEEconferenceoncomputervisionandpattern\nrecognition(pp.2360–2367).Piscataway:IEEE.\n6. Liu,C.,Gong,S.,Loy,C.C.,&Lin,X.(2012).Personre-identiﬁcation:what\nfeaturesareimportant?InA.Fusiello,V.Murino,&R.Cucchiara(Eds.),\nProceedingsofthe12thEuropeanconferenceoncomputervision (pp.\n391–401).Cham:Springer.\n7. Shi,Z.,Hospedales,T.M.,&Xiang,T.(2015).Transferringasemantic\nrepresentationforpersonre-identiﬁcationandsearch.In Proceedingsofthe\nIEEEconferenceoncomputervisionandpatternrecognition (pp.4184–4193).\nPiscataway:IEEE.\n8. Sun,Y.,Zheng,L.,Yang,Y.,Tian,Q.,&Wang,S.(2018).Beyondpartmodels:\npersonretrievalwithreﬁnedpartpooling(andastrongconvolutional\nYan et al.VisualIntelligence            (2023) 1:24 Page11of12\nbaseline).InF.Manhardt,W.Kehl,N.Navab,etal.(Eds.), Proceedingsofthe\n15thEuropeanconferenceoncomputervision (pp.480–496).Cham:\nSpringer.\n9. Zhou,K.,Yang,Y.,Cavallaro,A.,&Xiang,T.(2019).Omni-scalefeature\nlearningforpersonre-identiﬁcation.In 2019IEEEinternationalconference\noncomputervision (pp.3702–3712).Piscataway:IEEE.\n10. Wang,G.,Yuan,Y.,Chen,X.,Li,J.,&Zhou,X.(2018).Learningdiscriminative\nfeatureswithmultiplegranularitiesforpersonre-identiﬁcation.InS.Boll,K.\nM.Lee,J.Luo,etal.(Eds.), Proceedingsofthe26thACMinternational\nconferenceonmultimedia (pp.274–282).NewYork:ACM.\n11. Chen,T.,Ding,S.,Xie,J.,Yuan,Y.,Chen,W.,Yang,Y.,etal.(2019).ABD-net:\nattentivebutdiversepersonre-identiﬁcation.In 2019IEEEinternational\nconferenceoncomputervision (pp.8351–8361).Piscataway:IEEE.\n12. Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,\netal.(2017).Attentionisallyouneed.Preprint. arXiv:1706.03762.\n13. Zhu,K.,Guo,H.,Zhang,S.,Wang,Y.,Huang,G.,Qiao,H.,etal.(2021).\nAaformer:auto-alignedtransformerforpersonre-identiﬁcation.Preprint.\narXiv:2104.00921.\n14. He,S.,Luo,H.,Wang,P.,Wang,F.,Li,H.,&Jiang,W.(2021).TransReID:\ntransformer-basedobjectre-identiﬁcation.In 2021IEEEinternational\nconferenceoncomputervision (pp.15013–15022).Piscataway:IEEE.\n15. Zheng,F.,Deng,C.,Sun,X.,Jiang,X.,Guo,X.,Yu,Z.,etal.(2019).Pyramidal\npersonre-identiﬁcationviamulti-lossdynamictraining.In Proceedingsof\ntheIEEEconferenceoncomputervisionandpatternrecognition (pp.\n8514–8522).Piscataway:IEEE.\n16. Yang,Q.,Yu,H.-X.,Wu,A.,&Zheng,W.-S.(2019).Patch-baseddiscriminative\nfeaturelearningforunsupervisedpersonre-identiﬁcation.In Proceedings\noftheIEEEconferenceoncomputervisionandpatternrecognition (pp.\n3633–3642).Piscataway:IEEE.\n17. Cho,Y.,Kim,W.J.,Hong,S.,&Yoon,S.E.(2022).Part-basedpseudolabel\nreﬁnementforunsupervisedpersonre-identiﬁcation.In Proceedingsofthe\nIEEEconferenceoncomputervisionandpatternrecognition (pp.7308–7318).\nPiscataway:IEEE.\n18. Chen,B.,Deng,W.,&Hu,J.(2019).In 2019IEEEinternationalconferenceon\ncomputervision (pp.371–381).Piscataway:IEEE.\n19. Rao,Y.,Chen,G.,Lu,J.,&Zhou,J.(2021).Counterfactualattentionlearning\nforﬁne-grainedvisualcategorizationandre-identiﬁcation.In 2021IEEE\ninternationalconferenceoncomputervision (pp.1025–1034).Piscataway:\nIEEE.\n20. Chen,G.,Gu,T.,Lu,J.,Bao,J.-A.,&Zhou,J.(2021).Personre-identiﬁcationvia\nattentionpyramid. IEEETransactionsonImageProcessing ,30,7663–7676.\n21. Zhang,Z.,Lan,C.,Zeng,W.,Jin,X.,&Chen,Z.(2020).Relation-awareglobal\nattentionforpersonre-identiﬁcation.In ProceedingsoftheIEEEconference\noncomputervisionandpatternrecognition (pp.3186–3195).Piscataway:\nIEEE.\n22. Li,W.,Zhu,X.,&Gong,S.(2018).Harmoniousattentionnetworkforperson\nre-identiﬁcation.In ProceedingsoftheIEEEconferenceoncomputervision\nandpatternrecognition (pp.2285–2294).Piscataway:IEEE.\n23. Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,Zhai,X.,\nUnterthiner,T.,etal.(2020).Animageisworth16x16words:transformers\nforimagerecognitionatscale.In Proceedingsofthe10thinternational\nconferenceonlearningrepresentations (pp.1–13).RetrievedAugust25,\n2023,from https://openreview.net/pdf?id=YicbFdNTTy.\n24. Lai,S.,Chai,Z.,&Wei,X.(2021).Transformermeetspartmodel:adaptive\npartdivisionforpersonre-identiﬁcation.In 2019IEEEinternational\nconferenceoncomputervision (pp.4150–4157).Piscataway:IEEE.\n25. Li,Y.,He,J.,Zhang,T.,Liu,X.,Zhang,Y.D.,&Wu,F.(2021).Diversepart\ndiscovery:occludedpersonre-identiﬁcationwithpart-awaretransformer.\nInProceedingsoftheIEEEconferenceoncomputervisionandpattern\nrecognition(pp.2898–2907).Piscataway:IEEE.\n26. Liao,S.,&Shao,L.(2021).Transmatcher:deepimagematchingthrough\ntransformersforgeneralizablepersonre-identiﬁcation.InM.Ranzato,A.\nBeygelzimer,Y.Dauphin,etal.(Eds.), Advancesinneuralinformation\nprocessingsystems34 (pp.1992–2003).RedHook:CurranAssociates.\n27. Wang,G.,Chen,X.,Gao,J.,Zhou,X.,&Ge,S.(2021).Self-guidedbodypart\nalignmentwithrelationtransformersforoccludedpersonre-identiﬁcation.\nIEEESignalProcessingLetters ,28,1155–1159.\n28. Chen,X.,Xu,J.,Xu,J.,&Gao,S.(2021).OH-Former:omni-relational\nhigh-ordertransformerforpersonre-identiﬁcation.Preprint.\narXiv:2109.11159.\n29. Ma,Z.,Zhao,Y.,&Li,J.(2021).Pose-guidedinter-andintra-partrelational\ntransformerforoccludedpersonre-identiﬁcation.InH.T.Shen,Y.Zhuang,\nJ.R.Smith,etal.(Eds.), Proceedingsofthe29thACMinternationalconference\nonmultimedia (pp.1487–1496).NewYork:ACM.\n30. Liu,X.,Zhang,P.,Yu,C.,Lu,H.,Qian,X.,&Yang,X.(2021).Avideoisworth\nthreeviews:trigeminaltransformersforvideo-basedperson\nre-identiﬁcation.Preprint. arXiv:2104.01745.\n31. He,K.,Zhang,X.,Ren,S.,&Sun,J.(2016).Deepresiduallearningforimage\nrecognition.In ProceedingsoftheIEEEconferenceoncomputervisionand\npatternrecognition (pp.770–778).Piscataway:IEEE.\n32. Yu,F.,&Koltun,V.(2015).Multi-scalecontextaggregationbydilated\nconvolutions.Preprint. arXiv:1511.07122.\n33. Zhang,G.,Zhang,P.,Qi,J.,&Lu,H.(2021).HAT:hierarchicalaggregation\ntransformersforpersonre-identiﬁcation.InH.T.Shen,Y.Zhuang,J.R.\nSmith,etal.(Eds.), Proceedingsofthe29thACMinternationalconferenceon\nmultimedia(pp.516–525).NewYork:ACM.\n34. Zhang,P.,Wang,D.,Lu,H.,Wang,H.,&Ruan,X.(2017).Amulet:aggregating\nmulti-levelconvolutionalfeaturesforsalientobjectdetection.In 2017IEEE\ninternationalconferenceoncomputervision (pp.202–211).Piscataway:IEEE.\n35. Szegedy,C.,Vanhoucke,V.,Ioﬀe,S.,Shlens,J.,&Wojna,Z.(2016).Rethinking\ntheinceptionarchitectureforcomputervision.In ProceedingsoftheIEEE\nconferenceoncomputervisionandpatternrecognition (pp.2818–2826).\nPiscataway:IEEE.\n36. Hermans,A.,Beyer,L.,&Leibe,B.(2017).Indefenseofthetripletlossfor\npersonre-identiﬁcation.Preprint. arXiv:1703.07737.\n37. Zheng,L.,Shen,L.,Tian,L.,Wang,S.,Wang,J.D.,&Tian,Q.(2015).Scalable\npersonre-identiﬁcation:abenchmark.In 2015IEEEinternationalconference\noncomputervision (pp.1116–1124).Piscataway:IEEE.\n38. Zheng,Z.,Zheng,L.,&Yang,Y.(2017).Unlabeledsamplesgeneratedby\nGANimprovethepersonre-identiﬁcationbaselineinvitro.In 2017IEEE\ninternationalconferenceoncomputervision (pp.3754–3762).Piscataway:\nIEEE.\n39. Li,W.,Zhao,R.,Xiao,T.,&Wang,X.(2014).Deepreid:deepﬁlterpairing\nneuralnetworkforpersonre-identiﬁcation.In ProceedingsoftheIEEE\nconferenceoncomputervisionandpatternrecognition (pp.152–159).\nPiscataway:IEEE.\n40. Wei,L.,Zhang,S.,Gao,W.,&Tian,Q.(2018).PersontransferGANtobridge\ndomaingapforpersonre-identiﬁcation.In ProceedingsoftheIEEE\nconferenceoncomputervisionandpatternrecognition (pp.79–88).\nPiscataway:IEEE.\n41. Zhong,Z.,Zheng,L.,Kang,G.,Li,S.,&Yang,Y.(2020).Randomerasingdata\naugmentation.In Proceedingsofthe34thAAAIconferenceonartiﬁcial\nintelligence(pp.13001–13008).PaloAlto:AAAIPress.\n42. Kingma,D.P.,&Ba,J.(2014).Adam:amethodforstochasticoptimization.\nPreprint.arXiv:1412.6980.\n43. Chen,X.,Fu,C.,Zhao,Y.,Zheng,F.,Song,J.,Ji,R.,etal.(2020).\nSalience-guidedcascadedsuppressionnetworkforperson\nre-identiﬁcation.In ProceedingsoftheIEEEconferenceoncomputervision\nandpatternrecognition (pp.3300–3310).Piscataway:IEEE.\n44. Si,J.,Zhang,H.,Li,C.-G.,Kuen,J.,Kong,X.,Kot,A.C.,etal.(2018).Dual\nattentionmatchingnetworkforcontext-awarefeaturesequencebased\npersonre-identiﬁcation.In ProceedingsoftheIEEEconferenceoncomputer\nvisionandpatternrecognition (pp.5363–5372).Piscataway:IEEE.\n45. Wang,C.,Zhang,Q.,Huang,C.,Liu,W.,&Wang,X.(2018).Mancs:a\nmulti-taskattentionalnetworkwithcurriculumsamplingforperson\nre-identiﬁcation.InF.Manhardt,W.Kehl,N.Navab,etal.(Eds.),\nProceedings\nofthe15thEuropeanconferenceoncomputervision (pp.365–381).Cham:\nSpringer.\n46. Hou,R.,Ma,B.,Chang,H.,Gu,X.,Shan,S.,&Chen,X.(2019).\nInteraction-and-aggregationnetworkforpersonre-identiﬁcation.In\nProceedingsoftheIEEEconferenceoncomputervisionandpattern\nrecognition(pp.9317–9326).Piscataway:IEEE.\n47. Luo,H.,Gu,Y.,Liao,X.,Lai,S.,&Jiang,W.(2019).Bagoftricksandastrong\nbaselinefordeeppersonre-identiﬁcation.In ProceedingsoftheIEEE\nconferenceoncomputervisionandpatternrecognition (pp.1487–1495).\nPiscataway:IEEE.\n48. Kalayeh,M.M.,Basaran,E.,Gökmen,M.,Kamasak,M.E.,&Shah,M.(2018).\nHumansemanticparsingforpersonre-identiﬁcation.In Proceedingsofthe\nIEEEconferenceoncomputervisionandpatternrecognition (pp.1062–1071).\nPiscataway:IEEE.\n49. Tay,C.-P.,Roy,S.,&Yap,K.-H.(2019).AANet:attributeattentionnetworkfor\npersonre-identiﬁcations.In ProceedingsoftheIEEEconferenceoncomputer\nvisionandpatternrecognition (pp.7134–7143).Piscataway:IEEE.\nYan et al.VisualIntelligence            (2023) 1:24 Page12of12\n50. Zheng,M.,Karanam,S.,Wu,Z.,&Radke,R.J.(2019).Re-identiﬁcationwith\nconsistentattentiveSiamesenetworks.In ProceedingsoftheIEEEconference\noncomputervisionandpatternrecognition (pp.5735–5744).Piscataway:\nIEEE.\n51. Yang,W.,Huang,H.,Zhang,Z.,Chen,X.,Huang,K.,&Zhang,S.(2019).\nTowardsrichfeaturediscoverywithclassactivationmapsaugmentation\nforpersonre-identiﬁcation.In ProceedingsoftheIEEEconferenceon\ncomputervisionandpatternrecognition (pp.1389–1398).Piscataway:IEEE.\n52. Fang,P.,Zhou,J.,Roy,S.K.,Petersson,L.,&Harandi,M.(2019).Bilinear\nattentionnetworksforpersonretrieval.In 2019IEEEinternational\nconferenceoncomputervision (pp.8030–8039).Piscataway:IEEE.\n53. Dai,Z.,Chen,M.,Gu,X.,Zhu,S.,&Tan,P.(2019).Batchdropblocknetwork\nforpersonre-identiﬁcationandbeyond.In ProceedingsoftheIEEE\nconferenceoncomputervisionandpatternrecognition (pp.3691–3701).\nPiscataway:IEEE.\n54. Zheng,Z.,Yang,X.,Yu,Z.,Zheng,L.,Yang,Y.,&Kautz,J.(2019).Joint\ndiscriminativeandgenerativelearningforpersonre-identiﬁcation.In\nProceedingsoftheIEEEconferenceoncomputervisionandpattern\nrecognition(pp.2138–2147).Piscataway:IEEE.\n55. Jin,X.,Lan,C.,Zeng,W.,Chen,Z.,&Zhang,L.(2020).Stylenormalization\nandrestitutionforgeneralizablepersonre-identiﬁcation.In Proceedingsof\ntheIEEEconferenceoncomputervisionandpatternrecognition (pp.\n3143–3152).Piscataway:IEEE.\n56. Zhu,K.,Guo,H.,Liu,Z.,Tang,M.,&Wang,J.(2020).Identity-guidedhuman\nsemanticparsingforpersonre-identiﬁcation.Preprint. arXiv:2007.13467.\n57. Xu,B.,He,L.,Liao,X.,Liu,W.,Sun,Z.,&Mei,T.(2020).BlackRe-ID:a\nhead-shoulderdescriptorforthechallengingproblemofperson\nre-identiﬁcation.InC.W.Chen,R.Cucchiara,X.-S.Hua,etal.(Eds.),\nProceedingsofthe28thACMinternationalconferenceonmultimedia (pp.\n673–681).NewYork:ACM.\n58. Li,H.,Wu,G.,&Zheng,W.-S.(2021).Combineddepthspacebased\narchitecturesearchforpersonre-identiﬁcation.Preprint. arXiv:2104.04163.\n59. vanderMaaten,L.,&Hinton,G.(2008).Visualizingdatausingt-SNE.\nJournalofMachineLearningResearch ,9,2579–2605.\nPublisher’sNote\nSpringerNatureremainsneutralwithregardtojurisdictionalclaimsin\npublishedmapsandinstitutionalaﬃliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8004968166351318
    },
    {
      "name": "Transformer",
      "score": 0.7632877230644226
    },
    {
      "name": "Discriminative model",
      "score": 0.7373985052108765
    },
    {
      "name": "Convolutional neural network",
      "score": 0.7195042967796326
    },
    {
      "name": "Encoder",
      "score": 0.674519419670105
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6281305551528931
    },
    {
      "name": "Feature learning",
      "score": 0.5181697607040405
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4823895990848541
    },
    {
      "name": "Deep learning",
      "score": 0.4712415337562561
    },
    {
      "name": "Machine learning",
      "score": 0.4325759708881378
    },
    {
      "name": "Feature extraction",
      "score": 0.413360595703125
    },
    {
      "name": "Engineering",
      "score": 0.09782096743583679
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I27357992",
      "name": "Dalian University of Technology",
      "country": "CN"
    }
  ],
  "cited_by": 34
}