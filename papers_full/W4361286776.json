{
  "title": "MRET: Multi-resolution transformer for video quality assessment",
  "url": "https://openalex.org/W4361286776",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3092077092",
      "name": "Junjie Ke",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2103500744",
      "name": "Tianhao Zhang",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2120844825",
      "name": "Yilin Wang",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A735359232",
      "name": "Peyman Milanfar",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2097009727",
      "name": "Feng Yang",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3092077092",
      "name": "Junjie Ke",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2103500744",
      "name": "Tianhao Zhang",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2120844825",
      "name": "Yilin Wang",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A735359232",
      "name": "Peyman Milanfar",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2097009727",
      "name": "Feng Yang",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3035422918",
    "https://openalex.org/W6793119350",
    "https://openalex.org/W6776048684",
    "https://openalex.org/W6790307280",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W6786585107",
    "https://openalex.org/W3016028173",
    "https://openalex.org/W6676297131",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W6736872674",
    "https://openalex.org/W3191492288",
    "https://openalex.org/W6955071965",
    "https://openalex.org/W6800329793",
    "https://openalex.org/W6771626834",
    "https://openalex.org/W2950154603",
    "https://openalex.org/W2965644659",
    "https://openalex.org/W2368744241",
    "https://openalex.org/W6797423152",
    "https://openalex.org/W6792155083",
    "https://openalex.org/W6797737728",
    "https://openalex.org/W1982471090",
    "https://openalex.org/W2194363988",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2048042940",
    "https://openalex.org/W2791258091",
    "https://openalex.org/W6767402692",
    "https://openalex.org/W3206887033",
    "https://openalex.org/W3030701471",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6797845720",
    "https://openalex.org/W6786674354",
    "https://openalex.org/W6771843191",
    "https://openalex.org/W6767885762",
    "https://openalex.org/W3206968390",
    "https://openalex.org/W2970478035",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2619947201",
    "https://openalex.org/W2970678735",
    "https://openalex.org/W2612690371",
    "https://openalex.org/W3126721948",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W4292826048",
    "https://openalex.org/W3099047215",
    "https://openalex.org/W2611434713",
    "https://openalex.org/W4295838474"
  ],
  "abstract": "No-reference video quality assessment (NR-VQA) for user generated content (UGC) is crucial for understanding and improving visual experience. Unlike video recognition tasks, VQA tasks are sensitive to changes in input resolution. Since large amounts of UGC videos nowadays are 720p or above, the fixed and relatively small input used in conventional NR-VQA methods results in missing high-frequency details for many videos. In this paper, we propose a novel Transformer-based NR-VQA framework that preserves the high-resolution quality information. With the multi-resolution input representation and a novel multi-resolution patch sampling mechanism, our method enables a comprehensive view of both the global video composition and local high-resolution details. The proposed approach can effectively aggregate quality information across different granularities in spatial and temporal dimensions, making the model robust to input resolution variations. Our method achieves state-of-the-art performance on large-scale UGC VQA datasets LSVQ and LSVQ-1080p, and on KoNViD-1k and LIVE-VQC without fine-tuning.",
  "full_text": "MRET: Multi-resolution\ntransformer for video quality\nassessment\nJunjie Ke1*, Tianhao Zhang2, Yilin Wang2, Peyman Milanfar1 and\nFeng Yang1\n1Google Research, Mountain View, CA, United States,2Google, Mountain View, CA, United States\nNo-reference video quality assessment (NR-VQA) for user generated content\n(UGC) is crucial for understanding and improving visual experience. Unlike video\nrecognition tasks, VQA tasks are sensitive to changes in input resolution. Since\nlarge amounts of UGC videos nowadays are 720p or above, theﬁxed and relatively\nsmall input used in conventional NR-VQA methods results in missing high-\nfrequency details for many videos. In this paper, we propose a novel\nTransformer-based NR-VQA framework that preserves the high-resolution\nquality information. With the multi-resolution input representation and a novel\nmulti-resolution patch sampling mechanism, our method enables a\ncomprehensive view of both the global video composition and local high-\nresolution details. The proposed approach can effectively aggregate quality\ninformation across different granularities in spatial and temporal dimensions,\nmaking the model robust to input resolution variations. Our method achieves\nstate-of-the-art performance on large-scale UGC VQA datasets LSVQ and LSVQ-\n1080p, and on KoNViD-1k and LIVE-VQC withoutﬁne-tuning.\nKEYWORDS\nvideo quality assessment, transformer, no-reference, multi-resolution, user-generated\ncontent\n1 Introduction\nVideo quality assessment (VQA) has been an important research topic in the past years\nfor understanding and improving perceptual quality of videos. Conventional VQA methods\nmainly focus on full reference (FR) scenarios where distorted videos are compared against\ntheir corresponding pristine reference. In recent years, there has been an explosion of user\ngenerated content (UGC) videos on social media platforms such as Facebook, Instagram,\nYouTube, and TikTok. For most UGC videos, the high-quality pristine reference is\ninaccessible. This results in a growing demand for no-reference (NR) VQA models,\nwhich can be used for ranking, recommending and optimizing UGC videos.\nMany NR-VQA models (Li et al., 2019; You and Korhonen, 2019; Tu et al., 2021; Wang\net al., 2021; Ying et al., 2021) have achieved signiﬁcant success by leveraging the power of\ndeep-learning. Most existing deep-learning approaches use convolutional neural networks\n(CNNs) to extract frozen frame-level features and then aggregate them in the temporal\ndomain to predict the video quality. Since frozen frame-level features are not optimized for\ncapturing spatial-temporal distortions, this could be insufﬁcient to catch diverse spatial or\ntemporal impairments in UGC videos. Moreover, predicting UGC video quality often\ninvolves long-range spatial-temporal dependencies, such as fast-moving objects or rapid\nzoom-in views. Since convolutional kernels in CNNs are speciﬁcally designed for capturing\nOPEN ACCESS\nEDITED BY\nChang-Wen Chen,\nHong Kong Polytechnic University, Hong\nKong SAR, China\nREVIEWED BY\nTiesong Zhao,\nFuzhou University, China\nKe Gu,\nBeijing University of Technology, China\nEkrem Çetinkaya,\nUniversity of Klagenfurt, Austria\n*CORRESPONDENCE\nJunjie Ke,\njunjiek@google.com\nSPECIALTY SECTION\nThis article was submitted\nto Image Processing,\na section of the journal\nFrontiers in Signal Processing\nRECEIVED 03 January 2023\nACCEPTED 13 March 2023\nPUBLISHED 29 March 2023\nCITATION\nKe J, Zhang T, Wang Y, Milanfar P and\nYang F (2023), MRET: Multi-resolution\ntransformer for video quality assessment.\nFront. Sig. Proc.3:1137006.\ndoi: 10.3389/frsip.2023.1137006\nCOPYRIGHT\n© 2023 Ke, Zhang, Wang, Milanfar and\nYang. This is an open-access article\ndistributed under the terms of the\nCreative Commons Attribution License\n(CC BY). The use, distribution or\nreproduction in other forums is\npermitted, provided the original author(s)\nand the copyright owner(s) are credited\nand that the original publication in this\njournal is cited, in accordance with\naccepted academic practice. No use,\ndistribution or reproduction is permitted\nwhich does not comply with these terms.\nFrontiers inSignal Processing frontiersin.org01\nTYPE Original Research\nPUBLISHED 29 March 2023\nDOI 10.3389/frsip.2023.1137006\nshort-range spatial-temporal information, they cannot capture\ndependencies that extend beyond the receptive ﬁeld (Bertasius\net al., 2021). This limits CNN models’ ability to model complex\nspatial-temporal dependencies in UGC VQA tasks, and therefore it\nmay not be the best choice to effectively aggregate complex quality\ninformation in diverse UGC videos.\nRecently, architectures based on Transformer (Vaswani et al.,\n2017) have been proven to be successful for various vision tasks\n(Carion et al., 2020; Arnab et al., 2021; Chen et al., 2021; Dosovitskiy\net al., 2021), including image quality assessment (Ke et al., 2021).\nUnlike CNN models that are constrained by limited receptiveﬁelds,\nTransformers utilize the multi-head self-attention operation which\nattends over all elements in the input sequence. As a result,\nTransformers can capture both local and global long-range\ndependencies by directly comparing video quality features at all\nspace-time locations. This inspires us to apply Transformer on VQA\nin order to effectively model the complex space-time distortions in\nUGC videos.\nDespite the bene ﬁts of Transformers, directly applying\nTransformers on VQA is challenging because VQA tasks are\nresolution-sensitive. Video recognition models like ViViT (Arnab\net al., 2021) useﬁxed and relatively small input size,e.g., 224 × 224.\nThis is problematic for VQA since UGC videos with resolution\nsmaller than 224 are very rare nowadays [less than 1% in LSVQ\n(Ying et al., 2021)]. Such downsampling leads to missing high-\nfrequency details for many videos. As shown inFigure 1, some\nvisible artifacts in the high resolution video are not obvious when the\nvideo is downsampled. Human perceived video quality is affected by\nboth the global video composition,e.g., content, video structure and\nsmoothness and local details,e.g., texture and distortion artifacts.\nBut it is hard to capture both global and local quality information\nwhen using ﬁxed resolution inputs. Similarly for image quality\nassessment, Ke et al. (2021) showed the beneﬁt of applying the\nTransformer architecture on the image at the original resolution.\nAlthough processing the original high-resolution input is affordable\nfor a single image, it is computationally infeasible for videos, due to\nTransformer’s quadratic memory and time complexity.\nTo enable high-resolution views in video Transformers for a\nmore effective VQA model, we propose to leverage the\ncomplementary nature of low and high resolution frames. We\nuse the low-resolution frames for a complete global composition\nview, and sample spatially aligned patches from the high-resolution\nframes to complement the high-frequency local details. The\nproposed Multi-REsolution Transformer (MRET) can therefore\nefﬁciently extract and encode the multi-scale quality information\nfrom the input video. This enables more effective aggregation of\nboth global composition and local details of the video to better\npredict the perceptual video quality.\nAs illustrated inFigure 2,w eﬁrst group the neighboring frames\nto build a multi-resolution representation composed of lower-\nresolution frames and higher-resolution frames. We then\nintroduce a novel and effective multi-resolution patch sampling\nmechanism to sample spatially aligned patches from the multi-\nresolution frame input. These multi-resolution patches capture both\nthe global view and local details at the same location, and they serve\nas the multi-resolution input for the video Transformer. In addition\nto preserving high-resolution details, our proposed MRET model\nalso aligns the input videos at different resolutions, making the\nmodel more robust to resolution variations. After the multi-\nresolution tokens are extracted, a factorized spatial and temporal\nencoder is employed to efﬁciently process the large number of\nspatial-temporal tokens.\nThe major contributions of this paper are summarized into three\nfolds.\n We propose a multi-resolution Transformer for video quality\nassessment (MRET), which makes it possible to preserve high-\nresolution quality information for UGC VQA.\nFIGURE 1\nVideo quality is affected by both global video composition and\nlocal details. Although downsampled video frames provide the global\nview and are easier to process for deep-learning models, some\ndistortions visible on the original high resolution videos may\ndisappear when resized to a lower resolution.\nFIGURE 2\nThe proposed multi-resolution Transformer (MRET) for VQA. To\ncapture both global composition and local details of video quality, we\nbuild a multi-scale video representation with patches sampled from\nproportionally resized frames with different resolutions.\nFrontiers inSignal Processing frontiersin.org02\nKe et al. 10.3389/frsip.2023.1137006\n We propose a novel multi-resolution patch sampling\nmechanism, enabling the Transformer to efﬁciently process\nboth global composition information and local high-\nresolution details.\n We apply MRET on large-scale UGC VQA datasets. It\noutperforms the previous state-of-the-art methods on\nLSVQ (Ying et al., 2021 ) and LSVQ-1080p ( Ying et al.,\n2021). It also achieves state-of-the-art performance on\nKoNViD-1k (Hosu et al., 2017) and LIVE-VQC (Sinno and\nBovik, 2018 ) without ﬁne-tuning, demonstrating its\nrobustness and generalization capability.\n2 Related work\nVideo Quality Assessment:Video quality assessment aims to\nquantify video quality. FR-VQA methods measure quality changes\nfrom pristine videos, and NR-VQA methods measure video quality\nwithout a pristine reference. For UGC videos that lack high-quality\npristine reference, NR-VQA metrics are more applicable.\nConventional NR metrics (Saad et al., 2014; Mittal et al., 2015; Li\net al., 2016; Korhonen, 2019; Sinno and Bovik, 2019; Dendi and\nChannappayya, 2020; Tu et al., 2021) utilize distortion-speciﬁc\nfeatures and low-level features like natural scene statistics (NSS).\nThese feature-based NR-VQA methods mainly rely on hand-crafted\nstatistical features summarized from limited data and are harder to\ngeneralize to diversiﬁed UGC videos. In the past few years, CNN-\nbased NR metrics (Li et al., 2019; You and Korhonen, 2019; Wang\net al., 2021; Ying et al., 2021) achieve great success in VQA using\nfeatures extracted with CNNs. The features are then aggregated\ntemporally with pooling layers or recurrent units like LSTM. The\nPVQ (Ying et al., 2021) method learns to model the relationship\nbetween local video patches and the global original UGC video. It\nshows that exploiting both global and local information can be\nbeneﬁcial for VQA. Recent CNN-Transformer hybrid methods\n(Jiang et al., 2021; Li et al., 2021; Tan et al., 2021; You, 2021)\nshow the beneﬁt of using Transformer for temporal aggregation on\nCNN-based frame-level features. Since all these methods use CNN\nfor spatial feature extraction, they suffer from CNN’s limitation,i.e.,\na relatively small spatial receptiveﬁeld. Moreover, these frame-level\nfeatures are usually extracted from eitherﬁxed size inputs or a frozen\nbackbone without VQA optimization. Our method is a pure\nTransformer-based VQA model and can be optimized end-to-\nend. Unlike models that use ﬁxed small input, our proposed\nMRET model enables high-resolution inputs. The proposed\nmulti-resolution input representation allows the model to have a\nfull spatial receptiveﬁeld across multiple scales.\nVision Transformers:The Transformer (Vaswani et al., 2017)\narchitecture wasﬁrst proposed for NLP tasks and has recently been\nadopted for various computer vision tasks (Carion et al., 2020;\nArnab et al., 2021; Chen et al., 2021; Dosovitskiy et al., 2021; Ke et al.,\n2021). The Vision Transformer (ViT) (Dosovitskiy et al., 2021) ﬁrst\nproposes to classify an image by treating it as a sequence of patches.\nThis seminal work has inspired subsequent research to adopt\nTransformer-based architectures for other vision tasks. For video\nrecognition, ViViT (Arnab et al., 2021) examines four designs of\nspatial and temporal attention for the pretrained ViT model.\nTimeSformer ( Bertasius et al., 2021 ) studies ﬁve different\nspace-time attention methods and shows that a factorized space-\ntime attention provides better speed-accuracy tradeoff. Video Swin\nTransformer ( Liu et al., 2022 ) extends the local attention\ncomputation of Swin Transformer (Liu et al., 2021) to temporal\ndimension, and it achieves state-of-the-art accuracy on a broad\nrange of video recognition benchmarks such as Kinetics-400 (Kay\net al., 2017 ) and Kinetics-600 ( Kay et al., 2017 ). Since video\nrecognition tasks are less sensitive to input resolution than VQA,\nmost of the video Transformers proposed for video recognition tasks\nuse relatively small resolution andﬁxed square input,e.g., 224 × 224.\nThe objective for the VQA task is sensitive to both global\ncomposition and local details, and it motivates us to enable video\nTransformers to process frames in a multi-resolution manner,\ncapturing both global and local quality information.\n3 Multi-resolution transformer for\nvideo quality assessment\n3.1 Overall architecture\nUnderstanding the quality of UGC videos is hard because they\nare captured under very different conditions like unstable cameras,\nimperfect camera lens, varying resolutions and frame rates, different\nalgorithms and parameters for processing and compression. As a\nresult, UGC videos usually contain a mixture of spatial and temporal\ndistortions. Moreover, the way viewers perceive the content and\ndistortions also impact the perceptual quality of the video.\nSometimes transient distortions such as sudden glitches and\ndefocusing can signiﬁcantly impact the overall perceived quality,\nwhich makes the problem even more complicated. As a result, both\nglobal video composition and local details are important for\naccessing the quality of UGC videos.\nTo capture video quality at different granularities, we propose a\nmulti-resolution Transformer (MRET) for VQA which embeds\nvideo clips as multi-resolution patch tokens as shown in\nFigure 3. MRET is comprised of two major parts, namely, 1) a\nmulti-resolution video embedding module (Section 3.2), and 2) a\nspace-time factorized Transformer encoding module (Section 3.3).\nThe multi-resolution video embedding module aims to encode\nthe multi-scale quality information in the video, capturing both\nglobal video composition from lower resolution frames, and local\ndetails from higher resolution frames. The space-time factorized\nTransformer encoding module aggregates the spatial and temporal\nquality from the multi-scale embedding input.\n3.2 Multi-resolution video representation\nSince UGC videos are highly diverse, we need to design an\neffective multi-resolution video representation for capturing the\ncomplex global and local quality information. To achieve that, we\nﬁrst transform the input video into groups of multi-resolution\nframes. As shown inFigure 3, the input frames are divided into\ngroups ofN. N is the number of scales in the multi-resolution input.\nWe then resize theN frames into a pyramid of low-resolution and\nhigh-resolution frames. We preserve the aspect ratios of the frames\nduring resizing, and we control the shorter-side length for each\nFrontiers inSignal Processing frontiersin.org03\nKe et al. 10.3389/frsip.2023.1137006\nframe (Figure 4). Assuming the shorter-side length for the largest\nresolution isL, the resulting pyramid of frames will have shorter-side\nlength L, ... , 2L\nN, L\nN accordingly. As a result, we will have a pyramid of\nN frames, scaling from 1× to1\nN × resolution.\nAfter obtaining the multi-resolution frames, we need a way to\neffectively and efﬁciently encode them a s input tokens to the\nTransformer. Although low-resolution frames can be processed\nefﬁciently, processing the high-resolution frames in its entirety can\nbe computationally expensive. For the higher-resolution frames, we\npropose to sample patches instead to save computation. Intuitively, the\nlower-resolution frames provide global views of the video composition,\nwhile the higher-resolution ones provide complementary local details.\nWe want a patch sampling meth od that can best utilize the\ncomplementary nature of these multi-scale views. To achieve that,\nwe propose to sample spatially aligned grids of patches from the\ngrouped multi-resolution frames. In short, we use the lowest\nr e s o l u t i o nf r a m ef o rac o m p l e t eg l o b a lv i e w ,a n dw es a m p l el o c a l\npatches at the same location from the higher-resolution frames to\nprovide the multi-scale local details. Since the patches are spatially\naligned, the Transformer has accessto both the global view and local\ndetails at the same location. This allows it to better utilize the\ncomplementary multi-scale information for learning video quality.\nFigures 4, 5 demonstrate how we sample spatially aligned grids\nof patches. Firstly, we choose a frame center, as shown by the red\ntriangle in Figure 4. During training, the frame center is chosen\nrandomly along the middle line for the longer-length side. For\ninference, we use the center of the video input. After aligning the\nframes, we then sample center-aligned patches from the frames.P is\nthe patch size. For the smallest frame, we continuously sample the\ngrid of patches to capture the complete global view. For larger\nframes, we sample linearly spaced-out patches to provide multi-scale\nlocal details. The center for the patches remain aligned at the same\nlocation, as shown by the yellow triangles inFigure 4. For theith\nframe (i =1 ,... , N), the distance between patches can be calculated\nas (N − i)× P. Since the patches are center-aligned, they form a\n“tube” of multi-resolution patches for the same location. As a result,\nthose multi-resolution patches provide a gradual“zoom-out” view,\ncapturing both the local details and global view at the same location.\nAs shown inFigure 5, we then linearly project each tube of multi-\nresolution patchx\ni to a 1D tokenzi ∈ Rd using learned matrixE where\nd is the dimension of the Transformer input tokens. This can be\nimplemented using a 3D convolution with kernel sizeN × P × P.E a c h\nembedded token contains multi-resolution patches at the same location,\nallowing the model to utilize both global and local spatial quality\ninformation. Moreover, the multi-scale patches also fuse local spatio-\ntemporal information together during tokenization. Therefore, it\nprovides a comprehensive representation for the input video.\n3.3 Factorized spatial temporal transformer\nAs shown inFigure 3, after extracting the multi-resolution frame\nembedding, we apply a factorization of spatial and temporal\nTransformer encoders in series to ef ﬁciently encode the\nFIGURE 3\nModel overview for MRET. Neighboring video frames are grouped and rescaled into a pyramid of low-resolution and high-resolution frames.\nPatches are sampled from the multi-resolution frames and encoded as the Transformer input tokens. The spatial Transformer encoder takes the multi-\nresolution tokens to produce a representation per frame group at its time step. The temporal Transformer encoder then aggregates across time steps. To\npredict the video quality score, we follow a common strategy in Transformers to prepend a“classiﬁcation token” (z\ncls and hcls) to the sequence to\nrepresent the whole sequence input and to use its output as theﬁnal representation.\nFIGURE 4\nMulti-resolution patch sampling. Weﬁrst rescale theN frames to\nL, ... , 2L\nN , L\nN for the shorter side and uniformly sample grid of patches\nfrom the multi-resolution frames.P is the patch size. Patches are\nspatially aligned. The patches at the same location in the grid\nprovide a multi-scale view for the same location.\nFrontiers inSignal Processing frontiersin.org04\nKe et al. 10.3389/frsip.2023.1137006\nspace-time quality information. Firstly, the spatial Transformer\nencoder takes the tokens from each frame group to produce a\nlatent representation per frame group. It serves as the\nrepresentation at this time step. Secondly, the temporal\nTransformer encoder models temporal interaction by aggregating\nthe information across time steps.\n3.3.1 Spatial transformer encoder\nThe spatial Transformer encoder aggregates the multi-\nresolution patches extracted from the entire frame group to a\nrepresentation h\nt ∈ Rd at its time step wheret =1 , ... , T is the\ntemporal index for the frame group. T is the number of frame\ngroups. As mentioned in the previous section, for multi-resolution\npatches xi from each frame group, we project it to a sequence of\nmulti-resolution tokens as zi ∈ Rd, i =1 , ... , M using learnable\nmatrix E where M is the total number of patches. We follow the\nstandard approach of prepending an extra learnable“classiﬁcation\ntoken” (zcls ∈ Rd) (Devlin et al., 2019; Dosovitskiy et al., 2021) and\nuse its representation at theﬁnal encoder layer as theﬁnal spatial\nrepresentation for the frame group. Additionally, a learnable spatial\npositional embedding p ∈ R\nM×d is added element-wisely to the\ninput tokens zi to encode spatial position. The tokens are passed\nthrough a Transformer encoder withK layers. Each layerk consists\nof multi-head self-attention (MSA), layer normalization (LN), and\nmultilayer perceptron (MLP) blocks. The spatial Transformer\nencoder is formulated as:\nz\n0 /equals zcls, Ex1, Ex2, ... , ExM[] + p (1)\nzk′ /equals MSA LN zk−1()() + zk−1,k /equals 1/K (2)\nzk /equals MLP LN zk′()() + zk′,k /equals 1/K (3)\nht /equals LN z0\nK() (4)\n3.3.2 Temporal transformer encoder\nThe temporal Transformer encoder models the interactions\nbetween tokens from different time steps. We use thezcls token\nposition output from the spatial Transformer encoder as the frame\ngroup level representation. As shown inFigure 3, each group of\nframes will be encoded as a single tokenht, t =1 , ... , T. We then\nprepend a hcls ∈ Rd token and add a separate learnable temporal\npositional embedding pt ∈ RT×d to the temporal tokens. These\ntokens are then fed to the temporal Transformer encoder, which\nmodels the temporal interactions across time. The output at theh\ncls\ntoken is used as the ﬁnal representation for the whole video.\nSimilarly, the temporal Transformer encoder can be formulated as:\nh0 /equals hcls,h 1,h 2, ... ,h T[] + pt (5)\nhq′ /equals MSA LN hq−1()() + hq−1,q /equals 1/Q (6)\nhq /equals MLP LN hq′()() + hq′,q /equals 1/Q (7)\nv /equals LN h0\nQ() (8)\nQ is the number of layers for the temporal Transformer encoder.v is\noutput from the hcls token position from the temporal encoder,\nwhich is used as theﬁnal video representation.\n3.4 Video quality prediction\nTo predict theﬁnal quality score, we add an MLP layer on top of the\nﬁnal video representationv. The output of the MLP layer is regressed to\nthe video mean opinion score (MOS) label associated with each video in\nVQA datasets. The model istrained end-to-end withL\n2 loss.\n3.5 Initialization from pretrained models\nVision Transformers have been shown to be only effective when\ntrained on large-scale datasets (Arnab et al., 2021; Dosovitskiy et al.,\n2021) as they lack the inductive biases of 2D image structures, which\nneeds to be imposed during pretraining. However, existing video\nquality datasets are several magnitudes smaller than large-scale\nimage classi ﬁcation datasets, such as ILSVRC-2012 ImageNet\n(Russakovsky et al., 2015) (we refer to it as ImageNet in what\nfollows) and ImageNet-21k (Deng et al., 2009). As a result, training\nTransformer models from scratch using VQA datasets is extremely\nchallenging and impractical. We therefore also choose to initialize\nthe Transformer backbone from pretrained image models.\nFIGURE 5\nMulti-resolution video frames embedding. We extract center-aligned multi-resolution patches, and then linearly project the spatially aligned“tubes”\nof patches to 1D tokens.\nFrontiers inSignal Processing frontiersin.org05\nKe et al. 10.3389/frsip.2023.1137006\nUnlike the 3D video input, the image Transformer models only\nneed 2D projection for the input data. To initialize the 3D\nconvolutional ﬁlter E from 2D ﬁlters E\nimage in pretrained image\nmodels, we adopt the“central frame initialization strategy” used in\nViViT (Arnab et al., 2021). In short,E is initialized with zeros along\nall temporal positions, except at the center⌊N/2⌋. The initialization\nof E from pretrained image model can therefore be formulated as:\nE /equals 0, ... , Eimage, ... , 0[] (9)\n4 Experimental results\n4.1 Datasets\nWe run experiments on four UGC VQA datasets, including\nLSVQ (Ying et al., 2021), LSVQ-1080p (Ying et al., 2021), KoNViD-\n1k (Hosu et al., 2017), and LIVE-VQC (Sinno and Bovik, 2018).\nLSVQ (excluding LSVQ-1080p) consists of 38,811 UGC videos and\n116,433 space-time localized video patches. The original and patch\nvideos are all annotated with MOS scores in [0.0, 100.0], and it\ncontains videos of diverse resolutions. LSVQ-1080p contains\n3,573 videos with 1080p resolution or higher. Since our model\ndoes not make a distinction between original videos and\nvideo patches, we use all the 28.1k videos and 84.3k video\npatches from the LSVQ training split to train the model and\nevaluate the model on full-size videos from the testing splits of\nLSVQ and LSVQ-1080p. KoNViD-1k contains 1,200 videos with\nMOS scores in [0.0, 5.0] and 960p ﬁxed resolution. LIVE-VQC\ncontains 585 videos with MOS scores in [0.0, 100.0] and video\nresolution from 240p to 1080p. We use KoNViD-1k and LIVE-VQC\nfor evaluating the generalization ability of our model withoutﬁne-\ntuning. Since no training is involved, we use the entire dataset for\nevaluation.\n4.2 Implementation details\nWe set the number of multi-resolution frames in each group to\nN = 4. The shorter-side lengthL is set to 896 for the largest frame in\nthe frame group. Correspondingly, the group of frames are rescaled\nwith shorter-side length 896, 672, 448, and 224. We use patch size\np = 16 when generating the multi-resolution frame patches. For each\nframe, we sample a 14 × 14 grid of patches. Unless otherwise\nspeciﬁed, the input to our network is a video clip of 128 frames\nuniformly sampled from the video.\nThe hidden dimension for Transformer input tokens is set tod =\n768. For the spatial Transformer, we use the ViT-Base (Dosovitskiy\net al., 2021) model (12 Transformer layers with 12 heads and\n3072 MLP size), and we initialize it from the checkpoint trained\non ImageNet-21K ( Deng et al., 2009 ). For the temporal\nTransformer, we use 8 layers with 12 heads, and 3072 MLP size.\nThe ﬁnal model has 144M parameters and 577 GFLOPs.\nWe train the models with the synchronous SGD momentum\noptimizer, a cosine decay learning rate schedule from 0.3 and a batch\nsize of 256 for 10 epochs in total. All the models are trained on\nTPUv3 hardware. Spearman rank ordered correlation (SRCC) and\nPearson linear correlation (PLCC) are reported as performance\nmetrics.\n4.3 Comparison with the state-of-the-art\n4.3.1 Results on LSVQ and LSVQ-1080p\nTable 1 shows the results on full-size LSVQ and LSVQ-1080p\ndatasets. Our proposed MRET outperforms other methods by large\nTABLE 1 Results on full-size videos in LSVQ and LSVQ-1080p test sets. Blue and\nblack numbers in bold represent the best and second best respectively. We\ntake numbers from (Ying et al., 2021) for the results of the reference methods.\nOur ﬁnal method is marked in gray.\nLSVQ LSVQ -1080p\nModels SRCC PLCC SRCC PLCC\nBRISQUE Mittal et al. (2012) 0.576 0.576 0.497 0.531\nTLVQM Korhonen, (2019) 0.772 0.774 0.589 0.616\nVIDEVAL Tu et al. (2021) 0.794 0.783 0.545 0.554\nVSFA Li et al. (2019) 0.801 0.796 0.675 0.704\nPVQ Ying et al. (2021) 0.827 0.828 0.711 0.739\nMRET (Ours) 0.867 0 .865 0 .780 0 .817\nTABLE 2 Performance on KoNViD-1k and LIVE-VQC. Methods except LSCT-\nPHIQNet (You, 2021)i n“w/o Fine-tune” group are trained on LSVQ. Blue and\nblack numbers in bold represent the best and second best respectively. We\ntake numbers from (Ying et al., 2021; Jiang et al., 2021; You, 2021; Tan et al.,\n2021; Liao et al., 2022) for the results of the reference methods. Ourﬁnal\nmethod is marked in gray.\nLIVE-VQC KoNViD-1k\nModels SRCC PLCC SRCC PLCC\nw/Fine-\ntune\nTan et al. (2021) 0.760 0.795 0.798 0.797\nJiang et al. (2021) 0.776 0.789 0.789 0.788\nLSCT-PHIQNet\nYou (2021)\n-- 0.85 0 .86\nTPQI (Liao et al.,\n2022)\n0.718 0.730 0.693 0.693\nw/o Fine-\ntune\nBRISQUE Mittal et al.\n(2012)\n0.524 0.536 0.646 0.647\nTLVQM Korhonen\n(2019)\n0.670 0.691 0.732 0.724\nVIDEVAL Tu et al.\n(2021)\n0.630 0.640 0.751 0.741\nVSFA Li et al. (2019) 0.734 0.772 0.784 0.794\nPVQ Ying et al. (2021) 0.770 0 .807 0.791 0.795\nLSCT-PHIQNet\nYou (2021)\n0.737 0.762 - -\nMRET (Ours) 0.776 0 .817 0.846 0.854\nFrontiers inSignal Processing frontiersin.org06\nKe et al. 10.3389/frsip.2023.1137006\nm a r g i n so nb o t hd a t a s e t s .N o t a b l y ,o nthe higher resolution test dataset\nLSVQ-1080p, our model is able to outperform the strongest baseline by\n7.8% for PLCC (from 0.739 to 0.817). This shows that for high-\nresolution videos, the proposed multi-resolution Transformer is able\nto better aggregate local and global quality information for a more\naccurate video quality prediction.\n4.3.2 Performance on cross dataset\nSince existing VQA datasets are magnitudes smaller than popular\nimage classiﬁcation datasets, VQA models are prone to overﬁtting.\nTherefore, it is of great interest to obtain a VQA model that can\ngeneralize across datasets. To verify the generalization capability of\nMRET, we conduct a cross-dataset evaluation where we train the\nmodel using LSVQ training set and separately eval on LIVE-VQC and\nKoNViD-1k withoutﬁne-tuning. As shown inTable 2,M R E Ti sa b l et o\ngeneralize very well to both datasets, and it performs the best among\nmethodswithoutﬁne-tuning. Moreover, its performance is even as good\na st h eb e s to n e st h a ta r eﬁne-tuned on the target dataset. This demonstrates\nthe strong generalization capability of MRET. Intuitively, the proposed\nmulti-resolution input aligns the videos at different resolutions. Not only\ndoes it provide a more comprehensive view of the video quality, but it also\nmakes the model more robust to resolution variations. As a result, MRET\ncan learn to capture quality information for UGC videos under different\nconditions.\n4.4 Ablation studies\n4.4.1 Spatial temporal quality attention\nTo understand how MRET aggregates spatio-temporal\ninformation to predict the ﬁnal video quality, we visualize the\nattention weights on spatial and temporal tokens using Attention\nRollout (Abnar and Zuidema, 2020). In short, we average the\nattention weights of the Transformer across all heads and then\nrecursively multiply the weight matrices of all layers. Figure 6\nvisualizes temporal attention for each input time step and spatial\nattention for selected frames. As shown by temporal attention for the\nvideo, the model is paying more attention to the second section\nwhen the duck is moving rapidly across the grass. The spatial\nattention also shows that the model is focusing on the main\nsubject, i.e., duck in this case. This veriﬁes that MRET is able to\ncapture spatio-temporal quality information and utilize it to predict\nthe video quality.\n4.4.2 Effectiveness of multi-resolution frame\ninputs\nTo verify the effectiveness of the proposed multi-resolution\ninput representation, we run ablations by not using the multi-\nresolution input. The comparison result is shown inTable 3 as\n“MRET” and “w/o Multi-resolution” for with and without the multi-\nresolution frames respectively. For MRET, we resize the frames to\n[896, 672, 448, 224] for shorter-side lengths. For the method“w/o\nMulti-resolution”, we resize all the frames in the frame group to the\nsame shorter-side length (224). The GFLOPs is the same for both\nmodels because the patch size and number of patches are the same.\nThe multi-resolution frame input brings1%–2% boost in SRCC on\nLSVQ and 2%–3% boost in SRCC on LSVQ-1080p. The gain is\nlarger on LSVQ-1080p because the dataset contains more high-\nresolution videos, and therefore more quality information is lost\nwhen resized statically to a small resolution. Armed with the multi-\nresolution input representation, MRET is able to utilize both global\ninformation from lower-resolution frames and detailed information\nFIGURE 6\nVisualization of spatial and temporal attention from output tokens to the input. The heat-map on the top shows the spatial attention. The chart on\nthe bottom shows the temporal attention. Higher attention values correspond to the more important video segments and spatial regions for prediction.\nThe model is focusing on spatially and temporally more meaningful content when predicting theﬁnal video quality score.\nFrontiers inSignal Processing frontiersin.org07\nKe et al. 10.3389/frsip.2023.1137006\nfrom higher-resolution frames. The results demonstrate that the\nproposed multi-resolution representation is indeed effective for\ncapturing the complex multi-scale quality information that can\nbe lost when using statically resized frames.Table 3 also shows\nthat MRET performance improves with the increase of number of\ninput frames since more temporal information is preserved.\nAfter verifying that the multi-resolution representation is indeed\nmore effective than ﬁxed resolution, we also run ablations with\ndifferent multi-resolution patch sampling methods (Table 4). For\n“Random”,w eﬁrst resize the frames to the 4-scale multi-resolution\ninput, and then randomly sample the same number of patches from\neach resolution. For“High-res Patch on Last Frame”, we use low-\nresolution patches for theﬁrst 3 frames (224×), and only sample\nhigh-resolution patches from the last frame (896×). MRET samples\ncenter-aligned patches from the 4-scale input, and it performs the\nbest. This shows the proposed sampling method can more effectively\nutilize the complementary nature of the multi-resolution views.\nWith the center-aligned multi-resolution patches, MRET is able\nto better aggregate both the global view, and the multi-resolution\nlocal details.\n4.4.3 Number of grouped multi-resolution\nframes N\nIn Table 5we run ablations on the number of grouped framesN\nwhen building the multi-resolution video representation. The\nexperiment is run with 60 frames instead of 128 since smallerN\nincreases the number of input tokens for the temporal encoder and\nintroduces high computation and memory cost. For MRET, we use\nmulti-resolution input for the grouped frames and for“w/o Multi-\nresolution”, we resize all the frames to the same 224 shorter-side\nlength. For allN, using multi-resolution input is better than aﬁxed\nresolution. It further veriﬁes the effectiveness of the proposed multi-\nresolution input structure. For multi-resolution input, the\nperformance improves when increasing N from 2 to 5, but the\ngain becomes smaller asN grows larger. There is also a trade-off\nbetween getting higher resolution views and the loss of spatio-\ntemporal information with the increase ofN, since the area ratio of\nsampled patches becomes smaller as resolution increases Overall, we\nﬁnd N = 4 to be a good balance between performance and\ncomplexity.\n4.4.4 Pretrained checkpoint selection\nCompared to CNNs, Transformers impose less restrictive\ninductive biases which broadens their representation ability. On\nthe other hand, since Transformers lack the inductive biases of the\n2D image structure, it generally needs large datasets for pretraining\nto learn the inductive priors. InTable 6, we try initializing the spatial\nTransformer encoder in MRET model with checkpoints pretrained\non different image datasets, including two image classi ﬁcation\n(Class.) datasets, and one image quality assessment (IQA)\ndataset. ImageNet-21k is the largest and it performs the best,\nshowing that large-scale pretraining is indeed bene ﬁcial. This\nconforms with theﬁndings in previous vision Transformer works\n(Arnab et al., 2021; Dosovitskiy et al., 2021). LIVE-FB (Ying et al.,\n2020) is an IQA dataset on which PVQ (Ying et al., 2021) obtain\ntheir 2D frozen features. Since IQA is a very relevant task to VQA,\npretraining on this relatively small IQA dataset leads to superior\nTABLE 3 Ablation study results for multi-resolution input on LSVQ and LSVQ-1080p dataset. MRET uses multi-resolution input while“w/o Multi-resolution” uses\nﬁxed-resolution frames. Both models grouped the frames byN =4 when encoding video frames into tokens. Blue and black numbers in bold represent the best and\nsecond best respectively on the same dataset. Ourﬁnal method is marked in gray.\nLSVQ LSVQ -1080p\nMRET w/o Multi -resolution MRET w/o Multi -resolution\n# Frames GFLOPs SRCC PLCC SRCC PLCC SRCC PLCC SRCC PLCC\n32 144 0.844 0.841 0.828 0.828 0.749 0.788 0.726 0.759\n64 289 0.857 0.854 0.845 0.845 0.768 0.807 0.737 0.784\n96 433 0.862 0.860 0.851 0.851 0.776 0.813 0.754 0.771\n128 577\n0.867 0 .865 0.852 0.851 0.780 0 .817 0.749 0.782\nTABLE 4 Ablation for multi-resolution patch sampling method. Ourﬁnal\nmethod is marked in gray.\nLSVQ LSVQ -1080p\nPatch Sampling Method SRCC PLCC SRCC PLCC\nRandom 0.839 0.838 0.739 0.783\nHigh-res Patch on Last Frame 0.854 0.855 0.757 0.801\nMRET 0.867 0.865 0.780 0.817\nTABLE 5 Ablation study results for number of grouped framesN on the LSVQ-\n1080p dataset. MRET uses multi-resolution input while“w/o Multi-resolution”\nuse ﬁxed resolution frames. Models here are trained with 60 input frames\ninstead of 128.\nMRET W/o Multi -\nresolution\nN GFLOPs SRCC PLCC SRCC PLCC\n2 534 0.751 0.797 0.742 0.786\n3 358 0.757 0.794 0.741 0.786\n4 271 0.764 0.802 0.749 0.787\n5 218 0.764 0.805 0.743 0.783\nFrontiers inSignal Processing frontiersin.org08\nKe et al. 10.3389/frsip.2023.1137006\nresults than ImageNet. This shows that relevant task pretraining is\nbeneﬁcial when large-scale pretraining is not accessible.\n4.4.5 Frame sampling strategy\nWe run ablations on the frame sampling strategy inTable 7. For\nour default “Uniform Sample”, we sample 128 frames uniformly\nthroughout the video. For “Front Sample”, we sample the ﬁrst\n128 frames. For “Center Clip ” we take the center clip of\n128 frames from the video. On LSVQ and LSVQ-1080p dataset,\nuniformly sampling the frames is the best probably because there is\ntemporal redundancy between continuous frames and uniformly\nsampling the frames allows the model to see more diverse video\nclips. Since most of the videos in the VQA dataset are relatively\nshort, uniformly sampling the frames is good enough to provide a\ncomprehensive view.\n5 Conclusion and future work\nWe propose a multi-resolution Transformer (MRET) for\nVQA, which integrates multi-resolution views to capture both\nglobal and local quality information. By transforming the input\nframes to a multi-resolution representation with both low and\nhigh resolution frames, the model is able to capture video\nquality information at differen t granularities. To effectively\nhandle the variety of resolutions in the multi-resolution input\nsequence, we propose a multi-resolution patch sampling\nmechanism. A factorization of spatial and temporal\nTransformers is employed to ef ﬁciently model spatial and\ntemporal information and capture complex space-time\ndistortions in UGC videos. Experiments on several large-\nscale UGC VQA datasets show that MRET can achieve state-\nof-the-art performance and has strong generalization\ncapability, demonstrating th e effectiveness of the proposed\nmethod. MRET is designed for VQA, and it can be extended\nto other scenarios where the task labels are affected by both\nvideo global composition and local details. The limitation of\nTransformers is that it can be computationally expensive, and\nthus costly to make predictions on long videos. In this paper,\nwe focus on improving the performance of the VQA model and\nw el e a v ei ta sf u t u r ew o r kt oi m p r o v ei t se fﬁciency and to lower\nthe computation cost. One potential direction is to use more\nefﬁcient Transformer variants, such as Reformer (Kitaev et al.,\n2020 ) and Longformer (Beltagy et al., 2020) where the attention\ncomplexity has been greatly reduced. Those ef ﬁcient\nTransformers can be adopted as a drop-in replacement for\nthe current spatial and the temporal Transformer used\nin MRET.\nData availability statement\nThe original contributions presented in the study are included in\nthe article/Supplementary Material, further inquiries can be directed\nto the corresponding author.\nAuthor contributions\nThe authors conﬁrm contribution to the paper as follows: design\nand implementation: JK, TZ, and FY; draft manuscript preparation:\nJK, TZ, YW, and FY. All authors contributed to the article and\napproved the submitted version.\nConﬂict of interest\nJK, PM, and FY were employed by the Google Research. TZ and\nYW were employed by the Google.\nPublisher’s note\nAll claims expressed in this article are solely those of the authors\nand do not necessarily represent those of their af ﬁliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nTABLE 6 Results for initializing MRET model from checkpoints pretrained on different image datasests. Ourﬁnal method is marked in gray.\nLSVQ LSVQ -1080p\nPretrain Dataset #Images Task SRCC PLCC SRCC PLCC\nImageNet Russakovsky et al. (2015) 1M Class 0.839 0.837 0.748 0.780\nImageNet-21k Deng et al. (2009) 14M Class 0.867 0.865 0.780 0.817\nLIVE-FB Ying et al. (2020) 160K IQA 0.848 0.846 0.760 0.788\nTABLE 7 Ablation study results for frame sampling method. Ourﬁnal method is\nmarked in gray.\nLSVQ LSVQ -1080p\nFrame Sampling Method SRCC PLCC SRCC PLCC\nUniform Sample 0.867 0.865 0.780 0.817\nFront Sample 0.860 0.857 0.773 0.808\nCenter Clip 0.860 0.857 0.771 0.811\nFrontiers inSignal Processing frontiersin.org09\nKe et al. 10.3389/frsip.2023.1137006\nReferences\nAbnar, S., and Zuidema, W. (2020). Quantifying attentionﬂow in transformers.arXiv\npreprint arXiv:2005.00928.\nArnab, A., Dehghani, M., Heigold, G., Sun, C., Lučić, M., and Schmid, C. (2021).\n“Vivit: A video vision transformer,” in Proceedings of the IEEE/CVF international\nconference on computer vision(IEEE), 6836– 6846.\nBeltagy, I., Peters, M. E., and Cohan, A. (2020). Longformer: The long-document\ntransformer. arXiv preprint arXiv:2004.05150.\nBertasius, G., Wang, H., and Torresani, L. (2021). Is space-time attention all you need\nfor video understanding?Int. Conf. Mach. Learn. (ICML)2, 4.\nCarion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko, S.\n(2020). “End-to-end object detection with transformers,” in European conference on\ncomputer vision. Glasgow, UK (Springer International Publishing).\nChen, H., Wang, Y., Guo, T., Xu, C., Deng, Y., Liu, Z., et al. (2021).“Pre-trained image\nprocessing transformer,” in Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition. Nashville, TN (IEEE/CVF), 12299– 12310.\nDendi, S. V. R., and Channappayya, S. S. (2020). No-reference video quality\nassessment using natural spatiotemporal scene statistics.IEEE Trans. Image Process.\n29, 5612– 5624. doi:10.1109/tip.2020.2984879\nDeng, J., Dong, W., Socher, R., Li, L. J., Li, K., and Fei-Fei, L. (2009).“Imagenet: A\nlarge-scale hierarchical image database,” in 2009 IEEE Conference on Computer Vision\nand Pattern Recognition, Miami, FL, USA, 20-25 June 2009 (IEEE), 248– 255.\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. (2019).“Bert: Pre-training of deep\nbidirectional transformers for language understanding,” in Proceedings of the 2019 conference of\nthe north American chapter of the association for computational linguistics: Human language\ntechnologies, NAACL-HLT 2019, minneapolis, MN, USA, june 2-7, 2019, volume 1 (long and\nshort papers). Editors J. Burstein, C. Doran, and T. Solorio. Minneapolis, Minnesota\n(Association for Computational Linguistics), 4171– 4186. doi:10.18653/v1/n19-1423\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T.,\net al. (2021).“An image is worth 16x16 words: Transformers for image recognition at\nscale,” in International conference on learning representations. Virtual Event (ICLR).\nHosu, V., Hahn, F., Jenadeleh, M., Lin, H., Men, H., Szirányi, T., et al. (2017).“The\nkonstanz natural video database (konvid-1k),” in 2017 Ninth International Conference\non Quality of Multimedia Experience (QoMEX), Erfurt, Germany, 31 May 2017 -\n02 June 2017 (IEEE), 1– 6.\nJiang, J., Wang, X., Li, B., Tian, M., and Yao, H. (2021). Multi-dimensional feature\nfusion network for no-reference quality assessment of in-the-wild videos.Sensors 21,\n5322. doi:10.3390/s21165322\nKay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., et al.\n(2017). The kinetics human action video dataset.arXiv preprint arXiv:1705.06950.\nKe, J., Wang, Q., Wang, Y., Milanfar, P., and Yang, F. (2021).“Musiq: Multi-scale\nimage quality transformer,” in Proceedings of the IEEE/CVF international conference on\ncomputer vision. Nashville, TN (IEEE/CVF), 5148– 5157.\nKitaev, N., Kaiser,Ł., and Levskaya, A. (2020). Reformer: The efﬁcient transformer.\narXiv preprint arXiv:2001.04451.\nKorhonen, J. (2019). Two-level approach for no-reference consumer video quality\nassessment. IEEE Trans. Image Process.28, 5923– 5938. doi:10.1109/tip.2019.2923051\nL i ,D . ,J i a n g ,T . ,a n dJ i a n g ,M .( 2 0 1 9 ) .“Quality assessment of in-the-wild videos,” in\nProceedings of the 27th ACM international conference on multimedia. New York (Association\nfor Computing Machinery), 2351– 2359.\nLi, X., Guo, Q., and Lu, X. (2016). Spatiotemporal statistics for video quality\nassessment. IEEE Trans. Image Process.25, 3329– 3342. doi:10.1109/tip.2016.2568752\nLi, Y., Feng, L., Xu, J., Zhang, T., Liao, Y., and Li, J. (2021).“Full-reference and no-\nreference quality assessment for compressed user-generated content videos, ” in\n2021 IEEE international conference on multimedia and expo workshops (ICMEW)\n(IEEE), 1– 6.\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., et al. (2021).“Swin transformer:\nHierarchical vision transformer using shifted windows,” in Proceedings of the IEEE/CVF\ninternational conference on computer vision. Nashville, TN (IEEE/CVF), 10012– 10022.\nLiu, Z., Ning, J., Cao, Y., Wei, Y., Zhang, Z., Lin, S., et al. (2022).“Video swin\ntransformer,” in Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition. IEEE/CVF (IEEE/CVF), 3202– 3211.\nMittal, A., Moorthy, A. K., and Bovik, A. C. (2012). No-reference image quality\nassessment in the spatial domain.IEEE Trans. Image Process.21, 4695– 4708. doi:10.\n1109/tip.2012.2214050\nMittal, A., Saad, M. A., and Bovik, A. C. (2015). A completely blind video integrity\noracle. IEEE Trans. Image Process.25, 289– 300. doi:10.1109/tip.2015.2502725\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., et al. (2015).\nImageNet large scale visual recognition challenge. Int. J. Comput. Vis. (IJCV) 115,\n211– 252. doi:10.1007/s11263-015-0816-y\nSaad, M. A., Bovik, A. C., and Charrier, C. (2014). Blind prediction of natural video\nquality. IEEE Trans. Image Process.23, 1352– 1365. doi:10.1109/tip.2014.2299154\nSinno, Z., and Bovik, A. C. (2018). Large-scale study of perceptual video quality.IEEE\nTrans. Image Process.28, 612– 627. doi:10.1109/tip.2018.2869673\nSinno, Z., and Bovik, A. C. (2019).“Spatio-temporal measures of naturalness,” in\n2019 IEEE International Conference on Image Processing (ICIP), Taipei, Taiwan, 22-\n25 September 2019 (IEEE), 1750– 1754.\nTan, Y., Kong, G., Duan, X., Wu, Y., and Long, H. (2021). No-reference video quality\nassessment for user generated content based on deep network and visual perception.\nJ. Electron. Imaging30, 053026. doi:10.1117/1.jei.30.5.053026\nTu, Z., Wang, Y., Birkbeck, N., Adsumilli, B., and Bovik, A. C. (2021). Ugc-vqa:\nBenchmarking blind video quality assessment for user generated content.IEEE Trans.\nImage Process. 30, 4449– 4464. doi:10.1109/tip.2021.3072221\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). “Attention is all you need,” in Advances in neural information processing\nsystems. Long Beach, CA (Curran Associates Inc), 5998– 6008.\nWang, Y., Ke, J., Talebi, H., Yim, J. G., Birkbeck, N., Adsumilli, B., et al. (2021).“Rich\nfeatures for perceptual quality assessment of ugc videos,” in Proceedings of the IEEE/\nCVF conference on computer vision and pattern recognition. Nashville, TN (IEEE/CVF),\n13435– 13444.\nYing, Z., Mandal, M., Ghadiyaram, D., and Bovik, A. (2021).“Patch-vq:’patching\nup’the video quality problem,” in Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition. Nashville, TN (IEEE/CVF), 14019– 14029.\nYing, Z., Niu, H., Gupta, P., Mahajan, D., Ghadiyaram, D., and Bovik, A. (2020).\n“From patches to pictures (paq-2-piq): Mapping the perceptual space of picture\nquality,” in Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition. Seattle, WA (IEEE/CVF), 3575– 3585.\nYou, J., and Korhonen, J. (2019). “Deep neural networks for no-reference video\nquality assessment,” in 2019 IEEE International Conference on Image Processing\n(ICIP), Taipei, Taiwan, 22-25 September 2019 (IEEE), 2349– 2353.\nYou, J. (2021).“Long short-term convolutional transformer for no-reference video\nquality assessment,” in Proceedings of the 29th ACM international conference on\nmultimedia. Virtual Event, China (Association for Computing Machinery), 2112– 2120.\nFrontiers inSignal Processing frontiersin.org10\nKe et al. 10.3389/frsip.2023.1137006",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7933703064918518
    },
    {
      "name": "Transformer",
      "score": 0.5864416360855103
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49630290269851685
    },
    {
      "name": "Image resolution",
      "score": 0.47148606181144714
    },
    {
      "name": "Computer vision",
      "score": 0.42586636543273926
    },
    {
      "name": "High resolution",
      "score": 0.41772955656051636
    },
    {
      "name": "Data mining",
      "score": 0.38118335604667664
    },
    {
      "name": "Remote sensing",
      "score": 0.10982128977775574
    },
    {
      "name": "Voltage",
      "score": 0.06633234024047852
    },
    {
      "name": "Engineering",
      "score": 0.06627744436264038
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ],
  "cited_by": 7
}