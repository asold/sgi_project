{
  "title": "Perils and Opportunities in Using Large Language Models in Psychological Research",
  "url": "https://openalex.org/W4389515417",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2145804492",
      "name": "Morteza Dehghani",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A4201768271",
      "name": "Jackson Trager",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A3013656336",
      "name": "Ali Omrani",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A4270989430",
      "name": "Mona J. Xue",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A3101608876",
      "name": "Suhaib Abdurahman",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A3029530119",
      "name": "Preni Golazizian",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2754745577",
      "name": "Farzan Karimi-Malekabadi",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2142197048",
      "name": "Mohammad Atari",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2163000534",
      "name": "Peter S. Park",
      "affiliations": [
        "University of Southern California"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4294959075",
    "https://openalex.org/W3049476187",
    "https://openalex.org/W4214903622",
    "https://openalex.org/W4389989145",
    "https://openalex.org/W2314545043",
    "https://openalex.org/W2043045839",
    "https://openalex.org/W6846260263",
    "https://openalex.org/W2922202159",
    "https://openalex.org/W4327672398",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2788481061",
    "https://openalex.org/W6646014332",
    "https://openalex.org/W6667970269",
    "https://openalex.org/W4381163899",
    "https://openalex.org/W6657177047",
    "https://openalex.org/W6795342411",
    "https://openalex.org/W6777560052",
    "https://openalex.org/W6857745965",
    "https://openalex.org/W6676391964",
    "https://openalex.org/W2111267928",
    "https://openalex.org/W4321074112",
    "https://openalex.org/W6784358232",
    "https://openalex.org/W6639660458",
    "https://openalex.org/W4382240547",
    "https://openalex.org/W6855001726",
    "https://openalex.org/W4296169830",
    "https://openalex.org/W4362510886",
    "https://openalex.org/W6728271715",
    "https://openalex.org/W2606089314",
    "https://openalex.org/W6842813255",
    "https://openalex.org/W4380763235",
    "https://openalex.org/W4285192297",
    "https://openalex.org/W3003385079",
    "https://openalex.org/W3174174150",
    "https://openalex.org/W3008655042",
    "https://openalex.org/W4225165463",
    "https://openalex.org/W6765162019",
    "https://openalex.org/W1641003075",
    "https://openalex.org/W3022133124",
    "https://openalex.org/W6838865847",
    "https://openalex.org/W6683532468",
    "https://openalex.org/W4384261711",
    "https://openalex.org/W4385571890",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W3170390204",
    "https://openalex.org/W4366420437",
    "https://openalex.org/W4294982692",
    "https://openalex.org/W2005151061",
    "https://openalex.org/W6736067386",
    "https://openalex.org/W4375930358",
    "https://openalex.org/W6845398906",
    "https://openalex.org/W4323239061",
    "https://openalex.org/W6759320174",
    "https://openalex.org/W3161588210",
    "https://openalex.org/W1565959449",
    "https://openalex.org/W4385570906",
    "https://openalex.org/W2072385612",
    "https://openalex.org/W6736575291",
    "https://openalex.org/W4224296643",
    "https://openalex.org/W4365512576",
    "https://openalex.org/W3130319171",
    "https://openalex.org/W652269744",
    "https://openalex.org/W6765510851",
    "https://openalex.org/W18965947",
    "https://openalex.org/W1498436455",
    "https://openalex.org/W3034344071",
    "https://openalex.org/W4308760226",
    "https://openalex.org/W6669465837",
    "https://openalex.org/W2147792648",
    "https://openalex.org/W4313197536",
    "https://openalex.org/W6735236233",
    "https://openalex.org/W4366277658",
    "https://openalex.org/W4375853569",
    "https://openalex.org/W4375958655",
    "https://openalex.org/W6679436768",
    "https://openalex.org/W4376612976",
    "https://openalex.org/W2140910804",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2809932687",
    "https://openalex.org/W3008374555",
    "https://openalex.org/W6767060491",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W1985063293",
    "https://openalex.org/W2725671465",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2112902824",
    "https://openalex.org/W6849219588",
    "https://openalex.org/W4322720178",
    "https://openalex.org/W4365139036",
    "https://openalex.org/W3034282334",
    "https://openalex.org/W4235316990",
    "https://openalex.org/W2963119602",
    "https://openalex.org/W2953522645",
    "https://openalex.org/W4377231151",
    "https://openalex.org/W3104119924",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4241566321",
    "https://openalex.org/W4306247398",
    "https://openalex.org/W2975429091",
    "https://openalex.org/W4367623529",
    "https://openalex.org/W4365601249",
    "https://openalex.org/W4377865309",
    "https://openalex.org/W2601450892",
    "https://openalex.org/W4376654357",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4379259169",
    "https://openalex.org/W4254816979",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4382619745",
    "https://openalex.org/W4385571495",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4391090624",
    "https://openalex.org/W4372272969",
    "https://openalex.org/W4306808680",
    "https://openalex.org/W2530872785",
    "https://openalex.org/W4390513007",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W4385966247",
    "https://openalex.org/W4382404016",
    "https://openalex.org/W2242464395",
    "https://openalex.org/W4378508547",
    "https://openalex.org/W4385571830",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4379933116",
    "https://openalex.org/W4246976400",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4288593469",
    "https://openalex.org/W4254436260",
    "https://openalex.org/W4378474285",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4250431948",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W4379255800",
    "https://openalex.org/W4386567020",
    "https://openalex.org/W4233862346",
    "https://openalex.org/W4321524280",
    "https://openalex.org/W4367000100",
    "https://openalex.org/W4385430086",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3160638507",
    "https://openalex.org/W4361806395",
    "https://openalex.org/W2070907364",
    "https://openalex.org/W4361863170",
    "https://openalex.org/W4362700315",
    "https://openalex.org/W4361866126",
    "https://openalex.org/W4383647972",
    "https://openalex.org/W1810943226",
    "https://openalex.org/W4389518978",
    "https://openalex.org/W2963499153",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3095319910",
    "https://openalex.org/W4385734111",
    "https://openalex.org/W4396796749",
    "https://openalex.org/W4286901880",
    "https://openalex.org/W2807329388",
    "https://openalex.org/W4322008875",
    "https://openalex.org/W4231627133",
    "https://openalex.org/W4319083882",
    "https://openalex.org/W4384694026",
    "https://openalex.org/W4235791242",
    "https://openalex.org/W4387389584",
    "https://openalex.org/W4327811957",
    "https://openalex.org/W4291220703",
    "https://openalex.org/W2973379954",
    "https://openalex.org/W4383473944",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W3153094109",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4389523771",
    "https://openalex.org/W4313559133",
    "https://openalex.org/W2897679441",
    "https://openalex.org/W3197782487",
    "https://openalex.org/W3022499311",
    "https://openalex.org/W2489406233",
    "https://openalex.org/W4379925078",
    "https://openalex.org/W4292545669",
    "https://openalex.org/W4363671827",
    "https://openalex.org/W4378770584",
    "https://openalex.org/W4385897789",
    "https://openalex.org/W4386249236",
    "https://openalex.org/W4366733439",
    "https://openalex.org/W4362656192",
    "https://openalex.org/W4385363516",
    "https://openalex.org/W4298050894",
    "https://openalex.org/W4389519325",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W4249645064",
    "https://openalex.org/W4362673335",
    "https://openalex.org/W2511841052",
    "https://openalex.org/W3015368184",
    "https://openalex.org/W4390723916",
    "https://openalex.org/W4301481428",
    "https://openalex.org/W4287207937",
    "https://openalex.org/W4367679822"
  ],
  "abstract": "The emergence of large language models (LLMs) has sparked considerable interest in their potential application in psychological research, either as a human-like entity used as a model for the human psyche or as a general text-analysis tool. However, carelessly using LLMs in psychological studies, a trend we rhetorically refer to as “GPTology,” can have negative consequences, especially given the convenient access to models such as ChatGPT. We elucidate the promises, limitations, and ethical considerations of using LLMs in psychological research. First, LLM-based research should pay attention to the substantial psychological diversity around the globe, as well as demographic diversity within populations. Second, while LLMs are convenient tools, we caution against treating them as a one-size-fits-all method for psychological text analysis. Third, LLM-based psychological research needs to develop methods and standards to compensate for LLMs’ opaque black-box nature to facilitate reproducibility, transparency, and robust inference from AI-generated data. While acknowledging the prospects offered by LLMs for easy task automation (e.g., text annotation) and to expand our understanding of human psychology (e.g., by contrasting human and machine psychology), we make a case for diversifying human samples and expanding psychology’s methodological toolbox to achieve a truly inclusive and generalizable science, rather than homogenizing samples and methods through over-reliance on LLMs.",
  "full_text": "PERILS AND OPPORTUNITIES OF LLMS 1\nPerils and Opportunities in Using Large Language Models in Psychological\nResearch\nSuhaib Abdurahman1,2,*, Mohammad Atari3,*, Farzan Karimi-Malekabadi1,2, Mona J.\nXue3, Jackson Trager1,2, Peter S. Park4, Preni Golazizian2,5, Ali Omrani2,5,M o r t e z a\nDehghani1,2,5\n1Department of Psychology, University of Southern California\n2Brain and Creativity Institute, University of Southern California\n3Department of Human Evolutionary Biology, Harvard University\n4Department of Physics, Massachusetts Institute of Technology\n5Department of Computer Science, University of Southern California\nAuthor Note\n* These authors contributed equally to this work.\nMohammad Atari is now at the Department of Psychological and Brain Sciences,\nUniversity of Massachusetts Amherst. Correspondence concerning this article should be\naddressed to Mohammad Atari, Department of Psychological and Brain Sciences,\nUniversity of Massachusetts Amherst, 135 Hicks Way, Amherst, MA 01003, USA. E-mail:\nmatari@umass.edu\nPERILS AND OPPORTUNITIES OF LLMS 2\nAbstract\nThe emergence of large language models (LLMs) has sparked considerable interest in their\npotential application in psychological research, either as a human-like entity used as a\nmodel for the human psyche or as a general text-analysis tool. However, carelessly using\nLLMs in psychological studies, a trend we rhetorically refer to as “GPTology,” can have\nnegative consequences, especially given the convenient access to models such as ChatGPT.\nWe elucidate the promises, limitations, and ethical considerations of using LLMs in\npsychological research. First, LLM-based research should pay attention to the substantial\npsychological diversity around the globe, as well as demographic diversity within\npopulations. Second, while LLMs are convenient tools, we caution against treating them as\na one-size-ﬁts-all method for psychological text analysis. Third, LLM-based psychological\nresearch needs to develop methods and standards to compensate for LLMs’ opaque\nblack-box nature to facilitate reproducibility, transparency, and robust inference from\nAI-generated data.\nWhile acknowledging the prospects o\u0000ered by LLMs for easy task automation (e.g.,\ntext annotation) and to expand our understanding of human psychology (e.g., by\ncontrasting human and machine psychology), we make a case for diversifying human\nsamples and expanding psychology’s methodological toolbox to achieve a truly inclusive\nand generalizable science, rather than homogenizing samples and methods through\nover-reliance on LLMs.\nKeywords: psychology, large language models, natural language processing,\npsychological diversity, psychological text analysis, reproducibility, transparency\nPERILS AND OPPORTUNITIES OF LLMS 3\nPerils and Opportunities in Using Large Language Models in Psychological\nResearch\nTechnological innovations have enabled social and behavioral scientists to gather\ndiverse forms of data about human psychology, paving the way for signiﬁcant\nbreakthroughs in psychological science and neighboring ﬁelds. These advancements have\nplayed a pivotal role in expanding our understanding of psychological processes. The\ndevelopment of neuroimaging (e.g., fMRI), online survey platforms (e.g., Mechanical Turk),\nand eye-tracking technology are just a few examples that have revolutionized psychological\nresearch in the last few decades. The digital revolution and emergence of “big data”\nfacilitated the establishment of new ﬁelds such as computational social science (Lazer et al.,\n2009). More recently, there has been a notable paradigm shift in Artiﬁcial Intelligence (AI)\nwith the emergence of Large Language Models (LLMs): neural networks characterized by\ntheir deep layers and extensive scale, typically consisting of billions to over a hundred\ntrillion parameters and trained on vast text datasets, enabling an unprecedented ability to\nunderstand, generate, and translate human language with remarkable subtlety and\ncomplexity. These AI models are trained on extensive collections of unlabeled text using\nself-supervised or semi-supervised learning methods, contributing to their remarkable\nlanguage understanding and generation capacity. This new technology has been argued to\npossess the capacity to transform social science research (Grossmann et al., 2023).\nLanguage models have advanced signiﬁcantly through interdisciplinary\ncollaboration, including contributions from psychology, which laid the foundation for\nmodern language modeling. For example, inspired by the psychology of feedback and\nlearning mechanisms, McClelland and Rumelhart (1985) explored connectionist models\ndemonstrating the potential of neural networks, and Rumelhart et al. (1986) introduced\nkey algorithms like backpropagation. Building upon these developments and motivated by\ntheories in working memory and cognitive processes, Elman (1990) introduced Recurrent\nNeural Networks, enabling researchers to model sequential data in language models.\nPERILS AND OPPORTUNITIES OF LLMS 4\nThe availability of large textual corpora, increased computing power, and\nadvancements in deep-learning techniques have recently contributed to the progress and\nreﬁnement of language models. Notably, the Transformers architecture (Vaswani et al.,\n2017), which enables understanding of intricate relationships between di\u0000erent input\ncomponents in a remarkably e\u0000cient and precise manner substantially improved natural\nlanguage understanding and generation capabilities. Subsequently, LLMs, like ChatGPT\n(OpenAI, 2022), based on di\u0000erent versions of Generative Pre-trained Transformer (GPT;\nBrown et al., 2020; Dale, 2021), have had sizable implications in various domains, powering\napplications such as human-sounding chatbots and language translation systems. Their\nsuccess lies in their impressive language generation capabilities and public accessibility,\nwhich has permeated research in various ﬁelds from medicine (Singhal et al., 2023) to\npolitics (Motoki et al., 2023).\nLLM cognition o\u0000ers promising opportunities to gain previously inaccessible insights\ninto cognition as a whole, and perhaps even human cognition (Kalla & Smith, 2023;\nVan Dis et al., 2023). Additionally, these models’ capabilities in text analysis and\ngeneration could possibly be harnessed by researchers as an easy-to-use method for\nanalyzing textual data, such as coding texts for mental health assessment (Kjell et al.,\n2023). There has recently been a rapid string of psychological research output related to\nand facilitated by these models (Bail, 2023; Floridi, 2023; Park et al., 2023; Van Dis et al.,\n2023). In particular, ChatGPT has been employed in a range of social and behavioral\napplications, from hate-speech classiﬁcation (Huang et al., 2023) to sentiment analysis\n(Zhu et al., 2023), often with promising results.\nHowever, drawing parallels with the turbulent integration of previous technological\ninnovations into psychological research suggests that the hurried or negligent\nimplementation of LLMs in psychology could give rise to unintended consequences. For\nexample, when fMRI techniques ﬁrst appeared, some researchers began haphazardly\napplying these techniques, resulting in many nonsensical, but statistically signiﬁcant neural\nPERILS AND OPPORTUNITIES OF LLMS 5\ncorrelates: a phenomenon cleverly illustrated in an fMRI of post-mortem Atlantic Salmon\n(Bennett et al., 2009) and in a study on “Voodoo Correlations” (i.e., a cautionary term\nabout mysteriously high correlations due to misapplication of statistical methods yielding\nmisleading results; Fiedler, 2011). These two episodes now serve as a lesson for psychology\nstudents and researchers more broadly: that new technologies should be cautiously\nintegrated into psychological research. Speciﬁcally, integrating LLMs in the psychological\nresearch pipeline, such as in substitution for human participants, necessitates critically\nexamining its limitations. Researchers should not simply ask themselveshow they can use\na new technology; they need to also ask themselveswhether and why they should do so\n(Crockett & Messeri, 2023).\nBy no means do we uniformly cast doubt on the usefulness of LLMs in psychological\nresearch. Instead, we aim to contextualize current and future opportunities that LLMs may\no\u0000er psychological research, and suggest possible ways to navigate their limitations. While\nacknowledging their potential utility to improve psychological science, we advise caution\nregarding the unchecked application of LLMs, at least in their current state, in\npsychological studies. To prevent issues like the Voodoo Correlations, it is essential to\napproach LLMs with caution, keeping in mind similar challenges the ﬁeld has faced in\nrecent decades (e.g., the credibility revolution; Vazire, 2018). The following section\nprovides an overview of the downsides of the hurried use of LLMs in psychology, and how it\ncould negatively a\u0000ect psychological ﬁndings if not applied critically and cautiously.\nLLMs Should Not Replace Human Participants\nMany studies of state-of-the-art LLMs have concluded that their outputs are highly\n“human-like” (Bang et al., 2023; H. Liu et al., 2023; Webb et al., 2022). For instance,\nWebb et al. (2022) examined the analogical reasoning abilities of ChatGPT and found that\nit had developed an emergent capacity for zero-shot reasoning, allowing it to solve a wide\nrange of analogy problems without explicit training. Some have argued that if LLMs, such\nPERILS AND OPPORTUNITIES OF LLMS 6\nas ChatGPT, can indeed produce human-like responses to common measures in\npsychological science (e.g., judgments of actions, endorsements of values, perceptions of\nsocial issues), they might as well replace the human subject pool. For example, based on a\nsubstantive correlation between moral judgments made by humans and a language model\n(GPT-3.5), Dillion et al. (2023) argue that these models could replace human participants\nin psychological research. With the “AI as human participants” position (which is typically\nsupported by showing an LLM-human correlation in some psychological domain; e.g.,\nDillion et al., 2023; Park et al., 2023), there is much that we ﬁnd misleading, perhaps even\ndamaging to developing a generalizable and inclusive science of the human mind.\nFirst, research such as Dillion et al. (2023) does not pay nearly enough attention to\nthe substantial cross-cultural variation in cognitive processes—including moral\njudgments—around the globe (see Atari, Haidt, et al., 2023a; Barrett et al., 2016). Models\nlike GPT have been trained chieﬂy on WEIRD (Western, Educated, Industrialized, Rich,\nDemocratic; Henrich et al., 2010) people’s textual data primarily in English, perpetuating\nthe English-centricity of psychology (Blasi et al., 2022), hindering e\u0000orts to take linguistic\ndiversity seriously. Hence, these models may struggle with accurately representing diverse\npopulations. For example, ChatGPT has shown gender biases favoring male perspectives\nand narratives (Ghosh & Caliskan, 2023; Wong & Kim, 2023), cultural biases toward\nAmerican perspectives (Cao et al., 2023) or majority populations in general (Chen et al.,\n2022), and political biases favoring liberal, environmental, and left-libertarian viewpoints\n(Hartmann et al., 2023; Motoki et al., 2023; Santurkar et al., 2023). These biases also\nextend to personality, morality, and stereotypes (Bozkurt et al., 2023; Deshpande et al.,\n2023; Rutinowski et al., 2023).\nGenerally, these models’ outputs reﬂect a WEIRD psychology such that the\nAI-human link substantially weakens as we collect “human” data from less WEIRD\npopulations. In other words, the high AI-human correlation does not replicate when the\nhuman sample is less WEIRD (Atari, Xue, et al., 2023). This is especially concerning\nPERILS AND OPPORTUNITIES OF LLMS 7\nbecause the WEIRD-people problem was originally devised as an awareness-raising\nrhetorical device to push researchers away from heavily relying on WEIRD human research\nparticipants (e.g., undergraduate students in North America; see Thalmayer et al., 2021).\nSubstituting human participants with LLM outputs would, therefore, be a step backward.\nIgnoring human diversity in psychology, ampliﬁed by the convenience of online samples like\nMTurk, has already led psychology research to be tunnel-visioned toward an extremely thin\nslice of human diversity. GPTology will constitute a move toward an even more myopic and\nless generalizable discipline.\nSecond, LLMs seem to have a “correct answer” bias (Park et al., 2023; Santurkar\net al., 2023). Speciﬁcally, LLMs fail to produce much variance in their answers to\npsychology survey questions: even if these questions pertain to topics—like moral\njudgment—for which there is no actual correct answer, and for which human answers\nwould have diversity-of-thought. Simulating human diversity-of-thought using LLMs might\ngenerally be non-trivial. For example, simply prompting an LLM to respond to a question\nmultiple times and then measuring the response variance, a common strategy in the social\nsciences (Almeida et al., 2023; Atari, Xue, et al., 2023; Park et al., 2023), does not equate\nto meaningful variance that can be compared with humans. Generative LLMs, such as\nChatGPT, compute a probability distribution over possible next words to produce an\noutput sequence in an autoregressive manner (Radford et al., 2018; Vaswani et al., 2017).\nSpeciﬁcally, the probability of a word being predicted next is given asP (zi)= ezi/T\nqn\nk=1 ezk/T ,\nwhere zi represents the logit value associated with a potential output wordwi given an\ninput sequence{w1, ..., wi≠1}, which is, i.e., the model’s internal representation of the word\nafter processing it (Park et al., 2023). The “temperature” parameter (T ) expresses whether\nthe model chooses the most likely word according to the distribution (T æ 0), expressing\ndeterministic behavior, or samples the words according to the distribution (T æ 1),\nsupposedly expressing more creative non-deterministic behavior. However, the output\nvariance when repeating the same prompt using a temperature of 1 then simply reﬂects the\nPERILS AND OPPORTUNITIES OF LLMS 8\noutput probability over the response options and therefore how sure the model is about its\nresponse, which itself is a\u0000ected by the correct answer bias observed by Park et al. (2023)\nand others.\nConceptually, this is akin to repeatedly asking a question to the same participant\ninstead of di\u0000erent participants. However, psychologists are usually interested in studying\nvariance across participants to make inferences about behavioral patterns and the\nrobustness of psychological phenomena. Thus, researchers using LLMs to study human\nbehavior need to move beyond methods that simulate single responses –such as merely\npredicting group averages or simulating an individual’s response across various tasks (such\nas in Bang et al., 2023; Dillion et al., 2023; H. Liu et al., 2023)– and instead develop robust\nmethods to emulate the complexity of human samples. Furthermore, LLMs are trained on\nvast amounts of data, which can contain many of the items and tasks used in psychological\nexperiments, thus leading the model to rely on its memory instead of making inferences,\nexacerbating the problem above. To get unbiased evaluations of human-like LLM behavior,\nresearchers need to make sure that their tasks are not part of the model’s training data\n(Frank, 2023) or adjust the models to not a\u0000ect the outcome of their experiments, such as\nby “unlearning” data from an LLM (Eldan & Russinovich, 2023).\nTo showcase how these issues concretely manifest, we analyze LLM responses in the\ndomain of human morality, highlighting how simple prompting strategies fail to capture\nhuman response patterns and particularly human variance. We designed a prompt and\ngave the Moral Foundations Questionnaire-2 (MFQ-2; Atari, Haidt, et al., 2023a) to\nGPT-3.5 1,000 times. The MFQ-2 is a new measure published in 2023, and its items are\nunlikely to exist verbatim in the corpora on which GPT was trained; hence, it does not\nhave the problem of extensively appearing in the training corpora. We then compared\nGPT’s responses to the distributions of a culturally diverse sample of humans from 19\npopulations. As can be seen in Figure 1A, GPT produces substantially smaller variance in\nsix moral domains (i.e., care, equality, proportionality, loyalty, authority, and purity)\nPERILS AND OPPORTUNITIES OF LLMS 9\ncompared with actual human populations (allps < .001). GPT-3.5’s variance was 43–121\ntimes smaller than human data in di\u0000erent moral domains, even when using parameter\nsettings for maximum variability in generated responses. In contrast to the argument by\nDillion et al. (2023) that “silicon sampling allows researchers to simulate a diverse\npopulation of participants,” our results, along with emerging work from others (e.g., Atari,\nOmrani, & Dehghani, 2023a; Park et al., 2023), show that the so-called “silicon sampling”\nfails in mimicking a diverse population of humans: a ﬁnding which is consistent with that\nof Atari, Xue, et al. (2023).\nTo show that this phenomenon is robust across domains, we extended this analysis\nof GPT versus human responses to a broad range of self-report measures, with participants\nfrom over 43 countries and spanning various psychological domains, such as personality,\ncognition, political orientation, and emotions: Big Five Inventory (John & Srivastava,\n1999): N =3 , 924, Need for Closure (Webster & Kruglanski, 1994a):N =3 1 5, Need for\nCognition (Cacioppo & Petty, 1982a):N =9 0 0, Right-wing authoritarianism Scale\n(Zakrisson, 2005a): N =1 , 020, Emphasizing-Systemizing Scale (Baron-Cohen et al.,\n2003a): N =3 , 141, Rational-Experiential Inventory Scale (Pacini & Epstein, 1999a):\nN =1 , 456. Across these psychological constructs, we consistently found that ChatGPT\nresponses generally showed signiﬁcantly less variance across all measures (see Table S18 for\nan overview of variance di\u0000erences across all surveys) and di\u0000ered signiﬁcantly between\nvarious demographics. For example, when responding to personality surveys, ChatGPT\nwas signiﬁcantly more agreeable than politically liberal individuals (d = ≠0.230, p<. 001,\n95% CI [-0.369, -0.091]), conservatives (d = ≠0.403, p<. 001, 95% CI [-0.550, -0.257]), and\nmoderates (d =0 .285, p<. 001, 95% CI [-0.424, -0.145]) as shown in Figure 2. Beyond\npersonality dimensions, ChatGPT, for example, also endorsed signiﬁcantly less right-wing\nauthoritarianism than male participants (d =0 .44, p<. 001, 95%CI [0.24, 0.65]), White\nparticipants (d =0 .35, p<. 001, 95%CI [0.16, 0.54]), and younger participants (18-24;\nd =0 .49, p<. 001, 95%CI [0.24, 0.74]), but signiﬁcantly more than explicitly liberal\nPERILS AND OPPORTUNITIES OF LLMS 10\nFigure 1\nChatGPT vs. Human moral judgments\nNote. A) Distributions of moral judgments of humans (light blue) and GPT (light red) in six moral\ndomains. Dashed lines represent averages.B) Inter-correlations between moral values in humans (N =\n3,902) and ChatGPT queries (N = 1,000).C) Network of partial correlations between moral values based\non a diverse sample of humans from 19 nations (Atari et al., 2023) and 1,000 queries of GPT. Blue edges\nrepresent positive partial correlations and red edges represent negative partial correlations.\nPERILS AND OPPORTUNITIES OF LLMS 11\nparticipants (d = ≠0.23, p = .003, 95%CI [-0.39, -0.07]) as shown in Figure 3. Di\u0000erences\nof this kind and strength were observed across all surveys and demographic groups. See the\nSupplementary Materials for a detailed summary of all demographic di\u0000erences, across a\nvariety of psychological constructs.\nFigure 2\nComparing ChatGPT against Humans grouped by political opinion for responses on the Big Five\npersonality questionnaire\n1\n2\n3\n4\n5\nExtraversion AgreeablenessConscientiousnessNeuroticism Openness\nConstruct Score\nGPT\nConservative\nLiberal\nModerate\nNote. Figure shows the response distribution of humans and ChatGPT across the ﬁve-factor personality\nconstructs and for di\u0000erent human demographics. Figure shows that ChatGPT gives signiﬁcantly higher\nresponses on Agreeableness, Conscientiousness and signiﬁcantly lower responses on Openness and\nNeuroticism. Importantly, ChatGPT shows signiﬁcantly less variance compared with all demographic groups\non all personality constructs.\nThird, we are skeptical that GPT responses can make nomological networks in\nwell-established theoretical frameworks. Dillion et al. (2023) mention that “researchers can\ngive LLMs di\u0000erent questions and see if they act as expected within a nomological net\n(e.g., form a reliable scale). ” To demonstrate how GPT constructs a moral psychological\nnomological network, we looked at inter-correlations between moral domains (Figure 1B)\nand the network of moral domains based on partial correlations (Figure 1C) in a diverse\nPERILS AND OPPORTUNITIES OF LLMS 12\nFigure 3\nComparing ChatGPT against Humans across various demographic variables for the\nRight-Wing-Authoritarianism scale\n0\n2\n4\n6\nRWA\nConstruct Score\nGPT\n18−24\nLiberal\nMale\nWhite\nNote. Figure shows the response distribution of humans and ChatGPT on the RWA scale for di\u0000erent\nhuman demographics. ChatGPT shows signiﬁcantly lower average RWA than male, white, and young\nparticipants but not explicitly liberal participants. Importantly, ChatGPT shows signiﬁcantly less variance\ncompared with all demographic groups.\nhuman sample (N = 3,902; Atari, Haidt, et al., 2023a) and 1,000 GPT queries. As shown\nin Figures 1B and 1C, the networks of moral values are substantially di\u0000erent. Therefore,\nin contrast to the claim by Dillion et al. (2023), GPT seems to fail in producing previously\nestablished nomological networks.\nIn sum, probing LLMs can be a fruitful direction for future research (Grossmann\net al., 2023; Shi\u0000rin & Mitchell, 2023). However, we caution against the hasty replacement\nof human participants with LLMs, because (1) synthetic AI-simulated sampling would\nignore psychological and linguistic diversity that the ﬁeld desperately needs in order to\ninvestigate beyond WEIRD psychology (e.g., Apicella et al., 2020; Blasi et al., 2022;\nHenrich et al., 2010); (2) LLMs often fail to show meaningful variance (or diversity) in\ntheir judgments; and (3) the outputs of current LLMs do not seem to replicate previously\nPERILS AND OPPORTUNITIES OF LLMS 13\nestablished nomological networks.\nOverall, probing the psychology of LLMs is scientiﬁcally meaningful and practically\nimportant. However, it should not replace the scientiﬁc study ofHomo sapiens but rather\nsupplement it. Of course, these limitations do not imply that all uses of LLMs should be\navoided in psychological research. While Dillion et al. (2023) have portrayed AI as an\n“agent of replacement,” others have argued that AI should instead be an agent of\n“assistance,” “improvement,” or “augmentation” (e.g., Crockett & Messeri, 2023). Framed\nin the latter way, LLMs can be thought of as tools to help improve the research process in\npsychology. It is indeed exciting to try di\u0000erent psychological measures (traditionally\ndeveloped for humans) on AI tools, but such examinations should be cautious and\nexploratory, and their results should not be hyperbolized. Such careful considerations may\nlead to the development of more robust procedures to implement LLMs in psychology,\nsimilar to how psychologists have addressed issues with other methods. For example, when\nresearchers decide to use LLM outputs as a proxy for a human sample, they should take\ninto account task-relevant biases of the models in question (e.g., check for implicit biases\nagainst agents involved in a moral scenario when investigating moral judgments). This can\nbe done in conjunction with e\u0000orts to reduce LLM biases (Omrani et al., 2023).\nLLMs Are Not An All-Purpose Method\nIn this section, we discuss the perception of Large Language Models (LLMs) as a\nuniversal o\u0000-the-shelf text analysis tool in psychology. We begin by explaining key\ndistinctions in how LLMs can be applied for text analysis. This is followed by an\nexploration of the currently common practice of overly focusing on zero-shot capabilities of\nLLMs and an examination of their potential drawbacks. We then contrast LLMs with\nnon-LLM-based, top-down, theory-based text analysis tools, discussing the advantages of\nthese smaller, interpretable methods and how they can beneﬁt researchers. We highlight\nthat researchers must carefully consider the particular demands and subtleties of their\nPERILS AND OPPORTUNITIES OF LLMS 14\nstudy topics when selecting NLP methods for text analysis in psychology. This might\ninclude leveraging LLM-based approaches in scenarios where large-scale, general-purpose\nlanguage modeling is necessary. Conversely, in situations requiring theory-driven\nexplorations, top-down methods such as thematic content analysis or dictionary-based\ntechniques (e.g., LIWC) may be more suitable. By thoughtfully combining these methods\nbased on the study’s needs, researchers can achieve a more nuanced and comprehensive\nunderstanding of their textual data.\nNLP methods using pre-trained Language Models generally fall into two categories:\nthose involving parameter updates (e.g., gradient-based) and those that do not.\nFine-tuning is a primary method involving parameter updates, where the pretrained LLM\nis further trained on a task-speciﬁc dataset. This can range from updating a subset of\nparameters to adjusting the entire model, depending on task complexity and data\navailability (Devlin et al., 2018a; Houlsby et al., 2019; E. J. Hu et al., 2021; Ruder et al.,\n2019) In contrast, zero-shot, one-shot, and few-shot learning, can be used without gradient\nupdates by leveraging a pretrained model’s ability to generalize from limited or no\ntask-speciﬁc data (i.e., examples), using the model’s pre-existing knowledge and\nunderstanding (Brown et al., 2020; Radford et al., 2019; Xian et al., 2018).\nRecently, there has been a surge of studies using LLMs zero-shot capabilities, that is\ntheir capability to perform tasks without any prior training on that speciﬁc task (Brown\net al., 2020; Kojima et al., 2022; P. Liu et al., 2023; Romera-Paredes & Torr, 2015), for\npsychological text analysis, presumably due to their ease of use and accessibility. For\nexample, Markowitz (2023), Rathje et al. (2023), and Zhu et al. (2023) reported high\nperformance of ChatGPT as an automated text analysis tool, such as for sentiment\nanalysis, o\u0000ensive language, thinking style, or emotion detection. Rathje et al. (2023)\nfurther concluded that LLMs constitute a viable all-purpose method for psychological text\nanalysis, arguably more convenient than small(er) language models and traditional\ntechniques in NLP, due to their ability to handle diverse tasks within a single model\nPERILS AND OPPORTUNITIES OF LLMS 15\nwithout needing task-speciﬁc adjustments, and their user-friendly design that minimizes\nthe need for complex coding, making them more accessible to psychologists and potentially\nencouraging broader research engagement. However, this perspective of LLMs, as a\nconvenient and comprehensive tool in psychological text analysis, is challenged by recent\ncritiques emphasizing their limitations, such as inconsistencies in text annotations,\ndi\u0000culties in explaining complex constructs such as implicit hate speech, and a potential\nlack of depth in specialized or sensitive areas (Huang et al., 2023; KocoÒ et al., 2023; Reiss,\n2023). Furthermore, the di\u0000erence in convenience between zero-shot applications of LLMs\nand ﬁne-tuning of models may not be as stark as commonly perceived. Fine-tuned small(er)\nlanguage models for various tasks are now increasingly publicly available. Similarly, there\nare more and more high-quality and specialized datasets available to researchers for\nﬁne-tuning language models themselves. Examples of such corpora span across ﬁelds, such\nas morality, personality, or sentiment analysis (Alabi et al., 2022; ﬂöltekin, 2020; Golbeck,\n2016; Hoover et al., 2020; Rahman & Halim, 2022; Trager et al., 2022a).\nNotably, even smaller ﬁne-tuned models can in many cases, where data for\nﬁne-tuning is available, fare equally well or even better than zero-shot applications of\nLLMs. For instance, in most reported cases in Rathje et al. (2023), small ﬁne-tuned\nlanguage models, including older models such as base-BERT (Bidirectional Encoder\nRepresentations from Transformers; Devlin et al., 2018a), outperform even the newest\ngeneration of ChatGPT (GPT-4) on a variety of text annotation tasks. Notably, in Rathje\net al. (2023) GPT-4 (zero-shot) was outperformed on English language sentiment analysis\nby a ﬁne-tuned model developed before the breakthrough Transformers architecture\n(Cliche, 2017). Additionally, these smaller language models have since then further\nundergone signiﬁcant improvement, such as Sentence-BERT (Reimers & Gurevych, 2019),\nRoBERTa (Y. Liu et al., 2019), and their subsequent iterations, highlighting additional\npotential performance advantages. While zero-shot LLMs might o\u0000er immediate\naccessibility, the most expedient choice is often not the most e\u0000ective. Thus, researchers\nPERILS AND OPPORTUNITIES OF LLMS 16\nshould be cautious of leaning too heavily on the allure of convenience. We explicitly\nhighlight here that improved methods do not have to exclude LLMs. There are various\nalternative ways to utilize LLMs, such as by ﬁne-tuning the LLM on the respective task, or\nvia few-shot prompting which includes few examples of solving a task in a prompt so that\nthe LLM can generalize to the given examples (Brown et al., 2020; P. Liu et al., 2023; Snell\net al., 2017).\nBuilding on this perspective, we extend these recent studies by investigating\nChatGPT’s ability to annotate moral language across three distinct settings: zero-shot,\nfew-shot, and ﬁne-tuned. Human morality has been argued to be a particularly di\u0000cult\nconcept for language models, making it an interesting test case for comparing powerful\nLLMs against less complex smaller language models (Russell, 2019), as well as testing the\ne\u0000cacy of various LLM application strategies. Speciﬁcally, we gave ChatGPT 2,983 social\nmedia posts that contained moral or non-moral language and prompted it to determine if\nany and what speciﬁc type of moral language was used. We then compared it to a small\nBERT model that was ﬁne-tuned on a separate subset of social media posts (see\nSupplementary Materials for details). To supplement our comparison of smaller vs larger\nand complex vs less complex models, we repeated this analysis using the Linguistic Inquiry\nand Word Count (LIWC; Pennebaker et al. (2001)), which is a simple and commonly used\ndictionary-based text-analysis method producing psychologically validated, and\nimportantly interpretable features, from a given text. We compared the annotations of all\nmodels against the “ground truth” of human raters.\nWe ﬁnd that ﬁne-tuned BERT vastly outperformed ChatGPT applied in a zero-shot\nsetting, achieving an F1 score of 0.48 vs. ChatGPT’s 0.22. Additionally, ChatGPT was\nmore extreme in over- or under-predicting a moral sentiment, while BERT did in all but\none case not signiﬁcantly deviate from trained human annotators. Notably, even the\nLIWC-based approach outperformed ChatGPT (zero-shot) with an F1 score of 0.27 and\nwas signiﬁcantly less likely and less extreme in deviating from trained human annotators,\nPERILS AND OPPORTUNITIES OF LLMS 17\ndespite being a magnitudes smaller, less complex, and cheaper model. As anticipated,\nutilizing ChatGPT using few-shot learning or ﬁne-tuning, increased performance.\nSpeciﬁcally, ChatGPT applied in a few-shot setting achieved an F1 score of 0.32 and\nﬁne-tuning ChatGPT on the same data as the BERT model achieved an F1 score of 0.53.\nHowever, in both cases, ChatGPT was still more extreme in over- or under-predicting a\nmoral sentiment compared with BERT. See Table S1 in the Supplementary Materials for\nan overview of our results. It should be noted here that while ﬁne-tuning ChatGPT or\napplying a few-shot paradigm can yield better results, such processes are signiﬁcantly more\nresource-intensive and expensive compared with LIWC or even BERT.\nAdditionally, ChatGPT and other closed-source models require (if possible at all)\nﬁne-tuning on a corporate platform which introduces constraints compared with models\nlike BERT which provide full control about the ﬁne-tuning process and can be trained\no\u0000ine without additional costs (see the following section for a detailed discussion of issues\nwith closed-source and proprietary vs open-source models). Few-shot learning, although\npromising, hinges on the relevance and quality of examples, potentially making it less\nstable than ﬁne-tuning (Reynolds & McDonell, 2021). As such, researchers should choose a\nmodel and how to apply it based on the availability of task-speciﬁc data, available\ncomputing resources, and research-speciﬁc considerations (e.g., interpretability, control over\nthe model). Fine-tuning is preferable if training data is available. Smaller models (e.g.,\nBERT) are then preferable to larger models (e.g., ChatGPT) if computing resources (or\nﬁnancial for commercial services) are limited. In scenarios where gathering extensive\ntask-speciﬁc data is impractical or impossible, LLM’s zero-shot or few-shot capabilities are\nadvantageous.\nTo further highlight the limitations of zero-shot LLM applications, we examined to\nwhat extent ChatGPT’s zero-shot annotations were biased toward speciﬁc demographics.\nWe did so by using human annotator demographics and psychometrics provided for our\ntest data (see Supplementary Materials for a detailed description of the experimental\nPERILS AND OPPORTUNITIES OF LLMS 18\nprocedures). We found that ChatGPT is less likely to agree with conservative\n(\u0000log(odds)= ≠4%, p<. 001) or collectivist (\u0000log(odds)= ≠30%, p<. 001)\nannotators and more likely to agree with individualistic (\u0000log(odds)=+ 1 4 2 %,\np<. 001) annotators. Additionally, it is biased toward younger (\u0000log(odds)= ≠1%\nyear ,\np =0 .033), more open-minded (\u0000log(odds)=+ 1 2 4 4 %, p<. 001), and agreeable\n(\u0000log(odds)=+ 1 5 6 %, p<. 001) annotators. Interestingly in terms of the six moral\nfoundations (Atari, Haidt, et al., 2023a), it is biased toward annotators who have a lower\npreference for equality (\u0000log(odds)= ≠97%, p<. 001) and loyalty\n(\u0000log(odds)= ≠96%, p<. 001) and those who endorse more care\n(\u0000log(odds)=+ 3 1 , 159%, p<. 001) and proportionality (\u0000log(odds)=+ 2 1 , 215%,\np<. 001) values. This aligns with our additional ﬁndings of ChatGPT over-weighing the\ncare foundation when responding to the MFQ-2 (see additional analyses in Supplementary\nMaterials). Potentially, this could be linked to post-hoc measures by OpenAI to avoid an\nAI that endorses harm or does not care about people’s emotional well-being. Agreement\nwith proportionality and equality endorsing annotators aligns with past ﬁndings that\nChatGPT is (left-)libertarian-leaning (Hartmann et al., 2023; Motoki et al., 2023;\nSanturkar et al., 2023). See Tables S2 - S8 in the Supplementary Materials for a summary\nof annotation biases.\nFinally, we analyzed how LLMs, at least at this stage of their development, fare\nagainst theory-based, top-down constrained methods. This is relevant because LLMs are\nprimarily considered language analysis tools due to their broad capabilities stemming from\ntheir vast parameter sets, training data, and training procedures. However, this ﬂexibility\nand performance come at the price of reduced interpretability and reproducibility (i.e., the\ntrade-o\u0000 between a higher-performing black box and a lower-performing interpretable\nmethod). An often-stated reason for why some researchers of psychological text analysis\nprefer neural-network-based models over simple, theory-driven methods is the purportedly\nsuperior predictive power of the former. If LLMs cannot outperform top-down methods in\nPERILS AND OPPORTUNITIES OF LLMS 19\nzero or few-shot settings, this would be another reason for psychologists (and other social\nscientists) to consider using top-down models rather than LLMs (at least the existing\nones), because in social sciences, theory and interpretability are of prime concern\n(Muthukrishna & Henrich, 2019).\nWe compared ChatGPT against Contextualized Construct Representation (CCR;\nAtari, Omrani, & Dehghani, 2023a), which is a method that combines psychometric scales\nwith small language models (e.g., s-BERT; Reimers & Gurevych, 2019) to extract\npsychological information from texts. By relying on validated psychometric scales, CCR\nplaces strong, theory-based, top-down constraints on its underlying small language model,\nwhich allows for high interpretability and easy application. Comparing ChatGPT and\nCCR, we found that ChatGPT (zero-shot) fails to outperform CCR in inferring\npsychological outcomes from human-written essays. CCR substantially outperformed\nChatGPT ratings when prompted to infer psychological constructs directly (Dunnett’s\nTest; d = ≠2.25, p =0 .005, 95%CI [-3.88, -0.62]), that is predicting the construct scores\ndirectly. See Figures S33 and S34 for an overview of model performances across all\nself-report measures (i.e., cultural orientation, personal values, moral judgments, political\nideology, need for cognition, and norm violations). CCR and ChatGPT performed on par\nwhen inferring psychological variables from essays about everyday life (Figure S34) when\nprompted on the item-level, that is, when predicting each scale item separately and then\ncalculating the construct score (Dunnett’s Test;d =0 .21, p =0 .9404, 95%CI [-1.42, 1.84]).\nTaken together, past work and our present ﬁndings consistently demonstrate that\nfor many use cases, smaller (ﬁne-tuned) models can be more powerful and less biased than\nthe current large (generative) language models, particularly in zero-shot and few-shot\nsettings. For example, consider a study examining the language used in online support fora\nfor individuals with anxiety disorders. A researcher using a smaller, specialized language\nmodel may be able to uncover subtle nuances and speciﬁc patterns of language that are\ndirectly relevant to the domain of interest (e.g., worry, intolerance of uncertainty). This\nPERILS AND OPPORTUNITIES OF LLMS 20\ntargeted approach can yield more profound insights into the experiences of individuals with\nanxiety, shedding light on their unique challenges and potential interventions. By\nleveraging specialized language models or top-down methods such as CCR, or LIWC,\nresearchers can strike a balance between comprehensiveness and granularity, enabling a\nmore nuanced exploration of textual data.\nCrucially, we do not discourage the use of LLMs as a text-analysis tool in all\nrespects. Particularly in cases where data for ﬁne-tuning is scarce, such as with new\nconstructs or understudied populations, LLMs’ zero-shot capabilities may still o\u0000er\nacceptable performances and allow researchers to investigate urgent research questions.\nMethods such as few-shot prompting can be e\u0000ective and e\u0000cient in these cases as they\nonly demand a handful of representative examples. Across our experiments, we found that\nLLMs can achieve high performances but stress that this cannot always be achieved using\nthe model’s zero-shot capabilities and instead requires exploring techniques, such as\nfew-shot prompting or outright ﬁne-tuning of the LLM (see also work on optimizing\nprompt design, such as “Chain-of-Thought” (Kojima et al., 2022; Wei et al., 2022), a recent\ntechnique for eliciting complex multi-step reasoning). Similarly, our additional analyses,\nshowing increased performance for ChatGPT when predicting at the item level instead of\nthe construct level, highlight that LLMs can beneﬁt from integrating theory-driven\napproaches. Extending this line of work, developing methods that can combine the beneﬁts\nof both approaches is a promising endeavor for future work. With the constant and rapid\ndevelopment of LLMs that address performance and bias issues, these concerns could\nalready be mitigated in the near future.\nWe emphasize the importance of researchers constantly assessing these limitations\nand opportunities while exercising caution when defaulting to the most convenient choice.\nAs with all empirical methods, LLM-based methods need to be validated and\nbenchmarked. We recommend benchmarking LLM-based ﬁndings against more established\ntext-analytic methods to make LLMs more useful for psychological inference. For example,\nPERILS AND OPPORTUNITIES OF LLMS 21\nthe gold-standard measure for text annotation is human annotation, and LLM-based\nannotations should be comprehensively validated—on a task-by-task basis—against a small\nlabeled corpus before using these models at scale (Pangakis et al., 2023). This approach\nhelps establish the validity of the analysis by reducing the potential biases that may arise\nfrom relying solely on automated techniques. Additionally, validation allows researchers to\nassess the inter-rater reliability of the annotations, providing a measure of the robustness of\nthe analysis. The human element in validation brings valuable insights, subjective\njudgments, and contextual understanding that may be challenging for LLMs to capture\naccurately.\nReproducibility Matters\nReproducibility pertains to replicating and verifying results using the same data and\nmethods (Nosek et al., 2022). However, particular challenges arise when applying these\nprinciples to LLMs, particularly proprietary ones. Their black-box nature impedes the\nreproducibility of ﬁndings that pertain to them. This limitation poses a signiﬁcant obstacle\nto achieving reproducibility in studies that rely on LLM-generated data or analyses.\nAdditionally, their biases can also change over time for LLMs that undergo updates\nover time. Each time modiﬁcations are made to ChatGPT’s algorithms for performance\nenhancement, the nature and scope of biases embedded in the model may change. This\ncould impact the e\u0000ectiveness of previously established “best practices” and debiasing\nstrategies (Bail, 2023; Spirling, 2023). Currently, ChatGPT, and to our best knowledge any\nother closed-source models, does not freely provide past versions that allow researchers to\nuse the model from speciﬁc points in time (e.g., “gpt3.5-January-2023”) to reproduce\nresearch results. It only provides a snapshot after each major update, which is then\ndeprecated within three months to one year. This means that even when updates address\nand mitigate detected biases, they also introduce the potential for “process reproducibility\nfailure” (Nosek et al., 2022) in the generated data and impede reproducibility, critical for\nPERILS AND OPPORTUNITIES OF LLMS 22\nscientiﬁc rigor (Bail, 2023; Spirling, 2023; Zhai, 2023). This also a\u0000ects currently emerging\npractices for reproducibility, such as researchers sharing prompts and Application\nProgramming Interface (API) parameters because the e\u0000ect of these parameters (or the\nparameters themselves) and the outputs generated from these prompts can change over\ntime. Importantly, new iterations do not guarantee equal or better performance across all\ntasks. For example, Rathje et al. (2023) report inconsistent results between GPT-3.5 and\nGPT-4 performance on various text-analysis tasks—for example, GPT-4 sometimes\nperforms more poorly than GPT-3.5—which lends support to our concern that\nnontransparent changes to the model can cause unforeseen challenges. Additionally,\ncurrent state-of-the-art LLMs come with additional opaque ﬁne-tuning, such as\nreinforcement learning using human feedback (RLHF; Arumugam et al., 2019; Ziegler\net al., 2019), for which usually neither the training method nor the training data are\nknown. Therefore, researchers should carefully consider the trade-o\u0000 between forgoing\ncontrol over these procedures (Spirling, 2023) and ﬁne-tuning models themselves.\nResearchers should be aware of the current black-box nature of LLMs (Bail, 2023;\nSpirling, 2023). Not only from an open-science standpoint, but more generally, researchers\nshould be interested in access to high-quality, informative semantic representations and the\nalgorithms used to generate outputs, instead of only the outputs. One of the main\nadvantages of computational models is that they allow us to ‘look under the hood’ and\nthus make inferences about psychological processes which may be di\u0000cult to test\notherwise. Thus, using proprietary LLMs may constitute a missed opportunity for\ntheory-based work in psychology that aims to leverage innovations in computer science.\nNotably, the lack of transparency in how ChatGPT generates responses implies that\nresearchers cannot ascertain the underlying mechanisms and origin of biases that may be\ninﬂuencing the outputs (X. Hu et al., 2023; Pournaras, 2023; Zhai, 2023). Such lack of\ntransparency is antithetical to the scientiﬁc principles of openness and replicability that\nshould be central to computational-psychological research (Hofman et al., 2021).\nPERILS AND OPPORTUNITIES OF LLMS 23\nComputational-psychological research that aims to achieve replicable ﬁndings with\nLLMs can use models that have a publicly available open architecture, such as BigScience\nLarge Open-science Open-access Multilingual Language Model (BLOOM; Scao et al., 2022)\nand Large Language Model Meta AI (LLaMA; Touvron et al., 2023). These open-source\nmodels provide researchers with access to network architecture, including datasets and\nparameters such as pre-trained weights (Binz & Schulz, 2023). The less black-box nature of\nthese open-source models can help researchers make the exact version used in their works\navailable to others (Spirling, 2023), thus facilitating transparency and reproducibility. In\nthe Supplementary Materials, we detail how a complete research pipeline using a\nLLaMA-based model can be conducted and show that the results are comparable to a\npipeline using a closed-source LLM. Despite the model’s signiﬁcantly smaller size (7 billion\nvs. 175 billion parameters) and while being self-hosted on consumer-grade hardware, it\nachieved a comparable performance in a text annotation task discussed above (F1 of 0.23\nvs. 0.22) and a similar response pattern when answering self-report psychological measures.\nCompared with closed-source LLMs, open-source LLMs may thus come with a replicability\nbeneﬁt and increased control over the model when it comes to computational-psychological\nresearch. Additionally, certain approaches aimed to allow for more robust inferences\nregarding LLM outputs, such as “unlearning” (Eldan & Russinovich, 2023) data and tasks\nused for evaluating LLM capabilities from the training data, require an open model\narchitecture (e.g., the ability to access and manipulate the probability distribution over the\noutput tokens).\nHowever, the primary focus should not be a debate of closed-source vs. open-source\nLLMs, but instead addressing the speciﬁc problems for scientiﬁc research using either\napproach. Closed-source models, such as ChatGPT, may introduce ways to save and reload\nor share model weights and modify their models. For example, OpenAI continuously adds\n(but also deprecates) available model parameters. As of writing, increased reproducibility\nhas been announced as in beta development, and a “seed” parameter increasing\nPERILS AND OPPORTUNITIES OF LLMS 24\ndeterministic behavior as well as a “system_ﬁngerprint” value to help track model changes\nwas recently added. However, the “seed” does not guarantee reproducibility, and while the\n“system_ﬁngerprint” increases transparency, allowing researchers to identify potential\nreasons for failure to replicate, it does not increase reproducibility itself. Moreover, some\nproblems that might be more pronounced with proprietary models, such as economic\nmotivations, can still a\u0000ect open-source models (e.g., LLaMA being developed by Meta, a\nfor-proﬁt company). Additionally, openness is a spectrum requiring researchers to closely\nmonitor to what extent LLMs fulﬁll relevant openness criteria for their projects. See\nLiesenfeld et al. (2023) discussing of LLM openness in detail as well as their openness\nratings of current LLMs https://opening-up-chatgpt.github.io, including their speciﬁc\nopenness criteria and evaluation strategies https://github.com/opening-up-chatgpt/\nopening-up-chatgpt.github.io/tree/main/projects#criteria. As such, researchers should\nconsider to what extent each model and the organization behind it conﬂicts with scientiﬁc\nand ethical principles of research (Bonnefon et al., 2020; Hagendor\u0000, 2020; Windsor, 2006).\nWe caution against mistakenly treating the use of open-source rather than closed-source\nLLMs as su\u0000cient to solve the black-box problem: a problem that remains largely\nunsolved, even for open-source LLMs. The overconﬁdence with which researchers may trust\nopen-source LLMs’ ﬁndings as replicable and transparent can systematically cause such\nresearchers to over-rely on these ﬁndings, even in out-of-distribution settings where the\nﬁndings may not generalize.\nThe reproducibility concerns surrounding computational-psychological research\ndone via LLMs are further intensiﬁed by their ability to adopt di\u0000erent perspectives\nthrough prompting: explicit instructions to the model on which the output is conditioned\non (Deshpande et al., 2023; Hwang et al., 2023; H. Jiang et al., 2023). Prompting has been\nshown to be a promising technique to increase the range of applications and versatility of\nLLMs (Wang et al., 2023), and is also discussed as a strategy to mitigate some of the biases\nin these models’ outputs (Shaikh et al., 2022; Si et al., 2022). One idea is to condition\nPERILS AND OPPORTUNITIES OF LLMS 25\nLLMs’ outputs on explicit instructions to take the perspective of di\u0000erent groups or\ndemographics. For example, some work has shown that prompting can improve the\nalignment of group-relevant responses (e.g., if prompted to respond like a Democrat,\nChatGPT will answer opinion poll questions in the direction steered). However, at the\nmoment, prompting nonetheless falls short in closing the gap in demographic group biases\n(Santurkar et al., 2023) and tends to reﬂect overly-simplistic cultural stereotypes instead of\nperspective-taking (Durmus et al., 2023). Moreover, prompting also introduces signiﬁcant\nchallenges to reproducibility in psychological research, because prompts can be constructed\nin numerous ways. Past work has shown that slight alterations and modiﬁcations in\nphrasing, context, or order can lead to substantially di\u0000erent responses (Fujita et al., 2022;\nGan & Mori, 2023; Lu et al., 2021; Markowitz, 2023; Mishra et al., 2023; Park et al., 2023).\nWe extended this literature by directly testing if changes in prompts would a\u0000ect\nthe results of our previously presented experiments. We repeated the moral sentiment\nanalysis with a modiﬁed prompt (see Supplementary Materials) and tested whether this led\nto di\u0000erent classiﬁcation outputs. Furthermore, we repeated the survey responses collection\nusing several modiﬁed prompts (see Supplementary Materials), and tested whether the\nresponses changed. The changes to the prompts were derived from past work that showed\nhow minor changes to the self-report design, such as adding contextual information or\nchanging the response scale, can elicit di\u0000erent response patterns (Schwarz, 1999). Our\nﬁndings demonstrate, in line with recent works on the e\u0000ect of prompting (Fujita et al.,\n2022; Gan & Mori, 2023; Lu et al., 2021; Markowitz, 2023; Park et al., 2023), that minor\nchanges in prompts lead to signiﬁcant di\u0000erences in outputs. For example, adding a study\nintroduction when prompting ChatGPT to respond to the Big Five Inventory (BFI; John\n& Srivastava, 1999), signiﬁcantly reduced the scores for “Openness” (d = ≠0.30; p<. 001;\n95%CI [-0.34, -0.26]), Extraversion (d = ≠0.54; p<. 001; 95%CI [-0.58, -0.50]), and\nAgreeableness (d = ≠0.14; p<. 001; 95%CI [-0.18, -0.10]). Similarly, modifying the order\nin which the types of moral foundations are deﬁned in the prompt changed how texts were\nPERILS AND OPPORTUNITIES OF LLMS 26\nclassiﬁed. Using the modiﬁed prompt, ChatGPT was signiﬁcantly less likely to annotate\nthe care foundation (-56%,p<. 001), the equality foundation (-62%,p<. 001), the loyalty\nfoundation (-33%,p = .009), and signiﬁcantly more likely to annotate the authority\nfoundation (+71%,p<. 001), and purity foundation (+164%,p = .020). See the\nSupplementary Materials for extended results and a detailed overview of our procedures.\nOur research emphasizes the importance of considering context and speciﬁc prompts\nwhen using ChatGPT to emulate diverse human behaviors (Rao et al., 2023). Continuous\nalgorithmic changes necessitate ongoing validation of prompting strategies, a task made\nchallenging by the complex and opaque nature of LLMs (E. Jiang et al., 2022; P. Liu et al.,\n2023; Mishra et al., 2023). While these challenges do not negate the applicability of LLMs\nin psychological research, they do highlight the need for robust, standardized methods akin\nto those used in traditional human research (Schwarz, 1999; Schwarz et al., 1991). As this\nis a rapidly evolving ﬁeld, new AI models may account for all of the above concerns of\ndiversity, variation, transparency, accuracy, and robustness. The progress that is expected\nto be made by LLMs in the near future requires that empirical work be completed to\nhighlight their strengths, weaknesses, and promises for further enhancement Markowitz\n(2023).\nConclusion\nGPTology—which we deﬁne as the hurried and unjustiﬁed application of LLMs\neither as “replacements” for human participants, or as an o\u0000-the-shelf “one-size-ﬁts-all”\nmethod in psychological text analysis—can lead to a proliferation of low-quality research,\nespecially if the convenience of using LLMs such as ChatGPT leads researchers to rely too\nheavily on them. A WEIRD bias and the opaque, and often irreproducible nature of these\nmodels make the current LLMs, speciﬁcally proprietary ones, a double-edged sword for\npsychological research. This does not mean that LLMs are inadequate to aid psychological\nresearch, but researchers must actively exercise caution and critically evaluate the\nPERILS AND OPPORTUNITIES OF LLMS 27\nlimitations of these models before incorporating them into their research paradigms. In the\npresent work, we empirically quantiﬁed some of the biases and limitations of these models\nacross multiple psychological domains including moral judgments, personality traits,\ncultural orientation, and political ideology, among others. Psychological science, which has\nwitnessed multiple adverse consequences when new technologies were haphazardly and\nheedlessly used, needs to strive for diversiﬁcation of research samples, validation of\ndi\u0000erent methods against one another, transparency, and ethical considerations in\ndeploying LLMs to ensure that the ﬁndings are robust, generalizable, and free from\ndemographic biases. A commitment to rigor and replicability should guide the integration\nof AI into psychological research, not convenience.\nPERILS AND OPPORTUNITIES OF LLMS 28\nReferences\nAiyappa, R., An, J., Kwak, H., & Ahn, Y.-Y. (2023). Can we trust the evaluation on\nchatgpt? https://arxiv.org/abs/2303.12767\nAlabi, J. O., Adelani, D. I., Mosbach, M., & Klakow, D. (2022). Adapting pre-trained\nlanguage models to african languages via multilingual adaptive ﬁne-tuning.\nProceedings of the 29th International Conference on Computational Linguistics,\n4336–4349.\nAlmeida, G. F., Nunes, J. L., Engelmann, N., Wiegmann, A., & de Araśjo, M. (2023).\nExploring the psychology of gpt-4’s moral and legal reasoning.arXiv preprint\narXiv:2308.01264.\nApicella, C., Norenzayan, A., & Henrich, J. (2020). Beyond weird: A review of the last\ndecade and a look ahead to the global laboratory of the future.Evolution and\nHuman Behavior, 41(5), 319–329.\nArumugam, D., Lee, J. K., Saskin, S., & Littman, M. L. (2019). Deep reinforcement\nlearning from policy-dependent human feedback.arXiv preprint arXiv:1902.04257.\nAtari, M., Haidt, J., Graham, J., Koleva, S., Stevens, S. T., & Dehghani, M. (2023).\nMorality beyond the weird: How the nomological network of morality varies across\ncultures. Journal of Personality and Social Psychology.\nAtari, M., Omrani, A., & Dehghani, M. (2023). Contextualized construct representation:\nLeveraging psychometric scales to advance theory-driven text analysis.\nhttps://psyarxiv.com/m93pd\nAtari, M., Xue, M. J., Park, P. S., Blasi, D. E., & Henrich, J. (2023). Which humans?\npsyarxiv.com/5b26t\nAydın, Ö., & Karaarslan, E. (2023). Is chatgpt leading generative ai? what is beyond\nexpectations? What is Beyond Expectations.\nAzaria, A. (2023). Chatgpt: More human-like than computer-like, but not necessarily in a\ngood way.\nPERILS AND OPPORTUNITIES OF LLMS 29\nBail, C. A. (2023). Can generative ai improve social science?\nURL_WHERE_THE_PREPRINT_CAN_BE_FOUND\nBaktash, J. A., & Dawodi, M. (2023). Gpt-4: A review on advancements and opportunities\nin natural language processing. https://arxiv.org/abs/2305.03195\nBang, Y., Cahyawijaya, S., Lee, N., Dai, W., Su, D., Wilie, B., Lovenia, H., Ji, Z., Yu, T.,\nChung, W., et al. (2023). A multitask, multilingual, multimodal evaluation of\nchatgpt on reasoning, hallucination, and interactivity.\nhttps://arxiv.org/abs/2302.04023\nBaron-Cohen, S., Richler, J., Bisarya, D., Gurunathan, N., & Wheelwright, S. (2003). The\nsystemizing quotient: An investigation of adults with asperger syndrome or\nhigh–functioning autism, and normal sex di\u0000erences.Philosophical Transactions of\nthe Royal Society of London. Series B: Biological Sciences, 358(1430), 361–374.\nBarrett, H. C., Bolyanatz, A., Crittenden, A. N., Fessler, D. M., Fitzpatrick, S.,\nGurven, M., Henrich, J., Kanovsky, M., Kushnick, G., Pisor, A., et al. (2016).\nSmall-scale societies exhibit fundamental variation in the role of intentions in moral\njudgment. Proceedings of the National Academy of Sciences, 113(17), 4688–4693.\nBennett, C., Miller, M., & Wolford, G. (2009). Neural correlates of interspecies perspective\ntaking in the post-mortem atlantic salmon: An argument for multiple comparisons\ncorrection [Organization for Human Brain Mapping 2009 Annual Meeting].\nNeuroImage, 47, S125. https://doi.org/10.1016/S1053-8119(09)71202-9\nBian, N., Han, X., Sun, L., Lin, H., Lu, Y., & He, B. (2023). Chatgpt is a knowledgeable\nbut inexperienced solver: An investigation of commonsense problem in large\nlanguage models. https://arxiv.org/abs/2303.16421\nBinz, M., & Schulz, E. (2023). Turning large language models into cognitive models.\nhttps://arxiv.org/abs/2306.03917\nBlasi, D. E., Henrich, J., Adamou, E., Kemmerer, D., & Majid, A. (2022). Over-reliance on\nenglish hinders cognitive science.Trends in Cognitive Sciences.\nPERILS AND OPPORTUNITIES OF LLMS 30\nBonnefon, J.-F., Shari\u0000, A., & Rahwan, I. (2020). The moral psychology of ai and the\nethical opt-out problem.Ethics of Artiﬁcial Intelligence,1 0 9 – 1 2 6 .\nBozkurt, A., Xiao, J., Lambert, S., Pazurek, A., Crompton, H., Koseoglu, S., Farrow, R.,\nBond, M., Nerantzi, C., Honeychurch, S., et al. (2023). Speculative futures on\nchatgpt and generative artiﬁcial intelligence (ai): A collective reﬂection from the\neducational landscape.Asian Journal of Distance Education, 18(1).\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P.,\nNeelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. (2020). Language models\nare few-shot learners.Advances in Neural Information Processing Systems, 33,\n1877–1901.\nBuolamwini, J., & Gebru, T. (2018). Gender shades: Intersectional accuracy disparities in\ncommercial gender classiﬁcation.Proceedings of the Conference on fairness,\naccountability and transparency,7 7 – 9 1 .\nCacioppo, J. T., & Berntson, G. G. (1999). The a\u0000ect system: Architecture and operating\ncharacteristics. Current Directions in Psychological Science, 8(5), 133–137.\nCacioppo, J. T., & Petty, R. E. (1982). The need for cognition.Journal of Personality and\nSocial Psychology, 42(1), 116.\nCao, Y., Zhou, L., Lee, S., Cabello, L., Chen, M., & Hershcovich, D. (2023). Assessing\ncross-cultural alignment between chatgpt and human societies: An empirical study.\nhttps://arxiv.org/abs/2303.17466\nChen, K., Shao, A., Burapacheep, J., & Li, Y. (2022). A critical appraisal of equity in\nconversational ai: Evidence from auditing gpt-3’s dialogues with di\u0000erent publics on\nclimate change and black lives matter. https://arxiv.org/abs/2209.13627\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H.,\nBurda, Y., Joseph, N., Brockman, G., et al. (2021). Evaluating large language\nmodels trained on code. https://arxiv.org/abs/2107.03374\nPERILS AND OPPORTUNITIES OF LLMS 31\nCliche, M. (2017). Bb_twtr at semeval-2017 task 4: Twitter sentiment analysis with cnns\nand lstms.arXiv preprint arXiv:1704.06125.\nﬂöltekin, ﬂ. (2020). A corpus of turkish o\u0000ensive language on social media.Proceedings of\nthe Twelfth Language Resources and Evaluation Conference,6 1 7 4 – 6 1 8 4 .\nConte, R., Gilbert, N., Bonelli, G., Cio\u0000-Revilla, C., De\u0000uant, G., Kertesz, J., Loreto, V.,\nMoat, S., Nadal, J. .-., Sanchez, A., et al. (2012). Manifesto of computational social\nscience. The European Physical Journal Special Topics, 214,3 2 5 – 3 4 6 .\nCrockett, M., & Messeri, L. (2023). Should large language models replace human\nparticipants?\nCronbach, L. J. (1957). The two disciplines of scientiﬁc psychology.American psychologist,\n12(11), 671.\nDale, R. (2021). Gpt-3: What’s it good for?Natural Language Engineering, 27(1), 113–118.\nDeng, L., & Liu, Y. (2018).Deep learning in natural language processing. Springer.\nDeshpande, A., Murahari, V., Rajpurohit, T., Kalyan, A., & Narasimhan, K. (2023).\nToxicity in chatgpt: Analyzing persona-assigned language models.\nhttps://arxiv.org/abs/2304.05335\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep\nbidirectional transformers for language understanding.\nhttps://arxiv.org/abs/1810.04805\nDillion, D., Tandon, N., Gu, Y., & Gray, K. (2023). Can ai language models replace human\nparticipants? Trends in Cognitive Sciences.\nDurmus, E., Nyugen, K., Liao, T. I., Schiefer, N., Askell, A., Bakhtin, A., Chen, C.,\nHatﬁeld-Dodds, Z., Hernandez, D., Joseph, N., et al. (2023). Towards measuring the\nrepresentation of subjective global opinions in language models.\nhttps://arxiv.org/abs/2306.16388\nDwivedi, Y. K., Kshetri, N., Hughes, L., Slade, E. L., Jeyaraj, A., Kar, A. K.,\nBaabdullah, A. M., Koohang, A., Raghavan, V., Ahuja, M., et al. (2023). “so what\nPERILS AND OPPORTUNITIES OF LLMS 32\nif chatgpt wrote it?” multidisciplinary perspectives on opportunities, challenges and\nimplications of generative conversational ai for research, practice and policy.\nInternational Journal of Information Management, 71,1 0 2 6 4 2 .\nEdelmann, A., Wol\u0000, T., Montagne, D., & Bail, C. A. (2020). Computational social science\nand sociology.Annual Review of Sociology, 46,6 1 – 8 1 .\nEldan, R., & Russinovich, M. (2023). Who’s harry potter? approximate unlearning in llms.\nElman, J. L. (1990). Finding structure in time.Cognitive Science, 14(2), 179–211.\nEysenck, H. J. (1967).The biological basis of personality(Vol. 689). Transaction publishers.\nFiedler, K. (2011). Voodoo correlations are everywhere—not only in neuroscience.\nPerspectives on Psychological Science, 6(2), 163–171.\nFloridi, L. (2023). Ai as agency without intelligence: On chatgpt, large language models,\nand other generative models.Philosophy & Technology, 36(1), 15.\nFloridi, L., & Chiriatti, M. (2020). Gpt-3: Its nature, scope, limits, and consequences.\nMinds and Machines, 30,6 8 1 – 6 9 4 .\nFossati, A., Borroni, S., Marchione, D., & Ma\u0000ei, C. (2011). The big ﬁve inventory (bﬁ).\nEuropean Journal of Psychological Assessment.\nFrank, M. C. (2023a). Baby steps in evaluating the capacities of large language models.\nNature Reviews Psychology,1 – 2 .\nFrank, M. C. (2023b). Large language models as models of human cognition.\nFujita, H., et al. (2022). Prompt sensitivity of language model for solving programming\nproblems. New Trends in Intelligent Software Methodologies, Tools and Techniques:\nProceedings of the 21st International Conference on New Trends in Intelligent\nSoftware Methodologies, Tools and Techniques (SoMeT_22), 355,3 4 6 .\nGan, C., & Mori, T. (2023). Sensitivity and robustness of large language models to prompt\nin japanese. https://arxiv.org/abs/2305.08714\nGarten, J., Hoover, J., Johnson, K. M., Boghrati, R., Iskiwitch, C., & Dehghani, M. (2018).\nDictionaries and distributions: Combining expert knowledge and large scale textual\nPERILS AND OPPORTUNITIES OF LLMS 33\ndata content analysis: Distributed dictionary representation.Behavior research\nmethods, 50,3 4 4 – 3 6 1 .\nGhosh, A., & Bir, A. (2023). Evaluating chatgpt’s ability to solve higher-order questions on\nthe competency-based medical education curriculum in medical biochemistry.\nCureus, 15(4).\nGhosh, S., & Caliskan, A. (2023). Chatgpt perpetuates gender bias in machine translation\nand ignores non-gendered pronouns: Findings across bengali and ﬁve other\nlow-resource languages. https://arxiv.org/abs/2305.10510\nGolbeck, J. A. (2016). Predicting personality from social media text.AIS Transactions on\nReplication Research, 2(1), 2.\nGoldberg, Y. (2022).Neural network methods for natural language processing. Springer\nNature.\nGraham, J., Haidt, J., Motyl, M., Meindl, P., Iskiwitch, C., & Mooijman, M. (2018). Moral\nfoundations theory.Atlas of Moral Psychology, 211.\nGraves, A. (2013). Generating sequences with recurrent neural networks.\nhttps://arxiv.org/abs/1308.0850\nGrossmann, I., Feinberg, M., Parker, D. C., Christakis, N. A., Tetlock, P. E., &\nCunningham, W. A. (2023). Ai and the transformation of social science research.\nScience, 380(6650), 1108–1109.\nGuo, Y., Yang, Y., & Abbasi, A. (2022). Auto-debias: Debiasing masked language models\nwith automated biased prompts.Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics,1 0 1 2 – 1 0 2 3 .\nHagendor\u0000, T. (2020). The ethics of ai ethics: An evaluation of guidelines.Minds and\nMachines, 30(1), 99–120.\nHartmann, J., Schwenzow, J., & Witte, M. (2023). The political ideology of conversational\nai: Converging evidence on chatgpt’s pro-environmental, left-libertarian orientation.\nhttps://arxiv.org/abs/2301.01768\nPERILS AND OPPORTUNITIES OF LLMS 34\nHenrich, J., Heine, S. J., & Norenzayan, A. (2010). The weirdest people in the world?\nBehavioral and Brain Sciences, 33(2-3), 61–83.\nHofman, J. M., Watts, D. J., Athey, S., Garip, F., Gri\u0000ths, T. L., Kleinberg, J.,\nMargetts, H., Mullainathan, S., Salganik, M. J., Vazire, S., et al. (2021). Integrating\nexplanation and prediction in computational social science.Nature, 595(7866),\n181–188.\nHolterman, B., & van Deemter, K. (2023). Does chatgpt have theory of mind?\nhttps://arxiv.org/abs/2305.14020\nHoover, J., Portillo-Wightman, G., Yeh, L., Havaldar, S., Davani, A. M., Lin, Y.,\nKennedy, B., Atari, M., Kamel, Z., Mendlen, M., et al. (2020). Moral foundations\ntwitter corpus: A collection of 35k tweets annotated for moral sentiment.Social\nPsychological and Personality Science, 11(8), 1057–1071.\nHoulsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A.,\nAttariyan, M., & Gelly, S. (2019). Parameter-e\u0000cient transfer learning for nlp.\nInternational Conference on Machine Learning,2 7 9 0 – 2 7 9 9 .\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., & Chen, W.\n(2021). Lora: Low-rank adaptation of large language models.arXiv preprint\narXiv:2106.09685.\nHu, X., Tian, Y., Nagato, K., Nakao, M., & Liu, A. (2023). Opportunities and challenges\nof chatgpt for design knowledge management. https://arxiv.org/abs/2304.02796\nHuang, F., Kwak, H., & An, J. (2023). Is chatgpt better than human annotators? potential\nand limitations of chatgpt in explaining implicit hate speech.arXiv preprint\narXiv:2302.07736.\nHwang, E., Majumder, B. P., & Tandon, N. (2023). Aligning language models to user\nopinions. https://arxiv.org/abs/2305.14929\nPERILS AND OPPORTUNITIES OF LLMS 35\nJiang, E., Olson, K., Toh, E., Molina, A., Donsbach, A., Terry, M., & Cai, C. J. (2022).\nPromptmaker: Prompt-based prototyping with large language models.CHI\nConference on Human Factors in Computing Systems Extended Abstracts,1 – 8 .\nJiang, H., Zhang, X., Cao, X., Kabbara, J., & Roy, D. (2023). Personallm: Investigating\nthe ability of gpt-3.5 to express personality traits and gender di\u0000erences.\nhttps://arxiv.org/abs/2305.02547\nJobin, A., Ienca, M., & Vayena, E. (2019). The global landscape of ai ethics guidelines.\nNature Machine Intelligence, 1(9), 389–399.\nJohn, O. P., & Srivastava, S. (1999). The big-ﬁve trait taxonomy: History, measurement,\nand theoretical perspectives.\nKalla, D., & Smith, N. (2023). Study and analysis of chat gpt and its impact on di\u0000erent\nﬁelds of study.International Journal of Innovative Science and Research\nTechnology, 8(3).\nKennedy, B., Jin, X., Davani, A. M., Dehghani, M., & Ren, X. (2020). Contextualizing\nhate speech classiﬁers with post-hoc explanation.Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics,5 4 3 5 – 5 4 4 2 .\nKennedy, B., Reimer, N. K., & Dehghani, M. (2021). Explaining explainability:\nInterpretable machine learning for the behavioral sciences.\nhttps://psyarxiv.com/9h6qr/\nKeskar, N. S., McCann, B., Varshney, L. R., Xiong, C., & Socher, R. (2019). Ctrl: A\nconditional transformer language model for controllable generation.\nhttps://arxiv.org/abs/1909.05858\nKjell, O., Kjell, K., & Schwartz, H. A. (2023). Ai-based large language models are ready to\ntransform psychological health assessment.\nKocoÒ, J., Cichecki, I., Kaszyca, O., Kochanek, M., Szyd≥o, D., Baran, J., Bielaniewicz, J.,\nGruza, M., Janz, A., Kanclerz, K., et al. (2023). Chatgpt: Jack of all trades, master\nof none.Information Fusion,1 0 1 8 6 1 .\nPERILS AND OPPORTUNITIES OF LLMS 36\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y., & Iwasawa, Y. (2022). Large language models\nare zero-shot reasoners.Advances in neural information processing systems, 35,\n22199–22213.\nKoralus, P., & Wang-Maúcianica, V. (2023). Humans in humans out: On gpt converging\ntoward common sense in both success and failure. https://arxiv.org/abs/2303.17276\nKrügel, S., Ostermaier, A., & Uhl, M. (2023). Chatgpt’s inconsistent moral advice\ninﬂuences users’ judgment.Scientiﬁc Reports, 13(1), 4569.\nLazer, D., Pentland, A., Adamic, L., Aral, S., Barabási, A.-L., Brewer, D., Christakis, N.,\nContractor, N., Fowler, J., Gutmann, M., et al. (2009). Computational social\nscience. Science, 323(5915), 721–723.\nLewis, P., Stenetorp, P., & Riedel, S. (2020). Question and answer test-train overlap in\nopen-domain question answering datasets. https://arxiv.org/abs/2008.02637\nLi, J., Yang, Y., Wu, Z., Vydiswaran, V., & Xiao, C. (2023). Chatgpt as an attack tool:\nStealthy textual backdoor attack via blackbox generative model trigger.\nhttps://arxiv.org/abs/2304.14475\nLiesenfeld, A., Lopez, A., & Dingemanse, M. (2023). Opening up chatgpt: Tracking\nopenness, transparency, and accountability in instruction-tuned text generators.\nProceedings of the 5th International Conference on Conversational User Interfaces,\n1–6.\nLin, B. Y., Fu, Y., Yang, K., Ammanabrolu, P., Brahman, F., Huang, S., Bhagavatula, C.,\nChoi, Y., & Ren, X. (2023). Swiftsage: A generative agent with fast and slow\nthinking for complex interactive tasks. https://arxiv.org/abs/2305.17390\nLiscio, E., Araque, O., Gatti, L., Constantinescu, I., Jonker, C., Kalimeri, K., &\nMurukannaiah, P. K. (2023). What does a text classiﬁer learn about morality? an\nexplainable method for cross-domain comparison of moral rhetoric.Proceedings of\nthe 61st Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers),1 4 1 1 3 – 1 4 1 3 2 .\nPERILS AND OPPORTUNITIES OF LLMS 37\nLiu, H., Ning, R., Teng, Z., Liu, J., Zhou, Q., & Zhang, Y. (2023). Evaluating the logical\nreasoning ability of chatgpt and gpt-4. https://arxiv.org/abs/2304.03439\nLiu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., & Neubig, G. (2023). Pre-train, prompt,\nand predict: A systematic survey of prompting methods in natural language\nprocessing. ACM Computing Surveys, 55(9), 1–35.\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,\nZettlemoyer, L., & Stoyanov, V. (2019). Roberta: A robustly optimized bert\npretraining approach. https://arxiv.org/abs/1907.11692\nLlorens, A., Tzovara, A., Bellier, L., Bhaya-Grossman, I., Bidet-Caulet, A., Chang, W. K.,\nCross, Z. R., Dominguez-Faus, R., Flinker, A., Fonken, Y., et al. (2021). Gender\nbias in academia: A lifetime problem that needs solutions.Neuron, 109(13),\n2047–2074. https://doi.org/10.1016/j.neuron.2021.06.002\nLo, C. K. (2023). What is the impact of chatgpt on education? a rapid review of the\nliterature. Education Sciences, 13(4), 410.\nLu, Y., Bartolo, M., Moore, A., Riedel, S., & Stenetorp, P. (2021). Fantastically ordered\nprompts and where to ﬁnd them: Overcoming few-shot prompt order sensitivity.\nhttps://arxiv.org/abs/2104.08786\nMao, R., Liu, Q., He, K., Li, W., & Cambria, E. (2022). The biases of pre-trained language\nmodels: An empirical study on prompt-based sentiment analysis and emotion\ndetection. IEEE Transactions on A\u0000ective Computing.\nMarkowitz, D. M. (2023). Can generative ai infer thinking style from language? questioning\nthe utility of ai as a psychological text analysis tool. https://psyarxiv.com/zy8gr/\nMcClelland, J. L., & Rumelhart, D. E. (1985). Distributed memory and the representation\nof general and speciﬁc information.Journal of Experimental Psychology: General,\n114(2), 159.\nPERILS AND OPPORTUNITIES OF LLMS 38\nMeade, N., Poole-Dayan, E., & Reddy, S. (2021). An empirical survey of the e\u0000ectiveness\nof debiasing techniques for pre-trained language models.\nhttps://arxiv.org/abs/2110.08527\nMedin, D., Ojalehto, B., Marin, A., & Bang, M. (2017). Systems of (non-) diversity.Nature\nHuman Behaviour, 1(5), 0088.\nMichaux, C. (2023). Can chat gpt be considered an author? i met with chat gpt and asked\nsome questions about philosophy of art and philosophy of mind.Available at SSRN\n4439607.\nMischel, W. (2013).Personality and assessment. Psychology Press.\nMishra, A., Soni, U., Arunkumar, A., Huang, J., Kwon, B. C., & Bryan, C. (2023).\nPromptaid: Prompt exploration, perturbation, testing and iteration using visual\nanalytics for large language models. https://arxiv.org/abs/2304.01964\nMotoki, F., Pinho Neto, V., & Rodrigues, V. (2023). More human than human: Measuring\nchatgpt political bias.Available at SSRN 4372349.\nMuthukrishna, M., & Henrich, J. (2019). A problem in theory.Nature Human Behaviour,\n3(3), 221–229.\nNosek, B. A., Hardwicke, T. E., Moshontz, H., Allard, A., Corker, K. S., Dreber, A.,\nFidler, F., Hilgard, J., Kline Struhl, M., Nuijten, M. B., et al. (2022). Replicability,\nrobustness, and reproducibility in psychological science.Annual review of\npsychology, 73,7 1 9 – 7 4 8 .\nOchsner, K. N., Ray, R. R., Hughes, B., McRae, K., Cooper, J. C., Weber, J.,\nGabrieli, J. D., & Gross, J. J. (2009). Bottom-up and top-down processes in\nemotion generation: Common and distinct neural mechanisms.Psychological\nScience, 20(11), 1322–1331.\nOmrani, A., Salkhordeh Ziabari, A., Yu, C., Golazizian, P., Kennedy, B., Atari, M., Ji, H.,\n& Dehghani, M. (2023). Social-group-agnostic bias mitigation via the stereotype\ncontent model.Proceedings of the 61st Annual Meeting of the Association for\nPERILS AND OPPORTUNITIES OF LLMS 39\nComputational Linguistics (Volume 1: Long Papers),4 1 2 3 – 4 1 3 9 .\nhttps://aclanthology.org/2023.acl-long.227\nOpenAI. (2022, November). Introducing chatgpt.\nOpenAI. (2023). Gpt-4 technical report. https://arxiv.org/abs/2303.08774\nPacini, R., & Epstein, S. (1999). The relation of rational and experiential information\nprocessing styles to personality, basic beliefs, and the ratio-bias phenomenon.\nJournal of Personality and Social Psychology, 76(6), 972.\nPangakis, N., Wolken, S., & Fasching, N. (2023). Automated annotation with generative ai\nrequires validation. https://arxiv.org/abs/2306.00176\nPark, P. S., Goldstein, S., O’Gara, A., Chen, M., & Hendrycks, D. (2023). Ai deception: A\nsurvey of examples, risks, and potential solutions.arXiv preprint arXiv:2308.14752.\nPark, P. S., Schoenegger, P., & Zhu, C. (2023). Diminished diversity-of-thought in a\nstandard large language model. https://arxiv.org/abs/2302.07267\nPaul, J., Ueno, A., & Dennis, C. (2023). Chatgpt and consumers: Beneﬁts, pitfalls and\nfuture research agenda.\nPennebaker, J. W., Francis, M. E., & Booth, R. J. (2001). Linguistic inquiry and word\ncount: Liwc 2001.Mahway: Lawrence Erlbaum Associates, 71(2001), 2001.\nPournaras, E. (2023). Science in the era of chatgpt, large language models and ai:\nChallenges for research ethics review and how to respond.\nhttps://arxiv.org/abs/2305.15299\nRadford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al. (2018). Improving language\nunderstanding by generative pre-training.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. (2019). Language\nmodels are unsupervised multitask learners.OpenAI Blog, 1(8), 9.\nRahman, A. U., & Halim, Z. (2022). Predicting the big ﬁve personality traits from\nhand-written text features through semi-supervised learning.Multimedia Tools and\nApplications, 81(23), 33671–33687.\nPERILS AND OPPORTUNITIES OF LLMS 40\nRamezani, A., & Xu, Y. (2023). Knowledge of cultural moral norms in large language\nmodels. https://arxiv.org/abs/2306.01857\nRao, H., Leung, C., & Miao, C. (2023). Can chatgpt assess human personalities? a general\nevaluation framework. https://arxiv.org/abs/2303.01248\nRathje, S., Mirea, D.-M., Sucholutsky, I., Marjieh, R., Robertson, C., & Van Bavel, J. J.\n(2023). Gpt is an e\u0000ective tool for multilingual psychological text analysis.\nhttps://psyarxiv.com/sekf5/\nRay, P. P. (2023). Chatgpt: A comprehensive review on background, applications, key\nchallenges, bias, ethics, limitations and future scope.Internet of Things and\nCyber-Physical Systems.\nReimers, N., & Gurevych, I. (2019). Sentence-bert: Sentence embeddings using siamese\nbert-networks. https://arxiv.org/abs/1908.10084\nReiss, M. V. (2023). Testing the reliability of chatgpt for text annotation and classiﬁcation:\nA cautionary remark.arXiv preprint arXiv:2304.11085.\nReynolds, L., & McDonell, K. (2021). Prompt programming for large language models:\nBeyond the few-shot paradigm.Extended Abstracts of the 2021 CHI Conference on\nHuman Factors in Computing Systems,1 – 7 .\nRomera-Paredes, B., & Torr, P. (2015). An embarrassingly simple approach to zero-shot\nlearning. International conference on machine learning,2 1 5 2 – 2 1 6 1 .\nRoose, K. (2022). The brilliance and weirdness of chatgpt.The New York Times.\nRuder, S., Peters, M. E., Swayamdipta, S., & Wolf, T. (2019). Transfer learning in natural\nlanguage processing.Proceedings of the 2019 conference of the North American\nchapter of the association for computational linguistics: Tutorials,1 5 – 1 8 .\nRumelhart, D. E., Hinton, G. E., McClelland, J. L., et al. (1986). A general framework for\nparallel distributed processing.Parallel Distributed Processing: Explorations in the\nMicrostructure of Cognition,4 5 – 7 6 .\nPERILS AND OPPORTUNITIES OF LLMS 41\nRumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by\nback-propagating errors.Nature, 323(6088), 533–536.\nRussell, S. (2019).Human compatible: Artiﬁcial intelligence and the problem of control.\nPenguin.\nRutinowski, J., Franke, S., Endendyk, J., Dormuth, I., & Pauly, M. (2023). The\nself-perception and political biases of chatgpt. https://arxiv.org/abs/2304.07333\nSanturkar, S., Durmus, E., Ladhak, F., Lee, C., Liang, P., & Hashimoto, T. (2023). Whose\nopinions do language models reﬂect? https://arxiv.org/abs/2303.17548\nScao, T. L., Fan, A., Akiki, C., Pavlick, E., IliÊ, S., Hesslow, D., Castagné, R.,\nLuccioni, A. S., Yvon, F., Gallé, M., et al. (2022). Bloom: A 176b-parameter\nopen-access multilingual language model.ArXiv Preprint arXiv:2211.05100.\nSchwarz, N. (1999). Self-reports: How the questions shape the answers.American\npsychologist, 54(2), 93.\nSchwarz, N., Bless, H., Strack, F., Klumpp, G., Rittenauer-Schatka, H., & Simons, A.\n(1991). Ease of retrieval as information: Another look at the availability heuristic.\nJournal of Personality and Social psychology, 61(2), 195.\nShaikh, O., Zhang, H., Held, W., Bernstein, M., & Yang, D. (2022). On second thought,\nlet’s not think step by step! bias and toxicity in zero-shot reasoning.\nhttps://arxiv.org/abs/2212.08061\nShi\u0000rin, R., & Mitchell, M. (2023). Probing the psychology of ai models.Proceedings of the\nNational Academy of Sciences, 120(10), e2300963120.\nSi, C., Gan, Z., Yang, Z., Wang, S., Wang, J., Boyd-Graber, J., & Wang, L. (2022).\nPrompting gpt-3 to be reliable. https://arxiv.org/abs/2210.09150\nSinghal, K., Azizi, S., Tu, T., Mahdavi, S. S., Wei, J., Chung, H. W., Scales, N.,\nTanwani, A., Cole-Lewis, H., Pfohl, S., et al. (2023). Large language models encode\nclinical knowledge.Nature.\nPERILS AND OPPORTUNITIES OF LLMS 42\nSloan, L., Joyner, M., Stakeman, C., & Schmitz, C. (2018).Critical multiculturalism and\nintersectionality in a complex world. Oxford University Press.\nSnell, J., Swersky, K., & Zemel, R. (2017). Prototypical networks for few-shot learning.\nAdvances in neural information processing systems, 30.\nSoice, E. H., Rocha, R., Cordova, K., Specter, M., & Esvelt, K. M. (2023). Can large\nlanguage models democratize access to dual-use biotechnology?arXiv preprint\narXiv:2306.03809.\nSpirling, A. (2023). Why open-source generative ai models are an ethical way forward for\nscience. Nature, 616(7957), 413–413.\nStickland, A. C., & Murray, I. (2019). Bert and pals: Projected attention layers for e\u0000cient\nadaptation in multi-task learning.International Conference on Machine Learning,\n5986–5995.\nSundar, S. S., & Liao, M. (2023). Calling bs on chatgpt: Reﬂections on ai as a\ncommunication source.Journalism & Communication Monographs, 25(2), 165–180.\nSuri, G., Slater, L. R., Ziaee, A., & Nguyen, M. (2023). Do large language models show\ndecision heuristics similar to humans? a case study using gpt-3.5.\nhttps://arxiv.org/abs/2305.04400\nSutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural\nnetworks. Advances in Neural Information Processing Systems, 27.\nSzabo, A. (2023). Chatgpt is a breakthrough in science and education but fails a test in\nsports and exercise psychology.Baltic Journal of Sport and Health Sciences, 1(128),\n25–40.\nTausczik, Y. R., & Pennebaker, J. W. (2010). The psychological meaning of words: Liwc\nand computerized text analysis methods.Journal of Language and Social\nPsychology, 29(1), 24–54.\nThalmayer, A. G., Toscanelli, C., & Arnett, J. J. (2021). The neglected 95% revisited: Is\namerican psychology becoming less american?American Psychologist, 76(1), 116.\nPERILS AND OPPORTUNITIES OF LLMS 43\nTian, H., Lu, W., Li, T. O., Tang, X., Cheung, S.-C., Klein, J., & Bissyandé, T. F. (2023).\nIs chatgpt the ultimate programming assistant–how far is it?\nhttps://arxiv.org/abs/2304.11938\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T.,\nRozière, B., Goyal, N., Hambro, E., Azhar, F., et al. (2023). Llama: Open and\ne\u0000cient foundation language models. https://arxiv.org/abs/2302.13971\nTrager, J., Ziabari, A. S., Davani, A. M., Golazazian, P., Karimi-Malekabadi, F.,\nOmrani, A., Li, Z., Kennedy, B., Reimer, N. K., Reyes, M., et al. (2022). The moral\nfoundations reddit corpus. https://arxiv.org/abs/2208.05545\nTurc, I., Chang, M.-W., Lee, K., & Toutanova, K. (2019). Well-read students learn better:\nOn the importance of pre-training compact models.\nhttps://arxiv.org/abs/1908.08962\nVan Dis, E. A., Bollen, J., Zuidema, W., van Rooij, R., & Bockting, C. L. (2023). Chatgpt:\nFive priorities for research.Nature, 614(7947), 224–226.\nVan Rossum, G., & Drake, F. L. (2009).Python 3 reference manual. CreateSpace.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, £., &\nPolosukhin, I. (2017). Attention is all you need.Advances in Neural Information\nProcessing Systems, 30.\nVazire, S. (2018). Implications of the credibility revolution for productivity, creativity, and\nprogress. Perspectives on Psychological Science, 13(4), 411–417.\nWang, W., Wei, F., Dong, L., Bao, H., Yang, N., & Zhou, M. (2020). Minilm: Deep\nself-attention distillation for task-agnostic compression of pre-trained transformers.\nAdvances in Neural Information Processing Systems, 33,5 7 7 6 – 5 7 8 8 .\nWang, Z., Zhang, G., Yang, K., Shi, N., Zhou, W., Hao, S., Xiong, G., Li, Y., Sim, M. Y.,\nChen, X., et al. (2023). Interactive natural language processing.\nhttps://arxiv.org/abs/2305.13246\nPERILS AND OPPORTUNITIES OF LLMS 44\nWebb, T., Holyoak, K. J., & Lu, H. (2022). Emergent analogical reasoning in large\nlanguage models. https://arxiv.org/abs/2212.09196\nWebster, D. M., & Kruglanski, A. W. (1994). Need for closure scale.Journal of Personality\nand Social Psychology.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al.\n(2022). Chain-of-thought prompting elicits reasoning in large language models.\nAdvances in Neural Information Processing Systems, 35,2 4 8 2 4 – 2 4 8 3 7 .\nWindsor, D. (2006). Corporate social responsibility: Three key approaches.Journal of\nManagement Studies, 43(1), 93–114.\nWong, J., & Kim, J. (2023). Chatgpt is more likely to be perceived as male than female.\nhttps://arxiv.org/abs/2305.12564\nWu, Z., Qiu, L., Ross, A., Akyürek, E., Chen, B., Wang, B., Kim, N., Andreas, J., &\nKim, Y. (2023). Reasoning or reciting? exploring the capabilities and limitations of\nlanguage models through counterfactual tasks. https://arxiv.org/abs/2307.02477\nXian, Y., Lampert, C. H., Schiele, B., & Akata, Z. (2018). Zero-shot learning—a\ncomprehensive evaluation of the good, the bad and the ugly.IEEE transactions on\npattern analysis and machine intelligence, 41(9), 2251–2265.\nYang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R. R., & Le, Q. V. (2019). Xlnet:\nGeneralized autoregressive pretraining for language understanding.Advances in\nNeural Information Processing Systems, 32.\nZakrisson, I. (2005). Construction of a short version of the right-wing authoritarianism\n(rwa) scale.Personality and Individual Di\u0000erences, 39(5), 863–872.\nZhai, X. (2023). Chatgpt for next generation science learning.XRDS: Crossroads, The\nACM Magazine for Students, 29(3), 42–46.\nZhang, C., Zhang, C., Li, C., Qiao, Y., Zheng, S., Dam, S. K., Zhang, M., Kim, J. U.,\nKim, S. T., Choi, J., et al. (2023). One small step for generative ai, one giant leap\nfor agi: A complete survey on chatgpt in aigc era. https://arxiv.org/abs/2304.06488\nPERILS AND OPPORTUNITIES OF LLMS 45\nZhong, Q., Ding, L., Liu, J., Du, B., & Tao, D. (2023). Can chatgpt understand too? a\ncomparative study on chatgpt and ﬁne-tuned bert.\nhttps://arxiv.org/abs/2302.10198\nZhu, Y., Zhang, P., Haq, E.-U., Hui, P., & Tyson, G. (2023). Can chatgpt reproduce\nhuman-generated labels? a study of social computing tasks.\nhttps://arxiv.org/abs/2304.10145\nZiegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D.,\nChristiano, P., & Irving, G. (2019). Fine-tuning language models from human\npreferences. arXiv preprint arXiv:1909.08593.",
  "topic": "Psychological research",
  "concepts": [
    {
      "name": "Psychological research",
      "score": 0.5505849123001099
    },
    {
      "name": "Psyche",
      "score": 0.4636728763580322
    },
    {
      "name": "Psychology",
      "score": 0.4515126049518585
    },
    {
      "name": "Social psychology",
      "score": 0.37689408659935
    },
    {
      "name": "Social science",
      "score": 0.3250146508216858
    },
    {
      "name": "Sociology",
      "score": 0.21509864926338196
    },
    {
      "name": "Psychoanalysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1174212",
      "name": "University of Southern California",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I136199984",
      "name": "Harvard University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I63966007",
      "name": "Massachusetts Institute of Technology",
      "country": "US"
    }
  ]
}