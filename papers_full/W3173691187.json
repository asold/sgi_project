{
  "title": "G-Transformer for Document-Level Machine Translation",
  "url": "https://openalex.org/W3173691187",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3092229890",
      "name": "Guangsheng Bao",
      "affiliations": [
        "Institute for Advanced Study",
        "Westlake University"
      ]
    },
    {
      "id": "https://openalex.org/A2098449489",
      "name": "Yue Zhang",
      "affiliations": [
        "Westlake University",
        "Institute for Advanced Study"
      ]
    },
    {
      "id": "https://openalex.org/A2651952867",
      "name": "Zhiyang Teng",
      "affiliations": [
        "Westlake University",
        "Institute for Advanced Study"
      ]
    },
    {
      "id": "https://openalex.org/A2148341558",
      "name": "Boxing Chen",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2105392445",
      "name": "Weihua Luo",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2964298349",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2962802109",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W2971347700",
    "https://openalex.org/W2891534142",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2514722822",
    "https://openalex.org/W1851962382",
    "https://openalex.org/W2153653739",
    "https://openalex.org/W2989066524",
    "https://openalex.org/W3042199843",
    "https://openalex.org/W2952446148",
    "https://openalex.org/W2888159079",
    "https://openalex.org/W2038698865",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W3107826490",
    "https://openalex.org/W3035520602",
    "https://openalex.org/W2152263452",
    "https://openalex.org/W4241645538",
    "https://openalex.org/W3103878009",
    "https://openalex.org/W2964289193",
    "https://openalex.org/W2962943802",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W203948990",
    "https://openalex.org/W3001434439",
    "https://openalex.org/W3174955334",
    "https://openalex.org/W2006969979",
    "https://openalex.org/W2136353104",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2162245945",
    "https://openalex.org/W2970529093",
    "https://openalex.org/W1753482797",
    "https://openalex.org/W2963506925",
    "https://openalex.org/W2806412155",
    "https://openalex.org/W2964335437",
    "https://openalex.org/W2962712961",
    "https://openalex.org/W2141895568",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W3034351728",
    "https://openalex.org/W2963223306",
    "https://openalex.org/W2437005631",
    "https://openalex.org/W3100623455",
    "https://openalex.org/W2595715041",
    "https://openalex.org/W2806987872",
    "https://openalex.org/W3102507836"
  ],
  "abstract": "Guangsheng Bao, Yue Zhang, Zhiyang Teng, Boxing Chen, Weihua Luo. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 3442–3455\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3442\nG-Transformer for Document-level Machine Translation\nGuangsheng Bao1,2, Yue Zhang∗,1,2, Zhiyang Teng1,2, Boxing Chen3 and Weihua Luo3\n1 School of Engineering, Westlake University\n2 Institute of Advanced Technology, Westlake Institute for Advanced Study\n3 DAMO Academy, Alibaba Group Inc.\n{baoguangsheng, zhangyue, tengzhiyang}@westlake.edu.cn\n{boxing.cbx, weihua.luowh}@alibaba-inc.com\nAbstract\nDocument-level MT models are still far from\nsatisfactory. Existing work extend translation\nunit from single sentence to multiple sentences.\nHowever, study shows that when we further en-\nlarge the translation unit to a whole document,\nsupervised training of Transformer can fail. In\nthis paper, we ﬁnd such failure is not caused by\noverﬁtting, but by sticking around local min-\nima during training. Our analysis shows that\nthe increased complexity of target-to-source at-\ntention is a reason for the failure. As a solution,\nwe propose G-Transformer, introducing local-\nity assumption as an inductive bias into Trans-\nformer, reducing the hypothesis space of the\nattention from target to source. Experiments\nshow that G-Transformer converges faster and\nmore stably than Transformer, achieving new\nstate-of-the-art BLEU scores for both non-\npretraining and pre-training settings on three\nbenchmark datasets.\n1 Introduction\nDocument-level machine translation (MT) has re-\nceived increasing research attention (Gong et al.,\n2011; Hardmeier et al., 2013; Garcia et al., 2015;\nMiculicich et al., 2018a; Maruf et al., 2019; Liu\net al., 2020). It is a more practically useful task\ncompared to sentence-level MT because typical in-\nputs in MT applications are text documents rather\nthan individual sentences. A salient difference be-\ntween document-level MT and sentence-level MT\nis that for the former, much larger inter-sentential\ncontext should be considered when translating\neach sentence, which include discourse structures\nsuch as anaphora, lexical cohesion, etc. Studies\nshow that human translators consider such contexts\nwhen conducting document translation (Hardmeier,\n2014; L¨aubli et al., 2018). Despite that neural mod-\nels achieve competitive performances on sentence-\n∗* Corresponding author.\nContext Encoder Source Encoder Target Decoder\nSource\nTranslation\nContext\n(a) Sentence-by-sentence Translation\nSource Encoder Target Decoder\nSource1\nTranslation1\nSource2 …\nTranslation2 …\n(b) Multi-sentence Translation\nSource1\nTranslation1\nSource2 …\nTranslation2 …\n… …\n① ② ③ ① ② ③\n(c) G-Transformer (Doc-by-doc Translation)\nFigure 1: Overview of model structures for document-\nlevel machine translation.\nlevel MT, the performance of document-level MT\nis still far from satisfactory.\nExisting methods can be mainly classiﬁed into\ntwo categories. The ﬁrst category translates a doc-\nument sentence by sentence using a sequence-to-\nsequence neural model (Zhang et al., 2018; Mi-\nculicich et al., 2018b; Maruf et al., 2019; Zheng\net al., 2020). Document-level context is integrated\ninto sentence-translation by introducing additional\ncontext encoder. The structure of such a model\nis shown in Figure 1(a). These methods suffer\nfrom two limitations. First, the context needs to be\nencoded separately for translating each sentence,\nwhich adds to the runtime complexity. Second,\nmore importantly, information exchange cannot be\nmade between the current sentence and its docu-\nment context in the same encoding module.\nThe second category extends the translation\nunit from a single sentence to multiple sentences\n(Tiedemann and Scherrer, 2017; Agrawal et al.,\n3443\n2018; Zhang et al., 2020) and the whole document\n(Junczys-Dowmunt, 2019; Liu et al., 2020). Re-\ncently, it has been shown that when the translation\nunit increases from one sentence to four sentences,\nthe performance improves (Zhang et al., 2020;\nScherrer et al., 2019). However, when the whole\ndocument is encoded as a single unit for sequence\nto sequence translation, direct supervised training\nhas been shown to fail (Liu et al., 2020). As a\nsolution, either large-scale pre-training (Liu et al.,\n2020) or data augmentation (Junczys-Dowmunt,\n2019) has been used as a solution, leading to im-\nproved performance. These methods are shown in\nFigure 1(b). One limitation of such methods is that\nthey require much more training time due to the\nnecessity of data augmentation.\nIntuitively, encoding the whole input document\nas a single unit allows the best integration of con-\ntext information when translating the current sen-\ntence. However, little work has been done investi-\ngating the underlying reason why it is difﬁcult to\ntrain such a document-level NMT model. One re-\nmote clue is that as the input sequence grows larger,\nthe input becomes more sparse (Pouget-Abadie\net al., 2014; Koehn and Knowles, 2017). To gain\nmore understanding, we make dedicated experi-\nments on the inﬂuence of input length, data scale\nand model size for Transformer (Section 3), ﬁnd-\ning that a Transformer model can fail to converge\nwhen training with long sequences, small datasets,\nor big model size. We further ﬁnd that for the failed\ncases, the model gets stuck at local minima during\ntraining. In such situation, the attention weights\nfrom the decoder to the encoder are ﬂat, with large\nentropy values. This can be because that larger\ninput sequences increase the challenge for focusing\non a local span to translate when generating each\ntarget word. In other words, the hypothesis space\nfor target-to-source attention is increased.\nGiven the above observations, we investigate a\nnovel extension of Transformer, by restricting self-\nattention and target-to-source attention to a local\ncontext using a guidance mechanism. As shown in\nFigure 1(c), while we still encode the input docu-\nment as a single unit, group tags 1⃝ 2⃝ 3⃝are as-\nsigned to sentences to differentiate their positions.\nTarget-to-source attention is guided by matching\nthe tag of target sentence to the tags of source sen-\ntences when translating each sentence, so that the\nhypothesis space of attention is reduced. Intuitively,\nthe group tags serve as a constraint on attention,\nwhich is useful for differentiating the current sen-\ntence and its context sentences. Our model, named\nG-Transformer, can be thus viewed as a combina-\ntion of the method in Figure 1(a) and Figure 1(b),\nwhich fully separate and fully integrates a sentence\nbeing translated with its document level context,\nrespectively.\nWe evaluate our model on three commonly\nused document-level MT datasets for English-\nGerman translation, covering domains of TED\ntalks, News, and Europarl from small to large.\nExperiments show that G-Transformer converges\nfaster and more stably than Transformer on dif-\nferent settings, obtaining the state-of-the-art re-\nsults under both non-pretraining and pre-training\nsettings. To our knowledge, we are the ﬁrst\nto realize a truly document-by-document transla-\ntion model. We release our code and model at\nhttps://github.com/baoguangsheng/g-transformer.\n2 Experimental Settings\nWe evaluate Transformer and G-Transformer on\nthe widely adopted benchmark datasets (Maruf\net al., 2019), including three domains for English-\nGerman (En-De) translation.\nTED. The corpus is transcriptions of TED talks\nfrom IWSLT 2017. Each talk is used as a document,\naligned at the sentence level. tst2016-2017 is used\nfor testing, and the rest for development.\nNews. This corpus uses News Commentary\nv11 for training, which is document-delimited and\nsentence-aligned. newstest2015 is used for devel-\nopment, and newstest2016 for testing.\nEuroparl. The corpus is extracted from Eu-\nroparl v7, where sentences are segmented and\naligned using additional information. The train,\ndev and test sets are randomly split from the cor-\npus.\nThe detailed statistics of these corpora are shown\nin Table 1. We pre-process the documents by split-\nting them into instances with up-to 512 tokens, tak-\ning a sentence as one instance if its length exceeds\n512 tokens. We tokenize and truecase the sentences\nwith MOSES (Koehn et al., 2007) tools, applying\nBPE (Sennrich et al., 2016) with 30000 merging\noperations.\nWe consider three standard model conﬁgura-\ntions.\nBase Model. Following the standard Trans-\nformer base model (Vaswani et al., 2017), we use 6\nlayers, 8 heads, 512 dimension outputs, and 2048\n3444\nLanguage Dataset #Sentences #Documents #Instances Avg #Sents/Inst Avg #Tokens/Inst\ntrain/dev/test train/dev/test train/dev/test train/dev/test train/dev/test\nEn-De\nTED 0.21M/9K/2.3K 1.7K/92/22 11K/483/123 18.3/18.5/18.3 436/428/429\nNews 0.24M/2K/3K 6K/80/154 18.5K/172/263 12.8/12.6/11.3 380/355/321\nEuroparl 1.67M/3.6K/5.1K 118K/239/359 162K/346/498 10.3/10.4/10.3 320/326/323\nTable 1: En-De datasets for evaluation.\n-5\n0\n5\n10\n15\n20\n25\n30\n35d-BLEU\nTokens\n64 128 256 512 1024\n(a) Input Length (Base model\nwith ﬁltered data.)\n-5\n0\n5\n10\n15\n20\n25\n30\n35d-BLEU\nInstances\n1.25K 2.5K 5K 10K 20K 40K 80K 160K\n(b) Data Scale (Base model\nwith 512 tokens input.)\nFigure 2: Transformer on various input length and data\nscale.\ndimension hidden vectors.\nBig Model. We follow the standard Transformer\nbig model (Vaswani et al., 2017), using 6 layers, 16\nheads, 1024 dimension outputs, and 4096 dimen-\nsion hidden vectors.\nLarge Model. We use the same settings of\nBART large model (Lewis et al., 2020), which in-\nvolves 12 layers, 16 heads, 1024 dimension outputs,\nand 4096 dimension hidden vectors.\nWe use s-BLEU and d-BLEU (Liu et al., 2020)\nas the metrics. The detailed descriptions are in\nAppendix A.\n3 Transformer and Long Inputs\nWe empirically study Transformer (see Appendix\nB) on the datasets. We run each experiment ﬁve\ntimes using different random seeds, reporting the\naverage score for comparison.\n3.1 Failure Reproduction\nInput Length. We use the Base model and ﬁxed\ndataset for this comparison. We split both the train-\ning and testing documents from Europarl dataset\ninto instances with input length of 64, 128, 256,\n512, and 1024 tokens, respectively. For fair com-\nparison, we remove the training documents with a\nlength of less than 768 tokens, which may favour\nsmall input length. The results are shown in Fig-\nure 2a. When the input length increases from 256\ntokens to 512 tokens, the BLEU score drops dra-\nmatically from 30.5 to 2.3, indicating failed train-\ning with 512 and 1024 tokens. It demonstrates the\ndifﬁculty when dealing with long inputs of Trans-\n2\n4\n6\n8\n10\n12\n0K 2K 4K 6K 8K 10K 12K\nLoss\nSteps\nTrain Valid\n(a) Failed Model\n2\n4\n6\n8\n10\n12\n0K 10K 20K 30K 40K 50K 60K\nLoss\nSteps\nTrain Valid (b) Successful Model\nFigure 3: Loss curve of the models and the local min-\nima.\nformer.\nData Scale. We use the Base model and a ﬁxed\ninput length of 512 tokens. For each setting, we\nrandomly sample a training dataset of the expected\nsize from the full dataset of Europarl. The results\nare shown in Figure 2b. The performance increases\nsharply when the data scale increases from 20K to\n40K. When data scale is equal or less than 20K, the\nBLEU scores are under 3, which is unreasonably\nlow, indicating that with a ﬁxed model size and\ninput length, the smaller dataset can also cause\nthe failure of the training process. For data scale\nmore than 40K, the BLEU scores show a wide\ndynamic range, suggesting that the training process\nis unstable.\nModel Size. We test Transformer with different\nmodel sizes, using the full dataset of Europarl and a\nﬁxed input length of 512 tokens. Transformer-Base\ncan be trained successfully, giving a reasonable\nBLEU score. However, the training of the Big and\nLarge models failed, resulting in very low BLEU\nscores under 3. It demonstrates that the increased\nmodel size can also cause the failure with a ﬁxed\ninput length and data scale.\nThe results conﬁrm the intuition that the per-\nformance will drop with longer inputs, smaller\ndatasets, or bigger models. However, the BLEU\nscores show a strong discontinuity with the change\nof input length, data scale, or model size, falling\ninto two discrete clusters. One is successfully\ntrained cases with d-BLEU scores above 10, and\nthe other is failed cases with d-BLEU scores under\n3.\n3445\n7.6\n7.8\n8\n8.2\n8.4\n0K 2K 4K 6K 8K 10K 12K\nEntropy (bit)\nSteps\nTrain Valid\n(a) Failed Model\n5\n6\n7\n8\n9\n0K 10K 20K 30K 40K 50K 60K\nEntropy (bit)\nSteps\nTrain Valid (b) Successful Model\nFigure 4: Cross-attention distribution of Transformer\nshows that the failed model sticks at the local minima.\n4\n5\n6\n7\n8\n9\n0K 10K 20K 30K 40K 50K 60K\nEntropy (bit)\nSteps\nTrain Valid\n(a) Encoder\n4.6\n5\n5.4\n5.8\n6.2\n6.6\n7\n0K 10K 20K 30K 40K 50K 60K\nEntropy (bit)\nSteps\nTrain Valid (b) Decoder\nFigure 5: For the successful model, the attention distri-\nbution shrinks to narrow range (low entropy) and then\nexpands to wider range (high entropy).\n3.2 Failure Analysis\nTraining Convergence. Looking into the failed\nmodels, we ﬁnd that they have a similar pattern on\nloss curves. As an example of the model trained\non 20K instances shown in Figure 3a, although the\ntraining loss continually decreases during training\nprocess, the validation loss sticks at the level of 7,\nreaching a minimum value at around 9K training\nsteps. In comparison, the successfully trained mod-\nels share another pattern. Taking the model trained\non 40K instances as an example, the loss curves\ndemonstrate two stages, which is shown in Figure\n3b. In the ﬁrst stage, the validation loss similar\nto the failed cases has a converging trend to the\nlevel of 7. In the second stage, after 13K training\nsteps, the validation loss falls suddenly, indicating\nthat the model may escape successfully from local\nminima. From the two stages of the learning curve,\nwe conclude that the real problem, contradicting\nour ﬁrst intuition, is not about overﬁtting, but about\nlocal minima.\nAttention Distribution. We further look into\nthe attention distribution of the failed models, ob-\nserving that the attentions from target to source are\nwidely spread over all tokens. As Figure 4a shows,\nthe distribution entropy is high for about 8.14 bits\non validation. In contrast, as shown in Figure 4b,\nthe successfully trained model has a much lower\nattention entropy of about 6.0 bits on validation.\nFurthermore, we can see that before 13K training\nSource: <s> the Commission shares ... of the European Union institu-\ntional framework . </s> 1 <s> Commission participation is expressly\nprovided for ... of all its preparatory bodies . </s> 2 <s> only in excep-\ntional circumstances ... be excluded from these meetings . </s> 3 ...\nTarget: <s> die Kommission teilt die Ansicht ... des institutionellen\nRahmens der Europischen Union ist . </s> 1 <s> die Geschftsordnung\ndes Rates ... der Kommission damit ausdrcklich vor . </s> 2 <s> die\nKommission kann nur ... wobei fallweise zu entscheiden ist . </s> 3 ...\nFigure 6: Example of English-German translation with\ngroup alignments.\nsteps, the entropy sticks at a plateau, conﬁrming\nwith the observation of the local minima in Figure\n3b. It indicates that the early stage of the training\nprocess for Transformer is difﬁcult.\nFigure 5 shows the self-attention distributions\nof the successfully trained models. The attention\nentropy of both the encoder and the decoder drops\nfast at the beginning, leading to a shrinkage of\nthe attention range. But then the attention entropy\ngradually increases, indicating an expansion of the\nattention range. Such back-and-forth oscillation\nof the attention range may also result in unstable\ntraining and slow down the training process.\n3.3 Conclusion\nThe above experiments show that training failure\non Transformer can be caused by local minima.\nAdditionally, the oscillation of attention range may\nmake it worse. During training process, the atten-\ntion module needs to identify relevant tokens from\nwhole sequence to attend to. Assuming that the\nsequence length is N, the complexity of the at-\ntention distribution increases when N grows from\nsentence-level to document-level.\nWe propose to use locality properties (Rizzi,\n2013; Hardmeier, 2014; Jawahar et al., 2019) of\nboth the language itself and the translation task as\na constraint in Transformer, regulating the hypoth-\nesis space of the self-attention and target-to-source\nattention, using a simple group tag method.\n4 G-Transformer\nAn example of G-Transformer is shown in Fig-\nure 6, where the input document contains more\nthan 3 sentences. As can be seen from the ﬁgure,\nG-Transformer extends Transformer by augment-\ning the input and output with group tags (Bao and\nZhang, 2021). In particular, each token is assigned\na group tag, indicating its sentential index. While\n3446\nsource group tags can be assigned deterministically,\ntarget tags are assigned dynamically according to\nwhether a generated sentence is complete. Start-\ning from 1, target words copy group tags from its\npredecessor unless the previous token is </s>, in\nwhich case the tag increases by 1. The tags serve as\na locality constraint, encouraging target-to-source\nattention to concentrate on the current source sen-\ntence being translated.\nFormally, for a source document X and a target\ndocument Y, the probability model of Transformer\ncan be written as\nˆY = arg max\nY\nP(Y|X), (1)\nand G-Transformer extends it by having\nˆY = argY max\nY,GY\nP(Y,GY |X,GX), (2)\nwhere GX and GY denotes the two sequences of\ngroup tags\nGX = {gi = kif wi ∈sentX\nk else 0}||X|\ni=1,\nGY = {gj = kif wj ∈sentY\nk else 0}||Y |\nj=1,\n(3)\nwhere sentk represents the k-th sentence of X\nor Y. For the example shown in Figure 6,\nGX = {1,..., 1,2,..., 2,3,..., 3,4,...}and GY =\n{1,..., 1,2,..., 2,3,..., 3,4,...}.\nGroup tags inﬂuence the auto-regressive transla-\ntion process by interfering with the attention mech-\nanism, which we show in the next section. In G-\nTransformer, we use the group-tag sequence GX\nand GY for representing the alignment between X\nand Y, and for generating the localized contextual\nrepresentation of X and Y.\n4.1 Group Attention\nAn attention module can be seen as a function map-\nping a query and a set of key-value pairs to an out-\nput (Vaswani et al., 2017). The query, key, value,\nand output are all vectors. The output is computed\nby summing the values with corresponding atten-\ntion weights, which are calculated by matching\nthe query and the keys. Formally, given a set of\nqueries, keys, and values, we pack them into matrix\nQ, K, and V, respectively. We compute the matrix\noutputs\nAttention(Q,K,V ) =softmax\n(QKT\n√dk\n)\nV, (4)\nwhere dk is the dimensions of the key vector.\nAttention allows a model to focus on different\npositions. Further, multi-head attention (MHA)\nallows a model to gather information from different\nrepresentation subspaces\nMHA(Q,K,V ) =Concat(head1,...,head h)WO,\nheadi = Attention(QWQ\ni ,KW K\ni ,VW V\ni ),\n(5)\nwhere the projections of WO, WQ\ni , WK\ni , and WV\ni\nare parameter matrices.\nWe update Eq 4 using group-tags, naming it\ngroup attention (GroupAttn). In addition to inputs\nQ, K, and V, two sequences of group-tag inputs\nare involved, where GQ corresponds to Qand GK\ncorresponds to K. We have\nargs = (Q,K,V,G Q,GK),\nGroupAttn(args) =softmax\n(QKT\n√dk\n+ M(GQ,GK)\n)\nV,\n(6)\nwhere function M(·) works as an attention mask,\nexcluding all tokens outside the sentence. Speciﬁ-\ncally, M(·) gives a big negative number γto make\nsoftmax close to 0 for the tokens with a different\ngroup tag compared to current token\nM(GQ,GK) =min(1,abs(GQIT\nK −IQGT\nK)) ∗γ, (7)\nwhere IK and IQ are constant vectors with value\n1 on all dimensions, that IK has dimensions equal\nto the length of GK and IQ has dimensions equal\nto the length of GQ. The constant value γ can\ntypically be −1e8.\nSimilar to Eq 5, we use group multi-head atten-\ntion\nargs = (Q,K,V,G Q,GK),\nGroupMHA(args) =Concat(head1,...,head h)WO,\n(8)\nwhere\nheadi = GroupAttn(QWQ\ni ,KW K\ni ,VW V\ni ,GQ,GK),\n(9)\nand the projections of WO, WQ\ni , WK\ni , and WV\ni\nare parameter matrices.\nEncoder. For each layer a group multi-head at-\ntention module is used for self-attention, assigning\nthe same group-tag sequence for the key and the\nvalue that GQ = GK = GX.\nDecoder. We use one group multi-head attention\nmodule for self-attention and another group multi-\nhead attention module for cross-attention. Similar\nto the encoder, we assign the same group-tag se-\nquence to the key and value of the self-attention,\nthat GQ = GK = GY , but use different group-tag\nsequences for cross-attention that GQ = GY and\nGK = GX.\n3447\nMethod TED News Europarl\ns-BLEU d-BLEU s-BLEU d-BLEU s-BLEU d-BLEU\nSENT NMT (Vaswani et al., 2017) 23.10 - 22.40 - 29.40 -\nHAN (Miculicich et al., 2018b) 24.58 - 25.03 - 28.60 -\nSAN (Maruf et al., 2019) 24.42 - 24.84 - 29.75 -\nHybrid Context (Zheng et al., 2020) 25.10 - 24.91 - 30.40 -\nFlat-Transformer (Ma et al., 2020) 24.87 - 23.55 - 30.09 -\nTransformer on sent (baseline) 24.82 - 25.19 - 31.37 -\nTransformer on doc (baseline) - 0.76 - 0.60 - 33.10\nG-Transformer random initialized (ours) 23.53 25.84* 23.55 25.23* 32.18* 33.87*\nG-Transformer ﬁne-tuned on sent Transformer (ours) 25.12 27.17* 25.52 27.11* 32.39* 34.08*\nFine-tuning on Pre-trained Model\nFlat-Transformer+BERT (Ma et al., 2020) 26.61 - 24.52 - 31.99 -\nG-Transformer+BERT (ours) 26.81 - 26.14 - 32.46 -\nTransformer on sent ﬁne-tuned on BART (baseline) 27.78 - 29.90 - 31.87 -\nTransformer on doc ﬁne-tuned on BART (baseline) - 28.29 - 30.49 - 34.00\nG-Transformer ﬁne-tuned on BART (ours) 28.06 30.03* 30.34* 31.71* 32.74* 34.31*\nTable 2: Case-sensitive BLEU scores on En-De translation. “*” indicates statistically signiﬁcant at p < 0.01\ncompared to the Transformer baselines.\nComplexity. Consider a document with M sen-\ntences and N tokens, where each sentence con-\ntains N/Mtokens on average. The complexities of\nboth the self-attention and cross-attention in Trans-\nformer are O(N2). In contrast, the complexity\nof group attention in G-Transformer is O(N2/M)\ngiven the fact that the attention is restricted to a\nlocal sentence. Theoretically, since the average\nlength N/M of sentences tends to be constant, the\ntime and memory complexities of group attention\nare approximately O(N), making training and in-\nference on very long inputs feasible.\n4.2 Combined Attention\nWe use only group attention on lower layers for\nlocal sentence representation, and combined atten-\ntion on top layers for integrating local and global\ncontext information. We use the standard multi-\nhead attention in Eq 5 for global context, naming it\nglobal multi-head attention (GlobalMHA). Group\nmulti-head attention in Eq 8 and global multi-head\nattention are combined using a gate-sum module\n(Zhang et al., 2016; Tu et al., 2017)\nHL = GroupMHA(Q,K,V,G Q,GK),\nHG = GlobalMHA(Q,K,V ),\ng= sigmoid([HL,HG]W + b),\nH = HL ⊙g+ HG ⊙(1 −g),\n(10)\nwhere W and bare linear projection parameters,\nand ⊙denotes element-wise multiplication.\nPrevious study (Jawahar et al., 2019) shows that\nthe lower layers of Transformer catch more local\nsyntactic relations, while the higher layers repre-\nsent longer distance relations. Based on these ﬁnd-\nings, we use combined attention only on the top\nlayers for integrating local and global context. By\nthis design, on lower layers, the sentences are iso-\nlated from each other, while on top layers, the cross-\nsentence interactions are enabled. Our experiments\nshow that the top 2 layers with global attention\nare sufﬁcient for document-level NMT, and more\nlayers neither help nor harm the performance.\n4.3 Inference\nDuring decoding, we generate group-tag sequence\nGY according to the predicted token, starting with\n1 at the ﬁrst <s>and increasing 1 after each </s>.\nWe use beam search and apply the maximum length\nconstraint on each sentence. We generate the whole\ndocument from start to end in one beam search\nprocess, using a default beam size of 5.\n5 G-Transformer Results\nWe compare G-Transformer with Transformer base-\nlines and previous document-level NMT models\non both non-pretraining and pre-training settings.\nThe detailed descriptions about these training set-\ntings are in Appendix C.1. We make statistical\nsigniﬁcance test according to Collins et al. (2005).\n5.1 Results on Non-pretraining Settings\nAs shown in Table 2, the sentence-level Trans-\nformer outperforms previous document-level mod-\nels on News and Europarl. Compared to this\nstrong baseline, our randomly initialized model\nof G-Transformer improves the s-BLEU by 0.81\npoint on the large dataset Europarl. The results\non the small datasets TED and News are worse,\nindicating overﬁtting with long inputs. When G-\nTransformer is trained by ﬁne-tuning the sentence-\n3448\nlevel Transformer, the performance improves on\nthe three datasets by 0.3, 0.33, and 1.02 s-BLEU\npoints, respectively.\nDifferent from the baseline of document-level\nTransformer, G-Transformer can be successfully\ntrained on small TED and News. On Europarl,\nG-Transformer outperforms Transformer by 0.77\nd-BLEU point, and G-Transformer ﬁne-tuned on\nsentence-level Transformer enlarges the gap to 0.98\nd-BLEU point.\nG-Transformer outperforms previous document-\nlevel MT models on News and Europarl with a\nsigniﬁcant margin. Compared to the best recent\nmodel Hyrbid-Context, G-Transformer improves\nthe s-BLEU on Europarl by 1.99. These results\nsuggest that in contrast to previous short-context\nmodels, sequence-to-sequence model taking the\nwhole document as input is a promising direction.\n5.2 Results on Pre-training Settings\nThere is relatively little existing work about\ndocument-level MT using pre-training. Although\nFlat-Transformer+BERT gives a state-of-the-art\nscores on TED and Europarl, the score on News is\nworse than previous non-pretraining model HAN\n(Miculicich et al., 2018b). G-Transformer+BERT\nimproves the scores by margin of 0.20, 1.62, and\n0.47 s-BLEU points on TED, News, and Europarl,\nrespectively. It shows that with a better contextual\nrepresentation, we can further improve document-\nlevel MT on pretraining settings.\nWe further build much stronger Transformer\nbaselines by ﬁne-tuning on mBART25 (Liu et al.,\n2020). Taking advantage of sequence-to-sequence\npre-training, the sentence-level Transformer gives\nmuch better s-BLEUs of 27.78, 29.90, and\n31.87, respectively. G-Transformer ﬁne-tuned\non mBART25 improves the performance by 0.28,\n0.44, and 0.87 s-BLEU, respectively. Compared\nto the document-level Transformer baseline, G-\nTransformer gives 1.74, 1.22, and 0.31 higher\nd-BLEU points, respectively. It demonstrates\nthat even with well-trained sequence-to-sequence\nmodel, the locality bias can still enhance the per-\nformance.\n5.3 Convergence\nWe evaluate G-Transformer ad Transformer on var-\nious input length, data scale, and model size to\nbetter understand that to what extent it has solved\nthe convergence problem of Transformer.\n-5\n5\n15\n25\n35\n45d-BLEU\nTokens\n64 128 256 512 1024\nTransformer G-Transformer\n(a) Input Length\n-5\n5\n15\n25\n35\n45d-BLEU\nInstances\n1.25K 2.5K 5K 10K 20K 40K 80K 160K\nTransformer G-Transformer (b) Data Scale\nFigure 7: G-Transformer compared with Transformer.\n3\n4\n5\n6\n7\n8\n9\n0K 10K 20K 30K 40K 50K 60K\nEntropy (bit)\nSteps\nTransformer\nGroup Attention\nGlobal Attention\n(a) Cross-Attention\n3\n4\n5\n6\n7\n8\n9\n0K 10K 20K 30K 40K 50K 60K\nEntropy (bit)\nSteps\nTransformer\nGroup Attention\nGlobal Attention (b) Encoder Self-Attention\nFigure 8: Comparison on the development of cross-\nattention and encoder self-attention.\nInput Length. The results are shown in Figure\n7a. Unlike Transformer, which fails to train on\nlong input, G-Transformer shows stable scores for\ninputs containing 512 and 1024 tokens, suggesting\nthat with the help of locality bias, a long input does\nnot impact the performance obviously.\nData Scale. As shown in Figure 7b, overall G-\nTransformer has a smooth curve of performance on\nthe data scale from 1.25K to 160K. The variances\nof the scores are much lower than Transformer,\nindicating stable training of G-Transformer. Addi-\ntionally, G-Transformer outperforms Transformer\nby a large margin on all the settings.\nModel Size. Unlike Transformer, which fails\nto train on Big and Large model settings, G-\nTransformer shows stable scores on different model\nsizes. As shown in Appendix C.2, although per-\nformance on small datasets TED and News drops\nlargely for Big and Large model, the performance\non large dataset Europarl only decreases by 0.10\nd-BLEU points for the Big model and 0.66 for the\nLarge model.\nLoss. Looking into the training process of the\nabove experiments, we see that both the training\nand validation losses of G-Transformer converge\nmuch faster than Transformer, using almost half\ntime to reach the same level of loss. Furthermore,\nthe validation loss of G-Transformer converges to\nmuch lower values. These observations demon-\nstrate that G-Transformer converges faster and bet-\nter.\nAttention Distribution. Beneﬁting from the\nseparate group attention and global attention, G-\nTransformer avoids the oscillation of attention\n3449\nMethod TED News Europarl Drop\nG-Transformer (fnt.) 25.12 25.52 32.39 -\n- target-side context 25.05 25.41 32.16 -0.14\n- source-side context 24.56 24.58 31.39 -0.70\nTable 3: Impact of source-side and target-side context\nreporting in s-BLEU. Here, fnt. denotes the model ﬁne-\ntuned on sentence-level Transformer.\nMethod deixis el.inﬂ. el.VP\nCADec (V oita et al., 2019b) 81.6 72.2 80.0\nLSTM-Tran (Zhang et al., 2020) 91.0 82.2 78.2\nsent (V oita et al., 2019b) 50.0 53.0 28.4\nconcat (V oita et al., 2019b) 83.5 76.2 76.6\nG-Transformer 89.9 84.8 82.4\nTable 4: Impact on discourse by the source-side con-\ntext, in accuracy of correctly identifying the discourse\nphenomena. Here, el. means ellipsis. LSTM-Tran de-\nnotes LSTM-Transformer.\nrange, which happens to Transformer. As shown\nin Figure 8a, Transformer sticks at the plateau area\nfor about 13K training steps, but G-Transformer\nshows a quick and monotonic convergence, reach-\ning the stable level using about 1/4 of the time that\nTransformer takes. Through Figure 8b, we can ﬁnd\nthat G-Transformer also has a smooth and stable\ncurve for the convergence of self-attention distribu-\ntion. These observations imply that the potential\nconﬂict of local sentence and document context can\nbe mitigated by G-Transformer.\n5.4 Discussion of G-Transformer\nDocument Context. We study the contribution of\nthe source-side and target-side context by remov-\ning the cross-sentential attention in Eq 10 from the\nencoder and the decoder gradually. The results\nare shown in Table 3. We take the G-Transformer\nﬁne-tuned on the sentence-level Transformer as\nour starting point. When we disable the target-\nside context, the performance decreases by 0.14\ns-BLEU point on average, which indicates that the\ntarget-side context does impact translation perfor-\nmance signiﬁcantly. When we further remove the\nsource-side context, the performance decrease by\n0.49, 0.83, and 0.77 s-BLEU point on TED, News,\nand Europarl, respectively, which indicates that the\nsource-side context is relatively more important for\ndocument-level MT.\nTo further understand the impact of the source-\nside context, we conduct an experiment on auto-\nmatic evaluation on discourse phenomena which\nrely on source context. We use the human labeled\nevaluation set (V oita et al., 2019b) on English-\nMethod TED News Europarl Drop\nG-Transformer (rnd.) 25.84 25.23 33.87 -\n- word-dropout 25.49 24.65 33.70 -0.37\n- language locality 22.47 22.41 33.63 -1.78\n- translation locality 0.76 0.60 33.10 -14.68\nTable 5: Contribution of locality bias and word-dropout\nreporting in d-BLEU. Here, rnd. denotes the model\ntrained using randomly initialized parameters.\nMethod TED News Europarl Drop\nG-Transformer (rnd.)\nCombined attention 25.84 25.23 33.87 -\nOnly group attention 25.62 25.14 33.12 -0.35\nOnly global attention 25.00 24.54 32.87 -0.84\nTable 6: Separate effect of group and global attention\nreporting in d-BLEU. Here, rnd. denotes the model\ntrained using randomly initialized parameters.\nRussion (En-Ru) for deixis and ellipsis. We fol-\nlow the Transformer concat baseline (V oita et al.,\n2019b) and use both 6M sentence pairs and 1.5M\ndocument pairs from OpenSubtitles2018 (Lison\net al., 2018) to train our model. The results are\nshown in Table 4. G-Transformer outperforms\nTransformer baseline concat (V oita et al., 2019b)\nwith a large margin on three discourse features,\nindicating a better leverage of the source-side con-\ntext. When compared to previous model LSTM-T,\nG-Transformer achieves a better ellipsis on both\ninﬂ. and VP. However, the score on deixis is still\nlower, which indicates a potential direction that we\ncan investigate in further study.\nWord-dropout. As shown in Table 5, word-\ndropout (Appendix C.1) contributes about 0.37 d-\nBLEU on average. Its contribution to TED and\nNews is obvious in 0.35 and 0.58 d-BLEU, respec-\ntively. However, for large dataset Europarl, the\ncontribution drops to 0.17, suggesting that with suf-\nﬁcient data, word-dropout may not be necessary.\nLocality Bias. In G-Transformer, we introduce\nlocality bias to the language modeling of source\nand target, and locality bias to the translation be-\ntween source and target. We try to understand these\nbiases by removing them from G-Transformer.\nWhen all the biases removed, the model down-\ngrades to a document-level Transformer. The re-\nsults are shown in Table 5. Relatively speaking,\nthe contribution of language locality bias is about\n1.78 d-BLEU on average. While the translation\nlocality bias contributes for about 14.68 d-BLEU\non average, showing critical impact on the model\nconvergence on small datasets. These results sug-\ngest that the locality bias may be the key to train\n3450\nwhole-document MT models, especially when the\ndata is insufﬁcient.\nCombined Attention. In G-Transformer, we\nenable only the top K layers with combined atten-\ntion. On Europarl7, G-Transformer gives 33.75,\n33.87, and 33.84 d-BLEU with top 1, 2, and 3\nlayers with combined attention, respectively, show-\ning that K = 2 is sufﬁcient. Furthermore, we\nstudy the effect of group and global attention sep-\narately. As shown in Table 6, when we replace\nthe combined attention on top 2 layers with group\nattention, the performance drops by 0.22, 0.09, and\n0.75 d-BLEU on TED, News, and Europarl, respec-\ntively. When we replace the combined attention\nwith global attention, the performance decrease is\nenlarged to 0.84, 0.69, and 1.00 d-BLEU, respec-\ntively. These results demonstrate the necessity of\ncombined attention for integrating local and global\ncontext information.\n6 Related Work\nThe unit of translation has evolved from word\n(Brown et al., 1993; V ogel et al., 1996) to phrase\n(Koehn et al., 2003; Chiang, 2005, 2007) and fur-\nther to sentence (Kalchbrenner and Blunsom, 2013;\nSutskever et al., 2014; Bahdanau et al., 2014) in\nthe MT literature. The trend shows that larger units\nof translation, when represented properly, can lead\nto improved translation quality.\nA line of document-level MT extends translation\nunit to multiple sentences (Tiedemann and Scher-\nrer, 2017; Agrawal et al., 2018; Zhang et al., 2020;\nMa et al., 2020). However, these approaches are\nlimited within a short context of maximum four\nsentences. Recent studies extend the translation\nunit to whole document (Junczys-Dowmunt, 2019;\nLiu et al., 2020), using large augmented dataset\nor pretrained models. Liu et al. (2020) shows\nthat Transformer trained directly on document-\nlevel dataset can fail, resulting in unreasonably\nlow BLEU scores. Following these studies, we\nalso model translation on the whole document. We\nsolve the training challenge using a novel locality\nbias with group tags.\nAnother line of work make document-level ma-\nchine translation sentence by sentence, using addi-\ntional components to represent the context (Maruf\nand Haffari, 2018; Zheng et al., 2020; Zhang et al.,\n2018; Miculicich et al., 2018b; Maruf et al., 2019;\nYang et al., 2019). Different from these approaches,\nG-Transformer uses a generic design for both\nsource and context, translating whole document in\none beam search instead of sentence-by-sentence.\nSome methods use a two-pass strategy, generating\nsentence translation ﬁrst, integrating context infor-\nmation through a post-editing model (V oita et al.,\n2019a; Yu et al., 2020). In contrast, G-Transformer\nuses a single model, which reduces the complexity\nfor both training and inference.\nThe locality bias we introduce to G-Transformer\nis different from the ones in Longformer (Beltagy\net al., 2020) and Reformer (Kitaev et al., 2020) in\nthe sense that we discuss locality in the context\nof representing the alignment between source sen-\ntences and target sentences in document-level MT.\nSpeciﬁcally, Longformer introduces locality only\nto self-attention, while G-Transformer also intro-\nduces locality to cross-attention, which is shown to\nbe the key for the success of G-Transformer. Re-\nformer, basically same as Transformer, searches\nfor attention targets in the whole sequence, while\nG-Transformer mainly restricts the attention inside\na local sentence. In addition, the motivations are\ndifferent. While Longformer and Reformer focus\non the time and memory complexities, we focus\non attention patterns in cases where a translation\nmodel fails to converge during training.\n7 Conclusion\nWe investigated the main reasons for Transformer\ntraining failure in document-level MT, ﬁnding that\ntarget-to-source attention is a key factor. Accord-\ning to the observation, we designed a simple ex-\ntension of the standard Transformer architecture,\nusing group tags for attention guiding. Experiments\nshow that the resulting G-Transformer converges\nfast and stably on small and large data, giving the\nstate-of-the-art results compared to existing models\nunder both pre-training and random initialization\nsettings.\nAcknowledgments\nWe would like to thank the anonymous review-\ners for their valuable feedback. We thank West-\nlake University High-Performance Computing Cen-\nter for supporting on GPU resources. This work\nis supported by grants from Alibaba Group Inc.\nand Sichuan Lan-bridge Information Technology\nCo.,Ltd.\n3451\nReferences\nRuchit Rajeshkumar Agrawal, Marco Turchi, and Mat-\nteo Negri. 2018. Contextual handling in neural ma-\nchine translation: Look behind, ahead and on both\nsides. In 21st Annual Conference of the European\nAssociation for Machine Translation, pages 11–20.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473.\nGuangsheng Bao and Yue Zhang. 2021. Contextu-\nalized rewriting for text summarization. In The\nThirty-Fifth AAAI Conference on Artiﬁcial Intelli-\ngence, AAAI 2021.\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv:2004.05150.\nSamuel R. Bowman, Luke Vilnis, Oriol Vinyals, An-\ndrew Dai, Rafal Jozefowicz, and Samy Bengio.\n2016. Generating sentences from a continuous\nspace. In Proceedings of The 20th SIGNLL Con-\nference on Computational Natural Language Learn-\ning, pages 10–21, Berlin, Germany. Association for\nComputational Linguistics.\nPeter F. Brown, Stephen A. Della Pietra, Vincent J.\nDella Pietra, and Robert L. Mercer. 1993. The math-\nematics of statistical machine translation: Parameter\nestimation. Computational Linguistics, 19(2):263–\n311.\nDavid Chiang. 2005. A hierarchical phrase-based\nmodel for statistical machine translation. In Pro-\nceedings of the 43rd Annual Meeting of the As-\nsociation for Computational Linguistics (ACL’05) ,\npages 263–270, Ann Arbor, Michigan. Association\nfor Computational Linguistics.\nDavid Chiang. 2007. Hierarchical phrase-based trans-\nlation. Computational Linguistics, 33(2):201–228.\nMichael Collins, Philipp Koehn, and Ivona Ku ˇcerov´a.\n2005. Clause restructuring for statistical machine\ntranslation. In Proceedings of the 43rd Annual Meet-\ning of the Association for Computational Linguistics\n(ACL05), pages 531–540.\nEva Mart´ınez Garcia, Cristina Espa˜na-Bonet, and Llu´ıs\nM`arquez. 2015. Document-level machine transla-\ntion with word vector models. In Proceedings of\nthe 18th Annual Conference of the European Asso-\nciation for Machine Translation , pages 59–66, An-\ntalya, Turkey.\nZhengxian Gong, Min Zhang, and Guodong Zhou.\n2011. Cache-based document-level statistical ma-\nchine translation. In Proceedings of the 2011 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 909–919, Edinburgh, Scotland,\nUK. Association for Computational Linguistics.\nChristian Hardmeier. 2014. Discourse in statistical ma-\nchine translation. Ph.D. thesis, Acta Universitatis\nUpsaliensis.\nChristian Hardmeier, Sara Stymne, J ¨org Tiedemann,\nand Joakim Nivre. 2013. Docent: A document-level\ndecoder for phrase-based statistical machine trans-\nlation. In Proceedings of the 51st Annual Meet-\ning of the Association for Computational Linguistics:\nSystem Demonstrations, pages 193–198, Soﬁa, Bul-\ngaria. Association for Computational Linguistics.\nGanesh Jawahar, Beno ˆıt Sagot, and Djam ´e Seddah.\n2019. What does BERT learn about the structure\nof language? In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 3651–3657, Florence, Italy. Associa-\ntion for Computational Linguistics.\nMarcin Junczys-Dowmunt. 2019. Microsoft translator\nat WMT 2019: Towards large-scale document-level\nneural machine translation. In Proceedings of the\nFourth Conference on Machine Translation (Volume\n2: Shared Task Papers, Day 1), pages 225–233, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nNal Kalchbrenner and Phil Blunsom. 2013. Recurrent\ncontinuous translation models. In Proceedings of\nthe 2013 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1700–1709, Seattle,\nWashington, USA. Association for Computational\nLinguistics.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efﬁcient transformer. In Inter-\nnational Conference on Learning Representations.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\nBrooke Cowan, Wade Shen, Christine Moran,\nRichard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra\nConstantin, and Evan Herbst. 2007. Moses: Open\nsource toolkit for statistical machine translation. In\nProceedings of the 45th Annual Meeting of the As-\nsociation for Computational Linguistics Companion\nVolume Proceedings of the Demo and Poster Ses-\nsions, pages 177–180, Prague, Czech Republic. As-\nsociation for Computational Linguistics.\nPhilipp Koehn and Rebecca Knowles. 2017. Six chal-\nlenges for neural machine translation. In Proceed-\nings of the First Workshop on Neural Machine Trans-\nlation, pages 28–39, Vancouver. Association for\nComputational Linguistics.\nPhilipp Koehn, Franz J. Och, and Daniel Marcu. 2003.\nStatistical phrase-based translation. In Proceedings\nof the 2003 Human Language Technology Confer-\nence of the North American Chapter of the Associa-\ntion for Computational Linguistics, pages 127–133.\nSamuel L¨aubli, Rico Sennrich, and Martin V olk. 2018.\nHas machine translation achieved human parity? a\ncase for document-level evaluation. In Proceed-\nings of the 2018 Conference on Empirical Methods\n3452\nin Natural Language Processing, pages 4791–4796,\nBrussels, Belgium. Association for Computational\nLinguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nPierre Lison, J ¨org Tiedemann, and Milen Kouylekov.\n2018. OpenSubtitles2018: Statistical rescoring of\nsentence alignments in large, noisy parallel corpora.\nIn Proceedings of the Eleventh International Confer-\nence on Language Resources and Evaluation (LREC\n2018), Miyazaki, Japan. European Language Re-\nsources Association (ELRA).\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising\npre-training for neural machine translation. Transac-\ntions of the Association for Computational Linguis-\ntics, 8:726–742.\nShuming Ma, Dongdong Zhang, and Ming Zhou. 2020.\nA simple and effective uniﬁed encoder for document-\nlevel machine translation. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 3505–3511, Online. As-\nsociation for Computational Linguistics.\nSameen Maruf and Gholamreza Haffari. 2018. Docu-\nment context neural machine translation with mem-\nory networks. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers) , pages 1275–\n1284, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nSameen Maruf, Andr ´e F. T. Martins, and Gholamreza\nHaffari. 2019. Selective attention for context-aware\nneural machine translation. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and\nShort Papers), pages 3092–3102, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nLesly Miculicich, Dhananjay Ram, Nikolaos Pappas,\nand James Henderson. 2018a. Document-level neu-\nral machine translation with hierarchical attention\nnetworks. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Process-\ning, pages 2947–2954, Brussels, Belgium. Associa-\ntion for Computational Linguistics.\nLesly Miculicich, Dhananjay Ram, Nikolaos Pappas,\nand James Henderson. 2018b. Document-level neu-\nral machine translation with hierarchical attention\nnetworks. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Process-\ning, pages 2947–2954, Brussels, Belgium. Associa-\ntion for Computational Linguistics.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(Demonstrations), pages 48–53, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nJean Pouget-Abadie, Dzmitry Bahdanau, Bart van\nMerri¨enboer, Kyunghyun Cho, and Yoshua Bengio.\n2014. Overcoming the curse of sentence length for\nneural machine translation using automatic segmen-\ntation. In Proceedings of SSST-8, Eighth Workshop\non Syntax, Semantics and Structure in Statistical\nTranslation, pages 78–85, Doha, Qatar. Association\nfor Computational Linguistics.\nLuigi Rizzi. 2013. Locality. Lingua, 130:169–186.\nYves Scherrer, J ¨org Tiedemann, and Sharid Lo ´aiciga.\n2019. Analysing concatenation approaches to\ndocument-level NMT in two different domains. In\nProceedings of the Fourth Workshop on Discourse in\nMachine Translation (DiscoMT 2019), pages 51–61,\nHong Kong, China. Association for Computational\nLinguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\narXiv preprint arXiv:1409.3215.\nJ¨org Tiedemann and Yves Scherrer. 2017. Neural ma-\nchine translation with extended context. In Proceed-\nings of the Third Workshop on Discourse in Machine\nTranslation, pages 82–92, Copenhagen, Denmark.\nAssociation for Computational Linguistics.\nZhaopeng Tu, Yang Liu, Zhengdong Lu, Xiaohua Liu,\nand Hang Li. 2017. Context gates for neural ma-\nchine translation. Transactions of the Association\nfor Computational Linguistics, 5:87–99.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of the 31st International\nConference on Neural Information Processing Sys-\ntems, pages 6000–6010.\nStephan V ogel, Hermann Ney, and Christoph Tillmann.\n1996. HMM-based word alignment in statistical\ntranslation. In COLING 1996 Volume 2: The 16th\nInternational Conference on Computational Linguis-\ntics.\n3453\nElena V oita, Rico Sennrich, and Ivan Titov. 2019a.\nContext-aware monolingual repair for neural ma-\nchine translation. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 877–886, Hong Kong, China. As-\nsociation for Computational Linguistics.\nElena V oita, Rico Sennrich, and Ivan Titov. 2019b.\nWhen a good translation is wrong in context:\nContext-aware machine translation improves on\ndeixis, ellipsis, and lexical cohesion. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics , pages 1198–1212, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nZhengxin Yang, Jinchao Zhang, Fandong Meng,\nShuhao Gu, Yang Feng, and Jie Zhou. 2019. En-\nhancing context modeling with a query-guided cap-\nsule network for document-level translation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 1527–\n1537, Hong Kong, China. Association for Computa-\ntional Linguistics.\nLei Yu, Laurent Sartran, Wojciech Stokowiec, Wang\nLing, Lingpeng Kong, Phil Blunsom, and Chris\nDyer. 2020. Better document-level machine trans-\nlation with Bayes’ rule. Transactions of the Associ-\nation for Computational Linguistics, 8:346–360.\nJiacheng Zhang, Huanbo Luan, Maosong Sun, Feifei\nZhai, Jingfang Xu, Min Zhang, and Yang Liu. 2018.\nImproving the transformer translation model with\ndocument-level context. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing , pages 533–542, Brussels, Bel-\ngium. Association for Computational Linguistics.\nMeishan Zhang, Yue Zhang, and Duy-Tin V o. 2016.\nGated neural networks for targeted sentiment anal-\nysis. In Proceedings of the AAAI Conference on Ar-\ntiﬁcial Intelligence, volume 30.\nPei Zhang, Boxing Chen, Niyu Ge, and Kai Fan. 2020.\nLong-short term masking transformer: A simple\nbut effective baseline for document-level neural ma-\nchine translation. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1081–1087, Online. As-\nsociation for Computational Linguistics.\nZaixiang Zheng, Xiang Yue, Shujian Huang, Jiajun\nChen, and Alexandra Birch. 2020. Towards mak-\ning the most of context in neural machine trans-\nlation. In Proceedings of the Twenty-Ninth Inter-\nnational Joint Conference on Artiﬁcial Intelligence,\nIJCAI-20, pages 3983–3989.\n3454\nA Evaluation Metrics\nFollowing Liu et al. (2020), we use sentence-level\nBLEU score (s-BLEU) as the major metric for our\nevaluation. However, when document-level Trans-\nformer is compared, we use document-level BLEU\nscore (d-BLEU) since the sentence-to-sentence\nalignment is not available.\ns-BLEU. To calculate sentence-level BLEU\nscore on document translations, we ﬁrst split the\ntranslations into sentences, mapping to the corre-\nsponding source sentences. Then we calculate the\nBLEU score on pairs of translation and reference\nof the same source sentence.\nd-BLEU. When the alignments between transla-\ntion and source sentences are not available, we cal-\nculate the BLEU score on document-level, match-\ning n-grams in the whole document.\nB Transformer\nB.1 Model\nTransformer (Vaswani et al., 2017) has an encoder-\ndecoder structure, using multi-head attention and\nfeed-forward network as basic modules. In this pa-\nper, we mainly concern about the attention module.\nAttention. An attention module works as a func-\ntion, mapping a query and a set of key-value pairs\nto an output, that the query, keys, values, and out-\nput are all vectors. The output is computed as a\nweighted sum of the values, where the weight as-\nsigned to each value is computed by a matching\nfunction of the query with the corresponding key.\nFormally, for matrix inputs of queryQ, key K, and\nvalue V,\nAttention(Q,K,V ) =softmax\n(QKT\n√dk\n)\nV, (11)\nwhere dk is the dimensions of the key vector.\nMulti-Head Attention. Build upon single-head\nattention module, multi-head attention allows the\nmodel to attend to different positions of a sequence,\ngathering information from different representation\nsubspaces by heads.\nMultiHead(Q,K,V ) =Concat(head1,...,head h)WO,\n(12)\nwhere\nheadi = Attention(QWQ\ni ,KW K\ni ,VW V\ni ), (13)\nthat the projections of WO, WQ\ni , WK\ni , and WV\ni\nare parameter matrices.\nEncoder. The encoder consists of a stack of N\nidentical layers. Each layer has a multi-head self-\nattention, stacked with a feed-forward network. A\nresidual connection is applied to each of them.\nDecoder. Similar as the encoder, the decoder\nalso consists of a stack of N identical layers. For\neach layer, a multi-head self-attention is used to\nrepresent the target itself, and a multi-head cross-\nattention is used to attend to the encoder outputs.\nThe same structure of feed-forward network and\nresidual connection as the encoder is used.\nB.2 Training Settings\nWe build our experiments based on Transformer\nimplemented by Fairseq (Ott et al., 2019). We\nuse shared dictionary between source and target,\nand use a shared embedding table between the en-\ncoder and the decoder. We use the default setting\nproposed by Transformer (Vaswani et al., 2017),\nwhich uses Adam optimizer with β1 = 0.9 and\nβ2 = 0.98, a learning rate of5e−4, and an inverse-\nsquare schedule with warmup steps of 4000. We\napply label-smoothing of 0.1 and dropout of 0.3 on\nall settings. To study the impact of input length,\ndata scale, and model size, we take the learning rate\nand other settings as controlled variables that are\nﬁxed for all experiments. We determine the num-\nber of updates/steps automatically by early stop on\nvalidation set. We train base and big models on\n4 GPUs of Navidia 2080ti, and large model on 4\nGPUs of v100.\nC G-Transformer\nC.1 Training Settings\nWe generate the corresponding group tag sequence\ndynamically in the model according to the spe-\ncial sentence-mark tokens <s> and </s>. Tak-\ning a document “<s>there is no public transport\n. </s> <s> local people struggle to commute\n. </s>” as an example, a group-tag sequence\nG = {1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2} is\ngenerated according to Eq 3, where 1 starts on\nthe ﬁrst <s>and ends on the ﬁrst </s>, 2 the sec-\nond, and so on. The model can be trained either\nrandomly initialized or ﬁne-tuned.\nRandomly Initialized. We use the same set-\ntings as Transformer to train G-Transformer, using\nlabel-smoothing of 0.1, dropout of 0.3, Adam op-\ntimizer, and a learning rate of 5e−4 with 4000\nwarmup steps. To encourage inferencing the trans-\nlation from the context, we apply a word-dropout\n3455\nMethod TED News Europarl\ns-BLEU d-BLEU s-BLEU d-BLEU s-BLEU d-BLEU\nG-Transformer random initialized (Base) 23.53 25.84 23.55 25.23 32.18 33.87\nG-Transformer random initialized (Big) 23.29 25.48 22.22 23.82 32.04 33.77\nG-Transformer random initialized (Large) 6.23 8.95 13.68 15.33 31.51 33.21\nTable 7: G-Transformer on different model size.\n(Bowman et al., 2016) with a probability of 0.3 on\nboth the source and the target inputs.\nFine-tuned on Sentence-Level Transformer.\nWe use the parameters of an existing sentence-\nlevel Transformer to initialize G-Transformer. We\ncopy the parameters of the multi-head attention in\nTransformer to the group multi-head attention in\nG-Transformer, leaving the global multi-head at-\ntention and the gates randomly initialized. For the\nglobal multi-head attention and the gates, we use a\nlearning rate of 5e−4, while for other components,\nwe use a smaller learning rate of 1e−4. All the pa-\nrameters are jointly trained using Adam optimizer\nwith 4000 warmup steps. We apply a word-dropout\nwith a probability of 0.1 on both the source and the\ntarget inputs.\nFine-tuned on mBART25. Similar as the ﬁne-\ntuning on sentence-level Transformer, we also copy\nparameters from mBART25 (Liu et al., 2020) to\nG-Transformer, leaving the global multi-head at-\ntention and the gates randomly initialized. We fol-\nlowing the settings (Liu et al., 2020) to train the\nmodel, using Adam optimizer with a learning rate\nof 3e−5 and 2500 warmup steps. Here, we do\nnot apply word-dropout, which empirically shows\na damage to the performance.\nC.2 Results on Model Size\nAs shown in Table 7, G-Transformer has a rela-\ntively stable performance on different model size.\nWhen increasing the model size from Base to Big,\nthe performance drops for about 0.24, 1.33, and\n0.14 s-BLEU points, respectively. Further to Large\nmodel, the performance drops further for about\n17.06, 8.54, and 0.53 s-BLEU points, respectively.\nAlthough the performance drop on small dataset is\nlarge since overﬁtting on larger model, the drop on\nlarge dataset Europarl is relatively small, indicating\na stable training on different model size.",
  "topic": "Zhàng",
  "concepts": [
    {
      "name": "Zhàng",
      "score": 0.7844640016555786
    },
    {
      "name": "Machine translation",
      "score": 0.6566941738128662
    },
    {
      "name": "Chen",
      "score": 0.6507100462913513
    },
    {
      "name": "Computer science",
      "score": 0.6333894729614258
    },
    {
      "name": "Transformer",
      "score": 0.6020585894584656
    },
    {
      "name": "Natural language processing",
      "score": 0.5858297348022461
    },
    {
      "name": "Artificial intelligence",
      "score": 0.464496374130249
    },
    {
      "name": "Joint (building)",
      "score": 0.42011559009552
    },
    {
      "name": "Computational linguistics",
      "score": 0.419519305229187
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.4159521460533142
    },
    {
      "name": "Linguistics",
      "score": 0.36162516474723816
    },
    {
      "name": "Engineering",
      "score": 0.23636233806610107
    },
    {
      "name": "History",
      "score": 0.17288020253181458
    },
    {
      "name": "Electrical engineering",
      "score": 0.0905805230140686
    },
    {
      "name": "Philosophy",
      "score": 0.08536016941070557
    },
    {
      "name": "China",
      "score": 0.06841215491294861
    },
    {
      "name": "Architectural engineering",
      "score": 0.0677194595336914
    },
    {
      "name": "Physics",
      "score": 0.05582401156425476
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}