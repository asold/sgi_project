{
  "title": "Reducing hallucinations of large language models via hierarchical semantic piece",
  "url": "https://openalex.org/W4409175899",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2115283128",
      "name": "Yanyi Liu",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2113335972",
      "name": "Qingwen Yang",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2098660450",
      "name": "Jiawei Tang",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2099479469",
      "name": "Tiezheng Guo",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2096334414",
      "name": "Chen Wang",
      "affiliations": [
        "Neusoft (China)",
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2119656127",
      "name": "Pan Li",
      "affiliations": [
        "Neusoft (China)",
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2155706839",
      "name": "Sai Xu",
      "affiliations": [
        "Neusoft (China)",
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A3031824024",
      "name": "Xian-lin Gao",
      "affiliations": [
        "Neusoft (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2099417000",
      "name": "Zhi Li",
      "affiliations": [
        "Neusoft (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2097653880",
      "name": "Jun Liu",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2104126249",
      "name": "Yingyou Wen",
      "affiliations": [
        "Northeastern University",
        "Neusoft (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2115283128",
      "name": "Yanyi Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2113335972",
      "name": "Qingwen Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098660450",
      "name": "Jiawei Tang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099479469",
      "name": "Tiezheng Guo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096334414",
      "name": "Chen Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2119656127",
      "name": "Pan Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2155706839",
      "name": "Sai Xu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3031824024",
      "name": "Xian-lin Gao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099417000",
      "name": "Zhi Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097653880",
      "name": "Jun Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104126249",
      "name": "Yingyou Wen",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6600445788",
    "https://openalex.org/W4382246105",
    "https://openalex.org/W6600050485",
    "https://openalex.org/W6600503824",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W6600195515",
    "https://openalex.org/W7035904466",
    "https://openalex.org/W6851408956",
    "https://openalex.org/W6852048268",
    "https://openalex.org/W4402670859",
    "https://openalex.org/W4389520749",
    "https://openalex.org/W4386275705",
    "https://openalex.org/W6600741150",
    "https://openalex.org/W3155807546",
    "https://openalex.org/W4396718796",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4392353733",
    "https://openalex.org/W6756688054",
    "https://openalex.org/W6602752192",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W4390638022",
    "https://openalex.org/W6600291067",
    "https://openalex.org/W6822005569",
    "https://openalex.org/W4401023556",
    "https://openalex.org/W4389903865",
    "https://openalex.org/W6832427677",
    "https://openalex.org/W4389519928",
    "https://openalex.org/W4392637287",
    "https://openalex.org/W6600100092",
    "https://openalex.org/W4389519598",
    "https://openalex.org/W6602430550",
    "https://openalex.org/W4385570481",
    "https://openalex.org/W4221166835",
    "https://openalex.org/W6636568255",
    "https://openalex.org/W6600686112",
    "https://openalex.org/W4385572448",
    "https://openalex.org/W3106234277",
    "https://openalex.org/W3034188538",
    "https://openalex.org/W3159259047",
    "https://openalex.org/W6602945819",
    "https://openalex.org/W4287855110",
    "https://openalex.org/W4402670856",
    "https://openalex.org/W4389518784",
    "https://openalex.org/W4403560390"
  ],
  "abstract": "Abstract With the widespread application of large language models (LLMs) in natural language processing (NLP), hallucinations have become a significant impediment to their effective use of LLMs in industry applications. To address this challenge, we integrate existing hallucination detection and mitigation methods into a unified hallucination detection and mitigation framework. The framework consists of four main components: output parser, reference parser, fact verifier, and mitigator. These components collectively consolidate various hallucination detection and mitigation methods. Within this unified framework, we introduce the hierarchical semantic piece (HSP) for hallucination detection and mitigation. The HSP method extracts multi-granularity semantic pieces from both the reference material and the generated text. Sentence-level semantic pieces encapsulate global semantic information, while entity-level semantic pieces handle local semantic information. This method verifies the consistency between the generated text and the reference text at corresponding granularities, thereby enhancing the effectiveness of hallucination detection and mitigation. Experimental results show that the HSP method is very effective in detecting and mitigating hallucinations and shows lower computational resource consumption. Our method has great potential and promises for industry applications that rely on professionalism and reliability.",
  "full_text": "Complex & Intelligent Systems (2025) 11:231\nhttps://doi.org/10.1007/s40747-025-01833-9\nORIGINAL ARTICLE\nReducing hallucinations of large language models via hierarchical\nsemantic piece\nYanyi Liu 1 · Qingwen Yang 1 · Jiawei Tang 1 · Tiezheng Guo 1 · Chen Wang 1,2 · Pan Li 1,2 · Sai Xu 1,2 · Xianlin Gao 2 ·\nZhi Li 2 · Jun Liu 1 · Yingyou Wen 1,2\nReceived: 8 August 2024 / Accepted: 26 February 2025 / Published online: 2 April 2025\n© The Author(s) 2025\nAbstract\nWith the widespread application of large language models (LLMs) in natural language processing (NLP), hallucinations\nhave become a signiﬁcant impediment to their effective use of LLMs in industry applications. To address this challenge,\nwe integrate existing hallucination detection and mitigation methods into a uniﬁed hallucination detection and mitigation\nframework. The framework consists of four main components: output parser, reference parser, fact veriﬁer, and mitigator. These\ncomponents collectively consolidate various hallucination detection and mitigation methods. Within this uniﬁed framework,\nwe introduce the hierarchical semantic piece (HSP) for hallucination detection and mitigation. The HSP method extracts\nmulti-granularity semantic pieces from both the reference material and the generated text. Sentence-level semantic pieces\nencapsulate global semantic information, while entity-level semantic pieces handle local semantic information. This method\nveriﬁes the consistency between the generated text and the reference text at corresponding granularities, thereby enhancing\nthe effectiveness of hallucination detection and mitigation. Experimental results show that the HSP method is very effective in\ndetecting and mitigating hallucinations and shows lower computational resource consumption. Our method has great potential\nand promises for industry applications that rely on professionalism and reliability.\nKeywords Large language models · Hallucination detection · Hallucination mitigation · Industry applications\nB Yingyou Wen\nwenyingyou@mail.neu.edu.cn\nYanyi Liu\n2290175@stu.neu.edu.cn\nQingwen Yang\nyang_qw@neusoft.com\nJiawei Tang\n2301944@stu.neu.edu.cn\nTiezheng Guo\n2310725@stu.neu.edu.cn\nChen Wang\nwangchen-neu@neusoft.com\nPan Li\nli_pan@neusoft.com\nSai Xu\nxu_s@neusoft.com\nXianlin Gao\ngaoxianlin@neusoft.com\nZhi Li\nzhi-li@neusoft.com\nIntroduction\nIn recent years, large language models (LLMs) have demon-\nstrated exceptional performance in natural language gen-\neration tasks, becoming a central focus in AI and NLP\nresearch [1, 2]. They are also regarded as a crucial step toward\nachieving artiﬁcial general intelligence (AGI) [ 3, 4]. With the\nadvent of applications such as ChatGPT and GitHub Copi-\nlot, LLMs have exhibited signiﬁcant potential across various\nscenarios [ 5].\nIn practice, LLMs often generate hallucinations in their\noutputs, where the generated text is ﬂuent but disconnected\nfrom the objective facts or the context [ 6]. This phenomenon\narises due to the probabilistic nature of the models and is\nJun Liu\nliujun@cse.neu.edu.cn\n1 School of Computer Science and Engineering, Northeastern\nUniversity, Shenyang 110819, China\n2 Neusoft AI Magic Technology Research, Shenyang 110179,\nChina\n123\n231 Page 2 of 19 Complex & Intelligent Systems (2025) 11 :231\nchallenging to eliminate entirely. However, it can be mit-\nigated through various strategies, which is crucial for the\napplication of LLMs in industry application [ 6, 7].\nWith the development of model capabilities, the frequency\nof hallucinations has decreased in proprietary models such\nas GPT-4 [ 8], Claude-3, and Gemini [ 9]. However, hallu-\ncinations still exist. To address hallucinations, researchers\nhave proposed various strategies that encompass the entire\ntraining and inference loop [ 10], including pretraining [ 11],\nfaithful ﬁne-tuning [ 12, 13], reinforcement learning [ 8, 14],\nalignment decoding strategies [ 15], and post-processing [ 16,\n17].\nIn practice, knowledge-enhanced methods have become\nthe standard approach, given that the internal knowledge of\nthe model may be outdated [ 18, 19]. Prominent examples of\nthis approach include the RAG system [ 20]. By integrating\nhigh-quality reference data into the model input and provid-\ning supplementary information, it is possible to enhance the\nmodel’s precision and reliability.\nIn industry applications, such as legal [ 21], medical [ 22],\nand ﬁnancial [ 23] scenarios, data security and privacy are\nsigniﬁcant concerns [ 24, 25]. Consequently, open-source\nmodels that can be readily audited and modiﬁed are more\nfrequently employed. These models, including Llama [ 26],\nMistral [ 27], and Qwen [ 28], have comparatively limited\ncapabilities and are more prone to generate hallucinations.\nGiven the lower tolerance for hallucinations in industry\napplications, developing effective hallucination mitigation\nstrategies remains crucial.\nThe core contributions of this paper are as follows:\n• A uniﬁed hallucination detection and mitigation frame-\nwork is proposed, comprising four components: output\nsemantic decomposition, reference information retrieval\nand decomposition, fact veriﬁcation, and hallucination\nmitigation. This framework uniﬁes existing hallucination\ndetection and mitigation methods.\n• A novel approach, hierarchical semantic piece (HSP), is\nintroduced for detecting and mitigating hallucinations.\nHSP extracts sentence and entity-level semantic pieces\nfrom both the reference text and the output text, veriﬁes\nconsistency at different levels, and enhances the effec-\ntiveness of hallucination detection and mitigation.\n• The HSP method’s effectiveness was validated through\nexperiments. The results demonstrate that the HSP\nmethod is an effective approach for detecting and mit-\nigating hallucinations. Experiments on domain-speciﬁc\nscenarios show that our method has great potential for\nindustry applications.\nThe uniﬁed framework and the HSP method are general,\nmodular, and extensible. By leveraging existing large lan-\nguage models, they can be seamlessly integrated into existing\nLLM applications, thereby enhancing their professionalism\nand reliability.\nRelated work\nLarge language models in natural language\nprocessing\nWith the rapid development of artiﬁcial intelligence technol-\nogy, large language models (LLMs) have become increas-\ningly popular in the ﬁeld of natural language processing.\nLLMs have signiﬁcantly advanced natural language pro-\ncessing technology, enabling computers to better understand\nand generate natural language, and providing powerful\nsupport for the development of artiﬁcial intelligence tech-\nnology.\nMost large language models (LLMs) are currently based\non the transformer decoder architecture, such as GPT and\nLlama. These models are pretrained using self-supervised\nlearning on large amounts of text data [ 1]. In comparison\nto models in encoder-only architecture like BERT [ 29], cur-\nrent LLMs mainly employ decoder architecture, which is\nmore efﬁcient for generative tasks. Moreover, LLMs bene-\nﬁt from their larger model size and more extensive training\ndata, giving them more versatile language generation capa-\nbilities.\nPrompt engineering is an essential tool in current LLM\napplications [ 30], as it involves incorporating context,\ninstructions, and few-shot examples into the model input.\nThis capability, known as in-context learning, allows the\nmodel to adapt to new tasks without additional ﬁne-tuning.\nThis has changed the way natural language processing is\nresearched and applied, with LLMs becoming more widely\nused in practical scenarios.\nAdvanced prompt-based applications, such as RAG [ 31,\n32] and Agent [ 33, 34], have emerged as a new paradigm for\nindustry application development. By incorporating profes-\nsional knowledge into the model context, RAG enhances the\nprofessionalism of large language models. Agents achieve\nmore ﬂexible and versatile industry application development\nby invoking external tools and workﬂow orchestration.\nSystem engineering of large language\nmodels\nAlthough LLMs provide powerful language generation capa-\nbilities, they also face a number of challenges, particularly\nin terms of the reliability and accuracy of the generated con-\ntent [19]. Wang et al. proposed the concept of large language\nmodel system engineering (LLM-SE) [ 35] to address the\n123\nComplex & Intelligent Systems (2025) 11 :231 Page 3 of 19 231\nproblems faced by LLMs in industry applications through\na systematic approach. The feature requirements of industry\napplications based on LLMs include professionalism, value\nconsistency, interpretability, reliability, determinism, ﬂexi-\nbility, stability, low latency, etc.\nTwo core components of LLM-SE are quality engineering\nand interpretation engineering. Quality engineering ensures\nthat the content generated by LLMs is professional, consis-\ntent, and reliable. This is achieved by reviewing, monitoring,\ncontrolling, correcting, and supplementing the content to\nmeet these quality standards. Interpretation engineering aims\nto enhance the understandability and acceptance of the\ndecision-making process and the results generated by LLMs,\nwhich is essential for their successful application among pro-\nfessionals and end users.\nGiven the minimal tolerance for hallucinations in indus-\ntry applications, for example in legal, medical, and ﬁnancial\nscenarios, our research focuses on addressing this issue\nto enhance professionalism and reliability. From a quality\nengineering perspective, we aim to detect and mitigate hal-\nlucinations in large language models (LLMs) to improve their\nperformance and reliability in industry applications.\nPost-processing hallucination detection and\nmitigation\nTo mitigate hallucinations in large language models,\nresearchers have proposed various strategies at different\nstages of model construction and application [ 7]. Post-\nprocessing methods are of particular interest, as they do not\nrequire changes to the model’s parameters and can be easily\napplied without the need for retraining, making them more\nversatile.\nThere are two main categories of post-processing methods\nfor reducing hallucinations.\n1. Reference-free methods, which rectify hallucinations\nthrough self-reﬂection [ 36, 37], Chain of Thought (CoT) [ 38,\n39], or by reﬁning the decoding algorithm [ 40], are\ndesigned to enable models to self-evaluate and correct\ntheir outputs. However, their effectiveness is limited by\nthe model’s knowledge cutoff and the difﬁculty of accu-\nrate content attribution [ 41], making them more suitable\nfor general text generation tasks rather than domain-\nspeciﬁc applications.\n2. Reference-based techniques, which use external knowl-\nedge or materials as reference points, detect and mitigate\nhallucinations by comparing the model output with\ncredible sources. Methods such as FactScore [ 42] and\nSAFE [ 43] take a fact-based approach by decompos-\ning the output text into atomic facts, which are then\nchecked against reputable knowledge repositories such\nas Wikipedia. The RARR method generates veriﬁcation\nquestions based on the output text, seeks answers through\nweb searches, and assesses the veracity of the model’s\noutput [ 44]. CoNLI develops a Natural Language Infer-\nence chain for the fact-checking process, which is used\nto validate the entities and statements within the output\ntext [ 45].\nExisting methods have made signiﬁcant progress in hal-\nlucination detection and mitigation. In our work, we unify\nthese methods into a modular hallucination detection and mit-\nigation framework speciﬁcally designed for reference-based\ngeneration scenarios. Our method, hierarchical semantic\npiece (HSP), as a post-processing approach, enhances the\neffectiveness of hallucination detection and mitigation by\nextracting and verifying multi-level semantic pieces, thereby\nleveraging the reference text to ensure higher accuracy and\nconsistency.\nMethodology\nConsider the knowledge-enhanced methods represented by\nthe RAG System, the generation process can be deﬁned as\nfollows:\nLLM (Ref , Query ) → Output (1)\nHere’s an example of the generation process:\nGeneration Process Example\nReference:\nDutch programmer Guido van Rossum developed\nPython in 1991 after expressing frustration with\nthe limitations of the programming language ABC.\nPython, which he named after the British television\nseries Monty Python´ s Flying Circus, was publicly\nreleased in 1994.\nQuery:\nWhen was Python released?\nLLM Output:\nPython was created by Guido van Rossum and was\nﬁrst released in 1994.\nOur goal is to detect and mitigate hallucinations in large\nlanguage models (LLMs) Output in the reference-based sce-\nnario. The task is deﬁned as follows:\nGiven a text generated by an LLM, Output , and a refer-\nence text, Ref , our task is to detect hallucinations in Output\nand mitigate them to make the output more accurate and reli-\nable.\nF : (Ref , Output ) → Output\n′ (2)\n123\n231 Page 4 of 19 Complex & Intelligent Systems (2025) 11 :231\nHere’s a detailed example for task deﬁnition:\nHallucination Mitigation Example\nReference:\nDutch programmer Guido van Rossum developed\nPython in 1991 after expressing frustration with\nthe limitations of the programming language ABC.\nPython, which he named after the British television\nseries Monty Python´ s Flying Circus, was publicly\nreleased in 1994.\nLLM Output:\nPython was created by Guido van Rossum and was\nﬁrst released in 1994.\nDetection:\nHallucination Detected.\nMitigation Result:\nPython was created by Guido van Rossum and was\nﬁrst released in 1991.\nBased on the task deﬁnition, we ﬁrst review existing hal-\nlucination detection and mitigation methods, then propose\na uniﬁed hallucination detection and mitigation framework.\nThe framework consists of four key components: output\nsemantic decomposition, reference information retrieval and\ndecomposition, fact veriﬁcation, and hallucination mitiga-\ntion. Based on this framework, we propose a new method,\nhierarchical semantic piece (HSP), for hallucination detec-\ntion and mitigation.\nUnified hallucination detection and mitigation\nframework\nThe uniﬁed framework for hallucination detection and miti-\ngation is inspired by various methods in the reference-based\nscenario. It consists of four key components: output semantic\ndecomposition, reference information retrieval and decom-\nposition, fact veriﬁcation, and hallucination mitigation. As\nillustrated in Fig. 1, these four components collectively\nform a comprehensive hallucination detection and mitiga-\ntion framework.\n1. Output parser: Extracts semantic information from the\ntext for subsequent veriﬁcation.\n2. Reference parser: Retrieves and extracts reference infor-\nmation for comparison with the output semantic infor-\nmation.\n3. Fact veriﬁer: V alidates the output semantic information\nagainst the reference data.\n4. Mitigation: Mitigates hallucinations in the output text\nbased on the results of the fact veriﬁer.\nFig. 1 Uniﬁed hallucination detection and mitigation framework\nThe entire framework is interdependent, with the output\nof each level directly inﬂuencing the ﬁnal detection and mit-\nigation results.\nExisting methods of hallucination detection and\nmitigation can be integrated into the uniﬁed framework.\nTable 1 illustrates the correspondence between various hal-\nlucination detection and mitigation methods and the speciﬁc\ncomponents of the framework. This allows for a clearer\nunderstanding and comparison of the differences among\nthese methods.\nIn the mitigation part, nearly all methods rely on language\nmodel-based rewriting. However, the practical implementa-\ntion of these methods varies differs due to variances in the\noutputs of preceding stages.\nBased on the uniﬁed framework, we propose the hier-\narchical semantic piece (HSP) for hallucination detection\nand mitigation. Compared with existing methods, the HSP\nmethod introduces a hierarchical semantic extraction pro-\ncess on both the output and reference texts, enhancing the\neffectiveness of hallucination detection and mitigation. The\nfollowing sections provide a comprehensive overview of the\nHSP method.\nHierarchical semantic piece (HSP)\nWe propose the hierarchical semantic piece (HSP) for hallu-\ncination detection and mitigation. The overall process of the\nHSP method is illustrated in Fig. 2.\nOur method is centered on the extraction of semantic infor-\nmation and the consistency of semantic granularity. In HSP ,\nboth the output semantic parser and the reference seman-\ntic parser utilize the same module, the hierarchical semantic\npiece extractor. This extractor decomposes the semantic\ninformation in the text into two different levels: sentence-\nlevel and entity-level. In the fact veriﬁer, we introduce an\n123\nComplex & Intelligent Systems (2025) 11 :231 Page 5 of 19 231\nTable 1 Existing hallucination detection and mitigation methods in the uniﬁed framework\nMethod Output parser Reference source Reference parser Fact veriﬁer Mitigation\nFactscore [ 42] Atomic facts extraction Wikipedia LM retrieval Binary classiﬁcation None\nRARR [ 43] Generate questions Google search Text split and rearrange Binary classiﬁcation LM Rewrite\nCoNLI [ 45] Sentence split entity extraction Source texts – CoT based NLI LM rewrite\nSAFE [ 43] Atomic facts extraction with reﬁne Google search – Iterative retrieval triple classiﬁcation LM Rewrite\nHSP Hierarchical semantic extractor Source texts Hierarchical semantic extractor Triple classiﬁcation LM rewrite\nevidence matching method based on embedding similarity,\nfollowed by fact veriﬁcation based on LLM for the premise-\nhypothesis pairs. Finally, the hallucination mitigator applies\nthe veriﬁcation results to the original text, generating the\nreﬁned output.\nThe hierarchical semantic piece (HSP) is summarized in\nAlgorithm 1. The inputs to the algorithm are the reference\ntext Ref and the output text Output from LLM, while the\noutputs are the veriﬁcation results V and the mitigated output\nOutput\n′. In the algorithm, SPr and SPo represent the hier-\narchical semantic pieces extracted from the reference and\noutput texts, respectively. For each output semantic piece\nsp\noi , the evidence Ei is obtained by matching the top- k ref-\nerence semantic pieces that have the highest similarity to the\noutput semantic piece. The fact veriﬁcation results Vi are\nthen derived based on the output semantic piece and the cor-\nresponding evidence. If any contradictions are found in the\nveriﬁcation results, the hallucination mitigator is employed to\ngenerate the mitigated output Output\n′; otherwise, the orig-\ninal output is retained.\nAlgorithm 1 Hierarchical semantic piece (HSP) method\n1: Input: Reference Text Ref , Output Text Output\n2: Output: V eriﬁcation Results V , Mitigated Output Output ′\n3: SPr ← HierarchicalSemanticExtractor(Ref )\n4: SPo ← HierarchicalSemanticExtractor(Output )\n5: for spoi ∈ SPo do\n6: Ei ← EvidenceMatching(spoi , SPr )\n7: Vi ← FactV eriﬁer(spoi , Ei ) ⊿\nVi ∈{ Contradiction, Neutral, Entailment}\n8: end for\n9: if HasContradiction(V ) then\n10: Output ′ ← HallucinationMitigator(Output , V , E )\n11: else\n12: Output ′ ← Output\n13: end if\n14: return (Output ′, V )\nThe following section provides a detailed description of\nthe components constituting the HSP method. A full exam-\nple of the HSP method and detailed intermediate results are\nprovided in the Appendix B.\nHierarchical semantic extractor\nIn this module, we decompose the text into sentence-level and\nentity-level semantic pieces to better evaluate the consistency\nbetween the model output and the reference material. The\nReference text Ref and the Output text Output are decom-\nposed into sentence-level semantic pieces SP\nr and SPo:\nRef → SPr ={ spr 1, spr 2,..., sprn } (3)\nOutput → SPo ={ spo1, spo2,..., spom } (4)\n123\n231 Page 6 of 19 Complex & Intelligent Systems (2025) 11 :231\nFig. 2 Hierarchical semantic piece (HSP) overview. The yellow blocks are the key components in our method, which can be mapped to the\ncomponents in the uniﬁed framework\nFig. 3 Hierarchical semantic extractor\nThe workﬂow is illustrated in Fig. 3. There are two levels\nof semantic pieces: sentence-level and entity-level.\nInitially, sentence-level semantic units are extracted from\nthe original text, aiming to decompose the content into its\nfundamental components. Prompt engineering is employed\nto ensure that each sentence-level semantic piece is an inde-\npendent “claim\" or \"fact\" that can be understood in isolation\nwithout requiring additional context.\nIn the second step, we extract entities from the original text\nbased on the ERNIE-UIE [ 46]. The extracted entity informa-\ntion is combined with the corresponding original sentence in\norder to form an entity-level semantic piece. The entity-level\nsemantic pieces contain structured information, represented\nin the form of (entity type–value), which is used for auxiliary\njudgment.\nIn the entity extraction module, predeﬁned entity schemas\nare introduced, including time, number, location, and per-\nsonal information. This approach narrows the scope of\nentities, making the extraction more targeted and simplify-\ning the subsequent veriﬁcation. The full schema is shown in\nAppendix D.\nIn practice, we ﬁlter the original sentence using sentence-\nlevel semantic pieces to remove irrelevant information before\nentity-level extraction, thereby reducing computational com-\nplexity.\nBoth sentence-level and entity-level extractions form a\ncomplementary relationship within the hierarchical extrac-\ntion process. The sentence-level pieces ensure that the\ngeneral conveyed ideas and claims align, which is crucial\nfor verifying the overall consistency. Meanwhile, the entity-\nlevel pieces ensure that speciﬁc details are accurate, thus\n123\nComplex & Intelligent Systems (2025) 11 :231 Page 7 of 19 231\nFig. 4 Fact veriﬁer in the HSP method\nsupporting the veriﬁcation of precise information within the\nbroader context.\nFact veriﬁer for semantic piece\nWith the hierarchical semantic decomposition, our method\napplies natural language inference (NLI) to verify the consis-\ntency between the output semantic pieces and the reference\nsemantic pieces. The workﬂow is illustrated in Fig. 4.\nNatural language inference (NLI) task is to determine the\nrelationship between a premise and a hypothesis, which is a\ncommon task in natural language processing. In our method,\nthe reference sentence pieces SP\nr are considered the premise\nset, while the output sentence pieces SPo are considered the\nhypothesis set in the fact veriﬁcation module. The goal is\nto determine the relationship between the output semantic\npieces and the reference semantic pieces. Available relation-\nships include entailment, neutral, and contradiction.\n1. Entailment: The output semantic piece matches the ref-\nerence semantic piece.\n2. Neutral: The output semantic piece is unrelated to the\nreference semantic piece.\n3. Contradiction: The output semantic piece contradicts the\nreference semantic piece.\nIn the fact veriﬁcation module, the reference sentence\npieces SP\nr are considered the premise set, while the out-\nput sentence pieces SPo are considered the hypothesis set.\nConsidering the computational cost, it is impractical to per-\nform fact veriﬁcation on the entire combination of the whole\nsets. Instead, an evidence matching module is introduced to\nﬁnd the top- k reference semantic pieces as the evidence for\neach output semantic piece.\nThe mathematical expression for ﬁnding evidence for each\noutput sentence piece SP\no is as follows:\nEi =\n{\nprj |j ∈ Topk\n(\nsim(spoi , sprj )\n)}\n(5)\nFor each hypothesis, the evidence Ei is obtained by select-\ning the top k reference semantic pieces with the highest\nsimilarity to the output semantic piece spoi . The similarity\nfunction sim (·) calculates the similarity between two seman-\ntic pieces, speciﬁcally embedding cosine similarity is used in\nthis case. To maintain the granularity of the semantic pieces,\nwe keep top-3 semantic pieces as the evidence. For entity-\nlevel semantic pieces, we add an additional entity matching\nstep; if the entity type and value are exactly the same, the\nevidence is considered as entailment.\nAfter obtaining the evidence semantic pieces, they are\ninput into the natural language inference (NLI) module for\nfact veriﬁcation.\nV\ni = NLI(spoi , Ei ) ∀spoi ∈ SPo (6)\nFinally, we obtain the veriﬁcation results V :{ V0,...,\nVm }∈{ Entailment, Neutral, Contradiction}, which are used\nto guide the hallucination mitigation process.\n123\n231 Page 8 of 19 Complex & Intelligent Systems (2025) 11 :231\nFig. 5 Hallucination mitigator\nHallucination mitigator\nThe hallucination mitigator is designed to correct hallucina-\ntions detected in fact veriﬁcation. The hallucination mitigator\ngenerates repair sentences for each hallucination semantic\npiece based on the veriﬁcation results and its original text.\nThe workﬂow is illustrated in Fig. 5.\nFor each hallucination semantic piece, the workﬂow is as\nfollows:\n1. Find the original sentence: To locate the relevant original\nsentence, the most similar sentence to the hallucination\nsemantic piece is selected from the original text, based\non the cosine similarity of the sentence embeddings.\n2. Prompt LLM To Fix: We utilize the ability of LLM to\nﬁx hallucinations by providing information that includes\nthe original sentence, the hallucination semantic piece,\nand the reference semantic piece. The LLM generates a\nrepair sentence based on the input.\n3. Result integration: The repair sentence is integrated into\nthe original text to replace the hallucination semantic\npiece, generating the mitigated output.\nIf multiple hallucinations are present, the above steps are\niteratively applied to each one. The ﬁnal output is a revised\ntext that has been thoroughly vetted and corrected for factual\naccuracy.\nHere’s an example of the hallucination mitigation result,\nthe hallucinated part is marked in red, and the corrected part\nis marked in green.\nSentence to Fix:\nA massive earthquake hit Nepal, causing widespread\npanic and devastation. Survivors reported seeing wild\nanimals roaming the streets in the aftermath of the\ndisaster.\nGround Truth:\nMassive 7.8 magnitude earthquake has struck Nepal\nnear its capital, Kathmandu. As the death toll rises,\nwitnesses describe devastation and panic.\nHSP Result:\nA massive earthquake hit Nepal, causing widespread\npanic and devastation. Survivors reported seeing peo-\nple in a state of shock and confusion in the aftermath\nof the disaster.\nHSP Metrics\nBLEU-1:(0.58 → 0.5179 ↓); Rouge-L: (0.463 →\n0.4554 ↓); BertScore: (0.7486 → 0.7571 ↑); Align-\nscore: (0.4966 → 0.9528 ↑)\nExperiments\nTo verify the effectiveness of our proposed method, we\ndesigned two types of experiments: hallucination detection\nand hallucination mitigation.\nExperiment setup\n1. Models\nTo verify the generality of our proposed method, two types\nof models were studied. The proprietary models include\nthe latest GPT-4o and GPT-3.5-turbo, while the open-source\nmodels include the state-of-the-art Qwen2-72B-Instruct [ 47]\nand the cost-effective Qwen1.5-32B-Chat [ 28].\nTo ensure the consistency of the results, we used the same\ngeneration parameters for all models, with temperature = 0\nand top-p = 0.6.\nHSP method has some hyperparameters, including the\nsimilarity function sim (·) and the top- k for evidence match-\ning. To ensure fairness, we used the same hyperparameters for\nall models, with the SFR-Embedding-2-R model for embed-\nding similarity and top- k =3 .\n2. Evaluation Metrics\nFor the detection task, we deﬁne the hallucination detection\ntask as a binary classiﬁcation task, with the evaluation metric\nbeing the marco precision, recall and F1 score.\n123\nComplex & Intelligent Systems (2025) 11 :231 Page 9 of 19 231\nFor the mitigation task, we deﬁne the hallucination mit-\nigation task as a text rewriting task, with the evaluation\nmetrics being Rouge1, Rouge2, RougeL, Bleu-avg, Bleu-4,\nBertScore [ 48], Alignscore [ 49] and FactCC [ 50].\nThe cost and latency are key metrics for LLM-based meth-\nods. However, the deployment schemes and code parallelism\nof the models vary, making direct evaluation based on time\nunfair. Therefore, we propose the Token Expansion Ratio\n(TER) to evaluate the efﬁciency of the methods.\nTER = To k e n\nall\nTo k e ninput\n(7)\nThe token efﬁciency ratio (TER) measures the extra tokens\nneeded by a method that uses a language model (LLM) com-\npared to the number of tokens in the original input text.\nSpeciﬁcally, To k e nall refers to the total number of tokens\nused by the method, while To k e ninput denotes the number\nof tokens in the input text. A lower TER indicates higher\nefﬁciency. This metric helps to standardize the comparison\namong different methods by accounting for variations in tok-\nenization rates, thus making the efﬁciency of various methods\nmore comparable.\nHallucination detection\nThe hallucination detection task is deﬁned as a binary classi-\nﬁcation task. We set two baselines, Alignscore [ 49] and LLM\nPrompt-based classiﬁcation.\nDatasets\nFor the hallucination detection task, QAGS [ 51] and Sum-\nmEval [52] datasets are used to evaluate the effectiveness on\ngeneral domain, HaluBench subset [ 53]i su s e dt oe v a l u a t e\nthe effectiveness on domain-speciﬁc scenario. The datasets\ndistribution is shown in Table 2.\n• QAGS: The QAGS datasets [ 51] are built with CNN/\nDailymaill (QAGS-CNNDM) and XSUM (QAGS-\nXSUM). Each sample includes three crowdsourced con-\nsistency labels, and only when all three annotators agree\nthat the generated text is consistent with the reference\ntext, we consider the sample to be non-hallucination.\n• SummEval: The SummEval dataset is built upon the\nCNN/DailyMail dataset [ 52]. Each summary is labelled\nwith three human experts on a Likert scale from 1-5 on\n4 categories: consistency, coherence, ﬂuency and rele-\nvance. We follow the TRUE benchmark [ 54] in taking\nthe consistency scores and mapping a score of 5 to being\nnon-hallucination, and anything lower to being halluci-\nnation.\nTable 2 Sample distribution of datasets\nDataset Total Halu Non-Halu\nQAGS-CNNDM 235 122 113\nQAGS-XSum 239 57 182\nSummEval 1600 294 1306\nHaluBench-FinanceBench 1000 500 500\nHaluBench-CovidQA 1000 500 500\nHaluBench-PubMedQA 1000 500 500\n• HaluBench: The HaluBench dataset is a benchmark\ndataset for hallucination detection in various domains\n[53]. We choose FinanceBench, CovidQA and Pub-\nMedQA subsets for evaluation, which are more relevant\nto domain speciﬁc scenarios. All samples have been\nlabelled with binary labels, 1 for hallucination and 0 for\nnon-hallucination.\nExperimental results\nThe detection results are shown in Tables 3 and 4.D u et o\nthe cost limitation, we only evaluate HaluBench dataset on\nQwen1.5-32B-Chat and Qwen2-72B-Instruct models.\nThe HSP method achieved superior performance concern-\ning the macro F1 score, demonstrating high precision and\nrecall. Compared to CoNLI, our method exhibited similar\nperformance with a slight improvement in the F1 score. Addi-\ntionally, our method has an advantage in token expansion\nratio (TER), showing an average reduction of approximately\n30%, which indicates a lower cost in LLM tokens.\nThrough a series of experiments on various models, we\nobserved a positive correlation between a model’s capa-\nbility and its performance in hallucination detection. The\ncurrent leading model, GPT-4o, exhibited superior results\nacross multiple methods and datasets. The top-performing\nopen-source model, Qwen2-72B-Instruct, also demonstrated\ncommendable performance, surpassing GPT-3.5-turbo while\nremaining slightly behind GPT-4o. Additionally, the cost-\neffective Qwen1.5-32B-Chat model achieved better results\nthan Alignscore, making it more suitable for practical appli-\ncations (Table 3).\nIn Table 4, the results on the HaluBench dataset show\nthat LLM-based methods generally outperform Alignscore\nmethod on domain-speciﬁc datasets. This indicates that\nLLM-based methods have better generalization performance\non domain-speciﬁc datasets. Additionally, the HSP method\nachieved better results, demonstrating its effectiveness on\ndomain-speciﬁc datasets.\nThe baseline method based on LLMs performed best\nin terms of TER, requiring the fewest additional tokens.\nHowever, the overall performance was poor. Compared to\n123\n231 Page 10 of 19 Complex & Intelligent Systems (2025) 11 :231\nTable 3 Hallucination detection results with different models\nMethod Model QAGS-CNNDM QAGS-XSUM SummEval Average\nP ↑ R ↑ F1 ↑ TER ↓ P ↑ R ↑ F1 ↑ TER ↓ P ↑ R ↑ F1 ↑ TER ↓ P ↑ R ↑ F1 ↑ TER ↓\nAlignscore 72.95 61.38 54.90 – 66.55 70.77 56.77 – 84.54 62.35 65.60 – 74.68 64.83 59.09 –\nPrompt Q-32B 73.98 61.00 53.97 1.42 65.16 64.01 45.15 1.41 86.19 60.37 62.98 1.39 75.11 61.79 54.03 1.41\nCoNLI Q-32B 78.05 70.39 67.45 7.25 67.59 72 .20 58 .00 6.50 85.92 66.49 70.67 7.56 77.19 69.69 65.37 7.10\nHSP Q-32B 76.00 74.17 73 .26 4.70 65.42 69.84 56.97 3.75 76.59 76.10 76 .34 5.43 72.67 73.37 68 .86 4.63\nPrompt Q-72B 75.70 67.49 63.87 1.41 65.32 64.56 46.01 1.56 88.27 61.98 65.25 1.55 76.43 64.68 58.38 1.51\nCoNLI Q-72B 78.03 74.74 73.42 6.99 67.17 72 .64 60.08 6.37 85.87 72.83 76.98 7.56 77.02 73.40 70.16 6.97\nHSP Q-72B 78.22 77 .03 76 .42 4.76 65.07 70.68 61.48 3.66 79.15 77.90 78 .50 5.46 74.15 75.20 72 .13 4.63\nPrompt GPT −3.5 69.22 52.84 39.15 1.18 64.47 61.54 41.19 1.40 81.32 54.97 54.58 1.23 71.67 56.45 44.97 1.27\nCoNLI GPT −3.5 72.66 71 .17 70.27 6.46 67.97 73 .02 59.15 6.55 82.26 67 .61 71.43 7.39 74.30 70.60 66.95 6.80\nHSP GPT −3.5 70.45 70.51 70.45 4.88 65.13 71.01 63.22 3.71 71.26 72.71 71.93 5.57 68.95 71.41 68 .53 4.72\nPrompt GPT-4o 79.82 73.67 71.53 1.28 67.10 71.10 56.46 1.47 84.47 67.28 71.37 1.27 77.13 70.68 66.45 1.34\nCoNLI GPT-4o 82.40 78.97 77.83 6.68 71.43 79 .12 66 .81 6.25 85.96 72 .53 76 .73 7.48 79.93 76.87 73.79 6.80\nHSP GPT-4o 81.99 81.48 81 .60 5.33 68.14 75.20 66.20 3.77 77.79 74.02 75.64 5.61 75.97 76.90 74 .48 4.90\nP , R, F1 are short for precision, recall, F1 score, respectively. The best results for each model are in bold. Q-32B, Q-72B, GPT-3.5, and GPT-4o refer to the versions Qwen1.5-32B-Chat,\nQwen2-72B-Instruct, GPT-3.5-Turbo-0125, and GPT-4o-0513, respectively\n123\nComplex & Intelligent Systems (2025) 11 :231 Page 11 of 19 231\nTable 4 Hallucination detection results on HaluBench\nMethod Model FinanceBench CovidQA PubMedQA Average\nP ↑ R ↑ F1 ↑ TER ↓ P ↑ R ↑ F1 ↑ TER ↓ P ↑ R ↑ F1 ↑ TER ↓ P ↑ R ↑ F1 ↑ TER ↓\nAlignscore 55.63 51.90 42.35 – 77.67 74.40 73.62 – 68.60 60.20 55.13 – 67.30 32.17 57.03 –\nPrompt Q-32B 61.21 56.30 50.92 1.44 81.06 72.40 70.33 1.06 84.13 79.20 78.42 1.56 75.47 69.30 66.56 1.35\nCoNLI Q-32B 60.06 58.50 56.82 5.14 84.18 79 .70 79.01 2.36 82.20 82.20 82 .20 7.61 75.48 73.47 72 .68 5.04\nHSP Q-32B 65.77 60 .50 56 .90 3.72 79.39 79.30 79.28 2.04 81.65 80.00 79.74 6.57 75.60 73.27 71.97 4.11\nPrompt Q-72B 66.54 55.90 47.45 1.64 79.47 71.00 68.76 1.08 86.62 83.10 82.68 1.53 77.54 70.00 66.30 1.51\nCoNLI Q-72B 67.61 67.60 67 .60 5.60 87.37 87.00 86.97 2.22 83.63 83.50 83.48 5.11 79.54 79.37 79.35 6.97\nHSP Q-72B 72.76 67 .60 65.65 3.63 88.23 87 .20 87 .11 3.10 87.88 85 .40 85 .16 4.40 82.96 80 .07 79.31 4.63\nP , R, F1 are short for precision, recall, F1 score, respectively. The best results for each model are in bold. Q-32B, Q-72B, GPT-3.5, and GPT-4o refer to the versions Qwen1.5-32B-Chat,\nQwen2-72B-Instruct, GPT-3.5-Turbo-0125, and GPT-4o-0513, respectively\nthe baseline method based on Alignscore, the prompt-based\nmethod requires Qwen2-72B-Instruct to perform as well as\nwell-designed small model. This indicates that research on\nhallucination detection methods is necessary, current large\nlanguage models’ capabilities are insufﬁcient to complete\nhallucination detection tasks with simple prompts.\nAblation study\nTo verify the effectiveness of speciﬁc parameter choices and\ndesigns in our method, we conducted modular experiments\non QAGS-CNNDM and QAGS-XSum datasets with Qwen2-\n72B-Instruct.\nWe introduced an evidence matching method based on\nembedding similarity in the evidence matching part of our\nmethod. Different similarity functions sim (·) were tested,\nincluding fuzzy matching based on Levenshtein distance,\nrerank score matching, and embedding cosine similarity\nmatching using two different embedding models. One is the\nBGE-m3 [ 55] model based on XLM-RoBERTa, which is\nof moderate size, and the other is the SFR-Embedding-2-R\nmodel [56] based on Mistral-7B, which is larger and achieved\n1st place in the MTEB leaderboard in June 2023. The results\nare shown in Table 5.\nThe results show that the embedding similarity method\nachieves the best results on both datasets, and that larger\nembedding models lead to better performance but also to\nhigher computational costs. The differences in the token\nexpansion ratio (TER) among the methods are not signiﬁ-\ncant.\nRegarding the selection of retrieval top- k, the impact of\ndifferent k values on the results is shown in Table 6.\nWe can see that the top-3 perform best on the QAGS-\nCNNDM dataset, while the top-4 perform best on the\nQAGS-XSUM dataset. The results indicate that both smaller\nand larger values of k lead to performance degradation. It is\nhypothesized that when k is small, the semantic granularity is\ntoo ﬁne, resulting in insufﬁcient evidence. Conversely, when\nk is large, the introduction of excessive irrelevant informa-\ntion adversely affects performance. Moreover, the larger the\nk, the higher the TER, which means higher computational\ncost. To ensure consistency on different dataset, ﬁnally we\nchoose top-3 as evidence.\nIn the hierarchical semantic extractor module, we intro-\nduced two levels of semantic piece, sentence-level and\nentity-level. The experiments tested the effectiveness of\nextracting semantic pieces at different levels. The results are\npresented in Table 7.\nThe results indicate that hierarchical semantic piece\nextraction achieves the highest performance. While sentence-\nlevel semantic piece extraction alone yields good results,\nentity-level semantic piece extraction alone performs poorly\ndue to its limitation to four basic schema types, which fails to\n123\n231 Page 12 of 19 Complex & Intelligent Systems (2025) 11 :231\nTable 5 Effectiveness of different evidence matching methods\nMethod Model QAGS-CNNDM QAGS-XSUM\nP R F1 TER P R F1 TER\nFuzzy – 70.63 67.95 66.35 4.83 59.83 61.65 60.23 3.73\nRerank bge-reranker-v2-m3 [ 55] 76.93 74.64 73.61 4.77 63.79 68.53 57.99 3.70\nEmbedding bge-m3 [ 55] 78.45 76.31 75.40 4.76 63.77 68.92 60.43 3.67\nEmbedding SFR-Embedding-2-R [ 56] 78.22 77.03 76 .42 4.76 65.07 70 .68 61 .48 3.66\nP , R, F1 are short for precision, recall, F1 score, respectively. The best results are in bold\nTable 6 Effect of different k\nvalues on evidence selection k QAGS-CNNDM QAGS-XSUM\nPrecision Recall F1 TER Precision Recall F1 TER\n1 70.96 68.74 67.40 4.37 62.60 66.14 62.88 3.39\n2 75.69 73.38 72.28 4.56 64.99 70.62 62.04 3.53\n3 78.22 77 .03 76 .42 4.76 65.07 70.68 61.48 3.66\n4 75.79 74.51 73.81 4.94 65.67 71 .33 61.06 3.79\n5 73.99 72.81 72.10 5.14 64.47 69.63 59.45 3.93\nThe best results are in bold\nTable 7 Effectiveness of\ndifferent levels of semantic\npiece extraction\nLevel QAGS-CNNDM QAGS-XSUM\nP R F1 TER P R F1 TER\nSent 76.68 73.85 72.62 4.45 63.62 68.48 58.58 3.52\nEnt 65.71 55.98 47.04 0.30 62.19 58.19 36.65 0.14\nSent+Ent 78.22 77 .03 76 .42 4.76 65.07 70 .68 61 .48 3.66\nThe best results are in bold\nencompass all semantic information. However, when com-\nbined, the entity-level semantic pieces introduce additional\ninformation for the veriﬁcation module to consider, thereby\nenhancing the overall performance of the hierarchical seman-\ntic piece extraction method.\nError analysis\nTo further understand the ability range of the HSP method,\na manual analysis of a proportion of the error samples was\nconducted, and a number of primary error types were sum-\nmarized. Limited by space, we provide detailed error samples\nin the Appendix C.\nComplex text format: Some texts are in complex formats,\nleading to difﬁculty in extracting semantic pieces. Seman-\ntic piece extraction is the foundation of our method. If the\nextraction is incorrect, the subsequent reasoning tasks will\nbe affected.\nWrong evidence matching: In our method, cosine sim-\nilarity is used to match the evidence, for computational\nefﬁciency we use top-3 evidence. However, the embedding-\nbased cosine similarity does not always align perfectly with\nthe evidence. Optimal values for top-k vary across datasets,\nand incorrect settings can result in insufﬁcient or excessive\nevidence.\nNLI error: We use LLM for NLI task, sometimes the model\nmakes mistakes in reasoning. Occasionally, the model com-\nmits reasoning errors, which affect the ﬁnal outcome as this\nstep is the concluding phase of our method.\nHallucination mitigation\nFor the hallucination mitigation task, we adopt an end-to-end\ntesting method. First, we detect whether there is hallucination\nin the text, and then mitigate the hallucination. By compar-\ning the original text with the reﬁned text, we calculate the\nconsistency with the reference text.\nDatasets\nWe use four datasets for the hallucination mitigation task,\nincluding HaluEval Summarization, CRUD-QA-Qwen,\nCRUD-QA-SLM, and SummEval.\n123\nComplex & Intelligent Systems (2025) 11 :231 Page 13 of 19 231\nTable 8 Hallucination mitigation results on different datasets\nDataset Method Rouge1 Rouge2 RougeL Bleu avg Bleu-4 BertScore AlignScore FactCC\nHaluEval Summ Original 57.32 33.88 45.12 23.17 10.45 80.97 18.43 17.62\nPrompt 56.33 33.51 44.41 22.68 10.33 80.94 20.72 16.86\nHSP 57.45 34 .40 45 .27 24 .13 10 .96 81 .57 22 .20 17 .72\nCRUD QA Qwen Original 71.51 60.86 68.33 58.83 50 .84 91.75 75.14 –\nPrompt 70.29 59.89 67.16 58.06 50.25 91.58 72.79 –\nHSP 72.00 61 .00 68 .59 58.55 50.54 92.05 74.18 –\nCRUD QA SLM Original 37.32 25.16 34.37 26.00 20.70 79.86 46.51 –\nPrompt 33.16 22.55 30.48 23.82 19.13 78.82 43.65 –\nHSP 40.43 27 .90 37 .32 28 .18 22 .30 81 .38 46.42 –\nSummEval Original 66.47 49.19 55.98 48.75 36.48 83.11 35.78 19.93\nPrompt 64.99 46.89 54.84 46.16 33.98 82.16 35.53 19.59\nHSP 66.79 49 .89 55.92 49.67 36 .61 84 .87 35.63 20.94\nThe best results for each dataset are in bold. For FactCC metric, it doesn’t support other language except English, so we only report the results on\nHaluEval Summ and SummEval dataset\n• HaluEval Summ [ 57]: a subset of HaluEval dataset, we\nsample 250 samples for the hallucination mitigation task.\nEach sample contains hallucination text, reference text,\nand correct answer.\n• CRUD-QA [ 58]: CRUD Benchmark is a benchmark for\nevaluating the complete RAG process. From the News\nQA section of the dataset, we sampled 143 instances.\nWe use Qwen1.5-32B-Chat and an internal tiny language\nmodel to generate answers, thereby creating the CRUD-\nQA-Qwen and CRUD-QA-SLM datasets. The results of\nQwen1.5-32B-Chat are largely acceptable, and this set\nwill be used to ensure that our method does not over-\nmitigate.\n• SummEval [ 52]: For the mitigation task, we merge all\nsummaries in the SummEval dataset according to the ref-\nerence text. For each reference text, there are multiple\nsummaries, including hallucination content and correct\ncontent. We select all hallucination summaries to con-\nstruct the mitigation task dataset, which contains 294\nsamples. Each sample contains hallucination text, ref-\nerence text, and correct answer.\nExperimental results\nFor comparison, we use the prompt-based method as a base-\nline; the prompt-based method instructs the model to repair\nthe text directly. For all experiments, Qwen2-72B-Instruct is\nadopted for detection and mitigation, and the generated text\nis compared with the annotated correct answer. The results\nare shown in Table 8.\nOur method effectively optimizes the output content,\nbringing it closer to the reference answer. On the CRUD-QA-\nQwen dataset, it successfully maintains the original answer\nwithout over-mitigation. In comparison to the common\nprompt-based method, which often results in performance\ndecline, our approach avoids over-mitigation. The prompt-\nbased method tends to introduce unnecessary information\nand suffers from low hallucination detection accuracy, which\nleads to performance degradation.\nDiscussion and conclusion\nIn this paper, address the hallucination challenge in LLM-\nbased applications such as industry scenarios, a uniﬁed\nhallucination detection and mitigation framework is pro-\nposed. A novel method, hierarchical semantic piece (HSP),\nwhich uses hierarchical semantic piece extraction and evi-\ndence matching grounded in embedding cosine similarity, is\nintroduced. The experimental results show that our method\nhas better hallucination detection performance compared to\nthe baseline and existing methods, and reduces the token\nexpansion ratio (TER) by 30%, providing a more efﬁcient\nand cost-effective method. In terms of hallucination mitiga-\ntion, the HSP method can effectively optimize the output,\nmaking it closer to the reference answer while maintaining\noutput quality. We also conducted ablation experiments to\nverify the effectiveness of some designs and hyperparame-\nters choices in our method.\nOur method is built upon the capabilities of LLMs,\nalthough it has demonstrated lower computational cost com-\npared to other LLM-Based methods, it still can’t compete\nwith optimized small models in terms of efﬁciency. In the\nfuture we will consider using smaller models to achieve\nhigher efﬁciency and work towards real-time hallucination\ndetection and mitigation. In addition, some hyperparameters\n123\n231 Page 14 of 19 Complex & Intelligent Systems (2025) 11 :231\nof our processes in the current method still need to be manu-\nally tuned, and in the future we plan to dynamically optimize\nthese hyperparameters so that the method can automatically\nadapt to various scenarios.\nAppendix A: Prompt templates\nIn this section, we provide all the prompt templates we used\nin our experiments. All variables in the prompt templates are\ndenoted by curly braces.\nA.1 Sentence-level semantic piece extraction\nPlease breakdown the user input sentence into independent\nfacts.\n**Output format**: Use a newline character to separate each\nsemantic piece to ensure clear distinction between each piece.\n[Input sentence]\n{text}\nA.2 NLI task\nDetermine the relation between the premise and hypothe-\nsis. Available relations include entailment, contradiction, and\nneutral.\n[Classes]\n* Entailment: The hypothesis can be inferred from the\npremise.\n* Contradiction: The hypothesis contradicts the premise.\n* Neutral: The hypothesis neither entails nor contradicts the\npremise, or its relation cannot be determined.\n**Output format**: Output only \"Contradiction\", \"Entail-\nment\" to indicate the relation between the premise and\nhypothesis.\nNow it’s your turn! Please determine the relation between\nthe premise and hypothesis, carefully consider some special\ninformation such as time, location, and numbers.\n[Premise]\n{premise}\n[Hypothesis]\n{hypothsis}\nA.3 Hallucination mitigation\nY ou are a professional editor. Here is a sentence may have\nissues. Please directly correct the sentence with reasoning.\nI will give you some reference information to help you ﬁnd\nthe error. If you can’t ﬁnd any error, please return Correct in\nreason part, remember to keep the original language.\n**Output Format**\nThe output includes a reason, a correct sentence, and the\nﬁnal sentence. Please form them in the following format:\n[Reason]\n<reason>\n[Correct Sentence]\n<sentence>\n[Final Sentence]\n<ﬁnal_ﬁx_result >\nNow is your time to show your editing skills.\n[Reference Text]\n{premises}\n[Sentence to Fix]\n{hypothesis}\nA.4 Prompt-based hallucination detection\nGive the following premise and hypothesis, determine the\nrelationship between them, available choices are:\n* Entailment: The hypothesis can be inferred from the\npremise.\n* Contradiction: The hypothesis contradicts the premise.\n* Neutral: The hypothesis neither entails nor contradicts the\npremise, or its relation cannot be determined.\nExample 0:\nPremise: The quick brown fox jumps over the lazy dog.\nHypothesis: The fox is lazy.\nRelationship: Contradiction\nExample 1:\nPremise: Mrs. Smith is a teacher who loves to read.\nHypothesis: Mrs. Smith is a teacher.\nRelationship: Entailment\nExample 2:\nPremise:\nThe sun is shining in the sky. Hypothesis: A ﬁre is burning\nin the ﬁreplace.\nRelationship: Neutral Now is your turn:\n[Premise]\n{premise}\n[Hypothesis]\n{hypothesis}\nA.5 Prompt-based hallucination mitigation\nY ou are a professional editor. Here is a sentence may have\nissues. Please directly correct the sentence.\nI will give you some reference information to help you ﬁnd\nthe error.\nIf you can’t ﬁnd any error, please return Correct in reason\npart, remember to keep the original language.\n**Output Format**\nThe output includes the ﬁnal sentence. Please form them in\nthe following format:\n[Final Sentence]\n123\nComplex & Intelligent Systems (2025) 11 :231 Page 15 of 19 231\n<ﬁnal_ﬁx_result >\n[Reference]\n{premise}\n[Sentence]\n{hypothesis}\nAppendix B: Detailed example of HSP method\nTo provide a more detailed understanding of the HSP method,\nwe present detailed examples of the hallucination detection\nand mitigation processes. Here is a hallucination example.\nReference T ext Jane was an American magazine created to\nappeal to the women who grew up reading “Sassy Magazine”;\nJane Pratt was the founding editor of each.First for Women\nis a woman’s magazine published by Bauer Media Group in\nthe USA.\nSentence to Detect While Jane and First for Women are both\npublications, one is aimed at women while the other is not.\nB.1 Hierarchical semantic piece extraction\nIn this part, our method extracts hierarchical semantic pieces\nfrom the reference text and the sentence to detect. The seman-\ntic pieces are extracted at the sentence level and the entity\nlevel. The entity in the entity-level sentence piece is high-\nlighted in red.\nSemantic Pieces of Reference T ext:\n1. Jane was an American magazine.\n2. Jane was created to appeal to the women who grew up\nreading “Sassy Magazine”.\n3. Jane Pratt was the founding editor of Jane.\n4. Jane Pratt was the founding editor of Sassy Magazine.\n5. First for Women is a woman’s magazine.\n6. First for Women is published by Bauer Media Group.\n7. First for Women is published in the USA.\n8. [ Jane Pratt (Type:Name) ] was the founding editor of\neach.\n9. First for Women is a woman’s magazine published by\n[Bauer (Type:Name)] Media Group in the USA.\nSemantic Pieces of the Sentence:\n1. Jane is a publication.\n2. First for Women is a publication.\n3. Jane is aimed at women.\n4. First for Women is not speciﬁcally aimed at women.\n5. While [ Jane (Type:Name)] and First for Women are both\npublications, one is aimed at women while the other is\nnot.\nB.2 Evidence matching and fact verification\nIn this part, we match the semantic pieces of the sentence to\ndetect with the reference semantic piece and verify the facts.\nThe matched reference semantic pieces are listed, and the\nNLI results are provided (Table 9).\nDetection Result: The sentence contains hallucination, as it\ncontradicts the reference text.\nB.3 Hallucination mitigation\nIn this part, we mitigate the hallucination in the sentence to\ndetect. The reﬁned sentence is presented, and the comparison\nwith the reference text is provided (Table 10).\n• Original: While Jane and First for Women are both pub-\nlications, one is aimed at women while the other is not.\n• Fixed Sentence : While Jane and First for Women are\nboth publications aimed at women, they have different\nfocuses and audiences.\nAppendix C: Error examples\nC.1 Semantic piece extraction error\nIn following example, the semantic piece extraction module\nfails to extract. The reference text has a complex format,\nwhich makes it difﬁcult for LLM to understand. As a result,\nLLM directly responds original text, lead to wrong semantic\npiece extraction.\n123\n231 Page 16 of 19 Complex & Intelligent Systems (2025) 11 :231\nTable 9 Evidence matching and fact veriﬁcation results\nSemantic piece Matched reference semantic piece NLI result\nJane is a publication - Jane was created to appeal to the women who grew up\nreading “Sassy Magazine”\nEntailment\n- Jane was an American magazine\n- Jane Pratt was the founding editor of Jane\nFirst for Women is a publication - First for Women is published in the USA Entailment\n- First for Women is a woman’s magazine\n- First for Women is published by Bauer Media Group\nJane is aimed at women - Jane Pratt was the founding editor of Jane Entailment\n- Jane was an American magazine\n- Jane was created to appeal to the women who grew up\nreading “Sassy Magazine”\nFirst for Women is not speciﬁcally aimed at women - First for Women is a woman’s magazine published by\nBauer Media Group in the USA\nContradiction\n- First for Women is a woman’s magazine\n- First for Women is published by Bauer Media Group\nWhile Jane and First for Women are both\npublications, one is aimed at women while the\nother is not\n- First for Women is a woman’s magazine published by\nBauer Media Group in the USA\nNeutral\nTable 10 Hallucination\nmitigation results Hallucinated piece Original sentence Reﬁned sentence\nFirst for Women is not speciﬁcally\naimed at women\nWhile Jane and First for Women\nare both publications, one is\naimed at women while the other\nis not\nWhile Jane and First for Women\nare both publications aimed at\nwomen, they have different\nfocuses and audiences\nComplex reference text Example\nInput T ext:\nAt December 31,\nMaturities\nInterest\nRates %\n2022\n2021\nV erizon Communica-\ntions\n< 5 Years\n0.75 - 5.82\n$\n23,929\n$\n18,406\n5-10 Years\n…\nExtract semantic\npieces:\n1. At December 31\n2. Maturities\n3. Interest\n4. Rates %\n5. 2022\n6. 2021\n7. V erizon Communi-\ncations\n8. < 5 Years\n9. 0.75 - 5.82\n10. $\n11. 23,929\n12. $\n13. 18,406\n14. 5-10 Years\n…\nC.2 Wrong evidence matching\nIn the following example, the evidence matching module fails\nto match the correct evidence in top-3 evidence, the correct\nevidence is the 4th evidence, as shown in the Table 11, leading\nto a wrong detection result.\nHypothesis Semantic Pieces: Shenyang boasts the highest\nurban population in Northeast China.\nC.3 NLI error\nIn the following example, the NLI module makes a mistake\nin reasoning, leading to an incorrect result. In the hypothesis,\n\"born with\" is a key phrase that indicates a contradiction, but\nthe model incorrectly predicts an entailment relationship.\n123\nComplex & Intelligent Systems (2025) 11 :231 Page 17 of 19 231\nTable 11 Reference semantic pieces and matching scores\nEvidence Text Score\nShenyang is the largest city in Northeast\nChina by urban population\n0.8949\nShenyang is the most populous city in\nLiaoning province\n0.8508\nThe Shenyang metropolitan area is one of\nthe major megalopolises in China\n0.8107\nShenyang is the second-largest city in\nNortheast China by metropolitan\npopulation, behind Harbin.\n0.8088\nThe Shenyang metropolitan area has a\npopulation of over 23 million\n0.7967\nShenyang is a sub-provincial city in China 0.7891\nShenyang is the provincial capital of\nLiaoning province\n0.7656\nShenyang has a population of 9,070,093\nas of the 2020 census\n0.7546\nShenyang’s administrative region includes\nten metropolitan districts\n0.7036\nShenyang’s administrative region includes\nthe county-level city of Xinmin\n0.6936\nShenyang’s administrative region includes\nthe counties of Kangping and Faku\n0.6868\nNLI Error Example\nPremises:\n- Maickel melamed is a venezuelan native battling\nmuscular dystrophy.\n- Despite the odds against him, maickel melamed\ncrossed the ﬁnish line.\n- Maick melamed wants to show that life is great, no\nmatter how many problems you can have.\nHypothesis:\n- Maickel Melamed was born with muscular dystro-\nphy.\nNLI Result:\n- Entailment\nCorrect Label:\n- Contradiction\nAppendix D: Entity Extraction Schema\nSee Table 12.\nTable 12 Entity extraction schema\nType Schema\nTime Date, Calendar date, Date range, Year,\nMonth, Time, Time range, End time, Start\ntime, Time period, Duration\nNumber Price, Cost, Expenditure, Spending, Expense,\nQuantity, Amount, Number, Total, Ratio,\nPercentage, Rate, Proportion, Maximum\nvalue, Minimum value, Percentile, Length,\nArea, V olume, Weight, Temperature\nLocation City, Country, Province, State, Region, Area,\nStreet, Address, Place, Building, Scenic\nspot, Landmark\nPersonal Info Name, First name, Last name, Position,\nDegree, Education, Graduated from,\nEmployer\nAcknowledgements This study is supported by Liaoning Provincial\nScience and Technology Innovation Project in the Field of Artiﬁcial\nIntelligence (Project name: Research on key technologies for systems\nengineering of large language model) (Grant no. 2023JH26/10100005);\nShenyang Y oung and Middle-aged Science and Technology Innovation\nTalent Support Program (Grant no. RC220414).\nData availability The data used in this study are available from the\ncorresponding author upon reasonable request.\nDeclarations\nConﬂict of interest On behalf of all authors, the corresponding author\nstates that there is no conﬂict of interest.\nOpen Access This article is licensed under a Creative Commons\nAttribution-NonCommercial-NoDerivatives 4.0 International License,\nwhich permits any non-commercial use, sharing, distribution and repro-\nduction in any medium or format, as long as you give appropriate credit\nto the original author(s) and the source, provide a link to the Creative\nCommons licence, and indicate if you modiﬁed the licensed mate-\nrial. Y ou do not have permission under this licence to share adapted\nmaterial derived from this article or parts of it. The images or other\nthird party material in this article are included in the article’s Creative\nCommons licence, unless indicated otherwise in a credit line to the\nmaterial. If material is not included in the article’s Creative Commons\nlicence and your intended use is not permitted by statutory regula-\ntion or exceeds the permitted use, you will need to obtain permission\ndirectly from the copyright holder. To view a copy of this licence, visit\nhttp://creativecommons.org/licenses/by-nc-nd/4.0/.\nReferences\n1. Zhao WX, Zhou K, Li J, Tang T, Wang X, Hou Y , Min Y , Zhang\nB, Zhang J, Dong Z, Du Y Yang C, Chen Y , Chen Z, Jiang J, Ren\nR, Li Y , Tang X, Liu Z, Liu P , Nie J-Y , Wen J-R (2023) A survey\nof large language models. arXiv:2303.18223\n2. Min B, Ross H, Sulem E, V eyseh A, Nguyen TH, Sainz O, Agirre\nE, Heintz I, Roth D (2023) Recent advances in natural language\nprocessing via large pre-trained language models: a survey. ACM\nComput Surv 56(2):1–40\n123\n231 Page 18 of 19 Complex & Intelligent Systems (2025) 11 :231\n3. Morris MR, Sohl-Dickstein J, Fiedel N, Warkentin T, Dafoe A,\nFaust A, Farabet C, Legg S (2023) Levels of agi: operationalizing\nprogress on the path to agi. arXiv:2311.02462\n4. Zhang C, Zhang C, Li C, Qiao Y , Zheng S, Dam SK, Zhang M,\nKim JU, Kim ST, Choi J et al (2023) One small step for generative\nai, one giant leap for agi: a complete survey on chatgpt in aigc era.\narXiv:2304.06488\n5. Kaddour J, Harris J, Mozes M, Bradley H, Raileanu R, McHardy\nR (2023) Challenges and applications of large language models.\narXiv:2307.10169\n6. Ji Z, Lee N, Frieske R, Y u T, Su D, Xu Y , Ishii E, Bang YJ, Madotto\nA, Fung P (2023) Survey of hallucination in natural language gen-\neration. ACM Comput Surv 55(12):1–38\n7. Zhang Y , Li Y , Cui L, Cai D, Liu L, Fu T, Huang X, Zhao E, Zhang\nY , Chen Y et al (2023) Siren’s song in the ai ocean: a survey on\nhallucination in large language models. arXiv:2309.01219\n8. Achiam J, Adler S, Agarwal S, Ahmad L, Akkaya I, Aleman FL,\nAlmeida D, Altenschmidt J, Altman S, Anadkat S et al (2023) Gpt-4\ntechnical report. arXiv:2303.08774\n9. Team G, Anil R, Borgeaud S, Wu Y , Alayrac J-B, Y u J, Soricut R,\nSchalkwyk J, Dai AM, Hauth A et al (2023) Gemini: a family of\nhighly capable multimodal models. arXiv:2312.11805\n10. Tonmoy S, Zaman S, Jain V , Rani A, Rawte V , Chadha A, Das A\n(2024) A comprehensive survey of hallucination mitigation tech-\nniques in large language models. arXiv:2401.01313\n11. Penedo G, Malartic Q, Hesslow D, Cojocaru R, Alobeidli H, Cap-\npelli A, Pannier B, Almazrouei E, Launay J (2023) The reﬁnedweb\ndataset for falcon llm: outperforming curated corpora with web\ndata only. Adv Neural Inf Process Syst 36:79155–79172\n12. Tian K, Mitchell E, Yao H, Manning CD, Finn C (2023) Fine-tuning\nlanguage models for factuality. arXiv:2311.08401\n13. Wan F, Huang X, Cui L, Quan X, Bi W, Shi S (2024) Mitigating\nhallucinations of large language models via knowledge consistent\nalignment. arXiv:2401.10768\n1 4 . S u nZ ,S h e nS ,C a oS ,L i uH ,L iC ,S h e nY ,G a nC ,G u iL - Y ,W a n g\nY -X, Yang Y et al (2023) Aligning large multimodal models with\nfactually augmented rlhf. arXiv:2309.14525\n15. Li K, Patel O, Viégas F, Pﬁster H, Wattenberg M (2024) Inference-\ntime intervention: eliciting truthful answers from a language model.\nIn: Advances in neural information processing systems, vol 36\n16. Gou Z, Shao Z, Gong Y , Shen Yang Y , Duan N, Chen W\n(2024) CRITIC: large language models can self-correct with\ntool-interactive critiquing. In: The 12th international confer-\nence on learning representations. https://openreview.net/forum?\nid=Sx038qxjek\n17. Manakul P , Liusie A, Gales MJ (2023) Selfcheckgpt: zero-resource\nblack-box hallucination detection for generative large language\nmodels. arXiv:2303.08896\n18. Hu L, Liu Z, Zhao Z, Hou L, Nie L, Li J (2023) A survey of knowl-\nedge enhanced pre-trained language models. IEEE Trans Knowl\nData Eng\n19. Wang C, Liu X, Y ue Y , Tang X, Zhang T, Jiayang C, Yao Y ,\nGao W, Hu X, Qi Z et al (2023) Survey on factuality in large\nlanguage models: knowledge, retrieval and domain-speciﬁcity.\narXiv:2310.07521\n20. Shuster K, Poff S, Chen M, Kiela D, Weston J (2021)\nRetrieval augmentation reduces hallucination in conversation.\narXiv:2104.07567\n21. Novelli C, Casolari F, Hacker P , Spedicato G, Floridi L (2024)\nGenerative ai in eu law: liability, privacy, intellectual property, and\ncybersecurity. arXiv:2401.07348\n22. Thirunavukarasu AJ, Ting D, Elangovan K, Gutierrez L, Tan TF,\nTing D (2023) Large language models in medicine. Nat Med\n29(8):1930–1940\n23. Paul D, Namperumal G, Surampudi Y (2023) Optimizing llm train-\ning for ﬁnancial services: best practices for model accuracy, risk\nmanagement, and compliance in ai-powered ﬁnancial applications.\nJ Artif Intell Res Appl 3(2):550–588\n24. Yao Y , Duan J, Xu K, Cai Y , Sun Z, Zhang Y (2024) A survey on\nlarge language model (llm) security and privacy: the good, the bad,\nand the ugly. High-Conﬁd Comput 100211\n25. Urlana A, Kumar CV , Singh AK, Garlapati BM, Chalamala SR,\nMishra R (2024) Llms with industrial lens: deciphering the chal-\nlenges and prospects–a survey. arXiv:2402.14558\n26. Touvron H, Martin L, Stone K, Albert P , Almahairi A, Babaei Y ,\nBashlykov N, Batra S, Bhargava P , Bhosale S et al (2023) Llama\n2: open foundation and ﬁne-tuned chat models. arXiv:2307.09288\n27. Jiang AQ, Sablayrolles A, Mensch A, Bamford C, Chaplot DS,\nCasas Ddl, Bressand F, Lengyel G, Lample G, Saulnier L et al\n(2023) Mistral 7b. arXiv:2310.06825\n28. Bai J, Bai S, Chu Y , Cui Z, Dang K, Deng X, Fan Y , Ge W, Han Y ,\nHuang F et al (2023) Qwen technical report. arXiv:2309.16609\n29. Devlin J, Chang M-W, Lee K, Toutanova K (2018) Bert: pre-\ntraining of deep bidirectional transformers for language under-\nstanding. arXiv:1810.04805\n30. Marvin G, Hellen N, Jjingo D, Nakatumba-Nabende J (2023)\nPrompt engineering in large language models. In: International con-\nference on data intelligence and cognitive informatics. Springer, pp\n387–402\n31. Lewis P , Perez E, Piktus A, Petroni F, Karpukhin V , Goyal N,\nKüttler H, Lewis M, Yih W-T, Rocktäschel T et al (2020) Retrieval-\naugmented generation for knowledge-intensive nlp tasks. Adv\nNeural Inf Process Syst 33:9459–9474\n32. Li H, Su Y , Cai D, Wang Y , Liu L (2022) A survey on retrieval-\naugmented text generation. arXiv:2202.01110\n33. Xi Z, Chen W, Guo X, He W, Ding Y , Hong B, Zhang M, Wang J,\nJin S, Zhou E et al (2023) The rise and potential of large language\nmodel based agents: a survey. arXiv:2309.07864\n34. Guo T, Chen X, Wang Y , Chang R, Pei S, Chawla NV , Wiest O,\nZhang X (2024) Large language model based multi-agents: a survey\nof progress and challenges. arXiv:2402.01680\n35. Chen W, Yan-yi L, Tie-zheng G, Da-peng L, Tao H, Zhi L, Qing-\nwen Y , Hui-han W, Ying-you W (2024) Systems engineering issues\nfor industry applications of large language model. Appl Soft Com-\nput 151:111165\n36. Asai A, Wu Z, Wang Y , Sil A, Hajishirzi H (2023) Self-rag:\nlearning to retrieve, generate, and critique through self-reﬂection.\narXiv:2310.11511\n37. Ji Z, Y u T, Xu Y , Lee N, Ishii E, Fung P (2023) Towards mitigating\nllm hallucination via self reﬂection. In: Findings of the association\nfor computational linguistics: EMNLP 2023, pp 1827–1843\n38. Wei J, Wang X, Schuurmans D, Bosma M, Xia F, Chi E, Le QV ,\nZhou D et al (2022) Chain-of-thought prompting elicits reasoning\nin large language models. Adv Neural Inf Process Syst 35:24824–\n24837\n39. Lyu Q, Havaldar S, Stein A, Zhang L, Rao D, Wong E, Apidianaki\nM, Callison-Burch C (2023) Faithful chain-of-thought reasoning.\narXiv:2301.13379\n40. Waldendorf J, Haddow B, Birch A (2024) Contrastive decoding\nreduces hallucinations in large multilingual machine translation\nmodels. In: Proceedings of the 18th conference of the European\nchapter of the association for computational linguistics (volume 1:\nlong papers), pp 2526–2539\n41. Li D, Sun Z, Hu X, Liu Z, Chen Z, Hu B, Wu A, Zhang M (2023)\nA survey of large language models attribution. arXiv:2311.03731\n42. Min S, Krishna K, Lyu X, Lewis M, Yih W-t, Koh PW, Iyyer\nM, Zettlemoyer L, Hajishirzi H (2023) Factscore: ﬁne-grained\natomic evaluation of factual precision in long form text genera-\ntion. arXiv:2305.14251\n43. Wei J, Yang C, Song X, Lu Y , Hu N, Tran D, Peng D, Liu R, Huang\nD, Du C et al (2024) Long-form factuality in large language models.\narXiv:2403.18802\n123\nComplex & Intelligent Systems (2025) 11 :231 Page 19 of 19 231\n44. Gao L, Dai Z, Pasupat P , Chen A, Chaganty A T, Fan Y , Zhao V , Lao\nN, Lee H, Juan D-C, Guu K (2023) RARR: researching and revising\nwhat language models say, using language models. In: Rogers A,\nBoyd-Graber J, Okazaki N (eds) Proceedings of the 61st annual\nmeeting of the association for computational linguistics (volume 1:\nlong papers). Association for Computational Linguistics, Toronto,\npp 16477–16508. https://doi.org/10.18653/v1/2023.acl-long.910 .\nhttps://aclanthology.org/2023.acl-long.910\n45. Lei D, Li Y , Hu M, Wang M, Y un X (2023) Chain of natural lan-\nguage inference for reducing large language model hallucinations.\nIn: NeurIPS 2023 workshop on instruction tuning and instruction\nfollowing\n46. Lu Y , Liu Q, Dai D, Xiao X, Lin H, Han X, Sun L, Wu H (2022)\nUniﬁed structure generation for universal information extraction.\narXiv:2203.12277\n47. Yang A, Yang B, Hui B, Zheng B, Y u B, Zhou C, Li C, Li C, Liu D,\nHuang F et al (2024) Qwen2 technical report. arXiv:2407.10671\n48. Zhang T, Kishore V , Wu F, Weinberger KQ, Artzi Y (2019)\nBertscore: evaluating text generation with bert. arXiv:1904.09675\n49. Zha Y , Yang Y , Li R, Hu Z (2023) AlignScore: evaluating fac-\ntual consistency with a uniﬁed alignment function. In: Rogers A,\nBoyd-Graber J, Okazaki N (eds) Proceedings of the 61st annual\nmeeting of the association for computational linguistics (volume 1:\nlong papers). Association for Computational Linguistics, Toronto,\npp 11328–11348. https://doi.org/10.18653/v1/2023.acl-long.634 .\nhttps://aclanthology.org/2023.acl-long.634\n50. Kry´ sci´nski W, McCann B, Xiong C, Socher R (2019) Evalu-\nating the factual consistency of abstractive text summarization.\narXiv:1910.12840\n51. Wang A, Cho K, Lewis M (2020) Asking and answering ques-\ntions to evaluate the factual consistency of summaries. In:\nJurafsky D, Chai J, Schluter N, Tetreault J (eds) Proceed-\nings of the 58th annual meeting of the association for com-\nputational linguistics. Association for Computational Linguis-\ntics, pp 5008–5020. https://doi.org/10.18653/v1/2020.acl-main.\n450. https://aclanthology.org/2020.acl-main.450\n52. Fabbri AR, Kry´ sci´nski W, McCann B, Xiong C, Socher R,\nRadev D (2020) Summeval: re-evaluating summarization evalu-\nation. arXiv:2007.12626\n53. Ravi SS, Mielczarek B, Kannappan A, Kiela D, Qian R\n(2024) Lynx: an open source hallucination evaluation model.\narXiv:2407.08488\n54. Honovich O, Aharoni R, Herzig J, Taitelbaum H, Kukliansy D,\nCohen V , Scialom T, Szpektor I, Hassidim A, Matias Y (2022) True:\nre-evaluating factual consistency evaluation. arXiv:2204.04991\n55. Chen J, Xiao S, Zhang P , Luo K, Lian D, Liu, Z (2024)\nBge m3-embedding: multi-lingual, multi-functionality, multi-\ngranularity text embeddings through self-knowledge distillation.\narXiv:2402.03216\n56. Meng R, Liu Y , Joty SR, Xiong C, Zhou Y , Yavuz S (2024)\nSfrembedding-mistral: enhance text retrieval with transfer learn-\ning. Salesforce AI Research Blog, vol 3\n57. Li J, Cheng X, Zhao X, Nie J-Y , Wen J-R (2023) HaluEval: a large-\nscale hallucination evaluation benchmark for large language mod-\nels. In: Bouamor H, Pino J, Bali K (eds) Proceedings of the 2023\nconference on empirical methods in natural language processing.\nAssociation for Computational Linguistics, Singapore, pp 6449–\n6464. https://doi.org/10.18653/v1/2023.emnlp-main.397 . https://\naclanthology.org/2023.emnlp-main.397\n58. Lyu Y , Li Z, Niu S, Xiong F, Tang B, Wang W, Wu H, Liu H,\nXu T, Chen E (2024) Crud-rag: a comprehensive Chinese bench-\nmark for retrieval-augmented generation of large language models.\narXiv:2401.17043\nPublisher’s Note Springer Nature remains neutral with regard to juris-\ndictional claims in published maps and institutional afﬁliations.\n123",
  "topic": "Computational intelligence",
  "concepts": [
    {
      "name": "Computational intelligence",
      "score": 0.5802261233329773
    },
    {
      "name": "Computer science",
      "score": 0.5321869850158691
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4729699194431305
    },
    {
      "name": "Natural language processing",
      "score": 0.4642655849456787
    },
    {
      "name": "Linguistics",
      "score": 0.39528578519821167
    },
    {
      "name": "Psychology",
      "score": 0.33375489711761475
    },
    {
      "name": "Cognitive science",
      "score": 0.3237783908843994
    },
    {
      "name": "Philosophy",
      "score": 0.07796654105186462
    }
  ],
  "institutions": []
}