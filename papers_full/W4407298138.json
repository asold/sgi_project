{
    "title": "Investigating the Performance of Retrieval-Augmented Generation and Domain-Specific Fine-Tuning for the Development of AI-Driven Knowledge-Based Systems",
    "url": "https://openalex.org/W4407298138",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2674681296",
            "name": "Robert Lakatos",
            "affiliations": [
                "University of Debrecen",
                "John von Neumann University"
            ]
        },
        {
            "id": "https://openalex.org/A2043538959",
            "name": "Péter Pollner",
            "affiliations": [
                "Semmelweis University"
            ]
        },
        {
            "id": "https://openalex.org/A1614850445",
            "name": "András Hajdú",
            "affiliations": [
                "University of Debrecen"
            ]
        },
        {
            "id": "https://openalex.org/A2147199690",
            "name": "Tamás Joó",
            "affiliations": [
                "Semmelweis University",
                "John von Neumann University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2964110616",
        "https://openalex.org/W4389727268",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W4391855109",
        "https://openalex.org/W6779857854",
        "https://openalex.org/W2913352150",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W6682631176",
        "https://openalex.org/W2123301721",
        "https://openalex.org/W3174544005",
        "https://openalex.org/W3095203335",
        "https://openalex.org/W2970641574",
        "https://openalex.org/W6632926901",
        "https://openalex.org/W2180566385",
        "https://openalex.org/W1548207260"
    ],
    "abstract": "Generative large language models (LLMs) have revolutionized the development of knowledge-based systems, enabling new possibilities in applications like ChatGPT, Bing, and Gemini. Two key strategies for domain adaptation in these systems are Domain-Specific Fine-Tuning (DFT) and Retrieval-Augmented Generation (RAG). In this study, we evaluate the performance of RAG and DFT on several LLM architectures, including GPT-J-6B, OPT-6.7B, LLaMA, and LLaMA-2. We use the ROUGE, BLEU, and METEOR scores to evaluate the performance of the models. We also measure the performance of the models with our own designed cosine similarity-based Coverage Score (CS). Our results, based on experiments across multiple datasets, show that RAG-based systems consistently outperform those fine-tuned with DFT. Specifically, RAG models outperform DFT by an average of 17% in ROUGE, 13% in BLEU, and 36% in CS. At the same time, DFT achieves only a modest advantage in METEOR, suggesting slightly better creative capabilities. We also highlight the challenges of integrating RAG with DFT, as such integration can lead to performance degradation. Furthermore, we propose a simplified RAG-based architecture that maximizes efficiency and reduces hallucination, underscoring the advantages of RAG in building reliable, domain-adapted knowledge systems.",
    "full_text": null
}