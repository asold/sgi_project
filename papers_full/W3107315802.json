{
  "title": "CPM: A Large-scale Generative Chinese Pre-trained Language Model",
  "url": "https://openalex.org/W3107315802",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2348152382",
      "name": "Zhang ZhengYan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096209639",
      "name": "Han Xu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2106821041",
      "name": "Zhou Hao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096751096",
      "name": "Ke Pei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222962379",
      "name": "Gu, Yuxian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2569237400",
      "name": "Ye, Deming",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2361432882",
      "name": "Qin, Yujia",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2316431985",
      "name": "su yu-sheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224461102",
      "name": "Ji, Haozhe",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1932495077",
      "name": "Guan Jian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221809264",
      "name": "Qi, Fanchao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2222342641",
      "name": "Wang Xiao-zhi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2349028411",
      "name": "Zheng Yanan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227153058",
      "name": "Zeng, Guoyang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2488703460",
      "name": "Cao, Huanqi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2315586633",
      "name": "Chen Sheng-qi",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Li, Daixuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2347558725",
      "name": "Sun Zhen-bo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1860873322",
      "name": "Liu Zhi-Yuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2570381884",
      "name": "Huang, Minlie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2224166589",
      "name": "Han Wen-Tao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2076873566",
      "name": "Tang Jie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2057730499",
      "name": "Li, Juanzi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2042466159",
      "name": "Zhu Xiao-yan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2381112680",
      "name": "Sun, Maosong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2995435108",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3099414857",
    "https://openalex.org/W3099911888",
    "https://openalex.org/W3102725307",
    "https://openalex.org/W2328886022",
    "https://openalex.org/W3005700362",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2949884065",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2994915912",
    "https://openalex.org/W3093956460",
    "https://openalex.org/W3015253856",
    "https://openalex.org/W2963963856",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W3106031450",
    "https://openalex.org/W2995998574",
    "https://openalex.org/W3034199299",
    "https://openalex.org/W2971871542",
    "https://openalex.org/W3105111366",
    "https://openalex.org/W2799037524",
    "https://openalex.org/W2963957489",
    "https://openalex.org/W3081168214",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2963035145",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W3210120707",
    "https://openalex.org/W2996287690",
    "https://openalex.org/W3182352988",
    "https://openalex.org/W3151929433"
  ],
  "abstract": "Pre-trained Language Models (PLMs) have proven to be beneficial for various downstream NLP tasks. Recently, GPT-3, with 175 billion parameters and 570GB training data, drew a lot of attention due to the capacity of few-shot (even zero-shot) learning. However, applying GPT-3 to address Chinese NLP tasks is still challenging, as the training corpus of GPT-3 is primarily English, and the parameters are not publicly available. In this technical report, we release the Chinese Pre-trained Language Model (CPM) with generative pre-training on large-scale Chinese training data. To the best of our knowledge, CPM, with 2.6 billion parameters and 100GB Chinese training data, is the largest Chinese pre-trained language model, which could facilitate several downstream Chinese NLP tasks, such as conversation, essay generation, cloze test, and language understanding. Extensive experiments demonstrate that CPM achieves strong performance on many NLP tasks in the settings of few-shot (even zero-shot) learning. The code and parameters are available at https://github.com/TsinghuaAI/CPM-Generate.",
  "full_text": "CPM: A Large-scale Generative Chinese Pre-trained Language Model\nZhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye,\nYujia Qin, Yusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng,\nGuoyang Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun,\nZhiyuan Liu†, Minlie Huang†, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun\nDepartment of Computer Science and Technology, Tsinghua University & BAAI\nAbstract\nPre-trained Language Models (PLMs) have\nproven to be beneﬁcial for various downstream\nNLP tasks. Recently, GPT-3, with 175 bil-\nlion parameters and 570GB training data, drew\na lot of attention due to the capacity of few-\nshot (even zero-shot) learning. However, ap-\nplying GPT-3 to address Chinese NLP tasks\nis still challenging, as the training corpus\nof GPT-3 is primarily English, and the pa-\nrameters are not publicly available. In this\ntechnical report, we release the Chinese Pre-\ntrained Language Model (CPM) with genera-\ntive pre-training on large-scale Chinese train-\ning data. To the best of our knowledge, CPM,\nwith 2.6 billion parameters and 100GB Chi-\nnese training data, is the largest Chinese pre-\ntrained language model, which could facili-\ntate several downstream Chinese NLP tasks,\nsuch as conversation, essay generation, cloze\ntest, and language understanding. Extensive\nexperiments demonstrate that CPM achieves\nstrong performance on many NLP tasks in\nthe settings of few-shot (even zero-shot) learn-\ning. The code and parameters are avail-\nable at https://github.com/TsinghuaAI/CPM-\nGenerate.\n1 Introduction\nPre-trained Language Models (PLMs) (Peters et al.,\n2018; Radford et al., 2018; Devlin et al., 2019;\nBrown et al., 2020) have been developed for a\nvariety of tasks in Natural Language Processing\n(NLP), as they can learn rich language knowledge\nfrom large-scale corpora, which is beneﬁcial for\ndownstream tasks. ELMo (Peters et al., 2018)\nﬁrst introduces bidirectional language models to\nlearn contextual word vectors via large-scale pre-\ntraining. GPT (Radford et al., 2018) applies gener-\native pre-training to a Transformer-based language\n† Corresponding authors: Z. Liu (liuzy@tsinghua.edu.cn)\nand M. Huang (aihuang@tsinghua.edu.cn)\nmodel (Vaswani et al., 2017), which improves nat-\nural language understanding on a wide range of\nbenchmarks. BERT (Devlin et al., 2019) is pro-\nposed to pre-train deep bidirectional representa-\ntions on unlabeled texts by jointly conditioning on\nboth left and right contexts. RoBERTa (Liu et al.,\n2019) and ALBERT (Lan et al., 2020) enhance\nBERT (Devlin et al., 2019) by dynamic masking,\nparameter sharing and modifying pre-training tasks.\nERNIE (Zhang et al., 2019), KEPLER (Wang\net al., 2019) and SentiLARE (Ke et al., 2020) in-\ntroduce external knowledge to language represen-\ntation learning by auxiliary pre-training tasks.\nAmong these PLMs, GPT-3 (Brown et al., 2020),\nwith 175 billion parameters and 570GB training\ndata, has been the center of attention and proven\nto be effective in various few-shot (even zero-shot)\nNLP tasks. The powerful text generation capability\nof GPT-3 makes it available to diverse applications,\nsuch as question answering, summarization, con-\nversation, computing basic arithmetic, and gener-\nating kinds of text, including essay, ﬁction, code,\nspreadsheets, etc. However, incorporating GPT-3\nto address Chinese NLP tasks is still challenging,\nas the training corpus of GPT-3 is primarily En-\nglish (93% by word counting as reported by Brown\net al. (2020)), and the parameters are not publicly\navailable. Although there are some previous works\nproviding powerful Chinese pre-trained language\nmodels (Cui et al., 2020; Xu et al., 2020; Wei et al.,\n2019; Sun et al., 2019; Cui et al., 2019a), their ca-\npabilities are limited due to the model size. Hence,\nhow to pre-train a large-scale Chinese language\nmodel needs more exploration, such as the con-\nstruction of Chinese vocabulary and the design of\nthe training strategy.\nIn this technical report, we release the Chinese\nPre-trained Language Model ( CPM) with gen-\nerative pre-training on large-scale Chinese cor-\npora. CPM is a Transformer-based autoregressive\narXiv:2012.00413v1  [cs.CL]  1 Dec 2020\nnparam nlayers dmodel nheads dhead\nCPM-Small 109M 12 768 12 64\nCPM-Medium 334M 24 1,024 16 64\nCPM-Large 2.6B 32 2,560 32 80\nTable 1: Model sizes. nparam is the number of param-\neters. nlayers is the number of layers. dmodel is the di-\nmension of hidden states, which is consistent in each\nlayer. nheads is the number of attention heads in each\nlayer. dhead is the dimension of each attention head.\nlanguage model, with 2.6 billion parameters and\n100GB Chinese training data. To the best of our\nknowledge, CPM is the largest Chinese pre-trained\nlanguage model, which could facilitate downstream\nChinese NLP tasks, such as conversation, essay\ngeneration, cloze test, and language understanding.\nExperiments on various Chinese NLP tasks demon-\nstrate that CPM achieves strong performance on\nmany NLP tasks in the few-shot (even zero-shot)\nsettings. With the increase of parameters, CPM per-\nforms better on most datasets, indicating that larger\nmodels are more proﬁcient at language generation\nand language understanding.\nThe main contributions of this technical report\ncan be summarized as follows:\n• We release a Chinese autoregressive language\nmodel with generative pre-training, called\nCPM, which has 2.6 billion parameters.\n• We construct a new sub-word vocabulary\nbased on the word segmented corpus to adapt\nfor Chinese corpora and increase the batch\nsize to 3, 072 for more stable model training.\n• Extensive experiments demonstrate that CPM\nachieves strong performance on many NLP\ntasks in the few-shot (even zero-shot) settings.\n2 Our Approach\n2.1 Chinese PLM\nOur current model is a left-to-right Transformer\ndecoder, which is similar to the model architecture\nof GPT (Radford et al., 2019). We pre-train three\nmodels with different sizes, as shown in Table 1. In\norder to adapt CPM to Chinese corpora, we build\na new sub-word vocabulary and adjust the training\nbatch size.\nVocabulary Construction: Previous works on\nChinese pre-trained models usually adopt the sub-\nword vocabulary of BERT-Chinese (Devlin et al.,\n2019), which would split the input text to a\nData Source Encyclopedia Webpage Story News Dialog\nSize ∼40GB ∼39GB∼10GB∼10GB∼1GB\nTable 2: Details of training data.\ncharacter-level sequence. However, Chinese words\nusually contain several characters, and some impor-\ntant semantic meanings of words would be lost in\nthe character-level sequence. To solve this problem,\nwe construct a new sub-word vocabulary, contain-\ning both words and characters. For example, some\ncommon words would be added to the vocabulary.\nTraining Strategy: Since the sparseness of\nword distributions of Chinese is more serious than\nthat of English, we adopt a large batch size to\nmake the model training more stable. Compared\nto the batch size (1 million tokens) used in GPT-3\n2.7B (Brown et al., 2020), our batch size (3 mil-\nlion tokens) is two times larger. For the largest\nmodel, which cannot be stored in a single GPU dur-\ning training, we partition the model across GPUs\nalong the width dimension to make the large-scale\ntraining available and reduce data-transfer among\nnodes.\n2.2 Data Processing\nSpeciﬁcally, we construct a new sub-word vocab-\nulary based on the word segmented corpus using\nunigram language model(Kudo and Richardson,\n2018). Meanwhile, considering that the word seg-\nmentation introduces extra splitters between words,\nwe set a special token as the splitter to make the\nsub-word process reversible. In contrast, the tok-\nenizer of BERT-Chinese is irreversible because it\nwill insert extra spaces between Chinese characters\nand treat the extra spaces as the same as the original\nspaces in the text.\nWe collect different kinds of texts in our pre-\ntraining, including encyclopedia, news, novels, and\nQ&A. The details of our training data are shown in\nTable 2. Since the input sequence length is usually\nlarger than that of a single document, we concate-\nnate different documents together by adding “end\nof document” token after each document to make\nfull use of the input length.\n2.3 Pre-training Details\nBased on the hyper-parameter searching on the\nlearning rate and batch size, we set the learning rate\nas 1.5 × 10−4 and the batch size as 3, 072, which\nmakes the model training more stable. In the ﬁrst\nversion, we still adopt the dense attention and the\nmax sequence length is 1, 024. We will implement\nsparse attention in the future. We pre-train our\nmodel for 20, 000 steps, and the ﬁrst 5, 000 steps\nare for warm-up. The optimizer is Adam (Kingma\nand Ba, 2015). It takes two weeks to train our\nlargest model using 64 NVIDIA V100.\n3 Experiments\n3.1 Text Classiﬁcation\nDataset: We use TouTiao News Titles Classiﬁ-\ncation (TNEWS), IFLYTEK app description clas-\nsiﬁcation (IFLYTEK), and Original Chinese NLI\n(OCNLI) as our benchmark datasets for text clas-\nsiﬁcation (Xu et al., 2020; Hu et al., 2020). Since\nwe aim to evaluate the zero-shot ability of CPM on\ntext classiﬁcation tasks, we directly use the valida-\ntion sets of these three datasets without any train-\ning instance. The amount of the validation set of\nTNEWS / IFLYTEK / OCNLI is 10K / 2.6K / 3K.\nNote that, we exclude the instances with the label\n“-” in OCNLI.\nImplementation Details: We calculate the per-\nplexity of each candidate sentence-label pair and\ntreat the pair having the lowest perplexity as the\nprediction. The templates of these three tasks are\nformulated by\nTNEWS: 这是关于L的文章：P\n(This passage is about L: P),\nIFLYTEK: 这是关于L的应用程序：P\n(This application is about L: P),\nOCNLI: S1？对，S2 (S1? Yes, S2),\nS1？错，S2 (S1? No, S2),\nS1？也许，S2 (S1? Maybe, S2),\nwhere L is the label name, P is the input text, S1\nand S2 are the premise and hypothesis.\nSince TNEWS and IFLYTEK have more than\n10 kinds of labels, we adopt a simpler validation\nsetting, which randomly samples 3 false labels for\neach instance and performing 4-class classiﬁcation\nfor better efﬁciency. To make it more stable, we\nrepeat it three times and report the averaged re-\nsults. For OCNLI, which only has 3 kinds of labels,\nwe hold the original validation set. However, the\nvalidation set of OCNLI is unbalanced, where the\namount of “entailment” / “neutral” / “contradiction”\nis 947 / 1103 / 900. If the model only predicts the\nlabel “neutral”, the accuracy is about0.374.\nResults: As shown in Table 3, CPM-large\nachieves promising results on these classiﬁcation\nTNEWS IFLYTEK OCNLI\nCPM-Small 0.626 0.584 0.378\nCPM-Medium 0.618 0.635 0.379\nCPM-Large 0.703 0.708 0.442\nTable 3: Zero-shot performance on text classiﬁcation\ntasks (accuracy). Random prediction would have 0.25\non TNEWS and IFLYTEK, 0.33 on OCNLI.\nSupervised Unsupervised\nCPM-Small 0.657 0.433\nCPM-Medium 0.695 0.524\nCPM-Large 0.804 0.685\nTable 4: Results on ChID dataset in the supervised and\nunsupervised settings. The random prediction would\nhave 0.10 in the unsupervised setting.\ndatasets without any training samples. Com-\npared to random prediction, the knowledge learned\nfrom pre-training signiﬁcantly improves the per-\nformance. Although the medium model is three\ntimes as large as the small model, the performances\non TNEWS and OCNLI are very close. However,\nCPM-Large signiﬁcantly outperforms these two\nsmaller models on all three datasets. It indicates\nthat the magic of the model size is not linear and\nwould happen when the model size exceeds a spe-\nciﬁc boundary. Besides, the results of CPM-small\nand CPM-medium on OCNLI are close to that of\nthe strategy only predicting the label “neutral”. It\nsuggests that natural language inference is harder\nthan other downstream tasks in the setting of zero-\nshot learning, which is consistent with the observa-\ntion in Brown et al..\n3.2 Chinese Idiom Cloze\nDataset: We use the Chinese IDiom cloze test\ndataset (ChID) (Zheng et al., 2019) as our bench-\nmark dataset. Each passage in the dataset may\ncontain multiple blanks. For each blank, there are\n10 candidate idioms with 1 golden truth. Some of\nthe false candidates are similar to the answer in\nmeanings. The amount of training / validation / test\nset is 520K / 20K / 20K.\nImplementation Details: For the supervised\nsetting, we use a template to convert the passage\nand the candidates to a natural language question.\nGiven the passage P and 10 candidate idioms\nI1, I2, ..., I10, the template can be formulated as\nAverage Extrema Greedy Dist-1 Dist-2\nFew-shot (Unsupervised)\nCDial-GPT 0.899 0.797 0.810 1,963 / 0.011 20,814 / 0.126\nCPM-Large 0.928 0.805 0.815 3,229 / 0.007 68,008 / 0.154\nSupervised\nCDial-GPT 0.933 0.814 0.826 2,468 / 0.008 35,634 / 0.127\nCPM-Large 0.934 0.810 0.819 3,352 / 0.011 67,310 / 0.233\nTable 5: Results on STC dataset in the few-shot and supervised settings.\n选项1: I1 ... 选项10: I10 P 答案是:L\n(Option 1: I1 ... Option 10: I10 P Answer:L).\nThen, we train the model to predict the answer\nL. Note that if there exists more than one idiom\nin a passage, we predict each one independently.\nSpeciﬁcally, When we are predicting one idiom,\nwe leave its blank in the passage and remove the\nblanks of other idioms from the passage.\nFor the unsupervised setting, we ﬁll the can-\ndidate idioms into the blank to form a group of\ncomplete passages. We also consider each idiom\nblank individually if there are multiple blanks in a\npassage. For each blank, we can get 10 passages\ncorresponding to the 10 candidate idioms. Then we\ncalculate the perplexity of each passage and treat\nthe one with the lowest perplexity as the prediction.\nResults: The results are shown in Table 4.\nWe report the accuracy on the test set of each\nmodel. For the fully supervised setting, we can\nsee that CPM can be ﬁne-tuned for the speciﬁc\ninput template, solving multiple-choice tasks by\nuni-direction auto-regressive language modeling.\nIn our experiments, we didn’t take much time to\ndesign the input template for this task, and thus\nthere might exist better templates that can help the\nmodel to show its full ability. We will leave this\npart as future work. For the unsupervised setting,\nwe can see that CPM produces promising results.\nThe unsupervised result of CPM-Large even outper-\nforms the result of CPM-Small and is comparable\nto CPM-Medium in the supervised setting, reﬂect-\ning the strong power of CPM in Chinese language\nmodeling.\n3.3 Dialogue Generation\nDataset: We use Short-Text Conversation (STC)\n(Shang et al., 2015) as our benchmark dataset for di-\nalogue generation, which consists of post-response\npairs from Weibo. We adopt the same data split as\nthe existing work (Wang et al., 2020). The amount\nof training / validation / test set is 4.4M / 20K /\nPost: 现在高中数学竞赛这么凶残了！\nHow crazy the math competition of high\nschool is!\nCDial-GPT: 你还真信？\n(Few-shot) Do you really believe it?\nCDial-GPT: 数学让我们学会了很多，好好学，明天\n(Supervised) 回家\nMath lets us learn a lot. Study hard and go\nhome, tomorrow.\nCPM-Large: 现在都是微积分,大学的东西比高中简单\n(Few-shot) 多了,还有什么呢?\nNow it’s all calculus. The things in college\nare much simpler than those of high school.\nWhat else?\nCPM-Large: 这是要逼死我吗?每道题都不会做啊\n(Supervised) Is this going to kill me? I can’t answer\nany question.\nPost: 每天嘻嘻哈哈的笑着，只有自己才知道\n活的有多累。\nI laugh every day, but only I know how\ntired I am.\nCDial-GPT: 我也是\n(Few-shot) Me, too.\nCDial-GPT: 只有自己才知道活的有多累。\n(Supervised) Only I know how tired I am.\nCPM-Large: 当你想要哭泣的时候,就去那些熟悉的地\n(Few-shot) 方吧。\nWhen you want to cry, go to those familiar\nplaces.\nCPM-Large: 真的不知道,生活怎么会这么累\n(Supervised) I really don’t know how my life could be\nso tiring.\nTable 6: Examples of generated responses on STC.\n20K, respectively. The average length of posts /\nresponses is 20.6 / 15.4.\nBaseline: We choose CDial-GPT (Wang et al.,\n2020) as our baseline, which is the state-of-the-art\npre-trained model for Chinese dialogue generation.\nWe directly use the codes and the pre-trained model\nreleased by the original paper.\nImplementation Details: In the supervised ex-\nperiment, we utilize a similar hyper-parameter set-\nting as pre-training and ﬁne-tune CPM on the train-\ning set of STC. In the few-shot experiment which\ndoesn’t include the ﬁne-tuning process, we follow\nthe existing work (Radford et al., 2019; Brown\net al., 2020) to condition the language model on a\ncontext of 4 examples pairs of the format Context:\nAverage Dist-1 Dist-2\nCPM-Small 0.928 2,201 / 0.004 22,754 / 0.046\nCPM-Medium 0.910 2,842 / 0.005 31,934 / 0.058\nCPM-Large 0.928 3,229 / 0.007 68,008 / 0.154\nTable 7: Results of CPM with different amounts of pa-\nrameters on STC dataset in the few-shot setting.\nsentence Response: sentence. After a ﬁnal prompt\nContext: sentence Response:, we acquire the gener-\nation results with Top-p sampling (Holtzman et al.,\n2020), where p is set to 0.9. The temperature of\nsampling is 0.9 in both few-shot and supervised\nexperiments.\nMetrics: Since BLEU is not a proper metric\nfor dialogue generation, we use embedding-based\nmetrics (including greedy matching, embedding av-\nerage, and vector extrema) to evaluate the similarity\nbetween generated responses and references (Liu\net al., 2016). For diversity, we choose the number\nand proportion of distinct n-grams (Li et al., 2016;\nXing et al., 2017; Ke et al., 2018) as our metric.\nResults: We present the main results in the few-\nshot and supervised settings in Table 5. We can see\nthat CPM outperforms CDial-GPT with a large mar-\ngin in the few-shot experiment, showing the gener-\nalization ability of our model. As for the supervised\nexperiment, our model still performs better, espe-\ncially on the diversity metrics. Since ﬁne-tuning\nlarge pre-trained models on the supervised down-\nstream tasks is often challenging (Dodge et al.,\n2020; Mosbach et al., 2020; Lee et al., 2020), we\nleave how to further improve the performance in\nthe supervised setting as future work. Some cases\nare provided in Table 6 to intuitively show the ef-\nfectiveness of our model.\nWe also conduct experiments to show the few-\nshot performance of CPM with different parameter\nsizes in Table 7. As the number of parameters\ngrows, CPM can generate more diverse responses\nwith reasonable values on the embedding-based\nmetrics.\n3.4 Question Answering\nDataset: We adopt CMRC2018 (Cui et al., 2019b)\nand DuReader (He et al., 2018) as our benchmark\nfor Question Answering (QA). CMRC2018 re-\nquires the model to extract an answer span from a\nWikipedia passage for the given question, which\nis similar to SQuAD (Rajpurkar et al., 2016).\nDuReader consists of questions from real-world\nuser logs from Baidu Search and Baidu Zhidao.\nZhidao Search CMRC2018\nF1 EM F1 EM F1 EM\ns + zs 4.01 0.18 4.15 0.65 6.03 0.20\ns + os 4.75 0.34 4.45 0.59 6.14 0.22\nm + zs 5.29 0.29 5.03 0.61 8.60 0.53\nm + os 5.76 0.47 5.14 0.55 9.00 0.75\nl + zs 5.18 0.27 5.08 0.59 13.37 1.31\nl + os 6.08 0.56 5.19 0.68 16.56 3.73\nTable 8: Zero-shot (zs) and one-shot (os) results\non Question Answering (QA) datasets, including\nDuReader (Zhidao and Search) and CMRC2018, we\ndid experiments on models with three different sizes:\nsmall (s), medium (m) and large(l).\nThe answers in DuReader are manifold, such as an\nentity or a description. We treat DuReader as an\nextractive QA task and thus ignore those instances\nwith yes-or-no answers during evaluation.\nImplementation Details: We evaluate CPM on\nzero-shot (zs) and one-shot (os) setting and re-\nport F1 score (F1) and Exact Match (EM) for both\nCMRC2018 and DuReader. For the zero-shot set-\nting, we concatenate the passage and question as\ninput to CPM, and CPM is then required to gener-\nate an answer according to the observed (passage,\nquestion) pair. For the one-shot setting, we ran-\ndomly select a ground truth triple (passage, ques-\ntion, answer) in the training set and insert it to the\nfront of the instance to be treated as a hint for CPM\nto generate the answer.\nResults: As shown in Table 8, we perform the\nexperiments on three datasets and compare models\nwith different sizes: small (s), medium (m) and\nlarge (l). From the table, we can see that with the\nsize growing, CPM is performing better. Among\nall the models, large is always the best. And, the\nresults in the one-shot setting are better than those\nin the zero-shot setting. We guess CPM is able\nto imitate the format in previous sequences and\norganize the language accordingly. We also analyze\nthe generated answer and ﬁnd that CPM prefers to\ngenerate long and repetitive sentences instead of a\nshort and precise one, which results in low scores.\nWe believe it is worth exploring how to make CPM\ngenerate brief and proper answers in the future. In\ngeneral, CPM does not achieve very high scores\nin either benchmark. We guess it’s related to the\nformat of the pre-training data.\n3.5 Entity Generation\nDataset: We use XLORE, which includes 446,236\nrelations and 16,284,901 entities, as our benchmark\ndataset for entity generation. These relations and\nN = 2 N = 4\nCPM Small Medium LargeSmall Medium Large\n主要工艺(Main Process) 0.500 0.500 0.7000.400 0.200 0.400\n释义(Explanation) 0.000 0.000 0.0710.000 0.000 0.075\n商品品牌(Brand) 0.098 0.033 0.4830.183 0.050 0.450\n学科(Subject) 0.000 0.025 0.1240.059 0.053 0.108\n全名(Full Name) 0.035 0.010 0.1080.000 0.014 0.122\n涉及领域(Related Field) 0.042 0.065 0.1040.063 0.037 0.125\n主要作物(Main Crop) 0.000 0.150 0.0500.100 0.150 0.100\n所在国家(In Country) 0.033 0.033 0.033 0.050 0.000 0.050\n病原类型(Pathogen Type) 0.250 0.220 0.3700.200 0.300 0.340\n首任总统(The First President)0.000 0.000 0.000 0.016 0.009 0.014\nTable 9: BLEU-1 results of CPM with different amounts of parameters on XLORE dataset in the few-shot setting.\nRelation: 首都(Capital)\nPrompt: 美国 首都华盛顿\nAmerica Capital Washington\n中国 首都北京\nChina Capital Beijing\n日本 首都\nJapan Capital\nCPM: 东京\nTokyo\nRelation: 主要工艺(Main Process)\nPrompt: 酱焖辣椒主要工艺焖\n(Sauce Braised Chili) (Main Process) Stew\n当归鸭肉煲主要工艺煲\n(Duck with Angelica) (Main Process) Boil\n韭菜煎蛋饼主要工艺\n(Leek Omelette) (Main Process)\nCPM: 煎\nFried\nRelation: 学科 (Subject)\nPrompt: 恒星级黑洞学科宇宙论\n(Stellar Black Hole) Subject Cosmology\n品类需求强度学科品牌经济学\n(Category Demand Intensity) Subject Economics\n大地构造学 学科\n(Tectonic Geology) Subject\nCPM: 地质学\nGeology\nTable 10: Examples of generated entities on XLORE\nwith CPM-Large.\nentities are from Wikipedia and Baidu Baike.\nImplementation Details: We evaluate CPM on\nthe few-shot setting with different amounts of pa-\nrameters and report BLEU-1 results. In detail, we\nrandomly select triples (head entity, relation, tail\nentity) by the same relations from XLORE and\ncombine N triples and an incomplete triple (head\nentity, relation) into a prompt. Then, given the\nprompt, the models need to predict the correspond-\ning tail entity.\nResults: We present the results in Table 9. As\nwe can see from the table, CPM-large achieves\nthe best performance among these three models.\nSurprisingly, given a prompt with two triples, CPM\ncan achieve comparable results to that with four\ntriples. It indicates that CPM can imitate the format\nand probe factual knowledge to generate a proper\ntail entity in the extreme few-shot scenarios. We\nalso provide some cases in Table 10 to demonstrate\nthe ability of CPM.\n4 Future Work\nIn the future, we will further explore the power\nof large-scale pre-trained models on Chinese by\nadding more training data and increasing the model\nsize. Due to the extremely expensive cost of pre-\ntraining, we will try to optimize the training frame-\nwork, such as the data-transfer scheme between\ndifferent nodes, to accelerate the process. There\nare some previous works including LAMB (You\net al., 2020) and DeepSpeed (Rasley et al., 2020).\nBesides, it is important to reduce the model size by\nmodel compression (Sanh et al., 2019; Jiao et al.,\n2019; Zhang et al., 2020).\nMeanwhile, we will also include diverse data to\nenhance model performance. For text data, we will\nadd a multi-lingual corpus to train a large-scale\nChinese-centered multi-lingual language model.\nFor structured data such as knowledge graphs,\nwhich is important for PLMs (Peters et al., 2019;\nXiong et al., 2020; Su et al., 2020), we will ex-\nplore new learning algorithms to train a joint model,\nwhich can learn from both texts and knowledge\ngraphs for better general intelligence.\nAcknowledgments\nThanks to the Beijing Academy of Artiicial Intel-\nligence (BAAI) for providing the computing re-\nsources and web services of this work. In addition,\nwe would like to thank NetEase Inc., zhihu.com,\nand aminer.cn for the support in collecting the Chi-\nnese corpus.\nDisclaimer of Warranties\nThe text generated by CPM is automatically gener-\nated by a neural network model trained on a large\nnumber of texts, which does not represent our ofﬁ-\ncial attitudes and preferences. The text generated\nby CPM is only used for technical and scientiﬁc\npurposes. If it infringes on your rights and interests\nor violates social morality, please do not propagate\nit, but contact us and we will deal with it promptly.\nReferences\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. arXiv preprint arXiv:2005.14165.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shi-\njin Wang, and Guoping Hu. 2020. Revisiting pre-\ntrained models for chinese natural language process-\ning. In Findings of EMNLP.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin,\nZiqing Yang, Shijin Wang, and Guoping Hu. 2019a.\nPre-training with whole word masking for chinese\nbert. arXiv preprint arXiv:1906.08101.\nYiming Cui, Ting Liu, Wanxiang Che, Li Xiao,\nZhipeng Chen, Wentao Ma, Shijin Wang, and Guop-\ning Hu. 2019b. A span-extraction dataset for chi-\nnese machine reading comprehension. In Proceed-\nings of EMNLP.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of NAACL-HLT.\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\nFarhadi, Hannaneh Hajishirzi, and Noah Smith.\n2020. Fine-tuning pretrained language models:\nWeight initializations, data orders, and early stop-\nping. arXiv preprint arXiv:2002.06305.\nWei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao,\nXinyan Xiao, Yuan Liu, Yizhong Wang, Hua Wu,\nQiaoqiao She, Xuan Liu, Tian Wu, and Haifeng\nWang. 2018. DuReader: a chinese machine reading\ncomprehension dataset from real-world applications.\nIn Proceedings of ACL Workshop.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text\ndegeneration. In Proceedings of ICLR.\nHai Hu, Kyle Richardson, Liang Xu, Lu Li, San-\ndra Kuebler, and Lawrence S Moss. 2020. Ocnli:\nOriginal chinese natural language inference. arXiv\npreprint arXiv:2010.05444.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,\nXiao Chen, Linlin Li, Fang Wang, and Qun\nLiu. 2019. TinyBERT: Distilling bert for nat-\nural language understanding. arXiv preprint\narXiv:1909.10351.\nPei Ke, Jian Guan, Minlie Huang, and Xiaoyan Zhu.\n2018. Generating informative responses with con-\ntrolled sentence function. In Proceedings of ACL.\nPei Ke, Haozhe Ji, Siyang Liu, Xiaoyan Zhu, and Min-\nlie Huang. 2020. SentiLARE: Sentiment-aware lan-\nguage representation learning with linguistic knowl-\nedge. In Proceedings of EMNLP.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Interna-\ntional Conference on Learning Representations.\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of EMNLP.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite bert for self-supervised learn-\ning of language representations. In Proceedings of\nICLR.\nCheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang.\n2020. Mixout: Effective regularization to ﬁnetune\nlarge-scale pretrained language models. In Proceed-\nings of ICLR.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016. A diversity-promoting objec-\ntive function for neural conversation models. InPro-\nceedings of NAACL-HLT.\nChia-Wei Liu, Ryan Lowe, Iulian Vlad Serban, Mike\nNoseworthy, Laurent Charlin, and Joelle Pineau.\n2016. How not to evaluate your dialogue system:\nAn empirical study of unsupervised evaluation met-\nrics for dialogue response generation. In Proceed-\nings of EMNLP.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nMarius Mosbach, Maksym Andriushchenko, and Diet-\nrich Klakow. 2020. On the stability of ﬁne-tuning\nbert: Misconceptions, explanations, and strong base-\nlines. arXiv preprint arXiv:2006.04884.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of NAACL-HLT.\nMatthew E. Peters, Mark Neumann, Robert L. Logan\nIV , Roy Schwartz, Vidur Joshi, Sameer Singh, and\nNoah A. Smith. 2019. Knowledge enhanced con-\ntextual word representations. In Proceedings of\nEMNLP, pages 43–54.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. In Proceedings\nof OpenAI Technical report.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. In Pro-\nceedings of OpenAI Technical report.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100, 000+ questions\nfor machine comprehension of text. In Proceedings\nof EMNLP.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase,\nand Yuxiong He. 2020. Deepspeed: System opti-\nmizations enable training deep learning models with\nover 100 billion parameters. InProceedings of KDD,\npages 3505–3506.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. DistilBERT, a distilled version\nof BERT: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nLifeng Shang, Zhengdong Lu, and Hang Li. 2015. Neu-\nral responding machine for short-text conversation.\nIn Proceedings of ACL-IJCNLP.\nYusheng Su, Xu Han, Zhengyan Zhang, Peng Li,\nZhiyuan Liu, Yankai Lin, Jie Zhou, and Maosong\nSun. 2020. Contextual knowledge selection and\nembedding towards enhanced pre-trained language\nmodels. arXiv preprint arXiv:2009.13964.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi\nChen, Han Zhang, Xin Tian, Danxiang Zhu, Hao\nTian, and Hua Wu. 2019. Ernie: Enhanced rep-\nresentation through knowledge integration. arXiv\npreprint arXiv:1904.09223.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of NIPS.\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhiyuan\nLiu, Juanzi Li, and Jian Tang. 2019. Kepler: A\nuniﬁed model for knowledge embedding and pre-\ntrained language representation. arXiv preprint\narXiv:1911.06136.\nYida Wang, Pei Ke, Yinhe Zheng, Kaili Huang, Yong\nJiang, Xiaoyan Zhu, and Minlie Huang. 2020. A\nlarge-scale chinese short-text conversation dataset.\nIn Proceedings of NLPCC.\nJunqiu Wei, Xiaozhe Ren, Xiaoguang Li, Weny-\nong Huang, Yi Liao, Yasheng Wang, Jiashu\nLin, Xin Jiang, Xiao Chen, and Qun Liu. 2019.\nNezha: Neural contextualized representation for\nchinese language understanding. arXiv preprint\narXiv:1909.00204.\nChen Xing, Wei Wu, Yu Wu, Jie Liu, Yalou Huang,\nMing Zhou, and Wei-Ying Ma. 2017. Topic aware\nneural response generation. In Proceedings of AAAI.\nWenhan Xiong, Jingfei Du, William Yang Wang, and\nVeselin Stoyanov. 2020. Pretrained encyclopedia:\nWeakly supervised knowledge-pretrained language\nmodel. In Proceedings of ICLR.\nLiang Xu, Xuanwei Zhang, Lu Li, Hai Hu, Chen-\njie Cao, Weitang Liu, Junyi Li, Yudong Li, Kai\nSun, Yechen Xu, et al. 2020. Clue: A chinese lan-\nguage understanding evaluation benchmark. arXiv\npreprint arXiv:2004.05986.\nYang You, Jing Li, Sashank Reddi, Jonathan Hseu,\nSanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song,\nJames Demmel, Kurt Keutzer, and Cho-Jui Hsieh.\n2020. Large batch optimization for deep learning:\nTraining bert in 76 minutes. In Proceedings of ICLR.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: En-\nhanced language representation with informative en-\ntities. In Proceedings of ACL.\nZhengyan Zhang, Fanchao Qi, Zhiyuan Liu, Qun Liu,\nand Maosong Sun. 2020. Know what you don’t\nneed: Single-shot meta-pruning for attention heads.\narXiv preprint arXiv:2011.03770.\nChujie Zheng, Minlie Huang, and Aixin Sun. 2019.\nChID: A large-scale Chinese IDiom dataset for cloze\ntest. In Proceedings of ACL.\nA Contributions\nZhengyan Zhang, Xu Han, and Hao Zhou\nimplemented the large-scale models and model-\nparallel strategies.\nHuanqi Cao, Shengqi Chen, Daixuan Li, and\nZhenbo Sun built the training infrastructure.\nPei Ke, Deming Ye, Jian Guan, Fanchao Qi, and\nXiaozhi Wang collected, ﬁltered, deduplicated the\ntraining data.\nZhengyan Zhang, Pei Ke, Yuxian Gu, Deming\nYe, Yujia Qin, Yusheng Su, and Haozhe Ji im-\nplemented the downstream tasks and the software\nframework for supporting them.\nHao Zhou, Guoyang Zeng, Xu Han, and Yanan\nZheng implemented the demos of language gener-\nation and knowledge retrieval using our CPM.\nGuoyang Zeng conducted the human evaluations\nof the model.\nHao Zhou, Zhengyan Zhang, Pei Ke, Yuxian\nGu, Deming Ye, Yujia Qin, and Yusheng Su\nwrote the paper.\nZhiyuan Liu, Minlie Huang, and Wentao Han\ndesigned and led the research.\nJie Tang, Juanzi Li, Xiaoyan Zhu, Maosong\nSun provided valuable advices to the research.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7705070972442627
    },
    {
      "name": "Generative grammar",
      "score": 0.6473232507705688
    },
    {
      "name": "Natural language processing",
      "score": 0.6370737552642822
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6125152111053467
    },
    {
      "name": "Language model",
      "score": 0.5360854864120483
    },
    {
      "name": "Code (set theory)",
      "score": 0.4842347800731659
    },
    {
      "name": "Scale (ratio)",
      "score": 0.45796823501586914
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    }
  ]
}