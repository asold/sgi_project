{
  "title": "Efficient Training and Inference: Techniques for Large Language Models Using Llama",
  "url": "https://openalex.org/W4398774455",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5089131863",
      "name": "Sophia R. Cunningham",
      "affiliations": [
        "Defence Electronics Application Laboratory",
        "Laboratoire des Sciences de l’Information et de la Communication"
      ]
    },
    {
      "id": "https://openalex.org/A2076489048",
      "name": "Dominique Archambault",
      "affiliations": [
        "Defence Electronics Application Laboratory",
        "Laboratoire des Sciences de l’Information et de la Communication"
      ]
    },
    {
      "id": "https://openalex.org/A5113227532",
      "name": "Austin Kung",
      "affiliations": [
        "Laboratoire des Sciences de l’Information et de la Communication",
        "Defence Electronics Application Laboratory"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4378505278",
    "https://openalex.org/W4318541692",
    "https://openalex.org/W4379928343",
    "https://openalex.org/W4391681217",
    "https://openalex.org/W4378770452",
    "https://openalex.org/W3193344850",
    "https://openalex.org/W4324297016",
    "https://openalex.org/W4221145545",
    "https://openalex.org/W4391900182",
    "https://openalex.org/W3048889251",
    "https://openalex.org/W4388092394",
    "https://openalex.org/W4388581500",
    "https://openalex.org/W4396861092",
    "https://openalex.org/W4362508448",
    "https://openalex.org/W4389156555",
    "https://openalex.org/W4380874786",
    "https://openalex.org/W4390824394",
    "https://openalex.org/W4395704222",
    "https://openalex.org/W4389116129",
    "https://openalex.org/W4283313765",
    "https://openalex.org/W3131247433",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W3137439872",
    "https://openalex.org/W4303443398",
    "https://openalex.org/W4312056202",
    "https://openalex.org/W4392223539",
    "https://openalex.org/W4393186514",
    "https://openalex.org/W4323570490",
    "https://openalex.org/W4399915806",
    "https://openalex.org/W4390833061",
    "https://openalex.org/W4393027724",
    "https://openalex.org/W4378942311",
    "https://openalex.org/W4377371819",
    "https://openalex.org/W4390529182",
    "https://openalex.org/W4404788800",
    "https://openalex.org/W4391877130",
    "https://openalex.org/W4390827995",
    "https://openalex.org/W4402595168",
    "https://openalex.org/W4392781192",
    "https://openalex.org/W4393578753",
    "https://openalex.org/W4389761026",
    "https://openalex.org/W4390309921",
    "https://openalex.org/W4390493562",
    "https://openalex.org/W4393147284",
    "https://openalex.org/W4387321091",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W4395112660",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4389166691",
    "https://openalex.org/W4391901128",
    "https://openalex.org/W4387994442",
    "https://openalex.org/W4303447594",
    "https://openalex.org/W3138246627",
    "https://openalex.org/W3171155106",
    "https://openalex.org/W4383472770",
    "https://openalex.org/W4385210979",
    "https://openalex.org/W4388040387",
    "https://openalex.org/W4392796680",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W4384615697",
    "https://openalex.org/W4366341968",
    "https://openalex.org/W4361021241",
    "https://openalex.org/W4386556040",
    "https://openalex.org/W4392090145"
  ],
  "abstract": "To enhance the efficiency of language models, it would involve optimizing their training and inference processes to reduce computational demands while maintaining high performance. The research focuses on the application of model compression, quantization, and hardware acceleration techniques to the Llama model. Pruning and knowledge distillation methods effectively reduce the model size, resulting in faster training times and lower resource consumption. Quantization techniques, including 8-bit and 4-bit representations, significantly decrease memory usage and improve computational speed without substantial accuracy loss. The integration of GPUs and TPUs further accelerates the training and inference processes, demonstrating the crucial role of hardware in optimizing large-scale models. The study highlights the practical implications of those techniques, paving the way for more sustainable and scalable AI solutions.",
  "full_text": null,
  "topic": "Inference",
  "concepts": [
    {
      "name": "Inference",
      "score": 0.704106867313385
    },
    {
      "name": "Training (meteorology)",
      "score": 0.6555255651473999
    },
    {
      "name": "Computer science",
      "score": 0.6081448197364807
    },
    {
      "name": "Natural language processing",
      "score": 0.49446725845336914
    },
    {
      "name": "Artificial intelligence",
      "score": 0.40334880352020264
    },
    {
      "name": "Geography",
      "score": 0.09088686108589172
    },
    {
      "name": "Meteorology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4400600903",
      "name": "Laboratoire des Sciences de l’Information et de la Communication",
      "country": null
    },
    {
      "id": "https://openalex.org/I4210144350",
      "name": "Defence Electronics Application Laboratory",
      "country": "IN"
    }
  ]
}