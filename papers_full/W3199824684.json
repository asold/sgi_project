{
  "title": "CPT: a pre-trained unbalanced transformer for both Chinese language understanding and generation",
  "url": "https://openalex.org/W3199824684",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2807003669",
      "name": "Yunfan Shao",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2114013274",
      "name": "Zhichao Geng",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2103387990",
      "name": "Yitao Liu",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2413380657",
      "name": "Junqi Dai",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2116385994",
      "name": "Hang Yan",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2021846487",
      "name": "Fei Yang",
      "affiliations": [
        "Zhejiang Lab"
      ]
    },
    {
      "id": "https://openalex.org/A2104531756",
      "name": "Zhe Li",
      "affiliations": [
        "Zhejiang Lab"
      ]
    },
    {
      "id": "https://openalex.org/A2099886988",
      "name": "Hujun Bao",
      "affiliations": [
        "Zhejiang Lab"
      ]
    },
    {
      "id": "https://openalex.org/A2115470192",
      "name": "Xipeng Qiu",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2807003669",
      "name": "Yunfan Shao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2114013274",
      "name": "Zhichao Geng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103387990",
      "name": "Yitao Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2413380657",
      "name": "Junqi Dai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2116385994",
      "name": "Hang Yan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2021846487",
      "name": "Fei Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104531756",
      "name": "Zhe Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099886988",
      "name": "Hujun Bao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2115470192",
      "name": "Xipeng Qiu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3011574394",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W6802852670",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W2971871542",
    "https://openalex.org/W3187018546",
    "https://openalex.org/W3176617251",
    "https://openalex.org/W3158631574",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W3007759824",
    "https://openalex.org/W3137214022",
    "https://openalex.org/W3106339673",
    "https://openalex.org/W3098065087",
    "https://openalex.org/W2997200074",
    "https://openalex.org/W3174396451",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W3170083118",
    "https://openalex.org/W3173210704",
    "https://openalex.org/W3100128199",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W3124687886",
    "https://openalex.org/W6800875267",
    "https://openalex.org/W3125507956",
    "https://openalex.org/W3174851730",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W3114651185",
    "https://openalex.org/W3174461835",
    "https://openalex.org/W3034379414",
    "https://openalex.org/W3101805561",
    "https://openalex.org/W2897356710",
    "https://openalex.org/W2806081754",
    "https://openalex.org/W3021636956",
    "https://openalex.org/W2962996600",
    "https://openalex.org/W2970574558",
    "https://openalex.org/W3001434439",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W3210120707",
    "https://openalex.org/W3106031450",
    "https://openalex.org/W3177401493",
    "https://openalex.org/W3081031588",
    "https://openalex.org/W3107826490",
    "https://openalex.org/W3088409176",
    "https://openalex.org/W2252066972",
    "https://openalex.org/W3177365697",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3135335819",
    "https://openalex.org/W25062297",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W3113747735",
    "https://openalex.org/W4293454876",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W3182414949",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W3015253856",
    "https://openalex.org/W3016512233",
    "https://openalex.org/W3102725307"
  ],
  "abstract": null,
  "full_text": "SCIENCE CHINA\nInformation Sciences\n. RESEARCH PAPER .\nCPT: A Pre-Trained Unbalanced Transformer for\nBoth Chinese Language Understanding and\nGeneration\nYunfan Shao1,3, Zhichao Geng1,3, Yitao Liu1,3, Junqi Dai1,3, Hang Yan1,3,\nFei Yang2, Li Zhe2, Hujun Bao2 & Xipeng Qiu1,3*\n1School of Computer Science, Fudan University, Shanghai200433, China;\n2Zhejiang Lab, Hangzhou311121, China;\n3Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, Shanghai200433, China\nAbstract In this paper, we take the advantage of previous pre-trained models (PTMs) and propose a\nnovel Chinese Pre-trained Unbalanced Transformer (CPT). Diﬀerent from previous Chinese PTMs, CPT\nis designed to utilize the shared knowledge between natural language understanding (NLU) and natural\nlanguage generation (NLG) to boost the performance. CPT consists of three parts: a shared encoder, an\nunderstanding decoder, and a generation decoder. Two speciﬁc decoders with a shared encoder are pre-\ntrained with masked language modeling (MLM) and denoising auto-encoding (DAE) tasks, respectively.\nWith the partially shared architecture and multi-task pre-training, CPT can (1) learn speciﬁc knowledge of\nboth NLU or NLG tasks with two decoders and (2) be ﬁne-tuned ﬂexibly that fully exploits the potential of\nthe model. Moreover, the unbalanced Transformer saves the computational and storage cost, which makes\nCPT competitive and greatly accelerates the inference of text generation. Experimental results on a wide\nrange of Chinese NLU and NLG tasks show the eﬀectiveness of CPT ∗.\nKeywords pre-trained model, transformer, language model, generation, uniﬁed model\nCitation Shao Y F, Geng Z C, Liu Y T, et al. CPT: A Pre-Trained Unbalanced Transformer for Both Chinese\nLanguage Understanding and Generation. Sci China Inf Sci, for review\n1 Introduction\nRecently, large-scale pre-trained models (PTMs) have become backbone models for many natural language\nprocessing (NLP) tasks [1]. However, existing PTMs are usually trained with diﬀerent architectures and\npre-training tasks. When applying PTMs to a downstream task, we should choose a suitable one as the\nbackbone model according to its pre-training nature. For example, we usually select BERT or RoBERTa\n[2,3] as the backbone model for natural language understanding (NLU) tasks, and BART or GPT [4,5]\nfor natural language generation (NLG) tasks. With the success of PTMs in English, many works have\nbeen done to train the counterparts for Chinese [6–11]. However, these Chinese PTMs usually follow the\nsettings of English PTMs, which makes these models focus on either language understanding or language\ngeneration, lacking the use of sharing knowledge between NLU and NLG tasks. Therefore, it is attractive\nto pre-train a joint model for both NLU and NLG tasks.\nFew works attempt to fuse NLU and NLG into a uniﬁed model. UniLMs [12,13] and GLM [14] adapt a\nuniﬁed Transformer encoder for both understanding and generation; however, their architectures restrict\nthem to employ more ﬂexible pre-training tasks, such as denoising auto-encoding (DAE) used in BART,\na widely successful pre-training task for NLG. PALM [15] adopts the standard Transformer and adds an\nauxiliary masked language modeling (MLM) task to enhance the understanding ability; however, it still\nfocuses on language generation tasks.\n* Corresponding author (email: xpqiu@fudan.edu.cn)\n∗Code is available at https://github.com/fastnlp/CPT\narXiv:2109.05729v4  [cs.CL]  18 Jul 2022\nShao Y F, et al. Sci China Inf Sci 2\n[CLS] AB [M] D[M]CE\n[CLS] AB [M] D[M]A B CDE[SEP][CLS] AB C DE\n(a) BERT (b) BART\n[CLS] AB C DE\nA B C DE[SEP]\n[CLS] AB [M] D[M]CE\nABC DE[SEP][CLS] AB C DES-EncU-DecG-Dec\n(c) GPT (d) CPT\nFigure 1 Architecture of CPT and the counterpart PTMs. Diﬀerent from other PTMs, CPT consists of three parts: a shared\nencoder (S-Enc), an understanding decoder ( U-Dec) and a generation decoder ( G-Dec).\nIn this paper, we propose CPT, a novel Chinese Pre-trained Unbalanced Transformer for both NLU\nand NLG tasks. The architecture of CPT is very concise (as shown in Figure 1), which divides a full\nTransformer encoder-decoder into three parts: 1) a shared encoder to capture the common representation;\n2) a decoder for understanding, which uses full self-attention and is pre-trained with masked language\nmodeling (MLM); 3) a decoder for generation, which adopts masked self-attention and is pre-trained with\nthe DAE task. By multi-task pre-training, CPT is able to improve the performance on both language\nunderstanding and generation, respectively.\nTable 1 Summary of some representative Chinese PTMs. “# Params” refers to the number of parameters. “Arch.” refers to the\nmodel architecture. “LM” refers to language modeling in auto-regression fashion, while “Seq2Seq MLM” refers to masked language\nmodeling in Seq2Seq fashion. “Tok.”, “Masking” and “Prediction” refer to the tokenization, masking and prediction granularity of\nthe model, respectively. “ \u0013” means “could be directly used to”. And “ \u0017” means “need to be adapted to”.\nBERT\nRoBERTa\nZEN\nNEZHA\nERNIE-1.0/2.0\nPanGu-α CPM CPM-2 BART CPT\n# Params\nBase - 110M 32Layers - 2.6B Small - 110M Base - 11B Base - 139M Base - 121M\nLarge - 340M ≈BERT 40Layers - 13.1B Medium - 340M MOE - 198B Large - 406M Large - 393M\n64Layers - 207.0B Large - 2.6B\nArch. Transformer Transformer Transformer Transformer Full Full Unbalanced Full\nEncoder Encoder Variant Decoder Decoder Transformer Transformer Transformer\nPreTrain.\nTask\nMLM MLM LM LM Seq2Seq MLM DAE MLM+DAE\nTok. Char Char Word/Char Word/Char Word/Char Char Char\nMasking Word - - - - Word Word\nPrediction Char Char Word/Char Word/Char Word/Char Char Char\nNLU \u0013 \u0013 \u0017 \u0017 \u0017 \u0017 \u0013\nNLG \u0017 \u0017 \u0013 \u0013 \u0013 \u0013 \u0013\nThe main properties of CPT are as follows:\n(1) CPT can be regarded as two separated PTMs with a shared encoder. Two speciﬁc decoders are\npre-trained with MLM and DAE tasks, respectively. Each decoder can learn the speciﬁc knowledge on\neither NLU or NLG tasks, while the shared encoder learns the common knowledge for universal language\nrepresentation.\n(2) Two separated decoders enable CPT to adapt to various downstream tasks ﬂexibly. For example,\nCPT could be ﬁne-tuned with at least ﬁve modes for classiﬁcation tasks (as shown in Figure 2), which\nexploits the full potential of CPT. Thus, we could choose a suitable ﬁne-tuning mode based on the\nattributes and characteristics of downstream tasks.\nShao Y F, et al. Sci China Inf Sci 3\n(3) The overall architecture of CPT is an unbalance Transformer. To make the computational cost and\nthe size of CPT comparable with popular PTMs, such as BERT and BART, we use a novel architecture\nconsisting of a deeper shared encoder and two shallower decoders. Especially, the shallow generation\ndecoder greatly accelerates the inference of text generation.\nWe conduct experiments on various language understanding and text generation tasks, including\ndatasets for text classiﬁcation, sequence labeling, machine reading comprehension, summarization, data-\nto-text generation, etc. Results show that CPT could achieve competitive results with state-of-the-art\non these datasets.\n2 Related Work\n2.1 PTMs towards both NLU and NLG\nRecently, there are some eﬀorts to combine language understanding and generation into a single pre-\ntrained model. UniLM [12] pre-trained with an ensemble of attention masks, which allows the model to\nbe used for both generative and classiﬁcation tasks. A diﬀerence is that all parameters of UniLM are\nshared between generation and discrimination, whereas CPT uses two separated decoders. Thus, CPT\ncan utilize the DAE pre-training task which is proven to be eﬀective for NLG tasks [4].\nPALM [15] is a pre-trained model focusing on conditional generation. To force the encoder to compre-\nhend the meaning of the given context, MLM is added to pre-train the encoder. In contrast, CPT has an\nindividual decoder for MLM which can avoid the negative eﬀects brought by DAE. Therefore CPT also\nhas good performance on NLU tasks.\nMore recently, ERNIE 3.0 [16] also uses a universal encoder and several task-speciﬁc decoders, but it\nadopts Transformer-XL as the backbone and its generative pre-training task is left-to-right LM with a\nspecial masked attention matrix. Diﬀerent from ERNIE 3.0, CPT adopts the encoder-decoder architecture\nand is more suitable for sequence-to-sequence (Seq2Seq) tasks.\n2.2 Chinese PTMs\nMany attempts have been conducted to pre-train the Chinese counterparts of PTMs.\nThe ﬁrst line of works follows BERT and uses MLM with whole word masking strategy to pre-train\nTransformer encoder, such as Chinese versions of BERT and RoBERTa [6], NEZHA [8], ZEN [17]. Some\nof them add special features of Chinese characters or words to further boost the performance of NLU\ntasks, such as ERNIE 1.0/2.0 [7,18], ChineseBERT [19]. However, these PTMs could not be adopted to\ntext generation directly.\nThe second line of works follows GPT and uses the left-to-right LM task to pre-train a Transformer\ndecoder, such as CPM [10] and PanGu [11]. Although large-scale PTMs with tens of billions parameters\nhave been released recently, the huge computation and storage cost hinders their applications.\nThe third line of works aims to pre-train the full Transformer encoder-decoder. CPM-2 [10] follows\nT5 [20] and adopts a Seq2Seq MLM pre-training task, which predicts the masked tokens in a Seq2Seq\nfashion. Although BART [4] has achieved widely success on conditional text generation tasks, such as\ntext summarization [21,22] and dialogue system [23], it still lacks corresponding Chinese versions 1).\nDiﬀerent from the above Chinese PTMs, CPT is a pre-trained unbalanced Transformer with MLM and\nDAE tasks, which is capable of achieving competitive results on both NLU and NLG tasks. Besides, CPT\nis parameter eﬃcient compared to these large-scale models. Table 1 compares diﬀerent Chinese PTMs.\n2.3 Multi-Task Pre-Training\nIncorporating multi-task learning into pre-training has drawn increasingly attention recently. Most recent\nadvancements attempt to improve performance by leveraging multi-task learning beyond standard pre-\ntraining [20,24–26]. This line of works focuses on downstream task performance improvements by utilizing\na collection of labeled datasets. However, our work is focusing on close the gap between language\nunderstanding and text generation tasks by applying multi-task learning on large scale unlabeled texts.\n1) Besides CPT, we also provide a Chinese BART as a byproduct.\nShao Y F, et al. Sci China Inf Sci 4\n3 Model Architecture\nAs shown in Figure 1, The architecture of CPT is a variant of the full Transformer and consists of three\nparts:\n(1) Shared Encoder (S-Enc): a Transformer encoder with fully-connected self-attention, which is\ndesigned to capture the common semantic representation for both language understanding and generation.\n(2) Understanding Decoder (U-Dec): a shallow Transformer encoder with fully-connected self-\nattention, which is designed for NLU tasks. The input of U-Dec is the output of S-Enc.\n(3) Generation Decoder (G-Dec): a Transformer decoder with masked self-attention, which is de-\nsigned for generation tasks with auto-regressive fashion. G-Dec utilizes the output of S-Enc with cross-\nattention.\nWith the two speciﬁc decoders, CPT can be used ﬂexibly. For example, CPT can be easily ﬁne-tuned\nfor NLU tasks using just S-Enc and U-Dec, and can be regarded as the standard Transformer encoder;\nwhile for NLG tasks, CPT adopts S-Enc and G-dec, and forms a Transformer encoder-decoder. With\ndiﬀerent combinations, CPT is able to be eﬀectively applied on various downstream tasks, which fully\nexploits the pre-trained parameters and obtains competitive performance. More combinations and use\ncases will be discussed in Fine-Tuning Section.\nDiﬀerent from most PTMs with encoder-decoders, we exploit a deep-shallow framework for shared\nencoder and decoders. More speciﬁcally, we use a deeper encoder and two shallow decoders for CPT. We\nassume that a shallow decoder retains the performance on text generation and reduces decoding time,\nwhich has proven to be eﬀective for neural machine translation [27] and spell checking [28].\nThe deep-shallow setup makes CPT more general for both understanding and generative tasks with\nminor parameter overheads. It also accelerates the inference of CPT for text generation as the G-Dec is\na light decoder.\n4 Pre-Training\nTo make CPT good at both NLU and NLG tasks, we introduce two pre-training tasks.\n(1) Masked Language Modeling (MLM): We pre-train the parameters of S-Enc and U-Dec with\nMLM [2,6]. Given a sentence, we randomly replace some tokens with the [MASK] token and train S-Enc\nand U-Dec to predict the masked tokens. Following [6], we adopt Whole Word Masking (WWM) to\nreplace the tokens. Compared to randomly token masking, WWM is more suitable for inducing semantic\ninformation carried by words and spans.\n(2) Denoising Auto-Encoding (DAE): We pre-train the parameters of S-Enc and G-Dec by recon-\nstructing the original document based on the corrupted input. According to the studies of BART [4],\nwe corrupted the input by two eﬀective ways. 1) Token Inﬁlling : a Whole Word Masking (WWM)\nstrategy with single mask replacement. First, a number of words are sampled based on the segmentation.\nThen, each selected word is replaced with a single [MASK] token, regardless of how many tokens it con-\nsists; and 2) Sentence Permutation: sentences are extracted from a document based on punctuation,\nand shuﬄed in a random order.\nIn practice, We ﬁrst use a Chinese Word Segmentation (CWS) tool to split the sentences into words.\nThen, we select 15% of the words and mask the corresponding characters. For the masked characters,\nwe follow the setup of BERT to (1) replace 80% of them with a special [MASK] token, (2) replace 10% of\nthem by random tokens, (3) keep the rest 10% of them unchanged.\nFinally, we train CPT with two pre-training tasks under a multi-task learning framework. Thus, CPT\ncan learn for both understanding and generation, and can easily deal with downstream NLU or NLG\ntasks.\n5 Fine-Tuning\nPTMs are usually ﬁne-tuned in only few ways for a given downstream task. For example, for sentence-level\nclassiﬁcation, we ﬁne-tune BERT by taking the top-layer output of [CLS] token as the representation of\nthe whole sentence, while ﬁne-tune GPT by using the representation of the last token of the sequence.\nShao Y F, et al. Sci China Inf Sci 5\nLabelS-Enc[C] T1 T2 T3 T4 [S]U-Dec\nLabelS-EncG-Dec[C] T1 T2 T3 T4 [S][C] T1 T2 T3 T4 [S]\nLabelS-Enc[C] T1 T2 T3 T4 [S]U-DecG-Dec[C] T1 T2 T3 T4 [S]\n(a) CPTu (b) CPTg (c) CPTug\nLabelS-Enc[C] T1 T2 T3 P1 P2U-Dec\n[M][S]\nS-EncG-Dec[C] T1 T2 T3 T4 [S]Label\n[C] P1 P2\n(d) CPTu+p (e) CPTg+p\nFigure 2 Five ways to ﬁne-tune CPT for text classiﬁcation. “T1-4” and “P1-2” refer to text input x and prompt tokens,\nrespectively. V(y) is the mapping function that maps the language model predictions to the label. [C] and [S] are abbreviations\nfor [CLS] and [SEP], respectively.\nThanks to the separated understanding and generation decoders, CPT can be ﬁne-tuned in multiple\npatterns. For a given downstream task, one could choose the most suitable way to fully stimulate the\npotential of CPT to achieve competitive results.\n5.1 Fine-Tuning for Sentence-Level Classiﬁcation\nWhen incorporating external classiﬁers, CPT have three ﬁne-tuning modes for sequence-level classiﬁcation\n(As shown in Figure 2 (a),(b) and (c)).\n(1) CPT u: a BERT-style mode. The sentence representation is from U-Dec module only, which is\nusually the ﬁrst state of [CLS] token.\n(2) CPT g: a BART-style mode. The same input is fed into the S-Enc and G-Dec, and the represen-\ntation from the ﬁnal output token [SEP] from G-Dec is used.\n(3) CPT ug: The same input is fed into the S-Enc and G-Dec, and the ﬁnal representation is the\nconcatenation of the ﬁrst output of U-Dec and the ﬁnal output of G-Dec.\nRecently, a powerful and attractive framework, prompt-based learning [29–31], is also able to boost the\nperformance of PTMs. By deﬁning prompting templates and reformulating the classiﬁcation tasks into\na generative fashion, the framework utilizes PTMs to generate words corresponding to task labels. The\ngenerative patterns are so close to the pre-training tasks of PTMs that they have the ability of few-shot\nor even zero-shot learning.\nThe prompt-based methods could also be applied on CPT with more ﬂexibly fashions since CPT has\ntwo decoders. As shown in Figure 2 (d) and (e), we construct prompts and convert the task into an\ngeneration task with CPT by the following two modes:\n(1) CPT u+p: A MLM task. We manually construct an input template and assign a word to each\ntask label. CPT is ﬁne-tuned to predict the word at the masked positions, which will be mapped to the\ntask labels. Since a word may be tokenized into multiple character tokens, the predicted distributions at\nmasked positions are averaged to get the predicted distribution of labels.\n(2) CPT g+p: Conditional text generation. We encode the input text with S-Enc and train CPT to\ngenerate prompt text initialized with corresponding labels by teacher forcing. For inference, we ﬁrst\nconstruct the prompt text for each label. Then, the perplexity of each prompt text is calculated. Finally,\nthe prediction is assign to the label with the highest corresponding perplexity.\n5.2 Fine-Tuning for Sequence Labeling\nFor sequence labeling, each token needs a representation for token-level classiﬁcation. Similar to sequence-\nlevel classiﬁcation, we leverage PTMs to obtain high quality token representations and then put the\nrepresentations to a trainable classiﬁer to assign labels for these tokens. Thus, similar to sentence-level\nShao Y F, et al. Sci China Inf Sci 6\nclassiﬁcation, we can ﬁne-tune CPT for sequence labeling as CPT u, CPTg and CPTug, using (1) U-Dec\nonly, (2) G-Dec only, or (3) both U-Dec and G-Dec. Figure 3 shows two examples for sequence labeling.\nL1L2L3 L4S-Enc[C] T1 T2 T3 T4 [S]U-Dec\nG-DecDG RepresentationDU RepresentationL1L2L3 L4S-Enc[C] T1 T2 T3 T4 [S]U-Dec[C] T1 T2 T3 T4 [S]\n(a) CPTu (b) CPTug\nFigure 3 Two examples of ﬁne-tuning CPT for sequence labeling. “T1-4” and “L1-4” refer to text input x and token labels,\nrespectively.\n5.3 Fine-Tuning for Machine Reading Comprehension\nMachine Reading Comprehension requires the model to predict an answer span shown in the passage for\na given question. A typical ﬁne-tuning pattern is to train PTMs to predict the start and end positions of\nthe span in the passage. The prediction is based on the tokens of the passage. Thus, CPT u, CPTg and\nCPTug can be ﬁne-tuned, similar to sequence-labeling. Figure 4 shows the example of CPT u.\nS-EncU-DecStart EndP1 P2 P3[S] Q1 Q2[C]\nT1 T2 T3 T4 T5 [S]S-EncG-Dec[C] S1 S2 S3 S4 [S][C] T1 T2 T3 T4 [S]\nFigure 4 Two examples of ﬁne-tuning CPT for Machine\nReading Comprehension. “P1-3”, “Q1-2” refer to passages and\nquestions, respectively.\nFigure 5 Example of ﬁne-tuning CPTg for Conditional Gen-\neration. “S1-4” and “T1-4” refer to input and target sequences,\nrespectively.\n5.4 Fine-Tuning for Conditional Generation\nApart from NLU tasks, CPT can do text generation eﬃciently. As shown in Figure 5, we simply ﬁne-\ntune CPT g with S-Enc and G-Dec modules on text generation tasks, similar to the usage of other\nauto-regressive PTMs [4].\n6 Experiments\n6.1 Pre-Training Setups\nWe implement two versions of CPT, namely,base and large, respectively consisting of 14/28 Transformer\nlayers with 10/20 layers for shared encoder and 2/4 layers for each task speciﬁc decoder. And the hidden\nunits and attention heads per layer for base and large versions are 768/1,024 and 12/16, respectively.\nThe total number of layers activated for a given task is always equal to 12/24, which makes our model\ncomparable with base/large-size of BERT and its variants (RoBERTa, ERNIE 1.0/2.0, etc).\nWe train our models on the open source large-scale raw text, Chinese Wikipedia and a part of Wu-\nDaoCorpus. The training data contains 200GB cleaned text ranges from diﬀerent domains. We use Jieba\nto segment Chinese words for Whole Word Masking and use WordPiece tokenizer inherited from BERT\nto split input text into tokens. We use Adam to train the models for 500k steps, with the batch size of\n2048, the learning rate of 1e-4, β1 = 0.9, β2 = 0.98, weight decay of 0.01. We warmup the learning rate\nfor ﬁrst 10,000 steps then do linear decay. In addition, a Chinese BART is pre-trained with the same\ncorpora, tokenization and hyper-parameters as a baseline.\nShao Y F, et al. Sci China Inf Sci 7\n6.2 Evaluation Tasks\nTo evaluate the eﬀectiveness of our model, we conduct experiments on various NLP datasets across\ndiﬀerent understanding and generation tasks, with details illustrated below.\nClassiﬁcation We evaluate the model on the Chinese Language Understanding Evaluation Benchmark\n(CLUE) [32], which contains text classiﬁcation TNEWS, IFLYTEK, natural language inference (NLI),\nOCNLI, sentence pair matching (SPM) AFQMC, and coreference resolution (CoRE) CLUEWSC 2020\n(WSC.) key word recognition (KwRE) CSL. We conduct data augmentation CSL as [33] performed,\nand evaluate TNEWS on version 1.1 test set. Accuracy is used for these datasets.\nSequence Labeling We evaluate our model on Chinese word segmentation (CWS) and named entity\nrecognition (NER), which are two representative sequence labeling tasks. We use two datasets from\nSIGHAN2005 [34] for CWS, which are MSR, PKU. And for NER, MSRA [35], OntoNotes2) are\nused. We use the same dataset preprocessing and split methods as in previous work [36–38]. And F1\nscores are reported.\nMRC Span based machine reading comprehension (MRC) dataset CMRC 2018 ( CMRC) [39] and\nTraditional Chinese MRC dataset DRCD [40] are used. We follow the data processing in [6, 41] and\ntransform the text from DRCD is transformed to Simpliﬁed Chinese. The Exact Match (EM) scores are\nreported.\nText Generation We use two abstractive summarization datasets, LCSTS [42] and CSL3), and a\ndata-to-text generation dataset, ADGEN [43] to evaluate the text generation ability of our model.\nAmong them, LCSTS is a large corpus of Chinese short text summarization dataset constructed from\nSina Weibo, consisting of 2 million real Chinese short texts with short summaries. And CSL is an\nacademic domain text summarization dataset, constructed from abstract and titles from publications\nin computer science domain. And ADGEN is a data-to-text dataset that requires models to generate\nlong text for advertisement based on some keywords. And we evaluate PTMs on test sets of LCSTS\nand ADGEN and the development set of CSL. The character-level Rouge-L is used to evaluate the\nsummarization results. For ADGEN, we follow [10] to use BLEU-4.\n6.3 Compared PTMs\nWe compare CPT with a series of state-of-the-art PTMs for either natural language understanding or\ntext generation. The details are as follows.\nPTMs for NLU PTMs with the Transformer Encoder structure and pre-trained with MLM usually\nperform well in NLU tasks, such as the Chinese versions of BERT and RoBERTa [6], NEZHA [8], ERNIE\n2.0 [18], MacBERT [41]. Unless otherwise speciﬁed, we use BERT and RoBERTa to refer toBERT-wwm-ext\nand RoBERTa-wwm-ext, respectively.\nPTMs for NLG For text generation, we compare CPT with generative Transformers ranging from\nnormal size to large scale, including BART [4], mBART [44], mT5 [45], CPM-2 [10], and models with pre-\ntrained encoders. BART is a sequence-to-sequence model pre-trained with DAE task. Due to the missing\nof Chinese version, we train a Chinese BART as mentioned in Section 6.1. mBART is a multilingual\nvariant of BART. And mT5 is a multilingual variant of T5 pre-trained on over 101 languages, including\nChinese. CPM-2 is a large-scale encoder-decoder model with 11 billion parameters, pre-trained in multiple\nstages with large-scale Chinese and bilingual data. We also report generative models adopted from\nTransformer encoders such as RoBERTa and ERNIE 2.0 that follow the generation style of UniLM [12],\nto further evaluate the eﬀectiveness generative pre-training.\n2) https://catalog.ldc.upenn.edu/LDC2011T03\n3) https://github.com/CLUEbenchmark/CLGE\nShao Y F, et al. Sci China Inf Sci 8\n6.4 Main Results\nTo fully release the potential of our model, we ﬁne-tune CPT for NLU tasks in diﬀerent ways as mentioned\nin Fine-Tuning Section, denoted as CPT u, CPTg and CPTug, CPTu+p and CPTg+p, respectively. We\nuse (B) and (L) to distinguish base and large version of PTMs, respectively.\nClassiﬁcation Table 2 shows the development set results of CLUE Benchmark of diﬀerent ﬁne-tuning\nmodes. As a result, CPT u (B) achieves a 74.6 on average, surpassing other baselines and ﬁne-tuning\npatterns on base version of CPT. Besides, CPT ug (L) obtains a averaged accuracy 76.2, which is better\nthan RoBERTa (L) by a large margin. Therefore, we choose CPT u (B) and CPT ug (L) as the most\nsuitable ﬁne-tuning patterns to do the classiﬁcation. We ﬁnd that the best ﬁne-tuning modes are diﬀerent\nbetween base and large models. We believe the diﬀerence is brought by the scale of the parameters. For\nbase model, the G-Dec is too shallow to transfer for NLU tasks, which makes CPT ug could not beat the\nCPTu. And the G-Dec in large version of CPT has more parameters and layers, which makes the decoder\neasy to transfer.\nTable 2 Accuracy results on dev set of CLUE Benchmark. We ﬁne-tune CPT with ﬁve diﬀerent ways as shown in Figure 2. (B)\nand (L) refer to base-size and large-size of PTMs, respectively.\nModels TNEWS IFLYTEK OCNLI AFQMC CSL WSC AVG\nBERT (B) 56.8 58.9 75.4 72.0 82.3 83.2 71.4\nRoBERTa (B) 57.5 59.4 76.5 74.4 86.1 88.8 73.8\nBART (B) 57.2 60.0 76.1 73.0 85.8 79.6 71.9\nCPTu (B) 58.4 60.5 76.4 75.1 86.1 91.1 74.6\nCPTg (B) 57.3 60.4 76.3 71.4 86.4 87.2 73.2\nCPTug (B) 57.4 61.9 76.8 70.6 86.3 89.8 73.8\nCPTg+p (B) 54.9 25.4 76.6 73.7 86.9 79.9 66.2\nCPTu+p (B) 58.4 61.6 76.6 75.1 86.9 79.9 73.1\nRoBERTa (L) 58.3 61.7 78.5 75.4 86.3 89.5 75.0\nBART (L) 59.2 62.1 79.7 75.7 87.3 90.1 75.7\nCPTu (L) 58.8 61.8 79.5 75.9 86.5 92.1 75.8\nCPTg (L) 59.1 61.7 79.9 75.8 86.9 91.8 75.9\nCPTug (L) 59.2 62.4 79.8 75.8 86.6 93.4 76.2\nCPTg+p (L) 54.5 29.2 79.8 75.4 87.1 89.5 69.2\nCPTu+p (L) 59.0 61.2 79.6 75.4 87.3 87.8 75.1\nTable 3 Results on CLUE benchmarks. For all tasks we report accuracy on test sets.\nModels TNEWS IFLYTEK OCNLI AFQMC CSL WSC AVG\nBERT (B) 58.6 59.4 73.2 74.1 84.2 74.5 70.7\nRoBERTa (B) 59.5 60.3 73.9 74.0 84.7 76.9 71.5\nBART (B) 58.5 60.7 72.1 74.0 85.4 67.6 69.7\nCPTu (B) 59.2 60.5 73.4 74.4 85.5 81.4 72.4\nRoBERTa (L) 58.9 63.0 76.4 76.6 82.1 74.6 71.9\nBART (L) 58.6 62.7 78.1 74.3 86.7 82.1 73.7\nCPTug (L) 59.2 62.4 78.4 75.0 85.5 86.2 74.5\nFor prompt-based ﬁne-tuning (Table 2), we ﬁnd that directly ﬁne-tuning without prompt works well on\nsome datasets, with the small gaps between CPTu, CPTg and CPTug. Moreover, CPTu+p achieves good\nresults on some datasets that even outperform methods without prompt tuning. However, the accuracy\nof prompt-base methods on other datasets drops a lot. As there are many factors that aﬀect prompt\ntuning performance including prompt design, choices of words for labels, etc. Manually designed prompts\nmay be suboptimal. Besides, we ﬁnd that CPT g+p degenerates obviously on TNEWS and IFLYTEK.\nBoth datasets have more than 3 classes, which contains 15 and 112 labels, respectively. Moreover, these\nShao Y F, et al. Sci China Inf Sci 9\nlabels are hard to represented by a single character. In practice we assign words with up to 7 characters\nto a label. We presume that the large number of labels and the multi-token issue hinders CPT g+p to\ngenerate correctly.\nTable 3 reports the performance of CPT on classiﬁcation tasks and the comparison with previous\nrepresentative Chinese PTMs. We report accuracy on the test sets of these datasets. Among the ﬁne-\ntuned CPTs, we choose base version CPT u and large version CPT ug as they obtain the best results on\ndevelopment sets. Base size CPT consistently outperforms BERT, RoBERTa and ERNIE. Moreover,\nlarge size CPT achieves a 74.5 averaged score, outperforming RoBERTa (L) with a large margin. We ﬁnd\nthat generative PTMs, such as BART, also have the ability to handle discrimination tasks (see Table 2\nTable 3). However, their performance is suboptimal compared with the CPT. As the uni-directional\nlayers of generative models could hurt the performance of NLU tasks.\nSequence Labeling The CPT is ﬁne-tuned as CPTu, CPTg and CPTug and evaluated on development\nsets. We ﬁnd that CPT u constantly obtains the best development results. We conjecture that CWS and\nNER have more dependency on local syntax than complex semantics used for text generation. Thus,\nCPTu is more suitable for CWS and NER with its bidirectional fully connected self-attention. As a\nresult, we report the test set results of CPT u to compare with other PTMs.\nTable 4 Results on sequence labeling datasets. The F1 scores on test sets are reported. Models with ∗ indicate the results are\nfrom [18].\nCWS NER\nMSR PKU MSRA OntoNotes\nBERT (B) 98.24 96.50 95.13 81.73\nERNIE 2.0∗(B) - - 93.80 -\nRoBERTa (B) 98.14 96.15 95.23 81.52\nCPTu (B) 98.29 96.58 95.78 82.08\nERNIE 2.0∗(L) - - 95.00 -\nRoBERTa (L) 98.42 96.37 95.20 81.78\nCPTu (L) 98.51 96.70 96.20 83.08\nWe compare our model with other state-of-the-art methods on sequence labeling datasets. As shown\nin Table 4, CPT u (L) achieves the highest performance and exceed the BERT (L), RoBERTa (L) and\nERNIE (L) on all sequence labeling tasks, both CWS and NER. And CPT u (B) obtains a comparable\nresults, surpassing base versions of BERT and RoBERTa.\nNote that CPTugoutperforms the CPTu in the large size while surpassed by CPTu in the base version.\nWe believe that it is the large discrepancy between pre-training and ﬁne-tuning tasks, which makes the\nG-Dec trained by the DAE task hard to be transferred to classiﬁcation. G-Dec is harder to be ﬁne-tuned\nthan understanding decoder (U-Dec), especially in the base model where G-Dec is very shallow. And it\nalso explains that the performance gap between CPT u and CPTg in the base version is larger than the\nlarge size.\nTable 5 Results on MRC datasets. Exact Match (EM) scores are reported. Models with ∗ indicate the results from the corre-\nsponding work.\nCMRC 2018 DRCD\nDev Dev Test\nRoBERTa (B) 67.9 85.9 85.2\nMacBERT∗(B) 68.2 89.2 88.7\nERNIE 2.0∗(B) 69.1 88.5 88.0\nNEZHA∗(B) 67.8 - -\nCPTu (B) 68.8 89.0 89.0\nRoBERTa (L) 70.6 89.1 88.9\nMacBERT∗(L) 70.1 90.8 90.9\nERNIE 2.0∗(L) 71.5 89.7 89.0\nNEZHA∗(L) 68.1 - -\nCPTu (L) 72.3 91.0 91.1\nShao Y F, et al. Sci China Inf Sci 10\nMRC Table 5 shows the experimental results on MRC tasks, which also indicates the eﬀectiveness of\nCPT. We report the Exact Match (EM) score on CMRC dev set, DRCD dev and test sets. We try\nand evaluate CPTu, CPTu and CPTu on the development sets of these datasets and choose the pattern\nthat acquires the best results to report. As a conclusion, CPT u obtains comparable or higher results\ncompared to previous systems that are widely used, such as RoBERTa, MacBERT, ERNIE and NEZHA.\nMoreover, CPTu consistently outperforms other strong baselines by a large margin, with 72.3 EM score\non the CMRC development set and 91.1 EM on the DRCD test set.\nTable 6 Results on text generation datasets. The small(base) version of mT5 has almost the same parameters as the base(large)\nversion of other PTMs. CPM-2 has a much larger number of parameters than other large size PTMs. Models with ∗and †indicate\nthe results are from [16] and [10], respectively.\nModels LCSTS CSL ADGEN\n(Rouge-L) (Rouge-L) (BLEU-4)\nmT5 (S) 33.5 56.7 10.2\nBART (B) 37.8 62.1 9.9\nCPTg (B) 38.2 63.0 9.8\nCPM-2† 35.9 - 10.6\nmBART (L) 37.8 55.2 8.5\nmT5 (B) 36.5 61.8 -\nERNIE 2.0∗(L) 41.4 - -\nRoBERTa∗(L) 41.0 - -\nBART (L) 40.6 64.2 10.0\nCPTg (L) 42.0 63.7 10.7\nText Generation Table 6 compares the performance of our model on generation datasets with other\nstrong methods. The character-level Rouge-L is used to evaluate the summarization results. For ADGEN,\nwe follow [10] to use BLEU-4.\nADGEN CSLLCSTS\n220\n243\n268\n312\n366\n395\n# Tokens per Second\nBART CPT\nADGEN CSLLCSTS\n157 161162\n256 254\n278BART CPT\nFigure 6 Inference throughput for BART and CPT. It is measured on the same parts of datasets that the models are evaluated.\nThe beam size is 4 and the batch size is 8.\nAs a conclusion, CPT g achieves competitive performance on text generation compared with other\nmethods, such as mT5, CPM-2, BART. In addition, compared with other pre-trained encoders (RoBERTa\nand ERNIE 2.0), CPT g improves the generation score with the NLG enhanced pre-training. When\ncompared with pre-trained mT5 and CPM-2, CPTg acquires better results on both base and large versions.\nWe assume the diﬀerence of pre-training tasks that lead to the performance gaps. Both mT5 and CPM-2\nexploit a T5 style masked span generation as their pre-training task, while CPT is pre-trained with DAE,\nwhich shows the eﬀectiveness of DAE for text generation pre-training.\nIn addition, the shallow decoder of CPTg may aﬀect the performance on long text generation. However,\nthe performance gaps are still small. And we believe the multi-task pre-training of CPT closes the gaps.\nTable 7 and Table 8 illustrates some examples generated by BART (L) and CPT g (L). With the help of\npre-training for understanding, CPT g is able to summarize text with more information captured in the\ninput content.\nShao Y F, et al. Sci China Inf Sci 11\nTable 7 Summary examples generated by BART (L) and CPT (L) given input text on LCSTS.\nInput 今日，刘胜义在2013腾讯智慧峰会上指出，在移动化时代，数字媒体、消费行为、数字营销都需要重新定义。并且移动化媒体应具备三个特征：从实时媒体发展成全天候媒体；从大\n众媒体发展到智能媒体阶段；从资讯媒体发展到生活类型的媒体。\nToday, in the 2013 Tencent Wisdom Summit, Shengyi Liu pointed out that in the mobile era, digital media, consumer behavior, and\ndigital marketing all need to be redefined. And mobile media should have three characteristics: real-time media develope to 24-hour\nmedia; mass media develope to smart media; information and news media develope to life media.\nReference 腾讯刘胜义：移动化引发媒体及营销体系变革\nShengyi Liu from Tencent: Mobile process leads to the changes in media and marketing systems.\nBART (L) 刘胜义：移动化时代数字媒体需重新定义\nShengyi Liu: Digital media need to be redefined in the mobile era.\nCPTg (L) 腾讯总裁刘胜义：移动化时代数字媒体需重新定义\nTencent President Shengyi Liu: Digital media need to be redefined in the mobile era.\nInput 近年来，逢雨必涝、逢涝必瘫，几成我国城市通病。上周，中国青年报对全国31个省（区、市）5375人进行的调查显示，91.6%的人关注所在城市的排水问题；84.7%的受访者\n赞同，城市现代化更表现在地面之下，应加大地下民生工程建设投入。\nIn recent years, flooding and paralysis in floods have become a common problem in Chinese cities. Last week, the China Youth Daily\nconducted a survey of 5375 people in 31 provinces (regions and cities) across the country. It shows that 91.6% of people are concerned\nabout the drainage problems in their cities; 84.7% of the interviewees agree that urban modernization is shown under the ground, and\nthe government should increase investment in the construction of underground livelihood projects.\nReference 84.7%受访者期待国家加大地下民生工程投入\n84.7% of respondents expect the country to increase investment in underground livelihood projects.\nBART (L) 84.7%受访者赞同加大地下民生工程建设投入\n84.7% of respondents agree to increase investment in the construction of underground livelihood projects.\nCPTg (L) 超八成受访者赞同加大地下民生工程投入\nOver 80% of respondents agree to increase investment in the underground livelihood projects.\nTable 8 Text examples generated by BART (L) and CPT (L) given keywords on ADGEN.\nInput [类型, 上衣], [版型, 宽松], [颜色, 蓝色], [风格, 简约], [风格, 清新], [衣样式, 衬衫], [衣领型, 翻领], [衣长, 中长款], [衣门襟, 单排扣]\n[Type, Top], [Fit, Loose], [Color, Blue], [Style, Simple], [Style, Fresh], [Clothing Style, Shirt], [Collar Type, Lapel], [Cloth Length ,\nMid-length], [Clothes placket, Single-Breasted]\nReference 很适合学生穿的一件衬 衬 衬衫 衫 衫，蓝 蓝 蓝色 色 色调是属于比较小清 清 清新 新 新的色调，而且还能衬托出女性的完美好气色哦，让你穿上之后瞬间有个好心情；简 简 简约 约 约的小翻 翻 翻领 领 领设计加以精致的单 单 单排 排 排扣 扣 扣点缀，颇有\n一番正式的感觉，结合上宽 宽 宽松 松 松以及中 中 中长 长 长款 款 款的设计，从而打造出女性的率性bf风范。\nA shirt that is very suitable for students. Theblue tone is a relativelyfresh hue, which can bring out the perfect look of women,\nmaking you have a good mood instantly when wearing it; thesimple small design oflapel is combined withsingle-breasted embel-\nlishment, which shows quite a formal feel. And when combined withloose and mid-length design, it creates a feminine and casual\nbf style.\nBART (L) 这款衬 衬 衬衫 衫 衫采用清 清 清新 新 新的蓝 蓝 蓝色 色 色调，展现出女性内心的纯洁与美好。简 简 简约 约 约的小翻 翻 翻领 领 领设计，修饰出精致小巧的脸型。精致的单 单 单排 排 排扣 扣 扣门襟，方便穿脱。宽 宽 宽松 松 松的中 中 中长 长 长款 款 款版型，对身材的包容性很\n好。\nThis shirt uses a fresh blue tone to show the purity and beauty of women’s hearts. Thesimple design of smalllapel makes the\nface delicate and small. The exquisitesingle-breasted placket makes it easy to put on and take off. Theloose mid-lengthclothing\ndesign is easily fit the body.\nCPTg (L) 这款衬 衬 衬衫 衫 衫采用中 中 中长 长 长款 款 款的版型设计，搭配宽 宽 宽松 松 松的直筒版型，给身体更多的活动量，行动更为方便，穿着更为舒适。加上清 清 清新 新 新的蓝 蓝 蓝色 色 色调，更为衬托肤色白皙。结合简 简 简约 约 约的单 单 单排 排 排扣 扣 扣门襟，方\n便穿脱的同时，也丰富了整体的层次感。而精致的小翻 翻 翻领 领 领，则为整体增添了几分干练的气质。\nThis shirt adopts amid-length clothing design with aloose straight type, giving the body more activity, making it more convenient\nto move and to wear. Coupled with afresh bluetone, it whitens the skin. Combined with asimple single-breastedplacket, it is\nconvenient to put on and take off, which also enriches the overall sense of layers. The exquisite and smalllapel adds a capable and\nexperienced style to the whole.\nInput [类型, 上衣], [版型, 显瘦], [颜色, 黑色], [衣样式, 毛衣], [衣领型, v领]\n[Type, Top], [Fit, Skinny], [Color, Black], [Clothing Style, Sweater], [Collar Type, V-Neck]\nReference 金丝的加入为低调内敛的黑 黑 黑色 色 色添加闪闪亮色，胸前的v领 领 领加排扣设计，摈弃毛 毛 毛衣 衣 衣臃肿松散的固有印象，轻薄修 修 修身 身 身，保留温度的同时又不至于在猝不及防的倒春寒中失去风度。\nThe golden silk adds sparkling color to the low-key and restrainedblack. The v-neck on the chest and the breasted design abandons\nthe inherent impression of bloated and loosesweaters, which is light andslim. It keeps the temperature without losing its grace in\nthe unexpected cold spring.\nBART (L) 这款黑 黑 黑色 色 色的修 修 修身 身 身长款毛 毛 毛衣 衣 衣，最大的设计亮点在于衣身v领 领 领设计，这样的款式设计使得整件毛衣看起来与众不同，既个性又修身舒适哦。\nThe biggest highlighted design of thisblack slim long sweater is the v-neck design of the clothing. This design makes the whole\nsweater unique, personal and comfortable.\nCPTg (L) 这款毛 毛 毛衣 衣 衣采用了经典的黑 黑 黑色 色 色作为底色设计，搭配上修 修 修身 身 身的版型剪裁，穿起来更加的显 显 显瘦 瘦 瘦。领口处采用v领 领 领的剪裁方式，可以起到修饰脸型的作用，更显脸小精致。衣摆处的开叉处理，\n更是增添了几分随性的感觉。\nThis sweater uses a classicblack background with aslim fit cut, which makes you look thin. The neckline adopts av-neck tailoring\nmethod, which can frame the face and make the face small and delicate. The split treatment at the hem adds a casual feel.\nMoreover, because of the shallow decoder, CPT could generate texts more eﬃciently (Figure 6), which\ncould be faster than other depth symmetric encoder-decoder Transformers with the same number of layers\nof the encoder and the decoder. As BART and CPT have similar number of parameters in both base\nand large versions. On all generation dataset, the decoding speed of CPT surpass BART with a large\nmargin. Our model achieves 1 .4× ∼1.5× speedup compared with BART and still maintain comparable\ngeneration results in base size. And CPT (L) has up to 1 .7× relative speedup compared to BART (L).\nAs a conclusion, the shallow G-Dec is able to speed up the generation with minor performance loss.\n7 Conclusion\nIn this paper, we propose CPT, a novel Chinese PTM for both language understanding and generation.\nWith the ﬂexible design, CPT can be assembled and disassembled in various fashions, which could fully\nexploit the potential of CPT. Experimental results on a wide range of Chinese NLU and NLG tasks show\nthe eﬀectiveness of CPT.\nIn future work, we will introduce more speciﬁc designs according to Chinese properties, such as better\nShao Y F, et al. Sci China Inf Sci 12\ntokenization, pre-training tasks and model architectures.\nAcknowledgements This work was supported by the National Key Research and Development Program of China (No. 2020AAA0108702),\nNational Natural Science Foundation of China (No. 62022027) and Major Scientiﬁc Research Project of Zhejiang Lab (No.\n2019KD0AD01).\nReferences\n1 X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang, “Pre-trained models for natural language processing: A survey,”\nSCIENCE CHINA Technological Sciences, vol. 63, no. 10, p. 1872–1897, 2020.\n2 J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: pre-training of deep bidirectional transformers for language under-\nstanding,” in NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), J. Burstein,\nC. Doran, and T. Solorio, Eds., 2019, pp. 4171–4186. [Online]. Available: https://doi.org/10.18653/v1/n19-1423\n3 Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, “Roberta: A\nrobustly optimized BERT pretraining approach,” CoRR, vol. abs/1907.11692, 2019. [Online]. Available: http://arxiv.org/\nabs/1907.11692\n4 M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, “BART: denoising\nsequence-to-sequence pre-training for natural language generation, translation, and comprehension,” in ACL 2020, Online,\nJuly 5-10, 2020, D. Jurafsky, J. Chai, N. Schluter, and J. R. Tetreault, Eds., 2020, pp. 7871–7880. [Online]. Available:\nhttps://doi.org/10.18653/v1/2020.acl-main.703\n5 A. Radford, “Improving language understanding by generative pre-training,” 2018.\n6 Y. Cui, W. Che, T. Liu, B. Qin, Z. Yang, S. Wang, and G. Hu, “Pre-training with whole word masking for chinese BERT,”\nCoRR, vol. abs/1906.08101, 2019. [Online]. Available: http://arxiv.org/abs/1906.08101\n7 Y. Sun, S. Wang, Y. Li, S. Feng, X. Chen, H. Zhang, X. Tian, D. Zhu, H. Tian, and H. Wu, “ERNIE: enhanced representation\nthrough knowledge integration,” CoRR, vol. abs/1904.09223, 2019. [Online]. Available: http://arxiv.org/abs/1904.09223\n8 J. Wei, X. Ren, X. Li, W. Huang, Y. Liao, Y. Wang, J. Lin, X. Jiang, X. Chen, and Q. Liu, “NEZHA: neural contextualized\nrepresentation for chinese language understanding,” CoRR, vol. abs/1909.00204, 2019. [Online]. Available: http://arxiv.org/\nabs/1909.00204\n9 Z. Zhang, X. Han, H. Zhou, P. Ke, Y. Gu, D. Ye, Y. Qin, Y. Su, H. Ji, J. Guan, F. Qi, X. Wang, Y. Zheng, G. Zeng, H. Cao,\nS. Chen, D. Li, Z. Sun, Z. Liu, M. Huang, W. Han, J. Tang, J. Li, X. Zhu, and M. Sun, “CPM: A large-scale generative chinese\npre-trained language model,” CoRR, vol. abs/2012.00413, 2020. [Online]. Available: https://arxiv.org/abs/2012.00413\n10 Z. Zhang, Y. Gu, X. Han, S. Chen, C. Xiao, Z. Sun, Y. Yao, F. Qi, J. Guan, P. Ke, Y. Cai, G. Zeng, Z. Tan, Z. Liu,\nM. Huang, W. Han, Y. Liu, X. Zhu, and M. Sun, “CPM-2: large-scale cost-eﬀective pre-trained language models,” CoRR,\nvol. abs/2106.10715, 2021. [Online]. Available: https://arxiv.org/abs/2106.10715\n11 W. Zeng, X. Ren, T. Su, H. Wang, Y. Liao, Z. Wang, X. Jiang, Z. Yang, K. Wang, X. Zhang, C. Li, Z. Gong, Y. Yao,\nX. Huang, J. Wang, J. Yu, Q. Guo, Y. Yu, Y. Zhang, J. Wang, H. Tao, D. Yan, Z. Yi, F. Peng, F. Jiang, H. Zhang, L. Deng,\nY. Zhang, Z. Lin, C. Zhang, S. Zhang, M. Guo, S. Gu, G. Fan, Y. Wang, X. Jin, Q. Liu, and Y. Tian, “Pangu- α: Large-\nscale autoregressive pretrained chinese language models with auto-parallel computation,” CoRR, vol. abs/2104.12369, 2021.\n[Online]. Available: https://arxiv.org/abs/2104.12369\n12 L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao, M. Zhou, and H. Hon, “Uniﬁed language model pre-training for\nnatural language understanding and generation,” in NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, H. M.\nWallach, H. Larochelle, A. Beygelzimer, F. d’Alch´ e-Buc, E. B. Fox, and R. Garnett, Eds., 2019, pp. 13 042–13 054. [Online].\nAvailable: https://proceedings.neurips.cc/paper/2019/hash/c20bb2d9a50d5ac1f713f8b34d9aac5a-Abstract.html\n13 H. Bao, L. Dong, F. Wei, W. Wang, N. Yang, X. Liu, Y. Wang, J. Gao, S. Piao, M. Zhou, and H. Hon, “Unilmv2: Pseudo-\nmasked language models for uniﬁed language model pre-training,” in ICML 2020, 13-18 July 2020, Virtual Event, ser.\nProceedings of Machine Learning Research, vol. 119. PMLR, 2020, pp. 642–652. [Online]. Available: http://proceedings.\nmlr.press/v119/bao20a.html\n14 Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang, “All NLP tasks are generation tasks: A general pretraining\nframework,” CoRR, vol. abs/2103.10360, 2021. [Online]. Available: https://arxiv.org/abs/2103.10360\n15 B. Bi, C. Li, C. Wu, M. Yan, W. Wang, S. Huang, F. Huang, and L. Si, “PALM: pre-training an autoencoding&autoregressive\nlanguage model for context-conditioned generation,” in EMNLP 2020, Online, November 16-20, 2020, B. Webber, T. Cohn,\nY. He, and Y. Liu, Eds., 2020, pp. 8681–8691. [Online]. Available: https://doi.org/10.18653/v1/2020.emnlp-main.700\n16 Y. Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen, Y. Zhao, Y. Lu, W. Liu, Z. Wu, W. Gong,\nJ. Liang, Z. Shang, P. Sun, W. Liu, X. Ouyang, D. Yu, H. Tian, H. Wu, and H. Wang, “ERNIE 3.0: Large-scale knowledge\nenhanced pre-training for language understanding and generation,” CoRR, vol. abs/2107.02137, 2021. [Online]. Available:\nhttps://arxiv.org/abs/2107.02137\n17 S. Diao, J. Bai, Y. Song, T. Zhang, and Y. Wang, “ZEN: pre-training chinese text encoder enhanced by n-gram represen-\ntations,” in EMNLP 2020, Online Event, 16-20 November 2020, T. Cohn, Y. He, and Y. Liu, Eds., 2020, pp. 4729–4740.\n[Online]. Available: https://doi.org/10.18653/v1/2020.ﬁndings-emnlp.425\n18 Y. Sun, S. Wang, Y. Li, S. Feng, H. Tian, H. Wu, and H. Wang, “ERNIE 2.0: A continual pre-training framework for language\nunderstanding,” in AAAI 2020, New York, NY, USA, February 7-12, 2020. AAAI Press, 2020, pp. 8968–8975. [Online].\nAvailable: https://aaai.org/ojs/index.php/AAAI/article/view/6428\n19 Z. Sun, X. Li, X. Sun, Y. Meng, X. Ao, Q. He, F. Wu, and J. Li, “Chinesebert: Chinese pretraining enhanced by glyph and\npinyin information,” CoRR, vol. abs/2106.16038, 2021. [Online]. Available: https://arxiv.org/abs/2106.16038\n20 C. Raﬀel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, “Exploring the limits of\ntransfer learning with a uniﬁed text-to-text transformer,” J. Mach. Learn. Res., vol. 21, pp. 140:1–140:67, 2020. [Online].\nAvailable: http://jmlr.org/papers/v21/20-074.html\n21 Z. Dou, P. Liu, H. Hayashi, Z. Jiang, and G. Neubig, “Gsum: A general framework for guided neural abstractive sum-\nmarization,” in NAACL-HLT 2021, Online, June 6-11, 2021, K. Toutanova, A. Rumshisky, L. Zettlemoyer, D. Hakkani-\nT¨ ur, I. Beltagy, S. Bethard, R. Cotterell, T. Chakraborty, and Y. Zhou, Eds., 2021, pp. 4830–4842. [Online]. Available:\nhttps://doi.org/10.18653/v1/2021.naacl-main.384\n22 Y. Liu and P. Liu, “Simcls: A simple framework for contrastive learning of abstractive summarization,” in ACL/IJCNLP\n2021, (Volume 2: Short Papers), Virtual Event, August 1-6, 2021, C. Zong, F. Xia, W. Li, and R. Navigli, Eds., 2021, pp.\n1065–1072. [Online]. Available: https://doi.org/10.18653/v1/2021.acl-short.135\nShao Y F, et al. Sci China Inf Sci 13\n23 Z. Lin, A. Madotto, G. I. Winata, and P. Fung, “Mintl: Minimalist transfer learning for task-oriented dialogue systems,” in\nEMNLP 2020, Online, November 16-20, 2020, B. Webber, T. Cohn, Y. He, and Y. Liu, Eds., 2020, pp. 3391–3405. [Online].\nAvailable: https://doi.org/10.18653/v1/2020.emnlp-main.273\n24 X. Liu, P. He, W. Chen, and J. Gao, “Multi-task deep neural networks for natural language understanding,” in Proceedings\nof the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2,\n2019, Volume 1: Long Papers, A. Korhonen, D. R. Traum, and L. M` arquez, Eds. Association for Computational Linguistics,\n2019, pp. 4487–4496. [Online]. Available: https://doi.org/10.18653/v1/p19-1441\n25 A. Aghajanyan, A. Gupta, A. Shrivastava, X. Chen, L. Zettlemoyer, and S. Gupta, “Muppet: Massive multi-task representa-\ntions with pre-ﬁnetuning,” CoRR, vol. abs/2101.11038, 2021. [Online]. Available: https://arxiv.org/abs/2101.11038\n26 J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le, “Finetuned language models\nare zero-shot learners,” CoRR, vol. abs/2109.01652, 2021. [Online]. Available: https://arxiv.org/abs/2109.01652\n27 J. Kasai, N. Pappas, H. Peng, J. Cross, and N. A. Smith, “Deep encoder, shallow decoder: Reevaluating non-autoregressive\nmachine translation,” in ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. [Online]. Available:\nhttps://openreview.net/forum?id=KpfasTaLUpq\n28 X. Sun, T. Ge, F. Wei, and H. Wang, “Instantaneous grammatical error correction with shallow aggressive decoding,” in\nACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, C. Zong, F. Xia, W. Li, and R. Navigli,\nEds., 2021, pp. 5937–5947. [Online]. Available: https://doi.org/10.18653/v1/2021.acl-long.462\n29 T. Schick and H. Sch¨ utze, “It’s not just size that matters: Small language models are also few-shot learners,” in NAACL-\nHLT 2021, Online, June 6-11, 2021, K. Toutanova, A. Rumshisky, L. Zettlemoyer, D. Hakkani-T¨ ur, I. Beltagy, S. Bethard,\nR. Cotterell, T. Chakraborty, and Y. Zhou, Eds., 2021, pp. 2339–2352. [Online]. Available: https://doi.org/10.18653/v1/\n2021.naacl-main.185\n30 T. Gao, A. Fisch, and D. Chen, “Making pre-trained language models better few-shot learners,” in ACL/IJCNLP 2021,\n(Volume 1: Long Papers), Virtual Event, August 1-6, 2021, C. Zong, F. Xia, W. Li, and R. Navigli, Eds., 2021, pp.\n3816–3830. [Online]. Available: https://doi.org/10.18653/v1/2021.acl-long.295\n31 P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, “Pre-train, prompt, and predict: A systematic survey of\nprompting methods in natural language processing,” CoRR, vol. abs/2107.13586, 2021. [Online]. Available: https://arxiv.\norg/abs/2107.13586\n32 L. Xu, H. Hu, X. Zhang, L. Li, C. Cao, Y. Li, Y. Xu, K. Sun, D. Yu, C. Yu, Y. Tian, Q. Dong, W. Liu, B. Shi, Y. Cui, J. Li,\nJ. Zeng, R. Wang, W. Xie, Y. Li, Y. Patterson, Z. Tian, Y. Zhang, H. Zhou, S. Liu, Z. Zhao, Q. Zhao, C. Yue, X. Zhang, Z. Yang,\nK. Richardson, and Z. Lan, “CLUE: A chinese language understanding evaluation benchmark,” in COLING 2020, Barcelona,\nSpain (Online), December 8-13, 2020, D. Scott, N. Bel, and C. Zong, Eds. International Committee on Computational\nLinguistics, 2020, pp. 4762–4772. [Online]. Available: https://doi.org/10.18653/v1/2020.coling-main.419\n33 X. Zhang and H. Li, “AMBERT: A pre-trained language model with multi-grained tokenization,” CoRR, vol. abs/2008.11869,\n2020. [Online]. Available: https://arxiv.org/abs/2008.11869\n34 T. Emerson, “The second international chinese word segmentation bakeoﬀ,” in SIGHAN@IJCNLP 2005, Jeju Island, Korea,\n14-15, 2005. ACL, 2005. [Online]. Available: https://www.aclweb.org/anthology/I05-3017/\n35 G. Levow, “The third international chinese language processing bakeoﬀ: Word segmentation and named entity recognition,”\nin SIGHAN@COLING/ACL 2006, Sydney, Australia, July 22-23, 2006, H. T. Ng and O. O. Y. Kwong, Eds., 2006, pp.\n108–117. [Online]. Available: https://www.aclweb.org/anthology/W06-0115/\n36 X. Li, Y. Shao, T. Sun, H. Yan, X. Qiu, and X. Huang, “Accelerating BERT inference for sequence labeling via early-exit,” in\nACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, C. Zong, F. Xia, W. Li, and R. Navigli,\nEds., 2021, pp. 189–199. [Online]. Available: https://doi.org/10.18653/v1/2021.acl-long.16\n37 X. Li, H. Yan, X. Qiu, and X. Huang, “FLAT: chinese NER using ﬂat-lattice transformer,” in ACL 2020, Online, July\n5-10, 2020, D. Jurafsky, J. Chai, N. Schluter, and J. R. Tetreault, Eds., 2020, pp. 6836–6842. [Online]. Available: https:\n//doi.org/10.18653/v1/2020.acl-main.611\n38 X. Qiu, H. Pei, H. Yan, and X. Huang, “A concise model for multi-criteria chinese word segmentation with transformer\nencoder,” in Findings of EMNLP, T. Cohn, Y. He, and Y. Liu, Eds., vol. EMNLP 2020, 2020, pp. 2887–2897.\n39 Y. Cui, T. Liu, W. Che, L. Xiao, Z. Chen, W. Ma, S. Wang, and G. Hu, “A span-extraction dataset for chinese machine\nreading comprehension,” in EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, K. Inui, J. Jiang, V. Ng, and\nX. Wan, Eds., 2019, pp. 5882–5888. [Online]. Available: https://doi.org/10.18653/v1/D19-1600\n40 C. Shao, T. Liu, Y. Lai, Y. Tseng, and S. Tsai, “DRCD: a chinese machine reading comprehension dataset,” CoRR, vol.\nabs/1806.00920, 2018. [Online]. Available: http://arxiv.org/abs/1806.00920\n41 Y. Cui, W. Che, T. Liu, B. Qin, S. Wang, and G. Hu, “Revisiting pre-trained models for chinese natural language processing,”\nin EMNLP 2020, Online Event, 16-20 November 2020, T. Cohn, Y. He, and Y. Liu, Eds., 2020, pp. 657–668. [Online].\nAvailable: https://doi.org/10.18653/v1/2020.ﬁndings-emnlp.58\n42 B. Hu, Q. Chen, and F. Zhu, “LCSTS: A large scale chinese short text summarization dataset,” in EMNLP 2015, Lisbon,\nPortugal, September 17-21, 2015, L. M` arquez, C. Callison-Burch, J. Su, D. Pighin, and Y. Marton, Eds. The Association\nfor Computational Linguistics, 2015, pp. 1967–1972. [Online]. Available: https://doi.org/10.18653/v1/d15-1229\n43 Z. Shao, M. Huang, J. Wen, W. Xu, and X. Zhu, “Long and diverse text generation with planning-based hierarchical variational\nmodel,” in EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, K. Inui, J. Jiang, V. Ng, and X. Wan, Eds.,\n2019, pp. 3255–3266. [Online]. Available: https://doi.org/10.18653/v1/D19-1321\n44 Y. Liu, J. Gu, N. Goyal, X. Li, S. Edunov, M. Ghazvininejad, M. Lewis, and L. Zettlemoyer, “Multilingual denoising pre-\ntraining for neural machine translation,” Trans. Assoc. Comput. Linguistics, vol. 8, pp. 726–742, 2020. [Online]. Available:\nhttps://transacl.org/ojs/index.php/tacl/article/view/2107\n45 L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and C. Raﬀel, “mt5: A massively multilin-\ngual pre-trained text-to-text transformer,” in NAACL-HLT 2021, Online, June 6-11, 2021, K. Toutanova, A. Rumshisky,\nL. Zettlemoyer, D. Hakkani-T¨ ur, I. Beltagy, S. Bethard, R. Cotterell, T. Chakraborty, and Y. Zhou, Eds., 2021, pp. 483–498.\n[Online]. Available: https://doi.org/10.18653/v1/2021.naacl-main.41",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7956427335739136
    },
    {
      "name": "Transformer",
      "score": 0.7903077006340027
    },
    {
      "name": "Natural language understanding",
      "score": 0.6354696750640869
    },
    {
      "name": "Inference",
      "score": 0.614989161491394
    },
    {
      "name": "Encoder",
      "score": 0.5649539828300476
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47851288318634033
    },
    {
      "name": "Language model",
      "score": 0.4587874710559845
    },
    {
      "name": "Decoding methods",
      "score": 0.42983460426330566
    },
    {
      "name": "Exploit",
      "score": 0.41235750913619995
    },
    {
      "name": "Natural language",
      "score": 0.39484089612960815
    },
    {
      "name": "Natural language processing",
      "score": 0.3561030626296997
    },
    {
      "name": "Algorithm",
      "score": 0.10704776644706726
    },
    {
      "name": "Engineering",
      "score": 0.09106659889221191
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I24943067",
      "name": "Fudan University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210123185",
      "name": "Zhejiang Lab",
      "country": "CN"
    }
  ],
  "cited_by": 95
}