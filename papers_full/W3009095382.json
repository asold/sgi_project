{
  "title": "What the [MASK]? Making Sense of Language-Specific BERT Models",
  "url": "https://openalex.org/W3009095382",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2749579540",
      "name": "Nozza, Debora",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2789141146",
      "name": "Bianchi, Federico",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202140870",
      "name": "Hovy, Dirk",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3210120707",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2986154550",
    "https://openalex.org/W2988304195",
    "https://openalex.org/W2946676565",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W3216234416",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W131522978",
    "https://openalex.org/W2996580882",
    "https://openalex.org/W3088592174",
    "https://openalex.org/W3032532958",
    "https://openalex.org/W3000640960",
    "https://openalex.org/W2970752815",
    "https://openalex.org/W630532510",
    "https://openalex.org/W2992924668",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W2995647371",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2571911508",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2996400930",
    "https://openalex.org/W2948902769",
    "https://openalex.org/W2251529656",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W2999168658",
    "https://openalex.org/W2990188683",
    "https://openalex.org/W2973071945"
  ],
  "abstract": "Recently, Natural Language Processing (NLP) has witnessed an impressive progress in many areas, due to the advent of novel, pretrained contextual representation models. In particular, Devlin et al. (2019) proposed a model, called BERT (Bidirectional Encoder Representations from Transformers), which enables researchers to obtain state-of-the art performance on numerous NLP tasks by fine-tuning the representations on their data set and task, without the need for developing and training highly-specific architectures. The authors also released multilingual BERT (mBERT), a model trained on a corpus of 104 languages, which can serve as a universal language model. This model obtained impressive results on a zero-shot cross-lingual natural inference task. Driven by the potential of BERT models, the NLP community has started to investigate and generate an abundant number of BERT models that are trained on a particular language, and tested on a specific data domain and task. This allows us to evaluate the true potential of mBERT as a universal language model, by comparing it to the performance of these more specific models. This paper presents the current state of the art in language-specific BERT models, providing an overall picture with respect to different dimensions (i.e. architectures, data domains, and tasks). Our aim is to provide an immediate and straightforward overview of the commonalities and differences between Language-Specific (language-specific) BERT models and mBERT. We also provide an interactive and constantly updated website that can be used to explore the information we have collected, at https://bertlang.unibocconi.it.",
  "full_text": "What the [MASK]?\nMaking Sense of Language-Speciﬁc BERT Models\nDebora Nozza, Federico Bianchi and Dirk Hovy\nBocconi University\nVia Sarfatti 25, 20136 Milan\n{debora.nozza, f.bianchi, dirk.hovy}@unibocconi.it\nAbstract\nRecently, Natural Language Processing (NLP) has witnessed an impressive progress in many\nareas, due to the advent of novel, pretrained contextual representation models. In particular,\nDevlin et al. (2019) proposed a model, called BERT (Bidirectional Encoder Representations from\nTransformers), which enables researchers to obtain state-of-the art performance on numerous\nNLP tasks by ﬁne-tuning the representations on their data set and task, without the need for\ndeveloping and training highly-speciﬁc architectures. The authors also released multilingual\nBERT (mBERT), a model trained on a corpus of 104 languages, which can serve as a universal\nlanguage model. This model obtained impressive results on a zero-shot cross-lingual natural\ninference task. Driven by the potential of BERT models, the NLP community has started to\ninvestigate and generate an abundant number of BERT models that are trained on a particular\nlanguage, and tested on a speciﬁc data domain and task. This allows us to evaluate the true\npotential of mBERT as a universal language model, by comparing it to the performance of these\nmore speciﬁc models. This paper presents the current state of the art in language-speciﬁc BERT\nmodels, providing an overall picture with respect to different dimensions (i.e. architectures,\ndata domains, and tasks). Our aim is to provide an immediate and straightforward overview of\nthe commonalities and differences between Language-Speciﬁc (language-speciﬁc) BERT models\nand mBERT. We also provide an interactive and constantly updated website that can be used to\nexplore the information we have collected, at https://bertlang.unibocconi.it.\n1 Introduction\nIn all natural languages, word meaning varies with and is determined by context, and one of the main\nchallenges of Natural Language Processing (NLP) has been (and remains) to model this property of\nmeaning. Embedding-based language models (Mikolov et al., 2013) have been shown to capture word\nmeaning more efﬁciently than previous methods, allowing for both qualitative analysis of similarities\nand improved performance when used as input to predictive models. However, while embeddings rep-\nresent word types based on their general contextual co-occurrences, they do not learn context-speciﬁc\nrepresentations for each word token.\nRecently, NLP has witnessed the advent of a groundbreaking new language model developed by\nGoogle researchers, called Bidirectional Encoder Representations from Transformers (BERT) (Devlin\net al., 2019). It learns contextual representations for word tokens, thereby getting at their contextual\nvariation in meaning. Contextualized BERT embeddings have since also dominated the leaderboards in\na wide variety of NLP tasks.\nThe power of BERT representations lies in the fact that it is essentially a pretrained model that can\nbe ﬁne-tuned over speciﬁc downstream tasks, which enables it to achieve state-of-the-art results. The\nfundamental underlying component of this architecture is the Transformer model (Vaswani et al., 2017),\nan attention-based mechanism that has been shown to be effective in many different tasks. Both the\nTransformer and BERT have gathered much attention, and there is now a wealth of research articles and\nblog posts describing the inner workings of these models (Rogers et al., 2020, among others).\narXiv:2003.02912v1  [cs.CL]  5 Mar 2020\nGiven the overwhelming success of BERT, a multilingual BERT model (mBERT)1 has been proposed,\nsupporting over 100 languages, including Arabic, Dutch, French, German, Italian, or Portuguese. The\nmodel is trained on different domains, like social media posts or newspaper articles. mBERT has shown\ngreat capabilities in zero-shot cross-lingual tasks (Pires et al., 2019).\nDue to the remarkable results of these models, an abundant number of BERT model extension has\nrecently been introduced by researchers and industry practitioners from several countries: Currently,\nthere are around 5k repositories mentioning “bert” on GitHub.com, and we can expect further demand\nfor BERT extensions. These models are trained on a particular language and tested on a speciﬁc data\ndomain and task, with the promise of maximizing performance across more tasks in that language, saving\nother users further ﬁne-tuning.\nHowever, it has so far not been clearly demonstrated whether the advantage of training a language-\nspeciﬁc model is worth the expense in terms of computational resources2, rather than using theunspeciﬁc\nmultilingual model.\nMoreover, the NLP community is now facing a problem organizing the plethora of models that are\nbeing released. These models are not only trained on different data sets, but also use different conﬁg-\nurations and architectural variants. To give a concrete example, the original BERT model was trained\nusing the WordPiece tokenizer (Wu et al., 2016), however, a recent language-speciﬁc model (Camem-\nBERT (Martin et al., 2019)) used the SentencePiece tokenizer (available as OSS software) (Kudo and\nRichardson, 2018).\nIdentifying which model is the best for a speciﬁc task, and whether the mBERT model is better than\nlanguage-speciﬁc models is a key step future progress in NLP, and will impact the use of computational\nresources. Surveying both GitHub and the literature, we identiﬁed 30 different pretrained language-\nspeciﬁc BERT models, covering 18 Languages and tested on 29 tasks, resulting in 177 different\nperformance results (Le et al., 2019; Antoun et al., 2020; Martin et al., 2019; Alabi et al., 2019; Kuratov\nand Arkhipov, 2019; Arkhipov et al., 2019; Virtanen et al., 2019; Polignano et al., 2019; de Vries et al.,\n2019; Cui et al., 2019). We outline some of the parameters here, and introduce the associated website for\nup-to-date searches. We hope to give NLP researchers and practitioners a clear overview of the tradeoffs\nbefore approaching any NLP task with such a model.\nThe contributions of this paper are the following:\n1. we present an overall picture of language-speciﬁc BERT models from an architectural, task- and\ndomain-related point of view;\n2. we summarize the performance of language-speciﬁc BERT models and compare with the perfor-\nmance of the multilingual BERT model (if available);\n3. we introduce a website to interactively explore state-of-the-art models. We hope this can serve as a\nshared repository for researchers to decide which model best suits their needs.\n2 Bidirectional Encoder Representations from Transformers\nWe assume that most readers who are interested in the topic have a basic understanding of BERT and its\ncomponents. However, for completeness’ sake, we include a brief and high-level overview of the most\nimportant aspects here.\n2.1 BERT\nBERT uses the Transformer (Vaswani et al., 2017) architecture to learn word embeddings. The Trans-\nformer is a recent architectural advancement that can be included in deep networks for sequence mod-\neling. Instead of modeling sequences as RNNs or LSTMs, the Transformer learns global dependencies\nbetween input and output, using only attention mechanisms.\n1https://github.com/google-research/bert/blob/master/multilingual.md\n2These models require a large amount of computational resources unaffordable for many users, and comes with severe\necological costs: training BERT on a GPU is roughly equivalent to a trans-American ﬂight in terms of CO2 emissions (Strubell\net al., 2019)).\n                  BERT Language Model\nI play the violin\n                  BERT Language Model\nI play with my cat\nMasked Language Model I play the violin I play the [MASK]\nNext Sentence Prediction\nI play the violin and the tuba[SEP]\nI play the violin but I eat food[SEP]\n[SEP]\nY es\nNo\n... ...\nFigure 1: A schematic representation of BERT, masked language model and next sentence prediction.\nDifferent words have different meanings and BERT looks at the word context to generate contextual\nrepresentations.\nTransformers greatly shifted the focus of the research community towards attention-based architec-\ntures. The encoder-decoder structure based on transformers is also incorporated into BERT.\nDevlin et al. (2019) introduced BERT in 2018 as a context-sensitive alternative to previous word\nembeddings (which assume a word always has the same representation, independent of its context).\nThe model essentially stacks several encoder-decoder structures based on transformers together. It uses\nmasks to blank out individual words, forcing the model to “ﬁll in the blanks”, thereby increasing its\ncontext-sensitivity. Two key elements in the BERT pretraining process are the masked language model\nand the next sentence prediction. In the former process, a random subsample (in the BERT paper, 15%)\nof the words in a text are replaced by a [MASK] token, and the task is to predict the correct token.\nThe latter process instead is the task of predicting how likely one sentence is to follow another one\nin text. See Figure 1 for a schematic view on BERT. Other than traditional word embeddings, BERT\nrepresentations are not a ﬁxed lookup table, but require the full context to produce a word representation.\nThe vocabulary is deﬁned in advance and it is based on WordPiece (Wu et al., 2016), a tokenization\nalgorithm that generates sub-word tokens.\nDue to its size in terms of parameters, the model usually comes in a pretrained format, which can be\nﬁne-tuned on the task or data set. Simple classiﬁcation layers can be stacked on top of the pretrained\nBERT to provide predictions for several tasks such as sentiment analysis or text classiﬁcation.\n2.2 Multilingual BERT, ALBERT and RoBERTA\nSubsequently, BERT was extended to include several languages. Multilingual (mBERT) was part of the\noriginal paper (Devlin et al., 2019), and is pretrained over several languages using Wikipedia data. This\nallows for zero-shot learning across languages, i.e., training on data from one language and applying the\nmodel to data in another language.\nAlong the same lines, Lan et al. (2019) introduced A Lite BERT (ALBERT), to reduce the computa-\ntional needs of BERT. ALBERT includes two parameters reduction techniques, that reduce the number\nof trainable parameters without signiﬁcantly compromising performance. Moreover, the authors intro-\nduce another self-supervised loss, related to sentence order prediction that is meant to address the limits\nof next sentence prediction used in the BERT model. Another recent paper (Liu et al., 2019) has shown\nthat BERT is sensitive to different training strategies; the authors introduce RoBERTA (Liu et al., 2019)\nas a well-optimized version of BERT.\nFigure 2: The BertLang website front-end interface.\n3 Making-Sense of Language-Speciﬁc BERT Models\nWhile multi- and cross-lingual BERT representations allow for zero-shot learning and capture univer-\nsal semantic structures, they do gloss over language-speciﬁc differences. Consequently, a number of\nlanguage-speciﬁc BERT models have been developed to ﬁll that need. These models almost always\nshowed better performance on the language they were trained for than the universal model.\nIn order to navigate this wealth of constantly changing information, a simple overview paper is no\nlonger sufﬁcient. While we aim to give a general overview here, we refer the interested reader to the\nconstantly updated online resource, BertLang.\n3.1 BertLang\nWe introduce BertLang (https://bertlang.unibocconi.it), a website where we have gath-\nered different language-speciﬁc models that have been introduced on a variety of tasks and data sets.\nMost of the models are available as GitHub links, and some of them are described in research papers,\nbut very few have been published in peer-reviewed conferences 3. In addition to providing a searchable\ninterface, BertLang also provides the possibility to add new information While we hope to independently\nverify the reported results in the future, for now, we only list the various models and conditions.\nWe open-source both data and code to build the website 4, this will make it possible for other re-\nsearchers to contribute to the collection of language-speciﬁc BERT models.\nFigure 2 shows the frontend page of our website, showing a table that contains languages, tasks,\nand performances of different models. We also provide links to the references and code from which\nwe retrieved that information. Beyond this information, we report the performance evaluation metric,\nthe average performance obtained by the language-speciﬁc, and – where available – the corresponding\nperformance of mBERT model and their difference.\n3We do not include resources that feature only a model without reporting any performance results.\n4https://github.com/MilaNLProc/bertlang\nTask Metric Avg. lang-speciﬁc BERT Avg. mBERT Diff.\nNamed Entity Recognition F1 85.26 80.87 4.39\nNatural Language Inference Accuracy 78.35 74.60 3.75\nParaphrase Identiﬁcation Accuracy 88.44 87.74 0.70\nPart of Speech Tagging Accuracy 97.06 95.87 1.19\nPart of Speech Tagging UPOS 98.28 97.33 0.95\nSentiment Analysis Accuracy 90.17 83.80 6.37\nText Classiﬁcation Accuracy 88.96 85.22 3.75\nTable 1: Summary of average performance of different language-speciﬁc BERT models on various tasks.\n3.2 Language-Speciﬁc BERT Models\nThe models we index vary along with a number of dimensions, which we discuss below. The main\ndistinction, however, is the speciﬁc language the model was trained on. The availability of data sets in\nthat language determines the tasks and domains this model was applied to.\nTable 1 shows a summary of the results for the most frequent NLP tasks investigated across several\nlanguages. The results clearly show that on average, language-speciﬁc BERT models obtain higher re-\nsults with respect to mBERT in all the considered tasks. However, while this holds for averages, with the\nproliferation of languages, tasks, and data sets, there is a huge variation in the individual performances.\nIn the following, we analyze the possible views of the collected results in more detail.\nLanguages Covered The language-speciﬁc BERT models proposed range from languages that have\na high number of resources available on the web for training (e.g., French, Italian) to low-resource\nlanguages, such as Yorb and Mongolian. At the current date, we are covering 18 languages.\nInterestingly, from the results it is possible to grasp that low-resources languages (e.g., Yorb and\nArabic) are actually the ones with the highest improvement with respect to mBERT. Since mBERT is\ntrained on Wikipedia, this ﬁnding can probably be explained by the fact that developers of language-\nspeciﬁc BERT models are more likely to be experts on other resources for that language, or to collect\nmore data. This makes a greater difference for low-resource languages.\nArchitectures The most popular architecture is the standard BERT one, but lately, the introduction\n(and the good performances) of both ALBERT and RoBERTA has made researchers consider those two\nlatter models as well to pretrain language models.\nRoBERTA has been used as the base model for the French CamemBERT(Martin et al., 2019), as well\nas the Italian Gilberto5 and Umberto6.\nmBERT was used to initialize and ﬁne-tune models for languages such as Russian (Kuratov and\nArkhipov, 2019), Slavic languages (Arkhipov et al., 2019) 7 and Yorb (Alabi et al., 2019). The latter\nis a noteworthy example of how the scarcity of available data in low resource languages can be over-\ncome. Fine-tuning mBERT instead of pretraining from scratch allowed the authors to produce a model\nwithout access to large amounts of data.\nNLP tasks We currently index results for 29 NLP tasks. Table 1 reports the results for the most popular\ntasks in the collected data, with Named Entity Recognition (NER) the most frequent task (22 entries).\nLooking at the source of the test data (see the released website for the complete information), we observe\nthat there are some multilingual benchmark data sets that are used for the same NLP task in different lan-\nguages. Some of them have been released by research group publishing in well-known NLP conferences\n(Yang et al., 2019; Sanguinetti and Bosco, 2015; Conneau et al., 2018; V¨olker et al., 2019), while others\nhave been released in conjunction with shared tasks such as SemEval or CoNLL (Zeman et al., 2018;\n5https://github.com/idb-ita/GilBERTo\n6https://github.com/musixmatchresearch/umberto\n7Here “Slavic” includes Russian, Bulgarian, Czech and Polish.\nNavigli et al., 2013; Bosco et al., 2016; Benikova et al., 2014). The latter group shows the effect shared\ntask have on providing the NLP community with benchmark references.\nRemarkably, the noun sense disambiguation task is the only task where language-speciﬁc BERT per-\nformances are lower than the mBERT ones. As stated by the authors (Le et al., 2019), this could be\ndue to the fact that the training corpora have been machine-translated from English to French, making\nmBERT probably better suited for the task than a model trained on native French.\nSentiment analysis is the task where language-speciﬁc BERT models obtain the highest improvements\nwith respect to mBERT. Following the previous intuition, for Arabic (Antoun et al., 2020) this can be\nexplained by considering the peculiar language of the test data set, which demonstrates the ability of the\nlanguage-speciﬁc AraBERT model to handle dialects — even if they were not explicitly included in the\ntraining set.\nBeyond the well-known NLP tasks, it is interesting to note that language-speciﬁc tasks have been\ninvestigated as well, e.g., the Die/Dat (gendered determiners) disambiguation task in Dutch (Delobelle\net al., 2020), obtaining impressive improvements with respect to state-of-the-art (Allein et al., 2020)\n(∼23% points accuracy improvement).\nDomains There is a huge variety of domains considered in language-speciﬁc BERT models. We need\nto make a distinction, though, between data sets used to pretrain the models and data sets used to evaluate\nthe models.\nData used for training mainly varies across three source corpora: (i) Wikipedia, (ii) OPUS Cor-\npora (Tiedemann, 2012) and (iii) OSCAR (Ortiz Su´arez et al., 2019). Wikipedia is currently comprising\nmore than 40 million articles created and maintained as an open collaboration project in 301 different\nlanguages, making it the largest and most popular multilingual online encyclopedia. mBERT, for ex-\nample, was trained over 100 different language-speciﬁc Wikipedia versions. OPUS is a freely available\ncollection of parallel corpora, covering over 90 languages. The largest domains covered by OPUS are\nlegislative and administrative texts, translated movie subtitles and localization data from open-source\nsoftware projects (Tiedemann, 2012). OSCAR (Open Super-large Crawled Almanach coRpus) (Or-\ntiz Su´arez et al., 2019) is a huge multilingual corpus obtained by ﬁltering the Common Crawl corpus,\nwhich is a parallel multilingual corpus comprised of crawled documents from the internet.\nSeveral models concatenate more sources to have enough data to pretrain BERT, for example BERTje\n(Dutch BERT), which concatenates news, book data, and Wikipedia data and other text. Languages with\nmore limited availability of data, such as Yorb, have brought researchers to ﬁne-tune mBERT instead of\npretraining from scratch. A notable case is the Italian BERT model ALBERTO (Polignano et al., 2019),\nwhich is the only one that has been trained only on social media data (speciﬁcally, on 2 million Twitter\nposts in Italian language).\nOn the other hand, different domain data sets have been used to evaluate the models; these range from\nreview data for sentiment analysis tasks to transcripts and news for more traditional tasks, such as part of\nspeech tagging. News data are the most common domain, presumably because they are easier to retrieve,\nand because their more formal register makes them more suited for tasks such as part of speech tagging,\ndependency parsing, and named entity recognition. Similarly, social media posts from Twitter are mostly\nused in tasks like sentiment analysis and identiﬁcation of offensive language.\n4 Conclusions\nBERT (Devlin et al., 2019) has greatly improved results in many different NLP tasks and has become\na mainstay of the community. Following this development, a multilingual BERT and several language-\nspeciﬁc versions have been developed and contributed even more to the success of NLP applications.\nIn this paper, we have analyzed the current state-of-the-art, showing languages are covered, which\ntasks tackled, and which domains considered in pretrained language-speciﬁc BERT models. Moreover,\nwe have underlined the huge variability models and the difﬁculty for researchers to ﬁnd the best model\nfor a speciﬁc task, language, and domain. To this end, we have introduced BertLang, a website that\nallows researchers to search and explore the current state-of-the-art with respect to language-speciﬁc\nBERT models.\nIn the future, we plan to provide independent veriﬁcation of reported results and direct comparisons of\nlanguage-speciﬁc BERT models on speciﬁc domains and tasks. We plan to use the same data to ﬁne-tune\nthe models providing comparable performance values for the models. We believe these comparisons will\nbe beneﬁcial to the community of both researchers and beginning practitioners in NLP.\nReferences\nJesujoba O Alabi, Kwabena Amponsah-Kaakyire, David I Adelani, and Cristina Espa ˜na-Bonet. 2019. Mas-\nsive vs. curated word embeddings for low-resourced languages. The case of Yor `ub´a and Twi. arXiv preprint\narXiv:1912.02481.\nLiesbeth Allein, Artuur Leeuwenberg, and Marie-Francine Moens. 2020. Binary and multitask classiﬁcation\nmodel for Dutch anaphora resolution: Die/Dat prediction. arXiv preprint arXiv:2001.02943.\nWissam Antoun, Fady Baly, and Hazem Hajj. 2020. AraBERT: Transformer-based model for Arabic language\nunderstanding. arXiv preprint arXiv:2003.00104.\nMikhail Arkhipov, Maria Troﬁmova, Yuri Kuratov, and Alexey Sorokin. 2019. Tuning multilingual transformers\nfor language-speciﬁc named entity recognition. In Proceedings of the 7th Workshop on Balto-Slavic Natural\nLanguage Processing, pages 89–93. Association for Computational Linguistics.\nDarina Benikova, Chris Biemann, Max Kisselew, and Sebastian Pado. 2014. GermEval 2014 named entity recog-\nnition shared task: companion paper. In Proceedings of the KONVENS GermEval Shared Task on Named Entity\nRecognition, pages 104–112.\nCristina Bosco, Tamburini Fabio, Bolioli Andrea, and Alessandro Mazzei. 2016. Overview of the evalita 2016\npart of speech on twitter for italian task. In Proceedings of the 5th Evaluation Campaign of Natural Language\nProcessing and Speech Tools for Italian. Final Workshop, EVALITA 2016 , volume 1749, pages 1–7. CEUR\nWorkshop Proceedings (CEUR-WS. org).\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel R. Bowman, Holger Schwenk, and\nVeselin Stoyanov. 2018. XNLI: evaluating cross-lingual sentence representations. In Proceedings of the 2018\nConference on Empirical Methods in Natural Language Processing, EMNLP 2018, pages 2475–2485. Associ-\nation for Computational Linguistics.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, and Guoping Hu. 2019. Pre-training\nwith whole word masking for Chinese BERT. arXiv preprint arXiv:1906.08101.\nWietse de Vries, Andreas van Cranenburgh, Arianna Bisazza, Tommaso Caselli, Gertjan van Noord, and Malvina\nNissim. 2019. BERTje: A Dutch BERT model. arXiv preprint arXiv:1912.09582.\nPieter Delobelle, Thomas Winters, and Bettina Berendt. 2020. RobBERT: a dutch RoBERTa-based language\nmodel. arXiv preprint arXiv:2001.06286.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirec-\ntional transformers for language understanding. In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019,\npages 4171–4186.\nTaku Kudo and John Richardson. 2018. SentencePiece: A simple and language independent subword tokenizer\nand detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2018: System Demonstrations, pages 66–71. Association for Computa-\ntional Linguistics.\nYuri Kuratov and Mikhail Arkhipov. 2019. Adaptation of deep bidirectional multilingual transformers for Russian\nlanguage. arXiv preprint arXiv:1905.07213.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. AL-\nBERT: A lite BERT for self-supervised learning of language representations.arXiv preprint arXiv:1909.11942.\nHang Le, Lo¨ıc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen,\nBenoˆıt Crabb´e, Laurent Besacier, and Didier Schwab. 2019. FlauBERT: Unsupervised language model pre-\ntraining for French. arXiv preprint arXiv:1912.05372.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized BERT pretraining approach. arXiv\npreprint arXiv:1907.11692.\nLouis Martin, Benjamin Muller, Pedro Javier Ortiz Su´arez, Yoann Dupont, Laurent Romary, ´Eric Villemonte de la\nClergerie, Djam ´e Seddah, and Beno ˆıt Sagot. 2019. CamemBERT: a tasty French language model. arXiv\npreprint arXiv:1911.03894.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of\nwords and phrases and their compositionality. In Advances in Neural Information Processing Systems , pages\n3111–3119.\nRoberto Navigli, David Jurgens, and Daniele Vannella. 2013. SemEval-2013 task 12: Multilingual word sense\ndisambiguation. In Proceedings of the 7th International Workshop on Semantic Evaluation, SemEval 2013 ,\npages 222–231. The Association for Computer Linguistics.\nPedro Javier Ortiz Su´arez, Benoˆıt Sagot, and Laurent Romary. 2019. Asynchronous pipeline for processing huge\ncorpora on medium to low resource infrastructures. In Proceedings of the 7th Workshop on the Challenges in\nthe Management of Large Corpora, CMLC-7. Leibniz-Institut f¨ur Deutsche Sprache.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019. How multilingual is multilingual BERT? In Proceedings of\nthe 57th Conference of the Association for Computational Linguistics, ACL 2019, pages 4996–5001. Association\nfor Computational Linguistics.\nMarco Polignano, Pierpaolo Basile, Marco de Gemmis, Giovanni Semeraro, and Valerio Basile. 2019. ALBERTO:\nItalian BERT language understanding model for NLP challenging tasks based on tweets. In Proceedings of the\n6th Italian Conference on Computational Linguistics, CLiC-it 2019. CEUR Workshop Proceedings (CEUR-WS.\norg).\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky. 2020. A primer in BERTology: What we know about how\nBERT works. arXiv preprint arXiv:2002.12327.\nManuela Sanguinetti and Cristina Bosco. 2015. PartTUT: The Turin university parallel treebank. In Harmo-\nnization and Development of Resources and Tools for Italian Natural Language Processing within the PARLI\nProject, volume 589 of Studies in Computational Intelligence, pages 51–69. Springer.\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy considerations for deep\nlearning in NLP. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL\n2019, pages 3645–3650. Association for Computational Linguistics.\nJ¨org Tiedemann. 2012. Parallel data, tools and interfaces in OPUS. In Proceedings of the 8th International Con-\nference on Language Resources and Evaluation, LREC 2012, pages 2214–2218. European Language Resources\nAssociation (ELRA).\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and\nIllia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages\n5998–6008.\nAntti Virtanen, Jenna Kanerva, Rami Ilo, Jouni Luoma, Juhani Luotolahti, Tapio Salakoski, Filip Ginter, and\nSampo Pyysalo. 2019. Multilingual is not enough: BERT for Finnish. arXiv preprint arXiv:1912.07076.\nEmanuel Borges V¨olker, Maximilian Wendt, Felix Hennig, and Arne K¨ohn. 2019. HDT-UD: A very large univer-\nsal dependencies treebank for German. In Proceedings of the 3rd Workshop on Universal Dependencies (UDW,\nSyntaxFest 2019), pages 46–57. Association for Computational Linguistics.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim\nKrikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google’s neural machine translation system: Bridg-\ning the gap between human and machine translation. arXiv preprint arXiv:1609.08144.\nYinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. 2019. PAWS-X: A cross-lingual adversarial dataset for\nparaphrase identiﬁcation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019,\npages 3685–3690. Association for Computational Linguistics.\nDaniel Zeman, Jan Hajic, Martin Popel, Martin Potthast, Milan Straka, Filip Ginter, Joakim Nivre, and Slav Petrov.\n2018. CoNLL 2018 shared task: Multilingual parsing from raw text to universal dependencies. In Proceedings\nof the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 1–21.\nAssociation for Computational Linguistics.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8586307764053345
    },
    {
      "name": "Language model",
      "score": 0.8047778606414795
    },
    {
      "name": "Inference",
      "score": 0.6003332138061523
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5925295948982239
    },
    {
      "name": "Natural language processing",
      "score": 0.574893593788147
    },
    {
      "name": "Task (project management)",
      "score": 0.5566191673278809
    },
    {
      "name": "Natural language understanding",
      "score": 0.4997878074645996
    },
    {
      "name": "Transformer",
      "score": 0.47390902042388916
    },
    {
      "name": "Encoder",
      "score": 0.45557183027267456
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4509633183479309
    },
    {
      "name": "Representation (politics)",
      "score": 0.4368850290775299
    },
    {
      "name": "Language understanding",
      "score": 0.42184627056121826
    },
    {
      "name": "Natural language",
      "score": 0.3819962441921234
    },
    {
      "name": "Programming language",
      "score": 0.12457498908042908
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I71209653",
      "name": "Bocconi University",
      "country": "IT"
    }
  ]
}