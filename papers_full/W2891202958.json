{
  "title": "Recurrent Transformer Networks for Semantic Correspondence",
  "url": "https://openalex.org/W2891202958",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2747238167",
      "name": "Kim, Seungryong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2159534709",
      "name": "Lin, Stephen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4297738777",
      "name": "Jeon, Sangryul",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2725965989",
      "name": "Min Dongbo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2184771091",
      "name": "Sohn, Kwanghoon",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1763426478",
    "https://openalex.org/W2141362318",
    "https://openalex.org/W2949997773",
    "https://openalex.org/W2736876108",
    "https://openalex.org/W2952695679",
    "https://openalex.org/W209424029",
    "https://openalex.org/W1996140089",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2770302340",
    "https://openalex.org/W2611605760",
    "https://openalex.org/W2124861766",
    "https://openalex.org/W2144794286",
    "https://openalex.org/W603908379",
    "https://openalex.org/W2952064829",
    "https://openalex.org/W2604233003",
    "https://openalex.org/W2464606141",
    "https://openalex.org/W2950124505",
    "https://openalex.org/W2949896259",
    "https://openalex.org/W1960436198",
    "https://openalex.org/W2963020784",
    "https://openalex.org/W2898476649",
    "https://openalex.org/W2562066862",
    "https://openalex.org/W2612584387",
    "https://openalex.org/W2263386426",
    "https://openalex.org/W2535410496",
    "https://openalex.org/W2104974755",
    "https://openalex.org/W2151103935",
    "https://openalex.org/W1926639317",
    "https://openalex.org/W2747550417",
    "https://openalex.org/W2016120301",
    "https://openalex.org/W2435623039",
    "https://openalex.org/W2106505277",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W2964213755",
    "https://openalex.org/W2963840672"
  ],
  "abstract": "We present recurrent transformer networks (RTNs) for obtaining dense correspondences between semantically similar images. Our networks accomplish this through an iterative process of estimating spatial transformations between the input images and using these transformations to generate aligned convolutional activations. By directly estimating the transformations between an image pair, rather than employing spatial transformer networks to independently normalize each individual image, we show that greater accuracy can be achieved. This process is conducted in a recursive manner to refine both the transformation estimates and the feature representations. In addition, a technique is presented for weakly-supervised training of RTNs that is based on a proposed classification loss. With RTNs, state-of-the-art performance is attained on several benchmarks for semantic correspondence.",
  "full_text": "Recurrent Transformer Networks for Semantic\nCorrespondence\nSeungryong Kim1, Stephen Lin2, Sangryul Jeon1, Dongbo Min3, and Kwanghoon Sohn1\n1Yonsei University, Seoul, South Korea,2Microsoft Research, Beijing, China,\n3Ewha Womans University, Seoul, South Korea\n{srkim89,cheonjsr,khsohn}@yonsei.ac.kr, stevelin@microsoft.com,\ndbmin@ewha.ac.kr\nAbstract\nWe present recurrent transformer networks (RTNs) for obtaining dense correspon-\ndences between semantically similar images. Our networks accomplish this through\nan iterative process of estimating spatial transformations between the input images\nand using these transformations to generate aligned convolutional activations. By\ndirectly estimating the transformations between an image pair, rather than employ-\ning spatial transformer networks to independently normalize each individual image,\nwe show that greater accuracy can be achieved. This process is conducted in a\nrecursive manner to reﬁne both the transformation estimates and the feature repre-\nsentations. In addition, a technique is presented for weakly-supervised training of\nRTNs that is based on a proposed classiﬁcation loss. With RTNs, state-of-the-art\nperformance is attained on several benchmarks for semantic correspondence.\n1 Introduction\nEstablishing dense correspondences across semantically similar images can facilitate a variety of\ncomputer vision applications including non-parametric scene parsing, semantic segmentation, object\ndetection, and image editing [25; 22; 20]. In this semantic correspondence task, the images resemble\neach other in content but differ in object appearance and conﬁguration, as exempliﬁed in the images\nwith different car models in Fig. 1(a-b). Unlike the dense correspondence computed for estimating\ndepth [34] or optical ﬂow [4], semantic correspondence poses additional challenges due to intra-class\nappearance and shape variations among different instances from the same object or scene category.\nTo address these challenges, state-of-the-art methods generally extract deep convolutional neural\nnetwork (CNN) based descriptors [5; 45; 18], which provide some robustness to appearance variations,\nand then perform a regularization step to further reduce the range of appearance. The most recent\ntechniques handle geometric deformations in addition to appearance variations within deep CNNs.\nThese methods can generally be classiﬁed into two categories, namely methods for geometric\ninvariance in the feature extraction step, e.g., spatial transformer networks (STNs) [15; 5; 20], and\nmethods for geometric invariance in the regularization step, e.g., geometric matching networks [30;\n31]. The STN-based methods infer geometric deformation ﬁelds within a deep network and transform\nthe convolutional activations to provide geometric-invariant features [5; 41; 20]. While this approach\nhas shown geometric invariance to some extent, we conjecture that directly estimating the geometric\ndeformations between a pair of input images would be more robust and precise than learning to\ntransform each individual image to a geometric-invariant feature representation. This direct estimation\napproach is used by geometric matching-based techniques [30; 31], which recover a matching model\ndirectly through deep networks. Drawbacks of these methods include that globally-varying geometric\nﬁelds are inferred, and only ﬁxed, untransformed versions of the features are used.\n32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada.\narXiv:1810.12155v1  [cs.CV]  29 Oct 2018\n(a)\n (b)\n (c)\n (d)\n (e)\n (f)\nFigure 1: Visualization of results from RTNs: (a) source image; (b) target image; (c), (d) warped\nsource and target images using dense correspondences from RTNs; (e), (f) pseudo ground-truth\ntransformations as in [36]. RTNs learn to infer transformations without ground-truth supervision.\nIn this paper, we present recurrent transformer networks (RTNs) for overcoming the aforementioned\nlimitations of current semantic correspondence techniques. As illustrated in Fig. 2, the key idea\nof RTNs is to directly estimate the geometric transformation ﬁelds between two input images, like\nwhat is done by geometric matching-based approaches [30; 31], but also apply the estimated ﬁeld to\ntransform the convolutional activations of one of the images, similar to STN-based methods [15; 5; 20].\nWe additionally formulate the RTNs to recursively estimate the geometric transformations, which are\nused for iterative geometric alignment of feature activations. In this way, regularization is enhanced\nthrough recursive reﬁnement, while feature extraction is likewise iteratively reﬁned according to the\ngeometric transformations as well as jointly learned with the regularization. Moreover, the networks\nare learned in a weakly-supervised manner via a proposed classiﬁcation loss deﬁned between the\nsource image features and the geometrically-aligned target image features, such that the correct\ntransformation is identiﬁed by the highest matching score while other transformations are considered\nas negative examples.\nThe presented approach is evaluated on several common benchmarks and examined in an ablation\nstudy. The experimental results show that this model outperforms the latest weakly-supervised and\neven supervised methods for semantic correspondence.\n2 Related Work\nSemantic Correspondence To elevate matching quality, most conventional methods for semantic\ncorrespondence focus on improving regularization techniques while employing handcrafted features\nsuch as SIFT [27]. Liu et al. [25] pioneered the idea of dense correspondence across different scenes,\nand proposed SIFT ﬂow. Inspired by this, methods have been presented based on deformable spatial\npyramids (DSP) [ 17], object-aware hierarchical graphs [ 39], exemplar LDA [ 3], joint image set\nalignment [44], and joint co-segmentation [36]. As all of these techniques use handcrafted descriptors\nand regularization methods, they lack robustness to geometric deformations.\nRecently, deep CNN-based methods have been used in semantic correspondence as their descriptors\nprovide some degree of invariance to appearance and shape variations. Among them are techniques\nthat utilize a 3-D CAD model for supervision [45], employ fully convolutional feature learning [5],\nlearn ﬁlters with geometrically consistent responses across different object instances [ 28], learn\nnetworks using dense equivariant image labelling [ 37], exploit local self-similarity within a fully\nconvolutional network [18; 20], and estimate correspondences using object proposals [ 7; 8; 38].\nHowever, none of these methods is able to handle non-rigid geometric variations, and most of\nthem are formulated with handcrafted regularization. More recently, Han et al. [9] formulated the\nregularization into the CNN but do not deal explicitly with the signiﬁcant geometric variations\nencountered in semantic correspondence.\nSpatial Invariance Some methods aim to alleviate spatial variation problems in semantic corre-\nspondence through extensions of SIFT ﬂow, including scale-less SIFT ﬂow (SLS) [11], scale-space\nSIFT ﬂow (SSF) [29], and generalized DSP (GDSP) [13]. A generalized PatchMatch algorithm [1]\nwas proposed for efﬁcient matching that leverages a randomized search scheme. It was utilized by\nHaCohen et al. [6] in a non-rigid dense correspondence (NRDC) algorithm. Spatial invariance to scale\nand rotation is provided by DAISY ﬁlter ﬂow (DFF) [40]. While these aforementioned techniques\nprovide some degree of geometric invariance, none of them can deal with afﬁne transformations over\nan image. Recently, Kim et al. [19; 21] proposed the discrete-continuous transformation matching\n2\nFeature \nExtraction\nFeature \nExtraction\nFlow \nEstimation\nLocalisation\nif\niA\ns\niD\n()t\niD A\nSource\nTarget\n(a)\nFeature \nExtraction\nFeature \nExtraction\nGeometric \nMatching\ns\niD\nt\niD\niT\nSource\nTarget (b)\nFeature \nExtraction\nFeature \nExtraction\nGeometric \nMatching\ns\niD\n()t\niD T\niT\nSource\nTarget (c)\nFigure 2: Intuition of RTNs: (a) methods for geometric inference in the feature extraction step, e.g.,\nSTN-based methods [5; 20], (b) methods for geometric invariance in the regularization step, e.g.,\ngeometric matching-based methods [30; 31], and (c) RTNs, which weave the advantages of both\nexisting STN-based methods and geometric matching techniques, by recursively estimating geometric\ntransformation residuals using geometry-aligned feature activations.\n(DCTM) framework where dense afﬁne transformation ﬁelds are inferred using a hand-designed\nenergy function and regularization.\nTo deal with geometric variations within CNNs, STNs [15] offer a way to provide geometric invari-\nance by warping features through a global transformation. Inspired by STNs, Lin et al. [23] proposed\ninverse compositional STNs (IC-STNs) that replaces the feature warping with transformation pa-\nrameter propagation. Kanazawa et al. [16] presented WarpNet that predicts a warp for establishing\ncorrespondences. Rocco et al. [ 30; 31] proposed a CNN architecture for estimating a geometric\nmatching model for semantic correspondence. However, they estimate only globally-varying geomet-\nric ﬁelds, thus leading to limited performance in dealing with locally-varying geometric deformations.\nTo deal with locally-varying geometric variations, some methods such as UCN-spatial transformer\n(UCN-ST) [5] and convolutional afﬁne transformer-FCSS (CAT-FCSS) [20] employ STNs [15] at\nthe pixel level. Similarly, Yi et al. [41] proposed the learned invariant feature transform (LIFT) to\nlearn sparsely, locally-varying geometric ﬁelds, inspired by [42]. However, these methods determine\ngeometric ﬁelds by accounting for the source and target images independently, rather than jointly,\nwhich limits their prediction ability.\n3 Background\nLet us denote semantically similar source and target images as Is and It, respectively. The objective\nis to establish a correspondence ﬁeld fi = [ui,vi]T between the two images that is deﬁned for each\npixel i= [ix,iy]T in Is. Formally, this involves ﬁrst extracting handcrafted or deep features, denoted\nby Ds\ni and Dt\ni, from Is\ni and It\ni within local receptive ﬁelds, and then estimating the correspondence\nﬁeld fi of the source image by maximizing the feature similarity between Ds\ni and Dt\ni+fi over a set of\ntransformations using handcrafted or deep geometric regularization models. Several approaches [25;\n18] assume the transformation to be a 2-D translation with negligible variation within local receptive\nﬁelds. As a result, they often fail to handle complicated deformations caused by scale, rotation, or skew\nthat may exist among object instances. For greater geometric invariance, recent approaches [19; 21]\nhave modeled the deformations as an afﬁne transformation ﬁeld represented by a 2 ×3 matrix\nTi = [Ai |fi ] (1)\nthat maps pixel ito i′= i+ fi. Speciﬁcally, they maximize the similarity between the source Ds\ni and\ntarget Dt\ni′ (Ai), where D(Ai) represents the feature extracted from spatially-varying local receptive\nﬁelds transformed by a 2 ×2 matrix Ai [5; 20]. For simplicity, we denote Dt(Ti) =Dt\ni+fi (Ai).\nApproaches for geometric invariance in semantic correspondence can generally be classiﬁed into two\ncategories. The ﬁrst group infers the geometric ﬁelds in the feature extraction step by minimizing\na matching objective function [ 5; 20], as exempliﬁed in Fig. 2(a). Concretely, Ai is learned\nwithout a ground-truth A∗\ni by minimizing the difference between Ds\ni and Dt\ni+fi (Ai) according to a\nground-truth ﬂow ﬁeld f∗\ni . This enables explicit feature learning which aims to minimize/maximize\nconvolutional activation differences between matching/non-matching pixel pairs [5; 20]. However,\nground-truth ﬂow ﬁelds f∗\ni are still needed for learning the networks, and it predicts the geometric\n3\nSource\nTarget\nFeature Extraction Net.\nGeometric Matching Network\nEncoder Decoder\n1k\ni\nT\n1()tk\niD T\ns\niD\nCorrelation\n…\n…\nSkip Connection\n11 ( ( , ( )) | )k k s t k\ni i i i G DD T T T W\n1( ( , ( )) | )s t k\ni i GDD TW\n( | )iFI W\nFigure 3: Network conﬁguration of RTNs, consisting of a feature extraction network and a geometric\nmatching network in a recurrent structure.\nﬁelds Ai based only on the source or target feature, without jointly considering the source and target,\nthus limiting performance.\nThe second group estimates a geometric matching model directly through deep networks by consider-\ning the source and target features simultaneously [30; 31]. These methods formulate the geometric\nmatching networks by mimicking conventional RANSAC-like methods [14] through feature extrac-\ntion and geometric matching steps. As illustrated in Fig. 2(b), the geometric ﬁelds Ti are predicted\nin a feed-forward network from extracted source features Ds\ni and target features Dt\ni. By learning\nto extract source and target features and predict geometric ﬁelds in an end-to-end manner, more\nrobust geometric ﬁelds can be estimated compared to existing STN-based methods that consider\nsource or target features independently as shown in [31]. A major limitation of these learning-based\nmethods is the lack of ground-truth geometric ﬁelds T∗\ni between source and target images. To\nalleviate this problem, some methods use self-supervision such as synthetic transformations [30] or\nweak-supervision such as soft-inlier maximization [31], but these approaches constrain the global\ngeometric ﬁeld only. Moreover, these methods utilize feature descriptors extracted from the original\nupright images, rather than from geometrically transformed images, which limits their capability to\nrepresent severe geometric variations.\n4 Recurrent Transformer Networks\n4.1 Motivation and Overview\nIn this section, we describe the formulation of recurrent transformer networks (RTNs). The objective\nof our networks is to learn and infer locally-varying afﬁne deformation ﬁelds Ti in an end-to-end\nand weakly-supervised fashion using only image pairs without ground-truth transformations T∗\ni .\nToward this end, we present an effective and efﬁcient integration of the two existing approaches for\ngeometric invariance, i.e., STN-based feature extraction networks [5; 20] and geometric matching\nnetworks [30; 31], that includes a novel weakly-supervised loss function tailored to our objective.\nSpeciﬁcally, the ﬁnal geometric ﬁeld is recursively estimated by deforming the activations of feature\nextraction networks according to the intermediate output of the geometric matching networks, in\ncontrast to existing approaches based on geometric matching which consider only ﬁxed, upright\nversions of features [30; 31]. At the same time, our method outperforms STN-based approaches [5;\n20] by using a deep CNN-based geometric matching network instead of handcrafted matching\ncriteria. Our recurrent geometric matching approach intelligently weaves the advantages of both\nexisting STN-based methods and geometric matching techniques, by recursively estimating geometric\ntransformation residuals using geometry-aligned feature activations.\nConcretely, our networks are split into two parts, as shown in Fig. 3: a feature extraction network\nto extract source Ds\ni and target Dt(Ti) features, and a geometric matching networkto infer the\ngeometric ﬁelds Ti. To learn these networks in a weakly-supervised manner, we formulate a novel\nclassiﬁcation loss deﬁned without ground-truth T∗\ni based on the assumption that the transformation\nwhich maximizes the similarity of the source features Ds\ni and transformed target features Dt(Ti) at\na pixel ishould be correct, while the matching scores of other transformation candidates should be\nminimized.\n4\n(a)\n (b)\n (c)\n (d)\n (e)\n (f)\nFigure 4: Visualization of search window Ni in RTNs (e.g., |Ni|: 5×5): Source images with the\nsearch window of (a) stride 4, (c) stride 2 , (e) stride 1, and target images with (b), (d), (f) transformed\npoints for (a), (c), (e), respectively. As evolving iterations, the dilate strides are reduced to consider\nprecise matching details.\n4.2 Feature Extraction Network\nTo extract convolutional features for sourceDs and target Dt, the input source and target images (Is,\nIt) are ﬁrst passed through fully-convolutional feature extraction networks with shared parameters\nWF such that Di = F(IiWF ), and the feature for each pixel then undergoes L2 normalization.\nIn the recurrent formulation, at each iteration the target features Dt can be extracted according to\nTi such that Dt(Ti) = F(It(Ti)|WF ). However, extracting each feature by transforming local\nreceptive ﬁelds within the target image It according to Ti for each pixel iand then passing it through\nthe networks would be time-consuming when iterating the networks. Instead, we employ a strategy\nsimilar to UCN-ST [ 5] and CAT-FCSS [20] by ﬁrst extracting the convolutional features of the\nentire image It by passing it through the networks except for the last convolutional layer, and then\ncomputing Dt(Ti) by transforming the resultant convolutional features and ﬁnally passing it through\nthe last convolution with stride to combine the transformed activations independently [ 5; 20]. It\nshould be noted that any other convolutional features [35; 12] could be used in this framework.\n4.3 Recurrent Geometric Matching Network\nConstraint Correlation Volume To predict the geometric ﬁelds from two convolutional features\nDs and Dt, we ﬁrst compute the correlation volume with respect to translational motion only [30; 31]\nand then pass it to subsequent convolutional layers to determine dense afﬁne transformation ﬁelds. As\nshown in [31], this two-step approach reliably prunes incorrect matches. Speciﬁcally, the similarity\nbetween two extracted features is computed as the cosine similarity with L2 normalization:\nC(Ds\ni ,Dt(Tj)) =<Ds\ni ,Dt(Tj) >/\n√∑\nl\n<Ds\ni ,Dt(Tl) >2, (2)\nwhere j,l ∈Ni for the search window Ni of pixel i.\nCompared to [30; 31] that consider all possible samples within an image, the constraint correlation\nvolume deﬁned within Ni reduces the matching ambiguity and computational times. However, due\nto the limited search window range, it may not cover large geometric variations. To alleviate this\nlimitation, inspired by [43], we utilize dilation techniques in a manner that the local neighborhood\nNi is enlarged with larger stride than 1 pixel, and this dilation is reduced as the iterations progress, as\nexempliﬁed in Fig. 4.\nRecurrent Geometry Estimation Based on this matching similarity, the recurrent geometry es-\ntimation network with parameters WG iteratively estimates the residual between the previous and\ncurrent geometric transformation ﬁelds as\nTk\ni −Tk−1\ni = F(C(Ds\ni ,Dt(Tk−1)); WG), (3)\nwhere Tk\ni denotes the transformation ﬁelds at the k-th iteration. The ﬁnal geometric ﬁelds are then\nestimated in a recurrent manner as follows:\nTi = T0\ni +\n∑\nk∈{1,..,Kmax}\nF(C(Ds\ni ,Dt(Tk−1)); WG), (4)\nwhere Kmax denotes the maximum iteration and T0\ni is an initial geometric ﬁeld. Unlike [ 30; 31]\nwhich estimate a global afﬁne or thin-plate spline transformation ﬁeld, we formulate the encoder-\ndecoder networks as in [32] to estimate locally-varying geometric ﬁelds. Moreover, our networks are\n5\n(a)\n (b)\n (c)\n (d)\n (e)\n (f)\nFigure 5: Convergence of RTNs: (a) source image; (b) target image; Iterative evolution of warped\nimages (c), (d), (e), and (f) after iteration 1, 2, 3, and 4. In the recurrent formulation of RTNs, the\npredicted transformation ﬁeld becomes progressively more accurate through iterative estimation.\nformulated in a fully-convolutional manner, thus source and target inputs of any size can be processed,\nin contrast to [30; 31] which can take inputs of only a ﬁxed size.\nIteratively inferring afﬁne transformation residuals boosts matching precision and facilitates conver-\ngence. Moreover, inferring residuals instead of carrying the input information through the network has\nbeen shown to improve network optimization [12]. As shown in Fig. 5, the predicted transformation\nﬁeld becomes progressively more accurate through iterative estimation.\n4.4 Weakly-supervised Learning\nA major challenge of semantic correspondence with deep CNNs is the lack of ground-truth cor-\nrespondence maps for training. Obtaining such ground-truth data through manual annotation is\nlabor-intensive and may be degraded by subjectivity [ 36; 7; 8]. To learn the networks using only\nweak supervision in the form of image pairs, we formulate the loss function based on the intuition\nthat the matching score between the source feature Ds\ni at each pixel iand the target feature Dt(Ti)\nshould be maximized while keeping the scores of other transformation candidates low. This can\nbe treated as a classiﬁcation problem in that the network can learn a geometric ﬁeld as a hidden\nvariable for maximizing the scores for matchable Ti while minimizing the scores for non-matchable\ntransformation candidates. The optimal ﬁelds Ti can be learned with a classiﬁcation loss [20] in a\nweakly-supervised manner by minimizing the energy function\nL(Ds\ni ,Dt(T)) =−\n∑\nj∈Mi\np∗\nj log(p(Ds\ni ,Dt(Tj))), (5)\nwhere the function p(Ds\ni ,Dt(Tj)) is a softmax probability deﬁned as\np(Ds\ni ,Dt(Tj)) = exp(C(Ds\ni ,Dt(Tj)))∑\nl∈Mi\nexp(C(Ds\ni ,Dt(Tl))), (6)\nwith p∗\nj denoting a class label deﬁned as 1 if j = i, and 0 otherwise for j ∈Mi for the search\nwindow Mi, such that the center point iwithin Mi becomes a positive sample while the other points\nare negative samples.\nWith this loss function, the derivatives ∂L/∂Ds and ∂L/∂Dt(T) of the loss function Lwith respect\nto the features Ds and Dt(T) can be back-propagated into the feature extraction networks F(·|WF ).\nExplicit feature learning in this manner with the classiﬁcation loss has been shown to be reliable [20].\nLikewise, the derivatives ∂L/∂Dt(T) and ∂Dt(T)/∂T of the loss function Lwith respect to\ngeometric ﬁelds T can be back-propagated into the geometric matching networks F(·|WG) to learn\nthese networks without ground truth T∗.\nIt should be noted that our loss function is conceptually similar to [31] in that it is formulated with\nsource and target features in a weakly-supervised manner. While [31] utilizes only positive samples\nin learning feature extraction networks, our method considers both positive and negative samples to\nenhance network training.\n5 Experimental Results and Discussion\n5.1 Experimental Settings\nIn the following, we comprehensively evaluated our RTNs through comparisons to state-of-the-art\nmethods for semantic correspondence, including SF [25], DSP [17], Zhou et al. [45], Taniai et al. [36],\n6\n(a)\n (b)\n (c)\n (d)\n (e)\n (f)\nFigure 7: Qualitative results on the TSS benchmark [ 36]: (a) source image, (b) target image, (c)\nDCTM [18], (d) SCNet [9], (e) GMat. w/Inl. [31], and (f) RTNs. The source images are warped to\nthe target images using correspondences.\nPF [7], SCNet [9], DCTM [18], geometric matching (GMat.) [30], and GMat. w/Inl. [31], as well as\nemploying the SIFT ﬂow optimizer1 together with UCN-ST [5], FCSS [18], and CAT-FCSS [20].\nPerformance was measured on the TSS dataset [ 36], PF-WILLOW dataset [7], and PF-PASCAL\ndataset [8]. In Sec. 5.2, we ﬁrst analyze the effects of the components within RTNs, and then evaluate\nmatching results with various benchmarks and quantitative measures in Sec. 5.3.\n5.2 Ablation Study\n1 2 3 4 5 6\n#Iteration\n0.4\n0.5\n0.6\n0.7\n0.8Average flow accuracy\nFigure 6: Convergence analysis of RTNs\nw/ResNet [12] for various numbers of\niterations and search window sizes on\nthe TSS benchmark [36].\nTo validate the components within RTNs, we evaluated\nthe matching accuracy for different numbers of itera-\ntions, with various window sizes of Ni, for different back-\nbone feature extraction networks such as VGGNet [ 35],\nCAT-FCSS [20], and ResNet [12], and with pretrained or\nlearned backbone networks. For quantitative assessment,\nwe examined the matching accuracy on the TSS bench-\nmark [36], as described in the following section. As shown\nin Fig. 6, RTNs w/ResNet [12] converge in 3−5 iterations.\nBy enlarging the window size of Ni, the matching accu-\nracy improves until 9×9 with longer training and testing\ntimes, but larger window sizes reduce matching accuracy\ndue to greater matching ambiguity. Note that Mi = Ni.\nTable 1 shows that among the many state-of-the-art fea-\nture extraction networks, ResNet [ 12] exhibits the best\nperformance for our approach. As shown in comparisons\nbetween pretrained and learned backbone networks, learn-\ning the feature extraction networks jointly with geometric matching networks can boost matching\naccuracy, as similarly seen in [31].\n5.3 Matching Results\nTSS Benchmark We evaluated RTNs on the TSS benchmark [36], which consists of 400 image\npairs divided into three groups: FG3DCar [ 24], JODS [ 33], and PASCAL [ 10]. As in [ 18; 19],\nﬂow accuracy was measured by computing the proportion of foreground pixels with an absolute\nﬂow endpoint error that is smaller than a threshold of T = 5, after resizing each image so that\nits larger dimension is 100 pixels. Table 1 compares the matching accuracy of RTNs to state-of-\nthe-art correspondence techniques, and Fig. 7 shows qualitative results. Compared to handcrafted\nmethods [25; 17; 36; 7], most CNN-based methods have better performance. In particular, methods\nthat use STN-based feature transformations, namely UCN-ST [5] and CAT-FCSS [20], show improved\nability to deal with geometric variations. In comparison to the geometric matching-based methods\nGMat. [30] and GMat. w/Inl. [30], RTNs consisting of feature extraction with ResNet and recurrent\n1For these experiments, we utilized the hierarchical dual-layer belief propagation of SIFT ﬂow [25] together\nwith alternative dense descriptors.\n7\nMethods Feature Regular. Superv. FG3D. JODS PASC. Avg.\nSF [25] SIFT SF - 0.632 0.509 0.360 0.500\nDSP [17] SIFT DSP - 0.487 0.465 0.382 0.445\nTaniai et al. [36] HOG TSS - 0.830 0.595 0.483 0.636\nPF [7] HOG LOM - 0.786 0.653 0.531 0.657\nDCTM [18] CAT-FCSS† DCTM - 0.891 0.721 0.610 0.740\nUCN-ST [5] UCN-ST SF Sup. 0.853 0.672 0.511 0.679\nFCSS [18; 20] FCSS SF Weak. 0.832 0.662 0.512 0.668\nCAT-FCSS SF Weak. 0.858 0.680 0.522 0.687\nSCNet [9] VGGNet AG Sup. 0.764 0.600 0.463 0.609\nVGGNet AG+ Sup. 0.776 0.608 0.474 0.619\nGMat. [30] VGGNet GMat. Self. 0.835 0.656 0.527 0.673\nResNet GMat. Self. 0.886 0.758 0.560 0.735\nGMat. w/Inl. [31] ResNet GMat. Weak. 0.892 0.758 0.562 0.737\nRTNs VGGNet† R-GMat. Weak. 0.875 0.736 0.586 0.732\nRTNs VGGNet R-GMat. Weak. 0.893 0.762 0.591 0.749\nRTNs CAT-FCSS R-GMat. Weak. 0.889 0.775 0.611 0.758\nRTNs ResNet R-GMat. Weak. 0.901 0.782 0.633 0.772\nTable 1: Matching accuracy compared to state-of-the-art correspondence techniques (with feature,\nregularization, and supervision) on the TSS benchmark [36]. †denotes a pre-trained feature.\n(a)\n (b)\n (c)\n (d)\n (e)\n (f)\nFigure 8: Qualitative results on the PF-WILLOW benchmark [7]: (a) source image, (b) target image,\n(c) UCN-ST [5], (d) SCNet [9], (e) GMat. w/Inl. [31], and (f) RTNs. The source images are warped\nto the target images using correspondences.\ngeometric matching modules provide higher performance. RTNs additionally outperform existing\nCNN-based methods trained with supervision of ﬂow ﬁelds. It should be noted that GMat. w/Inl. [31]\nwas learned with the initial network parameters set through self-supervised learning as in [30]. RTNs\ninstead start from fully-randomized parameters in geometric matching networks.\nPF-WILLOW Benchmark We also evaluated our method on the PF-WILLOW benchmark [7],\nwhich includes 10 object sub-classes with 10 keypoint annotations for each image. For the evaluation\nmetric, we use the probability of correct keypoint (PCK) between ﬂow-warped keypoints and the\nground truth [ 26; 7] as in the experiments of [ 18; 9; 20]. Table 2 compares the PCK values of\nRTNs to state-of-the-art correspondence techniques, and Fig. 8 shows qualitative results. Our RTNs\nexhibit performance competitive to the state-of-the-art correspondence techniques including the\nlatest weakly-supervised and even supervised methods for semantic correspondence. Since RTNs\nestimate locally-varying geometric ﬁelds, it provides more precise localization ability, as shown in the\nresults of α= 0.05, in comparison to existing geometric matching networks [30; 31] which estimate\nglobally-varying geometric ﬁelds only.\nPF-PASCAL Benchmark Lastly, we evaluated our method on the PF-PASCAL benchmark [8],\nwhich contains 1,351 image pairs over 20 object categories with PASCAL keypoint annotations [2].\nFollowing the split in [ 9; 31], we used 700 training pairs, 300 validation pairs, and 300 testing\npairs. For the evaluation metric, we use the PCK between ﬂow-warped keypoints and the ground\n8\nMethods PF-WILLOW [7] PF-PASCAL [8]\nα= 0.05 α= 0.1 α= 0.15 α= 0.05 α= 0.1 α= 0.15\nPF [7] 0.284 0.568 0.682 0.314 0.625 0.795\nDCTM [18] 0.381 0.610 0.721 0.342 0.696 0.802\nUCN-ST [5] 0.241 0.540 0.665 0.299 0.556 0.740\nCAT-FCSS [20] 0.362 0.546 0.692 0.336 0.689 0.792\nSCNet [9] 0.386 0.704 0.853 0.362 0.722 0.820\nGMat. [30] 0.369 0.692 0.778 0.410 0.695 0.804\nGMat. w/Inl. [31] 0.370 0.702 0.799 0.490 0.748 0.840\nRTNs w/VGGNet 0.402 0.707 0.842 0.506 0.743 0.836\nRTNs w/ResNet 0.413 0.719 0.862 0.552 0.759 0.852\nTable 2: Matching accuracy compared to state-of-the-art correspondence techniques on the PF-\nWILLOW benchmark [7] and PF-PASCAL benchmark [8].\n(a)\n (b)\n (c)\n (d)\n (e)\n (f)\nFigure 9: Qualitative results on the PF-PASCAL benchmark [8]: (a) source image, (b) target image,\n(c) CAT-FCSS w/SF [20], (d) SCNet [9], (e) GMat. w/Inl. [ 31], and (f) RTNs. The source images are\nwarped to the target images using correspondences.\ntruth as done in the experiments of [ 9]. Table 2 summarizes the PCK values, and Fig. 9 shows\nqualitative results. Similar to the experiments on the PF-WILLOW benchmark [ 7], CNN-based\nmethods [9; 30; 31] including our RTNs yield better performance, with RTNs providing the highest\nmatching accuracy.\n6 Conclusion\nWe presented RTNs, which learn to infer locally-varying geometric ﬁelds for semantic correspondence\nin an end-to-end and weakly-supervised fashion. The key idea of this approach is to utilize and\niteratively reﬁne the transformations and convolutional activations via geometric matching between\nthe input image pair. In addition, a technique is presented for weakly-supervised training of RTNs. A\ndirection for further study is to examine how the semantic correspondence of RTNs could beneﬁt\nsingle-image 3D reconstruction and instance-level object detection and segmentation.\nReferences\n[1] C. Barnes, E. Shechtman, D. B. Goldman, and A. Finkelstein. The generalized patchmatch\ncorrespondence algorithm. In: ECCV, 2010.\n9\n[2] L. Bourdev and J. Malik. Poselets: Body part detectors trained using 3d human pose annotations,.\nIn: ICCV, 2009.\n[3] H. Bristow, J. Valmadre, and S. Lucey. Dense semantic correspondence where every pixel is a\nclassiﬁer. In: ICCV, 2015.\n[4] D. Butler, J. Wulff, G. Stanley, and M. Black. A naturalistic open source movie for optical ﬂow\nevaluation. In: ECCV, 2012.\n[5] C. B. Choy, Y . Gwak, and S. Savarese. Universal correspondence network. In:NIPS, 2016.\n[6] Y . HaCohen, E. Shechtman, D. B. Goldman, and D. Lischinski. Non-rigid dense correspondence\nwith applications for image enhancement. In: SIGGRAPH, 2011.\n[7] B. Ham, M. Cho, C. Schmid, and J. Ponce. Proposal ﬂow. In: CVPR, 2016.\n[8] B. Ham, M. Cho, C. Schmid, and J. Ponce. Proposal ﬂow: Semantic correspondences from\nobject proposals. IEEE Trans. PAMI, 2017.\n[9] K. Han, R. S. Rezende, B. Ham, K. Wong, M. Cho, C. Schmid, and J. Ponce. Scnet: Learning\nsemantic correspondence. In: ICCV, 2017.\n[10] B. Hariharan, P. Arbelaez, L. Bourdev, S. Maji, and J. Malik. Semantic contours from inverse\ndetectors. In: ICCV, 2011.\n[11] T. Hassner, V . Mayzels, and L. Zelnik-Manor. On sifts and their scales. In: CVPR, 2012.\n[12] K. He, X. Zhang, S. Ren, and Sun. J. Deep residual learning for image recognition. In: CVPR,\n2016.\n[13] J. Hur, H. Lim, C. Park, and S. C. Ahn. Generalized deformable spatial pyramid: Geometry-\npreserving dense correspondence estimation. In: CVPR, 2015.\n[14] Philbin. J., O. Chum, M. Isard, J. Sivic, and A. Zisserman. Object retrieval with large vocabu-\nlaries and fast spatial matching. In: CVPR, 2007.\n[15] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu. Spatial transformer networks.\nIn: NIPS, 2015.\n[16] A. Kanazawa, D. W. Jacobs, and M. Chandraker. Warpnet: Weakly supervised matching for\nsingle-view reconstruction. In: CVPR, 2016.\n[17] J. Kim, C. Liu, F. Sha, and K. Grauman. Deformable spatial pyramid matching for fast dense\ncorrespondences. In: CVPR, 2013.\n[18] S. Kim, D. Min, B. Ham, S. Jeon, S. Lin, and K. Sohn. Fcss: Fully convolutional self-similarity\nfor dense semantic correspondence. In: CVPR, 2017.\n[19] S. Kim, D. Min, S. Lin, and K. Sohn. Dctm: Discrete-continuous transformation matching for\nsemantic ﬂow. In: ICCV, 2017.\n[20] S. Kim, D. Min, B. Ham, S. Lin, and K. Sohn. Fcss: Fully convolutional self-similarity for\ndense semantic correspondence. TPAMI, 2018.\n[21] S. Kim, D. Min, S. Lin, and K. Sohn. Discrete-continuous transformation matching for dense\nsemantic correspondence. IEEE Trans. PAMI, 2018.\n[22] J. Liao, Y . Yao, L. Yuan, G. Hua, and S. B. Kang. Visual attribute transfer through deep image\nanalogy. In: SIGGRAPH, 2017.\n[23] Chen-Hsuan Lin and Simon Lucey. Inverse compositional spatial transformer networks. CVPR,\n2017.\n[24] Y . L. Lin, V . I. Morariu, W. Hsu, and L. S. Davis. Jointly optimizing 3d model ﬁtting and\nﬁne-grained classiﬁcation. In: ECCV, 2014.\n10\n[25] C. Liu, J. Yuen, and A Torralba. Sift ﬂow: Dense correspondence across scenes and its\napplications. IEEE Trans. PAMI, 33(5):815–830, 2011.\n[26] J. Long, N. Zhang, and T. Darrell. Do convnets learn correspondence? In: NIPS, 2014.\n[27] D.G. Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 60(2):91–110,\n2004.\n[28] D. Novotny, D. Larlus, and A. Vedaldi. Anchornet: A weakly supervised network to learn\ngeometry-sensitive features for semantic matching. In:CVPR, 2017.\n[29] W. Qiu, X. Wang, X. Bai, A. Yuille, and Z. Tu. Scale-space sift ﬂow. In: WACV, 2014.\n[30] I. Rocco, R. Arandjelovic, and J. Sivic. Convolutional neural network architecture for geometric\nmatching. In:CVPR, 2017.\n[31] I. Rocco, R. Arandjelovi´c, and J. Sivic. End-to-end weakly-supervised semantic alignment. In:\nCVPR, 2018.\n[32] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image\nsegmentation. In: MICCAI, 2015.\n[33] M. Rubinstein, A. Joulin, J. Kopf, and C. Liu. Unsupervised joint object discovery and\nsegmentation in internet images. In: CVPR, 2013.\n[34] D. Scharstein and R. Szeliski. A taxonomy and evaluation of dense two-frame stereo correspon-\ndence algorithms. IJCV, 47(1):7–42, 2002.\n[35] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image\nrecognition. In: ICLR, 2015.\n[36] T. Taniai, S. N. Sinha, and Y . Sato. Joint recovery of dense correspondence and cosegmentation\nin two images. In: CVPR, 2016.\n[37] J. Thewlis1, H. Bilen, and A. Vedald. Unsupervised learning of object frames by dense\nequivariant image labelling. In: NIPS, 2017.\n[38] N. Ufer and B. Ommer. Deep semantic feature matching. In:CVPR, 2017.\n[39] F. Yang, X. Li, H. Cheng, J. Li, and L. Chen. Object-aware dense semantic correspondence.\nIn:CVPR, 2017.\n[40] H. Yang, W. Y . Lin, and J. Lu. Daisy ﬁlter ﬂow: A generalized discrete approach to dense\ncorrespondences. In: CVPR, 2014.\n[41] K. M. Yi, E. Trulls, V . Lepetit, and P. Fua. Lift: Learned invariant feature transform.In: ECCV,\n2016.\n[42] K. M. Yi, Y . Verdie, P. Fua, and V . Lepetit. Learning to assign orientations to feature points.In:\nCVPR, 2016.\n[43] F. Yu and V . Koltun. Multi-scale context aggregation by dilated convolutions.In: ICLR, 2016.\n[44] T. Zhou, Y . J. Lee, S. X. Yu, and A. A. Efros. Flowweb: Joint image set alignment by weaving\nconsistent, pixel-wise correspondences. In: CVPR, 2015.\n[45] T. Zhou, P. Krahenbuhl, M. Aubry, Q. Huang, and A. A. Efros. Learning dense correspondence\nvia 3d-guided cycle consistency. In: CVPR, 2016.\n11",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7159873247146606
    },
    {
      "name": "Artificial intelligence",
      "score": 0.623801052570343
    },
    {
      "name": "Computer science",
      "score": 0.5735442638397217
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.546111524105072
    },
    {
      "name": "Image (mathematics)",
      "score": 0.42420607805252075
    },
    {
      "name": "Iterative and incremental development",
      "score": 0.4136062264442444
    },
    {
      "name": "Computer vision",
      "score": 0.35826316475868225
    },
    {
      "name": "Engineering",
      "score": 0.0706261694431305
    },
    {
      "name": "Software engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}