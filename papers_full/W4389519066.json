{
    "title": "Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models",
    "url": "https://openalex.org/W4389519066",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2993632252",
            "name": "Orevaoghene Ahia",
            "affiliations": [
                "Allen Institute for Artificial Intelligence"
            ]
        },
        {
            "id": "https://openalex.org/A2181361850",
            "name": "Sachin Kumar",
            "affiliations": [
                "Allen Institute for Artificial Intelligence"
            ]
        },
        {
            "id": "https://openalex.org/A2671828835",
            "name": "Hila Gonen",
            "affiliations": [
                "Allen Institute for Artificial Intelligence"
            ]
        },
        {
            "id": "https://openalex.org/A2760024017",
            "name": "Jungo Kasai",
            "affiliations": [
                "Allen Institute for Artificial Intelligence"
            ]
        },
        {
            "id": "https://openalex.org/A2102413935",
            "name": "David A. Mortensen",
            "affiliations": [
                "Allen Institute for Artificial Intelligence"
            ]
        },
        {
            "id": "https://openalex.org/A2166589550",
            "name": "Noah Smith",
            "affiliations": [
                "Allen Institute for Artificial Intelligence"
            ]
        },
        {
            "id": "https://openalex.org/A2234266251",
            "name": "Yulia Tsvetkov",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4307225507",
        "https://openalex.org/W3204712960",
        "https://openalex.org/W3105190698",
        "https://openalex.org/W3101140821",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W3120929527",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W4385571124",
        "https://openalex.org/W3174418826",
        "https://openalex.org/W4362511131",
        "https://openalex.org/W3169369929",
        "https://openalex.org/W2989539713",
        "https://openalex.org/W4225321655",
        "https://openalex.org/W2550821151",
        "https://openalex.org/W4225930230",
        "https://openalex.org/W3175212568",
        "https://openalex.org/W3173954987",
        "https://openalex.org/W4385571788",
        "https://openalex.org/W3037575273",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W4225619898",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4229005866",
        "https://openalex.org/W3175864309",
        "https://openalex.org/W4386566790",
        "https://openalex.org/W4287019748",
        "https://openalex.org/W4287993739",
        "https://openalex.org/W4285267582",
        "https://openalex.org/W3169244955",
        "https://openalex.org/W4297633153",
        "https://openalex.org/W3173360659",
        "https://openalex.org/W4283313765",
        "https://openalex.org/W2325864482",
        "https://openalex.org/W4378498597",
        "https://openalex.org/W4311642023",
        "https://openalex.org/W4285077564",
        "https://openalex.org/W4285199616",
        "https://openalex.org/W3035207248",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W2963979492",
        "https://openalex.org/W657922907",
        "https://openalex.org/W3174685870",
        "https://openalex.org/W4366733439",
        "https://openalex.org/W4287365906",
        "https://openalex.org/W4225973328",
        "https://openalex.org/W4226271314",
        "https://openalex.org/W4389519817",
        "https://openalex.org/W3034469191",
        "https://openalex.org/W4389523957",
        "https://openalex.org/W4385573090",
        "https://openalex.org/W3137010024",
        "https://openalex.org/W4285225959",
        "https://openalex.org/W4389520538",
        "https://openalex.org/W4366850543",
        "https://openalex.org/W3034515982",
        "https://openalex.org/W3035296331",
        "https://openalex.org/W3191132518",
        "https://openalex.org/W4389524534",
        "https://openalex.org/W4404783771",
        "https://openalex.org/W4206660322",
        "https://openalex.org/W2891555348"
    ],
    "abstract": "Language models have graduated from being research prototypes to commercialized products offered as web APIs, and recent works have highlighted the multilingual capabilities of these products. The API vendors charge their users based on usage, more specifically on the number of “tokens” processed or generated by the underlying language models. What constitutes a token, however, is training data and model dependent with a large variance in the number of tokens required to convey the same information in different languages. In this work, we analyze the effect of this non-uniformity on the fairness of an API’s pricing policy across languages. We conduct a systematic analysis of the cost and utility of OpenAI’s language model API on multilingual benchmarks in 22 typologically diverse languages. We show evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results. These speakers tend to also come from regions where the APIs are less affordable, to begin with. Through these analyses, we aim to increase transparency around language model APIs’ pricing policies and encourage the vendors to make them more equitable.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 9904–9923\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nDo All Languages Cost the Same?\nTokenization in the Era of Commercial Language Models\nOrevaoghene Ahia♢ Sachin Kumar♠♡ Hila Gonen♢ Jungo Kasai♢\nDavid R. Mortensen♠ Noah A. Smith♢♡ Yulia Tsvetkov♢\n♢Paul G. Allen School of Computer Science & Engineering, University of Washington\n♠Language Technologies Institute, Carnegie Mellon University\n♡Allen Institute for Artificial Intelligence\n{oahia,jkasai,nasmith,yuliats}@cs.washington.edu, sachink@allenai.org\nhilagnn@gmail.com, dmortens@cs.cmu.edu\nAbstract\nLanguage models have graduated from being\nresearch prototypes to commercialized prod-\nucts offered as web APIs, and recent works\nhave highlighted the multilingual capabilities\nof these products. The API vendors charge\ntheir users based on usage, more specifically\non the number of “tokens” processed or gener-\nated by the underlying language models. What\nconstitutes a token, however, is training data\nand model dependent with a large variance in\nthe number of tokens required to convey the\nsame information in different languages. In\nthis work, we analyze the effect of this non-\nuniformity on the fairness of an API’s pricing\npolicy across languages. We conduct a system-\natic analysis of the cost and utility of OpenAI’s\nlanguage model API on multilingual bench-\nmarks in 22 typologically diverse languages.\nWe show evidence that speakers of a large num-\nber of the supported languages are overcharged\nwhile obtaining poorer results. These speak-\ners tend to also come from regions where the\nAPIs are less affordable to begin with. Through\nthese analyses, we aim to increase transparency\naround language model APIs’ pricing policies\nand encourage the vendors to make them more\nequitable.\n1 Introduction\nLanguage models (LMs) have come to be known\nas general-purpose solutions capable of perform-\ning many tasks by following natural language\ninstructions (Brown et al., 2020; Ouyang et al.,\n2022; Chung et al., 2022), and generalizing to new\ntasks at test time using a handful of demonstra-\ntions (Brown et al., 2020; Su et al., 2023). Moti-\nvated by their potential for commercial use, many\nindustrial research institutions have moved away\n1OpenAI’s tokenizer interface displays byte tokens absent\nfrom their vocabulary as \"?\".\n��οια ����ναι �� \n��ρωτε��ο��σα το�� \n��αρ��κο��;?\nWhat is the \ncapital of \nMorocco?\nΠοια είναι η \nπρωτεύουσα \nτου Μαρόκου;?\nWhat is the \ncapital of \nMorocco?\nΗ πρωτεύουσα του \nΜαρόκου είναι η πόλη \nΡαμπάτ, ή επίσης \nγνωστή ως Ραμπάτ Σαλέ.\nThe capital of \nMorocco is Rabat.\n$ $ $ $\nFigure 1: We investigate the effects of subword tok-\nenization in LMs across languages with different writ-\ning systems. Our findings highlight disparities in the\nutility of LMs, as well as socio-economic disparities and\nincreased costs in using commercial APIs for speakers\nof underrepresented languages.1\nfrom openly releasing them (Abdalla et al., 2023).\nInstead, a new business model of LM as a Ser-\nvice (Sun et al., 2022) has emerged where LMs\ncan be accessed for inference using (paid) web\nAPIs. The majority of these models (Ouyang et al.,\n2022) offer multilingual capabilities, and the API\nproviders charge the users proportionally to the\nnumber of tokens processed or generated.\nIn this work, we examine the fairness of this\npricing model for different languages, based on\nhow a “token” is defined in practice.2 Most LMs\nrely on tokenizers that split text strings into chunks\n(subwords). Subword tokenizers (Sennrich et al.,\n2016; Kudo, 2018; Song et al., 2020) are typically\ndata-driven and learn to split text based on fre-\nquency patterns of characters or bytes in some\ncorpus. Prior work argued that, in multilingual\nsettings, subword tokenizers lead to disproportion-\n2Code available at https://github.com/orevaahia/l\nlm_tokenizer_cost\n9904\nate fragmentation rates for different languages and\nwriting scripts (Zhang et al., 2022a; Rust et al.,\n2021; Muller et al., 2021). Many commercial LMs\nare multilingual, and text from languages that suffer\nfrom excessive fragmentation will be represented\nusing more tokens. This directly increases cost of\nAPI usage for certain language speakers, even if\nthey convey the same information as the others.\nWe highlight this unfairness through three stages\nof systematic analyses. First, we show evi-\ndence that tokenizers of popular LMs indeed over-\nfragment texts in certain language scripts and quan-\ntify the API cost disparity that this issue causes.\nWe discover that the disparity is not caused just by\ndata imbalance, but is rooted in the language prop-\nerties or the ways they are represented in Unicode.\nSecond, we show that languages with longer token\nlengths as a result of greater fragmentation derive\nless model utility with in-context learning (Brown\net al., 2020). Finally, we find that languages that\ncost more and perform worse are often associated\nwith populations of speakers for whom the APIs\nare less affordable on average, exacerbating the\neconomic divide in the accessibility of NLP tech-\nnology.\nThrough these analyses, we argue that commer-\ncial LM API vendors should revisit their processing\nand pricing strategies to be more equitable. In ad-\ndition, we encourage the NLP community to pay\nbetter attention to tokenizers, an often neglected\npart of the LM pipeline.\n2 Do All Languages Cost the Same?\n2.1 Background\nLanguage Model APIs Autoregressive LMs are\ntrained to predict the next “token” given a previ-\nous context. Following the success of such models,\nmany commercial LM web APIs have emerged\nand allow users to interface with the models using\nnatural language instructions to perform various\ntasks with little to no exposure to the underlying\nworkings of the models. The API providers of-\nten support dozens of languages and charge users3\nat a fixed rate based on the total number of input\nand generated tokens.4 What constitutes a “token,”\nhowever, is not a universally accepted definition\nbut a design choice that the model developers make.\n3While most services also have free tiers, they limit daily\nusage to a small number of tokens.\n4E.g. see OpenAI models’ cost: https://openai.com/p\nricing.\nThe total token count is also not immediately obvi-\nous to users except through a tokenizer interface5\nseparate from the chat interface.\nTokenization in LMs Tokenization—\nsegmenting text into atomic units—is an\nactive research area. Proposed approaches range\nfrom defining tokens as whitespace-delimited\nwords (for languages that use whitespace) which\nmakes the vocabulary extremely large, to defining\ntokens as characters or bytes, making the tokenized\nsequences extremely long in terms of number of\ntokens; see Mielke et al. (2021) for a detailed\nsurvey. A commonly-used solution now is to\ntokenize text into subword chunks. With Sennrich\net al. (2016), one starts with a base vocabulary of\nonly characters adding new vocabulary items by\nrecursively merging existing ones based on their\nfrequency statistics in the data. Other approaches\njudge subword candidates to be included in the\nvocabulary using an LM (Kudo, 2018; Song et al.,\n2021). For multilingual models containing data in\na variety of scripts, even the base vocabulary of\nonly characters (based on Unicode symbols) can\nbe very large with over 130K types. Radford et al.\n(2019) instead proposed using a byte-level base\nvocabulary with only 256 tokens. Termed byte-\nlevel byte pair encoding (BBPE), this approach\nhas become a de facto standard used in most\nmodern language modeling efforts (Brown et al.,\n2020; Muennighoff et al., 2022; Scao et al., 2022;\nBlack et al., 2022; Rae et al., 2022; Zhang et al.,\n2022b). In this work, we investigate the impact\nthis tokenization strategy has on LM API cost\ndisparity as well as downstream task performance\n(i.e., utility) across different languages.\n2.2 Investigating the Impact of Byte-level\nSubword Segmentation\nThere are hundreds of distinct writing systems\nin the world (Hockett, 1997). BBPE, by design,\nmakes vocabulary construction script-agnostic, al-\nlowing (in principle) new scripts to be supported\nlater on without modifying the vocabulary. How-\never, not only are different scripts encoded dif-\nferently, their distribution in the training corpora\nvaries widely. To investigate the effects of this vari-\nation, we propose the following research questions\nas the main focus of this work.\n5https://platform.openai.com/tokenizer\n9905\nRQ1 (number of tokens): do all languages con-\nvey the same information with the same num-\nber of tokens? We analyze the fragmentation\nof sequences in different languages with different\ntokenizers. We find that among the supported lan-\nguages in popular LMs, there is a large variance in\nthe average number of tokens required to convey\nthe same information with some languages requir-\ning 5 times as many tokens than others. Previous\nwork has shown that tokenization in multilingual\nmodels is usually biased towards high-resourced\nlanguages in the pretraining data (Ács, 2019; Rust\net al., 2021); we observe that this is not always the\ncase, but it could also be dependent on linguistic\nfeatures or properties of language scripts.\nRQ2 (cost): do non-uniform tokenization rates\nlead to LM API cost disparity for speakers of dif-\nferent languages? LM APIs like ChatGPT are\navailable worldwide and have been widely claimed\nto have multilingual capabilities (Kasai et al., 2023;\nLai et al., 2023).6 We show that disparate fragmen-\ntation rates across languages lead to significantly\nhigh usage costs for less represented languages, and\nwe argue for a more equitable API pricing system.\nRQ3 (model utility): do non-uniform tokeniza-\ntion rates affect the models’ utility? LMs have\nexhibited in-context learning capabilities, perform-\ning new tasks with few demonstrations as input\n(without parameter finetuning). This is highly de-\nsirable in any LM API as it avoids computational,\nannotation (and financial) costs. We show that\nhigh fragmentation rate of a language negatively\naffects the in-context learning performance in that\nlanguage, resulting in reduced model utility.\nRQ4 (socio-economic aspects): what are the\nsocio-economic implications of the API’s cross-\nlingual cost and performance disparity? Our\nanalysis shows evidence that not only are LMs\nmore expensive for certain languages, they are also\nless effective for them. To highlight the implica-\ntions of these findings, we correlate those measure-\nments with the socio-economic indicators of lan-\nguage speakers as a proxy for affordability of the\nAPIs. This analysis indicates that users who likely\ncannot afford high API costs are charged more for\npoorer service, hindering uniform accessibility.\n6https://help.openai.com/en/articles/674236\n9-how-do-i-use-the-openai-api-in-different-lan\nguages\n3 Experimental Setup\n3.1 Models\nThroughout this work, we focus on two LMs: Chat-\nGPT (Ouyang et al., 2022; Brown et al., 2020)\n(gpt-3.5-turbo) and BLOOMZ (Muennighoff et al.,\n2022). Both of these models are trained and ad-\nvertised as general-purpose models capable of fol-\nlowing instructions and performing a wide range\nof tasks (Qin et al., 2023; Zhu et al., 2023; Ahuja\net al., 2023; Huang et al., 2023).\nChatGPT (Ouyang et al., 2022) is a closed model\nonly accessible through an API (with a premium\ntier) provided by OpenAI. Studies report that it\nsupports as many as 90 languages (Ahuja et al.,\n2023). ChatGPT can handle a maximum sequence\nlength of 4096 tokens (including both the prompt\nand generated tokens).\nBLOOMZ (Muennighoff et al., 2022) is an open-\nsource multilingual model trained on 46 natural\nlanguages and 13 programming languages. While\ntraining its tokenizer, sentences from different lan-\nguages were sampled according to a multinomial\ndistribution (Conneau et al., 2020), thereby in-\ncreasing the number of tokens associated with low-\nresource languages. The best-performing version\nof this model has 175B parameters and is not fea-\nsible to be loaded on our academic servers; hence\nwe rely on a free API of BLOOMZ hosted by Hug-\ngingface.7 Although BLOOMZ was trained with\nALiBi positional embeddings (Press et al., 2022)\nwhich allows the model to extrapolate to any length\nsequences during inference, the Huggingface API\nhas a context limit of 1000 tokens.\n3.2 Tasks and Datasets\nTo answer RQ1—whether the same information is\nconveyed with similar numbers of tokens in differ-\nent languages—we use a validation set of FLORES-\n200 (Goyal et al., 2022), a multilingual parallel cor-\npus containing examples in over 200 languages.8\nWe tokenize each sentence in the FLORES-200\nsubset with ChatGPT’s tokenizer9 and compute the\naverage number of tokens per sentence for each\nlanguage. Using parallel data controls for the same\ninformation across languages. We consider that\n7https://huggingface.co/docs/api-inference/q\nuicktour.\n8We also experimented with WMT 2021 data (Akhbardeh\net al., 2021) and found similar results. Note that the WMT\ndata are focused on European languages.\n9ChatGPT’s tokenizer https://github.com/openai/ti\nktoken\n9906\nlanguage A is more efficiently tokenized than lan-\nguage B if it uses fewer tokens per sentence on\naverage. While previous studies have computed\nfragmentation rates with fertility (Ács, 2019), we\ninstead define it as the average number of tokens\nin a sequence for two reasons. First, our goal is\nto compare LLM API costs across languages that\ncharge users based on the number of tokens. To\ncontrol for content, we use a parallel corpus for\nthis analysis. Second, many languages we analyze\nare understudied and do not have word tokenizers\navailable which are required to compute fertility.\nFor RQ2 and RQ3, to clearly highlight the cost\nand utility disparities, we evaluate the models on\nNLP tasks that involve long-form texts either at\ninput or output. We evaluate the models on di-\nverse, challenging natural language generation and\nclassification tasks on the following benchmarks:\nClassification We evaluate on (1) XNLI (Con-\nneau et al., 2018): a cross-lingual inference\nbenchmark comprising of 11 typologically diverse\nlanguages. It involves two sub-tasks, passage\nselection and minimum answer span (Gold-P).\nWe focus on the latter task in our experiments.\n(2) XFACT (Gupta and Srikumar, 2021): a multi-\nlingual fact verification dataset of naturally existing\nreal-world claims covering 25 languages.\nSpan Prediction We use XQUAD (Artetxe et al.,\n2019): a crosslingual question-answering dataset\nwhere each example consists of a paragraph, a ques-\ntion, and the answer as a span in the paragraph.\nGeneration We evaluate on (1) Cross\nSum (Hasan et al., 2021a): a cross-lingual\nabstractive summarization dataset comprising\n1.7 million article-summary samples in 1500+\nlanguage pairs, and, (2) XLSUM (Hasan et al.,\n2021b): a summarization dataset covering 44\ndiverse languages. It comprises news articles and\nsummaries in the same language as the article.\n3.3 Prompting Formulation\nWe evaluate both models in a k-shot in-context\nlearning setup where we also provide task instruc-\ntions. We experiment with 0 ≤ k ≤X, where X\nis the maximum number of in-context examples\nthat can be provided. Note that X is not a fixed\nvalue, but is determined by the LM API’s limit on\nthe number of input tokens and the fragmentation\nrate of the language.\nLatin\nJapanese\nHangul\nCyrillic\nArabic\nThai\nHebrew\nDevanagari\nGreek\nBengali\nTamil\nTelugu\nGeorgian\nTibetan\nLanguage Script\n0\n100\n200\n300Average number of tokens\n122 1 1\n12 21\n1 2 8 1\n3\n1\n1\n1\n2\nFigure 2: Average number of tokens by script after\ntokenizing the Flores dataset. The fragmentation rate\nis lower for Latin script languages and higher for other\nscripts. Number of languages per language group is\nindicated at the top of each bar.\nFor all tasks, we provide the instructions in En-\nglish following Ahuja et al. (2023), who show that\non several multilingual benchmarks, English in-\nstructions outperform the in-language prompts (see\nTable 2 in the Appendix for the prompting format\nfor all tasks). For each task, we randomly sample\nat most 500 test examples for evaluation.\n4 Results and Analysis\n4.1 RQ1 (number of tokens): do all languages\nconvey the same information with the\nsame number of tokens?\nIn Figure 2 we show that Latin-script languages are\nrepresented with substantially fewer tokens com-\npared to languages in other scripts. While Cyrillic\nand Japanese script languages come close to the\nLatin, languages with their own script (e.g., Telugu)\nrequire up to 5×more tokens to convey the same\ninformation. We hypothesize that this disparity is\ndue to training data imbalance since ChatGPT’s\ntokenizer was primarily trained on Latin-script lan-\nguages, mainly English. The training details of\nChatGPT are not available. However, we make\na reasonable assumption that its training data has\na similar proportion of languages as the publicly\navailable large corpus CC100 (Wenzek et al., 2020).\nIf we sort languages shown in Figure 2 based on\ntheir data size in CC100 (see Figure 14 in the Ap-\npendix), low-resourced languages of Latin script\nappear to be less fragmented compared to other\nmid-resourced languages of non-Latin scripts.\nIn Figure 15 in the Appendix, we present a sim-\nilar analysis for BLOOMZ’s tokenizer. We sort\n9907\nthe languages based on their size in the pretraining\ndata (ROOTS corpus; Laurençon et al., 2023). We\nobserve that languages with fewer resources gen-\nerally have a higher average token length. Arabic\nis an outlier here as it appears to have more tokens\nthan some other mid-resourced languages.\nWhat influences the non-uniformity of a to-\nkenizer across languages? From our analysis\nabove, we identify two influential factors: (1) the\nproportion of the language in the pretraining data,\nand (2) inherent properties of the language and its\nwriting script. While we see some correlation be-\ntween pretraining data size and fragmentation rate\nin BLOOMZ , with ChatGPT it is quite different\nas higher-resourced non-Latin script languages still\nget excessively tokenized.\nTo disentangle the effects of factors (1) and (2)\nwe train BBPE tokenizers on a variety of languages\nwith diverse scripts with vocabulary sizes ranging\nfrom 5,000 to 50,000, while controlling for content\nand data size. Specifically, we train the tokenizers\non parallel corpora and include one language per\nscript. We then use these tokenizers to tokenize the\ntext they were trained on, and compute the average\nnumber of tokens per sentence.\n200 500 1000 5000 10000 25000 50000\nVocab size\n50\n100\n150\n200\n250\n300\n350\n400Avg number of subwords\nMyanmar\nKhmer\nTibetan\nHangul\nLatin\nCyrillic\nJapanese\nArabic\nFigure 3: BBPE tokenizer trained on parallel text from\ndifferent language scripts with varying vocabulary sizes.\nWe display a larger version with 21 more scripts in\nFigure 19 in the Appendix.\nAs shown in Figure 3, even when controlling for\nthe content, there is still a disparity in the tokeniza-\ntion rate at different vocabulary sizes. In particular,\nmost scripts are very sensitive to small vocabulary\nsizes compared to Latin and Hangul scripts. We do\nnot achieve uniform fragmentation rate across all\nlanguage scripts even with large vocabulary sizes.\nWe therefore conclude that uniformity of BBPE\ntokenizers across languages is not just determined\nby the proportion of text from language in the pre-\ntraining data but also by language/script properties.\n4.2 RQ2 (cost): how do non-uniform\ntokenization rates affect LM API costs for\ndifferent languages?\nLM APIs charge users a fixed amount for a given\nnumber of input and generated tokens. Since the\nsame information is expressed using different num-\nber of tokens in different languages, we aim to\ninvestigate the disparity in what users pay to use\nthe API for different languages. From the results\nof our analysis in §4.1, we compute the estimated\ncost of API use per language as a function of the\naverage sequence length derived in Figure 2. We\nreport this on a subset of languages in Figure 16\nin the Appendix and present a granular analysis of\nlanguages that share family and script in Figure 4.\nLanguages that are more heavily segmented have\npredictably higher costs of usage. Overall, we\nsee that the API costs are biased towards (i.e.,\ncheaper for) Indo-European and Latin script lan-\nguages and against many non-Latin script lan-\nguages. In most mid-resourced Indic languages\nwith non-Latin scripts, we see close to a 5 ×in-\ncrease in cost compared to English.\n(IE, Latin)\n(ST, Latin)\n(AA, Latin)\n(AC, Latin)\n(AA, Arabic)\n(IE, Cyrillic)\n(AA, Hebrew)\n(IE, Arabic)\n(IE, Devanagari)\n(IE, Greek)\n(IE, Hebrew)\n(IE, Bengali)\n(ST, Bengali)\n(DR, Tamil)\n(DR, Telugu)\n(KA, Georgian)\n(ST, Tibetan)\nLanguage Family and Script\n1\n4\n8\n12Cost relative to English\nFigure 4: Estimated cost per language family/script, rel-\native to English. The language families are abbreviated\nas follows: IE: Indo-European, ST: Sino-Tibetan, AC:\nAtlantic-Congo, AA: Afro-Asiatic, DR: Dravidian, KA:\nKartvelian.\nNext, we report the costs of running experiments\nrelative to English. We report costs based on our\nzero-shot experiments across all tasks listed in §3.2.\nThis is due to excessive tokenization in some lan-\nguages for which we can only do zero-shot evalua-\ntions. For XLSUM, we show in Figure 5 that we\nspend up to 4×more for both prompting and gen-\neration in Telugu and Amharic. We observe similar\nfindings in XFACT and CROSSUM, as displayed\nin Figure 11 in the Appendix.\nWhile the majority of the commercial LMs are\nperhaps being optimized to perform well in many\n9908\nen fr pt sw es vi ar ko ru ja hi th te am\nLanguage\n0\n1\n2\n3\n4Experiment Cost relative to English\nFigure 5: Average cost of prompt + generated tokens\nfor XLSUM evaluations relative to English.\nlanguages, we show that there is less focus on indi-\nvidual experiences of speakers of languages other\nthan English. While LMs like ChatGPT might per-\nform tasks in Telugu, for example, a user in Andhra\nPradesh might pay 5×more than an English user\nin the US for an equivalent use of the model.\n4.3 RQ3 – Model utility: do non-uniform\ntokenization rates affect the models’\nutility?\n0 2 4 6 8 10\nIn-context examples\n0%\n20%\n40%\n60%\n80%\n100%% test data that doesn’t ﬁt the context length\nArabic\nJapanese\nFrench\nKorean\nTelugu\nSwahili\nSpanish\nRussian\nEnglish\nAmharic\nVietnamese\nThai\nHindi\nPortuguese\nFigure 6: Percentage of test examples per language\nin XLSUM that do not successfully fit into the context\nlength of ChatGPT. We can fit more few-shot examples\nin Latin script languages than in other languages.\nLMs typically have an upper bound of the num-\nber of tokens they can handle, e.g., ChatGPT can\nprocess a maximum of 4,096 tokens. Hence, due to\nnon-uniform fragmentation rates across languages,\nthere is a disparity in the amount of information\nthe models can process per language. In Figure 6\nwe plot the percentage of XLSUM test instances\nagainst the maximum number of in-context exam-\nples those instances can be accompanied with. For\nexample, Telugu struggles to fit even one in-context\nexample for the majority of the test set. Hence, the\nmodel can only do zero-shot prompting in this case.\nTo measure the impact of this issue on task per-\nformance, we evaluate ChatGPT and BLOOMZ\nwith a k-shot learning setup on the 5 tasks on di-\nverse languages as described in §3.2. Figure 7\nshows ChatGPT’s performance according to stan-\ndard automatic metrics of all tasks. Note that the\nfocus of this experiment is to illustrate the impact of\ntokenization in in-context learning settings. There-\nfore, we are interested not in the absolute value\nof the metrics or comparisons among languages\nbut the relative improvement within the test sets of\nthe same language as we increase the number of in-\ncontext examples. For all tasks and most languages,\nwe see consistent performance improvements as we\nincrease the number of in-context examples, from\nzero-shot to k (even for k = 1). For many lan-\nguages such as Telugu and Thai, due to their high\nfragmentation rates, we were unable to fit even\none complete demonstration and hence, only report\nzero-shot results. Based on trends from other lan-\nguages, we suspect that these languages could also\nhave benefitted from more demonstrations. Hence,\nas a result of unfair tokenization, ChatGPT’s utility\nis much lower for speakers of those languages com-\npared to better represented languages like English.\nFigure 8 reports the results of the same experi-\nment for BLOOMZ. Across all tasks we find that\nadding in-context examples does not help. In fact,\nin some cases, there is a performance drop even\nwith one in-context example. Upon manual inspec-\ntion of the generated outputs from the one-shot\nexperiments, the model has a tendency to copy\nspans from the in-context example, presenting that\nas output and thus not successfully utilize demon-\nstrations. Our hypothesis here is that BLOOMZ is\nbetter optimized for zero-shot prompting and is not\nas suitable for in-context learning.\nDue to the limited number of tokens that\nBLOOMZ’s inference API accepts, some examples\nin some languages cannot fit the 1000 token con-\ntext length when doing zero-shot prompting. We\nexperienced this with the XLSUM dataset as we\ncouldn’t fully fit news articles for some languages.\nUnderstandably, some of these languages are not\neven present in its pretraining data, and hence we\ndo not expect them to be tokenized efficiently. For\nthese examples that do not fit the context length,\nwe feed in truncated news articles into the model.\nWe therefore evaluate the generations for the frac-\ntion of examples that fit context and ones that do\nnot fit the context separately. Figure 9 shows the\nperformance comparison when we use truncated\nsummaries in the prompt and when we use the full\narticles. While the performance drop is expected,\n9909\n0 1 2 3 5\nNumber of in-context examples\n10\n20\n30Rouge-L\nar\nja\nfr\nko\nte\nsw\nes\nru\nen\nam\nvi\nth\nhi\npt\n(a) XLSUM\n0 1 2 3 5 8 10\nNumber of in-context examples\n10\n20\n30F1-Score\npt\nit\nhi\npl\nar\nde\nka\nid\nta\nro\ntr\nes (b) XFACT\n0 1 2 3 5\nNumber of in-context examples\n10\n20\n30Rouge-L\nta\nam\nhi\nar\npt\nja\nfr\nko\nte\nsw\nes\nru\nen\nid\nvi (c) CROSS-SUM\nFigure 7: Results from ChatGPT few-shot evaluations. In most tasks, we see an increase in performance as we\nincrease the number of in-context examples.\n310 5 8 10 152\nNumber of in-context examples\n20\n30\n40\n50F1-Score\nar\nbg\nde\nel\nen\nes\nfr\nhi\nru\nsw\nth\ntr\nur\nvi\nzh\n(a) XNLI\n0 1\nNumber of in-context examples\n5\n15\n25\n35Rouge-L\nta\nid\nhi\nar\npt\nja\nfr\nko\nte\nsw\nes\nru\nen\nam\nvi\nth\nne\nmr\nky\nuk\nur (b) XLSUM\n10 2 3\nNumber of in-context examples\n20\n30\n40\n50\n60\n70Exact Match\nar\nde\nzh\nvi\nen\nes\nhi\nel\nth\ntr\nru\nro (c) XQUAD\nFigure 8: Results from BLOOMz few-shot evaluations. The BLOOMz model is clearly better at zero-shot prompting\nthan few-shot.\nta id hi ar pt ja fr ko te sw es ru enam vi th ne mr ky uk ur\nLanguages\n10\n20\n30\n40Rouge-L\ntruncated article\nfull article\nFigure 9: Zero-shot evaluation of BLOOMz on XL-\nSUM. Since we cannot fit the full article in the context\nlength for some languages, we compare results on eval-\nuating full articles vs. truncated articles.\nour focus here is to highlight a consequence of\ndifferentiated tokenization in LMs.\n4.4 RQ4 – Socio-economic aspects: what are\nthe socio-economic implications of the\nAPI’s cross-lingual cost and performance\ndisparity?\nIn Figure 10, we plot the fragmentation rate per\nlanguage against the Human Development Index\nin the country with the highest absolute number\nof speakers of that language. We find a strong\nnegative correlation close to -0.5 showing that in\nmost cases, the lower the HDI index, the higher\nthe fragmentation rate and vice versa. Evidently,\nthe model’s vocabulary is biased towards users of\nmore developed countries.\nTask Cost-HDI HDI-Utility Cost-UtilitySpearman Pearson Spearman Pearson Spearman PearsonXFACT **–0.41 **–0.60 *0.34 **0.38 **–0.61 **–0.55XLSUM **–0.42 **–0.43 **–0.44 **–0.57 *–0.23 *0.21CROSS SUM **–0.41 **–0.45 *–0.18 *0.24 *0.27 *–0.17\nTable 1: Correlation between model utility, cost of API\naccess and Human Development Index (HDI) for each\ntask. We mark correlations with p < 0.05 with * and\nalso mark correlations with p < 0.0056 (according to\nBonferroni correction for multiple hypotheses) with **.\nThis bias is further validated by results shown in\nTable 1, where we mostly find negative correlations\nbetween pairs of each of the following variables:\naverage financial cost of experiments, model utility\n(performance), and human development index of\nthe country in which each language is spoken. We\nterm this “double unfairness” as people from less\neconomically developed countries are overcharged\nat a fixed rate per-token due to excessive tokeniza-\ntion, but often derive less utility from the model.\n9910\n0.4 0.5 0.6 0.7 0.8 0.9\nHuman Development Index\n50\n100\n150\n200\n250\n300Average number of tokens\nAssamese\nBelarusian\nBengali\nBosnian Catalan German\nGreek\nEnglish\nEstonianFinnishFrenchGalician\nGujarati\nHebrew\nHindi\nHungarian\nArmenian\nJapanese\nKannada\nGeorgian\nKazakh\nKhmer\nKyrgyz\nMalayalam\nMarathi\nMacedonian\nBurmese\nEastern Panjabi\nPortuguese\nRussian\nSinhala\nSomali\nSpanishSundanese\nTamil\nTelugu\nTagalog\nThai\nUyghur\nUkrainian\nUrdu\nVietnamese\nFigure 10: Fragmentation rate per language against the\nHuman Development Index in the top country where\nthe language is spoken.\n5 What is the Way Forward?\nTransparency in API limitations While the\nNLP community is aware of many of the issues we\npoint out in this work, LM APIs are advertised to a\ngeneral audience. Much like the policy of adding\nlimitations to research papers, LM API providers\nought to be more transparent about the flaws and\nbiases in their models, especially when describing\ntheir multilingual capabilities. Many users are not\nprivy to the inner workings of these models and\nwill be unknowingly charged higher prices if they\nuse the model in their native languages.\nRethinking the API pricing models Higher API\ncosts for languages in underprivileged communi-\nties risks excluding many populations from using\nlanguage technologies. A potential solution is to de-\nvelop pricing policies based on languages/regions\nwhile also accounting for model performance on\nlanguage-specific benchmarks. An alternative is\nto not charge by tokens at all. PaLM 2 API, for\nexample, charges the users based on characters.10\nHence, further analysis is needed to assess the fair-\nness of character-based pricing. Huggingface also\noffers an inference service for their enterprise cus-\ntomers11 relying on AWS instances and charging\nthem at an hourly rate. Future work may compare\nthis with a per-token rate we study in this work.\nOpen-source models vs. paid APIs Given issues\nwith paid APIs, the natural next question might be:\nshould the API users move to open-source mod-\nels or train their own? In fact, in our experiments,\nwe find BLOOMZ, an open-source model, to per-\nform better in the zero-shot setting than ChatGPT\n10Prior work has shown evidence that even the number of\ncharacters used to express the same information in different\nlanguages is\n11https://huggingface.co/pricing#endpoints\nperforms in the few-shot setting, in most cases.\nHowever, first, most open-source models are dis-\ntributed under an academic license whereas most\ndevelopers are interested in integrating these tech-\nnologies into their products for commercial use,\nwhich may incur licensing costs. Second, barring\nlicensing issues, LMs tend to be large and resource-\nintensive to train and deploy and require dedicated\nexpensive hardware to run at a commercial scale,\nwhich again might not be possible for most devel-\nopers and users, even exceeding the cost of using\nthe APIs. Research on reducing such hardware re-\nquirements (Dettmers et al., 2022; Park et al., 2023)\ncould increase accessibility. Still, this requires a\nconsiderable level of technical expertise from de-\nvelopers and users which might be infeasible.\nTechnological improvements in LMs Several\nsolutions proposed in recent work to improve lan-\nguage modeling performance can help alleviate\nthe cost and utility issues we highlight. Tokeniza-\ntion is an active area of research and various so-\nlutions based on data balancing (Johnson et al.,\n2017; Conneau and Lample, 2019), optimal trans-\nport (Xu et al., 2021), fuzzy subwords (Provilkov\net al., 2020), and many more (Chung et al., 2020;\nTay et al., 2022) have been proposed. BLOOMZ,\nfor instance, relies on data balancing to improve\nfragmentation rates across languages. Some works\nalso focused on increasing the context lengths of\nlanguage models (Bulatov et al., 2023; Press et al.,\n2022) which can help alleviate issues with utility\nby allowing more in-context examples as input.\n6 Related Work\nAnalyzing tokenization methods The impact of\ntokenization on model performance (Ács, 2019;\nRust et al., 2021; Zhang et al., 2022a; Klein and\nTsarfaty, 2020; Bostrom and Durrett, 2020; Ka-\nmali et al., 2022), inference speed and memory\nusage of LMs in practical settings (Sun et al.,\n2023; Hofmann et al., 2022) has been widely stud-\nied. Ács (2019) observes that mBERT’s vocabu-\nlary is largely dominated by Indo-European lan-\nguages. Rust et al. (2021) find that monolingual\nLMs perform better than mBERT because some\nlanguages suffer from over-fragmentation. Zhang\net al. (2022a) find that sentence-level MT models\nare not sensitive to language imbalance in their to-\nkenizer training data. In contrast to prior work, our\nfocus is on the cost and performance analysis of\nmultilingual LM APIs across languages with regard\n9911\nto over-fragmentation and in-context learning.\nSocio-economic impacts of language models\nPrior work show that unfairness in LMs is a\nconsequence of many stages in the development\npipeline (Cao and Daumé III, 2020; Talat et al.,\n2021). Efforts have tried to identify social biases\nin LM generations (Wolfe and Caliskan, 2021; Dev\net al., 2022; Sheng et al., 2021; Chen et al., 2021;\nHutchinson et al., 2020). Other works have sur-\nfaced the cultural and language disparity beyond\nand within multilingual LMs (Gururangan et al.,\n2022; Kreutzer et al., 2022; Virtanen et al., 2019).\nTalat et al. (2022) discuss challenges impacting\nbias evaluation in multilingual LMs. They exam-\nine power dynamics and consequences of training\nLMs emphasizing implications associated with ad-\nvancement of such technologies. In this work, we\nstudy economic unfairness of LMs across differ-\nent communities. Concurrent work (Petrov et al.,\n2023) analyses multilingual tokenizers focusing on\nfinancial cost, latency and context size. However,\napart from cost, our analysis also covers model\nutility and socio-economic implications. Kasai\net al. (2023) report unfair API costs as a result\nof tokenization differences between English and\nJapanese. We extend this to 21 more languages\nhighlighting the pervasiveness of this issue.\n7 Conclusion\nBy analyzing popular language model APIs on\nchallenging multilingual benchmarks, we find that\n(a) API tokenizers disproportionately favor Latin\nscripted languages and over-fragment less repre-\nsented languages and scripts, (b) the API pricing\npolicy of charging based on the number of tokens\nis flawed and extremely unfair towards speakers\nof the over-fragmented languages, and (c) the API\nperforms poorly on such languages compared to\nthe less-fragmented counterparts. In the current\nNLP research landscape, where more and more in-\ndustrial labs are building their own APIs, this is a\nconcerning trend that may reduce the accessibility\nof these technologies to already marginalized com-\nmunities. Hence, we encourage the vendors to be\nmore transparent about their models’ limitations\nand rethink their pricing policy.\nEthics Statement\nThis work sheds light on the consequences of unfair\ntokenization to users of commercial LM APIs that\nspeak languages with scripts less represented in\nthe pretraining data. With the recent widespread\nuse of commercial LMs, we believe that our work\nis crucial to ensuring that language technologies\nare accessible to diverse users irrespective of the\nlanguages they speak.\nThere are different factors that contribute to non-\nuniform tokenization across languages. Whilst our\nanalysis touches on the size of pretraining data\nand language writing systems we suspect that there\nmight be other factors not yet uncovered; we leave\nthat for future work. The lack of access to Ope-\nnAI’s training data prevents us from making solid\nclaims about all the languages that ChatGPT is\noptimized for; however, their models have been\nadvertised and shown to work well in many lan-\nguages. More work on large multilingual models\nshould include the release of (details of) training\ndata to further enable this kind of research.\nLimitations\nTranslationese We conduct the analysis to an-\nswer RQ1 using a parallel corpus, FLORES-\n200 (Team et al., 2022), in order to control for the\nsame information. This corpus consists of many\nexamples that have been professionally translated.\nPrior studies have shown that translated texts in any\nlanguage (referred to as translationese) may differ\nfrom original written text in many ways (Laviosa,\n2002). These may have caused the information\nconveyed in different languages to not be exactly\nthe same. We do not have a way to measure these\ndifferences. However, we expect them not to be\nso large as to meaningfully affect the trend of frag-\nmentation rates.\nLanguage statistics of ChatGPT training data\nChatGPT is a closed model developed by OpenAI\nwho have not released the training details of the\nmodel including any information of the languages\nit supports.12 Hence, we cannot ascertain the ac-\ntual statistics of all the languages in their training\ndata. We use CC100 (Wenzek et al., 2020), a large\nmultilingual corpus, to estimate these statistics.\nReproducibility One limitation with testing\nclosed LMs is lack of reproducubility particularly\n12The only official information they provide about Chat-\nGPT’s multilingual support is here: https://help.opena\ni.com/en/articles/6742369-how-do-i-use-the-opena\ni-api-in-different-languages Prior studies have specu-\nlated that ChatGPT was trained on at least 90 languages (Ahuja\net al., 2023).\n9912\nbecause the model weights are typically updated\ncontinually. However, this only affects the down-\nstream evaluations as our cost analysis is repro-\nducible, since the tokenizers we evaluate are open-\nsource.\nAcknowledgements\nWe thank the members of the Tsvetshop and Noah’s\nARK labs at the University of Washington for\nthe valuable discussions and useful feedback. We\nthank Lucille Njoo for help with our illustrations.\nWe also thank the reviewers and area chair for\ntheir valuable feedback. During the course of this\nstudy, S.K. was supported by Google Ph.D. Fellow-\nship. We also gratefully acknowledge support from\nNSF CAREER Grant No. IIS2142739, the Alfred\nP. Sloan Foundation Fellowship, and NSF grants\nNo. IIS2125201, IIS2203097, and IIS2113530.\nAny opinions, findings and conclusions or recom-\nmendations expressed in this material are those\nof the authors and do not necessarily state or re-\nflect those of the United States Government or any\nagency thereof.\nReferences\nMohamed Abdalla, Jan Philip Wahle, Terry Ruas, Au-\nrélie Névéol, Fanny Ducel, Saif M Mohammad, and\nKarën Fort. 2023. The elephant in the room: Ana-\nlyzing the presence of big tech in natural language\nprocessing research. In Proceedings of ACL.\nKabir Ahuja, Rishav Hada, Millicent Ochieng, Prachi\nJain, Harshita Diddee, Samuel Maina, Tanuja Ganu,\nSameer Segal, Maxamed Axmed, Kalika Bali, and\nSunayana Sitaram. 2023. Mega: Multilingual evalua-\ntion of generative ai.\nFarhad Akhbardeh, Arkady Arkhangorodsky, Mag-\ndalena Biesialska, Ond ˇrej Bojar, Rajen Chatter-\njee, Vishrav Chaudhary, Marta R. Costa-jussa,\nCristina España-Bonet, Angela Fan, Christian Fe-\ndermann, Markus Freitag, Yvette Graham, Ro-\nman Grundkiewicz, Barry Haddow, Leonie Harter,\nKenneth Heafield, Christopher Homan, Matthias\nHuck, Kwabena Amponsah-Kaakyire, Jungo Kasai,\nDaniel Khashabi, Kevin Knight, Tom Kocmi, Philipp\nKoehn, Nicholas Lourie, Christof Monz, Makoto\nMorishita, Masaaki Nagata, Ajay Nagesh, Toshiaki\nNakazawa, Matteo Negri, Santanu Pal, Allahsera Au-\nguste Tapo, Marco Turchi, Valentin Vydrin, and Mar-\ncos Zampieri. 2021. Findings of the 2021 conference\non machine translation (WMT21). In Proceedings of\nthe Sixth Conference on Machine Translation, pages\n1–88, Online. Association for Computational Linguis-\ntics.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2019. On the cross-lingual transferability of mono-\nlingual representations. In Annual Meeting of the\nAssociation for Computational Linguistics.\nSidney Black, Stella Biderman, Eric Hallahan, Quentin\nAnthony, Leo Gao, Laurence Golding, Horace\nHe, Connor Leahy, Kyle McDonell, Jason Phang,\n9913\nMichael Pieler, Usvsn Sai Prashanth, Shivanshu Puro-\nhit, Laria Reynolds, Jonathan Tow, Ben Wang, and\nSamuel Weinbach. 2022. GPT-NeoX-20B: An open-\nsource autoregressive language model. In Proceed-\nings of BigScience Episode #5 – Workshop on Chal-\nlenges & Perspectives in Creating Large Language\nModels, pages 95–136, virtual+Dublin. Association\nfor Computational Linguistics.\nKaj Bostrom and Greg Durrett. 2020. Byte pair encod-\ning is suboptimal for language model pretraining. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2020, pages 4617–4624, Online.\nAssociation for Computational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nAydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev.\n2023. Scaling transformer to 1M tokens and beyond\nwith RMT.\nYang Trista Cao and Hal Daumé III. 2020. Toward\ngender-inclusive coreference resolution. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 4568–4595, On-\nline. Association for Computational Linguistics.\nYan Chen, Christopher Mahoney, Isabella Grasso,\nEsma Wali, Abigail Matthews, Thomas Middleton,\nMariama Njie, and Jeanna Matthews. 2021. Gender\nbias and under-representation in natural language pro-\ncessing across human languages. In Proceedings of\nthe 2021 AAAI/ACM Conference on AI, Ethics, and\nSociety, AIES ’21, page 24–34, New York, NY , USA.\nAssociation for Computing Machinery.\nHyung Won Chung, Dan Garrette, Kiat Chuan Tan, and\nJason Riesa. 2020. Improving multilingual models\nwith language-clustered vocabularies. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n4536–4546, Online. Association for Computational\nLinguistics.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\nery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,\nDasha Valter, Sharan Narang, Gaurav Mishra, Adams\nYu, Vincent Zhao, Yanping Huang, Andrew Dai,\nHongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-\ncob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le,\nand Jason Wei. 2022. Scaling instruction-finetuned\nlanguage models.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances in\nNeural Information Processing Systems, volume 32.\nCurran Associates, Inc.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel R. Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. Xnli: Evaluating cross-\nlingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing. Association for Computa-\ntional Linguistics.\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke\nZettlemoyer. 2022. GPT3.int8(): 8-bit matrix mul-\ntiplication for transformers at scale. In Advances in\nNeural Information Processing Systems.\nSunipa Dev, Emily Sheng, Jieyu Zhao, Aubrie Amstutz,\nJiao Sun, Yu Hou, Mattie Sanseverino, Jiin Kim, Ak-\nihiro Nishi, Nanyun Peng, and Kai-Wei Chang. 2022.\nOn measures of biases and harms in NLP. In Find-\nings of the Association for Computational Linguis-\ntics: AACL-IJCNLP 2022, pages 246–267, Online\nonly. Association for Computational Linguistics.\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-\nJen Chen, Guillaume Wenzek, Da Ju, Sanjana Kr-\nishnan, Marc’Aurelio Ranzato, Francisco Guzmán,\nand Angela Fan. 2022. The Flores-101 evaluation\nbenchmark for low-resource and multilingual ma-\nchine translation. Transactions of the Association for\nComputational Linguistics, 10:522–538.\nAshim Gupta and Vivek Srikumar. 2021. X-fact: A new\nbenchmark dataset for multilingual fact checking. In\nAnnual Meeting of the Association for Computational\nLinguistics.\nSuchin Gururangan, Dallas Card, Sarah Dreier, Emily\nGade, Leroy Wang, Zeyu Wang, Luke Zettlemoyer,\nand Noah A. Smith. 2022. Whose language counts\nas high quality? measuring language ideologies in\ntext data selection. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 2562–2580, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\n9914\nTahmid Hasan, Abhik Bhattacharjee, Wasi Uddin Ah-\nmad, Yuan-Fang Li, Yong-Bin Kang, and Rifat\nShahriyar. 2021a. CrossSum: Beyond English-\ncentric cross-lingual abstractive text summarization\nfor 1500+ language pairs. ArXiv, abs/2112.08804.\nTahmid Hasan, Abhik Bhattacharjee, Md. Saiful Islam,\nKazi Samin, Yuan-Fang Li, Yong-Bin Kang, M. So-\nhel Rahman, and Rifat Shahriyar. 2021b. Xl-sum:\nLarge-scale multilingual abstractive summarization\nfor 44 languages. In Findings.\nCharles F. Hockett. 1997. The world’s writing systems.\nLanguage, 73(2):379–385. Accessed 17 Oct. 2023.\nValentin Hofmann, Hinrich Schuetze, and Janet Pierre-\nhumbert. 2022. An embarrassingly simple method\nto mitigate undesirable properties of pretrained lan-\nguage model tokenizers. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 385–393,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nHaoyang Huang, Tianyi Tang, Dongdong Zhang,\nWayne Xin Zhao, Ting Song, Yan Xia, and Furu\nWei. 2023. Not all languages are created equal in\nLLMs: Improving multilingual capability by cross-\nlingual-thought prompting.\nBen Hutchinson, Vinodkumar Prabhakaran, Emily Den-\nton, Kellie Webster, Yu Zhong, and Stephen Denuyl.\n2020. Social biases in NLP models as barriers for\npersons with disabilities. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 5491–5501, Online. Association\nfor Computational Linguistics.\nMelvin Johnson, Mike Schuster, Quoc V . Le, Maxim\nKrikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,\nFernanda Viégas, Martin Wattenberg, Greg Corrado,\nMacduff Hughes, and Jeffrey Dean. 2017. Google’s\nmultilingual neural machine translation system: En-\nabling zero-shot translation. Transactions of the As-\nsociation for Computational Linguistics, 5:339–351.\nDanial Kamali, Behrooz Janfada, Mohammad Ebrahim\nShenasa, and Behrouz Minaei-Bidgoli. 2022. Evalu-\nating Persian tokenizers.\nJungo Kasai, Yuhei Kasai, Keisuke Sakaguchi, Yutaro\nYamada, and Dragomir R. Radev. 2023. Evaluating\nGPT-4 and ChatGPT on Japanese medical licensing\nexaminations.\nStav Klein and Reut Tsarfaty. 2020. Getting the ##life\nout of living: How adequate are word-pieces for mod-\nelling complex morphology? In Proceedings of the\n17th SIGMORPHON Workshop on Computational\nResearch in Phonetics, Phonology, and Morphology,\npages 204–209, Online. Association for Computa-\ntional Linguistics.\nJulia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wahab,\nDaan van Esch, Nasanbayar Ulzii-Orshikh, Allah-\nsera Tapo, Nishant Subramani, Artem Sokolov, Clay-\ntone Sikasote, Monang Setyawan, Supheakmungkol\nSarin, Sokhar Samb, Benoî t Sagot, Clara Rivera,\nAnnette Rios, Isabel Papadimitriou, Salomey Osei,\nPedro Ortiz Suarez, Iroro Orife, Kelechi Ogueji, An-\ndre Niyongabo Rubungo, Toan Q. Nguyen, Math-\nias Müller, André Müller, Shamsuddeen Hassan\nMuhammad, Nanda Muhammad, Ayanda Mnyak-\neni, Jamshidbek Mirzakhalov, Tapiwanashe Matan-\ngira, Colin Leong, Nze Lawson, Sneha Kudugunta,\nYacine Jernite, Mathias Jenny, Orhan Firat, Bonaven-\nture F. P. Dossou, Sakhile Dlamini, Nisansa de Silva,\nSakine Çabuk Ballı, Stella Biderman, Alessia Bat-\ntisti, Ahmed Baruwa, Ankur Bapna, Pallavi Baljekar,\nIsrael Abebe Azime, Ayodele Awokoya, Duygu Ata-\nman, Orevaoghene Ahia, Oghenefego Ahia, Sweta\nAgrawal, and Mofetoluwa Adeyemi. 2022. Quality\nat a glance: An audit of web-crawled multilingual\ndatasets. Transactions of the Association for Compu-\ntational Linguistics, 10:50–72.\nTaku Kudo. 2018. Subword regularization: Improv-\ning neural network translation models with multiple\nsubword candidates. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 66–75,\nMelbourne, Australia. Association for Computational\nLinguistics.\nViet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Vey-\nseh, Hieu Man, Franck Dernoncourt, Trung Bui, and\nThien Huu Nguyen. 2023. ChatGPT beyond En-\nglish: Towards a comprehensive evaluation of large\nlanguage models in multilingual learning.\nHugo Laurençon, Lucile Saulnier, Thomas Wang,\nChristopher Akiki, Albert Villanova del Moral,\nTeven Le Scao, Leandro V on Werra, Chenghao Mou,\nEduardo González Ponferrada, Huu Nguyen, Jörg\nFrohberg, Mario Šaško, Quentin Lhoest, Angelina\nMcMillan-Major, Gerard Dupont, Stella Biderman,\nAnna Rogers, Loubna Ben allal, Francesco De Toni,\nGiada Pistilli, Olivier Nguyen, Somaieh Nikpoor,\nMaraim Masoud, Pierre Colombo, Javier de la Rosa,\nPaulo Villegas, Tristan Thrush, Shayne Longpre, Se-\nbastian Nagel, Leon Weber, Manuel Muñoz, Jian\nZhu, Daniel Van Strien, Zaid Alyafeai, Khalid Al-\nmubarak, Minh Chien Vu, Itziar Gonzalez-Dios,\nAitor Soroa, Kyle Lo, Manan Dey, Pedro Ortiz\nSuarez, Aaron Gokaslan, Shamik Bose, David Ade-\nlani, Long Phan, Hieu Tran, Ian Yu, Suhas Pai,\nJenny Chim, Violette Lepercq, Suzana Ilic, Margaret\nMitchell, Sasha Alexandra Luccioni, and Yacine Jer-\nnite. 2023. The BigScience ROOTS Corpus: A 1.6tb\ncomposite multilingual dataset.\nSara Laviosa. 2002. Corpus-based translation studies:\nTheory, findings, applications.\nSabrina J. Mielke, Zaid Alyafeai, Elizabeth Salesky,\nColin Raffel, Manan Dey, Matthias Gallé, Arun Raja,\nChenglei Si, Wilson Y . Lee, Benoît Sagot, and Sam-\nson Tan. 2021. Between words and characters: A\nbrief history of open-vocabulary modeling and tok-\nenization in NLP.\n9915\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Rose Biderman, Teven Le\nScao, M Saiful Bari, Sheng Shen, Zheng Xin Yong,\nHailey Schoelkopf, Xiangru Tang, Dragomir R.\nRadev, Alham Fikri Aji, Khalid Almubarak, Samuel\nAlbanie, Zaid Alyafeai, Albert Webson, Edward\nRaff, and Colin Raffel. 2022. Crosslingual gen-\neralization through multitask finetuning. ArXiv,\nabs/2211.01786.\nBenjamin Muller, Antonios Anastasopoulos, Benoît\nSagot, and Djamé Seddah. 2021. When being un-\nseen from mBERT is just the beginning: Handling\nnew languages with multilingual language models.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 448–462, Online. Association for Computa-\ntional Linguistics.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul F Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. In Advances in Neural Information\nProcessing Systems, volume 35, pages 27730–27744.\nCurran Associates, Inc.\nGunho Park, Baeseong Park, Minsub Kim, Sungjae Lee,\nJeonghoon Kim, Beomseok Kwon, Se Jung Kwon,\nByeongwook Kim, Youngjoo Lee, and Dongsoo Lee.\n2023. LUT-GEMM: Quantized matrix multiplication\nbased on luts for efficient inference in large-scale\ngenerative language models.\nAleksandar Petrov, Emanuele La Malfa, Philip H. S.\nTorr, and Adel Bibi. 2023. Language model tokeniz-\ners introduce unfairness between languages.\nOfir Press, Noah Smith, and Mike Lewis. 2022. Train\nshort, test long: Attention with linear biases enables\ninput length extrapolation. In International Confer-\nence on Learning Representations.\nIvan Provilkov, Dmitrii Emelianenko, and Elena V oita.\n2020. BPE-dropout: Simple and effective subword\nregularization. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1882–1892, Online. Association for\nComputational Linguistics.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao\nChen, Michihiro Yasunaga, and Diyi Yang. 2023. Is\nChatGPT a general-purpose natural language process-\ning task solver?\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, Eliza Rutherford, Tom Hennigan, Ja-\ncob Menick, Albin Cassirer, Richard Powell, George\nvan den Driessche, Lisa Anne Hendricks, Mari-\nbeth Rauh, Po-Sen Huang, Amelia Glaese, Jo-\nhannes Welbl, Sumanth Dathathri, Saffron Huang,\nJonathan Uesato, John Mellor, Irina Higgins, Anto-\nnia Creswell, Nat McAleese, Amy Wu, Erich Elsen,\nSiddhant Jayakumar, Elena Buchatskaya, David Bud-\nden, Esme Sutherland, Karen Simonyan, Michela Pa-\nganini, Laurent Sifre, Lena Martens, Xiang Lorraine\nLi, Adhiguna Kuncoro, Aida Nematzadeh, Elena\nGribovskaya, Domenic Donato, Angeliki Lazaridou,\nArthur Mensch, Jean-Baptiste Lespiau, Maria Tsim-\npoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sot-\ntiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong,\nDaniel Toyama, Cyprien de Masson d’Autume, Yujia\nLi, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin,\nAidan Clark, Diego de Las Casas, Aurelia Guy,\nChris Jones, James Bradbury, Matthew Johnson,\nBlake Hechtman, Laura Weidinger, Iason Gabriel,\nWilliam Isaac, Ed Lockhart, Simon Osindero, Laura\nRimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub,\nJeff Stanway, Lorrayne Bennett, Demis Hassabis, Ko-\nray Kavukcuoglu, and Geoffrey Irving. 2022. Scaling\nlanguage models: Methods, analysis & insights from\ntraining gopher.\nPhillip Rust, Jonas Pfeiffer, Ivan Vuli´c, Sebastian Ruder,\nand Iryna Gurevych. 2021. How good is your tok-\nenizer? on the monolingual performance of multilin-\ngual language models. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 3118–3135, Online. Association\nfor Computational Linguistics.\nTeven Le Scao, Angela Fan, Christopher Akiki,\nElizabeth-Jane Pavlick, Suzana Ili’c, Daniel Hesslow,\nRoman Castagn’e, Alexandra Sasha Luccioni, Franc-\ncois Yvon, Matthias Gallé, Jonathan Tow, Alexan-\nder M. Rush, Stella Rose Biderman, Albert Web-\nson, Pawan Sasanka Ammanamanchi, Thomas Wang,\nBenoît Sagot, Niklas Muennighoff, Albert Villanova\ndel Moral, Olatunji Ruwase, Rachel Bawden, Stas\nBekman, Angelina McMillan-Major, Iz Beltagy, Huu\nNguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz\nSuarez, Victor Sanh, Hugo Laurenccon, Yacine Jer-\nnite, Julien Launay, Margaret Mitchell, Colin Raf-\nfel, Aaron Gokaslan, Adi Simhi, Aitor Soroa Etx-\nabe, Alham Fikri Aji, Amit Alfassy, Anna Rogers,\nAriel Kreisberg Nitzav, Canwen Xu, Chenghao Mou,\nChris C. Emezue, Christopher Klamm, Colin Leong,\nDaniel Alexander van Strien, David Ifeoluwa Ade-\nlani, Dragomir R. Radev, Eduardo Gonz’alez Pon-\nferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar\nNatan, Francesco De Toni, Gérard Dupont, Germán\nKruszewski, Giada Pistilli, Hady ElSahar, Hamza\nBenyamina, Hieu Trung Tran, Ian Yu, Idris Abdul-\nmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier\nde la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu,\nJonathan Chang, Jorg Frohberg, Josephine L. To-\nbing, Joydeep Bhattacharjee, Khalid Almubarak,\n9916\nKimbo Chen, Kyle Lo, Leandro von Werra, Leon\nWeber, Long Phan, Loubna Ben Allal, Ludovic Tan-\nguy, Manan Dey, Manuel Romero Muñoz, Maraim\nMasoud, Mar’ia Grandury, Mario vSavsko, Max\nHuang, Maximin Coavoux, Mayank Singh, Mike\nTian-Jian Jiang, Minh Chien Vu, Mohammad Ali\nJauhar, Mustafa Ghaleb, Nishant Subramani, Nora\nKassner, Nurulaqilla Khamis, Olivier Nguyen, Omar\nEspejel, Ona de Gibert, Paulo Villegas, Peter Hen-\nderson, Pierre Colombo, Priscilla Amuok, Quentin\nLhoest, Rheza Harliman, Rishi Bommasani, Roberto\nL’opez, Rui Ribeiro, Salomey Osei, Sampo Pyysalo,\nSebastian Nagel, Shamik Bose, Shamsuddeen Has-\nsan Muhammad, Shanya Sharma, S. Longpre, So-\nmaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Syd-\nney Zink, Tiago Timponi Torrent, Timo Schick, Tris-\ntan Thrush, Valentin Danchev, Vassilina Nikoulina,\nVeronika Laippala, Violette Lepercq, Vrinda Prabhu,\nZaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin\nHeinzerling, Chenglei Si, Elizabeth Salesky, Sab-\nrina J. Mielke, Wilson Y . Lee, Abheesht Sharma, An-\ndrea Santilli, Antoine Chaffin, Arnaud Stiegler, Deba-\njyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han\nWang, Harshit Pandey, Hendrik Strobelt, Jason Alan\nFries, Jos Rozen, Leo Gao, Lintang Sutawika, M Sai-\nful Bari, Maged S. Al-shaibani, Matteo Manica, Ni-\nhal V . Nayak, Ryan Teehan, Samuel Albanie, Sheng\nShen, Srulik Ben-David, Stephen H. Bach, Taewoon\nKim, Tali Bers, Thibault Févry, Trishala Neeraj, Ur-\nmish Thakker, Vikas Raunak, Xiang Tang, Zheng Xin\nYong, Zhiqing Sun, Shaked Brody, Y Uri, Hadar\nTojarieh, Adam Roberts, Hyung Won Chung, Jae-\nsung Tae, Jason Phang, Ofir Press, Conglong Li,\nDeepak Narayanan, Hatim Bourfoune, Jared Casper,\nJeff Rasley, Max Ryabinin, Mayank Mishra, Minjia\nZhang, Mohammad Shoeybi, Myriam Peyrounette,\nNicolas Patry, Nouamane Tazi, Omar Sanseviero,\nPatrick von Platen, Pierre Cornette, Pierre Franc-\ncois Lavall’ee, Rémi Lacroix, Samyam Rajbhan-\ndari, Sanchit Gandhi, Shaden Smith, Stéphane Re-\nquena, Suraj Patil, Tim Dettmers, Ahmed Baruwa,\nAmanpreet Singh, Anastasia Cheveleva, Anne-Laure\nLigozat, Arjun Subramonian, Aur’elie N’ev’eol,\nCharles Lovering, Daniel H Garrette, Deepak R.\nTunuguntla, Ehud Reiter, Ekaterina Taktasheva, Eka-\nterina V oloshina, Eli Bogdanov, Genta Indra Winata,\nHailey Schoelkopf, Jan-Christoph Kalo, Jekate-\nrina Novikova, Jessica Zosa Forde, Xiangru Tang,\nJungo Kasai, Ken Kawamura, Liam Hazan, Ma-\nrine Carpuat, Miruna Clinciu, Najoung Kim, New-\nton Cheng, Oleg Serikov, Omer Antverg, Oskar\nvan der Wal, Rui Zhang, Ruochen Zhang, Sebastian\nGehrmann, S. Osher Pais, Tatiana Shavrina, Thomas\nScialom, Tian Yun, Tomasz Limisiewicz, Verena\nRieser, Vitaly Protasov, Vladislav Mikhailov, Yada\nPruksachatkun, Yonatan Belinkov, Zachary Bam-\nberger, Zdenvek Kasner, Alice Rueda, Amanda Pes-\ntana, Amir Feizpour, Ammar Khan, Amy Faranak,\nAnanda Santa Rosa Santos, Anthony Hevia, Antig-\nona Unldreaj, Arash Aghagol, Arezoo Abdollahi,\nAycha Tammour, Azadeh HajiHosseini, Bahareh\nBehroozi, Benjamin Olusola Ajibade, Bharat Kumar\nSaxena, Carlos Muñoz Ferrandis, Danish Contrac-\ntor, David M. Lansky, Davis David, Douwe Kiela,\nDuong Anh Nguyen, Edward Tan, Emily Baylor, Ez-\ninwanne Ozoani, Fatim T Mirza, Frankline Onon-\niwu, Habib Rezanejad, H.A. Jones, Indrani Bhat-\ntacharya, Irene Solaiman, Irina Sedenko, Isar Ne-\njadgholi, Jan Passmore, Joshua Seltzer, Julio Bonis\nSanz, Karen Fort, Lívia Macedo Dutra, Mairon Sama-\ngaio, Maraim Elbadri, Margot Mieskes, Marissa Ger-\nchick, Martha Akinlolu, Michael McKenna, Mike\nQiu, M. K. K. Ghauri, Mykola Burynok, Nafis\nAbrar, Nazneen Rajani, Nour Elkott, Nourhan Fahmy,\nOlanrewaju Samuel, Ran An, R. P. Kromann, Ryan\nHao, Samira Alizadeh, Sarmad Shubber, Silas L.\nWang, Sourav Roy, Sylvain Viguier, Thanh-Cong\nLe, Tobi Oyebade, Trieu Nguyen Hai Le, Yoyo Yang,\nZachary Kyle Nguyen, Abhinav Ramesh Kashyap,\nAlfredo Palasciano, Alison Callahan, Anima Shukla,\nAntonio Miranda-Escalada, Ayush Kumar Singh,\nBenjamin Beilharz, Bo Wang, Caio Matheus Fon-\nseca de Brito, Chenxi Zhou, Chirag Jain, Chuxin\nXu, Clémentine Fourrier, Daniel Le’on Perin’an,\nDaniel Molano, Dian Yu, Enrique Manjavacas, Fabio\nBarth, Florian Fuhrimann, Gabriel Altay, Giyased-\ndin Bayrak, Gully A. Burns, Helena U. Vrabec,\nIman I.B. Bello, Isha Dash, Ji Soo Kang, John\nGiorgi, Jonas Golde, Jose David Posada, Karthi\nSivaraman, Lokesh Bulchandani, Lu Liu, Luisa\nShinzato, Madeleine Hahn de Bykhovetz, Maiko\nTakeuchi, Marc Pàmies, María Andrea Castillo, Mar-\nianna Nezhurina, Mario Sanger, Matthias Samwald,\nMichael Cullan, Michael Weinberg, M Wolf, Mina\nMihaljcic, Minna Liu, Moritz Freidank, Myung-\nsun Kang, Natasha Seelam, Nathan Dahlberg,\nNicholas Michio Broad, Nikolaus Muellner, Pascale\nFung, Patricia Haller, R. Chandrasekhar, R. Eisen-\nberg, Robert Martin, Rodrigo L. Canalli, Rosaline\nSu, Ruisi Su, Samuel Cahyawijaya, Samuele Garda,\nShlok S Deshmukh, Shubhanshu Mishra, Sid Ki-\nblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Ku-\nmar, Stefan Schweter, Sushil Pratap Bharati, T. A.\nLaud, Th’eo Gigant, Tomoya Kainuma, Wojciech\nKusa, Yanis Labrak, Yashasvi Bajaj, Y . Venkatra-\nman, Yifan Xu, Ying Xu, Yun chao Xu, Zhee Xao\nTan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes\nBelkada, and Thomas Wolf. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\nArXiv, abs/2211.05100.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1715–1725,\nBerlin, Germany. Association for Computational Lin-\nguistics.\nEmily Sheng, Kai-Wei Chang, Prem Natarajan, and\nNanyun Peng. 2021. Societal biases in language\ngeneration: Progress and challenges. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 4275–4293, Online.\nAssociation for Computational Linguistics.\n9917\nXinying Song, Alex Salcianu, Yang Song, Dave Dopson,\nand Denny Zhou. 2021. Fast WordPiece tokenization.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2089–2103, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nXinying Song, Alexandru Salcianu, Yang Song, Dave\nDopson, and Denny Zhou. 2020. Fast WordPiece\ntokenization. In Conference on Empirical Methods\nin Natural Language Processing.\nHongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi,\nTianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf,\nLuke Zettlemoyer, Noah A. Smith, and Tao Yu. 2023.\nSelective annotation makes language models better\nfew-shot learners. In Proc. of ICLR.\nJimin Sun, Patrick Fernandes, Xinyi Wang, and Gra-\nham Neubig. 2023. A multi-dimensional evaluation\nof tokenizer-free multilingual pretrained models. In\nFindings of the Association for Computational Lin-\nguistics: EACL 2023, pages 1725–1735, Dubrovnik,\nCroatia. Association for Computational Linguistics.\nTianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing\nHuang, and Xipeng Qiu. 2022. Black-box tuning\nfor language-model-as-a-service. In Proceedings of\nICML.\nZeerak Talat, Joachim Bingel, and Isabelle Augenstein.\n2021. Disembodied machine learning: On the illu-\nsion of objectivity in NLP. ArXiv, abs/2101.11974.\nZeerak Talat, Aurélie Névéol, Stella Biderman, Miruna\nClinciu, Manan Dey, Shayne Longpre, Sasha Luc-\ncioni, Maraim Masoud, Margaret Mitchell, Dragomir\nRadev, Shanya Sharma, Arjun Subramonian, Jaesung\nTae, Samson Tan, Deepak Tunuguntla, and Oskar Van\nDer Wal. 2022. You reap what you sow: On the chal-\nlenges of bias evaluation under multilingual settings.\nIn Proceedings of BigScience Episode #5 – Workshop\non Challenges & Perspectives in Creating Large Lan-\nguage Models, pages 26–41, virtual+Dublin. Associ-\nation for Computational Linguistics.\nYi Tay, Vinh Q. Tran, Sebastian Ruder, Jai Gupta,\nHyung Won Chung, Dara Bahri, Zhen Qin, Simon\nBaumgartner, Cong Yu, and Donald Metzler. 2022.\nCharformer: Fast character transformers via gradient-\nbased subword tokenization. In International Con-\nference on Learning Representations.\nNLLB Team, Marta R. Costa-jussà, James Cross, Onur\nÇelebi, Maha Elbayad, Kenneth Heafield, Kevin Hef-\nfernan, Elahe Kalbassi, Janice Lam, Daniel Licht,\nJean Maillard, Anna Sun, Skyler Wang, Guillaume\nWenzek, Al Youngblood, Bapi Akula, Loic Bar-\nrault, Gabriel Mejia Gonzalez, Prangthip Hansanti,\nJohn Hoffman, Semarley Jarrett, Kaushik Ram\nSadagopan, Dirk Rowe, Shannon Spruit, Chau\nTran, Pierre Andrews, Necip Fazil Ayan, Shruti\nBhosale, Sergey Edunov, Angela Fan, Cynthia\nGao, Vedanuj Goswami, Francisco Guzmán, Philipp\nKoehn, Alexandre Mourachko, Christophe Ropers,\nSafiyyah Saleem, Holger Schwenk, and Jeff Wang.\n2022. No language left behind: Scaling human-\ncentered machine translation.\nAntti Virtanen, Jenna Kanerva, Rami Ilo, Jouni Luoma,\nJuhani Luotolahti, Tapio Salakoski, Filip Ginter, and\nSampo Pyysalo. 2019. Multilingual is not enough:\nBERT for Finnish.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzmán, Ar-\nmand Joulin, and Edouard Grave. 2020. CCNet:\nExtracting high quality monolingual datasets from\nweb crawl data. In Proceedings of the Twelfth Lan-\nguage Resources and Evaluation Conference, pages\n4003–4012, Marseille, France. European Language\nResources Association.\nRobert Wolfe and Aylin Caliskan. 2021. Low frequency\nnames exhibit bias and overfitting in contextualizing\nlanguage models. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 518–532, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nJingjing Xu, Hao Zhou, Chun Gan, Zaixiang Zheng,\nand Lei Li. 2021. V ocabulary learning via optimal\ntransport for neural machine translation. In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 7361–7373, Online.\nAssociation for Computational Linguistics.\nShiyue Zhang, Vishrav Chaudhary, Naman Goyal,\nJames Cross, Guillaume Wenzek, Mohit Bansal, and\nFrancisco Guzman. 2022a. How robust is neural ma-\nchine translation to language imbalance in multilin-\ngual tokenizer training? In Proceedings of the 15th\nbiennial conference of the Association for Machine\nTranslation in the Americas (Volume 1: Research\nTrack), pages 97–116, Orlando, USA. Association\nfor Machine Translation in the Americas.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022b. Opt: Open\npre-trained transformer language models.\nYiming Zhu, Peixian Zhang, Ehsan-Ul Haq, Pan Hui,\nand Gareth Tyson. 2023. Can ChatGPT reproduce\nhuman-generated labels? a study of social computing\ntasks.\nJudit Ács. 2019. Exploring BERT’s vocabulary. http:\n//juditacs.github.io/2019/02/19/bert-tok\nenization-stats.html.\nA Prompt template\nIn Table 2, we provide the exact prompts we use\nfor each respective task in our experiments.\n9918\nB Cost analysis\nIn Figure 11 we present the experimental cost rel-\native to English and Spanish for Crosssum and\nXFACT respectively. Figure 16 shows the esti-\nmated cost of GPT3.5 API access for all languages\nin CC100 relative to English .\nC Extra analysis of fragmentation rate\nIn Figure 12 and Figure 13 we present the fragmen-\ntation rate across language families and scripts for\nboth GPT3.5 and BLOOM respectively. Figure 17\nshows the fragmentation rate for all languages in\nFLORES grouped by language script.\nD Fragmentation rate vs pretraining data\nsize\nIn Figure 14 we sort languages based on their size\nin CC100 corpus (Wenzek et al., 2020) and plot\ntheir fragmentation rate with GPT3.5 tokenizer.\nFigure 15 shows the same statistics for BLOOM’s\ntokenizer based on language pretraining data size\nin (ROOTS corpus; Laurençon et al., 2023).\nE Pretrained tokenizers on more\nlanguages\nFigure 19 shows fragmentation rate across lan-\nguage scripts, when we train a BBPE tokenizer\ntrained on parallel text in 30 languages.\nF Fragmentation rate vs HDI\nFigure 18 shows GPT3.5’s fragmentation rate per\nlanguage against the Human Development Index\nof the country with the largest amount of speak-\ners of that language. We add more languages and\ncountries here compared to the figure in the main\npaper.\n9919\nTask Prompt Template\nXLSUM Write a short summary sentence of the following text in\n{language} Article: { article} Summary:\nXQUAD Context: context Question: question Answer: Template\nXNLI {Premise} Question : {hypothesis} True, False, or Neither?\nAnswer:\nCROSSUM Write a short summary sentence of the following text in\nEnglish. Article: { article} Summary:\nXFACT Tell me whether the following claim is {label 1 } or {label\n2 } or {label 3 } ... given evidence {evidence 1 }, {evidence\n2 }, {evidence 3 }\nTable 2: Prompt template used for each dataset.\nes id de pt it ro sr pl tr ar hi ka ta\nLanguage\n0\n1\n2\n3\n4Experiment Cost relative to Spanish\n(a) XFACT\nen id sw pt fr es ru vi ko ar ja hi ta am te\nLanguage\n0\n1\n2\n3\n4Experiment Cost relative to English (b) CROSSUM\nFigure 11: Relative cost of prompt + generated tokens for XFACT and CROSS-SUM evaluations.\nAfro-Asiatic\nAtlantic-Congo\nAustroasiatic\nAustronesian\nAymaran\nBasque\nCommon Turkic\nConstructed\nDravidian\nIndo-European\nJaponic\nKartvelian\nKoreanic\nMande\nMongolic-Khitan\nNilotic\nQuechuan\nSaharan\nSino-Tibetan\nTai-Kadai\nTupian\nTurkic\nUralic\nLanguage Family\n0\n100\n200Average number of tokens\nFigure 12: Average number of tokens per language\nfamily after tokenizing Flores dataset with GPT3.5 tok-\nenizer. The fragmentation rate is lower for Latin script\nlanguages and higher for other scripts.\nHan (Simpliﬁed)\nDevanagari\nArabic\nTamil\nBengali\nKannada\nTelugu\nGujarati\nMalayalam\nGurmukhi\nLatin\nLanguage Script\n0\n10\n20\n30\n40Average number of tokens\nFigure 13: Average number of tokens per language\nscript after tokenizing Flores dataset with BLOOM to-\nkenizer. The fragmentation rate is higher on average\nfor Latin script languages. This is because majority of\nthe low-resourced languages are latin-script and have\nhigher fragmentation rate.\n9920\nengrusindvieukrswethajpndeuronhunbulfraﬁnkorspaporellzhodanpolhebitanldslkhinhrvturceslittamcatslvkatsrpbenmalkazesturdhyemkdtelbelsinislkantglglgmareusgujkhmafrkirepoamhpancymglemyasomuigsanjavglabosasmsun\nLanguage\n0\n100\n200\n300Average number of tokens\nFigure 14: Average number of tokens per language after tokenizing FLORES with GPT3.5 tokenizer. Languages\nare arranged in descending order based on the size of pretraining data in Commoncrawl.\nengzhofraspaporarbviehinindbencattammaltelurdeuskanmarpangujasmswhyorkinxhoibozulsnalugwolrunfonnsolintsntwinyasottsoakabamkiktum\nLanguage\n0\n20\n40\n60Average number of tokens\nFigure 15: Average number of tokens per language after tokenizing FLORES with BLOOM tokenizer. Languages\nare arranged in descending order based on the size of pretraining data in the ROOTS corpus.\n9921\nengporspaindglgdeuswefranlddanitaafrcatjavsunhrvestronepobosslvzhoeusturpolﬁntglcescymhunslkislsomlitjpnglekorglavierusbulmkdsrpukrkirbelhebkazthaurdhinsanmarelluigbenasmtamgujpantelsinkankhmmalkathyemya\nLanguage\n1\n4\n8\n12Relative cost to English\nFigure 16: Estimated cost of GPT3.5 API access relative to English.\naceminsndckbuigurdpbtbjnprskaspeskncacmacqaryaebarsarbapcarzajphyeasmmnibensrpukrbultgkbeltatkazbakkirkhkmkdrusmaisanhnehinmagmarbhoawakatellgujpanzhoyuekoryddhebjpnkankhmlaonsoyorpltmltmosviewarwolpapmripagxhogazocinldnyavecnnonusnobtglporuzntaqszlswhswesunsswtpisrdalsspatsnsomtsosnasmoslvslktuktumscnturtwisagumbrunronquypolsotlinlvsdyuengepoesteusewedikfaoﬁnfonfrafurfuvgleﬁjdeudancymafrakaastayrazjbambanbembosbugcatcebcescjkcrhglggrnglahauhatkinkmbkmrkonlijlimkeazsmlmoltgltzlualugluoluslitkbpkikilohrvhuniboindislitazulkabkacjavkammalmyashnsatorysintamtelthadzobodtzmazb\nLanguage\n0\n50\n100\n150\n200\n250\n300\n350\n400Average number of tokens\nArabic\nArmenian\nBengali\nCyrillic\nDevanagari\nGeorgian\nGreek\nGujarati\nGurmukhi\nHan (Simpliﬁed)\nHan (Traditional)\nHangul\nHebrew\nJapanese\nKannada\nKhmer\nLao\nLatin\nMalayalam\nMyanmar\nOl Chiki\nOriya\nSinhala\nTamil\nTelugu\nThai\nTibetan\nTiﬁnagh\nTurkic\nFigure 17: Average number of tokens by script after tokenizing all languages in the Flores dataset with GPT3.5\ntokenizer.\n9922\n0.4 0.5 0.6 0.7 0.8 0.9\nHuman Development Index\n50\n100\n150\n200\n250\n300Average number of tokens\nAssamese\nBelarusian\nBengali\nBosnian Catalan German\nGreek\nEnglish\nEstonianFinnishFrenchGalician\nGujarati\nHebrew\nHindi\nHungarian\nArmenian\nJapanese\nKannada\nGeorgian\nKazakh\nKhmer\nKyrgyz\nMalayalam\nMarathi\nMacedonian\nBurmese\nEastern Panjabi\nPortuguese\nRussian\nSinhala\nSomali\nSpanishSundanese\nTamil\nTelugu\nTagalog\nThai\nUyghur\nUkrainian\nUrdu\nVietnamese\nFigure 18: Fragmentation rate per language against the Human Development Index in the country with the largest\namount of speakers of that language.\n200 500 1000 5000 10000 25000 50000\nVocab size\n50\n100\n150\n200\n250\n300\n350\n400Avg number of subwords\nArabic\nArmenian\nBengali\nCyrillic\nDevanagari\nGeez\nGeorgian\nGreek\nGujarati\nGurmukhi\nHan (Simpliﬁed)\nHan (Traditional)\nHangul\nHebrew\nJapanese\nKannada\nKhmer\nLao\nLatin\nMalayalam\nMyanmar\nOl Chiki\nOriya\nSinhala\nTamil\nTelugu\nThai\nTibetan\nTiﬁnagh\nTurkic\nFigure 19: BBPE tokenizer trained on parallel text from 30 language scripts with varying vocabulary sizes. It is\nimpossible to achieve uniform fragmnatation rate even when we have equal pretraining data sizes across all language\nscripts.\n9923"
}