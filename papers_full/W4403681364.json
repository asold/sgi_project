{
  "title": "From pre-training to fine-tuning: An in-depth analysis of Large Language Models in the biomedical domain",
  "url": "https://openalex.org/W4403681364",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5094302268",
      "name": "Agnese Bonfigli",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3118079133",
      "name": "Luca Bacco",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2016459725",
      "name": "Mario Merone",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3144560738",
      "name": "Felice Dell'Orletta",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4382246105",
    "https://openalex.org/W6842358254",
    "https://openalex.org/W3031200717",
    "https://openalex.org/W4385456320",
    "https://openalex.org/W6781031682",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W6856439586",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W4297253404",
    "https://openalex.org/W6762122294",
    "https://openalex.org/W4322766882",
    "https://openalex.org/W3041133507",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W3170643178",
    "https://openalex.org/W6842099467",
    "https://openalex.org/W6809509765",
    "https://openalex.org/W6803777490",
    "https://openalex.org/W4365511667",
    "https://openalex.org/W3034256339",
    "https://openalex.org/W4205528003",
    "https://openalex.org/W2997789497",
    "https://openalex.org/W2162800060",
    "https://openalex.org/W2070808142",
    "https://openalex.org/W2047782770",
    "https://openalex.org/W6852521717",
    "https://openalex.org/W3202070718",
    "https://openalex.org/W2128160875",
    "https://openalex.org/W1966554111",
    "https://openalex.org/W2043935429",
    "https://openalex.org/W2970120757",
    "https://openalex.org/W4293506705",
    "https://openalex.org/W3152996058",
    "https://openalex.org/W4223974161",
    "https://openalex.org/W4256093969",
    "https://openalex.org/W3135176278",
    "https://openalex.org/W3017190214",
    "https://openalex.org/W4287025617",
    "https://openalex.org/W170732776",
    "https://openalex.org/W4206437506",
    "https://openalex.org/W2888120268",
    "https://openalex.org/W3035625205",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W2951299559",
    "https://openalex.org/W4236623719",
    "https://openalex.org/W3103368673",
    "https://openalex.org/W3201663597",
    "https://openalex.org/W2971292190",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W4390490761",
    "https://openalex.org/W4295857769",
    "https://openalex.org/W4384389802",
    "https://openalex.org/W3176894732",
    "https://openalex.org/W3174088532",
    "https://openalex.org/W2612690371",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W4287646350",
    "https://openalex.org/W3155744586",
    "https://openalex.org/W2964242047",
    "https://openalex.org/W4287867774",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4205450747",
    "https://openalex.org/W2799051177",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W4367047780",
    "https://openalex.org/W3174782183",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3046375318"
  ],
  "abstract": "In this study, we delve into the adaptation and effectiveness of Transformer-based, pre-trained Large Language Models (LLMs) within the biomedical domain, a field that poses unique challenges due to its complexity and the specialized nature of its data. Building on the foundation laid by the transformative architecture of Transformers, we investigate the nuanced dynamics of LLMs through a multifaceted lens, focusing on two domain-specific tasks, i.e., Natural Language Inference (NLI) and Named Entity Recognition (NER). Our objective is to bridge the knowledge gap regarding how these models' downstream performances correlate with their capacity to encapsulate task-relevant information. To achieve this goal, we probed and analyzed the inner encoding and attention mechanisms in LLMs, both encoder- and decoder-based, tailored for either general or biomedical-specific applications. This examination occurs before and after the models are fine-tuned across various data volumes. Our findings reveal that the models' downstream effectiveness is intricately linked to specific patterns within their internal mechanisms, shedding light on the nuanced ways in which LLMs process and apply knowledge in the biomedical context. The source code for this paper is available at https://github.com/agnesebonfigli99/LLMs-in-the-Biomedical-Domain.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.804315984249115
    },
    {
      "name": "Training (meteorology)",
      "score": 0.614050030708313
    },
    {
      "name": "Language model",
      "score": 0.5742506980895996
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5286959409713745
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4882638454437256
    },
    {
      "name": "Natural language processing",
      "score": 0.4761795401573181
    },
    {
      "name": "Machine learning",
      "score": 0.324367493391037
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Meteorology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210124522",
      "name": "Institute for Computational Linguistics “A. Zampolli”",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I4210155236",
      "name": "National Research Council",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I155125353",
      "name": "Università Campus Bio-Medico",
      "country": "IT"
    }
  ],
  "cited_by": 4
}