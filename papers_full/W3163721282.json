{
  "title": "Relative Positional Encoding for Transformers with Linear Complexity",
  "url": "https://openalex.org/W3163721282",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3178437648",
      "name": "Liutkus, Antoine",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287356915",
      "name": "Cífka, Ondřej",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287356916",
      "name": "Wu, Shih-Lun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221462045",
      "name": "Şimşekli, Umut",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222713164",
      "name": "Yang, Yi-Hsuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2744512309",
      "name": "Richard, Gael",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3112344419",
    "https://openalex.org/W2211925278",
    "https://openalex.org/W3097030750",
    "https://openalex.org/W3120633509",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W2951008357",
    "https://openalex.org/W2946028745",
    "https://openalex.org/W2996287690",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2971270287",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W2944959599",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2079982527",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3175663427",
    "https://openalex.org/W2956669849",
    "https://openalex.org/W3037983807",
    "https://openalex.org/W3080857286",
    "https://openalex.org/W3120885796",
    "https://openalex.org/W2959020461",
    "https://openalex.org/W2948852532",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W2941814890",
    "https://openalex.org/W2178628967",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W2985287635",
    "https://openalex.org/W3101359644",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3000389243",
    "https://openalex.org/W3210914950",
    "https://openalex.org/W2024697317",
    "https://openalex.org/W3044284384",
    "https://openalex.org/W2962727817",
    "https://openalex.org/W1971713783",
    "https://openalex.org/W1799689209",
    "https://openalex.org/W3093260394",
    "https://openalex.org/W3121592593",
    "https://openalex.org/W3106210592",
    "https://openalex.org/W3091156754",
    "https://openalex.org/W2954914497",
    "https://openalex.org/W2016589492",
    "https://openalex.org/W3012835873",
    "https://openalex.org/W2286689764",
    "https://openalex.org/W3131922516",
    "https://openalex.org/W2943845043",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2963551352",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W658512522",
    "https://openalex.org/W3121309507",
    "https://openalex.org/W2896060389",
    "https://openalex.org/W2963393721",
    "https://openalex.org/W3102094970",
    "https://openalex.org/W3033187248",
    "https://openalex.org/W3168215316",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2405656250",
    "https://openalex.org/W2989156240",
    "https://openalex.org/W2789541106",
    "https://openalex.org/W3034573343",
    "https://openalex.org/W3003673875",
    "https://openalex.org/W3026709049",
    "https://openalex.org/W2997175890",
    "https://openalex.org/W2963341956"
  ],
  "abstract": "Recent advances in Transformer models allow for unprecedented sequence lengths, due to linear space and time complexity. In the meantime, relative positional encoding (RPE) was proposed as beneficial for classical Transformers and consists in exploiting lags instead of absolute positions for inference. Still, RPE is not available for the recent linear-variants of the Transformer, because it requires the explicit computation of the attention matrix, which is precisely what is avoided by such methods. In this paper, we bridge this gap and present Stochastic Positional Encoding as a way to generate PE that can be used as a replacement to the classical additive (sinusoidal) PE and provably behaves like RPE. The main theoretical contribution is to make a connection between positional encoding and cross-covariance structures of correlated Gaussian processes. We illustrate the performance of our approach on the Long-Range Arena benchmark and on music generation.",
  "full_text": "Relative Positional Encoding for Transformers with Linear Complexity\nAntoine Liutkus * 1 Ondˇrej C´ıfka* 2 Shih-Lun Wu3 4 5 Umut S ¸ims ¸ekli6 Yi-Hsuan Yang3 5 Ga¨el Richard 2\nAbstract\nRecent advances in Transformer models allow\nfor unprecedented sequence lengths, due to linear\nspace and time complexity. In the meantime, rela-\ntive positional encoding (RPE) was proposed as\nbeneﬁcial for classical Transformers and consists\nin exploiting lags instead of absolute positions\nfor inference. Still, RPE is not available for the\nrecent linear-variants of the Transformer, because\nit requires the explicit computation of the atten-\ntion matrix, which is precisely what is avoided by\nsuch methods. In this paper, we bridge this gap\nand present Stochastic Positional Encoding as a\nway to generate PE that can be used as a replace-\nment to the classical additive (sinusoidal) PE and\nprovably behaves like RPE. The main theoretical\ncontribution is to make a connection between posi-\ntional encoding and cross-covariance structures of\ncorrelated Gaussian processes. We illustrate the\nperformance of our approach on the Long-Range\nArena benchmark and on music generation.\n1. Introduction\n1.1. Linear Complexity Transformers\nThe Transformer model (Vaswani et al., 2017) is a new\nkind of neural network that quickly became state-of-the-\nart in many application domains, including the processing\nof natural language (He et al., 2020), images (Dosovitskiy\net al., 2020), audio (Huang et al., 2018; Pham et al., 2020)\nor bioinformatics (AlQuraishi, 2019) to mention just a few.\nThe core, novel component of the Transformer is the at-\ntention layer. It computes M output values ym from N\ninput values vn, all being vectors of an arbitrary dimen-\n*Equal contribution 1Inria, Zenith Team, UMR LIRMM, Univ.\nMontpellier, France 2LTCI, T´el´ecom Paris, Institut Polytechnique\nde Paris, France 3Research Center for IT Innovation, Academia\nSinica, Taiwan 4National Taiwan University, Taiwan5Taiwan AI\nLabs, Taiwan 6INRIA – D´epartement d’Informatique de l’´Ecole\nNormale Sup ´erieure – PSL Research University, Paris, France.\nCorrespondence to: Liutkus Antoine <antoine.liutkus@inria.fr>.\nProceedings of the 38 th International Conference on Machine\nLearning, PMLR 139, 2021. Copyright 2021 by the author(s).\nFigure 1.Examples of attention patterns observed in the Perform-\ners trained for pop piano music generation (section 3.2) at infer-\nence time, for sequence length M = N = 3 072while training\nsequences have length 2 048. (left) Absolute PE. (middle) Si-\nnusoidal SPE. (right) Convolutional SPE. Note that SPE never\nrequires computing these full attention patterns.\nsion. Following classical non-parametric regression princi-\nples (Nadaraya, 1964; Watson, 1964), it consists in a simple\nweighted sum:\nym =\n∑\nnamnvn∑\nnamn\n, (1)\nwhere each attention coefﬁcient amn ∈R+ – gathered in\nthe M ×N matrix A – indicates how important the value\nvn is in the computation of the output ym.\nOne of the main contributions of the Transformer is an orig-\ninal method to compute these coefﬁcients. D-dimensional\nfeature vectors kn and qm are attached to all items of the\ninput and output sequences and are called keys and queries,\nrespectively. Gathering them in the N ×D and M ×D\nmatrices K and Q, we get softmax dot-product attention as:\nA = exp\n(\nQK⊤\n/√\nD\n)\n≡[amn = K(qm,kn)]mn, (2)\nwhere the function exp is applied element-wise. The right-\nhand side in (2) is a generalization introduced by Tsai et al.\n(2019) and Choromanski et al. (2020), where Kis a kernel\nfunction. Parameters pertain to how keys kn, values vn and\nqueries qm are obtained from the raw sequences, usually by\ntime-distributed fully connected layers.\nThe original Transformer architecture (Vaswani et al., 2017)\nexplicitly computes the attention matrix A, leading to a\nO(MN) complexity that prevents it from scaling to very\nlong sequence lengths. Although this is not necessarily a\nproblem when sequence lengths are barely on the order of\na few hundreds, as in some language processing tasks, it is\narXiv:2105.08399v2  [cs.LG]  10 Jun 2021\nRelative Positional Encoding for Transformers with Linear Complexity\nprohibitive for very large signals like high-resolution images\nor audio.\nFocusing on this scalability issue, several approaches have\nbeen recently investigated to allow for long sequences:\n• Attention clustering schemes group items among which\ndependencies are computed through regular attention.\nThis is either done by using simple proximity rules within\nthe sequences, leading to chunking strategies (Dai et al.,\n2019), or by clustering the keys and values (Roy et al.,\n2020). Inter-cluster dependencies are either ignored or\nsummarized via ﬁxed-length context vectors that are\ncoined in as memory (Wu et al., 2020).\n• Assuming the attention matrix to be sparse. In this case,\nonly a few amn are nonzero (Child et al., 2019).\n• Assuming A has a particular (low-rank) structure and can\nbe decomposed as the product of two smaller matrices.\nA prototypical example is the Linformer (Wang et al.,\n2020b), which is limited to ﬁxed-length inputs. Another\nvery recent line of research in this same vein takes:\nA ≈φ(Q)φ(K)⊤, (3)\nwhere φ: RD →RR is a non-linear feature map applied\nto each key kn and query qm, and R ≪ min(M,N )\n(Shen et al., 2020; Katharopoulos et al., 2020).\n• When Kin (2) is a positive (semi)deﬁnite kernel, the\nPerformer (Choromanski et al., 2020) leverages reproduc-\ning kernel Hilbert spaces to show that a random φmay\nbe used to exploit this convenient decomposition (3) on\naverage, even when A is not low rank:\nK⪰ 0 ⇔A = Eφ\n[\nφ(Q)φ(K)⊤\n]\n, (4)\nwhere φis drawn from a distribution that depends on K.\nA simple example is φW(kn) = max(0,Wkn), with a\nrandom W ∈RR×D for some R∈N.\nWhenever an efﬁcient scheme like (3) or (4) is used, the\noutputs can be obtained without computing the attention\ncoefﬁcients amn, as in (10).1\n1.2. Positional Encoding\nIn Transformer networks, the outputs ym are computed as\nlinear combinations of all input values vn, weighted by\nattention coefﬁcients amn. In sequence modeling, it is rea-\nsonable to assume that the actual positions mand nshould\nplay a role in the computation, in addition to the content at\nthese locations; otherwise, any permutation of the sequence\nwould lead to the same output. Two core approaches were\nundertaken to incorporate position information:\n• The original Transformer (Vaswani et al., 2017) adds this\n1A somewhat related strategy is used by the recent LambdaNet-\nworks (Bello, 2020), which encapsulate the key-value information\nas a so-called lambda function to be applied query-wise, hence\nalso avoiding the computation of a full attention matrix.\ninformation to the inputs of the network, i.e. before the\nﬁrst attention layer. This can be equivalently understood\nas augmenting the keys, values and queries:\nkn ←kn + kn,vn ←vn + vn,qm ←qm + qm, (5)\nwhere we write kn ∈RD for the keys positional encoding\n(PE; Sukhbaatar et al., 2015) at position n∈N and anal-\nogously for values and queries. Vaswani et al. propose a\ndeterministic scheme based on trigonometric functions,\nwhich is shown to work as well as trainable embeddings.\n• As an example of positional encoding in the attention\ndomain, a relative positional encoding (RPE) was pro-\nposed by Shaw et al. (2018), building on the idea that\ntime lags m−n are more important than absolute po-\nsitional encoding (APE) for prediction. It is written as:\nA = exp\n((\nQK⊤+ Ω\n)/√\nD\n)\n, with: (6)\nΩ ≡\n[\nωmn =\nD∑\nd=1\nqmdPd(m−n)\n]\nmn\n. (7)\nThe terms Pd now act as Ddifferent encodings for time\nlags selected based on the queries. This change is advo-\ncated as bringing important performance gains in many\napplication areas and has enjoyed a widespread use ever\nsince.\nAlthough writing down the positional encoding in the at-\ntention domain is beneﬁcial for performance (Shaw et al.,\n2018; Dai et al., 2019; Tsai et al., 2019), we are only aware\nof implementations that either require the computation of\nA, or clustered attention schemes, which in ﬁne decompose\nA into smaller attention matrices, and compute them. This\nis in sharp contrast to (3) and (4), which never compute the\nattention matrix.\nOur contributions can be summarized as follows:\n• We propose Stochastic Positional Encoding (SPE) as a\ngeneral PE scheme in the keys domain, that enforces a par-\nticular attention pattern devised in the attention domain.\nThis enables RPE without explicit computation of atten-\ntion. To our knowledge, it is the ﬁrst RPE strategy that is\ncompatible with O(N) Transformers like Choromanski\net al. (2020) and Katharopoulos et al. (2020).\n• We study the impact of SPE on performance on the Long-\nRange Arena benchmark (Tay et al., 2021) and two music\ngeneration tasks. Since RPE was so far limited to short\nsequences, we believe this is the ﬁrst study of its advan-\ntages on long-range predictions. Our results demonstrate\nbetter validation losses and extrapolation ability.\n• We provide additional resources on our companion web-\nsite,2 including Python implementations of SPE for Py-\nTorch and JAX/Flax.\n2https://cifkao.github.io/spe/\nRelative Positional Encoding for Transformers with Linear Complexity\nAlgorithm 1 Stochastic Positional Encoding.\nInput\n• position kernel P(m,n), number of replicas R.\n• initial M ×Dand N ×Dqueries Q and keys K.\nPositional encoding:\n• Draw the Dindependent couples {Qd,Kd}d of M ×R\nand N ×Rmatrices as in section 2.1\n• Set ˆQ and ˆK as in (16) and (17)\nInference compute outputs Y with the O(N) Transformer:\nY ←diag(d)−1\n[\nφ\n(\nˆQ\n)[\nφ\n(\nˆK\n)⊤\nV\n]]\n(10)\nwith d = φ(ˆQ)\n[\nφ\n(\nˆK\n)⊤\n1N\n]\nand φdiscussed in (3)/(4).\n2. Stochastic Positional Encoding\nIndex set and notation. We assume that the input/output\nsequences are indexed by n,m ∈T, where T is the index\nset. For regularly sampled sequences, we have T = N, but\nmore settings are possible, like irregularly sampled time\nseries (T = R) or images (T = N2). In any case, the partic-\nular lists of input / output locations under consideration are\nwritten: Nand M, with respective sizesNand M(the case\nN= Mis called self-attention). The corresponding keys\nand values are hence indexed as {kn}n∈N and {vn}n∈N,\nwhile queries are {qm}m∈M. For convenience, we write\namn for the entries of the M ×N attention matrix A.\nWe use bold uppercase for matrices, bold lowercase for vec-\ntors and a NumPy-like notation: if Xk is a I×Jmatrix, xk,i\nand xk,:,j stand for its ith row and jth column, respectively.\nAssumptions. In the remainder of this paper, we will seek\nan attention matrix A given by:\nA = exp\n([D∑\nd=1\nqmdPd(m,n)knd\n]\nmn\n/\n√\nD\n)\n, (8)\nwhere {Pd}D\nd=1 are position kernels . Deﬁning Pd ≡\n[Pd(m,n)]mn, this can be written in matrix form as:\nA = exp\n(D∑\nd=1\ndiag\n(\nq:,d\n)\nPddiag(k:,d)\n/√\nD\n)\n, (9)\nwhich is understood as having Dpositional attention tem-\nplates Pd jointly activated by the queries q:,d and keys k:,d.\nOriginal RPE (7) can be seen as a special case, where some\nentries are kept constant.\nPositional attention as covariance. The key idea for SPE\nis to see the attention kernel Pd(m,n) as a covariance:\n(∀M,N) (∀m,n) Pd(m,n) =E\n[\nQd(m)Kd(n)\n]\n, (11)\nwhere Qd(m) and Kd(n) are two real and zero-mean ran-\ndom variables, which will be chosen with the single condi-\ntion that their covariance function matchesPd. Semantically,\nthey should be understood as (randomly) encoding position\nmfor queries and position nfor keys, respectively. When\nmultiplied together as in dot-product attention, they yield\nthe desired attention template Pd(m,n) on average. The\ncentral intuition is that the actual positional encodings do\nnot matter as much as their dot-product.\nIn what follows, we will impose speciﬁc structures on\nthe cross-covariance Pd(m,n), which will in turn allow\nus to design random processes Qd = {Qd(m)}m∈Mand\nKd = {Kd(n)}n∈N such that (11) holds. The core advan-\ntage of this construction is to allow for Pd to be factorized.\nLet us for now assume that we construct the distributions of\n{Qd(m),Kd(n)}d in such a way that we can sample from\nthem (we will see how in section 2.1) and consider Rinde-\npendent realizations of them for given Mand N, gathered\nin the M ×Rand N ×Rmatrices Qd and Kd:\nQd ≡[qd,m,r ∼Qd(m)]mr, Kd ≡[kd,n,r ∼Kd(n)]nr.\n(12)\nFor large R, by the law of large numbers, we obtain:\nPd ≈\n[\nQdK\n⊤\nd\n]\n/R. (13)\nThis leads A in (9) to be given by:\nA ≈exp\n(D∑\nd=1\ndiag\n(\nq:,d\n)QdK\n⊤\nd\nR diag(k:,d)/\n√\nD\n)\n(14)\n≈exp\n(D∑\nd=1\ndiag\n(\nq:,d\n)\nQd\n)(D∑\nd=1\ndiag(k:,d)Kd\n)⊤\nR\n√\nD\n.\n(15)\nHere, a crucial observation is that for large R, the cross-\nterms QdK\n⊤\nd′̸=d are negligible due to independence, pro-\nvided that the means of the processes are selected to be zero.\nFinally, picking queries and keys as:\nˆQ ←\nD∑\nd=1\ndiag\n(\nq:,d\n)\nQd/\n4√\nDR, (16)\nˆK ←\nD∑\nd=1\ndiag(k:,d)Kd/\n4√\nDR, (17)\nwe see from (15-17) that we get back to the usual multi-\nplicative scheme (2) with A = exp(ˆQˆK\n⊤\n/\n√\nR), where the\nqueries/keys now have dimension Rand can be used in (10)\nto directly get outputs without computing A.\nThe procedure is summarized in Algorithm 1: we provide a\nway (16-17) to achieve PE in the keys domain, such that the\ndesired model (8) is enforced in the attention domain, pa-\nRelative Positional Encoding for Transformers with Linear Complexity\nrameterized by the attention kernelsPd. Interestingly, this is\ndone without ever computing attention matrices, complying\nwith O(N) Transformers. The remaining challenge, which\nwe discuss next, is to generate Qd and Kd enforcing (13).\n2.1. Drawing Stochastic Positional Encodings\nInspecting (11), we notice that our objective is to draw\nsamples from D pairs of centered random processes{\nQd,Kd\n}\nd, with a prescribed cross-covariance structure\nPd. It is reasonable to use Gaussian processes for this\npurpose (Williams & Rasmussen, 2006), which have the\nmaximum entropy for known mean and covariance. Such\ndistributions are frequently encountered in geophysics in the\nco-kriging literature (Matheron, 1963; Genton & Kleiber,\n2015), where scientists routinely handle correlated random\nﬁelds. The particular twists of our setup are: we have a gen-\nerative problem, e.g. as in V oˇrechovsk´y (2008); however, as\nopposed to their setting, we are not directly interested in the\nmarginal covariance function of each output, provided that\nthe desired cross-covariance structure holds.\nThe most straightforward application of SPE arises when\nwe pick Pd(m,n) = Pd(m−n), i.e. a stationary posi-\ntion kernel, which was coined in as choosing relative at-\ntention in Shaw et al. (2018) and boils down to enforc-\ning a Toeplitz structure for the cross-covariance matrix\nPd ≡[Pd(m−n)]m,n between Qd and Kd.\nWe propose two variants of SPE to handle this important\nspecial case, illustrated in Figure 2. The ﬁrst variant yields\nperiodic covariance functions. It can be beneﬁcial when-\never attention should not vanish with large lags, as in trafﬁc\nprediction (Xue & Salim, 2020) or, as we show, in music\ngeneration. The second variant generates vanishing covari-\nance functions; a concept which has recently been shown\nuseful (Wang et al., 2021), and notably yields smaller vali-\ndation losses in some of our experiments.\nVariant I. Relative and periodic attention (sineSPE).\nIn our ﬁrst approach, we consider the case where Pd is\nperiodic, which gets a convenient treatment. We assume:\nPd(m,n) =\nK∑\nk=1\nλ2\nkdcos(2πfkd(m−n) +θkd) , (18)\nwhere K ∈N is the number of sinusoidal components and\nfd ∈[0 1]K, θd ∈[−π π]K and λd ∈RK gather their K\nfrequencies, phases, and weights, respectively. By using the\nmatrix notation, we can rewrite (18) as:\nPd = Ω(M,fd,θd) diag\n(\n¨λd\n)2\nΩ(N,fd,0)⊤, (19)\nwhere ¨v ≡\n[\nv⌊p/2⌋\n]\np ∈R2K denotes a twice upsampled\nversion of a vector v ∈RK, ⌊·⌋denotes the ﬂoor operation,\nand for an index set I, Ω(I,a,b) is a matrix of size |I|×\n2K, with entries (0-based indexing):\n[Ω (I,a,b)]nl =\n{\ncos(2πakn+ bk) if l= 2k\nsin(2πakn+ bk) if l= 2k+ 1\nIt can be shown that if θd = 0 and M= N, we get back to\nthe (unique) Vandermonde decomposition for positive deﬁ-\nnite Toeplitz matrices3 (Yang et al., 2016), which boils down\nin our context to assuming that ∀τ,Pd(0) ≥Pd(τ). Since\nthis is not always desirable, we keep the more general (19).\nAt this point, we can easily build Qd and Kd. We draw a\n2K ×R matrix Zd with independent and identically dis-\ntributed (i.i.d.) Gaussian entries of unit variance, and deﬁne:\nQd ←Ω(M,fd,θd) diag\n(\n¨λd\n)\nZd/\n√\n2K, (20)\nKd ←Ω(N,fd,0) diag\n(\n¨λd\n)\nZd/\n√\n2K. (21)\nIt is easy to check that such a construction leads to (13). Its\nparameters are {fd,θd,Λd}d, which can be trained through\nstochastic gradient descent (SGD) as usual.\nVariant II. Relative (vanishing) attention with regular\nsampling (convSPE). Due to their periodic structure,\nthe covariance functions generated by Variant I are non-\nvanishing. Yet, our framework is ﬂexible enough to allow\nfor vanishing covariance structures, which may be more\ndesirable depending on the application (Wang et al., 2021).\nAs opposed to Variant I, where we imposed a speciﬁc struc-\nture on Pd, we will now follow an indirect approach, where\nPd will be implicitly deﬁned based on our algorithmic con-\nstruction. In this case, we assume that the signals are regu-\nlarly sampled (typical in e.g. text, images, audio), and we\nwill exploit the structure of Gaussian random matrices and\nbasic properties of the convolution operation.\nFor ease of notation, we assume self attention, i.e. M=\nN. Let {ΦQ\nd ,ΦK\nd }d denote a collection of ﬁlters, which\nwill ultimately be learned from training data. The size and\nthe dimension of these ﬁlters can be chosen according to\nthe input data (i.e. can be vectors, matrices, tensors). We\nthen propose the following procedure, which leads to a\nToeplitz Pd by means of convolutions:\n• We ﬁrst draw an M ×R random matrix Zd with i.i.d.\nstandard Gaussian entries. For multidimensional signals,\nZd gathers Rrandom vectors, matrices, cubes, etc.\n• The desired Qd and Kd are obtained by convolving Zd\nwith respective ﬁlters ΦQ\nd and ΦK\nd :\nQd = Zd ∗ΦQ\nd , Kd = Zd ∗ΦK\nd , (22)\nwhere ∗denotes convolution with appropriate dimension\n(e.g. 1D, 2D or 3D). Using convolutions with ﬁnite ﬁlters\n3If Pd ⪰0 and K ≥N, (19) still holds but is not unique.\nRelative Positional Encoding for Transformers with Linear Complexity\nsinusoidal convolutional \n gating \ngating\nlayer l\nFigure 2.(left) Generation of Q and K in SPE, which approximate the templates Pd when multiplied together. (right) Q and K can be\nshared across layers. At each layer l, different gating is (optionally) used, before applying (16-17) to generate new queries Q and keys K.\nensures vanishing covariance, as proven in the appendix.\nDue to the independence of the entries of Zd, for large R,\nthe product ZdZ⊤\nd/Rwill tend to the identity matrix. Given\nthe fact the convolution operations in (22) can be equiva-\nlently expressed as a multiplication by triangular Toeplitz\nmatrices constructed from the respective ﬁlters, it can be\nshown that, as R →∞, 1\nRQdK\n⊤\nd tends to the product of\ntwo triangular Toeplitz matrices. Hence, by using the prop-\nerties of triangular Toeplitz matrices (cf. Kucerovsky et al.\n2016, Theorem 2.4), we conclude that, as R →∞, our\nconstruction yields a Toeplitz matrix Pd as desired.\nThis approach is parameterized by the ﬁlters {ΦQ\nd,ΦK\nd }d,\nwhich will be learned from training data through SGD.\nThe variety of attention patterns P(m−n) that can be ob-\ntained directly depends on the kernel sizes, which is a classi-\ncal result from signal processing (Vetterli et al., 2014). Cas-\ncading several convolutions as in the VGGNet (Simonyan\n& Zisserman, 2014) may be a convenient way to augment\nthe expressive power of this convolutional SPE variant.\nFrom a more general perspective, the two operations in (22)\ncan be understood as producing PE through ﬁltering white\nnoise, which is the core idea we introduce for PE. Other\nclassical signal processing techniques may be used like\nusing inﬁnite impulse response ﬁlters. Such considerations\nare close to the ideas proposed in (Engel et al., 2020).\nTo summarize, the core difference between the two pro-\nposed constructions (20-21) and (22) lies in the behaviour\nof RPE beyond a maximum lag, implicitly deﬁned through\nthe frequencies fd for (20-21) and through the sizes of the\nﬁlters for (22). While the sinusoidal construction leads to a\nperiodic RPE, the ﬁltering construction leads to a vanishing\nRPE, which is calledmonotonic in (Wang et al., 2021). Both\nmay be the desired option depending on the application.\n2.2. Gated SPE\nAlthough RPE and the generalization (9) we propose are\nnovel and efﬁcient strategies to handle position information,\nit may be beneﬁcial to also allow for attention coefﬁcients\nthat are computed without positional considerations, simply\nthrough ⟨qm,kn⟩. As a general gating mechanism, we\npropose to weight between positional and non-positional\nattention through a gate parameter δd ∈[0 1]:\nPd ≡[δd + (1−δd)Pd(m,n)]m,n. (23)\nThis gating scheme can be implemented simply by augment-\ning Qd and Kd generated as above through:\nqd,m ←\n√\n1 −δdqd,m +\n√\nδdϵd, (24)\nkd,m ←\n√\n1 −δdkd,m +\n√\nδdϵd, (25)\nwhere ϵd ∈RR in (24) and (25) is the same and has i.i.d.\nstandard Gaussian entries.\nIn practice, we can share some SPE parameters across the\nnetwork, notably across layers, to strongly reduce comput-\ning time and memory usage. In our implementation,sharing\nmeans generating a single instance ofQ and K for each head,\non which a layer-wise gating is applied, before achieving\nPE through (16-17). This is illustrated in Figure 2.\nRelative Positional Encoding for Transformers with Linear Complexity\nTable 1.Long-Range Arena results (higher scores are better). Mean and standard deviation of accuracy over three runs is reported, except\nfor Performer with convolutional SPE, where only a single run was completed. For comparison, the best result reported by Tay et al.\n(2021), along with the name of the best-performing model (in parentheses), is included.\nListOps Text Retrieval Image\nBest result from Tay et al. (2021) 37.27 65.90 59.59 44.24\n(Reformer) (Linear Trans.) (Sparse Trans.) (Sparse Trans.)\nLinear Transformer-ReLU from Tay et al. 18.01 65.40 53.82 42.77\nPerformer-softmax (APE) 17.80±0.00 62.58 ±0.22 59.84 ±1.46 41.81 ±1.16\nPerformer-softmax +sineSPE 17.43±0.32 62.60 ±0.50 60.00 ±1.20 41.12 ±1.70\nPerformer-softmax +convSPE 17.80 60.94 57.22 40.06\nLinear Transformer-ReLU (APE) 17.58 ±1.01 63.98 ±0.05 58.78 ±0.93 42.25±0.01\nLinear Transformer-ReLU +sineSPE 17.80±0.00 64.09±0.62 62.39±0.59 41.21 ±1.18\nLinear Transformer-ReLU +convSPE 9.50±1.17 63.23 ±1.31 61.00 ±1.34 39.96 ±1.31\n3. Experiments\n3.1. Long-Range Arena\nExperimental setup. We evaluate the proposed method in\nthe Long-Range Arena (LRA; Tay et al., 2021), a benchmark\nfor efﬁcient Transformers, consisting of sequence classiﬁca-\ntion tasks with a focus on long-range dependencies. We use\nthe following tasks from this benchmark:\n• ListOps: parsing and evaluation of hierarchical expres-\nsions. a longer variant of (Nangia & Bowman, 2018);\n• Text: movie review sentiment analysis on the IMDB cor-\npus (Maas et al., 2011);\n• Retrieval: article similarity classiﬁcation on the All About\nNLP (AAN) corpus (Radev et al., 2013);\n• Image: object recognition on the CIFAR10 dataset\n(Krizhevsky, 2009) represented as pixel sequences.\nThe tasks are challenging due to the large sequence lengths,\ndeliberately increased by choosing a character-/pixel-level\nrepresentation. An overview of the tasks can be found in the\nappendix. We do not include Pathﬁnder (a synthetic image\nclassiﬁcation task) as we were unable to reproduce the re-\nsults of Tay et al. on this task, even through correspondence\nwith the authors.\nWe evaluate SPE (the gated variant) on two efﬁcient Trans-\nformer models: the (softmax) Performer (Choromanski\net al., 2020), and a Linear Transformer (Katharopoulos\net al., 2020) with a ReLU feature map, i.e. choosing\nφ(·) = max(0,·) element-wise in (3).4 It should be noted\nthat the ReLU feature map does not approximate the soft-\nmax kernel, which SPE is designed for (see assumption 8).\nNevertheless, it is possible to use SPE with any feature\nmap in practice, allowing us to include Linear Transformer-\nReLU as an interesting test of generalization to alternative\nkernels.\n4A model named ‘Performer’ is reported by Tay et al., but com-\nmunication with the authors revealed it to be in fact equivalent to\nour Linear Transformer-ReLU, as it does not use random features.\nTo avoid confusion, we refer to this model as such herein.\nWe adopt the conﬁguration of Tay et al., only changing the\nPE and the batch sizes/learning rates to allow training on\nlimited hardware with similar results. All other hyperpa-\nrameters are kept identical to the original LRA. It is worth\nnoting that the Image models are different from the rest\nin that they employ a single-layer network and only use\nthe ﬁrst position for prediction, dramatically limiting their\nability to beneﬁt from relative positional information.\nSince we observe some variation between different runs, we\ntrain and evaluate each model 3 times (except for Performer\nwith convolutional SPE, which is computationally more\ncostly) and report the mean and standard deviation of the\nresults.\nThe results of the benchmark are given in Table 1. The\naccuracies achieved by the baseline Linear Transformer-\nReLU (APE) are similar to or surpass those reported by Tay\net al., which is a clear validation of our experimental setup.\nDiscussion. Results on ListOps are poor overall, with accu-\nracies around 17 %. This complies with Tay et al. (2021),\nwho reasoned that “kernel-based models [e.g. Performer,\nLinear Transformers] are possibly not as effective on hier-\narchically structured data,” leaving room for improvement.\nWe also hypothesize this is largely due to some known is-\nsues with the training data for this task, which unfortunately\nhave not been ﬁxed at the time of this writing.5\nRegarding performance of SPE, we ﬁrst notice that the\nsineSPE variant yields the best results on three tasks,\nwhich is a strong achievement and validates our approach,\nespecially considering the difﬁculty of this evaluation bench-\nmark. While it is only marginally better than APE for\nListOps and Text, it is worth mentioning that sineSPE\ncombined with the Linear Transformer-ReLU yields an ac-\ncuracy improvement of ∼3 %on Retrieval compared to the\nbest result obtained by Tay et al. (2021).\n5Currently, the ofﬁcial data loader for ListOps inadvertently\nstrips some characters from the input sequences.\nRelative Positional Encoding for Transformers with Linear Complexity\nRegarding convSPE, its performance in the LRA is not\nas remarkable as it is for the music generation experiment\nreported later in section 3.2. This mitigated result appears\nsomewhat in contradiction with the discussion found in\nWang et al. (2021), which presents vanishing attention as a\ndesirable property of PE. On the contrary, we empirically ob-\nserve that our non-vanishing sinusoidal version sineSPE\ndoes behave better in these particular tasks.\nFinally, the superior results of APE on Image are not unex-\npected, given the limited ability of these models to exploit\nrelative positions. On the contrary, the relatively good per-\nformance of SPE on this task is in fact remarkable, espe-\ncially considering that the baseline systems for this task use\nlearnable APE.\nAs we will see later in our music generation experiments,\nthere are tasks where our proposed SPE clearly yields re-\nmarkable improvements. Here in the LRA, we notice that\nit does not result in an obvious and systematic boost in\nperformance. This raises interesting considerations:\n(i) The variance of the Monte Carlo estimator might be prob-\nlematic. We are enthusiastic about the elegant formulation\nof stochastic feature maps as in the Performer, which was\na strong inspiration. Still, we must acknowledge that their\ncomputation relies on a Monte Carlo estimator (15). We\nsuspect that the variance of the estimator might play a role\nin the ﬁnal performance in large dimensions, which opens\nup the direction of exploring variance-reduced estimation\nmethods, rather than plain Monte Carlo.\n(ii) LRA tasks might not beneﬁt from strong (R)PE schemes.\nThe LRA was designed to compare Transformer architec-\ntures, ﬁlling a gap in this domain and standing as the de\nfacto standard, justifying our choice. Still, although PE\nis known to be important in many cases, it is not known\nwhether it is so in the LRA tasks. We feel that there is room\nfor such a specialized comparison, which is scheduled in\nour future work, possibly leading to new long-range tasks\nwhere PE is critical.\n3.2. Pop Piano Music Generation\nIn our music generation experiments (this subsection and\nsection 3.3), music is represented as sequences of symbols\n(tokens) and a Performer (Choromanski et al., 2020) is used\nas an autoregressive language model, which predicts a prob-\nability distribution over the next token given the past context.\nAt test time, a new sequence is generated by iteratively sam-\npling the next token, as commonly done in text generation.\nExperimental setup. We train Performers for music gen-\neration, with 24 layers and 8 heads per layer on a dataset\ncomposed of 1 747pop piano tracks, encoded using the re-\ncently proposed Revamped MIDI-derived format (REMI;\nHuang & Yang, 2020). The sequences are composed of\nFigure 3.Validation cross-entropy vs. token position on pop piano\nmusic generation task. (lower is better; the black vertical line\nindicates the maximum position to which the models are trained.)\nmetrical tokens: bar, subbeat, and tempo, which rep-\nresent musical timing; and note tokens: chord, pitch,\nduration, and volume, which describe the musical con-\ntent (see the appendix for more details). We hold out 5% of\nthe songs as the validation set.\nWe train the models with sequence length N = 2 048, cor-\nresponding to ∼1 minute of music. The only difference\nbetween our models is the PE strategy. We consider base-\nline APE, as well as SPE: sinusoidal or convolutional, with\nor without gating, resulting in 5 different models.\nResults and discussion . For qualitative assessment, we\nﬁrst display in Figure 1 one attention pattern for each PE\nmodel: APE and (gated) sineSPE/convSPE, obtained as\nan average over 20 from-scratch generations for a chosen\n(layer, head). More similar plots can be found in appendix.\nInterestingly, we notice that for early layers, APE attention\ndoes not go much beyond training sequence length. This\nbehaviour is not found in SPE variants, which consistently\nattend to all positions. Another remarkable feature of the\nproposed model (only displayed in the appendix) is that\ngating as described in section 2.2 visually disables PE al-\ntogether for some layers/heads, in which case attention is\nglobal.\nSince the literature suggests that RPE improves general-\nization performance (Shaw et al., 2018; Zhou et al., 2019;\nRosendahl et al., 2019), we display validation cross-entropy\ncomputed with teacher forcing (Williams & Zipser, 1989)\nin Figure 3, as a function of the target token position. The\nvalues would indicate how well the models predict the token\nat a certain position given the preceding tokens, for tracks\nin the validation set. We notice that all SPE variants, espe-\ncially convSPE, behave much better than APE for token\npositions beyond 2 048. This suggests that SPE inherits this\ncelebrated advantage of RPE (Huang et al., 2018) while\nbeing applicable to much longer sequences.\nRecently, Wang et al. (2021) deﬁned metrics for the eval-\nuation of PE, suggesting that translation invariance and\nmonotonicity are desirable properties. The former states that\nthe distances of two arbitrary τ-offset position embeddings\nshould be identical, while the latter states that neighboring\nRelative Positional Encoding for Transformers with Linear Complexity\nFigure 4.PE evaluation metrics (Wang et al., 2021) for the pop\npiano music generation task in the 1st layer (lower is better), w.r.t.\nquery positions. Training sequence length is 2 048. Only query-\nkey offsets <128 are considered here. See appendix for details.\nFigure 5.Musical style similarity for groove continuation (higher\nis better) between output and initial prompt through two musically-\nmotivated metrics, as a function of time in the output. Each data\npoint corresponds to a single musical style.\npositions should be assigned with position embeddings that\nare closer than faraway ones. Following their identical word\nprobing methodology, we report these metrics in Figure 4.\nAs expected, SPE variants greatly outperform APE in terms\nof translation invariance. However, monotonicity does not\nseem a very relevant criterion in our music application, as\ncan be seen when comparing scores in Figures 3 and 4. It\nseems that music modeling can beneﬁt from non-vanishing\nattention patterns. In any case, SPE scores are remarkably\nstable across positions, contrarily to APE, which rapidly\ndegrades beyond the training length.\n3.3. Groove Continuation\nIn this experiment, we evaluate Performers on a groove\ncontinuation task. After training on a dataset where each\nexample has a uniform style (‘groove’), we prime the model\nwith a short prompt (2-bar musical fragment) and let it gen-\nerate a continuation. We then observe whether the generated\ncontinuation matches the style of the prompt.\nExperimental setup. The models (24-layer Performers\nwith 8 attention heads) are trained on an accompaniment\ndataset comprising 5 522samples in 2 761different musical\nstyles, encoded in a token-based format adopted from C´ıfka\net al. (2020) and detailed in the appendix. All SPE-based\nmodels use gating in this experiment. Unlike the previous\nexperiment, which leverages long training sequences, we\nconsider training sequences of length N = 512, correspond-\ning to 2–10 bars. At test time, the model is prompted with 2\nbars in a style not seen during training and new tokens are\nsampled to complete the sequence to a length of 1 024, i.e.\ntwice the training length.\nWe use two musically motivated style similarity metrics –\ntime-pitch and onset-duration proposed by C ´ıfka et al.\n(2019; 2020) – to quantify the similarity of the generated\ncontinuation to the prompt. When listening to the generated\nmusic, we perceptually notice a drift in quality along time.\nFor this reason, we divide each generated sample into four\nsuccessive chunks of identical duration and evaluate them\nindependently. The results are displayed in Figure 5.\nDiscussion. We clearly see that SPE substantially outper-\nforms APE in both metrics. Although APE visibly does\nmanage to generate close to the desired style at the begin-\nning of the sequence, this similarity strongly degrades over\ntime. Both sineSPE and convSPE are much more stable\nin this regard, conﬁrming the result from section 3.2 that\nSPE extrapolates better beyond the training sequence length.\nThis matches our informal perceptual evaluation.6\nThis experiment suggests that exploiting a local neighbor-\nhood is a robust way to process long sequences. This could\nappear as contradicting the use of long-range Transformers,\nbut we highlight that gating is used here, enabling some\nheads to exploit long term-attention independently from po-\nsition. Further comparisons with local attention schemes\n(e.g. Dai et al., 2019; Hofst ¨atter et al., 2020) could be in-\nteresting, although they were not included here due to Tay\net al. (2021) suggesting that they are clearly inferior, at least\nin the LRA setting.\n4. Related Work\nThis paper is concerned with PE (Sukhbaatar et al., 2015),\nas a way to embed the position of each token as part of its\nfeatures. This idea is a core ingredient for many subsequent\ngroundbreaking studies (Gehring et al., 2017; Vaswani et al.,\n2017), and has been the actual topic of many investigations.\n6Examples: https://cifkao.github.io/spe/\nRelative Positional Encoding for Transformers with Linear Complexity\nAbsolute Positional Encoding (APE) based on sinusoids\nfrom Vaswani et al. (2017) is the most widely used for\nTransformer-like architectures. However, PE q(n) and k(n)\nin (5) can also be trained as in BERT (Devlin et al., 2019;\nLiu et al., 2019). Although the original Transformer only\nincludes PE at the input layer, it may be included at all\nlayers (Dehghani et al., 2019; Lan et al., 2020).\nRelative positional encoding (RPE;Shaw et al., 2018) is a\nway to leverage relative positions. It came with a O(N2D)\nspace complexity, which was reduced to O(N2) in Huang\net al. (2018); He et al. (2020). Considering log-distances\nwas proposed in Raffel et al. (2020). Several variants for\nRPE were introduced (Huang et al., 2020; Wang et al., 2021).\nThey all apply learned RPE in the attention domain. Using\nﬁxed embedding functions was also considered for RPE\n(Pham et al., 2020), and masking RPE is used in Kim et al.\n(2020) to promote local attention.\nKeys-domain vs attention domain. Doing PE in the keys\ndomain introduces position-content cross terms that are ad-\nvocated as noisy and not beneﬁcial in Ke et al. (2020) and\nreplaced by Untied attention, i.e. PE in the attention domain.\nThis is also called disantangled attention in He et al. (2020)\nand already proposed in Tsai et al. (2019) through separa-\nble content-position attention kernels. All of these studies\nrequire the explicit computation and storage of A.\nNon-integer positions were considered for structured in-\nputs. Tree-based PE was proposed both for APE (Shiv &\nQuirk, 2019; Xiao et al., 2019; Ma et al., 2019) and RPE\n(Omote et al., 2019). Positional encoding of robots within\narbitrary polygons is found in Bose et al. (2019).\nDynamical models for PE. Attention for machine transla-\ntion was introduced in Bahdanau et al. (2016), which was\nretrospectively understood in Ke et al. (2020) as using re-\ncurrent neural nets (RNN) for PE. In Chen et al. (2018), the\nhidden states of encoder RNNs are said to contain enough\nposition information to skip explicit PE. Neishi & Yoshinaga\n(2019) builds on this view, but explicitly describes the idea\nfor the ﬁrst time. Their contribution is to replace the additive\nPE in (5) by an RNN. In the same vein, Liu et al. (2020)\ngenerates PE using (neural) ordinary differential equations.\nConvolutional contexts. Our convSPE variant involves\nconvolving random noise. First, this can be related to Mo-\nhamed et al. (2019), who use convolutional neural networks\nfor queries and keys computation. Second, the connections\nbetween convolutions and stationary processes have recently\nbeen highlighted by Xu et al. (2020) as enforcing PE.\nMultiplicative PE. Various levels of content-position inter-\nactions are formalized in (Tsai et al., 2019). Multiplicative\nstrategies were proposed for both RPE (Huang et al., 2020)\nand APE (Dai et al., 2019). The latter was generalized in\nTsai et al. (2019). All these require the explicit computa-\ntion of the attention matrix. Wang et al. (2020a) presents a\nscheme that is close to our sinusoidal variant, but without\nthe stochastic part that is the key to go from (14) to (15).\nThe limits of APE and RPE were highlighted by some au-\nthors. In Wang & Chen (2020), the best performing models\nexploit both absolute and relative positions. In Irie et al.\n(2019) and Tsai et al. (2019), it is found that removing APE\naltogether in the causal decoder part of Transformer-based\narchitectures leads to comparable/better performance. It is\nalso not clear which one is best between incorporating PE\nin the raw input signal (and hence propagating it through\nthe value entries) or using it anew on the queries and keys\nonly, as we do. Our choice is backed by Tsai et al. (2019).\n5. Conclusion\nWe propose a new Stochastic Positional Encoding (SPE),\nbased on ﬁltering random noise. As we show, the proce-\ndure generalizes relative PE and is a principled means to\nenforce any prescribed (but trained) cross-covariance struc-\nture, which we demonstrated should be the central concern\nin dot-product attention. In our experiments, we show that\nSPE brings an interesting gain in performance for large-scale\ntransformer models (Choromanski et al., 2020; Katharopou-\nlos et al., 2020), as compared to classical (sinusoidal) PE.\nThis was expected, because RPE (Shaw et al., 2018) is often\nadvocated as beneﬁcial. However, no way to incorporate it\nfor long sequences was available so far and this is the core\ncontribution of this paper. The natural future directions for\nour study are (i) Signal-dependent PE that incorporates the\ninput sequence as an additional input for SPE, (ii) Nonsta-\ntionary PE that utilizes both relative and absolute positions,\n(iii) Extending our approach to arbitrary attention kernels,\ne.g. deﬁned implicitly through their (random) mappings as\nin (4). Indeed, SPE as it is presented here holds theoretically\nfor dot-product attention kernels only, but our results given\nin Table 1 suggest that this generalizes, asking an interesting\nresearch question.\nAcknowledgements\nThis work was supported by the European Union’s Hori-\nzon 2020 research and innovation programme under the\nMarie Skłodowska-Curie grant agreement No. 765068 (MIP-\nFrontiers) and in part by the French government under man-\nagement of Agence Nationale de la Recherche as part of\nthe “Investissements d’avenir” program, reference ANR-19-\nP3IA-0001 (PRAIRIE 3IA Institute).\nWe would like to thank Yi Tay, Mostafa Dehghani and Philip\nPham for their help with troubleshooting the Long-Range\nArena, and Krzysztof Choromanski for clariﬁcations about\nthe Performer.\nRelative Positional Encoding for Transformers with Linear Complexity\nReferences\nAlQuraishi, M. AlphaFold at CASP13. Bioinformatics, 35\n(22):4862–4865, 2019.\nBahdanau, D., Cho, K., and Bengio, Y . Neural machine\ntranslation by jointly learning to align and translate.\narXiv:1409.0473 [cs, stat], May 2016. URL http://\narxiv.org/abs/1409.0473. arXiv: 1409.0473.\nBello, I. LambdaNetworks: Modeling long-range inter-\nactions without attention. In Proc. Int. Conf. Learning\nRepresentations, 2020.\nB¨ock, S., Korzeniowski, F., Schl ¨uter, J., Krebs, F., and\nWidmer, G. Madmom: A new Python audio and music\nsignal processing library. In Proc. ACM International\nMultimedia Conf., pp. 1174–1178, 2016.\nBose, K., Adhikary, R., Kundu, M. K., and Sau, B.\nPositional encoding by robots with non-rigid move-\nments. arXiv:1905.09786 [cs] , May 2019. URL\nhttp://arxiv.org/abs/1905.09786. arXiv:\n1905.09786.\nChen, M. X., Firat, O., Bapna, A., Johnson, M., Macherey,\nW., Foster, G., Jones, L., Parmar, N., Schuster, M.,\nChen, Z., Wu, Y ., and Hughes, M. The best of both\nworlds: Combining recent advances in neural machine\ntranslation. arXiv:1804.09849 [cs], April 2018. URL\nhttp://arxiv.org/abs/1804.09849. arXiv:\n1804.09849.\nChild, R., Gray, S., Radford, A., and Sutskever, I.\nGenerating long sequences with sparse Transform-\ners. arXiv:1904.10509 [cs, stat] , April 2019. URL\nhttp://arxiv.org/abs/1904.10509. arXiv:\n1904.10509.\nChoromanski, K., Likhosherstov, V ., Dohan, D., Song, X.,\nGane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin,\nA., Kaiser, L., Belanger, D., Colwell, L., and Weller, A.\nRethinking attention with Performers. arXiv:2009.14794\n[cs, stat] , September 2020. URL http://arxiv.\norg/abs/2009.14794. arXiv: 2009.14794.\nC´ıfka, O.,S ¸ims ¸ekli, U., and Richard, G. Supervised sym-\nbolic music style translation using synthetic data. In\nProc. International Society for Music Information Re-\ntrieval Conf., pp. 588–595, 2019. doi: 10.5281/zenodo.\n3527878. URL https://doi.org/10.5281/\nzenodo.3527878.\nC´ıfka, O.,S ¸ims ¸ekli, U., and Richard, G. Groove2Groove:\nOne-shot music style transfer with supervision from syn-\nthetic data. IEEE/ACM Transactions on Audio, Speech\nand Language Processing , 28:2638–2650, 2020. doi:\n10.1109/TASLP.2020.3019642. URL https://hal.\narchives-ouvertes.fr/hal-02923548.\nDai, Z., Yang, Z., Yang, Y ., Carbonell, J., Le, Q. V ., and\nSalakhutdinov, R. Transformer-XL: Attentive language\nmodels beyond a ﬁxed-length context. arXiv preprint\narXiv:1901.02860, 2019.\nDehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and\nKaiser, L. Universal Transformers. arXiv:1807.03819\n[cs, stat], March 2019. URL http://arxiv.org/\nabs/1807.03819. arXiv: 1807.03819.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K.\nBERT: Pre-training of deep bidirectional Transformers\nfor language understanding. arXiv:1810.04805 [cs] ,\nMay 2019. URL http://arxiv.org/abs/1810.\n04805. arXiv: 1810.04805.\nDonahue, C., Mao, H. H., Li, Y . E., Cottrell, G. W., and\nMcAuley, J. Lakhnes: Improving multi-instrumental mu-\nsic generation with cross-domain pre-training. In ISMIR,\n2019.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\nHeigold, G., Gelly, S., et al. An image is worth 16x16\nwords: Transformers for image recognition at scale.arXiv\npreprint arXiv:2010.11929, 2020.\nEngel, J., Hantrakul, L., Gu, C., and Roberts, A. Ddsp:\nDifferentiable digital signal processing. arXiv preprint\narXiv:2001.04643, 2020.\nGehring, J., Auli, M., Grangier, D., Yarats, D., and\nDauphin, Y . N. Convolutional sequence to sequence\nlearning. arXiv:1705.03122 [cs] , July 2017. URL\nhttp://arxiv.org/abs/1705.03122. arXiv:\n1705.03122.\nGenton, M. G. and Kleiber, W. Cross-covariance functions\nfor multivariate geostatistics. Statistical Science, pp. 147–\n163, 2015.\nHawthorne, C., Elsen, E., Song, J., Roberts, A., Simon, I.,\nRaffel, C., Engel, J., Oore, S., and Eck, D. Onsets and\nFrames: Dual-objective piano transcription. In Proc. Int.\nSociety for Music Information Retrieval Conf., 2018.\nHe, P., Liu, X., Gao, J., and Chen, W. DeBERTa:\nDecoding-enhanced BERT with disentangled atten-\ntion. arXiv:2006.03654 [cs] , June 2020. URL\nhttp://arxiv.org/abs/2006.03654. arXiv:\n2006.03654.\nHofst¨atter, S., Zamani, H., Mitra, B., Craswell, N., and\nHanbury, A. Local self-attention over long text for ef-\nﬁcient document retrieval. In Proceedings of the 43rd\nInternational ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, pp. 2021–2024,\n2020.\nRelative Positional Encoding for Transformers with Linear Complexity\nHoltzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y .\nThe curious case of neural text degeneration. In Proc.\nInternational Conference on Learning Representations,\n2019.\nHsiao, W.-Y ., Liu, J.-Y ., Yeh, Y .-C., and Yang, Y .-H. Com-\npound Word Transformer: Learning to compose full-song\nmusic over dynamic directed hypergraphs. In Proc. AAAI\nConf. Artiﬁcial Intelligence, 2021.\nHuang, C.-Z. A., Vaswani, A., Uszkoreit, J., Shazeer,\nN., Simon, I., Hawthorne, C., Dai, A. M., Hoffman,\nM. D., Dinculescu, M., and Eck, D. Music Transformer.\narXiv:1809.04281 [cs, eess, stat], December 2018. URL\nhttp://arxiv.org/abs/1809.04281. arXiv:\n1809.04281.\nHuang, Y .-S. and Yang, Y .-H. Pop Music Transformer:\nGenerating music with rhythm and harmony. In Proc.\nACM International Multimedia Conf., 2020.\nHuang, Z., Liang, D., Xu, P., and Xiang, B. Improve Trans-\nformer models with better relative position embeddings.\narXiv preprint arXiv:2009.13658, 2020.\nIrie, K., Zeyer, A., Schl ¨uter, R., and Ney, H. Lan-\nguage modeling with deep Transformers. Proc. Inter-\nspeech, pp. 3905–3909, 2019. doi: 10.21437/Interspeech.\n2019-2225. URL http://arxiv.org/abs/1905.\n04226. arXiv: 1905.04226.\nKatharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.\nTransformers are RNNs: Fast autoregressive Transform-\ners with linear attention. In Proc. Int. Conf. Machine\nLearning, pp. 5156–5165, 2020.\nKe, G., He, D., and Liu, T.-Y . Rethinking positional en-\ncoding in language pre-training. arXiv:2006.15595 [cs],\nJuly 2020. URL http://arxiv.org/abs/2006.\n15595.\nKim, J., El-Khamy, M., and Lee, J. T-GSA: Trans-\nformer with Gaussian-weighted self-attention for speech\nenhancement. arXiv:1910.06762 [cs, eess] , Febru-\nary 2020. URL http://arxiv.org/abs/1910.\n06762. arXiv: 1910.06762.\nKrizhevsky, A. Learning multiple layers of features from\ntiny images. Technical report, University of Toronto,\n2009.\nKucerovsky, D., Mousavand, K., and Sarraf, A. On\nsome properties of toeplitz matrices. Cogent Math-\nematics, 3(1), 2016. doi: 10.1080/23311835.\n2016.1154705. URL http://doi.org/10.1080/\n23311835.2016.1154705.\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P.,\nand Soricut, R. ALBERT: A lite BERT for self-supervised\nlearning of language representations. arXiv:1909.11942\n[cs], February 2020. URL http://arxiv.org/\nabs/1909.11942. arXiv: 1909.11942.\nLiu, X., Yu, H.-F., Dhillon, I., and Hsieh, C.-J. Learn-\ning to encode position for Transformer with continu-\nous dynamical model. arXiv:2003.09229 [cs, stat] ,\nMarch 2020. URL http://arxiv.org/abs/\n2003.09229. arXiv: 2003.09229.\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,\nLevy, O., Lewis, M., Zettlemoyer, L., and Stoyanov,\nV . RoBERTa: A robustly optimized BERT pretrain-\ning approach. arXiv:1907.11692 [cs], July 2019. URL\nhttp://arxiv.org/abs/1907.11692. arXiv:\n1907.11692.\nMa, C., Tamura, A., Utiyama, M., Sumita, E., and Zhao,\nT. Improving neural machine translation with neural\nsyntactic distance. In Proc. Conf. North American Chap-\nter of the Association for Computational Linguistics, pp.\n2032–2037, 2019. doi: 10.18653/v1/N19-1205. URL\nhttp://aclweb.org/anthology/N19-1205.\nMaas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng,\nA. Y ., and Potts, C. Learning word vectors for sen-\ntiment analysis. In Proc. Annual Meeting of the As-\nsociation for Computational Linguistics: Human Lan-\nguage Technologies, pp. 142–150, 2011. URL https:\n//www.aclweb.org/anthology/P11-1015.\nMatheron, G. Principles of geostatistics. Economic geology,\n58(8):1246–1266, 1963.\nMohamed, A., Okhonko, D., and Zettlemoyer, L. Trans-\nformers with convolutional context for asr.arXiv preprint\narXiv:1904.11660, 2019.\nNadaraya, E. A. On estimating regression. Theory of Prob-\nability & Its Applications, 9(1):141–142, 1964.\nNangia, N. and Bowman, S. ListOps: A diagnostic dataset\nfor latent tree learning. In Proc. Conf. North American\nChapter of the Association for Computational Linguistics:\nStudent Research Workshop, pp. 92–99, 2018. doi: 10.\n18653/v1/N18-4013. URL https://www.aclweb.\norg/anthology/N18-4013.\nNeishi, M. and Yoshinaga, N. On the relation between\nposition information and sentence length in neural ma-\nchine translation. In Proc. Conf. Computational Natural\nLanguage Learning, pp. 328–338, 2019. doi: 10.18653/\nv1/K19-1031. URL https://www.aclweb.org/\nanthology/K19-1031.\nRelative Positional Encoding for Transformers with Linear Complexity\nOmote, Y ., Tamura, A., and Ninomiya, T. Dependency-\nbased relative positional encoding for Transformer\nNMT. In Proc. Natural Language Processing in a Deep\nLearning World, pp. 854–861, 2019. ISBN 978-954-452-\n056-4. doi: 10.26615/978-954-452-056-4 099. URL\nhttps://acl-bg.org/proceedings/2019/\nRANLP2019/pdf/RANLP099.pdf.\nPham, N.-Q., Ha, T.-L., Nguyen, T.-N., Nguyen, T.-S.,\nSalesky, E., Stueker, S., Niehues, J., and Waibel, A. Rela-\ntive positional encoding for speech recognition and direct\ntranslation. arXiv:2005.09940 [cs, eess], May 2020. URL\nhttp://arxiv.org/abs/2005.09940.\nRadev, D. R., Muthukrishnan, P., Qazvinian, V ., and Abu-\nJbara, A. The ACL anthology network corpus. Lan-\nguage Resources and Evaluation, 47(4):919–944, January\n2013. doi: 10.1007/s10579-012-9211-2. URL https:\n//doi.org/10.1007/s10579-012-9211-2 .\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring the\nlimits of transfer learning with a uniﬁed text-to-text Trans-\nformer. arXiv:1910.10683 [cs, stat] , July 2020. URL\nhttp://arxiv.org/abs/1910.10683. arXiv:\n1910.10683.\nRosendahl, J., Tran, V . A. K., Wang, W., and Ney, H. Analy-\nsis of positional encodings for neural machine translation.\nIn Proc. IWSLT, 2019.\nRoy, A., Saffar, M., Vaswani, A., and Grangier, D. Efﬁcient\ncontent-based sparse attention with routing Transformers.\narXiv:2003.05997 [cs, eess, stat], October 2020. URL\nhttp://arxiv.org/abs/2003.05997. arXiv:\n2003.05997.\nShaw, P., Uszkoreit, J., and Vaswani, A. Self-attention with\nrelative position representations. arXiv:1803.02155 [cs],\nApril 2018. URL http://arxiv.org/abs/1803.\n02155. arXiv: 1803.02155.\nShen, Z., Zhang, M., Zhao, H., Yi, S., and Li, H.\nEfﬁcient attention: Attention with linear complexi-\nties. arXiv:1812.01243 [cs] , November 2020. URL\nhttp://arxiv.org/abs/1812.01243. arXiv:\n1812.01243.\nShiv, V . and Quirk, C. Novel positional encodings to enable\ntree-based Transformers. In Proc. Advances in neural\ninformation processing systems, 2019.\nSimonyan, K. and Zisserman, A. Very deep convolu-\ntional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014.\nSukhbaatar, S., Szlam, A., Weston, J., and Fergus, R. End-to-\nend memory networks. arXiv:1503.08895 [cs], Novem-\nber 2015. URL http://arxiv.org/abs/1503.\n08895. arXiv: 1503.08895.\nTay, Y ., Dehghani, M., Abnar, S., Shen, Y ., Bahri, D.,\nPham, P., Rao, J., Yang, L., Ruder, S., and Metzler, D.\nLong Range Arena: A benchmark for efﬁcient Trans-\nformers. In Proc. Int. Conf. Learning Representations,\n2021. URL https://openreview.net/forum?\nid=qVyeW-grC2k.\nTsai, Y .-H. H., Bai, S., Yamada, M., Morency, L.-P., and\nSalakhutdinov, R. Transformer dissection: An uniﬁed\nunderstanding for Transformer’s attention via the lens of\nkernel. In Proc. Conf. Empirical Methods in Natural Lan-\nguage Processing and Int. Joint Conf. Natural Language\nProcessing, pp. 4335–4344, 2019.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser,Ł., and Polosukhin, I. Attention\nis all you need. In Proc. Advances in neural information\nprocessing systems, pp. 5998–6008, 2017.\nVetterli, M., Kovaˇcevi´c, J., and Goyal, V . K.Foundations of\nsignal processing. Cambridge University Press, 2014.\nV oˇrechovsk´y, M. Simulation of simply cross correlated\nrandom ﬁelds by series expansion methods. Structural\nsafety, 30(4):337–363, 2008.\nWang, B., Zhao, D., Lioma, C., Li, Q., Zhang, P., and\nSimonsen, J. G. Encoding word order in complex em-\nbeddings. arXiv:1912.12333 [cs], June 2020a. URL\nhttp://arxiv.org/abs/1912.12333. arXiv:\n1912.12333.\nWang, B., Shang, L., Lioma, C., Jiang, X., Yang, H., Liu,\nQ., and Simonsen, J. G. On position embeddings in\nBERT. In Proc. Int. Conf. Learning Representations ,\n2021. URL https://openreview.net/forum?\nid=onxoVA9FxMw.\nWang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma,\nH. Linformer: Self-attention with linear complex-\nity. arXiv:2006.04768 [cs, stat] , June 2020b. URL\nhttp://arxiv.org/abs/2006.04768. arXiv:\n2006.04768.\nWang, Y .-A. and Chen, Y .-N. What do position embeddings\nlearn? An empirical study of pre-trained language model\npositional encoding. In Proc. Conf. Empirical Methods\nin Natural Language Processing, 2020.\nWatson, G. S. Smooth regression analysis. Sankhy¯a: The\nIndian Journal of Statistics, Series A, pp. 359–372, 1964.\nRelative Positional Encoding for Transformers with Linear Complexity\nWilliams, C. K. and Rasmussen, C. E. Gaussian processes\nfor machine learning, volume 2. MIT press Cambridge,\nMA, 2006.\nWilliams, R. J. and Zipser, D. A learning algorithm for con-\ntinually running fully recurrent neural networks. Neural\ncomputation, 1(2):270–280, 1989.\nWu, Q., Lan, Z., Gu, J., and Yu, Z. Memformer: The\nmemory-augmented Transformer. arXiv:2010.06891\n[cs], October 2020. URL http://arxiv.org/abs/\n2010.06891. arXiv: 2010.06891.\nXiao, F., Li, J., Zhao, H., Wang, R., and Chen, K.\nLattice-Based Transformer encoder for neural machine\ntranslation. arXiv:1906.01282 [cs], June 2019. URL\nhttp://arxiv.org/abs/1906.01282. arXiv:\n1906.01282.\nXu, R., Wang, X., Chen, K., Zhou, B., and Loy, C. C. Posi-\ntional encoding as spatial inductive bias in gans. arXiv\npreprint arXiv:2012.05217, 2020.\nXue, H. and Salim, F. D. Trailer: Transformer-based time-\nwise long term relation modeling for citywide trafﬁc ﬂow\nprediction. arXiv preprint arXiv:2011.05554, 2020.\nYang, Z., Xie, L., and Stoica, P. Vandermonde decomposi-\ntion of multilevel Toeplitz matrices with application to\nmultidimensional super-resolution. IEEE Transactions\non Information Theory, 62(6):3685–3701, 2016.\nZhou, P., Fan, R., Chen, W., and Jia, J. Improving gen-\neralization of transformer for speech recognition with\nparallel schedule sampling and relative positional embed-\nding. arXiv preprint arXiv:1911.00203, 2019.\nRelative Positional Encoding for Transformers with Linear Complexity\nSupplementary Material\nIntroduction\nThis document comprises additional information that could\nnot be included in the paper due to space constraints. It\nis structured as follows. In appendix A, we provide some\nfurther theoretical developments. In appendix B, we de-\ntail the experimental setup on the Long Range Arena. In\nappendix C, we detail our music generation experiments.\nFinally, we provide additional results in appendix D.\nOur source code is available at:\nhttps://github.com/aliutkus/spe/\nSee also the companion website:\nhttps://cifkao.github.io/spe/\nA. Theory\nA.1. Convolutional SPE Leads to Vanishing Attention\nFigure 6.If ΦQ\nd and ΦK\nd have length P, Qd and Kd for convolu-\ntional SPE depend on the noise Zd over the intervals [m −P : m]\nand [n −P : n], respectively. Their correlation depends only on\nthe shaded area, due to whiteness of Zd. Whenever |m −n|> P,\nthe two signals are uncorrelated.\nIn the main document, we claim that the convolutional vari-\nant leads to vanishing attention. We shortly prove this claim\nhere. For ease of notation, the proof is given in the 1D case,\nbut extends trivially to higher dimensions. The core idea is\nillustrated in Figure 6. Convolutional SPE yields:\nQd(m,r) =\nP∑\np=0\nZd(m−p,r)φQ\nd(p),\nKd(n,r) =\nP∑\np=0\nZd(n−p,r)φK\nd (p),\nwhere Zd is a white Gaussian noise process, i.e.\nE[Zd(m,r)Zd(m′,r)] =δmm′ . Omitting the dependency\non rfor notational convenience (all realizations are indepen-\ndent), we can compute the positional attention as:\nPd(m,n) =E\n[\nQd(m)Kd(n)\n]\n= E\n[ P∑\np,τ=0\nzd(m−τ)zd(n−p)φQ\nd(τ)φK\nd (p)\n]\n= E\n[ P∑\np=0\nzd(n−p)2φQ\nd(p+ m−n)φK\nd (p)\n]\n=\nP∑\np=0\nφQ\nd(p+ m−n)φK\nd (p),\nwhere only the (p,τ) values such thatn−p= m−τremain,\nall other cross terms E[Zd(m)Zd(m′̸= m)] disappearing\ndue to whiteness of Zd. Filters are taken as 0-valued outside\nof [0 :P]. As can be seen, whenever |m−n|>P , we get\nPd(m,n) = 0, because φK\nd (p+ (m−n)) = 0.\nA.2. Complexity\nIn this section, we detail the additional complexity caused\nby the proposed SPE method.\n• Sinusoidal SPE ﬁrst requires the computation of the\nmodulation matrices Ω for each feature dimension d =\n1 ...D , which has a O(2NK) complexity. Then, this\nmatrix must be multiplied by the noise matrix Zd with\nshape 2K ×R, leading to an overall complexity of\nO(DRNK2). Since K is typically very small in our\nexperiments, SineSPE can be seen as quite light in terms\nof both time and space complexity.\n• Convolutional SPE involves drawing a new noise signal\nzd,:,r of length N for each dand r, and convolving it with\nthe ﬁlters φQ\nd and φK\nd , whose length is written P.\nIn the 1D case, this leads to an overall time complexity of\nO(DRNP), which can be replaced byO(DRNlog N)\nSupplementary Material: Relative Positional Encoding for Transformers with Linear Complexity\nwhen operating the convolutions in the frequency domain,\nwhich is advantageous for long ﬁlters.\nIn higher dimensions, say 2D, this becomes\nO(DRN1N2P1P2) in the original domain and\nO(DRN1N2 log N1 log N2) in the frequency domain,\nwhere (N1,N2) and (P1,P2) are the shapes of noise and\nﬁlters, respectively.\n• The bottleneck of gating is the generation of random\nnoise ϵd, which has complexity O(DR).\nNote that this complexities of course must be multiplied by\nthe number of heads considered, up to 8 in our experiments.\nAs can be seen, the complexities of the sinusoidal and con-\nvolutional variants are similar, depending on the length P\nof the ﬁlters and the number Kof sinusoids.\nStill, other aspects come into the play. First, the convolu-\ntional version requires generating noise whose size scales\nas N, while the sinusoidal version requires much smaller\n2K-large noise matrices. Second, only a very small number\nof sinusoids was required in our experiments, whereas the\nconvolutional version required longer contexts, so that we\noften had 2K ≪P in practice. Finally, although this may\nchange in the near future, deep learning frameworks like Py-\nTorch do not easily integrate convolutions in the frequency\ndomain.\nSample-wise noise sharing. In practice, SPEs do not need\nto be drawn anew for each example. The most straightfor-\nward trick to reduce memory and computational footprint of\nthe method is to share Q and K among all examples in each\nmini-batch, as we do in all our experiments. This can bring\nsigniﬁcant memory savings when SPE is used as a drop-in\naddition to networks trained with large batch sizes.\nB. Experimental Setup: Long-Range Arena\nAn overview of the Long-Range Arena (Tay et al., 2021)\ntasks is given in table 2. We do not include Pathﬁnder\n(a synthetic image classiﬁcation task) or its harder variant\nPathﬁnder-X in this paper as we were unable to reproduce\nthe results of Tay et al. on this task. All the datasets are\ndescribed in detail in Tay et al. and available from the ofﬁcial\nLRA repository.7\nIn all LRA experiments, we employ gated SPE with R ∈\n{32,64}. We consistently use K = 10for sinusoidal (peri-\nodic) SPE and ﬁlters of length 128 for convolutional SPE.\nFor convolutional SPE, we share Q and K across all layers\n(but not across attention heads); for sinusoidal SPE, Q and\nK are unique to each layer and head; in both cases, layer-\nspeciﬁc gating is employed. Baseline experiments employ\nthe same absolute positional encodings as Tay et al. (learn-\n7https://github.com/google-research/\nlong-range-arena\nable APE for Image and sinusoidal APE for the remaining\ntasks). In models employing SPE, APE is removed.\nThe numbers of parameters of the models presented in the\nmain document are shown in Table 3. We can see that\nSPE-based models have at most 3.1 % more parameters than\nthe baselines. In the Image column, the numbers for SPE-\nbased models are about 50 % lower due to the fact that the\nbaselines on this task employ learnable APE.\nWe use code from the ofﬁcial LRA repository, including\nthe authors’ Transformer implementation, modiﬁed as nec-\nessary to incorporate SPE. We keep the same training con-\nﬁguration as provided by the LRA authors, but decrease\nthe batch sizes (from 256 to 96 for Image and from 32 to 8\nfor the rest) and learning rates so as to ﬁt within 16 GB of\nGPU memory. Our modiﬁed code and conﬁguration ﬁles\nare available in our source code repository.\nB.1. Resource usage\nThe typical training times of the LRA models are displayed\nin Table 4. Note that the times may not be comparable\nacross models or tasks due to evaluation (which may be\ntime-consuming) being done more frequently in some runs\nthan others.\nThe total training time was 1 405 h (189 runs in total), out of\nwhich 273 h (61 runs) were spent on attempts to reproduce\nthe results of Tay et al. (2021) using Performer-softmax,\nLinear Transformer-ReLU and vanilla Transformer. Some\nof these preliminary experiments were distributed over 1–3\nTesla V100 GPUs with 32 GB of memory each. The ﬁnal\nmodels were all trained on a single Tesla V100 or P100\nGPU with 16 GB of memory.\nC. Experimental Setup: Music Generation\nOur music Performers are implemented using the\npytorch-fast-transformers package,8 modiﬁed\nas necessary to incorporate SPE. The modiﬁed code and\nconﬁguration ﬁles are available in our code repository.\nAll models have 24 layers with model dimension 512, 8\nattention heads and 2 048 feed-forward units, which amount\nto ∼80 million trainable parameters. In models that use SPE,\nQ and K are shared across all layers (but not across attention\nheads); layer-speciﬁc gating is employed for models trained\nwith gated SPE.\nThe models are trained with the Adam optimizer. We sched-\nule the learning rate with linear warmup, followed by cosine\ndecay. Full details of hyperparameters can be found in the\nprovided conﬁguration ﬁles.\n8https://github.com/idiap/\nfast-transformers\nSupplementary Material: Relative Positional Encoding for Transformers with Linear Complexity\nTable 2.Long-Range Arena classiﬁcation tasks used in this paper.\nName Dataset Input Length Goal # classes\nListOps ListOps expression with operations on lists of digits 2 k evaluate expression 10\nText IMDB movie review as byte string 8 k classify sentiment 2\nRetrieval AAN pair of articles as byte strings 2 ×4 k detect citation link 2\nImage CIFAR10 8-bit gray-scale 32 ×32 image as byte string 1 k recognize object 10\nTable 3.Numbers of parameters of LRA models, identical for both Performer-softmax and Linear Transformer-ReLU.\nListOps Text Retrieval Image\nBaseline (APE) 19 982 858 3 486 722 1 087 618 248 458\n+ sineSPE 20 078 090 3 518 466 1 103 490 119 242\n+ convSPE 20 117 002 3 553 282 1 120 898 133 706\nC.1. Pop Piano Music Generation\nTraining data. The pop piano MIDI dataset we use is\nderived from the one provided in Hsiao et al. (2021), open-\nsourced on GitHub.9 It consists of 1,747 pure piano per-\nformances of various Japanese, Korean, and Western pop\nsongs, amounting to a total duration of ∼100 hours. All the\nsongs are in 4/4 time signature, namely four beats per bar\n(measure). We leave 5% (87 songs) as the validation set.\nAccording to Hsiao et al. (2021), the piano performances\nare originally collected from the Internet in the MP3 (audio)\nformat. Hsiao et al. further employed Onsets and Frames\npiano transcription (Hawthorne et al., 2018), madmom beat\ntracking tool (B¨ock et al., 2016), and chorder rule-based\nchord detection10 to transcribe the audio into MIDI format\nwith tempo, beat, and chord information.\nData representation. The representation adopted here is\nlargely identical to the Revamped MIDI-derived (REMI)\nencoding by Huang & Yang (2020), except that an extended\nset of chord tokens (described below) is used. REMI\nencodes a piano piece into a sequence composed of two\ntypes, metrical and note, of tokens. The metrical tokens are:\n• bar: Marks the start of a musical bar.\n• subbeat: Marks the musical timing within a bar. A bar\nis divided into 16 subbeats, which is equivalent to 4\nbeats. This symbolic timing provides an explicit time grid\nfor sequence models to model music.\n• tempo: Determines the pace (in beats per minute, or\nbpm) at which the piece is played, varied per bar. The\nrange of tempo tokens is [32,224] bpm, in steps of 3\nbpm for quantization.\n9https://github.com/YatingMusic/\ncompound-word-transformer\n10https://github.com/joshuachang2311/\nchorder\nThe note tokens are:\n• pitch: Marks a note played. The 88 pitch-es corre-\nspond to each key on the piano.\n• duration: Denotes the length of a played note, ranging\nfrom 1/2 to 16 subbeats, in steps of 1/2 subbeat.\n• volume (or, velocity): Denotes how loud a note is played.\nA total of 24 volume levels are considered.\n• chord: Marks a change on the accompanying chord.\nEach chord is described by its root note and quality,\ne.g., C-Maj7, E-min. A total of 133 distinct chord\ntokens are found in the dataset.\nPlease note that a single note played is represented by a co-\noccurring triple of ( pitch, duration, volume). The\naforementioned tokens constitute a vocabulary of size∼340\nfor our REMI encoding. On average, we need a sequence\nwith 5 300 tokens to represent a song.\nTraining and inference. In each training epoch, we ran-\ndomly crop a segment of length 2 048 from each sample,\nand shift the pitches of the entire segment by −6 to 6 semi-\ntones randomly (this is called transposition in music) as\ndata augmentation. We use batch size = 4, and set the learn-\ning rate to 0.0001 for APE and 0.0002 for all SPE models.\nFor sineSPE, we choose the number of sines K = 5; for\nconvSPE, the convolutional ﬁlter size is set to be 128, 512\nfor the gated and ungated variants respectively.\nDetailed resource usage of each model is shown in Table 5.\nDuring inference, we employ nucleus sampling (Holtzman\net al., 2019) with p= 0.9 and softmax temperature t= 1.2.\nNo post-processing on enforcing the grammatical correct-\nness of the generated sequence is done.\nValidation loss of the models trained on this task is listed in\nTable 6. On this metric, our convSPE variant performs the\nSupplementary Material: Relative Positional Encoding for Transformers with Linear Complexity\nTable 4.Training times for LRA models (hours). Numbers in parentheses are from Tesla P100 GPUs, the rest from Tesla V100 GPUs.\nListOps Text Retrieval Image\nPerformer-softmax 1.1 4.8 1.2 4.8\nPerformer-softmax + sineSPE (4.2) 11.7 2.9 5.0\nPerformer-softmax + convSPE 8.9 23.2 21.9 5.3\nLinear Transformer-ReLU 0.6 (3.2) 0.7 4.8\nLinear Transformer-ReLU + sineSPE 2.0 6.8 2.1 5.0\nLinear Transformer-ReLU + convSPE 15.0 18.6 19.0 5.3\nTable 5.Resource usage of models trained on pop piano music\ngeneration, on a Tesla V100 GPU with 32GB of memory. # of\nepochs and time to the checkpoint with the lowest validation loss\nare displayed. (ug: trained without SPE gating.)\n# epochs Time Memory\nAPE 72 9.74 h 14.34 GB\nsineSPE 78 17.92 h 29.80 GB\nsineSPE (ug) 78 16.31 h 18.29 GB\nconvSPE 80 28.02 h 30.01 GB\nconvSPE (ug) 68 24.76 h 18.99 GB\nTable 6.Validation cross-entropy for models trained for pop piano\nmusic generation (mean and standard deviation) over all sequences.\n(ug: trained without SPE gating). Trained: pos ≤2 048, Extrapo-\nlation: 2 048 < pos ≤3 072.\nPositions Trained Extrapolation\nAPE 1.721 ±0.148 3.215 ±0.200\nsineSPE 1.694 ±0.148 2.396 ±0.359\nsineSPE (ug) 1.754 ±0.146 1.965 ±0.170\nconvSPE 1.685 ±0.151 1.932 ±0.225\nconvSPE (ug) 1.733 ±0.145 1.805 ±0.163\nbest both within the trained positions and on extrapolation.\nC.2. Groove Continuation\nTraining data. The Groove2Groove MIDI dataset11 con-\nsists of accompaniments generated by the Band-in-a-Box\nsoftware (BIAB).12 We only use the training section of the\nGroove2Groove MIDI dataset and perform a custom train-\ning/validation/test split such that each section contains a\nunique set of BIAB styles (2 761 for training and 50 each\nfor validation and testing). The code necessary to download,\npre-process and split the dataset is included in the repository.\nWe convert each accompaniment to a trio consisting of bass,\n11http://doi.org/10.5281/zenodo.3958000\n12https://www.pgmusic.com/\ndrums and another randomly selected accompaniment track\n(e.g. piano, guitar). We then perform random data augmenta-\ntion by skipping measures at the beginning, dropping some\nof the instruments, and transposition (pitch-shifting by −5\nto +5 semitones). All randomization is done anew in each\nepoch.\nData representation. We use a representation similar to\nthe one proposed by C ´ıfka et al. (2020), but adapted to\na multi-track (multi-instrument) setting. Speciﬁcally, we\nencode a piece of music as a sequence of the following types\nof event tokens, each with two integer arguments:\n• note on(track, pitch): Begins a new note at the\ngiven pitch (0–127).\n• note off(track, pitch): Ends the note at the\ngiven pitch (0–127).\n• time shift(beats, offset): Advances current\ntime by a given number of beats and then sets the offset\nwithin the beat, given as the number of ticks from its\nbeginning (0–11). Maximum possible shift is (2, 0).\nThe track numbers range from 1 to 3, where 1 is always bass\nand 2 is always drums. The vocabulary of the model then\nconsists of 794 tokens (3×128 note-ons, 3×128 note-offs,\n24 time shifts, and 2 beginning-/end-of-sequence markers).\nThe main differences to the representation described in Sec-\ntion C.1 are a more compact encoding of timing, no repre-\nsentation of musical dynamics (for simplicity), and support\nfor multiple tracks (not originally proposed by C´ıfka et al.,\n2020 but introduced here inspired by Donahue et al., 2019).\nTraining and inference. During training, each example\nis pre-processed and encoded as described above and the\nresulting token sequence is truncated to a length of 512. We\ntrain each model for a total of 24 epochs.\nAt test time, we sample with a softmax temperature of 0.6.\nWe disallow sampling tokens that would result in invalid\nsequences (i.e. spurious note-offs, backward time shifts) in\norder to ensure that the generated sequence can be correctly\ndecoded.\nSupplementary Material: Relative Positional Encoding for Transformers with Linear Complexity\nVarious training details. Hyperparameter tuning was\nmostly performed in preliminary experiments (∼100 runs);\nthese were mostly done on other variants of the dataset and\nwith different sequence lengths (ranging from 256 to 20 k);\nthis includes experiments discarded due to bugs discovered\nduring or after training. Learning rates between 0.0001 and\n0.0008 and batch sizes between 1 and 24 were considered.\nFor SPE, we considered both the gated and ungated vari-\nants with as many realizations as ﬁt in memory (between\n16 and 64). Model selection was based on validation loss\nand informal perceptual evaluation. Only a minimal attempt\nat further learning rate tuning was made for the ﬁnal set of\nmodels with length 512, which did not appear to be particu-\nlarly sensitive to it, and we chose to keep the initial learning\nrate 0.0004, which was found to perform well in all cases.\nThe models included in the main document – APE,\nsineSPE and convSPE – all use a batch size of 10 and\nﬁnished training in about 3 h, 5 h and 6 h, respectively, using\n9.7 GB, 14.4 GB and 14.8 GB of GPU memory. The total\ntraining time including all preliminary experiments was 852\nhours.\nEvaluation metrics. We use the objective metrics pro-\nposed by C ´ıfka et al. (2019; 2020) to measure the style\nsimilarity between the generated continuation and the ﬁle\nfrom which the prompt was extracted. Given two pieces\nof music, each metric gathers musical event statistics of\nthe two pieces in histograms called style proﬁles, and then\ncomputes the cosine similarity between them.\nThe two metrics used here, onset-duration and time-pitch,\ndiffer in what kind of events they use to construct the style\nproﬁle:\n• The onset-duration proﬁle is deﬁned as a 2D histogram\nrelating note onset positions to note durations. More\nprecisely, for all notes ain a piece of music, it records a\ntuple of the form\n(start(a) mod 4, end(a) −start(a)) ∈[0,4) ×[0,2),\nwhere start(a) and end(a) refer to the onset and offset\ntime of ain beats. The expression start(a) mod 4then\nrepresents the position of the note onset relative to the\ncurrent bar, since all examples in the dataset are in a 4-\nbeat meter. These tuples are gathered in24×12 histogram\nbins (24 for onset time and 12 for duration).\n• The time-pitch proﬁle is also obtained as a 2D histogram,\nthis time capturing time differences and pitch differences\n(intervals) between notes. The tuples it considers have the\nform\n(start(b) −start(a), pitch(b) −pitch(a))\n∈[0,4) ×{−20,−19,..., 20}, a̸= b,\nwhere a,b is a pair of notes and pitch(·) represents the\npitch of a note as its MIDI note number (the number of\nsemitones from C−1). The histogram has 24 ×41 bins\n(24 for time lags between 0 and 4 beats and 41 bins for\nintervals between −20 and 20 semitones).\nIn both cases, the 2D histograms are ﬂattened to vectors\nbefore computing cosine similarities.\nD. Additional Results\nD.1. Attention Visualization: Music Generation\nIn this section, we display attention patterns produced by\nour pop piano music generation models.\nLearned positional templates. We share the SPE mod-\nules across all layers of the Performer, but not across the\nattention heads, resulting in 512 learned positional kernels\nPd) (number of heads ×key dimensions per head. In Figure\n7, we display 16 randomly picked resulting templatesPd for\nboth sineSPE and convSPE, trained with gating. Details\nof the two variants are:\n• sineSPE: We set the number of sines K = 5.\n• convSPE: We use ﬁlters of size 128.\nIn accordance with the deﬁnition, all of the visualizations\nare plotted with the equation Pd = QdK\n⊤\nd, which we never\nneed to explicitly compute for linear transformers. From\nFigure 7, we can observe that sineSPE learns to exploit a\nwide range of frequencies, and that convSPE is effective\nwithin small query-key offsets corresponding to the ﬁlter\nsize, as expected.\nFull Attention. Although the full attention matrix A is\nnot computed in linear transformers, we can still obtain\nit ofﬂine by multiplying queries and keys through either\nA = exp(QK⊤/\n√\nD) (in the case of APE, where Dis the\nkey dimensions per head), or A = exp(ˆQˆK\n⊤\n/\n√\nR) (in the\ncase of SPEs); then apply row-wise softmax operation on A\nas normalization.\nHere, we present the (softmax-ed) attention matrices in the\n1st, 3rd, 12th, 20th, and 24th (last) layers of all the ﬁve\nmodels trained on pop piano music generation in Figures 8–\n12. These are computed from one of each model’s random\nfrom-scratch music generations. To examine the models’\nextrapolation ability, we let them generate a sequence of\nlength 3 072, while the training sequence length is only\n2 048. The attention matrices are lower-triangular due to\ncausal masking. For better visualization, the color of each\npixel is adjusted through min{1,amn0.4/0.020.4}in the\nplots, where amn ∈[0,1] is the softmax-ed attention score.\nFigure 8 reveals a major drawback of APE: the attention of\nSupplementary Material: Relative Positional Encoding for Transformers with Linear Complexity\nHead 1, Dim 3\n Head 3, Dim 24\n Head 5, Dim 41\n Head 7, Dim 7\nHead 1, Dim 22\n Head 3, Dim 50\n Head 5, Dim 48\n Head 7, Dim 49\nHead 2, Dim 33\n Head 4, Dim 4\n Head 6, Dim 22\n Head 8, Dim 42\nHead 2, Dim 47\n Head 4, Dim 29\n Head 6, Dim 49\n Head 8, Dim 44\n(a) sineSPE\nHead 1, Dim 13\n Head 3, Dim 51\n Head 5, Dim 24\n Head 7, Dim 39\nHead 1, Dim 60\n Head 3, Dim 54\n Head 5, Dim 46\n Head 7, Dim 53\nHead 2, Dim 13\n Head 4, Dim 5\n Head 6, Dim 22\n Head 8, Dim 7\nHead 2, Dim 28\n Head 4, Dim 60\n Head 6, Dim 43\n Head 8, Dim 50 (b) convSPE\nFigure 7.Examples of Pd learned by SPE. X- and Y-axes arekey and query positions respectively. Max position = 2 048.\nLayer 1, Head 1\n Layer 1, Head 2\n Layer 1, Head 3\n Layer 1, Head 4\n Layer 1, Head 5\n Layer 1, Head 6\n Layer 1, Head 7\n Layer 1, Head 8\nLayer 3, Head 1\n Layer 3, Head 2\n Layer 3, Head 3\n Layer 3, Head 4\n Layer 3, Head 5\n Layer 3, Head 6\n Layer 3, Head 7\n Layer 3, Head 8\nLayer 12, Head 1\n Layer 12, Head 2\n Layer 12, Head 3\n Layer 12, Head 4\n Layer 12, Head 5\n Layer 12, Head 6\n Layer 12, Head 7\n Layer 12, Head 8\nLayer 20, Head 1\n Layer 20, Head 2\n Layer 20, Head 3\n Layer 20, Head 4\n Layer 20, Head 5\n Layer 20, Head 6\n Layer 20, Head 7\n Layer 20, Head 8\nLayer 24, Head 1\n Layer 24, Head 2\n Layer 24, Head 3\n Layer 24, Head 4\n Layer 24, Head 5\n Layer 24, Head 6\n Layer 24, Head 7\n Layer 24, Head 8\nFigure 8.Full attention matrices of APE (baseline). X- and Y-axes are key and query positions respectively. Max position = 3 072.\nSupplementary Material: Relative Positional Encoding for Transformers with Linear Complexity\nLayer 1, Head 1\n Layer 1, Head 2\n Layer 1, Head 3\n Layer 1, Head 4\n Layer 1, Head 5\n Layer 1, Head 6\n Layer 1, Head 7\n Layer 1, Head 8\nLayer 3, Head 1\n Layer 3, Head 2\n Layer 3, Head 3\n Layer 3, Head 4\n Layer 3, Head 5\n Layer 3, Head 6\n Layer 3, Head 7\n Layer 3, Head 8\nLayer 12, Head 1\n Layer 12, Head 2\n Layer 12, Head 3\n Layer 12, Head 4\n Layer 12, Head 5\n Layer 12, Head 6\n Layer 12, Head 7\n Layer 12, Head 8\nLayer 20, Head 1\n Layer 20, Head 2\n Layer 20, Head 3\n Layer 20, Head 4\n Layer 20, Head 5\n Layer 20, Head 6\n Layer 20, Head 7\n Layer 20, Head 8\nLayer 24, Head 1\n Layer 24, Head 2\n Layer 24, Head 3\n Layer 24, Head 4\n Layer 24, Head 5\n Layer 24, Head 6\n Layer 24, Head 7\n Layer 24, Head 8\nFigure 9.Full attention matrices of sineSPE (with gated SPE). Max token position = 3 072.\nLayer 1, Head 1\n Layer 1, Head 2\n Layer 1, Head 3\n Layer 1, Head 4\n Layer 1, Head 5\n Layer 1, Head 6\n Layer 1, Head 7\n Layer 1, Head 8\nLayer 3, Head 1\n Layer 3, Head 2\n Layer 3, Head 3\n Layer 3, Head 4\n Layer 3, Head 5\n Layer 3, Head 6\n Layer 3, Head 7\n Layer 3, Head 8\nLayer 12, Head 1\n Layer 12, Head 2\n Layer 12, Head 3\n Layer 12, Head 4\n Layer 12, Head 5\n Layer 12, Head 6\n Layer 12, Head 7\n Layer 12, Head 8\nLayer 20, Head 1\n Layer 20, Head 2\n Layer 20, Head 3\n Layer 20, Head 4\n Layer 20, Head 5\n Layer 20, Head 6\n Layer 20, Head 7\n Layer 20, Head 8\nLayer 24, Head 1\n Layer 24, Head 2\n Layer 24, Head 3\n Layer 24, Head 4\n Layer 24, Head 5\n Layer 24, Head 6\n Layer 24, Head 7\n Layer 24, Head 8\nFigure 10.Full attention matrices of sineSPE (without SPE gating). Max token position = 3 072.\nSupplementary Material: Relative Positional Encoding for Transformers with Linear Complexity\nLayer 1, Head 1\n Layer 1, Head 2\n Layer 1, Head 3\n Layer 1, Head 4\n Layer 1, Head 5\n Layer 1, Head 6\n Layer 1, Head 7\n Layer 1, Head 8\nLayer 3, Head 1\n Layer 3, Head 2\n Layer 3, Head 3\n Layer 3, Head 4\n Layer 3, Head 5\n Layer 3, Head 6\n Layer 3, Head 7\n Layer 3, Head 8\nLayer 12, Head 1\n Layer 12, Head 2\n Layer 12, Head 3\n Layer 12, Head 4\n Layer 12, Head 5\n Layer 12, Head 6\n Layer 12, Head 7\n Layer 12, Head 8\nLayer 20, Head 1\n Layer 20, Head 2\n Layer 20, Head 3\n Layer 20, Head 4\n Layer 20, Head 5\n Layer 20, Head 6\n Layer 20, Head 7\n Layer 20, Head 8\nLayer 24, Head 1\n Layer 24, Head 2\n Layer 24, Head 3\n Layer 24, Head 4\n Layer 24, Head 5\n Layer 24, Head 6\n Layer 24, Head 7\n Layer 24, Head 8\nFigure 11.Full attention matrices of convSPE (with SPE gating, conv ﬁlter size = 128). Max token position = 3 072.\nLayer 1, Head 1\n Layer 1, Head 2\n Layer 1, Head 3\n Layer 1, Head 4\n Layer 1, Head 5\n Layer 1, Head 6\n Layer 1, Head 7\n Layer 1, Head 8\nLayer 3, Head 1\n Layer 3, Head 2\n Layer 3, Head 3\n Layer 3, Head 4\n Layer 3, Head 5\n Layer 3, Head 6\n Layer 3, Head 7\n Layer 3, Head 8\nLayer 12, Head 1\n Layer 12, Head 2\n Layer 12, Head 3\n Layer 12, Head 4\n Layer 12, Head 5\n Layer 12, Head 6\n Layer 12, Head 7\n Layer 12, Head 8\nLayer 20, Head 1\n Layer 20, Head 2\n Layer 20, Head 3\n Layer 20, Head 4\n Layer 20, Head 5\n Layer 20, Head 6\n Layer 20, Head 7\n Layer 20, Head 8\nLayer 24, Head 1\n Layer 24, Head 2\n Layer 24, Head 3\n Layer 24, Head 4\n Layer 24, Head 5\n Layer 24, Head 6\n Layer 24, Head 7\n Layer 24, Head 8\nFigure 12.Full attention matrices of convSPE (without SPE gating, conv ﬁlter size = 512). Max token position = 3 072.\nSupplementary Material: Relative Positional Encoding for Transformers with Linear Complexity\ntokens beyond position 2 048 (the training sequence length)\nseems to concentrate around 2 048 in earlier layers, rather\nthan paying global or local attention. Such behavior is not\nseen in any of our SPE models. This potentially explains\nAPE’s poor generalization to long sequences suggested by\nthe stark increase in validation loss after position 2 048 (see\nFigure 3 in the main paper, and Table 6 here).\nNext, comparing Figures 9 and 10, it is obvious that gated\nSPE gives the model the freedom to switch off PE in some\nheads to achieve global attention (see Figure 9), whereas\nthe attention of ungated sineSPE (Figure 10) largely stays\nperiodic, which might not be always desirable. The same\ncan be said for convSPE (Figures 11 and 12). The gated\nconvSPE is able to look much further back in the middle\nlayers than its ungated counterpart.\nD.2. Attention Visualization: CIFAR10\nFigure 13 displays attention maps extracted from models\ntrained on the LRA CIFAR10 task. Note that these are one-\nlayer networks, and classiﬁcation is done by prepending\na special CLS token to the sequence of pixel values and\nusing the output at this ﬁrst position as input to a feed-\nforward classiﬁer. Consequently, only the attention map\nat this single position (which is the one we display here)\nmatters. (The model is therefore de facto not using self-\nattention, but rather attention with a single query and many\nkeys. This removes the distinction between relative and\nabsolute positions, which might explain why trainable APE\nperforms better than SPE on this task.)\nD.3. Evaluation of Desired PE Properties\nWe employ identical word probing and the associated met-\nrics introduced in Wang et al. (2021) to compare the trans-\nlation invariance and monotonicity properties of APE and\nour SPEs. The other properties mentioned in that work,\nnamely symmetry and direction balance, are not evaluated\nhere since the attention is uni-directional in our case. The\nmodels are also trained on pop piano music generation.\nThe metrics are calculated from attention matrices of each\nhead in the 1st layer, averaged over all possible identical-\ntoken sequences (i.e., a sequence composed of repeated,\nsame tokens; there are ∼340 of them for our REMI vo-\ncabulary). To eliminate the impact of applying row-wise\nsoftmax with causal masking on the translation invariance\nproperty, we compute the metrics on the unnormalized at-\ntention matrices, i.e., A = exp(QK⊤/\n√\nD) for APE, and\nA = exp(ˆQˆK\n⊤\n/\n√\nR) for SPEs. Various combinations of\nquery positions and query-key offsets are considered to ex-\namine whether the PE properties stay consistent when we\nextrapolate to longer sequences, as well as to look into their\nbehavior in local and long-range attention spans.\nWe report the scores of the best-performing (i.e., lowest-\nscoring) head of each model in Table 7. From the table,\nwe can notice that the PE properties of APE often deterio-\nrate drastically in cases of extrapolation. On the contrary,\nthe scores of ungated SPE models, i.e., models in which\nwe enforce the incorporation of positional information in\nevery layer, remain remarkably consistent throughout the\npositions. The evaluation here provides additional evidence\nfor the extrapolation capability of SPEs.\nD.4. Impact of the Number Rof Realizations\nIn the main document, we discussed how SPE asymptoti-\ncally leads to the desired cross-covariance structure as R\ngrows to inﬁnity. In this section, we empirically study how\nperformance is affected by that parameter in practice. A\nﬁrst thing to highlight is that each training batch yields a\nnew set of realizations for the noise Zd, so that the network\nsees the right attention pattern on average.\nHowever, we may wonder whether how the number of real-\nizations Rimpacts training and test performance. One can\nindeed notice that Rmay totally be set differently during\ntraining and inference, since it has no impact on the shape\nof the actual parameters/structure of the model. For this\nreason, we performed an ablation study where we use differ-\nent values for Rtrain at training time, resulting in a trained\nmodel, and then evaluate its performance using a possibly\ndifferent value Rtest. The results are displayed in Figure 14.\nWe can notice that the result achieved with Rtest = Rtrain\n(highlighted in bold) is consistently close to the best result\nfor the same Rtrain, and conversely, choosingRtest ̸= Rtrain\noften leads to a poor result. In other words, training and test-\ning with the same Rappears to be favorable for consistently\ngood performance.\nAnother remarkable fact is that a higher Rdoes not seem to\nimply better performance, even when Rtest = Rtrain. On the\ncontrary, convSPE achieved by far the highest accuracy\nwith R = 4. This unexpected result seems contradictory\nto the fact that it means noisier attention patterns. Further\ninvestigation is required to explain this phenomenon, but we\nconjecture that this additional noise in the attention patterns\nleads to increased robustness of the trained model, helping\ngeneralization.\nSupplementary Material: Relative Positional Encoding for Transformers with Linear Complexity\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0.000\n0.002\n0.004\n0.006\n0.008\n0.0100.0120.014\n0.000\n0.002\n0.004\n0.006\n0.008\n0.0100.0120.014\n0.000\n0.002\n0.004\n0.006\n0.008\n0.0100.0120.014\nFigure 13.CIFAR10 attention maps for 3 variants of Linear Transformer-ReLU: learnable APE (top),sineSPE (middle), and convSPE\n(bottom). Each row displays the input image, followed by attention weights of the 8 respective heads for each pixel, with the special CLS\ntoken as the query.\nSupplementary Material: Relative Positional Encoding for Transformers with Linear Complexity\nTable 7.Evaluation of PEs metrics. T: translation invariance, M: monotonicity (lower is better). ug: models trained without SPE gating.\nQuery positions 0 <pos ≤1 024 1 024<pos ≤2 048 2 048<pos ≤2 560(extrapolation)\nQuery-key offset <128 <512 <1 024 <128 <512 <1 024 <128 <512 <1 024\nAPE T: 0.4335\nM: 0.0152\nT: 0.2063\nM: 0.0625\nT: 0.1845\nM: 0.0616\nT: 0.9142\nM: 0.0193\nT: 0.6953\nM: 0.0413\nT: 0.6458\nM: 0.0713\nT: 0.9599\nM: 0.3974\nT: 0.8959\nM: 0.2429\nT: 0.5886\nM: 0.1637\nsineSPE T: 0.1660\nM: 0.2893\nT: 0.3078\nM: 0.4406\nT: 0.3527\nM: 0.4283\nT: 0.1337\nM: 0.2826\nT: 0.2504\nM: 0.4063\nT: 0.3228\nM: 0.4167\nT: 0.2167\nM: 0.3253\nT: 0.3599\nM: 0.4060\nT: 0.4147\nM: 0.3913\nsineSPE (ug) T: 0.0141\nM: 0.6295\nT: 0.0242\nM: 0.1844\nT: 0.0231\nM: 0.1582\nT: 0.0135\nM: 0.6238\nT: 0.0206\nM: 0.1623\nT: 0.0190\nM: 0.1061\nT: 0.0105\nM: 0.6189\nT: 0.0196\nM: 0.1609\nT: 0.0163\nM: 0.0994\nconvSPE T: 0.3422\nM: 0.1781\nT: 0.5637\nM: 0.2242\nT: 0.6389\nM: 0.2189\nT: 0.3209\nM: 0.1735\nT: 0.6239\nM: 0.3624\nT: 0.7648\nM: 0.4192\nT: 0.3462\nM: 0.1486\nT: 0.6135\nM: 0.3247\nT: 0.7025\nM: 0.2740\nconvSPE (ug) T: 0.2828\nM: 0.1234\nT: 0.0192\nM: 0.0249\nT: 0.0107\nM: 0.0620\nT: 0.3334\nM: 0.1505\nT: 0.0188\nM: 0.0253\nT: 0.0109\nM: 0.1254\nT: 0.2207\nM: 0.1342\nT: 0.0171\nM: 0.0217\nT: 0.0106\nM: 0.0989\n1 2 4 8 16 32 64 128\nRtest\n128644\nRtrain\n59.79 61.17 62.01 54.67 55.44 61.10 55.04 62.99\n57.30 56.48 57.24 62.28 59.92 59.93 62.21 61.76\n51.79 54.32 62.65 56.52 55.63 59.30 53.66 52.31\n(a) sineSPE\n1 2 4 8 16 32 64 128\nRtest\n128644\nRtrain\n57.86 57.34 55.91 56.34 56.22 58.61 54.82 57.66\n57.20 55.85 57.10 58.32 55.04 57.73 59.86 58.01\n61.03 60.31 62.08 58.68 58.31 57.80 59.98 59.71 (b) convSPE\nFigure 14.Accuracy of Performer-softmax with SPE on the LRA Text task, with different numbers of realizationsR during training/testing.\nEach value is the result of a single run. Highlighted in bold are values obtained with Rtest = Rtrain. Higher (brighter) is better.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6241998076438904
    },
    {
      "name": "Computer science",
      "score": 0.6238057613372803
    },
    {
      "name": "Algorithm",
      "score": 0.5920125842094421
    },
    {
      "name": "Computation",
      "score": 0.5878409743309021
    },
    {
      "name": "Inference",
      "score": 0.499600887298584
    },
    {
      "name": "Encoding (memory)",
      "score": 0.4730999767780304
    },
    {
      "name": "Computational complexity theory",
      "score": 0.4450308680534363
    },
    {
      "name": "Gaussian",
      "score": 0.43058618903160095
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2256631851196289
    },
    {
      "name": "Engineering",
      "score": 0.06836485862731934
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210101743",
      "name": "Laboratoire d'Informatique, de Robotique et de Microélectronique de Montpellier",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210163794",
      "name": "Data Management (Italy)",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I12356871",
      "name": "Télécom Paris",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I16733864",
      "name": "National Taiwan University",
      "country": "TW"
    },
    {
      "id": "https://openalex.org/I4210086894",
      "name": "Research Center for Information Technology Innovation, Academia Sinica",
      "country": "TW"
    },
    {
      "id": "https://openalex.org/I4210161954",
      "name": "Département d'Informatique",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I29607241",
      "name": "École Normale Supérieure - PSL",
      "country": "FR"
    }
  ]
}