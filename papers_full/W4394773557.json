{
  "title": "Novel Transformer Based Contextualized Embedding and Probabilistic Features for Depression Detection From Social Media",
  "url": "https://openalex.org/W4394773557",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5114149184",
      "name": "Muhammad Asad Abbas",
      "affiliations": [
        "Khwaja Fareed University of Engineering and Information Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5039561400",
      "name": "Kashif Munir",
      "affiliations": [
        "Khwaja Fareed University of Engineering and Information Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5085489082",
      "name": "Ali Raza",
      "affiliations": [
        "University of Lahore"
      ]
    },
    {
      "id": "https://openalex.org/A5002227806",
      "name": "Nagwan Abdel Samee",
      "affiliations": [
        "Princess Nourah bint Abdulrahman University"
      ]
    },
    {
      "id": "https://openalex.org/A5051158911",
      "name": "Mona Jamjoom",
      "affiliations": [
        "Princess Nourah bint Abdulrahman University"
      ]
    },
    {
      "id": "https://openalex.org/A5030346209",
      "name": "Zahid Ullah",
      "affiliations": [
        "King Abdulaziz University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3131947689",
    "https://openalex.org/W4286206444",
    "https://openalex.org/W6966838249",
    "https://openalex.org/W3158087093",
    "https://openalex.org/W3213632075",
    "https://openalex.org/W4309777556",
    "https://openalex.org/W3042902594",
    "https://openalex.org/W3149451022",
    "https://openalex.org/W2741216199",
    "https://openalex.org/W3000918316",
    "https://openalex.org/W2794320209",
    "https://openalex.org/W2905587047",
    "https://openalex.org/W3167939777",
    "https://openalex.org/W4226377277",
    "https://openalex.org/W3004297621",
    "https://openalex.org/W4220916703",
    "https://openalex.org/W3199956316",
    "https://openalex.org/W3115063109",
    "https://openalex.org/W3120564254",
    "https://openalex.org/W4390544388",
    "https://openalex.org/W4297988790",
    "https://openalex.org/W4389459449",
    "https://openalex.org/W4387129526",
    "https://openalex.org/W4312204145",
    "https://openalex.org/W4386590663",
    "https://openalex.org/W4366463043",
    "https://openalex.org/W4391305663",
    "https://openalex.org/W2920568733",
    "https://openalex.org/W4312875997",
    "https://openalex.org/W3164561728"
  ],
  "abstract": "Depression constitutes a significant mental health condition, impacting an individual&#x2019;s emotional state, thought processes, and ability to carry out everyday tasks. Depression is defined by ongoing feelings of sadness, diminished interest in previously enjoyed activities, alterations in hunger, sleep disturbances, decreased vitality, and challenges with focus. The impact of depression extends beyond the individual, affecting society at large through decreased productivity and higher healthcare costs. In the realm of social media, users often express their thoughts and emotions through posts, which can provide insightful data for identifying patterns of depression. This research aims to detect depression early by analyzing social media user content with machine learning techniques. We have built advanced machine learning models using a benchmark depression database containing 20,000 tagged tweets from user profiles identified as depressed or non-depressed. We are introducing an innovative BERT-RF feature engineering method that extracts Contextualized Embeddings and Probabilistic Features from textual input. The Bidirectional Encoder Representations from Transformers (BERT) model, based on the Transformer architecture, is used to extract Contextualized Embedding features. These features are then fed into a random forest model to generate class probabilistic features. These prominent features aid in enhancing the identification of depression from social media. In order to classify tweets using the features derived from the BERT-RF features selection step, we have used five popular classifiers: Random Forest (RF), Multilayer Perceptron (MLP), K-Neighbors Classifier (KNC), Logistic Regression (LR), and Long Short-Term Memory (LSTM). Evaluation experiments show that our approach, using BERT-RF for feature engineering, enables the Logistic Regression model to outperform state-of-the-art methods with a high accuracy score of 99&#x0025;. We have validated the results through k-fold cross-validation and statistical T-tests. We achieved 99&#x0025; k-fold accuracy during the validation of the proposed approach. This research contributes significantly to computational linguistics and mental health analytics by providing a robust approach to the early detection of user depression from social media content.",
  "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1 109/ACCESS.2023.1 120000\nNovel Transformer Based Contextualized\nEmbedding and Probabilistic Features for\nDepression Detection from Social Media\nMUHAMMAD ASAD ABBAS1, KASHIF MUNIR1,*, ALI RAZA2, NAGWAN ABDEL SAMEE3,*,\nMONA M. JAMJOOM4, AND ZAHID ULLAH5\n1Institute of Information Technology, Khwaja Fareed University of Engineering and Information Technology, Rahim Yar Khan 64200, Pakistan; (e-mail:\nasadabbasarbi@gmail.com; kashif.munir@kfueit.edu.pk;)\n2Department of Software Engineering, University Of Lahore, Lahore 54000, Pakistan; (e-mail: ali.raza.scholarly@gmail.com)\n3Department of Information Technology, College of Computer and Information Sciences, Princess Nourah bint Abdulrahman University, P.O. Box 84428, Riyadh\n11671, Saudi Arabia; (e-mail:nmabdelsamee@pnu.edu.sa )\n4Department of Computer Sciences, College of Computer and Information Sciences, Princess Nourah bint Abdulrahman University, Riyadh 11671, Saudi Arabia;\n(e-mail: mmjamjoom@pnu.edu.sa)\n5Department of information system, King Abdulaziz University, Jeddah, Saudi Arabia; (e-mail: zasultan@kau.edu.sa)\nCorresponding author: Kashif Munir (e-mail: kashif.munir@kfueit.edu.pk) and Nagwan Abdel Samee (e-mail:\nnmabdelsamee@pnu.edu.sa).\nThis research was funded by Princess Nourah bint Abdulrahman University Researchers Supporting Project number (PNURSP2024R104),\nPrincess Nourah bint Abdulrahman University, Riyadh, Saudi Arabia.\nABSTRACT Depression constitutes a significant mental health condition, impacting an individual’s\nemotional state, thought processes, and ability to carry out everyday tasks. Depression is defined by\nongoing feelings of sadness, diminished interest in previously enjoyed activities, alterations in hunger, sleep\ndisturbances, decreased vitality, and challenges with focus. The impact of depression extends beyond the\nindividual, affecting society at large through decreased productivity and higher healthcare costs. In the realm\nof social media, users often express their thoughts and emotions through posts, which can provide insightful\ndata for identifying patterns of depression. This research aims to detect depression early by analyzing\nsocial media user content with machine learning techniques. We have built advanced machine learning\nmodels using a benchmark depression database containing 20,000 tagged tweets from user profiles identified\nas depressed or non-depressed. We are introducing an innovative BERT-RF feature engineering method\nthat extracts Contextualized Embeddings and Probabilistic Features from textual input. The Bidirectional\nEncoder Representations from Transformers (BERT) model, based on the Transformer architecture, is used\nto extract Contextualized Embedding features. These features are then fed into a random forest model\nto generate class probabilistic features. These prominent features aid in enhancing the identification of\ndepression from social media. In order to classify tweets using the features derived from the BERT-RF\nfeatures selection step, we have used five popular classifiers: Random Forest (RF), Multilayer Perceptron\n(MLP), K-Neighbors Classifier (KNC), Logistic Regression (LR), and Long Short-Term Memory (LSTM).\nEvaluation experiments show that our approach, using BERT-RF for feature engineering, enables the Logistic\nRegression model to outperform state-of-the-art methods with a high accuracy score of 99%. We have\nvalidated the results through k-fold cross-validation and statistical T-tests. We achieved 99% k-fold accuracy\nduring the validation of the proposed approach. This research contributes significantly to computational\nlinguistics and mental health analytics by providing a robust approach to the early detection of user\ndepression from social media content.\nINDEX TERMS Depression Detection, Machine Learning, Deep Learning, Text Mining, BERT, Trans-\nformer.\nI. INTRODUCTION\nD\nEPRESSION is a complex mental health condition that\naffects an individual’s emotions, cognition, and daily\nfunctioning [1]. Characterized by more than transient sad-\nness or a challenging period, it manifests as a persistent\nsense of melancholy, diminished interest or pleasure in pre-\nVOLUME 11, 2023 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3387695\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nMuhammad et al.et al.: Novel Transformer Based Contextualized Embedding and Probabilistic Features for Depression Detection from Social Media\nviously enjoyed activities, and a variety of intense and endur-\ning physical and emotional symptoms. These symptoms can\ndiffer among individuals, typically encompassing persistent\nsadness, a marked loss of interest or pleasure, and notable\nchanges in diet or weight [2], among others. The etiology of\ndepression is multifaceted, involving genetic, biological, en-\nvironmental, and psychological factors. Additionally, trauma,\nstress, significant life alterations, certain medications, and\npre-existing medical conditions can precipitate its develop-\nment [3]. Effective management of depression often requires\na comprehensive approach, including therapy, pharmacolog-\nical treatment, lifestyle modifications, and support networks,\nwhich can significantly mitigate symptoms and improve qual-\nity of life [4].\nTreating depression often involves a multifaceted approach\ntailored to the individual. Preventing depression necessitates\na proactive stance towards mental health. Regular exercise,\nhealthy nutrition, and adequate sleep form the cornerstone\nof maintaining a sound and resilient mind [5]. Depression\nis a mental illness that can cause significant disruptions\nacross all facets of life. It extends beyond sadness, impairing\ndaily functioning through diminished cognitive abilities and\ndecision-making capacities. In severe cases, it can escalate\nthe risk of suicidal thoughts or behaviors [6]. Addressing\ndepression effectively requires a comprehensive strategy that\nincludes professional support, therapy, medication, lifestyle\nadjustments, and motivational encouragement.\nThe prevalence of depression can vary by population and\nregion, with data subject to change over time due to numerous\nfactors, including shifts in diagnostic criteria, advances in\nknowledge, and societal changes [7]. Despite these variations,\ndepression remains a significant global mental health chal-\nlenge. Recent updates, as of 2022, indicate that the incidence\nof depression has been on the rise over the years. The World\nHealth Organization (WHO) identifies depression as the lead-\ning cause of disability worldwide, impacting over 264 million\nindividuals across various age groups as reported in 2020 [8].\nThe significance of early intervention cannot be overstated,\nhighlighting the need to diminish stigma, enhance access to\nmental health services, and increase awareness about this\nissue. However, while these traditional approaches are cost-\neffective, there is an emerging need for advanced machine-\nlearning approaches for the early detection of depression\nthrough social media analysis.\nSocial media platforms have presented an unparalleled\nchance to examine mental health disorders, such as depres-\nsion, on a significant scale. Twitter has become a wonderful\nresource for gaining insights into people’s thoughts and emo-\ntions. Twitter users frequently disclose their personal expe-\nriences, sentiments, and emotions, enabling the examination\nand detection of indications of depression in their public\nmessages.\nAdvanced machine learning-based early detection of de-\npression from social media is essential for studying depres-\nsion in medicine [9]. Machine learning offers a variety of\nmodels that can be trained using accurate data to generate\nprecise predictions. This work introduces a novel BERT-RF\n(Bidirectional Encoder Representations from Transformers-\nRandom Forest) based stress detection model that improves\nefficiency during training and enhances the accuracy of pre-\ndictions. Our new research makes significant contributions in\nthe following areas:\n• We proposed a novel BERT-RF feature engineering ap-\nproach that extracts Contextualized Embeddings and\nProbabilistic Features from textual data. First, the Trans-\nformer architecture-based BERT extracts Contextual-\nized Embedding features, which are input into a random\nforest model for generating class probabilistic features.\nThese salient features help to improve depression detec-\ntion from social media.\n• We employed four advanced machine-learning models\nand a deep-learning model for results comparison. We\nimproved the performance by optimizing the hyperpa-\nrameters. To validate performance, we applied k-fold\ncross-validation and statistical T-test analysis.\nThe remaining manuscript is set as Section II describes\nthe study and is devoted to examining the limitations of the\nexisting literature. In section III, we have described in-depth\nour new approach to researching depression using the best\nfeatures in BERT-based content embedding and social media\ndata. Then, in Section IV, we compare the results obtained\nfrom our study’s various machine-learning methods. Finally,\nChapter V details the results of our new research.\nII. LITERATURE REVIEW\nThe global impact of depression, problems with early diag-\nnosis, and widespread stigma in Arab culture require a new\napproach. Social media is researching mental health services\nand has recently increased interest in depression research,\nespecially in English studies.\nThis study [10] conducted Arab-centered research by an-\nalyzing Twitter data from the Gulf region to detect depres-\nsion. Use supervised learning algorithms such as Random\nForest and Naive Bayes to build predictive models based\non online depression behaviors rather than symptoms. More\nimportantly, the model, specifically the Liblinear classifier,\nin this study achieved 87.5% accuracy in detecting depres-\nsion tweets, demonstrating the effectiveness of this feature in\ncapturing messages related to mental illness from Arab users.\nThis study shows the potential of digital media to promote\nearly detection of depression and improve cultural awareness\nand depression intervention in Arabic-speaking communities.\nThis [11] study combines deep learning with traditional\nmachine learning techniques to distinguish normal users from\nabnormal users on social media profiles. This study explored\nextensive literature to identify mental health indicators using\ndifferent media and behavioral approaches. Integrating deep\nlearning makes it more useful, especially in distinguishing\nnormal users from abnormal users. The important thing is that\nthis method achieves a lower error rate than traditional meth-\nods. This study achieved an accuracy rate of 89%, demon-\nstrating the effectiveness of deep learning in the psycholog-\n2 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3387695\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nMuhammad et al.et al.: Novel Transformer Based Contextualized Embedding and Probabilistic Features for Depression Detection from Social Media\nTABLE 1. The summary analysis of reviewed literature studies.\nRef. Year Dataset Technique Performance Score\n[10] 2019 Dataset collected from self-reported Random forest algorithm 87.5% Accuracy Score\n[11] 2018 Dataset collected from different me-\ndia and behavioral approaches\nK-Neighbor 89% Accuracy Score\n[12] 2019 data collect from clinical interviews GRU 90% Accuracy Score\n[13] 2021 Dataset collected from public Tweets\nfrom Twitter\nRandom forest algorithm 77% Accuracy Score\n[14] 2022 Dataset collected from Kaggle TF-IDF 89% Accuracy Score\n[15] 2021 Data collected using the Twitter\nApplication Programming Interface\n(API)\nSVM Classifier 87.5% Accuracy Score\n[16] 2019 Dataset collected from PHQ-9 logistic regression classi-\nfier\n86.45% Accuracy Score\n[17] 2022 Dataset collected from self-reported Convolutional Neural Net-\nwork(CNN)\n94.28% Accuracy Score\n[18] 2022 data collect from UCI Catboost/GB model 91% Accuracy Score\n[19] 2020 self from Facebook and Twitter Random Forest Model 90.3% Accuracy Score\nical analysis of social media data. This study highlights the\nimportant role of deep learning-based inference in improving\npsychological analysis in social media and suggests a signif-\nicant increase in accuracy and dispersion.\nThis study [12] achieved an accuracy rate of 90%. The\nsystem has been validated by tests showing its superiority\nover existing systems. The proposed model showed more than\na 30% reduction in errors, demonstrating its effectiveness in\nsearching for depression in users. Various experiments and\nexamples have confirmed the effectiveness of this model in\nanalyzing the level of emphasis in user texts. Additionally,\nthe model has been shown to perform well in real-world\nsituations, confirming its effectiveness. This study highlights\nthe importance of multi-modal collaboration in advancing\ndepression research on social media platforms, highlighting\nsignificant improvements in accuracy and robustness in cap-\nturing content expressed in user messages.\nThis [13] study was diagnostic criteria are based on patient-\nreported symptoms and have implications for patient manage-\nment; Therefore, alternative methods should be investigated.\nSocial media platforms like Facebook, Twitter, Reedit, and\nTumbler offer new ways to collect behavioral data that can\nreveal insights into a user’s emotional state. This research\nfocused on creating a machine learning framework to ex-\namine linguistic trends in Twitter user data for detecting\ndepression indicators. The performance of support vector\nmachine and random forest algorithms was evaluated and\ncontrasted, with the random forest algorithm demonstrating\nsuperior effectiveness. This study achieved an accuracy of\n77%. Machine learning models, specifically random forests,\nhave been shown to best detect depressive symptoms based\non message patterns in Twitter data.\nThis study [14] analyzed users’ Twitter posts to determine\nthe likelihood of depressive symptoms among online users.\nThis study focused on using machine learning and language\ntechniques to teach material and evaluate the plan’s effec-\ntiveness. This study reports the numerical score from tweet\nsentiment and achieved 78% accuracy in detecting depression\nusing the XGBoost classifier. Additionally, by combining\nfeatures such as TFIDF, NGram and LDA, 89% accuracy is\nachieved with the support vector machine classifier. Correct\nselection and their combination are important factors con-\ntributing to improved performance. The findings highlight the\nimportance of integrating emotion and speech in identifying\ndepressive symptoms from Twitter data. This study demon-\nstrated the effectiveness of machine learning techniques in\nidentifying potential signs of depression in online users.\nThis [15] study used machine learning techniques that\nfocus on detecting depression in Twitter users by extracting\ntweet features. In this study, a classification technique was\nused that aims to distinguish depressed users from other\nusers by analyzing features extracted from tweets. A machine\nlearning algorithm is used to classify the collected tweet data\nto detect whether the user is depressed. This study achieved an\naccuracy rate of 87.5%. This prediction is for early detection\nof depression or other mental health conditions.\nThis study [16] evaluated depression and suicidal thoughts\naccording to depression level. Data collection includes a sur-\nvey similar to the PHQ-9 that surveys demographic infor-\nmation, including current age, gender, and school attendance\n[20]. Based on the collected data, a classification algorithm\nwas used to classify severe depression into five levels. The\nXGBoost classifier achieved an accuracy of 83.87% on this\ndata, demonstrating the model’s effectiveness. Additionally,\nthe information collected through tweets is classified to de-\ntermine whether the user is depressed. The maximum value\nof the logistic regression classifier for detecting depression in\ntweets is 86.45%.\nThis study [17] aims to predict users’ psychological states\nby using deep learning models to classify depressed and non-\ndepressive tweets. Leveraging text content and deep learn-\ning architectures, specifically CNNs and LSTMs, a hybrid\nmodel was created that achieved 94.28% accuracy on Twit-\nter’s distressed dataset. Compared to RNN, CNN, and the\nbasic method, it can be seen that the best CNN and LSTM\nmodel in terms of prediction performance is based on differ-\nent parameters. Statistical and visual methods highlight the\nimportance of distinguishing between melancholic and non-\nVOLUME 11, 2023 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3387695\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nMuhammad et al.et al.: Novel Transformer Based Contextualized Embedding and Probabilistic Features for Depression Detection from Social Media\nmelancholic subjects. This study used Twitter’s depression\ndata using deep learning techniques to recognize language\npatterns and predict users’ emotional states. The findings\nhighlight the effectiveness of the CNN and LSTM model in\nclassifying depressed and non-depressed tweets and demon-\nstrate its potential to improve predictive performance for\nassessing mental health control.\nThis [18] article introduces a self-report method using self-\nreports in tweets and proposes a new multi-modal frame-\nwork for predicting depression symptoms based on user data.\nThis study uses the n-gram language model, LIWC dictio-\nnary, automatic image tagging, and bag-of-words and adopts\nrelationship-based selection and nine categories to measure\nperformance. The analysis showed that tweets and texts were\n91% and 83% accurate in predicting depression symptoms,\nrespectively, yielding positive results. This study suggests\nthat efficiency can be improved by reducing the number of\nusers using or participating in medical records. The data was\ncollected from the social platform focused on self-expression\nand user data in tweets to predict depression symptoms using\nvarious techniques and classifications. The findings highlight\nthe effectiveness of using user-generated content, particularly\ntweets, to predict symptoms of depression and highlight the\npotential of multiple baselines in mental health assessment.\nThis study [19] begins with a comprehensive review of\nBengali language literature on depression-related reporting\nand comments on social media. Machine learning meth-\nods, including support vector machines, decision trees, ran-\ndom forests, polynomial naive Bayes, K-nearest neighbors,\nand logistic regression, are used to predict depression in\nmany ways. This study achieved an accuracy of 90.3% with\nTF-IDF, a Random Forest Model. Data collection focused\non depression-related content in tweets and responses in\nBangladesh to facilitate algorithmic prediction. Different ma-\nchine learning algorithms have different predictive values;\nhowever, the accuracy of each algorithm applied to the dataset\nremains the same.\nA. RESEARCH QUESTIONS AND GAPS\nIn order to demonstrate the uniqueness of our work, we pre-\nsented a summary in Table 1 of the most significant distinc-\ntions that exist between the approaches applied in the current\nliterature and the methodology that we have proposed, as well\nas the accuracy that has been achieved. During our literature\nanalysis, we identified several gaps that are addressed in our\nproposed research approach. We propose a novel transformer-\nbased feature engineering method, BERT-RF, which extracts\ncontextualized embeddings and probabilistic features from\ntextual data. This contrasts with the classical approaches\npredominantly used in current literature.\n• Previously, researchers relied solely on the BERT model\nfor feature representation and also deployed classical\ntree-based models. How can advanced features engineer-\ning and models be built for depression detection?\n• The performance scores of these methods were moderate\nwhen compared to the state-of-the-art. How can high-\nperformance scores using applied models be achieved\nfor depression detection?\nIII. PROPOSED METHODOLOGY\nIn Figure 1, our proposed methodology process begins with\nthe user’s text message dataset. These raw data undergo a\nprequalification process to enhance their suitability for anal-\nysis. Preprocessing includes methods such as tokenization,\nnormalization, and stemming. Tokenization divides the text\ninto individual units, normalization standardizes the repre-\nsentation and stemming simplifies the text by reducing words\nto their base forms. Subsequently, the feature extraction step\nisolates significant information from the preprocessed data.\nThis step involves selecting key features using the novel\nmodel proposed by BERT-RF. Following feature extraction,\nthe dataset is split into two parts: a training set and a test\nset, with 80% of the data allocated to training and 20% to\ntesting. This division allows the model to learn patterns from\nthe training set and assess its performance on the test set,\nincluding data not seen during the training phase. We applied\nseveral advanced machine learning models. The final step\ninvolves analyzing the prediction model to draw conclusions.\nThe results typically highlight conditions under which the\nmodel predicts depression and those under which it predicts\nthe absence of depression. Based on their posts, this final\nanalysis provides crucial information regarding the user’s\noverall mental health status.\nA. PHASE 1: DEPRESSION TEXTUAL DATA\nThis study used a benchmark depression database [21] con-\ntaining 20,000 tagged English tweet user profiles of depressed\nand non-depressed users. The data includes important details\nabout users such as post text, friends, followers, and social\nmedia activities. This information is stored in a data set and\nused as a reference for subsequent stress tests. The data set\nis treated according to the main features, referring to fea-\ntures that may indicate melancholic preferences and ignoring\nothers represented by the content of the melancholic or non-\nmelancholic text. By taking into account the user’s history of\nsocial media posts and statements, it is aimed to increase the\naccuracy of the results and contribute to better results in the\nresearch of depression by psychologists.\nB. PHASE 2: TEXT PREPROCESSING AND DATA ANALYSIS\nFigure 2 shows that Data preprocessing is the major portion\nof data mining. The first method removes the user’s cus-\ntom text format. The objective of this technique is to elimi-\nnate elements such as \"usernames (@usernames)\", \"hashtags\n(hashtags),\" \"URLs,\" \"non-alphabetic characters, symbols,\nand digits,\" \"blank strings,\" \"rows containing NaN values,\"\nand \"Black-line\" among others. This approach ensures the\npurification of each tweet in the collection by excluding all\nURLs present within the tweets. Subsequently, it focuses on\ndiscarding dates, times, numbers, and hashtags. The process\nthen advances to eliminating emojis, redundant spaces, and\nextra spaces within sentences. Following this, the technique\n4 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3387695\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nMuhammad et al.et al.: Novel Transformer Based Contextualized Embedding and Probabilistic Features for Depression Detection from Social Media\nTextual Users\nPosts Data\nTokenization\nNormalization\nStemming\nText Pre-processing\n Novel Proposed\nFeature Extraction\nDataset\nSplitting\nTest(20%)\nTrain(80%)\nHyperparameterized\nAI model\nDepressed\nNon-Depressed\nPrediction\n Results\nModel Prediction\nFIGURE 1. Proposed methodology workflow\ninvolves the extraction of stems through the removal of stop\nwords, which are words like \"if,\" \"of,\" and \"else\" that do\nnot add significant meaning to sentences. The NLTK library\nprovides a collection of stop words to filter out these non-\ncontributory words from the text. Stemming is employed to\nreduce words to their base or root form. The ultimate aim\nis to transform each word in a tweet into a sequence of\ndigits, substituting them with their respective values found in\na dictionary index.\nTweet Data\nTokenization\nNormalization\nLowercase\nConversion\nRemove\nstopwords\nRemove special\ncharacters\nProcessed Tweet\nTokens\nConcatenation\nFIGURE 2. The user post text data workflow.\nIn deep learning, the word cloud is not used directly but\nhas implications in the broader field of natural language\nprocessing (NLP) and text analysis, as shown in Figure 3.\nIt works as a visual method rather than a deep learning\nmethod. A word cloud visually represents the frequency of\nwords in text; frequently used words appear in larger letters.\nWhile not as complex as deep learning models often used in\nsentiment analysis or translation tasks, word clouds provide a\nsimple and intuitive way to search for and communicate body\nlandmarks.\nFIGURE 3. Visualizing Textual Patterns: A Word Cloud Analysis.\nC. PHASE 3: NOVEL PROPOSED BERT-RF TEXTUAL\nFEATURE EXTRACTION\nIn this section, we describe our novel approach to salient\nfeature engineering. The workflow for extracting features\nfrom textual data is illustrated in Figure 4. The use of BERT-\nbased contextual embeddings has become a popular method\nin researching social anxiety. Initially, we input the prepro-\ncessed textual data into the pre-trained BERT transformer\nmodel, which generates embeddings for each token in the\nVOLUME 11, 2023 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3387695\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nMuhammad et al.et al.: Novel Transformer Based Contextualized Embedding and Probabilistic Features for Depression Detection from Social Media\ntext while preserving context information. Features extracted\nfrom these embeddings can be selected for specific tokens or\naggregated across the entire sentence [22]. To enhance the\nmodel’s capabilities, we include potential features as inputs\nto a random forest model to generate probabilistic features.\nThese probabilistic features are then utilized for training the\nmodels applied in detecting depression from social media.\nCareful interpretation and validation of predictive models are\ncrucial, especially when addressing mental health issues.\nAlgorithm 1 outlines the sequential process for extracting\nnovel features.\nAlgorithm 1 BERT-RF Algorithm\nInput: Depression Post Textual Content.\nOutput: Novel Feature Set.\ninitiate;\n1- BERTce ←− EBERT (Dt ) // here BERTce are the\nContextualized Embedding features and Dt are input Textual\nContent.\n2- RFpf ←− PRF (BERTce) // here RFpf are the\nProbabilistic features and BERTce are input Contextualized\nEmbedding features set.\n3- Ft ←− RFpf // here Ft are the Novel feature set.\nend;\nD. PHASE 4: FEATURES DATA SPLITTING\nIn this study, we used an 80:20 data split, with 80% of the\ndataset used to train the machine learning model and 20%\nto evaluate the model’s performance. To achieve this, the\ndata is split using the train-test-split() method in the scikit-\nlearn module. This separation method is chosen to reduce the\nrisk of overworking and improve the model’s overall perfor-\nmance [23]. This study also includes k-fold cross-validation\nto validate the results obtained from the profile segmentation\nprocess. This data splitting helps ensure that the model’s\nperformance is measured consistently across different parts\nof the data set, thus increasing the reliability of the results.\nE. PHASE 5: APPLIED ARTIFICIAL INTELLIGENCE MODELS\nIn artificial intelligence, many well-known algorithms [24]\nhave become the backbone of solving various problems, such\nas depression detection from social media. Together, these\nAI-based models create comprehensive tools that address\nnumerous real-world situations and allow clinicians to derive\nvaluable insights from the data.\n• Random Forest Classifier (RF) is a classification and\nregression technique that uses bootstrapping and multi-\nple decision trees to create forests [25]. RF represents the\nunion of prediction trees; each tree depends on the value\nof the selected value vector. After receiving new input\ndata, the algorithm creates a decision tree for that data\nand merges it with another decision tree in the forest. The\nRF uses the feature vectors {V1, V2, . . . ,Vn} as input\nto classify each post into categories such as depressed\nor not depressed . The prediction for a post Pi can be\nrepresented as:\nˆyi = RFC(Vi) (1)\nwhere the predicted classification for each post, denoted\nas Pi, is represented by ˆyi. Here, a value of ˆyi = 1\nsignifies that the post is classified as depressed, whereas\nˆyi = 0denotes classification as not depressed.\nThe Random Forest Classifier decision function for a\ngiven post Pi can be further detailed as:\nˆyi = 1\nN\nNX\nj=1\nDTj(Vi) (2)\nWhere N is the number of decision trees in the forest,\nand DT j is the prediction of the j-th decision tree. A\nmajority vote among all trees typically determines the\nfinal classification.\n• Multilayer Perceptron (MLP) is a good neural net-\nwork model used specifically for task classification [26].\nThere are many layers of connections between nodes or\nneurons, and the model works in a feed-forward manner,\nwith information passing through each layer and making\nconnections within and between layers. Let’s denote the\ninput layer by x ∈ Rd , where d is the number values\nof features extracted from social media data. The MLP\nconsists of L layers, each with its own set of weights\nW(l) and biases b(l), where l ∈ {1, 2, . . . ,L} represents\nthe layer index.\nThe output of each layer l is calculated as:\nh(l) = f (W(l)h(l−1) + b(l)), (3)\nWhere f represents a non-linear activation function,\nlike the ReLU function, which is denoted by f (z) =\nmax(0, z), and h(0) equals the initial layer x.\nThe final output layer (assuming binary classification for\ndepression detection, with 1 indicating depression and 0\nindicating no depression) is given by:\nˆy = σ(W(L)h(L−1) + b(L)), (4)\nwhere σ(z) = 1\n1+e−z is the sigmoid activation module,\nand ˆy is the predicted probability of depression.\n• K-Neighbors Classifier (KNC) utilized to classify new\ndata based on the most common classes of nearest\nneighbors at a given location [27]. Calculate the dis-\ntance between data points to determine proximity. K,\nthe number of neighbors data, and the distance mea-\nsure are important. Given a set of n social media posts\nP = {p1, p2, ...,pn}, where each post pi is represented\nby a feature vector xi ∈ Rd extracted from the text,\nand a corresponding label yi ∈ {0, 1} indicating the\nabsence (0) or presence ( 1) of depressive indicators. The\nK-Neighbors Classifier (KNC) method for depression\ndetection can be described as follows:\n6 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3387695\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nMuhammad et al.et al.: Novel Transformer Based Contextualized Embedding and Probabilistic Features for Depression Detection from Social Media\nDepression Data\nBERT\nModel\nContextualized\nEmbedding RF Model\nClass Prediction\nProbability Features\nNew Transfer\nFeature\nFIGURE 4. The architecture analysis of the novel proposed BERT-RF textual feature extraction method.\n1) For a given unlabelled post pu with feature vector\nxu, compute the distance D(xu, xi) between xu and\neach xi in the training set P. Common distance\nmetrics include Euclidean distance:\nD(xu, xi) =\nvuut\ndX\nj=1\n(x(j)\nu − x(j)\ni )2\n2) Identify the k nearest neighbors of xu, denoted as\nNk (xu), based on the smallest distances D(xu, xi).\n3) Determine the majority label values among the k\nnearest neighbors:\nˆyu = arg max\ny∈{0,1}\nX\nxi∈Nk (xu)\nI(yi = y)\nwhere I is an indicator values function that is 1 if\nyi = y and 0 otherwise.\nThe predicted label ˆyu indicates whether the post pu is\nlikely to exhibit depressive indicators ( 1) or not ( 0),\nbased on the content similarity to the labeled posts in\nthe training set.\n• Logistic Regression (LR) uses the sigmoid function to\nmodel the probability that inputs belong to a particular\nclass [28]. This model involves a combination of input\nstrategies converted to a quality between 0 and 1. The ba-\nsic mathematical equation of the LR model is expressed\nas follows:\nP(Y = 1) = 1\n1 +e−(β0+β1X1+β2X2+···+βnXn) (5)\nWhere:\n-- P(Y = 1)is the probability value of an individual\nbeing detected as depressed (Y=1) based on their\nsocial media activity.\n-- e is the base value of the natural logarithm.\n-- β0 is the intercept term value of the model.\n-- β1, β2, . . . , βn is the value coefficients of the pre-\ndictor variables X1, X2, . . . ,Xn, which represent\ndifferent features extracted from social media ac-\ntivity, such as the frequency of posts, sentiment\nanalysis scores, or the use of specific words related\nto depression.\n-- X1, X2, . . . ,Xn are the predictor variables (features)\nderived from social media data.\n• Long Short-Term Memory (LSTM) networks [29] are\na special type of recurrent neural network (RNN) de-\nsigned to solve the missing space problem in traditional\nRNNs. LSTM is particularly useful for tasks that deal\nwith continuous objects, such as time estimation and\nnatural language processing. The key to their success is\nintegrating brain memory, which is equipped with gates\n(input, memory, and output) that control the network’s\ninformation flow. The input gate decides what data to\nstore in memory, the memory gate decides what to dis-\ncard, and the output gate calculates the next hidden state\nbased on the ideas and current state of memory. The\nbasic equations governing an LSTM unit are as follows:\n-- Forget gate:\nft = σ(Wf · [ht−1, xt ] +bf ) (6)\n-- Input gate:\nit = σ(Wi · [ht−1, xt ] +bi) (7)\n-- Cell state update:\n˜Ct = tanh(WC · [ht−1, xt ] +bC ) (8)\nCt = ft ∗ Ct−1 + it ∗ ˜Ct (9)\n-- Output gate:\not = σ(Wo · [ht−1, xt ] +bo) (10)\nVOLUME 11, 2023 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3387695\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nMuhammad et al.et al.: Novel Transformer Based Contextualized Embedding and Probabilistic Features for Depression Detection from Social Media\n-- Hidden state update:\nht = ot ∗ tanh(Ct ) (11)\nWhere:\n-- ft is the forget gate’s value activation vector,\n-- it is the input gate’s value activation vector,\n-- ˜Ct is the cell input value activation vector,\n-- Ct is the cell state vector value,\n-- ot is the output gate’s value activation vector,\n-- ht is the hidden state vector values (also the output\nvector of the LSTM unit),\n-- xt is the input vector at time step t,\n-- ht−1 is the hidden state vector at time step t − 1,\n-- Ct−1 is the cell state vector at time step t − 1,\n-- W and b are the weights and biases of their respec-\ntive gates,\n-- σ is the sigmoid function, and\n-- ∗ denotes element-wise multiplication.\nF. PHASE 6: HYPERPARAMETER SETTING\nTable 2 presents the analysis of fine-tuning conducted. We\nexplore the key areas of each applied model used to enhance\nperformance by selecting critical features in deep learning\nmodels for predicting depression-related information [30].\nThrough iterative training and validation processes, we iden-\ntify the optimal hyperparameters, which help improve effi-\nciency and increase accuracy in depression analysis.\nTABLE 2. Optimizing hyperparameters in used deep learning models\nModel Parameters and description\nLR random_state=0, test_size=0.2, n_splits=10 ,Input Size\n(input_dim), random_state=Non, shuffle=True\nRF n_estimators=100, max_depth=100, random_state=0,\ncriterion=’gini’, verbose=0,class_weight=None,\nmulti_class=’auto’\nKN n_neighbors=3, test_size=0.3, ran-\ndom_state=40,algorithm=’auto’, weights=’uniform’\nMLP random_state=1, max_iter=35, test_size=0.2,\nrandom_state=48,nesterovs_momentum=True,\npower_t=0.5\nLSTM Dense(16,activation=’relu’),\nDense(32,activation=’relu’),\nDense(1,activation=’sigmoid’)\nIV. EXPERIMENTS AND OBSERVATIONS\nThis section reviews the results and discusses using deep\nlearning models for depression recognition. This study in-\nvolved standard data, including different depression research\ncases, to test the model’s effectiveness. The results and anal-\nyses of this study show that the use of deep learning models\nis effective in identifying the problem of depression.\nA. EXPERIMENTAL SETUP\nWork with deep and machine learning models, including\ndeveloping complex Python programs, specifically version\n3.6 of the language. The Pandas module is used to import\nand analyze stress data. The evaluation is carried out on\nGoogle Colab, utilizing a configuration that includes a GPU\nbackend, 13 GB of RAM, and 90 GB of storage. To assess\nthe performance of the machine learning models, metrics like\nrecall, accuracy, precision, and the F1 score are employed.\nThe used metrics are described below:\n• TP = True Positives: The number of correctly identified\ndepressed posts.\n• TN = True Negatives: The number of correctly identified\nnot depressed posts.\n• FP = False Positives: The number of not-depressed posts\nincorrectly identified as depressed.\n• FN = False Negatives: The number of depressed posts\nincorrectly identified as not depressed.\n1) Recall\nRecall, also known as sensitivity, measures the proportion of\nactual depressed users that were correctly identified:\nRecall = TP\nTP + FN (12)\n2) Accuracy\nAccuracy is simply a ratio of correctly predicted observations\nto the total observations:\nAccuracy = TP + TN\nTP + TN + FP + FN (13)\n3) Precision\nPrecision, also known as positive predictive value, is the ratio\nof true positive predictions to the total positive predictions:\nPrecision = TP\nTP + FP (14)\n4) F1\nThe F1 score, a harmonic mean of precision and recall,\nensures that the model’s precision and recall are taken into\naccount, providing a more balanced view of its performance.:\nF1 = 2· precision · recall\nprecision + recall (15)\nB. RESULTS WITH BERT EMBEDDING FEATURES\nThe performance results of machine learning models apply-\ning BERT features for depression detection are analyzed in\nthis section. The performance metrics for each method are\nassessed using unseen test data. Performance results from\napplying BERT embedding features are presented in Table\n3. The analysis reveals that the Logistic Regression (LR)\nclassifier achieved an accuracy, precision, recall, and F1 score\nof 0.56. The K-Neighbors classifier followed with a recall,\naccuracy, precision, and F1 score of 0.61. The Random Forest\n(RF) classifier achieved the highest performance with accu-\nracy, precision, recall, and F1 score of 0.71. The Multi-Layer\nPerceptron (MLP) classifier recorded an accuracy of 0.51, a\nprecision of 0.55, a recall of 0.51, and an F1 score of 0.42.\nThis analysis concludes that while moderate performance\n8 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3387695\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nMuhammad et al.et al.: Novel Transformer Based Contextualized Embedding and Probabilistic Features for Depression Detection from Social Media\nscores are achieved using BERT embedding features, there\nis still a need for improvement.\nTABLE 3. Results with BERT Embedding features.\nTechnique Accuracy Precision Recall F1\nLR 0.56 0.56 0.56 0.56\nRF 0.71 0.72 0.72 0.72\nKNC 0.61 0.61 0.61 0.61\nMLP 0.51 0.55 0.51 0.42\nTABLE 4. Class-wise performance analysis with the BERT features.\nModels Accuracy Target Class Precision Recall F1Score\nLR 0.56\nDepressed 0.56 0.57 0.56\nNon-Depressed 0.56 0.56 0.56\nAverage 0.56 0.56 0.56\nRF 0.71\nDepressed 0.72 0.71 0.72\nNon-Depressed 0.72 0.73 0.72\nAverage 0.72 0.72 0.72\nKN 0.61\nDepressed 0.60 0.63 0.62\nNon-Depressed 0.62 0.59 0.60\nAverage 0.61 0.61 0.61\nMLP 0.51\nDepressed 0.51 0.92 0.65\nNon-Depressed 0.58 0.11 0.18\nAverage 0.55 0.51 0.42\nIn addition, we have determined the class-wise perfor-\nmance results of the models applied with BERT Embedding\nfeatures, as described in Table 4. The analysis reveals that the\nRF model achieved a 72% precision score for the depressed\nclass. This analysis indicates that the results for each class are\nlow.\nC. RESULTS WITH NOVEL PROPOSED BERT-RF FEATURES\nIn this section, the performance results of the proposed BERT-\nRF features with deep learning models applied to depression\ndetection are analyzed. An evaluation of the performance\nmetrics of each method is conducted using test data. Table\n5 describes the performance of the applied methods using\ntesting data based on the BERT-RF model. Results show that\nthe LR (Logistic Regression) classifier outperformed others\nwith a recall of 0.99, precision of 0.99, and an F1 score of\n0.99. The RF (Random Forest) classifier has an accuracy\nof 0.98, recall of 0.99, precision of 0.99, and an F1 score\nof 0.99. The K-Neighbors classifier achieves an accuracy of\n0.99, precision of 0.99, recall of 0.99, and an F1 score of 0.99.\nThe multilayer perceptron (MLP) classifier has an accuracy of\n0.99, recall of 0.98, precision of 0.98, and an F1 score of 0.98.\nThis analysis shows the superiority of the proposed BERT-RF\nfeatures in achieving high-performance accuracy scores for\ndepression recognition from social media.\nTables 6 present a class-wise performance analysis using\nthe BERT-RF-based model. Our analysis aims to provide an\noverall assessment of the performance of various models on\nthe given data, with a particular emphasis on class accuracy.\nThe results indicate that the performance of the Multilayer\nPerceptron (MLP) approach achieved a score of 0.97 for\nthe depression group, which is lower compared to others.\nTABLE 5. Results with novel proposed BERT-RF features.\nTechnique Accuracy Precision Recall F1\nRF 0.98 0.99 0.99 0.99\nKNC 0.98 0.99 0.99 0.99\nMLP 0.98 0.98 0.98 0.98\nLSTM 0.94 0.95 0.94 0.94\nLR 0.99 0.99 0.99 0.99\nThis lower score demonstrates the challenges in accurately\ndefining the nature of depression using the MLP approach.\nIn contrast, other models perform well, with category scores\nconsistently above 0.98 in our analysis. This suggests that\nwhile MLPs struggle with accurately classifying stressors,\nother models excel in the decision-making process across\nall groups in the dataset. Overall, this analysis indicates that\nall models achieved high-performance scores using a novel\nproposed approach for depression recognition from social\nmedia.\nTABLE 6. Class-wise performance analysis after the BERT\nModels Accuracy Target Class Precision Recall F1Score\nRF 0.98\nDepressed 0.99 0. 99 0. 99\nNon-Depressed 0. 99 0. 99 0. 99\nAverage 0. 99 0. 99 0. 99\nKNC 0.98\nDepressed 0. 99 0. 99 0. 99\nNon-Depressed 0. 99 0. 99 0. 99\nAverage 0. 99 0. 99 0. 99\nMLP 0.98\nDepressed 0.97 1.00 0.98\nNon-Depressed 1.00 0.97 0.98\nAverage 0.98 0.98 0.98\nThe time series baseline chart performance results analysis\nof the applied LSTM model is illustrated in Figure 5. The\nanalysis shows that the neural network LSTM achieved high\nerror rates when the model started its training; however, after\nthe second epoch, the results improved. In addition, we have\nperformed the radar chart-based performance mapping, as\nshown in Figure 6. This analysis demonstrates the superiority\nof the proposed LR models for detecting depression from\nsocial media.\nFIGURE 5. Impact of Learning Rate on Convergence: Training Loss over\nEpoch.\nVOLUME 11, 2023 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3387695\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nMuhammad et al.et al.: Novel Transformer Based Contextualized Embedding and Probabilistic Features for Depression Detection from Social Media\n \nAccuracy Precision Recall F1\n \nFIGURE 6. Multivariate Analysis of Performance Metrics Using Radar\nGraphs: Deep Learning Models.\nD. RESULTS OF PROPOSED METHOD\nIn this section, we examine the performance metric results\nof our proposed logistic regression (LR) model. Table 7\npresents an overview of the performance metrics and results\nfor a specific plan category. Our model achieves an accuracy,\nrecall, precision, and F1 score of 0.99. This underscores the\neffectiveness of the LR method in accurately identifying vari-\nous levels of depression, including depressed, non-depressed,\nand moderate cases. Performance evaluations indicate that\nthe LR model can differentiate between these categories of\ndepression with high accuracy and confidence.\nTABLE 7. Performance results of proposed LR method.\nModels Accuracy Target Class Precision Recall F1Score\nLR 0.99\nDepressed 0. 99 0. 99 0. 99\nNon-Depressed 0. 99 0. 99 0. 99\nAverage 0. 99 0. 99 0. 99\nE. CONFUSION MATRIX RESULTS AND HISTOGRAM\nANALYSIS\nFigure 7 presents the outcomes of the confusion matrix anal-\nysis conducted to assess the performance of various machine\nlearning models. The results indicate that the Multilayer\nPerceptron (MLP) model generated 65 incorrect predictions,\nthe LSTM model produced 228 inaccurate predictions, and\nthe K-Nearest Neighbors (KNN) algorithm accounted for\n53 errors. In addition, the Logistic Regression (LR) model\nyielded 43 incorrect predictions. These findings validate the\neffectiveness of the error evaluation method when applied to\nthe dataset used. Overall, the analysis suggests that the perfor-\nmance of these machine-learning technologies is suboptimal.\nThe comparison between the BERT and the proposed\nBERT-RF approach, as illustrated in Figures 8 and 9, demon-\nstrates that the BERT-RF model significantly outperforms\nother methods. This superiority is clearly illustrated by the\nhigher accuracy metrics using the BERT-RF model achieved\non the chart. The graphical representation, particularly, under-\nscores the superior performance of the BERT-RF approach,\nfurther supported by consistently high scores across various\nperformance metrics.\nF. KFOLD CROSS-VALIDATION RESULTS\nIn this section, we employ k-fold validation to assess the\nperformance of the newly developed BERT-based logistic\nregression model. Table 8 illustrates that utilizing k-fold val-\nidation can enhance the prediction model’s performance by\nmitigating the effects of variance across different datasets and\nfacilitating a more accurate evaluation of the model’s predic-\ntive capability. We opted for a 10-fold validation approach\nto analyze the results. The aggregated model demonstrated a\nremarkable average k-fold accuracy of 0.99. Ultimately, the\nanalysis revealed that the composites prepared using logistic\nregression (LR) achieved a significance level of 0.99 and a\nminimal variance of (+/-) 0.0131. These findings underscore\nthe reliability and consistency of the proposed BERT-RF-\nbased logistic regression model in delivering accurate and\nstable outcomes.\nTABLE 8. The 10-fold-based performance validations Bert base LR model.\nK-Folds Kfold Accuracy\n1 0.98\n2 0.99\n3 0.99\n4 0.98\n5 0.98\n6 0.98\n7 0.98\n8 0.98\n9 0.94\n10 0.99\nAverage 0.99\nStandard Deviation (+/-) 0.0131\nIn addition, we also performed a k-fold cross-validation\nanalysis of other applied methods, as shown in Table 9.\nThis analysis further demonstrates that by utilizing the pro-\nposed BERT-RF features, all applied machine learning mod-\nels achieved generalization in detecting depression from so-\ncial media.\nTABLE 9. Kfold results of all applied methods.\nMethod Fold 1 Fold 2 Fold 3 Fold 4 Fold 5 Mean\nAccuracy\nRF 0.983 0.984 0.985 0.985 0.985 0.98\nMLP 0.990 0.989 0.988 0.990 0.988 0.99\nKNC 0.988 0.987 0.98 0.99 0.987 0.99\nG. STATISTICAL T-TEST ANALYSIS\nWe have also conducted a statistical T-Test analysis to com-\npare the proposed Logistic Regression (LR) approach against\nother methodologies, as illustrated in Table 10. This analysis\nreveals that our proposed approach significantly outperforms\nthe alternatives in terms of performance scores, leading to\n10 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3387695\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nMuhammad et al.et al.: Novel Transformer Based Contextualized Embedding and Probabilistic Features for Depression Detection from Social Media\n(a) LSTM\n (b) LR\n(c) KNC\n (d) MLP\nFIGURE 7. The Performance Evaluation: Confusion Matrix Analysis\n \n0.0\n0.2\n0.4\n0.6\n0.8\nLR RFC KNC MLP\nAccuracy Precision Recall F1\n \nFIGURE 8. Histogram results with only BERT features.\nthe rejection of the null hypothesis in each instance for the\ndetection of depression.\n \n0.00\n0.25\n0.50\n0.75\n1.00\nRFC LR KNC MLP\nAccuracy Precision Recall F1\n \nFIGURE 9. Histogram results with proposed BERT-RF features.\nH. FEATURE SPACE ANALYSIS WITH PROPOSED BERT-RF\nWe also visualized the features using a 3D scatter plot, which\nis a visualization technique. This analysis aims to explore\nand represent data in three-dimensional space, as shown in\nVOLUME 11, 2023 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3387695\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nMuhammad et al.et al.: Novel Transformer Based Contextualized Embedding and Probabilistic Features for Depression Detection from Social Media\nTABLE 10. Results of statistical T-Test analysis.\nProposed vs. Others T-statistic P-value Null Hypothesis ( Ho)\nLR vs RF 108.99 1.702 Rejected\nLR vs MLP inf 0.0 Rejected\nLR vs KN 17.90 0.0003 Rejected\nLR vs LSTM 19.0 0.0003 Rejected\nFigure 10. Data points are viewed line by line in 3D space,\nwhere closeness in the diagram indicates the similarity of the\nmain points. This analysis shows that newly created features\nusing the BERT-RF approach are highly linearly separable,\nwhich helps us achieve high-performance results for applied\nmachine learning models.\nFIGURE 10. Feature Reduction for Improved Insight: PCA in Text.\nI. STATE OF THE ART RESULTS COMPARISON\nThis section focuses on objective and qualitative analysis\nin state-of-the-art comparison analyses. The basis of our\nresearch is to compare our scheme with previous studies\nexamining depression data, as shown in Table 11. The com-\nparison results show that our proposed LR method stands out\nin the search for depression detection. What sets it apart is\nits exceptional quality, which boasts an impressive accuracy\nof 0.99. It plays a crucial role in bridging the gap observed\nin previous studies. Our scheme successfully addresses the\ndiscrepancy between the best-performing scores in the search\nfor depression. By achieving high accuracy, our method not\nonly proves its effectiveness in the study but also contributes\nto the advancement of depression research and the treatment\nof limitations identified in previous studies.\nJ. DISCUSSIONS AND LIMITATIONS\nOur research on detecting depression from social data using\nBERT-based content embeddings and advanced probabilistic\nTABLE 11. State of the art results comparisons of the proposed approach.\nRef. Proposed Approach Performance Score\n[31] DistilBert 89%\n[13] Random forest algorithm 77%\n[16] logistic regression classifier 86%\n[14] TF-IDF Apporach 89%\n[32] Convolutional Neural Network(CNN) 94%\nOur BERT-RF Logistic Regression 99%\nfeatures provides a new way to understand negative emotions.\nAlthough our method significantly improves to 0.99% ac-\ncuracy, some inconsistent studies and limitations are worth\nnoting.\n• First, the generalizability of our model across many\nsocial networks, demographic groups, and cultural con-\ntexts remains an area for investigation. Examining dif-\nferent regulations and their long-term performance will\nincrease the validity of our approach.\n• Additionally, considering the use of BERT-based em-\nbeddings, the interpretation of the prediction model\nneeds to be further evaluated for clarity and reliability.\n• Ethical considerations regarding mental health profil-\ning and responsible use of information on social media\nshould be validated, including issues of user consent and\nprivacy.\n• Additionally, our model is based on specific language\npatterns that express melancholy. This leads to a discus-\nsion of the possible limitations of why individuals may\ndisplay melancholic behaviour differently in less formal\nsettings.\nV. CONCLUSION AND FUTURE DIRECTIONS\nThis article presents advanced machine learning models for\nstress analysis, including Random Forest Classifier (RFC),\nMultilayer Perceptron (MLP), K-Nearest Neighbors Classi-\nfier (KNC), and Logistic Regression (LR), with a focus on\nBERT-based deep learning techniques. The results demon-\nstrate that the proposed scheme is highly effective, achieving\nan accuracy rate of 99%, and outperforms existing models\nin detecting depression. This study aims to contribute to the\nfield by addressing the limitations of previous research and\nshowcasing the effectiveness of deep learning in identifying\npatterns of depression. These findings are significant and\npromising for both the research and treatment of depres-\nsion. The success and accuracy of this study underscore the\npotential of novel machine-learning approaches in mental\nhealth. Overall, this study underscores the critical role of deep\nlearning models in accurately identifying and understanding\ndepression, offering insights into future developments in this\nfield.\nA. FUTURE WORK\nIn future work, we will build a web-based API framework\nthat detects depression from user posts in real-time on social\nmedia platforms.\n12 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3387695\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nMuhammad et al.et al.: Novel Transformer Based Contextualized Embedding and Probabilistic Features for Depression Detection from Social Media\nACKNOWLEDGMENT\nThe authors would like to express their grateful to Princess\nNourah bint Abdulrahman University Researchers Support-\ning Project number (PNURSP2024R104), Princess Nourah\nbint Abdulrahman University, Riyadh, Saudi Arabia.\nFUNDING\nThis research was funded by Princess Nourah bint Abdul-\nrahman University Researchers Supporting Project number\n(PNURSP2024R104), Princess Nourah bint Abdulrahman\nUniversity, Riyadh, Saudi Arabia.\nREFERENCES\n[1] D. William and D. Suhartono, ‘‘Text-based depression detection on social\nmedia posts: A systematic literature review,’’ Procedia Computer Science ,\nvol. 179, pp. 582–589, 2021.\n[2] J. S. L. Figuerêdo, A. L. L. Maia, and R. T. Calumby, ‘‘Early depression\ndetection in social media based on deep learning and underlying emotions,’’\nOnline Social Networks and Media , vol. 31, p. 100225, 2022.\n[3] H. Zogan, I. Razzak, S. Jameel, and G. Xu, ‘‘Depressionnet: A novel\nsummarization boosted deep framework for depression detection on social\nmedia,’’arXiv preprint arXiv:2105.10878 , 2021.\n[4] D. S. Smys and D. J. S. Raj, ‘‘Analysis of deep learning techniques for early\ndetection of depression on social media network-a comparative study,’’\nJournal of Trends in Computer Science and Smart Technology , vol. 3, no. 1,\npp. 24–39, 2021.\n[5] J. de Jesús Titla-Tlatelpa, R. M. Ortega-Mendoza, M. Montes-y Gómez,\nand L. Villaseñor-Pineda, ‘‘A profile-based sentiment-aware approach for\ndepression detection in social media,’’ EPJ data science , vol. 10, no. 1,\np. 54, 2021.\n[6] V . Adarsh, P. A. Kumar, V . Lavanya, and G. Gangadharan, ‘‘Fair and\nexplainable depression detection in social media,’’ Information Processing\n& Management, vol. 60, no. 1, p. 103168, 2023.\n[7] E. A. Ríssola, S. A. Bahrainian, and F. Crestani, ‘‘A dataset for research on\ndepression in social media,’’ in Proceedings of the 28th ACM conference\non user modeling, adaptation and personalization , pp. 338–342, 2020.\n[8] H. Senra and S. McPherson, ‘‘Depression in disabling medical conditions–\ncurrent perspectives,’’ International Review of Psychiatry , vol. 33, no. 3,\npp. 312–325, 2021.\n[9] S. C. Guntuku, D. B. Yaden, M. L. Kern, L. H. Ungar, and J. C. Eichstaedt,\n‘‘Detecting depression and mental illness on social media: an integrative\nreview,’’Current Opinion in Behavioral Sciences , vol. 18, pp. 43–49, 2017.\n[10] S. Almouzini, A. Alageel, et al. , ‘‘Detecting arabic depressed users from\ntwitter data,’’ Procedia Computer Science , vol. 163, pp. 257–265, 2019.\n[11] J. Deepali, J. Makhija, Y . Nabar, and N. Nehet, ‘‘Mental health analysis\nusing deep learning for feature extraction,’’ 2018.\n[12] T. Gui, L. Zhu, Q. Zhang, M. Peng, X. Zhou, K. Ding, and Z. Chen,\n‘‘Cooperative multimodal approach to depression detection in twitter,’’\nin Proceedings of the AAAI conference on artificial intelligence , vol. 33,\npp. 110–117, 2019.\n[13] F. Azam, M. Agro, M. Sami, M. H. Abro, and A. Dewani, ‘‘Identifying\ndepression among twitter users using sentiment analysis,’’ in 2021 Interna-\ntional Conference on Artificial Intelligence (ICAI) , pp. 44–49, IEEE, 2021.\n[14] P. Kumar, P. Samanta, S. Dutta, M. Chatterjee, and D. Sarkar, ‘‘Feature\nbased depression detection from twitter data using machine learning tech-\nniques,’’Journal of Scientific Research , vol. 66, no. 2, pp. 220–228, 2022.\n[15] S. Pachouly, G. Raut, K. Bute, R. Tambe, and S. Bhavsar, ‘‘Depression\ndetection on social media network (twitter) using sentiment analysis,’’ Int.\nRes. J. Eng. Technol , vol. 8, pp. 1834–1839, 2021.\n[16] S. Jain, S. P. Narayan, R. K. Dewang, U. Bhartiya, N. Meena, and V . Ku-\nmar, ‘‘A machine learning based depression analysis and suicidal ideation\ndetection system using questionnaires and twitter,’’ in 2019 IEEE students\nconference on engineering and systems (SCES) , pp. 1–6, IEEE, 2019.\n[17] H. Kour and M. K. Gupta, ‘‘An hybrid deep learning approach for depres-\nsion prediction from user tweets using feature-rich cnn and bi-directional\nlstm,’’ Multimedia Tools and Applications , vol. 81, no. 17, pp. 23649–\n23685, 2022.\n[18] R. Safa, P. Bayat, and L. Moghtader, ‘‘Automatic detection of depression\nsymptoms in twitter using multimodal analysis,’’ The Journal of Supercom-\nputing, vol. 78, no. 4, pp. 4709–4744, 2022.\n[19] D. B. Victor, J. Kawsher, M. S. Labib, and S. Latif, ‘‘Machine learning\ntechniques for depression analysis on social media-case study on bengali\ncommunity,’’ in 2020 4th International Conference on Electronics, Com-\nmunication and Aerospace Technology (ICECA) , pp. 1118–1126, IEEE,\n2020.\n[20] S. R. Kamite and V . Kamble, ‘‘Detection of depression in social me-\ndia via twitter using machine learning approach,’’ in 2020 International\nConference on Smart Innovations in Design, Environment, Management,\nPlanning and Computing (ICSIDEMPC) , pp. 122–125, IEEE, 2020.\n[21] INFAMOUSCODER, ‘‘Depression: Twitter dataset + feature extraction.’’\nhttps://www.kaggle.com/datasets/infamouscoder/mental-health-social-\nmedia. (Accessed on 02/11/2024).\n[22] M. I. MOBIN, M. F. MRIDHA, and S. H. MAHMUD, ‘‘Exploratory anal-\nysis of suicidal tendency in depression investigation social media post,’’\n2024.\n[23] A. Raza, K. Munir, and M. Almutairi, ‘‘A novel deep learning approach\nfor deepfake image detection,’’ Applied Sciences , vol. 12, no. 19, p. 9820,\n2022.\n[24] A. Raza, I. Akhtar, L. Abualigah, R. A. Zitar, M. Sharaf, M. S. Daoud, and\nH. Jia, ‘‘Preventing road accidents through early detection of driver behav-\nior using smartphone motion sensor data: An ensemble feature engineering\napproach,’’IEEE Access, vol. 11, pp. 138457–138471, 2023.\n[25] A. Raza, A. M. Qadri, I. Akhtar, N. A. Samee, and M. Alabdulhafith,\n‘‘Logrf: An approach to human pose estimation using skeleton landmarks\nfor physiotherapy fitness exercise correction,’’ IEEE Access, 2023.\n[26] A. Raza, F. Rustam, H. U. R. Siddiqui, I. d. l. T. Diez, B. Garcia-Zapirain,\nE. Lee, and I. Ashraf, ‘‘Predicting genetic disorder and types of disorder\nusing chain classifier approach,’’ Genes, vol. 14, no. 1, p. 71, 2022.\n[27] A. Raza, K. Munir, M. S. Almutairi, and R. Sehar, ‘‘Novel class probability\nfeatures for optimizing network attack detection with machine learning,’’\nIEEE Access, 2023.\n[28] A. Raza, F. Rustam, H. U. R. Siddiqui, I. d. l. T. Diez, and I. Ashraf,\n‘‘Predicting microbe organisms using data of living micro forms of life and\nhybrid microbes classifier,’’ Plos one, vol. 18, no. 4, p. e0284522, 2023.\n[29] A. Raza, K. Munir, M. S. Almutairi, and R. Sehar, ‘‘Novel transfer learning\nbased deep features for diagnosis of down syndrome in children using facial\nimages,’’IEEE Access, vol. 12, pp. 16386–16396, 2024.\n[30] A. Alsaeedi and M. Z. Khan, ‘‘A study on sentiment analysis techniques\nof twitter data,’’ International Journal of Advanced Computer Science and\nApplications, vol. 10, no. 2, pp. 361–374, 2019.\n[31] M. Rizwan, M. F. Mushtaq, U. Akram, A. Mehmood, I. Ashraf, and\nB. Sahelices, ‘‘Depression classification from tweets using small deep\ntransfer learning language models,’’ IEEE Access , vol. 10, pp. 129176–\n129189, 2022.\n[32] A. A. Baale, O. R. Olasunkanmi, F. E. Adelodun, and A. A. Adigun, ‘‘Opin-\nion analysis and machine learning modeling for depression detection,’’\nMUHAMMAD ASAD ABBASpursuing his MS\nInformation Technology degree from the Khwaja\nFareed University of Engineering and Informa-\ntion Technology Rahim Yar Khan, Pakistan. He\nreceived the BS. Degree in Information Technol-\nogy from the Department of Information Technol-\nogy, Bahauddin Zakariya University sub campus\nLodhran, Pakistan, in 2021. His current research\ninterest includes Data Science, Artificial Intelli-\ngence, Data mining, Natural Language Processing,\nand Machine learning.\nVOLUME 11, 2023 13\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3387695\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nMuhammad et al.et al.: Novel Transformer Based Contextualized Embedding and Probabilistic Features for Depression Detection from Social Media\nKASHIF MUNIR earned a B.Sc. degree in math-\nematics and physics from Islamia University Ba-\nhawalpur, Pakistan, in 1999. Subsequently, he ob-\ntained an M.Sc. degree in information technology\nfrom the University Sains Malaysia in 2001, an\nM.S. degree in software engineering from the Uni-\nversity of Malaya, Malaysia, in 2005, and a Ph.D.\ndegree in informatics from the Malaysia University\nof Science and Technology in 2015. Engaged in\nhigher education since 2002, he taught initially at\nBinary College, Malaysia, for a semester, followed by approximately four\nyears at Stamford College, Malaysia. Later, he moved to Saudi Arabia, work-\ning at the King Fahd University of Petroleum and Minerals from September\n2006 to December 2014. In January 2015, he transitioned to the University\nof Hafr Al-Batin, Saudi Arabia, and in July 2021, he joined Khwaja Fareed\nUniversity of Engineering & IT, Rahim Yar Khan, as an Assistant Profes-\nsor in the IT Department. With a substantial publication record, including\njournal articles, conference papers, books, and book chapters, he has served\non technical program committees for numerous peer-reviewed conferences\nand journals, contributing to the review of numerous research papers. His\nresearch interests span cloud computing security, software engineering, and\nproject management.\nALI RAZA is a lecturer at the Faculty of Infor-\nmation Technology, Department of Software En-\ngineering, University Of Lahore, Pakistan. He re-\nceived a Bachelor of Science degree in computer\nscience from the Department of Computer Sci-\nence, Khwaja Fareed University of Engineering\nand Information Technology (KFUEIT), Rahim\nYar Khan, Pakistan, in 2021, where he completed\nthe M.S. degree in computer science in 2023. He\nhas published several articles in reputed journals.\nHis current research interests include data science, artificial intelligence, data\nmining, natural language processing, machine learning, deep learning, and\nimage processing.\nNAGWAN ABDEL SAMEEreceived the B.S. de-\ngree in computer engineering from Ein Shams\nUniversity, Egypt, in 2000 and the M.S. degree\nin computer engineering from Cairo University,\nEgypt in 2008. In 2012, she has received the Ph.D.\ndegree in Systems and Biomedical Engineering\nfrom Cairo University, Egypt. Since 2013, she has\nbeen an Assistant Professor with the Information\nTechnology Department, CCIS, Princess Nourah\nbint Abdulrahman University, Riyadh, KSA. Her\nresearch interests include Data Science, Machine Learning, Bioinformatics,\nParallel Computing. Dr. Nagwan awards and honors include the Takafull\nPrize (Innovation Project Track), Princess Nourah Award in innovation,\nMastery Award in predictive analytics (IBM), Mastery Award in Big Data\n(IBM), and Mastery Award in Cloud Computing(IBM).\nMONA M. JAMJOOMis an associate professor\nat the department of computer sciences, College\nof Computer and Information Sciences, Princess\nNourah Bint Abdulrahman University, Riyadh\nSaudi Arabia. She got her PhD degree in com-\nputer science from King Saud University. Her area\nof interest include artificial intelligence, machine\nlearning, deep learning, medical imaging, and data\nscience. She has published several research articles\nin her field.\nZAHID ULLAHis an experienced educator and re-\nsearcher of computer science and information sys-\ntems. He got his doctorate from the University of\nKuala Lumpur in Malaysia. He is currently work-\ning as assistant professor at King Abdulaziz Uni-\nversity, Jeddah, Saudi Arabia. His research areas\ninclude machine learning, deep learning, medical\nimaging, and data science. He published various\narticles in his field of specialization.\n14 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3387695\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Probabilistic logic",
  "concepts": [
    {
      "name": "Probabilistic logic",
      "score": 0.7203227281570435
    },
    {
      "name": "Computer science",
      "score": 0.6180272102355957
    },
    {
      "name": "Embedding",
      "score": 0.5896053910255432
    },
    {
      "name": "Transformer",
      "score": 0.5270407795906067
    },
    {
      "name": "Social media",
      "score": 0.4785090982913971
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4084143042564392
    },
    {
      "name": "Computer security",
      "score": 0.3398156464099884
    },
    {
      "name": "World Wide Web",
      "score": 0.1714896261692047
    },
    {
      "name": "Electrical engineering",
      "score": 0.10553213953971863
    },
    {
      "name": "Engineering",
      "score": 0.08059889078140259
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}