{
  "title": "How much pretraining data do language models need to learn syntax?",
  "url": "https://openalex.org/W3198757395",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5043850175",
      "name": "Laura Pérez-Mayos",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2159534029",
      "name": "Miguel Ballesteros",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1828080704",
      "name": "Leo Wanner",
      "affiliations": [
        "Institució Catalana de Recerca i Estudis Avançats",
        "Pompeu Fabra University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6631349028",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4289552613",
    "https://openalex.org/W2515741950",
    "https://openalex.org/W3105069964",
    "https://openalex.org/W2962961857",
    "https://openalex.org/W3103469330",
    "https://openalex.org/W3104235057",
    "https://openalex.org/W2964204621",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W2996728628",
    "https://openalex.org/W2552110825",
    "https://openalex.org/W3176198948",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W3035137491",
    "https://openalex.org/W2973154008",
    "https://openalex.org/W2963940534",
    "https://openalex.org/W2963691697",
    "https://openalex.org/W2953369973",
    "https://openalex.org/W3031001133",
    "https://openalex.org/W3099668342",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W3100251125",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W3088059392",
    "https://openalex.org/W2972896975",
    "https://openalex.org/W3037115370",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3034510440",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2531882892",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2971016963",
    "https://openalex.org/W3005700362",
    "https://openalex.org/W4287867774",
    "https://openalex.org/W2918996109",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2250263931",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W2963751529",
    "https://openalex.org/W3035305735",
    "https://openalex.org/W2888922637",
    "https://openalex.org/W3102021588",
    "https://openalex.org/W4302023899",
    "https://openalex.org/W2921890305",
    "https://openalex.org/W3098613713",
    "https://openalex.org/W2891399254",
    "https://openalex.org/W3099299360",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970862333",
    "https://openalex.org/W3168987555"
  ],
  "abstract": "Comunicació presentada a 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP 2021), celebrat del 7 a l'11 de novembre de 2021 de manera virtual.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1571–1582\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n1571\nHow much pretraining data do language models need to learn syntax?\nLaura Pérez-Mayos*1, Miguel Ballesteros2, Leo Wanner3,1\n1 TALN Research Group, Pompeu Fabra University, Barcelona, Spain\n2 Amazon AI\n3 Catalan Institute for Research and Advanced Studies (ICREA), Barcelona, Spain\n{laura.perezm|leo.wanner}@upf.edu\nballemig@amazon.com\nAbstract\nTransformers-based pretrained language mod-\nels achieve outstanding results in many well-\nknown NLU benchmarks. However, while pre-\ntraining methods are very convenient, they are\nexpensive in terms of time and resources. This\ncalls for a study of the impact of pretraining\ndata size on the knowledge of the models. We\nexplore this impact on the syntactic capabili-\nties of RoBERTa, using models trained on in-\ncremental sizes of raw text data. First, we\nuse syntactic structural probes to determine\nwhether models pretrained on more data en-\ncode a higher amount of syntactic informa-\ntion. Second, we perform a targeted syntactic\nevaluation to analyze the impact of pretraining\ndata size on the syntactic generalization perfor-\nmance of the models. Third, we compare the\nperformance of the different models on three\ndownstream applications: part-of-speech tag-\nging, dependency parsing and paraphrase iden-\ntiﬁcation. We complement our study with an\nanalysis of the cost-beneﬁt trade-off of train-\ning such models. Our experiments show that\nwhile models pretrained on more data encode\nmore syntactic knowledge and perform bet-\nter on downstream applications, they do not\nalways offer a better performance across the\ndifferent syntactic phenomena and come at a\nhigher ﬁnancial and environmental cost.\n1 Introduction\nThe use of unsupervised pretrained language\nmodels in the context of supervised tasks has\nbecome a widely spread practice in NLP, with\nTransformer-based models such as BERT (Devlin\net al., 2019) and RoBERTa (Liu et al., 2019b)\nachieving outstanding results in many well-known\nNatural Language Understanding benchmarks such\nas GLUE (Wang et al., 2018) and SQuAD (Ra-\njpurkar et al., 2018). Consequently, several stud-\nies investigate the types of knowledge learned by\n∗ Work partially done during internship at Amazon AI.\nBERT, how and where this knowledge is repre-\nsented and what the best methods to improve it are;\nsee, e.g., (Rogers et al., 2020). There is evidence\nthat, among other information (e.g., part-of-speech,\nsyntactic chunks and roles (Tenney et al., 2019; Lin\net al., 2019; Belinkov et al., 2017), morphology in\ngeneral (Peters et al., 2018), or sentence length\n(Adi et al., 2016)), BERT representations implicitly\nembed entire syntax trees (Hewitt and Manning,\n2019b).\nLanguage models are traditionally assessed by\ninformation-theoretical metrics such as perplexity,\ni.e., the probability of predicting a word in its con-\ntext. The general wisdom is that the more pretrain-\ning data a model is fed, the lower its perplexity gets.\nHowever, large volumes of pretraining data are not\nalways available and pretraining is costly, such that\nthe following questions need to be answered: (i)\nDo we always need models pretrained on internet-\nscale corpora? (ii) As the models are pretrained\non more data, and their perplexity improves, do\nthey encode more syntactic information and offer a\nbetter syntactic generalization? (iii) Do the models\nwith more pretraining perform better when applied\nin downstream tasks? To address these questions,\nwe explore the relation between the size of the\npretraining data and the syntactic capabilities of\nRoBERTa by means of the MiniBERTas models, a\nset of 12 RoBERTa models pretrained from scratch\nby Warstadt et al. (2020b) on quantities of data\nranging from 1M to 1B words. In particular:\n• We use the syntactic structural probes from\nHewitt and Manning (2019b) to determine\nwhether those models pretrained on more data\nencode a higher amount of syntactic informa-\ntion than those trained on less data;\n• We perform a targeted syntactic evaluation to\nanalyze the generalization performance of the\ndifferent models using SyntaxGym (Gauthier\net al., 2020) and the syntactic tests presented\nin (Hu et al., 2020);\n1572\n• We compare the performance of the different\nmodels on two morpho-syntactic tasks (PoS\ntagging and dependency parsing), and a non-\nsyntactic task (paraphrase identiﬁcation);\n• We conduct a cost-beneﬁt trade-off analysis\n(Strubell et al., 2019; Bhattacharjee et al.,\n2020) of the models training.\nWe observe that models pretrained on more data\nencode a higher amount of syntax according to He-\nwitt and Manning (2019b)’s metrics, but do not\nalways lead to a better syntactic generalization. In-\ndeed, we ﬁnd that models pretrained on less data\nperform equally good or even better than those pre-\ntrained on more data on 3 out of 6 syntactic test\nsuites. When applied to downstream tasks, the mod-\nels pretrained on more data perform generally bet-\nter. However, the analysis of the trade-off between\nthe cost of training a model and its performance\nshows that small performance gains come at a high\neconomical and environmental cost that should be\nconsidered when developing new models.\nIn what follows, Section 2 provides some back-\nground on the syntactic assessment of language\nmodels, model costs, and the works related to ours.\nSection 3 describes our experimental setup, intro-\nducing the MiniBERTas models and the syntactic\ntests as well as the downstream applications we\nexplore. Section 4 presents the outcome of our ex-\nperiments. Section 5 offers a cost-beneﬁt analysis\nof the pretraining of the different models, and Sec-\ntion 6 summarizes the implications that our work\nhas for the use of pretrained language models.\n2 Background\n2.1 Syntactic assessment of language models\nThe targeted syntactic evaluation incorporates\nmethods from psycholinguistic experiments, fo-\ncusing on highly speciﬁc measures of language\nmodeling performance and allowing to distinguish\nmodels with human-like representations of syn-\ntactic structure (Linzen et al., 2016; Lau et al.,\n2017; Gulordava et al., 2018; Marvin and Linzen,\n2018; Futrell et al., 2019). Regarding the evalu-\nation of modern language models, Warstadt et al.\n(2020a) present a challenge set that isolates speciﬁc\nphenomena in syntax, morphology, and semantics,\nﬁnding that state-of-the-art models struggle with\nsome subtle semantic and syntactic phenomena,\nsuch as negative polarity items and extraction is-\nlands. Hu et al. (2020) test 20 model type combi-\nnations and data sizes on 34 English syntactic test\nsuites, ﬁnding substantial differences in syntactic\ngeneralization performance by model architecture.\nSupervised probing models have also been used\nto test for the presence of a wide range of linguistic\nphenomena (Conneau et al., 2018; Liu et al., 2019a;\nTenney et al., 2019; V oita and Titov, 2020; Elazar\net al., 2020), and it has been shown that entire syn-\ntax trees are embedded implicitly in BERT’s vector\ngeometry (Hewitt and Manning, 2019b; Chi et al.,\n2020). However, other works have criticized some\nprobing methods, claiming that classiﬁer probes\ncan learn the linguistic task from training data\n(Hewitt and Liang, 2019), and can fail to deter-\nmine whether the detected features are actually\nused (V oita and Titov, 2020; Pimentel et al., 2020;\nElazar et al., 2020).\n2.2 Costs of modern language models\nWhile modern language models keep growing in\norders of magnitude, so do the resources necessary\nfor their development and, consequently, also the\ninclusivity gap. The ﬁnancial cost of the required\nhardware and electricity favors industry-powered\nresearch, and harms academics, students, and non-\nindustry researchers, particularly those from emerg-\ning economies. Moreover, the training of such mod-\nels is not only ﬁnancially expensive, but has also\na large carbon footprint. Schwartz et al. (2019)\npropose to report the ﬁnancial cost of developing,\ntraining, and running models in order to provide\nbaselines for the investigation of increasingly ef-\nﬁcient methods. Along the same lines, Strubell\net al. (2019) offer an analysis of the computation\nrequired for the research, development and hyper-\nparameter tuning of several recently successful neu-\nral network models for NLP, and propose action-\nable recommendations to reduce costs and improve\nequity, namely 1) reporting training time and sensi-\ntivity to hyperparameters; 2) a government-funded\nacademic compute cloud to provide equitable ac-\ncess to all researchers; and 3) prioritizing computa-\ntionally efﬁcient hardware and algorithms.\n2.3 Related work\nSeveral studies investigate the relation between pre-\ntraining data size and linguistic knowledge in lan-\nguage models. van Schijndel et al. (2019); Hu\net al. (2020); Micheli et al. (2020) ﬁnd out that,\ngiven a relatively large data size (e.g., 10M words),\nmodels with less pretraining perform similarly to\nmodels with much more pretraining, concluding\nthat model architecture plays a more important role\n1573\nthan training data scale in yielding correct syntactic\ngeneralizations (Hu et al., 2020). Complementary,\nRaffel et al. (2020) shows that performance can de-\ngrade when an unlabeled data set is small enough\nthat it is repeated many times over the course of\npretraining. In contrast, Zhang et al. (2020) argue\nthat while relatively small datasets sufﬁce to reli-\nably encode most syntactic and semantic features,\na much larger quantity of data is needed to master\nconventional NLU tasks. This discrepancy may be\ndue to the difference in model architectures, pre-\ntraining techniques and the scaling and nature of\nthe difference datasets.\nOur work differs signiﬁcantly from recent works.\nWe make use of a single architecture and data\nsource, and focus exclusively on the syntactic capa-\nbilities of the models, offering an in-depth analysis\nthat includes structural syntactic probing, detailed\nsyntactic generalization, and downstream applica-\ntions performance. Moreover, we also provide a\ncost-beneﬁt analysis of the models.\n3 Experimental setup\n3.1 The MiniBERTas models\nThe MiniBERTas are a set of 12 RoBERTa models\npretrained from scratch by Warstadt et al. (2020b)\non 4 datasets containing 1B, 100M, 10M and 1M\ntokens, available through HuggingFace Transform-\ners.1 The datasets are sampled from Wikipedia\nand Smashwords – the two datasets that make up\nthe original pretraining dataset of BERT and that\nare included in the RoBERTa pretraining data. For\neach dataset size, pretraining is run 25 times (10\ntimes for 1B) with varying hyperparameter values;\nthe three models with the lowest development set\nperplexity are released. For the smaller dataset, a\nsmaller model size is used to prevent over-ﬁtting.\nWe refer to models trained on the same amount of\ndata as a family of models, and models inside a\nfamily as intra-family members(e.g.,the roberta-\nbase-100M-1 model is a member of the roberta-\nbase-100M family). Table 1 offers an overview of\nthe hyperparameters per model size.\n3.2 Structural probing\nHewitt and Manning (2019b)’s structural probes as-\nsess how well syntax trees are embedded in a linear\ntransformation of the network representation space\napplying two different evaluations: Tree distance\nevaluation, in which squared L2 distance encodes\n1https://huggingface.co/nyu-mll\nModel Size L AH HS FFN P\nBASE 12 12 768 3072 125M\nMED-SMALL 6 8 512 2048 45M\nTable 1: Hyperparameters per model sizes. AH = num-\nber of attention heads; HS = hidden size; FFN = feed-\nforward network dimension; P = number of parameters.\nthe distance between words in the parse tree, and\nTree depth evaluation, in which squared L2 norm\nencodes the depth in the parse tree.\nTree distance evaluation. Evaluates how well\nthe predicted distances between all pairs of words\nin a model reconstruct gold parse trees by comput-\ning the Undirected Unlabeled Attachment Score\n(UUAS). It also computes the Spearman correla-\ntion between true and predicted distances for each\nword in each sentence, averaging across all sen-\ntences with lengths between 5 and 50 (we refer to\nas DSpr.).\nTree depth evaluation. Evaluates the ability of\nmodels to recreate the order of words speciﬁed by\ntheir depth in the parse tree, assessing their ability\nto identify the root of the sentence as the least deep\nword (Root %) and computing the Spearman corre-\nlation between the predicted and the true depth or-\ndering, averaging across all sentences with lengths\nbetween 5 and 50 (we refer to as NSpr).\n3.3 Targeted syntactic evaluation\nWe test the MiniBERTas on the syntactic tests as-\nsembled by Hu et al. (2020), accessible through\nthe SyntaxGym toolkit (Gauthier et al., 2020). The\ntests are divided into 6 syntactic circuits, intro-\nduced below, based on the type of algorithm re-\nquired to successfully process each construction.\n1. Agreement: Tests a language model for how\nwell it predicts the number marking on English\nﬁnite present tense verbs. It is composed of 3\nSubject-Verb Number Agreement tests from Mar-\nvin and Linzen (2018),\n2. Center Embedding: Tests the ability to em-\nbed a phrase in the middle of another phrase of\nthe same type. Subject and verbs must match in\na ﬁrst-in-last-out order, meaning models must ap-\nproximate a stack-like data-structure in order to\nsuccessfully process them. The circuit is composed\nof 2 tests from Wilcox et al. (2019a).\n3. Garden-Path Effects: Measures the syntac-\ntic phenomena that result from tree structural ambi-\nguities that give rise to locally coherent but globally\n1574\nimplausible syntactic parses. The circuit is com-\nposed of 2 Main Verb / Reduced Relative Clause\n(MVRR) tests and 4 NP/Z Garden-paths (NPZ)\ntests, all from Futrell et al. (2018).\n4. Gross Syntactic Expectation: Tests the abil-\nity of the models to distinguish between coordinate\nand subordinate clauses: introducing a subordina-\ntor at the beginning of the sentence should make an\nending without a second clause less probable, and\nshould make a second clause more probable. The\ncircuit is composed of 4 Subordination tests from\nFutrell et al. (2018).\n5. Licensing: Measures when a particular token\nmust exist within the scope of an upstream licen-\nsor token. The circuit is composed of 4 Negative\nPolarity Item Licensing (NPI) tests and 6 Reﬂex-\nive Pronoun Licensing tests, all from Marvin and\nLinzen (2018).\n6. Long-Distance Dependencies: Measures\ncovariations between two tokens that span long\ndistances in tree depth. The circuit is composed\nof 6 Filler-Gap Dependencies (FGD) tests from\nWilcox et al. (2018) and Wilcox et al. (2019b), and\n2 Cleft tests from (Hu et al., 2020).\n3.4 Encoding unidirectional context with\nbidirectional models\nThe tests in SyntaxGym evaluate whether models\nare able to assign a higher probability to gram-\nmatical and natural continuations of sentences. As\nRoBERTa is a bidirectional model, to be able to ask\nit to predict the probability of a token given the con-\ntext of previous tokens we test it in a left-to-right\ngenerative setup, as done in (Rongali et al., 2020;\nZhu et al., 2020). More precisely, we follow Wang\nand Cho (2019)’s sequential sampling procedure,\nwhich is not affected by the error that was reported\nin equations 1-3, related to the Non-sequential sam-\npling procedure. To compute the probability dis-\ntribution for a sentence with N tokens, we start\nwith a sequence of begin_of_sentence token plus\nN mask tokens plus an extra mask token to account\nfor the end_of_sentence token. For each masked\nposition in [1, N], we compute the probability dis-\ntribution over the vocabulary given the left context\nof the original sequence, and select the probability\nassigned by the model to the original word. Note\nthat this setup allows the models to know how many\ntokens there are in the sentences, and therefore the\nresults are not directly comparable with those of\nunidirectional models, that do not have any infor-\nmation regarding the length of the sequence.\nFor example, in a Subordination test with the\nexamples ‘Because the students did not like the ma-\nterial.’ and ‘The students did not like the material.’,\nwe expect the model to assign a higher surprisal\n(Wilcox et al., 2019c) to the ﬁrst example, because\nthe initial \"Because\" implies that the immediately\nfollowing clause is not the main clause of the sen-\ntence, but instead is a subordinate that must be fol-\nlowed by the main clause. However, instead of ﬁnd-\ning the main clause, the model encounters a dot in-\ndicating the end of the sentence. To test whether the\nmodel has learned about subordination, we feed the\nmodels the tokens sequences [begin_of_sentence,\nBecause, the, students, did, not, like, the, materials,\nmask, mask] and [begin_of_sentence, The, students,\ndid, not, like, the, materials, mask, mask], and com-\npare the surprisal of the model predicting a dot ‘.’\nfor the ﬁrst masked position in each case.\n3.5 Downstream applications\nTo compare the performance of the models on\ndownstream applications, we analyze their learn-\ning curves along the ﬁne-tuning process on two\nmorpho-syntactic tasks (PoS tagging and depen-\ndency parsing) and a non-syntactic task (paraphrase\nidentiﬁcation). Each task is ﬁne-tuned for 3 epochs,\nwith the default learning rate of 5e−5. To mitigate\nthe variance in performance induced by weight ini-\ntialization and training data order (Dodge et al.,\n2020; Reimers and Gurevych, 2017), we repeat\nthis process 5 times per task with different random\nseeds and average results.2 For PoS tagging, we\nﬁne-tune RoBERTa with a linear layer on top of\nthe hidden-states output for token classiﬁcation.3\nDataset: Universal Dependencies Corpus for En-\nglish (UD 2.5 English EWT (Silveira et al., 2014)).\nFor Dependency parsing, we ﬁne-tune a Deep Bi-\nafﬁne neural dependency parser (Dozat and Man-\nning, 2016). Dataset: UD 2.5 English EWT (Sil-\nveira et al., 2014). For Paraphrase identiﬁcation,\nwe ﬁne-tune RoBERTa with a linear layer on top\nof the pooled sentence representation. 4 Dataset:\nMicrosoft Research Paraphrase Corpus (MRPC)\n2The implementation relies in the Transformers library\n(Wolf et al., 2020) and AllenNLP (Gardner et al., 2018). For\nimplementation details, pretrained weights and hyperparame-\nter values, cf. the documentation of the libraries.\n3Source: https://github.com/Tarpelite/\nUniNLP/blob/master/examples/run_pos.py\n4Source: https://github.com/huggingface/\ntransformers/blob/master/examples/\ntext-classification/run_glue.py.\n1575\n(Dolan and Brockett, 2005).\n4 Results\nIn this section, we explore the impact of the size\nof pretraining data on the syntactic information\nencoded by RoBERTa from three different angles.\n4.1 Structural probing\nWe use Hewitt and Manning’s syntactic structural\nprobes to determine whether the MiniBERTa mod-\nels pretrained on more data encode a higher amount\nof syntactic information than those trained on less\ndata. Following the original work, we probe layer\n7 of all models, as it was shown to encode most of\nthe syntax. Results are shown in Table 2.\nTree distance evaluation. The models trained\nwith more data encode better syntactic information\n(as measured by the probe metrics). While DSpr.\nshows a less pronounced variability between family\nmembers, and smaller differences across families,\nUUAS shows a higher intra-family variability and\nbigger differences between families. Noticeably,\nfor the roberta-base-1B family, there is a 7 points\ndifference in UUAS between model 1 and model\n3, which have a difference of only 0.09 points in\nperplexity, highlighting the importance of training\nhyperparameters for the performance of the mod-\nels.\nTree depth evaluation. As for the distance met-\nrics, the models trained on more data show a bet-\nter encoding of syntactic information. Again, the\ncorrelation shows less variability between family\nmembers and smaller differences between families,\nwhile Root %shows a higher intra-family variabil-\nity (especially noticeable for roberta-base-10M).\n4.2 Syntactic generalization evaluation\nWe assess the syntactic generalization performance\nof the different MiniBERTas models using Hu et al.\n(2020)’s test suites (cf. Subsection 3.3) to answer\nthe following questions: Do models pretrained on\nmore data generalize better? Do models with lower\nperplexity perform better in the syntactic tests? Do\nmodels with more pretraining or better perplexity\nperform better in all circuits?\nAverage SG Score. Figure 1 shows the per-\nformance of each model averaged across all 6\ncircuits. We observe a variability between fam-\nily members, especially for roberta-base-100M,\nwith a difference of 15 points between models 1\nand 2. As intuitively expected, the smallest fam-\nModel\n1b-1\n1b-2\n1b-3\n100m-1\n100m-2\n100m-3\n10m-1\n10m-2\n10m-3\n1m-1\n1m-2\n1m-3\nTree distance eval.\nUUAS Dspr.\n70.75 78.82\n72.93 79.86\n77.23 82.66\n68.46 76.95\n70.02 78.11\n69.35 78.73\n61.48 73.19\n62.01 73.78\n60.12 72.58\n56.96 71.70\n55.78 71.33\n55.84 71.33\nTree depth eval.\nRoot % Nspr.\n83.92 85.38\n83.53 85.92\n85.13 86.87\n81.21 84.06\n81.25 84.53\n79.88 84.59\n70.88 81.65\n70.07 81.89\n67.14 80.62\n57.12 74.16\n56.56 74.74\n57.41 74.46\nTable 2: Structural probing with Hewitt and Man-\nning’s syntactic structural probes. ‘1b-*’ corresponds\nto the family roberta-base-1B, ‘100M-*’ to roberta-\nbase-100M, ‘10M-* to roberta-10M, and ‘1M-*’ to\nroberta-med-small-1M.\nFigure 1: Syntactic generalization evaluation. Average\nSyntaxGym score.\nily of models, roberta-med-small-1M, performs\nclearly worse than the other families. However,\nit is interesting to observe that more training\ndata does not always imply better syntactic gen-\neralization: model roberta-base-100M-1 performs\nworse than the whole roberta-base-10M family,\nand model roberta-base-100M-2 performs better\nthan the whole roberta-base-1B family.\nStability with respect to modiﬁers. Five of\nthe test suites (Center Embedding, Cleft structure,\nMVRR, NPZ-Verb, NPZ-Object) include tests with\nand without modiﬁers, i.e,. intervening content in-\nserted before the critical region. These additional\nclauses or phrases increase the linear distance be-\ntween two co-varying items, making the task more\ndifﬁcult, and sometimes they also include a distrac-\ntor word in the middle of a syntactic dependency,\n1576\nFigure 2: Syntactic generalization evaluation. Syn-\ntaxGym score on Center Embedding, Cleft structure,\nMVRR, NPZ-Verb, and NPZ-Object, without (dark\nbars) and with (light bars) modiﬁers.\nwhich can lead the models to misinterpret the de-\npendency. Figure 2 shows the models’ average\nscores on these test suites, without modiﬁers (dark\nbars) and with modiﬁers (light bars), evaluating\nhow robust each model is with respect to the in-\ntervening content. We observe that all models are\naffected by the presence of modiﬁers, but the differ-\nence is narrower for roberta-base-1b, which offers\nthe best stability.\nPerplexity vs. SG Score. Figure 3 shows the\nrelation between the average score across all cir-\ncuits (SG score) and the perplexity of the models.\nAs previously observed in (Hu et al., 2020), even\nthough there is a (not perfect) negative correlation\nbetween the two metrics when comparing different\nfamilies, when comparing points corresponding to\nthe same family of models (with equal architecture\nand training data size, points of the same color in\nFigure 3), there is no clear relation between them.\nThis suggests that both metrics capture different\naspects of the knowledge of the models.\nSyntactic generalization of the models.\nFigure 4 offers an overview of the syntactic\ncapabilities of all the models on the different\nsyntactic circuits. The family with more pre-\ntraining data, roberta-base-1B, outperforms all\nother families in 3 out of 6 circuits, but offers a\nsurprisingly low performance in Gross Syntactic\nState, clearly outperformed by roberta-base-100M\nand roberta-base-10M, and matched by the\nroberta-med-small-1M. Again, the smallest family\noffers the lowest performance across all circuits,\nwith individual models outperforming isolated\nFigure 3: Relationship between average SyntaxGym\nscore and model perplexity.\nFigure 4: SyntaxGym evaluation across circuits.\nmodels of other families in Center Embedding,\nGross Syntactic State and Long Distance De-\npendencies. There is a high variability between\nthe scores achieved by the models of the same\nfamily in the same circuit, with the exception of\nroberta-base-1B in Licensing, where all models\noffer a similar performance. Interestingly, there\nis not a single model for any family that performs\nbest (nor worst) across all tests.\n4.3 Targeted downstream tasks evaluation\nWe compare the performance of the different mod-\nels on three different downstream tasks: PoS tag-\nging (Figure 5), dependency parsing (Figure 6) and\nparaphrase identiﬁcation (Figures 7) to determine\nif models pretrained on more data perform better\non downstream applications. We observe the same\ntendency for all tasks: models with more train-\ning data perform better, and the model with the\n1577\nFigure 5: Targeted downstream task evaluation. PoS\ntagging accuracy evolution.\nsmaller architecture (roberta-med-small-1M) per-\nforms remarkably worse. Although note that while\nthe increase of training data between families is ex-\nponential (1M, 10M, 100M, 1B), the performance\ngrows at a slower rate. This observation suggests\nthat there may be a limit to the amount of data\nthat we can feed into a RoBERTa model and the\nknowledge that the model can acquire.\n5 Cost-beneﬁt analysis\nFor the sake of a more holistic view on the quality\nof the models, we perform a cost–beneﬁt analysis\nof the performance gains in the different tasks, with\nan estimate of the ﬁnancial and environmental cost\nof developing the models. As the resources used to\ntrain the MiniBERTas are not publicly available, we\nrely on the data provided in (Strubell et al., 2019)\nto estimate the cost of developing each individual\nmodel based on the costs of RoBERTa, trained on\n30B words, in proportion to the amount of words\nused to train each family of models.\nFinancial cost. As RoBERTa base was trained\non 1024 Nvidia V100 GPUs for 24 hours (i.e.,\n24,576 GPU hours), and the price per hour of\nNvidia V100 (on-demand) is $2.48 (Strubell et al.,\n2019), the cost of training RoBERTa base amounts\nto $60,948, and the cost of training a MiniBERTas\nmodel can be estimated to be $60,948 / 30B words\n* #TrainingWords. E.g., for the roberta-base-1b\nmodel: $60,948 / 30B words * 1B words = $2,032.\nCO2 Emissions. Using Strubell et al. (2019),\nwe extrapolate that Nvidia V100 GPUs emit\n0.28441456 lbs of CO2 per GPU per hour, which\nmeans that the training of RoBERTa base emitted\nFigure 6: Targeted downstream tasks evaluation. De-\npendency parsing UAS and LAS evolution.\nFigure 7: Targeted downstream tasks evaluation. Para-\nphrase identiﬁcation accuracy and F1 evolution.\n1578\nModel family Cost CO 2e (lbs) PoS Dep. parsing Paraphrase id.\nroberta-base-1B $20320 2330 96.03 (+0.5%) 85.73 (+1.76%) 89.59 (+2.02%)\nroberta-base-100M $5075 582.5 95.53 (+1.11%) 83.97 (+4.04%) 87.57 (+2.79%)\nroberta-base-10M $500 58.25 94.42 (+2.73%) 79.93 (+14.48%) 84.78 (+5.34%)\nrob-med-small-1M $50 5.825 91.69 (base) 65.45 (base) 79.44 (base)\nTable 3: Comparison of the estimated cost of developing the different MiniBERTas families in terms of cloud\ncompute cost (USD) and CO2 emissions (lbs) and their averaged performances on PoS tagging (acc), Dep. Parsing\n(LAS), and Paraphrase identiﬁcation (F1). In parentheses, we show the increment with respect to the previous\nsmaller model.\n6,990 lbs of CO 2. We estimate the emissions of\nthe training of each MiniBERTas model as 6,990\nlbs / 30B * #TrainingWords.\nTo develop each MiniBERTas models, Warstadt\net al. run the pretraining 10 times for the bigger\nfamily (roberta-base-1B), and 25 times for the other\nthree families (roberta-base-100M, roberta-base-\n10M and roberta-med-small-1M) with varying hy-\nperparameters. Therefore, to compute the cost of\ndeveloping each family of models, we multiply\nthe cost of training a single model by the number\nof pretraining runs needed to obtain it. Table 3\nlists the estimated costs and CO2 emissions of the\ndevelopment of each MiniBERTas family, along\nwith their averaged performance on the three stud-\nied downstream applications. We see that small\nperformance gains come at high ﬁnancial and envi-\nronmental costs. E.g., for roberta-base-1B, a per-\nformance increase of 0.5%–2.02% on downstream\napplications has a cost of $20K in computing re-\nsources and signiﬁcant carbon emissions, higher\nthan the estimated 1984 lbs generated by a single\npassenger ﬂying between New York and San Fran-\ncisco (Strubell et al., 2019).\n6 Discusion and conclusions\nOur experiments shed light on the impact of pre-\ntraining data size on the syntactic capabilities of\nRoBERTa. Our results indicate that models pre-\ntrained with more data encode better syntactic in-\nformation (as measured by Hewitt and Manning’s\nstructural probes) and offer a higher syntactic gen-\neralization over the different syntactic phenomena\ncovered by the tests assembled in (Hu et al., 2020).\nMoreover, models pretrained with more data seem\nto be more robust to the presence of modiﬁers\nin the syntactic tests, i.e,. intervening content in-\nserted before the critical region. As was already\nobserved in (Hu et al., 2020), there is no simple\nrelationship between the perplexity of the models\nand the SyntaxGym score: the variance in intra-\nfamily SG score is not explained by the perplexity\ndifferences. When zooming in on the different test\ncircuits, probing different linguistic phenomena,\nwe observe that there is a high variability between\nthe scores achieved by the models of the same fam-\nily, with no single model for any family performing\nbest across all tests. While the family pretrained\nwith more data outperforms all the models of the\nother families on 3 out of 6 circuits, it offers a sur-\nprisingly low performance in Gross Syntactic State,\nclearly outperformed by the smaller models.\nWe also compare the performance of the differ-\nent models ﬁne-tuned on PoS tagging, dependency\nparsing and paraphrase identiﬁcation, observing\nthat models with more training data offer a bet-\nter performance, and the model with the smaller\narchitecture (roberta-med-small-1M) performs re-\nmarkably worse. However, while the amount of\ntraining data between families grows exponentially,\nwe observe that the performance grows at a much\nslower rate, suggesting that there may be a limit to\nthe knowledge that a RoBERTa model can acquire\nsolely from raw pretraining data.\nWe complement our ﬁndings with a ﬁnancial and\nenvironmental cost–beneﬁt analysis of pretraining\nmodels on different amounts of data. We show\nthat while models pretrained on more data encode\nmore syntactic information and perform generally\nbetter on downstream applications, small perfor-\nmance gains come at a huge ﬁnancial and environ-\nmental cost. Thus, when developing and training\nnew models we should weigh between the beneﬁt\nof making models bigger and pretraining them on\nhuge datasets and the costs this implies, prioritizing\ncomputationally efﬁcient hardware and algorithms.\nA question that still needs to be addressed by\nfuture work is whether it is possible to complement\ninformation-theoretical metrics such as perplexity\n1579\nwith metrics measuring speciﬁc types of knowl-\nedge, e.g., syntax, in order to develop and select\nmore robust and efﬁcient models to solve Natural\nLanguage Understanding tasks.\nAcknowledgments\nThis work has been partially funded by the Eu-\nropean Commission via its H2020 Research Pro-\ngram under the contract numbers 786731, 825079,\n870930, and 952133. This work has been partially\nsupported by the ICT PhD program of Universitat\nPompeu Fabra through a travel grant.\nReferences\nYossi Adi, Einat Kermany, Yonatan Belinkov, Ofer\nLavi, and Yoav Goldberg. 2016. Fine-grained anal-\nysis of sentence embeddings using auxiliary predic-\ntion tasks. arXiv preprint arXiv:1608.04207.\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi, Has-\nsan Sajjad, and James Glass. 2017. What do neu-\nral machine translation models learn about morphol-\nogy? In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 861–872, Vancouver,\nCanada. Association for Computational Linguistics.\nKasturi Bhattacharjee, Miguel Ballesteros, Rishita\nAnubhai, Smaranda Muresan, Jie Ma, Faisal Lad-\nhak, and Yaser Al-Onaizan. 2020. To BERT or not\nto BERT: Comparing task-speciﬁc and task-agnostic\nsemi-supervised approaches for sequence tagging.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7927–7934, Online. Association for Computa-\ntional Linguistics.\nEthan A. Chi, John Hewitt, and Christopher D. Man-\nning. 2020. Finding universal grammatical rela-\ntions in multilingual BERT. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5564–5577, Online. As-\nsociation for Computational Linguistics.\nAlexis Conneau, German Kruszewski, Guillaume Lam-\nple, Loïc Barrault, and Marco Baroni. 2018. What\nyou can cram into a single $&!#* vector: Probing\nsentence embeddings for linguistic properties. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 2126–2136, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\nFarhadi, Hannaneh Hajishirzi, and Noah Smith.\n2020. Fine-tuning pretrained language models:\nWeight initializations, data orders, and early stop-\nping. arXiv preprint arXiv:2002.06305.\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\nTimothy Dozat and Christopher D. Manning. 2016.\nDeep biafﬁne attention for neural dependency pars-\ning.\nYanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav\nGoldberg. 2020. When bert forgets how to pos: Am-\nnesic probing of linguistic properties and mlm pre-\ndictions.\nRichard Futrell, Ethan Wilcox, Takashi Morita, and\nRoger Levy. 2018. Rnns as psycholinguistic sub-\njects: Syntactic state and grammatical dependency.\nRichard Futrell, Ethan Wilcox, Takashi Morita, Peng\nQian, Miguel Ballesteros, and Roger Levy. 2019.\nNeural language models as psycholinguistic sub-\njects: Representations of syntactic state. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 32–42, Minneapolis,\nMinnesota. Association for Computational Linguis-\ntics.\nMatt Gardner, Joel Grus, Mark Neumann, Oyvind\nTafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Pe-\nters, Michael Schmitz, and Luke Zettlemoyer. 2018.\nAllenNLP: A deep semantic natural language pro-\ncessing platform. In Proceedings of Workshop for\nNLP Open Source Software (NLP-OSS), pages 1–\n6, Melbourne, Australia. Association for Computa-\ntional Linguistics.\nJon Gauthier, Jennifer Hu, Ethan Wilcox, Peng Qian,\nand Roger Levy. 2020. SyntaxGym: An online\nplatform for targeted evaluation of language models.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics: System\nDemonstrations, pages 70–76, Online. Association\nfor Computational Linguistics.\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. 2018. Colorless\ngreen recurrent networks dream hierarchically. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 1195–1205, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\n1580\nJohn Hewitt and Percy Liang. 2019. Designing and\ninterpreting probes with control tasks. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 2733–2743, Hong\nKong, China. Association for Computational Lin-\nguistics.\nJohn Hewitt and Christopher D. Manning. 2019a. A\nstructural probe for ﬁnding syntax in word repre-\nsentations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4129–4138, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJohn Hewitt and Christopher D Manning. 2019b. A\nstructural probe for ﬁnding syntax in word represen-\ntations. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4129–4138.\nJennifer Hu, Jon Gauthier, Peng Qian, Ethan Wilcox,\nand Roger Levy. 2020. A systematic assessment\nof syntactic generalization in neural language mod-\nels. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics,\npages 1725–1744, Online. Association for Compu-\ntational Linguistics.\nJey Han Lau, Alexander Clark, and Shalom Lappin.\n2017. Grammaticality, acceptability, and probabil-\nity: A probabilistic view of linguistic knowledge.\nCognitive Science, 41(5):1202–1241.\nYongjie Lin, Yi Chern Tan, and Robert Frank. 2019.\nOpen sesame: Getting inside BERT’s linguistic\nknowledge. In Proceedings of the 2019 ACL Work-\nshop BlackboxNLP: Analyzing and Interpreting Neu-\nral Networks for NLP, pages 241–253, Florence,\nItaly. Association for Computational Linguistics.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of LSTMs to learn\nsyntax-sensitive dependencies. Transactions of the\nAssociation for Computational Linguistics, 4:521–\n535.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019a. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 1073–1094, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\ntactic evaluation of language models. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 1192–1202,\nBrussels, Belgium. Association for Computational\nLinguistics.\nVincent Micheli, Martin d’Hoffschmidt, and François\nFleuret. 2020. On the importance of pre-training\ndata volume for compact language models. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7853–7858, Online. Association for Computa-\ntional Linguistics.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nTiago Pimentel, Josef Valvoda, Rowan Hall Maudslay,\nRan Zmigrod, Adina Williams, and Ryan Cotterell.\n2020. Information-theoretic probing for linguistic\nstructure. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4609–4622, Online. Association for Computa-\ntional Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the lim-\nits of transfer learning with a uniﬁed text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable ques-\ntions for SQuAD. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 784–\n789, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nNils Reimers and Iryna Gurevych. 2017. Reporting\nscore distributions makes a difference: Performance\nstudy of LSTM-networks for sequence tagging. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing, pages\n338–348, Copenhagen, Denmark. Association for\nComputational Linguistics.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in BERTology: What we know\nabout how BERT works. Transactions of the Associ-\nation for Computational Linguistics, 8:842–866.\n1581\nSubendhu Rongali, Luca Soldaini, Emilio Monti, and\nWael Hamza. 2020. Don’t parse, generate! a se-\nquence to sequence architecture for task-oriented se-\nmantic parsing. Proceedings of The Web Conference\n2020.\nRoy Schwartz, Jesse Dodge, Noah A Smith, and\nOren Etzioni. 2019. Green ai. arXiv preprint\narXiv:1907.10597.\nNatalia Silveira, Timothy Dozat, Marie-Catherine\nde Marneffe, Samuel Bowman, Miriam Connor,\nJohn Bauer, and Chris Manning. 2014. A gold stan-\ndard dependency corpus for English. InProceedings\nof the Ninth International Conference on Language\nResources and Evaluation (LREC’14), pages 2897–\n2904, Reykjavik, Iceland. European Language Re-\nsources Association (ELRA).\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 3645–3650, Florence, Italy.\nAssociation for Computational Linguistics.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Sam Bowman, Dipanjan Das,\nand Ellie Pavlick. 2019. What do you learn from\ncontext? probing for sentence structure in contextu-\nalized word representations. In International Con-\nference on Learning Representations.\nMarten van Schijndel, Aaron Mueller, and Tal Linzen.\n2019. Quantity doesn’t buy quality syntax with\nneural language models. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5831–5837, Hong Kong,\nChina. Association for Computational Linguistics.\nElena V oita and Ivan Titov. 2020. Information-\ntheoretic probing with minimum description length.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 183–196, Online. Association for Computa-\ntional Linguistics.\nAlex Wang and Kyunghyun Cho. 2019. BERT has\na mouth, and it must speak: BERT as a Markov\nrandom ﬁeld language model. In Proceedings of\nthe Workshop on Methods for Optimizing and Eval-\nuating Neural Language Generation, pages 30–36,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R.\nBowman. 2020a. BLiMP: The benchmark of lin-\nguistic minimal pairs for English. Transactions\nof the Association for Computational Linguistics,\n8:377–392.\nAlex Warstadt, Yian Zhang, Xiaocheng Li, Haokun\nLiu, and Samuel R. Bowman. 2020b. Learning\nwhich features matter: RoBERTa acquires a prefer-\nence for linguistic generalizations (eventually). In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 217–235, Online. Association for Computa-\ntional Linguistics.\nEthan Wilcox, Roger Levy, and Richard Futrell. 2019a.\nHierarchical representation in neural language mod-\nels: Suppression and recovery of expectations. In\nProceedings of the 2019 ACL Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 181–190, Florence, Italy. As-\nsociation for Computational Linguistics.\nEthan Wilcox, Roger Levy, Takashi Morita, and\nRichard Futrell. 2018. What do RNN language\nmodels learn about ﬁller–gap dependencies? In\nProceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 211–221, Brussels, Belgium.\nAssociation for Computational Linguistics.\nEthan Wilcox, Peng Qian, Richard Futrell, Miguel\nBallesteros, and Roger Levy. 2019b. Structural su-\npervision improves learning of non-local grammati-\ncal dependencies. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 3302–3312, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nEthan Wilcox, Peng Qian, Richard Futrell, Miguel\nBallesteros, and Roger Levy. 2019c. Structural su-\npervision improves learning of non-local grammati-\ncal dependencies.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nYian Zhang, Alex Warstadt, Haau-Sing Li, and\nSamuel R Bowman. 2020. When do you need bil-\nlions of words of pretraining data? arXiv preprint\narXiv:2011.04946.\n1582\nQile Zhu, Haidar Khan, Saleh Soltan, Stephen Rawls,\nand Wael Hamza. 2020. Don’t parse, insert: Multi-\nlingual semantic parsing with insertion based decod-\ning. In Proceedings of the 24th Conference on Com-\nputational Natural Language Learning, pages 496–\n506, Online. Association for Computational Linguis-\ntics.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8597537279129028
    },
    {
      "name": "Parsing",
      "score": 0.6746838092803955
    },
    {
      "name": "Syntax",
      "score": 0.6693300008773804
    },
    {
      "name": "Natural language processing",
      "score": 0.6394007802009583
    },
    {
      "name": "ENCODE",
      "score": 0.6215370893478394
    },
    {
      "name": "Language model",
      "score": 0.6009215712547302
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6009188294410706
    },
    {
      "name": "Transformer",
      "score": 0.5947891473770142
    },
    {
      "name": "Paraphrase",
      "score": 0.516925036907196
    },
    {
      "name": "Dependency grammar",
      "score": 0.4323676526546478
    },
    {
      "name": "Dependency (UML)",
      "score": 0.4266752302646637
    },
    {
      "name": "Generalization",
      "score": 0.4115658104419708
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I170486558",
      "name": "Universitat Pompeu Fabra",
      "country": "ES"
    },
    {
      "id": "https://openalex.org/I1311688040",
      "name": "Amazon (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I11932220",
      "name": "Institució Catalana de Recerca i Estudis Avançats",
      "country": "ES"
    }
  ],
  "cited_by": 31
}