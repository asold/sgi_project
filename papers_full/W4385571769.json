{
  "title": "Efficient Transformers with Dynamic Token Pooling",
  "url": "https://openalex.org/W4385571769",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1298852000",
      "name": "Piotr Nawrot",
      "affiliations": [
        "University of Edinburgh"
      ]
    },
    {
      "id": "https://openalex.org/A2335651288",
      "name": "Jan Chorowski",
      "affiliations": [
        "Nvidia (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2012507924",
      "name": "Adrian Lancucki",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2655028980",
      "name": "Edoardo Maria Ponti",
      "affiliations": [
        "University of Edinburgh"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2548228487",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W3198782837",
    "https://openalex.org/W4225909425",
    "https://openalex.org/W4297808394",
    "https://openalex.org/W4205537173",
    "https://openalex.org/W2547875792",
    "https://openalex.org/W3162090017",
    "https://openalex.org/W2964182247",
    "https://openalex.org/W3101140821",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W2099257174",
    "https://openalex.org/W4323654151",
    "https://openalex.org/W2121879602",
    "https://openalex.org/W2963735467",
    "https://openalex.org/W2288336636",
    "https://openalex.org/W4281771945",
    "https://openalex.org/W2042541403",
    "https://openalex.org/W4385573804",
    "https://openalex.org/W3181186005",
    "https://openalex.org/W2618854269",
    "https://openalex.org/W4309793872",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3174418826",
    "https://openalex.org/W2325237720",
    "https://openalex.org/W2767693128",
    "https://openalex.org/W3135427360",
    "https://openalex.org/W3012990076",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W1646152356",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3112776819",
    "https://openalex.org/W2321470647",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W3137010024",
    "https://openalex.org/W2989539713",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4224035735",
    "https://openalex.org/W3033188311",
    "https://openalex.org/W3131922516",
    "https://openalex.org/W4293714597",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3209873929",
    "https://openalex.org/W2914048279",
    "https://openalex.org/W2546325545",
    "https://openalex.org/W3096656254",
    "https://openalex.org/W2964110616"
  ],
  "abstract": "Transformers achieve unrivalled performance in modelling language, but remain inefficient in terms of memory and time complexity. A possible remedy is to reduce the sequence length in the intermediate layers by pooling fixed-length segments of tokens. Nevertheless, natural units of meaning, such as words or phrases, display varying sizes. To address this mismatch, we equip language models with a dynamic-pooling mechanism, which predicts segment boundaries in an autoregressive fashion. We compare several methods to infer boundaries, including end-to-end learning through stochastic re-parameterisation, supervised learning (based on segmentations from subword tokenizers or spikes in conditional entropy), as well as linguistically motivated boundaries. We perform character-level evaluation on texts from multiple datasets and morphologically diverse languages. The results demonstrate that dynamic pooling, which jointly segments and models language, is both faster and more accurate than vanilla Transformers and fixed-length pooling within the same computational budget.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 6403–6417\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nEfficient Transformers with Dynamic Token Pooling\nPiotr Nawrot† Jan Chorowski‡ Adrian Ła´ncucki⋄♣ Edoardo M. Ponti†\n†University of Edinburgh ‡Pathway ⋄NVIDIA ♣University of Wrocław\npiotr.nawrot@ed.ac.uk\nAbstract\nTransformers achieve unrivalled performance\nin modelling language, but remain inefficient in\nterms of memory and time complexity. A possi-\nble remedy is to reduce the sequence length in\nthe intermediate layers by pooling fixed-length\nsegments of tokens. Nevertheless, natural units\nof meaning, such as words or phrases, display\nvarying sizes. To address this mismatch, we\nequip language models with a dynamic-pooling\nmechanism, which predicts segment bound-\naries in an autoregressive fashion. We com-\npare several methods to infer boundaries, in-\ncluding end-to-end learning through stochastic\nre-parameterisation, supervised learning (based\non segmentations from subword tokenizers or\nspikes in conditional entropy), as well as lin-\nguistically motivated boundaries. We perform\ncharacter-level evaluation on texts from multi-\nple datasets and morphologically diverse lan-\nguages. The results demonstrate that dynamic\npooling, which jointly segments and models\nlanguage, is both faster and more accurate than\nvanilla Transformers and fixed-length pooling\nwithin the same computational budget.\n1 Introduction\nThe Transformer architecture (Vaswani et al., 2017)\nlies at the heart of cutting-edge generative mod-\nels, such as GPT-3 (Brown et al., 2020) for text\nand DALL·E 2 (Ramesh et al., 2022) for images.\nIts success can be largely attributed to the ability\nto leverage a considerable amount of data, which\nyields performance gains (Kaplan et al., 2020) and\nemergent abilities (Wei et al., 2022) in accordance\nwith well-established scaling laws. Nonetheless,\nthe time and memory efficiency of Transformers\nremains constrained by their algorithmic complex-\nity of O(l2n), where lstands for sequence length\nand nfor the number of layers.\nTo remedy this shortcoming without renouncing\nthe expressivity of a deep model, the quadratic self-\nattention can be sparsified (Child et al., 2019; Roy\net al., 2021; Ren et al., 2021) or linearly approxi-\nmated (Beltagy et al., 2020). Hourglass Transform-\ners (Nawrot et al., 2022) provide an alternative so-\nlution, where the sequence length is reduced in the\nintermediate layers by merging fixed-size groups of\ntokens, similar to (Dai et al., 2020). These pooled\nrepresentations are up-sampled back to the orig-\ninal length in order to generate sequences in an\nauto-regressive fashion (Ronneberger et al., 2015).\nNevertheless, pooling groups of fixed size is sub-\noptimal in several respects. First, these groups\nare misaligned with linguistic primitives: units of\nmeaning such as morphemes, words, phrases, and\nclauses vary in size. Second, the elements of a se-\nquence may carry different degrees of information\n(for instance, silence and voice in speech). Ideally,\nthe model should perform hierarchical computa-\ntion, relying on the same abstractions as human\nprocessing of language, and conditional, by allocat-\ning resources to sub-sequences in proportion to the\nmodel uncertainty. In this work, we demonstrate\nthat dynamic pooling results not only in higher\nshortening rates of input sequences, and thus in-\ncreased efficiency, but also superior performance\nin next token prediction due to adopting the correct\ninductive bias in grouping tokens.\nTo this end, we propose a new Transformer vari-\nant that jointly learns token sequences and dynami-\ncally pools them into latent groupings of variable\nsize (Figure 1). Crucially, the segmentation must\npreserve the auto-regressive property, and typical\nsubword tokenizers cannot be applied to incom-\nplete sequences during generation. Rather, we learn\na neural boundary predictor during training: 1) su-\npervised by tokenizers such as Unigram (Kudo,\n2018); 2) supervised by spikes in the conditional\nentropy of the predictive distribution, which en-\nsure that the computation is adaptive to the level of\nuncertainty of the sequence model; 3) end-to-end\nthrough stochastic re-parameterisation (Maddison\net al., 2017; Jang et al., 2017); 4) use natural data\n6403\nT\nr\na\nnsformer layers\nT\nr\na\nnsformer layers\nPooling\nh\nUp-sampling\nBoundary predictor\nResidual connection\nT\nr\na\nnsformer\nlayers\ns\ns'\nu\nx<t\nx<t+1\nNull Null\nNull\nNull\nFigure 1: The architecture of a dynamic-pooling Transformer, which jointly performs language modelling and token\nsegmentation. The boundary predictor predicts segment boundaries and pools together groups of variable length by\naveraging. The shortened sequence is processed efficiently by a series of intermediate layers, then up-sampled back\nto the original length via duplication. The model generates the next token xxxt in the same resolution as the input.\nboundaries such as whitespaces, which separate\nwords in many scripts, without a predictor.\nTo validate our model, we experiment with\ncharacter-level language modelling of text in sev-\neral English benchmarks, including text8 (Ma-\nhoney, 2006), CC-100 (Wenzek et al., 2020), and\nwiki40b (Guo et al., 2020), as well as in a series\nof languages representing different morphological\ntypes: Finnish, Hebrew, and Vietnamese. We find\nthat dynamic pooling not only achieves lower time\nand memory complexity, but even surpasses the\nperformance of vanilla Transformers and fixed-size\npooling Transformers in most benchmarks by sta-\ntistically significant margins.\nOverall, our results indicate a promising direc-\ntion to further accelerate training and therefore fa-\ncilitate scaling. A FAQ section about our methods,\nfindings, and the experimental setup is available\nin Appendix A. We release the code at https://\ngithub.com/PiotrNawrot/dynamic-pooling.\n2 Background\n2.1 Language Modelling with Transformers\nLet xxx = (x1,...,x l) denote the input sequence.\nA language model assigns a probability value to\nany possible sequence of tokens from a vocabulary\nV. The parameters of a model θ are optimised to\nmaximise the aggregate probability of all xxx∈V∗\nin the training set D:\nargmax\nθ\n∑\nxxx∈D\nl∑\nt=1\nlog p(xt |xxx<t,θ), (1)\nwhere t indexes time steps. In our experiments, θ\ncorresponds to the parameters of an autoregressive\nTransformer model (Vaswani et al., 2017).\nA key advantage of Transformers is their ability\nto scale, which ultimately reaps the largest ben-\nefits according to (Sutton, 2019)’s ‘bitter lesson’\nand reveals surprising emergent capabilities of lan-\nguage models (Kaplan et al., 2020; Wei et al., 2022).\nNevertheless, the algorithmic complexity of self-\nattention, O(l2) where l is the length of the se-\nquence, creates a bottleneck. To alleviate this cost,\nprevious work (Clark et al., 2022; Tay et al., 2022;\nNawrot et al., 2022) proposed to reduce the se-\nquence length after the initial layers by pooling\ntogether groups of tokens. A single shortening by\na factor kreduces the complexity to O( l2\nk2 ). This\nallows for increasing either the model efficiency or\nits depth within the same compute budget.\n2.2 Hourglass Transformer\nNaïve length reduction through pooling would re-\nduce the length of output, however language mod-\nels operate with the same input and output reso-\nlutions. For this reason, (Nawrot et al., 2022) in-\ntroduced the Hourglass Transformer composed of\nthree blocks of Transformer layers, which down-\nsample, process, and upsample the tokens back to\nthe original granularity. The first block encodes\neach input token xt into hhht. Afterwards, groups of\nadjacent tokens of fixed length kare mean-pooled\n6404\nto form ⌈l\nk⌉representations sss:\nsssm = 1\nk\nmk∑\ni=mk−k+1\nhhhi (2)\nNext, each pooled representation sssm is processed\nby the middle block of Transformer layers, which\noperates with complexityO( l2\nk2 ), yielding sss′\nm. This\nsequence is up-sampled to its original resolution\nby duplication: uuut = sss′\n⌈t−k+1\nk ⌉, and added to the\nhidden representations hhhfrom before shortening\nthrough a skip connection, and passed to the third\nblock.\nNote that we subtract k−1 from the index. This\nis because pooling and up-sampling in an autore-\ngressive model pose a risk of data leakage from the\nfuture to the past. In fact, up-sampled representa-\ntions might encompass future tokens if no measures\nare taken to prevent this. As a remedy, Hourglass\nTransformer shifts the up-sampled sequence to the\nright by k−1 positions, and pads it with a learnable\nnull-group representation uuu0 at the beginning. This\nis sufficient to satisfy the autoregressive property\nin the fixed pooling scenario.1\nHourglass Transformer was shown to improve\ntime and space complexity in a number of language\nand image modelling tasks, for a given parameter\ncount. However, this came at the expense of degrad-\ning the perplexity of the language model, especially\nwith shortening factors k >2. We conjecture that\nthis undesirable side effect is due to two main rea-\nsons. Firstly, the distribution of lengths of natural\nunits of meaning such as morphemes and phrases\nin natural languages is uneven: for instance, word\nlength is correlated with its frequency (Zipf, 1949;\nBentz and Ferrer-i Cancho, 2016). Secondly, infor-\nmation content tends to be distributed uniformly\nacross units of meaning (Meister et al., 2021).\nAs a consequence, fixed pooling creates seg-\nments with incongruous boundaries and unequal\ninformation content. For instance, in speech, this\nresults in giving silence and voice the same im-\nportance. Instead, an ideal model should allocate\ncompute conditionally on the information content\nof a given token. This would also ultimately lead\nto interpreting language hierarchically based on\nthe same abstractions that humans adopt for lan-\nguage processing. Hence, we present a method to\nenable variable-length pooling and up-sampling in\nautoregressive language models.\n1We refer to (Nawrot et al., 2022) for more details.\n3 Dynamic-Pooling Transformer\n3.1 Boundary Prediction\nIn order to augment the Hourglass architecture with\nvariable-size pooling, we seek to find a sequence\nof segment boundaries bbb∈{0,1}l for every input\nxxx. Let bt = 1denote a segment boundary between\nelements xt and xt+1. The boundary predictor is\nimplemented as a Multi-Layer Perceptron with pa-\nrameters ϕ. As shown in Figure 1, this module\nmaps each representation hhht encoded by the first\nstack of Transformer layers into a Bernoulli proba-\nbility distribution:\nˆbt = p(bt=1) = sigmoid (MLPϕ (hhht)) . (3)\nSince segment boundaries are discrete, sampling\nfrom this distribution is not differentiable with\nrespect to the model perplexity. Hence, we op-\ntimise this latent variable through stochastic re-\nparametrisation (Jang et al., 2017; Maddison et al.,\n2017) via hard Gumbel-sigmoid (Section 3.1.1),\njointly learning the language model and boundary\npredictor. We favour this solution over a score-\nfunction estimator of the gradient, as it suffers from\nhigh variance and computation costs due to sam-\npling (Schulman et al., 2015).\nAs an alternative, we explore training the bound-\nary predictor module with a binary cross-entropy\nloss with respect to two different sources of super-\nvision: a Unigram tokenizer (Section 3.1.2) and\nspikes in conditional entropy (Section 3.1.3). Fi-\nnally, we consider resorting to linguistically in-\nspired boundaries (Section 3.1.4). During training\nand evaluation, we perform maximum likelihood\ninference for these variables. In other words, each\nˆbt from Equation (3) is rounded to the closest bi-\nnary scalar such that bt = ⌊ˆbt⌉.\n3.1.1 Segmenting with Gumbel-Sigmoid\nIn order to learn the input segmentation end-to-\nend based on the model perplexity, we can re-\nparameterise the Bernoulli distribution of Equa-\ntion (3) by injecting stochasticity in this form:\nˆbt = sigmoid\n\nlog\nˆbtu\n(1 −ˆbt) (1−u)\n1/τ\n\nu∼Uniform(0,1). (4)\nwhere τis the temperature, a hyper-parameter. This\nestimator, however, is biased and might lead to\nsub-optimal results. As a consequence, we also\npropose methods based on supervised learning of\nthe boundary predictor in the following sections.\n6405\nwi th one of his greatest  performances in last  tango \n0\n1\n2\n3\n such as cpu clock speeds or measures of performance\n0\n1\n2\n3\nFigure 2: Entropy of a Transformer character-level lan-\nguage model in two text segments. Red vertical lines\nindicate the boundaries according to spikes in condi-\ntional entropy. Most of them coincide with whitespaces,\ndue to the high uncertainty at word starts, but they also\nfall after morphemes like ‘great’ or ‘measure’. Segmen-\ntation may vary based on the context, e.g., of the word\n‘performance’.\n3.1.2 Segmenting with Subword Tokenizers\nWidespread algorithms for extracting variable-\nlength boundaries for text are subword tokenizers,\nincluding Unigram (Kudo, 2018), Byte Pair\nEncoding (BPE; Sennrich et al., 2016), and Word-\nPiece (Schuster and Nakajima, 2012). However,\nthese create subwords greedily, and might change\nthe segmentation of a given sequence prefix after\nmore tokens are observed. For instance, consider\nthe phrase ‘ civil aviation’. A Unigram model\nmight segment its prefix ‘civil aviatio’ differently\nbefore and after observing the character ‘n’:\n_civil _a vi ati o\n_civil _a vi ation\nDuring training an entire sentence is tokenized, but\nduring inference a prefix is extended one charac-\nter at a time and re-tokenized, possibly changing\nthe boundaries like in the example above. Hence,\ndeploying off-the-shelf tokenizers naïvely during\ninference does not recover the oracle segments and\ncreates a mismatch between training and evaluation\nboundaries.\nAs a remedy, we provide the training tokeniza-\ntion as supervision to our autoregressive boundary\npredictor instead. More specifically, we employ\na Unigram tokenizer (Kudo, 2018), as it aligns\nwith morphological units better than other algo-\nrithms (Bostrom and Durrett, 2020). To prevent\nsubword units from crossing word boundaries, we\nsplit the text on whitespace characters beforehand.\nV ocabulary size is a tunable hyper-parameter which\noffers different efficiency–performance trade-offs.\n3.1.3 Segmenting with Entropy Spikes\nAs an alternative to providing supervision through\nUnigram, we also propose a new segmentation\nmethod based on spikes of conditional entropy,\nwhich is agnostic about the presence of natural\nboundaries (such as whitespaces) or the availability\nof tokenizers. These properties make it suitable for\nother modalities in addition to text, such as speech\nand vision. Moreover, this enables top-down su-\npervision and end-to-end training without external\ndependencies.\nIntuitively, in natural language the information\ncontent tends to be spread evenly throughout a sen-\ntence, to facilitate communication. The conditional\nentropy is the expectation of such information con-\ntent over the tokens in the vocabulary:\nH(xt |xxx<t) =\n∑\nx∈V\np(xt |xxx<t) (−log p(xt |xxx<t))  \ninformation content\n(5)\nTherefore, peaks in this conditional entropy pro-\nvide indications of surprisal, and can serve as natu-\nral boundaries between segments. More formally,\nlet Ht be the conditional entropy at time t. We\nselect local spikes by comparing their value within\na (left) window of size k. We place boundaries\naccording to the following conditions:\nbt =\n{\n1 if Ht >Hi ∀i∈{t−k,...,t −1}\n0 otherwise.\n(6)\nEmpirically, entropy spikes in language models\noverlap with word boundaries to a significant de-\ngree (Hutchens and Alder, 1998). However, they\nare also more flexible as they enable conditional\ncomputation based on the model’s confidence about\nits next token prediction. As an example of segmen-\ntation based on entropy spikes, consider Figure 2.\n3.1.4 Linguistically Inspired Segments\nFinally, perhaps the most straightforward source\nof segmentation is word boundaries. In fact, in\nmany scripts, these are marked by whitespace char-\nacters.2 The simplicity of this method of segmen-\ntation comes with the obvious drawback of not\nproviding control over the rate of shortening, while\nwe found that the optimal rate varies with the lan-\nguage. Hence its efficiency–performance trade-off\nis not tunable.\n2Several scripts such as Chinese characters, however, do\nnot adopt this convention.\n6406\nSegment boundaries are placed in between two\nsymbols. In our experiments, we put a boundary\nafter a whitespace character. Thus, we do not need\nto train a boundary predictor, since predicting a\nwhitespace character is a signal to close the group\nin the next iteration of auto-regressive generation.\nThis would not be possible, had we chosen to put a\nboundary before a whitespace character.\n3.2 Pooling and Up-sampling\nIn the pooling step (Figure 1) a generated sequence\nof boundariesbbbis used to pool the tokens belonging\nto the same segment by averaging. Thus, we form∑l\nt=1 bt+ 1shortened representations sss, which are\nthen passed to the middle block of Transformer\nlayers. Note that for Gumbel-sigmoid, to keep\npooling differentiable, we algebraically manipulate\nbbb ∈Rl into B ∈Rl×1+\n∑\nt bt , i.e. a binary matrix\nthat maps from the original length to the shortened\nlength, following (Bhati et al., 2021). The cell Bij\nis 1 if token iis merged into the j-th group, and\n0 otherwise. Thus, sss = hhhB/∑\niBi⋆, where the\ndenominator unit-normalises the matrix columns.\nTo obtain the up-sampled representationuuutwhile\npreserving the autoregressive property, we calcu-\nlate the largest index mso that the output of the\nmiddle block sss′\nm does include future information:\nuuut = sss′\nm, where m= ∑t\ni=1 bi. As a consequence,\na segment representation sss′\nm can only be added to\nthe last token pooled into group m. For all the\nother non-final tokens, we take the representation\nof a previous segment sss′\nm−1. Similar to Hourglass,\nthe representation for the first (null) group sss0 is a\nlearnable vector. Afterwards, uuut is added to the\nhighway layer representation hhht.\n3.3 Auxiliary Objectives\nIn addition to minimising the language modelling\nloss with respect to the parameters θ as shown in\nEquation (1), we use auxiliary objectives to train\nthe boundary predictor parameters ϕ. For super-\nvised learning with subword tokenizers and entropy\nspikes, we minimise the cross-entropy between pre-\ndicted boundaries bbband gold ones. For end-to-end\nlearning with Gumbel softmax, we introduce a reg-\nularizer based on a Binomial prior. Let k= ∑\ntbt:\nBinomial(α; l,k) =\n(\nl\nk\n)\nαk(1 −α)l−k (7)\nwhere α ∈[0,1] is a hyper-parameter. This reg-\nularizer prevents the model from collapsing into\ntrivially predicting each position as a boundary.\n4 Experimental Setup\n4.1 Datasets\nIn addition to English, we evaluate our model on\ndata in three languages, which represent different\nmorphological types: Finnish for agglutinative, He-\nbrew for introflexive, and Vietnamese for isolating.\nThus, we ensure that dynamic pooling is robust to\ndifferent word length distributions. For English, we\nuse text8 (CC-BY-SA) (Mahoney, 2006), CC-100\n(MIT) (Conneau et al., 2020) and wiki40b (CC-\nBY-SA) (Guo et al., 2020) as they are established\nbenchmarks for character-level language models.\nFor the rest of the languages, we use the corre-\nsponding subsets of wiki40b. To make results\ncomparable across languages and prevent data im-\nbalance, we limit the size of CC-100 and wiki40b\nto the first 400M tokens of the training set and the\nfirst 2M tokens of the validation set. We retain the\noriginal splits for each dataset.\nFor all datasets and languages, we follow the\nsame pre-processing steps of (Mahoney, 2006) for\ncreating text8. Specifically, for each language we\nkeep only the characters from its script, as well as\nwhitespace and an end-of-line. The text is lower-\ncased, and the digits are spelt out in the target\nlanguage. For wiki40b, we also remove special\nstructural markers and normalise homoglyphs. Fi-\nnally, for Hebrew we also remove diacritics as they\nare not required to understand the text. This way,\nwe filter out excerpts in different languages, which\nare known to contaminate noisy multilingual texts\n(Kreutzer et al., 2022). The pre-processing scripts\ncan be found as part of our code.\n4.2 Models\nAll of our experiments, except for the scaling ab-\nlation, use 12-layer Hourglass Transformers with\n2 layers in the first block, 8 layers in the second\nblock which operates on shortened sequences, and\n2 layers in the final block, following (Nawrot et al.,\n2022). For every Transformer layer, the hidden\ndimension is 512, the intermediate feed-forward\ndimension is 2048. Self-attention is split into 8\nheads. We use a post-norm architecture, GELU\nactivation function (Hendrycks and Gimpel, 2016)\nin feed-forward layers and the relative attention\nparametrisation from Transformer XL (Dai et al.,\n2019). In total, the model has ~41M parameters.\nThe boundary predictor is a 2-layer MLP that\n6407\ntakes a hidden state as input and outputs a scalar at\nevery time step. For models with dynamic pooling,\nthis module adds around 1M additional parameters.\nWe use the SentencePiece (Kudo and Richardson,\n2018) library to train Unigram segmentation for\nevery dataset separately. We detect spikes in condi-\ntional entropy according to a window of sizek= 2,\nwhich we select from range k=1 ... 4 for optimal\nBPC on text8. For Gumbel Sigmoid, we set the\nprior probability of a boundaryαto 0.2 for English,\nVietnamese and Hebrew, and0.37 for Finnish. The\nGumbel temperature parameter was set to 0.5 in\nall experiments. For Unigram vocabulary size, we\nset |V|= 10000for English and Vietnamese and\n|V|= 200for Finnish and Hebrew. We list training\nhyper-parameters in Appendix B.\n5 Results\nThe results for the experiments on character-level\nlanguage modelling are shown in Table 1. In ad-\ndition to the four proposed segmentation methods,\nwe include a vanilla Transformer and fixed-size\npooling Transformers with multiple shortening fac-\ntors as baselines. Every model is evaluated with\nrespect to two metrics: bits per character (BPC; ↓)\nand shortening factor (SF;↑). The former measures\nthe negative log-probability of the language model\npredictions, and thus its quality; the latter measures\nthe average reduction of the sequence length in in-\ntermediate layers, and thus the model efficiency.\nFigure 5 shows how higher SF translates to lower\ntraining time and memory consumption in practice,\nas measured on a common GPU with an optimised\nmodel implementation.\nSegmentation Methods In all the English evalu-\nation benchmarks (text8, wiki40b, and CC-100),\nboth whitespace-based and Unigram-based seg-\nmentations achieve the lowest BPC, outperform-\ning both vanilla and fixed-pooling Transformers\nby statistically significant margins.3 Moreover, the\nsame two methods achieve the highest degrees of\nshortening. Note that for equivalent SFs, fixed-size\npooling becomes detrimental to performance. The\napproaches based on entropy spikes and Gumbel-\nSigmoid are generally inferior to the alternatives for\ndynamic pooling. However, for comparable short-\nening factors, they always outperform vanilla and\nfixed-pooling Hourglass models. Moreover, they\nmake the fewest assumptions about the data and the\n3We indicate with a ⋆ wherever this is the case according\nto a Paired Student’s t-test with p <0.05.\n1.13\n1.14\n1.15\n1.16BPC\ntext8\n1.28\n1.30\nwiki40b/he\n1 2 3 4 5 6\nShortening factor\n0.94\n0.95\n0.96BPC\nwiki40b/fi\n1 2 3 4 5\nShortening factor\n1.06\n1.07\n1.08\nwiki40b/vi\nFixed Dynamic (Gumbel) Dynamic (Unigram)\nFigure 3: Test BPC ( ↓) and shortening factor (SF; ↑).\nThe higher the SF, the more efficient the model is (cf.\nFigure 5 in the Appendix). SF increases with higher\nvocabulary size (Unigram) or smaller prior boundary\nprobability (Gumbel). Dynamic pooling methods shift\nthe Pareto front, i.e., increase performance for the same\nefficiency (and vice versa). Note that fixed-pooling at\nk=1 corresponds to the vanilla Transformer model.\navailability of external supervision, so they might\nbe appropriate for other domains (such as speech\nand vision) in future work. In general, providing\na Transformer with the correct inductive bias for\npooling variable-size segments not only facilitates\nscaling but also enhances prediction quality.\nNotably, the gains resulting from whitespace seg-\nmentation are not identical in all languages, due to\ntheir inherent differences in morphological types\nand average word length. Shortening Factors for\nthis method range from 3.8 ×in introflexive He-\nbrew, to 7.9 ×in agglutinative Finnish, whereas\nisolating Vietnamese and mildly fusional English\nlie in between with 4.4 ×and 5.7×, respectively.\nThe larger SFs of dynamic pooling methods trans-\nlate into higher training speed, from 1.7×for Un-\nigram in Hebrew to over 2.5 ×for whitespaces\nin English, while simultaneously lowering BPC.\nOverall, the gains from dynamic pooling are ro-\nbust cross-lingually, but the optimal segmentation\nmethod may vary.\nEfficiency–Performance Pareto Front While\nboth low BPC and high SF are desirable, there ex-\nists a trade-off between them which is specific to\neach boundary prediction method. Hence, the ideal\nmodel should strike the right balance to improve\nin both respects simultaneously. Intuitively, vocab-\n6408\nEnglish Finnish Hebrew Vietnamese\ntext8 wiki40b cc-100 wiki40b wiki40b wiki40b\nBPC SF BPC SF BPC SF BPC SF BPC SF BPC SF\nVanilla 1.143 (1.0x) 1.091 (1.0x) 1.225 (1.0x) 0.945 (1.0x) 1.274 (1.0x) 1.065 (1.0x)\nFixed (SF=2)1.149 (2.0x) 1.084 (2.0x) 1.224 (2.0x) 0.946 (2.0x) 1.279 (2.0x) 1.060 (2.0x)\nFixed (SF=3)1.155 (3.0x) 1.093 (3.0x) 1.229 (3.0x) 0.951 (3.0x) 1.290 (3.0x) 1.068 (3.0x)\nFixed (SF=4)1.166 (4.0x) 1.102 (4.0x) 1.240 (4.0x) 0.961 (4.0x) 1.304 (4.0x) 1.087 (4.0x)\nGumbel 1.136⋆ (4.6x) 1.080 (4.7x) 1.212⋆ (4.6x) 0.941 (2.6x) 1.281 (4.7x) 1.061 (4.3x)\nEntropy 1.138⋆ (4.1x) 1.083 (4.1x) 1.218⋆ (3.8x) 0.949 (4.1x) 1.276 (3.6x) 1.072 (4.2x)\nUnigram 1.134⋆ (5.0x) 1.078⋆ (5.0x) 1.212⋆ (4.8x) 0.937 (2.1x) 1.270⋆ (1.9x) 1.058 (4.0x)\nWhitespaces 1.133⋆ (5.7x) 1.077⋆ (5.6x) 1.214⋆ (5.2x) 0.955 (7.9x) 1.284 (3.8x) 1.057⋆ (4.4x)\nTable 1: Language modelling results on 3 English datasets and 3 other morphologically diverse languages. For each\npair of method and dataset, we report test BPC (↓) and average shortening factor (SF; ↑). We run each experiment 3\ntimes with different random seeds. We mark with a star (⋆) symbol results that are statistically better than both the\nvanilla Transformer baseline and fixed shortening by means of a Paired Student’s t-test withp< 0.05. We report\nresults based on the best hyper-parameter configuration for each language.\nulary size in Unigram and the prior αin Gumbel-\nSigmoid provide easily controllable knobs to study\nthis interaction: as they change, so does the short-\nening factor. In Figure 3, we plot BPC and SF for\nsix vocabulary sizes (200, 500, 1k, 3k, 5k, 10k)\nand five α values (0.20, 0.25, 0.30, 0.37, 0.45)\nand compare them with fixed-size pooling in Hour-\nglass Transformers. Manifestly, dynamic pooling\nenhances the Pareto front by finding more opti-\nmal trade-offs between efficiency and performance.\nMoreover, while fixed pooling follows a similar\ntrend cross-lingually, dynamic pooling behaves\nmore idiosyncratically: e.g. BPC in Vietnamese\nand English surprisingly improves with higher SFs.\nDuring our study of the Efficiency–Performance\nPareto Front, we noticed that the Gumbel-Sigmoid\npooling approach exhibits greater instability com-\npared to the Unigram-based pooling method. This\ncan be observed through artifacts such as the spikes\nin BPC for Hebrew, depicted in Figure 3.\nTime and Space Complexity To capture the con-\ncrete gains in efficiency of models with higher SFs,\nwe have measured the memory consumption and\ntraining time of our PyTorch implementation of\ntext8 models on a typical GPU (NVIDIA GV100\n32GB). The results in Figure 5 apply to dynamic-\npooling (Gumbel, Whitespace, Unigram, and En-\ntropy), fixed-pooling, and vanilla Transformers\n(only for SF=1). Note that these results are iden-\ntical for both fixed-pooling and dynamic-pooling\nHourglass for the same SF as the cost of the bound-\nary predictor is negligible. With a shortening factor\nSF = 2, the model reduces both memory con-\nsumption and training time by over 40%, com-\npared to a vanilla Transformer. At SF = 4, where\ndynamic-pooling Hourglass still achieves superior\nBPC scores, resource consumption is reduced be-\ntween 50% and 60% and training is 2.5 ×faster.\nThis allows models to increase in size with the same\ncompute budget (which depends on the hardware),\nwhile simultaneously benefiting their performance.\nScaling the Model We investigate if dynamic-\npooling Transformers scale well in terms of model\nsize, by adding more layers in the middle block\n(Figure 4). We focus on this block as it increases\nthe model depth (and hence its capacity) while\nretaining a higher efficiency due to operating on\nshortened sequences. We find that the gains from\ndynamic pooling are consistent across all numbers\nof layers. Extrapolating from the trends, dynamic\npooling holds promise to continue providing bene-\nfits even in extremely large language models.\nAverage-pooling vs Sub-sampling As an abla-\ntion, we also compare two different methods to\nrepresent groups of tokens when shortening the in-\nput sequence length: average pooling, used in our\nexperiments, and sub-sampling, i.e. selecting only\nthe last token as a representative for each group.\nAs it emerges from Table 2, average pooling yields\nsuperior performance in all models, including both\nfixed and dynamic pooling Transformers.\nOther Efficient Transformer Models Finally,\nwe remark that our method differs from most effi-\ncient Transformer algorithms, which reduce the\nquadratic complexity of attention (Child et al.,\n2019; Lee-Thorp et al., 2022; Choromanski et al.,\n2021; Wang et al., 2020), as it focuses on length\nreduction. While previous efficient variants tend to\ntrade quality for efficiency, we have shown that the\n6409\nShortening\nSegmentation Avg-Pooling Sub-sampling\nFixed (SF = 2) 1.149 1.180\nEntropy 1.138 1.151\nWhitespaces 1.133 1.144\nTable 2: BPC results on text8 for two shortening meth-\nods (average-pooling and sub-sampling) and three seg-\nmentation methods.\n8 10 12 14 16 18 20\nNumber of layers\n1.12\n1.14\n1.16\n1.18BPC\nFixed (SF=2)\nFixed (SF=3)\nFixed (SF=4)\nDynamic (Unigram)\nDynamic (Whitespaces)\nFigure 4: Test BPC ontext8 plotted against the number\nof Transformer layers for different shortening methods.\nWe use two layers in the first and last transformer block\nand only scale the middle, downsampled block. There\nare 28M parameters in models with 8 layers, up to 69M\nparameters in models with 20 layers. For all variants we\nobserve performance gains with dynamic pooling.\ndynamic-pooling mechanism improves both simul-\ntaneously in our experiments. Moreover, Nawrot\net al. (2022) has shown that combining both strate-\ngies yields further gains.\n6 Related Work\nDynamic RNNs Our approach is inspired by\nvariants of RNNs that process sequences at vary-\ning time scales by introducing a hierarchy of hid-\nden units. For instance, RNNs that mimic speed-\nreading by introducing hidden units that can skip\nover some input elements (Campos et al., 2018;\nSeo et al., 2018). Similarly, (Chung et al., 2017)\ndiscovers the latent hierarchy of an input sequence\nusing a stack of LSTMs. Each layer is equipped\nwith a binary gate responsible for hard boundary\ndetection, where lower-level boundaries determine\nstate updates made by higher-level layers. When-\never the detector ends a segment, its representation\nis fed to the upper layer.\nEarly slow- and fast-changing units were already\nGPU memory\n1\n2\n3\n4\n5\n6 Shortening factor\n26.1G\n14.9G\n12.9G\n12.3G\n11.7G\n11.5G\nTraining step time\n817ms\n461ms\n374ms\n330ms\n322ms\n311ms\nFigure 5: Memory consumption and duration of a train-\ning step for different shortening factors on English\ntext8. These results apply to both dynamic pooling\nand fixed pooling Hourglass models, as well as vanilla\nTransformers (for SF=1).\ndescribed by (Hihi and Bengio, 1995). Similarly,\nClockwork RNN (Koutnik et al., 2014) introduces\na hierarchy of hidden state units that make transi-\ntions at a set of different, fixed frequencies. Adap-\ntive Computation Time networks perform a differ-\nent amount of computation on each sequence item\n(Graves, 2016). Both ideas were combined in Fast-\nSlow RNNs (Mujika et al., 2017) which can choose\na heavy or light transition between timesteps.\nPooling Transformer models While pooling\nblocks in Transformers are related to slowly vary-\ning units in RNNs, their operation is different.\nRNNs suffer from unreliable transport of informa-\ntion across long time spans. Units that act like skip-\nconnections over time can help them to carry in-\nformation (Krueger et al., 2017). In a Transformer\nnetwork, a unit at time t can directly communi-\ncate with any other unit, including previous ones,\nand we find it important to confirm the benefits of\ndynamic pooling in Transformer models.\nPerhaps the most similar approach to ours is\nFunnel Transformer (Dai et al., 2020) which uses\na similar, hourglass-shaped Transformer architec-\nture. After passing through the first block, the data\nis pooled at a fixed rate, processed by the deep\nmiddle Transformer block, and up-sampled for the\nlast block. Canine (Clark et al., 2022) has a simi-\nlar three-part architecture, and processes Unicode\ninputs, which are downsampled with Transformer\nand convolution layers. (Tay et al., 2022) imple-\nments gradient-based subword tokenization within\na Transformer model, which learns dynamic group-\nings of tokens into fixed-size groups. In (Bai et al.,\n2021), sentence and paragraph boundaries were\nused as additional conditioning for the model.\n6410\nBoundary Detection We investigate boundaries\nprovided by an external model, derived directly\nfrom the data, or top-down from the model’s en-\ntropy. (Kreuk et al., 2020) shows a bottom-up ap-\nproach to phoneme segmentation task combining\ncontrastive learning (van den Oord et al., 2019)\nwith a method for boundary detection based on\ndissimilarity between subsequent frames. It was\nlater extended by (Bhati et al., 2021) to segment\nthe sequence of speech frames dynamically. Re-\ncently, (Cuervo et al., 2022) introduced a hierar-\nchical sequence processing model in which units\nin the upper layer operate on a dynamically short-\nened sequence, with the shortening guided by a\nboundary prediction model.\n(Rocki et al., 2016) control the activity of LSTM\ngates with the model’s output cross-entropy. (Al-\npay et al., 2019) used a similar mechanism based on\ninformation content to guide the copying of individ-\nual activations in an LSTM network. Similarly, we\nemploy the entropy of model predictions to choose\nwhere to insert boundaries.\n7 Conclusions\nWe proposed a new family of language models that\npool variable-size segments of tokens in the inter-\nmediate layers in order to enhance the efficiency\nand performance of the Transformer architecture.\nIn particular, we learn a boundary predictor either\nend-to-end through stochastic re-parameterisation,\nthrough supervision (obtained from subword to-\nkenization or spikes in the conditional entropy),\nor based on linguistic boundaries such as words.\nWe evaluate this model extensively on multiple\nlanguage modelling benchmarks in English and\nin other typologically diverse languages: Finnish,\nHebrew, and Vietnamese. Compared to vanilla\nTransformers and fixed pooling, we observe a sig-\nnificant decrease in model perplexity as well as\ntime and space complexity. This opens up the per-\nspective to develop Transformer models capable\nof computing language both hierarchically, with\nthe same abstractions humans perform at different\nlevels of linguistic structure, and conditionally on\nthe information content of each segment.\nIn the future, our dynamic-pooling Transformer\ncan be combined with methods relying on external\nmemory (Wu et al., 2022), encoders operating at a\nfine resolution (Xue et al., 2022; Tay et al., 2022),\nand more generally any task with long-context in-\nputs (Shaham et al., 2022). This may further facili-\ntate the scalability of current language modelling\narchitectures.\n8 Limitations\nLinguistic variation Our results are highly de-\npendent on the target language and its morphol-\nogy. For example, word boundaries might seem\nlike an obvious choice for dynamic segmentation,\nand in fact they achieve the best performance in\nEnglish and Vietnamese. However, for some lan-\nguages like agglutinative Finnish, whitespaces are\nless frequent, which is detrimental to model perfor-\nmance. Explicit word boundaries are not available\nfor all scripts. For example, in Chinese charac-\nters, or in modalities other than text like speech\nor vision, there is no obvious equivalent to whites-\npaces. However, segmentation based on stochastic\nre-parameterisation, subword tokenizers and spikes\nin conditional entropy overcomes these limitations.\nContiguous segments In its current formulation,\ndynamic pooling only allows for merging contigu-\nous segments of tokens in a sequence. However,\nthis is not ideal for morphology types like Hebrew\nwhere morphemes are discontinuous: vowels are\ninterspersed between consonant roots for inflection.\nMoreover, future works should consider higher\nlevels of linguistic structure than words, such as\ndependency trees, for pooling. In this case, dis-\ncontinuous segments may be necessary to handle\nnon-projective syntactic dependencies.\nIndependent boundary decisions The decision\nto emit a boundary at time step tdepends on previ-\nous boundaries only indirectly through the hidden\nrepresentation of the first Transformer block, as\nthis preserves the efficiency of the boundary predic-\ntor. Instead, a recurrent model could be explicitly\nconditioned on previous boundary decisions, which\nhowever would negatively affect the time complex-\nity of the language model.\nWork contribution of authors\nThe idea of training the models with pooling of\nvariable-length segments was discussed among the\nauthors while Jan Chorowski was at the University\nof Wrocław. Experiments were performed by Piotr\nNawrot while he was employed in a research grant\nat the University of Wrocław, under the supervision\nof Adrian Ła ´ncucki and Edoardo M. Ponti. The\nmanuscript was written by Piotr Nawrot, Adrian\nŁa´ncucki and Edoardo M. Ponti.\n6411\nAcknowledgements\nThis work was supported in part by the UKRI\nCentre for Doctoral Training in Natural Lan-\nguage Processing, funded by the UKRI (grant\nEP/S022481/1) and the University of Edinburgh,\nSchool of Informatics and School of Philosophy,\nPsychology & Language Sciences; and the Pol-\nish National Science Center under the OPUS-18\n2019/35/B/ST6/04379 grant.\nReferences\nTayfun Alpay, Fares Abawi, and Stefan Wermter. 2019.\nPreserving activations in recurrent neural networks\nbased on surprisal. Neurocomputing, 342(C):75–82.\nHe Bai, Peng Shi, Jimmy J. Lin, Yuqing Xie, Luchen\nTan, Kun Xiong, Wen Gao, and Ming Li. 2021. Sega-\ntron: Segment-aware transformer for language mod-\neling and understanding. In AAAI.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer. arXiv\npreprint arXiv:2004.05150.\nChris Bentz and Ramon Ferrer-i Cancho. 2016. Zipf’s\nlaw of abbreviation as a language universal. In Pro-\nceedings of the Leiden workshop on capturing phylo-\ngenetic algorithms for linguistics, pages 1–4.\nSaurabhchand Bhati, Jesús Villalba, Piotr ˙Zelasko, Lau-\nreano Moro-Velazquez, and Najim Dehak. 2021.\nSegmental contrastive predictive coding for un-\nsupervised word segmentation. arXiv preprint\narXiv:2106.02170.\nKaj Bostrom and Greg Durrett. 2020. Byte pair encod-\ning is suboptimal for language model pretraining. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2020, pages 4617–4624.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in Neural Information Processing\nSystems, 33:1877–1901.\nVíctor Campos, Brendan Jou, Xavier Giró i Nieto, Jordi\nTorres, and Shih-Fu Chang. 2018. Skip RNN: Learn-\ning to skip state updates in recurrent neural networks.\nIn International Conference on Learning Representa-\ntions.\nRewon Child, Scott Gray, Alec Radford, and\nIlya Sutskever. 2019. Generating long se-\nquences with sparse transformers. arXiv preprint\narXiv:1904.10509.\nKrzysztof Marcin Choromanski, Valerii Likhosherstov,\nDavid Dohan, Xingyou Song, Andreea Gane, Tamas\nSarlos, Peter Hawkins, Jared Quincy Davis, Afroz\nMohiuddin, Lukasz Kaiser, David Benjamin Be-\nlanger, Lucy J Colwell, and Adrian Weller. 2021.\nRethinking attention with performers. In Interna-\ntional Conference on Learning Representations.\nJunyoung Chung, Sungjin Ahn, and Yoshua Bengio.\n2017. Hierarchical multiscale recurrent neural net-\nworks. In International Conference on Learning Rep-\nresentations.\nJonathan H. Clark, Dan Garrette, Iulia Turc, and John\nWieting. 2022. Canine: Pre-training an efficient\ntokenization-free encoder for language representa-\ntion. Transactions of the Association for Computa-\ntional Linguistics, 10:73–91.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451.\nSantiago Cuervo, Adrian Ła ´ncucki, Ricard Marxer,\nPaweł Rychlikowski, and Jan Chorowski. 2022.\nVariable-rate hierarchical CPC leads to acous-\ntic unit discovery in speech. arXiv preprint\narXiv:2206.02211.\nZihang Dai, Guokun Lai, Yiming Yang, and Quoc Le.\n2020. Funnel-Transformer: Filtering out sequential\nredundancy for efficient language processing. Ad-\nvances in Neural Information Processing Systems,\n33:4271–4282.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na fixed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 2978–2988.\nAlex Graves. 2016. Adaptive computation time\nfor recurrent neural networks. arXiv preprint\narXiv:1603.08983.\nMandy Guo, Zihang Dai, Denny Vrandeˇci´c, and Rami\nAl-Rfou. 2020. Wiki-40B: Multilingual language\nmodel dataset. In Proceedings of the 12th Language\nResources and Evaluation Conference, pages 2440–\n2452.\nDan Hendrycks and Kevin Gimpel. 2016. Gaus-\nsian error linear units (GELUs). arXiv preprint\narXiv:1606.08415.\nSalah Hihi and Yoshua Bengio. 1995. Hierarchical re-\ncurrent neural networks for long-term dependencies.\nIn Advances in Neural Information Processing Sys-\ntems, volume 8.\nJason L. Hutchens and Michael D. Alder. 1998. Find-\ning structure via compression. In New Methods in\nLanguage Processing and Computational Natural\nLanguage Learning.\n6412\nEric Jang, Shixiang Gu, and Ben Poole. 2017. Cate-\ngorical reparameterization with gumbel-softmax. In\nInternational Conference on Learning Representa-\ntions.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. arXiv\npreprint arXiv:2001.08361.\nJan Koutnik, Klaus Greff, Faustino Gomez, and Juergen\nSchmidhuber. 2014. A clockwork RNN. In Proceed-\nings of the 31st International Conference on Machine\nLearning, pages 1863–1871.\nFelix Kreuk, Joseph Keshet, and Yossi Adi. 2020. Self-\nSupervised Contrastive Learning for Unsupervised\nPhoneme Segmentation. In Interspeech 2020, pages\n3700–3704.\nJulia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wahab,\nDaan van Esch, Nasanbayar Ulzii-Orshikh, Allahsera\nTapo, Nishant Subramani, Artem Sokolov, Claytone\nSikasote, Monang Setyawan, et al. 2022. Quality at\na Glance: An Audit of Web-Crawled Multilingual\nDatasets. Transactions of the Association for Com-\nputational Linguistics, 10:50–72.\nDavid Krueger, Tegan Maharaj, Janos Kramar, Moham-\nmad Pezeshki, Nicolas Ballas, Nan Rosemary Ke,\nAnirudh Goyal, Yoshua Bengio, Aaron Courville,\nand Christopher Pal. 2017. Zoneout: Regularizing\nRNNs by randomly preserving hidden activations. In\nInternational Conference on Learning Representa-\ntions.\nTaku Kudo. 2018. Subword regularization: Improv-\ning neural network translation models with multiple\nsubword candidates. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 66–75.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71.\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and\nSantiago Ontanon. 2022. FNet: Mixing tokens with\nFourier transforms. In Proceedings of the 2022 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 4296–4313.\nChris J. Maddison, Andriy Mnih, and Yee Whye Teh.\n2017. The concrete distribution: A continuous relax-\nation of discrete random variables. In International\nConference on Learning Representations.\nMatt Mahoney. 2006. Large text compression bench-\nmark. http://www.mattmahoney.net/dc/text.\nhtml. (Online; accessed November 5, 2022).\nClara Meister, Tiago Pimentel, Patrick Haller, Lena\nJäger, Ryan Cotterell, and Roger Levy. 2021. Re-\nvisiting the Uniform Information Density hypothesis.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n963–980.\nAsier Mujika, Florian Meier, and Angelika Steger. 2017.\nFast-slow recurrent neural networks. In Advances in\nNeural Information Processing Systems, volume 30.\nPiotr Nawrot, Szymon Tworkowski, Michał Tyrolski,\nLukasz Kaiser, Yuhuai Wu, Christian Szegedy, and\nHenryk Michalewski. 2022. Hierarchical transform-\ners are more efficient language models. In Findings\nof the Association for Computational Linguistics:\nNAACL 2022, pages 1559–1571.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey\nChu, and Mark Chen. 2022. Hierarchical text-\nconditional image generation with CLIP latents.\narXiv preprint arXiv:2204.06125.\nHongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang,\nJure Leskovec, Dale Schuurmans, and Bo Dai. 2021.\nCombiner: Full attention Transformer with sparse\ncomputation cost. In Advances in Neural Information\nProcessing Systems, volume 34, pages 22470–22482.\nKamil Rocki, Tomasz Kornuta, and Tegan Maharaj.\n2016. Surprisal-driven zoneout. arXiv preprint\narXiv:1610.07675.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox.\n2015. U-Net: Convolutional networks for biomedical\nimage segmentation. In International Conference\non Medical image computing and computer-assisted\nintervention, pages 234–241.\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and\nDavid Grangier. 2021. Efficient content-based sparse\nattention with routing transformers. Transactions of\nthe Association for Computational Linguistics, 9:53–\n68.\nJohn Schulman, Nicolas Heess, Theophane Weber, and\nPieter Abbeel. 2015. Gradient estimation using\nstochastic computation graphs. In Advances in Neu-\nral Information Processing Systems, volume 28.\nMike Schuster and Kaisuke Nakajima. 2012. Japanese\nand Korean voice search. In 2012 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 5149–5152.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1715–1725.\nMinjoon Seo, Sewon Min, Ali Farhadi, and Hannaneh\nHajishirzi. 2018. Neural speed reading via skim-\nRNN. In International Conference on Learning Rep-\nresentations.\n6413\nUri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori\nYoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor\nGeva, Jonathan Berant, et al. 2022. Scrolls: Stan-\ndardized comparison over long language sequences.\narXiv preprint arXiv:2201.03533.\nRichard Sutton. 2019. The bitter lesson.\nhttp://incompleteideas.net/IncIdeas/\nBitterLesson.html. (Online; accessed November\n5, 2022).\nYi Tay, Vinh Q. Tran, Sebastian Ruder, Jai Gupta,\nHyung Won Chung, Dara Bahri, Zhen Qin, Simon\nBaumgartner, Cong Yu, and Donald Metzler. 2022.\nCharformer: Fast character transformers via gradient-\nbased subword tokenization. In International Con-\nference on Learning Representations.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. 2019.\nRepresentation learning with contrastive predictive\ncoding. arXiv preprint arXiv:1807.03748.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in Neural Information Process-\ning Systems, 30.\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang,\nand Hao Ma. 2020. Linformer: Self-attention with\nlinear complexity. arXiv preprint arXiv:2006.04768.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al.\n2022. Emergent abilities of large language models.\narXiv preprint arXiv:2206.07682.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzmán, Ar-\nmand Joulin, and Edouard Grave. 2020. CCNet: Ex-\ntracting high quality monolingual datasets from web\ncrawl data. In Proceedings of the 12th Language\nResources and Evaluation Conference, pages 4003–\n4012.\nYuhuai Wu, Markus Norman Rabe, DeLesley Hutchins,\nand Christian Szegedy. 2022. Memorizing transform-\ners. In International Conference on Learning Repre-\nsentations.\nLinting Xue, Aditya Barua, Noah Constant, Rami Al-\nRfou, Sharan Narang, Mihir Kale, Adam Roberts,\nand Colin Raffel. 2022. Byt5: Towards a token-free\nfuture with pre-trained byte-to-byte models. Transac-\ntions of the Association for Computational Linguis-\ntics, 10:291–306.\nGeorge Kingsley Zipf. 1949. Human behavior and the\nprinciple of least effort: An introduction to human\necology. Addison–Wesley.\n6414\nAppendix\nA Frequently Asked Questions\nA.1 Pros and Cons of shortening methods\nPros Cons\nFixed - Simple - Sub-optimal results, especially for SF > 2\nWhitespaces - Linguistically inspired\n- Does not require a boundary predictor\n- Not available in all languages, e.g., Chinese\n- No control over SF\nEntropy - Better performance than Fixed\n- Suitable for other modalities such as speech and vision\n- Requires a boundary predictor\n- Worse than Unigram and Gumbel\nUnigram - Best trade-off between efficiency and performance\n- Shown to align well with morphological units\n- Requires a boundary predictor\n- Works only in sequential discrete data\n- Requires training a tokenizer up-front\nGumbel - Good trade-off between efficiency and performance\n- Suitable for other modalities such as speech and vision\n- Requires a boundary predictor\n- High variance performance\nTable 3: Pros and cons of different shortening methods. SF is a shorthand for Shortening Factor.\nA.2 What is the ultimate segmentation method?\nWhile Whitespace offers the best performance in many cases, this is not always true even in the linguistic\ndomain. In agglutinative languages (e.g., Finnish), words are longer than in English, which has a\ndetrimental effect on the Whitespace method. For such languages, other dynamic methods that allow\nfor controlling the shortening factor (SF), such as Unigram, are better suited. Moreover, languages with\nnon-Latin scripts (like Chinese) may lack explicit whitespaces. For modalities different from text, such as\nspeech and vision, Gumbel and Entropy are to be favoured as they do not assume the discreteness of the\ninput sequence.\nA.3 Why evaluating on language modelling rather than downstream tasks?\nSince we present a proof of concept for dynamic-pooling Transformers, we limit the experiments to\nlanguage modelling because: 1) it is a foundational NLP task; 2) previous efficient Transformer variants\nwere evaluated on similar benchmarks. Crucially, there is a strong correlation between performance in\nlanguage modelling and downstream tasks.\nA.4 How do you ensure that the results are reliable?\nOur code is based on the optimised, open-source implementation of Transformer-XL from NVIDIA\n(Apache 2.0 License), which reproduces the scores reported by (Dai et al., 2019). Our implementation of\nthe fixed-pooling Hourglass Transformer model similarly reproduces the results from (Nawrot et al., 2022).\nWe make our code publicly available, under the Apache 2.0 License, inheriting from the original source,\nto ensure the reproducibility of our results. Moreover, memory utilisation was measured by controlling\nresource allocation on GPUs (Figure 5) rather than through a naive nvidia-smi readout, as this would\noverestimate the reserved buffers.\nB Hyper-parameters\nFollowing (Dai et al., 2019), we train for 2 ·105 steps with a batch size of 8 and a learning rate 2.5 ·10−4\non 2x NVIDIA RTX 3080. Each training run took from approximately 12h to 30h, depending on the\nconfiguration. We use a linear warm-up schedule for the first 4k steps, followed by a single-cycle cosine\nscheduler. We use an Adam optimiser with β1 = 0.9, β2 = 0.999 and ϵ= 1e−8, and clip the gradients at\n0.25. We apply a 0.1 dropout rate in the attention matrix and feed-forward layers. Before every epoch, we\ncyclically shift the text stream, divide it into non-overlapping chunks of 2048, and shuffle. During the\nevaluation, to provide context to the model, we split the test set into partially overlapping sequences of\nsize l= 2048with a step size of 512 and calculate the model perplexity only over the last 512 tokens.\n6415\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\n8\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. We propose the generic modiﬁcation of the Transformer architecture. Potential risks\nare the same as for any other Transformer based model.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\n1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\n4.1\n□\u0013 B1. Did you cite the creators of artifacts you used?\n4.1\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\n4.1 A.4\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. Dataset we used to train the model were released for research purposes.\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nFor our experiments we used CC-100 (MIT License) and Wikipedia (CC-BY-SA) datasets (text8 and\nwiki40b) that are publicly available datasets released for research purposes with a goal to give the\ncommunity easier access to the information. We assumed that the authors of these datasets took all\nnecessary steps to not allow for any undesirable situations such as leak of private informations.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\n4.1\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\n4.1 B\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n6416\nC □\u0013 Did you run computational experiments?\n4, 5\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\n4.2 B\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\n4.2 B\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nFigure 4 Table 1\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\n4.2 B We also report the libraries and settings in our repository.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n6417",
  "topic": "Pooling",
  "concepts": [
    {
      "name": "Pooling",
      "score": 0.9066701531410217
    },
    {
      "name": "Computer science",
      "score": 0.7749096155166626
    },
    {
      "name": "Transformer",
      "score": 0.5648967623710632
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5280633568763733
    },
    {
      "name": "Autoregressive model",
      "score": 0.5200942754745483
    },
    {
      "name": "Security token",
      "score": 0.508567214012146
    },
    {
      "name": "Entropy (arrow of time)",
      "score": 0.4989042282104492
    },
    {
      "name": "Language model",
      "score": 0.45620298385620117
    },
    {
      "name": "Natural language",
      "score": 0.43868115544319153
    },
    {
      "name": "Cross entropy",
      "score": 0.41390231251716614
    },
    {
      "name": "Machine learning",
      "score": 0.37800899147987366
    },
    {
      "name": "Natural language processing",
      "score": 0.3714396357536316
    },
    {
      "name": "Theoretical computer science",
      "score": 0.33734509348869324
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.2980394661426544
    },
    {
      "name": "Mathematics",
      "score": 0.11433985829353333
    },
    {
      "name": "Econometrics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I98677209",
      "name": "University of Edinburgh",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I219388962",
      "name": "University of Wrocław",
      "country": "PL"
    }
  ],
  "cited_by": 12
}