{
  "title": "LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference",
  "url": "https://openalex.org/W3146091044",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287441572",
      "name": "Graham, Ben",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222055945",
      "name": "El-Nouby, Alaaeldin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222055943",
      "name": "Touvron, Hugo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4214255972",
      "name": "Stock, Pierre",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4214255973",
      "name": "Joulin, Armand",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2296768883",
      "name": "Jégou, Hervé",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2946677435",
      "name": "Douze Matthijs",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2911925209",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W2927302606",
    "https://openalex.org/W2953937638",
    "https://openalex.org/W2331143823",
    "https://openalex.org/W2979314664",
    "https://openalex.org/W2947946877",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2622263826",
    "https://openalex.org/W2950181225",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W1665214252",
    "https://openalex.org/W2994759459",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W3114896399",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W3034885317",
    "https://openalex.org/W3128633047",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W2147800946",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W2469490737",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W3092354667",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3033210410",
    "https://openalex.org/W3145444543",
    "https://openalex.org/W3035452548",
    "https://openalex.org/W2401231614",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2981413347",
    "https://openalex.org/W2963495494",
    "https://openalex.org/W3172509117",
    "https://openalex.org/W2949117887",
    "https://openalex.org/W2960643108",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2274287116",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W3139587317",
    "https://openalex.org/W3128099838",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2922509574",
    "https://openalex.org/W2982083293",
    "https://openalex.org/W2789541106",
    "https://openalex.org/W3034421924",
    "https://openalex.org/W1902934009",
    "https://openalex.org/W2752782242",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W3130071011",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W2884585870",
    "https://openalex.org/W3129603602",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W2964299589",
    "https://openalex.org/W2946948417",
    "https://openalex.org/W2741430497",
    "https://openalex.org/W2937843571",
    "https://openalex.org/W3034363135",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W3016719260"
  ],
  "abstract": "We design a family of image classification architectures that optimize the trade-off between accuracy and efficiency in a high-speed regime. Our work exploits recent findings in attention-based architectures, which are competitive on highly parallel processing hardware. We revisit principles from the extensive literature on convolutional neural networks to apply them to transformers, in particular activation maps with decreasing resolutions. We also introduce the attention bias, a new way to integrate positional information in vision transformers. As a result, we propose LeVIT: a hybrid neural network for fast inference image classification. We consider different measures of efficiency on different hardware platforms, so as to best reflect a wide range of application scenarios. Our extensive experiments empirically validate our technical choices and show they are suitable to most architectures. Overall, LeViT significantly outperforms existing convnets and vision transformers with respect to the speed/accuracy tradeoff. For example, at 80% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on CPU. We release the code at https://github.com/facebookresearch/LeViT",
  "full_text": "LeViT: a Vision Transformer in ConvNet’s Clothing\nfor Faster Inference\nBenjamin Graham Alaaeldin El-Nouby Hugo Touvron Pierre Stock\nArmand Joulin Herv ´e J´egou Matthijs Douze\nAbstract\nWe design a family of image classiﬁcation architectures\nthat optimize the trade-off between accuracy and efﬁ-\nciency in a high-speed regime. Our work exploits re-\ncent ﬁndings in attention-based architectures, which are\ncompetitive on highly parallel processing hardware. We\nrevisit principles from the extensive literature on convo-\nlutional neural networks to apply them to transformers,\nin particular activation maps with decreasing resolutions.\nWe also introduce the attention bias, a new way to inte-\ngrate positional information in vision transformers.\nAs a result, we propose LeVIT: a hybrid neural network\nfor fast inference image classiﬁcation. We consider dif-\nferent measures of efﬁciency on different hardware plat-\nforms, so as to best reﬂect a wide range of application\nscenarios. Our extensive experiments empirically vali-\ndate our technical choices and show they are suitable to\nmost architectures. Overall, LeViT signiﬁcantly outper-\nforms existing convnets and vision transformers with re-\nspect to the speed/accuracy tradeoff. For example, at 80%\nImageNet top-1 accuracy, LeViT is 5 times faster than\nEfﬁcientNet on CPU. We release the code at https:\n//github.com/facebookresearch/LeViT.\n1 Introduction\nTransformer neural networks were initially introduced for\nNatural Language Processing applications [1]. They now\ndominate in most applications of this ﬁeld. They manipu-\nlate variable-size sequences of token embeddings that are\nfed to a residual architecture. The model comprises two\nsorts for residual blocks: Multi-Layer Perceptron (MLP)\nand an original type of layer: the self-attention, which al-\nlows all pairs of tokens in the input to be combined via a\n1\n 2\n 5\n 10\n 20\n 50\n 100\n 200\n 500\n 1k\n 2k\n 5k\n 10k\n 20k\nimages/s\n77\n78\n79\n80\n81\n82\n83Imagenet top-1 accuracy (%)\nDeiT-\nSmall\nB0\nB1\nB2\nB3\nB4\nDeiT-Tiny\nLeViT-\n128S\nLeViT-128\nLeViT-192\nLeViT-256\nLeViT-384\nB0\nB1\nB2\nB3\nB4\nLeViT-128S\nLeViT-128\nLeViT-192\nLeViT-256\nLeViT-384\nEfficientNet\nDeiT\nLeViT\nDeiT-\nSmall\nDeiT-Tiny\n1 CPU thread                    1 GPU\nFigure 1: Speed-accuracy operating points for convolu-\ntional and visual transformers. Left plots: on 1 CPU core,\nRight: on 1 GPU. LeViT is a stack of transformer blocks,\nwith pooling steps to reduce the resolution of the activa-\ntion maps as in classical convolutional architectures.\nbilinear function. This is in contrast to 1D convolutional\napproaches that are limited to a ﬁxed-size neighborhood.\nRecently, the vision transformer (ViT) architecture [2]\nobtained state-of-the-art results for image classiﬁcation\nin the speed-accuracy tradeoff with pre-training on large\nscale dataset. The Data-efﬁcient Image Transformer [3]\nobtains competitive performance when training the ViT\nmodels only on ImageNet [4]. It also introduces smaller\nmodels adapted for high-throughput inference.\nIn this paper, we explore the design space to offer even\nbetter trade-offs than ViT/DeiT models in the regime of\nsmall and medium-sized architectures. We are especially\ninterested in optimizing the performance–accuracy trade-\noff, such as the throughput (images/second) performance\ndepicted in Figure 1 for Imagenet-1k-val [5].\n1\narXiv:2104.01136v2  [cs.CV]  6 May 2021\nWhile many works [6, 7, 8, 9, 10] aim at reducing the\nmemory footprint of classiﬁers and feature extractors, in-\nference speed is equally important, with high throughput\ncorresponding to better energy efﬁciency. In this work,\nour goal is to develop a Vision Transformer-based fam-\nily of models with better inference speed on both highly-\nparallel architectures like GPU, regular Intel CPUs, and\nARM hardware commonly found in mobile devices. Our\nsolution re-introduces convolutional components in place\nof transformer components that learn convolutional-like\nfeatures. In particular, we replace the uniform structure\nof a Transformer by a pyramid with pooling, similar to\nthe LeNet [11] architecture. Hence we call it LeViT.\nThere are compelling reasons why transformers are\nfaster than convolutional architectures for a given compu-\ntational complexity. Most hardware accelerators (GPUs,\nTPUs) are optimized to perform large matrix multipli-\ncations. In transformers, attention and MLP blocks rely\nmainly on these operations. Convolutions, in contrast, re-\nquire complex data access patterns, so their operation is\noften IO-bound. These considerations are important for\nour exploration of the speed/accuracy tradeoff.\nThe contributions of this paper are techniques that al-\nlow ViT models to be shrunk down, both in terms of the\nwidth and spatial resolution:\n• A multi-stage transformer architecture using attention\nas a downsampling mechanism;\n• A computationally efﬁcient patch descriptor that\nshrinks the number of features in the ﬁrst layers;\n• A learnt, per-head translation-invariant attention bias\nthat replaces ViT’s positional embeddding;\n• A redesigned Attention-MLP block that improves the\nnetwork capacity for a given compute time.\n2 Related work\nThe convolutional networks descended from LeNet [11]\nhave evolved substantially over time [12, 13, 14, 15, 16,\n17]. The most recent families of architectures focus on\nﬁnding a good trade-off between efﬁciency and perfor-\nmance [18, 17, 19]. For instance, the EfﬁcientNet [17]\nfamily was discovered by carefully designing individual\ncomponents followed by hyper-parameters search under a\nFLOPs constraint.\nTransformers. The transformer architecture was ﬁrst\nintroduced by Vaswani et al. [1] for machine transla-\ntion. Transformer encoders primarily rely on the self-\nattention operation in conjunction with feed-forward lay-\ners, providing a strong and explicit method for learning\nlong range dependencies. Transformers have been subse-\nquently adopted for NLP tasks providing state-of-the-art\nperformance on various benchmarks [20, 21]. There have\nbeen many attempts at adapting the transformer architec-\nture to images [22, 23], ﬁrst by applying them on pixels.\nDue to the quadratic computational complexity and num-\nber of parameters involved by attention mechanisms, most\nauthors [23, 24] initially considered images of small sizes\nlike in CIFAR or Imagenet64 [25]. Mixed text and im-\nage embeddings already use transformers with detection\nbounding boxes as input [26], i.e. the bulk of the image\nprocessing is done in the convolutional domain.\nThe vision transformer (ViT) [2]. Interestingly, this\ntransformer architecture is very close to the initial NLP\nversion, devoid of explicit convolutions (just ﬁxed-size\nimage patch linearized into a vector), yet it competes with\nthe state of the art for image classiﬁcation. ViT achieves\nstrong performance when pre-trained on a large labelled\ndataset such as the JFT300M (non-public, although train-\ning on Imagenet-21k also produces competitive results).\nThe need for this pre-training, in addition to strong data\naugmentation, can be attributed to the fact that transform-\ners have less built-in structure than convolutions, in partic-\nular they do not have an inductive bias to focus on nearby\nimage elements. The authors hypothesized that a large\nand varied dataset is needed to regularize the training.\nIn DeiT [3], the need for the large pre-training dataset\nis replaced with a student-teacher setup and stronger\ndata augmentation and regularization, such as stochas-\ntic depth [27] or repeated augmentation [28, 29]. The\nteacher is a convolutional neural network that “helps” its\nstudent network to acquire an inductive bias for convo-\nlutions. The vision transformer has been thereafter suc-\ncessfully adopted by a wider range of computer vision\ntasks including object detection [30], semantic segmen-\ntation [31] and image retrieval [32].\nPositional encoding. Transformers take a set as input,\nand hence are invariant to the order of the input. How-\never, in language as well as in images, the inputs come\nfrom a structure where the order is important. The origi-\n2\nnal Transformer [1] incorporates absolute non-parametric\npositional encoding with the input. Other works has\nreplaced them with parametric encoding [33] or adopt\nFourier-based kernelized versions [22]. Absolute position\nencoding enforce a ﬁxed size for the set of inputs, but\nsome works use relative position encoding [34] that en-\ncode the relative position between tokens. In our work, we\nreplace these explicit positional encoding by positional bi-\nases that implicitly encode the spatial information.\nAttention along other mechanisms. Several works\nhave included attention mechanisms in neural network\narchitectures designed for vision [35, 36, 37, 38]. The\nmechanism is used channel-wise to capture cross-feature\ninformation that complements convolutional layers [39,\n40, 41], select paths in different branch of a network [42],\nor combine both [43]. For instance, the squeeze-and-\nexcite network of Huet al. [44] has an attention-like mod-\nule to model the channel-wise relationships between the\nfeatures of a layer. Li et al. [37] use the attention mecha-\nnism between branches of the network to adapt the recep-\ntive ﬁeld of neurons.\nRecently, the emergence of transformers led to hybrid\narchitectures that beneﬁt from other modules. Bello [45]\nproposes an approximated content attention with a posi-\ntional attention component. Child et al. [23] observe that\nmany early layers in the network learn locally connected\npatterns, which resemble convolutions. This suggests that\nhybrid architectures inspired both by transformers and\nconvnets are a compelling design choice. A few recent\nworks explore this avenue for different tasks [46, 47].\nIn image classiﬁcation, a recent work that comes out\nin parallel with ours is the Pyramid Vision Transformer\n(PVT) [48], whose design is heavily inspired by ResNet.\nIt is principally intended to address object and instance\nsegmentation tasks.\nAlso concurrently with our work, Yuan et al. [49] pro-\npose the Tokens-to-Tokens ViT (T2T-ViT) model. Simi-\nlar to PVT, its design relies on re-tokenization of the out-\nput after each layer by aggregating the neighboring tokens\nsuch number of tokens are progressively reduced. Addi-\ntionally, Yuan et al. [49] investigate the integration of ar-\nchitecture design choices from CNNs [44, 50, 51] that can\nimprove the performance and efﬁciency of vision trans-\nformers. As we will see, these recent methods are not as\nmuch focused as our work on the trade-off between accu-\nhead 3 head 8\nK\nQ\nV\nFigure 2: Patch-based convolutional masks in the pre-\ntrained DeiT-base model [3]. The ﬁgure shows 12 of the\n64 ﬁlters per head. Note that the K and Q ﬁlters are very\nsimilar, this is because the weights are entangled in the\nWQW⊤\nK multiplication.\nracy and inference time. They are not competitive with\nrespect to that compromise.\n3 Motivation\nIn this section we discuss the seemingly convolutional be-\nhavior of the transformer patch projection layer. We then\ncarry out “grafting experiments” of a transformer (DeiT-\nS) on a standard convolutional architecture (ResNet-50).\nThe conclusions drawn by this analysis will motivate our\nsubsequent design choices in Section 4.\n3.1 Convolutions in the ViT architecture\nViT’s patch extractor is a 16x16 convolution with stride\n16. Moreover, the output of the patch extractor is mul-\ntiplied by learnt weights to form the ﬁrst self-attention\nlayer’sq,k and vembeddings, so we may consider these\nto also be convolutional functions of the input. This is\nalso the case for variants like DeiT [3] and PVT [48]. In\nFigure 2 we visualize the ﬁrst layer of DeiT’s attention\nweights, broken down by attention head. This is a more\ndirect representation than the principal components de-\npicted by Dosovitskiy et al. [2]. One can observe the typi-\ncal patterns inherent to convolutional architectures: atten-\ntion heads specialize in speciﬁc patterns (low-frequency\ncolors / high frequency graylelvels), and the patterns are\n3\n0 50 100 150 200 250 300\nEpochs\n40\n45\n50\n55\n60\n65\n70\n75\n80top-1 accuracy (%)\nDeiT-S\nResNet-50\n1 stage R50 + 9 Transformer layers\n2 stage R50 + 6 Transformer layers\n3 stage R50 + 3 Transformer layers\nFigure 3: Models with convolutional layers show a faster\nconvergence in the early stages compared to their DeiT\ncounterpart.\nsimilar to Gabor ﬁlters.\nIn convolutions where the convolutional masks overlap\nsigniﬁcantly, the spatial smoothness of the masks comes\nfrom the overlap: nearby pixel receive approximately the\nsame gradient. For ViT convolutions there is no over-\nlap. The smoothness mask is likely caused by the data\naugmentation: when an image is presented twice, slightly\ntranslated, the same gradient goes through each ﬁlter, so\nit learns this spatial smoothness.\nTherefore, in spite of the absence of “inductive bias” in\ntransformer architectures, the trainingdoes produce ﬁlters\nthat are similar to traditional convolutional layers.\n3.2 Preliminary experiment: grafting\nThe authors of the ViT image classiﬁer [2] experimented\nwith stacking the transformer layers above a traditional\nResNet-50. In that case, the ResNet acts as a feature ex-\ntractor for the transformer layers and the gradients can be\npropagated back through the two networks. However, in\ntheir experiments, the number of transformer layers was\nﬁxed (e.g. 12 layers for ViT-Base).\nIn this subsection, we investigate the potential of mix-\ning transformers with convolutional networkunder a sim-\nilar computational budget: We explore trade-offs ob-\ntained when varying the number of convolutional stages\nand transformer layers. Our objective is to evaluate varia-\ntions of convolutional and transformer hybrids while con-\ntrolling for the runtime.\nGrafting. The grafting combines a ResNet-50 and a\nDeiT-Small. The two networks have similar runtimes.\nWe crop the upper stages of the ResNet-50 and like-\nwise reduce the number of DeiT layers (while keeping\nthe same number of transformer and MLP blocks). Since\na cropped ResNet produces larger activation maps than\nthe 14×14 activations consumed by DeiT, we introduce a\npooling layer between them. In preliminary experiments\nwe found average pooling to perform best. The positional\nembedding and classiﬁcation token are introduced at the\ninterface between the convolutional and transformer layer\nstack. For the ResNet-50 stages, we use ReLU activation\nunits [52] and batch normalization [53].\nResults. Table 1 summarizes the results. The grafted\narchitecture produces better results than both DeiT and\nResNet-50 alone. The smallest number of parameters and\nbest accuracy are with two stages of ResNet-50, because\nthis excludes the convnet’s large third stage. Note that in\nthis experiment, the training process is similar to DeiT:\n300 epochs, we measure the top-1 validation accuracy on\nImageNet, and the speed as the number of images that one\nGPU can process per second.\nOne interesting observation that we show Figure 3 is\nthat the convergence of grafted models during training\nseems to be similar to a convnet during the early epochs\nand then switch to a convergence rate similar to DeiT-S. A\nhypothesis is that the convolutional layers have the ability\nto learn representations of the low-level information in the\nearlier layers more efﬁciently due to their strong inductive\nbiases, noticeably their translation invariance. They rely\nrapidly on meaningful patch embeddings, which can ex-\nplain the faster convergence during the ﬁrst epochs.\nDiscussion. It appears that in a runtime controlled\nregime it is beneﬁcial to insert convolutional stages be-\nlow a transformer. Most of the processing is still done in\nthe transformer stack for the most accurate variants of the\ngrafted architecture. Thus, the priority in the next sections\nwill be to reduce the computational cost of the transform-\ners. For this, instead of just grafting, the transformer ar-\nchitecture needs to be merged more closely with the con-\nvolutional stages.\n4\n#ResNet #DeiT-S nb. of FLOPs (M) Speed IMNET\nstages layers Params conv transformer im/s top-1\n0 12 22.0M 57 4519 966 79.9\n1 9 17.1M 820 3389 995 80.6\n2 6 13.1M 1876 2260 1048 80.9\n3 3 15.1M 3385 1130 1054 80.1\n4 0 25.5M 4119 0 1254 78.4\nTable 1: DeiT architecture grafted on top of a truncated\nResNet-50 convolutional architecture.\n4 Model\nIn this section we describe the design process of the\nLeViT architecture and what tradeoffs were taken. The\narchitecture is summarized in Figure 4.\n4.1 Design principles of LeViT\nLeViT builds upon the ViT [2] architecture and DeiT [3]\ntraining method. We incorporate components that were\nproven useful for convolutional architectures. The ﬁrst\nstep is to get a compatible representation. Discounting\nthe role of the classiﬁcation embedding, ViT is a stack\nof layers that processes activation maps. Indeed, the in-\ntermediate “token” embeddings can be seen as the tradi-\ntional C×H×W activation maps in FCN architectures\n(BCHW format). Therefore, operations that apply to ac-\ntivation maps (pooling, convolutions) can be applied to\nthe intermediate representation of DeiT.\nIn this work we optimize the architecture for compute,\nnot necessarily to minimize the number of parameters.\nOne of the design decisions that makes the ResNet [14]\nfamily more efﬁcient than the VGG network [13] is to\napply strong resolution reductions with a relatively small\ncomputation budget in its ﬁrst two stages. By the time\nthe activation map reaches the big third stage of ResNet,\nits resolution has already shrunk enough that the convolu-\ntions are applied to small activation maps, which reduces\nthe computational cost.\n4.2 LeViT components\nPatch embedding. The preliminary analysis in Sec-\ntion 3 showed that the accuracy can be improved when a\nsmall convnet is applied on input to the transformer stack.\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n256x14x14\nattention\n4 heads\nattention\n4 heads\nattention\n4 heads\nattention\n4 heads\nMLP 2x\nMLP 2x\nMLP 2x\nMLP 2x\n+\n+\n+\n+\n+\n+\n+\n+\n+\n512x4x4\nshrink \nattn, 12 heads\nattention\n8 heads\nattention\n8 heads\nattention\n8 heads\nattention\n8 heads\nMLP 2x\nMLP 2x\nMLP 2x\nMLP 2x\nMLP 2x\nconv 3x3 \nImage\n384x7x7\n512\n3x224x224\nDistillation\nclassifier\nSupervised \nclassifier\nshrink \nattn, 8 heads\nconv 3x3 \nconv 3x3 \nconv 3x3 \navg \npool\nattention\n6 heads\nattention\n6 heads\nattention\n6 heads\nattention\n6 heads\nMLP 2x\nMLP 2x\nMLP 2x\nMLP 2x\nMLP 2x\nConvolutions stage 1 stage 2 stage 3\nFLOPs\n1120M\nparameters\n    18.9M\nFigure 4: Block diagram of the LeViT-256 architecture.\nThe two bars on the right indicate the relative resource\nconsumption of each layer, measured in FLOPs, and the\nnumber of parameters.\n5\nIn LeViT we chose to apply 4 layers of3×3 convolutions\n(stride 2) to the input to perform the resolution reduction.\nThe number of channels goes C = 3,32,64,128,256.\nThis reduces the activation map input to the lower layers\nof the transformer without losing salient information. The\npatch extractor for LeViT-256 transforms the image shape\n(3,224,224) into (256,14,14) with 184 MFLOPs. For\ncomparison, the ﬁrst 10 layers of a ResNet-18 perform\nthe same dimensionality reduction with 1042 MFLOPs.\nNo classiﬁcation token. To use theBCHW tensor for-\nmat, we remove the classiﬁcation token. Similar to convo-\nlutional networks, we replace it by average pooling on the\nlast activation map, which produces an embedding used\nin the classiﬁer. For distillation during training, we train\nseparate heads for the classiﬁcation and distillation tasks.\nAt test time, we average the output from the two heads. In\npractice, LeViT can be implemented using eitherBNC or\nBCHW tensor format, whichever is more efﬁcient.\nNormalization layers and activations. The FC layers\nin the ViT architecture are equivalent to 1 ×1 convolu-\ntions. The ViT uses layer normalization before each at-\ntention and MLP unit. For LeViT, each convolution is\nfollowed by a batch normalization. Following [54], each\nbatch normalization weight parameter that joins up with a\nresidual connection is initialized to zero. The batch nor-\nmalization can be merged with the preceding convolution\nfor inference, which is a runtime advantage over layer\nnormalization (for example, on EfﬁcientNet B0, this fu-\nsion speeds up inference on GPU by a factor 2). Whereas\nDeiT uses the GELU function, all of LeViT’s non-linear\nactivations are Hardswish [19].\nMulti-resolution pyramid. Convolutional architec-\ntures are built as pyramids, where the resolution of the\nactivation maps decreases as their number of channels\nincreases during processing. In Section 3 we used the\nResNet-50 stages to pre-process the transformer stack.\nLeViT integrates the ResNet stages within the trans-\nformer architecture. Inside the stages, the architecture is\nsimilar to a visual transformer: a residual structure with\nalternated MLP and activation blocks. In the following we\nreview the modiﬁcations of the attention blocks (Figure 5)\ncompared to the classical setup [1].\nDownsampling. Between the LeViT stages, a shrink-\ning attention blockreduces the size of the activation map:\na subsampling is applied before the Q transformation,\nwhich then propagates to the output of the soft activation.\nThis maps an input tensor of size (C,H,W ) to an out-\nput tensor of size (C′,H/2,W/2) with C′ > C. Due to\nthe change in scale, this attention block is used without a\nresidual connection. To prevent loss of information, we\ntake the number of attention heads to be C/D.\nAttention bias instead of a positional embedding.\nThe positional embedding in transformer architectures is\na location-dependent trainable parameter vector that is\nadded to the token embeddings prior to inputting them to\nthe transformer blocks. If it was not there, the transformer\noutput would be independent to permutations of the input\ntokens. Ablations of the positional embedding result in a\nsharp drop of the classiﬁcation accuracy [55].\nHowever positional embeddings are included only on\ninput to the sequence of attention blocks. Therefore, since\nthe positional encoding is important for higher layers as\nwell, it is likely that it remains in the intermediate repre-\nsentations and needlessly uses representation capacity.\nTherefore, our goal is to provide positional informa-\ntion within each attention block, and to explicitly inject\nrelative position information in the attention mechanism:\nwe simply add an attention bias to the attention maps.\nThe scalar attention value between two pixels (x,y) ∈\n[H]×[W] and (x′,y′) ∈[H]×[W] for one head h∈[N]\nis calculated as\nAh\n(x,y),(x′,y′) = Q(x,y),: •K(x′,y′),: +Bh\n|x−x′|,|y−y′|. (1)\nThe ﬁrst term is the classical attention. The second is the\ntranslation-invariant attention bias. Each head hasH×W\nparameters corresponding to different pixel offsets. Sym-\nmetrizing the differences x−x′and y−y′encourages the\nmodel to train with ﬂip invariance.\nSmaller keys. The bias term reduces the pressure on the\nkeys to encode location information, so we reduce the size\nof the keys matrices relative to the V matrix. If the keys\nhave size D ∈{16,32}, V will have 2D channels. Re-\nstricting the size of the keys reduces the time needed to\ncalculate the key product QK⊤.\nFor downsampling layers, where there is no residual\nconnection, we set the dimension of V to 4D to prevent\nloss of information.\n6\n×\n×+\nK Q V\nconv 1x1 conv 1x1 \nbatchnorm batchnorm batchnorm\nconv 1x1 \nbatchnorm\nHardswish\nSoftmax\nconv 1x1 \nCxHxW\nNx\n(HWxD)\nNx\n(DxHW)\nNx(HWxHW)\nNx(HWx2D)\n(2DN)xHxW\nNx\n(HWx2D)\nCxHxW\nSoftmax+\nconv 1x1 conv 1x1 \n   sub-\nsample\nbatchnorm batchnorm batchnorm\nconv 1x1 \nbatchnorm\nconv 1x1 \nCxHxW\nCxH/2xW/2\nNx\n(HWxD)\nNx\n(DxHW/4)\nNx(HW/4xHW)\nNx(HW/4x4D)\n(4DN)xH/2xW/2\nNx(HWx4D)\nC'xH/2xW/2\nK Q V×\n×\nHardswish\nFigure 5: The LeViT attention blocks, using similar nota-\ntions to [39]. Left: regular version, Right: with 1/2 reduc-\ntion of the activation map. The input activation map is of\nsize C×H×W. N is the number of heads, the multipli-\ncation operations are performed independently per head.\nAttention activation. We apply a Hardswish activation\nto the product AhV before the regular linear projection is\nused to combine the output of the different heads. This is\nakin to a ResNet bottleneck residual block, in the sense\nthat V is the output of a 1 ×1 convolution, AhV cor-\nresponds to a spatial convolution, and the projection is\nanother 1 ×1 convolution.\nReducing the MLP blocks. The MLP residual block\nin ViT is a linear layer that increases the embedding di-\nmension by a factor 4, applies a non-linearity and reduces\nit back with another non-linearity to the original embed-\nding’s dimension. For vision architectures, the MLP is\nusually more expensive in terms of runtime and parame-\nters than the attention block. For LeViT, the “MLP” is a\n1×1 convolution, followed by the usual batch normaliza-\ntion. To reduce the computational cost of that phase, we\nreduce the expansion factor of the convolution from 4 to\n2. One design objective is that attention and MLP blocks\nconsume approximately the same number of FLOPs.\n4.3 The LeViT family of models\nThe LeViT models can spawn a range of speed-accuracy\ntradeoffs by varying the size of the computation stages.\nWe identify them by the number of channels input to the\nﬁrst transformer, e.g. LeViT-256 has 256 channels on in-\nput of the transformer stage. Table 2 shows how the stages\nare designed for the models that we evaluate in this paper.\n5 Experiments\n5.1 Experimental context\nDatasets and evaluation. We model our experiments\non the DeiT work, that is closest to our approach. It builds\nupon PyTorch [56] and the Timm library [57]. We train\non the ImageNet-2012 dataset and evaluate on its valida-\ntion set. We do not explore using more training data in\nthis work.\nResource consumption. The generally accepted mea-\nsure for inference speed is in units of multiply-add opera-\ntions (aka FLOPs) because ﬂoating-point matrix multipli-\ncations and convolutions can be expressed as those.\nHowever, some operations, most notably non-linear ac-\ntivations, do not perform multiply-add operations. They\nare generally ignored in the FLOP counts (or counted as\na single FLOP) because it is assumed that their cost is\nnegligible w.r.t. the cost of higher-order matrix multipli-\ncations and convolutions. However, for a small number\nof channels, the runtime of complicated activations like\nGELU is comparable to that of convolutions. Moreover,\noperations with the same number of FLOPs can be more\nor less efﬁcient depending on the hardware and API used.\nTherefore, we additionally report raw timings on refer-\nence hardware, like recent papers [2, 58]. The efﬁciency\nof transformers relies almost exclusively on matrix multi-\nplications with a large reduction dimension.\nHardware. In this work, we run all experiments in Py-\nTorch, thus we are dependent on the available optimiza-\ntions in that API. In an attempt to obtain more objective\ntimings, we time the inference on three different hardware\nplatforms, each corresponding to one use case:\n• One 16GB NVIDIA V olta GPU (peak performance is\n12 TFLOP/s). This is a typical training accelerator.\n• An Intel Xeon 6138 CPU at 2.0GHz. This is a typical\nserver in a datacenter, that performs feature extraction\non streams of incoming images. PyTorch is well op-\ntimized for this conﬁguration, using MKL and A VX2\ninstructions (16 vector registers of 256 bits each).\n• An ARM Graviton2 CPU (Amazon C6g instance). It\nis a good model for the type of processors that mobile\nphones and other edge devices are running. The Gravi-\nton2 has 32 cores supporting the NEON vector instruc-\ntion set with 32 128-bit vector registers (NEON).\n7\nModel LeViT-128S LeViT-128 LeViT-192 LeViT-256 LeViT-384\n(D= 16,p = 0) (D= 16,p = 0) (D= 32,p = 0) (D= 32,p = 0) (D= 32,p = 0.1)\nStage 1:\n14 ×14 2×\n[C=128\nN=4\n]\n4×\n[C=128\nN=4\n]\n4×\n[C=192\nN=3\n]\n4×\n[C=256\nN=4\n]\n4×\n[C=384\nN=6\n]\nSubsample\n[\nN=8\n] [\nN=8\n] [\nN=6\n] [\nN=8\n] [\nN=12\n]\nStage 2:\n7 ×7 3×\n[C=256\nN=6\n]\n4×\n[C=256\nN=8\n]\n4×\n[C=288\nN=5\n]\n4×\n[C=384\nN=6\n]\n4×\n[C=512\nN=9\n]\nSubsample\n[ N=16 ] [ N=16 ] [ N=9 ] [ N=12 ] [ N=18 ]\nStage 3:\n4 ×4 4×\n[C=384\nN=8\n]\n4×\n[C=384\nN=12\n]\n4×\n[C=384\nN=6\n]\n4×\n[C=512\nN=8\n]\n4×\n[C=768\nN=12\n]\nTable 2: LeViT models. Each stage consists of a number of pairs of Attention and MLP blocks. N: number of heads,\nC: number of channels, D: output dimension of the Q and K operators. Separating the stages are shrinking attention\nblocks whose values of C, C′are taken from the rows above and below respectively. Drop path with probability pis\napplied to each residual connection. The value ofN in the stride-2 blocks isC/Dto make up for the lack of a residual\nconnection. Each attention block is followed by an MLP with expansion factor two.\nOn the GPU we run timings on large image batches\nbecause that corresponds to typical use cases; following\nDeiT we use the maximum power-of-two batchsize that\nﬁts in memory. On the CPU platforms, we measure in-\nference time in a single thread, simulating a setting where\nseveral threads process separate streams of input images.\nIt is difﬁcult to dissociate the impact of the hardware\nand software, so we experiment with several ways to op-\ntimize the network with standard PyTorch tools (the just-\nin-time compiler, different optimization proﬁles).\n5.2 Training LeViT\nWe use 32 GPUs that perform the 1000 training epochs\nin 3 to 5 days. This is more than the usual schedule for\nconvolutional networks, but visual transformers require a\nlong training, for example training DeiT for 1000 epochs\nimproves by another 2 points of top-1 precision over 300\nepochs. To regularize the training, we use distillation\ndriven training, similar to DeiT. This means that LeViT is\ntrained with two classiﬁcation heads with a cross entropy\nloss. The ﬁrst head receives supervision from the ground-\ntruth classes, the second one from a RegNetY-16GF [18]\nmodel trained on ImageNet. In fact, the LeViT training\ntime is dominated by the teacher’s inference time.\n5.3 Speed-accuracy tradeoffs\nTable 3 shows the speed-precision tradeoffs that we ob-\ntain with LeViT, and a few salient numbers are plotted\nin Figure 1. We compare these with two competitive ar-\nchitectures from the state of the art: EfﬁcientNet [17] as\na strong convolutional baseline, and likewise DeiT [3] a\nstrong transformer-only architecture. Both baselines are\ntrained under to maximize their accuracy. For example,\nwe compare with DeiT trained during 1000 epochs.\nIn the range of operating points we consider, the LeViT\narchitecture largely outperforms both the transformer and\nconvolutional variants. LeViT-384 is on-par with DeiT-\nSmall in accuracy but uses half the number of FLOPs.\nThe gap widens for faster operating points: LeViT-128S\nis on-par with DeiT-Tiny and uses 4×fewer FLOPs.\nThe runtime measurements follow closely these trends.\nFor example LeViT-192 and LeViT-256 have about the\nsame accuracies as EfﬁcientNet B2 and B3 but are 5×and\n7×faster on CPU, respectively. On the ARM platform,\n8\n# params FLOPs inference speed ImageNet\ntop-1 GPU Intel ARM -Real -V2.\nArchitecture (M) (M) % im/s im/s im/s % %\nLeViT-128S(ours) 7.8 305 76.6 12880 131.1 39.1 83.1 64.3\nEfﬁcientNet B0 5.3 390 77.1 4754 30.1 3.5 83.5 64.3\nLeViT-128(ours) 9.2 406 78.6 9266 94.0 30.8 84.7 66.6\nLeViT-192(ours) 10.9 658 80.0 8601 65.0 24.2 85.7 68.0\nEfﬁcientNet B1 7.8 700 79.1 2882 20.0 2.3 84.9 66.9\nEfﬁcientNet B2 9.2 1000 80.1 2149 13.1 1.3 85.9 68.8\nLeViT-256(ours) 18.9 1120 81.6 6582 42.5 16.4 86.8 70.0\nDeiT-Tiny 5.9 1220 76.6 3973 39.1 16.8 83.9 65.4\nEfﬁcientNet B3 12 1800 81.6 1272 5.9 0.8 86.8 70.6\nLeViT-384(ours) 39.1 2353 82.6 4165 23.1 9.4 87.6 71.3\nEfﬁcientNet B4 19 4200 82.9 606 2.5 0.5 88.0 72.3\nDeiT-Small 22.5 4522 82.6 1931 13.7 7.6 87.8 71.7\nTable 3: Characteristics of LeViT w.r.t. two strong families of competitors: DeiT [3] and EfﬁcientNet [17]. The top-1\nnumbers are accuracies on ImageNet or ImageNet-Real and ImageNet-V2 (two last columns). The others are images\nper second on the different platforms. LeViT models optimize the trade-off between efﬁciency and accuracy (and not\n#params). The rows are sorted by FLOP counts.\nArchitecture #params FLOPs INET top-1\nT2T-ViTt-14 [49] 21.5M 5200M 80.7\nT2T-ViTt-19 39.0M 8400M 81.4\nT2T-ViTt-24 64.1M 13200M 82.2\nBoT-S1-50 [46] 20.8M 4270M 79.1\nVT-R34 [47] 19.2M 3236M 79.9\nVT-R50 21.4M 3412M 80.6\nVT-R101 41.5M 7129M 82.3\nPiT-Ti [59] 4.9M 700M 74.6\nPiT-XS 10.6M 1400M 79.1\nPiT-S 23.5M 2900M 81.9\nCvT-13-NAS [60] 18M 4100M 82.2\nTable 4: Comparison with the recent state of the art in\nthe high-throughput regime. All inference are performed\non images of size 224×224, and training is done on Ima-\ngeNet only.\nthe ﬂoat32 operations are not as well optimized compared\nto Intel. However, the speed-accuracy trade-off remains\nin LeViT’s favor.\n5.4 Comparison with the state of the art\nTable 4 reports results with other transformer based archi-\ntectures for comparison with LeViT (Table 3). Since our\napproach specializes in the high-throughput regime, we\ndo not include very large and slow models [61, 62].\nWe compare in the FLOPs-accuracy tradeoff, since the\nother works are very recent and do not necessarily provide\nreference models on which we can time the inference. All\nToken-to-token ViT [49] variants take around 5 ×more\nFLOPs than LeViT-384 and more parameters for compa-\nrable accuracies than LeViT. Bottleneck transformers [46]\nand “Visual Transformers” [47] (not to be confused with\nViT) are both generic architectures that can also be used\nfor detection and object segmentation. Both are about 5×\nslower than LeViT-192 at a comparable accuracy. The\nsame holds for the pyramid vision transformer [48] (not\nreported in the table) but its design objectives are differ-\nent. The advantage of LeViT compared to these archi-\ntectures is that it beneﬁted from the DeiT-like distillation,\nwhich makes it much more accurate when training on Im-\nageNet alone. Two architecture that comes close to LeViT\n9\n#id↓ Ablation of LeViT-128S#params FLOPs INET top-1\nBase model 7.4M 305M 71.9\nA1 – without pyramid shape 1.2M 308M 56.5\nA2 – without PatchConv 7.4M 275M 65.3\nA3 – without BatchNorm 7.4M 305M 66.6\nA4 – without distillation 7.4M 305M 69.7\nA5 – without attention bias 7.4M 305M 70.4\nA6 – without wider blocks 6.2M 312M 70.9\nA7 – without attention activ. 7.4M 305M 71.1\nTable 5: Ablation of various components w.r.t. the base-\nline LeViT-128S. Each row is the baseline minus some\nLeViT component (1st column: experiment id). The train-\ning is run for 100 epochs only.\nare the pooling-based vision transformer (PiT) [59] and\nCvT [60], ViT variants with a pyramid structure. PiT, the\nmost promising one, incorporates many of the optimiza-\ntion ingredients for DeiT but is still 1.2 ×to 2.4×slower\nthan LeViT.\nAlternaltive evaluations. In Table 3 we evaluate LeViT\non alternative test sets, Imagenet Real [63] and Ima-\ngenet V2 matched frequency [64]. The two datasets use\nthe same set of classes and training set as ImageNet.\nImagenet-Real has re-assessed labels with potentially sev-\neral classes per image. Imagenet-V2 (in our case match\nfrequency) employs a different test set. It is interesting\nto measure the performance on both to verify that hyper-\nparameters adjustments have not led to overﬁtting to the\nvalidation set of ImageNet. Thus, we measure the classi-\nﬁcation performance on the alternative test sets for mod-\nels that have equivalent accuracies on ImageNet valida-\ntion. LeViT-256 and EfﬁcientNet B3: the LeViT variant\nachieves the same score on -Real, but is slightly worse\n(-0.6) on -V2. LeViT-384 and DeiT-Small: LeViT is\nslightly worse on -Real (-0.2) and -V2 (-0.4). Although\nin these evaluations LeViT is relatively slightly less accu-\nrate, the speed-accuracy trade-offs still hold, compared to\nEfﬁcientNet and DeiT.\n5.5 Ablations\nTo evaluate what contributes to the performance of LeViT,\nwe experiment with the default setting and replace one pa-\nrameter at a time. We train the LeViT-128S model, and a\nnumber of variants, to evaluate the design changes relative\nto ViT/DeiT. The experiments are run with only 100 train-\ning epochs to magnify the differences and reduce train-\ning time. The conclusions remain for larger models and\nlonger training schedules. We replace one component at\na time, when the network needs to be reworked, we make\nsure the FLOP count remains roughly the same (see Ap-\npendix A.2 for details). Table 5 shows that all changes\ndegrade the accuracy:\nA1– The without pyramid shapeablation makes a straight\nstack of attention and MLPs (like DeiT). However, in or-\nder to keep the FLOP count similar to the baseline, the\nnetwork width is reduced, resulting in a network with a\nsmall number of parameters, resulting in a very low ﬁnal\naccuracy. This evidences that the reduction of the reso-\nlution in LeViT is the main tool to keep computational\ncomplexity under control.\nA2– without PatchConv: we remove the four pre-\nprocessing convolutions with a single size-16 convolu-\ntion. This has little effect on the number of parameters,\nbut the number of ﬂops is 10% less. The , and has a strong\nnegative impact on the accuracy. This can be explained\nbecause in a low-capacity regime, the convolutions are an\neffective way to compress the 3 ·162 = 768dimensional\npatch input.\nA3– In without BatchNorm,we replaces BatchNorm with\npreactivated LayerNorm, as used in the ViT/DeiT archi-\ntecture. This slows down the model slightly, as batch\nstatistics need to be calculated at test time. Removing\nthe BatchNorm also removes the zero-initialization of the\nresidual connections, which disrupts training.\nA4– Removing the use of hard distillation from a\nRegNetY-16GF teacher model reduces performance, as\nseen with DeiT.\nA5– The without attention biasablation replaces the atten-\ntion bias component with a classical positional embedding\nadded on input to the transformer stack (like DeiT). Al-\nlowing each attention head to learn a separate bias seems\nto be useful.\nA6– We use DeiT style blocks, i.e.Q,Kand V all have di-\nmension D= C/N, and the MLP blocks have expansion\nfactor 4.\nA7– LeViT has an extra Hardswish non-linearity added\nto the attention, in addition to the softmax non-linearity.\nRemoving it, the without attention activationablation de-\n10\ngrades performance, suggesting that extra non-linearity is\nhelpful for learning classiﬁcation class boundaries.\n6 Conclusion\nThis paper introduced LeViT, a transformer architecture\ninspired by convolutional approaches. The accuracy of\nLeViT stems mainly from the training techniques in DeiT.\nIts speed comes from a series of carefully controlled de-\nsign choices. Compared to other efﬁcient neural nets used\nfor feature extraction in datacenters or on mobile phones,\nLeViT is 1.5 to 5 times faster at comparable precision.\nThus to the best of our knowledge, it sets a new state of\nthe art in the trade-off between accuracy and precision in\nthe high-speed domain. The corresponding PyTorch code\nand models is available at https://github.com/\nfacebookresearch/LeViT.\nReferences\n[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin, “Attention is all you\nneed,” in Advances in Neural Information Process-\ning Systems, 2017. 1, 2, 3, 6\n[2] Alexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly et al.,\n“An image is worth 16x16 words: Transform-\ners for image recognition at scale,” arXiv preprint\narXiv:2010.11929, 2020. 1, 2, 3, 4, 5, 7\n[3] Hugo Touvron, Matthieu Cord, Matthijs Douze,\nFrancisco Massa, Alexandre Sablayrolles, and\nHerv´e J ´egou, “Training data-efﬁcient image trans-\nformers & distillation through attention,” arXiv\npreprint arXiv:2012.12877, 2021. 1, 2, 3, 5, 8, 9\n[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai\nLi, and Li Fei-Fei, “Imagenet: A large-scale hierar-\nchical image database,” in Conference on Computer\nVision and Pattern Recognition, 2009. 1\n[5] Olga Russakovsky, Jia Deng, Hao Su, Jonathan\nKrause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael\nBernstein, Alexander C. Berg, and Li Fei-Fei, “Im-\nagenet large scale visual recognition challenge,” In-\nternational journal of Computer Vision, 2015. 1\n[6] Song Han, Huizi Mao, and William J. Dally, “Deep\ncompression: Compressing deep neural networks\nwith pruning, trained quantization and huffman cod-\ning,” arXiv preprint arXiv:1510.00149, 2016. 2\n[7] Matthieu Courbariaux, Yoshua Bengio, and Jean-\nPierre David, “Binaryconnect: Training deep neural\nnetworks with binary weights during propagations,”\narXiv preprint arXiv:1511.00363, 2016. 2\n[8] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu\nZhou, He Wen, and Yuheng Zou, “Dorefa-net:\nTraining low bitwidth convolutional neural net-\nworks with low bitwidth gradients,” arXiv preprint\narXiv:1606.06160, 2018. 2\n[9] Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin,\nand Song Han, “Haq: Hardware-aware automated\nquantization with mixed precision,” arXiv preprint\narXiv:1811.08886, 2019. 2\n[10] Pierre Stock, Armand Joulin, R ´emi Gribonval, Ben-\njamin Graham, and Herv ´e J´egou, “And the bit goes\ndown: Revisiting the quantization of neural net-\nworks,”arXiv preprint arXiv:1907.05686, 2020. 2\n[11] Yann LeCun, Bernhard Boser, John S Denker, Don-\nnie Henderson, Richard E Howard, Wayne Hubbard,\nand Lawrence D Jackel, “Backpropagation applied\nto handwritten zip code recognition,” Neural com-\nputation, vol. 1, no. 4, pp. 541–551, 1989. 2\n[12] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E.\nHinton, “Imagenet classiﬁcation with deep convo-\nlutional neural networks,” inAdvances in Neural In-\nformation Processing Systems, 2012. 2\n[13] K. Simonyan and A. Zisserman, “Very deep convo-\nlutional networks for large-scale image recognition,”\nin International Conference on Learning Represen-\ntations, 2015. 2, 5\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and\nJian Sun, “Deep residual learning for image recogni-\ntion,” inConference on Computer Vision and Pattern\nRecognition, 2016. 2, 5\n11\n[15] Christian Szegedy, Sergey Ioffe, and Vincent Van-\nhoucke, “Inception-v4, inception-resnet and the im-\npact of residual connections on learning,” arXiv\npreprint arXiv:1602.07261, 2016. 2\n[16] Saining Xie, Ross B. Girshick, Piotr Doll ´ar,\nZhuowen Tu, and Kaiming He, “Aggregated residual\ntransformations for deep neural networks,” Confer-\nence on Computer Vision and Pattern Recognition,\n2017. 2\n[17] Mingxing Tan and Quoc V . Le, “Efﬁcientnet: Re-\nthinking model scaling for convolutional neural net-\nworks,” arXiv preprint arXiv:1905.11946, 2019. 2,\n8, 9\n[18] Ilija Radosavovic, Raj Prateek Kosaraju, Ross B.\nGirshick, Kaiming He, and Piotr Doll´ar, “Designing\nnetwork design spaces,” Conference on Computer\nVision and Pattern Recognition, 2020. 2, 8\n[19] A. Howard, Mark Sandler, G. Chu, Liang-Chieh\nChen, B. Chen, M. Tan, W. Wang, Y . Zhu, R. Pang,\nV . Vasudevan, Quoc V . Le, and H. Adam, “Search-\ning for MobileNetV3,” in International Conference\non Computer Vision, 2019. 2, 6\n[20] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova, “Bert: Pre-training of deep bidi-\nrectional transformers for language understanding,”\narXiv preprint arXiv:1810.04805, 2018. 2\n[21] Alec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever, “Improving language under-\nstanding with unsupervised learning,” 2018. 2\n[22] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit,\nLukasz Kaiser, Noam Shazeer, Alexander Ku, and\nDustin Tran, “Image transformer,” in International\nConference on Machine Learning. PMLR, 2018,\npp. 4055–4064. 2, 3\n[23] Rewon Child, Scott Gray, Alec Radford, and Ilya\nSutskever, “Generating long sequences with sparse\ntransformers,” arXiv preprint arXiv:1904.10509 ,\n2019. 2, 3\n[24] Jean-Baptiste Cordonnier, Andreas Loukas, and\nMartin Jaggi, “On the relationship between self-\nattention and convolutional layers,” arXiv preprint\narXiv:1911.03584, 2020. 2\n[25] Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hut-\nter, “A downsampled variant of imagenet as an\nalternative to the cifar datasets,” arXiv preprint\narXiv:1707.08819, 2017. 2\n[26] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang,\nXiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu,\nLi Dong, Furu Wei et al., “Oscar: Object-semantics\naligned pre-training for vision-language tasks,” in\nEuropean Conference on Computer Vision, 2020. 2\n[27] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and\nKilian Q. Weinberger, “Deep networks with stochas-\ntic depth,” inEuropean Conference on Computer Vi-\nsion, 2016. 2\n[28] Maxim Berman, Herv ´e J ´egou, Andrea Vedaldi, Ia-\nsonas Kokkinos, and Matthijs Douze, “Multigrain: a\nuniﬁed image embedding for classes and instances,”\narXiv preprint arXiv:1902.05509, 2019. 2\n[29] Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi,\nTorsten Hoeﬂer, and Daniel Soudry, “Augment your\nbatch: Improving generalization through instance\nrepetition,” in Conference on Computer Vision and\nPattern Recognition, 2020. 2\n[30] Josh Beal, Eric Kim, Eric Tzeng, Dong Huk\nPark, Andrew Zhai, and Dmitry Kislyuk, “Toward\ntransformer-based object detection,” arXiv preprint\narXiv:2012.09958, 2020. 2\n[31] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xi-\natian Zhu, Zekun Luo, Yabiao Wang, Yanwei\nFu, Jianfeng Feng, Tao Xiang, Philip HS Torr\net al., “Rethinking semantic segmentation from a\nsequence-to-sequence perspective with transform-\ners,” arXiv preprint arXiv:2012.15840, 2020. 2\n[32] Alaaeldin El-Nouby, Natalia Neverova, Ivan Laptev,\nand Herv´e J´egou, “Training vision transformers for\nimage retrieval,” arXiv preprint arXiv:2102.05644,\n2021. 2\n[33] Jonas Gehring, Michael Auli, David Grangier, De-\nnis Yarats, and Yann N. Dauphin, “Convolutional\nsequence to sequence learning,” arXiv preprint\narXiv:1705.03122, 2017. 3\n[34] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani,\n“Self-attention with relative position representa-\n12\ntions,” arXiv preprint arXiv:1803.02155, 2018. 3\n[35] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang,\nCheng Li, Honggang Zhang, Xiaogang Wang, and\nXiaoou Tang, “Residual attention network for im-\nage classiﬁcation,” in Conference on Computer Vi-\nsion and Pattern Recognition, 2017. 3\n[36] Sanghyun Woo, Jongchan Park, Joon-Young Lee,\nand In So Kweon, “Cbam: Convolutional block at-\ntention module,” in European Conference on Com-\nputer Vision, 2018, pp. 3–19. 3\n[37] Xiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang,\n“Selective kernel networks,” inConference on Com-\nputer Vision and Pattern Recognition, 2019, pp.\n510–519. 3\n[38] Irwan Bello, Barret Zoph, Ashish Vaswani,\nJonathon Shlens, and Quoc V Le, “Attention aug-\nmented convolutional networks,” in Conference on\nComputer Vision and Pattern Recognition, 2019, pp.\n3286–3295. 3\n[39] X. Wang, Ross B. Girshick, A. Gupta, and Kaim-\ning He, “Non-local neural networks,”Conference on\nComputer Vision and Pattern Recognition, 2018. 3,\n7\n[40] Yinpeng Chen, Xiyang Dai, Mengchen Liu, Dong-\ndong Chen, Lu Yuan, and Zicheng Liu, “Dynamic\nconvolution: Attention over convolution kernels,” in\nConference on Computer Vision and Pattern Recog-\nnition, 2020. 3\n[41] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun,\n“Exploring self-attention for image recognition,” in\nConference on Computer Vision and Pattern Recog-\nnition, 2020. 3\n[42] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre\nSermanet, Scott Reed, Dragomir Anguelov, Du-\nmitru Erhan, Vincent Vanhoucke, and Andrew Rabi-\nnovich, “Going deeper with convolutions,” in Con-\nference on Computer Vision and Pattern Recogni-\ntion, 2015. 3\n[43] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi\nZhu, Zhi Zhang, Haibin Lin, Yue Sun, Tong He,\nJonas Muller, R. Manmatha, Mu Li, and Alexander\nSmola, “Resnest: Split-attention networks,” arXiv\npreprint arXiv:2004.08955, 2020. 3\n[44] Jie Hu, Li Shen, and Gang Sun, “Squeeze-\nand-excitation networks,” arXiv preprint\narXiv:1709.01507, 2017. 3\n[45] Irwan Bello, “Lambdanetworks: Modeling long-\nrange interactions without attention,” arXiv preprint\narXiv:2102.08602, 2021. 3\n[46] A. Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon\nShlens, P. Abbeel, and Ashish Vaswani, “Bottleneck\ntransformers for visual recognition,” arXiv preprint\narXiv:2101.11605, 2021. 3, 9\n[47] Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin\nWan, Peizhao Zhang, Masayoshi Tomizuka, Kurt\nKeutzer, and Peter Vajda, “Visual transform-\ners: Token-based image representation and pro-\ncessing for computer vision,” arXiv preprint\narXiv:2006.03677, 2020. 3, 9\n[48] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan,\nKaitao Song, Ding Liang, Tong Lu, Ping Luo, and\nLing Shao, “Pyramid vision transformer: A versa-\ntile backbone for dense prediction without convolu-\ntions,” arXiv preprint arXiv:2102.12122, 2021. 3,\n9\n[49] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yu-\njun Shi, Francis EH Tay, Jiashi Feng, and Shuicheng\nYan, “Tokens-to-token vit: Training vision trans-\nformers from scratch on imagenet,” arXiv preprint\narXiv:2101.11986, 2021. 3, 9\n[50] Sergey Zagoruyko and Nikos Komodakis,\n“Wide residual networks,” arXiv preprint\narXiv:1605.07146, 2016. 3\n[51] Gao Huang, Zhuang Liu, Laurens Van Der Maaten,\nand Kilian Q Weinberger, “Densely connected con-\nvolutional networks,” in Conference on Computer\nVision and Pattern Recognition, 2017. 3\n[52] Vinod Nair and Geoffrey E Hinton, “Rectiﬁed linear\nunits improve restricted boltzmann machines,” inIn-\nternational Conference on Machine Learning, 2010.\n4\n[53] Sergey Ioffe and Christian Szegedy, “Batch nor-\nmalization: Accelerating deep network training by\n13\nreducing internal covariate shift,” in International\nConference on Machine Learning, 2015. 4\n[54] Priya Goyal, Piotr Doll ´ar, Ross B. Girshick, Pieter\nNoordhuis, Lukasz Wesolowski, Aapo Kyrola, An-\ndrew Tulloch, Yangqing Jia, and Kaiming He, “Ac-\ncurate, large minibatch sgd: Training imagenet in 1\nhour,”arXiv preprint arXiv:1706.02677, 2017. 6\n[55] Xiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei,\nand Huaxia Xia, “Do we really need explicit position\nencodings for vision transformers?” arXiv preprint\narXiv:2102.10882, 2021. 6\n[56] Adam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga et al., “Pytorch: An imperative style, high-\nperformance deep learning library,” in Advances in\nNeural Information Processing Systems, 2019. 7\n[57] Ross Wightman, “Pytorch image models,” https:\n//github.com/rwightman/pytorch-image-models,\n2019. 7\n[58] Zhuoran Shen, Irwan Bello, Raviteja Vemulapalli,\nXuhui Jia, and Ching-Hui Chen, “Global self-\nattention networks for image recognition,” arXiv\npreprint arXiv:2010.03019, 2020. 7\n[59] Byeongho Heo, Sangdoo Yun, Dongyoon Han,\nSanghyuk Chun, Junsuk Choe, and Seong Joon Oh,\n“Rethinking spatial dimensions of vision transform-\ners,” 2021. 9, 10\n[60] Haiping Wu, Bin Xiao, Noel Codella, Mengchen\nLiu, Xiyang Dai, Lu Yuan, and Lei Zhang, “Cvt:\nIntroducing convolutions to vision transformers,”\n2021. 9, 10\n[61] A. Brock, Soham De, S. L. Smith, and K.\nSimonyan, “High-performance large-scale image\nrecognition without normalization,” arXiv preprint\narXiv:2102.06171, 2021. 9\n[62] Mingxing Tan and Quoc V . Le, “Efﬁcientnetv2:\nSmaller models and faster training,” 2021. 9\n[63] Lucas Beyer, Olivier J. H ´enaff, Alexander\nKolesnikov, Xiaohua Zhai, and Aaron van den\nOord, “Are we done with imagenet?”arXiv preprint\narXiv:2006.07159, 2020. 10\n[64] B. Recht, Rebecca Roelofs, L. Schmidt, and V .\nShankar, “Do imagenet classiﬁers generalize to ima-\ngenet?” arXiv preprint arXiv:1902.10811, 2019. 10\n14\nAppendix\nIn this appendix, we report more details and results.\nAppendix A details the timings of constituent block and\nprovides more details about our ablation. We provide vi-\nsualizations of the attention bias in Appendix B.\nA Detailed analysis\nA.1 Block timings\nIn this section we compare the differences in design be-\ntween DeiT and LeViT blocks from the perspective of\na detailed runtime analysis. We measure the runtime of\ntheir constituent parts side-by-side in the supplementary\nTable 6. For DeiT-Tiny, we replace the GELU activation\nwith Hardswish, as otherwise it dominates the runtime.\nFor DeiT, we consider a block from DeiT-tiny. For\nLeViT, we consider a block from the ﬁrst stage of LeViT-\n256. Both operate at resolution14×14 and have compara-\nble run times, although LeViT is 33% wider (C = 256vs\nC = 192). Note that stage 1 is the most expensive part of\nLeViT-256. In stages 2 and 3, the cost is lower due to the\nreduction in resolution (see Figure 4 of the main paper).\nLeViT spends less time calculating the attentionQKT ,\nbut more time on the subsequent matrix product AV. De-\nspite having the larger block width C, LeViT spends less\ntime on the MLP component as the expansion factor is\nhalved from four to two.\nA.2 More details on our ablation\nHere we give additional details of the ablation experi-\nments in Section 5.6 and Table 4 of the main paper.\nA1 – without pyramid shape. We test the effect of the\nLeViT pyramid structure, we replace the three stages with\na single stage of depth 11 at resolution 14 ×14. To pre-\nserve the FLOP count, we take D = 19, N = 3 and\nC = 2ND = 114.\nA6 – without wider blocks. Compared to DeiT, LeViT\nblocks are relatively wide given the number of FLOPs,\nwith smaller keys and MLP expansion factors. To test\nthis change we modify LeViT-128S to have more tradi-\ntional blocks while preserving the number of FLOPs. We\nTable 6: Timings for the components of the LeViT ar-\nchitecture on an Intel Xeon E5-2698 CPU core with batch\nsize 1.\nModel DeiT-tiny LeViT-256\nDimensions\nC= 192\nN= 3\nD= 64\nC= 256\nN= 4\nD= 32\nComponent Runtime ( µs) Runtime ( µs)\nLayerNorm 49 n/a\nKeysQ,K 299 275\nValuesV 172 275\nProductQKT 228 159\nProduct AttentionAV 161 206\nAttention projection 175 310\nMLP 1390 1140\nTotal 2474 2365\ntherefore take Q,K,V to all have dimension D = 30,\nand C = ND = 120,180,240 for the three stages. As in\nDeiT, the MLP expansion ratio is 4. In the subsampling\nlayers we use N = 4C/D = 16,24, respectively.\nB Visualizations: attention bias\nThe attention bias maps from Eqn. 1 in the main paper are\njust two-dimensional maps. Therefore we can vizualize\nthem, see Figure 6. They can be read as the amount of\nattention between two pixels that are at a certain relative\nposition. The lowest values of the bias are low enough (-\n20) to suppress the attention between the two pixels, since\nthey are input to a softmax.\nWe can observe that some heads are quite uniform,\nwhile other heads specialize in nearby pixels ( e.g. most\nheads of the shrinking attention). Some are clearly direc-\ntional, e.g. heads 1 and 4 of Stage 2/block 1 handle the\npixels adjacent vertically and horizontally (respectively).\nHead 1 of stage 2, block 4 has a speciﬁc period-2 pattern\nthat may be due to the fact that its output is fed to a sub-\nsampling ﬁlter in the next shrinking attention block.\n15\nStage 1, attention block 1 (ﬁrst one)\nhead 0\n head 1\n head 2\n head 3\n...\nStage 1, attention block 4\nhead 0\n head 1\n head 2\n head 3\nShrinking attention between stages 1 and 2\nhead 0\n head 1\n head 2\n head 3\n head 4\n head 5\n head 6\n head 7\nStage 2, attention block 1\nhead 0\n head 1\n head 2\n head 3\n head 4\n head 5\n...\nStage 2, attention block 4\nhead 0\n head 1\n head 2\n head 3\n head 4\n head 5\n...\nStage 3, attention block 4 (last one)\nhead 0\n head 1\n head 2\n head 3\n head 4\n head 5\n head 6\n head 7\nFigure 6: Visualization of the attention bias for several blocks of a trained LeViT-256 model. The center for which\nthe attention is computed is the upper left pixel of the map (with a square). Higher bias values are in yellow, lower\nvalues in dark blue (values range from -20 to 7). 16",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8104122281074524
    },
    {
      "name": "Inference",
      "score": 0.7966287136077881
    },
    {
      "name": "Transformer",
      "score": 0.7530887722969055
    },
    {
      "name": "Exploit",
      "score": 0.6498768329620361
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6325803399085999
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5313081741333008
    },
    {
      "name": "Artificial neural network",
      "score": 0.4919531047344208
    },
    {
      "name": "Computer engineering",
      "score": 0.45603927969932556
    },
    {
      "name": "Code (set theory)",
      "score": 0.44655105471611023
    },
    {
      "name": "Machine learning",
      "score": 0.41872870922088623
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "topic": "Computer science",
  "institutions": []
}