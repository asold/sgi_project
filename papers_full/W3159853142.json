{
    "title": "Lossless text compression using GPT-2 language model and Huffman coding",
    "url": "https://openalex.org/W3159853142",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A5101622878",
            "name": "Md. Atiqur Rahman",
            "affiliations": [
                "University of Aizu"
            ]
        },
        {
            "id": "https://openalex.org/A5020292023",
            "name": "Mohamed Hamada",
            "affiliations": [
                "University of Aizu"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2893804842",
        "https://openalex.org/W2980205694",
        "https://openalex.org/W2914675515",
        "https://openalex.org/W2461402339",
        "https://openalex.org/W3022739281",
        "https://openalex.org/W2989658595",
        "https://openalex.org/W3126271085",
        "https://openalex.org/W4247067284",
        "https://openalex.org/W1990653637",
        "https://openalex.org/W1572929865",
        "https://openalex.org/W2768628697",
        "https://openalex.org/W3092535255",
        "https://openalex.org/W2066637933",
        "https://openalex.org/W2078164219",
        "https://openalex.org/W2912956249",
        "https://openalex.org/W2022126655",
        "https://openalex.org/W2024171325",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W1645564793",
        "https://openalex.org/W2736290231",
        "https://openalex.org/W2767921645",
        "https://openalex.org/W2185881956"
    ],
    "abstract": "Modern daily life activities produced lots of information for the advancement of telecommunication. It is a challenging issue to store them on a digital device or transmit it over the Internet, leading to the necessity for data compression. Thus, research on data compression to solve the issue has become a topic of great interest to researchers. Moreover, the size of compressed data is generally smaller than its original. As a result, data compression saves storage and increases transmission speed. In this article, we propose a text compression technique using GPT-2 language model and Huffman coding. In this proposed method, Burrows-Wheeler transform and a list of keys are used to reduce the original text file’s length. Finally, we apply GPT-2 language mode and then Huffman coding for encoding. This proposed method is compared with the state-of-the-art techniques used for text compression. Finally, we show that the proposed method demonstrates a gain in compression ratio compared to the other state-of-the-art methods.",
    "full_text": "Lossless text compression using GPT-2 language model\nand Huﬀman coding\nMd. AtiqurRahman1,∗ and Mohamed Hamada1,†\n1School of Computer Science and Engineering, The University of Aizu, Aizu-Wakamatsu\nCity, Fukushima, Japan\nAbstract. Modern daily life activities produced lots of information for\nthe advancement of telecommunication. It is a challenging issue to store\nthem on a digital device or transmit it over the Internet, leading to the\nnecessity for data compression. Thus, research on data compression to\nsolvetheissuehasbecomeatopicofgreatinteresttoresearchers. More-\nover, the size of compressed data is generally smaller than its original.\nAs a result, data compression saves storage and increases transmission\nspeed. In this article, we propose a text compression technique using\nGPT-2 language model and Huﬀman coding. In this proposed method,\nBurrows-Wheeler transform and a list of keys are used to reduce the\noriginal text ﬁle’s length. Finally, we apply GPT-2 language mode and\nthen Huﬀman coding for encoding. This proposed method is compared\nwith the state-of-the-art techniques used for text compression. Finally,\nwe show that the proposed method demonstrates a gain in compression\nratio compared to the other state-of-the-art methods.\n1 Introduction\nIt is not easy to manage the increasing amount of data produced every day, especially\ninmedicalcentersandonsocialmedia. Tosavethedatainadigitaldevicerequirestoo\nmuch storage space. According to the research of the6th edition of DOMO’s report\n[1], more than 2.5 quintillion bytes of data are produced daily, and it is growing.\nIt further estimates that approximately 90% of the world’s data has been produced\nbetween 2018 and 2019. It also calculates that each person will create 1.7MB of data\nper second on the earth by 2020. There are three possible solutions to this problem,\nusing better hardware or software or a combination of them. Nowadays, so much\ninformation is created that it is impossible to design new hardware competing with\ndata creation because of the limitations of hardware construction reported in [2].\nTherefore, developing better software is the only solution to the problem.\nA solution from the software viewpoint is compression. A way of representing data\nusing fewer bits than an original is called compression. Compression reduces the con-\nsumption of storage and bandwidth; and increases transmission speed over the cyber\nworld [3-4]. Compression is applied in many areas such as audio, video, text, images,\n∗e-mail: atick.rasel@gmail.com\n†e-mail: mhamada2000@gmail.com\n© The Authors, published by EDP Sciences. This is an open access article distributed under the terms of the Creative Commons \nAttribution License 4.0 (http://creativecommons.org/licenses/by/4.0/).\nSHS Web of Conferences 102, 04013 (2021) \nETLTC2021\nhttps://doi.org/10.1051/shsconf/202110204013\nEtc. Two types of compression are lossless and lossy. The less important and irrele-\nvant data are removed permanently for lossy compression, whereas lossless preserves\nevery detail. Lossless compression eliminates only statistical redundancy. In short,\nlossy allows a little bit of degradation in data and lossless reconstructs perfectly from\nits compressed form [5-6]. There are many applications of lossless compressions, such\nas electronic document compression, medical imagery, zip ﬁle format, and facsimile\ntransmissions of bitonal images [7-9].\nOur primary focus in this article is to compress a text ﬁle. Usually, Burrows-\nWheeler Transform (BWT), Huﬀman coding, LZW (Lempel-Ziv-Welch), LZMA,\nGzip, Bzip2, and Deﬂate are the most popular text compression algorithms [10-11].\nSome statistical compression techniques such as Huﬀman coding, arithmetic coding\nput the shorter codes to the characters repeated more frequently. On the other hand,\nLZ77, LZW compress a text ﬁle based on a dictionary where a set of sub-strings is\ncreated and assigns them a pointer reported in [12-13]. Storer et al. in [14] show that\nthough LZW is a good compression technique, its searching complexity is very high.\nSalomon in [15] shows that Deﬂate is Huﬀman and LZSS based text compression al-\ngorithm and provides better compression, but it is slow due to searching the longer\nand duplicate substrings. Gzip is an LZ77 and Huﬀman coding-based text compres-\nsion algorithm and provides a speedy compression than Deﬂate reported in [16-17].\nRahman et al. show a Burrows-Wheeler transform (BWT), pattern matching, and\nHuﬀman coding-based text compression technique in [18] and Claims a better com-\npression than Deﬂate, Bzip2, Gzip, LZMA, and LZW. It also shows that the method\nis a bit slow.\nBurrows-Wheeler transform (BWT), a reversible technique that converts a set of\ncharacters into runs of similar characters [19], and the technique is used by Bzip2, a\nlossless text compression algorithm [20]. Though many text compression techniques\nhave already been developed, current technology needs a more eﬀective text compres-\nsion strategy.\nFrom this point of view, we propose a straightforward but eﬃcient lossless text\ncompression procedure using GPT-2 language model and Huﬀman coding in this\npaper. Our primary focus is on compression, not the speed of compression. There are\nthree steps in our proposal. First of all, we split a large text ﬁle into a set of small\nﬁles and then apply Burrows-Wheeler transform to each ﬁle individually to speed\nup transformation. Secondly, we use a list of uniquely deﬁned keys to reducing the\nlength of each ﬁle. Finally, the GTP-2 language model and then Huﬀman coding is\napplied for compression. We compare the proposed approach against some standard\nand advanced popular alternatives. In this article, background studies are discussed\nin segment 2. The proposed technique is explained in segment 3. In segment 4,\nwe show the experimental outcomes and analysis. We ﬁnally conclude the paper in\nsegment 5.\n2 Background studies\nHuﬀman is a lossless entropy coding technique that constantly generates a most ef-\nfective tree [21] and works on the contrary path of Shannon-Fano coding. It sorts the\nprobabilities in descending order and makes a tree connecting the two lowest proba-\nbilities each time, and the tree is ﬁnally used for encoding. There are two barriers to\nHuﬀman coding. First of all, it is extremely sensitive to error and may smash nearly\nthe entire message for only modifying one or two bits in transmission. Secondly, it\nprovides a relatively lower compression rate as it assigns a code-word for each pixel\n2\nSHS Web of Conferences 102, 04013 (2021) \nETLTC2021\nhttps://doi.org/10.1051/shsconf/202110204013\nFigure 1: Huﬀman Tree\nin the encoded matrix [22-23]. However, Huﬀman coding provides better compression\nthan Shannon-Fano coding. A detailed analysis of Huﬀman and Shannon-Fano coding\nwith a numeric example has been given in [3]. For the dataset A=[1 2 2 1 2 2 2 6\n6 6 4 4 4 3 3 4 4 4 4 5 5 5], Huﬀman tree is shown in Figure 1. The ﬁgure shows\nthat Huﬀman provides 2.4545 bits averagely for the data list A, which is 0.0122% less\nstorage than Shannon-Fano coding.\nLempel–Ziv–Storer–Szymanski(LZSS),developedbyJamesA.StorerandThomas\nSzymanski, is a dictionary-based text compression technique and a derivative of LZ77\n[24]. Deﬂate was created by Phil Katz in 1951 to compresses data using LZSS and\nHuﬀman coding together [25]. Deﬂate was covered by patents that led to developing\nanother lossless text compression algorithm for its widespread use. For this reason, a\ndeﬂate-based data compression algorithm called Gzip was developed for free use by\nJean-loup Gailly and Mark Adler. The pseudocode of LZ77 is shown in the following\nAlgorithm 1.\nGenerative Pre-trained Transformer 2 (GPT-2) is a language model that uses Byte\nPair Encoding (BPE) techniques for compression. BPE technique chooses subwords\ninsteadofwordsorcharactersinaneuralnetworkreportedin[26-28]. Thepseudocode\nof Byte Pair Encoding (BPE) is demonstrated in the following Algorithm 2.\n3 Proposed method\nThe most popular text compression algorithms are Gzip, Bzip2, Lempel–Ziv–Markov\nchain algorithm (LZMA), Brotli, Etc. Many of them concentrate on the compression\nratios, while others focus on speed. In this article, we mainly focus on compression\nratios. Burrows-Wheeler transform (BWT) takes a long time for a large ﬁle trans-\nformation reported in [18]. To solve the issue, we split a large text ﬁle into a set of\nsmall ﬁles and apply the BWT to each ﬁle to speed up the conversion in our proposed\ntechnique. Secondly, each converted text is reduced by keys. It has been analyzed\n3\nSHS Web of Conferences 102, 04013 (2021) \nETLTC2021\nhttps://doi.org/10.1051/shsconf/202110204013\nAlgorithm 1:The pseudo-code of LZ77 algorithm\n1 Take a text (input);\n2 while input , ′′ do\n3 Find the longest preﬁx of the input in a window and assign it to PF;\n4 if preﬁx , ′′ then\n5 Calculate the distance (X) from where the preﬁx has started and assign\nit to X;\n6 Calculate the length of the preﬁx (Y);\n7 Assign the character to Z that follows preﬁx in the input;\n8 else\n9 Assign 0 to X and Y;\n10 Assign the ﬁrst character of the input to Z;\n11 end\n12 Output a triple (X, Y, Z);\n13 Move the cursor Y+1 positions to the right;\n14 end\nAlgorithm 2:The pseudo-code of Byte Pair Encoding (BPE) algorithm\n1 Split each word into a series of characters;\n2 Find out the highest frequency patterns and perform the joining operation\nwith the highest frequency patterns;\n3 Repeat step 2 until it gets the predeﬁned highest number of subword of\niterations;\nthat GPT-2 model produces a Hangul Syllables list as an output that contains much\nfewer Hangul characters than its input text. We save the number of Hangul characters\nproduced in each section for later reconstruction. Lastly, we combine the outputs of\nthe GPT-2 segments and apply Huﬀman coding for encoding. Figure 2 shows the\ngeneral encoding procedure of the proposed technique.\n4 Experimental results and analysis\nThis section shows the experimental results of some of the most commonly used text\ncompression methods and the proposed technique and explains the methods’ overall\nperformance. The most important thing is to determine the evaluation parameters.\nWe evaluate the methods mentioned in this article based on the compression ratio\ndeﬁned in equation 1. There are many text compression methods used in real applica-\ntions. However, we select Brotli, Bzip2, LZMA, and Gzip for comparison. We choose\nten diﬀerent text samples and apply the state-of-the-art and proposed techniques to\nthem. We show the samples’ compression ratios in Table 1, and Figure 3 demonstrates\na graphical representation of the compression ratios for quick comparison.\nTable 1 shows that, on average, Bzip2, Gzip, LZMA, Brotli, and the proposed\ntechniques give 2.91, 2.5954, 2.8924, 2.7791, and 2.9066 compression ratios for the\nten samples. The table demonstrates that Bzip2 and Gzip provide the highest and\nthe lowest compression ratios. It is calculated that the proposed technique averagely\nprovides 10.707%, 0.489%, and 4.387% better compression than Gzip, LZMA, and\n4\nSHS Web of Conferences 102, 04013 (2021) \nETLTC2021\nhttps://doi.org/10.1051/shsconf/202110204013\nFigure 2: Proposed encoding technique\nBrotli, respectively. However, Bzip2 shows 0.117% better results than the proposed\ntechnique. On average, though Bzip2 provides better compression than the proposed\ntechnique, the proposed technique sometimes shows a better result than Bzip2. As\nan example, the proposed technique provides 0.741%, 4.159%, and 0.133% more com-\npression than Bzip2 for the text samples 2, 3, and 9, respectively.\nCR = Number of bits of an original text\nNumber of bits of compressed text 1\n5 Conclusions\nThis work proposes an easy yet eﬀective text compression procedure using Burrows-\nWheeler transform, GPT-2 language model, and Huﬀman coding. It is inspired as the\n5\nSHS Web of Conferences 102, 04013 (2021) \nETLTC2021\nhttps://doi.org/10.1051/shsconf/202110204013\nTable 1: Experimental compression ratios\nSample Texts Bzip2 Gzip LZMA Brotli Proposed\n1 3.642 2.768 3.601 3.283 3.472\n2 2.948 2.731 2.928 2.853 2.97\n3 3.503 3.503 3.503 3.503 3.655\n4 2.895 2.582 2.878 2.765 2.889\n5 3.169 2.827 3.15 3.027 3.157\n6 3.219 2.871 3.199 3.074 3.208\n7 1.957 1.745 1.945 1.869 1.947\n8 2.932 2.615 2.914 2.8 2.932\n9 2.999 2.675 2.981 2.864 3.003\n10 1.836 1.637 1.825 1.753 1.833\nAverage 2.91 2.5954 2.8924 2.7791 2.9066\nFigure 3: Comparison of compression ratios\n6\nSHS Web of Conferences 102, 04013 (2021) \nETLTC2021\nhttps://doi.org/10.1051/shsconf/202110204013\nGPT-2 works with Byte Pair Encoding and provides much fewer Hangul characters\nthan the original one, and Huﬀman coding provides a better result for a small number\nof symbols. The experimental results show that the proposed technique averagely\nprovides better compression than state-of-the-art techniques without Bzip2. However,\nthe proposed approach shows better reduction for at least 30% of text samples than\nthe Bzip2. Finally, we conclude that the proposed technique sometimes outperforms\nthe state-of-the-art methods.\nReferences\n1 Domo.com. 2020. Becoming A Data-Driven CEO — Domo. [online] Available at:\nhttps://www.domo.com/solution/data-never-sleeps-6 [Accessed 12 June 2020].\n2 Pan, W., Li, Z., Zhang, Y. and Weng, C., 2018. The new hardware development\ntrend and the challenges in data management and analysis.Data Science and\nEngineering, 3(3), pp.263-276.\n3 Rahman, M. and Hamada, M., 2019. Lossless Image Compression Techniques:\nA State-of-the-Art Survey.Symmetry, 11(10), p.1274.\n4 Rahman, M.A., Shin, J., Saha, A.K. and Islam, M.R., 2018, June. A Novel\nLossless Coding Technique for Image Compression.In 2018 Joint 7th Interna-\ntional Conference on Informatics, Electronics & Vision (ICIEV) and 2018 2nd\nInternational Conference on Imaging, Vision & Pattern Recognition (icIVPR)\n(pp. 82-86). IEEE.\n5 Sadchenko, A.; Kushnirenko, O.; Plachinda, O. Fast lossy compression algo-\nrithm for medical images.In Proceedings of the 2016 International Conference\non Electronics and Information Technology (EIT), Odessa, Ukraine, 23–27 May\n2016; pp. 1–4.\n6 Pandey, M.; Shrivastava, S.; Pandey, S.; Shridevi, S. An Enhanced Data\nCompression Algorithm. In Proceedings of the 2020 International Conference\non Emerging Trends in Information Technology and Engineering (ic-ETITE),\nTamil Nadu, India, 24–25 February 2020; pp. 1–4.\n7 Bovik, A.C. ed., 2009. The essential guide to image processing.Academic Press.\n8 Rahman, M.A. and Hamada, M., 2019, October. A Semi-Lossless Image Com-\npression Procedure using a Lossless Mode of JPEG.In 2019 IEEE 13th Inter-\nnational Symposium on Embedded Multicore/Manycore Systems-on-Chip (MC-\nSoC) (pp. 143-148). IEEE.\n9 Rahman, M., Hamada, M. and Shin, J., 2021. The Impact of State-of-the-Art\nTechniques for Lossless Still Image Compression.Electronics, 10(3), p.360.\n10 Oswald, C.; Sivaselvan, B. An optimal text compression algorithm based on fre-\nquent pattern mining.J. Ambient. Intell. Humaniz. Comput. 2018, 9, 803–822.\n11 Portell, J.; Iudica, R.; Garc /acute.ts1ıa-Berro, E.; Villafranca, A.G.; Artigues, G.\nFAPEC, a versatile and eﬃcient data compressor for space missions.Int. J.\nRemote Sens. 2018, 39, 2022–2042.\n12 Rahim, R. Combination of the Blowﬁsh and Lempel-Ziv-Welch Algorithms for\nText Compression;OSF Storage: STMIK Triguna Dharma, Universiti Malaysia\nPerlis, 2017.\n7\nSHS Web of Conferences 102, 04013 (2021) \nETLTC2021\nhttps://doi.org/10.1051/shsconf/202110204013\n13 Welch, T.A. A technique for high-performance data compression. Computer\n1984, 17, 8–19.\n14 Storer, J.A. (Ed.) Image and Text Compression;Springer Science & Business\nMedia: Berlin/Heidelberg, Germany, 2012; Volume 176.\n15 Salomon, D. A Concise Introduction to Data Compression;Springer Science\n& Business Media: Berlin/Heidelberg, Germany, 2007.\n16 Nelson, M.; Gailly, J.L. The Data Compression Book, 2nd ed.;M & T Books:\nNew York, NY, USA, 1995.\n17 Gupta, A.; Bansal, A.; Khanduja, V. Modern lossless compression techniques:\nReview, comparison and analysis.In Proceedings of the 2017 Second Interna-\ntional Conference on Electrical, Computer and Communication Technologies\n(ICECCT), Coimbatore, India, 22–24 February 2017; pp. 1–8.\n18 Rahman, M. and Hamada, M., 2020. Burrows–Wheeler Transform Based Loss-\nless Text Compression Using Keys and Huﬀman Coding.Symmetry, 12(10),\np.1654.\n19 Burrows, M.; Wheeler, D.J. A Block-Sorting Lossless Data Compression Algo-\nrithm; Systems Research Center: Palo Alto, CA, USA, 1994.\n20 Patel, R.A.; Zhang, Y.; Mak, J.; Davidson, A.; Owens, J.D. Parallel lossless\ndata compression on the GPU.In Proceedings of the 2012 Innovative Parallel\nComputing (InPar), San Jose, CA, USA, 13–14 May 2012; pp. 1–9.\n21 Sharma, M., 2010. Compression using Huﬀman coding.IJCSNS International\nJournal of Computer Science and Network Security, 10(5), pp.133-141.\n22 Rufai, A.M., Anbarjafari, G. and Demirel, H., 2013, April. Lossy medical image\ncompression using Huﬀman coding and singular value decomposition.In 2013\n21st Signal Processing and Communications Applications Conference (SIU) (pp.\n1-4). IEEE.\n23 Rahman, M.A., Rabbi, M.F., Rahman, M.M., Islam, M.M. and Islam, M.R.,\n2018, September.Histogrammodiﬁcationbasedlossyimagecompressionscheme\nusing Huﬀman coding.In 2018 4th International Conference on Electrical En-\ngineering and Information & Communication Technology (iCEEiCT) (pp. 279-\n284). IEEE.\n24 Storer, J.A. and Szymanski, T.G., 1982. Data compression via textual substi-\ntution. Journal of the ACM (JACM), 29(4), pp.928-951.\n25 Deutsch, P., 1996. RFC1951: DEFLATE compressed data format speciﬁcation\nversion 1.3.\n26 Radford, A., Wu, J., Child, R., Luan, D., Amodei, D. and Sutskever, I., 2019.\nLanguage models are unsupervised multitask learners.OpenAI blog, 1(8), p.9.\n27 Radford, A., Narasimhan, K., Salimans, T. and Sutskever, I., 2018. Improving\nlanguage understanding by generative pre-training. https://s3-us-west-2.\namazonaws.com/openaiassets/research-covers/language-unsupervised/\nlanguageunderstandingpaper.pdf\n28 Sennrich, R., Haddow, B. and Birch, A., 2015. Neural machine translation of\nrare words with subword units.arXiv preprint arXiv:1508.07909.\n8\nSHS Web of Conferences 102, 04013 (2021) \nETLTC2021\nhttps://doi.org/10.1051/shsconf/202110204013"
}