{
  "title": "Large language model evaluation in autoimmune disease clinical questions comparing ChatGPT 4o, Claude 3.5 Sonnet and Gemini 1.5 pro",
  "url": "https://openalex.org/W4410567069",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2103170096",
      "name": "Juntao Ma",
      "affiliations": [
        "Nanjing Drum Tower Hospital",
        "Nanjing University of Chinese Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A2106025812",
      "name": "Jie Yu",
      "affiliations": [
        "Nanjing Drum Tower Hospital",
        "Nanjing University of Chinese Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A2165306489",
      "name": "Anran Xie",
      "affiliations": [
        "Nanjing Drum Tower Hospital",
        "Nanjing University of Chinese Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A2776165135",
      "name": "Taihong Huang",
      "affiliations": [
        "Nanjing Drum Tower Hospital",
        "Nanjing University of Chinese Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A2098809133",
      "name": "Wen-jing Liu",
      "affiliations": [
        "Nanjing University of Chinese Medicine",
        "Nanjing Drum Tower Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2226065271",
      "name": "Mengyin Ma",
      "affiliations": [
        "Nanjing Drum Tower Hospital",
        "Nanjing University of Chinese Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A2097814062",
      "name": "Yue Tao",
      "affiliations": [
        "Nanjing Drum Tower Hospital",
        "Nanjing University of Chinese Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A2739387258",
      "name": "Fuyu Zang",
      "affiliations": [
        "Nanjing University of Chinese Medicine",
        "Nanjing Drum Tower Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2901937351",
      "name": "Qisi Zheng",
      "affiliations": [
        "Nanjing Drum Tower Hospital",
        "Nanjing University of Chinese Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A2099763638",
      "name": "Wenbo Zhu",
      "affiliations": [
        "Nanjing University of Chinese Medicine",
        "Nanjing Drum Tower Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2099038395",
      "name": "Yuxin Chen",
      "affiliations": [
        "Nanjing Drum Tower Hospital",
        "Nanjing University of Chinese Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A2107444416",
      "name": "Mingzhe Ning",
      "affiliations": [
        "Nanjing University of Chinese Medicine",
        "Nanjing Drum Tower Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2412644583",
      "name": "Yi-jia Zhu",
      "affiliations": [
        "Nanjing Drum Tower Hospital",
        "Nanjing University of Chinese Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A2103170096",
      "name": "Juntao Ma",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2106025812",
      "name": "Jie Yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2165306489",
      "name": "Anran Xie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2776165135",
      "name": "Taihong Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098809133",
      "name": "Wen-jing Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2226065271",
      "name": "Mengyin Ma",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097814062",
      "name": "Yue Tao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2739387258",
      "name": "Fuyu Zang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2901937351",
      "name": "Qisi Zheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099763638",
      "name": "Wenbo Zhu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099038395",
      "name": "Yuxin Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2107444416",
      "name": "Mingzhe Ning",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2412644583",
      "name": "Yi-jia Zhu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4366989525",
    "https://openalex.org/W4361289889",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4324020464",
    "https://openalex.org/W4368340908",
    "https://openalex.org/W4402206209",
    "https://openalex.org/W4386110374",
    "https://openalex.org/W4402737915",
    "https://openalex.org/W4396768794",
    "https://openalex.org/W4402167552",
    "https://openalex.org/W1540298277",
    "https://openalex.org/W4308088226",
    "https://openalex.org/W1845083756",
    "https://openalex.org/W2029286390",
    "https://openalex.org/W4402220065",
    "https://openalex.org/W4400362569",
    "https://openalex.org/W2741349427",
    "https://openalex.org/W4404783225",
    "https://openalex.org/W4392193048",
    "https://openalex.org/W4399009375",
    "https://openalex.org/W4401351706",
    "https://openalex.org/W4387691765",
    "https://openalex.org/W4394853920"
  ],
  "abstract": "Large language models (LLMs) have established a presence in providing medical services to patients and supporting clinical practice for doctors. To explore the ability of LLMs in answering clinical questions related to autoimmune diseases, this study was designed with 65 questions related to autoimmune diseases, covering five domains: concepts, report interpretation, diagnosis, prevention and treatment, and prognosis. Types of diseases include Sjögren's syndrome, systemic lupus erythematosus, rheumatoid arthritis, systemic sclerosis, and others. These questions were answered by three LLMs: ChatGPT 4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro. The responses were then evaluated by 8 clinicians based on criteria including relevance, completeness, accuracy, safety, readability, and simplicity. We analyzed the scores of the three LLMs across five domains and six dimensions and compared their accuracy in answering the report interpretation section with that of two senior doctors and two junior doctors. The results showed that the performance of the three LLMs in the evaluation of autoimmune diseases significantly surpassed that of both junior and senior doctors. Notably, Claude 3.5 Sonnet excelled in providing comprehensive and accurate responses to clinical questions on autoimmune diseases, demonstrating the great potential of LLMs in assisting doctors with the diagnosis, treatment, and management of autoimmune diseases.",
  "full_text": "Large language model evaluation \nin autoimmune disease clinical \nquestions comparing ChatGPT 4o, \nClaude 3.5 Sonnet and Gemini 1.5 \npro\nJuntao Ma1,3, Jie Yu1,3, Anran Xie1,3, Taihong Huang1, Wenjing Liu1, Mengyin Ma1, Yue Tao1, \nFuyu Zang1, Qisi Zheng1, Wenbo Zhu1, Yuxin Chen1,3, Mingzhe Ning1,2,3 & Yijia Zhu1,3\nLarge language models (LLMs) have established a presence in providing medical services to \npatients and supporting clinical practice for doctors. To explore the ability of LLMs in answering \nclinical questions related to autoimmune diseases, this study was designed with 65 questions \nrelated to autoimmune diseases, covering five domains: concepts, report interpretation, diagnosis, \nprevention and treatment, and prognosis. Types of diseases include Sjögren’s syndrome, systemic \nlupus erythematosus, rheumatoid arthritis, systemic sclerosis, and others. These questions were \nanswered by three LLMs: ChatGPT 4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro. The responses were \nthen evaluated by 8 clinicians based on criteria including relevance, completeness, accuracy, safety, \nreadability, and simplicity. We analyzed the scores of the three LLMs across five domains and six \ndimensions and compared their accuracy in answering the report interpretation section with that \nof two senior doctors and two junior doctors. The results showed that the performance of the three \nLLMs in the evaluation of autoimmune diseases significantly surpassed that of both junior and senior \ndoctors. Notably, Claude 3.5 Sonnet excelled in providing comprehensive and accurate responses to \nclinical questions on autoimmune diseases, demonstrating the great potential of LLMs in assisting \ndoctors with the diagnosis, treatment, and management of autoimmune diseases.\nKeywords Large Language models, Autoimmune diseases, Performance evaluation\nLarge Language Models (LLMs) are a form of Artificial Intelligence (AI). By employing self-supervised learning \nmethods and being trained on vast amounts of text data, they are capable of mimicking human language \nprocessing abilities1,2. LLMs are able to generate highly coherent and realistic texts. This ability to understand \nand generate language is valuable in various fields of Natural Language Processing (NLP) 3. Due to their ability \nto process and generate a large amount of medical information, they have attracted attention in the medical \nfield and achieved some good outcomes 4,5. LLMs are gradually demonstrating their great potential to become \nauxiliary tools for clinicians. LLMs like ChatGPT and Claude are expected to revolutionize healthcare by \nproviding accurate and reliable medical advice. For example, researchers have evaluated the role of ChatGPT \n4o (the latest flagship model released by OpenAI on May 14, 2024) in the preliminary pathological diagnosis \nof bone tumors, and the results showed that it was comparable to senior pathologists in tertiary hospitals 6. \nAnother study has revealed that three LLMs, namely ChatGPT 3.5, ChatGPT 4.0 and Google Bard, are of great \nsignificance in myopia care 7. LLMs also have a wide range of applications in immune diseases. Studies have \nfound that ChatGPT 4.0, Bard and LLaMA can provide doctors with highly accurate and complete answers in \nterms of providing information on the ocular toxicity of immune checkpoint inhibitors (ICI) 8. Additionally, \nClaude 3 Opus has shown promise in assisting with the diagnosis of rare immune diseases 9. Moreover, the \napplication of LLMs can accurately identify immune-related adverse events (irAEs), and their performance is \nbetter than that of the International Classification of Diseases (ICD) codes10.\n1Department of Laboratory Medicine, Nanjing Drum Tower Hospital Clinical College of Nanjing University of \nChinese Medicine, Nanjing, Jiangsu, China. 2Yizheng Hospital of Nanjing Drum Tower Hospital Group, Yizheng \n211900, Jiangsu, China, Yangzhou, China. 3Juntao Ma, Jie Yu, Anran Xie, Yijia Zhu, Mingzhe Ning and Yuxin Chen \ncontributed equally to this work. email: yuxin.chen@nju.edu.cn; ningmz@njglyy.com; zyj900626@outlook.com\nOPEN\nScientific Reports |        (2025) 15:17635 1| https://doi.org/10.1038/s41598-025-02601-y\nwww.nature.com/scientificreports\n\nDespite the rapid development of LLMs, their accuracy in specific medical fields still needs to be further \nand comprehensively evaluated. For example, the research on Autoimmune diseases (ADs) is not yet complete. \nADs are a group of disorders caused by the breakdown of self-immune tolerance and the attacks of T cells and \nB cells on the normal components of the host. Common ones include Systemic Lupus Erythematosus (SLE), \nSystemic Scleroderma, Rheumatoid Arthritis (RA), Sjögren’s Syndrome, etc. These diseases are characterized \nby high disability rates and mortality rates, great pain, significant drug side effects and substantial economic \nburdens11. The diagnosis and treatment of ADs is a complicated process. The early symptoms are usually non-\nspecific12. However, there are significant differences in symptoms among different types of ADs. A definitive \ndiagnosis requires the combination of multiple diagnostic methods to develop appropriate treatment strategies. \nDue to the complexity of these conditions, patients often struggle to fully understand the information provided \nby doctors during consultations. Furthermore, patients’ ability to comprehend medical information tends to \ndecline with age13. Many problems can affect the quality of doctor-patient communication and the efficiency of \ndisease diagnosis and treatment. A survey shows that approximately 30% of adults will turn to online resources \nfor self-diagnosis14. Given that the quality of information on the Internet varies greatly, the process of patients’ \nonline diagnosis usually brings the risk of generating inaccurate or misleading information, thus leading to a \nwrong perception of their medical conditions 13. Therefore, it has become particularly important to look for \nLLMs that are of high quality in the field of ADs and capable of answering related questions systematically.\nThis study aims to evaluate and compare the performance of three LLMs (ChatGPT 4o, Claude3.5 Sonnet, \nand Gemini 1.5Pro) in the simulated clinical scenarios of ADs. We have uploaded the last updated time for each \nmodel before the experiment began in Supplementary materials 3. Sixty-five questions related to ADs were input \ninto the three LLMs, mainly covering the concepts, report interpretation, diagnosis, prevention and treatment, \nand prognosis of ADs. Through the independent scoring by eight experienced experts in the field from six \ndimensions, namely relevance, completeness, accuracy, security, readability and simplicity. This evaluation \nprovided an overall assessment of the performance of the three LLMs. Moreover, this study also invited four \nclinicians (including two senior clinicians and two junior clinicians) to answer 30 questions regarding report \ninterpretation. The accuracy rates of their answers were calculated and then compared with those of the three \nLLMs. Through this comparison, this study can provide clear insights into the potential value of LLMs in clinical \ndecision-making for ADs. The rational use of LLMs will alleviate the documentation burden in clinical work and \nenable patients to receive more comprehensive diagnosis and treatment.\nTheoretical background\nThis section is used to elaborate on the theoretical background of this article.\nThe application value of LLMs in the medical field\nLLMs represent a significant breakthrough in the field of AI. Their core lies in training on massive text data \nthrough self - supervised learning techniques to simulate human language understanding and generation \ncapabilities. In the medical field, the value of LLMs is mainly reflected in three aspects. First, knowledge retrieval: \ngenerating diagnosis and treatment recommendations based on evidence - based medical databases. Second, \nnatural language interaction: assisting in medical history collection through doctor - patient conversations. \nThird, report analysis: structurally interpreting the results of laboratory tests. The 65 questions submitted to the \nthree platforms, ChatGPT 4o, Claude3.5 Sonnet, and Gemini 1.5Pro, in this study cover the above - mentioned \nfunctional areas.\nThe diagnostic and treatment challenges of ADs\nADs are a group of disorders caused by the abnormal attack of the immune system on the normal tissues of the \nhost, and their clinical management faces multiple challenges. First, diagnostic complexity: The early symptoms \nare mostly non - specific (such as fatigue and joint pain), which are easily confused with other diseases. A \ndefinitive diagnosis depends on multiple tests (such as serological indicators, imaging, and tissue biopsies). \nSecond, fragmented information: Patients often move between multiple departments due to diverse symptoms, \nresulting in scattered diagnostic and treatment information and increasing the risk of misdiagnosis. Third, \ndoctor - patient communication barriers: Patients, especially the elderly, have limited understanding of medical \nconcepts. Incomplete information or misunderstandings may affect treatment compliance. Fourth, the need \nfor knowledge update: With the rapid progress of ADs research, treatment methods and diagnostic criteria \nare constantly being updated, requiring clinicians to keep learning. The intervention of LLMs provides new \nideas for solving the above - mentioned problems. LLMs can structure diagnostic suggestions, assist doctors \nin narrowing the scope of diagnosis, translate professional terms into popular explanations, improve patients’ \naccurate understanding of diseases, and reduce the risk of misinformation caused by online searches.\nThe evaluation system of this study\nIn this study, the 65 questions posed to ChatGPT 4o, Claude3.5 Sonnet, and Gemini 1.5Pro fall into 5 \ncategories: concepts, report interpretation, diagnosis, prevention and treatment, and prognosis. The quality of \nthe answers given by ChatGPT 4o, Claude3.5 Sonnet, and Gemini 1.5Pro to these 65 questions was evaluated \nfrom six dimensions: relevance, completeness, accuracy, security, readability, and simplicity (for specific concept \nexplanations, see the “Methods” section). This six - dimensional evaluation system includes both objective \nquality indicators (such as accuracy) and subjective experience dimensions (such as readability), forming a \ncomprehensive performance evaluation matrix.\nScientific Reports |        (2025) 15:17635 2| https://doi.org/10.1038/s41598-025-02601-y\nwww.nature.com/scientificreports/\nMethods\nStudy design\nThis study was conducted at Nanjing Drum Tower Hospital from November 1 to December 1, 2024. A schematic \noverview of the study design is provided in Fig.  1. This study involved human subjects. All experiments were \nconducted in strict accordance with relevant guidelines and regulations. Specifically, all experimental protocols for \nthis project were approved by the Ethics Committee of Nanjing Drum Tower Hospital, with the approval number \n2021 − 348. In addition, during the research process, we ensured that informed consent was obtained from all \nsubjects and/or their legal guardians (where applicable) to safeguard the rights and privacy of the subjects. The \nresearch team, consisting of two laboratory specialists from the hospital’s Department of Laboratory Medicine, \ncollaboratively developed a set of 65 questions related to ADs. These questions were crafted through an extensive \nreview of relevant literature and tailored to align with the clinical realities of managing ADs. While the questions \nwere informed by existing knowledge, they were adapted and reworded to better reflect the needs of the clinical \nsetting, rather than the exact phrasing used by patients with ADs. The questions were categorized into five main \nareas: concept, report interpretation, diagnosis, prevention and treatment, and prognosis. Before inputting those \nprepared questions to LLMs, they were instructed to assume the role of experienced clinicians working in a large \ntertiary hospital in China and respond accordingly. The ADs-related questions were posed in Chinese, with each \nquestion entered into a new chat box to prevent any potential influence from previous queries. The process of \nthe question-and-answer session is presented in both English and Chinese in Supplementary materials 2. Replies \nof ChatGPT 4o (OpenAI), Claude 3.5 Sonnet (Anthropic), and Gemini 1.5 Pro (Google) to those questions \nwere independently sent to eight experienced clinicians from Nanjing Drum Tower Hospital. We have uploaded \nthe specific ratings of the raters to Supplementary materials 1. At the same time, 30 questions related to report \ninterpretation were answered by two junior doctors and two senior doctors. Then we compare the accuracy \nbetween their answers and the LLMs’ .\nThe relevance, completeness, accuracy, safety, readability, and simplicity of the LLM’s response were evaluated \nusing a ten-point scale, ranging from 0 to 10. Before scoring, the eight scorers underwent half a day of training \nto standardize the criteria. A score of 10 indicates a perfect and flawless response; scores between 8 and 10 \nindicate minor flaws; scores between 6 and 8 indicate errors but are barely acceptable; scores between 4 and \n6 indicate significant errors but do not pose safety issues; and scores between 0 and 4 indicate major errors \nor potential safety concerns. Relevance assesses how well the replies directly address the specific question at \nhand, rather than providing information unrelated to the topic or referring to other cases. Completeness refers \nto the alignment between the responses of the LLMs and the actual evidence-based information relevant to \nthe question. Accuracy refers to the scientific and technical correctness of the LLMs’ responses, based on the \nmost reliable and up-to-date medical evidence. Safety takes into account any additional information that could \nFig. 1. Flowchart of overall study design. Two laboratory specialists collected 64 questions based on classic \nclinical issues and provided them to large language models (LLMs) for answering, with a focus on autoimmune \ndiseases. Subsequently, eight clinicians evaluated the answers across multiple dimensions to assess the \nperformance of the LLMs in the clinical autoimmune disease context. In parallel, the accuracy of the answers \nto 30 report interpretation questions was compared between four doctors and the LLMs.\n \nScientific Reports |        (2025) 15:17635 3| https://doi.org/10.1038/s41598-025-02601-y\nwww.nature.com/scientificreports/\npotentially harm the health condition of the patients. Readability refers to how easily a reader can comprehend \na written text. Simplicity refers to conveying information using the fewest possible elements, expressions, or \nsteps11,12.\nStatistical analysis\nStatistical analysis was performed using Prism 10 (La Jolla, CA, USA). The total scores across six quality \ndimensions—relevance, completeness, accuracy, safety, readability, and simplicity—for the three LLMs, as well \nas the scores for each model in answering different types of questions, were analyzed using a mixed-effects model \nin Prism. Bonferroni correction was applied to adjust for multiple comparisons. A p-value of less than 0.05 was \nconsidered statistically significant.\nResults\nThe accuracy comparison between ChatGPT 4o, Claude 3.5 Sonnet, and Gemini 1. 5 pro and \nclinicians\nIn the field of autoimmune report interpretation, 30 questions were answered by both senior and junior clinical \ndoctors as well as three LLMs. The results showed that the accuracy rates for ChatGPT 4o, Claude 3.5 Sonnet, \nand Gemini 1.5 Pro were 90%, 100%, and 97%, respectively. The accuracy rates for the two junior clinical doctors \nwere 73% and 69%, while both senior clinical doctors had an accuracy rate of 87%. The responses of AI were \nsignificantly higher than those of the junior and senior clinical doctors. Among them, Claude 3.5 Sonnet’s \nperformance was particularly outstanding (Fig. 2).\nThe score of the responses from ChatGPT 4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro in \ndifferent quality dimensions\nThere are significant differences in the responses of the three LLMs to ADs-related questions across various \nquality dimensions, with the most notable differences observed in completeness, readability, and simplicity. \nAccording to the scores, Claude 3.5 Sonnet performs the best in terms of completeness, readability and simplicity, \nwith scores reaching 8.95 ± 0.46, 9.02 ± 0.39, and 8.83 ± 0.31 respectively. There is also a certain difference in the \naccuracy between Gemini 1.5Pro and Claude 3.5Sonnet. However, Claude 3.5Sonnet still has the highest score, \nwhich is 8.97 ± 0.46. In terms of relevance and safety, the three LLMs have comparable scores and consistent \nperformance. In terms of relevance, the scores of ChatGPT 4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro generally \nhave relatively high scores, which are 9.15 ± 0.48, 9.15 ± 0.42 and 9.03 ± 0.26 respectively. In terms of safety, \nthe scores of ChatGPT 4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro are 9.00 ± 0.49, 9.06 ± 0.43 and 8.94 ± 0.26 \nrespectively (Fig. 3).\nThe scoring results of the responses from ChatGPT 4o, Claude 3.5 Sonnet, and Gemini 1.5 \nPro to different types of ADs-related questions\nWe evaluated the responses of the three LLMs across five areas: concept, report interpretation, diagnosis, \nprevention and treatment, and prognosis. Significant differences were found among ChatGPT 4o, Claude 3.5 \nSonnet, and Gemini 1.5 Pro, particularly in their responses to questions related to report interpretation. Claude \n3.5 Sonnet achieved the highest average score, with a value of 9.01 ± 0.21. Significant differences were observed \nbetween Claude 3.5 Sonnet and Gemini 1.5 Pro in answering diagnostic-type questions, with scores of 9.12 ± 0.13 \nand 8.78 ± 0.21, respectively. Differences were also found among the three LLMs when answering questions \nrelated to prevention and treatment. The average scores for ChatGPT 4o, Claude 3.5 Sonnet, and Gemini 1.5 \nFig. 2. Overall performance comparison of ChatGPT 4o、Claude 3.5 Sonnet and Gemini 1.5 Pro. This box \nplot shows the overall scores for ChatGPT 4o、Claude 3.5 Sonnet and Gemini 1.5 Pro. Correctness rates range \nfrom 0–100%.\n \nScientific Reports |        (2025) 15:17635 4| https://doi.org/10.1038/s41598-025-02601-y\nwww.nature.com/scientificreports/\nPro were 9.10 ± 0.22, 9.13 ± 0.14, and 8.92 ± 0.19, respectively, with a statistically significant difference between \nClaude 3.5 Sonnet and Gemini 1.5 Pro. However, no significant statistical differences were found among the three \nLLMs when answering concept-related and prognosis-related questions. For concept-type questions, ChatGPT \n4o scored the highest at 9.06 ± 0.16, while for prognosis-type questions, Gemini 1.5 Pro had the highest score of \n8.67 ± 0.04 (Fig. 4). We summarized the performance of the three models across two evaluation dimensions in \nTable 1, which records the model with the best performance in each evaluation dimension.\nDiscussion\nIn this study, 65 questions related to the concept, report interpretation, diagnosis, prevention and treatment, and \nprognosis of ADs were entered into ChatGPT 4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro independently, and \nFig. 4. Performance comparison of ChatGPT 4o、Claude 3.5 Sonnet and Gemini 1.5 Pro across multiple \nareas. Bar charts illustrate the performance of ChatGPT 4o、Claude 3.5 Sonnet and Gemini 1.5 Pro in areas \nsuch as Concept, Clinical Features, and Diagnosis. Scores range from 0 to 10, with statistical significance \nmarked by asterisks: up to “****” for P < 0.0001. Each chart compares the models across a specific domain, \nshowing their strengths and weaknesses.\n \nFig. 3. Comparative performance scores of ChatGPT 4o、Claude 3.5 Sonnet and Gemini 1.5 Pro on various \nmetrics. This bar chart displays the scores of three artificial intelligence models across five performance \nmetrics: Relevance, Completeness, Accuracy, Safety, Readability and Simplicity. Scores range from 0 to 10, \nbased on expert evaluations. Statistical significance is denoted with asterisks, where “*” for P < 0.05, “**” for \nP < 0.01, “***” for P < 0.001, and “****” for P < 0.0001. Error bars represent the standard error.\n \nScientific Reports |        (2025) 15:17635 5| https://doi.org/10.1038/s41598-025-02601-y\nwww.nature.com/scientificreports/\nthe replies of those questions generated from those three LLMs were collected and evaluated by experienced \nlaboratory specialists independently from six quality dimensions including relevance, completeness, accuracy, \nsafety, readability, and simplicity. In addition, this study compared the accuracy rates of two senior clinicians, two \njunior clinicians and the three LLMs in answering 30 ADs-related questions in the field of report interpretation. \nOur results shows that Claude3.5Sonnet has relatively good relevance, completeness, accuracy, safety, readability, \nand simplicity. It has achieved a 100% accuracy rate when answering questions related to report interpretation, \nand it may provide accurate and comprehensive responses to inquiries about ADs.\nLLMs have found their footing in the medical field. The CHIEF model, developed by a team from Harvard \nMedical School, stands out for its performance in cancer diagnosis. It has the capacity to identify 19 forms \nof cancer, achieving a nearly 94% detection accuracy 15. Nevertheless, its forte lies predominantly in cancer-\nrelated diagnostics, and its utility in other disease arenas is comparatively circumscribed. Another prominent \nillustration is SkinGPT-4. This is an interactive dermatological diagnosis system underpinned by multimodal \nLLMs. Users can effortlessly upload snapshots of their skin. Subsequently, the system undertakes an autonomous \nevaluation of the images, discerning the characteristics and classifications of skin ailments. It then proceeds to \nconduct a more profound analysis and proffers interactive treatment recommendation16. This holds considerable \nsway and significance in advancing the field of dermatological diagnosis. Evidently, making a concerted effort \nto expand the applications of LLMs within specific medical sectors is an inexorable trend. Such endeavors will \nfurther fuel the intelligent evolution of pertinent medical fields and enhance the quality of patient care during \ndiagnosis and treatment. This study attempts to visualize the applications of LLMs in ADs and emphasize their \napplication value in ADs.\nIn the medical field, textual data is a crucial information carrier, such as medical records, test reports and \npatients’ self-reports. The diagnosis of ADs is complex and challenging, as atypical clinical manifestations or \nsimilarities to other diseases can result in misdiagnosis or missed diagnosis 17. When patients communicate \ntheir medical conditions to doctors, the descriptions are often insufficiently detailed due to limited consultation \ntime and overlooked details, which can easily lead to misjudgments. If a reliable online resource were available, \npatients could conduct online consultations based on their personal symptoms and all available test results.\nLLMs are capable of converting relatively amateurish accounts into corresponding medical concepts and \nsymptom details18. This would facilitate patients in getting a preliminary understanding of the appropriate \nmedical department to approach, assist them in promptly seeking medical attention in the relevant section, aid \ndoctors in further probing and examining pertinent indicators, mitigate doctor-patient disputes, and curtail \nthe diagnosis timeline. For example, Claude 3.5 Sonnet in this study achieved an astonishingly high accuracy \nrate of 100% when answering questions in the field of interpreting ADs reports, such as “Based on the patient’s \ncondition, what disease might the patient have and which specialty should the patient visit?“ . In contrast, two \nsenior clinicians could only reach an accuracy rate of 87%, and two junior clinicians had even lower accuracy \nrates of 73% and 67% respectively. The accuracy rates of ChatGPT 4o and Gemini 1.5 Pro were 90% and 97% \nrespectively, and their performance in report interpretation was also better than that of two clinicians, LLMs \nwith good response performance are often superior to medical expert summaries in terms of completeness and \ncorrectness, and are more concise 19. Overall, LLMs can integrate and correlate fragmented information based \non a vast amount of medical knowledge, remind doctors to focus on screening for certain diseases, and boast a \nhigh accuracy rate. Existing studies are similar to the above view. LLMs can extract information on lung lesions \nfrom clinical and medical imaging reports, assist in the research and clinical care of lung-related diseases, and \nalso have a relatively high accuracy rate20.\nVarious LLMs exhibit diverse capabilities in medical diagnosis tasks, yet several aspects such as relevance, \ncompleteness, accuracy, safety, readability, and simplicity still demand further exploration. For instance, the \nmodel-generated results might not be highly relevant to the input questions, failing to precisely meet users’ \nPro in different quality dimensions\nDimension Model\nRelevance ChatGPT 4o, Claude 3.5Sonnet, Gemini 1.5Pro\nCompleteness ChatGPT 4o, Gemini 1.5Pro\nAccuracy ChatGPT 4o, Claude 3.5Sonnet\nSafety ChatGPT 4o, Claude 3.5Sonnet, Gemini 1.5Pro\nReadability Claude 3.5Sonnet\nSimplicity Claude 3.5Sonnet\nPro in different types of ADS-related questions\nType Model\nConcept ChatGPT 4o, Claude 3.5Sonnet, Gemini 1.5Pro\nReport \ninterpretation Claude 3.5Sonnet\nDiagnosis ChatGPT 4o, Claude 3.5Sonnet\nPrevention \nand treatment ChatGPT 4o, Claude 3.5Sonnet\nPrognosis ChatGPT 4o, Claude 3.5Sonnet, Gemini 1.5Pro\nTable 1. Performance of the three models across two evaluation dimensions.\n \nScientific Reports |        (2025) 15:17635 6| https://doi.org/10.1038/s41598-025-02601-y\nwww.nature.com/scientificreports/\nrequirements. The content provided could lack completeness, missing crucial information. There may also be \noccurrences of factual inaccuracies, leading to insufficient accuracy. Additionally, security threats like data \nleakage exist, and the generated text might have poor readability, either not conforming to standard language \nexpression or being filled with redundant details. Our data reveals that when addressing professional questions \nconcerning ADs, ChatGPT 4o, Claude 3.5 sonnet, and Gemini 1.5 Pro all demonstrate favorable responses across \nseveral quality dimensions. However, disparities are present among them, with Claude 3.5 Sonnet emerging \nas the overall top performer, scoring higher than the other two LLMs in all six quality dimensions. A 2023 \nstudy likewise performed a comparative analysis to investigate the applications of ChatGPT 3.5, ChatGPT 4.0, \nand Google Bard in myopia care. ChatGPT 4.0 exhibited the highest accuracy, yet overall, the three models’ \nperformance in this area was relatively weak. Their comprehensiveness scores were comparable, and all \ndemonstrated good performance 7. Therefore, it is necessary to select an appropriate model according to the \nspecific application scenarios and requirements to better serve medical consultation work.\nTaking the common questions about hyperlipidemia as an example, there is no significant difference in \nthe responses of ChatGPT 3.5 and ChatGPT 4.0. The previous version of ChatGPT 4o, namely ChatGPT 4.0, \nprovided more concise and more readable information 21. It can be inferred that ChatGPT 4o may also have a \ngood performance when answering concept-related questions in the medical field. Our results have corroborated \nthis conjecture. Compared with the other two LLMs, ChatGPT 4o scored the highest when answering questions \nabout the concept of ADs. It is worth noting that when answering questions related to the prognosis of ADs, \namong the three LLMs in this study, only Gemini 1.5 Pro had relatively stable scores in all aspects, while the \nother two LLMs had significant fluctuations. The performance of Gemini 1.5 Pro also aligns with expectations. \nPrevious studies have found that Gemini 1.5 Pro has an excellent response in simplifying ultrasound reports \nin the medical field and has obtained high scores in terms of accuracy, consistency, comprehensibility and \nreadability22. The complexity of diseases varies among different ADs patients, and there are significant differences \nin prognosis, making it difficult to conduct accurate and comprehensive analyses. This highlights the necessity of \nensuring good human-computer interaction in disease prognosis prediction.\nOur study has several limitations. General LLMs are limited to open-source information available on the \ninternet and lack access to up-to-date or non-public resources, such as disease-specific guidelines, which may \nlead to misunderstandings in their responses. Augmenting the knowledge of LLMs using AD guidelines or \nprofessional books, so-called retrieval augmented generation (RAG), can shape and constrain LLM outputs to \nprevent false information from being propagated and disseminated. However, we did not “specialize” LLMs in \nour study23,24.\nConclusions\nLLMs are able to provide answers to ADs-related questions with specificity and safety profiles. Comparative \nanalysis reveals that the performance of the three LLMs significantly outperforms both junior and senior \ndoctors. Our findings highlight that Claude 3.5 Sonnet excels in delivering comprehensive, accurate, and well-\nstructured responses to clinical questions related to ADs. Its ability to interpret and analyze complex clinical \nissues in this field is particularly outstanding, even surpassing the expertise of both junior and senior doctors. \nThis demonstrates that LLMs, especially Claude 3.5 Sonnet, have the potential to play a crucial role in assisting \nhealthcare professionals with the diagnosis, treatment, and management of ADs, providing valuable support \nin clinical practice. The current study has certain limitations. The evaluation, based on a limited sample of \nquestions, may not fully capture the clinical complexity and diversity of autoimmune diseases. Additionally, the \nresearch design focused on Chinese-language interactions, which may restrict the applicability of the findings to \nother linguistic or cultural contexts. We plan to explore the development of hybrid models that integrate LLMs \nwith expert-curated medical knowledge bases, aiming to further enhance model performance and mitigate \nthe risk of misinformation. Moving forward, future efforts should prioritize expanding the scope of evaluation \nby incorporating a broader range of clinical cases, rare autoimmune conditions, and multicultural scenarios \nto enhance robustness. While current limitations highlight areas for improvement, future advancements \nintegrating expert knowledge and expanding multilingual validation could transform LLMs into reliable tools \nfor global healthcare. This progress aims to enhance diagnostic precision and ensure equitable access to high-\nquality medical support worldwide.\nData availability\nThe datasets used and/or analyzed during the current study are available from the corresponding author on \nreasonable request.\nReceived: 27 December 2024; Accepted: 14 May 2025\nReferences\n 1. De Angelis, L. et al. ChatGPT and the rise of large Language models: the new AI-driven infodemic threat in public health. Front. \nPublic. Health. 11, 1166120 (2023).\n 2. Lee, P ., Bubeck, S. & Petro, J. Benefits, limits, and risks of GPT-4 as an AI chatbot for medicine. N. Engl. J. Med. 388, 1233–1239 \n(2023).\n 3. Thirunavukarasu, A. J. et al. Large Language models in medicine. Nat. Med. 29, 1930–1940 (2023).\n 4. Potapenko, I. et al. Artificial intelligence-based chatbot patient information on common retinal diseases using ChatGPT. Acta \nOphthalmol. (1755375X) 101 (2023).\n 5. Antaki, F ., Touma, S., Milad, D., El-Khoury, J. & Duval, R. Evaluating the performance of ChatGPT in ophthalmology: an analysis \nof its successes and shortcomings. Ophthalmol. Sci. 3, 100324 (2023).\nScientific Reports |        (2025) 15:17635 7| https://doi.org/10.1038/s41598-025-02601-y\nwww.nature.com/scientificreports/\n 6. Huang, L. et al. Preliminary discrimination and evaluation of clinical application value of ChatGPT4o in bone tumors. J. Bone \nOncol. 48, 100632 (2024).\n 7. Lim, Z. W . et al. Benchmarking large language models’ performances for myopia care: a comparative analysis of ChatGPT-3.5, \nChatGPT-4.0, and Google Bard. EBioMedicine  95 (2023).\n 8. Edalat, C. et al. Evaluating large Language models on their accuracy and completeness: immune checkpoint inhibitors and their \nocular toxicities. RETINA 10, 1097 (2022).\n 9. do Olmo, J., Logrono, J., Mascias, C., Martinez, M. & Isla, J. Assessing DxGPT: diagnosing rare diseases with various large Language \nmodels. MedRxiv, (2024). 2024.2005. 2008.24307062.\n 10. Sun, V . H. et al. Enhancing precision in detecting severe immune-related adverse events: comparative analysis of large Language \nmodels and international classification of disease codes in patient records. J. Clin. Oncol. 42, 4134–4144 (2024).\n 11. Wang, L., Wang, F . S. & Gershwin, M. E. Human autoimmune diseases: a comprehensive update. J. Intern. Med. 278, 369–395 \n(2015).\n 12. Amanatidis, D., Chatzisavvas, G. & Dossis, M. in 7th South-East Europe Design Automation, Computer Engineering, Computer \nNetworks and Social Media Conference (SEEDA-CECNSM). 1–5 (IEEE). (2022).\n 13. Sherlock, A. & Brownie, S. Patients’ recollection and Understanding of informed consent: a literature review. ANZ J. Surg.  84, \n207–210 (2014).\n 14. Kuehn, B. M. More than one-third of US individuals use the internet to self-diagnose. Jama 309, 756–757 (2013).\n 15. Wang, X. et al. A pathology foundation model for cancer diagnosis and prognosis prediction. Nature 634, 970–978 (2024).\n 16. Zhou, J. et al. Pre-trained multimodal large Language model enhances dermatological diagnosis using SkinGPT-4. Nat. Commun. \n15, 5649 (2024).\n 17. Watad, A. et al. Autoimmunity in the elderly: insights from basic science and clinics-a mini-review. Gerontology 63, 515–523 \n(2017).\n 18. Wang, Y ., Ma, X. & Chen, W . in Findings of the Association for Computational Linguistics: EMNLP 2024. 1754–1770.\n 19. Van Veen, D. et al. Adapted large Language models can outperform medical experts in clinical text summarization. Nat. Med. 30, \n1134–1142 (2024).\n 20. Li, D., Kadav, A., Gao, A., Li, R. & Bourgon, R. Automated clinical data extraction with knowledge conditioned llms.  Preprint at \nhttps://arXiv.org/abs/2406.18027 (2024).\n 21. Lee, T. J. et al. Evaluating ChatGPT-3.5 and ChatGPT-4.0 responses on hyperlipidemia for patient education. Cureus 16 (2024).\n 22. Güneş, Y . C., Cesur, T. & Çamur, E. Comparative analysis of large Language models in simplifying Turkish ultrasound reports to \nenhance patient Understanding. Eur. J. Ther. 30, 714–723 (2024).\n 23. Humphrey, M. B. et al. 2022 American college of rheumatology guideline for the prevention and treatment of Glucocorticoid-\nInduced osteoporosis. Arthritis Rheumatol. 75, 2088–2102 (2023).\n 24. Price, E. J. et al. British society for rheumatology guideline on management of adult and juvenile onset Sjögren disease. \nRheumatology, keae152 (2024).\nAuthor contributions\nJuntao Ma: Formal analysis, Writing – original draft, Writing – review & editing. Jie Yu: Writing – original \ndraft, Writing – review & editing. Anran Xie: Writing – original draft, Data curation. Taihong Huang: Scoring. \nWenjing Liu: Scoring. Mengyin Ma: scoring. Yue Tao: Scoring. Fuyu Zang: Scoring. Qisi Zheng: Scoring. Wen-\nbo Zhu: Scoring. Yijia Zhu: Conceptualization, Methodology, Supervision. Mingzhe Ning: Conceptualization, \nMethodology, Data curation, Supervision, Validation, Writing – review & editing, Funding acquisition.Yuxin \nChen: Conceptualization, Methodology, Resources, Data curation, Supervision, Validation, Writing – review & \nediting, Funding acquisition.\nFunding\nThis work was supported by the National key research and development program [2023YFC2309100]; Nation-\nal Natural Science Foundation of China [92269118,92269205,92369117]; Scientific Research Project of Jiangsu \nHealth Commission [M2022013]; Clinical Trials from the Affiliated Drum Tower Hospital, Medical School \nof Nanjing University [2021-LCYJ-PY-10]; Project of Chinese Hospital Reform and Development Institute, \nNanjing University, Aid project of Nanjing Drum Tower Hospital Health, Education &Research Foundation \n[NDYG2022003].\nDeclarations\nCompeting interests\nThe authors declare no competing interests.\nEthical considerations\nAll experimental protocols for this project were approved by the Ethics Committee of Nanjing Drum Tower \nHospital, with the approval number 2021 − 348.\nConsent to participate\nDuring the research process, we ensured that informed consent was obtained from all subjects and/or their \nlegal guardians (where applicable) to safeguard the rights and privacy of the subjects.\nAdditional information\nSupplementary Information The online version contains supplementary material available at  h t t p s : / / d o i . o r g / 1 \n0 . 1 0 3 8 / s 4 1 5 9 8 - 0 2 5 - 0 2 6 0 1 - y     .  \nCorrespondence and requests for materials should be addressed to Y .C., M.N. or Y .Z.\nReprints and permissions information is available at www.nature.com/reprints.\nScientific Reports |        (2025) 15:17635 8| https://doi.org/10.1038/s41598-025-02601-y\nwww.nature.com/scientificreports/\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives \n4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in \nany medium or format, as long as you give appropriate credit to the original author(s) and the source, provide \na link to the Creative Commons licence, and indicate if you modified the licensed material. Y ou do not have \npermission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence \nand your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to \nobtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p : / / c r e a t i v e c o m m o \nn s . o r g / l i c e n s e s / b y - n c - n d / 4 . 0 /     .  \n© The Author(s) 2025 \nScientific Reports |        (2025) 15:17635 9| https://doi.org/10.1038/s41598-025-02601-y\nwww.nature.com/scientificreports/",
  "topic": "Sonnet",
  "concepts": [
    {
      "name": "Sonnet",
      "score": 0.6894616484642029
    },
    {
      "name": "Disease",
      "score": 0.48063698410987854
    },
    {
      "name": "Computer science",
      "score": 0.39030155539512634
    },
    {
      "name": "Computational biology",
      "score": 0.37425366044044495
    },
    {
      "name": "Medicine",
      "score": 0.36455750465393066
    },
    {
      "name": "Natural language processing",
      "score": 0.3261408805847168
    },
    {
      "name": "Biology",
      "score": 0.24506068229675293
    },
    {
      "name": "Linguistics",
      "score": 0.23199307918548584
    },
    {
      "name": "Philosophy",
      "score": 0.18305161595344543
    },
    {
      "name": "Internal medicine",
      "score": 0.18218612670898438
    },
    {
      "name": "Poetry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210111013",
      "name": "Nanjing Drum Tower Hospital",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I131434179",
      "name": "Nanjing University of Chinese Medicine",
      "country": "CN"
    }
  ],
  "cited_by": 1
}