{
  "title": "Medical Semantic Similarity with a Neural Language Model",
  "url": "https://openalex.org/W1485069053",
  "year": 2014,
  "authors": [
    {
      "id": "https://openalex.org/A2152283292",
      "name": "Lance De Vine",
      "affiliations": [
        "Queensland University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1551779932",
      "name": "Guido Zuccon",
      "affiliations": [
        "Queensland University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2154133250",
      "name": "Bevan Koopman",
      "affiliations": [
        "Commonwealth Scientific and Industrial Research Organisation",
        "Queensland University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A279890580",
      "name": "Laurianne Sitbon",
      "affiliations": [
        "Queensland University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2101395307",
      "name": "Peter Bruza",
      "affiliations": [
        "Queensland University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1987180264",
    "https://openalex.org/W2122402213",
    "https://openalex.org/W2251803266",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W1988624983",
    "https://openalex.org/W2076766778",
    "https://openalex.org/W2117130368",
    "https://openalex.org/W6753056052",
    "https://openalex.org/W2032472148",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W6675844511",
    "https://openalex.org/W2084377579",
    "https://openalex.org/W2437096199",
    "https://openalex.org/W6697469132"
  ],
  "abstract": "Advances in neural network language models have demonstrated that these models can effectively learn representations of words meaning. In this paper, we explore a variation of neural language models that can learn on concepts taken from structured ontologies and extracted from free-text, rather than directly from terms in free-text. This model is employed for the task of measuring semantic similarity between medical concepts, a task that is central to a number of techniques in medical informatics and information retrieval. The model is built with two medical corpora (journal abstracts and patient records) and empirically validated on two ground-truth datasets of human-judged concept pairs assessed by medical professionals. Empirically, our approach correlates closely with expert human assessors ($\\\\approx$ 0.9) and outperforms a number of state-of-the-art benchmarks for medical semantic similarity. The demonstrated superiority of this model for providing an effective semantic similarity measure is promising in that this may translate into effectiveness gains for techniques in medical information retrieval and medical informatics (e.g., query expansion and literature-based discovery).",
  "full_text": "This may be the author’s version of a work that was submitted/accepted\nfor publication in the following source:\nDe Vine, Lance, Zuccon, Guido, Koopman, Bevan, Sitbon, Laurianne, &\nBruza, Peter\n(2014)\nMedical semantic similarity with a neural language model.\nIn Suel, T, Wang, M, Soboroff, I, & Garofalakis, M (Eds.) Proceedings of\nthe 23rd ACM International Conference on Conference on Information and\nKnowledge Management.\nAssociation for Computing Machinery, United States of America, pp. 1819-\n1822.\nThis ﬁle was downloaded from: https://eprints.qut.edu.au/83704/\n© Consult author(s) regarding copyright matters\nThis work is covered by copyright. Unless the document is being made available under a\nCreative Commons Licence, you must assume that re-use is limited to personal use and\nthat permission from the copyright owner must be obtained for all other uses. If the docu-\nment is available under a Creative Commons License (or other speciﬁed license) then refer\nto the Licence for details of permitted re-use. It is a condition of access that users recog-\nnise and abide by the legal requirements associated with these rights. If you believe that\nthis work infringes copyright please provide details by email to qut.copyright@qut.edu.au\nNotice: Please note that this document may not be the Version of Record\n(i.e. published version) of the work. Author manuscript versions (as Sub-\nmitted for peer review or as Accepted for publication after peer review) can\nbe identiﬁed by an absence of publisher branding and/or typeset appear-\nance. If there is any doubt, please refer to the published source.\nhttps://doi.org/10.1145/2661829.2661974\nMedical Semantic Similarity with a Neural Language Model\nLance De Vine1, Guido Zuccon2, Bevan Koopman3,2, Laurianne Sitbon1, Peter Bruza2\n1Electrical Engineering & Computer Science, Queensland University of Technology, Brisbane, Australia\n2Information Systems, Queensland University of Technology, Brisbane, Australia\n3Australian e-Health Research Centre, CSIRO, Brisbane, Australia\nl.devine@student.qut.edu.au, g.zuccon@qut.edu.au, bevan.koopman@csiro.au,\nlaurianne.sitbon@qut.edu.au, p.bruza@qut.edu.au\nABSTRACT\nAdvances in neural network language models have demon-\nstrated that these models can eﬀectively learn representa-\ntions of words meaning. In this paper, we explore a varia-\ntion of neural language models that can learn on concepts\ntaken from structured ontologies and extracted from free-\ntext, rather than directly from terms in free-text.\nThis model is employed for the task of measuring semantic\nsimilarity between medical concepts, a task that is central to\na number of techniques in medical informatics and informa-\ntion retrieval. The model is built with two medical corpora\n(journal abstracts and patient records) and empirically val-\nidated on two ground-truth datasets of human-judged con-\ncept pairs assessed by medical professionals. Empirically,\nour approach correlates closely with expert human assessors\n(≈0.9) and outperforms a number of state-of-the-art bench-\nmarks for medical semantic similarity.\nThe demonstrated superiority of this model for providing\nan eﬀective semantic similarity measure is promising in that\nthis may translate into eﬀectiveness gains for techniques in\nmedical information retrieval and medical informatics (e.g.,\nquery expansion and literature-based discovery).\nCategories and Subject Descriptors: H.3.3 [Informa-\ntion Storage and Retrieval]\nGeneral Terms: Theory, Experimentation, Measurement\nKeywords: Neural Language Model; Skip-gram; Distributed\nRepresentations; Word2Vec; Semantic Similarity; Medical\nInformation Retrieval.\n1. INTRODUCTION\nA variety of neural network-based methods have emerged\nas eﬀective approaches for generating representations of words [4,\n16, 12]; these are referred to as neural language models.\nThese methods learn word embeddings based on the opti-\nmisation of an objective function. The term “word embed-\ndings” generally refers to representations for words occupy-\ning a real valued vector space where the similarity between\nThis is a pre-print of the conference paper submitted to CIKM 2014\n.\nwords is measured by cosine similarity. An objective func-\ntion that is often used for training word embeddings is to\nlearn a vector for a target word which predicts the vectors\nfor words occurring near to it (Skip-gram).\nRecent research has demonstrated that neural language\nmodels (NLM) based on the continuous Skip-gram model\nproposed by Mikolov et al. [11] are highly eﬀective in deter-\nmining semantic relationships between words [13]. It is still\nnot clear, however, whether these neurally inspired models\nare better than traditional distributional semantic methods.\nFor example, Lebret et. al. [10] report results that suggest\nthat computing a Hellinger PCA of a word co-occurence\nmatrix provides similar results to neural network models on\nnatural language processing tasks. On the other hand, Ba-\nroni et. al [3] report on comparisons between standard dis-\ntributional semantic models and neural network models and\nconclude that neural network models do indeed provide su-\nperior word representations. They note however that not all\nneural network word models are equal.\nSemantic similarity measures are central to several tech-\nniques used in health informatics and medical information\nretrieval, e.g., query expansion [6] and literature-based dis-\ncovery [1]. A number of previous corpus-based approaches\nhave been employed for semantic similarity measurements\nand have been evaluated by how well they correlate with\nhuman-judged similarity [15, 9]. These approaches were ap-\nplied to medical concepts taken from the UMLS medical\nthesaurus and extracted from medical free-text. The results\nfrom these studies show that although corpus-based mea-\nsures of similarity do correlate with human judgments, there\nis considerable room for improvement. Motivated by this\nand the recent ﬁndings in neural language models, we ex-\nplore a variation to the original continuous Skip-gram NLM\nof Mikolov et al. [11], where instead of learning a distributed\nvector representation over sequences of terms, we train the\nmodel over sequences of UMLS medical concepts. This ap-\nproach is evaluated over two human-judged semantic simi-\nlarity datasets and is trained using two corpora: a collec-\ntion of clinical records and a large set of MEDLINE medical\njournal abstracts. The empirical results of this study demon-\nstrate that the proposed neural language models outperform\na number of benchmark corpus-based approaches, strongly\ncorrelating with semantic similarity judgements provided by\nmedical, expert judges.\n2. SKIP-GRAM NEURAL LANGUAGE MODEL\nThe eﬀectiveness of corpus-driven approaches relies on the\ndistributional hypothesis [8, 14], which states that the degree\nof semantic similarity between two terms (or some other\nlinguistic units) can be modelled as a function of the degree\nof overlap of their linguistic contexts. In practice, the counts\nof contextual features are generally accumulated into a term-\ncontext matrix and a transformation is then applied which\nre-weights the accumulated counts.\nNeural language models also construct representations for\nterms based on linguistic contexts; however, they do so by\noptimising an objective function involving the target term\nand its linguistic context. The representations produced are\noften called“word embeddings”. Word embeddings were ﬁrst\ndeveloped in the context of language modelling to overcome\nsome of the well known problems relating to data spar-\nsity that existed with n-gram based language models [4].\nWhile NLMs were originally developed to model sequential\nterm dependencies departing from the n-gram approach, a\nby-product of these models is that the constructed word\nrepresentations were found to have useful semantic prop-\nerties [11]. NLMs have more recently been employed for\na large variety of natural language processing tasks, such\nas semantic role labelling, part-of-speech tagging, chunking,\nsentiment analysis and named entity recognition [7, 13]; they\nwere found to be as good as, or better than, other state-of-\nthe-art methods.\nA particular instance of a NLM is the continuous Skip-\ngram model of Mikolov et al. [11]. The Skip-gram model\nconstructs term representations by optimising their ability\nto predict the representations of surrounding terms. In this\npaper, we evaluate the continuous Skip-gram model on the\ntask of predicting the semantic similarity of concept pairs.\nWe employ the Skip-gram model in a way not previously\nseen in the literature; speciﬁcally, we use it to learn embed-\nding vectors for concepts taken from structured ontologies\nrather than for terms. While previous work has considered\nthe use of compound terms (e.g., named entities) in NLMs\n[13], these compound terms are not actually used as features;\nin addition, ontology concepts have not been used (to our\nknowledge).\nGiven a sequence W = {w1, . . . , wt, . . . , wn}of training\nwords, the objective of the Skip-gram model is to maximise\nthe following average log probability\n1\n2r\n2r∑\ni=1\n∑\n−r≤j≤r,j̸=0\nlog p(wt+j|wt) (1)\nwhere r is the context window radius. The context window\nradius determines which words surrounding the target term\nwt are considered for the computation of the log probability;\nthe window is centred around the target term. The proba-\nbility of an output word is computed according to\np(wO|wI) = exp(v⊤\nwOvwI )\n∑W\nw=1 exp(vw⊤, vwI )\n(2)\nwhere the vwI and vwO are the vector representations of the\ninput and output vectors, respectively, and∑W\nw=1 exp(v⊤\nw , vwI )\nis the normalisation factor, whose role is to normalise the\ninner product results across all vocabulary words ( W is the\nvocabulary size). In practice, a hierarchical approximation\nto this probability is used to reduce computational complex-\nity [11]. At initialisation, the vector representations of the\nwords are assigned random values; these vector representa-\ntions are then optimised using gradient descent with decay-\ning learning rate by iterating through sentences observed in\nthe training corpus.\nFigure 1: Skip-gram Neural Language Model ap-\nplied to sequences of UMLS concept identiﬁers. In\nthis example, the context radius r is set to 2.\nIn this paper, we explore a variation of the described Skip-\ngram NLM, where sequences of terms are substituted with\nsequences of UMLS concept identiﬁers. Thus, in practice,\ntraining is performed by iterating through sequences of con-\ncepts as shown in Figure 1. This method builds representa-\ntions of concepts that are predictive of nearby concepts. It\nis this feature that, we hypothesise, would enhance semantic\nsimilarity measurements between medical concepts.\n3. EXPERIMENT SETTINGS\n3.1 Corpora and Human-judged Datasets\nIn this paper we adopted the evaluation framework setup\nby Koopman et al. [9], who empirically evaluated a number\nof diﬀerent corpus-driven measures of semantic similarities\nfor medical concepts. We refer to that work for details about\nthe evaluation framework that are not reported in this paper.\nThe evaluation framework comprised of two datasets:\n•Ped: 29 UMLS medical concept pairs developed by Ped-\nersen et al. [15]. Semantic similarity judgements were\nprovided by 3 physician and 9 clinical terminologists,\nwith an inter-coder correlation of 0.85.\n•Cav: 45 MeSH/UMLS concept pairs developed by Ca-\nvides and Cimino [5]. Similarity between concept pairs\nwas judged by 3 physicians, with no exact consensus\nvalue reported by Cavides and Cimino.\nIn addition, two corpora were used in the evaluation frame-\nwork for learning concept representations:\n•MedTrack: a collection of 17,198 clinical patient records\nused in the TREC 2011 and 2012 Medical Records Track [17].\nAverage document length was 932 tokens (words).\n•OHSUMED: a collection of 348,566 MEDLINE medical\njournal abstracts used in TREC 2000 Filtering Track.\nAverage document length was 100 tokens (words).\nWe speciﬁcally focus on two corpora because previous\nwork has found that the eﬀectiveness of corpus-based mea-\nsures is inﬂuenced by corpus characteristics [9]. In partic-\nular, previous work has found that measures suited to the\ncharacteristics of one corpus are often not suited to those\nof another corpus, yielding signiﬁcant diﬀerences in perfor-\nmance across corpora.\nIn accordance with [9], documents in both corpora where\npre-processed using MetaMap v11.2, a state-of-the-art biomed-\nical concept identiﬁcation system [2], which converted the\nfree-text into sequences of UMLS concept identiﬁers. Con-\nverting the test corpora to concepts allowed for direct com-\nparison of the concept pairs contained in both the Ped and\nNLM−400−10\nNLM−200−10\nNLM−200−5\nNLM−400−5\nNLM−100−10\nNLM−100−5\nNLM−200−2\nNLM−400−2\nNLM−100−2\n+PMI\nDocCosine\nRI500\nCER\nLM (JSD)\nLSA50\nCorrelation coefficient against human judges\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n● ● ● ● ●\n● ● ● ● ●\n● ● ●\n●\n●\n●MedTrack (Ped) OHSUMED (Ped) MedTrack (Cav) OHSUMED (Cav) Mean\nFigure 2: Pearson correlation coeﬃcient against expert judged semantic similarity for the NLM and bench-\nmark comparison methods. Correlations are computed for two gold standard datasets ( Ped & Cav) using\ntwo corpora (MedTrack & OHSUMED). Methods are ordered from left to right by decreasing correlation\naveraged across all datasets/corpora, which is summarised by the trendline. Error bars for points in the\ntrendline signify conﬁdence intervals at 95% for the mean correlation value.\nCav datasets. Three concepts appearing in the Ped dataset\nwere not found in the translated corpora (two in Medtrack\nand one in OHSUMED) and were, therefore, removed. 1\n3.2 Benchmark Comparison Methods\nA number of other corpus-based measures of semantic sim-\nilarity were included as benchmarks for comparison against\nthe neural language model approach:\n1. Random Indexing (RI)\n2. Latent Semantic Analysis (LSA)\n3. Document Vector Cosine Similarity (DocCosine)\n4. Positive Pointwise Mutual Information (+PMI)\n5. Cross Entropy Reduction (CER)\n6. Language Model + Jensen-Shannon divergence (LM\nJSD)\nA previous evaluation of the above models on the same task\nfound that these were the most eﬀective in terms of correla-\ntions with human judges [9]. We refer the reader to [9] for\na description of each method.\n3.3 Parameters Settings\nFor the benchmark comparison methods (e.g., RI and\nLSA) we selected the parameter settings, e.g., latent space\ndimensionality, that produced the highest correlations with\nhuman experts as reported in previous work [9] 2.\nFor the Skip-gram NLM, we adopted the word2vec im-\nplementation provided by Mikolov et al. [11] 3. We used the\nhierarchical soft-max classiﬁcation layer and set the “min-\ncount” parameter to 1, thus eﬀectively not excluding any\nconcept occurrence from the computation of statistics. Each\ncorpora was processed using only one thread so that process-\ning was purely sequential. We studied the eﬀect of window\n1Removed concepts were C0702166, C0224701, C0029456.\n2Tested dimensionalities: 50, 150, 300 and 500.\n3http://word2vec.googlecode.com/.\nradius and embedding dimensionality (i.e. the dimensional-\nity of the reduced space) on semantic similarity by consid-\nering 2, 5 and 10 as window radius and 100, 200 and 400\nas latent dimensions; these are values typical of the range\ngenerally reported in the NLM literature [7, 11, 13].\n4. RESULTS & DISCUSSION\nResults showing the Pearson correlation coeﬃcient against\nhuman judges for each semantic similarity method are re-\nported in Figure 2. The methods on the x-axis are ordered\nfrom left to right in decreasing correlation averaged across all\ndatasets/corpora: the leftmost method exhibited the highest\noverall correlation with human experts. Signiﬁcance inter-\nvals are also reported for the mean correlation values.\nAccording to the empirical results reported in Figure 2,\nthe mean correlation between diﬀerent settings of the Skip-\ngram neural language model provides overall higher correla-\ntions with human assessed semantic similarity than the other\nbenchmark methods. In particular, the NLM approach is\nfound to consistently outperform the benchmarks for all but\none datasets-corpora combinations, with DocCosine and RI\nproviding stronger correlations with human experts in the\nPed dataset when trained with the Medtrack corpus.\nWhen MedTrack is used to train the methods, the correla-\ntion between NLM semantic similarity estimations and the\nexpert assessments for the Ped dataset is less strong than\nthat obtained by the DocCosine and RI benchmarks. This\nmay suggest that NLM does not appropriately use evidence\nencoded in the Medtrack corpus to construct eﬀective con-\ncept representations. However, this is not conﬁrmed when\nanalysing the results on the Cav dataset: in the latter case\nNLM is found to strongly correlate with expert assessments\nwhen using MedTrack. Previous work has found that there\nis no single method that does consistently outperform any\nother method across all datasets-corpora combinations con-\nsidered in this evaluation framework: it was the choice of\ncorpora used to prime the measures that aﬀected their per-\nformance [9]. While NLM does not provide strong correla-\ntions on Ped when using Medtrack, the use of this corpus\ndoes not seem to detriment NLM’s performance when con-\nsidering the Cav dataset.\nWe now consider how window radius and embedding di-\nmensionality aﬀect performance of the studied NLM. We\nfound that the best performing model was the Skip-gram\nmodel with the largest dimensionality and window radius.\nOverall we found that increasing both the embedding dimen-\nsionality and the window radius helped to improve perfor-\nmance, with larger window radius contributing more than\nlarger dimensionalities. While not true in every case, the\noverall trend suggests as a guideline for building NLM mod-\nels for this tasks, that vectors with larger window radius and\nlarger embedding dimensionality should be used.\nThe empirical results highlight that the investigated Skip-\ngram NLM constructs representations for concepts that, when\nused as a measure of semantic relations, strongly correlate\nwith semantic similarity judgements provided by medical\nexperts. We conjecture that the predictive nature of the\nobjective function used by the considered Skip-gram NLM\nis the core feature that produces such strong performance.\nThe validation of this intriguing conjecture would require\nfurther investigation; this is left for future work.\n5. CONCLUSIONS\nNeural network language models (NLM) have recently at-\ntracted attention because of promising results obtained in a\nnumber of natural language processing tasks, e.g., semantic\nrole labelling and sentiment analysis, among others. The in-\ntuition behind these models is that eﬀective representations\nthat synthesise word meaning can be learnt by iteratively ob-\nserving word occurrences in the close surroundings of target\nwords along with the optimisation of a task-speciﬁc function.\nIn this paper, we have explored a variation of a speciﬁc\nNLM approach, the Skip-gram model, applied to the task of\nmeasuring the semantic similarity between medical concepts.\nWhile the traditional Skip-gram model creates distributed\nvector representations of words, the model in this study\nleverages distributed representations of UMLS concepts ex-\ntracted from medical corpora, including clinical records and\nmedical journal abstracts.\nEmpirical ﬁndings demonstrate that the concept-based\nSkip-gram NLM correlates more strongly to expert judge-\nment of semantic similarity than established benchmark ap-\nproaches. Window radius in primis, along with embedding\ndimensionality, are factors that inﬂuence performance, with\nrepresentations learnt with larger radius and dimensionali-\nties more strongly correlating with expert judgements.\nThis work opens up a number of avenues for future re-\nsearch. One important research question is why the predic-\ntive nature of the objective function used by the Skip-gram\nNLM is conducive of such strong performance. We also con-\njecture that the use of “mixed” features, e.g., learning repre-\nsentations from both term and concept corpora, may result\nin further improvements. Another factor that may inﬂuence\nperformance is the ordering of the training data, consider-\ning that the importance of data samples varies according to\nthe learning rate parameter included in the gradient descent\nprocedure.\n6. REFERENCES\n[1] P. Agarwal and D. B. Searls. Can literature analysis\nidentify innovation drivers in drug discovery? Nature\nreviews. Drug discovery, 8(11):865–78, Nov. 2009.\n[2] A. R. Aronson and F.-M. Lang. An overview of\nMetaMap: historical perspective and recent advances.\nJAMIA, 17(3):229–236, 2010.\n[3] M. Baroni, G. Dinu, and G. Kruszewski. Don’t count,\npredict! a systematic comparison of context-counting\nvs. context-predicting semantic vectors. In Proceedings\nof the 52nd Annual Meeting of the Association for\nComputational Linguistics, 2014.\n[4] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin.\nA neural probabilistic language model. Journal of\nMachine Learning Research, 3:1137–1155, 2003.\n[5] J. E. Caviedes and J. J. Cimino. Towards the\ndevelopment of a conceptual distance metric for the\nUMLS. Journal of biomedical informatics,\n37(2):77–85, Apr. 2004.\n[6] T. Cohen and D. Widdows. Empirical distributional\nsemantics: Methods and biomedical applications.\nJournal of Biomedical Informatics, 42(2):390–405,\n2009.\n[7] R. Collobert and J. Weston. A uniﬁed architecture for\nnatural language processing: Deep neural networks\nwith multitask learning. In Proceedings of the 25th\ninternational conference on Machine learning, pages\n160–167. ACM, 2008.\n[8] Z. S. Harris. Distributional structure. Word, 1954.\n[9] B. Koopman, G. Zuccon, P. Bruza, L. Sitbon, and\nM. Lawley. An evaluation of corpus-driven measures of\nmedical concept similarity for information retrieval. In\nProceedings of the 21st ACM international conference\non Information and knowledge management, pages\n2439–2442. ACM, 2012.\n[10] R. Lebret, J. Legrand, and R. Collobert. Is deep\nlearning really necessary for word embeddings? In\nNIPS Workshop on Deep Learning, 2013.\n[11] T. Mikolov, K. Chen, G. Corrado, and J. Dean.\nEﬃcient estimation of word representations in vector\nspace. arXiv preprint arXiv:1301.3781, 2013.\n[12] T. Mikolov, M. Karaﬁ´ at, L. Burget, J. Cernock` y, and\nS. Khudanpur. Recurrent neural network based\nlanguage model. In INTERSPEECH, pages\n1045–1048, 2010.\n[13] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and\nJ. Dean. Distributed representations of words and\nphrases and their compositionality. In Advances in\nNeural Information Processing Systems, pages\n3111–3119, 2013.\n[14] G. A. Miller and W. G. Charles. Contextual correlates\nof semantic similarity. Language and cognitive\nprocesses, 6(1):1–28, 1991.\n[15] T. Pedersen, S. Pakhomov, S. Patwardhan, and\nC. Chute. Measures of semantic similarity and\nrelatedness in the biomedical domain. Journal of\nBiomedical Informatics, 40(3):288–299, 2007.\n[16] H. Schwenk and J.-L. Gauvain. Training neural\nnetwork language models on very large corpora. In\nProceedings of the conference on Human Language\nTechnology and Empirical Methods in Natural\nLanguage Processing, pages 201–208. Association for\nComputational Linguistics, 2005.\n[17] E. Voorhees and R. Tong. Overview of the TREC\nMedical Records Track. In Twentieth Text REtrieval\nConference (TREC 2011), MD, USA, 2011.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8024189472198486
    },
    {
      "name": "Semantic similarity",
      "score": 0.7568947672843933
    },
    {
      "name": "Natural language processing",
      "score": 0.7025517821311951
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.6480993032455444
    },
    {
      "name": "Information retrieval",
      "score": 0.6364301443099976
    },
    {
      "name": "Task (project management)",
      "score": 0.622280478477478
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6189507246017456
    },
    {
      "name": "Unified Medical Language System",
      "score": 0.5602930784225464
    },
    {
      "name": "Meaning (existential)",
      "score": 0.5348443388938904
    },
    {
      "name": "Language model",
      "score": 0.4400203824043274
    },
    {
      "name": "Artificial neural network",
      "score": 0.4371680021286011
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Psychology",
      "score": 0.0
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    }
  ]
}