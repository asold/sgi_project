{
  "title": "Low-Resource Language Modelling of South African Languages",
  "url": "https://openalex.org/W3139663946",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4320857332",
      "name": "Mesham, Stuart",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Hayward, Luc",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Shapiro, Jared",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287445014",
      "name": "Buys, Jan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2560082712",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W4919037",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2571859396",
    "https://openalex.org/W1591801644",
    "https://openalex.org/W2963045354",
    "https://openalex.org/W2514728809",
    "https://openalex.org/W2888799392",
    "https://openalex.org/W2742448943",
    "https://openalex.org/W2962832505",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2792376130",
    "https://openalex.org/W2963357986",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W1995875735",
    "https://openalex.org/W2553397501",
    "https://openalex.org/W3100075909",
    "https://openalex.org/W2158195707",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W1992306964",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2251689968",
    "https://openalex.org/W2964091467",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W46679369",
    "https://openalex.org/W2963266340",
    "https://openalex.org/W1985057352"
  ],
  "abstract": "Language models are the foundation of current neural network-based models for natural language understanding and generation. However, research on the intrinsic performance of language models on African languages has been extremely limited, which is made more challenging by the lack of large or standardised training and evaluation sets that exist for English and other high-resource languages. In this paper, we evaluate the performance of open-vocabulary language models on low-resource South African languages, using byte-pair encoding to handle the rich morphology of these languages. We evaluate different variants of n-gram models, feedforward neural networks, recurrent neural networks (RNNs), and Transformers on small-scale datasets. Overall, well-regularized RNNs give the best performance across two isiZulu and one Sepedi datasets. Multilingual training further improves performance on these datasets. We hope that this research will open new avenues for research into multilingual and low-resource language modelling for African languages.",
  "full_text": "Low-Resource Language Modelling of South African Languages\nStuart Mesham Luc Hayward Jared Shapiro Jan Buys\nDepartment of Computer Science\nUniversity of Cape Town, South Africa\n{MSHSTU001,HYWLUC001,SHPJAR002}@myuct.ac.za,jbuys@cs.uct.ac.za\nAbstract\nLanguage models are the foundation of cur-\nrent neural network-based models for natural\nlanguage understanding and generation. How-\never, research on the intrinsic performance of\nlanguage models on African languages has\nbeen extremely limited, which is made more\nchallenging by the lack of large or standard-\nised training and evaluation sets that exist\nfor English and other high-resource languages.\nIn this paper, we evaluate the performance\nof open-vocabulary language models on low-\nresource South African languages, using byte-\npair encoding to handle the rich morphology\nof these languages. We evaluate different vari-\nants of n-gram models, feedforward neural net-\nworks, recurrent neural networks (RNNs), and\nTransformers on small-scale datasets. Over-\nall, well-regularized RNNs give the best per-\nformance across two isiZulu and one Sepedi\ndatasets. Multilingual training further improve\nperformance on these datasets. We hope that\nthis research will open new avenues for re-\nsearch into multilingual and low-resource lan-\nguage modelling for African languages.\n1 Introduction\nLanguage modelling has applications in many ar-\neas of NLP including machine translation, infor-\nmation retrieval, voice recognition and question\nanswering (Wu et al., 2016; Franz and Milch, 2002;\nChavula and Suleman, 2016; Ndaba et al., 2016;\nKumar et al., 2016). Improvements in language\nmodelling have resulted in improved model perfor-\nmance in the above tasks, making language mod-\nelling a valuable area of study. High resource lan-\nguages have enjoyed substantial improvements in\nlanguage modelling performance in recent years\ndue to large neural models such as GPT-2, BERT\nand XLNet (Radford et al., 2019; Devlin et al.,\n2019; Yang et al., 2019). However, most African\nlanguages are low-resource, and the limited avail-\nUbusuku obuhle namaphupho amamnandi!Ubu_sukuobu _hle nama _phupho ama _mnandi !Robalang gabotseR _o _ba _la _nggabotse\nFigure 1: Example sentences and their BPE tokeniza-\ntions in isiZulu (top) and Sepedi (bottom). The tok-\nenizers use BPE vocabulary sizes of 8000 and 2000 re-\nspectively.\nability of high-quality training data makes training\nlarge language models challenging.\nIn this paper we focus on South African Benue-\nCongo languages, which are more resourced than\nmost other Benue-Congo languages, but still\nclearly low-resourced.1 The two groups of South\nAfrican languages with the largest number of total\nspeakers are the Nguni and Sotho-Tswana groups\nof closely-related languages. In South Africa these\nlanguages represent 43.3% and 24.7% of speakers\nrespectively (Africa, 2012). In our data sources,\nthe isiZulu and Sepedi languages had the largest\namounts of text available, respectively, within these\nlanguage groups.\nIn addition to the lack of large amounts of high\nquality data, Benue-Congo languages are typologi-\ncally2 very different from the Indo-European lan-\nguages most widely studied for language modelling.\nEven in large multilingual studies, African lan-\nguages are usually underrepresented if included at\nall. Benue-Congo languages are agglutinative and\nmorphologically rich (Pretorius and Bosch, 2009):\nMost words are made up by combination of smaller\nmorphological units, grammatical relations (such\n1The Benue-Congo languages is a subdivision of the Niger-\nCongo language family. Most Benue-Congo languages are\npart of what linguists refer to as the Bantu or Bantoid sub-\nfamilies.\n2Typology refers to the linguistic properties and character-\nization of a language.\narXiv:2104.00772v1  [cs.CL]  1 Apr 2021\nas subject or object) are indicated by changes in\nthe words rather than the relative position of words\nin the sentence, and all nouns belong to one of a\nlarge number of noun classes which governs the\nchoice of many morphemes. This leads to poten-\ntially very large and sparse word-level vocabulary,\neven though individual morphemes or sub-words\nmany be more frequent in a corpus (as they are\nused in many different words).\nThis paper examines the application of n-gram\nmodels (Chen and Goodman, 1999), Feed-forward\nneural networks (FFNNs) (Bengio et al., 2003),\nRecurrent neural networks including Long Short\nTerm Memory (LSTMs; Hochreiter and Schmidhu-\nber, 1997) and Transformer (Vaswani et al., 2017)\nmodels on isiZulu and Sepedi. We use byte pair en-\ncoding (BPE; Sennrich et al., 2016) to control the\nvocabulary size and to enable open-vocabulary lan-\nguage modelling (see Figure 1), making the choice\nof vocabulary size a hyperparameter of the models.\nOur results show that the relative performance\nof the different model classes is similar to what\nhave been found in previous work on small-scale\nlanguage modelling in English and other languages.\nWell-regularized RNNs, the AWD-LSTM (Merity\net al., 2017) and QRNN (Bradbury et al., 2017),\nhave the best overall performance, outperforming\nthe Transformer. The n-gram, FFNN and baseline\nLSTM models performed worse across all datasets.\nWe also perform an evaluation of multilingual train-\ning, showing that training on text from multiple\nrelated languages improves performance without\nany modiﬁcations to the model architecture. The\nbeneﬁts can be seen using text from either the same\nlanguage group or a different but related language\ngroup, despite orthographic differences. Code and\ntrained models can be found at https://github.\ncom/StuartMesham/low_resource_lm.\n2 Background\nA language model assigns a probability P(Wn\n1 )\nto a sequence of n words Wn\n1 = w1, ..., wn. The\nprobability is usually decomposed using the chain\nrule to predict the words one at a time (from left to\nright) by assigning a probability to each word for\nfollowing the given context (Jurafsky and Martin,\n2020):\nP(Wn\n1 ) =\nn∏\nk=2\nP(wk|Wk−1\n1 ). (1)\n2.1 Sub-word Tokenization\nLanguage models traditionally estimate the next\nword probability as a distribution over a ﬁxed vo-\ncabulary, where the input text has been tokenized\ninto words, and all words outside the vocabulary\nreplaced with a special unknown token. South\nAfrican Benue-Congo languages are highly ag-\nglutinative, making whole-word tokenization sub-\noptimal for language modelling due to potentially\nlarge vocabulary sizes and subsequent data sparsity.\nIn contrast, character-level tokenization requires\nthe model to learn to model very long sequences.\nTo better represent the structure of the languages,\nwe use byte-pair encoding (Gage, 1994; Sennrich\net al., 2016) to break words into sub-word units\nbased on their frequency. Language modelling with\nBPE has previously been shown to perform com-\npetitively for open-vocabulary language modelling\n(Mielke and Eisner, 2019).\nByte-pair encoding is a compression algorithm\nwhich has been adapted for sub-word tokenization.\nThe algorithm starts with character-level tokens and\nﬁnds pairs of adjacent tokens which occur most\nfrequently. These token pairs are replaced with\nsingle tokens containing the concatenation of the\ncharacters in each token. This process is repeated\nuntil a desired vocabulary size is reached (Sennrich\net al., 2016). To ensure fair model evaluation, we\ntrain BPE tokenizers using only the training sets.\nExample BPE tokenizations in isiZulu and Sepedi\nare shown in Figure 1.\n2.2 Evaluation\nThe quality of a language model can be evaluated\neither extrinsically or intrinsically. Extrinsic evalu-\nation measures a model’s usefulness in some down-\nstream task such as speech recognition or machine\ntranslation whereas intrinsic evaluation uses sta-\ntistical measures to assess a model’s quality. In\nthis paper we focus on intrinsic evaluation metrics\nrelated to cross-entropy and perplexity.\nIn information theory, entropy represents the av-\nerage number of units of information produced per\nobservation (Shannon, 1948). The cross-entropy of\na language model on a given sample of text Wn\n1 is\nestimated as\nH(Wn\n1 ) =−1\nn log2 P(Wn\n1 ), (2)\nwith the units of information being bits due to the\nlog base 2 (Jurafsky and Martin, 2020). The more\naccurately the model approximates the true distri-\nbution of the language, the lower the cross-entropy.\nLanguage models with a ﬁxed vocabulary are usu-\nally evaluated based on perplexity, which is com-\nputed as 2H(Wn\n1 ). However, closed-vocabulary lan-\nguage models have to set the size of the vocabu-\nlary and treat all other words as unknown. Con-\nsequently, perplexity cannot be compared directly\nacross models with different vocabularies.\nIn this paper we are studying open-vocabulary\nmodels, and we want the choice of tokenization and\nvocabulary to be a modelling choice. This necessi-\ntates an evaluation metric which is independent of\nthe tokenization.\nAs evaluation metric we use bits per character\n(BPC), a measure of cross-entropy which is nor-\nmalised by the character length of the text and\nis therefore independent of the tokenization. The\nBPC of a model on a test set Wn\n1 is calculated as\nBPC(Wn\n1 ) =n\nc H(Wn\n1 ), (3)\nwhere the text consists of c characters.\n2.3 Models\n2.3.1 n-gram Models\nn-gram language models make the Markov assump-\ntion of restricting the context for predicting the next\nword to the last n −1 words (Jurafsky and Martin,\n2020). Traditional n-grams are based on various\nsmoothing methods, of which modiﬁed Knesser-\nNey smoothing has been shown to lead to the best\nperformance in general (Kneser and Ney, 1995;\nChen and Goodman, 1999). Sparsity increases as\nthe n-gram size increases, which leads to practical\nlimits on the size of n that is used.\n2.3.2 Feedforward Neural Networks\nThe ﬁrst neural network-based language mod-\nels were based on feedforward neural networks\n(FFNNs), which also make the Markov assump-\ntion, and are therefore effectively neuralized n-\ngram models (Bengio et al., 2003). One of the\nkey advantages of neural language models over\nn-grams is that word embeddings allow them to\ngeneralise better, as words with similar meanings\nor grammatical functions will have similar embed-\ndings (Mikolov et al., 2013).\nThe ﬁrst layer of an FFNN takes the concatena-\ntion of the context word embeddings as input. The\nembedding layer is learned jointly with the rest of\nthe model and weight-tied to the output layer, fol-\nlowing standard practice in RNN-based language\nmodelling. We use a rectiﬁed linear unit as non-\nlinearity.\n2.3.3 LSTMs\nLSTMs (Hochreiter and Schmidhuber, 1997) are\na widely used variant of the standard RNN archi-\ntecture allowing for longer term dependencies to\nbe modelled more effectively by using a number of\ngates along with a memory vector in the recurrent\ncell. The gates and the memory vector enable infor-\nmation to pass more effectively across time steps.\nWe use a Basic-LSTM model as a baseline for the\nmore complex AWD-LSTM and QRNN models\n(see below).\nThis model is regularized using dropout, which\ntemporarily hides a random subset of neurons dur-\ning each training step (Srivastava et al., 2014). This\nadds noise and prevents the model from being\noverly reliant on any particular neuron. However,\ndropout in RNN models cannot be applied between\ntime steps on the recurrent connection as it inhibits\nthe model’s ability to retain long term dependen-\ncies, so the standard approach is to apply dropout\nonly on the input and output connections (Zaremba\net al., 2015). The Basic-LSTM baseline does not\nuse the more complex regularization and optimiza-\ntion techniques used by the other models.\n2.3.4 A WD-LSTM\nThe AWD-LSTM model (Merity et al., 2018b) is\nused widely for language modelling and forms the\nbases of the current state-of-the-art language mod-\nelling on small English datasets without dynamic\nevaluation (Takase et al., 2018). In order to enable\na fair comparison across models we are not using\na continuous cache pointer (Grave et al., 2017) or\ndynamic evaluation.\nThe AWD-LSTM uses a number of improved\nregularization and optimization techniques. Regu-\nlarization is particularly important in low-resource\nsettings. DropConnect (Wan et al., 2013) is a form\nof dropout on the hidden-to-hidden weights.3 Vari-\national dropout (Gal and Ghahramani, 2016) gener-\nates a dropout mask once which is then used over\nthe entire forward and backward pass, rather than\nresampling at every timestep. The AWD-LSTM\n3This method is particularly useful as it is applied once to\nthe weight matrices before the forward and backward pass,\nallowing the use of black box RNN implementations such as\nNVIDIA’s cuDNN LSTM which can be many times faster due\nto hardware optimisations (Merity et al., 2018b).\nmodel uses a combination of DropConnect for\nthe hidden-to-hidden transitions within the LSTM\nand variational dropout over the inputs and out-\nputs. Other tehchniques used include using variable\nlength backpropagation sequences, word dropout\n(masking entire word embeddings), and L1 and L2\nregularisation.\n2.3.5 Quasi-Recurrent Neural Networks\nQuasi-Recurrent Neural Networks (QRNNs) (Brad-\nbury et al., 2017) is a modiﬁcation of RNNs that\nparallizes parts of the RNN computation and ob-\ntained similar or even slightly better performance\nthan the AWD-LSTM on some English datasets\n(Merity et al., 2018a). The QRNN applies convolu-\ntional layers on the input, followed by an recurrent\npooling function resembling LSTM gating. This\nsigniﬁcantly increases training speed compared to\nLSTMs of similar sizes.\n2.3.6 Transformers\nThe Transformer (Vaswani et al., 2017) presents\nanother approach to speeding up sequential pro-\ncessing over RNNs by relying entirely on attention\nmechanisms (Bahdanau et al., 2015) instead of re-\ncurrent connections for propagating information\nacross time steps. An attention mechanism can pro-\ncess all the input embeddings for a (ﬁxed-length)\nsequence simultaneously and selectively weight\ncertain features based on a learned function.\nThe original Transformer model was used for\ntranslation and has an encoder-decoder structure\n(Vaswani et al., 2017). For the task of language\nmodelling, only the decoder architecture is used\n(Liu et al., 2018). We follow the architecture used\nby GPT-2 (Radford et al., 2019). A learned po-\nsitional embedding is added to each input token\nembedding. Multiple layers, each including an at-\ntention and a feedforward sub-layer, are stacked\nto create the larger model that can propagate in-\nformation more efﬁciently across time steps. In\neach attention sub-layer multiple attention mecha-\nnisms are used to extract features; this strategy is\ntermed multi-headed self-attention. Finally, a resid-\nual connection and layer normalisation is applied\nover each sub-layer. To regularise the Transformer\nmodels we use dropout on all weights of the model.\n3 Experimental Setup\n3.1 Datasets\nWe focus on language modelling for isiZulu and Se-\npedi, but we processed data for all 11 non-European\nCorpus Words\nTraining Valid/Test\nNCHLT (isiZulu) 978.6 122.3\nIsolezwe (isiZulu) 940.2 117.5\nNCHLT (Sepedi) 1357.3 169.7\nTable 1: Dataset sizes, reported in thousands of words,\nafter preprocessing. The validation and test sets of each\ncorpus are approximately equal in size.\nofﬁcial South African languages, and use the other\nlanguages’ data for multilingual training (Section\n5). We use two dataset sources:\nNCHLT:We use the corpora from the National\nCentre for Human Language Technology (NCHLT)\nText project (Eiselen and Puttkammer, 2014) made\navailable by the South African Centre for Digi-\ntal Language Resources (SADiLaR). 4 Monolin-\ngual text corpora are available for all 11 of South\nAfrica’s ofﬁcial languages. We processed the cor-\npora for the Nguni languages (isiZulu, Siswati,\nisiNdebele and isiXhosa) and the Sotho-Tswana\nlanguages (Sesotho, Sepedi, Setswana), as well\nas Xitsonga and Tshivenda, the other two Benue-\nCongo languages. A signiﬁcant proportion of these\ntexts were scraped from governmental websites.\nThe corpora range in size from 1 to 3 million to-\nkens. Sepedi and isiZulu have the largest datasets\nin their respective language groups.\nIsolezwe: News articles from the isiZulu\nIsolezwe newspaper, one of the largest daily\nAfrican language newspapers in South Africa, have\nbeen scraped and consolidated by the Newstools\ninitiative.5 This is the largest publicly-available\nnewspaper corpus among the languages we are con-\nsidering that we are aware of. The dataset has a\nsimilar size to the NCHLT isiZulu corpus but pro-\nvides a second evaluation domain.\nWe performed a number of data proprocessing\nand normalization steps. We removed instances\nof English, HTML and Javascript lines, and other\nrepetitive or erroneous data, as these would not nat-\nurally be found in general language. Each dataset\nwas split into a training, validation and test set us-\ning an 80% / 10% /10% split. The splits were done\nusing sequential blocks to preserve the order of the\nsentences. Table 1 compares the dataset sizes.\n4Datasets are available at https://repo.sadilar.\norg/handle/20.500.12185/7\n5Available at https://github.com/newstools\n3.2 Model Implementation and Optimization\nThe BPE preprocessing for all models uses the\nHuggingFace tokenizers library.6\n3.2.1 n-gram Models\nWe use an n-gram language model with modiﬁed\nKneser-Ney (Chen and Goodman, 1999) smooth-\ning, as implemented in KenLM. 7 We tuned the\nmodels by testing BPE vocabulary sizes ranging\nfrom 100 to 10000 and n-gram orders from 2 to 6.\nThe isiZulu and Sepedi models performed best with\nBPE vocabulary sizes of 500 and 2000 respectively.\nFor all datasets, an n-gram order of 6 yielded the\nbest performance.\n3.2.2 Feedforward Neural Networks\nWe implemented a feed-forward neural network\n(FFNN) language model so that it can be trained\nin a similar manner to RNN and Transformer lan-\nguage models. The training data is divided into\nchunks of 64 tokens and batched to enable parallel\nprocessing. We follow the optimization and regu-\nlarization setup of the FFNN baseline used by Chiu\nand Rush (2020). We use of a learning rate decay\nschedule where the learning rate is multiplied by\n0.25 after each epoch if the validation loss does not\nimprove. The models were trained for 50 epochs\nwith a batch size of 32 and an AdamW weight de-\ncay of 0.01. Both word embeddings and hidden\nlayers had a size of 500.\nUsing grid search, we evaluated BPE vocabulary\nsizes 1000 and 2000 to 10 000 with an interval size\nof 2000, n-gram orders {2, 4, 6}, word embedding\nand hidden layer sizes in the range {500, 2500}\nwith an interval of 250, dropout rates of {0.3, 0.5}\nand {2, 4, 6}hidden layers.\nFor both NCHLT isiZulu and NCHLT Sepedi\na BPE vocabulary size of 8000 yielded the best\nperformance, and on Isolezwe 10 000 performed\nbest. For both Isolezwe and NCHLT isiZulu, an\nn-gram order of 2 performed best and for NCHLT\nSepedi an order of 4. We were unable to ﬁnd a fully\nsatisfactory explanation of why the FFNN did not\nperform better with a higher n-gram orders.\n3.2.3 LSTMs\nWe use the PyTorch implementation of the AWD-\nLSTM (Merity et al., 2018b).8 We took the hyper-\n6https://github.com/huggingface/\ntokenizers\n7Available at https://github.com/kpu/kenlm\n8Available at https://github.com/\nsalesforce/awd-lstm-lm\nFigure 2: The validation loss of the basic LSTM (pink),\nAWD-LSTM (grey) and QRNN (blue) while training\non the NCHLT isiZulu dataset.\nparameters of Merity et al. (2018b) on the word-\nlevel WikiText 2 dataset as the starting point for\ntuning our models, as its size is comparable to\nour dataset. We performed a partial grid search\nover the embedding size {400, 800}, hidden layer\nsize {1150, 1200, 1550}, number of layers {1, 2,\n3, 4 }, learning rate {5, 10, 30 }, batch size {40,\n80}, vocab size {2500, 5000, 7500, 10000 }as\nwell as dropout rate {0 - 0.7}and weight drop {0\n- 0.5}(both in increments of 0.1) and L1/L2 reg-\nularisation values {0, 1, 2}. Model development\nwas primarily done on the isiZulu NCHLT corpus.\nMost improvement came from increasing the to-\ntal model size by either increasing the number of\nhidden layers or increasing the input embedding\nsize. Changing the BPE vocabulary size did not\nhave a signiﬁcant effect on performance. The Basic\nLSTM was tuned similarly, excluding the regular-\nization techniques it does not implement.\n3.2.4 QRNN\nThe QRNN is also implemented in the AWD-\nLSTM packages. We tuned the embedding size,\nvocabulary size, number of hidden layers and batch\nsize, using similar ranges as for the AWD-LSTM.\nThe best QRNNs used an embedding size of 800,\nhidden layer sizes of 1550, and 4 hidden layers.\nFigure 2 shows how the validation loss changes\nwhile the RNN-based models (Basic LSTM, AWD-\nLSTM, and QRNN) train on the NCHLT isiZulu\ncorpus. The plot shows how the QRNN’s loss de-\ncreases faster than that of the AWD-LSTM time.\nThe Basic LSTM initially trains faster, but then\noverﬁts drastically.\n3.2.5 Transformers\nWe used the GPT-2 (Radford et al., 2019) PyTorch\nimplementation provided by the open-source Hug-\ngingFace transformers library.9 The training data\nwas fed to the model in blocks of 128 consecutive\ntokens with a batch size of 32, created using a slid-\ning window over the training data with a stride of\n16 tokens. Model evaluation was performed using\nan input block size of 128 with a stride of 64.\nFor hyper-parameter tuning, models were trained\nfor up to 200k steps, with evaluation on a validation\nset every 5k steps. Training was stopped early if\nthe validation loss did not decrease after any four\nsuccessive evaluations. The model and vocabulary\nsizes were tuned ﬁrst with little regularization to\nensure that the models had enough capacity to over-\nﬁt the data. Increasing amounts of regularization\nwere then applied until the model no longer overﬁt\nthe data.\nWe used 8 hidden layers and 8 attention heads.\nPreliminary experiments showed that the model\nwas relatively insensitive to the number of hidden\nlayers and the number of attention heads. We used\nan initial learning rate of 10−4 with a learning rate\nschedule that linearly decreases to 0 over the course\nof the training. Across all 3 corpora, the best per-\nforming models had a hidden layer size of 256,\na dropout probability of 0.3 and a weight decay\nof 0.2. The isiZulu and Sepedi models performed\nbest with BPE vocabulary sizes of 8000 and 2000\nrespectively.\n4 Results and Discussion\n4.1 Results\nAll the test set results are given in Table 2. The n-\ngram and FFNN language models performed fairly\nsimilarly to each other across the datasets and lan-\nguages, even though the FFNNs used smaller n-\ngram orders. On the isiZulu datasets, the FFNN\nperformed slightly better than the n-gram models,\nwhile on the Sepedi dataset the n-gram model per-\nformed better. On all datasets, we found that the n-\ngram models tended to perform better with smaller\nBPE vocabulary sizes, whereas the FFNN models\nperformed better with larger vocabulary sizes.\nThe performance of the AWD-LSTM and\nQRNN models was closely matched (within 0.005\nBPC) across all datasets with the QRNN slightly\noutperforming on the two NCHLT datasets, and the\n9https://huggingface.co/transformers/\nFigure 3: Test set results, plotted to show the relative\nperformance of the models on each of the three datasets\n(lower is better). The AWD-LSTM and QRNN consis-\ntently outperform the other models while within close\nmargin of each other, followed by the Transformer,\nwhile the n-gram, FFNN and Basic-LSTM perform\nsubstantially worse.\nAWD-LSTM ahead on the Isolezwe dataset. The\nbasic LSTM under-performed the others substan-\ntially, with performance closer to, or even worse\nthan, that of the n-gram and FFNN models.\nThe transformer models achieved competitive\nperformance on all datasets, but were outperformed\nby the QRNN and AWD-LSTM. We hypothesize\nthat the main reason is that these models used\nmore sophisticated regularization techniques that\nour Transformer implementation did not use. Ad-\nditionally, the RNNs had more parameters, but the\nTransformer’s performance did not improve with\nmore parameters in our experiments.\n4.2 Discussion\nThe results show that the relative performance of\nthe models is similar to language modelling results\npreviously reported on widely used PTB and Wiki-\nText2 English datasets (Merity et al., 2017), which\nare comparable in size to our corpora. Regarding\nthe performance of the Transformer, it has been\nreported that a modiﬁed Transformer architecture\nwith segment-level recurrence can obtain similar re-\nsults to the AWD-LSTM when ﬁne-tuned using the\nsame sophisticated regularization techniques (Dai\net al., 2019), but other researchers have struggled\nto reproduce these results independently.10\nWe found that the relative performance of the lan-\nguage models was similar across the three datasets\n(Figure 3). This supports the hypothesis that the\nsame models would likely perform well across all\nthe languages in the Nguni and Sotho-Tswana lan-\nguage groups. The AWD-LSTM and QRNN mod-\n10https://twitter.com/srush_nlp/status/\n1245825437240102913\nNCHLT (isiZulu) Isolezwe (isiZulu) NCHLT (Sepedi)\nModel Params V ocab BPC Params V ocab BPC Params V ocab BPC\nn-gram 7.5M 500 1.588 6.9M 500 1.544 5.7M 2000 1.656\nFFNN 4.7M 8000 1.572 5.7M 1000 1.532 5.1M 8000 1.723\nBasic-LSTM 3.3M 5000 1.548 3.3M 5000 1.677 3.3M 5000 1.625\nAWD-LSTM 29.8M 5000 1.325 29.8M 5000 1.259 29.8M 5000 1.421\nQRNN 29.5M 10000 1.323 29.5M 10000 1.264 29.5M 5000 1.421\nTransformer 8.6M 8000 1.391 8.6M 8000 1.320 7.1M 2000 1.495\nTable 2: Language modelling results on the isiZulu and Sepedi corpora, reported as bits-per-character (BPC). The\nBPE vocabulary size and number of parameters of each model are also reported.\nels were consistently close in performance, fol-\nlowed by the transformer model across all datasets.\nThe remaining n-gram, FFNN and Basic-LSTM\nmodels had different relative performances on the\ndatasets with no consistent pattern, although the\nn-gram and FFNN are closer to each other. The\npoor performance of the n-gram and FFNN models\nrepresents a trade-off between training time and\nmodel performance. If training time was a factor,\nreduced performance could be accepted in order to\nproduce models more quickly. The n-gram models\nare also much faster when queried in downstream\napplications.\n5 Multilingual Models\nAs an additional experiment, we investigate the po-\ntential for multilingual language modelling by con-\ncatenating training data from multiple languages\nand evaluating on the same target languages as\nbefore. For practical reasons, we only train Trans-\nformer models for this experiment. We use the\nNCHLT corpora as they provide text in the same\ndomain across all South African languages.\nWe train models in a number of different settings.\nIn particular, we were interested in comparing the\neffect of training on additional languages from the\nsame language group (isiZulu: all Nguni languages;\nSepedi: all Sotho-Tswana languages) compared\nto training on languages from the other language\ngroup (isiZulu: Sotho-Tswana languages; Sepedi:\nNguni languages). Finally, we also evaluated a\nmodel trained on all 9 Benue-Congo South African\nlanguages in the NCHLT corpus. Model hyper-\nparameters were tuned separately in each instance.\nThe results are shown in Figure 4. For both target\nlanguages, concatenating training data from other\nBenue-Congo languages improves performance. In\nisiZulu Sepedi\n1.25\n1.3\n1.35\n1.4\n1.45\n1.5\n1.391\n1.495\n1.334\n1.447\n1.331\n1.477\n1.298\n1.416\nBPC\nMonolingual Same Group\nDifferent Group All Languages\nFigure 4: Multilingual language modelling results, re-\nported as bits-per-character (BPC), evaluated on the\nisiZulu and Sepedi test sets. Models were trained\non the target language (Monolingual), and addition-\nally also on multiple languages in the same language\ngroup (Nguni and Sotho-Tswana, respectively), lan-\nguages from the other language group, or on text from\nall 9 non-European ofﬁcial South African languages.\ngeneral, training on more languages improves per-\nformance regardless of the language group. In the\ncase of Sepedi as target language, concatenating the\nother Sotho-Tswana languages yields a greater per-\nformance improvement than concatenating Nguni\nlanguages. On the other hand, for isiZulu the re-\nsults of including additional data from the same\nor the other langauge family were similar. For\nboth isiZulu and Sepedi models, the best perfor-\nmance is obtained by concatenating data from all\nlanguages. We hypothesize that transfer may be\nmore effective from disjunctively written languages\n(Sotho-Tswana) to conjunctively written languages\n(Nguni) than the other way around, but this needs\nto be investigated further. Our results suggest that\nthe use of data from multiple languages is a promis-\ning future direction for modelling South African\nlanguages.\n6 Conclusions\nThe experiments conducted in this paper demon-\nstrated that improved regularization techniques\nand model architectures developed on relatively\nsmall English datasets also improves language\nmodelling performance when applied to African\nlanguages such as isiZulu and Sepedi. The\nAWD-LSTM and QRNN performed notably bet-\nter than the other models. As expected, n-grams\nand FFNNs, as well as the Basic LSTM, under-\nperformed the more advanced models. However,\nthe stronger models are computationally more ex-\npensive. Our results suggest that further improve-\nments in RNN- and Transformer-based language\nmodelling would likely be directly applicable to\nlow-resource African languages. Additionally, we\nshowed that BPE is an effective method for open\nvocabulary language modelling across multiple\nmodels, effectively accounting for the large (word-\nlevel) vocabulary sizes of agglutinative African\nLanguages. Finally, we showed that multilingual\nlanguage modelling is a promising direction for\nfuture research, as many African languages occur\nin groups of closely related languages which might\nbeneﬁt from such an approach.\nAcknowledgments\nThis work is based on research supported in part by\nthe National Research Foundation of South Africa\n(Grant Number: 129850) and the South African\nCentre for High Performance Computing.\nReferences\nStatistics South Africa. 2012. Census 2011, Census in\nBrief . Statistics South Africa, Pretoria.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015.\nYoshua Bengio, R´ejean Ducharme, Pascal Vincent, and\nChristian Jauvin. 2003. A neural probabilistic lan-\nguage model. Journal of machine learning research,\n3(Feb):1137–1155.\nJames Bradbury, Stephen Merity, Caiming Xiong, and\nRichard Socher. 2017. Quasi-Recurrent Neural Net-\nworks. In International Conference on Learning\nRepresentations.\nCatherine Chavula and Hussein Suleman. 2016. As-\nsessing the impact of vocabulary similarity on mul-\ntilingual information retrieval for bantu languages.\nIn Proceedings of the 8th Annual Meeting of the Fo-\nrum on Information Retrieval Evaluation, FIRE ’16,\npage 16–23, New York, NY , USA. Association for\nComputing Machinery.\nStanley F Chen and Joshua Goodman. 1999. An\nempirical study of smoothing techniques for lan-\nguage modeling. Computer Speech & Language ,\n13(4):359–394.\nJustin Chiu and Alexander Rush. 2020. Scaling hid-\nden Markov language models. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 1341–1349,\nOnline. Association for Computational Linguistics.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988, Florence, Italy.\nAssociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nRoald Eiselen and Martin Puttkammer. 2014. De-\nveloping text resources for ten south African lan-\nguages. In Proceedings of the Ninth International\nConference on Language Resources and Evalua-\ntion (LREC’14), pages 3698–3703, Reykjavik, Ice-\nland. European Language Resources Association\n(ELRA).\nAlexander Franz and Brian Milch. 2002. Searching\nthe web by voice. In Proceedings of the 19th Inter-\nnational Conference on Computational Linguistics -\nVolume 2, COLING ’02, page 1–5, USA. Associa-\ntion for Computational Linguistics.\nPhilip Gage. 1994. A new algorithm for data compres-\nsion. C Users Journal, 12(2):23–38.\nYarin Gal and Zoubin Ghahramani. 2016. A theoret-\nically grounded application of dropout in recurrent\nneural networks. In Proceedings of the 30th Interna-\ntional Conference on Neural Information Processing\nSystems, NIPS’16, page 1027–1035, Red Hook, NY ,\nUSA. Curran Associates Inc.\nEdouard Grave, Armand Joulin, and Nicolas Usunier.\n2017. Improving neural language models with a\ncontinuous cache. In International Conference on\nLearning Representations.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nDaniel Jurafsky and James H Martin. 2020. N-gram\nLanguage Models. In Speech and Language Pro-\ncessing, 3 edition, chapter 3. Online Draft.\nReinhard Kneser and Hermann Ney. 1995. Improved\nbacking-off for M-gram language modeling. In\nICASSP , IEEE International Conference on Acous-\ntics, Speech and Signal Processing - Proceedings ,\nvolume 1, pages 181–184. IEEE.\nAnkit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer,\nJames Bradbury, Ishaan Gulrajani, Victor Zhong,\nRomain Paulus, and Richard Socher. 2016. Ask me\nanything: Dynamic memory networks for natural\nlanguage processing. In Proceedings of the 33rd In-\nternational Conference on International Conference\non Machine Learning - Volume 48 , ICML’16, page\n1378–1387. JMLR.org.\nPeter J. Liu, Mohammad Saleh, Etienne Pot, Ben\nGoodrich, Ryan Sepassi, Lukasz Kaiser, and Noam\nShazeer. 2018. Generating wikipedia by summariz-\ning long sequences. In International Conference on\nLearning Representations.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018a. An Analysis of Neural Language\nModeling at Multiple Scales. CoRR.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018b. Regularizing and optimizing LSTM\nlanguage models. In International Conference on\nLearning Representations.\nStephen Merity, Bryan Mccann, and Richard Socher.\n2017. Revisiting Activation Regularization for Lan-\nguage RNNs. CoRR.\nSebastian J Mielke and Jason Eisner. 2019. Spell once,\nsummon anywhere: A two-level open-vocabulary\nlanguage model. In Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence , volume 33, pages\n6843–6850.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their composition-\nality. In C. J. C. Burges, L. Bottou, M. Welling,\nZ. Ghahramani, and K. Q. Weinberger, editors, Ad-\nvances in Neural Information Processing Systems\n26, pages 3111–3119. Curran Associates, Inc.\nB. Ndaba, H. Suleman, C. M. Keet, and L. Khumalo.\n2016. The effects of a corpus on isizulu spellcheck-\ners based on n-grams. In 2016 IST-Africa Week Con-\nference, pages 1–10.\nLaurette Pretorius and Sonja Bosch. 2009. Exploiting\ncross-linguistic similarities in Zulu and Xhosa com-\nputational morphology. In Proceedings of the First\nWorkshop on Language Technologies for African\nLanguages, pages 96–103, Athens, Greece. Associa-\ntion for Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. Techni-\ncal report, OpenAI.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nC. E. Shannon. 1948. A Mathematical Theory of\nCommunication. Bell System Technical Journal ,\n27(3):379–423.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: a simple way to prevent neural networks\nfrom overﬁtting. The journal of machine learning\nresearch, 15(1):1929–1958.\nSho Takase, Jun Suzuki, and Masaaki Nagata. 2018.\nDirect output connection for a high-rank language\nmodel. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 4599–4609, Brussels, Belgium. Association\nfor Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nLi Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun,\nand Rob Fergus. 2013. Regularization of neural net-\nworks using dropconnect. In International confer-\nence on machine learning, pages 1058–1066.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin John-\nson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws,\nYoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith\nStevens, George Kurian, Nishant Patil, Wei Wang,\nCliff Young, Jason Smith, Jason Riesa, Alex Rud-\nnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\nand Jeffrey Dean. 2016. Google’s neural machine\ntranslation system: Bridging the gap between human\nand machine translation. CoRR, abs/1609.08144.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural\nInformation Processing Systems , volume 32, pages\n5753–5763. Curran Associates, Inc.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.\n2015. Recurrent Neural Network Regularization .\nCoRR.",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7854650020599365
    },
    {
      "name": "Recurrent neural network",
      "score": 0.6868879795074463
    },
    {
      "name": "Natural language processing",
      "score": 0.5569270253181458
    },
    {
      "name": "Transformer",
      "score": 0.5478614568710327
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5451431274414062
    },
    {
      "name": "Vocabulary",
      "score": 0.5038833022117615
    },
    {
      "name": "Language model",
      "score": 0.4732376039028168
    },
    {
      "name": "Natural language",
      "score": 0.43111446499824524
    },
    {
      "name": "Artificial neural network",
      "score": 0.4251907765865326
    },
    {
      "name": "Linguistics",
      "score": 0.19343769550323486
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "topic": "Computer science",
  "institutions": []
}