{
    "title": "Generalized Planning in PDDL Domains with Pretrained Large Language Models",
    "url": "https://openalex.org/W4393160795",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2527339991",
            "name": "Tom Silver",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2223366505",
            "name": "Soham Dan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2238314490",
            "name": "Kavitha Srinivas",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2149572011",
            "name": "Joshua B. Tenenbaum",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2635294610",
            "name": "Leslie Kaelbling",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2098402621",
            "name": "Michael Katz",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W6674236629",
        "https://openalex.org/W2296450740",
        "https://openalex.org/W2785422319",
        "https://openalex.org/W6685061678",
        "https://openalex.org/W6782666160",
        "https://openalex.org/W6655395985",
        "https://openalex.org/W6786039469",
        "https://openalex.org/W2149361370",
        "https://openalex.org/W4221152848",
        "https://openalex.org/W955033985",
        "https://openalex.org/W6772383937",
        "https://openalex.org/W6604353299",
        "https://openalex.org/W1505350346",
        "https://openalex.org/W6675011887",
        "https://openalex.org/W6666083373",
        "https://openalex.org/W3106271432",
        "https://openalex.org/W6616173779",
        "https://openalex.org/W2888241791",
        "https://openalex.org/W3150583184",
        "https://openalex.org/W6801811728",
        "https://openalex.org/W6810647384",
        "https://openalex.org/W2575572349",
        "https://openalex.org/W1561957164",
        "https://openalex.org/W2221785343",
        "https://openalex.org/W2132622533",
        "https://openalex.org/W171572779",
        "https://openalex.org/W2157340420",
        "https://openalex.org/W4285605417",
        "https://openalex.org/W3023380900",
        "https://openalex.org/W2998185271",
        "https://openalex.org/W4382202844",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W3177813494",
        "https://openalex.org/W4368893797",
        "https://openalex.org/W4309581942",
        "https://openalex.org/W4382202802",
        "https://openalex.org/W4318902699",
        "https://openalex.org/W4283330306",
        "https://openalex.org/W4280543132",
        "https://openalex.org/W4361807047",
        "https://openalex.org/W4385572162",
        "https://openalex.org/W4285428875",
        "https://openalex.org/W4309591663",
        "https://openalex.org/W4383097638",
        "https://openalex.org/W2520858206",
        "https://openalex.org/W4366999541",
        "https://openalex.org/W2101901809",
        "https://openalex.org/W4226485558",
        "https://openalex.org/W2020149918",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W4320559489",
        "https://openalex.org/W3065734471",
        "https://openalex.org/W567721252",
        "https://openalex.org/W4224912544",
        "https://openalex.org/W4303478243",
        "https://openalex.org/W2921844528",
        "https://openalex.org/W4324325724",
        "https://openalex.org/W2094878426",
        "https://openalex.org/W2171798962",
        "https://openalex.org/W1628070947",
        "https://openalex.org/W4360836968",
        "https://openalex.org/W4383108457",
        "https://openalex.org/W3202187802",
        "https://openalex.org/W4312045619",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4388660746",
        "https://openalex.org/W4365205411",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W3101355526",
        "https://openalex.org/W4320165837"
    ],
    "abstract": "Recent work has considered whether large language models (LLMs) can function as planners: given a task, generate a plan. We investigate whether LLMs can serve as generalized planners: given a domain and training tasks, generate a program that efficiently produces plans for other tasks in the domain. In particular, we consider PDDL domains and use GPT-4 to synthesize Python programs. We also consider (1) Chain-of-Thought (CoT) summarization, where the LLM is prompted to summarize the domain and propose a strategy in words before synthesizing the program; and (2) automated debugging, where the program is validated with respect to the training tasks, and in case of errors, the LLM is re-prompted with four types of feedback. We evaluate this approach in seven PDDL domains and compare it to four ablations and four baselines. Overall, we find that GPT-4 is a surprisingly powerful generalized planner. We also conclude that automated debugging is very important, that CoT summarization has non-uniform impact, that GPT-4 is far superior to GPT-3.5, and that just two training tasks are often sufficient for strong generalization.",
    "full_text": "Generalized Planning in PDDL Domains with Pretrained Large Language Models\nTom Silver1, Soham Dan 2, Kavitha Srinivas 2,\nJoshua B. Tenenbaum1, Leslie Kaelbling 1, Michael Katz 2\n1MIT Computer Science and Artiﬁcial Intelligence Laboratory\n2IBM Research\nAbstract\nRecent work has considered whether large language mod-\nels (LLMs) can function as planners: given a task, generate\na plan. We investigate whether LLMs can serve as general-\nized planners: given a domain and training tasks, generate a\nprogram that efﬁciently produces plans for other tasks in the\ndomain. In particular, we consider PDDL domains and use\nGPT-4 to synthesize Python programs. We also consider (1)\nChain-of-Thought (CoT) summarization, where the LLM is\nprompted to summarize the domain and propose a strategy\nin words before synthesizing the program; and (2) automated\ndebugging, where the program is validated with respect to the\ntraining tasks, and in case of errors, the LLM is re-prompted\nwith four types of feedback. We evaluate this approach in\nseven PDDL domains and compare it to four ablations and\nfour baselines. Overall, we ﬁnd that GPT-4 is a surprisingly\npowerful generalized planner. We also conclude that auto-\nmated debugging is very important, that CoT summarization\nhas non-uniform impact, that GPT-4 is far superior to GPT-\n3.5, and that just two training tasks are often sufﬁcient for\nstrong generalization.\nIntroduction\nWhile some classes of sequential decision-making tasks are\nprovably intractable (Chapman 1987), others can be solved\nefﬁciently with a single domain-speciﬁc program. In the lat-\nter case, there is considerable interest in automatically syn-\nthesizing these programs given a small number of training\ntasks. In AI planning, several approaches to this generalized\nplanning problem have been proposed, with programs ex-\npressed as lifted decision lists, as ﬁnite state machines, or\nin domain-speciﬁc languages (Srivastava 2011; Bonet and\nGeffner 2015; Jim ´enez, Segovia-Aguas, and Jonsson 2019;\nRivlin, Hazan, and Karpas 2020). In reinforcement learning,\ngoal-conditioned policies and value functions can be under-\nstood as particular kinds of programs learned with the same\ngeneralized planning objective (Sutton et al. 2011; Schaul\net al. 2015). Despite these efforts, it remains challenging to\nefﬁciently synthesize programs from few training tasks that\ngeneralize to a wide variety of held-out tasks.\nGiven the tremendous recent progress in large language\nmodels (LLMs) (Brown et al. 2020; Chen et al. 2021;\nCopyright © 2024, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nChowdhery et al. 2022), especially in code generation (Chen\net al. 2021; Nijkamp et al. 2023; Chen et al. 2023a), this\nwork asks a simple question: can pretrained LLMs be\nused for generalized planning? In particular, we investi-\ngate whether GPT-4 (OpenAI 2023) can be used to write a\ndomain-speciﬁc Python program that solves a set of tasks in\na planning domain. For each domain, we prompt GPT-4 with\nthe domain and a small number of training tasks encoded in\nthe Planning Domain Deﬁnition Language (PDDL) (McDer-\nmott 2000). We then ask GPT-4 to write a program that con-\nsumes a (parsed) task description and outputs a plan. To pre-\nvent it from writing domain-general search-based code—a\nnatural inclination given the association between PDDL and\nsearch in its pretraining data—we instruct GPT-4 to imple-\nment “a simple strategy that does not use search.”\nBeyond this basic protocol, we consider two extensions.\nFirst, inspired by Chain-of-Thought (CoT) (Wei et al. 2022;\nJiang et al. 2023), we prompt GPT-4 to write a natu-\nral language summary of the PDDL domain. We then\nask it to describe a solution strategy before ﬁnally imple-\nmenting the strategy in Python. Second, inspired by In-\nner Monologue (Huang et al. 2022b) and Corrective Re-\nprompting (Raman et al. 2022), we automatically provide\nfeedback to GPT-4 in the case where it fails to solve train-\ning tasks. For example, if executing the Python code results\nin an exception, we present GPT-4 with that exception and\nask it to ﬁx the code. We repeat this automated debugging\nprocess up to four times or until all training tasks are solved.\nSee Figure 1 for an overview of this pipeline.\nIn our experiments, we evaluate this approach on seven\nPDDL domains: six from recent work in generalized plan-\nning (Yang et al. 2022), and a seventh novel domain. We\nﬁnd that the approach is a strong baseline compared to ex-\nisting generalized planning approaches. This is an important\nﬁnding that we expect to inform further research in general-\nized planning. We also present a suite of ablations and ad-\nditional analyses to unpack the contributions of CoT sum-\nmarization, automated debugging, names in the PDDL, and\nGPT-4 vs. GPT-3.5. Our results suggest that automated de-\nbugging, PDDL names, and GPT-4 are very important, while\nthe impact of CoT is non-uniform. Finally, we provide qual-\nitative analyses of common failure cases, suggesting direc-\ntions for future work. We conclude that GPT-4 is a surpris-\ningly powerful generalized planner when properly guided.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n20256\nDomain & \nTrain \nTasks\nDomain \nSummaryLLM LLM Proposed \nStrategy LLM Generalized \nPlan Val Done\nCoT Summarization Automated Debugging\nFigure 1: Overview of pipeline for generalized planning with pretrained LLMs. See text for details.\nRelated Work\nLLMs for (PDDL) Planning. Generalized planning with\nLLMs can be seen as an alternative to planning with\nLLMs (Sharma, Torralba, and Andreas 2022; Ahn et al.\n2022; Huang et al. 2022a; Raman et al. 2022; Lin et al.\n2022). Most relevant is work by Valmeekam et al. (2022);\nSilver et al. (2022) who consider LLM-based planning\nin PDDL domains. There are several advantages to using\nLLMs for generalized planning, rather than planning: (1)\nprograms produced by the LLM can be inspected and vali-\ndated; (2) running a synthesized program can be much faster\n(and cheaper) than querying the LLM for each new task;\n(3) synthesized programs can scale to arbitrarily large tasks,\nwhereas current LLMs are limited by context window size.\nPallagani et al. (2022) consider ﬁne-tuning an LLM to solve\nPDDL tasks. Other recent work has considered using LLMs\nfor translating between natural language and PDDL (Collins\net al. 2022; Lin et al. 2023; Xie et al. 2023; Liu et al. 2023).\nThese efforts could be combined with our approach.\nGeneralized Planning. This work contributes to a grow-\ning literature on generalized planning (Fikes, Hart, and\nNilsson 1972; Jim ´enez, Segovia-Aguas, and Jonsson 2019).\nPrior work has considered synthesizing generalized plans in\nseveral ways: (1) performing a search through a hypothesis\nclass of generalized policies (Levine and Humphreys 2003;\nJim´enez and Jonsson 2015; Segovia-Aguas, Jim ´enez, and\nJonsson 2018, 2021); (2) using example plans to construct\na generalized plan, often represented with a ﬁnite-state ma-\nchine (Levesque 2005; Srivastava et al. 2011; Winner 2008);\nand (3) discovering state and action abstractions and then\nusing them in a generalized plan (Bonet and Geffner 2018).\nOne pervasive challenge is that there are often many valid\nplans for any given task, and only some of these plans are\nconsistent with a simple generalized plan. PG3 addresses\nthis challenge by using candidate generalized plans (rep-\nresented as lifted decision list goal-conditioned policies) to\nconstrain the generation of example plans (Yang et al. 2022).\nWe use PG3 as the main point of comparison in experiments.\nLLMs for Code Generation. Our work builds on recent\ntechniques that use LLMs for code generation (Chen et al.\n2021; Nijkamp et al. 2023). CoT summarization is related to\nseveral techniques that ask the LLM to outline its “thinking”\nbefore arriving at a ﬁnal implementation (Wei et al. 2022;\nJiang et al. 2023; Zheng et al. 2023). A number of recent\nworks also use programs as prompts (i.e., a structured chain\nof thought) in an attempt to help LLMs perform mathemat-\nical reasoning (Gao et al. 2022; Imani, Du, and Shrivastava\n2023). Related to our automated debugging, Xia and Zhang\n(2023); Chen et al. (2023b) consider automated program re-\npair by re-prompting the LLM with feedback from failed\nvalidation checks. Chen et al. (2023a) consider a related\nparadigm, but where feedback comes from humans, rather\nthan automated checks. Also relevant are efforts to generate\ncode that can be used for robotic decision-making (Liang\net al. 2022; Singh et al. 2022). Beyond LLMs, code genera-\ntion has been studied extensively in program synthesis (Alur\net al. 2013; Gulwani et al. 2017) and inductive logic pro-\ngramming (Muggleton 1991; Cropper and Duman ˇci´c 2022).\nBackground and Problem Setting\nPDDL Domains and Tasks. We consider deterministic,\nfully-observed planning tasks represented in PDDL. In ex-\nperiments, we use the STRIPS subset with types and neg-\native preconditions. We describe PDDL informally and\nrefer the reader to other references for a formal treat-\nment (McDermott 2000). A PDDL domain is character-\nized by a name, a set of types, a set of predicates, and\na set of operators. For example, in the Delivery domain,\na robot must pick up newspapers from a home base\nand then deliver them to certain locations. The domain\nhas two types: loc and paper. One predicate is (at\n?l - loc), where ?l is a placeholder for a loc ob-\nject. The domain has three operators: (pick-up ?p -\npaper ?l - loc), (move ?from - loc ?to -\nloc), (deliver ?p - paper ?l - loc). For ex-\nample, the pick-up operator in its entirety is:\n(:action pick-up\n:parameters (?p - paper ?l - loc)\n:precondition (and (at ?l)\n(isHomeBase ?l)\n(unpacked ?p))\n:effect (and\n(not (unpacked ?p))\n(carrying ?p)))\nA PDDL task is characterized by a domain, a set of ob-\njects, an initial state, and a goal. An object has a name and\na type, e.g., paper1 - paper. A ground atom is a pred-\nicate and a tuple of objects of the appropriate types, e.g.,\n(unpacked paper1). A state consists of a conjunction\nof ground atoms that are true, assuming all other ground\natoms to be false. A goal is a conjunction of ground atoms\nthat must be true in any goal state. (More general goal ex-\npressions are also possible in PDDL.) For example, in De-\nlivery, the goal may include (satisfied loc1) and\n(satisfied loc2).\nAn action is an operator and a tuple of objects of the\nappropriate types, e.g., (pick-up paper1 loc4). The\noperator’s preconditions determine whether the action is ap-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n20257\nplicable and the effects deﬁne what ground atoms would be\nadded or deleted if the operator is executed. A plan is a ﬁnite\nsequence of actions. The plan is valid for a task if all actions\nare applicable when executed in succession from the initial\nstate and if the ﬁnal state is a goal state.\nPDDL domains, types, predicates, operators, objects, and\ntypes often include human-readable names like the ones\nshown above. These names are not important for standard\nAI planners or previous generalized planning approaches.\nHowever, the names are very important for humans—and,\nwe expect, for LLMs—trying to make sense of the PDDL.\nGeneralized Planning in PDDL Domains. A general-\nized planning instance is characterized by a PDDL domain\nand a distribution of tasks. A small set of training tasks (10\nor fewer in experiments) from the distribution is given at\ntraining time. A set of held-out evaluation tasks—typically\ninvolving many more objects—are used to measure perfor-\nmance. The objective is to use the training tasks to synthe-\nsize a program that will produce valid plans for all of the\nevaluation tasks. We consider an evaluation task solved if\nthe program returns a valid plan within a ﬁxed wall-clock\ntime budget (30 seconds in experiments). In other words, we\nare interested in satisﬁcing, not optimal, planning, and our\nprimary concern is the efﬁciency of planning itself.\nGeneralized Planning with LLMs\nWe are interested in the extent to which pretrained large\nlanguage models (LLMs) can be used for generalized\nplanning in PDDL domains. We assume familiarity with\nLLMs (Brown et al. 2020; Chen et al. 2021; Chowdhery\net al. 2022; OpenAI 2023). To use LLMs for generalized\nplanning, we need to deﬁne a protocol for prompting.\nPrompting Protocol\nPrevious work on Chain-of-Thought (CoT) prompting has\nshown that asking an LLM to “think step by step” can im-\nprove performance in reasoning tasks (Wei et al. 2022).\nWith these results in mind, we hypothesized that decompos-\ning generalized planning into three stages—domain summa-\nrization, strategy proposal, and strategy implementation—\nwould improve performance.\nDomain Summarization. Our ﬁrst prompt to the LLM is\nin the following form:\nDomain: [PDDL Domain]\nExample problems: [PDDL Training Tasks]\nWrite a short summary of this domain in words.\nTo compensate for the limited context window size of\ntransformer-based LLMs like GPT-4, we abbreviate the en-\ncoding of the training tasks in two ways. First, we always use\nonly two training tasks, even when more are given. Second,\nwithin each training task, we limit the number of objects and\ninitial state ground atoms shown. For each object type, if the\nnumber of objects of that type exceeds 10, we truncate the\nobject set and add ellipses. Similarly, for each predicate, if\nthe number of ground atoms with that predicate exceeds 10,\nwe truncate and add ellipses. The fact that we only need to\ncommunicate the “gist” of the task distribution, rather than\nwhole tasks, is another advantage of generalized planning\nwith LLMs versus planning with LLMs.\nStrategy Proposal. After the LLM responds to the ﬁrst\nprompt, we ask for a generalized planning strategy:\nThere is a simple strategy for solving all problems in this\ndomain without using search. What is that strategy?\nIn preliminary experiments, omitting the phrase “without\nusing search” would often lead the LLM to propose a search-\nbased planning strategy.\nStrategy Implementation. Finally, we ask the LLM to\nimplement the strategy as a Python program:\nImplement the strategy as a Python function. The code should\nbe of the form\ndef get_plan(objects, init, goal):\n# Your code here\nreturn plan\nwhere\n• objects is a set of (object name, type name) tuples\n• init is a set of ground atoms represented as tuples of pred-\nicate names and arguments (e.g., (‘predicate-foo’, ‘object-\nbar’, ...))\n• goal is also a set of ground atoms represented in the same\nway\n• plan is a list of actions, where each action is a ground oper-\nator represented as a string (e.g., ‘(operator-baz object-qux\n...)’)\nIn domains without object types, objects is instead just\na set of object names.\nAutomated Interactive Debugging\nAfter the LLM has proposed an implementation of\nget plan, we use the training tasks to validate the imple-\nmentation. For each training task, we execute get plan\nuntil it returns an output, throws an exception, or reaches a\ntimeout (30 seconds). If the output is a valid plan, we con-\ntinue onto the next training task. Otherwise, we re-prompt\nwith one of four types of feedback.\nPython Exceptions. If executing get\nplan results in a\nPython exception, we capture the traceback and report it to\nthe LLM along with the input. An example is shown below,\nwith the traceback abbreviated for clarity.\nGiven this task: [PDDL Training Task]\nThe code raised the following exception:\nFile \"<file-name-omitted>\", line 86\nlift_at = {atom[1]: atom[2] ...}\n˜˜˜˜ˆˆˆ\nIndexError: tuple index out of range\nFix the code.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n20258\nIn preliminary experiments, we found that including the\nfull traceback can improve performance.\nTimeout. If get plan does not ﬁnish before the time-\nout, we report to the LLM that the program did not ﬁnish\nand suggest that an inﬁnite loop may be to blame. We also\nprovide a traceback showing where the program was execut-\ning when it was interrupted. An example is shown below.\nGiven this task: [PDDL Training Task]\nThe code raised the following exception:\nFile \"<file-name-omitted>\", line 23\nwhile not any(span_loc[1] == ...:\nˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆ\nKeyboardInterrupt\nThe code was interrupted because it timed out (possible\ninﬁnite loop).\nFix the code.\nThe traceback is again abbreviated for clarity. Note that\nthe KeyboardInterrupt is automatically thrown after\n30 seconds. In practice, nearly all timeouts we observe are\ndue to logic errors in the code, rather than inefﬁcient but\ncorrect implementations.\nPlan Syntax. If get\nplan returns an output, we check\nits syntax: whether it is a list of strings, whether each string\nis enclosed in parentheses and space-separated, and whether\nthe action names, object names, and number of objects per\naction are valid with respect to the domain and task. If any\nof these checks fail, we report the failure to the LLM. For\nthis type of failure, we also remind the LLM about the valid\noperators. An example is shown below.\nGiven this task: [PDDL Training Task]\nThe code returned this plan:\n['walk r0_c0 r0_c1', 'walk ...]\nHowever, the action walk r0\nc0 r0 c1 is invalid at\nstep 0. NOTE: the valid operators are: (climb ?from\n?to) (walk ?from ?to).\nFix the code.\nThe full plan is shown to the LLM but abbreviated in the\nexample for clarity. The issue in this example is that the ac-\ntions are not enclosed in parentheses.\nPlan Semantics. If all of the previous checks pass, we\nuse the V AL tool (Howey, Long, and Fox 2004) to check\nwhether the get\nplan output is a semantically valid plan.\nIf not, V AL provides “plan repair advice”, e.g., if there is an\naction with invalid preconditions. We extract this plan repair\nadvice and report it to the LLM. Note that we use this advice\nnot to repair the plan, but rather, to repair the generalized\nplan. An example is shown below.\nGiven this task: [PDDL Training Task]\nThe code failed. It returned the following plan:\n['(pick-up paper-1 loc-0)', ...].\nNOTE: (pick-up paper-0 loc-0) has an unsat-\nisﬁed precondition at time 3\n(Set (at loc-0) to true)\nFix the code.\nAdditional Details. After re-prompting the LLM, we re-\npeat the process of checking the code and reporting any fail-\nures up to four times. To handle rare cases where the LLM\nimplements its own helper functions and then assumes dur-\ning debugging that the helper functions are still available,\nwe append each new response from the LLM to a growing\nPython ﬁle, rather than overwriting the previous responses.\nIf a failure is still encountered on the last attempt, the ﬁnal\nresponse is used during evaluation.\nExperiments and Results\nThrough experiments, we address these questions: 1. Can\nGPT-4 be used for generalized (PDDL) planning? 2. Are the\nsynthesized programs efﬁcient? 3. Does CoT summarization\nhelp? 4. Does automated debugging help? 5. To what extent\ndoes GPT-4 rely on names in the PDDL? 6. How does GPT-\n4 compare to GPT-3.5? 7. Do each of the four error types\nhelp? 8. How many training tasks are needed?\nExperimental Setup\nWe evaluate nine generalized planning approaches on seven\nPDDL domains over 10 random seeds. Tasks are randomly\ngenerated for each seed.\nDomains. The ﬁrst six domains (and tasks) are taken di-\nrectly from the previous work by Yang et al. (2022). Of\nthese, four (Gripper, Miconic, Ferry, Spanner) are standard\nplanning benchmarks and the other two (Delivery, Forest)\nwere introduced by that work. The last domain (Heavy) is\nnew to this work. The pretraining data for GPT-4 is not pub-\nlicly available, but it is likely that the domain deﬁnitions for\nat least the four standard domains were included in that data.\nHowever, we believe it is unlikely that generalized plans\nwere included, and for the Heavy domain, we can guaran-\ntee that neither the domain nor generalized plans were in-\ncluded. We now brieﬂy describe each domain. Unless other-\nwise speciﬁed, there are 10 training tasks and 30 evaluation\ntasks per domain and seed.\n• Delivery: Newspapers at a home base must be delivered\nto multiple locations. There are ﬁve training tasks with\n9–17 objects; evaluation tasks have 70–100 objects.\n• Forest: A hiker must navigate a 2D grid to reach a\ngoal location while climbing hills and avoiding water. A\nmarked trail leads to the goal, but there are shorter paths\nthrough dirt. There are 4 training tasks with 64-100 ob-\njects; evaluation tasks have 100–144 objects.\n• Gripper: Balls must be transported between rooms by a\nrobot with two grippers. Training tasks have 20–30 ob-\njects; evaluation tasks have 60–80 objects.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n20259\nDomain GPT-4 No CoT No Debug No Names GPT-3.5 PG3 Policy Eval Plan Compare Random\nDelivery 0.90 0.70 0.10 0.10 0.00 1.00 0.00 0.10 0.00\nForest 1.00 1.00 0.62 0.11 0.32 1.00 1.00 0.16 0.03\nGripper 0.90 0.80 0.50 0.10 0.00 1.00 0.00 0.20 0.00\nMiconic 0.01 0.13 0.00 0.00 0.00 1.00 0.00 0.10 0.13\nFerry 0.80 0.20 0.26 0.00 0.00 1.00 0.00 0.90 0.00\nSpanner 0.10 0.00 0.00 0.00 0.00 1.00 1.00 0.56 0.06\nHeavy 0.60 1.00 0.20 0.00 0.00 0.00 0.00 0.00 0.00\nTable 1: Fraction of evaluation tasks solved. All results are averaged over 10 random seeds and 30 evaluation tasks per seed.\nFigure 2: GPT-4 synthesized program runtime compared to a state-of-the-art planner (Fast Downward). Note the log-log axes.\nEach point is a median over 10 newly generated tasks, over all seeds where generalized planning solved all evaluation tasks.\nFigure 3: Fraction of evaluation tasks solved by GPT-4 ver-\nsus number of debugging steps allowed, averaged over all\ndomains and seeds. The shaded region is standard error.\n• Miconic: Passengers in multiple buildings, each with an\nelevator, must be picked up and dropped off on different\nﬂoors. Training tasks have 6–30 objects; evaluation tasks\nhave 11–150 objects.\n• Ferry: Cars must be sailed between islands using a ferry\nthat can carry at most one car. Training tasks have 13–20\nobjects; evaluation tasks have 30–50 objects.\n• Spanner: Wrenches (spanners) and nuts are distributed\nalong a one-way corridor. An agent must move down the\ncorridor, pick up wrenches, and tighten the nuts, using\neach wrench at most once. Training tasks have 9–15 ob-\njects; evaluation tasks have 30–60 objects.\n• Heavy: Items must be stacked into an empty box. An\nitem can only be stacked on another item if the lat-\nter is heavier. The weight relations are expressed via\na (heavier ?x ?y) predicate. One challenge is in\nError Type All Success Failure\nPython Exception 40.0 28.9 42.5\nPlan Semantics 34.0 44.7 31.4\nPlan Syntax 13.0 18.4 11.7\nTimeout 13.0 8.0 14.4\nTable 2: Percentages of error types encountered by GPT-4 in\ntraining tasks over all domains and seeds. “All” is the break-\ndown for all training tasks; “Success” is the breakdown for\ntrials where all evaluation tasks were subsequently solved;\n“Failure” is the breakdown for the non-Success trials.\ndetermining which item to place into the box ﬁrst, i.e.,\nwhich item is the heaviest. Training tasks have 3–10 ob-\njects; evaluation tasks have 100–250 objects.\nApproaches. We evaluate the main approach, four abla-\ntions, and four baselines. The baselines are taken from the\nwork by Yang et al. (2022); see that work for details.\n• GPT-4: Our main approach with CoT summarization\nand automated debugging.\n• No CoT: An ablation of the main approach that does not\nuse CoT summarization. The three initial prompts are\ncombined and “Write a short summary of this domain in\nwords.” and “What is that strategy?” are removed.\n• No Debug: An ablation of the main approach that does\nnot use automated debugging. The ﬁrst implementation\nof get\nplan is used for evaluation.\n• No Names: An ablation of the main approach where all\nnames in the PDDL domains and tasks are replaced with\nnondescriptive identiﬁers. For instance, predicates are\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n20260\nrenamed to predicate1, predicate2, etc., opera-\ntors are renamed to operator1, operator2, etc. Al-\ntogether, the names of the domain, problem, predicates,\noperators, variables, types, and objects are ablated.\n• GPT-3.5: GPT-3.5 with CoT summarization and auto-\nmated debugging.\n• PG3: The generalized planning approach proposed by\nYang et al. (2022). The synthesized programs are goal-\nconditioned policies implemented as lifted decision lists.\nSynthesis is performed via heuristic search in policy\nspace with their novel heuristic.\n• Policy Evalulation (PE): An approach from Yang et al.\n(2022) that is identical to PG3 except that the heuristic\nused for policy search is sparse: each candidate policy is\nscored based on the number of training tasks solved.\n• Plan Compare (PC): Another approach from Yang et al.\n(2022) that is identical to PG3 except for the policy\nsearch heuristic: example plans for each training task are\ngenerated ofﬂine, and the policy is scored based on its\nagreement with the example plans.\n• Random: Valid actions are randomly sampled and ex-\necuted until a dead-end is encountered, the goal is\nreached, or a maximum horizon (default 1000, but see\nthe previous work) is exceeded.\nExperimental Details. We used a Macbook Pro laptop\nwith an M1 chip and 64 GB RAM. Since an API for GPT-\n4 is not publicly available, we used the ChatGPT browser\ninterface for all experiments (including the GPT-3.5 base-\nline). The pipeline is fully automated except that prompts\nand responses are manually copied and pasted between the\nterminal and browser, with the clipboard programmatically\nupdated. To facilitate reproducibility, we have released all\nchat logs and code.\nResults and Analysis\nMain results are presented in Table 1. Examples of syn-\nthesized programs are presented in the appendix. Overall,\nthe performance of GPT-4 with CoT summarization and au-\ntomated debugging is strong in Delivery, Forest, Gripper,\nFerry, and Heavy, and poor in Miconic and Spanner. Note\nthat the reported success rates are averaged over all LLM\nconversations. In practice, performance could be boosted by\nrestarting the conversation multiple times and using the best-\nfound program (Chowdhery et al. 2022). The strong per-\nformance in Heavy is especially notable. The generalized\nplanning baselines fail in this domain because lifted deci-\nsion lists are overly restrictive as program representations\nand cannot discover a concept like “heaviest overall” from\npairwise heavier relations. GPT-4’s ability to write general\nPython code is one of its biggest advantages as a general-\nized planning approach.\nWe also observe that in nearly all cases, GPT-4 either (1)\nsolves all of the training tasks and then solves all of the eval-\nuation tasks; or (2) fails to solve at least one training task and\nthen fails to solve all of the evaluation tasks. In other words,\noverﬁtting to the training tasks is very rare, and evaluation\nperformance is typically all-or-nothing. See Table 3 in the\nappendix for the maximum fraction of tasks solved.\nMiconic failures. GPT-4 has a number of consistent fail-\nure modes in Miconic. First, at the strategy proposal level,\nit often fails to recognize that there can be multiple build-\nings, each with their own elevator. This is admittedly difﬁ-\ncult to recognize given the PDDL encoding: buildings ex-\nist only implicitly based on the above relation between\nﬂoor objects. For example, one would need to see that\nneither (above f1\nb1, f1 b2) nor (above f1 b2,\nf1 b1) are true and conclude that the ﬂoors are in two\ndifferent buildings. However, especially after automated de-\nbugging, GPT-4 can realize that there are multiple buildings,\nand furthermore, that building names (e.g., b1, b2) can be\nextracted from the ﬂoor names. But then other failures often\noccur, for example, attempting and failing to create a total\nordering of the ﬂoors from the above predicate. Overall,\nwe believe that Miconic is just beyond the limit of GPT-4’s\ncurrent capabilities and would likely be solved by the next\ngeneration of LLMs, or by GPT-4 with additional guidance.\nSpanner failures. GPT-4 consistently fails in Spanner\nduring strategy proposal. In particular, GPT-4 does not ap-\npear to realize that locations in Spanner are connected in a\none-way chain. The strategy proposed is often “ﬁrst collect\nall of the spanners, then tighten all of the nuts” or similar.\nA correct strategy would instead be to “move to each lo-\ncation in the chain, picking up any spanners and tightening\nany nuts at each location.” Recognizing the existence of the\none-way chain requires examining the link atoms in the\ntraining problems. Even after automated debugging, GPT-4\noften assumes, incorrectly, that links are commutative.\nProgram efﬁciency. Although we prompt the LLM to\nimplement a “simple” program that does not use search,\nit is still possible for the LLM to produce a program that\ndoes use search or is slow for other reasons (e.g., poor algo-\nrithmic complexity). We therefore measure synthesized pro-\ngram runtime. As a baseline for our comparison we use a\nstate-of-the-art domain-independent PDDL planner LAMA\n(Richter and Westphal 2010) via Fast Downward (Helmert\n2006), stopping after the ﬁrst plan is found.\n1 In Figure 2, we\nplot wall-clock runtimes as a function of problem size (num-\nber of objects). Overall, we see that the synthesized pro-\ngrams not only scale favorably with respect to the planner,\nbut also consistently beat the planner in absolute runtime\nby large margins. This is notable given that the LLM syn-\nthesizes Python programs, while the PDDL planner uses a\nhighly optimized combination of Python and C++ code. The\nbottleneck for Fast Downward is often operator grounding.\nThe LLM’s programs do not need to ground operators—they\ncan go directly from task to plan.\nThe role of CoT. Comparing GPT-4 to No CoT, we see\nthat the impact of CoT summarization is mixed: it seems to\nhelp in most cases, but hurt in Miconic and Heavy. Miconic\nis an especially interesting case. When using CoT summa-\nrization, GPT-4 nearly always proposes a “sweep” strategy,\nwhere the elevator(s) are ﬁrst moved to the bottom ﬂoor;\nthen moved up one ﬂoor at a time until the top ﬂoor, picking\nup and dropping off passengers along the way; then moved\n1Our intention is not to compare planners, but rather to provide\na frame of reference for runtime.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n20261\ndown one ﬂoor at a time, again picking up and dropping\noff passengers. This strategy would work in theory, but it\nrequires ﬁnding a total ordering of ﬂoors within buildings.\nWithout CoT, GPT-4 often attempts a different strategy: pick\nup, move, and drop off each passenger, one at a time. The\nlatter strategy does not require a total ordering over ﬂoors\nand is arguably simpler to implement in Python. This exam-\nple shows that CoT can inﬂuence the strategy proposed by\nGPT-4. Moreover, strategies that are “simple” to describe in\nnatural language may not be simple to implement in code. In\nHeavy, there is not a clear difference in strategies with and\nwithout CoT. Since a good strategy is evidently discernible\nfrom the PDDL alone, it is possible that CoT “distracts”\nGPT-4 during implementation.\nThe role of automated debugging. Comparing GPT-4 to\nNo Debug, we see that automated debugging generally im-\nproves performance dramatically. Figure 3 shows that even\none step of automated debugging helps substantially, and\nfurther steps exhibit diminishing marginal improvements.\nTable 2 reports the fraction of error types encountered dur-\ning training across. Python exceptions are most common,\nfollowed closely by errors in plan semantics, then errors in\nplan syntax, and ﬁnally timeouts. We also see that the error\ntypes are well-distributed within successful trials, suggest-\ning that each of the four types of feedback are beneﬁcial.\nIn general, GPT-4 tends to make small, local corrections to\ncode during automated debugging. If the code is structurally\nﬂawed and requires a signiﬁcant rewrite, restarting the dia-\nlogue from the beginning may be required.\nThe role of PDDL names. Examining the results for the\nNo Names ablation, we see performance overall is very poor.\nThis conﬁrms our hypothesis that the terms present in the\nPDDL domains and tasks are helpful to the LLM, as they\nwould be to a human. Note that planners like Fast Downward\nand generalized planners like PG3 would be unaffected by\nname changes. However, there are a few cases where the No\nNames ablation does succeed, suggesting that the LLM has\nsome capacity for purely syntactic generalized planning.\nGPT-3.5 vs. GPT-4. Examining the results for GPT-3.5,\nwe see that it performs much worse than GPT-4. This is con-\nsistent with other reports (OpenAI 2023; Bubeck et al. 2023)\nthat GPT-4 is far superior on reasoning and coding tasks.\nQualitatively, the programs proposed by GPT-3.5 are ﬂawed\nin myriad ways and do not usually appear “close”. They also\ndo not seem to improve with automated debugging.\nData efﬁciency. In the appendix, we analyze the number\nof training tasks used in each successful trial. A training task\nis used if it appeared in the prompt and/or triggered feedback\nduring automated debugging. Since two training tasks are\nalways used in the prompt, the minimum used is two. Inter-\nestingly, in the vast majority of cases, only those two train-\ning tasks are used. During automated debugging, these two\nprompting tasks are always checked ﬁrst, and most of the\ntime, they are sufﬁcient to identify issues. In a small number\nof cases, a third task is also used during automated debug-\nging. This result speaks to the strong few-shot learning ca-\npabilities of GPT-4. We expect that in many cases, even one\ntraining task would sufﬁce, although we did witness a drop\nin performance in preliminary experiments with one task.\nDiscussion and Future Work\nIn this work, we showed that GPT-4 with CoT summariza-\ntion and automated debugging is a surprisingly strong gen-\neralized planner in PDDL domains. We conclude with limi-\ntations of this work, reﬂections about the implications of our\nﬁndings, and opportunities for future work.\nLimitations. A major limitation of this work and previ-\nous work on generalized planning is that it is easy enough\nto hand-design generalized plans for all of the domains con-\nsidered. Nonetheless, we expect this line of work to be prac-\ntically useful for at least three reasons. (1) In some cases,\nit may be considerably easier to specify PDDL domain and\nproblem descriptions than it is to directly specify a general-\nized plan. (2) In a fully autonomous system, where opera-\ntors and predicates are learned in association to natural lan-\nguage, we would want the system to also synthesize gener-\nalized plans autonomously. (3) Beyond PDDL, generalized\nplanning with LLMs would be an even more attractive op-\ntion, since other approaches rely strongly on formal speciﬁ-\ncations. Another limitation of this work is our use of training\ntasks to communicate the task distribution of interest to the\nLLM. In general, a few example tasks may be insufﬁcient to\nexpress the full distribution. Other representations like nat-\nural language or procedural generation code may be better,\nbut would require more human input.\nIs (generalized) planning now obsolete? No. First, there\nremains a performance gap between GPT-4 and PG3, and\nother generalized planners may be even better. However,\neven if this gap is closed by the next generation of LLMs, we\nwould still say no. Planning remains essential in domains\nwhere no simple program exists. An interesting direction\nfor future work would be automatically detecting whether\na simple program might exist before attempting to synthe-\nsize one. We tried the Sokoban domain and found that GPT-\n4 correctly indicates that no simple program exists. How-\never, this property of Sokoban is well-known, so it is likely\nparroting pretraining data. We also tried the Slitherlink do-\nmain, which was featured in the 2023 International Planning\nCompetition, and found that GPT-4 did not recognize that no\nsimple strategy exists (Takayuki 2000). Generalized plan-\nning without LLMs also remains important in cases where\ndomain descriptions are not human-readable, e.g., because\nthe predicates or operators are learned (Silver et al. 2023).\nEven with natural language descriptions, combining “classi-\ncal” approaches with LLMs may be best.\nWhat if we gave the LLM access to a planner? Giving\nan LLM access to APIs is a very powerful idea (Schick et al.\n2023) and one such API could be a PDDL planner (Liu et al.\n2023). An LLM could potentially use such a planner for\ngeneralized planning, especially given that approaches like\nPG3 rely on access to a planner to generate example plans.\nIn some domains, generating example plans naively would\nlikely confuse the LLM. For example, plans generated in\nthe Forest domain would follow arbitrary paths through the\ndirt rather than following the slightly longer marked trail. In\nother cases, though, example plans could be very useful, es-\npecially if the LLM generates them in a targeted way. Lever-\naging diverse plans (Sohrabi et al. 2016; Katz and Sohrabi\n2020) could be particularly useful.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n20262\nReferences\nAhn, M.; Brohan, A.; Brown, N.; Chebotar, Y .; Cortes,\nO.; David, B.; Finn, C.; Gopalakrishnan, K.; Hausman,\nK.; Herzog, A.; et al. 2022. Do as I can, not as I say:\nGrounding language in robotic affordances. arXiv preprint\narXiv:2204.01691.\nAlur, R.; Bodik, R.; Juniwal, G.; Martin, M. M.;\nRaghothaman, M.; Seshia, S. A.; Singh, R.; Solar-Lezama,\nA.; Torlak, E.; and Udupa, A. 2013. Syntax-guided synthe-\nsis. IEEE.\nBonet, B.; and Geffner, H. 2015. Policies that generalize:\nSolving many planning problems with the same policy. In\nTwenty-Fourth International Joint Conference on Artiﬁcial\nIntelligence.\nBonet, B.; and Geffner, H. 2018. Features, Projections, and\nRepresentation Change for Generalized Planning. CoRR,\nabs/1801.10055.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners. Ad-\nvances in neural information processing systems, 33: 1877–\n1901.\nBubeck, S.; Chandrasekaran, V .; Eldan, R.; Gehrke, J.;\nHorvitz, E.; Kamar, E.; Lee, P.; Lee, Y . T.; Li, Y .; Lundberg,\nS.; et al. 2023. Sparks of artiﬁcial general intelligence: Early\nexperiments with gpt-4. arXiv preprint arXiv:2303.12712.\nChapman, D. 1987. Planning for Conjunctive Goals. Artiﬁ-\ncial Intelligence, 32: 333–377.\nChen, A.; Scheurer, J.; Korbak, T.; Campos, J. A.; Chan,\nJ. S.; Bowman, S. R.; Cho, K.; and Perez, E. 2023a. Improv-\ning Code Generation by Training with Natural Language\nFeedback. arXiv:2303.16749.\nChen, M.; Tworek, J.; Jun, H.; Yuan, Q.; Pinto, H. P. d. O.;\nKaplan, J.; Edwards, H.; Burda, Y .; Joseph, N.; Brockman,\nG.; et al. 2021. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374.\nChen, X.; Lin, M.; Sch ¨arli, N.; and Zhou, D. 2023b. Teach-\ning large language models to self-debug. arXiv preprint\narXiv:2304.05128.\nChowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra,\nG.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.;\nGehrmann, S.; et al. 2022. Palm: Scaling language modeling\nwith pathways. arXiv preprint arXiv:2204.02311.\nCollins, K. M.; Wong, C.; Feng, J.; Wei, M.; and Tenen-\nbaum, J. B. 2022. Structured, ﬂexible, and robust: bench-\nmarking and improving large language models towards more\nhuman-like behavior in out-of-distribution reasoning tasks.\narXiv preprint arXiv:2205.05718.\nCropper, A.; and Duman ˇci´c, S. 2022. Inductive logic pro-\ngramming at 30: a new introduction. Journal of Artiﬁcial\nIntelligence Research, 74: 765–850.\nFikes, R. E.; Hart, P. E.; and Nilsson, N. J. 1972. Learn-\ning and executing generalized robot plans. Artiﬁcial intelli-\ngence, 3: 251–288.\nGao, L.; Madaan, A.; Zhou, S.; Alon, U.; Liu, P.; Yang, Y .;\nCallan, J.; and Neubig, G. 2022. PAL: Program-aided Lan-\nguage Models. arXiv preprint arXiv:2211.10435.\nGulwani, S.; Polozov, O.; Singh, R.; et al. 2017. Program\nsynthesis. Foundations and Trends® in Programming Lan-\nguages, 4(1-2): 1–119.\nHelmert, M. 2006. The fast downward planning system.\nJournal of Artiﬁcial Intelligence Research, 26: 191–246.\nHowey, R.; Long, D.; and Fox, M. 2004. V AL: Automatic\nplan validation, continuous effects and mixed initiative plan-\nning using PDDL. In 16th IEEE International Conference\non Tools with Artiﬁcial Intelligence, 294–301. IEEE.\nHuang, W.; Abbeel, P.; Pathak, D.; and Mordatch, I. 2022a.\nLanguage Models as Zero-Shot Planners: Extracting Action-\nable Knowledge for Embodied Agents. In International\nConference on Machine Learning (ICML).\nHuang, W.; Xia, F.; Xiao, T.; Chan, H.; Liang, J.; Flo-\nrence, P.; Zeng, A.; Tompson, J.; Mordatch, I.; Chebotar,\nY .; et al. 2022b. Inner Monologue: Embodied Reasoning\nthrough Planning with Language Models. arXiv preprint\narXiv:2207.05608.\nImani, S.; Du, L.; and Shrivastava, H. 2023. MathPrompter:\nMathematical Reasoning using Large Language Models.\narXiv:2303.05398.\nJiang, X.; Dong, Y .; Wang, L.; Shang, Q.; and Li, G.\n2023. Self-planning Code Generation with Large Language\nModel. arXiv preprint arXiv:2303.06689.\nJim´enez, S.; and Jonsson, A. 2015. Computing plans with\ncontrol ﬂow and procedures using a classical planner. In\nProceedings of the Eighth Annual Symposium on Combina-\ntorial Search, SOCS-15, 62–69.\nJim´enez, S.; Segovia-Aguas, J.; and Jonsson, A. 2019. A re-\nview of generalized planning. The Knowledge Engineering\nReview, 34.\nKatz, M.; and Sohrabi, S. 2020. Reshaping diverse plan-\nning. In Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, 06, 9892–9899.\nLevesque, H. 2005. Planning with Loops. In IJCAI.\nLevine, J.; and Humphreys, D. 2003. Learning action strate-\ngies for planning domains using genetic programming. In\nWorkshops on Applications of Evolutionary Computation ,\n684–695. Springer.\nLiang, J.; Huang, W.; Xia, F.; Xu, P.; Hausman, K.; Ichter,\nB.; Florence, P.; and Zeng, A. 2022. Code as policies: Lan-\nguage model programs for embodied control. arXiv preprint\narXiv:2209.07753.\nLin, B. Y .; Huang, C.; Liu, Q.; Gu, W.; Sommerer, S.; and\nRen, X. 2022. On Grounded Planning for Embodied Tasks\nwith Language Models. arXiv preprint arXiv:2209.00465.\nLin, K.; Agia, C.; Migimatsu, T.; Pavone, M.; and Bohg, J.\n2023. Text2motion: From natural language instructions to\nfeasible plans. arXiv preprint arXiv:2303.12153.\nLiu, B.; Jiang, Y .; Zhang, X.; Liu, Q.; Zhang, S.; Biswas, J.;\nand Stone, P. 2023. LLM+ P: Empowering Large Language\nModels with Optimal Planning Proﬁciency. arXiv preprint\narXiv:2304.11477.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n20263\nMcDermott, D. 2000. The 1998 AI Planning Systems Com-\npetition. AI Magazine, 21(2): 35–55.\nMuggleton, S. 1991. Inductive logic programming. New\ngeneration computing, 8: 295–318.\nNijkamp, E.; Pang, B.; Hayashi, H.; Tu, L.; Wang, H.; Zhou,\nY .; Savarese, S.; and Xiong, C. 2023. CodeGen: An Open\nLarge Language Model for Code with Multi-Turn Program\nSynthesis. arXiv:2203.13474.\nOpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.\nPallagani, V .; Muppasani, B.; Murugesan, K.; Rossi, F.;\nHoresh, L.; Srivastava, B.; Fabiano, F.; and Loreggia, A.\n2022. Plansformer: Generating Symbolic Plans using Trans-\nformers. arXiv preprint arXiv:2212.08681.\nRaman, S. S.; Cohen, V .; Rosen, E.; Idrees, I.; Paulius,\nD.; and Tellex, S. 2022. Planning with Large Lan-\nguage Models via Corrective Re-prompting. arXiv preprint\narXiv:2211.09935.\nRichter, S.; and Westphal, M. 2010. The LAMA planner:\nGuiding Cost-based Anytime Planning with Landmarks.\nJournal of Artiﬁcial Intelligence Research, 39: 127–177.\nRivlin, O.; Hazan, T.; and Karpas, E. 2020. General-\nized Planning With Deep Reinforcement Learning. arXiv\npreprint arXiv:2005.02305.\nSchaul, T.; Horgan, D.; Gregor, K.; and Silver, D. 2015. Uni-\nversal value function approximators. In International con-\nference on machine learning, 1312–1320. PMLR.\nSchick, T.; Dwivedi-Yu, J.; Dess `ı, R.; Raileanu, R.; Lomeli,\nM.; Zettlemoyer, L.; Cancedda, N.; and Scialom, T. 2023.\nToolformer: Language models can teach themselves to use\ntools. arXiv preprint arXiv:2302.04761.\nSegovia-Aguas, J.; Jim ´enez, S.; and Jonsson, A. 2018. Com-\nputing hierarchical ﬁnite state controllers with classical\nplanning. Journal of Artiﬁcial Intelligence Research, 62:\n755–797.\nSegovia-Aguas, J.; Jim ´enez, S.; and Jonsson, A. 2021. Gen-\neralized Planning as Heuristic Search. In Proceedings of\nthe International Conference on Automated Planning and\nScheduling, volume 31, 569–577.\nSharma, P.; Torralba, A.; and Andreas, J. 2022. Skill Induc-\ntion and Planning with Latent Language. In Proceedings of\nthe 60th Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), 1713–1726.\nSilver, T.; Chitnis, R.; Kumar, N.; McClinton, W.; Lozano-\nPerez, T.; Kaelbling, L. P.; and Tenenbaum, J. 2023. Predi-\ncate Invention for Bilevel Planning. In AAAI Conference on\nArtiﬁcial Intelligence (AAAI).\nSilver, T.; Hariprasad, V .; Shuttleworth, R. S.; Kumar, N.;\nLozano-P´erez, T.; and Kaelbling, L. P. 2022. PDDL Plan-\nning with Pretrained Large Language Models. In NeurIPS\n2022 Foundation Models for Decision Making Workshop.\nSingh, I.; Blukis, V .; Mousavian, A.; Goyal, A.; Xu, D.;\nTremblay, J.; Fox, D.; Thomason, J.; and Garg, A. 2022.\nProgprompt: Generating situated robot task plans using large\nlanguage models. arXiv preprint arXiv:2209.11302.\nSohrabi, S.; Riabov, A. V .; Udrea, O.; and Hassanzadeh, O.\n2016. Finding diverse high-quality plans for hypothesis gen-\neration. In ECAI 2016, 1581–1582. IOS Press.\nSrivastava, S. 2011. Foundations and applications of gener-\nalized planning. AI Communications, 24(4): 349–351.\nSrivastava, S.; Immerman, N.; Zilberstein, S.; and Zhang, T.\n2011. Directed Search for Generalized Plans Using Classi-\ncal Planners. In ICAPS.\nSutton, R. S.; Modayil, J.; Delp, M.; Degris, T.; Pilarski,\nP. M.; White, A.; and Precup, D. 2011. Horde: A scalable\nreal-time architecture for learning knowledge from unsuper-\nvised sensorimotor interaction. In The 10th International\nConference on Autonomous Agents and Multiagent Systems-\nVolume 2, 761–768.\nTakayuki, Y . 2000. On the NP-completeness of the Slither\nLink puzzle. IPSJ SIGNotes ALgorithms.\nValmeekam, K.; Olmo, A.; Sreedharan, S.; and Kambham-\npati, S. 2022. Large Language Models Still Can’t Plan\n(A Benchmark for LLMs on Planning and Reasoning about\nChange). arXiv preprint arXiv:2206.10498.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Chi, E.;\nLe, Q.; and Zhou, D. 2022. Chain of thought prompting\nelicits reasoning in large language models. arXiv preprint\narXiv:2201.11903.\nWinner, E. Z. 2008. Learning Domain-Speciﬁc Planners\nfrom Example Plans. Ph.D. thesis, Carnegie Mellon Uni-\nversity, USA.\nXia, C. S.; and Zhang, L. 2023. Conversational Automated\nProgram Repair. arXiv:2301.13246.\nXie, Y .; Yu, C.; Zhu, T.; Bai, J.; Gong, Z.; and Soh, H. 2023.\nTranslating natural language to planning goals with large-\nlanguage models. arXiv preprint arXiv:2302.05128.\nYang, R.; Silver, T.; Curtis, A.; Lozano-Perez, T.; and Kael-\nbling, L. P. 2022. PG3: Policy-Guided Planning for Gener-\nalized Policy Generation. In IJCAI.\nZheng, W.; Sharan, S.; Jaiswal, A. K.; Wang, K.; Xi, Y .;\nXu, D.; and Wang, Z. 2023. Outline, Then Details: Syn-\ntactically Guided Coarse-To-Fine Code Generation. arXiv\npreprint arXiv:2305.00909.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n20264"
}