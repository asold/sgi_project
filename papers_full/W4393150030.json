{
  "title": "Rethinking Multi-Scale Representations in Deep Deraining Transformer",
  "url": "https://openalex.org/W4393150030",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2103126149",
      "name": "Hongming Chen",
      "affiliations": [
        "Shenyang Aerospace University"
      ]
    },
    {
      "id": "https://openalex.org/A2105016708",
      "name": "Xiang Chen",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4224881034",
      "name": "Jiyang Lu",
      "affiliations": [
        "Shenyang Aerospace University"
      ]
    },
    {
      "id": "https://openalex.org/A2034690793",
      "name": "Yu-Feng Li",
      "affiliations": [
        "Shenyang Aerospace University"
      ]
    },
    {
      "id": "https://openalex.org/A2103126149",
      "name": "Hongming Chen",
      "affiliations": [
        "Shenyang Aerospace University"
      ]
    },
    {
      "id": "https://openalex.org/A2105016708",
      "name": "Xiang Chen",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4224881034",
      "name": "Jiyang Lu",
      "affiliations": [
        "Shenyang Aerospace University"
      ]
    },
    {
      "id": "https://openalex.org/A2034690793",
      "name": "Yu-Feng Li",
      "affiliations": [
        "Shenyang Aerospace University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2126926806",
    "https://openalex.org/W4360103831",
    "https://openalex.org/W4286984630",
    "https://openalex.org/W4382449856",
    "https://openalex.org/W2768610172",
    "https://openalex.org/W2154621477",
    "https://openalex.org/W3189755165",
    "https://openalex.org/W6742120935",
    "https://openalex.org/W3174756006",
    "https://openalex.org/W4318953193",
    "https://openalex.org/W2114770744",
    "https://openalex.org/W4286750439",
    "https://openalex.org/W3013510249",
    "https://openalex.org/W2121396509",
    "https://openalex.org/W4221155341",
    "https://openalex.org/W2884068670",
    "https://openalex.org/W6719777491",
    "https://openalex.org/W2209874411",
    "https://openalex.org/W3217648369",
    "https://openalex.org/W233979554",
    "https://openalex.org/W6611075124",
    "https://openalex.org/W3023003829",
    "https://openalex.org/W2930755307",
    "https://openalex.org/W2133665775",
    "https://openalex.org/W3166368936",
    "https://openalex.org/W6838994554",
    "https://openalex.org/W6730307226",
    "https://openalex.org/W2995668749",
    "https://openalex.org/W3193890688",
    "https://openalex.org/W3212228063",
    "https://openalex.org/W6790137093",
    "https://openalex.org/W2788708632",
    "https://openalex.org/W2009548700",
    "https://openalex.org/W4225672218",
    "https://openalex.org/W4230654051",
    "https://openalex.org/W2466666260",
    "https://openalex.org/W3028045870",
    "https://openalex.org/W2963263347",
    "https://openalex.org/W4304099129",
    "https://openalex.org/W3035250394",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2740982616",
    "https://openalex.org/W4312399981",
    "https://openalex.org/W4283023197",
    "https://openalex.org/W3170697543",
    "https://openalex.org/W4312812783",
    "https://openalex.org/W2912435603",
    "https://openalex.org/W2964212750",
    "https://openalex.org/W2559264300",
    "https://openalex.org/W3035326127",
    "https://openalex.org/W4382457774",
    "https://openalex.org/W4386075800",
    "https://openalex.org/W3202040256"
  ],
  "abstract": "Existing Transformer-based image deraining methods depend mostly on fixed single-input single-output U-Net architecture. In fact, this not only neglects the potentially explicit information from multiple image scales, but also lacks the capability of exploring the complementary implicit information across different scales. In this work, we rethink the multi-scale representations and design an effective multi-input multi-output framework that constructs intra- and inter-scale hierarchical modulation to better facilitate rain removal and help image restoration. We observe that rain levels reduce dramatically in coarser image scales, thus proposing to restore rain-free results from the coarsest scale to the finest scale in image pyramid inputs, which also alleviates the difficulty of model learning. Specifically, we integrate a sparsity-compensated Transformer block and a frequency-enhanced convolutional block into a coupled representation module, in order to jointly learn the intra-scale content-aware features. To facilitate representations learned at different scales to communicate with each other, we leverage a gated fusion module to adaptively aggregate the inter-scale spatial-aware features, which are rich in correlated information of rain appearances, leading to high-quality results. Extensive experiments demonstrate that our model achieves consistent gains on five benchmarks.",
  "full_text": "Rethinking Multi-Scale Representations in Deep Deraining Transformer\nHongming Chen1, Xiang Chen2∗, Jiyang Lu1, Yufeng Li1*\n1 College of Electronic Information Engineering, Shenyang Aerospace University\n2 School of Computer Science and Engineering, Nanjing University of Science and Technology\n{chenhongming,lujiyang1}@stu.sau.edu.cn, chenxiang@njust.edu.cn, liyufeng@sau.edu.cn\nAbstract\nExisting Transformer-based image deraining methods depend\nmostly on fixed single-input single-output U-Net architecture.\nIn fact, this not only neglects the potentially explicit informa-\ntion from multiple image scales, but also lacks the capability\nof exploring the complementary implicit information across\ndifferent scales. In this work, we rethink the multi-scale rep-\nresentations and design an effective multi-input multi-output\nframework that constructs intra- and inter-scale hierarchical\nmodulation to better facilitate rain removal and help image\nrestoration. We observe that rain levels reduce dramatically\nin coarser image scales, thus proposing to restore rain-free re-\nsults from the coarsest scale to the finest scale in image pyra-\nmid inputs, which also alleviates the difficulty of model learn-\ning. Specifically, we integrate a sparsity-compensated Trans-\nformer block and a frequency-enhanced convolutional block\ninto a coupled representation module, in order to jointly learn\nthe intra-scale content-aware features. To facilitate represen-\ntations learned at different scales to communicate with each\nother, we leverage a gated fusion module to adaptively aggre-\ngate the inter-scale spatial-aware features, which are rich in\ncorrelated information of rain appearances, leading to high-\nquality results. Extensive experiments demonstrate that our\nmodel achieves consistent gains on five benchmarks.\nIntroduction\nAdverse visual conditions have become a major obstacle to\nthe application of artificial intelligence, especially computer\nvision. As one of the harsh environments, rainy days have\nsparked a wave of research on low-level vision communities\nin recent years. The objective of single image deraining is to\nremove or reduce the undesired degradation caused by rain\nfrom input images, enhancing its visual quality and improv-\ning the accuracy of perception system (Chen et al. 2022).\nOver the years, numerous techniques have been proposed\nto address the image deraining problem. Early prior-based\nmethods (Kang, Lin, and Fu 2011; Chen and Hsu 2013; Luo,\nXu, and Ji 2015; Li et al. 2016) utilize prior information\nabout clean images or specific characteristics of rain streaks\nto guide the restoration process. Afterwards, the emergence\nof deep learning has attracted significant advancements in\n*Corresponding author\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nUformer\nRestormer\nIDT\nDRSformer\nOur method\nInput 1 Output 1\nInput 2 Input 3 Output 3 Output 2\nFigure 1: The architectures of deep deraining Transformers.\nCompared to existing models, our method first introduces a\ncoarse-to-fine scheme to better capture multi-scale represen-\ntations (intra-scale and inter-scale hierarchical modulation).\nthe field of image rain removal (Yang et al. 2020). CNN-\nbased methods (Jiang et al. 2020; Chen et al. 2022) excel in\ntheir ability to learn intricate mappings between rainy and\nclean images, allowing them to effectively handle various\nshapes, sizes, and densities of rain streaks. More recently,\nTransformer-based methods (Chen et al. 2023a; Wang et al.\n2022; Xiao et al. 2022; Chen et al. 2023b) have demon-\nstrated remarkable performance in image deraining, primar-\nily due to their ability to model non-local information, which\nis crucial for achieving high-quality image reconstruction.\nDespite achieving impressive performance, we note that\nexisting deep image deraining Transformers depend mostly\non single-input single-output architecture. Figure 1 provides\na pipeline for network architecture. By this way, it not only\nneglects the potentially explicit information from multiple\nimage scales, but also lacks the capability of exploring the\ncomplementary implicit information across different scales.\nIn general, multi-scale visual information flow, encompass-\ning feature representations at both small (global contextual)\nand large (local connectivity) scales, was frequently used in\ncomputer vision, particularly the solution of creating feature\npyramids from image pyramid inputs (Chen, Zhu, and Gong\n2017; Jiang et al. 2020; Mao et al. 2023). Thus, it is of great\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n1046\nLarge Scale\nSmall Scale\nLarge Scale\nhidden\nclean \nclues\nMulti-Scale \nRepresentations\nFigure 2: An example of rain image and clean image, and\ntheir coarser version at 1/4 the scale. In these four images,\nwe show the10×10 patches at the same relative coordinates.\nNote that some signals similar to clean images are hidden on\nthe some coarser scale of the rainy image.\ninterest to investigate multi-scale representations optimized\nfor Transformer-based image deraining networks.\nIn fact, a single-scale representation tends to weaken hid-\nden clean signals from other scales useful in feature learn-\ning. In Figure 2, we show multi-scale representations can be\nexplored to “pull out” cleaner versions of the signal from the\nrainy image’s coarser scales. We observe that rain levels re-\nduce dramatically in coarser image scales. This permits the\nthose potentially clean representations to “naturally emerge”\nin the rainy image at a coarser scale (Zontak, Mosseri, and\nIrani 2013; Michaeli and Irani 2014). With this finding, we\nare first attempt to formulate a new deep image Transformer\narchitecture using a multi-input encoder and a multi-output\ndecoder. Motivated by the coarse-to-fine scheme (Cho et al.\n2021), we restore rain-free results from the coarsest scale to\nthe finest scale in image pyramid inputs, which employs ini-\ntial solutions estimated from those coarse scales to alleviate\nthe difficulty of network learning. Here, we name our ap-\nproach Multi-Scale Deraining Transformer(MSDT) design\nfor not only excavating scale-specific discriminative feature,\nbut also maximising scale-space complementary signals.\nSpecifically, our proposed MSDT is made up of 3 intra-\nscale branches that each learn one input image scale in the\npyramid, as well as inter-scale branches that learn a compli-\nmentary combination of multi-scale rich representations. In\nthe intra-scale branch, a sparsity-compensated Transformer\nblock and a frequency-enhanced convolutional block is in-\ntegrated into a coupled representation module. The former\nfocuses on the most relevant global features via a neighbor\nsoftmax operator to facilitate a more accurate representation,\nwhile the latter integrates the local features information via a\nresidual fourier transformation to better help image restora-\ntion. In addition, in the inter-scale branch, we leverage a\ngated fusion module to adaptively aggregate the multi-scale\nfeatures generated by different encoder blocks to prompt the\nlearning of various decoder blocks, thus boosting the final\nreconstruction performance. With this formulation, we al-\nlow all branches to be learned concurrently in an end-to-end\nfashion so as to achieve high-quality deraining outputs. Fi-\nnally, extensive experiments show that our proposed MSDT\ndelivers significant performance gains, exceeding the state-\nof-the-art Transformer-based approach DRSformer by 0.53\ndB in PSNR on the real-world SPA-Data benchmark.\nThis paper makes the following contributions to the field:\n• We rethink the multi-scale representations for single im-\nage deraining problem, and propose an effective end-to-\nend multi-input multi-output architecture to better facili-\ntate rain removal in the richer scale space.\n• We show that coupled representation modules can jointly\nlearn the intra-scale content-aware features and gated fu-\nsion modules can be beneficial for the inter-scale spatial-\naware features, in order to help hierarchical modulation.\n• We perform comprehensive experiments to demonstrate\nthe effectiveness of our method against the state-of-the-\nart Transformer-based image deraining approahces.\nRelated Work\nWe categorize existing methods into prior-based algorithms,\nCNN-based approaches and Transformer-based methods.\nPrior-based methods. As image deraining is ill-posed, tra-\nditional approaches often employ hand-crafted priors based\non image statistics to reconstruct images,e.g., image decom-\nposition (Kang, Lin, and Fu 2011), low-rank representation\n(Chen and Hsu 2013), discriminative sparse coding (Luo,\nXu, and Ji 2015), and Gaussian mixture model (Li et al.\n2016). However, these methods tend to rely on empirical ob-\nservations and lead to complicated optimization problems.\nCNN-based methods. Instead of manually designing image\npriors, CNN-based frameworks (Yang et al. 2020) have out-\nperformed their conventional counterparts to achieve decent\nrestoration performance. A wide array of network structures\nand designs have been effectively employed to significantly\nboost the capacity of end-to-end learning, e.g., multi-scale\n(Jiang et al. 2020) and multi-stage fusion (Zamir et al. 2021).\nHowever, they have difficulty capturing non-local informa-\ntion due to the intrinsic limitations of convolution operators.\nTransformer-based methods. Driven by the success of the\nTransformer network (Dosovitskiy et al. 2020), researchers\nhave endeavored to replace CNN baselines with Transform-\ners as the network backbone for vision tasks. In recent years,\nTransformer-based methods have been increasingly used for\nimage restoration due to their superior capacity for modeling\nlong-range dependencies. For example, Wang et al. (Wang\net al. 2022) developed a general U-shaped Transformer ar-\nchitecture to solve image restoration problem. Zamir et al.\n(Zamir et al. 2022) designed an efficient Transformer model\nby estimating self-attention along the channel dimension,\nachieving remarkable performance. In the field of image rain\nremoval, Xiao et al. (Xiao et al. 2022) first introduced the\nimage deraining Transformer (IDT) using spatial-based and\nwindow-based self-attention modules. Recently, Chen et al.\n(Chen et al. 2023a) developed a sparse Transformer to make\nfull use of the most useful features for better image restora-\ntion. However, existing Transformer-based methods are still\nlimited to fixed single-input single-output U-Net baseline,\nlacking in exploring feature correlations in different image\nscale spaces. Our work will fill the gap in this research.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n1047\nFigure 3: The architecture of the proposed multi-scale deraining Transformer (MSDT), which takes multi-input rainy images\nand generates multi-output derained images. It mainly contains (1) coupled representation module (CRM) with frequency-\nenhanced convolutional block (FECB) and sparsity-compensated Transformer block (SCTB), (2) gated fusion module (GFM).\nProposed Method\nOur goal is to develop an effective deep image Transformer\nfor high-quality image deraining, which fully makes full use\nof multi-scale features extracted from an input image. We\ndescribe each component of our network in the following.\nNetwork Architecture\nThe overall framework of our proposed multi-scale derain-\ning Transformer (MSDT) is presented in Figure 3, which is\ndivided into 3 scales based on previous coarse-to-fine tech-\nniques (Kim, Lee, and Cho 2022). Given a input rainy im-\nage, our method first produces pyramid rainy images using\nthe interpolation operator to down-sample the original rainy\nimage into multiple scales, i.e., 1/2 and 1/4. From the coars-\nest to the finest image scales, we designate each scale asS3,\nS2, and S1, respectively. The network takes pyramid rainy\nimages as multi-inputs, and extracts the shallow features us-\ning a shallow 3×3 convolutional layers. Based on the initial\nfeatures from each scale, N stacked coupled representation\nmodules (CRMs) then perform the deep feature extraction\nand fusion of multi-scale rain information by inserting two\nparallel gated fusion modules (GFMs). On one side, intra-\nscale networks with the same structure are used to capture\nthe most discriminative visual cues for each individual pyra-\nmid scale of rain appearances. On the other side, inter-scale\nnetworks are adopted for performing the discriminative fea-\nture selection and optimal integration of scale-specific rep-\nresentations from different scales. In the multi-input multi-\noutput framework, the ultimate goal is to find relevant com-\nplementary combinations for feature selection at different\nscales, while optimizing the discriminative feature represen-\ntation at each scale. In summary, compared to previous deep\nderaining Transformers, our design can more effectively find\nmore potential clean signals in multi-scale feature learning.\nModel Optimization\nIn order to supervise the learning process of the network, we\nchoose three kinds of loss functions as training objectives to\nguide the optimization, which are calculated as follows:\nLmsc =\n3X\nk=1\nr\n\r\r\rˆBk − Bk\n\r\r\r\n2\n+ ε2, (1)\nwhere ˆBk and Bk denote the k-th scale reconstructed im-\nage and ground-truth image, respectively. Lmsc represents\nthe Multi-Scale Charbonnier (MSC) loss (Charbonnier et al.\n1994). Here, the penalty coefficient ε is set to 10−3.\nLmsed =\n3X\nk=1\nr\r\r\r∆\n\u0010\nˆBk\n\u0011\n− ∆ (Bk)\n\r\r\r\n2\n+ ε2, (2)\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n1048\nwhere Lmsed is the Multi-Scale Edge (MSED) loss (Zamir\net al. 2021). Here, ∆ represents the Laplacian operator.\nLmsfr =\n3X\nk=1\n\r\r\rFT\n\u0010\nˆBk\n\u0011\n− FT(B)\n\r\r\r\n1\n, (3)\nwhere Lmsfr is the Multi-Scale Frequency Reconstruction\n(MSFR) loss (Cho et al. 2021). FT represents the Fourier\ntransform operator to obtain the frequency domain of the\noriginal image. Finally, the overall loss Ltotal is defined as:\nLtotal = λ1Lmsc + λ2Lmsed + λ3Lmsfr , (4)\nwhere the trade-off weight λ1, λ2 and λ3 are empirically set\nto 1, 0.05, and 0.01 as in (Mao et al. 2023), respectively.\nCoupled Representation Module (CRM)\nIn order to better satisfy rain removal, both local and global\ninformation representations are increasingly indispensable\n(Jiang et al. 2022; Chen et al. 2023b). Hence, we propose a\ncoupled representation module (CRM) that combines unique\nadvantages of CNN and Transformer. Here, our developed\nCRM consists two main designs: frequency-enhanced con-\nvolutional block (FECB) and sparsity-compensated Trans-\nformer block (SCTB). In this way, integrating sparsity and\nfrequency guidance into multi-scale architecture is of great\nimportance for boosting image restoration performance. We\nwill describe the details about these two components.\nFrequency-enhanced convolutional block (FECB) is in-\ntroduced to improve the locality of the network in the fre-\nquency domain (Mao et al. 2023). To recover the texture\ndetails of the background images, we first use Fast Fourier\nTransform (FFT) (Nussbaumer and Nussbaumer 1981) to\nextract high-frequency components. Due to the symmetric\nnature of the FFT, we only use the Real FFT in order to re-\nduce the computational overhead. LetX ∈ RH×W×C be the\ninput feature of FECB. We first transformXinto a frequency\ndomain with 2d Real FFT to obtain F(X) ∈ RH×W\n2 ×C.\nThen, the real and imaginary parts ofF(X) are concatenated\nalong the channel dimension to obtain Y ∈ RH×W\n2 ×2C.\nNext, Y is applied to two 1 × 1 convolutions and one non-\nlinear function. Finally, Y is recovered to the spatial struc-\nture with the inverse 2d Real FFT, we define it as Yfft =\nF−1(Y) ∈ RH×W×C. Similar to (Mao et al. 2023), we also\nadd residual paths to boost inter-scale feature propagation.\nSparsity-compensated Transformer block (SCTB) is in-\ntroduced to focus on the most relevant non-local information\nin each scale, which enables much accurate representations.\nIn each SCTB, given the input features at the (l -1)-th block\nXl−1, the encoding procedures of SCTB can be formulated\nas:\nX′\nl = Xl−1 + NSSA (LN (Xl−1)) , (5)\nXl = X′\nl + GDFN (LN (X′\nl)) , (6)\nwhere X′\nl and Xl represent the outputs from the neigh-\nbor softmax self-attention (NSSA) and gated-Dconv feed-\nforward network (GDFN) (Zamir et al. 2022). Here, LN\nrefers to the layer normalization.\nMotivated by (Chen et al. 2023a), we note that the soft-\nmax normalization in the standard Transformer is performed\non all the input tokens, thus redundant irrelevant represen-\ntations will interfere with the feature aggregation and dis-\ntract the attention. To alleviate this problem, we develop the\nneighbor softmax to replace the normal softmax. Specifi-\ncally, we first encode channel-wise context by applying1×1\nconvolutions followed by 3 × 3 depth-wise convolutions.\nGiven the query Q, key K, and value V , we generate the\nattention values P of all pixel pairs between Q and K:\nP = QKT\n√\nd\n, (7)\nwhere d = C/k is the head dimension and k is the head\nnumber. We presum that two tokens are likely to be rel-\nevant if they are latent neighbours to one another in fea-\nture space. Here, we propose a simple but effective mask-\ning function M(·) is performed upon P to select the top-k\nneighbors from each row of the similarity matrix. For other\nelements that are smaller than threshold, we replace them\nwith 0. By adding this mask M(·) to the regular softmax\nfunction, we achieve sparse self-attention only occurring in\nneighbors, which is calculated by\nM(P, k)ij =\n\u001aPij if Pij ≥ threshold\n0 if Pij < threshold , (8)\nwhere threshold is a kth largest value of row. Considering\nthe discrepancies in the degree of rain degradation at differ-\nent image scales, we adopt different thresholds at each scale\nbranches to better maximise scale-specific feature. Due to\nthe partial attention value being set to zero, the relationship\nis constrained to the relevant neighbors, making the aggre-\ngation of features within the scale more focused and robust.\nGated Fusion Module (GFM)\nFor feature fusion, scale-specific representations from differ-\nent scales that have different spatial resolutions are typically\nscaled by down-sampling or up-sampling procedures to the\nsame spatial resolution. The final image restoration may be\nimpacted by these resizing operations because they could re-\nsult in the loss of some crucial structural elements. To this\nend, we formulate a gated fusion module to adaptively ag-\ngregate the inter-scale information flow. Given input features\nat three scales, the process of GFM can be expressed as:\nFGFM = F1×1 (Gate (F1×1(Gate(S3, S2)), S1)) , (9)\nwhere F1×1 denotes a 1 × 1 convolutional layer. Here, we\nadopt dual gate units to dynamically control hierarchical in-\nformation. Each gate unit Gate(·) is formulated as:\nGate(X, Y) = σ (F3×3 (F1×1(Y))) ⊙ F3×3 (F1×1(X)) ,\n(10)\nwhere σ refers to the tanh activation function, and ⊙ rep-\nresents the element-wise multiplication. The output of the\nGFM is delivered to its corresponding decoder part. By in-\ncorporating the GFM, our proposed model effectively en-\nsure synergistically correlated multi-scale feature learning.\nIn what follows, we will show the effectiveness of these de-\nsign choices in the experimental section.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n1049\nDatasets Rain200L Rain200H DID-Data DDN-Data SPA-Data\nMetrics PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM\nPrior-based\nmethods DSC 27.16 0.8663 14.73 0.3815 24.24 0.8279 27.31 0.8373 34.95 0.9416\nGMM 28.66 0.8652 14.50 0.4164 25.81 0.8344 27.55 0.8479 34.30 0.9428\nCNN-based methods\nDDN 34.68 0.9671 26.05 0.8056 30.97 0.9116 30.00 0.9041 36.16 0.9457\nRESCAN 36.09 0.9697 26.75 0.8353 33.38 0.9417 31.94 0.9345 38.11 0.9707\nPReNet 37.80 0.9814 29.04 0.8991 33.17 0.9481 32.60 0.9459 40.16 0.9816\nMSPFN 38.58 0.9827 29.36 0.9034 33.72 0.9550 32.99 0.9333 43.43 0.9843\nRCDNet 39.17 0.9885 30.24 0.9048 34.08 0.9532 33.04 0.9472 43.36 0.9831\nMPRNet 39.47 0.9825 30.67 0.9110 33.99 0.9590 33.10 0.9347 43.64 0.9844\nDualGCN 40.73 0.9886 31.15 0.9125 34.37 0.9620 33.01 0.9489 44.18 0.9902\nSPDNet 40.50 0.9875 31.28 0.9207 34.57 0.9560 33.15 0.9457 43.20 0.9871\nTransformer\n-based methods\nUformer 40.20 0.9860 30.80 0.9105 35.02 0.9621 33.95 0.9545 46.13 0.9913\nRestormer 40.99 0.9890 32.00 0.9329 35.29 0.9641 34.20 0.9571 47.98 0.9921\nIDT 40.74 0.9884 32.10 0.9344 34.89 0.9623 33.84 0.9549 47.35 0.9930\nDRSformer 41.23 0.9894 32.17 0.9326 35.35 0.9646 34.35 0.9588 48.54 0.9924\nOurs 41.75 0.9904 32.45 0.9379 35.37 0.9652 34.36 0.9593 49.07 0.9926\nTable 1: Comparison of quantitative results on five benchmarks. Bold and underline indicate the best and second-best results.\nExperiments\nIn this section, comprehensive image deraining experiments\nis performed on commonly used benchmark datasets to eval-\nuate the effectiveness of the proposed method. The training\ncode and test model will be available to the public.\nExperimental Setup\nDatasets and metrics. We evaluate the performance of our\nmodel on five publicly rain streak datasets: Rain200L (Yang\net al. 2017), Rain200H (Yang et al. 2017), DID-Data (Zhang\nand Patel 2018), DDN-Data (Fu et al. 2017), and SPA-\nData (Wang et al. 2019). Rain200L and Rain200H com-\nprise 1,800 synthetic rainy images for training, along with\n200 images designated for testing. DID-Data and DDN-Data\ncomprise 12,000 and 12,600 synthetic images, featuring dis-\ntinct rain directions and density levels. Each dataset includes\n1,200 and 1,400 rainy images specifically designated for\ntesting. In addition, SPA-Data is a large-scale real-world rain\nbenchmark, encompassing 638,492 image pairs for training,\nalongside 1,000 image pairs designated for testing. Given\nthe availability of ground truths, we employ two widely used\nmetrics for quantitative comparison: Peak Signal to Noise\nRatio (PSNR) (Huynh-Thu and Ghanbari 2008) and Struc-\ntural Similarity (SSIM) (Wang et al. 2004). Following pre-\nvious deraining methods (Fu et al. 2023; Chen et al. 2023a),\nwe calculate those metrics in Y channel of YCbCr space.\nComparison methods. We compare our proposed approach\nwith diverse state-of-the-art image deraining baselines, in-\ncluding two prior-based algorithms (i.e., DSC (Luo, Xu, and\nJi 2015) and GMM (Li et al. 2016)), eight CNN-based ap-\nproaches (i.e., DDN (Fu et al. 2017), RESCAN (Li et al.\n2018), PReNet (Ren et al. 2019), MSPFN (Jiang et al. 2020),\nRCDNet (Wang et al. 2020), MPRNet (Zamir et al. 2021),\nDualGCN (Fu et al. 2021), and SPDNet (Yi et al. 2021)) and\nfour Transformer-based networks (i.e., Uformer (Wang et al.\n2022), Restormer (Zamir et al. 2022), IDT (Xiao et al. 2022)\nand DRSformer (Chen et al. 2023a)). To ensure fair compar-\nison, we refer to their online experimental data provided by\n(Chen et al. 2023a) using same evaluation protocol.\nFigure 4: The average fitting results of the synthetic datasets\nbased on the histogram curve of Y channel in YCbCr space,\nsuggesting that our proposed MSDT leads in greater fitting\naccuracy to the ground-truths distribution than other com-\nparison approaches.\nImplementation details. During training, the proposed net-\nwork is implemented in PyTorch framework using Adam op-\ntimizer with a learning rate of 2 × 10−4 to minimize Ltotal.\nThe final learning rate is steadily decreased to1×10−4 using\nthe cosine annealing strategy (Loshchilov and Hutter 2016).\nFor Rain200L, Rain200H, DID-Data and DDN-Data, 500\nepochs are trained, while SPA-Data is trained for 5 epochs.\nFor data augmentation, we also randomly adopt horizontal\nand vertical flips. In our model, we adopt a stack of 8 CRMs\n(i.e., N = 8 in Figure 3. We set the thresholds of SCTB in\nS1, S2, and S3 to 0.6, 0.7, and 0.8, respectively. The setting\nof GDFN in SCTB is consistent with (Zamir et al. 2022). We\nrun all of our experiments with batch size of 2 and patch size\nof 256 on one NVIDIA GeForce RTX 4090 GPU (24G). For\ntesting, sliding window slicing crop method is employed.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n1050\n(a) Rainy Input\n (b) GT\n (c) SPDNet\n (d) Restormer\n (e) IDT\n (f) DRSformer\n (g) Ours\nFigure 5: Visual comparison on the Rain200H dataset. Best viewed by zooming in the figures on high-resolution displays.\nUformer Restormer\nIDT DRSformer Ours\n#P 50.8\n26.1 16.4 33.7 16.6\n#F 45.9 174.7 61.9 242.9 129.9\n#R 0.19 0.28 0.28 0.31 0.09\nTable 2: Comparison of model efficiency on a256×256 im-\nage. “#P”, “#F” and “#R” represent the number of trainable\nparameters (in M), FLOPs (in G) and inference time (in sec-\nond), respectively.\nExperimental Results\nSynthetic datasets. Table 1 compares our method with 14\nrepresentative and state-of-the-art deraining approaches. We\ncan clearly see that our proposed MSDT consistently out-\nperforms all the other baselines in terms of PSNR and SSIM\nvalues thanks to our multi-scale architecture, especially ex-\nceeding the recent Transformer-based approach DRSformer\n(Chen et al. 2023a) by 0.52 dB in PSNR on the Rain200L\nbenchmark. Following (Jiang et al. 2022), we further show\nthe average fitting results of the synthetic datasets based on\nthe histogram curve of Y channel in the YCbCr space, in-\ndicating that our deraining results of MSDT are close to the\ndistribution of ground-truths images (Figure 4). The high-\nquality performance gains also confirm that our framework\nopens up a new perspective for deep image deraining archi-\ntecture. Furthermore, Figure 5 shows a qualitative compar-\nison on the Rain200H dataset. As can be seen, our method\nyields cleaner results compared to other approaches, which\nis also consistent with the quantitative values.\nReal-world datasets. To facilitate real-world performance\nanalysis, we conduct experiments on the SPA-Data. The last\ncolumn of Table 1 records the corresponding the quantita-\ntive results. As expected, it can be seen that the PSNR and\nSSIM values of our method still maintain a high level of\ncompetitiveness. Meanwhile, we present several examples\nof visual comparison results in Figure 7. According to the\nground truth, all of the other efforts undertake undesirable\nresults in rain removal or detail restoration, and our pro-\nposed MSDT outperforms the others. This further validates\nthe performance advantage of our multi-scale coarse-to-fine\nFigure 6: Five dimensional radar map of comprehensive ca-\npability of deep deraining Transformers, including PSNR,\nSSIM, #Params, #FLOPs and #Runtimes.\nfeature learning method over single-scale feature learning\nbased single-input single-output U-Net framework.\nModel efficiency. In Table 2, we also compare the computa-\ntional complexity of different deep deraining Transformer-\nbased methods, including the number of trainable parame-\nters, FLOPs and inference time on a 256 × 256 image. To\nintuitively compare the comprehensive capabilities of recent\nstate-of-the-art methods, we present a radar map of model\ncapability in Figure 6. Compared to other deep Transformer-\nbased methods, our model achieves competitive results in\nfour dimensions except for #FLOPs, which implies the po-\ntential of our method to become a pentagon warrior.\nAblation Studies\nIn what follows, we adopt the challenging Rain200H dataset\nto conduct ablation studies for analysis and discussions.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n1051\n(a) Rainy Input\n (b) GT\n (c) DualGCN\n (d) SPDNet\n (e) Restormer\n (f) IDT\n (g) DRSformer\n (h) Ours\nFigure 7: Visual comparison on the SPA-Data dataset. Best viewed by zooming in the figures on high-resolution displays.\nFigure 8: Ablation analysis for different thresholds (from left\nto right: S1, S2, S3) in the SCTB.\nScale Le\nvel S1 S1+S2 S1+S2+S3\nPSNR 32.08\n32.20 32.45\nTable 3: Ablation analysis for different levels of image scale.\nEffectiveness of multi-scale. To investigate the effect about\ndifferent levels of image scale, Table 3 reports the PSNR/S-\nSIM values of corresponding models. Compared to perform-\ning single-scale deraining, richer multi-scale representations\ncan bring a great contribution to the baseline model. Because\nof the negligible rain effect at a coarse scale, it can predict\na more accurate result, which acts as an initial solution for a\nfiner scale, resulting in a higher-quality deraining result.\nEffectiveness of CRM. The influence of threshold, the cru-\ncial hyper-parameter for our proposed SCTB, is investigated\nin Figure 8. As shown, our developed neighbor softmax out-\nperforms normal softmax function (i.e., set all thresholds to\n1) in terms of PSNR, which suggests that paying attention\nto relevant neighbors only leads to better feature represen-\ntations. In addition, setting different thresholds for different\nscales can be more conducive to global information aggre-\ngation. We further demonstrate the effectiveness of FECB in\nMethod w/o\nGFM w/o FECB Ours\nPSNR 32.33 31.55 32.45\nTable 4: Ablation analysis for GFM and FECB in the CRM.\nLoss w/\nMSC w/o MSFR w/o MSED Ours\nPSNR 32.17\n32.32 32.35 32.45\nTable 5: Ablation analysis for different loss functions.\nTable 4. Through the quantitative results, the recovered re-\nsults of the model with FECB in the CRM tend to be better.\nEffectiveness of GFM. We further analyze the effectiveness\nof GFM in Table 4. Compared to direct feature concatena-\ntion (i.e., without GFM), our designed solution tends to be\nmore effective for combining multi-scale representations.\nEffectiveness of loss functions. The quantitative compar-\nisons in Table 5 show the significance of using hybrid losses,\nwhich indicates that each component we considered has its\nown contribution to the final deraining performance.\nConcluding Remarks\nThis paper first proposes a high-quality multi-scale derain-\ning Transformer (MSDT) for further boosting image restora-\ntion performance. In our designed architecture, we show that\ncoupled representation modules can jointly learn the intra-\nscale content-aware features and gated fusion modules can\nbe beneficial for the inter scale spatial-aware features. Ex-\nperiments demonstrate the superiority of our method over\nthe state-of-the-arts. We hope that our proposed multi-input\nmulti-output architecture can provide a new perspective and\nsolution for exploring more related low-level vision tasks.\nAcknowledgements\nThis work has been supported in part by the Liaoning\nProvincial Applied Basic Research Project under Grant\n2022JH2/101300247, and Shenyang Science and Technol-\nogy Project under Grant 23-503-6-18.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n1052\nReferences\nCharbonnier, P.; Blanc-Feraud, L.; Aubert, G.; and Barlaud,\nM. 1994. Two deterministic half-quadratic regularization\nalgorithms for computed imaging. In ICIP, volume 2, 168–\n172.\nChen, X.; Li, H.; Li, M.; and Pan, J. 2023a. Learning A\nSparse Transformer Network for Effective Image Deraining.\nIn CVPR, 5896–5905.\nChen, X.; Pan, J.; Jiang, K.; Li, Y .; Huang, Y .; Kong, C.; Dai,\nL.; and Fan, Z. 2022. Unpaired deep image deraining using\ndual contrastive learning. In CVPR, 2017–2026.\nChen, X.; Pan, J.; Lu, J.; Fan, Z.; and Li, H. 2023b. Hybrid\ncnn-transformer feature fusion for single image deraining.\nIn AAAI, volume 37, 378–386.\nChen, Y .; Zhu, X.; and Gong, S. 2017. Person re-\nidentification by deep learning multi-scale representations.\nIn ICCVW, 2590–2600.\nChen, Y .-L.; and Hsu, C.-T. 2013. A generalized low-\nrank appearance model for spatio-temporally correlated rain\nstreaks. In ICCV, 1968–1975.\nCho, S.-J.; Ji, S.-W.; Hong, J.-P.; Jung, S.-W.; and Ko, S.-J.\n2021. Rethinking coarse-to-fine approach in single image\ndeblurring. In ICCV, 4641–4650.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nFu, X.; Huang, J.; Zeng, D.; Huang, Y .; Ding, X.; and Pais-\nley, J. 2017. Removing rain from single images via a deep\ndetail network. In CVPR, 3855–3863.\nFu, X.; Qi, Q.; Zha, Z.-J.; Zhu, Y .; and Ding, X. 2021. Rain\nstreak removal via dual graph convolutional network. In\nAAAI, volume 35, 1352–1360.\nFu, X.; Xiao, J.; Zhu, Y .; Liu, A.; Wu, F.; and Zha, Z.-J.\n2023. Continual image deraining with hypergraph convolu-\ntional networks. IEEE TPAMI.\nHuynh-Thu, Q.; and Ghanbari, M. 2008. Scope of valid-\nity of PSNR in image/video quality assessment. Electronics\nletters, 44(13): 800–801.\nJiang, K.; Wang, Z.; Chen, C.; Wang, Z.; Cui, L.; and Lin,\nC.-W. 2022. Magic ELF: Image deraining meets association\nlearning and transformer. ACM MM.\nJiang, K.; Wang, Z.; Yi, P.; Chen, C.; Huang, B.; Luo, Y .;\nMa, J.; and Jiang, J. 2020. Multi-scale progressive fusion\nnetwork for single image deraining. In CVPR, 8346–8355.\nKang, L.-W.; Lin, C.-W.; and Fu, Y .-H. 2011. Automatic\nsingle-image-based rain streaks removal via image decom-\nposition. IEEE TIP, 21(4): 1742–1755.\nKim, K.; Lee, S.; and Cho, S. 2022. Mssnet: Multi-scale-\nstage network for single image deblurring. In ECCV, 524–\n539.\nLi, X.; Wu, J.; Lin, Z.; Liu, H.; and Zha, H. 2018. Recur-\nrent squeeze-and-excitation context aggregation net for sin-\ngle image deraining. In ECCV, 254–269.\nLi, Y .; Tan, R. T.; Guo, X.; Lu, J.; and Brown, M. S. 2016.\nRain streak removal using layer priors. In CVPR, 2736–\n2744.\nLoshchilov, I.; and Hutter, F. 2016. Sgdr: Stochas-\ntic gradient descent with warm restarts. arXiv preprint\narXiv:1608.03983.\nLuo, Y .; Xu, Y .; and Ji, H. 2015. Removing rain from a sin-\ngle image via discriminative sparse coding. In ICCV, 3397–\n3405.\nMao, X.; Liu, Y .; Liu, F.; Li, Q.; Shen, W.; and Wang, Y .\n2023. Intriguing findings of frequency selection for image\ndeblurring. In AAAI, volume 37, 1905–1913.\nMichaeli, T.; and Irani, M. 2014. Blind deblurring using\ninternal patch recurrence. In ECCV, 783–798.\nNussbaumer, H. J.; and Nussbaumer, H. J. 1981. The fast\nFourier transform.\nRen, D.; Zuo, W.; Hu, Q.; Zhu, P.; and Meng, D. 2019.\nProgressive image deraining networks: A better and simpler\nbaseline. In CVPR, 3937–3946.\nWang, H.; Xie, Q.; Zhao, Q.; and Meng, D. 2020. A model-\ndriven deep neural network for single image rain removal.\nIn CVPR, 3103–3112.\nWang, T.; Yang, X.; Xu, K.; Chen, S.; Zhang, Q.; and Lau,\nR. W. 2019. Spatial attentive single-image deraining with a\nhigh quality real rain dataset. In CVPR, 12270–12279.\nWang, Z.; Bovik, A. C.; Sheikh, H. R.; and Simoncelli, E. P.\n2004. Image quality assessment: from error visibility to\nstructural similarity. IEEE TIP, 13(4): 600–612.\nWang, Z.; Cun, X.; Bao, J.; Zhou, W.; Liu, J.; and Li, H.\n2022. Uformer: A general u-shaped transformer for image\nrestoration. In CVPR, 17683–17693.\nXiao, J.; Fu, X.; Liu, A.; Wu, F.; and Zha, Z.-J. 2022. Image\nde-raining transformer. IEEE TPAMI.\nYang, W.; Tan, R. T.; Feng, J.; Liu, J.; Guo, Z.; and Yan, S.\n2017. Deep joint rain detection and removal from a single\nimage. In CVPR, 1357–1366.\nYang, W.; Tan, R. T.; Wang, S.; Fang, Y .; and Liu, J. 2020.\nSingle image deraining: From model-based to data-driven\nand beyond. IEEE TPAMI, 43(11): 4059–4077.\nYi, Q.; Li, J.; Dai, Q.; Fang, F.; Zhang, G.; and Zeng, T.\n2021. Structure-preserving deraining with residue channel\nprior guidance. In ICCV, 4238–4247.\nZamir, S. W.; Arora, A.; Khan, S.; Hayat, M.; Khan, F. S.;\nand Yang, M.-H. 2022. Restormer: Efficient transformer for\nhigh-resolution image restoration. In CVPR, 5728–5739.\nZamir, S. W.; Arora, A.; Khan, S.; Hayat, M.; Khan, F. S.;\nYang, M.-H.; and Shao, L. 2021. Multi-stage progressive\nimage restoration. In CVPR, 14821–14831.\nZhang, H.; and Patel, V . M. 2018. Density-aware single\nimage de-raining using a multi-stream dense network. In\nCVPR, 695–704.\nZontak, M.; Mosseri, I.; and Irani, M. 2013. Separating\nsignal from noise using patch recurrence across scales. In\nCVPR, 1195–1202.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n1053",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6121941804885864
    },
    {
      "name": "Computer science",
      "score": 0.49072110652923584
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4047100841999054
    },
    {
      "name": "Engineering",
      "score": 0.24961945414543152
    },
    {
      "name": "Electrical engineering",
      "score": 0.17620041966438293
    },
    {
      "name": "Voltage",
      "score": 0.05127623677253723
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I125904092",
      "name": "Shenyang Aerospace University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I36399199",
      "name": "Nanjing University of Science and Technology",
      "country": "CN"
    }
  ],
  "cited_by": 21
}