{
    "title": "Eliciting Knowledge from Language Models for Event Extraction.",
    "url": "https://openalex.org/W3199247012",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2642144699",
            "name": "Jiaju Lin",
            "affiliations": [
                "East China Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A2098075726",
            "name": "Jian Jin",
            "affiliations": [
                "East China Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A1977215684",
            "name": "Qin Chen",
            "affiliations": [
                "East China Normal University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3176795049",
        "https://openalex.org/W3176456866",
        "https://openalex.org/W2891553865",
        "https://openalex.org/W3121525843",
        "https://openalex.org/W3044438666",
        "https://openalex.org/W2475245295",
        "https://openalex.org/W2946760275",
        "https://openalex.org/W3098267758",
        "https://openalex.org/W3176489198",
        "https://openalex.org/W3170759063",
        "https://openalex.org/W2963248507",
        "https://openalex.org/W1753482797",
        "https://openalex.org/W3165416482",
        "https://openalex.org/W2971296908",
        "https://openalex.org/W3176453404",
        "https://openalex.org/W3102925419",
        "https://openalex.org/W2250575108",
        "https://openalex.org/W3035229828",
        "https://openalex.org/W2971148526",
        "https://openalex.org/W3176634681",
        "https://openalex.org/W3173777717"
    ],
    "abstract": "Eliciting knowledge contained in language models via prompt-based learning has shown great potential in many natural language processing tasks, such as text classification and generation. Whereas, the applications for more complex tasks such as event extraction are less studied, since the design of prompt is not straightforward due to the complicated types and arguments. In this paper, we explore to elicit the knowledge from pre-trained language models for event trigger detection and argument extraction. Specifically, we present various joint trigger/argument prompt methods, which can elicit more complementary knowledge by modeling the interactions between different triggers or arguments. The experimental results on the benchmark dataset, namely ACE2005, show the great advantages of our proposed approach. In particular, our approach is superior to the recent advanced methods in the few-shot scenario where only a few samples are used for training.",
    "full_text": "Eliciting Knowledge from Language Models for Event Extraction\nJiaju Lin,1 Jin Jian,2 Qin Chen3\n1 2 3 East China Normal University\njiaju lin@stu.ecnu.edu.cn , jjin@cs.ecnu.edu.cn , qchen@cs.ecnu.edu.cn\nAbstract\nEliciting knowledge contained in language models via\nprompt-based learning has shown great potential in many nat-\nural language processing tasks, such as text classiﬁcation and\ngeneration. Whereas, the applications for more complex tasks\nsuch as event extraction are less studied, since the design of\nprompt is not straightforward due to the complicated types\nand arguments. In this paper, we explore to elicit the knowl-\nedge from pre-trained language models for event trigger de-\ntection and argument extraction. Speciﬁcally, we present var-\nious joint trigger/argument prompt methods, which can elicit\nmore complementary knowledge by modeling the interac-\ntions between different triggers or arguments. The experi-\nmental results on the benchmark dataset, namely ACE2005,\nshow the great advantages of our proposed approach. In par-\nticular, our approach is superior to the recent advanced meth-\nods in the few-shot scenario where only a few samples are\nused for training.\nIntroduction\nEvent extraction aims to transform text into structural\nrecords. For example, in Figure 1, the raw sentence would\nbe transformed into the structural informative records shown\nin the table on the right side. The table-like records include\nevent types, trigger words and corresponding arguments.\nMainstream methods can be roughly categorized into two\ngroups: (1) The sequence tagging based approaches that\ntreat the event extraction task as trigger/argument identiﬁca-\ntion and classiﬁcation.(Lin et al. 2020)(Wadden et al. 2019)\n(2) The generation based approaches that generate trigger\nand argument tokens given the event descriptions or event\nschema(Lu et al. 2021)(Li, Ji, and Han 2021). Recent studies\nfocus on enhancing the performance with the pretrained lan-\nguage models (PLMs)(Du and Cardie 2020)( ?)(?). In par-\nticular, QA based MRC models are proposed for event ex-\ntraction, which design the related questions manually and\nprovide the answer as the trigger/argument.\nDespite these inspiring progress, current methods still\nstruggle due to the following critical issues. (1) The knowl-\nedge contained in the pretrained language models is not\nfully exploited. (Lu et al. 2021) modiﬁes event extraction\nas a generation task, and introduces a sequence-to-structure\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nFour U.S. soldiers from the 3rd Infantry \nDivision manning a checkpoint near the \ntown of Najaf killed today in an apparent \nsuicide bombing .\n: Arguments : Triggers\n: Direct linkage : Indirect inference Trigger Event \nType\nArgument\nRole Mention\nkilled Die victim soldiers\nplace checkpoint\nbombing Attack target soldiers\nplace checkpoint\nsuicide Die place checkpoint\nFigure 1: An example of event extraction from ACE205\ncorpus. There are three events happening in this sentence.\nWhile only one trigger word, ‘killed’, links to two arguments\ndirectly, other triggers’ arguments need to be inferred via the\ninformation provided by ‘killed’ event.\nmodel to extract trigger and arguments jointly. But the event\nschema they used lacks informative notion, making the po-\ntential of PLMs can not be completely unleashed. (2) Mech-\nanisms to capture the interactions among arguments are\nnot elegant and optimal. (Xiangyu et al. 2021) attempted\nto model the relations among both inter and intra event\narguments via an RNN model. Whereas, the performance\nwas heavily dependent on named entity recognition, which\nwas prone to error propagation. (Wei et al. 2021) intro-\nduced a teacher-student mechanism to distill the knowledge\nfrom various teacher models. Speciﬁcally, the teacher mod-\nels were trained to predict argument given relevant roles of a\nspeciﬁc event, while the student model was trained for learn\nthe knowledge from teachers for argument prediction. To\nimpart all-around knowledge for argument extraction, many\nwell-trained teacher models are needed, which poses big\nchallenge for ﬁne-tuning.\nTo address the above issues, we propose a PrOmpt-based\nKnowledge Eliciting approach (PoKE), which elicits spe-\nciﬁc knowledge from PLMs for event extraction. Specif-\nically, we ﬁrst design trigger/argument prompts based on\nthe annotation guidelines, and then ask the model to ﬁll the\nblanks with trigger words or argument texts. In this way, the\nevent extraction is transformed into a conditional generation\ntask, where the knowledge obtained during pretraining are\nelicited to generate the answer. In order to better capture the\nrelations between different events and arguments, we present\nvarious joint prompt strategies to gauge the complementary\nknowledge about triggers and arguments that occur in the\nsame context.\nOur approach is superior to the existing methods in three\nfolds. (1) We mingle description understanding and interac-\ntion modeling task together. Not only can we elicit knowl-\nedge embedded in PLM, but also making full use of the\nPLM’s understanding ability. For instance, it is challeng-\ning for most existing models to extract all information cor-\nrectly from the sentence in Figure1, where the arguments of\nthree events overlap each other and some trigger-argument\nlinkages are not direct. However, our model is able to in-\nfers the ‘place’ and ‘target’ argument of ‘suicide bombing’\nwith the help of trigger ‘killed’, capturing the interactions\nbetween triggers efﬁciently. (2) Different from current gen-\neration models that generate arguments according to a pre-\ndeﬁned schema, we generate arguments separately. Thus our\nresults would not suffer from the sub-optimal order of gener-\nation. Moreover, our model can take related arguments into\nconsideration with the joint prompt. (3) Instead of introduc-\ning extra modules, we simply adjust the format of input data\nand realize the purpose of capturing interactions among trig-\ngers and arguments. Such brief mechanism decreases the\namount of model parameters and alleviates the difﬁculty of\nﬁne tuning.\nWe evaluate our approach on the benchmark event extrac-\ntion dataset, namely ACE2005. Experimental results show\nthat our approach outperforms the state-of-the-art methods\nfor argument extraction. Besides we accomplish competi-\ntive performance with our event extraction pipeline. It is also\nnotable that our approach is capable of better learning in the\nlow resource set, and outperforms the recent advanced meth-\nods under the few-shot scenario.\nOur contributions can be summarized as follows:\n• As far as we know, it is the ﬁrst attempt to explore the ef-\nfectiveness of eliciting knowledge from PLMs for event\nextraction, where the prompt design is not straightfor-\nward due to the complicated types and arguments.\n• We develop various joint prompt strategies, which can\ncapture the interactions among different triggers, intra\nand inter-event arguments, without any redundant mod-\nules.\n• Extensive experiments on the most widely used ACE-\n2005 event extraction dataset demonstrate the superiority\nof our approach under both fully supervised and few-shot\nlearning scenarios.\nThe Proposed Approach\nIn this section, we ﬁrst present an overview of our frame-\nwork. Then, we introduce various prompt methods to elicit\nknowledge from PLMs for trigger identiﬁcation and ar-\ngument extraction. After that we describe the details of\nprompt-based event extraction.\nFramework Overview\nOur PoKE framework depends on the natural language\nunderstanding ability of T5. To be speciﬁc, the different\nprompts we design would make our model understand the\npurpose of our task, i.e. comprehend linkage between an-\nswer and given context, rather than remember the distribu-\ntion of answers. For event detection task, we split it into two\nparts. First, we generate the trigger words corresponding to\n8 main types in each sentence,i.e., Movement, Personnel,\nConﬂict, Contact, Life, Transaction, Justice, Business. With\nthat result, we further classify triggers into 33 subtypes, also\nmodel it as a generation task. For event argument extraction\ntask, our model would ﬁll the bank in each prompt with ar-\ngument span from the sentence, with trigger words and event\ntype provided.\nEvent Detection Owing to the diversity of event subtypes,\nwe ﬁrst extract triggers according to 8 main types, and then\nclassify them into subtypes. For extracting triggers for main\ntype, we develop two joint trigger prompts to elicit more\nrelated knowledge by modeling the event interactions.\nExternal Joint Trigger Prompt.Given a sentence, we\nadd 8 prompts of all main types after it, asking the model\nto generate the trigger words jointly. In this manner, model\ncan detect the discrepancy between various event types. In-\ntuitively, we use the event type as an inevitable part of the\nprompt. Besides, considering the fact that contexts may ren-\nder cues for the event, we expand the boundary of sentence.\nWith a window of size=100 words, our sentence is set at the\nmiddle position and includes contexts in the window into the\nﬁnal passage. To illuminate which sentence triggers should\nbe extracted from, two special tokens ‘ ⟨e⟩’ and ‘⟨/e⟩’ are\napplied to mark the original boundary of the raw sentence.\nFurthermore, to make the paragraph more natural sounding,\nwe insert a bridge section, ‘In the passage above’, between\nthe passage and prompts. A typical trigger external joint trig-\nger prompt is shown in Figure 2. If an event does not occur\nin the sentence, we use ‘None’ to represent the NA answer.\nInternal Joint Trigger Prompt.Given the fact that rele-\nvant events occur close to each other in a paragraph of text,\nit is sensible to teach our model how to capture the interac-\ntion between triggers. Due to that reason, we present internal\njoint trigger prompt for trigger extraction. Events within one\nsentence has certain relation with each other. Besides, we\nassume that the events appearing within a span of three sen-\ntences have potential interdependence. Thus we ﬁlter ineligi-\nble samples and concatenate adjoin eligible sentences as raw\nsentences. Similar to trigger prompt, we employ a window\nwith size = 120 words to incorporate the contextual informa-\ntion. Unlike external joint trigger prompt that the masks are\noutside of the raw sentence, the internal joint trigger prompt\nmasks the trigger words inside the raw sentence as shown in\nFigure 2\nWith the identiﬁed triggers for the main event type, we\nneed to further classify them into 33 subtypes. We inherit\nthe trigger prompts and make classiﬁcations among the\nsubtypes. As shown in Figure 2, with the identiﬁed trig-\ngers ‘convicted’ and ‘sentenced’ for the main event type\n‘Justice’, we perform ﬁne-grained classiﬁcations to clas-\nsify them into the subtypes of events, namely ‘Convivt’ and\n‘Sentence’.\nArgument Extraction After event detection, we need to\nextract the event related arguments for better understanding.\nEvent Type Justice-Convict Justice-Sentence\nTrigger convicted sentenced\nAbbas was convicted in absentia by an Italian court and sentencedto five life terms in prison.\nRaw Sample\nCONTEXT <e>Abbas was <X> in absentia by an Italian court\nand <Y> to five life terms in prison.</e> CONTEXT\nPassage\nIn the passage above，\nBridge\nInternal Joint Trigger Prompt\nverb *convicted | sentenced * indicates Justice Event. \nverb *None* indicates Conflict Event. …\n…\nT5\n<X>=convicted <Y>=sentenced\nsentenced\nterms\nAbbas\nprison\nconvicted\nabsentia\ncourt\nprison\n: Triggers\n: Concatenate Jointly\n: Concatenate Respectively\n: Masked Triggers\nPrompts\n<X1>=convicted | sentenced  <X2>=None <X3>=None ….\nsentenced\nterms\nAbbas\nprison\nconvicted\nabsentia\ncourt\nprison\nCONTEXT <e>Abbas was convicted in absentia by an Italian court\nand sentenced to five life terms in prison.</e> CONTEXT\nPassage\nIn the passage above，\nBridge\nExternal Joint Trigger Prompt\nverb *<X1>* indicates Justice Event. \nverb *<X2>* indicates Conflict Event. …\nPrompts\nFigure 2: Architecture of PoKE for coarse-grained event detection. It is the ﬁrst part of event detection, in this step, we extract\ntriggers for eight main types. For this purpose, we introduce external joint trigger prompt and internal joint trigger prompt. We\ndevelop the former by setting masks in the prompts, to teach model how to infer trigger via the combination of passage and\nprompts. To generate the latter, we mask the triggers in the passage, asking the model to ﬁll the mask via the information in the\nprompts. During this process, our model would model the interactions among triggers.\nFor example, given the description of ‘convicted’ event, it is\nnecessary to identify different argument roles as ‘defendant’\nand ‘adjudicator’ to make further decisions. In this section,\nwe present a single and joint argument based prompts for\nargument extraction.\nSingle Argument Prompt.For each role in an event, to\nincorporate more semantic information into our templates,\nwe ﬁrst utilize the descriptions in the ACE annotation guide-\nlines for events. Particularly, we make a masked position in\nthe argument description text and urge the model to generate\nthe corresponding words. In the case where multiple entities\nin a passage represent the same argument, we insert ‘|’ in the\ngold answer to split them. In addition, it is necessary for the\nmodel to have the knowledge of the trigger and its type. To\nprovide the trigger and event information, we stick the event\ntype and the corresponding trigger at end of the sentence.\nBoth the trigger word in the sentence and template is sur-\nrounded by a pair of special tokens as ⟨t⟩⟨/t⟩. An example\nof the single argument prompt is presented in Figure 3.\nJoint Argument Prompt.When trying to understand an\nevent from a passage, it is natural for a human to infer-\nence the arguments from other co-occurred arguments. To\nstimulate such learning process, we present a joint argument\nprompt method to elicit more related knowledge by mod-\neling the interactions among the intra-event arguments and\ninter-event arguments.\nFor each sentence that contains an event, we provide the\ncorresponding arguments information behind it, while mask\nthe arguments words in the sentence. The ACE annotation\nguidelines are applied to generate the arguments descrip-\ntion information. Also, the trigger word and event type are\nadded after the argument descriptions. Besides, to prevent\nour model from cheating by heuristic relations, like ﬁlling\nthe blanks according to the order of arguments mentioned\nin prompts, we shufﬂe the list of prompts randomly. Fig-\nure 3 shows an example of our joint argument prompt. With\nthis prompt, more complementary knowledge can be elicited\nfrom the pre-trained language models, building solid rela-\ntions between the descriptions in prompts and the concrete\nargument role for more accurate argument extraction.\nPrompt-based Event Extraction\nSince our base model architecture is an encoder-decoder de-\nsign. We employed T5 for the generation task. The gener-\nation process follows T5 pre-training task, a slew of ex-\ntra tokens, from ‘⟨extra id 0⟩’ to ‘⟨extra id 99⟩’ are ex-\nploited as masks. We replace the speciﬁc word spans with\nthese special tokens. And the generation output is the an-\nswers joined by these extra tokens, like ⟨extra id 0⟩Ar-\ngument 0 ⟨extra id 1⟩Argument 1 ⟨extra id 2⟩⟨/s⟩, here\n⟨/s⟩means the end of the sentence.\nThe generation process models the conditional probability\n<X>=Abbas<X>=None\nJoint Argument Prompt\n<X> was </t0>convicted</t0> in absentia by an Italian <Y> and <t1>sentenced</t1> to five life terms in prison.\nPassage\nin this Justice.Convicted\n_<t0> convicted </t0> event\nSuffix\nIn the passage above，\nBridge\nAbbas is convicted,\nThe event takes place None,\ncourt is the judge, Prompts\nin this Justice.Sentence\n_<t1> sentenced </t1> event\nSuffix\nAbbas is sentenced,\nThe event takes place None,\ncourt is the judge, Prompts\nT5\nEvent Type Justice-Convict Justice-Sentence\nTrigger convicted sentenced\nArguments Role Defendant Adjudicator Defendant Adjudicator\nMention Abbas court Abbas court\nAbbas was convicted in absentia \nby an Italian court and sentenced to five life \nterms in prison .\nRaw Sample\n: Masked Arguments\n: Triggers\n: Concatenate Jointly\n: Concatenate Respectively\nAbbas was </t0>convicted</t0> in absentia by an Italian court and <t1>sentenced</t1> to five life terms in prison.\nPassage\nin this Justice.Convict_<t0>convicted</t0> event\nSuffix\nIn the passage above，\nBridge\nin this Justice.Sentence_<t1>sentenced</t1> event\nSuffix\nIn the passage above，\nBridge\nSingle Argument Prompt\n<X> is convicted.\nThe event takes place <X>.\n<X> is the judge.\nPrompts\n<X> is sentenced.\nThe event takes place <X>.\n<X> is the judge.\nPrompts\nAbbas\nterms\nItalian\nprison\n<X>=Abbas     <Y>=court\n<X>=Abbas\ncourt\nterms\nItalian\nprison\nAbbas\nfive\ncourt\nprison\nFigure 3: Architecture of PoKE for argument extraction. In the training stage, we design two varieties of prompts: Joint Argu-\nment Prompt and Single Argument Prompt. To generate the former, we mask the arguments in the sentence, asking the model to\nﬁll the mask via the information in the prompts. During this process, our model would model the interactions among arguments\nand triggers. Meanwhile, we develop Single Argument Prompt, whose masks are in the prompts, to teach model how to infer\narguments in the inference stage.\nof selecting a new token given the previous tokens and the\ninput to the encoder.\np(x|prompt) =\n|x|∏\ni=1\np(xi|xi, prompt) (1)\nThe model is trained by minimizing the cross entropy loss\nover vocabulary V .\n−\nV∑\ni=1\nyi log(pi(x|prompt)) (2)\nIn inference stage, we use greedy decoding, i.e. choosing the\nhighest-probability logit at every times step. Given the fact\nthat the vocabulary space is huge and it is possible for the\nmodel to generate words out of the passage. We constrain\nthe candidate vocabulary to Vs the set of tokens in the input\n,by setting logits of extraneous words to -inf.\np =\n{\np(x|prompt) w ∈Vs\n−inf w / ∈Vs\n(3)\nExperiments\nExperimental Setup\nData set. We evaluate our work on the most widely used\nACE2005 dataset, containing 599 documents annotated with\n33 event subtypes and 35 argument types. For fair compe-\ntition, we follow the same split and preprocessing step as\nthe previous work(Yang et al. 2018) (Wadden et al. 2019)\n(Du and Cardie 2020). The data set splits are as follows, 40\nnewswire documents as test set, 30 randomly selected doc-\numents for development and a training set including the rest\n529 documents.\nBaselines. For complete event extraction task we compare\nour method with ﬁve baselines. Considering the difference\nin the annotation methods and label-intensity, we categorize\nthem into three types following (Lu et al. 2021) 1.Baseline\nusing token annotation and entity annotation: OneIE (Lin\net al. 2020), an end-to-end system, employs global features\nto conduct named entity recognition ,relation extraction and\nevent extraction at the same time.\n2.Baselines using token annotation: CondiGen (Li, Ji, and\nHan 2021) is one of the cutting-edge method for argument\nextraction task. It utilizes the conditionally generation abil-\nity of BART. Given description of arguments and their rela-\ntions then generate the corresponding words in the sentence.\nThe candidate arguments are predicted jointly.However, it\ngives up generation model and introduces a modiﬁed Tap-\nNet for event detection, formulating it as classic sequence\ntagging task. EEQA which is one of the earliest work that in-\ntroduces a new paradigm for event extraction by formulating\nit as a question answering task. It generates questions from\nannotation guidelines and raise question for each argument\nTrigger-C Arg-C\nModel P R F1 P R F1 PLM Annotation\nOneIE - - 74.7 - - 56.8 BERT-large Token+Entity\nText2Event - - 69.2 46.7 53.4 49.8 T5-base TextRecord\nTANL - - 68.5 - - 48.5 T5-base Token\nEEQA 71.1 73.7 72.3 56.7 50.2 53.3 2*BERT Token\nCondiGen - - 71.1 - - 53.7 BART-large Token\nPoKE 64.1 76.2 69.6 47.7 58.1 52.4 T5-base Token\nTable 1: Experiment results of event extraction on ACE2005. Trig-C indicates trigger identiﬁcation and classiﬁcation. Arg-C\nindicates argument identiﬁcation and classiﬁcation. The column PLM indicates the pre-trained language models used by each\nmethod.\nModel P R F1 Encoder/PLM\nHMEAE 66.04 68.58 67.28 CNN\nCondiGen 69.66 67.63 68.83 BART-large\nTANL 65.19 64.21 64.69 T5-base\nEEQA 67.88 63.02 65.36 2*BERT\nPoKE single only 60.79 64.92 62.79\nPoKE w.o. inter 61.52 67.13 64.20 T5-base\nPoKE 66.20 74.48 70.10\nTable 2: Results of event argument extraction. PoKE single\nonly means we train the model only with single argument\nprompts. PoKE w.o. inter indicates we replace joint argu-\nment prompts with the single prompts but mask in the pas-\nsage, which means no interaction information is impart to\nthe model.\nunder one event type respectively(Du and Cardie 2020).\nTANL, a sequence generation-based method, frame event\nextraction as a translation task between augmented natural\nlanguages. Multi-task TANL extends TANL by transferring\nstructure knowledge from other tasks.(Paolini et al. 2021)\n3. Baseline using Text-Record Annotation: TexT2Event,\nan end-to-end generation-based, modeling event-detection\nand argument extraction tasks in a single model. Compared\nwith normal generation methods, it outputs a structural event\nrecord with event type, trigger, and arguments(Lu et al.\n2021).\nImplementation. We optimized our model using AdamW\n(Kalchbrenner and Blunsom 2013) with different hyperpa-\nrameters for each task. For coarse-grained event detection,\nwe set learning rate=1e-4 training epoch=6. For ﬁne-grain\ntrigger classiﬁcation, learning rate=5e-4 batch size=8 train-\ning epoch=3. For argument extraction task, learning rate=1e-\n3 and batch size=64. The experiments are conducted on a\nsingle NVIDIA GeForce RTX 3090. The random seed set\nfor all tasks is 42.\nSupervised Learning Results\nEvent Extraction Results The performance of PoKE and\nall baselines on ACE05 are presented in Table 1, where\n‘Trig-C’ indicates trigger identiﬁcation and classiﬁcation\nand ‘Arg-C’ reﬂects the result of argument identiﬁcation and\nclassiﬁcation. We can observe that by leveraging interac-\ntions among triggers and arguments and taking advantage\nof prompts, our model achieve comparable or better perfor-\nmance with the baseline models. Compared with the other\ntwo global-generation model, TANL and Text2Event, our\nmethod overtake them both in event detection and argument\nextraction task. Notice that the results of argument extrac-\ntion is directly affected by the output of event detection. An\nerror prediction of event type would render wrong candidate\nclasses for argument extractor. Hence the reduction in F1\nafter argument extraction indicates our merits in argument\nextraction.\nArgument Extraction Results Since the current compar-\nisons are based on the trigger predictions of previous event\ndetection work(Wang et al. 2019a). It maybe reasonable\nwhen no advance can be achieved for event detection task.\nHowever, with the rapid evolution in event detection meth-\nods(Pouran Ben Veyseh et al. 2021), we believe such previ-\nous criterion can not show the whole picture of each archi-\ntecture’s ability. Hence, following the hyper parameter set-\ntings mentioned in their paper, we reproduce the results of\nseveral cutting-edge models with gold triggers as input to do\na fair comparison. Meanwhile, we introduce another base-\nline developed for event argument extraction speciﬁcally.\nHMEAE which applies a concept hierarchy indicating ar-\ngument roles. With the help of hierarchical modular atten-\ntion, correlation between argument roles sharing the same\nhigh-level unit can be utilized.(Wang et al. 2019b). It should\nbe noticed that HMEAE takes use of the named entity infor-\nmation as candidate arguments. Also, we do some ablation\nstudy to examine the efﬁciency of our model. ‘PoKE single\nonly’ means we train the model only with single argument\nprompts. ‘PoKE w.o. inter’ indicates we replace joint argu-\nment prompts with the single prompts but mask in the pas-\nsage, which means no interaction information is impart to\nthe model.\nThe results are shown in Table 2, from which we can observe\nthat. (1) Our method surpasses all baseline methods, with an\nabsolute improvement of at least 1.3 in F1. (2) Models which\nconsider the interactions between arguments perform better.\nFor instance, CondiGen is trained to generate all arguments\nof a single event jointly. Although the manual templates used\nfor its inference are sub-optimal, it overtakes EEQA with\n3.1 in F1. Taking advantages of both joint prediction and in-\nformative description, our model achieves the sate-of-the-art\nperformance. (3) Named entity annotation is not an absolute\nMethod P R F1\nEDA 54.46 77.43 63.94\nBackTranslation 54.03 76.95 63.48\nPoKE single only 60.79 64.92 62.79\nPoKE 66.20 74.48 70.10\nTable 3: Compare PoKE with Data Augment Methods\nnecessity. Although without the assistance of named entity\nrecognition results, models utilize PLMs achieve competi-\ntive performances. Particularly, input of HMEAE are sen-\ntences with named entity annotation, however, its encoder, a\nvanilla CNN, restricts its ability. (4) Joint argument prompts\nenhance model’s performance dramatically, with almost 8 in\nF1 , from 62.79 to 70.10 .\nFigure 4: Few-shot experiment results. K-shot corresponds\nto the number of data in training and validation set.\nFew Shot Scenario\nImitating few-shot settings in LM-BFF(Gao, Fisch, and\nChen 2021), we conduct 8,16,32 shot experiments to ex-\namine the few-shot ability of our model. Speciﬁcally, for\neach event type, we sample k instances from initial train-\ning and validation set. After training, we test the model per-\nformance on standard scale test set. The experiment results\nare in Table??. It can be observed that our model perform\nbetter than baselines in few-shot scenario. Although EEQA\nand CondiGen both have outstanding zero-shot ability, when\nit comes to few-shot task, they are exceeded by our PoKE.\nEven a 32-shot trained EEQA model can not overtake PoKE\ntrained by 8-shot data. Our mechanism leverages naturalness\ninto prompts which ensure its few-shot performance.\nCompared with Data Augment Methods\nPoKE increases the number and variety of training samples,\nwhich would enhance model’s ability naturally, while data\naugment methods can improve model’s performance in the\nsame way. Hence we compare our method with two most\ncommon seen data augment methods, back translation and\nEDA(Wei and Zou 2019),which consists of four word-level\noperations, replacement, insertion, swap, and deletion, to\nboost diversity of sentences. Experiments are conducted on\nargument extraction task. We discard the interaction learning\ndata for two data augment methods and expand the argument\ninference data set to the same number of training data used\nfor PoKE. The results are demonstrated in Table 3. From\nthe result, we can see that compared with using inference\ndata only, data augment methods do improve model’s per-\nformance. Nonetheless, the improvement is limited. It fur-\nther demonstrates that our mechanism unleash PLM’s un-\nderstanding ability.\nCase Study and Error Analysis\nCase Study\nTo demonstrate how our model utilize information from co-\noccuring events, we show extraction results of samples with\nmulti events in Figure5. Sentence S1 contains two trigger\nwords: ‘destroyed’ and ‘killed’, indicating ‘Die’ and ’At-\ntack’ event respectively. It is a common fact that the agent\nresponsible for a death is usually the attacking agent in an at-\ntack. Also, word ‘commandos’ is the subject of the sentence,\nstrongly syntactically related to both two triggers. Given\nthat, our model is capable of exploiting the clues hidden be-\nhind and makes the correct prediction of ‘Die Agent’ and\n‘AttackAttacker’. Besides, although there is a false predic-\ntion of ‘Victim’, i.e., our model identiﬁes ‘soldiers’ as vic-\ntims while no ‘Victim’ is annotated. We believe it is the gold\nanswer that contains a mistake. In many cases, our model\nmakes more sensible argument extraction that human anno-\ntated data.\nError Analysis\nTo facilitate further research, we investigate the error predic-\ntions of our model. We randomly sampled 50 wrong answers\nand attributed them to 5 types. Here we list 3 most common\nproblems caused by our model design.\nShallow Heuristics obtained in training set(24%)Data\nin the training set always contains some bias that induce\nheuristic relations between entity mentions and correspond-\ning argument types. Model learns these heuristic relations\nto enhance its performance, while neglect the intended task.\nWe deﬁne the event argument extraction task as utilizing the\nsentence s, trigger t, event type e to infer corresponding ar-\nguments a, {s, t, e}→{ a}. However, we observed that, in\nsome sense, extraction of arguments relies on the entity men-\ntion in the sentence and event type, {s, e}→{ a}, omitting\nthe relation between entity and trigger word. For example,\nin the training set, when an event about ‘Justice’ and entity\n‘court’ appear in the same sentence, ‘court’ always plays a\nrole of ‘Adjudicator’. Thus, when the same situation occurs\nin the inference period, our model identiﬁes ‘court’ in the\nsentence as ‘Adjudicator’, no matter whether it is linked to\nthe trigger word.\nOne of the error predictiond is as follows:\nThe Belgrade district [court]Adjudicator said that Markovic\nwill be tried along with 10 other Milosevic-era ofﬁcials who\nface similar ⟨t0⟩charges⟨/t0⟩of inappropriate use of state\nproperty that carry a sentence of up to ﬁve years in jail.\nIn this instance, ‘court’ has no direct relation to the trig-\nger ‘charge’.Thus the gold annotation of ‘Adjudicator’ is\nSentence Australian commandos , who have been operating deep in Iraq , destroyed a \ncommand and control post and killed a number of soldiers , according to the \ncountry's defense chief , Gen. Peter\nTrigger destroyed killed\nEvent Type Attack Die\nArgument Type Attacker Target Place Agent Victim Place\nGolden Annotation commandos post Iraq commandos NULL Iraq\nPKEE prediction commandos post Iraq commandos soldiers Iraq\nEEQA prediction commandos post Iraq NULL soldiers NULL\nFigure 5: Case study of PoKE and EEQA\n‘None’. However, due to the wrong experience accumulated\nin training stage, our model classiﬁes it as a adjudicator be-\ncause the sentence contains a Justice-Sentence event.\nAmbiguous expression (20%)Owing to the ambiguity\nof natural language, even human reader would make some\nmistakes, especially under the situation where no context are\ngiven. Ambiguous expression error refers to the case when\nour model makes a prediction inconsistent with gold answer,\nbut reasonable from some perspectives. These mistakes usu-\nally occur when inferring ‘Place’ arguments or arguments\nexpressed implicitly. For example, in the sentence ‘Kelly ,\nthe US assistant secretary for East Asia and Paciﬁc Affairs ,\narrived in[Seoul]Place from Beijing Friday to⟨t0⟩brief⟨/t0⟩\nYoon , the foreign minister.’ Seoul is predicted as the ‘Place’\nof ‘Contact.Meet’ event, but no ‘Place’ argument is anno-\ntated. It is true that the place of meeting is not mentioned\nclearly, but for humans, applying rhetoric to imply implicit\narguments is not rare sense.\nWe also ﬁnd the gold sample expressing place argument\nimplicitly: ⟨t0⟩Former⟨/t0⟩ [Illinois]Place Senator Carol\nMoseley - Braun recently decided to run for president . Here\n‘Illinois’ is annotated as the place where employment rela-\ntionship ends.\nCoreference & head of entity mistake(10%)Errors as-\ncribed to this kind refers to the situation when the model pre-\ndicts a coreferent of gold annotation or the whole entity span\nrather than the head of it. For example, “ [Erdogan]Person\n, a leader of Turkey ’s pro - Islamic movement when\nhe was ⟨t0⟩jailed⟨/t0⟩, said he moderated his policies\nin prison .” ‘Erdogan’ is predicted as ’Person’ argument\nof ’Justice.Arrest-Jail’ event, while the gold annotation is\n’leader’. For head of entity mistake, the model fails to ﬁgure\nout the head word of entity span correctly. Here is a con-\ncrete example: “ Anwar will be taken to the⟨t0⟩appeal⟨/t0⟩\ncourt early Friday for a bail application pending his appeal\nto the country ’s highest[FederalCourt ]Adjudicator against\nhis sodomy conviction , counsel Sankara Nair said .” The\ngold answer of ’Adjudicator’ is ’Court’ but ‘Federal Court’\nis predicted by our model.\nRelated Work\nEvent extraction\nMost efforts in event extraction ﬁeld are concentrated\non the ACE2005 corpus. Various of neural models have\nbeen exploited for better performance. (Nguyen and Grish-\nman 2015) ﬁrst introduced Convolutional Meural Metwork,\n(CNN) for event detection for better representation of fea-\ntures. And (Nguyen, Cho, and Grishman 2016) employed re-\ncurrent neural network to build robust model.Besides, Graph\nConvulutional Neural Networks (Liu, Luo, and Huang 2018)\nand plenty of fancy ideas have been applies to handle difﬁ-\nculties we meet(Lu et al. 2021). Recently, there is a trend\nof modeling event extraction into QA-based tasks (Du and\nCardie 2020)(?). These researches employ reading compre-\nhension abilities of natural language models. Despite these\nadvantages, current methods still suffers from the bottle-\nnecks mentioned in Introduction.\nPrompt-based learning\nPrompt-based learning is based on language models that\nmodel the probability of text directly. Prompt is a piece of\nstring that contains descriptive information about the answer\nof the task. The language model is used to probabilistically\nﬁll the unﬁlled slot to obtain a ﬁnal string as the prediction of\nthe task. Exploration in prompt methods can be categorized\ninto prompt engineering which refers to creating a prompt-\ning function fp(x) that results in efﬁcient performance on\nthe downstream task(Shin et al. 2020)(Jiang et al. 2020), an-\nswer engineering which targets at looking for an optimal\nanswer space and a map to the original output (Shin et al.\n2020) and multi-prompt learning which involves prompt en-\nsemble(Yuan, Neubig, and Liu 2021), prompt augmenta-\ntion(Gao, Fisch, and Chen 2021), prompt composition(Han\net al. 2021) and prompt decomposition(Cui et al. 2021). In\nthis work, we utilize manual prompts and take the idea of\nprompt ensemble, but only modify the format of prompts in\ntraining stage to increase the PLM’s understanding of the\ntask.\nConclusion and Future Work\nWe propose a prompt-based approach (PoKE), which in-\ncludes various trigger/argument prompts to elicit knowledge\nfrom language models for event extraction. In particular,\nthe prompts model the interactions between different events\nand arguments, which gauges more complementary knowl-\nedge and yields better performance. The experimental re-\nsults demonstrate the superiority of our approach in achiev-\ning comparable performance without additional model pa-\nrameters or modules. It is also notable that we can still per-\nform well under that few-shot scenario. In the future, we\nwill introduce automatic prompt methods for this task and\nexplore the effectiveness of prompt for joint event extrac-\ntion.\nReferences\nCui, L.; Wu, Y .; Liu, J.; Yang, S.; and Zhang, Y . 2021.\nTemplate-Based Named Entity Recognition Using BART. CoRR,\nabs/2106.01760.\nDu, X.; and Cardie, C. 2020. Event Extraction by Answering (Al-\nmost) Natural Questions. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Processing (EMNLP),\n671–683. Online: Association for Computational Linguistics.\nGao, T.; Fisch, A.; and Chen, D. 2021. Making Pre-trained Lan-\nguage Models Better Few-shot Learners. InProceedings of the 59th\nAnnual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), 3816–3830. Online: Associ-\nation for Computational Linguistics.\nHan, X.; Zhao, W.; Ding, N.; Liu, Z.; and Sun, M. 2021.\nPTR: Prompt Tuning with Rules for Text Classiﬁcation. CoRR,\nabs/2105.11259.\nJiang, Z.; Xu, F. F.; Araki, J.; and Neubig, G. 2020. How Can We\nKnow What Language Models Know? Transactions of the Associ-\nation for Computational Linguistics, 8: 423–438.\nKalchbrenner, N.; and Blunsom, P. 2013. Recurrent Continuous\nTranslation Models. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing, 1700–1709.\nSeattle, Washington, USA: Association for Computational Lin-\nguistics.\nLi, S.; Ji, H.; and Han, J. 2021. Document-Level Event Argument\nExtraction by Conditional Generation. In Proceedings of the 2021\nConference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, 894–\n908. Online: Association for Computational Linguistics.\nLin, Y .; Ji, H.; Huang, F.; and Wu, L. 2020. A Joint Neural Model\nfor Information Extraction with Global Features. InProceedings of\nthe 58th Annual Meeting of the Association for Computational Lin-\nguistics, 7999–8009. Online: Association for Computational Lin-\nguistics.\nLiu, X.; Luo, Z.; and Huang, H. 2018. Jointly Multiple Events Ex-\ntraction via Attention-based Graph Information Aggregation. In\nProceedings of the 2018 Conference on Empirical Methods in Nat-\nural Language Processing, 1247–1256. Brussels, Belgium: Asso-\nciation for Computational Linguistics.\nLu, Y .; Lin, H.; Xu, J.; Han, X.; Tang, J.; Li, A.; Sun, L.; Liao,\nM.; and Chen, S. 2021. Text2Event: Controllable Sequence-to-\nStructure Generation for End-to-end Event Extraction. InProceed-\nings of the 59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long Papers), 2795–\n2806. Online: Association for Computational Linguistics.\nNguyen, T. H.; Cho, K.; and Grishman, R. 2016. Joint Event Ex-\ntraction via Recurrent Neural Networks. In Proceedings of the\n2016 Conference of the North American Chapter of the Associa-\ntion for Computational Linguistics: Human Language Technolo-\ngies, 300–309. San Diego, California: Association for Computa-\ntional Linguistics.\nNguyen, T. H.; and Grishman, R. 2015. Event Detection and Do-\nmain Adaptation with Convolutional Neural Networks. InProceed-\nings of the 53rd Annual Meeting of the Association for Computa-\ntional Linguistics and the 7th International Joint Conference on\nNatural Language Processing (Volume 2: Short Papers), 365–371.\nBeijing, China: Association for Computational Linguistics.\nPaolini, G.; Athiwaratkun, B.; Krone, J.; Ma, J.; Achille, A.; Anub-\nhai, R.; dos Santos, C. N.; Xiang, B.; and Soatto, S. 2021. Struc-\ntured Prediction as Translation between Augmented Natural Lan-\nguages. In 9th International Conference on Learning Representa-\ntions, ICLR 2021.\nPouran Ben Veyseh, A.; Lai, V .; Dernoncourt, F.; and Nguyen, T. H.\n2021. Unleash GPT-2 Power for Event Detection. In Proceedings\nof the 59th Annual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers), 6271–6282. On-\nline: Association for Computational Linguistics.\nShin, T.; Razeghi, Y .; Logan IV , R. L.; Wallace, E.; and Singh, S.\n2020. AutoPrompt: Eliciting Knowledge from Language Models\nwith Automatically Generated Prompts. InProceedings of the 2020\nConference on Empirical Methods in Natural Language Process-\ning (EMNLP), 4222–4235. Online: Association for Computational\nLinguistics.\nWadden, D.; Wennberg, U.; Luan, Y .; and Hajishirzi, H. 2019.\nEntity, Relation, and Event Extraction with Contextualized Span\nRepresentations. In Proceedings of the 2019 Conference on Em-\npirical Methods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), 5784–5789. Hong Kong, China: Association\nfor Computational Linguistics.\nWang, X.; Han, X.; Liu, Z.; Sun, M.; and Li, P. 2019a. Adver-\nsarial Training for Weakly Supervised Event Detection. In Pro-\nceedings of the 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), 998–1008. Min-\nneapolis, Minnesota: Association for Computational Linguistics.\nWang, X.; Wang, Z.; Han, X.; Liu, Z.; Li, J.; Li, P.; Sun, M.; Zhou,\nJ.; and Ren, X. 2019b. HMEAE: Hierarchical Modular Event Ar-\ngument Extraction. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), 5777–5783. Hong Kong, China: Association\nfor Computational Linguistics.\nWei, J.; and Zou, K. 2019. EDA: Easy Data Augmentation Tech-\nniques for Boosting Performance on Text Classiﬁcation Tasks. In\nProceedings of the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-IJCNLP), 6382–\n6388. Hong Kong, China: Association for Computational Linguis-\ntics.\nWei, K.; Sun, X.; Zhang, Z.; Zhang, J.; Zhi, G.; and Jin, L. 2021.\nTrigger is Not Sufﬁcient: Exploiting Frame-aware Knowledge for\nImplicit Event Argument Extraction. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), 4672–4682. Online: Associ-\nation for Computational Linguistics.\nXiangyu, X.; Ye, W.; Zhang, S.; Wang, Q.; Jiang, H.; and Wu, W.\n2021. Capturing Event Argument Interaction via A Bi-Directional\nEntity-Level Recurrent Decoder. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), 210–219. Online: Association\nfor Computational Linguistics.\nYang, P.; Sun, X.; Li, W.; Ma, S.; Wu, W.; and Wang, H. 2018.\nSGM: Sequence Generation Model for Multi-label Classiﬁcation.\nIn Proceedings of the 27th International Conference on Computa-\ntional Linguistics, 3915–3926. Santa Fe, New Mexico, USA: As-\nsociation for Computational Linguistics.\nYuan, W.; Neubig, G.; and Liu, P. 2021. BARTScore: Evaluating\nGenerated Text as Text Generation. arXiv:2106.11520."
}