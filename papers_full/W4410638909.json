{
  "title": "Evaluating the performance of large language &amp; visual-language models in cervical cytology screening",
  "url": "https://openalex.org/W4410638909",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2017260057",
      "name": "Qi Hong",
      "affiliations": [
        "Southern Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A2121687581",
      "name": "Shijie Liu",
      "affiliations": [
        "Huazhong University of Science and Technology",
        "Wuhan National Laboratory for Optoelectronics"
      ]
    },
    {
      "id": "https://openalex.org/A2139579664",
      "name": "Liying Wu",
      "affiliations": [
        "Union Hospital",
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2098179378",
      "name": "Qiqi Lu",
      "affiliations": [
        "Southern Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A3029094557",
      "name": "Pinglan Yang",
      "affiliations": [
        "Huazhong University of Science and Technology",
        "Union Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2428555904",
      "name": "Dingyu Chen",
      "affiliations": [
        "Huazhong University of Science and Technology",
        "Union Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2600295535",
      "name": "Gong Rao",
      "affiliations": [
        "Wuhan National Laboratory for Optoelectronics",
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2097023228",
      "name": "Xinyi Liu",
      "affiliations": [
        "Huazhong University of Science and Technology",
        "Wuhan National Laboratory for Optoelectronics",
        "Southern Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A2102814332",
      "name": "Hua Ye",
      "affiliations": [
        "Southern Medical University"
      ]
    },
    {
      "id": null,
      "name": "Peiqi Zhuang",
      "affiliations": [
        "Southern Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A2105691580",
      "name": "Wenxiu Yang",
      "affiliations": [
        "Southern Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A2168061948",
      "name": "Shaoqun Zeng",
      "affiliations": [
        "Huazhong University of Science and Technology",
        "Wuhan National Laboratory for Optoelectronics"
      ]
    },
    {
      "id": "https://openalex.org/A2115134365",
      "name": "Qian-Jin Feng",
      "affiliations": [
        "Southern Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A2097098261",
      "name": "Xiuli Liu",
      "affiliations": [
        "Southern Medical University",
        "Wuhan National Laboratory for Optoelectronics",
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2096374510",
      "name": "Jing Cai",
      "affiliations": [
        "Union Hospital",
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2110622856",
      "name": "Shenghua Cheng",
      "affiliations": [
        "Southern Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A2017260057",
      "name": "Qi Hong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2121687581",
      "name": "Shijie Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2139579664",
      "name": "Liying Wu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098179378",
      "name": "Qiqi Lu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3029094557",
      "name": "Pinglan Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2428555904",
      "name": "Dingyu Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2600295535",
      "name": "Gong Rao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097023228",
      "name": "Xinyi Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102814332",
      "name": "Hua Ye",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Peiqi Zhuang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105691580",
      "name": "Wenxiu Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2168061948",
      "name": "Shaoqun Zeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2115134365",
      "name": "Qian-Jin Feng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097098261",
      "name": "Xiuli Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096374510",
      "name": "Jing Cai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2110622856",
      "name": "Shenghua Cheng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4393935425",
    "https://openalex.org/W2138556740",
    "https://openalex.org/W3171746194",
    "https://openalex.org/W3156423522",
    "https://openalex.org/W4394008820",
    "https://openalex.org/W4398229357",
    "https://openalex.org/W4387372193",
    "https://openalex.org/W4365141522",
    "https://openalex.org/W4387609066",
    "https://openalex.org/W4309022712",
    "https://openalex.org/W2329826697",
    "https://openalex.org/W3127764565",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4390041933",
    "https://openalex.org/W4409235391",
    "https://openalex.org/W4403160758",
    "https://openalex.org/W4401042829",
    "https://openalex.org/W4396510754",
    "https://openalex.org/W4404764563",
    "https://openalex.org/W4400145417",
    "https://openalex.org/W4401507156",
    "https://openalex.org/W4403420208",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4393153069",
    "https://openalex.org/W4403229099",
    "https://openalex.org/W4389519254",
    "https://openalex.org/W600225349",
    "https://openalex.org/W4387225871",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W2604433698",
    "https://openalex.org/W4404709263"
  ],
  "abstract": "Large language models (LLMs) and large visual-language models (LVLMs) have exhibited near-human levels of knowledge, image comprehension, and reasoning abilities, and their performance has undergone evaluation in some healthcare domains. However, a systematic evaluation of their capabilities in cervical cytology screening has yet to be conducted. Here, we constructed CCBench, a benchmark dataset dedicated to the evaluation of LLMs and LVLMs in cervical cytology screening, and developed a GPT-based semi-automatic evaluation pipeline to assess the performance of six LLMs (GPT-4, Bard, Claude-2.0, LLaMa-2, Qwen-Max, and ERNIE-Bot-4.0) and five LVLMs (GPT-4V, Gemini, LLaVA, Qwen-VL, and ViLT) on this dataset. CCBench comprises 773 question-answer (QA) pairs and 420 visual-question-answer (VQA) triplets, making it the first dataset in cervical cytology to include both QA and VQA data. We found that LLMs and LVLMs demonstrate promising accuracy and specialization in cervical cytology screening. GPT-4 achieved the best performance on the QA dataset, with an accuracy of 70.5% for close-ended questions and average expert evaluation score of 6.9/10 for open-ended questions. On the VQA dataset, Gemini achieved the highest accuracy for close-ended questions at 67.8%, while GPT-4V attained the highest expert evaluation score of 6.1/10 for open-ended questions. Besides, LLMs and LVLMs revealed varying abilities in answering questions across different topics and difficulty levels. However, their performance remains inferior to the expertise exhibited by cytopathology professionals, and the risk of generating misinformation could lead to potential harm. Therefore, substantial improvements are required before these models can be reliably deployed in clinical practice.",
  "full_text": "npj |precision oncology Article\nPublished in partnership with The Hormel Institute, University of Minnesota\nhttps://doi.org/10.1038/s41698-025-00916-7\nEvaluating the performance of large\nlanguage & visual-language models in\ncervical cytology screening\nCheck for updates\nQi Hong1,6,S h i j i eL i u2,6,L i y i n gW u3,6,Q i q iL u1, Pinglan Yang3, Dingyu Chen3,G o n gR a o2, Xinyi Liu1,H u aY e1,\nPeiqi Zhuang1, Wenxiu Yang1, Shaoqun Zeng2, Qianjin Feng1,4,5, Xiuli Liu2 ,J i n gC a i3 &\nShenghua Cheng1,4,5\nLarge language models (LLMs) and large visual-language models (LVLMs) have exhibited near-human\nlevels of knowledge, image comprehension, and reasoning abilities, and their performance has\nundergone evaluation in some healthcare domains. However, a systematic evaluation of their capabilities\nin cervical cytology screening has yet to be conducted. Here, we constructed CCBench, a benchmark\ndataset dedicated to the evaluation of LLMs and LVLMs in cervical cytology screening, and developed a\nGPT-based semi-automatic evaluation pipeline to assess the performance of six LLMs (GPT-4, Bard,\nClaude-2.0, LLaMa-2, Qwen-Max, and ERNIE-Bot-4.0) andﬁve LVLMs (GPT-4V, Gemini, LLaVA, Qwen-\nVL, and ViLT) on this dataset. CCBench comprises 773 question-answer (QA) pairs and 420 visual-\nquestion-answer (VQA) triplets, making it theﬁrst dataset in cervical cytology to include both QA and VQA\ndata. We found that LLMs and LVLMs demonstrate promising accuracy and specialization in cervical\ncytology screening. GPT-4 achieved the best performance on the QA dataset, with an accuracy of 70.5%\nfor close-ended questions and average expert evaluation score of 6.9/10 for open-ended questions. On\nthe VQA dataset, Gemini achieved the highest accuracy for close-ended questions at 67.8%, while GPT-\n4V attained the highest expert evaluation score of 6.1/10 for open-ended questions. Besides, LLMs and\nLVLMs revealed varying abilities in answering questions across different topics and difﬁculty levels.\nHowever, their performance remains inferior to the expertise exhibited by cytopathology professionals,\nand the risk of generating misinformation could lead to potential harm. Therefore, substantial\nimprovements are required before these models can be reliably deployed in clinical practice.\nCervical cancer is one of the most commonly diagnosed cancers and a\nleading cause of cancer death in women. In 2022, there were 661,021 women\ndiagnosed with cervical cancer and 348,189 died of the disease in the world\n1.\nCervical cancer screening facilitates early diagnosis and timely intervention\nand treatment, thereby reducing the incidence and mortality of cervical\ncancer\n2.\nManual screening requires doctorsto identify a small number of lesion\ncells among tens of thousands under a microscope, which is labor-intensive\nand experience-dependent3.A r t iﬁcial intelligence (AI)-assisted screening\nsystems3–12 can signiﬁcantly improve screening efﬁciency and reduce doc-\ntors’ workload. However, they are typically task-speciﬁc, such as lesion\ndetection and image classiﬁcation, and lack the capabilities to make cyto-\npathological interpretation and reasoning like a cytopathologist. Recently\nemerging large language models (LLMs) and large visual-language models\n(LVLMs), such as GPT-413,B a r d14,G e m i n i15,a n dC l a u d e16, have exhibited\nnear-human or even human-level performance in medical-related tasks17–21\n1School of Biomedical Engineering, Southern Medical University, Guangzhou, 510515, China.2Britton Chance Center and MoE Key Laboratory for Biomedical\nPhotonics, Wuhan National Laboratory for Optoelectronics-Huazhong University of Science and Technology, Wuhan, China.3Department of Obstetrics and\nGynecology, Union Hospital, Tongji Medical College, Huazhong University of Science and Technology, Wuhan, China.4Guangdong Provincial Key Laboratory of\nMedical Image Processing, Southern Medical University, Guangzhou, 510515, China.5Guangdong Province Engineering Laboratory for Medical Imaging and\nDiagnostic Technology, Southern Medical University, Guangzhou, 510515, China.6These authors contributed\nequally: Qi Hong, Shijie Liu, Liying Wu.e-mail: xlliu@mail.hust.edu.cn; jingcai@hust.edu.cn;\nchengsh2023@smu.edu.cn\nnpj Precision Oncology|           (2025) 9:153 1\n1234567890():,;\n1234567890():,;\nand are promising candidates for the application in cervical cytology\nscreening.\nThe application of general-purpose LLMs and LVLMs in the medical\nﬁeld faces challenges related to accuracy22, bias, ethical concerns, and\nknowledge hallucination23,t o x i c i t y24. To ensure the reliability of LLMs and\nLVLMs for speciﬁc clinical tasks, it is necessary to systematically evaluate\ntheir performance using standardized datasets. Recent works on evaluating\nLLMs in medical related tasks indicate the supportive role of LLM in medical\neducation\n25, research26, and clinical practice27. However, in cervical cytology\nscreening, there is a lack of a standardized benchmark dataset and sys-\ntematic evaluation of the performance of different LLMs and LVLMs.\nIn this study, we constructed a benchmark dataset, termed CCBench,\ndesigned for LLMs and LVLMs evaluation in cervical cancer screening, and\ndeveloped a GPT-based semi-automatic evaluation pipeline. CCBench\nconsisted of 773 question-answer (QA) pairs and 420 visual-question-\nanswer (VQA) triplets, which were constructed by utilizing the textual and\nimage-text paired contents extracted from the widely recognized the\nBethesda system (TBS) textbook. Theperformance of LLMs and LVLMs on\nCCBench was assessed using multifaceted metrics, such as accuracy, GPT-4\nbased G-Eval score\n28, and expert evaluation score. We found that LLMs and\nLVLMs demonstrated promising accuracy and specialization in cervical\ncytology screening. On the QA dataset, the average accuracy across all\nmodels was 62.9% for close-ended questions, and the average expert eva-\nluation score was 6.3/10 for open-ended questions. Notably, GPT-4\nachieved the best performance, with an accuracy of 70.5% for close-ended\nquestions and average expert evaluation score of 6.9/10 for open-ended\nquestions. On the VQA dataset, the average accuracy across all models was\n55.0% for close-ended questions, and the average expert evaluation score\nwas 5.15/10 for open-ended questions.Gemini achieved the highest accu-\nracy of 67.8% for close-ended questions, while GPT-4V attained the highest\naverage expert evaluation score of 6.1/10 for open-ended questions. Besides,\nthe abilities of LLMs and LVLMs exhibited signiﬁcant variation on ques-\ntions across different topics and difﬁculty levels. However, there remains a\ndisparity between their responses and those of cytopathologists. The risk of\ngenerating misinformation could lead to potential harm, emphasizing the\nneed for further reﬁnement of these models to ensure their reliability before\ndeployment in clinical practice. We have publicly released our dataset and\ncodebase to promote future research and contribute to the development of\nthe open-source community.\nResults\nBenchmark dataset CCBench\nDue to the lack of datasets for evaluating the performance of LLM and\nLVLM in cervical cytology screening, weﬁrst constructed CCBench, a\nbenchmark dataset sourcing from the TBS textbook29.T h r o u g haG P T - 4\nbased semi-automatic pipeline (Fig.1a, Construction of QA and VQA\ndatasets in“Methods”section, and Supplementary Note 1), we extracted 128\nimage-text pairs and 424 textual knowledge points from the textbook and its\nonline atlas (https://bethesda.soc.wisc.edu), and created a QA sub-dataset\n(773 QA pairs) and a VQA sub-dataset (420 image-question-answer tri-\nplets) (Fig.1b). The QA dataset encompasses knowledge points from the\nchapters of endometrial cells (7.50%), atypical squamous cells (12.16%),\nsquamous epithelial cell abnormalities (41.01%), and glandular epithelial\ncell abnormalities (39.33%) (Fig.1d). The VQA dataset covers the chapters\nof non-neoplasticﬁndings (31.67%), atypical squamous cells (20.71%),\nsquamous epithelial cell abnormalities (22.86%) and glandular epithelial cell\nabnormalities (15.95%), and other malignant neoplasms (8.81%). The\nknowledge points in each chapter are from sections of criteria, deﬁnition,\nexplanatory notes, problematic patterns, and tables (Fig.1d), which provide\nguidelines and protocols for collecting, preparing, staining, and evaluating\ncervical cell samples, terms for uniform terminology and classiﬁcation,\nadditional explanations about interpreting and applying the criteria, com-\nplex cases encountered in clinical practice, and standardized forms and\ntemplates for recording and reporting cervical cytopathologicalﬁndings,\nrespectively. We evaluated the performance of LLMs and LVLMs using\nclose-ended and open-ended questions (Fig.1b), which were designed to\nsimulate real-world clinical scenarios in cervical pathology screening. Figure\n1c, e, f, and g showed the characteristics of the questions and answers in the\nQA and VQA dataset, including theﬁrst three words of questions (Fig.1c),\ndistribution of question length (Fig.1e) and answer length (Fig.1f), and the\ntop 50 most frequent medical terms in the questions and answers (Fig.1g).\nEvaluation pipeline\nWe developed a semi-automated evaluation pipeline (Fig.2) to access the\nperformance of LLMs and LVLMs and utilized prompt engineering, con-\nversation isolation, and result post-processing to enhance the reliability of\nthe evaluation process. Firstly, prompts and questions were simultaneously\nfed into models to generate answers. The prompts served to instruct models\nin generating speciﬁc answers and directly impacted the quality of answers.\nThus, we needed to provide the modelswith relevant knowledge in advance\nto ensure that models accurately comprehended the context of the questions\nand responded using specialized terminology. We elaborated system\nprompts for each dataset, ensuring that all models receive consistent con-\ntext, prior knowledge, behavioral instructions, and answer format (Sup-\nplementary Note 2). Inspired by the chain of thought framework\n30,i nt h e s e\nsystem prompts, we required the models to generate answers step by step\nand provide reasons for their answers, and created different response\ntemplates and question-answer examples for both close and open-ended\nquestions (Supplementary Note 2d). Besides, Models were deployed\nthrough application programming interface (API) calls to ensure efﬁciency\nand prevent interference between dialogues and data leakage during ques-\ntioning (i.e., conversation isolation). Then, we employed manual review to\nassess and handle unformatted answers (Supplementary Table 1 and\nUnformatted answers processing in“Methods” section). Finally, we used\nstatistical metrics, LLM-based G-Eval score\n28, and expert evaluation to\nassess the answers (Evaluation metrics in“Methods” section).\nPerformance on QA dataset\nWe assessed the performance ofﬁve commercial closed-source LLMs\n(GPT-4, Bard, Claude-2.031,Q w e n - M a x32, and ERNIE-Bot-4.033)a n do n e\nopen-source LLM (LLaMa-2) on the QAdataset. We employed accuracy,\nprecision, recall, F1 score, and speciﬁcity to evaluate the answers of different\nLLMs on the close-ended questions in the QA dataset (Fig.3a, b). GPT-4\ndemonstrated the highest accuracy (0.705) (Fig.3a), and Qwen-Max\nobtained the highest precision (0.764), recall (0.866), and F1 score (0.812)\n(Fig. 3b). While Bard and LLaMa-2 demonstrated outstanding speciﬁcity,\ntheir low recall values highlighted a signiﬁcant risk of false negatives,\nunderscoring potential limitations in their clinical reliability (Fig.3b).\nFor the open-ended questions in the QA dataset, we utilized the expert\nevaluation scores and G-Eval scores to assess the quality of the answers of\nLLMs. All three experts agreed that GPT-4 achieved superior performance,\nearning the highest average expert evaluation score of 6.9/10 (Fig.4a). In\nterms of the number of times receivingthe highest ratings from experts,\nwhether from a single expert (Fig.4b) or multiple experts simultaneously\n(Fig.4c), GPT-4 received the most recognition, followed by Claude-2.0. The\nconsistency matrix of the three experts illustrated the degree of agreement\na m o n gt h e mi nr a n k i n gt h em o d e l s( F i g .4d). The G-Eval scores of different\nLLMs were consistent with those from expert evaluation, with GPT-4\nachieving the second highest G-Eval score (Fig.4e). The example answers\nfrom different LLMs to the same questions revealed their varying abilities to\ncomprehend and response accurately (Fig.4f, g). The answers from GPT-4\nand Claude-2.0 were more accurate and consistent with the ground truth\n(Fig. 4g).\nFigure 5a shows the performance of different LLMs on the QA\ndataset across different chapters. For the close-ended questions, LLMs\nexhibited varying levels of accuracy across questions on different topics\n(i.e., chapters). GPT-4, Claude-2.0, and Qwen-Max demonstrated high\naccuracy on the questions about glandular epithelial cell abnormalities\n(Ch.6), while all the models obtained relatively low accuracy on the\nquestions about endometrial cell (Ch. 3). For the open-ended questions,\nhttps://doi.org/10.1038/s41698-025-00916-7 Article\nnpj Precision Oncology|           (2025) 9:153 2\nGPT-4, Claude-2.0, and Qwen-Max showed superior capabilities in\nresolving questions about atypical squamous cells (Ch. 4), achieving\nconsistently high scores of G-Eval and expert evaluation. We retro-\nspectively divided the questions into three difﬁculty levels based on the\nanswers of LLMs (Fig.5c). For the close-ended questions, questions with\nfewer than two LLMs answering incorrectly were classiﬁed as “easy”,\nthose with two to four LLMs answering incorrectly as“normal”,a n d\nthose with more than four LLMs answering incorrectly as“hard”. For the\nopen-ended questions, questions were sorted according to their average\nexpert evaluation scores across all models, and the top 25% were\nFig. 1 | Construction pipeline of CCBench and its characteristics. aThe GPT-4\nbased semi-automatic pipeline for dataset construction. Textual knowledge points\nand image-text pairs were extracted from the TBS textbook and its online atlas. GPT-\n4 was then employed to generate close/open-ended QA pairs and VQA triplets using\nthese data, followed by a manual review to ensure their quality.b QA pair (middle)\nand VQA triplets (right) examples generated from the TBS textbook (left).c The\ndistribution of theﬁrst three words of open-ended questions in the CCBench, with\nthe order of words radiating outward from the center.d The proportion of knowl-\nedge points from different chapters and sections of the textbook.e, f Distribution of\nquestion length (e) and answer length (f) in QA and VQA datasets.g The top 50 most\nfrequent medical terms in the questions and answers of CCBench.\nhttps://doi.org/10.1038/s41698-025-00916-7 Article\nnpj Precision Oncology|           (2025) 9:153 3\nclassiﬁed as “easy”, the bottom 25% as “hard”, and the remaining\nquestions as“normal”. For the close-ended QA task, GPT-4 obtained the\nhighest accuracy (0.682) on the normal questions, while Qwen-Max\nretained the highest accuracy (0.954) on the easy questions. However, all\nthe LLMs demonstrated low accuracies when addressing the hard\nquestions (Fig. 5c). For the open-ended QA task, GPT-4 obtained an\noverall higher G-Eval score and expert evaluation score across three\ndifﬁculty levels (Fig.5c). As the difﬁculty level of question increased, the\nperformance of all the LLMs decreased (Fig.5c).\nPerformance on VQA dataset\nWe assessed the performance of three commercial closed-source LVLMs\n(Gemini, GPT-4V, and Qwen-VL) and two open-source LVLMs (ViLT and\nLLaVa) on the VQA dataset. We employed accuracy, precision, recall, F1\nFig. 3 | Performance of different LLMs and LVLMs on the close-ended questions in QA and VQA dataset. a, c Accuracy of different LLMs (a) and LVLMs (c). b, d\nPrecision, recall, F1 score, and speciﬁcity of different LLMs (b) and LVLMs (d).\nHow are the \nnuclei generally \nin LSIL?\nSystem-prompt\nWhat is the \ncharacteristic of the \nHSIL in this image?\nSystem-prompt\nLLM\nLVLM\nRead \nDataset Input EvaluationAssemble ResultFormat\nAnswer\nCollect\nAswer\nRequest for \nAnswer\nPost-processing\nAnswers\nPost-processing\nAnswersParseing\nAnswers\nExpert \nEvaluation\nQuestionImage\nExpert \nEvaluation\nAnswer \n(ground thruth)\nAnswer (model)\nStatistical \nmetrics\nG-Eval\nScore\nFig. 2 | Semi-automated pipeline for LLMs and LVLMs evaluation.Fed questions and system prompts into each model, collect and format answers, andﬁnally use\nstatistical metrics, LLM-based metrics, and expert evaluation to assess the answers.\nhttps://doi.org/10.1038/s41698-025-00916-7 Article\nnpj Precision Oncology|           (2025) 9:153 4\nscore, and speciﬁcity to evaluate the answers of different LVLMs on the\nclose-ended questions in the VQA dataset (Fig.3c, d). Gemini achieved the\nhighest accuracy (0.678) (Fig.3c) and precision (0.736), and the second\nhighest recall (0.811) and F1 score (0.772) (Fig.3d), demonstrating its\nsuperior overall effectiveness in accurate judgments. Although Qwen-VL\nstood out with the highest recall (0.994) and F1 score (0.788) (Fig.3d), it\nobtained signiﬁcantly lower speciﬁcity (0.074) (Fig.3d), demonstrating its\nhigh risk of producing false positives.\nFor the open-ended questions in the VQA dataset, all three experts\nagreed that GPT-4V achieved signiﬁcantly superior performance than the\nothers (Fig.6a). In terms of the number of times receiving the highest ratings\nfrom experts, whether from a single expert (Fig.6b) or multiple experts\nsimultaneously (Fig. 6c), GPT-4V received the most recognition. The\nconsistency matrix of the three experts illustrated the degree of agreement\namong them in ranking the models (Fig.6d ) .A d d i t i o n a l l y ,G P T - 4 V\nobtained higher G-Eval scores (Fig.6e). Although GPT-4V and Gemini\noccasionally produced incorrect results, they can identify certain cell sub-\ntypes in a complex smear; in contrast, LLaVa may refuse to answer, and\nQwen-VL may only provide descriptions of cytological features (Fig.6f).\nGPT-4V can accurately describe smear characteristics, while other LVLMs\nexhibited some hallucinations\n34, introducing fabricated information that\ncould potentially mislead clinical decision-making (Fig.6g).\nFigure 5b shows the performance of different LVLMs on the VQA\ndataset across different chapters. For the close-ended questions, Qwen-VL\nobtained the highest accuracy on questions about squamous epithelial cell\nabnormalities (Ch. 5), glandular epithelial cell abnormalities (Ch. 6), and\nother malignant neoplasms (Ch. 7),but the lowest at non-neoplasticﬁnd-\nings (Ch. 2) and atypical squamouscells (Ch. 4), which may result in the\noverall low accuracy (Fig.3c). Gemini demonstrated relatively high accuracy\nacross all topics. For the open-ended questions, GPT-4V obtained the\nhighest expert evaluation scores on questions from all chapters, especially\nthe questions about atypical squamous cells (Ch. 4), squamous epithelial cell\nabnormalities (Ch. 5), and other malignant neoplasms (Ch. 7). However,\nthere were deviations between the results of G-Eval and expert evaluation,\nwhich may be limited to the ability of G-Eval framework. Similarly, the\nquestions in the VQA dataset were divided into three difﬁculty levels based\non the answers of LVLMs (Fig.5d). For the close-ended questions, questions\nwith fewer than one LVLM answering incorrectly were classiﬁed as“easy”,\nthose with two to three LVLMs answering incorrectly were deemed“nor-\nmal”, and questions with more than three LVLMs providing incorrect\nFig. 4 | The performance of different LLMs on the open-ended questions in QA\ndataset. aThe average expert evaluation scores of different LLMs from three experts.\nb The number of times that different expert recognizes each LLM performs better\nthan all other models.c The number of times that each LLM is recognized as the best\nby 1, 2, or all 3 experts simultaneously.d The average Spearman’s rank correlation of\nevaluation among experts.e The distribution of G-Eval scores of different LLM. The\ndata are shown as boxplots and whiskers (min to max) with all data points, where the\nupper and lower hinges represent the 25th and 75th percentiles, and the center is the\naverage score.f, gTwo representative questions and answers from different LLMs, as\nwell as their corresponding G-Eval scores and expert evaluation scores. Owing to\nspace constraints, the reasoning part of the answer was not shown.\nhttps://doi.org/10.1038/s41698-025-00916-7 Article\nnpj Precision Oncology|           (2025) 9:153 5\nanswers were categorized as “hard”. The open-ended questions were\ndesignated as“easy” or “hard” if they fell within the top 30% or bottom 30%\nof the average expert evaluation scoresacross all models, and the remaining\nquestions as“normal”.A st h ed i fﬁculty level of question increased, the\nperformance of all the LVLMs decreased (Fig.5d). For the close-ended\nquestions, Gemini obtained the highest accuracy on easy and normal\nquestions (Fig.5d). However, all the LVLMs obtained low accuracies (below\n0.5) on hard questions (Fig.5d). For the open-ended questions, GPT-4V\nconsistently obtained higher expert evaluation scores across all three difﬁ-\nculty levels (Fig.5d).\nError pattern analysis in LVLMs\nTo better understand the reasoning processes of different models in cer-\nvical cytology interpretation, we analyzed error patterns in their responses\nacross VQA datasets. Our analysis revealed several common error pat-\nterns shared across different models, providing insights into their lim-\nitations when handling specialized medical tasks (Fig.7). We identiﬁed\ntwo main categories of errors: knowledge-based errors and hallucination-\nbased errors. Knowledge-based errors included misclassiﬁcation/feature\nidentiﬁcation errors and answer matching failures, which manifested\nwhen models generated responses that were grammatically correct and\npotentially factually accurate in isolation but failed to directly address the\nquestion posed. For example, both GPT-4V and Gemini produced well-\nformatted responses that appeared professional but did not identify the\nkey cytomorphological features described in the reference answer (Fig.\n7a). They missed critical diagnostic elements such as glandular archi-\ntecture, irregular chromatin distribution, and prominent macronucleoli,\nwhich are essential features for identifying endocervical adenocarcinoma.\nMisclassiﬁcation and feature identiﬁcation errors also occurred when\nmodels failed to correctly identify or classify cytomorphological features\ncritical for diagnosis.\nAmong the hallucination-based errors, logical reasoning failures\nwere particularly concerning. Although both GPT-4V and Gemini cor-\nrectly identiﬁed some cellular features like nuclear enlargement and\nhyperchromasia, they made unsupported diagnostic leaps to wrong\nsubclass such as high-grade squamous intraepithelial lesion (HSIL) or\nsquamous cell carcinoma (SCC) (Fig.7b). This tendency to reach pre-\nmature conclusions without sufﬁcient evidence represents a critical lim-\nitation in medical diagnosis contexts. In addition, instruction drifting was\nobserved when models failed to adhere to the speciﬁc requirements of the\nprompt; for instance, when asked to identify the components of a three-\ndimensional cluster, multiple models instead attempted to provide a\ndiagnosis, completely diverging from the instructional intent (Fig.7b).\nFluent nonsense generation was notably observed in LLaVA’s responses\nacross both examples. LLaVA made illogical inferences about cell\nabnormality based solely on color attributes— stating that cells were\n“predominantly blue and purple in color, indicating that they are likely\nabnormal or atypical”— a conclusion with no cytopathological validity.\nSimilarly (Fig. 7a), LLaVA produced text that, while grammatically\ncoherent, offered no meaningful description of the cellular morphology\nshown in the image (Fig.7b). The prevalence of these error patterns across\ndifferent models suggests common limitations possibly stemming from\nsimilar training approaches or knowledge gaps, and the parallel reasoning\nfailures observed indicate fundamental challenges in medical reasoning\nthat require targeted improvements beyond general model capabilities.\nThese ﬁndings underscore the need for careful assessment and potentially\ndomain-speciﬁcr eﬁnement before deploying such models in clinical\ncytopathology settings.\nFig. 5 | Performance of different LLMs and LVLMs across different chapters and\ndifﬁculty levels. a, b The performance of different models on QA (a) and VQA (b)\ndatasets across different chapters.c, d The performance of different models on QA\n(c) and VQA (d) datasets across three difﬁculty levels:“easy”, “normal”, and“hard”.\nThe titles of the chapters are as follows: non-neoplasticﬁndings (Ch. 2), endometrial\ncell (Ch. 3), atypical squamous cells (Ch. 4), squamous epithelial cell abnormalities\n(Ch. 5), glandular epithelial cell abnormalities (Ch. 6), and other malignant neo-\nplasms (Ch. 7).\nhttps://doi.org/10.1038/s41698-025-00916-7 Article\nnpj Precision Oncology|           (2025) 9:153 6\nDiscussion\nIt is of great concern to evaluate the performance of LLMs and LVLMs on\nspeciﬁc clinical tasks before their deployment in clinical practice. This study\nconstructeda dedicated benchmark dataset CCBench (Fig.1) and developed\na GPT-based semi-automated evaluation pipeline (Fig.2) to assess the\nperformance of popular LLMs and LVLMs in cervical cytology screening.\nAcross the QA tasks, GPT-4 demonstrated better overall performance than\nthe other LLMs, with higher accuracyfor close-endedquestions (Fig.3a)\nand more recognition from the experts for open-ended questions (Fig.4). In\nthe more challenging VQA tasks, Gemini exhibited better overall perfor-\nmance for close-ended questions (Fig.3c, d), while GPT-4V obtained sig-\nniﬁcantly more recognition from the experts for the open-ended questions\n(Fig.6). Our study highlighted the advanced text comprehension capability\nof GPT-4 and the superior cytopathological image understanding and\nreasoning abilities of both Gemini and GPT-4V, suggesting they may serve\nas promising alternatives for the cervical cytology screening application.\nMoreover, the benchmark dataset CCBench and the well-designed eva-\nluation pipeline can serve as valuable tools for assessing LLMs and LVLMs\nbefore their deployment in cervical cytology screening application.\nAlthough the evaluated LLMs and LVLMs (such as GPT-4, GPT-4V,\nand Gemini) have shown promising results in text and image understanding\nand reasoning tasks for cervical cancer screening, there are still defects\nhindering their application in clinical practice. They may generate correct\nanswers mixed with erroneous information (Figs.4g, 6f, g), which could\nnegatively impact medical diagnoses. Besides, they are obviously incom-\npetent in solving relatively difﬁcult questions, and their performance on\nclose-ended tasks is even lower than the random baseline (Fig.5c, d).\nDespite prior knowledge about cervical cancer screening being provided in\nsystem prompts (Supplementary Note 2e), it is still limited and insufﬁcient\ncompared to the knowledge reserve of cytopathologists. Thus, utilizing more\nspecialized data about cervical cancer screening toﬁne-tune the models may\nfurther enhance their performance and reliability.\nFig. 6 | The performance of different LVLMs on the open-ended questions in\nVQA dataset. aThe average expert evaluation scores of different LVLMs from three\nexperts. b The number of times that different expert recognizes each LVLM performs\nbetter than all other models.c The number of times that each LVLM is recognized as\nthe best by 1, 2, or all 3 experts simultaneously.d The average Spearman’s rank\ncorrelation of evaluation among experts.e The distribution of G-Eval scores of\ndifferent LVLM. The data are presented in the form of boxplots and whiskers,\nencompassing the entire range from minimum to maximum values, with the upper\nand lower hinges corresponding to the 25th and 75th percentiles, respectively. The\ndashed line denotes the average score.f, g Two representative questions and answers\nfrom different LVLMs, as well as their corresponding G-Eval scores and expert\nevaluation scores. The reasoning part of the answer was not shown due to space\nconstraints. Due to the context length limitation, ViLT could not process questions\nwith a length acceptable by other models during open-ended question evaluations,\nand therefore, it was not assessed.\nhttps://doi.org/10.1038/s41698-025-00916-7 Article\nnpj Precision Oncology|           (2025) 9:153 7\nWe acknowledge the limitations of this study. The CCBench was\nconstructed from the representative data in the TBS textbook, which\nensured its reliability and standardization. However, in clinical practice, the\nvariations in staining and imaging characteristics\n35 pose additional chal-\nlenges for LVLMs. To further assess the capability of LVLMs in real-world\nclinical scenarios and mitigate potential data contamination concerns, we\nconstructed a private VQA dataset using real-world clinical pathological\nsmear images. The evaluation resultson our private clinical dataset (Sup-\nplementary Note 3, Supplementary Fig. 1) aligned with ourﬁndings on the\nCCBench dataset, conﬁrming the reliability of our evaluation method and\nthe capabilities of the evaluated models. Besides, incorporating a larger\nvolume of high-quality, heterogeneous data from real-world clinical sce-\nnarios, such as data extracted from clinical cases, could further enhance the\nreliability of the evaluation results and help identify potential issues and\nlimitations of LLMs and LVLMs.\nMethods\nConstruction of QA and VQA datasets\nKnowledge points extraction. The TBS standard29 is a widely recog-\nnized gold standard for cytopathological interpretation in cervical\ncytology, widely accepted by cytopathologists worldwide. The TBS\ntextbook establishes a grading system for cervical cytopathology, clearly\ndeﬁnes the interpretation criteria for each grade, provides numerous\ncomplex case examples and expert interpretations of smear images, and\nstandardizes the terminology used in cytological evaluation. Moreover,\nthere is a large number of high-quality image-text paired data in the TBS\ntextbook and its online atlas (https://bethesda.soc.wisc.edu). The online\natlas offers the higher-resolution versions of the images presented in the\ntextbook. Thus, we used the TBS textbook and its online atlas as the data\nsource and extracted key knowledge points and images (the high-\nresolution versions in the online atlas were used), which can serve as a\nbenchmark for assessing whether LLMs or LVLMs possess the capability\nof cytopathological interpretation. We manually extracted sentences\nfrom the background, criteria, deﬁnition, explanatory notes, and pro-\nblematic patterns sections, as well as from tables andﬁgure captions. To\navoid loss of contextual information, we replaced the pronouns in the\nsentences using content to which they refer. Each image was saved in\n‘png’ format, and its corresponding caption and associated text were\nsaved as‘txt’ﬁle. In each sentence, we replaced pronouns, conjunctions,\nand elements that imply connections to surrounding sentences with the\nelements they actually referred to, and removed footnotes and citation\ninformation. Sentences were manually revised to enhance theirﬂuency\nand coherence. For the image-text pairs, we removed pairs unrelated to\npathology-related topics, corrected any mismatches, split grouped ima-\nges into individual images, and removed marks (e.g.,ﬁgure numbers and\narrows) from the images.\nQA pair and VQA triplet generation. We developed a GPT-4-based\nsemi-automated pipeline (Fig. 1a) to convert the extracted texts and\nimages into QA pairs and VQA triplets. For the QA dataset construction,\nthe extracted sentences were commonly long and complex. Thus, uti-\nlizing prompt engineering (Supplementary Note 1), we decomposed and\nsimpliﬁed the sentences to make each sentence focus on a single\nknowledge point. We retained sentences describing cell morphology and\ngrading, supplemented incomplete sentences, and excluded those con-\ntaining the term“follow-up”. Then, we deﬁned rules for GPT-4 to gen-\nerate questions and answers based on these sentences. To ensure the\nvariety of questions, we required the generated questions to include the\nterms “what”, “where”, “when”, “how much”, “how many” or “is”. The\nquestions and answers were mandated to be coherent and prohibited\nfrom using pronouns like “these”. Besides, the system prompts also\nincluded few-shot examples, speciﬁc prompting rules, and context-aware\nprompting standards to help GPT-4 generate compliant QA pairs more\neffectively. Finally, the pipeline used regular expressions to match and\ncollect the correctly formatted QA pairs, their reasoning, and any doubts\nregarding the knowledge points in the input sentences. Theﬁnal QA pairs\nwere determined through manual review and veriﬁcation, ensuring their\nconformity to the rules speciﬁed in the prompts (Supplementary Note 1).\nFor the VQA dataset construction, since the sentences in captions were\ngenerally simpler than those for QA dataset construction, we did not per-\nform sentence simpliﬁcation. We only split a single caption into multiple\nsentences using GPT-4. We formulated additional rules (Supplementary\nFig. 7 | Error pattern analysis across different LVLMs. aExamples of error patterns in identifying endocervical adenocarcinoma characteristics.b Error patterns in\nidentifying three-dimensional cell cluster features.\nhttps://doi.org/10.1038/s41698-025-00916-7 Article\nnpj Precision Oncology|           (2025) 9:153 8\nNote 1b) to exclude descriptions unrelated to cytopathology and sentences\nthat evaluate cell types or smear types, and extracted the subject from the\nc o n t e x tf o rt h es e n t e n c el a c k i n gas u b j e c t .F o l l o w i n gt h es a m ep i p e l i n eu s e d\nin QA dataset construction,ﬁrstly, we generated QA pairs, their underlying\nreasoning, and any doubts regarding the knowledge points in the input\nsentences. Then, the QA pairs and their corresponding images constituted\nthe VQA dataset.\nFor the close-ended questions in both datasets, we used GPT-4 to insert\n“not” into some questions to convert their answers from“yes” to “no,”\naiming for a balanced distribution of“yes” and “no” responses. All the QA\npairs and VQA triplets in QA and VQA datasets were manually reviewed to\nidentify and correct logic, grammar,context, question phrasing, and type\ncategorization errors.\nCompeting models\nWe selected the state-of-the-art commercial closed-source models Claude-\n2.0, GPT-4, Bard, Qwen-Max, and ERNIE-Bot-4.0 for text-based language\ntasks, and GPT-4V, Qwen-VL, and Gemini for multimodal tasks. We chose\nthree popular open-source models: ViLT, LLaMa-2, and LLaVA (the\nmultimodal extension of LLaMa-2). We used the latest and most commonly\nused version of each model at the time of evaluation (Supplementary Table\n2). For the Gemini model, we selected the version accessible via API. We\ndisabled the network access of all internet-enabled model APIs during the\nevaluation. In addition, in order to reﬂect the actual capabilities of each\nmodel and ensure that there were no hyperparameters that affected the\nsmooth progress of the experiment, the hyperparameters of each model\nwere kept at their default settings without any modiﬁcations.\nUnformatted answers processing\nWe instructed the models to answer in a speciﬁcf o r m a tf o re a c ht a s k\n(Supplementary Note 2d) and subsequently performed a validity check on\nthe format of answers (Supplementary Table 1). For close-ended and open-\nended tasks in QA datasets, as well as close-ended tasks in the VQA dataset,\nwe required the answers to be formatted in JSON, consisting of two key-\nvalue pairs:“answer” and “reason”. Due to the context length limitation of\nViLT and the capability constraint ofQ w e n - V L ,w ew e r eu n a b l et oc o l l e c t\nanswers from them in JSON format and only requested unformatted\nanswers. Subsequently, we converted these unformatted answers into the\ncorrect format using GPT-4. For open-ended tasks in the VQA dataset,\nimposing strict formatting requirements could negatively affect the quality\nof the answers due to the complexity of the task. Therefore, all models\nprovide answers in an unformatted form. There were four types of unfor-\nmatted answers: formatting issues, blank responses, unexpected returns,\nand refuse to answer (Supplementary Table 1), which corresponded to cases\nwhere the model did not adhere to theprescribed JSON format, produced\nblank responses for unknown reasons,exhibited incorrect answer format\ndue to a misunderstanding of the question (e.g., answering open-ended\nquestions in a close-ended format or generating irrelevant contents), and\navoided directly answering the question, respectively. We only retested\nanswers with formatting issues and blank responses. Since answers with\nformatting issues were automatically retested by an error correction pro-\ngram within the pipeline, we did not record their occurrence. The remaining\ntwo types of answers reﬂected the comprehension capability of the model,\nand thus, no additional processing was performed.\nEvaluation metrics\nWe used accuracy, speciﬁcity, precision, recall, and F1 score to evaluate the\nperformance of different LLMs and LVLMs on close-ended questions. For\nthe open-ended questions, we conducted extensive expert evaluation to\nassess the quality of answers. Three experienced experts manually scored the\nanswers according to custom criteria, which comprehensively evaluated the\naccuracy, completeness, logic, precision, risk awareness, and conciseness of\nthe answers (Supplementary Tables3 and 4). The maximum expert eva-\nluation score is set to 10 points. Besides, we employed G-Eval score\n28 to\nassess the quality of answers generated by models automatically. G-Eval is a\nframework that uses LLM (we used GPT-4 in this study) to evaluate LLM\noutputs based on custom criteria. Thecustom evaluation criteria of G-Eval\nwere consistent with those used in expert evaluation (Supplementary Tables\n3a n d4 ) .T h em a x i m u mv a l u eo ft h eG-Eval score was set to 1 point. To\navoid the rater bias between different experts, we employed the mean\nSpearman’s rank correlation coefﬁcient\n36 across all the questions to assess\nthe consistency of experts’rankings derived from their evaluation scores.\nInstitutional review board statement\nThe collection of data for the private VQA dataset (Supplementary Note 3)\nwas approved by the Medical Ethics Committee of Medical Ethics Com-\nmittee of Tongji Medical College at Huazhong University of Science and\nTechnology. The collection and analysis were conducted in accordance with\nthe Declaration of Helsinki.\nData availability\nW er e l e a s eC C B e c hd a t a s e ta tG o o g l eD r i v e(https://drive.google.com/drive/\nfolders/1TBIUX74JKIdaU5C3YrTGMO7QJ_t0pVdK) The source data\nunderlying Figs.1c–g,3, 4a–e,5,a n d6a–e, are provided as a Source Data File.\nCode availability\nWe release all code used to produce the main experiments. The code can be\nfound at the GitHub repository:https://github.com/systemoutprintlnhello\nworld/CCBench.\nReceived: 17 January 2025; Accepted: 19 April 2025;\nReferences\n1. Bray, F. et al. Global cancer statistics 2022: GLOBOCAN estimates of\nincidence and mortality worldwide for 36 cancers in 185 countries.\nCA. Cancer J. Clin.74, 229– 263 (2024).\n2. Peto, J., Gilham, C., Fletcher, O. & Matthews, F. E. The cervical cancer\nepidemic that screening has prevented in the UK.Lancet Lond. Engl.\n364, 249– 256 (2004).\n3. Zhu, X., et al. Hybrid AI-assistive diagnostic model permits rapid TBS\nclassiﬁcation of cervical liquid-based thin-layer cell smears.Nat.\nCommun. 12, 3541 (2021).\n4. Cheng, S., et al. Robust whole slide image analysis for cervical cancer\nscreening using deep learning.Nat. Commun.12, 5639 (2021).\n5. Bai, X. et al. Assessment of Efﬁcacy and Accuracy of Cervical\nCytology Screening With Artiﬁcial Intelligence Assistive System.Mod.\nPathol. 37, 100486 (2024).\n6. Wang, J., et al. Artiﬁcial intelligence enables precision diagnosis of\ncervical cytology grades and cervical cancer.Nat. Commun.15, 4369\n(2024).\n7. Jiang, P. et al. A systematic review of deep learning-based cervical\ncytology screening: from cell identiﬁcation to whole slide image\nanalysis. Artif. Intell. Rev.56, 2687– 2758 (2023).\n8. Xue, P., et al. Improving the Accuracy and Efﬁciency of Abnormal\nCervical Squamous Cell Detection With Cytologist-in-the-Loop\nArtiﬁcial Intelligence.Mod. Pathol. J. U.S. Can. Acad. Pathol. Inc.36,\n100186 (2023).\n9. Cao, M. et al. Patch-to-Sample Reasoning for Cervical Cancer Screening\no fW h o l eS l i d eI m a g e .IEEE Trans. Artif. Intell.5, 2779–2789 (2024).\n10. Jiang, H. et al. Deep learning for computational cytology: A survey.\nMed. Image Anal.84, 102691 (2023).\n11. Song, Y. et al. A deep learning based framework for accurate\nsegmentation of cervical cytoplasm and nuclei. in2014 36th Annual\nInternational Conference of the IEEE Engineering in Medicine and\nBiology Society2903– 2906 https://doi.org/10.1109/EMBC.2014.\n6944230 (2014).\n12. Lin, H. et al. Dual-path network with synergistic grouping loss and\nevidence driven risk stratiﬁcation for whole slide cervical image\nanalysis. Med. Image Anal.69, 101955 (2021).\nhttps://doi.org/10.1038/s41698-025-00916-7 Article\nnpj Precision Oncology|           (2025) 9:153 9\n13. OpenAI et al. GPT-4 Technical Report. Preprint athttps://doi.org/10.\n48550/arXiv.2303.08774 (2024).\n14. An important next step on our AI journey.Google https://blog.google/\ntechnology/ai/bard-google-ai-search-updates/ (2023).\n15. Gemini Team et al. Gemini: A Family of Highly Capable Multimodal\nModels. Preprint athttps://doi.org/10.48550/arXiv.2312.11805\n(2024).\n16. Meet Claude.https://www.anthropic.com/claude.\n17. Liu, F. et al. Application of large language models in medicine.Nat.\nRev. Bioeng.3, 103– 115 (2025).\n18. Safavi-Naini, S. A. A. et al. Vision-Language and Large Language\nModel Performance in Gastroenterology: GPT, Claude, Llama, Phi,\nMistral, Gemma, and Quantized Models. Preprint athttps://doi.org/\n10.48550/arXiv.2409.00084 (2024).\n19. Pal, A. & Sankarasubbu, M. Gemini Goes to Med School: Exploring the\nCapabilities of Multimodal Large Language Models on Medical\nChallenge Problems & Hallucinations.Proceedings of the 6th Clinical\nNatural Language Processing Workshophttps://doi.org/10.18653/v1/\n2024.clinicalnlp-1.3 (2024).\n20. Cozzi, A. et al. BI-RADS Category Assignments by GPT-3.5, GPT-4,\nand Google Bard: A Multilanguage Study.Radiology 311, e232133\n(2024).\n21. Luo, X. et al. Large language models surpass human experts in\npredicting neuroscience results.Nat. Hum. Behav.1 – 11 https://doi.\norg/10.1038/s41562-024-02046-9 (2024).\n22. Xu, T. et al. Current Status of ChatGPT Use in Medical Education:\nPotentials, Challenges, and Strategies.J. Med. Internet Res.26,\ne57896 (2024).\n23. Du, X. et al. Generative Large Language Models in Electronic Health\nRecords for Patient Care Since 2023: A Systematic Review.https://\ndoi.org/10.1101/2024.08.11.24311828 (2024).\n24. Bedi, S. et al. Testing and Evaluation of Health Care Applications of\nLarge Language Models: A Systematic Review.JAMA. https://doi.\norg/10.1001/jama.2024.21700 (2024).\n25. Kung, T. H. et al. Performance of ChatGPT on USMLE: Potential for AI-\nassisted medical education using large language models.PLOS Digit.\nHealth 2,1 – 12 (2023).\n26. Hou, W. & Ji, Z. Assessing GPT-4 for cell type annotation in single-cell\nRNA-seq analysis.Nat. Methods21, 1462– 1465 (2024).\n27. Williams, C. Y. K., Miao, B. Y., Kornblith, A. E. & Butte, A. J. Evaluating\nthe use of large language models to provide clinical recommendations\nin the Emergency Department.Nat. Commun.15, 8236 (2024).\n28. Liu, Y. et al. G-Eval: NLG Evaluation using GPT-4 with Better Human\nAlignment. Preprint athttp://arxiv.org/abs/2303.16634 (2023).\n29. The Bethesda System for Reporting Cervical Cytology: Deﬁnitions,\nCriteria, and Explanatory Notes. (Springer International Publishing,\nCham, 2015).https://doi.org/10.1007/978-3-319-11074-5.\n30. Chain-of-Thought Prompting Elicits Reasoning in Large Language\nModels. https://papers.nips.cc/paper_ﬁles/paper/2022/hash/\n9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html.\n31. Claude 2.https://www.anthropic.com/news/claude-2.\n32. Bai, J. et al. Qwen Technical Report. Preprint athttps://doi.org/10.\n48550/arXiv.2309.16609 (2023).\n33. ERNIE-4.0-Turbo-8K - ModelBuilder.https://cloud.baidu.com/doc/\nWENXINWORKSHOP/s/7lxwwtafj.\n34. Bender, E. M., Gebru, T., McMillan-Major, A. & Shmitchell, S. On the\nDangers of Stochastic Parrots: Can Language Models Be Too Big? in\nProceedings of the 2021 ACM Conference on Fairness,\nAccountability, and Transparency610– 623 (ACM, Virtual Event\nCanada, 2021).https://doi.org/10.1145/3442188.3445922.\n35. Bethesda Interobserver Reproducibility Study-2 (BIRST-2) Bethesda\nSystem 2014.J. Am. Soc. Cytopathol.6, 131– 144 (2017).\n36. Rotello, C. M., Myers, J. L., Well, A. D. & Jr, R. F. L. inResearch Design\nand Statistical Analysis(Taylor & Francis, 2025).\nAcknowledgements\nThis work is supported by the National Natural Science Foundation of China\n(grant 62471212, 62375100, and 62201221), China Postdoctoral Science\nFoundation (grant 2021M701320, 2022T150237), and Science and\nTechnology Projects in Guangzhou (grant 2024A04J4960).\nAuthor contributions\nS.C., J. C., and X.L. conceived the project. Q.H., S.L., X.Y.L., H.Y., P.Z., W.Y.,\nS. C., X.L., and J.C. constructed the CCBench dataset. Q.H., S.L., and S. C.\ndeveloped the semi-automatic evaluation pipeline. J.C., L.W., P.Y., D.C.,\nQ.H., and G.R. assessed the performance of the LLM and LVLM models\nincluding expert evaluation score and algorithm evaluation metric. Q.H.,\nQ.L., S.L., S.C., X.L., and J.C. analyzed the evaluation results with con-\nceptual advice from S.Z. and Q.F. Q.H., Q.L., S.C., and S.L. wrote the\nmanuscript with input from all authors. S.C. and J.C. supervised the project.\nAll authors discussed the results and commented on the manuscript.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41698-025-00916-7\n.\nCorrespondenceand requests for materials should be addressed to\nXiuli Liu, Jing Cai or Shenghua Cheng.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution-NonCommercial-NoDerivatives 4.0 International License,\nwhich permits any non-commercial use, sharing, distribution and\nreproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the Creative\nCommons licence, and indicate if you modiﬁed the licensed material. You\ndo not have permission under this licence to share adapted material\nderived from this article or parts of it. The images or other third party\nmaterial in this article are included in the article’s Creative Commons\nlicence, unless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted use,\nyou will need to obtain permission directly from the copyright holder. To\nview a copy of this licence, visithttp://creativecommons.org/licenses/by-\nnc-nd/4.0/\n.\n© The Author(s) 2025\nhttps://doi.org/10.1038/s41698-025-00916-7 Article\nnpj Precision Oncology|           (2025) 9:153 10",
  "topic": "Cytology",
  "concepts": [
    {
      "name": "Cytology",
      "score": 0.6367751359939575
    },
    {
      "name": "Computer science",
      "score": 0.4810633659362793
    },
    {
      "name": "Natural language processing",
      "score": 0.3582300543785095
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3414919972419739
    },
    {
      "name": "Medicine",
      "score": 0.31546077132225037
    },
    {
      "name": "Pathology",
      "score": 0.218414306640625
    }
  ],
  "cited_by": 1
}