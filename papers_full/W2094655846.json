{
    "title": "Syntactic and Semantic Features For Code-Switching Factored Language Models",
    "url": "https://openalex.org/W2094655846",
    "year": 2015,
    "authors": [
        {
            "id": "https://openalex.org/A1997977529",
            "name": "Heike Adel",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2026971652",
            "name": "Ngoc Thang Vu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2068133273",
            "name": "Katrin Kirchhoff",
            "affiliations": [
                "University of Washington"
            ]
        },
        {
            "id": "https://openalex.org/A1461859266",
            "name": "Dominic Telaar",
            "affiliations": [
                "Karlsruhe Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2141485797",
            "name": "Tanja Schultz",
            "affiliations": [
                "Karlsruhe Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A1997977529",
            "name": "Heike Adel",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2026971652",
            "name": "Ngoc Thang Vu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2068133273",
            "name": "Katrin Kirchhoff",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1461859266",
            "name": "Dominic Telaar",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2141485797",
            "name": "Tanja Schultz",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2153433699",
        "https://openalex.org/W6611373715",
        "https://openalex.org/W2171928131",
        "https://openalex.org/W179875071",
        "https://openalex.org/W2135957668",
        "https://openalex.org/W2056250865",
        "https://openalex.org/W2100506586",
        "https://openalex.org/W6720877245",
        "https://openalex.org/W2098355803",
        "https://openalex.org/W121747529",
        "https://openalex.org/W6676357141",
        "https://openalex.org/W171608785",
        "https://openalex.org/W6691537765",
        "https://openalex.org/W6731987800",
        "https://openalex.org/W6680890276",
        "https://openalex.org/W1996430422",
        "https://openalex.org/W2049907881",
        "https://openalex.org/W2061272101",
        "https://openalex.org/W6678914141",
        "https://openalex.org/W1876715006",
        "https://openalex.org/W2013489815",
        "https://openalex.org/W2048512899",
        "https://openalex.org/W6622259991",
        "https://openalex.org/W2003458432",
        "https://openalex.org/W6636649193",
        "https://openalex.org/W2096204319",
        "https://openalex.org/W6636811518",
        "https://openalex.org/W6678277124",
        "https://openalex.org/W6677119991",
        "https://openalex.org/W2110372834",
        "https://openalex.org/W2141599568",
        "https://openalex.org/W2115847145",
        "https://openalex.org/W2292896937",
        "https://openalex.org/W2314734636",
        "https://openalex.org/W1767292398",
        "https://openalex.org/W2014962660",
        "https://openalex.org/W2092286949",
        "https://openalex.org/W332214200",
        "https://openalex.org/W2565276127",
        "https://openalex.org/W1632114991",
        "https://openalex.org/W2571803900",
        "https://openalex.org/W2252095989",
        "https://openalex.org/W2054750161",
        "https://openalex.org/W1631260214",
        "https://openalex.org/W2127218421",
        "https://openalex.org/W2187953100",
        "https://openalex.org/W2121227244",
        "https://openalex.org/W761725120",
        "https://openalex.org/W2474824677",
        "https://openalex.org/W2168708086",
        "https://openalex.org/W4244103815"
    ],
    "abstract": "This paper presents our latest investigations on different features for factored language models for Code-Switching speech and their effect on automatic speech recognition (ASR) performance. We focus on syntactic and semantic features which can be extracted from Code-Switching text data and integrate them into factored language models. Different possible factors, such as words, part-of-speech tags, Brown word clusters, open class words and clusters of open class word embeddings are explored. The experimental results reveal that Brown word clusters, part-of-speech tags and open-class words are the most effective at reducing the perplexity of factored language models on the Mandarin-English Code-Switching corpus SEAME. In ASR experiments, the model containing Brown word clusters and part-of-speech tags and the model also including clusters of open class word embeddings yield the best mixed error rate results. In summary, the best language model can significantly reduce the perplexity on the SEAME evaluation set by up to 10.8% relative and the mixed error rate by up to 3.4% relative.",
    "full_text": "This is the author’s version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication.\nThe ﬁnal version of record is available at http://dx.doi.org/10.1109/TASLP.2015.2389622\nIEEE/ACM TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING 1\nSyntactic and Semantic Features For\nCode-Switching Factored Language Models\nHeike Adel, Ngoc Thang Vu, Katrin Kirchhoff, Dominic Telaar, and Tanja Schultz\nAbstract\nThis paper presents our latest investigations on different features for factored language models for Code-Switching speech\nand their effect on automatic speech recognition (ASR) performance. We focus on syntactic and semantic features which can be\nextracted from Code-Switching text data and integrate them into factored language models. Different possible factors, such as\nwords, part-of-speech tags, Brown word clusters, open class words and clusters of open class word embeddings are explored. The\nexperimental results reveal that Brown word clusters, part-of-speech tags and open-class words are the most effective at reducing the\nperplexity of factored language models on the Mandarin-English Code-Switching corpus SEAME. In ASR experiments, the model\ncontaining Brown word clusters and part-of-speech tags and the model also including clusters of open class word embeddings\nyield the best mixed error rate results. In summary, the best language model can signiﬁcantly reduce the perplexity on the SEAME\nevaluation set by up to 10.8% relative and the mixed error rate by up to 3.4% relative.\nI. I NTRODUCTION\nThe term Code-Switching (CS) denotes speech with more than one language. Speakers switch the language while they are\ntalking. This phenomenon appears in multilingual communities, such as in India, Hong Kong or Singapore. Furthermore, it\nincreasingly occurs in formerly monolingual cultures due to the strong growth of globalization. In many contexts and domains,\nspeakers switch between their native language and English within their utterances. This is a challenge for speech recognition\nsystems, which are typically monolingual. While there have been promising approaches to handle Code-Switching in the ﬁeld\nof acoustic modeling, language modeling (LM) is still a challenge. The main reason is a shortage of training data. While about\n50h of training data might be sufﬁcient for the estimation of acoustic models, the transcriptions of these data are not enough\nto build reliable LMs.\nThe main contribution of this paper is the extensive investigation of syntactic and semantic features for language modeling of\nCS speech. Not only traditional features like POS tags and Brown clusters are used but also low dimensional word embeddings.\nTo easily integrate them into the models, we apply factored language models with generalized backoff. The features are analyzed\nin the context of CS language prediction and automatic speech recognition.\nThe paper is organized as follows: Section II gives a short overview of related works. In Section III, we describe the data\nresources which are used in this research work, present different features and analyze them with respect to Code-Switching\nprediction. Section IV introduces factored language models. In Section V and VI, we summarize our most important experiments\nand results. The study is concluded in Section VII.\nII. R ELATED WORK\nThis section describes previous studies in the ﬁeld of Code-Switching, language modeling for Code-Switching and factored\nlanguage models. Furthermore, a study of obtaining vector representations for words is presented since they will be used to\ncreate additional features in this paper.\nIn [1], [2], [3], it is observed that Code-Switching occurs at positions in an utterance where it does not violate the syntactic\nrules of the languages involved. Code-Switching can be regarded as a speaker dependent phenomenon [4], [5] but particular\nCS patterns can also be shared across speakers [6]. It can be observed that part-of-speech (POS) tags may predict CS points\nmore reliably than words themselves. The authors of [7] predict CS points using several linguistic features, such as word form,\nlanguage ID, POS tags or the position of the word relative to the phrase. The authors of [8] compare four different kinds of\nn-gram language models to predict Code-Switching. They discover that clustering all foreign words into their POS classes\nleads to the best performance. In [9], the authors propose to integrate the equivalence constraint into language modeling for\nMandarin and English CS speech recorded in Hong Kong. In [10], we extended recurrent neural network language models\nfor CS speech by adding features to the input vector and factorizing the output vector into language classes. These models\nreduce the perplexities and mixed error rates when they are applied to rescore n-best lists. In contrast to this previous work,\nwe now focus on feature engineering and use a model which can be more efﬁciently integrated into the ﬁrst decoding pass\nthan a neural network.\nDue to the possibility of integrating various features into factored language models (FLMs), it is possible to handle rich\nmorphology in languages like Arabic [11], [12]. In [13], we report results of initial experiments with FLMs for CS speech\nCopyright (c) 2013 IEEE. Personal use of this material is permitted. However, permission to use this material for any other purposes must be obtained\nfrom the IEEE by sending a request to pubs-permissions@ieee.org.\nCopyright (c) 2015 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\narXiv:1710.01809v1  [cs.CL]  4 Oct 2017\nThis is the author’s version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication.\nThe ﬁnal version of record is available at http://dx.doi.org/10.1109/TASLP.2015.2389622\nIEEE/ACM TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING 2\nand show that they outperform n-gram language models. The best performance is achieved by combining their estimates with\nrecurrent neural network probabilities. In [14], we present syntactic and semantic features for modeling CS language. This\npaper is an extension of that study and includes more explanations and analyses, especially for the vector based open class\nword clusters.\nIn [15], the authors explore the linguistic information in the word representation learned by a recurrent neural network.\nThey discover that the network is able to capture both syntactic and semantic regularities. For example, the relationship of the\nvectors for “man” and “king” is the same as the relationship of the vectors for “woman” and “queen”. In this paper, these\nword representations will be used to derive features for FLMs.\nIII. A NALYSES OF THE DATA CORPUS WITH RESPECT TO POSSIBLE FACTORS\nThis section introduces the corpus used in this work. Furthermore, it presents CS analyses of the text data. Textual features\nare examined which may trigger language changes. They are ranked according to their Code-Switching rate (CS rate). The CS\nrate of each feature f is calculated by its frequency of occurrences preceding CS points divided by its frequency in the entire\ntext:\nCS rate(f) = frequency of f in front of CS points\ntotal frequency of f (1)\nTo provide reliable estimates, only those features are considered whose total frequency exceeds a feature-speciﬁc threshold.\nA. The SEAME Corpus\nThe corpus used in this thesis is called SEAME (South East Asia Mandarin-English). It is a conversational Mandarin-English\nCS speech corpus recorded by [16]. Originally, it was used for the research project “Code-Switch” which was jointly performed\nby Nanyang Technological University (NTU) and Karlsruhe Institute of Technology (KIT) from 2009 until 2012. The corpus\nconsists of 63 hours of audio data and their transcriptions. The audio data were recorded from Singaporean and Malaysian\nspeakers. The recordings consist of spontanously spoken interviews and conversations. For the task of language modeling and\nspeech recognition, the corpus has been divided into three disjoint sets: training, development (dev) and evaluation (eval) set.\nThe data is assigned to the three different sets based on the following criteria: a balanced distribution of gender, speaking\nstyle, ratio of Singaporean and Malaysian speakers, ratio of the four language categories, and the duration in each set. Table I\nlists the statistics of the SEAME corpus.\nTABLE I\nSTATISTICS OF THE SEAME CORPUS\nTraining set Dev set Eval set\n# Speakers 139 8 8\nDuration(hours) 59.2 2.1 1.5\n# Utterances 48,040 1,943 1,029\n# Words 575,641 23,293 11,541\nThe words can be divided into four language categories: English words ( 34.3% of all tokens), Mandarin words ( 58.6%),\nparticles (Singaporean and Malayan discourse particles, 6.8% of all tokens) and others (other languages, 0.4% of all tokens).\nThe language distribution shows that the corpus does not contain a clearly predominant language. In total, the corpus contains\n9,210 unique English and 7,471 unique Mandarin words. The Mandarin character sequences have been segmented into words\nmanually. Furthermore, the number of CS points is quite high: On average, there are 2.6 switches per utterance. Additionally,\nthe duration of the monolingual segments is rather short: More than 82% of the English segments and 73% of the Mandarin\nsegments last less than one second. The average duration of English and Mandarin segments is only 0.67 seconds and 0.81\nseconds, respectively. This corresponds to an average length of monolingual segments of 1.8 words in English and 3.6 words\nin Mandarin.\nB. Trigger Words\nFirst, the words occuring in front of CS points are analyzed. For this, only those words are considered which appear more\nthan 1,000 times in the text, corresponding to more than 0.2% of all word tokens. By regarding the words with highest CS\nrates, we notice that in both languages mainly function words (e.g. “then”, “but”, “in”) appear in front of CS points. Hence in\nthe next step, CS rates of POS tags are examined.\nC. Trigger Part-of-Speech Tags\nDue to the rather small size of the SEAME training text, more general features than words are explored. Since part-of-speech\n(POS) tags show the syntactical role of the words in the sentence, they can be regarded as syntactic features. To be able to\ninvestigate POS tags and their distribution in front of CS points, a tagging process needs to be applied ﬁrst.\nCopyright (c) 2015 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\nThis is the author’s version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication.\nThe ﬁnal version of record is available at http://dx.doi.org/10.1109/TASLP.2015.2389622\nIEEE/ACM TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING 3\n“Matrix language“ = Mandarin\n“Embedded language“ = English\nCS-text\nLanguage islands\n(> 2 embedded\nwords)\nRemaining\ntext\nPOS\ntagger for \nMandarin\nPOS\ntagger for \nEnglish\nOutput Output\nEnglish segments\nin remaining text\nPostprocessing:\nAnalysis\nFig. 1. Part-of-speech tagger for Code-Switching speech\n1) Part-of-speech tagging of Code-Switching speech:For POS tagging of monolingual texts, high quality taggers exist [17].\nHowever, CS speech contains more than one language. Hence, POS tags cannot be determined using a traditional monolingual\ntagger. This work uses the POS tagger for CS speech as described in [18] and illustrated in Figure 1.\nThe matrix language is the main language of an utterance, the embedded language is the second language [19]. In the SEAME\ntranscriptions, Mandarin can be determined as the matrix language. Three or more consecutive words of the embedded language\n(English) are called language islands [19]. All the language islands are passed to the monolingual POS tagger of the embedded\nlanguage. The remaining part is tagged by the monolingual tagger of the matrix language. The idea behind this approach\nis to provide the taggers with as much context as possible. Hence, Mandarin segments with only one or two English words\nare passed to the Mandarin tagger instead of splitting the segments into short monolingual parts. This work uses the Stanford\nlog-linear POS tagger for Chinese and English [17], [20]. The tags are derived from the Penn Treebank POS tag set for Chinese\nand English [21], [22].\nHowever, an analysis shows that most English words which are passed to the Mandarin tagger are incorrectly tagged as nouns\n(instead of as foreign words). Hence, a post-processing step is added to the tagging process to avoid subsequent errors in the\ndetermination of trigger POS tags: All English words which do not belong to language islands are selected and passed to the\nEnglish POS tagger. The resulting tags replace the ones obtained from the Mandarin tagger. In this step, the English tagger\ndoes not get any substantial context (at most one word context) but it is assumed that, nevertheless, its estimates are more\nappropriate than the estimates of the Mandarin tagger.\n2) Part-of-speech tag analysis:In this experiment, we ﬁnd that CS points from Mandarin to English are primarily triggered\nby determiners, while CS from English to Mandarin mainly happens after verbs and nouns. This seems reasonable since it is\npossible that a speaker switches to English for the noun and immediately afterwards back to Mandarin.\nD. Trigger Brown Word Cluster\nThe main disadvantage of the POS tags described in the previous section is the lack of evaluation material. Since no reference\n(i.e., correct tagging) for the CS corpus exists, the correctness of the POS tags derived from the tagging process cannot be\nmeasured. Nevertheless, clustering the words into higher level classes seems to be promising due to the rather small size of\nthe corpus. If, for instance, the word “Monday” occurs only once and the word “Tuesday” occurs once, too, then the class\n“days” occurs at least twice. Hence, probabilities might be better estimated for fewer classes than for many different words.\nTherefore, the unsupervised clustering method by Brown et al. [23] is applied. In contrast to POS tags, Brown clusters (Br) are\nbased on word contributions in a text and are, therefore, probably more robust in the case of Code-Switching. This clustering\nmethod is implemented in the SRILM toolkit [24]. It uses statistical bigram information to assign words to classes. Given\nthe number of classes C, the C most frequent words are assigned to their own classes. Then, successively, the next most\nfrequent word is selected and assigned to a new class. Afterwards, two of the classes are merged. The classes to be merged are\nselected to minimize the average mutual information loss. The average mutual information of two classes c1 and c2 is deﬁned\nas follows [23]:\nI(c1,c2) =\n∑\nc1c2\nP(c1c2)logP(c2|c1)\nP(c2) (2)\nCopyright (c) 2015 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\nThis is the author’s version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication.\nThe ﬁnal version of record is available at http://dx.doi.org/10.1109/TASLP.2015.2389622\nIEEE/ACM TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING 4\nThe sequence c1c2 denotes that class c1 directly preceeds class c2 in the training text.\nThe resulting classes can be viewed as syntactico-semantic features [25].\nThe CS rates observed for Brown word clusters are substantially higher than those of the previous two analyses (ranging up\nto 73%, see Table II), especially for language changes from English to Mandarin. While the rates for a language change from\nMandarin to English are higher than 50% for only two classes, seven classes provide higher rates for a language change in the\nopposite direction. Although the classes have been obtained on the whole training text and, therefore, contain both Mandarin\nand English words, there is only one class which triggers Code-Switching in both directions. It is notable that the classes with\nthe highest CS rates contain more words of the second language than of the one in which they have a trigger function. Hence,\nit can be speculated that if a foreign word of those syntactical classes is used, it more probably triggers a language change\nthan other words.\nDue to the higher CS rates compared to the other trigger features, Brown word clusters have a high potential to provide\nvaluable information for language modeling.\nE. Open Class Words and Word Vector Based Clusters\nWhile POS tags assign words to classes according to their syntactic function, the algorithm by Brown et al. clusters words\nbased on distributional similarities since it uses bigram counts of the training text. In the next step, a different kind of semantic\nfeatures is investigated. Since the SEAME corpus does not contain any semantic tagging, this paper focuses on open class\nwords and clusters of open class words. As described in [15], neural networks are able to learn semantic similarities among\nwords. Since those words and similarities are represented as vectors in a continuous space, they can be clustered using vector\nclustering algorithms, such as k-means or spectral clustering methods. Those methods will be described in the following\nsubsections after exploring the usage of open class words as trigger events.\n1) Trigger open class words: Typically, words can be categorized into closed class words (function words) and open\nclass words (content words). Closed class words specify grammatical relations rather than semantic meaning. Examples are\nconjugations, prepositions and determiners. The class of these words is called “closed” since their number is ﬁnite and typically\nno new words are added to them. On the other hand, open class (oc) words express meaning, such as ideas, concepts or attributes.\nTheir class is called “open” since it can be extended with new words, such as “Bollywood”. It contains, for example, nouns,\nverbs, adjectives and adverbs [26].\nSince open class words carry the meaning of sentences, they can be used to determine the topic of a current utterance. For\nboth languages, English and Mandarin, lists of function words are obtained on the Internet [27], [28] and for each word, the\npreceding open class word is used as a factor. Since the CS text contains about 335k open class words, only those open class\nwords with more than 600 occurences in the text are regarded in the CS rate analysis. This corresponds to about 0.2% of all\nopen class words and is, therefore, comparable to the threshold of the trigger words (see Section III-B).\nCompared to the trigger words, the CS rates of the open class words do not seem to be promising to predict Code-Switching\nfrom Mandarin to English (they are below 35%). It is notable that for all language changes, Mandarin words were the preceding\nopen class words in most of the cases. There are also open class words which appear in front of CS points in both directions.\n2) Trigger open class word clusters:Since the CS rates of open class words are rather low, the implication of clustering\nthem is investigated in the following paragraphs. In this research, k-means [29] and spectral clustering [30] are applied to word\nembeddings extracted from recurrent neural network language models (RNNLMs). In order to create semantic clusters, only\nopen class words are taken into account because only those are considered to contain meaning (see Section III-E1). To increase\nthe number of training examples, two monolingual texts are created: The English text is based on English Gigaword data\n(ﬁfth edition, corpus number: LDC2011T07) and several more corpora (ACL/DCI (LDC93T1), American National Corpus\n(LDC2005T35), ACQUAINT corpus (LDC2002T31)). The Chinese text is based on Chinese Gigaword data (ﬁfth edition,\ncorpus number: LDC2011T13). All the texts mainly contain news articles. In each text, function words are deleted and only\nlines with a high coverage of the SEAME vocabulary are selected. In particular, the resulting texts consist of about 630k\nChinese words and 654k English words. They are divided into a training and a development set (at a ratio of 10 to 1). Based\non these texts, two monolingual RNNLMs are trained using the toolkit provided in [31].\nAn RNNLM consists of three layers (see Figure 2): an input layer, a hidden layer and an output layer. The input of the hidden\nlayer does not only depend on the input layer but also on the hidden layer of the previous time step. This is why the network is\ncalled “recurrent”. The input layer is formed by a vector of the size of the vocabulary. A word in the training text is represented\nby a vector containing “ 1” at the word index position and “ 0” in all the other entries. Similar to the input vector, the output\nvector consists of one entry for each word of the vocabulary. It provides a probability distribution for the next word in the\ntext. For training, backpropagation through time [32], [33] is applied.\nAfter training, embeddings for the words can be found in the weight matrix connecting the input and the hidden layer [15]. For\nthe creation of syntactic and semantic clusters, we extract all the vectors whose corresponding words are part of the SEAME\nvocabulary and cluster them using k-means and spectral clustering.\n3) Open class word clusters analysis:Similar to the CS rates of the open class words (see Section III-E1), most clusters\npreceding a CS point from English to Mandarin are Mandarin clusters. For the opposite direction, however, two English clusters\nCopyright (c) 2015 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\nThis is the author’s version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication.\nThe ﬁnal version of record is available at http://dx.doi.org/10.1109/TASLP.2015.2389622\nIEEE/ACM TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING 5\nx(t)\ns(t)\n y(t)\nU  W\n V\nFig. 2. Illustration of the components of an RNNLM [34]\nare now among the clusters preceding CS points most often. The CS rates of the open class word clusters improve the CS\nrates of the open class words slightly. In total, open class words (clusters) better predict switches from EN to MAN than POS\ntags.\nF . Summary: Comparison of the Trigger Features\nTo sum up, for Code-Switching from English to Mandarin, Brown word clusters provide the highest CS rates. For Code-\nSwitching from Mandarin to English, the best CS rates are obtained with trigger words. This motivates the combination of\ndifferent features into one FLM. Table II provides an overview of the CS rates of the different trigger features.\nTABLE II\nOVERVIEW OF THE CS RATES OF DIFFERENT TRIGGER FEATURES\nFeature CS rate: MAN to EN CS rate: EN to MAN\nWords ≤ 53.43% ≤ 56.25%\nPart-of-speech tags ≤ 43.13% ≤ 47.78%\nBrown word clusters ≤ 52.73% ≤ 72.67%\nOpen class words ≤ 33.33% ≤ 54.53%\nOpen class word clusters ≤ 34.44% ≤ 56.66%\nIn average, the Brown word clusters seem to be the most promising features for the prediction of CS points.\nIV. U SE OF FACTORED LANGUAGE MODELS\nA. Language modeling\nLanguage models calculate the probability of a word sequence W [35]:\nP(W) =P(w1) ·P(w2|w1) ·...·P(wn|w1w2...wn−1)\n=\nn∏\ni=1\nP(wi|w1,w2,...,w i−1) (3)\nN-gram language models limit the context for this computation to the previous n−1 words and the current (next) word.\nP(s) =\nk∏\ni=1\nP(wi|wi−1\ni−n+1) (4)\nThe probabilities are estimated based on counts of words and contexts in a training text. Due to their computational efﬁciency,\nmainly n-gram models are used in the ﬁeld of speech recognition.\nB. Factored language models\nFactored language models (FLMs) consider vectors of features (e.g. words, morphological classes, word stems or clus-\nters) [36], [37]. The following equation expresses that a word wt is regarded as a collection of factors f1\nt ,f2\nt ,...fK\nt :\nwt ≡\n{\nf1\nt ,f2\nt ,...fK\nt\n}\n= f1:K\nt (5)\nSimilar to n-gram language models, the probability for the next word is computed based on counts of factor context occurrences\nin the training text. However instead of a word context wi−n+1...wi−1, a pre-deﬁned factor context is used. This context is one\nof the main design choices when building a factored language model. It could look as follows: f1\nt−2f1\nt−1f2\nt−1. This example\ncontext would lead to the following equation for calculating the probability of word w:\nP(w|context) = Count(f1\nt−2f1\nt−1f2\nt−1w)\nCount(f1\nt−2f1\nt−1f2\nt−1) (6)\nCopyright (c) 2015 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\nThis is the author’s version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication.\nThe ﬁnal version of record is available at http://dx.doi.org/10.1109/TASLP.2015.2389622\nIEEE/ACM TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING 6\nThe advantage of regarding factor contexts instead of word contexts is that usually there are fewer different factors than different\nwords. Hence, the coverage of factor contexts in training texts may be greater than that of equally long word contexts. This is\nespecially important for short training texts. Nevertheless, it is unlikely to see all factor context combinations. In the case of\nunseen contexts, generalized backoff is performed. This means that some of the factors in the context are dropped. For each\nomitted factor, a backoff result is calculated. If there is more than one factor which can be dropped, the backoff results are\ncombined, for instance using their average, their sum, or their product.\nV. FLM S: PERPLEXITY RESULTS\nFor the task of Code-Switching, a variety of FLMs is trained and evaluated with different conditioning factors. The backoff\npaths and smoothing options are chosen for each FLM individually in order to minimize its perplexity on the development set.\nFor each feature combination, an initial set of parameters is obtained using a genetic algorithm [11]. Its results are then improved\nmanually by changing single parameters. According to the analyses in Section III, the following factors are investigated: words,\nPOS tags, Brown word clusters, open class words and open class word clusters. 1\nFor each feature (besides words), an FLM which uses only words and this feature as conditioning factors is built. Furthermore,\nFLMs are created which combine different features.\nThe following subsections describe the perplexity results for the different FLMs.\nA. Factors: part-of-speech tags and language information\nTABLE III\nPPL OF FLM S WITH POS TAGS AND LID\nModel PPL dev PPL eval\nBaseline (3-gram) 268.39 282.86\nPOS 260.70 267.86\nLID 263.24 267.63\nPOS + LID 257.62 264.20\nFirst, the factor POS tag is explored. A language model containing the last word and the two last POS tags as conditioning\nfactors is trained. This choice of conditioning factors is obtained by the genetic algorithm and maintained during the manual\noptimization. Second, a language model using only language identiﬁers (LID) and words as features is trained. Finally, the\nfactors words, POS tags and language information are combined. The perplexity results show that they provide complementary\ninformation which helps to improve the language model predictions. Table III summarizes the results of these experiments.\nB. Factor: Brown word clusters (BrC)\nIn the next experiments, Brown word clusters are explored. As described in Section III-D, the SRILM toolkit [24] is used\nto obtain the clusters. To determine the number of Brown word clusters, FLMs are trained using words and Brown word\nclusters of different sizes as features. Since the clusters should help to improve the CS language models, their perplexity on\nthe SEAME development set is calculated. Class numbers in the range of 50 to 100 lead to the best performance. A class size\nof 70 is chosen for the following experiments.\nBrown word clusters with 70 classes are ﬁrst investigated as the only factor besides words. Then, they are combined with\nthe factors POS tags and LID. Table IV presents the experimental results. The results show that the combination of Brown\nword clusters and POS tags leads to the best word prediction results on the CS text. The additional integration of LID does\nnot improve the results.\nTABLE IV\nPPL OF FLM S WITH BROWN WORD CLUSTERS , POS TAGS AND LID\nModel PPL dev PPL eval\nBaseline (3-gram) 268.39 282.86\nBrC 257.17 265.50\nBrC + POS 249.00 255.34\nBrC + LID 260.39 268.71\nBrC + POS + LID 251.39 259.05\n1Note that the Brown word clusters are bilingual clusters since they are created on the CS training text while the POS tags group the words into monolingual\nclasses. For the open class word clusters, bilingual classes led to better results than monolingual classes. This is further described in Section V-D.\nCopyright (c) 2015 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\nThis is the author’s version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication.\nThe ﬁnal version of record is available at http://dx.doi.org/10.1109/TASLP.2015.2389622\nIEEE/ACM TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING 7\nC. Factor: open class words\nFirst, three different ways of assigning an open class word factor to words are compared: For each word, the previous open\nclass word is determined and added as a factor. At the beginning of each sentence, this factor can be reset to an unknown tag\n(1). This is based on the idea that in each sentence, there might be different topics and open class words and, therefore, a reset\nmight be necessary. This approach will be referred to as “last open class word per sentence”. Another possibility is to keep\nthe previous open class word over sentence boundaries but to reset it at every speaker change (2). This is reasonable since the\nsame speaker may talk about only a limited number of topics and, therefore, use similar open class words. Another speaker,\nhowever, may address a different subject. Although the corpus contains conversations, there is no information about which\nspeakers talk with each other about the same topic. Hence, the open class words used by different speakers may be different,\ntoo. This approach is called “last open class word per speaker” in the following table. The last approach tries to generalize the\nopen class words into topics. For each speaker, the most frequent open class word in a window of the previous n open class\nwords is used as a factor (3). If several open class words have the same frequency, the most recent one is chosen. This method\nwill be referred to as “most frequent open class word in window”. Tables V and VI provide the perplexity results when FLMs\nbased on these different approaches are built. Table V shows the results if only words and open class words are used as factors\nwhile Table VI provides an overview of the results if POS tags and Brown word clusters are also integrated into the models.\nBased on the results of the previous experiments, language information tags are not used as additional factors.\nTABLE V\nPPL OF FLM S WITH WORDS AND OPEN CLASS WORDS\nApproach PPL dev PPL eval\nBaseline (3-gram) 268.39 282.86\n(1) Last open class word per sentence 278.33 279.60\n(2) Last open class word per speaker 278.12 281.31\n(3) Most frequent open class word in window:\nWindow size: unlimited 296.52 299.35\nWindow size: 10 287.19 290.80\nWindow size: 5 284.19 288.95\nTABLE VI\nPPL OF FLM S WITH WORDS , BROWN WORD CLUSTERS , POS TAGS AND OPEN CLASS WORDS\nApproach PPL dev PPL eval\nBaseline (3-gram) 268.39 282.86\n(1) Last open class word per sentence 247.64 251.73\n(2) Last open class word per speaker 247.18 252.37\n(3) Most frequent open class word in window:\nWindow size: unlimited 262.13 263.12\nWindow size: 10 254.31 260.68\nWindow size: 5 252.40 259.25\nThe results show that changing the realization of the open class word feature more often (in the case of smaller window\nsizes) leads to better results. The model “last open class word per speaker” corresponds to the model “most frequent open\nclass word in window” with a window size of 1. It results in the lowest perplexities. Resetting the open class word after each\nsentence leads to only slightly worse results. It is notable that the perplexities are reduced very much by combining Brown\nword clusters, POS tags and open class words. The reason for this could be the backoff. The FLM with words and open class\nwords needs to backoff to (open class) words while the other FLM can also backoff to word clusters. Since there are fewer\nclusters than words in the text, speciﬁc cluster combinations may appear more often than speciﬁc word combinations. For the\nfollowing experiments, the last open class word per speaker is used as the realization of the open class word factor. In order to\nimprove the so-far best language model, the FLMs of the following experiments use the factors words, POS tags and Brown\nword clusters in addition to open class words (clusters).\nD. Factor: open class word clusters\nIn the following experiments, the open class words are grouped using different clustering methods. For this, both the CS\ntraining text and the monolingual texts as described in Section III-E3 are used. If the clusters are based on the monolingual\ntexts, not every word from the CS text is covered by the classes. In this case, the factor is formed by the word itself instead\nof a class. The results are evaluated regarding the perplexities of the FLMs. For all the language models, the same parameters\nare used in order to ensure comparability. Table VII summarizes all the results. The different approaches are explained in the\nfollowing paragraphs.\nAs a ﬁrst approach, the Brown word clusters as described in Section V-B are used instead of the open class words themselves\n(1). Hence, the number of possible realizations of the open class factor is limited to 70. The factor values are more general\nCopyright (c) 2015 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\nThis is the author’s version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication.\nThe ﬁnal version of record is available at http://dx.doi.org/10.1109/TASLP.2015.2389622\nIEEE/ACM TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING 8\nTABLE VII\nPPL OF FLM S WITH WORDS , PART-OF-SPEECH TAGS , BROWN WORD CLUSTERS AND DIFFERENT OPEN CLASS WORD CLUSTERS\nApproach PPL dev PPL eval\nUnclustered 247.18 252.37\n(1) Brown clusters 254.23 260.29\n(2) Oc BrC 2k CS classes 248.07 253.01\n(2) Oc BrC 4k CS classes 247.52 252.44\n(2) Oc BrC 6k CS classes 247.47 252.53\n(2) Oc BrC 1k EN + 1k MAN classes 248.40 253.78\n(2) Oc BrC 2k EN + 2k MAN classes 247.89 252.84\n(2) Oc BrC 3k EN + 3k MAN classes 247.56 252.61\n(3) K-means clusters 1k EN + 1k MAN classes 249.13 254.13\n(3) K-means clusters 2k EN + 2k MAN classes 248.26 253.18\n(3) K-means clusters 3k EN + 3k MAN classes 247.97 252.81\n(4) Spectral clusters 1k EN + 1k MAN classes 248.93 254.07\n(4) Spectral clusters 2k EN + 2k MAN classes 248.31 252.94\n(4) Spectral clusters 3k EN + 3k MAN classes 248.02 252.69\n(5) Spectral clusters 1k CS classes 251.61 255.87\n(5) Spectral clusters 2k CS classes 250.53 254.65\n(5) Spectral clusters 3k CS classes 249.97 254.65\n(6) ML spectral clusters 250 classes 249.04 254.61\n(6) ML spectral clusters 500 classes 248.12 253.35\n(6) ML spectral clusters 800 classes 247.24 252.60\ncompared to using the open class words themselves. This approach is called “Brown clusters” in the table. The result shows\nthat although the Brown word cluster of the previous word has added useful information to the language modeling process,\nthe Brown cluster of the preceding open class word seems to rather add more confusability since the perplexity is increased.\nAn explanation could be that the Brown clusters have been trained on the whole training text including function words.\nSecond, the Brown clustering algorithm is applied only to the open class words (2). For this, all function words are deleted\nfrom the CS text. Again, different cluster sizes are explored. Furthermore, the monolingual open class texts as described in\nSection III-E3 are used to cluster the English and Mandarin words individually. Thus, the models called “oc Brown clusters\nCS classes” consist of classes containing both English and Mandarin words while the models “oc Brown clusters EN + MAN\nclasses” include separate classes for English and Mandarin words. The number of the CS classes is chosen to be the same as the\nsum of EN classes and MAN classes. The results show that the models perform better than “(1) Brown clusters”. Furthermore,\nit can be seen that the performance is improved with an increasing number of classes.\nThe clusters labeled with (1) and (2) are distribution based clusters. The following experiments explore semantic clusters. As\ndescribed in Section III-E3, two RNNLMs are trained on English and Mandarin monolingual texts, respectively. Then, the\nword vectors which are stored in the weight matrizes between the input and the hidden layer are extracted and clustered. Those\nclusters are, then, used as features in the FLMs. All the open class words which are not covered by the classes because they do\nnot appear in the monolingual texts, stay the same. This affects 5,271 different words (32.21% of all open class words) which\noccur in total 73,478 times (12.76% of all tokens) in the training text. Since the two different monolingual networks learn\ndifferent word representations, Mandarin words which are similar to English words might not be assigned to similar vectors\nand as a result, not to the same class. Hence, monolingual clusters are computed for English and Mandarin.\nFirst, k-means is used for clustering (3). The results are listed with the name “k-means clusters”. Since the previous results\nshowed that an increase of the number of classes leads to better performance, rather high class numbers are used in the\nexperiments. Indeed, the results show again that the perplexity decreases if the number of classes is raised. A possible evaluation\nof the clustering quality is the calculation of inter-cluster and intra-cluster variances. Inter-cluster variance denotes the distance\nof different clusters while intra-cluster variance shows how compact a cluster is. Based on [38], the variances are calculated\nas shown in the following equations.\nintra= 1\nN\nk∑\ni=1\n∑\nx∈ci\n|x−µ(ci)|2\ninter= min(|µ(ci) −µ(cj )|2),i = 1..k−1,j = i+ 1..k\n(7)\nThe term µ(ci) denotes the mean vector of class ci, k the number of classes and N the number of vectors. Furthermore, a\nvalidity ratio is computed as follows:\nratio= intra\ninter (8)\nSince the intra-cluster variance should be minimized while the inter-cluster variance should be maximized, lower ratios\ncorrespond to better clustering results. Table VIII provides the variances and ratios for the k-means clusters of different\nsizes.\nCopyright (c) 2015 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\nThis is the author’s version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication.\nThe ﬁnal version of record is available at http://dx.doi.org/10.1109/TASLP.2015.2389622\nIEEE/ACM TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING 9\nTABLE VIII\nINTRA -CLASS VARIANCES , INTER -CLASS VARIANCES AND VALIDITY RATIOS FOR DIFFERENT K -MEANS CLUSTER SIZES\nClustering intra-class variance inter-class variance ratio\n1000 EN classes 0.0911 0.0088 10.39\n2000 EN classes 0.0505 0.0105 4.82\n3000 EN classes 0.0277 0.0104 2.67\n1000 MAN classes 0.0804 0.0020 40.54\n2000 MAN classes 0.0454 0.0011 40.38\n3000 MAN classes 0.0262 0.0016 15.99\nThe ratios of the different k-means clusters show that the clustering quality is increased with a larger amount of classes. While\nthe intra-class variances are improved in all cases, the inter-class variances are not always raised. This shows that a higher\nclass number leads to more compact classes which are not necessarily better separated from each other.\nSince the word classes might not be linearly separable, spectral clustering is applied to the word vectors in the next step (4).\nThe results are called “spectral clusters”. To provide comparability, the same class sizes are used as for k-means. The results\nshow that spectral clustering leads to better perplexities than k-means clustering although the difference is very small.\nTable IX provides examples for words which are grouped into one class using spectral clustering with 2000 classes.\nTABLE IX\nEACH COLUMN REPRESENTS AN EXAMPLE OF THE CLASSES OBTAINED BY SPECTRAL CLUSTERING WITH 2000 CLASSES\nfriday august championships brazilian gym\nthursday books elephants german swim\ntuesday june olympics italian ski\nwednesday december stadium swiss skiing\nIn order to see how much additional information monolingual texts provide compared to the CS training text, a third RNNLM\nis trained using the open class words of the CS text as input (5). Its word vectors are clustered using spectral clustering again.\nThe table entries “spectral clusters CS classes” provide the results of this experiment. These models perform worse than the\nmodels with clusters based on the monolingual texts. The reason for this may lie in the clustering results. The CS spectral\nclusters do not group semantically similar words into the same class. The word “august”, for example, is grouped with the\nEnglish words “lag” and “subjects” and the Mandarin words “ 墨” (ink), “ 层” (layer), “ 成了” (became), “ 断绝” (sever) and\n“用法” (usage). Reasons for this may be the small amount of CS training data or the bilinguality of the text.\nIn order to experiment with multilingual spectral clusters, a training text for the RNNLM is created using lines of both the\nEnglish and the Mandarin texts (6). During training, the hidden layer of the network is reset after each line. Hence, the English\nand Mandarin words are trained with the same network but separately from each other. This seems to be reasonable since the\nsentences are extracted from different news texts. Therefore, an English sentence does not depend on the previous Mandarin\nsentence and vice versa. Again, the resulting word vectors are clustered using spectral clustering. Due to the combination of\nboth languages, the classes will consist of both English and Mandarin words. The results of the FLMs using those classes\nas features are called “multilingual (ML) spectral clusters” in the table. Class sizes beyond 800 classes are not investigated\nsince the 800 classes only contain about 1.17 words on average (minimum: 1 word, maximum: 4 words). Furthermore, 15,450\nwords (94.40% of all distinct open class words) which occur in total 201,210 times (34.95% of all tokens) in the training text\nare not covered by the multilingual clusters. For these words, the open class words themselves are used instead of classes as\ndescribed above.\nAll the clustering experiments could not lead to FLMs superior to the model with unclustered open class words. However,\nthe difference among the perplexity results is not large enough to be able to decide which model performs the best. The best\ncluster size seems to be at or even beyond 3000 classes. However, those classes do not contain many words. The classes of\nthe English “oc Brown clusters” of size 3000, for example, contain 1 to 9 words per class and on average 5.45 words. The\nclasses of the Mandarin “oc Brown clusters” of size 3000 also contain 1 to 9 words per class but on average only 1.89 words.\nHence, the difference to unclustered words is rather small. An explanation why open class words seem to outperform open\nclass word classes could be the higher branching factor after a class with many members compared to the branching factor\nafter a single word. This might suppress the positive backoff effect of clusters in this case.\nSince the sixth approach (multilingual spectral clustering) with 800 classes performed best in terms of perplexity on the dev\nset, this model will be used in the decoding experiments. It will be referred to as “open class word clusters”.\n1) Analysis of open class word clusters:To further evaluate the open class word clusters, an analysis of their distribution is\nperformed. The number of occurrences of each class of the ML spectral clusters with 800 classes in the SEAME development\nset is counted. Then, it is extracted how many clusters occur more than 10, 50, 100, 250 and 500 times. Figure 3 shows the\nresults. It can be noted that only few clusters occur more than 100 times in the text.\nCopyright (c) 2015 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\nThis is the author’s version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication.\nThe ﬁnal version of record is available at http://dx.doi.org/10.1109/TASLP.2015.2389622\nIEEE/ACM TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING 10\n 0\n 100\n 200\n 300\n 400\n 500\n 600\n 0  50  100  150  200  250  300  350  400  450  500\nnumber of clusters\nnumber of occurrences\n(1) Brown clusters\n(2) 6000 cs oc-Brown clusters\n(3) 3000 en + 3000 man k-means clusters\n(4) 3000 en + 3000 man spectral clusters\n(5) 3000 cs spectral clusters\n(6) 1000 ml spectral clusters\nFig. 3. Number of clusters occuring certain times in the SEAME development text\nE. Perplexity results: summary\nThe largest perplexity improvements are obtained by using Brown word clusters (alone and in combination with other\nfeatures). This corresponds to the observations in Section III since Brown word clusters provide the highest CS rates on\naverage. Interestingly, the CS rates of open class word clusters are superior to the rates of open class words but this does not\ntransfer to the perplexity results. A possible explanation is a higher branching factor after clusters in contrast to words.\nThe FLM which performs best in terms of perplexity consists of the factors words, POS tags, Brown word clusters and open\nclass words. Its conditioning factors and backoff paths are shown in Figure 4. The main idea behind the backoff graph is to\nP(-1)   C(-1)OC(-1)   C(-1) OC(-1)   P(-1) W(-1)   C(-1)W(-1)   P(-1)W(-1)   OC(-1)\nW(-1)   OC(-1)   C(-1) W(-1)   OC(-1)   P(-1) W(-1)   C(-1)   P(-1)P(-1)   OC(-1)   C(-1) W(-1)   C(-2)   P(-1) W(-1)   C(-1)   C(-2)P(-1)   C(-1)   C(-2)\nW(-1)   C(-1)   P(-1)   OC(-1) W(-1)   C(-1)   P(-1)   C(-2)\nW(-1)   C(-1)   P(-1)   OC(-1)   C(-2)\nW(-1)OC(-1) P(-1) C(-1)\nunigram\nP(-1)   C(-2) C(-1)   C(-2) W(-1)   C(-2)\nC(-2)\nFig. 4. The backoff graph of the best FLM (Dashed lines indicate the application of general backoff instead of averaging the results of ﬁxed backoff paths)\nﬁrst drop either the oldest feature ( C(-2)) or the open class word ( OC(-1)) in order to continue with a model similar to the\nsecond best model (FLM Brown word clusters + POS). Afterwards, the results of all possible backoff paths are combined using\ntheir average. In the case of backoff to one or two factors including the penultimate Brown word cluster ( C(-2)), general\nbackoff is applied (indicated by dashed lines in the ﬁgure). A possible explanation is that the penultimate Brown word cluster\nmay be necessary for the prediction of the next word in some but not in all cases. The backoff graph has been investigated\nexperimentally and chosen because of superior results in terms of PPL on the dev set compared to other backoff strategies.\nTable X summarizes the most important results of the experiments conducted with different factors for FLMs. In addition, it\nprovides results and weights for interpolating the FLMs with the baseline 3-gram language model.\nIt can be found that interpolating the FLMs with the baseline 3-gram model leads to superior perplexity results in all cases.\nExcept for one model (the FLM using only open class words), the interpolation weight for the FLM is always above 0.5. This\nshows the high impact of syntactic and semantic features for language modeling for Code-Switching.\nAll factored language models lead to perplexity results which are statistically signiﬁcantly better than the baseline 3-gram\nperplexities. The models with Brown word clusters are also signiﬁcantly superior to the models without. However, the difference\nbetween the model with open class words and the best model with open class word clusters cannot be considered statistically\nsigniﬁcant.\nCopyright (c) 2015 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\nThis is the author’s version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication.\nThe ﬁnal version of record is available at http://dx.doi.org/10.1109/TASLP.2015.2389622\nIEEE/ACM TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING 11\nTABLE X\nSUMMARY : PPL OF DIFFERENT FLM S, COMPARED TO AND INTERPOLATED WITH THE BASELINE 3-GRAM MODEL\nModel PPL dev PPL eval\nBaseline (3-gram) 268.39 282.86\nFLM POS + LID 257.62 264.20\n+ CS 3-gram ( wFLM = 0.55) 246.36 253.27\nFLM BrC + POS 249.00 255.34\n+ CS 3-gram ( wFLM = 0.63) 241.89 248.53\nFLM open class words + BrC + POS 247.18 252.37\n+ CS 3-gram ( wFLM = 0.63) 238.87 245.27\nFLM open class clusters + BrC + POS 247.24 252.60\n+ CS 3-gram ( wFLM = 0.63) 238.86 245.40\nVI. FLM S IN THE DECODING PROCESS\nA. Using FLM during decoding\nFor the ASR experiments, the speaker independent acoustic model and the pronunciation dictionary of the ASR system\ndescribed in [39] are used. The decoding is performed using the BioKit decoder [40]. During decoding, the baseline 3-gram\nlanguage model is used for lookahead. At every word end, the language model score is obtained by interpolating the FLM\nand the 3-gram language model. The interpolation weight is chosen based on mixed error rate results on the development\nset. For these experiments, the FLM containing only words and POS tags is used. For each speaker, the ﬁrst 50 sentences\nare decoded. This corresponds to more than 20% of all sentences of the development set. This number has been chosen to,\non the one hand, achieve reliable results but, on the other hand, reduce computational efforts. The experiments reveal that an\ninterpolation weight of 0.45 lead to the lowest mixed error rates. Table XI presents the mixed error rates when the different\nFLMs are used in the decoding process. To be able to compare the mixed error rate results with the perplexities, the perplexity\nresults of the FLMs are presented when they are interpolated with the decoder baseline language model using a weight of\n0.45. The decoding results show that the mixed error rate is not always correlated with the perplexity results. However, all the\nFLMs outperform the traditional 3-gram language model.\nTABLE XI\nMIXED ERROR RATE AND PERPLEXITY RESULTS FOR THE DIFFERENT FLM S WHEN THEY ARE INTERPOLATED WITH THE CS 3- GRAM USING AN FLM\nINTERPOLATION WEIGHT OF 0.45\nModel MER dev MER eval PPL dev\nDecoder baseline 3-gram 39.96% 34.31% 292.58\nPOS 39.47% 33.46% 250.64\nPOS + LID 39.66% 33.30% 248.38\nBrC 39.45% 33.93% 249.05\nBrC + POS 39.30% 33.60% 244.62\nBrC + POS + LID 39.39% 33.16% 248.64\nOc words + BrC + POS 39.33% 33.15% 245.79\nOc clusters (ML spectral 800 cl) 39.30% 33.16% 245.79\n+ BrC + POS\nB. Analysis of results\nTo obtain a better understanding of the advantages of the FLMs, an error analysis is provided in Table XII. The results of\nthe FLMs which lead to the best mixed error rate results on the development set (FLM Brown word clusters + POS tags and\nFLM Brown word clusters + POS tags + open class word clusters) are compared to the results of the baseline model in detail.\nSince some of the numbers denote accuracy values and others are error rates, a language model is not always superior if its\nnumber is higher (lower). The FLMs lead to improvements regarding insertion errors and monolingual segments. Furthermore,\nwords at CS points are recognized more robustly.\nThe mixed error rate results obtained by using FLMs are statistically signiﬁcantly better than those by using only the baseline\n3-gram model. However, the different FLMs do not lead to statistically signiﬁcantly different results.\nVII. C ONCLUSION AND FUTURE WORK\nThe factored language models outperform a traditional 3-gram language model both in terms of perplexity and mixed error\nrate on the SEAME Code-Switching corpus. The combination of the features open class words, Brown word clusters and\nPOS tags achieves the best perplexity results on the development and evaluation sets and the best mixed error rate results on\nthe evaluation set. Brown word clusters alone lead to a similar performance as POS tags alone. Their advantage is that they\ndo not rely on an expensive tagging process with unknown accuracy. On the development data, the combination of Brown\nword clusters, POS tags and clusters of open class word embeddings leads to the best mixed error rate results. Most of these\nCopyright (c) 2015 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\nThis is the author’s version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication.\nThe ﬁnal version of record is available at http://dx.doi.org/10.1109/TASLP.2015.2389622\nIEEE/ACM TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING 12\nTABLE XII\nRESULT ANALYSIS AFTER DECODING WITH THE DECODER BASELINE MODEL AND FLM 1 (B ROWN WORD CLUSTERS + POS TAGS) OR FLM 2 (B ROWN\nWORD CLUSTERS + POS TAGS + OC WORD CLUSTERS )\nBaseline FLM 1 FLM 2\nMER in English segments 59.40% 57.52% 56.02%\nMER in Mandarin segments 36.48% 36.12% 36.24%\nCorrect words 64.37% 64.39% 64.35%\nDeletion of English words 1.65% 1.83% 1.86%\nDeletion of Mandarin words 5.79% 6.32% 6.31%\nInsertion of English words 1.09% 0.87% 0.84%\nInsertion of Mandarin words 2.99% 2.62% 2.67%\nSubstitution of EN with EN 4.87% 4.76% 4.68%\nSubstitution of EN with MAN 4.38% 4.41% 4.44%\nSubstitution of MAN with MAN 15.30% 14.93% 14.93%\nSubstitution of MAN with EN 2.85% 2.50% 2.57%\nWord correct after CS 37.52% 37.80% 37.61%\nLanguage correct after CS 68.23% 66.70% 66.79%\nimprovements are also statistically signiﬁcant.\nAlthough the method and features are evaluated only on a Mandarin-English Code-Switching corpus in this paper, the\nmethodology is language pair independent. Hence, it can be applied to corpora with different languages, too. Especially\nthe Brown word clusters and open class word clusters do not require knowledge about the language of a certain word.\nPossible future work is the integration of machine translation in order to create monolingual corpora based on the bilingual\ntext and extract additional features from them.\nACKNOWLEDGMENT\nThe authors would like to thank Dr Li Haizhou to allow us to use the SEAME corpus for this research work.\nREFERENCES\n[1] S. Poplack, Syntactic structure and social function of code-switching. Centro de Estudios Puertorrique ˜nos, City University of New York, 1978.\n[2] E. Bokamba, “Are there syntactic constraints on code-mixing?” World Englishes, vol. 8, no. 3, pp. 277–292, 1989.\n[3] P. Muysken, Bilingual speech: A typology of code-mixing. Cambridge University Press, 2000, vol. 11.\n[4] P. Auer, “From codeswitching via language mixing to fused lects toward a dynamic typology of bilingual speech,” International Journal of Bilingualism,\nvol. 3, no. 4, pp. 309–332, 1999.\n[5] N. T. Vu, H. Adel, and T. Schultz, “An investigation of code-switching attitude dependent language modeling,” in SLSP, 2013.\n[6] S. Poplack, “Sometimes i’ll start a sentence in Spanish y termino en Espa ˜nol: Toward a typology of code-switching,” Linguistics, vol. 18, no. 7-8, pp.\n581–618, 1980.\n[7] T. Solorio and Y . Liu, “Learning to predict code-switching points,” in EMNLP. ACL, 2008.\n[8] J. Chan, P. Ching, T. Lee, and H. Cao, “Automatic speech recognition of Cantonese-English code-mixing utterances,” in Interspeech, 2006.\n[9] Y . Li and P. Fung, “Code-switch language model with inversion constraints for mixed language speech recognition.” in COLING, 2012.\n[10] H. Adel, N. T. Vu, F. Kraus, T. Schlippe, H. Li, and T. Schultz, “Recurrent neural network language modeling for code switching conversational speech,”\nin ICASSP. IEEE, 2013.\n[11] K. Duh and K. Kirchhoff, “Automatic learning of language model structure,” in COLING, 2004.\n[12] A. El-Desoky, R. Schl ¨uter, and H. Ney, “A hybrid morphologically decomposed factored language models for Arabic LVCSR,” in NAACL. ACL, 2010.\n[13] H. Adel, N. T. Vu, and T. Schultz, “Combination of recurrent neural networks and factored language models for code-switching language modeling,” in\nACL, 2013.\n[14] H. Adel, K. Kirchhoff, D. Telaar, N. T. Vu, T. Schlippe, and T. Schultz, “Features for factored language models for code-switching speech,” in SLTU,\n2014.\n[15] T. Mikolov, W.-T. Yih, and G. Zweig, “Linguistic regularities in continuous space word representations,” in NAACL. ACL, 2013.\n[16] D. Lyu, T. Tan, E. Chng, and H. Li, “An analysis of a Mandarin-English code-switching speech corpus: SEAME,” Age, vol. 21, pp. 25–8, 2010.\n[17] K. Toutanova, D. Klein, C. Manning, and Y . Singer, “Feature-rich part-of-speech tagging with a cyclic dependency network,” in NAACL. ACL, 2003.\n[18] T. Schultz, P. Fung, and C. Burgmer, “Detecting code-switch events based on textual features,” Diploma Thesis, KIT, 2009.\n[19] C. M. Scotton, Duelling languages: Grammatical structure in codeswitching. Oxford University Press, 1997.\n[20] K. Toutanova and C. Manning, “Enriching the knowledge sources used in a maximum entropy part-of-speech tagger,” in EMNLP/VLC. ACL, 2000.\n[21] N. Xue, F. Xia, F. Chiou, and M. Palmer, “The Penn Chinese treebank: Phrase structure annotation of a large corpus,” Natural Language Engineering,\nvol. 11, no. 2, p. 207, 2005.\n[22] M. Marcus, M. Marcinkiewicz, and B. Santorini, “Building a large annotated corpus of English: The Penn treebank,” Computational Linguistics, vol. 19,\nno. 2, pp. 313–330, 1993.\n[23] P. F. Brown, P. V . Desouza, R. L. Mercer, V . J. D. Pietra, and J. C. Lai, “Class-based n-gram models of natural language,” Computational Linguistics,\nvol. 18, no. 4, pp. 467–479, 1992.\n[24] A. Stolcke et al., “SRILM - an extensible language modeling toolkit,” in SLP, vol. 2, 2002.\n[25] G. Haffari, M. Razavi, and A. Sarkar, “An ensemble model that combines syntactic and semantic clustering for discriminative dependency parsing,” in\nACL: HLT, 2011, pp. 710–714.\n[26] V . Fromkin, An introduction to language. Cengage Learning, 2013.\n[27] “English functional words,” 2014. [Online]. Available: http://www2.fs.u-bunkyo.ac.jp/ ∼gilner/wordlists.html#functionwords\n[28] “Mandarin functional words,” 2014. [Online]. Available: http://chinesenotes.com/topic.php?english=Function+Words\n[29] J. MacQueen, “Some methods for classiﬁcation and analysis of multivariate observations,” in Berkeley symposium on mathematical statistics and\nprobability, 1967, pp. 281–297.\n[30] I. S. Dhillon, Y . Guan, and B. Kulis, “Weighted graph cuts without eigenvectors: a multilevel approach,” Pattern Analysis and Machine Intelligence,\nIEEE Transactions on, vol. 29, no. 11, pp. 1944–1957, 2007.\nCopyright (c) 2015 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\nThis is the author’s version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication.\nThe ﬁnal version of record is available at http://dx.doi.org/10.1109/TASLP.2015.2389622\nIEEE/ACM TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING 13\n[31] T. Mikolov, M. Karaﬁ ´at, L. Burget, J. Cernocky, and S. Khudanpur, “Recurrent neural network based language model,” in Interspeech, 2010.\n[32] M. Bod ´en, “A guide to recurrent neural networks and backpropagation,” The Dallas project, SICS technical report, 2002.\n[33] T. Mikolov, S. Kombrink, L. Burget, J. Cernocky, and S. Khudanpur, “Extensions of recurrent neural network language model,” in ICASSP. IEEE,\n2011.\n[34] T. Mikolov, S. Kombrink, A. Deoras, L. Burget, and J. Cernock `y, “RNNLM–recurrent neural network language modeling toolkit,” in ASRU. IEEE,\n2011.\n[35] R. Rosenfeld, “Two decades of statistical language modeling: Where do we go from here?” Proc. of the IEEE, no. 88, pp. 1270–1278, 2000.\n[36] K. Kirchhoff, J. Bilmes, and K. Duh, “Factored language models tutorial,” Dept of EE, University of Washington, Tech. Rep., 2007.\n[37] J. Bilmes and K. Kirchhoff, “Factored language models and generalized parallel backoff,” in NAACL. ACL, 2003.\n[38] S. Ray and R. Turi, “Determination of number of clusters in k-means clustering and application in colour image segmentation,” in ICAPRDT, 1999.\n[39] N. T. Vu, D. Lyu, J. Weiner, D. Telaar, T. Schlippe, F. Blaicher, E. Chng, T. Schultz, and H. Li, “A ﬁrst speech recognition system for Mandarin-English\ncode-switch conversational speech,” in ICASSP. IEEE, 2012.\n[40] D. Telaar, M. Wand, D. Gehrig, F. Putze, C. Amma, D. Heger, N. Vu, M. Erhardt, T. Schlippe, M. Janke, C. Herff, and T. Schultz, “Automatic speech\nrecognition of Cantonese-English code-mixing utterances,” in Interspeech, 2014.\nHeike Adel is a PhD student at the Center for Information and Language Processing, University of Munich in Germany. She studied Computer Science at\nKarlsruhe Institute of Technology (KIT) in Karlsruhe, Germany and received her Bachelor degree in 2011 and her Master degree in 2014. Since April 2014,\nshe works as a research and teaching assistant at University of Munich under the supervision of Prof. Hinrich Schuetze. The main focus of her work is natural\nlanguage processing using deep learning techniques.\nNgoc Thang Vureceived his PhD in Computer Science from the Karlsruhe Institute of Technology, Germany in 2014. He joined Nuance Communications\nas a senior research scientist in 2014. Currently he is also a visiting professor in Computational Linguistics at the University of Munich (LMU). His research\ninterests are multilingual speech recognition for low resource languages and accents, and natural language processing.\nKatrin Kirchhoffreceived her PhD in Computer Science from the University of Bielefeld, Germany, in 1999. She is currently a Research Associate Professor\nin Electrical Engineering at the University of Washington. Her research focuses on speech recognition, natural language processing, machine translation,\nand human-computer interfaces. She has authored or co-authored over 100 publications on speech and language processing. From From 2009-2011 she was\na Member of the IEEE Speech Technical Committee. She currently serves on the editorial boards of Speech Communication and Computer, Speech and\nLanguage.\nDominic Telaaris a research assistant at the Cognitive Systems Lab, at the Institute of Anthropomatics and Robotics at the Karlsruhe Institute of Technology\n(KIT) in Germany. He received his Diploma degree in Computer Science at KIT in 2010. Afterwards, he has worked as a research and teaching assistant at\nKIT. The main focus of his work is the development of techniques for the BioKIT recognition toolkit.\nTanja Schultzreceived her Ph.D. and Masters in Computer Science from University of Karlsruhe, Germany in 2000 and 1995 respectively. She joined Carnegie\nMellon University in 2000 and became a Research Professor at the Language Technologies Institute. Since 2007 she is a Full Professor at the Department of\nInformatics of the Karlsruhe Institute of Technology (KIT) in Germany. Her research activities focus on human-machine interfaces with a particular area of\nexpertise in rapid adaptation of speech processing systems to new domains and languages. She has published more than 250 articles in books, journals, and\nproceedings, and has received several awards and prizes for her work. She is a member of the Society of Computer Science (GI) for more than 20 years, of\nthe IEEE Computer Society, and the International Speech Communication Association (ISCA) where she was elected as the president in 2013.\nCopyright (c) 2015 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org."
}