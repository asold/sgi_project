{
  "title": "Athena: Automated Tuning of k-mer based Genomic Error Correction Algorithms using Language Models",
  "url": "https://openalex.org/W2986786507",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2908038247",
      "name": "Mustafa Abdallah",
      "affiliations": [
        "Purdue University West Lafayette"
      ]
    },
    {
      "id": "https://openalex.org/A2624676009",
      "name": "Ashraf Mahgoub",
      "affiliations": [
        "Purdue University West Lafayette"
      ]
    },
    {
      "id": "https://openalex.org/A2160773908",
      "name": "Hany Ahmed",
      "affiliations": [
        "Cairo University"
      ]
    },
    {
      "id": "https://openalex.org/A2068600733",
      "name": "Somali Chaterji",
      "affiliations": [
        "Purdue University West Lafayette"
      ]
    },
    {
      "id": "https://openalex.org/A2908038247",
      "name": "Mustafa Abdallah",
      "affiliations": [
        "Purdue University West Lafayette"
      ]
    },
    {
      "id": "https://openalex.org/A2624676009",
      "name": "Ashraf Mahgoub",
      "affiliations": [
        "Purdue University West Lafayette"
      ]
    },
    {
      "id": "https://openalex.org/A2160773908",
      "name": "Hany Ahmed",
      "affiliations": [
        "Cairo University"
      ]
    },
    {
      "id": "https://openalex.org/A2068600733",
      "name": "Somali Chaterji",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2748933472",
    "https://openalex.org/W2210296741",
    "https://openalex.org/W2268029186",
    "https://openalex.org/W2119745866",
    "https://openalex.org/W2133956160",
    "https://openalex.org/W2001079689",
    "https://openalex.org/W2129276621",
    "https://openalex.org/W1980667059",
    "https://openalex.org/W1598134722",
    "https://openalex.org/W2101250487",
    "https://openalex.org/W2104677379",
    "https://openalex.org/W2747175821",
    "https://openalex.org/W2760000637",
    "https://openalex.org/W2113154626",
    "https://openalex.org/W2098531465",
    "https://openalex.org/W2515791790",
    "https://openalex.org/W4211088835",
    "https://openalex.org/W6743198006",
    "https://openalex.org/W1975570633",
    "https://openalex.org/W2293185259",
    "https://openalex.org/W1631260214",
    "https://openalex.org/W6713134421",
    "https://openalex.org/W2170551349",
    "https://openalex.org/W2412192467",
    "https://openalex.org/W2152246890",
    "https://openalex.org/W2887758720",
    "https://openalex.org/W2168602018",
    "https://openalex.org/W2114931329",
    "https://openalex.org/W2167469524",
    "https://openalex.org/W2160969485",
    "https://openalex.org/W2107772251",
    "https://openalex.org/W2604585222",
    "https://openalex.org/W1966711026",
    "https://openalex.org/W2792561556",
    "https://openalex.org/W2121085676",
    "https://openalex.org/W2123552972",
    "https://openalex.org/W1972046858",
    "https://openalex.org/W2146712012",
    "https://openalex.org/W2140890018",
    "https://openalex.org/W2774492845",
    "https://openalex.org/W2613206411",
    "https://openalex.org/W2318383848",
    "https://openalex.org/W2058340614",
    "https://openalex.org/W2604856537",
    "https://openalex.org/W2549416390",
    "https://openalex.org/W2163382007",
    "https://openalex.org/W2121227244",
    "https://openalex.org/W2154598603",
    "https://openalex.org/W2953384591",
    "https://openalex.org/W2171928131"
  ],
  "abstract": "Abstract The performance of most error-correction (EC) algorithms that operate on genomics reads is dependent on the proper choice of its configuration parameters, such as the value of k in k -mer based techniques. In this work, we target the problem of finding the best values of these configuration parameters to optimize error correction and consequently improve genome assembly. We perform this in an adaptive manner, adapted to different datasets and to EC tools, due to the observation that different configuration parameters are optimal for different datasets, i . e ., from different platforms and species, and vary with the EC algorithm being applied. We use language modeling techniques from the Natural Language Processing (NLP) domain in our algorithmic suite, Athena, to automatically tune the performance-sensitive configuration parameters. Through the use of N -Gram and Recurrent Neural Network (RNN) language modeling, we validate the intuition that the EC performance can be computed quantitatively and efficiently using the “perplexity” metric, repurposed from NLP. After training the language model, we show that the perplexity metric calculated from a sample of the test (or production) data has a strong negative correlation with the quality of error correction of erroneous NGS reads. Therefore, we use the perplexity metric to guide a hill climbing-based search, converging toward the best configuration parameter value. Our approach is suitable for both de novo and comparative sequencing (resequencing), eliminating the need for a reference genome to serve as the ground truth. We find that Athena can automatically find the optimal value of k with a very high accuracy for 7 real datasets and using 3 different k -mer based EC algorithms, Lighter, Blue, and Racer. The inverse relation between the perplexity metric and alignment rate exists under all our tested conditions—for real and synthetic datasets, for all kinds of sequencing errors (insertion, deletion, and substitution), and for high and low error rates. The absolute value of that correlation is at least 73%. In our experiments, the best value of k found by A thena achieves an alignment rate within 0.53% of the oracle best value of k found through brute force searching ( i . e ., scanning through the entire range of k values). Athena’s selected value of k lies within the top-3 best k values using N-Gram models and the top-5 best k values using RNN models With best parameter selection by Athena, the assembly quality (NG50) is improved by a Geometric Mean of 4.72X across the 7 real datasets.",
  "full_text": "1Scientific  RepoRtS  | (2019) 9:16157 | https://doi.org/10.1038/s41598-019-52196-4\nwww.nature.com/scientificreports\nAthena: Automated tuning \nof k-mer based Genomic error \ncorrection Algorithms using \nLanguage Models\nMustafa Abdallah1,4, Ashraf Mahgoub1,4, Hany Ahmed2 & Somali chaterji3*\nthe performance of most error-correction (ec) algorithms that operate on genomics reads is dependent \non the proper choice of its configuration parameters, such as the value of k in k-mer based techniques. \nIn this work, we target the problem of finding the best values of these configuration parameters \nto optimize error correction and consequently improve genome assembly. We perform this in an \nadaptive manner, adapted to different datasets and to EC tools, due to the observation that different \nconfiguration parameters are optimal for different datasets, i.e., from different platforms and species, \nand vary with the EC algorithm being applied. We use language modeling techniques from the Natural \nLanguage Processing (NLP) domain in our algorithmic suite, Athena, to automatically tune the \nperformance-sensitive configuration parameters. Through the use of N-Gram and Recurrent neural \nNetwork (RNN) language modeling, we validate the intuition that the EC performance can be computed \nquantitatively and efficiently using the “perplexity” metric, repurposed from NLP . After training \nthe language model, we show that the perplexity metric calculated from a sample of the test (or \nproduction) data has a strong negative correlation with the quality of error correction of erroneous NGS \nreads. Therefore, we use the perplexity metric to guide a hill climbing-based search, converging toward \nthe best configuration parameter value. Our approach is suitable for both de novo and comparative \nsequencing (resequencing), eliminating the need for a reference genome to serve as the ground truth. \nWe find that Athena can automatically find the optimal value of k with a very high accuracy for 7 real \ndatasets and using 3 different k-mer based EC algorithms, Lighter, Blue, and Racer. The inverse relation \nbetween the perplexity metric and alignment rate exists under all our tested conditions—for real and \nsynthetic datasets, for all kinds of sequencing errors (insertion, deletion, and substitution), and for high \nand low error rates. The absolute value of that correlation is at least 73%. In our experiments, the best \nvalue of k found by Athena achieves an alignment rate within 0.53% of the oracle best value of k found \nthrough brute force searching (i.e., scanning through the entire range of k values). Athena’s selected \nvalue of k lies within the top-3 best k values using N-Gram models and the top-5 best k values using \nRNN models With best parameter selection by Athena, the assembly quality (NG50) is improved by a \nGeometric Mean of 4.72X across the 7 real datasets.\nRapid advances in next-generation sequencing (NGS) technologies, with the resulting drops in sequencing costs, \noffer unprecedented opportunities to characterize genomes across the tree-of-life. While NGS techniques allow \nfor rapid parallel sequencing, they are more error-prone than Sanger reads and generate different error profiles, \ne.g., substitutions, insertions, and deletions. The genome analysis workflow needs to be carefully orchestrated so \nerrors in reads are not magnified downstream. Consequently, multiple error-correction (EC) techniques have \nbeen developed for improved performance for applications ranging from de novo  variant calling to differential \nexpression, iterative k-mer selection for improved genome assembly\n1 and for long-read error correction2,3.\n1School of Electrical and Computer Engineering, Purdue University, West Lafayette, USA. 2Department of \nElectronics and Electrical Communications Engineering, Cairo University, Cairo, Egypt. 3Department of Agricultural \nand Biological Engineering, Purdue University, West Lafayette, USA. 4These authors contributed equally: Mustafa \nAbdallah and Ashraf Mahgoub. *email: schaterji@schaterji.io\nCorrected: Author Correction\nopen\n2Scientific  RepoRtS  | (2019) 9:16157 | https://doi.org/10.1038/s41598-019-52196-4\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\nDataset Exhaustive Search With Athena (RNN) With Athena (N-gram)\nLighter\nSelected k\nAlignment \nRate (%)\nEC Gain \n(%) Selected k\nAlignment \nRate (%)\nEC Gain \n(%) Selected k\nAlignment \nRate(%)\nEC \nGain \n(%)\nD1 k = 17 98.95% 96.30% Same as Exhaustive Search Same as Exhaustive Search\nD2 k = 15 61.42% 73.80% k = 17 61.15% 80.10% Same as RNN\nD3 k = 15 80.44% 86.78% k = 17 80.39% 95.34% Same as Exhaustive Search\nD4 k = 17 93.95% 89.87% Same as Exhaustive Search Same as Exhaustive Search\nD5 k = 17 92.15% 81.70% k = 25 92.09% 83.80% Same as Exhaustive Search\nD6 k = 25 86.16% NA k = 17 85.63% NA Same as RNN\nD7 k = 15 40.53% 37.58% Same as Exhaustive Search k = 17 40.24% 7.70%\nBlue\nD1 k = 20 99.53% 99% k = 25 99.29% 98.60% Same as Exhaustive Search\nD2 k = 20 57.44% 4.61% Same as Exhaustive Search Same as Exhaustive Search\nD3 k = 20 84.17% 99.20% Same as Exhaustive Search Same as Exhaustive Search\nD4 k = 20 95.31% 98.50% Same as Exhaustive Search Same as Exhaustive Search\nD5 k = 20 92.33% 88.90% Same as Exhaustive Search Same as Exhaustive Search\nD6 k = 30 86.18% NA Same as Exhaustive Search k = 25 86.07% NA\nD7 k = 25 17.19% 3.57% k = 30 16.96% 1.47% Same as Exhaustive Search\nRACER\nD1 GL = 4.7M 99.26% 84.80% Same as Exhaustive Search Same as Exhaustive Search\nD2 GL = 4.7M 81.15% 92.90% Same as Exhaustive Search Same as Exhaustive Search\nD3 GL = 3.7M 84.11% 88.27% Same as Exhaustive Search Same as Exhaustive Search\nD4 GL = 4.2M 95.33% 97% Same as Exhaustive Search Same as Exhaustive Search\nD5 GL = 4.2M 92.29% 81.63% GL = 20M 92.28% 80.50% Same as Exhaustive Search\nD6 GL = 120M 86.36% NA Same as Exhaustive Search GL = 20M 86.12% NA\nD7 GL = 3M 17.55% 21.10% GL = 20M 17.40% 26.50% Same as Exhaustive Search\nTable 2. Comparison of Lighter, Blue, and RACER using 7 datasets. This is for finding the best k-value (GL \nfor RACER) using Athena variants vs. exhaustive search. We find either the optimal value or within 0.53% \n(over Alignment Rate) and within 8.5% (EC Gain) of the theoretical best (in the worst case), consistent with \nthe reported results by Lighter (Figure 5 in\n6). These slightly sub-optimal configurations is due to the impact of \nsub-sampling. However, with appropriate sampling rate selection, Athena achieves configurations that is 0.53% \nof the oracle best configuration (found with exhaustive searching). We also notice that for RACER, GL found \nby Athena is within 3% of the reference GL (except for the RNN model with D5, which still achieves very close \nperformance for both Alignment Rate and EC Gain). The Gain metric is not shown for D6 as the tool used \nto compute it was not able to handle reads of length 250 bp. We notice that the best genome length found by \nAthena for D7 (human genome) is 20Mbp, which is very low compared to the actual human genome length \n(≈3 k Mbp). This shows that using heuristics to estimate the optimal value of K based on only one parameter \n(genome length) can produce significantly suboptimal performance, even if the actual value of the genome \nlength is provided. Moreover, GenomeLength parameter in Racer represents the approximate length of the \nDNA molecule that originated the reads. If only parts of a genome were sequenced, then only the total length of \nthose parts should be used, instead of the length of the total genome. Dataset D7 is just for a part of the genome \n(24 Mbp), and Athena ‘s genome length selection of 20Mbp shows the efficacy of Athena for this usecase.\nDataset Coverage #Reads Read Length Genome Type Accession Number\nD1 80X 20.8M 136 bp E. coli str. K-12 substr SRR001665\nD2 71X 7.1M 47 bp E. coli str. K-12 substr SRR022918\nD3 173X 18.1M 36 bp Acinetobacter sp. ADP1 SRR006332\nD4 62X 3.5M 75 bp B. subtilis DRR000852\nD5 166X 7.1M 100 bp L. interrogans C sp. \nADP1 SRR397962\nD6 70X 33.6M 250 bp A. thaliana ERR2173372\nD7 67X 202M 101 bp Homo sapiens SRR1658570\nTable 1. Datasets’ description with coverage, number of reads, read lengths, genome type, and the Accesson \nnumber. Coverage is estimated according to Illumina’s documentation49.\n3Scientific  RepoRtS  | (2019) 9:16157 | https://doi.org/10.1038/s41598-019-52196-4\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\nImportantly, the values of the performance-sensitive configuration parameters are dependent not only on the \ndataset but also on the specific EC tool (Table 2). The performance of many EC algorithms is highly dependent \non the proper choice of configuration parameters, e.g., k-value (length of the substring) in k -spectrum-based \ntechniques. Selecting different k -values has a trade-off such that small values increase the overlap probability \nbetween reads, however, an unsuitably small value degrades EC performance because it does not allow the algo-\nrithm to discern correct k-mers from erroneous ones. In contrast, unsuitably high k-values decrease the overlap \nprobability and hurts EC performance because most k -mers will now appear unique. The k -mers that appear \nabove a certain threshold frequency, and are therefore expected to be legitimate, are solid k-mers, the others are \ncalled insolid or untrusted k-mers. In k-spectrum-based methods, the goal is to convert insolid k -mers to solid \nones with a minimum number of edit operations. Thus, an adaptive method for finding the best k-value and other \nparameters is needed for improved EC performance, and in turn, genome assembly.\nMany existing EC solutions (e.g., Reptile 4, Quake5, Lighter6, Blue7) require users to specify the k -mer size. \nAlthough these EC tools do not rely on a reference genome to perform correction, the best configuration value \nis usually found by exploration over the range of k -values\n8 and evaluating performance metrics, e.g., EC Gain, \nAlignment Rate, Accuracy, Recall, and Precision using a reference genome. Therefore, a reference genome is \ntypically needed to serve as ground truth for such evaluations, making this tuning approach infeasible for de \nnovo sequencing tasks. Existing tools leave the best parameter choice to the end user and this has been explicitly \npointed out as an open area of work in\n1,9. However, a number of recent surveys highlighted that the automated \nchoice of parameters for the specific dataset being processed is crucial for the user to avoid inadvertently selecting \nthe wrong parameters\n10.\nSome existing tools (e.g., KMERGENIE 11) provide intuitive abundance histograms or heuristics to guide \nk-value selection when performing de Bruijn graph based genome assembly. However, they only account for the \ndataset when performing the optimal k -value selection. We find that this approach is unsuitable for our prob-\nlem (i.e., finding best k -value for error correction) as the optimal k -value here also depends on the correction \nalgorithm (e.g., the optimal k-values for Blue and Lighter in our evaluation vary, Table 2). Also, the user is finally \nresponsible for interpreting the visualization and selecting the optimal k-mer value. To make the above argument \nspecific, we found that k  = 25 achieves within 0.04% from the best EC Gain performance for dataset D1 when \nusing Blue. On the other hand, if the same k-value was used for D1 again but this time with the tool Lighter, the \nEC Gain drops by 26.8% from the maximum Gain (Tables 1 and 2 in Appendix).\nIn addition, there is no single k -value that works optimally for all datasets, even when using the same  EC \ntool. For example, we found that k = 25 gives the best EC Gain performance for D5 using Lighter, showing an EC \nGain of 83.8%. However, the same k-value used for D3 with Lighter gives an EC Gain of only 65% compared to a \nGain of 95.34% when using the optimal k-value (Table 1 in Appendix). Thus, there is a need for a data-driven and \ntool-specific method to select the optimal k-value.\nOur solution, Athena finds the best value of the configuration parameters for correcting errors in genome \nsequencing, such as the value of k in k-mer based methods (Just as Athena is the Greek Goddess of wisdom and \na fierce warrior, we wish our technique to unearth the genomic codes underlying disease in a fearless war against \nmaladies). Further, Athena does not require access to a reference genome to determine the optimal parameter \nconfiguration. In our evaluation, we use Bowtie2 for alignment and measure alignment rate as a metric to evaluate \nAthena. However, alignment is not needed for Athena to work as shown in Fig.  1. Athena, like other EC tools, \nleverages the fact that NGS reads have the property of reasonably high coverage, 30X–150X coverage depth is \ncommonplace. From this, it follows that the likelihood of correct overlaps for a given portion of the genome will \noutnumber the likelihood of erroneous ones\n4. Athena uses a language model (LM) to estimate the correctness of \nthe observed sequence considering the frequency of each subsequence and its fitness with respect to the context. \nThis is integral to traditional NLP tasks such as speech recognition, machine translation, or text summarization \nin which LM is a probability distribution capturing certain characteristics of a sequence of symbols and words, \nwhich in this case is specialized to the dataset and to the EC tool of choice for optimal performance.\nIn our context, we use LM to estimate the probabilistic likelihood that some observed sequence is solid or insolid, \nin the context of the specific genome. We create an LM using as training, the enire original dataset (with uncor-\nrected reads). We then run the EC algorithm on a subset of the overall data with a specific value of the configura-\ntion parameter. Subsequently, we use the trained LM at runtime to compute a metric called the “Perplexity metric” \n(which is widely used in the NLP literature). We show empirically that the perplexity metric has a strong negative \ncorrelation with the quality of the error correction (measured typically through the metric called “EC gain”).  \nFigure 1. Overview of Athena’s workflow. First, we train the language model using the entire set of uncorrected \nreads for the specific dataset. Second, we perform error correction on a subsample from the uncorrected reads \nusing an EC tool (e.g., Lighter or Blue) and a range of k-values. Third, we compute perplexity of each corrected \nsample, corrected with a specific EC tool, and decide on the best k-value for the next iteration, i.e., the one \ncorresponding to the lowest perplexity metric because EC quality is negatively correlated with the perplexity \nmetric. This process continues until the termination criteria are met. Finally, the complete set of reads is \ncorrected with the best k-value found and then used for evaluation.\n4Scientific  RepoRtS  | (2019) 9:16157 | https://doi.org/10.1038/s41598-019-52196-4\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\nCrucially, the Perplexity metric evaluation does not require the computationally expensive alignment to a refer-\nence genome, even when available. Through a stochastic optimization method, we evaluate the search space to \npick the best configuration-parameter value for the EC algorithm-k  in k-mer-based methods and the Genome \nLength (GL) in the RACER EC tool. Moreover, most EC tools are evaluated based on their direct ability to reduce \nerror rates rather than to improve genome assembly. Although in general, assembly benefits from the error cor-\nrection pre-processing step, sub-optimal error correction can reduce assembly quality due to conversion of benign \nerrors into damaging ones\n12. In our evaluation, we see that the EC improvement due to Athena also leads to \nhigher quality assembly (Table 3).\nIn summary, this paper makes the following contributions.\n 1. We compare and contrast two LM variants of Athena, N-gram and RNN-based. Through this, we show \nthat N-Gram modeling can be faster to train while char-RNN provides similar accuracy to N-gram, albeit \nwith significantly lower memory footprint, conducive to multi-tenant analysis pipelines (e.g., MG-RAST \nfor metagenomics processing\n13).\n 2. We introduce a likelihood-based metric, the Perplexity metric (repurposed from NLP), to evaluate EC \nquality without the need for a reference genome. This is the first such use of this metric in the computa-\ntional genomics domain.\n 3. We compare and contrast two LM variants of Athena, N-gram and RNN-based. Through this, we show that \nN-Gram modeling can be faster to train while char-RNN provides similar accuracy to N-gram, albeit with \nsignificantly lower memory footprint, conducive to multi-tenant analysis pipelines (e.g., MG-RAST for \nmetagenomics processing\n13).\n 4. We apply Athena to 3 k-mer based EC tools: Lighter6, Blue7, and RACER14 on 7 real datasets, with varied \nerror rates and read lengths. We show that Athena was successful in finding either the best parameters \n(k for Lighter and Blue, and GenomeLength for RACER) or parameters that perform within 0.53% of the \noverall alignment rate to the best values using exhaustive search against a reference genome. We couple EC, \nwith best parameter selection by Athena, to the Velvet genome assembler and find that it improves assem-\nbly quality (NG50) by a Geometric Mean of 4.72X across 7 evaluated datasets.\nBackground\nError correction and evaluation.  The majority of error correction tools share the following intuition: \nhigh-fidelity sequences (or, solid sequences) can be used to correct errors in low-fidelity sequences (or, in-solid \nsequences). However, they vary significantly in the way they differentiate between solid and in-solid sequences. \nFor example\n4, corrects genomic reads containing insolid k -mers using a minimum number of edit operations \nsuch that these reads contain only solid k-mers after correction. The evaluation of de novo sequencing techniques \nrely on likelihood-based metrics such as ALE 15 and CGAL16, without relying on the availability of a reference \ngenome. On the other hand, comparative sequencing or re-sequencing, such as to study structural variations \namong two genomes, do have reference genomes available.\nLanguage modeling. To increase the accuracy of detecting words in speech recognition, language modeling \ntechniques have been used to see which word combinations have higher likelihood of occurrence than others, \nthus improving context-based semantics. Thus, language modeling is being used in many applications such as \nspeech recognition\n17, text retrieval, and many NLP applications. The main task of these statistical models is to \ncapture historical information and predict the future sequences based on that information 18. Language models \nare classified into two main categories: (i) Count-based methods that represent traditional statistical models, \n— Correlation to Alignment Comparison with FIONA Assembly quality\nRuntime \nImprovement\nDataset\nCorrelation \n(N-Gram)\nCorrelation \n(RNN)\nFiona + Bowtie2 \n(Alignment Rate)\nRACER w/o \nAthena + Bowtie2 \n(Alignment Rate)\nRACER w/\nAthena + Bowtie2 \n(Alignment Rate)\nNG50 of \nVelvet \nw/o EC\nNG50 of Velvet w/\n(Racer + Athena) Athena Bowtie2\nD1 −0.977 −0.938 99.25% 85.01% 99.26% 3019 6827 (2.26X) 1 m 38 s 10 m 5 s\nD2 −0.981 −0.969 73.75% 58.66% 81.15% 47 2164 (46X) 49 s 3 m 53 s\nD3 −0.982 −0.968 83.12% 80.79% 84.11% 1042 4164 (4X) 1 m 39 s 7 m 50 s\nD4 −0.946 −0.930 95.33% 93.86% 95.33% 118 858 (7.27X) 52 s 3 m 8 s\nD5 −0.970 −0.962 92.34% 90.91% 92.29% 186 2799 (15X) 1 m 40 s 9 m 42 s\nD6 −0.944 −0.979 87.43% 85.76% 86.84% 1098 1237 (1.12X) 6 m 40 s 1 h 42 m\nD7 −0.723 −0.862 NA 17.17% 17.55% 723 754 (1.04X) 16 m 71 m\nTable 3. Comparison of Overall Alignment Rate of Fiona versus RACER (with and without Athena’s tuning). \nRACER requires the user to enter a value for the “Genome Length” , which has no default value. Therefore, \n“RACER w/o Athena” is RACER operating with a fixed Genome Length of 1M. Columns 5 & 6 demonstrate the \nstrong anti-correlation values between Perplexity and Alignment Rate. The last two columns show the assembly \nquality (in terms of NG50) before and after correction by RACER, tuned with Athena. Improvements in NG50 \nare shown between parentheses, while NGA50 and the amount of assembly errors metrics showed similar \nimprovements and hence omitted. We also show the search time comparison for estimating the perplexity \nmetric with Athena (N-gram) for a point in search space vs. estimating overall alignment rate with Bowtie2.\n5Scientific  RepoRtS  | (2019) 9:16157 | https://doi.org/10.1038/s41598-019-52196-4\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\nusually involve estimating N-gram probabilities via counting and subsequent smoothing. (ii) Continuous-space \nlanguage modeling is based on training deep learning algorithms. In recent years, continuous-space LMs such as \nfully-connected Neural Probabilistic Language Models (NPLM) and Recurrent Neural Network language models \n(RNNs) are proposed. Now we describe in detail each class of our language models.\nn-Gram-based language modeling.  This type of modeling is word-based. The main task that N-Gram \nbased models19 have been used for is to estimate the likelihood of observing a word Wi, given the set of previous \nwords W0, …Wi−1, estimated using the following equation:\n∏∏... =| ... ≈| ...\n=\n−\n=\n−−PW WW PW WW PW WW(, ,, )( ,, )( ,, )\n(1)\nm\ni\nm\nii\ni\nm\nii in01\n1\n11\n1\n1\nwhere n represents the number of history words the model uses to predict the next word. Obviously, a higher n \nresults in better prediction, at the cost of higher training time resulting from a more complex model. Also notice that \nfor this model to operate, it has to store all conditional probability values and hence has a high memory footprint.\nChar-RNN-Based language modeling. Recurrent neural network (RNN) is a very popular class of neural \nnetworks for dealing with sequential data, frequently encountered in the NLP domain. The power of RNN is that \neach neuron or unit can use its internal state memory to save information from the previous input and use that \nstate, together with the current input, to determine what the next output should be. Character-level RNN mod-\nels, char-RNN for short, operate by taking a chunk of text and modeling the probability distribution of the next \ncharacter in the sequence, given a sequence of previous characters. This then allows it to generate new text, one \ncharacter at a time\n20. RNNs consist of three main layers: Input Layer, Hidden Layer, and Output Layer. First, Input \nLayer takes xt vector, which is input at a time step t, usually a one-hot encoding vector of the tth word or character \nof the input sentence. Second, Hidden Layer consists of the hidden state at the same time step st, which represents \nthe memory of this network. It is calculated as a non-linear function f (e.g., tanh) of the previous hidden state st−1 \nand the input at current time step xt with the following relation:\n=+ .−sf Ux Ws() (2)tt t 1\nHere, W is a matrix that consists of hidden weights of this hidden layer. Finally, Output Layer consists of a vector \not, which represents the output at step t and contains prediction probabilities for the next character in the sentence. \nFormally, its length equals the size of the vocabulary and is calculated using a softmax function. Backpropagation \nwas used to train the RNN to update weights and minimize the error between the observed and the estimated next \nword. For Deep RNN architectures, there are multiple parameters that affect the performance of the model. The \ntwo main parameters are: Number of Hidden Layers and Number of Neurons per Layer. For our Char-RNN language \nmodeling, vocabulary would include the four nucleotide bases as characters A, C, G, and T. Each input is a one-hot \nencoding vector for the four nucleotides. Each output vector at each time step also has the same dimension.\nPerplexity of the language model. Perplexity is a measurement of how well a language model predicts \na sample. In NLP , perplexity is one of the most effective ways of evaluating the goodness of fit of a language \nmodel since a language model is a probability distribution over entire sentences of text\n21. For example, 5 per word \nperplexity of a model translates to the model being as confused on test data as if it had to select uniformly and \nindependently from 5 possibilities for each word. Thus, a lower perplexity indicates that language model is better \nat making predictions. For an N-Gram language model, perplexity of a sentence is the inverse probability of the \ntest set, normalized by the number of words\n21.\n=\n....\n≈\n∏ | ...= −−\nPP W\nPW WW PW WW\n() 1\n(, ,, )\n1\n(, ,) (3)m i\nm\nii in12 1 1\nmm\nIt is clear from (3) that minimizing perplexity is the same as maximizing the probability of the observed set of \nm words from W1 to Wm.\nFor RNN, perplexity is measured as the exponential of the mean of the cross-entropy loss (CE) as shown in (4)22, \nwhere ˆy is the predicted next character–the output of the RNN–and |V| is the vocabulary size used during training.\n∑=− .\n=\n||\nˆˆCE yy py py(, )( )log(( ))\n(4)i\nV\nii\n1\nAlthough these two models estimate the perplexity metric differently, they achieve the same purpose, which \nis estimating the correctness of a sequence given the trained probability distribution. In the next section, we \ndescribe how our system Athena uses these models to find the best k-value for a given tool and dataset.\nour Solution: Athena\nApplication of language models. We use two different LM variants in our Athena algorithm. We describe \nthem next.\nN-Gram language models. We train an N-Gram model19, which is word-based, from the input set of reads before \ncorrection. This N-Gram model needs word-based segmentation of the input read as a pre-processing phase. \nThen, we use this trained LM to evaluate EC performance.\n6Scientific  RepoRtS  | (2019) 9:16157 | https://doi.org/10.1038/s41598-019-52196-4\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\nRNN language models. The second technique is RNN-based LM 23, using different RNN architectures, e.g., \nstandard RNNs, LSTMs, and GRUs. These models can be trained either as word-based models or character-based \nmodels. We train our RNN variant as character-based model to avoid having to make the decision about how to \nsegment the genomic string, as we have to do for the N-Gram model.\nTraining time and memory footprint. Contrasting our 2 LM variants: Although training N-Gram LMs is much \nfaster relative to RNN-based models (3–5 minutes for N-Gram vs. 10–11 hours for RNN-based across the 7 data-\nsets), they still have the requirement of splitting a read into words of specific length. Further, RNN-based models \nhave much lower memory footprint and storage requirements relative to N-Gram. This is because N-Gram mod-\nels need to store conditional probabilities in large tables with an average size of 0.6–1.1 GB across the 7 datasets. \nIn contrast, RNNs only need to store the network architecture and weights with an average size of 3–5 MB across \nthe 7 datasets. For instance, the size of N-gram model for D7 is 2 GB while the size for the RNN model is only 3.5 \nMB. With respect to run time, N-gram takes 16 minutes for the whole dataset D7 while RNN takes 30 minutes on \na 50 K sample of D7. Also, we used an LSTM variant of Athena and found that it took 3 times longer to train and \ntest but gave insignificant improvement in perplexity over RNN.\nIntuition for the use of the perplexity metric. Our design uses the Perplexity metric to provide an accu-\nrate, and importantly, quick estimation of the EC quality with the current configuration parameter value(s). The \nPerplexity metric is based on the LM trained on the entire original (uncorrected) dataset. The Perplexity metric \nis then calculated on the dataset of the corrected reads (entire dataset for N-Gram and 1% for RNN) to measure \nthe EC performance. It measures how well LM can predict the next element in an input stream. Suppose the input \nstream is H  and the next element is e . Then, the Perplexity metric is inversely proportional to the probability \nof seeing “e” in the stream, given the history H  for the learned model. Moreover, the Perplexity metric has the \nadvantage of taking the context of the element (e.g., previous and subsequent k-mers) into consideration when \nestimating the probability of observing that element. This context-awareness feature is not considered in simple \nk-mer counting methods. This method works because we see empirically that there is a high negative correlation \nof the Perplexity metric with both EC metrics–Alignment Rate and EC Gain. Given this anti-correlation, we can \nrely on the Perplexity metric as an evaluation function, and apply a simple search technique (e.g., hill climbing) \nto find the best k-value for a given dataset. In this description, for simplicity of exposition, we use the k-value in \nk-mer based techniques as an example of Athena -tuned configuration parameter. However, Athena can tune \nany other relevant configuration parameter in EC algorithms and we experimentally show the behavior with \nanother parameter–Genome Length–in the RACER tool. Figure 2 shows an example how Perplexity can evaluate \nthe likelihood of a sequence of k-mers using their frequencies and contextual dependencies. In this example, we \nnotice that the corrected read set (i.e., on the right) has a considerably lower Perplexity value (15.2), relative to \nthe erroneous set (77.72). Thus, our intuition that the Perplexity metric reflects the correctness of the read dataset \nholds true here through the observed negative relationship.\nSearch through the parameter space.  Our objective is to find the best k -value that will minimize the \nPerplexity of the corrected dataset. The Perplexity function is denoted by f in (5).\n==kP erplexityf LM Dkargm in argm in (, ,) (5)optk ki 0ii\nHere, LM: trained language model, D 0: uncorrected read set, and k : the configuration parameter we wish \nto tune. f is a discrete function as k-values are discrete, and therefore, its derivative is not computable. Thus, a \ngradient-based optimization technique is inapplicable. Hence, we use a simple hill-climbing technique to find the \nvalue of k that gives the minimum value of f, for the given LM and D\n0 in (5).\nThe following pseudo-code describes the steps used for finding the best k-value for a given dataset. We start \nwith Algorithm 1, which invokes Algorithm 2 multiple times, each time with a different starting value. We begin \nby training an LM on the original uncorrected read set (D\n0). Second, we assume that the best value of k lies in a \nrange from A to B (initially set to either the tool’s recommended range, or between 1 and L, where L is the read \nsize).\nWe apply an existing EC algorithm (Lighter, Blue, or RACER in our evaluation) with different initial values \n…∈kk AB(, ,) (, )m0  to avoid getting stuck in local minima, going through multiple iterations for a given initial \nvalue. We evaluate the Perplexity for the corrected dataset with current values of k: ki, and its neighbors, (ki − δ \nand ki + δ). Athena takes δ as a user input. The larger values of δ allows Athena to search the k-mers space faster, \nbut it has the downside of missing good values of k*. The default value for δ is 1. Notice that using δ = 1 is not the \nsame as applying exhaustive search to all possible values of k, as with the hill-climbing technique that Athena uses, \nthe search is terminated when the current k-mer is better than its neighbours (as shown in step 2 in Algorithm 2). \nIn each iteration, we apply hill-climbing search to identify the next best value of k\ni for the following iteration. The \nalgorithm terminates whenever the Perplexity relative to ki is less than the perplexities of both its neighbors or the \nmaximum number of (user-defined) iterations is reached. However, all shown results are with respect to only one \ninitial value (i.e., m = 0 in k\n0, ki, …, km).\nTime and space complexity. Because we apply hill climbing search to find the best value of k, the worst-case time \ncomplexity of the proposed algorithm is L  × |S′|, where L is the upper bound of the range of k -values to search \nfor and |S′| is size of selected sample. For the space complexity, Athena only needs to save the Perplexity values of \npreviously investigated k-values, which is also linear in terms of L.\n7Scientific  RepoRtS  | (2019) 9:16157 | https://doi.org/10.1038/s41598-019-52196-4\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\nEvaluation with Real Datasets\nIn this section, we evaluate Athena variants separately by correcting errors in 6 real datasets and evaluating the \nquality of the resultant assembly.\nimplementation notes and dataset. We implement the N-Gram model using SRILM toolkit24. SRILM \nis an open source toolkit that supports building statistical N-Gram LMs, in addition to evaluating the likelihood \nof new sequences using the Perplexity metric. For the RNN LM implementation, we build on the TensorFlow \nplatform\n25. Specifically, we utilized Character-level RNN models, char-RNN for short, which takes a chunk of text \nand models the probability distribution of the next character in the sequence, given a sequence of previous char-\nacters. After correction, we run the Bowtie2 aligner26 and measure the Alignment Rate and the Error Correction \nGain (EC Gain). A higher value for either metric implies superior error correction. We do a sweep through a \nrange of k-values and measure the alignment rate to determine if the Athena -generated k-value is optimal or its \ndistance from optimality.\nFor interpreting the execution time results, our experiments were performed on Dell Precision T3500 \nWorkstation, with 8 CPU cores, each running at 3.2 GHZ, 12GB RAM, and Ubuntu 16.04 Operating System. We \nuse 3 EC tools, in pipeline mode with Athena, namely, Lighter, Blue, and RACER. Blue uses a k-mer consensus \nto target different kinds of errors, e.g., substitution, deletion and insertion errors, as well as uncalled bases (rep-\nresented by N). This improves the performance of both alignment and assembly\n7. In contrast, Lighter is much \nfaster as it uses only a sample of k-mers to perform correction. We use the automated subsampling factor (called \nalpha) selection option in Lighter, which is estimated based on the genome size. Third, RACER uses a different \nconfiguration parameter distinct from the k-value, specifically Genome Length (GL), and we are able to tune GL \nas well. Our ability to tune any of these EC algorithm’s parameters is in line with our vision and ongoing work to \ndesign extensible blocks of software to expedite algorithmic development in bioinformatics\n27. Incidentally, we \nstarted using another popular EC tool, Reptile, but it only allowed for a smaller range of k-values, beyond which \nit ran into out-of-memory errors. Hence, to demonstrate results with the full range of k values, we restricted our-\nselves to Lighter, Blue, and RACER. Our datasets are Illumina short reads (Table 1), used in multiple prior studies \n(e.g.4,6,28,29). For these, there exist ground-truth reference genomes, which we use to evaluate the EC quality. The \nseven datasets have different read lengths (from 36 bp to 250 bp) and different error rates (from <3% to 43%).\noptimal parameter selection. The results of using Athena with Lighter, Blue, and RACER tools are shown \nin Table 2 for each dataset. The value of k found through exhaustive testing (or, GL for RACER) along with the EC \nquality is given first. Then it is noted when the Athena -selected configuration matches this theoretical best. We \nfind that in 27 of 36 cases (75%) the configuration found by Athena matches the theoretical best. In the remaining \ncases, our chosen parameters are within 0.53% in overall alignment rate to the best values. We see that the optimal \nparameter value almost always corresponds to the lowest perplexity scores computed using Athena’s language \nmodeling (Tables 1 and 2 in Appendix). Further, the anti-correlation between perplexity and alignment rate holds \nfor both optimal and non-optimal k-values. This shows that our hypothesis is valid across a range of k-values. We \nnotice that the feasible range of k -values in Blue is (20, 32), distinct from Lighter’s. Another interesting obser -\nvation is that the optimal k-values are different across the two different EC tools, Lighter and Blue, for the same \ndataset, as observed before\n6. Athena can be applied to a different configuration parameter, GL for the RACER \ntool, in line with our design as a general-purpose tuning tool.\nn-gram language model results. We start by training an N-Gram language model from the original \ndataset. We divide each read into smaller segments (words) of length Ls (set to 7 by default). A careful reader may \nAlgorithm 1. Correct Set of Reads.\nAlgorithm 2. Find Optimal k.\n8Scientific  RepoRtS  | (2019) 9:16157 | https://doi.org/10.1038/s41598-019-52196-4\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\nbe concerned that selecting the best value of L s is a difficult problem in itself, of the same order of difficulty as \nour original problem. Fortunately, this is not the case and Ls selection turns out to be a much simpler task. From \ndomain knowledge, we know that if we use L s = 4 or less, the frequencies of the different words will be similar \nas this increases the overlap probability between the generated words, thus reducing the model’s discriminatory \npower, while a large value will mean that the model’s memory footprint increases. We find that for a standard \ndesktop-class machine with 32 GB of memory, L\ns = 8 is the maximum that can be accommodated. Further, we \nfind that the model performance is not  very sensitive in the range (5–7), so we end up using L s = 7. The same \nargument holds for selecting a history of words, and we use a tri-gram model (history of 3 words, i.e., n = 3) for \nall our experiments. Second, we compare the perplexity metric for datasets corrected with different k-values and \ncompare the perplexity metric (without a reference genome) to the alignment rate (using a reference genome). \nWe always report the average perplexity, which is just the total perplexity averaged across all words. Our results \nshow a high negative correlation between the two metrics on the 7 datasets ( ≤−0.930 for the first six datasets \nand ≤−0.723 for D7), as shown in Table 3. To reiterate, the benefit of using the perplexity metric is that it can be \ncomputed without the ground truth and even where a reference genome is available, it is more computationally \nefficient than aligning to the reference and then computing the alignment rate.\nchar-Rnn language model results. For training our RNN, we used the “tensorflow-char-rnn” library25. \nAfter parameter tuning, we use the following architecture for our experiments: 2 hidden layers with 300 neurons \nper layer, output layer with size 4 (i.e., corresponding to the four base pairs), mini-batch size 200, and learning \nrate 2e\n−3 respectively. For each of the 7 datasets, we used 90% for training and 10% for validation, with no overlap, \nfor coming up with the optimal RNN architecture.\nFor our char-RNN results, we find that the perplexity metric has a strong negative relation to the overall \nalignment rate (Table 3), with the absolute value of the correlation always greater than 0.86. Here, we have to \nsample the corrected set for calculating the perplexity measure because using an RNN to perform the calculation \nis expensive. This is because it involves, for predicting each character, doing 4 feed-forward passes (correspond-\ning to the one-hot encodings for A, T, G, or C), each through 600 neurons. Empirically, for a test sample size of \n50 K, this translates to approximately 30 minutes on a desktop-class machine. In the experiments with the real \ndatasets, we use 50 K samples with uniform sampling, and in synthetic experiments, we use 100 K samples (i.e., \nonly 1% of the dataset). Importantly, the strong quantitative relationship between perplexity and the EC quality \nis maintained even at the low sampling rate (0.5% for real dataset). Note that this test sample size is different from \nthe sample size used by the EC tool to perform the correction, shown in Fig. 1, which must be at least of coverage \n30X to ensure accurate k-mer analysis\n30.\ncomparison with a self-tuning ec tool.  Here, we compare Athena with the EC tool, Fiona 31, which \nestimates its parameters internally. The purpose of this comparison is to show that Athena can tune k-mer-based \napproaches (RACER specifically for this experiment) to achieve comparable performance to suffix array-based \napproaches (e.g., Fiona), reducing the gap between the two approaches.\nThe works by\n10 and32 show a similar comparison between different EC approaches concluding that the auto-\nmatic selection of configuration parameters, based on the datasets, is crucial for EC performance. However, they \ndo not perform such parameter tuning automatically. Table 3 presents the overall alignment rate for our 6 eval-\nuation datasets, calculated after doing correction by Fiona. We notice that RACER, when tuned with Athena, \noutperforms automatic tuning by Fiona in 3 of the 7 datasets (i.e., by about 0.01%, 7.4% and 1% on D1, D2, and \nD3 respectively), while they are equal in one dataset. Finally, Fiona is better on D5 by 0.05% and on D6 by 0.59%. \nNotice that Racer’s runtime is 5–6X faster compared to Fiona, which is similar to the runtimes reported in\n33. \nMoreover, we omit the result of Fiona with D7 as the tool took more than 6 hours without producing the corrected \nreads.\nFigure 2. An example showing how the perplexity metric encodes errors in genomic reads. The read on the \nleft is an erroneous read selected from dataset D3, while the read on the right is the same read, after correction \nwith Lighter. When using language modeling to compute the perplexity for both reads, we notice that the read \non the right has a lower perplexity value (15.2), relative to the erroneous read (77.72), as the sequence of k-mers \nafter correction has a higher probability of occurrence. Also notice that the probability of a sequence of k-mers \ndepends on both their frequencies and their relative order in the read, which allows the perplexity metric to \ncapture how likely it is to observe this k-mer given the neighboring k-mers in a given read.\n9Scientific  RepoRtS  | (2019) 9:16157 | https://doi.org/10.1038/s41598-019-52196-4\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\nImpact on assembly quality and searching time. Here we show the impact on genome assembly qual-\nity of using an EC tool tuned with Athena. We use Velvet34 to perform the assembly and QUAST35 to evaluate the \nassembly quality. We compare the NG50 before and after correction done by RACER using the best GL found \nby Athena. The results (Table 3) show a significant improvement on NG50 by 2.26X, 46X, 4X, 7.27X, 15X, 1.2X, \nand 1.04X respectively. For D7, the improvement is the lowest, since D7 has the lowest alignment rate across all \ndatasets. We also collect the NGA50 scores and it shows identical improvements as the NG50. These improve-\nments are consistent with what was reported in7,12, which measured improvement due to the use of EC tools with \nmanually tuned configuration parameters.\nSearch time improvement with Athena. Consider that in our problem statement, we are trying to search through \na space of configuration parameters in order to optimize a metric (EC Gain or Alignment Rate). The search space \ncan be large and since the cost of searching shows up as a runtime delay, it is important to reduce the time that it \ntakes to evaluate that metric of each search point. Although EC tools don’t need a reference genome to operate, \ncurrent state-of-the-art method use a reference genome to tune EC performance. As shown in\n1,11, the best value \nof configuration parameter k  is found by iteratively picking a single k -value, run the EC tool with that value, \nthen perform alignment (with one of several available tools such as Bowtie2), and finally compute the metric for \nthat value. In contrast, with Athena, to explore one point in the search space, we run the EC algorithm with the \nk-value, and then compute the Perplexity metric, which does not  involve the time-consuming alignment step. \nHere, we evaluate the relative time spent in exploring one point in the search space using Athena vis-à -vis the \ncurrent state-of-the-art. The result is shown in Table 3. For this comparison, the alignment is done by Bowtie2\n26. \nWe find that using the baseline approach, each step in the search takes respectively 6.2X, 4.8X, and 4.7X, 3.6X, \n5.82X, 15.3X, and 4.4X longer for the 7 datasets. That is because the time taken by the LM to calculate the per -\nplexity is linear in terms of the input (number of reads x read length), while the runtimes of alignment algorithms \nare superlinear\n36.\nFurther, while we use the hill-climbing technique to search through the space, today’s baseline methods use \nexhaustive search, such as in Lighter6 and thus the end-to-end runtime advantage of Athena will be magnified.\nimpact of sub-sampling. One drawback of sub-sampling is the reduction of coverage, which can negatively \nimpact the accuracy of k-mer analysis. Therefore, we select the sub-sampling ratio so that the sample size has a \ncoverage of at least 30X, which was found sufficient by several prior works (such as\n30) for accurate k-mer analysis. \nWhen the coverage is less than 30X, the distribution of solid k-mers vs. non-solid k-mers becomes very close, for \nwhich EC tools (e.g., Lighter with dataset D2) will not perform any correction for any given value of k. So it is a \nlimitation in EC tools in general to require higher coverage for accurate error correction. For datasets with cover-\nage less than 30X, we use the complete dataset without any subsampling.\nWe use the well-known Lander-Waterman’s formula37 to estimate the required sample-size to reach such cov-\nerage. If the data coverage is less than 30X, the whole dataset is used.\nWe also notice from Table 2 that Athena sometimes proposes slightly sub-optimal configurations compared \nto oracle best (within 0.53% in terms of overall alignment rate). This is due to the impact of sub-sampling as the \nbest configuration for the sample can be slightly different from the best configuration for the complete dataset. We \nstudy the impact of sub-sampling on the accuracy of Athena. We use D2 for this experiment as it is the one with \nthe highest error rate across all 7 datasets. First, we select two random samples from D2 with sampling rates of \n35% (30X) and 70% (50X) respectively and investigate the correlation between perplexity and EC gain. As shown \nin Fig. 3, perplexity and EC gain have a very high inverse correlation, for all three tools, across the entire range \nof k-values. This shows that sampling in Athena has the advantage of significant improvements in runtime, while \npreserving the accuracy of optimal parameter selection.\nSecond, we select a random sample from D2 with a sampling rate of 40%. We scan over the range of values of \nthe tuning parameter (k-mer size for Lighter and Blue, Genome length for RACER) and calculate perplexity and \nEC gain for each value. We repeat this experiment with 70 different random samples and find that the selected \nvalue did not change in all runs, which also matches the best value with exhaustive searching over the complete \ndataset. This shows that the sub-sampling does not overfit the data and preserves the accuracy of the optimal \nparameter selection.\nFigure 3. Impact of sub-sampling on perplexity and gain. We compare the perplexity and gain with samples \nof sizes 35% and 70% of the D2 dataset. We observe the negative correlation between both metrics and also the \npositive correlation between the values of each metric on the two samples.\n10Scientific  RepoRtS  | (2019) 9:16157 | https://doi.org/10.1038/s41598-019-52196-4\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\nEvaluation with Synthetically Injected Errors\nHere we experiment on datasets where we synthetically inject errors of three kinds - insertion, deletion, and \nsubstitution. The real data sets used in our evaluation in Section belonged to Illumina sequencing platform and \ntherefore had primarily substitution errors (about 99% of all errors). However, other platforms such as 454 or Ion \ntorrent sequencing suffers primarily from insertions and deletions\n10. Hence our synthetic injections are meant \nto uncover if the relationship between perplexity and error rate holds for these other error types. We start by \nrandomly collecting 100 K short reads from the reference genome (i.e., almost error-free) for two organisms used \nin the real datasets–E. coli (D1, D2) and Acinetobacter (D3). Afterward, we inject errors of each type as follows:\n 1. Deletion: We select the index of injection (position of the deleted segment), which follows a uniform \ndistribution U(0, L − d), where L is the length of the read and d is the length to delete.\n 2. Insertion: Similar to deletion errors, the index of insertion follows a uniform distribution U(0, L − I), \nwhere I is the length of inserted segment. Moreover, each base pair in the inserted segment follows a uni-\nform distribution over the four base pairs (A, C, G, or T).\n 3. Substitution: For this type of synthetic errors, we select k positions of substitutions following a uniform \ndistribution U(0, L). Then, we substitute each base pair of the k positions with another base pair following a \nuniform distribution over the four base pairs (A, C, G, or T). Note that there is one-in-four chance that the \nsubstitution results in the same base pair.\nWe test the proposed approach against two levels of synthetic errors: low error rate (Uniform in (1, 5) bp in \nerror per read), and high error rate (Uniform in (6, 10) bp in error per read). In all cases, the language model is \ntrained using the original data set, mentioned in Table 1, with the natural ambient rate of error. Figure 4 shows \nthe results for the perplexity metric in the data set after  error injection (i.e., without doing correction), for the \nthree error types and an equi-probable mixture of the three. The results show that for all types of errors, the direct \nquantitative relation between error rate and perplexity holds–the perplexity values are higher for high error rate \ninjection. One observation is that the values of perplexity for insertion and substitution errors are higher than \nfor deletion errors. For example, insertion and substitution perplexities are 3X & 4.7X the deletion perplexity for \nhigh error rate data sets, and 2X & 1.5X for low error rate data sets. This is expected because in deletion errors, the \nsegment of the read after the index of deletion is a correct segment and hence is expected by the language model. \nOn the other hand, for insertion and substitutions, the added synthetic base pairs will most likely construct wrong \nsegments. Such segments will have very low counts in the language model and therefore produce higher overall \nperplexities. Another observation is that the perplexities for the injection into data set D1 are higher than for D3. \nThis is likely because D3 had a higher natural ambient rate of error and hence the additional injection does not \ncause as much increase in the perplexity metric.\nRelated Work\nerror correction approaches. EC tools can be mainly divided into three categories: k -spectrum based, \nsuffix tree/array-based, and multiple sequence alignment-based (MSA) methods. Each tool takes one or more \nconfiguration parameters. While we have experimented with Athena applied to the first kind, with some engi-\nneering effort, it can be applied to tuning tools that belong to the other two categories.\nFigure 4. N-Gram (Figure A,B) and RNN (Figure C,D) Perplexity metric for different types of synthetic errors: \nIndels and Substitution errors, and a mixture of the three for E. coli str reference genome (Figure A,C) and \nAcinetobacter sp. reference genome (Figure B,D). We compare two versions of such errors: high and low error \nrates.\n11Scientific  RepoRtS  | (2019) 9:16157 | https://doi.org/10.1038/s41598-019-52196-4\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\nLanguage modeling in genomics. In the genomics domain, LM was used in38 to find the characteristics \nof organisms in which N-Gram analysis was applied to 44 different bacterial and archaeal genomes and to the \nhuman genome. In subsequent work, they used N-Gram-based LM for extracting patterns from whole genome \nsequences. Others\n39 have used LM to enhance domain recognition in protein sequences. For example40, has used \nN-Gram analysis specifically to create a Bayesian classifier to predict the localization of a protein sequence over \n10 distinct eukaryotic organisms. RNNs can be thought of as a generalization of Hidden Markov Models (HMMs) \nand HMMs have been applied in several studies that seek to annotate epigenomic data. For example\n41, presents \na fast method using spectral learning with HMMs for annotating chromatin states in the human genome. Thus, \nwe are seeing a steady rise in the use of ML techniques, traditionally used in NLP , being used to make sense of \n-omics data.\nAutomatic parameter tuning.  used a Feature-based Accuracy Estimator as a parameter advisor for the \nOpal aligner software42,43. The field of computer systems has had several successful solutions for automatic con-\nfiguration tuning of complex software systems. Our own work 44 plus others 45 have shown how to do this for \ndistributed databases, while other works have done this for distributed computing frameworks like Hadoop46,47 \nor cloud configurations48. We take inspiration from them but our constraints and requirements are different (such \nas, avoiding reliance on ground truth corrected sequences).\nDiscussion\nThe space to search for finding the optimal configuration is non-convex in general. Therefore, it is possible to \nget stuck in a local minima, and hence, we use multiple random initializations. However, in our evaluation, we \nfind that a single initialization suffices. Some EC tools have a number of performance-sensitive configuration \nparameters with interdependencies. There, systems such as Rafiki\n44 can encode the dependencies, while relying \non Athena ‘s LM to compute the corresponding performance metric, converging toward optimal parameters. \nWith some engineering effort, Athena can be used to optimize the k -value in DBG-based assemblers as well, \nthough there will be different optimization passes since the optimal values are likely to be different for the error \ncorrection and assembly stages.\nFinally, a careful reader may wonder if we can use copy number of all solid k-mers instead of the perplexity \nmetric. The problem with this approach is that it will require a predefined frequency threshold to identify the \nsolid k-mers. Using the perplexity metric, there is no need for such a threshold. Also the perplexity metric takes \ninto account the context of the k-mer (i.e., previous and subsequent k-mers) in deciding the accuracy of the EC \ntool output. Also, notice that our main target is to find the optimal k -mer size, and different k -mers will have \ndifferent thresholds as well.\nconclusion\nThe performance of most EC tools for NGS reads is highly dependent on the proper choice of its configuration \nparameters, e.g., k-value selection in k-mer based techniques. It is computationally expensive to search through \nthe entire range of parameters to determine the optimal value, which varies from one dataset to another. Using \nour Athena suite, we target the problem of automatically tuning these parameters using language modeling tech-\nniques from the NLP domain without  the need for a ground truth genome. Through N-Gram and char-RNN \nlanguage modeling, we compute the “perplexity” metric, a novel one for this problem domain, and find that the \nmetric has a high negative correlation with the quality of genomic assembly and can be computed efficiently \nwithout using a reference genome. The perplexity metric then guides a hill climbing-based search toward the best \nk-value. We evaluate Athena with 6 different real datasets, plus with synthetically injected errors. We find that the \npredictive performance of the perplexity metric is maintained under all scenarios. Further, using the perplexity \nmetric, Athena can search for and arrive at the best k -value, or within 0.53% of the assembly quality obtained \nusing brute force. Athena suggests a k-value within the top-3 best k-values for N-Gram models and the top-5 best \nk-values for RNN models (the top values were determined by brute force searching).\nReceived: 1 April 2019; Accepted: 7 October 2019;\nPublished online: 06 November 2019\nReferences\n 1. Mahadik, K., Wright, C., Kulkarni, M., Bagchi, S. & Chaterji, S. Scalable genomic assembly through parallel de bruijn graph \nconstruction for multiple k-mers. In Proceedings of the 8th ACM International Conference on Bioinformatics, Computational Biology, \nand Health Informatics, 425–431 (ACM, 2017).\n 2. Szalay, T. & Golovchenko, J. A. De novo sequencing and variant calling with nanopores using poreseq. Nat. biotechnology 33, 1087 \n(2015).\n 3. Sameith, K., Roscito, J. G. & Hiller, M. Iterative error correction of long sequencing reads maximizes accuracy and improves contig \nassembly. Briefings bioinformatics bbw003 (2016).\n 4. Y ang, X., Dorman, K. S. & Aluru, S. Reptile: representative tiling for short read error correction. Bioinforma. 26, 2526–2533 (2010).\n 5. Kelley, D. R., Schatz, M. C. & Salzberg, S. L. Quake: quality-aware detection and correction of sequencing errors. Genome biology 11, \nR116 (2010).\n 6. Song, L., Florea, L. & Langmead, B. Lighter: fast and memory-efficient sequencing error correction without counting. Genome \nbiology 15, 509 (2014).\n 7. Greenfield, P ., Duesing, K., Papanicolaou, A. & Bauer, D. C. Blue: correcting sequencing errors using consensus and context. \nBioinforma. 30, 2723–2732 (2014).\n 8. Kao, W .-C., Chan, A. H. & Song, Y . S. Echo: a reference-free short-read error correction algorithm. Genome research (2011).\n 9. Peng, Y ., Leung, H. C., Yiu, S.-M. & Chin, F . Y . Idba–a practical iterative de bruijn graph de novo assembler. In RECOMB, 426–440 \n(2010).\n12Scientific  RepoRtS  | (2019) 9:16157 | https://doi.org/10.1038/s41598-019-52196-4\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\n 10. Y ang, X., Chockalingam, S. P . & Aluru, S. A survey of error-correction methods for next-generation sequencing. Briefings \nbioinformatics 14, 56–66 (2012).\n 11. Chikhi, R. & Medvedev, P . Informed and automated k-mer size selection for genome assembly. Bioinforma. 30, 31–37 (2013).\n 12. Heydari, M., Miclotte, G., Demeester, P ., Van de Peer, Y . & Fostier, J. Evaluation of the impact of illumina error correction tools on \nde novo genome assembly. BMC bioinformatics 18, 374 (2017).\n 13. Meyer, F. et al. Mg-rast version 4—lessons learned from a decade of low-budget ultra-high-throughput metagenome analysis. \nBriefings Bioinforma. 105 (2017).\n 14. Ilie, L. & Molnar, M. Racer: Rapid and accurate correction of errors in reads. Bioinforma. 29, 2490–2493 (2013).\n 15. Clark, S. C., Egan, R., Frazier, P . I. & Wang, Z. Ale: a generic assembly likelihood evaluation framework for assessing the accuracy of \ngenome and metagenome assemblies. Bioinforma. 29, 435–443 (2013).\n 16. Fabri, A. & Teillaud, M. Cgal-the computational geometry algorithms library. In 10e colloque national en calcul des structures, 6 \n(2011).\n 17. Elaraby, M. S., Abdallah, M., Abdou, S. & Rashwan, M. A deep neural networks (dnn) based models for a computer aided \npronunciation learning system. In International Conference on Speech and Computer, 51–58 (Springer, 2016).\n 18. Zhai, C. Statistical language models for information retrieval. Synth. Lect. on Hum. Lang. Technol. 1, 1–141 (2008).\n 19. Brown, P . F ., Desouza, P . V ., Mercer, R. L., Pietra, V . J. D. & Lai, J. C. Class-based n-gram models of natural language. Comput. \nlinguistics 18, 467–479 (1992).\n 20. Graves, A. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850 (2013).\n 21. Azzopardi, L., Girolami, M. & Van Rijsbergen, K. Investigating the relationship between language model perplexity and ir precision-\nrecall measures. (2003).\n 22. Inan, H., Khosravi, K. & Socher, R. Tying word vectors and word classifiers: A loss framework for language modeling. arXivpreprint \narXiv:1611.01462 (2016).\n 23. Kombrink, S., Mikolov, T., Karafiát, M. & Burget, L. Recurrent neural network based language modeling in meeting recognition. In \nTwelfth annual conference of the international speech communication association (2011).\n 24. Stolcke, A. Srilm – an extensible language modeling toolkit. In ICSLP, 901–904 (2002).\n 25. Abadi, M. et al. Tensorflow: A system for large-scale machine learning. In 12th USENIX Symposium on Operating Systems Design and \nImplementation (OSDI 16), 265–283 (2016).\n 26. Langmead, B. & Salzberg, S. L. Fast gapped-read alignment with bowtie 2. Nat. methods 9, 357–359 (2012).\n 27. Mahadik, K. et al. Sarvavid: a domain specific language for developing scalable computational genomics applications. In Proceedings \nof the 2016 International Conference on Supercomputing, 34 (ACM, 2016).\n 28. Schröder, J., Schröder, H., Puglisi, S. J., Sinha, R. & Schmidt, B. Shrec: a short-read error correction method. Bioinforma.  25, \n2157–2163, https://doi.org/10.1093/bioinformatics/btp379 (2009).\n 29. Guo, H. et al. degsm: memory scalable construction of large scale de bruijn graph. bioRxiv 388454 (2018).\n 30. Liu, B. et al. Estimation of genomic characteristics by analyzing k-mer frequency in de novo genome projects. arXivpreprint \narXiv:1308.2012 (2013).\n 31. Schulz, M. H. et al. Fiona: a parallel and automatic strategy for read error correction. Bioinforma. 30, i356–i363 (2014).\n 32. Molnar, M. & Ilie, L. Correcting illumina data. Briefings bioinformatics 16, 588–599 (2014).\n 33. Allam, A., Kalnis, P . & Solovyev, V . Karect: accurate correction of substitution, insertion and deletion errors for next-generation \nsequencing data. Bioinforma. 31, 3421–3428 (2015).\n 34. Zerbino, D. & Birney, E. Velvet: algorithms for de novo short read assembly using de bruijn graphs. Genome research gr–074492 \n(2008).\n 35. Gurevich, A., Saveliev, V ., Vyahhi, N. & Tesler, G. Quast: quality assessment tool for genome assemblies. Bioinforma. 29, 1072–1075 \n(2013).\n 36. Baichoo, S. & Ouzounis, C. A. Computational complexity of algorithms for sequence comparison, short-read assembly and genome \nalignment. Biosyst. 156, 72–85 (2017).\n 37. Lander, E. S. & Waterman, M. S. Genomic mapping by fingerprinting random clones: a mathematical analysis. Genomics 2, 231–239 \n(1988).\n 38. Ganapathiraju, M. K. et al. Comparative n-gram analysis of whole-genome sequences. 2nd Int. Conf. on Hum. Lang. Technol. Res. \n(HLT) 76–81 (2002).\n 39. Coin, L., Bateman, A. & Durbin, R. Enhanced protein domain discovery by using language modeling techniques from speech \nrecognition. Proc. Natl. Acad. Sci. 100, 4516–4520 (2003).\n 40. King, B. R. & Guda, C. ngloc: an n-gram-based bayesian method for estimating the subcellular proteomes of eukaryotes. Genome \nbiology 8, R68 (2007).\n 41. Song, J. & Chen, K. C. Spectacle: fast chromatin state annotation using spectral learning. Genome biology 16, 1–18 (2015).\n 42. DeBlasio, D. & Kececioglu, J. Parameter advising for multiple sequence alignment. In BMC bioinformatics , vol. 16, A3 (BioMed \nCentral, 2015).\n 43. Wheeler, T. J. & Kececioglu, J. D. Multiple alignment by aligning alignments. Bioinforma. 23, i559–i568 (2007).\n 44. Mahgoub, A. et al. Rafiki: a middleware for parameter tuning of nosql datastores for dynamic metagenomics workloads. In \nProceedings of the 18th ACM/IFIP/USENIX Middleware Conference, 28–40 (ACM, 2017).\n 45. Van Aken, D., Pavlo, A., Gordon, G. J. & Zhang, B. Automatic database management system tuning through large-scale machine \nlearning. In Proceedings of the 2017 ACM International Conference on Management of Data (SIGMOD), 1009–1024 (ACM, 2017).\n 46. Bei, Z. et al . RFHOC: A Random-Forest Approach to Auto-Tuning Hadoop’s Configuration. IEEE Transactions on Parallel \nDistributed Syst. 27, 1470–1483 (2016).\n 47. Li, M. et al. Mronline: Mapreduce online performance tuning. In Proceedings of the 23rd international symposium on High-\nperformance parallel and distributed computing, 165–176 (ACM, 2014).\n 48. Alipourfard, O. et al. Cherrypick: Adaptively unearthing the best cloud configurations for big data analytics. NSDI 2, 4–2 (2017).\n 49. Illumina. Estimating Sequencing Coverage.\nAcknowledgements\nThis work is supported in part by the NIH R01 Grant 1R01AI123037, a Lilly Endowment grant, a gift from Adobe \nResearch, and funding from Purdue’s College of Engineering and Department of Ag. and Biological Engineering. \nAny opinions, findings, and conclusions or recommendations expressed in this paper are those of the authors and \ndo not necessarily reflect the views of the funding agencies.\nAuthor contributions\nM.A., A.M., H.A., and S.C. participated in the computational analyses. M.A., A.M. and S.C. wrote the manuscript. \nS.C. provided overall guidance and funding for the project. All authors read and edited the final manuscript.\n13Scientific  RepoRtS  | (2019) 9:16157 | https://doi.org/10.1038/s41598-019-52196-4\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\ncompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary information is available for this paper at https://doi.org/10.1038/s41598-019-52196-4.\nCorrespondence and requests for materials should be addressed to S.C.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Cre-\native Commons license, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons license and your intended use is not per-\nmitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the \ncopyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.\n \n© The Author(s) 2019",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.8879052400588989
    },
    {
      "name": "Computer science",
      "score": 0.7828549742698669
    },
    {
      "name": "Language model",
      "score": 0.6998889446258545
    },
    {
      "name": "Metric (unit)",
      "score": 0.5303880572319031
    },
    {
      "name": "Algorithm",
      "score": 0.48464447259902954
    },
    {
      "name": "Ground truth",
      "score": 0.4685951769351959
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46012258529663086
    },
    {
      "name": "Computation",
      "score": 0.44651710987091064
    },
    {
      "name": "Error detection and correction",
      "score": 0.4197942912578583
    },
    {
      "name": "Machine learning",
      "score": 0.325168251991272
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I219193219",
      "name": "Purdue University West Lafayette",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I145487455",
      "name": "Cairo University",
      "country": "EG"
    }
  ],
  "cited_by": 9
}