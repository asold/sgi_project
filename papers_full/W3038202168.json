{
  "title": "Relevance Transformer: Generating Concise Code Snippets with Relevance Feedback",
  "url": "https://openalex.org/W3038202168",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5013122602",
      "name": "Carlos Gemmell",
      "affiliations": [
        "University of Glasgow"
      ]
    },
    {
      "id": "https://openalex.org/A5085689170",
      "name": "Federico Rossetto",
      "affiliations": [
        "University of Glasgow"
      ]
    },
    {
      "id": "https://openalex.org/A5071842569",
      "name": "Jeff Dalton",
      "affiliations": [
        "University of Glasgow"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2162059093",
    "https://openalex.org/W2044157185",
    "https://openalex.org/W1767022565",
    "https://openalex.org/W2169213601",
    "https://openalex.org/W2248414903",
    "https://openalex.org/W2123570619",
    "https://openalex.org/W2964315653",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2949734169",
    "https://openalex.org/W2963829526",
    "https://openalex.org/W2890397703",
    "https://openalex.org/W2949888546",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2962728167",
    "https://openalex.org/W2507756961",
    "https://openalex.org/W2304240348",
    "https://openalex.org/W2242083635",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2951352467",
    "https://openalex.org/W2795933031",
    "https://openalex.org/W2952913664",
    "https://openalex.org/W2889467844",
    "https://openalex.org/W2964325845",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "Tools capable of automatic code generation have the potential to augment programmer's capabilities. While straightforward code retrieval is incorporated into many IDEs, an emerging area is explicit code generation. Code generation is currently approached as a Machine Translation task, with Recurrent Neural Network (RNN) based encoder-decoder architectures trained on code-description pairs. In this work we introduce and study modern Transformer architectures for this task. We further propose a new model called the Relevance Transformer that incorporates external knowledge using pseudo-relevance feedback. The Relevance Transformer biases the decoding process to be similar to existing retrieved code while enforcing diversity. We perform experiments on multiple standard benchmark datasets for code generation including Django, Hearthstone, and CoNaLa. The results show improvements over state-of-the-art methods based on BLEU evaluation. The Relevance Transformer model shows the potential of Transformer-based architectures for code generation and introduces a method of incorporating pseudo-relevance feedback during inference.",
  "full_text": "arXiv:2007.02609v2  [cs.CL]  8 Dec 2020\nRelevance Transformer: Generating Concise Code Snippets\nwith Relevance Feedback\nCarlos Gemmell, Federico Rossetto, and Jeﬀrey Dalton\nUniversity of Glasgow, Scotland, UK\n{carlos.gemmell,federico.rossetto,jeﬀ.dalton}@glasgow.ac.uk\nABSTRACT\nTools capable of automatic code generation have the potenti al to\naugment programmer’s capabilities. While straightforwar d code\nretrieval is incorporated into many IDEs, an emerging area i s ex-\nplicit code generation. Code generation is currently appro ached as\na Machine Translation task, with Recurrent Neural Network ( RNN)\nbased encoder-decoder architectures trained on code-desc ription\npairs. In this work we introduce and study modern Transforme r ar-\nchitectures for this task. We further propose a new model cal led the\nRelevance Transformer that incorporates external knowled ge us-\ning pseudo-relevance feedback. The Relevance Transformer biases\nthe decoding process to be similar to existing retrieved cod e while\nenforcing diversity. We perform experiments on multiple st andard\nbenchmark datasets for code generation including Django, H earth-\nstone, and CoNaLa. The results show improvements over state -of-\nthe-art methods based on BLEU evaluation. The Relevance Tra ns-\nformer model shows the potential of Transformer-based arch itec-\ntures for code generation and introduces a method of incorpo rat-\ning pseudo-relevance feedback during inference.\nCCS CONCEPTS\n• Information systems → Information retrieval;\nKEYWORDS\nCode Generation, Code Retrieval, Neural Machine Translati on\nACM Reference Format:\nCarlos Gemmell, Federico Rossetto, and Jeﬀrey Dalton. 2020. Re levance\nTransformer: Generating Concise Code Snippets with Relevance Feedbac k.\nIn Proceedings of the 43rd International ACM SIGIR Conference on Research\nand Development in Information Retrieval (SIGIR ’20), July 25–30, 2020, China.\nACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/33 97271.3401215\n1 INTRODUCTION\nTo eﬀectively write code a programmer requires parallel kno wl-\nedge of many diﬀerent programming languages, libraries, an d tech-\nniques. The sheer amount of structured information require d is of-\nten too much to memorize, resulting in frequent online searc hes\nfor library examples or syntax clariﬁcations. This lengthe ns the\ndevelopment process and reduces productivity.\nPermission to make digital or hard copies of all or part of thi s work for personal or\nclassroom use is granted without fee provided that copies ar e not made or distributed\nfor proﬁt or commercial advantage and that copies bear this n otice and the full cita-\ntion on the ﬁrst page. Copyrights for components of this work owned by others than\nACM must be honored. Abstracting with credit is permitted. T o copy otherwise, or re-\npublish, to post on servers or to redistribute to lists, requ ires prior speciﬁc permission\nand/or a fee. Request permissions from permissions@acm.or g.\nSIGIR ’20, July 25–30, 2020, China\n© 2020 Association for Computing Machinery.\nACM ISBN 978-1-4503-8016-4/20/07. . . $15.00\nhttps://doi.org/10.1145/3397271.3401215\nDescription:\n<sos> get the first object from a queryset\nin django model /grave.ts1Entry /grave.ts1<eos>\nCode ground truth:\n<sos> Entry . objects . filter ( ) [ : 1 ] . get ( ) <eos>\nModel current decoding sequence:\n<sos> Entry .\nRelevant words:\n[/quotesingle.Varfilter/quotesingle.Var, /quotesingle.Varobjects/quotesingle.Var, /quotesingle.Varid/quotesingle.Var, /quotesingle.Varauthor__id/quotesingle.Var, /quotesingle.VarBook/quotesingle.Var, /quotesingle.Varpk/quotesingle.Var,\n/quotesingle.Var*/quotesingle.Var, /quotesingle.VarSample/quotesingle.Var, /quotesingle.VarEntry/quotesingle.Var, /quotesingle.Varname/quotesingle.Var, /quotedbl.Var/quotesingle.Varname/quotesingle.Var/quotedbl.Var, /quotesingle.Vartitle/quotesingle.Var,\n/quotedbl.Var/quotesingle.Vartitle/quotesingle.Var/quotedbl.Var, /quotesingle.Varexists/quotesingle.Var, /quotesingle.Var-/quotesingle.Var]\nNext token prediction:\nPredicted /quotesingle.Varobjects/quotesingle.Var over /quotesingle.Vargroupby/quotesingle.Var\nFigure 1: Generation sample from the Relevance Trans-\nformer on the Django dataset. The sample shows a sentence\nunder construction and the token to be produced at the next\ntime step.\nWhile code retrieval [9] is a helpful feature in many IDEs, it is\noften inﬂexible to the varying demands of a programmer and ha s\ntrouble adapting to context. Code generation seeks to solve these\nproblems by allowing the programmer to express their ideas i n\nnatural language and have the code be generated via an algori thm.\nIn doing so, the programmer can focus on higher-level tasks.\nCurrent work in Neural Machine Translation (NMT) systems\nrelated to code generation use RNN-based encoder-decoder m od-\nels, often Long Short-Term Memory (LSTM) networks. While RN N-\nbased models are useful in many translation tasks [1, 14], ne wer\nmodels such as Transformer [15] show signiﬁcant advances in NMT\ndue to their self-attentive architectures. However, the pr oblem with\nall these architectures is their inability incorporate ext ernal knowl-\nedge.\nTo our knowledge, we are the ﬁrst to use Transformer-based ar -\nchitectures for the task of code generation. We propose Rele vance\nTransformer, a new model that incorporates pseudo-relevan ce feed-\nback for translation during the decoding phase. Following m ethods\nfrom Lavrenko and Croft [7], we induce a positive bias on auto re-\ngressive generation improving decoding quality. This bias is pro-\nduced by retrieving relevant code snippets to the English de scrip-\ntion and extracting common tokens proportional to their rel evance\nfor the model. Results on standard benchmark collections sh ow\nconsistent gains over both retrieval and generation baseli nes, in-\ncluding signiﬁcant gains on the realistic CoNaLa dataset [1 7] based\non Stack Overﬂow questions.\n2 RELATED WORK\nRetrieval models are well established in the ﬁeld of code imp rove-\nment. Many attempts emphasize helping programmers debug pr o-\ngrams and remove duplicate code by identifying close matche s in\nsource code. Early approaches [6] rely on highly structured for-\nmal methods to convert queries into a structured query langu age\nto search for exact matches. Mishne et al. [9] propose a code s nip-\npet retrieval method by forming unstructured queries over s ource\ncode and use a \"fuzzy\" matching approach to help programmers\nﬁnd similar snippets to their query. These approaches attem pt to\nsearch the code to ﬁnd relevant results. Sindhgatta [13] emp loys a\ndiﬀerent approach by querying over code authors’ annotatio ns to\nretrieve relevant code snippets. This last approach is most similar\nto our retrieval model.\nMost recent work treats code generation as a Machine Transla -\ntion task and applies translation models, such as encoder-d ecoder\nnetworks [14]. These sequence to sequence (Seq2Seq) models al-\nlow for variable-length input and output. While Seq2Seq mod els\nprovide a strong baseline, Ling et al. [8] propose a latent pr edictor\nnetwork which allows selective copying of input tokens rele vant\nto the output sequence by selecting diﬀerent predictors. La ter net-\nworks incorporate structural information from code as ASTs [4, 11].\nThese models use code speciﬁc actions and build the target co de\nby specifying a sequence of rules to construct the tree.\nOther work focuses on maintaining the token representation by\nenhancing their input with retrieved snippets of code. Hash imoto\net al. [3] use a two-stage training method by retrieving simi lar snip-\npets of code and then using these snippets as input to a Seq2Se q\nmodel. The retrieval algorithm solely takes an English desc ription\nand is trained using an oracle to produce a ranking that retur ns\nthe pairs with most similar code to the desired output. This p ro-\ncess adds context to support the decoder in producing the tar get\ncode.\nWhile the ﬁeld of cross-lingual information retrieval empl oys\ntranslation dictionaries [5] and Statistical Machine Tran slation [2]\nto improve eﬀectiveness, the inverse problem is seldom appr oached.\nZhang et al. [18] use retrieved translation chunks to boost t he prob-\nability of decoding certain tokens. While this decoding pro cess\nis similar to ours, they employ an alignment dictionary to br ing\nin external knowledge and don’t normalize their increments with\nrespect to the retrieved documents. In contrast, we don’t re quire\nany structured knowledge relying only on documents found in the\ntraining set.\n3 GENERATION AND RETRIEVAL METHODS\n3.1 Task Deﬁnition\nWe deﬁne the task of code generation from natural language as :\ngiven a query description, /u1D45E, the goal is to generate a single most\nrelevant snippet of code, /u1D450, that satisﬁes the query.\nTo perform this task we formulate as follows:\nInput: Tokens from /u1D45Eare split into a sequence { /u1D45E/u1D456} /u1D456∈[ 0,...,/u1D45B ] , with\n/u1D456denoting the position of the token in the sequence.\nOutput: Code tokens from /u1D450are split into a sequence { /u1D450/u1D456} /u1D456∈[ 0,...,/u1D45A ] .\nWe note that /u1D450can come from either retrieval (existing code) or\nbe produced by a generative model. The output is a short snipp et\nequivalent to a small line (or lines) of code.\n3.2 Baseline Retrieval\nOne of the core components of our model is the retrieval algor ithm.\nIt is responsible for producing a ranking of relevant docume nts\nwith respect to an input query. In our problem, the query is th e nat-\nural language English description from the code-descripti on pair,\n( /u1D45E,/u1D450) . Our search corpus is composed of all English descriptions o f\nthe training set. The retrieval algorithm then scores a docu ment /u1D451\nthrough its similarity function /u1D445/u1D446( /u1D45E,/u1D451) . We identify two eﬀective\nmethods for retrieving snippets. The ﬁrst is a BM25 implemen ta-\ntion in Lucene, using PyLucene as an interface. The second is the\nsimilarity scoring function from ReCode [4], a token level s tring\nsimilarity score. While we test both, we opt for BM25 due to th e\nmore eﬃcient implementation.\nThe ranking produced by the retrieval algorithm is used to th en\npick the top /u1D458documents. We extract the code from the pairs and\nuse it either as the ﬁnal output, as is the case for our baselin e re-\ntrieval methods, or as a guide for our Relevance model.\n3.3 Baseline Transformer\nOur system uses the Transformer [15] at its core. This archit ec-\nture employs several self-attentive layers in an encoder-d ecoder\nstructure to map variable-length input to a variable-lengt h output\nsequence. The output is produced autoregressively, genera ting a\nconditional distribution over the entire vocabulary at eac h time\nstep/u1D461. During training, the model uses a look-ahead attention mas k\nto hide future predictions from the current step, thus only b asing\nits prediction on the English tokens /u1D45Eand the currently produced\noutput sequence /u1D4500:/u1D461− 1. Given the smaller size of the datasets in\ncontrast to the original uses of Transformers, we reduce the size\nof our model to two attention layers for both the encoder and d e-\ncoder, four attention heads, embedding dimension of 512, an d a\npointwise feed-forward network dimension of 1024.\n3.4 Relevance Transformer\nIn this section, we outline how the Relevance Transformer co pes\nwith the unique challenges of generating code. Initial naïv e attempts\nconsisted of simply appending top code results to the input, but\nthese proved unsuccessful. There are several key component s in\nthe Relevance Transformer that provide signiﬁcant improve ments\nover the base implementation: pseudo-relevance feedback d ecod-\ning and input token copying.\n3.4.1 Pseudo-Relevance Feedback. Our second key aspect in our\nproposed network is a sequence aware pseudo-relevance feed back\n[7] decoding method. During a decoding step our copy augment ed\nTransformer produces a probability distribution over each token\nin the vocabulary, as well as positional out-of-vocabulary terms,\nwe denote this as /u1D440( /u1D45E,/u1D4500:/u1D461− 1) where /u1D4500:/u1D461− 1 = { /u1D4500, ..., /u1D450/u1D461− 1} is the\ncurrent decoded sequence. We aim to improve decoding qualit y by\nretrieving the top /u1D458documents /u1D437( /u1D45E, /u1D458) and emphasizing a set of\ncommon words /u1D446/u1D447( /u1D45B) in the results. We achieve this by interpo-\nlating normalized token frequency scores with the original NMT\ndistribution, Equation 1.\n/u1D443( /u1D464/u1D461| /u1D45E, /u1D4500:/u1D461− 1) =[ /u1D706· /u1D440( /u1D45E, /u1D4500:/u1D461− 1) + ( 1 − /u1D706) · /u1D445/u1D439( /u1D45E,/u1D464/u1D461)\n· /u1D445/u1D443( /u1D4500:/u1D461− 1,/u1D464/u1D461)] · /u1D44D (1)\n/u1D453 /u1D45F( /u1D464/u1D461, /u1D451) = /u1D450/u1D45C/u1D462/u1D45B/u1D461( /u1D464/u1D461,/u1D451)/ /u1D459/u1D452/u1D45B/u1D454/u1D461ℎ( /u1D451)\n/u1D445/u1D439( /u1D45E,/u1D464/u1D461) =\n[\n1 − ⊮/u1D446/u1D447( /u1D45B ) ( /u1D464/u1D461)\n]\n·\n/summationdisplay.1\n/u1D451 ∈/u1D437( /u1D45E,/u1D458)\n/u1D453 /u1D45F( /u1D464/u1D461, /u1D451) · /u1D445/u1D446( /u1D45E,/u1D451) (2)\nWhere /u1D44Dis the normalization constant. For each token, we take\ninto account the score given by the retrieval algorithm as we ll\nas the document length to emphasize top-scoring snippets. W hile\nthere is no guarantee a top-scoring snippet will provide goo d sug-\ngestions for words in the output, however, the aggregation o f mul-\ntiple top-scoring snippets it gives conﬁdence to increase t he prob-\nability of common words, Equation 2.\nWe also take into account terms that have already been seen in\nthe current decoded sequence. As such we use a repetition pen alty\n(Equation 3) to condition the probability given to a term bas ed on\nits previous presence in the prediction.\n/u1D445/u1D443( /u1D4500:/u1D461− 1,/u1D464/u1D461) = [ 1 − ⊮/u1D4500:/u1D461− 1 ( /u1D464/u1D461)] (3)\n3.4.2 Copy Generation Methods. Copy methods stem from Pointer\nNetworks [16] which use the attention distribution produce d over\nthe input sequence to choose an element from the input at each\ndecoding time step. While at its core Pointer Networks only a l-\nlow copying elements from the input, Copy Generator Network s\n[12] support both generation of new tokens and copying relev ant\ntokens from the input. Our code generation task beneﬁts from hav-\ning many tokens in the input sequence in common with the outpu t\nsequence, such as variable names and method identiﬁers. The se are\nnotoriously troublesome for sequence generation tasks sin ce they\nare often very rare in the small code-descriptions pair coll ections.\nAs such, Copy Generator Networks provide an eﬀective method to\nemphasize tokens regardless of their frequency in the datas et by\ncopying them from the input.\n/u1D440( /u1D464/u1D461| /u1D45E,/u1D4500:/u1D461− 1) = /u1D45D/u1D454/u1D452/u1D45B· /u1D447( /u1D464/u1D461| /u1D45E,/u1D4500:/u1D461− 1) + ( 1 − /u1D45D/u1D454/u1D452/u1D45B) · /u1D44E/u1D461( /u1D464/u1D461) (4)\nOur implementation of the copy generation in the Transforme r\nis inspired by See et al. [12]. We use the ﬁnal encoder attenti on\nvector and produce a copying vector emphasising each input t oken\nrelative to its attention weight /u1D44E/u1D461( /u1D464/u1D461) , Equation 4. This is then in-\nterpolated with the original vocabulary distribution /u1D447( /u1D464/u1D461| /u1D45E,/u1D4500:/u1D461− 1)\nthrough a /u1D45D/u1D454/u1D452/u1D45Bfunction. The use of out-of-vocabulary tokens for\nvery rare words, described in Section 4.2, allows for even mo re\ngeneric copying of words that haven’t even been seen in the tr ain-\ning dataset.\n4 EXPERIMENTAL SETUP\nIn this section, we describe the collections of code, the dat a pre-\nprocessing, and our evaluation metrics.\n4.1 Code collections\n4.1.1 Django [10]. This dataset was produced by a single engi-\nneer tasked to annotate the entire DJANGO source code line by\nline (18k+ lines). The original aim for the dataset was to map from\nDjango samples:\nDesc : description(COPY) is a string /quotedbl.VarThe /quotesingle.Var%s/quotesingle.Var\nfunction/quotedbl.Var(COPY) replaced by value of\nreceiver(COPY) . __name__ .\nTruth: description(COPY) = /quotedbl.VarThe /quotesingle.Var%s/quotesingle.Var function/quotedbl.Var(COPY)\n% receiver(COPY) . __name__\nPred : description(COPY) = /quotedbl.VarThe /quotesingle.Var%s/quotesingle.Var function/quotedbl.Var(COPY)\n% receiver(COPY)\nBLEU : 0.67\nCoNaLa sample:\nDesc : split string /grave.ts1input /grave.ts1based on occurrences of\nregex pattern /quotesingle.Var[ ](?=[A-Z]+\\\\b)/quotesingle.Var(COPY)\nTruth: re . split ( /quotesingle.Var[ ](?=[A-Z]+\\\\b)/quotesingle.Var(COPY) , input )\nPred : re . split ( /quotesingle.Var[ ](?=[A-Z]+\\\\b)/quotesingle.Var(COPY) , input )\nBLEU : 1.0\nFigure 2: Multiple predicted samples from the Relevance\nTransformer on Django and CoNaLa datasets\ncode to pseudo-code. This leads to relatively detailed desc riptions\nof each line which map to code.\n4.1.2 Hearthstone [8]. The dataset consists of 665 samples, each\nsourced from the cards of the game. A card consists of a name, d e-\nscription, and several key statistics. These ﬁelds form the whole of\nthe English description. The code consists of the associate d Python\nsource code from the game ﬁles. In contrast to the other datas ets,\nHearthstone consists of much longer sequences of approxima tely\n400 tokens. However, many of these sequences have similar bo iler-\nplate python code.\n4.1.3 CoNaLa [17]. This dataset is sourced from StackOverﬂow\nquestions and answers. It consists of over 2k hand-written s hort\nanswers to programming questions. These are high-quality c ode-\ndescription pairs. However, the dataset size is limited. Th e authors\nprovide an additional automatically annotated set of 600k+ pairs.\nDuring evaluation of the automatically annotated dataset, we deem\nit too noisy for our task and decide to solely use the 2k hand-\nwritten pairs.\n4.2 Pre-Processing\nOur training samples consist of two parallel languages: Eng lish\nand code. We process our samples into a common vocabulary set\nby tokenizing by spaces and speciﬁc code identiﬁers. This ki nd\nof tokenization is equivalent to that of ReCode [4] and prese rves\nstrsengs, variable names and function identiﬁers as indivi dual to-\nkens. A uniﬁed vocabulary is especially important since com mon\ntokens shared from input to output sequences allow for copyi ng.\nWe assign each out-of-vocabulary token shared between each se-\nquence a generic positional token, this gives the model the ﬂ exibil-\nity to copy potentially unseen relevant tokens to the output based\non context. As such, our vocabulary size is comparatively sm all at\nunder 1k tokens, while still allowing rare tokens to be predi cted.\nRetrieval Methods Django Hearthstone CoNaLa\nBM25 (ﬁne tuned baseline) 43.1 59.5 13.2\nReCode sequence similarity 43.4 65.1 11.2\nOracle retrieval similarity 58.1 74.2 38.0\nGenerative Methods\nSeq2Seq LSTM 58.9 60.4 10.6\nLatent predictor networks [8] 77.6 67.1 —\nRetrieve and Edit LSTM [3] — 70.0 —\nTransformer baseline [15] 79.2 72.5 17.5\nTransformer + Copy 81.8 74.0 20.8\nTransformer + Copy\n+ Naïve Retrieval\n80.7 60.1 19.0\nRelevance Transformer 82.3 74.5 22.3\nTable 1: Analysis of performance on various test collection s\nusing BLEU. In italic we show the previous state-of-the-art\nnon-AST methods. In bold we outline the best scores for\neach dataset.\n4.3 Evaluation\nBLEU is a standard metric in the ﬁeld of code generation [4, 8] . We\nfollow this standard and use the BLEU implementation from Re -\nCode [4] to evaluate the quality of our model’s output. The sc ores\nfor each pair is averaged to give an overall BLEU score for the\ndataset. We also test for signiﬁcance with a paired t-test an d ap-\nply Bonferroni corrections where applicable.\n5 RESULTS\nIn this section, we examine the results of our experiments on three\ncollections. Table 1 is divided into retrieval and generati ve meth-\nods. Despite being simple, retrieval methods are strong bas elines\nin a code setting. Code repetition and similar patterns, suc h as\nin Hearthstone, lead to high sequence similarity despite on ly be-\ning able to retrieve code from the training set. We test an ora cle\nmethod by taking the highest scoring retrieved snippet acco rding\nto BLEU, setting an upper bound on the eﬀectiveness of these m eth-\nods.\nIn the generative methods section, we outline ﬁrst the state -of-\nthe-art non-AST methods for each of the datasets. The base Tr ans-\nformer [15] model is used as a baseline for comparison. We not e\nthat the base Transformer model is already very eﬀective at t his\ntask, surpassing the previously stated results. Following this, the\nnaïve retrieval method is tested, which concatenates the to p code\ndocument to the input and uses our copy mechanism. Our exper-\niments show that the more complex input reduces overall eﬀec -\ntiveness. In contrast, the Relevance Transformer comprise s of both\nrelevance feedback and a copy mechanism and shows statistic ally\nsigniﬁcant improvements over the base Transformer at a 95% c onﬁ-\ndence interval for Django and CoNaLa. Hearthstone’s 66 test sam-\nples give inconclusive but suggestive results. Following a closer\ninspection of the decoded results, the eﬀectiveness increa se for\nCoNaLa suggests pseudo-relevance feedback is particularl y useful\nat boosting low scoring sequences by providing a starting po int of\npotentially useful terms for the model.\nIn Figure 1, we show how our Relevance Transformer plays a\nkey role in emphasising words that are likely to be in the targ et se-\nquence. In that example, the Transformer on its own predicts the\nnext token in the sequence to be ‘groupby’. This token is stil l rele-\nvant in the context but it is not the correct prediction. The p seudo-\nrelevance feedback corrects this by emphasising common tok ens\nfrom the top retrieved documents and results in the producti on of\nthe correct token, ‘objects’.\n6 CONCLUSION\nIn this work, we study the challenging task of code generatio n. We\nintroduce the Relevance Transformer, a model that leverage s exter-\nnal knowledge from pseudo-relevance feedback to increase t rans-\nlation quality and diversity. It uses feedback results at in ference\ntime with a copy mechanism to improve over the baseline Trans -\nformer and achieves state-of-the-art results on three stan dard code\ndatasets. Our approach is general and our results demonstra te that\nincorporating knowledge from retrieval can provide a signi ﬁcant\nbeneﬁt to generative models, in code generation and potenti ally in\nother domains as well.\nACKNOWLEDGEMENTS\nWe thank Iain Mackie for his contributions during developme nt.\nREFERENCES\n[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 201 4. Neural ma-\nchine translation by jointly learning to align and translat e. arXiv preprint\narXiv:1409.0473 (2014).\n[2] Jianfeng Gao, Jian-Yun Nie, Endong Xun, Jian Zhang, Ming Zhou, and Changn-\ning Huang. 2001. Improving query translation for cross-lan guage information\nretrieval using statistical models. In Proceedings of the 24th annual international\nACM SIGIR conference on Research and development in informa tion retrieval. 96–\n104.\n[3] Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Per cy S Liang. 2018. A\nretrieve-and-edit framework for predicting structured ou tputs. In Advances in\nNeural Information Processing Systems . 10052–10062.\n[4] Shirley Anugrah Hayati, Raphael Olivier, Pravalika Avv aru, Pengcheng Yin, An-\nthony Tomasic, and Graham Neubig. 2018. Retrieval-based ne ural code genera-\ntion. arXiv preprint arXiv:1808.10025 (2018), 925–930.\n[5] David A Hull and Gregory Grefenstette. 1996. Querying ac ross languages: a\ndictionary-based approach to multilingual information re trieval. In Proceedings\nof the 19th annual international ACM SIGIR conference on Res earch and develop-\nment in information retrieval . 49–57.\n[6] Jun-Jang Jeng and Betty HC Cheng. 1993. Using formal meth ods to construct\na software component library. In European Software Engineering Conference .\nSpringer, 397–417.\n[7] Victor Lavrenko and W Bruce Croft. 2017. Relevance-base d language models. In\nACM SIGIR Forum , Vol. 51. ACM New York, NY, USA, 260–267.\n[8] Wang Ling, Edward Grefenstette, Karl Moritz Hermann, To máš Kočisk `y, An-\ndrew Senior, Fumin Wang, and Phil Blunsom. 2016. Latent pred ictor networks\nfor code generation. arXiv preprint arXiv:1603.06744 (2016).\n[9] Gilad Mishne, Maarten De Rijke, et al. 2004. Source Code R etrieval using Con-\nceptual Similarity.. In RIAO, Vol. 4. Citeseer, 539–554.\n[10] Yusuke Oda, Hiroyuki Fudaba, Graham Neubig, Hideaki Ha ta, Sakriani Sakti,\nTomoki Toda, and Satoshi Nakamura. 2015. Learning to genera te pseudo-code\nfrom source code using statistical machine translation (t) . In 2015 30th IEEE/ACM\nASE. IEEE, 574–584.\n[11] Maxim Rabinovich, Mitchell Stern, and Dan Klein. 2017. Abstract syntax net-\nworks for code generation and semantic parsing. arXiv preprint arXiv:1704.07535\n(2017).\n[12] Abigail See, Peter J Liu, and Christopher D Manning. 201 7. Get to the point: Sum-\nmarization with pointer-generator networks. arXiv preprint arXiv:1704.04368\n(2017).\n[13] Renuka Sindhgatta. 2006. Using an information retriev al system to retrieve\nsource code samples. In Proceedings of the 28th international conference on Soft-\nware engineering. 905–908.\n[14] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequ ence to sequence learn-\ning with neural networks. In Advances in neural information processing systems .\n3104–3112.\n[15] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko reit, Llion Jones,\nAidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. At tention is all you\nneed. In Advances in neural information processing systems . 5998–6008.\n[16] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 20 15. Pointer networks. In\nAdvances in neural information processing systems . 2692–2700.\n[17] Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilesc u, and Graham Neu-\nbig. 2018. Learning to mine aligned code and natural languag e pairs from stack\noverﬂow. In 2018 IEEE/ACM 15th International Conference on Mining Soft ware\nRepositories (MSR) . IEEE, 476–486.\n[18] Jingyi Zhang, Masao Utiyama, Eiichro Sumita, Graham Ne ubig, and Satoshi\nNakamura. 2018. Guiding neural machine translation with re trieved translation\npieces. arXiv preprint arXiv:1804.02559 (2018).\nThis figure \"attention_mechanism.jpg\" is available in \"jpg\"\n format from:\nhttp://arxiv.org/ps/2007.02609v2",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8302890062332153
    },
    {
      "name": "Code generation",
      "score": 0.6257696151733398
    },
    {
      "name": "Transformer",
      "score": 0.606441080570221
    },
    {
      "name": "Inference",
      "score": 0.5481000542640686
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5191028118133545
    },
    {
      "name": "Programmer",
      "score": 0.5104795694351196
    },
    {
      "name": "Encoder",
      "score": 0.501861572265625
    },
    {
      "name": "Machine translation",
      "score": 0.4837779700756073
    },
    {
      "name": "Relevance (law)",
      "score": 0.48123908042907715
    },
    {
      "name": "Relevance feedback",
      "score": 0.47858190536499023
    },
    {
      "name": "Decoding methods",
      "score": 0.4765782356262207
    },
    {
      "name": "Machine learning",
      "score": 0.4183557629585266
    },
    {
      "name": "Natural language processing",
      "score": 0.3779309391975403
    },
    {
      "name": "Programming language",
      "score": 0.3776301145553589
    },
    {
      "name": "Algorithm",
      "score": 0.1406879723072052
    },
    {
      "name": "Image retrieval",
      "score": 0.07654678821563721
    },
    {
      "name": "Engineering",
      "score": 0.0731111466884613
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Key (lock)",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I7882870",
      "name": "University of Glasgow",
      "country": "GB"
    }
  ],
  "cited_by": 18
}