{
  "title": "KiRAG: Knowledge-Driven Iterative Retriever for Enhancing Retrieval-Augmented Generation",
  "url": "https://openalex.org/W4412944881",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2364479313",
      "name": "Jinyuan Fang",
      "affiliations": [
        "University of Glasgow"
      ]
    },
    {
      "id": "https://openalex.org/A2512114965",
      "name": "Zaiqiao Meng",
      "affiliations": [
        "University of Glasgow"
      ]
    },
    {
      "id": "https://openalex.org/A2113060918",
      "name": "Craig Macdonald",
      "affiliations": [
        "University of Glasgow"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4399553992"
  ],
  "abstract": null,
  "full_text": "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 18969–18985\nJuly 27 - August 1, 2025 ©2025 Association for Computational Linguistics\nKiRAG: Knowledge-Driven Iterative Retriever for Enhancing\nRetrieval-Augmented Generation\nJinyuan Fang\nUniversity of Glasgow\nj.fang.2@research.gla.ac.uk\nZaiqiao Meng*\nUniversity of Glasgow\nzaiqiao.meng@glasgow.ac.uk\nCraig Macdonald\nUniversity of Glasgow\ncraig.macdonald@glasgow.ac.uk\nAbstract\nIterative retrieval-augmented generation(iRAG)\nmodels offer an effective approach for multi-\nhop question answering (QA). However, their\nretrieval processesface two key challenges: (1)\nthey can be disrupted by irrelevant documents\nor factually inaccurate chain-of-thoughts; (2)\ntheir retrievers are not designed to dynami-\ncally adapt to the evolving information needs\nin multi-step reasoning, making it difficult to\nidentify and retrieve the missing information re-\nquired at each iterative step. Therefore, we pro-\npose KiRAG1, which uses a knowledge-driven\niterative retriever model to enhance the retrieval\nprocess of iRAG. Specifically, KiRAG decom-\nposes documents into knowledge triples and\nperforms iterative retrieval with these triples\nto enable a factually reliable retrieval process.\nMoreover, KiRAG integrates reasoning into the\nretrieval process to dynamically identify and re-\ntrieve knowledge that bridges information gaps,\neffectively adapting to the evolving information\nneeds. Empirical results show that KiRAG sig-\nnificantly outperforms existing iRAG models,\nwith an average improvement of 9.40% in R@3\nand 5.14% in F1 on multi-hop QA datasets.\n1 Introduction\nRetrieval-augmented generation (RAG) models\nhave demonstrated superior performance in ques-\ntion answering (QA) tasks (Lewis et al., 2020; Ram\net al., 2023; Lin et al., 2024). While standard RAG\nmodels excel at single-hop questions, they often\nstruggle with multi-hop questions (Trivedi et al.,\n2023), which require reasoning over multiple inter-\nconnected pieces of information to derive correct\nanswers. The key limitation is that their single-\nstep retrieval process often fails to retrieve all the\nrelevant information needed to answer multi-hop\nquestions (Shao et al., 2023), leading to knowledge\ngaps in the reasoning process. To address this lim-\nitation, iterative RAG (iRAG) models have been\n*Corresponding Author.\n1Code: https://github.com/jyfang6/kirag\nQuestion According to the 2001 census, what was the populationof the city in which Kirton End is located?\nKiRAG (Ours)\nStep 1: Kirton End is a hamlet in the civil parish ofKirton in theBoston district of Lincolnshire, England. ...\nStep 2: Boston is a town and small port in Lincolnshire,on the east coast of England... while the town itselfhad apopulation of 35,124 at the 2001 census.\nIRCoT\nStep 1: Kirton End is a hamlet in the civil parish ofKirton in theBoston district of Lincolnshire, England. ...\nStep 2: Kirton is a village in Nottinghamshire, England...According to the United Kingdom Census 2001 it had a\npopulation of 273 , reducing to 261 at the 2011 census.\nIRDoc\nStep 1: Kirton End is a hamlet in the civil parish ofKirton in theBoston district of Lincolnshire, England. ...\nStep 2: Ollerton is a small town in Nottinghamshire...The population of this parish at the 2011 census was 9,840 .\nHotPotQA 2Wiki\n64\n72\n80\n88R@3 (%)\nDocument Retrieval\nHotPotQA 2Wiki\n30\n40\n50\n60\n70F1 (%)\nMulti-Hop QA\nIRDoc IRCoT KiRAG\nFigure 1: (top) Example of top-ranked documents at\neach step, with relevant content marked in blue and\ndistracting content in orange . We compare KiRAG\nwith IRCoT (Trivedi et al., 2023) and its variant IRDoc,\nwhere we replace generated thoughts with top-ranked\ndocuments. (Bottom) The corresponding retrieval and\nQA performance on HotPotQA and 2Wiki datasets.\nproposed (Trivedi et al., 2023; Asai et al., 2024;\nSu et al., 2024; Yao et al., 2024). These models\nemploy multiple steps of retrieval and reasoning\nto iteratively gather the necessary information for\naddressing multi-hop questions.\nDespite the effectiveness of existing iRAG mod-\nels, their retrieval processfaces two key challenges:\n(1) These models perform iterative retrieval by it-\neratively augmenting the query with either previ-\nously retrieved documents (Zhao et al., 2021) or\ngenerated chain-of-thoughts (Trivedi et al., 2023).\nHowever, retrieved documents often include noise\nor irrelevant information (Yoran et al., 2024), while\ngenerated chain-of-thoughts can contain factually\ninaccurate content (Wang et al., 2023; Luo et al.,\n18969\n2024). The propagation of these distracting con-\ntexts can degrade retrieval quality and ultimately\nhinder overall RAG performance. (2) Answering a\nmulti-hop question requires multi-step reasoning,\nwhere the information needed to derive the correct\nanswer evolves with each iteration. For example,\nto answer the question in Figure 1, the first iterative\nstep requires retrieving the location of Kirton End\n(Boston). Once this information is obtained, the\nnext step shifts to retrieving Boston’s population in\n2001, demonstrating how the information needed\nto answer a multi-hop question evolves with each\niteration. However, existing iRAG models often\nrely on off-the-shelf retrieval models that retrieve\ninformation based on semantic similarity. These\nretrievers are not designed to dynamically adapt to\nthe evolving information needs in multi-step rea-\nsoning, making it difficult to identify and retrieve\nthe missing pieces of information needed at each\niteration, thereby hindering the overall retrieval ef-\nfectiveness. Figure 1 illustrates these two key chal-\nlenges, highlighting the necessity of developing a\nretrieval approach that can mitigate the impact of\nirrelevant documents or inaccurate thoughts, and\ndynamically adapt to evolving information needs.\nTo this end, we propose KiRAG, which lever-\nages a Knowledge-driven iterative retriever model\nto enhance the retrieval process of iRAG models.\nSpecifically, to address the challenge of irrelevant\ndocuments and inaccurate thoughts, inspired by\nprior works (Fang et al., 2024a,b) that use knowl-\nedge triples for enhanced reasoning, KiRAG de-\ncomposes documents into knowledge triples, for-\nmatted as ⟨head entity, relation, tail entity⟩, and per-\nforms iterative retrieval with these triples. By lever-\naging knowledge triples, which are compact and\ngrounded in documents, KiRAG enables a more\nfocused and factually reliable retrieval process.\nMoreover, to address the challenge of evolving\ninformation needs, KiRAG employs a knowledge-\ndriven iterative retrieval framework to retrieve rele-\nvant knowledge triples from the corpus systemat-\nically. This framework integrates reasoning into\nretrieval process, enabling the system to identify\nand retrieve knowledge that bridges information\ngaps dynamically. Specifically, the iterative re-\ntrieval process incrementally builds a knowledge\ntriple-based reasoning chain, such as “⟨Kirton End;\nlocation; Boston⟩,⟨Boston; population in 2001 cen-\nsus; 35,124⟩”, by retrieving triples step-by-step.\nAt each iteration, given the current step reasoning\nchain, e.g., “⟨Kirton End; location; Boston⟩”, Ki-\nRAG dynamically identifies and retrieves the miss-\ning knowledge triples needed to coherently extend\nthe chain towards answering the question. This tar-\ngeted approach can effectively guide the retrieval\nprocess in acquiring multiple interconnected pieces\nof information needed for addressing a question.\nWe evaluate KiRAG on five multi-hop and one\nsingle-hop QA datasets. KiRAG outperforms ex-\nisting iRAG models, achieving average improve-\nments of 9.40% in R @3 and 7.59% in R @5 on\nmulti-hop QA, which leads to an improvement of\n5.14% in F1. Despite that KiRAG is designed for\nmulti-hop QA, it achieves comparable retrieval and\nQA performance with state-of-the-art baselines on\nthe single-hop QA dataset, demonstrating its effec-\ntiveness across different types of questions.\nOur contributions can be summarised as follows:\n(1) We propose KiRAG, which performs iterative\nretrieval with knowledge triples to enhance the re-\ntrieval process of iRAG models; (2) KiRAG uses\na knowledge-driven iterative retrieval framework\nto dynamically adapt the retrieval process to the\nevolving information needs in multi-step reasoning;\n(3) Empirical results show that KiRAG achieves\nsuperior performance on multi-hop QA.\n2 Problem Formulation\nOur approach builds on the iRAG process. Given a\nquestion qand its answer a, iRAG is formalised as:\npθ,ϕ(a|q,C) ∼pϕ(a|q,Dq)pθ(Dq|q,C), (1)\npθ(Dq|q,C) ∼∏L\ni=1 pθ(Di\nq|q,D<i\nq ), (2)\nwhere pθ denotes the retriever modelthat iteratively\nretrieves documents Dq={Di\nq}L\ni=1 from a corpus C\nand pϕ is the reader model. At the i-th iteration, the\nretriever model retrieves documents Di\nq based on\nquestion qand previously retrieved documentsD<i\nq .\nIn this paper, we primarily focus on enhancing the\nretriever model, pθ, to effectively retrieve relevant\ndocuments from the corpus. To evaluate the effec-\ntiveness of our approach, we focus on multi-hop\nQA, a standard type of benchmark for assessing\niRAG systems (Gao et al., 2023).\n3 KiRAG\nThis section begins with an overview of KiRAG\nin §3.1. Next, we present a detailed explanation\nof each component from §3.2 to §3.3. Finally, the\ntraining strategy is introduced in §3.5.\n18970\nCorpus\nOffline Indexing\nKnowledge Triple ExtractionKnowledge Graph Corpus\nDoc Triples\n...\nStep 1: Knowledge-Driven Iterative Retrieval\n1.1. Knowledge\nDecomposition\n   -th Step\nReasoning Chain\nRanked Candidate Triples \n1.3. Reasoning\nChain Construction\nRetrieve &\nDecompose\nQuery\nSelected TripleKnowledge Triples\n... ...\nCoK Prompting\n1\nRetrieved Documents\n...\nRetrieved Knowledge Triples\n... + +... ... ...\nStep 2: Document Ranking\nAppend Selected Triple (-th iteration, where )\n1.2. Candidate Knowledge\nIdentification\nTraining of Reasoning Chain Aligner\n1 Training Data Construction\nContrastive Training2\nQuestion\n+\nGold Relevant Docs\n...\n+ > T1<  T2,\nT1 T2\nConstruct Reasoning Chain\nDecompose Reasoning Chain\nT1+\nQuery\n  Reasoning\nChain Aligner\nT3 T2...\nT3\nT2...\nCandidates\nTraining SetT1\nT1 T2\n+\nQuestion:\nLLM\n Boston\nlocation\nReader\nStep 3: Answer Generation\nAnswer\nReasoning Chain\nConstructor\nReasoning\nChain Aligner\nScore & Rank\nRetriever\nFigure 2: (left) Overview of KiRAG. Given a question, it employs a knowledge-driven iterative retrieval process\n(Step 1) to retrieve relevant knowledge triples, including three iterative steps: knowledge decomposition, candidate\nknowledge identification and reasoning chain construction. The retrieved triples are used to rank documents (Step\n2), which are passed to the reader for answer generation (Step 3). (right) Training strategy for the Reasoning Chain\nAligner, designed to optimise the identification of relevant knowledge triples at each step of the retrieval process.\n3.1 Overview\nFigure 2 provides an overview of our approach. Ki-\nRAG uses a knowledge-driven iterative retrieval\nframework to systematically retrieve a comprehen-\nsive set of relevant knowledge triples Tq (see §3.2).\nNext, it leverages the retrieved knowledge triples\nTq to identify and rank documents based on their\nrelevance to the question (see §3.3). Therefore, the\nretriever model of KiRAG can be formulated as:\npθ(Dq|q,C) ∼pθ(Dq|q,Tq)∏L\ni=1pθ(Tiq|q,T<iq ,C), (3)\nwhere Ti\nq is the set of knowledge triples retrieved\nat the i-th iteration, T<i\nq represents all previously\nretrieved triples and Lis the maximum number of\niterations. Once we obtain the retrieved documents\nDq, KiRAG employs an LLM-based reader model\nqϕ to generate the answer to the question.\n3.2 Knowledge-Driven Iterative Retrieval\nKiRAG retrieves relevant knowledge triples from\nthe corpus by progressively building a knowledge\ntriple-based reasoning chain, i.e., a sequence of\nlogically connected knowledge triples that support\nanswering a given question. For instance, the chain\n⟨Kirton End; location; Boston ⟩,⟨Boston; popula-\ntion in 2001 census; 35,124 ⟩provides relevant\nknowledge for answering the question in Figure 2.\nThe reasoning chain is built iteratively by select-\ning triples step-by-step. At the i-th iteration, given\nthe i-th step reasoning chain, which is a sequence\nof triples obtained up to the i-th iteration, such\nas ⟨Kirton End; location; Boston⟩, the framework\nretrieves and selects the next triple to extend the\nreasoning chain through the following three steps:\nKnowledge Decomposition. To enable a factually\nreliable retrieval process, KiRAG decomposes doc-\numents into knowledge triples. At the i-th iteration,\nthe query qi is formed by concatenating the ques-\ntion with the i-th step reasoning chain in the format\n“{question}. knowledge triples: {triple1}... ”. Ki-\nRAG employs an off-the-shelf Retriever model to\nretrieve K02 documents from the corpus, providing\nan initial pool of information for extracting relevant\nknowledge (see Step 1.1 in Figure 2).\nBuilding on recent advancements in extracting\nknowledge triples using LLMs (Edge et al., 2024;\nFang et al., 2024b), we employ in-context learning\nto prompt an LLM to extract knowledge triples for\neach retrieved document independently. Since the\nextraction process is query-independent, triples can\nbe precomputed offline for all documents in the cor-\npus3. This enables the construction of a knowledge\ngraph (KG) corpus, effectively improving retrieval\nefficiency. The prompt used for extracting knowl-\nedge triples is provided in Appendix A.1, where the\nLLM is instructed to extract all knowledge triples\ncontained within a document in a single pass. We\n2We provide analysis of the effect ofK0 in Appendix C.6.\n3The knowledge triples for retrieved documents can be\nobtained using the document IDs during the retrieval process.\n18971\ndenote the set of knowledge triples extracted from\nall the retrieved documents at step ias ˜Ti.\nCandidate Knowledge Identification. To adapt\nthe retrieval process to evolving information needs,\nKiRAG retrieves a subset of candidate knowledge\ntriples, i.e., Ti\nq, from all the extracted triples that\nare most likely to address the information gaps\nin the i-th step reasoning chain. These candidate\ntriples are selected based on their relevance to the\nquestion and their potential to form a coherent rea-\nsoning process with the i-th step reasoning chain.\nTo achieve this, we propose a Reasoning Chain\nAligner, which is designed to identify candidate\ntriples that advance the reasoning process (see Step\n1.2 in Figure 2). We instantiate the Aligner as a\nbi-encoder model. At the i-th iteration, the Aligner\nencodes the query qi, comprising the question and\nthe i-th step reasoning chain, and each triple tin\n˜Ti independently into a shared space. The score\nof each triple for addressing the information gaps\nin the i-th step reasoning chain is computed by\ntaking the inner-product of the query and triple\nembeddings: sθ(qi,t) = fθ(qi)⊤fθ(t), ∀t ∈ ˜Ti,\nwhere fθ(·) denotes the embedding function param-\neterised by θ. The top-N4 triples with the highest\nscores are selected as candidate triples to extend\nthe i-th step reasoning chain, i.e., Ti\nq. The reason-\ning chain Aligner is trained to retrieve triples that\ncontribute to building a coherent reasoning chain.\nDetails of the training process are provided in §3.5.\nReasoning Chain Construction. Given the can-\ndidate triples Ti\nq from the Aligner at the i-th iter-\nation, KiRAG employs an LLM-based Reasoning\nChain Constructor to select a single triple from\nthe candidates to extend the i-th step reasoning\nchain (see Step 1.3 in Figure 2). Our approach\nis inspired by IRCoT (Trivedi et al., 2023), which\niteratively generates individual sentences in a chain-\nof-thought (CoT). However, instead of relying on\npotentially inaccurate CoTs, we instruct the LLM\nto generate a chain-of-knowledge (CoK) (Wang\net al., 2024a), where free-form thoughts are re-\nplaced with document-grounded knowledge triples\nto ensure factual reliability.\nThe prompt used by the Constructor is provided\nin Appendix A.2. The inputs include the question,\nthe i-step reasoning chain and candidate triples Ti\nq.\nThe Constructor selects triples fromTi\nq to complete\nthe i-th step reasoning chain. The first triple in\n4We provide analysis of the effect of N in Appendix C.7.\nthe generated result is appended to the i-th step\nreasoning chain, forming a new chain that serves\nas input for subsequent iterations. Note that the\nConstructor aims to complete the whole chain, but\nwe only take the first triple. Asking the Constructor\nto complete the whole chain reduces hallucination,\nand avoids a sub-optimal greedy approach.\nThe iterative process terminates when the Con-\nstructor generates a reasoning chain containing\n“the answer is” or reaches the maximum number\nof iterative steps L. The candidate knowledge\ntriples collected during the iterative process, i.e.,\nTq = {Ti\nq}L\ni=1, along with their associated scores,\nare output for document retrieval and ranking.\n3.3 Document Ranking\nSince the retrieved knowledge triples Tq may lack\ncertain contextual information, we use these triples\nto identify and rank their source documents, i.e.,\np(Dq|q,Tq) in Eq. (3), to provide a more compre-\nhensive and precise context. Specifically, the re-\ntrieved documents Dq are collected by aggregat-\ning all the documents from which the triples in Tq\nare derived. To rank these documents, we assign\neach document the score of its associated triple(s)\nsθ(qi,t) from the iterative process. For a document\nassociated with multiple triples, its score is selected\nas the highest one. These documents are ranked\nin descending order of their scores, with top- K\ndocuments returned as the final retrieval results.\n3.4 Answer Generation\nGiven the question qand the ranked documents Dq,\nKiRAG leverages an LLM-based reader model to\ndirectly generate the answer. The prompt used for\nanswer generation is provided in Appendix A.3,\nwhich instructs the model to leverage the context\nprovided by the documents to answer the question.\n3.5 Training Strategy\nIn KiRAG, the Reasoning Chain Aligner is the\nkey component that requires training to effectively\nidentify candidate triples for extending the reason-\ning chain, while the other components, i.e., Re-\ntriever and Constructor, remain frozen. This sec-\ntion outlines the training strategy for the Aligner.\nDue to the lack of existing datasets specifically de-\nsigned for this task, we construct a silver training\ndataset by adapting data from existing multi-hop\nQA datasets. Specifically, given a question and its\nground-truth relevant documents, we construct a\n18972\nknowledge triple-based reasoning chain that sup-\nports answering the question. The reasoning chain\nand the question will serve as the labelled data for\ntraining the Aligner.\nTo train the Aligner, we decompose the complete\nreasoning chain into multiple incomplete reasoning\nchains and the corresponding next triples (see the\nright part of Figure 2). For each incomplete reason-\ning chain, the correct next triple is treated as the\npositive sample, while the other triples from the\ncandidate set ˜Ti are treated as negative samples.\nThe aligner is trained with contrastive learning loss:\nL= −\n∑\n(q,r,t+)∈P\nlog gθ(qr,t+)\ngθ(qr,t+) + ∑\nt−∈˜T|r|\ngθ(qr,t−), (4)\nwhere Pis the training set. Each data-point in-\ncludes a question q, an incomplete reasoning chain\nrand a positive triple t+. The query qr is the con-\ncatenation of qand r, and the function gθ(qr,t) =\nexp(sθ(qr,t))/τ computes the logits, with τ being\nthe temperature. Further details on the training data\nand training process are provided in Appendix B.3.\n4 Experiments\n4.1 Experimental Setup\nDatasets. We conduct experiments on five multi-\nhop QA datasets: HotPotQA (Yang et al., 2018),\n2WikiMultiHopQA (2Wiki) (Ho et al., 2020),\nMuSiQue (Trivedi et al., 2022),Bamboogle (Press\net al., 2023) and WebQuestions (WebQA) (Be-\nrant et al., 2013). We also use a single-hop QA\ndataset: Natural Questions (NQ) (Kwiatkowski\net al., 2019). We report the performance on the full\ntest sets of these datasets. For datasets with non-\npublic test sets (HotPotQA, 2Wiki and MuSiQue),\nwe use their development sets as test sets and re-\nport corresponding results. Detailed statistics and\ncorpus information are provided in Appendix B.1.\nBaselines. Since KiRAG aims to improve the re-\ntrieval performance of iRAG models, we primarily\ncompare it with iRAG models. Specifically, we\ncompare KiRAG with models from the following\ncategories: (1) Standard RAG model (Lewis et al.,\n2020), which retrieves documents once and gen-\nerates answers based on the retrieved documents;\n(2) iRAG models, which perform multiple steps of\nretrieval and reasoning to generate answers, such\nas IRCoT (Trivedi et al., 2023), FLARE (Jiang\net al., 2023b), and DRAGIN (Su et al., 2024); (3)\nModel HotPotQA 2Wiki MuSiQue\nR@3 R @5 R @3 R @5 R @3 R @5\nRAG 65.47∗ 70.78∗ 60.87∗ 65.20∗ 41.29∗ 46.53∗\nVector-PRF65.37∗ 70.06∗ 60.60∗ 64.85∗ 40.93∗ 45.46∗\nBeamDR 67.07∗ 71.89∗ 36.07∗ 42.08∗ 24.17∗ 28.18∗\nFLARE 54.79∗ 59.72∗ 60.84∗ 70.04∗ 39.79∗ 45.81∗\nDRAGIN 69.95∗ 75.85∗ 61.30∗ 70.43∗ 48.67∗ 54.67∗\nIRCoT 71.44∗ 77.57∗ 64.30∗ 75.56∗ 45.61∗ 52.21∗\nKiRAG-Doc67.80∗ 72.20∗ 45.85∗ 63.07∗ 25.86∗ 39.49∗\nKiRAG-Sent54.43∗ 69.26∗ 43.53∗ 59.33∗ 31.08∗ 43.69∗\nKiRAG 80.32† 84.08† 77.76† 85.32† 54.53† 61.16†\nTable 1: Retrieval performance (%) on multi-hop QA\ndatasets, with the best and second-best results marked\nin bold and underlined, respectively, and †denotes p-\nvalue<0.05 compared with best-performing baseline.\nModel HotPotQA 2Wiki MuSiQue\nEM F1 EM F1 EM F1\nRAG 34.54∗ 47.35∗ 14.78∗ 30.48∗ 09.10∗ 16.98∗\nVector-PRF34.40∗ 47.31∗ 14.96∗ 30.37∗ 09.23∗ 16.98∗\nBeamDR 38.34∗ 51.64∗ 14.42∗ 27.25∗ 07.08∗ 14.42∗\nFLARE 35.58∗ 47.74∗ 26.36∗ 41.82∗ 13.07∗ 21.94∗\nDRAGIN 41.74∗ 55.69∗ 25.58∗ 40.83∗ 16.87∗ 26.71∗\nIRCoT 42.38∗ 56.38∗ 25.12∗ 41.36∗ 15.76∗ 24.94∗\nKiRAG-Doc33.87∗ 46.43∗ 14.37∗ 27.54∗ 07.49∗ 15.34∗\nKiRAG-Sent34.14∗ 46.63∗ 14.22∗ 27.50∗ 10.51∗ 18.21∗\nKiRAG 45.09 † 59.76† 30.72† 50.57† 19.16† 30.00†\nTable 2: QA performance (%) on multi-hop QA datasets,\nwith the best and second-best results marked in bold\nand underlined, respectively. †denotes p-value<0.05\ncompared with best-performing baseline.\nEnhanced retrieval models, which improve the re-\ntrieval performance by using feedback from earlier\nretrieval steps, such as BeamDR (Zhao et al., 2021)\nand Vector-PRF (Li et al., 2023). Moreover, to eval-\nuate the effectiveness of using knowledge triples\nfor iterative retrieval, we introduce two variants:\nKiRAG-Doc and KiRAG-Sent, where the triples are\nreplaced with documents and sentences, respec-\ntively. Both variants follow the same procedure as\nKiRAG to retrieve documents. More details about\nthe baselines can be found in Appendix B.2.\nEvaluation. To evaluate the retrieval performance,\nwe follow previous works (Trivedi et al., 2023;\nGutiérrez et al., 2024) and use R@{3, 5} as the\nmetrics. To evaluate the QA performance, we use\nExact Match (EM) and F1 as evaluation metrics,\nwhich are the standard metrics for these datasets.\nTraining and Implementation Details. To train\nthe Aligner, we use TRACE (Fang et al., 2024b),\nwhich constructs knowledge triple-based reasoning\nchains from a fixed set of documents, to gener-\nate ground-truth reasoning chains. The reasoning\nchain that leads to the correct answer is used for\n18973\nModel Bamboogle WebQA NQ\nR@3 R @5 R @3 R @5 R @3 R @5\nRAG 20.80∗ 25.60∗ 64.91∗ 70.32∗ 73.07∗ 78.56∗\nVector-PRF20.60∗ 24.80∗ 64.86∗ 69.54∗ 72.82∗ 78.03∗\nBeamDR 12.00∗ 15.20∗ 41.63∗ 50.25∗ 33.88∗ 42.16∗\nFLARE 32.80∗ 37.60∗ 55.91∗ 60.97∗ 68.98∗ 73.43∗\nDRAGIN 36.80∗ 40.40∗ 65.11∗ 70.03∗ 68.98∗ 73.43∗\nIRCoT 28.00∗ 32.80∗ 65.50∗ 70.42∗ 73.38∗ 78.59∗\nKiRAG-Doc20.80∗ 27.20∗ 62.40∗ 68.60∗ 68.59∗ 74.99∗\nKiRAG-Sent26.40∗ 32.00∗ 62.16∗ 68.06∗ 67.48∗ 74.13∗\nKiRAG 45.60 † 49.60† 69.05† 73.08† 72.11∗ 77.28†\nTable 3: Retrieval performance (%) on unseen multi-\nhop and single-hop QA datasets, where † denotes p-\nvalue<0.05 compared with best-performing baselines.\ntraining. Training data is generated from the train-\ning sets of three multi-hop QA datasets: HotPotQA,\n2Wiki and MuSiQue. The combined data is used\nto train the Aligner. The Aligner is initialised with\nE5 (Wang et al., 2022) and finetuned with the con-\nstructed training data.\nKiRAG uses Llama3 (Dubey et al., 2024) to ex-\ntract triples and serve as the Constructor to build\nreasoning chains. It uses frozen E5 or BGE (Xiao\net al., 2024) as the Retriever. We use different read-\ners, including Llama3, Qwen2.5 (Yang et al., 2024),\nFlan-T5 (Chung et al., 2024) and TRACE (Fang\net al., 2024b) to generate answers. We mainly re-\nport results using E5 as the retriever and Llama3\nas the reader, with additional results from other re-\ntrievers and readers provided in Appendix C.1. For\nfair comparison, RAG baselines employ the same\nretriever and reader as KiRAG. More training and\nimplementation details are in Appendix B.3.\n4.2 Results and Analysis\nWe present our primary results in this section. Ad-\nditional results are provided in Appendix C.\n(RQ1): How does KiRAG perform in multi-hop\nQA compared with baselines?\nThe retrieval and QA 5 results are provided in\nTable 1 and Table 2, respectively, which yield the\nfollowing findings:\n(1) KiRAG consistently outperforms all baselines\nin retrieval performance on all datasets. Compared\nto the strongest baselines, KiRAG achieves statis-\ntically significant average improvements of 9.40%\nin R@3 and 7.59% in R@5, demonstrating its su-\nperior ability to enhance retrieval performance.\n(2) KiRAG consistently achieves the best QA per-\nformance on all datasets. It significantly outper-\n5QA performance is based on the top-3 retrieved doc-\numents. The results for the top-5 retrieved documents are\nprovided in Appendix C.2, which demonstrate similar results.\n1st Step 2nd Step 3rd Step\n(a) HotPotQA\n40\n50\n60\n70\n80R@3 (%)\n1st Step 2nd Step\n(b) 2Wiki\n20\n40\n60\n80\n100R@3 (%)\n1st Step 2nd Step 3rd Step\n(c) HotPotQA\n40\n50\n60\n70\n80R@5 (%)\n1st Step 2nd Step\n(d) 2Wiki\n20\n40\n60\n80\n100R@5 (%)\nKiRAG-Sent KiRAG-Doc RAG IRCoT KiRAG\nFigure 3: Retrieval performance (%) for relevant docu-\nments required across different steps, where most ques-\ntions in 2Wiki have only two relevant documents.\nforms best-performing baselines, with average im-\nprovements of 3.12% in EM and 5.14% in F1. The\nresults validate the effectiveness of KiRAG in facili-\ntating multi-hop QA through high-quality retrieval.\n(3) Compared to KiRAG-Doc and KiRAG-Sent,\nwhich perform iterative retrieval at document and\nsentence levels, KiRAG achieves substantially\nhigher retrieval performance. The suboptimal per-\nformance of these variants stems from the iterative\nretrieval process being misled by noise in docu-\nments and sentences. In contrast, KiRAG uses\nfiner-grained knowledge triples, reducing the im-\npact of noise and improving retrieval recall.\n(4) Compared to IRCoT, which uses CoT for iter-\native retrieval, KiRAG achieves superior retrieval\nresults, with an average improvement of 10.42%\nin R@3. This improvement stems from LLM’s\ntendency to generate hallucinated CoT. By using\ndocument-grounded knowledge triples, KiRAG en-\nsures a more reliable and faithful retrieval process.\n(RQ2): Why does KiRAG improve retrieval per-\nformance for multi-hop questions?\nTo explain why KiRAG achieves superior re-\ntrieval performance, we analyse its ability to re-\ntrieve relevant documents required at different steps\nof the reasoning process. For a multi-hop ques-\ntion, there are multiple logically ordered relevant\ndocuments. For instance, the first relevant docu-\nment for the question in Figure 2 is about “Kirton\nEnd”, while the second relevant document relates\nto “Boston”. At step i, we only consider the docu-\nment required at that specific step as relevant and\ncompute its recall. This approach allows us to\nassess how well KiRAG retrieves the necessary\ninformation at each step.\nThe results on HotPotQA and 2Wiki are shown\nin Figure 3, yielding the following findings: (1) The\nrecall of both KiRAG and baselines declines with\n18974\nModel Retriever Aligner Constructor HotPotQA 2Wiki MuSiQue\nR@3 R @5 F1 R @3 R @5 F1 R @3 R @5 F1\nKiRAG ✓ ✓ (trained) ✓ 80.32∗ 84.08∗ 59.76∗ 77.76∗ 85.32∗ 50.57∗ 54.53∗ 61.16∗ 30.00∗\nKiRAG w/o Retriever✗ ✓ (trained) ✓ 74.09† 78.03† 57.07† 77.29† 83.65† 48.76† 53.93† 60.68† 27.14†\nKiRAG w/o Aligner✓ ✗ ✓ 73.34† 75.79† 53.47† 67.66† 70.73† 39.06† 45.60† 49.62† 21.80†\nKiRAG w/o Constructor✓ ✓ (trained) ✗ 74.96† 79.51† 55.67† 72.64† 80.89† 45.69† 46.98† 55.12† 23.71†\nKiRAG w/o Training✓ ✓(w/o training)✓ 76.35† 81.56† 59.03† 75.37† 82.50† 48.33† 51.33† 58.66† 28.08†\nTable 4: Ablation studies of KiRAG, where †indicates p-value < 0.05 compared with KiRAG.\nModel Retrieval QA\nR@3 R@5 EM F1\nE5 29.50∗ 43.25∗ 23.00∗ 31.56∗\nKiRAG w/o Constructor76.50† 79.25† 31.00† 48.22†\nKiRAG 84.25 † 86.50† 39.00† 53.63†\nTable 5: Performance in retrieving relevant knowledge\ntriples for the 100 manually labelled questions on 2Wiki,\nwhere †denotes p-value<0.05 compared with E5.\nincreasing steps, highlighting the growing chal-\nlenge of retrieving relevant documents for later\nsteps in the reasoning process; (2) Compared to\nRAG and IRCoT, KiRAG shows comparable, and\noccasionally slightly lower, retrieval recall at the\nfirst step. However, it achieves substantially higher\nretrieval recall in subsequent steps, which con-\ntributes to its overall retrieval effectiveness. This\nimprovement stems from KiRAG’s iterative re-\ntrieval process, which dynamically adapts to evolv-\ning information needs, enabling the effective re-\ntrieval of relevant documents required at each step.\n(RQ3): Can KiRAG effectively generalise to un-\nseen multi-hop and single-hop QA datasets?\nTo evaluate the generalisation ability of KiRAG,\nwe conduct additional experiments on two multi-\nhop QA datasets, Bamboogle and WebQA, as well\nas a single-hop QA dataset, NQ, none of which\nwere included during training. The retrieval re-\nsults6 are presented in Table 3, which shows that\nKiRAG significantly outperforms all baselines on\ntwo multi-hop QA datasets, and demonstrates com-\nparable retrieval performance to IRCoT, the best-\nperforming baseline, on the single-hop QA dataset.\nThese results highlight the strong generalisation\nability of KiRAG in handling diverse QA tasks.\n(RQ4): What are the effects of each component\nand the training strategy in KiRAG?\nTo evaluate the impact of the Retriever, we intro-\nduce KiRAG w/o Retriever, where the retriever is\nremoved and candidate triples are directly retrieved\n6The QA performance, presented in Table 13 of the Ap-\npendix, shows consistent results with retrieval performance.\n1 2 3 4 5 6\nL\n40\n50\n60\n70\n80R@3 (%)\n51\n59\n67\n76\n84\n1 2 3 4 5 6\nL\n40\n50\n60\n70\n80\n42\n55\n68\n81\n94\nF1 (%)\n(a) HotPotQA (b) 2Wiki\nIRCoT R@3 KiRAG R@3 IRCoT F1 KiRAG F1\nFigure 4: The effect of the number of iterative steps L.\nfrom the knowledge graph corpus using the Reason-\ning Chain Aligner. Table 4 shows that removing the\nRetriever leads to a significant performance drop on\nHotPotQA while maintaining comparable perfor-\nmance on 2Wiki and MuSiQue. This demonstrates\nthe Aligner’s effectiveness in identifying relevant\nknowledge triples but highlights the limitations of\nrelying solely on the knowledge graph, which may\nlose contextual information present in documents.\nTo assess the impact of the Aligner, we introduce\nKiRAG w/o Aligner, where the Aligner is removed\nand all knowledge triples from the retrieved docu-\nments are passed to the Reasoning Chain Construc-\ntor. Table 4 shows that KiRAG w/o Aligner suffers\nan average decrease of 8.67% in R@3 and 11.47%\nin R@5 compared to KiRAG. This decline is due\nto the absence of filtering or ranking by the Aligner,\nresulting in noisy and irrelevant triples that hinder\nthe Reasoning Chain Constructor’s ability to build\ncoherent reasoning chains, which is essential for\nguiding the iterative retrieval process effectively.\nMoreover, to evaluate the impact of the Construc-\ntor, we introduce KiRAG w/o Constructor, which\nconstructs a reasoning chain using only the top-\nranked triple identified by the Aligner. Table 4\nindicates that removing the Constructor leads to\nsignificantly inferior performance, highlighting the\nimportance of the LLM-based Constructor in build-\ning coherent reasoning chains through its advanced\nreasoning and contextual understanding capability.\nTo assess the impact of training the Aligner for\nretrieving and integrating triples, we introduce Ki-\nRAG w/o Training, where the Aligner is replaced\nwith a frozen E5, which is trained for general text\nretrieval. Table 4 shows that KiRAG w/o Training\n18975\n5 10 15 20 25\nLatency (s)\n50\n60\n70\n80\n90R@3 (%)\nKiRAG KiRAG (online)\nFLARE\nDRAGINIRCoT\nKiRAG-Doc\nKiRAG-Sent\nFigure 5: Retrieval performance v.s. average latency per\nquestion for different models on the HotPotQA test set.\nKiRAG (online) represents a variant of our approach\nwhere knowledge triples are extracted dynamically dur-\ning iterative retrieval, without precomputation.\nexhibits a significant decline in both retrieval and\nQA results. These results highlight the effective-\nness of our training strategy in enabling the Aligner\nto identify relevant knowledge triples.\n(RQ5): Can KiRAG retrieve relevant knowledge\ntriples to address multi-hop questions?\nTo evaluate the quality of knowledge triples re-\ntrieved by KiRAG, we randomly select 100 ques-\ntions from the 2Wiki test set and manually identify\nknowledge triples that are useful in answering these\nquestions. These manually selected triples are con-\nsidered relevant7. We use R @K to measure re-\ntrieval performance and compute QA metrics (EM\nand F1) using the retrieved triples as context. We\ncompare KiRAG with E5, which directly retrieves\nknowledge triples from the knowledge graph cor-\npus, and the Reasoning Chain Aligner, which iter-\natively retrieves triples using the trained Aligner.\nTable 5 shows that KiRAG significantly outper-\nforms E5 in both retrieval and QA performance,\ndemonstrating its effectiveness in retrieving rele-\nvant knowledge triples. This superior performance\nis attributed to the Aligner and the Constructor,\nwhich are designed to identify relevant triples.\n(RQ6): How does the number of iterative steps\nLaffect the performance of KiRAG? Figure 4\nshows the retrieval and QA results of KiRAG with\ndifferent values of Lon HotPotQA and 2Wiki de-\nvelopment sets. The results show that as the value\nof Lincreases, both retrieval and QA performance\ninitially improve and then reach a plateau, with\nKiRAG achieving optimal performance at a moder-\n7Appendix C.5 provides details and examples of the man-\nually curated data, which will be released alongside the code.\nQ: Which man who presented the Australia\n2022 FIFA World Cup bid was born on\nOctober 22, 1930?\nInput\nA: Frank Lowy\nKiRAG Generated Reasoning Chain:\nStep 1: <Australia 2022 FIFA World Cup bid;\nbid presenters; Frank Lowy, Ben Buckley,\nQuentin Bryce, Elle Macpherson> \nStep 2: <Frank Lowy; birth year; 1930> \nStep 3: <Frank Lowy; birth date; 22 October>\nStep 4: So the answer is: Frank Lowy. \nOutput\nIRCoT Generated Chain-of-Thought:\nStep1: There is no mention of a person born\non October 22, 1930, in the provided texts. \nStep 2: The texts mention Frank Lowy, Ben\nBuckley, Quentin Bryce, and Elle Macpherson\nas the presenters of the Australia 2022 FIFA\nWorld Cup bid, but none None of the\nindividuals mentioned in the provided texts\nwere born on October 22, 1930.\nQ: The Memphis Hustle are based in a\nsuburb of a city with a population of what\nin 2010?\nInput\nA: 48,982\nKiRAG Generated Reasoning Chain:\nStep 1: <Memphis Hustle; location;\nSouthaven, Mississippi>\nStep 2: <Southaven; population; 48,982\nas of 2010> \nStep 3: So the answer is: 48,982.\nOutput\nIRCoT Generated Chain-of-Thought:\nStep 1: The Memphis Hustle are based in\nSouthaven, Mississippi, which is a suburb\nof Memphis, Tennessee. \nStep 2: According to the Wikipedia article\non Collierville, Tennessee, the population\nof Memphis was 43,965 at the 2010\ncensus. So the answer is: 43,965.\nFigure 6: Case study of KiRAG and IRCoT on Hot-\nPotQA test set, where the relevant and irrelevant con-\ntexts are marked in blue and orange , respectively.\nate value of L. This highlights the importance of\nselecting a proper value of Lin KiRAG to balance\nhigh accuracy and efficiency in multi-hop QA.\nEfficiency Analysis. We evaluate the efficiency\nof KiRAG in comparison to the baseline models.\nSpecifically, we conduct experiments on a 3.5 GHZ,\n32-core AMD Ryzen Threadripper Process paired\nwith an NVIDIA A6000 GPU. For fair compari-\nson, both KiRAG and baselines leverage the same\nE5 model for document retrieval and the same\nLlama3 model as the reasoning component. It is\nworth noting that the knowledge triple extraction\nin KiRAG is query-independent and precomputed,\nwhich helps to improve efficiency. To evaluate the\nimpact of precomputing triples, we introduce a vari-\nant: KiRAG (online), where triples are dynamically\nextracted during the iterative retrieval process.\nFigure 5 presents the average latency and re-\ntrieval performance of different models on the Hot-\nPotQA test set, which yields the following findings:\n(1) Compared with KiRAG (online), KiRAG sub-\nstantially reduces latency without compromising\nretrieval performance, highlighting the efficiency\nbenefits of precomputed knowledge triple extrac-\ntion; (2) KiRAG exhibits latency comparable to IR-\nCoT while achieving significantly better retrieval\nperformance, indicating that our approach effec-\ntively enhances retrieval effectiveness without in-\ntroducing substantial computational overhead. (3)\nKiRAG achieves a better balance between retrieval\neffectiveness and efficiency compared to baselines,\nas evidenced by the relatively lower latency and\nhigher retrieval recall.\n18976\nCase Study. We conduct a case study to examine\nthe reasoning chains generated by KiRAG. Figure 6\nshows examples of the reasoning chains produced\nby KiRAG and the CoTs generated by IRCoT. The\nexamples show that KiRAG can generate coherent\nand contextually relevant reasoning chains for an-\nswering multi-hop questions, which are essential\nfor effectively guiding the iterative retrieval process.\nIn contrast, IRCoT may struggle with missing in-\nformation or hallucinations, hindering its ability to\nretrieve the necessary knowledge.\n5 Related Work\nRAG Models. RAG models have shown superior\nperformance in QA tasks (Lewis et al., 2020; Izac-\nard and Grave, 2021b; Ram et al., 2023). These\nmodels typically employ the retriever-reader archi-\ntecture, which consists of a retriever (Karpukhin\net al., 2020; Wang et al., 2022; Fang et al., 2023)\nand a reader (Izacard and Grave, 2021b; Jiang et al.,\n2023b). Efforts to improve RAG models generally\nfollow three main directions: (1) enhancing the\nretriever, which aims to improve the retrieval effec-\ntiveness to better identify relevant documents (Izac-\nard and Grave, 2021a; Shi et al., 2023; Wang et al.,\n2024b); (2) enhancing the reader, which aims to\nimprove the reader’s reasoning abilities for better\ncomprehension and answer generation (Lin et al.,\n2024; Xu et al., 2024; Wang et al., 2024c); (3) in-\ntroducing additional modules to bridge the retriever\nand the reader (Yu et al., 2023; Xu et al., 2023; Ye\net al., 2024). While these models often employ\na single-step retrieval to retrieve documents, our\nKiRAG adopts a multi-step retrieval approach, en-\nabling iterative refinement of retrieved information.\nIterative RAG Models for Multi-Hop QA. Itera-\ntive RAG models (Trivedi et al., 2023; Shao et al.,\n2023; Asai et al., 2024; Liu et al., 2024; Yao et al.,\n2024) have been proposed to address multi-hop QA.\nThese models leverage multiple steps of retrieval\nand reasoning to answer questions. For instance,\nIRCoT (Trivedi et al., 2023) use LLM-generated\nchain-of-thoughts for retrieval, while DRAGIN (Su\net al., 2024) dynamically decides when and what\nto retrieve based on the LLM’s information needs.\nHowever, these models rely on the thoughts or next\nsentence generated by LLM to guide the retrieval\nprocess, which makes them prone to hallucination.\nIn contrast, KiRAG employs knowledge triples and\na trained retriever to actively identify and retrieve\nmissing information, enabling a more reliable and\naccurate retrieval for multi-hop QA.\nKG-Enhanced RAG Models. KGs have been\nintegrated into RAG models (Peng et al., 2024).\nSome studies leverage information from existing\nKGs (Vrandeˇci´c and Krötzsch, 2014) for additional\ncontext (Yu et al., 2022; Sun et al., 2024), while\nothers generate KGs from documents to improve\nknowledge organisation (Edge et al., 2024; Gutiér-\nrez et al., 2024; Chen et al., 2024) or enhance\nreader comprehension (Li and Du, 2023; Fang\net al., 2024a,b; Panda et al., 2024). These mod-\nels primarily follow the standard RAG pipeline,\nwhereas our work focuses on the iRAG pipeline.\nMoreover, while they use single-step retrieval with\npre-existing retrievers, KiRAG employs a trained\nretriever tailored for iterative retrieval, allowing\nit to adapt to the evolving information needs in\nmulti-step reasoning. More comparisons between\nKiRAG and existing KG-enhanced RAG models\ncan be found in Appendix C.9.\n6 Conclusions\nThis paper proposed KiRAG to enhance the re-\ntrieval process of iRAG models. KiRAG decom-\nposes documents into knowledge triples and em-\nploys a knowledge-driven iterative retrieval frame-\nwork to systematically retrieve relevant knowledge\ntriples. The retrieved triples are used to rank docu-\nments, which serve as inputs for answer generation.\nEmpirical results showed that KiRAG achieves sig-\nnificant retrieval and QA improvements, with an\naverage increase of 9.40% in R@3 and 5.14% in F1\ncompared to the strongest state-of-the-art baselines,\nhighlighting its effectiveness in multi-hop QA.\nLimitations\nWe identify the following limitations of our work:\n(1) The Aligner model is trained using silver data\nconstructed from only three multi-hop QA datasets.\nWhile our results demonstrate its effectiveness, we\nleave the exploration of methods to construct larger-\nscale and higher-quality training data for future\nwork; (2) In KiRAG, we train only the Aligner\nmodel and keep the Constructor model frozen.\nWhile further training the Constructor could po-\ntentially improve performance, we choose to keep\nit frozen to maintain our framework’s adaptability\nto different LLMs, rather than relying on a specific\nfine-tuned LLM. Appendix C.8 provides a detailed\nanalysis of the performance using different LLM-\nbased Constructor within our framework.\n18977\nReferences\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\nHannaneh Hajishirzi. 2024. Self-RAG: Learning to\nretrieve, generate, and critique through self-reflection.\nIn The Twelfth International Conference on Learning\nRepresentations.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on freebase from\nquestion-answer pairs. In Proceedings of the 2013\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1533–1544.\nWeijie Chen, Ting Bai, Jinbo Su, Jian Luan, Wei Liu,\nand Chuan Shi. 2024. KG-Retriever: Efficient knowl-\nedge indexing for retrieval-augmented large language\nmodels. arXiv preprint arXiv:2412.05547.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac\nSuzgun, Xinyun Chen, Aakanksha Chowdhery, Alex\nCastro-Ros, Marie Pellat, Kevin Robinson, and 16\nothers. 2024. Scaling instruction-finetuned language\nmodels. Journal of Machine Learning Research ,\n25:70:1–70:53.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, and 1 others. 2024. The Llama 3 herd of models.\narXiv preprint arXiv:2407.21783.\nDarren Edge, Ha Trinh, Newman Cheng, Joshua\nBradley, Alex Chao, Apurva Mody, Steven Truitt,\nand Jonathan Larson. 2024. From local to global: A\ngraph RAG approach to query-focused summariza-\ntion. arXiv preprint arXiv:2404.16130.\nJinyuan Fang, Zaiqiao Meng, and Craig Macdonald.\n2023. KGPR: Knowledge graph enhanced passage\nranking. In Proceedings of the 32nd ACM Interna-\ntional Conference on Information and Knowledge\nManagement, pages 3880–3885.\nJinyuan Fang, Zaiqiao Meng, and Craig Macdonald.\n2024a. REANO: Optimising retrieval-augmented\nreader models through knowledge graph generation.\nIn Proceedings of the 62nd Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 2094–2112.\nJinyuan Fang, Zaiqiao Meng, and Craig MacDon-\nald. 2024b. TRACE the evidence: Constructing\nknowledge-grounded reasoning chains for retrieval-\naugmented generation. In Findings of the Associa-\ntion for Computational Linguistics: EMNLP, pages\n8472–8494.\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,\nJinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen\nWang. 2023. Retrieval-augmented generation for\nlarge language models: A survey. arXiv preprint\narXiv:2312.10997.\nBernal Jiménez Gutiérrez, Yiheng Shu, Yu Gu, Michi-\nhiro Yasunaga, and Yu Su. 2024. HippoRAG: Neu-\nrobiologically inspired long-term memory for large\nlanguage models. arXiv preprint arXiv:2405.14831.\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,\nand Akiko Aizawa. 2020. Constructing A multi-\nhop QA dataset for comprehensive evaluation of\nreasoning steps. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics,\npages 6609–6625.\nGautier Izacard and Edouard Grave. 2021a. Distilling\nknowledge from reader to retriever for question an-\nswering. In International Conference on Learning\nRepresentations.\nGautier Izacard and Edouard Grave. 2021b. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics, pages 874–880.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, and 1 others. 2023a.\nMistral 7b. arXiv preprint arXiv:2310.06825.\nZhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun,\nQian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie\nCallan, and Graham Neubig. 2023b. Active retrieval\naugmented generation. In Proceedings of the 2023\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 7969–7992.\nJiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang,\nand Zhicheng Dou. 2024. FlashRAG: A modular\ntoolkit for efficient retrieval-augmented generation\nresearch. arXiv preprint arXiv:2405.13576.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nS. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,\nand Wen-tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing, pages 6769–6781.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In International\nConference on Learning Representations.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, and 1 others. 2019. Natural questions: a\nbenchmark for question answering research. Trans-\nactions of the Association for Computational Linguis-\ntics, 7:453–466.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, and 1 others. 2020. Retrieval-augmented\ngeneration for knowledge-intensive NLP tasks. Ad-\nvances in Neural Information Processing Systems ,\n33:9459–9474.\n18978\nHang Li, Ahmed Mourad, Shengyao Zhuang, Bevan\nKoopman, and Guido Zuccon. 2023. Pseudo rele-\nvance feedback with deep language models and dense\nretrievers: Successes and pitfalls. ACM Transation\non Information System, 41(3):62:1–62:40.\nRuosen Li and Xinya Du. 2023. Leveraging structured\ninformation for explainable multi-hop question an-\nswering and reasoning. In Findings of the Associa-\ntion for Computational Linguistics: EMNLP, pages\n6779–6789.\nXi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi,\nMaria Lomeli, Rich James, Pedro Rodriguez, Jacob\nKahn, Gergely Szilvasy, Mike Lewis, and 1 others.\n2024. RA-DIT: Retrieval-augmented dual instruc-\ntion tuning. In International Conference on Learning\nRepresentations.\nYanming Liu, Xinyue Peng, Xuhong Zhang, Weihao\nLiu, Jianwei Yin, Jiannan Cao, and Tianyu Du. 2024.\nRA-ISF: learning to answer and understand from re-\ntrieval augmentation via iterative self-feedback. In\nFindings of the Association for Computational Lin-\nguistics, pages 4730–4749. Association for Compu-\ntational Linguistics.\nLinhao Luo, Yuan-Fang Li, Gholamreza Haffari, and\nShirui Pan. 2024. Reasoning on graphs: Faithful\nand interpretable large language model reasoning. In\nThe Twelfth International Conference on Learning\nRepresentations.\nPranoy Panda, Ankush Agarwal, Chaitanya Devagup-\ntapu, Manohar Kaul, and Prathosh A P. 2024.\nHOLMES: hyper-relational knowledge graphs for\nmulti-hop question answering using LLMs. In Pro-\nceedings of the 62nd Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 13263–13282.\nBoci Peng, Yun Zhu, Yongchao Liu, Xiaohe Bo,\nHaizhou Shi, Chuntao Hong, Yan Zhang, and Siliang\nTang. 2024. Graph retrieval-augmented generation:\nA survey. arXiv preprint arXiv:2408.08921.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A. Smith, and Mike Lewis. 2023. Measuring\nand narrowing the compositionality gap in language\nmodels. In Findings of the Association for Computa-\ntional Linguistics: EMNLP, pages 5687–5711.\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\nShoham. 2023. In-context retrieval-augmented lan-\nguage models. Transactions of the Association for\nComputational Linguistics, 11:1316–1331.\nZhihong Shao, Yeyun Gong, Yelong Shen, Minlie\nHuang, Nan Duan, and Weizhu Chen. 2023. En-\nhancing retrieval-augmented large language models\nwith iterative retrieval-generation synergy. In Find-\nings of the Association for Computational Linguistics:\nEMNLP, pages 9248–9274.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Min-\njoon Seo, Rich James, Mike Lewis, Luke Zettle-\nmoyer, and Wen-tau Yih. 2023. REPLUG: Retrieval-\naugmented black-box language models. arXiv\npreprint arXiv:2301.12652.\nWeihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu,\nand Yiqun Liu. 2024. DRAGIN: Dynamic retrieval\naugmented generation based on the real-time informa-\ntion needs of large language models. In Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 12991–13013.\nJiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo\nWang, Chen Lin, Yeyun Gong, Lionel M. Ni, Heung-\nYeung Shum, and Jian Guo. 2024. Think-on-Graph:\nDeep and responsible reasoning of large language\nmodel on knowledge graph. In The Twelfth Interna-\ntional Conference on Learning Representations.\nGemma Team, Morgane Riviere, Shreya Pathak,\nPier Giuseppe Sessa, Cassidy Hardin, Surya Bhupati-\nraju, Léonard Hussenot, Thomas Mesnard, Bobak\nShahriari, Alexandre Ramé, and 1 others. 2024.\nGemma 2: Improving open language models at a\npractical size. arXiv preprint arXiv:2408.00118.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\nand Ashish Sabharwal. 2022. MuSiQue: Multi-\nhop questions via single-hop question composition.\nTrans. Assoc. Comput. Linguistics, 10:539–554.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\nand Ashish Sabharwal. 2023. Interleaving retrieval\nwith chain-of-thought reasoning for knowledge-\nintensive multi-step questions. In Proceedings of the\n61st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n10014–10037.\nDenny Vrandeˇci´c and Markus Krötzsch. 2014. Wiki-\ndata: a free collaborative knowledgebase. Communi-\ncations of the ACM, 57(10):78–85.\nCunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xiangru\nTang, Tianhang Zhang, Cheng Jiayang, Yunzhi Yao,\nWenyang Gao, Xuming Hu, Zehan Qi, and 1 others.\n2023. Survey on factuality in large language models:\nKnowledge, retrieval and domain-specificity. arXiv\npreprint arXiv:2310.07521.\nJianing Wang, Qiushi Sun, Xiang Li, and Ming Gao.\n2024a. Boosting language models reasoning with\nchain-of-knowledge prompting. In Proceedings of\nthe 62nd Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers) ,\npages 4958–4981.\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing\nJiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\nand Furu Wei. 2022. Text embeddings by weakly-\nsupervised contrastive pre-training. arXiv preprint\narXiv:2212.03533.\n18979\nShuting Wang, Xin Yu, Mang Wang, Weipeng Chen,\nYutao Zhu, and Zhicheng Dou. 2024b. RichRAG:\nCrafting rich responses for multi-faceted queries\nin retrieval-augmented generation. arXiv preprint\narXiv:2406.12566.\nYuhao Wang, Ruiyang Ren, Junyi Li, Xin Zhao,\nJing Liu, and Ji-Rong Wen. 2024c. REAR: A\nrelevance-aware retrieval-augmented framework for\nopen-domain question answering. In Proceedings\nof the 2024 Conference on Empirical Methods in\nNatural Language Processing, pages 5613–5626.\nShitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muen-\nnighoff, Defu Lian, and Jian-Yun Nie. 2024. C-pack:\nPacked resources for general chinese embeddings. In\nProceedings of the 47th International ACM SIGIR\nConference on Research and Development in Infor-\nmation Retrieval, pages 641–649.\nFangyuan Xu, Weijia Shi, and Eunsol Choi. 2023.\nRECOMP: Improving retrieval-augmented lms with\ncompression and selective augmentation. arXiv\npreprint arXiv:2310.04408.\nShicheng Xu, Liang Pang, Mo Yu, Fandong Meng,\nHuawei Shen, Xueqi Cheng, and Jie Zhou. 2024. Un-\nsupervised information refinement training of large\nlanguage models for retrieval-augmented generation.\nIn Proceedings of the 62nd Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 133–145.\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui,\nBo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu,\nFei Huang, Haoran Wei, and 1 others. 2024. Qwen2.\n5 technical report. arXiv preprint arXiv:2412.15115.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W. Cohen, Ruslan Salakhutdinov, and\nChristopher D. Manning. 2018. HotpotQA: A\ndataset for diverse, explainable multi-hop question\nanswering. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Process-\ning, pages 2369–2380.\nZijun Yao, Weijian Qi, Liangming Pan, Shulin Cao, Lin-\nmei Hu, Weichuan Liu, Lei Hou, and Juanzi Li. 2024.\nSEAKR: Self-aware knowledge retrieval for adap-\ntive retrieval augmented generation. arXiv preprint\narXiv:2406.19215.\nFuda Ye, Shuangyin Li, Yongqi Zhang, and Lei Chen.\n2024. R2AG: Incorporating retrieval information\ninto retrieval augmented generation. arXiv preprint\narXiv:2406.13249.\nOri Yoran, Tomer Wolfson, Ori Ram, and Jonathan\nBerant. 2024. Making retrieval-augmented language\nmodels robust to irrelevant context. In International\nConference on Learning Representations.\nDonghan Yu, Chenguang Zhu, Yuwei Fang, Wenhao\nYu, Shuohang Wang, Yichong Xu, Xiang Ren, Yim-\ning Yang, and Michael Zeng. 2022. KG-FiD: Infus-\ning knowledge graph in fusion-in-decoder for open-\ndomain question answering. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 4961–4974.\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,\nMingxuan Ju, Soumya Sanyal, Chenguang Zhu,\nMichael Zeng, and Meng Jiang. 2023. Generate\nrather than retrieve: Large language models are\nstrong context generators. In The Eleventh Inter-\nnational Conference on Learning Representations.\nChen Zhao, Chenyan Xiong, Jordan L. Boyd-Graber,\nand Hal Daumé III. 2021. Multi-step reasoning over\nunstructured text with beam dense retrieval. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n4635–4641.\nA Prompts\nA.1 Prompt for Knowledge Triple Extraction\nThe prompt used for extracting knowledge triples\nfrom a document is illustrated in Figure 7.\nA.2 Prompt for Reasoning Chain\nConstruction\nThe prompt used by the reasoning chain constructor\nto build reasoning chains is illustrated in Figure 8,\nwhere we instruct the Reasoning Chain Constructor\nto complete the i-th step reasoning chain with the\nprovided candidate knowledge triples.\nA.3 Prompt for Answer Generation\nThe prompt used by the reader to generate answers\nis illustrated in Figure 9.\nB Experimental Details\nB.1 Datasets\nIn our experiments, we employ five multi-hop QA\ndatasets: HotPotQA, 2WikiMultiHopQA (2Wiki),\nMuSiQue, Bamboogle as well as WebQuestions\n(WebQA), and one single-hop QA dataset: Nat-\nural Questions (NQ). For HotpotQA, we use the\ncorpus provided by its authors for retrieval. For\n2WikiMultihopQA and MuSiQue, we construct the\nretrieval corpus following the exact same proce-\ndure outlined by Trivedi et al. (2023). For all other\ndatasets, we leverage the Wikipedia corpus intro-\nduced by Karpukhin et al. (2020).\nFor datasets with public test sets (Bamboogle,\nWebQA and NQ), we report performance on their\nfull test sets. For those with non-public test sets\n(HotPotQA, 2Wiki and MuSiQue), we use their\nfull development sets as test sets and report the cor-\nresponding performance. Since these three datasets\n18980\nPrompt Used for Knowledge Triple Extraction\nInstruction: You are a knowledge graph con-\nstructor tasked with extracting knowledge triples\nin the form of <head entity; relation; tail entity>\nfrom a document. Each triple denotes a specific\nrelationship between entities or an event. The\nhead entity and tail entity can be the provided\ntitle or phrases in the text. If multiple tail entities\nshare the same relation with a head entity, ag-\ngregate these tail entities using commas. Format\nyour output in the form of <head entity; relation;\ntail entity>.\nExamples:\nTitle: Dana Blankstein\nText: Dana Blankstein- Cohen( born March 3, 1981)\nis the director of the Israeli Academy of Film and\nTelevision. She is a film director, and an Israeli cul-\nture entrepreneur.\nKnowledge Triples: <Dana Blankstein; full name;\nDana Blankstein-Cohen>, <Dana Blankstein; birth\ndate; March 3, 1981>, <Dana Blankstein; nation-\nality; Israeli>, <Dana Blankstein; position; direc-\ntor of the Israeli Academy of Film and Television>,\n<Dana Blankstein; profession; film director, culture\nentrepreneur>\nInputs:\nTitle: {document title}\nText: {document text}\nKnowledge Triples:\nFigure 7: Prompt used for extracting knowledge triples.\nare also used for training, we randomly select 500\nquestions from their original training sets to serve\nas development sets, while the remaining questions\nare used for training. The statistics of experimen-\ntal datasets can be found in Table 6. Moreover, in\nKiRAG, we precompute knowledge triples for all\nthe documents in the corpus. The statistics of the\nresulting KG corpus are also provided in Table 6.\nB.2 Baselines\nStandard RAG model follows the vanilla retriever-\nreader pipeline, where the retriever model first\nretrieves top-K documents from the corpus and\nthe reader model then generates answers based\non these retrieved documents. For IRCoT and\nFLARE, we use the implementations provided by\nFlashRAG (Jin et al., 2024). For other models, in-\ncluding DRAGIN, BeamDR and Vector-PRF, we\nPrompt Used by Reasoning Chain Constructor\nInstruction: Follow the examples to answer the\ninput question by reasoning step-by-step. Output\nboth reasoning steps and the answer.\nExamples:\nQuestion: Consider the racer for whom the bend at\nthe 26th Milestone, Isle of Man is dedicated. When\nwere they born?\nThought: <26th Milestone, Isle of Man; named after;\nJoey Dunlop>,<Joey Dunlop; date of birth; 25 Febru-\nary 1952>. So the answer is 25 February 1952.....\nInputs:\nContext: {candidate triples}\nQuestion: {question}\nThought: {i-th step reasoning chain}\nFigure 8: Prompt used by Reasoning Chain Constructor.\nPrompt Used for Answer Generation\nInstruction: Given some context and a question,\nplease only output the answer to the question.\nInputs:\nContext: {retrieved documents}\nQuestion: {question}\nAnswer:\nFigure 9: Prompt used by the Reader.\nadapt the code released by their authors to align\nwith our experimental setup. Notably, for fair com-\nparison, both our KiRAG and baselines use the\nsame retriever for retrieving documents from the\ncorpus and the same reader for generating answers.\nB.3 Training and Hyperparameter Details\nTraining Data Construction. We generate train-\ning data for the Reasoning Chain Aligner using\nexisting multi-hop QA datasets. Specifically, for\neach multi-hop question and its ground-truth rel-\nevant documents, we apply TRACE (Fang et al.,\n2024b) (using the default hyperparameter setting)\nto construct five potential knowledge triple-based\nreasoning chains for answering the question. For\neach chain, we use Llama3 as the reader to gener-\nate an answer based on the context provided by the\nchain. The first chain that successfully produces the\ncorrect answer is selected as the ground-truth rea-\nsoning chain for that question. The question and its\nground-truth reasoning chain will serve as labelled\ndata for training. We filter out questions where all\nreasoning chains fail to produce the correct answer.\nIn practice, we build training data from thetraining\n18981\nHotPotQA 2Wiki MuSiQue Bamboogle WebQA NQ\nTrain Dev. Test Train Dev. Test Train Dev. Test Test Test Test\nStatistics of Experimental Datasets\n# Questions 89,947 500 7,405 166,954 500 12,576 19,438 500 2,417 125 2,032 3,610\nStatistics of Retrieval Corpus\nCorpus HotPotQA 2WikiMultiHopQA MuSiQue Wikipedia Wikipedia Wikipedia# Documents 5M 431K 117K 21M 21M 21M\nStatistics of the Extracted Knowledge Graph Corpus\nAvg. # Entities per Document6.93 8.16 9.40 11.12 11.12 11.12Avg. # Triples per Document5.91 7.32 8.20 08.33 08.33 08.33\nTable 6: Statistics of experimental datasets, retrieval corpus, and pre-computed knowledge graph corpus.\nTrain Dev.\n# Questions 115,567 815\nAvg. Chain Length 2.36 2.35\nTable 7: Statistics of the data used for training the Rea-\nsoning Chain Aligner.\nsets of three multi-hop QA datasets: HotPotQA,\n2Wiki and MuSiQue. In addition, we use the same\nprocedure to construct development data from the\ndevelopment sets of these three datasets for hyper-\nparameter tuning. The statistics of the data used to\ntrain the Aligner are presented in Table 7.\nTraining Details. For an incomplete reasoning\nchain r, we treat the correct next triple as the pos-\nitive sample. To generate negative samples, we\nfollow the procedure described in “Knowledge De-\ncomposition” section to obtain a set of candidate\ntriples ˜Ti. The training process uses the Adam\noptimizer (Kingma and Ba, 2015) with a learning\nrate of 2e-5 and a weight decay of 0.01. We set\nthe batch size to 64, include 7 negative samples\nper data point, and use a temperature parameter τ\nof 0.01. The Aligner is trained for 10 epochs, and\nwe select the checkpoint with the best performance\n(R@5) on the development set.\nImplementation and Hyperparameter Details.\nThroughout the experiments, we set the maximum\nnumber of iterative stepsLto 5. The details of each\ncomponent in our KiRAG are outlined as follows:\nFor the Retriever model, we use either E5 (Wang\net al., 2022) or BGE (Xiao et al., 2024) to retrieve\ndocuments. The number of retrieved documents\nper iteration (i.e., K0) is 10. For the Knowledge\nDecomposition component, we use Llama3 (Dubey\net al., 2024) to extract knowledge triples for each\nretrieved document. For the Reasoning Chain\nAligner, given the question and partial reasoning\nchain, it selects top-20 (i.e., N = 20) knowledge\ntriples that are likely to extend the existing chain.\nModel Huggingface Checkpoint\nE5 intfloat/e5-large-v2\nBGE BAAI/bge-large-en-v1.5\nLlama3 meta-llama/Meta-Llama-3-8B-Instruct\nMistral mistralai/Mistral-7B-Instruct-v0.2\nGemma2 google/gemma-2-9b-it\nQwen2.5 Qwen/Qwen2.5-7B-Instruct\nFlan-T5 google/flan-t5-xl\nTable 8: The specific huggingface checkpoints used in\nour experiments.\nFor the Reasoning Chain Constructor , we try\ndifferent LLMs, including Llama3, Mistral (Jiang\net al., 2023a) and Gemma2 (Team et al., 2024), to\nselect a triple to extend the partial reasoning chain\nfor subsequent retrieval. We main report the per-\nformance of using Llama3 as the Constructor as it\nachieves the best performance (see Appendix C.8).\nMoreover, when completing the partial reasoning\nchain, we filter triples that are not present in the\nprovided candidate set to ensure factual reliability.\nMoreover, we leverage different readers to eval-\nuate the QA performance, which includes Llama3,\nQwen2.5 (Yang et al., 2024), Flan-T5 (Chung et al.,\n2024) and TRACE (Fang et al., 2024b). The spe-\ncific huggingface checkpoints we used in our ex-\nperiments are provided in Table 8.\nC Additional Experimental Results and\nAnalysis\nC.1 Overall Performance of Using Different\nRetrievers and Readers\nTo validate the effectiveness of KiRAG, we pro-\nvide additional results using different retrievers and\nreaders. Specifically, we replace the E5 Retriever\nwith the BGE Retriever for retrieving documents\nfrom the corpus and the other components remain\nunchanged. The corresponding retrieval and QA\nperformance are presented in Table 9 and Table 10,\nrespectively. The results are consistent with those\n18982\nModel HotPotQA 2Wiki MuSiQue\nR@3 R @5 R @3 R @5 R @3 R @5\nRAG 64.46∗ 69.71∗ 60.50∗ 64.91∗ 39.40∗ 45.16∗\nVector-PRF64.38∗ 69.34∗ 60.19∗ 64.37∗ 39.16∗ 43.86∗\nFLARE 53.63∗ 58.83∗ 60.28∗ 69.30∗ 37.10∗ 43.16∗\nDRAGIN 71.71∗ 76.93∗ 62.42∗ 71.14∗ 45.78∗ 52.44∗\nIRCoT 69.96∗ 75.62∗ 60.20∗ 72.23∗ 42.13∗ 48.91∗\nKiRAG-Doc52.42∗ 67.81∗ 41.42∗ 56.55∗ 28.64∗ 39.82∗\nKiRAG-Sent47.81∗ 62.99∗ 41.42∗ 56.55∗ 28.27∗ 38.26∗\nKiRAG 79.69 † 83.61† 78.50† 88.94† 52.62† 58.39†\nTable 9: Retrieval performance (%) using BGE as the re-\ntriever model, where the best and the second-best results\nare marked in bold and underlined, respectively, and†\ndenotes p-value<0.05 compared to the best-performing\nbaseline. Results for BeamDR are omitted as it relies\non its own trained BERT model for retrieval, yielding\nthe same results as presented in Table 1.\nModel HotPotQA 2Wiki MuSiQue\nEM F1 EM F1 EM F1\nRAG 34.11∗ 46.68∗ 14.73∗ 30.16∗ 08.77∗ 17.01∗\nVector-PRF33.94∗ 46.58∗ 14.70∗ 30.00∗ 08.65∗ 16.97∗\nBeamDR 38.34∗ 51.64∗ 14.42∗ 27.25∗ 07.08∗ 14.42∗\nFLARE 34.49∗ 46.65∗ 25.01∗ 40.59∗ 13.07∗ 21.38∗\nDRAGIN 41.73∗ 55.68∗ 24.62∗ 40.69∗ 16.43∗ 26.29∗\nIRCoT 43.18∗ 57.08∗ 24.25∗ 40.12∗ 14.89∗ 23.99∗\nKiRAG-Doc30.47∗ 42.52∗ 11.97∗ 23.97∗ 07.03∗ 14.59∗\nKiRAG-Sent30.44∗ 42.00∗ 12.82∗ 25.22∗ 08.56∗ 16.16∗\nKiRAG 45.16 † 59.85† 35.02† 54.01† 18.87† 29.17†\nTable 10: QA performance (%) using BGE as the Re-\ntriever. The best and second-best performance are high-\nlighted in bold and underlined, respectively. †indicates\np-value<0.05 compared with best-performing baseline.\nobtained using the E5 Retriever, demonstrating the\nadaptability and effectiveness of our KiRAG across\ndifferent retriever models.\nMoreover, to assess the quality of the documents\nretrieved by KiRAG, we report QA performance\nusing different reader models in Table 11. The re-\nsults suggest that KiRAG consistently outperforms\nall the baselines across different readers, demon-\nstrating its ability to provide high-quality retrieval\nresults that enhance downstream QA performance.\nC.2 QA Performance based on Top-5\nDocuments\nTable 12 presents the QA performance using the\ntop-5 retrieved documents as the context, demon-\nstrating similar results to those obtained with the\ntop-3 retrieved documents.\nC.3 Retrieval Performance at Different Steps\non MuSiQue Dataset\nFigure 10 presents the retrieval performance of\nKiRAG and baseline methods at different steps\non the MuSiQue dataset, showing similar trends\nReader Model HotPotQA 2Wiki MuSiQue\nEM F1 EM F1 EM F1\nQwen2.5\nRAG 34.69∗ 46.15∗ 33.37∗ 38.51∗ 09.14∗ 17.17∗\nVector-PRF34.54∗ 46.12∗ 33.41∗ 38.51∗ 08.90∗ 16.91∗\nBeamDR39.61∗ 51.51∗ 22.41∗ 29.95∗ 06.70∗ 14.06∗\nFLARE 36.30∗ 47.33∗ 36.73∗ 44.38∗ 12.58∗ 21.22∗\nDRAGIN44.07∗ 56.91∗ 36.75∗ 44.49∗ 18.16∗ 28.68∗\nIRCoT 43.44∗ 56.46∗ 38.39∗ 45.97∗ 15.60∗ 25.36∗\nKiRAG-Doc35.68∗ 47.15∗ 29.23∗ 34.51∗ 07.61∗ 15.27∗\nKiRAG-Sent34.21∗ 45.64∗ 29.78∗ 34.92∗ 09.64∗ 17.83∗\nKiRAG 47.89† 61.41† 47.42† 56.02† 19.73† 30.79†\nFlan-T5\nRAG 37.08∗ 47.32∗ 17.42∗ 22.05∗ 08.94∗ 15.06∗\nVector-PRF37.02∗ 47.23∗ 31.58∗ 36.26∗ 08.98∗ 15.16∗\nBeamDR41.89∗ 52.83∗ 18.73∗ 23.29∗ 07.03∗ 12.21∗\nFLARE 39.81∗ 50.46∗ 33.31∗ 39.95∗ 13.28∗ 19.74∗\nDRAGIN46.75∗ 58.52∗ 34.19∗ 40.68∗ 18.12∗ 25.22∗\nIRCoT 47.32∗ 59.05∗ 35.90∗ 42.31∗ 16.42∗ 23.61∗\nKiRAG-Doc36.18∗ 46.45∗ 25.03∗ 29.58∗ 07.45∗ 13.53∗\nKiRAG-Sent38.86∗ 49.59∗ 31.27∗ 36.58∗ 11.34∗ 17.98∗\nKiRAG 49.31† 61.38† 39.99† 46.51† 19.07† 27.29†\nTRACE\nRAG 39.18† 51.82† 21.10† 34.28† 11.63∗ 19.49∗\nVector-PRF38.85∗ 51.54∗ 21.91∗ 34.81∗ 11.58∗ 19.59∗\nBeamDR43.21∗ 56.28∗ 21.01∗ 33.23∗ 10.67∗ 18.26∗\nFLARE 39.31† 51.40† 31.85∗ 45.50∗ 14.89∗ 23.75∗\nDRAGIN44.29∗ 57.64∗ 31.88∗ 45.55∗ 18.11∗ 27.41∗\nIRCoT 45.29∗ 58.77∗ 32.05∗ 46.55∗ 16.84∗ 25.78∗\nKiRAG-Doc45.36∗ 58.82∗ 29.41∗ 43.99∗ 13.69∗ 22.44∗\nKiRAG-Sent43.38∗ 56.68∗ 27.51∗ 41.92∗ 16.84∗ 25.59∗\nKiRAG 46.41† 60.22† 33.13† 48.49† 19.32† 29.10†\nTable 11: QA performance (%) using different Reader\nmodels, where †indicates p-value<0.05 compared with\nbest-performing baseline.\nModel HotPotQA 2Wiki MuSiQue\nEM F1 EM F1 EM F1\nRAG 34.34∗ 47.72∗ 12.95∗ 29.94∗ 09.43∗ 17.50∗\nVector-PRF34.56∗ 47.87∗ 13.76∗ 30.42∗ 10.10∗ 17.74∗\nBeamDR 38.60∗ 52.24∗ 14.49∗ 28.71∗ 07.74∗ 15.13∗\nFLARE 35.08∗ 48.07∗ 24.87∗ 41.95∗ 13.20∗ 22.23∗\nDRAGIN 41.16∗ 55.48∗ 24.47∗ 41.37∗ 17.54∗ 27.74∗\nIRCoT 41.61∗ 56.01∗ 24.89∗ 42.90∗ 14.85∗ 24.48∗\nKiRAG-Doc36.87∗ 50.68∗ 15.54∗ 32.17∗ 09.23∗ 17.50∗\nKiRAG-Sent36.58∗ 50.09∗ 15.49∗ 31.59∗ 12.16∗ 20.66∗\nKiRAG 43.81 † 58.42† 27.26† 47.59† 17.58 28.92†\nTable 12: QA performance (%) using top-5 retrieved\ndocument as context. The best and second-best results\nmarked in bold and underlined, respectively. †denote\np-value<0.05 compared with best-performing baseline.\nto those observed on the HotPotQA and 2Wiki\ndatasets.\nC.4 QA Performance on Unseen Datasets\nDue to page limit, we present the QA performance\non unseen multi-hop and single-hop QA datasets\nin Table 13, which aligns with the retrieval perfor-\nmance reported in Table 3. The results highlight\nthat KiRAG can effectively generalise to different\ntypes of QA tasks, maintaining high performance\nwithout overfitting to specific training data.\nC.5 Details and Examples of Manually\nLabelled Relevant Knowledge Triples\nTo quantitatively evaluate the quality of knowledge\ntriples retrieved using our proposed knowledge-\n18983\nModel Bamboogle WebQA NQ\nEM F1 EM F1 EM F1\nRAG 15.20∗ 22.66∗ 18.41∗ 31.04∗ 35.93∗ 41.18∗\nVector-PRF15.20∗ 23.73∗ 18.31∗ 31.02∗ 36.09∗ 41.25∗\nBeamDR 11.20∗ 15.29∗ 15.50∗ 25.46∗ 21.63∗ 25.31∗\nFLARE 24.00∗ 31.93∗ 20.57∗ 31.47∗ 31.22∗ 35.17∗\nDRAGIN 26.20∗ 37.68∗ 20.37∗ 32.31∗ 35.43∗ 39.87∗\nIRCoT 21.60∗ 33.69∗ 19.39∗ 31.31∗ 37.34∗ 42.50∗\nKiRAG-Doc17.60∗ 27.92∗ 18.36∗ 30.75∗ 33.63∗ 39.22∗\nKiRAG-Sent16.00∗ 28.15∗ 19.14∗ 31.27∗ 33.60∗ 38.12∗\nKiRAG 29.60 † 42.00† 20.67∗ 32.87∗ 36.29∗ 41.49∗\nTable 13: QA performance (%) on unseen multi-hop and\nsingle-hop QA datasets, where †denotes p-value<0.05\ncompared with best-performing baselines.\n1-Step 2-Step 3-Step\n(a) MuSiQue\n15\n30\n45\n60\n75R@3 (%)\n1-Step 2-Step 3-Step\n(b) MuSiQue\n15\n30\n45\n60\n75R@5 (%)\nKiRAG-Sent KiRAG-Doc RAG IRCoT KiRAG\nFigure 10: Retrieval performance (%) at different steps\non MuSiQue dataset.\ndriven iterative retrieval framework, we manually\nlabel relevant knowledge triples for 100 questions\nrandomly sampled from the 2Wiki dataset. Specifi-\ncally, for each multi-hop question and its ground-\ntruth relevant documents, we use Llama3 to extract\nknowledge triples from these relevant documents,\nand then manually select a subset of knowledge\ntriples that directly support answering the question.\nWe provide some examples of the manually curated\ndata in Table 14.\nC.6 Effect of the Number of Initially\nRetrieved Documents\nDuring the iterative retrieval process of KiRAG, the\nRetriever model initially retrieves K0 documents\nfrom the corpus, from which relevant knowledge\ncan be extracted. To examine the impact of K0, we\nvary its value from 10 to 100. Figure 11 illustrates\nthe retrieval performance of KiRAG under differ-\nent values of K0 on the development sets of three\n78\n81\n84\n20 40 60 80 100\nNumber of Documents\n61\n63\n65\n84\n88\n92\n20 40 60 80 100\nNumber of Documents\n67\n71\n75\nR@3 (%)\nR@5 (%)\nHotPotQA 2WikiMultiHopQA MuSiQue\nFigure 11: Retrieval performance (%) of KiRAG under\ndifferent values of K0 on three multi-hop QA datasets.\n75\n80\n85\n10 15 20 25 30 35 40\nNumber of Candidate Triples\n60\n65\n84\n87\n90\n10 15 20 25 30 35 40\nNumber of Candidate Triples\n65\n70\nR@3 (%)\nR@5 (%)\nHotPotQA 2WikiMultiHopQA MuSiQue\nFigure 12: Retrieval performance (%) of KiRAG under\ndifferent values of N on three multi-hop QA datasets.\nmulti-hop QA datasets. The results indicate that\nincreasing K0 beyond a certain point can degrade\nperformance. This occurs because a larger docu-\nment pool raises the likelihood of including noisy\nor irrelevant knowledge triples, making it more\nchallenging for the Reasoning Chain Aligner to\naccurately identify the triples essential for answer-\ning multi-hop questions. Therefore, it is crucial\nto select a proper K0 to achieve superior retrieval\nperformance.\nC.7 Effect of the Number of Candidate\nTriples\nIn the iterative retrieval process of KiRAG, the Rea-\nsoning Chain Aligner selects N knowledge triples\nthat are most likely to form a coherent reasoning\nchain with the existing chain. To investigate the\neffect of N, we vary its value from 10 to 40. Fig-\nure 12 shows the retrieval performance of KiRAG\nunder different values of N on the development\nsets of three multi-hop QA datasets. The results\nindicate that KiRAG is not sensitive to the value\nof N, as the performance remains relatively sta-\nble across different values. This stability can be\nattributed to the powerful reasoning and contex-\ntual understanding abilities of the Reasoning Chain\nConstructor, which effectively identifies the most\nuseful triple even from a potentially noisy set of\ncandidate triples.\nC.8 Performance of Using Different\nLLM-Based Constructors\nKiRAG leverages a frozen LLM as the Reasoning\nChain Constructor to maintain the adaptability of\nour framework. Figure 15 presents the retrieval\nperformance of our KiRAG using different LLM-\nbased constructors. The results indicate that Ki-\nRAG consistently outperforms IRCoT across dif-\nferent constructors, indicating the robustness of\nour approach in improving retrieval performance\nregardless of the specific LLM used.\n18984\nQuestion: Which film came out first, Blind Shaft or The Mask Of Fu Manchu?\nRelevant Knowledge Triples: <Blind Shaft; release year; 2003>, <The Mask of Fu Manchu; release year; 1932>\nQuestion: When did John V , Prince Of Anhalt-Zerbst’s father die?\nRelevant Knowledge Triples: <John V , Prince of Anhalt-Zerbst; father; Ernest I, Prince of Anhalt-Dessau>,\n<Ernest I, Prince of Anhalt-Dessau; death date; 12 June 1516>\nQuestion: Which film has the director died first, Crimen A Las Tres or The Working Class Goes To Heaven?\nRelevant Knowledge Triples: <Crimen a las tres; director; Luis Saslavsky>, <The Working Class Goes to Heaven;\ndirector; Elio Petri>, <Luis Saslavsky; death date; March 20, 1995>, <Elio Petri; death date; 10 November 1982>\nQuestion: Who died first, Fleetwood Sheppard or George William Whitaker?\nRelevant Knowledge Triples: <Fleetwood Sheppard; death date; 25 August 1698>, <George William Whitaker;\ndeath date; March 6, 1916>\nQuestion: Who is the spouse of the director of film Eden And After?\nRelevant Knowledge Triples: <Eden and After; director; Alain Robbe-Grillet>, <Alain Robbe-Grillet; spouse;\nCatherine Robbe-Grillet>\nTable 14: Examples of manually labelled relevant knowledge triples for multi-hop questions on the 2Wiki dataset.\nModel HotPotQA 2Wiki MuSiQue\nR@3 R @5 R @3 R @5 R @3 R @5\nIRCoT 71.44 77.57 64.30 75.56 45.61 52.21\nKiRAG (Llama3) 80.32 84.08 77.76 85.3254.53 61.16\nKiRAG (Mistral)74.14 79.51 74.14 82.30 49.10 56.65\nKiRAG (Gemma2)79.66 84.03 77.04 83.5954.82 62.42\nTable 15: Retrieval performance (%) of KiRAG using\ndifferent LLM-based Reasoning Chain Constructor.\nModel HotPotQA 2Wiki MuSiQue\nR@2 R @5 R @2 R @5 R @2 R @5\nHippoRAG59.0∗ 76.2∗ 71.5∗ 89.5∗ 41.0∗ 52.1∗\nKiRAG 70.7 ∗ 88.3∗ 73.1∗ 90.2∗ 51.6∗ 64.3∗\nTable 16: Retrieval performance (%) of KiRAG and\nHippoRAG on three multi-hop QA datasets, where ∗\nindicates the result is taken from the HippoRAG paper.\nC.9 Comparison with Existing KG-Enhanced\nRAG Models\nKiRAG focuses on enhancing the retrieval com-\nponent of iterative RAG (iRAG) models. In con-\ntrast, existing KG-enhanced RAG models such as\nGraphRAG (Edge et al., 2024), LightRAG (Chen\net al., 2024) and TRACE (Fang et al., 2024b) are\nprimarily designed to enhance the Reader compo-\nnent by incorporating knowledge graphs into the\ngeneration process. These models do not explicitly\nretrieve documents and therefore are not directly\ncomparable in terms of retrieval performance.\nHippoRAG (Gutiérrez et al., 2024) is more rel-\nevant to our work, as it also utilises knowledge\ngraphs to enhance document retrieval. However,\nHippoRAG focuses on single-turn retrieval, while\nKiRAG is designed for iterative multi-step retrieval,\nwhich is essential for complex multi-hop QA tasks.\nWe conduct additional experiments to compare Ki-\nRAG and HippoRAG in terms of retrieval perfor-\nmance. Specifically, to ensure a fair comparison,\nwe follow the experimental setup described in Sec-\ntion 3 of the original HippoRAG paper and use the\nContriever model to retrieve documents. Experi-\nmental results on three multi-hop QA datasets are\npresented in Table 16. The results indicate that our\nKiRAG consistently outperforms HippoRAG on\nall the multi-hop QA datasets in R@2 and R@5,\ndemonstrating its superior ability to retrieve rele-\nvant documents for multi-hop reasoning.\n18985",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6963498592376709
    },
    {
      "name": "Information retrieval",
      "score": 0.32881373167037964
    }
  ],
  "institutions": []
}