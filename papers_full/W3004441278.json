{
  "title": "Improved and scalable online learning of spatial concepts and language models with mapping",
  "url": "https://openalex.org/W3004441278",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A1980454158",
      "name": "Akira Taniguchi",
      "affiliations": [
        "Ritsumeikan University"
      ]
    },
    {
      "id": "https://openalex.org/A2118555907",
      "name": "Yoshinobu Hagiwara",
      "affiliations": [
        "Ritsumeikan University"
      ]
    },
    {
      "id": "https://openalex.org/A2172163185",
      "name": "Tadahiro Taniguchi",
      "affiliations": [
        "Ritsumeikan University"
      ]
    },
    {
      "id": "https://openalex.org/A2090525466",
      "name": "Tetsunari Inamura",
      "affiliations": [
        "The Graduate University for Advanced Studies, SOKENDAI",
        "National Institute of Informatics"
      ]
    },
    {
      "id": "https://openalex.org/A1980454158",
      "name": "Akira Taniguchi",
      "affiliations": [
        "Ritsumeikan University"
      ]
    },
    {
      "id": "https://openalex.org/A2118555907",
      "name": "Yoshinobu Hagiwara",
      "affiliations": [
        "Ritsumeikan University"
      ]
    },
    {
      "id": "https://openalex.org/A2172163185",
      "name": "Tadahiro Taniguchi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2090525466",
      "name": "Tetsunari Inamura",
      "affiliations": [
        "Ritsumeikan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W263845233",
    "https://openalex.org/W2567388415",
    "https://openalex.org/W2022231668",
    "https://openalex.org/W2066006005",
    "https://openalex.org/W2121780146",
    "https://openalex.org/W2143654354",
    "https://openalex.org/W2250252080",
    "https://openalex.org/W2125271209",
    "https://openalex.org/W4233071312",
    "https://openalex.org/W159230833",
    "https://openalex.org/W2950350592",
    "https://openalex.org/W2011760672",
    "https://openalex.org/W2130422193",
    "https://openalex.org/W2550725077",
    "https://openalex.org/W2794177543",
    "https://openalex.org/W2801217509",
    "https://openalex.org/W2325094110",
    "https://openalex.org/W2029806546",
    "https://openalex.org/W4235169531",
    "https://openalex.org/W2766934580",
    "https://openalex.org/W2098091224",
    "https://openalex.org/W2218343212",
    "https://openalex.org/W2097627707",
    "https://openalex.org/W2265661972",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2573646622",
    "https://openalex.org/W72347498",
    "https://openalex.org/W2800549968",
    "https://openalex.org/W2140991203",
    "https://openalex.org/W6812684363",
    "https://openalex.org/W2963640382",
    "https://openalex.org/W2142390309",
    "https://openalex.org/W2346811982",
    "https://openalex.org/W2000321478",
    "https://openalex.org/W2802730780",
    "https://openalex.org/W6682569104",
    "https://openalex.org/W2402652555",
    "https://openalex.org/W2407753819",
    "https://openalex.org/W2256409625",
    "https://openalex.org/W2607257024",
    "https://openalex.org/W2767009175",
    "https://openalex.org/W2963475757",
    "https://openalex.org/W6703161083",
    "https://openalex.org/W2586049587",
    "https://openalex.org/W2296721894",
    "https://openalex.org/W2732026016",
    "https://openalex.org/W3102640141",
    "https://openalex.org/W2336416123",
    "https://openalex.org/W2103393159",
    "https://openalex.org/W620953717",
    "https://openalex.org/W2117614116",
    "https://openalex.org/W3103934441",
    "https://openalex.org/W2151967501",
    "https://openalex.org/W3104490327",
    "https://openalex.org/W2976490211",
    "https://openalex.org/W2950094539",
    "https://openalex.org/W2226734577",
    "https://openalex.org/W1967465043",
    "https://openalex.org/W3100945159",
    "https://openalex.org/W2149020252",
    "https://openalex.org/W2155893237"
  ],
  "abstract": null,
  "full_text": "Autonomous Robots (2020) 44:927–946\nhttps://doi.org/10.1007/s10514-020-09905-0\nImproved and scalable online learning of spatial concepts and\nlanguage models with mapping\nAkira Taniguchi 1 · Yoshinobu Hagiwara 1 · Tadahiro Taniguchi 1 · Tetsunari Inamura 2\nReceived: 27 December 2018 / Accepted: 24 January 2020 / Published online: 8 February 2020\n© The Author(s) 2020\nAbstract\nWe propose a novel online learning algorithm, called SpCoSLAM 2.0, for spatial concepts and lexical acquisition with\nhigh accuracy and scalability. Previously, we proposed SpCoSLAM as an online learning algorithm based on unsupervised\nBayesian probabilistic model that integrates multimodal place categorization, lexical acquisition, and SLAM. However, our\noriginal algorithm had limited estimation accuracy owing to the inﬂuence of the early stages of learning, and increased\ncomputational complexity with added training data. Therefore, we introduce techniques such as ﬁxed-lag rejuvenation to\nreduce the calculation time while maintaining an accuracy higher than that of the original algorithm. The results show that,\nin terms of estimation accuracy, the proposed algorithm exceeds the original algorithm and is comparable to batch learning.\nIn addition, the calculation time of the proposed algorithm does not depend on the amount of training data and becomes\nconstant for each step of the scalable algorithm. Our approach will contribute to the realization of long-term spatial language\ninteractions between humans and robots.\nKeywords Online learning · Place categorization · Scalability · Semantic mapping · Lexical acquisition · Unsupervised\nBayesian probabilistic model\n1 Introduction\nRobots operating in various human environments must adap-\ntively and sequentially acquire new categories for places and\nunknown words related to various places as well as the map\nof the environment (Kostavelis and Gasteratos 2015). It is\nThis work was partially supported by JST CREST Grant Number\nJPMJCR15E3, and JSPS KAKENHI Grant Numbers JP17J07842,\nJP16H06561, and JP16K12497.\nB Akira Taniguchi\na.taniguchi@em.ci.ritsumei.ac.jp\nY oshinobu Hagiwara\nyhagiwara@em.ci.ritsumei.ac.jp\nTadahiro Taniguchi\ntaniguchi@em.ci.ritsumei.ac.jp\nTetsunari Inamura\ninamura@nii.ac.jp\n1 Ritsumeikan University, 1-1-1 Noji-Higashi, Kusatsu,\nShiga 525-8577, Japan\n2 The National Institute of Informatics / SOKENDAI (The\nGraduate University for Advanced Studies),\n2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo 101-8430, Japan\ndesirable for robots to acquire place categories and vocab-\nulary autonomously based on their experience because it is\ndifﬁcult to manually design spatial knowledge in advance.\nRelated research in the ﬁelds of semantic mapping and place\ncategorization (Pronobis and Jensfelt 2012; Kostavelis and\nGasteratos 2015; Sünderhauf et al. 2016; Landsiedel et al.\n2017; Rangel et al. 2018) has attracted considerable interest\nin recent years. However, conventional approaches in most\nof these studies are limited insofar as the robots cannot learn\nunknown words and unknown place categories without pre-\nset vocabulary and categories. In addition, the processes for\nSimultaneous Localization And Mapping (SLAM) (Thrun\net al. 2005) and for estimating semantics related to place have\nbeen addressed as separated module processes. However,\nin our proposed approach, the robot can automatically and\nsimultaneously perform place categorization and environ-\nment mapping, and it can learn unknown words without prior\nknowledge. Our previously proposed unsupervised Bayesian\nprobabilistic model integrates multimodal place categoriza-\ntion, lexical acquisition, and SLAM. In particular, this paper\nfocuses on the problems of estimation accuracy and compu-\ntational scalability in online learning.\n123\n928 Autonomous Robots (2020) 44:927–946\nWe deﬁne a spatial concept as a place category is\nautonomously learned by the robot based on multimodal\nperceptual information, which includes names of places, fea-\ntures of scene images, and position distributions. Then, we\ndeﬁne a position distribution as the spatial extent representing\na place in the environment. Our study regarding the spatial\nconcept formation and the lexical acquisition also consti-\ntute constructive approaches to the human developmental\nprocess and symbol emergence in cognitive developmental\nsystems (Cangelosi and Schlesinger 2015; Taniguchi et al.\n2018b). Thus, we assume that the robot has not acquired any\nvocabulary in advance and can recognize only phonemes or\nsyllables. In addition, the robot does not have prior knowl-\nedge of the current environment. In this study, a scenario in\nwhich the user teaches the robot the name of a place using a\nspoken utterance while moving together in the environment\nis studied. An overview of the scenario for online learning\ntask is shown in Fig. 1. The robot and the user move around\nthe environment. When they come to a place where the user\nwishes to teach, the user speaks a sentence regarding the\nplace to the robot. The robot recognizes the speech, includ-\ning unknown words, and segments the speech into words.\nThen, the robot obtains the present estimated position, the\nscene image, and the speech signal at that time, and acquires\nspatial knowledge regarding the environment, such as the\nrelationship between words and places.\nIn online learning, also called sequential learning or incre-\nmental learning, an increase in scalability without reducing\naccuracy is especially important but difﬁcult to achieve for\nmobile robots. Online learning has the advantage of being\nperformed in real-time. This means that it can be used to\nadapt immediately to new data by sequentially estimating\nparameters each time. On the other hand, batch learning takes\ntime to collect large amounts of data and to iterate it for\nlearning. In the case of online learning, previous knowledge\ncan be used immediately for reasoning and tasks such as\nlanguage communication. Taniguchi et al. ( 2017) focused\non deriving and constructing an appropriate online learn-\ning algorithm mathematically based on a theory of machine\nlearning. In our previous work, we proposed SpCoSLAM\nas an integrated model of nonparametric Bayesian multi-\nmodal categorization, a Bayesian ﬁlter-based SLAM, speech\nrecognition, and word segmentation, from the standpoint\nof unsupervised machine learning. However, this algorithm\n(Taniguchi et al. 2017) had inferior accuracy in terms of\ncategorization and word segmentation compared to batch\nlearning, owing to a situation whereby sufﬁcient statistical\ninformation could not be used at the early stages of learning.\nIn addition, speech recognition and unsupervised word seg-\nmentation were not completely online, and batch learning\nwas used as an approximation. Therefore, the computa-\ntional complexity of the processes of speech recognition and\nunsupervised word segmentation increased with an increase\nin training data. To enable online learning based on long-\nterm human–robot interactions with limited computational\nresources, the following core problems need to be solved: (i)\nthe increase in calculation cost owing to an increase in data,\nand (ii) the decrease in estimation accuracy when compared\nwith batch learning. In intelligent robotics, the framework of\nonline learning is regarded as important. In particular, online\nlearning, which solves the above problem, is required for\nrobots that gain knowledge while moving in the real world.\nWe here describe improved and scalable algorithms to\nsolve the above-mentioned problems. The improved algo-\nrithm mainly addresses the problems of misrecognition\n(misclassiﬁcation) and word segmentation in online learn-\ning. The scalable algorithm mainly addresses the problem of\nthe increase in computation time. In this study, we introduce\nthe approach of ﬁxed-lag rejuvenation, which is considered\nparticularly effective at solving these problems. Regarding\nthe problem of online lexical acquisition, the improved and\nscalable algorithms take two respective approaches to the\nsolution. The improved algorithm addresses the problem\nof under-segmentation, whereby the phoneme sequence is\ninsufﬁciently segmented, by changing the manner by which\nthe language model is updated such that it re-segments\nthe word sequence. The scalable algorithm performs in a\npseudo-online manner by introducing a ﬁxed-lag rejuvena-\ntion approach to speech recognition and word segmentation.\nOne of the advantages to the proposed online learning\nalgorithm is that spatial concepts mistakenly learned by the\nrobot can be corrected sequentially, something that could not\nbe achieved thus far. Moreover, with the proposed algorithm,\nthe robot can ﬂexibly deal with changes in the environment\nand the names of places. The lower part in Fig. 1 shows the\nprogress of online learning. In the lower left of Fig. 1,c l u s -\ntered places and words are incorrectly estimated, as shown by\nthe elongated purple and blue ellipses. In the lower right of\nFig. 1, by contrast, more accurate estimation is achieved by\ncorrecting errors as learning progresses. This is realized by\nreviewing and rethinking previous estimation results when\nnew data is obtained.\nThe main contributions of this paper are as follows:\n– We propose an improved and scalable online learning\nalgorithm with several novel techniques such as ﬁxed-\nlag rejuvenation.\n– The improved online algorithm achieves an accuracy of\nplace categorization and lexical acquisition comparable\nto batch learning.\n– The scalable online algorithm achieves faster learning\ncompared to original algorithms by reducing the order of\ncomputational complexity.\nThe remainder of this paper is organized as follows. In\nSect. 2, we discuss related work on the formation of spatial\n123\nAutonomous Robots (2020) 44:927–946 929\nFig. 1 Overview of the scenario for online learning in this study. We\nassume a scenario in which the user teaches the robot the name of the\nplace using a spoken utterance while moving together in the environ-\nment. The robot learns spatial concepts, language models, and maps\nwhile sequentially correcting mistakes from previous learnings based\non its interaction with the user and environment, as shown from the\nbottom left to the bottom right\nconcepts and online learning that is relevant to our study.\nIn Sect. 3, we present an overview of the model, along with\nthe formulation and the original online learning algorithm,\nSpCoSLAM. In Sect. 4, we present our proposed algorithms\nfor improved and scalable online learning. In Sect. 5,w ed i s -\ncuss the effectiveness of the proposed algorithms in a real\nenvironment. In Sect. 6, we evaluate the performance of place\ncategorization and lexical acquisition in various virtual home\nenvironments. Section 7 concludes the paper.\n2 Related work\n2.1 Spatial concept formation\nTaguchi et al. ( 2011) proposed an unsupervised method\nfor simultaneously categorizing self-positions and phoneme\nsequences from user speech without any prior language\nmodel. Taniguchi et al. ( 2016, 2018a) proposed the non-\nparametric Bayesian Spatial Concept Acquisition method\n(SpCoA) using an unsupervised word segmentation method,\nlatticelm (Neubig et al. 2012), and SpCoA++ for highly accu-\nrate lexical acquisition as a result of updating the language\nmodel. Gu et al. ( 2016) proposed a method to learn rela-\ntive spatial concepts, i.e., the words related to distance and\ndirection, from the positional relationship between an utterer\nand objects. Isobe et al. ( 2017) proposed a learning method\nto derive the relationship between objects and places using\nimage features obtained by a Convolutional Neural Network\n(CNN) (Krizhevsky et al. 2012). Hagiwara et al. ( 2018)\nimplemented a hierarchical clustering method for the for-\nmation of hierarchical place concepts. However, none of the\nabove methods can sequentially learn spatial concepts from\nunknown environments without a map, because they rely\non batch-learning algorithms. Therefore, we developed in\nprevious work an online algorithm, SpCoSLAM (Taniguchi\net al. 2017), that can sequentially learn a map, a lexicon, and\nspatial concepts to integrate positions, speech signals, and\nscene images. In Taniguchi et al. ( 2017), however, the accu-\nracy was inferior to that of SpCoA. In this paper, we also\ncompare our proposal to the latest batch learning method,\nSpCoA++. Because SpCoA++ is able to achieve nearly cor-\nrect lexical acquisition, if we can successfully overcome the\nabove problems by appropriately devising the learning algo-\nrithm, its accuracy should improve even with online lexical\nacquisition.\nOur approach is relevant to research integrating seman-\ntic mapping with natural language processing (Walter et al.\n2013; Hemachandra et al. 2014). Walter et al. ( 2013)d e v e l -\noped an algorithm that can learn semantic graphs to integrate\nsemantic representation into metric maps from natural lan-\nguage descriptions of aspects such as labels and spatial rela-\ntionships. Hemachandra et al. ( 2014) proposed a mechanism\nto more effectively ground natural language descriptions\n123\n930 Autonomous Robots (2020) 44:927–946\nby integrating scene appearance observations using camera\nimages and laser data. In these studies, a word list, place\nlabels, and the number of category types were known in\nadvance. However, it is challenging to sequentially acquire\nnew words and categories efﬁciently from a situation in\nwhich the lists of words and categories are not provided in\nadvance. Our study includes lexical acquisition for unknown\nwords and formation of new categories from speech signals\nusing spatial information.\nBall et al. ( 2013) implemented a biologically inspired\nmapping system, RatSLAM, which is related to pose cells\nin the hippocampus of a rodent. In addition, robots called\nLingodroids using RatSLAM could acquire a lexicon related\nto places through robot-to-robot communication (Heath et al.\n2016). These studies reported that robots created their own\nvocabulary. Ueda et al. ( 2016) proposed a brain-inspired\nmethod, namely, a Particle Filter on Episode (PFoE) for agent\ndecision making. PFoE can estimate the agent’s internal state\nbased on previous events recalled at the time. All previ-\nous data is thus accumulated to construct a state space in\nPFoE. We believe that PFoE is unsuitable for long-term tri-\nals because the state space becomes enormous. By contrast,\nour approach forms concepts from episodes using resources\nmore reasonably for calculations, insofar as the state space is\nreduced through clustering. Although our proposed method\nwas not originally inspired by biology or brain science, such\nresearch is highly suggestive. SpCoSLAM is an integrated\nmodel of self-localization, mapping, concept formation, and\nlexical acquisition. From the point of view of the brain, it may\nbe possible to regard SpCoSLAM as a model that imitates\nsome functions of the hippocampus and the cerebral cortex.\nIf we assume that the training data—i.e., the robot’s experi-\nences based on a user’s utterances—is the episodic memory,\nand that spatial concepts are semantic memory, the proposed\nalgorithm can be interpreted as a representation of the process\nof forming concepts by extracting meaning from short-term\nepisodic memory sequentially. Such matters are not further\ndiscussed in this paper, although they remain important for\nfuture research.\n2.2 Improvement of online learning based on\nparticle filters in unsupervised Bayesian models\nAs an approach involving Bayesian models that is similar to\nour model, there are related studies on object concepts. In\nparticular, Araki et al. ( 2012b) proposed online Multimodal\nLatent Dirichlet Allocation (oMLDA) to acquire object con-\ncepts in an online manner, and combined this with the Nested\nPitman–Y or Language Model (NPYLM), making it possible\nto perform lexical acquisition of unknown words sequen-\ntially. Aoki et al. ( 2016) constructed an algorithm that can\ninfer an approximately global optimal solution by represent-\ning it as a single integrated model. The NPYLM is an unsu-\npervised morphological analysis method based on a statisti-\ncal model that enables word segmentation exclusively from\nphoneme sequences (Mochihashi et al. 2009). In addition,\nNishihara et al. ( 2017) was able to reduce phoneme recogni-\ntion errors by applying PFoMDLA to inferences using a parti-\ncle ﬁlter instead of oMLDA. In these studies, online learning\nwas realized as an algorithm in unsupervised machine learn-\ning. A spatial concept requires more real-time processing\nthan an object concept because the robot learns spatial con-\ncepts while it moves through the environment. The mobile\nrobot should not halt its spatial movement for calculations.\nTherefore, a more efﬁcient and scalable algorithm is required.\nCanini et al. ( 2009) improved the accuracy of an online\nalgorithm based on a particle ﬁlter with the rejuvenation tech-\nnique. This technique resamples some randomly selected\nsamples of previous observation data from a conditional\nprobabilistic distribution similar to Gibbs sampling. For a\ncompletely random choice, the robot needs to memorize all\nof the previous data. Rejuvenation can deal with the problem\nof degenerating particles in particle ﬁlters. In this study, we\nintroduce rejuvenation into our SpCoSLAM online learning\nalgorithm. In our algorithm, we perform resampling from\nsome recent data. Therefore, we consider that it will be pos-\nsible to improve the estimation accuracy efﬁciently.\nAs another particle ﬁlter approach, Börschinger and John-\nson (2011) proposed an online algorithm based on a Bayesian\nmodel for word segmentation. In addition, Börschinger and\nJohnson ( 2012) presented an incremental learning algo-\nrithm that introduces rejuvenation to a particle ﬁlter. They\nimproved the performance of word segmentation with higher\naccuracy. The studies above were premised on segmenta-\ntion of sequences without phoneme recognition errors. In\nthis study, by contrast, the online word segmentation task is\nparticularly challenging because phoneme recognition errors\nare included in speech recognition results.\n3 SpCoSLAM: Online learning for spatial\nconcepts and lexical acquisition with\nmapping\n3.1 Overview\nSpCoSLAM has the advantage that spatial concept for-\nmation, lexical acquisition, and SLAM, can be performed\nsimultaneously by an integrated model. Figure 2 shows the\ngraphical model of SpCoSLAM and lists each variable of the\ngraphical model. The details of the formulation of the genera-\ntion process represented by the graphical model are described\nin Taniguchi et al. ( 2017). The method learns sequential spa-\ntial concepts for unknown environments without maps. It also\nlearns the many-to-many correspondences between places\nand words via spatial concepts and can mutually complement\n123\nAutonomous Robots (2020) 44:927–946 931\n1−tx xt xt +1\ntz\ntu\ntC\nti\nα\nlφ γ\nβ\n00,κm\n00,νV\n1−tz\n1−tu\n1+tz\n1+tu\nπ\nLMtyAM\ntS\nλ\nlθtf χ\nm\nlW\nkμ\nkΣ\nSymbol Deﬁnition\nm Environmental map\nxt Self-position of a robot\nzt Depth data\nut Control data\nft Image feature\nyt Speech signal\nit Index of position distributions\nCt Index of spatial concepts\nSt Word sequence (word segmentation result)\nμk , Σk Parameters of Gaussian distribution (position distribution)\nπ Parameter of multinomial distribution for index Ct of spatial\nconcepts\nφl Parameter of multinomial distribution for index it of the po-\nsition distribution\nθl Parameter of multinomial distribution for the image feature\nWl Parameter of multinomial distribution for the names of places\nLM Language model (word dictionary)\nAM Acoustic model for speech recognition\nα, β, γ, χ, λ, Hyperparameters of prior distributions\nm0, κ0, V0, ν0\nFig. 2 Left: graphical model representation of SpCoSLAM (Taniguchi et al. 2017). Gray nodes indicate observation variables, and white nodes are\nunobserved latent variables. Right: description of random variables in SpCoSLAM\nthe uncertainty of information using multimodal information.\nFurthermore, the proposed method estimates an appropriate\nnumber of clusters of spatial concepts and position distri-\nbutions depending on the data by using the so-called online\nChinese Restaurant Process (CRP) (Aldous 1985), one of the\nconstitutive methods of the Dirichlet Process (DP). In addi-\ntion, lexical acquisition including unknown words is possible\nby sequentially updating the language model.\nThe procedure of SpCoSLAM for each step is described as\nfollows. (a) The robot obtains Weighted Finite-State Trans-\nducer (WFST) speech recognition results from the user’s\nspeech signals using a language model. (b) The robot obtains\nthe likelihood of self-localization by performing FastSLAM.\n(c) The robot segments the WFST speech recognition results\nusing an unsupervised word segmentation approach called\nlatticelm (Neubig et al. 2012). (d) The robot obtains the\nlatent variables of spatial concepts by sampling. (e) The robot\nobtains the marginal likelihood of the observed data as the\nimportance weight. (f) The robot updates the environmental\nmap. (g) The robot estimates the set of model parameters\nof the spatial concepts from the observed data and the sam-\npled variables. (h) The robot updates the language model\nof the maximum weight for the next step. (i) The particles\nare resampled according to their weights. Steps (b)–(g) are\nperformed for each particle.\n3.2 Formulation of the online learning algorithm\nOur previously proposed online learning algorithm,\nSpCoSLAM, introduces sequential equation updates to esti-\nmate the parameters of the spatial concepts into the for-\nmulation of a Rao-Blackwellized Particle Filter (RBPF)\n(Doucet et al. 2000) in the FastSLAM 2.0 algorithm, which\nis landmark-based SLAM (Montemerlo et al. 2003), and the\ntechnique (Grisetti et al. 2007) applied to grid-based SLAM\nin a similar manner to that in FastSLAM 2.0. The parti-\ncle ﬁlter is advantageous in that parallel processing can be\neasily applied because each particle can be calculated inde-\npendently.\nIn the formulation of SpCoSLAM, the joint posterior dis-\ntribution can be factorized to the probability distributions of\na language model LM ,am a p m, the set of model parame-\nters of spatial concepts Θ ={ W,μ,Σ,θ,φ,π }, the joint\ndistribution of the self-position trajectory x\n0:t , and the set of\nlatent variables C1:t ={ i1:t ,C1:t , S1:t }. We describe the joint\nposterior distribution of SpCoSLAM as follows:\np(x0:t ,C1:t , LM ,Θ, m | u1:t , z1:t , y1:t , f1:t , AM ,h)\n= p(LM | S1:t ,λ )p(Θ | x0:t ,C1:t , f1:t ,h)p(m | x0:t , z1:t )\n· p(x0:t ,C1:t | u1:t , z1:t , y1:t , f1:t , AM ,h)  \nParticle ﬁlter\n(1)\nwhere the set of hyperparameters is denoted by h =\n{α, β, γ, χ, λ,m0,κ0, V0,ν0}. It is noteworthy that the speech\nsignal yt is not observed during all time-steps. Herein, the\nproposed method is equivalent to FastSLAM 2.0 when yt is\nnot observed, i.e., when the speech signal is a trigger for the\nplace categorization.\n3.2.1 Particle ﬁlter algorithm\nThe particle ﬁlter algorithm uses Sampling Importance\nResampling (SIR). The importance weight is denoted by\n123\n932 Autonomous Robots (2020) 44:927–946\nω[r]\nt = P[r]\nt /Q[r]\nt for each particle, where r is the particle\nindex. The target distribution is P[r]\nt , and the proposal distri-\nbution is Q[r]\nt . The number of particles is R. The following\nequations are also calculated for each particle r; however, the\nsubscripts representing the particle index are omitted.\nWe apply two modiﬁcations related to the weighting of\nthe original SpCoSLAM algorithm (Taniguchi et al. 2017):\n(i) additional weight for it , Ct , and xt (AW), and (ii)\nweight for selecting a language model LM (WS). These\nmodiﬁcations are more theoretically reasonable than the orig-\ninal SpCoSLAM model, and our proposed SpCoSLAM 2.0\nonline learning algorithm is extended on their basis.\nWe describe the target distribution Pt that modiﬁed the\nderivation of Taniguchi et al. ( 2017) as follows:\nPt = p(x0:t ,C1:t | u1:t , z1:t , y1:t , f1:t , AM ,h)\n≈ p(it ,Ct | x0:t ,i1:t−1,C1:t−1, S1:t , f1:t ,h)\n· p(zt | xt ,mt−1)p( ft | C1:t−1, f1:t−1,h)\n· p(xt | xt−1,ut )p(St | S1:t−1, y1:t , AM ,λ )\n· p(xt | x0:t−1,i1:t−1,C1:t−1,h)  \nAdditional part\n· p(St | S1:t−1,C1:t−1,α ,β)\np(St | S1:t−1,β) · Pt−1, (2)\nwhere the term p(xt | x0:t−1,i1:t−1,C1:t−1,h) is the addi-\ntional part compared to the original equation.\nHere, the target distribution for the particle ﬁlter is the\nmarginal joint posterior distribution of the self-positions x0:t\nand the set of latent variables C1:t because it is based on the\nRBPF technique adopted in FastSLAM in the same manner.\nThe latent variables that are local parameters are estimated\nby a particle ﬁlter, and the probability distributions for global\nparameters LM , Θ, and m are calculated and held indepen-\ndently for each estimated particle.\nWe describe the proposal distribution Q\nt as follows:\nQt = q(x0:t ,C1:t | u1:t , z1:t , y1:t , f1:t , AM ,h)\n= p(xt | xt−1, zt ,mt−1,ut )\n· p(it ,Ct | x0:t ,i1:t−1,C1:t−1, S1:t , f1:t ,h)\n· p(St | S1:t−1, y1:t , AM ,λ )· Qt−1. (3)\nThen, p(xt | xt−1, zt ,mt−1,ut ) is equivalent to the\nproposal distribution of FastSLAM 2.0. The probability dis-\ntribution of i\nt and Ct is the marginal distribution pertaining to\nthe set of model parameters Θ. This distribution can be calcu-\nlated using a formula equivalent to collapsed Gibbs sampling.\nThe details are described in Taniguchi et al. ( 2017).\n3.2.2 Sampling of words using speech recognition and\nword segmentation\nWe approximate the probability distribution of S\nt in ( 3)a s\nspeech recognition with the language model LM t−1 and\nunsupervised word segmentation using the WFST speech\nrecognition results with latticelm (Neubig et al. 2012)a sf o l -\nlows:\np(S\nt | S1:t−1, y1:t , AM ,λ )\n≈ latticelm(S1:t | L1:t ,λ )SR(L1:t | y1:t , AM , LM t−1)\n(4)\nwhere SR () denotes the function of speech recognition, L1:t\ndenotes the speech recognition results in WFST format,\nwhich is a word graph representing the speech recognition\nresults. In the original mathematical formulas, only St should\nbe obtained by sampling. However, latticelm is a tool orig-\ninally designed for batch learning. In addition, in order to\nperform unsupervised word segmentation, it is necessary\nto extract statistical information from the observation data.\nTherefore, resampling is necessary using all data from 1 to\nt, instead of exclusively using the distribution at time-step t.\n3.2.3 Additional weight for i\nt, Ct, and xt (AW)\nFinally, the importance weight ωt modiﬁed from Taniguchi\net al. ( 2017) is represented as follows:\nωt ≈\n∑\nit =k\n[\np(xt | x0:t−1,i1:t−1,it = k,h)\n·\n∑\nCt =l\np(it = k,Ct = l | C1:t−1,i1:t−1,h)\n]\n  \nAdditional part\n· p(zt | mt−1, xt−1,ut )p( ft | C1:t−1, f1:t−1,h)\n· p(St | S1:t−1,C1:t−1,α ,β)\np(St | S1:t−1,β) · ωt−1. (5)\nUnlike the original SpCoSLAM algorithm, the marginal\nlikelihood for it and Ct weighted by the marginal likelihood\nfor the position distribution was added to the additional part\nof the ﬁrst term on the right side of ( 5). The amount of cal-\nculations does not increase because most of the formulas for\nweight ω\nt are already calculated when it and Ct are sam-\npled. Weight calculation in consideration of the likelihood of\nthe entire model can be realized by ( 5). This is described in\nAlgorithm 1 (Line 16) and Algorithm 2 (Line 17).\n3.2.4 Weight for selecting a language model LM (WS)\nIn the formulation of ( 1), it is desirable to estimate the lan-\nguage model LM\nt for each particle. In other words, speech\n123\nAutonomous Robots (2020) 44:927–946 933\nrecognition of the amount of data multiplied by the number\nof particles for each teaching utterance must be performed.\nIn this paper, to reduce the computational cost, we use a lan-\nguage model LM\nt of a particle with the maximum weight\nfor speech recognition.\nWe also modify the weight for selecting the language\nmodel from the entire weight ωt of the model to the weight\nωS related to word information:\nωS = p(S1:t | C1:t−1,α ,β)\np(S1:t | β) . (6)\nThe segmentation result from all of the uttered sentences\nfor each particle changes at every step because the word\nsegmentation processes use all previous data. Indeed, bet-\nter word segmentation results can be selected by a weight\nthat considers not only current data but also previous data. In\naddition, this modiﬁed weight corresponds to mutual infor-\nmation used for selecting the word segmentation results in\nSpCoA++ (Taniguchi et al. 2018a). This is described in Algo-\nrithm 1 (Line 23) and Algorithm 2 (Line 24).\n4 SpCoSLAM 2.0: improved and scalable\nonline learning algorithm\nIn this section, we describe an improved and scalable online\nlearning algorithm, SpCoSLAM 2.0, that overcomes the\nproblems in the original algorithm. Although the genera-\ntive process and graphical model for SpCoSLAM are the\nsame, the learning algorithm is different. SpCoSLAM 2.0 is\na novel learning algorithm proposed with a modiﬁed mathe-\nmatical formulation that retains the model structure, similar\nto the extension from FastSLAM to FastSLAM 2.0. First,\nthe algorithm is improved by introducing techniques such\nas rejuvenation, as explained in Sect. 4.1. Next, a scalable\nalgorithm is developed to reduce the calculation time while\nmaintaining higher accuracy than the original algorithm, as\ndescribed in Sect. 4.2.\n4.1 Improving the estimation accuracy\nWe now turn to the details of the improved algorithm. Here,\nwe introduce two elements: ﬁxed-lag rejuvenation of latent\nvariables, and re-segmentation of word sequences. A pseudo-\ncode for the improved algorithm is given in Algorithm 1.\n4.1.1 Fixed-lag rejuvenation of it and Ct (FLR–it, Ct)\nCanini et al. ( 2009) demonstrated improved accuracy with\nrejuvenation by resampling previous samples randomly. This\nis based on a result of the independent and identically dis-\ntributed (i.i.d.) assumption on the latent variables in the\nAlgorithm 1 SpCoSLAM 2.0: Improved algorithm\n1: procedure SpCoSLAM2.0(Xt−1,ut , zt , f1:t , y1:t )\n2: ¯Xt = Xt =∅\n3: L1:t = SR(L1:t | y1:t , AM , LM t−1)\n4: for r = 1t o R do\n5: ´x[r]\nt = sample_motion_model(ut , x[r]\nt−1)\n6: x[r]\nt = scan_matching(zt , ´x[r]\nt ,m[r]\nt−1)\n7: for j = 1t o J do\n8: x j = sample_motion_model(ut , x[r]\nt−1)\n9: end for\n10: ω[r]\nz =\nJ∑\nj=1\nmeasurement_model(zt , x j ,m[r]\nt−1)\n11: S[r]\n1:t ∼ latticelm(S1:t | L1:t ,λ )\n12: for τ = t − TL + 1t o t do\n13: i[r]\nτ ,C[r]\nτ ∼ p(iτ ,Cτ | x[r]\n0:t , S[r]\n1:t , f1:t ,\ni[r]\n{1:t|¬τ},C[r]\n{1:t|¬τ},h)\n14: end for\n15: ω[r]\nf = p( ft | C[r]\n1:t−1, f1:t−1,α ,χ)\n16: ω[r]\nic =\n∑\nit =k\n[\np(x[r]\nt | x[r]\n0:t−1,i[r]\n1:t−1,it = k,h)\n·\n∑\nCt =l\np(it = k,Ct = l | C[r]\n1:t−1,i[r]\n1:t−1,h)\n]\n17: ω[r]\ns = p(S[r]\nt | S[r]\n1:t−1,C[r]\n1:t−1,α ,β)\np(S[r]\nt | S[r]\n1:t−1,β)\n18: ω[r]\nt = ω[r]\nz · ω[r]\nf · ω[r]\ns · ω[r]\nic\n19: m[r]\nt = updated_occupancy_grid(zt , x[r]\nt ,m[r]\nt−1)\n20: Θ[r]\nt = E[p(Θ | x[r]\n0:t ,C[r]\n1:t , f1:t ,h)]\n21: ¯Xt = ¯Xt ∪⟨ x[r]\n0:t ,C[r]\n1:t ,m[r]\nt ,Θ [r]\nt ,ω [r]\nt ⟩\n22: end for\n23: S∗\n1:t = argmaxS[r]\n1:t\nR∑\nr=1\nω[r]\nS δ(S1:t − S[r]\n1:t )\n24: LM t ∼ NPYLM(LM | S∗\n1:t ,λ )\n25: for r = 1t o R do\n26: draw i with probability ∝ ω[i]\nt\n27: add ⟨x[i]\n0:t ,C[i]\n1:t ,m[i]\nt ,Θ [i]\nt , LM t ⟩ to Xt\n28: end for\n29: return Xt\n30: end procedure\nLatent Dirichlet Allocation (LDA) model. However, in the\ncase of selecting from previous data of all time points, all\nprevious samples need to be held in the memory. In the\nproposed algorithm, we introduce Fixed-Lag Rejuvenation\n(FLR) inspired by the Monte Carlo ﬁxed-lag smoother (Kita-\ngawa 2014). This approach is similar to the sampling strategy\nof ﬁxed-lag roughening for particle ﬁlter-based SLAM in\nBeevers and Huang ( 2007). Beevers and Huang ( 2007) indi-\ncated that the statistical estimation error could be reduced by\napplying Markov Chain Monte Carlo (MCMC)–based sam-\npling to the trajectory samples over a ﬁxed lag at each time\nstep.\nThe ﬁxed-lag smoother is a particle smoothing method\nthat estimates particles approximating the smoothing distri-\nbution p(C\nτ | D1:t )( τ < t), where D is observed data. It\nis obtained by a simple modiﬁcation to the particle ﬁlter. In\n123\n934 Autonomous Robots (2020) 44:927–946\nAlgorithm 2 SpCoSLAM 2.0: Scalable algorithm\n1: procedure SpCoSLAM2.0(Xt−1,ut , zt , ft′+1:t , yt′+1:t )\n2: ¯Xt = Xt =∅\n3: t′ = t − TL\n4: Lt′+1:t = SR(Lt′+1:t | yt′+1:t , AM , LM t′)\n5: for r = 1t o R do\n6: ´x[r]\nt = sample_motion_model(ut , x[r]\nt−1)\n7: x[r]\nt = scan_matching(zt , ´x[r]\nt ,m[r]\nt−1)\n8: for j = 1t o J do\n9: x j = sample_motion_model(ut , x[r]\nt−1)\n10: end for\n11: ω[r]\nz =\nJ∑\nj=1\nmeasurement_model(zt , x j ,m[r]\nt−1)\n12: S[r]\nt′+1:t ∼ latticelm(St′+1:t | Lt′+1:t ,λ )\n13: for τ = t′ + 1t o t do\n14: i[r]\nτ ,C[r]\nτ ∼ p(iτ ,Cτ | x[r]\nt′+1:t , S[r]\nt+1:t , ft′+1:t ,\ni[r]\n{t′+1:t|¬τ},C[r]\n{t′+1:t|¬τ}, H[r]\nt′ )\n15: end for\n16: ω[r]\nf = p( ft | C[r]\nt′+1:t−1, ft′+1:t−1, H[r]\nt′ )\n17: ω[r]\nic =\n∑\nit =k\n[\np(x[r]\nt | x[r]\nt′+1:t−1,i[r]\nt′+1:t−1,\nit = k, H[r]\nt′ )\n∑\nCt =l\np(it = k,Ct = l |\nC[r]\nt′+1:t−1,i[r]\nt′+1:t−1, H[r]\nt′ )\n]\n18: ω[r]\ns =\np(S[r]\nt | S[r]\nt′+1:t−1,C[r]\nt′+1:t−1, H[r]\nt′ )\np(S[r]\nt | S[r]\nt′+1:t−1, H[r]\nt′ )\n19: ω[r]\nt = ω[r]\nz · ω[r]\nf · ω[r]\ns · ω[r]\nic\n20: m[r]\nt = updated_occupancy_grid(zt , x[r]\nt ,m[r]\nt−1)\n21: H[r]\nt = F[p(Θ | x[r]\nt′+1:t ,C[r]\nt′+1:t , ft′+1:t , H[r]\nt′ )]\n22: ¯Xt = ¯Xt ∪⟨ x[r]\nt′+1:t ,C[r]\nt′+1:t ,m[r]\nt , H[r]\nt′+1:t ,ω [r]\nt ⟩\n23: end for\n24: S∗\nt′+1:t = argmaxS[r]\nt′+1:t\nR∑\nr=1\nω[r]\nS δ(St′+1:t − S[r]\nt′+1:t )\n25: LM t = argmaxLM p(LM | S∗\nt′+1:t , LM t′,λ )\n26: for r = 1t o R do\n27: draw i with probability ∝ ω[i]\nt\n28: add ⟨x[i]\nt′+1:t ,C[i]\nt′+1:t ,m[i]\nt , H[i]\nt′+1:t , LM t′+1:t ⟩ to Xt\n29: end for\n30: return Xt\n31: end procedure\nthis algorithm, particles are saved from time-step t − TL + 1\nto t and are resampled according to the weight based on\nnewly observed data each step. Here, the value of the ﬁxed-\nlag is denoted by T\nL . This technique means that the particles\nat step τ can be estimated not by using the observed data\nD1:τ , but rather with D1:τ+TL , i.e., the smoothing distribution\np(Cτ | D1:τ+TL ). In general, a smoothing method such as a\nﬁxed-lag particle smoother provides more accurate estima-\ntions than naive online estimation methods such as a particle\nﬁlter in estimating the joint posterior distribution of latent\nvariables.\nFigure 3 shows an overview of the FLR of i\nt and Ct .T h e\nnotation τ | t in the box in Fig. 3 is shorthand notation for\nthe subscript representing the time-step in the conditional\nmarginal posterior distribution, e.g., p(Cτ | D1:t ). The FLR\nis the process of sampling the latent variables iτ and Cτ by\niterating TL times from the previous step t − TL + 1t ot h e\ncurrent step t for each particle as follows:\niτ ,Cτ ∼ p(iτ ,Cτ | x0:t , S1:t , f1:t ,i{1:t|¬τ},C{1:t|¬τ},h)\n(7)\nwhere i{1:t|¬τ} and C{1:t|¬τ} denote sets of elements from 1\nto t without the elements of step τ. In this case, the latent\nvariables of step t − TL can be sampled using data up to step\nt, as described in Algorithm 1 (Lines 12–14). Equation ( 7)i s\nthe same as the conditional posterior probability distribution\nfor marginalized (collapsed) Gibbs sampling used in batch\nlearning. Therefore, the FLR corresponds to slightly iterate\nGibbs sampling for some recent previous latent variables in\nonline learning.\n4.1.2 Re-segmentation of word sequences (RS)\nWe introduce re-segmentation of word sequences to improve\nthe accuracy of word segmentation. In the original algo-\nrithm, we approximated the left side of ( 4)b yr e g i s t e r i n g\nthe word sequences segmented by latticelm to the word\ndictionary. However, this can be considered a process of\nsampling a language model LM from word sequences S\n∗\n1:t\nand a hyperparameter λ of a language model. Therefore, we\nadopt NPYLM, an unsupervised word segmentation method\n(Mochihashi et al. 2009), to estimate a language model from\nthe word sequences as follows:\nLM ∼ NPYLM(LM | S∗\n1:t ,λ ) . (8)\nThe procedure of introducing the RS is as follows: (i) word\nsequences S1:t are obtained by WFST speech recognition and\nlatticelm; (ii) word sentences S∗\n1:t of a maximum likelihood\nparticle are converted into syllable sequences, and segmented\ninto word sequences using NPYLM; (iii) the word dictio-\nnary LM is updated using segmented words, as described in\nAlgorithm 1 (Line 24). In this manner, we can overcome\nproblematic words that tend to become under-segmented\nwhile taking into account the uncertainty of speech recog-\nnition errors by latticelm. Note that there is a discrepancy\nbetween the words used for spatial concept acquisition and\nthe word set registered in the word dictionary.\n4.2 Scalability for reduced computational cost\nIn this section, we describe the details of the scalable algo-\nrithm. Here, we introduce two elements: the sequential\nBayesian update of the parameters in the posterior distri-\nbution, and unsupervised word segmentation from WFST\nspeech recognition results using FLR. The scalable algorithm\n123\nAutonomous Robots (2020) 44:927–946 935\n1|1\n2|2\n3|3\n4|4\n5|5\nTime of state vector\nTime of data\nFixed-lag area\n(Lag value: 3)\n1|1\n1|2 2|2\n1|3 2|3 3|3\n2|4 3|4 4|4\n3|5 4|5 5|5\nOriginal algorithm Improved algorithm\nDistribu/g415ons of established latent variables\n(Gray boxes)\nFixed-lag rejuvena/g415on by resampling\nFig. 3 Overview of the Fixed-Lag Rejuvenation of it and Ct .L e f t :n a i v e\nonline learning in the original algorithm. Right: online learning using\nFLR in the improved algorithm. The thick orange frame is estimated by\nsampling. In this case, the ﬁxed-lag value T\nL is three. The gray boxes\nmean that the estimated value will never again be updated, i.e., distribu-\ntions of already immobilized (ﬁxed) latent variables by online learning\n(Color ﬁgure online)\n1|1\n1|2 2|2\n1|3 2|3 3|3\n1|4 2|4 3|4 4|4\n1|5 2|5 3|5 4|5 5|5\nTime of state vector\nTime of data\nFixed-lag area\n(Lag value: 3)\n1|1\n1|2 2|2\n1|3 2|3 3|3\n2|4 3|4 4|4\n3|5 4|5 5|5\nOriginal algorithm Scalable algorithm\nSimultaneous sampling Fixed-lag rejuvena/g415on by simultaneous sampling\nFig. 4 Overview of the ﬁxed-lag rejuvenation of St . Left: batch learn-\ning with the original algorithm. Right: pseudo-online learning using\nFLR in the scalable algorithm. The thick orange frame is estimated by\nsampling from the joint distribution. In this case, the ﬁxed-lag value T\nL\nis three. The gray boxes denote that the estimated value will never be\nupdated again, i.e., distributions of already immobilized (ﬁxed) latent\nvariables by online learning (Color ﬁgure online)\ncan be combined with the FLR Ct , it of the improved algo-\nrithm. The pseudo-code for the scalable algorithm is given\nin Algorithm 2.\n4.2.1 Sequential Bayesian update of parameters in the\nposterior distribution (SBU)\nWe introduce a Sequential Bayesian Update (SBU) for the\nposterior hyperparameters Ht in the posterior distribution.\nIn the original algorithm, the model parameters Θ are esti-\nmated from all the data D1:t ={ f1:t , y1:t }and the set of latent\nvariables C1:t during each step. However, FastSLAM avoids\nholding all the previous data by updating a map mt from xt ,\nzt , and mt−1 sequentially. That is, it assumes the measure-\nment model p(zt | x0:t , z1:t−1) = p(zt | xt ,mt−1) and the\nupdated occupancy grid map p(mt | x0:t , z1:t ) = p(mt |\nxt , zt ,mt−1). Similarly, the posterior hyperparameters Ht\ncan be calculated from the new data Dt , latent variables\nCt , and posterior hyperparameters Ht−1 from previous steps.\nThus, both the computational and memory efﬁciency, crucial\nfor long-term learning with real robots, can be signiﬁcantly\nimproved. The SBU for the posterior hyperparameters is cal-\nculated as follows:\np(Θ | H\nt ) = p(Θ | D1:t ,C1:t ,h)\n= p(Θ | Dt ,Ct ,{D1:t−1,C1:t−1,h})\n= p(Θ | Dt ,Ct , Ht−1)\n∝ p(Dt | Ct ,Θ) p(Θ | Ht−1). (9)\n123\n936 Autonomous Robots (2020) 44:927–946\nThese posterior hyperparameters Ht can also be used to sam-\nple Ct . In the implementation, it sufﬁces to hold values of the\nstatistics obtained during the calculation of the posterior dis-\ntribution. Here, the calculation results from the left side and\nthe right side of ( 9) are strictly the same. The SBU approach\nis also said to keep track of sufﬁcient statistics in the particle\nﬁlter (Kantas et al. 2015).\nThe SBU equation is used together with FLR as follows:\np(Θ | H\nt ) = p(Θ | D1:t ,C1:t ,h)\n= p(Θ | Dt′+1:t ,Ct′+1:t ,{D1:t′,C1:t′,h})\n= p(Θ | Dt′+1:t ,Ct′+1:t , Ht′)\n∝ p(Dt′+1:t | Ct′+1:t ,Θ) p(Θ | Ht′), (10)\nwhere a time-step before the lag value is t′ = t − TL .I n\nthis case, it is only necessary to hold the observed data and\nposterior hyperparameters of the number corresponding to\nthe lag value T\nL . Equation ( 10) is applied to Algorithm 2.\n4.2.2 WFST speech recognition and unsupervised word\nsegmentation using FLR (FLR– St)\nWe describe the proposed algorithm that combines FLR and\nSBU to address problems of the unsupervised online word\nsegmentation and to reduce the computation time simulta-\nneously. FLR can also be extended to the sampling of S\nt\nin a pseudo-online manner. Figure 4 shows an overview of\nthe FLR of St . The notation τ | t takes the same meaning\nas it does in Fig. 3. The data used for speech recognition\nand word segmentation is modiﬁed from that in ( 4) to data\nwith a ﬁxed-lag interval. In addition, speech recognition is\nperformed using the initial syllable dictionary in the steps\nbefore step T\nL and using a word dictionary from step t′ in\nthe steps proceeding step TL +1. In this case, we can perform\nword segmentation based on the statistical information col-\nlected from the WFSTs recognized using the number of data\nfor the lag value T\nL . FLR performs simultaneous sampling\nof word sequences St′+1:t of time-steps from t′ + 1t ot h e\ncurrent step t as follows:\nSt′+1:t ∼ p(St′+1:t | yt′+1:t , AM , S1:t′,λ )\n≈ latticelm(St′+1:t | Lt′+1:t ,λ )\n· SR(Lt′+1:t | yt′+1:t , AM , LM t′). (11)\nTherefore, this approach can address the problem in the orig-\ninal algorithm by which incorrect word segmentation in early\nlearning stages was propagated to the following learning\nstages.\nHere, the amount of calculations is constant throughout\neach step, irrespective of the total amount of data. This\nproperty of the FLR of St is an important advantage in scal-\nability. However, there is a concern that word segmentation\nTable 1 Computational complexity of the learning algorithms\nAlgorithm Order\nSpCpSLAM (Taniguchi et al. 2017) O(NR )\nSpCoSLAM 2.0 (Improved) O(NR )\nSpCoSLAM 2.0 (Scalable) O(TL R)\nSpCoA (Taniguchi et al. 2016) (Batch\nlearning)\nO(NG )\nSpCoA++ (Taniguchi et al. 2018a)\n(Batch learning)\nO(NGMI )\nusing FLR becomes inaccurate compared to batch learning\nbecause of the limited availability of statistical information.\nEssentially, the scalable algorithm is a trade-off between\ncalculation time and word segmentation accuracy. In the\nlanguage model update, the word dictionary LM\nt holds infor-\nmation regarding words St′+1:t segmented from steps t′ + 1\nto t and the previous word dictionary LM t′. This is described\nin Algorithm 2 (Lines 4, 12, and 25).\nTable 1 shows the order of computational complexity for\neach learning algorithm. The data number is denoted N,t h e\nnumber of particles R, the value of ﬁxed-lag TL , the number\nof iterations for Gibbs sampling in batch learning G, the num-\nber of candidates of word segmentation results for updating\nthe language model in SpCoA++ M, and the number of iter-\nations for the parameter estimation in SpCoA++ I . V ariables\nwithout N are constants that can be preset by the user. Among\nthese algorithms, therefore, only the scalable algorithm does\nnot depend on the number of data N. In this case, the com-\nputational efﬁciency of the scalable algorithm is better than\nthe original SpCoSLAM algorithm when TL < N.\n5 Experiment I\nWe performed experiments to demonstrate online learning\nof spatial concepts in a novel environment. In addition, we\nperformed evaluations of place categorization and lexical\nacquisition related to places. We compared the performance\nof the following methods:\n(A) SpCoSLAM (Taniguchi et al. 2017)\n(B) SpCoSLAM with AW + WS (Sect. 3.2)\n(C) SpCoSLAM 2.0 (FLR– i\nt ,Ct )\n(D) SpCoSLAM 2.0 (FLR– it ,Ct +R S )\n(E) SpCoSLAM 2.0 (FLR– it ,Ct , St +S B U )\n(F) SpCoA++ (Batch learning) (Taniguchi et al. 2018a)\nMethods (A) and (B) used the original and modiﬁed\nSpCoSLAM algorithms. Methods (C) and (D) used the pro-\nposed improved algorithms under different conditions. In\nmethods (C) and (D), the lag value for FLR was set to T\nL =\n123\nAutonomous Robots (2020) 44:927–946 937\nImage Correct word it Step 15 it Step 30 it Step 50\n(English)\n/kyouyuuseki/\n(Shared desk ) 1\n/nobasyanyanamae/\n/bawakyoo/\n/yuseki/\n1\n/yusekinikibashita/\n/kyoyuseki/\n/kyoyusekidayo/\n1\n/kibashita/\n/ni/\n/kyouyuseki/\n/ikidomari/\n(End of the corridor ) 3\n/namaewa/\n/enikimashita/\n/gonobashiha/\n3\n/dayo/\n/ikidomari/\n/fokoga/\n3\n/idomari/\n/miriiNgusupesu/\n/koko/\n/roboqtookiba/\n(Robot storage space ) 4\n/robotokiba/\n/robotokibanya/\n/rimasu/\n6\n/kochirawaagaro/\n/botoki/\n/baninarimasu/\n6\n/wabotookiba/\n/robotookiba/\n/kochiraga/\nFig. 5 Top: learning results of position distributions in a generated map.\nEllipses denote the position distributions drawn on the map at steps 15,\n30, and 50. The colors of the ellipses were randomly determined for\neach index number i\nt = k. Bottom: examples of scene images cap-\ntured by the robot. The correct word (in English) and estimated words\nare shown for each position distribution at steps 15, 30, and 50 (Color\nﬁgure online)\n10. Method (E) used the proposed scalable algorithm under\nthree different conditions: the lag values for the FLR were set\nto T\nL = 1, 10, and 20 for (E1), (E2), and (E3), respectively.\nBatch-learning methods (F) was estimated by Gibbs sam-\npling based on a weak-limit approximation (Fox et al. 2011)\nof the Stick-Breaking Process (SBP) (Sethuraman 1994), one\nof the constitutive methods of the Dirichlet Process (DP). The\nupper limits of the spatial concepts and position distributions\nwere set to L = 50 and K = 50, respectively. We set the\nnumber of iterations for Gibbs sampling to G = 100. In\nmethod (F), we set the number of candidate word segmenta-\ntion results for updating the language model to M = 6, and\nthe number of iterative estimation procedures to I = 10. In\naddition, (F) did not use image features in the same manner\nas the original model setting. Note that SpCoA++ (F) was not\nevaluated in Taniguchi et al. ( 2017) because it is the latest\nbatch-learning method.\n5.1 Online learning\nWe conducted experiments of online spatial concept acquisi-\ntion in a real environment. We implemented SpCoSLAM 2.0\nbased on the open-source SpCoSLAM\n1, extending the gmap-\nping package and implementing grid-based FastSLAM 2.0\n(Grisetti et al. 2007) in the Robot Operating System (ROS).\nWe used an open dataset, albert-b-laser-vision, i.e., a ros-\nbag ﬁle containing the odometry, laser range data, and image\ndata. This dataset was obtained from the Robotics Data Set\nRepository (Radish) (Howard and Roy 2003). We prepared\nJapanese speech data corresponding to the movement of the\nrobot from the above-mentioned dataset because speech data\nwas not initially included. The total number of taught utter-\nances was N = 50, including 10 types of phrases. The\nrobot learned 10 places and 9 place names. The microphone\nwas a SHURE PG27-USB. Julius dictation-kit-v4.4 (DNN-\nHMM decoding) (Lee and Kawahara 2009) was used as\na speech recognizer. The initial word dictionary contained\n115 Japanese syllables. The unsupervised word segmentation\nsystem used latticelm (Neubig et al. 2012). The image fea-\nture extractor was implemented with Caffe, a deep-learning\nframework (Jia et al. 2014). We used a pre-trained CNN\nmodel, Places365-ResNet, trained with 365 scene categories\nfrom the Places2 Database with 1.8 million images (Zhou\n1 https://github.com/a-taniguchi/SpCoSLAM2 .\n123\n938 Autonomous Robots (2020) 44:927–946\net al. 2018). The number of particles was R = 30. The hyper-\nparameters for online learning were set as follows: α = 20,\nγ = 0.1, β = 0.1, χ = 0.1, m0 =[ 0,0]T, κ0 = 0.001,\nV0 = diag(2,2), and ν0 = 3. The above-mentioned parame-\nters were set such that all online methods were tested under\nthe same conditions. The hyperparameters for batch learning\nwere set as follows: α = 10, γ = 10, β = 0.1, m\n0 =[ 0,0]T,\nκ0 = 0.001, V0 = diag(2,2), and ν0 = 3. The hyperparam-\neters were determined manually and empirically according\nto each method. Note that the speech recognition decoder,\nthe image feature extractor, and the hyperparameters were\nchanged from Taniguchi et al. ( 2017).\nFigure 5 (top) shows the position distributions in the envi-\nronmental maps at steps 15, 30, and 50 with (D). This ﬁgure\nvisualizes how spatial concepts are acquired during sequen-\ntial mapping of the environment. The position distributions\nwere appropriately formed for places uttered by a user each\ntime. In step 15, the map covers only 2 rooms (in the upper\nright) and a corridor, with 5 position distributions. The map\nobtained at step 50 covers the entire environment, and there\nwere eventually 11 estimated position distributions. Figure 5\n(bottom) shows an example of the correct phoneme sequence\nof the place name, and the three best words estimated by the\nprobability distribution p(S\nt | it ,Θt , LM t ) at step t.T h el e f t\nside shows an example of the scene images observed in the\nit -th position distribution corresponding to the name of each\nplace. As the steps proceed, it can be seen that the words cor-\nresponding to the places were stably learned as phoneme\nsequences closer to the correct answers. For example, in\n/kyouyuuseki/ ( shared desk), in step 15, the correspondence\nbetween the place and phoneme sequence was insufﬁciently\nlearned: e.g., /bawakyoo/ and /yuseki/. However, by step\n50, the word was learned correctly: /kyouyuseki/. The index\nof the position distribution of /roboqtookiba/ ( robot storage\nspace) was changed from 4 to 6. This change means that the\nlabel number switched as a result of the previous estimate val-\nues being modiﬁed while learning progressed. Details of the\nonline learning experiment can be found in a video online\n2.\n5.2 Evaluation metrics\nWe evaluated the different algorithms according to the fol-\nlowing metrics: the Adjusted Rand Index (ARI) (Hubert and\nArabie 1985) of the classiﬁcation results of spatial concepts\nC\n1:N and position distribution i1:N ; the Estimation Accuracy\nRate (EAR) of the estimated total numbers of spatial concepts\nL and position distributions K ; and the Phoneme Accuracy\nRate (PAR) of uttered sentences and words related to places.\nWe conducted six learning trials under each algorithm con-\ndition. The details of the evaluation metrics are described in\nthe following sections.\n2 https://youtu.be/H5yztfmxGbc\n5.2.1 Estimation accuracy of spatial concepts\nWe compared the matching rate for the estimated indices\nC1:N of the spatial concept and the classiﬁcation results of\nthe correct answers given by a person. In this experiment, the\nevaluation metric adopts the ARI, which is a measure of the\nsimilarity between two clustering results. The matching rate\nfor the estimated indices i\n1:N of the position distributions\nwas evaluated in the same manner.\nIn addition, we evaluated the estimated number of spatial\nconcepts L and position distributions K using the EAR. The\nEAR was calculated as follows:\nEAR = max\n(\n1 − | nC\nt − nE\nt |\nnCt\n,0\n)\n(12)\nwhere nC\nt is the correct number and nE\nt is the estimated num-\nber at time-step t.\n5.2.2 PAR of uttered sentences\nWe next compared the accuracy rate of phoneme recogni-\ntion and word segmentation for all the recognized sentences.\nHowever, it was difﬁcult to separately weigh the ambiguous\nphoneme recognition and the unsupervised word segmenta-\ntion. Therefore, the experiment considered the position of a\ndelimiter as a single letter. The correct phoneme sequence\nwas suitably segmented into Japanese morphemes using\nMeCab (Kudo 2006), an off-the-shelf Japanese morpho-\nlogical analyzer that is widely used for natural language\nprocessing. However, the name of the place was considered\na single word.\nWe calculated the PAR of the uttered sentences with the\ncorrect phoneme sequence s\nP\nt , and a phoneme sequence sR\nt\nof the recognition result of each uttered sentence. The PAR\nwas calculated as follows:\nPAR = max\n(\n1 − LD(sP\nt ,sR\nt )\nnp ,0\n)\n(13)\nwhere LD () was calculated using the Levenshtein distance\nbetween sP\nt and sR\nt . Here, nP denotes the number of phonemes\nof the correct phoneme sequence.\n5.2.3 PAR of words related to places\nWe also evaluated whether a phoneme sequence has learned\nthe properly segmented place names. This experiment\nassumed a request for the best phoneme sequence, s∗\nt , rep-\nresenting the self-position xt of the robot. We compared the\nPAR of words with the correct place name and a selected\nword for each teaching place. The PAR was calculated using\n(13).\n123\nAutonomous Robots (2020) 44:927–946 939\nTable 2 Evaluation results in a real environment\nMetric Improved Scalable ARI EAR PAR\nCt it LK Sentence Word\n(A) SpCoSLAM 0.273 0.502 0.756 0.881 0.524 0.154\n(B) SpCoSLAM with AW + WS 0.233 0.420 0.805 0.901 0.496 0.086\n(C) SpCoSLAM 2.0 (10 FLR– i\nt ,Ct ) ✓ 0.324 0.602 0.876 0.913 0.533 0.157\n(D) SpCoSLAM 2.0 (10 FLR– it ,Ct +R S ) ✓ 0.320 0.555 0.881 0.901 0.801 0.419\n(E1) SpCoSLAM 2.0 ✓✓ 0.244 0.443 0.869 0.923 0.648 0.158\n(1 FLR– it ,Ct , St +S B U )\n(E2) SpCoSLAM 2.0 ✓✓ 0.314 0.570 0.790 0.801 0.690 0.262\n(10 FLR– it ,Ct , St +S B U )\n(E3) SpCoSLAM 2.0 ✓✓ 0.351 0.673 0.748 0.890 0.704 0.292\n(20 FLR– it ,Ct , St +S B U )\n(F) SpCoA++ (Batch learning) 0.387 0.624 0.700 0.648 0.787 0.524\nBold underlined indicate the highest evaluation values, and underline indicates the second highest evaluation values\nThe selection of a word s∗\nt,b was calculated as follows:\ns∗\nt = argmaxSt,b p(St,b | xt ,Θt , LM t ). (14)\nIn this experiment, we used the self-position xt that was\nnot included in the training data to evaluate the PAR of\nwords. Here, the robot can perform sufﬁciently accurate self-\nlocalization using a laser range ﬁnder. Therefore, in this\nexperiment, we assume that x\nt is given an accurate coor-\ndinate value without errors.\nThe more a method accurately recognized words and\nacquired spatial concepts, the higher is the PAR. We con-\nsider this evaluation metric to be an overall measure of the\nproposed method.\n5.3 Evaluation results and discussion\nIn this section, we discuss the improvement and scalability of\nthe proposed learning algorithms. Table 2 lists the averages\nof the evaluation values calculated using the metrics ARI,\nEAR, and PAR at step 50.\n5.3.1 ARI and EAR results\nIn terms of categorization accuracy, the proposed algorithms\nthat introduced FLR tended to show higher ARI values than\nthe original algorithms (A) and (B) of SpCoSLAM. Figure 6\nshows examples of the progress of place clustering for posi-\ntion distributions in (A) and (D). The step numbers in the\nﬁgures on the left (A) and right (D) are not the same. In these\ncases, large position distributions covering distant areas were\nlearned, i.e., the purple ellipses in the ﬁgures on top. In (A),\nincorrect clustering results were obtained during the ﬁnal\nstep (i.e., step 50) because the original SpCoSLAM algo-\nrithm cannot correct past erroneous estimations. By contrast,\nIncorrect\nclustering\nresults\n(A)\nCorrection of place clustering\n(D)\nFig. 6 Examples of corrected place clustering results. Left: the original\nalgorithm (A). Right: the improved SpCoSLAM 2.0 algorithm (D)\nin (D) by introducing FLR, an incorrect cluster occurred at\nstep 25 (top right ﬁgure). However, the proposed algorithm\ncould correct previous erroneous estimates at step 30 (bot-\ntom right ﬁgure). Therefore, in the original algorithm (A),\nestimation errors adversely affect subsequent estimations.\nHowever, SpCoSLAM 2.0 (D) obtained more accurate esti-\nmations immediately, despite previous incorrect estimations.\nSimilar situations to (D) were also conﬁrmed in other pro-\nposed algorithms that introduced FLR. Experimental results\ndemonstrated that FLR, which resamples the latent variables\nof the previous step using observations up to the current step,\ncontributes to improving the accuracy of online place clus-\ntering.\nFigure 7 shows the results of the EAR values with spatial\nconcepts and position distributions, i.e., the accuracy of the\n123\n940 Autonomous Robots (2020) 44:927–946\n10 20 30 40 50\nstep\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n10 20 30 40 50\nstep\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(A) SpCoSLAM\n(B) SpCoSLAM with AW+WS\n(C) 2.0 (10 FLR)\n(D) 2.0 (10 FLR + RS)\n(E1) 2.0 (1 FLR + SBU)\n(E2) 2.0 (10 FLR + SBU)\n(E3) 2.0 (20 FLR + SBU)\nFig. 7 Change in the EAR regarding the estimated total number of\nspatial concepts L (top) and position distributions K (bottom) for each\nstep\nestimated number of clusters, for each step. The EAR values\nwere not stable in the steps during the ﬁrst half, although they\nconverged stably to high values in the latter half. In the result\nat step 50, (D) showed the highest EAR value L and (E1)\nshowed the highest EAR value K . However, for both L and\nK , looking at all the steps on average, (A) and (B) yielded\nrelatively low values overall, and (C) and (D) yielded rela-\ntively high values. (E1)–(E3) tended to show values between\noriginal algorithms, (A) and (B), and improved algorithms\nwith FLR, (C) and (D). From the results of (C) and (D), EAR\nvalues improved considerably by introducing the FLR of C\nt\nand it .\n5.3.2 PAR sentence and word results\nFrom the results of the improved algorithm (D), the PAR\nvalues (sentence and word) improved markedly by adding\nthe re-segmentation of the word sequences. These results\nshow that the robot can accurately segment the names of\nplaces and learn the relationship between places and words\nmore precisely. In particular, method (D), which combines\n10 20 30 40 50\nstep\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nPAR (Word)\n(A) SpCoSLAM\n(B) SpCoSLAM with AW+WS\n(C) 2.0 (10 FLR)\n(D) 2.0 (10 FLR + RS)\n(E1) 2.0 (1 FLR + SBU)\n(E2) 2.0 (10 FLR + SBU)\n(E3) 2.0 (20 FLR + SBU)\nFig. 8 Change in the PAR of words for each step\nthe FLR and RS, achieved an overall improvement com-\nparable to the other online algorithms. Some trial results\nshowed PAR values comparable to those of SpCoA++ (F).\nFigure 8 shows the PAR of words for each step. The PAR\ntended to increase as a whole. Therefore, it can be expected\nthat the PAR values will further increase as the number of\nsteps advances. Table 3 presents examples of word segmen-\ntation results with the four methods. The correct phoneme\nsequence, i.e., ground truth, was segmented into Japanese\nmorphemes using MeCab (Kudo 2006), where “ |” denotes a\ndelimiter, i.e., a word segment position. The parts in bold cor-\nrespond to the name for each place. SpCoSLAM (A) showed\nunder-segmentation results in many cases. On the other hand,\nit can be seen that SpCoSLAM 2.0 (D) and (E3) properly seg-\nmented the phoneme sequences representing the name of the\nplace. Comparing (D) and (E3), (D) obtained segmentation\nresults close to those of the batch learning method (F), and\n(E3) sometimes slightly over-segmented words. Therefore,\nSpCoSLAM 2.0 can mitigate under-segmentation when the\nword segmentation of the batch learning method is applied\nin a pseudo-online manner.\n5.3.3 Original and modiﬁed SpCoSLAM algorithms\nAlthough the modiﬁed SpCoSLAM (B) is theoretically more\nappropriate than the original algorithm (A), few differences\nwere found between them. In the proposed algorithms, the\ntime-driven process, i.e., SLAM part, and the event-driven\nprocess, i.e., spatial concept formation and lexical acquisi-\ntion, were estimated by the same particle ﬁlter. Although\nself-localization and mapping were performed each time the\nrobot moved in an environment, latent variables for the spa-\ntial concepts and lexicon are updated only upon the user’s\nutterance. Thus, particles can ﬂuctuate as a result of resam-\npling due to movement in the absence of the user’s utterance.\nConsequently, the weight for self-localization might be inﬂu-\n123\nAutonomous Robots (2020) 44:927–946 941\nTable 3 Examples of word segmentation results of uttered sentences\nEnglish “This place is the shared desk .”\nGround truth kochira |ga |kyouyuuseki |ni |nari | masu\n(A) a |kochiragagyoyusekiNni |narimasu\n(D) kochira |ga |kyouyuseki |ninarimasu\n(E3) uo |kochi |ra |ga |kyoyuseki |nina |ri | ma |su\n(F) ochiraga |kyoyuseki |ninarimasu\nEnglish “This is the meeting space .”\nGround truth koko |wa |miitiNgusupeisu |desu\n(A) kokowaga |midigisupesudesujoouya\n(D) kokowa |miriiNgusupesu |desu\n(E3) kowa |midigyusu |pesu |desu\n(F) gokoga |miidiNgusupesu |desu\nEnglish “ The printer room is here.”\nGround truth puriNtaabeya |wa |kochira |desu\n(A) io poriNtabeaakochiragadesuduuryuzu qaqo\n(D) puriNtabeya |kochira |desu\n(E3) puriNpabeya |ta |kochiradesu\n(F) poriNpabeya |wakochiradesu\n10 20 30 40 50\nstep\n0\n100\n200\n300\n400\n500\n600\n700\n800\nCalculation time [sec.]\n(A) SpCoSLAM\n(B) SpCoSLAM with AW+WS\n(C) 2.0 (10 FLR)\n(D) 2.0 (10 FLR + RS)\n(E1) 2.0 (1 FLR + SBU)\n(E2) 2.0 (10 FLR + SBU)\n(E3) 2.0 (20 FLR + SBU)\nFig. 9 Calculation times par step for evaluating scalability\nential, rather than the weight for the spatial concept and\nlexicon. This will be investigated in future work.\n5.3.4 Calculation time and scalable algorithm\nFigure 9 shows the calculation times between online learning\nalgorithms. With batch learning, SpCoA++’s overall cal-\nculation time including the runtime of rosbag for SLAM\nwas 13,850.873 s, and the calculation times per iteration\nfor the iterative estimation procedure and Gibbs sampling\nwere 1,318.954 s and 1.833 s, respectively. In the origi-\nnal SpCoSLAM algorithm, (A) and (B), and the improved\nSpCoSLAM 2.0 algorithm, (C) and (D), the calculation time\nincreased with the number of steps, i.e., as the amount of data\nFig. 10 Examples of home environments in SIGV erse\nincreased. However, the scalable SpCoSLAM 2.0 algorithm\n(E1)–(E3) retained a constant calculation time regardless of\nan increase in the amount of data. Therefore, we can exert\nparticularly powerful effects for long-term learning.\nIn the scalable algorithm (E1)–(E3), the evaluation values\nof ARI and PAR tended to improve overall when the lag\nvalue increased. In particular, when the lag value was 20,\nrelatively high evaluation values are seen to approach those\nof the improved algorithm.\nOwing to a trade-off between the ﬁxed-lag size and accu-\nracy, the algorithm needs to be set appropriately according\nto both the computational power embedded in the robot and\nthe duration requirements for actual operation. In this exper-\niment, we did not evaluate the scalability of the algorithm\nwith parallel processing. However, we considered that the\nproposed algorithm could be executed even faster by paral-\nlelizing the particle process and by using Graphics Processing\nUnits (GPUs). As such, we consider that the robot would be\nable to move within the environment while learning in real-\ntime.\n6 Experiment II\nIn this experiment, it is investigated whether trends similar\nto the evaluation results of the real environmental dataset in\nSect. 5 can be stably obtained across different environments.\nPlace categorization and lexical acquisition related to places\nin virtual home environments were evaluated, and the evalu-\nation metrics ARI, EAR, and PAR for the methods (A)–(F)\nwere compared in the same manner as in Sect. 5.\n6.1 Condition\nOnline spatial concept acquisition experiments were con-\nducted in various virtual home environments. The simulator\nenvironment was SIGV erse version 3.0 (Inamura et al. 2010),\na client-server based architecture that can connect the ROS\nand Unity. The virtual robot in SIGV erse was Toyota’s\nHuman Support Robot (HSR), and we used 10 different\n123\n942 Autonomous Robots (2020) 44:927–946\nTable 4 Evaluation results in simulator environments\nMetric ARI EAR PAR\nCt it LK Sentence Word\n(A) SpCoSLAM 0.252 0.604 0.785 0.818 0.558 0.098\n(B) SpCoSLAM with AW + WS 0.347 0.684 0.802 0.815 0.565 0.141\n(C) SpCoSLAM 2.0 (10 FLR– i\nt ,Ct ) 0.346 0.713 0.733 0.868 0.553 0.096\n(D) SpCoSLAM 2.0 (10 FLR– it ,Ct + RS) 0.314 0.719 0.730 0.840 0.835 0.464\n(E1) SpCoSLAM 2.0 (1 FLR– it ,Ct , St + SBU) 0.307 0.672 0.817 0.800 0.671 0.165\n(E2) SpCoSLAM 2.0 (10 FLR– it ,Ct , St + SBU) 0.385 0.688 0.833 0.782 0.733 0.305\n(E3) SpCoSLAM 2.0 (20 FLR– it ,Ct , St + SBU) 0.354 0.790 0.883 0.898 0.768 0.350\n(F) SpCoA++ (Batch learning) 0.522 0.899 0.800 0.850 0.830 0.480\nBold underlined indicate the highest evaluation values, and underline indicates the second highest evaluation values\nhome environments3 created using Sweet Home 3D 4, which\nis a free software for interior design application. Figure 10\nshows examples of the home environments. For each place,\n10 training data were provided on average. The total num-\nber of taught utterances was N = 60, including 10 types\nof phrases. The robot learned six places and their respective\nnames. The microphone and speech recognizer were the same\nas those in Sect. 5.1. The image feature extractor was a pre-\ntrained BVLC CaffeNet model (Jia et al. 2014). The number\nof particles was R = 10. The hyperparameters for learning\nwere set as follows: α = 10.0, γ = 1.0, β = 0.1, χ = 0.1,\nm\n0 =[ 0,0]T, κ0 = 0.001, V0 = diag(2,2), and ν0 = 3. The\nhyperparameters were determined manually and empirically.\nThe above-mentioned parameters were set such that all meth-\nods were tested under the same conditions. In method (F),\nthe upper limits of the spatial concepts and position distri-\nbutions were set to L = 20 and K = 20, respectively. The\nother settings were identical to those in Sect. 5.\nThe main target of the evaluation in this study is the\naccuracy of place clustering and lexical acquisition, i.e.,\nextended points in SpCoSLAM 2.0. Therefore, in this exper-\niment, it is assumed that sufﬁciently accurate mapping and\nself-localization are possible with a high-precision distance\nsensor, and using an online learning algorithm which sepa-\nrates and omits the SLAM process was executed. The true\nvalues obtained by the simulator were used as the self-\nposition data.\n6.2 Result\nIn this section, the improvement and scalability of the\nproposed learning algorithms in home environments are dis-\ncussed. Table 4 lists the averages of the evaluation values\ncalculated using the metrics ARI, EAR, and PAR at step 60.\n3 3D models of home environments are available in https://github.com/\na-taniguchi/SweetHome3D_rooms.\n4 Sweet Home 3D: http://www.sweethome3d.com/\n10 20 30 40 50 60\nstep\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nPAR (Word)\n(A)\n(B)\n(C)\n(D)\n(E1)\n(E2)\n(E3)\nFig. 11 Change in PAR of words for each step in simulator environ-\nments\nThe ARI showed a similar trend as the result of real\nenvironmental data. However, compared to the original algo-\nrithms (A) and (B), there was almost no difference in the\nvalues in algorithms that introduced FLR. In addition, the\nEAR showed a slightly different trend than the real environ-\nmental data. In the improved algorithms (C) and (D), the\nnumber L of categories of spatial concepts smaller than the\ntrue value was estimated compared to other algorithms. We\nconsider that this reason was due to the fact that it was re-\ncombined into the same category by FLR. Because the dataset\nwas obtained in the simulator environment, for example, the\nimage features could be insufﬁciently obtained for place cat-\negorization, i.e., similar features might be found in different\nplaces. Such a problem did not occur when using real envi-\nronmental data.\nThe PAR had the same tendency as the result of real envi-\nronment data. Similar to Sect. 5.3, the improved algorithm\nwith RS (D) showed lexical acquisition accuracy compara-\nble to batch learning (F). In addition, the scalable algorithms\nwith FLR of S\nt (E2) and (E3) showed higher values than the\noriginal algorithms. Figure 11 shows the average values of\n123\nAutonomous Robots (2020) 44:927–946 943\n10 20 30 40 50 60\nstep\n0\n100\n200\n300\n400\n500\nCalculation time [sec.]\n(A)\n(B)\n(C)\n(D)\n(E1)\n(E2)\n(E3)\nFig. 12 Calculation times par step in simulator environments\nthe PAR of words for each step in different environments.\nSimilar to Fig. 8, the PAR tended to increase overall. Thus, it\ncan be seen that RS and FLR of St work effectively in virtual\nhome environments.\nIn the comparison of the original and modiﬁed\nSpCoSLAM algorithms (A) and (B), the modiﬁed algorithm\n(B) showed higher overall values in the evaluation values of\nARI and PAR. We consider that the weight for the spatial\nconcept and lexicon acted more directly in this experiment\nthan in the experiment in Sect. 5, because it was not affected\nby the weight for self-localization.\nIn scalable algorithms (E1)–(E3), as the FLR value\nincreased, the tendency for the overall evaluation values to\nincrease appeared more prominently than for the results of\nreal environment data.\nFigure 12 shows the average calculation times between\nonline learning algorithms in simulator environments. We\nconﬁrmed that the result was similar to Fig. 9, which was the\nresult using the real environment data. With batch learning,\nSpCoA++’s overall average calculation time was 8,076.288\ns, and the calculation times per iteration for the iterative esti-\nmation procedure and Gibbs sampling were 807.623 s and\n1.346 s, respectively.\nThe following are the common inferences from the results\nof both the simulation and real-world environments. For the\nonline learning, if the user requires the performance of lexical\nacquisition even at an increased time cost, they can exe-\ncute the improved algorithm (D) or scalable algorithm with\na larger lag value, e.g., (E2) and (E3). If the user requires\nhigh-speed calculation, they can obtain better results faster\nthan the conventional algorithm (A) by executing a scalable\nalgorithm such as (E1) and (E2).\n7 Conclusion\nThis paper proposed an improved and scalable online learn-\ning algorithm to address the problems encountered by our\npreviously proposed SpCoSLAM algorithm. Speciﬁcally, we\nproposed online learning algorithm, called SpCoSLAM 2.0,\nfor spatial concepts and lexical acquisition, for higher accu-\nracy and scalability. In experiments, we conducted online\nlearning with a robot in a novel environment without any\npre-existing lexicon and map. In addition, we compared the\nproposed algorithm to the original online algorithm and to\nbatch learning in terms of the estimation accuracy and cal-\nculation time. The results demonstrate that the proposed\nalgorithm is more accurate than the original algorithm and\nof comparable accuracy to batch learning. Moreover, the cal-\nculation time of the proposed scalable algorithm becomes\nconstant for each step, regardless of the amount of training\ndata. We expect this work to contribute to the realization of\nlong-term spatial language interactions between humans and\nrobots.\nIn the future, we shall experiment with long-term online\nlearning of spatial concepts in large-scale environments\nbased on the scalable algorithm proposed in this paper. Fur-\nthermore, with additional development, it will be possible\nto introduce a forgetting mechanism to the proposed algo-\nrithm as with Araki et al. ( 2012a). When a robot continues to\noperate over a long period of time it will encounter changes\nin the environment, such as the names of places and areas.\nConsequently, the robot will beneﬁt from using the latest\nobservation data as opposed to the previous observation data.\nWe believe that such a mechanism will be especially effective\nfor long-term learning.\nThe proposed method constructs spatial concepts on a\nmetric map; however, it can also be extended to learning the\ntopological structure of places as with Karao˘ guz and Bozma\n(2016); Luperto and Amigoni ( 2018). We explore whether\nthis facilitates navigation tasks with human–robot linguis-\ntic interactions. In addition, loop-closure detection has been\nstudied actively in recent years, as is evident from long-term\nvisual SLAM (Han et al. 2018). The generative model of\nSpCoSLAM is connected to SLAM and lexical acquisition\nvia latent variables related to the spatial concepts. There-\nfore, we shall also explore loop-closure detection based on\nspeech signals and investigate whether spatial concepts can\npositively affect mapping.\nWe will explore whether the SpCoSLAM model pro-\nposed herein can be integrated with other probabilistic\nmodels to form a large-scale cognitive model for general-\npurpose autonomous intelligent robots using a SERKET\narchitecture (Nakamura et al. 2018). However, applications\nof the SERKET architecture are limited due to its compu-\ntational cost for learning the enormous parameters of the\nwhole model. Even in such a case, we consider that our\nproposed approach to online learning will be extensively\nuseful because it can be applied to various other Bayesian\nmodels.\n123\n944 Autonomous Robots (2020) 44:927–946\nAcknowledgements The authors thank Cyrill Stachniss for providing\nthe dataset in the real robot. The authors also thank Kazuya Asada\nand Keishiro Taguchi for providing virtual home environments and the\ndataset for spatial concepts in SIGV erse simulator.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing, adap-\ntation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indi-\ncate if changes were made. The images or other third party material\nin this article are included in the article’s Creative Commons licence,\nunless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your\nintended use is not permitted by statutory regulation or exceeds the\npermitted use, you will need to obtain permission directly from the copy-\nright holder. To view a copy of this licence, visit http://creativecomm\nons.org/licenses/by/4.0/.\nReferences\nAldous, D. (1985). Exchangeability and related topics. École d’Été de\nProbabilités de Saint-Flour XIII-1983 (pp. 1–198).\nAoki, T., Nishihara, J., Nakamura, T., & Nagai, T. (2016). Online joint\nlearning of object concepts and language model using multimodal\nhierarchical Dirichlet process. In Proceedings of the IEEE/R SJ\ninternational conference on intelligent robots and s ystems (IROS)\n(pp. 2636–2642). IEEE\nAraki, T., Nakamura, T., Nagai, T., Funakoshi, K., Nakano, M., & Iwa-\nhashi, N. (2012a). Online object categorization using multimodal\ninformation autonomously acquired by a mobile robot. Advanced\nRobotics, 26(17), 1995–2020.\nAraki, T., Nakamura, T., Nagai, T., Nagasaka, S., Taniguchi, T., & Iwa-\nhashi, N. (2012b). Online learning of concepts and words using\nmultimodal LDA and hierarchical Pitman-Y or Language Model.\nIn Proceedings of the IEEE/R SJ international conference on intel-\nligent robots and s ystems (IROS) (pp. 1623–1630). IEEE\nBall, D., Heath, S., Wiles, J., Wyeth, G., Corke, P ., & Milford, M.\n(2013). OpenRatSLAM: an open source brain-based slam system.\nAutonomous Robots , 34(3), 149–176.\nBeevers, K. R., & Huang, W. H. (2007). Fixed-lag sampling strategies\nfor particle ﬁltering slam. In Proceedings of the IEEE international\nconference on robotics and a utomation (ICRA) (pp. 2433–2438).\nIEEE\nBörschinger, B., & Johnson, M. (2011). A particle ﬁlter algorithm for\nBayesian wordsegmentation. In Australasian language technology\nassociation wor kshop 2011 (p. 10). Citeseer\nBörschinger, B., & Johnson, M. (2012). Using rejuvenation to improve\nparticle ﬁltering for Bayesian word segmentation. In Proceedings\nof the 50th ann ual meeting of the association for co mputational\nlinguistics, association for co mputational linguistics (pp. 85–89).\nCangelosi, A., & Schlesinger, M. (2015). Developmental robotics:\nFrom babies to robots. intelligent robotics and autonomous\nagents series. MIT Press. https://books.google.co.jp/books?\nid=AbKPoAEACAAJ.\nCanini, K. R., Shi, L., & Grifﬁths, T. L. (2009). Online inference\nof topics with latent Dirichlet allocation. Proceedings of the\nInternational Conference on Artiﬁcial Intelligence and Statistics\n(AISTATS), 9, 65–72.\nDoucet, A., De Freitas, N., Murphy, K., & Russell, S. (2000). Rao-\nblackwellised particle ﬁltering for dynamic bayesian networks.\nIn Proceedings of the 16th conference on uncertainty in artiﬁcial\nintelligence (pp. 176–183). Morgan Kaufmann Publishers Inc.\nFox, E. B., Sudderth, E. B., Jordan, M. I., & Willsky, A. S. (2011).\nA sticky HDP-HMM with application to speaker diarization. The\nAnnals of Applied Statistics, 5(2A), 1020–1056.\nGrisetti, G., Stachniss, C., & Burgard, W. (2007). Improved techniques\nfor grid mapping with Rao-Blackwellized particle ﬁlters. IEEE\nTransactions on Robotics , 23, 34–46.\nGu, Z., Taguchi, R., Hattori, K., Hoguro, M., & Umezaki, T. (2016).\nLearning of relative spatial concepts from ambiguous instructions.\nIn Proceedings of the 13th IFAC/IFIP/IFOR S/IEA s ymposium on\nanalysis, design, and eval uation of h uman-machine systems( I F A C\nHMS) (V ol. 49, pp. 150–153). Elsevier\nHagiwara, Y ., Inoue, M., Kobayashi, H., & Taniguchi, T. (2018). Hierar-\nchical spatial concept formation based on multimodal information\nfor human support robots. Frontiers in Ne urorobotics, 12, 11.\nhttps://doi.org/10.3389/fnbot.2018.00011.\nHan, F., Wang, H., Huang, G., & Zhang, H. (2018). Sequence-based\nsparse optimization methods for long-term loop closure detection\nin visual slam. Autonomous Robots, 42(7), 1323–1335. https://doi.\norg/10.1007/s10514-018-9736-3 .\nHeath, S., Ball, D., & Wiles, J. (2016). Lingodroids: Cross-situational\nlearning for episodic elements. IEEE Transactions on Cognitive\nand Develop mental Systems,\n8(1), 3–14. https://doi.org/10.1109/\nTAMD.2015.2442619.\nHemachandra, S., Walter, M. R., Tellex, S., & Teller, S. (2014).\nLearning spatial-semantic representations from natural language\ndescriptions and scene classiﬁcations. In Proceedings of the IEEE\ninternational conference on robotics and a utomation (ICRA) (pp.\n2623–2630). IEEE\nHoward, A., & Roy, N. (2003). The robotics data set repository (radish).\nhttp://radish.sourceforge.net/.\nHubert, L., & Arabie, P . (1985). Comparing partitions. Journal of Clas-\nsiﬁcation, 2(1), 193–218.\nInamura, T., Shibata, T., Sena, H., Hashimoto, T., Kawai, N., Miyashita,\nT., Sakurai, Y ., Shimizu, M., Otake, M., Hosoda, K., et al. (2010).\nSimulator platform that enables social interaction simulation—\nSIGV erse: SocioIntelliGenesis simulator. In: Proceedings of the\nIEEE/SICE international s ymposium on s ystem integration (pp.\n212–217).\nIsobe, S., Taniguchi, A., Hagiwara, Y ., & Taniguchi, T. (2017). Learning\nrelationships between objects and places by multimodal spatial\nconcept with bag of objects. In Proceedings of the international\nconference on social robotics (IC SR) (pp. 115–125). Springer\nJia, Y ., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick,\nR., Guadarrama, S., & Darrell, T. (2014). Caffe: Convolu-\ntional architecture for fast feature embedding. arXiv preprint\narXiv:1408.5093.\nKantas, N., Doucet, A., Singh, S. S., Maciejowski, J., Chopin, N., et al.\n(2015). On particle methods for parameter estimation in state-\nspace models. Statistical Science, 30(3), 328–351.\nKarao˘guz, H., & Bozma, H. I. (2016). An integrated model of\nautonomous topological spatial cognition. Autonomous Robots ,\n40(8), 1379–1402. https://doi.org/10.1007/s10514-015-9514-4 .\nKitagawa, G. (2014). Computational aspects of sequential Monte Carlo\nﬁlter and smoother. Annals of the Instit ute of Statistical Mathe-\nmatics, 66(3), 443–471.\nKostavelis, I., & Gasteratos, A. (2015). Semantic mapping for mobile\nrobotics tasks: A survey. Robotics and A utonomous System\ns, 66,\n86–103.\nKrizhevsky, A., Sutskever, I., & Hinton, G. (2012). Imagenet classiﬁ-\ncation with deep convolutional neural networks. In Proceedings\nof the advances in ne ural information processing s ystems( N I PS),\nNevada, United States (pp. 1097–1105).\nKudo, T. (2006). MeCab: Yet another part-of-speech and morphological\nanalyzer. https://github.com/taku910/mecab.\n123\nAutonomous Robots (2020) 44:927–946 945\nLandsiedel, C., Rieser, V ., Walter, M., & Wollherr, D. (2017). A\nreview of spatial reasoning and interaction for real-world robotics.\nAdvanced Robotics , 31(5), 222–242.\nLee, A., & Kawahara, T. (2009). Recent development of open-source\nspeech recognition engine Julius. In Proceedings of the AP SIPA\nASC (pp. 131–137).\nLuperto, M., & Amigoni, F. (2018). Predicting the global structure of\nindoor environments: A constructive machine learning approach.\nAutonomous Robots . https://doi.org/10.1007/s10514-018-9732-\n7.\nMochihashi, D., Yamada, T., & Ueda, N. (2009). Bayesian unsupervised\nword segmentation with nested Pitman-Y or language modeling.\nIn Proceedings of the joint conference of the 47th ann ual meeting\nof the ACL and the 4th international joint conference on nat ural\nlanguage processing of the AFNLP (ACL-IJCNLP) (pp. 100–108).\nMontemerlo, M., Thrun, S., Koller, D., Wegbreit, B., et al. (2003).\nFastSLAM 2.0: An improved particle ﬁltering algorithm for simul-\ntaneous localization and mapping that provably converges. In\nProceedings of the international joint conference on artiﬁcial intel-\nligence (IJCAI) (pp. 1151–1156).\nNakamura, T., Nagai, T., & Taniguchi, T. (2018). Serket: An architecture\nfor connecting stochastic models to realize a large-scale cognitive\nmodel. Frontiers in Neurorobotics, 12, 25. https://doi.org/10.3389/\nfnbot.2018.00025.\nNeubig, G., Mimura, M., & Kawahara, T. (2012). Bayesian learning of\na language model from continuous speech. IEICE Transactions on\nInformation and Systems, 95(2), 614–625.\nNishihara, J., Nakamura, T., & Nagai, T. (2017). Online algorithm for\nrobots to learn object concepts and language model. IEEE Trans-\nactions on Cognitive and Develop mental Systems, 9(3), 255–268.\nhttps://doi.org/10.1109/TCDS.2016.2552579.\nPronobis, A., & Jensfelt, P . (2012). Large-scale semantic mapping and\nreasoning with heterogeneous modalities. In Proceedings of the\nIEEE international conference on robotics and a utomation (ICRA)\n(pp. 3515–3522). IEEE\nRangel, J. C., Cazorla, M., García-V area, I., Romero-González, C., &\nMartínez-Gómez, J. (2018). Automatic semantic maps generation\nfrom lexical annotations. Autonomo\nus Robots . https://doi.org/10.\n1007/s10514-018-9723-8 .\nSethuraman, J. (1994). A constructive deﬁnition of Dirichlet priors.\nStatistica Sinica, 4, 639–650.\nSünderhauf, N., Dayoub, F., McMahon, S., Talbot, B., Schulz, R.,\nCorke, P ., Wyeth, G., Upcroft, B., & Milford, M. (2016). Place\ncategorization and semantic mapping on a mobile robot. In Pro-\nceedings of the IEEE international conference on robotics and\nautomation (ICRA) (pp. 5729–5736). IEEE\nTaguchi, R., Yamada, Y ., Hattori, K., Umezaki, T., Hoguro, M.,\nIwahashi, N., Funakoshi, K., & Nakano, M. (2011). Learning\nplace-names from spoken utterances and localization results by\nmobile robot. In Proceedings of the ann ual conference of the inter-\nnational speech co mmunication association (INTER SPEECH)\n(pp. 1325–1328).\nTaniguchi, A., Taniguchi, T., & Inamura, T. (2016). Spatial concept\nacquisition for a mobile robot that integrates self-localization and\nunsupervised word discovery from spoken sentences. IEEE Trans-\nactions on Cognitive and Develop mental Systems, 8(4), 285–297.\nhttps://doi.org/10.1109/TCDS.2016.2565542.\nTaniguchi, A., Hagiwara, Y ., Taniguchi, T., & Inamura, T. (2017).\nOnline spatial concept and lexical acquisition with simultaneous\nlocalization and mapping. In Proceedings of the IEEE/R SJi n t e r -\nnational conference on intelligent robots and s ystems (IROS) (pp.\n811–818). https://doi.org/10.1109/IROS.2017.8202243.\nTaniguchi, A., Taniguchi, T., & Inamura, T. (2018a). Unsupervised spa-\ntial lexical acquisition by updating a language model with place\nclues. Robotics and A utonomous Systems, 99, 166–180. https://\ndoi.org/10.1016/j.robot.2017.10.013.\nTaniguchi, T., Ugur, E., Hoffmann, M., Jamone, L., Nagai, T., Rosman,\nB., et al. (2018b). Symbol emergence in cognitive developmental\nsystems: a survey. IEEE transactions on cognitive and devel-\nopmental s ystems (pp. 1–1). https://doi.org/10.1109/TCDS.2018.\n2867772.\nThrun, S., Burgard, W., & Fox, D. (2005). Probabilistic robotics\n.C a m -\nbridge: MIT Press.\nUeda, R., Mizuta, K., Yamakawa, H., & Okada, H. (2016). Particle ﬁlter\non episode for learning decision making rule. In Proceedings of the\ninternational conference on intelligent a utonomouss ystems( I AS)\n(pp. 737–754). Springer\nWalter, M.R., Hemachandra, S., Homberg, B., Tellex, S., & Teller, S.\n(2013). Learning semantic maps from natural language descrip-\ntions. In Proceedings of robotics: science and s ystems( R SS).\nZhou, B., Lapedriza, A., Khosla, A., Oliva, A., & Torralba, A. (2018).\nPlaces: A 10 million image database for scene recognition. IEEE\nTransactions on Pattern Analysis and Machine Intelligence , 40(6),\n1452–1464.\nPublisher’s Note Springer Nature remains neutral with regard to juris-\ndictional claims in published maps and institutional afﬁliations.\nAkira Taniguchi received his B.E.,\nM.E., and Ph.D. degree from Rit-\nsumeikan University, Kyoto, Japan,\nin 2013, 2015, and 2018, respec-\ntively. From April 2017 to March\n2018, he was a research fellow of\nJapan Society for the Promotion\nof Science (DC2). From April 2018\nto March 2019, he was a research\nfellow of Japan Society for the\nPromotion of Science (PD). From\nApril 2019, he is currently a Spe-\ncially Appointed Assistant Pro-\nfessor at the College of Infor-\nmation Science and Engineering,\nRitsumeikan University. His research interests include intelligent\nrobotics, artiﬁcial intelligence, and symbol emergence in robotics.\nYoshinobu Hagiwara received his\nPh.D. degree from Soka Univer-\nsity, Japan, in 2010. He was an\nAssistant Professor at the Depart-\nment of Information Systems Sci-\nence, Soka University from 2010,\na Specially Appointed Researcher\nat the Principles of Informatics\nResearch Division, National Insti-\ntute of Informatics from 2013,\nand an Assistant Professor at the\nDepartment of Human and Com-\nputer Intelligence, Ritsumeikan\nUniversity from 2015. He is cur-\nrently a Lecture at the Department\nof Information Science and Engineering, Ritsumeikan University. His\nresearch interests include human-robot interaction, machine learning,\nintelligent robotics and symbol emergence in robotics. He is a member\nof IEEE, RSJ, IEEJ, JSAI, SICE, and IEICE.\n123\n946 Autonomous Robots (2020) 44:927–946\nTadahiro Taniguchi received M.E.,\nand Ph.D. degrees from Kyoto\nUniversity in 2003 and 2006,\nrespectively. From April 2005 to\nMarch 2008, he was a research\nfellow of Japan Society for the\nPromotion of Science. He was an\nassistant professor from April 2008\nto March 2010, an associate pro-\nfessor from April 2010 to March\n2017. He has been a professor in\nthe College of Information Sci-\nence and Engineering,\nRitsumeikan University since\nApril 2017, and a visiting general\nchief scientist in Panasonic since April 2017. From September 2015\nto September 2016, he was a visiting associate professor at Imperial\nCollege London. He is currently engaged in research on artiﬁcial intel-\nligence, symbol emergence in robotics and emergent systems.\nTetsunari Inamura received B.E.,\nM.S., and PhD. degrees from the\nUniversity of Tokyo in 1995, 1997,\nand 2000, respectively. He was\na Researcher of the JST/CREST\nprogram from 2000 to 2003, and\nthen joined the Department of\nMechano-Informatics, School of\nInformation Science and Technol-\nogy, the University of Tokyo as a\nLecturer till 2006. He is now an\nAssociate Professor in National\nInstitute of Informatics and the\nDepartment of Informatics, SOK-\nENDAI (The Graduate University\nfor Advanced Studies). His research interests include imitation learn-\ning, human motion analysis, and development of interactive robots\nthrough virtual reality.\n123",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.9078422784805298
    },
    {
      "name": "Scalability",
      "score": 0.6397002339363098
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5625069737434387
    },
    {
      "name": "Realization (probability)",
      "score": 0.5203094482421875
    },
    {
      "name": "Machine learning",
      "score": 0.4802301526069641
    },
    {
      "name": "Probabilistic logic",
      "score": 0.4417329728603363
    },
    {
      "name": "Robot",
      "score": 0.41444656252861023
    },
    {
      "name": "Categorization",
      "score": 0.4116147756576538
    },
    {
      "name": "Algorithm",
      "score": 0.36265993118286133
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I135768898",
      "name": "Ritsumeikan University",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I184597095",
      "name": "National Institute of Informatics",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I200475212",
      "name": "The Graduate University for Advanced Studies, SOKENDAI",
      "country": "JP"
    }
  ],
  "cited_by": 35
}