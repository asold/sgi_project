{
  "title": "How to Query Language Models?",
  "url": "https://openalex.org/W3193084543",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4221350750",
      "name": "Adolphs, Leonard",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226723678",
      "name": "Dhuliawala, Shehzaad",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2247984814",
      "name": "Hofmann, Thomas",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3033187248",
    "https://openalex.org/W2996887765",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W3126960149",
    "https://openalex.org/W3100283070",
    "https://openalex.org/W2141599568",
    "https://openalex.org/W3002104146",
    "https://openalex.org/W2971869958",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W3034942609",
    "https://openalex.org/W2986266667",
    "https://openalex.org/W2785611959",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W2316250576",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W3035850279",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3001393026",
    "https://openalex.org/W2481265265",
    "https://openalex.org/W2964207259",
    "https://openalex.org/W2954579883",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2460442863"
  ],
  "abstract": "Large pre-trained language models (LMs) are capable of not only recovering linguistic but also factual and commonsense knowledge. To access the knowledge stored in mask-based LMs, we can use cloze-style questions and let the model fill in the blank. The flexibility advantage over structured knowledge bases comes with the drawback of finding the right query for a certain information need. Inspired by human behavior to disambiguate a question, we propose to query LMs by example. To clarify the ambivalent question \"Who does Neuer play for?\", a successful strategy is to demonstrate the relation using another subject, e.g., \"Ronaldo plays for Portugal. Who does Neuer play for?\". We apply this approach of querying by example to the LAMA probe and obtain substantial improvements of up to 37.8% for BERT-large on the T-REx data when providing only 10 demonstrations--even outperforming a baseline that queries the model with up to 40 paraphrases of the question. The examples are provided through the model's context and thus require neither fine-tuning nor an additional forward pass. This suggests that LMs contain more factual and commonsense knowledge than previously assumed--if we query the model in the right way.",
  "full_text": "How to Query Language Models?\nLeonard Adolphs Shehzaad Dhuliawala\nDepartment of Computer Science\nETH Zürich\n{firstname.lastname}@inf.ethz.ch\nThomas Hofmann\nAbstract\nLarge pre-trained language models (LMs) are\ncapable of not only recovering linguistic but\nalso factual and commonsense knowledge. To\naccess the knowledge stored in mask-based\nLMs, we can use cloze-style questions and\nlet the model ﬁll in the blank. The ﬂexibil-\nity advantage over structured knowledge bases\ncomes with the drawback of ﬁnding the right\nquery for a certain information need. Inspired\nby human behavior to disambiguate a question,\nwe propose to query LMs by example. To clar-\nify the ambivalent question Who does Neuer\nplay for? , a successful strategy is to demon-\nstrate the relation using another subject, e.g.,\nRonaldo plays for Portugal. Who does Neuer\nplay for? . We apply this approach of query-\ning by example to the LAMA probe and obtain\nsubstantial improvements of up to 37.8% for\nBERT-large on the T-REx data when provid-\ning only 10 demonstrations—even outperform-\ning a baseline that queries the model with up\nto 40 paraphrases of the question. The exam-\nples are provided through the model’s context\nand thus require neither ﬁne-tuning nor an ad-\nditional forward pass. This suggests that LMs\ncontain more factual and commonsense knowl-\nedge than previously assumed—if we query\nthe model in the right way.\n1 Introduction\nLanguage Models (LM) are omnipresent in modern\nNLP systems. In just a few years, they’ve been es-\ntablished as the standard feature extractor for many\ndifferent language understanding tasks (Karpukhin\net al., 2020; Zhang et al., 2020; Wang et al., 2019;\nHe et al., 2020). Typically, they are used to create a\nlatent representation of natural language input and\nthen ﬁne-tuned to the task at hand. However, recent\nwork (Petroni et al., 2019; Jiang et al., 2020; Brown\net al., 2020; Roberts et al., 2020) has shown that\noff-the-shelve language models capture not only lin-\nguistic features but also large amounts of relational\nknowledge, not requiring any form of re-training.\nGeorge Robert Gray died in [MASK].office34.0 %infancy10.2 %London10.1 %\nLondon34.3 %Frankfurt7.9 %Berlin5.3 %[MASK] =\nNo Example\nExampleFritz Umgelterdied in Frankfurt.George Robert Gray died in [MASK].\n[MASK] =\nFigure 1: BERT’s top-3 predictions with probabilites\nwhen prompted with the cloze-style question (top) ver-\nsus when prompted with one additional example of the\nsame relation (bottom).\nThe LAMA probe by Petroni et al. (2019) was\ndesigned to quantify the amount of relational\nknowledge present in (mask-based) language mod-\nels. While the task of predicting the right object\nfor a subject-relation tuple remains the same as\nfor a standard knowledge base (KB) completion\nquery, the input is structured in a cloze-style sen-\ntence. For example, a KB completion query of\nthe form (Dante, born-in, X) becomes \"Dante was\nborn in [MASK].\". Petroni et al. (2019) show that\nBERT (Devlin et al., 2019) performs on par with\ncompetitive specialized models on factual and com-\nmonsense knowledge. The performance on this\ntask can only be seen as a lower bound to the ac-\ntual knowledge present in language models as the\nchoice of natural language template for a given re-\nlation might be suboptimal (Petroni et al., 2019;\nJiang et al., 2020). The more general question here\nis \"How to query an LM for a speciﬁc information\nneed?\". Jiang et al. (2020) propose to use multi-\nple paraphrases of the probe and then aggregate\nthe solutions. Petroni et al. (2020), on the other\nhand, add relevant context. Both approaches can\nbe linked to common human behavior. In human\narXiv:2108.01928v1  [cs.CL]  4 Aug 2021\ndialog, a question can be made more precise both\nby paraphrasing or adding additional context infor-\nmation. Since language models are trained on large\namounts of human-generated data, the intuition\nof phrasing the information need most naturally\nseems obvious. Humans excel at pattern recogni-\ntion and pattern continuation for many different\nmodes of representation (Shugen, 2002). Concepts\nembedded in language are no exception to this.\nTherefore, another common way to probe a hu-\nman’s knowledge is by providing examples and\nasking them to transfer the relation provided to a\nnew object. For example, asking Who plays Neuer\nfor? is ambiguous as both Bayern Munich and Ger-\nmany would be correct answers. However, when\ncontextualizing the question with an example, the\nanswer is clear: I know Ronaldo plays for Portugal.\nWho plays Neuer for?.\nIn this work, we apply the concept of querying\nby example to probe language models. Additional\nto the cloze-style question, we provide other exam-\nples of the same relation to the model’s input. The\nprevious example’s input then becomes\"Ronaldo\nplays for Portugal. Neuer plays for [MASK].\". We\nshow that by providing only a few demonstrations,\nstandard language models’ prediction performance\nimproves drastically. So much so that for the TREx\ndataset, it becomes an even more powerful tech-\nnique to retrieve knowledge than using an ensem-\nble of up to 40 different paraphrases (Jiang et al.,\n2020), while requiring only a single forward pass\ninstead of 40.\n2 Related Work\nLanguage Model Probes Petroni et al. (2019)\nstarted to investigate how much factual and com-\nmonsense knowledge LMs posses. They released\nthe LAMA probe, which is a dataset consisting of\nT-REx (Elsahar et al., 2018), Google-RE, Concept-\nNet (Speer et al., 2018), and SQuAD (Rajpurkar\net al., 2016). Each dataset is transformed to be a\ncollection of ⟨subject, relation, object⟩-triplets and\npruned to only contain single token objects present\nin BERT’s vocabulary. Additionally, they provide\ntemplates in natural language for each relation.\nTheir investigation reveals that BERT-large has re-\nmarkable capabilities in recalling factual knowl-\nedge, competitive to supervised baseline systems.\nSince there is usually more than one way to ex-\npress a relation, the LAMA probe score can only\nbe regarded as a lower bound (Petroni et al., 2019;\nJiang et al., 2020). To tighten this lower bound,\nJiang et al. (2020) propose an automatic discover-\ning mechanism for paraphrases together with an\naggregation scheme. By querying the LM with a\ndiverse set of prompts, they signiﬁcantly improve\nthe LAMA probe’s baseline numbers for BERT\nmodels. However, this approach incurs the cost of\nadditional queries to the LM, an optimization pro-\ncedure to aggregate the results, and the extraction\nof paraphrases.\nMachine reading comprehension (MRC) and open-\ndomain question answering (QA) are ﬁelds in NLP\ndominated by large pre-trained LMs. Here, the\npremise typically is that the model is capable of\nextracting the answer from the provided context,\nrather than having it stored in its parameters 1.\nPetroni et al. (2020) extend this line of thought\nto retrieve factual knowledge from LMs by pro-\nviding relevant context but without ﬁne-tuning the\nmodel. Their experiments show that providing rel-\nevant passages signiﬁcantly improves the scores on\nthe LAMA probe for BERT models.\nFew-Shot Learning The term few-shot learning\nrefers to the practice of only providing a few exam-\nples when training a model, compared to the typ-\nical approach of using large datasets (Wang et al.,\n2020). In the NLP domain, recent work by Brown\net al. (2020) suggests to use these few examples\nonly in the context, as opposed to actually training\nwith it. Fittingly, they call this approach in-context\nlearning. Here, they condition the model on a natu-\nral language description of the task together with\na few demonstrations. Their experiments reveal\nthat the larger the model, the better its in-context\nlearning capabilities. Our approach is very simi-\nlar to in-context learning, with the difference that\nwe do not provide a description of the task and\nutilize natural language templates for the relations.\nThe motivation is that this should closely resem-\nble human behavior of providing examples of a\nrelation: instead of providing a list of subject and\nobjects and let the other person ﬁgure out the re-\nlation, a human typically provides the subject and\nobjects embedded in the template relation. More-\nover, we understand our approach not as a learning\nmethod, but rather as a querying technique that dis-\nambiguates the information need.\nSchick and Schütze (2020b) argue that small LMs\n1With the notable exception of the work of Roberts et al.\n(2020), which uses a T-5 model without any access to an\nadditional knowledge base.\ncan be effective for few-shot learning too. How-\never, they approach the problem of limited exam-\nples differently; instead of providing it as condi-\ntioning in the input, they actually train with it. By\nembedding the data into relation templates, they\nobtain training data that is closer in style to the\npre-training data and, thus, can learn with fewer\nsamples. Gao et al. (2020) take this concept even\nfurther and automate the template generation. Ad-\nditionally, they also ﬁnd that—when ﬁne-tuning\nwith few samples—providing good demonstrations\nin the context improves the model’s performance.\n3 Background\n3.1 Language Models for cloze-style QA\nIn this work, we probe mask-based language mod-\nels for their relational knowledge. The considered\nfacts are triplets consisting of a subject, a rela-\ntion, and an object ⟨s,r,o⟩. Language models are\ntrained to predict the most probable word given\nthe (surrounding) context. Hence, to test a model’s\nfactual knowledge, we feed it natural text with the\nobject masked out. This requires a mapping from\nthe relation rto a natural language prompt tr with\nplaceholders for subject and object, e.g., the re-\nlation r = age becomes tr = [s] is [o] years old .\nWhen probing for a single ⟨s,r,o⟩-triplet, the in-\nput to the language model is the natural language\nprompt tr of the relation rtogether with the subject\ns. It outputs a likelihood score PLM for each token\nin its vocabulary Vwhich we use to construct a\ntop-kprediction subset V′for the object o:\nV′= arg max\nV′⊂V,|V′|=k\n∑\no′∈V′\nPLM(o′|s,tr) (1)\nThe language model succeeds for the triplet @kif\no∈V′. For example, we say that it knows the fact\n⟨s= Tiger Woods,r = age,o = 45⟩@3, if for the\nquery \"Tiger Woods is [MASK] years old\"it ranks\nthe token \"45\" within the top-3 of the vocabulary.\n3.2 Datasets\nWe use the LAMA probe in our experi-\nments (Petroni et al., 2019). It’s a collection of\nfactual and commonsense examples provided as\n⟨s,r,o⟩-triplets2 with single token objects. More-\nover, it provides human-generated templates tr\nfor each relation r. The statistics about the three\nconsidered corpora T-REx (Elsahar et al., 2018),\n2We do not consider the SQuAD dataset of the probe as it\nhas no clear notion of relation.\nCorpus Relation Statistics\n#Facts #Relations\nGoogle-RE\nbirth-place 2937 1\nbirth-date 1825 1\ndeath-place 765 1\nTotal 5527 3\nT-REx\n1-1 937 2\nN-1 20006 23\nN-M 13096 16\nTotal 34039 41\nConceptNet Total 11458 16\nTable 1: Statistics for the corpora of the LAMA data.\nGoogle-RE3, and ConceptNet (Speer et al., 2018)\nare provided in Table 1.\n3.3 Models\nWe investigate the usefulness of querying by\nexample, for three individual language models:\nBERT-base, BERT-large (Devlin et al., 2019), and\nALBERT-xxl (Lan et al., 2020). These models are\namong the most frequently used language mod-\nels these days4. For both BERT models, we con-\nsider the cased variant, unless explicitly noted oth-\nerwise.\n4 Method\nOur proposed method for querying relational\nknowledge from LMs is simple yet effective.\nWhen we construct the query for the triplet⟨s,r,o⟩,\nwe provide the model with additional samples\n{⟨s′,r,o′⟩,⟨s′′,r,o′′⟩,... }of the same relation\nr. These additional examples are converted\nto their natural language equivalent using the\ntemplate tr and prepend to the cloze-style sentence\nrepresentation of ⟨s,r,o⟩. The intuition is that the\nnon-masked examples provide the model with an\nidea of ﬁlling in the gap for the relation at hand. As\ncan be seen in Figure 1, providing a single example\nin the same structure clariﬁes the object requested\nfor both humans and BERT. This is particularly\nuseful when the template tr does not capture the\ndesired relation rbetween subject sand object o\nunambiguously, which in natural language is likely\nto be the case for many relations. In this sense, it\ntries to solve the same problem as paraphrasing. A\n3https://github.com/google-research-datasets/\nrelation-extraction-corpus\n4According to the statistics from https:\n//huggingface.co/models?filter=pytorch,\nmasked-lm.\nquery is paraphrased multiple times to align the\nmodel’s understanding of the query with the actual\ninformation need. When we provide additional\nexamples, we do the same by showing the model\nhow to apply the relation to other instances and\nask it to generalize. Of course, the model does not\nreason in this exact way; rather, through its training\ndata, it is biased towards completing patterns as\nthis is a ubiquitous behavior in human writing.\nQuery Predictions\nNo Example\nRodmarton5is a . farmer (3.9%)\nbusinessman (2.5%)\nRandom Example\nM.S.I. Airport is a airport.\nRodmarton is a . town (16.9%)\nvillage (14.7%)\nClose Example\nNantmor is a village.\nRodmarton is a . village (75.5%)\nhamlet (16.0%)\nArrow Operator\nTotopara →village\nThe argument →album\nTisza →river\nRodmarton → village (21.4%)\ntown (8.7%)\nTable 2: Example queries with predictions (from BERT-\nlarge) for the different querying methods. The correct\nanswer is marked in bold.\nSince we only adjust the context fed to the model,\nwe do not incur the cost of additional forward\npasses. When paraphrasing, on the other hand,\neach individual template requires another query\nto the model. Moreover, our approach does not\nrequire any learning, i.e., backward passes, and\nhence is very different from the classic ﬁne-tuning\napproach and pattern-exploiting training (Schick\nand Schütze, 2020a,b).\nIn Table 2, we compare different approaches\nof querying by example. The left column shows\nthe input to the model, i.e., the query. The right\ncolumn shows BERT-large’s top-2 prediction,\nwith its corresponding probabilities 6. The ﬁrst\nrow of the table shows that completing the is-a\nrelation for the village Rodmarton is tricky for the\nmodel. Its top predictions are not even close to\nthe correct answer suggesting that BERT either\ndoes not know about this particular village or that\n5A village in South West England.\n6The probabilities are obtained by applying a softmax on\nthe logit output over the token vocabulary.\nthe information need is not well enough speciﬁed.\nInterestingly, when prepending the query with\nanother random example of the same relation (2nd\nrow), the model’s top predictions are town and\nthe ground-truth village. This proves that BERT\nknows the type of instance Rodmarton is; only the\nextraction method (the cloze-style template) was\nnot expressive enough.\nClose Examples When humans use examples,\nthey typically do not use a completely random sub-\nject but use one that is, by some measure, close\nto the subject at hand. In our introductory exam-\nple, we used Ronaldo to exemplify an information\nneed about Neuer. It would have been unnatural\nto use a musician here, even when describing a\nformally correct plays-for relation with them. We\nextend our approach by only using examples for\nwhich the subject is close in latent space to the\nsubject querying for. We use the cosine similarity\nbetween the subject encodings using BERT-base.\nMore formally, we encode a subject susing\nfθ(s) = Bθ([CLS] + s+ [SEP])CLS, (2)\nwith B(x)CLS being the BERT encoding of the\nCLS-token for the input x, and θbeing the BERT\nmodel’s parameters. We then obtain the top-kmost\nsimilar subjects to sin the dataset Dthrough maxi-\nmizing the cosine similarity, i.e.,\nD′= arg max\nD′⊂D\\{s},|D′|=k\n∑\ns′∈D′\nfθ(s)⊤fθ(s′)\n∥fθ(s)∥∥fθ(s′)∥\n(3)\nFrom the top-ksubset of most similar subjects\nD′, we randomly sample to obtain our priming ex-\namples. Table 2 (3rd row) shows the chosen close\nexample to Rodmarton, which is Nantmor, another\nsmall village in the UK. Provided with this particu-\nlar example, BERT-large predicts the ground-truth\nlabel village with more than 75% probability.\nArrow Operator Brown et al. (2020) propose to\nuse LMs as in-context learners. They suggest pro-\nviding \"training\" examples in the model’s context\nusing the arrow operator, i.e., to express an⟨s,r,o⟩\ntriplet they provide the model with s⇒o. We can\napply this concept to the LAMA data by using the\nsame template tr =\" [s] ⇒[o]\" ∀r. In Table 2 (last\nrow), we see that by providing a few examples of\nthe is-a relation, BERT-large can rank the ground-\ntruth highest even though the relationship is never\nexplicitly described in natural language. However,\nnot using a natural language template makes the\nmodel less conﬁdent in its prediction, as can be\nseen by the lower probability mass it puts on the\ntarget.\n5 Results\nWe focus the reporting of the results on the mean\nprecision at k (P@k) metric. In line with previous\nwork (Petroni et al., 2019, 2020; Jiang et al., 2020)7,\nwe compute the results per relation and then aver-\nage across all relations of the dataset. More for-\nmally, for the dataset D= {R1,..., Rn}that con-\nsists of nrelations where each relation has multiple\ndatapoints ⟨x,y⟩, we compute the P@k score as:\nP@k= 1\n|D|\n∑\nRi∈D\n1\n|Ri|\n∑\n⟨x,y⟩∈Ri\n1 V′x (y), (4)\nwhere 1 denotes the indicator function that is 1 if\nthe ground truth yis in the top-k prediction set V′\nfor the input xand 0 otherwise.\nTable 3 shows the P@1 scores of different mod-\nels and querying approaches across the LAMA\nprobe’s corpora. While for the Google-RE data,\nproviding additional examples shows to be detri-\nmental, we see massive prediction performance\ngains for T-REx and ConceptNet. Most notably, the\nP@1 score of BERT-large on T-REx increases by\n37.8% to 44.8% when providing 10 close examples.\nSimilarly, the lower bound on Albert’s performance\nfor T-REx (ConceptNet) can be improved by up to\n72.3% (25.0%) with 10 close examples.\nGoogle-RE For the Google-RE subset of the\ndata, querying by example hurts the predictive ca-\npabilities of LMs. In the following, we provide an\nintuition of why we think this is the case. Look-\ning at the baseline numbers of the individual rela-\ntions for this data, we see that the performance is\nlargely driven by predicting a person’s birth and\ndeath place; the birth-date relation doesn’t play a\nsigniﬁcant role because BERT is incapable of ac-\ncurately predicting numbers (i.e., dates) (Lin et al.,\n2020; Wallace et al., 2019). The birth and death\nplace of a person BERT-large predicts correctly\n7The P@1 score corresponds to Jiang et al. (2020)’s micro-\naveraged accuracy\n16.1% and 14.0% of the time, respectively; signiﬁ-\ncantly lower than the 32.5% P@1 score among the\nrelations of the T-REx data. Recent work describes\nthat BERT has a bias to predict that a person with,\ne.g., an Italian sounding name is Italian (Rogers\net al., 2020; Poerner et al., 2020). We suspect that\nthis bias helps BERT predict birth and death places\nwithout knowing the actual person, and therefore it\nis not an adequate test of probing an LMs factual\nknowledge. As a consequence, the predictions it\nmakes are more prone to errors when inﬂuenced by\nprevious examples.\nT-REx Figure 2 depicts the mean precision at\n1 on the T-REx corpus for a varying number of\nexamples provided. It shows that even a few ad-\nditional examples can signiﬁcantly improve the\nperformance of the LMs. However, there is a satu-\nration of usefulness for more examples that seems\nto be reached at around 10 examples already. In-\nterestingly, with 10 examples, BERT-large even\nslightly improves upon the optimized paraphrase\nbaseline from Jiang et al. (2020), while only requir-\ning a single forward pass.\nTable 4 shows the improvement in P@1 score for\nthe individual relations that most (and least) bene-\nﬁt from additional examples for BERT-large. The\nrelations for which demonstrations improve the\nperformance the most typically have one thing in\ncommon: they are ambiguous. Prototypical am-\nbiguous relations like located-in or is-a are among\nthe top beneﬁting relations. One rather untypi-\ncal improvement candidate is the top-scoring one\nof religion-afﬁliation. Suspiciously, this is also\nthe most improved relation by the paraphrasing of\nJiang et al. (2020). A closer look at the examples\nreveals the cause: the target object labels for the\nreligions are provided as nouns (e.g., Christianity,\nIslam), while the template ( [s] is afﬁliated with\nthe [o] religion) indicates to use the religion as an\nadjective (e.g., Christian, Islamic). Hence, both\nparaphrasing the sentence such that it is clear to\nuse a noun or providing example sentences that\ncomplete the template with nouns alleviate this\nproblem. The relations that beneﬁt the least from\ndemonstrations are unambiguous, like capital-of\nor developed-by.\nConceptNet While T-REx probes for factual\nknowledge, the ConceptNet corpus is concerned\nwith commonsense relations. The improvements\nof querying by example are signiﬁcant with\nCorpus Relation Baselines LM\nBb Bl Al Bb opt Blopt Bb3 Bb10 Bb10ce Bl3 Bl10 Bl10ce Al10ce\nGoogle-RE\nbirth-place14.9 16.1 6.3 - - 10.5±0.4 13.2±0.3 11.7±0.3 8.9±0.5 11.5±0.3 11.0±0.3 7.0±0.3\nbirth-date 1.6 1.5 1.5 - - 1.1±0.3 1.1±0.2 1.2±0.1 1.4±0.3 1.4±0.2 1.5±0.1 1.4±0.3\ndeath-place13.1 14.0 2.0 - - 9.2±0.5 11.8±0.7 10.4±1.0 7.2±0.7 9.1±0.5 8.5±1.1 5.0±0.6\nTotal 9.9 10.5 3.3 10.4 11.3 6.9±0.1 8.7±0.2 7.8±0.4 5.8±0.4 7.4±0.1 7.0±0.4 4.5±0.3\nT-REx\n1-1 68.0 74.5 71.2 - - 59.7±0.6 62.0±0.6 62.6±0.8 66.4±0.9 67.6±0.6 68.7±0.7 69.0±0.7\nN-1 32.4 34.2 24.9 - - 32.3±0.1 37.9±0.2 41.7±0.4 38.8±0.2 44.8±0.2 47.9±0.2 45.0±0.2\nN-M 24.7 24.8 17.2 - - 27.9±0.4 31.3±0.2 34.8±0.1 31.4±0.4 35.0±0.1 37.2±0.3 33.5±0.2\nTotal 31.1 32.5 24.2 39.6 43.9 31.9±0.2 36.5±0.2 40.0±0.2 37.3±0.2 42.1±0.2 44.8±0.1 41.7±0.1\nConceptNet Total 15.9 19.5 21.2 - - 15.2±0.2 16.2±0.2 17.1±0.2 19.6±0.3 21.2±0.2 22.0±0.3 26.5±0.2\nTable 3: Mean precision at one (P@1) in percent across the different corpora of the LAMA probe. The baseline\nmodels shown are BERT-base (Bb), BERT-large (Bl), Albert-xxlarge-v2 (Al), and the best versions of BERT-large\nand BERT-base by Jiang et al. (2020) that are optimized across multiple paraphrases 8(Bbopt and Blopt). The LM\nsection on the right shows the results for different querying by example approaches. Here, the superscript denotes\nthe number of examples used and the subscript ce denotes that only close examples have been used. Since the\nchoice of examples alters the predictions of the model and thus introduces randomness, we provide the standard\ndeviation measured over 10 evaluations.\n0 5 10 15 20\n# Examples\n0.0\n0.1\n0.2\n0.3\n0.4P@1\nTREx\nbert-large\nbert-base\nalbert-xxlarge\nFigure 2: P@1 score for TREx over the number of ex-\namples provided. The dashed line shows the baseline\nvalue for when no additional example is given.\n12%, 7.5%, and 25% relative improvement for\nBERT-base, BERT-large, and Albert-xxlarge.\nMore detailed plots for all the corpora and sev-\neral metrics are provided in Appendix A.4.\n5.1 The Change of Embedding\nTo further investigate the disambiguation effect\nof additional examples, we take a look at the\nlatent space. In particular, we’re interested in\nhow the clusters of particular relations, formed by\nthe queries’ embeddings, change when providing\nthe context with additional examples. Figure 3\nvisualizes BERT-large’s [CLS]-token embedding\nfor queries from the T-REx corpus, using t-SNE\n(van der Maaten and Hinton, 2008). The individ-\nual colors represent the relations of the queries.\nThe ﬁrst two images depict the clustering when\nID Template ∆P@1\nn=1 n=3 n=5\nP140 [s] is afﬁliated with the [o] religion . 51.0 67.4 70.0\nP30 [s] is located in [o] . 47.8 55.3 55.8\nP136 [s] plays [o] music . 12.8 44.0 54.5\nP31 [s] is a [o] . 8.2 20.3 24.4\n. . .\nP178 [s] is developed by [o] . -8.3 -4.2 -6.8\nP1376 [s] is the capital of [o] . -16.3 -8.2 -8.6\nTable 4: List of relations of T-REx that beneﬁt the most\n(least) by additional examples. The right column pro-\nvides the improvement in precision at 1 score when {1,\n3, 5} examples are provided for BERT-large.\nusing the natural language template without addi-\ntional demonstrations (left) and ten demonstrations\n(right). The fact that the clusters become better\nseparated is visual proof that providing examples\ndisambiguates the information need expressed by\nthe queries. The two plots on the right show the\nclustering when instead of a natural language tem-\nplate, the subject and object are only separated by\nthe arrow operator \"⇒\". Here, we see an even more\nsigniﬁcant change in separability when providing\nadditional demonstrations, as the actual informa-\ntion need is more ambiguous.\n5.2 TextWorld Commonsense Evaluation\nAn emerging ﬁeld of interest inside the NLP com-\nmunity is text-based games (TBG). An agent is\nplaced inside an interactive text environment in\nthese games and tries to complete speciﬁed goals–\nonly using language commands. To succeed, it\n8These models involve one query to the model per para-\nphrase.\n50\n 0 50\n50\n0\n50\nNL Template w/ 0 Examples\n50\n 0 50\n50\n0\n50\nNL Template w/ 10 Examples\n100\n 0 100\n50\n0\n50\n{} => {} Template w/ 0 Examples\n50\n 0 50\n50\n0\n50\n{} => {} Template w/ 10 Examples\n[s] was born in [o] .\n[s] died in [o] .\n[s] is [o] citizen .\n[s] is a subclass of [o] .\n[s] is located in [o] .\n[s] is a [o] .\n[s] is a member of [o] .\n[s] shares border with [o] .\nFigure 3: BERT-large’s [CLS]-token embedding of a subset of T-REx queries visualized in two dimensions using\nt-SNE (van der Maaten and Hinton, 2008). Each point is a single query and the color represents the corresponding\nrelation class. The ellipses depict the 2-std conﬁdence intervals. The individual images show the clustering for\nboth the natural language and the ([s]; [o]) template with either no examples or ten examples provided.\nrequires a deep language understanding to decide\nwhat are reasonable actions to take in the scene that\nmove it closer to its ﬁnal goal. These environments\nare often modeled on real-world scenes to foster\nthe commonsense-learning capabilities of an agent.\nThe TextWorld Commonsense (TWC) game world\nby Murugesan et al. (2020) focus speciﬁcally on\nthis aspect. There, the agent is placed in a typical\nmodern-house environment to tidy up the room.\nThis involves moving all the objects in the scene to\ntheir commonsense location, e.g., the dirty dishes\nbelong in the dishwasher and not in the cupboard.\nMurugesan et al. (2020) approach this problem by\nequipping the agent with access to a commonsense\nknowledge base. Replacing a traditional KB with\nan LM for this task is very intriguing as the LM has\nrelational knowledge stored implicitly and is capa-\nble of generalizing to similar objects. To test the\nfeasibility of using LMs as commonsense knowl-\nedge source in the TWC environment, we design the\nfollowing experiment9: We use a static agent that\npicks up any misplaced objectoat random and puts\nit to one of the possible locations lin the scene ac-\ncording to a speciﬁc prior p(l|o). This prior p(l|o)\nis computed at the start of an episode for all object-\nlocation combinations in the scene, using an LM.\nWe use the arrow operator as described in Table 2\nand vary the number of examples provided. In Fig-\nure 4, we show the result for albert-xxlarge on the\nhard games of TWC, compared to a simple uniform\nprior (i.e., p(li|o) = const.∀i), and Murugesan\net al. (2020)’s RL agent with access to a common-\nsense KB. We see the same trend as in the LAMA\nexperiments: providing additional examples of the\nsame relation boosts performance signiﬁcantly and\nsaturates after 10-15 instances.\n9Details and the pseudocode are provided in Apendix A.3\n0 5 10 15 20\n#/uni00A0Examples\n0.40\n0.45\n0.50\n0.55\n0.60Normalized/uni00A0Score\nTextWorld/uni00A0Commonsense\nTWC\nuniform/uni00A0prior\nalbert/uni00ADxxlarge\nFigure 4: Normalized score for the hard games of the\nTWC environment over the number of examples pro-\nvided for albert-xxlarge. The dashed baselines are\nthe static agent with a uniform prior and the TWC\ncommonsense agent by Murugesan et al. (2020). The\nshaded regions depict the standard deviation over 10\nruns.\n5.3 Word Analogy Evaluation\nTo evaluate the usefulness of querying pre-trained\nlanguage models by examples for linguistic knowl-\nedge, we move to the word analogy task—a stan-\ndard benchmark for non-contextual word embed-\ndings. This evaluation is based on the premise\nthat a good global word embedding deﬁnes a latent\nspace in which basic arithmetic operations corre-\nspond to linguistic relations (Mikolov et al., 2013b).\nWith the rise of contextual word embeddings and\nlarge pre-trained language models, this evaluation\nhas lost signiﬁcance. However, we consider ap-\nproaching this task from the angle of querying\nlinguistic knowledge from an LM instead of per-\nforming arithmetics in latent space. By providing\nexamples of the linguistic relation with a regular\npattern in the context of the LM, we prime it to\napply the relation to the ﬁnal word with its masked\nout correspondence.\n0 5 10 15 20\n# Examples\n0.0\n0.1\n0.2\n0.3\n0.4p@1\nGloVe\nSVD\nBATS ([s]; [o])\nbert-large\nbert-base\nalbert-xxlarge\nFigure 5: P@1 score on BATS over the number of ex-\namples provided. The performance of the GloVe and\nSVD benchmark models by Gladkova et al. (2016) is\nshown with the black, dashed lines.\nWe consider the Bigger Analogy Test Set (BATS)\n(Gladkova et al., 2016) for our experiments. BATS\nconsists of 40 different relations covering inﬂec-\ntional and derivational morphology, as well as lex-\nicographic and encyclopedic semantics. Each re-\nlation consists of 50 unique word pairs. However,\nsince most pre-trained LMs, including BERT and\nAlbert, use subword-level tokens for their vocab-\nulary, not all examples can be solved. In particu-\nlar, 76.1% and 76.2% of the targets are contained\nin BERT’s and Albert’s vocabulary, respectively—\nupper bounding their P@1 performance.\nFigure 5 depicts the P@1 score10 for the individual\nLMs on BATS. Noticeably, also on this task, the\nLMs beneﬁt from additional examples up to a cer-\ntain threshold for which the usefulness stagnates.\nBoth BERT models do not beat Gladkova et al.\n(2016)’s GloVe (Pennington et al., 2014) bench-\nmark. This is in part because not all targets are\npresent in the token vocabulary. Considering only\nthe solvable word pairs, BERT-large achieves a\nP@1 score of 30.6% with 15 examples—beating\nthe GloVe baseline achieving 28.5%. Interestingly,\nAlbert-xxlarge outperforms all other models, in-\ncluding the baselines, by a large margin. Figure 7\nin Appendix A.4 breaks down the LM’s perfor-\nmance across the different relations of BATS and\ncompares it against the GloVe baseline. Albert\nbeats GloVe on almost all relations where its vo-\ncabulary does not limit it; the most signiﬁcant im-\nprovements are in the derivational morphology and\nlexicographic semantics categories. It is outper-\nformed by GloVe only on two relations: coun-\n10The P@1 score corresponds to Gladkova et al. (2016)’s\nreported accuracy score.\ntry:capital and UK city:county. Especially the for-\nmer country:capital category is very prominent and\nconstituted 56.7% of all semantic questions of the\noriginal Google test set (Mikolov et al., 2013a)—\npotentially inﬂuencing the design and tuning of\nnon-contextual word embeddings.\n6 Discussion\nAugmenting the context of LMs with demonstra-\ntions is a very successful strategy to disambiguate\nthe query. Notably, it is as successful, on TRE-x,\nas using an ensemble of multiple paraphrases. The\nbeneﬁt of additional examples decreases when the\ninformation need is clear to the model; this is the\ncase for unambiguous prompts or when enough\n(around 10) demonstrations are provided. Even in\nthe extreme case of ambiguity, for example, when\nthe arrow operator ([s] =>[o]) is used to indicate\na relation, providing only a handful of examples\nclariﬁes the relation sufﬁciently in many cases. We\nshowed that the usefulness of providing additional\ndemonstrations quickly vanishes. Hence, when\nhaving access to more labeled data and the option\nto re-train the model, a ﬁne-tuning strategy is still\nbetter suited to maximize the performance on a\ngiven task. Moreover, casting NLP problems as\nlanguage modeling tasks only works as long as the\ntarget is a single-token word of the LM’s vocabu-\nlary. While technically large generation-based LMs\nas GPT (Brown et al., 2020; Radford et al., 2018)\nor T5 (Raffel et al., 2019) can generate longer se-\nquences, it is not clear how to compare solutions\nof varying length.\n7 Conclusion\nIn this work, we explored the effect of providing\nexamples to probing LMs relational knowledge.\nWe showed that already a few demonstrations—\nsupplied in the context of the LM—disambiguate\nthe query to the same extent as using an optimized\nensemble of multiple paraphrases. We base our\nﬁndings on experimental results of the LAMA\nprobe, the BATS word analogy test, and a TBG\ncommonsense evaluation. On the T-REx corpus’\nfactual relations, providing 10 demonstrations im-\nproves BERT’s P@1 performance by 37.8%. Simi-\nlarly, on ConceptNet’s commonsense relations, Al-\nbert’s performance improves by 25% with access to\n10 examples. We conclude that providing demon-\nstrations is a simple yet effective strategy to clarify\nambiguous prompts to a language model.\nReferences\nAshutosh Adhikari, Xingdi Yuan, Marc-Alexandre\nCôté, Mikuláš Zelinka, Marc-Antoine Rondeau, Ro-\nmain Laroche, Pascal Poupart, Jian Tang, Adam\nTrischler, and William L. Hamilton. 2021. Learning\ndynamic belief graphs to generalize on text-based\ngames.\nLeonard Adolphs and Thomas Hofmann. 2019.\nLedeepchef: Deep reinforcement learning\nagent for families of text-based games. CoRR,\nabs/1909.01646.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners.\nMarc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben\nKybartas, Tavian Barnes, Emery Fine, James Moore,\nMatthew J. Hausknecht, Layla El Asri, Mahmoud\nAdada, Wendy Tay, and Adam Trischler. 2018.\nTextworld: A learning environment for text-based\ngames. CoRR, abs/1806.11532.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning.\nHady Elsahar, Pavlos V ougiouklis, Arslen Remaci,\nChristophe Gravier, Jonathon Hare, Frederique\nLaforest, and Elena Simperl. 2018. T-REx: A large\nscale alignment of natural language with knowledge\nbase triples. In Proceedings of the Eleventh Interna-\ntional Conference on Language Resources and Eval-\nuation (LREC 2018) , Miyazaki, Japan. European\nLanguage Resources Association (ELRA).\nTianyu Gao, Adam Fisch, and Danqi Chen. 2020.\nMaking pre-trained language models better few-shot\nlearners.\nAnna Gladkova, Aleksandr Drozd, and Satoshi Mat-\nsuoka. 2016. Analogy-based detection of morpho-\nlogical and semantic relations with word embed-\ndings: What works and what doesn’t. In Proceed-\nings of the NAACL-HLT SRW , pages 47–54, San\nDiego, California, June 12-17, 2016. ACL.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2020. Deberta: Decoding-enhanced\nbert with disentangled attention.\nInfocom. 1980. Zork i.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know?\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learn-\ning of language representations.\nBill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xi-\nang Ren. 2020. Birds have four legs?! numersense:\nProbing numerical commonsense knowledge of pre-\ntrained language models.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013a. Efﬁcient estimation of word represen-\ntations in vector space.\nTomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.\n2013b. Linguistic regularities in continuous space\nword representations. In Proceedings of the 2013\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies , pages 746–751, Atlanta,\nGeorgia. Association for Computational Linguistics.\nKeerthiram Murugesan, Mattia Atzeni, Pavan Kapani-\npathi, Pushkar Shukla, Sadhana Kumaravel, Gerald\nTesauro, Kartik Talamadupula, Mrinmaya Sachan,\nand Murray Campbell. 2020. Text-based rl agents\nwith commonsense knowledge: New challenges, en-\nvironments and baselines.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP) , pages 1532–1543, Doha,\nQatar. Association for Computational Linguistics.\nFabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim\nRocktäschel, Yuxiang Wu, Alexander H. Miller, and\nSebastian Riedel. 2020. How context affects lan-\nguage models’ factual predictions.\nFabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton\nBakhtin, Yuxiang Wu, Alexander H. Miller, and Se-\nbastian Riedel. 2019. Language models as knowl-\nedge bases?\nNina Poerner, Ulli Waltinger, and Hinrich Schütze.\n2020. E-bert: Efﬁcient-yet-effective entity embed-\ndings for bert.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2018. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. CoRR, abs/1910.10683.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev,\nand Percy Liang. 2016. Squad: 100, 000+ ques-\ntions for machine comprehension of text. CoRR,\nabs/1606.05250.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow Much Knowledge Can You Pack Into the Pa-\nrameters of a Language Model? arXiv e-prints, page\narXiv:2002.08910.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in bertology: What we know about\nhow bert works.\nTimo Schick and Hinrich Schütze. 2020a. Exploiting\ncloze questions for few shot text classiﬁcation and\nnatural language inference.\nTimo Schick and Hinrich Schütze. 2020b. It’s not just\nsize that matters: Small language models are also\nfew-shot learners.\nWang Shugen. 2002. Framework of pattern recogni-\ntion model based on the cognitive psychology. Geo-\nspatial Information Science, 5(2):74–78.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2018.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge.\nLaurens van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of Machine\nLearning Research, 9(86):2579–2605.\nEric Wallace, Yizhong Wang, Sujian Li, Sameer Singh,\nand Matt Gardner. 2019. Do nlp models know num-\nbers? probing numeracy in embeddings.\nWei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao,\nJiangnan Xia, Liwei Peng, and Luo Si. 2019. Struct-\nbert: Incorporating language structures into pre-\ntraining for deep language understanding.\nYaqing Wang, Quanming Yao, James Kwok, and Li-\nonel M. Ni. 2020. Generalizing from a few exam-\nples: A survey on few-shot learning.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. CoRR, abs/1910.03771.\nZhuosheng Zhang, Junjie Yang, and Hai Zhao. 2020.\nRetrospective reader for machine reading compre-\nhension.\nA Appendices\nA.1 Implementation Details\nThe source code to reproduce all the experiments is available at https://github.com/leox1v/\nlmkb_public. All individual runs reported in the paper can be carried out on a single GPU (TESLA\nP100 16GB), though speedups can be realized when using multiple GPUs in parallel. The wall-clock\nruntime for the corpora of the LAMA probe is shown in Table 5.\nAll models used in this work are accessed from the Huggingface’s list of pre-trained models for PyTorch\n(Wolf et al., 2019). Further details about these models are provided on the following webpage: https:\n//huggingface.co/transformers/pretrained_models.html.\nCorpus Model # Parameters Avg. Input Length Runtime [s]\nGoogle-RE\nbert-base-cased\n109M\n5.5 12.8\nbert-base-cased10 60.3 36.1\nbert-base-cased10\nce 60.1 39.6\nbert-large-cased\n335M\n5.5 20.5\nbert-large-cased10 60.3 85.5\nbert-large-cased10\nce 60.1 99.7\nalbert-xxlarge-v2\n223M\n5.5 85.4\nalbert-xxlarge-v210 60.3 466.0\nalbert-xxlarge-v210\nce 60.1 544.9\nT-REx\nbert-base-cased\n109M\n7.6 72.6\nbert-base-cased10 83.2 239.0\nbert-base-cased10\nce 82.7 234.1\nbert-large-cased\n335M\n7.6 119.3\nbert-large-cased10 83.2 747.5\nbert-large-cased10\nce 82.7 596.5\nalbert-xxlarge-v2\n223M\n7.6 504.1\nalbert-xxlarge-v210 83.2 3227.4\nalbert-xxlarge-v210\nce 82.7 3340.9\nConceptNet\nbert-base-cased\n109M\n9.4 38.5\nbert-base-cased10 102.8 121.9\nbert-base-cased10\nce 104.5 124.6\nbert-large-cased\n335M\n9.4 80.4\nbert-large-cased10 102.8 311.4\nbert-large-cased10\nce 104.5 324.3\nalbert-xxlarge-v2\n223M\n9.4 408.0\nalbert-xxlarge-v210 102.8 1760.8\nalbert-xxlarge-v210\nce 104.5 1853.6\nTable 5: The runtime in seconds to go once through the full data from the LAMA probe on a single TESLA P100\nGPU with a batch size of 32. The superscript of the model represents the number of examples used for querying\nand the subscript of ce indicates that close examples are used.\nA.2 The Choice of Template\nWhen providing examples, we give the model the chance to understand the relationship for which we\nquery without providing additional instructions. This naturally raises the question of whether or not\nnatural language templates are even necessary to query LMs. Most prominently, the in-context learning\n0 5 10 15 20\n# Examples\n0.0\n0.1\n0.2\n0.3\n0.4p@1\nTREx BERT-large\nNL Template\n([s]; [o])\n[s] -> [o]\n[s] => [o]\nFigure 6: P@1 score for BERT-large on TREx over the number of examples provided. Each line corresponds to\none template determining how the examples are provided: (i) with the natural language templates from the LAMA\nprobe (NL Template), (ii) separated by a semicolon (([s]; [o])), (iii) separated by a one-lined arrow ([s] - >[o]),\nor (iv) separated by a double-lined arrow ([s] = > [o]). The dashed line shows the baseline value for when no\nadditional example is given.\nof Brown et al. (2020) shows that large LMs can complete patterns even when not provided in natural\nlanguage. In particular, they use the \"= >\"-operator to express the relation between input and output.\nIn Figure 6, we compare the natural language cloze-style template against three different non-language\ntemplates: (i) [s] =>[o], (ii) [s] ->[o], (iii) ([s]; [o]). Surprisingly, Brown et al. (2020)’s \"=>\"-operator\nperforms the worst for BERT-large on T-TREx, while separating the subject and objects by a semicolon\nworks best—almost on par with the performance of the natural language template after providing just a\nsingle example. This result underlines BERT’s remarkable pattern-matching capabilities and suggests that\na natural language description of the relation is not always needed—even when querying relatively small\nLMs.\nA.3 Details TextWorld Commonsense Evaluation\nText-based games (TBG) are computer games where the sole modality of interaction is text. Classic\ngames like Zork (Infocom, 1980) used to be played by a large fan base worldwide. Today, they provide\ninteresting challenges for the research ﬁeld of interactive NLP. With the TextWorld framework by Côté\net al. (2018), it is possible to design custom TBGs; allowing to adapt the objects, locations, and goals\naround the investigated research objectives. TBGs of this framework can vary from treasure hunting\n(Côté et al., 2018) to cooking recipes (Adhikari et al., 2021; Adolphs and Hofmann, 2019), or–as in the\nexperiment at hand–tidying up a room (Murugesan et al., 2020). Murugesan et al. (2020) designed the\nTextWorld Commonsense environmentTWC around the task of cleaning up a modern house environment\nto probe an agent about its commonsense abilities. For example, a successful agent should understand\nthat dirty dishes belong in the dishwasher while clean dishes in the cupboard. Murugesan et al. (2020)\napproach this problem by developing an agent that, through a graph-based network, has access to relevant\nfacts from the ConceptNet (Speer et al., 2018) commonsense knowledge base. Here, the obvious downside\nof static KBs for commonsense knowledge extraction becomes apparent: it does not generalize to not\nlisted object-location pairs. Hence, slight deviations of typical entities require additional processing to be\nable to query the KB. A large pre-trained LM seems to be better suited for this task due to its querying\nﬂexibility and generalization capabilities. We test these abilities by designing a static agent as described\nin the following Algorithm 1, that has access to a large pre-trained LM.\nAlgorithm 1:LM-prior Agent\nInput: TWC game G, pre-trained language model LM\nos ←objects in the scene\nls ←locations in the scene\no←large list of all possible objects across all games\nFunction GetPrior(os,ls,o, LM):\n/* Function to determine a probability distribution over the\nlocations ls for each object in os using the language model\nLM. */\np←empty array of size |os|×|ls|\nforall object oi ∈os do\nd←Randomly sample demonstrations for objects ∈o\\os with locations ∈ls\n/* Use demonstrations d to build context for LM, e.g.: */\n/* milk ⇒ fridge */\n/* dirty dishes ⇒ sink */\n/* oi ⇒ [MASK] */\nc←build_context(d)\n/* Compute MASK-token probabilities for the locations in ls\nusing LM */\npoi ←LM(c,ls)\np.append(poi )\nend\nreturn p\nprior ←GetPrior(os,ls,o, LM)\nwhile G not ﬁnished & max steps not exhausted do\nif agent holds an object oi then\nli ←sample location according to prior[oi]\nif li correct location for oi then\nremove oi from os\nelse\nprior[oi] ←0\nend\nelse\noi ←random_choice(os)\nend\nend\nA.4 Omitted Figures\nGoogle-RE T-REx ConceptNet\nRandom\n0 5 10 15 20\n# Examples\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10P@1\nGoogle_RE\nbert-large\nbert-base\nalbert-xxlarge\n0 5 10 15 20\n# Examples\n0.0\n0.1\n0.2\n0.3\n0.4P@1\nTREx\nbert-large\nbert-base\nalbert-xxlarge\n0 5 10 15 20\n# Examples\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25P@1\nConceptNet\nbert-large\nbert-base\nalbert-xxlargeClose\n0 5 10 15 20\n# Examples\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10P@1\nGoogle_RE\nbert-large\nbert-base\nalbert-xxlarge\n0 5 10 15 20\n# Examples\n0.0\n0.1\n0.2\n0.3\n0.4P@1\nTREx\nbert-large\nbert-base\nalbert-xxlarge\n0 5 10 15 20\n# Examples\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25P@1\nConceptNet\nbert-large\nbert-base\nalbert-xxlarge\nTable 6: P@1 score for the different corpora of the LAMA probe over the number of examples provided. The\ndashed line shows the baseline values for when no additional example is given. The upper row depicts the scores\nfor when the examples are chosen randomly among the same relation, while the lower row only considers examples\nfrom close subjects as deﬁned in Section 4.\nGoogle-RE T-REx ConceptNet\nRandom\n0 5 10 15 20\n# Examples\n0.00\n0.05\n0.10\n0.15MRR\nGoogle_RE\nbert-large\nbert-base\nalbert-xxlarge\n0 5 10 15 20\n# Examples\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5MRR\nTREx\nbert-large\nbert-base\nalbert-xxlarge\n0 5 10 15 20\n# Examples\n0.0\n0.1\n0.2\n0.3MRR\nConceptNet\nbert-large\nbert-base\nalbert-xxlargeClose\n0 5 10 15 20\n# Examples\n0.00\n0.05\n0.10\n0.15MRR\nGoogle_RE\nbert-large\nbert-base\nalbert-xxlarge\n0 5 10 15 20\n# Examples\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5MRR\nTREx\nbert-large\nbert-base\nalbert-xxlarge\n0 5 10 15 20\n# Examples\n0.0\n0.1\n0.2\n0.3MRR\nConceptNet\nbert-large\nbert-base\nalbert-xxlarge\nTable 7: Mean reciprocal rank (MRR) score for the different corpora of the LAMA probe over the number of\nexamples provided. The dashed line shows the baseline values for when no additional example is given. The upper\nrow depicts the scores for when the examples are chosen randomly among the same relation, while the lower row\nonly considers examples from close subjects as deﬁned in Section 4.\nGoogle-RE T-REx ConceptNet\nRandom\n0 5 10 15 20\n# Examples\n0.00\n0.01\n0.02\n0.03\n0.04Avg. Probability\nGoogle_RE\nbert-large\nbert-base\nalbert-xxlarge\n0 5 10 15 20\n# Examples\n0.0\n0.1\n0.2\n0.3Avg. Probability\nTREx\nbert-large\nbert-base\nalbert-xxlarge\n0 5 10 15 20\n# Examples\n0.00\n0.05\n0.10\n0.15Avg. Probability\nConceptNet\nbert-large\nbert-base\nalbert-xxlargeClose\n0 5 10 15 20\n# Examples\n0.00\n0.01\n0.02\n0.03\n0.04Avg. Probability\nGoogle_RE\nbert-large\nbert-base\nalbert-xxlarge\n0 5 10 15 20\n# Examples\n0.0\n0.1\n0.2\n0.3Avg. Probability\nTREx\nbert-large\nbert-base\nalbert-xxlarge\n0 5 10 15 20\n# Examples\n0.00\n0.05\n0.10\n0.15Avg. Probability\nConceptNet\nbert-large\nbert-base\nalbert-xxlarge\nTable 8: Probability assigned to the ground-truth object for the different corpora of the LAMA probe over the\nnumber of examples provided. The dashed line shows the baseline values for when no additional example is given.\nThe upper row depicts the scores for when the examples are chosen randomly among the same relation, while the\nlower row only considers examples from close subjects as deﬁned in Section 4.\nI01 [noun - plural_reg]\nI02 [noun - plural_irreg]\nI03 [adj - comparative]\nI04 [adj - superlative]\nI05 [verb_inf - 3pSg]\nI06 [verb_inf - Ving]\nI07 [verb_inf - Ved]\nI08 [verb_Ving - 3pSg]\nI09 [verb_Ving - Ved]\nI10 [verb_3pSg - Ved]\nD01 [noun+less_reg]\nD02 [un+adj_reg]\nD03 [adj+ly_reg]\nD04 [over+adj_reg]\nD05 [adj+ness_reg]\nD06 [re+verb_reg]\nD07 [verb+able_reg]\nD08 [verb+er_irreg]\nD09 [verb+tion_irreg]\nD10 [verb+ment_irreg]\nL01 [hypernyms - animals]\nL02 [hypernyms - misc]\nL03 [hyponyms - misc]\nL04 [meronyms - substance]\nL05 [meronyms - member]\nL06 [meronyms - part]\nL07 [synonyms - intensity]\nL08 [synonyms - exact]\nL09 [antonyms - gradable]\nL10 [antonyms - binary]\nE01 [country - capital]\nE02 [country - language]\nE03 [UK_city - county]\nE04 [name - nationality]\nE05 [name - occupation]\nE06 [animal - young]\nE07 [animal - sound]\nE08 [animal - shelter]\nE09 [things - color]\nE10 [male - female]\nCategory\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0P@1\nBATS Albert (10 examples)\nalbert-xxlarge\nGloVe\nFigure 7: P@1 score on BATS for Albert-xxlarge with 10 examples that use the \"([s]; [o])\"-template. The x-axis\nbreaks down the performance for the individual relations of the BATS dataset. As a benchmark, we use the GloVe\nmodel from Gladkova et al. (2016). The frame around the bar indicates the maximum possible score that the Albert\nmodel could have scored because not all targets are tokens in its vocabulary.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8262401819229126
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5839607119560242
    },
    {
      "name": "Flexibility (engineering)",
      "score": 0.537672758102417
    },
    {
      "name": "Relation (database)",
      "score": 0.5276205539703369
    },
    {
      "name": "Blank",
      "score": 0.5148924589157104
    },
    {
      "name": "Commonsense knowledge",
      "score": 0.5065881013870239
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5002744197845459
    },
    {
      "name": "Language model",
      "score": 0.46980175375938416
    },
    {
      "name": "Natural language processing",
      "score": 0.46633458137512207
    },
    {
      "name": "Baseline (sea)",
      "score": 0.46038272976875305
    },
    {
      "name": "Domain knowledge",
      "score": 0.15915387868881226
    },
    {
      "name": "Database",
      "score": 0.08723026514053345
    },
    {
      "name": "Mathematics",
      "score": 0.06952771544456482
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I35440088",
      "name": "ETH Zurich",
      "country": "CH"
    }
  ],
  "cited_by": 10
}