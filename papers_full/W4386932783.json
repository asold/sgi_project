{
  "title": "Developing prompts from large language model for extracting clinical information from pathology and ultrasound reports in breast cancer",
  "url": "https://openalex.org/W4386932783",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2556572152",
      "name": "Hyeon Seok Choi",
      "affiliations": [
        "Seoul National University Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A4316005454",
      "name": "Jun Yeong Song",
      "affiliations": [
        "Seoul National University Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2120801033",
      "name": "Kyung Hwan Shin",
      "affiliations": [
        "Seoul National University Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2619552951",
      "name": "Ji Hyun Chang",
      "affiliations": [
        "Seoul National University Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2890764760",
      "name": "Bum‐Sup Jang",
      "affiliations": [
        "Seoul National University Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2556572152",
      "name": "Hyeon Seok Choi",
      "affiliations": [
        "Seoul National University Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A4316005454",
      "name": "Jun Yeong Song",
      "affiliations": [
        "Seoul National University Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2120801033",
      "name": "Kyung Hwan Shin",
      "affiliations": [
        "Seoul National University Hospital",
        "Seoul National University"
      ]
    },
    {
      "id": "https://openalex.org/A2619552951",
      "name": "Ji Hyun Chang",
      "affiliations": [
        "Seoul National University Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2890764760",
      "name": "Bum‐Sup Jang",
      "affiliations": [
        "Seoul National University Hospital"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2534719059",
    "https://openalex.org/W4283317429",
    "https://openalex.org/W2236920519",
    "https://openalex.org/W2095343092",
    "https://openalex.org/W3127145819",
    "https://openalex.org/W3095319910",
    "https://openalex.org/W4361289889",
    "https://openalex.org/W4229446528",
    "https://openalex.org/W3153853714",
    "https://openalex.org/W4376871976",
    "https://openalex.org/W2997514453",
    "https://openalex.org/W4319662928"
  ],
  "abstract": "Purpose: We aimed to evaluate the time and cost of developing prompts using large language model (LLM), tailored to extract clinical factors in breast cancer patients and their accuracy.Materials and Methods: We collected data from reports of surgical pathology and ultrasound from breast cancer patients who underwent radiotherapy from 2020 to 2022. We extracted the information using the Generative Pre-trained Transformer (GPT) for Sheets and Docs extension plugin and termed this the “LLM” method. The time and cost of developing the prompts with LLM methods were assessed and compared with those spent on collecting information with “full manual” and “LLM-assisted manual” methods. To assess accuracy, 340 patients were randomly selected, and the extracted information by LLM method were compared with those collected by “full manual” method.Results: Data from 2,931 patients were collected. We developed 12 prompts for Extract function and 12 for Format function to extract and standardize the information. The overall accuracy was 87.7%. For lymphovascular invasion, it was 98.2%. Developing and processing the prompts took 3.5 hours and 15 minutes, respectively. Utilizing the ChatGPT application programming interface cost US $65.8 and when factoring in the estimated wage, the total cost was US $95.4. In an estimated comparison, “LLM-assisted manual” and “LLM” methods were time- and cost-efficient compared to the “full manual” method.Conclusion: Developing and facilitating prompts for LLM to derive clinical factors was efficient to extract crucial information from huge medical records. This study demonstrated the potential of the application of natural language processing using LLM model in breast cancer patients. Prompts from the current study can be re-used for other research to collect clinical information.",
  "full_text": "209www.e-roj.org\nCopyright © 2023 The Korean Society for Radiation Oncology\nThis is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/) \nwhich permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.\nPurpose: We aimed to evaluate the time and cost of developing prompts using large language model \n(LLM), tailored to extract clinical factors in breast cancer patients and their accuracy. \nMaterials and Methods: We collected data from reports of surgical pathology and ultrasound from \nbreast cancer patients who underwent radiotherapy from 2020 to 2022. We extracted the informa-\ntion using the Generative Pre-trained Transformer (GPT) for Sheets and Docs extension plugin and \ntermed this the “LLM” method. The time and cost of developing the prompts with LLM methods were \nassessed and compared with those spent on collecting information with “full manual” and “LLM-as-\nsisted manual” methods. To assess accuracy, 340 patients were randomly selected, and the extracted \ninformation by LLM method were compared with those collected by “full manual” method. \nResults: Data from 2,931 patients were collected. We developed 12 prompts for Extract function and \n12 for Format function to extract and standardize the information. The overall accuracy was 87.7%. \nFor lymphovascular invasion, it was 98.2%. Developing and processing the prompts took 3.5 hours \nand 15 minutes, respectively. Utilizing the ChatGPT application programming interface cost US $65.8 \nand when factoring in the estimated wage, the total cost was US $95.4. In an estimated comparison, \n“LLM-assisted manual” and “LLM” methods were time- and cost-efficient compared to the “full man-\nual” method. \nConclusion: Developing and facilitating prompts for LLM to derive clinical factors was efficient to ex-\ntract crucial information from huge medical records. This study demonstrated the potential of the \napplication of natural language processing using LLM model in breast cancer patients. Prompts from \nthe current study can be re-used for other research to collect clinical information. \nKeywords: Automatic data processing, Artificial intelligence, Natural language processing, Breast \ncancer, Clinical reports  \nDeveloping prompts from large language model for extracting \nclinical information from pathology and ultrasound reports in \nbreast cancer \nHyeon Seok Choi\n1\n, Jun Yeong Song\n1\n, Kyung Hwan Shin\n1,2\n, Ji Hyun Chang\n1\n, Bum-Sup Jang\n1\n  \n1\nDepartment of Radiation Oncology, Seoul National University Hospital, Seoul National University College of Medicine, Seoul, Korea \n2\nInstitute of Radiation Medicine, Seoul National University Medical Research Center, Seoul, Korea \nReceived: July 24, 2023 \nRevised: September 11, 2023 \nAccepted: September 11, 2023 \nCorrespondence:\nBum-Sup Jang \nDepartment of Radiation Oncology, \nSeoul National University Hospital, \n101 Daehak-ro, Jongno-gu, Seoul \n03080, Korea. \nTel: +82-2-2072-1161 \nE-mail: bigwiz83@gmail.com  \nORCID: \nhttps://orcid.org/0000-0002-7064-9855\nOriginal Article\npISSN 2234-1900 · eISSN 2234-3156\nRadiat Oncol J 2023;41(3):209-216\nhttps://doi.org/10.3857/roj.2023.00633\nIntroduction \nIn radiation therapy for breast cancer patients, numerous clinical \nfactors are considered. For instance, when the National Compre-\nhensive Cancer Network panels [1] recommend comprehensive re-\ngional nodal irradiation for pN1 patients, it also recommends con-\nsidering clinical factors such as whether the primary tumor is small \nor there is only one metastasis. Indeed, clinical decision to irradiate \nfull regional lymph nodes or not depend on clinical factors includ-\ning age, nuclear grade, molecular subtype, resection margin status, \nlymphovascular invasion and extranodal extension [2–5] as well as \nTNM stage. Thus, pathologic factors and radiologic findings are \ncarefully reviewed to make an optimal decision in clinical practice. \nRadiation oncologists manually reviewed the medical record of \n\nhttps://doi.org/10.3857/roj.2023.00633210\nHyeon Seok Choi, et al.\neach patient, seeking important factors to support their decision. \nWhen collecting clinical information for research work, medical re-\nports are thoroughly reviewed, factors are manually classified, and \nthe case report form is constructed. This process is labor-intensive, \ncostly, and time-consuming. However, automation of these pro-\ncesses has been challenging due to difficulties in processing un-\nstructured, narrative-style reports generated from diagnostic work-\nup for breast cancer patients. \nRecent advances in natural language processing (NLP) play a \npivotal role in solving complex problems like the challenges men-\ntioned above. NLP makes it possible to reliably extract important \ninformation from free text, and many fields of medicine, including \nradiation oncology, could benefit from these techniques [6]. In the \nfield of NLP, state-of-the-art large language models (LLMs) like \nGPT-4 (Generative Pre-trained Transformer 4) have shown out -\nstanding abilities in understanding and generating human lan -\nguage [7]. This enables LLMs to comprehend textual data and es-\ntablish contextual connections, leading to revolutionary achieve-\nments. Further, this capability allows LLMs to analyze complex \nmedical data, extract crucial information, and support deci -\nsion-making. As such, the effective utilization of LLMs offers tre-\nmendous potential in the automation of traditional medical chart \nreview. \nTo use LLM effectively, “prompts” must be properly developed. \nPrompts are input sentences or phrases for LLM to perform a spe-\ncific action. The content and structure of these prompts greatly in-\nfluence the output and performance of LLMs [8]. Depending on the \nprompt, LLM model determine which information to process and \nwhat type of answers to generate. Thus, design of proper prompts \nis key to maximizing the capabilities of LLMs, given that inappro-\npriate prompts can cause models to misunderstand or produce un-\nexpected results. In particular, in the field of radiation oncology, re-\nsearch on prompt engineering is required for LLM to extract accu-\nrate information from unstructured medical reports. \nIn this study, prompts were developed to extract the required in-\nformation using LLM from surgical pathology reports and preoper-\native ultrasound reports of breast cancer patients. The time and \ncost needed to develop the prompt was assessed and compared \nwith manual methods. Also, the accuracy of the extracted informa-\ntion was evaluated by comparing it with information collected \nmanually. \nMaterials and Methods \nWe collected data from breast cancer patients who received post-\noperative radiotherapy (RT) from 2020 to 2022 in our institution. \nMale breast cancer patients, patients who received palliative RT, \nand patients who did not undergo surgery at our institution were \nexcluded. For the study population, the findings from the earliest \nbreast ultrasound within one year before the surgery and the report \nfrom surgical pathology were collected. The overall study schema is \ndepicted in Fig. 1. \nAs the target LLM, we selected the ChatGPT and accessed it with \nan extension program of Google Sheets, named as GPT for Sheets \nand Docs (https://gptforwork.com). This plugin, developed by Talar-\nian, is an application that allows the GPT model to be used directly \nin Google Sheets and provides additional custom features. This \nmakes using various LLM models such as GPT3 and ChatGPT mod-\nels in Google Sheets possible. In this study, we used the ChatGPT \n(gpt-3.5-turbo) model. \nFrom the ultrasound reports, prompts were designed to extract \nand organize factors related to the clinical stage. To determine the \nclinical T stage, we designed prompts to extract the size and loca-\ntion of the largest suspected cancerous mass within the breast. To \nextract information about the clinical N stage, we designed \nprompts to extract the number of suspected metastatic lymph \nnodes for each nodal area. In addition, information about laterality \nand tumor location was also extracted. \nIn the surgical pathology report, prompts were designed to ex-\ntract factors like tumor size and number of metastatic lymph \nnodes, which determines the pathological stage. We also designed \nprompts to extract clinical factors such as histologic grade, neoad-\njuvant chemotherapy status, resection margin status, molecular \nsubtype, lymphovascular invasion, and extracapsular extension. In \naddition, we designed prompts to extract factors, such as surgery \ntype and metastatic lymph node ratio. \nTo evaluate the efficiency and accuracy of the extraction method \nusing LLM, we compared three methods of collecting clinical infor-\nmation from medical records. First, the “full manual” method was \ndone by JYS, a resident physician in Radiation Oncology. The meth-\nod is a way to manually collect information from the medical re-\ncords of each patient, and to ensure 100% accuracy, the informa-\ntion was verified by another physician after collection. Second, the \n“LLM-assisted manual” method was done by the same resident \nphysician who did the “full manual” method, but with the assis-\ntance of the information already collected using LLM. Lastly, the \n“LLM” method used LLM alone to extract information of clinical \nfactors. To establish comparability throughout the three methods, \n340 patients, were randomly selected using the RAND function in \nMicrosoft Excel to extract information. We calculated the sample \nsize to represent the accuracy of the entire 2,913 cases with a con-\nfidence level of 95% and a margin of error of 5%. \nThe accuracy and time- and cost-efficiency were assessed using \nthe following methods. First, the accuracy of the “LLM” method \n211https://doi.org/10.3857/roj.2023.00633\nDeveloping LLM prompt to extract data\nwas assessed by regarding the information collected by the “full \nmanual” method as the ground truth and comparing the data col-\nlected with both methods. The accuracy rate was calculated by \neach factor. Also, the time spent to collect the information, to de-\nsign prompts, or to trim information collected by LLM was mea-\nsured. Since only 340 patients were included in “full manual” and \n‘LLM-assisted manual’ methods, a factor of 8.57 (2,931/340) was \nmultiplied by the measured time to extrapolate, and establish com-\nparability between the three methods. Furthermore, the cost spent \nto collect the information was estimated. In the “full manual” and \n“LLM-assisted manual” methods, the cost was estimated by multi-\nplying the measured time by US $7.4 per hour, which is South Ko-\nrea’s minimum wage in 2023. In the “LLM-assisted manual” and \n“LLM” methods, the estimated cost for prompt design and the GPT \napplication programming interface (API) fee was measured. Since \nthe “LLM” method was performed on entire patients, the cost for \n340 patients was calculated by dividing the total GPT API fee by \n8.57. \nResults \nIn total, 2,931 breast cancer patients treated at our institution \nwere included in this study. Table 1 shows the factors, function \ntypes, prompts designed, and their corresponding results in this \nstudy. A representative example of a surgical pathology report and \nan ultrasound report could be found in Supplementary Figs. S1 and \nS2. \nPrompts were developed using the Extract function and the For-\nmat function from GPT for Sheets and Docs. The Extract function \nextracts the required information from the reports. The Format \nfunction was used to convert them into a structured format. For \nfuture statistical analysis, it is essential to standardize the format \nof responses. Therefore, the choice or type of response was prede-\ntermined through the prompts. When the required information \ncould not be extracted at once, the extraction was performed in \ntwo steps. For example, when the information regarding the largest \nnodule among the nodes with suspected malignancy is needed, the \ninformation of the node that corresponds to the condition is first \nextracted, and then the information on diameter, laterality, and \nclockwise location is extracted again from that extracted informa-\ntion.  \nIn some cases, simple computations to trim the extracted infor-\nmation were necessary. These were performed using IF, AND, and \nOR functions in Microsoft Excel. For instance, functions were used \nto determine the breast cancer subtype based on immunohisto -\nchemistry results or to identify whether the breast cancer was lo-\ncated on the inner or outer side based on the laterality and clock-\nwise location. When additional trimming was needed for statistical \nTotal n = 2,931\nReport of breast ultrasound\nThe largest nodule information\n(size, laterality, clock-wise position) \nMetastatic lymph node information \n(location, number)\nPathologic T stage\nPathologic N stage \nHistologic grade \nSurgery type \nSubtype by IHC result\nReport of pathology after\ndefinitive surgery\nTrimming with \nregular expression\nExcel function\nChatGPT 3.5 with \nGoogle spreadsheet \nExtension\nTabular\nForm\nManual validation on random \nsamples (n = 340)\nFig. 1. The schema of the current study. Using ChatGPT 3.5 model, information about clinical T and N stage was extracted from ultrasound \nreadings, and pathologic T and N stage and additional factors were extracted from pathology readings. Then, trimming was performed and or-\nganized in tabular form. For validation, a sample was randomly selected to evaluate the accuracy. GPT, Generative Pre-trained Transformer; IHC, \nimmunohistochemistry.\nhttps://doi.org/10.3857/roj.2023.00633212\nHyeon Seok Choi, et al.\nTable 1. The factors, function types, prompts designed, and their corresponding results \nFactors Function type Prompt Result\nUltrasound reportsLargest nodule in-\nformation\nExtract Information about largest nodule in diameter or mass with BI-\nRADS classification of C4 or higher. if there is no C4 or higher \nnodule, just say 'no cancer'. if not 'no cancer', answer form is \n'longest diameter (e.g., 1.0 cm, 2.5 cm by cm), laterality (e.g., \nRt/Lt), orientation (by clockface, e.g., 1H, 11.5H) or by quad-\nrant (e.g., SA, center, UO, IL), BI-RADS classification'. all an-\nswer is in a single line.\n2.3, Lt, 2H, C5/6\nSize Format Longest one direction diameter. say just number without unit, \nnot other information. If the unit is mm, change it to cm.\n2.3\nLaterality Format Laterality by Lt or Rt. only answer Lt or Rt Lt\nClockwise orienta-\ntion\nFormat Orientation by clockface, like 1H, 2H, 10.5H 2\nMetastatic lymph \nnode\nExtract This is sono reading. from now on, you are radiologist, count \nnumber of suspicion or enlarged lymph nodes. do not count \nsuspicion breast nodule. do things step by step.\nInner : N/A / axillary : 5 / \nIMN : N/A / SCL : N/A\nAnswer form 'inner: ## / axillary : ## / IMN : ## / SCL : ##' \nanswer in single line. 'inner' means intramammary lymph \nnodes. 'IMN' means internal mammary lymph nodes. 'SCL' \nmean supra clavicular lymph nodes.\nPathology reports Pathological T stage Extract The invasive tumor size 또는 종괴의 크기 as a long diameter 또는 \nthe extent of in situ.  \n1.5 ×  0.4 ×  2.0 cm (inva-\nsive tumor size)\nFormat T stage. You are breast cancer pathologist interpreting reports \nwith AJCC 8th staging system\nT1c\nHistologic grade Extract The histologic grade written in pathologic reports II/III\nFormat Answer just number according to histologic grade. You are \nbreast cancer pathologist\n2\nSurgery type Extract Surgery type Breast conserving surgery\nPathological N stage Extract The number of positive or metastatic lymph nodes out of total \ndissected lymph nodes. If nodes were not dissected or not \nsubmitted, just say 'not submitted'\nNumber of metastatic \nlymph nodes: 1 out of 11 \nexamined lymph nodes.\nFormat N stage. You are breast cancer pathologist interpreting reports \nwith AJCC 8th staging system. If not submitted, just say 'Nx'\nN1\nMetastatic lymph \nnode ratio\nFormat Just say only the number after calculating the ratio of the \nnumber of positive or metastatic lymph nodes by the total \nnumber of examined or dissected lymph nodes\n0.09\nLymph node sam-\npling type\nFormat Say SLNB if all examined or dissected lymph nodes were senti-\nnel nodes. If not, say ALND\nALND\nNumber of meta-\nstatic lymph nodes\nExtract Just say the number of metastatic or involved lymph nodes, \nnot total harvested or dissected lymph nodes\n1\nNeoadjuvant che-\nmotherapy\nExtract Just say 'Yes' if this pathologic report described post neo-adju-\nvant chemotherapy or 'yp'stages according to AJCC staging \nsystem. If not, just say 'No'.\nYes (post-neoadjuvant \nchemotherapy status)\nResection margin Extract Evaluate the margin status, reviewing this pathologic report. \nAnswer in one of 3 words: 'Clear', 'Close', or 'Positive'. You are \nbreast cancer pathologist.\nClear\nIHC result Extract Concatenate the immunohistochemistry results including es-\ntrogen, progesterone, Ki-67, and gene amplification results in \nsingle line. If there isn't, just say 'N/A'. Don't make multiple \nlines in answer.\nEstrogen Receptor alpha, \npositive in 90%; proges-\nterone receptor, positive \nin 40% (S 21-0030586)\nEstrogen Receptor alpha, \npositive in 90%; proges-\nterone receptor, positive \nin 80% (S 21-0030587)\nKi-67, positive in 1%\nFormat Just say 'Positive' if the expression of estrogen receptor is posi-\ntive or more than 1+, based on AJCC breast cancer staging. \nOtherwise, just say 'No'.\nPositive\nFormat Just say 'Positive' if the expression of HER2 is more than 2+ or \nFISH amplification is positive, based on AJCC breast cancer \nstaging. Otherwise, just say 'Negative'.\nNegative\n(Continued to the next page)\n213https://doi.org/10.3857/roj.2023.00633\nDeveloping LLM prompt to extract data\nanalysis, the information was trimmed using regular expressions in \nPython. \nThe accuracy of the information extraction by LLM was calculat-\ned by each factor and is shown in Table 2. Regarding all the factors, \nthe average accuracy was 87.7%, which could be translated to \nroughly 298 out of 340 patients. Among all factors, lymphovascular \ninvasion had the highest accuracy of 98.2%. In contrast, neoadju-\nvant chemotherapy status and tumor location had the lowest ac-\ncuracy of 47.6% (162 out of 340) and 63.8% (217 out of 340), re-\nspectively. \nThe time- and cost-efficiency of the information extraction by \nLLM were also assessed. Regarding the time-efficiency, it took 1.5 \nhours for designing the prompts for the ultrasound report and 2 \nhours for the surgical pathology report. Responses to the prompts \nwere outputted in parallel for each cell in the table via the GPT \nserver. The entire response output process took 15 minutes. Trim-\nming the data took approximately 30 minutes in total, which was \nmostly coding time, and the actual application was completed in a \nfew seconds. In total, approximately 4 hours were needed in ex-\ntracting clinical factors from 2,931 breast cancer patients. In terms \nof cost, using the GPT model through the GPT API incurs a fee per \ntoken. In the current study, US $6.04 for ultrasound interpretation \nand US $59.76 for surgical pathology interpretation was charged \nusing the GPT API, for a total of US $65.8. Also, the whole process \ntook 4 hours to design the prompts and trim the data, which could \nbe translated to a wage cost of US $29.6, when applying the mini-\nmum wage of South Korea in 2023. The response output process \nwas excluded from the wage cost calculation because the process \ncould be done automatically. \nThe time and cost spent on collecting the information using “full \nmanual,” “LLM-assisted manual,” and “LLM” methods were mea-\nsured and estimated for comparison ( Table 3). For all 2,913 pa -\ntients, the time spent for the “full manual,” “LLM-assisted manual,” \nand “LLM” methods were 122.6 hours, 79.4 hours, and 4 hours, re-\nspectively. The estimated cost for “full manual,” “LLM-assisted \nmanual,” and “LLM” methods were US $909.3, $653.4, and $95.4, \nrespectively. By using “LLM-assisted manual” and “LLM” methods \ncompared to the “full manual” method, we could save 43.2 hours \nand US $255.9, and 118.6 hours and US $813.9 in all 2,913 pa -\ntients, respectively. \nDiscussion and Conclusion \nIn the current study, we investigated the efficiency and accuracy of \nthe utilization of LLM in extracting RT-related factors. From 2,931 \nbreast cancer patients, we extracted clinical factors from the re-\nports of ultrasound and surgical pathology. The whole process took \n4 hours and cost US $95.4, and the average accuracy was 87.7%. \nFactors Function type Prompt Result\nFormat Just extract the number how much percentage of Ki-67 ex-\npression is in positive. If Ki-67 was not found, just say 'N/A'.\n0.01\nFormat As shown in immunochemistry result, breast cancer is tri-\nple-negative type, then say 'Yes'. If not, just say 'No'\nNo\nLymphovascular in-\nvasion\nExtract The lymphatic invasion in pathologic report Lymphatic emboli: present, \nminimal\nExtracapsular ex-\ntension\nExtract The presence or absence or N/A of extracapsular extension in \nthe pathology results\nExtracapsular extension: \nN/A\nAJCC, American Joint Committee on Cancer; ALND, axillary lymph node dissection; BI-RADS, Breast Imaging Reporting and Data System; FISH, fluo-\nrescence in situ hybridization; HER2, human epidermal growth factor receptor 2; IL, inferio-lateral; IMN, internal mammary lymph node; N/A, not ac-\ncessible; SA, subareolar; SCL, supraclavicular lymph node; SLNB, sentinel lymph node biopsy; UO, upper-outer.\nTable 1. Continued\nTable 2. The accuracy of the data extraction by LLM \nFactor Correct Total Accuracy (%)\nClinical T stage 279 340 81.9\nClinical N stage 311 340 91.5\nTumor location 217 340 63.8\nSurgery type 318 340 93.4\nNeoadjuvant chemotherapy 162 340 47.6\nPathologic T stage 295 340 86.7\nHistologic grade 334 340 98.1\nLymphovascular invasion 334 340 98.2\nResection margin 298 340 87.7\nLymph node sampling type 295 340 86.7\nPathologic N stage 314 340 92.4\nMetastatic lymph node ratio 324 340 95.3\nExtracapsular extension 305 340 89.6\nEstrogen receptor 324 340 95.3\nHER2 327 340 96.3\nKi-67 311 340 91.5\nTriple-negative breast cancer 324 340 95.3\nOverall (average) 298 340 87.7\nLLM, large language model; HER2, human epidermal growth factor 2.\nhttps://doi.org/10.3857/roj.2023.00633214\nHyeon Seok Choi, et al.\nTable 3. The time and cost spent on collecting the data using “full manual,” “LLM-assisted manual,” and “LLM” methods \nFull manual LLM-assisted manual LLM\n340 patients All patients  \n(extrapolated) 340 patients All patients  \n(extrapolated) 340 patients All patients\nTime spent (hr)\n For data collection 14.3 122.6 8.8 75.4 - -\n For prompt design and trimming - - 4.0 4.0 4.0 4.0\n Total 14.3 122.6 12.8 79.4 4.0 4.0\n Time saved compared to “full manual” - - 1.5 43.2 9.3 118.6\nEstimated cost (US dollar)\n Manual data collection wage cost 106.1 909.3 65.6 558 - -\n Prompt design and trimming wage cost - - 29.6 29.6 29.6 29.6\n GPT API usage fee - - 7.7 65.8 7.7 65.8\n Total 106.1 909.3 102.9 653.4 37.3 95.4\n Cost saved compared to “full manual” - - 3.2 255.9 68.8 813.9\nAPI, application programming interface; LLM, large language model; Bold, total time and cost.\nThe GPT model, which is employed in the current study, is in -\ntended to imitate natural conversations, and does not contain logi-\ncal thinking [9]. Thus, it does not understand the meaning of sen-\ntences but is merely aligning the most likely word to use. In other \nwords, there is no logic in the response it gives. For example, in the \ncurrent study, we first wanted to separate the location of a breast \ntumor into inner and outer based on laterality and the clockwise \ndirection from the nipple. Despite many trials and errors, we failed \nto implement the function in a GPT model. This may be because \nthe GPT model failed to comprehend the logic that the clockwise \ndirection is opposite according to the laterality of the breast in \nseparating inner from outer lesions. Also, another well-known fea-\nture of the GPT model is a phenomenon called the hallucination [10]. \nThis is a phenomenon that the model responds with a plausible an-\nswer that is incorrect. For example, when the GPT-3.5 is asked “Ex-\nplain to me the clinical N stage of breast cancer according to the \nAmerican Joint Committee on Cancer (AJCC) 8th edition,” it comes \nup with a plausible answer which is an explanation about the patho-\nlogical N stage. The hallucination was seen time to time in extracting \ninformation in the current study. For example, when asked about the \ndiameter of the tumor from the ultrasound report, it sometimes re-\nsponded with the distance from the nipple. We speculate that this \nhallucination occurred because the two values are both written in \nnumerical values in centimeter. In another example, the category \n“Neoadjuvant chemotherapy” was accurate at less than 50%. It \ncould be due to hallucination. Even though there was no information \nabout neoadjuvant chemotherapy in the pathology report, LLM gave \na yes or no answer instead of saying that there was insufficient in-\nformation. A deep understanding in such features of LLMs is crucial \nto designing prompts and applying it to use. \nDeveloping an effective prompt to get the desired outcome, \nwhich is called prompt engineering, is the most crucial point in uti-\nlizing LLM. In prompt engineering, the features of LLMs should be \nwell understood by developers. When using an LLM, users may avoid \nusing jargon, instead, should provide all the information needed to \ngenerate a response. Also, it is recommended to write the logic \nleading up to the desired outcome in the prompt, rather than trying \nto achieve the desired outcome all at once, thereby, the LLM can \nfollow the logical process. For example, in interpreting an ultraso-\nnography report, instead of saying, “Identify the clinical T stage,” it \nis better to say, “Here's an ultrasound reading of a breast cancer pa-\ntient, find the mass with the largest diameter and tell me its diame-\nter in centimeters.” Also, these results can be improved with a well-\nknown few-shot learning or fine-tuning method by introducing an \nexample within the prompt and having the LLM replicate the logical \nflow [11]. Since we could not fully predict the output of LLM, it is \nessential to modify and correct the prompt through a trial-and-er-\nror method, rather than completing it at once. Understanding these \nfeatures will provide appropriate answers to users when effective \nprompt engineering is adopted in coding clinical data. \nInformation extraction by developing prompts from LLM is ex-\ntremely efficient in terms of both time and expenses, especially \nwhen it is applied to a task handling huge data. From raw data of \nthousand patients to well-organized spreadsheet data, we could \nsave 118.6 hours and US $813.9 by using “LLM” method. After \ncompleting the prompt design, the cost of developing the prompt \nis fixed, which could save expenses for research that needs to be \nencoded from large data, such as pathological or radiographic find-\nings of patients with breast or prostate cancer treated with radia-\ntion therapy. This method is a much-awaited in labor market like \nSouth Korea, where the hiring of qualified healthcare provider is \nexpensive and scarce. For the last decade in South Korea, there has \n215https://doi.org/10.3857/roj.2023.00633\nDeveloping LLM prompt to extract data\nbeen a steep growth in wages with the minimum wage almost \ndoubling [12], and that of the healthcare providers are no excep-\ntion [13]. In addition, the “Act on the Improvement of Training \nConditions and Status of Medical Residents” was enacted in 2015, \nrestricting the working hours of medical residents [14]. Due to ad-\nvantage of the using LLM in large-scale tasks, development, and \nfacilitating prompts are employed in other expert fields as well [15]. \nThe accuracy of the information extracted by prompts using LLM \nwas 87.7%, which is an encouraging result compared to conven-\ntional NLP models. Juhn et al. [16] reported 80%–98% accuracy in \nidentifying the presence or severity of allergic conditions through \nmedical record review using an NLP-based model [16]. In addition, \nTang et al. [17] reported a low accuracy of 23.4% in biomedical \nnamed entity recognition using the ChatGPT, and a higher accuracy \nof 75.9% in LLM for medical tasks. Although the accuracy of \n87.7% in the current study is notable, it should be cautioned to use \nwithout supervision. Manual examination should be done to re-ex-\namine the extracted information. Alternatively, a hybrid method \nthat we named the “LLM-assisted manual” method could be a rea-\nsonable way to compromise in a real-world setting. By referring to \nthe information extracted, one can collect information more effi-\nciently, while still maintaining the level of accuracy of a manual \ninformation collection. Recently, the GPT-4 was released and ex-\npected to reduce the frequency of hallucinations and improve ac-\ncuracy [7]. We expect that LLMs specialized in medical tasks may \nelevate the accuracy of extraction even better [18,19]. \nThere were several limitations in this study. First, the clinical N \nstage did not strictly follow the AJCC staging system. The informa-\ntion in the ultrasound report was insufficient to extract whether a \nlymph node is movable or not. Thus, distinguishing between clinical \nN1 and N2 solely depended on the number of nodal metastasis or \nthe presence of an internal mammary lymph node metastasis with-\nout axillary metastasis, and has discrepancy with the AJCC staging \nsystem. Also, the “full manual” method, which is used as a ground \ntruth, is not perfect. Although manual process is the control for the \n“LLM” process, the “full manual” method does not guarantee 100% \naccuracy in information collection due to the human error. It could \nbe improved if two or more people could cross-check each other’s \ncollected information to approach 100% accuracy for a solid \nground truth. Moreover, using minimum wage in the evaluation of \ntotal cost may be inaccurate. Healthcare providers such as doctors \nor nurses, who would likely collect the clinical information from \nthe medical records in real-life, receive more than a minimum \nwage. Therefore, the exact cost of manual information collection \ncould be higher. Finally, the extrapolation of time and cost taken in \n60 patients into 2,913 patients may be inaccurate. Validation with \nmore data is required. \nIn conclusion, we showed that using LLM prompts is an efficient \nway to extract crucial information from the medical records of \nbreast cancer patients and to construct well-fined clinical data. \nThis method is expected to save lots of effort from daily practice \nand research work. Prompts from the current study can be re-used \nfor other investigators to collect clinical information. \nStatement of Ethics \nThis study was approved by the Institutional Review Board of Seoul \nNational University Hospital (IRB No. H-2304-072-1422). \nConflict of Interest \nNo potential conflict of interest relevant to this article was report-\ned. \nFunding \nThis work was supported by the National R&D Program for Cancer \nControl through the National Cancer Center (NCC) funded by the \nMinistry of Health & Welfare, Republic of Korea (No. HA22C0044) \nto Kyung Hwan Shin and the Ministry of Science and Information \n& Communication Technology (No. NRF-2022R1F1A106345712) to \nBum-Sup Jang. \nAuthor Contributions \nConceptualization, Choi HS, Song JY, Jang BS. Funding acquisition, \nShin KH, Jang BS. Investigation and methodology, Choi HS, Song JY, \nJang BS. Project administration, Choi HS. Resources, Shin KH, \nChang JH, Jang BS. Supervision, Jang BS. Writing of the original \ndraft, Choi HS, Song JY. Writing of the review and editing, Choi HS, \nSong JY. Software, Choi HS. Validation, Choi HS. Formal analysis, \nChoi HS, Song JY. Data curation, Choi HS. Visualization, Choi HS. \nData Availability Statement \nThe data that support the findings of this study are not publicly \navailable due to their containing information that could compro-\nmise the privacy of research participants but are available from \nBum Sup Jang upon reasonable request. \nSupplementary Materials \nSupplementary materials can be found via https://doi.org/10.3857/\nroj.2023.00633. \nhttps://doi.org/10.3857/roj.2023.00633216\nHyeon Seok Choi, et al.\nReferences \n1. National Comprehensive Cancer Network. NCCN Clinical Practice \nGuidelines in Oncology: breast cancer [Internet]. Plymouth Meet-\ning, PA: National Comprehensive Cancer Network; 2023 [cited \n2023 Sep 13]. Available from: https://www.nccn.org/profession-\nals/physician_gls/pdf/breast.pdf. \n2. Park HJ, Shin KH, Kim JH, et al. Incorporating risk factors to iden-\ntify the indication of post-mastectomy radiotherapy in N1 breast \ncancer treated with optimal systemic therapy: a multicenter \nanalysis in Korea (KROG 14-23). Cancer Res Treat 2017;49:739–\n47. \n3. Yamada A, Hayashi N, Kumamaru H, et al. Prognostic impact of \npostoperative radiotherapy in patients with breast cancer and \nwith pT1-2 and 1-3 lymph node metastases: a retrospective co-\nhort study based on the Japanese Breast Cancer Registry. Eur J \nCancer 2022;172:31–40. \n4. Jwa E, Shin KH, Lim HW, et al. Identification of risk factors for lo-\ncoregional recurrence in breast cancer patients with nodal stage \nN0 and N1: who could benefit from post-mastectomy radiother-\napy? PLoS One 2015;10:e0145463. \n5. Viani GA, Godoi da Silva LB, Viana BS. Patients with N1 breast \ncancer: who could benefit from supraclavicular fossa radiothera-\npy? Breast 2014;23:749–53. \n6. Bitterman DS, Miller TA, Mak RH, Savova GK. Clinical natural \nlanguage processing for radiation oncology: a review and practi-\ncal primer. Int J Radiat Oncol Biol Phys 2021;110:641–55. \n7. OpenAI. GPT-4 technical report [Internet]. Ithaca, NY: arXiv.org; \n2023 [cited 2023 Sep 13]. Available from: https://doi.org/10. \n48550/arXiv.2303.08774. \n8. Clavie B, Ciceu A, Naylor F, Soulie G, Brightwell T. Large language \nmodels in the workplace: a case study on prompt engineering for \njob type classification. In: Metais E, Meziane F, Sugumaran V, \nManning W, Reiff-Marganiec S, editors. Natural language to in-\nformation systems. Cham, Switzerland: Springer; 2023, p. 3–17. \n9. Floridi L, Chiriatti M. GPT-3: its nature, scope, limits, and conse-\nquences. Minds Mach 2020;30:681–94. \n10. Lee P, Budeck S, Petro J, et al. Benefits, limits, and risks of GPT-4 \nas an AI chatbot for medicine. N Engl J Med 2023;388:1233–9. \n11. Wang J, Shi E, Yu S, et al. Prompt engineering for healthcare: \nmethodologies and applications [Internet]. Ithaca, NY: arXiv.org; \n2023 [cited 2023 Sep 13]. Available from: https://doi.org/10. \n48550/arXiv.2304.14670. \n12. Seok BH, You HM. Macroeconomic impacts of increasing the  \nminimum wage: the case of Korea. Econ Model 2022;113:105880. \n13. Minimum Wage Commission, Republic of Korea. Announcement \nof the Results of the Healthcare Workforce Status Survey [Inter-\nnet]. Sejong, Korea: Minimum Wage Commission; c2022 [cited \n2023 Sep 13]. Available from: https://www.minimumwage.go.kr/\nminWage/policy/decisionMain.do. \n14. Sohn S, Seo Y, Jeong Y, Lee S, Lee J, Lee KJ. Changes in the work-\ning conditions and learning environment of medical residents af-\nter the enactment of the Medical Resident Act in Korea in 2015: \na national 4-year longitudinal study. J Educ Eval Health Prof \n2021;18:7. \n15. van Heerden AC, Pozuelo JR, Kohrt BA. Global mental health ser-\nvices and the impact of artificial intelligence-powered large lan-\nguage models. JAMA Psychiatry 2023;80:662–4. 16.Juhn Y, Liu H. \nArtificial intelligence approaches using natural language pro-\ncessing to advance EHR-based clinical research. J Allergy Clin \nImmunol 2020;145:463–9. \n17. Tang R, Han X, Jiang X, Hu X. Does synthetic data generation of \nLLMs help clinical text mining? [Internet]. Ithaca, NY: arXiv.org; \n2023 [cited 2023 Sep 13]. Available from: https://doi.org/10. \n48550/arXiv.2303.04360. \n18. Kung TH, Cheatham M, Medenilla A, et al. Performance of \nChatGPT on USMLE: potential for AI-assisted medical education \nusing large language models. PLOS Digit Health 2023;2:  \ne0000198. \n19. Singhal K, Tu T, Gottweis J, et al. Towards expert-level medical \nquestion answering with large language models [Internet]. Itha-\nca, NY: arXiv.org; 2023 [cited 2023 Sep 13]. Available from: \nhttps://doi.org/10.48550/arXiv.2305.09617.  ",
  "topic": "Medicine",
  "concepts": [
    {
      "name": "Medicine",
      "score": 0.8418352007865906
    },
    {
      "name": "Breast cancer",
      "score": 0.5977741479873657
    },
    {
      "name": "Medical physics",
      "score": 0.5268852710723877
    },
    {
      "name": "Cancer",
      "score": 0.30313435196876526
    },
    {
      "name": "Internal medicine",
      "score": 0.09253400564193726
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2802835388",
      "name": "Seoul National University Hospital",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I139264467",
      "name": "Seoul National University",
      "country": "KR"
    }
  ]
}