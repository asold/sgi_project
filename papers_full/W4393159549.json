{
  "title": "Relational Programming with Foundational Models",
  "url": "https://openalex.org/W4393159549",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5100422865",
      "name": "Ziyang Li",
      "affiliations": [
        "California University of Pennsylvania"
      ]
    },
    {
      "id": "https://openalex.org/A5102892217",
      "name": "Jiani Huang",
      "affiliations": [
        "California University of Pennsylvania"
      ]
    },
    {
      "id": "https://openalex.org/A5083193232",
      "name": "Jason Liu",
      "affiliations": [
        "California University of Pennsylvania"
      ]
    },
    {
      "id": "https://openalex.org/A5113151797",
      "name": "Felix Zhu",
      "affiliations": [
        "California University of Pennsylvania"
      ]
    },
    {
      "id": "https://openalex.org/A5030306558",
      "name": "Eric Yanfei Zhao",
      "affiliations": [
        "California University of Pennsylvania"
      ]
    },
    {
      "id": "https://openalex.org/A5104264409",
      "name": "William Dodds",
      "affiliations": [
        "California University of Pennsylvania"
      ]
    },
    {
      "id": "https://openalex.org/A5040750910",
      "name": "Neelay Velingker",
      "affiliations": [
        "California University of Pennsylvania"
      ]
    },
    {
      "id": "https://openalex.org/A5065059795",
      "name": "Rajeev Alur",
      "affiliations": [
        "California University of Pennsylvania"
      ]
    },
    {
      "id": "https://openalex.org/A5075879790",
      "name": "Mayur Naik",
      "affiliations": [
        "California University of Pennsylvania"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6635551685",
    "https://openalex.org/W6789431987",
    "https://openalex.org/W4311432567",
    "https://openalex.org/W2995628494",
    "https://openalex.org/W3210720616",
    "https://openalex.org/W6838865847",
    "https://openalex.org/W3035225320",
    "https://openalex.org/W6852103076",
    "https://openalex.org/W3034903373",
    "https://openalex.org/W4319792263",
    "https://openalex.org/W6846907077",
    "https://openalex.org/W4226317937",
    "https://openalex.org/W4306802146",
    "https://openalex.org/W2945570796",
    "https://openalex.org/W2891021031",
    "https://openalex.org/W4300427681",
    "https://openalex.org/W2561715562",
    "https://openalex.org/W4372342402",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4385570090",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W4385775261",
    "https://openalex.org/W4386847924",
    "https://openalex.org/W4312933868",
    "https://openalex.org/W4282961889",
    "https://openalex.org/W2962766044",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W3126220825",
    "https://openalex.org/W4280647773",
    "https://openalex.org/W4386065691",
    "https://openalex.org/W4388626886",
    "https://openalex.org/W1595443289",
    "https://openalex.org/W4289785045",
    "https://openalex.org/W4385574156",
    "https://openalex.org/W4386211255",
    "https://openalex.org/W4389519585",
    "https://openalex.org/W2971107062",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4309591663",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4367185264",
    "https://openalex.org/W4320165837",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W3016473712",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4386076215",
    "https://openalex.org/W3035398729",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W4309088836",
    "https://openalex.org/W4379536072",
    "https://openalex.org/W3126792443",
    "https://openalex.org/W2938082352",
    "https://openalex.org/W4298084898",
    "https://openalex.org/W4392637287",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4362679702",
    "https://openalex.org/W2953388933",
    "https://openalex.org/W4288346960"
  ],
  "abstract": "Foundation models have vast potential to enable diverse AI applications. The powerful yet incomplete nature of these models has spurred a wide range of mechanisms to augment them with capabilities such as in-context learning, information retrieval, and code interpreting. We propose Vieira, a declarative framework that unifies these mechanisms in a general solution for programming with foundation models. Vieira follows a probabilistic relational paradigm and treats foundation models as stateless functions with relational inputs and outputs. It supports neuro-symbolic applications by enabling the seamless combination of such models with logic programs, as well as complex, multi-modal applications by streamlining the composition of diverse sub-models. We implement Vieira by extending the Scallop compiler with a foreign interface that supports foundation models as plugins. We implement plugins for 12 foundation models including GPT, CLIP, and SAM. We evaluate Vieira on 9 challenging tasks that span language, vision, and structured and vector databases. Our evaluation shows that programs in Vieira are concise, can incorporate modern foundation models, and have comparable or better accuracy than competitive baselines.",
  "full_text": "Relational Programming with Foundation Models\nZiyang Li, Jiani Huang, Jason Liu, Felix Zhu, Eric Zhao, William Dodds, Neelay Velingker,\nRajeev Alur, Mayur Naik\nUniversity of Pennsylvania\nliby99@seas.upenn.edu, jianih@seas.upenn.edu, jasonhl@seas.upenn.edu, zhufelix@seas.upenn.edu,\nzhaoer@seas.upenn.edu, wdodds@sas.upenn.edu, neelay@seas.upenn.edu, alur@seas.upenn.edu, mhnaik@seas.upenn.edu\nAbstract\nFoundation models have vast potential to enable diverse AI\napplications. The powerful yet incomplete nature of these\nmodels has spurred a wide range of mechanisms to augment\nthem with capabilities such as in-context learning, informa-\ntion retrieval, and code interpreting. We propose V IEIRA ,\na declarative framework that unifies these mechanisms in a\ngeneral solution for programming with foundation models.\nVIEIRA follows a probabilistic relational paradigm and treats\nfoundation models as stateless functions with relational in-\nputs and outputs. It supports neuro-symbolic applications by\nenabling the seamless combination of such models with logic\nprograms, as well as complex, multi-modal applications by\nstreamlining the composition of diverse sub-models. We im-\nplement VIEIRA by extending the SCALLOP compiler with a\nforeign interface that supports foundation models as plugins.\nWe implement plugins for 12 foundation models including\nGPT, CLIP, and SAM. We evaluate V IEIRA on 9 challeng-\ning tasks that span language, vision, and structured and vector\ndatabases. Our evaluation shows that programs in VIEIRA are\nconcise, can incorporate modern foundation models, and have\ncomparable or better accuracy than competitive baselines.\nIntroduction\nFoundation models are deep neural models that are trained\non a very large corpus of data and can be adapted to a wide\nrange of downstream tasks (Bommasani et al. 2021). Exem-\nplars of foundation models include language models(LMs)\nlike GPT (Bubeck et al. 2023), vision modelslike Segment\nAnything (Kirillov et al. 2023), andmulti-modal modelslike\nCLIP (Radford et al. 2021). While foundation models are\na fundamental building block, they are inadequate for pro-\ngramming AI applications end-to-end. For example, LMs\nhallucinate and produce nonfactual claims or incorrect rea-\nsoning chains (McKenna et al. 2023). Furthermore, they lack\nthe ability to reliably incorporate structured data, which is\nthe dominant form of data in modern databases. Finally,\ncomposing different data modalities in custom or complex\npatterns remains an open problem, despite the advent of\nmulti-modal foundation models such as ViLT (Radford et al.\n2021) for visual question answering.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n@gpt(\"The height of {{x}} is {{y}} in meters\")\ntype height(bound x: String, y: i32)\n// Retrieving height of mountains\nrel mount_height(m, h) = mountain(m) and height(m, h)\n(a) Program P1: Extracting knowledge using GPT.\n@clip([\"cat\", \"dog\"])\ntype classify(bound img: Tensor, label: String)\n// Classify each image as cat or dog\nrel cat_or_dog(i, l) = image(i, m) and classify(m, l)\n(b) Program P2: Classifying images using CLIP.\nid img \n1 \n2 \nid label \n1 cat \n1 dog \n2 cat \n2 dog \nEverest \nFuji \n8848 \n3776 \nmountain mount_height \nP2 \nimage cat_or_dog \nname height \nEverest \nFuji \nname \nK2 8611 K2 \nMt.Blanc 4808 Mt.Blanc \nP1 \nprob \n0.02 \n0.98 \n0.99 \n0.01 \n(c) Example input-output relations of the programs.\nFigure 1: Programs in VIEIRA using foundation models.\nVarious mechanisms have been proposed to augment\nfoundation models to overcome these limitations. For exam-\nple, PAL (Gao et al. 2023), WebGPT (Nakano et al. 2021),\nand Toolformer (Schick et al. 2023) connect LMs with\nsearch engines and external tools, expanding their informa-\ntion retrieval and structural reasoning capabilities. LMQL\n(Beurer-Kellner, Fischer, and Vechev 2022) generalizes pure\ntext prompting in LMs to incorporate scripting. In the do-\nmain of computer vision (CV), neuro-symbolic visual rea-\nsoning frameworks such as VISPROG (Gupta and Kembhavi\n2022) compose diverse vision models with LMs and image\nprocessing subroutines. Despite these advances, program-\nmers lack a general solution that systematically incorporates\nthese methods into a single unified framework.\nIn this paper, we propose V IEIRA , a declarative frame-\nwork for programming with foundation models. VIEIRA fol-\nlows a (probabilistic) relational paradigm due to its theoret-\nical and practical versatility. Structured data is commonly\nstored in relational databases. Relations can also represent\nstructures such as scene graphsin vision and abstract syntax\ntrees in natural and formal languages. Moreover, extensions\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10635\nfor probabilistic and differentiable reasoning enable the in-\ntegration of relational programming with deep learning in\nneuro-symbolic frameworks like DeepProbLog (Manhaeve\net al. 2018) and SCALLOP (Li, Huang, and Naik 2023).\nIn VIEIRA , relations form the abstraction layer for inter-\nacting with foundation models. Our key insight is that foun-\ndation models are stateless functionswith relational inputs\nand outputs. Fig. 1a shows a V IEIRA program which in-\nvokes GPT to extract the height of mountains whose names\nare specified in a structured table. Likewise, the program\nin Fig. 1b uses the image-text alignment model CLIP to\nclassify images into discrete labels such as cat and dog.\nFig. 1c shows relational input-output examples for the two\nprograms. Notice that the CLIP model also outputs proba-\nbilities that allow for probabilistic reasoning.\nWe implement VIEIRA by extending the S CALLOP com-\npiler with a foreign interfacethat supports foundation mod-\nels as plugins. We implement a customizable and extensi-\nble plugin library comprising 12 foundation models includ-\ning GPT, CLIP, and SAM. The resulting unified interface\nenables a wide spectrum of applications with benefits such\nas reduced hallucination, retrieval augmentation, and multi-\nmodal compositionality. We evaluate VIEIRA on 9 applica-\ntions that span natural language reasoning, information re-\ntrieval, visual question answering, image generation, and\nimage editing. For these applications, we explore diverse\nmethods for programming with foundation models, such as\nneuro-symbolic reasoning, combining semantic searching\nwith question answering, and modularly composing founda-\ntion models. We not only observe on-par or superior perfor-\nmance of our solutions compared to competitive baselines,\nbut also demonstrate their succinctness and ease-of-use.\nWe summarize our contributions as follows: (1) we in-\ntroduce a new approach based on relational programming\nto build applications on top of foundation models; (2) we\nimplement an extensible plugin library of 12 programmable\nfoundation models; and (3) we evaluate V IEIRA on 9\nbenchmark tasks, and demonstrate comparable or better no-\ntraining accuracy than neural-only as well as task-specific\nbaselines. Our framework, plugin library, and evaluations\nare open-source and available at https://github.com/scallop-\nlang/scallop.\nRelated Work\nNeuro-symbolic methods. These methods combine the\ncomplementary benefits of neural learning and symbolic rea-\nsoning. They include domain-specific solutions (Yi et al.\n2018; Mao et al. 2019; Li et al. 2020; Wang et al. 2019;\nXu et al. 2022; Chen et al. 2020; Minervini et al. 2020)\nas well as general programming frameworks, such as Deep-\nProbLog (Manhaeve et al. 2018) and SCALLOP (Li, Huang,\nand Naik 2023). These methods typically concern training\nor fine-tuning neural models in the presence of logical pro-\ngrams, whereas we target building applications atop foun-\ndation models with zero-shot or few-shot examples. An-\nother recent work, the STAR framework (Rajasekharan et al.\n2023) also connects a language model (neural) to an an-\nswer set programming reasoner (symbolic). It is conceptu-\nally similar to VIEIRA but only focuses on natural language\nunderstanding and does not support probabilistic reasoning.\nFoundation models. These models target different modal-\nities and domains (Touvron et al. 2023; OpenAI 2023; Rad-\nford et al. 2021; Kirillov et al. 2023; Radford et al. 2021).\nTheir reasoning capabilities continue to improve with larger\ncontext sizes (Ratner et al. 2023), smarter data selection\n(Adadi 2021), and the discovery of new prompting meth-\nods, such as chain-of-thought (Wei et al. 2023; Kojima et al.\n2022), self-consistency (Wang et al. 2023), and ReAct (Yao\net al. 2023). V IEIRA is orthogonal to these techniques and\nstands to further enhance the robustness and reliability of\nfoundation models in end-to-end AI applications.\nTools aiding language models. There are many efforts\nthat seek to improve the reasoning abilities of language\nmodels (LMs) by incorporating external programs and\ntools (Gao et al. 2023; Schick et al. 2023; Nakano et al.\n2021; Davis and Aaronson 2023). For instance, AutoGPT\n(Richards 2023) and TaskMatrix.AI (Liang et al. 2023) al-\nlows black-box LMs to control symbolic reasoning by in-\nvoking commands or calling APIs. On the other hand, many\nworks attempt to extract structured information from LMs\nfor downstream tasks (Gupta and Kembhavi 2022; Beurer-\nKellner, Fischer, and Vechev 2022). V IEIRA unifies these\ntwo strategies for augmenting model capabilities, and ex-\ntends them into a glue language for composing multi-modal\nfoundation models.\nLanguage\nVIEIRA employs a declarative logic programming language\nbased on Datalog (Abiteboul, Hull, and Vianu 1994). In this\nsection, we present the core language and its foreign inter-\nface for incorporating diverse foundation models.\nCore Language\nRelations and data types. The fundamental data type\nin V IEIRA is set-valued relations comprising tuples of\nstatically-typed primitive values. Besides the standard prim-\nitive types such as integers (e.g. i32) and string ( String),\nVIEIRA introduces two additional types for seamless inte-\ngration of foundation models: Tensor and Algebraic Data\nTypes(ADTs). For example, we can declare a relation named\nimage to store tuples of image IDs and image Tensors:\ntype image(img_id: i32, img: Tensor)\nThe contents of this relation can be specified via a set of\ntuples using the built-in foreign function$load_image:\nrel image = {(0, $load_image(\"cat.png\")), ...}\nADTs in V IEIRA enable the specification of domain spe-\ncific languages (DSLs) to bridge structured and unstructured\ndata. For example, the following DSL for visual question an-\nswering (VQA) describes queries to retrieve scene objects,\ncount objects, and check the existence of objects:\ntype Query = Scene() | Filter(Query, String)\n| Count(Query) | Exists(Query) | ...\n// How many balls are there?\nconst MY_QUERY = Count(Filter(Scene(), \"ball\"))\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10636\nLogical reasoning. Being based on Datalog, VIEIRA sup-\nports defining Horn rules, thereby allowing logical reason-\ning constructs such as conjunction, disjunction, recursion,\nstratified negation, and aggregation. Recursion is particu-\nlarly useful for inductively defining the semantics of a DSL.\nFor example, a (partial) semantics for the above DSL is de-\nfined as follows, where eval_o and eval_n are recursively\ndefined to evaluate objects and numbers, respectively:\n// Scene returns all objects\nrel eval_o(e, o) = case e is Scene() and obj(o)\n// Filter applies filter using attributes\nrel eval_o(e, o) = case e is Filter(f, a)\nand eval_o(f, o) and attr(o, a)\n// Count returns the number of evaluated objects\nrel eval_n(e, n) = n := count(o: eval_o(e1, o)\nwhere e1: case e is Count(e1))\n... // other cases of ‘e’\nNote that thecase-is operator matches patterns of the ADT\nand the count aggregator counts the number of entities.\nWhen combined with foundation models, principled reason-\ning semantics in this style can compensate for individual\nfoundation models’ lack of reasoning capability.\nProbabilistic soft logic. Tuples can be tagged with proba-\nbilities. The example below shows hard-coded probabilities,\nsuggesting that the entity is more likely a dog than a cat:\nrel animal = {0.1::(1,\"cat\"), 0.9::(1,\"dog\")}\nSoft-logic operations produce probabilities as well. For in-\nstance, the soft-eq operator (˜=) on Tensors derives cosine-\nsimilarity between tensors, enabling features like soft-join\nand applications like semantic search. In the following ex-\nample, we compute similarity scores between distinct docu-\nments by performing soft-join on their embeddings:\ntype doc(id: i32, embed: Tensor) // embed docs\nrel sim(i, j) = doc(i, v) and doc(j, v) and i!=j\n// equiv: sim(i, j) = doc(i, v1) and doc(j, v2)\nand i!=j and v1~=v2\nNotice that in the above rule, a join on a tensor valuev is de-\nsugared into a soft-eq on two individual variables (denoted\nv1 and v2). Internally, with the provenance framework pro-\nvided by SCALLOP (Li, Huang, and Naik 2023), we use the\ntop-k-proofs semiring (Huang et al. 2021) for scalable prob-\nabilistic reasoning, thus enabling features such as ranking\nand uncertainty estimation.\nForeign Interface\nIn order to incorporate foundation models, we design a\nforeign interface with two main programming constructs,\ncalled foreign predicateand foreign attribute. They can be\ndefined externally in languages like Python and imported\ninto VIEIRA for application.\nForeign Predicate (FP). Foreign predicates can be used in\nrules just like other relations. However, instead of grounding\nrelational facts from a table, FPs ground facts by invoking\nexternal functions. The syntax for defining FPs is as follows:\nextern type PRED([bound|free]? ARG: TYPE, ...)\nIn addition to the type, each argument is specified either as\na bounded argument (using the keyword bound) or a free\n@foreign_attribute\ndef clip(pred: Predicate, labels: List[str]):\n# Sanity checks for predicate and labels...\nassert pred.args[0].ty == Tensor and ...\n@foreign_predicate(name=pred.name)\ndef run_clip(img: Tensor) -> Facts[str]:\n# Invoke CLIP to classify image into labels\nprobs = clip_model(img, labels)\n# Each result is tagged by a probability\nfor (prob, label) in zip(probs, labels):\nyield (prob, (label,)) # prob::(label,)\nreturn run_clip\nFigure 2: Snippet of Python implementation of the foreign\nattribute clip which uses the CLIP model for image classi-\nfication. Notice that the FA clip returns the FP run_clip.\nargument (using free or omitted for brevity). Semantically,\nFPs are functions that take in a tuple of bounded arguments\nand return a list of tuples of free arguments. The runtime\nof VIEIRA performs memoization on FP results to avoid re-\ndundant computation. Optionally, FPs can tag a probability\nto each returned tuple for further probabilistic reasoning.\nForeign Attribute (FA). In VIEIRA , attributes can be used\nto decorate declarations of predicates. They are higher-order\nfunctions that take in the provided arguments and the dec-\norated predicate to return a new predicate. The syntax for\nusing an attribute to decorate a predicate is:\n@ATTR(POS_ARG, ..., KEY=KW_ARG, ...)\ntype PRED([bound|free]? ARG: TYPE, ...)\nThe attribute is applied prior to the compilation of V IEIRA\nprograms. For interfacing with foundation models, the po-\nsitional and keyword arguments are particularly helpful in\nconfiguring the underlying model, hiding low-level details.\nFig. 2 illustrates one succinct implementation of the FA that\nenables the use of the CLIP model shown in Fig. 1b.\nFoundation Models\nVIEIRA provides an extensibleplugin frameworkthat adapts\nto the evolving landscape of foundation models. In this\nwork, we have implemented 7 plugins, covering 12 foun-\ndation models, all through the foreign interface. Our design\nprinciple for the interface is three-fold: simplicity, config-\nurability, and compositionality. In this section, we present\nseveral representative predicates and attributes which sub-\nstantially support the applicability of VIEIRA to diverse ma-\nchine learning tasks.\nText completion. In V IEIRA , language models like GPT\n(OpenAI 2023) and LLaMA (Touvron et al. 2023) can be\nused as basic foreign predicates for text completion:\nextern type gpt(bound p: String, a: String)\nrel ans(a) = gpt(\"population of NY is\", a)\nIn this case, gpt is an arity-2 FP that takes in a String\nas the prompt and produces a String as the response. It\nuses the model gpt-3.5-turbo by default. To make the\ninterface more relational and structural, we provide an FA:\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10637\n@gpt(\"the population of {{loc}} is {{num}}\",\nexamples=[(\"NY\", 8468000), ...])\ntype population(bound loc: String, num: u32)\nHere, we declare a relation named population which pro-\nduces a population number (num) given a location (loc) as\ninput. Notice that structured few-shot examples are provided\nthrough the argument examples.\nSemantic parsing. One can directly configure language\nmodels to perform semantic parsing. For instance, the se-\nmantic parser for the simple Query DSL (partially defined\nin the Language section) can be declared as follows:\n@gpt_semantic_parse(\n\"Please semantically parse questions...\",\nexamples=[(\"How many red things are there?\",\n\"Count(Filter(Scene(), ’red’))\"), ...])\ntype parse_query(bound x: String, y: Query)\nInternally, the language model is expected to generate a fully\nstructured Query in its string form. Then, V IEIRA attempts\nto parse the string to construct actual ADT values. In prac-\ntice, the success of semantic parsing depends heavily on\nthe design of the DSL, involving factors like intuitiveness\n(e.g., names and arguments of ADT variants) and complex-\nity (e.g., number of possible ADT variants).\nRelational data extraction. Structural relational knowl-\nedge available in free-form textual data can be extracted\nby language models. We introduce a foreign attribute\n@gpt_extract_relation for this purpose. For instance,\nthe following declared predicate takes in a context and pro-\nduces (subject, object, relation) triplets:\n@gpt_extract_relation(\n\"Extract the implied kinship relations\",\nexamples=[(\"Alice and her son Bob went to...\",\n[(\"alice\", \"bob\", \"son\"), ...])])\ntype extract_kinship(bound ctx: String,\nsub: String, obj: String, rela: String)\nThis attribute differs from the text completion attribute in\nthat it can extract an arbitrary number of facts. The under-\nlying implementation prompts LMs to respond with JSON-\nformatted strings, allowing structured facts to be parsed.\nLanguage models for textual embedding. Textual em-\nbeddings are useful in performing tasks such as information\nretrieval. The following example declares an FP encapsulat-\ning a cross-encoder (Nogueira and Cho 2019):\n@cross_encoder(\"nli-deberta-v3-xsmall\")\ntype enc(bound input: String, embed: Tensor)\nrel sim() = enc(\"cat\", e) and enc(\"neko\", e)\nIn the last line, we compute the cosine-similarity of the en-\ncoded embeddings using a soft-join on the variable e. As\na result, we obtain a probabilistic fact like 0.9::sim()\nwhose probability encodes the cosine-similarity between the\ntextual embeddings of \"cat\" and \"neko\".\nImage classification models. Image-text alignment mod-\nels, such as CLIP (Radford et al. 2021), can naturally be\nused as zero-shot image classification models. Fig. 1b shows\nan example usage of the @clip attribute. We also note that\ndynamically-generated classification labels can be provided\nto CLIP via a bounded argument in the predicate.\nImage segmentation models. OWL-ViT (Minderer et al.\n2022), Segment Anything Model (SAM) (Kirillov et al.\n2023), and DSFD (Li et al. 2018) are included in VIEIRA as\nimage segmentation (IS) and object localization (LOC)\nmodels. IS and LOC models can provide many outputs, such\nas bounding boxes, classified labels, masks, and cropped im-\nages. For instance, the OWL-ViT model can be used and\nconfigured as follows:\n@owl_vit([\"human face\", \"rocket\"])\ntype find_obj(bound img: Tensor,\nid: u32, label: String, cropped_image: Tensor)\nHere, the find_obj predicate takes in an image, and finds\nimage segments containing “human face” or “rocket”. Ac-\ncording to the names of the arguments, the model extracts\n3 values per segment: ID, label, and cropped image. Note\nthat each produced fact will be associated with a probability,\nrepresenting the confidence from the model.\nImage generation models. Visual generative models such\nas Stable Diffusion (Rombach et al. 2022) and DALL-\nE (Ramesh et al. 2021) can be regarded as relations as\nwell. The following example shows the declaration of\nthe gen_image predicate, which encapsulates a diffusion\nmodel:\n@stable_diffusion(\"stable-diffusion-v1-4\")\ntype gen_image(bound txt: String, img: Tensor)\nAs can be seen from the signature, it takes in a String text\nas input and produces a Tensor image as output. Optional\narguments such as the desired image resolution and the num-\nber of inference steps can be supplied to dictate the granu-\nlarity of the generated image.\nTasks and Solutions\nWe apply V IEIRA to solve 9 benchmark tasks depicted in\nFig. 3. Table 1 summarizes the datasets, evaluation metrics,\nand the foundation models used in our solutions. We elabo-\nrate upon the evaluation settings and our solutions below.\nDate reasoning (DR). In this task adapted from BIG-\nbench (Srivastava et al. 2023), the model is given a context\nand asked to compute a date. The questions test the model’s\ntemporal and numerical reasoning skills, as well as its grasp\nof common knowledge. Unlike BIG-bench where multiple-\nchoice answers are given, we require the model to directly\nproduce its answer in MM/DD/YYYY form.\nOur solution leverages GPT-4 (5-shot 1) for extracting 3\nrelations: mentioned dates, duration between date labels, and\nthe target date label. From here, our relational program iter-\nates through durations to compute dates for all date labels.\nLastly, the date of the target label is returned as the output.\nTracking shuffled objects (TSO). In this task from BIG-\nbench, a textual description of pairwise object swaps among\npeople is given, and the model needs to track and derive\nwhich object is in a specified person’s possession at the end.\n1In this work, k in “k-shot” means the number of examples pro-\nvided to the LM component within the full solution. Each example\nis a ground-truth input-output pair for the LM.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10638\nImage Generation and Editing\nA bowl full of apples Replace the bowl with\nother containers\nReplace the apple with\nother fruits\nA plate full of apples A plate full of oranges\nInput\nPrompts\nOutput\nImages\nDate Reasoning\nMay 6, 1992 is like yesterday to Jane, but that is\nactually ten years ago. What is the date one week\nfrom today in MM/DD/YYYY?\n05/13/2002\nQuestion:\nAnswer:\nTracking Shuffled Objects \nAlice has an orange ball, Bob has a white ball, and\nClaire has a blue ball. Alice and Bob swap balls.\nThen, Bob and Claire swap balls. Alice has the __.\nwhite ball\nQuestion:\nAnswer:\nKinship Reasoning \nRich's daughter Kelly made dinner for her sister\nKim. Dorothy went to her brother Rich's birthday\nparty. Anne went shopping with her sister Kim. How\nis Dorothy related to Anne?\nniece\nQuestion:\nAnswer:\nQA\nCompositional VQA\nProduct Search \nMath Reasoning\nImage Editing\nDocuments: Products:\nTag \"microsoft ceos.jpg\"Instruction:\nAnswer:\nWhich team does the player named 2015 Diamond\nHead Classic’s MVP play for?\nQuestion:\nSacramento Kings\nlawnmower tires without rimsQuery:\nProduct Ranking: 1st: #2, 2nd: #6, 3rd: #4, ...\nQuestion:\nAlice is required to submit a 15-page paper. She finished\nwriting 1/3 of the paper. How many pages are left to write?\nAnswer: 10\n                       Hide Walter Thurnherr\nwith smiling_face_with_halo and Alain\nBerset with crying_cat.\nIs the tray on top of the table\nblack or light brown?\nHow many objects are\nred\n in this image?\nObj Tagging\nQuestion:\nlight brown\nAnswer:\nQuestion:\nAnswer:\n3\nGPT\nLostAlone\nwere a\nBritish rock\nband ...\nThe 2015\nDiamond\nHead\nClassic was\n...\nSteven\nBattelle,\nAlan\nWilliamson,\nand ...\nGuster is\nan\nAmerican\nalternative\nrock band\n....\nFounding\nmembers\nAdam\nGardner,\nRyan\nMiller,...\nChavano\nRainier Buddy\nHield is a\nBahamian ...\nBrian\nRosenworcel\nbegan..\nSeveral\ncurrent and\nformer\nmembers of\nthe  ...\nDavid Gene\nParker,\nnicknamed\n”The Cobra” ...\nAn American\nformer player\nin Major\nLeague\nBaseball...\n...\nInput Image Edited Image\nInstruction:\n1\nGPT\nGPT\nGPT GPT-Enc GPT Cross-Enc\nGPT\nGPT ViLT OWL-ViT CLIP\nRamPro 10\"\nAll Purpose\nUtility Air\nTires/Wheel\n2 3 4 5\n(Set of 2)\n15x6.00-6\nHusqvarna\n/Poulan Tire ...\nMaxAuto 2-\nPack\n13x5.00-6\n2PLY Turf\nNEIKO\n20601A 14.5\ninch  Steel\nTire Spoon ...\n2PK\n13x5.00-6\n13x5.00x6\n13x5x6\n13x5-6 ...\nBIG-bench\nBIG-bench\nCLUTRR\nHotpotQA Amazon ESCI\nGSM8K\nGQA CLEVR\nVQAR\nInput Image Tagged Image\nIGP20OFCP GPT CLIP DSFDOFCP GPT CLIP DSFD GPT Stable-Diffusion\nFigure 3: Benchmark tasks. The top of each box lists the dataset(s) and the foundation models used in our solutions.\nThere are three difficulty levels depending on the number of\nobjects to track, denoted by n ∈ {3,5, 7}.\nOur solution for tracking shuffled objects relies on GPT-4\n(1-shot) to extract 3 relations: initial possessions, swaps, and\nthe target person whose final possessed object is expected\nas the answer. Our reasoning program iterates through all\nthe swaps starting from the initial state and retrieves the last\npossessed object associated with the target.\nKinship reasoning (KR). CLUTRR (Sinha et al. 2019) is\na kinship reasoning dataset of stories which indicate the kin-\nship between characters, and requires the model to infer the\nrelationship between two specified characters. The questions\nhave different difficulty levels based on the length of the rea-\nsoning chain, denoted by k ∈ {2. . .10}.\nOur solution for kinship reasoning invokes GPT-4 (2-\nshot) to extract the kinship graph from the context. We also\nprovide an external common-sense knowledge base for rules\nlike “mother’s mother is grandmother”. Our program then\nuses the rules to derive other kinship relations. Lastly, we\nretrieve the kinship between the specified pair of people.\nMath reasoning (MR). This task is drawn from the\nGSM8K dataset of arithmetic word problems (Cobbe et al.\n2021). The questions involve grade school math word prob-\nlems created by human problem writers, and the model is\nasked to produce a number as the result. Since the output\ncan be fractional, we allow a small delta when comparing\nthe derived result with the ground truth.\nOur solution to this task prompts GPT-4 (2-shot) to pro-\nduce step-by-step expressions, which can contain constants,\nvariables, and simple arithmetic operations. We evaluate all\nthe expressions through a DSL, and the result associated\nwith the goal variable is returned. By focusing the LM’s re-\nsponsibility solely on semantic parsing, our relational pro-\ngram can then achieve faithful numerical computation via\nDSL evaluation.\nQuestion answering with information retrieval (QA).\nWe choose HotpotQA (Yang et al. 2018), a Wikipedia-based\nquestion answering (QA) dataset under the “distractor” set-\nting. Here, the model takes in 2 parts of inputs: 1) a question,\nand 2) 10 Wikipedia paragraphs as the context for answering\nthe question. Among the 10 Wikipedia pages, at most 2 are\nrelevant to the answer, while the others are distractors.\nOur solution is an adaptation of FE2H (Li, Lei, and Yang\n2022), which is a 2-stage procedure. First, we turn the 10\ndocuments into a vector database by embedding each docu-\nment. We then use the embedding of the question to retrieve\nthe 2 most related documents, which are then fed to a lan-\nguage model to do QA. In this case, the QA model does not\nhave to process all 10 documents, leading to less distraction.\nProduct search (PS). We use Amazon’s ESCI Product\nSearch dataset (Reddy et al. 2022). The model is provided\nwith a natural language (NL) query and a list of products (23\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10639\nTask Dataset #Test\nSamples Metric Foundation\nModels Used\nDR DR 369 EM GPT-4\nTSO TSO 150 EM GPT-4\nKR CLUTRR 1146 EM GPT-4\nMR GSM8K 1319 EM GPT-4\nQA Hotpot QA 1000 EM GPT-4\nada-002\nPS Amazon\nESCI 1000 nDCG GPT-4\nada-002\nVQA\nCLEVR 480 Recall@1\nRecall@3\nGPT-4\nOWL-ViT\nGQA 500 VilT\nCLIP\nVOT\nVQAR 100\nMI\nOWL-ViT\nVilT\nGPT-4\nOFCP 50 DSFD\nCLIP\nIGE\nOFCP 50\nMI\nDFSD\nCLIP\nIGP20 20 GPT-4\nDiffusion\nTable 1: Characteristics of benchmark tasks including the\ndataset used, its size, and evaluation metrics. Metrics include\nexact match (EM), normalized discounted cumulative gain\n(nDCG), and manual inspection (MI). We also denote the\nfoundation models used in our solution for each task.\nproducts on average). The goal is to rank the products that\nbest match the query. In the dataset, for each pair of query\nand product, a label among E (exact match), S (substitute),\nC (complementary), and I (irrelevant) is provided. The met-\nric we use to evaluate the performance is nDCG. The gains\nare set to be 1.0 for E, 0.1 for S, 0.01 for C, and 0.0 for I.\nOne challenge of this dataset is that many queries contain\nnegative statements. For example, in the query “#1 treadmill\nwithout remote”, the “remote” is undesirable. Therefore, in-\nstead of computing the embedding of the full query, we de-\ncompose the query into positive and negative parts. We then\nperform semantic search by maximizing the similarity of the\npositive part while minimizing that of the negative part.\nCompositional visual question answering (VQA). We\nchoose two compositional VQA datasets, GQA (Hudson\nand Manning 2019) and CLEVR (Johnson et al. 2016).\nIn this task, the model is given an image and a question,\nand needs to answer the question. For GQA, the majority\nof questions expect yes/no answers, while CLEVR’s ques-\ntions demand features like counting and spatial reasoning.\nWe uniformly sample 500 and 480 examples from GQA\nand CLEVR datasets respectively. Following VQA conven-\ntions (Kim, Son, and Kim 2021), we use Recall@k where\nk ∈ {1,3} as the evaluation metrics.\nOur solution for GQA is an adaptation of V ISPROG\n(Gupta and Kembhavi 2022). We create a DSL for invok-\ning vision modules such as ViLT and OWL-ViT, and use\nGPT-4 for converting questions into programs in this DSL.\nOur solution for CLEVR is similar, directly replicating the\nDSL provided by the original work. OWL-ViT and CLIP are\nused to detect objects and infer attributes, while the spatial\nrelations are directly computed using the bounding box data.\nVisual object tagging (VOT). We evaluate on two\ndatasets, VQAR (Huang et al. 2021) and OFCP. For VQAR,\nthe model is given an image and a programmatic query, and\nis asked to produce bounding boxes of the queried objects\nin the image. Our solution composes a relational knowledge\nbase, defining entity names and relationships, with object re-\ntrieval (OWL-ViT) and visual QA (ViLT) models.\nOnline Faces of Celebrities and Politicians (OFCP) is a\nself-curated dataset of images from Wikimedia Commons\namong other sources. For this dataset, the model is given\nan image with a descriptive NL filename, and needs to de-\ntect faces relevant to the description and tag them with their\nnames. Our solution obtains a set of possible names from\nGPT-4 and candidate faces from DSFD. These are provided\nto CLIP for object classification, after which probabilistic\nreasoning filters the most relevant face-name pairs.\nLanguage-guided image generation and editing (IGE).\nWe adopt the task of image editing from (Gupta and Kem-\nbhavi 2022). In this task, the instruction for image editing\nis provided through NL, and can invoke operations such as\nblurring background, popping color, and overlaying emojis.\nDue to the absence of an existing dataset, we repurpose the\nOFCP dataset by introducing 50 NL image editing prompts.\nOur solution for this task is centered around a DSL for image\nediting. We incorporate GPT-4 for semantic parsing, DSFD\nfor face detection, and CLIP for entity classification. Mod-\nules for image editing operations are implemented as indi-\nvidual foreign functions.\nFor free-form generation and editing of images, we cu-\nrate IGP20, a set of 20 prompts for image generation and\nediting. Instead of using the full prompt, we employ an LM\nto decompose complex NL instructions into simpler steps.\nWe define a DSL with high-level operators such as generate,\nreweight, refine, replace, and negate. We use a combination\nof GPT-4, Prompt-to-Prompt (Hertz et al. 2022), and diffu-\nsion model (Rombach et al. 2022) to implement the seman-\ntics of our DSL. We highlight our capability of grounding\npositive terms from negative phrases, which enables han-\ndling prompts like “replace apple with other fruits” (Fig. 3).\nExperiments and Analysis\nWe aim to answer the following research questions:\nRQ1. Is VIEIRA programmable enough to be applicable to\na diverse range of applications with minimal effort?\nRQ2. How do solutions using V IEIRA compare to other\nbaseline methods in the no-training setting?\nRQ1: Programmability\nWhile a user study for V IEIRA ’s programmability is out of\nscope in this paper, we qualitatively evaluate its programma-\nbility on three aspects. First, we summarize the lines-of-code\n(LoC) for each of our solutions in Table 2. The programs\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10640\nDataset LoC Prompt\nLoC Dataset LoC Prompt\nLoC\nDR 69 48 CLEVR 178 45\nTSO 34 16 GQA 82 36\nCLUTRR 61 45 VQAR 53 11\nGSM8K 47 28 OFCP (VOT) 33 2\nHotpotQA 47 24 OFCP (IGE) 117 44\nESCI 32 7 IGP20 50 12\nTable 2: The lines-of-code (LoC) numbers of our solutions\nfor each dataset. The LoC includes empty lines, comments,\nnatural language prompts, and DSL definitions. We note\nspecifically the LoC of prompts in the table.\nMethod DR TSO CLUTRR GSM8K\nGPT-4 71.00\n(0-shot)\n30.00\n(0-shot)\n43.10\n(3-shot)\n87.10\n(0-shot)\nGPT-4 (CoT) 87.26\n(0-shot)\n84.00\n(0-shot)\n24.17\n(3-shot)\n92.00\n(5-shot)\nOurs 92.41 100.00 72.50 90.60\nTable 3: The performance on the natural language reasoning\ndatasets. Numbers are in percentage (%).\nHotpotQA Amazon ESCI\nMethod Fine-tuned EM Method Fine-tuned nDCG\nC2FM ✓ 72.07% BERT ✓ 0.830\nFE2H ✓ 71.89% CE-MPNet ✓ 0.857\n— — — MIPS ✗ 0.797\nOurs ✗ 67.3% Ours ✗ 0.798\nTable 4: The performance on the HotpotQA and Amazon\nESCI. We also include performance numbers from methods\nwhich are fine-tuned on the corresponding dataset.\nare concise, as most are under 100 lines. Notably, natural\nlanguage prompts (including few-shot examples) take up a\nsignificant portion of each solution. Secondly, 8 out of 10\nsolutions are coded by undergraduate students with no back-\nground in logic and relational programming, providing fur-\nther evidence of V IEIRA ’s user-friendliness. Last but not\nleast, our solutions are interpretable and thus offer debug-\ngability. Specifically, all the intermediate relations are avail-\nable for inspection, allowing systematic error analysis.\nRQ2: Baselines and Comparisons\nWe compare the performance of our solutions to existing\nbaselines under the no-training setting. In particular, our so-\nlutions achieve better performance than comparable base-\nlines on 6 out of 8 studied datasets with baselines. Below,\nwe classify the tasks into 4 categories and discuss the re-\nspective performance and comparisons.\nNatural language reasoning. For the tasks of DR, TSO,\nCLUTRR, and GSM8K, we pick a generic baseline of GPT-\nDR (Date Reasoning)\nToday's meeting is rescheduled to 11 am tomorrow, 10/16/1924.\nWhat is the date one year ago from today in MM/DD/YYYY?Question\nVIEIRA\nExtracted\nRelations\nVIEIRA\nAnswer 10/15/1923\nGPT-4\nResponse\nGPT-4\nAnswer\nLet's think step by step. First, we need to subtract one year from\nthe given date, 10/16/1924. When we do this, we get 10/16/1923.\n10/16/1923\nCLUTRR (Kinship Reasoning)\nMyrna and her husband Christopher went on a cruise. They had\na wonderful time. Christopher and his daughter Lucille took a\nday off school to go to the zoo. Who is Lucille to Myrna? \nContext &\nQuestion\nVIEIRA\nExtracted\nRelations\nVIEIRA\nAnswer daughter\nGPT-4\nResponse\nGPT-4\nAnswer\nLucille is Myrna's stepdaughter. In the given scenario,\nChristopher is Myrna's husband, and Christopher is also the\nfather of Lucille. This makes Lucille the daughter of Myrna's\nhusband, which means she is Myrna's stepdaughter.\nstepdaughter\nGSM8K (Math Reasoning)\nIt takes 3 kangaroos traveling at the same speed a total of 18\nhours to travel across a highway. How many hours will it take 4\nturtles,  each traveling at half the speed of a kangaroo, to do so?\nQuestion\nVIEIRA\nExtracted\nRelations\nVIEIRA\nAnswer 48\nGPT-4\nResponse\nGPT-4\nAnswer\nLet's think step by step. First, we know that the total time it takes for 3 kangaroos\nto travel across the highway is 18 hours. This means that one kangaroo takes 18\nhours to travel across the highway. Then, we know that a turtle travels at half the\nspeed of a kangaroo, which means it takes twice as long for a turtle to travel the\nsame distance. So, one turtle would take 18 hours * 2 = 36 hours to travel across\nthe highway. However, we have four turtles traveling together at the same speed,\nso they would still take 36 hours to travel across the highway, because they are\ntraveling simultaneously.\n36\nFigure 4: Illustrative comparisons between our solution and\nGPT-4 (zero-shot CoT) on selected questions from DR,\nCLUTRR, and GSM8K datasets. We also include the ex-\ntracted relations used for subsequent reasoning.\n4 under zero-shot, few-shot, and chain-of-thought (CoT) set-\ntings. All our solutions also rely on GPT-4 (few-shot), but\nwe note that our shots only include extracted facts, and not\nthe final answer or any reasoning chains. The data in Ta-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10641\n2 4 6 8 10\n25\n50\n75\nk, length of reasoning chain\nAccuracy\n(%)\nOurs GPT-4 GPT-4 (CoT)\n(a) CLUTRR\n3 5 7\n25\n50\n75\n100\nn, number of objects\nAccuracy\n(%)\n(b) TSO\nFigure\n5: Systematic generalizability comparisons on the\nCLUTRR and TSO datasets.\nMethod GQA CLEVR\nRecall@1 Recall@3 Recall@1 Recall@3\nViLT-VQA 0.049 0.462 0.241 0.523\nPNP-VQA 0.419 — — —\nOurs 0.579 0.665 0.463 0.638\nTable 5: Quantitative results on the VQA datasets.\nble 3 indicates that our method can significantly enhance\nreasoning performance and reduce hallucination, exempli-\nfied by achieving a flawless 100% accuracy on the TSO\ndataset. Note that on GSM8K, our method scores slightly\nlower than the baseline; we conjecture that our solution de-\nmands more from GPT-4 itself to extract structured compu-\ntation steps. On CLUTRR, our solution even outperforms\nfCoT (Lyu et al. 2023), a special prompting technique with\nexternal tool use, by 0.6%. In Fig. 5 we illustrate the system-\natic generalizability of our methods. The performance of our\nsolutions remains relatively consistent even when the prob-\nlems become harder. We provide illlustrative examples in\nFig. 4 showing comparisons between our method and GPT-\n4 (zero-shot CoT).\nRetrieval augmentation and semantic search. For the\nHotpotQA dataset, our solution is an adaptation of FE2H\n(Li, Lei, and Yang 2022), a retrieval-augmented question an-\nswering approach. As seen in Table 4, with no fine-tuning,\nour method scores only a few percentages lower than fine-\ntuned methods C2FM (Yin et al. 2022) and FE2H. For\nthe Amazon ESCI dataset, our solution performs seman-\ntic search for product ranking. While performing slightly\nlower than the fine-tuned methods (Reddy et al. 2022; Song\net al. 2020), our solution outperforms maximum inner prod-\nuct search (MIPS) based on GPT text encoder ( text-\nembedding-ada-002).\nCompositional multi-modal reasoning. For VQA, we\npick ViLT-VQA (Kim, Son, and Kim 2021), a pre-trained\nfoundation model ViLT-VQA, and PNP-VQA (Tiong et al.\n2022), a zero-shot VQA method as baselines. As shown in\nTable 5, our method significantly outperforms the baseline\nmodel on both datasets. Compared to the neural-only base-\nline, our approach that combines DSL and logical reasoning\nmore effectively handles intricate logical operations such as\ncounting and numerical comparisons. On GQA, out method\nOurs InstructPix2Pix Original \nInstruction: Replace the bowl with something\nelse, and change the apples to other fruits.\nFigure 6: Qualitative comparison of image editing. Com-\npared to InstructPix2Pix, our image editing method follows\nthe instructed edits better, as it successfully changed the\nbowl into plate and apples to oranges.\nMethod Visual Object Tagging Image Editing\nVQAR OFCP OFCP\nOurs 67.61% 60.82% 74.00%\nTable 6: Quantitative results on object tagging and image\nediting tasks. We manually evaluate the tagged entities and\nthe edited images for semantic correctness rates.\noutperforms previous zero-shot state-of-the-art, PNP-VQA,\nby 0.16 (0.42 to 0.58). For object and face tagging, with-\nout training or fine-tuning, our method achieves 67.61% and\n60.82% semantic correctness rates (Table 6).\nImage generation and editing. For image generation and\nediting, we apply our technique to the OFCP and IGP20\ndatasets. We rely on manual inspection for evaluating our\nperformance on the OFCP dataset, and we observe 37 cor-\nrectly edited images out of the 50 evaluated ones, resulting\nin a 74% semantic correctness rate (Table 6). For IGP20, we\nchoose as the baseline a diffusion model, InstructPix2Pix\n(Brooks, Holynski, and Efros 2023), which also combines\nGPT-3 with image editing. We show one example baseline\ncomparison illustrated in Figure 6.\nConclusion\nWe introduced V IEIRA , a declarative framework designed\nfor relational programming with foundation models. VIEIRA\nbrings together foundation models from diverse domains,\nproviding a unified interface for composition and the abil-\nity to perform probabilistic logical reasoning. This results in\nsolutions with comparable and often superior performance\nthan neural-based baselines. In the future, we aim to extend\nthe capabilities of V IEIRA beyond the current in-context\nlearning settings to weakly-supervised training and fine-\ntuning of foundation models in an end-to-end manner.\nAcknowledgements\nWe thank the anonymous reviewers for useful feedback.\nThis research was supported by NSF grant #2313010 and\nDARPA grant #FA8750-23-C-0080. Ziyang Li was sup-\nported by an Amazon Fellowship.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10642\nReferences\nAbiteboul, S.; Hull, R.; and Vianu, V . 1994.Foundations of\nDatabases: The Logical Level. Pearson, 1st edition.\nAdadi, A. 2021. A survey on data-efficient algorithms in big\ndata era. Journal of Big Data, 8(1): 24.\nBeurer-Kellner, L.; Fischer, M.; and Vechev, M. 2022.\nPrompting Is Programming: A Query Language For Large\nLanguage Models. In PLDI.\nBommasani, R.; Hudson, D. A.; Adeli, E.; Altman, R. B.;\nArora, S.; von Arx, S.; Bernstein, M. S.; Bohg, J.; Bosselut,\nA.; Brunskill, E.; and et al. 2021. On the Opportunities and\nRisks of Foundation Models. arXiv:2108.07258.\nBrooks, T.; Holynski, A.; and Efros, A. A. 2023. Instruct-\nPix2Pix: Learning to Follow Image Editing Instructions.\narXiv:2211.09800.\nBubeck, S.; Chandrasekaran, V .; Eldan, R.; Gehrke, J.;\nHorvitz, E.; Kamar, E.; Lee, P.; Lee, Y . T.; Li, Y .; Lundberg,\nS.; et al. 2023. Sparks of artificial general intelligence: Early\nexperiments with GPT-4. arXiv:2303.12712.\nChen, X.; Liang, C.; Yu, A. W.; Zhou, D.; Song, D.; and\nLe, Q. V . 2020. Neural Symbolic Reader: Scalable Integra-\ntion of Distributed and Symbolic Representations for Read-\ning Comprehension. In ICLR.\nCobbe, K.; Kosaraju, V .; Bavarian, M.; Chen, M.; Jun, H.;\nKaiser, L.; Plappert, M.; Tworek, J.; Hilton, J.; Nakano, R.;\nHesse, C.; and Schulman, J. 2021. Training Verifiers to\nSolve Math Word Problems. arXiv:2110.14168.\nDavis, E.; and Aaronson, S. 2023. Testing GPT-4 with Wol-\nfram Alpha and Code Interpreter plug-ins on math and sci-\nence problems. arXiv:2308.05713.\nGao, L.; Madaan, A.; Zhou, S.; Alon, U.; Liu, P.; Yang, Y .;\nCallan, J.; and Neubig, G. 2023. PAL: Program-aided Lan-\nguage Models. arXiv:2211.10435.\nGupta, T.; and Kembhavi, A. 2022. Visual Program-\nming: Compositional visual reasoning without training.\narXiv:2211.11559.\nHertz, A.; Mokady, R.; Tenenbaum, J.; Aberman, K.; Pritch,\nY .; and Cohen-Or, D. 2022. Prompt-to-Prompt Image Edit-\ning with Cross Attention Control. arXiv:2208.01626.\nHuang, J.; Li, Z.; Chen, B.; Samel, K.; Naik, M.; Song,\nL.; and Si, X. 2021. Scallop: From Probabilistic Deduc-\ntive Databases to Scalable Differentiable Reasoning. In\nNeurIPS.\nHudson, D. A.; and Manning, C. D. 2019. GQA: a new\ndataset for compositional question answering over real-\nworld images. arXiv:1902.09506.\nJohnson, J.; Hariharan, B.; van der Maaten, L.; Fei-Fei, L.;\nZitnick, C. L.; and Girshick, R. B. 2016. CLEVR: A Diag-\nnostic Dataset for Compositional Language and Elementary\nVisual Reasoning. arXiv:1612.06890.\nKim, W.; Son, B.; and Kim, I. 2021. ViLT: Vision-and-\nLanguage Transformer Without Convolution or Region Su-\npervision. arXiv:2102.03334.\nKirillov, A.; Mintun, E.; Ravi, N.; Mao, H.; Rolland, C.;\nGustafson, L.; Xiao, T.; Whitehead, S.; Berg, A. C.; Lo, W.-\nY .; et al. 2023. Segment Anything. arXiv:2304.02643.\nKojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y .; and Iwasawa,\nY . 2022. Large language models are zero-shot reasoners. In\nNeurIPS.\nLi, J.; Wang, Y .; Wang, C.; Tai, Y .; Qian, J.; Yang, J.; Wang,\nC.; Li, J.; and Huang, F. 2018. DSFD: Dual Shot Face De-\ntector. arXiv:1810.10220.\nLi, Q.; Huang, S.; Hong, Y .; Chen, Y .; Wu, Y . N.; and Zhu,\nS.-C. 2020. Closed Loop Neural-Symbolic Learning via\nIntegrating Neural Perception, Grammar Parsing, and Sym-\nbolic Reasoning. In ICML.\nLi, X.-Y .; Lei, W.-J.; and Yang, Y .-B. 2022. From Easy to\nHard: Two-stage Selector and Reader for Multi-hop Ques-\ntion Answering. arXiv:2205.11729.\nLi, Z.; Huang, J.; and Naik, M. 2023. Scallop: A Language\nfor Neurosymbolic Programming. In PLDI.\nLiang, Y .; Wu, C.; Song, T.; Wu, W.; Xia, Y .; Liu, Y .;\nOu, Y .; Lu, S.; Ji, L.; Mao, S.; Wang, Y .; Shou, L.; Gong,\nM.; and Duan, N. 2023. TaskMatrix.AI: Completing Tasks\nby Connecting Foundation Models with Millions of APIs.\narXiv:2303.16434.\nLyu, Q.; Havaldar, S.; Stein, A.; Zhang, L.; Rao, D.; Wong,\nE.; Apidianaki, M.; and Callison-Burch, C. 2023. Faithful\nChain-of-Thought Reasoning. arXiv:2301.13379.\nManhaeve, R.; Dumancic, S.; Kimmig, A.; Demeester, T.;\nand Raedt, L. D. 2018. DeepProbLog: Neural Probabilistic\nLogic Programming. arXiv:1805.10872.\nMao, J.; Gan, C.; Kohli, P.; Tenenbaum, J. B.; and Wu, J.\n2019. The Neuro-Symbolic Concept Learner: Interpreting\nScenes, Words, and Sentences From Natural Supervision.\narXiv:1904.12584.\nMcKenna, N.; Li, T.; Cheng, L.; Hosseini, M. J.; John-\nson, M.; and Steedman, M. 2023. Sources of Hallu-\ncination by Large Language Models on Inference Tasks.\narXiv:2305.14552.\nMinderer, M.; Gritsenko, A.; Stone, A.; Neumann, M.;\nWeissenborn, D.; Dosovitskiy, A.; Mahendran, A.; Arnab,\nA.; Dehghani, M.; Shen, Z.; Wang, X.; Zhai, X.; Kipf, T.;\nand Houlsby, N. 2022. Simple Open-V ocabulary Object De-\ntection with Vision Transformers. arXiv:2205.06230.\nMinervini, P.; Riedel, S.; Stenetorp, P.; Grefenstette, E.; and\nRocktäschel, T. 2020. Learning Reasoning Strategies in\nEnd-to-End Differentiable Proving. In ICML.\nNakano, R.; Hilton, J.; Balaji, S.; Wu, J.; Ouyang, L.;\nKim, C.; Hesse, C.; Jain, S.; Kosaraju, V .; Saunders, W.;\nJiang, X.; Cobbe, K.; Eloundou, T.; Krueger, G.; Button, K.;\nKnight, M.; Chess, B.; and Schulman, J. 2021. WebGPT:\nBrowser-assisted question-answering with human feedback.\narXiv:2112.09332.\nNogueira, R.; and Cho, K. 2019. Passage Re-ranking with\nBERT. arXiv:1901.04085.\nOpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\nKrueger, G.; and Sutskever, I. 2021. Learning Transfer-\nable Visual Models From Natural Language Supervision.\narXiv:2103.00020.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10643\nRajasekharan, A.; Zeng, Y .; Padalkar, P.; and Gupta, G.\n2023. Reliable Natural Language Understanding with Large\nLanguage Models and Answer Set Programming. In Inter-\nnational Conference on Logic Programming.\nRamesh, A.; Pavlov, M.; Goh, G.; Gray, S.; V oss, C.; Rad-\nford, A.; Chen, M.; and Sutskever, I. 2021. Zero-shot text-\nto-image generation. In ICML.\nRatner, N.; Levine, Y .; Belinkov, Y .; Ram, O.; Magar, I.;\nAbend, O.; Karpas, E.; Shashua, A.; Leyton-Brown, K.; and\nShoham, Y . 2023. Parallel Context Windows for Large Lan-\nguage Models. In Proceedings of the ACL.\nReddy, C. K.; Màrquez, L.; Valero, F.; Rao, N.; Zaragoza,\nH.; Bandyopadhyay, S.; Biswas, A.; Xing, A.; and Sub-\nbian, K. 2022. Shopping Queries Dataset: A Large-\nScale ESCI Benchmark for Improving Product Search.\narXiv:2206.06588.\nRichards, T. B. 2023. AutoGPT. https://github.com/\nSignificant-Gravitas/AutoGPT. Accessed: 2024-02-12.\nRombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Om-\nmer, B. 2022. High-Resolution Image Synthesis With Latent\nDiffusion Models. In CVPR.\nSchick, T.; Dwivedi-Yu, J.; Dessì, R.; Raileanu, R.; Lomeli,\nM.; Zettlemoyer, L.; Cancedda, N.; and Scialom, T. 2023.\nToolformer: Language Models Can Teach Themselves to\nUse Tools. arXiv:2302.04761.\nSinha, K.; Sodhani, S.; Dong, J.; Pineau, J.; and Hamilton,\nW. L. 2019. CLUTRR: A Diagnostic Benchmark for Induc-\ntive Reasoning from Text. arXiv:1908.06177.\nSong, K.; Tan, X.; Qin, T.; Lu, J.; and Liu, T.-Y . 2020. MP-\nNet: Masked and Permuted Pre-training for Language Un-\nderstanding. arXiv:2004.09297.\nSrivastava, A.; Rastogi, A.; Rao, A.; Shoeb, A. A. M.; Abid,\nA.; Fisch, A.; Brown, A. R.; Santoro, A.; Gupta, A.; Garriga-\nAlonso, A.; and et al. 2023. Beyond the Imitation Game:\nQuantifying and extrapolating the capabilities of language\nmodels. arXiv:2206.04615.\nTiong, A. M. H.; Li, J.; Li, B.; Savarese, S.; and Hoi, S. C.\n2022. Plug-and-Play VQA: Zero-shot VQA by Conjoining\nLarge Pretrained Models with Zero Training. In Goldberg,\nY .; Kozareva, Z.; and Zhang, Y ., eds.,Findings of the ACL:\nEMNLP.\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;\nBabaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,\nS.; et al. 2023. Llama 2: Open Foundation and Fine-Tuned\nChat Models. arXiv:2307.09288.\nWang, P.-W.; Donti, P. L.; Wilder, B.; and Kolter, Z. 2019.\nSATNet: Bridging Deep Learning and Logical Reasoning\nUsing a Differentiable Satisfiability Solver. In ICML.\nWang, X.; Wei, J.; Schuurmans, D.; Le, Q.; Chi, E.; Narang,\nS.; Chowdhery, A.; and Zhou, D. 2023. Self-Consistency\nImproves Chain of Thought Reasoning in Language Models.\narXiv:2203.11171.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Ichter, B.;\nXia, F.; Chi, E.; Le, Q.; and Zhou, D. 2023. Chain-of-\nThought Prompting Elicits Reasoning in Large Language\nModels. arXiv:2201.11903.\nXu, Z.; Rawat, Y . S.; Wong, Y .; Kankanhalli, M.; and Shah,\nM. 2022. Don’t Pour Cereal into Coffee: Differentiable\nTemporal Logic for Temporal Action Segmentation. In\nNeurIPS.\nYang, Z.; Qi, P.; Zhang, S.; Bengio, Y .; Cohen, W. W.;\nSalakhutdinov, R.; and Manning, C. D. 2018. HotpotQA: A\ndataset for diverse, explainable multi-hop question answer-\ning. arXiv:1809.09600.\nYao, S.; Zhao, J.; Yu, D.; Du, N.; Shafran, I.; Narasimhan,\nK.; and Cao, Y . 2023. ReAct: Synergizing Reasoning and\nActing in Language Models. arXiv:2210.03629.\nYi, K.; Wu, J.; Gan, C.; Torralba, A.; Kohli, P.; and Tenen-\nbaum, J. 2018. Neural-Symbolic VQA: Disentangling\nReasoning from Vision and Language Understanding. In\nNeurIPS.\nYin, Z.; Wang, Y .; Wu, Y .; Yan, H.; Hu, X.; Zhang, X.; Cao,\nZ.; Huang, X.; and Qiu, X. 2022. Rethinking Label Smooth-\ning on Multi-hop Question Answering. arXiv:2212.09512.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10644",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7981244325637817
    },
    {
      "name": "Compiler",
      "score": 0.5775774717330933
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5553325414657593
    },
    {
      "name": "Plug-in",
      "score": 0.49640899896621704
    },
    {
      "name": "Programming language",
      "score": 0.48048415780067444
    },
    {
      "name": "Stateless protocol",
      "score": 0.44829463958740234
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4173540472984314
    },
    {
      "name": "Software engineering",
      "score": 0.3387317657470703
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "State (computer science)",
      "score": 0.0
    }
  ]
}