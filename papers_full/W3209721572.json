{
    "title": "Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey",
    "url": "https://openalex.org/W3209721572",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2374505395",
            "name": "Min, Bonan",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Ross, Hayley",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4293323087",
            "name": "Sulem, Elior",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221378411",
            "name": "Veyseh, Amir Pouran Ben",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3176157683",
            "name": "Nguyen, Thien Huu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4223316880",
            "name": "Sainz, Oscar",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2744374220",
            "name": "Agirre, Eneko",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Heinz, Ilana",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2746559556",
            "name": "Roth Dan",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3163016119",
        "https://openalex.org/W2963970792",
        "https://openalex.org/W2984354699",
        "https://openalex.org/W3118781290",
        "https://openalex.org/W3103469330",
        "https://openalex.org/W2950813464",
        "https://openalex.org/W3206996280",
        "https://openalex.org/W3113303810",
        "https://openalex.org/W2917128112",
        "https://openalex.org/W3093830302",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2911300548",
        "https://openalex.org/W2963323070",
        "https://openalex.org/W2969740599",
        "https://openalex.org/W3104163040",
        "https://openalex.org/W3092288641",
        "https://openalex.org/W2996580882",
        "https://openalex.org/W2970853769",
        "https://openalex.org/W2962911926",
        "https://openalex.org/W3203279312",
        "https://openalex.org/W3176951816",
        "https://openalex.org/W2915128308",
        "https://openalex.org/W2953307569",
        "https://openalex.org/W3214897310",
        "https://openalex.org/W3160638507",
        "https://openalex.org/W3154200459",
        "https://openalex.org/W3119175506",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3152497014",
        "https://openalex.org/W2475245295",
        "https://openalex.org/W3032532958",
        "https://openalex.org/W3118364895",
        "https://openalex.org/W3153427360",
        "https://openalex.org/W2983040767",
        "https://openalex.org/W3113057009",
        "https://openalex.org/W3172642864",
        "https://openalex.org/W3155979791",
        "https://openalex.org/W2982756474",
        "https://openalex.org/W2997763445",
        "https://openalex.org/W2507756961",
        "https://openalex.org/W2964022985",
        "https://openalex.org/W3176253974",
        "https://openalex.org/W2982399380",
        "https://openalex.org/W3091984855",
        "https://openalex.org/W3034937228",
        "https://openalex.org/W3156470785",
        "https://openalex.org/W3155183406",
        "https://openalex.org/W3042266831",
        "https://openalex.org/W3111372685",
        "https://openalex.org/W3169130206",
        "https://openalex.org/W3102749280",
        "https://openalex.org/W3194309076",
        "https://openalex.org/W3116103312",
        "https://openalex.org/W1599016936",
        "https://openalex.org/W2995335514",
        "https://openalex.org/W3198002980",
        "https://openalex.org/W2963101081",
        "https://openalex.org/W3021526287",
        "https://openalex.org/W2919115771",
        "https://openalex.org/W3119438769",
        "https://openalex.org/W3116459227",
        "https://openalex.org/W3102925419",
        "https://openalex.org/W2950339735",
        "https://openalex.org/W3174863418",
        "https://openalex.org/W3104588838",
        "https://openalex.org/W3170790933",
        "https://openalex.org/W2946676565",
        "https://openalex.org/W3134354193",
        "https://openalex.org/W3019779721",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W3153805297",
        "https://openalex.org/W3001434439",
        "https://openalex.org/W2998901379",
        "https://openalex.org/W2952826823",
        "https://openalex.org/W2969605360",
        "https://openalex.org/W2805206884",
        "https://openalex.org/W3157374291",
        "https://openalex.org/W3164972323",
        "https://openalex.org/W3201174429",
        "https://openalex.org/W3093166897",
        "https://openalex.org/W2998230451",
        "https://openalex.org/W3099793224",
        "https://openalex.org/W3032099552",
        "https://openalex.org/W1840435438",
        "https://openalex.org/W3127069001",
        "https://openalex.org/W2963609017",
        "https://openalex.org/W3101246020",
        "https://openalex.org/W2947412852",
        "https://openalex.org/W2117130368",
        "https://openalex.org/W3169934659",
        "https://openalex.org/W2985808369",
        "https://openalex.org/W3177813494",
        "https://openalex.org/W3194836374",
        "https://openalex.org/W2154474435",
        "https://openalex.org/W2252123671",
        "https://openalex.org/W2945824677",
        "https://openalex.org/W2949922292",
        "https://openalex.org/W3023237241",
        "https://openalex.org/W3187134297",
        "https://openalex.org/W2963246595",
        "https://openalex.org/W3176456866",
        "https://openalex.org/W3004346089",
        "https://openalex.org/W3132736064",
        "https://openalex.org/W3122241445",
        "https://openalex.org/W3006381853",
        "https://openalex.org/W2962881743",
        "https://openalex.org/W2910243263",
        "https://openalex.org/W3176828726",
        "https://openalex.org/W3154903254",
        "https://openalex.org/W3154461875",
        "https://openalex.org/W2975059944",
        "https://openalex.org/W3173586048",
        "https://openalex.org/W2976444281",
        "https://openalex.org/W2995647371",
        "https://openalex.org/W2963206679",
        "https://openalex.org/W2164343063",
        "https://openalex.org/W2937845937",
        "https://openalex.org/W3089105501",
        "https://openalex.org/W3156012351",
        "https://openalex.org/W3185341429",
        "https://openalex.org/W3035169973",
        "https://openalex.org/W3021016503",
        "https://openalex.org/W3154147337",
        "https://openalex.org/W2984147501",
        "https://openalex.org/W3035488502",
        "https://openalex.org/W3008282111",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W2963087868",
        "https://openalex.org/W3034408878",
        "https://openalex.org/W3198599617",
        "https://openalex.org/W2962800603",
        "https://openalex.org/W3041594829",
        "https://openalex.org/W2949849869",
        "https://openalex.org/W3035556416",
        "https://openalex.org/W2936215830",
        "https://openalex.org/W3173777717",
        "https://openalex.org/W2282821441",
        "https://openalex.org/W3094573908",
        "https://openalex.org/W3034336785",
        "https://openalex.org/W3212496002",
        "https://openalex.org/W2971600926",
        "https://openalex.org/W2988252747",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3171388604",
        "https://openalex.org/W2093390569",
        "https://openalex.org/W3129289122",
        "https://openalex.org/W2940152587",
        "https://openalex.org/W2147880316",
        "https://openalex.org/W3199958362",
        "https://openalex.org/W3093713068",
        "https://openalex.org/W3102085674",
        "https://openalex.org/W3133101440",
        "https://openalex.org/W3001279689",
        "https://openalex.org/W3139080614",
        "https://openalex.org/W3174432206",
        "https://openalex.org/W3176793246",
        "https://openalex.org/W3209051700",
        "https://openalex.org/W2510153535",
        "https://openalex.org/W3182414949",
        "https://openalex.org/W3173617765",
        "https://openalex.org/W3034861927",
        "https://openalex.org/W3169948074",
        "https://openalex.org/W2947915358",
        "https://openalex.org/W3176690085",
        "https://openalex.org/W3097392354",
        "https://openalex.org/W2955403921",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W3092143352",
        "https://openalex.org/W2963365341",
        "https://openalex.org/W3035628711",
        "https://openalex.org/W3122175177",
        "https://openalex.org/W3165416482",
        "https://openalex.org/W3154863804",
        "https://openalex.org/W3008088841",
        "https://openalex.org/W3104890489",
        "https://openalex.org/W2972324944",
        "https://openalex.org/W3034556525",
        "https://openalex.org/W2068905009",
        "https://openalex.org/W3034862440",
        "https://openalex.org/W3174870841",
        "https://openalex.org/W2990188683",
        "https://openalex.org/W3177174258"
    ],
    "abstract": "Large, pre-trained transformer-based language models such as BERT have drastically changed the Natural Language Processing (NLP) field. We present a survey of recent work that uses these large language models to solve NLP tasks via pre-training then fine-tuning, prompting, or text generation approaches. We also present approaches that use pre-trained language models to generate data for training augmentation or other purposes. We conclude with discussions on limitations and suggested directions for future research.",
    "full_text": "Recent Advances in Natural Language Processing via Large Pre-Trained\nLanguage Models: A Survey\nBonan Min*1, Hayley Ross*2, Elior Sulem*3, Amir Pouran Ben Veyseh*4,\nThien Huu Nguyen4, Oscar Sainz5, Eneko Agirre5, Ilana Heinz1, and Dan Roth3\n1Raytheon BBN Technologies\n{bonan.min, ilana.Heintz}@raytheon.com\n2Harvard University\nhayleyross@g.harvard.edu\n3University of Pennsylvania\n{eliors, danroth}@seas.upenn.edu\n4University of Oregon\n{apouran, thien}@cs.uoregon.edu\n5University of the Basque Country (UPV/EHU)\n{oscar.sainz, e.agirre}@ehu.eus\n* indicate equal contribution\nAbstract\nLarge, pre-trained transformer-based language\nmodels such as BERT have drastically\nchanged the Natural Language Processing\n(NLP) ﬁeld. We present a survey of re-\ncent work that uses these large language mod-\nels to solve NLP tasks via pre-training then\nﬁne-tuning, prompting, or text generation ap-\nproaches. We also present approaches that use\npre-trained language models to generate data\nfor training augmentation or other purposes.\nWe conclude with discussions on limitations\nand suggested directions for future research.\n1 Introduction\nIn recent years, large pre-trained transformer-based\nlanguage models (PLMs), such as the BERT (De-\nvlin et al., 2019) and GPT (Radford et al., 2018)\nfamilies of models, have taken Natural Language\nProcessing (NLP) by storm, achieving state-of-the-\nart performance on many tasks.\nThese large PLMs have fueled a paradigm shift\nin NLP. Take a classiﬁcation task p(y|x) (classi-\nfying textual input x into a label y) as an exam-\nple: traditional statistical NLP approaches often\ndesign hand-crafted features to represent x, and\nthen apply a machine learning model (e.g. SVM\n(Cortes and Vapnik, 1995), logistic regression) to\nlearn the classiﬁcation function. Deep learning\nmodels learn the latent feature representation via\na deep neural network (LeCun et al., 2015) in ad-\ndition to the classiﬁcation function. Note that the\nlatent representation needs to be learned afresh for\neach new NLP task, and that, in many cases, the\nsize of the training data limits the quality of the\nlatent feature representation. Given that the nu-\nances of language are common to all NLP tasks,\none could posit that we could learn a generic la-\ntent feature representations from some generic task\nonce, and then share it across all NLP tasks. Lan-\nguage modeling, where the model needs to learn\nhow to predict the next word given previous words,\nis such a generic task with abundant naturally oc-\ncurring text to pre-train such a model (hence the\nname pre-trained language models). In fact, the lat-\nest, ongoing paradigm shift begins when PLMs are\nintroduced: for numerous NLP tasks, researchers\nnow leverage existing PLMs via ﬁne-tuning for the\ntask of interest, prompting the PLMs to perform the\ndesired task, or reformulating the task as a text gen-\neration problem with application of PLMs to solve\nit accordingly. Advances in these three PLM-based\nparadigms have continuously established new state-\nof-the-art performances.\nThis paper surveys recent works that leverage\nPLMs for NLP. We organize these works into the\nfollowing three paradigms:\n• Pre-train then ﬁne-tune (§ 2): perform general-\npurpose pre-training with a large unlabeled\n1\narXiv:2111.01243v1  [cs.CL]  1 Nov 2021\ncorpus, and then perform a small amount of\ntask-speciﬁc ﬁne-tuning for the task of inter-\nest.\n• Prompt-based learning (§ 3): prompt a PLM\nsuch that solving an NLP task is reduced to\na task similar to the PLM’s pre-training task\n(e.g. predicting a missing word), or a simpler\nproxy task (e.g. textual entailment). Prompt-\ning can usually more effectively leverage the\nknowledge encoded in the PLMs, leading to\nfew-shot approaches.\n• NLP as text generation ( § 4): Reformulate\nNLP tasks as text generation, to fully leverage\nknowledge encoded in a generative language\nmodel such as GPT-2 (Radford et al., 2019)\nand T5 (Raffel et al., 2020).\nGenerative PLMs can be also used for text gen-\neration tasks. We refer readers to the excellent\nsurveys on text generation such as Li et al. (2021b)\nand Yu et al. (2021b). This paper, unless otherwise\nspeciﬁed, focuses on tasks that are not generative\nin nature (e.g. classiﬁcation, sequence labeling and\nstructure prediction) that still cover a broad range\nof NLP tasks including syntactic or semantic pars-\ning of text, Information Extraction (IE), Question\nAnswering (QA), Textual Entailment (TE), senti-\nment analysis, and so on.\nIn addition to the three paradigms, there is an-\nother, complementary method: to indirectly use any\nof the PLM paradigms above to improve results of\ntarget NLP tasks:\n• Data generation (§ 5): run PLMs to automat-\nically generate data for NLP tasks. The gen-\nerated data can be silver labeled data, where\ntypically the generative PLM is ﬁne-tuned for\nthe task, or some auxiliary data, such as coun-\nterexamples, clariﬁcations, contexts, or other.\nIn the ﬁrst case, the silver labeled data can be\nadded to existing labeled data. In the second\ncase, the auxiliary data supports the target task\nin some way.\nThe paper is organized as follows: Section 2\nprovides background on the PLMs and describes\nthe ﬁrst paradigm, pre-train then ﬁne-tune. Sec-\ntion 3 discusses the second paradigm, prompt-\nbased learning. Section 4 summarizes works in\nthe third paradigm, NLP as text generation. In Sec-\ntion 5, we describe approaches that generate data\nvia PLMs for a broad range of NLP tasks. We dis-\ncuss limitations and provide directions for future\nresearch in Section 6 and conclude in Section 7.\n2 Paradigm 1: Pre-Train then Fine-Tune\nWhile work in traditional statistical NLP focused\non training task-speciﬁc models on labeled datasets,\nthis paradigm shifts to training one large model\non a shared, “fundamental” pre-training task and\nthen adapting (“ﬁne-tuning”) it to a variety of tasks\nin a second step. The pre-training task is almost\ninvariably a type of language modeling task1 that\ncan leverage a massive quantity of unlabelled data\nto learn representations that beneﬁt a range of NLP\ntasks (Rogers et al., 2020).\nIn this section, we ﬁrst provide a primer on\npre-trained large language models (PLMs), then\ndescribe approaches that use frozen or ﬁne-tuned\nPLMs for NLP tasks.\n2.1 The Beginnings of the Paradigm Shift\nWhile pre-training in machine learning and, in par-\nticular, computer vision has been studied since at\nleast 2010 (Erhan et al., 2010; Yosinski et al., 2014;\nHuh et al., 2016), the technique did not gain trac-\ntion in NLP until later in the decade, with the publi-\ncation of Vaswani et al. (2017). The delay in uptake\nis partly due to the later arrival of deep neural mod-\nels to NLP compared to computer vision, partly due\nto the difﬁculty of choosing a self-supervised task2\nsuitable for pre-training, and above all, due to the\nneed for drastically larger model sizes and corpora\nin order to be effective for NLP tasks. We explore\nthese aspects further in the discussion below.\nThe idea of pre-training on a language model-\ning task is quite old. Collobert and Weston (2008)\nﬁrst suggested pre-training a model on a number\nof tasks to learn features instead of hand-crafting\nthem (the predominant approach at the time). Their\nversion of language model pre-training, however,\ndiffered signiﬁcantly from the methods we see to-\nday. They used language modeling as only one of\nmany tasks in a multitask learning setting, along\nwith other supervised tasks such as part-of-speech\n(POS) tagging, named entity recognition (NER)\n1The exact formulation varies from the classic unidirec-\ntional language modeling (next word prediction) to cloze-style\nﬁll-in-the-blank, uncorrupting spans, and other variants (see\nSection 2.3).\n2In self-supervised learning, the ground truth (e.g. the\nmissing word) comes from the unlabeled text itself. This al-\nlows the pre-training to scale up with the near-inﬁnite amount\nof text available on the web.\n2\nFigure 1: Three types of pre-trained language models. Model architecture illustrations are from Lewis et al.\n(2020). For the encoder-decoder model, the corruption strategy of document rotation is shown. Alternatives\ninclude sentence permutation, text inﬁlling, token deletion/masking, etc.\nand semantic role labeling (SRL). Collobert and\nWeston proposed sharing the weights of their deep-\nest convolutional layer – the word embeddings\nlearned by the model – between the multiple train-\ning tasks and ﬁne-tuning the weights of the two\nremaining two feed-forward layers for each indi-\nvidual task.\nPre-training and ﬁne-tuning did not gain popular-\nity in NLP until the advent of ELMo (Peters et al.,\n2018) and ULMFiT (Howard and Ruder, 2018).\nBoth models are based on Long Short-Term Mem-\nory architecture (LSTMs) (Hochreiter and Schmid-\nhuber, 1997), but differ in signiﬁcant ways. ULM-\nFiT pre-trains a three-layer LSTM on a standard\nlanguage modeling objective, predicting the next\ntoken in a sequence. ELMo uses layers of bidirec-\ntional LSTMs that combine two language model\ntasks in forward and backward directions to capture\ncontext from both sides. Both proposed ﬁne-tuning\nthe language model layer by layer for downstream\napplication. Both studies also suggested adding\nadditional classiﬁer layers on top of the language\nmodel, which were ﬁne-tuned alongside the lan-\nguage model layers. These changes, combined\nwith the substantially larger model size and pre-\ntraining corpus size compared to previous models,\nallowed the pre-training then ﬁne-tuning paradigm\nto succeed. Both ELMo and ULMFiT showed com-\npetitive or improved performance compared to the\nthen-state-of-the-art for a number of tasks, demon-\nstrating the value of language model pre-training\non a large scale.\nThe pace of this paradigm shift picked up dra-\nmatically in late 2018 when Vaswani et al. (2017)\nintroduced the Transformer architecture that can be\nused for language model pre-training. The Trans-\nformer’s multi-head self-attention mechanism al-\nlows every word to attend to all previous words or\nevery word except the target, allowing the model to\nefﬁciently capture long-range dependencies with-\nout the expensive recurrent computation in LSTMs.\nMultiple layers of multi-head self-attention allow\nfor increasingly more expressive representations,\nuseful for a range of NLP problems. As a result,\nnearly all popular language models, including GPT,\nBERT, BART (Lewis et al., 2020) and T5 (Raffel\net al., 2020), are now based on the Transformer\narchitecture. They also differ in a number of im-\nportant ways, which we discuss in the following\nsections. For more details about the Transformer ar-\nchitecture, we refer the reader to the original paper\nor to the excellent tutorials available3,4.\n2.2 Modern Pre-Trained Language Models\nThere are three classes of pre-trained language\nmodels: autoregressive language models (e.g.\nGPT), masked language models (e.g. BERT), and\nencoder-decoder models (e.g. BART, T5). Fig-\nure 1 shows the difference in model architecture\nand training objectives with an example training\ninput for each.\n2.2.1 Autoregressive Language Models\nAn autoregressive language model is trained\nto predict the next word xi given all previ-\n3http://nlp.seas.harvard.edu/2018/04/\n03/attention.html\n4http://jalammar.github.io/\nillustrated-transformer/\n3\nModel Pre-Training SourcesSize of Pre-Training Corpus# Model parameters\n(1)English Monolingual Models\nBERT(BASE)(Devlin et al., 2019) Wiki, books 3.3B tokens (13GB data) 110M\nBERT(LARGE)(DEVLIN ET AL., 2019) Wiki, books 3.3B tokens (13GB data) 340M\nROBERTA(Liu et al., 2019) Wiki, books, web crawl 161GB data 340M\nXLNET(Yang et al., 2019) Wiki, books, web crawl 142GB data 340M\nGPT(Radford et al., 2018) Web crawl 800M tokens 117M\nGPT-2(Radford et al., 2019) Web crawl 8M documents (40GB data) 1.5B\nGPT-3(Brown et al., 2020) Wiki, books, web crawl 500B tokens 175B\nBART (Lewis et al., 2020) Wiki, books 3.3B tokens ∼370M\nT5 (Raffel et al., 2020) Web crawl 200B tokens (750GB data) 11B\n(2)Multilingual Models\nMBERT(Devlin et al., 2019) Wiki 21.9B tokens 172M\nXLM-R(BASE) (Conneau et al., 2020) Web crawl 295B tokens 270M\nXLM-R(LARGE) (Conneau et al., 2020) Web crawl 295B tokens 550M\nMT5 (LARGE) (Raffel et al., 2020) Web crawl 6.3T tokens 1.2B\nMT5 (XXL) (Raffel et al., 2020) Web crawl 6.3T tokens 13B\nTable 1: Training sources, dataset size, and model parameters for popular PLMs. Data sources differ, and are\ndescribed in the citations listed in each row.\nous words x1, x2, ..., and xi−1. The train-\ning objective is to maximize the log-likelihood∑\ni log(P(xi|x1,x2,...,x i−1); θT ), in which θT\nare the model parameters. In a Transformer de-\ncoder, these are in multiple layers of multi-head\nself-attention modules. Typical models include\nGPT (Radford et al., 2018), GPT-2 (Radford et al.,\n2019) and GPT-3 (Brown et al., 2020)5.\nGPT only utilizes the autoregressive decoder\nportion of the Transformer architecture, stacking\nmultiple transformer decoder layers with masked\nself-attention. This allows the model to attend to\nall previous tokens in the sequence when predict-\ning the next token. Each newer version of GPT\nis trained with increasingly large amounts of text\n(Table 1).\nThe GPT paper (Radford et al., 2018) proposed\nﬁne-tuning GPT for speciﬁc tasks, providing exam-\nples for natural language inference, QA (including\ncommonsense reasoning), semantic similarity and\nparaphrase detection, sentiment analysis, and lin-\nguistic acceptability (CoLA, Warstadt et al., 2019),\nas well as the GLUE benchmark. In particular,\nGPT achieves a dramatic improvement on CoLA\n(scoring 45.4 compared to the previous state of\nthe art of 35.0), showcasing the model’s ability to\ngain a much more sophisticated grasp of language\nthan previous models. Subsequent versions of GPT\n(GPT-2 and GPT-3, Radford et al., 2019; Brown\n5Open-source re-implementations of GPT are also avail-\nable, such as GPT-Neo (Black et al., 2021) and GPT-J (Wang,\n2021), trained on an 800GB open-source dataset (Gao et al.,\n2020a), with model sizes similar to GPT-2 (2.7B and 6B pa-\nrameters respectively).\net al., 2020), however, do not opt for the ﬁne-tuning\napproach and instead leverage GPT’s generative de-\nsign to tackle tasks in a prompt-based manner or\nvia outright language generation, as described in\nSections 3 and 4.\n2.2.2 Masked Language Models\nWhereas autoregressive models are unidirectional,\nmasked language models (MLMs), predict a\n“masked” word conditioned on all other words\nin the sequence. When training an MLM,\nwords are chosen at random to be masked, us-\ning a special token [MASK], or replaced by\na random token. This forces the model to\ncollect bidirectional information in making pre-\ndictions. The training objective is to recover\nthe original tokens at the masked positions:∑\ni milog(P(xi|x1,...,x i−1,xi+1,...,x n); θT ), in\nwhich mi ∈{0,1}indicates whether xi is masked\nor not, and θT are the parameters in a Transformer\nencoder. Note that in BERT and similar models,\nit is a common practice to mask multiple words\nfrom a sequence to allow parallel training. Popular\nexamples of MLMs include BERT (Devlin et al.,\n2019), RoBERTa (Liu et al., 2019), and XLM-R\n(Conneau et al., 2020).\nSpeciﬁcally, MLMs such as BERT use the en-\ncoder portion of the Transformer architecture. Like\nautoregressive models, MLMs stack multiple trans-\nformer encoder layers to learn increasingly com-\nplex and meaningful representations, but it uses\nmasked self-attention to attend to all other tokens\nin the sequence in both directions when learning\na representation for a particular token. The non-\n4\nautoregressive nature allows the computation to be\nparallelized, so it is often more efﬁcient at infer-\nence time. Dynamic unfolding of all positions in\nrelation to the masked word provides efﬁciency at\ntraining time.\nThere is a large family of models derived\nfrom BERT, including RoBERTa (Liu et al.,\n2019), which improves BERT’s pre-training, AL-\nBERT (Lan et al., 2020), which is smaller and\nfaster to train, and XLNet (Yang et al., 2019) and\nTransformer-XL (Dai et al., 2019), which incor-\nporate an autoregressive pre-training approach to\nbetter handle long-distance dependencies. There\nare also a range of derived models trained on spe-\nciﬁc domains (Table 6 in Appendix A). See Qiu\net al. (2020) for a full taxonomy of BERT-derived\nmodels.\n2.2.3 Encoder-Decoder Language Models\nThe encoder-decoder model is a more ﬂexible “text\nin, text out” model that learns to generate a se-\nquence of token y1,...,y n given an input sequence\nx1,...,x m. Given a pair of sequences, the train-\ning objective is to maximize the log-likelihood of\nlog(P(y1,...,y n|x1,...,x m); θT ), in which θT are\nthe parameters in a full encoder-decoder Trans-\nformer model (Vaswani et al., 2017).\nTo generate adequate data for self-supervised\npre-training, researchers experiment with different\nforms of sequence corruption. The input is a token\nsequence modiﬁed in some particular way, and\nthe output is the reconstructed, original sequence.\nForms of sequence corruption include document\nrotation, shown in Figure 1, sentence permutation,\ntext inﬁlling, token deletion/masking, and others.\nRepresentative models include BART (Lewis et al.,\n2020) and T5 (Raffel et al., 2020).\nGiven the sequence-to-sequence (seq2seq) na-\nture, it is straightforward to ﬁne-tune the encoder-\ndecoder language model to perform seq2seq tasks\nsuch as Machine Translation, style transfer, and\ntext summarization. The seq2seq formulation is\nalso versatile: many tasks can be reformulated as\n“text in, text out”. We describe those approaches in\ndetails in Section 4.\n2.3 Pre-Training Corpora\nThe pre-training corpus is a primary distinguishing\nfactor between language models. Both the size and\nthe quality (source data characteristics) are impor-\ntant considerations. Table 1 presents the sources\nand the corpus size used for several popular lan-\nguage models. There is a clear trend of increasing\nthe size of the pre-training corpus as well as in-\ncreasing the diversity of the data. For example,\nULMFiT (Howard and Ruder, 2018) is trained on\na small, highly pre-processed corpus of ∼29,000\nWikipedia articles (103 million words), and is rep-\nresentative of models of that year. A few years later,\nmodels such as XLM-R (Conneau et al., 2020) and\nGPT-3 (Brown et al., 2020) leveraged billions of\nwords of crawled web data (diverse in nature). Raf-\nfel et al. (2020) observe that the primary gains in\nperformance are typically driven by model size and\ndataset size (“the bigger, the better”), if the qual-\nity of the dataset is held constant. They ﬁnd that\nquality can play a larger role if there is a genre\nmatch to the task, but a larger dataset provides\nmore advantages, eventually overcoming any gain\nfrom quality. For a detailed discussion of model\nperformance scaling by model size, dataset size,\nand other factors, see Kaplan et al. (2020). De-\nspite the advantages of the larger dataset, Raffel\net al. (2020) also demonstrate the importance of\ncleaning large crawled datasets. They show that\na model trained on such an unﬁltered dataset per-\nforms substantially worse than if ﬁltering heuristics\nare applied. Similarly, GPT-2 (Radford et al., 2019)\nand GPT-3 (Brown et al., 2020) use heuristics to\nimprove the quality of the training data. However,\nHendrycks et al. (2020) noted that larger models do\nnot necessarily perform better out of domain. Lin\net al. (2021) also observe that larger language mod-\nels (trained on these very diverse sources) are more\nlikely to incorrectly answer questions that some hu-\nmans would answer incorrectly due to false beliefs\nor misconceptions, thus mimicking the inaccura-\ncies in their training data.\nThe domain of intended downstream applica-\ntions is an important consideration for pre-training\nsource data selection. Table 6 (Appendix A) pro-\nvides a list of domain-speciﬁc pre-trained language\nmodels that achieved signiﬁcantly better perfor-\nmance in the intended domain than general-purpose\nlanguage models. These models are either trained\nfrom scratch or trained with domain-speciﬁc text\nusing a general-purpose model as the initialization.\n2.4 Fine-Tuning: Applying PLMs to NLP\nTasks\nHaving described the various approaches to creat-\ning complex, meaningful representations through\npre-training, we turn to the ﬁne-tuning step that\n5\nFigure 2: Typical “pre-train then ﬁne-tune” strategies. We illustrate strategies that ﬁne-tune the full PLM (left),\nﬁne-tune the full PLM in a custom model (center), and ﬁne-tune just a small adapter sub-layer per each Transformer\nlayer (right). We show the Transformer blocks that will be ﬁne-tuned for the speciﬁc tasks in blue, and the frozen\nblocks (keep the pre-trained weights unchanged) in grey. For brevity, we represent the entire Transformer block\n(stacked in nlayers) by its multi-head self-attention and (if applicable) adapter layers. We refer interested readers\nto Vaswani et al., 2017 and Pfeiffer et al., 2020a for more architecture details. “Heads” refers to task-speciﬁc\nprediction functions (Wolf et al., 2020).\nallows PLMs to perform accurately on disparate\nNLP tasks. Figure 2 illustrates typical pre-training\nthen ﬁne-tuning strategies. We describe each of\nthem below. A more comprehensive list of prior\nwork using different pre-training then ﬁne-tuning\nstrategies are in Table 8 (Appendix B).\n2.4.1 Contextual Embeddings\nThe simplest approach to using large pre-trained\nlanguage models is to “freeze” the model and use\nits output as sophisticated, context-sensitive word\nembeddings for a subsequent architecture, which\nis trained from scratch for the speciﬁc task. In\nother words, while this still involves a forward pass\nthrough the pre-trained language model over the\ninput text, the language model’s weights are not\nﬁne-tuned, rendering this approach closer to a fea-\nture extraction family of approaches in classic sta-\ntistical NLP. There are three types of scenarios for\nusing frozen PLMs.\nIn contexts with insufﬁcient labeled data or com-\npute power, “frozen” contextual embeddings are\nemployed. For non-benchmark tasks, the only la-\nbeled training datasets are too small to ﬁne-tune\neven the top layers of BERT-base, let alone larger\nmodels. The computational cost of ﬁne-tuning the\nentire PLM may be prohibitive for some applica-\ntions or developers, leading to use of the more\nefﬁcient frozen PLM solution. Other data-efﬁcient\nand time-efﬁcient approaches to ﬁne-tuning are dis-\ncussed in Section 2.4.4.\nHighly complex or difﬁcult NLP tasks often\nmake use of the frozen PLM technique to help\nreduce training complexity. Examples are con-\nstituency parsing (Zhang et al., 2020c), semantic\ngraph parsing using UCCA 6 (Jiang et al., 2019)\nand AMR7 (Zhang et al., 2019b; Naseem et al.,\n2019; Zhou et al., 2020b), Aspect-Based Sentiment\nAnalysis (Li et al., 2019b) and Machine Transla-\ntion (Zhu et al., 2020). For instance, Zhang et al.\n(2020c) uses frozen BERT embeddings to seed an\ninnovative approach to Conditional Random Field\n(CRF) modeling (Lafferty et al., 2001) that replaces\nthe inside-outside algorithm with backpropagation,\nusing a two-step process to ﬁrst bracket then label\nthe parses, and a batched version of the CKY al-\ngorithm. For complex tasks like these, there may\nonly be enough data or compute power available\nto train the secondary model (Zhang et al. (2019b)\ncited limitations in compute power). While the use\nof frozen PLM parameters is currently in vogue for\nthese tasks, perhaps due to researcher preference\nfor simplicity as well as computational require-\nments, we may see a shift to full-model ﬁne-tuning\nfor tasks with sufﬁcient training data.\nUnsupervised tasks such as word sense disam-\nbiguation (Hadiwinoto et al., 2019) and word sense\ninduction (Amrami and Goldberg, 2019) are not\nassociated with a supervised dataset for ﬁne-tuning.\nInstead, frozen BERT embeddings are fed through\na variety of strategies such as nearest-neighbour\n6Universal Conceptual Cognitive Annotation (Abend and\nRappoport, 2013)\n7Abstract Meaning Representation (Banarescu et al., 2013)\n6\nmatching, afﬁne transformations, gated linear units\n(GLU, Dauphin et al., 2017) or clustering algo-\nrithms to perform these tasks.\n2.4.2 Fine-tuning the PLM\nThis approach ﬁne-tunes some or all the layers of\nthe PLM and then adds one or two simple out-\nput layers (known as prediction heads, Wolf et al.,\n2020). Typically, these are feed-forward layers for\nclassiﬁcation. The output layers and the PLM are\ntrained together in an end-to-end setup, but the bulk\nof the computation is applied to ﬁne-tuning the lan-\nguage model to produce the desired representation\nof the input. The task of the output layers is merely\nto condense the information provided by the em-\nbeddings of each token into the number of desired\nclasses. The word embeddings may come from the\ntop layer, or from a concatenation or a weighted\naverage of the top n(often n = 4) layers (Peters\net al., 2018). Figure 2 (left) shows an illustration\nof this approach.\nThis approach is most suitable for sequence clas-\nsiﬁcation tasks (e.g. sentiment analysis, NLI, se-\nmantic similarity), sequence tagging tasks such as\nNER, and span extraction tasks (e.g. QA) in which\nthe newly trained layers learn the start and end span\nof an answer.\nFor sequence classiﬁcation tasks, Devlin et al.\n(2019) suggests ﬁne-tuning BERT’s representation\nof the special [CLS] token, and following with a\nsingle feed-forward layer that classiﬁes it as one\nof the task labels. For token-level or span-level\nclassiﬁcation tasks, the representations of each to-\nken, or alternatively just the representation of the\nﬁrst sub-token of each token or span (as in De-\nvlin et al., 2019), may be passed to the classiﬁer.\nThis ﬁne-tuning approach is use to apply BERT\nto all 11 tasks in GLUE, as well as QA (SQuAD),\nNER (CoNLL 2003), and common-sense inference\n(SWAG). For many additional examples of this\nhighly popular approach, see Table 8 (Appendix\nB).\nIn this setting, care is needed to choose an appro-\npriate learning rate that works for both the weights\nof the feed-forward layer(s) and for the PLM. Since\nthe PLM is already largely trained, a low learning\nrate should be used (between 1e-3 (Raffel et al.,\n2020) and 1e-5 (Liu et al., 2019)), with a lower\nlearning rate for smaller datasets. However, the\nrandomly initialized feed-forward layer weights\nstill require signiﬁcant training. As such, it is a\ncommon practice to freeze the language model lay-\ners temporarily while initially training the feed-\nforward layers, then unfreeze the language model\ngradually for additional ﬁne-tuning (Howard and\nRuder, 2018; Yang et al., 2019). The degree to\nwhich this should be done depends on the size of\nfeed-forward layers, and whether a token such as\nBERT’s[CLS] is being used. If the majority of\nthe labour is being done by [CLS], as in all the\nexamples in Devlin et al. (2019), there are fewer\nbeneﬁts to training the feed-forward layer alone.\nAgain, this is a function of the availability of super-\nvised training data.\nThe next choice is how many layers of the PLM\nto ﬁne-tune. While the examples in the BERT pa-\nper ﬁne-tune the entire model, this is not feasible\nfor NLP tasks with small datasets or in situations\nwhere compute power is a limitation. Often, tun-\ning just the top few layers of the language model\nis sufﬁcient; for example, Ross et al. (2020) only\nﬁne-tune the top layer of BERT on their small su-\npervised dataset of 2000 sentences. A range of pa-\npers in the growing ﬁeld of “BERTology” (Tenney\net al., 2019, Clark et al., 2019b, Rogers et al., 2020)\nshow that the lower layers of BERT contain word-\nspeciﬁc and syntactic information such as part of\nspeech, while the upper layers contain more seman-\ntic and increasingly complex information such as\nsemantic roles and coreference information.\n2.4.3 Fine-tuning the PLM in Customized\nModels\nSome tasks require signiﬁcant additional architec-\nture on top of a language model, as illustrated in\nFigure 2 (center). With sufﬁcient training data and\ncomputational power, researchers may choose to\ntrain both a substantial task-speciﬁc architecture\nand also ﬁne-tune the language model. This is\nthe preferred choice for structure prediction tasks,\nin particular parsing tasks and occasionally se-\nquence tagging tasks. Examples of sequence tag-\nging models using this approach include BERT-\nCRF for NER (Souza et al., 2020b; Taher et al.,\n2019), though notably Devlin et al. (2019) show\nthat the Conditional Random Field (CRF) layer is\nnot necessarily needed for NER with BERT. Ex-\namples of parsing models using this approach in-\nclude UDapter for dependency parsing (¨Ust¨un et al.,\n2020).\nAny sequence-to-sequence task that uses a pre-\ntrained language model as its encoder may em-\nploy this approach. An interesting example is Zhu\net al. (2020)’s formulation of machine translation.\n7\nHowever, Zhu et al. did not ﬁnd any signiﬁcant\nimprovement over using BERT-based frozen word\nembeddings.\nA related and highly successful approach is to\nﬁne-tune the entire language model with a small\nnumber of feed-forward layers, then layer on an\nalgorithmic approach that provides a substantial\namount of task-speciﬁc heavy lifting. For example,\nit might transform the task from a classiﬁcation\nproblem (as understood by the language model)\ninto the desired target formulation, often a struc-\ntured form such as a tree or a set of clusters. For\ncoreference resolution, Joshi et al. (2019, 2020)\nadds a substantial algorithm, in their case e2e-coref\n(Lee et al., 2018) which transforms ratings of pairs\nof spans into valid mention clusters. Speciﬁcally,\nfor each candidate mention span, the algorithm\ncomputes a distribution over possible antecedent\nspans from the mention score (whether it is likely\nto be a mention) and the compatibility score of\nthe two spans, which itself involves a feed-forward\nnetwork to compute. Two more structural pars-\ning examples in this vein are temporal dependency\nparsing (Ross et al., 2020) and modal dependency\nparsing (Yao et al., 2021). These studies approach\ntree building algorithmically by ﬁrst performing a\nclassiﬁcation problem to identify suitable depen-\ndency pairs, then ranking them to construct a valid\ntree.\n2.4.4 Efﬁcient Fine-tuning Approaches\nA wide range of approaches, in addition to limiting\nﬁne-tuning to the top layers, seek to ﬁne-tune only\na small number of model weights. These can be\nclassiﬁed into two types: (a) ﬁne-tuning a separate,\nsmall network that is tightly coupled with the PLM\n(but does not change it), and (b) selecting only a\nsmall number of the PLM’s weights to ﬁne-tune or\nkeep.\nThe most prominent approach of the ﬁrst type\nare adapter modules (Houlsby et al., 2019; Bapna\nand Firat, 2019; Pfeiffer et al., 2020b,a), as illus-\ntrated in Figure 2 (right). Adapters add a small\nset of newly initialized weights at every layer of\nthe transformer. Houlsby et al. (2019) show that a\ntwo-layer feed-forward network with a bottleneck\nworks well. The placement and conﬁguration of\nthe adapters within the Transformer blocks varies\nin the literature (Houlsby et al., 2019; Bapna and\nFirat, 2019; Stickland and Murray, 2019; Pfeiffer\net al., 2020b). During ﬁne-tuning, all weights in\nthe PLM remain frozen except for the few weights\nin the adapters. One set of adapters is ﬁne-tuned\nper task of interest. This approach is more efﬁcient\nin training (typically < 5% of all PLM weights),\nand allows efﬁcient weight-sharing, both in terms\nof using the same frozen PLM for each task, and in\nallowing the weights of adapter modules to be dis-\ntributed and also re-used. Notably, the weights of\nadapters independently trained for different tasks\ncan be successfully combined to solve a new task\n(Pfeiffer et al., 2020b). Finally, catastrophic for-\ngetting of old capabilities when ﬁne-tuning on a\nnew task or language is prevented. AdapterHub\n(Pfeiffer et al., 2020a) and Trankit (Nguyen et al.,\n2021) are examples of frameworks promoting an\nadapter ecosystem; an example of using adapters\nfor Universal Dependency Parsing is ¨Ust¨un et al.\n(2020).\nA similar method is side-tuning (Zhang et al.,\n2020b), which adapts a pre-trained network by\ntraining a lightweight “side” network that is fused\nwith the (unchanged) pre-trained network using\na simple additive process. Also closely related\nis diff-pruning (Guo et al., 2021), which adds a\nsparse, task-speciﬁc difference vector to the orig-\ninal (frozen) parameters. These difference vec-\ntors are regularized to be sparse, which further de-\ncreases the number of weights that need to be stored\n(around 0.5% of the original model’s parameters).\nMoving to the second type of approach, Bit-\nFit (Zaken et al., 2021) proposes to limit ﬁne-tuning\nto the bias terms (or a subset of the bias terms,\naround 0.1% of the total parameters) of pre-trained\nBERT models, plus a task-speciﬁc classiﬁcation\nlayer. This is shown to be competitive with, and\nfor some tasks better than, ﬁne-tuning all of BERT.\nBitFit builds on the intuition that ﬁne-tuning ex-\nposes existing capabilities, rather than teaching the\nmodel new ones.\nSimilarly, Radiya-Dixit and Wang (2020) show\nthat it sufﬁces to ﬁne-tune only the “most sensitive”\nlayers, i.e. those which are most distant in param-\neter space from the rest of the model. In parallel,\nthey sparsify the model substantially by setting 1-\n4% of pre-trained parameters to zero. This retains\nperformance, as also demonstrated by work like\nDistilBERT (Sanh et al., 2020) and other pruning\nstudies (Prasanna et al., 2020 inter alia) which show\nthat many parameters in a large PLM are redundant.\nIn fact, Zhao et al. (2020a) propose masking, i.e.\nsetting weights to zero, as a sole alternative to ﬁne-\ntuning the model weights. This approach freezes\n8\nall the weights of the PLM, selects the weights\nthat are relevant for a given task, and masks (dis-\ncards) the rest. They train one mask per down-\nstream task, with every layer masked except the\nembedding layer. While in principle this trains as\nmany parameters as the original model, the mask\nis both binary and sparse and thus much simpler to\nlearn and store. The initial sparsity of the mask is\nan important hyperparameter in this approach, as\nis deciding which layers to mask, since the differ-\nent layers encode various degrees of syntactic and\nsemantic knowledge (Tenney et al., 2019). Zhao\net al. show that masking “top-down” (mostly the\ntop layers, which are more task-speciﬁc and encode\nmore semantic and long-distance information) is\nmore effective than masking “bottom-up” (which\nwould mask mostly the layers dealing with ele-\nmentary word meaning and syntax). In particular,\nperformance on CoLA increases as more layers are\nmasked top-down. The authors further show that\nmasking yields entirely comparable performance\nto ﬁne-tuning on a range of tasks from POS tagging\nto reading comprehension.\n3 Paradigm 2: Prompt-based Learning\nWe use prompting to refer to the practice of adding\nnatural language text, often short phrases, to the\ninput or output to encourage pre-trained models to\nperform speciﬁc tasks (Yuan et al., 2021). There are\nseveral advantages to using prompts. Prompting,\nespecially in-context learning (e.g. Brown et al.,\n2020), may not require updates to the PLM’s pa-\nrameters, reducing computational requirements as\ncompared to ﬁne-tuning approaches, or in addition\nto those described in 2.4.4. Prompts also encour-\nage a better alignment of the new task formulation\nwith the pre-training objective, leading to better\nuse of knowledge captured in pre-training. The\ncloser match also enables a few-shot approach (Liu\net al., 2021b), especially for tasks with small train-\ning datasets; a good prompt can be worth hundreds\nof labeled data points (Le Scao and Rush, 2021).\nFinally, prompts allow probing of the PLMs, of-\nten in an unsupervised way, in order to assess the\nknowledge acquired by the PLM for speciﬁc tasks\nof interest (e.g. Petroni et al., 2019).\nWe discuss 3 types of prompt-based learning ap-\nproaches below: Learning from instructions and\ndemonstrations, template-based learning, and learn-\ning from proxy tasks. Figure 3 shows illustrations\nfor each of the three approaches.\nInput Output\n(1) Text pair generation (Schick and Sch¨utze, 2021)\nTask: Write two sentences\nthat mean the same thing.\nSentence 1: “A man is play-\ning a ﬂute.” Sentence 2:\n“He’s playing a ﬂute.”\n(2) Mathematical reasoning (Reynolds and McDonell, 2021)\nf(x) =x∗x. What is\nf(f(3))? Let’s solve this\nproblem by splitting it into\nsteps.\nf(f(3)) =f(3∗3) = 3∗3∗3 =\n27. We can see thatf(3) = 3∗\n3 = 9, sof(f(3)) = 27.\nTable 2: Example prompt designs for learning from in-\nstructions.\n3.1 Learning from Instructions and\nDemonstrations\nFirst attempts made use of instructions such as\n“translate X to Y:” (Raffel et al., 2020) to simultane-\nously teach the model varied tasks in a text-to-text\nmanner. However, this approach required a large\namount of labeled data.\nWith the emergence of large generative PLMs\n(Radford et al., 2019), the ﬁrst signs that language\nmodels are multi-task learners emerged. For in-\nstance, GPT-2 understands that if the instruction\n“TL;DR” (“too long; didn’t read”) is given, then\nit should generate a summary of the context fol-\nlowing the instruction. More recently, and with\neven larger generative PLMs (GPT-3), Brown et al.\n(2020) showed that those models are indeed very\ngood at few-shot learning. Brown et al. showed\nthat GPT-3 can perform few-shot tasks via priming\n(in-context learning): given instructions and a few\ninput/output pairs, GPT-3 is able to produce the de-\nsired outputs for new inputs. No gradient updates\nare performed (see Figure 3, left box). Caveats\ninclude the requirement of a very large LM to work\nwell, and an inability to scale to more than a few ex-\namples, because the context window of most LMs\nis limited to a few hundred tokens. We refer readers\nto the GPT-3 paper (Brown et al., 2020) for many\nadditional examples of learning from instructions\nand/or demonstrations.\nSchick and Sch ¨utze (2021) and Reynolds and\nMcDonell (2021) introduce new tasks based on de-\nscriptions. For example, the text pair generation\ntask Schick and Sch¨utze (2021) consists in gener-\nating a continuation sentence (Sentence 2) given\nan input sentence (Sentence 1) and a description of\nthe relations between the sentences (Table 2(1)).8\n8A variation of this task consists in generating both Sen-\ntence 1 and Sentence 2 given the description (Schick and\nSch¨utze, 2021).\n9\nFigure 3: The three main prompt-based approaches. On the instruction based learning (left box) the instructions are\nmarked in purple, the in-context examples in blue and the prompt in cyan. On the prompt based learning (middle\nbox), the text to classify is marked on light cyan and the prompt on dark cyan; the label verbalizations are shown\nin small boxes. On the proxy task based learning (right box), prompts are marked with dark cyan, the context is on\nlight cyan and the answers generated by the model are in blue.\nTo address this task, Schick and Sch ¨utze (2021)\nuse a generative PLM (GPT2-XL) that generates\nSentence 2, replacing the token. Impressively,\neven mathematical reasoning can be handled (Ta-\nble 2(2)): Reynolds and McDonell (2021) show\nthat by inserting a natural language prompt (“Let’s\nsolve . . . steps.”) after the math problem statement,\nGPT-3 can generate a procedure that solves the\nmath problem.\nRecently, Wei et al. (2021) showed that teaching\na very large PLM to follow instructions with su-\npervised data improves the zero and few-shot abili-\nties of these PLMs. They carried out a large scale\nmulti-task experiment over more than 60 datasets\ngrouped into 12 task different tasks, and showed\nthat a PLM trained via natural language instruc-\ntions on other tasks outperforms a standard lan-\nguage model on the test task. Mishra et al. (2021)\nﬁne-tuned BART (Lewis et al., 2020) to perform a\nsimilar task using instructions and few-shot exam-\nples for a variety of crowd-sourced NLP tasks. The\ncrowdsourcing process of each task consists of sev-\neral steps that are natural and intuitive for human\nannotators. The instructions to the PLM match the\nstep-by-step crowdsourcing instructions, decom-\nposed into self-contained, separate tasks, leading to\nimproved performance on unseen tasks, in contrast\nto an earlier work (Efrat and Levy, 2020) that re-\nported negative performance when using the crowd-\nsourcing instructions as-is.\nScaling limitations may affect the broad appli-\ncability of this approach: Wei et al. (2021) show\nthat instruction tuning achieves signiﬁcant improve-\nments on held-out tasks in the zero-shot setting\nwhen using very large PLMs (e.g. with 68B or\n137B parameters), but hurts performance when ap-\nplied to PLMs with 10B parameters or less. In a\nsimilar setting, Sanh et al. (2021) showed that it is\npossible for a model with 11B parameters to bene-\nﬁt from instruction tuning, and identiﬁed three key\ndifferences compared to Wei et al. (2021). (1) They\nuse a encoder-decoder model trained ﬁrst with the\nMLM objective, then as a standard LM, and ﬁnally\nﬁne-tuned on a multitask objective, rather than a\ndecoder-only autoregressive LM. (2) They argue\nthat their prompts are qualitatively more diverse in\nterms of length and creativity. (3) They hold out\nmultiple tasks at once, rather than only one at a\ntime.\nWe note that the descriptions in instruction learn-\ning can be very detailed. For example, the crowd-\nsourcing instructions in Mishra et al. (2021) contain\nthe task deﬁnition, things to avoid, emphasis and\ncaution (i.e. required properties for the output), and\npositive and negative examples.\n3.2 Template-based Learning\nA more widely used approach, template-based\nlearning, reformulates NLP tasks into tasks that\nare closer to language models’ pre-training tasks\nvia template-based prompts. This better lever-\nages the knowledge captured in the pre-training\ntasks, leading to a signiﬁcant reduction in the\nnumber of task-speciﬁc training examples required\nto achieve a similar performance to previous ap-\nproaches(Le Scao and Rush, 2021), or even elim-\ninating the need for training data. To achieve this\ngoal, template-based learning reformulates various\nNLP tasks into language modeling tasks via care-\nfully designed templates with open slots. In this\n10\nInput Output\n(1) Topic/sentiment classiﬁcation (Schick and Sch¨utze, 2021a)\nBest pizza ever!. It was. great→Positive\n(2) Textual entailment (Schick and Sch¨utze, 2021a)\nMia likes pie?, Mia hates\npie.\nNo→Contradiction\n(3) Event argument extraction (Chen et al., 2020c)\nAmericans sought to bring\ncalm to Mosul, where U.S.\ntroops killed 17 people\nin clashes earlier in the\nweek. someonekilled\nsomeonewithsomethingin\nsome placeat some time.\nU.S. troopskilled 17 people\nwithsomethingin Mosulat\nearlier in the week.\n(4) Probing for relations/facts (Petroni et al., 2019)\nDante was born in. Florence\n(5) Probing for commonsense (Trinh and Le, 2019)\nThe trophy doesn’t ﬁt in the\nsuitcase becauseitis too big\nit →trophy:0.9\nit →suitcase:0.2\n(6) Probing for reasoning (Talmor et al., 2020)\nThe size of an airplane is\nthan the size of a house . A.\nlarger B. smaller\nlarger\nTable 3: Example prompt designs for template-based\nmethods. great →Positive means that the answer great\nwill be converted to label Positive. For Chen et al.\n(2020c), each of the underlined words (e.g. someone)\nwill be replaced with the underlined phrase on the out-\nput side by a PLM. “it →trophy: 0.9” means by replac-\ning the underlined pronoun it with trophy, the modiﬁed\nsentence has a likelihood score of 0.9 according to the\nPLM.\nway, solving the tasks is reduced to ﬁlling the slots\nwith words or phrases using PLMs, and then pro-\njecting these outputs into the task-speciﬁc labels.\nTemplate-based learning differs from instruction\nlearning (Section 3.1) in that templates are less\ndetailed and do not explicitly describe the task.\n3.2.1 Template Design\nUsing a cloze-style prompt design, inputs to a PLM\nare converted into a format such that solving the\nNLP task only requires the PLM to predict missing\nword(s). Table 3 (1) shows the most straightfor-\nward example of this approach, as applied in the\nsentiment detection domain.\nFor classiﬁcation tasks, each predicted word or\nphrase is converted into a class label of interest.\nFor example, we can design a cloze-style prompt\nfor a textual entailment task in which the goal is to\npredict the entail/contradict relation between a pair\nof input sentences ⟨X1,X2⟩. Pattern-Exploiting\nTraining (PET) (Schick and Sch ¨utze, 2021a) (Ta-\nble 3 (2)) converts a pair of inputs ⟨X1,X2⟩into\n“X1? ,X2” and asks a masked language model to\npredict the missing word. The prediction (here yes\nor no) is directly mapped to one of the textual entail-\nment class labels. This template design allows PET\nto reformulate the text entailment problem into the\nsame masked language modeling problem that was\nused to pre-train the PLM. Therefore, it is popular\namong classiﬁcation tasks that may be reformu-\nlated as predicting a masked word or short phrase\n(e.g. topic classiﬁcation, textual entailment, and\nknowledge probing). Chen et al. (2020c) reformu-\nlates the event argument extraction challenge as a\ncloze-style problem (Table 3 (3)), predicting ﬁllers\nfor the underlined positions, and then apply greedy\ndecoding to ﬁll in the position incrementally.\nPetroni et al. (2019) similarly use the cloze-style\ntask for relation/fact probing (Table 3 (4))\nA multiple-choice style prompt proves useful\nfor probing for commonsense knowledge. This\nkind of template provides a selection of hypotheses\nfor the PLM, which selects its preferred answer.\nFor example, in Table 3 (5), Trinh and Le (2019)’s\nmodel selects trophy instead of suitcase to replace it\nin the original sentence. Table 3 (6) shows work by\nTalmor et al. (2020), expressing similar reasoning\nthrough a hypothesis-driven approach.\nPreﬁx prompts (Li and Liang, 2021; Ham-\nbardzumyan et al., 2021; Lester et al., 2021) are\nanother common type of template. Preﬁxes are\ntask-speciﬁc vectors prepended to the input. They\ndo not correspond to actual words but consist of\nfree parameters. Preﬁx prompts are usually the\nbest choice for tasks that require generating text\nor predicting a next word or phrase, because the\npreﬁx-based prompt design is consistent with the\nleft-to-right nature of the autoregresive model (Liu\net al., 2021b).\nPrompts can be further augmented via adding\ndemonstrations ( demonstration learning ) (Gao\net al., 2021a). In that case, a few labeled exam-\nples are appended to the template to make it more\ninformative, allowing the PLMs to better predicting\nthe answer.\n3.2.2 Template Construction\nTemplates can be either manually crafted or au-\ntomatically generated. We here survey the differ-\nent methods for generating them as well as ways\nto combine and manipulate the template-based\nprompts (multi-prompt learning).\nManually-crafted Templates. Most early work\nin prompt-based learning uses some form of manu-\nally crafted templates. For example, manual cloze\n11\ntemplates are used in Petroni et al. (2019) to probe\nthe knowledge of the model, as well as in Schick\nand Sch¨utze (2020), Schick et al. (2020) and Schick\nand Sch¨utze (2021a) for text classiﬁcation in a few-\nshot setting. Manually designed preﬁx prompts are\nleveraged in Brown et al. (2020) for QA, transla-\ntion, and probing tasks for commonsense reason-\ning. The quality of the prompts impacts perfor-\nmance. Indeed, Zhao et al. (2021) showed that\ndifferent prompts can cause accuracy to vary from\nnear chance to near state-of-the-art.\nAutomatically-generated Discrete Templates.\nDiscrete templates, which usually correspond to\nnatural language phrases, are described in a discrete\nspace. To search for such templates given a set of\ninputs and outputs, Jiang et al. (2021) proposed a\nmining-based approach called MINE that aims to\nﬁnd either the middle words or dependency paths\nbetween the inputs and outputs. A second approach\n(Jiang et al., 2021; Yuan et al., 2021) consists of\nparaphrasing an existing template prompt using\nback and forth machine translation, and then se-\nlecting the best prompt among the new paraphrases\nwith guidance from a thesaurus. Prompt paraphras-\ning is also used by Haviv et al. (2021) who used a\nneural prompt rewriter that optimizes the accuracy\nof systems using the prompt. In that case, a differ-\nent paraphrase is generated for each input. A third\napproach uses gradient-based search to ﬁnd short\nsequences that can serve as prompt (Wallace et al.,\n2019; Shin et al., 2020). Gao et al. (2021a) and\nBen-David et al. (2021) further generate prompts\nusing standard generation models such as T5 (Raf-\nfel et al., 2020). In the latter, the authors proposed\na domain adaptation algorithm that trains T5 to\ngenerate unique domain relevant features that can\nbe concatenated with the input to form a template\nfor downstream tasks.\nAutomatically-generated Continuous Tem-\nplates. Continuous prompts, which perform\nprompting directly in the embedding space\nof the model, allow us to abstract away from\nnatural language prompts (i.e. the prompts do\nnot correspond to actual words) and from the\nparameters of the LM (Liu et al., 2021b). These\ncontinuous prompts often require tuning on\ntask-speciﬁc data. Li and Liang (2021) propose\npreﬁx tuning, which prepends a sequence of\ncontinuous, task-speciﬁc vectors to the input while\nkeeping the LM parameters frozen. This allows\nthem to ﬁne-tune just 0.1% of the total model\nparameters. A similar method is used by Lester\net al. (2021), who differ from Li and Liang (2021)\nby adding special tokens to form a template and\ntuning the embeddings of these tokens directly,\nwithout introducing additional tunable parameters\nwithin each network layer. Continuous preﬁx\ntuning is also used by Tsimpoukelli et al. (2021)\nin the context of multimodal learning (language\nand vision) but in that case the preﬁx is sample\ndependent. Tuning can be initialized with discrete\nprompts as in Zhong et al. (2021b), Qin and\nEisner (2021) and Hambardzumyan et al. (2021).\nIt can also be done by inserting some tunable\nembeddings into a hard prompt template as in\nLiu et al. (2021c) and Han et al. (2021b), who\npropose prompt tuning with rules (PTR). This\nuses manually crafted sub-templates to compose a\ncomplete template using logic rules (see Section\n3.2.5 for its application to relation extraction).\nIt is worth noting that Logan IV et al. (2021)\nshowed that ﬁne-tuning PLMs in the few-shot set-\nting can avoid prompt engineering, and that one can\nuse prompts that contain neither task-speciﬁc tem-\nplates nor training examples, and evennull prompts\nthat are simple concatenations of the inputs and the\n[MASK] token and still achieve competitive accu-\nracy on NLU tasks.\nMulti-Prompt Learning A number of ap-\nproaches use prompt ensembling, augmentation,\nand decomposition/composition for a more ﬂexible\ntask design. We describe them below.\nFirst, multiple prompts can be used for an in-\nput (dubbed prompt ensembling) at inference time.\nThe prompts can be combined using a uniform\naverage (Jiang et al., 2021; Schick and Sch ¨utze,\n2021a; Yuan et al., 2021) or a weighted average\n(Jiang et al., 2021; Qin and Eisner, 2021; Schick\nand Sch¨utze, 2021a,b). Another way to combine\nthe prompts is majority voting to combine the re-\nsults of the different prompts as in Lester et al.\n(2021) and Hambardzumyan et al. (2021). Knowl-\nedge distillation (Allen-Zhu and Li, 2021), where\nthe idea is that the knowledge present in an ensem-\nble of models can be distilled into a single model,\nhas been borrowed to the context of prompt com-\nbination by Schick and Sch¨utze (2021a,b); Schick\nand Sch¨utze (2020) and Gao et al. (2021a) where\nfor each template-answer pair a separate model\nis trained, before ensembling them to annotate an\nunlabeled dataset. Then, the authors train a new\n12\nmodel to distill the knowledge from the annotated\ndataset. In the case of generation tasks, Schick and\nSch¨utze (2020) trained a separate model for each\nprompt. Then the model outputs were scored by\naveraging their generation probability across all\nmodels.\nSecond, prompts can be decomposed or com-\nposed to more effectively solve an NLP task. De-\ncomposition involves ﬁnding sub-problems for\nwhich prompts can be generated separately. For\nexample, Cui et al. (2021) proposed an approach\nfor named entity recognition, where the different\nprompts for each candidate span were created and\npredicted separately.\nThird, augmentation methods such as demon-\nstration learning (Gao et al., 2021a) create more\ndescriptive prompts, as in a multiple-choice prob-\nlem. Lu et al. (2021a) showed that both the choice\nof examples in the prompts and the order of the\nprompts can considerably affect the results. To se-\nlect the examples from which the PLM must choose\nthe correct response (example sampling), Gao et al.\n(2021a) and Liu et al. (2021a) used sentence em-\nbeddings to ﬁnd examples semantically close to\nthe input. Mishra et al. (2021) used both positive\nand negative examples, teaching the PLM types of\nitems to avoid in performing new tasks with only\ninstructions. As for the order of the selected exam-\nples (sample ordering), Kumar and Talukdar (2021)\nsearched for the best permutation of prompts and\nalso learned a segmentation token to separate be-\ntween the prompts. They showed the usefulness\nof this method for few-shot learning on the task of\nsentiment classiﬁcation.\n3.2.3 Answer Generation\nThere are two main types of answers to prompts:\nthose that map to a classiﬁcation label (e.g. Yin\net al., 2019; Cui et al., 2021), and those intended\nas the ﬁnal answer (e.g. Petroni et al., 2019; Jiang\net al., 2020; Radford et al., 2019). For classiﬁ-\ncation tasks, typically addressed with cloze-style\nprompts, the developers identify a subset of words\nand phrases from which the PLM may choose, and\nthat choice is easily mapped to the class of inter-\nest. For instance, in a sentiment detection task, the\nPLM may answer a prompt with “good,” “great,” or\n“excellent,” all of which are mapped to a “positive”\nsentiment label. The second type of answer, free\ntext, prevails for text generation tasks. Examples\nof both types are shown in Table 3.\nIn either case, the deﬁnition of the answer space\nmay be optimized to produce ideal prompt re-\nsponses. Jiang et al. (2021) used paraphrasing\nto extend the search space with back translation\n(translating to another language, then back to the\noriginal). Another approach, explored by Schick\nand Sch ¨utze (2021a), Schick et al. (2020), Shin\net al. (2020) and Gao et al. (2021a), is prune-then-\nsearch, a two-step method where the answer space\nis pruned, for example by only selecting a subset\nof words according to their zero-shot accuracy on\nthe training data (Gao et al., 2021a) and then an an-\nswer is searched in the pruned space. An approach\ncalled label decomposition optimizes the search\nspace by modeling the label names for comparison\nto the answer tokens; for instance, in Chen et al.\n(2021d) the decomposed relation labels (their indi-\nvidual tokens) represent the answer space. Lastly,\nHambardzumyan et al. (2021) add a virtual token\nfor each class label and optimize its embedding\ntogether with the token embeddings of the prompts,\nusing gradient descent. This gradient descent op-\ntimization approach allows direct optimization of\nthe answers instead of using a discrete search.\n3.2.4 Task-speciﬁc Tuning\nWhile prompts can be directly used in a zero-shot,\nunsupervised setting, prompts have also been used\nin fully supervised or few-shot settings where ei-\nther all or part of the speciﬁc-task training data is\navailable. Two main approaches currently prevail\nfor tuning a PLM with prompts.\nThe ﬁrst approach uses a ﬁxed template-style\nprompt to perform tuning of the PLM. Here, a\nﬁxed template is usually applied to every training\nand test example as in the PET-TC (Schick and\nSch¨utze, 2021a), PET-Gen (Schick and Sch ¨utze,\n2020) and LM-BFF (Gao et al., 2021a) models.\nLe Scao and Rush (2021) quantiﬁed the beneﬁt of\nusing prompts in classiﬁcation tasks by ﬁne-tuning\nin equal conditions across many tasks and data\nsizes. They showed that prompting consistently\nimproves the results across tasks over just ﬁne-\ntuning, that it is most robust to the choice of pattern,\nand that it can be learned without an informative\nverbalizer (a function that maps each label to a\nsingle vocabulary token). Logan IV et al. (2021)\nshowed that only tuning 0.1% of the parameters in\nthe prompt-based few-shot setting can achieve com-\nparable or better accuracy than standard ﬁne-tuning.\nFor this purpose, they explored different ways to\nperform memory-efﬁcient ﬁne-tuning, including (i)\nAdapters (Houlsby et al., 2019), which are neural\n13\nnetwork layers inserted between the feed-forward\nportion of the Transformer architecture (see Sec-\ntion 2.4.4); (ii) BitFit (Zaken et al., 2021), where\nonly the bias terms inside the Transformer are up-\ndated; (iii) PLM head tuning, where the embed-\ndings in the MLM output layer that are associated\nwith the tokens of the verbalizer are updated; and\n(iv) Calibration (Zhao et al., 2021), where an afﬁne\ntransformation on top of the logits associated with\nthe verbalizer tokens is learned. They found that\nthe best results are achieved using BitFit.\nThe second approach is joint tuning of the\nprompt and the PLM. Here, prompt-relevant pa-\nrameters are ﬁne-tuned together with the all or\nsome of the parameters of the PLM, as in PADA\n(Ben-David et al., 2021), where the prompts are\nproperties of source domains, generated based on\ntheir relatedness to the input example (from a new\ndomain), and P-Tuning (Liu et al., 2021c), which\nmakes use of trainable continuous prompt embed-\ndings when applying GPT models on NLU tasks.\nFinetuning both the model and the prompt-relevant\nparameters makes this approach very expressive.\nOn the other hand, it requires the storage of all the\nparameters, with makes it less applicable to small\ndatasets (Liu et al., 2021b).\nIt is worth noting that task-speciﬁc training can\nalso be used earlier during the construction and val-\nidation of the prompts. Indeed, as pointed out by\nPerez et al. (2021), previous PLM-based few-shot\nlearning approaches used many held-out examples\nto tune various aspects of learning, such as hyperpa-\nrameters, training objectives, and natural language\ntemplates (“prompts”). Perez et al. (2021) propose\ninstead to evaluate the few-shot ability of PLMs in\na true few-shot learning setting, where such held-\nout examples are unavailable.\n3.2.5 Applications of Template-based\nMethods\nTemplate-based prompting methods are currently\napplied to a growing list of NLP tasks. We provide\na survey of how recent studies have addressed a\nvaried set of NLP applications.\nText Classiﬁcation. In Puri and Catanzaro\n(2019), natural language descriptions of classiﬁ-\ncation tasks were given as input. Then, the model\nwas trained to generate the correct answer in natural\nlanguage via a language modeling objective, aim-\ning to generalize to new classiﬁcation tasks without\ntask-speciﬁc tuning.\nInformation Extraction (IE). Cui et al. (2021)\nconsidered the NER task as a language model\nranking problem in a sequence-to-sequence frame-\nwork where the source sequence corresponds to\nthe original sentence and the target sequence corre-\nsponds to the template prompt, ﬁlled by candidate\nspans. For the relation extraction task, Han et al.\n(2021b) proposed a model called Prompt Tuning\nwith Rules (PTR), which applies logic rules to con-\nstruct prompts with several sub-prompts. Chen\net al. (2021d), instead of using rules, constructed\nthe prompts by leveraging learnable virtual tem-\nplate words and virtual answer words. Their repre-\nsentation is synergistically optimized with knowl-\nedge constraints. For the event extraction task in a\ncross-lingual setting, Fincke et al. (2021) proposed\nusing the event type and an integer representing the\nargument type as preﬁxes.\nKnowledge Probing. Factual probing has been\nexplored in particular by Petroni et al. (2019) and\nJiang et al. (2020) to quantify the amount of factual\nknowledge already present in the PLMs, providing\nthe LAMA and X-FACTR datasets, respectively.\nOther works that investigated model knowledge\nwith discrete template search include Petroni et al.\n(2020), Jiang et al. (2021), Haviv et al. (2021),\nShin et al. (2020) and Perez et al. (2021). Continu-\nous template learning was used in Qin and Eisner\n(2021), Liu et al. (2021c) and Zhong et al. (2021b).\nPrompt ensemble learning was applied to knowl-\nedge probing by Jiang et al. (2021) and Qin and\nEisner (2021).\nIn addition to factual knowledge, additional\ntypes of knowledge that have been probed using\nthe cloze test include commonsense (Trinh and\nLe, 2019), relational knowledge (Petroni et al.,\n2019), reasoning (Talmor et al., 2020) and under-\nstanding rare words (Schick and Sch ¨utze, 2019).\nFor commonsense reasoning, Winograd Schemas\n(Levesque et al., 2012) require the model to iden-\ntify the antecedent of an ambiguous pronoun within\ncontext, or involve completing a sentence given\nmultiple choices. For commonsense knowledge\nmining, Feldman et al. (2019) construct a candi-\ndate piece of knowledge as a sentence, then use a\nlanguage model to approximate the likelihood of\nthe text as a proxy for its truthfulness.\nPrompts can also be used to explore the linguistic\nknowledge of PLMs, focusing on different phenom-\nena such as analogies (Brown et al., 2020), nega-\ntion (Ettinger, 2020) or semantic similarity (Sun\n14\net al., 2021). Linguistic evaluation of language\nmodels (Linzen et al.; Gulordava et al., 2018; Gold-\nberg, 2019; Tran et al., 2018; Bacon and Regier,\n2019; McCoy et al., 2020; Linzen, 2020) usually\nconsiders minimal pairs of grammatical and non-\ngrammatical sentences addressing a speciﬁc phe-\nnomenon that differs in a single place in the sen-\ntence. To succeed, a model must score the grammat-\nical sentence higher than its ungrammatical coun-\nterpart. A main resource in this context is BLiMP\n(Benchmark of Linguistic Minimal Pairs, Warstadt\net al., 2020a) which provides minimal pairs for var-\nious grammatical phenomena. Recently, the use of\nthis benchmark was adapted for language acquisi-\ntion research (Huebner et al., 2021): the authors\nprobe a RoBERTa-based model pre-trained on tran-\nscriptions of child-directed speech (MacWhinney,\n2000) to complete the benchmark task. The pref-\nerence score can be calculated either holistically,\nsumming the cross-entropy errors at each position\nin the sentence (Zaczynska et al., 2020; Huebner\net al., 2021), or in an MLM-based way, where each\ncandidate sentence is masked by a language model\nmultiple times with the mask changing position.\nThe score is computed by summing the log-losses\nat the different masked positions (Salazar et al.,\n2020).\nOther tasks. The PET procedure (Schick and\nSch¨utze, 2021a) was also applied to the Textual\nEntailment task. QA is addressed in Khashabi et al.\n(2020) with appropriate prompts from the context\nand questions, formulating several QA tasks into\na uniﬁed text generation problem with encoder-\ndecoder pre-trained models such as T5.\nPrompts have also been used for the evalua-\ntion of text generation. Yuan et al. (2021) used\nprompts in the BARTSCORE-PROMPT variant of\nthe BARTSCORE measure they propose that treats\nthe evaluation of various text generation tasks as a\ngeneration problem. In BARTSCORE-PROMPT,\nprompts are either appended to the source text or\nprepended to the target text and are shown to be\nuseful. For example, adding the phrase “such as” to\nthe translated text when using pre-trained models\nsigniﬁcantly improves the correlation with human\nevaluation on German-English machine translation\nevaluation.\nSchick et al. (2021) showed that PLMs are able\nto recognize the toxicity of the text they produce\n(self-diagnosis). Then they proposed an algorithm\nthat permits the language model to produce less\nproblematic text (self-debiasing), using a textual\ndescription of the undesired behavior.\nShin et al. (2021) explore the use of PLMs as\nfew-shot semantic parsers. The authors use GPT-3\nto convert text into a canonical text (in a controlled\nsub-language) satisfying a grammar, that is then\nautomatically mapped to the target structured mean-\ning representation.\n3.3 Learning from Proxy Tasks\nTemplates and prompts play a role again in an indi-\nrect approach to NLP tasks called “proxy tasks”.\nExamples for the use of this approach are emo-\ntion classiﬁcation or event and argument extraction,\nboth shown in Figure 3 (right box) with prompt-\nbased proxy tasks. See Table 4 for additional ex-\namples of proxy tasks and prompt design.\nThe key distinction between learning from proxy\ntasks and previous methods is the use of supervised\nNatural Language Understanding (NLU) tasks as\na proxy instead of self-supervised language mod-\neling for the target task. Indeed, taking advantage\nof large NLU datasets for extra supervision results\nin better zero and few-shot performance in the tar-\nget task with relatively small PLMs (Wang et al.,\n2021b), commonly RoBERTalarge at 345M param-\neters. Knowledge-rich classiﬁcation tasks in par-\nticular beneﬁt from PLM proxy tasks, because the\nlatter can reformulate the class label as a prompt,\ntaking advantage of the meaning of class labels\ninstead of treating them as indices. In this section,\nwe describe the main proxy-task-based learning\napproaches using QA (Section 3.3.1) and Textual\nEntailment (Section 3.3.2).\n3.3.1 Question Answering as Proxy Task\nIn a strong move away from traditional informa-\ntion extraction, recent studies replace modeling of\nexplicit entity, relation, and event classes with nat-\nural language questions that get at the exact item\nof interest. Questions can be used to probe for the\nrequired information in the text.\nThe choice of using QA as a proxy task is mo-\ntivated by the relative ease of answering simple\nquestions, as compared to performing expert anno-\ntation for complex linguistic phenomena.\nIn information extraction tasks, question\nprompts typically address identiﬁcation and clas-\nsiﬁcation jointly, by constructing the question to\nidentify a particular type. For example, the ques-\ntion “Who bought something?” will produce an\nanswer speciﬁc to the Buyer argument role in an\n15\nApplication Work Task design Prompt design\nRelation Extrac-\ntion\nLi et al. (2019a)Use question-answering to iden-\ntify the most appropriate entity\nspan, given an incomplete text\nand an indication of the class\ntype\nInput: The armory is north of the music center. Prompt: Find a\nfacility nearE1?E1, physical, facility\nSainz et al.\n(2021)\nUse textual entailment to deter-\nmine the likelihood of a can-\ndidate relation (such as Place-\nOfDeath(X,Y) given an input\nsentence.\nInput: Gary’s car crash occurred in Houston; Prompt: Gary died\nin Houston\nEvent Extrac-\ntion\nDu and Cardie\n(2020)\nUse a series of ordered ques-\ntions, each leveraging the out-\nput of the previous answer, to\nﬁnd event triggers and appropri-\nate arguments\n(1) Input: Donna purchased a new laptop; Prompt: What is the\ntrigger? purchased(2) Prompt: What was purchased? laptop\nTopic and Senti-\nment Classiﬁca-\ntion\nYin et al. (2019)Use textual entailment to deter-\nmine whether a topic nameTis\nsuitable for a text.\nInput: Dinosaurs and humans never coexisted. Prompt: This\ntext is aboutT.\nPuri and Catan-\nzaro (2019)\nUse question answering to\nprobe for a topic or sentiment\nname from among a closed set\nof responses.\nInput: Dinosaurs and humans never coexisted. Prompt: How is\nthe text best described?T1,T2, orT3\nCoreference\nResolution\nWu et al.\n(2020b)\nUse question-answering to ﬁnd\na coreferent mention of a\nmarked mention from within\nthe same text.\nInput: I arrived at the party with my tux on, and introduced\nmyself as George. I told them that<mention>I</mention>\nwas hired to do some Christmas music; Prompt: Who does it I\nrefer to?\nTable 4: Examples of task design and example prompts for four different applications of prompt-based proxy tasks.\nevent of type Exchange-Ownership (see Figure 3,\nright box).\nLi et al. (2020c) formulates Named Entity\nRecognition (NER) as a QA problem. For ex-\nample, the prompt “which person is mentioned\nin the text?” will identify a mention classiﬁed as a\nPERSON. The proposed BERT-based system per-\nforms detection of multiple spans through the use\nof separate binary classiﬁers identifying start and\nend tokens. The authors incorporate synonyms and\nexamples into the queries.\nWu et al. (2020b) formulated coreference reso-\nlution as a span prediction task via QA, where a\nquery is generated for each candidate mention us-\ning its surrounding context, and a span prediction\nmodule uses the query to extract the coreference\nspans in the document.\nLevy et al. (2017) ﬁrst formulated relation ex-\ntraction as a QA task. This approach has been\npursued in the context of PLMs by Li et al. (2019a)\nand Zhao et al. (2020b). Han et al. (2021b) ad-\ndresses relation extraction with sub-prompts foren-\ntity recognition and relation classiﬁcation, com-\nposing them into a complete prompt using logic\nrules. Both types of questions are used to probe a\nQA system in a supervised setting to perform the\ntwo sub-tasks. Task decomposition is also used in\nthe work of Zhou et al. (2021) for event extraction\nwhere natural questions for argument identiﬁca-\ntion (“What plays the role?”) andargument clas-\nsiﬁcation (“What is the role?”) mutually improve\neach other.\nChen et al. (2020c) reformulated event extrac-\ntion as a cloze task with QA model based on BERT\nand the SQuAD 2.0 dataset (Rajpurkar et al., 2018).\nQuestion answering is used directly, preserving\nthe QA format, in Du and Cardie (2020), Feng\net al. (2020a), Li et al. (2020a), Zhou et al. (2021)\nand Liu et al. (2020a) for argument extraction, in-\ncluding the argument identiﬁcation and classiﬁca-\ntion sub-tasks. In these cases the event extraction\ntraining data is converted to the QA format, where\nthe questions are derived from the ontology. Liu\net al. (2020a) also experimented in a zero-shot set-\nting where no task-speciﬁc data is used for train-\ning, only using prompts for probing. The zero-\nshot setting for the full event extraction pipeline\nhas been explored in Lyu et al. (2021) where QA-\nbased prompts are used for argument extraction and\nprompts based on Textual Entailment (Dagan et al.,\n2013) are used for trigger classiﬁcation (see Sec-\ntion 3.3.1 below). Several ablation experiments an-\nalyzed the different components of the system such\nas the choice of PLM, the choice of QA dataset and\nthe way to generate the questions (ﬁxed vs. contex-\ntualized). It was shown in particular that RoBERTA\n16\ntrained on QAMR (Michael et al., 2018) achieved\nthe best results for argument extraction.\nIdentiﬁcation-only sub-tasks such as trigger\nidentiﬁcation (Du and Cardie, 2020), are ad-\ndressed by more general questions, e.g. “What\nis the trigger?”. In contrast, Zhou et al. (2021) uses\nseparate questions to address the identiﬁcation and\nclassiﬁcation of arguments.\nDu et al. (2021a) addressed slot-ﬁlling, which\naims to extract task-speciﬁc slot ﬁllers (for exam-\nple, a ﬂight date) from user utterances by formulat-\ning it as a QA task. In particular, they addressed\nthe zero-shot slot-ﬁlling problem, where the model\nneeds to predict spans and their values, given utter-\nances from new, unsupervised domains. Extracting\nslot-ﬁller spans from utterances with a QA model\nimproved the performance, compared to a direct\nencoding of the slot descriptions.\nLastly, Gao et al. (2019) formulated thedialogue\nstate tracking task that aims to the estimate of the\ncurrent belief state of a dialog given all the pre-\nceding conversation, as a QA problem. The pro-\nposed system uses a simple attention-based neural\nnetwork to point to the slot values within the con-\nversation. This direction was pursued by Gao et al.\n(2020b) who also included a multiple-choice set-\nting, where several candidate values for each slot in\nthe question are given. The latter setting was also\ninvestigated by Zhou and Small (2020) who fur-\nther improved the results. Namazifar et al. (2020)\nused this approach to address language understand-\ning problems in the dialogue context, experiment-\ning on ATIS (Airline Travel Information Systems,\nHemphill et al., 1990) and on the Restaurants-8k\ndataset (Coope et al., 2020).\nQA Task Design. Questions are typically gener-\nated via hand-crafted templates derived from the\ntask-speciﬁc ontologies. Some of the works intro-\nduce contextualization, integrating relevant words\nfrom the text into the question. For example, in\nargument extraction, the question can include the\ntrigger extracted from the text (e.g Liu et al., 2020a;\nLyu et al., 2021) or another argument that was pre-\nviously identiﬁed (Li et al., 2020a) (see the Event\nExtraction row in Table 4). Neural based question\ngeneration models can also improve the quality of\nthe question, as in Liu et al. (2020a), where mono-\nlingual unsupervised machine translation (Lample\net al., 2018) is used to generate the part of the ques-\ntion that does not depend on the template, trans-\nlating a descriptive statement into a question-style\nexpression.\nOther aspects of QA-style proxy tasks are the\nability to use multiple questions, and to formulate\nquestions in any style. In addition to sequential\nquestions for determining event arguments, multi-\nple formulations of the same question may be used\nin a weighted voting scheme to generate an ensem-\nble answer Zhao et al. (2020b). The input to the QA\nsystem need not necessarily include natural ques-\ntions. It may instead consist of pseudo-questions\nsuch as keywords, synonyms, position index of la-\nbels, or a single word/type from the ontology or\nannotation guidelines (e.g. Li et al., 2020c; Du and\nCardie, 2020).\nPLMs ﬁne-tuned on the SQuAD 2.0 dataset (Ra-\njpurkar et al., 2018) or on QAMR are particularly\nuseful to initialize QA-style prompt-based learn-\ning methods.9 With the advent of web-scale QA\ndatasets (Huber et al., 2021), QA-infused PLMs\nmay provide signiﬁcantly richer representation, en-\nabling a wider range of applications.\n3.3.2 Textual Entailment as Proxy Task\nTextual Entailment is a popular proxy for classi-\nﬁcation tasks (Yin et al., 2019), as these models\nhave shown a striking ability to perform few-shot\nlearning. Wang et al. (2021b) hypothesizes that\nthis phenomenon might be because the entailment\ntask is a true language understanding task; a model\nthat performs entailment well is likely to succeed\non similarly-framed tasks. An example of textual\nentailment as a proxy for emotion classiﬁcation is\nshown in Figure 3, while an example of its use for\ntopic detection is shown in Table 4.\nFor entailment prompting, developers deﬁne a\ntemplate that describes the task, and create a natural\nlanguage version (“verbalization”) of each poten-\ntial label. Multiple hypotheses for entailment are\nproduced by inserting the potential labels into the\ntemplate. The inference is performed by selecting\nthe most probable candidate hypothesis given the\ninput. Some recent works also make use of multi-\nple verbalizations for each label to boost the system\nperformance (Sainz and Rigau, 2021; Sainz et al.,\n2021).\nSainz et al. (2021) also proposed an approach\nto guiding the “art” that is prompt crafting more\ntowards a “science”: the authors ﬁne-tune a model\non Textual Entailment data and use the model’s\nprobability of a prompt given the template, applied\n9Fine-tuning on a PLM on QAMR corresponds to the p-\nQuASE representation presented in He et al. (2020).\n17\non the guideline example(s), to measure the quality\nof manually designed prompts.\nObamuyide and Vlachos (2018) reformulated\nrelation extraction as a textual entailment task.\nThis approach has been pursued in the context of\nPLMs by Sainz et al. (2021).\nRoughly equivalent to textual entailment is\nYes/No Question Answering (Clark et al., 2019a)\nwhere a model is asked about the veracity of some\nfact given a passage. It has also been used as a\nproxy task for text classiﬁcation by Zhong et al.\n(2021a).\nPLMs needs to be ﬁne-tuned to solve the textual\nentailment task. They are commonly ﬁne-tuned on\nMNLI (Williams et al., 2018), but other datasets\nsuch as SNLI (Bowman et al., 2015), FEVER\n(Thorne et al., 2018), ANLI (Nie et al., 2020) or\nXNLI (Conneau et al., 2018) are also used. In ad-\ndition, data from different tasks can be used when\nframed properly (Zhong et al., 2021a).\n4 Paradigm 3: NLP as Text Generation\nThe success of generative Transformer-based\nPLMs10 such as GPT, BART, and T5 has recently\nsparked interest in leveraging generative PLMs to\nsolve various non-generative NLP tasks. These\ntasks include, but are not limited to, traditional dis-\ncriminative tasks such as classiﬁcation and struc-\nture prediction. For example, Figure 4 illustrates\nthis “text-to-text” approach as described in Raf-\nfel et al. (2020). Instead of using traditional dis-\ncriminative models for NLP tasks, these tasks are\nreformulated as text generation problems so that\nthey can be directly solved with generative PLMs.\nThe generated output sequences usually include the\ndesired labels or other auxiliary information for the\ngiven task, enabling accurate reconstruction of the\nexpected class labels (i.e. to avoid ambiguities in\nmapping) and facilitating the generation/decoding\nprocess (i.e. to provide sufﬁcient context for pre-\ndictions).\nIt is worth noting that some NLP tasks are al-\nready text generation tasks. Therefore, a straight-\nforward strategy for those tasks is to ﬁne-tune a\ngenerative PLM using task-speciﬁc training data to\nperform the speciﬁc tasks of interest. Examples in-\nclude Machine Translation (Cooper Stickland et al.,\n2021), text summarization (Lewis et al., 2020), text\nstyle transfer (Lai et al., 2021), etc. We refer read-\n10In this section and next, we use the term PLM to refer to\na generative PLM.\ners to Section 2 for more detailed discussion of\nthis “pre-train then ﬁne-tune” approach. In this\nsection, we focus on tasks that are not traditionally\ntext generation tasks.\nReformulating NLP Tasks as Text Generation\nProblems Pre-trained from large corpora, PLMs\ndemonstrate an extraordinary ability to generate\ntext. PLMs also capture rich knowledge that could\nbe used for many NLP tasks and show strong per-\nformance on learning new patterns via ﬁne-tuning.\nThese factors lead to the hypothesis that many NLP\ntasks can be reformulated as text generation prob-\nlems. In particular, given an NLP task with an input\ntext x, this approach ﬁrst attempts to design an out-\nput sequence ythat includes information about the\ndesired labels for x(e.g. markers). Then, a PLM\ndirectly generates y, conditioning on the input x,\nmodeling P(y|x). In this formulation, the desired\nlabels/outputs for the task on xneed to be retrieved\nunambiguously from y, requiring yto be generated\nin a valid format by the design of the reformulated\ntask. In addition to the label information, evidence\nuseful for providing context can also be incorpo-\nrated into the formulation of yto aid the generation\nprocess. To train the PLMs, the original training\ndata of the NLP task is ﬁrst converted into pairs\n(x,y) following the designed format. The PLMs\nare usually ﬁne-tuned with such pairs using the\nstandard maximum likelihood loss.\nThere are a few advantages of this approach.\nFirst, in this formulation, a uniﬁed text-to-\ntext/seq2seq framework can be used to solve differ-\nent NLP tasks via encoder-decoder architectures,\nthus facilitating multi-task learning and transfer\nlearning across tasks of different natures (Raffel\net al., 2020). Second, the direct generation of la-\nbels in output sequences allows the PLMs to exploit\nthe semantics of the labels to improve the perfor-\nmance and data efﬁciency, a beneﬁt that cannot be\nachieved in discriminative models (Paolini et al.,\n2021). Finally, when adapting to structure predic-\ntion problems, PLM-based models can naturally\ncapture the inter-dependencies between prediction\nsteps/tasks in the modeling process to further im-\nprove the performance (Athiwaratkun et al., 2020).\nAs such, the formation of the output sequence\ny for an input x is critical for the performance\nof the PLM-based methods. Existing works tend\nto customize such output sequences for speciﬁc\nNLP tasks to better capture the nature of the tasks.\nTherefore, in the rest of this section, we group\n18\nFigure 4: An illustration of T5 (Raffel et al., 2020) text-to-text text generation approach for Machine Translation,\nlinguistic acceptability, text semantic similarity and summarizing tasks. Figure source: Raffel et al. (2020).\nprior works according to their strategies in design-\ning the output sequences to solve NLP tasks with\ngenerative models, and discuss their representative\ntechniques in each subsection. Table 5 provides a\nbrief summary.\n4.1 Generating Label-Augmented Texts\nIn this strategy, the output sequence y copies\nthe input text x and augments it with additional\nmarkers that can be decoded into desired label an-\nnotations for xfor a given NLP task. The repetition\nof the words from the input text aims to provide\nexplicit context to reduce ambiguity for the genera-\ntion process (Paolini et al., 2021). This strategy is\noften applied to structure prediction tasks that aim\nto jointly extract the text spans of interest and their\nrelations or dependencies in an input text.\nAthiwaratkun et al. (2020) explores the idea of\nlabel-augmented text generation for sequence la-\nbeling problems, e.g. slot ﬁlling (identifying spans\nthat deﬁne the left or right “slot” of a relationship)\nand Named Entity Recognition (NER). Given an\ninput sentence, the output sequence is formed by\nmarking the token sequences for the slots or entity\ntypes of interest, for instance with square brack-\nets or another identiﬁer. The corresponding labels\nare then introduced immediately after the token\nsequences, within the brackets, separated by the\ntoken by a bar “|”. The encoder-decoder PLM T5\nis used to generate label-augmented texts. Paolini\net al. (2021) extends this idea to other structure\nprediction tasks, including joint entity and relation\nextraction, relation classiﬁcation, semantic role la-\nbeling (SRL), event extraction, coreference reso-\nlution, and dialogue state tracking. To encode a\nrelation between two text spans the input text, the\nsecond text span might be annotated with both the\nrelation label and an indicator of the ﬁrst text span.\nFor example, for the joint entity and relation\nextraction task, the input sentence xcan be trans-\nformed into the label-augmented output sequence\ny, where (1) the square brackets indicate token\nspans for entity mentions; (2) person and book\nare the corresponding entity type labels; and\n(3) author=Tolkien indicates the author re-\nlation between Tolkien and The Lord of\nthe Rings:\nx= Tolkien’s epic novel The\nLord of the Rings was\npublished in 1954-1955.\ny = [Tolkien|person]’s epic\nnovel [The Lord of the\nRings|book|author=Tolkien]\nwas published in 1954-1955.\nIn order to transform the generated label-\naugmented texts into desired annotations, Paolini\net al. (2021) uses dynamic programming to match\nthe generated output sequence and the input text,\nsearching for the closest entity mention that exactly\nmatches the predicted tail entity and discarding in-\nvalid entity/relation types. Similarly, Zhang et al.\n(2021a) utilize label-augmented text generation for\ndifferent variations of aspect-based sentiment anal-\nysis (ABSA), including aspect opinion pair extrac-\ntion, uniﬁed ABSA, aspect sentiment triplet extrac-\ntion, and target aspect sentiment detection. Zhang\net al. (2021a), also propose a normalization pre-\ndiction mechanism: if a generated token does not\nbelong to the original sentence or set of expected\nlabels, the closest word from the input sentence\nusing the Levenshtein distance is used instead.\nDue to the uniﬁed text-to-text formulation, label-\n19\nOutputType Work Task ExampleInput Output\nLabel-augmentedText\nPaolini et al. (2021)\nJoint Entity and Relation ExtractionTolkien’s epic novel The Lord of the[ Tolkien|person ]‘s epic novel [ The Lord of the RingsRings was published in 1954-1955.|book|author = Tolkien ] was published in 1954-1955\nRelation ClassiﬁcationBorn in Bologna, Orlandi was a student of the famous Italianrelationship between [ Carmen Melis ] and[ soprano ] and voice teacher [ Carmen Melis ] in Milan.[ soprano ] = voice typeThe relationship between [ Carmen Melis ] and [ soprano ] is\nSemantic Role LabelingThe luxury auto maker last year [ sold ][ The luxury auto maker|subject ] [ last year|temporal ]1,214 cars in the U.S. sold [ 1,214 cars|object ] [ in the U.S.|location ]Event Extraction Two soldiers were attacked and injured yesterdayTwo soldiers were [ attacked|attack ] and [ injured|injury ] yester-day\nCoreference ResolutionBarack Obama nominated Hillary Rodham Clinton[ Barack Obama ] nominated [ Hillary Rodham Clinton ] as [ his|as his secretary of state on Monday.Barack Obama ] [ secretary of state|Hillary Rodham Clinton ] onMonday\nDialogue State Tracking[ user ] : I am looking for a cheap place[ belief ] hotel price range cheap, hotelto stay [ agent ] : How long? [ user ] : Twotype hotel, duration two [ belief ]Athiwaratkun et al.(2020) Slot Filling Add Kent James to the Disney soundtrack(( AddToPlaylist )) Add [ Kent James|artist ] to the [ Disney|playlist ]Named Entity RecognitionHe is John Wethy from NBC NewsHe is [ John Wethy|person ] from [ NBC News|org ]\nZhang et al. (2021a)\nAspect Opinion Pair ExtractionSalads were fantastic, our server was also very helpful.[Salads|fantastic] were fantastic,our [server|helpful] was also very helpful.\nAspect Sentiment Triplet ExtractionThe Unibody construction is solid, sleek and beautiful.The [Unibody construction|positive|solid, sleek,beautiful] is solid, sleek and beautiful.Target Aspect Sentiment DetectionThe pizza was cold. The [pizza|food quality|negative] was cold.\nGeneratingWordIndices\nYan et al. (2021b)Named Entity Recognitionhave muscle pain and fatigue 2 3 7 2 5 6\nYan et al. (2021a)\nAspect Term ExtractionThe wine list is interesting andhas good values , but the serviceis dreadful\n1, 2, 12, 12Opinion Term Extraction 4, 4, 7, 8, 14, 14Aspect-level Sentiment Classiﬁcation 1, 2 , PositiveAspect-oriented Opinion Extraction 1, 2, 4, 4, 7, 8\nRongali et al. (2020)Slot Filling play the song don’t stop believin by journeyPlaySongIntent SongName( @pt r3 @pt r4 @pt r5)SongName ArtistName( @pt r7 )ArtistNameGeneratingAnswersWang et al. (2021a)Closed-book QA What is Southern California often abbreviated as?Southern California, often abbreviated SoCal, is...ANSWER SoCalHsu et al. (2021)Answer Selection How a water pump works? A water pump is a device that moves ﬂuids by mechanical action.\nFillingTemplates\nDu et al. (2021b)Event Extraction [CLS] Attack, Bombing, Arson,...[SEPT] (Document[CLS] Attack -T1 REEs- [SEPT] Bombing -T2 REEs- [SEPT]tokens): Several attacks were carried out in La Paz...[SEP]\nLi et al. (2021c)Event Argument ExtractionElliott testiﬁed that on April 15, McVeigh came into the body-tgr- reserved -tgr- the truck, to be picked up at 4pm two dayslater shop and\nElliott bought, sold or traded truck to McVeigh in exchange for$280.32 for the beneﬁt of -arg- at body shop place\nStructure-linearizedTexts\nRen et al. (2021)Joint Entity and Relation ExtractionHe was captured in Baghdad late Monday night“He” Type PER [SEP] “Baghdad” Type GPE PHYS “He”\nLu et al. (2021b)Event Extraction The man returned to Los Angeles from Mexico((Transport returned (Artifact The man) (Destination Los Angeles)(Origin Mexico))\nRankingInput-outputPairs\nNogueira dosSantos et al. (2020)Answer Selection <bos>Ice formations in the Titlis glacier cave0.5<boq>How are glacier cave formed<eoq>\nNogueira et al. (2020)Document RetrievalHow are glacier cave formed [Q] A glacierTruecave is a cave formed within the ice of a glacier [D]De Cao et al. (2021)Entity Retrieval Superman saved [START] Metropolis [END]Metropolis (comics)|Metropolis (1927 ﬁlm)Cui et al. (2021)Named Entity RecognitionACL will be held in Bangkok Bangkok is a location\nTable 5: A summary of methods reformulating NLP task as a generation task solved by PLMs.\naugmented text generation allows multi-task learn-\ning where a single generative model can be trained\nto simultaneously perform multiple tasks of differ-\nent natures. Paolini et al. (2021) and Athiwaratkun\net al. (2020) show that learning from multiple\ntasks with a single model can improve the perfor-\nmance on the individual tasks. Furthermore, label-\naugmented text generation also shows impressive\nperformance in few-shot learning settings (Paolini\net al., 2021), improving the data efﬁciency.\n4.2 Generating Word Indices\nFor many text understanding problems (e.g. span\ntagging problems such as NER), the generative\nPLM must not generate words that are not in the\ninput text, other than markers or labels as shown in\nthe example in Section 4.1. Restricting the PLMs to\nconsider only words in the input text as candidates\nat decoding (text generation) time enforces this\nconstraint.\nAn alternative approach is to directly generate\nindices of the words of interest in the input text.\nGiven the input x, the output sequence yprovides\na sequence of index numbers referring to the po-\nsitions of words in x. Label indices encode class\nlabels within y. A few examples are included in\nTable 5 in the “Generating Word Indices” rows.\nYan et al. (2021b) explores an index generation\nidea for NER that can naturally handle different\nsettings, e.g. ﬂat, nested, and discontinuous NER.\nGiven the input sequence x = [x1,x2,...,x n],\nthe output sequence y is formed via the indices:\ny = [s11,e11,...,s 1k1\n,e1k1\n,t1,...,s i1,ei1,...,\nsiki\n,eiki\n,ti] where s and e indicates the start\nand end indexes of a span. The spans for the\ni-th name in x are represented by the tuple\n[si1,ei1,...,s iki\n,eiki\n,ti] where ti is the index of\nthe entity type andki is the number of text spans for\nthe i-th name (a name can have multiple spans due\nto the consideration of discontinuous names). As\nsuch, sij and eij should be between 1 and nwhile\nthe entity types can be indexed from n+ 1(i.e.,\nti >n). To compute the hidden vectors at decod-\ning time, the representations for the span indices\ncan be obtained from the representations of the cor-\nresponding words in the input sentence x(i.e., via\npointer networks (Vinyals et al., 2015)). BART is\nused as the base model for the index generation for\nNER.\nSimilarly, Yan et al. (2021a) generates indices\n20\nof the spans of interest for variations of the aspect-\nbased sentiment analysis (ABSA) task, including\naspect term extraction, opinion term extraction,\naspect-level sentiment classiﬁcation and aspect-\noriented opinion extraction. Finally, casting a prob-\nlem into an index generation task is also proposed\nfor semantic parsing (i.e. ﬁlling slots) (Rongali\net al., 2020). The output sequence in this work\nstarts with the intent, followed by slot names and\nthe index sequences of the words in the input for\nthe slots. At decoding time, each step produces\na distribution over the word indexes in the input\nsentence (as a pointer network) and the vocabulary\nfor slots and intents in the datasets.\n4.3 Generating Answers\nThis strategy is designed mainly for the QA task.\nThe basic idea is to ﬁne-tune PLMs to generate an-\nswers for the QA problems of interest. Wang et al.\n(2021a) use BART for closed-book QA that aims to\ndirectly provide answers for input questions. They\nshow that BART struggles on a version of SQuAD\nfor closed-book QA where the test and training\ndata do not have much question and answer over-\nlap. It also shows that BART cannot remember\nknowledge from the ﬁne-tuning data if there are\nmany training passages for ﬁne-tuning. Sugges-\ntions to address those issues include decoupling the\nknowledge memorization and QA ﬁne-tuning, and\nforcing the model to recall relevant knowledge in\nthe answer generation step.\nHsu et al. (2021) applies answer generation to\nthe problem of answer selection, in which the sys-\ntem must choose the correct answer from a pro-\nvided candidate set (it is also provided the question).\nInstead of training an answer selector (Han et al.,\n2021a), Hsu et al. (2021) uses answer generation\nthrough ﬁne-tuning PLMs such as T5 and BART,\nwhich consume the input question and the top an-\nswer candidates, then generate an answer for the\nquestion. To prepare training data for ﬁne-tuning,\nthe output answers might come from human anno-\ntators or be directly inherited from the provided\ncorrect answer (i.e. the correct answer will be re-\nmoved from the input for the generative models\nand maybe replaced by another answer candidate).\n4.4 Filling templates\nFor many extraction tasks, the output are spans or-\nganized into one or several templates. For example,\nevent extraction tasks require a system to extract\ntemplates in the form of who did what to whom\nwhere and when.\nA template deﬁnes the appropriate relationship\nand order for the spans and labels for generation,\nforming the output sequence y. Du et al. (2021b)\nexplores the template ﬁlling idea for an IE task:\ngiven a document, a model must identify event\ntemplates/types (via trigger words) and entity men-\ntion ﬁllers for the argument roles. A sequence-\nto-sequence model for template ﬁlling takes the\npossible event types concatenated with words in\nthe input document xas the input, and outputs a\nsequence of tuples. Each tuple corresponds to a\ndetected event template, starting with an event type\nand followed by the text span ﬁllers for the roles in\nthe input document (following an order). The roles\nwith no ﬁllers are associated with null. (Zhang\net al., 2021a) also examines a similar approach of\ntuple generation for ABSA.\nThe template ﬁlling methods can also introduce\nadditional information into the templates to aid the\nlabel generation process, such as natural descrip-\ntions or deﬁnitions of the labels. In particular, Li\net al. (2021c) pursue a general template ﬁlling ap-\nproach for document-level event argument extrac-\ntion: given an event trigger in an input document,\nﬁnd entity mentions to ﬁll in the roles for the event.\nA conditional generative model (e.g. BART) is\nemployed for argument extraction where the input\n(the condition) to the model is created by combin-\ning an unﬁlled template and the document context.\nThe template is essentially a sentence describing\nthe event type augmented with placeholders for\nargument role ﬁllers. The output sequence yis a\nﬁlled template where placeholders are replaced by\nconcrete arguments (entity mentions). To avoid\nentity type mismatch for arguments, the templates\nin the inputs are also appended with sentences to\nindicate entity types for arguments (e.g. arg1 is a\nperson) that can be used to re-rank the output se-\nquences to follow the type constraints. Below is an\nexample input xin which a template over a list of\nevent arguments arg1,...,arg 6 and the document\ntext DOC TEXT are concatenated, and output y,\nin which the underlined text spans are ﬁllers from\nDOC TEXT) from Li et al. (2021c):\nx = ⟨s⟩ ⟨arg1⟩ bought, sold,\nor traded ⟨arg3⟩ to ⟨arg2⟩ in\nexchange for ⟨arg4⟩ for the\nbenefit of ⟨arg5⟩ at ⟨arg6⟩\n21\nplace. ⟨s⟩ ⟨/s⟩ DOC TEXT ⟨/s⟩\ny = Elliott bought, sold or\ntraded truck to McVeigh in\nexchange for 280.32 for the\nbenefit of ⟨arg⟩ at body shop\nplace.\n4.5 Generating Structure-Linearized Texts\nStructure prediction problems in NLP typically re-\nquire multiple prediction outputs for an input text\nx that are interconnected to form a single struc-\nture that represents the input. To cast structure\nprediction tasks as text generation problems, one\napproach involves linearizing the output structure\nto serve as the output sequencey. For example, tak-\ning xas input, TEXT2EVENT (Lu et al., 2021b)\ndirectly generates the event structures y:\nx = The man returned to Los\nAngeles from Mexico following\nhis capture Tuesday by bounty\nhunters.\ny = ((Transport returned\n(Artifact The man)\n(Destination Los Angeles)\n(Origin Mexico)) (Arrest-Jail\ncapture (Person The man)\n(Time Tuesday) (Agent bounty\nhunters))\nGraph traversal algorithms are often used to ac-\ncomplish the linearization in this approach. Ren\net al. (2021) study structure-linearization for joint\nentity and relation extraction (Li et al., 2014; Miwa\nand Bansal, 2016). The main idea is to construct\nan information graph for each input sentence to\ncapture entity mentions, their entity types, and\nrelations. Depth or breath ﬁrst traversal can be\nused for graph linearization for y. To solve the\nsequence-to-sequence problem for pairs of ⟨x,y⟩,\nRen et al. (2021) linearize the information graph\nto an alternating sequence of nodes and edge types\n(given depth/breath ﬁrst traversal), and directly gen-\nerate such sequences via a hybrid span decoder that\ndecodes both the spans and the types recurrently.\nFor event extraction with joint extraction of event\ntriggers and arguments (Li et al., 2014; Nguyen\net al., 2016), a structure-linearization and text gen-\neration approach comes from Lu et al. (2021b).\nThe authors ﬁrst build a labeled tree to capture\nthe event types and argument roles in the sentence\n(i.e. event schema), with trigger and argument text\nspans as leaves. The labeled tree is transformed\ninto the output sequence y by depth-ﬁrst traver-\nsal where T5 is used to perform the conditional\ngeneration of yfrom x. To improve the model, a\ntrie-based constrained decoding procedure (Chen\net al., 2020a; De Cao et al., 2021) is introduced to\nensure the generation of valid event structures. A\ntrie (preﬁx-tree) determines possible candidates for\nthe next generation step given the previously gen-\nerated tokens to guarantee valid output sequences.\nLu et al. (2021b) also report the effectiveness of\nthe generation-based model for extending models\nto extract new event types.\n4.6 Ranking Input-Output Pairs\nSome NLP tasks require choosing the best response\nfrom among many: answer selection in multiple\nchoice-sytle QA, information retrieval, and certain\nkinds of entity retrieval all provide a set of can-\ndidate answers to a posed query from which the\nsystem selects the best one. Typically, a system will\nrank the candidates in relation to the input query,\na task at which PLMs can excel. The idea has its\nroots in the classical literature on probabilistic mod-\nels for information retrieval that rank documents\nusing language models (Ponte and Croft, 1998; Laf-\nferty and Zhai, 2001). Given an input query, a can-\ndidate document is scored in two steps: (i) training\na language model on the candidate document, and\n(ii) computing the likelihood of generating the in-\nput query from that language model, which serves\nas the candidate’s ranking score.\nWe now see the use of PLMs to per-\nform generation-based ranking for selection.\nNogueira dos Santos et al. (2020) apply the idea for\nanswer selection by ﬁne-tuning generative models\n(GPT-2 or BART) over ⟨answer, question⟩pairs,\nthus learning to generate questions given correct\nanswer passages. The simplest approach is to\nﬁne-tune the models over only the positive pairs.\nNogueira dos Santos et al. (2020) also explore ﬁne-\ntuning with negative pairs using an unlikelihood\nobjective or ranking-based objective (e.g. the hinge\nloss). At inference time, the ranking score for an\ninput passage is obtained via the likelihood of the\nﬁne-tuned PLM over the input question condition-\ning on that passage.\nNogueira et al. (2020) approach the document\nrelevance ranking problem in a similar way. The pa-\nper concatenates the input query and each candidate\ndocument and feeds them as an input/condition for\n22\na ﬁne-tuned T5 model. To ﬁne-tune T5, the model\nis asked to generate “True” or “False” as the output\nsequence, indicating the document’s relevance to\nthe query. The probability of generating “True” is\nused as the ranking score for the candidate.\nDe Cao et al. (2021) address the entity retrieval\nproblem: given a set of Wikipedia articles repre-\nsenting entities, return the entity that is most rel-\nevant to a textual input source x. Each entity is\nrepresented by its textual representation (e.g. the\ntitle of its Wikipedia article), which will be used\nas the output sequence yfor the generative models.\nBART is ﬁne-tuned to rank the entities using the\ngeneration likelihood P(y|x). Cui et al. (2021) ex-\nplore generation-based ranking for NER, especially\nin few-shot and cross-domain few-shot settings.\nGiven an input sentence and a text span, a template\nis formed by concatenating the words in the span\nand an expression of type “is a entity type entity”.\nThe original sentence and the template serve as an\ninput-output pair in sequence-to-sequence models.\nBART is then employed to score this pair (using the\nprobability of the template output produced by the\ndecoder of BART). For each span, the entity type\ncorresponding to the template with highest score\nis selected. Original NER training data is used to\ncreate gold standard templates to ﬁne-tune BART.\nIn addition to question answering, other genera-\ntive tasks have been shown to beneﬁt from PLMs.\nFor instance, semantic parsing, generating a struc-\nture representing the semantics of the sentence, is\nexplored in a recent work by Shin et al. (2021).\nAuthors show that by reformulating the output of\nPLMs the generated natural language can be used\nto recover the semantic structure of the input text.\nThey use GPT-3 in the experiments.\n5 Data Generation via PLM\nIn addition to using PLMs to perform NLP tasks\ndirectly, PLMs can be used to generate data that\ncan be used to enhance the performance of NLP\nsystems in two ways. Note that these data gener-\nation approaches are complementary to the three\nparadigms of PLM-for-NLP discussed in previous\nsections.\nFirst, data generated by PLMs can be combined\nwith original training data to improve NLP mod-\nels where training data is too sparse. Typically,\nthis is applied to create new labeled data to in-\ncrease diversity, enrich the models, and otherwise\nalleviate common limitations of hand-labeled data.\nThe studies presented below discuss, for various\ndownstream NLP tasks: approaches for ﬁne-tuning\nPLMs to ensure they capture the key characteristics\nof the task when performing data generation; ap-\npropriate reformulation of the original training data\nfor PLM ﬁne-tuning and generation; and ﬁltering\nthe new data for noise introduced by the generation\nprocess.\nSecond, we discuss the use of auxiliary data\ngenerated by PLMs to shed light on interesting\naspects of NLP models. This approach plays a role\nin machine learning explainability by providing\ngenerations such as counterexamples, clarifying\nquestions, context for answers, inference rules, and\nother insight-rich sequences.\n5.1 Augmenting NLP Models with\nAutomatically Generated Data\nTraditional approaches to data augmentation, in-\ncluding generation via semi-supervised learning on\nlarge unlabeled data sets and synthesis with back-\ntranslation or synonymous word replacement (Feng\net al., 2021; Chen et al., 2021a) were shown to be ef-\nfective for increasing NLP models’ accuracy and/or\ncoverage. Newer studies show that PLMs can be\nalso used as an effective method for data augmen-\ntation (Zhang et al., 2020a; Yang et al., 2020; Peng\net al., 2020; Kumar et al., 2020; Anaby-Tavor et al.,\n2020), requiring no signiﬁcant change to the model\narchitecture. The ﬂuency of PLM text generations\nstand in contrast to the outcomes of traditional ap-\nproaches that may produce less natural samples. As\ndiscussed in previous sections, the massive amount\nof linguistic knowledge accumulated by the PLM\nallows for adaptation to many domains and tasks,\nincluding those with very limited labeled data. The\nvast knowledge may also produce a greater vari-\nety of new examples, further improving the NLP\nmodels trained on them.\nWe organize the discussion of data augmentation\nmethods according to the NLP tasks they support.\n5.1.1 Information Extraction (IE)\nPrior works explored synthetic data generation with\nPLMs (Madaan et al., 2020; Bosselut et al., 2019)\nfor a variety of IE tasks.\nVeyseh et al. (2021a) and Veyseh et al. (2021b)\nuse GPT-2 to produce synthetic labeled data for\nevent detection. Sentences in existing training\ndatasets are augmented with markers to indicate\npositions of event trigger words. The resulting la-\nbeled sentences are used to ﬁne-tune GPT-2 us-\n23\ning the standard autoregressive next word pre-\ndiction (NWP) objective. Veyseh et al. (2021a)\nshows that the ﬁne-tuned GPT-2 model can gener-\nate label-augmented data for different domains (e.g.\nnewswire, cybersecurity); however, the generated\ndata might include some noise, for instance, incor-\nrect grammar, meaningless sentences, or incorrect\nannotations. To minimize the impact of the noisy\ngenerated examples and maximize the beneﬁts of\nthe generated data, Veyseh et al. (2021a) and Vey-\nseh et al. (2021b) present a student-teacher network\nframework: the teacher network is trained on the\noriginal labeled data to obtain anchor knowledge,\nwhile the student is trained over the combination\nof original and synthetic data, with constraints in-\ntroduced to enforce consistency with the teacher’s\nlearned anchor knowledge. The framework leads\nto signiﬁcant performance improvement over dif-\nferent datasets for event detection.\nGuo and Roth (2021) employ GPT-2 to generate\nsynthetic labeled data for cross-lingual NER fol-\nlowing the annotation projection approach: training\ndata in a source language is translated and projected\ninto a target language to train models. To project an-\nnotation, a training sentence in the source language\nis ﬁrst translated into the target language using\nword-to-word translation (via a dictionary). GPT-2\nis then ﬁne-tuned to generate complete sentences\nfrom the important words in target languages. A\nhard-constrained generation mechanism is also en-\ncoded into the decoding process of GPT-2 to ensure\nthe appearance of the named entities in the origi-\nnal source sentence in the automatically generated\nsentences.\nSynthetic data generation with GPT-2 is also ex-\nplored for relation extraction in Papanikolaou and\nPierleoni (2020). This paper ﬁne-tunes GPT-2 over\nlabeled examples of the same relation type, where\neach sentence in the training data is marked with\nthe two entity mentions in the corresponding rela-\ntion. The ﬁne-tuned model for each relation type\nis then leveraged to produce new training instances\nfor that relation.\n5.1.2 Question Answering (QA)\nGiven an input paragraph C and a sampled ex-\ntractive short answer Ain C, Alberti et al. (2019)\nattempts to generate a questionQusing a sequence-\nto-sequence Transformer (with BERT as its en-\ncoder). The triple, consisting of the input para-\ngraph, the generated question, and the sampled\nanswer (C,Q,A ), can be used as a new training\ninstance for QA models. To mitigate the noise in\nthe generated data, Alberti et al. (2019) present a\nround trip consistency approach where a second\ngenerative model is trained to take the input pas-\nsage C and generated question Qfrom the prior\nstep to produce an answer A′. The tuple (C,Q,A )\nis only retained as new training data if A′== A.\nFollowing a similar principle, Shakeri et al.\n(2020) explore synthetic data generation for cross-\ndomain QA where models trained on a source do-\nmain (typically SQuAD) are evaluated on datasets\nfrom a different target domain. The paper aims to\ngenerate QA pairs in the target domain and com-\nbine them with the source-domain training data to\ntrain improved QA models. The data generation\nmodel is also trained on the source domain dataset\nSQuAD using BART and GPT-2. Starting with a\npassage as the context, the generative models di-\nrectly generate QA pairs. Generated QA pairs are\nﬁltered by the likelihood scores of the generative\nmodels to reduce noise.\nThe data generation idea is extended to multi-\nhop QA that requires combining disjoint pieces of\nevidence to answer a question. In particular, Pan\net al. (2021b) aim to generate human-like multi-hop\nquestion–answer pairs to train QA models. The\nmodel consists of three components: operators,\nreasoning graphs, and question ﬁltration. Opera-\ntors are atomic operations that are implemented by\nrules or off-the-shelf pretrained models to retrieve,\ngenerate, or fuse relevant information from input\ncontexts. Approaches to fusing relevant informa-\ntion from across contexts include: ﬁne-tuning a\nT5 model on SQuAD to generate single-hop ques-\ntions; generating descriptions of table entities with\nGPT-TabGen (Chen et al., 2020b); and combining\nsingle-hop questions with sentences about the same\nentities to produce multi-hop questions via ﬁlling\nin masked tokens of designed templates. Reason-\ning graphs then deﬁne different types of reasoning\nchains for multi-hop QA using the operators as\nbuilding blocks. Training QA pairs are generated\nby executing the reasoning graphs, which generate\noutput texts. Finally, question ﬁltration removes\nirrelevant and unnatural QA pairs to produce the\nﬁnal generated training set for multi-hop QA. The\nﬁltration is done by choosing the samples ranked\nas most ﬂuent by GPT-2, and paraphrasing each\ngenerated question using BART.\n5.1.3 Sentiment Analysis (SA)\nYu et al. (2021a) applies data augmentation for\n24\naspect-based SA in the unsupervised domain adap-\ntation setting, aiming to transform labeled datasets\nin a source domain to a new target domain. The\nmain approach involves two steps. In the ﬁrst step\nof domain generalization, domain-speciﬁc words\nand phrases in the labeled source data and un-\nlabeled target data are identiﬁed and masked in\nthe inputs. Opinion words for the source domain\nand target-speciﬁc terms and opinion words are\nretrieved via sentiment lexicon and bootstrapping\nmethods using relations in dependency trees. The\ntarget-speciﬁc terms in the unlabeled data will be\nmasked to ﬁne-tune BERT. In the second step of\ndomain speciﬁcation, the source-speciﬁc terms in\nthe source data are masked (thus producing domain-\nindependent texts) and sent into the ﬁne-tuned\nBERT to produce labeled sentences in the target do-\nmain. Here, some constraints based on dictionaries\nare necessary to ensure that the inﬁlled words are\nterms or opinion words with the same sentiment\npolarity. The generated data can be used indepen-\ndently or combined with original source training\ndata to train a SA model for the target domain.\nLi et al. (2020b) use PLMs to generate synthetic\ndata for aspect term extraction (cast as a sequence\nlabeling problem). To ﬁne-tune PLMs with the\nsequence-to-sequence framework for this purpose,\nthe input includes a masked sentence from a train-\ning dataset and the corresponding label sequence\nwhile the output are the masked tokens in the input.\nThe ﬁne-tuned PLMs are then exploited to generate\nnew possibilities for the masked tokens that can be\ninjected into the masked input, using the original\nlabel sequence to obtain synthetic labeled data to\ntrain models.\n5.1.4 Fact Veriﬁcation\nFact veriﬁcation aims to predict whether a given\nclaim is supported, denied, or unresolved based on\nthe given evidence. Automatically generated texts\ncan be used to generate claim-evidence pairs for\neach label category. To this end, Pan et al. (2021a)\nemploy a two-step approach to generate synthetic\ndata for fact veriﬁcation. In the ﬁrst step of ques-\ntion generation, given the evidence and an answer,\na BART model, ﬁne-tuned on the SQuAD dataset\nusing the similar input-output format, generates a\nquestion for that answer. Next, a question-to-claim\nmodel is employed to take the question and answer\nas inputs and generate a claim (also using a BART\nmodel ﬁne-tuned on SQuAD). To produce ⟨claim,\nevidence⟩pairs with the “support” relation, an en-\ntity is selected in the original evidence in the ﬁrst\nstep of the process. To produce a “refute” claim,\nthe work replaces the original answer with another\nentity in the generation process. Finally, to create a\n“not-enough-evidence” claim, the paper expands the\noriginal evidence to include other paragraphs in the\nsame document and produce claims for some en-\ntity in the extended paragraph. Experiments show\ncompetitive results when the augmented data is\ncombined with few or even no human-labeled ex-\namples for model training.\n5.1.5 Document Classiﬁcation\nA typical approach to generating synthetic data\nfor text classiﬁcation is to build a conditional gen-\nerative model for each class by ﬁne-tuning with\nlabeled data from that class. While these models\ncan be ﬁne-tuned with the next word prediction ob-\njective with generative PLMs such as GPT-2, Liu\net al. (2020b) use reinforcement learning to train\ngenerative models to augment text classiﬁcation\nlabeled data. The rewards for training are based on\nthe similarity between the generated tokens and a\nsalient lexicon of the target class computed via top\nfrequency-based salient words, and the divergence\nbetween the conditional and unconditional models.\nLiu et al. (2020b) demonstrate the effectiveness of\nusing the automatically generated data in multiple\ntext classiﬁcation problems and datasets, including\nsentiment analysis and offense detection.\n5.2 Generating Auxiliary Data to Improve\nDifferent Aspects of NLP Models\nThe following sections, again arranged by task, dis-\ncuss ways of using PLM-generated text to aid in\nauxiliary tasks, helping developers or users under-\nstand model strengths and weaknesses or decision-\nmaking characteristics.\n5.2.1 Explaining Models’ Decisions\nDespite the impressive performance of deep learn-\ning models for various NLP tasks, a remaining\nchallenge to widespread adoption is the lack of\nexplanations for the models’ decisions. This hin-\nders the development and debugging process, as\nwell as user trust. This is especially true for appli-\ncation domains such as healthcare, security, and\nonline education. As such, a considerable number\nof approaches have been proposed for explaining\ndeep learning models’ behavior, including model-\nintrinsic (Ribeiro et al., 2016; Lundberg and Lee,\n2017; Chen et al., 2018) and model-agnostic ap-\n25\nproaches (Park et al., 2018; Kim et al., 2018; Ling\net al., 2017). While model-intrinsic explanations\nexpose internal model state (e.g. feature impor-\ntance or attention scores), in model-agnostic (post-\nhoc) methods, explanations are generated via the\nmodel predictions without inspecting the internal\nstate. Generative models are often applied for post-\nhoc explanations, aiming to obtain either counterex-\namples (Kim et al., 2016; Wachter et al., 2018; Wu\net al., 2021a) or natural language texts (Camburu\net al., 2018; Kumar and Talukdar, 2020; Chen et al.,\n2021c) for explaining purposes.\nGenerating counterexamples can shed light\non the decision boundaries of the models (i.e.\nexplaining when a model changes its decision),\nthus improving intepretability. To this end, the\ngenerated counterexamples should be close to the\ndecision boundaries so that small modiﬁcations\nresult in changing the model predictions. Tradi-\ntionally, heuristic rules applied to the original\ninputs create likely counterexamples (Wachter\net al., 2018; Ribeiro et al., 2018; Iyyer et al., 2018;\nLi et al., 2021a). PLMs have been leveraged\nto generate more diverse examples for better\nevaluation (Madaan et al., 2021b; Wu et al.,\n2021a; Ross et al., 2021). In particular, Wu et al.\n(2021a) proposes a method based on GPT-2 to\ngenerate counterfactuals that are close to the\noriginal sentences and entail speciﬁc relationships\nwith the original, facilitating label induction (e.g.\nnegation, insertion, shufﬂe). Concretely, an input\nsentence is concatenated with a relation label\n(e.g. negation) and a template consisting of the\nspecial tokens [BLANK] to form the prompt for\nGPT-2 model. For instance, for the sentence “It is\ngreat for kids” and the relation label “negate”,\nthe following prompt is constructed: “ It is\ngreat for kids. [negation] It is\n[BLANK] great for [BLANK]. [SEP]”.\nNext, the GPT-2 model generates answers\nfor the [BLANK] in the template (e.g. “ not\n[ANSWER] children”, separated by the\nspecial token [ANSWER]). To ﬁne-tune the GPT-2\nmodel, non-parallel datasets (e.g. CommonGen,\nNatural Questions and SQuAD) are automatically\nprocessed to ﬁnd the relations between pairs of\nsentences and to construct the templates for each\nrelation based on the obtained pairs. It is worth\nnoting that the sentences generated by GPT-2\nmight have the same label as the original input\nsentence. In addition, Wu et al. (2021a) show that\nthe generated counterexamples can be helpful to\nimprove the performance of the downstream mod-\nels, e.g. for natural language inference, duplicate\nquestion detection, and sentiment analysis.\nOther research is informing the task of natural\nlanguage explanation generation, where the goal is\nto expose the rationale behind the model decisions\nin automatically generated natural language text.\nAny approach must critically require that the gen-\nerated response is faithful to the model behavior.\nTo this end, Kumar and Talukdar (2020) propose to\nﬁrst generate the explanations, and then employ the\nexplanations to obtain the ﬁnal model predictions.\nThey use natural language inference as the task re-\nquiring explanations. Label-speciﬁc GPT-2 models\nare ﬁne-tuned over concatenations of correspond-\ning premises, hypotheses, and human-provided ex-\nplanations, so that at inference, the model generates\nan explanation based on premise and hypothesis.\nNext, the explanations together with the premise\nand the hypothesis are consumed by an explanation\nprocessor model (e.g. RoBERTa) to select the most\nlikely label. This process obtains a more faithful\nexplanation for the label choice, compared to tradi-\ntional prediction-ﬁrst approaches (Camburu et al.,\n2018). However, this approach does not provide\nexplanations that reference non-selected labels. To\naddress the question of why other labels are not cho-\nsen, Chen et al. (2021c) exploit counterexamples,\nderiving them from original samples with heuristic\nrules. The original samples and counterexamples\nare provided to GPT-2 to generate an explanation\nfor the question “Why A not B”.\n5.2.2 Knowledge Extraction\nGenerative PLMs are pre-trained on massive text\ncorpora containing a large amount of information\nabout entities and commonsense knowledge. As\nsuch, PLMs might directly be used to elicit knowl-\nedge required for downstream applications such\nas information extraction, sentiment analysis and\nquestion answering. To this end, it is important\nto properly prompt these models so their outputs\ncontain the required information. Section 3.2 de-\nscribes the prompt design for knowledge extrac-\ntion/probing tasks, and in particular, the “Knowl-\nedge Probing” subsection describes applications in\ndetails. Here we focus on the text generation aspect\nof knowledge extraction approaches.\nPrior works can be categorized into two sub-\ncategories. The ﬁrst category involves prompting\nPLMs with partial knowledge via a prompt and\n26\nasking the models to complete the prompt. Specif-\nically, pre-deﬁned templates can be designed and\nﬁlled with partial knowledge (e.g. the two entities\ninvolved in a relation) and the generative PLMs can\npredict the missing words in the templates (e.g. the\nrelation type between the two entities.) The tem-\nplates can be ﬁxed (Goswami et al., 2020) or they\ncan be dynamically constructed by a pre-trained\nmodel (Shin et al., 2020) (further details are in Sec-\ntion 3.2). The second category instead proposes\nto prompt the PLMs with full knowledge and ask\nthe models to generate a natural language text to\ndescribe that knowledge. This task is known as\nData-to-Text (Kukich, 1983), and the goal is to\nobtain a textual description of existing knowledge\nbases. The generated textual descriptions can be\nused by downstream applications such as knowl-\nedge probing (Petroni et al., 2019) or QA (Agarwal\net al., 2021), among others. Agarwal et al. (2021)\nintroduce a model based on T5 to convert Wiki-\ndata knowledge graphs (with triples of relations be-\ntween two entities) into textual data. The proposed\napproach consists of three stages. First, create a\nlarge but noisy training dataset using distant super-\nvision for relation extraction by aligning knowl-\nedge base (KB) triples to Wikipedia texts. Next,\nﬁne-tune T5 in stages, starting with the distantly\nsupervised dataset for better coverage, then moving\non to a small clean dataset for less hallucination.\nThe model learns to generate descriptive sentences\nfrom KB triples. Last, build a ﬁlter for the gener-\nated texts based on semantic quality with respect\nto the KB triples by scoring the concatenation of\ninput and output with BERT.\n5.2.3 Question Generation\nWhile PLMs can be directly used for generating\nanswers for questions, they might be also help-\nful to support existing QA systems. Speciﬁcally,\nPLMs can be employed to provide clariﬁcation for\ndownstream QA systems. The clariﬁcation can be\nrealized in terms of question clariﬁcation when the\nquestion is ambiguous or it can be fulﬁlled by pro-\nviding more context. For instance, in Gao et al.\n(2021b) and Min et al. (2020), multi-step question\ngeneration approaches are proposed for ambiguous\nQA in which the BART model is prompted with an\nambiguous question and the top similar passages\nretrieved in a document to generate candidate an-\nswers. If multiple answers are generated, another\nBART model is employed to generate a disambigua-\ntion question for each answer. The newly generated\nquestions are later used to extract other candidate\nanswers. Finally, the generated answer-question\npairs are ranked to select the top one for the am-\nbiguous QA problem. Min et al. (2020) show that\nthe process of generating auxiliary disambiguation\nquestions could further help the models to encode\nthe interactions between the original input question\nand the candidate answers.\nIn another line of work, Mao et al. (2021) seek\nto generate clariﬁcation texts for input questions\nto improve the retrieval quality in open-domain\nQA (answering factoid questions without a pre-\nspeciﬁed domain). The most common approach\nfor this problem involves a retriever-reader archi-\ntecture (Chen et al., 2017), which ﬁrst retrieves a\nsmall subset of documents in the pool using the\ninput question as the query and then analyzes the\nretrieved documents to extract (or generate) an an-\nswer. To generate augmented texts for the input\nquestion in the ﬁrst retrieval component, Mao et al.\n(2021) ﬁne-tune BART to consume the input ques-\ntion and attempt to produce the answer and the sen-\ntence or title of the paragraph containing the answer.\nThis method demonstrates superior performance\nfor both retrieval and end-to-end QA performance.\nIn addition to clariﬁcation information, PLMs\ncan also be used to paraphrase questions to support\nQA models. Mass et al. (2020) explore the problem\nof FAQ retrieval, retrieving the top QA pair given\na user query. Based on the returned QA pairs (q,a)\nfrom a retrieval system, this work proposes an un-\nsupervised method to re-rank the pairs to improve\nthe performance. One of the ranking scores is a\nmatching score between the question pin the pair\n(q,a) with respect to the user question. A triple\nnetwork is trained over the tuples (p,q,q ′), where\nqis a paraphrase of the question pwhile q′is ran-\ndomly selected questions from other QA pairs. To\nthis end, Mass et al. (2020) ﬁne-tune GPT-2 over\nthe concatenations of the corresponding answers\nand questions in the FAQ. The ﬁne-tuned GPT-2\nis then prompted with the answer ato produce a\nparaphrase q′for qin the ranking network.\n5.2.4 Inference Rule Generation\nFor some applications, it is important to understand\nthe process by which the ﬁnal predictions of the\nmodels are obtained. These intermediate inference\nrules provide are another form of model explana-\ntion and provide insights for improving model per-\nformance.\n27\nPaul and Frank (2021) exploit GPT-2 to perform\nnarrative story completion: given a few sentences\nof a story, the goal is to complete the story us-\ning sentences that logically follow the narrative\nin the given incomplete story. In an incremental\ngeneration method, each step seeks to generate a\ncontextualized inference rule conditioned on the\ncurrent incomplete story. To accomplish this, GPT-\n2 is ﬁne-tuned on human annotation of story line\ninferences. Next, given the current story and gener-\nated inference rule, a new sentence for the story is\ngenerated (using another ﬁne-tuned GPT-2 model).\nBy interspersing the inference rules, the storyline\ngenerations should create a coherent story that fol-\nlows logical connections and causal relationships\nbetween events.\nMadaan et al. (2021a) employ T5 to generate in-\nference graphs for defeasible inference (Rudinger\net al., 2020). In this mode of reasoning, given a\npremise, a hypothesis may be weakened or over-\nturned in light of new evidence. As training in-\nference graphs for this problem requires a large\namount of human-annotated inference graphs, they\npropose to exploit reasoning graphs in related tasks\nto ﬁne-tune T5. In particular, this work leverages\nthe inﬂuence graphs in the WIQA dataset that in-\ncludes a set of procedural passages, each accompa-\nnied by a human-curated inﬂuence graph. The in-\nﬂuence graphs are linearized to ﬁt into the seq2seq\nframework for ﬁne-tuning T5 and producing infer-\nence graphs for defeasible inference afterward. It\nhas been shown that the generated inference graphs\ncan improve human accuracy on defeasible infer-\nence (which is originally challenging for humans).\n6 Discussion\nMix of paradigms or PLMs. The three\nparadigms presented in this paper are by no means\nmutually exclusive. Instead, it is not rare to see\napproaches that use two or three paradigms to-\ngether: ﬁne-tuning techniques are often used as part\nof prompt-based methods; NLP-as-text-generation\napproaches often use carefully crafted templates\n(prompts); and prompt-based learning often lever-\nages the text generation capabilities of PLMs to\ngenerate words, phrases, or sentences.\nA representative example is Khashabi et al.\n(2020), which combined three paradigms: appro-\npriate prompts from the context and questions help\nto formulate several QA tasks into a uniﬁed text\ngeneration problem with seq2seq-based pre-trained\nmodels such as T5, with model ﬁne-tuning to im-\nprove performance in several QA tasks.\nAs independently trained models, PLMs are also\nby no means mutually exclusive. For example,\nACE (Wang et al., 2021c) shows that combining\nmultiple PLMs (e.g ELMo, BERT, mBERT, XLM-\nR) yields further improvements over using a single\nPLM for a range of NLP tasks. Investigation of\nthe complementarity of different PLMs is a future\nresearch direction.\nFrom another perspective, the design of the train-\ning for MLMs has been driven by the results on the\nﬁne-tuning paradigm, but it is not clear whether an\nexploration of different training objectives could\nlead to PLMs that are more effective when used\nwith prompting or generation to solve NLP tasks.\nHow much unlabeled data is needed? While\nPLMs are usually trained on billions of words,\nsome works have investigated what can be learned\nwith less pre-training data. Zhang et al. (2021b),\nexperimenting on RoBERTa models trained on 1M,\n10M, 100M and 1B words (Warstadt et al., 2020b,\nMiniBERTas), showed that 10M to 100M words\nare sufﬁcient to acquire many syntactic and se-\nmantic features. Huebner et al. (2021) presented\nBabyBERTa, a RoBERTa-based model trained on\nlanguage acquisition data that acquires grammat-\nical knowledge comparable to that of pre-trained\nRoBERTa-base – and does so with approximately\n15x fewer parameters and 6,000x fewer words. On\nthe other hand, Zhang et al. (2021b), using the pre-\ntrain then ﬁne-tune paradigm for NLU tasks, found\nthat millions of words are not sufﬁcient for key\nNLU skills, which instead may require billions of\nwords and continue improvements with additional\npre-training data.\nHow much labeled data is still needed? While\nLe Scao and Rush (2021) present experiments to\nquantify the impact of prompts, there has been little\nwork in designing rigorous experiments to study\nhow many labeled examples are required by PLMs\nto achieve various levels of performance for a range\nof NLP tasks, and using each of the three paradigms\noutlined in this survey. Such studies will provide a\nbetter understanding of the pros and cons of each\nformulation, including cost-beneﬁt analyses weigh-\ning the impact of more labeled data, helping devel-\nopers design NLP systems that achieve the desired\ngoal while minimizing human labeling effort.\n28\nCan we reduce the amount and cost of compu-\ntation? The development of deep learning in gen-\neral and the use of PLMs in particular have dramat-\nically increased the amount of computation used\nin NLP, leading to a high environmental footprint.\nSchwartz et al. (2020) argue for Green AI, suggest-\ning that we should consider efﬁciency, measured\nby the number of ﬂoating-point operations used to\ngenerate a result, as a main evaluation criterion, to-\ngether with accuracy. Green AI also aims to reduce\nthe ﬁnancial cost of the computation. In line with\nthis approach, Izsak et al. (2021) propose software\noptimization and design choices for pre-training\nBERT in 24 hours using a single low-end deep\nlearning server.\nDo PLMs excel at semantic understanding or\nmemorization? Another interesting avenue to\nexplore is separating extraction or text under-\nstanding from memorization. To what extent can\nPLMs memorize facts and extract an answer from\na passage provided (understanding a text), for\nknowledge-intensive tasks such as Questions An-\nswering (QA) and Information Retrieval (IR)? This\nis motivated by the observation by Wang et al.\n(2021a) that PLMs are terrible at remembering\ntraining facts with high precision and that it is also\nchallenging for them to answer closed-book ques-\ntions even if relevant knowledge is retained.\nIs explicit linguistic information needed? A re-\nlated debate is whether a symbolic annotation cov-\nering syntax or semantics should be integrated to\nimprove the performance of a PLM-based system,\nor whether this information is already present in the\nmodel. Below we list some successes in leverag-\ning syntax or semantics, though there is no def-\ninite answer yet. In terms of syntax, Xu et al.\n(2021) utilize automatically produced syntax in\nboth the pre-training and ﬁne-tuning stages, and\nshow improved performance on several benchmark\ndatasets. Nguyen et al. (2020b) and Sachan et al.\n(2021) inject syntax only in the ﬁne-tuning stage.\nRegarding semantics, Zhang et al. (2020d) incor-\nporate Semantic Role Labeling predictions into\nthe pre-training procedure of BERT, improving the\nperformance on textual entailment and QA tasks.\nWu et al. (2021b) integrate semantic information\ninto the task-speciﬁc ﬁne-tuning stage, focusing on\nthe DELPHIN dependencies formalism or “DM”\n(Ivanova et al., 2012). Experimenting on RoBERTa,\nthey obtained improvements on the GLUE bench-\nmark. Syntax and semantics can also be jointly in-\ntegrated, as in Zhou et al. (2020a), where multi-task\nlearning was used to combine BERT pre-training\nwith both semantic and syntactic parsing tasks, im-\nproving the performance on the GLUE benchmark.\nCan we integrate implicit semantic information\nusing QA? Instead of enriching PLMs with sym-\nbolic annotations, a possible alternative for a su-\npervision signal is QA data, as it is easier to an-\nswer questions relative to a sentence than to an-\nnotate linguistic phenomena in it (Roth, 2017; He\net al., 2020). In the s-QuASE PLM presented in He\net al. (2020), further pre-training of BERT on QA\ndatasets is done while restricting the interaction be-\ntween the question and context inputs. s-QuASE is\nparticularly useful in single-sentence tasks such as\nSemantic Role Labeling and NER. A similar direc-\ntion was pursued by Jia et al. (2021) who leveraged\nquestion generation and knowledge distillation to\nbuild a QA-based pre-training objective.\nDo PLMs need meaningful prompts? The suc-\ncess of prompts in zero- and few-shot learning has\nbeen attributed to the prompts serving as instruc-\ntions that allow the PLM to learn with fewer exam-\nples, much the way humans would (Mishra et al.,\n2021; Schick and Sch ¨utze, 2021a; Brown et al.,\n2020). In fact, the excellent results may instead be\nattributable to the mere exploitation of patterns in\nthe training data of PLMs, and not to PLMs’ per-\nceived ability to interpret and follow meaningful\ninstructions. Webson and Pavlick (2021) show,\nfor instance, that irrelevant templates match the\nperformance of meaningful ones in few-shot en-\ntailment experiments, adding that some of the tem-\nplates discovered by automatic generation of dis-\ncrete prompts are also unnatural (Shin et al., 2020).\nIn this sense, the results of continuous prompts also\nshow that PLMs do not need meaningful instruc-\ntions for improving few-shot performance.\nTheoretical and empirical analysis The theo-\nretical understanding of the paradigms presented\nin this survey is preliminary. Apart from the issues\nmentioned above, there is a lack of understanding\nof what actually makes these paradigms so suc-\ncessful, and whether their success can be general-\nized across models and languages. For instance,\nprompts may be PLM-dependent, or they may be\ntransferable across models as indicated in (Perez\net al., 2021). There is very little work on study-\ning the generalization of prompting and generation\n29\nacross languages, in the way that transfer learning\nhas been applied to learning in one language and\ntesting in another (Conneau et al., 2020).\n7 Conclusion\nIn this paper, we present a survey of the three trend-\ning paradigms that use pre-trained language models\nfor NLP. We describe each of them in depth, and\nsummarize prior works whose applications have\nshown promise. In addition, we describe the use\nof pre-trained language models to automatically\ngenerate data that is used to improve performance\nin NLP tasks. We hope this survey will provide\nreaders with key fundamental concepts and a com-\nprehensive view of the paradigm shift.\nAcknowledgments\nThis research is based upon work supported in part\nby the Ofﬁce of the Director of National Intelli-\ngence (ODNI), Intelligence Advanced Research\nProjects Activity (IARPA), via Contract No. 2019-\n19051600006 under the IARPA BETTER program\nand by Contracts FA8750- 19-2-0201 and FA8750-\n19-2-1004 with the US Defense Advanced Re-\nsearch Projects Agency (DARPA). Approved for\nPublic Release, Distribution Unlimited. The views\nand conclusions contained herein are those of the\nauthors and should not be interpreted as necessarily\nrepresenting the ofﬁcial policies, either expressed\nor implied, of ODNI, IARPA, the Department of\nDefense or the U.S. Government. The U.S. Gov-\nernment is authorized to reproduce and distribute\nreprints for governmental purposes not withstand-\ning any copyright annotation therein.\nWe would like to thank Paul Cummer for his\ninsightful comments on this work.\nReferences\nOmri Abend and Ari Rappoport. 2013. Universal Con-\nceptual Cognitive Annotation (UCCA). In Proceed-\nings of the 51st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 228–238, Soﬁa, Bulgaria. Association\nfor Computational Linguistics.\nOshin Agarwal, Heming Ge, Siamak Shakeri, and\nRami Al-Rfou. 2021. Knowledge graph based syn-\nthetic corpus generation for knowledge-enhanced\nlanguage model pre-training. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, NAACL-HLT.\nRodrigo Agerri, I˜naki San Vicente, Jon Ander Campos,\nAnder Barrena, Xabier Saralegi, Aitor Soroa, and\nEneko Agirre. 2020. Give your text representation\nmodels some love: the case for Basque. In Proceed-\nings of the 12th Language Resources and Evaluation\nConference, pages 4781–4788, Marseille, France.\nEuropean Language Resources Association.\nChris Alberti, Daniel Andor, Emily Pitler, Jacob De-\nvlin, and Michael Collins. 2019. Synthetic QA cor-\npora generation with roundtrip consistency. In Pro-\nceedings of the 57th Annual Meeting of the Associa-\ntion for Computational Linguistics.\nZeyuan Allen-Zhu and Yuanzhi Li. 2021. Towards\nunderstanding ensemble, knowledge distillation and\nself-distillation in deep learning. arXiv preprint\narXiv:2012.09816.\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly available clini-\ncal BERT embeddings. In Proceedings of the 2nd\nClinical Natural Language Processing Workshop ,\npages 72–78, Minneapolis, Minnesota, USA. Asso-\nciation for Computational Linguistics.\nAsaf Amrami and Yoav Goldberg. 2019. Towards bet-\nter substitution-based word sense induction. arXiv\npreprint arXiv:1905.12598.\nAteret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich,\nAmir Kantor, George Kour, Segev Shlomov, Naama\nTepper, and Naama Zwerdling. 2020. Do not have\nenough data? deep learning to the rescue! In The\nThirty-Fourth AAAI Conference on Artiﬁcial Intelli-\ngence (AAAI).\nBen Athiwaratkun, Cicero Nogueira dos Santos, Jason\nKrone, and Bing Xiang. 2020. Augmented natural\nlanguage for generative sequence labeling. In Pro-\nceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP).\nGeoff Bacon and Terry Regier. 2019. Does bert\nagree? evaluating knowledge of structure depen-\ndence through agreement relations. arXiv preprint\narXiv:1908.09892.\nLivio Baldini Soares, Nicholas FitzGerald, Jeffrey\nLing, and Tom Kwiatkowski. 2019. Matching the\nblanks: Distributional similarity for relation learn-\ning. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics ,\npages 2895–2905, Florence, Italy. Association for\nComputational Linguistics.\nLaura Banarescu, Claire Bonial, Shu Cai, Madalina\nGeorgescu, Kira Grifﬁtt, Ulf Hermjakob, Kevin\nKnight, Philipp Koehn, Martha Palmer, and Nathan\nSchneider. 2013. Abstract Meaning Representation\nfor sembanking. In Proceedings of the 7th Linguis-\ntic Annotation Workshop and Interoperability with\nDiscourse, pages 178–186, Soﬁa, Bulgaria. Associa-\ntion for Computational Linguistics.\n30\nAnkur Bapna and Orhan Firat. 2019. Simple, Scal-\nable Adaptation for Neural Machine Translation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 1538–\n1548, Hong Kong, China. Association for Computa-\ntional Linguistics.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A Pretrained Language Model for Scientiﬁc\nText. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3615–3620, Hong Kong, China. Association for\nComputational Linguistics.\nEyal Ben-David, Nadav Oved, and Roi Reichart. 2021.\nPada: A prompt-based autoregressive approach for\nadaptation to unseen domains. arXiv preprint\narXiv:2102.12206.\nSid Black, Gao Leo, Phil Wang, Connor Leahy, and\nStella Biderman. 2021. GPT-Neo: Large Scale\nAutoregressive Language Modeling with Mesh-\nTensorﬂow.\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, Asli Celikyilmaz, and Yejin Choi.\n2019. Comet: Commonsense transformers for auto-\nmatic knowledge graph construction. arXiv preprint\narXiv:1906.05317.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nSamuel Broscheit. 2019. Investigating entity knowl-\nedge in BERT with simple neural end-to-end en-\ntity linking. In Proceedings of the 23rd Confer-\nence on Computational Natural Language Learning\n(CoNLL), pages 677–685, Hong Kong, China. Asso-\nciation for Computational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nDeng Cai and Wai Lam. 2020. AMR parsing via graph-\nsequence iterative inference. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 1290–1301, Online. As-\nsociation for Computational Linguistics.\nOana-Maria Camburu, Tim Rockt ¨aschel, Thomas\nLukasiewicz, and Phil Blunsom. 2018. e-snli: Nat-\nural language inference with natural language expla-\nnations. In Advances in Neural Information Process-\ning Systems.\nJos´e Ca ˜nete, Gabriel Chaperon, Rodrigo Fuentes, Jou-\nHui Ho, Hojin Kang, and Jorge P ´erez. 2020. Span-\nish pre-trained BERT model and evaluation data. In\nPML4DC Workshop at ICLR.\nIlias Chalkidis, Manos Fergadiotis, Prodromos Malaka-\nsiotis, Nikolaos Aletras, and Ion Androutsopoulos.\n2020. LEGAL-BERT: The muppets straight out of\nlaw school. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020 , pages 2898–\n2904, Online. Association for Computational Lin-\nguistics.\nBranden Chan, Stefan Schweter, and Timo M ¨oller.\n2020. German’s next language model. In Proceed-\nings of the 28th International Conference on Com-\nputational Linguistics, pages 6788–6796, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nWanxiang Che, Longxu Dou, Yang Xu, Yuxuan Wang,\nYijia Liu, and Ting Liu. 2019. HIT-SCIR at MRP\n2019: A uniﬁed pipeline for meaning representa-\ntion parsing via efﬁcient training and effective en-\ncoding. In Proceedings of the Shared Task on Cross-\nFramework Meaning Representation Parsing at the\n2019 Conference on Natural Language Learning ,\npages 76–85, Hong Kong. Association for Compu-\ntational Linguistics.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to answer open-\ndomain questions. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (ACL).\nJiaao Chen, Derek Tam, Colin Raffel, Mohit Bansal,\nand Diyi Yang. 2021a. An empirical survey of data\naugmentation for limited data learning in nlp. arXiv\npreprint arXiv:2106.07499.\nJianbo Chen, Le Song, Martin J. Wainwright, and\nMichael I. Jordan. 2018. Learning to explain: An\ninformation-theoretic perspective on model interpre-\ntation. In Proceedings of the 35th International Con-\nference on Machine Learning (ICML).\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\n31\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welin-\nder, Bob McGrew, Dario Amodei, Sam McCandlish,\nIlya Sutskever, and Wojciech Zaremba. 2021b. Eval-\nuating Large Language Models Trained on Code.\narXiv:2107.03374 [cs]. ArXiv: 2107.03374.\nPinzhen Chen, Nikolay Bogoychev, Kenneth Heaﬁeld,\nand Faheem Kirefu. 2020a. Parallel sentence min-\ning by constrained decoding. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics (ACL).\nQian Chen, Zhu Zhuo, and Wen Wang. 2019. Bert\nfor joint intent classiﬁcation and slot ﬁlling. arXiv\npreprint arXiv:1902.10909.\nQianglong Chen, Feng Ji, Xiangji Zeng, Feng-Lin Li,\nJi Zhang, Haiqing Chen, and Yin Zhang. 2021c.\nKACE: Generating knowledge aware contrastive ex-\nplanations for natural language inference. In Pro-\nceedings of the 59th Annual Meeting of the Associa-\ntion for Computational Linguistics and the 11th In-\nternational Joint Conference on Natural Language\nProcessing (ACL).\nWenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, and\nWilliam Yang Wang. 2020b. Logical natural lan-\nguage generation from open-domain tables. In Pro-\nceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics (ACL).\nXiang Chen, Ningyu Zhang, Xin Xie, Shumin\nDeng, Yunzhi Yao, Chuanqi Tan, Fei Huang,\nLuo Si, and Huajun Chen. 2021d. Knowprompt:\nKnowledge-aware prompt-tuning with synergistic\noptimization for relation extraction. arXiv preprint\narXiv:2104.07650.\nYunmo Chen, Tongfei Chen, Seth Ebner, Aaron Steven\nWhite, and Benjamin Van Durme. 2020c. Reading\nthe manual: Event extraction as deﬁnition compre-\nhension. In Proceedings of the Fourth Workshop on\nStructured Prediction for NLP, pages 74–83, Online.\nAssociation for Computational Linguistics.\nAvihay Chriqui and Inbal Yahav. 2021. Hebert &\nhebemo: a hebrew bert model and a tool for polar-\nity analysis and emotion recognition. arXiv preprint\narXiv:2102.01909.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019a. BoolQ: Exploring the surprising\ndifﬁculty of natural yes/no questions. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers) , pages 2924–2936, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019b. What does BERT\nlook at? an analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for\nNLP, pages 276–286, Florence, Italy. Association\nfor Computational Linguistics.\nRonan Collobert and Jason Weston. 2008. A uniﬁed\narchitecture for natural language processing: deep\nneural networks with multitask learning. In Pro-\nceedings of the 25th international conference on Ma-\nchine learning , ICML ’08, pages 160–167, New\nYork, NY , USA. Association for Computing Machin-\nery.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. arXiv\npreprint arXiv:1911.02116.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. XNLI: Evaluating\ncross-lingual sentence representations. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 2475–2485,\nBrussels, Belgium. Association for Computational\nLinguistics.\nSamuel Coope, Tyler Farghly, Daniela Gerz, Ivan Vuli´c,\nand Matthew Henderson. 2020. Span-ConveRT:\nFew-shot span extraction for dialog with pretrained\nconversational representations. In Proceedings of\nthe 58th Annual Meeting of the Association for Com-\nputational Linguistics, pages 107–121, Online. As-\nsociation for Computational Linguistics.\nAsa Cooper Stickland, Xian Li, and Marjan\nGhazvininejad. 2021. Recipes for adapting\npre-trained monolingual and multilingual models\nto machine translation. In Proceedings of the\n16th Conference of the European Chapter of the\nAssociation for Computational Linguistics: Main\nVolume, pages 3440–3453, Online. Association for\nComputational Linguistics.\nCorinna Cortes and Vladimir Vapnik. 1995. Support-\nvector networks. Machine learning, 20(3):273–297.\nLeyang Cui, Yu Wu, Jian Liu, Sen Yang, and Yue\nZhang. 2021. Template-based named entity recog-\nnition using BART. In Findings of the Association\nfor Computational Linguistics (ACL-IJCNLP).\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shi-\njin Wang, and Guoping Hu. 2020. Revisiting pre-\ntrained models for Chinese natural language process-\ning. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020 , pages 657–668,\nOnline. Association for Computational Linguistics.\n32\nIdo Dagan, Dan Roth, Mark Sammons, and Fabio Mas-\nsimo Zanzotto. 2013. Recognizing textual entail-\nment: Models and applications. Synthesis Lectures\non Human Language Technologies, 6(4):1–220.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive Language Models be-\nyond a Fixed-Length Context. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 2978–2988, Florence,\nItaly. Association for Computational Linguistics.\nYann N Dauphin, Angela Fan, Michael Auli, and David\nGrangier. 2017. Language modeling with gated con-\nvolutional networks. In International conference on\nmachine learning, pages 933–941. PMLR.\nNicola De Cao, Gautier Izacard, Sebastian Riedel, and\nFabio Petroni. 2021. Autoregressive entity retrieval.\nIn Proceedings of the 9th International Conference\non Learning Representations (ICLR).\nPieter Delobelle, Thomas Winters, and Bettina Berendt.\n2020. RobBERT: a Dutch RoBERTa-based Lan-\nguage Model. In Findings of the Association for\nComputational Linguistics: EMNLP 2020 , pages\n3255–3265, Online. Association for Computational\nLinguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nXinya Du and Claire Cardie. 2020. Event extrac-\ntion by answering (almost) natural questions. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 671–683, Online. Association for Computa-\ntional Linguistics.\nXinya Du, Luheng He, Qi Li, Dian Yu, Panupong Pa-\nsupat, and Yuan Zhang. 2021a. QA-driven zero-\nshot slot ﬁlling with weak supervision pretraining.\nIn Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 2: Short Papers) , pages\n654–664, Online. Association for Computational\nLinguistics.\nXinya Du, Alexander Rush, and Claire Cardie. 2021b.\nTemplate ﬁlling with generative transformers. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies\n(NAACL-HLT).\nAvia Efrat and Omer Levy. 2020. The turking test: Can\nlanguage models understand instructions? arXiv\npreprint arXiv:2010.11982.\nDumitru Erhan, Yoshua Bengio, Aaron Courville,\nPierre-Antoine Manzagol, Pascal Vincent, and Samy\nBengio. 2010. Why Does Unsupervised Pre-training\nHelp Deep Learning? Journal of Machine Learning\nResearch, 11(19):625–660.\nAllyson Ettinger. 2020. What BERT is not: Lessons\nfrom a new suite of psycholinguistic diagnostics for\nlanguage models. Transactions of the Association\nfor Computational Linguistics, 8:34–48.\nMehrdad Farahani, Mohammad Gharachorloo,\nMarzieh Farahani, and Mohammad Manthouri.\n2021. Parsbert: Transformer-based model for\npersian language understanding. Neural Processing\nLetters.\nJoshua Feldman, Joe Davison, and Alexander M. Rush.\n2019. Commonsense knowledge mining from pre-\ntrained models. arXiv preprint arXiv:1909.00505.\nRui Feng, Jie Yuan, and Chao Zhang. 2020a. Prob-\ning and ﬁne-tuning reading comprehension mod-\nels for few-shot event extraction. arXiv preprint\narXiv:2010.11325.\nSteven Y . Feng, Varun Gangal, Jason Wei, Sarath Chan-\ndar, Soroush V osoughi, Teruko Mitamura, and Ed-\nuard Hovy. 2021. A survey of data augmentation\napproaches for NLP. In Findings of the Association\nfor Computational Linguistics (ACL-IJCNLP).\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan,\nXiaocheng Feng, Ming Gong, Linjun Shou, Bing\nQin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020b.\nCodeBERT: A pre-trained model for programming\nand natural languages. In Findings of the Associa-\ntion for Computational Linguistics: EMNLP 2020 ,\npages 1536–1547, Online. Association for Compu-\ntational Linguistics.\nSteven Fincke, Shantanu Agarwal, Scott Miller, and\nElizabeth Boschee. 2021. Language model prim-\ning for cross-lingual event extraction.arXiv preprint\narXiv:2109.12383.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020a.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nShuyang Gao, Sanchit Agarwal, Di Jin, Tagyoung\nChung, and Dilek Hakkani-Tur. 2020b. From ma-\nchine reading comprehension to dialogue state track-\ning: Bridging the gap. In Proceedings of the 2nd\nWorkshop on Natural Language Processing for Con-\nversational AI, pages 79–89, Online. Association for\nComputational Linguistics.\n33\nShuyang Gao, Abhishek Sethi, Sanchit Agarwal, Tagy-\noung Chung, and Dilek Hakkani-Tur. 2019. Dialog\nstate tracking: A neural reading comprehension ap-\nproach. In Proceedings of the 20th Annual SIGdial\nMeeting on Discourse and Dialogue, pages 264–273,\nStockholm, Sweden. Association for Computational\nLinguistics.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021a.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Nat-\nural Language Processing (Volume 1: Long Papers),\npages 3816–3830, Online. Association for Computa-\ntional Linguistics.\nYifan Gao, Henghui Zhu, Patrick Ng, Cicero\nNogueira dos Santos, Zhiguo Wang, Feng Nan, De-\njiao Zhang, Ramesh Nallapati, Andrew O Arnold,\nand Bing Xiang. 2021b. Answering ambiguous\nquestions through generative evidence fusion and\nround-trip prediction. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics (ACL).\nYoav Goldberg. 2019. Assessing bert’s syntactic abili-\nties. arXiv preprint arXiv:1901.05287.\nAnkur Goswami, Akshata Bhat, Hadar Ohana, and\nTheodoros Rekatsinas. 2020. Unsupervised relation\nextraction from language models using constrained\ncloze completion. In Findings of the Association for\nComputational Linguistics (EMNLP).\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. 2018. Colorless\ngreen recurrent networks dream hierarchically. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers) , pages 1195–1205, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\nDemi Guo, Alexander M. Rush, and Yoon Kim.\n2021. Parameter-Efﬁcient Transfer Learning with\nDiff Pruning. arXiv:2012.07463 [cs] . ArXiv:\n2012.07463.\nRuohao Guo and Dan Roth. 2021. Constrained labeled\ndata generation for low-resource named entity recog-\nnition. In Findings of the Association for Computa-\ntional Linguistics: ACL-IJCNLP 2021.\nChristian Hadiwinoto, Hwee Tou Ng, and Wee Chung\nGan. 2019. Improved word sense disambiguation us-\ning pre-trained contextualized word representations.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 5297–\n5306, Hong Kong, China. Association for Computa-\ntional Linguistics.\nKaren Hambardzumyan, Hrant Khachatrian, and\nJonathan May. 2021. W ARP: Word-level Adversar-\nial ReProgramming. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 4921–4933, Online. Associa-\ntion for Computational Linguistics.\nRujun Han, Luca Soldaini, and Alessandro Moschitti.\n2021a. Modeling context in answer sentence selec-\ntion systems on a latency budget. In Proceedings of\nthe 16th Conference of the European Chapter of the\nAssociation for Computational Linguistics (EACL).\nXu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu,\nand Maosong Sun. 2021b. Ptr: Prompt tuning\nwith rules for text classiﬁcation. arXiv preprint\narXiv:2105.11259.\nAdi Haviv, Jonathan Berant, and Amir Globerson.\n2021. BERTese: Learning to speak to BERT. In\nProceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume , pages 3618–3623, Online.\nAssociation for Computational Linguistics.\nHangfeng He, Qiang Ning, and Dan Roth. 2020.\nQuASE: Question-answer driven sentence encoding.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8743–8758, Online. Association for Computational\nLinguistics.\nLuheng He, Kenton Lee, Omer Levy, and Luke Zettle-\nmoyer. 2018. Jointly predicting predicates and argu-\nments in neural semantic role labeling. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers), pages 364–369, Melbourne, Australia. Asso-\nciation for Computational Linguistics.\nCharles T. Hemphill, John J. Godfrey, and George R.\nDoddington. 1990. The ATIS spoken language sys-\ntems pilot corpus. In Speech and Natural Language:\nProceedings of a Workshop Held at Hidden Valley,\nPennsylvania, June 24-27,1990.\nDan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam\nDziedzic, Rishabh Krishnan, and Dawn Song. 2020.\nPretrained transformers improve out-of-distribution\nrobustness. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2744–2751, Online. Association for Computa-\ntional Linguistics.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nEhsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu,\nSemih Yavuz, and Richard Socher. 2020. A simple\nlanguage model for task-oriented dialogue. arXiv\npreprint arXiv:2005.00796.\n34\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-Efﬁcient Transfer Learning for\nNLP. In International Conference on Machine\nLearning, pages 2790–2799. PMLR.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nLanguage Model Fine-tuning for Text Classiﬁcation.\nIn Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers) , pages 328–339, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nChao-Chun Hsu, Eric Lind, Luca Soldaini, and\nAlessandro Moschitti. 2021. Answer generation for\nretrieval-based question answering systems. InFind-\nings of the Association for Computational Linguis-\ntics (ACL-IJCNLP).\nKexin Huang, Jaan Altosaar, and Rajesh Ran-\nganath. 2020. ClinicalBERT: Modeling Clini-\ncal Notes and Predicting Hospital Readmission.\narXiv:1904.05342 [cs]. ArXiv: 1904.05342.\nPatrick Huber, Armen Aghajanyan, Barlas O ˘guz,\nDmytro Okhonko, Wen tau Yih, Sonal Gupta, and\nXilun Chen. 2021. Ccqa: A new web-scale ques-\ntion answering dataset for model pre-training. arXiv\npreprint arXiv:2110.07731.\nPhilip Huebner, Elior Sulem, Cynthia Fisher, and Dan\nRoth. 2021. Babyberta: Learning more grammar\nwith small-scale child-directed language. In Proc.\nof the Conference on Computational Natural Lan-\nguage Learning (CoNLL).\nMinyoung Huh, Pulkit Agrawal, and Alexei A. Efros.\n2016. What makes ImageNet good for transfer learn-\ning? arXiv:1608.08614 [cs]. ArXiv: 1608.08614.\nJena D. Hwang, Chandra Bhagavatula, Ronan Le Bras,\nJeff Da, Keisuke Sakaguchi, Antoine Bosselut, and\nYejin Choi. 2020. Comet-atomic 2020: On symbolic\nand neural commonsense knowledge graphs. arXiv\npreprint arXiv:2010.05953.\nTohoku University Inui Laboratory. 2021. Pretrained\nJapanese BERT models. GitHub.\nAngelina Ivanova, Stephan Oepen, Lilja Øvrelid, and\nDan Flickinger. 2012. Who did what to whom? a\ncontrastive study of syntacto-semantic dependencies.\nIn Proceedings of the Sixth Linguistic Annotation\nWorkshop, pages 2–11, Jeju, Republic of Korea. As-\nsociation for Computational Linguistics.\nMohit Iyyer, John Wieting, Kevin Gimpel, and Luke\nZettlemoyer. 2018. Adversarial example generation\nwith syntactically controlled paraphrase networks.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies\n(NAACL-HLT).\nPeter Izsak, Moshe Berchansky, and Omer Levy. 2021.\nHow to train bert with an academic budget. arXiv\npreprint arXiv:2104.07705.\nRobin Jia, Mike Lewis, and Luke Zettlemoyer. 2021.\nQuestion answering infused pre-training of general-\npurpose contextualized representations. arXiv\npreprint arXiv:2106.08190.\nWei Jiang, Zhenghua Li, Yu Zhang, and Min Zhang.\n2019. HLT@SUDA at SemEval-2019 task 1:\nUCCA graph parsing as constituent tree parsing.\nIn Proceedings of the 13th International Workshop\non Semantic Evaluation , pages 11–15, Minneapo-\nlis, Minnesota, USA. Association for Computational\nLinguistics.\nZhengbao Jiang, Antonios Anastasopoulos, Jun Araki,\nHaibo Ding, and Graham Neubig. 2020. X-FACTR:\nMultilingual factual knowledge retrieval from pre-\ntrained language models. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5943–5959,\nOnline. Association for Computational Linguistics.\nZhengbao Jiang, Jun Araki, Haibo Ding, and Graham\nNeubig. 2021. How can we know when language\nmodels know? on the calibration of language mod-\nels for question answering. Transactions of the As-\nsociation of Computational Linguistics, 8:423–438.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S.\nWeld, Luke Zettlemoyer, and Omer Levy. 2020.\nSpanBERT: Improving pre-training by representing\nand predicting spans. Transactions of the Associa-\ntion for Computational Linguistics, 8:64–77.\nMandar Joshi, Omer Levy, Daniel S. Weld, and\nLuke Zettlemoyer. 2019. Bert for coreference res-\nolution: Baselines and analysis. arXiv preprint\narXiv:1908.09091.\nMihir Kale and Abhinav Rastogi. 2020. Text-to-text\npre-training for data-to-text tasks. In Proceedings of\nthe 13th International Conference on Natural Lan-\nguage Generation, pages 97–102, Dublin, Ireland.\nAssociation for Computational Linguistics.\nJared Kaplan, Sam McCandlish, Tom Henighan,\nTom B. Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario\nAmodei. 2020. Scaling Laws for Neural Language\nModels. arXiv:2001.08361 [cs, stat] . ArXiv:\n2001.08361.\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish\nSabharwal, Oyvind Tafjord, Peter Clark, and Han-\nnaneh Hajishirzi. 2020. UNIFIEDQA: Crossing for-\nmat boundaries with a single QA system. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2020 , pages 1896–1907, Online. As-\nsociation for Computational Linguistics.\nBeen Kim, Oluwasanmi Koyejo, and Rajiv Khanna.\n2016. Examples are not enough, learn to criticize!\n35\ncriticism for interpretability. In Advances in Neu-\nral Information Processing Systems 29: Annual Con-\nference on Neural Information Processing Systems\n(NIPS).\nJinkyu Kim, Anna Rohrbach, Trevor Darrell, John F.\nCanny, and Zeynep Akata. 2018. Textual expla-\nnations for self-driving vehicles. In In Proceed-\nings of the European conference on computer vision\n(ECCV).\nKaren Kukich. 1983. Design of a knowledge-based re-\nport generator. In 21st Annual Meeting of the Asso-\nciation for Computational Linguistics (ACL).\nSawan Kumar and Partha Talukdar. 2021. Reorder-\ning examples helps during priming-based few-shot\nlearning. In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021 , pages\n4507–4518, Online. Association for Computational\nLinguistics.\nSawan Kumar and Partha P. Talukdar. 2020. NILE :\nNatural language inference with faithful natural lan-\nguage explanations. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, (ACL).\nVarun Kumar, Ashutosh Choudhary, and Eunah Cho.\n2020. Data augmentation using pre-trained trans-\nformer models. In arXiv.\nYuri Kuratov and Mikhail Arkhipov. 2019. Adaptation\nof deep bidirectional multilingual transformers for\nrussian language. arXiv preprint arXiv:1905.07213.\nJohn Lafferty and Chengxiang Zhai. 2001. Document\nlanguage models, query models, and risk minimiza-\ntion for information retrieval. In Proceedings of the\n24th Annual International ACM SIGIR Conference\non Research and Development in Information Re-\ntrieval.\nJohn D. Lafferty, Andrew McCallum, and Fernando\nC. N. Pereira. 2001. Conditional random ﬁelds:\nProbabilistic models for segmenting and labeling se-\nquence data. In Proceedings of the Eighteenth Inter-\nnational Conference on Machine Learning , ICML\n’01, page 282–289, San Francisco, CA, USA. Mor-\ngan Kaufmann Publishers Inc.\nHuiyuan Lai, Antonio Toral, and Malvina Nissim.\n2021. Thank you bart! rewarding pre-trained mod-\nels improves formality style transfer. arXiv preprint\narXiv:2105.06947.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. arXiv preprint\narXiv:.\nGuillaume Lample, Myle Ott, Alexis Conneau, Lu-\ndovic Denoyer, and Marc’Aurelio Ranzato. 2018.\nPhrase-based & neural unsupervised machine trans-\nlation. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 5039–5049, Brussels, Belgium. Association\nfor Computational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Sori-\ncut. 2020. ALBERT: A Lite BERT for Self-\nsupervised Learning of Language Representations.\narXiv:1909.11942 [cs].\nHang Le, Lo¨ıc Vial, Jibril Frej, Vincent Segonne, Max-\nimin Coavoux, Benjamin Lecouteux, Alexandre Al-\nlauzen, Benoit Crabb´e, Laurent Besacier, and Didier\nSchwab. 2020. FlauBERT: Unsupervised language\nmodel pre-training for French. In Proceedings of\nthe 12th Language Resources and Evaluation Con-\nference, pages 2479–2490, Marseille, France. Euro-\npean Language Resources Association.\nTeven Le Scao and Alexander Rush. 2021. How many\ndata points is a prompt worth? In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 2627–2636, On-\nline. Association for Computational Linguistics.\nYann LeCun, Yoshua Bengio, and Geoffrey Hinton.\n2015. Deep learning. nature, 521(7553):436–444.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So,\nand Jaewoo Kang. 2019. BioBERT: a pre-\ntrained biomedical language representation model\nfor biomedical text mining. Bioinformatics,\n36(4):1234–1240.\nKenton Lee, Luheng He, and Luke Zettlemoyer. 2018.\nHigher-order coreference resolution with coarse-to-\nﬁne inference. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 2 (Short Papers), pages\n687–692, New Orleans, Louisiana. Association for\nComputational Linguistics.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efﬁcient prompt\ntuning. In EMNLP.\nHector Levesque, Ernest Davis, and Leora Morgen-\nstern. 2012. The winograd schema challenge. In\nThirteenth International Conference on the Princi-\nples of Knowledge Representation and Reasoning.\nOmer Levy, Minjoon Seo, Eunsol Choi, and Luke\nZettlemoyer. 2017. Zero-shot relation extraction via\nreading comprehension. In Proceedings of the 21st\nConference on Computational Natural Language\nLearning (CoNLL 2017) , pages 333–342, Vancou-\nver, Canada. Association for Computational Linguis-\ntics.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\n36\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nDianqi Li, Yizhe Zhang, Hao Peng, Liqun Chen, Chris\nBrockett, Ming-Ting Sun, and Bill Dolan. 2021a.\nContextualized perturbation for textual adversarial\nattack. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies (NAACL-HLT).\nFayuan Li, Weihua Peng, Yuguang Chen, Quan Wang,\nLu Pan, Yajuan Lyu, and Yong Zhu. 2020a. Event\nextraction as multi-turn question answering. InFind-\nings of the Association for Computational Linguis-\ntics: EMNLP 2020, pages 829–838, Online. Associ-\nation for Computational Linguistics.\nJunyi Li, Tianyi Tang, Wayne Xin Zhao, and Ji-\nRong Wen. 2021b. Pretrained language models\nfor text generation: A survey. arXiv preprint\narXiv:2105.10311.\nKun Li, Chengbo Chen, Xiaojun Quan, Qing Ling,\nand Yan Song. 2020b. Conditional augmentation\nfor aspect term extraction via masked sequence-to-\nsequence generation. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics (ACL).\nQi Li, Heng Ji, Yu Hong, and Sujian Li. 2014. Con-\nstructing information networks using one single\nmodel. In Proceedings of the 2014 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP). Association for Computational Linguis-\ntics.\nSha Li, Heng Ji, and Jiawei Han. 2021c. Document-\nlevel event argument extraction by conditional gener-\nation. In Proceedings of the 2021 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, pages 894–908, Online. Association for Com-\nputational Linguistics.\nXiang Lisa Li and Percy Liang. 2021. Preﬁx-\ntuning: Optimizing continuous prompts for genera-\ntion. arXiv preprint arXiv:2101.00190.\nXiaoya Li, Jingrong Feng, Yuxian Meng, Qinghong\nHan, Fei Wu, and Jiwei Li. 2020c. A uniﬁed MRC\nframework for named entity recognition. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 5849–\n5859, Online. Association for Computational Lin-\nguistics.\nXiaoya Li, Fan Yin, Zijun Sun, Xiayu Li, Arianna\nYuan, Duo Chai, Mingxin Zhou, and Jiwei Li. 2019a.\nEntity-relation extraction as multi-turn question an-\nswering. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 1340–1350, Florence, Italy. Association\nfor Computational Linguistics.\nXin Li, Lidong Bing, Wenxuan Zhang, and Wai Lam.\n2019b. Exploiting BERT for End-to-End Aspect-\nbased Sentiment Analysis. In Proceedings of the 5th\nWorkshop on Noisy User-generated Text (W-NUT\n2019), pages 34–41, Hong Kong, China. Association\nfor Computational Linguistics.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2021.\nTruthfulQA: Measuring How Models Mimic Hu-\nman Falsehoods. arXiv:2109.07958 [cs] . ArXiv:\n2109.07958.\nJeffrey Ling, Nicholas FitzGerald, Zifei Shan,\nLivio Baldini Soares, Thibault F ´evry, David Weiss,\nand Tom Kwiatkowski. 2020. Learning cross-\ncontext entity representations from text. arXiv\npreprint arXiv:2001.03765.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\nsom. 2017. Program induction by rationale genera-\ntion: Learning to solve and explain algebraic word\nproblems. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(ACL).\nTal Linzen. 2020. How can we accelerate progress to-\nwards human-like linguistic generalization? In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 5210–\n5217, Online. Association for Computational Lin-\nguistics.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\nAssessing the ability of lstms to learn syntax-\nsensitive dependencies. Transactions of the Associa-\ntion for Computational Linguistics, 4:521–535.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2021a. What\nmakes good in-context examples for gpt- 3? arXiv\npreprint arXiv:2101.06804.\nJian Liu, Yubo Chen, Kang Liu, Wei Bi, and Xiaojiang\nLiu. 2020a. Event extraction as machine reading\ncomprehension. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1641–1651, Online. As-\nsociation for Computational Linguistics.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021b. Pre-\ntrain, Prompt, and Predict: A Systematic Survey of\nPrompting Methods in Natural Language Processing.\narXiv:2107.13586 [cs]. ArXiv: 2107.13586.\nRuibo Liu, Guangxuan Xu, Chenyan Jia, Weicheng\nMa, Lili Wang, and Soroush V osoughi. 2020b. Data\nboost: Text data augmentation through reinforce-\nment learning guided conditional generation. In Pro-\nceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP).\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2021c. Gpt\nunderstands, too. arXiv preprint arXiv:2103.10385.\n37\nYang Liu and Mirella Lapata. 2019. Text summa-\nrization with pretrained encoders. arXiv preprint\narXiv:1908.08345.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020c. Multilingual denoising\npre-training for neural machine translation. arXiv\npreprint arXiv:2001.08210.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv:1907.11692 [cs].\nRobert L. Logan IV, Ivana Bala ˇzevi´c, Eric Wallace,\nFabio Petroni, Sameer Singh, and Sebastian Riedel.\n2021. Cutting down on prompts and parameters:\nSimple few-shot learning with language models.\narXiv preprint arXiv:2106.13353.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian\nRiedel, and Pontus Stenetorp. 2021a. Fantastically\nordered prompts and where to ﬁnd them: Over-\ncoming few-shot prompt order sensitivity. arXiv\npreprint arXiv:2104.08786.\nYaojie Lu, Hongyu Lin, Jin Xu, Xianpei Han, Jialong\nTang, Annan Li, Le Sun, Meng Liao, and Shaoyi\nChen. 2021b. Text2Event: Controllable sequence-\nto-structure generation for end-to-end event extrac-\ntion. In Proceedings of the 59th Annual Meeting of\nthe Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural\nLanguage Processing (ACL).\nScott M. Lundberg and Su-In Lee. 2017. A uniﬁed\napproach to interpreting model predictions. In Ad-\nvances in Neural Information Processing Systems\n(NIPS).\nQing Lyu, Hongming Zhang, Elior Sulem, and Dan\nRoth. 2021. Zero-shot event extraction via trans-\nfer learning: Challenges and insights. In Proceed-\nings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 2: Short Papers) , pages 322–332,\nOnline. Association for Computational Linguistics.\nShuming Ma, Jian Yang, Haoyang Huang, Zewen Chi,\nLi Dong, Dongdong Zhang, Hany Hassan Awadalla,\nAlexandre Muzio, Akiko Eriguchi, Saksham Sing-\nhal, Xia Song, Arul Menezes, and Furu Wei. 2020.\nXlm-t: Scaling up multilingual machine translation\nwith pretrained cross-lingual transformer encoders.\narXiv preprint arXiv:2012.15547.\nBrian MacWhinney. 2000. The CHILDES Project:\nTools for analyzing talk. transcription format and\nprograms, volume 1. Psychology Press.\nAman Madaan, Dheeraj Rajagopal, Niket Tandon, Yim-\ning Yang, and Eduard Hovy. 2021a. Could you give\nme a hint? generating inference graphs for defea-\nsible reasoning. In Findings of the Association for\nComputational Linguistics: ACL-IJCNLP 2021.\nAman Madaan, Dheeraj Rajagopal, Yiming Yang, Ab-\nhilasha Ravichander, Eduard Hovy, and Shrimai\nPrabhumoye. 2020. EIGEN: event inﬂuence genera-\ntion using pre-trained language models. In arXiv.\nNishtha Madaan, Inkit Padhi, Naveen Panwar, and Dip-\ntikalyan Saha. 2021b. Generate your counterfactu-\nals: Towards controlled counterfactual generation\nfor text. In Thirty-Fifth AAAI Conference on Arti-\nﬁcial Intelligence, (AAAI).\nEric Malmi, Sebastian Krause, Sascha Rothe, Daniil\nMirylenka, and Aliaksei Severyn. 2019. Encode,\ntag, realize: High-precision text editing. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 5054–5065, Hong\nKong, China. Association for Computational Lin-\nguistics.\nYuning Mao, Pengcheng He, Xiaodong Liu, Yelong\nShen, Jianfeng Gao, Jiawei Han, and Weizhu Chen.\n2021. Generation-augmented retrieval for open-\ndomain question answering. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (ACL).\nLouis Martin, Angela Fan, ´Eric de la Clergerie, An-\ntoine Bordes, and Benoˆıt Sagot. 2021. Muss: Multi-\nlingual unsupervised sentence simpliﬁcation by min-\ning paraphrases. arXiv preprint arXiv:2005.00352.\nLouis Martin, Benjamin Muller, Pedro Javier Or-\ntiz Su ´arez, Yoann Dupont, Laurent Romary, ´Eric\nde la Clergerie, Djam ´e Seddah, and Beno ˆıt Sagot.\n2020. CamemBERT: a tasty French language model.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n7203–7219, Online. Association for Computational\nLinguistics.\nYosi Mass, Boaz Carmeli, Haggai Roitman, and David\nKonopnicki. 2020. Unsupervised FAQ retrieval with\nquestion generation and BERT. In Proceedings of\nthe 58th Annual Meeting of the Association for Com-\nputational Linguistics (ACL).\nR. Thomas McCoy, Robert Frank, and Tal Linzen.\n2020. Does syntax need to grow on trees? sources of\nhierarchical inductive bias in sequence-to-sequence\nnetworks. Transactions of the Association for Com-\nputational Linguistics, 8:125–140.\nJulian Michael, Gabriel Stanovsky, Luheng He, Ido Da-\ngan, and Luke Zettlemoyer. 2018. Crowdsourcing\nquestion-answer meaning representations. In Pro-\nceedings of the 2018 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Vol-\nume 2 (Short Papers), pages 560–568, New Orleans,\n38\nLouisiana. Association for Computational Linguis-\ntics.\nSewon Min, Julian Michael, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2020. AmbigQA: Answering am-\nbiguous open-domain questions. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP).\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and\nHannaneh Hajishirzi. 2021. Cross-task generaliza-\ntion via natural language crowdsourcing instructions.\narXiv preprint arXiv:2104.08773.\nMakoto Miwa and Mohit Bansal. 2016. End-to-end re-\nlation extraction using LSTMs on sequences and tree\nstructures. In Proceedings of the 54th Annual Meet-\ning of the Association for Computational Linguistics\n(ACL). Association for Computational Linguistics.\nKhalil Mrini, Franck Dernoncourt, Quan Hung Tran,\nTrung Bui, Walter Chang, and Ndapa Nakashole.\n2020. Rethinking self-attention: Towards inter-\npretability in neural parsing. In Findings of the As-\nsociation for Computational Linguistics: EMNLP\n2020, pages 731–742, Online. Association for Com-\nputational Linguistics.\nMahdi Namazifar, Alexandros Papangelis, Gokhan\nTur, and Dilek Hakkani-T ¨ur. 2020. Language\nmodel is all you need: Natural language under-\nstanding as question answering. arXiv preprint\narXiv:2011.03023.\nTahira Naseem, Abhishek Shah, Hui Wan, Radu Flo-\nrian, Salim Roukos, and Miguel Ballesteros. 2019.\nRewarding Smatch: Transition-based AMR parsing\nwith reinforcement learning. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 4586–4592, Florence,\nItaly. Association for Computational Linguistics.\nDat Quoc Nguyen, Thanh Vu, and Anh Tuan Nguyen.\n2020a. BERTweet: A pre-trained language model\nfor English tweets. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing: System Demonstrations , pages 9–14,\nOnline. Association for Computational Linguistics.\nMinh Van Nguyen, Viet Dac Lai, Amir Pouran\nBen Veyseh, and Thien Huu Nguyen. 2021. Trankit:\nA light-weight transformer-based toolkit for multi-\nlingual natural language processing. In Proceedings\nof the 16th Conference of the European Chapter of\nthe Association for Computational Linguistics: Sys-\ntem Demonstrations, pages 80–90, Online. Associa-\ntion for Computational Linguistics.\nThien Huu Nguyen, Kyunghyun Cho, and Ralph Gr-\nishman. 2016. Joint event extraction via recurrent\nneural networks. In Proceedings of the 2016 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies.\nXuan-Phi Nguyen, Shaﬁq Joty, Steven C. H. Hoi,\nand Richard Socher. 2020b. Tree-structured atten-\ntion with hierarchical accumulation. arXiv preprint\narXiv:2002.08046.\nYixin Nie, Adina Williams, Emily Dinan, Mohit\nBansal, Jason Weston, and Douwe Kiela. 2020. Ad-\nversarial NLI: A new benchmark for natural lan-\nguage understanding. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 4885–4901, Online. Association\nfor Computational Linguistics.\nRodrigo Nogueira, Zhiying Jiang, Ronak Pradeep, and\nJimmy Lin. 2020. Document ranking with a pre-\ntrained sequence-to-sequence model. In Findings\nof the Association for Computational Linguistics\n(EMNLP).\nAbiola Obamuyide and Andreas Vlachos. 2018. Zero-\nshot relation classiﬁcation as textual entailment. In\nProceedings of the First Workshop on Fact Extrac-\ntion and VERiﬁcation (FEVER), pages 72–78, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nKostiantyn Omelianchuk, Vipul Raheja, and Oleksandr\nSkurzhanskyi. 2021. Text Simpliﬁcation by Tag-\nging. In Proceedings of the 16th Workshop on Inno-\nvative Use of NLP for Building Educational Appli-\ncations, pages 11–25, Online. Association for Com-\nputational Linguistics.\nLiangming Pan, Wenhu Chen, Wenhan Xiong, Min-\nYen Kan, and William Yang Wang. 2021a. Zero-\nshot fact veriﬁcation by claim generation. In Pro-\nceedings of the 59th Annual Meeting of the Associa-\ntion for Computational Linguistics and the 11th In-\nternational Joint Conference on Natural Language\nProcessing (ACL).\nLiangming Pan, Wenhu Chen, Wenhan Xiong, Min-\nYen Kan1, and William Yang Wang. 2021b. Unsu-\npervised multi-hop question answering by question\ngeneration:. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies (ACL-HLT).\nGiovanni Paolini, Ben Athiwaratkun, Jason Krone,\nJie Ma, Alessandro Achille, Rishita Anubhai, Ci-\ncero dos Santos Nogueira, Bing Xiang, and Stefano\nSoatto. 2021. Structured prediction as translation\nbetween augmented natural languages. In Proceed-\nings of the 9th International Conference on Learning\nRepresentations (ICLR).\nYannis Papanikolaou and Andrea Pierleoni. 2020.\nDare: Data augmented relation extraction with gpt-2.\narXiv preprint arXiv:2004.13845.\nLoreto Parisi, Simone Francia, and Paolo Magnani.\n2020. UmBERTo: an Italian language model trained\nwith whole word masking. GitHub.\n39\nDong Huk Park, Lisa Anne Hendricks, Zeynep Akata,\nAnna Rohrbach, Bernt Schiele, Trevor Darrell, and\nMarcus Rohrbach. 2018. Multimodal explanations:\nJustifying decisions and pointing to the evidence. In\nProceedings of 2018 IEEE Conference on Computer\nVision and Pattern Recognition (CVPR).\nDebjit Paul and Anette Frank. 2021. Coins: Dynam-\nically generating contextualized inference rules for\nnarrative story completion. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics (ACL).\nBaolin Peng, Chenguang Zhu, Michael Zeng, and Jian-\nfeng Gao. 2020. Data augmentation for spoken lan-\nguage understanding via pretrained models.\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. 2021.\nTrue few-shot learning with language models. arXiv\npreprint arXiv:2105.11447.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep Contextualized Word Rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nFabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim\nRockt¨aschel, Yuxiang Wu, Alexander H. Miller, and\nSebastian Riedel. 2020. How context affects lan-\nguage models’ factual predictions. In AKBC.\nFabio Petroni, Tim Rockt ¨aschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Process-\ning and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP).\nJonas Pfeiffer, Andreas R ¨uckl´e, Clifton Poth, Aish-\nwarya Kamath, Ivan Vuli ´c, Sebastian Ruder,\nKyunghyun Cho, and Iryna Gurevych. 2020a.\nAdapterHub: A framework for adapting transform-\ners. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 46–54, Online. Asso-\nciation for Computational Linguistics.\nJonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Se-\nbastian Ruder. 2020b. MAD-X: An Adapter-Based\nFramework for Multi-Task Cross-Lingual Transfer.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7654–7673, Online. Association for Computa-\ntional Linguistics.\nMarco Polignano, Pierpaolo Basile, Marco Degemmis,\nGiovanni Semeraro, and Valerio Basile. 2019. Al-\nberto: Italian bert language understanding model for\nnlp challenging tasks based on tweets. In CLiC-it.\nJay M. Ponte and W. Bruce Croft. 1998. A language\nmodeling approach to information retrieval. In Pro-\nceedings of the 21st Annual International ACM SI-\nGIR Conference on Research and Development in\nInformation Retrieval.\nJakob Prange, Nathan Schneider, and Vivek Sriku-\nmar. 2021. Supertagging the long tail with tree-\nstructured decoding of complex categories. Transac-\ntions of the Association for Computational Linguis-\ntics, 9(0):243–260.\nSai Prasanna, Anna Rogers, and Anna Rumshisky.\n2020. When BERT Plays the Lottery, All Tickets\nAre Winning. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 3208–3229, Online. As-\nsociation for Computational Linguistics.\nRaul Puri and Bryan Catanzaro. 2019. Zero-shot\ntext classiﬁcation with generative language models.\narXiv preprint arXiv:1912.10165.\nGuanghui Qin and Jason Eisner. 2021. Learning how\nto ask: Querying LMs with mixtures of soft prompts.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 5203–5212, Online. Association for Compu-\ntational Linguistics.\nXiPeng Qiu, TianXiang Sun, YiGe Xu, YunFan Shao,\nNing Dai, and XuanJing Huang. 2020. Pre-\ntrained models for natural language processing: A\nsurvey. Science China Technological Sciences ,\n63(10):1872–1897.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving Language Under-\nstanding by Generative Pre-Training. OpenAI blog,\npage 12.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Lan-\nguage models are unsupervised multitask learners.\nOpenAI blog, 1(8):9.\nEvani Radiya-Dixit and Xin Wang. 2020. How ﬁne can\nﬁne-tuning be? learning efﬁcient language models.\nIn Proceedings of the Twenty Third International\nConference on Artiﬁcial Intelligence and Statistics ,\nvolume 108 ofProceedings of Machine Learning Re-\nsearch, pages 2435–2443. PMLR.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the Lim-\nits of Transfer Learning with a Uniﬁed Text-to-Text\nTransformer. In Journal of Machine Learning Re-\nsearch.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable ques-\ntions for squad. arXiv preprint arXiv:1806.03822.\n40\nGiulio Ravasio and Leonardo Di Perna. 2020.\nGilBERTo: An Italian pretrained language\nmodel based on roberta. https://github.com/idb-\nita/GilBERTo. GitHub.\nLiliang Ren, Chenkai Sun, Heng Ji, and Julia Hock-\nenmaier. 2021. HySPA: Hybrid span generation\nfor scalable text-to-graph extraction. In Findings of\nthe Association for Computational Linguistics (ACL-\nIJCNLP).\nLaria Reynolds and Kyle McDonell. 2021. Prompt pro-\ngramming for large language models: Beyond the\nfew-shot paradigm. In Extended Abstracts of the\n2021 CHI Conference on Human Factors in Com-\nputing Systems, CHI EA ’21 , New York, NY , USA.\nAssociation for Computing Machinery.\nMarco T ´ulio Ribeiro, Sameer Singh, and Carlos\nGuestrin. 2016. ”why should I trust you?”: Explain-\ning the predictions of any classiﬁer. In Proceedings\nof the Demonstrations Session of 2016 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies (NAACL-HLT).\nMarco T ´ulio Ribeiro, Sameer Singh, and Carlos\nGuestrin. 2018. Semantically equivalent adversarial\nrules for debugging NLP models. In Proceedings of\nthe 56th Annual Meeting of the Association for Com-\nputational Linguistics, (ACL).\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in BERTology: What we know\nabout how BERT works. Transactions of the Associ-\nation for Computational Linguistics, 8:842–866.\nSubendhu Rongali, Luca Soldaini, Emilio Monti, and\nWael Hamza. 2020. Don’t parse, generate! a se-\nquence to sequence architecture for task-oriented se-\nmantic parsing. In Proceedings of the International\nWorld Wide Web Conference (WWW).\nAlexis Ross, Ana Marasovic, and Matthew E. Peters.\n2021. Explaining NLP models via minimal con-\ntrastive editing (mice). In Findings of the Associa-\ntion for Computational Linguistics: (ACL/IJCNLP).\nHayley Ross, Jonathon Cai, and Bonan Min. 2020. Ex-\nploring Contextualized Neural Language Models for\nTemporal Dependency Parsing. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 8548–\n8553, Online. Association for Computational Lin-\nguistics.\nDan Roth. 2017. Incidental supervision: Moving be-\nyond supervised learning. In Proc. of the Conferen-\nnce on Artiﬁcial Intelligence (AAAI).\nRachel Rudinger, Vered Shwartz, Jena D. Hwang,\nChandra Bhagavatula, Maxwell Forbes, Ronan\nLe Bras, Noah A. Smith, and Yejin Choi. 2020.\nThinking like a skeptic: Defeasible inference in nat-\nural language. In Findings of the Association for\nComputational Linguistics (EMNLP).\nDevendra Sachan, Yuhao Zhang, Peng Qi, and\nWilliam L. Hamilton. 2021. Do syntax trees help\npre-trained transformers extract information? In\nProceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume , pages 2647–2661, Online.\nAssociation for Computational Linguistics.\nAli Safaya, Moutasem Abdullatif, and Deniz Yuret.\n2020. KUISAIL at SemEval-2020 task 12: BERT-\nCNN for offensive speech identiﬁcation in social me-\ndia. In Proceedings of the Fourteenth Workshop on\nSemantic Evaluation, pages 2054–2059, Barcelona\n(online). International Committee for Computational\nLinguistics.\nOscar Sainz, Oier Lopez de Lacalle, Gorka Labaka, An-\nder Barrena, and Eneko Agirre. 2021. Label verbal-\nization and entailment for effective zero- and few-\nshot relation extraction. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP) , Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nOscar Sainz and German Rigau. 2021.\nAsk2Transformers: Zero-shot domain labelling\nwith pretrained language models. In Proceedings\nof the 11th Global Wordnet Conference , pages\n44–52, University of South Africa (UNISA). Global\nWordnet Association.\nJulian Salazar, Davis Liang, Toan Q. Nguyen, and Ka-\ntrin Kirchhoff. 2020. Masked language model scor-\ning. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics ,\npages 2699–2712, Online. Association for Compu-\ntational Linguistics.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2020. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChafﬁn, Arnaud Stiegler, Teven Le Scao, Arun Raja,\nManan Dey, M Saiful Bari, Canwen Xu, Urmish\nThakker, Shanya Sharma Sharma, Eliza Szczechla,\nTaewoon Kim, Gunjan Chhablani, Nihal Nayak,\nDebajyoti Datta, Jonathan Chang, Mike Tian-Jian\nJiang, Han Wang, Matteo Manica, Sheng Shen,\nZheng Xin Yong, Harshit Pandey, Rachel Baw-\nden, Thomas Wang, Trishala Neeraj, Jos Rozen,\nAbheesht Sharma, Andrea Santilli, Thibault Fevry,\nJason Alan Fries, Ryan Teehan, Stella Biderman,\nLeo Gao, Tali Bers, Thomas Wolf, and Alexan-\nder M. Rush. 2021. Multitask prompted training en-\nables zero-shot task generalization. arXiv preprint\narXiv:2110.0820.\nCicero Nogueira dos Santos, Xiaofei Ma, Ramesh Nal-\nlapati, Zhiheng Huang, and Bing Xiang. 2020. Be-\nyond [CLS] through ranking by generation. In Pro-\nceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP).\n41\nMaarten Sap, Ronan LeBras, Emily Allaway, Chan-\ndra Bhagavatula, Nicholas Lourie, Hannah Rashkin,\nBrendan Roof, Noah A. Smith, and Yejin Choi. 2019.\nAtomic: An atlas of machine commonsense for if-\nthen reasoning. arXiv preprint arXiv:1811.00146.\nTimo Schick, Helmut Schmid, and Hinrich Sch ¨utze.\n2020. Automatically identifying words that can\nserve as labels for few-shot text classiﬁcation. In\nProceedings of the 28th International Conference\non Computational Linguistics , pages 5569–5578,\nBarcelona, Spain (Online). International Committee\non Computational Linguistics.\nTimo Schick and Hinrich Sch ¨utze. 2020. BERTRAM:\nImproved word embeddings have big impact on con-\ntextualized model performance. In Proceedings of\nthe 58th Annual Meeting of the Association for Com-\nputational Linguistics , pages 3996–4007, Online.\nAssociation for Computational Linguistics.\nTimo Schick and Hinrich Sch ¨utze. 2021a. Exploiting\ncloze-questions for few-shot text classiﬁcation and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Main Vol-\nume, pages 255–269, Online. Association for Com-\nputational Linguistics.\nTimo Schick and Hinrich Sch ¨utze. 2021b. It’s not just\nsize that matters: Small language models are also\nfew-shot learners. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 2339–2352, Online. As-\nsociation for Computational Linguistics.\nTimo Schick and Hinrich Sch ¨utze. 2019. Rare words:\nA major problem for contextualized embeddings and\nhow to ﬁx it by attentive mimicking. arXiv preprint\narXiv:1904.06707.\nTimo Schick and Hinrich Sch¨utze. 2020. Few-shot text\ngeneration with pattern-exploiting training. arXiv\npreprint arXiv:2012.11926.\nTimo Schick and Hinrich Sch ¨utze. 2021. Generating\ndatasets with pretrained language models. arXiv\npreprint arXiv:2104.07540.\nTimo Schick, Sahana Udupa, and Hinrich Sch ¨utze.\n2021. Self-diagnosis and self-debiasing: A pro-\nposal for reducing corpus-based bias in nlp. arXiv\npreprint arXiv:2103.00453. To appear in TACL.\nTal Schuster, Ori Ram, Regina Barzilay, and Amir\nGloberson. 2019. Cross-lingual alignment of con-\ntextual word embeddings, with applications to zero-\nshot dependency parsing. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and\nShort Papers), pages 1599–1613, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nRoy Schwartz, Jesse Dodge, Noah A. Smith, and Oren\nEtzioni. 2020. Green AI. Communications of the\nACM, 63(12):54–63.\nStefan Schweter. 2020. Berturk - bert models for turk-\nish.\nAmit Seker, Elron Bandel, Dan Bareket, Idan\nBrusilovsky, Refael Shaked Greenfeld, and Reut\nTsarfaty. 2021. Alephbert:a hebrew large pre-\ntrained language model to start-off your hebrew nlp\napplication with. arXiv preprint arXiv:2104.04052.\nSiamak Shakeri, Cicero Nogueira dos Santos, Henghui\nZhu, Patrick Ng, Feng Nan, Zhiguo Wang, Ramesh\nNallapati, and Bing Xiang. 2020. End-to-end syn-\nthetic data generation for domain adaptation of ques-\ntion answering systems. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP).\nPeng Shi and Jimmy Lin. 2019. Simple bert models for\nrelation extraction and semantic role labeling. arXiv\npreprint arXiv:1904.05255.\nRichard Shin, Christopher Lin, Sam Thomson, Charles\nChen, Subhro Roy, Emmanouil Antonios Platanios,\nAdam Pauls, Dan Klein, Jason Eisner, and Benjamin\nVan Durme. 2021. Constrained language models\nyield few-shot semantic parsers.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV ,\nEric Wallace, and Sameer Singh. 2020. AutoPrompt:\nEliciting Knowledge from Language Models with\nAutomatically Generated Prompts. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n4222–4235, Online. Association for Computational\nLinguistics.\nF´abio Souza, Rodrigo Nogueira, and Roberto Lotufo.\n2020a. BERTimbau: pretrained BERT models for\nBrazilian Portuguese. In 9th Brazilian Conference\non Intelligent Systems, BRACIS.\nF´abio Souza, Rodrigo Nogueira, and Roberto Lotufo.\n2020b. Portuguese named entity recognition using\nbert-crf. arXiv preprint arXiv:1909.10649.\nAsa Cooper Stickland and Iain Murray. 2019. BERT\nand PALs: Projected Attention Layers for Efﬁcient\nAdaptation in Multi-Task Learning. In Proceedings\nof the 36th International Conference on Machine\nLearning, pages 5986–5995. PMLR. ISSN: 2640-\n3498.\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\n2020. How to ﬁne-tune bert for text classiﬁcation?\narXiv preprint arXiv:1905.05583.\nYu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding,\nChao Pang, Junyuan Shang, Jiaxiang Liu, Xuyi\nChen, Yanbin Zhao, Yuxiang Lu, Weixin Liu, Zhi-\nhua Wu, Weibao Gong, Jianzhong Liang, Zhizhou\nShang, Peng Sun, Wei Liu, Xuan Ouyang, Dian-\nhai Yu, Hao Tian, Hua Wu, and Haifeng Wang.\n42\n2021. Ernie 3.0: Large-scale knowledge enhanced\npre-training for language understanding and genera-\ntion. arXiv preprint arXiv:2107.02137.\nEhsan Taher, Seyed Abbas Hoseini, and Mehrnoush\nShamsfard. 2019. Beheshti-NER: Persian named\nentity recognition using BERT. In Proceedings of\nThe First International Workshop on NLP Solutions\nfor Under Resourced Languages (NSURL 2019) co-\nlocated with ICNLSP 2019 - Short Papers, pages 37–\n42, Trento, Italy. Association for Computational Lin-\nguistics.\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2020. olmpics – on what lan-\nguage model pre-training captures. arXiv preprint\narXiv:1912.13283.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFEVER: a large-scale dataset for fact extraction\nand VERiﬁcation. In Proceedings of the 2018\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long\nPapers), pages 809–819, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nKe Tran, Arianna Bisazza, and Christof Monz. 2018.\nThe importance of being recurrent for modeling hi-\nerarchical structure. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 4731–4736, Brussels, Bel-\ngium. Association for Computational Linguistics.\nTrieu H. Trinh and Quoc V . Le. 2019. A simple\nmethod for commonsense reasoning. arXiv preprint\narXiv:1806.02847.\nMaria Tsimpoukelli, Jacob Menick, Serkan Cabi,\nS. M. Ali Eslami, Oriol Vinyals, and Felix Hill. 2021.\nMultimodal few-shot learning with frozen language\nmodels. arXiv preprint arXiv:2106.13884.\nAhmet ¨Ust¨un, Arianna Bisazza, Gosse Bouma, and\nGertjan van Noord. 2020. UDapter: Language adap-\ntation for truly Universal Dependency parsing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 2302–2315, Online. Association for Computa-\ntional Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nAmir Pouran Ben Veyseh, Viet Lai, Franck Dernon-\ncourt, and Thien Huu Nguyen. 2021a. Unleash GPT-\n2 power for event detection. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 6271–6282.\nAmir Pouran Ben Veyseh, Minh Van Nguye, Bonan\nMin, and Thien Huu Nguyen. 2021b. Augment-\ning open-domain event detection with synthetic data\nfrom gpt-2. In Proceedings of the European Confer-\nence on Machine Learning and Principles and Prac-\ntice of Knowledge Discovery in Databases.\nOriol Vinyals, Meire Fortunato, and Navdeep Jaitly.\n2015. Pointer networks. Advances in Neural Infor-\nmation Processing Systems, 28:2692–2700.\nAntti Virtanen, Jenna Kanerva, Rami Ilo, Jouni Luoma,\nJuhani Luotolahti, Tapio Salakoski, Filip Ginter, and\nSampo Pyysalo. 2019. Multilingual is not enough:\nBert for ﬁnnish. arXiv preprint arXiv:1912.07076.\nWietse de Vries, Andreas van Cranenburgh, Arianna\nBisazza, Tommaso Caselli, Gertjan van Noord, and\nMalvina Nissim. 2019. Bertje: A dutch bert model.\narXiv preprint arXiv:1912.09582.\nSandra Wachter, Brent Mittelstadt, and Chris Russell.\n2018. Counterfactual explanations without opening\nthe black box: Automated decisions and the gdpr.\nEric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner,\nand Sameer Singh. 2019. Universal adversarial trig-\ngers for attacking and analyzing NLP. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 2153–2162, Hong\nKong, China. Association for Computational Lin-\nguistics.\nBen Wang. 2021. Mesh-Transformer-\nJAX: Model-Parallel Implementation of\nTransformer Language Model with JAX.\nhttps://github.com/kingoﬂolz/mesh-transformer-\njax.\nCunxiang Wang, Pai Liu, and Yue Zhang. 2021a. Can\ngenerative pre-trained language models serve as\nknowledge bases for closed-book QA? In Proceed-\nings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 3241–3251,\nOnline. Association for Computational Linguistics.\nSinong Wang, Han Fang, Madian Khabsa, Hanzi Mao,\nand Hao Ma. 2021b. Entailment as few-shot learner.\narXiv preprint arXiv:2104.14690.\nXinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang,\nZhongqiang Huang, Fei Huang, and Kewei Tu.\n2021c. Automated concatenation of embed-\ndings for structured prediction. arXiv preprint\narXiv:2010.05006.\n43\nXinyu Wang and Kewei Tu. 2020. Second-order neural\ndependency parsing with message passing and end-\nto-end training. In Proceedings of the 1st Confer-\nence of the Asia-Paciﬁc Chapter of the Association\nfor Computational Linguistics and the 10th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing, pages 93–99, Suzhou, China. Association\nfor Computational Linguistics.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R.\nBowman. 2020a. BLiMP: A benchmark of linguis-\ntic minimal pairs for English. In Proceedings of the\nSociety for Computation in Linguistics 2020 , pages\n409–410, New York, New York. Association for\nComputational Linguistics.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. CoLA: The Corpus of Linguistic Ac-\nceptability (with added annotations). http://nyu-\nmll.github.io/cola.\nAlex Warstadt, Yian Zhang, Xiaocheng Li, Haokun\nLiu, and Samuel R. Bowman. 2020b. Learning\nwhich features matter: RoBERTa acquires a prefer-\nence for linguistic generalizations (eventually). In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 217–235, Online. Association for Computa-\ntional Linguistics.\nAlbert Webson and Ellie Pavlick. 2021. Do prompt-\nbased models really understand the meaning of their\nprompts? arXiv preprint arXiv:2109.01247.\nJason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V . Le. 2021. Fine-\ntuned Language Models Are Zero-Shot Learners.\narXiv:2109.01652 [cs]. ArXiv: 2109.01652.\nRongxiang Weng, Heng Yu, Shujian Huang, Shanbo\nCheng, and Weihua Luo. 2020. Acquiring knowl-\nedge from pre-trained model to neural machine\ntranslation. In Proceedings of the AAAI Conference\non Artiﬁcial Intelligence , volume 34, pages 9266–\n9273.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers) , pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nLedell Wu, Fabio Petroni, Martin Josifoski, Sebastian\nRiedel, and Luke Zettlemoyer. 2020a. Scalable zero-\nshot entity linking with dense entity retrieval. arXiv\npreprint arXiv:1911.03814.\nShanchan Wu and Yifan He. 2019. Enriching\npre-trained language model with entity informa-\ntion for relation classiﬁcation. arXiv preprint\narXiv:1905.0828.\nTongshuang Wu, Marco Tulio Ribeiro, Jeffrey Heer,\nand Daniel S. Weld. 2021a. Polyjuice: Generating\ncounterfactuals for explaining, evaluating, and im-\nproving models. arXiv preprint arXiv:2101.00288.\nWei Wu, Fei Wang, Arianna Yuan, Fei Wu, and Ji-\nwei Li. 2020b. CorefQA: Coreference resolution as\nquery-based span prediction. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 6953–6963, Online. As-\nsociation for Computational Linguistics.\nZhaofeng Wu, Hao Peng, and Noah A. Smith. 2021b.\nInfusing ﬁnetuning with semantic dependencies.\nTransactions of the Association of Computational\nLinguistics, 9:226–242.\nDongqin Xu, Junhui Li, Muhua Zhu, Min Zhang, and\nGuodong Zhou. 2020. Improving AMR parsing\nwith sequence-to-sequence pre-training. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n2501–2511, Online. Association for Computational\nLinguistics.\nZenan Xu, Daya Guo, Duyu Tang, Qinliang Su, Linjun\nShou, Ming Gong, Wanjun Zhong, Xiaojun Quan,\nDaxin Jiang, and Nan Duan. 2021. Syntax-enhanced\npre-trained model. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 5412–5422, Online. Associa-\ntion for Computational Linguistics.\nHang Yan, Junqi Dai, Tuo Ji, Xipeng Qiu, and Zheng\nZhang. 2021a. A uniﬁed generative framework for\naspect-based sentiment analysis. In Proceedings of\nthe 59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers).\nHang Yan, Tao Gui, Junqi Dai, Qipeng Guo, Zheng\nZhang, and Xipeng Qiu. 2021b. A uniﬁed genera-\ntive framework for various NER subtasks. In Pro-\nceedings of the 59th Annual Meeting of the Associa-\ntion for Computational Linguistics and the 11th In-\nternational Joint Conference on Natural Language\nProcessing (ACL).\n44\nKaiyu Yang and Jia Deng. 2020. Strongly incremen-\ntal constituency parsing with graph neural networks.\narXiv preprint arXiv:2010.14568.\nYiben Yang, Chaitanya Malaviya, Jared Fernandez,\nSwabha Swayamdipta, Ronan Le Bras, Ji-Ping\nWang, Chandra Bhagavatula, Yejin Choi, and Doug\nDowney. 2020. G-daug: Generative data augmen-\ntation for commonsense reasoning. In Findings\nof the Association for Computational Linguistics\n(EMNLP).\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXLNet: Generalized Autoregressive Pretraining for\nLanguage Understanding. In Advances in Neural In-\nformation Processing Systems , volume 32. Curran\nAssociates, Inc.\nJiarui Yao, Haoling Qiu, Jin Zhao, Bonan Min, and Ni-\nanwen Xue. 2021. Factuality assessment as modal\ndependency parsing. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 1540–1550, Online. Associa-\ntion for Computational Linguistics.\nWenpeng Yin, Jamaal Hay, and Dan Roth. 2019.\nBenchmarking zero-shot text classiﬁcation:\nDatasets, evaluation and entailment approach.\nIn Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP) , pages\n3914–3923, Hong Kong, China. Association for\nComputational Linguistics.\nJason Yosinski, Jeff Clune, Yoshua Bengio, and Hod\nLipson. 2014. How transferable are features in deep\nneural networks? In Advances in Neural Informa-\ntion Processing Systems , volume 27. Curran Asso-\nciates, Inc.\nJianfei Yu, Chenggong Gong, and Rui Xia. 2021a.\nCross-domain review generation for aspect-based\nsentiment analysis. In Findings of the Association\nfor Computational Linguistics (ACL-IJCNLP).\nWenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu,\nQingyun Wang, Heng Ji, and Meng Jiang. 2021b.\nA survey of knowledge-enhanced text generation.\narXiv preprint arXiv:2010.04389.\nXiaodong Yu, Wenpeng Yin, and Dan Roth. 2020.\nPaired representation learning for event and entity\ncoreference. arXiv preprint arXiv:2010.12808.\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\nBartscore: Evaluating generated text as text genera-\ntion. arXiv preprint arXiv:2106.11520.\nKarolina Zaczynska, Nils Feldhus, Robert Schwarzen-\nberg, Aleksandra Gabryszak, and Sebastian M ¨oller.\n2020. Evaluating German transformer language\nmodels with syntactic agreement tests. In Proceed-\nings of the 5th Swiss Text Analytics Conference and\nthe 16th Conference on Natural Language Process-\ning, SwissText/KONVENS 2020, Zurich, Switzerland,\nJune 23-25, 2020 , volume abs/2007.03765, Zurich,\nSwitzerland. CEUR Workshop Proceedings.\nElad Ben Zaken, Shauli Ravfogel, and Yoav Gold-\nberg. 2021. Bitﬁt: Simple parameter-efﬁcient\nﬁne-tuning for transformer-based masked language-\nmodels. arXiv preprint arXiv:2106.10199.\nDanqing Zhang, Tao Li, Haiyang Zhang, and Bing\nYin. 2020a. On data augmentation for ex-\ntreme multi-label classiﬁcation. arXiv preprint\narXiv:2009.10778.\nHaoyu Zhang, Jingjing Cai, Jianjun Xu, and Ji Wang.\n2019a. Pretraining-based natural language gener-\nation for text summarization. In Proceedings of\nthe 23rd Conference on Computational Natural Lan-\nguage Learning (CoNLL) , pages 789–797, Hong\nKong, China. Association for Computational Lin-\nguistics.\nJeffrey O Zhang, Alexander Sax, Amir Zamir,\nLeonidas Guibas, and Jitendra Malik. 2020b.\nSide-tuning: A baseline for network adapta-\ntion via additive side networks. arXiv preprint\narXiv:1912.13503.\nSheng Zhang, Xutai Ma, Kevin Duh, and Benjamin\nVan Durme. 2019b. AMR parsing as sequence-to-\ngraph transduction. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 80–94, Florence, Italy. Associa-\ntion for Computational Linguistics.\nWenxuan Zhang, Xin Li, Yang Deng, Lidong Bing, and\nWai Lam. 2021a. Towards generative aspect-based\nsentiment analysis. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (ACL).\nYian Zhang, Alex Warstadt, Xiaocheng Li, and\nSamuel R. Bowman. 2021b. When do you need\nbillions of words of pretraining data? In Proceed-\nings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 1112–1125,\nOnline. Association for Computational Linguistics.\nYu Zhang, Houquan Zhou, and Zhenghua Li. 2020c.\nFast and accurate neural crf constituency parsing.\nProceedings of the Twenty-Ninth International Joint\nConference on Artiﬁcial Intelligence.\nZhuosheng Zhang, Yuwei Wu, Hai Zhao, Zuchao Li,\nShuailiang Zhang, Xi Zhou, and Xiang Zhou. 2020d.\nSemantics-aware BERT for language understanding.\nIn The Thirty-Fourth AAAI Conference on Artiﬁcial\nIntelligence (AAAI).\n45\nMengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and Hin-\nrich Sch ¨utze. 2020a. Masking as an efﬁcient alter-\nnative to ﬁnetuning for pretrained language models.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 2226–2241, Online. Association for Computa-\ntional Linguistics.\nTianyang Zhao, Zhao Yan, Yunbo Cao, and Zhoujun\nLi. 2020b. Asking effective and diverse questions:\nA machine reading comprehension based framework\nfor joint entity-relation extraction. In Proceedings of\nthe Twenty-Ninth International Joint Conference on\nArtiﬁcial Intelligence, IJCAI-20 , pages 3948–3954.\nInternational Joint Conferences on Artiﬁcial Intelli-\ngence Organization. Main track.\nTony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein,\nand Sameer Singh. 2021. Calibrate before use: Im-\nproving few-shot performance of language models.\narXiv preprint arXiv:2102.09690.\nRuiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein.\n2021a. Adapting language models for zero-shot\nlearning by meta-tuning on dataset and prompt col-\nlections. In Findings of the Association for Compu-\ntational Linguistics (EMNLP).\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021b.\nFactual probing is [MASK]: Learning vs. learning\nto recall. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 5017–5033, Online. Association for\nComputational Linguistics.\nJunru Zhou, Zhuosheng Zhang, Hai Zhao, and Shuail-\niang Zhang. 2020a. LIMIT-BERT : Linguistics in-\nformed multi-task BERT. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2020 ,\npages 4450–4461, Online. Association for Computa-\ntional Linguistics.\nJunru Zhou and Hai Zhao. 2019. Head-Driven Phrase\nStructure Grammar parsing on Penn Treebank. In\nProceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n2396–2408, Florence, Italy. Association for Compu-\ntational Linguistics.\nLi Zhou and Kevin Small. 2020. Multi-domain dia-\nlogue state tracking as dynamic knowledge graph\nenhanced question answering. arXiv preprint\narXiv:1911.06192.\nQiji Zhou, Yue Zhang, Donghong Ji, and Hao Tang.\n2020b. AMR parsing with latent structural infor-\nmation. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4306–4319, Online. Association for Computa-\ntional Linguistics.\nYang Zhou, Yubo Chen, Jun Zhao, Yin Wu, Jiexin Xu,\nand Jinlong Li. 2021. What the role is vs. what plays\nthe role: Semi-supervised event argument extrac-\ntion via dual question answering. In Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence , vol-\nume 35, pages 14638–14646.\nJinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin,\nWengang Zhou, Houqiang Li, and Tie-Yan Liu.\n2020. Incorporating bert into neural machine trans-\nlation. arXiv preprint arXiv:2002.06823.\n46\nA PLMs for specialized domains or\nlanguages\nTable 6 shows PLMs for special domains. Table 7\npresents PLMs pre-trained on different languages.\nB Pre-train then ﬁne-tune approaches\nTable 8 shows a summary of prior work organized\nby the strategies in the ﬁrst paradigm, “pretrain\nthen ﬁne-tune”. It is worth noting that “Contextual\nembeddings” does not involve ﬁne-tuning, but we\nincluded here because it is architecturally similar\nto other strategies, except that the PLM’s weights\nare frozen, that is they are not ﬁne-tuned for the\nspeciﬁc tasks.\n47\nModel Domain Training Sources\nSCIBERT (Beltagy et al., 2019) Science Scientiﬁc articles in computer science and biomedicine\nBIOBERT (Lee et al., 2019) Biomedical Biomedical publications (abstracts and full-text articles)\nCLINICALBERT(Huang et al., 2020),\nAlsentzer et al. (2019)\nClinical Clinical notes\nLEGALBERT (Chalkidis et al., 2020)Legal Legal documents (e.g. contracts)\nCODEBERT (Feng et al., 2020b),\nCODEX(Chen et al., 2021b)\nSource code GitHub repositories\nBERTweet (Nguyen et al., 2020a), Al-\nBERTo (for Italian; Polignano et al., 2019)\nTwitter Tweets\nBabyBERTa (Huebner et al., 2021)Child-directed speechChild-directed speech transcriptions\nTable 6: PLMs pre-trained on speciﬁc domains.\nLanguageModel\nArabic Arabic-BERT (Safaya et al., 2020)\nBasque BERTeus (Agerri et al., 2020)\nChinese MacBERT (Cui et al., 2020)\nDutch BERTje (de Vries et al., 2019), RobBERT (Delobelle et al., 2020)\nFarsi ParsBERT (Farahani et al., 2021)\nFinnish FinBERT (Virtanen et al., 2019)\nFrench CamemBERT (Martin et al., 2020), FlauBERT (Le et al., 2020)\nGerman GBERT and GELECTRA (Chan et al., 2020)\nHebrew HeBERT (Chriqui and Yahav, 2021), AlphaBERT (Seker et al., 2021)\nItalian GilBERTo (Ravasio and Perna, 2020), UmBERTo (Parisi et al., 2020)\nJapanese Japanese BERT (Inui Laboratory, 2021)\nPortugueseBERTimbau (Souza et al., 2020a)\nRussian RuBERT (Kuratov and Arkhipov, 2019)\nSpanish BETO (Ca˜nete et al., 2020)\nTurkish BERTurk (Schweter, 2020)\nTable 7: PLMs pre-trained on different languages.\n48\nTask Work PLM\n(1)Contextual Embeddings\nWord Sense disambiguation/induction Hadiwinoto et al. (2019), Amrami and Goldberg (2019)BERT\nCoreference resolution (Lee et al., 2018) ElMO\nConstituency parsing (Zhang et al., 2020c) BERT\nYang and Deng (2020) XLNet, BERT\nZhou and Zhao (2019) ELMo, BERT\nConstituency parsing, Dependency parsing(Mrini et al., 2020) XLNet, BERT\nDependency parsing Wang and Tu (2020) BERT\nSchuster et al. (2019) ElMo\nSemantic role labeling (He et al., 2018) ElMO\nAMR parsing Cai and Lam (2020), Xu et al. (2020) BERT\nUCCA parsing (Jiang et al., 2019) BERT, mBERT\nCommonsense reasoning ATOMIC (Sap et al., 2019) ELMo\nMachine Translation Zhu et al. (2020) BERT\nText summarization (Zhang et al., 2019a) BERT\n(2)Fine-tuning the PLM\nText classiﬁcation (Yang et al., 2019) XLNet\n(Sun et al., 2020) BERT\n(Peters et al., 2018) ElMO\nSemantic textual similarity (Yang et al., 2019) XLNet\nNER ELMo (Peters et al., 2018) ELMo\nNER, QA, Textual Entailement (TE) Devlin et al. (2019) BERT\nTE (Liu et al., 2019) RoBERTa\nEntity Linking Broscheit (2019), Ling et al. (2020), (Wu et al., 2020a)BERT\nRelation extraction (Baldini Soares et al., 2019; Wu and He, 2019; Shi and Lin, 2019)BERT\nIntent Detection and Slot Filling (Chen et al., 2019) BERT\nXLNet (Yang et al., 2019) XLNet\nText generation (Kale and Rastogi, 2020) T5\nCoreference resolution (Joshi et al., 2019) BERT\n(Yu et al., 2020) RoBERTa\nText simpliﬁcation (Martin et al., 2021) BART/mBART\n(Raffel et al., 2020) T5\nDialogue (Hosseini-Asl et al., 2020) GPT-2\nSemantic role labeling (Shi and Lin, 2019) BERT\nText summarization (Liu and Lapata, 2019) BERT\n(Lewis et al., 2020) BART\nCommonsense reasoning COMET (Bosselut et al., 2019) GPT\nATOMIC2020 (Hwang et al., 2020) GPT2\nMachine Translation (Liu et al., 2020c) mBART\n(Lample and Conneau, 2019) XLM\n(3)Fine-tuning Customized Models\nNER, POS tagging, dependency parsing, aspect extractionACE (Wang et al., 2021c) ELMo, (m)BERT, XLM-R\nSemantic parsing (Che et al., 2019) BERT\nTemporal relation extraction (Ross et al., 2020) BERT\nText simpliﬁcation (Omelianchuk et al., 2021) RoBERTa\nText simpliﬁcation, summarization (Malmi et al., 2019) BERT\nCoreference resolution CorefQA (Wu et al., 2020b) SpanBERT\nMachine Translation (Weng et al., 2020) BERT, GPT\n(Ma et al., 2020) XLM-R\nCCG parsing Tree-structured supertagger (Prange et al., 2021)RoBERTa-base\n(4)Efﬁcient Fine-tuning Approaches\nBitFit (Zaken et al., 2021)\nAdapter-Transformer (Pfeiffer et al., 2020a)\nPOS tagging, dependency parsing Trankit (Nguyen et al., 2021) XLM-R\nTable 8: A summary of prior work organized by the strategies in the ﬁrst paradigm “pretrain then ﬁne-tune”.\n49"
}