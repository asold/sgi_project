{
  "title": "Knowledge Enhanced Masked Language Model for Stance Detection",
  "url": "https://openalex.org/W3168028266",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2951708570",
      "name": "Kornraphop Kawintiranon",
      "affiliations": [
        "Georgetown University"
      ]
    },
    {
      "id": "https://openalex.org/A2095714368",
      "name": "Lisa Singh",
      "affiliations": [
        "Georgetown University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2462365838",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W3102551064",
    "https://openalex.org/W3105111366",
    "https://openalex.org/W1499164265",
    "https://openalex.org/W2945089338",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2774413609",
    "https://openalex.org/W2767329425",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2971336321",
    "https://openalex.org/W2970773744",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2509222396",
    "https://openalex.org/W2038411619",
    "https://openalex.org/W2964230653",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2009055572",
    "https://openalex.org/W3035153870",
    "https://openalex.org/W2169203263",
    "https://openalex.org/W2572163506",
    "https://openalex.org/W3118043957",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2946133598",
    "https://openalex.org/W2460159515",
    "https://openalex.org/W3033317208",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2889191048",
    "https://openalex.org/W3004975108",
    "https://openalex.org/W2757512670",
    "https://openalex.org/W2923280274",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2803437449",
    "https://openalex.org/W2792632295",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2251900677",
    "https://openalex.org/W2984259019",
    "https://openalex.org/W2113125055",
    "https://openalex.org/W2757541972",
    "https://openalex.org/W3035419191"
  ],
  "abstract": "Detecting stance on Twitter is especially challenging because of the short length of each tweet, the continuous coinage of new terminology and hashtags, and the deviation of sentence structure from standard prose. Fine-tuned language models using large-scale in-domain data have been shown to be the new state-of-the-art for many NLP tasks, including stance detection. In this paper, we propose a novel BERT-based fine-tuning method that enhances the masked language model for stance detection. Instead of random token masking, we propose using a weighted log-odds-ratio to identify words with high stance distinguishability and then model an attention mechanism that focuses on these words. We show that our proposed approach outperforms the state of the art for stance detection on Twitter data about the 2020 US Presidential election.",
  "full_text": "Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 4725–4735\nJune 6–11, 2021. ©2021 Association for Computational Linguistics\n4725\nKnowledge Enhanced Masked Language Model for Stance Detection\nKornraphop Kawintiranon and Lisa Singh\nDepartment of Computer Science\nGeorgetown University\nWashington, DC, USA\n{kk1155,lisa.singh}@georgetown.edu\nAbstract\nDetecting stance on Twitter is especially chal-\nlenging because of the short length of each\ntweet, the continuous coinage of new termi-\nnology and hashtags, and the deviation of sen-\ntence structure from standard prose. Fine-\ntuned language models using large-scale in-\ndomain data have been shown to be the new\nstate-of-the-art for many NLP tasks, including\nstance detection. In this paper, we propose a\nnovel BERT-based ﬁne-tuning method that en-\nhances the masked language model for stance\ndetection. Instead of random token masking,\nwe propose using a weighted log-odds-ratio to\nidentify words with high stance distinguisha-\nbility and then model an attention mechanism\nthat focuses on these words. We show that our\nproposed approach outperforms the state of the\nart for stance detection on Twitter data about\nthe 2020 US Presidential election.\n1 Introduction\nStance detection refers to the task of classifying\na piece of text as either being in support, oppo-\nsition, or neutral towards a given target. While\nthis type of labeling is useful for a wide range of\nopinion research, it is particularly important for un-\nderstanding the public’s perception of given targets,\nfor example, candidates during an election. For\nthis reason, our focus in this paper is on detecting\nstance towards political entities, namely Joe Biden\nand Donald Trump during the 2020 US Presidential\nelection.\nStance detection is related to, but distinct from\nthe task of sentiment analysis, which aims to ex-\ntract whether the general tone of a piece of text\nis positive, negative, or neutral. Sobhani and col-\nleagues (Sobhani et al., 2016) show that measures\nof stance and sentiment are only 60% correlated.\nFor example, the following sample tweet1 has an\n1All of the sample tweets in this paper are invented by\nthe authors. They are representative of real data, but do not\nobvious positive sentiment, but an opposing stance\ntowards Donald Trump.\nI’m so happy Biden beat Trump in the\ndebate.\nStance detection is an especially difﬁcult prob-\nlem on Twitter. A large part of this difﬁculty\ncomes from the fact that Twitter content is short,\nhighly dynamic, continually generating new hash-\ntags and abbreviations, and deviates from standard\nprose sentence structure. Recently, learning mod-\nels using pre-training (Peters et al., 2018; Radford\net al., 2018; Devlin et al., 2019; Yang et al., 2019)\nhave shown a strong ability to learn semantic rep-\nresentation and outperform many state-of-the-art\napproaches across different natural language pro-\ncessing (NLP) tasks. This is also true for stance\ndetection. The strongest models for stance detec-\ntion on Twitter use pre-trained BERT (Ghosh et al.,\n2019; Sen et al., 2018).\nA recent study that proposed models for senti-\nment analysis (Tian et al., 2020) showed that focus-\ning the learning model on some relevant words, i.e.\nsentiment words extracted using Pointwise Mutual\nInformation (PMI) (Bouma, 2009), performed bet-\nter than using the standard pre-trained BERT model.\nWe are interested in understanding whether or not\nfocusing attention on speciﬁc stance-relevant vo-\ncabulary during the learning process will improve\nstance detection. To accomplish this, we consider\nthe following two questions. First, how do we\nidentify the most important stance-relevant words\nwithin a data set? And second, how much attention\nneeds to be paid to these words versus random do-\nmain words? Toward that end, we propose building\ndifferent knowledge enhanced learning models that\nintegrate an understanding of important context-\nspeciﬁc stance words into the pre-training process.\ncorrespond to any actual tweet in the data set in order to\npreserve the privacy of Twitter users.\n4726\nWhile we consider PMI as a way to identify impor-\ntant stance words, we ﬁnd that using the log-odds\nratio performs better.\nWe also consider different options for ﬁne-tuning\nan attention-based language model. To ﬁne-tune an\nattention-based language model to a speciﬁc task,\nthe most common approach is to ﬁne-tune using\nunlabeled data with random masking (Devlin et al.,\n2019; Liu et al., 2019). Because of the noise within\nsocial media posts, random tokens that are not task-\nrelevant can impact sentence representation nega-\ntively. Therefore, instead of letting the model pay\nattention to random tokens, we introduce Knowl-\nedge Enhanced Masked Language Modeling (KE-\nMLM), where signiﬁcant tokens generated using\nthe log-odds ratio are incorporated into the learning\nprocess and used to improve a downstream classi-\nﬁcation task. To the best of our knowledge, this\nis the ﬁrst work that identiﬁes signiﬁcant tokens\nusing log-odds-ratio for a speciﬁc task and inte-\ngrates those tokens into an attention-based learning\nprocess for better classiﬁcation performance.\nIn summary, we study stance detection on En-\nglish tweets and our contributions are as follows.\n(i) We propose using the log-odds-ratio with Dirich-\nlet prior for knowledge mining to identify the most\ndistinguishable stance words. ( ii) We propose a\nnovel method to ﬁne-tune a pre-trained masked\nlanguage model for stance detection that incorpo-\nrates background knowledge about the stance task.\n(iii) We show that our proposed knowledge mining\napproach and our learning model outperform the\nﬁne-tuned BERT in a low resource setting in which\nthe data set contains 2500 labeled tweets about the\n2020 US Presidential election. (iv) We release our\nlabeled stance data to help the research commu-\nnity continue to make progress on stance detection\nmethods.2\n2 Related Work\nIn the NLP community, sentiment analysis is a\nmore established task that has received more atten-\ntion than stance detection. A sub-domain of senti-\nment analysis is target-directed or aspect-speciﬁc\nsentiment, which refers to the tone with which an\nauthor writes about a speciﬁc target/entity or an\naspect of a target (Mitchell et al., 2013; Jiang et al.,\n2011). One common use case is breaking down\nsentiment toward different aspects of a product\n2https://github.com/GU-DataLab/\nstance-detection-KE-MLM\nin reviews, e.g., the price of a laptop versus its\nCPU performance (Schmitt et al., 2018; Chen et al.,\n2017; Poddar et al., 2017; Tian et al., 2020). Dif-\nferent approaches have been proposed to tackle\nthis problem. Chen and colleagues combine atten-\ntion with recurrent neural networks (Chen et al.,\n2017). Schmitt and colleagues propose combin-\ning a convolutional neural network and fastText\nembeddings (Schmitt et al., 2018). A recent study\nproposes modifying the learning objective of the\nmasked language model to pay attention to a spe-\nciﬁc set of sentiment words extracted by PMI (Tian\net al., 2020). The model achieves new state-of-\nthe-art results on most of the test data sets. Be-\ncause stance is a different task, 3 we will adjust\ntheir target-directed sentiment approach for stance\nand compare to it in our empirical evaluation.\nThe most well-known data for political stance\ndetection is published by the SemEval 2016 (Mo-\nhammad et al., 2016b; Aldayel and Magdy, 2019).\nThe paper describing the data set provides a high-\nlevel review of approaches to stance detection us-\ning Twitter data. The best user-submitted system\nwas a neural classiﬁer from MITRE (Zarrella and\nMarsh, 2016) which utilized a pre-trained language\nmodel on a large amount of unlabeled data. An\nimportant contribution of this study was using pre-\ntrained word embeddings from an auxiliary task\nwhere a language model was trained to predict a\nmissing hashtag from a given tweet. The runner-up\nmodel was a convolutional neural network for text\nclassiﬁcation (Wei et al., 2016).\nFollowing the MITRE model, there were a num-\nber of both traditional and neural models proposed\nfor stance detection. A study focusing on tradi-\ntional classiﬁers proposed using a support vector\nmachine (SVM) with lexicon-based features, senti-\nment features and textual entailment feature (Sen\net al., 2018). Another SVM-based model con-\nsisted of two-step SVMs (Dey et al., 2017). In\nthe ﬁrst step, the model predicts whether an in-\nput sequence is relevant to a given target. The\nnext step detects the stance if the input sequence\nis relevant. Target-speciﬁc attention neural net-\nwork (TAN) is a novel bidirectional LSTM-based\nattention model. In this study, Dey and colleagues\ntrained it on unpublished unlabeled data to learn\nthe domain context (Du et al., 2017). Recently,\n3Stance detection aims to detect the opinion s to the spe-\nciﬁc target e, aspect-based sentiment focuses on extracting\nthe aspect a towards the target e and corresponding opinion\ns (Wang et al., 2019).\n4727\na neural ensemble model consisting of bi-LSTM,\nnested LSTMs, and an attention model was pro-\nposed for stance detection on Twitter (Siddiqua\net al., 2019). The model’s embedding weights were\ninitialized with the pre-trained embeddings from\nfastText (Bojanowski et al., 2017).\nThe emergence of transformer-based deep learn-\ning models has led to high levels of improve-\nment for many NLP tasks, including stance de-\ntection (Ghosh et al., 2019; Küçük and Can, 2020;\nAlDayel and Magdy, 2020). BERT (Devlin et al.,\n2019) is the most used deep transformer encoder.\nMore speciﬁcally, BERT uses Masked Language\nModeling (MLM) to pre-train a transformer en-\ncoder by predicting masked tokens in order to learn\nthe semantic representation of a corpus. Ghosh\nand colleagues (Ghosh et al., 2019) show that the\noriginal pre-trained BERT without any further ﬁne-\ntuning outperforms other former state-of-the-art\nmodels on the SemEval set including the model that\nutilizes both text and user information (Del Tredici\net al., 2019). Because we are interested in the 2020\nUS Presidential election and many temporal fac-\ntors relevant to stance exist (e.g. political topics),\nwe introduce a new Election 2020 data set. For\nour empirical analysis, we will use this data set,\nand compare our approach to other state-of-the-art\nmethods that used the SemEval data set. Our data\nsets are described in Section 5.1.\nInspired by BERT, different variations of BERT\nhave been proposed to solve different speciﬁc NLP\ntasks. SpanBERT (Joshi et al., 2019) masks to-\nkens within a given span range. ERNIE (Sun et al.,\n2019) ﬁnds and masks entity tokens achieving new\nstate-of-the-art results on many Chinese NLP tasks,\nincluding sentiment analysis. GlossBERT (Huang\net al., 2019) uses gloss knowledge (sense deﬁni-\ntion) to improve performance on a word sense dis-\nambiguation task. SenseBERT (Levine et al., 2020)\naims to predict both masked words and the Word-\nNet super-sense to improve word-in-context tasks.\nZhang and colleagues introduce entity token mask-\ning (Zhang et al., 2019) for relation classiﬁcation\nwhere the goal is to classify relation labels of given\nentity pairs based on context. A number of stud-\nies have been working on adjusting transformers\nfor sentiment analysis tasks. A recent study (Tian\net al., 2020) proposes a sentiment knowledge en-\nhanced pre-training method (SKEP). It shows that\nmasking sentiment words extracted by PMI guides\nthe language model to learn more sentiment knowl-\nedge resulting in better sentiment classiﬁcation per-\nformance. SentiLARE (Ke et al., 2020) uses an\nalternative approach that injects word-level linguis-\ntic knowledge, including part-of-speech tags and\nsentiment polarity scores obtained by SentiWord-\nNet (Guerini et al., 2013), into the pre-training\nprocess. Following these works, SENTIX (Zhou\net al., 2020) was proposed to incorporate domain-\ninvariant sentiment knowledge for cross-domain\nsentiment data sets. Our work differs because our\ntask is stance detection and we employ a novel\nknowledge mining step that uses log-odds-ratio\nto determine signiﬁcant tokens that need to be\nmasked.\n3 KE-MLM: Knowledge Enhanced\nMasked Language Modeling\nWe propose Knowledge Enhanced Masked Lan-\nguage Modeling (KE-MLM), which integrates\nknowledge that enhances the classiﬁcation task in\nthe ﬁne-tuning process. We identify task-relevant\ntokens using text mining (Section 3.1). We then use\nthese discovered tokens within a masked language\nmodel (Section 3.2).\n3.1 Knowledge Mining for Classiﬁcation\nWhile TF-IDF is the preferred method for identify-\ning important words in a corpus, we are interested\nin identifying important words for distinguishing\nstance, not just words that are important within the\ncorpus. Therefore, we propose using the weighted\nlog-odds-ratio technique with informed Dirichlet\npriors proposed by Monroe and colleagues (Mon-\nroe et al., 2008) to compute signiﬁcant words for\neach stance class. Intuitively, this measure attempts\nto account for the amount of variance in a word’s\nfrequency and uses word frequencies from a back-\nground corpus as priors to reduce the noise gener-\nated by rare words. This technique has been shown\nto outperform other methods that were designed to\nﬁnd signiﬁcant words within a corpus such as PMI\nand TF-IDF (Monroe et al., 2008; Jurafsky et al.,\n2014; Budak, 2019).\nMore formally, we compute the usage difference\nfor word wamong two corpora using the log-odds-\nratio with informative Dirichlet priors as shown in\nthe Equation 1, where ni is the size of corpus iand\nnj is the size of corpus j. yi\nw and yj\nw indicate the\nword count of win corpus iand j, respectively. α0\nis the size of the background corpus and αw is the\nword count of win the background corpus.\n4728\nδ(i−j)\nw = log yi\nw + αw\nni + α0 −yiw −αw\n−\nlog yj\nw + αw\nnj + α0 −yj\nw −αw\n(1)\nTo measure the signiﬁcance of each word, we\nﬁrst compute the variance (σ2) of the log-odds-ratio\nusing Equation 2, and then compute the Z-score\nusing Equation 3. A higher score indicates more\nsigniﬁcance of word wwithin corpus icompared to\ncorpus j. A lower score means more signiﬁcance\nof word wwithin corpus jcompared to corpus i.\nσ2(δ(i−j)\nw ) ≈ 1\nyiw + αw\n+ 1\nyj\nw + αw\n(2)\nZ = δ(i−j)\nw√\nσ2(δ(i−j)\nw )\n(3)\nSince stance has three different classes (support,\nopposition and neutral), we need to adjust the log-\nodds-ratio technique in order to obtain a set of\nsigniﬁcant stance words. Using a training set, we\nﬁnd stance tokens which are signiﬁcant tokens for\nsupport/non-support or opposition/non-opposition\nas follows:\n• Supportive & Non-supportive tokens are\nthe highest and lowest Z-score tokens, respec-\ntively when ionly contains the support class\nand jcontains only the opposition and neutral\nclasses.\n• Opposing & Non-opposing tokens are the\nhighest and lowest Z-score tokens, respec-\ntively when i only contains the opposition\nclass and jonly contains the support and neu-\ntral classes.\nWe select the highest and lowest ktokens based\non Z-score from each token list above. This results\nin four k-token lists. The combined tokens of these\nlists after removing duplicates are deﬁned to be the\nstance tokens. We hypothesize that these stance\ntokens will play a key role during stance detection.\n3.2 Signiﬁcant Token Masking\nThere are two main approaches to train a trans-\nformer encoder, Causal Language Modeling (CLM)\nand Masked Language Modeling (MLM). CLM has\na standard language modeling objective, predicting\nthe next token given all previous tokens in the input\nsequence. This means that it needs to learn tokens\nin order and can only see the previous tokens. On\nthe other hand, MLM uses a masking technique\nthat is more ﬂexible, allowing researchers to explic-\nitly assign which tokens to mask. The other tokens\nare used for masked token recovery. Intuitively, a\nlanguage model that learns to recover a speciﬁc set\nof tokens well will tend to produce a better seman-\ntic representation for sequences containing those\ntokens (Tian et al., 2020; Ke et al., 2020; Zhou\net al., 2020). Generally, randomly masking tokens\nis preferred when the task requires the language\nmodel to learn to recover all tokens equally. This\ntends to result in a semantic representation that is\nequally good for any input sequences.\nIn many BERT-based models, when training the\ntransformer encoder with masked language mod-\neling, the input sequence is modiﬁed by randomly\nsubstituting tokens of the sequence. Speciﬁcally,\nBERT uniformly chooses 15% of input tokens of\nwhich 80% are replaced with a special masked to-\nken [MASK], 10% are replaced with a random\ntoken, and 10% are not replaced and remain un-\nchanged. The goal of signiﬁcant token masking\nis to produce a corrupted version of the input se-\nquence by masking the signiﬁcant tokens rather\nthan random tokens. We keep the same ratio of\nmasked tokens by masking up to 15% of the signif-\nicant tokens. If fewer than 15% of the tokens are\nsigniﬁcant, we randomly mask other tokens to ﬁll\nup to 15%.4\nFormally, signiﬁcant word masking creates a cor-\nrupted version ´X for an input sequence X that is\ninﬂuenced by the extracted knowledge G. Tokens\nof sequences X and ´X are denoted by xi and ´xi,\nrespectively. In the ﬁne-tuning process, the trans-\nformer encoder is trained using a masked word\nprediction objective that is supervised by recover-\ning masked signiﬁcant words using the ﬁnal state\nof the encoder ´x1,..., ´xn, where nis the length of\nthe sequence.\nAfter constructing this corrupted version of the\nsequence, MLM aims to predict the masked tokens\nto recover the original tokens. In this paper, we\ninject knowledge for our speciﬁc classiﬁcation task\nduring MLM, causing the model to pay more at-\ntention to stance tokens instead of random tokens.\n4With a set of 20-40 signiﬁcant words, their word counts\nare roughly 1% of the total number of tokens of the unlabeled\ndata that we trained the language model on.\n4729\nTable 1: Example sets of strong supportive and opposing tokens for both candidates on Twitter.\nBiden Trump\nSupport administration, ballot, bluewave, early,\nkamala, rule, safe, show, trust, voteblue\namericaﬁrst, follow, help, ifbap, kag,\nmaga, patriots, retweet, thanks, votered\nOppose bernie, black, blah, cities, kag, maga,\nmoney, patriots, woman, wwg1wga\nbluewave, consequences, demconvention,\ndivision, liar, make, republicans, resist, stand\nThese results are based on real data. Tokens are sorted alphabetically.\nFormally, we get an embedding vector ˜xi from the\ntransformer encoder by feeding the corrupted ver-\nsion ´X of input sequence X. Next, the embedding\nvector is fed into a single layer of neural network\nwith a softmax activation layer in order to produce\na normalized probability vector ˆyi over the entire\nvocabulary as shown in Equation 4, where W is a\nweight vector and bis a bias vector. Therefore, the\nprediction objective Lis to maximize the proba-\nbility of original token xi computed in Equation 5,\nwhere mi = 1 if the token at the i-th position is\nmasked, otherwise mi = 0 and yi is a one-hot\nrepresentation of the original token.\nˆyi = softmax( ˜xiW + b) (4)\nL= −\ni=n∑\ni=1\nmi ×yi log ˆyi (5)\nFinally, we ﬁne-tune a pre-trained BERT with\nunlabeled in-domain (election 2020) data. The\nrepresentation learned by the language model is\nexpected to be customized for the stance detection\ntask.\n4 Experimental Design\nIn this section we describe our experimental design,\nbeginning with the knowledge mining decisions,\nfollowed by the decisions and parameters used for\nthe language models.\n4.1 Stance Knowledge Mining\nWe begin by determining the number of signiﬁcant\nstance words to identify. Based on a sensitivity\nanalysis, we set k= 10 to extract the top-10 signif-\nicant words for each stance category as described\nin Section 3.1 (support, non-support, oppose, non-\noppose). Examples of signiﬁcant tokens from the\nstrong supportive/opposing stance are shown in\nTable 1. Our stance detection models are indepen-\ndently trained for each candidate, so overlapping\ntokens are allowed (e.g. the word patriots tends to\nsupport Trump but oppose Biden). Once we have a\nset of tokens for the four categories, we union these\nfour token sets. After removing duplicates, there\nare roughly 30 stance tokens for each candidate.\n4.2 Language Models\nBecause the state-of-the-art models for stance de-\ntection are neural models with pre-trained lan-\nguage models on a large amount of in-domain data,\n(Zarrella and Marsh, 2016; Küçük and Can, 2020),\nwe use both original pre-trained BERT and BERT\nﬁne-tuned on the unlabeled election data as our\nbenchmarks. We ﬁne-tuned BERT for two epochs\nsince it gives the best perplexity score 5. For KE-\nMLM, we ﬁrst initialize the weights of the model\nusing the same values as the original BERT, then\nwe ﬁne-tune the model with unlabeled election data\nusing the identiﬁed stance tokens masked. We ex-\nhaustively ﬁne-tuned KE-MLM to produce the lan-\nguage model that focuses attention on the stance\ntokens from the training set.\nBecause BERT’s tokenizer uses WordPiece (Wu\net al., 2016), a subword segmentation algorithm,\nit cannot learn new tokens after the pre-training\nis ﬁnished without explicitly specifying it. How-\never, adding new tokens with random embedding\nweights would cause the pre-trained model to work\ndifferently since it was not pre-trained with those\nnew tokens. We realize that some signiﬁcant to-\nkens for the stance of Election 2020 are new to\nthe BERT and were not in the original BERT pre-\ntraining process. Therefore, we consider adding\nall the stance words to the BERT tokenizer. We\nhypothesize that adding such a small number of\ntokens will barely affect the pre-trained model. To\ntest the effect of adding stance tokens into the nor-\nmal ﬁne-tuning process, we train language models\nin which stance tokens are added, but we ﬁne-tune\nthem with the normal random masking method. We\nrefer to this model as a-BERT, where stance tokens\n5Perplexity is a performance measurement of the masked\nlanguage model, a lower score is better.\n4730\nare added to the BERT tokenizer, but only the stan-\ndard ﬁne-tuning method is performed. To compare\nour performance to the sentiment knowledge en-\nhanced pre-training method or SKEP (Tian et al.,\n2020), we use the pre-training method proposed in\ntheir paper and then ﬁne-tune the model using our\nelection 2020 data (SKEP).\nWe hypothesize that applying KE-MLM may\nguide the language model to focus too much atten-\ntion on the stance knowledge and learn less seman-\ntic information about the election itself. Therefore,\nwe consider a hybrid ﬁne-tuning strategy. We begin\nby ﬁne-tuning BERT for one epoch. Then we ﬁne-\ntune using KE-MLM in the next epoch. This hy-\nbrid strategy forces the model to continually learn\nstance knowledge along with semantic information\nabout the election. We expect that this dual learning\nwill construct a language model biased toward nec-\nessary semantic information about the election, as\nwell as the necessary embedded stance knowledge.\nWe refer to this approach as our KE-MLM (with\ncontinuous ﬁne-tuning), while KE-MLM– refers to\na model that is overly ﬁne-tuned with only stance\ntoken masking.\nTo summarize, the language models we will\nevaluate are as follows: the original pre-trained\nBERT (o-BERT), a normally ﬁne-tuned BERT that\nuses our election data (f-BERT), a normally ﬁne-\ntuned BERT that uses stance tokens as part of\nits tokenizer (a-BERT), a ﬁne-tuned BERT using\nthe SKEP method (Tian et al., 2020) (SKEP), our\noverly ﬁne-tuned model (KE-MLM–), and our hy-\nbrid ﬁne-tuned model (KE-MLM). For all the lan-\nguage models, we truncate the size of an input se-\nquence to 512 tokens. The learning rate is constant\nat 1e−4 and the batch size is 16.\n4.3 Classiﬁcation Models\nIn masked language modeling, we ﬁne-tune the\nmodel using a neural layer on top with the learning\nobjective to predict masked tokens. In this step, we\nsubstitute that layer with a new neural layer as a\nstance classiﬁer layer. Its weights are arbitrarily\ninitialized. The prediction equation is similar to\nEquation 4 but now the input is not corrupted, and\nthe output is a vector of the normalized probability\nof the three stance classes. We use a cross-entropy\nloss function and the objective is to minimize it. We\nuse the Adam optimizer (Kingma and Ba, 2015)\nwith ﬁve different learning rates, including 2e−5,\n1e−5, 5e−6, 2e−6 and 1e−6. The batch\nsize is constantly set to 32 during the classiﬁcation\nlearning process.\nWe train and test our models on each candidate\nindependently with ﬁve different learning rates.\nThe best model is determined by the best macro\naverage F1 score over three classes among ﬁve\nlearning rates. Because the weights of the classiﬁer\nlayer are randomly initialized, we run each model\nﬁve times. The average F1 score is reported in\nTable 2 as the classiﬁcation performance.\n5 Empirical Evaluation\nAfter describing our data set (Section 5.1), we\npresent our experimental evaluation, both quan-\ntitative (Section 5.2), and qualitative (Section 5.3).\n5.1 Data Sets\nFor this study, our research team collected English\ntweets related to the 2020 US Presidential elec-\ntion. Through the Twitter Streaming API, we col-\nlected data using election-related hashtags and key-\nwords. Between January 2020 and September 2020,\nwe collected over 5 million tweets, not including\nquotes and retweets. These unlabeled tweets were\nused to ﬁne-tune all of our language models.\nOur speciﬁc stance task is to determine the\nstance for the two presidential candidates, Joe\nBiden and Donald Trump. For each candidate,\nwe had three stance classes: support, opposition,\nand neutral.6 We consider two stance-labeled data\nsets, one for each candidate, Biden and Trump.\nOur data were labeled using Amazon Mechanical\nTurk (MTurk) workers (Crowston, 2012). These\nworkers were not trained. Instead, we provided\na set of examples for each stance category that\nthey could refer to as they conducted the labeling\ntask. Examples of statements presented to MTurk\nworkers are presented in Table 3. We asked an-\nnotators to carefully review each tweet ti\nc from\nthe tweet set TC = {t1\nc,t2\nc,...}and determine\nwhether the tweet ti\nc is (i) clearly in support of C,\n(ii) clearly in opposition to C or (iii) not clearly\nin support or opposition to C, where ti\nc ∈ TC\nand C ∈{Donald Trump,Joe Biden}. To increase\nthe labeling yield, we verify that two tweet sets\nTC=Donald Trump and TC=Joe Biden are mutually ex-\nclusive. Each tweet was labeled by three annotators\nand the majority vote is considered to be the true\nlabel. If all three annotators vote for three differ-\n6Our deﬁnition of stance labels is consistent with the deﬁ-\nnition from (Mohammad et al., 2016a)\n4731\nTable 2: The average F1 scores over ﬁve runs. The conﬁdence intervals for the macro F1 scores are computed\nbased on a signiﬁcance level of 0.05, meaning a 95% conﬁdence level. The highest scores are shown in boldface.\nBiden Trump\nModel F1-Support F1-Oppose F1-Neutral F1-macro F1-Support F1-Oppose F1-Neutral F1-macro\no-BERT 0.7324 0.6875 0.7151 0.7117 ( ±0.0063) 0.7574 0.8101 0.6955 0.7543 ( ±0.0069)\nf-BERT 0.7743 0.7226 0.7347 0.7439 ( ±0.0049) 0.7921 0.8147 0.6961 0.7677 ( ±0.0084)\na-BERT 0.7905 0.7234 0.7432 0.7523 ( ±0.0049) 0.8090 0.8154 0.6926 0.7724 ( ±0.0078)\nSKEP 0.7923 0.7153 0.7349 0.7475 ( ±0.0047) 0.7852 0.8169 0.7151 0.7724 ( ±0.0067)\nKE-MLM– 0.7618 0.7303 0.7380 0.7434 ( ±0.0040) 0.7854 0.7968 0.7083 0.7635 ( ±0.0081)\nKE-MLM 0.7927 0.7329 0.7475 0.7577 ( ±0.0032) 0.8094 0.8184 0.7354 0.7877 ( ±0.0075)\nTable 3: Sample of stance examples presented to\nMTurk labelers.\nCandidate Statement Stance\nBiden\nBiden will be a great president. I am voting\nfor him in November. Support\nBiden has handled the pandemic poorly.Oppose\nBiden spoke in Pennsylvania. Neutral\nTrump\nTrump has been a great president. I am voting\nfor him in November. Support\nTrump has handled the pandemic poorly.Oppose\nTrump held a rally yesterday. Neutral\nent classes, we assume the tweet’s label is neutral\nbecause the stance is ambiguous.\nOur data set contains 1250 stance-labeled tweets\nfor each candidate. The stance label distributions\nare shown in Table 4. The distributions of both\ncandidates are skewed towards the opposition label.\nOverall, the stance class proportions vary from 27%\nto 39%. The inter-annotator agreement scores from\ndifferent metrics are shown in Table 5. The task-\nbased and worker-based metrics are recommended\nby the MTurk ofﬁcial site (Amazon, 2011), given\ntheir annotating mechanism. All scores are range\nfrom 86% up to 89%, indicating the high inter-rater\nreliability for these data sets.\nTable 4: Stance distribution for Biden and Trump.\n%SUPPORT %OPPOSE %NEUTRAL\nBiden 31.3 39.0 29.8\nTrump 27.3 39.9 32.8\nTable 5: Mechanical Turk inter-annotator agreement\nfor Biden and Trump.\nMetric Biden Trump\nTask-based 0.8693 0.8920\nWorker-based 0.8915 0.8969\n5.2 Experimental Results\nWe conducted experiments on train-test sets using\na 70:30 split for both the Biden and Trump data\nsets.7 We evaluate the classiﬁcation performance\nusing the macro-average F1 score along with the\nF1 score of each class. The results presented in\nTable 2 show the average F1 scores over ﬁve runs\nwith different random seeds. The highest score for\neach evaluation metric is highlighted in bold.\nFor Biden-stance, every ﬁne-tuning method (f-\nBERT, a-BERT, SKEP, KE-MLM– and KE-MLM)\nimproves the average F1 score from the original\npre-trained model by 3.2%, 4.1%, 3.6%, 3.2% and\n4.6%, respectively. For Trump-stance, the aver-\nage F1 scores are also improved by 1.3%, 1.8%,\n1.8%, 0.9% and 3.3%. The improvement is twice\nas much for Biden than for Trump. This is an indi-\ncation that the additional background knowledge is\nmore important for detecting stance for Biden than\nfor Trump. In general, our knowledge enhanced\nmodel performs better than all the other models\nand outperforms the original BERT by three to\nﬁve percent. a-BERT performs similarly to SKEP\nfor Trump, but its performance is better for Biden.\nThe model’s overall performances are second-best\nwith only a difference of 0.5% and 1.5% in the\naverage F1-macro score when compared to KE-\nMLM for Biden and Trump, respectively. These\nresults further highlight the importance of incor-\nporating stance tokens into the tokenizer. While\nadding stance to the tokenization is important, the\nadditional improvement of KE-MLM comes from\nfocusing attention on both the stance tokens and the\ngeneral election data. The result also supports our\nhypothesis that training KE-MLM– alone for two\nepochs would result in better accuracy than orig-\ninal BERT (o-BERT), but a lower accuracy than\nnormally ﬁne-tuned BERT (f-BERT) because it\nlearns stance knowledge but lacks in-domain elec-\ntion knowledge.\nTo better understand the robustness of our mod-\nels, we analyze the variance in the F1 scores across\n7Because we do not have sufﬁcient unlabeled election data\nfrom 2016, we cannot fairly test our model with the SemEval\n2016 stance data.\n4732\nthe different runs. Figure 1 shows the box plots\nof the macro average F1 scores for each model.\nThe scores of both candidates follow a similar pat-\ntern. For Biden, the highest F1 score and the lowest\nvariance is KE-MLM. For Trump, the highest F1\nscore is KE-MLM, but the variance is comparable\nto the other models. The model with the lowest\nvariance is SKEP. These ﬁgures further emphasize\nKE-MLM’s ability to detect stance better than nor-\nmally ﬁne-tuning methods. Interestingly, a-BERT\nperforms second-best (see gray boxes in Figure 1),\nfurther highlighting the importance of not ignor-\ning stance tokens. Forcefully adding unseen stance\ntokens to the BERT tokenizer with random initial\nweights beneﬁts overall classiﬁcation performance.\n(a) Biden\n(b) Trump\nFigure 1: The distribution of macro average F1 scores\nfrom ﬁve independent runs.\nAdditionally, we conducted a sensitivity anal-\nysis on different sizes of unlabeled data for pre-\ntraining to verify that the large unlabeled data is\nactually beneﬁcial. We ﬁne-tune f-BERT using\ndifferent sizes of data (100K, 500K, 1M, 2M) and\ncompare the results to those of BERT with zero-\nﬁne-tuning (o-BERT) and ﬁne-tuning using the en-\ntire 5M tweets (f-BERT). We train each pre-trained\nlanguage model on training and test on testing data\nset ﬁve times. The average F1 scores are shown\nin Fig 2. For Biden, the average F1 score is 3%\nlower when there is no ﬁne-tuning compared to\nusing all 5M tweets. For Trump, the score only\nimproves a little over 1%. Interestingly, as the size\nof the unlabeled data increases, the F1 score also\nincreases even though the increase is not always\nlarge. Therefore, pre-training using a smaller size\nunlabeled data set does still produce beneﬁts, but\nwhen possible, using a large sample does lead to\nimprovement.\nFigure 2: The model performance by f-BERT pre-\ntrained on different sizes of unlabeled data. We train\neach model ﬁve times and report the average F1 scores.\n5.3 Qualitative Analysis of the Effect of\nStance Knowledge\nWhile we see from Table 2 that KE-MLM outper-\nforms all baselines on average, we are interested\nin understanding when there is labeling disagree-\nment between other methods and KE-MLM, what\nfeatures are driving the disagreement. Therefore,\nwe manually investigate samples in which f-BERT\nand a-BERT produced incorrect predictions, while\nKE-MLM produced correct ones. On average over\nmultiple runs, 28.8% and 38.5% of misclassiﬁed\ntweets by f-BERT are correctly predicted by KE-\nMLM for Biden and Trump, respectively. For a-\nBERT, they are 22.5% and 25.7% on average. As a\ncase example, Table 6 illustrates the attention dis-\ntribution of the sequence representation learned by\neach language model for a few mislabeled tweets.\nSigniﬁcant words are colored. The color darkness\nis determined by the attention weights of the repre-\nsentation learned for the classiﬁcation token.8 The\n8The representation of classiﬁcation tokens produced by a\ntransformer encoder is usually referred to as [CLS]. Please\nsee (Devlin et al., 2019) for details about the attention weight\ncalculation.\n4733\nTable 6: Visualization of selected samples with attention weight distribution by color darkness.\nCandidate Model Sampled Sentence Prediction\nf-BERT The democratsand @joebidenbelieve in the power ofthe government.\nThe #gopand @realdonaldtrump believe in the power of the american people. #magaNeutral\nBiden a-BERT The democrats and@joebidenbelievein the powerof the government.\nThe#gop and @realdonaldtrumpbelievein the power of the americanpeople . #maga Neutral\nKE-MLMThedemocratsand @joebidenbelievein the power of the government.\nThe#gop and @realdonaldtrumpbelievein the powerof theamerican people.#maga Opposition\nf-BERT Covid -19 was Trump’sbiggest test. He failedmiserably . #demconvention Neutral\nTrump a-BERT Covid -19 was Trump’sbiggest test. Hefailed miserably. #demconvention Neutral\nKE-MLMCovid -19was Trump’sbiggest test. Hefailedmiserably .#demconvention Opposition\ndarker the color the more important the word. From\nthe selected samples, we know from the knowledge\nmining step that the word \"maga\" and \"demcon-\nvention\" are two of the most distinguishing stance\nwords (see Table 1), but both f-BERT and a-BERT\nfail to identify these strong stance words and there-\nfore, produced incorrect predictions. In contrast,\nKE-MLM produces the correct predictions by pay-\ning reasonable attention to the stance information,\nfurther supporting the notion that KE-MLM is us-\ning meaningful, interpretable tokens.\n6 Conclusions and Future Directions\nIntuitively, a language model ﬁne-tuned using in-\ndomain unlabeled data should result in better clas-\nsiﬁcation performance than using the vanilla pre-\ntrained BERT. Since our goal is to maximize the\naccuracy of a speciﬁc classiﬁcation task, we train\nan attention-based language model to pay attention\nto words that help distinguish between the classes.\nWe have shown that for stance detection, using the\nlog-odds-ratio to identify signiﬁcant tokens that\nseparate the classes is important knowledge for this\nclassiﬁcation task. Once these important tokens\nare identiﬁed, forcing the language model to pay\nattention to these tokens further improves the per-\nformance when compared to using standard data\nfor ﬁne-tuning. To the best of our knowledge, our\napproach is better than the other state-of-the-art\napproaches for stance detection. Additionally, we\nare releasing our data set to the community to help\nother researchers continue to make progress on the\nstance detection task. We believe this is the ﬁrst\nstance-labeled Twitter data for the 2020 US Presi-\ndential election.\nThere are several future directions of this work.\nFirst, to relax the trade-off between learning elec-\ntion semantics in general and learning stance\nknowledge, instead of ﬁne-tuning one epoch with\nthe normal ﬁne-tuning method and another epoch\nwith KE-MLM, we could reduce the masking prob-\nability of stance distinguishing words from 100% to\nsomething lower based on the distinguishability of\nthe token. Theoretically, this would give a higher\nweight to words that are more polarizing. This\nalso relaxes the potential overﬁtting that may oc-\ncur when learning only stance knowledge and lets\nthe model randomly learn more tokens. Another\nfuture direction is to test our language modeling\nmethod on other classiﬁcation tasks (e.g. sentiment\nanalysis, spam detection). Also, this paper uses\nBERT as the base language model. There are many\nvariations of BERT that can be further investigated\n(e.g. RoBERTa). Finally, we view stance as an im-\nportant task for understanding public opinion. As\nour models get stronger, using them to gain insight\ninto public opinion on issues of the day is another\nimportant future direction.\nAcknowledgements\nThis research was funded by National Science\nFoundation awards #1934925 and #1934494, and\nthe Massive Data Institute (MDI) at Georgetown\nUniversity. We would like to thank our funders,\nthe MDI staff, and the members of the Georgetown\nDataLab for their support. We would also like to\nthank the anonymous reviewers for the detailed and\nthoughtful reviews.\nReferences\nAbeer Aldayel and Walid Magdy. 2019. Your stance\nis exposed! analysing possible factors for stance\ndetection on social media. The ACM on Human-\nComputer Interaction, 3(CSCW):1–20.\nAbeer AlDayel and Walid Magdy. 2020. Stance de-\ntection on social media: State of the art and trends.\narXiv preprint arXiv:2006.03644.\n4734\nAmazon. 2011. Hit review policies. https:\n//docs.aws.amazon.com/AWSMechTurk/\nlatest/AWSMturkAPI/ApiReference_\nHITReviewPolicies.html.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics (TACL) , 5:135–\n146.\nGerlof Bouma. 2009. Normalized (pointwise) mutual\ninformation in collocation extraction. In Proceed-\nings of the biennial Conference of the German So-\nciety for Computational Linguistics and Language\nTechnology (GSCL).\nCeren Budak. 2019. What happened? the spread of\nfake news publisher content during the 2016 us pres-\nidential election. In Proceedings of the World Wide\nWeb Conference (WWW).\nPeng Chen, Zhongqian Sun, Lidong Bing, and Wei\nYang. 2017. Recurrent attention network on mem-\nory for aspect sentiment analysis. In Proceedings\nof the Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP).\nKevin Crowston. 2012. Amazon mechanical turk: A\nresearch tool for organizations and information sys-\ntems scholars. In Shaping the Future of ICT Re-\nsearch. Methods and Approaches.\nMarco Del Tredici, Diego Marcheggiani, Sabine\nSchulte im Walde, and Raquel Fernández. 2019.\nYou shall know a user by the company it keeps:\nDynamic representations for social media users in\nNLP. In Proceedings of the Conference on Empir-\nical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP).\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics (NAACL).\nKuntal Dey, Ritvik Shrivastava, and Saroj Kaushik.\n2017. Twitter stance detection—a subjectivity and\nsentiment polarity inspired two-phase approach. In\nIEEE international conference on data mining work-\nshops (ICDMW).\nJiachen Du, Ruifeng Xu, Yulan He, and Lin Gui. 2017.\nStance classiﬁcation with target-speciﬁc neural at-\ntention networks. In Proceedings of the Twenty-\nSixth International Joint Conference on Artiﬁcial In-\ntelligence (IJCAI).\nShalmoli Ghosh, Prajwal Singhania, Siddharth Singh,\nKoustav Rudra, and Saptarshi Ghosh. 2019. Stance\ndetection in web and social media: a comparative\nstudy. Experimental IR Meets Multilinguality, Mul-\ntimodality, and Interaction, pages 75–87.\nMarco Guerini, Lorenzo Gatti, and Marco Turchi. 2013.\nSentiment analysis: How to derive prior polarities\nfrom SentiWordNet. In Proceedings of the Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP).\nLuyao Huang, Chi Sun, Xipeng Qiu, and Xuanjing\nHuang. 2019. GlossBERT: BERT for word sense\ndisambiguation with gloss knowledge. In Proceed-\nings of the Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP).\nLong Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and\nTiejun Zhao. 2011. Target-dependent twitter senti-\nment classiﬁcation. In Proceedings of the Annual\nMeeting of the Association for Computational Lin-\nguistics (ACL).\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\nLuke Zettlemoyer, and Omer Levy. 2019. Span-\nBERT: Improving pre-training by representing and\npredicting spans. Transactions of the Association\nfor Computational Linguistics (TACL), 8:64–77.\nDan Jurafsky, Victor Chahuneau, Bryan R Routledge,\nand Noah A Smith. 2014. Narrative framing of con-\nsumer sentiment in online restaurant reviews. First\nMonday.\nPei Ke, Haozhe Ji, Siyang Liu, Xiaoyan Zhu, and Min-\nlie Huang. 2020. SentiLARE: Sentiment-aware lan-\nguage representation learning with linguistic knowl-\nedge. In Proceedings of the Conference on Em-\npirical Methods in Natural Language Processing\n(EMNLP).\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In Proceedings\nof the International Conference on Learning Repre-\nsentations (ICLR).\nDilek Küçük and Fazli Can. 2020. Stance detection: A\nsurvey. ACM Computing Surveys (CSUR), 53(1).\nYoav Levine, Barak Lenz, Or Dagan, Dan Padnos,\nOr Sharir, Shai Shalev-Shwartz, Amnon Shashua,\nand Yoav Shoham. 2020. SenseBERT: Driving\nsome sense into BERT. In Proceedings of the An-\nnual Meeting of the Association for Computational\nLinguistics (ACL).\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nMargaret Mitchell, Jacqui Aguilar, Theresa Wilson,\nand Benjamin Van Durme. 2013. Open domain tar-\ngeted sentiment. In Proceedings of the Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP).\nSaif Mohammad, Svetlana Kiritchenko, Parinaz Sob-\nhani, Xiaodan Zhu, and Colin Cherry. 2016a. A\n4735\ndataset for detecting stance in tweets. In Proceed-\nings of the International Conference on Language\nResources and Evaluation (LREC).\nSaif Mohammad, Svetlana Kiritchenko, Parinaz Sob-\nhani, Xiaodan Zhu, and Colin Cherry. 2016b.\nSemeval-2016 task 6: Detecting stance in tweets. In\nProceedings of the International Workshop on Se-\nmantic Evaluation (SemEval).\nBurt L Monroe, Michael P Colaresi, and Kevin M\nQuinn. 2008. Fightin’words: Lexical feature selec-\ntion and evaluation for identifying the content of po-\nlitical conﬂict. Political Analysis, 16(4):372–403.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the Conference of\nthe North American Chapter of the Association for\nComputational Linguistics (NAACL).\nLahari Poddar, Wynne Hsu, and Mong Li Lee. 2017.\nAuthor-aware aspect topic sentiment model to re-\ntrieve supporting opinions from reviews. In Pro-\nceedings of the Conference on Empirical Methods\nin Natural Language Processing (EMNLP).\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding with unsupervised learning. Technical re-\nport, OpenAI.\nMartin Schmitt, Simon Steinheber, Konrad Schreiber,\nand Benjamin Roth. 2018. Joint aspect and polar-\nity classiﬁcation for aspect-based sentiment analy-\nsis with end-to-end neural networks. In Proceedings\nof the Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP).\nAnirban Sen, Manjira Sinha, Sandya Mannarswamy,\nand Shourya Roy. 2018. Stance classiﬁcation of\nmulti-perspective consumer health information. In\nProceedings of the ACM India Joint International\nConference on Data Science and Management of\nData.\nUmme Aymun Siddiqua, Abu Nowshed Chy, and\nMasaki Aono. 2019. Tweet stance detection using\nan attention based neural ensemble model. In Pro-\nceedings of the Conference of the North American\nChapter of the Association for Computational Lin-\nguistics (NAACL).\nParinaz Sobhani, Saif Mohammad, and Svetlana Kir-\nitchenko. 2016. Detecting stance in tweets and ana-\nlyzing its interaction with sentiment. In Proceedings\nof the Fifth Joint Conference on Lexical and Compu-\ntational Semantics (*SEM).\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi\nChen, Han Zhang, Xin Tian, Danxiang Zhu, Hao\nTian, and Hua Wu. 2019. ERNIE: Enhanced rep-\nresentation through knowledge integration. arXiv\npreprint arXiv:1904.09223.\nHao Tian, Can Gao, Xinyan Xiao, Hao Liu, Bolei He,\nHua Wu, Haifeng Wang, and Feng Wu. 2020. SKEP:\nSentiment knowledge enhanced pre-training for sen-\ntiment analysis. In Proceedings of the Annual Meet-\ning of the Association for Computational Linguistics\n(ACL).\nRui Wang, Deyu Zhou, Mingmin Jiang, Jiasheng Si,\nand Yang Yang. 2019. A survey on opinion min-\ning: From stance to product aspect. IEEE Access,\n7:41101–41124.\nWan Wei, Xiao Zhang, Xuqin Liu, Wei Chen, and\nTengjiao Wang. 2016. pkudblab at SemEval-2016\ntask 6 : A speciﬁc convolutional neural network sys-\ntem for effective stance detection. In Proceedings of\nthe International Workshop on Semantic Evaluation\n(SemEval).\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural machine\ntranslation system: Bridging the gap between hu-\nman and machine translation. arXiv preprint\narXiv:1609.08144.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXLNet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural In-\nformation Processing Systems (NeurIPS).\nGuido Zarrella and Amy Marsh. 2016. Mitre at\nsemeval-2016 task 6: Transfer learning for stance\ndetection. In Proceedings of the International Work-\nshop on Semantic Evaluation (SemEval).\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: En-\nhanced language representation with informative en-\ntities. In Proceedings of the Annual Meeting of the\nAssociation for Computational Linguistics (ACL).\nJie Zhou, Junfeng Tian, Rui Wang, Yuanbin Wu,\nWenming Xiao, and Liang He. 2020. SentiX: A\nsentiment-aware pre-trained model for cross-domain\nsentiment analysis. In Proceedings of the Inter-\nnational Conference on Computational Linguistics\n(COLING).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7722776532173157
    },
    {
      "name": "Language model",
      "score": 0.7268534898757935
    },
    {
      "name": "Security token",
      "score": 0.5922396779060364
    },
    {
      "name": "Masking (illustration)",
      "score": 0.5754016041755676
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5601358413696289
    },
    {
      "name": "Natural language processing",
      "score": 0.5320234298706055
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.45555222034454346
    },
    {
      "name": "Sentence",
      "score": 0.4491163194179535
    },
    {
      "name": "Terminology",
      "score": 0.42238736152648926
    },
    {
      "name": "Speech recognition",
      "score": 0.4077315330505371
    },
    {
      "name": "Linguistics",
      "score": 0.1566397249698639
    },
    {
      "name": "Mathematics",
      "score": 0.0888834297657013
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I184565670",
      "name": "Georgetown University",
      "country": "US"
    }
  ]
}