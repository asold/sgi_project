{
  "title": "Galaxy morphology classification based on Convolutional vision Transformer (CvT)",
  "url": "https://openalex.org/W4391493884",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2032104534",
      "name": "Cao Jie",
      "affiliations": [
        "Minzu University of China"
      ]
    },
    {
      "id": "https://openalex.org/A1929929420",
      "name": "Tingting Xu",
      "affiliations": [
        "Minzu University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2096700163",
      "name": "Yuhe Deng",
      "affiliations": [
        "Minzu University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2421524102",
      "name": "Linhua Deng",
      "affiliations": [
        "Minzu University of China"
      ]
    },
    {
      "id": "https://openalex.org/A4378246229",
      "name": "Mingcun Yang",
      "affiliations": [
        "Minzu University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2114605491",
      "name": "Zhijing Liu",
      "affiliations": [
        "Minzu University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2097705559",
      "name": "Wei-Hong Zhou",
      "affiliations": [
        "Minzu University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2032104534",
      "name": "Cao Jie",
      "affiliations": [
        "Minzu University of China"
      ]
    },
    {
      "id": "https://openalex.org/A1929929420",
      "name": "Tingting Xu",
      "affiliations": [
        "Minzu University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2096700163",
      "name": "Yuhe Deng",
      "affiliations": [
        "Minzu University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2421524102",
      "name": "Linhua Deng",
      "affiliations": [
        "Minzu University of China"
      ]
    },
    {
      "id": "https://openalex.org/A4378246229",
      "name": "Mingcun Yang",
      "affiliations": [
        "Minzu University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2114605491",
      "name": "Zhijing Liu",
      "affiliations": [
        "Minzu University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2097705559",
      "name": "Wei-Hong Zhou",
      "affiliations": [
        "Minzu University of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2124199755",
    "https://openalex.org/W2968857122",
    "https://openalex.org/W2170689655",
    "https://openalex.org/W2155488926",
    "https://openalex.org/W3006505613",
    "https://openalex.org/W2768455873",
    "https://openalex.org/W6600423979",
    "https://openalex.org/W3173211033",
    "https://openalex.org/W4294975187",
    "https://openalex.org/W3111030224",
    "https://openalex.org/W4361251615",
    "https://openalex.org/W2755925698",
    "https://openalex.org/W4311344718",
    "https://openalex.org/W4378174676",
    "https://openalex.org/W2112193291",
    "https://openalex.org/W2530273440",
    "https://openalex.org/W4224216771",
    "https://openalex.org/W2015159529",
    "https://openalex.org/W2946593308",
    "https://openalex.org/W4214708455",
    "https://openalex.org/W4310220169",
    "https://openalex.org/W2007765466",
    "https://openalex.org/W2023037133",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W2157917411",
    "https://openalex.org/W2883005809"
  ],
  "abstract": "Context. The classification of galaxy morphology is among the most active fields in astronomical research today. With the development of artificial intelligence technology, deep learning is a useful tool in the classification of the morphology of galaxies and significant progress has been made in this domain. However, there is still some room for improvement in terms of classification accuracy, automation, and related issues. Aims. Convolutional vision Transformer (CvT) is an improved version of the Vision Transformer (ViT) model. It improves the performance of the ViT model by introducing a convolutional neural network (CNN). This study explores the performance of the CvT model in the area of galaxy morphology classification. Methods. In this work, the CvT model was applied, for the first time, in a five-class classification task of galaxy morphology. We added different types and degrees of noise to the original galaxy images to verify that the CvT model achieves good classification performance, even in galaxy images with low signal-to-noise ratios (S/Ns). Then, we also validated the classification performance of the CvT model for galaxy images at different redshifts based on the low-redshift dataset GZ2 and the high-redshift dataset Galaxy Zoo CANDELS. In addition, we visualized and analyzed the classification results of the CvT model based on the t-distributed stochastic neighborhood -embedding (t-SNE) algorithm. Results. We find that (1) compared with other five-class classification models of galaxy morphology based on CNN models, the average accuracy, precision, recall, and F1_score evaluation metrics of the CvT classification model are all higher than 98%, which is an improvement of at least 1% compared with those based on CNNs; (2) the classification visualization results show that different categories of galaxies are separated from each other in multi-dimensional space. Conclusions. The application of the CvT model to the classification study of galaxy morphology is a novel undertaking that carries important implications for future studies.",
  "full_text": "Astronomy &Astrophysics \nA&A, 683, A42 (2024)\nhttps://doi.org/10.1051/0004-6361/202348544\n© The Authors 2024\nGalaxy morphology classification based on Convolutional vision\nTransformer (CvT)⋆\nJie Cao1, Tingting Xu1, Yuhe Deng1, Linhua Deng1, Mingcun Yang1, Zhijing Liu1, and Weihong Zhou1,2\n1 School of Mathematics and Computer Science, Yunnan Minzu University, Kunming, Yunnan 650504, PR China\ne-mail: xutingting@ymu.edu.cn; linhua.deng@ymu.edu.cn; ynzwh@163.com\n2 Key Laboratory for the Structure and Evolution of Celestial Objects, Chinese Academy China of Sciences, Kunming,\nYunnan 650011, PR China\nReceived 10 November 2023 / Accepted 2 January 2024\nABSTRACT\nContext. The classification of galaxy morphology is among the most active fields in astronomical research today. With the devel-\nopment of artificial intelligence technology, deep learning is a useful tool in the classification of the morphology of galaxies and\nsignificant progress has been made in this domain. However, there is still some room for improvement in terms of classification accu-\nracy, automation, and related issues.\nAims. Convolutional vision Transformer (CvT) is an improved version of the Vision Transformer (ViT) model. It improves the perfor-\nmance of the ViT model by introducing a convolutional neural network (CNN). This study explores the performance of the CvT model\nin the area of galaxy morphology classification.\nMethods. In this work, the CvT model was applied, for the first time, in a five-class classification task of galaxy morphology. We\nadded different types and degrees of noise to the original galaxy images to verify that the CvT model achieves good classification\nperformance, even in galaxy images with low signal-to-noise ratios (S/Ns). Then, we also validated the classification performance of\nthe CvT model for galaxy images at different redshifts based on the low-redshift dataset GZ2 and the high-redshift dataset Galaxy Zoo\nCANDELS. In addition, we visualized and analyzed the classification results of the CvT model based on the t-distributed stochastic\nneighborhood -embedding (t-SNE) algorithm.\nResults. We find that (1) compared with other five-class classification models of galaxy morphology based on CNN models, the\naverage accuracy, precision, recall, and F1_score evaluation metrics of the CvT classification model are all higher than 98%, which\nis an improvement of at least 1% compared with those based on CNNs; (2) the classification visualization results show that different\ncategories of galaxies are separated from each other in multi-dimensional space.\nConclusions. The application of the CvT model to the classification study of galaxy morphology is a novel undertaking that carries\nimportant implications for future studies.\nKey words. methods: data analysis – techniques: image processing – Galaxy: general\n1. Introduction\nGalaxies are the fundamental units making up the Universe, con-\nsisting mainly of stars, stellar remnants, interstellar gas, dust, and\ndark matter. One of the important goals of galaxy research is to\nexplain how galaxies evolved over time from their early stages to\nthe large, diverse types of galaxies that can be seen today (Wen\net al. 2014). In particular, studies of the morphology and struc-\nture of galaxies are crucial in understanding their evolution, as\nthe morphology and structure of galaxies are not only closely\nrelated to the evolutionary history of the galaxy, but also one\nof the characteristics of exploring the physical parameters of\nthe galaxy. By studying the formation and evolution of galax-\nies, astronomers can gain a comprehensive understanding of the\nformation history of the universe. At the same time, the mor-\nphology and characteristics of galaxies vary greatly in different\nevolutionary processes. Early-type galaxies in their photometry\ntend to have a reddish color, older stellar populations, and most\nof them feature bulges dominated by velocity dispersion in their\n⋆ Details on how to access these can be found at https://github.\ncom/C-JIe123/Galaxy-Morphology\ncenters. They also have shell structures left behind by multi-\nple galaxy mergers. High-mass early-type galaxies are mostly\nlocated in dense environments and appear as elliptical galax-\nies in morphology. Late-type galaxies exhibit a bluish color in\nphotometry, with a relatively young stellar population compo-\nsition, containing a large amount of cold gas and exhibiting\nintense star formation activity. The motion state of stars in late-\ntype galaxies is dominated by rotation around the center and is\nmanifested as a disk-like galaxy with a stellar disk and spiral\narm structure in terms of its morphology (Colless et al. 2001).\nTherefore, accurately classifying galaxies based on their mor-\nphological characteristics is the foundation for subsequent data\nanalysis and mining.\nThere are different classification standards for galaxy mor-\nphology and the Hubble sequence proposed by Hubble in 1926\nis the most famous early classification standard for galaxy mor-\nphology. Due to its significant correlation with physical parame-\nters such as the mass of neutral hydrogen, integral color of galax-\nies, galaxy luminosity, and environment, it still exhibits a high\nreference value to this day. Based on the Hubble sequence, the\nGalaxy Zoo (GZ; Lintott et al. 2008) has provided a classification\nstandard for galaxy morphology since 2007.\nA42, page 1 of 11\nOpen Access article, published by EDP Sciences, under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0),\nwhich permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.\nThis article is published in open access under the Subscribe to Open model. Subscribe to A&A to support open access publication.\nCao, J., et al.: A&A, 683, A42 (2024)\nIn recent years, the survey depth and detection efficiency\nof astronomical observation equipment have been continuously\nimproved. The observation of Sloan Digital Sky Survey (SDSS;\nYork et al. 2000, LAMOST (Cui et al. 2012), and James Webb\nSpace Telescope (JWST; Gardner et al. 2023) as well as other\nsurvey projects have produced massive galaxy spectral data and\nimage data, Thus, there is an urgent need for more automated\nand intelligent classification methods to meet the processing\nneeds of large-scale galaxy image data. With the continuous\ndevelopment of deep learning technology, deep learning-related\nalgorithms have been widely applied in the field of astronomy.\nThe study of galaxy morphology classification based on deep\nlearning algorithms is one of the hotspots in this research.\nMachine learning techniques achieve very good results in\nthe binary classification task of galaxy morphology. Domínguez\nSánchez et al. (2018) used a convolutional neural network (CNN)\nto obtain classifications by a deep learning algorithm and clas-\nsified the morphological catalog of 670 000 galaxies from the\nSDSS into two types, T-type and Galaxy Zoo 2 (GZ2), associ-\nated with the Hubble sequence and obtained good classification\nresults: T-type with smaller offsets and scatter and GZ2 with a\nhigher accuracy (>97%), precision, and recall (>90%). Cheng\net al. (2020) employed various machine learning and deep learn-\ning models to classify the morphology of a sample of 2800\ngalaxies in the GZ dataset, including support vector machine\n(SVM), random forest, and CNNs, and compared the differ-\nences in the classification performance and efficiency among\nthese models. The results show that CNN is the most successful\nmethod among these methods, with a morphological classifi-\ncation accuracy of 0.99 for elliptical and spiral galaxies, and\nthey improve the average classification accuracy of CNN to\n0.994 after correcting for labels. Walmsley et al. (2020) pro-\nposed a Bayesian CNN-based approach to handle uncertainties\nin morphological classification by introducing a probabilistic\nmodel. These authors used the GZ dataset for experimental\nevaluations and comparisons with other classification meth-\nods. The experimental results show that the Bayesian CNN\nand active learning-based approach show better performance in\nterms of morphological classification accuracy and uncertainty\nestimation.\nSecondly, machine learning techniques have also been very\nwidely used in tasks based on classes of three, five, and ten in\nterms of galaxy morphology. The multi-class classification task\nof galaxy morphology is intended to categorize galaxies into\nmultiple classes based on their appearance and structural fea-\ntures, it helps to reveal relationships among different types of\ngalaxies and provides a deeper understanding of the structure\nand evolution of the universe. Multi-class classification tasks are\nmore complex and varied than binary classification tasks and\nthe classification results often depend on the diversity and com-\nplexity of the galaxy images to be classified. Zhu et al. (2019)\nobtained the deep residual network model ResNet-26 by improv-\ning the residual unit of the residual network (ResNet). They\nextracted and analyzed the morphological features of galaxies,\nand verified the effectiveness of the model in classifying galaxy\nmorphology with a classification accuracy of 95.12%. Gupta\net al. (2022) used the NODE method to classify galaxy morphol-\nogy based on galaxy images from the GZ2 dataset and obtained\nan accuracy of 91–95%. The results show that the accuracy of\nNODE is comparable to that of the ResNet model, but the advan-\ntage of this method is that the classification process has fewer\nparameters and consumes less memory. Ai et al. (2022) applied\nthe EfficientNet model to the classification research of galaxy\ndata morphology. The experimental results show that the average\naccuracy, precision, recall, and F1_score based on EfficientNet-\nB5 model are all above 96.6%, which shows a great improvement\ncompared with the classification results of ResNet-26 model in\nthe residual network (ResNet). Wei et al. (2022) proposed a\nmethod based on contrastive learning. With the aim of charac-\nterizing galaxy images with sparse semantic information and\ndominated contour, vision transformers and CNNs were used\ncoherently in the feature extraction layer to provide rich seman-\ntic representation by fusing multi-level features. The method was\ntrained and tested on GZ2 and other data sets, and the accuracy\nrates reached 94.7%, 96.5%, and 89.9% respectively. He et al.\n(2023) designed a multi-channel depth residual network frame-\nwork ResNet-Core based on the ResNet-50 network structure.\nBy respectively targeting the characteristics of spectral images\nand galaxy images by incorporating convolutional kernel vari-\nance control technology to extract contour and detail features, the\naverage accuracy was effectively improved, surpassing the high-\nest performance ResNet-50 at the time. The results showed that\nthe ResNet-Core model had better classification performance\nand better robustness. Hui et al. (2022) proposed applying the\nDenseNet algorithm to galaxy morphology classification. The\nexperimental results showed that the accuracy obtained using the\nDenseNet-121 model was 91.79%, which means that out of 3044\ntest images, 2794 galaxy images can be accurately classified;\nin addition, the accuracy of the model is 79.92%, the recall is\n73.20%, and the F1_Score is 75.48%. Li et al. (2023) proposed a\nmulti-scale convolutional capsule network (MSCCN) model for\ngalaxy morphology classification research and this model uses\na multi-branch structure to extract multi-scale hidden features\nof galaxy images, and is trained and tested on the GZ2 dataset.\nThe experimental results show that the model achieves a macro-\nscopic averaging accuracy of 97%, 96%, and 98% recall and a\n97% F1_score. It is worth noting that the MSCCN 5-class clas-\nsification model based on the GZ2 dataset has achieved the best\nclassification performance so far.\nSince 2021, the transformer model (Vaswani et al. 2017) in\ndeep learning has achieved great success in natural language pro-\ncessing (NLP). At the same time, the Google team has developed\na new image classification architecture, called Vision Trans-\nformer (ViT; Dosovitskiy et al. 2021). The ViT model has been\nwidely used in various fields of classification tasks since its\nrelease. Gheflati & Rivaz (2022) applied the ViT model to the\nmedical field and classified breast ultrasound images. The results\nshowed that the ViT model had better classification performance\nthan the CNN model for breast ultrasound images. Gao et al.\n(2021) participated in the MIA-COV19 challenge with the ViT\nmodel. They classified COVID-19 and non COVID-19 according\nto CT lung images. The result of the ViT model was better than\nthat of DenseNet model in the same period, with an F1_score\nof 0.76. Tanzi et al. (2022) used the ViT architecture to clas-\nsify images of different fracture types and compared it with\nclassic CNN and multi-stage architecture composed of succes-\nsive CNNs. The results showed that the ViT model was able to\naccurately predict 83% of test images, with better performance\nthan the CNN model. Yao-Yu Lin et al. (2021) explored the use\nof ViT on GZ2 for galaxy morphology classification into eight\ncategories and the ViT model obtained the best overall accu-\nracy is 80.55%. Their results illustrate that the ViT models do\nnot outperform CNN over the entire sample, but they do find\nthat ViT reaches a higher classification accuracy in classifying\nsmaller and fainter galaxies. Karpoor (2022) present the results\nfor the classification of galaxies between spirals, ellipticals, and\nirregulars using a supervised deep learning model based on the\nViT architecture and they find that the ViT-based deep-learning\nA42, page 2 of 11\nCao, J., et al.: A&A, 683, A42 (2024)\n…\n…\n…\n…\nFlattened Patches\n…\n…\nFlattened Patches\nInput\nimage\nLinear \nProjection\nTransformer \nEncoder\nMLP \nHead Class\nEmbedded Patches\nNorm\nMulti-Head Attention\nNorm\nMLP\n+\n+\nTransformer  Encoder\nL×\nEmbedded Patches\nNorm\nMulti-Head Attention\nNorm\nMLP\n+\n+\nTransformer  Encoder\nL×\nPatch+Position Embedding\n…\n…\nClass Embedding\n* 0\n1\n13\n14\nPatch+Position Embedding\n…\n…\nClass Embedding\n* 0\n1\n13\n14\nFig. 1.Vision Transformer (ViT) structure.\nmodel performs a very well, offering an accurate classification\nof different types of galaxy morphologies.\nAfter the release of ViT types, many researchers made\nimprovements to the model. For example, Chu et al. (2023)\nproposed a conditional position encodings visual transformer\n(CPVT) structure that replaces predefined position embeddings\nin ViT with conditional position encodings (CPE), enabling\ntransformers to process images of any size without interpola-\ntion. Han et al. (2021) proposed the Transformer-iN-Transformer\n(TNT) model, which utilizes outer transformer blocks that han-\ndle patch embedding and inner transformer blocks that model\nthe relationship between pixel embeddings to model patch-level\nand pixel-level representations. Yuan et al. (2021) proposed the\nTokens-To-Token (T2T) model, which mainly improves the ViT\nmodel by connecting multiple tokens within a sliding window\ninto one token. Wang et al. (2021) proposed the Pyramid Vision\nTransformer (PVT), which adopts a multi-stage design (with-\nout convolutions) for the transformer, similar to multi-scales\nin CNNs, which is beneficial for dense prediction tasks. Wu\net al. (2021) proposed a Convolutional vision Transformer (CvT)\nmodel, which introduces convolution into the ViT model to\nimprove its performance. The CvT model achieved a top-1 accu-\nracy of 87.7% on the public dataset ImageNet-1k, surpassing the\nViT model’s accuracy of 76% on that dataset.\nThanks to the powerful advantages of the CvT model in\nimage classification tasks, this work is the first to apply the CvT\nmodel to galaxy image morphology classification. The organi-\nzational structure of this article is as follows. In Sect. 2, the\ntraditional ViT network structure is discussed, followed by a dis-\ncusion of CvT, which introduces CNNs into ViT. In Sect. 3, we\nintroduce the dataset used in this work and also augmente the\ndata for categories with fewer data samples. In the Sect. 4, we\npresent and analyze the classification results of the CvT model,\nand compared our work with other similar works. In addition, we\nalso visualize the classification results of the CvT model. Finally,\nwe summarize our findings in Sect. 5.\n2. Methods\n2.1. Vision Transformer (ViT)\nIn 2017, Google’s machine translation team in the field of\nsequence transduction, completely abandoned CNN and RNN\nmodels and relied solely on attention structures, which became\nknown as transformers (Vaswani et al. 2017). In 2021, inspired\nby transformers’ tremendous success in NLP, the Google team\ndeveloped a new image classification architecture and named\nVision Transformer (ViT; Dosovitskiy et al. 2021), with its basic\nstructure shown in Fig. 1. The input image of the model is\nfirst cut into fixed-size patches and then the flattened patches\nare linear projections (the dimension is transformed by matrix\nmultiplication). In order to preserve the position information of\neach patch, position encoding is added to each patch before it\nis sent to the Transformer encoder. The Transformer encoder\nconsists of L standard Transformer blocks, each of which is\ncomposed of layer normalization (LN), multi-head self-attention\n(MHSA), multi-layer perceptron (MLP), and residual connection\n(RC). Specifically, LN is a normalization technique that helps\nthe model training process to stabilize for better gradient prop-\nagation and model convergence. In particular, MHSA is one of\nthe core components of ViT, which allows the model to make\nglobal associations and interactions within the input sequence to\ncapture the dependencies between different features. Then, MLP\nis used to help models extract and transform features in the input\ndata to better understand and process complex patterns, while\nRC improves the information transfer and training effectiveness\nby directly connecting inputs and outputs.\nIt is worth noting that for ViT to achieve a level of per-\nformance that is comparable to that of the state-of-the-art\nconvolutional structures, it had to be pre-trained on large-\nscale datasets before being migrated to small and medium-sized\ndatasets.\n2.2. Convolutional vision Transformer (CvT)\nThe CvT model (Wu et al. 2021) is a new model architecture\nproposed based on the ViT model. The CvT model intro-\nduces convolutional neural networks into the ViT architecture to\nimprove performance and robustness, while maintaining a high\ndegree of computational and memory efficiency.\nThe overall structure of CvT is shown in Fig. 2. First, it\nintroduces convolution into two core parts of the ViT archi-\ntecture: a transformer hierarchy containing convolutional token\nembeddings, and a convolutional transformer block utilizing\nconvolutional projection. Among them: (i) the main function\nof convolutional token embedding is to simulate the downsam-\npling design of CNN and increase the width of the token while\nreducing the number of tokens; (ii) Convolutional Transformer\nBlocks use depth-wise separable convolutional operations called\nconvolutional projection, which is used for query, key, and\nvalue embeddings, rather than the standard position-wise linear\nprojection in ViT. For convolutional projection, 1D tokens are\nA42, page 3 of 11\nCao, J., et al.: A&A, 683, A42 (2024)\nFlatten 1-dimensional tokens sequence\nConvolutional Token Embedding\nConvolutional Transformer BlockMLP\nHead\nInput \nimages Class\n×3\n…\n…\n…\n…\nreshape 2-dimensional padreshape 2-dimensional pad\nFig. 2.Convolutional vision Transformer (CvT) structure.\nfirst reshaped to 2D features. Then, the convolution is used to\nextract token features for each position separately. Finally, tokens\nare flattened to get 1D features, that is, Q, K, and V. At last,\nadded the classification token to the sequence, then MLP Head\nis used to predict the results on the classification token output.\nMore detailed descriptions of the principles of the CvT model\ncan be found in Wu et al. (2021).\nBy adjusting different parameters such as the number of\nTransformer blocks and the feature dimensions in the CvT\nmodel, several different versions of the CVT model have come\nabout. In these different versions of the CvT model, CvT-13, and\nCvT-21 as the basic models, and CvT-X represents a Convolu-\ntional Vision Transformer with a total of X Transformer blocks.\nIn addition, there are models with larger token dimensions and\nwider widths, namely, CvT-W24 (where W represents width).\nIn this study, we also compare the classification performance of\ndifferent versions of the CvT model.\n3. Data\n3.1. Galaxy Zoo sample\nThe Galaxy Zoo project invited a large number of volunteers to\nclassify galaxies based on the morphology of the given color\nimages (Lintott et al. 2008). This paper uses data and classi-\nfication standards provided by GZ2 (Willett et al. 2013). This\ndataset is from the Galaxy Zoo-the Galaxy Challenge compe-\ntition held by the GZ on the Kaggle platform. The training set\nfor this competition includes 61 578 labeled color RGB images\nof galaxy observations from SDSS DR7 (Abazajian et al. 2009),\nwith a size of 424 × 424 × 3.\nThe observation of galaxies by SDSS includes 5 optical\nbands (u, g, r, i, and z), and in related research work, data from\nthe first u, g, and r bands are usually taken to synthesize cor-\nresponding RGB galaxy images. The vector labeled 1 × 37 for\neach image is derived from the corrected cumulative frequency\nvalue of GZ2 volunteer voting scores. GZ2 divides galaxy mor-\nphology into 11 questions and 37 answers. With reference to\nthe current research, we selected five types of galaxy data:\ncompletely round smooth galaxies, in-between smooth galax-\nies, cigar-shaped smooth galaxies, edge-on galaxies, and spiral\ngalaxies. We then applied the CvT model to classify and study\nthem.\nThe GZ2 data release has a clear threshold selection rule for\nwell-sampled galaxies, which means that for the same galaxy\nimage, the number of volunteers to classify it must be greater\nthan 20, and the calculated cumulative voting score correc-\ntion value must meet a certain threshold before the image can\nbe classified into a certain galaxy category. For example, an\nimage must meet three threshold conditions (ffeatures/disk ≥ 0.430,\nfedge−on,no ≥ 0.715, fspiral,yes ≥ 0.619) to be classified as a spiral\ngalaxy. Due to the strict threshold selection rules for well-\nsampled galaxies in GZ2, the number of data samples that can be\nselected for smooth galaxies (completely round smooth galaxies,\nin-between smooth galaxies and cigar-shaped smooth galaxies)\nin the above five categories is slightly insufficient. In order to\nobtain a sufficient number of samples for model training and\ntesting, this paper appropriately widens the threshold selection\nstandard for smooth galaxies from 0.8 to 0.5, while the threshold\nselection rules for edge-on galaxies and spiral galaxies still refer\nto the default values in the GZ2 data release white papers. The\nwell-sampled galaxies obtained based on the above rules include\na total of 28 790 galaxy images, this data set is consistent with\nanother work (Li et al. 2023). Figure 3 shows five types of galaxy\nimage data randomly selected from the well-sampled original\ndataset.\nThe work then divides the 28 790 well-sampled galaxies into\na training and a test set in a ratio of 9:1. Table 1 shows the number\nof images for each type of galaxy in each training and test set.\nIt can be seen that the number of images for the five types of\ngalaxies matches the same distribution and proportion.\nIn addition, to maintain a balance between the number of\nsample data in each category, this study has enhanced the train-\ning set data by rotating a relatively small number of cigar-shaped\ngalaxy data. We rotated the image data of cigar-shaped galax-\nies by 45◦, 90◦, 120◦, and 180◦, respectively. The rotated galaxy\nimages are shown in Fig. 4.\nIt should be noted that when the rotation angle is not 90◦\nthe corners of the image will produce empty pixels that will\naffect the final classification. Therefore, to eliminate the effect\nof empty pixels we used nearest-neighbour interpolation to fill\nthese empty pixels. The basic steps of the nearest-neighbor inter-\npolation are as follows: (i) Determine the location of the empty\nA42, page 4 of 11\nCao, J., et al.: A&A, 683, A42 (2024)\nTable 1.Sample statistics of galaxy images in the training and validation sets.\nClass Sample of Galaxy Train Val Dataset Proportion (%)\n0 In-between smooth 7262 807 8069 28\n1 Completely round smooth 7591 843 8434 29\n2 Edge-on 3513 390 3903 14\n3 Spiral 7025 781 7806 27\n4 Cigar-shaped smooth 520 58 578 2\nAmount 25 911 2879 28 790 100\nIn-between smooth\nCompletely round smooth\nEdge-on\nSpiral\nCigar-shape smooth\nFig. 3.Images of a randomly selected galaxy from GZ2.\npixel, namely, the pixel location in the corner. (ii) Calculate the\ntransformed position of the empty pixel location with respect to\nthe original image. (iii) Find the nearest original pixel position\nto the transformed position. (iv) Assign the value of the nearest\noriginal pixel to the empty pixel position.\nThanks to this method, pixels can be filled at the corners to\nmake the image appear more complete.\n3.2. High-redshift sample\nTo verify the model’s classification performance for different\nredshift samples, we tried to use an additional high redshift\ndataset for the purposes of this study.\nRedshift is an astronomical phenomenon caused by the\nexpansion of the Universe, which results in a change in the wave-\nlength of light emitted by celestial objects. As the object moves\naway from the observer, its light is shifted towards the red end\nof the spectrum. Different redshift values correspond to differ-\nent cosmic epochs, and by categorizing images of galaxies in\ndifferent redshift ranges, astronomers can study the evolution of\nthe Universe at different times. For example, by comparing early\nand late galaxies, we can understand the changes in galaxy for-\nmation and evolution throughout the Universe. At the same time,\nthe nature and morphology of galaxies change as the universe\nevolves. Studying the results of the same classification model\nat different redshifts helps us to understand the evolutionary\ntrend of galaxies and the characteristics of different evolutionary\nstages. In addition, redshift is related to the distance of galaxies.\nBy studying how the same classification model classifies galaxy\nimages at different redshifts, we can determine the redshift val-\nues of galaxies and infer the distance distribution of galaxies in\nthe universe.\nIn our study, we utilized the GZ2 dataset, which encom-\npasses a low-redshift range of 0.0005 ≤ z ≤ 0.25. For a more\ncomprehensive analysis, we extended our experiments to incor-\nporate the Galaxy Zoo CANDELS dataset, cited from Simmons\net al. (2016), which surveys high-redshift galaxies within the\nrange of 1 ≤ z ≤ 3. The CANDELS dataset, a key subset of the\nGZ project, encompasses approximately 800 square arcminutes\nacross five significant astronomical fields, providing the most\nprofound infrared imaging available to date.\nHocking et al. (2017) employed a robust unsupervised\nmachine learning algorithm combining growing neural gas\n(GNG), hierarchical clustering (HC), and connected component\nlabeling. This approach facilitated the identification of similar\ngalactic structures within the CANDELS dataset, leading to the\ncategorization of roughly 60 000 galaxies into 200 distinct clus-\nters. Each galaxy’s unique vector representation, coupled with\nthe Pearson correlation coefficient, enabled the precise match-\ning of galaxies within the dataset based on their similarities. This\nmethodology adeptly segregated galaxies into classifications of\n“early” and “late” types, delineated by the reddish hues of older\ngalaxies and the bluish tones of younger ones. To test the effi-\ncacy of the CvT model in classifying high-redshift imagery, we\nselected five groups from the CANDELS catalog, constructed\nby Hocking et al. (2017), which exhibits a high Pearson similar-\nity and encompasses diverse morphological characteristics and\ndevelopmental stages. The selected galaxy images are shown in\nFig. 5.\nAmong the five selected groups, there is Group 6, which is\ncharacterized by elliptical galaxies devoid of conspicuous spi-\nral arms, exhibiting a radial gradient in luminosity that fades\nfrom a central yellow to a pale yellow-white, indicative of galac-\ntic antiquity. Group 8 comprises spiral galaxies with luminous\ncores and dimmer spiral arms, rendered in shades of blue, sig-\nnaling the presence of younger galactic formations. Group 11\nincludes spiral galaxies as well, with pronounced arms and\nluminous centers, yet their reddish palette points to the older,\nearly-type galactic forms. Group 16 consists of irregular galax-\nies, whose blue coloring denotes active star formation typically\nassociated with late-type galaxies. Group 45 also presents irreg-\nular galaxies, but their reddish tint suggests classification as\nearly-type galaxies. The catalog used for this experiment can be\nsearched online1.\n4. Galaxy morphology classification results based\non the CvT-13 model\n4.1. Computer set-up\nThis work is based on a server with a V100-SXM2-32GB GPU\nand a 12 vCPU Intel (R) Xeon (R) Platinum 8255C CPU. The\n1 www.galaxyml.uk\nA42, page 5 of 11\nCao, J., et al.: A&A, 683, A42 (2024)\nOriginal r45 r90 r120 r180Original r45 r90 r120 r180\nFig. 4.Images of a cigar-shaped galaxy after rotation.\nTable 2.Classification performance in five classes of galaxies based on\nthe CvT-13 model.\nClass Accuracy Precision Recall F1_Score\n0 0.998 0.996 0.991 0.990\n1 0.999 0.997 0.990 0.992\n2 0.994 0.991 0.989 0.991\n3 0.999 0.991 0.990 0.987\n4 0.948 0.931 0.942 0.938\nAverage 0.988 0.981 0.980 0.980\nGROUP 6\nGROUP 8\nGROUP 11\nGROUP 16\nGROUP 45\nFig. 5. Images of five types of galaxies selected in the Galaxy Zoo\nCANDELS dataset.\ncompiler is Pycharm-professional version 2021.1 and CUDA ver-\nsion 11.3. In addition, this work was implemented using Python\nlanguage based on the Pytorch 1.11.0 framework (Paszke et al.\n2019), utilizing Python libraries such as sklearn (Pedregosa et al.\n2011), Scikit image (van der Walt et al. 2014), and transforms\n(Wolf et al. 2019).\n4.2. Result analysis\nTo verify the classification performance of the CvT model, we\nbased this work on the basic CvT-13 model and using evalua-\ntion indicators, such as accuracy, precision, recall, and F1_score\nto measure the classification performance of the model. Table 2\nshows the best classification results of CvT among various galax-\nies. We can see that (except for cigar-shaped smooth galaxies)\nthe classification accuracy of all types is above 98%, while the\nprecision, recall, and F1_score are also above 99% except for\ncigar-shaped smooth galaxies. This is mainly due to the limited\ndata volume of cigar-shaped smooth galaxies. At the same time,\nunder the average classification of five types of galaxies, the\naccuracy is 98.8%, the precision is 98.1%, the recall is 98.0%,\nand the F1_score is 98.0%, which verifies the robustness of the\nCvT model for morphological galaxy classification.\nSubsequently, we analyzed the problem of poor classification\nresults for cigar-shaped smooth galaxies in detail. In addition\nto the limited amount of image data for this class of galaxies,\nother potential factors that may make this class of galaxies dif-\nficult to classify have also been considered. Firstly, cigar-shaped\nsmooth galaxies can make it relatively difficult for automatic\nclassification algorithms to accurately identify and classify them\ndue to their elongated shape or special structural properties;\nspecifically, the morphological features of cigar-shaped smooth\ngalaxies can affect their performance in classification tasks. Due\nto the smooth appearance of cigar-shaped smooth galaxies and\nthe absence of prominent features such as spiral arms, they may\nshow higher similarity to other morphological types (e.g., spiral\ngalaxies, edge-on galaxies), which can lead to lower classifica-\ntion accuracy. This is particularly true in the case of unbalanced\ndatasets or poor feature selection, where machine learning auto-\nmatic classifiers may misclassify spheroidal galaxies as other\ntypes or have other difficulties in accurately distinguishing them\nfrom other morphologies. Another reason to consider is the pos-\nsible contamination caused by GZ volunteers. GZ is a citizen\nscience project which allows volunteers to help classify galax-\nies. While the project produces valuable data, there is always\nthe potential for human error or subjectivity in the classifi-\ncation process. In some cases, misclassifications or category\ninconsistencies contributed by volunteers may affect the over-\nall classification results. Given that the GZ2 dataset establishes\nclear classification threshold criteria for each galaxy image, its\ncumulatively voted positive assessments must exceed a specific\nthreshold to be formally categorized as a galaxy. Hence, to effec-\ntively address the issue of inconsistent volunteer categorization,\nwe can resolve it by implementing a more stringent threshold\nselection criterion. The precise threshold criteria employed in\nthis study are elaborated in detail in Sect. 3.1.\nFurthermore, in this work, we also compare different ver-\nsions of the CvT model. The results of the comparison are shown\nin Table 3. The various classification performance metrics of\nthe different versions of the CvT model for five-class classifi-\ncations of the GZ2 dataset are not very different, but those of\nCvT-21 and CvT-W24 are much higher than CvT-13 for Params\nand FLOPs, which evaluate the computational resources and the\nnumber of parameters. Therefore, we used the CvT-13 model for\nother comparisons in our subsequent experiments.\nIn Fig. 6, we use ROC curves to calculate AUC values to\nevaluate the model performance. The outcome indicates that the\nA42, page 6 of 11\nCao, J., et al.: A&A, 683, A42 (2024)\nTable 3.Classification performance of different versions of CvT models.\nModel Accuracy Precision Recall F1_Score Params(M) FLOPs(G)\nCvT-13 0.988 0.981 0.980 0.980 20 4.5\nCvT-21 0.989 0.990 0.990 0.988 32 7.1\nCvT-W24 0.992 0.989 0.992 0.990 278 60.9\n0.0 0.1 0.2 0.3 0.4 0.5\nFalse Positive Rate\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nTrue Positive Rate\nReceiver operating characteristic\nROC curve of class 0 (area = 0.991)\nROC curve of class 1 (area = 0.996)\nROC curve of class 2 (area = 0.996)\nROC curve of class 3 (area = 0.996)\nROC curve of class 4 (area = 0.977)\nFig. 6.ROC curves of CvT-13 in the classification of galaxies of differ-\nent morphologies.\nmodel exhibits a good classification result for each category of\ngalaxies. Except for cigar-shaped smooth galaxies with a small\nnumber of data samples, which have an AUC value of 0.977, the\nAUC value of each category of galaxies is above 0.99.\nFigure 7 presents the confusion matrix of the testing dataset\non the CvT-13 model, which uses indicators of 0 to 4 represent\ngalaxy classes, corresponding to Table 1. In the figure, the num-\nber of completely round smooth galaxies and in-between smooth\nand spiral galaxies is 827, 791, and 781, respectively, and when\nthese three types of galaxies were misclassified, they were all\nmisclassified with respect to each other – and not misclassified\nas cigar-shaped smooth and edge-on galaxies. For cigar-shaped\nsmooth galaxies and edge-on galaxies, the correct number of\nclassifications is 51 and 389, respectively. Among them, 2 and\n4 cigar-shaped smooth galaxies are misclassified as in-between\nsmooth and edge-on galaxies, while 1 and 4 edge-on galaxies\nare misclassified as in-between smooth and cigar-shaped smooth\ngalaxies, respectively. This is mainly because completely round\nsmooth galaxies (in-between smooth galaxies and spiral galax-\nies) do have certain similarities in terms of their morphology. In\naddition, for cigar-shaped smooth galaxies and edge-on galaxies,\nthe classification results may be poor due to the relatively small\namount of data for these two types of galaxies and the failure to\nadequately learn their morphological features during the model\ntraining.\nIn addition, the generalization capability of the model is\nverified in this work for low signal-to-noise ratio (S/N) galaxy\nimage data. Table 4 shows the specifics of adding Gaussian\nnoise and salt-and-pepper noise to the original galaxy images.\n0 1 2 3 4\nPredicted Label\n0\n1\n2\n3\n4 True Label\n827 3 0 5 0\n8 719 0 6 0\n1 0 389 0 4\n1 7 0 781 0\n2 0 4 0 51\nConfusion Matrix\n0\n100\n200\n300\n400\n500\n600\n700\n800\nFig. 7.Confusion matrix of CvT-13 in the classification of galaxies of\ndifferent morphologies.\nBy adjusting the size of the Gaussian distribution standard devi-\nation (sigma) to control the level of Gaussian noise addition, the\nsigma values of 5, 15, and 25 were set in this work; salt-and-\npepper noise refers to adding black and white noisy points to an\nimage and controlling the proportion of noise added by setting\nthe amount. In the experiment, the amount s were set to 0.05,\n0.1, and 0.2, respectively. The larger the values of Sigma and\namount, the more noise added, and the more severe the image\ndamage. The galaxy image with added noise is shown in Fig. 8,\nand the classification results based on the CvT model in the\ngalaxy morphology with added noise are shown in Table 4. From\nthe perspective of classification performance, the overall classi-\nfication performance of galaxy images has decreased compared\nto those without noise. However, the overall classification accu-\nracy with the addition of noise is stable at over 80%, indicating\nthat the CvT-13 model comparison with EfficientNet-B5 model\n(Ai et al. 2022) has stable classification performance for low\nS/N galaxy images and has relatively good generalization ability.\nHere, we chose to compare our results with the EfficientNet-B5\nmodel primarily because this was the only study that featured\nsimilar noise experiments and provided these results in their\nwork. It should be noted that this work by EfficientNet-B5 only\nadds different degrees of Gaussian noise to the images, so we\nonly based our comparison on the Gaussian noise correlation\nresults.\nIn astronomical observations, images with different S/Ns are\nusually obtained due to the differences in equipment and obser-\nvation conditions, and so on. Therefore, it is very important\nin astronomical research that automatic classification models\nfor image data with different S/N can achieve better classifi-\ncation performance. Firstly, astronomical images usually have\nvarying degrees of noise and background interference. There-\nfore the model’s ability to classify images with different S/Ns\nallows astronomers to obtain reliable and consistent results\nA42, page 7 of 11\nCao, J., et al.: A&A, 683, A42 (2024)\nTable 4.Comparison of classification results under different levels of Gaussian noise and salt-and-pepper noise by different models.\nReference Model Add noise type Add noise level Accuracy\nThis work CvT-13\nGaussian noise\nsigma=0 0.988\nsigma=5 0.931\nsigma=25 0.864\nsigma=50 0.837\nSalt-and-pepper noise\namount=0 0.988\namount=0.05 0.902\namount=0.1 0.831\namount=0.2 0.815\n[1] EfficientNet-B5 Gaussian noise\nsigma=0 0.968\nsigma=5 0.783\nsigma=20 0.847\nsigma=50 0.815\nReferences. [1] Ai et al. (2022).\noriginal\n sigma=5\nsigma=25\n sigma=50\noriginal\n amount=0.05\namount=0.1\n amount=0.2\n(a)Gaussian Noise (b)Salt-and-pepper Noise\nFig. 8.Adding images of galaxies with different types and levels of noise.\nacross observations. It ensures that classification algorithms can\naccurately identify and distinguish astronomical objects or phe-\nnomena even in the presence of noise. Secondly, in the field of\nastronomical research, researchers are constantly pursuing the\ndetection of faint or distant objects, such as distant galaxies,\nfaint stars, or exoplanets. These objects may have low signal lev-\nels compared to the background noise. Through the ability to\nclassify low S/N images, astronomers can increase the chances\nof detecting and identifying these elusive objects, which may\nhold important scientific insights. Astronomers can then quantify\nthe uncertainty associated with the classification by classifying\nimages with different S/Ns. Also, by understanding the effect of\nnoise on the classification process, they can assign probabilities\nor confidence levels to their classification tasks, thus provid-\ning a more complete and accurate representation of the data.\nFinally, astronomers are better able to study transient or variable\nobjects. Observations of transient objects such as supernovae,\nstellar flares, or gamma-ray bursts, whose brightness or spec-\ntral properties change rapidly, often involve images with different\nS/Ns due to the temporal conditions or intrinsic properties of the\nobject. The ability to classify images with different S/Ns allows\nastronomers to follow and study the temporal evolution and\nbehavior of these transient or variable objects. In summary, the\nability to classify astronomical images with different S/Ns is cru-\ncial in astronomical observations. It ensures the reliability and\nrobustness of classification results, aids in the detection of faint\nor distant objects, allows for the quantification of uncertainties,\nand facilitates the study of transient or variable phenomena.\nAdditionally, we have conducted experiments with images\nof galaxies at different resolutions based on the CvT-13 model.\nAstronomical observations often require capturing images of\nlarge areas of the sky at different resolutions. Classifying images\nof different resolutions helps to optimize observing strategies\nand data analysis. By understanding the characteristics and lim-\nitations of different resolutions, astronomers can design sky sur-\nveys that take into account the coverage of large-scale structures\nand the need for detailed information on specific regions or spe-\ncific objects. On the other hand, different telescopes, instruments\nor observing techniques may produce images with different res-\nolutions. Astronomers must have the ability to classify images\nA42, page 8 of 11\nCao, J., et al.: A&A, 683, A42 (2024)\noriginal DPI=10 DPI=25 DPI=50\nFig. 9.Images of galaxies at different resolutions.\nTable 5.Comparison of classification results at different resolutions.\nModel DPI value Accuracy\nCvT-13\n10 0.755\n25 0.858\n50 0.911\n100(original) 0.998\nacquired by different instruments or observations to ensure con-\nsistency and comparability between different data sets. With\nsuch a capability, data from different sources can be integrated\nand analyzed to enhance the scientific understanding of celes-\ntial bodies and celestial processes. In this work, we set different\nresolutions of the galaxy images by adjusting the dots per inch\n(DPI) parameter, which is a unit of measurement for image res-\nolution that describes the number of pixels per inch in an image.\nThe larger the DPI value, the higher the resolution of the image,\nand the clearer the image. The galaxy images after adjusting\nthe resolution are shown in Fig. 9, and the average classifica-\ntion accuracies of galaxy images with different DPI values are\nshown in Table 5.\nFurthermore, we used the CvT-13 model to classify images\nof galaxies at high redshifts. We used the datasets Galaxy zoo\nCANDELS introduced in Sect. 3.2, based on the high-redshift\ndatasets we investigated the classification of galaxy images with\nhigh-redshift ranges using the CvT-13 model. The experimental\nresults are shown in Table 6, where we can see that the CvT-\n13 model also has a good classification effect on high-redshift\ngalaxy images.\n4.3. Performance comparison and analysis of different\nmodels\nIn this section, we compare the classification performance\nof the CvT-13 model with other works, including ResNet-26\n(Zhu et al. 2019), NODE (Gupta et al. 2022), EfficientNet-\nB5 (Ai et al. 2022), MSCCN (Li et al. 2023), and ViT B/16\n(Dosovitskiy et al. 2021). The data sources used in these works\nare the same and the division ratio between the training dataset\nand the testing dataset is consistent. The specific comparison\nof classification results is shown in Table 7. From the com-\nparison results, the accuracy obtained with the CvT-13 model\nused in this work exceeds that of the classification models used\nin other works, indicating that the CvT model is very effec-\ntive in classifying galaxy morphology. Meanwhile, as can be\nseen from the comparison results, the MSCCN model (Li et al.\n2023) also shows impressive efficacy in the galaxy morphology\nclassification task, especially on the GZ2 dataset. However, the\nCvT model we have adopted stands out with its cutting-edge\ntransformer-based technology. This model not only excels in the\nsame classification tasks, showcasing exceptional performance\nbut also brings to the table unique and advanced features. These\ndistinguishing characteristics of the CvT model not only comple-\nment its high-level performance but also highlight its significant\nadvantages and potential for broader applications in the field of\ngalaxy morphology classification.\nIn addition, we have also compared ViT B/16 and CvT-13,\nwhich are the two models based on the transformer structure,\nand these two models are the most basic versions of all of them.\nAs shown in Table 8, the results of the comparison experiments\nshow that the CvT-13 model has a large improvement over the\nViT B/16 model in two aspects: computational resources and\nmemory use. Meanwhile, the basic structure of the ViT has been\nused in related works on eight and three classifications of galaxy\nmorphology (Yao-Yu Lin et al. 2021; Karpoor 2022), their results\nare superior to traditional CNN-based classification networks. In\nthe subsequent work, we will use the CVT model to classify\ngalaxy images at levels 3 and 8, comparing them with the ViT\nmodel.\n4.4. Visualization of classification results\nTo explore the information on galaxy morphology features from\nthe classification results, we visualized the classification results\nof the testing dataset. In this part of the study, we used the t-SNE\nalgorithm to visualize and analyze the classification results of\nthe CvT model. The t-SNE algorithm is a nonlinear dimension-\nality reduction algorithm used for multidimensional data scaling\n(Devassy & George 2020), which can preserve the local structure\nof data samples and obtain low-dimensional data that is more\nsimilar to the original high-dimensional data. Due to its sig-\nnificant performance in scaling high-dimensional data to lower\ndimensional data, it is widely used in machine learning. The t-\nSNE algorithm converts the similarity between data points into\nprobability and the similarity in the original high-dimensional\nspace is represented by a Gaussian distribution. The probabil-\nity of embedding space is represented by a T-distribution, which\nprojects data from high-dimensional space to low-dimensional\nspace and visualizes it.\nFigure 10 shows a visualization of the results of galaxy mor-\nphology classification using the CvT model, from which it can\nbe seen that clusters of various galaxies have clear and dis-\ntinct boundaries, indicating that the CvT model performs well in\nclassifying galaxy morphology. The boundary between smooth\ngalaxies and completely round smooth galaxies has a very small\noverlap because these two types of galaxies are smooth galaxies\nA42, page 9 of 11\nCao, J., et al.: A&A, 683, A42 (2024)\nTable 6.Classification performance of five classes of galaxies based on the CvT-13 model under the Galaxy Zoo CANDELS dataset.\nClass Accuracy Precision Recall F1_score\n0 0.986 0.979 0.984 0.981\n1 0.987 0.975 0.981 0.986\n2 0.990 0.979 0.988 0.983\n3 0.979 0.981 0.975 0.976\n4 0.978 0.976 0.972 0.975\nAverage 0.984 0.978 0.980 0.980\nTable 7.Comparison results of classification evaluation indexes of five models.\nReference Model Accuracy Precision Recall F1_Score\n[1] ResNet-26 0.952 0.951 0.952 0.952\n[2] ViT B/16 0.946 0.941 0.942 0.941\n[3] EfficientNet-B5 0.968 0.967 0.968 0.967\n[4] NODE 0.917 0.916 0.934 0.926\n[5] MSCCN 0.970 0.961 0.982 0.964\nThis work CvT-13 0.988 0.981 0.980 0.980\nNotes. Among them: ResNet-26 is a modified deep residual network model, ViT B/16 is a basic version of the vision transformer network structure,\nEfficientNet-B5 is a CNN-based model, NODE is a continuous depth version of the residual network, and MSCCN is an improved capsule network\nusing multiscale convolution.\nReferences. [1] Zhu et al. (2019); [2] Dosovitskiy et al. (2021); [3] Ai et al. (2022); [4] Gupta et al. (2022); [5] Li et al. (2023).\nTable 8.Comparison of computational and memory use between CvT-\n13 and ViT B/16.\nModel Params(M) FLOPs(G)\nViT B/16 86.4 55.5\nCvT-13 20 4.5\nwith very similar shapes, which leads to some images of these\ntwo types of galaxies being misclassified. In addition, cigar-\nshaped smooth galaxies have been partially misclassified as\nspiral galaxies due to limited data samples.\n5. Summary and prospects\nWith the continuous improvement of the depth and detection\nefficiency of large-scale sky surveys, the accumulated mas-\nsive astronomical data poses new challenges to their processing\nmethods. Given the widespread application of deep learning in\nastronomical data and the tremendous success of the transformer\nmethod in the NLP field, this work applies the CvT model to the\nclassification of galaxy morphology. Our study novelty resides\nin the use, for the first time, of CvTs to the classification of\ngalaxy images and the performance of the model is also validated\nacross various aspects. We also compared a number of different\nclassification methods and demonstrated that the CvT model’s\nperformance for a quintuple classification of galaxy morphology\nis by far the best.\nAmong them, transformer-based classification models have\nachieved high accuracy in galaxy morphology classification. In\nCvT, the overall highest accuracy reached 98.8%, average preci-\nsion reached 98.1%, average recall reached 98.0%, and average\nFig. 10.Visualization of CvT classification results.\nF1_score reached 98.0%. This has shown a significant improve-\nment compared to CNN-based classification models, proving\nthat transformer-based classification models can be applied to\ngalaxy morphology classification. Meanwhile, the classification\naccuracy of CvT for low-S/N galaxy images is above 80%, indi-\ncating that the model also has good generalization ability for\nlow-S/N galaxy images. In addition, the t-SNE algorithm is also\nused in this work to visualize the classification results of the\nA42, page 10 of 11\nCao, J., et al.: A&A, 683, A42 (2024)\nmodel, which can more intuitively demonstrate the effectiveness\nof the CvT model in classifying galaxy morphology.\nThe CvT model used in this article provides more possibili-\nties for subsequent data analysis of more large-scale sky survey\ntelescopes in the future, such as the China Space Station Tele-\nscope (CSST) and the Large Synthetic Survey Telescope (LSST),\nwhich China plans to launch in 2024. In future works, we plan\nto use CvT to conduct galaxy image classification research that\nextends beyond the morphology presented in this paper. We\nwill investigate the impact of the network structure in CvT on\nits morphology classification performance and further verify its\neffectiveness in galaxy morphology classification.\nAcknowledgements. We thank the anonymous referee for valuable and help-\nful comments and suggestions. This work is supported by the National Nature\nScience Foundation of China (61561053), the Scientific Research Foundation\nProject of Yunnan Education Department (2023J0624), the Yunnan Fundamental\nResearch Projects (grant no. 202301A V070007), and the “Yunnan Revitalization\nTalent Support Program” Innovation Team Project.\nReferences\nAbazajian, K. N., Adelman-McCarthy, J. K., Agüeros, M. A., et al. 2009, ApJS,\n182, 543\nAi, L. P., Xu, Q. F., Du, L. T., et al. 2022, Acta Astron. Sin., 63, 42\nCheng, T.-Y., Conselice, C. J., Aragón-Salamanca, A., et al. 2020, MNRAS, 493,\n4209\nChu, X., Tian, Z., Zhang, B., Wang, X., & Shen, C. 2023, in The Eleventh\nInternational Conference on Learning Representations\nColless, M., Dalton, G., Maddox, S., et al. 2001, MNRAS, 328, 1039\nCui, X.-Q., Zhao, Y.-H., Chu, Y.-Q., et al. 2012, Res. Astron. Astrophys., 12,\n1197\nDevassy, B. M., & George, S. 2020, Forensic Sci. Int., 311, 110194\nDomínguez Sánchez, H., Huertas-Company, M., Bernardi, M., Tuccillo, D., &\nFischer, J. L. 2018, MNRAS, 476, 3661\nDosovitskiy, A., Beyer, L., Kolesnikov, A., et al. 2021, in International Confer-\nence on Learning Representations\nGao, X., Qian, Y., & Gao, A. 2021, ArXiv e-prints [arXiv:2107.01682]\nGardner, J. P., Mather, J. C., Abbott, R., et al. 2023, PASP, 135, 068001\nGheflati, B., & Rivaz, H. 2022, in 2022 44th Annual International Conference of\nthe IEEE Engineering in Medicine & Biology Society (EMBC), 480\nGupta, R., Srijith, P. K., & Desai, S. 2022, Astron. Comput., 38, 100543\nHan, K., Xiao, A., Wu, E., et al. 2021, in Advances in Neural Information Pro-\ncessing Systems, 34, eds. M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang,\n& J. W. Vaughan (Curran Associates, Inc.), 15908\nHe, Y., Zhang, Y., Chen, S., & Hu, Y. 2023, in 2023 IEEE 6th Information Tech-\nnology,Networking,Electronic and Automation Control Conference (ITNEC),\n6, 1648\nHocking, A., Geach, J. E., Sun, Y., & Davey, N. 2017, MNRAS, 473, 1108\nHui, W., Robert Jia, Z., Li, H., & Wang, Z. 2022, in J. Phys. Conf. Ser., 2402,\n012009\nKarpoor, P. 2022, in Am. Astron. Soc. Meeting Abstracts, 54, 201.13\nLi, G., Xu, T., Li, L., et al. 2023, MNRAS, 523, 488\nLintott, C. J., Schawinski, K., Slosar, A., et al. 2008, MNRAS, 389, 1179\nPaszke, A., Gross, S., Massa, F., et al. 2019, Advances in Neural Information\nProcessing Systems, eds. H. Wallach, H. Larochelle, A. Beygelzimer, et al.\n(Curran Associates, Inc.), 32\nPedregosa, F., Varoquaux, G., Gramfort, A., et al. 2011, J. Mach. Learn. Res., 12,\n2825\nSimmons, B. D., Lintott, C., Willett, K. W., et al. 2016, MNRAS, 464, 4420\nTanzi, L., Audisio, A., Cirrincione, G., Aprato, A., & Vezzetti, E. 2022, Injury,\n53, 2625\nvan der Walt, S., Schönberger, J. L., Nunez-Iglesias, J., et al. 2014, PeerJ, 2, e453\nVaswani, A., Shazeer, N., Parmar, N., et al. 2017, in Advances in Neural\nInformation Processing Systems (Curran Associates, Inc.)\nWalmsley, M., Smith, L., Lintott, C., et al. 2020, MNRAS, 491, 1554\nWang, W., Xie, E., Li, X., et al. 2021, in 2021 IEEE/CVF International Confer-\nence on Computer Vision (ICCV), 548\nWei, S., Li, Y., Lu, W., et al. 2022, PASP, 134, 114508\nWen, Z. Z., Zheng, X. Z., & An, F. X. 2014, ApJ, 787, 130\nWillett, K. W., Lintott, C. J., Bamford, S. P., et al. 2013, MNRAS, 435, 2835\nWolf, T., Debut, L., Sanh, V., et al. 2019, ArXiv e-prints [arXiv:1910.03771]\nWu, H., Xiao, B., Codella, N., et al. 2021, 2021 IEEE/CVF International\nConference on Computer Vision (ICCV), 22\nYao-Yu Lin, J., Liao, S.-M., Huang, H.-J., Kuo, W.-T., & Hsuan-Min Ou, O. 2021,\nArXiv e-prints [arXiv:2110.01024]\nYork, D. G., Adelman, J., Anderson, John E., J., et al. 2000, AJ, 120, 1579\nYuan, L., Chen, Y., Wang, T., et al. 2021, Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV), 558\nZhu, X.-P., Dai, J.-M., Bian, C.-J., et al. 2019, Ap&SS, 364, 55\nA42, page 11 of 11",
  "topic": "Convolutional neural network",
  "concepts": [
    {
      "name": "Convolutional neural network",
      "score": 0.628998875617981
    },
    {
      "name": "Physics",
      "score": 0.5964117646217346
    },
    {
      "name": "Galaxy",
      "score": 0.5635688900947571
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5391258597373962
    },
    {
      "name": "Astrophysics",
      "score": 0.44376614689826965
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4162459671497345
    },
    {
      "name": "Computer science",
      "score": 0.3079332709312439
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I145897649",
      "name": "Minzu University of China",
      "country": "CN"
    }
  ]
}