{
    "title": "Group-Free 3D Object Detection via Transformers",
    "url": "https://openalex.org/W3143166043",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A5102808150",
            "name": "Ze Liu",
            "affiliations": [
                "University of Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A5100459168",
            "name": "Zheng Zhang",
            "affiliations": [
                "Microsoft Research (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A5101593662",
            "name": "Yue Cao",
            "affiliations": [
                "Microsoft Research (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A5101976576",
            "name": "Han Hu",
            "affiliations": [
                "Microsoft Research (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A5100784734",
            "name": "Xin Tong",
            "affiliations": [
                "Microsoft Research (United Kingdom)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2603429625",
        "https://openalex.org/W6637108133",
        "https://openalex.org/W2963053547",
        "https://openalex.org/W3015558290",
        "https://openalex.org/W6739778489",
        "https://openalex.org/W2229637417",
        "https://openalex.org/W1923184257",
        "https://openalex.org/W2949708697",
        "https://openalex.org/W3034314779",
        "https://openalex.org/W6763422710",
        "https://openalex.org/W2964062501",
        "https://openalex.org/W6703933998",
        "https://openalex.org/W2963351448",
        "https://openalex.org/W2211722331",
        "https://openalex.org/W2988715931",
        "https://openalex.org/W3034429258",
        "https://openalex.org/W2798965597",
        "https://openalex.org/W2963727135",
        "https://openalex.org/W6679792166",
        "https://openalex.org/W6817046665",
        "https://openalex.org/W6749845755",
        "https://openalex.org/W2963312728",
        "https://openalex.org/W2520005737",
        "https://openalex.org/W6771790678",
        "https://openalex.org/W2964080601",
        "https://openalex.org/W2983446232",
        "https://openalex.org/W2264432461",
        "https://openalex.org/W2963400571",
        "https://openalex.org/W6754918364",
        "https://openalex.org/W2555618208",
        "https://openalex.org/W3034579518",
        "https://openalex.org/W2594519801",
        "https://openalex.org/W3034430142",
        "https://openalex.org/W6750189863",
        "https://openalex.org/W2799162093",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W6754997583",
        "https://openalex.org/W2798462325",
        "https://openalex.org/W6640300118",
        "https://openalex.org/W2990613095",
        "https://openalex.org/W2737234477",
        "https://openalex.org/W6749116069",
        "https://openalex.org/W3039448353",
        "https://openalex.org/W3096754345",
        "https://openalex.org/W2134670479",
        "https://openalex.org/W1644641054",
        "https://openalex.org/W2894705404",
        "https://openalex.org/W2794561444",
        "https://openalex.org/W2560609797",
        "https://openalex.org/W1920022804",
        "https://openalex.org/W2963121255",
        "https://openalex.org/W3109646990",
        "https://openalex.org/W3104141662",
        "https://openalex.org/W3034428269",
        "https://openalex.org/W2963830382",
        "https://openalex.org/W2962731536",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963158438",
        "https://openalex.org/W3100020884",
        "https://openalex.org/W2962851485",
        "https://openalex.org/W2890556874"
    ],
    "abstract": "Recently, directly detecting 3D objects from 3D point clouds has received increasing attention. To extract object representation from an irregular point cloud, existing methods usually take a point grouping step to assign the points to an object candidate so that a PointNet-like network could be used to derive object features from the grouped points. However, the inaccurate point assignments caused by the hand-crafted grouping scheme decrease the performance of 3D object detection. In this paper, we present a simple yet effective method for directly detecting 3D objects from the 3D point cloud. Instead of grouping local points to each object candidate, our method computes the feature of an object from all the points in the point cloud with the help of an attention mechanism in the Transformers \\cite{vaswani2017attention}, where the contribution of each point is automatically learned in the network training. With an improved attention stacking scheme, our method fuses object features in different stages and generates more accurate object detection results. With few bells and whistles, the proposed method achieves state-of-the-art 3D object detection performance on two widely used benchmarks, ScanNet V2 and SUN RGB-D. The code and models are publicly available at \\url{https://github.com/zeliu98/Group-Free-3D}",
    "full_text": "Group-Free 3D Object Detection via Transformers\nZe Liu1,2*\nZheng Zhang2†\nYue Cao2 Han Hu2 Xin Tong2\n1University of Science and Technology of China\nliuze@mail.ustc.edu.cn\n2Microsoft Research Asia\n{zhez,yuecao,hanhu,xtong}@microsoft.com\nAbstract\nRecently, directly detecting 3D objects from 3D point\nclouds has received increasing attention. To extract object\nrepresentation from an irregular point cloud, existing meth-\nods usually take a point grouping step to assign the points\nto an object candidate so that a PointNet-like network could\nbe used to derive object features from the grouped points.\nHowever, the inaccurate point assignments caused by the\nhand-crafted grouping scheme decrease the performance of\n3D object detection.\nIn this paper, we present a simple yet effective method for\ndirectly detecting 3D objects from the 3D point cloud. In-\nstead of grouping local points to each object candidate, our\nmethod computes the feature of an object from all the points\nin the point cloud with the help of an attention mechanism\nin the Transformers [42], where the contribution of each\npoint is automatically learned in the network training. With\nan improved attention stacking scheme, our method fuses\nobject features in different stages and generates more ac-\ncurate object detection results. With few bells and whistles,\nthe proposed method achieves state-of-the-art 3D object de-\ntection performance on two widely used benchmarks, Scan-\nNet V2 and SUN RGB-D. The code and models are pub-\nlicly available at https://github.com/zeliu98/\nGroup-Free-3D\n1. Introduction\n3D object detection on point cloud simultaneously local-\nizes and recognizes 3D objects from a 3D point set. As a\nfundamental technique for 3D scene understanding, it plays\nan important role in many applications such as autonomous\ndriving, robotics manipulation, and augmented reality.\nDifferent from 2D object detection that works on 2D reg-\nular images, 3D object detection takes irregular and sparse\n*This work is done when Ze Liu is an intern at MSRA.\n†Contact person\nRoI-Pooling\nV oting Group-Free\nScene\nFigure 1. With the heuristic point grouping step, all points in blue\nbox of RoI-Pooling or blue ball of V oting are assigned and aggre-\ngated to derive the object features, resulting in wrong assignments.\nOur group-free based approach automatically learn the contribu-\ntion of all points to each object, which has ability to alleviate the\ndrawbacks of the hand-crafted grouping.\npoint cloud as input, which makes it difﬁcult to directly ap-\nply techniques used for 2D object detection techniques. Re-\ncent studies [27, 35, 26, 51] infer the object location and ex-\ntract object features directly from the irregular input point\ncloud for object detection. In these methods, a point group-\ning step is required to assign a group of points to each object\ncandidate, and then computes object features from assigned\ngroups of points. For this purpose, different grouping strate-\ngies have been investigated. Frustum-PointNet [27] applies\nthe Frustum envelop of a 2D proposal box for point group-\ning. Point R-CNN [35] groups points within the 3D box\nproposals to objects. V oteNet [26] determines the group as\nthe points which vote to the same (or spatially-close) center\npoint. Although these hand-crafted grouping schemes fa-\ncilitate 3D object detection, the complexity and diversity of\nobjects in real scene may lead to wrong point assignments\n(shown in Figure. 1) and degrade the 3D object detection\nperformance.\nIn this paper, we propose a simple yet effective tech-\nnique for detecting 3D objects from point clouds without the\narXiv:2104.00678v2  [cs.CV]  23 Apr 2021\nhandcrafted grouping step. The key idea of our approach is\nto take all points in the point cloud for computing features\nfor each object candidate, in which the contribution of each\npoint is determined by an automatically learned attention\nmodule. Based on this idea, we adapt the Transformer to ﬁt\nfor 3D object detection, which could simultaneously model\nthe object-object and object-pixel relationships, and extract\nthe object features without handcrafted grouping.\nTo further release the power of the transformer architec-\nture, we improve it in two aspects. First, we propose to\niteratively reﬁne the prediction of objects by updating the\nspatial encoding of objects in different stages, while the\noriginal application of Transformers adopt the ﬁxed spatial\nencoding. Second, we use the ensemble of detection results\npredicted at all stages during inference, instead of only us-\ning the results in the last stage as the ﬁnal results. These\ntwo modiﬁcations efﬁciently improve the performance of\n3D object detection with few computational overheads.\nWe validate our method with both ScanNet V2 [6]\nand SUN RGB-D [52] benchmarks. Results show that\nour method is effective and robust to the quality of ini-\ntial object candidates, where even a simple farthest point\nsampling approach has been able to produce strong re-\nsults on ScanNet V2 and SUN RGB-D benchmarks. For\nthe SUN RGB-D dataset, our method with the ensem-\nble scheme results in signiﬁcant performance improvement\n(+3.8 mAP@0.25). With few bells and whistles, the pro-\nposed approach achieved state-of-the-art performance on\nboth benchmarks.\nWe believe that our method also advocates a strong po-\ntential by using the attention mechanism or Transformers\nfor point cloud modeling, as it naturally addresses the in-\ntrinsic irregular and sparse distribution problems encoun-\ntered by 3D point clouds. This is contrary to 2D image\nmodeling, where such modeling tools mainly act as a chal-\nlenger or a complementary component to the mature grid\nmodeling tools such as ConvNets variants [16, 32, 46] and\nRoI Align [2, 5].\n2. Related Work\nGrid Projection/Voxelization based DetectionEarly 3D\nobject detection approaches project point cloud to 2D grids\nor 3D voxels so that the advanced convolutional networks\ncan be directly applied. A set of methods [18, 19, 50]\nproject point cloud to the bird’s view and then employ\n2D ConvNets for learning features and generate 3D boxes.\nThese methods are mainly applied for the outdoor scenes\nin autonomous driving where objects are distributed on a\nhorizontal plane so that their projections on the bird-view\nare occlusion-free. Note these approaches also need to\naddress the irregular and sparse distribution issues of the\n2D point projections, usually by pixelization. Other meth-\nods [4, 48] project point clouds into frontal views and then\napply 2D ConvNets for object detection. V oxel-based meth-\nods [37, 53] convert points into voxels and employ 3D Con-\nvNets to generate features for 3D box generation. All these\nprojection/voxelization based methods suffer from quanti-\nzation errors. The voxel-based methods also suffer from the\nlarge memory and computational cost of 3D convolutions.\nPoint based Detection Recent methods directly process\npoint clouds for 3D object detection. A core task of these\nmethods is to compute object features from the irregularly\nand sparsely distributed points. All existing methods ﬁrst\nassign a group of points to each object candidate and then\ncompute object features from each point group. Frustum-\nPointNet [27] groups points by the 3D Frustum envelope\nof a 2D box detected using an RGB object detector, and\napplies a PointNet on the grouped points to extract object\nfeatures for 3D box prediction. Point R-CNN [35] directly\ncomputes 3D box proposals, where the points within this 3D\nbox are used for object feature extraction. PV-RCNN [34]\nleverages the voxel representation to complement the point-\nbased representation in Point R-CNN [35] for 3D object de-\ntection and achieves better performance.\nV oteNet [26] groups points according to their voted cen-\nters and extract object features from grouped points by the\nPointNet. Some follow-up works further improve the point\ngroup generation procedure [51] or the object box localiza-\ntion and recognition procedure [3].\nOur method is also a point-based detection approach.\nUnlike existing point-based approaches, our method in-\nvolves all the points for computing the features of each ob-\nject candidate by an attention module. We also stack the\nattention modules to iteratively reﬁne the detection results\nwhile maintaining the simplicity of our method.\nNetwork architecture for Point Cloud A large set of\nnetwork architectures [38, 12, 29, 9, 47, 23, 44, 39, 45,\n28, 30, 33, 43, 40, 17, 1, 41, 49, 10, 22] have been proposed\nfor various point cloud based learning tasks. [13] provides\na good taxonomy and review of all these architectures, and\ndiscussing all of them is beyond the scope of this paper. Our\nmethod can take any point cloud architecture as the back-\nbone network for computing the point features. We adopt\nPointNet++ [30] used in previous methods [26, 25, 51] in\nour implementation for a fair comparison.\nAttention Mechanism/Transformer in NLP and 2D Im-\nage Recognition The attention-based Transformer is the\ndominant network architecture for the learning tasks in the\nﬁeld of NLP [42, 7, 21]. They have been also applied in\nthe ﬁeld of 2D image recognition [16, 32, 46] as a strong\ncompetitor to the dominant grid/dense modeling tools such\nas ConvNets and RoI-Align. The most related works in 2D\nN × 3\nbackbone\nM × (3+C)\nself\nattention\nmodule\nK object candidates\nK × (3+C)\nK' boxes\nM pointsInput points\ninitial object \ncandidate sampling\n(e.g. KPS)\n×6\ncross\nattention\nmodule\nFFN\nFigure 2. This ﬁgure illustrates the simple architecture of our approach, including three major components: a backbone network to extract\nfeature representations for each point in the point cloud, a sampling method to generate initial object candidates, and stacked attention\nmodules to extract and reﬁne object representations from all points.\nimage recognition to this paper are those who apply the at-\ntention mechanism or Transformer architectures into 2D ob-\nject detection [15, 11, 5, 2].\nAmong these approaches, our method is most similar to\n[2], which also applies a Transformer architecture for 2D\nobject detection. However, we found that directly apply-\ning this method to point clouds leads to signiﬁcantly lower\nperformance than our approach in 3D object detection task.\nOn the one hand, this is caused by the new technologies we\nproposed, and on the other hand, it probably because our\nmethod better integrated the advantage of traditional 3D de-\ntection framework. We discussed these factors in Sec. 4.6.\nOur approach improves the Transformer models to better\nadapt the 3D object detection task, including the update of\nobject query locations in the multi-stage iterative box pre-\ndiction, and an ensemble of detection results of stages. Al-\nthough the attention mechanisms still have a certain per-\nformance gap compared to the dominant convolution-based\nmethods in other tasks, we found that this architecture may\nwell address the point grouping issue for object detection\non point clouds. As a result, we advocate a strong potential\nof this architecture for modeling irregular 3D point clouds.\n3. Methodology\nIn 3D object detection on point clouds, we are given a\nset of N points S ∈RN×3 and the goal is to produce a set\nof 3D (oriented) bounding boxes with categorization scores\nOS to cover all ground-truth objects. Our overall architec-\nture is illustrated in Figure 2, involving three major compo-\nnents: a backbone networkto extract feature representations\nfor each point in point clouds, a sampling methodto gener-\nate initial object candidates, and stacked attention modules\nto extract and reﬁne object representations from all points.\nBackbone Architecture While our framework can lever-\nage any point cloud network to extract point features, we\nadopt PointNet++ [30] as the backbone network for a fair\ncomparison with the recent methods [26, 51].\nThe backbone network receives a point cloud ofNpoints\n(i.e. 2048) as input. We follow the encoder-decoder archi-\ntecture in [30] to ﬁrst down-sample the point cloud input\ninto 8×resolution (i.e. 256 points) through four stages of\nset abstraction layers, and then up-sample it to the resolu-\ntion of 2×(i.e. 1024 points) by feature propagation lay-\ners. The network will produce a C-channel vector repre-\nsentation for each point on the 2×resolution, denoted as\n{zi}M\ni=1, which are then used in theinitial object candidates\nsampling module and the stacked attentionmodules. In the\nfollowing parts, we will ﬁrst describe these two modules in\ndetail, and then present the loss function and head design\nfor this framework.\n3.1. Initial Object Candidate Sampling\nWhile object detection on 2D images usually adopts\ndata-independent anchor boxes as initial object candidates,\nit is generally intractable or impractical for 3D object detec-\ntion to apply this simple top-down strategy, as the number\nof anchor boxes in 3D search space is too huge to handle.\nInstead, we follow recent practice [35, 26] to sample initial\nobject candidates directly from the points on a point cloud,\nby a bottom-up way.\nWe consider three simple strategies to sample initial ob-\nject candidates from a point cloud:\n• Farthest Point Sampling (FPS). The FPS approach has\nbeen widely adopted to generate a point cloud from\na 3D shape or to down-sample the point clouds to a\nlower resolution. This method can be also employed to\nsample initial candidates from a point cloud. Firstly, a\npoint is randomly sampled from the point cloud. Then\nthe farthest point to the already-chosen point set is it-\neratively selected until the number of chosen points\nmeets the candidate budget. Though it is simple, we\nshow in experiments that this sampling approach along\nwith our framework has been able to be comparable to\nthe previous state-of-the-art 3D object detectors.\n• k-Closest Points Sampling (KPS). In this approach, we\nclassify each point on a point cloud to be a real ob-\nject candidate or not. The label assignment in training\nfollows this rule: a point is assigned positive if it is\ninside a ground-truth object box and it is one of the\nk-closest points to the object center. In inference, the\ninitial candidates are selected according to the classiﬁ-\ncation score of the point.\n• KPS with non-maximal suppression (KPS-NMS). Built\non the above KPS method, we introduce an additional\nnon-maximal suppression (NMS) step, which itera-\ntively removes spatially close object candidates, to im-\nprove the recall of sampled object candidates given a\nﬁxed number of objects, following the common prac-\ntice in 2D object detection. In addition to the object-\nness scores, we predict also the object center that each\npoint belongs to, where the NMS is conducted accord-\ningly. Speciﬁcally, the candidates locating within a ra-\ndius of the selected object center will be suppressed.\nThe radius is set to 0.05 in our experiments.\nIn experiments, we will demonstrate that our framework\nhas strong compatibility with the choice of these sampling\napproaches, mainly ascribed to the robust object feature ex-\ntraction approach described in the next subsection (see Ta-\nble 3). We use theKPS approach by default, due to its\nbetter performance than the FPS approach, and the same\neffectiveness as the more complex KPS-NMS approach.\n3.2. Iterative Object Feature Extraction and Box\nPrediction by Transformer Decoder\nWith the initial object candidates generated by a sam-\npling approach, we adopt the Transformer as the decoder to\nleverage all points on a point cloud to compute the object\nfeature of each candidate. The multi-head attention net-\nwork is the foundation of Transformer, it has three input\nsets: query set, key set and value set. Usually, the key set\nand value set are different projections of the same set of\nelements. Given a query set {qi}and a common element\nset {pk}of key set and value set, the output feature of the\nmulti-head attention of each query element is the aggrega-\ntion of the values that weighted by the attention weights,\nformulated as:\nAtt(qi,{pk}) =\nH∑\nh=1\nWh(\nK∑\nk=1\nAh\ni,k ·Vhpk), (1)\nAh\ni,k = exp[(Qhqi)T (Uhpk)]∑K\nk=1 exp[(Qhqi)T (Uhpk)]\n(2)\nwhere h indexes over attention heads, Ah is the atten-\ntion weight, Qh,Vh,Uh,Wh indicate the query projection\nweight, value projection weight, key projection weight, and\noutput projection weight, respectively.\nWhile the standard Transformer predicts the sentence of\na target language sequentially in an auto-regressive way,\nour Transformer computes object features and predicts 3D\nobject boxes in parallel. The Transformer consists of sev-\neral stacked multi-head self-attention and multi-head cross-\nattention modules, as illustrated in Figure 3.\nDenote the input point features at stagelas {z(l)\ni }M\ni=1 and\nthe object features at the same stage as {o(l)\ni }K\ni=1. A self-\nmul ti-head\t\tsel f - at tention \nQ K\t&\tV \nadd\t&\tnorm \nmul ti-head\tcr oss- at tention \nQ K\t&\tV \nadd\t&\tnorm \nobjects\t{ o i } bo x\tposi tion \nencoding \npoint\tposi tion \nencoding points\t{ z i } \nattention module\nFFN \nadd\t&\tnorm \nFigure 3. Architecture of the attention module.\nattention module models interaction between object fea-\ntures, formulated as:\nSelf-Att(o(l)\ni ,{o(l)\nj }) =Att(o(l)\ni ,{o(l)\nj }), (3)\nA cross-attention module leverages point features to com-\npute object features, formulated as:\nCross-Att(o(l)\ni ,{z(l)\nj }) =Att(o(l)\ni ,{z(l)\nj }), (4)\nwhere the notations are similar to those in Eq. (3). After the\nobject feature are updated through the self-attention module\nand cross attention module, a feed-forward network (FFN)\nis then applied to further transformed feature of each object.\nThere are a few differences compared to the original\nTransformer decoders, as described below.\nIterative Object Box Prediction and Spatial Encoding\nThe original Transformer adopts a ﬁxed spatial encoding\nfor all of the stacked attention modules, indicating the in-\ndices of each word. The application of Transformers to 2D\nobject detection [2] instantiate the spatial encoding (object\nprior) as a learnable weight. During inference, the spatial\nencoding is ﬁxed and same for any images.\nIn this work, we propose to reﬁne the spatial encodings\nof an object candidate stage by stage. Speciﬁcally, we pre-\ndict the 3D box locations and categories at each decoder\nstage, and the predicted location of a box in one stage will\nbe used to produce the reﬁned spatial encoding of the same\nobject, the reﬁned spatial encoding vector is then added\nto the output feature of this decoder stage and fed into\nthe next stage. The spatial encodings of an object and a\npoint are computed by applying independent linear layers\non the parameterization vector of a 3D box (x,y,z,l,h,w )\nand a point (x,y,z ), respectively. In the experiments, we\nwill show this approach can improve the mAP@0.25 and\nmAP@0.5 by 1.6 and 5.0 on the ScanNet V2 benchmark,\ncompared to the approach without iterative reﬁnement.\nEnsemble from Multi-Stage PredictionsAnother differ-\nence is that we ensemble the predictions of different stages\nto produce ﬁnal detection results, while previous methods\nusually adopt the output of the last stage as the ﬁnal results.\nConcretely, the detection results of different stages are com-\nbined and they together go through an NMS (IoU threshold\nof 0.25) procedure to generate the ﬁnal object detection re-\nsults. We ﬁnd this approach can signiﬁcantly improve the\nperformance of some benchmarks, e.g. +3.8 mAP@0.25\non the SUN RGB-D dataset. Also note the overhead of\nthis ensembling approach is marginal, mainly ascribed to\nthe multi-stage nature of the Transformer decoder.\n3.3. Heads and Loss Functions\nDecoder Head We apply head networks on all decoder\nstages, with each mostly following the setting in [26]. There\nare 5 prediction tasks: objectness prediction with a binary\nfocal loss [20] Lobj, box classiﬁcation with a cross entropy\nloss Lcls, center offset prediction with a smooth-L1 loss\nLcenter off, size classiﬁcation with a cross entropy lossLsz cls,\nand size offset prediction with a smooth-L1 loss Lsz off.\nAlso, all 5 prediction tasks are obtained by a shared 2-layer\nMLP and an independent linear layer.\nThe loss of l-th decoder stage is the combination of these\n5 loss terms by weighted summation:\nL(l)\ndecoder = β1L(l)\nobj+β2L(l)\ncls +β3L(l)\ncenter off+β4L(l)\nsz cls+β5L(l)\nsz off,\n(5)\nwhere the balancing factors are set default as β1 = 0.5,\nβ2 = 0.1, β3 = 1.0, β4 = 0.1 and β5 = 0.1. The losses on\nall decoder stages are averaged to form the ﬁnal loss:\nLdecoder = 1\nL\nL∑\nl=1\nL(l)\ndecoder. (6)\nSampling Head The head designs and the loss functions\nof the sampling module are similar to those of the decoders.\nThere are two differences: ﬁrstly, the box classiﬁcation task\nis not involved; secondly, the objectness task follows the\nlabel assignment as described in Sec. 3.1. Our ﬁnal loss is\nthe sum of decoder and sampling heads:\nL= Ldecoder + Lsampler (7)\n4. Experiments\n4.1. Datasets and Evaluation Protocol\nWe validate our approach on two widely-used 3D object\ndetection datasets: ScanNet V2 [6] and SUN RGB-D [36],\nand we follow the standard data splits [26] for them both.\nScanNet V2 [6]is constructed from an 3D reconstruction\ndataset of indoor scenes by enriched annotations. It con-\nsists of 1513 indoor scenes and 18 object categories. The\nannotations of per-point instance, semantic labels, and 3D\nbounding boxes are provided. We follow a standard evalu-\nation protocol [26] by using mean Average Precision(mAP)\nunder different IoU thresholds, without considering the ori-\nentation of bounding boxes.\nSUN RGB-D [36]is a single-view RGB-D dataset for 3D\nscene understanding, consisting of ∼5K indoor RGB and\ndepth images. The annotation consists of per-point semantic\nlabels and oriented bounding object bounding boxes of 37\nobject categories. The standard mean Average Precision is\nused as evaluation metrics and the evaluation is reported on\nthe 10 most common categories, following [26].\n4.2. Implementation Details\nScanNet V2 We follow recent practice [26, 31] to use\nPointNet++ as default backbone network for a fair compari-\nson. The backbone has 4 set abstraction layers and 2 feature\npropagation layers. For each set abstraction layer, the in-\nput point cloud is sub-sampled to 2048, 1024, 512, and 256\npoints with the increasing receptive radius of 0.2, 0.4, 0.8,\nand 1.2, respectively. Then, two feature propagation layers\nsuccessively up-sample the points to 512 and 1024. More\ntraining details are given in Appendix.\nSUN RGB-DThe implementation mostly follow [26]. We\nuse 20k points as input for each point cloud. The network\narchitecture and the data augmentation are the same as that\nfor ScanNet V2. As the orientation of the 3D box is re-\nquired in evaluation, we include an additional orientation\nprediction branch for all decoder stages. More training de-\ntails are given in Appendix.\n4.3. System-level Comparison\nIn this section, we compare with previous state-of-the-\narts on ScanNet V2 and SUN RGB-D. Since previous\nworks [26, 24] usually report the best results of multiple\ntimes on training and testing in the system-level compari-\nson, we report both best results and average results1\nScanNet V2 The results are shown in Table 1. With\nthe same backbone network of a standard PointNet++,\nthe proposed approach achieves 67.3 mAP@0.25 and 48.9\n1We train each setting 5 times and test each training trial 5 times. The\naverage performance of these 25 trials is reported to account for algorithm\nrandomness.\nmethods backbone mAP@0.25 mAP@0.5\nHGNet [3] GU-net 61.3 34.4\nGSDN [14] MinkNet 62.8 34.8\n3D-MPA [8] MinkNet 64.2 49.2\nV oteNet [26]2 PointNet++ 62.9 39.9\nMLCVNet [31] PointNet++ 64.5 41.4\nH3DNet [51] PointNet++ 64.4 43.4\nH3DNet [51] 4×PointNet++ 67.2 48.1\nOurs (L6, O256) PointNet++ 67.3 (66.3) 48.9 (48.5)\nOurs (L12, O256) PointNet++ 67.2 (66.6) 49.7 (49.0)\nOurs (L12, O256) PointNet++w2× 68.8 (67.7) 52.1 (50.6)\nOurs (L12, O512) PointNet++w2× 69.1 (68.6) 52.8 (51.8)\nTable 1. System level comparison on ScanNet V2 with state-of-the-arts. The main comparison is based on the best results of multiple\nexperiments between different methods, and the number within the bracket is the average result.\nNotations: 4×PointNet++ denotes 4 individual PointNet++; PointNet++w2 × denotes the backbone width is expanded by 2 times; L denotes the decoder\ndepth, and O denotes the number of object candidates, e.g. Ours (L6, O256) denotes a model with 6-layer decoder(i.e. 6 attention modules) and 256 object\ncandidates.\nmethods backbone inputs mAP@0.25 mAP@0.5\nV oteNet [26]2 PointNet++ point 59.1 35.8\nMLCVNet [31] PointNet++ point 59.8 -\nHGNet [3] GU-net point 61.6 -\nH3DNet [51] 4×PointNet++ point 60.1 39.0\nimV oteNet [25]∗ PointNet++ point+RGB 63.4 -\nOurs (L6, O256) PointNet++ point 63.0 (62.6) 45.2 (44.4)\nTable 2. System level comparison on SUN RGB-D with state-of-the-arts. The main comparison is based on the best results of multiple\nexperiments between different methods, and the number within the bracket is the average result. ∗imV oteNet use RGB images as addition\ninputs.\nmAP@0.5 using 6 decoder stages and 256 object candi-\ndates, which is 2.8 and 5.5 better than previous best results\nusing the same backbones. By more decoder stages as 12,\nthe gap increases to 6.3 on mAP@0.5.\nWith stronger backbones and more sampled object can-\ndidates, i.e. 2×more channels and 512 candidates, the\nperformance of the proposed approach is improved to 69.1\nmAP@0.25 and 52.8 mAP@0.5, outperforming previous\nbest method by a large margin.\nSUN RGB-D We also compare the proposed approach\nwith previous state-of-the-arts on the SUN RGB-D dataset,\nwhich is another widely used 3D object detection bench-\nmark. In this dataset, the ensemble approach over multi-\nple stages is used by default during inference. The results\nare shown in Table. 2. Our base model achieves 63.0 on\nmAP@0.25 and 45.2 on mAP@0.5, which outperforms all\nprevious state-of-the-arts that only use the point cloud. In\nparticular, it outperforms the H3DNet on mAP@0.5 by 6.2.\n4.4. Ablation Study\nIn this section, we validate our key designs on ScanNet\nV2. If not speciﬁed, all models have 6 attention modules,\n2We report the results of MMDetection3D(https://github.com/open-\nmmlab/mmdetection3d) instead of the ofﬁcial paper, which reported 46.8\nmAP@0.25 and 24.7 mAP@0.5 on ScanNet V2, and 57.7 mAP@0.25 and\n32.0 mAP@0.5 on SUN RGB-D.\nsampling method mAP@0.25 mAP@0.5\nFPS 64.5 46.2\nKPS-NMS 65.8 48.7\nKPS 66.3 48.5\nTable 3. Ablation study on applying different sampling strategies.\nk mAP@0.25 mAP@0.5\n1 65.7 48.7\n2 65.8 48.3\n4 66.3 48.5\n6 66.1 48.4\nTable 4. Ablation study on different values of k in KPS sampling\nstrategy.\n256 sampled candidates, and are equipped with the pro-\nposed iterative object prediction approach. In evaluation,\nwe report the average performance of 25 trials by default.\nSampling Strategy We ﬁrst ablate the effects of different\nsampling strategies in Table. 3. It shows that our approach\nperforms well by using different sampling strategies. It also\nworks well in a wide range of hyper-parameters, such as k\nin the KPS sampling approach (see Table. 4).\nThese results indicate the robustness of our framework\nfor choosing different sampling approaches.\nIterative Box Prediction Table 5 ablates several design\nchoices for iterative box prediction. With a naive iterative\niterative position encoding mAP@0.25 mAP@0.5\nnone 64.7 43.4\ncenter+size 64.6 43.5\n✓ center 65.2 47.5\n✓ center+size 66.3 48.5\nTable 5. Ablation study on the effectiveness of iterative box pre-\ndiction.\n# of layers mAP@0.25 mAP@0.5\n0 63.3 40.7\n1 64.8 43.9\n2 66.0 45.6\n3 66.4 46.6\n4 66.2 47.9\n5 66.3 48.3\n6 66.3 48.5\nTable 6. Ablation study on the performance of iterative box pre-\ndiction with different decoder layers.\nensemble ScanNet V2 SUN RGB-D\nmAP@0.25 mAP@0.5 mAP@0.25 mAP@0.5\n66.3 48.5 59.2 43.3\n✓ 66.4 48.7 63.0 45.2\nTable 7. Ablation study on the effectiveness of multi-stage ensem-\nble.\nmethod where no spatial encoding is involved in the decoder\nstages, the approach shows reasonably good performance\nof 64.7 mAP@0.25 and 43.4 mAP@0.25, likely because\nthe location information may have been implicitly included\nin the input object features. Actually, an additional ﬁxed\nposition encoding does not improve detection performance\n(64.6 mAP@0.25 and 43.5 mAP@0.5).\nBy reﬁning the encodings of the box location stage by\nstage, the localization ability of the approach is signiﬁcantly\nimproved of the 4.1 points gains on the mAP@0.5 metric\nover the naive implementation (47.5 vs. 43.4). Also, more\ndetailed spatial encoding by both box center and size is ben-\neﬁcial, compared to that only encodes box centers (66.3 vs.\n65.2 on mAP@0.25 and 48.5 vs. 47.5 on mAP@0.5).\nTable. 6 shows the performance of iterative box pre-\ndiction with different decoder stages. More stages can\nbring signiﬁcant performance improvement, especially in\nthe mAP@0.5. Compared with not applying any attention\nmodules, our 6-stage model performs better on mAP@0.25\nand mAP@0.5 by 3.0 and 7.8, respectively.\nEnsemble Multi-stage PredictionsEach decoder stage of\nour approach will predict a set of 3D boxes. It is nat-\nural to ensemble these results of different decoder stages\nin expecting better ﬁnal detection results. Table 7 shows\nthe results, where signiﬁcantly performance improvements\nare observed on SUN RGB-D (+3.8 mAP@0.25 and +1.9\nmAP@0.5) and maintained performance on ScanNet V2.\nWe hypothesize that it is because the point clouds of SUN\nmethod mAP@0.25 mAP@0.5\nRoI-Pooing 65.1 44.4\nV oting 64.2 44.1\nOurs 66.3 48.5\nTable 8. Comparison with grouping-based approaches.\nmethod backbone mAP frames/s0.25 0.5\nMLCVNet [31] PointNet++ 64.5 41.4 5.44\nH3DNet [51] 4×PointNet++ 67.2 48.1 3.76\nOurs (L6, O256) PointNet++ 67.3 48.9 6.71\nOurs (L12, O256) PointNet++ 67.2 49.7 5.70\nOurs (L12, O256) PointNet++w2× 68.8 52.1 5.23\nOurs (L12, O512) PointNet++w2× 69.1 52.8 5.17\nTable 9. Comparison on realistic inference speed on ScanNet V2.\nRGB-D have lower quality than those of ScanNet V2: SUN\nRGB-D adopts real RGB-D signals to generate point clouds\nthat many objects have missing parts due to occlusion,\nwhile the ScanNet V2 generate point clouds from 3D shape\nmeshes which are more complete. The ensemble method\ncan boost the performance more on real 3D scenes.\nComparison with Group-based Approaches Aggregat-\ning point features through RoI-Pooing, or according to the\nvoted centers are two typical handcrafted grouping strate-\ngies [35, 26] in 3D object detection. We refer these two\ngrouping strategies as baselines and compare with them.\nFor a fair comparison, we only switch the feature aggre-\ngation mechanism while all other settings (e.g. the 6-stage\ndecoder) remain unchanged. More details are in Appendix.\nTable 8 show the results. Although RoI-Pooling outper-\nforms than the voting approach, it is still worse than our\ngroup-free approach by 1.2 points on mAP@0.25 and 4.1\npoints on mAP@0.5.\n4.5. Inference Speed\nThe computational complexity of the attention model\nis determined by the number of points in a point cloud\nand the number of sampled object candidates. In our ap-\nproach, only a small number of object candidates are sam-\npled, which makes the cost of the attention model insigniﬁ-\ncant. With our default setting (256 object candidates, 1024\noutput points), stacking one attention model brings 0.95\nGFLOPs, which is quite light compared to the backbone.\nIn addition, the realistic inference speed of our method\nis also very competitive, compared to other state-of-the-art\nmethods. For a fair comparison, all experiments are run on\nthe same workstation (single Titan-XP GPU, 256G RAM,\nand Xeon E5-2650 v3) and environment (Ubuntu-16.04,\nPython 3.6, Cuda-10.1, and PyTorch-1.3.1). The ofﬁcial\ncode of other methods is used for evaluation. The batch\nsize of all experiments is set to 1 (i.e. single image). The\nresults are shown in Table. 9. Our method achieves better\nperformance and also higher inference speed.\nGT\n Proposal\n Layer6\nLayer3\nScanNet V2 SUN RGB-D\nFigure 4. Qualitative results of different decoder stages. The ﬁrst row is the results on SUN RGB-D, and the second row is the results on\nScanNet V2. The color of bounding boxes represents their category.\nmethod epoch mAP@0.25 mAP@0.5\nDETR 400 39.6 21.4\nDETR+KPS 400 59.6 41.0\nDETR+KPS+iter pred 400 59.9 42.9\nDETR+KPS+iter pred 1200 61.8 45.2\nOurs 400 66.3 48.5\nTable 10. The comparison between DETR and our method on\nScanNet V2. KPS represent k-Closest Points Sampling, iter pred\nrepresents iterative prediction.\n4.6. Comparison with DETR\nDETR [2] is a pioneer work that applies the Trans-\nformer to 2D object detection. Compared with DETR, our\nmethod involves more domain knowledge, such as the data-\ndependent initial object candidate generation, where DETR\nuses a data-independent object prior to representing each\nobject candidate and is automatically learned without ex-\nplicit supervision. Moreover, there is no iterative reﬁnement\non spatial encodings in DETR as in our approach. We evalu-\nate these differences in 3D object detection. For a fair com-\nparison, the backbone and decoder heads used in DETR are\nthe same as in ours. We carefully tune the hyper-parameters\nfor DETR and chose the best setting in comparison.\nThe results are shown in Table 10. With the same train-\ning length of 400 epochs, DETR achieves 39.6 mAP@0.25\nand 21.4 mAP@0.5, signiﬁcantly worse than our method.\nWe guess it is mainly because of optimization difﬁculty by\nthe data-independent object representation. The ﬁxed spa-\ntial encoding also may contribute to inferior performance.\nIn fact, the performance can be improved signiﬁcantly by\nbridging these differences, reaching 59.9 mAP@0.25 and\n42.9 mAP@0.5 using the same training epochs, and 61.8\nmAP@0.25 and 45.2 mAP@0.5 by longer training.\nThe remaining performance gap is due to the difference\nin ground-truth assignments, where DETR adopts a set loss\nto automatically determine the assignments by detection\nLayer3\n Layer6\nLayer1\nScene\nFigure 5. Visualizations on cross-attention weight in different de-\ncoder stages. The green point represents the reference object can-\ndidates. The redder color represent higher attention weight.\nlosses and our approach manually assigns object candidates\nto ground-truths. This assignment may also be difﬁcult for\na network to learn.\n4.7. Qualitative Results\nFig. 4 illustrates the qualitative results on both ScanNet\nV2 and SUN RGB-D. As the decoder networks go deeper,\nthe more accurate detection results are observed.\nFig. 5 visualizes the learned cross-attention weights of\ndifferent decoder stages. We could observe that the model\nof the lower stage always focuses on the surrounding points\nwithout considering the geometry. With the reﬁnement, the\nmodel of the higher stage could focus more on the geometry\nand extract more high-quality object features.\n5. Conclusion\nIn this paper, we present a simple yet effective 3D object\ndetector based on the attention mechanism in Transform-\ners. Unlike previous methods that require a grouping step\nfor object feature computation, this detector is group-free\nwhich computes object features from all points in a point\ncloud, with the contribution of each point automatically de-\ntermined by the attention modules. The proposed method\nachieves state-of-the-art performance on ScanNet V2 and\nSUN RGB-D benchmarks.\nA1. Training Details\nA1.1. Our Approach\nScanNet V2 We follow recent practice [26, 31] to use\nthe PointNet++ as our default backbone network for a fair\ncomparison. The backbone network has four set abstraction\nlayers and two feature propagation layers. For each set ab-\nstraction layer, the input point cloud is sub-sampled to 2048,\n1024, 512, and 256 points with the increasing receptive ra-\ndius of 0.2, 0.4, 0.8, and 1.2, respectively. Then, two feature\npropagation layers successively up-sample the points to 512\nand 1024, respectively.\nIn the training phase, we use 50k 1 points as input and\nadopt the same data augmentation as in [26], including a\nrandom ﬂip, a random rotation between [ −5◦, 5◦], and\na random scaling of the point cloud by [0.9, 1.1]. The\nnetwork is trained from scratch by the AdamW optimizer\n(β1=0.9, β2=0.999) with 400 epochs. The weight decay is\nset to 5e-4. The initial learning rate is 0.006 and decayed by\n10×at the 280-th epoch and the 340-th epoch. The learn-\ning rate of the attention modules is set as 1/10 of that in\nthe backbone network. The gradnorm clip is applied to sta-\nbilize the training dynamics. Following [26] we use class-\naware head for box size prediction.\nSUN RGB-D The implementation settings mostly fol-\nlow [26]. We use 20k points as input for each point cloud.\nThe network architecture and the data augmentation are the\nsame as that for ScanNet V2. As the orientation of the 3D\nbox is required in evaluation, we include an additional ori-\nentation prediction branch for all decoder layers. The ori-\nentation branch contains a classiﬁcation task and an offset\nregression task with loss weights of 0.1 and 0.04, respec-\ntively.\nIn training, the network is trained from scratch by the\nAdamW optimizer ( β1=0.9, β2=0.999) with 600 epochs if\nnot speciﬁed. The initial learning rate is 0.004 and decayed\nby 10×at the 420-th epoch, the 480-th epoch, and the 540-\nth epoch. The learning rate of attention modules is set as\n1/20 of the backbone network. The weight decay is set to\n1e-7, and the gradnorm clip is used. We use class-agnostic\nhead for size prediction.\n1We evaluate our model on 40k points on ScanNet V2 according to\nprevious works and the performance is similar: 66.3(40k) vs. 66.2(50k) on\nmAP@0.25, and 48.5(40k) vs. 48.6(50k) on mAP@0.5.\nmethod mAP@0.25 mAP@0.5\naverage 64.2 44.2\nmax 65.1 44.4\nTable 11. Comparison between average-pooling and max-pooling\non ScanNet V2.\nA1.2. Other Pooling Mechanisms\nFor a fair comparison, we only switch the feature ag-\ngregation mechanism while all other settings remain un-\nchanged. In the following, we will introduce the implemen-\ntation details of RoI-Pooling and V oting aggregation mech-\nanism.\nRoI-Pooling For a given object candidate, the points\nwithin the predicted box of the object candidate are aggre-\ngated together, and the reﬁned box is predicted from the ag-\ngregated features. The same as our group-free approach, the\nmulti-stage reﬁnement is also adopted. Thus the aggregated\npoints and features will be updated and reﬁned in multiple\nstages. Also, we tried two different strategies for feature\naggregation: average-pooling and max-pooling. The results\nare shown in Table. 11. We could ﬁnd that the approach\nwith max-pooling performs better, so we use it for compar-\nison by default.\nVoting The voting mechanism is ﬁrst introduced by\nV oteNet [26] and we implement it in our framework.\nSpeciﬁcally, each point predicts the center of its correspond-\ning object, and if the distance between the predicted center\nof points and the center of an object candidate is less than a\nthreshold (set to 0.3 meters), then these points and the can-\ndidate are grouped. Further, a two-layer MLP with max-\npooling is used to form the aggregation feature of the object\ncandidate, and the reﬁned boxes are predicted from the ag-\ngregated features in the multi-stage reﬁnement process.\nA2. More Results\nWe show per-category results on ScanNet V2 and SUN\nRGB-D under different IoU thresholds. Table 12 and Ta-\nble 13 show the results of mAP@0.25 and mAP@0.5 on\nScanNet V2, respectively. Table 14 and Table 15 show the\nresults of mAP@0.25 and mAP@0.5 on SUN RGB-D, re-\nspectively.\nWe also show more qualitative results of our method on\nScanNet V2 and SUN RGB-D. The results are shown in\nFigure 6 (ScanNet V2) and Figure 7 (SUN RGB-D).\nReferences\n[1] Matan Atzmon, Haggai Maron, and Yaron Lipman. Point\nconvolutional neural networks by extension operators. arXiv\npreprint arXiv:1803.10091, 2018. 2\nmethods backbone cab bed chair sofa tabl door wind bkshf pic cntr desk curt fridg showr toil sink bath ofurn mAP\nV oteNet [26] PointNet++ 47.7 88.7 89.5 89.3 62.1 54.1 40.8 54.3 12.0 63.9 69.4 52.0 52.5 73.3 95.9 52.0 92.5 42.4 62.9\nMLCVNet [31] PointNet++ 42.5 88.5 90.0 87.4 63.5 56.9 47.0 56.9 11.9 63.9 76.1 56.7 60.9 65.9 98.3 59.2 87.2 47.9 64.5\nH3DNet [51] 4×PointNet++ 49.4 88.6 91.8 90.2 64.9 61.0 51.9 54.9 18.6 62.0 75.9 57.3 57.2 75.3 97.9 67.4 92.5 53.6 67.2\nOurs (L6, O256) PointNet++ 54.1 86.2 92.0 84.8 67.8 55.8 46.9 48.5 15.0 59.4 80.4 64.2 57.2 76.3 97.6 76.8 92.5 55.0 67.3\nOurs (L12, O256) PointNet++ 55.4 86.6 91.8 86.6 73.0 54.5 49.4 47.7 13.1 63.3 82.4 63.3 53.2 74.0 99.2 67.7 91.7 55.8 67.2\nOurs (L12, O256) PointNet++w2× 56.5 88.2 92.5 88.2 71.6 57.5 48.3 53.7 17.5 71.0 79.5 63.4 58.1 71.7 99.4 71.1 93.0 57.8 68.8\nOurs (L12, O512) PointNet++w2× 52.1 91.9 93.6 88.0 70.7 60.7 53.7 62.4 16.1 58.5 80.9 67.9 47.0 76.3 99.6 72.0 95.3 56.4 69.1\nTable 12. Performance of mAP@0.25 for each category on the ScanNet V2 dataset.\nmethods backbone cab bed chair sofa tabl door wind bkshf pic cntr desk curt fridg showr toil sink bath ofurn mAP\nV oteNet [26] PointNet++ 14.6 77.8 73.1 80.5 46.5 25.1 16.0 41.8 2.5 22.3 33.3 25.0 31.0 17.6 87.8 23.0 81.6 18.7 39.9\nH3DNet [51] 4×PointNet++ 20.5 79.7 80.1 79.6 56.2 29.0 21.3 45.5 4.2 33.5 50.6 37.3 41.4 37.0 89.1 35.1 90.2 35.4 48.1\nOurs (L6, O256) PointNet++ 23.0 78.4 78.9 68.7 55.1 35.3 23.6 39.4 7.5 27.2 66.4 43.3 43.0 41.2 89.7 38.0 83.4 37.3 48.9\nOurs (L12, O256) PointNet++ 23.8 77.2 81.6 65.1 62.8 35.0 21.3 39.4 7.0 33.1 66.3 39.3 43.9 47.0 91.2 38.5 85.2 37.4 49.7\nOurs (L12, O256) PointNet++w2× 26.2 80.7 83.5 70.7 57.0 37.4 21.2 47.7 8.8 45.3 60.7 42.2 43.5 42.7 95.5 42.3 89.7 43.4 52.1\nOurs (L12, O512) PointNet++w2× 26.0 81.3 82.9 70.7 62.2 41.7 26.5 55.8 7.8 34.7 67.2 43.9 44.3 44.1 92.8 37.4 89.7 40.6 52.8\nTable 13. Performance of mAP@0.5 for each category on the ScanNet V2 dataset.\nmethods backbone bathtub bed bkshf chair desk drser nigtstd sofa table toilet mAP\nV oteNet [26] PointNet++ 75.5 85.6 31.9 77.4 24.8 27.9 58.6 67.4 51.1 90.5 59.1\nMLCVNet [31] PointNet++ 79.2 85.8 31.9 75.8 26.5 31.3 61.5 66.3 50.4 89.1 59.8\nHGNet [3] PointNet++ w/ FPN 78.0 84.5 35.7 75.2 34.3 37.6 61.7 65.7 51.6 91.1 61.6\nH3DNet [51] 4×PointNet++ 73.8 85.6 31.0 76.7 29.6 33.4 65.5 66.5 50.8 88.2 60.1\nOurs (L6, O256) PointNet++ 80.0 87.8 32.5 79.4 32.6 36.0 66.7 70.0 53.8 91.1 63.0\nTable 14. Performance of mAP@0.25 for each category on the SUN RGB-D validation set.\nmethods backbone bathtub bed bkshf chair desk drser nigtstd sofa table toilet mAP\nV oteNet [26] PointNet++ 45.4 53.4 6.8 56.5 5.9 12.0 38.6 49.1 21.3 68.5 35.8\nH3DNet [51] 4×PointNet++ 47.6 52.9 8.6 60.1 8.4 20.6 45.6 50.4 27.1 69.1 39.0\nOurs (L6, O256) PointNet++ 64.0 67.1 12.4 62.6 14.5 21.9 49.8 58.2 29.2 72.2 45.2\nTable 15. Performance of mAP@0.5 for each category on the SUN RGB-D validation set.\n[2] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-\nto-end object detection with transformers. arXiv preprint\narXiv:2005.12872, 2020. 2, 3, 4, 8\n[3] Jintai Chen, Biwen Lei, Qingyu Song, Haochao Ying,\nDanny Z Chen, and Jian Wu. A hierarchical graph network\nfor 3d object detection on point clouds. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 392–401, 2020. 2, 6, 10\n[4] Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia.\nMulti-view 3d object detection network for autonomous\ndriving. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, pages 1907–1915,\n2017. 2\n[5] Cheng Chi, Fangyun Wei, and Han Hu. Relationnet++:\nBridging visual representations for object detection via trans-\nformer decoder. arXiv preprint arXiv:2010.15831, 2020. 2,\n3\n[6] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-\nber, Thomas Funkhouser, and Matthias Nießner. Scannet:\nRichly-annotated 3d reconstructions of indoor scenes. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 5828–5839, 2017. 2, 5\n[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018. 2\n[8] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian\nLeibe, and Matthias Nießner. 3d-mpa: Multi-proposal ag-\ngregation for 3d semantic instance segmentation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 9031–9040, 2020. 6\n[9] Yifan Feng, Zizhao Zhang, Xibin Zhao, Rongrong Ji, and\nYue Gao. Gvcnn: Group-view convolutional neural networks\nOurs GT\nFigure 6. Qualitative results on ScanNet V2.\nGTOurs\nImage\nFigure 7. Qualitative results on SUN RGB-D.\nfor 3d shape recognition. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, pages\n264–272, 2018. 2\n[10] Fabian Groh, Patrick Wieschollek, and Hendrik PA Lensch.\nFlex-convolution. In Asian Conference on Computer Vision,\npages 105–122. Springer, 2018. 2\n[11] Jiayuan Gu, Han Hu, Liwei Wang, Yichen Wei, and Jifeng\nDai. Learning region features for object detection. In Pro-\nceedings of the European Conference on Computer Vision\n(ECCV), pages 381–395, 2018. 3\n[12] Haiyun Guo, Jinqiao Wang, Yue Gao, Jianqiang Li, and\nHanqing Lu. Multi-view 3d object retrieval with deep em-\nbedding network. IEEE Transactions on Image Processing,\n25(12):5526–5537, 2016. 2\n[13] Yulan Guo, Hanyun Wang, Qingyong Hu, Hao Liu, Li Liu,\nand Mohammed Bennamoun. Deep learning for 3d point\nclouds: A survey. IEEE transactions on pattern analysis and\nmachine intelligence, 2020. 2\n[14] JunYoung Gwak, Christopher Choy, and Silvio Savarese.\nGenerative sparse detection networks for 3d single-shot ob-\nject detection. arXiv preprint arXiv:2006.12356, 2020. 6\n[15] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen\nWei. Relation networks for object detection. InCVPR, pages\n3588–3597, 2018. 3\n[16] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Lo-\ncal relation networks for image recognition. In Proceedings\nof the IEEE International Conference on Computer Vision,\npages 3464–3473, 2019. 2\n[17] Varun Jampani, Martin Kiefel, and Peter V Gehler. Learn-\ning sparse high dimensional ﬁlters: Image ﬁltering, dense\ncrfs and bilateral neural networks. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, pages 4452–4461, 2016. 2\n[18] Jason Ku, Melissa Moziﬁan, Jungwook Lee, Ali Harakeh,\nand Steven L Waslander. Joint 3d proposal generation and\nobject detection from view aggregation. In 2018 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems\n(IROS), pages 1–8. IEEE, 2018. 2\n[19] Ming Liang, Bin Yang, Shenlong Wang, and Raquel Urtasun.\nDeep continuous fusion for multi-sensor 3d object detection.\nIn Proceedings of the European Conference on Computer Vi-\nsion (ECCV), pages 641–656, 2018. 2\n[20] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and\nPiotr Doll´ar. Focal loss for dense object detection. In Pro-\nceedings of the IEEE international conference on computer\nvision, pages 2980–2988, 2017. 5\n[21] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar\nJoshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-\nmoyer, and Veselin Stoyanov. Roberta: A robustly optimized\nbert pretraining approach. arXiv preprint arXiv:1907.11692,\n2019. 2\n[22] Ze Liu, Han Hu, Yue Cao, Zheng Zhang, and Xin Tong. A\ncloser look at local aggregation operators in point cloud anal-\nysis. arXiv preprint arXiv:2007.01294, 2020. 2\n[23] Daniel Maturana and Sebastian Scherer. V oxnet: A 3d con-\nvolutional neural network for real-time object recognition.\nIn 2015 IEEE/RSJ International Conference on Intelligent\nRobots and Systems (IROS), pages 922–928. IEEE, 2015. 2\n[24] MMDetection3D. open-mmlab/mmdetection3d. 5\n[25] Charles R Qi, Xinlei Chen, Or Litany, and Leonidas J\nGuibas. Imvotenet: Boosting 3d object detection in point\nclouds with image votes. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 4404–4413, 2020. 2, 6\n[26] Charles R Qi, Or Litany, Kaiming He, and Leonidas J\nGuibas. Deep hough voting for 3d object detection in point\nclouds. In Proceedings of the IEEE International Conference\non Computer Vision, pages 9277–9286, 2019. 1, 2, 3, 5, 6, 7,\n9, 10\n[27] Charles R Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas J\nGuibas. Frustum pointnets for 3d object detection from rgb-\nd data. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 918–927, 2018. 1, 2\n[28] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.\nPointnet: Deep learning on point sets for 3d classiﬁcation\nand segmentation. InProceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pages 652–660,\n2017. 2\n[29] Charles R Qi, Hao Su, Matthias Nießner, Angela Dai,\nMengyuan Yan, and Leonidas J Guibas. V olumetric and\nmulti-view cnns for object classiﬁcation on 3d data. In Pro-\nceedings of the IEEE conference on computer vision and pat-\ntern recognition, pages 5648–5656, 2016. 2\n[30] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J\nGuibas. Pointnet++: Deep hierarchical feature learning on\npoint sets in a metric space. In NIPS, 2017. 2, 3\n[31] Xie Qian, Lai Yu-kun, Wu Jing, Wang Zhoutao, Zhang Yim-\ning, Xu Kai, and Wang Jun. Mlcvnet: Multi-level context\nvotenet for 3d object detection. In The IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2020. 5,\n6, 7, 9, 10\n[32] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan\nBello, Anselm Levskaya, and Jonathon Shlens. Stand-\nalone self-attention in vision models. arXiv preprint\narXiv:1906.05909, 2019. 2\n[33] Yiru Shen, Chen Feng, Yaoqing Yang, and Dong Tian. Min-\ning point cloud local structures by kernel correlation and\ngraph pooling. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 4548–4557,\n2018. 2\n[34] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping\nShi, Xiaogang Wang, and Hongsheng Li. Pv-rcnn: Point-\nvoxel feature set abstraction for 3d object detection. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 10529–10538, 2020. 2\n[35] Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. Pointr-\ncnn: 3d object proposal generation and detection from point\ncloud. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 770–779, 2019. 1, 2,\n3, 7\n[36] Shuran Song, Samuel P Lichtenberg, and Jianxiong Xiao.\nSun rgb-d: A rgb-d scene understanding benchmark suite. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 567–576, 2015. 5\n[37] Shuran Song and Jianxiong Xiao. Deep sliding shapes for\namodal 3d object detection in rgb-d images. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 808–816, 2016. 2\n[38] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik\nLearned-Miller. Multi-view convolutional neural networks\nfor 3d shape recognition. In Proceedings of the IEEE in-\nternational conference on computer vision, pages 945–953,\n2015. 2\n[39] Maxim Tatarchenko, Alexey Dosovitskiy, and Thomas Brox.\nOctree generating networks: Efﬁcient convolutional archi-\ntectures for high-resolution 3d outputs. InProceedings of the\nIEEE International Conference on Computer Vision, pages\n2088–2096, 2017. 2\n[40] Gusi Te, Wei Hu, Amin Zheng, and Zongming Guo. Rgcnn:\nRegularized graph cnn for point cloud segmentation. In\n2018 ACM Multimedia Conference on Multimedia Confer-\nence, pages 746–754. ACM, 2018. 2\n[41] Hugues Thomas, Charles R Qi, Jean-Emmanuel Deschaud,\nBeatriz Marcotegui, Franc ¸ois Goulette, and Leonidas J\nGuibas. Kpconv: Flexible and deformable convolution for\npoint clouds. In Proceedings of the IEEE International Con-\nference on Computer Vision, pages 6411–6420, 2019. 2\n[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need, 2017. 1, 2\n[43] Chu Wang, Babak Samari, and Kaleem Siddiqi. Local spec-\ntral graph convolution for point set feature learning. In Pro-\nceedings of the European Conference on Computer Vision\n(ECCV), pages 52–66, 2018. 2\n[44] Peng-Shuai Wang, Yang Liu, Yu-Xiao Guo, Chun-Yu Sun,\nand Xin Tong. O-cnn: Octree-based convolutional neu-\nral networks for 3d shape analysis. ACM Transactions on\nGraphics (TOG), 36(4):72, 2017. 2\n[45] Peng-Shuai Wang, Chun-Yu Sun, Yang Liu, and Xin Tong.\nAdaptive o-cnn: A patch-based deep representation of 3d\nshapes. ACM Transactions on Graphics (TOG), 37(6):1–11,\n2018. 2\n[46] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\ning He. Non-local neural networks. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 7794–7803, 2018. 2\n[47] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lin-\nguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d\nshapenets: A deep representation for volumetric shapes. In\nCVPR, 2015. 2\n[48] Bin Xu and Zhenzhong Chen. Multi-level fusion based 3d\nobject detection from monocular images. In Proceedings of\nthe IEEE conference on computer vision and pattern recog-\nnition, pages 2345–2353, 2018. 2\n[49] Yifan Xu, Tianqi Fan, Mingye Xu, Long Zeng, and Yu Qiao.\nSpidercnn: Deep learning on point sets with parameterized\nconvolutional ﬁlters. In Proceedings of the European Con-\nference on Computer Vision (ECCV), pages 87–102, 2018.\n2\n[50] Bin Yang, Wenjie Luo, and Raquel Urtasun. Pixor: Real-\ntime 3d object detection from point clouds. InProceedings of\nthe IEEE conference on Computer Vision and Pattern Recog-\nnition, pages 7652–7660, 2018. 2\n[51] Zaiwei Zhang, Bo Sun, Haitao Yang, and Qixing Huang.\nH3dnet: 3d object detection using hybrid geometric primi-\ntives. arXiv preprint arXiv:2006.05682, 2020. 1, 2, 3, 6, 7,\n10\n[52] Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Tor-\nralba, and Aude Oliva. Learning deep features for scene\nrecognition using places database. In Advances in neural\ninformation processing systems, pages 487–495, 2014. 2\n[53] Yin Zhou and Oncel Tuzel. V oxelnet: End-to-end learning\nfor point cloud based 3d object detection. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 4490–4499, 2018. 2"
}