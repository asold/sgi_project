{
  "title": "Functional annotation of enzyme-encoding genes using deep learning with transformer layers",
  "url": "https://openalex.org/W4388665407",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2888308720",
      "name": "Gi-Bae Kim",
      "affiliations": [
        "Korea Advanced Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2116942412",
      "name": "Ji Yeon Kim",
      "affiliations": [
        "Korea Advanced Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2810563333",
      "name": "Jong-An Lee",
      "affiliations": [
        "Korea Advanced Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2794792605",
      "name": "Charles J. Norsigian",
      "affiliations": [
        "University of California, San Diego",
        "La Jolla Bioengineering Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2068736320",
      "name": "Bernhard Ø. Palsson",
      "affiliations": [
        "La Jolla Bioengineering Institute",
        "Novo Nordisk Foundation",
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A2150485893",
      "name": "Sang Yup Lee",
      "affiliations": [
        "Korea Advanced Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2888308720",
      "name": "Gi-Bae Kim",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2116942412",
      "name": "Ji Yeon Kim",
      "affiliations": [
        "Korea Advanced Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2810563333",
      "name": "Jong-An Lee",
      "affiliations": [
        "Korea Advanced Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2794792605",
      "name": "Charles J. Norsigian",
      "affiliations": [
        "Korea Advanced Institute of Science and Technology",
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A2068736320",
      "name": "Bernhard Ø. Palsson",
      "affiliations": [
        "Novo Nordisk Foundation",
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A2150485893",
      "name": "Sang Yup Lee",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2730472814",
    "https://openalex.org/W2980789587",
    "https://openalex.org/W2951433247",
    "https://openalex.org/W2953321375",
    "https://openalex.org/W2992752586",
    "https://openalex.org/W4236358448",
    "https://openalex.org/W4318071656",
    "https://openalex.org/W3177500196",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W4213112325",
    "https://openalex.org/W3006069876",
    "https://openalex.org/W4220933834",
    "https://openalex.org/W4283032119",
    "https://openalex.org/W3082397379",
    "https://openalex.org/W2903357856",
    "https://openalex.org/W3207760869",
    "https://openalex.org/W3122486793",
    "https://openalex.org/W3126464728",
    "https://openalex.org/W4220723906",
    "https://openalex.org/W3116625060",
    "https://openalex.org/W3092146751",
    "https://openalex.org/W3201757323",
    "https://openalex.org/W4379087313",
    "https://openalex.org/W4362471278",
    "https://openalex.org/W4322494707",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3112376646",
    "https://openalex.org/W3143063265",
    "https://openalex.org/W2146611007",
    "https://openalex.org/W3008588639",
    "https://openalex.org/W4362522281",
    "https://openalex.org/W3216325381",
    "https://openalex.org/W2072408029",
    "https://openalex.org/W2029839377",
    "https://openalex.org/W1986878268",
    "https://openalex.org/W2963691377",
    "https://openalex.org/W3185391990",
    "https://openalex.org/W4220991280",
    "https://openalex.org/W3183475563",
    "https://openalex.org/W6791955017",
    "https://openalex.org/W3105895642",
    "https://openalex.org/W2769860552",
    "https://openalex.org/W3146384714",
    "https://openalex.org/W2995997429",
    "https://openalex.org/W2950954328",
    "https://openalex.org/W2127322768",
    "https://openalex.org/W2170747616",
    "https://openalex.org/W3156458251",
    "https://openalex.org/W4388665407",
    "https://openalex.org/W4394472537",
    "https://openalex.org/W2966590054",
    "https://openalex.org/W4251027331",
    "https://openalex.org/W2971227267"
  ],
  "abstract": "Abstract Functional annotation of open reading frames in microbial genomes remains substantially incomplete. Enzymes constitute the most prevalent functional gene class in microbial genomes and can be described by their specific catalytic functions using the Enzyme Commission (EC) number. Consequently, the ability to predict EC numbers could substantially reduce the number of un-annotated genes. Here we present a deep learning model, DeepECtransformer, which utilizes transformer layers as a neural network architecture to predict EC numbers. Using the extensively studied Escherichia coli K-12 MG1655 genome, DeepECtransformer predicted EC numbers for 464 un-annotated genes. We experimentally validated the enzymatic activities predicted for three proteins (YgfF, YciO, and YjdM). Further examination of the neural network’s reasoning process revealed that the trained neural network relies on functional motifs of enzymes to predict EC numbers. Thus, DeepECtransformer is a method that facilitates the functional annotation of uncharacterized genes.",
  "full_text": "Article https://doi.org/10.1038/s41467-023-43216-z\nFunctional annotation of enzyme-encoding\ngenes using deep learning with\ntransformer layers\nGi Bae Kim 1,2,3,J iY e o nK i m1,2,3, Jong An Lee1,2,3, Charles J. Norsigian4,5,\nBernhard O. Palsson5,6,7 &S a n gY u pL e e1,2,3,8\nFunctional annotation of open reading frames in microbial genomes remains\nsubstantially incomplete. Enzymes constitute the most prevalent functional\ngene class in microbial genomes and can be described by their speciﬁc cata-\nlytic functions using the Enzyme Commission (EC) number. Consequently, the\nability to predict EC numbers could substantially reduce the number of un-\nannotated genes. Here we present a deep learning model, DeepECtransformer,\nwhich utilizes transformer layers as a neural network architecture to predict EC\nnumbers. Using the extensively studiedEscherichia coliK-12 MG1655 genome,\nDeepECtransformer predicted EC numbers for 464 un-annotated genes. We\nexperimentally validated the enzymatic activities predicted for three proteins\n(YgfF, YciO, and YjdM). Further examination of the neural network’sr e a s o n i n g\nprocess revealed that the trained neural network relies on functional motifs of\nenzymes to predict EC numbers. Thus, DeepECtransformer is a method that\nfacilitates the functional annotation of uncharacterized genes.\nEnzymes are proteins that catalyze various reactions in living\norganisms. Understanding the functions of enzymes is vital for\ncomprehending metabolic processes and characteristics. The Inter-\nnational Union of Biochemistry and Molecular Biology devised\nthe Enzyme Commission (EC) number system, which assigns enzyme\nfunctions using four hierarchical digits separated by periods (e.g.,\nEC:1.1.1.1 for alcohol dehydrogenase). EC numbers can facilitate\nthe annotation and classiﬁcation of enzymes in the growing number\nof genome sequences, playing a crucial role in understanding\nthe metabolism of organisms and enabling metabolic engineering\napplications.\nThe surge in biological sequence data, along with the advance-\nment of data-driven methodologies, particularly deep learning,\nhas facilitated the large-scale characterization of proteins\n1– 10.D e e p\nlearning has proven useful in analyzing biological sequences,\nincluding enzyme functions\n4,11,12, turnover numbers13– 15, and Michaelis\nconstants16. Although deep learning models have been criticized for\nbeing “black boxes,” recent studies have used various methods, such\nas integrated gradients, to interpret the reasoning process of artiﬁ-\ncial intelligence (AI)17– 22. For instance, we used the integrated gra-\ndients method to interpret the reasoning process of DeepTFactor, a\ndeep learning tool for transcription factor prediction, demonstrating\nthat AI comprehends DNA-binding domains of transcription factors\nwithout having been trained with such information\n20. Interpreting\ndeep learning models is expected to deepen our understanding of\nthe models’ processes and unveil unknown biological features.\nReceived: 28 July 2023\nAccepted: 3 November 2023\nCheck for updates\n1Metabolic and Biomolecular Engineering National Research Laboratory, Department of Chemical and Biomolecular Engineering (BK21 four), Korea Advanced\nInstitute of Science and Technology (KAIST), Daejeon 34141, Republic of Korea.2Systems Metabolic Engineering and Systems Healthcare Cross-Generation\nCollaborative Laboratory, Department of Chemical and Biomolecular Engineering (BK21 four), KAIST, Daejeon 34141, Republic of Korea.3KAIST Institute for the\nBioCentury and KAIST Institute for Artiﬁcial Intelligence, KAIST, Daejeon 34141, Republic of Korea.4Division of Biological Sciences, University of California San\nDiego, La Jolla, CA 92093, USA.5Department of Bioengineering, University of California San Diego, La Jolla, CA 92093, USA.6Bioinformatics and Systems\nBiology Program, University of California San Diego, La Jolla, CA 92093, USA.7Novo Nordisk Foundation Center for Biosustainability, 2800 Kongens\nLyngby, Denmark.8BioProcess Engineering Research Center and BioInformatics Research Center, KAIST, Daejeon 34141, Republic of Korea.\ne-mail: leesy@kaist.ac.kr\nNature Communications|         (2023) 14:7370 1\n1234567890():,;\n1234567890():,;\nVarious deep learning models for the prediction of EC numbers\nhave also been developed. For instance, HDMLF was developed by\nintegrating multiple sequence alignment with a deep neural network\nleveraging learned representations from a protein language model and\nbidirectional gated recurrent units\n23. CLEAN, another deep learning\nmodel addressed imbalances in EC number distribution within the\ntraining dataset by employing contrastive learning, leading to predic-\ntion performance superior to the previous models\n24.H o w e v e r ,t h e s e\nmodels did not provide insights into the interpretability of AI reason-\ning. ProteInfer used a deep dilated convolutional network for EC\nnumber prediction and also provided interpretation of the prediction\nby class activation mapping\n25. Nevertheless, the class activation map-\nping yielded coarse-grained feature maps, lackingﬁne-grained details,\nwhich are important for the residue-level analysis of protein sequen-\nces. We previously developed DeepEC, a deep learning-based com-\nputational framework for EC number prediction that uses only the\namino acid sequences of proteins as input\n4 (Supplementary Fig. 10). In\nthis study, we present the development of DeepECtransformer, which\nemploys transformer layers as the neural network architecture to\neffectively predict EC numbers, covering 5360 EC numbers and\nincluding the EC:7 class (translocase) that was previously not covered\nin DeepEC. Also, the improved performance of DeepECtransformer\nhas suggested a list of entries that need careful inspection of whether\nthey have mis-annotated EC numbers in the UniProt Knowledgebase\n(UniProtKB).\nBy analyzing the regions of focus during the prediction of enzyme\nfunctions by transformer layers, we have conﬁrmed that DeepEC-\ntransformer has learned to identify important regions, such as active\nsites or cofactor binding sites. To unveil the functions of y-ome\n(unknown) proteins in the model organism Escherichia coli K-12\nMG1655, we employed DeepECtransformer to predict EC numbers for\n464 proteins out of 1569 y-ome proteins. Out of the 464 proteins, the\nfunctions of three predicted enzymes (YgfF, YciO, and YjdM) were\nvalidated through in vitro enzyme activity assays. This demonstrates\nthe capability of DeepECtransformer not only to quickly annotate\nenzyme functions from increasing amounts of DNA sequences but also\nto discover metabolic functions of proteins that were previously\nunknown.\nResults\nDevelopment and evaluation of DeepECtransformer\nDeepECtransformer incorporates two prediction engines: a neural\nnetwork and a homologous search. The neural network uses a trans-\nformer architecture to predict EC numbers by extracting latent fea-\ntures from the amino acid sequences of enzymes (Fig.1a)\n8,26.T h e\nneural network was trained on a uniprot dataset consisting of the\namino acid sequences of 22 million enzymes from UniProtKB/TrEMBL\nentries, covering 2802 EC numbers with all four digits (see\n“Methods”)\n27. If the neural network predicts no EC numbers for a given\namino acid sequence, homologous enzymes for the amino acid\nsequence are analyzed using UniProtKB/Swiss-Prot enzymes as the\nsubject database and EC numbers of the homologous enzymes are\nassigned\n4,28. Including EC numbers that can be predicted by neural\nnetwork and homology search, DeepECtransformer covers a total of\n5360 EC numbers (Supplementary Fig. 11).\nThe performance of the neural network was evaluated on a\nseparate test dataset that was not used during training. The perfor-\nmance results varied depending on the EC number class, with preci-\nsion ranging from 0.7589 to 0.9506, recall ranging from 0.6830 to\n0.9445, andF\n1 score ranging from 0.6990 to 0.9469 (Fig.1b). The\nevaluation metrics (precision, recall, andF1 score) of the neural net-\nwork were lowest for the EC:1 class (oxidoreductases), which made up\n13.4% of enzymes (i.e., 313,328 sequences) and made up 25.7% of EC\nnumbers (i.e., 720 EC numbers) in the uniprot dataset (Fig.1c, d). The\nlow performance for EC:1 class resulted from the inherent dataset\nimbalance, as the EC:1 class exhibited the lowest average number of\nsequences per EC number, with an average of 4352 sequences com-\npared to the other EC number classes (ranging from 6819 sequences\nfor EC:3 to 16,525 sequences for EC:6). Additionally, a statistical ana-\nlysis of the data distribution conﬁrmed that EC numbers belonging to\nEC:1 class generally had fewer sequences compared to other EC\nnumber classes (one-way ANOVA test,p value < 7.2473e– 15) (Supple-\nmentary Fig. 1). Hence, the neural network showed a bias towards\nvarying performances across different EC numbers. Furthermore, a\npositive correlation was observed between theF\n1 score and the num-\nber of sequences per EC number (Spearman coefﬁcient of 0.6872,\np < 0.001,n = 2802; Fig.1e).\nThe performance of DeepECtransformer was evaluated by com-\nparing it with two baseline methods: DeepEC and a homology-based\nsearch tool, DIAMOND4,28. For the comparison, the test dataset that has\nbeen used for the evaluation of DeepECtransformer neural network was\ncurated to consist of the amino acidsequences of 2,013,612 enzymes,\nfor which EC numbers can be predicted by all three tools. DeepEC-\ntransformer showed superior performance in terms of precision, recall,\nand F\n1 score, with the exception of micro precision, which was slightly\nlower than those of DIAMOND and DeepEC (Table1 and Supplementary\nFig. 2). Moreover, DeepECtransformer demonstrated an improved\nability to predict EC numbers for enzymes that have low sequence\nidentities to those in the trainingdataset (Supplementary Fig. 3).\nThe accuracy of DeepECtransformer was further demonstrated by\nits ability to correct mis-annotated EC numbers in UniProtKB. An\nexample is the enzyme P93052 fromBotryococcus braunii,w h i c hw a s\noriginally annotated as an\nL-lactate dehydrogenase (EC:1.1.1.27)29.H o w -\never, DeepECtransformer predicted it as a malate dehydrogenase\n(EC:1.1.1.37). We performed heterologous expression experiments\n(Supplementary Fig. 12 and Supplementary Notes 1 and 4), which con-\nﬁrmed that P93052 is a malate dehydrogenase (EC:1.1.1.37). Similarly,\nDeepECtransformer correctly predicted EC numbers for Q8U4R3 from\nPyrococcus furiosusand Q038Z3 fromLacticaseibacillus paracaseias\nD-cysteine desulfhydrase (EC:4.4.1.15) and dihydroorotate dehy-\ndrogenase (NAD) (EC:1.3.1.14), respectively, which were misannotated as\n1-aminocyclopropane-1-carboxylate deaminase (EC:3.5.99.7) and dihy-\ndroorotate dehydrogenase (fumarate) (EC:1.3.98.1). DeepECtransformer\nmade predictions for 26,140 proteins with EC numbers that differed\nfrom those in Swiss-Prot (Supplementary Data 1). Among them, 2062\nproteins were predicted to have additional EC numbers. For example,\nQ9WVK7 was annotated as EC:1.1.1.35, NAD-dependent 3-hydroxyacyl-\nCoA dehydrogenase in Swiss-Prot, while DeepECtransformer predicted\nit as EC:1.1.1.157, NADP-dependent 3-hydroxybutyryl-CoA dehy-\ndrogenase, in addition to EC:1.1.1.35. Additionally, DeepECtransformer\ncould ﬁll in the incomplete EC numbers of 6454 proteins. It added the\nfourth digit to 5012 proteins, the third and fourth digits to 1019 proteins,\nand the second, third, and fourth digits to 423 proteins. For example,\nDeepECtransformer predicted C9K7D8as EC:2.6.1.42, a branched-chain\namino acid transaminase, which was previously annotated as EC:2.6.1, a\ntransaminase. As the EC numbers suggested by DeepECtransformer are\npredictions, we further analyzed how AI made the predictions in silico\n(Supplementary Note 2 and Supplementary Figs. 14– 17). Even though in\nsilico analysis of the predictions can provide clues for the predicted\nfunctionality, it is necessary to experimentally validate their functions.\nHowever, given the rapidly increasing numbers of genomes and meta-\ngenomes, it is not feasible to experimentally validate the functions of all\nunknown proteins. The predictions made by DeepECtransformer can\nprovide candidate entries for further review, contributing to the con-\nstruction of a more robust knowledgebase.\nAI learns the functional regions of enzymes\nDeepECtransformer can classify enzymes based on their EC numbers\nby utilizing inherentﬁlters that extract latent features from the amino\nacid sequences of enzymes. To understand the classi ﬁcation\nArticle https://doi.org/10.1038/s41467-023-43216-z\nNature Communications|         (2023) 14:7370 2\nmechanism, we projected the latent vectors of the amino acid\nsequences into a two-dimensional space using TMAP (Supplementary\nData 2)30. Although enzymes with identical EC numbers tend to cluster\ntogether, this pattern is less prominent at higher levels of EC classiﬁ-\ncation (e.g., EC numbers with second digit). This suggests that the\nneural network may not consider the common features of higher-level\nEC classes during the classiﬁcation process. To examine the speciﬁc\nfeatures learned by the network, we analyzed the attention scores\ncomputed in the self-attention layers (see“Methods”). For instance,\nDeepECtransformer uses the active site and NAD binding residues to\npredict the EC number of NAD-dependent malate dehydrogenase\n(EC:1.1.1.37) inE. coliK-12 (Fig.2a and Supplementary Fig. 4). Similarly,\nEC:1\nEC:2\nEC:3\nEC:4\nEC:5\nEC:6\nEC:7\n25.7%\n29.55%\n23.16%\n10.1%\n5.32%\n4.6%\n1.6%\nEC:1\nEC:2\nEC:3\nEC:4\nEC:5\nEC:6\nEC:7\n13.4%\n35.5%\n18.9%\n10.0%\n6.1%\n9.1%\n7.0%\na.\nb.\nc. d.\nRecall\nEC:1 EC:2 EC:3 EC:4 EC:5 EC:6 EC:7\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nF1 score\nEC:1 EC:2 EC:3 EC:4 EC:5 EC:6 EC:7\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPrecision\nEC:1 EC:2 EC:3 EC:4 EC:5 EC:6 EC:7\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ne.\nEC:1 EC:2 EC:3 EC:4\nEC:5 EC:6 EC:7 Total\n2\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n46 2 46 2 46 2 46\nNumber of sequences (log)\nF1 score F 1 score\nInput enzyme sequence\nPositional\nEncoding\nOutput EC number\nAdd & Normalize\nConvolutional\nMulti-head attention\nFeed forward\nConvolutional\nLinear\nEncoder\nAdd & Normalize\nInput embedding\nX2\nMSEEQAKPKAWET\nMSE E\nFig. 1 | The network architecture of DeepECtransformer and the prediction\nperformance of the neural network. aNetwork architecture of DeepEC-\ntransformer. DeepECtransformer employs the BERT architecture adopted from\nProtTrans\n26,43. The network consists of two transformer encoders, two convolu-\ntional layers, and a linear layer. Taking the amino acid sequence of an enzyme, the\nneural network predicts the EC numbers of the enzyme.b The prediction perfor-\nmance of the neural network for the test dataset. The boxplots illustrate the varying\nperformance of the neural network by theﬁrst-level EC numbers. Each data point in\nthe boxplot represents the performance of the neural network for a single EC\nnumber. The precision, recall, andF\n1 score differ for EC number classes. Center line,\nbox limits, whiskers, and points of the box-plots represent median, upper and lower\nquartiles, 1.5× interquartile range, and outliers, respectively. The sample sizes of the\nboxplots for EC:1, EC:2, EC:3, EC:4, EC:5, EC:6, and EC:7 are 720, 828, 649, 283, 149,\n129, and 44, respectively.c Distribution of the amino acid sequences of enzymes\nper ﬁrst-level EC number.d Distribution of types of EC numbers perﬁrst-level EC\nnumber. The amino acid sequences of enzymes in the uniprot dataset were used to\nanalyze the distributions.e The prediction performance of the neural network by\nthe number of the amino acid sequences of enzymes in the uniprot dataset. Source\ndata are provided as a Source dataﬁle.\nArticle https://doi.org/10.1038/s41467-023-43216-z\nNature Communications|         (2023) 14:7370 3\nin the prediction of the EC number of NADP-dependent malate dehy-\ndrogenase (EC:1.1.1.82) in Flaveria bidentis, DeepECtransformer\nassigned high attention scores to the active site and NADP binding\nresidues (Fig.2b and Supplementary Fig. 5). These results suggest that\nDeepECtransformer can identify the critical motifs in the amino acid\nsequences of enzymes without requiring any prior knowledge. To\nsystematically examine the functional motifs utilized by DeepEC-\ntransformer, the common motifs that contain residues with high\nattention scores in the self-attention layer were extracted. Attention\nscores were computed for the residues in the amino acid sequences of\nenzymes in Swiss-Prot for each EC number. The motif with the highest\nattention score in each attention head was extracted and clustered to\nidentify the common motifs. Multiple sequence alignments were\nconducted for each cluster, which revealed up toﬁve common motifs\nfor each EC number that were assigned high scores by DeepEC-\ntransformer. These common motifs represent functionally important\nresidues, such as active sites or substrate binding sites. For example,\nthe common motifs for NAD-dependent malate dehydrogenase\n(EC:1.1.1.37) were identiﬁed to comprise two Pfam domains (PF00056\nand PF02866) that correspond to the NAD binding domain andɑ/β\nC-terminal domain of malate dehydrogenase, respectively (Fig.2c).\nSimilarly, the common motifs for pyruvate kinase (EC:2.7.1.40) include\na Pfam domain PF00224, which corresponds to the pyruvate kinase\nbarrel domain. Additionally, the common motifs for fumarase\n(EC:4.2.1.2) contain a Pfam domain PF10415, which corresponds to the\nfumarase C C-terminus. To provide functional units for EC numbers\nthat can be predicted by the DeepECtransformer neural network, we\nextracted commonly highlighted motifs for 2547 EC numbers from the\namino acid sequences of enzymes in Swiss-Prot (Supplementary\nData 3). These motifs are expected to be utilized in future studies to\nuncover unknown shared characteristics among proteins with corre-\nsponding EC numbers.\nAnalysis of the metabolic function of alleles forE. colistrains\nTo evaluate the ability of DeepECtransformer in predicting changes in\nmetabolic functions across different strains, DeepECtransformer and\nDIAMOND were employed to predict the EC numbers of 312,274 pro-\nteins encoded by 3967 genes in 1122E. colistrains collected from the\nNCBI Genome database (Supplementary Data 4). The metabolic func-\ntions of the enzymes encoded by different alleles may vary although\nthey are annotated as the same gene. Out of the 312,274 proteins,\n238,575 proteins exhibited identical predictions from both DeepEC-\ntransformer and DIAMOND. For 2732 genes, representing 68.87% of\nthe total, at least 90% of the alleles showed identical predictions\nbetween DeepECtransformer and DIAMOND (Fig.3a). Among the\n73,669 alleles with non-identical predictions, 41,372 were newly pre-\ndicted as enzymes, 1270 were predicted to lose their metabolic func-\ntions, and 31,057 were predicted to undergo changes in their metabolic\nTable 1 | Performance of EC number prediction tools for the test dataset\nTool Number of predicted sequences Macro precision Macro recall Macro F1 score Micro precision Micro recall Micro F1 score\nDIAMOND 1,300,039 0.8168 0.4590 0.5390 0.9813 0.5867 0.7343\nDeepEC 1,272,079 0.8357 0.3931 0.4802 0.9727 0.6062 0.7469\nDeepECtransformer 1,952,172 0.8537 0.7942 0.8093 0.9709 0.9516 0.9611\na. b. c.\n06 0 Sequence position0.000\n0.025\n170 230 Sequence position0.000\n0.010\nG7\nA9\nG10\nG11\nI12\nH177M227\n245 305 Sequence position0.000\n0.008\n100 160 Sequence position0.000\n0.010\nH293\nA119\nM121\nProbability\n1.0\n0.5\n0.0\n05 1 5 10\n05 1 5 10\n05 1 5 10 20\n05 1 5 10 20\n05 1 5 10 20\nProbability\n1.0\n0.5\n0.0 Probability\n1.0\n0.5\n0.0 Probability\n1.0\n0.5\n0.0 Probability\n1.0\n0.5\n0.0\nFig. 2 | Highlighted amino acid residues by the DeepECtransformer neural\nnetwork. aDeepECtransformer pays attention to the NAD binding residues and the\nactive site to predict the EC number of NAD-dependent malate dehydrogenase ofE.\ncoli K-12 (Protein Data Bank [PDB] ID code1IB6). b DeepECtransformer pays\nattention to the NADP binding residues and the active site to predict the EC number\nof NADP-dependent malate dehydrogenase ofF. bidentis(PDB ID code1CIV). The\nwhole attention scores of the neural network prediction for the enzymes are\navailable in Supplementary. 4 and Supplementary Fig. 5.c Commonly highlighted\nmotifs for prediction of NAD-dependent malate dehydrogenase by DeepEC-\ntransformer neural network.\nArticle https://doi.org/10.1038/s41467-023-43216-z\nNature Communications|         (2023) 14:7370 4\nfunctions (Fig.3b). Through the analysis of these non-identically pre-\ndicted alleles, we were able to analyze how mutations in the alleles\naffected their metabolic functions.\nFor instance, among the 42 alleles of thearoL gene that encodes\nshikimate kinase II (EC:2.7.1.71), three alleles (aroL_3, aroL_33, and\naroL_34) were predicted to have an additional metabolic function of\n7-α-hydroxysteroid dehydrogenase (EC:1.1.1.159). In the phylogenetic\nanalysis ofaroL alleles, theses alleles (aroL_3, aroL_33, andaroL_34)\nformed a distinct clade in the phylogenetic tree, suggesting a diver-\ngent evolutionary trajectory compared to other alleles (Fig.3c, d).\nThe strains possessing these three alleles (i.e.,E. coli KTE11, KTE31,\nKTE33, KTE96, KTE114, and KTE159) exhibit a common feature of\ncarrying the allele hdhA_61, which encodes 7- α-hydroxysteroid\ndehydrogenase. In a recent study, these six strains were also identi-\nﬁed as belonging to the same phylogroup ofE. colistrains\n31. Further\ninvestigation into the coevolutionary relationship betweenhdhA_61\nand the three aroL alleles may provide insights into how theses\nstrains have adapted to their environment. There are other examples\nof changes in metabolic functions found among the alleles within\nthe distinct clades of phylogenetic trees. For instance, the lsrF\ngene (encoding 3-hydroxy-5-phosphooxypentane-2,4-dione thiolase;\nEC:2.3.1.245) has six alleles predicted to have an additional metabolic\nfunction of fructose-bisphosphate aldolase (EC:4.1.2.13), whilettdA\n(encoding L-tartrate dehydratase; EC:4.2.1.32) has three alleles pre-\ndicted to have an additional metabolic function of fumarase\n(EC:4.2.1.2) (Supplementary Fig. 6). Such observations can provide\nvaluable insights into the evolutionary trajectories of strains from a\nmetabolic perspective. In summary, theseﬁndings demonstrate that\nDeepECtransformer can detect changes in metabolic functions\nresulting from only a few mutations, which are not easily identiﬁable\nthrough homologous searches.\nDiscovering the unknown functions of enzymes inE. coli\nK-12 MG1655\nAs discussed earlier, DeepECtransformer has shown the capability to\npredict enzyme functions for proteins with low sequence identities to\nenzymes seen by the model during the training. Therefore, our next\nobjective was to employ DeepECtransformer to uncover unknown\nmetabolic functions of enzymes.E. coli K-12 MG1655, an extensively\nstudied model organism, still has approximately 30% of genes that\nremain incompletely characterized. Utilizing DeepECtransformer, we\nconducted an analysis of the EC numbers associated withE. coli’sy - o m e ,\nwhich comprises genes inE. coliK-12 MG1655 with insufﬁcient experi-\nmental evidence regarding their functions. Out of the 1600 genes in the\ny-ome, protein sequences for 1569 were retrievable from the UniProt\ndatabase. DeepECtransformer successfully predicted EC numbers for\n464 proteins, with 390 of them having complete four-digit EC numbers\n(Fig. 4a and Supplementary Data 5). In comparison, our previous algo-\nrithm DeepEC predicted EC numbers for 82 proteins, of which 71 were\nprojected to possess complete four-digit EC numbers, while the UniProt\ndatabase provided annotations for 71 of these proteins (Supplementary\nFig. 7). DeepECtransformer exclusively predicted complete four-digit\nEC numbers for 295 proteins.\nTo validate the ability of DeepECtransformer in identifying meta-\nbolic functions that cannot be detected by DeepEC and Swiss-Prot\nfunctional annotation processes, we performed in vitro enzyme\nassays to validate the predicted enzyme functions. Weﬁrst selected\nthree representative EC number classes, namely oxidoreductase\n(EC:1), transferase (EC:2), and hydrolase (EC:3) to show the capability\nof DeepECtransformer to predi ct unknown enzyme functions.\nAmong the 295 proteins that are exclusively predicted by DeepEC-\ntransformer to have all four digits of EC numbers, 179 proteins are\npredicted to be soluble inE. coliby NetSolP, a deep learning model for\na. c.\nb.\nd.\nGene ID\nIdentically predicted allele\nfrequency\n0.0\n1.0\n2,732 genes\nGenes\n(3,967)\nGenes\n(3,967)\nAlleles\n(312,274)\nIA\n(238,575) EA\n(115,811) EG\n(1,595)\nNG\n(2,273)\nNEG\n(1,526)\nDFG\n(746)\nNNG\n(185)\nNA\n(122,764)\nNEA\n(41,372)\nDFA\n(31,057)\nNNA\n(1,270)\nNIA\n(73,669)\naroL 8\naroL 9\naroL 14\naroL 10\naroL 11\naroL 35\naroL 23\naroL 4\n1\naroL 13\naro\nL 5\naro\nL 37\naroL \n6\nar\noL 42\naroL\n 4\naroL 1aroL\n 2\naroL \n36\naroL \n12\naroL 38\naroL 39\naroL 19\naroL 18\naroL 24\naroL 29\naroL 26\naroL 20\naroL 21\naroL 22\naroL 28\naro\nL 25aroL 7aroL\n 15\naroL 17aroL \n30\naroL 31aroL \n32\naroL 16\naroL 2\n7\naro\nL 40aroL 33\naroL 34\naroL 3\nS86A\nD135N T133N\nQ73R\nF71Y\nFig. 3 | EC number prediction results for 312,274 alleles in 1,122E. colistrains.\na Frequency of alleles identically predicted by DeepECtransformer and DIAMOND.\nb Sankey diagram of the number of genes and alleles by the EC number prediction.\nNumbers in the parentheses indicate the number of each item. Abbreviations are as\nfollows: IA identically predicted alleles, NIA non-identically predicted alleles, EA\nalleles for enzymes, NA alleles for non-enzymes, NEA alleles newly predicted as\nenzymes, DFA alleles predicted to differ in functions, NNA alleles newly predicted\nas non-enzymes, EG genes for enzymes, NG genes for non-enzymes, NEG genes for\nnewly predicted as enzymes, DFG gene for predicted to differ in functions, NNG\ngene for newly predicted as non-enzymes.c The phylogenetic tree ofaroL alleles in\n1122 E. colistrains. Three alleles (aroL_3, aroL_33, andaroL_34) are located in a\ndistinct clade.d The three-dimensional structure of AroL and common mutations\nin aroL_3, aroL_33, andaroL_34 (AlphaFoldDB ID code: P0A6E1). Source data are\nprovided as a Source dataﬁle.\nArticle https://doi.org/10.1038/s41467-023-43216-z\nNature Communications|         (2023) 14:7370 5\nprotein solubility prediction32. From the 179 proteins, we randomly\nselected three proteins, YgfF, YciO, and YdjM, that are predicted to be\noxidoreductase, transferase, and hydrolase, respectively. For YgfF,\nDeepECtransformer predicted its EC number to be EC:1.1.1.47 (glucose\n1-dehydrogenase). The enzyme assay results showed that YgfF exhib-\nited a speciﬁc glucose 1-dehydrogenase activity of 305.55 U mg\n−1 (Fig.4b\nand Supplementary Table 1), which was comparable to the previously\nreported value of 205.70 U mg\n−1 for the glucose 1-dehydrogenase from\nLysinibacillus sphaericusG1033.I nt h ec a s eo fY c i O ,w h i c hw a sp r e v i o u s l y\nannotated to belong to the SUA5 family, DeepECtransformer predicted\nits EC number to be EC:2.7.7.87 (L-threonylcarbamoyladenylate syn-\nthase) with the prediction score of 0.9108. The speciﬁc activity of YciO\nwas measured to be 0.0705 U mg\n−1 (Fig.4b and Supplementary Table 1),\nwhich was higher than the previously reported speciﬁc activity range of\nL-threonylcarbamoyladenylate synthase (0.00556 – 0.01112 U mg−1)\nfrom Bacillus subtilis34. Lastly, YjdM was predicted by DeepEC-\ntransformer to have the EC numberEC:3.11.1.2 (phosphonoacetate\nhydrolase). The speciﬁc phosphonoacetate hydrolase activity of YjdM\nobtained by enzyme assay was 139.85 U mg−1 (Fig. 4b and Supplemen-\ntary Table 1), which exceeded the speciﬁc activity of phosphonoacetate\nhydrolase (60 U mg−1)f r o mPseudomonas ﬂuorescens 23F35.W ea l s o\nanalyzed whether the EC number predictions for these three enzymes\nwere made by the neural network or bythe homology search. First, YgfF\nwas predicted to have an EC number of EC:1.1.1.47 by the neural network\nwith a prediction score of 0.6331. It should be noted that although\nYgfF exhibited a higher sequenceidentity with a different enzyme\n(A0A069CGU9_ECOLX; EC:1.1.1.100) from the training dataset than\nglucose 1-dehydrogenase exhibiting the maximum sequence identity\nwithin the training dataset, the neural network made an accurate\nprediction. Likewise, for YjdM, predicted by the neural network\nas EC:3.11.1.2 with a prediction scoreo f0 . 6 1 0 3 ,t h et r a i n i n gs e q u e n c e\nwith the highest sequence similarity had a different EC number\n(C9Y1B8_CROTZ; EC:2.7.7.6). Lastl y ,t h ep r e d i c t e dE Cn u m b e rf o rY c i O\nby the neural network was EC:3.1.11.2 with a prediction score of 0.9108,\nand the training sequence with the highest sequence identity with YciO\nwas also a L-threonylcarbamoylade nylate synthase (EC:3.1.11.2).\nTo examine whether the neural network understands the functional\nregions of YciO rather than relying on the sequence identity, the motifs\nwith high attention scores were analyzed. It was found that the high-\nlighted motifs correspond to TIGR00057, a NCBIfam family for\nL-threonylcarbamoyladenylate synthase (Supplementary Fig. 13). These\nresults suggest that DeepECtransformer leverages not only homology\nsearch but employs latent features learned during the training process\nduring the prediction of EC numbers. Notably, DeepEC was unable to\nprovide predictions for the three proteins, likely due to its low recall\nassociated with predicting enzyme functions, stemming from a highly\nimbalanced dataset. Also, Swiss-Prot functional annotation failed to\nmake predictions for the three proteins, possibly due to the limited\nsequence identity shared between the y-ome proteins and protein sig-\nnatures available in existing databases. These results suggest that\nDeepECtransformer can effectively contribute to the discovery of\nmetabolic functions of enzymes that have yet to be fully characterized.\nDiscussion\nEC numbers are a four-digit code that describes the catalytic function\nof an enzyme. We developed DeepECtransformer, which combines\ndeep learning with transformer layers and homologous searches, to\npredict EC numbers. DeepECtransformer was found to outperform\nboth DeepEC and homologous search using DIAMOND in predicting\nfour-digit EC numbers. The neural network of DeepECtransformer was\ntrained using the amino acid sequences of 22 million enzymes covering\n2802 EC numbers with all four digits. Despite the large number of\nsequences in the dataset, the class imbalance made it challenging to\npredict EC numbers that had few sequences. To address this issue, we\ntrained DeepECtransformer using focal loss, which reduces the impact\nof class imbalance during training (see”Methods”)\n36.U s i n gw e i g h t e d\nloss functions37 or creating a more balanced dataset through data\naugmentation38 can further enhance the prediction performance of\nthe neural network.\nThere have been other attempts tocreate high-performance neural\nnetworks for biological sequences, such as splitting datasets based on\nsequence identities10,39, using learned protein representations39,a n d\nincorporating evolutionary information40,41. A recently developed EC\nnumber prediction tool, CLEAN, has used contrastive learning to\naddress class imbalances by aiming to differentiate data classes in the\nlearned latent space, resulting in improved performance compared to\nDeepECtransformer (Supplementary Note 3 and Supplementary\nTables 2– 4)\n24.H o w e v e r ,w h i l eC L E A Ns h o w e di m p r o v e dp e r f o r m a n c e\ncompared to DeepECtransformer, it was not able to predict the EC\nnumbers of YgfF and YjdM. Also, the use of CLEAN for the annotation of\nuncharacterized proteins requires careful interpretation of the predic-\ntion results because CLEAN assigns EC numbers for any input amino\nacid sequences, including non-enzyme amino acid sequences. CLEAN\nprovides the conﬁdence level (i.e., high, medium, low) of the predic-\ntions using a Gaussian mixture model. However, as the conﬁdence level\ndoes not provide a detailed interpretation of how AI performs the\nreasoning process, careful inspection of the predictions should be\nconducted for the analysis of uncharacterized proteins, especially when\nthe uncharacterized proteins contain non-enzyme proteins. While\nDeepECtransformer has an advantage in terms of interpretability by\nproviding attention weight-basedﬁne-grained details, it is important to\nEC:1\n(21.12%)\nEC:2\n(31.03%)\nEC:3\n(34.91%)\nEC:4\nEC:5 (5.6%)\nEC:6\nEC:7 (2.16%)\n(3.66%)\n(1.51%)\nCharacterized genes\n(2,648)\ny-ome\n(1,600)\nE. coli K-12\nMG1655\nTF (84)\nEnzymes (464)\na.\n150\n100\n50\n0\n200\n250\n300\n350\n0.06\n0.04\n0.02\n0\n0.08\n0.10\n120\n80\n40\n0\n160\n200b.\nSpecific activity (U mg-1)\nYgfF\n(EC:1.1.1.47)\nYciO\n(EC:2.7.7.87)\nYjdM\n(EC:3.11.1.2)\nOH\nOH\nHO\nHO\nOH\nO H\nH\nH\nH\nH\nOH\nOH\nHO\nHO\nO\nO H\nH\nH\nH\nH2N\nOH O\nOH\nH\nH\nOH\nN\nO\nO-\nO-\nH\nH\nAMP\nO\nO-\nO\nO--O\nOH\nO\nP\nNAD\nNADH + H+\nH2O\nPi + H+\nATP + HCO3\nPPi + H2O\nFig. 4 | EC number prediction results for proteins of theE. coliK−12 MG1655\ny-ome. aDistribution of predicted EC numbers of the y-ome proteins. The number\nof the transcription factor (TF) is predicted using DeepTFactor.b Speciﬁc activities\nof YfgF, YciO, and YjdM. Error bars, mean ± SD (n = 3 independent experiments).\nBlack circles represent each data point. Source data are provided as a Source\ndata ﬁle.\nArticle https://doi.org/10.1038/s41467-023-43216-z\nNature Communications|         (2023) 14:7370 6\nacknowledge that other EC number prediction tools address different\nfacets of the problem. For instance, CLEAN tackles class imbalance by\nutilizing contrastive learning rather than supervised learning. ProteInfer\nuses a deep dilated convolutional network, which is not constrained by\ninput amino acid sequence length and extends its predictions to include\nGene Ontology (GO) terms, thereby offering a richer information of\nprotein functionality. This highlights the importance of considering an\nensemble of prediction tools forcomprehensive enzyme function\ncharacterization. Further optimization of the neural network could lead\nto the development of an AI tool for more accurate enzyme function\ncharacterization.\nHere, we used DeepECtransformer to predict EC numbers for the\ny-ome of the well-studied E. coli K-12 MG1655 genome to reveal\nputative enzymatic functions for 464 proteins. We experimentally\nvalidated the predicted EC numbers for three enzymes, YgfF,\nYciO, and YjdM, among the 390 proteins with four-digit EC numbers\npredicted by DeepECtransformer. DeepECtransformer showed\nimproved performance compared to previous methods, such as\nsequence alignment, not only in predicting the correct EC numbers\nfor mis-annotated enzymes but also in predicting EC numbers for\nproteins with poor characterization. Moreover, DeepECtransformer\nwas employed to re-evaluate the EC numbers of 128,100,490 protein\nsequences in 70,600 genomes in the NCBI genome database (Sup-\nplementary Data 6), introducing the potential for characterizing\npreviously unknown enzyme functions. For example, this resource\nhas the potential to bridge the knowledge gap between biochemical\nreactions and their associated genes by identifying the enzymes\nresponsible for catalyzing these reactions. In the BRENDA database\n(v. 1.1.0), 2293 out of a total of 7753 EC numbers lack annotated\nprotein information\n42. DeepECtransformer generated 7,694,772\nputative protein sequences across 47,692 genomes for 271 of these\nEC numbers. These results provide a valuable resource for dis-\ncovering unknown metabolic functions by enriching the pool of\nsequences available for future analysis.\nIn addition to predicting protein characteristics from a primary\nsequence, it is also crucial to understand the sequence features that\nimpact enzyme function. Recent studies on biological sequences\nusing deep learning have attempted to interpret the inner workings\nof neural networks through various approaches\n17– 22. In this study, we\nconducted an analysis of the self-attention layers within the Dee-\npECtransformer neural network to identify the speciﬁc features that\nAI focuses on to classify enzyme functions. Our results showed that\nthe AI effectively detects functional regions such as active sites and\nligand interaction sites, in addition to well-known functional domains\nsuch as Pfam domains. These identiﬁed motifs have the potential\nto enhance our understanding of enzyme functions. Our analysis\nsuccessfully visualized commonly highlighted motifs consisting of\n16 amino acid residues, but it is possible that DeepECtransformer\nmay also be capable of detecting longer-range protein interactions.\nAs an example, the self-attention layer of DeepECtransformer iden-\ntiﬁed multiple ligand interaction residues spanning the entire\nsequence ofE. coli’s malate dehydrogenase (Supplementary Fig. 8).\nFurther application of alternative interpretation methods holds the\npromise of uncovering previously unknown yet critical features of\nenzymes\n17– 22.\nThe use of EC numbers as the standard classiﬁcation scheme for\nenzyme functionality has been well-established and continually\nupdated. However, the four-digit structure of EC numbers often\nintroduces ambiguity and makes it challenging to provide clear\ndescriptions of metabolic functions using data-driven classiﬁers. For\ninstance, a single metabolic reaction can be described by multiple\nEC numbers (e.g., malate dehydrogenase can be EC:1.1.1.37 and\nEC:1.1.1.375), and some EC numbers may represent general types of\nenzymes with ambiguous metabolic reactions (e.g., alcohol dehy-\ndrogenase, EC:1.1.1.1). Furthermore, the EC classiﬁcation scheme is\nconstrained by known substrates and reactions, posing difﬁculties in\nextrapolating classiﬁers for enzymes whose substrates are not clearly\nknown. While GO terms are widely used for protein functionality\nclassiﬁcation, they may not comprehensively cover all metabolic\nreactions. As of July 2023, out of the 8056 EC numbers with complete\nfour digits, only 5216 had corresponding GO terms (http://current.\ngeneontology.org/ontology/external2go/ec2go). Hence, establishing\na clear scheme for describing metabolic reactions could signiﬁcantly\nenhance our understanding of enzyme functionality.\nIn conclusion, we expect DeepECtransformer, as a tool for pre-\ndicting EC numbers, to become widely used in functional genomics. Its\ncapabilities allow for the analysis of metabolism at a systems level,\nfacilitating the construction of comprehensive genome-scale meta-\nbolic models, potentially minimizing gaps or missing information.\nMethods\nReagents\nAll oligonucleotides were purchased from Genotech (Daejeon, Korea),\nand gene sequencing was performed at Macrogen (Daejeon, Korea). All\nenzymes and reagents for DNA manipulation were purchased from\nNew England Biolabs (Berverly, MA, USA), TaKaRa Shuzo (Shiga,\nJapan), and Sigma-Aldrich (St. Louis, MO, USA). DNA fragments and\nplasmid DNA were puriﬁed using Qiagen kit (Qiagen, Chatsworth, CA,\nUSA). All chemicals used in this study were purchased from either\nSigma-Aldrich or Junsei Chemical (Tokyo, Japan).\nConstruction of plasmids and strains\nThe strains, plasmids, and oligonucleotides used in this study are listed\nin Supplementary Table 5. To construct pET22b(+)-ygfF, pET22b(+)-\nyciO, and pET22b(+)-yjdM, pET22b(+) was linearized usingAvaIa n d\nNdeI. ygfF, yciO,a n dyjdM gene fragments were prepared by poly-\nmerase chain reaction using the primers P3/P4, P5/6, and P7/8,\nrespectively, using genomic DNA ofE. coli MG1655 as a template.\nPrepared gene fragments were then ligated with linearized pET22b(+)\nusing Gibson assembly. Correct vector construction was veriﬁed using\nDNA sequencing. Plasmids pET22b(+)-ygfF, pET22b(+)-yciO, and\npET22b(+)-yjdM were then transformed toE. coli BL21 (DE3) strain\nresulting in BL21 (DE3) (pET22b(+)-ygfF), BL21 (DE3) (pET22b(+)-yciO),\nand BL21 (DE3) (pET22b(+)-yjdM) strains.\nPuriﬁcation of YgfF, YciO, and YjdM\nE. coli BL21 (DE3) strains harboring pET22b(+)-ygfF, pET22b(+)-yciO,\nand pET22b(+)-yjdM were cultured for the overexpression of\nC-terminus his-tagged YgfF, YciO, and YjdM in 500 ml of LB medium at\n37 °C. The expression of his-tagged YgfF, YciO, and YjdM were induced\nby adding 1 mM IPTG after 3 h of cultivation. Cells were collected after\nadditional 6 h of cultivation by centrifugation at 2090 ×g for 15 min.\nHarvested cells were suspended in 30 ml of equilibrium buffer that\nconsists of 50 mM NaH2PO4, 0.3 M NaC1, 10 mM imidazole (pH 7.5).\nThe cells were disrupted using ultrasonic homogenizer (VCX-600;\nSonics and Materials Inc., Newtown, CT) with a titanium probe 40 T\n(Sonics and Materials Inc.). Cell debris was separated by centrifugation\nat 15,044 ×g for 40 min, and the resulting supernatants were loaded\nonto Talon metal afﬁnity resin (Clontech, Mountain View, CA). Equili-\nbrium buffer supplemented with 10 mM of imidazole (5 ml) wasﬂown\nthrough the resin to elute his-tagged YgfF, YciO, and YjdM. Finally, the\nbuffer solution of the eluted protein was changed to their reaction\nbuffers by using Amicon Ultra-15 Centricon (Millipore, Beilerica, MA)\nwith a pore size of 10 kDa. The composition of each reaction buffer is:\nglucose dehydrogenase assay buffer (Sigma-Aldrich, St. Louis, MO,\nUSA) for YgfF, a mixture of 50 mM MOPS, 20 mM MgCl\n2,2 5m MK C l ,\nand 20 mM NaHCO3 for YciO and 50 mM Tris-HCl (pH 8.0) for YjdM.\nThe concentrations of the puriﬁed YgfF, YciO, and YjdM were mea-\nsured by the Bio-Rad Protein Assay Kit (Bio-Rad, Hercules, CA) using\nbovine serum albumin (BSA) as a standard.\nArticle https://doi.org/10.1038/s41467-023-43216-z\nNature Communications|         (2023) 14:7370 7\nIn vitro enzyme assay\nThe reaction mixture for YgfF is composed of 82μl of glucose dehy-\ndrogenase (GDH) assay buffer, 8μl of GDH developer, and 10μlo f2M\nglucose were used, and supplemented with 50μlo ft h ep u r iﬁed his-\ntagged YgfF. The enzyme reaction was carried out for 3 min at 37 °C\nand measured A0.A f t e r3 0 m i n ,A1 was measured using a spectro-\nphotometer at OD450. Enzyme activity was measured total GDH\namount using a GDH colorimetric kit (Cat# K786-100; BioVision, Mil-\npitas, CA). For absolute quantiﬁcation of the GDH concentration, a\nstandard curve was prepared according to the manufacturer’s proto-\ncol. The reaction mixture for YciO is composed of 2μl of 200 mM ATP,\n2 μl of 1 M L-threonine, 8μlo ft h ep u r iﬁed his-tagged YciO, and 188μl\nof reaction buffer mentioned above. The enzyme reaction was carried\nout for 20 min at 25 °C. Enzyme activity was determined by measuring\nthe total inorganic pyrophosphate (PP\ni) level using a PPi assay kit (Cat#\nMAK386, Sigma-Aldrich). Each well of the 96-well plate was read using\na spectrophotometer at OD570. For absolute quantiﬁcation of the PPi\nconcentration, a standard curve was prepared according to the man-\nufacturer’s protocol. The reaction mixture for YjdM is composed of\n94 μlo f5 0m MT r i s - H C l( p H8 . 0 ) ,2μl of 10 mM phosphonoacetic acid,\nand 4μlo ft h ep u r iﬁed his-tagged YjdM. The enzyme reaction was\ncarried out for 30 min at 35 °C. Enzyme activity was determined by\nmeasuring the total phosphate level using a phosphate assay kit (Cat#\nMAK308, Sigma-Aldrich). Each well of the 96-well plate was read using\na spectrophotometer at OD\n620. For absolute quantiﬁcation of the\nphosphate concentration, a standard curve was prepared according to\nthe manufacturer’s protocol. All the in vitro enzyme assays were con-\nducted in triplicates.\nDataset construction\nThe amino acid sequences of enzymes were retrieved from UniProt\nKnowledgebase (UniProtKB)/Swiss-Prot and TrEMBL entries\n27 which\nwere released in April 2018. Sequences are processed toﬁlter out (i)\nsequences without all four EC digits, (ii) sequences with non-standard\namino acids, (iii) sequences that are longer than 1000 amino acids\n(which only account for 3.56% of the dataset), (iv) redundant sequen-\nces, and (v) sequences of which EC number has less than 100 sequen-\nces in the dataset. The processed dataset, called uniprot dataset,\ncontains the amino acid sequences of 22,477,695 enzymes which cover\n2802 EC numbers. The dataset was randomly split into a training\ndataset, validation dataset, and test dataset by the ratio of 8:1:1. All of\nthe split datasets contain the whole EC number types (2802 EC num-\nbers). For the homologous enzyme search, we used the amino acid\nsequences of enzymes in Swiss-Prot entries which contain at least\none EC number. The processed dataset, called the swissprot dataset,\ncontains the amino acid sequences of 226,325 enzymes which cover\n5179 EC numbers including EC numbers without all four EC digits.\nTo compare the performance of EC number prediction tools, we used\nNEW-392 dataset, which contains 392 amino acid sequences covering\n177 types of EC numbers, and Price-149 dataset, which contains 149\namino acid sequences covering 56 types of EC numbers, provided by\nYu et al.\n24.\nNeural network architecture and training process\nDeepECtransformer employs a neural network to predict the EC\nnumbers of enzymes. The neural network uses the BERT architecture\nwhich encodes the input amino acid sequences of enzymes using self-\nattention layers (Fig.1a)\n26,43.I nt h i sw o r k ,w em o d iﬁed the pretrained\nProtBert model pretrained on UniRef1008. The neural network takes a\nprotein sequence which is tokenized by each amino acid. The tokens\nare embedded into vectors with 128 dimensions, which are fed into\ntwo subsequent self-attention encoders. Each self-attention encoder\ncontains a multi-head self-attention layer with eight heads. The\nnumber of hidden nodes of feed-forward layers in a multi-head self-\nattention layer is 128, and the number of hidden nodes of feed-\nforward layers in a self-attention intermediate layer is 256. The out-\nput of the BERT is processed by two convolutional layers with 128\nﬁlters of which sizes are (4, 128) and (4, 1), respectively for each layer.\nBatch normalization and rectiﬁed linear unit (ReLU) activation layers\nwere used after each convolutional layer. At the end of the network, a\nmax-pooling layer, a linear layer and a sigmoid layer were used to\nclassify the corresponding EC numbers of the embedded repre-\nsentation. The neural network was trained on the training dataset for\n30 epochs using an Adam optimizer with a learning rate of 0.001,\ndecaying with a factor of 0.95 for every epoch. The batch size was 512.\nFocal loss with a tunable focusing parameter (γ) 1.0 was used as a loss\nfunction\n36. For the inference, an EC number is assigned to the input\nsequence if the output score, calculated from the sigmoid layer,\nexceeds a threshold. In the case of promiscuous enzymes, all EC\nnumbers exceeding their respective thresholds were assigned as the\npredicted EC numbers. Using the validation dataset, the optimal\nthreshold for each EC number was searched that maximized theF\n1\nscore of the EC number. The evaluation metrics are calculated as\nfollows. Macro precision= 1\nC\nPC\ni =1\nTP i\nTP i + FP i\n,\nmacro recall= 1\nC\nPC\ni =1\nTP i\nTP i + FN i\n, macro F1 score = 1\nC\nPC\ni =1\n2/C15 Precision i/C15 Recalli\nPrecision i + Recalli\n,\nmicro precision=\nPC\ni =1\nTP i\nPC\ni =1\nðTP i + FP iÞ\n, micro recall=\nPC\ni =1\nTP i\nPC\ni =1\nðTP i + FN iÞ\n,\nmicro F1 score = 2/C15 micro precision/C15 micro recall\nmicro precision+ micro recall ,w h e r eTP i is the number of\ntrue positive prediction for classi, FP i is the number of false positive\nprediction for classi, FN i is the number of false negative prediction for\nclass i,a n dC is the number of classes. Source code for DeepEC-\ntransformer is available at https://github.com/kaistsystemsbiology/\nDeepProZyme.\nHomologous enzyme searches\nWe used DIAMOND v2.0.11 to search for homologous enzyme28.T h e\nminimum percent of sequence identity and coverage were optimized\nby searching 361 parameter sets (sequence identity and coverage in 5%\nsteps from 5% to 95%) (Supplementary Fig. 9)\n4. The swissprot dataset\nwas randomly divided into queries (113,162 sequences) and a reference\ndatabase (113,163 sequences) for the optimal parameter set searching.\nUsing the parameter sets, we aligned the queries on the reference\ndatabase to assign queries with homologous enzymes and the corre-\nsponding EC numbers. A parameter set with a minimum sequence\nidentity of 50% and a minimum sequence coverage of 75% showed the\nhighest microF\n1 score, 0.8739, was selected. The optimal parameter\nset was used for the DeepEC homologous search using the whole\nswissprot dataset as the reference database.\nVisualizing the latent space of the neural network\nLatent representations of the amino acid sequences of 217,123\nenzymes in the Swiss-Prot database were visualized using the TMAP\nalgorithm\n30 and Faerun library44,45. The embedded latent vectors\nbefore the last linear layer were used as the latent representations. To\nconstruct a tree map that compares the Swiss-Prot annotations and\nDeepECtransformer predictions, 179,655 enzyme entries that have at\nleast one predicted EC number by the neural network were used.\nAnalysis of attention layers\nLatent representations of the amino acid sequence of an enzyme\nafter self-attention layers were calculated using the DeepEC-\ntransformer neural network. Each attention head learns different\ninter-residue dependencies using self-attention\n26. The highlighted\nresidues of each attention head were analyzed using attention scores,\nwhich are the average attention values of each residue in the atten-\ntion map. The attention scores were visualized into sequence logos\nArticle https://doi.org/10.1038/s41467-023-43216-z\nNature Communications|         (2023) 14:7370 8\nusing Logomaker46. Because the former self-attention layer only\ncaptured inter-residue dependencies between proximal residues, the\naveraged values of residues in the amino acid sequence of an enzyme\ndid not show any speciﬁcally highlighted residues. Therefore, the\nhighlighted residues are analyzed using the latter self-attention layer\nin this study.\nCommon motifs used for each EC number were extracted using\nthe amino acid sequences of enzymes in Swiss-Prot entries having\nsequence lengths of 50– 1000 without any non-canonical amino acid.\nThe amino acid sequences of enzymes which have EC numbers that\ncannot be predicted by DeepECtransformer were also excluded. For\neach attention head in the second self-attention layer, a motif having\nthe length of 16 amino acid residues was extracted, of which a residue\nhaving the maximum attention score is centered. Cluster analysis was\nperformed for motifs of each EC number using MMseqs2\n47.T og e tt h e\nrepresentative common motifs for the clusters, multiple sequence\nalignment was performed for the top 5 clusters having the largest\nmotifs per EC number\n48.T h ei d e n t iﬁed representative common motifs\nare visualized using Logomaker46.\nProcessing 323,985 protein sequences of 1122E. colistrains\nThe pan-genome of 1122 publicly available strains ofE. coliwas con-\nstructed by clustering protein sequences based on their sequence\nhomology using the CD-hit package (v4.6)\n49. CD-hit clustering was\nperformed with 0.8 threshold for sequence identity and a word length\nof 5. Clusters by CD-hit were used as representative gene families and\nthe associated strain-speciﬁc sequences per gene family comprise the\nallele set studied.\nPhylogeny analysis of protein variants\nThe phylogeny ofaroL, lsrF,a n dttdA variants was analyzed using\nClustalW48 and the phylogenetic trees were visualized using iTOL50.\nProgram environment\nAll the development and analysis of DeepECtransformer were imple-\nmented using Python 3.6 under Ubuntu 16.04 LTS. The neural network\nof DeepECtransformer was trained and executed on NVDIA Tesla V100\nGPUs. The following Python modules were used in this study: biopy-\nthon v1.78, numpy v1.17.3, pandas v0.25.2, CD-hit package v4.6, Cluster\nOmega v1.2.3, DIAMOND v.2.0.11, faerun v0.3.20, logomaker v0.8,\nmatplotlib v3.2.2, MMseq2, pytorch v1.7.0, scikit-learn v0.21.3, tmap\nv1.0.4, and transformers v3.5.1.\nReporting summary\nFurther information on research design is available in the Nature\nPortfolio Reporting Summary linked to this article.\nData availability\nSupplementary Data 1– 7 are available at https://doi.org/10.5281/\nzenodo.10023678(ref. 51). Source data are provided with this paper\nand also available from Figshare:https://doi.org/10.6084/m9.ﬁgshare.\n23577036(ref.52). UniProtKB/Swiss-Prot and TrEMBL entries (released\nin April 2018) were used to construct the uniprot dataset. Protein 3D\nstructures used in this paper can be accessed with PDB ID1IB6 and\n1CIV. Source data are provided with this paper.\nCode availability\nThe program is available athttps://github.com/kaistsystemsbiology/\nDeepProZyme(ref. 53).\nReferences\n1 . A l m a g r oA r m e n t e r o s ,J .J . ,S o n d e r b y ,C .K . ,S o n d e r b y ,S .K . ,N i e l s e n ,\nH. & Winther, O. DeepLoc: prediction of protein subcellular locali-\nzation using deep learning.Bioinformatics33,3 3 8 7– 3395 (2017).\n2. Alley, E. C., Khimulya, G., Biswas, S., AlQuraishi, M. & Church, G. M.\nUniﬁed rational protein engineering with sequence-based deep\nrepresentation learning.Nat. Methods.16,1 3 1 5– 1322 (2019).\n3. Rao, R. et al. Evaluating protein transfer learning with TAPE.Adv.\nNeural Inf. Process. Syst. 32,9 6 8 9– 9701 (2019).\n4. Ryu, J. Y., Kim, H. U. & Lee, S. Y. Deep learning enables high-quality\nand high-throughput predictionof enzyme commission numbers.\nP r o c .N a t lA c a d .S c i .U S A116, 13996– 14001 (2019).\n5. Gainza, P. et al. Deciphering interactionﬁngerprints from protein\nmolecular surfaces using geometric deep learning.Nat. Methods.\n17,1 8 4– 192 (2020).\n6. Kulmanov, M. & Hoehndorf, R.DeepGOPlus: improved protein\nfunction prediction from sequence.Bioinformatics36,\n422– 429 (2020).\n7. Madani, A. et al. Large language models generate functional protein\nsequences across diverse families.Nat. Biotechnol. 41,1 0 9 9– 1106\n(2023).\n8. Elnaggar, A. et al. ProtTrans: towards cracking the language of life’s\ncode through self-supervised learning.IEEE Trans. Pattern Anal.\nMach. Intell. 44, 7112– 7127 (2022).\n9. Rives, A. et al. Biological structure and function emerge from\nscaling unsupervised learning to 250 million protein sequences.\nP r o c .N a t lA c a d .S c i .U S A118, e2016239118 (2021).\n10. Bileschi, M. L. et al. Using deep learning to annotate the protein\nuniverse.Nat. Biotechnol.40,9 3 2– 937 (2022).\n11. Watanabe, N. et al. Exploration and evaluation of machine learning-\nbased models for predicting enzymatic reactions.J. Chem. Inf.\nModel. 60,1 8 3 3– 1843 (2020).\n12. Vavricka, C. J. et al. Machine learning discovery of missing links that\nmediate alternative branches to plant alkaloids.Nat. Commun.13,\n1405 (2022).\n13. Li, F. et al. Deep learning-based kcat prediction enables improved\nenzyme-constrained model reconstruction.Nat. Catal.5,\n662– 672 (2022).\n14. Heckmann, D. et al. Kinetic proﬁling of metabolic specialists\ndemonstrates stability and consist e n c yo fi nv i v oe n z y m et u r n o v e r\nnumbers.P r o c .N a t lA c a d .S c i .U S A117,2 3 1 8 2– 23190 (2020).\n15. Heckmann, D. et al. Machine learning applied to enzyme turnover\nnumbers reveals protein structural correlates and improves meta-\nbolic models.Nat. Commun.9, 5252 (2018).\n1 6 . K r o l l ,A . ,E n g q v i s t ,M .K .M . ,H e c k m a n n ,D .&L e r c h e r ,M .J .D e e p\nlearning allows genome-scale prediction of Michaelis constants\nfrom structural features.PLoS Biol.19, e3001402 (2021).\n17. Zheng, A. et al. Deep neural networks identify sequence context\nfeatures predictive of transcription factor binding.Nat. Mach. Intell.\n3,1 7 2– 180 (2021).\n18. Koo, P. K. & Ploenzke, M. Improving representations of genomic\nsequence motifs in convolutionaln e t w o r k sw i t he x p o n e n t i a la c t i -\nvations.Nat. Mach. Intell.3,2 5 8– 266 (2021).\n19. Linder, J. et al. Interpreting neural networks for biological\nsequences by learning stochastic masks.Nat. Mach. Intell.4,\n41– 54 (2022).\n2 0 . K i m ,G .B . ,G a o ,Y . ,P a l s s o n ,B .O .&L e e ,S .Y .D e e p T F a c t o r :ad e e p\nlearning-based tool for the prediction of transcription factors.Proc.\nNatl Acad. Sci. USA118, e2021171118 (2021).\n21. Valeri, J. A. et al. Sequence-to-function deep learning frameworks\nfor engineered riboregulators.Nat. Commun.11, 5058 (2020).\n22. Taujale, R. et al. Mapping the glycosyltransferase fold landscape\nusing interpretable deep learning.Nat. Commun.12, 5656 (2021).\n23. Shi, Z. et al. Enzyme commission number prediction and bench-\nmarking with hierarchical dual-core multitask learning rramework.\nResearch6, 0153 (2023).\n24. Yu, T. et al. Enzyme function prediction using contrastive learning.\nScience 379,1 3 5 8– 1363 (2023).\nArticle https://doi.org/10.1038/s41467-023-43216-z\nNature Communications|         (2023) 14:7370 9\n25. Sanderson, T., Bileschi, M. L., Belanger, D. & Colwell, L. J. ProteInfer,\ndeep neural networks for protein functional inference.Elife 12,\ne80942 (2023).\n26. Vaswani, A. et al. Attention is all you need.Adv. Neural Inf. Process.\nSyst. 30,6 0 0 0– 6010 (2017).\n27. UniProt, C. UniProt: the universal protein knowledgebase in 2021.\nNucleic Acids Res.49,D 4 8 0– D489 (2021).\n28. Buchﬁnk, B., Reuter, K. & Drost, H. G. Sensitive protein alignments\nat tree-of-life scale using DIAMOND.Nat. Methods.18, 366– 368\n(2021).\n2 9 . L o u i s ,A . ,O l l i v i e r ,E . ,A u d e ,J .C .&R i s l e r ,J .L .M a s s i v es e q u e n c e\ncomparisons as a help in annotating genomic sequences.Genome\nRes. 11,1 2 9 6– 1303 (2001).\n30. Probst, D. & Reymond, J. L. Visualization of very large high-\ndimensional data sets as minimum spanning trees.J. Cheminform.\n12,1 2( 2 0 2 0 ) .\n31. Catoiu, E. A., Phaneuf, P., Monk, J. & Palsson, B. O. Whole-genome\nsequences from wild-type and laboratory-evolved strains deﬁne\nthe alleleome and establish its hallmarks.Proc. Natl Acad. Sci. USA\n120, e2218835120 (2023).\n32. Thumuluri, V. et al. NetSolP: predicting protein solubility in\nEscherichia coliusing language models.Bioinformatics38,\n941– 946 (2022).\n33. Ding, H. T. et al. Cloning and expression inE. coliof an organic\nsolvent-tolerant and alkali-resistant glucose 1-dehydrogenase from\nLysinibacillus sphaericusG10. Bioresour. Technol.102,\n1528– 1536 (2011).\n34. Lauhon, C. T. Mechanism of N6-threonylcarbamoyladenonsine\n(t\n6A) biosynthesis: isolation and characterization of the inter-\nmediate threonylcarbamoyl-AMP.Biochemistry 51,8 9 5 0– 8963\n(2012).\n35. Kulakova, A. N., Kulakov, L. A. & Quinn, J. P. Cloning of the phos-\nphonoacetate hydrolase gene fromPseudomonasﬂuorescens23F\nencoding a new type of carbon-phosphorus bond cleaving enzyme\nand its expression inEscherichia coliand Pseudomonas putida.\nGene 195,4 9– 53 (1997).\n36. Lin, T.-Y., Goyal, P., Girshick, R., He, K. & Dollár, P. InProc.\nIEEE international Conference on Computer Vision2980– 2988\n(IEEE, 2017).\n37. Cui, Y., Jia, M., Lin, T.-Y., Song, Y. & Belongie, S. Focal loss for dense\nobject detection. Class-balanced loss based on effective number of\nsamples. InProc. IEEE/CVF Conference on Computer Vision and\nPattern Recognition9268– 9277 (IEEE, 2019).\n38. Skinnider, M. A., Stacey, R. G., Wishart, D. S. & Foster, L. J. Chemical\nlanguage models enable navigation in sparsely populated chemical\nspace. Nat. Mach. Intell.3,7 5 9– 770 (2021).\n39. Unsal, S. et al. Learning functional properties of proteins with lan-\nguage models.Nat. Mach. Intell.4, 227– 245 (2022).\n40. Tunyasuvunakool, K. et al. Highly accurate protein structure pre-\ndiction for the human proteome.Nature 596,5 9 0– 596 (2021).\n41. Rao, R. M. et al. MSA Transformer.Proceedings of the 38th\nInternational Conference on Machine Learning139, 8844– 8856\n(2021).\n42. Chang, A. et al. BRENDA, the ELIXIR core data resource in 2021: new\ndevelopments and updates.Nucleic Acids Res.49, D498– D508\n(2021).\n43. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: pre-training\nof deep bidirectional transformers for language understanding.\nProceedings of naacL-HLT1,2( 2 0 1 9 ) .\n44. Probst, D. & Reymond, J. L. FUn: a framework for interactive visua-\nlizations of large, high-dimensional datasets on the web.Bioinfor-\nmatics 34,1 4 3 3– 1435 (2018).\n45. Schwaller, P., Hoover, B., Reymond, J. L., Strobelt, H. & Laino, T.\nExtraction of organic chemistry grammar from unsupervised\nlearning of chemical reactions.Sci. Adv.7, eabe4166 (2021).\n46. Tareen, A. & Kinney, J. B. Logomaker: beautiful sequence logos in\nPython. Bioinformatics36, 2272– 2274 (2020).\n47. Steinegger, M. & Soding, J. MMseqs2 enables sensitive protein\nsequence searching for the analysis of massive data sets.Nat.\nBiotechnol.35,1 0 2 6– 1028 (2017).\n48. Sievers, F. et al. Fast, scalable generation of high-quality protein\nmultiple sequence alignments using Clustal Omega.Mol. Syst. Biol.\n7, 539 (2011).\n49. Fu, L., Niu, B., Zhu, Z., Wu, S. & Li, W. CD-HIT: accelerated for\nclustering the next-generation sequencing data.Bioinformatics28,\n3150– 3152 (2012).\n50. Letunic, I. & Bork, P. Interactive Tree Of Life (iTOL) v5: an online tool\nfor phylogenetic tree display and annotation.Nucleic Acids Res.49,\nW293– W296 (2021).\n51. Kim, G. B. et al. Improved annotation of enzyme-enconding genes\nu s i n gd e e pl e a r n i n g .z e n o d ohttps://zenodo.org/records/\n10023678(2023).\n52. Kim, G. B. et al. Improved annotation of enzyme-enconding genes\nusing deep learning.ﬁgsharehttps://doi.org/10.6084/m9.ﬁgshare.\n23577036(2023).\n53. Kim, G. B. et al. Improved annotation of enzyme-enconding genes\nu s i n gd e e pl e a r n i n g .g i t h u bhttps://github.com/\nkaistsystemsbiology/DeepProZyme(2023).\nAcknowledgements\nThis work is supported by the Development of next-generation bior-\neﬁnery platform technologies for leading bio-based chemicals industry\nproject (2022M3J5A1056072) and by Development of platform tech-\nnologies of microbial cell factories for the next-generation bioreﬁneries\nproject (2022M3J5A1056117) from National Research Foundation sup-\nported by the Korean Ministry of Science and ICT.\nAuthor contributions\nS.Y.L. conceived and designed the experiments. J.Y.K. and J.A.L. per-\nformed the experiments. G.B.K., C.J.N., B.O.P. and S.Y.L. analyzed the\ndata. G.B.K. contributed analysis tools. G.B.K., J.Y.K., J.A.L., C.J.N., B.O.P.,\nand S.Y.L. wrote the paper.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41467-023-43216-z.\nCorrespondenceand requests for materials should be addressed to\nSang Yup Lee.\nPeer review informationNature Communicationsthanks Sriram Chan-\ndresekaran, Feiran Li, and the other, anonymous, reviewer for their\ncontribution to the peer review of this work. A peer reviewﬁle is avail-\nable.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jur-\nisdictional claims in published maps and institutional afﬁliations.\nArticle https://doi.org/10.1038/s41467-023-43216-z\nNature Communications|         (2023) 14:7370 10\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indicate if\nchanges were made. The images or other third party material in this\narticle are included in the article’s Creative Commons licence, unless\nindicated otherwise in a credit line to the material. If material is not\nincluded in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright\nholder. To view a copy of this licence, visithttp://creativecommons.org/\nlicenses/by/4.0/.\n© The Author(s) 2023\nArticle https://doi.org/10.1038/s41467-023-43216-z\nNature Communications|         (2023) 14:7370 11",
  "topic": "Annotation",
  "concepts": [
    {
      "name": "Annotation",
      "score": 0.7081307768821716
    },
    {
      "name": "Gene",
      "score": 0.5973950624465942
    },
    {
      "name": "Computational biology",
      "score": 0.523871123790741
    },
    {
      "name": "Encoding (memory)",
      "score": 0.48953065276145935
    },
    {
      "name": "Computer science",
      "score": 0.4492170810699463
    },
    {
      "name": "Biology",
      "score": 0.36727362871170044
    },
    {
      "name": "Genetics",
      "score": 0.32382261753082275
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2887643277645111
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I157485424",
      "name": "Korea Advanced Institute of Science and Technology",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I36258959",
      "name": "University of California, San Diego",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2801134892",
      "name": "Novo Nordisk Foundation",
      "country": "DK"
    }
  ]
}