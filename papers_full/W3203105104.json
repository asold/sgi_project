{
  "title": "MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer",
  "url": "https://openalex.org/W3203105104",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3133831624",
      "name": "Mehta, Sachin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4282742321",
      "name": "Rastegari, Mohammad",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2612445135",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3136416617",
    "https://openalex.org/W2531409750",
    "https://openalex.org/W2037227137",
    "https://openalex.org/W3146097248",
    "https://openalex.org/W3040573126",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W3137963805",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W2963163009",
    "https://openalex.org/W3146091044",
    "https://openalex.org/W2144794286",
    "https://openalex.org/W3122154272",
    "https://openalex.org/W3170227631",
    "https://openalex.org/W2937843571",
    "https://openalex.org/W3175544090",
    "https://openalex.org/W2963420686",
    "https://openalex.org/W3202053489",
    "https://openalex.org/W3206034005",
    "https://openalex.org/W2570343428",
    "https://openalex.org/W2982083293",
    "https://openalex.org/W3172509117",
    "https://openalex.org/W3171087525",
    "https://openalex.org/W3022419825",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2883780447",
    "https://openalex.org/W3204575409",
    "https://openalex.org/W2963918968",
    "https://openalex.org/W2630837129",
    "https://openalex.org/W1650736245",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W3106250896",
    "https://openalex.org/W2963399829",
    "https://openalex.org/W2949736877",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W3202715235",
    "https://openalex.org/W3110420020",
    "https://openalex.org/W2962772649",
    "https://openalex.org/W2962834855",
    "https://openalex.org/W3168124404",
    "https://openalex.org/W3136635488",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W2963840672",
    "https://openalex.org/W3203606893",
    "https://openalex.org/W2970389371",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W3139633126",
    "https://openalex.org/W2981413347",
    "https://openalex.org/W2998508940",
    "https://openalex.org/W2963263347",
    "https://openalex.org/W3156811085",
    "https://openalex.org/W2963418739",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3190492058",
    "https://openalex.org/W2194775991"
  ],
  "abstract": "Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision trans-formers (ViTs) have been adopted. Unlike CNNs, ViTs are heavy-weight. In this paper, we ask the following question: is it possible to combine the strengths of CNNs and ViTs to build a light-weight and low latency network for mobile vision tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose vision transformer for mobile devices. MobileViT presents a different perspective for the global processing of information with transformers, i.e., transformers as convolutions. Our results show that MobileViT significantly outperforms CNN- and ViT-based networks across different tasks and datasets. On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4% with about 6 million parameters, which is 3.2% and 6.2% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number of parameters. On the MS-COCO object detection task, MobileViT is 5.7% more accurate than MobileNetv3 for a similar number of parameters. Our source code is open-source and available at: https://github.com/apple/ml-cvnets",
  "full_text": "Published as a conference paper at ICLR 2022\nMOBILE VIT: L IGHT -WEIGHT , G ENERAL -PURPOSE ,\nAND MOBILE -FRIENDLY VISION TRANSFORMER\nSachin Mehta\nApple\nMohammad Rastegari\nApple\nABSTRACT\nLight-weight convolutional neural networks (CNNs) are the de-facto for mo-\nbile vision tasks. Their spatial inductive biases allow them to learn representa-\ntions with fewer parameters across different vision tasks. However, these net-\nworks are spatially local. To learn global representations, self-attention-based\nvision transformers (ViTs) have been adopted. Unlike CNNs, ViTs are heavy-\nweight. In this paper, we ask the following question: is it possible to com-\nbine the strengths of CNNs and ViTs to build a light-weight and low latency\nnetwork for mobile vision tasks? Towards this end, we introduce MobileViT, a\nlight-weight and general-purpose vision transformer for mobile devices. Mobile-\nViT presents a different perspective for the global processing of information with\ntransformers. Our results show that MobileViT signiﬁcantly outperforms CNN-\nand ViT-based networks across different tasks and datasets. On the ImageNet-1k\ndataset, MobileViT achieves top-1 accuracy of 78.4% with about 6 million pa-\nrameters, which is 3.2% and 6.2% more accurate than MobileNetv3 (CNN-based)\nand DeIT (ViT-based) for a similar number of parameters. On the MS-COCO\nobject detection task, MobileViT is 5.7% more accurate than MobileNetv3 for a\nsimilar number of parameters. Our source code is open-source and available at:\nhttps://github.com/apple/ml-cvnets.\n1 I NTRODUCTION\nSelf-attention-based models, especially vision transformers (ViTs; Figure 1a; Dosovitskiy et al.,\n2021), are an alternative to convolutional neural networks (CNNs) to learn visual representations.\nBrieﬂy, ViT divides an image into a sequence of non-overlapping patches and then learns inter-\npatch representations using multi-headed self-attention in transformers (Vaswani et al., 2017). The\ngeneral trend is to increase the number of parameters in ViT networks to improve the performance\n(e.g., Touvron et al., 2021a; Graham et al., 2021; Wu et al., 2021). However, these performance\nimprovements come at the cost of model size (network parameters) and latency. Many real-world\napplications (e.g., augmented reality and autonomous wheelchairs) require visual recognition tasks\n(e.g., object detection and semantic segmentation) to run on resource-constrained mobile devices in\na timely fashion. To be effective, ViT models for such tasks should be light-weight and fast. Even if\nthe model size of ViT models is reduced to match the resource constraints of mobile devices, their\nperformance is signiﬁcantly worse than light-weight CNNs. For instance, for a parameter budget\nof about 5-6 million, DeIT (Touvron et al., 2021a) is 3% less accurate than MobileNetv3 (Howard\net al., 2019). Therefore, the need to design light-weight ViT models is imperative.\nLight-weight CNNs have powered many mobile vision tasks. However, ViT-based networks are\nstill far from being used on such devices. Unlike light-weight CNNs that are easy to optimize and\nintegrate with task-speciﬁc networks, ViTs are heavy-weight (e.g., ViT-B/16 vs. MobileNetv3: 86\nvs. 7.5 million parameters), harder to optimize (Xiao et al., 2021), need extensive data augmen-\ntation and L2 regularization to prevent over-ﬁtting (Touvron et al., 2021a; Wang et al., 2021), and\nrequire expensive decoders for down-stream tasks, especially for dense prediction tasks. For in-\nstance, a ViT-based segmentation network (Ranftl et al., 2021) learns about 345 million parameters\nand achieves similar performance as the CNN-based network, DeepLabv3 (Chen et al., 2017), with\n59 million parameters. The need for more parameters in ViT-based models is likely because they\nlack image-speciﬁc inductive bias, which is inherent in CNNs (Xiao et al., 2021). To build robust\nand high-performing ViT models, hybrid approaches that combine convolutions and transformers\n1\narXiv:2110.02178v2  [cs.CV]  4 Mar 2022\nPublished as a conference paper at ICLR 2022\nN\n3P\nFlatten image patches\nLinearN\nd\n⊕\nPositional encoding\nTransformer\nL×\nLinear Logits\n(a) Standard visual transformer (ViT)\nMobileViT block\nLocalrepresentations\nTransformers as Convolutions(global representations)\nFusion\nW\nH\nC\nX\nConv-n×n↓Conv-1x1 W\nH\nd\nP\nN\nd\nTransformerL×\nP\nN\nd\nW\nH\nd\nConv-1x1\nW\nH\nC\nWH\n2C\nConv-n×n\nW\nH\nC\nY\nUnfold Fold\nConv-3×3↓2 MV2MV2↓2 MV2\n2×\nMV2↓2 MobileViTblock\nL= 2\nh=w= 2\nMV2↓2 MobileViTblock\nL= 4\nh=w= 2\nMV2↓2 MobileViTblock\nL= 3\nh=w= 2\nConv-1×1 Global pool→LinearLogits\nOutput spatial→dimensions\n128×128 64×64 32×32 16×16 8×8 1×1\n⇑\n\n(b) MobileViT. Here, Conv-n ×n in the MobileViT block represents a standard n ×n convolution and\nMV2 refers to MobileNetv2 block. Blocks that perform down-sampling are marked with ↓2.\nFigure 1: Visual transformers vs. MobileViT\n70 71 72 73 74 75 76 77 78 79\nTop 1 accuracy (ImageNet)\n21\n22\n23\n24\n25\n26\n27\n28mAP (MS COCO)\n5.1 M\n4.3 M 4.9 M\n4.9 M\n4.5 M\n2.7M\n5.7 M\nBetter : 1.8%\nSmaller : 1.8 ×\nBackbones\nMobilenetv1\nMobileNetv2\nMobileNetv3\nMNASNet\nMixNet\nMobileViT (Ours)\nFigure 2: MobileViT shows better task-level gener-\nalization properties as compared to light-weight CNN\nmodels. The network parameters are listed for SSDLite\nnetwork with different feature extractors (MobileNetv1\n(Howard et al., 2017), MobileNetv2 (Sandler et al.,\n2018), MobileNetv3 (Howard et al., 2019), MNASNet\n(Tan et al., 2019), MixNet (Tan & Le, 2019b), and Mo-\nbileViT (Ours)) on the MS-COCO dataset.\nare gaining interest (Xiao et al., 2021; d’Ascoli et al., 2021; Chen et al., 2021b). However, these\nhybrid models are still heavy-weight and are sensitive to data augmentation. For example, remov-\ning CutMix (Zhong et al., 2020) and DeIT-style (Touvron et al., 2021a) data augmentation causes a\nsigniﬁcant drop in ImageNet accuracy (78.1% to 72.4%) of Heo et al. (2021).\nIt remains an open question to combine the strengths of CNNs and transformers to build ViT models\nfor mobile vision tasks. Mobile vision tasks require light-weight, low latency, and accurate models\nthat satisfy the device’s resource constraints, and are general-purpose so that they can be applied to\ndifferent tasks (e.g., segmentation and detection). Note that ﬂoating-point operations (FLOPs) are\nnot sufﬁcient for low latency on mobile devices because FLOPs ignore several important inference-\nrelated factors such as memory access, degree of parallelism, and platform characteristics (Ma et al.,\n2018). For example, the ViT-based method of Heo et al. (2021), PiT, has 3×fewer FLOPs than\nDeIT (Touvron et al., 2021a) but has a similar inference speed on a mobile device (DeIT vs. PiT\non iPhone-12: 10.99 ms vs. 10.56 ms). Therefore, instead of optimizing for FLOPs 1, this paper\nfocuses on designing a light-weight (§3), general-purpose (§4.1 & §4.2), and low latency(§4.3)\nnetwork for mobile vision tasks. We achieve this goal with MobileViT that combines the beneﬁts\nof CNNs (e.g., spatial inductive biases and less sensitivity to data augmentation) and ViTs (e.g.,\ninput-adaptive weighting and global processing). Speciﬁcally, we introduce the MobileViT block\nthat encodes both local and global information in a tensor effectively (Figure 1b). Unlike ViT and its\nvariants (with and without convolutions), MobileViT presents a different perspective to learn global\nrepresentations. Standard convolution involves three operations: unfolding, local processing, and\n1MobileViT FLOPs can be further reduced using existing methods (e.g., DynamicViT (Rao et al., 2021)).\n2\nPublished as a conference paper at ICLR 2022\nfolding. MobileViT block replaces local processing in convolutions with global processing using\ntransformers. This allows MobileViT block to have CNN- and ViT-like properties, which helps it\nlearn better representations with fewer parameters and simple training recipes (e.g., basic augmen-\ntation). To the best of our knowledge, this is the ﬁrst work that shows that light-weight ViTs can\nachieve light-weight CNN-level performance with simple training recipes across different mobile\nvision tasks. For a parameter budget of about 5-6 million, MobileViT achieves a top-1 accuracy of\n78.4% on the ImageNet-1k dataset (Russakovsky et al., 2015), which is 3.2% more accurate than\nMobileNetv3 and has a simple training recipe (MobileViT vs. MobileNetv3: 300 vs. 600 epochs;\n1024 vs. 4096 batch size). We also observe signiﬁcant gains in performance when MobileViT is\nused as a feature backbone in highly optimized mobile vision task-speciﬁc architectures. Replacing\nMNASNet (Tan et al., 2019) with MobileViT as a feature backbone in SSDLite (Sandler et al., 2018)\nresulted in a better (+1.8% mAP) and smaller (1.8×) detection network (Figure 2).\n2 R ELATED WORK\nLight-weight CNNs.The basic building layer in CNNs is a standard convolutional layer. Because\nthis layer is computationally expensive, several factorization-based methods have been proposed to\nmake it light-weight and mobile-friendly (e.g., Jin et al., 2014; Chollet, 2017; Mehta et al., 2020).\nOf these, separable convolutions of Chollet (2017) have gained interest, and are widely used across\nstate-of-the-art light-weight CNNs for mobile vision tasks, including MobileNets (Howard et al.,\n2017; Sandler et al., 2018; Howard et al., 2019), ShufﬂeNetv2 (Ma et al., 2018), ESPNetv2 (Mehta\net al., 2019), MixNet (Tan & Le, 2019b), and MNASNet (Tan et al., 2019). These light-weight\nCNNs are versatile and easy to train. For example, these networks can easily replace the heavy-\nweight backbones (e.g., ResNet (He et al., 2016)) in existing task-speciﬁc models (e.g., DeepLabv3)\nto reduce the network size and improve latency. Despite these beneﬁts, one major drawback of these\nmethods is that they are spatially local. This work views transformers as convolutions; allowing to\nleverage the merits of both convolutions (e.g., versatile and simple training) and transformers (e.g.,\nglobal processing) to build light-weight (§3) and general-purpose (§4.1 and §4.2) ViTs.\nVision transformers. Dosovitskiy et al. (2021) apply transformers of Vaswani et al. (2017) for\nlarge-scale image recognition and showed that with extremely large-scale datasets (e.g., JFT-300M),\nViTs can achieve CNN-level accuracy without image-speciﬁc inductive bias. With extensive data\naugmentation, heavy L2 regularization, and distillation, ViTs can be trained on the ImageNet dataset\nto achieve CNN-level performance (Touvron et al., 2021a;b; Zhou et al., 2021). However, unlike\nCNNs, ViTs show substandard optimizability and are difﬁcult to train. Subsequent works (e.g.,\nGraham et al., 2021; Dai et al., 2021; Liu et al., 2021; Wang et al., 2021; Yuan et al., 2021b; Chen\net al., 2021b) shows that this substandard optimizability is due to the lack of spatial inductive biases\nin ViTs. Incorporating such biases using convolutions in ViTs improves their stability and perfor-\nmance. Different designs have been explored to reap the beneﬁts of convolutions and transformers.\nFor instance, ViT-C of Xiao et al. (2021) adds an early convolutional stem to ViT. CvT (Wu et al.,\n2021) modiﬁes the multi-head attention in transformers and uses depth-wise separable convolutions\ninstead of linear projections. BoTNet (Srinivas et al., 2021) replaces the standard3×3 convolution in\nthe bottleneck unit of ResNet with multi-head attention. ConViT (d’Ascoli et al., 2021) incorporates\nsoft convolutional inductive biases using a gated positional self-attention. PiT (Heo et al., 2021)\nextends ViT with depth-wise convolution-based pooling layer. Though these models can achieve\ncompetitive performance to CNNs with extensive augmentation, the majority of these models are\nheavy-weight. For instance, PiT and CvT learns 6.1×and 1.7×more parameters than EfﬁcientNet\n(Tan & Le, 2019a) and achieves similar performance (top-1 accuracy of about 81.6%) on ImageNet-\n1k dataset, respectively. Also, when these models are scaled down to build light-weight ViT models,\ntheir performance is signiﬁcantly worse than light-weight CNNs. For a parameter budget of about 6\nmillion, ImageNet-1k accuracy of PiT is 2.2% less than MobileNetv3.\nDiscussion. Combining convolutions and transformers results in robust and high-performing ViTs\nas compared to vanilla ViTs. However, an open question here is: how to combine the strengths of\nconvolutions and transformers to build light-weight networks for mobile vision tasks? This paper\nfocuses on designing light-weight ViT models that outperform state-of-the-art models with simple\ntraining recipes. Towards this end, we introduce MobileViT that combines the strengths of CNNs\nand ViTs to build a light-weight, general-purpose, and mobile-friendly network. MobileViT brings\nseveral novel observations. (i)Better performance:For a given parameter budget, MobileViT mod-\n3\nPublished as a conference paper at ICLR 2022\n0 50 100 150 200 250 300\nEpochs\n20\n25\n30\n35\n40\n45\n50\n55\n60Training error (%)\nMobileViT-XXS\nMobileViT-XS\nMobileViT-S\n(a) Training error\n0 50 100 150 200 250 300\nEpochs\n20\n25\n30\n35\n40\n45\n50\n55\n60Validation error (%)\nMobileViT-XXS\nMobileViT-XS\nMobileViT-S (b) Validation error\nModel # Params. Top-1 Top-5MobileViT-XXS 1.3 M 69.0 88.9MobileViT-XS 2.3 M 74.8 92.3MobileViT-S 5.6 M 78.4 94.1\n(c) Validation accuracy\n1282 642 322 162 82 12\nOutput spatial dimensions\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nParameters\n(in million)\nMobileViT-XXS\nMobileViT-XS\nMobileViT-S\n(d) Parameter distribution\nFigure 3: MobileViT shows similar generalization capabilities as CNNs. Final training and vali-\ndation errors of MobileNetv2 and ResNet-50 are marked with ⋆ and ◦, respectively (§B).\nels achieve better performance as compared to existing light-weight CNNs across different mobile\nvision tasks (§4.1 and §4.2). (ii) Generalization capability: Generalization capability refers to\nthe gap between training and evaluation metrics. For two models with similar training metrics,\nthe model with better evaluation metrics is more generalizable because it can predict better on an\nunseen dataset. Unlike previous ViT variants (with and without convolutions) which show poor\ngeneralization capability even with extensive data augmentation as compared to CNNs (Dai et al.,\n2021), MobileViT shows better generalization capability (Figure 3). (iii) Robust: A good model\nshould be robust to hyper-parameters (e.g., data augmentation and L2 regularization) because tuning\nthese hyper-parameters is time- and resource-consuming. Unlike most ViT-based models, Mobile-\nViT models train with basic augmentation and are less sensitive to L2 regularization (§C).\n3 M OBILE VIT: A L IGHT -WEIGHT TRANSFORMER\nA standard ViT model, shown in Figure 1a, reshapes the input X ∈RH×W×C into a sequence of\nﬂattened patches Xf ∈RN×PC , projects it into a ﬁxed d-dimensional space Xp ∈RN×d, and then\nlearn inter-patch representations using a stack of L transformer blocks. The computational cost of\nself-attention in vision transformers is O(N2d). Here, C, H, and W represent the channels, height,\nand width of the tensor respectively, andP = wh is number of pixels in the patch with heighth and\nwidth w, and N is the number of patches. Because these models ignore the spatial inductive bias\nthat is inherent in CNNs, they require more parameters to learn visual representations. For instance,\nDPT (Dosovitskiy et al., 2021), a ViT-based network, learns 6×more parameters as compared to\nDeepLabv3 (Chen et al., 2017), a CNN-based network, to deliver similar segmentation performance\n(DPT vs. DeepLabv3: 345 M vs. 59 M). Also, in comparison to CNNs, these models exhibit sub-\nstandard optimizability. These models are sensitive to L2 regularization and require extensive data\naugmentation to prevent overﬁtting (Touvron et al., 2021a; Xiao et al., 2021).\nThis paper introduces a light-weight ViT model, MobileViT. The core idea is to learn global repre-\nsentations with transformers as convolutions. This allows us to implicitly incorporate convolution-\nlike properties (e.g., spatial bias) in the network, learn representations with simple training recipes\n(e.g., basic augmentation), and easily integrate MobileViT with downstream architectures (e.g.,\nDeepLabv3 for segmentation).\n3.1 M OBILE VIT ARCHITECTURE\nMobileViT block. The MobileViT block, shown in Figure 1b, aims to model the local and global\ninformation in an input tensor with fewer parameters. Formally, for a given input tensor X ∈\nRH×W×C, MobileViT applies a n ×n standard convolutional layer followed by a point-wise (or\n1 ×1) convolutional layer to produceXL ∈RH×W×d. The n×n convolutional layer encodes local\nspatial information while the point-wise convolution projects the tensor to a high-dimensional space\n(or d-dimensional, where d > C) by learning linear combinations of the input channels.\nWith MobileViT, we want to model long-range non-local dependencies while having an effective\nreceptive ﬁeld of H ×W. One of the widely studied methods to model long-range dependencies\n4\nPublished as a conference paper at ICLR 2022\n1\nFigure 4: Every pixel sees every other pixel in the MobileViT block.In this example, the red\npixel attends toblue pixels (pixels at the corresponding location in other patches) using transformers.\nBecause blue pixels have already encoded information about the neighboring pixels using convolu-\ntions, this allows the red pixel to encode information from all pixels in an image. Here, each cell in\nblack and gray grids represents a patch and a pixel, respectively.\nis dilated convolutions. However, such approaches require careful selection of dilation rates. Oth-\nerwise, weights are applied to padded zeros instead of the valid spatial region (Yu & Koltun, 2016;\nChen et al., 2017; Mehta et al., 2018). Another promising solution is self-attention (Wang et al.,\n2018; Ramachandran et al., 2019; Bello et al., 2019; Dosovitskiy et al., 2021). Among self-attention\nmethods, vision transformers (ViTs) with multi-head self-attention are shown to be effective for\nvisual recognition tasks. However, ViTs are heavy-weight and exhibit sub-standard optimizability.\nThis is because ViTs lack spatial inductive bias (Xiao et al., 2021; Graham et al., 2021).\nTo enable MobileViT to learn global representations with spatial inductive bias, we unfold XL\ninto N non-overlapping ﬂattened patches XU ∈ RP×N×d. Here, P = wh, N = HW\nP is the\nnumber of patches, and h ≤ n and w ≤ n are height and width of a patch respectively. For\neach p ∈ {1, ··· , P}, inter-patch relationships are encoded by applying transformers to obtain\nXG ∈RP×N×d as:\nXG(p) =Transformer(XU (p)), 1 ≤p ≤P (1)\nUnlike ViTs that lose the spatial order of pixels, MobileViT neither loses the patch order nor the\nspatial order of pixels within each patch (Figure 1b). Therefore, we can fold XG ∈RP×N×d to\nobtain XF ∈ RH×W×d. XF is then projected to low C-dimensional space using a point-wise\nconvolution and combined with X via concatenation operation. Another n ×n convolutional layer\nis then used to fuse these concatenated features. Note that becauseXU (p) encodes local information\nfrom n ×n region using convolutions and XG(p) encodes global information across P patches for\nthe p-th location, each pixel in XG can encode information from all pixels in X, as shown in Figure\n4. Thus, the overall effective receptive ﬁeld of MobileViT is H ×W.\nRelationship to convolutions.Standard convolutions can be viewed as a stack of three sequential\noperations: (1) unfolding, (2) matrix multiplication (to learn local representations), and (3) fold-\ning. MobileViT block is similar to convolutions in the sense that it also leverages the same build-\ning blocks. MobileViT block replaces the local processing (matrix multiplication) in convolutions\nwith deeper global processing (a stack of transformer layers). As a consequence, MobileViT has\nconvolution-like properties (e.g., spatial bias). Hence, the MobileViT block can be viewed as trans-\nformers as convolutions. An advantage of our intentionally simple design is that low-level efﬁcient\nimplementations of convolutions and transformers can be used out-of-the-box; allowing us to use\nMobileViT on different devices without any extra effort.\nLight-weight. MobileViT block uses standard convolutions and transformers to learn local and\nglobal representations respectively. Because previous works (e.g., Howard et al., 2017; Mehta et al.,\n2021a) have shown that networks designed using these layers are heavy-weight, a natural question\narises: Why MobileViT is light-weight? We believe that the issues lie primarily in learning global\nrepresentations with transformers. For a given patch, previous works (e.g., Touvron et al., 2021a;\nGraham et al., 2021) convert the spatial information into latent by learning a linear combination\nof pixels (Figure 1a). The global information is then encoded by learning inter-patch information\nusing transformers. As a result, these models lose image-speciﬁc inductive bias, which is inherent in\nCNNs. Therefore, they require more capacity to learn visual representations. Hence, they are deep\nand wide. Unlike these models, MobileViT uses convolutions and transformers in a way that the\nresultant MobileViT block has convolution-like properties while simultaneously allowing for global\nprocessing. This modeling capability allows us to design shallow and narrow MobileViT models,\nwhich in turn are light-weight. Compared to the ViT-based model DeIT that uses L=12 and d=192,\n5\nPublished as a conference paper at ICLR 2022\nGPU-4\nGPU-3\nGPU-2\nGPU-1\ntime\nStandard sampler\nGPU-4\nGPU-3\nGPU-2\nGPU-1\ntime\nMulti-scale sampler\nHeight\nBatch\nWidth\nTensor\nGradient sync.\n(a) Standard vs. multi-scale sampler illustration\nSampler # Updates Epoch time\nStandard 375 k 380 secMulti-scale (ours) 232 k 270 sec\n(b) Training efﬁciency. Here, standard\nsampler refers to PyTorch’s Distributed-\nDataParallel sampler.\nFigure 5: Multi-scale vs. standard sampler.\nMobileViT model uses L= {2, 4, 3}and d={96, 120, 144}at spatial levels 32 ×32, 16 ×16, and\n8 ×8, respectively. The resulting MobileViT network is faster ( 1.85×), smaller ( 2×), and better\n(+1.8%) than DeIT network (Table 3; §4.3).\nComputational cost.The computational cost of multi-headed self-attention in MobileViT and ViTs\n(Figure 1a) is O(N2Pd) and O(N2d), respectively. In theory, MobileViT is inefﬁcient as compared\nto ViTs. However, in practice, MobileViT is more efﬁcient than ViTs. MobileViT has 2×fewer\nFLOPs and delivers 1.8% better accuracy than DeIT on the ImageNet-1K dataset (Table 3; §4.3).\nWe believe that this is because of similar reasons as for the light-weight design (discussed above).\nMobileViT architecture.Our networks are inspired by the philosophy of light-weight CNNs. We\ntrain MobileViT models at three different network sizes (S: small, XS: extra small, and XXS: extra\nextra small) that are typically used for mobile vision tasks (Figure 3c). The initial layer in Mo-\nbileViT is a strided 3 ×3 standard convolution, followed by MobileNetv2 (or MV2) blocks and\nMobileViT blocks (Figure 1b and §A). We use Swish (Elfwing et al., 2018) as an activation func-\ntion. Following CNN models, we use n = 3 in the MobileViT block. The spatial dimensions of\nfeature maps are usually multiples of 2 and h, w≤n. Therefore, we set h = w = 2 at all spatial\nlevels (see §C for more results). The MV2 blocks in MobileViT network are mainly responsible for\ndown-sampling. Therefore, these blocks are shallow and narrow in MobileViT network. Spatial-\nlevel-wise parameter distribution of MobileViT in Figure 3d further shows that the contribution of\nMV2 blocks towards total network parameters is very small across different network conﬁgurations.\n3.2 M ULTI -SCALE SAMPLER FOR TRAINING EFFICIENCY\nA standard approach in ViT-based models to learn multi-scale representations isﬁne-tuning. For in-\nstance, Touvron et al. (2021a) ﬁne-tunes the DeIT model trained at a spatial resolution of224 ×224\non varying sizes independently. Such an approach for learning multi-scale representations is prefer-\nable for ViTs because positional embeddings need to be interpolated based on the input size, and the\nnetwork’s performance is subjective to interpolation methods. Similar to CNNs, MobileViT does\nnot require any positional embeddings and it may beneﬁt from multi-scale inputs during training.\nPrevious CNN-based works (e.g., Redmon & Farhadi, 2017; Mehta et al., 2021b) have shown that\nmulti-scale training is effective. However, most of these works sample a new spatial resolution after\na ﬁxed number of iterations. For example, YOLOv2 (Redmon & Farhadi, 2017) samples a new\nspatial resolution from a pre-deﬁned set at every 10-th iteration and uses the same resolution across\ndifferent GPUs during training. This leads to GPU under-utilization and slower training because\nthe same batch size (determined using the maximum spatial resolution in the pre-deﬁned set) is\nused across all resolutions. To facilitate MobileViT learn multi-scale representations without ﬁne-\ntuning and to further improve training efﬁciency (i.e., fewer optimization updates), we extend the\nmulti-scale training method to variably-sized batch sizes. Given a sorted set of spatial resolutions\nS= {(H1, W1), ··· , (Hn, Wn)}and a batch size b for a maximum spatial resolution of (Hn, Wn),\nwe randomly sample a spatial resolution (Ht, Wt) ∈S at t-th training iteration on each GPU and\ncompute the batch size for t-th iteration as: bt = HnWnb\nHtWt\n. As a result, larger batch sizes are used for\nsmaller spatial resolutions. This reduces optimizer updates per epoch and helps in faster training.\nFigure 5 compares standard and multi-scale samplers. Here, we refer to DistributedDataParallel in\nPyTorch as the standard sampler. Overall, the multi-scale sampler (i) reduces the training time as it\nrequires fewer optimizer updates with variably-sized batches (Figure 5b), (ii) improves performance\nby about 0.5% (Figure 10; §B), and (iii) forces the network to learn better multi-scale representations\n(§B), i.e., the same network when evaluated at different spatial resolutions yields better performance\n6\nPublished as a conference paper at ICLR 2022\n1 2 3 4 5 6 7\n# Parameters (in million)\n55\n60\n65\n70\n75\n80T op-1 accuracy (%)\nMobilenetv1\nMobileNetv2\nMobileNetv3\nShuffleNetv2\nESPNetv2\nMobileViT (Ours)\n(a) Comparison with light-weight CNNs\nModel # Params.⇓Top-1⇑MobileNetv1 2.6 M 68.4MobileNetv2 2.6 M 69.8MobileNetv3 2.5 M 67.4ShufﬂeNetv2 2.3 M 69.4ESPNetv2 2.3 M 69.2MobileViT-XS (Ours) 2.3 M74.8\n(b) Comparison with light-weight CNNs (similar parameters)\nModel # Params.⇓Top-1⇑DenseNet-169 14 M 76.2EfﬁcientNet-B0 5.3 M 76.3ResNet-101 44.5 M 77.4ResNet-101-SE 49.3 M 77.6MobileViT-S (Ours) 5.6 M78.4\n(c) Comparison with heavy-weight CNNs\nFigure 6: MobileViT vs. CNNson ImageNet-1k validation set. All models use basic augmentation.\nas compared to the one trained with the standard sampler. In §B, we also show that the multi-scale\nsampler is generic and improves the performance of CNNs (e.g., MobileNetv2).\n4 E XPERIMENTAL RESULTS\nIn this section, we ﬁrst evaluate MobileViTs performance on the ImageNet-1k dataset and show that\nMobileViT delivers better performance than state-of-the-art networks (§4.1). In §4.2 and §4.3, we\nshow MobileViTs are general-purpose and mobile-friendly, respectively.\n4.1 I MAGE CLASSIFICATION ON THE IMAGE NET-1K DATASET\nImplementation details.We train MobileViT models from scratch on the ImageNet-1k classiﬁca-\ntion dataset (Russakovsky et al., 2015). The dataset provides 1.28 million and 50 thousand images\nfor training and validation, respectively. The MobileViT networks are trained using PyTorch for 300\nepochs on 8 NVIDIA GPUs with an effective batch size of 1,024 images using AdamW optimizer\n(Loshchilov & Hutter, 2019), label smoothing cross-entropy loss (smoothing=0.1), and multi-scale\nsampler (S= {(160, 160), (192, 192), (256, 256), (288, 288), (320, 320)}). The learning rate is in-\ncreased from 0.0002 to 0.002 for the ﬁrst 3k iterations and then annealed to 0.0002 using a cosine\nschedule (Loshchilov & Hutter, 2017). We use L2 weight decay of 0.01. We use basic data augmen-\ntation (i.e., random resized cropping and horizontal ﬂipping) and evaluate the performance using a\nsingle crop top-1 accuracy. For inference, an exponential moving average of model weights is used.\nComparison with CNNs.Figure 6a shows that MobileViT outperforms light-weight CNNs across\ndifferent network sizes (MobileNetv1 (Howard et al., 2017), MobileNetv2 (Sandler et al., 2018),\nShufﬂeNetv2 (Ma et al., 2018), ESPNetv2 (Mehta et al., 2019), and MobileNetv3 (Howard et al.,\n2019)). For instance, for a model size of about 2.5 million parameters (Figure 6b), MobileViT out-\nperforms MobileNetv2 by 5%, ShufﬂeNetv2 by 5.4%, and MobileNetv3 by 7.4% on the ImageNet-\n1k validation set. Figure 6c further shows that MobileViT delivers better performance than heavy-\nweight CNNs (ResNet (He et al., 2016), DenseNet (Huang et al., 2017), ResNet-SE (Hu et al.,\n2018), and EfﬁcientNet (Tan & Le, 2019a)). For instance, MobileViT is 2.1% more accurate than\nEfﬁcentNet for a similar number of parameters.\nComparison with ViTs. Figure 7 compares MobileViT with ViT variants that are trained from\nscratch on the ImageNet-1k dataset without distillation (DeIT (Touvron et al., 2021a), T2T (Yuan\net al., 2021b), PVT (Wang et al., 2021), CAIT (Touvron et al., 2021b), DeepViT (Zhou et al., 2021),\nCeiT (Yuan et al., 2021a), CrossViT (Chen et al., 2021a), LocalViT (Li et al., 2021), PiT (Heo et al.,\n2021), ConViT (d’Ascoli et al., 2021), ViL (Zhang et al., 2021), BoTNet (Srinivas et al., 2021),\nand Mobile-former (Chen et al., 2021b)). Unlike ViT variants that beneﬁt signiﬁcantly from ad-\nvanced augmentation (e.g., PiT w/ basic vs. advanced: 72.4 (R4) vs. 78.1 (R17); Figure 7b), Mo-\nbileViT achieves better performance with fewer parameters and basic augmentation. For instance,\nMobileViT is 2.5×smaller and 2.6% better than DeIT (R3 vs. R8 in Figure 7b).\nOverall, these results show that, similar to CNNs, MobileViTs are easy and robust to optimize.\nTherefore, they can be easily applied to new tasks and datasets.\n7\nPublished as a conference paper at ICLR 2022\n2 4 6 8 10\n# Parameters (in million)\n60.0\n62.5\n65.0\n67.5\n70.0\n72.5\n75.0\n77.5\n80.0T op-1 accuracy (%)\nDeIT\nConViT\nMobile-former\nCrossViT\nDeepViT\nLocalViT\nT2T\nMobileViT (Ours)\n(a)\nRow # Model Augmentation # Params.⇓Top-1⇑R1 DeIT Basic5.7 M 68.7R2 T2T Advanced4.3 M 71.7R3 DeITAdvanced5.7 M 72.2R4 PiT Basic10.6 M 72.4R5 Mobile-formerAdvanced4.6 M 72.8R6 PiT Advanced4.9 M 73.0R7 CrossViTAdvanced6.9 M 73.4R8 MobileViT-XS (Ours)Basic2.3 M74.8R9 CeiTAdvanced6.4 M 76.4R10 DeITAdvanced10 M 75.9R11 T2TAdvanced6.9 M 76.5R12 ViLAdvanced6.7 M 76.7R13 LocalVitAdvanced7.7 M 76.1R14 Mobile-formerAdvanced9.4 M 76.7R15 PVTAdvanced13.2 M 75.1R16 ConViTAdvanced10 M 76.7R17 PiTAdvanced10.6 M 78.1R18 BoTNetBasic20.8 M 77.0R19 BoTNetAdvanced20.8 M 78.3R20 MobileViT-S (Ours)Basic5.6 M78.4\n(b)\nFigure 7: MobileViT vs. ViTson ImageNet-1k validation set. Here, basic means ResNet-style aug-\nmentation while advanced means a combination of augmentation methods with basic (e.g., MixUp\n(Zhang et al., 2018), RandAugmentation (Cubuk et al., 2019), and CutMix (Zhong et al., 2020)).\n4.2 M OBILE VIT AS A GENERAL -PURPOSE BACKBONE\nTo evaluate the general-purpose nature of MobileViT, we benchmark MobileViT on two widely\nstudied mobile vision tasks: (1) object detection (§4.2.1) and (2) semantic segmentation (§4.2.2).\n4.2.1 M OBILE OBJECT DETECTION\nFeature backbone # Params.⇓mAP⇑MobileNetv3 4.9 M 22.0MobileNetv2 4.3 M 22.1MobileNetv1 5.1 M 22.2MixNet 4.5 M 22.3MNASNet 4.9 M 23.0MobileViT-XS (Ours)2.7 M24.8MobileViT-S (Ours) 5.7 M27.7\n(a) Comparison w/ light-weight CNNs\nFeature backbone # Params.⇓mAP⇑VGG 35.6 M 25.1ResNet50 22.9 M 25.2MobileViT-S (Ours)5.7 M 27.7\n(b) Comparison w/ heavy-weight CNNs\nTable 1: Detection w/ SSDLite.\nImplementation details. We integrate MobileViT with\na single shot object detection backbone (SSD; Liu et al.,\n2016). Following light-weight CNNs (e.g., MobileNets), we\nreplace standard convolutions in the SSD head with separa-\nble convolutions and call the resultant network as SSDLite.\nWe ﬁnetune MobileViT, pre-trained on the ImageNet-1k\ndataset, at an input resolution of 320 ×320 using AdamW\non the MS-COCO dataset (Lin et al., 2014) that contains\n117k training and 5k validation images. We use smooth L1\nand cross-entropy losses for object localization and classi-\nﬁcation, respectively. The performance is evaluated on the\nvalidation set using mAP@IoU of 0.50:0.05:0.95. For other\nhyper-parameters, see §D.\nResults. Table 1a shows that, for the same input resolu-\ntion of 320 ×320, SSDLite with MobileViT outperforms\nSSDLite with other light-weight CNN models (MobileNetv1/v2/v3, MNASNet, and MixNet). For\ninstance, SSDLite’s performance improves by 1.8%, and its model size reduces by1.8×when Mo-\nbileViT is used as a backbone instead of MNASNet. Further, SSDLite with MobileViT outperforms\nstandard SSD-300 with heavy-weight backbones while learning signiﬁcantly fewer parameters (Ta-\nble 1b). Also, qualitative results in §F conﬁrms MobileViT’s ability to detect variety of objects.\n4.2.2 M OBILE SEMANTIC SEGMENTATION\nFeature backbone # Params.⇓mIOU⇑MobileNetv1 11.2 M 75.3MobileNetv2 4.5 M 75.7MobileViT-XXS (Ours) 1.9 M 73.6MobileViT-XS (Ours) 2.9 M77.1ResNet-101 58.2 M80.5MobileViT-S (Ours) 6.4 M 79.1\nTable 2: Segmentation w/ DeepLabv3.\nImplementation details. We integrate Mobile-\nViT with DeepLabv3 (Chen et al., 2017). We ﬁne-\ntune MobileViT using AdamW with cross-entropy\nloss on the PASCAL VOC 2012 dataset (Ever-\ningham et al., 2015). Following a standard train-\ning practice (e.g., Chen et al., 2017; Mehta et al.,\n2019), we also use extra annotations and data from\nHariharan et al. (2011) and Lin et al. (2014), re-\nspectively. The performance is evaluated on the validation set using mean intersection over union\n(mIOU). For other hyper-parameters, see §D.\n8\nPublished as a conference paper at ICLR 2022\n3 4 5 6 7 8 9 10\nInference time (in ms)\n66\n68\n70\n72\n74\n76\n78\n80T op-1 accuracy\nReal-timeXXS\nXS\nS\nXXS\nXS\nS\nPatch sizes\n8,4,2\n2,2,2\n(a) Classiﬁcation @ 256 ×256\n6 8 10 12 14 16 18\nInference time (in ms)\n18\n20\n22\n24\n26\n28mAP@.50:.05:.95\nReal-timeXXS\nXS\nS\nXXS\nXS\nSPatch sizes\n8,4,2\n2,2,2 (b) Detection @ 320 ×320\n15 20 25 30 35 40\nInference time (in ms)\n70\n72\n74\n76\n78\n80mIoU\nReal-timeXXS\nXS\nS\nXXS\nXS\nSPatch sizes\n8,4,2\n2,2,2 (c) Segmentation @ 512 ×512\nFigure 8: Inference time of MobileViT models on different tasks.Here, dots in green color region\nrepresents that these models runs in real-time (inference time < 33 ms).\nResults. Table 2 shows that DeepLabv3 with MobileViT is smaller and better. The performance\nof DeepLabv3 is improved by 1.4%, and its size is reduced by 1.6×when MobileViT is used as a\nbackbone instead of MobileNetv2. Also, MobileViT gives competitive performance to model with\nResNet-101 while requiring 9×fewer parameters; suggesting MobileViT is a powerful backbone.\nAlso, results in §G shows that MobileViT learns generalizable representations of the objects and\nperform well on an unseen dataset.\n4.3 P ERFORMANCE ON MOBILE DEVICES\nLight-weight and low latency networks are important for enabling mobile vision applications. To\ndemonstrate the effectiveness of MobileViT for such applications, pre-trained full-precision Mobile-\nViT models are converted to CoreML using publicly available CoreMLTools (2021). Their inference\ntime is then measured (average over 100 iterations) on a mobile device, i.e., iPhone 12.\nMobile-friendly. Figure 8 shows the inference time of MobileViT networks with two patch size\nsettings (Conﬁg-A: 2, 2, 2 and Conﬁg-B: 8, 4, 2) on three different tasks. Here p1, p2, p3 in Conﬁg-\nX denotes the height h (width w = h) of a patch at an output stride 2 of 8, 16, and 32, respectively.\nThe models with smaller patch sizes (Conﬁg-A) are more accurate as compared to larger patches\n(Conﬁg-B). This is because, unlike Conﬁg-A models, Conﬁg-B models are not able to encode the\ninformation from all pixels (Figure 13 and §C). On the other hand, for a given parameter budget,\nConﬁg-B models are faster than Conﬁg-A even though the theoretical complexity of self-attention\nin both conﬁgurations is the same, i.e., O(N2Pd). With larger patch sizes (e.g., P=82=64), we\nhave fewer number of patches N as compared to smaller patch sizes (e.g., P=22=4). As a result,\nthe computation cost of self-attention is relatively less. Also, Conﬁg-B models offer a higher degree\nof parallelism as compared to Conﬁg-A because self-attention can be computed simultaneously for\nmore pixels in a larger patch (P=64) as compared to a smaller patch (P=4). Hence, Conﬁg-B models\nare faster than Conﬁg-A. To further improve MobileViT’s latency, linear self-attention (Wang et al.,\n2020) can be used. Regardless, all models in both conﬁgurations run in real-time (inference speed\n≥30 FPS) on a mobile device except for MobileViT-S models for the segmentation task. This is\nexpected as these models process larger inputs (512×512) as compared to classiﬁcation (256×256)\nand detection (320 ×320) networks.\nModel # Params.⇓FLOPs⇓Time⇓Top-1⇑MobileNetv2† 3.5 M0.3 G 0.92 ms73.3DeIT 5.7 M 1.3 G 10.99 ms 72.2PiT 4.9 M 0.7 G 10.56 ms 73.0MobileViT (Ours)2.3 M0.7 G 7.28 ms74.8\nTable 3: ViTs are slower than CNNs.\n†Results with multi-scale sampler (§B).\nDiscussion. We observe that MobileViT and other ViT-\nbased networks (e.g., DeIT and PiT) are slower as com-\npared to MobileNetv2 on mobile devices (Table 3). This\nobservation contradicts previous works which show that\nViTs are more scalable as compared to CNNs (Dosovit-\nskiy et al., 2021). This difference is primarily because\nof two reasons. First, dedicated CUDA kernels exist for\ntransformers on GPUs, which are used out-of-the-box in\nViTs to improve their scalability and efﬁciency on GPUs (e.g., Shoeybi et al., 2019; Lepikhin et al.,\n2021). Second, CNNs beneﬁt from several device-level optimizations, including batch normaliza-\ntion fusion with convolutional layers (Jacob et al., 2018). These optimizations improve latency and\nmemory access. However, such dedicated and optimized operations for transformers are currently\nnot available for mobile devices. Hence, the resultant inference graph of MobileViT and ViT-based\nnetworks for mobile devices is sub-optimal. We believe that similar to CNNs, the inference speed\nof MobileViT and ViTs will further improve with dedicated device-level operations in the future.\n2Output stride: Ratio of the spatial dimension of the input to the feature map.\n9\nPublished as a conference paper at ICLR 2022\n5 A CKNOWLEDGEMENTS\nWe are grateful to Ali Farhadi, Peter Zatloukal, Oncel Tuzel, Ashish Shrivastava, Frank Sun, Max\nHorton, Anurag Ranjan, and anonymous reviewers for their helpful comments. We are also thankful\nto Apple’s infrastructure and open-source teams for their help with training infrastructure and open-\nsource release of the code and pre-trained models.\nREFERENCES\nIrwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V Le. Attention augmented\nconvolutional networks. In Proceedings of the IEEE/CVF international conference on computer\nvision, pp. 3286–3295, 2019.\nChun-Fu Chen, Quanfu Fan, and Rameswar Panda. CrossVit: Cross-attention multi-scale vision\ntransformer for image classiﬁcation. In Proceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV), 2021a.\nLiang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous\nconvolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017.\nYinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen Liu, Xiaoyi Dong, Lu Yuan, and Zicheng\nLiu. Mobile-former: Bridging mobilenet and transformer. arXiv preprint arXiv:2108.05895 ,\n2021b.\nFranc ¸ois Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceedings\nof the IEEE conference on computer vision and pattern recognition, pp. 1251–1258, 2017.\nCoreMLTools. Use coremltools to convert models from third-party libraries to CoreML. https:\n//coremltools.readme.io/docs, 2021. [Online; accessed 2-September-2021].\nEkin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:\nLearning augmentation strategies from data. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 113–123, 2019.\nZihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying convolution and\nattention for all data sizes. arXiv preprint arXiv:2106.04803, 2021.\nSt´ephane d’Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio Biroli, and Levent Sagun.\nConvit: Improving vision transformers with soft convolutional inductive biases. arXiv preprint\narXiv:2103.10697, 2021.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-\nreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at\nscale. In International Conference on Learning Representations, 2021.\nStefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network\nfunction approximation in reinforcement learning. Neural Networks, 107:3–11, 2018.\nMark Everingham, SM Ali Eslami, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew\nZisserman. The pascal visual object classes challenge: A retrospective. International journal of\ncomputer vision, 111(1):98–136, 2015.\nBen Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Herv ´e J´egou, and\nMatthijs Douze. Levit: a vision transformer in convnet’s clothing for faster inference. arXiv\npreprint arXiv:2104.01136, 2021.\nBharath Hariharan, Pablo Arbel ´aez, Lubomir Bourdev, Subhransu Maji, and Jitendra Malik. Se-\nmantic contours from inverse detectors. In 2011 International Conference on Computer Vision ,\npp. 991–998. IEEE, 2011.\n10\nPublished as a conference paper at ICLR 2022\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp.\n770–778, 2016.\nByeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe, and Seong Joon\nOh. Rethinking spatial dimensions of vision transformers. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV), 2021.\nAndrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun\nWang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In Pro-\nceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1314–1324, 2019.\nAndrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,\nMarco Andreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolutional neural networks for\nmobile vision applications. arXiv preprint arXiv:1704.04861, 2017.\nJie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pp. 7132–7141, 2018.\nGao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected\nconvolutional networks. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pp. 4700–4708, 2017.\nBenoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard,\nHartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for\nefﬁcient integer-arithmetic-only inference. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pp. 2704–2713, 2018.\nJonghoon Jin, Aysegul Dundar, and Eugenio Culurciello. Flattened convolutional neural networks\nfor feedforward acceleration. arXiv preprint arXiv:1412.5474, 2014.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. {GS}hard: Scaling giant models with condi-\ntional computation and automatic sharding. In International Conference on Learning Represen-\ntations, 2021.\nYawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc Van Gool. Localvit: Bringing locality\nto vision transformers. arXiv preprint arXiv:2104.05707, 2021.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European\nconference on computer vision, pp. 740–755. Springer, 2014.\nWei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and\nAlexander C Berg. Ssd: Single shot multibox detector. In European conference on computer\nvision, pp. 21–37. Springer, 2016.\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint\narXiv:2103.14030, 2021.\nJonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic\nsegmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npp. 3431–3440, 2015.\nIlya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. In Interna-\ntional Conference on Learning Representations, 2017.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-\nence on Learning Representations, 2019.\nNingning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufﬂenet v2: Practical guidelines for\nefﬁcient cnn architecture design. In Proceedings of the European conference on computer vision\n(ECCV), pp. 116–131, 2018.\n11\nPublished as a conference paper at ICLR 2022\nSachin Mehta, Mohammad Rastegari, Anat Caspi, Linda Shapiro, and Hannaneh Hajishirzi. Espnet:\nEfﬁcient spatial pyramid of dilated convolutions for semantic segmentation. In Proceedings of\nthe european conference on computer vision (ECCV), pp. 552–568, 2018.\nSachin Mehta, Mohammad Rastegari, Linda Shapiro, and Hannaneh Hajishirzi. Espnetv2: A light-\nweight, power efﬁcient, and general purpose convolutional neural network. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9190–9200, 2019.\nSachin Mehta, Hannaneh Hajishirzi, and Mohammad Rastegari. Dicenet: Dimension-wise convo-\nlutions for efﬁcient networks. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n2020.\nSachin Mehta, Marjan Ghazvininejad, Srinivasan Iyer, Luke Zettlemoyer, and Hannaneh Hajishirzi.\nDelight: Deep and light-weight transformer. In International Conference on Learning Represen-\ntations, 2021a.\nSachin Mehta, Amit Kumar, Fitsum Reda, Varun Nasery, Vikram Mulukutla, Rakesh Ranjan, and\nVikas Chandra. Evrnet: Efﬁcient video restoration on edge devices. In Proceedings of the ACM\nMultimedia, 2021b.\nPyTorch. Torchvision semantic segmentation. https://pytorch.org/vision/stable/\nmodels.html#semantic-segmentation, 2021. Online; accessed 15 November 2021.\nPrajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens.\nStand-alone self-attention in vision models. In H. Wallach, H. Larochelle, A. Beygelzimer,\nF. d'Alch´e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Sys-\ntems, volume 32. Curran Associates, Inc., 2019.\nRen´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction.\narXiv preprint arXiv:2103.13413, 2021.\nYongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. Dynamicvit:\nEfﬁcient vision transformers with dynamic token sparsiﬁcation.arXiv preprint arXiv:2106.02034,\n2021.\nJoseph Redmon and Ali Farhadi. Yolo9000: better, faster, stronger. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pp. 7263–7271, 2017.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual\nrecognition challenge. International journal of computer vision, 115(3):211–252, 2015.\nMark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mo-\nbilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp. 4510–4520, 2018.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan\nCatanzaro. Megatron-lm: Training multi-billion parameter language models using model par-\nallelism. arXiv preprint arXiv:1909.08053, 2019.\nAravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani.\nBottleneck transformers for visual recognition. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 16519–16529, 2021.\nMingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model scaling for convolutional neural net-\nworks. In International Conference on Machine Learning, pp. 6105–6114. PMLR, 2019a.\nMingxing Tan and Quoc V Le. Mixconv: Mixed depthwise convolutional kernels. In Proceedings\nof the British Machine Vision Conference (BMVC), 2019b.\nMingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and\nQuoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2820–2828, 2019.\n12\nPublished as a conference paper at ICLR 2022\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and\nHerv´e J ´egou. Training data-efﬁcient image transformers & distillation through attention. In\nInternational Conference on Machine Learning, pp. 10347–10357. PMLR, 2021a.\nHugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herv´e J´egou. Going\ndeeper with image transformers. arXiv preprint arXiv:2103.17239, 2021b.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems, pp. 5998–6008, 2017.\nSinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention\nwith linear complexity. arXiv preprint arXiv:2006.04768, 2020.\nWenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo,\nand Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without\nconvolutions. In Proceedings of the IEEE/CVF international conference on computer vision ,\n2021.\nXiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pp. 7794–7803,\n2018.\nHaiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:\nIntroducing convolutions to vision transformers. arXiv preprint arXiv:2103.15808, 2021.\nTete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Doll ´ar, and Ross Girshick. Early\nconvolutions help transformers see better. arXiv preprint arXiv:2106.14881, 2021.\nFisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. In Interna-\ntional Conference on Learning Representations, 2016.\nKun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu. Incorporating con-\nvolution designs into visual transformers. arXiv preprint arXiv:2103.11816, 2021a.\nLi Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi\nFeng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on\nimagenet. In Proceedings of the IEEE/CVF international conference on computer vision, 2021b.\nHongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empiri-\ncal risk minimization. In International Conference on Learning Representations, 2018.\nPengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, and Jianfeng Gao.\nMulti-scale vision longformer: A new vision transformer for high-resolution image encoding.\narXiv preprint arXiv:2103.15358, 2021.\nZhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data aug-\nmentation. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence , volume 34, pp.\n13001–13008, 2020.\nDaquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Zihang Jiang, Qibin Hou,\nand Jiashi Feng. Deepvit: Towards deeper vision transformer. arXiv preprint arXiv:2103.11886,\n2021.\n13\nPublished as a conference paper at ICLR 2022\nA M OBILE VIT ARCHITECTURE\nMobileViT’s are inspired by the philosophy of light-weight CNNs and the overall architecture of\nMobileViT at different parameter budgets is given in Table 4. The initial layer in MobileViT is\na strided 3 ×3 standard convolution, followed by MobileNetv2 (or MV2) blocks and Mobile-\nViT blocks. We use Swish (Elfwing et al., 2018) as an activation function. Following CNN models,\nwe use n = 3 in the MobileViT block. The spatial dimensions of feature maps are usually mul-\ntiples of 2 and h, w≤n. Therefore, we set h = w = 2 at all spatial levels. The MV2 blocks\nin MobileViT network are mainly responsible for down-sampling. Therefore, in these blocks, we\nuse an expansion factor of four, except for MobileViT-XXS where we use an expansion factor of 2.\nThe transformer layer in MobileViT takes a d-dimensional input, as shown in Figure 1b. We set the\noutput dimension of the ﬁrst feed-forward layer in a transformer layer as 2d instead of 4d, a default\nvalue in the standard transformer block of Vaswani et al. (2017).\nB M ULTI -SCALE SAMPLER\nMulti-scale sampler reduces generalization gap.Generalization capability refers to the gap be-\ntween training and evaluation metrics. For two models with similar training metrics,the model with\nbetter evaluation metrics is more generalizable because it can predict better on an unseen dataset.\nFigure 9a and Figure 9b compares the training and validation error of the MobileViT-S model trained\nwith standard and multi-scale samplers. The training error of MobileViT-S with multi-scale sampler\nis higher than standard sampler while validation error is lower. Also, the gap between training error\nand validation error of MobileViT-S with multi-scale sampler is close to zero. This suggests that a\nmulti-scale sampler improves generalization capability. Also, when MobileViT-S trained indepen-\ndently with standard and multi-scale sampler is evaluated at different input resolutions (Figure 9c),\nwe observe that MobileViT-S trained with multi-scale sampler is more robust as compared to the one\ntrained with the standard sampler. We also observe that multi-scale sampler improves the perfor-\nmance of MobileViT models at different model sizes by about 0.5% (Figure 10). These observations\nin conjunction with impact on training efﬁciency (Figure 5b) suggests that a multi-scale sampler is\neffective. Pytorch implementation of multi-scale sampler is provided in Listing 1.\nMulti-scale sampler is generic. We train a heavy-weight (ResNet-50) and a light-weight\n(MobileNetv2-1.0) CNN with the multi-scale sampler to demonstrate its generic nature. Results\nin Table 5 show that a multi-scale sampler improves the performance as well as training efﬁciency.\nLayer Output size Output stride Repeat Output channels\nXXS XS S\nImage 256×256 1\nConv-3×3, ↓2 128×128 2 1 16 16 16\nMV2 1 16 32 32\nMV2,↓2 64×64 4 1 24 48 64\nMV2 2 24 48 64\nMV2,↓2 32×32 8 1 48 64 96\nMobileViT block (L= 2) 1 48 ( d= 64) 64 ( d= 96) 96 ( d= 144)\nMV2,↓2 16×16 16 1 64 80 128\nMobileViT block (L= 4) 1 64 ( d= 80) 80 ( d= 120) 128 ( d= 192)\nMV2,↓2\n8×8 32\n1 80 96 160\nMobileViT block (L= 3) 1 80 ( d= 96) 96 ( d= 144) 160 ( d= 240)\nConv-1×1 1 320 384 640\nGlobal pool 1×1 256 1Linear 1000 1000 1000\nNetwork Parameters 1.3 M 2.3 M 5.6 M\nTable 4: MobileViT architecture.Here, d represents dimensionality of the input to the transformer\nlayer in MobileViT block (Figure 1b). By default, in MobileViT block, we set kernel sizen as three\nand spatial dimensions of patch (height h and width w) in MobileViT block as two.\n14\nPublished as a conference paper at ICLR 2022\n0 50 100 150 200 250 300\nEpochs\n20\n25\n30\n35\n40\n45\n50\n55\n60Training error (%)\nStandard\nMulti-scale (Ours)\n(a) Training error\n0 50 100 150 200 250 300\nEpochs\n20\n25\n30\n35\n40\n45\n50\n55\n60Validation error (%)\nStandard\nMulti-scale (Ours) (b) Validation error\n160x160 192x192 256x256\nInput resolution\n68\n70\n72\n74\n76\n78T op-1 accuracy (%)\nMulti-scale (Ours)\nStandard (c) Validation acc. vs. input resolution\nFigure 9: MobileViT-S learns better representations with multi-scale sampler on ImageNet-1k.\nXXS XS S\nMobileViT models\n68\n70\n72\n74\n76\n78T op-1 accuracy (%)\nStandard\nMulti-scale (Ours)\nFigure 10: MobileViT’s performance on ImageNet-1k with standard and multi-scale sampler.\nModel # Params # Epochs # Updates ⇓ Top-1 accuracy⇑ Training time⇓\nResNet-50 w/ standard sampler (PyTorch) 25 M – – 76.2 (0.0%) – –ResNet-50 w/ standard sampler (our repro.)† 25 M 150 187 k 77.1 (+0.9%) 54 k sec. (1.35 ×)ResNet-50 w/ multi-scale sampler (Ours)† 25 M 150 116 k 78.6 (+2.4%) 40 k sec. (1 ×)\nMobileNetv2-1.0 w/ standard sampler (PyTorch) 3.5 M – – 71.9 (0.0%) – (–)MobileNetv2-1.0 w/ standard sampler (our repro.)† 3.5 M 300 375 k 72.1 (+0.2%) 78 k sec. (1.16 ×)MobileNetv2-1.0 w/ multi-scale sampler (Ours)† 3.5 M 300 232 k 73.3 (+1.4%) 67 k sec. (1 ×)\nTable 5: Multi-scale sampler is generic.All models are trained with basic augmentation on the\nImageNet-1k. †Results are with exponential moving average.\nFor instance, a multi-scale sampler improves the performance of MobileNetv2-1.0 by about 1.4%\nwhile decreasing the training time by 14%.\nC A BLATIONS\nImpact of weight decay.A good model should be insensitive or less sensitive to L2 regularization\n(or weight decay) because tuning it for each task and dataset is time- and resource-consuming.\nUnlike CNNs, ViT models are sensitive to weight decay (Dosovitskiy et al., 2021; Touvron et al.,\n2021a; Xiao et al., 2021). To study if MobileViT models are sensitive to weight decay or not, we\ntrain the MobileViT-S model by varying the value of weight decay from 0.1 to 0.0001. Results are\nshown in Figure 11. With an exception to the MobileViT model trained with a weight decay of 0.1,\nall other models converged to a similar solution. This shows that MobileViT models are robust to\nweight decay. In our experiments, we use the value of weight decay as 0.01. Note that 0.0001 is\nthe widely used value of weight decay in most CNN-based models, such as ResNet and DenseNet.\nEven at this value of weight decay, MobileViT outperforms CNNs on the ImageNet-1k dataset (e.g.,\nDenseNet vs. MobileViT: 76.2 with 14 M parameters vs. 77.4 with 5.7 M parameters).\nImpact of skip-connection.Figure 12 studies the impact of skip-connection in the MobileViT block\n(red arrow in Figure 1b). With this connection, the performance of MobileViT-S improves by 0.5%\non the ImageNet dataset. Note that even without this skip-connection, MobileViT-S delivers similar\nor better performance than state-of-the-art CNN- (Figure 6) and ViT-based (Figure 7b) models, that\ntoo with basic data augmentation.\nImpact of patch sizes.MobileViT combines convolutions and transformers to learn local and global\nrepresentations effectively. Because convolutions are applied on n ×n regions and self-attention\n15\nPublished as a conference paper at ICLR 2022\n0 50 100 150 200 250 300\nEpochs\n20\n25\n30\n35\n40\n45\n50\n55Training error (%)\nWeight decay\n0.1\n0.01\n0.001\n0.0001\n(a) Training error\n0 50 100 150 200 250 300\nEpochs\n20\n25\n30\n35\n40\n45\n50\n55Validation error (%)\nWeight decay\n0.1\n0.01\n0.001\n0.0001 (b) Validation error\nWeight decay Top-1⇑\n0.1 76.5\n0.01 78.4\n0.001 77.6\n0.0001 77.4\n(c) Validation accuracy\nFigure 11: Impact of weight decay. Here, results are shown for MobileViT-S model (5.7 M param-\neters) on the ImageNet-1k dataset. Results in (c) are with exponential moving average.\n0 50 100 150 200 250 300\nEpochs\n20\n25\n30\n35\n40\n45Training error (%)\nw/o skip\nw/ skip\n(a) Training error\n0 50 100 150 200 250 300\nEpochs\n20\n25\n30\n35\n40\n45Validation error (%)\nw/o skip\nw/ skip (b) Validation error\nSkip-connection Top-1⇑\n\u0017 77.9\u0013 78.4\n(c) Validation accuracy\nFigure 12: Impact of skip connection. Here, results are shown for MobileViT-S model (5.7 M\nparameters) on the ImageNet-1k dataset. Results in (c) are with exponential moving average.\nPatch sizes # Params. Time⇓ Top-1⇑\n2,2,2 5.7 M 9.85 ms 78.4\n3,3,3† 5.7 M 14.69 ms 78.5\n4,4,4 5.7 M 8.23 ms 77.6\n8,4,2 5.7 M 8.20 ms 77.3\nTable 6: Impact of patch sizes. Here, the patch sizes are for spatial levels at 32 ×32, 16 ×16, and\n8 ×8, respectively. Also, results are shown for MobileViT-S model on the ImageNet-1k dataset.\nResults are with exponential moving average. †Spatial dimensions of feature map are not multiple\nof patch dimensions. Therefore, we use bilinear interpolation in folding and unfolding operations to\nresize the feature map.\nis computed over patches with spatial dimensions of h and w, it is essential to establish a good\nrelationship between n, h, and w. Following previous works on CNN designs, we set n = 3\nand then vary h and w. Speciﬁcally, we study four conﬁgurations: (i) h = w = 2 at all spatial\nlevels (Figure 13a). In this case, h, w < nand would allow each pixel to encode information from\nevery other pixel using MobileViT. (ii) h = w = 3 at all spatial levels (Figure 13b). In this case,\nh = w = n. Similar to (i), this conﬁguration would also allow each pixel to encode information\nfrom every other pixel using MobileViT. (iii) h = w = 4 at all spatial levels (Figure 13c). In this\ncase, h, w > nand would not allow each pixel to aggregate information from other pixels in the\ntensor. (iv) h = w = 8, h = w = 4, and h = w = 2at spatial level of 32 ×32, 16 ×16, and 8 ×8,\nrespectively. Unlike (i), (ii), and (iii), the number of patches N is the same across different spatial\nresolutions in (iv). Also, h, w < nonly for a spatial level of 8 ×8 where h = w = 2. Note that\nall these models have the same number of network parameters and the same computational cost of\nself-attention, i.e., O(N2Pd). Here, N is the number of patches, P = hw is the number of pixels\nin a patch with height h and width w, and d is the model dimension.\nResults are shown in Table 6. We can see that whenh, w≤n, MobileViT can aggregate information\nmore effectively, which helps improve performance. In our experiments, we used h = w = 2\ninstead of h = w = 3 because spatial dimensions of feature maps are multiples of 2, and using\n16\nPublished as a conference paper at ICLR 2022\n1\n(a) h = w = 2< n= 3\n1 (b) h = w = n = 3\n1 (c) h = w = 4> n= 3\nFigure 13: Relationship between kernel size (n ×n) for convolutions and patch size (h ×w)\nfor folding and unfolding in MobileViT.In a and b, the red pixel is able to aggregate information\nfrom all pixels using local ( cyan colored arrows) and global ( orange colored arrows) information\nwhile in (c), every pixel is not able to aggregate local information using convolutions with kernel\nsize of 3 ×3 from 4 ×4 patch region. Here, each cell in black and gray grids represents a patch and\npixel, respectively.\nLS EMA Top-1 ⇑\n\u0017 \u0017 78.0\n\u0013 \u0017 78.3\n\u0013 \u0013 78.4\nTable 7: Effect of label smoothing (LS) and exponential moving average (EMA) on the perfor-\nmance of MobileViT-S on the ImageNet-1k dataset.First row results are with cross-entropy.\nh = w = 3 requires additional operations. For folding and unfolding, we need to either pad or\nresize. In the case of padding, we need to mask the padded pixels in self-attention in transformers.\nThese additional operations result in latency, as shown in Table 6. To avoid these extra operations,\nwe choose h = w = 2 in our experiments, which also provides a good trade-off between latency\nand accuracy.\nImpact of exponential moving average and label smoothing.Exponential moving average (EMA)\nand label smoothing (LS) are two standard training methods that are used to improve CNN- and\nTransformer-based models performance (Sandler et al., 2018; Howard et al., 2019; Tan et al., 2019;\nTouvron et al., 2021a; Dai et al., 2021; Xiao et al., 2021). Table 7 shows that LS marginally improves\nthe performance of MobileViT-S while EMA has little or no effect on model’s performance on the\nImageNet-1k dataset. Because previous works have shown these methods to be effective in reducing\nstochastic noise and prevent network from becoming over-conﬁdent, we use these methods to train\nMobileViT models.\nD T RAINING DETAILS FOR SSDL ITE AND DEEP LABV 3\nAll SSDLite-MobileViT and DeepLabv3-MobileViT networks are trained for 200 and 50 epochs\nwith a standard sampler on 4 NVIDIA GPUs and with an effective batch size of 128 images, re-\nspectively. The learning rate is increased from 0.00009 to 0.0009 in the ﬁrst 500 iterations and\nthen annealed to 0.00009 using a cosine learning rate scheduler. We use L2 weight decay of 0.01.\nWe change the stride of MV2 block from two to one at an output stride of 32 in Table 4 to obtain\nDeepLabv3-MobileViT models at an output stride of 16.\nFor these models, we do not use a multi-scale sampler. This is because these task-speciﬁc networks\nare resolution-dependent. For example, DeepLabv3 uses an atrous (or dilation) rate of 6, 12, and\n18 at an output stride of 16 to learn multi-scale representations. If we use a lower resolution (say\n256 ×256) than 512 ×512, then the atrous kernel weights will be applied to padded zeros; making\nmulti-scale learning ineffective.\n17\nPublished as a conference paper at ICLR 2022\nE E XTENDED DISCUSSION\nMemory footprint. A light-weight network running on mobile devices should be memory efﬁ-\ncient. Similar to MobileNetv2, we measure the memory that needs to be materialized at each spatial\nlevel (Table 8). At lower spatial levels (i.e., an output stride of 8, 16, and 32) where MobileViT\nblocks are employed, required memory is lesser or comparable to light-weight CNNs. Therefore,\nsimilar to light-weight CNNs, MobileViT networks are also memory efﬁcient.\nFLOPs. Floating point operations (FLOPs) is another metric that is widely used to measure the\nefﬁciency of a neural network. Table 9 compare FLOPs of MobileViT with different ViT-based\nnetworks on the ImageNet-1k dataset. For similar number of FLOPs, MobileViT is faster, smaller,\nand better. For instance, PiT and MobileViT has the same number of FLOPs, but MobileViT is\n1.45×faster, 2.1×smaller, and1.8% better (R2 vs. R4 in Table 9). It is important to note that FLOPs\nfor networks in R2-R4 are the same, but their latency and performance are different. This shows that\nFLOPs is not a sufﬁcient metric for network efﬁciency as it does not account for inference-related\nfactors such as memory access, degree of parallelism, and platform characteristics.\nThe ImageNet-1k pre-training helps in performance improvement in down-stream tasks such as\nobject detection and semantic segmentation (Long et al., 2015; Chen et al., 2017; Redmon & Farhadi,\n2017). Because such tasks are used in real-world applications and often uses higher image inputs as\ncompared to the ImageNet-1k classiﬁcation task, it is important to compare the FLOPs of a network\non down-stream tasks. Towards this end, we compare the FLOPs of MobileViT with MobileNetv2\non three tasks, i.e., classiﬁcation, detection, and segmentation. Results are shown in Table 10. We\ncan observe that (1) the gap between MobileNetv2 and MobileViT FLOPs reduces as the input\nresolution increases. For instance, MobileNetv2 has 2×fewer FLOPs as compared to MobileViTon\nthe ImageNet-1k classiﬁcation task, but on the semantic segmentation, they have similar FLOPs\n(Table 10a vs. Table 10c) and (2) MobileNetv2 models are signiﬁcantly faster but less accurate than\nMobileViT models across different tasks. The low-latency of MobileNetv2 models is likely because\nof dedicated and optimized hardware-level operations on iPhone. We believe that (1) the inference\nspeed of MobileViT will further improve with such dedicated operations and (2) our results will\ninspire future research in the area of hardware design and optimization.\nInference time on different devices.Table 11 compares the inference time of different models\non three different devices, i.e., iPhone12 CPU, iPhone12 neural engine, and NVIDIA V100 GPU.\nMobileNetv2 is the fastest network across all devices. On iPhone (both CPU and neural engine),\nMobileViT delivers better performance as compared to DeIT and PiT. However, on GPU, DeIT\nand PiT are faster than MobileViT. This is likely because MobileViT models (1) are shallow and\nnarrow, (2) run at higher spatial resolution ( 256 ×256 instead of 224 ×224), and (2) did not use\nOS MobileNetv2-1.0 MobileViT-XS\n2 400 7844 200 2948 100 9816 62 3132 32 37\nTop-1 73.3 74.8\nTable 8: Comparison between MobileNetv2 and MobileViT in terms of maximum memory (in\nkb) that needs to be materialized at each spatial resolution in the network.The top-1 accuracy\nis measured on the ImageNet-1k validation set. Here, OS (output stride) is the ratio of spatial\ndimensions of the input to the feature map.\nModel # Params. ⇓ FLOPs⇓ Time⇓ Top-1⇑\n(R1) DeIT 5.7 M 1.3 G 10.99 ms 72.2(R2) PiT 4.9 M 0.7 G 10.56 ms 73.0(R3) MobileViT-XS (Ours; 8,4,2)2.3 M 0.7 G 5.93 ms 73.8(R4) MobileViT-XS (Ours; 2,2,2)2.3 M 0.7 G 7.28 ms 74.8\nTable 9: Comparison of different ViT-based networks. The performance of MobileViT-XS model\nis reported at two different patch-size settings. See §A for details.\n18\nPublished as a conference paper at ICLR 2022\nModel # Params. ⇓ FLOPs⇓ Time⇓ Top-1⇑\nMobileNetv2 3.5 M 0.3 G 0.92 ms73.3MobileViT-XS (Ours; 8,4,2)2.3 M 0.7 G 5.93 ms 73.8MobileViT-XS (Ours; 2,2,2)2.3 M 0.7 G 7.28 ms74.8\n(a) ImageNet-1k classiﬁcation\nBackbone # Params. ⇓ FLOPs⇓ Time⇓ mAP⇑\nMobileNetv2 4.3 M 0.8 G 2.3 ms22.1MobileViT-XS (Ours; 8,4,2)2.7 M 1.6 G 10.7 ms 23.1MobileViT-XS (Ours;2,2,2)2.7 M 1.6 G 12.6 ms24.8\n(b) Object detection w/ SSDLite.\nBackbone # Params.⇓ FLOPs⇓ Time⇓ mIOU⇑\nMobileNetv2 4.3 M 5.8 G 6.5 ms75.7MobileViT-XS (Ours)2.9 M 5.7 G 25.1 ms 75.4MobileViT-XS (Ours)2.9 M 5.7 G 32.3 ms77.1\n(c) Semantic segmentation w/ DeepLabv3.\nTable 10: MobileViT vs. MobileNetv2 on different tasks. The FLOPs and inference time in (a),\n(b) and (c) are measured at 224 ×224, 320 ×320, and 512 ×512 respectively with an exception\nto MobileViT-XS model in (a) which uses256 ×256 as an input resolution for measuring inference\ntime on iPhone 12 neural engine. Here, the performance of MobileViT-XS models is reported at two\ndifferent patch-size settings. See §A for details.\nModel # Params⇓ FLOPs⇓ Top-1⇑ Inference time⇓\niPhone12 - CPU iPhone12 - Neural Engine NVIDIA V100 GPU\nMobileNetv2 3.5 M 0.3 G 73.3 7.50 ms 0.92 ms 0.31 msDeIT 5.7 M 1.3 G 72.2 28.15 ms 10.99 ms 0.43 msPiT 4.9 M 0.7 G 73.0 24.03 ms 10.56 ms 0.46 msMobileViT (Ours)2.3 M 0.7 G 74.8 17.86 ms 7.28 ms 0.62 ms/0.47 ms †\nTable 11: Inference time on different devices.The run time of MobileViT is measured at256×256\nwhile for other networks, it is measured at 224 ×224. For GPU, inference time is measured for a\nbatch of 32 images while for other devices, we use a batch size of one. Here, †represents that Mo-\nbileViT model uses PyTorch’s Unfold and Fold operations. Also, patch sizes for MobileViT model\nat an output stride of 8, 16, and 32 are set to two.\nGPU- accelerated operations for folding and unfolding as they are not supported on mobile devices.\nHowever, when we replaced ourunoptimized fold and unfold operations with PyTorch’s Unfold and\nFold operations, the latency of MobileViT model is improved from 0.62 ms to 0.47 ms.\nOverall, our ﬁndings suggest that they are opportunities for optimizing ViT-based models, includ-\ning MobileViT, for different accelerators. We believe that our work will inspire future research in\nbuilding more efﬁcient networks.\nF Q UALITATIVE RESULTS ON THE TASK OF OBJECT DETECTION\nFigures 15, 14, and 16 shows that SSDLite with MobileViT-S can detect different objects under\ndifferent settings, including changes in illumination and viewpoint, different backgrounds, and non-\nrigid deformations.\n19\nPublished as a conference paper at ICLR 2022\nFigure 14: Object detection resultsof SSDLite-MobileViT-S on the MS-COCO validation set.\n20\nPublished as a conference paper at ICLR 2022\nFigure 15: Object detection resultsof SSDLite-MobileViT-S on the MS-COCO validation set.\n21\nPublished as a conference paper at ICLR 2022\nFigure 16: Object detection resultsof SSDLite-MobileViT-S on the MS-COCO validation set.\n22\nPublished as a conference paper at ICLR 2022\nG S EMANTIC SEGMENTATION RESULTS ON AN UNSEEN DATASET\nTo demonstrate that MobileViT learns good generalizable representations of objects, we evaluate\nthe DeepLabv3-MobileViT model in Section 4.2.2 on the MS-COCO validation set that contains\n5k images. Following ofﬁcial torchvision segmentation models (PyTorch, 2021), object classes in\nthe MS-COCO dataset are mapped to the object classes in the PASCAL VOC dataset and models\nare evaluated in terms of mIOU. Note that the MS-COCO validation set is an unseen test set for\nDeepLabv3-MobileViT models because these images are neither part of the training nor the valida-\ntion set used for training DeepLabv3-MobileViT models.\nTable 12 compares the performance of DeepLabv3-MobileViT models with MobileNetv3-Large\nthat was trained with three different segmentation backbones (LR-ASPP (Howard et al., 2019),\nDeepLabv3, and FCN (Long et al., 2015)). For the same segmentation model, i.e., DeepLabv3,\nMobileViT is a more effective backbone than MobileNetv3. DeepLabv3-MobileViT-S model is\n1.7×smaller and 5.1% more accurate than DeepLabv3-MobileNetv3-Large model. Furthermore,\nqualitative results in Figure 17 and Figure 18 further demonstrates that MobileViT learns good gen-\neralizable representations of the objects and perform well in the wild.\nModel # Params ⇓ mIOU⇑\nLR-ASPP w/ MobileNetV3-Large 3.2 M 57.9FCN w/ MobileNetV3-Large 5.1 M 57.8\nDeepLabv3 w/ MobileNetV3-Large 11.0 M 60.3DeepLabv3 w/ MobileViT-XXS (Ours)1.9 M 46.7DeepLabv3 w/ MobileViT-XS (Ours) 2.9 M 57.4DeepLabv3 w/ MobileViT-S (Ours) 6.4 M65.4\nTable 12: Semantic segmentation on the MS-COCO validation set.MobileNetv3-Large results\nare from ofﬁcial torchvision segmentation models (PyTorch, 2021).\n23\nPublished as a conference paper at ICLR 2022\nFigure 17: Semantic segmentation resultsof Deeplabv3-MobileViT-S model on the unseen MS-\nCOCO validation set ( left: input RGB image, middle: predicted segmentation mask, and right:\nSegmentation mask overlayed on RGB image). Color encoding for different objects in the PASCAL\nVOC dataset is shown in the last row.\n24\nPublished as a conference paper at ICLR 2022\nFigure 18: Semantic segmentation resultsof Deeplabv3-MobileViT-S model on the unseen MS-\nCOCO validation set ( left: input RGB image, middle: predicted segmentation mask, and right:\nSegmentation mask overlayed on RGB image). Color encoding for different objects in the PASCAL\nVOC dataset is shown in the last row.\n25\nPublished as a conference paper at ICLR 2022\n1 i m p o r t t o r c h\nfrom t o r c h . u t i l s . d a t a . s a m p l e r i m p o r t Sampler\n3 i m p o r t t o r c h . d i s t r i b u t e d a s d i s t\ni m p o r t math\n5 i m p o r t random\ni m p o r t numpy a s np\n7\n9 c l a s s MultiScaleSamplerDDP ( Sampler ) :\nd e f i n i t ( s e l f , base im w : i n t , b a s e i m h : i n t , b a s e b a t c h s i z e : i n t , n d a t a s a m p l e s : i n t ,\n↪→ m i n s c a l e m u l t : f l o a t = 0 . 5 , m a x s c a l e m u l t : f l o a t = 1 . 5 , n s c a l e s : i n t = 5 , i s t r a i n i n g : b o o l =\n↪→ F a l s e ) −> None :\n11 # min . and max . s p a t i a l d i m e n s i o n s\nmin im w , max im w = i n t ( base im w * m i n s c a l e m u l t ) , i n t ( base im w * m a x s c a l e m u l t )\n13 min im h , max im h = i n t ( b a s e i m h * m i n s c a l e m u l t ) , i n t ( b a s e i m h * m a x s c a l e m u l t )\n15 # Get t h e GPU and node r e l a t e d i n f o r m a t i o n\nn u m r e p l i c a s = d i s t . g e t w o r l d s i z e ( )\n17 r a n k = d i s t . g e t r a n k ( )\n19 # a d j u s t t h e t o t a l s a m p l e s t o a v o i d b a t c h d r o p p i n g\nn u m s a m p l e sp e r r e p l i c a = i n t ( math . c e i l ( n d a t a s a m p l e s * 1 . 0 / n u m r e p l i c a s ) )\n21 t o t a l s i z e = n u m s a m p l e sp e r r e p l i c a * n u m r e p l i c a s\ni m g i n d i c e s = [ i d x f o r i d x i n r a n g e ( n d a t a s a m p l e s ) ]\n23 i m g i n d i c e s += i m g i n d i c e s [ : ( t o t a l s i z e − n d a t a s a m p l e s ) ]\na s s e r t l e n ( i m g i n d i c e s ) == t o t a l s i z e\n25\ns e l f . s h u f f l e = F a l s e\n27 i f i s t r a i n i n g :\n# compute t h e s p a t i a l d i m e n s i o n s and c o r r e s p o n d i n g b a t c h s i z e\n29 w i d t h d i m s = l i s t ( np . l i n s p a c e ( min im w , max im w , n s c a l e s ) )\nh e i g h t d i m s = l i s t ( np . l i n s p a c e ( min im h , max im h , n s c a l e s ) )\n31 # ImageNet models down− sample images by a f a c t o r o f 3 2 .\n# Ensure t h a t w i d t h and h e i g h t d i m e n s i o n s a r e m u l t i p l e o f 3 2 .\n33 w i d t h d i m s = [ (w / / 32) * 32 f o r w i n w i d t h d i m s ]\nh e i g h t d i m s = [ ( h / / 32) * 32 f o r h i n h e i g h t d i m s ]\n35\ni m g b a t c h p a i r s = l i s t ( )\n37 b a s e e l e m e n t s = base im w * b a s e i m h * b a s e b a t c h s i z e\nf o r ( h , w) i n z i p ( h e i g h t d i m s , w i d t h d i m s ) :\n39 b a t c h s i z e = max ( 1 , ( b a s e e l e m e n t s / ( h * w) )\ni m g b a t c h p a i r s . append ( ( h , w, b a t c h s i z e ) )\n41 s e l f . i m g b a t c h p a i r s = i m g b a t c h p a i r s\ns e l f . s h u f f l e = True\n43 e l s e :\ns e l f . i m g b a t c h p a i r s = [ ( b a s e i m h , base im w , b a s e b a t c h s i z e ) ]\n45\ns e l f . i m g i n d i c e s = i m g i n d i c e s\n47 s e l f . n s a m p l e s p e r r e p l i c a = n u m s a m p l e sp e r r e p l i c a\ns e l f . epoch = 0\n49 s e l f . r a n k = r a n k\ns e l f . n u m r e p l i c a s = n u m r e p l i c a s\n51\nd e f i t e r ( s e l f ) :\n53 i f s e l f . s h u f f l e :\nrandom . s e e d ( s e l f . epoch )\n55 random . s h u f f l e ( s e l f . i m g i n d i c e s )\nrandom . s h u f f l e ( s e l f . i m g b a t c h p a i r s )\n57 i n d i c e s r a n k i = s e l f . i m g i n d i c e s [ s e l f . r a n k : l e n ( s e l f . i m gi n d i c e s ) : s e l f . n u mr e p l i c a s ]\ne l s e :\n59 i n d i c e s r a n k i = s e l f . i m g i n d i c e s [ s e l f . r a n k : l e n ( s e l f . i m gi n d i c e s ) : s e l f . n u mr e p l i c a s ]\n61 s t a r t i n d e x = 0\nw h i l e s t a r t i n d e x < s e l f . n s a m p l e s p e r r e p l i c a :\n63 c u r r h , curr w , c u r r b s z = random . c h o i c e ( s e l f . i m g b a t c h p a i r s )\n65 e n d i n d e x = min ( s t a r t i n d e x + c u r r b s z , s e l f . n s a m p l e s p e r r e p l i c a )\nb a t c h i d s = i n d i c e s r a n k i [ s t a r t i n d e x : e n d i n d e x ]\n67 n b a t c h s a m p l e s = l e n ( b a t c h i d s )\ni f n b a t c h s a m p l e s != c u r r b s z :\n69 b a t c h i d s += i n d i c e s r a n k i [ : ( c u r r b s z − n b a t c h s a m p l e s ) ]\ns t a r t i n d e x += c u r r b s z\n71\ni f l e n ( b a t c h i d s ) > 0 :\n73 b a t c h = [ ( c u r r h , curr w , b i d ) f o r b i d i n b a t c h i d s ]\ny i e l d b a t c h\n75\nd e f s e t e p o c h ( s e l f , epoch : i n t ) − > None :\n77 s e l f . epoch = epoch\nListing 1: PyTorch implementation of multi-scale sampler\n26",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7097644805908203
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6419700980186462
    },
    {
      "name": "Transformer",
      "score": 0.6338926553726196
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5761176943778992
    },
    {
      "name": "De facto",
      "score": 0.5142883658409119
    },
    {
      "name": "Mobile device",
      "score": 0.47685015201568604
    },
    {
      "name": "Computer vision",
      "score": 0.39254945516586304
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.36667299270629883
    },
    {
      "name": "Engineering",
      "score": 0.11053189635276794
    },
    {
      "name": "Electrical engineering",
      "score": 0.09212389588356018
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": []
}