{
  "title": "Implicit Language Model in LSTM for OCR",
  "url": "https://openalex.org/W2786019807",
  "year": 2017,
  "authors": [
    {
      "id": "https://openalex.org/A2724413328",
      "name": "Ekraam Sabir",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2342121770",
      "name": "Stephen Rawls",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2135073364",
      "name": "Prem Natarajan",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2724413328",
      "name": "Ekraam Sabir",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2342121770",
      "name": "Stephen Rawls",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2135073364",
      "name": "Prem Natarajan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2112796928",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2060580591",
    "https://openalex.org/W6638545294",
    "https://openalex.org/W2127141656",
    "https://openalex.org/W6637409405",
    "https://openalex.org/W6683078286",
    "https://openalex.org/W2010595692",
    "https://openalex.org/W4246411900",
    "https://openalex.org/W2168868236",
    "https://openalex.org/W4254816979",
    "https://openalex.org/W46134239",
    "https://openalex.org/W2171498274",
    "https://openalex.org/W2142069714",
    "https://openalex.org/W2147800946",
    "https://openalex.org/W6640820311",
    "https://openalex.org/W6638154518",
    "https://openalex.org/W2099106019",
    "https://openalex.org/W2763468124",
    "https://openalex.org/W4388297464",
    "https://openalex.org/W2123772730",
    "https://openalex.org/W1951216520",
    "https://openalex.org/W1689711448",
    "https://openalex.org/W1788185260",
    "https://openalex.org/W1815076433",
    "https://openalex.org/W1554663460",
    "https://openalex.org/W581956982",
    "https://openalex.org/W2964335273",
    "https://openalex.org/W2159505618",
    "https://openalex.org/W2170942820",
    "https://openalex.org/W2088622394",
    "https://openalex.org/W2110485445"
  ],
  "abstract": "Neural networks have become the technique of choice for OCR, but many aspects\\nof how and why they deliver superior performance are still unknown. One key\\ndifference between current neural network techniques using LSTMs and the\\nprevious state-of-the-art HMM systems is that HMM systems have a strong\\nindependence assumption. In comparison LSTMs have no explicit constraints on\\nthe amount of context that can be considered during decoding. In this paper we\\nshow that they learn an implicit LM and attempt to characterize the strength of\\nthe LM in terms of equivalent n-gram context. We show that this implicitly\\nlearned language model provides a 2.4\\\\% CER improvement on our synthetic test\\nset when compared against a test set of random characters (i.e. not naturally\\noccurring sequences), and that the LSTM learns to use up to 5 characters of\\ncontext (which is roughly 88 frames in our configuration). We believe that this\\nis the first ever attempt at characterizing the strength of the implicit LM in\\nLSTM based OCR systems.\\n",
  "full_text": "Implicit Language Model in LSTM for OCR\nEkraam Sabir\nInformation Sciences Institute\nUniversity of Southern California\nMarina Del Rey, California 90007\nEmail: esabir@isi.edu\nStephen Rawls\nInformation Sciences Institute\nUniversity of Southern California\nMarina Del Rey, California 90007\nEmail:srawls@isi.edu\nPrem Natarajan\nInformation Sciences Institute\nUniversity of Southern California\nMarina Del Rey, California 90007\nEmail: pnataraj@isi.edu\nAbstract—Neural networks have become the technique of\nchoice for OCR, but many aspects of how and why they deliver\nsuperior performance are still unknown. One key difference\nbetween current neural network techniques using LSTMs and\nthe previous state-of-the-art HMM systems is that HMM systems\nhave a strong independence assumption. In comparison LSTMs\nhave no explicit constraints on the amount of context that can\nbe considered during decoding. In this paper we show that they\nlearn an implicit LM and attempt to characterize the strength\nof the LM in terms of equivalent n-gram context. We show that\nthis implicitly learned language model provides a 2.4% CER\nimprovement on our synthetic test set when compared against\na test set of random characters (i.e. not naturally occurring\nsequences), and that the LSTM learns to use up to 5 characters\nof context (which is roughly 88 frames in our conﬁguration). We\nbelieve that this is the ﬁrst ever attempt at characterizing the\nstrength of the implicit LM in LSTM based OCR systems.\nI. I NTRODUCTION\nAt the heart of any Optical Character Recognition (OCR)\nsystem is a glyph recognition model whose purpose is to iden-\ntify individual glyphs based on extracted features. However, in\naddition to the glyph model, OCR systems typically employ\nfeature extraction, segmentation and language modeling mod-\nules to get competitive performance [1]. Of these, language\nmodeling improves the output of a glyph recognition model\nconditioned on the distribution of characters or words from\ntask-relevant-text i.e. a language model. A language model\ncan build a word or a character language model [2]. [3] gives\na general survey on the use of language modeling. Irrespective\nof the OCR method, language modeling has been investigated\nindependently and has also played a crucial role in achieving\nbetter performance [4][5].\nHidden Markov Model (HMM) based systems provided\nsegmentation free OCR and outperformed then existing seg-\nmentation based approaches [6][7]. This conditional indepen-\ndence limitation was addressed by Recurrent Neural Networks\n(RNNs) [8] which theoretically have no limitations on the\nlength of context they can utilize.\nWhile Neural Networks have been used with success for\nOCR in the past [9][10], it is only recently that recurrent\nnetworks and particularly LSTMs [11] became popular for\nthe OCR task, improving upon the performance of HMM\nOCR systems [12][13]. Solutions to the vanishing and ex-\nploding gradient problems associated with the training of\nRNNs [14][11] coupled with the introduction of Connectionist\nTemporal Classiﬁcation (CTC) loss [15] played a major role in\nthis resurgence. The CTC loss was particularly well suited to\ntackling the OCR problem, removing the necessity for frame\nlevel label assignment.\nThe convincing performance improvements made by\nLSTMs however stand in stark contrast to the limited inter-\npretability of these networks. The functionality of individual\nneurons, weights and to some extent the hidden layers them-\nselves remains ambiguous at best. To this end, a signiﬁcant\namount of effort has been expended in explaining LSTMs\nregarding their structure such as performance of LSTM vari-\nants with and without hyperparameter tuning and the effects\nof depth [16][17][18]. [19] and [20] explore the memory\nand functionality of neurons with experiments that test long\nterm reasoning among others, on a character language model\nlearning task.\nContinuing in the general direction of unraveling LSTMs,\nwe explore their possibility of learning a language model when\ntrained on a different but related OCR task. Foundational\ncredibility for LSTMs learning an internal language model\nwhen trained for OCR can be enumerated from previous\ndiscussion as follows: 1) LSTMs do not have an explicit\nrestriction on the amount of context they can learn; 2) They\nhave been shown to learn character language models when\ntrained for it speciﬁcally as in [20]; and 3) Learning a language\nmodel in general helps improve performance on the OCR\ntask. We ﬁnd additional evidence for this idea in [21] where\nan LSTM is trained on a multilingual OCR task. The setup\ninvolves testing multiple LSTM models which are trained on\none native language and tested on other foreign languages with\nthe same glyphs. The results on a real world problem show\nup to 3.6% CER difference in performance when testing on\nforeign languages, which is indicative of the model’s reliance\non the native language model. However, the authors of [21]\ndo not explain this phenomena.\nIn this paper we attempt to advance our scientiﬁc un-\nderstanding of LSTMs, particularly the interactions between\nlanguage model and glyph model present within an LSTM.\nWe call this internal language model the implicit language\nmodel (implicit LM). Our contributions in this paper include:\n1) Establishing the presence of implicit LM under controlled\nconditions; and 2) characterizing the nature of implicit LM\nby ﬁnding how many characters of context it makes use\nof. The implicit LM we characterize is different from the\nlanguage model in [19][20] discussed above in that the setting\narXiv:1805.09441v1  [cs.CV]  23 May 2018\nFig. 1. A visualization of the context and glyph frames for identifying the\nglyph a with respect to surrounding characters. Each frame is an OCR input\nfor one time-step.\nand requirement for learning a language model is different:\nOCR explicitly requires learning a glyph model instead of a\nlanguage model. A recent benchmarking paper on the use of\nLSTM for OCR [22] has not covered this and to the best of\nour knowledge has also not been covered in literature.\nII. METHODOLOGY\nThe implicit LM is a learned aspect of the LSTM, whose\ncontextual extent cannot be evaluated by known methods or\nmetrics. It is also intertwined with the LSTM’s learned glyph\nmodel, which is expected to rely on the glyph frames of a\ncharacter, while the implicit LM relies on the context frames\nas shown in Figure 1. Any measure of performance on the\nOCR task is a result of both aspects of the model working\nin unison to predict a character. In such conditions, it is not\npossible to isolate the contributions of implicit LM by a single\nmeasure of performance on any data. This makes analysis of\nimplicit LM a challenging task and any approach to distinguish\nits contributions has to be novel. We attempt to characterize the\nlanguage model by a series of experiments which together give\na clearer picture and address the entangled issues described.\nAn LSTM beneﬁting from context is nothing new in itself.\nIn OCR, this temporal aspect of an LSTM allows it to take\nslices of image across variable width characters and recognize\nit. However, we create synthetic datasets where characters are\nnot connected and a sufﬁciently complex model could learn\nto rely on glyph frames alone for recognition. In such condi-\ntions we measure performance on test sets which deliberately\ndeviate from the training character language model, to show\nthat an LSTM beneﬁts from assimilated memory of context\nframes, in addition to glyph frames. This idea is the basis for\nthe Shufﬂed Character and N-gram experiments in Section 4.\nEven though LSTMs do not have an explicit restriction\non memory, as with n-gram language models the amount\nof useful context is limited. When testing on increasing\nlength sequences seen in training, we expect the performance\nimprovement to saturate beyond a certain length. The length\nat which the improvement plateaus should be the length\nof character context implicit LM considers in predicting a\ncharacter. This gives us motivation for the N-gram experiments\nin Section 4.\nSince LSTMs by design learn context over time-steps in-\nstead of characters, an argument can be made for measuring\nimplicit LM over time-steps. A time-step based approach\nhowever, may not be a consistent indicator across proportional\nfonts and different font sizes. Additionally, the CTC loss\nmakes character predictions at varying intermediate frames\nof characters, giving a special blank prediction for all other\nframes. A control over font size, font type and character to\nget a reliable estimate of the memory in implicit LM is an\nextremely constrained setting to perform experiments in. A\ncharacter level estimate on other hand though not perfect, has\nthe added advantage of being interpretable.\nIII. EXPERIMENTAL SETUP\nA. Data\nThe experiments we perform require controlled datasets of\nﬁxed length sequences with speciﬁc requirements, which is\neasily created from synthetic images but intractable to ﬁnd in\nreal world data. Fortunately our objective is to advance the un-\nderstanding of LSTMs and not directly effect an improvement\nin performance, thus real world data is not explicitly required.\nUsing synthetic images over handwritten image datasets has\nthe added advantage of eliminating image background noise\nfrom interfering with experiment results.\nFor training, we intend to train a robust model that gener-\nalizes reasonably well to variations in fonts, font sizes. We\nrender 32,180 unique sentences from the books Tale of Two\nCities and Ivanhoe with font sizes in the range 8 to 16 and 6\nfonts: Californian FB Italic, Garamond, Georgia, Arial, Comic,\nCourier Italic for each sentence. If the model is found to learn\na character language model, it would be from this set of 32,180\nsentences. The validation images are from The Adventures\nof Tom Sawyer and have 1585 unique sentences. They are\nrendered in a similar fashion and with the same fonts as in\ntraining images.\nWe choose test fonts different from training fonts and with\nlarge enough error to be reliably measurable. The training fonts\nwhen used for testing give near 0% error. The test fonts are\nComic Bold, Times and Arial Narrow. Sample images of all\nthe fonts are shown in Figure 2. The test dataset for shufﬂed\ncharacters experiment in section 4 contains 3742 sentences\nfrom Wuthering Heights.\nFor the remaining experiments, multiple test datasets of\nsmall case character sequences or N-grams are created. The\nN-gram may be Seen or Unseen in training. We consider an\nN-gram to be Seen if it has a frequency above 10 in the unique\ntraining sentences and Unseen for 0 frequency. N-grams with\nfrequency between 0-10 are ignored. We create additional sets\nof N-grams which we call Purely Unseen. They are N-grams\nwhose sub N-grams are also Unseen. For instance an Unseen\n5-gram ameoy has one Seen 3-gram ame and four Seen 2-\ngrams am, me, eo and oy. The sequence bcgpq is an example\nof a Purely Unseen N-gram. All N-gram test data comprises of\n26 small case English alphabets. Even though spaces appear\nabundantly in training, we remove them since the model\nmight trip on inconsequential sequences of consecutive spaces\nFig. 2. Sample images of the fonts used in training and test. From top to\nbottom: training: Californian FB Italic, Garamond, Georgia, Arial, Comic,\nCourier Italic and test: Times, Comic Bold and Arial Narrow\nleading to an exaggerated estimate of implicit LM. Each N-\ngram test set of some ﬁxed length and Seen, Unseen or Purely\nUnseen type has up to 10,000 samples, which should be large\nenough to measure error reliably.\nB. Preprocessing\nTo ensure a constant input size to the model, images are\nscaled to a constant height of 30 pixels while conserving the\naspect ratio. They are also normalized to have zero mean and\nunit standard deviation as has been recommended in [23].\nC. Model\nFor a reasonably well performing OCR model, we choose\none that is very similar in structure to [24] but not ﬁne-\ntuned like it. The model is segmentation free and the LSTM\noutput does not require processing except for decoding. It takes\nsliding window image frames of 2 pixel width transformed\ninto a 60x1 vector as raw input features for 2x fully connected\nlayers with 60 units each. All activation functions are Rectiﬁed\nLinear Units (ReLU). It is followed by 2 bidirectional LSTM\nlayers with 256 units in each layer and 700 time-steps, where\neach time-step is a potential character prediction. The objective\nfunction is the CTC loss function [15]. The model architecture\nis shown in Figure 2.\nD. Training\nThe model is trained with a starting learning rate of 0.001\nand dropout 0.5. These numbers turn out to be sufﬁcient to\ntrain the model and are not ﬁne-tuned. It is trained for over\n1 epoch with 0.04% CER and 0.02% WER on training and\n0.02% CER and 0.01% WER on validation.\nE. Testing\nWe measure error in terms of Character Error Rate (CER)\nthroughout, ignoring Word Error Rate (WER). For comparable\nCER, the WER of a longer sequence will be inevitably larger\nFig. 3. An overview of the entire system. The feature extraction layer takes\nimage frame input and the LSTM layers function as recognition layers.\nthan that for a shorter sequence. A single CER is reported on a\ntest dataset of some ﬁxed length containing N-grams of Seen,\nUnseen or Purely Unseen type.\nIV. EXPERIMENTS\nWe present the results of experiments in the following\nsubsections. While each experiment on its own is not sufﬁcient\nto characterize the implicit LM, together they present a more\ncoherent picture.\nA. Shufﬂed Characters Experiment\nIn [21] we see that an LSTMs performance improves\nby up to 3.6% CER when using a mixed language model\ntraining setup instead of the original character language model.\nHowever, the authors of [21] do not investigate the possibility\nof an internal language model. We ﬁrst establish the pres-\nence of an implicit LM with an experiment on a controlled\ndataset. The test dataset for this experiment consists of full\nlength English sentences sampled from Wuthering Heights and\nrendered in test fonts. We shufﬂe the characters randomly\nin these sentences and re-render them, resulting in a dataset\nwith same characters in corresponding sentences, but with a\nrandom character language model. Ideally the performance\nbetween these two sets should be the same, and any difference\nshould come from the implicit LM. Table 1 shows the results\nfrom these experiments. Font Arial Narrow shows the biggest\ndifference with CER up to 2.4% better on normal English\nlanguage sentences.\nTABLE I\nDIFFERENCE IN CER AND WER MEASURE OF NORMAL TEST SENTENCES\nAND SHUFFLED TEST SENTENCES\nFont CER WER\nNormal Shufﬂed Normal Shufﬂed\nComic bold 2.4% 2.6% 6.4% 8.9%\nTimes 7.2% 8.6% 25.1% 26.5%\nArial Narrow 0.5% 2.9% 2.1% 6.8%\nB. N-gram Experiment\nWe have already established the presence of implicit LM\nin the shufﬂed characters experiment. The objective of this\nexperiment is to quantify its contextual limit in terms of\ncharacters. As with other language models, the contextual\nbeneﬁts from implicit LM should be limited. Test sets with\nlonger sequences should beneﬁt more from the extra characters\nup to a certain length after which the implicit LM should\nsaturate in performance. Our hypothesis is that the perfor-\nmance will improve as length increases and plateau where\nimplicit LM stops considering more context frames. We run\nthese experiments on Seen 2-gram to 7-gram test sets which\nderive their language model from the training set. In Table 2\nwe observe that the performance stops improving beyond 5\ncharacters, indicating that the implicit LM can beneﬁt from\nup to 5 characters in context for a bidirectional LSTM model.\nThis corresponds to 88 frames of input in our conﬁguration\nfor font size 16 on the widest test font comic bold.\nWhile the reasoning, of the above analysis is sound, it is\nnot complete by itself. It is possible that ﬂuctuations in the\nfrequency of characters across test sets of different length may\ninﬂuence the experiments. To resolve any ambiguities from\nthis, we inspect the performance on some characters across\n2-gram to 5-gram datasets. The results are shown in Table 3.\nWe reassert results from the shufﬂed characters experiment\nby evaluating CER on Purely Unseen 2-gram to 5-gram\ndatasets. We omit 6 and 7-grams datasets for lack of sufﬁcient\nsamples. The N-grams in this case not only do not follow\nthe language model seen in training, but also go out of their\nway in ensuring that any subsequence seen in training is\nnot repeated while testing. Going with the complementary\nreasoning presented in the evaluation of Seen experiments, we\ndo not expect the performance to improve as length increases.\nThe error should also stay consistently above the error on Seen\ntest sets. The results presented in Table 2 are consistent with\nour reasoning. We also perform and show results onUnseen N-\ngram datasets, in Table 2. Consistent with previous reasoning\nit has consistently worse performance than Seen datasets, but\nshows improvement with increasing N-gram length due to\nsubsequences which have been Seen in training.\nC. What about other fonts?\nThe fonts highlighted in our experiments so far show\nimprovement across all characters on Seen sequences and\ntherefore the overall performance measure is consistent with\nhypotheses across all Seen N-gram test sets. However, that\nmay not always be the case as we show with the third testing\nfont Times Roman. The model has a proclivity for confusing\nonly two characters in this font: l gets confused for I and\ne for c on the Seen N-gram experiments. The performance\nfor l improves as N-increases dropping from 20.6% to 0.5%\nerror, however the performance for e stays approximately the\nsame around 72%. This exceptionally high error on a single\ncharacter forces the results on any test set to be dictated by the\nfrequency of e. In order to highlight this point, we run another\nset of Seen experiments where we recreate the datasets ﬁxing\nthe percentage of e to be the same as that of 2-gram test set\ni.e. 6%. We compare the results of both sets of experiments,\nwhere we regulate the ﬁnal percentage composition of e and\nwhere we do not in Table 4. The results become consistent\nfor hypotheses once we regulate the rogue character e. We\ninspect confusions on e for a possible explanation on why it\ndoes not show any improvement, but we do not come across\nanything credible. The mistakes are spread across all font sizes\nand across different preceding and succeeding characters.\nV. C ONCLUSION\nLSTM networks have been successful in OCR, but insight\ninto what they learn for a given task is still lacking. We present\nevidence that LSTMs when trained for the OCR task, learn an\nimplicit LM. We ﬁnd that implicit LM improves performance\nup to 2.4% CER when tested on synthetic English language\ndata. As a real world problem extension it has also been shown\nthat this implicit LM improves performance by up to 3.6%\nCER on a multilingual OCR task [21]. We also show that it\nmakes use of up to 5 characters in making predictions. It does\nnot necessarily help in making predictions on current character\nalways as we saw with the indifference in performance on\ncharacter e in Times font. All experiments were conducted\nusing English, but the general inference should hold good for\nany language.\nREFERENCES\n[1] R. Plamondon and S. N. Srihari, “Online and off-line handwriting\nrecognition: a comprehensive survey,” IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, vol. 22, no. 1, pp. 63–84, Jan. 2000.\n[2] A. L. Koerich, R. Sabourin, and C. Y . Suen, “Large vocabulary\noff-line handwriting recognition: A survey,” Pattern Analysis &\nApplications, vol. 6, no. 2, pp. 97–121, 2003. [Online]. Available:\nhttp://dx.doi.org/10.1007/s10044-002-0169-3\n[3] K. Kukich, “Techniques for Automatically Correcting Words in Text,”\nACM Comput. Surv., vol. 24, no. 4, pp. 377–439, Dec. 1992. [Online].\nAvailable: http://doi.acm.org/10.1145/146370.146380\n[4] F. Farooq, D. Jose, and V . Govindaraju, “Phrase based direct model\nfor improving handwriting recognition accuracies,” in International\nConference on Frontiers in Handwriting Recognition , 2008.\n[5] H. Bunke, S. Bengio, and A. Vinciarelli, “Ofﬂine recognition of uncon-\nstrained handwritten texts using HMMs and statistical language mod-\nels,” IEEE Transactions on Pattern Analysis and Machine Intelligence ,\nvol. 26, no. 6, pp. 709–720, Jun. 2004.\n[6] V . Margner and H. E. Abed, “Arabic Handwriting Recognition Com-\npetition,” in Ninth International Conference on Document Analysis and\nRecognition (ICDAR 2007) , vol. 2, Sep. 2007, pp. 1274–1278.\n[7] P. Natarajan, S. Saleem, R. Prasad, E. MacRostie, and K. Subramanian,\n“Multi-lingual Ofﬂine Handwriting Recognition Using Hidden Markov\nModels: A Script-Independent Approach,” in Arabic and Chinese Hand-\nwriting Recognition: SACH 2006 Summit College Park, MD, USA,\nSeptember 27-28, 2006 Selected Papers , D. Doermann and S. Jaeger,\nTABLE II\nRESULTS ON N-GRAM EXPERIMENTS OF SEEN , PURELY UNSEEN AND UNSEEN TYPE . THE LENGTH RANGES FROM 2 TO 7\nN-gram Arial Narrow Comic Bold\nSeen Unseen Purely Unseen Seen Unseen Purely Unseen\n2 0.9% 4.9% 4.9% 2.4% 6.9% 6.9%\n3 0.5% 2.5% 5.0% 2.3% 4.1% 7.9%\n4 0.5% 1.9% 5.0% 2.2% 3.6% 9.1%\n5 0.4% 1.8% 5.0% 2.0% 3.7% 9.4%\n6 0.4% 1.5% - 2.1% 3.7% -\n7 0.4% 1.5% - 2.0% 3.7% -\nTABLE III\nERROR MATRIX ON SPECIFIC CHARACTERS CORRESPONDING TO THE N-GRAM EXPERIMENTS\nFont Character Dataset Type 2-gram 3-gram 4-gram 5-gram\nComic Bold\nj Seen 1.1% 0% 0% 0%\nPurely Unseen 6.5% 6.8% 5.6% 5.5%\nw Seen 7.8% 4.2% 3.0% 2.1%\nPurely Unseen 7.8% 7.4% 6.2% 6.8%\nArial Narrow\nj Seen 6.6% 3.5% 0% 0%\nPurely Unseen 29% 30.4% 29.4% 28.5%\nt Seen 1.9% 1.2% 1.1% 0.4%\nPurely Unseen 7.9% 5.0% 5.1% 5.4%\nTimes e Seen 70.5% 73.1% 72.2% 73.2%\nl Seen 20.6% 4.3% 1.2% 0.5%\nTABLE IV\nDIFFERENCES IN PERFORMANCE WHEN FREQUENCY OF e IS CONTROLLED\nIN THE TIMES FONT\nN-gram\nTimes Roman\nSeen\ne unregulated e regulated\n2 5.6% 5.6%\n3 7.4% 4.7%\n4 9% 4.6%\n5 9.7% 4.6%\n6 10.1% 4.6%\n7 9.7% 4.6%\nEds. Berlin, Heidelberg: Springer Berlin Heidelberg, 2008, pp. 231–\n250.\n[8] J. L. Elman, “Finding structure in time,” Cognitive Science ,\nvol. 14, no. 2, pp. 179–211, Apr. 1990. [Online]. Available:\nhttp://www.sciencedirect.com/science/article/pii/036402139090002E\n[9] Y . LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,\nW. Hubbard, and L. D. Jackel, “Backpropagation Applied to Handwritten\nZip Code Recognition,”Neural Comput., vol. 1, no. 4, pp. 541–551, Dec.\n1989. [Online]. Available: http://dx.doi.org/10.1162/neco.1989.1.4.541\n[10] Y . Lecun, L. Bottou, Y . Bengio, and P. Haffner, “Gradient-based learning\napplied to document recognition,” Proceedings of the IEEE , vol. 86,\nno. 11, pp. 2278–2324, Nov. 1998.\n[11] S. Hochreiter and J. Schmidhuber, “Long Short-Term Memory,” Neural\nComputation, vol. 9, no. 8, pp. 1735–1780, Nov. 1997. [Online].\nAvailable: http://dx.doi.org/10.1162/neco.1997.9.8.1735\n[12] T. M. Breuel, A. Ul-Hasan, M. A. Al-Azawi, and F. Shafait, “High-\nPerformance OCR for Printed English and Fraktur Using LSTM Net-\nworks,” in 2013 12th International Conference on Document Analysis\nand Recognition, Aug. 2013, pp. 683–687.\n[13] A. Graves and J. Schmidhuber, “Ofﬂine Handwriting Recognition\nwith Multidimensional Recurrent Neural Networks,” in Advances\nin Neural Information Processing Systems 21 , D. Koller,\nD. Schuurmans, Y . Bengio, and L. Bottou, Eds. Curran Associates,\nInc., 2009, pp. 545–552. [Online]. Available: http://papers.nips.cc/paper/\n3449-ofﬂine-handwriting-recognition-with-multidimensional-recurrent-neural-networks.\npdf\n[14] R. Pascanu, T. Mikolov, and Y . Bengio, “On the difﬁculty of\ntraining recurrent neural networks.” ICML (3), vol. 28, pp. 1310–1318,\n2013. [Online]. Available: http://www.jmlr.org/proceedings/papers/v28/\npascanu13.pdf\n[15] A. Graves, S. Fernndez, F. Gomez, and J. Schmidhuber, “Connectionist\nTemporal Classiﬁcation: Labelling Unsegmented Sequence Data\nwith Recurrent Neural Networks,” in Proceedings of the 23rd\nInternational Conference on Machine Learning , ser. ICML ’06. New\nYork, NY , USA: ACM, 2006, pp. 369–376. [Online]. Available:\nhttp://doi.acm.org/10.1145/1143844.1143891\n[16] K. Greff, R. K. Srivastava, J. Koutnk, B. R. Steunebrink, and\nJ. Schmidhuber, “LSTM: A search space odyssey,” IEEE transactions\non neural networks and learning systems , 2016. [Online]. Available:\nhttp://ieeexplore.ieee.org/abstract/document/7508408/\n[17] W. Zaremba, “An empirical exploration of recurrent network\narchitectures,” 2015. [Online]. Available: http://www.jmlr.org/\nproceedings/papers/v37/jozefowicz15.pdf\n[18] R. Pascanu, C. Gulcehre, K. Cho, and Y . Bengio, “How to construct\ndeep recurrent neural networks,” arXiv preprint arXiv:1312.6026, 2013.\n[Online]. Available: https://arxiv.org/abs/1312.6026\n[19] M. Hermans and B. Schrauwen, “Training and analysing deep recurrent\nneural networks,” in Advances in neural information processing\nsystems, 2013, pp. 190–198. [Online]. Available: http://papers.nips.cc/\npaper/5166-training-and-analysing-deep-recurrent-neural-networks\n[20] A. Karpathy, J. Johnson, and F.-F. Li, “Visualizing and Understanding\nRecurrent Networks,” CoRR, vol. abs/1506.02078, 2015. [Online].\nAvailable: http://arxiv.org/abs/1506.02078\n[21] A. Ul-Hasan and T. M. Breuel, “Can We Build Language-\nindependent OCR Using LSTM Networks?” in Proceedings of the\n4th International Workshop on Multilingual OCR , ser. MOCR ’13.\nNew York, NY , USA: ACM, 2013, pp. 9:1–9:5. [Online]. Available:\nhttp://doi.acm.org/10.1145/2505377.2505394\n[22] T. M. Breuel, “Benchmarking of LSTM Networks,” CoRR, vol.\nabs/1508.02774, 2015. [Online]. Available: http://arxiv.org/abs/1508.\n02774\n[23] C. M. Bishop, Neural Networks for Pattern Recognition . New York,\nNY , USA: Oxford University Press, Inc., 1995.\n[24] S. Rawls, H. Cao, E. Sabir, and P. Natarajan, “Combining Deep\nLearning and Language Modeling for Segmentation-Free OCR From\nRaw Pixels,” in 1st International Workshop on Arabic Script Analysis\nand Recognition, ASAR 2017 , Nancy, France, 2017.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.855912446975708
    },
    {
      "name": "Context (archaeology)",
      "score": 0.7089042067527771
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.6137943863868713
    },
    {
      "name": "Test set",
      "score": 0.612501859664917
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6123311519622803
    },
    {
      "name": "Hidden Markov model",
      "score": 0.5932581424713135
    },
    {
      "name": "Independence (probability theory)",
      "score": 0.5911943912506104
    },
    {
      "name": "Key (lock)",
      "score": 0.565502405166626
    },
    {
      "name": "Artificial neural network",
      "score": 0.5446091294288635
    },
    {
      "name": "Decoding methods",
      "score": 0.5296432971954346
    },
    {
      "name": "Speech recognition",
      "score": 0.5087843537330627
    },
    {
      "name": "Language model",
      "score": 0.48749351501464844
    },
    {
      "name": "Natural language processing",
      "score": 0.4240856468677521
    },
    {
      "name": "Algorithm",
      "score": 0.21332675218582153
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1174212",
      "name": "University of Southern California",
      "country": "US"
    }
  ],
  "cited_by": 36
}