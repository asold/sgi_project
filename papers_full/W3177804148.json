{
  "title": "Can Transformers Jump Around Right in Natural Language? Assessing Performance Transfer from SCAN",
  "url": "https://openalex.org/W3177804148",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5022256197",
      "name": "Rahma Chaabouni",
      "affiliations": [
        "École Normale Supérieure"
      ]
    },
    {
      "id": "https://openalex.org/A5081298299",
      "name": "Roberto Dessì",
      "affiliations": [
        "École Normale Supérieure"
      ]
    },
    {
      "id": "https://openalex.org/A5102924622",
      "name": "Eugene Kharitonov",
      "affiliations": [
        "École Normale Supérieure"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2946567085",
    "https://openalex.org/W3034561418",
    "https://openalex.org/W4299802238",
    "https://openalex.org/W2962968135",
    "https://openalex.org/W2284050935",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2990379018",
    "https://openalex.org/W2946533209",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W3192500523",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2964302946",
    "https://openalex.org/W2951227497",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2887970879",
    "https://openalex.org/W2963970792",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W4288094673",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2913659301",
    "https://openalex.org/W3017374003",
    "https://openalex.org/W2939413764",
    "https://openalex.org/W4301259831",
    "https://openalex.org/W2980433389",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W104184427",
    "https://openalex.org/W2567070169",
    "https://openalex.org/W3105725479",
    "https://openalex.org/W2804339109",
    "https://openalex.org/W3100385063",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W3035435378",
    "https://openalex.org/W4287900772",
    "https://openalex.org/W2963267799",
    "https://openalex.org/W4288331674",
    "https://openalex.org/W2123442489",
    "https://openalex.org/W2114719613",
    "https://openalex.org/W2994759459",
    "https://openalex.org/W3129389009",
    "https://openalex.org/W2996346899",
    "https://openalex.org/W2979065840",
    "https://openalex.org/W2996556191",
    "https://openalex.org/W3014415613",
    "https://openalex.org/W3038040931",
    "https://openalex.org/W3000779003",
    "https://openalex.org/W2963685250",
    "https://openalex.org/W4300618906"
  ],
  "abstract": "Despite their failure to solve the compositional SCAN dataset, seq2seq architectures still achieve astonishing success on more practical tasks. This observation pushes us to question the usefulness of SCAN-style compositional generalization in realistic NLP tasks. In this work, we study the benefit that such compositionality brings about to several machine translation tasks. We present several focused modifications of Transformer that greatly improve generalization capabilities on SCAN and select one that remains on par with a vanilla Transformer on a standard machine translation (MT) task. Next, we study its performance in low-resource settings and on a newly introduced distribution-shifted English-French translation task. Overall, we find that improvements of a SCAN-capable model do not directly transfer to the resource-rich MT setup. In contrast, in the low-resource setup, general modifications lead to an improvement of up to 13.1% BLEU score w.r.t. a vanilla Transformer. Similarly, an improvement of 14% in an accuracy-based metric is achieved in the introduced compositional English-French translation task. This provides experimental evidence that the compositional generalization assessed in SCAN is particularly useful in resource-starved and domain-shifted scenarios.",
  "full_text": "Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 136–148\nOnline, November 11, 2021. ©2021 Association for Computational Linguistics\n136\nCan Transformers Jump Around Rightin Natural Language?\nAssessing Performance Transfer from SCAN\nRahma Chaabouni∗†\nEcole Normale Superieure\n{chaabounirahma, roberto.dessi11}@gmail.com kharitonov@fb.com\nRoberto Dess`ı\nFacebook AI & Pompeu Fabra\nEugene Kharitonov∗\nFacebook AI\nAbstract\nDespite their failure to solve the composi-\ntional SCAN dataset, seq2seq architectures\nstill achieve astonishing success on more prac-\ntical tasks. This observation pushes us to ques-\ntion the usefulness of SCAN-style composi-\ntional generalization in realistic NLP tasks. In\nthis work, we study the beneﬁt that such com-\npositionality brings about to several machine\ntranslation tasks. We present several focused\nmodiﬁcations of Transformer that greatly im-\nprove generalization capabilities on SCAN\nand select one that remains on par with a\nvanilla Transformer on a standard machine\ntranslation (MT) task. Next, we study its per-\nformance in low-resource settings and on a\nnewly introduced distribution-shifted English-\nFrench translation task.\nOverall, we ﬁnd that improvements of a\nSCAN-capable model do not directly transfer\nto the resource-rich MT setup. In contrast, in\nthe low-resource setup, general modiﬁcations\nlead to an improvement of up to 13.1% BLEU\nscore w.r.t. a vanilla Transformer. Similarly,\nan improvement of 14% in an accuracy-based\nmetric is achieved in the introduced compo-\nsitional English-French translation task. This\nprovides experimental evidence that the com-\npositional generalization assessed in SCAN\nis particularly useful in resource-starved and\ndistribution-shifted scenarios.\n1 Introduction\nWhile sequence-to-sequence (seq2seq) mod-\nels achieve remarkable performance in many\ntasks (Sutskever et al., 2014; Raffel et al., 2019;\nAdiwardana et al., 2020), they often fail to gener-\nalize in a systematic way (Baroni, 2019; McCoy\net al., 2020; Hupkes et al., 2020; Kharitonov and\nChaabouni, 2021; Dankers et al., 2021). These\n∗Equal contribution. †work was done while R.C. was at\nFacebook AI.\nshortcomings are particularly obvious in the\nexperiments on the SCAN domain (Lake and\nBaroni, 2018; Loula et al., 2018; Bastings et al.,\n2018).\nIn SCAN, inputs are instructions that describe\ntrajectories and outputs deﬁne sequences of actions\nto follow them (see Table 1). To illustrate how\nSCAN probes a model for compositional general-\nization, imagine that we train it on a set of instruc-\ntions {jump, run, turn, turn twice, run\ntwice}, but test it on jump twice. Strictly\nspeaking, nothing in the training data indicates that\nthe model must output JUMP JUMP instead of e.g.\nJUMP. However, it is hypothesised that a bias for\nsuch compositional, human-like induction is bene-\nﬁcial (Lake and Baroni, 2018; Lake, 2019).\nThis hypothesis stumbles into a perplexing situ-\nation: despite failing at compositional generaliza-\ntion, considered as a core requirement for language\nunderstanding, seq2seq models have tremendous\nsuccess in practice. Is the type of compositional\ngeneralization, that SCAN probes for, useful for\nNLP tasks? If so, in what scenarios?\nIn this work, we aim to answer this question.\nFirstly, we introduce focused modiﬁcations to\nTransformer that greatly improve accuracy per-\nformance on SCAN. To build such modiﬁcations,\nwe exploit two observations: (i) Transformer’s\narchitecture is very similar to convolution-based\nseq2seq models (ConvS2S) (Gehring et al., 2017),\n(ii) ConvS2S performs well on SCAN (Dess`ı and\nBaroni, 2019). This capability (ii) is hypothesized\nto be due to explicitly localized representation of\nthe sequences, where only deep layers of the model\ncan access more distant tokens (Dess`ı and Baroni,\n2019; Hupkes et al., 2020). Such a capability\nshould also beneﬁt to natural language processing\nas human languages are proved to favor local syn-\ntactic constructions (Futrell et al., 2015). Motivated\nby these observations, we focus on the major differ-\n137\nences between ConvS2S and Transformer, namely\nthe localized receptive ﬁeld span and the gating\nmechanism, to inject into Transformer inductive\nbiases useful for SCAN.\nAs a testbed, we use the machine translation\n(MT) domain as one of the most popular appli-\ncations for seq2seq models. We consider both\nresource-rich (IWSLT’14 German→English) and\nlow-resource (Nepali&Sinhala↔English) setups.\nFinally, to evaluate SCAN-style capabilities in nat-\nural language tasks, we build a dataset that probes\nwhether models can systematically generalize w.r.t.\nnoun-adjective ordering while translating from En-\nglish to French. We construct this dataset by ﬁlter-\ning the EuroParl corpus (a part of WMT’14).\nOur results indicate that combining two\nConvS2S-inspired changes improves accuracy on\none SCAN split (SCAN-jump) from 3.4% to\n43.0%, while maintaining a high accuracy on\nthe other splits (SCAN-simple and SCAN-around-\nright). As expected, given that SCAN is an artiﬁ-\ncial diagnostic dataset, not all modiﬁcations lead\nto equal improvements on an MT task. We select\none of the considered modiﬁcations that performs\non par with the vanilla Transformer on IWSLT’14.\nTesting the selected modiﬁcation on low-\nresource data, we observe that it provides between\n3.6% and 13.1% BLEU improvements over Trans-\nformer. On the noun-adjective ordering dataset,\nwe ﬁnd that our modiﬁcation results into gains in\ngeneralization of 14%.\nThis leads to the following picture: the localized\nattention, augmented by a gating mechanism, pro-\nvides a useful inductive bias that proves to be bene-\nﬁcial for SCAN-style generalization. Additionally,\nit turns out useful in low-resource and distribution-\nshifted settings. Thus, testing seq2seq models on\nSCAN while controlling for a non-degraded per-\nformance leads to improvement in domains where\nsyntactic compositionality is crucial for a task suc-\ncess.\n2 Transformers and ConvS2S\nArchitecture overview Both Transformer\nand ConvS2S are encoder-decoder architec-\ntures (Sutskever et al., 2014), where the decoder\nhas an attention mechanism to peek into the\nencoder’s representation (Bahdanau et al., 2014).\nThese representations are obtained by embedding\nthe inputs, adding a positional embedding, and\npassing them through a sequence of layers.\nIn Transformer’s encoder, the output represen-\ntations are the result of a sequential application of\ntwo (sub)layer types: self-attention and fully con-\nnected layers. The input representation can “skip”\nany sublayer via a residual connection. The out-\nput of the sublayer is passed through a dropout\nmechanism and added to the residual. This sum is\nthen layer-normalized. Any relation between input\ntokens is modeled solely by self-attention modules.\nIn ConvS2S, the encoder is also a sequence of\nidentical blocks. The inter-term dependencies are\nmodeled by 1D convolutions with GLU activation\nfunctions (Dauphin et al., 2017). 1 In contrast to\nself-attention, convolutions have a ﬁnite kernel size,\nthus effectively capping the maximal distance of in-\ntermediate dependencies that can be modeled. The\nGLU activation function serves as a gate, allowing\nConvS2S to control the balance between residuals\nand the output of the convolution. After the GLU\noperation, the intermediate representation is added\nto the residual and scaled. The output of the ﬁnal\nconvolution is then passed into a fully connected\nlayer. In ConvS2S and Transformer, decoders have\nsimilar structures to those of encoders, with an ad-\nditional decoder→encoder attention layer after the\nconvolution and self-attention blocks, respectively.\nDespite the similarities, there are numerous\nlow-level differences between the two architec-\ntures: normalization (layer norm (Ba et al., 2016)\nvs. weight normalization (Salimans and Kingma,\n2016)), optimization (Adam (Kingma and Ba,\n2014) with a ramp-up vs. NAG (Sutskever et al.,\n2013)), etc. A priori, any of those can affect mod-\nels’ inductive biases. However, we concentrate on\nsome of the most obvious architectural differences:\nthe limited convolution span and GLU activations.\nWe believe these features can greatly affect mod-\nels’ performance on SCAN. Indeed, SCAN has\nonly local dependencies between tokens, thus the\nability to avoid spurious correlations with more\ndistant tokens can be useful. Similarly, the ability\nto weight contributions from the token interactions\ninto the intermediate representation is intuitively\nprerequisite to build compositional representation.\nGLU (Dauphin et al., 2017) Given a vector input\nx, GLU splits it in two equally sized halves x1 and\nx2; one is passed through a sigmoid (σ(x) = (1 +\ne−x)−1). Then both parts are pointwise multiplied:\n1GLU was introduced as a combination of a convolution\nnetwork and an activation; we follow the Pytorch convention\nand consider it as separate blocks for convenience.\n138\njump ⇒JUMP\njump around right ⇒RTURN JUMP RTURN JUMP RTURN JUMP RTURN JUMP\nturn left twice ⇒LTURN LTURN\njump opposite left after walk around left⇒LTURN W ALK LTURN W ALK LTURN W ALK LTURN W ALK LTURN LTURN JUMP\nTable 1: Examples of SCAN trajectories and instructions, adopted from (Lake and Baroni, 2018).\nGLU(x) := σ(x1) ⊙x2 (1)\nThis allows a network to implement a gating mech-\nanism, where one half of its output gates the signal\nfrom the second.\nSelf-attention Instead of convolutions, Trans-\nformer uses multi-headed self-attention to model\ninteractions between tokens. Given nembeddings,\nx1,x2,...xn of dimensionality d, the self-attention\nlayer transforms them in the following way.\nFirstly, each embedding xi is projected by three\nmatrices Q, K, and V to get query qi, key ki, and\nvalue vi representations, respectively: qi,ki,vi ←\nQxi,Kxi,V xi. Next, a scaled dot-product be-\ntween query qi and key kj is calculated as follow:\nαij = 1√\nd\nqi ·kT\nj (2)\nThis dot-product deﬁnes the attention weights\nwij = eαij /∑eαij which are used to get the out-\nput representations: oi = ∑\nj wijvj. This process\nis done in parallel for multiple heads, acting on\nindependent slices of the input embeddings; their\noutputs are concatenated and passed through a fully\nconnected layer.\n3 Transformer Modiﬁcations\nSelf-attention gate (SAG) The simplest way to\nimitate the effect of GLU activation (Eq. 1) is\nto weight (gate) the output of self-attention by a\nlearned scalar parameter. To ensure that it is non-\nnegative and is scaled in [0,1], we parameterize it\nas a sigmoid of a real-valued learned parameter β.\nAlgorithm 1 illustrates the introduced change. In\ncomparison to Transformer, SAG adds one scalar\nparameter for each encoder and decoder layer.\nWe treatβ0, the value βis initialized with before\ntraining, as a hyperparameter. In the preliminary ex-\nperiments, we found that after training, encoder lay-\ners often have small negativeβvalues (−2..−0.5),\nwhile decoder layers have positive values (0.2..4.5)\nthat grow monotonically for higher layers.\nA similar modiﬁcation was considered in an ef-\nfort to stabilize Transformer training in the Rein-\nforcement Learning domain (Parisotto et al., 2020).\nConvolution as self-attention In the limit case,\nwe can entirely replace self-attention with convo-\nlutions. This modiﬁcation introduces one hyper-\nparameter (kernel size). However, convolutional\nlayers have fewer parameters than the self-attention\nmechanism. One might consider this not to be a\nTransformer variant due to the lack of self-attention,\nbut as self-attention generalizes convolutions (Cor-\ndonnier et al., 2020), we consider this as an extreme\nform of regularization.\nFixed-span self-attention A less extreme mod-\niﬁcation would be to use the regular multi-head\nself-attention mechanism, but without allowing at-\ntention to peek beyond some distance. This mimics\nthe limited kernel size of convolutions in ConvS2S.\nWe achieve this by adding a ﬁxed bias term bij to\nthe self-attention logits (Eq. 2):\nαij = 1√\nd\nqi ·kT\nj + bij (3)\nSetting bij to −∞when the difference |i−j|ex-\nceeds some ﬁxed value sand to 0 otherwise pre-\nvents the self-attention to look beyond distance s.\nFixed-span self-attention with a span parameter\nshas the same “receptive ﬁeld” as 1D convolution\nwith kernel size 2s+ 1. This modiﬁcation adds one\nhyperparameter (span size), but does not introduce\nnew learned parameters.\nT5 attention Further relaxing constraints on\nself-attention, we consider the case where we al-\nlow Transformer to learn how to (soft-)limit its\nself-attention. We introduce the bias term bij that\nis learned as a function of a (signed) difference\ni−j, capping it to [−s,+s] (e.g., positions with\ndifference above swould have the same bias bs).\nThis modiﬁcation is similar to one introduced\nby Raffel et al. (2019) in T5, with the only excep-\ntion that we allow each head to have its own bias.\nAgain, the span size is a new hyperparameter. In a\nmodel with nh heads and nl layers, this modiﬁca-\ntion requires (2s+ 1)×nl ×nh new parameters,\nwhich is negligible in comparison with the sizes\nof fully connected layers. Examples of the learned\nbij parameters are shown in Supplementary when\ntraining on SCAN.\n139\n1: procedure COMPUTE SELFATTENTION\n2: res←x\n3: x←self attn(x)\n4: x←x∗σ(β)\n5: x←layer norm(res+ dropout(x))\nAlgorithm 1: Self-attention gate (SAG). The\nonly introduced change is on line 4. β is a\nlearned per-layer scalar parameter.\nImplementation details We used the\nfairseq (Ott et al., 2019) implementation of\nTransformer seq2seq as a foundation, with its\ninitialization and default parameters. T5 and\nﬁxed-span attentions are implemented by providing\nadditive attention masks to Transformer.\n4 Datasets\n4.1 SCAN\nIntroduced by Lake and Baroni (2018), SCAN is\na collection of tasks used for studying systematic\ngeneralization of seq2seq models (see Table 1 for\nsome input-output examples). A set of 4 primitive\nverbs are combined with different modiﬁers gener-\nating around 27k unique samples. Lake and Baroni\n(2018) and, later, Loula et al. (2018) prepared sev-\neral non-i.i.d. splits of the data into training and test\nsets. To successfully generalize on such non-i.i.d.\nsplits, a model has to generalize systematically, in\na compositional way.\nWe experiment with three tasks, often focused on\nin the literature2 (Dess`ı and Baroni, 2019).SCAN-\nsimple splits all sequences in train and test sets\nuniformly at random. Hence, both train and test are\nidentically distributed. Typically models succeed\nat it easily. In SCAN-jump, the test set contains all\ncompositional uses of one of the primitives, jump.\nThe train set contains all uses of other primitives,\nand inputs wherejump is used in isolation. SCAN-\naround-right tests if a model is capable to general-\nize to combinations of two modiﬁers, around and\nright, that never co-occur in the training data.\nThe test data contain all examples where the two\nmodiﬁers are combined.\n2We have also ensured that our best modiﬁcation performs\non par ( ≈ 10%) with Transformer on SCAN-length; how-\never SCAN-length is believed to require a different type of\ngeneralization (Gordon et al., 2019).\n4.2 Machine Translation\nWe hypothesize that the type of systematic gen-\neralization that SCAN probes for could be most\nuseful in data-poor tasks or tasks with train-test\ndistribution shift. Hence, we complement the stan-\ndard IWSLT’14 En-De dataset with a low-resource\ntask, FLoRes. To study whether our models can\nperform SCAN-style generalization on natural lan-\nguage data, we also build a dataset that probes for\ncompositional generalization in noun-adjective or-\ndering in French, when translating from English.\nIWSLT’14 En-De This is a standard MT\ndataset, that includes train, validation, & test sets.\nWe apply preprocessing as in the fairseq example.3\nFLoRes (Guzm´an et al., 2019) FloRes is a low-\nresource dataset for English↔Nepali and English\n↔Sinhala translation. The dataset is split into dev,\ndevtest, and test subsets. We only use the provided\nsupervised data.\nNoun-adjective ordering We take inspiration\nfrom SCAN-jump to construct an MT dataset\nthat probes for compositional generalization us-\ning noun-adjective ordering in French. In French,\nboth adjective noun (forward) and noun adjective\n(backward) orders are used, unlike English that\nonly has the forward order. Which order is used\nlargely depends on the adjective. For example, to\nrefer to a speciﬁc response, French speakers say\nr´esponse sp´eciﬁque (backward order), while new\nresponse would be nouvelle r´esponse (forward).\nTo draw a parallel with SCAN-jump, we con-\nsider the nouns as primitives and adjectives as mod-\niﬁers. Modiﬁers appear with different primitives,\nhowever, some primitives appear with only one\nmodiﬁer. For instance, if, in the training set, re-\nsponse only appears with speciﬁc (backward), we\ntest models on translating sentences containingnew\nresponse, where new modiﬁes many other nouns in\nthe training set in the forward order. Such general-\nization is required by humans when dealing with\nrare or too speciﬁc nouns.\nTo construct our dataset, we start from\nthe English-French Europarl dataset (a part\nof WMT’14 En-Fr) 4 and select 8 nouns,\nN={response, institution, organisation, solution,\nsource, decision, responsibility, population}. We\nconstrain our train set so that each of the nouns\nin N appears only with one adjective (hence in\n3https://github.com/pytorch/fairseq/\ntree/master/examples/translation\n4http://www.statmt.org/europarl/\n140\nbackward order forward order\n(‘speciﬁc’, ‘response’)\n(‘particular’, ‘institution’)\n(‘effective’, ‘organisation’)\n(‘permanent’, ‘solution’)\n(‘new’, ‘source’)\n(‘good’, ‘decision’)\n(‘big’, ‘responsibility’)\n(‘ﬁrst’, ‘population’)\nTable 2: (adjective, noun) pairs in the train set of the\nnoun-adjective ordering dataset, classiﬁed by their or-\nder in French language.\none particular order) as shown in Table 2. For\nexample, the noun response will only be composed\nwith the adjective speciﬁc. However, speciﬁc\n(and all other adjectives in Table 2) appears with\nother nouns. To select these sentences, we use the\nCoreNLP parser (Manning et al., 2014).5 Finally,\nall sentences with nouns that are not among the\nselected ones are kept. In other words, the training\nset may contain sentences that have neither the\nselected adjectives nor the selected nouns. This\nresults to 1641681 sentence pairs split into train\n(1478195 pairs) and validation (163486 pairs) sets.\nThe test set is composed of the ﬁltered sentences of\nthe original Europarl dataset: we select sentences\nwhere nouns in the backward column of Table 2\n({response, institution, organism, solution }) are\nonly modiﬁed by the adjectives in the forward\ncolumn ({new, good, big, ﬁrst }). Similarly, we\nalso consider the sentences where the nouns\nof the forward column are composed with the\nadjectives of the backward column of the Table.6\nThis process will ensure that in the test set, the\nnoun-adjective only appears in the reverse order\ncompared to the train set. Unlike the training\ndata, the test data contains only sentences with\nthe target nouns and adjectives. In total, we test\nmodels on 659 sentences. Note that the train and\nvalidation sets are identically distributed, however,\nthe test set is distribution-shifted w.r.t. train, akin\nto SCAN-jump.\nWe follow the preprocessing steps on the fairseq\nexample page for WMT’14 English to French.3\n5 Methodology\nSCAN Lake and Baroni (2018) were concerned\nby the feasibility of systematic generalization in\nseq2seq models. Hence, in their experiments, they\ntuned the models on the train data and then directly\nevaluated them on test set, reporting test scores.\n5https://stanfordnlp.github.io/\n6We use the Stanford parser to select these sentences.\nWe follow the same protocol: given a grid of hy-\nperparameters, we ﬁt models on the training data.\nNext, for each hyperparameter conﬁguration, we\naverage the performance of the models across ran-\ndom seeds. Such a setup demonstrates that, at\nleast for some hyperparameter conﬁgurations, the\nintroduced models can learn to generalize system-\natically. At evaluation time, we decode greedily.\nIWSLT’14 De-EnWe run a grid search on train\ndata; next we select the best performing checkpoint\non the validation dataset. We report performance\non the test data. We use the same training and\nevaluation protocols as suggested on the fairseq\nMT example page.3 We use beam size 5.\nFLoRes This dataset has dev, devtest, and test\nsplits provided. We run a hyperparameter grid\nsearch training on the dev data. Next, we select\nthe hyperparameter conﬁguration that has the best\naverage (across seeds) performance on devtest. We\nreport the performance of the selected hyperparam-\neter conﬁguration on the test set, averaged across\nseeds. We use the training/evaluation scripts, to-\nkenization and other parameters suggested on the\ndataset page: beam size 5 and length penalty 1.2.\nNoun-adjective ordering We run the hyperpa-\nrameter search similarly to IWSLT’14 De-En. The\ntraining and evaluation protocols are the ones sug-\ngested by the fairseq page for WMT’14 En-Fr.3 We\nalso use beam size 5.\nAs we aim to probe abilities for compositional\ngeneralization, we introduce an accuracy-based\nmeasure, COMP. When analyzing models’ errors,\nwe encountered 3 common errors: (1) removing the\nadjective (example 1 in Table 6), (2) replacing the\nadjective with a synonym and reversing the order\n(examples 2 and 3 in Table 6), and (3) producing a\ncompletely wrong generalization while removing\nthe adjective. While (2) provides a good enough\ntranslation, it is a mistake in the noun-adjective or-\nder. However, when outputting the right noun and\nadjective, the order is always preserved. Hence,\nto measure if a model is compositional, we only\nlook if both the target adjective and the target noun\nappear in the prediction, irrespective of their or-\nder. We deﬁne thus COMP as the ratio of predicted\nsentences that include both the target adjective and\nnoun.7\n7It happens that models use a synonym in the right order\nas shown in SAG+T5’s prediction 2 in Table 6. In that case,\nmodels had generalized well but are still penalized by COMP.\nCOMP is hence only a proxy measure for compositional gen-\neralization based on the common failures.\n141\njump around-right simple\nTransformer 3.4±2.0 97.6±1.5 100.0±0.0\nself-att. gate (SAG) 17.2±5.8 85.2±10.0 100.0±0.0\n+ Conv as s.-a. 25.7±20.4 38.4±7.8 99.8±0.0\n+ Fixed-span 33.6±9.5 97.6±1.3 100.0±0.0\n+ T5 43.0±9.5 92.6±2.8 100.0±0.0\nLSTM seq2seq (Lake and Baroni, 2018) 1.2 2.5 99.8\nConvS2S (Dess`ı and Baroni, 2019) 69.2±8.2 56.7±10.2 100.0±0.0\nTable 3: Accuracy on SCAN tasks, %. For each architecture and task, we report the mean accuracy of the best\nhyperparameter conﬁguration. ±denotes 1 SEM.\nHyperparameter search Transformer models\nhave multiple hyperparameters (embeddings di-\nmensionality, number of layers and attention heads,\ndropout probabilities, etc.). On top of those, our\nintroduced models add the attention span s, and\nthe initial gate state β0. For MT tasks, we start\nfrom the existing strong baseline hyperparameter\nconﬁgurations (FLoRes: speciﬁed by Guzm ´an et al.\n(2019), De-En & En-Fr: following the fairseq ex-\nample page) and only tune (a) the parameters in-\ntroduced by our architectures, and (b) the attention\ndropout parameter (for all architectures, including\nTransformer). For SCAN, there is no baseline hy-\nperparameter conﬁguration, so we start with tuning\nTransformer and then base hyperparameters of the\nintroduced architectures on it. We report full hyper-\nparameter grids in Supplementary.\n6 SCAN experiments\nIn our preliminary experiments, we found that our\nmodiﬁcations of the self-attention mechanism do\nnot lead to improvements over the standard Trans-\nformer when they are not combined with the self-\nattention gate (SAG). Hence, we focus our experi-\nments on architectures that include SAG.\nWe report our results in Table 3. We also include\nresults for LSTM- and Conv-based seq2seq models\nthat were reported in earlier work (Lake and Baroni,\n2018; Dess`ı and Baroni, 2019). From Table 3, we\nsee that the unmodiﬁed Transformer has very low\naccuracy on jump (3.4%), which is only slightly\nabove that of LSTM seq2seq (1.2%) and well below\nConvS2S (69.2%). This indicates that Transformer\nmodels are indeed failing in compositional general-\nization on jump. However, they have a very high\nscore on the around-right split (97.6%) and\nsimple (≥99.8%). By introducing the differ-\nent modiﬁcations described in Section 3, making\nTransformers closer to ConvS2S, we aim at pre-\nTransformer SAG + Conv s.-a. + ﬁx. span + T5\n34.64±0.03 34.28±0.08 33.44±0.04 34.32±0.01 34.66±0.04\nTable 4: BLEU on test set. IWSLT’14 German to En-\nglish dataset. ±denotes 1 SEM.\nserving the high performance of Transformers on\naround-right and simple while signiﬁcantly\nimproving it on jump.\nAdding SAG increases accuracy on jump 5-\nfold (17.2%) at the expense of a small drop in\naround-right scores (not stat. sig.).\nFurther, we observe that changes of the self-\nattention mechanism (replacing it with Convs,\nlimiting its span, and adding a relative position-\ndependent bias), can further increase the perfor-\nmance on jump. Apart from SAG+Conv as s.-a,\nthe self-attention modiﬁcations do not signiﬁcantly\nalter the performance on around-right.\nWe see that the architectural changes that we pro-\nposed improve the compositional capabilities of the\nTransformer models. As expected, the introduced\nhybrid architectures reach signiﬁcantly better per-\nformance on jump (up to 12x improvements for\nSAG+T5) while keeping high performance on the\naround-right & simple tasks.\n7 Machine Translation experiments\nIWSLT’14 De-En In Table 4, we report BLEU\nscores on German-English translation. SAG + T5\nperforms slightly better (0.02 BLEU, not stat. sig.),\nbut other modiﬁcations underperform w.r.t. Trans-\nformer. Replacing self-attention with convolutions\nresulted in the largest drop, 3%. Other differences\nare smaller, ≤1%. For the following parts, we only\nexperiment with the SAG + T5 model as the only\nnon-degraded one. However, results with the re-\nmaining models on FLoRes and the Noun-adjective\nordering datasets are reported in Supplementary.\nFLoRes, Si/Ne ↔En We report results on\n142\nEnglish↔Nepali and English↔Sinhala translation\nin Table 5. Following Guzm´an et al. (2019), we use\ntokenized BLEU when translating from English.\nWe run standard Transformer models as speciﬁed\nin Guzm´an et al. (2019), but adding a search over\nthe attention dropout probability. We verify that\nwe have close results compared to Guzm ´an et al.\n(2019).8 Table 5 shows that SAG + T5 outperforms\nTransformer on all language pairs and directions\nwith relative improvements between 3.6% (si-en)\nand 13.1% (en-ne).\nNoun-adjective ordering BLEU scores on the\ntest set are reported in Table 5. SAG + T5 leads\nto a relative improvement of 1.39% compared to\nstandard Transformer. BLEU, however, is not infor-\nmative about the particular noun-adjective general-\nization. We hence also report COMP scores. From\nTable 5, we see that SAG + T5 demonstrates a sig-\nniﬁcant improvement with 14% relative gain com-\npared to the standard Transformer architecture. Our\nfollow-up experiments show that the hybrid model\nrecovers an average of 43.3% of cases where the\nbest Transformer model (best seed w.r.t. COMP)\nfailed in compositional generalization, whereas\nTransformer is only correct at 21.5% of SAG +\nT5’s errors. We report in Table 6 examples compar-\ning SAG + T5 and Transformer translations.\nDiscussion Upon analyzing our experiments on\nSCAN and machine translation tasks, we see the\nfollowing picture. Indeed the hybrid models that\nwe described in Section 2 have considerably higher\naccuracy on SCAN-jump w.r.t. Transformer and a\ncomparable performance on the other SCAN splits.\nHence, our results suggest the importance of both\ngating and (the ability of) limiting the attention\nspan for SCAN generalization.\nAs expected, the improvement on SCAN do not\nconsistently entail improvements on the resource-\nrich dataset, and only the combination of SAG and\nT5 showed a tiny improvement. This emphasizes\nthe importance of testing models on realistic setups\nto model from being too SCAN-tailored.\nFinally, we test SAG + T5 on low-resource and\ncompositional tasks. The hybrid architecture shows\nconsistent improvements on FLoRes for all trans-\nlation directions, with at up to 13.1% relative im-\nprovement, and on the the natural language com-\npositional task with 14% relative improvement on\nCOMP. Our qualitative analysis also showed that\nSAG + T5 correctly handles noun-adjective order-\n8We got better BLEU scores due to the extra grid search.\ning in most cases, while Transformer makes more\nmistakes.\n8 Related Work\nCompositionally-biased models Several ap-\nproaches were proposed to build SCAN-capable\narchitectures. They span from meta-learning (Lake,\n2019), disentangling syntax and semantics (Russin\net al., 2019), learning equivariant (Gordon et al.,\n2019) and disentangled representations (Li\net al., 2019) or combining neural & symbolic\ncomputations (Chen et al., 2020). In contrast, we\ndo not build new models that are specialized to\nSCAN. Instead, we show that a standard model can\nbe incrementally modiﬁed so that performs well\non SCAN and still performs well on a standard\nMT task. Having such incrementally improved\nmodels allows us to step back and wonder if SCAN\n(or similar artiﬁcial tasks) should be used as a\nguidance when developing new models.\nBastings et al. (2018) raised concerns due to\nSCAN being too artiﬁcial by showing that even\ndegenerate architectures can perform well on some\nSCAN tasks. Our results echo their ﬁndings: by de-\nveloping architectures tailored for SCAN, one can\neasily come up with models that perform worse on\ngeneral tasks. However, we ﬁnd that if one avoids\nthis “SCAN overﬁtting” and endows a model with\ncapabilities that SCAN probes for without harm-\ning its general performance, they can gain in low-\nresource scenarios and better handle relevant phe-\nnomena in language.\nChanging attention mechanisms Self- and\ncross-attention mechanisms were tweaked in earlier\nwork in order to inject useful biases, e.g., by adding\ninformation of relative positions of tokens (Shaw\net al., 2018; Raffel et al., 2019) or accounting for\nthe locality bias in cross-attention (Yang et al.,\n2018). Sukhbaatar et al. and Rae and Razavi (2020)\ndemonstrated that having a short attention span on\nthe lower layers of Transformer models is enough\nfor good language modeling performance.\n9 Conclusion\nIn this work, we primarily focused on whether and\nin which scenarios the inductive bias for composi-\ntional generalization, that SCAN looks for, can be\nuseful in natural language tasks.\nWe ran study in two steps. As the ﬁrst step, by\nexploiting ConvS2S/Transformer similarities, we\ncame up with a modiﬁcation of the Transformer\n143\nFLoRes (BLEU) Noun-Adj. ordering\nne-en si-en en-ne en-si BLEU COMP\nTransformer 7.94±0.05 7.15±0.07 4.43±0.01 2.32±0.08 40.86±0.34 0.64±0.01\nSAG + T5 8.40±0.02 7.41±0.10 5.01±0.10 2.54±0.03 41.43±0.29 0.73±0.01\nTable 5: Models performance on FLoRes and Noun-adjective ordering (English to French) dataset. For FLoRes, we\nreport the BLEU dev-test scores for the different translation directions. For the Noun-adjective ordering dataset, we\nreport both BLEU and COMP measures on the test set. In bold are values that stat. sig. improve over Transformer.\n±denotes 1 SEM.\nTarget: Nous sommes face `a une responsabilit´e politique particuli`ere.\nPrediction SAG+T5: Nous sommes accabl´es par une responsabilit´e politique particuli`ere.\nPrediction Transformer: Nous sommes accabl´es par une responsabilit´e politique.\nTarget: Nous voulons trouver une bonne solution `a ce probl`eme.\nPrediction SAG+T5: Nous voulons trouver une bonne solution `a ce probl`eme.\nPrediction Transformer: Nous voulons trouver une solution ad´equate `a ce probl`eme.\nTarget: Ce qui nous d´ec ¸oit par rapport `a cette d´ecision particuli`ere, c’est que le projet aurait pu clairement voir le jour.\nPrediction SAG+T5: Ce qui est triste dans cette d´ecision pr´ecise, c’est que le projet aurait ´et´e clairement r´ealis´e.\nPrediction Transformer: Ce qui est triste dans cette mauvaise d´ecision, c’est que le projet aurait clairement ´et´e.\nTable 6: Generation Examples for Noun-adjective ordering dataset. Models are tested on the underlined and italic\n(adjective, noun). For the ﬁrst 2 examples, SAG+T5 predicted the right(adjective, noun) translation. In the last one,\nSAG+T5 replaced the adjective with a synonym but in the right target order (the one not seen in the training set).\nIn the ﬁrst example, Transformer removed the adjective particuli`ere. In the two following examples, Transformer\nreplaced the right adjective with a close synonym adjective to be conform with the training order. For instance, in\nthe second example, bonne (an adjective that appears in the forward order) was replaced byad´equate (an adjective\nthat appears in the backward order) as the solution appears only in the backward order at training.\narchitecture that performs considerably better than\nvanilla Transformer on SCAN-jump (43.0% vs\n3.4% accuracy) and performs equally well on\nSCAN-simple, SCAN-around-right, and on a stan-\ndard resource-rich MT task (IWSLT’14 De-En).\nNext, we tested this modiﬁcation in low-resource\nand distribution-shifted setups. In the low-resource\nMT setup (FLoRes Si/Ne↔En), we found that our\nconsidered architecture improves by up to 13.1%\nin BLEU score over the vanilla Transformer. Then,\nwe introduced a new dataset that probes speciﬁ-\ncally for compositional reasoning in natural lan-\nguage. Unlike SCAN, our compositional dataset\nis built by ﬁltering an existing natural language\ncorpus (EuroParl En-Fr) to probe how models per-\nform noun-adjective ordering under a (minimal)\ndistribution shift. Thus, we are largely closer to\ntesting the compositional generalization required\nby humans compared to SCAN, and succeeding on\nthe test set requires both compositional reasoning\nand good language model performance (see exam-\nples in Table 6). We believe that such a dataset is\nbeneﬁcial for future research to test more complex\ncompositionality skills. Finally, our experiments\non our dataset demonstrated that better SCAN gen-\neralization leads to better results on noun-adjective\nordering (14% on COMP).\nOur ﬁndings indicate the following. Firstly, as\nhypothesized before (Dess`ı and Baroni, 2019; Hup-\nkes et al., 2018), the limited attention span pro-\nvides a useful inductive bias that allows models to\nperform better on compositional generalization in-\nduction, that SCAN probes for. Further, endowing\na model with SCAN-style generalization capabili-\nties can lead to improvements in low-resource and\ndistribution-shifted scenarios as long as we ensure\nthat we do not overﬁt to SCAN.\nWe believe that the contribution of diagnostic\ndatasets like SCAN is of great value. As perfor-\nmance grows on tasks such as MT, identifying gaps\nwhere a model’s performance lags will become fun-\ndamental and will guide us to develop architectures\nthat cover genuine new linguistic grounds and not\njust overﬁt to peculiarities of standard datasets.\n144\nAcknowledgments\nThe authors are grateful to Marco Baroni and the\nreviewers for feedback that helped us to improve\nour .\nReferences\nDaniel Adiwardana, Minh-Thang Luong, David R So,\nJamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,\nApoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,\net al. 2020. Towards a human-like open-domain\nchatbot. arXiv preprint arXiv:2001.09977.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473.\nMarco Baroni. 2019. Linguistic generalization and\ncompositionality in modern artiﬁcial neural net-\nworks. Philosophical Transactions of the Royal So-\nciety B: Biological Sciences, 375(1791):20190307.\nJasmijn Bastings, Marco Baroni, Jason Weston,\nKyunghyun Cho, and Douwe Kiela. 2018. Jump\nto better conclusions: Scan both left and right. In\nMNLP Workshop BlackboxNLP: Analyzing and In-\nterpreting Neural Networks for NLP.\nXinyun Chen, Chen Liang, Adams Wei Yu, Dawn\nSong, and Denny Zhou. 2020. Compositional gen-\neralization via neural-symbolic stack machines.\nJean-Baptiste Cordonnier, Andreas Loukas, and Mar-\ntin Jaggi. 2020. On the relationship between self-\nattention and convolutional layers. In ICLR.\nVerna Dankers, Elia Bruni, and Dieuwke Hupkes. 2021.\nThe paradox of the compositionality of natural lan-\nguage: a neural machine translation case study.\nYann N Dauphin, Angela Fan, Michael Auli, and David\nGrangier. 2017. Language modeling with gated con-\nvolutional networks. In ICML.\nRoberto Dess`ı and Marco Baroni. 2019. CNNs found\nto jump around more skillfully than RNNs: Compo-\nsitional generalization in seq2seq convolutional net-\nworks. In ACL.\nRichard Futrell, Kyle Mahowald, and Edward Gib-\nson. 2015. Large-scale evidence of dependency\nlength minimization in 37 languages. PNAS,\n112(33):10336–10341.\nJonas Gehring, Michael Auli, David Grangier, Denis\nYarats, and Yann N. Dauphin. 2017. Convolutional\nsequence to sequence learning. In ICML.\nJonathan Gordon, David Lopez-Paz, Marco Baroni,\nand Diane Bouchacourt. 2019. Permutation equiv-\nariant models for compositional generalization in\nlanguage. In ICLR.\nFrancisco Guzm ´an, Peng-Jen Chen, Myle Ott, Juan\nPino, Guillaume Lample, Philipp Koehn, Vishrav\nChaudhary, and Marc’Aurelio Ranzato. 2019. Two\nnew evaluation datasets for low-resource machine\ntranslation: Nepali-english and sinhala-english.\nDieuwke Hupkes, Verna Dankers, Mathijs Mul, and\nElia Bruni. 2020. Compositionality decomposed:\nHow do neural networks generalise? Journal of Ar-\ntiﬁcial Intelligence Research, 67:757–795.\nDieuwke Hupkes, Anand Singh, Kris Korrel, German\nKruszewski, and Elia Bruni. 2018. Learning compo-\nsitionally through attentive guidance. arXiv preprint\narXiv:1805.09657.\nEugene Kharitonov and Rahma Chaabouni. 2021.\nWhat they do when in doubt: a study of inductive\nbiases in seq2seq learners. In ICLR.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nBrenden Lake. 2019. Compositional generalization\nthrough meta sequence-to-sequence learning. In\nNeurIPS.\nBrenden Lake and Marco Baroni. 2018. Generaliza-\ntion without systematicity: On the compositional\nskills of sequence-to-sequence recurrent networks.\nIn ICML.\nYuanpeng Li, Liang Zhao, Jianyu Wang, and Joel Hest-\nness. 2019. Compositional generalization for primi-\ntive substitutions. arXiv preprint arXiv:1910.02612.\nJoao Loula, Marco Baroni, and Brenden M Lake. 2018.\nRearranging the familiar: Testing compositional\ngeneralization in recurrent networks. EMNLP 2018.\nChristopher Manning, Mihai Surdeanu, John Bauer,\nJenny Finkel, Steven Bethard, and David McClosky.\n2014. The Stanford CoreNLP natural language pro-\ncessing toolkit. In ACL: System Demonstrations.\nR Thomas McCoy, Robert Frank, and Tal Linzen. 2020.\nDoes syntax need to grow on trees? sources of hier-\narchical inductive bias in sequence-to-sequence net-\nworks. TACL, 8:125–140.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In NAACL-HLT\n2019: Demonstrations.\nEmilio Parisotto, Francis Song, Jack Rae, Razvan Pas-\ncanu, Caglar Gulcehre, Siddhant Jayakumar, Max\nJaderberg, Raphael Lopez Kaufman, Aidan Clark,\nSeb Noury, et al. 2020. Stabilizing transformers for\nreinforcement learning. In ICLR.\n145\nJack Rae and Ali Razavi. 2020. Do transformers need\ndeep long-range memory? In ACL.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nJake Russin, Jason Jo, Randall C O’Reilly, and Yoshua\nBengio. 2019. Compositional generalization in a\ndeep seq2seq model by separating syntax and seman-\ntics. arXiv preprint arXiv:1904.09708.\nTim Salimans and Durk P Kingma. 2016. Weight nor-\nmalization: A simple reparameterization to acceler-\nate training of deep neural networks. In NeurIPS.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. In NAACL, pages 464–468.\nSainbayar Sukhbaatar, ´Edouard Grave, Piotr Bo-\njanowski, and Armand Joulin. Adaptive attention\nspan in transformers.\nIlya Sutskever, James Martens, George Dahl, and Geof-\nfrey Hinton. 2013. On the importance of initializa-\ntion and momentum in deep learning. In ICML.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nIn NeurIPS.\nBaosong Yang, Zhaopeng Tu, Derek F Wong, Fandong\nMeng, Lidia S Chao, and Tong Zhang. 2018. Mod-\neling localness for self-attention networks. arXiv\npreprint arXiv:1810.10182.\n146\nA Hyperparameter grids\nA.1 SCAN\nFor each architecture, we used the same hyperpa-\nrameter grids for all splits of SCAN.\nAll models were trained by Adam with defaultβ1\nand β2 parameters, for 250 epochs, with batch size\n256, learning rate 5 ·10−4, dropout and attention\ndropout 0.1, random seeds {0,1,2}. We vary the\nEncoder and Decoder parameters independently:\nnumber of attention heads {4,8}, embedding di-\nmensions {128,256}, FFN dimensions{256,512},\nand the number of layers {4,6,8}; clip norm 1.0.\nFor hybrids models, add the following parame-\nters.\nSAG To reduce the search space, we did not vary\nβ0, setting it to −1.\nSAG + CNN as self-attention Span: {2,4,6},\nβ0 = −1, number of layers: {4, 6}.\nSAG + ﬁxed span self-attention Span:\n{2,4,6}, β0 = −1, number of layers: 4.\nSAG + T5 Span: {2,4,6}, β0 = −1, number of\nlayers: 4.\nA.2 Machine Translation\nDe-En We start from the standard architecture\nsuggested by Fairseq examples for IWSLT’14 De-\nEn. That is we share decoder input and output\nembeddings. Both Encoder and decoder have an\nembedding size of 512 FFN dimensions of 1024, 4\nattention heads, 6 encoder & decoder layers. We\nused adam optimizer with learning rate of 5e-4,\nno clip norm, warm-up learning rate 1e-7, inverse\nsquare root learning rate scheduler, 4000 warm-\nup updates, dropout 0.3, weight decay 1e-4, label\nsmoothing 0.1, max. tokens per batch per GPU:\n1024, 40 epochs. We used 4 GPUs for training.\nFor SAG-enabled architectures, we additionally\nsearched for Encoder’sβ0 in {−1,0}and {1,0}for\nDecoder. We varied attention span in{2,4,6}, tten-\ntion dropout in {0.0,0.2}and pre-block encoder\nand decoder normalization.\nFor model selection, we also follow Fairseq ex-\nample and checkpoint the best model on validation\nset based on BLEU score. BLEU score is computed\nwith a beam of size 5.\nFLoRes We used shared embeddings between\nEncoder and Decoder, embedding dimenions of\n512, FFN dimensions of 2048, 2 attention heads,\n5 encoder & decoder layers. We used pre-block\nnormalization9, learning rate of 1e-3, no clip norm,\nwarm-up learning rate 1e-7, inverse square root\nlearning rate scheduler, 4000 warm-up updates,\ndropout 0.4, activation dropout 0.2, weight decay\n1e-4, label smoothing 0.2, max. tokens per batch\nper GPU: 4000, 100 epochs. We searched for at-\ntention dropout in {0.0,0.2}. We used 4 GPUs for\ntraining.\nFor SAG-enabled arcchitectures, we addition-\nally searched for Encoder’sβ0 in {−2,−1,0}and\n{2,1,0}for Decoder. We varied attention span in\n{2,4,6}.\nNoun-adjective order agreement We start from\nthe standard architecture suggested by Fairseq ex-\namples for WMT’14 En-Fr. That is we share en-\ncoder, decoder and output embeddings. Both En-\ncoder and decoder have an embedding size of 1024\nFFN dimensions of 4096, 16 attention heads, 6 en-\ncoder & decoder layers. We used adam optimizer\nwith learning rate of 7e-4, no clip norm, warm-up\nlearning rate 1e-7, inverse square root learning rate\nscheduler, 4000 warm-up updates, dropout 0.1, la-\nbel smoothing 0.1, max. tokens per batch per GPU:\n4000, 30 epochs. We used 6 GPUs for training.\nFor SAG-enabled architectures, we additionally\nsearched for Encoder’sβ0 in {−1,0}and {1,0}for\nDecoder. We varied attention span in{2,4,6}, tten-\ntion dropout in {0.0,0.2}and pre-block encoder\nand decoder normalization.\nBest checkpoint is based on the loss of the vali-\ndation set.\nB Other modiﬁcations on FloRes and\nNoun-adjective ordering datasets\nIn the main paper, we only experimented with SAG\n+ T5 as the only non-degraded modiﬁcation on the\nIWSLT’14 En-De dataset. Our intuition is that\nthe remaining hybrid models are SCAN-tailored\nand would not lead to any improvement in the\nlow-resource (FloRes) and domain-shifted (Noun-\nadjective ordering dataset) settings. In this section,\nwe verify our intuition and report the results of\nall the introduced variants. The hyper-parameters\nsearch is reported in Section A.\nFloRes We report results on English ↔Nepali\nand English↔Sinhala translation in Table 7. We\n9--encoder-normalize-before and\n--decoder-normalize-before in fairseq.\n147\nalso report Guzm ´an et al. (2019) results under\n“Baseline”.\nAnalyzing results of SAG, we notice that it is\nusually very close to Transformer’s results on all\ntasks, apart from Nepali →English, where it lags\nbehind. The ﬁxed-span modiﬁcation performs\nworse than Transformer on in all directions. Re-\nplacing self-attention with convolutions results in\nbetter scores on En →Ne and worse scores on\nNe→En/En→Si.\nHence, as expected, only the SAG + T5 model\noutperforms Transformer on all language pairs and\ndirections, highlighting the importance of verifying\nthe generality of the model on realistic datasets.\nNoun-adjective order agreement BLEU scores\non the test set are reported in Table 8. SAG leads\nto a relative improvement of 1.44% compared to\nstandard Transformer, closely followed by SAG +\nT5. Still, in total, the differences are very small\nacross all models. On the other hand, all intro-\nduced variants outperform standard Transformer\non COMP. However, only SAG + T5 demonstrates\na signiﬁcant improvement with 14% relative gain.\nOverall, we observe that the SCAN-tailored vari-\nants do not degrade performances on the Noun-\nadjective order agreement dataset, but still do not\nlead to any signiﬁcant improvement, contrary to\nSAG + T5.\nC Visualizing attention biases\nIn this Section, we illustrate how a successful SAG\n+ T5 model uses its bij terms (Eq. 1) to control its\nattention.\nWe take the most successful hyperparameter\ncombination on SCAN- jump in Table 3 and se-\nlect a model instance that has the best accuracy\n(≈60%). Next, for each attention head of each\nencoder and decoder layer, we retrieve its learned\nrelative-position bias bd, where dis a (signed) rel-\native distance between positions i and j, that is\ncapped to be within [−s,+s] (see Section 3). For\neach head, we apply a softmax, to ﬁnd its “prefer-\nence” ˆbd over relative positions d:\nˆbd = exp(bd)∑\ndexp(bd)\nWe report the results in Figure 1. Interestingly,\nquite a few attention heads have very strong pref-\nerences for ﬁxed relative positions and some are\neven dominantly focused on particular positions\n(Encoder: head 7 in the layer 0; heads 4, 5 in layer\n1, heads 3, 7 in layer 2; head 2 in layer 3; Decoder:\nhead 4 in layer 0, head 2 in layer 1, heads 3,4,5 in\nlayer 2; heads 2, 6, 7 in layer 3)10. More often than\nnot, those “specialized” heads look within the span\nand not on the “border” values of d(due to dbeing\ncapped, they also correspond to arbitrary distant\npositions to the left and right).\nHence we conclude that in a T5 model (among\nmost successful on SCAN- jump), several heads\nleverage the ability pay attention locally; support-\ning our ﬁnding that local attention is connected\nwith the compositional generalization needed to\nsucceed at SCAN. At the same time, some heads\nhave large relative-position bias for distant posi-\ntions ([s,+∞[ or ] −∞,−s]). This general ability\nto optionally look beyond a ﬁxed span in T5 could\nbe responsible for its better performance compared\nto the ﬁxed span modiﬁcation.\n10T5 reduces to the vanilla Transformer if all bd are equal\nto zero. That corresponds to the uniform bias ˆbd.\n148\nBaseline Transformer SAG + Conv s.-a. + ﬁxed span + T5\nne-en 7.6 7.94±0.05 7.58±0.06 7.59±0.02 7.44±0.08 8.40±0.02\nsi-en 7.2 7.15±0.07 7.14±0.10 7.18±0.10 .78±0.07 7.41±0.10\nen-ne 4.3 4.43±0.01 4.36±0.08 4.63±0.03 4.12±0.05 5.01±0.10\nen-si 1.2 2.32±0.08 2.37±0.10 2.14±0.03 2.12±0.04 2.54±0.03\nTable 7: BLEU dev-test scores on FLoRes. Baseline scores are taken from (Guzm ´an et al., 2019). In bold are\nvalues that stat. sig. improve over Transformer (p< 10−3). ±indicates 1 SEM.\nTransformer SAG + Conv s.-a. + ﬁxed span + T5\nBLEU 40.86±0.34 41.45±0.14 39.89±0.27 41.01±0.24 41.43±0.29\nCOMP 0.64±0.01 0.70±0.03 0.67±0.01 0.68±0.01 0.73±0.01\nTable 8: BLEU and COMP measures on test sets: compositional English to French dataset. In bold are values that\nstat. sig. improve over Transformer (p< 0.05). ±denotes 1 SEM.\n0 1 2 3 4 5 6 7\nhead\n- ..-4\n-3\n-2\n-1\n0\n1\n2\n3\n4..\nrelative distance\n0.2\n0.4\n0.6\n(a) Encoder layer 0.\n0 1 2 3 4 5 6 7\nhead\n- ..-4\n-3\n-2\n-1\n0\n1\n2\n3\n4..\nrelative distance\n0.2\n0.4\n0.6 (b) Encoder layer 1.\n0 1 2 3 4 5 6 7\nhead\n- ..-4\n-3\n-2\n-1\n0\n1\n2\n3\n4..\nrelative distance\n0.2\n0.4 (c) Encoder layer 2.\n0 1 2 3 4 5 6 7\nhead\n- ..-4\n-3\n-2\n-1\n0\n1\n2\n3\n4..\nrelative distance\n0.2\n0.4 (d) Encoder layer 3.\n0 1 2 3 4 5 6 7\nhead\n- ..-6\n-5\n-4\n-3\n-2\n-1\n0\nrelative distance\n0.2\n0.4\n0.6\n(e) Decoder layer 0.\n0 1 2 3 4 5 6 7\nhead\n- ..-6\n-5\n-4\n-3\n-2\n-1\n0\nrelative distance\n0.2\n0.4 (f) Decoder layer 1.\n0 1 2 3 4 5 6 7\nhead\n- ..-6\n-5\n-4\n-3\n-2\n-1\n0\nrelative distance\n0.2\n0.4\n0.6 (g) Decoder layer 2.\n0 1 2 3 4 5 6 7\nhead\n- ..-6\n-5\n-4\n-3\n-2\n-1\n0\nrelative distance\n0.25\n0.50\n0.75 (h) Decoder layer 3.\nFigure 1: Relative attention biases for T5 + SAG architecture (after a softmax). Each cell indicates preference\nof a head to a position at a signed relative distance. The relative distances are capped. For the decoder we\nonly represent relative attention biases for d ≤0, as positions with positive relative distance are masked in the\nautoregressive decoder.",
  "topic": "Machine translation",
  "concepts": [
    {
      "name": "Machine translation",
      "score": 0.8116581439971924
    },
    {
      "name": "Computer science",
      "score": 0.80372154712677
    },
    {
      "name": "Transformer",
      "score": 0.7630071043968201
    },
    {
      "name": "Natural language processing",
      "score": 0.5556579828262329
    },
    {
      "name": "Generalization",
      "score": 0.544922947883606
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5442950129508972
    },
    {
      "name": "Task (project management)",
      "score": 0.43221527338027954
    },
    {
      "name": "Principle of compositionality",
      "score": 0.412986695766449
    },
    {
      "name": "Machine learning",
      "score": 0.36948996782302856
    },
    {
      "name": "Voltage",
      "score": 0.12154677510261536
    },
    {
      "name": "Engineering",
      "score": 0.08025491237640381
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210103330",
      "name": "École Normale Supérieure",
      "country": "BI"
    }
  ]
}