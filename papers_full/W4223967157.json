{
    "title": "ODE Transformer: An Ordinary Differential Equation-Inspired Model for Sequence Generation",
    "url": "https://openalex.org/W4223967157",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2099334605",
            "name": "Bei Li",
            "affiliations": [
                "Northeastern University"
            ]
        },
        {
            "id": "https://openalex.org/A2069793417",
            "name": "Quan Du",
            "affiliations": [
                "Northeastern University"
            ]
        },
        {
            "id": "https://openalex.org/A1979050912",
            "name": "Tao Zhou",
            "affiliations": [
                "Northeastern University"
            ]
        },
        {
            "id": "https://openalex.org/A2119632725",
            "name": "Yi Jing",
            "affiliations": [
                "Northeastern University"
            ]
        },
        {
            "id": "https://openalex.org/A2113355927",
            "name": "Shuhan Zhou",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2021422979",
            "name": "Xin Zeng",
            "affiliations": [
                "Northeastern University"
            ]
        },
        {
            "id": "https://openalex.org/A1983914940",
            "name": "Tong Xiao",
            "affiliations": [
                "Northeastern University"
            ]
        },
        {
            "id": "https://openalex.org/A2496766346",
            "name": "Jingbo Zhu",
            "affiliations": [
                "Northeastern University"
            ]
        },
        {
            "id": "https://openalex.org/A2098611229",
            "name": "Xuebo Liu",
            "affiliations": [
                "Harbin Institute of Technology",
                "Northeastern University"
            ]
        },
        {
            "id": "https://openalex.org/A2005452123",
            "name": "Min Zhang",
            "affiliations": [
                "Harbin Institute of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2933138175",
        "https://openalex.org/W3035747971",
        "https://openalex.org/W2408279554",
        "https://openalex.org/W4287597717",
        "https://openalex.org/W2963755523",
        "https://openalex.org/W2970868759",
        "https://openalex.org/W2962737770",
        "https://openalex.org/W2121498459",
        "https://openalex.org/W2963925437",
        "https://openalex.org/W2896807716",
        "https://openalex.org/W2963542740",
        "https://openalex.org/W3035083896",
        "https://openalex.org/W2866343820",
        "https://openalex.org/W2948981900",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W3046835050",
        "https://openalex.org/W3034465644",
        "https://openalex.org/W2974916071",
        "https://openalex.org/W2970731908",
        "https://openalex.org/W2975381464",
        "https://openalex.org/W4295915208",
        "https://openalex.org/W3113395007",
        "https://openalex.org/W3103334733",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W3047743574",
        "https://openalex.org/W2788421913",
        "https://openalex.org/W2600297185",
        "https://openalex.org/W2888520903",
        "https://openalex.org/W2785047343",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W4287812455",
        "https://openalex.org/W3200345787",
        "https://openalex.org/W2947156405",
        "https://openalex.org/W2606974598",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2124725212",
        "https://openalex.org/W2767008699",
        "https://openalex.org/W3098011980",
        "https://openalex.org/W1597944220",
        "https://openalex.org/W3006381853",
        "https://openalex.org/W2169062786",
        "https://openalex.org/W3174283492",
        "https://openalex.org/W3173417753",
        "https://openalex.org/W2185726469",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2170527467",
        "https://openalex.org/W2970290486",
        "https://openalex.org/W2964093309",
        "https://openalex.org/W3035618017",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W3102816807",
        "https://openalex.org/W4286952135",
        "https://openalex.org/W2561907692",
        "https://openalex.org/W3093968463",
        "https://openalex.org/W4298625139",
        "https://openalex.org/W2949615363",
        "https://openalex.org/W2741494657",
        "https://openalex.org/W2125616599",
        "https://openalex.org/W3034742481",
        "https://openalex.org/W2963359731",
        "https://openalex.org/W4394643672",
        "https://openalex.org/W2889518897"
    ],
    "abstract": "Bei Li, Quan Du, Tao Zhou, Yi Jing, Shuhan Zhou, Xin Zeng, Tong Xiao, JingBo Zhu, Xuebo Liu, Min Zhang. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.",
    "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 8335 - 8351\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nODE Transformer: An Ordinary Differential Equation-Inspired Model\nfor Sequence Generation\nBei Li1, Quan Du1,2, Tao Zhou1, Yi Jing1, Shuhan Zhou1, Xin Zeng1\nTong Xiao1,2∗, Jingbo Zhu1,2, Xuebo Liu3 and Min Zhang3\n1School of Computer Science and Engineering, Northeastern University, Shenyang, China\n2NiuTrans Research, Shenyang, China\n3Institute of Computing and Intelligence, Harbin Institute of Technology, Shenzhen, China\nlibei_neu@outlook.com, {xiaotong,zhujingbo}@mail.neu.edu.cn,\n{liuxuebo,zhangmin2021}@hit.edu.cn\nAbstract\nResidual networks are an Euler discretization\nof solutions to Ordinary Differential Equa-\ntions (ODE). This paper explores a deeper re-\nlationship between Transformer and numeri-\ncal ODE methods. We ﬁrst show that a resid-\nual block of layers in Transformer can be de-\nscribed as a higher-order solution to ODE.\nInspired by this, we design a new architec-\nture, ODE Transformer, which is analogous\nto the Runge-Kutta method that is well moti-\nvated in ODE. As a natural extension to Trans-\nformer, ODE Transformer is easy to imple-\nment and efﬁcient to use. Experimental results\non the large-scale machine translation, abstrac-\ntive summarization, and grammar error cor-\nrection tasks demonstrate the high genericity\nof ODE Transformer. It can gain large im-\nprovements in model performance over strong\nbaselines (e.g., 30.77 and 44.11 BLEU scores\non the WMT’14 English-German and English-\nFrench benchmarks) at a slight cost in infer-\nence efﬁciency.\n1 Introduction\nResidual networks have been used with a great\nsuccess as a standard method of easing information\nﬂow in multi-layer neural models (He et al., 2016;\nVaswani et al., 2017). Given an input yt, models of\nthis kind deﬁne the output of a layer tto be:\nyt+1 = yt + F(yt,θt) (1)\nwhere F(·,·) is the function of the layer andθtis its\nparameter. Interestingly, recent work in machine\nlearning (Weinan, 2017; Lu et al., 2018; Haber\net al., 2018; Chang et al., 2018; Ruthotto and Haber,\n2019) points out that Eq. (1) is an Euler discretiza-\ntion of the Ordinary Differential Equation (ODE),\nlike this:\ndy(t)\ndt = F(y(t),θ(t)) (2)\n∗Corresponding author.\n6 ODE blocks with\n1st-order solutions\nθ1\nθ2\nθ3\nθ4\nθ5\nθ6\n3 ODE blocks with\n2nd-order solutions\nθ1\nθ2\nθ3\n: F(·,θt) : ODE block\nFigure 1: Models with different ODE blocks.\nwhere y(t) and θ(t) are continuous with respect to\nt. In this way, we can call Eq. (1) an ODE block.\nThis ﬁnding offers a new way of explaining resid-\nual networks in the view of numerical algorithms.\nThen, one can think of a multi-layer network as\napplying the Euler method (i.e., Eq. (1)) to solve\nEq. (2) subject to the initial conditions y(0) = y0\nand θ(0) = θ0.\nThe solution of Eq. (2) has a sufﬁciently low\nerror bound (call it a stable solution) only if θ(t)\nchanges slow along t(Haber and Ruthotto, 2017;\nChen et al., 2018). But this assumption does not\nalways hold for state-of-the-art natural language\nprocessing (NLP) systems, in which models are\nnon-linear and over-parameterized. For example,\nlanguage modeling and machine translation sys-\ntems learn quite different parameters for different\nlayers, especially when the layers are close to the\nmodel input (Vaswani et al., 2017; Dai et al., 2019).\nAlso, truncation errors are nonnegligible for the\nEuler method because it is a ﬁrst-order approxima-\ntion to the true solution (He et al., 2019). These\nproblems make the situation worse, when more lay-\ners are stacked and errors are propagated through\nthe neural network. It might explain why recent\n8335\nMachine Translation (MT) systems cannot beneﬁt\nfrom extremely deep models (Wang et al., 2019;\nLiu et al., 2020a; Wei et al., 2020; Li et al., 2020).\nThis paper continues the line of research on the\nODE-inspired method. The basic idea is to use\na high-order method for more accurate numerical\nsolutions to the ODE. This leads to a larger ODE\nblock that generates a sequence of intermediate ap-\nproximations to the solution. We ﬁnd that the larger\nODE block is sufﬁcient to take the role of several\nODE blocks with ﬁrst-order solutions. The beneﬁt\nis obvious: the use of fewer ODE blocks lowers\nthe risk of introducing errors in block switching,\nand the high-order method reduces the approxima-\ntion error in each ODE block. See Figure 1 for a\ncomparison of different models.\nOur method is parameter-efﬁcient because θ(t)\nis re-used within the same ODE block. As another\n“bonus\", the model can be improved by learning\ncoefﬁcients of different intermediate approxima-\ntions in a block. We evaluate our method in strong\nTransformer systems, covering both the wide (and\nbig) model and the deep model. For machine trans-\nlation tasks, ODE Transformer achieves 30.77 and\n44.11 BLEU scores on the WMT’14 En-De and\nEn-Fr test sets, setting a new state-of-the-art on the\nWMT’14 En-Fr task. It also signiﬁcantly outper-\nforms baselines on abstractive summarization and\ngrammar error correction tasks.\n2 Transformer and ODEs\nWe start with a description of Transformer, fol-\nlowed by its relationship with ODEs. We choose\nTransformer for our discussion and experiments\nbecause it is one of the state-of-the-art models in\nrecent sentence generation tasks.\n2.1 Transformer\nTransformer is an example of the encoder-decoder\nparadigm (Vaswani et al., 2017). The encoder is\na stack of identical layers. Each layer consists of\na self-attention block and a feedforward network\n(FFN) block. Both of them equip with a residual\nconnection and a layer normalization unit. Note\nthat the term “block” is used in many different\nways. In this paper, the term refers to any neural\nnetwork that is enhanced by the residual connection\n(occasionally call it a residual block). Following\nthe Pre-norm architecture (Wang et al., 2019), we\ndeﬁne a block as\nyt+1 = yt + G(LN(yt),θt) (3)\nwhere LN(·) is the layer normalization function,1\nand G(·) is either the self-attention or feedforward\nnetwork. The decoder shares a similar architec-\nture, having an additional encoder-decoder atten-\ntion block sandwiched between the self-attention\nand FFN blocks.\n2.2 Ordinary Differential Equations\nAn ordinary differential equation is an equation\ninvolving a function y(t) of a variable t and its\nderivatives. A simple form of ODE is an equation\nthat deﬁnes the ﬁrst-order derivative of y(t), like\ndy(t)\ndt = f(y(t),t) (4)\nwhere f(y(t),t) deﬁnes a time-dependent vector\nﬁeld if we know its value at all points of yand all\ninstants of time t. Eq. (4) covers a broad range\nof problems, in that the change of a variable is de-\ntermined by its current value and a time variable t.\nThis formulation also works with Pre-norm Trans-\nformer blocks. For notational simplicity, we re-\ndeﬁne G(LN(yt),θt) as a new function F(yt,θt):\nF(yt,θt) = G(LN(yt),θt)) (5)\nWe then relax yt and θt to continuous functions\ny(t) and θ(t), and rewrite Eq. (3) to be:\ny(t+ ∆t) = y(t) + ∆t·F(y(t),θ(t)) (6)\nwhere ∆tis the change of t, and is general called\nstep size. Obviously, we have ∆t = 1 in Trans-\nformer. But we can adjust step size ∆t using a\nlimit, and have\nlim\n∆t→0\ny(t+ ∆t) −y(t)\n∆t = F(y(t),θ(t)) (7)\nGiven the fact that lim∆t→0\ny(t+∆t)−y(t)\n∆t = dy(t)\ndt ,\nEq. (7) is an instance of Eq. (4). The only differ-\nence lies in that we introduce θ(t) into the right-\nhand side of Eq. (4). Then, we say that a Pre-norm\nTransformer block describes an ODE. It has been\nfound that Eq. (3) shares the same form as the Eu-\nler method of solving the ODE described in Eq. (7)\n(Haber and Ruthotto, 2017). This establishes a re-\nlationship between Transformer and ODEs, in that,\ngiven F(·,·) and learned parameters {θt}, the for-\nward pass of a multi-block Transformer is a process\nof running the Euler method for several steps.\n1We drop the parameter of LN(·) for simplicity.\n8336\n3 The ODE Transformer\nIn numerical methods of ODEs, we want to en-\nsure the precise solutions to the ODEs in a mini-\nmum number of computation steps. But the Euler\nmethod is not “precise” because it is a ﬁrst-order\nmethod, and naturally with local truncation errors.\nThe global error might be larger if we run it for\na number of times. 2 This is obviously the case\nfor Transformer, especially when the multi-layer\nneural network arises a higher risk of instability in\nsolving the ODEs (Haber and Ruthotto, 2017).\n3.1 High-Order ODE Solvers\nHere we use the Runge-Kutta methods for a higher\norder solution to ODEs (Runge, 1895; Kutta, 1901;\nButcher, 1996; Ascher and Petzold, 1998). They\nare a classic family of iterative methods with dif-\nferent orders of precision.3 More formally, the ex-\nplicit Runge-Kutta methods of an n-step solution\nis deﬁned to be:\nyt+1 = yt +\nn∑\ni=1\nγiFi (8)\nF1 = hf(yt,t) (9)\nFi = hf(yt +\ni−1∑\nj=1\nβijFj,t + αih) (10)\nwhere his the step size and could be simply 1 in\nmost cases. Fi is an intermediate approximation\nto the solution at step t+ αih. α, βand γare co-\nefﬁcients which can be determined by the Taylor\nseries of yt+1 (Butcher, 1963). Eq. (10) describes a\nsequence of solution approximations {F1,...,F n}\nover n steps {t+ α1h,...,t + αnh}. These ap-\nproximations are then interpolated to form the ﬁnal\nsolution, as in Eq. (8).\nThe Runge-Kutta methods are straightforwardly\napplicable to the design of a Transformer block. All\nwe need is to replace the function f (see Eq. (10))\nwith the function F (see Eq. (5)). The advantage\nis that the function F is re-used in a block. Also,\nthe model parameter θt can be shared within the\nblock.4 In this way, one can omit t+ αihin Eq.\n2The global error is what we would ordinarily call the error:\nthe difference between y(t) and the true solution. The local\nerror is the error introduced in a single step: the difference\nbetween y(t) and the solution obtained by assuming thaty(t−\n1) is the true solution\n3A p-order numerical method means that the global trun-\ncation error is proportional to ppower of the step size.\n4Although we could distinguish the parameters at different\nsteps in a block, we found that it did not help and made the\nmodel difﬁcult to learn.\n(10), and compute Fi by\nFi = F(yt +\ni−1∑\nj=1\nβijFj,θt) (11)\nThis makes the system more parameter-efﬁcient.\nAs would be shown in our experiments, the high-\norder Runge-Kutta methods can learn strong NMT\nsystems with signiﬁcantly smaller models.\nThe Runge-Kutta methods are general. For ex-\nample, the Euler method is a ﬁrst-order instance\nof them. For a second-order Runge-Kutta (RK2)\nblock, we have\nyt+1 = yt + 1\n2(F1 + F2) (12)\nF1 = F(yt,θt) (13)\nF2 = F(yt + F1,θt) (14)\nThis is also known as the improved Euler method.\nLikewise, we can deﬁne a fourth-order Runge-\nKutta (RK4) block to be:\nyt+1 = yt +\n1\n6(F1 + 2F2 + 2F3 + F4) (15)\nF1 = F(yt,θt) (16)\nF2 = F(yt + 1\n2F1,θt) (17)\nF3 = F(yt + 1\n2F2,θt) (18)\nF4 = F(yt + F3,θt) (19)\nSee Figure 2 for a comparison of different\nRunge-Kutta blocks. It should be noted that the\nmethod presented here can be interpreted from\nthe perspective of representation reﬁnement (Greff\net al., 2017). It provides a way for a function to\nupdate the function itself. For example, Universal\nTransformer reﬁnes the representation of the input\nsequence using the same function and the same pa-\nrameters in a block-wise manner (Dehghani et al.,\n2019). Here we show that inner block reﬁnements\ncan be modeled with good theoretical support.\n3.2 Coefﬁcient Learning\nIn our preliminary experiments, the RK2 and RK4\nmethods yielded promising BLEU improvements\nwhen the model was shallow. But it was found that\nthe improvements did not persist for deeper models.\nTo ﬁgure out why this happened, let us review the\nRunge-Kutta methods from the angle of training.\n8337\nyt+1\nF\nyt\n(a) Residual block\nyt+1\nF\nF\nyt\n1\n2\n1\n2\n(b) RK2-block\nyt+1\nF\nF\nF\nF\nyt\n1\n6\n1\n2\n2\n6\n2\n6\n1\n6\n1\n2\n(c) RK4-block\nyt+1\nF\nσ\n×\nF\n×1-\nyt\n(d) RK2-block (learnable γi)\nFigure 2: Architectures of ODE Transformer blocks.\nTake the RK2 method as an example. We rewrite\nEq. (12) by substituting F1 and F2, as follow\nyt+1 = yt + 1\n2F(yt,θt) +\n1\n2F(yt + F(yt,θt),θt) (20)\nLet Ebe the loss of training, L be the number\nblocks of the model, and yL be the model output.\nThe gradient of Eat yt is\n∂E\n∂yt\n= ∂E\n∂yL\n· 1\n2L−t ·\nL−1∏\nk=t\n(1 + gk) (21)\nwhere\ngk =\n(\n1 + ∂F(yk,θk)\n∂yk\n)\n·\n(\n1 + ∂F(yk + F(yk,θk),θk)\n∂yk + F(yk,θk)\n)\n(22)\nSeen from Eq. (21), ∂E\n∂yt is proportional to the\nfactor 1\n2L−t. This leads to a higher risk of gradient\nvanishing when Lis larger.\nThe problem somehow attributes to the small\ncoefﬁcients of Fi, that is, γ1 = γ2 = 1\n2 . A natural\nidea is to empirically set γi = 1 to eliminate the\nproduct factor of less than 1 in gradient compu-\ntation, although this is not theoretically grounded\nin standard Runge-Kutta methods. We rewrite Eq.\n(20) with the new coefﬁcients, as follows\nyt+1 = yt + F(yt,θt) +\nF(yt + F(yt,θt),θt) (23)\nThen, we have the gradient, like this\n∂E\n∂yt\n= ∂E\n∂yL\n·\nL−1∏\nk=t\ngk (24)\nThis model is easy to optimize because ∂E\n∂yL\ncan be\npassed to lower-level blocks with no scales. Note\nthat, the methods here are instances of parameter\nsharing (Dehghani et al., 2019; Lan et al., 2020).\nFor example, in each ODE block, we use the same\nfunction F with the same parameter θt for all in-\ntermediate steps. Setting γi = 1 is a further step\ntowards this because Fi is passed to the following\ncomputations with the same scale. Here we call it\nimplicit parameter sharing.\nAnother way of scaling Fi to further improve\nODE functions is to learn the coefﬁcients automati-\ncally on the training data. The simplest method is to\ninitialize γi = 1 and independently optimize each\nscale. It helps the system learn the way of ﬂowing\nFi in a block. Based on it, scaling Fi by a weighted\ngate mechanism (Srivastava et al., 2015) empiri-\ncally achieves the best performance (see Section\n4). Take RK2-block as an instance, the concatena-\ntion of F1 and F2 is transformed to a scalar (0,1)\nthrough a sigmoid gate, then the block output yt+1\nis\nyt+1 = yt + g·F1 + (1 −g) ·F2 (25)\ng = sigmoid([F1,F2] ·W + b) (26)\nwhere [,] denotes the concatenation operation and\nW,b are learnable parameters. We call it RK2-\nblock (learnable γi), and the architecture is shown\nin Figure 2 (d). This kind of formulation offers a\nmore ﬂexible way to decide which part contributes\nmore and is also easy to be optimized. Moreover,\nwe also summarize the comparison of various scal-\ning functions in Appendix C.\n8338\nModel Layers WMT En-De WMT En-Fr\n#Param Steps BLEU SBLEU #Param Steps BLEU SBLEU\nTransformer (Vaswani et al., 2017) 6-6 213M 100K 28.40 - 222M 300K 41.00 -\nMacaronNet (Lu et al., 2019) 6-6 - - 30.20 - - - - -\nDepth growing (Wu et al., 2019) 8-8 270M 800K 29.92 - - - 43.27 -\nTransformer-DLCL (Wang et al., 2019) 30-6 137M 50K 29.30 28.6 - - - -\nMultiscale Collaborative (Wei et al., 2020) 18-6 512M 300K 30.56 - - - - -\nADMIN (Liu et al., 2020a) 60-12 262M 250K 30.01 29.5 - 250K 43.80 41.8\nSDT (Li et al., 2020) 48-6 192M 50K 30.21 29.0 198M 100K 43.28 41.5\nBERT-fused model (Zhu et al., 2020) 6-6 - - 30.75 - - - 43.78 -\nBase and Deep Models\nResidual-block 6-6 61M 50K 27.89 26.8 69M 100K 41.05 39.1\nRK2-block 6-6 61M 50K 28.67 27.5 69M 100K 42.08 40.1\nRK2-block (learnable γi) 6-6 61M 50K 28.89 27.7 69M 100K 42.31 40.3\nRK4-block 6-6 61M 50K 29.03 27.9 69M 100K 42.56 40.6\nResidual-block 24-6 118M 50K 29.43 28.3 123M 100K 42.67 40.6\nRK2-block 24-6 118M 50K 29.85 28.7 123M 100K 43.04 41.1\nRK2-block (learnable γi) 24-6 118M 50K 30.29 29.2 123M 100K 43.48 41.5\nRK4-block 24-6 118M 50K 29.80 28.8 123M 100K 43.28 41.3\nWide Models\nResidual-block-Big 6-6 211M 100K 29.21 28.1 221M 100K 42.89 40.9\nRK2-block 6-6 211M 100K 30.11 29.0 221M 100K 43.34 41.3\nRK2-block (learnable γi) 6-6 211M 100K 30.53 29.4 221M 100K 43.59 41.6\nRK4-block 6-6 211M 100K 30.39 29.3 221M 100K 43.55 41.6\nResidual-block-Big 12-6 286M 100K 29.91 28.9 297M 100K 43.22 41.2\nRK2-block 12-6 286M 100K 30.58 29.4 297M 100K 43.88 42.0\nRK2-block (learnable γi) 12-6 286M 100K 30.77 29.6 297M 100K 44.11 42.2\nRK4-block 12-6 286M 100K 30.55 29.4 297M 100K 43.81 41.9\nTable 1: Comparison with the state-of-the-arts on the WMT En-De and WMT En-Fr tasks. We both report the\ntokenized BLEU and SacreBLEU scores for comparison with previous work.\n3.3 Efﬁciency Discussion\nODE Transformer is efﬁcient to use. As we only\napply the ODE design schema to the encoder side,\nit only brings minor impacts on the inference speed\ndue to the autoregressive decoding schema. An-\nother concern here is memory consumption. ODE\nTransformer consumes more memory than the base-\nline in the same depth since we need to store the\nintermediate approximations in the forward pass.\nBut the additional consumption is less than that of\nthe baseline who has the same computation cost,\nwhich is acceptable for most scenarios. We give a\nquantitative analysis in Section 5.\n4 Experiments\nWe evaluated the ODE Transformer on three se-\nquence generation tasks: machine translation, ab-\nstractive summarization and grammar error correc-\ntion. The datasets we used are elaborated in the\nfollowing section, and more details of experimental\nsetups could be found in Appendix A and B.\n4.1 Datasets\nMachine Translation We report results on three\nWMT benchmarks. For the WMT’14 English-\nGerman (En-De) task, the training data consisted\nof approximately 4.5M tokenized sentence pairs,\nas in (Vaswani et al., 2017). All sentences were\nsegmented into sequences of sub-word units (Sen-\nnrich et al., 2016) with32K merge operations using\na shared vocabulary. We selected newstest2013\nas the validation data and newstest2014 as the\ntest data. For the WMT’14 English-French (En-\nFr) task, we used the dataset provided within\nFairseq, i.e., 36M training sentence pairs from\nWMT’14. newstest2012+newstest2013 was the\nvalidation data and newstest2014 was the test data.\nFor the WMT’16 English-Romanian (En-Ro) task,\nwe replicated the setup of (Mehta et al., 2020),\nwhich used 600K/2K/2K sentence pairs for train-\ning, evaluation and inference, respectively.\nAbstractive Summarization We also tested the\nmodels’ ability to process long sequences on the\nCNN-DailyMail summarization task (Nallapati\net al., 2016; Hermann et al., 2015). The prepro-\n8339\nModel Params Epochs BLEU\nTransformer in Mehta et al. (2020) 62M 170 34.30\nDeLight (Mehta et al., 2020) 53M 170 34.70\nInt Transformer†(Lin et al., 2020) - - 32.60\nTransformer (Our impl.) 69M 20 33.49\nRK2-block (learnable γi) 69M 20 34.94\nRK2-block-Big (learnable γi) 226M 20 35.28\nTable 2: Results on the WMT En-Ro task. †indicates\nthe related information is not reported.\ncessed method was the same as in (Ott et al., 2019).\nWe used a shared BPE with 30K operations, result-\ning in a vocabulary of 32,580 entries. The evalu-\nation metric was F1-Rouge (Lin, 2004) (Rouge-1,\nRouge-2 and Rouge-L).\nGrammar Error Correction We used the fol-\nlowing datasets as the training data, including Na-\ntional University of Singapore Corpus of Learner\nEnglish (NUCLE) (Dahlmeier et al., 2013), Lang-8\nCorpus of Learner English (Lang-8) (Tajiri et al.,\n2012), FCE dataset (Yannakoudakis et al., 2011),\nand Write & Improve + LOCNESS Corpus (Bryant\net al., 2019). We borrowed the setup from Chollam-\npatt and Ng (2018) and used the provided prepro-\ncessed script. The word-level dropout technique\nwas also applied to prevent the overﬁtting problem.\nLanguage Modeling The truncation error anal-\nysis is conducted on the Penn Treebank (Mikolov\net al., 2011), which is a widely-used language\nmodel dataset. It contains 88K, 3,370 and 3,761\nsentences for training, validation and test. The vo-\ncabulary size was 10K. We set the layer depth of\nthe language model to 1 or 2 to make a fair com-\nparison. Assume the layer depth is 1, then the\nloss between the block output and the ground-truth\ncould be regarded as the truncation error. It allevi-\nates the inﬂuence of the error accumulation across\ndifferent layers.\n4.2 Experimental Results\nResults of En-De and En-Fr Table 1 compares\nODE Transformer with several state-of-the-art sys-\ntems. Both RK2-block and RK4-block outper-\nform the baselines by a large margin with different\nmodel capacities. For example, RK2-block obtains\na +1.00 BLEU improvement with the base conﬁgu-\nration when the depth is6. RK4-block yields a gain\nof 0.17 BLEU points on top of RK2-block. This\nobservation empirically validates the conjecture\nthat high-order ODE functions are more efﬁcient.\nModel Params BLEU\nTransformer (Vaswani et al., 2017) 62M 27.30\nEvolved Transformer (So et al., 2019) 46M 27.70\nLite Transformer†(Wu et al., 2020) - 26.50\nDeLight (Mehta et al., 2020) 37M 27.60\nRK2-block (learnable γi, H=256, L=28) 37M 28.24\nRK2-block (learnable γi, H=256, L=18) 29M 27.84\nTable 3: The comparison of model efﬁciency on the\nWMT En-De task.\nWhen we switch to deep models, our method is\nmore parameter efﬁcient. E.g., RK2-block is com-\nparable with a strong 48-layer system (Li et al.,\n2020) with half of the encoder depth. Similarly,\nwide models can also beneﬁt from the enlarging\nlayer depth (Wei et al., 2020; Li et al., 2020). RK2-\nblock achieves BLEU scores of 30.77 and 44.11\non the En-De and the En-Fr tasks, signiﬁcantly sur-\npassing the standard Big model by 1.32 and 0.70\nBLEU points. This sets a new state-of-the-art on\nthese tasks with fewer parameters.\nResults of En-Ro Table 2 exhibits model param-\neters, total training steps and BLEU scores of sev-\neral strong systems on the En-Ro task. Again, ODE\nTransformer outperforms these baselines. As stated\nin (Mehta et al., 2020), they trained the model up\nto 170 epochs and obtained a BLEU score of 34.70\nthrough the DeLight model. However, the obser-\nvation here is quite different. The validation PPL\nbegins to increase after 20 epochs. Thus, our base-\nline is slightly inferior to theirs, but matches the\nresult reported in Lin et al. (2020). ODE blocks\nachieve even better performance with DeLight\nwithin much less training cost. For a bigger model\n(line 6), it obtains a BLEU score of 35.28.\nParameter Efﬁciency Table 3 summaries the re-\nsults of several efﬁcient Transformer variants, in-\ncluding Lite Transformer (Wu et al., 2020), De-\nLight (Mehta et al., 2020) and a light version of\nthe Evolved Transformer (So et al., 2019). As ex-\npected, ODE Transformer is promising for smaller\nmodels. It is comparable in BLEU with DeLight\nbut having 9M fewer parameters. Under the same\nmodel capacity, it outperforms DeLight by 0.64\nBLEU points. It may offer a new choice for deploy-\ning NMT systems on edge devices.\nResults of Summarization and Correction We\nalso evaluated the ODE Transformer on another\ntwo sequence generation tasks. Table 4 shows that\nboth RK2-block and RK4-block outperform the\n8340\nModel Summarization Correction\nRG-1 RG-2 RG-L Prec. Recall F 0.5\nLiu et al. (2020b) 41.00 18.30 37.90 66.80 35.00 56.60\nResidual-block 40.47 17.73 37.29 67.97 32.17 55.61\nRK2-block 41.58 18.57 38.41 68.21 35.30 57.49\nRK4-block 41.83 18.84 38.68 66.20 38.13 57.71\nTable 4: Results of ODE Transformer on the summa-\nrization and correction tasks.\nbaselines by a margin. Similarly, RK4-block is\nsuperior to RK2-block when the model is shallow.\nMore results and case studies could be found in\nAppendix C.\n5 Analysis\nHere we investigate some interesting issues. For\nsimplicity, we call RK2-block with coefﬁcients ini-\ntialized by 1 as RK2-block-v1, and learnable coef-\nﬁcients (Eq. (25) ) as RK2-block-v2.\nQuantization of the Truncation Error In fact,\nwe cannot obtain the “true” solution of each block\noutput in NMT, because we mainly experimented\non the encoder side. Instead, we tested our system\non the language modeling task, where the perplex-\nity between the single-layer model output and the\nground truth could be regarded as the truncation\nerror with no error propagations. Table 5 shows the\nperplexities on the Penn Treebank dataset (Mikolov\net al., 2011). All ODE Transformer variants reduce\nthe errors signiﬁcantly. RK4-order achieves the\nlowest PPL on both settings. In addition, RK2-\nblock can even obtain a lower PPL than a 2-layer\nresidual-block. The observation here again veriﬁes\nlarger ODE blocks behave superior to the standard\nresidual block.\nInference Speed and Memory Consumption\nTable 6 shows the comparison of inference speed\nand memory consumption discussed in Section\n3.3. Experimental results demonstrate the proposed\nODE design schema results in acceptable inference\nspeeds. And it is also memory-friendly through the\nmemory comparison between the baseline and the\nRK variants in both base and big conﬁgurations.\nBLEU against Encoder Depth Figure 3 (left)\ndepicts BLEU scores of several ODE Transformer\nvariants and the baseline under different encoder\ndepths. All ODE Transformer variants are signif-\nicantly superior to the baseline when depth ≤24.\nRK2-block-v2 almost achieves the best perfor-\nModel 1-Layer 2-Layer\nResidual-Block 142.33 136.07\nRK2-block 131.80 123.12\nRK2-block (γi = 1) 132.67 123.90\nRK2-block (learnable γi) 128.48 121.02\nRK4-block 126.89 119.46\nTable 5: Comparison of PPL on systems with different\nODE blocks.\nModel Depth Inference Memory\nBase Big Base Big\nResidual-Block 6 147.1 98.7 7.2 13.2\nResidual-Block 12 141.3 94.5 10.9 18.7\nResidual-Block 24 122.0 87.3 14.1 23.5\nRK2-Block 6 141.6 93.9 8.5 15.1\nRK4-Block 6 124.8 87.1 9.7 18.2\nTable 6: Comparison of inference speed (sentences/s)\nand memory consumption (G).\nmance over all depths, especially when the model\nbecomes deeper. Interestingly, Figure 3 conﬁrms\nagain that ODE Transformer is parameter efﬁcient,\ne.g., a 6-layer RK2-block is comparable with the\n18-layer baseline system. Another ﬁnding here is\nRK4-block performs well on shallow models, but\nit is inferior to RK2-block when the depth is go-\ning deep. This is because original coefﬁcients may\ncause the optimization problem in the backward\npropagation in deep models (see Section 3.2). Also,\nFigure 3 (right) plots BLEU as a function of the\nmodel size when the hidden size is 256. The RK2\nmethod signiﬁcantly surpasses the baseline using\nmuch fewer parameters.\nAblation Study on DifferentF(·,·) As stated in\nSection 3, the F(·,·) function can either be SAN,\nFFN or both of them (SAN+FFN). As shown in\nFigure 4, high-order ODE works better with FFN\nthan SAN. An explanation might be that the FFN\ncomponent has more parameters than the SAN com-\nponent.5 The model that treats FFN and SAN as a\nsingle ODE block behaves the best.\nTraining and Validation Perplexity Figure 5\nplots the training and validation PPL curves of RK\nblocks and the baseline enhanced by RPR (Shaw\net al., 2018). RK2-block obtains lower training and\nvalidation PPLs in both conﬁgurations (base and\nwide models).\n5There are 2 · dmodel · 4dmodel parameters in FFN and\ndmodel · 3dmodel + dmodel · dmodel in SAN.\n8341\n6 12 18 24 30 3627.5\n28.5\n29.5\n30.5\nEncoder Depth\nBLEU RK2-block\nRK2-block-v1\nRK4-block\n20 40 60 80 100 12024.0\n25.0\n26.0\n27.0\n28.0\n29.0\nNumber of Parameters (M)\nBLEU\nResidual-block\nRK2-block-v2\nFigure 3: The comparison of BLEU against different\nencoder depth and the number of model parameters.\n27.5 28.0 28.5 29.0\nSAN/FNN\nSAN\nFFN\nSAN+FFN\nBaseline\n28.65\n28.17\n28.55\n28.89\n27.6\nBLEU (%)\nFigure 4: BLEU scores [%] of several F(·,·) on the\nWMT En-De task.\nVisualization of the Gradient Norm We also\ncollect the gradient information of several well-\ntrained systems during training. Figure 6 plots the\ngradient norm of RK2-block-v2, RK4-block and\nthe standard residual-block (baseline). As we can\nsee that Pre-Norm residual block is able to make\nthe training stable (Wang et al., 2019). Both RK2-\nblock-v2 and RK4-block provide richer signals due\nto the implicit parameter sharing among interme-\ndiate approximations. The two learning curves\nappear to be nearly the same, which is consistent\nwith the results in Table 1.\nComparison of Different ODE Design Schemas\nThen, we take a comprehensive analysis of sev-\neral ODE design schemas. As stated in Lu et al.\n(2018)’s work, several models in computer vision,\nsuch as LeapfrogNet (He et al., 2019), PolyNet\n(Zhang et al., 2017) and MultistepNet (Lu et al.,\n2018), can also be interpreted from the ODE per-\nspective. The related ODE functions are summa-\nrized in Table 7. We re-implemented these methods\nusing the same codebase for fair comparisons. We\nconducted experiments following the base conﬁgu-\nration on the En-De task.\nAt the time t, Multistep Euler methods require\nprevious states, e.g. yt−1, to generate the cur-\nrent approximation, instead of iterative reﬁnements\nbased on the current-time state. So these meth-\nods are heavier than ODE Transformer. Note that\nDLCL (Wang et al., 2019) can also be regarded as a\n2 6 10 14 18 22\n4.0\n8.0\n12.0\n16.0\nEpoch\nTraining PPL\nRK2-Big\nBig\nRK2-Base\nBase\n2 6 10 14 18 22\n4.0\n5.0\n6.0\n7.0\n8.0\nEpoch\nValidation PPL\nRK2-Big\nBig\nRK2-Base\nBase\nFigure 5: The comparison of training and validation\nPPL on base and wide models.\n0 10 20 30 40 50\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nStep (K)\nValue\nResidual-block\nRK2-block-v2\nRK4-block\nFigure 6: Visualization of the gradient norm of ODE\nTransformers compared with the baseline.\nmultistep Euler method, which is more competitive\nin deep Transformer. But there is just a modest\nimprovement upon the shallow baseline. Theoreti-\ncally, the Backward Euler method is slightly better\nthan the Forward Euler method in numerical analy-\nsis, but the improvement is marginal. Note that our\nODE Transformer achieves consistent BLEU im-\nprovements over the aforementioned methods. The\nreason is that such iterative reﬁnements provide\nmore efﬁcient and effective parameter learning.\n6 Related Work\nDeep Transformer models Recently, deep\nTransformer has witnessed tremendous success\nin machine translation, especially on WMT news\ntasks (Li et al., 2019; Zhang et al., 2020; Zhou et al.,\n2021; Tran et al., 2021). A straightforward way is\nto shorten the path from upper-level layers to lower-\nlevel layers thus to alleviate the gradient vanishing\nor exploding problems (Bapna et al., 2018; Wang\net al., 2019; Wu et al., 2019; Wei et al., 2020). For\ndeeper models, the training cost is nonnegligible.\nTo speed up the training, an alternative way is to\ntrain a shallow model ﬁrst and progressively in-\ncrease the model depth (Li et al., 2020; Dong et al.,\n2020). Apart from the model architecture improve-\nments, another way of easing the optimization is\nto utilize carefully designed parameter initializa-\ntion strategies (Zhang et al., 2019; Xu et al., 2020;\nHuang et al., 2020; Liu et al., 2020a). With the\nmodel capacity going larger, one can use Layer-\n8342\nModel Information Flow Related ODEs BLEU\nLeapfrog (He et al., 2019) yt+1 = yt−1 + 2F(yt,θt) Multistep Euler 28.07\nMultistep (Lu et al., 2018) yt+1 = kn · yt + (1− kn) · yt−1 + F(yt,θt) Multistep Euler 28.17\nDLCL (Wang et al., 2019) yt+1 = y0 + ∑t\nl=0 WlF(yl,θl) Multistep Euler 27.78\nPolyNet (Zhang et al., 2017) yt+1 = yt + F(yt,θt) +F(F(yt,θt),θt) Backward Euler 28.15\nRK2-block yt+1 = yt + 1\n2 F(yt,θt) + 1\n2 F(yt + F(yt,θt),θt) Improved Euler 28.67\nRK2-block (γi = 1) yt+1 = yt + F(yt,θt) +F(yt + F(yt,θt),θt) RK 2nd-order 28.77\nRK2-block (learnable γi) yt+1 = yt + γ1 · F(yt,θt) +γ2 · F(yt + F(yt,θt),θt) RK 2nd-order 28.86\nRK4-block yt+1 = yt + 1\n6 F1 + 2\n6 F2 + 2\n6 F3 + 1\n6 F4 RK 4th-order 29.03\nTable 7: Comparison of several ODE-inspired design schemas on the En-De task. We re-implement and apply\nthese methods into Transformer. Note that yn denotes the model input of layer n. Due to the limited space, we use\nFi to denote the intermediate representation, where i∈[1,4].\nDrop (Fan et al., 2020) or Skipping Sublayers (Li\net al., 2021) to prevent deep models from the over-\nﬁtting problem. Note that ODE Transformer is\northogonal to the aforementioned methods, and we\nwill test it on these methods in future work.\nOrdinary Differential Equations The relation-\nship between ResNet and ODEs was ﬁrst proposed\nby Weinan (2017). This shows a brand-new per-\nspective on the design of effective deep architec-\ntures. Moreover, the success of Neural ODENet\n(Chen et al., 2018) has attracted researchers. Some\ninsightful architectures (Zhang et al., 2017; Lars-\nson et al., 2017; Lu et al., 2018; He et al., 2019; Zhu\nand Fu, 2018; Lu et al., 2019; Sander et al., 2021)\ncan also be interpreted from the ODE perspective.\nBut, in NLP, it is still rare to see studies on design-\ning models from the ODE perspective. Zhang et al.\n(2021) proposed continuous self-attention models\nusing the same merit with neural ODE. Perhaps\nthe most relevant work with us is an (2021)’s work.\nThey redesigned the Transformer architecture from\na multi-particle dynamic system view in terms of\nefﬁciency. Unlike them, we show that the stacked\nﬁrst-order ODE blocks may cause error accumu-\nlation, thus hindering the model performance. We\naddress this issue by introducing high-order blocks,\nand demonstrate signiﬁcant performance improve-\nments on three sequence generation tasks, which\nis complementary to Baier-Reinio and De Sterck\n(2020)’s work.\n7 Conclusions\nThis paper explores the relationship between Trans-\nformer and ODEs. We propose ODE Transformer\nto help the model beneﬁt from high-order ODE\nsolutions. Experimental results on the three repre-\nsentative sentence generations tasks (i.e., machine\ntranslation, abstractive summarization, and gram-\nmatical error correction) show the effectiveness\nand efﬁciency of ODE Transformer. It achieves\n30.77 and 44.11 BLEU scores on the WMT’14\nEn-De and En-Fr benchmarks, setting a new state-\nof-the-art result on the En-Fr. Note that our\ncode is publicly available at https://github.\ncom/libeineu/ODE-Transformer.\nAcknowledgments\nThis work was supported in part by the National\nScience Foundation of China (Nos. 61732005\nand 61876035), the National Key R&D Project\nof China (No. 2019QY1801), the China HTRD\nCenter Project (No. 2020AAA0107904) and Yun-\nnan Provincial Major Science and Technology Spe-\ncial Plan Projects (Nos. 201902D08001905 and\n202103AA080015). The authors would like to\nthank anonymous reviewers for their valuable com-\nments. And thank Yufan Jiang for his helpful ad-\nvice to improve the paper.\nReferences\nSubhabrata Dutta an. 2021. Redesigning the trans-\nformer architecture with insights from multi-particl.\nArXiv preprint, abs/2109.15142.\nUri M Ascher and Linda R Petzold. 1998. Computer\nmethods for ordinary differential equations and\ndifferential-algebraic equations, volume 61. Siam.\nAaron Baier-Reinio and Hans De Sterck. 2020. N-ode\ntransformer: A depth-adaptive variant of the trans-\nformer using neural ordinary differential equations.\nArXiv preprint, abs/2010.11358.\nAnkur Bapna, Mia Chen, Orhan Firat, Yuan Cao,\nand Yonghui Wu. 2018. Training deeper neural\nmachine translation models with transparent atten-\ntion. In Proceedings of the 2018 Conference on\n8343\nEmpirical Methods in Natural Language Processing,\npages 3028–3033, Brussels, Belgium. Association\nfor Computational Linguistics.\nChristopher Bryant, Mariano Felice, Øistein E. An-\ndersen, and Ted Briscoe. 2019. The BEA-2019\nshared task on grammatical error correction. In Pro-\nceedings of the Fourteenth Workshop on Innovative\nUse of NLP for Building Educational Applications ,\npages 52–75, Florence, Italy. Association for Com-\nputational Linguistics.\nChristopher Bryant, Mariano Felice, and Ted Briscoe.\n2017. Automatic annotation and evaluation of error\ntypes for grammatical error correction. In Proceed-\nings of the 55th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 793–805, Vancouver, Canada.\nJohn C Butcher. 1963. Coefﬁcients for the study of\nrunge-kutta integration processes. Journal of the\nAustralian Mathematical Society, 3(2):185–201.\nJohn Charles Butcher. 1996. A history of runge-\nkutta methods. Applied numerical mathematics ,\n20(3):247–260.\nBo Chang, Lili Meng, Eldad Haber, Lars Ruthotto,\nDavid Begert, and Elliot Holtham. 2018. Reversible\narchitectures for arbitrarily deep residual neural\nnetworks. In Proceedings of the Thirty-Second\nAAAI Conference on Artiﬁcial Intelligence, (AAAI-\n18), the 30th innovative Applications of Artiﬁcial In-\ntelligence (IAAI-18), and the 8th AAAI Symposium\non Educational Advances in Artiﬁcial Intelligence\n(EAAI-18), New Orleans, Louisiana, USA, February\n2-7, 2018, pages 2811–2818. AAAI Press.\nTian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and\nDavid Duvenaud. 2018. Neural ordinary differen-\ntial equations. In Advances in Neural Information\nProcessing Systems 31: Annual Conference on Neu-\nral Information Processing Systems 2018, NeurIPS\n2018, December 3-8, 2018, Montréal, Canada ,\npages 6572–6583.\nShamil Chollampatt and Hwee Tou Ng. 2018. A multi-\nlayer convolutional encoder-decoder neural network\nfor grammatical error correction. In Proceedings\nof the Thirty-Second AAAI Conference on Artiﬁ-\ncial Intelligence, (AAAI-18), the 30th innovative Ap-\nplications of Artiﬁcial Intelligence (IAAI-18), and\nthe 8th AAAI Symposium on Educational Advances\nin Artiﬁcial Intelligence (EAAI-18), New Orleans,\nLouisiana, USA, February 2-7, 2018 , pages 5755–\n5762. AAAI Press.\nDaniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.\n2013. Building a large annotated corpus of learner\nEnglish: The NUS corpus of learner English. In\nProceedings of the Eighth Workshop on Innova-\ntive Use of NLP for Building Educational Applica-\ntions, pages 22–31, Atlanta, Georgia. Association\nfor Computational Linguistics.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988, Florence, Italy.\nAssociation for Computational Linguistics.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals,\nJakob Uszkoreit, and Lukasz Kaiser. 2019. Univer-\nsal transformers. In 7th International Conference on\nLearning Representations, ICLR 2019, New Orleans,\nLA, USA, May 6-9, 2019. OpenReview.net.\nChengyu Dong, Liyuan Liu, Zichao Li, and Jingbo\nShang. 2020. Towards adaptive residual network\ntraining: A neural-ode perspective. In Proceed-\nings of the 37th International Conference on Ma-\nchine Learning, ICML 2020, 13-18 July 2020, Vir-\ntual Event, volume 119 of Proceedings of Machine\nLearning Research, pages 2616–2626. PMLR.\nAngela Fan, Edouard Grave, and Armand Joulin. 2020.\nReducing transformer depth on demand with struc-\ntured dropout. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nSebastian Gehrmann, Yuntian Deng, and Alexander\nRush. 2018. Bottom-up abstractive summarization.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n4098–4109, Brussels, Belgium.\nKlaus Greff, Rupesh Kumar Srivastava, and Jürgen\nSchmidhuber. 2017. Highway and residual net-\nworks learn unrolled iterative estimation. In 5th\nInternational Conference on Learning Representa-\ntions, ICLR 2017, Toulon, France, April 24-26, 2017,\nConference Track Proceedings. OpenReview.net.\nEldad Haber and Lars Ruthotto. 2017. Stable architec-\ntures for deep neural networks. Inverse Problems,\n34(1):014004.\nEldad Haber, Lars Ruthotto, Elliot Holtham, and\nSeong-Hwan Jun. 2018. Learning across scales -\nmultiscale methods for convolution neural networks.\nIn Proceedings of the Thirty-Second AAAI Confer-\nence on Artiﬁcial Intelligence, (AAAI-18), the 30th\ninnovative Applications of Artiﬁcial Intelligence\n(IAAI-18), and the 8th AAAI Symposium on Educa-\ntional Advances in Artiﬁcial Intelligence (EAAI-18),\nNew Orleans, Louisiana, USA, February 2-7, 2018 ,\npages 3142–3148. AAAI Press.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In 2016 IEEE Conference on Computer Vi-\nsion and Pattern Recognition, CVPR 2016, Las Ve-\ngas, NV , USA, June 27-30, 2016 , pages 770–778.\nIEEE Computer Society.\nXiangyu He, Zitao Mo, Peisong Wang, Yang Liu,\nMingyuan Yang, and Jian Cheng. 2019. Ode-\ninspired network design for single image super-\nresolution. In IEEE Conference on Computer Vision\n8344\nand Pattern Recognition, CVPR 2019, Long Beach,\nCA, USA, June 16-20, 2019, pages 1732–1741. Com-\nputer Vision Foundation / IEEE.\nKarl Moritz Hermann, Tomás Kociský, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to\nread and comprehend. In Advances in Neural Infor-\nmation Processing Systems 28: Annual Conference\non Neural Information Processing Systems 2015,\nDecember 7-12, 2015, Montreal, Quebec, Canada ,\npages 1693–1701.\nXiao Shi Huang, Felipe Pérez, Jimmy Ba, and Mak-\nsims V olkovs. 2020. Improving transformer opti-\nmization through better initialization. In Proceed-\nings of the 37th International Conference on Ma-\nchine Learning, ICML 2020, 13-18 July 2020, Vir-\ntual Event, volume 119 of Proceedings of Machine\nLearning Research, pages 4475–4483. PMLR.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nWilhelm Kutta. 1901. Beitrag zur naherungsweisen in-\ntegration totaler differentialgleichungen. Z. Math.\nPhys., 46:435–453.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nGustav Larsson, Michael Maire, and Gregory\nShakhnarovich. 2017. Fractalnet: Ultra-deep\nneural networks without residuals. In 5th Inter-\nnational Conference on Learning Representations,\nICLR 2017, Toulon, France, April 24-26, 2017,\nConference Track Proceedings. OpenReview.net.\nBei Li, Yinqiao Li, Chen Xu, Ye Lin, Jiqiang Liu,\nHui Liu, Ziyang Wang, Yuhao Zhang, Nuo Xu,\nZeyang Wang, Kai Feng, Hexuan Chen, Tengbo Liu,\nYanyang Li, Qiang Wang, Tong Xiao, and Jingbo\nZhu. 2019. The NiuTrans machine translation sys-\ntems for WMT19. In Proceedings of the Fourth Con-\nference on Machine Translation (Volume 2: Shared\nTask Papers, Day 1), pages 257–266, Florence, Italy.\nAssociation for Computational Linguistics.\nBei Li, Ziyang Wang, Hui Liu, Quan Du, Tong Xiao,\nChunliang Zhang, and Jingbo Zhu. 2021. Learn-\ning light-weight translation models from deep trans-\nformer. In Thirty-Fifth AAAI Conference on Artiﬁ-\ncial Intelligence, AAAI 2021, Thirty-Third Confer-\nence on Innovative Applications of Artiﬁcial Intelli-\ngence, IAAI 2021, The Eleventh Symposium on Ed-\nucational Advances in Artiﬁcial Intelligence, EAAI\n2021, Virtual Event, February 2-9, 2021 , pages\n13217–13225. AAAI Press.\nBei Li, Ziyang Wang, Hui Liu, Yufan Jiang, Quan Du,\nTong Xiao, Huizhen Wang, and Jingbo Zhu. 2020.\nShallow-to-deep training for neural machine trans-\nlation. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 995–1005, Online. Association for\nComputational Linguistics.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out , pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nYe Lin, Yanyang Li, Tengbo Liu, Tong Xiao, Tongran\nLiu, and Jingbo Zhu. 2020. Towards fully 8-bit inte-\nger inference for the transformer model. In Proceed-\nings of the Twenty-Ninth International Joint Confer-\nence on Artiﬁcial Intelligence, IJCAI 2020 , pages\n3759–3765. ijcai.org.\nLiyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu\nChen, and Jiawei Han. 2020a. Understanding the\ndifﬁculty of training transformers. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 5747–\n5763, Online. Association for Computational Lin-\nguistics.\nXuebo Liu, Longyue Wang, Derek F Wong, Liang\nDing, Lidia S Chao, and Zhaopeng Tu. 2020b.\nUnderstanding and improving encoder layer fusion\nin sequence-to-sequence learning. ArXiv preprint,\nabs/2012.14768.\nYiping Lu, Zhuohan Li, Di He, Zhiqing Sun, Bin\nDong, Tao Qin, Liwei Wang, and Tie-Yan Liu. 2019.\nUnderstanding and improving transformer from a\nmulti-particle dynamic system point of view. ArXiv\npreprint, abs/1906.02762.\nYiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin\nDong. 2018. Beyond ﬁnite layer neural networks:\nBridging deep architectures and numerical differen-\ntial equations. In Proceedings of the 35th Inter-\nnational Conference on Machine Learning, ICML\n2018, Stockholmsmässan, Stockholm, Sweden, July\n10-15, 2018, volume 80 of Proceedings of Machine\nLearning Research, pages 3282–3291. PMLR.\nSachin Mehta, Marjan Ghazvininejad, Srinivasan Iyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2020.\nDelight: Very deep and light-weight transformer.\nArXiv preprint, abs/2008.00623.\nTomáš Mikolov, Anoop Deoras, Stefan Kombrink,\nLukáš Burget, and Jan ˇCernock`y. 2011. Empirical\nevaluation and combination of advanced language\nmodeling techniques. In Twelfth annual conference\nof the international speech communication associa-\ntion.\nRamesh Nallapati, Bowen Zhou, Cícero Nogueira dos\nSantos, Çaglar Gülçehre, and Bing Xiang. 2016.\nAbstractive text summarization using sequence-to-\nsequence rnns and beyond. In Proceedings of the\n8345\n20th SIGNLL Conference on Computational Natural\nLanguage Learning, CoNLL 2016, Berlin, Germany,\nAugust 11-12, 2016, pages 280–290. ACL.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(Demonstrations), pages 48–53, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nCarl Runge. 1895. Über die numerische auﬂösung von\ndifferentialgleichungen. Mathematische Annalen ,\n46(2):167–178.\nLars Ruthotto and Eldad Haber. 2019. Deep neural\nnetworks motivated by partial differential equations.\nJournal of Mathematical Imaging and Vision vol-\nume, 62:352–364.\nMichael E. Sander, Pierre Ablin, Mathieu Blondel, and\nGabriel Peyré. 2021. Momentum residual neural\nnetworks. In Proceedings of the 38th International\nConference on Machine Learning, ICML 2021, 18-\n24 July 2021, Virtual Event, volume 139 of Proceed-\nings of Machine Learning Research , pages 9276–\n9287. PMLR.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1073–\n1083, Vancouver, Canada.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 464–468,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nDavid R. So, Quoc V . Le, and Chen Liang. 2019. The\nevolved transformer. In Proceedings of the 36th In-\nternational Conference on Machine Learning, ICML\n2019, 9-15 June 2019, Long Beach, California, USA,\nvolume 97 of Proceedings of Machine Learning Re-\nsearch, pages 5877–5886. PMLR.\nRupesh Kumar Srivastava, Klaus Greff, and Jürgen\nSchmidhuber. 2015. Highway networks. ArXiv\npreprint, abs/1505.00387.\nToshikazu Tajiri, Mamoru Komachi, and Yuji Mat-\nsumoto. 2012. Tense and aspect error correction\nfor ESL learners using global context. In Proceed-\nings of the 50th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers), pages 198–202, Jeju Island, Korea. Associa-\ntion for Computational Linguistics.\nChau Tran, Shruti Bhosale, James Cross, Philipp\nKoehn, Sergey Edunov, and Angela Fan. 2021. Face-\nbook AI’s WMT21 news translation task submission.\nIn Proceedings of the Sixth Conference on Machine\nTranslation, pages 205–215, Online. Association for\nComputational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-\n9, 2017, Long Beach, CA, USA, pages 5998–6008.\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu,\nChangliang Li, Derek F. Wong, and Lidia S. Chao.\n2019. Learning deep transformer models for ma-\nchine translation. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1810–1822, Florence, Italy. Associa-\ntion for Computational Linguistics.\nXiangpeng Wei, Heng Yu, Yue Hu, Yue Zhang, Rongx-\niang Weng, and Weihua Luo. 2020. Multiscale col-\nlaborative deep models for neural machine transla-\ntion. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics ,\npages 414–426, Online. Association for Computa-\ntional Linguistics.\nE Weinan. 2017. A proposal on machine learning via\ndynamical systems. Communications in Mathemat-\nics and Statistics, 5(1):1–11.\nLijun Wu, Yiren Wang, Yingce Xia, Fei Tian, Fei\nGao, Tao Qin, Jianhuang Lai, and Tie-Yan Liu.\n2019. Depth growing for neural machine transla-\ntion. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics ,\npages 5558–5563, Florence, Italy. Association for\nComputational Linguistics.\nZhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and\nSong Han. 2020. Lite transformer with long-short\nrange attention. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nHongfei Xu, Qiuhui Liu, Josef van Genabith, Deyi\nXiong, and Jingyi Zhang. 2020. Lipschitz con-\nstrained parameter initialization for deep transform-\ners. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics ,\npages 397–402, Online. Association for Computa-\ntional Linguistics.\n8346\nHelen Yannakoudakis, Ted Briscoe, and Ben Medlock.\n2011. A new dataset and method for automatically\ngrading ESOL texts. In Proceedings of the 49th An-\nnual Meeting of the Association for Computational\nLinguistics: Human Language Technologies , pages\n180–189, Portland, Oregon, USA. Association for\nComputational Linguistics.\nBiao Zhang, Ivan Titov, and Rico Sennrich. 2019.\nImproving deep transformer with depth-scaled ini-\ntialization and merged attention. In Proceedings\nof the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP) , pages 898–909, Hong\nKong, China. Association for Computational Lin-\nguistics.\nJing Zhang, Peng Zhang, Baiwen Kong, Junqiu Wei,\nand Xin Jiang. 2021. Continuous self-attention mod-\nels with neural ODE networks. In Thirty-Fifth AAAI\nConference on Artiﬁcial Intelligence, AAAI 2021,\nThirty-Third Conference on Innovative Applications\nof Artiﬁcial Intelligence, IAAI 2021, The Eleventh\nSymposium on Educational Advances in Artiﬁcial In-\ntelligence, EAAI 2021, Virtual Event, February 2-9,\n2021, pages 14393–14401. AAAI Press.\nXingcheng Zhang, Zhizhong Li, Chen Change Loy,\nand Dahua Lin. 2017. Polynet: A pursuit of struc-\ntural diversity in very deep networks. In 2017 IEEE\nConference on Computer Vision and Pattern Recog-\nnition, CVPR 2017, Honolulu, HI, USA, July 21-26,\n2017, pages 3900–3908. IEEE Computer Society.\nYuhao Zhang, Ziyang Wang, Runzhe Cao, Binghao\nWei, Weiqiao Shan, Shuhan Zhou, Abudurexiti Re-\nheman, Tao Zhou, Xin Zeng, Laohu Wang, Yongyu\nMu, Jingnan Zhang, Xiaoqian Liu, Xuanjun Zhou,\nYinqiao Li, Bei Li, Tong Xiao, and Jingbo Zhu.\n2020. The NiuTrans machine translation systems for\nWMT20. In Proceedings of the Fifth Conference on\nMachine Translation, pages 338–345, Online. Asso-\nciation for Computational Linguistics.\nQingyu Zhou, Nan Yang, Furu Wei, Shaohan Huang,\nMing Zhou, and Tiejun Zhao. 2018. Neural docu-\nment summarization by jointly learning to score and\nselect sentences. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers) , pages 654–663,\nMelbourne, Australia.\nShuhan Zhou, Tao Zhou, Binghao Wei, Yingfeng Luo,\nYongyu Mu, Zefan Zhou, Chenglong Wang, Xuan-\njun Zhou, Chuanhao Lv, Yi Jing, Laohu Wang, Jing-\nnan Zhang, Canan Huang, Zhongxiang Yan, Chi Hu,\nBei Li, Tong Xiao, and Jingbo Zhu. 2021. The\nNiuTrans machine translation systems for WMT21.\nIn Proceedings of the Sixth Conference on Machine\nTranslation, pages 265–272, Online. Association for\nComputational Linguistics.\nJinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin,\nWengang Zhou, Houqiang Li, and Tie-Yan Liu.\n2020. Incorporating BERT into neural machine\ntranslation. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nMai Zhu and Chong Fu. 2018. Convolutional neu-\nral networks combined with runge-kutta methods.\nCoRR, abs/1802.08831.\nA Experimental Setups\nTable 8 summarizes the details of our datasets. We\nboth present the sentences and tokens of each task.\nFor the En-De and En-Fr tasks, the datasets used\nin this work could be found in Fairseq.6 For\nthe En-Ro task, we used the preprocessed dataset\nprovided by DeLight.7 Note that we only shared\nthe target embedding and the softmax embedding\ninstead of a shared vocabulary between the source\nside and the target side. The CNN/DailyMail\ndataset consists of CNN stories8 and Daily emails.9\nFor the grammar error correction task (GEC), we\nconducted experiments on the CONLL dataset.10\nB Training and Evaluation\nTraining As suggested in Li et al. (2020)’s work,\nwe adopted relative positional representation (RPR)\n(Shaw et al., 2018) for stronger baselines. Dense\nconnections among layers (Wang et al., 2019) are\nalso applied for stable learning since the model\nis optimized with FP16 training. All experiments\nwere trained on 8 GPUs with 4,096 tokens on each\nGPU. For the En-De and the En-Fr tasks, we em-\nployed the gradient accumulation strategy with a\nstep of 2 and 8, respectively. We used the Adam op-\ntimizer (Kingma and Ba, 2015) whose hyperparam-\neters were set to(0.9,0.997). The hyperparameters\nincluding the learning rate, the warmup step and\nthe total training steps of three tasks could be found\nin Table 8. Note that we trained Base/Deep and Big\nmodels for 50K and 100K steps on the En-De task.\nWe regarded merging SAN and FFN as the default\nODE block. In addition, main results were the av-\nerage of three times running with different random\n6https://github.com/pytorch/fairseq/\ntree/master/examples/scaling_nmt\n7https://github.com/sacmehta/delight/\nblob/master/readme_files/nmt/wmt16_en2ro.\nmd\n8https://drive.google.com/uc?export=\ndownload&id=0BwmD_VLjROrfTHk4NFg2SndKcjQ\n9https://drive.google.com/uc?export=\ndownload&id=0BwmD_VLjROrfM1BxdkxVaTY2bWs\n10https://www.cl.cam.ac.uk/research/nl/\nbea2019st\n8347\nDataset Vocab Dataset Training Inference\nTrain Dev Test Lr Warmup Batch Steps WD Beam LP\nWMT’14 En-De 34040 4.5M 3000 3003 0.002 16000 80K 50K × 4 0.6\nWMT’14 En-Fr 44424 35.7M 26822 3003 0.002 16000 320K 100K × 4 0.6\nWMT’16 En-Ro 34976 602K 1999 1999 0.002 8000 80K 17K × 5 1.3\nCNN/DailyMail 32584 287K 13368 11490 0.002 8000 160K 50K × 4 2.0\nCONLL 33136 827K 5448 1312 0.0015 4000 160K 15K ✓ 6 0.6\nTable 8: Statistics of the datasets and hyperparameters for three sequence generation tasks. For the dataset, we\nboth report the vocabulary size, sentence numbers of training, validation and test sets. For the training, Lr denotes\nthe peaking learning rate and Warmup denotes the warmup step of the Adam optimizer. WD denotes whether we\napplied word dropout. For the inference, Beam and LP denote the beam size and length penalty, respectively.\nseeds, and we averaged the last 5/10 checkpoints\nfor fair comparisons with previous work. The detail\nof Base/Deep/Wide conﬁgurations is as follows:\n•Base/Deep Model. The hidden size of self-\nattention was 512, and the dimension of the\ninner-layer in FFN was 2,048. We used 8\nheads for attention. For training, we set all\ndropout to 0.1 as default, including residual\ndropout, attention dropout, ReLU dropout. La-\nbel smoothing ϵls = 0 .1 was applied to en-\nhance the generation ability of the model. For\ndeep models, we only enlarged the encoder\ndepth considering the inference speed.\n•Wide (or Big) Model. We used the same archi-\ntecture as Transformer-Base but with a larger\nhidden layer size 1,024, more attention heads\n(16), and a larger feed forward inner-layer\n(4,096 dimensions). The residual dropout\nwas set to 0.3 for the En-De task and 0.1 for\nthe En-Fr task.\nFor the language modeling task, the hidden\nsize was 512, and the ﬁlter size of the FFN was\n2,048. We set all the dropout rates as0.1, including\nthe residual dropout, attention dropout and ReLU\ndropout. Each model was trained up to 20 epochs,\nand most models achieved the lowest PPL on the\nvalidation set when the epoch is 10. Then the vali-\ndation PPL began to increase, though the training\nPPL is still declining. The warmup step was 2,000\nand the batch size was 4,096. The max learning\nrate was set to 0.0007.\nEvaluation For machine translation, we mea-\nsured performance in terms of BLEU. Both tok-\nenized BLEU and SacreBLEU 11 scores were re-\n11BLEU+case.mixed+numrefs.1+smooth.exp+\ntok.13a+version.1.2.12\nported on the En-De and En-Fr tasks. Also, we\nreported tokenized BLEU scores on the En-Ro\ntask. In addition, we measured Rouge-1, Rouge-2,\nRouge-L for CNN/DailyMail and precision, recall,\nF0.5 for CONLL. The beam size and length penalty\nof each task are summarized in Table 8.\nC Additional Results and Analyses\nComparison on the CNN/DailyMail Dataset\nWe summarize the previous results on the\nCNN/DailyMail dataset (See Table 9). The perfor-\nmance was evaluated by ROUGE-1, ROUGE-2 and\nROUGE-L, respectively. Intuitively, high-order\nODE functions can signiﬁcantly improve on top of\nthe Euler method as well as several strong existing\nmodels.12 Again, RK4-block beats the baseline\nand RK2-block by up to 1.36 and 0.25 scores in\nterms of ROUGE-1, respectively.\nComparison of Various Scaling Methods We\nhave emphasized the importance of automatic co-\nefﬁcient learning in Section 3.2. The forward\npass of RK2-block can be described as yt+1 =\nyt + γ1 ·F1 + γ2 ·F2, where γ1 and γ2 are coefﬁ-\ncients which can be numerical suggested or learn-\nable. Here we exhibit the comparison of various\nscaling methods on the WMT’14 En-De dataset,\nand the results are listed in Table 10. We can see\nthat RK2-block (learnable γi) equips with a sin-\ngle sigmoid gate (line 5 in Table 10) yields best\nresults on both shallow and deep conﬁgurations.\nThe observation here reveals that appropriate scal-\ning functions can further improve the RK2-block.\nTanh activation even brings negative impacts on the\nperformance, especially when the model is deep. A\npossible explanation is that Tanh produces a larger\n12We only compared models without using pre-training.\n8348\nModel ROUGE-1 ROUGE-2 ROUGE-L\nLEAD3 40.24 17.70 36.45\nNEUSUM (Zhou et al., 2018) 41.59 19.01 37.98\nPGNet (See et al., 2017) 39.53 17.28 36.38\nSoft Fusion (Liu et al., 2020b) 41.00 18.30 37.90\nBottom-Up Summarization (Gehrmann et al., 2018) 41.22 18.68 38.34\nResidual-block 40.47 17.73 37.29\nRK2-block 41.58 18.57 38.41\nRK4-block 41.83 18.84 38.68\nTable 9: ROUGE scores of various models on the CNN/DailyMail dataset.\nModel γ1 γ2 6-layer 24-layer\nweight sharing 1 1 28.51 29.60\nRK2-block 1/2 1/2 28.67 29.85\nRK2-block (γi = 1) 1 1 28.77 30.01\nRK2-block (learnable γi = 1) scalar scalar 28.80 30.13\nRK2-block (learnable γi) sigmoid sigmoid 28.74 30.06\nRK2-block (learnable γi) sigmoid (1 - sigmoid) 28.86 30.29\nRK2-block (learnable γi) tanh tanh 28.45 29.47\nTable 10: Comparison of various scaling functions on the WMT14’ En-De dataset.\nrange ([−1,1]) which is more difﬁcult to optimize\nthan the sigmoid function.\nCase Study on the GEC Task Table 11 sum-\nmarizes several cases from the GEC task. Here,\nwe make a comparison between the baseline and\nthe RK4-block due to its superiority on the GEC\ntask. We can clearly see that the proposed RK4-\nblock delivers more accurate corrections compared\nwith the baseline when handling subject-verb agree-\nment (Case2), collocation (Case1, Case3), spelling\n(Case4) and other issues. More speciﬁcally, Figure\n7 illustrates the statistics of different error types\nannotated by ERRANT (Bryant et al., 2017), a\ngrammatical ERRor ANnotation Toolkit designed\nto automatically annotate parallel error correction\ndata. For more details please refer to Bryant et al.\n(2017)’s work. With the help of ERRANT, we\ncan carry out a detailed error type analysis. As\nshown in Figure 7, RK4-block corrects the input\nin a more similar way with the reference, though\nthere is still a large gap between them. Limited by\nthe model ability, thebaseline sometimes even can-\nnot generate the right corrections, e.g. R:PUNCT\nand M:OTHER cases.\nD Comparison with Related Work\nAs we aforementioned, the ODE design schema\nsomehow shares a similar merit with the weight\nsharing, especially when the coefﬁcients are set to\n1. This is because we reuse the same function F\nto compute the intermediate approximation at each\ntimestep, and it is also an effective way to apply\nthe higher-order ODE into the Transformer archi-\ntecture. Compared with weight sharing (line 1 in\nTable 10), ODE Transformer variants can deliver\nbetter performance within the same computation\ncost, demonstrating the effectiveness of ODE de-\nsign schema.\nNext, we make a detailed comparison between\nthe proposed ODE Transformer and previous stud-\nies (Baier-Reinio and De Sterck, 2020; Zhu and\nFu, 2018; Zhang et al., 2021) to avoid the potential\nmisunderstandings.\nCompared with RKNet RKNet (Zhu and Fu,\n2018) is mainly designed to improve the ResNet\nusing implicit Runge-Kutta methods for vision\ntasks. There are some differences between ours\nand RKNet. (i) We mainly conduct experiments\non sequence generation tasks, e.g. machine trans-\nlation, abstract summarization, and grammar error\ncorrection tasks. They focused on the image clas-\n8349\nCase1\nSource What ’s more ,various of cultures can be shown to us through social medias .\nReference What ’s more ,various cultures can be shown to us through social media .\nBaseline What ’s more ,various cultures can be shown to us through social medias .\nRK4 What ’s more ,various cultures can be shown to us through social media .\nCase2\nSource Social media sites such as Facebook has allow us to share our pictures or even chat online with\nour parents while we are overseas .\nReference Social media sites such as Facebook have allowed us to share our pictures or even chat online\nwith our parents while we are overseas .\nBaseline Social media sites such as Facebook allow us to share our pictures or even chat online with our\nparents while we are overseas .\nRK4 Social media sites such as Facebook have allowed us to share our pictures or even chat online\nwith our parents while we are overseas .\nCase3\nSource On one side , it is obvioualy that many advantages have been brought to our lives .\nReference On the one hand , it is obvious that many advantages have been brought to our lives .\nBaseline On one hand , it is obvious that many advantages have been brought to our lives .\nRK4 On the one hand , it is obvious that many advantages have been brought to our lives .\nCase4\nSource Other than that , I believe that the stong bond we have with our family is the biggest pillar of\nsupport to the carrier .\nReference Other than that , I believe that the strong bond we have with our family is the biggest pillar of\nsupport to the carrier .\nBaseline Other than that , I believe that the stong bond we have with our family is the biggest pillar of\nsupport to the carrier .\nRK4 Other than that , I believe that the strong bond we have with our family is the biggest pillar of\nsupport to the carrier .\nTable 11: Several examples from the GEC task. Here, source and reference denote the model input and the\ncorrection result, respectively. Green words are good corrections, while Red words are bad corrections.\nR:NOUN:NUM\nR:PREP\nM:PUNCT\nU:DET\nR:VERB\nR:VERB:TENSE\nM:DET\nR:NOUN\nR:VERB:SV A\nR:DET\nU:OTHER\nR:VERB:FORM\nR:MORPH\nR:SPELL\nR:PRON\nM:OTHER\nM:PREP\nR:ORTH\nR:PUNCT\nU:PREP0\n50\n100\n150\n200\n250\n300\nReference\nResidual-block\nRK4-block\nFigure 7: Statistics of different error type information.\n8350\nsiﬁcation task. (ii) Except for the integration of\nODE into the Transformer design schema, we also\nmake an analysis on how to choose appropriate co-\nefﬁcients of intermediate approximations. And we\nbridge the relationship between the ODE design\nschema with the explicit weight sharing. (iii) We\nalso offer an automatic coefﬁcient learning method\nfor RK2-block which delivers the best performance\nin different conﬁgurations.\nCompared with N-ODE As we discussed in the\nrelated work, our work is complementary to Baier-\nReinio and De Sterck (2020)’s work. We empiri-\ncally demonstrate the effectiveness of integrating\nODE design schema into Transformer on several se-\nquence generation tasks. This work may shed light\non the design of effective Transformer architec-\ntures from the numerical perspective and provides\nstronger baselines to the literature.\nCompared with CSAODE The differences be-\ntween these two works are summarized below: (i)\nAs we emphasized above, the benchmarks we ex-\nperimented on are quite different. They mainly\nvalidated the proposed CSAODE on text classiﬁ-\ncation and QA tasks. (ii) The proposed CSAODE\n(Zhang et al., 2021) is an extension of neural ODE\n(cheng et al., 2018), where the motivation is quite\ndifferent. They aim to effectively calculate the con-\ntiguous states of hidden features only via one-layer\nparameters and proposed a self-attention solver to\nﬁx the issue. While our motivation is to employ\nhigher-order ODE solutions to reduce the trunca-\ntion errors produced by each layer. On the other\nhand, CSAODE is still a single-layer model, and\nours is a multi-layer sequence-to-sequence model.\nWe also show the comparison of different compo-\nnents based on higher-order ODE solutions (See\nFigure 4). (iii) The single-layer model is not strong\nenough to solve complicated tasks, e.g. machine\ntranslation. However, when stacking several lay-\ners, we need to re-consider the error accumulation\namong layers, that each layer is an individual ODE\nsolver. How to mitigate the error accumulation is\nthe main goal in this work, which is not discussed\nin their work.\nE Derivations of the Equation\nLet Ebe the loss of training, L be the number\nblocks of the model, and yL be the model output.\nHere, we deﬁne\nzk = yk + F(yk,θk) (27)\nThen the information ﬂow of the RK2 method\ncan be described as follows:\nyk+1 = yk + 1\n2F(yk,θk) +\n1\n2F(yk + F(yk,θk),θk)\n= yk + 1\n2F(yk,θk) + 1\n2F(zk,θk)(28)\nwhere ∂zk\n∂yk\n= 1 + ∂F(yk,θk)\n∂yk\n. In this way, the detail\nderivation of Eq. (28) is as follows:\n∂yk+1\n∂yk\n= 1\n2 ·\n(\n1 + 1 + ∂F(yk,θk)\n∂yk\n+\n∂F(zk,θk)\n∂zk\n·\n(\n1 + ∂F(yk,θk)\n∂yk\n))\n= 1\n2 ·\n(\n1 +\n(\n1 + ∂F(zk,θk)\n∂zk\n)\n·\n(\n1 + ∂F(yk,θk)\n∂yk\n))\n(29)\nWith the chain rule, the error Epropagates from\nthe top layer yL to layer yt by the following for-\nmula:\n∂E\n∂yt\n= ∂E\n∂yL\n· ∂yL\n∂yL−1\n·∂yL−1\n∂yL−2\n··· ∂yt+1\n∂yt\n(30)\nHere we have\ngk =\n(\n1 + ∂F(yk,θk)\n∂yk\n)\n·\n(\n1 + ∂F(zk,θk)\n∂zk\n)\nThen, put the Eq. (30) into Eq. (29), the gradient\nof Eat yt is\n∂E\n∂yt\n= ∂E\n∂yL\n· 1\n2L−t ·\nL−1∏\nk=t\n(1 + gk) (31)\nSimilarly, we can easily obtain the gradient of\nRK2 method where γi = 1:\n∂E\n∂yt\n= ∂E\n∂yL\n·gL−1 ·gL−2 ···gt\n= ∂E\n∂yL\n·\nL−1∏\nk=t\ngk (32)\n8351"
}