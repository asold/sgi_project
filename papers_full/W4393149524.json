{
  "title": "BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions",
  "url": "https://openalex.org/W4393149524",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2307169946",
      "name": "Wenbo Hu",
      "affiliations": [
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A2096640705",
      "name": "Yi-Fan Xu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1910560345",
      "name": "Yi Li",
      "affiliations": [
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A2115371623",
      "name": "Weiyue Li",
      "affiliations": [
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A2115917937",
      "name": "Zeyuan Chen",
      "affiliations": [
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A2134014566",
      "name": "Zhuowen Tu",
      "affiliations": [
        "University of California, San Diego"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6745764446",
    "https://openalex.org/W6853163053",
    "https://openalex.org/W2560730294",
    "https://openalex.org/W2788643321",
    "https://openalex.org/W6796581206",
    "https://openalex.org/W3023989664",
    "https://openalex.org/W6856148249",
    "https://openalex.org/W4226182655",
    "https://openalex.org/W6845850023",
    "https://openalex.org/W6851592950",
    "https://openalex.org/W2947312908",
    "https://openalex.org/W3004268082",
    "https://openalex.org/W2109586012",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W6761041305",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W4281633595",
    "https://openalex.org/W2765716052",
    "https://openalex.org/W3107583115",
    "https://openalex.org/W6729503815",
    "https://openalex.org/W2185175083",
    "https://openalex.org/W4380353763",
    "https://openalex.org/W4367367040",
    "https://openalex.org/W4312846625",
    "https://openalex.org/W4385572845",
    "https://openalex.org/W3106859150",
    "https://openalex.org/W4361194507",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W4375958680",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4382142077",
    "https://openalex.org/W3197457832",
    "https://openalex.org/W4378711593",
    "https://openalex.org/W4376122449",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4286894277",
    "https://openalex.org/W4385572634",
    "https://openalex.org/W4385570412",
    "https://openalex.org/W4366850747",
    "https://openalex.org/W4376653374",
    "https://openalex.org/W4385570984",
    "https://openalex.org/W4376312115",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W2963622213",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3116651605",
    "https://openalex.org/W4381802186",
    "https://openalex.org/W4380137118",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W3209532394",
    "https://openalex.org/W4386071707",
    "https://openalex.org/W4385565405",
    "https://openalex.org/W4297816851",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W2979382951"
  ],
  "abstract": "Vision Language Models (VLMs), which extend Large Language Models (LLM) by incorporating visual understanding capability, have demonstrated significant advancements in addressing open-ended visual question-answering (VQA) tasks. However, these models cannot accurately interpret images infused with text, a common occurrence in real-world scenarios. Standard procedures for extracting information from images often involve learning a fixed set of query embeddings. These embeddings are designed to encapsulate image contexts and are later used as soft prompt inputs in LLMs. Yet, this process is limited to the token count, potentially curtailing the recognition of scenes with text-rich context. To improve upon them, the present study introduces BLIVA: an augmented version of InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings from InstructBLIP and also directly projects encoded patch embeddings into the LLM, a technique inspired by LLaVA. This approach assists the model to capture intricate details potentially missed during the query decoding process. Empirical evidence demonstrates that our model, BLIVA, significantly enhances performance in processing text-rich VQA benchmarks (up to 17.76% in OCR-VQA benchmark) and in undertaking general (not particularly text-rich) VQA benchmarks (up to 7.9% in Visual Spatial Reasoning benchmark), and achieved 17.72% overall improvement in a comprehensive multimodal LLM benchmark (MME), comparing to our baseline InstructBLIP. BLIVA demonstrates significant capability in decoding real-world images, irrespective of text presence. To demonstrate the broad industry applications enabled by BLIVA, we evaluate the model using a new dataset comprising YouTube thumbnails paired with question-answer sets across 11 diverse categories. For researchers interested in further exploration, our code and models are freely accessible at https://github.com/mlpc-ucsd/BLIVA.",
  "full_text": "BLIV A: A Simple Multimodal LLM for Better Handling of Text-Rich Visual\nQuestions\nWenbo Hu*1, Yifan Xu*2, Yi Li1 Weiyue Li1 Zeyuan Chen1 Zhuowen Tu1\n1University of California, San Diego\n2Coinbase Global, Inc.\n{w1hu, yil115, wel019, zec016, ztu}@ucsd.edu, yifan.xu@coinbase.com\nAbstract\nVision Language Models (VLMs), which extend Large Lan-\nguage Models (LLM) by incorporating visual understanding\ncapability, have demonstrated significant advancements in ad-\ndressing open-ended visual question-answering (VQA) tasks.\nHowever, these models cannot accurately interpret images\ninfused with text, a common occurrence in real-world sce-\nnarios. Standard procedures for extracting information from\nimages often involve learning a fixed set of query embed-\ndings. These embeddings are designed to encapsulate im-\nage contexts and are later used as soft prompt inputs in\nLLMs. Yet, this process is limited to the token count, po-\ntentially curtailing the recognition of scenes with text-rich\ncontext. To improve upon them, the present study introduces\nBLIV A: an augmented version of InstructBLIP withVisual\nAssistant. BLIV A incorporates the query embeddings from\nInstructBLIP and also directly projects encoded patch em-\nbeddings into the LLM, a technique inspired by LLaV A. This\napproach assists the model to capture intricate details po-\ntentially missed during the query decoding process. Empir-\nical evidence demonstrates that our model, BLIV A, signif-\nicantly enhances performance in processing text-rich VQA\nbenchmarks (up to 17.76% in OCR-VQA benchmark) and in\nundertaking general (not particularly text-rich) VQA bench-\nmarks (up to 7.9% in Visual Spatial Reasoning benchmark),\nand achieved 17.72% overall improvement in a comprehen-\nsive multimodal LLM benchmark (MME), comparing to our\nbaseline InstructBLIP. BLIV A demonstrates significant capa-\nbility in decoding real-world images, irrespective of text pres-\nence. To demonstrate the broad industry applications enabled\nby BLIV A, we evaluate the model using a new dataset com-\nprising YouTube thumbnails paired with question-answer sets\nacross 11 diverse categories. For researchers interested in fur-\nther exploration, our code and models are freely accessible at\nhttps://github.com/mlpc-ucsd/BLIV A.\nIntroduction\nRecently, Large Language Models (LLMs) have trans-\nformed the field of natural language understanding, exhibit-\ning impressive capabilities in generalizing across a broad ar-\nray of tasks, both in zero-shot and few-shot settings. This\nsuccess is mainly contributed by instruction tuning (Wu\n*These authors contributed equally.\nCopyright ¬© 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\net al. 2023) which improves generalization to unseen tasks\nby framing various tasks into instructions. Vision Lan-\nguage Models (VLMs) such as OpenAI‚Äôs GPT-4 (OpenAI\n2023), which incorporates LLM with visual understanding\ncapability, have demonstrated significant advancements in\naddressing open-ended visual question-answering (VQA)\ntasks. Several approaches have been proposed for employ-\ning LLMs on vision-related tasks by directly aligning with\na visual encoder‚Äôs patch feature (Liu et al. 2023a) or ex-\ntracting image information through a fixed number of query\nembeddings. (Li et al. 2023b; Zhu et al. 2023).\nHowever, despite exhibiting considerable abilities for\nimage-based human-agent interactions, these models strug-\ngle with interpreting text within images. Images with text are\npervasive in our daily lives, and comprehending such content\nis essential for human visual perception. Previous works uti-\nlized an abstraction module with queried embeddings, lim-\niting their capabilities in textual details within images (Li\net al. 2023b; Awadalla et al. 2023; Ye et al. 2023).\nIn our work, we employ learned query embeddings with\nadditional visual assistant branches, utilizing encoded patch\nembeddings. This approach addresses the constraint image\ninformation typically provided to language models, leading\nto improved text-image visual perception and understand-\ning. Empirically, we report the results of our model in gen-\neral (not particularly text-rich) VQA benchmarks following\nthe evaluation datasets of (Dai et al. 2023) and text-rich im-\nage evaluation protocol from (Liu et al. 2023b). Our model\nis initialized from a pre-trained InstructBLIP and an en-\ncoded patch projection layer trained from scratch. Following\n(Zhu et al. 2023; Liu et al. 2023a), we further demonstrate\na two-stage training paradigm. We begin by pre-training the\npatch embeddings projection layer. Subsequently, with the\ninstruction tuning data, we fine-tune both the Q-former and\nthe patch embeddings projection layer. During this phase,\nwe maintain both the image encoder and LLM in a frozen\nstate. We adopt this approach based on two findings from\nour experiments: firstly, unfreezing the vision encoder re-\nsults in catastrophic forgetting of prior knowledge; secondly,\ntraining the LLM concurrently didn‚Äôt bring improvement but\nbrought significant training complexity.\nIn summary, our study consists of the following high-\nlights:\n‚Ä¢ We present BLIV A, which leverages both learned query\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n2256\nQ-Former \nüî•\nLLM\n ‚ùÑ\na) Flamingo b) BLIP -2 / InstructBLIP c) LLaVA d) BLIVA (Ours)\n‚ùÑ\nVision\nEncoder\nPerceiver\n üî•\nLLM Block\n ‚ùÑ\nOutput Text\nüî•\nPretrained and frozen\nXATTN Layer\n üî•\nQ-Former \nüî•\nLLM\n ‚ùÑ\nImage              \nTrained from scratch or Finetuning\n‚ùÑ\nQuestion\nx N\nQuestion\nLLM\n ‚ùÑ ‚Üí \nüî•\nProjection \nüî• ‚Üí \n‚ùÑ\nProjection \nüî•\nQueries\nOutput Text Output Text\nOutput Text\nQueries\nQuestion\n‚ùÑ\nVision\nEncoder\nImage              \nQuestion\n‚ùÑ\nVision\nEncoder\nImage              \nQuestion\nQuestion\nQueries\n‚ùÑ\nVision\nEncoder\nImage              \nFigure 1: Comparison of various VLM approaches. Both (a) Flamingo (Alayrac et al. 2022) and (b) BLIP-2 / InstructBLIP (Li\net al. 2023b; Dai et al. 2023) architecture utilize a fixed, small set of query embeddings. These are used to compress visual\ninformation for transfer to the LLM. In contrast, (c) LLaV A aligns the encoded patch embeddings directly with the LLM. (d)\nBLIV A (Ours) builds upon these methods by merging learned query embeddings with additional encoded patch embeddings.\nembeddings and encoded patch embeddings, providing\nan effective method for interpreting text within images.\n‚Ä¢ Our experimental results showcase that BLIV Aprovides\nimprovements in the understanding of text within images\nwhile maintaining a robust performance in general (not\nparticularly text-rich) VQA benchmarks and achieving\nthe best performance on MME benchmark among pre-\nvious methods.\n‚Ä¢ To underscore the real-world applicability of BLIV A,\nwe evaluate the model using a new dataset of YouTube\nthumbnails with associated question-answer pairs.\nRelated Work\nMultimodal Large Language Model\nLarge Language Models (LLMs) have demonstrated im-\npressive zero-shot abilities across various open-ended tasks.\nRecent research has explored the application of LLMs for\nmultimodal generation to understand visual inputs. Some\napproaches leverage the pre-trained LLM to build unified\nmodels for multi-modality. For example, Flamingo (Alayrac\net al. 2022) connects the vision encoder and LLM by a Per-\nceiver Resampler which exhibits impressive few-shot per-\nformance. Additionally, BLIP-2 (Li et al. 2023b) designs a\nQ-former to align the visual feature with OPT (Zhang et al.\n2022) and FLAN-T5 (Wei et al. 2021). MiniGPT-4 (Zhu\net al. 2023) employed the same Q-former but changed the\nLLM to Vicuna (Zheng et al. 2023). Some approaches\nalso finetuned LLM for better alignment with visual fea-\ntures such as LLaV A (Liu et al. 2023a) directly finetuned\nLLM and mPLUG-Owl (Ye et al. 2023) performs low-rank\nadaption (LoRA) (Hu et al. 2022) to finetune a LLaMA\nmodel (Touvron et al. 2023). PandaGPT (Su et al. 2023)\nalso employed LoRA to finetune a Vicuna model on top\nof ImageBind (Girdhar et al. 2023), which can take multi-\nmodal inputs besides visual. While sharing the same two-\nstage training paradigm, we focus on developing an end-to-\nend multimodal model for both text-rich VQA benchmarks\nand general VQA benchmarks.\nMultimodal Instruction Tuning\nInstruction tuning has been shown to improve the general-\nization performance of language models to unseen tasks. In\nthe natural language processing (NLP) community, some ap-\nproaches collect instruction-tuning data by converting exist-\ning NLP datasets into instruction format (Wang et al. 2022b;\nWei et al. 2021; Sanh et al. 2022; Chung et al. 2022) oth-\ners use LLMs to generate instruction data (Taori et al. 2023;\nZheng et al. 2023; Wang et al. 2023; Honovich et al. 2022).\nRecent research expanded instruction tuning to multimodal\nsettings. In particular, for image-based instruction tuning,\nMiniGPT-4 (Zhu et al. 2023) employs human-curated in-\nstruction data during the finetuning stage. LLaV A (Liu et al.\n2023a) generates 156K multimodal instruction-following\ndata by prompting GPT-4 (OpenAI 2023) with image cap-\ntions and bounding boxes coordinates. mPLUG-Owl (Ye\net al. 2023) also employs 400K mixed text only and multi-\nmodal instruction data for finetuning. Instruction tuning also\nenhanced the previous vision language foundation model‚Äôs\nperformance. For example, MultimodalGPT (Gong et al.\n2023) designed various instruction templates that incorpo-\nrate vision and language data for multi-modality instruction\ntuning OpenFlamingo (Awadalla et al. 2023). (Xu, Shen, and\nHuang 2023) built a multimodal instruction tuning bench-\nmark dataset that consists of 62 diverse multimodal tasks\nin a unified seq-to-seq format and finetuned OFA (Wang\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n2257\nInput Image... ...Text Embeddings Queries \nVision Encoder\nEncoded Patch EmbeddingsProjection\nPre-trained LLM\nQ-Former\nProjection\nThe image depicts the famous Hollywoodsign located on a hillside, surrounded by mountains. The sign is prominently displayed in the center of the image,with its letters spelling out \"HOLLYWOOD.\" In addition to the Hollywood sign, there are several trees scattered throughout the scene,providing a natural backdrop for the iconic landmark.\n...\n...Text Embeddings Learned Query Embeddings Self-AttentionCross-AttentionFeed-Forward\nUser InstructionWhat is this image about?\n...\n... ...Text Embeddings Queries Encoded Patch Embeddings\nQ-Former\nLearned Query Embeddings ...\nFigure 2: Model architecture of BLIV A. BLIV A uses a Q-Former to draw out instruction-aware visual features from the patch\nembeddings generated by a frozen image encoder. These learned query embeddings are then fed as soft prompt inputs into\nthe frozen Language-Learning Model (LLM). Additionally, the system repurposes the originally encoded patch embeddings\nthrough a fully connected projection layer, serving as a supplementary source of visual information for the frozen LLM.\net al. 2022a). MIMIC-IT (Li et al. 2023a) built a big-\nger dataset comprising 2.8 million multimodal instruction-\nresponse pairs to train a stronger model Otter (Li et al.\n2023a). We also employed instruction tuning data following\nthe same prompt as InstructBLIP(Dai et al. 2023) to demon-\nstrate the effectiveness of utilizing additional encoded patch\nembeddings.\nMethod\nArchitecture Overview\nAs illustrated in Figure 1, there are mainly two types of\nend-to-end multimodal LLMs: 1) Models that utilize learned\nquery embeddings for LLM. For instance, MiniGPT-4 (Zhu\net al. 2023) used the frozen Q-former module from BLIP-\n2 (Li et al. 2023b) to extract image features by querying the\nCLIP vision encoder. Flamingo (Alayrac et al. 2022), em-\nployed a Perceiver Resampler, which reduced image features\nto a fixed number of visual outputs for LLM. 2) Models that\ndirectly employed image-encoded patch embeddings, such\nas LLaV A (Liu et al. 2023a), which connect its vision en-\ncoder to the LLM using an MLP. Nevertheless, these mod-\nels exhibit certain constraints. Some models employ learned\nquery embeddings for LLM, which help in better under-\nstanding the vision encoder but may miss crucial informa-\ntion from encoded patch embeddings. On the other hand,\nsome models directly use encoded image patch embeddings\nthrough a linear projection layer, which might have limited\ncapability in capturing all the information required for LLM.\nTo address this, we introduce BLIV A, a multimodal LLM\ndesigned to incorporate both learned query embeddings ‚Äî\nwhich are more closely aligned with the LLM ‚Äî and image-\nencoded patch embeddings that carry richer image informa-\ntion. In particular, Figure 2 illustrates that our model incor-\nporates a vision tower, which encodes visual representations\nfrom the input image into encoded patch embeddings. Sub-\nsequently, it is sent separately to the Q-former to extract re-\nfined learned query embeddings, and to the projection layer,\nallowing the LLM to grasp the rich visual knowledge. We\nconcatenate the two types of embeddings and feed them di-\nrectly to the LLM. These combined visual embeddings are\nappended immediately after the question text embedding to\nserve as the final input to the LLM. During inference, we\nemployed beam search to select the best-generated output.\nConversely, for classification and multi-choice VQA bench-\nmarks, we adopted the vocabulary ranking method as out-\nlined in InstructBLIP (Dai et al. 2023). Given our prior\nknowledge of a list of candidates, we calculated the log-\nlikelihood for each and chose the one with the highest value\nas the final prediction. To support another version for com-\nmercial usage of our architecture, we also selected FlanT5\nXXL as our LLM. This is named as BLIV A (FLanT5XXL) in\nthis paper.\nTwo Stages Training Scheme\nWe adopted the typical two-stage training scheme: 1) In\nthe pre-training stage, the goal is to align the LLM with\nvisual information using image-text pairs from image cap-\ntioning datasets that provide global descriptions of images.\n2) After pre-training, the LLM becomes familiar with the\nvisual embedding space and can generate descriptions of\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n2258\nimages. However, it still lacks the capability to discern the\nfiner details of images and respond to human questions. In\nthe second stage, we use instruction tuning data to enhance\nperformance and further align the visual embeddings with\nthe LLM and human values. Recent methods have predom-\ninantly adopted a two-stage training approach (Zhu et al.\n2023; Liu et al. 2023a; Ye et al. 2023) except PandaGPT (Su\net al. 2023), which utilizes a one-stage training method, has\nalso demonstrated commendable results.\nIn BLIV A, our visual assistant branch,specifically the en-\ncoded patch embeddings, diverges from the approach of\nBLIP-2 (Li et al. 2023b), which uses a 129M pre-training\ndataset. Instead, it leverages a more compact 0.5M pre-\ntraining caption data following (Liu et al. 2023a). This\npresents a more efficient strategy for aligning the visual en-\ncoder and LLM at the first stage. We employed language\nmodel loss as our training objective. The model learns to\ngenerate subsequent tokens based on the preceding context.\nThumbnails Dataset\nTo showcase the wide-ranging industry applications made\nfeasible by BLIV A, we assess the model by introduc-\ning a new evaluation dataset, named YTTB-VQA which\nconsists of 400 YouTube Thumbnail Visual Question-\nAnswer pairs to evaluate the visual perception abili-\nties of in-text images. It covers 11 different categories\nwhich is illustrated in the Appendix of arXiv ver-\nsion. During the data collection, we randomly selected\nYouTube videos with text-rich thumbnails from different\ncategories. We recorded the unique video ID for each\nYouTube video and obtained the high-resolution thumb-\nnail from the URL ‚Äùhttp://img.youtube.com/vi/<YouTube-\nVideo-ID>/maxresdefault.jpg‚Äù. After retrieving all the\nYouTube thumbnails, we created the annotation file with the\nfollowing fields: ‚Äùvideo\nid‚Äù representing the unique identifi-\ncation for a specific YouTube video, ‚Äùquestion‚Äù representing\nthe human-made question based on the text and image in the\nthumbnail, ‚Äùvideo\nclasses‚Äù representing the 11 video cate-\ngories, ‚Äùanswers‚Äù representing the ground truth answer, and\n‚Äùvideo\nlink‚Äù representing the URL link for each YouTube\nvideo.\nExperiment\nIn this section, we conduct extensive experiments and anal-\nyses to show the efficacy of our model. We evaluate our\nmodel, baseline, and other SOTA models on 10 OCR-related\ntasks and 8 general (not particularly text-rich) VQA bench-\nmarks, including image captioning, image question answer-\ning, visual reasoning, visual conversational QA, image clas-\nsification, and video question answering. We also evaluated\non a comprehensive multimodal LLM benchmark (MME).\nWe seek to answer the following:\n‚Ä¢ How does our proposed method compare to alternative\nsingle image embeddings approaches in text-rich VQA,\ngeneral VQA benchmarks and MME benchmark?\n‚Ä¢ How do the individual components of our method influ-\nence its success?\n‚Ä¢ How does BLIV A enhance the recognition of YouTube\nthumbnails?\nDatasets\nTo demonstrate the effectiveness of patch embeddings, we\nfollowed (Dai et al. 2023) to use the same training and\nevaluation data unless mentioned explicitly. Due to the ille-\ngal contents involved in LAION-115M dataset (Schuhmann\net al. 2021), we cannot download it securely through the uni-\nversity internet. Besides lacking a subset of samples of im-\nage captioning, we keep all other training data the same. It\nincludes MSCOCO (Lin et al. 2015) for image captioning,\nTextCaps (Sidorov et al. 2020), VQAv2 (Goyal et al. 2017),\nOKVQA (Marino et al. 2019), A-OKVQA (Schwenk et al.\n2022), OCR-VQA (Mishra et al. 2019) and LLaV A-Instruct-\n150K (Liu et al. 2023a). For evaluation datasets, we also fol-\nlow (Dai et al. 2023) but only keep Flickr30K (Young et al.\n2014), VSR (Liu, Emerson, and Collier 2023), IconQA (Lu\net al. 2022), TextVQA (Singh et al. 2019), Visual Dia-\nlog (Das et al. 2017), Hateful Memes (Kiela et al. 2020),\nVizWiz (Gurari et al. 2018), and MSRVTT QA (Xu et al.\n2017) datasets. The detailed dataset information can be\nfound at Appendix of arXiv version.\nImplementation Details\nWe selected the ViT-G/14 from EV A-CLIP (Sun et al. 2023)\nas our visual encoder. The pre-trained weights are initial-\nized and remain frozen during training. We removed the last\nlayer from ViT (Dosovitskiy et al. 2020) and opted to use\nthe output features of the second last layer, which yielded\nslightly better performance. We first pre-train our patch\nembeddings projection layer using LLaV A filtered 558K\nimage-text pairs from LAION (Schuhmann et al. 2021), CC-\n3M (Sharma et al. 2018), and SBU (Ordonez, Kulkarni, and\nBerg 2011), captioned by BLIP (Li et al. 2022). Using the\npre-training stage leads to slightly better performance. Dur-\ning the vision-language instruction tuning stage, we initial-\nize the Q-Former from InstructBLIP‚Äôs weight and finetune\nthe parameters of the Q-former and projection layer together\nwhile keeping both the image encoder and LLM frozen. We\npre-trained the projection layer with 3 epochs with a batch\nsize of 64. During the instruction finetuning stage, we em-\nploy a batch size of 24 with a maximum of 200K steps which\nroughly iterates two epochs of the training data. For both\nstage training, we used the AdamW (Loshchilov and Hutter\n2017) optimizer, with Œ≤1 = 0.9, Œ≤2 = 0.999, and a weight\ndecay of 0.05. Additionally, we apply a linear warmup of\nthe learning rate during the initial 1K steps, increasing from\n10‚àí8 to 10‚àí5, followed by a cosine decay with a minimum\nlearning rate of 0. The pre-training stage takes 6 hours and\nthe instruction finetuning stage finished within two days on\n8 Nvidia A6000 Ada (48G) GPUs.\nResults & Discussions\nWe introduce our results in the context of each of our three\nquestions and discuss our main findings.\n1. How does our proposed method compare to alterna-\ntive single image embeddings approaches in text-rich VQA,\ngeneral VQA benchmarks and MME benchmark?\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n2259\nST OCR\nText Doc Info Chart EST FUN SRO PO\nVQA ‚Üë VQA ‚Üë VQA ‚Üë VQA ‚Üë VQA ‚Üë QA ‚Üë VQA ‚Üë SD ‚Üë IE ‚Üë IE ‚Üë Av\nerage ‚Üë\nOpenFlamingo (A\nwadalla et al. 2023) 19.32 27.82\n29.08 5.05 14.99 9.12 28.20 0.85 0.12 2.12 13.67\nBLIP2-OPT6.7b (Li et\nal. 2023b) 13.36 10.58\n21.18 0.82 8.82 7.44 27.02 0.00 0.00 0.02 8.92\nBLIP2-FLanT5XXL (Li et\nal. 2023b) 21.38 30.28\n30.62 4.00 10.17 7.20 42.46 1.19 0.20 2.52 15.00\nMiniGPT4 (Zhu\net al. 2023) 14.02 11.52\n18.72 2.97 13.32 4.32 28.36 1.19 0.04 1.31 9.58\nLLaVA\n(Liu et al. 2023a) 22.93 15.02\n28.30 4.40 13.78 7.28 33.48 1.02 0.12 2.09 12.84\nmPLUG-Owl (Y\ne et al. 2023) 26.32 35.00\n37.44 6.17 16.46 9.52 49.68 1.02 0.64 3.26 18.56\nInstructBLIP (FlanT5XXL)\n(Dai et al. 2023) 26.22 55.04\n36.86 4.94 10.14 8.16 43.84 1.36 0.50 1.91 18.90\nInstructBLIP (V\nicuna-7B) (Dai et al. 2023) 28.64 47.62\n39.60 5.89 13.10 5.52 47.66 0.85 0.64 2.66 19.22\nBLIVA\n(FlanT5XXL) 28.24 61.34\n39.36 5.22 10.82 9.28 45.66 1.53 0.50 2.39 20.43\nBLIVA\n(Vicuna-7B) 29.08 65.38\n42.18 6.24 13.50 8.16 48.14 1.02 0.88 2.91 21.75\nTable\n1: Zero-Shot OCR-Free Results on Text-Rich VQA benchmarks. This table presents the accuracy (%) results for OCR-\nfree methods, implying no OCR-tokens were used. Note that our work follows InstructBLIP which incorporated OCR-VQA in\nits training dataset, thus inevitably making OCR-VQA evaluation not zero-shot.\nModels VSR ‚Üë IconQA ‚Üë Te\nxtVQA ‚Üë Visdial ‚Üë Flickr30K ‚Üë HM ‚Üë VizW\niz ‚Üë MSRVTT ‚Üë\n(val)\n(val-dev) (val-dev)\nFlamingo-3B (Alayrac\net al. 2022) - -\n30.1 - 60.6 - -\n-\nFlamingo-9B (Alayrac et al. 2022) - -\n31.8 - 61.5 - -\n-\nFlamingo-80B (Alayrac et al. 2022) - -\n35.0 - 67.2 - -\n-\nMiniGPT-4 (Zhu et al. 2023) 50.65 -\n18.56 - - 29.0 34.78\n-\nLLaV A (Liu et al. 2023a) 56.3 -\n37.98 - - 9.2 36.74\n-\nBLIP-2 (Vicuna-7B) (Dai et al. 2023) 50.0 39.7\n40.1 44.9 74.9 50.2 49.34 4.17\nInstructBLIP (V\nicuna-7B) (Dai et al. 2023) 54.3 43.1\n50.1 45.2 82.4 54.8 43.3\n18.7\nInstructBLIP Baseline\n(Vicuna-7B) 58.67 44.34\n37.58 40.58 84.61 50.6 44.10\n20.97\nBLIV A (Vicuna-7B) 62.2 44.88\n57.96 45.63 87.1 55.6 42.9 23.81\nTable\n2: Zero-shot results on general (not particularly text-rich)VQA benchmarks. Our baseline is obtained by directly fine-\ntuning InstructBLIP (Dai et al. 2023). For the three datasets on the right, due to the unavailability of test-set answers, we have\nevaluated them using validation dev. Here, Visdial and HM denote the Visual Dialog and Hateful Memes datasets, respectively.\nFollowing previous works (Alayrac et al. 2022; Yang et al. 2021; Murahari et al. 2020), we report the CIDEr score (Vedantam,\nZitnick, and Parikh 2015) for Flickr30K, AUC score for Hateful Memes, and Mean Reciprocal Rank (MRR) for Visual Dialog.\nFor all remaining datasets, we report the top-1 accuracy (%). Notably, for Text-VQA, we have followed InstructBLIP‚Äôs method\nof using OCR-tokens for comparison. While InstructBLIP also included GQA, iVQA, and MSVDQA, we were unable to ac-\ncess these datasets due to either unresponsive authors or the datasets being removed from their websites. For ScienceQA and\nNocaps, we were unable to reproduce the results of InstructBLIP, hence their results are not reported here.\nZero-shot evaluation for text-rich VQA benchmarks\nWe compared our data with state-of-the-art Multimodality\nLLMs. This includes LLaV A, which showcases robust OCR\ncapabilities using only patch embedding. We also consid-\nered BLIP2‚Äôs previous best version, BLIP-FLanT5xxL, the\nstate-of-the-art vision-language model mPlug-Owl (trained\non a vast amount of both text and vision-text data), and\nour baseline, InstructBLIP. The results are illustrated in Ta-\nble 1. Our model consistently shows significant improve-\nment across all the text-rich VQA datasets compared to\nInstructBLIP. Note that since InstructBLIP utilized OCR-\nVQA as its training dataset, the comparison for this spe-\ncific dataset isn‚Äôt zero-shot. We evaluated both InstructBLIP\nand our model using the OCR-VQA validation set. BLIV A\nachieved state-of-the-art results among 6 text-rich datasets\nwhile mPlug-Owl performed the best in 4 datasets. Com-\npared to mPlug-Owl, which employs about 1104M image\ncaptioning data in the Pre-training stage, BLIV A only em-\nploys 558K image caption pairs which could explain why\nBLIV A is not performing the best in information-based\nVQA tasks such as InfoVQA, ChartQA and ESTVQA.\nBLIV A demonstrates the best performance on average com-\npared to all previous methods, underscoring our design\nchoice to employ learned query embeddings, further aided\nby encoded patch embeddings.\nZero-shot evaluation for general(not particularly text-\nrich) VQA benchmarks Next, we compared BLIV A with\nmodels that employ single image features. Results are given\nin Table 2 and in Table 3 for LLMs available for commer-\ncial use. Our model consistently and significantly outper-\nformed the original InstructBLIP model in VSR, IconQA,\nTextVQA, Visual Dialog, Hateful Memes, MSRVTT, and\nFlickr30K. For VizWiz, our model nearly matched Instruct-\nBLIP‚Äôs performance. This naturally raises the question: why\ndidn‚Äôt additional visual assistance improve all the bench-\nmarks? We speculate that the additional visual information\ndidn‚Äôt aid VizWiz task. We continue to investigate this phe-\nnomenon in the next ablation study section. Overall, our de-\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n2260\nModels VSR ‚Üë IconQA ‚Üë Te\nxtVQA ‚Üë Visdial ‚Üë Flickr30K ‚Üë HM ‚Üë VizW\niz ‚Üë MSRVTT ‚Üë\n(val)\n(val-dev) (val-dev)\nBLIP-2 (FlanT5XXL)\n(Li et al. 2023b) 68.2 45.4\n44.1 46.9 73.7 52.0 29.4\n17.4\nInstructBLIP (FlanT5XXL) (Dai et al. 2023) 65.6 51.2\n46.6 48.5 83.5 53.6 41.35 20.79\nBLIV A\n(FlanT5XXL) 68.82 52.42\n57.2 36.18 87.66 50.0 43.97 23.78\nTable\n3: Zero-shot results on general (not particularly text-rich)VQA benchmarks for models with open LLM eligible for\ncommercial use. Here, the commercial use applicable LLM we reported is FlanT5 XXL. Same as Table 2, we report the same\nevaluation datasets with the same evaluation metrics.\nModel Overall ‚Üë Perception ‚Üë Cognition ‚Üë\nExist. Count\nPos. Color OCR Poster Cele. Scene Land. Art. Comm. Num.\nTrans. Code\nLLaVA(Liu\net al. 2023a) 712.5 50.0 50.0\n50.0 50.0 50.0 50.0 48.8 50.0 50.0 49.0 57.1 50.0\n57.5 50.0\nMiniGPT-4(Zhu et al. 2023) 694.3 68.3 55.0\n43.3 43.3 57.5 41.8 54.4 71.8 54.0 60.5 59.3 45.0\n0.0 40.0\nmPLUG-Owl(Ye et al. 2023) 1238.4 120.0 50.0\n50.0 50.0 65.0 136.1 100.3 135.5 159.3 96.3 78.6 60.0 80.0 57.5\nInstructBLIP(Dai et\nal. 2023) 1417.9 185.0 143.3 66.7 66.7\n72.5 123.8 101.2 153.0 79.8 134.3 129.3 40.0 65.0\n57.5\nBLIP-2(Li et al. 2023b) 1508.8 160.0 135.0\n73.3 73.3 110.0 141.8 105.6 145.3 138.0 136.5 110.0 40.0\n65.0 75.0\nBLIVA 1669.2 180.0 138.3 81.7 180.0 87.5 155.1 140.9 151.5 89.5 133.3 136.4 57.5 77.5 60.0\nTable 4: Evaluation of MME-Benchmark. Here we report the results on all the sub tasks, including Existence(Exist.),\nCount, Position(Pos.), Color, OCR, Poster, Celebrity(Cele.), Scene, Landmark(Land.), Artwork(Art.), Commonsense Reason-\ning(Comm.), Numerical Calculation(Num.), Text Translation(Trans.), and Code Reasoning(Code). We bold the highest overall\nscore and highlight the Top-2 model of each sub task with underline.\nInstruct- Baseline\nPatch Pre- Fine- ST-\nOCR- Text‚Äì Doc- Info- Chart- EST- FUNSD SROIE POIE Improv\nement\nBLIP (Instruction Embed- Training tuning VQA VQA\nVQA VQA VQA QA VQA\nTuning\ndings LLM\nQformer)\n‚úì 28.64 47.62\n39.60 5.89 13.10 5.52 47.66 0.85 0.64 2.66 + 0\n%\n‚úì ‚úì 30.08 65.8 40.5\n6.13 12.03 8.08 47.02 0.85 0.57 2.62 + 7.40%\n‚úì\n‚úì ‚úì 28.86 65.04\n40.7 6.65 14.28 8.24 47.72 1.19 1.66 2.83 + 31.72%\n‚úì ‚úì\n‚úì ‚úì 29.08 65.38 42.18 6.24\n13.50 8.16 48.14 1.02 0.88 2.91 + 17.01%\n‚úì\n‚úì ‚úì ‚úì ‚úì 29.94 66.48 41.9 6.47\n12.51 7.52 46.76 1.02 0.51 2.85 + 9.65%\nTable\n5: Results of adding individual techniques of our framework in text-rich VQA benchmarks. We include four ablations\nthat accumulate each technique (i) baseline: instruction tuning InstructBLIP‚Äôs Qformer. (ii) instruction tuning patch embeddings\n(iii) pre-training stage of patch embeddings (iv) Finetuning LLM with LORA during the instruction tuning stage.\nInstruct- Baseline\nPatch Pre- Fine- VSR IconQA\nTextVQA Visdial Flickr HM V\nizWiz MSRVTT Improv\nement\nBLIP (Instruction Embed- Training tuning 30K (val)\n(val-dev) (val-dev)\nTuning\ndings LLM\nQformer)\n‚úì 54.3 43.1\n50.1 45.2 82.4 54.8 43.3\n18.7 + 0%\n‚úì\n‚úì 58.67 44.34\n37.58 40.58 84.61 50.6 44.1\n20.97 - 1.91%\n‚úì\n‚úì ‚úì 58.85 44.91 58.8 41.67 87.4 49.1 42.83\n23.70 + 5.43%\n‚úì\n‚úì ‚úì ‚úì 62.2 44.88 57.96 45.63 87.1 55.6 42.9 23.81 + 8.61%\n‚úì\n‚úì ‚úì ‚úì ‚úì 51.39 41.34\n57.82 42.32 82.7 46.2 44.91 22.67 + 1.15%\nTable\n6: Results of adding individual techniques of our framework in general(not particularly text-rich)VQA benchmarks. We\ninclude four ablations that accumulate each technique same as in Table 5.\nsign not only achieved significant improvements in under-\nstanding text-rich images but also improves 7 out of 8 gen-\neral VQA benchmarks.\nMME Benchmark We further evaluated BLIV A on a\ncomprehensive Multlimodal LLM benchmark (MME) (Fu\net al. 2023). As illustrated in Table 4, BLIV A demonstrates\nthe best performance among the current methods for both the\nperception and cognition tasks overall. For all text-rich tasks\nsuch as OCR, Poster, Numerical Calculation, Text Trans-\nlation, and code, BLIV A outperforms InstructBLIP. BLIV A\nachieved top 2 performance across all the tasks except art-\nwork and landmark which demand extensive informational\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n2261\nknowledge. This is consistent with our findings from infor-\nmational VQA, indicating that our light-weight pre-training\nstage and the missing LAION-115M web image caption\ndataset during instruction tuning stage both likely contribute\nto a degradation in BLIV A‚Äôs internet knowledge base.\n2. How do the individual components of our method\ninfluence its success?\nTo investigate the impact of image-encoded patch embed-\ndings, the pre-training stage, and fine-tuning the LLM, we\nconducted ablation studies, incorporating each element re-\nspectively. For simplicity, here we only conduct ablation on\nthe BLIV A (Vicuna-7B) model. Since our baseline is the In-\nstructBLIP, we report the results of using baseline alone as\ndirectly finetuning the InstructBLIP model with our data and\nimplementation.\nAblation in text-rich VQA benchmarksFor text-rich\nimage related tasks, Table 5 illustrates the results of adding\neach technique separately. Compared to the baseline, adding\npatch embeddings improved performance across all tasks\nwith the exception of ST-VQA and OCR-VQA. This can\nstem from data contamination, as STVQA includes data\nalready present in InstructBLIP‚Äôs Qformer training set but\nnot included in patch embedding‚Äôs training set. Without the\npre-training stage, the performance of ST-VQA, OCR-VQA,\nTextVQA, ESTVQA, and POIE decreased, while the rest are\nbenefited. Since the pre-training stage employs image cap-\ntion pairs, we observed that it didn‚Äôt benefit BLIV A‚Äôs perfor-\nmance in text-rich VQA tasks as consistently as in the gen-\neral VQA tasks. Considering the improvement of all tasks,\npre-training is still adopted. BLIV A on average outperforms\nInstructBLIP by 31.72% without pre-training and 17.01%\nwith it, both outpacing the 7.40% improvement from instruc-\ntion tuning Qformer. These studies indicate that our design\nof employing patch embeddings provides more detailed vi-\nsual information. It also supports our hypothesis that an ad-\nditional visual assistant improves visual knowledge in areas\nwhere the query embeddings either neglect or have limited\nextraction capabilities.\nAblation in general (not particularly text-rich) VQA\nbenchmarks As illustrated in Table 6, the presence of\nencoded patch embeddings improves performance in all\nbenchmarks significantly except HM and VizWiz. For tasks\nwhere we observed a drop in performance, such as HM,\nwhich focuses on interpreting the feeling of hatefulness, and\nVizWiz, which predicts whether a visual question can be an-\nswered. We conjecture these tasks can be fulfilled by utiliz-\ning global-level query embeddings information such as feel-\ning the hatefulness in the image or if the image‚Äôs object is\nunrelated to the question asking. When adding the first pre-\ntraining stage, the performance for VSR, VisDial, HM, and\nMSRVTT tasks improves substantially while others are kept\nroughly the same. These ablation results confirmed the ne-\ncessity of two-stage training. During the instruction tuning\nstage, we also experimented with fine-tuning the LLM us-\ning LoRA in conjunction with Q-former and encoded patch\nembeddings. However, this approach didn‚Äôt yield as much\nimprovement as our best model and even reduced perfor-\nmance in many tasks. Nonetheless, we have included these\nresults in the ablation study for completeness. We conjec-\nModels Accuracy\n(%)\nMiniGPT4 (Zhu\net al. 2023) 47.75\nLLaVA\n(Liu et al. 2023a) 41.75\nInstructBLIP (Dai\net al. 2023) 82.2\nBLIVA\n(Vicuna-7B) 83.5\nTable\n7: Evaluation results of our collected Youtube thumb-\nnails dataset. We report the top-1 accuracy (%).\nture that frozen LLM has a satisfactory understanding of vi-\nsual information after our two-stage alignment. The visual\nembeddings are interpreted as a ‚Äùforeign language‚Äù to LLM\nand thus finetuning LLM together is not needed in this case.\n3. How does BLIVA enhance the recognition of\nYouTube thumbnails?\nYoutube Thumbnails EvaluationTable 7 illustrates the\nresults of the youtube thumbnail dataset with BLIV A achiev-\ning the best performance. From an application perspective,\nBLIV A has the ability to extract extra visual information\nfrom images besides extracting information from YouTube\ncaptions alone like LLMs. Our success in this use case can\nbe further expanded to large-scale thumbnail images.\nQualitative Analysis\nWe use real-life scene images, movie posters, webpages, and\nmemes to demonstrate our model‚Äôs performance regarding\ninteraction with humans based on text-rich images. The ex-\namples are in Appendix of arXiv version. BLIV A showcases\nexceptional OCR capabilities, paired with a robust localiza-\ntion ability that accurately identifies texts and objects within\nimages.\nConclusion\nIn this paper, we illustrate the effectiveness of assisting\nlearned query embeddings with encoded image patch em-\nbeddings as a visual assistant. This straightforward yet in-\nnovative design bolsters performance in both general VQA\nbenchmarks and text-rich VQA benchmarks. Our model,\nBLIV A, demonstrates superior performance in both aca-\ndemic benchmarks and qualitative real-world examples.\nMoreover, human evaluation of the model‚Äôs performance re-\nveals that BLIV A struggles with deciphering numerical sym-\nbols in images. This could be attributed to the reduced pixel\nrepresentation often used for these symbols and needs future\nwork to develop valuable insights. Our work also demon-\nstrates the effectiveness of mixing different types of visual\nembeddings. We encourage more future work to explore\nhow to scale more visual embeddings to LLM which can be\nthe key to the next stage of Large Vision-Language Models.\nAcknowledgements\nZhuowen Tu is funded by NSF Award IIS-2127544.\nReferences\nAlayrac, J.-B.; Donahue, J.; Luc, P.; Miech, A.; Barr, I.; Has-\nson, Y .; Lenc, K.; Mensch, A.; Millican, K.; Reynolds, M.;\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n2262\net al. 2022. Flamingo: a visual language model for few-shot\nlearning. Advances in Neural Information Processing Sys-\ntems (NeurIPS), 35: 23716‚Äì23736.\nAwadalla, A.; Gao, I.; Gardner, J.; Hessel, J.; Hanafy, Y .;\nZhu, W.; Marathe, K.; Bitton, Y .; Gadre, S.; Jitsev, J.; et al.\n2023. Openflamingo.\nChung, H. W.; Hou, L.; Longpre, S.; Zoph, B.; Tay, Y .; Fe-\ndus, W.; Li, Y .; Wang, X.; Dehghani, M.; Brahma, S.; Web-\nson, A.; Gu, S. S.; Dai, Z.; Suzgun, M.; Chen, X.; Chowd-\nhery, A.; Castro-Ros, A.; Pellat, M.; Robinson, K.; Val-\nter, D.; Narang, S.; Mishra, G.; Yu, A.; Zhao, V .; Huang,\nY .; Dai, A.; Yu, H.; Petrov, S.; Chi, E. H.; Dean, J.; De-\nvlin, J.; Roberts, A.; Zhou, D.; Le, Q. V .; and Wei, J.\n2022. Scaling Instruction-Finetuned Language Models.\narXiv:2210.11416.\nDai, W.; Li, J.; Li, D.; Tiong, A. M. H.; Zhao, J.; Wang, W.;\nLi, B.; Fung, P.; and Hoi, S. 2023. InstructBLIP: Towards\nGeneral-purpose Vision-Language Models with Instruction\nTuning. arXiv:2305.06500.\nDas, A.; Kottur, S.; Gupta, K.; Singh, A.; Yadav, D.; Moura,\nJ. M.; Parikh, D.; and Batra, D. 2017. Visual Dialog. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR).\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nFu, C.; Chen, P.; Shen, Y .; Qin, Y .; Zhang, M.; Lin, X.; Qiu,\nZ.; Lin, W.; Yang, J.; Zheng, X.; Li, K.; Sun, X.; and Ji,\nR. 2023. MME: A Comprehensive Evaluation Benchmark\nfor Multimodal Large Language Models. arXiv preprint\narXiv:2306.13394.\nGirdhar, R.; El-Nouby, A.; Liu, Z.; Singh, M.; Alwala, K. V .;\nJoulin, A.; and Misra, I. 2023. ImageBind: One Embedding\nSpace To Bind Them All. In Computer Vision and Pattern\nRecognition Conference (CVPR).\nGong, T.; Lyu, C.; Zhang, S.; Wang, Y .; Zheng, M.; Zhao,\nQ.; Liu, K.; Zhang, W.; Luo, P.; and Chen, K. 2023.\nMultiModal-GPT: A Vision and Language Model for Dia-\nlogue with Humans. arXiv:2305.04790.\nGoyal, Y .; Khot, T.; Summers-Stay, D.; Batra, D.; and\nParikh, D. 2017. Making the V in VQA Matter: Elevating\nthe Role of Image Understanding in Visual Question An-\nswering. In Conference on Computer Vision and Pattern\nRecognition (CVPR).\nGurari, D.; Li, Q.; Stangl, A. J.; Guo, A.; Lin, C.; Grauman,\nK.; Luo, J.; and Bigham, J. P. 2018. Vizwiz grand challenge:\nAnswering visual questions from blind people. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition (CVPR), 3608‚Äì3617.\nHonovich, O.; Scialom, T.; Levy, O.; and Schick, T. 2022.\nUnnatural Instructions: Tuning Language Models with (Al-\nmost) No Human Labor. arXiv:2212.09689.\nHu, E. J.; Shen, Y .; Wallis, P.; Allen-Zhu, Z.; Li, Y .; Wang,\nS.; Wang, L.; and Chen, W. 2022. LoRA: Low-Rank Adap-\ntation of Large Language Models. In International Confer-\nence on Learning Representations (ICLR).\nKiela, D.; Firooz, H.; Mohan, A.; Goswami, V .; Singh, A.;\nRingshia, P.; and Testuggine, D. 2020. The hateful memes\nchallenge: Detecting hate speech in multimodal memes. Ad-\nvances in neural information processing systems (NeurIPS),\n33: 2611‚Äì2624.\nLi, B.; Zhang, Y .; Chen, L.; Wang, J.; Pu, F.; Yang, J.; Li,\nC.; and Liu, Z. 2023a. Mimic-it: Multi-modal in-context\ninstruction tuning. arXiv preprint arXiv:2306.05425.\nLi, D.; Li, J.; Le, H.; Wang, G.; Savarese, S.; and Hoi, S. C.\n2023b. LA VIS: A One-stop Library for Language-Vision In-\ntelligence. In Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 3: Sys-\ntem Demonstrations), 31‚Äì41. Toronto, Canada: Association\nfor Computational Linguistics.\nLi, J.; Li, D.; Xiong, C.; and Hoi, S. 2022. BLIP: Boot-\nstrapping Language-Image Pre-training for Unified Vision-\nLanguage Understanding and Generation. In ICML.\nLin, T.-Y .; Maire, M.; Belongie, S.; Bourdev, L.; Girshick,\nR.; Hays, J.; Perona, P.; Ramanan, D.; Zitnick, C. L.; and\nDoll¬¥ar, P. 2015. Microsoft COCO: Common Objects in Con-\ntext. arXiv:1405.0312.\nLiu, F.; Emerson, G.; and Collier, N. 2023. Visual spatial\nreasoning. Transactions of the Association for Computa-\ntional Linguistics (TACL), 11: 635‚Äì651.\nLiu, H.; Li, C.; Wu, Q.; and Lee, Y . J. 2023a. Visual Instruc-\ntion Tuning.\nLiu, Y .; Li, Z.; Li, H.; Yu, W.; Liu, Y .; Yang, B.; Huang, M.;\nPeng, D.; Liu, M.; Chen, M.; Li, C.; Yin, X.; lin Liu, C.; Jin,\nL.; and Bai, X. 2023b. On the Hidden Mystery of OCR in\nLarge Multimodal Models. arXiv:2305.07895.\nLoshchilov, I.; and Hutter, F. 2017. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101.\nLu, P.; Qiu, L.; Chen, J.; Xia, T.; Zhao, Y .; Zhang, W.; Yu, Z.;\nLiang, X.; and Zhu, S.-C. 2022. IconQA: A New Benchmark\nfor Abstract Diagram Understanding and Visual Language\nReasoning. arXiv:2110.13214.\nMarino, K.; Rastegari, M.; Farhadi, A.; and Mottaghi, R.\n2019. Ok-vqa: A visual question answering benchmark re-\nquiring external knowledge. In Proceedings of the IEEE/cvf\nconference on computer vision and pattern recognition\n(CVPR), 3195‚Äì3204.\nMishra, A.; Shekhar, S.; Singh, A. K.; and Chakraborty, A.\n2019. OCR-VQA: Visual Question Answering by Reading\nText in Images. In ICDAR.\nMurahari, V .; Batra, D.; Parikh, D.; and Das, A. 2020. Large-\nScale Pretraining for Visual Dialog: A Simple State-of-the-\nArt Baseline. In Vedaldi, A.; Bischof, H.; Brox, T.; and\nFrahm, J.-M., eds., ECCV.\nOpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.\nOrdonez, V .; Kulkarni, G.; and Berg, T. 2011. Im2Text:\nDescribing Images Using 1 Million Captioned Photographs.\nIn Shawe-Taylor, J.; Zemel, R.; Bartlett, P.; Pereira, F.; and\nWeinberger, K., eds., Advances in Neural Information Pro-\ncessing Systems, volume 24. Curran Associates, Inc.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n2263\nSanh, V .; Webson, A.; Raffel, C.; Bach, S. H.; Sutawika, L.;\nAlyafeai, Z.; Chaffin, A.; Stiegler, A.; Scao, T. L.; Raja, A.;\nDey, M.; Bari, M. S.; Xu, C.; Thakker, U.; Sharma, S. S.;\nSzczechla, E.; Kim, T.; Chhablani, G.; Nayak, N.; Datta, D.;\nChang, J.; Jiang, M. T.-J.; Wang, H.; Manica, M.; Shen, S.;\nYong, Z. X.; Pandey, H.; Bawden, R.; Wang, T.; Neeraj, T.;\nRozen, J.; Sharma, A.; Santilli, A.; Fevry, T.; Fries, J. A.;\nTeehan, R.; Bers, T.; Biderman, S.; Gao, L.; Wolf, T.; and\nRush, A. M. 2022. Multitask Prompted Training Enables\nZero-Shot Task Generalization. arXiv:2110.08207.\nSchuhmann, C.; Vencu, R.; Beaumont, R.; Kaczmarczyk,\nR.; Mullis, C.; Katta, A.; Coombes, T.; Jitsev, J.; and Ko-\nmatsuzaki, A. 2021. LAION-400M: Open Dataset of CLIP-\nFiltered 400 Million Image-Text Pairs. arXiv:2111.02114.\nSchwenk, D.; Khandelwal, A.; Clark, C.; Marino, K.; and\nMottaghi, R. 2022. A-OKVQA: A Benchmark for Visual\nQuestion Answering using World Knowledge. arXiv.\nSharma, P.; Ding, N.; Goodman, S.; and Soricut, R. 2018.\nConceptual Captions: A Cleaned, Hypernymed, Image Alt-\ntext Dataset For Automatic Image Captioning. In Proceed-\nings of ACL.\nSidorov, O.; Hu, R.; Rohrbach, M.; and Singh, A. 2020.\nTextCaps: a Dataset for Image Captioning with Reading\nComprehension. arXiv:2003.12462.\nSingh, A.; Natarajan, V .; Shah, M.; Jiang, Y .; Chen, X.; Ba-\ntra, D.; Parikh, D.; and Rohrbach, M. 2019. Towards vqa\nmodels that can read. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition (CVPR),\n8317‚Äì8326.\nSu, Y .; Lan, T.; Li, H.; Xu, J.; Wang, Y .; and Cai, D. 2023.\nPandaGPT: One Model To Instruction-Follow Them All.\narXiv preprint arXiv:2305.16355.\nSun, Q.; Fang, Y .; Wu, L.; Wang, X.; and Cao, Y . 2023.\nEV A-CLIP: Improved Training Techniques for CLIP at\nScale. arXiv:2303.15389.\nTaori, R.; Gulrajani, I.; Zhang, T.; Dubois, Y .; Li, X.;\nGuestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Stanford\nAlpaca: An Instruction-following LLaMA model. https:\n//github.com/tatsu-lab/stanford\nalpaca. Accessed:2023-06-\n04.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lample,\nG. 2023. LLaMA: Open and Efficient Foundation Language\nModels. arXiv:2302.13971.\nVedantam, R.; Zitnick, C. L.; and Parikh, D. 2015. CIDEr:\nConsensus-based image description evaluation. In 2015\nIEEE Conference on Computer Vision and Pattern Recog-\nnition (CVPR), 4566‚Äì4575.\nWang, P.; Yang, A.; Men, R.; Lin, J.; Bai, S.; Li, Z.; Ma,\nJ.; Zhou, C.; Zhou, J.; and Yang, H. 2022a. OFA: Unify-\ning Architectures, Tasks, and Modalities Through a Sim-\nple Sequence-to-Sequence Learning Framework. CoRR,\nabs/2202.03052.\nWang, Y .; Kordi, Y .; Mishra, S.; Liu, A.; Smith, N. A.;\nKhashabi, D.; and Hajishirzi, H. 2023. Self-Instruct: Align-\ning Language Models with Self-Generated Instructions.\narXiv:2212.10560.\nWang, Y .; Mishra, S.; Alipoormolabashi, P.; Kordi, Y .;\nMirzaei, A.; Arunkumar, A.; Ashok, A.; Dhanasekaran,\nA. S.; Naik, A.; Stap, D.; Pathak, E.; Karamanolakis, G.;\nLai, H. G.; Purohit, I.; Mondal, I.; Anderson, J.; Kuznia,\nK.; Doshi, K.; Patel, M.; Pal, K. K.; Moradshahi, M.; Par-\nmar, M.; Purohit, M.; Varshney, N.; Kaza, P. R.; Verma, P.;\nPuri, R. S.; Karia, R.; Sampat, S. K.; Doshi, S.; Mishra, S.;\nReddy, S.; Patro, S.; Dixit, T.; Shen, X.; Baral, C.; Choi, Y .;\nSmith, N. A.; Hajishirzi, H.; and Khashabi, D. 2022b. Super-\nNaturalInstructions: Generalization via Declarative Instruc-\ntions on 1600+ NLP Tasks. arXiv:2204.07705.\nWei, J.; Bosma, M.; Zhao, V . Y .; Guu, K.; Yu, A. W.; Lester,\nB.; Du, N.; Dai, A. M.; and Le, Q. V . 2021. Finetuned\nlanguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\nWu, Y .; Zhao, Y .; Li, Z.; Qin, B.; and Xiong, K. 2023.\nImproving Cross-Task Generalization with Step-by-Step In-\nstructions. arXiv preprint arXiv:2305.04429.\nXu, D.; Zhao, Z.; Xiao, J.; Wu, F.; Zhang, H.; He, X.; and\nZhuang, Y . 2017. Video Question Answering via Gradually\nRefined Attention over Appearance and Motion. InProceed-\nings of the 25th ACM International Conference on Multime-\ndia, 1645‚Äì1653.\nXu, Z.; Shen, Y .; and Huang, L. 2023. MultiInstruct: Im-\nproving Multi-Modal Zero-Shot Learning via Instruction\nTuning. arXiv:2212.10773.\nYang, A.; Miech, A.; Sivic, J.; Laptev, I.; and Schmid, C.\n2021. Just ask: Learning to answer questions from millions\nof narrated videos. In International Conference on Com-\nputer Vision (ICCV), 1686‚Äì1697.\nYe, Q.; Xu, H.; Xu, G.; Ye, J.; Yan, M.; Zhou, Y .; Wang, J.;\nHu, A.; Shi, P.; Shi, Y .; Jiang, C.; Li, C.; Xu, Y .; Chen, H.;\nTian, J.; Qi, Q.; Zhang, J.; and Huang, F. 2023. mPLUG-\nOwl: Modularization Empowers Large Language Models\nwith Multimodality. arXiv:2304.14178.\nYoung, P.; Lai, A.; Hodosh, M.; and Hockenmaier, J. 2014.\nFrom image descriptions to visual denotations: New simi-\nlarity metrics for semantic inference over event descriptions.\nNlp.cs.illinois.edu.\nZhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.;\nChen, S.; Dewan, C.; Diab, M.; Li, X.; Lin, X. V .; Mihaylov,\nT.; Ott, M.; Shleifer, S.; Shuster, K.; Simig, D.; Koura, P. S.;\nSridhar, A.; Wang, T.; and Zettlemoyer, L. 2022. OPT: Open\nPre-trained Transformer Language Models. arXiv preprint\narXiv:2205.01068.\nZheng, L.; Chiang, W.-L.; Sheng, Y .; Zhuang, S.; Wu,\nZ.; Zhuang, Y .; Lin, Z.; Li, Z.; Li, D.; Xing, E. P.;\nZhang, H.; Gonzalez, J. E.; and Stoica, I. 2023. Judg-\ning LLM-as-a-judge with MT-Bench and Chatbot Arena.\narXiv:2306.05685.\nZhu, D.; Chen, J.; Shen, X.; Li, X.; and Elhoseiny, M.\n2023. MiniGPT-4: Enhancing Vision-Language Under-\nstanding with Advanced Large Language Models. arXiv\npreprint arXiv:2304.10592.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n2264",
  "topic": "Simple (philosophy)",
  "concepts": [
    {
      "name": "Simple (philosophy)",
      "score": 0.8347891569137573
    },
    {
      "name": "Computer science",
      "score": 0.5383417010307312
    },
    {
      "name": "Artificial intelligence",
      "score": 0.36023208498954773
    },
    {
      "name": "Information retrieval",
      "score": 0.3330535292625427
    },
    {
      "name": "Epistemology",
      "score": 0.15242746472358704
    },
    {
      "name": "Philosophy",
      "score": 0.10285884141921997
    }
  ]
}