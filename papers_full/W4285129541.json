{
    "title": "Global in Local: A Convolutional Transformer for SAR ATR FSL",
    "url": "https://openalex.org/W4285129541",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2105094737",
            "name": "Chenwei Wang",
            "affiliations": [
                "University of Electronic Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A2098520868",
            "name": "Yulin Huang",
            "affiliations": [
                "University of Electronic Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A2104523245",
            "name": "Xiaoyu Liu",
            "affiliations": [
                "University of Electronic Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A2100356479",
            "name": "Jifang Pei",
            "affiliations": [
                "University of Electronic Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A2112981592",
            "name": "Yin Zhang",
            "affiliations": [
                "University of Electronic Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A1542405392",
            "name": "Jianyu Yang",
            "affiliations": [
                "University of Electronic Science and Technology of China"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2997085028",
        "https://openalex.org/W3121583704",
        "https://openalex.org/W3127085598",
        "https://openalex.org/W2922891743",
        "https://openalex.org/W2886603761",
        "https://openalex.org/W3056736931",
        "https://openalex.org/W4206706211",
        "https://openalex.org/W6735013348",
        "https://openalex.org/W6735531217",
        "https://openalex.org/W3012255272",
        "https://openalex.org/W2979689312",
        "https://openalex.org/W6735236233",
        "https://openalex.org/W3205196764",
        "https://openalex.org/W3022640406"
    ],
    "abstract": "Convolutional neural networks (CNNs) have dominated the synthetic aperture\\nradar (SAR) automatic target recognition (ATR) for years. However, under the\\nlimited SAR images, the width and depth of the CNN-based models are limited,\\nand the widening of the received field for global features in images is\\nhindered, which finally leads to the low performance of recognition. To address\\nthese challenges, we propose a Convolutional Transformer (ConvT) for SAR ATR\\nfew-shot learning (FSL). The proposed method focuses on constructing a\\nhierarchical feature representation and capturing global dependencies of local\\nfeatures in each layer, named global in local. A novel hybrid loss is proposed\\nto interpret the few SAR images in the forms of recognition labels and\\ncontrastive image pairs, construct abundant anchor-positive and anchor-negative\\nimage pairs in one batch and provide sufficient loss for the optimization of\\nthe ConvT to overcome the few sample effect. An auto augmentation is proposed\\nto enhance and enrich the diversity and amount of the few training samples to\\nexplore the hidden feature in a few SAR images and avoid the over-fitting in\\nSAR ATR FSL. Experiments conducted on the Moving and Stationary Target\\nAcquisition and Recognition dataset (MSTAR) have shown the effectiveness of our\\nproposed ConvT for SAR ATR FSL. Different from existing SAR ATR FSL methods\\nemploying additional training datasets, our method achieved pioneering\\nperformance without other SAR target images in training.\\n",
    "full_text": "1\nGlobal in Local: A Convolutional Transformer for\nSAR ATR FSL\nChenwei Wang, Student Member, IEEE,Yulin Huang, Senior Member, IEEE,Xiaoyu Liu, Student Member, IEEE,\nJifang Pei, Member, IEEE,Yin Zhang, Member, IEEE,Jianyu Yang, Member, IEEE\nAbstract—Convolutional neural networks (CNNs) have dom-\ninated the synthetic aperture radar (SAR) automatic target\nrecognition (ATR) for years. However, under the limited SAR\nimages, the width and depth of the CNN-based models are\nlimited, and the widening of the received field for global features\nin images is hindered, which finally leads to the low performance\nof recognition. To address these challenges, we propose a Con-\nvolutional Transformer (ConvT) for SAR ATR few-shot learning\n(FSL). The proposed method focuses on constructing a hierar-\nchical feature representation and capturing global dependencies\nof local features in each layer, named global in local. A novel\nhybrid loss is proposed to interpret the few SAR images in the\nforms of recognition labels and contrastive image pairs, construct\nabundant anchor-positive and anchor-negative image pairs in\none batch and provide sufficient loss for the optimization of the\nConvT to overcome the few sample effect. An auto augmentation\nis proposed to enhance and enrich the diversity and amount of the\nfew training samples to explore the hidden feature in a few SAR\nimages and avoid the over-fitting in SAR ATR FSL. Experiments\nconducted on the Moving and Stationary Target Acquisition and\nRecognition dataset (MSTAR) have shown the effectiveness of\nour proposed ConvT for SAR ATR FSL. Different from existing\nSAR ATR FSL methods employing additional training datasets,\nour method achieved pioneering performance without other SAR\ntarget images in training.\nIndex Terms—SAR ATR, FSL, convolutional transformer,\nhybrid loss, auto data augmentation\nI. I NTRODUCTION\nS\nAR is an important microwave remote sensing system\nin both mechanism and application [1]–[5], which makes\nSAR automatic target recognition (ATR) become one of the\nmost important and crucial issues in SAR practical application.\nIn recent several years, deep learning has also illustrated its\neffectiveness in the field of SAR ATR. Many deep learning-\nbased methods are proposed in SAR ATR applications and\nachieved remarkable results [6]–[12].\nHowever, to acquire great generalization performance of\nrecognition under various imaging scenarios, these existing\nSAR ATR algorithms require abundant labeled samples for\neach target type to train the deep network. However, it is\noften impossible to provide sufficient images in practical\napplications. These problems have promoted the researches on\nfew-shot learning (FSL) in SAR ATR, which can be divided\nThis work was supported by the National Natural Science Foundation of\nChina under Grants 61901091 and 61901090. ( Corresponding author: Yulin\nHuang.)\nThe authors are with the Department of Electrical Engineering, University\nof Electronic Science and Technology of China, Chengdu 611731, China (e-\nmail: yulinhuang@uestc.edu.cn; dbw181101@163.com).\ninto two categories: data augmentation methods and deep\nmodel-based methods [13]–[22].\nData augmentation method is a technique to enhance the\namount and enrich the quality of training SAR images with\nthe help of sufficient similar SAR images. For example, Wang\net al. [23] designed a semi-supervised learning framework\nincluding self-consistent augmentation rule, mixup-based la-\nbeled and unlabeled mixture, and weighted loss, to utilize unla-\nbeled data during training. Deep model-based methods mainly\ndepend upon architecture design for rapid generalization of\nfew-shot learning tasks. For example, Wang et al. [7] proposed\na hybrid inference network (HIN) including an embedding\nnetwork stage and a hybrid inference strategy stage, which\nobtained good performance of 3-target classification.\nThese data augmentation methods construct the manifold\nstructure of unlabeled samples similar to few labeled training\nsamples. However, these methods still acquire the similar\nmanifold structure of sufficient unlabeled samples. The suf-\nficient unlabeled samples do not always exist, let alone in the\nscene of changing characteristics in SAR images under various\nimaging scenarios. These deep model-based methods exploit\nsufficient labeled training samples under quite similar imaging\nconditions to train the models. However, these model-based\nmethods still need sufficient SAR images to avoid overfitting\nand acquire great generalization of recognition performance.\nThe key to addressing the problem of insufficient SAR\nsamples in SAR ATR needs two critical elements: an ef-\nfective framework to extract optimal features, and a hybrid\noptimization for limited SAR images. The framework can\nextract effective feature for the recognition with a relatively\nshadow structure. The hybrid optimization can exploit the\nsufficient information for the optimization of the framework\nto overcome the limited sample effect. In light of the vigorous\ndevelopment and superior performance of visual transformers,\nit can provide a novel view for SAR image interpretation\ndifferent from the convolutional neural networks (CNNs).\nAs opposed to CNNs that mainly focus on extracting local\nfeatures, a transformer can capture global dependencies in the\nimage, which is mainly based on the self-attention mechanism\nwith strong representation capabilities [24]. For SAR ATR\nFSL, limited SAR images hinder the width and depth of the\nCNN-based model, which also hinder widening the received\nfield for great generalization of recognition performance and\nachieving high performance of recognition. These limitations\nof CNN-based backbone may be the reason that these existing\nSAR ATR methods for FSL still need sufficient training data\nto support the CNN model [24].\narXiv:2308.05464v1  [eess.IV]  10 Aug 2023\n2\n…\nFeature maps 1\nFeature maps 2Input image \n…\n…\nPatch Partition\nConvolution \nEmbedding\nTransformer Encoder\nStage 1\nConvolution \nEmbedding\nTransformer Encoder\nStage 2\n…\n……\nConvolution \nEmbedding\nTransformer Encoder\nStage 3\n……\nMLP head\nIdentity AutoContrast Equalize\nPosterize Contrast Brightness \nSharpness TranslateX TranslateY\nRotate Solarize Color \nCross Entropy \nLoss\nLM-SoftMax\nFeature Vector\nTriplet Loss\nHybrid Loss\nLoss\nFramework\nTraining Dataset\nAuto Augmentation\ntype set\nAugmentation Modules\nMulti-Head\nAttention\nMLP\nNorm\nNorm\nLinear Linear Linear\nV K Q\nScaled Dot-Product\nAttention\nConcat\nLinear\nh\nFig. 1. Framework of the proposed ConvT.\nTherefore, we proposed a Convolutional Transformer\n(ConvT) for SAR ATR FSL, which naturally integrates the\nlocal features of CNNs and global dependencies of transform-\ners with a novel hybrid loss for the limited SAR images. Our\nmodel consists of three parts as shown in Fig.1, convolutional\ntransformer (orange box), hybrid loss (green box), and auto\ndata augmentation (blue box). The pipeline of our model\ncan be described as follow. Limited SAR images go through\nauto data augmentation to enhance its amount and diver-\nsity. Local and global features are extracted and interpreted\nby convolutional transformer, and the hybrid loss of label\npropagation and contrastive learning is proposed to provide\nsufficient optimization for better generalization of recognition\nperformance. The main contributions are as follows.\ni) We proposed a convolutional transformer, which con-\nstructs a hierarchical feature representation and extracts the\nglobal dependency of local features in each layer. This model\nnot only preserves the capability of CNNs to extract local\nfeatures and widen the received field in a hierarchical structure\nbut also acquires the global dependencies in the whole SAR\nimages and local features which provides better generalization\nfor recognition under limited training samples.\nii) The hybrid loss is proposed for providing more abundant\noptimization for the proposed model. The hybrid loss can\novercome the few sample effect by interpreting the few SAR\nimages in the forms of recognition labels and contrastive image\npairs. The auto data augmentation is proposed to express many\naugmentation policies for each epoch in training to explore the\nhidden feature in a few SAR images.\niii) The proposed model achieves competitive performance\nto state-of-the-art methods on standard benchmarks. Without\nany other SAR target images in training except k-shot for each\nclass (support samples), the recognition rates of 10 support\nsamples for each class are above 85.00%, and the rates of 5\nsupport samples for each class are above 75.00%.\nThe remaining structure of this paper is organized as fol-\nlows. The proposed method is described in Section II. Section\nIII illustrates the experiments and results. Finally, a brief\nconclusion is drawn in Section IV .\nII. P ROPOSED METHOD\nTo address the FSL problem in SAR ATR, and overcome\nthe limitation of the CNN-based backbone for SAR ATR,\nthe proposed method consists of three main components: 1)\na convolutional transformer. 2) a hybrid loss. 3) an auto\naugmentation. The three parts are introduced in detail as\nfollows.\nA. Framework of ConvT\nThe proposed ConvT aims to overcome the limitation of\nthe CNN-based backbone for SAR ATR FSL. It naturally\nintegrates convolutional layers and transformers to construct\na hierarchical feature representation that can extract local\nfeatures layer by layer and focuses on global dependencies\nof local features in each layer and the whole image.\nThe whole framework of ConvT is constructed in several\nstages by stacking convolution embedding and transformer\nencoder as shown in Fig.1. The input image is split into\nnon-overlapping patches by a patch partition module. Each\npatch is one part of the whole image and all the patches can\nrestructure back into the original images. In each stage, there\nare two parts. First, these patches are reshaped to the 2D spatial\ngrid and go through convolution embedding to extract local\n3\nfeatures. This convolution embedding can allow each stage\nto widen the received field and reduce the feature resolution\nprogressively to acquire spatial downsampling and boost the\ndensity of features. Second, a transformer encoder is employed\nto capture global dependencies of local features in each stage\nwith the position embedding. Finally, through several stages\nas above, the feature maps of the last transformer encoder are\ninput into one MLP layer to predict the class. The iterative\nprocess in ConvT can be formulated as\nIi = conv (Ti−1) (1)\nLi = MHA (LN (Ii)) +Ii (2)\nTi = MLP (LN (Li)) +Li (3)\nwhere Ti−1 and Ti mean the output feature maps of i−1 stage\nand i stage, conv means the convolution embedding, LN and\nMHA denote the layernorm and multi-head self attention,\nMLP means multilayer perceptron layer.\nThe structure of the transformer encoder is shown in Fig.1.\nAfter the layer normalization, the feature maps are transformed\ninto three matrixes, Q, K and V . Then the three matrixes go\nthrough the multi-head attention which is shown in Modules\nof Fig.1. For a more comprehensive exploration of the target\ninformation, the attention employed different linear transfor-\nmations to the features and calculate the scaled dot-product\nattention as\nattention = softmax\n\u0012QKT\n√dk\n\u0013\nV (4)\nwhere dk is the scaled parameter. The outputs of the scale dot-\nproduct attention are concatenated and linearly transformed.\nAnother layer normalization and one MLP layer are employed\nafter the addition. Besides, the layer normalization is a hor-\nizontal normalization which comprehensively considers the\ninputs of all dimensions in one layer. It calculates the mean\nand variance of the inputs in one layer and employs the same\nnormalization operation in the inputs of all dimensions.\nThrough the designed framework above, the model not only\nretains the hierarchical feature representation of CNNs, but\nalso captures the global dependencies in local features of each\nstage and whole image, which has fewer demands for deep\nstructures and acquires high potential for better recognition\nperformance than CNNs.\nThe computational complexity of the proposed method is\nin the order of O\n\u0010\nˆV1 · ˆV2 · W1 · W2 · F · I\n\u0011\n, where ˆV1 · ˆV2 is\nthe size of input images, W1 · W2 is the size of convolution\nkernel, F and I are the numbers of the convolution kernels\nand feature maps. Therefore, the complexity of our method is\nin the same order as that of other deep learning methods.\nTo provide enough optimization gradient and overcome the\nfew sample effect, hybrid loss and auto augmentation are\nproposed as follows.\nB. Hybrid loss and Auto Augmentation\nTo tackle the insufficient optimization of the FSL problem\nin SAR ATR, we proposed hybrid loss consisting of cross\nentropy loss and triplet loss to construct abundant positive and\nT62\nD7\nBTR60\nZSU_234\nZIL_131\nI2S1\nBMP2\n BRDM_2\n BTR70\nT72\nFig. 2. Optical images and corresponding SAR images of ten classes of\nobjects in the MSTAR dataset.\nTABLE I\nORIGINAL IMAGES IN MSTAR D ATASET UNDER SOC\nTarget Type BMP2 BRDM2 BTR60 BTR70 D7\nTraining(17◦) 233 298 256 233 299\nTesting(15◦) 195 274 195 196 274\nTarget Type 2S1 T62 T72 ZIL131 ZSU235\nTraining(17◦) 299 299 232 299 299\nTesting(15◦) 274 273 196 274 274\nnegative image pairs in one batch and provide sufficient loss\nfor the optimization of the ConvT. When the SAR images go\nthrough the framework, and the MLP layer gives the predicted\nprobability vectors of recognition results, the hybrid loss is\ncalculated. The visual interpretation is shown in Fig.1, the\ncross entropy loss is calculated after LM-SoftMax [25] to\nexpand the inter-class gap, and the triplet loss [26] has a\nmargin to expand the desired difference between the anchor-\npositive distance and the anchor-negative distance. In our\nmethod, the LM-SoftMax-based cross entropy loss and triplet\nloss are calculated as\nLe (w, b) =−\nCX\ni=1\nyi log (p (yi |x)) (5)\nLt = max (d (a, p) − d (a, n) + margin, 0) (6)\nLb = Le + Lt (7)\nwhere p (yi |x) is the probability vector of the recognition\nresult of the ith SAR chip, yi is the recognition labels and C\nis the number of the recognition classes, d and margin mean\nthe distance and the desired difference between the anchor-\npositive distance and the anchor-negative distance.\nAuto augmentation aims to enhance and enrich the diver-\nsity and amount of the few training samples to explore the\nhidden feature in a few SAR images and avoid the over-\nfitting in SAR ATR FSL. It consists of two main stages with\none transformation set. Given N available transformations in\nthe transformation set, the transformation number K, global\n4\nTABLE II\nTRAINING AND TESTING DATASET UNDER EOC S\nTrain Number Test(EOC-D) Number\n2S1 299 2S1(b01) 288\nBRDM2 298 BRDM2(E71) 287\nT72 232 T72(A64) 288\nZSU234 299 ZSU234(d08) 288\nTrain Number Test(EOC-CV) Number\nBMP2 233 T72(S7) 419\nBRDM2 298 T72(A32) 572\nBTR70 233 T72(A62) 573\nT72 232 T72(A63) 573\nT72(A64) 573\nTrain Number Test(EOC-VV) Number\nBMP2 233 T72(SN812) 426\nT72(A04) 573\nBRDM2 298 T72(A05) 573\nT72(A07) 573\nBTR70 233 T72(A10) 567\nBMP2(9566) 428\nT72 232 BMP2(C21) 429\ndistortion D, and threshold for auto augmentation and each\ntransformation in one epoch, Ma and Meach. First, for each\nepoch in training, a number m conforming to the standard\nnormal distribution is generated. if m ≥ Ma, the augmen-\ntation is utilized in this epoch. Then K transformations are\nrandomly chosen from the N available transformations in\nthe transformation set. K numbers conforming to standard\nnormal distribution are generated\n\b\nm1, m2, m3 . . . , mk\t\n, if\nmi ≥ Meach, then ith transformer of the randomly chosen\nK transformations is applied in this epoch. Therefore, auto\naugmentation may thus express NK potential policies for each\nepoch in training to explore the hidden feature in a few SAR\nimages and increase regularization strength.\nIII. E XPERIMENTAL RESULTS\nIn this section, the extensive experiments are conducted\nunder both the Standard Operating Condition (SOC) and the\nExtended Operating Condition (EOC) consisting of EOC- CV\n(configuration variant), EOC-D (depression variant) and EOC-\nVV (version variant). N-way K-shot denotes the K training\nsamples for all N target classes.\nA. Dataset\nThe MSTAR dataset is a benchmark dataset for the SAR\nATR performance assessment. The dataset contains a series of\n0.3m × 0.3m SAR images of ten different classes of ground\ntargets. The optical images and corresponding SAR images\nof ten classes of targets in the MSTAR dataset are shown in\nFig.2. The training and testing data under SOC and EOCs have\nbeen shown in Table I and Table II. The proposed method is\ntested and evaluated on a computer with Intel Core I7-9700K\nat 3.6GHz CPU, Gefore GTX 1080ti GPU with two 16GB\nmemories with the open-source PyTorch framework. In our\nexperiments, the parameters of the auto augmentation are set\nas below. The available transformations N is 12 as shown in\nFig.1, global distortion D is 3, Ma and Meach are 0. For 1-\nshot and 2-shot, the transformation number K is 5. For other\nN-shot, the transformation number K is 3.\nB. Recognition Performance and Comparison\nThis subsection presents the recognition performance and\ncomparison under SOC and EOCs. Other algorithms [27]–[30]\nare compared with our ConvT. The recognition ratios of the\nother algorithms are cited from [30]. For K-shot, our method\nrandomly chooses K images for each class in MSTAR to train\nthe model. Other methods also randomly choose K images for\neach class in MSTAR.\nIn Table III, the recognition performances of SOC are\npresent. Ten-way K-shot involved in the experiments were\nrandomly selected with a 17◦ depression angle from the SOC\ntraining dataset. The average recognition ratios are calculated\nafter 20 experiments. From Table III, it is clear that when\nthe training samples increase, the recognition performance is\nimproved gradually. For the experiments of 25-shot, 10-shot\nand 5-shot, our ConvT can achieve the highest recognition\nratios, 96.52% for 25-shot, 88.63% for 10-shot and 75.16%\nfor 5-shot. Under 10-shot and 5-shot, our ConvT has more\nobvious advantages than other methods. For the experiments of\n1-shot and 2-shot, our method is closer to the best recognition\nperformance of DKTS-N, 54.37% for 2-shot and 42.57% for\n1-shot.\nIn Table III, the recognition performances of EOCs are\npresent. Four-way K-shot involved in the experiments were\nrandomly selected from the EOCs’ training dataset. The av-\nerage recognition ratios are calculated after 20 experiments.\nFrom Table III, the recognition performance of EOC-D has\nshown the effectiveness of the capability of extracting global\nfeatures at the local image level. The recognition ratio of\nEOC-D under 25-shot, 10-shot, 5-shot, 2-shot and 1-shot\nare 79.14%, 74.80%, 68.17%, 64.06% and 59.57% respec-\ntively. The recognition performance decreased lightly when the\ntraining samples is reduced. For EOC-CV and EOC-VV , the\nrecognition performances under 25-shot, 10-shot and 5-shot\nare obviously better than other algorithms, which illustrated\nour method has a higher upper bound. The recognition ratios\nof EOC-CV and EOC-VV under 2-shot and 1-shot are close\nto the best recognition performance from DKTS-N. Besides,\nthe computation time for each image is approximately 15ms.\nFrom the recognition performance of SOC and EOCs, the\nresults have illustrated that our method has the robustness and\neffectiveness for SOC and EOCs, the recognition ratios hold\nat a high level facing the large depression angel, configura-\ntion variant and version variant. Without additional training\ndatasets, our method has achieved state-of-the-art performance\nof SAR ATR FSL.\nIV. C ONCLUSION\nFor SAR ATR FSL, limited SAR images hinder the width\nand depth of the CNN-based model, which is the key to wider\nthe receive field of the model for extracting global features in\nimages and achieving high performance of recognition. The\n5\nTABLE III\nRESULTS OF FSL A LGORITHMS UNDER SOC AND EOC S\nSOC\nAlgorithms 10-way 1-shot 10-way 2-shot 10-way 5-shot 10-way 10-shot 10-way 25-shot\nDeepEMD [27] 36.19 ±0.46 43.49 ±0.44 53.14 ±0.40 59.64 ±0.39 59.71 ±0.31\nDN4 [28] 33.25 ±0.49 44.15 ±0.45 36.19 ±0.46 36.19 ±0.46 36.19 ±0.46\nPrototypical Network [29] 40.94 ±0.47 54.54 ±0.44 36.19 ±0.46 36.19 ±0.46 36.19 ±0.46\nDKTS-N [30] 49.26±0.48 58.51 ±0.42 72.32±0.32 84.59 ±0.24 96.15 ±0.08\nOurs 42.57 ±0.79 54.37 ±0.62 75.16±0.21 88.63 ±0.22 96.52 ±0.15\nEOC-D\nAlgorithms 4-way 1-shot 4-way 2-shot 4-way 5-shot 4-way 10-shot 4-way 25-shot\nDeepEMD [27] 56.81 ±0.99 62.80 ±0.78 36.19 ±0.46 36.19 ±0.46 36.19 ±0.46\nDN4 [28] 46.59 ±0.83 51.41 ±0.69 36.19 ±0.46 36.19 ±0.46 36.19 ±0.46\nPrototypical Network [29] 53.59 ±0.93 56.57 ±0.53 36.19 ±0.46 36.19 ±0.46 36.19 ±0.46\nDKTS-N [30] 61.91±0.91 63.94±0.73 67.43 ±0.48 71.09 ±0.41 78.94 ±0.31\nOurs 59.57 ±0.76 64.06±0.88 68.17 ±0.38 74.80 ±0.20 79.14 ±0.42\nEOC-CV\nAlgorithms 4-way 1-shot 4-way 2-shot 4-way 5-shot 4-way 10-shot 4-way 25-shot\nDeepEMD [27] 38.39 ±0.86 45.65 ±0.75 36.19 ±0.46 36.19 ±0.46 36.19 ±0.46\nDN4 [28] 46.13 ±0.69 51.21 ±0.62 36.19 ±0.46 36.19 ±0.46 36.19 ±0.46\nPrototypical Network [29] 43.59 ±0.84 51.17 ±0.78 36.19 ±0.46 36.19 ±0.46 36.19 ±0.46\nDKTS-N [30] 47.26±0.79 53.61 ±0.70 62.23±0.56 68.41 ±0.51 74.51 ±0.36\nOurs 44.32 ±0.65 51.93 ±0.82 64.12±0.34 89.74 ±0.18 90.95 ±0.23\nEOC-VV\nAlgorithms 4-way 1-shot 4-way 2-shot 4-way 5-shot 4-way 10-shot 4-way 25-shot\nDeepEMD [27] 40.92 ±0.76 49.12 ±0.65 36.19 ±0.46 36.19 ±0.46 36.19 ±0.46\nDN4 [28] 47.00 ±0.72 52.21 ±0.61 36.19 ±0.46 36.19 ±0.46 36.19 ±0.46\nPrototypical Network [29] 45.13 ±0.72 52.86 ±0.65 36.19 ±0.46 36.19 ±0.46 36.19 ±0.46\nDKTS-N [30] 48.91±0.70 55.14±0.58 65.63 ±0.49 70.18 ±0.42 76.97 ±0.35\nOurs 42.27 ±0.89 58.27±0.68 68.05 ±0.52 83.55 ±0.25 91.98 ±0.31\nproposed ConvT constructed a hierarchical feature represen-\ntation and extract global features in the local representation\nof each layer. The hybrid loss constructed abundant anchor-\npositive and anchor-negative image pairs in one batch and\nprovided sufficient loss for the optimization of the ConvT. The\nauto augmentation is employed to enhance and enrich the di-\nversity and amount of the few training samples. Experimental\nresults on the MSTAR dataset have validated the effectiveness\nand robustness of the proposed ConvT in few-shot recognition\nin SAR. Different from existing SAR ATR FSL methods\nemploying additional training datasets, our ConvT achieved\npioneering performance without other SAR target images in\ntraining besides support samples of MSTAR. Though our\nmethod achieves high performances in SAR ATR, it still lacks\nthe ability to employ the unlabeld data. In the future work, we\nwill focus on the growing amount of unlabeled data to further\nimprove the robustness of the model and expand the scope of\npractical applications of SAR ATR.\nREFERENCES\n[1] J. C. Curlander and R. N. McDonough, Synthetic aperture radar. Wiley,\nNew York, 1991, vol. 11.\n[2] C. Wang, X. Liu, Y . Huang, S. Luo, J. Pei, J. Yang, and D. Mao, “Semi-\nsupervised sar atr framework with transductive auxiliary segmentation,”\nRemote Sensing, vol. 14, no. 18, p. 4547, 2022.\n[3] C. Wang, J. Pei, Z. Wang, Y . Huang, J. Wu, H. Yang, and J. Yang,\n“When deep learning meets multi-task learning in sar atr: Simultaneous\ntarget recognition and segmentation,” Remote Sensing, vol. 12, no. 23,\np. 3863, 2020.\n[4] C. Wang, J. Pei, X. Liu, Y . Huang, D. Mao, Y . Zhang, and J. Yang, “Sar\ntarget image generation method using azimuth-controllable generative\nadversarial network,” IEEE Journal of Selected Topics in Applied Earth\nObservations and Remote Sensing, vol. 15, pp. 9381–9397, 2022.\n[5] C. Wang, J. Pei, X. Liu, Y . Huang, and J. Yang, “A deep deformable\nresidual learning network for sar image segmentation,” in 2021 IEEE\nRadar Conference (RadarConf21). IEEE, 2021, pp. 1–5.\n[6] C. Cao, Z. Cao, and Z. Cui, “Ldgan: A synthetic aperture radar image\ngeneration method for automatic target recognition,” IEEE Transactions\non Geoscience and Remote Sensing, vol. 58, no. 5, pp. 3495–3508, 2019.\n[7] L. Wang, X. Bai, C. Gong, and F. Zhou, “Hybrid inference network\nfor few-shot sar automatic target recognition,” IEEE Transactions on\nGeoscience and Remote Sensing, 2021.\n[8] C. Wang, S. Luo, J. Pei, X. Liu, Y . Huang, Y . Zhang, and J. Yang,\n“An entropy-awareness meta-learning method for sar open-set atr,”IEEE\nGeoscience and Remote Sensing Letters, 2023.\n[9] C. Wang, J. Pei, S. Luo, W. Huo, Y . Huang, Y . Zhang, and J. Yang, “Sar\nship target recognition via multiscale feature attention and adaptive-\nweighed classifier,” IEEE Geoscience and Remote Sensing Letters ,\nvol. 20, pp. 1–5, 2023.\n[10] C. Wang, X. Liu, J. Pei, Y . Huang, Y . Zhang, and J. Yang, “Multiview\nattention cnn-lstm network for sar automatic target recognition,” IEEE\nJournal of Selected Topics in Applied Earth Observations and Remote\nSensing, vol. 14, pp. 12 504–12 513, 2021.\n[11] C. Wang, J. Pei, M. Li, Y . Zhang, Y . Huang, and J. Yang, “Parking\ninformation perception based on automotive millimeter wave sar,” in\n2019 IEEE Radar Conference (RadarConf). IEEE, 2019, pp. 1–6.\n[12] C. Wang, J. Pei, Z. Wang, Y . Huang, and J. Yang, “Multi-view cnn-lstm\nneural network for sar automatic target recognition,” in IGARSS 2020-\n2020 IEEE International Geoscience and Remote Sensing Symposium.\nIEEE, 2020, pp. 1755–1758.\n[13] X. Wang, Z. Cao, and Y . Pi, “Semisupervised classification with adaptive\nanchor graph for polsar images,” IEEE Geoscience and Remote Sensing\nLetters, vol. 19, pp. 1–5, 2021.\n[14] Z. Yue, F. Gao, Q. Xiong, J. Wang, T. Huang, E. Yang, and H. Zhou,\n“A novel semi-supervised convolutional neural network method for\n6\nsynthetic aperture radar image recognition,” Cognitive Computation,\nvol. 13, no. 4, pp. 795–806, 2021.\n[15] F. Gao, F. Ma, J. Wang, J. Sun, E. Yang, and H. Zhou, “Semi-\nsupervised generative adversarial nets with multiple generators for sar\nimage recognition,” Sensors, vol. 18, no. 8, p. 2706, 2018.\n[16] X. Liu, Z. Tao, J. Shao, L. Yang, and X. Huang, “Elimrec: Eliminating\nsingle-modal bias in multimedia recommendation,” in Proceedings of\nthe 30th ACM International Conference on Multimedia, ser. MM ’22.\nAssociation for Computing Machinery, 2022.\n[17] Z. Tao, X. Liu, Y . Xia, X. Wang, L. Yang, X. Huang, and T.-S.\nChua, “Self-supervised learning for multimedia recommendation,” IEEE\nTransactions on Multimedia, 2022.\n[18] Y . Wei, X. Liu, Y . Ma, X. Wang, L. Nie, and T. Chua, “Strategy-aware\nbundle recommender system,” in SIGIR. ACM, 2023.\n[19] Y . Zhao, J. Wei, Z. Lin, Y . Sun, M. Zhang, and M. Zhang, “Visual\nspatial description: Controlled spatial-oriented image-to-text generation,”\nin Proceedings of the 2022 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates,\nDecember 7-11, 2022. Association for Computational Linguistics, 2022,\npp. 1437–1449.\n[20] Y . Zhao, H. Fei, W. Ji, J. Wei, M. Zhang, M. Zhang, and T. Chua,\n“Generating visual spatial description via holistic 3d scene understand-\ning,” in Proceedings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto,\nCanada, July 9-14, 2023. Association for Computational Linguistics,\n2023, pp. 7960–7977.\n[21] C. Wang, J. Pei, J. Yang, X. Liu, Y . Huang, and D. Mao, “Recognition in\nlabel and discrimination in feature: A hierarchically designed lightweight\nmethod for limited data in sar atr,” IEEE Transactions on Geoscience\nand Remote Sensing, vol. 60, pp. 1–13, 2022.\n[22] C. Wang, Y . Huang, X. Liu, J. Pei, Y . Zhang, and J. Yang, “Global in\nlocal: A convolutional transformer for sar atr fsl,” IEEE Geoscience and\nRemote Sensing Letters, vol. 19, pp. 1–5, 2022.\n[23] C. Wang, J. Shi, Y . Zhou, X. Yang, Z. Zhou, S. Wei, and X. Zhang,\n“Semisupervised learning-based sar atr via self-consistent augmenta-\ntion,” IEEE Transactions on Geoscience and Remote Sensing, vol. 59,\nno. 6, pp. 4862–4873, 2020.\n[24] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah,\n“Transformers in vision: A survey,” arXiv preprint arXiv:2101.01169,\n2021.\n[25] W. Liu, Y . Wen, Z. Yu, and M. Yang, “Large-margin softmax loss for\nconvolutional neural networks.” in ICML, vol. 2, no. 3, 2016, p. 7.\n[26] A. Hermans, L. Beyer, and B. Leibe, “In defense of the triplet loss for\nperson re-identification,” arXiv preprint arXiv:1703.07737, 2017.\n[27] C. Zhang, Y . Cai, G. Lin, and C. Shen, “Deepemd: Few-shot image\nclassification with differentiable earth mover’s distance and structured\nclassifiers,” in Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, 2020, pp. 12 203–12 213.\n[28] W. Li, L. Wang, J. Xu, J. Huo, Y . Gao, and J. Luo, “Revisiting\nlocal descriptor based image-to-class measure for few-shot learning,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2019, pp. 7260–7268.\n[29] J. Snell, K. Swersky, and R. S. Zemel, “Prototypical networks for few-\nshot learning,” arXiv preprint arXiv:1703.05175, 2017.\n[30] L. Zhang, X. Leng, S. Feng, X. Ma, K. Ji, G. Kuang, and L. Liu,\n“Domain knowledge powered two-stream deep network for few-shot\nsar vehicle recognition,” IEEE Transactions on Geoscience and Remote\nSensing, 2021."
}