{
  "title": "Representations of language in a model of visually grounded speech signal",
  "url": "https://openalex.org/W2586148577",
  "year": 2017,
  "authors": [
    {
      "id": "https://openalex.org/A3150780103",
      "name": "Grzegorz Chrupała",
      "affiliations": [
        "Tilburg University"
      ]
    },
    {
      "id": "https://openalex.org/A2553819564",
      "name": "Lieke Gelderloos",
      "affiliations": [
        "Tilburg University"
      ]
    },
    {
      "id": "https://openalex.org/A1976713734",
      "name": "Afra Alishahi",
      "affiliations": [
        "Tilburg University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3213502289",
    "https://openalex.org/W2155889433",
    "https://openalex.org/W2143623448",
    "https://openalex.org/W2963899908",
    "https://openalex.org/W68733909",
    "https://openalex.org/W4245833664",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4299801216",
    "https://openalex.org/W2531381952",
    "https://openalex.org/W2473934411",
    "https://openalex.org/W2580178245",
    "https://openalex.org/W2137010615",
    "https://openalex.org/W2134670479",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2556930864",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W2562979205",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2962753610",
    "https://openalex.org/W2962862718",
    "https://openalex.org/W4237938692",
    "https://openalex.org/W2396384435",
    "https://openalex.org/W2127438782",
    "https://openalex.org/W4394453761",
    "https://openalex.org/W2951674897",
    "https://openalex.org/W2251861449",
    "https://openalex.org/W2953177656",
    "https://openalex.org/W2250790822",
    "https://openalex.org/W2953188482",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2292919134",
    "https://openalex.org/W2953318193",
    "https://openalex.org/W2172888184",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4297826211",
    "https://openalex.org/W2962776659",
    "https://openalex.org/W4294555862",
    "https://openalex.org/W2952020226",
    "https://openalex.org/W385555557",
    "https://openalex.org/W1905882502",
    "https://openalex.org/W2160783091",
    "https://openalex.org/W2282219577",
    "https://openalex.org/W2107917162",
    "https://openalex.org/W2524611247",
    "https://openalex.org/W2515741950",
    "https://openalex.org/W2123815913",
    "https://openalex.org/W2471839888"
  ],
  "abstract": "We present a visually grounded model of speech perception which projects spoken utterances and images to a joint semantic space. We use a multi-layer recurrent highway network to model the temporal nature of spoken speech, and show that it learns to extract both form and meaning-based linguistic knowledge from the input signal. We carry out an in-depth analysis of the representations used by different components of the trained model and show that encoding of semantic aspects tends to become richer as we go up the hierarchy of layers, whereas encoding of form-related aspects of the language input tends to initially increase and then plateau or decrease.",
  "full_text": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 613–622\nVancouver, Canada, July 30 - August 4, 2017.c⃝2017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1057\nProceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 613–622\nVancouver, Canada, July 30 - August 4, 2017.c⃝2017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1057\nRepresentations of language in a model of visually grounded speech signal\nGrzegorz Chrupała\nTilburg University\ng.chrupala@uvt.nl\nLieke Gelderloos\nTilburg University\nl.j.gelderloos@uvt.nl\nAfra Alishahi\nTilburg University\na.alishahi@uvt.nl\nAbstract\nWe present a visually grounded model of\nspeech perception which projects spoken\nutterances and images to a joint seman-\ntic space. We use a multi-layer recurrent\nhighway network to model the temporal\nnature of spoken speech, and show that it\nlearns to extract both form and meaning-\nbased linguistic knowledge from the input\nsignal. We carry out an in-depth analy-\nsis of the representations used by different\ncomponents of the trained model and show\nthat encoding of semantic aspects tends\nto become richer as we go up the hierar-\nchy of layers, whereas encoding of form-\nrelated aspects of the language input tends\nto initially increase and then plateau or de-\ncrease.\n1 Introduction\nSpeech recognition is one of the success stories\nof language technology. It works remarkably well\nin a range of practical settings. However, this\nsuccess relies on the use of very heavy supervi-\nsion where the machine is fed thousands of hours\nof painstakingly transcribed audio speech signal.\nHumans are able to learn to recognize and under-\nstand speech from notably weaker and noisier su-\npervision: they manage to learn to extract struc-\nture and meaning from speech by simply being ex-\nposed to utterances situated and grounded in their\ndaily sensory experience. Modeling and emulat-\ning this remarkable skill has been the goal of nu-\nmerous studies; however in the overwhelming ma-\njority of cases researchers used severely simpliﬁed\nsettings where either the language input or the ex-\ntralinguistic sensory input, or both, are small scale\nand symbolically represented. Section 2 provides\na brief overview of this research.\nMore recently several lines of work have moved\ntowards more realistic inputs while modeling or\nemulating language acquisition in a grounded set-\nting. Gelderloos and Chrupała (2016) use the\nimage captioning dataset MS COCO (Lin et al.,\n2014) to mimic the setting of grounded language\nlearning: the sensory input consists of images of\nnatural scenes, while the language input are pho-\nnetically transcribed descriptions of these scenes.\nThe use of such moderately large and low-level\ndata allows the authors to train a multi-layer re-\ncurrent neural network model, and to explore the\nnature and localization of the emerging hierarchy\nof linguistic representations learned in the process.\nFurthermore, in a series of recent studies Harwath\nand Glass (2015); Harwath et al. (2016); Harwath\nand Glass (2017) use image captioning datasets\nto model learning to understand spoken language\nfrom visual context with convolutional neural net-\nwork models. Finally, there is a small but grow-\ning body of work dedicated to elucidating the na-\nture of representations learned by neural networks\nfrom language data (see Section 2.2 for a brief\noverview). In the current work we build on these\nthree strands of research and contribute the follow-\ning advances:\n• We use a multi-layer gated recurrent neural\nnetwork to properly model the temporal na-\nture of speech signal and substantially im-\nprove performance compared to the convolu-\ntional architecture from Harwath and Glass\n(2015);\n• We carry out an in-depth analysis of the rep-\nresentations used by different components of\nthe trained model and correlate them to repre-\nsentations learned by a text-based model and\nto human patterns of judgment on linguistic\nstimuli. This analysis is especially novel for\na model with speech signal as input.\nThe general pattern of ﬁndings in our analysis is\n613\nas follows: The model learns to extract from the\nacoustic input both form-related and semantics-\nrelated information, and encodes it in the activa-\ntions of the hidden layers. Encoding of semantic\naspects tends to become richer as we go up the hi-\nerarchy of layers. Meanwhile, encoding of form-\nrelated aspects of the language input, such as ut-\nterance length or the presence of speciﬁc words,\ntends to initially increase and then decay.\nWe release the code for our models\nand analyses as open source, available at\nhttps://github.com/gchrupala/visually-grounded-\nspeech. We also release a dataset of synthetically\nspoken image captions based on MS COCO, avail-\nable at https://doi.org/10.5281/zenodo.400926.\n2 Related work\nChildren learn to recognize and assign meaning\nto words from continuous perceptual data in ex-\ntremely noisy context. While there have been\nmany computational studies of human word mean-\ning acquisition, they typically make strong sim-\nplifying assumptions about the nature of the in-\nput. Often language input is given in the form\nof word symbols, and the context consists of a\nset of symbols representing possible referents (e.g.\nSiskind, 1996; Frank et al., 2007; Fazly et al.,\n2010). In contrast, several studies presented mod-\nels that learn from sensory rather than symbolic in-\nput, which is rich with regards to the signal itself,\nbut very limited in scale and variation (e.g. Roy\nand Pentland, 2002; Yu and Ballard, 2004; Lazari-\ndou et al., 2016).\n2.1 Multimodal language acquisition\nChrupała et al. (2015) introduce a model that\nlearns to predict the visual context from image\ncaptions. The model is trained on image-caption\npairs from MSCOCO (Lin et al., 2014), captur-\ning both rich visual input as well as larger scale\ninput, but the language input still consists of word\nsymbols. Gelderloos and Chrupała (2016) propose\na similar architecture that instead takes phoneme-\nlevel transcriptions as language input, thereby in-\ncorporating the word segmentation problem into\nthe learning task. In this work, we introduce an ar-\nchitecture that learns from continuous speech and\nimages directly.\nThis work is related to research on visual\ngrounding of language. The ﬁeld is large and\ngrowing, with most work dedicated to the ground-\ning of written text, particularly in image cap-\ntioning tasks (see Bernardi et al. (2016) for an\noverview). However, learning to ground language\nto visual information is also interesting from an\nautomatic speech recognition point of view. Po-\ntentially, ASR systems could be trained from nat-\nurally co-occurring visual context information,\nwithout the need for extensive manual annota-\ntion – a particularly promising prospect for speech\nrecognition in low-resource languages. There\nhave been several attempts along these lines. Syn-\nnaeve et al. (2014) present a method of learning\nto recognize spoken words in isolation from co-\noccurrence with image fragments. Harwath and\nGlass (2015) present a model that learns to map\npre-segmented spoken words in sequence to as-\npects of the visual context, while in Harwath and\nGlass (2017) the model also learns to recognize\nwords in the unsegmented signal.\nMost closely related to our work is that of Har-\nwath et al. (2016), as it presents an architecture\nthat learns to project images and unsegmented\nspoken captions to the same embedding space.\nThe sentence representation is obtained by feed-\ning the spectrogram to a convolutional network.\nThe architecture is trained on crowd-sourced spo-\nken captions for images from the Places dataset\n(Zhou et al., 2014), and evaluated on image search\nand caption retrieval. Unfortunately this dataset is\nnot currently available and we were thus unable to\ndirectly compare the performance of our model to\nHarwath et al. (2016). We do compare to Harwath\nand Glass (2015) which was tested on a public\ndataset. We make different architectural choices,\nas our models are based on recurrent highway net-\nworks (Zilly et al., 2016). As in human cognition,\nspeech is processed incrementally. This also al-\nlows our architecture to integrate information se-\nquentially from speech of arbitrary duration.\n2.2 Analysis of neural representations\nWhile analysis of neural methods in NLP is of-\nten limited to evaluation of the performance on\nthe training task, recently methods have been in-\ntroduced to peek inside the black box and explore\nwhat it is that enables the model to perform the\ntask. One approach is to look at the contribution\nof speciﬁc parts of the input, or speciﬁc units in the\nmodel, to ﬁnal representations or decisions. K´ad´ar\net al. (2016) propose omission scores, a method to\nestimate the contribution of input tokens to the ﬁ-\n614\nnal representation by removing them from the in-\nput and comparing the resulting representations to\nthe ones generated by the original input. In a sim-\nilar approach, Li et al. (2016) study the contribu-\ntion of individual input tokens as well as hidden\nunits and word embedding dimensions by erasing\nthem from the representation and analyzing how\nthis affects the model.\nMiao et al. (2016) and Tang et al. (2016) use vi-\nsualization techniques for ﬁne-grained analysis of\nGRU and LSTM models for ASR. Visualization\nof input and forget gate states allows Miao et al.\n(2016) to make informed adaptations to gated re-\ncurrent architectures, resulting in more efﬁciently\ntrainable models. Tang et al. (2016) visualize\nqualitative differences between LSTM- and GRU-\nbased architectures, regarding the encoding of in-\nformation, as well as how it is processed through\ntime.\nWe speciﬁcally study linguistic properties of the\ninformation encoded in the trained model. Adi\net al. (2016) introduce prediction tasks to ana-\nlyze information encoded in sentence embeddings\nabout word order, sentence length, and the pres-\nence of individual words. We use related tech-\nniques to explore encoding of aspects of form and\nmeaning within components of our stacked archi-\ntecture.\n3 Models\nWe use a multi-layer, gated recurrent neural net-\nwork (RHN) to model the temporal nature of\nspeech signal. Recurrent neural networks are de-\nsigned for modeling sequential data, and gated\nvariants (GRUs, LSTMs) are widely used with\nspeech and text in both cognitive modeling and\nengineering contexts. RHNs are a simple gener-\nalization of GRU networks such that the transform\nbetween time points can consist of several steps.\nOur multimodal model projects spoken utter-\nances and images to a joint semantic space. The\nidea of projecting different modalities to a shared\nsemantic space via a pair of encoders has been\nused in work on language and vision (among them\nVendrov et al. (2015)). The core idea is to en-\ncourage inputs representing the same meaning in\ndifferent modalities to end up nearby, while main-\ntaining a distance from unrelated inputs.\nThe model consists of two parts: an utterance\nencoder, and an image encoder. The utterance en-\ncoder starts from MFCC speech features, while\nthe image encoder starts from features extracted\nwith a VGG-16 pre-trained on ImageNet. Our loss\nfunction attempts to make the cosine distance be-\ntween encodings of matching utterances and im-\nages greater than the distance between encodings\nof mismatching utterance/image pairs, by a mar-\ngin:\n(1)\n∑\nu,i\n(∑\nu′\nmax[0,α+ d(u,i) −d(u′,i)]\n+\n∑\ni′\nmax[0,α + d(u,i) − d(u,i′)]\n)\nwhere d(u,i) is the cosine distance between the\nencoded utterance u and encoded image i. Here\n(u,i) is the matching utterance-image pair, u′\nranges over utterances not describing i and i′\nranges over images not described by u.\nThe image encoder enci is a simple linear pro-\njection, followed by normalization to unit L2\nnorm:\nenci(i) = unit(Ai + b) (2)\nwhere unit(x) = x\n(xT x)0.5 and with (A,b) as\nlearned parameters. The utterance encoder encu\nconsists of a 1-dimensional convolutional layer of\nlength s, size d and stride z, whose output feeds\ninto a Recurrent Highway Network with k lay-\ners and L microsteps, whose output in turn goes\nthrough an attention-like lookback operator, and\nﬁnally L2 normalization:\nencu(u) = unit(Attn(RHNk,L(Convs,d,z(u))))\n(3)\nThe main function of the convolutional layer\nConvs,d,z is to subsample the input along the tem-\nporal dimension. We use a 1-dimensional convo-\nlution with full border mode padding. The atten-\ntion operator simply computes a weighted sum of\nthe RHN activation at all timesteps:\nAttn(x) =\n∑\nt\nαtxt (4)\nwhere the weights αt are determined by learned\nparameters U and W, and passed through the\ntimewise softmax function:\nαt = exp(U tanh(Wxt))∑\nt′exp(U tanh(Wxt′)) (5)\nThe main component of the utterance encoder is a\nrecurrent network, speciﬁcally a Recurrent High-\nway Network (Zilly et al., 2016). The idea behind\n615\nRHN is to increase the depth of the transform be-\ntween timesteps, or the recurrence depth. Other-\nwise they are a type of gated recurrent networks.\nThe transition from timestep t− 1 to tis then de-\nﬁned as:\nrhn(xt,s(L)\nt−1) =s(L)\nt (6)\nwhere xt stands for input at time t, and s(l)\nt de-\nnotes the state at time tat recurrence layer l, with\nLbeing the top layer of recurrence. Furthermore,\ns(l)\nt = h(l)\nt ⊙ t(l)\nt + s(l−1)\nt ⊙\n(\n1 − t(l)\nt\n)\n(7)\nwhere ⊙ is elementwise multiplication, and\nh(l)\nt = tanh\n(\nI[l= 1]WHxt + UHls(l−1)\nt\n)\n(8)\nt(l)\nt = σ\n(\nI[l= 1]WT xt + UTls(l−1)\n)\n(9)\nHere I is the indicator function: input is only in-\ncluded in the computation for the ﬁrst layer of re-\ncurrence l = 1. By applying the rhn function re-\npeatedly, an RHN layer maps a sequence of inputs\nto a sequence of states:\n(10)RHN(X,s0)\n= rhn(xn,..., rhn(x2,rhn(x1,s(L)\n0 )))\nTwo or more RHN layers can be composed into a\nstack:\nRHN2(RHN1(X,s1\n(L)\n0 ),s2\n(L)\n0 ), (11)\nwhere sn\n(l)\nt stands for the state vector of layernof\nthe stack, at layer lof recurrence, at time t. In our\nversion of the Stacked RHN architecture we use\nresidualized layers:\nRHNres(X,s0) = RHN(X,s0) +X (12)\nThis formulation tends to ease optimization in\nmulti-layer models (cf. He et al., 2015; Oord et al.,\n2016).\nIn addition to the speech model described\nabove, we also deﬁne a comparable text model.\nAs it takes a sequence of words as input, we re-\nplace the convolutional layer with a word embed-\nding lookup table. We found the text model did\nnot beneﬁt from the use of the attention mecha-\nnism, and thus the sentence embedding is simply\nthe L2-normalized activation vector of the topmost\nlayer, at the last timestep.\n4 Experiments\nOur main goal is to analyze the emerging repre-\nsentations from different components of the model\nand to examine the linguistic knowledge they en-\ncode. For this purpose, we employ a number of\ntasks that cover the spectrum from fully form-\nbased to fully semantic.\nIn Section 4.2 we assess the effectiveness of our\narchitecture by evaluating it on the task of rank-\ning images given an utterance. Sections 4.3 to 4.6\npresent our analyses. In Sections 4.3 and 4.4 we\ndeﬁne auxiliary tasks to investigate to what extent\nthe network encodes information about the surface\nform of an utterance from the speech input. In Sec-\ntion 4.5 and 4.6 we focus on where semantic infor-\nmation is encoded in the model. In the analyses,\nwe use the following features:\nUtterance embeddings: the weighted sum of the\nunit activations on the last layer, as calculated\nby Equation (3).\nAverage unit activations: hidden layer activa-\ntions averaged over time and L2-normalized\nfor each hidden layer.\nAverage input vectors: the MFCC vectors aver-\naged over time. We use this feature to exam-\nine how much information can be extracted\nfrom the input signal only.\n4.1 Data\nFor the experiments reported in the remainder of\nthe paper we use two datasets of images with spo-\nken captions.\n4.1.1 Flickr8K\nThe Flickr8k Audio Caption Corpus was con-\nstructed by having crowdsource workers read\naloud the captions in the original Flickr8K cor-\npus (Hodosh et al., 2013). For details of the\ndata collection procedure refer to Harwath and\nGlass (2015). The datasets consist of 8,000 im-\nages, each image with ﬁve descriptions. One\nthousand images are held out for validation, and\nanother one thousand for the ﬁnal test set. We\nuse the splits provided by (Karpathy and Fei-Fei,\n2015). The image features come from the ﬁnal\nfully connect layer of VGG-16 (Simonyan and\nZisserman, 2014) pre-trained on Imagenet (Rus-\nsakovsky et al., 2014).\nWe generate the input signal as follows: we ex-\ntract 12-dimensional mel-frequency cepstral coef-\nﬁcients (MFCC) plus log of the total energy. We\n616\nthen compute and add ﬁrst order and second order\ndifferences (deltas) for a total of 37 dimensions.\nWe use 25 milisecond windows, sampled every 10\nmiliseconds.1\n4.1.2 Synthetically spoken COCO\nWe generated synthetic speech for the captions\nin the MS COCO dataset (Lin et al., 2014) via\nthe Google Text-to-Speech API. 2 The audio and\nthe corresponding MFCC features are released as\nChrupała et al. (2017)3. This TTS system we used\nproduces high-quality realistic-sounding speech.\nIt is nevertheless much simpler than real human\nspeech as it uses a single voice, and lacks tempo\nvariation or ambient noise. The data consists of\nover 300,000 images, each with ﬁve spoken cap-\ntions. Five thousand images each are held out for\nvalidation and test. We use the splits and image\nfeatures provided by Vendrov et al. (2015). 4 The\nimage features also come from the VGG-16 net-\nwork, but are averages of feature vectors for ten\ncrops of each image. For the MS COCO captions\nwe extracted only plain MFCC and total energy\nfeatures, and did not add deltas in order to keep\nthe amount of computation manageable given the\nsize of the dataset.\n4.2 Image retrieval\nWe evaluate our model on the task of ranking im-\nages given a spoken utterance, such that highly\nranked images contain scenes described by the ut-\nterance. The performance on this task on valida-\ntion data is also used to choose the best variant\nof the model architecture and to tune the hyperpa-\nrameters. We compare the speech models to mod-\nels trained on written sentences split into words.\nThe best settings found for the four models were\nthe following:\nFlickr8K Text RHN 300-dimensional word em-\nbeddings, 1 hidden layer with 1024 dimen-\nsions, 1 microstep, initial learning rate 0.001.\nFlick8K Speech RHN convolutional layer with\nlength 6, size 64, stride 2, 4 hidden layers\nwith 1024 dimensions, 2 microsteps, atten-\n1We noticed that for a number of utterances the audio sig-\nnal was very long: on inspection it turned out that most of\nthese involved failure to switch off the microphone on the\npart of the workers, and the audio contained ambient noise or\nunrelated speech. We thus trucated all audio for this dataset\nat 10,000 miliseconds.\n2Available at https://github.com/pndurette/gTTS.\n3Available at https://doi.org/10.5281/zenodo.400926.\n4See https://github.com/ivendrov/order-embedding.\ntion MLP with 128 hidden units, initial learn-\ning rate 0.0002\nCOCO Text RHN 300-dimensional word em-\nbeddings, 1 hidden layer with 1024 dimen-\nsions, 1 microstep, initial learning rate 0.001\nCOCO Speech RHN convolutional layer with\nlength 6, size 64, stride 3, 5 hidden layers\nwith 512 dimensions, 2 microsteps, attention\nMLP with 512 hidden units, initial learning\nrate 0.0002\nAll models were optimized with Adam\n(Kingma and Ba, 2014) with early stopping: we\nkept the parameters for the epoch which showed\nthe best recall@10 on validation data.\nModel R@1 R@5 R@10 ˜r\nSpeech RHN4,2 0.055 0.163 0.253 48\nSpectr. CNN - - 0.179 -\nText RHN1,1 0.127 0.364 0.494 11\nTable 1: Image retrieval performance on Flickr8K.\nR@N stands for recall at N; ˜r stands for median\nrank of the correct image.\nModel R@1 R@5 R@10 ˜r\nSpeech RHN5,2 0.111 0.310 0.444 13\nText RHN1,1 0.169 0.421 0.565 8\nTable 2: Image retrieval performance on\nMS COCO. R@N stands for recall at N; ˜rstands\nfor median rank of the correct image.\nTable 1 shows the results for the human speech\nfrom the Flickr8K dataset. The Speech RHN\nmodel scores substantially higher than model of\nHarwath and Glass (2015) on the same data. How-\never the large gap between its perfomance and the\nscores of the text model suggests that Flickr8K\nis rather small for the speech task. In Table 2\nwe present the results on the dataset of synthetic\nspeech from MS COCO. Here the text model is\nstill better, but the gap is much smaller than for\nFlickr8K. We attribute this to the much larger size\nof dataset, and to the less noisy and less variable\nsynthetic speech.\nWhile the MS COCO text model is overall bet-\nter than the speech model, there are cases where\nit outperforms the text model. We listed the top\nhundred cases where the ratio of the ranks of the\ncorrect image according to the two models was the\nsmallest, as well as another hundred cases where\nit was the largest. Manual inspection did not turn\n617\nup any obvious patterns for the cases of text be-\ning better than speech. For the cases where speech\noutperformed text, two patterns stood out: (i) sen-\ntences with spelling mistakes, (ii) unusually long\nsentences. For example for the sentence a yellow\nFigure 1: Images returned for utterance a yellow\nand white birtd is in ﬂight by the text (left) and\nspeech (right) models.\nand white birtd is in ﬂight the text model misses\nthe misspelled word birtd and returns an irrelevant\nimage, while the speech model seems robust to\nsome degree of variation in pronunciation and re-\nturns the target image at rank 1 (see Figure 1). In\nan attempt to quantify this effect we counted the\nnumber of unique words with training set frequen-\ncies below 5 in the top 100 utterances with lowest\nand highest rank ratio: for the utterances where\ntext was better there were 16 such words; for ut-\nterances where speech was better there were 28,\namong them misspellings such as streeet, scears\n(for skiers), contryside, scull, birtd, devise.\nThe distribution of utterance lengths in Fig-\nure 2 conﬁrms pattern (ii): the set of 100 sen-\ntences where speech beats text by a large margin\nare longer on average and there are extremely long\noutliers among them. One of them is the 36-word-\n●● ●●\nspeech\ntext\n10 20 30 40\nLength\nbetter\nFigure 2: Length distribution for sentences where\none model performs much better than the other.\nlong utterance depicted in Figure 3, with ranks 470\nand 2 for text and speech respectively. We suspect\nthat the speech model’s attention mechanism en-\nables it to cherry pick key fragments of such mon-\nster utterances, while the text model lacking this\nmechanism may struggle. Figure 3 shows the plot\nof the attention weights for this utterance from the\nspeech model.\n4.3 Predicting utterance length\nOur ﬁrst auxiliary task is to predict the length of\nthe utterance, using the features explained at the\nbeginning of Section 4. Since the length of an ut-\nterance directly corresponds to how long it takes to\narticulate, we also use the number of time steps5 as\na feature and expect it to provide the upper bound\nfor our task, especially for synthetic speech. We\nuse a Ridge Regression model for predicting utter-\nance length using each set of features. The model\nis trained on 80% of the sentences in the validation\nset, and tested on the remaining 20%. For all fea-\ntures regularization penalty α= 1.0 gave the best\nresults.\nFigure 4 shows the results for this task on hu-\nman speech from Flickr8K and synthetic speech\nfrom COCO. With the exception of the average in-\nput vectors for Flickr8K, all features can explain\na high proportion of variance in the predicted ut-\nterance length. The pattern observed for the two\ndatasets is slightly different: due to the systematic\nconversion of words to synthetic speech in COCO,\nusing the number of time steps for this dataset\nyields the highest R2. However, this feature is not\nas informative for predicting the utterance length\nin Flickr8K due to noise and variation in human\nspeech, and is in fact outperformed by some of the\nfeatures extracted from the model. Also, the input\nvectors from COCO are much more informative\nthan Flickr8K due to larger quantity and simpler\nstructure of the speech signal. However, in both\ndatasets the best (non-ceiling) performance is ob-\ntained by using average unit activations from the\nhidden layers (layer 2 for COCO, and layers 3 and\n4 for Flickr8K). These features outperform utter-\nance embeddings, which are optimized according\nto the visual grounding objective of the model and\nmost probably learn to ignore the superﬁcial char-\nacteristics of the utterance that do not contribute to\nmatching the corresponding image.\nNote that the performance on COCO plateaus\nafter the second layer, which might suggest that\nform-based knowledge is learned by lower layers.\nSince Flickr8K is much smaller in size, the stabil-\nising happens later in layer 3.\n5This is approximately duration in milliseconds\n10×stride .\n618\nFigure 3: Attention weight distribution for a long utterance.\nFigure 4: R2 values for predicting utterance length\nfor Flickr8K and COCO. Layers 1–5 represent\n(normalized) average unit activation, whereas the\nﬁrst (#0) and last point represent average input\nvectors and utterance embeddings, respectively.\n4.4 Predicting word presence\nResults from the previous experiment suggest that\nour model acquires information about higher level\nbuilding blocks (words) in the continuous speech\nsignal. Here we explore whether it can detect the\npresence or absence of individual words in an ut-\nterance. We formulate detecting a word in an ut-\nterance as a binary classiﬁcation task, for which\nwe use a multi-layer perceptron with a single hid-\nden layer of size 1024, optimized by Adam. The\ninput to the model is a concatenation of the fea-\nture vector representing an utterance and the one\nrepresenting a target word. We again use utter-\nance embeddings, average unit activations on each\nlayer, and average input vectors as features, and\nrepresent each target word as a vector of MFCC\nfeatures extracted from the audio signal syntheti-\ncally produced for that word.\nFor each utterance in the validation set, we ran-\ndomly pick one positive and one negative target\n(i.e., one word that does and one that does not ap-\npear in the utterance) that is not a stop word. To\nbalance the probability of a word being positive\nor negative, we use each positive target as a neg-\native target for another utterance in the validation\nset. The MLP model is trained on the positive and\nnegative examples corresponding to 80% of the ut-\nterances in the validation set of each dataset, and\nevaluated on the remaining 20%.\nFigure 5 shows the mean accuracy of the MLP\non Flickr8K and COCO. All results using features\nextracted from the model are above chance (0.5),\nwith the average unit activations of the hidden lay-\ners yielding the best results (0.65 for Flickr8K on\nlayer 3, and 0.79 for COCO on layer 4). These\nnumbers show that the speech model infers re-\nliable information about word-level blocks from\nthe low-level audio features it receives as input.\nThe observed trend is similar to the previous task:\naverage unit activations on the higher-level hid-\nden layers are more informative for this task than\nthe utterance embeddings, but the performance\nplateaus before the topmost layer.\nFigure 5: Mean accuracy values for predicting the\npresence of a word in an utterance for Flickr8K\nand COCO. Layers 1–5 represent the (normalized)\naverage unit activations, whereas the ﬁrst (#0) and\nlast point represent average input vectors and ut-\nterance embeddings, respectively.\n4.5 Sentence similarity\nNext we explore to what extent the model’s rep-\nresentations correspond to those of humans. We\nemploy the Sentences Involving Compositional\nKnowledge (SICK) dataset (Marelli et al., 2014).\nSICK consists of image descriptions taken from\n619\nFigure 6: Pearson’srof cosine similarities of aver-\naged input MFCCs and COCO Speech RHN hid-\nden layer activation vectors and embeddings of\nsentence pairs with relatedness scores from SICK,\ncosine similarity of COCO Text RHN embed-\ndings, and edit similarity.\nFlickr8K and video captions from the SemEval\n2012 STS MSRVideo Description data set (STS)\n(Agirre et al., 2012). Captions were paired at ran-\ndom, as well as modiﬁed to obtain semantically\nsimilar and contrasting counterparts, and the re-\nsulting pairs were rated for semantic similarity.\nFor all sentence pairs in SICK, we generate\nsynthetic spoken sentences and feed them to the\nCOCO Speech RHN, and calculate the cosine sim-\nilarity between the averaged MFCC input vectors,\nthe averaged hidden layer activation vectors, and\nthe sentence embeddings. Z-score transformation\nwas applied before calculating the cosine similar-\nities. We then correlate these cosine similarities\nwith\n• semantic relatedness according to human rat-\nings\n• cosine similarities according to z-score trans-\nformed embeddings from COCO Text RHN\n• edit similarities , a measure of how sim-\nilar the sentences are in form, speciﬁ-\ncally, 1−normalized Levenshtein distance\nover character sequences\nFigure 6 shows a boxplot over 10,000 bootstrap\nsamples for all correlations. We observe that (i)\ncorrelation with edit similarity initially increases,\nthen decreases; (ii) correlation with human re-\nlatedness scores and text model embeddings in-\ncreases until layer 4, but decreases for hidden layer\n5. The initially increasing and then decreasing cor-\nrelation with edit similarity is consistent with the\nﬁndings that information about form is encoded\nby lower layers. The overall growing correlation\nwith both human semantic similarity ratings and\n0.00\n0.25\n0.50\n0.75\n1.00\n0 2 4 6\nlayer\nRER\nwords\npeaking/peeking\ngreat/grate\nmantle/mantel\npeer/pier\ntale/tail\nwit/whit\nweight/wait\nisle/aisle\nsight/site\npic/pick\nsun/son\nwears/wares\npause/paws\ntied/tide\nware/wear\nsales/sails\nboarder/border\nplane/plain\nlapse/laps\nrose/rows\nstares/stairs\nseen/scene\nplains/planes\nsee/sea\nmain/mane\nrains/reins\ntea/tee\nstair/stare\nwaist/waste\nhole/whole\nsuite/sweet\npairs/pears\ncole/coal\nsale/sail\nlog(mincount)4 5 6 7\nFigure 7: Disambiguation performance per layer.\nPoints #0 and #6 (connected via dotted lines) rep-\nresent the input vectors and utterance embeddings,\nrespectively. The black line shows the overall\nmean RER.\nthe COCO Text RHN indicate that higher layers\nlearn to represent semantic knowledge. We were\nsomewhat surprised by the pattern for the correla-\ntion with human ratings and the Text model simi-\nlarities which drops for layer 5. We suspect it may\nbe caused by the model at this point in the layer\nhierarchy being strongly tuned to the speciﬁcs of\nthe COCO dataset. To test this, we checked the\ncorrelations with COCO Text embeddings on val-\nidation sentences from the COCO dataset instead\nof SICK. These increased monotonically, in sup-\nport of our conjecture.\n4.6 Homonym disambiguation\nNext we simulate the task of distinguishing be-\ntween pairs of homonyms, i.e. words with the\nsame acoustic form but different meaning. We\ngroup the words in the union of the training and\nvalidation data of the COCO dataset by their pho-\nnetic transcription. We then pick pairs of words\nwhich have the same pronunciation but different\nspelling, for example suite/sweet. We impose the\nfollowing conditions: (a) both forms appear more\nthan 20 times, (b) the two forms have different\nmeaning (i.e. they are not simply variant spellings\nlike theater/theatre), (c) neither form is a func-\ntion word, and (d) the more frequent form con-\nstitutes less than 95% of the occurrences. This\n620\ngives us 34 word pairs. For each pair we gener-\nate a binary classiﬁcation task by taking all the ut-\nterances where either form appears, using average\ninput vectors, utterance embeddings, and average\nunit activations as features. Instances for all fea-\nture sets are normalized to unit L2 norm.\nFor each task and feature set we run strati-\nﬁed 10-fold cross validation using Logistic Re-\ngression to predict which of the two words the\nutterance contains. Figure 7 shows, for each\npair, the relative error reduction of each feature\nset with respect to the majority baseline. There\nis substantial variation across word pairs, but\noverall the task becomes easier as the features\ncome from higher layers in the network. Some\nforms can be disambiguated with very high accu-\nracy (e.g. sale/sail, cole/coal, pairs/pears ), while\nsome others cannot be distinguished at all ( peak-\ning/peeking, great/grate, mantle/mantel). We ex-\namined the sentences containing the failing forms,\nand found out that almost all occurrences of peak-\ning and mantle were misspellings of peeking and\nmantel, which explains the impossibility of disam-\nbiguating these cases.\n5 Conclusion\nWe present a multi-layer recurrent highway net-\nwork model of language acquisition from visually\ngrounded speech signal. Through detailed analy-\nsis we uncover how information in the input sig-\nnal is transformed as it ﬂows through the network:\nformal aspects of language such as word identities\nthat not directly present in the input are discovered\nand encoded low in the layer hierarchy, while se-\nmantic information is most strongly expressed in\nthe topmost layers.\nGoing forward we would like to compare the\nrepresentations learned by our model to the brain\nactivity of people listening to speech in order to\ndetermine to what extent the patterns we found\ncorrespond to localized processing in the human\ncortex. This will hopefully lead to a better under-\nstanding of language learning and processing by\nboth artiﬁcial and neural networks.\nAcknowledgements\nWe would like to thank David Harwath for mak-\ning the Flickr8k Audio Caption Corpus publicly\navailable.\nReferences\nYossi Adi, Einat Kermany, Yonatan Belinkov, Ofer\nLavi, and Yoav Goldberg. 2016. Fine-grained anal-\nysis of sentence embeddings using auxiliary predic-\ntion tasks. arXiv preprint arXiv:1608.04207 .\nEneko Agirre, Mona Diab, Daniel Cer, and Aitor\nGonzalez-Agirre. 2012. Semeval-2012 task 6: A pi-\nlot on semantic textual similarity. In Proceedings of\nthe First Joint Conference on Lexical and Compu-\ntational Semantics . Association for Computational\nLinguistics, volume 2, pages 385–393.\nRaffaella Bernardi, Ruket Cakici, Desmond Elliott,\nAykut Erdem, Erkut Erdem, Nazli Ikizler-Cinbis,\nFrank Keller, Adrian Muscat, and Barbara Plank.\n2016. Automatic description generation from im-\nages: A survey of models, datasets, and evaluation\nmeasures. arXiv preprint arXiv:1601.03896 .\nGrzegorz Chrupała, Akos K ´ad´ar, and Afra Alishahi.\n2015. Learning language through pictures. In Pro-\nceedings of the 53rd Annual Meeting of the Associa-\ntion for Computational Linguistics.\nGrzegorz Chrupała, Lieke Gelderloos, and Afra\nAlishahi. 2017. Synthetically spoken COCO.\nhttps://doi.org/10.5281/zenodo.400926.\nAfsaneh Fazly, Afra Alishahi, and Suzanne Steven-\nson. 2010. A probabilistic computational model of\ncross-situational word learning. Cognitive Science:\nA Multidisciplinary Journal 34(6):1017–1063.\nMichael C. Frank, Noah D. Goodman, and Joshua B.\nTenenbaum. 2007. A Bayesian framework for cross-\nsituational word-learning. In Advances in Neural In-\nformation Processing Systems. volume 20.\nLieke Gelderloos and Grzegorz Chrupała. 2016. From\nphonemes to images: levels of representation in\na recurrent neural model of visually-grounded lan-\nguage learning. In Proceedings of COLING 2016,\nthe 26th International Conference on Computational\nLinguistics: Technical Papers.\nDavid Harwath and James Glass. 2015. Deep multi-\nmodal semantic embeddings for speech and images.\nIn IEEE Automatic Speech Recognition and Under-\nstanding Workshop.\nDavid Harwath and James R Glass. 2017. Learn-\ning word-like units from joint audio-visual analysis.\narXiv preprint arXiv:1701.07481 .\nDavid Harwath, Antonio Torralba, and James Glass.\n2016. Unsupervised learning of spoken language\nwith visual context. In Advances in Neural Infor-\nmation Processing Systems. pages 1858–1866.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2015. Deep residual learning for image recog-\nnition. arXiv:1512.03385 .\n621\nMicah Hodosh, Peter Young, and Julia Hockenmaier.\n2013. Framing image description as a ranking task:\nData, models and evaluation metrics. Journal of Ar-\ntiﬁcial Intelligence Research 47:853–899.\n´Akos K ´ad´ar, Grzegorz Chrupała, and Afra Alishahi.\n2016. Representation of linguistic form and\nfunction in recurrent neural networks. CoRR\nabs/1602.08952.\nAndrej Karpathy and Li Fei-Fei. 2015. Deep visual-\nsemantic alignments for generating image descrip-\ntions. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition . pages\n3128–3137.\nDiederik P. Kingma and Jimmy Ba. 2014. Adam:\nA method for stochastic optimization. CoRR\nabs/1412.6980.\nAngeliki Lazaridou, Grzegorz Chrupała, Raquel\nFern´andez, and Marco Baroni. 2016. Multimodal\nsemantic learning from child-directed input. In\nThe 15th Annual Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies.\nJiwei Li, Will Monroe, and Dan Jurafsky. 2016. Un-\nderstanding neural networks through representation\nerasure. arXiv preprint arXiv:1612.08220 .\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar,\nand C Lawrence Zitnick. 2014. Microsoft coco:\nCommon objects in context. In Computer Vision–\nECCV 2014, Springer, pages 740–755.\nMarco Marelli, Stefano Menini, Marco Baroni, Luisa\nBentivogli, Raffaella Bernardi, and Roberto Zam-\nparelli. 2014. A sick cure for the evaluation of com-\npositional distributional semantic models. In LREC.\npages 216–223.\nYajie Miao, Jinyu Li, Yongqiang Wang, Shi-Xiong\nZhang, and Yifan Gong. 2016. Simplifying long\nshort-term memory acoustic models for fast training\nand decoding. In IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP).\nIEEE, pages 2284–2288.\nAaron van den Oord, Nal Kalchbrenner, and Koray\nKavukcuoglu. 2016. Pixel recurrent neural net-\nworks. arXiv preprint arXiv:1601.06759 .\nDeb K Roy and Alex P Pentland. 2002. Learning words\nfrom sights and sounds: a computational model.\nCognitive Science 26(1):113 – 146.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,\nSanjeev Satheesh, Sean Ma, Zhiheng Huang, An-\ndrej Karpathy, Aditya Khosla, Michael Bernstein,\nAlexander C. Berg, and Li Fei-Fei. 2014. ImageNet\nLarge Scale Visual Recognition Challenge.\nKaren Simonyan and Andrew Zisserman. 2014. Very\ndeep convolutional networks for large-scale image\nrecognition. CoRR abs/1409.1556.\nJeffrey M. Siskind. 1996. A computational study of\ncross-situational techniques for learning word-to-\nmeaning mappings. Cognition 61(1-2):39–91.\nGabriel Synnaeve, Maarten Versteegh, and Emmanuel\nDupoux. 2014. Learning words from images and\nspeech. In NIPS Workshop on Learning Semantics,\nMontreal, Canada.\nZhiyuan Tang, Ying Shi, Dong Wang, Yang Feng,\nand Shiyue Zhang. 2016. Memory visualization for\ngated recurrent neural networks in speech recogni-\ntion. arXiv preprint arXiv:1609.08789 .\nIvan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel\nUrtasun. 2015. Order-embeddings of images and\nlanguage. arXiv preprint arXiv:1511.06361 .\nChen Yu and Dana H Ballard. 2004. A multimodal\nlearning interface for grounding spoken language in\nsensory perceptions. ACM Transactions on Applied\nPerception (TAP)1(1):57–80.\nBolei Zhou, Agata Lapedriza, Jianxiong Xiao, Anto-\nnio Torralba, and Aude Oliva. 2014. Learning deep\nfeatures for scene recognition using places database.\nIn Z. Ghahramani, M. Welling, C. Cortes, N. D.\nLawrence, and K. Q. Weinberger, editors, Advances\nin Neural Information Processing Systems 27 , Cur-\nran Associates, Inc., pages 487–495.\nJulian Georg Zilly, Rupesh Kumar Srivastava,\nJan Koutn ´ık, and J ¨urgen Schmidhuber. 2016.\nRecurrent highway networks. arXiv preprint\narXiv:1607.03474 .\n622",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7522540092468262
    },
    {
      "name": "Hierarchy",
      "score": 0.6214008927345276
    },
    {
      "name": "Perception",
      "score": 0.602988600730896
    },
    {
      "name": "Encoding (memory)",
      "score": 0.6003929376602173
    },
    {
      "name": "Meaning (existential)",
      "score": 0.5881355404853821
    },
    {
      "name": "Speech recognition",
      "score": 0.544836163520813
    },
    {
      "name": "SIGNAL (programming language)",
      "score": 0.5431787371635437
    },
    {
      "name": "Natural language processing",
      "score": 0.5380423069000244
    },
    {
      "name": "Joint (building)",
      "score": 0.5019180774688721
    },
    {
      "name": "Spoken language",
      "score": 0.498974084854126
    },
    {
      "name": "Space (punctuation)",
      "score": 0.48551106452941895
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46679526567459106
    },
    {
      "name": "Language model",
      "score": 0.44465896487236023
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.4238082468509674
    },
    {
      "name": "Linguistics",
      "score": 0.3854040503501892
    },
    {
      "name": "Psychology",
      "score": 0.1465139091014862
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Market economy",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    }
  ]
}