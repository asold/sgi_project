{
    "title": "Tuning Multilingual Transformers for Language-Specific Named Entity Recognition",
    "url": "https://openalex.org/W2973071945",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A2074186000",
            "name": "Mikhail Arkhipov",
            "affiliations": [
                "Moscow State University",
                "Lomonosov Moscow State University",
                "Moscow Institute of Physics and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2094572716",
            "name": "Maria Trofimova",
            "affiliations": [
                "Moscow State University",
                "Lomonosov Moscow State University",
                "Moscow Institute of Physics and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2803864553",
            "name": "Yuri Kuratov",
            "affiliations": [
                "Moscow Institute of Physics and Technology",
                "Lomonosov Moscow State University",
                "Moscow State University"
            ]
        },
        {
            "id": "https://openalex.org/A2289912120",
            "name": "Alexey Sorokin",
            "affiliations": [
                "Lomonosov Moscow State University",
                "Moscow Institute of Physics and Technology",
                "Moscow State University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2296283641",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2952729433",
        "https://openalex.org/W2953320089",
        "https://openalex.org/W1572167796",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2952230511",
        "https://openalex.org/W2143017621",
        "https://openalex.org/W2251163391",
        "https://openalex.org/W2158899491",
        "https://openalex.org/W2170973209",
        "https://openalex.org/W2739922228",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W2952087486",
        "https://openalex.org/W2493916176",
        "https://openalex.org/W2574640638",
        "https://openalex.org/W2963724887",
        "https://openalex.org/W2527945540"
    ],
    "abstract": "Our paper addresses the problem of multilingual named entity recognition on the material of 4 languages: Russian, Bulgarian, Czech and Polish. We solve this task using the BERT model. We use a hundred languages multilingual model as base for transfer to the mentioned Slavic languages. Unsupervised pre-training of the BERT model on these 4 languages allows to significantly outperform baseline neural approaches and multilingual BERT. Additional improvement is achieved by extending BERT with a word-level CRF layer. Our system was submitted to BSNLP 2019 Shared Task on Multilingual Named Entity Recognition and demonstrated top performance in multilingual setting for two competition metrics. We open-sourced NER models and BERT model pre-trained on the four Slavic languages.",
    "full_text": "Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing, pages 89–93,\nFlorence, Italy, 2 August 2019.c⃝2019 Association for Computational Linguistics\n89\nTuning Multilingual Transformers for Named Entity Recognition on\nSlavic Languages\nMikhail Arkhipov*,1 Maria Troﬁmova*,2 Yuri Kuratov*,3 Alexey Sorokin*,⋄,4\n*Neural Networks and Deep Learning Laboratory, Moscow Institute of Physics and Technology\n⋄Faculty of Mathematics and Mechanics, Moscow State University\n1arkhipov.mu@mipt.ru\n2mary.vikhreva@gmail.com\n3yurii.kuratov@phystech.edu\n4alexey.sorokin@list.ru\nAbstract\nOur paper addresses the problem of multilin-\ngual named entity recognition on the mate-\nrial of 4 languages: Russian, Bulgarian, Czech\nand Polish. We solve this task using the\nBERT model. We use a hundred languages\nmultilingual model as base for transfer to the\nmentioned Slavic languages. Unsupervised\npre-training of the BERT model on these 4\nlanguages allows to signiﬁcantly outperform\nbaseline neural approaches and multilingual\nBERT. Additional improvement is achieved by\nextending BERT with a word-level CRF layer.\nOur system was submitted to BSNLP 2019\nShared Task on Multilingual Named Entity\nRecognition and took the1st place in3 compe-\ntition metrics out of 4 we participated in. We\nopen-sourced NER models and BERT model\npre-trained on the four Slavic languages.\n1 Introduction\nNamed Entity Recognition (further, NER) is a task\nof recognizing named entities in running text, as\nwell as detecting their type. For example, in the\nsentence Asia Bibi is from Pakistan, the following\nNER classes can be detected: [ Asia Bibi]PER is\nfrom [Pakistan]LOC . The commonly used BIO-\nannotation for this sentence is shown in Figure 1.\nThe recognizer of named entities can be trained\non a single target task dataset as any other se-\nquence tagging model. However, it often beneﬁts\nfrom additional data from a different source, either\nlabeled or unlabeled, which is known as transfer\nlearning. To enrich the model one can either train\nit on several tasks simultaneously (Collobert et al.,\n2011), which makes its word representations more\nﬂexible and robust, or pretrain on large amounts of\nunlabeled data to utilize unlimited sources avail-\nable in the Web and then ﬁne-tune them on a spe-\nciﬁc task (Dai and Le, 2015; Howard and Ruder,\n2018).\nOne of the most powerful unsupervised mod-\nels is BERT (Devlin et al., 2018), which is a\nmulti-layer Transformer trained on the objective\nof masked words recovery and on the task of next\nsentence prediction (known also as Natural Lan-\nguage Inference (NLI) task). The original model\nwas trained on vast amounts of data for more\nthan 104 languages which makes its representa-\ntions useful for almost any task. Our contribu-\ntion is three-fold: ﬁrst, multilingual BERT embed-\ndings with a dense layer on the top clearly beat\nBiLSTM-CRF over FastText embeddings trained\non the four target languages. Second, language-\nspeciﬁc BERT, trained only on the target lan-\nguages from Wikipedia and news dump, signiﬁ-\ncantly outperforms the multilingual BERT. Third,\nwe adapt a CRF layer as a a top module over the\noutputs of the BERT-based model and demonstrate\nthat it improves performance even further.\n2 Model Architecture\nOur model extends the recently introduced BERT\n(Devlin et al., 2018) model. BERT itself is a mul-\ntilayer transformer (Vaswani et al., 2017) which\ntakes as input a sequence of subtokens, obtained\nusing WordPiece tokenization (Wu et al., 2016),\nand produces a sequence of context-based embed-\ndings of these subtokens. When a word-level task,\nsuch as NER, is being solved, the embeddings of\nword-initial subtokens are passed through a dense\nlayer with softmax activation to produce a proba-\nbility distribution over output labels. We refer the\nreader to the original paper, see also Figure 2.\nWe modify BERT by adding a CRF layer in-\nstead of the dense one, which was commonly used\nin other works on neural sequence labeling (Lam-\nple et al., 2016) to ensure output consistency. It\nalso transforms a sequence of word-initial subto-\nken embeddings to a sequence of probability dis-\n90\nAsia Bibi is from Pakistan .\nB-PER I-PER O O B-LOC O\nFigure 1: An example of BIO-annotation for tokens.\ntributions, however, each prediction depends not\nonly on the current input, but also from the previ-\nous one.\n3 Transfer from Multilingual Language\nModel\nThere are two basic options for building multilin-\ngual system: to train a separate model for each lan-\nguage or to use a single multilingual model for all\nlanguages. We follow the second approach since it\nenriches the model with the data from related lan-\nguages, which was shown to be beneﬁcial in recent\nstudies (Mulcaire et al., 2018).\nThe original BERT embedder itself is essen-\ntially multilingual since it was trained on 104 lan-\nguages with largest Wikipedias 1. However, for\nour four Slavic languages (Polish, Czech, Rus-\nsian, and Bulgarian) we do not need the full in-\nventory of multilingual subtokens. Moreover, the\noriginal WordPiece tokenization may lack Slavic-\nspeciﬁc ngrams, which makes the input sequence\nlonger and the training process more problematic\nand computationally expensive.\nHence we retrain the Slavic BERT on stratiﬁed\nWikipedia data for Czech, Polish and Bulgarian\nand News data for Russian. Our main innova-\ntion is the training procedure: training BERT from\nscratch is extremely expensive computationally so\nwe initialize our model with the multilingual one.\nWe rebuild the vocabulary of subword tokens us-\ning subword-nmt2. When a single Slavic subto-\nken may consist of multiple multilingual subto-\nkens, we initilalize it as an average of their vectors,\nresembling (Bojanowski et al., 2016). All weights\nof transformer layers are initialized using the mul-\ntilingual weights.\n4 Experiment Details\n4.1 Target Task and Dataset\nThe 2019 edition of the Balto-Slavic Natural\nLanguage Processing (BSNLP) (Piskorski et al.,\n1https://github.com/google-research/\nbert\n2https://github.com/rsennrich/\nsubword-nmt\n2019) shared task aims at recognizing mentions\nof named entities in web documents in Slavic lan-\nguages. The input text collection consists of sets\nof news articles from online media, each collec-\ntion revolving around a certain entity or an event.\nThe corpus was obtained by crawling the web and\nparsing the HTML of relevant documents. The\n2019 edition of the shared task covers4 languages\n(Bulgarian, Czech, Polish, Russian) and focuses\non recognition of ﬁve types of named entities in-\ncluding persons (PER), locations (LOC), organiza-\ntions (ORG), events (EVT) and products (PRO).\nThe dataset consists of pairs of ﬁles: news text\nand a ﬁle with mentions of entities with corre-\nsponding tags. There are two groups of documents\nin the train part of the dataset. Namely, news about\nAsia Bibi and Brexit. Brexit part is substantially\nbigger, therefore, we used it for training and Asia\nBibi for validation.\n4.2 Pre- and Post-processing\nWe use NLTK (Loper and Bird, 2002) sentence to-\nkenizers for Bulgarian, Polish, and Czech. Due\nto the absence of Bulgarian sentence tokenizer we\napply the English NLTK one instead. For Russian\nlanguage we use DeepMIPT sentence tokenizer 3.\nWe replace all UTF separators and space char-\nacters with regular spaces. Due to mismatch of\nBSNLP 2019 data format and common format for\ntagging tasks we ﬁrst convert the dataset to BIO\nformat to obtain training data. After getting pre-\ndictions in BIO format we transform them back\nto the labeling scheme proposed by Shared Task\norganizers. This step probably causes extra errors,\nso we partially correct them using post-processing.\nWe found that sometimes the model predicts\na single opening quote without closing one. So\nwe ﬁlter out all single quotation marks in the pre-\ndicted entities. At the prediction stage we perform\ninference for a sliding window of two sentences\nwith overlaps to reduce sentence tokenization er-\nrors.\nThe Shared Task also included the entity nor-\nmalization subtask: for example, the phrase\n3https://github.com/deepmipt/ru_\nsentence_tokenizer\n91\nFigure 2: In the ﬁgure, Es and Rs represent the input embedding and the contextual representation of subtoken\ns, [CLS] is the special symbol to get full input representation, and [SEP ] is the special symbol to separate non-\nconsecutive token sequences.\n“Верховным судом Пакистана” (Supreme+Ins\nCourt+Ins of Pakistan+Gen) should be “ Верхов-\nный суд Пакистана”. We used the UDPipe 2.3\n(Straka et al., 2016) lemmatizers whose output\nwas corrected using language-speciﬁc rules. For\nexample, “ Пакистана” (Pakistan+Gen) should\nnot be lemmatized because in Russian noun mod-\niﬁers remain in Genitive.\n4.3 Model Parameters\nSee below parameters of transferring multilingual\nBERT from to Slavic languages. The training\ntook 9 days with DGX-1 comprising of eight P-\n100 16Gb GPUs. We train BERT in two stages:\ntrain full BERT on sequences with 128 subtokens\nlength and then train only positional embeddings\non 512 length sequences. We found that both ini-\ntialization from multilingual BERT and reassem-\nbling of embeddings speed up convergence of the\nmodel.\n• Batch size: 256\n• Learning rate: 2e-5\n• Iterations of full BERT training: 1M\n• Iterations of positional embeddings train-\ning: 300k\nParameters of all BERT-based NER models are:\n• Batch size: 16\n• BERT layers learning rate: 1e-5\n• Top layers learning rate: 3e-4\n• Optimizer: AdamOptimizer\n• Epochs: 3\nIn contrast to original BERT paper (Devlin\net al., 2018), we use different learning rates for\nthe task-speciﬁc top layers and BERT layers when\ntraining BERT-based NER models. We found that\nthis modiﬁcation leads to faster convergence and\nhigher scores.\nWe evaluate the model every 10 batches on the\nwhole validation set and chose the one that per-\nformed best on it. Despite this strategy being very\ntime consuming, we found it crucial to get extra\ncouple of points. For all experiments we used the\nspan F1 score for validation.\nOur best model used CRF layer and performed\nmoving averages of variables by employing an ex-\nponential decay to model parameters.\n5 Results\nWe evaluated Slavic BERT NER model on the\nBSNLP 2019 Shared Task dataset. The model\nis compared with two baselines: Bi-LSTM-CRF\n(Lample et al., 2016) and NER model based on\nmultilingual BERT. For Bi-LSTM-CRF we use\nFastText word embeddings trained on the same\ndata as Slavic BERT.\nTable 1 presents the scores of our model on de-\nvelopment set (Asia Bibi documents) when train-\ning on Brexit documents. We report a standard\nspan-level F1-score based on the CONLL-2003\nevaluation script (Sang and De Meulder, 2003)\nand three ofﬁcial evaluation metrics(Piskorski\net al., 2019) 4: Relaxed Partial Matching (RPM),\nRelaxed Exact Matching (REM), and Strict\n4http://bsnlp.cs.helsinki.fi/\nBSNLP-NER-Evaluator-19.0.1.zip\n92\nMatching (SM). Our system showed top perfor-\nmance in multilingual setting for all mentioned\nmetrics except RPM.\nEven without CRF the multilingual BERT\nmodel signiﬁcantly outperforms Bi-LSTM-CRF\nmodel. Adding a CRF layer strongly increases\nperformance both for multilingual and Slavic\nBERT models. Slavic BERT is the top perform-\ning model. The error rate of Slavic BERT-CRF is\nmore than one third less than the one of Multilin-\ngual BERT baseline.\nWe experimented with transfer learning from\nother NER corpora. We used three corpora\nas source for transfer: Russian NER corpus\n(Mozharova and Loukachevitch, 2016), Bulgar-\nian BulTreeBank (Simov et al., 2004; Georgiev\net al., 2009), and BSNLP 2017 Shared Task\ndataset(Piskorski et al., 2017) 6 with Czech, Rus-\nsian, and Polish data. For pre-training we use\nstratiﬁed sample from the concatenated dataset.\nThe set of tags for the task-speciﬁc layer includes\nall tags that occur in at least one dataset. Af-\nter pre-training we replace the task-speciﬁc layer\nwith the one suited for the BSNLP 2019 dataset\nand train until convergence. We ﬁnd this approach\nto be beneﬁcial for models without CRF, however,\nthe CRF-enhanced model without NER pretrain-\ning demonstrates slightly higher scores.\nTable 2 presents a detailed evaluation report\nacross 4 languages for the top performing Slavic\nBERT-CRF model. Note that the languages\nwith Latin script (Polish and Czech) demonstrate\nhigher scores than Cyrillic-based ones (Russian\nand Bulgarian). Low scores for Russian might be\ncaused by the dataset imbalance, since it covers\nonly 7.7% of the whole BSNLP dataset, however,\nBulgarian includes 39% but shows even lower\nquality, especially in terms of recall. We have two\nexplanations: ﬁrst, incorrect sentence tokenization\nsince we used English sentence tokenizer for Bul-\ngarian (this may explain the skew towards preci-\nsion). Second, Russian and Bulgarian are much\nless related than Czech and Polish so they obtain\nless gain from having additional multilingual data.\n5.1 Releasing the Models\nWe release the best BERT based NER model along\nwith the BERT model pre-trained on the four com-\n6http://bsnlp-2017.cs.helsinki.fi/\nshared_task.html\npetition languages7. We provide the code for the\ninference of our NER model as well as for using\nthe pretrained BERT. The BERT model is fully\ncompatible with original BERT repository.\n6 Conclusion\nWe have established that BERT models pre-\ntrained on task-speciﬁc languages and initialized\nusing the multilingual model, signiﬁcantly outper-\nform multilingual baselines on the task of Named\nEntity Recognition. We also demonstrate that\nadding a word-level CRF layer on the top im-\nproves the quality of both extended models. We\nhope our approach will be useful to ﬁne-tune\nlanguage-speciﬁc BERTs not only for Named En-\ntity Recognition but for other NLP tasks as well.\nAcknowledgements\nThe research was conducted under support\nof National Technological Initiative Foundation\nand Sberbank of Russia. Project identiﬁer\n0000000007417F630002.\nReferences\nPiotr Bojanowski, Edouard Grave, Armand Joulin,\nand Tomas Mikolov. 2016. Enriching word vec-\ntors with subword information. arXiv preprint\narXiv:1607.04606.\nRonan Collobert, Jason Weston, Léon Bottou, Michael\nKarlen, Koray Kavukcuoglu, and Pavel Kuksa.\n2011. Natural language processing (almost) from\nscratch. Journal of machine learning research,\n12(Aug):2493–2537.\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079–3087.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nGeorgi Georgiev, Preslav Nakov, Kuzman Ganchev,\nPetya Osenova, and Kiril Ivanov Simov. 2009.\nFeature-rich named entity recognition for Bulgarian\nusing conditional random ﬁelds. In RANLP.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation.\narXiv preprint arXiv:1801.06146.\n7https://github.com/deepmipt/\nSlavic-BERT-NER\n93\nModel Span F1 RPM REM SM\nBi-LSTM-CRF (Lample et al., 2016) 75.8 73.9 72.1 72.3\nMultilingual BERT 5 79.6 77.8 76.1 77.2\nMultilingual BERT-CRF 81.4 80.9 79.2 79.6\nSlavic BERT 83.5 83.8 82.0 82.2\nSlavic BERT-CRF 87.9 85.7 (90.9) 84.3 (86.4) 84.1 (85.7)\nTable 1: Metrics for BSNLP on validation set (Asia Bibi documents). Metrics on the test set are in the brackets.\nLanguage P R F1\ncs 93.6 94.7 93.9\nru 88.2 86.6 87.3\nbg 90.3 84.3 87.2\npl 93.4 93.1 93.2\nTable 2: Precision (P), Recall (R), and F1 RPM scores\nof Slavic BERT-CRF model for Czech (cs), Russian\n(ru), Bulgarian (bg) and Polish (pl) languages.\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\nNeural architectures for named entity recognition.\narXiv preprint arXiv:1603.01360.\nEdward Loper and Steven Bird. 2002. NLTK: the nat-\nural language toolkit. arXiv preprint cs/0205028.\nValerie Mozharova and Natalia Loukachevitch. 2016.\nTwo-stage approach in Russian named entity recog-\nnition. In 2016 International FRUCT Confer-\nence on Intelligence, Social Media and Web (ISMW\nFRUCT), pages 1–6. IEEE.\nPhoebe Mulcaire, Swabha Swayamdipta, and Noah\nSmith. 2018. Polyglot semantic role labeling. arXiv\npreprint arXiv:1805.11598.\nJakub Piskorski, Laska Laskova, Michał Marci ´nczuk,\nLidia Pivovarova, Pavel P ˇribáˇn, Josef Steinberger,\nand Roman Yangarber. 2019. The second cross-\nlingual challenge on recognition, classiﬁcation,\nlemmatization, and linking of named entities across\nSlavic languages. In Proceedings of the 7th Work-\nshop on Balto-Slavic Natural Language Processing,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nJakub Piskorski, Lidia Pivovarova, Jan Šnajder, Josef\nSteinberger, and Roman Yangarber. 2017. The ﬁrst\ncross-lingual challenge on recognition, normaliza-\ntion, and matching of named entities in Slavic lan-\nguages. In Proceedings of the 6th Workshop on\nBalto-Slavic Natural Language Processing, pages\n76–85, Valencia, Spain. Association for Computa-\ntional Linguistics.\nErik F Sang and Fien De Meulder. 2003. Introduc-\ntion to the CoNLL-2003 shared task: Language-\nindependent named entity recognition. arXiv\npreprint cs/0306050.\nKiril Simov, Petya Osenova, Sia Kolkovska, Elisaveta\nBalabanova, and Dimitar Doikoff. 2004. A language\nresources infrastructure for Bulgarian. In Proceed-\nings of the Fourth International Conference on Lan-\nguage Resources and Evaluation (LREC’04), Lis-\nbon, Portugal. European Language Resources Asso-\nciation (ELRA).\nMilan Straka, Jan Hajic, and Jana Straková. 2016. UD-\nPipe: Trainable pipeline for processing CoNLL-U\nﬁles performing tokenization, morphological analy-\nsis, POS tagging and parsing. In LREC.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin\nJohnson, Xiaobing Liu, Lukasz Kaiser, Stephan\nGouws, Yoshikiyo Kato, Taku Kudo, Hideto\nKazawa, Keith Stevens, George Kurian, Nishant\nPatil, Wei Wang, Cliff Young, Jason Smith, Jason\nRiesa, Alex Rudnick, Oriol Vinyals, Greg Corrado,\nMacduff Hughes, and Jeffrey Dean. 2016. Google’s\nneural machine translation system: Bridging the gap\nbetween human and machine translation. CoRR,\nabs/1609.08144."
}