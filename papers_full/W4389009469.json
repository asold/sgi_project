{
  "title": "Math Word Problem Generation with Multilingual Language Models",
  "url": "https://openalex.org/W4389009469",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5093339389",
      "name": "Kashyapa Niyarepola",
      "affiliations": [
        "University of Moratuwa"
      ]
    },
    {
      "id": "https://openalex.org/A5093339390",
      "name": "Dineth Athapaththu",
      "affiliations": [
        "University of Moratuwa"
      ]
    },
    {
      "id": "https://openalex.org/A5113054554",
      "name": "Savindu Ekanayake",
      "affiliations": [
        "University of Moratuwa"
      ]
    },
    {
      "id": "https://openalex.org/A5002889503",
      "name": "Surangika Ranathunga",
      "affiliations": [
        "University of Moratuwa"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4285296929",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3186655327",
    "https://openalex.org/W2996621388",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W2402499785",
    "https://openalex.org/W2988647680",
    "https://openalex.org/W3173585224",
    "https://openalex.org/W3092766609",
    "https://openalex.org/W3167303745",
    "https://openalex.org/W3176929804",
    "https://openalex.org/W2396145442",
    "https://openalex.org/W2512924740",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W3131933120",
    "https://openalex.org/W3095551580",
    "https://openalex.org/W3100198908",
    "https://openalex.org/W4229543565",
    "https://openalex.org/W3200127809",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3196234484",
    "https://openalex.org/W4287694131",
    "https://openalex.org/W1977298367",
    "https://openalex.org/W2532931541",
    "https://openalex.org/W2578193051",
    "https://openalex.org/W3128902667",
    "https://openalex.org/W3031261421",
    "https://openalex.org/W2615346089",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3058517477",
    "https://openalex.org/W3015298228",
    "https://openalex.org/W4221151728",
    "https://openalex.org/W2043688868",
    "https://openalex.org/W3081826137",
    "https://openalex.org/W2415830263",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W3198965934",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2513499049",
    "https://openalex.org/W2787623698",
    "https://openalex.org/W3203998766",
    "https://openalex.org/W2962970011"
  ],
  "abstract": "Auto regressive text generation for lowresource languages, particularly the option of using pre-trained language models, is a relatively under-explored problem.In this paper, we model Math Word Problem (MWP) generation as an auto-regressive text generation problem.We evaluate the pre-trained sequence-tosequence language models (mBART and mT5) in the context of two low-resource languages, Sinhala and Tamil, as well as English.For the evaluation, we create a multi-way parallel MWP dataset for the considered languages.Our empirical evaluation analyses how the performance of the pre-trained models is affected by the (1) amount of language data used during pre-training, (2) amount of data used in finetuning, (3) input seed length and (4) context differences in MWPs.Our results reveal that the considered pre-trained models are capable of generating meaningful MWPs even for the languages under-represented in these models, even though the amount of fine-tuning data and seed length are small.Our human evaluation shows that a Mathematics tutor can edit a generation question fairly easily, thus highlighting the practical utility ofautomatically generating MWPs.",
  "full_text": "Proceedings of the 15th International Conference on Natural Language Generation, pages 144 - 155\nJuly 18-22, 2022c⃝2022 Association for Computational Linguistics\nMath Word Problem Generation with Multilingual Language Models\nKashyapa Niyarepola,Dineth Athapaththu,Savindu Ekanayake,Surangika Ranathunga\nDepartment of Computer Science and Engineering, University of Moratuwa\nKatubedda 10400, Sri Lanka\n[kashyapabandara.17, dinethnaradaam.17,\nsavinduekanayake.17, surangika]@cse.mrt.ac.lk\nAbstract\nAuto regressive text generation for low-\nresource languages, particularly the option of\nusing pre-trained language models, is a rela-\ntively under-explored problem. In this paper,\nwe model Math Word Problem (MWP) genera-\ntion as an auto-regressive text generation prob-\nlem. We evaluate the pre-trained sequence-to-\nsequence language models (mBART and mT5)\nin the context of two low-resource languages,\nSinhala and Tamil, as well as English. For\nthe evaluation, we create a multi-way paral-\nlel MWP dataset for the considered languages.\nOur empirical evaluation analyses how the per-\nformance of the pre-trained models is affected\nby the (1) amount of language data used during\npre-training, (2) amount of data used in fine-\ntuning, (3) input seed length and (4) context\ndifferences in MWPs. Our results reveal that\nthe considered pre-trained models are capable\nof generating meaningful MWPs even for the\nlanguages under-represented in these models,\neven though the amount of fine-tuning data and\nseed length are small. Our human evaluation\nshows that a Mathematics tutor can edit a gen-\neration question fairly easily, thus highlighting\nthe practical utility ofautomatically generating\nMWPs.\n1 Introduction\nDespite being one of the most important sub-\njects, many school children find Mathematics diffi-\ncult (Acharya, 2017), with many exams reporting\nhigh failure rates in Mathematics (Rylands and\nCoady, 2009). One way of improving Mathemat-\nics skills is to practice solving Mathematics prob-\nlems (Thompson, 1985). However, this places extra\nburden on the tutors - they have to create different\nMathematics questions and grade student answers.\nThe alternative is to automatically generate Mathe-\nmatics questions and grade student answers. The\nneed of such systems that support as many lan-\nguages as possible, is even more pronounced dur-\ning the times of pandemics and war, where students\nget confined to homes/shelters without access to\nphysical schools.\nIn this paper, we focus on the problem of auto-\nmatically generating Mathematical Word problems\n(MWPs). Considering the fact that learning Math-\nematics is not a privilege to students speaking a\nparticular language, we want to investigate the pos-\nsibility of MWP generation in multiple languages.\nAn MWP is a “narrative with a specific topic that\nprovides clues to the correct equation with numer-\nical quantities and variables therein” (Zhou and\nHuang, 2019). MWPs can be in categories such\nas algebra, geometry and statistics. An elementary\nMWP written in English is shown in the below\nexample.\nRosy made cookies and she used 2 kg flour and\n1.5 kg sugar. How much more flour than sugar did\nRosy use?\nEarly solutions to MWP generation relied on\ntemplate-based approaches (Polozov et al., 2015),\nand question rewriting (Koncel-Kedziorski et al.,\n2016). More recently, Recurrent Neural Networks\n(RNN) (Zhou and Huang, 2019; Liyanage and\nRanathunga, 2020), fine-tuning pre-trained lan-\nguage models (Wang et al., 2021) as well as Varia-\ntional Autoencoders (V AE) (Liu et al., 2020; Cao\net al., 2021) have been proposed. Only Liyanage\nand Ranathunga (2020) have tried their NN solu-\ntion in a multilingual setting, however the results\nare sub-optimal.\nThus, our objective is to investigate the use of\nmultilingual pre-trained models for MWP genera-\ntion. Here, we treat MWP generation as an auto-\nregressive problem - the system has to generate a\nquestion starting with the provided seed (the start-\ning portion of the question that is expected to be\ngenerated). Compared to text generation tasks such\nas story generation (Roemmele, 2016) or news gen-\neration (Leppänen et al., 2017), MWP generation\nis challenging because MWPs have mathematical\nconstraints, units and numerical values as shown in\n144\nthe above example.\nAs mentioned above, auto-regressive language\nmodels such as GPT-x (Radford et al., 2019) have\nbeen already used for MWP generation (Wang\net al., 2021). They are a common choice for\nNatural Language Generation (NLG) tasks (Lee\nand Hsiang, 2020; Mosallanezhad et al., 2020;\nBudzianowski and Vuli ´c, 2019). Sequence-to-\nsequence models such as BART (Lewis et al., 2019)\nand T5 (Raffel et al., 2019) have also been used\nfor NLG in an auto-regressive manner (Tan et al.,\n2020; Lewis et al., 2020). However, this option has\nbeen used to a lesser extent compared to GPT-x in\nsimilar text generation tasks, and never for MWP\ngeneration.\nDespite their success on English text generation,\nGPT-x models are not available for other languages.\nBuilding multilingual or language-specific GPT\nmodels is not practical for many languages, par-\nticularly the low-resource ones. In contrast, T5\nand BART both have their multilingual versions:\nmT5 (Xue et al., 2020) and mBART (Tang et al.,\n2020) (respectively). We are only aware of the em-\npirical analysis of Chen et al. (2021), who tested\nthe auto-regressive text generation capabilities of\nmT5 and mBART in the context of 4 high resource\nlanguages (for four tasks: story, question and title\ngeneration).\nWe carry out an empirical study on the mBART\nand mT5 models for MWP generation, consider-\ning two low-resource languages Sinhala and Tamil,\nalong with English. All these languages are in-\ncluded in mBART and mT5. For a more compre-\nhensive analysis, we evaluate T5, BART and GPT-2\nfor English MWP generation as well. Our experi-\nments answer four important questions:\n1. How the performance of mT5 and mBART\nvaries depending on the language - because,\nfor the related Machine Translation task, it has\nbeen shown that the model performance on\nindividual languages depends on the amount\nof language-specific data used during model\npre-training (Lee et al., 2022)\n2. How the performance of the models varies\ndepending on the amount of fine-tuning data\n- because for many languages, having a large\ntraining set is not realistic\n3. How much information (size of the seed)\nshould be provided to the model at the in-\nference stage for it to generate a meaningful\nMWP - because a tutor should be able to gen-\nerate a new MWP by providing minimal infor-\nmation.\n4. How the context of an MWP affects the gen-\neration performance - because there is a wide\nvariety of MWPs\nAs an additional contribution, we create a bench-\nmark dataset by extending the dataset created by\nLiyanage and Ranathunga (2020) for MWP gener-\nation. Each English question was manually trans-\nlated to Sinhala and Tamil, creating a multi-way\nparallel dataset. Our dataset is publicly released1,\nand can be considered as a test set even for Machine\nTranslation.\nWe believe that our work is the first to conduct an\nempirical analysis on the use of (1) GPT, BART, T5,\nmBART and mT5 for auto-regressive generation of\nMWPs and (2) mBART and mT5 for the general\ntask of auto-regressive text generation considering\nlow-resource languages. Our findings are indeed\nvery promising for low-resource languages. Even\nfor very small seeds and fine-tuning dataset sizes,\nthese models (mBART in particular) yield very\ngood results with very little grammar and spelling\nerrors. Thus we can present the use of these models\nas a very promising avenue for auto-regressive text\ngeneration for low-resource languages, at least for\nthose that are included in the pre-trained models.\n2 Related Work\n2.1 MWP Generation\nPrevious research has addressed the problem of\nMWP generation using three main techniques:\nquestion rewriting, template-based generation and\ntext generation with Neural Networks (NNs).\nQuestion rewriting technique rewrites a human-\nwritten question by replacing words with new ones\nfrom different contexts (Koncel-Kedziorski et al.,\n2016). However, the numerical values in all the\nrewritten questions are the same.\nIn the template-based techniques, first a question\ntemplate is either provided by a tutor (Nandhini and\nBalasundaram, 2011; Polozov et al., 2015; Wang\nand Su, 2016), or generated from an MWP (Bekele,\n2020). Most of these template-based techniques are\nlong and tedious processes, with some requiring\nlanguage specific tools or resources.\n1https://huggingface.co/datasets/\nNLPC-UOM/MWP_Dataset\n145\nZhou and Huang (2019) present a Deep Neural\nNetwork model that has two encoders and one de-\ncoder, all based on RNNs. The equation encoder\ntakes in an equation template, and the topic encoder\ntakes in a topic (context). The system is trained in\na supervised manner, using an MWP dataset. Thus,\nfor training purposes, the equation and the topic\nof each training MWP has to be extracted. Wang\net al. (2021) also take in an equation and context,\nhowever MWP generation is done using GPT-2.\nAdditionally, they introduce constraints to satisfy\nequation and context correctness. Liu et al. (2020)\nalso take in an equation as the input. However,\nthey expect an external knowledge graph to repre-\nsent the context. Both the knowledge graph and\nthe equation are encoded using a Convolutional\nGated Neural Network model. A Variational Auto-\nEncoder (V AE) is used to generate the MWP from\nthis encoding. Cao et al. (2021) also make use of\na V AE to bridge the gap between abstract math\ntokens and text. In addition to the equation and\ncommon sense knowledge graph as input, they take\nin the question text, as well as a set of words repre-\nsenting a topic.\nIn contrast to above research, Liyanage and\nRanathunga (2019, 2020) train a single RNN en-\ncoder in an auto-regressive manner using the MWP\ntext. Liyanage and Ranathunga (2019) impose\nMathematical constraints during post processing,\nwhile Liyanage and Ranathunga (2020) achieve\nthe same using POS embeddings as input to the\nmodel. As for NN-based solutions, only Liyanage\nand Ranathunga (2019, 2020) considered MWP\ngeneration in languages other than English.\n2.2 Bench-marking NLG with Pre-trained\nModels\nNLG is an umbrella term used for a set of tasks\nwhere the objective is to generate a text as the out-\nput. In addition to auto regressive text generation,\nNLG covers tasks such as text summarization, text\nsimplification, and graph to text generation.\nThe GEM benchmark (Gehrmann et al., 2021) eval-\nuates BART, T5, mBART and mT5 for 11 different\nNLG tasks. However, there is no evaluation on an\nauto regressive text generation task. Moreover, ex-\ncept for one dataset, all the others are focused only\non high-resource languages. The GLGE bench-\nmark (Liu et al., 2021), which evaluated BART\nand MASS pre-trained models also does not have a\ndataset for auto regressive text generation. Further,\nevaluation is only done for English.\nSeveral shared tasks have been organized for\nmultilingual NLG tasks such as surface realiza-\ntion (Mille et al., 2020) and RDF triples to text (Fer-\nreira et al., 2020). Submissions to these shared\ntasks have experimented with various pre-trained\nmodels. However, the datasets focus only on high\nand mid-resource languages. In contrast to the\nabove datasets, Kumar et al. (2022)’s multilingual\nNLG dataset suit covers many low-resource Indic\nlanguages. They use mT5 and IndicBART for eval-\nuation. However, an auto regressive text genera-\ntion task is not included in this suit. As for auto-\nregressive text generation evaluation, we are only\naware of Chen et al. (2021), who considered mT5\nand mBART. However, evaluation was done only\non 4 high-resource languages.\n3 Methodology\nAll the models considered in this research\nare trained using the Transformer architec-\nture (Vaswani et al., 2017), which is an Encoder-\nDecoder model that contains a set of encoder layers\nand decoder layers. GPT, BART and T5 are pre-\ntrained with English data. mBART and mT5 are\npre-trained with data from multiple languages (50\nand 101, respectively). Here, pre-training means,\nthe models have been trained with a self-supervised\nobjective such as ‘span corruption’ (Xue et al.,\n2020). All these models have to be fine-tuned for\nthe selected downstream task.\nGPT models are decoder based. Here, the\nencoder-decoder cross attention block is discarded\nbecause there is no encoder. Self-attention has\nbeen replaced by masked self-attention. We follow\nthe standard training procedure of GPT-2 model\nin training it for MWP generation. T5, BART,\nmBART and mT5 are encode-decoder models.\nThey expect a text sequence as the input and out-\nput. For auto-regressive text generation, we use\nthe conditional generator option of BART/mBART\nand T5/mT5, which makes the output of the model\nconditioned on the preceding input sequence. In\nboth these cases, the models generate the rest of\nthe MWP for a given seed.\n4 Experiments\n4.1 Dataset\nLiyanage and Ranathunga (2020)’s dataset contains\ntwo types of MWPs: simple MWPs and algebraic\nMWPs. The simple MWP dataset contains 2000\n146\nquestions and the Algebraic MWP dataset contains\n2350 questions. This dataset contains questions in\nEnglish, Tamil and Sinhala, but is not multi-way\nparallel.\nWe extended this dataset using the Dolphin18K\ndataset (Huang et al., 2016) and the allArith dataset\n(Roy and Roth, 2016) to add more diversity to\nthe dataset. We selected questions that are simi-\nlar or slightly higher in complexity compared to\nLiyanage and Ranathunga (2020)’s corpus. Ques-\ntions that have lengthy descriptions and those corre-\nsponding to complex Mathematical equations were\nomitted. The extended dataset now contains 4210\nAlgebraic MWPs and 3160 simple MWPs. Simple\nMWP dataset contains simple arithmetic questions\nas the example shown in the introduction. These\nquestions contain constraints such as ‘first number\nis always larger than the second one’. Algebraic\nMWPs are more logical and require two or more\nequations to solve.\nE.g.: The sum of two numbers is twenty-three,\nand the larger number is five more than the smaller\nnumber. Find these numbers.\nCorresponding Sinhala and Tamil examples are\ngiven in the Figure 1 in Appendix.\nTable 1: Statistics of the multi-way parallel dataset\nDataset type\nAvg. Num.\nof words per\nquestion\nAvg. Num.\nof characters\nper question\nEnglish Simple (ES) 15 54\nEnglish Algebraic (EA) 14 62\nSinhala Simple (SS) 19 61\nSinhala Algebraic (SA) 17 59\nTamil Simple (TS) 13 49\nTamil Algebraic (TA) 16 57\nMathematics tutors translated these questions to\nSinhala and Tamil. They were asked to retain the\nsame sentence count and syntactic structure as the\nEnglish source question, as much as possible. On\naverage, there are two sentences per question, with\na maximum of four sentences. Other statistics of\nthe dataset are given in Table 1.\nIn order to verify the quality of the manual\ntranslations, we used the Direct Assessment (DS)\nmethod (Bojar et al., 2016). We selected three\nbilingual speakers (undergraduate students who are\nproficient in Mathematics) for each language pair\n(English-Sinhala, English-Tamil). Each evaluator\nwas assigned 200 translated MWPs along with the\noriginal English question. They were asked to rate\nthe translated version with respect to adequacy and\nTable 2: Quality estimation results of the translated\ndataset\nData\nset Rank\n0-10 11-29 30-50 51-69 70-90 91-100\nSS 0% 1.6% 3% 6.3% 22.6% 66%\nSA 0% 0% 0.3% 2.6% 12.6% 84.3%\nTS 0% 1% 4% 8.3% 27.6% 59%\nTA 7% 12% 6.3% 6% 11.3% 57%\nTable 3: Language Coverage of pre-trained models\nModel English Tamil Sinhala\nBART Storage(GB) 160 - -\nT5 Storage(GB) 700 - -\nmT5 Token(B) 2733 3.4 0.8\nPages(M) 3,067 3.5 0.5\nmBART Token(B) 55.61 0.595 0.243\nStorage(GiB) 300.8 12.2 3.6\nfluency and give a rating between 1-100, where\n0-10: incorrect translation, 11-29: a translation\nwith few correct keywords, but the overall meaning\nis different from the source, 30-50: a translation\nwith major mistakes, 51-69: a translation which is\nunderstandable and conveys the overall meaning\nof the source but contains typos or grammatical\nerrors, 70-90: a translation that closely preserves\nthe semantics of the source sentence and 91-100: a\nperfect translation (Bojar et al., 2016). As shown\nin Table 2, except for the Tamil Algebraic dataset,\nall the others report a quality level greater than 85.\n4.2 Model Selection\nAccording to Huggingface2, GPT2-Medium, T5-\nbase and BART-large variants have approximately\n300M model parameters. Therefore these were\nused for further experiments. For multilingual\nMWP generation, we selected mT5-base and\nmBART50-large models, to correspond to their\nmonolingual counterparts. As shown in Table 3,\nSinhala and Tamil are largely under-represented in\nboth multilingual models.\n4.3 Experiment Setup\nFine-tuning for the selected Huggingface models\nwas set-up with 20 epochs, 4-batch size and 1e-4\nlearning rate. All the experiments were done on\na system that has 15 Intel(R) Core(TM) i9-9900K\nCPUs and Quadro RTX 6000 GPU with 24GB\nmemory.\n2https://huggingface.co/transformers/\nv3.3.1/pretrained_models.html\n147\n4.4 Evaluation Metrics\nTest BLEU (Papineni et al., 2002) and ROUGE\n(ROUGE-1 and ROUGE-2) (Lin, 2004) scores\nwere used as the automatic evaluation metrics, as\nthey are still very commonly used (Gehrmann et al.,\n2021). For all the experiments, we use BLEU-1\nfor results analysis, with ROUGE results reported\nin the Appendix. We note that results reported via\nthese two metrics show a correlation.\nThe generated MWPs should have correct\nspelling/grammar and satisfy different Mathemat-\nical constraints. A Maths tutor should be able to\nedit a generated MWP in less time compared to\nwriting a question from scratch. We carried out\na human evaluation to validate the quality of the\ngenerated questions and the time taken by a tutor\nto correct a generated MWP.\n5 Results and Evaluation\n5.1 Pre-trained models vs Baseline\nSince Liyanage and Ranathunga (2020) have pro-\nvided the evaluation results for their dataset of En-\nglish, Tamil and Sinhala, we considered this as\nour baseline. Our first experiment is to determine\nwhether fine-tuning the pre-trained models is better\nthan the selected RNN baseline.\nFor this experiment, we used only Liyanage and\nRanathunga (2020)’s dataset, and used the same\ndata split (train:validation:test 80:10:10) they have\nused3. Note that for English, results are obtained\nusing the monolingual models.\nAs mentioned earlier, during training and infer-\nence of auto-regressive text generation models, the\ninput to the model is the initial portion of text. This\nis called a seed. In this experiment, we tested our\nmodels with a quarter of a question (quarter seed).\nIn contrast, Liyanage and Ranathunga (2020) used\nthe first (50-100) characters. Usually, this attributed\nto more than half of the question. Note that this\nmeans the length of the seed varies from question\nto question.\nResults are shown in Table 4. All our models,\neven when using just the quarter seed, outperform\nthe baseline by a significant margin, thus highlight-\ning the robustness of the pre-trained models even\nfor low-resource language text generation. Sample\nquestions generated from the models are shown\nin Table 5. Here, compared to the output of the\npre-trained models, the question generated by the\n3They reported results only using BLEU\nbaseline is incomplete, not in a question format and\nhas spelling errors.\nTable 4: BLEU for the baseline experiments of English,\nSinhala and Tamil MWPs.\nDataset\ntype Model Seed\nsize En Si Ta\nSimple Baseline >Half 22.97 24.49 20.74\nGPT-2 Quarter 67.00 - -\nBART/\nmBART Quarter 80.93 74.52 71.07\nT5/\nmT5 Quarter 88.42 68.02 66.45\nAlgebraic Baseline >Half 33.53 - -\nGPT-2 Quarter 48.93 - -\nBART/\nmBART Quarter 62.99 58.13 68.21\nT5/\nmT5 Quarter 72.69 47.19 55.33\n5.2 Effect of Fine-tuning Dataset Size\nWe conducted comprehensive experiments on our\nmodels to analyze how the quality of the results\nvaries with different fine-tuning dataset sizes. We\nsplit the dataset for train:validate:test in such a man-\nner that the training set has 80, 40, and 20 percent\nof the total dataset per MWP category, and con-\nducted three exeriments. Validation and test sets\nwere always kept to be 10% of the total dataset per\nMWP category. Results are shown in Table 6.\nThe obvious observation is that the performance\nof all the models drop when the fine-tuning dataset\nsize drops, which of course is not surprising.\nAs for English auto-regressive text generation\nresults with monolingual models, both sequence-\nto-sequence models outperform GPT-2. This is in\nline with observations for other types of text gen-\neration tasks such as graph-to-text generation and\nquestion answering (Ribeiro et al., 2021). Further,\nT5 outperforms BART. We believe this is due to\nT5 being trained with more data, and this observa-\ntion confirms with what has been reported for tasks\nsuch as machine reading comprehension (Tanaka\net al., 2021) and text summarization (Garg et al.,\n2020). English results with mBART and mT5 lag\nbehind their monolingual counterparts. This is to\nbe expected - the multilingual models do not have\nEnglish data in the same quantities as their mono-\nlingual counterparts. However, this lag is usually\naround 2 BLEU.\nAs for multilingual models, mBART outper-\nforms mT5 in all the cases except for the 20%\ntrain set scenario of the English Algebraic dataset.\n148\nTable 5: Sample English MWPs generated using the baseline and the fine-tuned models. Seed size: Quarter of the\nquestion\nModel Generated MWPs\nReference The sum of two numbers is 56, their difference is 22,\nFind the larger number.\nBaseline the sum of two numbers is 12. their differenct are the two\nconsecutive integers if the sum of the second integers is 10.\nFine-tuned GPT2 The sum of two numbers is 76, the second is 8 more than\n3 times first, what are these 2 numbers?\nFine-tuned BART The sum of two numbers is 60. three times the smaller number\nminus twice the larger number is 56. Find the larger number.\nFine-tuned T5 The sum of two numbers is 91. the larger number is 1 more\nthan 4 times the smaller number. Find the numbers.\nTable 6: Effect of the fine-tuning Dataset Size reported in BLEU (for quarter seed length)\nDataset\nsize\nTrain\nSize\nTest\nSize English Tamil Sinhala\nGPT2 BART T5 mBART mT5 mBART mT5 mBART mT5\nALG\n4210\n3370\n(80%)\n420\n(10%) 55.88 60.22 65.32 67.06 62.78 52.68 50.65 45.46 42.44\n1679\n(40%)\n420\n(10%) 54.23 57.76 62.2 60.76 58.86 50.344 49.34 42.58 38.32\n835\n(20%)\n420\n(10%) 51.87 54.93 59.64 53.27 56.34 47.37 42.26 41.03 34.26\nSIM\n3160\n2530\n(80%)\n316\n(10%) 57.65 65.13 67.82 67.74 66.67 65.85 61.67 65.44 61.71\n1264\n(40%)\n316\n(10%) 55.56 57.99 64.43 64.08 62.25 60.24 58.60 60.48 54.08\n632\n(20%)\n316\n(10%) 54.48 55.52 62.09 61.47 57.13 59.5 53.87 56.81 50.92\nThis is surprising, because as reported in Table 3,\nmT5 has more Sinhala and Tamil data compared\nto mBART. Noting that mT5 has more language\ncoverage than mBART, one possible reason for this\ncould be the problem of curse of multilinguality -\nwhere the cross-lingual transfer in a multilingual\nmodel degrades when the language coverage in-\ncreases in a model (Conneau et al., 2019).\n5.3 Effect of Pre-training Dataset Size\nAn interesting observation is that, although the\ndataset is multi-way parallel, the result of a model\nfor the same train-test split is not the same across\nlanguages. This difference is the highest for the\nalgebraic dataset. Specifically, always English has\nthe highest result, followed by Tamil, and then Sin-\nhala. We attribute this to the amount of language\ndata included in model pre-training (refer Table 3).\nMoreover, the results gap between Sinhala and En-\nglish is higher for mT5 compared to mBART. This\ncould be due to the effect of curse of multilinguality\nthat we mentioned earlier - suficient cross-lingual\ntransfer does not happen between Sinhala and En-\nglish due to mT5’s high language coverage.\n5.4 Effect of the Context of MWPs\nWe note that all the models find the algebraic MWP\ngeneration more difficult than simple MWP genera-\ntion. This indicates that text generation capabilities\nof pre-trained models depend on the context of\nthe text - algebraic MWPs have more Mathemati-\ncal context than the simple MWPs, which contain\nmore open-domain text that is similar to the text\nused to pre-train the models.\nThis may be the reason for the simple MWP\ndataset to have less language-wise difference in\nmodel performance compared to the Algebraic\ndataset as discussed above - the maximum differ-\nence is about 5 BLEU between the best performing\nEnglish and least performing Sinhala. Given the\ncontext of simple MWPs is more similar to the\npre-training data, simple MWP generation bene-\nfits better from cross-lingual knowledge transfer\nbetween related languages.\nIn order to further evaluate this effect, we car-\nried out an additional experiment - for the 40%-\n50% train-test split, we trained the models with\none dataset, and tested with the other. Results are\nreported in Table 7. Compared to the results re-\n149\nported in Table 6, we see a substantial drop in the\nresults, when the models are fine-tuned with the\nother dataset. This highlights the model’s inabil-\nity to generalize to the general problem of MWP\ngeneration, if the dataset contains MWPs only rep-\nresenting a specific context.\nTable 7: BLEU score results for different domain train\nand test sizes\nTrain\nID\nTrain\nSize\nTest\nID\nTest\nSize mBART mT5\nSA 1679\n(40%) SS 1580\n(50%) 32.39 29.23\nSS 1264\n(40%) SA 2088\n(50%) 27.01 17.87\nTA 1679\n(40%) TS 1580\n(50%) 35.27 33.44\nTS 1264\n(40%) TA 2088\n(50%) 32.12 27.75\n5.5 Zero-shot MWP Generation\nMotivated by the results we obtained in Table 6 for\nsmall amounts of fine-tuning data, we carried out\nzero-shot text generation experiments. However,\nas seen in Table 8, all the models miserably fail\non zero-shot text generation. The sample gener-\nations shown in Table 9 evidence that the gener-\nated sentences are not questions but more like sto-\nries. This is because these pre-trained models are\nnot specifically trained on a question-type dataset.\nHowever, when fine-tuned with just 100 data sam-\nples, the performance increases by a significant\nmargin. This result agrees with the observations\nof Burnyshev et al. (2021) on few-shot text gener-\nation of task-oriented utterances. This provides a\nray of hope for low-resource languages - at least for\nthose that are covered by pre-trained multilingual\nmodels, even with a very small training dataset, a\ndescent result can be expected. We also note that\nthis zero-shot/few-shot observation in in-line with\nthose reported for other pre-trained models such as\nmBERT (Lauscher et al., 2020).\n5.6 Effect of Seed Length\nThe next experiment is to determine the impact of\nseed length. For this, we fixed the train set size\nto 40% and tested with 50% of the dataset. Ex-\nperiments are run on mBART, which is shown to\noutperform mT5. We varied the seed length from\n10%-40%. Table 10 reports the results for Sinhala\nand Tamil. As expected, the quality of the gener-\nated text goes up when the seed length increases.\nHowever, even 10% of the seed is enough to pro-\nTable 8: Zero-shot and few-shot results for Sinhala and\nTamil\nTest\nDataset\nTrain\nSize\nTest\nSize mBART mT5\nES 0 986 5.96 0.05\nEA 0 1175 8.50 0.42\nSS 0 986 6.37 0.01\nSA 0 1175 7.50 0.03\nTS 0 986 4.57 0.02\nTA 0 1175 6.54 0.03\nES 100 986 23.24 4.30\nEA 100 1175 34.50 3.93\nSS 100 986 52.72 5.42\nSA 100 1175 18.21 2.36\nTS 100 986 48.86 2.87\nTA 100 1175 39.95 0.60\nTable 9: Sample Zero shot Generation results\nModel Generated MWPs\nReference\nThe difference between two\nnumbers is 24, Find the numbers\nif their sum is 88.\nGPT2\nThe difference betweena \"first,\"\nand an ordinary, job is that the\nformer often requires significant\nskills.What’s next?Well. . . not much\nreally right nowthough!\nBART The...\nThe difference betweenthe two\nT5\nThe difference between\nthe two is that the difference\nbetween the two is the difference\nbetween the\nvide an acceptable result - the lowest is 30 BLEU\nreported for Sinhala Algebra MPW dataset. The\nimpact of question type and the pre-training data\namount of the language can be seen here as well.\nTable 10: Text generation results for different seed sizes\nSeed\nsize SS TS SA TA\n10% 48.9 45.48 30.19 36.77\n20% 58.25 57.74 39.91 45.82\n30% 65.47 65.02 47.38 54.21\n40% 71.51 72.39 53.85 62.5\n5.7 Human Evaluation\nWe analysed the questions generated by the differ-\nent models to identify the types of errors in MWP\ngeneration. The identified errors are given in Table\n11.\nWe also wanted to identify the actual utility of\n150\nTable 11: Identified errors in the generated MWPs\nError Type Description Example\nCo-reference inconsistent co-reference Muralihad 9 balls in his house and his friend gave him 4. How many balls doesSamhave?.Here, the second sentence has the proper noun Sam, instead of Murali\nUnit A numerical quantity is associated withan inconsistent unit Kamal built a house and he used 90 kg cement and40 l sand. How much more cement than sand did Kamal use?.Here, sand is given the unit liter (l), insted of kg\nSpelling Spelling mistakes in a word What three consecutie odd integers have a sum of -105?Word ‘consecutie’ is mispelled.\nGrammar A sentence has grammar mistakesThedifferenceof the squares of a number and 6are18. Find the number.Here, the noun ‘difference’ is associated with the auxiliary verb ‘are’.\nMath constraintsThe given numerical values do not leadto a meaningful Mathematical equationThe sum of three consecutive odd integers is 194, what are the integers?This question cannot be solved without changing the values\nthe generated questions - whether it is more ef-\nfective for a tutor to correct a generated question,\nrather than generating a question from scratch. This\nexperiment was conducted only for Sinhala and\nEnglish, considering mBART-large and mT5-base\nmodels. We gave 20 MWPs (10 simple MWPs and\n10 Algebraic MWPs) generated by both mBART\nand mT5 using 50:40 train:test fine-tuning dataset\nsizes for quarter input seed to 5 university students4\nwho are proficient in English and Sinhala. They\nwere asked to record the time taken to correct each\nquestion (refer Table 12 & 13 ). Then they were\ngiven the list of errors we identified in Table 11,\nand were asked to mark the type of errors they iden-\ntified. Results of the manual analysis are reported\nin Tables 14. Note that one generated question may\ncontain more than one type of error.\nTable 12: Time taken for a human to correct Simple\nMWPs (reported in minutes). TTE: Time to Edit 10\ngenerated MWPs, TTG: Time To Generate 10 MWPs\nmBART mT5\nTTG TTE TTE TTE\nSE SS SE SS SE SS SE SS\nT1 18 15 2 2.5 0.5 0.38 0.66 0.66\nT2 20 25 2.2 3 0.75 0.45 0.48 0.58\nT3 15 17.5 1 1.5 0.55 0.38 0.71 0.51\nT4 15 28 2.5 1 0.6 0.83 0.6 0.75\nT5 21 26.5 3 2 0.63 0.91 0.45 0.6\nAv 17.8 22.4 2.14 2 0.60 0.59 0.58 0.62\nTable 13: Human evaluation results for Algebraic\nMWPs in minutes AE: Algebraic English, AS: Alge-\nbraic Sinhala, (Number of minutes taken to Edit 10\ngenerated MWPs)\nmBART mT5\nAE AS AE AS\nTutor 1 2 0.66 1.16 2\nTutor 2 0.73 0.65 0.58 0.73\nTutor 3 0.42 0.75 0.83 0.78\nTutor 4 0.9 0.88 1.26 1.41\nTutor 5 1.25 1.08 0.91 0.95\nAverage 1.06 0.80 0.95 1.17\nFor English MWPs, mT5 model takes the short-\nest time to correct. For Sinhala MWPs, mBART\n4Not the same ones who did the translation evaluation\nTable 14: Percentages of different types of errors found\nin simple MWPs\nErrors% mBART mT5\nSE AE SS AS SE AE SS AS\nCo-reference 4 4 6 4 8 2 6 2\nUnit 4 1 1 1 2 1 1 1\nSpelling 0 0 4 2 2 0 0 2\nGrammar 16 12 16 10 8 10 14 10\nmath constraint % 12 38 22 30 14 22 24 32\nmodel takes the shortest time to correct. Note that\nall these times are less than what Liyanage and\nRanathunga (2020) have reported, who in turn have\nshown that writing questions from scratch takes\nconsiderably more time than text generation from\ntheir technique.\nCo-reference, unit, spelling and grammar are\nusually less than 20% even in the worst performing\nmodel. However, errors related to Math constraint\nviolations are relatively high. This implies that\nthe pre-trained models do not have sufficient infor-\nmation to capture constraints specific to a domain,\nwhich of course is not surprising.\n6 Conclusion\nWe evaluated several multilingual and monolin-\ngual pre-trained models for the task of MWP gen-\neration considering four factors - the amount of\nlanguage-specific pre-trained data, amount of fine-\ntuning data, length of the seed and type of the MWP.\nWe also presented a multi-way parallel dataset for\nMWP evaluation, which includes two languages\nunder-represented in these pre-trained models. Our\nresults are very promising - even with a small\namount of parallel data and a short seed, all the\nmodels are capable of producing acceptable results\nfor all the considered languages. Human evaluation\nshowed that a Mathematics tutor can take benefit\nof this automated MWP generation, as it saves time\ncompared to writing an MWP from scratch.\nIn this research, we did not specifically focus on\nhow to satisfy Maths constraints in an MWP. The\neffect of this was shown in human evaluation - the\nquestions had a noticeable number of issues related\nto Math constraints. Thus in the future, we plan\n151\nto focus on constraint-based generation of MWP.\nA starting point would be the work of Wang et al.\n(2021), who investigated this problem for MWP\ngeneration with GPT-2. A major criticism of the\npre-trained models is that they support a very small\nfraction of languages. Thus we want to investigate\nhow the model performance can be improved in the\ncontext of languages not included in the model.\n7 Ethical Considerations\nWe have obtained the permission to republish the\nbaseline (Liyanage and Ranathunga, 2020) datasets.\nIn Dolphin18K dataset (Huang et al., 2016) and al-\nlArith dataset (Roy and Roth, 2016), they have\nnot mentioned any restrictions on using the data.\nWe cited their papers as requested in their repos.\nWe paid the workers according to the rates defined\nin our university. We verbally explained the pur-\npose of the dataset and the process they have to\nfollow. Worker information was not collected nor\nincluded in the dataset, as this is not relevant to\nthe task. In the fine-tuning process, we only fo-\ncused on elementary-level MWPs. This dataset is\npublicly released. It does not have any offensive\ncontent, nor specific references to individuals or\norganizations. Thus the fine-tuning process can-\nnot introduce any additional harmful content to the\nmodels. We believe that MWP generation in multi-\nple languages has a long-term positive benefit for\nschool children, and the education sector in general.\nThus, the positive impact of this research would\noutweigh any unforeseen negative impacts it could\nbring.\n8 Acknowledgement\nDataset creation of this project was funded by a\nSenate Research Committee (SRC) grant of Uni-\nversity of Moratuwa (UoM), Sri Lanka. The au-\nthors would like to thank the National Language\nProcessing Center (NLPC) of UoM for funding the\npublication of this paper at INLG.\nReferences\nBed Raj Acharya. 2017. Factors affecting difficulties in\nlearning mathematics by mathematics learners. In-\nternational Journal of Elementary Education, 6(2):8–\n15.\nAndinet Assefa Bekele. 2020. Automatic generation of\namharic math word problem and equation. Journal\nof Computer and Communications, 8(8):59–77.\nOndˇrej Bojar, Rajen Chatterjee, Christian Federmann,\nYvette Graham, Barry Haddow, Matthias Huck, An-\ntonio Jimeno Yepes, Philipp Koehn, Varvara Lo-\ngacheva, Christof Monz, et al. 2016. Findings of the\n2016 conference on machine translation. In Proceed-\nings of the First Conference on Machine Translation:\nVolume 2, Shared Task Papers, pages 131–198.\nPaweł Budzianowski and Ivan Vuli´c. 2019. Hello, it’s\ngpt-2–how can i help you? towards the use of pre-\ntrained language models for task-oriented dialogue\nsystems. arXiv preprint arXiv:1907.05774.\nPavel Burnyshev, Valentin Malykh, Andrey Bout, Eka-\nterina Artemova, and Irina Piontkovskaya. 2021. Sin-\ngle example can improve zero-shot data generation.\nIn Proceedings of the 14th International Conference\non Natural Language Generation, pages 201–211.\nTianyang Cao, Shuang Zeng, Songge Zhao, Mairgup\nMansur, and Baobao Chang. 2021. Generating math\nword problems from equations with topic consistency\nmaintaining and commonsense enforcement. In Inter-\nnational Conference on Artificial Neural Networks,\npages 66–79. Springer.\nYiran Chen, Zhenqiao Song, Xianze Wu, Danqing\nWang, Jingjing Xu, Jiaze Chen, Hao Zhou, and Lei Li.\n2021. Mtg: A benchmarking suite for multilingual\ntext generation. arXiv preprint arXiv:2108.07140.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. arXiv\npreprint arXiv:1911.02116.\nThiago Castro Ferreira, Claire Gardent, Nikolai Ilinykh,\nChris van der Lee, Simon Mille, Diego Moussallem,\nand Anastasia Shimorina. 2020. The 2020 bilin-\ngual, bi-directional webnlg+ shared task: Overview\nand evaluation results (webnlg+ 2020). In Proceed-\nings of the 3rd International Workshop on Natu-\nral Language Generation from the Semantic Web\n(WebNLG+), pages 55–76.\nApar Garg, Saiteja Adusumilli, Shanmukha Yenneti,\nTapas Badal, Deepak Garg, Vivek Pandey, Abhishek\nNigam, Yashu Kant Gupta, Gyan Mittal, and Rahul\nAgarwal. 2020. News article summarization with\npretrained transformer. In International Advanced\nComputing Conference, pages 203–211. Springer.\nSebastian Gehrmann, Tosin Adewumi, Karmanya\nAggarwal, Pawan Sasanka Ammanamanchi,\nAnuoluwapo Aremu, Antoine Bosselut, Khy-\nathi Raghavi Chandu, Miruna-Adriana Clinciu,\nDipanjan Das, Kaustubh Dhole, et al. 2021. The\ngem benchmark: Natural language generation,\nits evaluation and metrics. In Proceedings of the\n1st Workshop on Natural Language Generation,\nEvaluation, and Metrics (GEM 2021), pages 96–120.\n152\nDanqing Huang, Shuming Shi, Chin-Yew Lin, Jian Yin,\nand Wei-Ying Ma. 2016. How well do computers\nsolve math word problems? large-scale dataset con-\nstruction and evaluation. In Proceedings of the 54th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 887–896.\nRik Koncel-Kedziorski, Ioannis Konstas, Luke Zettle-\nmoyer, and Hannaneh Hajishirzi. 2016. A theme-\nrewriting approach for generating algebra word prob-\nlems. arXiv preprint arXiv:1610.06210.\nAman Kumar, Himani Shrotriya, Prachi Sahu, Raj\nDabre, Ratish Puduppully, Anoop Kunchukuttan,\nAmogh Mishra, Mitesh M Khapra, and Pratyush Ku-\nmar. 2022. Indicnlg suite: Multilingual datasets for\ndiverse nlg tasks in indic languages. arXiv preprint\narXiv:2203.05437.\nAnne Lauscher, Vinit Ravishankar, Ivan Vuli ´c, and\nGoran Glavaš. 2020. From zero to hero: On the\nlimitations of zero-shot language transfer with mul-\ntilingual transformers. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 4483–4499.\nEn-Shiun Annie Lee, Sarubi Thillainathan, Shravan\nNayak, Surangika Ranathunga, David Ifeoluwa Ade-\nlani, Ruisi Su, and Arya McCarthy. 2022. Pre-trained\nmultilingual sequence-to-sequence models: A hope\nfor low-resource language translation? In Findings of\nthe Association for Computational Linguistics 2022.\nJieh-Sheng Lee and Jieh Hsiang. 2020. Patent claim\ngeneration by fine-tuning openai gpt-2. World Patent\nInformation, 62:101983.\nLeo Leppänen, Myriam Munezero, Mark Granroth-\nWilding, and Hannu Toivonen. 2017. Data-driven\nnews generation for automated journalism. In Pro-\nceedings of the 10th International Conference on\nNatural Language Generation, pages 188–197.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVes Stoyanov, and Luke Zettlemoyer. 2019. Bart: De-\nnoising sequence-to-sequence pre-training for natural\nlanguage generation, translation, and comprehension.\narXiv preprint arXiv:1910.13461.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. arXiv preprint\narXiv:2005.11401.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nDayiheng Liu, Yu Yan, Yeyun Gong, Weizhen Qi, Hang\nZhang, Jian Jiao, Weizhu Chen, Jie Fu, Linjun Shou,\nMing Gong, et al. 2021. Glge: A new general lan-\nguage generation evaluation benchmark. In Find-\nings of the Association for Computational Linguis-\ntics: ACL-IJCNLP 2021, pages 408–420.\nTianqiao Liu, Qian Fang, Wenbiao Ding, and Zitao Liu.\n2020. Mathematical word problem generation from\ncommonsense knowledge graph and equations. arXiv\npreprint arXiv:2010.06196.\nVijini Liyanage and Surangika Ranathunga. 2019. A\nmulti-language platform for generating algebraic\nmathematical word problems. In 2019 14th Confer-\nence on Industrial and Information Systems (ICIIS),\npages 332–337. IEEE.\nVijini Liyanage and Surangika Ranathunga. 2020.\nMulti-lingual mathematical word problem genera-\ntion using long short term memory networks with\nenhanced input features. In Proceedings of The\n12th Language Resources and Evaluation Confer-\nence, pages 4709–4716.\nSimon Mille, Anja Belz, Bernd Bohnet, Thiago Cas-\ntro Ferreira, Yvette Graham, and Leo Wanner. 2020.\nThe third multilingual surface realisation shared task\n(sr’20): Overview and evaluation results. In Proceed-\nings of the Third Workshop on Multilingual Surface\nRealisation, pages 1–20.\nAhmadreza Mosallanezhad, Kai Shu, and Huan Liu.\n2020. Topic-preserving synthetic news generation:\nAn adversarial deep reinforcement learning approach.\narXiv preprint arXiv:2010.16324.\nKumaresh Nandhini and Sadhu Ramakrishnan Bala-\nsundaram. 2011. Math word question generation\nfor training the students with learning difficulties.\nIn Proceedings of the International Conference &\nWorkshop on Emerging Trends in Technology, pages\n206–211.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computa-\ntional Linguistics, pages 311–318.\nOleksandr Polozov, Eleanor O’Rourke, Adam M\nSmith, Luke Zettlemoyer, Sumit Gulwani, and Zo-\nran Popovi´c. 2015. Personalized mathematical word\nproblem generation. In Twenty-Fourth International\nJoint Conference on Artificial Intelligence.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nLeonardo FR Ribeiro, Martin Schmitt, Hinrich Schütze,\nand Iryna Gurevych. 2021. Investigating pretrained\nlanguage models for graph-to-text generation. In Pro-\nceedings of the 3rd Workshop on Natural Language\nProcessing for Conversational AI, pages 211–227.\n153\nMelissa Roemmele. 2016. Writing stories with help\nfrom recurrent neural networks. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol-\nume 30.\nSubhro Roy and Dan Roth. 2016. Unit dependency\ngraph and its application to arithmetic word problem\nsolving. arXiv preprint arXiv:1612.00969.\nLeanne J Rylands and Carmel Coady. 2009. Perfor-\nmance of students with weak mathematics in first-\nyear mathematics and science. International Journal\nof Mathematical Education in Science and Technol-\nogy, 40(6):741–753.\nBowen Tan, Zichao Yang, Maruan AI-Shedivat, Eric P\nXing, and Zhiting Hu. 2020. Progressive generation\nof long text with pretrained language models. arXiv\npreprint arXiv:2006.15720.\nRyota Tanaka, Kyosuke Nishida, and Sen Yoshida. 2021.\nVisualmrc: Machine reading comprehension on docu-\nment images. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 35, pages 13878–\n13888.\nYuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Na-\nman Goyal, Vishrav Chaudhary, Jiatao Gu, and An-\ngela Fan. 2020. Multilingual translation with exten-\nsible multilingual pretraining and finetuning. arXiv\npreprint arXiv:2008.00401.\nPatrick W Thompson. 1985. Experience, problem solv-\ning, and learning mathematics: Considerations in\ndeveloping mathematics curricula. Teaching and\nlearning mathematical problem solving: Multiple\nresearch perspectives, pages 189–243.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nKe Wang and Zhendong Su. 2016. Dimensionally\nguided synthesis of mathematical word problems. In\nIJCAI, pages 2661–2668.\nZichao Wang, Andrew Lan, and Richard Baraniuk. 2021.\nMath word problem generation with mathematical\nconsistency and problem context constraints. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 5986–\n5999.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2020. mt5: A massively multilingual\npre-trained text-to-text transformer. arXiv preprint\narXiv:2010.11934.\nQingyu Zhou and Danqing Huang. 2019. Towards gen-\nerating math word problems from equations and top-\nics. In Proceedings of the 12th International Confer-\nence on Natural Language Generation, pages 494–\n503.\nA Appendix\nFigure 1: Sinhala and Tamil Example MWPs\n.\nTable 15: Zeroshot result ROUGE score for Sinhala and\nTamil\nTest\nDataset\nTrain\nSize Test\nSize\nmBART mT5\nR-1 R-2 R-1 R-2\nES 0 986 0.467 0.342 0.026 0.005\nEA 0 1175 0.439 0.322 0.022 0.003\nSS 0 986 0.411 0.275 0.013 0.001\nSA 0 1175 0.378 0.248 0.010 0.001\nTS 0 986 0.423 0.286 0.007 0.001\nTA 0 1175 0.363 0.247 0.005 0.001\nES 100 986 0.241 0.172 0.057 0.024\nEA 100 1175 0.352 0.129 0.117 0.022\nSS 100 986 0.539 0.362 0.156 0.048\nSA 100 1175 0.212 0.074 0.050 0.010\nTS 100 986 0.494 0.221 0.076 0.018\nTA 100 1175 0.411 0.189 0.031 0.001\nTable 16: ROUGE score results for different domain\ntrain and test sizes\nTrain\nDataset\nTrain\nSize\nTest\nDataset\nTest\nSize mBART mT5\nR-1 R-2 R-1 R-2\nSA 1679\n(40%) SS 1580\n(50%) 0.354 0.246 0.372 0.249\nSS 1264\n(40%) SA 2088\n(50%) 0.301 0.193 0.271 0.142\nTA 1679\n(40%) TS 1580\n(50%) 0.384 0.276 0.467 0.324\nTS 1264\n(40%) TA 2088\n(50%) 0.355 0.253 0.323 0.209\n154\nTable 17: Effect of the fine-tuning Dataset Size reported in ROUGE (for quarter seed length)\nDataset\nsize\nTrain\nSize\nTest\nSize English Tamil Sinhala\nGPT2 BART T5 mBART mT5 mBART mT5 mBART mT5\nR-1 R-2 R-1 R-2 R-1 R-2 R-1 R-2 R-1 R-2 R-1 R-2 R-1 R-2 R-1 R-2 R-1 R-2\nALG\n4210\n3370\n(80%)\n420\n(10%) 0.61 0.44 0.61 0.42 0.66 0.50 0.68 0.53 0.65 0.47 0.56 0.40 0.54 0.36 0.49 0.30 0.48 0.28\n1679\n(40%)\n420\n(10%) 0.60 0.42 0.59 0.39 0.64 0.62 0.63 0.46 0.61 0.43 0.54 0.38 0.53 0.36 0.46 0.28 0.44 0.26\n835\n(20%)\n420\n(10%) 0.59 0.51 0.57 0.38 0.62 0.44 0.57 0.38 0.59 0.40 0.51 0.35 0.50 0.34 0.45 0.27 0.42 0.24\nSIM\n3160\n2530\n(80%)\n316\n(10%) 0.64 0.46 0.66 0.51 0.72 0.58 0.72 0.59 0.71 0.57 0.70 0.56 0.66 0.50 0.67 0.52 0.62 0.47\n1264\n(40%)\n316\n(10%) 0.63 0.45 0.61 0.44 0.68 0.68 0.68 0.54 0.66 0.52 0.65 0.49 0.64 0.47 0.63 0.58 0.56 0.53\n632\n(20%)\n316\n(10%) 0.62 0.45 0.59 0.42 0.66 0.52 0.66 0.51 0.62 0.45 0.64 0.48 0.60 0.44 0.59 0.43 0.53 0.36\n155",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8354766368865967
    },
    {
      "name": "Natural language processing",
      "score": 0.6523209810256958
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6396446228027344
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6264159083366394
    },
    {
      "name": "Tamil",
      "score": 0.6242664456367493
    },
    {
      "name": "Word (group theory)",
      "score": 0.6125552654266357
    },
    {
      "name": "Language model",
      "score": 0.5841986536979675
    },
    {
      "name": "TUTOR",
      "score": 0.49945926666259766
    },
    {
      "name": "Linguistics",
      "score": 0.21026083827018738
    },
    {
      "name": "Programming language",
      "score": 0.1243031919002533
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I195740183",
      "name": "University of Moratuwa",
      "country": "LK"
    }
  ]
}