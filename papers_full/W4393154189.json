{
  "title": "AdaFormer: Efficient Transformer with Adaptive Token Sparsification for Image Super-resolution",
  "url": "https://openalex.org/W4393154189",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2124316872",
      "name": "Xiaotong Luo",
      "affiliations": [
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A5106329729",
      "name": "Zekun Ai",
      "affiliations": [
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A3113798966",
      "name": "Qiuyuan Liang",
      "affiliations": [
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A2098763328",
      "name": "Ding Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2095844698",
      "name": "Yuan Xie",
      "affiliations": [
        "East China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2125810221",
      "name": "Yanyun Qu",
      "affiliations": [
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A2123131494",
      "name": "Yun Fu",
      "affiliations": [
        "Universidad del Noreste"
      ]
    },
    {
      "id": "https://openalex.org/A2124316872",
      "name": "Xiaotong Luo",
      "affiliations": [
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A5106329729",
      "name": "Zekun Ai",
      "affiliations": [
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A3113798966",
      "name": "Qiuyuan Liang",
      "affiliations": [
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A2098763328",
      "name": "Ding Liu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2095844698",
      "name": "Yuan Xie",
      "affiliations": [
        "East China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2125810221",
      "name": "Yanyun Qu",
      "affiliations": [
        "Xiamen University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4221150810",
    "https://openalex.org/W4386071519",
    "https://openalex.org/W6786585107",
    "https://openalex.org/W4309803824",
    "https://openalex.org/W1885185971",
    "https://openalex.org/W6854673259",
    "https://openalex.org/W6775335079",
    "https://openalex.org/W4376607835",
    "https://openalex.org/W3105328221",
    "https://openalex.org/W4280491778",
    "https://openalex.org/W3134058515",
    "https://openalex.org/W3102020262",
    "https://openalex.org/W4322825305",
    "https://openalex.org/W6800689796",
    "https://openalex.org/W3088246754",
    "https://openalex.org/W6840201148",
    "https://openalex.org/W3215986121",
    "https://openalex.org/W3211827226",
    "https://openalex.org/W4306291600",
    "https://openalex.org/W3168124404",
    "https://openalex.org/W2476548250",
    "https://openalex.org/W2739757502",
    "https://openalex.org/W4221138841",
    "https://openalex.org/W3136378982",
    "https://openalex.org/W3188427387",
    "https://openalex.org/W4221160837",
    "https://openalex.org/W6848003854",
    "https://openalex.org/W4221147129",
    "https://openalex.org/W3207918547",
    "https://openalex.org/W4312290555",
    "https://openalex.org/W4287020683",
    "https://openalex.org/W4225576932",
    "https://openalex.org/W3176997885",
    "https://openalex.org/W4378804909",
    "https://openalex.org/W4308536459",
    "https://openalex.org/W3012118477",
    "https://openalex.org/W4312849330",
    "https://openalex.org/W4230098700",
    "https://openalex.org/W4386076602",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W4384159680",
    "https://openalex.org/W4386076603",
    "https://openalex.org/W3164705069",
    "https://openalex.org/W3133953507",
    "https://openalex.org/W4287115696",
    "https://openalex.org/W4315606099"
  ],
  "abstract": "Efficient transformer-based models have made remarkable progress in image super-resolution (SR). Most of these works mainly design elaborate structures to accelerate the inference of the transformer, where all feature tokens are propagated equally. However, they ignore the underlying characteristic of image content, i.e., various image regions have distinct restoration difficulties, especially for large images (2K-8K), failing to achieve adaptive inference. In this work, we propose an adaptive token sparsification transformer (AdaFormer) to speed up the model inference for image SR. Specifically, a texture-relevant sparse attention block with parallel global and local branches is introduced, aiming to integrate informative tokens from the global view instead of only in fixed local windows. Then, an early-exit strategy is designed to progressively halt tokens according to the token importance. To estimate the plausibility of each token, we adopt a lightweight confidence estimator, which is constrained by an uncertainty-guided loss to obtain a binary halting mask about the tokens. Experiments on large images have illustrated that our proposal reduces nearly 90% latency against SwinIR on Test8K, while maintaining a comparable performance.",
  "full_text": "AdaFormer: Efficient Transformer with Adaptive Token Sparsification\nfor Image Super-resolution\nXiaotong Luo1*, Zekun Ai1*, Qiuyuan Liang1, Ding Liu2, Yuan Xie3†, Yanyun Qu1†, Yun Fu4\n1School of Informatics, Xiamen University, Fujian, China\n2Bytedance Inc.\n3School of Computer Science and Technology, East China Normal University, Shanghai, China\n4Northeastern University\n{xiaotluo, aizekun}@stu.xmu.edu.cn, yyqu@xmu.edu.cn\nAbstract\nEfficient transformer-based models have made remarkable\nprogress in image super-resolution (SR). Most of these works\nmainly design elaborate structures to accelerate the inference\nof the transformer, where all feature tokens are propagated\nequally. However, they ignore the underlying characteristic\nof image content, i.e., various image regions have distinct\nrestoration difficulties, especially for large images (2K-8K),\nfailing to achieve adaptive inference. In this work, we propose\nan adaptive token sparsification transformer (AdaFormer) to\nspeed up the model inference for image SR. Specifically, a\ntexture-relevant sparse attention block with parallel global\nand local branches is introduced, aiming to integrate informa-\ntive tokens from the global view instead of only in fixed local\nwindows. Then, an early-exit strategy is designed to progres-\nsively halt tokens according to the token importance. To es-\ntimate the plausibility of each token, we adopt a lightweight\nconfidence estimator, which is constrained by an uncertainty-\nguided loss to obtain a binary halting mask about the tokens.\nExperiments on large images have illustrated that our pro-\nposal reduces nearly 90% latency against SwinIR on Test8K,\nwhile maintaining a comparable performance.\nIntroduction\nSingle image super-resolution (SISR) aims to reconstruct\nhigh-resolution (HR) images from the degraded low-\nresolution (LR) counterparts. With the remarkable progress\nof transformer on high-level vision tasks (Yin et al. 2022b;\nPang et al. 2023), a growing number of transformer-based\nmethods (Liang et al. 2021a; Cai et al. 2023) have emerged\nfor image SR, which significantly exceed the convolutional\nneural network (CNN) based methods in performance by\nmining the long-range pixel dependencies. However, most\nof these methods are time-consuming with substantial com-\nputational complexity. It is unbearable for intelligent devices\nwith image resolution reached 4K (4096× 2160) or even 8K\n(7680 × 4320). Therefore, how to achieve efficient SR for\nlarge images with lower computational complexity and in-\nference time is an urgent problem to be solved.\n*These authors contributed equally.\n†Corresponding authors.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: The SR result (4×) of AdaFormer. It visualizes\nthe token halted at different layers in yellow, green, and\nbrown masks. Compared with ELAN (Zhang et al. 2022),\nour method achieves better PSNR with lower latency.\nRecently, several efficient transformer-based SR meth-\nods have been proposed to reduce computational expenses.\nSwinIR (Liang et al. 2021a) is the first efficient transformer\nfor image restoration, which introduces the local sliding\nwindow mechanism and performs self-attention (SA) within\neach window to speed up inference. ESRT (Lu et al. 2022)\ndesigns a lightweight hybrid backbone, which reduces the\nchannel number of multi-head SA with a high-frequency fil-\nter module. Besides, ELAN (Zhang et al. 2022) employs\ngroup-wise multi-scale SA with different local window sizes\nand the shared-attention mechanism to save more compu-\ntational costs. Although these methods have made some\nprogress in inference speed, there still exist two underlying\nlimitations. Firstly, they cannot achieve dynamic inference\nonce the model is trained, since all the tokens are propa-\ngated without distinction and the relevance between recov-\nery degree and computational resources for different tokens\nis ignored. Secondly, they mainly focus on designing the ef-\nficient window-based local SA, whereas the global edge and\ntexture information cannot be well captured due to the lim-\nited receptive field of fixed window size.\nAccording to the fact that different image patches have\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4009\nvarious reconstruction difficulties (Kong et al. 2021), some\neffective dynamic inference strategies for CNN-based mod-\nels have been proposed (Xie et al. 2021; Wang et al. 2022),\nwhile it is still undeveloped for transformer-based SR mod-\nels. Meanwhile, the dynamic token selection in high-level\ntasks (Fayyaz et al. 2021; Liang 2022; Yin et al. 2022b; Feng\nand Zhang 2023) motivates us to explore the token sparsifi-\ncation for efficient image SR. Note that these methods can-\nnot be directly applied to low-level tasks. They either rely on\nthe class token or introduce an extra scoring network to eval-\nuate the token importance. However, image SR is a regres-\nsion task, i.e., the output is pixel intensities, which lacks ex-\nplicit semantic information and cannot provide direct guid-\nance for token exiting. Thus, how to dynamically select the\nwell-recovered tokens for SR is challenging.\nTo address the above issues, we propose an efficient\ntransformer with adaptive token sparsification (AdaFormer)\nto speed up the inference on large images. Specifically, a\ntexture-relevant sparse attention block (TSAB) is first de-\nsigned, which includes a global cross-attention branch and\na standard local SA branch. The global branch aims to re-\ntrieve global edge or texture information, which makes up\nfor the limited receptive field of local windows. Then, a to-\nken early-exit strategy is introduced to dynamically filter out\nthe trivial tokens in the two branches. To evaluate the token\nimportance, a lightweight confidence estimator constrained\nwith an uncertainty-guided loss is adopted to obtain a pixel-\nwise confidence map. Then, a binary halting mask is gen-\nerated by accumulating the confidence score so as to adap-\ntively halt the corresponding tokens. As shown in Fig. 1, we\npresent the visualization diagram of adaptive token halting\ndepth for our AdaFormer, where the image is divided into\ndifferent windows (SA calculation units) to match the cor-\nresponding tokens. It is observed that different layers are al-\nlocated to tokens with various restoration difficulties. To the\nbest of our knowledge, this is the first work to investigate\nthe dynamic inference of the transformer with token-level\nsparsification for image SR.\nIn summary, the main contributions are four-fold:\n• An adaptive token sparsification transformer\n(AdaFormer) is proposed for efficient image SR,\nwhich introduces a texture-relevant sparse attention\nblock (TSAB) with a token early-exit strategy.\n• TSAB is designed for integrating the global and local tex-\nture information, which aims to eliminate the limited re-\nceptive field of the standard local sliding window.\n• The early-exit strategy is adopted to achieve dynamic in-\nference, where the token importance is evaluated via a\nconfidence estimator constrained by an uncertainty loss.\n• Extensive experiments demonstrate that our AdaFormer\noutperforms the state-of-the-art efficient transformer-\nbased SR methods with less inference time.\nRelated Work\nEfficient CNN-based SR Methods\nThe CNN-based SR methods have revealed remarkable\nprogress, whereas most efficient models mainly rely on elab-\norate structure design. SRCNN (Dong et al. 2016) firstly\ndesigns a three-layer CNN to learn the mapping relation\nbetween the bicubic-upsampled LR image and the HR im-\nage. IMDN (Hui et al. 2019) designs information multi-\ndistillation blocks to capture multi-level features by enlarg-\ning the receptive field. RFDN (Liu, Tang, and Wu 2020)\nand RLFN (Kong et al. 2022) propose the feature distil-\nlation connection and residual local feature learning for\nlightweight SR. LatticeNet (Luo et al. 2023) proposes a lat-\ntice block to assemble pair-wise residual blocks by learn-\nable combination coefficients. EDTS (Chao et al. 2023)\ntransforms time-consuming operations and speeds up the in-\nference without damaging reconstruction accuracy. Though\nthese models have obtained excellent results, the perfor-\nmance is still restricted by the local property of the convolu-\ntion operation and the equal treatment of spatial features.\nEfficient Transformer-based SR Methods\nTransformer has emerged promising potential in computer\nvision (Cai et al. 2023; Hsu, Liao, and Huang 2023). IPT\n(Chen et al. 2021) is a backbone model based on the standard\ntransformer for various low-level tasks and is pre-trained on\nlarge-scale datasets with abundant computational resources.\nSwinIR (Liang et al. 2021a) utilizes multiple swin trans-\nformer blocks with local attention and shifted-window inter-\naction to generate excellent results. ESRT (Lu et al. 2022)\ndesigns a hybrid model including a lightweight CNN back-\nbone with a high preserving block and a lightweight trans-\nformer backbone with a folding technique to reduce the\nchannel numbers. ELAN (Zhang et al. 2022) excavates the\nlong-range image dependency by calculating SA with differ-\nent window sizes on non-overlapping feature groups. GRL-\nB (Li et al. 2023) models feature hierarchies within the re-\ngional, local and global range by the window-based SA,\nchannel attention enhanced convolution operation and an-\nchored stripe SA. N-Gram (Choi, Lee, and Yang 2023) in-\ntroduces the N-Gram context to enlarge the receptive field\nfor restoring the degraded pixel via the sliding-WSA. How-\never, these methods only focus on how to design efficient SA\nstructures. Here, we propose a token early-exit mechanism\nto accelerate inference by the recovery degree of each token.\nAdaptive Inference in Visual Transformers\nDynamic inference in vision transformers for high-level\ntasks has been widely explored, which can be classified as\nhard pruning and soft pruning. The hard pruning approaches\naim to filter out the trivial tokens by a predefined scoring\nmechanism. DynamicViT (Rao et al. 2021) and AdaViT\n(Meng et al. 2022) introduce additional predictive networks\nto score tokens. Evo-ViT (Xu et al. 2022), ATS (Fayyaz et al.\n2021), and EViT (Liang 2022) use the class tokens to assess\nother token importance. However, it is difficult to achieve\naccurate scoring so as to suffer from a significant drop in\naccuracy. The soft pruning method generates new tokens\nfrom image tokens via introducing additional attention mod-\nels (Ryoo et al. 2021). Various attempts have been made in\nhigh-level tasks, whereas few discuss them for low-level.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4010\nConv\nUpsample\n.........\nAda\nTSAB\n...\nLR SR\n......\nHR|SR-HR|Confidence\nMap\nEncoder Decoder\nConv\nConv\nConfidence Estimator\nTSAB\nTSAB\nConv\n......\nS\nK,V\nQ\nK,V\nTSAB\nQ,K,VInput\nLinear\nMLP\nConfidence Estimator\nPrototype\nFigure 2: The overall framework of the proposed AdaFormer. It includes an Encoder, anAdaptive token sparsification Module\n(AdaM) with the token early-exit strategy, a Decoder, and an Upsampling module. AdaM mainly consists ofL texture-relevant\nsparse attention blocks (TSAB), which combine a local self-attention (SA) branch with a global cross-attention (CA) branch.\nThe confidence estimator aims to measure the token importance and provide the halting indication.\nProposed Method\nNetwork Architecture\nOverview. In Fig. 2, AdaFormer includes an Encoder, an\nAdaptive token sparsification Module (AdaM), a Decoder,\nand an Upsampling module. Let’s denote the LR image as\nIlr ∈ RH×W×3, where H and W are the height and width.\nFirst, Ilr is fed into the Encoder Hef consisting of several\nresidual blocks to extract the local context feature Fef :\nFef = Hef (Ilr) ∈ RH×W×C, (1)\nwhere C is the number of feature channels. Then, AdaM\n(HAdaM ) is employed to adaptively mine the global and lo-\ncal similarity dependence among Fef :\nFgf = HAdaM (Fef ) ∈ RH×W×C. (2)\nNext, Fgf is input to the Decoder Hd f, which includes\nseveral residual blocks to further enhance local information:\nFd f= Hd f(Fgf ) ∈ RH×W×C. (3)\nFinally, Fd fis fed to the Upsampling module Hup with\npixel shffule (Shi et al. 2016) to obtain the SR output:\nIsr = Hup(Fd f). (4)\nBy integrating the convolution-based encoder and de-\ncoder with the transformer, the advantages of local infor-\nmation extraction and global context modeling can be suffi-\nciently exploited so as to enhance model representation.\nAdaptive token sparsification module. AdaM consists of\ntexture-relevant sparse attention blocks (TSAB) and convo-\nlutional layers. Given the input feature Fef from the En-\ncoder, we first tokenize it by a 1 × 1 convolutional layer:\nT0 = Hconv(Fef ) ∈ RH×W×D. (5)\nwhere D is the embedding dimension. Then, we excavate\nlocal and global texture dependencies by L TSABs Htsab:\nTl = Htsab(Tl−1), l = 1, 2, ..., L, (6)\nFinally, a 1 ×1 convolutional layer is adopted to align the\nfeature dimension and obtain the output Fgf :\nFgf = Hconv(TL). (7)\nTexture-relevant sparse attention block. The existing\ntransformer-based SR works adopt the window based self-\nattention (SA) as the basic component. However, they can-\nnot effectively aggregate global information since SA is cal-\nculated within limited local range. To address this, we design\nTSAB, which introduces a global cross-attention branch par-\nallel with the local SA branch. It aims to mine effective in-\nformation within local windows and global dependencies.\nGiven the input tokens Tl ∈ RH×W×D from the l-th\nTSAB, we feed them into the local and global branches.\nNote that the token early-exit strategy is performed on Tl\nto get the local tokens Tl\nlocal and global tokens Tl\nglobal.\n(1) Local self-attention branch. Similar to SwinIR (Liang\net al. 2021b), we adopt the standard window-based SA for\nthe local branch. The local tokens Tl\nlocal ∈ R\nHW\nS2 ×S2×D\nconsist of HW\nS2 windows, which are obtained by partition-\ning Tl into non-overlapped S × S local windows. Then, SA\nis performed within each local window feature X ∈ RS2×D\nto get the enhanced feature ˆTl\nlocal:\nQl = WqlX, Kl = WklX, Vl = WvlX, (8)\nˆTl\nlocal = SoftMax(QlKT\nl /\n√\nd)Vl, (9)\nwhere Ql, Kl and Vl are generated by linear projections with\nmatrix Wql, Wkl and Wvl, respectively.\n(2) Global cross-attention branch. The global branch is\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4011\nConfidence\nEstimator\nAdaptive Token Exit \n. . . \nAdaptive\nToken\nExit\nLayer L\nTSABTSAB\n? ??\nHBM:\nsplit mean\nAdaptive Token Exit\nLocal halting mask generation\nConfidence\nEstimator\nCM: \nHSM:\nCM: \nHSM:\n? ?\nHBM:\n? ? ?\nFigure 3: The pipeline of the token early-exit strategy. The confidence map (CM) Cl is measured for the input tokens Tl from\nthe l-th layer. Then, a halting score map (HSM) Rl is calculated by accumulating the confidence map to generate a halting\nbinary mask (HBM) Ml. Finally, with the guide of Ml, only the kept tokens will participate in the attention calculation.\nadopted to remedy the limited receptive field of the local\nSA branch. Inspired by (Yin et al. 2022a), we introduce the\ntexture-relevant prototype P ∈ RM×D as the query to re-\ntrieve the global edge and texture, which is initialized as 1\nand learns M texture-shared prototypes as seed vectors. It\nacts as the role of class token in image classification, which\nrepresents the global attention of the image by weight allo-\ncation to other tokens to some extent. Therefore, the cross-\nattention (CA) between the texture-relevant prototypeP and\nthe global tokens Tl\nglobal is calculated as:\nQp = WqpP, Kg = WkgTl\nglobal, Vg = WvgTl\nglobal, (10)\nˆP = SoftMax(QpKT\ng /\n√\nd)Vg, (11)\nwhere ˆP is the learned informative prototype, Qp ∈ RM×D\nand Kg, Vg ∈ RHW ×D are the query, key and value tensors,\nprojected by learnable linear matrix Wqp, Wkg and Wvg.\nNext, we aggregate informative texture by performing cross-\nattention between the global tokens Tl\nglobal with ˆP:\nQg = WqgTl\nglobal, Kp = Wkp ˆP, Vp = Wvp ˆP, (12)\nˆTl\nglobal = SoftMax(QgKT\np /\n√\nd)Vp, (13)\nwhere Wqg, Wkp and Wvp are learnable matrices. Finally,\nwe project the element-wise sum of these extracted local and\nglobal features by a multi-layer perception (MLP) to obtain\nthe output of the l-th TSAB as:\nTl+1 = MLP(ˆTl\nlocal + ˆTl\nglobal) ∈ RH×W×D, (14)\nwhere MLP follows a standard structure composed of 1 × 1\nconvolution, GLUE, and 1 × 1 convolution layers.\nToken Early-Exit Strategy\nInspired by the dynamic inference in high-level tasks (Yin\net al. 2022b; Liang 2022), we propose a token early-exit\nstrategy to speed up the model inference. The whole pipeline\nis depicted in Fig. 3. The core idea is to adaptively halt\nthe SA calculation of well-recovered tokens as the network\ndepth increases. Unlike image classification where the im-\nportance of each token depends on its contribution to the\nclassified result, the main challenge for image SR is how to\nprovide accurate halting signals.\nSpecifically, we adopt a confidence estimator to calculate\nthe token importance, which indicates the recovery degree\nof tokens in the TSAB. The higher the confidence, the better\nthe recovery effect. In order to halt tokens at different depths,\nthe confidence map is progressively accumulated to obtain\nthe halting score map, thus generating a binary halting mask\nto indicate whether the current token should exit or not.\nConfidence estimation. Given the intermediate tokensTl ∈\nRH×W×D from the l-th TSAB, the confidence map Cl ∈\nRH×W×1 for Tl is measured by a weight-shared lightweight\nconfidence estimator, which consists of Conv-Tanh-Conv-\nSigmoid layers, i.e.,\nCl = Sigmoid(Conv(Tanh(Conv(Tl)))). (15)\nwhere Conv denotes a 3 × 3 convolutional layer.\nInspired by (Ning et al. 2021), we adopt aleatoric uncer-\ntainty to perform confidence estimation, which aims to trans-\nform texture and edge pixels with high uncertainty into low-\nconfidence representations, and flat regions with low un-\ncertainty into high-confidence representations. Specifically,\ngiven the LR image Ilr and the corresponding HR image\nIhr and the SR image Isr, the aleatoric uncertainty can be\nmodeled with an additional parameter term θ. In order to\naccurately estimate θ, Laplace distribution is used to model\nthe Likelihood Function, which can be formulated as:\nln p (Ihr, θ | Ilr) =−∥Ihr − Isr∥1\nθ − ln θ − ln 2. (16)\nTo transform the uncertainty estimation into the confi-\ndence estimation, we model θ = 1\n(Cl)↑, where the bilinear\ninterpolation (·) ↑ is adopted to upsample Cl to align the\nsize with Ihr. Then, Eq. (16) can be reformulated as:\nLUSL =\nLX\nl=1\n(Cl ∥Ihr − Isr∥1 + log 1\n(Cl + ϵ)), (17)\nwhere ϵ = 1e−8 is a small constant for stable training. By\nthe confidence estimation about the recovery credibility of\neach token, the tokens with high enough confidence can be\nhalted in the current layer to reduce inference time.\nSparse attention with halting mask. Here, we present how\nto calculate the sparse attention in the local and global\nbranches of the TSAB with dynamic token selection. To\nachieve this, we first calculate the halting score map Rl ∈\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4012\nRH×W×1 by accumulating Cl layer by layer:\nRl = Rl−1 + Cl−1. (18)\nWhen Rl exceeds some threshold, we can obtain a halting\nmask to indicate whether the tokens should halt.\nIn the local SA branch, the local halting mask Mloc is\ngenerated to progressively reduce well-recovered patch win-\ndows. As depicted in the upper right corner of Fig. 3, we\npartition the halting score map Rl ∈ RH×W×1 into non-\noverlapping windows with the size of HW\nS2 × S2 × 1, and\ncalculate the mean value within each window to obtainRl\nloc.\nThen, Mloc can be obtained by:\nMloc =\n(\n0 if Rl\nloc ≤ 1 − ϵh,\n1 if Rl\nloc ≥ 1 − ϵh, (19)\nwhere ϵh is a small positive constant. The local window with\nMloc = 1 will be halted for the next attention calculation.\nTherefore, the input token Tl\nlocal for the local branch is:\nTl\nlocal ← Tl ⊙ (1 − Mloc). (20)\nNext, the SA calculation within each kept window is per-\nformed as Eq. (8) and Eq. (9) to obtain ˆTl\nlocal during the\ninference and the number of local tokens entered in TSAB\nis less than HW/S 2. And then we reshape ˆTl\nlocal and Mloc\nback to the size of H × W × D and H × W × 1, respec-\ntively. The original input value of halted windows are added,\nso the output of the local branch is represented as:\nˆTl\nlocal ← ˆTl\nlocal ⊙ (1 − Mloc) +Tl ⊙ Mloc. (21)\nIn the global CA branch, the global halting mask Mglo is\ncalculated for each spatial position as follows:\nMglo =\n(\n0 if Rl ≤ 1 − ϵh,\n1 if Rl ≥ 1 − ϵh, ∈ RH×W×1 (22)\nWhen Mij\nglo = 1, it means that the token in the position(i, j)\nis credible enough to be halted. Conversely, it will be fed to\nthe next processing. With the guide ofMglo, the kept tokens\nTl\nglobal are selected from Tl with Mglo = 0as:\nTl\nglobal ← Tl ⊙ (1 − Mglo). (23)\nThen, Tl\nglobal is flattened to RHW ×D to calculate CA as\nEq. (13). Finally, ˆTglobal and Mglo are reshaped to the origi-\nnal dimensions RH×W×D and RH×W×1, and add the orig-\ninal halt tokens to get the output of the global branch:\nˆTl\nglobal ← ˆTl\nglobal ⊙ (1 − Mglo) +Tl ⊙ Mglo. (24)\nTraining Objective\nOur AdaFormer adopts the commonly used L1 loss and the\nuncertainty loss in Eq. (17) as the training objective, i.e.,\nL = L1 + αLUSL , (25)\nwhere α is a regularization coefficient, empirically set as 1.\nExperiments\nExperimental Setup\nDatasets. We use DIV2K (Timofte et al. 2017) (0001-0800)\nas the training dataset and evaluate our model on 100 images\n(0801-0900) of DIV2K. Following ClassSR (Kong et al.\n2021), the model is tested on 300 images (1201-1500) from\nthe DIV8K (Gu et al. 2019) for 4× SR, which consists of\nTest2K, Test4K, and Test8K. The LR images are captured\nby bicubic downsampling to HR images. To further illustrate\nthe effectiveness and robustness of our proposal, we also test\non four SR benchmarks: Set5, Set14, B100 and Urban100.\nImplementation details. The hyperparameters of\nAdaFormer are set as: the number of residual blocks\nin the Encoder and Decoder is 8; the number of TSABs L is\n6; the number of feature channels C is 64, the embedding\ndimension D is 64, ϵh is 0.05 and M is 16. Following\nSwinIR, the window size S in the local branch of TSAB\nis 8. During training, we randomly crop 16 LR patches\nwith the size of 48 × 48 as the input, which are further\naugmented by randomly rotated with 90◦, 180◦, 270◦ and\nflipped horizontally. ADAM optimizer with β1 = 0 .9,\nβ2 = 0.999 and ϵ = 10−8 is adopted to train the model.\nThe learning rate is initialized as 2 × 10−4 and decreased\nby half every 200 epochs. We implement our model using\nPyTorch on 1 NVIDIA 2080Ti GPU and train for 1000\nepochs in total. PSNR and SSIM are adopted as objective\nmetrics, which are measured on the Y channel of YCbCr\nspace. Besides, we present the inference time (Latency) and\nFLOPs to measure the model complexity, which are both\nmeasured by averaging each benchmark.\nComparisons with the State-of-arts\nQuantitative results. To demonstrate the effectiveness of\nAdaFormer, we first compare with state-of-the-art effi-\ncient transformer-based SR models, including SwinIR-light\n(Liang et al. 2021a), ELAN-light (Zhang et al. 2022) and N-\nGram (Choi, Lee, and Yang 2023) on large images for 4×\nSR. In Tab. 1, it is observed that AdaFormer obtains compa-\nrable performance with less latency. Especially, it reduces\n281ms latency and gains 0.05dB improvement in PSNR\nwhen compared with ELAN-light on Test8K. Meanwhile,\nwe compare with several efficient CNN-based SR methods\non four standard benchmarks in Tab. 2, i.e., IMDN (Hui et al.\n2019), RFDN (Liu, Tang, and Wu 2020), RLFN (Kong et al.\n2022), LAPAR-A (Li et al. 2020), and ETDS (Chao et al.\n2023). Compared with CNN-based methods, transformer-\nbased methods perform much better in accuracy, while infe-\nrior in latency. Compared with transformer-based methods,\nAdaFormer has obvious superiority in latency time, which\nis nearly 1.22× faster than ELAN-light with comparable re-\nsults on B100. Our method unites the advantages of trans-\nformer and CNN, balancing latency and accuracy well.\nQualitative results. The visual comparisons on Test2K and\nTest4K datasets are presented in Fig. 5. For “1215” in\nTest2K, our AdaFormer recovers more clear edge and tex-\nture details than ELAN-light and SwinIR-light. Especially,\nour AdaFormer obtains a clear boundary between leaves and\nbranches. For “1318” in Test4K, our AdaFormer also obtains\nmore favorable results on the building texture than other\nmethods. Therefore, the proposed AdaFormer has the supe-\nriority of capturing better structural information.\nThe sparsity of halting mask. As shown in Fig. 4, we an-\nalyze the sparsity (the percentage of halted tokens to all to-\nkens) of the global and local halting masks on Test4K and\nSet14 datasets. It shows that the sparsity gradually increases\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4013\nModel DIV2K Test2K Test4K Test8K\nPSNR SSIM Latency(ms) PSNR SSIM Latency(ms) PSNR SSIM Latency(ms) PSNR SSIM Latency(ms)\nSwinIR-light 30.60 0.8427 934.96 27.69 0.7786 466.48 29.11 0.8249 2103.71 35.04 0.8969 11766.19\nELAN-light 30.60 0.8417 147.26 27.69 0.7778 82.61 29.12 0.8246 300.31 35.08 0.8970 1636.38\nN-Gram 30.60 0.8418 345. 27.70 0.7780 508.15 29.10 0.8245 1908.14 35.08 0.8971 3441.76\nAdaFormer (ours) 30.63 0.8424 112.35 27.70 0.7782 71.99 29.15 0.8252 257.94 35.13 0.8974 1355.34\nTable 1: The quantitative (PSNR(dB)/SSIM) and latency (ms) comparisons with different efficient transformer-based SR models\non DIV2K, Test2K, Test4K, and Test8K for4× SR. The best and second best results are highlighted in bold and underline.\nMethod Set5 Set14 B100 Urban100\nPSNR SSIM Latency(ms) PSNR SSIM Latency(ms) PSNR SSIM Latency(ms) PSNR SSIM Latency(ms)\nIMDN 32.21 0.9605 8.40 28.58 0.7811 8.78 27.56 0.7353 6.92 26.04 0.7838 12.22\nLAPAR-A 32.15 0.8944 12.28 28.61 0.7818 17.75 27.61 0.766 12.89 26.14 0.7871 37.14\nRFDN 32.28 0.8957 44.22 28.61 0.7818 19.55 27.58 0.7363 17.96 26.20 0.7883 29.55\nRLFN 32.24 0.8952 - 28.62 0.7813 - 27.60 0.7364 - 26.17 0.7877 -\nETDS 31.69 0.8889 11.19 28.31 0.7751 11.39 27.37 0.7302 13.22 25.47 0.7643 8.49\nSwinIR-light 32.44 0.8976 54.41 28.77 0.7858 79.83 27.69 0.7406 64.02 26.47 0.7980 202.21\nESRT 32.19 0.8947 22.75 28.69 0.7833 26.60 27.69 0.7379 21.54 26.39 0.7962 88.22\nELAN-light 32.43 0.8975 36.06 28.78 0.7858 29.52 27.69 0.7406 27.06 26.54 0.7982 59.11\nN-Gram 32.33 0.8963 82.76 28.78 0.7859 116.05 27.66 0.7396 96.26 26.45 0.7963 238.51\nAdaFormer (ours) 32.43 0.8974 22.28 28.80 0.7858 25.28 27.70 0.7407 20.87 26.48 0.7982 55.05\nTable 2: The quantitative (PSNR(dB)/SSIM) and latency (ms) comparisons with different efficient SR models on benchmark\ndatasets for 4× SR. The best and second best results are highlighted in bold and underline.\nFigure 4: The sparsity of the global and local halting masks\nof TSAB for different depths on Test4K and Set14.\nas the number of TSAB increases. Especially, the sparsity\non Test4K is almost 90% in the last TSAB. Therefore, it\ndemonstrates that our adaptive token-exit strategy reduces\nlatency and computational costs substantially.\nVisualization of confidence map and halting mask. We vi-\nsualize the learned confidence map and the halting mask on\nTest4K in Fig. 6. Remarkably, our method adaptively halts\ntokens according to different restoration difficulties. For flat\nregions, plenty of the tokens are halted in the early stage to\nspeed up the inference, while for regions with more com-\nplicated textures and edges, the model tends to keep the to-\nkens restored until they reach the required confidence score.\nTherefore, it demonstrates the reliability of the exit signals\nguided by the uncertainty constraint.\nAblation Study\nThe ablation analysis includes the local and global branches\nand the early-exit strategy. Note that all SR models in the\nablation are trained for 400 epochs and tested on Test4K.\nBreak-down ablation. As shown in Tab. 3, we perform an\nablation to investigate the effect of different components in\nAdaFormer, which includes the following variants: Case 1:\nfollowing SwinIR (Liang et al. 2021a), we adopt the local\nbranch as the baseline model, which calculates the SA in a\nlocal sliding window. It obtains 29.03dB on Test4K. Case\n2: introducing the global CA branch based on the baseline\nmodel. It gains by 0.03dB with 42G FLOPs and 92ms la-\ntency increase. This means that the global CA branch can\nlead to an increase in performance, FLOPs, and latency.\nCase 3: adopting the early-exit strategy on Case 2 with the\nlocal halting mask. The FLOPs and latency are reduced by\n10G and 72ms with comparable performance. Case 4: ap-\nplying the early-exit strategy on both branches. The Flops\nand latency are further reduced by 63G and 77ms, while the\nperformance is slightly improved by 0.06dB. It also shows\nthat the token early-exit strategy reduces FLOPs and infer-\nence time. Meanwhile, it alleviates the overfitting problem\nand unnecessary noise by performing attention calculations\non the informative tokens Therefore, our method strikes an\nexcellent tradeoff between latency and accuracy.\nEarly-exit strategy comparison. To validate the effective-\nness of our token early-exit strategy, we compare it with\nseveral other halting strategies as shown in Tab. 4. 1) Com-\nparison to uniform-exit and random-exit.We first compare\nwith uniform-exit and random-exit strategies. It is observed\nthat our strategy obtains 0.32dB and 0.25dB improvement\nin PSNR with lower FLOPs and inference time. 2) Com-\nparison to A-Vit.Following the exit strategy of A-Vit (Yin\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4014\n“1215” from Test2K\n HR SwinIR ELAN Ours\n “1318” from Test4K\n HR SwinIR ELAN Ours\nFigure 5: Visual comparisons with the state-of-the-art methods on Test2K and Test4K for4× SR. Zoom in for a better view.\n‘1376’ from Test4K Global Mask from Layer.1 Global Mask from Layer.6\nConfidence Map from Layer.6 Local Mask from Layer.1 Local Mask from Layer.6\nFigure 6: Visualization of the confidence map (left bottom) and binary halting mask (right) for the original image (left upper). In\nthe confidence map, light/dark color represents a higher/lower confidence score requiring less/more computation. In the halting\nmask, the halting pixel/patch is represented by the white color. Please zoom in for a better view.\nCase TSAB Early-exit Test4K\nLB GB LM GM FLOPs(G) Latency(ms) PSNR(dB)\n1 ✓ 654.25 314.98 29.03\n2 ✓ ✓ 696.08 406.83 29.06\n3 ✓ ✓ ✓ 686.83 334.69 29.04\n4 ✓ ✓ ✓ ✓ 623.51 257.94 29.10\nTable 3: Ablation study of the proposed AdaFormer on\nTest4K dataset for 4× SR.\nEarly-Exit Strategy PSNR (dB) Latency(ms) FLOPs (G)\nUniform Exit 28.78 310.01 671.78\nRandom Exit 28.85 238.10 653.67\nA-Vit 28.81 374.87 648.52\nAPE 29.10 499.45 664.92\nWU estimator 29.09 279.92 666.44\nWS estimator 29.10 267.24 623.51\nTable 4: Quantitative comparisons of different early-exit\nstrategies on Test4K for 4× SR.\net al. 2022b), we adopt the ponder loss to encourage early\nexit and regularize the halting distribution towards Gaussian\ndistribution using KL divergence. It suffers a 0.29dB drop\nin PSNR with higher latency and FLOPs compared to our\nmethod. The reason is that A-Vit is designed for image clas-\nsification task, which adopts predefined prior knowledge to\nconstrain the token halting distribution to Gaussian distribu-\ntion. Since objects in classification tasks are mainly concen-\ntrated in the image center, it is consistent with the fact that\nmost samples in ImageNet are centered. However, image SR\nis more concerned with the image texture than the central\nobject, so it is unreasonable to use predefined priors for the\nconstraints. 3) Comparison to APE.APE (Wang et al. 2022)\nintroduces an incremental capacity measured by PSNR for\nCNN-based methods to judge whether the patch should exit\nor not. We apply the incremental capacity as an exit signal\nto train the transformer-based SR model. It shows that our\nproposal reduces the inference time by nearly50% with sim-\nilar performance. Besides, we compare weight-shared (WS)\nand weight-unshared (WU) confidence estimators , showing\nthat the shared-weight estimator performs better with less\nlatency. Therefore, it demonstrates that our adaptive token\nearly-exit strategy is superior in performance and efficiency.\nConclusion\nIn this paper, we propose an adaptive token sparsification\nmodule with an early-exit strategy to accelerate the infer-\nence of the transformer for image SR. The key idea is using\na confidence estimator constrained by an uncertainty-driven\nloss to obtain the binary halting mask, which provides a halt-\ning signal for each token to indicate its recovery importance.\nBesides, a texture-relevant sparse attention block is designed\nfor local and global texture information interaction. Exten-\nsive experimental results show that our proposal outper-\nforms the mainstream efficient transformer-based methods\nin less latency with comparable performance.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4015\nAcknowledgments\nThe authors, except Yun Fu, were supported by Na-\ntional Natural Science Foundation of China under\nGrant No.62176224, No.62222602, No.62176092;\nNatural Science Foundation of Chongqing under\nNo.CSTB2023NSCOJOX0007, CCF-Lenovo Blue Ocean\nResearch Fund.\nReferences\nCai, Q.; Qian, Y .; Li, J.; Lyu, J.; Yang, Y .; Wu, F.; and Zhang,\nD. 2023. HIPA: Hierarchical Patch Transformer for Single\nImage Super Resolution. TIP.\nChao, J.; Zhou, Z.; Gao, H.; Gong, J.; Yang, Z.; Zeng, Z.;\nand Dehbi, L. 2023. Equivalent Transformation and Dual\nStream Network Construction for Mobile Image Super-\nResolution. In CVPR.\nChen, H.; Wang, Y .; Guo, T.; Xu, C.; Deng, Y .; Liu, Z.; Ma,\nS.; Xu, C.; Xu, C.; and Gao, W. 2021. Pre-trained image\nprocessing transformer. In CVPR.\nChoi, H.; Lee, J.; and Yang, J. 2023. N-Gram in Swin Trans-\nformers for Efficient Lightweight Image Super-Resolution.\nDong, C.; Loy, C. C.; He, K.; and Tang, X. 2016. Im-\nage Super-Resolution Using Deep Convolutional Networks.\nTPAMI.\nFayyaz, M.; Koohpayegani, S. A.; Jafari, F. R.; Sommerlade,\nE.; Joze, H. R. V .; Pirsiavash, H.; and Gall, J. 2021. ATS:\nAdaptive Token Sampling For Efficient Vision Transform-\ners. abs/2111.15667.\nFeng, Z.; and Zhang, S. 2023. Efficient Vision Transformer\nvia Token Merger. TIP.\nGu, S.; Lugmayr, A.; Danelljan, M.; Fritsche, M.; Lamour,\nJ.; and Timofte, R. 2019. Div8k: Diverse 8k resolution im-\nage dataset. In ICCVW.\nHsu, T.; Liao, Y .; and Huang, C. 2023. Video Summarization\nWith Spatiotemporal Vision Transformer. TIP.\nHui, Z.; Gao, X.; Yang, Y .; and Wang, X. 2019. Lightweight\nImage Super-Resolution with Information Multi-distillation\nNetwork. In ACM MM.\nKong, F.; Li, M.; Liu, S.; Liu, D.; He, J.; Bai, Y .; Chen, F.;\nand Fu, L. 2022. Residual Local Feature Network for Effi-\ncient Super-Resolution. In CVPR.\nKong, X.; Zhao, H.; Qiao, Y .; and Dong, C. 2021. ClassSR:\nA General Framework to Accelerate Super-Resolution Net-\nworks by Data Characteristic. In CVPR.\nLi, W.; Zhou, K.; Qi, L.; Jiang, N.; Lu, J.; and Jia, J.\n2020. LAPAR: Linearly-Assembled Pixel-Adaptive Regres-\nsion Network for Single Image Super-resolution and Be-\nyond. In NeurIPS.\nLi, Y .; Fan, Y .; Xiang, X.; Demandolx, D.; Ranjan, R.; Tim-\nofte, R.; and Van Gool, L. 2023. Efficient and explicit mod-\nelling of image hierarchies for image restoration. In CVPR.\nLiang, J.; Cao, J.; Sun, G.; Zhang, K.; Van Gool, L.; and\nTimofte, R. 2021a. Swinir: Image restoration using swin\ntransformer. In ICCV.\nLiang, J.; Cao, J.; Sun, G.; Zhang, K.; Van Gool, L.; and\nTimofte, R. 2021b. Swinir: Image restoration using swin\ntransformer. In ICCVW.\nLiang, Y . 2022. EViT: Expediting Vision Transformers via\nToken Reorganizations. In ICLR.\nLiu, J.; Tang, J.; and Wu, G. 2020. Residual Feature Distil-\nlation Network for Lightweight Image Super-Resolution. In\nBartoli, A.; and Fusiello, A., eds., ECCVW.\nLu, Z.; Li, J.; Liu, H.; Huang, C.; Zhang, L.; and Zeng, T.\n2022. Transformer for single image super-resolution. In\nCVPRW.\nLuo, X.; Qu, Y .; Xie, Y .; Zhang, Y .; Li, C.; and Fu, Y .\n2023. Lattice Network for Lightweight Image Restoration.\nTPAMI.\nMeng, L.; Li, H.; Chen, B.; Lan, S.; Wu, Z.; Jiang, Y .; and\nLim, S. 2022. AdaViT: Adaptive Vision Transformers for\nEfficient Image Recognition. In CVPR.\nNing, Q.; Dong, W.; Li, X.; Wu, J.; and Shi, G.\n2021. Uncertainty-Driven Loss for Single Image Super-\nResolution. In NeurIPS.\nPang, Y .; Zhao, X.; Zhang, L.; and Lu, H. 2023.\nCA VER: Cross-Modal View-Mixed Transformer for Bi-\nModal Salient Object Detection. TIP.\nRao, Y .; Zhao, W.; Liu, B.; Lu, J.; Zhou, J.; and Hsieh, C.-J.\n2021. Dynamicvit: Efficient vision transformers with dy-\nnamic token sparsification. NeurIPS.\nRyoo, M. S.; Piergiovanni, A. J.; Arnab, A.; Dehghani, M.;\nand Angelova, A. 2021. TokenLearner: What Can 8 Learned\nTokens Do for Images and Videos? abs/2106.11297.\nShi, W.; Caballero, J.; Huszar, F.; Totz, J.; Aitken, A. P.;\nBishop, R.; Rueckert, D.; and Wang, Z. 2016. Real-Time\nSingle Image and Video Super-Resolution Using an Effi-\ncient Sub-Pixel Convolutional Neural Network. In CVPRW.\nTimofte, R.; Agustsson, E.; Van Gool, L.; Yang, M.-H.; and\nZhang, L. 2017. Ntire 2017 challenge on single image super-\nresolution: Methods and results. In CVPRW.\nWang, S.; Liu, J.; Chen, K.; Li, X.; Lu, M.; and Guo, Y .\n2022. Adaptive Patch Exiting for Scalable Single Image\nSuper-Resolution. In ECCV.\nXie, W.; Song, D.; Xu, C.; Xu, C.; Zhang, H.; and Wang,\nY . 2021. Learning Frequency-aware Dynamic Network for\nEfficient Super-Resolution. In ICCV.\nXu, Y .; Zhang, Z.; Zhang, M.; Sheng, K.; Li, K.; Dong, W.;\nZhang, L.; Xu, C.; and Sun, X. 2022. Evo-ViT: Slow-Fast\nToken Evolution for Dynamic Vision Transformer. InAAAI.\nYin, D.; Ren, X.; Luo, C.; Wang, Y .; Xiong, Z.; and Zeng, W.\n2022a. Retriever: Learning Content-Style Representation as\na Token-Level Bipartite Graph. In ICLR.\nYin, H.; Vahdat, A.; Alvarez, J. M.; Mallya, A.; Kautz, J.;\nand Molchanov, P. 2022b. A-ViT: Adaptive Tokens for Effi-\ncient Vision Transformer. In CVPR.\nZhang, X.; Zeng, H.; Guo, S.; and Zhang, L. 2022. Ef-\nficient Long-Range Attention Network for Image Super-\nResolution. In ECCV.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4016",
  "topic": "Security token",
  "concepts": [
    {
      "name": "Security token",
      "score": 0.7105008363723755
    },
    {
      "name": "Transformer",
      "score": 0.5620149970054626
    },
    {
      "name": "Computer science",
      "score": 0.5221691131591797
    },
    {
      "name": "Image (mathematics)",
      "score": 0.4105912446975708
    },
    {
      "name": "Artificial intelligence",
      "score": 0.36651402711868286
    },
    {
      "name": "Electrical engineering",
      "score": 0.25129926204681396
    },
    {
      "name": "Computer network",
      "score": 0.16448158025741577
    },
    {
      "name": "Voltage",
      "score": 0.1409892737865448
    },
    {
      "name": "Engineering",
      "score": 0.10402056574821472
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I191208505",
      "name": "Xiamen University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I66867065",
      "name": "East China Normal University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I87182695",
      "name": "Universidad del Noreste",
      "country": "MX"
    }
  ],
  "cited_by": 10
}