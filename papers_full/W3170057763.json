{
  "title": "Neural Language Modeling for Contextualized Temporal Graph Generation",
  "url": "https://openalex.org/W3170057763",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2701979523",
      "name": "Aman Madaan",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2098495947",
      "name": "Yi-Ming Yang",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2132022337",
    "https://openalex.org/W2157275230",
    "https://openalex.org/W2983354073",
    "https://openalex.org/W2892202918",
    "https://openalex.org/W2996287690",
    "https://openalex.org/W2949611472",
    "https://openalex.org/W1513657772",
    "https://openalex.org/W2760579680",
    "https://openalex.org/W2089458547",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2950523284",
    "https://openalex.org/W2988647680",
    "https://openalex.org/W2161484642",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2116492146",
    "https://openalex.org/W1564519640",
    "https://openalex.org/W2963101081",
    "https://openalex.org/W2951101948",
    "https://openalex.org/W2561529111",
    "https://openalex.org/W1965142824",
    "https://openalex.org/W1529628505",
    "https://openalex.org/W2806115886",
    "https://openalex.org/W2251325107",
    "https://openalex.org/W2950339735",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2420671943",
    "https://openalex.org/W2964207259",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W3015571765",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2095293504",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2964263366",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2185599447",
    "https://openalex.org/W2729046720",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W3094500738",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2741237963",
    "https://openalex.org/W2047940964",
    "https://openalex.org/W3106477919",
    "https://openalex.org/W2028559148",
    "https://openalex.org/W1944671275",
    "https://openalex.org/W2123167824",
    "https://openalex.org/W2647907601",
    "https://openalex.org/W2890515900",
    "https://openalex.org/W3150468745",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W2970942496",
    "https://openalex.org/W2158794898",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3099246072",
    "https://openalex.org/W2151295812",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W135031542",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2130942839"
  ],
  "abstract": "This paper presents the first study on using large-scale pre-trained language models for automated generation of an event-level temporal graph for a document. Despite the huge success of neural pre-training methods in NLP tasks, its potential for temporal reasoning over event graphs has not been sufficiently explored. Part of the reason is the difficulty in obtaining large training corpora with human-annotated events and temporal links. We address this challenge by using existing IE/NLP tools to automatically generate a large quantity (89,000) of system-produced document-graph pairs, and propose a novel formulation of the contextualized graph generation problem as a sequence-to-sequence mapping task. These strategies enable us to leverage and fine-tune pre-trained language models on the system-induced training data for the graph generation task. Our experiments show that our approach is highly effective in generating structurally and semantically valid graphs. Further, evaluation on a challenging hand-labeled, out-of-domain corpus shows that our method outperforms the closest existing method by a large margin on several metrics. We also show a downstream application of our approach by adapting it to answer open-ended temporal questions in a reading comprehension setting.",
  "full_text": "Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 864–881\nJune 6–11, 2021. ©2021 Association for Computational Linguistics\n864\nNeural Language Modeling for Contextualized Temporal Graph\nGeneration\nAman Madaan, Yiming Yang\nLanguage Technologies Institute, Carnegie Mellon University\nPittsburgh, PA, USA\namadaan@cs.cmu.edu\nAbstract\nThis paper presents the ﬁrst study on using\nlarge-scale pre-trained language models for\nautomated generation of an event-level tem-\nporal graph for a document. Despite the\nhuge success of neural pre-training methods in\nNLP tasks, its potential for temporal reasoning\nover event graphs has not been sufﬁciently ex-\nplored. Part of the reason is the difﬁculty in\nobtaining large training corpora with human-\nannotated events and temporal links. We ad-\ndress this challenge by using existing IE/NLP\ntools to automatically generate a large quantity\n(89,000) of system-produced document-graph\npairs, and propose a novel formulation of the\ncontextualized graph generation problem as a\nsequence-to-sequence mapping task. These\nstrategies enable us to leverage and ﬁne-tune\npre-trained language models on the system-\ninduced training data for the graph generation\ntask. Our experiments show that our approach\nis highly effective in generating structurally\nand semantically valid graphs. Further, eval-\nuation on a challenging hand-labeled, out-of-\ndomain corpus shows that our method outper-\nforms the closest existing method by a large\nmargin on several metrics. We also show a\ndownstream application of our approach by\nadapting it to answer open-ended temporal\nquestions in a reading comprehension setting.1\n1 Introduction\nTemporal reasoning is crucial for analyzing the in-\nteractions among complex events and producing\ncoherent interpretations of text data (Duran et al.,\n2007). There is a rich body of research on the\nuse of temporal information in a variety of impor-\ntant application domains, including topic detection\nand tracking (Makkonen et al., 2003), information\nextraction (Ling and Weld, 2010), parsing of clin-\nical records (Lin et al., 2016), discourse analy-\n1Code and pre-trained models available at https://\ngithub.com/madaan/temporal-graph-gen\nsis (Evers-Vermeul et al., 2017), and question an-\nswering (Ning et al., 2020).\nGraphs are a natural choice for representing the\ntemporal ordering among events, where the nodes\nare the individual events, and the edges capture\ntemporal relationships such as “before”, “after” or\n“simultaneous”. Representative work on automated\nextraction of such graphs from textual documents\nincludes the early work by Chambers and Jurafsky\n(2009), where the focus is on the construction of\nevent chains from a collection of documents, and\nthe more recent CAEVO (Chambers et al., 2014) and\nCogcomptime (Ning et al., 2018), which extract\na graph for each input document instead. These\nmethods focus on rule-based and statistical sub-\nmodules to extract verb-centered events and the\ntemporal relations among them. As an emerging\narea of NLP , large scale pre-trained language mod-\nels have made strides in addressing challenging\ntasks like commonsense knowledge graph comple-\ntion (Bosselut et al., 2019) and task-oriented dialog\ngeneration (Budzianowski and Vuli´c, 2019). These\nsystems typically ﬁne-tune large language models\non a corpus of a task-speciﬁc dataset. However,\nthese techniques have not been investigated for\ntemporal graph extraction.\nThis paper focuses on the problem of generation\nof an event-level temporal graph for each docu-\nment, and we refer to this task as contextualized\ngraph generation. We address this open challenge\nby proposing a novel reformulation of the task as a\nsequence-to-sequence mapping problem (Sutskever\net al., 2014), which enables us to leverage large pre-\ntrained models for our task. Further, different from\nexisting methods, our proposed approach is com-\npletely end-to-end and eliminates the need for a\npipeline of sub-systems commonly used by tradi-\ntional methods.\nWe also address a related open challenge, which\nis a prerequisite to our main goal: the difﬁculty of\nobtaining a large quantity of training graphs with\n865\nFigure 1: Task overview: given a document (left), automatically extract a temporal graph (right).\nhuman-annotated events and temporal relations. To\nthis end, we automatically produce a large collec-\ntion of document-graph pairs by using CAEVO , fol-\nlowed by a few rule-based post-processing steps\nfor pruning and noise reduction. We then encode\nthe graph in each training pair as a string in the\ngraph representation format DOT, transforming the\ntext-to-graph mapping into sequence-to-sequence\nmapping. We ﬁne-tune GPT-2 on this dataset of\ndocument-graph pairs, which yields large perfor-\nmance gains over strong baselines on system gener-\nated test set and outperforms CAEVO on TimeBank-\nDense (Cassidy et al., 2014) on multiple metrics.\nFigure 1 shows an example of the input document\nand the generated graph by our system. In sum-\nmary, our main contributions are:\n1. We present the ﬁrst investigation on using\nlarge pre-trained language models for contex-\ntualized temporal event graph generation by\nproposing a new formulation of the problem\nas a sequence-to-sequence mapping task.\n2. We address the difﬁculty of obtaining a large\ncollection of human-annotated graphs, which\nis crucial for effective ﬁne-tuning of pre-\ntrained models, by automatically producing a\ncollection of 89,000 document-graph pairs.\n3. Our experimental results on both the system-\ngenerated test set (which allows us to com-\npare the relative performance of different\nmodels) and a hand-labeled, out-of-domain\ndataset (TimeBank-Dense), show the advan-\ntage of our proposed approach over strong\nbaselines. Further, we show that our approach\ncan help in generating plausible answers for\nopen ended-temporal questions in a reading\ncomprehension dataset, Torque (Ning et al.,\n2020).\n2 Related Work\nTemporal Graph Extraction Tempeval-3 (Uz-\nZaman et al., 2013) introduced the task of temporal\ngraph extraction as “the ultimate task for evaluating\nan end-to-end system that goes from raw text to\nTimeML annotation”. Notable systems developed\nin response include CAEVO (Chambers et al., 2014),\nfollowed by the more recent Cogcomptime (Ning\net al., 2018). Both CAEVO and Cogcomptime use\nseveral statistical and rule-based methods like event\nextractors, dependency parsers, semantic role la-\nbelers, and time expression identiﬁers for the task.\nOur work differs from these systems in both the\nmethodology and desired result in the following\nways: i) Instead of using specialized sub-systems,\nwe transform the task into a sequence-to-sequence\nmapping problem and use a single language model\nto generate such temporal graphs in an end-to-end\nfashion from text, subsuming all the intermediate-\nsteps. ii) We develop our system using a corpus\nof 89,000 documents, which is ∼300x larger com-\n866\npared to datasets used by CAEVO (36 documents)\nand Cogcomptime on (276 documents); iii) We re-\nmove the noisy events included by CAEVO , but do\nnot limit the extracted events to any speciﬁc seman-\ntic axis as done by Cogcomptime; and ﬁnally, iv)\nOur method generates graphs where the nodes are\nnot simple verbs but augmented event phrases, con-\ntaining the subject and the object of each verb. We\nuse CAEVO over Cogcomptime to generate a large-\nscale corpus for our task and to evaluate our system\nfor the following reasons: i) We foundCAEVO to be\nmuch more scaleable, a critical feature for our task\nof annotating close to 100k documents, ii) CAEVO\nover-generates (and not excludes) verbs from its\noutput, giving us the ﬂexibility to ﬁlter out noisy\nevents without inadvertently missing out on any\ncritical events. However, our method makes no as-\nsumption speciﬁc to CAEVO and is adaptable to any\nother similar system (including Cogcomptime).\nTemporal relation extraction We note that the\nproblem of temporal graph extraction is different\nfrom the more popular task of Temporal relation ex-\ntraction (Temprel), which deals with classifying the\ntemporal link between two already extracted events.\nState of the art Temprel systems use neural meth-\nods (Ballesteros et al., 2020; Ning et al., 2019b;\nGoyal and Durrett, 2019; Han et al., 2019; Cheng\nand Miyao, 2017), but typically use a handful of\ndocuments for their development and evaluation.\nVashishtha et al. (2019) are a notable exception by\nusing Amazon Mechanical Turks to obtain manual\nannotations over a larger dataset of 16,000 sen-\ntences. We believe that the techniques presented in\nour work can be applied to scale the corpus used\nfor training Temprel systems.\nLanguage Models for Graph Generation Re-\ncently, Bosselut et al. (2019) proposed COMET , a\nsystem that ﬁne-tunes GPT (Radford et al., 2018) on\ncommonsense knowledge graphs likeATOMIC (Sap\net al., 2019) and conceptnet (Speer et al., 2017) for\ncommonsense kb completion. Similar to COMET ,\nwe adopt large-scale language models for such a\nconditional generation of text. However, our task\ndiffers from COMET in the complexity of both the\nconditioning text and generated text: we seek to\ngenerate temporal graphs grounded in a document,\nwhereas COMET generates a short event/concept\nphrase conditioned on a relation and an input\nevent/concept phrase. Madaan et al. (2020) and\nRajagopal et al. (2021) aim to generate event inﬂu-\nence graphs grounded in a situation. Similar to this\nwork, these methods rely on pre-trained language\nmodels to generate informative structures grounded\nin text. Different from us, these methods break\nthe generation process into a sequence of natural\nlanguage queries. Each query results in an event\nnode, which are ﬁnally assembled into a tree. In\ncontrast, we propose a method to directly generate\ngraphs with arbitrary topology from text. Addi-\ntionally, the events generated by these methods are\nnot present in text making event event prediction,\nrather than event extraction as their primary focus.\nYou et al. (2018) formulate graphs as a sequence\nfor learning generative models of synthetic and\nreal-world graphs. Similar to their work, we for-\nmulate graph generation as an auto-regressive task.\nHowever, our goal is the conditional generation of\ntemporal graphs, and not learning unconditional\ngenerative distributions. Finally, inspired by re-\ncent trends (Raffel et al., 2019), we do not make\nany graph speciﬁc modiﬁcations to the model or\nthe decoding process and formulate the problem as\na straightforward sequence-to-sequence mapping\ntask. While our approach does not rely on any\nparticular language model, it would be interesting\nto see the gains achieved by the much larger GPT-\n3 (Brown et al., 2020) on the dataset produced by\nour method.2\n3 Deriving Large-scale Dataset for the\nTemporal Graph Generation\nDeﬁnitions and Notations: Let G(V,E) be a\ntemporal graph associated with a document D,\nsuch that vertices V are the events in document\nD, and the edges E are temporal relations (links)\nbetween the events. Every temporal link in E takes\nthe form r(eq,et) where the query event eq and the\ntarget event et are in V, and ris a temporal relation\n(e.g., before or after). In this work, we undertake\ntwo related tasks of increasing complexity: i) Node\ngeneration, and ii) Temporal graph generation:\nTask 1: Node Generation: Let r(eq,et) be an\nedge in E. Let Cr be the set of sentences in the\ndocument D that contains the events eq or et or\nare adjacent to them. Given a query consisting of\nCr, r, and eq, generate et.\nTask 2: Temporal Graph Generation: Given a\ndocument D, generate the corresponding temporal\ngraph G(E,V).\n2Not available for research as of April 2021.\n867\nFigure 1 illustrates the two tasks. Task 1 is simi-\nlar to knowledge base completion, except that the\noutput events eq are generated, and not drawn from\na ﬁxed set of events. Task 2 is signiﬁcantly more\nchallenging, requiring the generation of both the\nstructure and semantics of G.\nThe training data for both the tasks consists of\ntuples {(xi,yi)}N\ni=1. For Task 1, xi is the con-\ncatenation of the query tokens (Cr,eq,r), and yi\nconsists of tokens of event et. For Task 2, xi is\nthe ith document Di, and yi is the corresponding\ntemporal graph Gi.\nWe use the New York Times (NYT ) Annotated\nCorpus 3 to derive our dataset of document-graph\npairs. The corpus has 1.8 million articles written\nand published by NYT between 1987 and 2007.\nEach article is annotated with a hand-assigned\nlist of descriptive terms capturing its subject(s).\nWe ﬁlter articles with one of the following de-\nscriptors: {“bomb”, “terrorism”, “murder”, “ri-\nots”, “hijacking”, “assassination”, “kidnapping”,\n“arson”, “vandalism”, “hate crime”, “serial murder”,\n“manslaughter”, “extortion”}, yielding 89,597 ar-\nticles, with a total of 2.6 million sentences and\n66 million tokens. For each document D, we use\nCAEVO (Chambers et al., 2014) to extract the dense\ntemporal graph consisting of i) the set of verbs,\nand ii) the set of temporal relations between the\nextracted verbs. CAEVO extracts six temporal rela-\ntions: before, after, includes, is included, simulta-\nneous, and vague.\nWe process each dense graph extracted by\nCAEVO with a series of pruning and augmenta-\ntion operations: i) We observed that some of the\nmost frequent verbs extracted by CAEVO were the\nso-called reporting verbs (Liu et al., 2018), like\nsaid, say, and told, which do not contribute to\nthe underlying events. For example, said formed\nnearly 10% of all the verbs extracted by CAEVO\nas an event. To remove such noisy events, we re-\nmove the ﬁve verbs with the lowest inverse docu-\nment frequencies, as well as an additional set of\nlight and reporting verbs (Liu et al., 2018; Re-\ncasens et al., 2010) 4 ii) To make event annota-\ntions richer, we follow (Chambers and Jurafsky,\n2008), and preﬁx and sufﬁx every verb with its\n3https://catalog.ldc.upenn.edu/LDC2008T19\n4The ﬁnal list of verbs is: i) low idf: “said”, “say”, “had”,\n“made”, “told”, ii) light: “appear”, “be”, “become”, “do”,\n“have”, “seem”, “get”, “give”, “go”, “have”, “keep”, “make”,\n“put”, “set”, “take”, iii) reporting: “argue”, “claim”, “say”,\n“suggest”, “tell”.\nnoun-phrase and object, respectively. This aug-\nmentation helps in adding a context to each verb,\nthus making events less ambiguous. For instance,\ngiven a sentence: A called B, after which B called\nC, CAEVO extracts AFTER (called,called). With\nthe proposed augmentation, the relation becomes\nAFTER (A called B,B called C), clearly differenti-\nating the two different called events. Our notion of\nevents refers to such augmented verbs. Crucially,\ndifferent from prior work, our system is trained to\nextract these augmented event phrases. We also\ndrop all the verbs that do not have either a subject\nor an object. iii) We remove the relations extracted\nby the statistical sieves if they have a conﬁdence\nscore of less than 0.50 and retain the rule-based\nrelations as those were shown to be extracted with\na high precision by Chambers et al. (2014). Finally,\nwe only retain event-event relations (dropping links\nbetween verbs and time expressions) and drop the\nvague relations as they typically do not play any\nrole in improving the understanding of the tempo-\nral sequences in a document. As Table 1 shows,\npruning noisy verbs and relations yields sparser\nand more informative graphs.\nInitial Pruned % Reduction\n#Relations 27,692,365 4,469,298 83.86\n#Events 6,733,396 2,615,296 61.15\nTable 1: Effect of pruning operations on the number of\nrelations and events.\nCreating Sub-graphs using Event Communities\nWe discovered that the (pruned) graph generated\nfor a given document typically has several sub-\ngraphs that are either completely disconnected or\nhave high intra-link density. Further, we found that\neach of these sub-graphs are grounded in different\nparts of the document. We exploit this phenomenon\nto map each sub-graph to its correct context, thus\nreducing the noise in the data.\nRelying merely on connectivity for creating sub-\ngraphs is still prone to noise, as largely unrelated\nsub-graphs are often connected via a single event.\nInstead, we propose a novel approach based on the\ndetection of event communities to divide a graph\ninto sub-graphs, such that the events in a sub-graph\nare more densely connected to each other. We learn\nthese event communities using the concept of mod-\nularity, ﬁrst introduced by (Newman and Girvan,\n2004). We defer the derivation of modularity opti-\n868\nmization to the Appendix.\nDatasets for Task 1 and Task 2After running the\npruning and clustering operations outlined above\non 89k documents, we obtain a corpus of over\n890,677 text-graph pairs, with an average of 120.31\ntokens per document, and 3.33 events and 4.91\nedges per graph. These text-graph pairs consti-\ntute the training data for Task 2. We derive the\ndata for Task 1 from the original (undivided) 89k\ngraphs (each document-graph pair contributes mul-\ntiple examples for Task 1). In Task 1 data, nearly\n80% of the queries (Cr,eq,r) had a unique answer\net, and nearly 16% of the queries had two different\ntrue et. We retain examples with multiple true et in\nthe training data because they help the model learn\ndiverse temporal patterns that connect two events.\nFor fairness, we retain such cases in the test set.\nTable 2 lists the statistics of the dataset. The splits\nwere created using non-overlapping documents.\nTask train valid test\nTask 1 4.26 0.54 0.54\nTask 2 0.71 0.09 0.09\nTable 2: Dataset statistics (counts in million).\n3.1 Graph Representation\nWe use language models to generate each graph\nas a sequence of tokens conditioned on the docu-\nment, thus requiring that the graphs are represented\nas strings. We use DOT language (Gansner et al.,\n2006) to format each graph as a string. While\nour method does not rely on any speciﬁc graph\nrepresentation format, we use DOT as it supports\na wide variety of graphs and allows augmenting\ngraphs with node, edge, and graph level informa-\ntion. Further, graphs represented in DOT are read-\nily consumed by popular graph libraries like Net-\nworkX (Hagberg et al., 2008), making it possible\nto use the graphs for several downstream applica-\ntions. Figure 2 shows an example graph and the\ncorresponding DOT code. The edges are listed in\nthe order in which their constituent nodes appear in\nthe document. This design choice was inspired by\nour ﬁnding that a vast majority of temporal links\nexist between events that are either in the same or\nin the adjoining sentence (this phenomenon was\nalso observed by Ning et al. (2019a)). Thus, list-\ning the edges in the order in which they appear in\nthe document adds a simple inductive bias of lo-\ncality for the auto-regressive attention mechanism,\nwhereby the attention weights slide from left to\nright as the graph generation proceeds. Addition-\nally, a ﬁxed order makes the problem well deﬁned,\nas the mapping between a document and a graph\nbecomes deterministic.\nFigure 2: Temporal graph and the corresponding DOT\nrepresentation for the sentence: Roma clashed ﬁercely\nwith the police, leading to arrests in which Roma ac-\ntivists said excessive force was used.\n4 Model\nThe training data X for both Tasks 1 and 2 com-\nprises of tuples {(xi,yi)}N\ni=1. For task 1 (node\ngeneration), xi the concatenation of context, the\nsource, node, and the relation. The target yi con-\nsists of the tokens of the target event. For task\n2 (graph generation), xi is a document and yi is\nthe corresponding temporal graph represented in\nDOT. We train a (separate) conditional language\nmodel to solve both the tasks. Speciﬁcally, given\na training corpus of the form {(xi,yi)}, we aim\nto estimate the distribution pθ(yi |xi). Given a\ntraining example (xi,yi) we set ui = xi∥yi5.\npθ(ui) can then be factorized as a sequence\nof auto-regressive conditional probabilities using\nthe chain rule: pθ(ui) = ∏n\nk=1 p(ui,k|ui,<k),\nwhere ui,k denotes the kth token of the ith se-\nquence, and ui,<k denotes the sequence of to-\nkens {u1,u2,...,u k−1}. Language models are typ-\nically trained by minimizing a cross-entropy loss\n−logpθ(ui) over each sequenceui in X. However,\nthe cross-entropy loss captures the joint distribu-\ntion pθ(xi,yi), and is not aligned with our goal of\nlearning conditional distribution pθ(yi |xi). To\n5∥ denotes concatenation\n869\nMethod Dataset BLEU MTR RG ACC\nSeq2Seq TG-Gen (-C) 20.20 14.62 31.95 19.68\nSeq2Seq TG-Gen 21.23 16.48 35.54 20.99\nGPT-2 TG-Gen (-C) 36.60 25.11 43.07 35.07\nGPT-2 TG-Gen 62.53 43.78 69.10 61.35\nSeq2Seq TB-Dense (-C) 11.55 9.23 21.87 10.06\nSeq2Seq TB-Dense 16.68 12.69 27.75 13.97\nGPT-2 TB-Dense (-C) 22.35 15.04 27.73 20.81\nGPT-2 TB-Dense 52.21 35.69 57.98 47.91\nTable 3: Node Generation (task 1) results.\ncircumvent this, we train our model by masking\nthe loss terms corresponding to the input xi, sim-\nilar to Bosselut et al. (2019). Let mi be a mask\nvector for each sequence ui, set to 0 for positions\ncorresponding to xi, and 1 otherwise i.e. mi,j = 1\nif j > |xi|, else 0. We combine the mask vec-\ntor with our factorization of pθ(ui) to formulate a\nmasked language modeling loss L, which is min-\nimized over the training corpus X to estimate the\noptimal θ:\nL(X) =−\n|X|∑\ni=1\n|xi|+|yi|∑\nj=1\nmi,j∗log (pθ(ui,j|ui,<j))\nNote that the formulation of masked loss is opaque\nto the underlying architecture, and can be imple-\nmented with a simple change to the loss function.\nIn practice, we use GPT-2 (Radford et al., 2019)\nbased on transformer architecture (Vaswani et al.,\n2017) for our implementation. Having trained a\npθ for each task, we generate a node ( y) given\na query ( x) (for Task 1), or a graph ( y) given a\ndocument ( x) (for Task 2) by drawing samples\nfrom the appropriate pθ(y |x) using nucleus sam-\npling (Holtzman et al., 2019). We provide more\ndetails of our training procedure and the architec-\nture in the Appendix (C.1).\n5 Experiments and Results\n5.1 Evaluation Datasets\nWe evaluate our method on two different datasets:\ni) TG-Gen: Test split of synthetically created\ndataset (Section 3), and ii) TB-Dense: A mixed-\ndomain corpus, with human-annotated temporal an-\nnotations. We create TB-Dense from the test splits\nof TimeBank-Dense (Cassidy et al., 2014) by apply-\ning the same pre-processing operations as we did\nfor TG-Gen. TB-Dense forms a very challenging\ndataset for our task because of domain mismatch;\nour system was trained on a corpus of terrorism-\nrelated events, whereas TB-Dense includes docu-\nments from a wide array of domains, forming a\nzero-shot evaluation scenario for our method.\n5.2 Implementation Setup\nGPT-2: We use GPT-2 medium (355M parame-\nters) for our experiments with 24-layers, a hidden\nsize of 1024, and 16 self-attention heads. We build\non the implementation by Wolf et al. (2019), using\nthe default hyperparameters and a block size input\nsequence length after tokenization) of 512. For op-\ntimization, we use AdamW (Loshchilov and Hutter,\n2017) with a learning rate of 5e-5, a batch size of\n1, and no learning rate scheduling. We also experi-\nmented with a block size of 300 and a batch size of\n2. We found the results (presented in the appendix)\nto be worse, underscoring the importance of using\na larger block size for generating larger outputs.\nWe generate samples using nucleus sampling using\np = 0.9, and set maximum output length to be\n500 for graph generation and 60 foe node genera-\ntion. All of our experiments were done on a single\nNvidia GeForce RTX 2080 Ti. The models were\ninitialized with the pre-trained weights provided\nby Radford et al. (2019), and ﬁne-tuned for ﬁve\nepochs, with a run-time of 48 hours/epoch for Task\n1 and 52 hours/epoch for Task 2. We use the last\ncheckpoint (i.e., at the end of ﬁne-tuning) for all\nexperiments. Despite the higher perplexity on the\ndevelopment set, we found the overall performance\nof the last checkpoint to be better.\nWe also experimented with GPT-2 without ﬁne-\ntuning (i.e., by directly using pre-trained weights).\nThe non-ﬁnetuned GPT-2 fared poorly for both the\ntasks across all the metrics, getting a BLEU score\nof near 0 for Task 1. This dismal performance un-\nderscores the importance of ﬁne-tuning on the end\ntask for large-scale pre-trained language models.\nFinally, we note that our method does not make\nany model-speciﬁc assumption, and can be used\nwith any auto-regressive language model (i.e., a lan-\nguage model that generates a sequence of tokens\nfrom left-to-right). We use GPT-2 as a representa-\ntive for large pre-trained language models.\nSeq2Seq: We train a bi-directionalLSTM (Hochre-\niter and Schmidhuber, 1997) based sequence-to-\nsequence model (Bahdanau et al., 2015) with global\nattention (Luong et al., 2015) and a hidden size of\n500 as a baseline to contrast with GPT-2. The to-\n870\nken embeddings initialized using 300-dimensional\npre-trained Glove (Pennington et al., 2014).\n5.3 Task 1: Node Generation\nParagraph: Mr. Grier, a former defensive lineman for the\nNew York Giants who was ordained as a minister in 1986,\ntestiﬁed on Dec. 9 that he had visited Mr. Simpson a month\nearlier\nTable 4: An example of GPT-2 ﬁxing the label given\nby CAEVO . Given a query event after “Mr. Grier vis-\nited”, CAEVO incorrectly extracts Mr. Grier ordained,\nwhereas GPT-2 generates the correct event: Mr. Grier\ntestiﬁed.\nMetrics Given a query(Cr,eq,r), with Cr being\nthe context (sentences containing events eq,et and\ntheir neighboring sentences) and eq as the source\nevent, Task 1 is to generate a target event et such\nthat r(eq,et). We format each query as “In the\ncontext of C, what happens req?”. We found for-\nmatting the query in natural language to be empiri-\ncally better. Let ˆet be the system generated event.\nWe compare et vs. ˆet using BLEU (Papineni et al.,\n2002), METEOR (Denkowski and Lavie, 2011), and\nROUGE (Lin, 2004)6, and measure the accuracy\n(ACC) as the fraction of examples where et = ˆet.\nResults on TG-Gen The results are listed in Ta-\nble 3. Unsurprisingly, GPT-2 achieves high scores\nacross the metrics showing that it is highly effec-\ntive in generating correct events. To test the gen-\nerative capabilities of the models, we perform an\nablation by removing the sentence containing the\ntarget event et from Cr (indicated with -C). Re-\nmoval of context causes a drop in performance\nfor both GPT-2 and Seq2Seq, showing that it is\ncrucial for generating temporal events. However,\nGPT-2 obtains higher relative gains with context\npresent, indicating that it uses its large architecture\nand pre-training to use the context more efﬁciently.\nGPT-2 also fares better as compared with Seq2Seq\nin terms of drop in performance for the out-of-\ndomain TB-Dense dataset on metrics like accuracy\n(−21% vs. −33%) and BLEU (−16% vs. −21%),\nindicating that pre-training makes helps GPT-2 in\ngeneralizing across the domains.\nHuman Evaluation To understand the nature of\nerrors, we analyzed 100 randomly sampled incor-\nrect generations. For 53% of the errors, GPT-2\ngenerated a non-salient event which nevertheless\n6Sharma et al. (2017), https://github.com/\nMaluuba/nlg-eval\nhad the correct temporal relation with the query.\nInterestingly, for 10% of the events, we found that\nGPT-2 ﬁxed the label assigned by CAEVO (Table 4),\ni.e., et was incorrect but ˆet was correct.\n5.4 Task 2: Graph Generation\nDataset BLEU MTR RG DOT %\nSeq2Seq TG-Gen 4.79 15.03 45.95 86.93\nGPT-2 TG-Gen 37.77 37.22 64.24 94.47\nSeq2Seq TB-Dense 2.61 12.76 28.36 89.31\nGPT-2 TB-Dense 26.61 29.49 49.26 92.37\nTable 5: Graph string metrics.\nDataset vP vR vF1 eP eR eF1\nSeq2Seq TG-Gen 36.84 24.89 28.11 9.65 4.29 4.70\nGPT-2 TG-Gen 69.31 66.12 66.34 27.95 25.89 25.22\nSeq2Seq TB-Dense 24.86 15.25 17.99 4.7 0.14 0.24\nCAEVO TB-Dense 37.53 79.83 48.96 7.95 14.62 8.96\nGPT-2 TB-Dense 45.96 48.44 44.97 8.74 8.89 7.96\nTable 6: Graph semantic metrics.\nMetrics Let Gi(Vi,Ei) and ˆGi( ˆVi,ˆEi) be the\ntrue and the generated graphs for an example iin\nthe test corpus. Please recall that our proposed\nmethod generates a graph from a given document\nas a string in DOT. Let yi and ˆyi be the string\nrepresentations of the true and generated graphs.\nWe evaluate our generated graphs using three types\nof metrics:\n1. Graph string metrics: To compare yi vs. ˆyi,\nwe use BLEU , METEOR , and ROUGE , and also mea-\nsure parse accuracy (DOT %) as the % of generated\ngraphs ˆyi which are valid DOT ﬁles.\n2. Graph structure metrics To compare the struc-\ntures of the graphs Gi vs. ˆGi, we use i) Graph\nedit distance ( GED ) (Abu-Aisheh et al., 2015) -\nthe minimum numbers of edits required to trans-\nform the predicted graph to the true graph by ad-\ndition/removal of an edge/node; ii) Graph isomor-\nphism (ISO ) (Cordella et al., 2001) - a binary mea-\nsure set to 1 if the graphs are isomorphic (without\nconsidering the node or edge attributes); iii) The\naverage graph size (|Vi|,|Ei|,|ˆVi|,|ˆEi|) and the\naverage degree (d(V)).\n3. Graph semantic metrics: We evaluate the\nnode sets (Vi vs. ˆVi) and the edge sets (Ei vs. ˆEi)\nto compare the semantics of the true and generated\n871\ngraphs. For every example i, we calculate node-set\nprecision, recall, and F1 score, and average them\nover the test set to obtain node precision (vP), re-\ncall (vR), and F1 (vF). We evaluate the predicted\nedge set using temporal awareness (UzZaman and\nAllen, 2012; UzZaman et al., 2013). For an exam-\nple i, we calculateei\nP = |ˆE−\ni ∩Ei+|\n|ˆE−\ni | ,ei\nR = |ˆE+\ni ∩Ei−|\n|Ei−|\nwhere symbol + denotes the temporal transitive\nclosure (Allen, 1983) of the edge set. Similarly,\n−indicates the reduced edge set, obtained by re-\nmoving all the edges that can be inferred from\nother edges transitively. The F1 score ei\nF1 is the\nharmonic mean of ei\nP and ei\nR, and these metrics\nare averaged over the test set to obtain the tempo-\nral awareness precision ( eP), recall ( eR), and F1\nscore (eF1 ). Intuitively, the node metrics judge the\nquality of generated events in the graph, and the\nedge metrics evaluate the corresponding temporal\nrelations.\nResults Tables 5, 7, and 6 present results for\ngraph generation, and we discuss them next.\nDataset |V| | E| d(V) GED ↓ ISO ↑\nTrue TG-Gen 4.15 5.47 1.54 0 100\nSeq2Seq TG-Gen 2.24 2.23 1.12 6.09 32.49\nGPT-2 TG-Gen 3.81 4.60 1.40 2.62 41.66\nTrue TB-Dense 4.39 6.12 2.02 0 100\nSeq2Seq TB-Dense 2.21 2.20 1.11 6.22 23.08\nCAEVO TB-Dense 10.73 17.68 2.76 18.68 11.11\nGPT-2 TB-Dense 3.72 4.65 1.75 4.05 24.00\nTable 7: Graph structure metrics.\nGPT-2 vs. Seq2Seq GPT-2 outperforms Seq2Seq\non all the metrics by a large margin in both ﬁne-\ntuned (TG-Gen) and zero-shot settings (TB-Dense).\nGPT-2 generated graphs are closer to the true\ngraphs in size and topology, as shown by lower\nedit distance and a higher rate of isomorphism in\nTable 7. Both the systems achieve high parsing\nrates (DOT %), with GPT-2 generating valid DOT\nﬁles 94.6% of the time. The high parsing rates are\nexpected, as even simpler architectures like vanilla\nRNNs have been shown to generate syntactically\nvalid complex structures like LATEXdocuments with\nease (Karpathy, 2015).\nGPT-2 vs. CAEVO We compare the graphs\ngenerated by GPT-2 with those extracted\nby CAEVO (Chambers et al., 2014) 7 from the\nTB-Dense documents. We remove all the vague\n7https://github.com/nchambers/caevo\nTop 10 Verbs:found, killed, began, called, want,\ntook, came, used, trying, asked\nRandomly Sampled Verbs: shooting, caused,\naccused, took, conceived, visit, vowing, play,\nwithdraw, seems\nTable 8: Verbs in GPT-2 generated graphs.\nedges and the light verbs from the output ofCAEVO\nfor a fair comparison. Please recall that CAEVO\nis the tool we used for creating the training data\nfor our method. Further, CAEVO was trained using\nTB-Dense, while GPT-2 was not. Thus, CAEVO\nforms an upper bound over the performance of\nGPT-2. The results in Tables 6 and 7 show that\ndespite these challenges, GPT-2 performs strongly\nacross a wide range of metrics, including GED ,\nISO , and temporal awareness. Comparing the\nnode-set metrics, we see that GPT-2 leads CAEVO\nby over eight precision points ( vP), but loses on\nrecall (vR) as CAEVO extracts nearly every verb\nin the document as a potential event. On temporal\nawareness (edge-metrics), GPT-2 outperforms both\nCAEVO and Seq2Seq in terms of average precision\nscore eP and achieves a competitive eF1 score.\nThese results have an important implication: they\nshow that our method can best or match a pipeline\nof specialized systems given reasonable amounts\nof training data for temporal graph extraction.\nCAEVO involves several sub-modules to perform\npart-of-speech tagging, dependency parsing, event\nextraction, and several statistical and rule-based\nsystems for temporal extraction. In contrast, our\nmethod involves no hand-curated features, is\ntrained end-to-end (single GPT-2), and can be\neasily scaled to new datasets.\nNode extraction and Edge Extraction The\nnode-set metrics in Table 6 shows that GPT-2\navoids generating noisy events (high P), and ex-\ntracts salient events (high R). This is conﬁrmed\nby manual analysis, done by randomly sampling\n100 graphs from the GPT-2 generated graphs and\nisolating the main verb in each node (Table 8). We\nprovide several examples of generated graphs in\nthe Appendix. We note from Table 6 that the rel-\native difference between the eF1 scores for GPT-2\nand Seq2Seq (25.22 vs. 4.70) is larger than the rel-\native difference between their vF1 scores (66.34 vs.\n28.11), showing that edge-extraction is the more\nchallenging task which allows GPT-2 to take full\nadvantage of its powerful architecture. We also ob-\nserve that edge extraction (eF1 ) is highly sensitive\n872\nQuery (C,eq,r) et Explanation\nThe suspected car\nbombings...turning\nbusy streets...Which\nevent happened before\nthe suspected car\nbombings?\nmany cars\ndrove\nPlausible:\nThe passage\nmentions busy\nstreets and car\nbombing.\nHe...charged...killed\none person. Which\nevent happened after he\nwas charged?\nHe was acquit-\nted\nSomewhat\nplausible:An\nacquittal is\na possible\noutcome of a\ntrial.\nTable 9: Sample open-ended questions and the answers\net generated by our system. Note that the answers gen-\nerated by our systemet are complete event phrases (not\njust verbs).\nto node extraction ( vF1 ); for GPT-2, a 27% drop\nin vF1 (66.34 on TG-Gen vs. 44.97 on TB-Dense)\ncauses a 68% drop in eF1 (25.22 on TG-Gen vs.\n7.96 on TB-Dense). As each node is connected\nto multiple edges on average (Table 7), missing a\nnode during the generation process might lead to\nmultiple edges being omitted, thus affecting edge\nextraction metrics disproportionately.\n5.5 Answering for Open-ended Questions\nA beneﬁt of our approach of using a pre-trained lan-\nguage model is that it can be used togenerate an an-\nswer for open-ended temporal questions. Recently,\nNing et al. (2020) introduced Torque, a temporal\nreading-comprehension dataset. Several questions\nin Torque have no answers, as they concern a time\nscope not covered by the passage (the question is\nabout events not mentioned in the passage). We\ntest the ability of our system for generating plau-\nsible answers for such questions out of the box\n(i.e., without training on Torque). Given a (passage,\nquestion) pair, we create a query (C,eq,r), where\nCis the passage, and eq and rare the query event\nand temporal relation in the question. We then use\nour GPT-2 based model for node-generation trained\nwithout context and generate an answer et for the\ngiven query. A human-judge rated the answers gen-\nerated for 100 such questions for plausibility, rating\neach answer as being plausible, somewhat plausi-\nble, or incorrect. For each answer rated as either\nplausible or somewhat plausible, the human-judge\nwrote a short explanation to provide a rationale for\nthe plausibility of the generated event. Out of the\n100 questions, the human-judge rated 22 of the gen-\nerated answers as plausible and ten as somewhat\nplausible, showing the promise of our method on\nthis challenging task (Table 9).\n6 Conclusion and Future Work\nCurrent methods for generating event-level tem-\nporal graphs are developed with relatively small\namounts of hand-labeled data. On the other hand,\nthe possibility of using pre-trained language mod-\nels for this task has not received sufﬁcient attention.\nThis paper addresses this open challenge by ﬁrst de-\nveloping a data generation pipeline that uses exist-\ning IE/NLP /clustering techniques for automated ac-\nquisition of a large corpus of document-graph pairs,\nand by proposing a new formulation of the graph\ngeneration task as a sequence-to-sequence map-\nping task, allowing us to leverage and ﬁne-tune pre-\ntrained language models. Our experiments strongly\nsupport the effectiveness of the proposed approach,\nwhich signiﬁcantly outperforms strong baselines.\nWe plan to explore techniques for adapting large-\nscale language models on unseen domains and at\nmultiple granularity levels in the future.\nAcknowledgments\nThanks to Nathanael Chambers and Dheeraj Ra-\njagopal for the helpful discussion, and to the anony-\nmous reviewers for their constructive feedback.\nThis material is based on research sponsored in\npart by the Air Force Research Laboratory under\nagreement number FA8750-19-2-0200. The U.S.\nGovernment is authorized to reproduce and dis-\ntribute reprints for Governmental purposes notwith-\nstanding any copyright notation thereon. The views\nand conclusions contained herein are those of the\nauthors and should not be interpreted as necessarily\nrepresenting the ofﬁcial policies or endorsements,\neither expressed or implied, of the Air Force Re-\nsearch Laboratory or the U.S. Government.\nReferences\nZeina Abu-Aisheh, Romain Raveaux, Jean-Yves\nRamel, and Patrick Martineau. 2015. An exact\ngraph edit distance algorithm for solving pattern\nrecognition problems. In An exact graph edit\ndistance algorithm for solving pattern recognition\nproblems.\nJames F Allen. 1983. Maintaining knowledge about\ntemporal intervals. Communications of the ACM,\n26(11):832–843.\n873\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In 3rd International\nConference on Learning Representations, ICLR\n2015.\nMiguel Ballesteros, Rishita Anubhai, Shuai Wang,\nNima Pourdamghani, Yogarshi Vyas, Jie Ma, Par-\nminder Bhatia, Kathleen McKeown, and Yaser Al-\nOnaizan. 2020. Severing the edge between before\nand after: Neural architectures for temporal ordering\nof events. arXiv preprint arXiv:2004.04295.\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, Asli C ¸ elikyilmaz, and Yejin Choi.\n2019. Comet: Commonsense transformers for auto-\nmatic knowledge graph construction. In ACL.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nPaweł Budzianowski and Ivan Vuli´c. 2019. Hello, it’s\ngpt-2–how can i help you? towards the use of pre-\ntrained language models for task-oriented dialogue\nsystems. arXiv preprint arXiv:1907.05774.\nTaylor Cassidy, Bill McDowell, Nathanel Chambers,\nand Steven Bethard. 2014. An annotation frame-\nwork for dense event ordering. Technical report,\nCARNEGIE-MELLON UNIV PITTSBURGH PA.\nNathanael Chambers, Taylor Cassidy, Bill McDowell,\nand Steven Bethard. 2014. Dense event ordering\nwith a multi-pass architecture. Transactions of the\nAssociation for Computational Linguistics, 2:273–\n284.\nNathanael Chambers and Dan Jurafsky. 2008. Un-\nsupervised learning of narrative event chains. In\nProceedings of ACL-08: HLT, pages 789–797.\nNathanael Chambers and Dan Jurafsky. 2009. Unsu-\npervised learning of narrative schemas and their par-\nticipants. In Proceedings of the Joint Conference of\nthe 47th Annual Meeting of the ACL and the 4th\nInternational Joint Conference on Natural Language\nProcessing of the AFNLP, pages 602–610.\nFei Cheng and Yusuke Miyao. 2017. Classifying tem-\nporal relations by bidirectional lstm over depen-\ndency paths. In Proceedings of the 55th Annual\nMeeting of the Association for Computational\nLinguistics (V olume2: Short Papers), pages 1–6.\nAaron Clauset, Mark EJ Newman, and Cristopher\nMoore. 2004. Finding community structure in very\nlarge networks. Physical review E, 70(6):066111.\nLuigi Pietro Cordella, Pasquale Foggia, Carlo San-\nsone, and Mario Vento. 2001. An improved algo-\nrithm for matching large graphs. In 3rd IAPR-TC15\nworkshop on graph-based representations in pattern\nrecognition, pages 149–159.\nMichael Denkowski and Alon Lavie. 2011. Me-\nteor 1.3: Automatic metric for reliable optimiza-\ntion and evaluation of machine translation systems.\nIn Proceedings of the sixth workshop on statistical\nmachine translation, pages 85–91. Association for\nComputational Linguistics.\nNicholas D Duran, Philip M McCarthy, Art C Graesser,\nand Danielle S McNamara. 2007. Using temporal\ncohesion to predict temporal coherence in narrative\nand expository texts. Behavior Research Methods,\n39(2):212–223.\nJacqueline Evers-Vermeul, Jet Hoek, and Merel CJ\nScholman. 2017. On temporality in discourse an-\nnotation: Theoretical and practical considerations.\nDialogue & Discourse, 8(2):1–20.\nEmden Gansner, Eleftherios Koutsoﬁos, and Stephen\nNorth. 2006. Drawing graphs with dot.\nTanya Goyal and Greg Durrett. 2019. Embedding\ntime expressions for deep temporal ordering models.\nIn Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n4400–4406, Florence, Italy. Association for Compu-\ntational Linguistics.\nAric A. Hagberg, Daniel A. Schult, and Pieter J. Swart.\n2008. Exploring network structure, dynamics, and\nfunction using networkx. In Proceedings of the\n7th Python in Science Conference, pages 11 – 15,\nPasadena, CA USA.\nRujun Han, I-Hung Hsu, Mu Yang, Aram Galstyan,\nRalph Weischedel, and Nanyun Peng. 2019. Deep\nstructured neural network for event temporal re-\nlation extraction. In Proceedings of the 23rd\nConference on Computational Natural Language\nLearning (CoNLL), pages 666–106.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–\n778.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation,\n9(8):1735–1780.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2019. The curious case of neural text\ndegeneration. arXiv preprint arXiv:1904.09751.\nAndrej Karpathy. 2015. The unreasonable effective-\nness of recurrent neural networks. Andrej Karpathy\nblog, 21:23.\n874\nChen Lin, Dmitriy Dligach, Timothy A Miller, Steven\nBethard, and Guergana K Savova. 2016. Mul-\ntilayered temporal modeling for the clinical do-\nmain. Journal of the American Medical Informatics\nAssociation, 23(2):387–395.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nXiao Ling and Daniel S Weld. 2010. Temporal in-\nformation extraction. In AAAI, volume 10, pages\n1385–1390.\nZhengzhong Liu, Chenyan Xiong, Teruko Mitamura,\nand Eduard Hovy. 2018. Automatic event salience\nidentiﬁcation. arXiv preprint arXiv:1809.00647.\nIlya Loshchilov and Frank Hutter. 2017. Decou-\npled weight decay regularization. arXiv preprint\narXiv:1711.05101.\nMinh-Thang Luong, Hieu Pham, and Christopher D\nManning. 2015. Effective approaches to attention-\nbased neural machine translation. In Proceedings\nof the 2015 Conference on Empirical Methods in\nNatural Language Processing, pages 1412–1421.\nAman Madaan, Dheeraj Rajagopal, Yiming Yang, Ab-\nhilasha Ravichander, Eduard Hovy, and Shrimai\nPrabhumoye. 2020. Eigen: Event inﬂuence gen-\neration using pre-trained language models. arXiv\npreprint arXiv:2010.11764.\nJuha Makkonen, Helena Ahonen-Myka, and Marko\nSalmenkivi. 2003. Topic detection and tracking with\nspatio-temporal evidence. In European Conference\non Information Retrieval, pages 251–265. Springer.\nMark EJ Newman. 2004. Fast algorithm for detecting\ncommunity structure in networks. Physical review\nE, 69(6):066133.\nMark EJ Newman and Michelle Girvan. 2004. Find-\ning and evaluating community structure in networks.\nPhysical review E, 69(2):026113.\nQiang Ning, Zhili Feng, and Dan Roth. 2019a. A struc-\ntured learning approach to temporal relation extrac-\ntion. arXiv preprint arXiv:1906.04943.\nQiang Ning, Sanjay Subramanian, and Dan Roth.\n2019b. An improved neural baseline for temporal re-\nlation extraction. arXiv preprint arXiv:1909.00429.\nQiang Ning, Hao Wu, Rujun Han, Nanyun Peng, Matt\nGardner, and Dan Roth. 2020. Torque: A reading\ncomprehension dataset of temporal ordering ques-\ntions. arXiv preprint arXiv:2005.00242.\nQiang Ning, Ben Zhou, Zhili Feng, Haoruo Peng,\nand Dan Roth. 2018. Cogcomptime: A tool\nfor understanding time in natural language. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 72–77.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic\nevaluation of machine translation. In Proceedings\nof the 40th annual meeting on association for\ncomputational linguistics, pages 311–318. Associa-\ntion for Computational Linguistics.\nJeffrey Pennington, Richard Socher, and Christopher D.\nManning. 2014. Glove: Global vectors for word\nrepresentation. In Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 1532–1543.\nAlec Radford, Karthik Narasimhan, Tim Sali-\nmans, and Ilya Sutskever. 2018. Improving\nlanguage understanding by generative pre-\ntraining. URL https://s3-us-west-2. amazonaws.\ncom/openai-assets/researchcovers/languageunsupervised/language\nunderstanding paper. pdf.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Lan-\nguage models are unsupervised multitask learners.\nOpenAI Blog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nDheeraj Rajagopal, Aman Madaan, Niket Tandon,\nYiming Yang, Shrimai Prabhumoye, Abhilasha\nRavichander, Peter Clark, and Eduard Hovy. 2021.\nCurie: An iterative querying approach for reasoning\nabout situations.\nMarta Recasens, Eduard H Hovy, and Maria Ant `onia\nMart´ı. 2010. A typology of near-identity relations\nfor coreference (nident). In LREC.\nMaarten Sap, Ronan Le Bras, Emily Allaway, Chan-\ndra Bhagavatula, Nicholas Lourie, Hannah Rashkin,\nBrendan Roof, Noah A Smith, and Yejin Choi. 2019.\nAtomic: An atlas of machine commonsense for\nif-then reasoning. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence, volume 33,\npages 3027–3035.\nShikhar Sharma, Layla El Asri, Hannes Schulz, and\nJeremie Zumer. 2017. Relevance of unsupervised\nmetrics in task-oriented dialogue for evaluating nat-\nural language generation. CoRR, abs/1706.09799.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge. In Thirty-First AAAI Conference\non Artiﬁcial Intelligence.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le.\n2014. Sequence to sequence learning with neu-\nral networks. In Advances in neural information\nprocessing systems, pages 3104–3112.\nNaushad UzZaman and James F Allen. 2012.\nInterpreting the temporal aspects of language.\nCiteseer.\n875\nNaushad UzZaman, Hector Llorens, Leon Derczyn-\nski, James Allen, Marc Verhagen, and James Puste-\njovsky. 2013. Semeval-2013 task 1: Tempeval-3:\nEvaluating time expressions, events, and temporal\nrelations. In Second Joint Conference on Lexical\nand Computational Semantics (* SEM), V olume2:\nProceedings of the Seventh International Workshop\non Semantic Evaluation (SemEval 2013), pages 1–9.\nSiddharth Vashishtha, Benjamin Van Durme, and\nAaron Steven White. 2019. Fine-grained tem-\nporal relation extraction. In Proceedings of\nthe 57th Annual Meeting of the Association for\nComputational Linguistics, pages 2906–2919, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is\nall you need. In Advances in neural information\nprocessing systems, pages 5998–6008.\nThomas Wolf, L Debut, V Sanh, J Chaumond, C De-\nlangue, A Moi, P Cistac, T Rault, R Louf, M Fun-\ntowicz, et al. 2019. Huggingface’s transformers:\nState-of-the-art natural language processing. ArXiv,\nabs/1910.03771.\nJiaxuan You, Rex Ying, Xiang Ren, William L Hamil-\nton, and Jure Leskovec. 2018. Graphrnn: Generat-\ning realistic graphs with deep auto-regressive mod-\nels. arXiv preprint arXiv:1802.08773.\n876\nA Learning Event Communities Using\nCommunity Detection\nIn this section, we provide the details on\nthe community detection algorithm used\nby our method. We deﬁne the temporal\nevent communities to be a division of the\ntemporal graph G(V,E) into sub-graphs\nG1(V1,E1),G2(V2,E2),..., Gk(Vk,Ek) such\nthat the events in a community (sub-graph) Gi are\nmore co-referential to each other as opposed to\nthe other events in the temporal graph. We use\nthe undirected link between two events ej,ei as\na proxy for them being co-referential, and learn\ntemporal event communities utilizing the concept\nof modularity, ﬁrst introduced by (Newman and\nGirvan, 2004).\nFormally, let A be the undirected adjacency\nmatrix for a temporal graph G(V,E) such that\nA(ei,ej) = 1 if ei and ej are connected by a\ntemporal relation, and 0 otherwise. Further, let\nδ(ei,ej) = 1if events ei,ej belong to the same\ntemporal community, and 0 otherwise. For a given\nδ, we denote the fraction of the edges that exist\nbetween events that belong to the same commu-\nnities by fsame =\n∑\nei,ej∈E A(ei,ej)δ(ei,ej)\n2|E| . Where\nthe 2|E|in the denominator is due to the fact that\nA treats G as an undirected graph. Let the pop-\nularity p of an event ei be the number of events\nthat are linked to it i.e. p(ei) =∑\nej∈E A(ei,ej).\nThe probability of randomly picking an event ei\nwhen sampled by popularity is p(ei)∑\nej∈E p(ei) = p(ei)\n2|E|.\nThus, if edges are created randomly by sampling\nnodes by popularity pof the nodes, the fraction of\nedges within the communities, frandom, is given\nby\nfrandom =\n∑\nei,ej∈E p(ei)p(ej)δ(ei,ej)\n2|E|∗2|E|\nFinally, deﬁning modularity, Q, to be fsame −\nfrandom:\nQ= 1\n2|E|∗\n∑\nei,ej∈E\nA(ei,ej) −p(ei)p(ej)δ(ei,ej)\n2|E|\nWe want to learn community assignments δ\nthat maximize Q. A high Q would promote\nfsame > frandom and thereby encourage highly\ninter-connected event communities. Calculating\nsuch δdirectly is not tractable, since the complexity\nof such an operation would be at least exponential\nin the number of events (Newman, 2004). We use\nthe fast implementation provided by (Clauset et al.,\n2004) for calculating event communities iteratively.\nThe algorithm converges atQ0.3. We use a similar\napproximation at test time: given a document D,\nwe ﬁrst break it down into sub-documents using\nCAEVO and then feed each sub-document to our\nmethod.\nB Using a smaller block size\nWe found that the performance drops when using\na block size of 300 and batch size of 2. Table 10\npresents the results.\nBLEU MTR RG DOT %\n25.01 27.95 60.99 91.71\nvP vR vF1 eP eR eF1\n70.31 64.75 65.68 29.43 24.83 24.27\nTable 10: Results for TG-Gen using a block size of 300\nand a block size of 2.\nC Masked Language Modeling Using\nTransformers\nIn this section, we expand on the design of the\ntransformer blocks. For ease of reference, we re-\niterate our training methodology. We train a (sep-\narate) conditional language model to solve both\nthe tasks. Speciﬁcally, given a training corpus of\nthe form {(xi,yi)}, we aim to estimate the dis-\ntribution pθ(yi |xi). Given a training example\n(xi,yi) we set ui = xi∥yi8. pθ(ui) can then be\nfactorized as a sequence of auto-regressive condi-\ntional probabilities using the chain rule: pθ(ui) =∏n\nk=1 p(ui,k|ui,<k), where ui,k denotes the kth\ntoken of the ith sequence, and ui,<k denotes the\nsequence of tokens {u1,u2,...,u k−1}. Language\nmodels are typically trained by minimizing a cross-\nentropy loss −logpθ(ui) over each sequence ui\nin X. However, the cross-entropy loss captures\nthe joint distribution pθ(xi,yi), and is not aligned\nwith our goal of learning conditional distribution\npθ(yi|xi). To circumvent this, we train our model\nby masking the loss terms corresponding to the in-\nput xi, similar to Bosselut et al. (2019). Let mi\nbe a mask vector for each sequence ui, set to 0\nfor positions corresponding to xi, and 1 otherwise\n8∥ denotes concatenation\n877\nFigure 3: Event temporal graph and the extracted communities for a sample document. Each community is shown\nin different color . The singleton nodes ( gray) are dropped. The nodes are only annotated with the verbs for\nbrevity. The edge labels and directions are not used for community detection.\ni.e. mi,j = 1if j >|xi|, else 0. We combine the\nmask vector with our factorization of pθ(ui) to for-\nmulate a masked language modeling loss, which is\nminimized over the training corpus X to estimate\nthe optimal θ:\nLmasked(X) =−\n|X|∑\ni=1\n|xi|+|yi|∑\nj=1\nmi,j∗log(pθ(ui,j|ui,<j))\nNote that the formulation of masked loss is opaque\nto the underlying architecture, and can be imple-\nmented with a simple change to the loss function.\nIntuitively, the model is optimized for only the out-\nput sequence yi.\nC.1 Adapting GPT-2 for Masked Language\nModeling\nIn practice, we use GPT-2 (Radford et al., 2019)\nbased on transformer architecture (Vaswani et al.,\n2017) for our implementation. An input sequence\nui of length nis ﬁrst embedded to a continuous rep-\nresentation denoted by ui(0) ∈Rnd. ui(0) is then\npassed through a series of L transformer blocks\nto obtain the output sequence ui(L) ∈Rnh. Each\ntransformer block (Vaswani et al., 2017) consists\nof two operations: an auto-regressive version of the\nmultiheaded self-attention (Vaswani et al., 2017)\noperation (AutoRegMultiHead) followed by a feed-\nforward operation (FFN). Each of these operations\nis surrounded by a residual connection (He et al.,\n2016) and followed by a layer normalization (Ba\net al., 2016) operation. Denoting by u(l−1) the in-\nput to the lth transformer block , the operations are\nin a transformer block are deﬁned as follows:\n˜ul\nattn = AutoRegMultiHead(u(l−1))\nu(l)\natt = LayerNorm(˜u(l)\natt + u(l−1))\n˜u(l)\nffn = FFN(u(l)\natt)\nu(l) = LayerNorm(˜u(l)\nffn + u(l)\natt)\nWhere AutoRegMultiHead is an auto-regressive\nversion of the multiheaded self-attention (Vaswani\net al., 2017) that restricts the attention to the\nsequence seen so far (in accordance with the\nchain rule), and FFN is a feed-forward network\n(MLP ). After obtaining ui(L), we set pφ(ui) =\nsoftmax(ui(L) ∗We), where We ∈Rh|V|(|V|is\nthe size of the vocabulary). Finally, we calculate\nthe masked loss as L(ui) =miT ⊙log(pφ(ui)),\nand the optimal φ is obtained by minimizing\nLmasked(X) =−∑|X|\ni=1 L(ui).\nD Dataset Statistics\nTables 11, 12, and 13 list various statistics calcu-\nlated from the source data.\nE Examples\nFigures 4-9 show randomly picked examples from\nthe test corpus. Each ﬁgure shows the text, the\ncorresponding true graph, and the graph predicted\nby GPT-2.\n878\nFigure 4\nDescriptor #Articles\nterrorism 40909\nmurders and attempted murders 25169\nunited states international relations 17761\nunited states armament and defense 16785\nairlines and airplanes 16103\nworld trade center (nyc) 15145\ndemonstrations and riots 14477\nhijacking 14472\npolitics and government 6270\nbombs and explosives 5607\nTable 11: Top Descriptors for the ﬁltered Dataset. Note\nthat each article is typically assigned more than one de-\nscriptor.\nEvent verb Raw frequency % Frequency\nsaid 647685 9.60\nsay 57667 0.86\nhad 47320 0.70\nkilled 43369 0.64\ntold 42983 0.64\nfound 41733 0.62\nmade 40544 0.60\nwar 35257 0.52\nget 30726 0.46\nmake 29407 0.44\nTable 12: Most frequent events extracted by CAEVO .\nRelation Raw Frequency % Frequency\nBEFORE 2436201 54.51\nAFTER 1772071 39.65\nIS INCLUDED 131052 2.93\nSIMULTANEOUS 112509 2.52\nINCLUDES 17465 0.39\nTable 13: Relation Frequence in our Corpus\nRelation Frequency\nBEFORE 98715\nAFTER 68582\nIS INCLUDED 6179\nSIMULTANEOUS 6209\nINCLUDES 285\nTable 14: Edges in Generated Graphs: Top\n879\nFigure 5\nFigure 6\n880\nFigure 7\nFigure 8\n881\nFigure 9",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8695864081382751
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.7403173446655273
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6800163984298706
    },
    {
      "name": "Natural language processing",
      "score": 0.5688909292221069
    },
    {
      "name": "Language model",
      "score": 0.5655916333198547
    },
    {
      "name": "Graph",
      "score": 0.5160036087036133
    },
    {
      "name": "Knowledge graph",
      "score": 0.5122525095939636
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.49776318669319153
    },
    {
      "name": "Question answering",
      "score": 0.4480127990245819
    },
    {
      "name": "Machine learning",
      "score": 0.35064417123794556
    },
    {
      "name": "Theoretical computer science",
      "score": 0.1449528932571411
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    }
  ],
  "cited_by": 10
}