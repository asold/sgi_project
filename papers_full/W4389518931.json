{
  "title": "Auto-Instruct: Automatic Instruction Generation and Ranking for Black-Box Language Models",
  "url": "https://openalex.org/W4389518931",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5101798380",
      "name": "Zhihan Zhang",
      "affiliations": [
        "Microsoft (Belgium)",
        "University of Notre Dame"
      ]
    },
    {
      "id": "https://openalex.org/A5090420113",
      "name": "Shuohang Wang",
      "affiliations": [
        "Microsoft (Belgium)",
        "University of Notre Dame"
      ]
    },
    {
      "id": "https://openalex.org/A5055111969",
      "name": "Wenhao Yu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5070165835",
      "name": "Xu Yi‚Äêchong",
      "affiliations": [
        "Microsoft (Belgium)",
        "University of Notre Dame"
      ]
    },
    {
      "id": "https://openalex.org/A5037853792",
      "name": "Dan Iter",
      "affiliations": [
        "Microsoft (Belgium)",
        "University of Notre Dame"
      ]
    },
    {
      "id": "https://openalex.org/A5030469532",
      "name": "Qingkai Zeng",
      "affiliations": [
        "Microsoft (Belgium)",
        "University of Notre Dame"
      ]
    },
    {
      "id": "https://openalex.org/A5100355692",
      "name": "Yang Liu",
      "affiliations": [
        "Microsoft (Belgium)",
        "University of Notre Dame"
      ]
    },
    {
      "id": "https://openalex.org/A5100355246",
      "name": "Chenguang Zhu",
      "affiliations": [
        "Microsoft (Belgium)",
        "University of Notre Dame"
      ]
    },
    {
      "id": "https://openalex.org/A5100670646",
      "name": "Meng Jiang",
      "affiliations": [
        "Microsoft (Belgium)",
        "University of Notre Dame"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4384644140",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W4287207937",
    "https://openalex.org/W4281493460",
    "https://openalex.org/W4385572845",
    "https://openalex.org/W4385572921",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W4306808908",
    "https://openalex.org/W4378468558",
    "https://openalex.org/W4287891464",
    "https://openalex.org/W4389518888",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4320342989",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4293569541",
    "https://openalex.org/W4378469111",
    "https://openalex.org/W4385573003",
    "https://openalex.org/W4362656192",
    "https://openalex.org/W4298184221",
    "https://openalex.org/W4318621294",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4303448941",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W4313483544",
    "https://openalex.org/W4321649710",
    "https://openalex.org/W4385859558",
    "https://openalex.org/W3164972323",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4286987939"
  ],
  "abstract": "Large language models (LLMs) can perform a wide range of tasks by following natural language instructions, without the necessity of task-specific fine-tuning. Unfortunately, the performance of LLMs is greatly influenced by the quality of these instructions, and manually writing effective instructions for each task is a laborious and subjective process. In this paper, we introduce Auto-Instruct, a novel method to automatically improve the quality of instructions provided to LLMs. Our method leverages the inherent generative ability of LLMs to produce diverse candidate instructions for a given task, and then ranks them using a scoring model trained on a variety of 575 existing NLP tasks. In experiments on 118 out-of-domain tasks, Auto-Instruct surpasses both human-written instructions and existing baselines of LLM-generated instructions. Furthermore, our method exhibits notable generalizability even with other LLMs that are not incorporated into its training process.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 9850‚Äì9867\nDecember 6-10, 2023 ¬©2023 Association for Computational Linguistics\nAuto-Instruct: Automatic Instruction Generation and Ranking\nfor Black-Box Language Models\nZhihan Zhang\u0000 ‚ô†‚àó, Shuohang Wang‚ô¢, Wenhao Yu‚ô†, Yichong Xu‚ô¢, Dan Iter‚ô¢,\nQingkai Zeng‚ô†, Yang Liu‚ô¢, Chenguang Zhu‚ô¢, Meng Jiang‚ô†\n‚ô†University of Notre Dame\n‚ô¢Microsoft Azure AI\nzzhang23@nd.edu\nAbstract\nLarge language models (LLMs) can perform a\nwide range of tasks by following natural lan-\nguage instructions, without the necessity of\ntask-specific fine-tuning. Unfortunately, the\nperformance of LLMs is greatly influenced by\nthe quality of these instructions, and manually\nwriting effective instructions for each task is a\nlaborious and subjective process. In this paper,\nwe introduce Auto-Instruct, a novel method to\nautomatically improve the quality of instruc-\ntions provided to LLMs. Our method lever-\nages the inherent generative ability of LLMs\nto produce diverse candidate instructions for a\ngiven task, and then ranks them using a scor-\ning model trained on a variety of 575 exist-\ning NLP tasks. In experiments on 118 out-\nof-domain tasks, Auto-Instruct surpasses both\nhuman-written instructions and existing base-\nlines of LLM-generated instructions. Further-\nmore, our method exhibits notable generaliz-\nability even with other LLMs that are not incor-\nporated into its training process.1\n1 Introduction\nInstruction-tuned large language models (LLMs)\nhave gained considerable popularity as solutions to\na myriad of NLP tasks, owing to their proficiency\nin interpreting natural language instructions (Wei\net al., 2021; Chung et al., 2022; Ouyang et al., 2022;\nTaori et al., 2023). As fine-tuning LLMs often be-\ncomes unfeasible, instructions play an increasingly\ncrucial role in prompting such black-box LLMs.\nEspecially in thetrue few-shot 2 setting (Perez et al.,\n2021) where the user aims to tackle a new task\nwith only a basic task description and a few data\n‚àó This work was done when Zhihan was an intern at Mi-\ncrosoft Azure AI.\n1Model and code are available at https://github.com/\nytyz1307zzh/Auto-Instruct.\n2A scenario where no additional training or validation data\nare available for hyperparameter tuning and prompt selection,\nin addition to the few-shot examples (Perez et al., 2021).\nOrder adjectives correctly \nin English sentences.\nInput: Which sentence has \nthe correct adjective order:\n(A) rubber terrible ship \n(B) terrible rubber ship\nOutput: (B)\nFLAN-T5\nDemonstrations\nSeed Instruction\nIn this task, you will be \ngiven two sentences \nwith adjectives‚Ä¶\ngenerate\nrank & select\n+\nconcatenate\n(B)inference\nSelected Instruction\n575 training tasks\n‚Ä¶‚Ä¶\nCandidate instructions\nWhich sentence has the \ncorrect adjective order:\n(A) green new chair\n(B) new green chair\nTest Example\nInstruction Generation\nInstruction Ranking\nDownstream Inference\nCandidate instructions \nof diverse styles\nIn this task, you will be \ngiven two sentences \nwith adjectives‚Ä¶\nSelected Instruction\nWhich sentence has the \ncorrect adjective order:\n(A) green new chair\n(B) new green chair\nTest Example\none-sentence instruction\none-paragraph instruction\nstep-by-step instruction\nexample-explaining instruction\nFigure 1: The Auto-Instruct pipeline. We first prompt\nthe LLM to generate a diverse set of candidate instruc-\ntions with different styles, and then train a model to\nrank and select the most effective instruction for a given\nexample. Finally, the selected instruction is used to\nprompt the LLM to infer the output for this example.\nexamples at hand, a well-crafted instruction is im-\nperative in enabling the LLM to grasp the required\ninput-output mapping to complete the task.\nDespite the significance of instructions, the pre-\nvailing approach when using a black-box LLM on\na new task remains to be manual prompt engineer-\ning (White et al., 2023; Mishra et al., 2023). Such\nan approach, however, is not only time-consuming\nbut also tends to yield suboptimal instructions.\nAgainst this backdrop, efforts have been made to\nempower LLMs to generate instructions automat-\nically (Honovich et al., 2022; Zhou et al., 2022;\nSingh et al., 2022). These approaches feed the\nLLM a handful of examples and prompt it to gen-\nerate an instruction based on these demonstrations.\nWhile such methods showcase the LLM‚Äôs capa-\nbility to generate coherent instructions (Honovich\net al., 2022), only generating a single instruction\n9850\ncannot guarantee reliable performance for unseen\nexamples in the given task. As a straightforward\nsolution, validation sets have been used to evalu-\nate the effectiveness of a set of sampled instruc-\ntions (Zhou et al., 2022; Singh et al., 2022), but\nthis is impracticable for many tasks defined under\nthe true few-shot setting (Suzgun et al., 2022). Be-\nsides, these approaches have primarily been tested\non simple tasks where basic instructions are already\nsufficient, such as arithmetic operations or senti-\nment classification. More complex tasks in NLP\nbenchmarks (Wang et al., 2022), which necessi-\ntate careful instruction engineering, remain largely\nunexamined for an automatic solution.\nTo address the aforementioned challenges, we\npropose Auto-Instruct, a novel approach to auto-\nmatically generate and rank instructions for black-\nbox LLMs across various NLP tasks, under the\ntrue few-shot setting. For each downstream task,\nwe first prompt the LLM to sample a variety of\ncandidate instructions, based on a basic seed in-\nstruction and few-shot demonstrations. We collect\na diverse candidate set by specifying the expected\nstyle of each instruction. Recognizing the variable\nperformance of LLMs across different instructions,\ncoupled with the lack of validation data for pre-\nemptive instruction selection, we train a scoring\nmodel to rank and select the most appropriate in-\nstruction for each downstream test example. To\nensure necessary generalizability in the few-shot\nsetting, the model is trained on 575 exisiting NLP\ntasks before being deployed for out-of-domain test\ntasks. Finally, the selected instruction is used to\nprompt the LLM for downstream inference.\nIn experiments with OpenAI‚Äôs text-davinci-003,\nAuto-Instruct yields remarkable performance on\n118 out-of-domain tasks from Super Natural In-\nstructions (SuperNI; Wang et al., 2022) and Big\nBench Hard (BBH; Suzgun et al., 2022). Showing\nrobust generalizability in out-of-domain scenarios,\nAuto-Instruct outperforms human-written seed in-\nstructions, the state-of-the-art instruction genera-\ntion approach iPrompt (Singh et al., 2022), and vari-\nous baselines of prompting the LLM for instruction\nselection. Moreover, Auto-Instruct exhibits impres-\nsive performance in the zero-shot setting and in\ngeneralization to other LLMs (i.e., ChatGPT and\nGPT-4). Our study underlines that automatically\ngenerating and ranking instructions is a promising\napproach for leveraging the power of black-box\nLLMs effectively.\n2 Related Work\nThe choice of instructions plays a pivotal role in\neffectively utilizing LLMs. To this end, a range of\napproaches has been implemented, withparametric\noptimization and LLM-based generation standing\nout as prominent methods. Parametric optimiza-\ntion primarily involves utilizing parameters to tune\ninstructions (Shin et al., 2020; Shi et al., 2022;\nDeng et al., 2022). For instance, Shin et al. (2020)\nemployed a gradient-based search over a predeter-\nmined length of discrete tokens as the instruction.\nShi et al. (2022) further improved this approach by\npreserving the readability of the sampled tokens\nthrough a perplexity constraint. As a more flexible\napproach, Deng et al. (2022) optimized instruction\ngeneration through reinforcement learning, with\nrewards computed based on the LLM output. How-\never, these strategies require access to either LLM\nparameters or a training set for optimization, mak-\ning them less applicable to black-box LLMs with\nonly a limited number of available examples. More-\nover, instructions generated by these methods often\nlack fluency or even become gibberish, thereby\ncompromising their interpretability.\nIn contrast, the LLM-based generation thread\nselects instructions by directly prompting the\nLLM (Honovich et al., 2022; Zhou et al., 2022;\nSingh et al., 2022). For example, Honovich et al.\n(2022) were among the first to reveal that LLMs\ncould write an instruction for a given task after ob-\nserving just a few demonstrations, and Zhou et al.\n(2022) improved the quality of the generated in-\nstructions by selecting the best performed one on\nthe validation data. iPrompt (Singh et al., 2022)\nis the most capable method so far with its itera-\ntive generation and validation process for selecting\ninstructions. Nevertheless, these approaches still\nnecessitate a validation set for instruction ranking,\nand the instructions they generate typically under-\nperform compared to those written by humans.\nBesides the choice of instructions, researchers\nhave also explored other orthogonal directions of\nimproving LLM prompts, such as the selection of\nin-context demonstrations. Some works focused on\nidentifying the most suitable demonstrations from\ntraining examples (Rubin et al., 2022; Lu et al.,\n2022a; Wang et al., 2023a) and their optimal order-\ning (Lu et al., 2022b) in the few-shot prompt. Other\nstudies examined the engineering and selection of\nreasoning chains that are paired with the few-shot\ndemonstrations on multi-step reasoning tasks (Wei\n9851\net al., 2022; Zhang et al., 2022b; Ye and Durrett,\n2023; Liang et al., 2023b). We reserve the explo-\nration of integrating these orthogonal techniques\nwith our approach to holistically optimize the entire\nLLM prompt for future work.\n3 Problem Formulation\nIn this work, we focus on the true few-shot set-\nting where a user aims to tackle a new task with a\nblack-box LLM. While it is easy to come up with\na handful of examples and a basic description, the\nuser may not have insights into what kind of in-\nstructions would be effective for unseen examples.\nHence, given the few-shot examples as demonstra-\ntions and the basic description as a seed instruction,\nour goal is to automate the process of creating a\nmore effective instruction for the given task.\nWe formulate our problem following the con-\nventional practices of in-context learning (Dong\net al., 2023). In the aforementioned few-shot set-\nting, the prompt to query a black-box LLM com-\nprises an instruction I, the test input x, and a few\ninput-output pairs as demonstrations {xd\ni ,yd\ni }n\ni=1.\nThe LLM is expected to generate an output y ‚àº\nP(¬∑|I,{xd\ni ,yd\ni }n\ni=1,x). This work aims to automat-\nically find a superior instruction I‚Ä≤based on the\nhuman-written seed instruction Is, thereby circum-\nventing the need for substantial manual engineer-\ning. Besides, we also explore the zero-shot setting\nwhere no demonstrations are given to the LLM.\nDespite the instruction potentially having mul-\ntiple ways of integrating with the demonstrations\nand the test input, to reduce the complexity of the\nproblem, we format the whole prompt in the order\nof (I,xd\n1,yd\n1 ,¬∑¬∑¬∑ ,xd\nn,yd\nn,x). This aligns with the\nconvention of problem-solving where the task is\nfirst outlined, followed by the provision of data\nexamples, and the test input is finally provided. In\npractice, we use n= 3for all tasks.\n4 Auto-Instruct\nAuto-Instruct is composed of two steps: instruction\ngeneration and instruction ranking. We first prompt\nthe black-box LLM to generate a diverse set of can-\ndidate instructions (¬ß4.1) for each downstream task.\nNext, we train a scoring model to rank all candidate\ninstructions for each given test example, as differ-\nent examples can benefit from different instructions\n(¬ß4.2). Then, the top-ranked instruction is selected\nto prompt the black-box LLM on that specific test\nexample for downstream inference.\nWrite a step-by-step instruction on how \nto solve the following task.\nTask: [seed instruction]\nExamples:\nInput: [input of demonstration #1]\nOutput: [output of demonstration #1]\n‚Ä¶‚Ä¶\nInput: [input of demonstration #n]\nOutput: [output of demonstration #n]\nInstruction: \nFigure 2: The meta-prompt that guides the LLM to\ngenerate a step-by-step instruction for the given task.\nOther meta-prompts are shown in Appendix E.\n4.1 Instruction Generation\nAs mentioned in ¬ß3, we leverage a basic human-\nwritten task description as the seed instruction\nIs and prompt the black-box LLM to gener-\nate a number of candidate instructions {Ic\nj }m\nj=1.\nSpecifically, in the few-shot setting, we prompt\nthe LLM to generate candidate instructions\nIc ‚àºP(¬∑|Is,{xd\ni ,yd\ni }n\ni=1) based on the seed in-\nstruction and few-shot demonstrations. Previous\napproaches (Zhou et al., 2022; Singh et al., 2022)\nonly utilized a single meta-prompt3 and collected\ncandidate instructions via token sampling. Usually,\nsuch sampled instructions only show minor vari-\nations in phrasing rather than substantial content\ndiversity. Moreover, their quality recursively rely\non the arbitrary choice of the meta-prompt, which\ntransfers the unreliability of manual instruction en-\ngineering to manual meta-prompt engineering.\nIn our improved approach, we curate a set of\nmeta-prompts to stimulate the LLM to sample di-\nverse candidate instructions by defining different\nrequired styles of the instruction. These meta-\nprompts include:\n1. Write an instruction on how to solve the fol-\nlowing task in one sentence.\n2. Write an instruction on how to solve the fol-\nlowing task in one paragraph.\n3. Write a step-by-step instruction on how to\nsolve the following task.\n4. Write an instruction on how to solve the fol-\nlowing task. The instruction must include the\nexplanations of the given examples.\nAlongside these 4 meta-prompts, we also bring\nin human-written instructions from existing NLP\ntasks as demonstrations to guide the generation of\n3The prompt for the LLM to generate instructions.\n9852\nSuperNI few-shot SuperNI zero-shot BBH few-shot BBH zero-shot\n0\n5\n10\n15\n20Standard Deviation of Performance\nFigure 3: Box plot showing how much the LLM per-\nformance varies with different instructions, tested on\nOpenAI‚Äôs text-davinci-003. Performance is evaluated\nby ROUGE-L on SuperNI and Accuracy on BBH. Each\nvalue represents the standard deviation of LLM perfor-\nmance across all generated instructions on a single task.\ninstructions. Intuitively, we prompt the LLM to\nemulate the style of human-written instructions in\nthese demonstration tasks. We source demonstra-\ntion tasks with their instructions from our training\ntasks in SuperNI, grouping them into 3 clusters\nbased on the length of their instructions, so as to\nguide the LLM to generate instructions of different\ngranularities. Figure 2 provides an example of the\nmeta-prompt #3. Other meta-prompts are detailed\nin Appendix E.\nBased on these 7 distinct meta-prompts (i.e., 4\nstyle-specific meta-prompts + 3 groups of demon-\nstration tasks), we sample 3 instructions under each\nmeta-prompt via nucleus sampling (Holtzman et al.,\n2020). Including the original seed instruction, we\ncollect a total of 22 candidate instructions for each\ntask. As a result, we create a diverse and com-\nprehensive set of candidate instructions, thereby\nreducing the randomness brought by the nuances\nof different meta-prompts.\nIn the zero-shot setting, due to the absence of\ndemonstrations, the LLM is prompted to generate\nthe candidate instruction Ic ‚àºP(¬∑|Is) based solely\non the seed instruction. Besides, the example-\nexplaining meta-prompt is removed. As we demon-\nstrate in ¬ß5.4.5, even without the aid of demonstra-\ntions, our style-specific meta-prompts still enable\nthe LLM to generate informative instructions.\n4.1.1 Instability Under Different Instructions\nWhile LLMs are capable of generating meaningful\ninstructions, relying on a single generated instruc-\ntion will probably lead to suboptimal performance\ndue to the LLM‚Äôs sensitivity to the phrasing of\nthe instructions. This instability is particularly ev-\nident in the zero-shot setting due to the lack of\ndemonstrations to assist prediction. In Figure 3,\nwe calculate the standard deviation of LLM per-\nformance using different instructions, after having\nevaluated all instructions for each downstream task.\nThis indicates the expected performance fluctua-\ntion when substituting one instruction for another.\nThe median standard deviation across all tasks are\n3.1 and 4.2 points in ROUGE-L for few-shot and\nzero-shot settings respectively on SuperNI, and the\nupper quartiles are 5.7 and 6.9 points respectively.\nThe choice of instruction even causes double-digit\nperformance fluctuation on many tasks. Therefore,\nthe development of a method to rank and select\ninstructions becomes an essential undertaking.\n4.2 Instruction Ranking\nIn a true few-shot setting, demonstrations are in-\nadequate to reflect the effectiveness of candidate\ninstructions due to the small sample size. To cir-\ncumvent this limitation, we train a generalizable\ninstruction ranking model across a variety of NLP\ntasks, and subsequently apply it to each test exam-\nple in out-of-domain tasks. Intuitively, this model\nis trained to rank instructions against their down-\nstream performance on the LLM, i.e., to assign\nhigher scores to more effective instructions.\n4.2.1 Model\nOwing to the proven generalizibility of the T5\nmodel family (Raffel et al., 2020; Sanh et al.,\n2022), we start from the instruction-tuned FLAN-\nT5-Large model (Chung et al., 2022) and train it\nwith our instruction ranking objective. Given a\nspecific example (x,y) where xis the input and\ny is the ground-truth output, as well as an arbi-\ntrary candidate instruction Ic, the model predicts\na score QT5(Ic,x) as an estimate of the instruc-\ntion‚Äôs effectiveness on the example. Leveraging the\ninstruction-following nature of FLAN-T5, we give\nthe following prompt to the ranking model:\nExample: ùë•ùë•. Input: ùêºùêºùëêùëê. Is this a good instruction \nto solve the example? \nQT5(Ic,x) is then calculated as the logit of the\n‚Äúyes‚Äù token at the starting position of the decoder.\nAdditionally, we obtain the downstream perfor-\nmance of Ic by calculating the ROUGE-L score\nbetween the LLM‚Äôs predicted output ÀÜy(when Ic is\n9853\nused as the instruction) against the groud-truth out-\nput y, denoted as r(y,ÀÜy). The model is then trained\nwith a list-wise loss to align the scores QT5(Ic,x)\nof all candidate instructions with their correspond-\ning downstream performance r(y,ÀÜy), while consid-\nering relative superiority among different instruc-\ntions. Specifically, we first normalize both the list\nof predicted scores QT5(Ic,x) and the list of down-\nstream performance r(y,ÀÜy) by applying softmax\nacross all candidate instructions, and then compute\nthe KL-divergence between these two normalized\ndistributions as the training loss:\nL= 1\n|B|\n‚àë\n(x,y)‚ààB\nKL\n(\nœÉ\n(\nr(y,ÀÜy)\n)\n||œÉ\n(\nQT5 (Ic,x)\n))\n,\nwhere ÀÜy‚àºPLLM(¬∑|Ic,{xd\ni ,yd\ni }n\ni=1,x).\nNote that Bis a batch of examples and œÉ is the\nsoftmax function. During testing, given a spe-\ncific test example, among all candidate instructions,\nwe select the Ic that achieves the highest score\nQT5(Ic,x) as the final instruction, and prompt\nLLM with it to obtain the desired output.\n4.2.2 Training Data\nTo train such a ranking model with generalizabil-\nity to out-of-domain tasks, we categorize the tasks\nin the SuperNI benchmark by their task type (e.g.,\nQA, sentiment analysis, etc.) and group these cate-\ngories into training and test sets. We exclude tasks\ninvolving non-English languages or those with ex-\ncessively long inputs. To avoid data leakage, we\nalso exclude tasks from the training data which\nare sourced from the same dataset as any test task.\nThis yields 575 tasks for training and 91 for testing.\nWe sample up to 400 examples from each train-\ning task, which leads to 122k in total. Additional\ndata pre-processing and filtering methods utilized\nto accelerate the training process can be found in\nAppendix A.\n5 Experiments\n5.1 Settings\nTo evaluate our approach under the true few-shot\nsetting, we test it on a variety of out-of-domain\ntasks ‚Äî 91 from SuperNI (Wang et al., 2022) and\n27 from BBH (Suzgun et al., 2022), where there is\nno overlap between task categories in training and\ntesting. The SuperNI test set comprises both clas-\nsification and generation tasks, e.g., commonsense\nclassification, information extraction, etc4. BBH\n4The full list of SuperNI test tasks is in Appendix G.\npresents a diverse set of tasks spanning common-\nsense QA and math problems. Average ROUGE-\nL5 and exact-match accuracy are used for evalua-\ntion on SuperNI and BBH, respectively. Our main\nexperiments are conducted using OpenAI‚Äôs text-\ndavinci-003 for instruction generation and down-\nstream inference. We also explored the instructions\ngenerated by ChatGPT ( gpt-3.5-turbo) or GPT-\n4 (OpenAI, 2023) in ¬ß5.4.1.\nIn the zero-shot setting, the ranking model is sep-\narately trained on data where downstream ROUGE\nscores of candidate instructions are likewise ob-\ntained under zero-shot prompting. For zero-shot\nclassification tasks, we append additional format-\nting instructions to the seed instruction to narrow\ndown the answer options in both instruction gener-\nation and downstream inference. Additional exper-\nimental settings can be found in Appendix B.\n5.2 Baselines\nAs baselines in our experiments, we first con-\nsider three alternative approaches based solely on\nprompting the LLM:\n(1) Cross-Validation. We leverage the 3-shot\ndemonstrations as validation data to rank the in-\nstructions, with each one acting as the test example\niteratively while the other two serve as demonstra-\ntions. The ROUGE-L score (or accuracy for BBH)\nis used as the primary ranking criterion, and the\nlog-probability of the ground-truth output is com-\npared as tiebreaker. The instruction selected by the\ndemonstrations is then applied on all test examples\nwithin the same task.\n(2) LM Selection. We directly prompt the LLM\nitself to select an instruction by enumerating all\ncandidate instructions in a single prompt. We num-\nber the instructions and ask the LLM to generate\nthe number of the instruction it deems most suitable\nto each test example.\n(3) On-the-fly Generation. As a simplified variant\nwithout instruction ranking, the model is asked to\ndirectly generate an instruction for each test ex-\nample. The generated instruction is then used to\nprompt the LLM for the same example.\nFurthermore, we consider iPrompt (Singh et al.,\n2022), the existing state-of-the-art approach in opt-\nmizing instructions with LLMs. iPrompt iteratively\ngenerates instructions until it cannot find one with\nbetter performance on a validation set. To evaluate\n5The original authors of SuperNI found ROUGE-L posi-\ntively correlated to accuracy on classification tasks, so average\nROUGE-L is applied for simplicity.\n9854\nMethods Generation Ranking Few-shot Zero-shot\nSuperNI BBH SuperNI BBH\nEmpty Instruction* None None 57.03 51.18 35.86 45.12\nHuman Instruction* Human None 60.94 50.30 46.81 45.59\nRandom Selection‚Ä† LLM Random 61.61 50.88 45.80 45.98\niPrompt* LLM (iterative) Examples 57.08 50.46 - -\niPrompt+* LLM (iterative) Examples 61.13 50.82 - -\nCross-Validation* LLM Examples 62.02 51.20 - -\nLM Selection‚Ä† LLM LLM 61.69 51.96 44.19 45.05\nOn-the-fly Generation‚Ä† LLM None 61.03 51.38 45.85 45.47\nAuto-Instruct‚Ä† LLM Trained Model 64.35 52.04 49.50 47.35\nTable 1: Results on SuperNI (91 tasks) and BBH (27 tasks) under the few-shot and zero-shot setting respectively.\nWe report ROUGE-L on SuperNI and accuracy on BBH. Methods with * apply the same instruction for a certain\ntask, while methods with ‚Ä† can select different instructions for different examples. iPrompt iteratively generates\nand ranks candidate instructions, while other methods adopt a generate-then-rank pipeline. We note that iPrompt,\niPrompt+ and Cross-Validation are not applicable under the zero-shot setting due to the need of validation examples.\nDetailed results on SuperNI of different task categories can be found at Appendix D.1.\niPrompt under the true few-shot setting, we con-\nduct its validation on the 3-shot demonstrations.\nBesides, since the original iPrompt generates in-\nstructions based on the examples without any task\ndescription, for a fair comparison, we implement an\niPrompt+ baseline that uses a similar meta-prompt\nto ours with the seed instruction (See Appendix C\nfor details). In addition, we evaluate the perfor-\nmance of not using any instruction ( Empty In-\nstruction), directly using the human-written seed\ninstruction (Human Instruction) or randomly se-\nlecting an instruction from the generated candidates\n(Random Selection) on each task.\n5.3 Results\nThe overall results of SuperNI and BBH are shown\nin Table 1, where scores are averaged across all\ntasks. Auto-Instruct shows notable consistency and\ngeneralizability in out-of-domain scenarios, sur-\npassing all baselines across different benchmarks\nand settings. Key findings are outlined below.\nThe LLM shows competitive ability in gener-\nating effective instructions, yet ranking is still\nnecessary. In alignment with previous work (Zhou\net al., 2022; Singh et al., 2022), the LLM is able\nto generate effective instructions for various tasks.\nOur style-specific meta-prompts enables the LLM\nto produce a diverse set of instructions to cater to\nvaried scenarios where different tasks may favor\ndifferent styles of instructions. In the few-shot\nsetting, the LLM-generated instructions already\nsurpass their human-written counterparts on aver-\nage, as indicated by the random selection scores.\nAlthough humans may have prior knowledge of\nsome examples when they write the instructions,\nthe LLM, not given any demonstrations in the zero-\nshot setting, generates instructions of comparable\nquality to those written by humans. Nevetheless,\nneither random selection nor directly generating a\nsingle instruction (i.e., on-the-fly generation) signif-\nicantly improves over the human-written baseline.\nThis aligns with the instability of the LLM perfor-\nmance across different instructions as discussed in\nFigure 3, which indicates further instruction rank-\ning is still essential.\nSimply prompting the LLM or using the vali-\ndation data are not reliable in the low-resource\nsetting. Although offering the convenience of not\ntraining any models, both directly prompting the\nLLM (LM selection) and using few-shot demonstra-\ntions for validation (iPrompt and cross-validation)\nfail to deliver consistently improved results com-\npared to random selection. This highlights that (1)\nthe LLM itself lacks clue of the expected down-\nstream performance of different instructions; (2)\nthe volume of validation data must be substantial\nenough to effectively estimate the performance of\ninstructions on the test data, which brings high cost\nin many realistic scenarios.\nOur trained instruction ranking model is the\nmost effective approach to select instructions so\nfar. Although the data and instructions for out-of-\n9855\nMethods ChatGPT GPT-4\nFew-shot, instructions from text-davinci-003\nHuman 60.39 67.31\nRandom 60.44 67.07\nAuto-Instruct 62.88 69.45\nFew-shot, instructions fromChatGPT/GPT-4\nHuman 60.39 67.31\nRandom 60.44 66.77\nAuto-Instruct 62.32 68.16\nZero-shot, instructions fromChatGPT/GPT-4\nHuman 47.77 54.11\nRandom 46.22 53.06\nAuto-Instruct 49.04 55.53\nTable 2: SuperNI results of transferring Auto-Instruct\nto ChatGPT and GPT-4, using either (1) instructions\ngenerated by text-davinci-003, or (2) instructions gener-\nated by the same model as downstream inference (i.e.,\nChatGPT or GPT-4). The instruction ranking model is\nstill the one trained on text-davinci-003 instructions.\ndomain tasks are not seen by the ranking model,\nit exhibits promising generalizability in selecting\neffective instructions thanks to the training on hun-\ndreds of different tasks. For example, on the Su-\nperNI benchmark, it outperforms random selection\nby 4% and 8% on few-shot and zero-shot settings\nrespectively. Besides, our complete pipeline deliv-\ners a relative 6% improvement over the original\nhuman instructions in both few-shot and zero-shot\nsettings, indicating that the human-written instruc-\ntions still need improvement in many contexts.\n5.4 Analysis\nIn this section, we delve deeper into the perfor-\nmance of our approach by analyzing the use of\nother LLMs for instruction generation, the perfor-\nmance on seen tasks, the size of training data, and\ncase studies. Additional analysis of the comparison\nbetween Auto-Instruct and multi-answer ensemble\nis in Appendix D. These analyses are conducted in\nthe few-shot setting unless stated otherwise.\n5.4.1 Generalization to other LLMs\nTo further test the generalizability of our approach,\nwe transfer Auto-Instruct to other LLMs by us-\ning ChatGPT (gpt-3.5-turbo) and GPT-4 as down-\nstream inference models. As Table 2 suggests, in-\nstructions selected by Auto-Instruct ontext-davinci-\n003 are still effective if transferred to ChatGPT and\nMethods Selection Acc Win Rate\nTop1 Top5 vs.Empty vs.Human\nHuman 45.25 70.35 22.43 -\nRandom 46.76 70.13 24.95 16.87\nCross-Validation47.61 68.73 26.77 20.74\nLM Selection 47.53 71.07 25.17 17.93\nAuto-Instruct 52.54 73.10 29.51 23.89\nTable 3: Evaluation of instruction ranking on silver la-\nbels. Left: we evaluate the percentage of cases where the\nselected instruction is the best (top-1) or is among top-5\ncandidates, according to the actual downstream perfor-\nmance. We note that there could be multiple instructions\nsharing the best score. Right: we check the percentage\nof selected instructions that outperform either the empty\ninstruction or the human-written ones.\nGPT-4. Furthermore, our instruction ranking model\nis able to rank instructions generated by ChatGPT\nor GPT-4 under both few-shot and zero-shot scenar-\nios, despite not having seen any instruction created\nby these LLMs during training. Improved results\ncan also be seen when transferring Auto-Instruct\nto LLaMA-2-chat (Touvron et al., 2023), a recent\nopen-source LLM, as shown in Appendix D.2. As a\nconclusion, despite variations in phrasing across in-\nstructions generated by different LLMs, the under-\nlying pattern determining instruction effectiveness\nis transferable, although the largest improvement\nis still seen in the same-LLM experiments. Suffice\nto say, our trained instruction ranking model can\nbe directly applied to select instructions for other\nLLMs without the need of re-training.\n5.4.2 Evaluation of Instruction Ranking\nTo investigate the effectiveness of the instruction\nranking model, we compare it with other instruc-\ntion selection baselines by assigning silver labels\nto candidate instructions, with results detailed in\nTable 3. First, we use the actual downstream per-\nformance of the candidate instructions as silver\nlabels. Our ranking model is more capable of dis-\ntinguishing better instructions, as shown by an evi-\ndently higher accuracy of picking the top-1 or top-\n5 instructions among all 22 candidates. Second,\nwe evaluate how often the selected instruction im-\nproves the downstream performance in comparison\nto either the empty instruction or the human-written\ninstruction. Once again, the instructions from our\nranking model makes the most significant improve-\nments, advancing the human-written counterparts\nin 7% more cases than random selection. The con-\n9856\nMethods Unseen Tasks Seen Tasks\nHuman 54.59 40.32\nRandom 55.57 39.74\nAuto-Instruct 60.18 45.89\n‚ä¢(vs. Random) (+8.3%) (+15.5%)\nTable 4: Results on instruction-sensitive test data of both\nseen tasks (100 tasks seen in training) and unseen tasks\n(the same as Table 1) from SuperNI. We additionally\nreport the relative improvement ratio to the random\nselection baseline since the vanilla performance is not\non the same scale.\n0% 25% 50% 75% 100%\n% Training T asks\n60.5\n61.0\n61.5\n62.0\n62.5\n63.0\n63.5\n64.0Avg. ROUGE\nFigure 4: Results of using different number of training\ntasks. 0% means directly using the pre-trained FLAN-\nT5 checkpoint in instruction ranking, which shows a\nsimilar performance to random instruction selection.\nsistent performance gain across all silver-label eval-\nuations further corroborates the superiority of our\nmodel over alternative ranking methods based on\ncross-validation or LM selection.\n5.4.3 Auto-Instruct on Seen Tasks\nBesides the out-of-domain setting, we explore an\nin-domain setting where we select additional exam-\nples from tasks seen during training, so as to further\nexamine the competency of the instruction rank-\ning model. For a fair comparison of the model‚Äôs\nranking abilities across different tasks, we experi-\nment with instruction-sensitive examples, defined\nas examples where not all candidate instructions\nyield the same ROUGE score. We sample 100 ad-\nditional examples from each of 100 tasks that are\nseen in training but not included in the dev set. As\npresented in Table 4, the model shows enhanced\nranking ability on seen tasks due to prior exposure\nto the instructions during training. This indicates\nthat our approach is useful in both data-rich and\ndata-scarce circumstances.\n5.4.4 Effect of More Training Tasks\nTo analyze the effect of large-scale multi-task train-\ning on out-of-domain generalizability, we manipu-\nlate the number of training tasks of the instruction\nranking model. Specifically, We exclude tasks from\nthe training set by their category,i.e., all tasks from\nselected categories are removed. As shown in Fig-\nure 4, the increment in the number of training tasks\nfrom additional categories is a key contributor to\nthe superior performance of our model compared\nto the random selection baseline. Since the per-\nformance has not plateaued when all tasks are in-\ncluded, it is plausible to expect further performance\ngains if more training tasks are available.\n5.4.5 Analysis of the Selected Instructions\nFigure 6 illustrates how our selected instructions\nimprove the original human instructions. As in-\ndicated by the average similarity scores, Auto-\nInstruct is able to provide instructions more simi-\nlar to the optimal ones among the candidates. As\ndemonstrated by the scatter plot, in scenarios where\nthe selected instruction outperforms the human in-\nstruction, its embedding usually deviates signifi-\ncantly from that of the human instruction but stays\nclose to the optimal one. These results suggest that\nthe selected instruction refines the human-written\nseed instruction by progressing towards the ideal\nsolution, while the embedding distance between the\nselected and seed instructions makes such improve-\nment hard to achieve by pure manual engineering.\nIn addition, we offer a case study in Figure 5\nin the zero-shot setting where the LLM cannot re-\nfer to any demonstrations. Nevertheless, the LLM\nmanages to generate additional examples using the\nknowledge gained from its extensive pre-training.\nThese additional examples can act as demonstra-\ntions to create a ‚Äú2-shot inference‚Äù setting, leading\nto a correct prediction that could not be achieved\nvia the original zero-shot inference. Conversely, we\nalso present an example where the LLM-generated\ninstruction includes hallucinated descriptions that\ndistort the original meaning of the seed instruc-\ntion. The mismatch between this instruction and\nthe test example results in its rejection by the rank-\ning model. Readers may find further case studies\nin Appendix F.\n6www.sbert.net/docs/pretrained_models.html\n9857\nHuman Instruction: Given a tuple, determine whether the Head is used for the Tail or not. Only answer with ‚ÄúYes‚Äù or ‚ÄúNo‚Äù.\nGenerated Instruction (Selected by the Ranking Model): In this task, you are given a tuple which is a set of two \nelements, Head and Tail. Your task is to determine whether the Head is used for the Tail or not. To do this, you need to \nconsider the meaning of the Head and Tail and determine whether the Head can be used to achieve the purpose of the Tail. \nFor example, if the Head is ‚Äúhammer‚Äù and the Tail is ‚Äúnailing a nail‚Äù, then the answer is ‚ÄúYes‚Äù because a hammer can be \nused to nail a nail. If the Head is ‚Äúhammer‚Äù and the Tail is ‚Äúcutting a board‚Äù, then the answer is ‚ÄúNo‚Äù because a hammer \ncannot be used to cut a board. Only answer with ‚ÄúYes‚Äù or ‚ÄúNo‚Äù.\nTest Input: Head: dental floss. Tail: provide dental hygiene.\n Human Instruction \nAnswer: No\nAuto-Instruct \nAnswer: Yes\nCase Study (Zero-Shot)\nGenerated Instruction (Discarded by the Ranking Model): To determine whether the Head is used for the Tail or not, you \nneed to compare the two elements of the tuple. If the Head is the same as the Tail, then the answer is ‚ÄúYes‚Äù. If the Head is \nnot the same as the Tail, then the answer is ‚ÄúNo‚Äù. \nFigure 5: In this case, Auto-Instruct selects an instruction which ‚Äútransforms‚Äù the zero-shot inference to a ‚Äú2-\nshot‚Äù inference by providing additional examples (highlight in red), while discarding an instruction that includes\nhallucination in the task description (highlight in blue). The human instruction is also included in ranking candidates.\nCosine Similarity GPT-ada-002        MPNet-base\nHuman v.s. Best 89.22                    73.22\nAuto-Instruct v.s. Best          91.20 75.43\nFigure 6: Above: Instruction embeddings of 5 SuperNI\ntasks where Auto-Instruct selected instruction performs\nbetter than human instruction, as visualized by T-SNE.\n‚ÄúBest‚Äù refers to the instruction with the highest ROUGE\nscore. Below: Average cosine similarity between in-\nstruction embeddings on all SuperNI tasks. Two embed-\nding models are text-embedding-ada-002 from OpenAI\nand all-mpnet-base-v2 from Sentence-Transformers6.\nBest viewed in color.\n6 Conclusion\nIn this work, we introduce Auto-Instruct, an au-\ntomatic approach of generating, ranking and se-\nlecting instructions, which offers a solution to the\nhigh cost and subjectivity associated with human-\nengineered instructions. Our approach begins by\nprompting the LLM to generate a diverse set of\ncandidate instructions. Next, an instruction ranking\nmodel trained on hundreds of tasks is used to rank\nthe candidate instructions and select the most effec-\ntive one to solve a specific example. Experimental\nresults demonstrate that our approach provides bet-\nter instructions than both human-written ones and\nthose produced by previous instruction generation\napproaches, as tested on 118 out-of-domain tasks.\nLimitations\nTo our knowledge, this work has the following\nlimitations:\n‚Ä¢ Due to the considerable cost associated with\nOpenAI models, and the limited capacity of\ntheir API interface, we only score the can-\ndidate instructions on a moderate number of\ntasks as described in ¬ß4.2.2. Given the re-\nsults in Figure 4, we expect that the model\ncould demonstrate improved generalizability\nif more training data with labeled instructions\nwere available.\n‚Ä¢ The scope of this study is limited to the gener-\nation of instructions in English; tasks in non-\nEnglish languages are not part of our training\ndata. As a result, the model might not perform\nsatisfactorily for non-English tasks. Further\ninvestigation into generating cross-lingual in-\nstructions is left for future work.\n‚Ä¢ Despite employing a wide range of meta-\nprompts, which significantly mitigates the de-\npendence on prompt engineering, the phrasing\nof these meta-prompts could still influence the\nquality of the instructions generated. We leave\nthe exploration of automatically diversify the\ngenerated instructions as future work.\n9858\nAcknowledgements\nThis work was supported by NSF IIS-2119531,\nIIS-2137396, IIS-2142827, IIS-2234058, CCF-\n1901059, and ONR N00014-22-1-2507. We thank\nCanwen Xu (University of California San Diego)\nfor his valuable suggestions during paper writing.\nReferences\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Y . Zhao,\nYanping Huang, Andrew M. Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V . Le, and Jason Wei.\n2022. Scaling instruction-finetuned language models.\nArXiv preprint, 2210.11416.\nMingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan\nWang, Han Guo, Tianmin Shu, Meng Song, Eric P\nXing, and Zhiting Hu. 2022. Rlprompt: Optimizing\ndiscrete text prompts with reinforcement learning.\nArXiv preprint, 2205.12548.\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong\nWu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and\nZhifang Sui. 2023. A survey for in-context learning.\nArXiv preprint, 2301.00234.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text\ndegeneration. In 8th International Conference on\nLearning Representations, ICLR 2020.\nOr Honovich, Uri Shaham, Samuel R. Bowman, and\nOmer Levy. 2022. Instruction induction: From\nfew examples to natural language task descriptions.\nArXiv preprint, 2205.10782.\nZhenwen Liang, Dian Yu, Xiaoman Pan, Wenlin Yao,\nQingkai Zeng, Xiangliang Zhang, and Dong Yu.\n2023a. Let gpt be a math tutor: Teaching math word\nproblem solvers with customized exercise generation.\nIn Proceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2023.\nZhenwen Liang, Dian Yu, Xiaoman Pan, Wenlin Yao,\nQingkai Zeng, Xiangliang Zhang, and Dong Yu.\n2023b. Mint: Boosting generalization in mathe-\nmatical reasoning via multi-view fine-tuning. ArXiv\npreprint, 2307.07951.\nPan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu,\nSong-Chun Zhu, Tanmay Rajpurohit, Peter Clark,\nand Ashwin Kalyan. 2022a. Dynamic prompt learn-\ning via policy gradient for semi-structured mathemat-\nical reasoning. ArXiv preprint, 2209.14610.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2022b. Fantastically ordered\nprompts and where to find them: Overcoming few-\nshot prompt order sensitivity. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics, ACL 2022.\nAditi Mishra, Utkarsh Soni, Anjana Arunkumar, Jinbin\nHuang, Bum Chul Kwon, and Chris Bryan. 2023.\nPromptaid: Prompt exploration, perturbation, testing\nand iteration using visual analytics for large language\nmodels. ArXiv preprint, 2304.01964.\nOpenAI. 2023. GPT-4 technical report. ArXiv preprint,\n2303.08774.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex\nRay, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke Miller, Maddie Simens, Amanda Askell, Peter\nWelinder, Paul F. Christiano, Jan Leike, and Ryan\nLowe. 2022. Training language models to follow\ninstructions with human feedback. In Annual Con-\nference on Neural Information Processing Systems\n2020, NeurIPS 2022.\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. 2021.\nTrue few-shot learning with language models. In Ad-\nvances in Neural Information Processing Systems 34:\nAnnual Conference on Neural Information Process-\ning Systems 2021, NeurIPS 2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\n2022. Learning to retrieve prompts for in-context\nlearning. In Proceedings of the 2022 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2022.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal V . Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala Neeraj, Jos Rozen, Abheesht Sharma, An-\ndrea Santilli, Thibault F√©vry, Jason Alan Fries, Ryan\nTeehan, Teven Le Scao, Stella Biderman, Leo Gao,\nThomas Wolf, and Alexander M. Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization. In The Tenth International Conference on\nLearning Representations, ICLR 2022.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\n9859\nIn Proceedings of the 35th International Conference\non Machine Learning, ICML 2018, Proceedings of\nMachine Learning Research.\nWeijia Shi, Xiaochuang Han, Hila Gonen, Ari Holtzman,\nYulia Tsvetkov, and Luke Zettlemoyer. 2022. Toward\nhuman readable prompt tuning: Kubrick‚Äôs the shining\nis a good movie, and a good prompt too? ArXiv\npreprint, 2212.10539.\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV ,\nEric Wallace, and Sameer Singh. 2020. Autoprompt:\nEliciting knowledge from language models with au-\ntomatically generated prompts. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2020.\nChandan Singh, John X. Morris, Jyoti Aneja, Alexan-\nder M. Rush, and Jianfeng Gao. 2022. iprompt: Ex-\nplaining data patterns in natural language via inter-\npretable autoprompting. ArXiv preprint, 2210.01848.\nMirac Suzgun, Nathan Scales, Nathanael Sch√§rli, Se-\nbastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V . Le, Ed H. Chi,\nDenny Zhou, and Jason Wei. 2022. Challenging\nbig-bench tasks and whether chain-of-thought can\nsolve them. ArXiv preprint, 2210.09261.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aur√©lien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023. Llama 2: Open foundation and fine-\ntuned chat models. ArXiv preprint, 2307.09288.\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-\n6B: A 6 Billion Parameter Autoregressive Lan-\nguage Model. https://github.com/kingoflolz/\nmesh-transformer-jax.\nXinyi Wang, Wanrong Zhu, and William Yang Wang.\n2023a. Large language models are implicitly topic\nmodels: Explaining and finding good demonstrations\nfor in-context learning. ArXiv preprint, 2301.11916.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V .\nLe, Ed H. Chi, Sharan Narang, Aakanksha Chowd-\nhery, and Denny Zhou. 2023b. Self-consistency im-\nproves chain of thought reasoning in language mod-\nels. In The Eleventh International Conference on\nLearning Representations, ICLR 2023.\nYizhong Wang, Swaroop Mishra, Pegah Alipoormo-\nlabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva\nNaik, Arjun Ashok, Arut Selvan Dhanasekaran, An-\njana Arunkumar, David Stap, Eshaan Pathak, Gi-\nannis Karamanolakis, Haizhi Gary Lai, Ishan Puro-\nhit, Ishani Mondal, Jacob Anderson, Kirby Kuz-\nnia, Krima Doshi, Kuntal Kumar Pal, Maitreya Pa-\ntel, Mehrad Moradshahi, Mihir Parmar, Mirali Puro-\nhit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit\nVerma, Ravsehaj Singh Puri, Rushang Karia, Savan\nDoshi, Shailaja Keyur Sampat, Siddhartha Mishra,\nSujan Reddy A, Sumanta Patro, Tanay Dixit, and\nXudong Shen. 2022. Super-naturalinstructions: Gen-\neralization via declarative instructions on 1600+ NLP\ntasks. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2022.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021. Finetuned lan-\nguage models are zero-shot learners. ArXiv preprint,\n2109.01652.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed H Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. In Advances in Neu-\nral Information Processing Systems, NeurIPS 2022.\nJules White, Quchen Fu, Sam Hays, Michael Sandborn,\nCarlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse\nSpencer-Smith, and Douglas C. Schmidt. 2023. A\nprompt pattern catalog to enhance prompt engineer-\ning with chatgpt. ArXiv preprint, 2302.11382.\nXi Ye and Greg Durrett. 2023. Explanation selection\nusing unlabeled data for in-context learning. ArXiv\npreprint, 2302.04813.\nMengxia Yu, Zhihan Zhang, Wenhao Yu, and Meng\nJiang. 2023. Pre-training language models for com-\nparative reasoning. In Proceedings of the 2023 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2023.\nZhihan Zhang, Wenhao Yu, Zheng Ning, Mingxuan Ju,\nand Meng Jiang. 2023. Exploring contrast consis-\ntency of open-domain question answering systems\non minimally edited questions. Transactions of the\nAssociation for Computational Linguistics, TACL, 11.\nZhihan Zhang, Wenhao Yu, Chenguang Zhu, and Meng\nJiang. 2022a. A unified encoder-decoder framework\n9860\nwith entity memory. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2022.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2022b. Automatic chain of thought prompt-\ning in large language models. ArXiv preprint ,\n2210.03493.\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,\nKeiran Paster, Silviu Pitis, Harris Chan, and Jimmy\nBa. 2022. Large language models are human-level\nprompt engineers. ArXiv preprint, 2211.01910.\nA Training Data Pre-Processing\nAs detailed in ¬ß4.2, the instruction ranking model is\ntrained to rank candidate instructions against their\ndownstream performance on the LLM. The down-\nstream performance of an instruction Ic refers to\nhow well the LLM‚Äôs predicted outputÀÜymatches the\nground-truth output ywhen using Ic to prompt the\nLLM, as quantified by the ROUGE-L scorer(y,ÀÜy).\nTo calculate this score, we pair each training exam-\nple with all 22 candidate instructions of the corre-\nsponding task (generated with the method in ¬ß4.1),\nand collect the LLM‚Äôs predicted output to the ex-\nample prompted by each candidate instruction. Af-\nter calculating the ROUGE-L scores against the\nground-truth, we discard examples where candi-\ndate instructions are not distinctly rankable ‚Äì in\ncases where the range of downstream performance\nacross different instructions is less than 10 points\nin ROUGE-L.\nTo accelerate the training process, we sample 8\ncandidate instructions from the total pool of 22 for\neach example, and train the model to rank these\n8 instructions. However, in some tasks, certain\ninstructions may significantly outperform others.\nUniformly sampling 8 candidate instructions could\nresult in such ‚Äúextraordinary‚Äù instructions being\ndisproportionately favored too many times in the\ntraining of the ranking model. To address this, we\ninversely proportion the sampling rate of each in-\nstruction to its popularity (i.e., the number of cases\nwhere this instruction is superior to all others). Fi-\nnally, we sample up to 400 examples from each\ntraining task, which leads to 122k training exam-\nples in total.\nB Detailed Experimental Settings\nThe instruction ranking model is initialized with\nFLAN-T5-Large (780M parameters; Chung et al.,\n2022), and is trained using Adafactor (Shazeer and\nStern, 2018) with learning rate 5e-5, batch size 128\nand dropout rate 0.1. We employ an in-domain\ndev set including a total of 5k unseen examples\nfrom 100 training tasks to select the best check-\npoint within 5 epochs. The validation performance\non the dev set is 67.66 in ROUGE-L, while random\nselection only achieves a score of 54.28. When\nusing OpenAI models, for instruction generation,\nwe set the maximum instruction length to be 300 to-\nkens, and we use a temperature of 1.0 andtop_p of\n0.75 for token sampling; for downstream inference,\nwe set both to 0 for deterministic outputs. Gen-\n9861\nData:\nInput: [input of demonstration #1]\nOutput: [output of demonstration #1]\n‚Ä¶‚Ä¶\nInput: [input of demonstration #n]\nOutput: [output of demonstration #n]\nInstruction: \nFigure 7: The meta-prompt of instruction generation\nwith iPrompt7.\nWrite an instruction on how to solve the \nfollowing task.\nTask: [seed instruction]\nExamples:\nInput: [input of demonstration #1]\nOutput: [output of demonstration #1]\n‚Ä¶‚Ä¶\nInput: [input of demonstration #n]\nOutput: [output of demonstration #n]\nInstruction: \nFigure 8: The meta-prompt of instruction generation\nwith iPrompt+, similar to ours in Figure 10.\nerating all candidate instructions for 91 SuperNI\ntest tasks cost us 18 USD in total, according to\nOpenAI‚Äôs pricing (0.02 USD per 1k tokens for text-\ndavinci-003). In text-davinci-003 experiments, the\nrandom selection score is calculated as the aver-\nage score across all instructions on each example,\nincluding the human-written seed instruction. In\nChatGPT and GPT-4 instructions, due to the lim-\nited capacity of their API interfaces, we randomly\nsample an instruction for each example and test its\nperformance.\nC The iPrompt Baseline\nIn this section, we outline the adaptations made to\nthe iPrompt8 (Singh et al., 2022) method for our\nsetting. We mainly address two discrepancies be-\ntween its original implementation and our setup:\n(1) the original iPrompt generates instructions us-\ning GPT-J (Wang and Komatsuzaki, 2021), and (2)\nit uses a validation set to score and select instruc-\ntions. To address (1), we use text-davinci-003 for\nits instruction generation, identical to the model\n7In the original iPrompt implementation, the meta-prompt\nends with the suffix Prompt:. However, this leads to incoher-\nent instruction generation on our benchmarks. Therefore, we\nchanged it to Instruction: which addressed this issue.\n8www.github.com/csinva/imodelsX/tree/master/\nimodelsx/iprompt\nused for downstream inference. For (2), we con-\nduct its instruction validation on the 3-shot demon-\nstrations. Due to the cost of iteratively requesting\nthe OpenAI API, we incorporate an early stopping\ncriterion which halts the process if the validation\nperformance9 has not improved for 10 iterations.\nActually, almost all tasks stopped before 30 iter-\nations. Following this, We select the instruction\nwith the best validation performance to evaluate on\nthe test examples.\nAccording to the original codebase, we use the\nmeta-prompt shown in Figure 7 for instruction gen-\neration with iPrompt. Since this meta-prompt does\nnot utilize any task description, for a fair compar-\nison, we implement an iPrompt+ baseline with a\nsimilar meta-prompt to our method which utilizes\nthe seed instruction, as shown in Figure 8. Readers\ncan refer to the original paper (Singh et al., 2022)\nfor technical details of iPrompt.\nD Additional Experimental Results\nIn this section, we present more experimental re-\nsults in addition to those analyzed in Section 5. All\nexperiments in this section are conducted in the\nfew-shot setting unless stated otherwise.\nD.1 SuperNI Results by Task Category\nHere, we present the detailed experimental results\non 8 different categories of SuperNI test tasks (see\nAppedix G for the list of test tasks). As shown\nin Figure 9, Auto-Instruct surpasses the human-\nwritten and random instructions no matter it is\nevaluated on classification, extraction or genera-\ntion tasks, with the only exception as answerability\nclassification. Notably, Auto-Instruct outperforms\nthe original human-written instruction by 10%, 9%\nand 8% on commonsense classification (classifica-\ntion tasks), word analogy (short generation tasks)\nand dialogue generation (long generation tasks),\nrespectively.\nD.2 Generalization to Other LLMs\nIn addition to Section 5.4.1, we further assess the\ngeneralizability of Auto-Instruct to open-source\nLLMs. As demonstrated in Table 5, instructions\nselected by Auto-Instruct enhance the performance\nof LLaMA-2-chat (Touvron et al., 2023). This once\nagain underscores the capability of Auto-Instruct\n9The average ROUGE-L score between the LLM‚Äôs pre-\ndicted output and the ground-truth on validation data.\n9862\nHuman Random Selected\n75\n77\n79\n81\n83\n77.65\n79.78\n81.35\nCoherence Classification\nHuman Random Selected\n40\n41\n42\n43\n44\n45\n41.82\n42.44\n42.98\nData to T ext\nHuman Random Selected\n68\n69\n70\n71\n72\n70.12\n70.61\n70.3\nAnswerability Classification\nHuman Random Selected\n65\n67\n69\n71\n73\n67.57\n68.04\n70.44\nInformation Extraction\nHuman Random Selected\n58\n60\n62\n64\n66\n68\n70\n61.28\n62.41\n67.17\nCommonsense Classification\nHuman Random Selected\n60\n62\n64\n66\n68\n70\n72\n63.59 62.98\n69.06\nWord Analogy\nHuman Random Selected\n60\n62\n64\n66\n62.8\n63.79 64.02\nCode to T ext\nHuman Random Selected\n32\n34\n36\n38\n34.13\n34.66\n36.73\nDialogue Generation\nFigure 9: Few-shot performance of instructions selected by Auto-Instruct (denoted as ‚ÄúSelected‚Äù) on all 8 categories\nof SuperNI test tasks, compared to human-written and random selected instructions.\nMethods LLaMA-2-chat-7B\nFew-shot, instructions from text-davinci-003\nHuman 53.87\nRandom 54.18\nAuto-Instruct 55.90\nTable 5: SuperNI results of transferring Auto-Instruct\nto LLaMA-2-chat-7B, using instructions generated by\ntext-davinci-003. The instruction ranking model is still\nthe one trained on text-davinci-003 instructions.\nto generalize across different LLMs without re-\ntraining the instruction ranking model. It is worth\nnoting that we use instructions generated by text-\ndavinci-003 in these experiments, because both\nthe 7B and 13B versions of LLaMA-2-chat exhibit\nweaker abilities in following our meta-prompts for\ninstruction generation, contrasted with mega-size\nGPT models. We leave the study of instruction\ngeneration with recent open-source LLMs as future\nwork.\nD.3 Compare to Answer Ensemble\nGiven that Auto-Instruct includes sampling mul-\ntiple candidate instructions before selecting the\nbest one, we compare it to another sampling ap-\nproach, i.e., sampling and ensembling multiple an-\nswers. Using the original human-written instruc-\ntion, we sample responses 10 times with nucleus\nsampling (Holtzman et al., 2020), without sam-\npling multiple instructions. Then, we ensemble\nall 10 responses by marginalizing the LM prob-\nability of each unique response before selecting\nthe most probable one, similar to the idea of self-\nMethod Score\nHuman 60.94\nHuman (Ensemble) 61.08\nAuto-Instruct 64.35\nTable 6: Results of multi-answer ensemble prompted by\nhuman-written instructions on SuperNI test tasks.\nconsistency (Wang et al., 2023b). The results,\nshown in Table 6, indicate that the answer ensem-\nble approach only brings a marginal improvement\non SuperNI, which is not comparable to the perfor-\nmance gain achieved with Auto-Instruct.\nE Meta-Prompts for Instruction\nGeneration\nIn this section, we list all meta-prompts utilized dur-\ning instruction generation, as outlined in ¬ß4.1. For\nthe zero-shot setting, we omit the ‚ÄúExamples‚Äù field\nin the meta-prompt to let the LLM rephrase the seed\ninstruction. Besides, the meta-prompt with explana-\ntions to the demonstrations is not applicable in the\nzero-shot setting. The meta-prompt that uses other\ntasks as demonstrations (Figure 10e) is integrated\nwith three groups of demonstration tasks, each vary-\ning in the average instruction length. Therefore, the\nLLM is prompted to generate instructions of similar\ngranularity to the demonstration tasks. Demonstra-\ntion tasks are sampled from SuperNI. In SuperNI,\neach task is paired with a concise task summary\nand a detailed task definition which is usually much\nlonger. For each demonstration task, we use the\n9863\nWrite an instruction on how to solve the \nfollowing task in one sentence.\nTask: [seed instruction]\nExamples:\nInput: [input of demonstration #1]\nOutput: [output of demonstration #1]\n‚Ä¶‚Ä¶\nInput: [input of demonstration #n]\nOutput: [output of demonstration #n]\nInstruction: \n(a) One-sentence instruction\nWrite an instruction on how to solve the \nfollowing task in one paragraph.\nTask: [seed instruction]\nExamples:\nInput: [input of demonstration #1]\nOutput: [output of demonstration #1]\n‚Ä¶‚Ä¶\nInput: [input of demonstration #n]\nOutput: [output of demonstration #n]\nInstruction: (b) One-paragraph instruction\nWrite a step-by-step instruction on how \nto solve the following task.\nTask: [seed instruction]\nExamples:\nInput: [input of demonstration #1]\nOutput: [output of demonstration #1]\n‚Ä¶‚Ä¶\nInput: [input of demonstration #n]\nOutput: [output of demonstration #n]\nInstruction: (c) Step-by-step instruction\nWrite a step-by-step instruction on how \nto solve the following task.\nTask: [seed instruction]\nExamples:\nInput: [input of demonstration #1]\nOutput: [output of demonstration #1]\n‚Ä¶‚Ä¶\nInput: [input of demonstration #n]\nOutput: [output of demonstration #n]\nInstruction: \n(d) Example explanation instruction\nWrite an instruction on how to solve the following task.\nTask: [seed instruction of task #1]\nExamples: [input-output demonstrations of task #1]\nInstruction: [instruction of task #1]\n‚Ä¶‚Ä¶\nTask: [seed instruction of task #N]\nExamples: [input-output demonstrations of task #N]\nInstruction: [instruction of task #N]\nTask: [seed instruction of the test task]\nExamples: [input-output demonstrations of the test task]\nInstruction: (e) Use other tasks as demonstrations\nFigure 10: Meta-prompts that we use to specify different desired styles of instructions during instruction generation.\nFor Figure 10e, we collect 3 groups of demonstration tasks with varying average instruction length (short, medium,\nlong), so as to guide the LLM to generate instructions of different granularities.\ntask summary as the seed instruction and the task\ndefinition as the target instruction. We abstain from\nutilizing the task definition in test tasks because (1)\nsome task definitions are too long to fit in the T5\nmodel together with the input (2) we practically\nfind that the LLM tends to repeat the task definition\nto a large extent if it is used as the seed instruction.\nAlthough Auto-Instruct has never seen the much\nlonger task definition of test tasks, our selected in-\nstruction still performs better than using the task\ndefinition as the instruction, which holds an av-\nerage score of 62.41 on SuperNI in the few-shot\nsetting. We leave the exploration of integrating\nmore complicated instructions as future work.\nF Additional Case Study\nIn this section, we provide 3 more cases (2 few-shot\nand 1 zero-shot) where Auto-Instruct improves the\noriginal human-written instructions. These case\nstudies are shown in Figure 11, 12, and 13. Please\nrefer to the corresponding captions for detailed case\nexplanations.\nG All Test Tasks\nIn Table 7, we list all 91 SuperNI test tasks used\nin our out-of-domain experiments. Since the size\nof tasks is imbalanced on SuperNI, for efficient\nevaluation, we randomly sample 200 instances for\neach task, making a total of 18,200 test examples.\n9864\nHuman Instruction: Given a sentence from a medical study paper, select the tokens representing information about participants.\nAuto-Instruct: Scan the sentence and look for information related to participants, such as descriptions of age, numbers, or \nhealth conditions, and select the tokens that encapsulate this information. For example, given the sentence ‚ÄúParticipants were 58 \nchildren (46 boys) with autism between 3 and 4 years of age‚Äù, the output should be ‚Äú58 children (46 boys) with autism between\n3 and 4 years of age.‚Äù\nTest Input: To consider the impact on primary care patient outcomes of \nusing both a screener to determine elevated anxiety levels among patients \nwith previously undetected anxiety and a physician intervention to inform \nphysicians of their patients‚Äô conditions .\nHuman Instruction Answer: patients\nAuto-Instruct Answer: patients with \npreviously undetected anxiety\nCase Study (Few-Shot)\nFigure 11: A case study in the few-shot setting, where the few-shot demonstrations are omitted for brevity. The\ninitial human-written instruction provides general guidelines for extracting participant information from a medical\nstudy. However, it does not specify the scope of such information, leading to an answer that only includes the\nkeyword ‚Äúpatients‚Äù but ignores other relevant information. In contrast, Auto-Instruct provides an instruction that\ndelineates the types of participant information to be extracted (highlight in red), after seeing the output formats\nof the demonstrations. Prompted by the improved instruction which suggests health conditions are examples of\nthe requested information, the LLM generates a more comprehensive answer that incorporates the patients‚Äô health\nconditions, i.e., ‚Äúwith undetected anxiety‚Äù.\nHuman Instruction: Task description: Answer questions about which times certain events could have occurred.\nAuto-Instruct: To solve this task, you must answer questions about when certain events could have occurred. To do this, \nyou must analyze the information given in the question and the given examples. \nFor each example, you must analyze the information given about the events that happened before and after the event you \nare trying to find the time for. The question will usually give you the time that the person woke up and the time when the \nevent they are going to closed. From this, you must determine the time frame in which the event could have occurred. \nFor example, in the first example, we know that Emily woke up at 1pm and the museum closed after 7pm. Therefore, the \ntime frame in which Emily could have gone to the museum is between 1pm and 7pm. From the other information given, \nwe know that Emily was at the library from 2pm to 4pm, at the theater from 4pm to 5pm, at the airport from 5pm to 6pm, \nand at the mall from 6pm to 7pm. Therefore, the only time frame in which Emily could have gone to the museum is \nbetween 1pm and 2pm. Therefore, the correct answer is (A)‚Ä¶ (explanations of other two demonstrations are omitted)\nTest Input: Today, Jason went to the movies. Between what times could they have gone?\nWe know that: Jason woke up at 10am. Linda saw Jason getting a coffee at the cafe from \n10am to 3pm. Jennifer saw Jason walking towards the Statue of Liberty from 6pm to 7pm. \nSean saw Jason buying a phone at the electronics store from 7pm to 10pm. The movies was \nclosed after 10pm. Between what times could Jason have gone to the movies?\nOptions: (A) 3pm to 6pm (B) 10am to 3pm (C) 7pm to 10pm (D) 6pm to 7pm\nHuman Instruction \nAnswer: (C)\nAuto-Instruct \nAnswer: (A)\nCase Study (Few-Shot)\nFigure 12: A case study in the few-shot setting, where the few-shot demonstrations are omitted for brevity. The\nhuman instruction provides a general and concise description of the question‚Äôs requirements. In contrast, generated\nbased on the demonstrations, the instruction from Auto-Instruct offers a more concrete description about the\ninformation present in the input and emphasizes which aspects should be focused on (highlight in red). Besides,\nAuto-Instruct provides explanations of the few-shot demonstrations as complementary information for the LLM to\nunderstand these examples.\n9865\nHuman Instruction: Categorize the knowledge required to answer a pronoun coreference resolution question. Only choose \nyour answer from ‚ÄúQuantity‚Äù, ‚ÄúObject‚Äù, ‚ÄúOthers‚Äù, ‚ÄúProperty‚Äù, ‚ÄúSpatial‚Äù, and ‚ÄúTemporal‚Äù.\nAuto-Instruct: In this task, you will be given a pronoun coreference resolution question and you need to categorize the \nknowledge required to answer the question. You can only choose your answer from the provided list: ‚ÄúQuantity‚Äù, ‚ÄúObject‚Äù, \n‚ÄúOthers‚Äù, ‚ÄúProperty‚Äù, ‚ÄúSpatial‚Äù, and ‚ÄúTemporal‚Äù. ‚ÄúQuantity‚Äù knowledge refers to the knowledge of quantities, such as how \nmany, how much, etc. ‚ÄúObject‚Äù knowledge refers to the knowledge of the properties of objects, such as size, color, etc. \n‚ÄúOthers‚Äù knowledge refers to the knowledge of entities other than objects, such as people, places, events, etc. ‚ÄúProperty‚Äù \nknowledge refers to the knowledge of the properties of entities, such as occupation, location, etc. ‚ÄúSpatial‚Äù knowledge \nrefers to the knowledge of the spatial relations between entities, such as distance, direction, etc. ‚ÄúTemporal‚Äù knowledge \nrefers to the knowledge of the temporal relations between entities, such as order, sequence, etc.\nTest Input: Sentence: The painting in Mark's living room shows an oak \ntree. It is to the right of the bookcase. The ‚ÄúIt‚Äù refers to the painting. \nQuestion: What is the knowledge type needed to solve this relation?\nHuman Instruction Answer: Spatial\nAuto-Instruct Answer: Property\nCase Study (Zero-Shot)\nFigure 13: In this zero-shot classification case, the human-written instruction only provides the name of each\ncategory. As a result, the LLM can only attempt to determine the target category based on these single-word surface\nnames, which often lack sufficient clarity for differentiation. In contrast, the instruction provided by Auto-Instruct\nexplains the meaning of each category, which greatly facilitates the LLM‚Äôs comprehension of these categories. While\nAuto-Instruct tends to over-interpret when explaining the ‚ÄúOthers‚Äù category, most of the additional information\n(highlight in red) are useful for making more accurate predictions.\n9866\nTask Category Task Names\nCoherence\nClassification\ntask066_timetravel_binary_consistency_classification task070_abductivenli_incorrect_classification\ntask1573_samsum_classification task065_timetravel_consistent_sentence_classification\ntask298_storycloze_correct_end_classification\nData to Text\ntask1728_web_nlg_data_to_text task1407_dart_question_generation\ntask677_ollie_sentence_answer_generation task1409_dart_text_generation\ntask1598_nyc_long_text_generation task957_e2e_nlg_text_generation_generate\nAnswerability\nClassification\ntask349_squad2.0_answerable_unanswerable_question_classification task226_english_language_answer_relevance_classification\ntask020_mctaco_span_based_question task290_tellmewhy_question_answerability\ntask1439_doqa_cooking_isanswerable task1442_doqa_movies_isanswerable\ntask242_tweetqa_classification task1624_disfl_qa_question_yesno_classification\ntask520_aquamuse_answer_given_in_passage task050_multirc_answerability\nInformation\nExtraction\ntask1506_celebrity_minimal_dob_span task1517_limit_classfication\ntask456_matres_intention_classification task388_torque_token_classification\ntask1518_limit_answer_generation task1410_dart_relationship_extraction\ntask676_ollie_relationship_answer_generation task180_intervention_extraction\ntask749_glucose_reverse_cause_emotion_detection task684_online_privacy_policy_text_information_type_generation\ntask958_e2e_nlg_text_generation_parse task1413_dart_object_identification\ntask292_storycommonsense_character_text_generation task578_curiosity_dialogs_answer_generation\ntask1597_nyc_slot_filling task747_glucose_cause_emotion_detection\ntask678_ollie_actual_relationship_answer_generation task1510_evalution_relation_extraction\ntask1451_drug_dose_extraction task683_online_privacy_policy_text_purpose_answer_generation\ntask179_participant_extraction task1411_dart_subject_identification\ntask181_outcome_extraction task748_glucose_reverse_cause_event_detection\ntask621_ohsumed_yes_no_numerical_answer_generation task647_answer_generation\nCommonsense\nClassification\ntask1210_atomic_classification_madeupof task1215_atomic_classification_capableof\ntask1216_atomic_classification_causes task1202_atomic_classification_xneed\ntask136_winowhy_knowledge_categorization task1196_atomic_classification_oeffect\ntask291_semeval_2020_task4_commonsense_validation task1208_atomic_classification_xreason\ntask1206_atomic_classification_isbefore task1197_atomic_classification_oreact\ntask1213_atomic_classification_desires task116_com2sense_commonsense_reasoning\ntask1201_atomic_classification_xintent task1198_atomic_classification_owant\ntask1212_atomic_classification_hasproperty task1203_atomic_classification_xreact\ntask1214_atomic_classification_xwant task1200_atomic_classification_xeffect\ntask1209_atomic_classification_objectuse task1204_atomic_classification_hinderedby\ntask1207_atomic_classification_atlocation task1205_atomic_classification_isafter\ntask1199_atomic_classification_xattr\nWord Analogy\ntask1156_bard_analogical_reasoning_tools task1159_bard_analogical_reasoning_containers\ntask1155_bard_analogical_reasoning_trash_or_treasure task1157_bard_analogical_reasoning_rooms_for_containers\ntask1154_bard_analogical_reasoning_travel task1158_bard_analogical_reasoning_manipulating_items\ntask1152_bard_analogical_reasoning_causation task1153_bard_analogical_reasoning_affordance\nCode to Texttask131_scan_long_text_generation_action_command_long task129_scan_long_text_generation_action_command_short\ntask110_logic2text_sentence_generation\nDialogue\nGeneration\ntask1603_smcalflow_sentence_generation task1714_convai3_sentence_generation\ntask360_spolin_yesand_response_generation task574_air_dialogue_sentence_generation\ntask565_circa_answer_generation task576_curiosity_dialogs_answer_generation\ntask1600_smcalflow_sentence_generation task1729_personachat_generate_next\ntask1730_personachat_choose_next task361_spolin_yesand_prompt_response_classification\nTable 7: All SuperNI test tasks, grouped into different categories. These task categories are not seen during the\ntraining of the instruction ranking model. Besides, any task that is sourced from the same original dataset as any test\ntask is excluded from training.\n9867",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8139709234237671
    },
    {
      "name": "Task (project management)",
      "score": 0.768494725227356
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.6570855975151062
    },
    {
      "name": "Generalizability theory",
      "score": 0.595566987991333
    },
    {
      "name": "Process (computing)",
      "score": 0.5395210981369019
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.5281698107719421
    },
    {
      "name": "Natural language processing",
      "score": 0.4996681213378906
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47461429238319397
    },
    {
      "name": "Psychology",
      "score": 0.1361963450908661
    },
    {
      "name": "Programming language",
      "score": 0.12647190690040588
    },
    {
      "name": "Engineering",
      "score": 0.06328064203262329
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Developmental psychology",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210151458",
      "name": "Microsoft (Belgium)",
      "country": "BE"
    },
    {
      "id": "https://openalex.org/I107639228",
      "name": "University of Notre Dame",
      "country": "US"
    }
  ],
  "cited_by": 9
}