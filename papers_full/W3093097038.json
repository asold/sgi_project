{
  "title": "BioMegatron: Larger Biomedical Domain Language Model",
  "url": "https://openalex.org/W3093097038",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4221859382",
      "name": "Shin, Hoo Chang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1999911198",
      "name": "Zhang Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222949136",
      "name": "Bakhturina, Evelina",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227412475",
      "name": "Puri, Raul",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221878990",
      "name": "Patwary, Mostofa",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221882394",
      "name": "Shoeybi, Mohammad",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Mani, Raghav",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2971008823",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2169099542",
    "https://openalex.org/W2096235960",
    "https://openalex.org/W2346452181",
    "https://openalex.org/W2805206884",
    "https://openalex.org/W2971258845",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2922551710",
    "https://openalex.org/W1981208470",
    "https://openalex.org/W2937845937",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2148143831",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W1623072288",
    "https://openalex.org/W3034328552",
    "https://openalex.org/W2973727699"
  ],
  "abstract": "There has been an influx of biomedical domain-specific language models, showing language models pre-trained on biomedical text perform better on biomedical domain benchmarks than those trained on general domain text corpora such as Wikipedia and Books. Yet, most works do not study the factors affecting each domain language application deeply. Additionally, the study of model size on domain-specific models has been mostly missing. We empirically study and evaluate several factors that can affect performance on domain language applications, such as the sub-word vocabulary set, model size, pre-training corpus, and domain transfer. We show consistent improvements on benchmarks with our larger BioMegatron model trained on a larger domain corpus, contributing to our understanding of domain language model applications. We demonstrate noticeable improvements over the previous state-of-the-art (SOTA) on standard biomedical NLP benchmarks of named entity recognition, relation extraction, and question answering. Model checkpoints and code are available at [https://ngc.nvidia.com] and [https://github.com/NVIDIA/NeMo].",
  "full_text": "BioMegatron: Larger Biomedical Domain Language Model\nHoo-Chang Shin, Yang Zhang, Evelina Bakhturina,\nRaul Puri, Mostofa Patwary, Mohammad Shoeybi, Raghav Mani\nNVIDIA / Santa Clara, California, USA\nhshin@nvidia.com\nAbstract\nThere has been an inﬂux of biomedical\ndomain-speciﬁc language models, showing\nlanguage models pre-trained on biomedical\ntext perform better on biomedical domain\nbenchmarks than those trained on general do-\nmain text corpora such as Wikipedia and\nBooks. Yet, most works do not study the\nfactors affecting each domain language ap-\nplication deeply. Additionally, the study of\nmodel size on domain-speciﬁc models has\nbeen mostly missing. We empirically study\nand evaluate several factors that can affect\nperformance on domain language applica-\ntions, such as the sub-word vocabulary set,\nmodel size, pre-training corpus, and domain\ntransfer. We show consistent improvements\non benchmarks with our larger BioMegatron\nmodel trained on a larger domain corpus, con-\ntributing to our understanding of domain lan-\nguage model applications. We demonstrate\nnoticeable improvements over the previous\nstate-of-the-art (SOTA) on standard biomedi-\ncal NLP benchmarks of named entity recogni-\ntion, relation extraction, and question answer-\ning. Model checkpoints and code are avail-\nable at ngc.nvidia.com and github.com/\nNVIDIA/NeMo.\n1 Introduction\nEffectively transferring the success of BERT (De-\nvlin et al., 2018) to the biomedical domain, most\nnotably Lee et al. (2019) (BioBERT) and Beltagy\net al. (2019) (SciBERT) inspired a large number of\nsimilar works last year. For example, Peng et al.\n(2019); Alsentzer et al. (2019); Huang et al. (2019)\nadded clinical text to the PubMed biomedical pre-\ntraining corpus and tested on standard biomedical\nand clinical NLP benchmarks. Many other sim-\nilar works appeared at the ACL BioNLP Work-\nshop (Demner-Fushman et al., 2019).\nMore recently, Gu et al. (2020) performed a com-\nprehensive study on the pre-training corpus domain,\nlanguage model masking method, and adversarial\ntraining, benchmarking on a number of different\ndatasets for token classiﬁcation, sequence classiﬁ-\ncation, and sequence regression.\nCompared to the previous works, we perform a\nmore detailed study on (1) subword vocabulary, (2)\nlabeling method, (2) model size, and (3) domain\ntransfer, showing gains in token classiﬁcation, se-\nquence classiﬁcation, and question answering.\n2 Related Works\nA prime example of Language Models (LMs)\nin the biomedical domain is BioBERT (Lee\net al., 2019). It is a transformer LM pre-trained\non the PubMed (www.ncbi.nlm.nih.gov/pubmed)\nbiomedical text corpus comprised of biomedical\nliterature abstracts. Their pre-training started from\nthe checkpoint of Devlin et al. (2018) trained on\nWikipedia and Books-Corpus. Independently, Belt-\nagy et al. (2019) (SciBERT) pre-trained BERT\nfrom scratch using their vocabulary set on scientiﬁc\ntext corpora, including PubMed abstracts and com-\nputer science papers. Both demonstrated increased\nperformance over the previous non-BERT SOTA on\nbiomedical benchmarks, including Named Entity\nRecognition (NER), Relation Extraction (RE), and\nQuestion Answering (QA). BioBERT and SciB-\nERT report similar results on NER and RE, while\nonly BioBERT report QA results.\nThey inspired other follow-up works (Alsentzer\net al., 2019; Huang et al., 2019; Peng et al., 2019),\nmost notably translating their success to the clini-\ncal domain, adding the MIMIC-III (Johnson et al.,\n2016) clinical text corpus. Gu et al. (2020) (Pub-\nMedBERT) used the PubMed full-text for pre-\ntraining in addition to the abstracts, and use a do-\nmain vocabulary set learned from PubMed corpus.\nMeanwhile, they mostly report similar NER and\nRE tests and results, and only BioBERT reports QA\narXiv:2010.06060v2  [cs.CL]  14 Oct 2020\nresults. Additionally, most use a BERTBase with\n110M parameters. Peng et al. (2019) report slightly\nimproved performance on RE using BERTLarge\nwhile reporting worse results on NER, compared\nto BERTBase. These results on biomedical tasks do\nnot beneﬁt from scaling model size to the same de-\ngree as standard NLP benchmarks such as GLUE or\nSQuAD (Shoeybi et al., 2019; Raffel et al., 2019).\n3 Language Model Pre-training\nBERTBase & Large We compare our models\nto the pre-trained BERTBase & Large models of\nBioBERT (Lee et al., 2019) and PubMedBERT (Gu\net al., 2020) (BERTBase) for ﬁne-tuning and eval-\nuation. For QA we use the BERTLarge variant of\nBioBERT following the authors’ recommendation.\nBioMegatron Megatron-LM (Shoeybi et al.,\n2019) was introduced for efﬁcient model parallel\ntraining of large LMs, with up to 8.3B parameters.\nShoeybi et al. (2019) showed that rearranging the\norder of the layer normalization and the residual\nconnections is critical to enabling the scaling of the\nBERT-style models beyond 336m parameters, and\nwe use the same architecture.\nMegatron-LM also used a larger pre-training\ntext corpus, comprised of Wikipedia (Devlin et al.,\n2018), CC-Stories (Trinh and Le, 2018), Real-\nNews (Zellers et al., 2019), and OpenWebtext\n(Radford et al., 2019). For our LM training,\nwe use the 4.5 billion-word PubMed abstract set\nand the 1.6 billion-word CC0-licensed Commer-\ncial Use Collection of the PMC full-text corpus\n(www.ncbi.nlm.nih.gov/pmc).\nWe train three sizes of BioMegatron: with\n345 million, 800 million, and 1.2 billion num-\nber of parameters (Table 1). We compare\nfour pre-training scenarios in the smallest 345m\nmodel - using BERT-cased/uncased vocabular-\nies, each pre-trained from scratch and ﬁne-\ntuned from general domain LM. We also com-\npare two sets of domain vocabularies learned\non PubMed text corpus using SentencePiece\n(github.com/google/sentencepiece) library, each\ncontaining 30k and 50k subword units.\nWe train the larger BioMegatron models with\nless variation: 800m models from scratch on\nPubMed with BERT -cased/-uncased vocabular-\nies; and 1.2b model starting from general domain\nLM checkpoint using BERT-uncased vocabulary.\n#Parameters #Layers #Hidden Size #Attention Heads\n345m 24 1024 16\n800m 36 1280 20\n1.2b 24 2048 16\nTable 1: Model conﬁgurations.\n4 Downstream Benchmark Tasks\nWe use the most widely used downstream biomedi-\ncal benchmark datasets for NER, RE, and QA.\nNamed Entity Recognition The BC5CDR (Li\net al., 2016) NER dataset annotated disease and\nchemical terms with IOB tagging (Ramshaw and\nMarcus, 1999). In NCBI-disease (Do ˘gan et al.,\n2014), only disease entities are IOB-tagged.\nRelation Extraction The ChemProt (Krallinger\net al., 2015) dataset contains sentences from\nPubMed abstracts, where chemical-protein interac-\ntion types are annotated as ﬁve categories. Relation\nExtraction is essentially a sequence classiﬁcation\ntask, classifying a set of sentences into a category.\nQuestion Answering The BioASQ-7b factoid\ntask (Tsatsaronis et al., 2015) is a biomedical QA\ndataset whose format is similar to the SQuAD\ndataset (Rajpurkar et al., 2016). In this task,\ncontext-snippet, question and answer triplets, and\nfactoid question/answers are evaluated with strict\naccuracy (SAcc) , lenient accuracy (LAcc) , and\nmean reciprocal rank (MRR).\n5 Results and Discussion\nThe evaluation results on NER and RE are shown in\nTable 2, and QA are shown in Table 3. We perform\nentity-level F1 NER using the ofﬁcial CoNLL eval-\nuation script translated into Python (github.com/\nspyysalo/conlleval.py). RE uses micro-level\nF1, and QA uses the BioASQ evaluation script\n(github.com/BioASQ/Evaluation-Measures).\n5.1 Named Entity Recognition\nWhile the NER benchmark datasets appear satu-\nrated due to the small sample size, we ﬁnd that the\nsubword vocabulary is the most critical factor. Ex-\namples of tokenization with different vocabularies\nare shown in Figure 1. Representing named enti-\nties as single terms is more helpful than breaking\nthem into several subtokens. Table 4 shows the\nrate named entities break into sub-tokens for each\nbenchmark training set with different sub-word vo-\ncabularies. PubMedBERT vocabulary set has a low\nBenchmark Model #Parameters Vocabulary Prec Rec F1\nNER\nBC5CDR-chem\nBioBERT 110m BERT-cased 90.0 93.4 91.7\nPubMedBERT 110m PubMedBERT-vocab (30k) 92.1 93.2 92.6\nBioMegatron 345m Bio-vocab-30k 92.1 93.6 92.9\nBioMegatron 345m Bio-vocab-50k 92.9 92.0 92.5\nBioMegatron 800m BERT-cased 91.3 92.9 92.1\nBioMegatron 1.2b BERT-uncased 92.0 90.5 91.3\nBC5CDR-disease\nBioBERT 110m BERT-cased 85.0 89.4 87.2\nPubMedBERT 110m PubMedBERT-uncased (30k) 86.2 88.4 87.3\nBioMegatron 345m Bio-vocab-30k 85.2 88.8 87.0\nBioMegatron 345m Bio-vocab-50k 86.1 91.0 88.5\nBioMegatron 800m BERT-cased 85.8 90.1 87.9\nBioMegatron 1.2b BERT-uncased 83.8 89.2 86.4\nNCBI-disease\nBioBERT 110m BERT-cased 85.0 90.0 87.5\nPubMedBERT 110m PubMedBERT-uncased (30k) 85.9 87.7 86.8\nBioMegatron 345m Bio-vocab-30k 85.6 88.6 87.1\nBioMegatron 345m Bio-vocab-50k 83.7 90.4 87.0\nBioMegatron 800m BERT-cased 87.0 88.8 87.8\nBioMegatron 1.2b BERT-uncased 83.5 90.1 86.7\nREChemProt\nBioBERT 110m BERT-cased 76.5 73.3 74.8\nPubMedBERT 110m PubMedBERT-uncased (30k) 73.6 77.7 75.6\nBioMegatron 345m Bio-vocab-30k 77.8 72.5 75.1\nBioMegatron 345m Bio-vocab-50k 74.5 79.7 77.0\nBioMegatron 800m BERT-cased 80.4 68.9 74.3\nBioMegatron 1.2b BERT-uncased 82.0 65.6 72.9\nTable 2: Evaluation results on NER and RE after ﬁne-tuning for 30 epochs with hyper-parameter settings of:\nnum-fc-layers: {1, 2}; fc-hidden-size: {512, 1024}; fc-dropout: 0.5; max-seq-length: 128;\nlearning-rate: 5e-5; cross-entropy loss, with Adam optimizer. BioMegatron models are pre-trained from\nscratch on PubMed, except 1.2b model which is ﬁne-tuned from a general domain model checkpoint.\nBenchmark Model #Parameters Vocabulary SAcc LAcc MRR\nQABioASQ-7b-factoid\nBioBERT-Base 110m BERT-cased 30.8 64.1 41.1\nBioBERT-Large 345m BERT-cased 42.8 62.8 50.1\nBioMegatron 345m BERT-uncased 46.2 62.6 52.5\nBioMegatron 800m BERT-uncased 45.2 58.6 50.4\nBioMegatron 1.2b BERT-uncased 47.4 60.9 52.4\nTable 3: Evaluation results on QA after ﬁne-tuning for 30 epochs on checkpoints ﬁne-tuned on SQuAD dataset\nwith ﬁxed hyper-parameter settings as num-fc-layers: 2; fc-hidden-size: 2048; fc-dropout: 0.1;\nmax-seq-length: 512; learning-rate: 3e-5; cross-entropy loss, using Adam optimizer. BioMegatron\nmodels are pre-trained from scratch on PubMed, except 1.2b model which is ﬁne-tuned from a general domain\nmodel checkpoint.\nFigure 1: Examples of tokenization with different sub-\nword vocabularies. Blue and purple text show word-\nlevel and subtoken-level entity labeling, respectively.\nbreak-out rate while being smaller in size than our\n50k-size vocabulary. A lower break-out rate with\nsmaller vocabulary size probably helps achieve bet-\nSub-word vocabulary BC5-chem BC5-disease\nBERT-cased 3.012 2.42\nPubMedBERT-uncased (30k)1.654 1.236\nBioMegatron-bio-30k-cased 1.753 1.272\nBioMegatron-bio-50k-cased 1.478 1.116\nTable 4: The rate of named entities breaking into subto-\nkens (#tokens/#words) in NER training sets.\nter NER performance despite smaller model size.\nWe can label the entities for NER training as:\n(1) marking the whole entity as a single label, and\n(2) labeling sub-tokens separately. Figure 1 shows\nexamples of the labeling methods. We ﬁnd these\ndifferent schemes can result in as much as ∼2%\ndifference in the F1-score on NER evaluation, pos-\nsibly indicating that the datasets are too small. We\nreport NER results by labeling sub-tokens sepa-\nrately, except for NCBI-disease dataset, which re-\nsults in better whole-entity labeling across models.\n5.2 Relation Extraction\nSince RE is a classiﬁcation task, albeit on se-\nquences rather than on tokens, the choice of sub-\nword vocabulary has a notable effect.\nWe can also observe that larger models result in\nhigher precision for lower recall, both for NER and\nRE. More hyper-parameter tuning could achieve\nhigher F1-scores, even the generalization ability of\nsuch result may be questionable.\n5.3 Question Answering\nTable 3 show evaluation results after ﬁne-tuning on\nSQuAD for 10 epochs and BioASQ for 30 epochs\neach, following the recipe found to work best by\nLee et al. (2019). We found large batch size to be\nbeneﬁcial, as Q&A pairs repeat up to 88 times. We\nuse batch size of 64 per GPU with data parallelism\non 16 GPUs. Using biomedical vocabularies re-\nsult in much worse results, possibly due to its low\nrelevance in the ﬁrst SQuAD ﬁne-tuning task.\nLarger models tend to perform better in QA,\nthough it levels off after 345m parameters. The\nlarger model size effect is more evident when ﬁne-\ntuning on BioASQ directly, as shown in Table 5.\nModel SAcc LAcc MRR\nBioMegatron-345m 33.1 50.4 39.8\nBioMegatron-800m 37.7 56.3 45.1\nBioMegatron-1.2b 40.6 53.7 45.6\nTable 5: Results on BioASQ-7b factoid, without ﬁne-\ntuning on SQuAD dataset ﬁrst. The other models, in-\ncluding those using domain vocabularies, could not\nachieve any comparable results. A consistent pattern\nof improvement over model size noticeable on par with\nﬁndings in general domain LM on SQuAD.\n5.4 Domain Transfer and Generalization\nWe examine how well a general- or domain- spe-\nciﬁc LM generalizes across domains related to the\nmodel size. Gu et al. (2020) studied the effect of\n“domain-speciﬁc” vs. “mixed-domain” pre-training,\ni.e., pre-training on PubMed from scratch vs. pre-\ntraining starting from a general domain LM (ﬁne-\ntuning). They found that pre-training on PubMed\nfrom scratch is better for biomedical NLP bench-\nmarks, but we analyze its effect with further pre-\ntraining (ﬁne-tuning) steps. In other words, if start-\ning from a general domain LM, does sufﬁcient ﬁne-\ntuning make it as good as a fully domain-speciﬁc\nmodel? Can such model have any advantage for\ncross-domain or cross-discipline generalization?\nBenchmark Fine-tuning steps F1\nNER\nBC5CDR-chem\n103 steps 63.2\n104 steps 74.3\n105 steps 89.7\n2·105 steps 89.37\n3·105 steps 91.8\n4· 105 steps 92.1\n5·105 steps 91.2\nBC5CDR-disease\n103 steps 39.4\n104 steps 63.6\n105 steps 79.8\n2·105 steps 81.2\n3·105 steps 79.2\n4· 105 steps 81.9\n5·105 steps 81.8\nREChemProt\n103 steps 0.00\n104 steps 34.1\n105 steps 63.4\n2· 105 steps 71.1\n3·105 steps 70.4\n4·105 steps 69.7\n5·105 steps 68.3\nTable 6: Comparison of ﬁne-tuning steps for NER\nand RE benchmark when pre-training general-domain\nMegatron-1.2b model on PubMed. Cross-domain LMs\nshould be trained sufﬁciently long on domain text to\nachieve comparable performance.\nTable 6 shows F1-score evaluation on NER\nand RE benchmarks using a general-domain\nBioMegatron-1.2b with additional ﬁne tuning. It\nshows that even for a large LM that was pre-trained\non a large text corpus, it needs sufﬁcient further pre-\ntraining on domain text (PubMed). After sufﬁcient\npre-training on domain text, it can be as good as\nan LM pre-trained on domain-text only, except that\nvocabulary has more signiﬁcant effect on NER.\nModel SAcc LAcc MRR\nMegatron-345m(general LM) 38.5 52.6 43.7\nMegatron-1.2b (general LM) 29.3 39.7 32.7\nTable 7: Fine-tuning and evaluating on BioASQ-7b us-\ning general domain LMs not trained on PubMed corpus.\nLarger model does not perform better.\nTable 7 shows the results of general-domain LMs\nﬁne-tuned on BioASQ-7b-factoid. Larger models\ndo not perform better, which may indicate overﬁt-\nting is occuring on the small training set.\nTable 8 shows the generalization ability of\nModel SQuAD-v1.1 SQuAD-v2.0\nBioMegatron-345m 90.4 84.2\nBioMegatron-345m-ft 86.5 77.9\nBioMegatron-800m 91.6 86.1\nBioMegatron-1.2b-ft 91.8 86.4\nBERTLARGE 90.9 81.8\nRoBERTa 94.6 89.4\nMegatron-3.9b 95.8 91.2\nTable 8: Fine-tuning on SQuAD -v1.1/-v2.0 using\nBioMegatron and evaluating on F1-score on dev-set.\nBioMegatron with ‘-ft’ are pre-trained from general do-\nmain checkpoints (ﬁne-tuned). Results of other gen-\neral domain LMs are compared: RoBERTa (Liu et al.,\n2019), Megatron-LM (Shoeybi et al., 2019).\nBioMegatron models on SQuAD datasets. Here, a\nlarge biomedical LM pre-trained on large text cor-\npus performs better than smaller general domain\nLMs such as BERTLARGE, even when pre-trained\non the biomedical text.\n5.5 Other Domain-Speciﬁc Factors\nSize and Bias in Biomedical Datasets Anno-\ntating biomedical data requires in-depth domain\nknowledge. Besides, data often have substantial la-\nbel bias as the occurrences of “abnormal” or “ﬁnd-\nings” are rare by nature. As a result, biomedical\nbenchmark data tend to be smaller and highly bi-\nased than their general domain counterparts.\nTask Dataset # Samples Bias %\nNER CONLL-2003 14987 0.18\nBC5CDR 5235 0.08\nCLS MRPC 3668 0.48\nChemProt 19461 0.27\nQA SQuAD-v1.0 87599 0.4\nBioASQ-7b 5537 0.02\nTable 9: Label bias in general and biomedical bench-\nmark dataset. CONLL-2003 (Sang and De Meulder,\n2003), MRPC (Dolan et al., 2005), and SQuAD (Ra-\njpurkar et al., 2016) are general domain dataset for\nNER, CLS (RE), and QA, respectively, for compar-\nison against biomedical domain dataset. Label bias\nis computed as [ sum of the #samples of minority\nlabels]/[#samples of majority label ], for NER and\nRE (CLS), and [ #minimum repeat of the same an-\nswer]/[#maximum repeat of the same answer] for QA.\nTable 9 shows a comparison of benchmark\ndatasets for NER, RE (CLS), and QA in the biomed-\nical domain and their general-domain counterparts.\nThe SQuAD Q&A set is 15 times larger than the\nBioASQ data, where the same question-answer\ncombinations appear up to 88 times in BioASQ.\nQuestion-answer pairs are seldom repeated in\nSQuAD data, at most twice. The BC5CDR NER\ndataset is 1/3 size of CONLL-2003 and the ratio of\nI/O to O tags 0.08, compared to 0.18 for CONLL.\nMethods to circumvent data imbalance issues\nsuch as oversampling the minority classes (Chawla\net al., 2002; Chen et al., 2010) and using weighted\ncross-entropy gave minor effects on our NER and\nRE benchmarks. Recently, Li et al. (2019) pro-\nposed dice-loss for data-imbalance issues in NLP,\nwith SOTA results on NER and QA, which could be\na future avenue to explore for domain LMs. Trans-\nfer learning showed effectiveness in the biomedical\nQA task. However, it is somewhat unclear how to\napply it to NER and RE tasks.\nModel PubMed Corpus #Words\nBioBERT abstracts 4.5 billion\nPubMedBERT abstracts + full-text 16.8 billion\nBioMegatron abstracts + full-text-CC 6.1 billion\nTable 10: Pre-training text corpus of each biomedical\nLM. We pre-train on PubMed abstracts and full-text\ncommercial-collection (CC) that are free of copyrights.\nPre-training Corpus and Duration PubMed-\nBERT is pre-trained on a much larger text corpus,\nas shown in Table 10. It is a performant domain-\nLM with a larger pre-training corpus and adequate\ndomain vocabulary compared to its model size. We\npre-train our LMs for about one epoch, reaching a\nmasked-LM loss of about 1.2 (Devlin et al., 2018).\nFurther pre-training may be helpful, but it is chal-\nlenging to have strictly controlled experiments with\nmany different settings.\n6 Conclusion\nWe review and test several factors that can affect\nthe performance of domain language models. We\nﬁnd that a language model targeted for a domain\nand application performs best. For example, model\nsize is a secondary factor to vocabulary set for\ntoken classiﬁcation task. Larger model size does\nnot necessarily translate to better performance on a\ncross-domain benchmark task.\nThis probably indicates that there is no master\nmodel that can “do it all”, at least well enough as a\ntargeted one. The model size is a secondary factor;\nlarger model size can probably further improve\nthe performance of a a domain- and application-\nspeciﬁc language model.\nAcknowledgement\nThe authors would like to thank Sun Kim at\nNIH/NCBI (now at Amazon Alexa AI) for helpful\ndiscussions and suggestions.\nReferences\nEmily Alsentzer, John R Murphy, Willie Boag, Wei-\nHung Weng, Di Jin, Tristan Naumann, and Matthew\nMcDermott. 2019. Publicly available clinical bert\nembeddings. arXiv preprint arXiv:1904.03323.\nIz Beltagy, Arman Cohan, and Kyle Lo. 2019. Scibert:\nPretrained contextualized embeddings for scientiﬁc\ntext. arXiv preprint arXiv:1903.10676.\nNitesh V Chawla, Kevin W Bowyer, Lawrence O Hall,\nand W Philip Kegelmeyer. 2002. Smote: synthetic\nminority over-sampling technique. Journal of artiﬁ-\ncial intelligence research, 16:321–357.\nSheng Chen, Haibo He, and Edwardo A Garcia.\n2010. Ramoboost: ranked minority oversampling in\nboosting. IEEE Transactions on Neural Networks ,\n21(10):1624–1642.\nDina Demner-Fushman, K Bretonnel Cohen, Sophia\nAnaniadou, and Jun’ichi Tsujii. 2019. Proceedings\nof the 18th bionlp workshop and shared task. InPro-\nceedings of the 18th BioNLP Workshop and Shared\nTask.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nRezarta Islamaj Do ˘gan, Robert Leaman, and Zhiyong\nLu. 2014. Ncbi disease corpus: a resource for dis-\nease name recognition and concept normalization.\nJournal of biomedical informatics, 47:1–10.\nBill Dolan, Chris Brockett, and Chris Quirk. 2005.\nMicrosoft research paraphrase corpus. Retrieved\nMarch, 29(2008):63.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas,\nNaoto Usuyama, Xiaodong Liu, Tristan Naumann,\nJianfeng Gao, and Hoifung Poon. 2020. Domain-\nspeciﬁc language model pretraining for biomedi-\ncal natural language processing. arXiv preprint\narXiv:2007.15779.\nKexin Huang, Jaan Altosaar, and Rajesh Ranganath.\n2019. Clinicalbert: Modeling clinical notes and\npredicting hospital readmission. arXiv preprint\narXiv:1904.05342.\nAlistair EW Johnson, Tom J Pollard, Lu Shen,\nH Lehman Li-wei, Mengling Feng, Moham-\nmad Ghassemi, Benjamin Moody, Peter Szolovits,\nLeo Anthony Celi, and Roger G Mark. 2016. Mimic-\niii, a freely accessible critical care database. Scien-\ntiﬁc data, 3:160035.\nMartin Krallinger, Obdulia Rabal, Florian Leitner,\nMiguel Vazquez, David Salgado, Zhiyong Lu,\nRobert Leaman, Yanan Lu, Donghong Ji, Daniel M\nLowe, et al. 2015. The chemdner corpus of chemi-\ncals and drugs and its annotation principles. Journal\nof cheminformatics, 7(1):1–17.\nJ Lee, W Yoon, S Kim, D Kim, CH So, and J Kang.\n2019. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics (Oxford, England).\nJiao Li, Yueping Sun, Robin J Johnson, Daniela Sci-\naky, Chih-Hsuan Wei, Robert Leaman, Allan Peter\nDavis, Carolyn J Mattingly, Thomas C Wiegers, and\nZhiyong Lu. 2016. Biocreative v cdr task corpus:\na resource for chemical disease relation extraction.\nDatabase, 2016.\nXiaoya Li, Xiaofei Sun, Yuxian Meng, Junjun\nLiang, Fei Wu, and Jiwei Li. 2019. Dice loss\nfor data-imbalanced nlp tasks. arXiv preprint\narXiv:1911.02855.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nYifan Peng, Shankai Yan, and Zhiyong Lu. 2019.\nTransfer learning in biomedical natural language\nprocessing: An evaluation of bert and elmo\non ten benchmarking datasets. arXiv preprint\narXiv:1906.05474.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Better lan-\nguage models and their implications.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250.\nLance A Ramshaw and Mitchell P Marcus. 1999. Text\nchunking using transformation-based learning. In\nNatural language processing using very large cor-\npora, pages 157–176. Springer.\nErik F Sang and Fien De Meulder. 2003. Intro-\nduction to the conll-2003 shared task: Language-\nindependent named entity recognition. Proceedings\nof CoNLL-2003.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-lm: Training multi-billion\nparameter language models using gpu model paral-\nlelism. arXiv preprint arXiv:1909.08053.\nTrieu H. Trinh and Quoc V . Le. 2018. A sim-\nple method for commonsense reasoning. CoRR,\nabs/1806.02847.\nGeorge Tsatsaronis, Georgios Balikas, Prodromos\nMalakasiotis, Ioannis Partalas, Matthias Zschunke,\nMichael R Alvers, Dirk Weissenborn, Anastasia\nKrithara, Sergios Petridis, Dimitris Polychronopou-\nlos, et al. 2015. An overview of the bioasq\nlarge-scale biomedical semantic indexing and ques-\ntion answering competition. BMC bioinformatics ,\n16(1):138.\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2019. Defending against neural fake\nnews. CoRR, abs/1905.12616.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8416992425918579
    },
    {
      "name": "Language model",
      "score": 0.7899022102355957
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.7549712657928467
    },
    {
      "name": "Natural language processing",
      "score": 0.7213043570518494
    },
    {
      "name": "Vocabulary",
      "score": 0.666621208190918
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6131067276000977
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5278917551040649
    },
    {
      "name": "Code (set theory)",
      "score": 0.5215144753456116
    },
    {
      "name": "Word (group theory)",
      "score": 0.48252221941947937
    },
    {
      "name": "Relationship extraction",
      "score": 0.4751403331756592
    },
    {
      "name": "Information extraction",
      "score": 0.2701367735862732
    },
    {
      "name": "Programming language",
      "score": 0.15560024976730347
    },
    {
      "name": "Linguistics",
      "score": 0.14843541383743286
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210127875",
      "name": "Nvidia (United States)",
      "country": "US"
    }
  ],
  "cited_by": 10
}