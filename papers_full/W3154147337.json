{
  "title": "AlephBERT:A Hebrew Large Pre-Trained Language Model to Start-off your Hebrew NLP Application With",
  "url": "https://openalex.org/W3154147337",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4288896846",
      "name": "Seker, Amit",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226723938",
      "name": "Bandel, Elron",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288896844",
      "name": "Bareket, Dan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223030615",
      "name": "Brusilovsky, Idan",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Greenfeld, Refael Shaked",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222740992",
      "name": "Tsarfaty, Reut",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3037575273",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2988304195",
    "https://openalex.org/W3035110948",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2880029892"
  ],
  "abstract": "Large Pre-trained Language Models (PLMs) have become ubiquitous in the development of language understanding technology and lie at the heart of many artificial intelligence advances. While advances reported for English using PLMs are unprecedented, reported advances using PLMs in Hebrew are few and far between. The problem is twofold. First, Hebrew resources available for training NLP models are not at the same order of magnitude as their English counterparts. Second, there are no accepted tasks and benchmarks to evaluate the progress of Hebrew PLMs on. In this work we aim to remedy both aspects. First, we present AlephBERT, a large pre-trained language model for Modern Hebrew, which is trained on larger vocabulary and a larger dataset than any Hebrew PLM before. Second, using AlephBERT we present new state-of-the-art results on multiple Hebrew tasks and benchmarks, including: Segmentation, Part-of-Speech Tagging, full Morphological Tagging, Named-Entity Recognition and Sentiment Analysis. We make our AlephBERT model publicly available, providing a single point of entry for the development of Hebrew NLP applications.",
  "full_text": "AlephBERT: A Hebrew Large Pre-Trained Language Model\nto Start-off your Hebrew NLP Application With\nAmit Seker, Elron Bandel, Dan Bareket, Idan Brusilovsky, Refael Shaked Greenfeld, Reut Tsarfaty\nBar-Ilan University, Computer Science Department, Ramat-Gan, Israel\n{aseker00,elronbandel,dbareket,shakedgreenfeld,brusli1,reut.tsarfaty}@gmail.com\nAbstract\nLarge Pre-trained Language Models (PLMs)\nhave become ubiquitous in the development\nof language understanding technology and lie\nat the heart of many artiﬁcial intelligence ad-\nvances. While advances reported for English\nusing PLMs are unprecedented, reported ad-\nvances using PLMs in Hebrew are few and\nfar between. The problem is twofold. First,\nHebrew resources available for training NLP\nmodels are not at the same order of magnitude\nas their English counterparts. Second, there\nare no accepted tasks and benchmarks to eval-\nuate the progress of Hebrew PLMs on. In this\nwork we aim to remedy both aspects. First,\nwe present AlephBERT, a large pre-trained\nlanguage model for Modern Hebrew, which\nis trained on larger vocabulary and a larger\ndataset than any Hebrew PLM before. Sec-\nond, using AlephBERT we present new state-\nof-the-art results on multiple Hebrew tasks and\nbenchmarks, including: Segmentation, Part-\nof-Speech Tagging, full Morphological Tag-\nging, Named-Entity Recognition and Senti-\nment Analysis. We make our AlephBERT\nmodel publicly available, providing a single\npoint of entry for the development of Hebrew\nNLP applications.\n1 Introduction\nContextualized word representations, provided by\nmodels such as BERT (Devlin et al., 2019) and\nRoBERTa (Liu et al., 2019), were shown in recent\nyears to be critical for obtaining state-of-the-art\nperformance on a wide range of Natural Language\nProcessing (NLP) tasks — such as syntactic and\nsemantic parsing, question answering, natural lan-\nguage inference, text summarization, natural lan-\nguage generation, and more. These contextualized\nword representations are obtained by pre-training\na large language model on massive quantities of\nunlabeled data, aiming to maximize a simple yet\neffective objective of masked word prediction.\nWhile advances reported for English using such\nmodels are unprecedented, in Hebrew previously\nreported results using BERT-based models are far\nfrom impressive. Speciﬁcally, the BERT-based He-\nbrew section of multilingual-BERT (Devlin et al.,\n2019) (henceforth, mBERT), did not provide a sim-\nilar boost in performance to what is observed for\nthe English section of mBERT. In fact, for several\nreported tasks, the mBERT model results are on a\npar with pre-neural models, or neural models based\non non-contextialized embeddings (Tsarfaty et al.,\n2020; Klein and Tsarfaty, 2020). An additional He-\nbrew BERT-based model, HeBERT (Chriqui and\nYahav, 2021), has been released, yet there is no\nreported evidence on performance improvements\non key component of the Hebrew NLP pipeline —\nwhich includes, at the very least: morphological\nsegmentation, full morphological tagging, and full\n(token/morpheme-based) named entity recognition.\nIn this work we present AlephBERT, a Hebrew\npre-trained language model, larger and more effec-\ntive than any Hebrew PLM before. Using Aleph-\nBERT we show substantial improvements on all\nessential tasks in the Hebrew NLP pipeline, tasks\ntailored to ﬁt a morphologically-rich language,\nincluding: Segmentation, Part-of-Speech Tag-\nging, full morphological tagging, Named Entity\nRecognition and Sentiment Analysis. Since pre-\nvious Hebrew NLP studies used varied corpora\nand annotation schemes, we conﬁrm our results on\nall existing Hebrew benchmarks and variants. For\nmorphology and POS tagging, we test on both the\nHebrew section of the SPMRL shared task (Seddah\net al., 2013), and the Hebrew UD corpus (Sadde\net al., 2018). For Named Entity recognition, we test\non both the corpus of Ben Mordecai and Elhadad\n(2005) and that of Bareket and Tsarfaty (2020). For\nsentiment analysis we test on the facebook corpus\nof Amram et al. (2018), as well as a newer (ﬁxed)\nvariant of this benchmark.\narXiv:2104.04052v1  [cs.CL]  8 Apr 2021\nWe make our pre-trained model publicly avail-\nable1 and additionally we deliver an online\ndemo2 allowing to qualitatively compare the mask-\nprediction capacity of different PLMs available for\nHebrew. In the near future we will release the com-\nplete AlephBERT-geared pipeline we developed,\ncontaining the aforementioned tasks, as means for\nevaluating and comparing future Hebrew PLMs,\nand as a starting point for developing further down-\nstream applications and tasks. We also plan to\nshowcase AlephBERT’s capacities on downstream\nlanguage understanding tasks such as: Information\nExtraction, Text Summarization, Reading Compre-\nhension, and more. As future research, we are\npursuing a plan to investigate the effect of different\nword decomposition algorithms and input represen-\ntation variants on the different tasks in the Pipeline.\n2 The Challenge\nThis paper presents a case study for PLM develop-\nment for a morphologically-rich and resource-poor\nlanguage. Speciﬁcally, we address Modern Hebrew,\na Semitic, morphologically-rich language, that is\nlong known to be notoriously hard to parse.\nThe challenges posed to automatically process-\ning Hebrew texts and obtaining good accuracies on\ndownstream tasks stem from (at least) two main\nfactors. The ﬁrst is the internal-complexity of word-\ntokens, resulting from the rich morphology, com-\nplex orthography, and lack of diacritization in He-\nbrew written texts. Space-delimited tokens have\nnon-transparent decomposition and are highly am-\nbiguous, making even the simplest of the tasks in\nthe pipeline very challenging (Tsarfaty et al., 2019).\nThe second factor is the fact that Modern Hebrew,\nwith only a few dozens of millions of native speak-\ners, is often studied in resource-scarce settings.\nThe resource-scarce setting is problematic for\nPLM development in at least two ways. First, there\nare insufﬁcient amounts of free unlabeled text for\npre-training. To wit, the Hebrew Wikipedia that\nwas the source for training multilingual BERT is\nof orders of magnitude smaller than the English\nWikipedia (See Table 1 ).3 Secondly, there are no\nlarge-scale open-access commonly accepted bench-\nmarks for ﬁne-tuning and/or evaluating the perfor-\nmance of Hebrew PLMs on NLP/NLU downstream\ntasks.\n1huggingface.co/onlplab/alephbert-base\n2nlp.biu.ac.il/˜elronbandel/alephbert/\n3Of course, ample Hebrew data does exist online, but most\nof it is closed due to copy-right issues and paywalls.\nLanguage Oscar Size Wikipedia Articles\nEnglish 2.3T 6,282,774\nRussian 1.2T 1,713,164\nChinese 508G 1,188,715\nFrench 282G 2,316,002\nArabic 82G 1,109,879\nHebrew 20G 292,201\nTable 1: Corpora Size Comparison: High-resource\n(and Medium-resourced) languages vs. Hebrew.\nCorpus File Size Sentences Words\nOscar (deduped) 9.8GB 20.9M 1,043M\nTwitter 6.9GB 71.5M 774M\nWikipedia 1.1GB 6.3M 127M\nTotal 17.9GB 98.7M 1.9B\nTable 2: Data Statistics for AlephBERT’s training sets.\nPrevious studies on various tasks on Hebrew data\ndo exist, each relying on disparate data sources,\nwith varied evaluation metrics and annotation\nschemes even for the same task. To investigate\nHebrew PLMs and probe their ability to capture\nlinguistic structure, we introduce and evaluate He-\nbrew PLMs on the full set of tasks, sentence-based ,\ntoken-based and morpheme-based tasks, including\nspeciﬁc task variants and evaluation metrics.\n3 The Model\nData. The PLM nicknamed AlephBERT is\ntrained on a larger dataset and a larger vocabulary\nthan any Hebrew BERT instantiation before. Data\nstatistics are provided in Table 2. Speciﬁcally, we\nemploy the following datasets for pre-training:\n• Oscar: A deduplicated Hebrew portion of\nthe OSCAR corpus, which is “extracted from\nCommon Crawl via language classiﬁcation,\nﬁltering and cleaning” (Ortiz Su ´arez et al.,\n2020).\n• Twitter: Texts of Hebrew tweets collected\nbetween 2014-09-28 and 2018-03-07. We\nslightly cleaned up the texts by removing\nretweet signals “ RT:”, user mentions (e.g.\n“@username”), and URLs.\n• Wikipedia: The texts in all of Hebrew\nWikipedia,4 extracted using Attardi (2015)\nThis corpus is available on our github.5\n4Dump: hewiki-20200201-pages-articles.xml.bz2\n5https://github.com/OnlpLab/AlephBERT/\nblob/main/data/wikipedia/\nOne of the most important factors driving the suc-\ncess of PLMs in other languages is the availability\nof enormous amounts of text to learn from. The\nHebrew portions of Oscar and Wikipedia provides\nus with a training set size which is an order of\nmagnitude smaller compared with resource-savvy\nlanguages, as shown in Table 1. In order to build\na strong PLM we need a considerable boost in the\namount of text that the PLM can learn from, which\nin our case comes form massive amounts of tweets\nadded to the training set. The textual utterances pro-\nvided by the Twitter sample API tend be short and\ndiverge from valid syntax and canonical language\nuse for the most part. And while the free form\nlanguage expressed in tweets might differ signiﬁ-\ncantly from the text found in Oscar and Wikipedia,\nthe sheer volume of tweets helps us close the re-\nsource gap substantially. Combining all resources\ntogether we have tweets comprising the lion’s share\nof sentences in our dataset (72%).\nTraining We used the Transformers training\nframework of Huggingface (Wolf et al., 2020) and\ntrained two different models — a small model with\n6 hidden layers learned from the Oscar portion of\nour dataset, and a base model with 12 hidden layers\nwhich was trained on the entire dataset. The pro-\ncessing units used in both the small and base Aleph-\nBERT models are wordpieces generated by training\nBERT tokenizers over the respective datasets with\na vocabulary size of 52K in both cases.\nTraditionally, BERT models are optimized with\nan objective function optimized using both masked\ntoken prediction as well as next sentence predic-\ntion losses. Following the work on RoBERTa (Liu\net al., 2019) we employ masked-token prediction\nloss only in our training objective. Incidentally our\nchoice of dataset also forces us to ignore next sen-\ntence prediction because a large portion of our data\ncomprises of tweets which are unrelated and inde-\npendent of each other (we did not attempt to recon-\nstruct the discourse threads of retweets and replies).\nFor more training details see the Appendix.\n4 Experiments\nGoal We set out to pre-train Hebrew PLMs and\nevaluate them empirically on a range of Hebrew\nNLP tasks. We evaluated the two AlephBERT vari-\nants (small and base) on the different tasks, in or-\nder to empirically gauge the effect of model size\nand data size on the quality of the language model.\nIn addition, we compared the performance of our\nmodels to existing Hebrew BERT-based instantia-\ntions (mBERT (Devlin et al., 2019) and HeBERT\n(Chriqui and Yahav, 2021)). We evaluated the\nPLMs on all key tasks of the Hebrew NLP pipeline.\nBenchmarks We evaluate our BERT-based mod-\nels on various Hebrew NLP tasks using the follow-\ning benchmarks:\n• Word Segmentation, Part-of-Speech Tag-\nging, Full Morphological Tagging:\n– The Hebrew Section of the SPMRL Task\n(Seddah et al., 2013)\n– The Hebrew Section of the UD 6 tree-\nbanks collection (Sadde et al., 2018)\n• Named Entity Recognition:\n– Token-based NER evaluation based on\nthe corpus of Ben-Mordecai and Elhadad\n(Ben Mordecai and Elhadad, 2005)\n– Token-based and Morpheme-based NER\nevaluation based on the Named Entities\nand MOrphology (henceforth NEMO)\ncorpus (Bareket and Tsarfaty, 2020)\n• Sentiment Analysis:\n– Sentiment Analysis evaluation based on\nthe corpus of Amram et al. (2018).\n– Since the aforementioned corpus is re-\nported to be leaking (shared material\nbetween test and train), we provide a\ncleaned up version and evaluate on the\nupdated split.\n5 Tasks and Modeling Strategies\nA key question when assessing BERT-based PLM\nperformance for Hebrew concerns how to develop\nmodels for the different levels of granularity. Here\nwe brieﬂy sketch our modeling strategies, starting\nwith the easiest (classiﬁcation) tasks and continuing\nto the more challenging setups, involving the use\nof PLMs to predict the tokens’ internal structures.\n5.1 Sentence-Based Modeling\nSentiment Analysis The ﬁrst task we report on\nis a simple sentence classiﬁcation task, classifying\nthe sentiment of a given sentence to one of three\nvalues: negative, positive, neutral. We trained and\nevaluated BERT-based sentence classiﬁcation on\n6https://universaldependencies.org\ntwo variants of the Hebrew Sentiment dataset of\nAmram et al. (2018).\nThe ﬁrst variant is the original sentiment dataset\nof Amram et al. (2018) with an additional split to\ncreate a dev set (the original paper had only train\nand test split, and the test set remains the same).\nThe dev set contains 10% of the train data which\nleaves us with a split of 70-10-20.\nUnfortunately, the original dataset of Amram et\nal. had a signiﬁcant data leakage between the splits,\nwith duplicates in the data samples. After removing\nthe duplicates out of the original 12,804 sentences,\nwe are left with a dataset of size 8,465.7\nWe ﬁne-tuned all the models for 15 epochs with\nthe default Huggingface (Wolf et al., 2020) param-\neters on 5 different seeds. We report per-comment\naccuracy, and take the mean of these 5 runs.\n5.2 Token-Based Modeling\nNamed Entity Recognition For the NER task,\nwe initially assume a token-based sequence label-\ning model. The input comprises of the sequence\nof tokens in the sentence, and the output contains\nBIOES tags indicating entity spans. The token-\nbased model is a simple ﬁne-tuned model using the\nTransformer’s token-classiﬁcation script of Wolf\net al. (2020).\nWe evaluate this model on two corpora. The ﬁrst\nis the corpus by Ben Mordecai and Elhadad (2005),\nhenceforth, the BMC corpus. The BMC corpus\nannotates entities at Token-level. This means that a\nHebrew token containing both a preposition and an\nentity mention will not deliver the entity-mention\nboundaries. The BMC contains 3294 sentences and\n4600 entities, and has seven different entity cate-\ngories (DATE, LOC, MONEY , ORG, PER, PER-\nCENT, TIME). To remain compatible with the orig-\ninal work we train and test the models on the 3 dif-\nferent splits as in Bareket and Tsarfaty (2020).8 For\nthe BMC corpus we report token-based F1 scores\non the detected entity mentions.\nThe second corpus is an extension of the SPMRL\ndataset with Named Entities annotation, also\nmarked by BIOSE tags, respecting the precise\n(token-internal) morphological boundaries of NEs\n(henceforth, NEMO, standing for Named Entities\nand MOrphology) (Bareket and Tsarfaty, 2020).\nThis corpus provides both a token-based and a\n7https://github.com/OnlpLab/\nHebrew-Sentiment-Data\n8https://github.com/OnlpLab/\nHebrewResources/tree/master/BMCNER\nmorpheme-based annotation of the entities, where\nthe latter contains the accurate (token-internal) en-\ntity boundaries. The NEMO corpus has nine cate-\ngories (ANG, DUC, EVE, FAC, GPE, LOC, ORG,\nPER, WOA). It contains 6220 sentences and 7713\nentities, and we used the standard SPMRL Train-\nDev-Test, as in Bareket and Tsarfaty (2020)\nThe models were trained over 15 epochs and no\nhyper parameter tuning. For the BMC we used 3\ndifferent seeds for each split set, leading to overall\nnine different training rounds, and for the NEMO\nset we used the average mean of ﬁve different seeds.\nFor both benchmarks we report token-based F1\nscores on the detected entity mentions.\n5.3 Morpheme-Based Modeling\nModern Hebrew is a Semitic language with rich\nmorphology and complex orthography. As a re-\nsult, the basic processing units in the language are\ntypically smaller than a given token’s span. To\nprobe AlephBERT’s capacity to accurately predict\nsuch token-internal linguistic structure, we test our\nmodels on four tasks that require knowledge of the\ninternal morphology of the raw tokens:\n• Segmentation\nInput: A Hebrew sentence containing raw\nspace-delimited tokens\nOutput: A sequence of morphological seg-\nments representing basic processing units.9\n• Part-of-Speech Tagging\nInput: A Hebrew sentence containing raw\nspace-delimited tokens\nOutput: Segmentation of the tokens to basic\nprocessing units as above, where each seg-\nment is tagged with its single disambiguated\npart-of-speech tag.\n• Morphological Tagging\nInput: A Hebrew sentence containing raw\nspace-delimited tokens\nOutput: Segmentation of the tokens to basic\nprocessing units as above, where each seg-\nment is tagged with a single POS tag and a set\nof morphological features.10\n9These units comply with the 2-level representation of\ntokens deﬁned by UD, where each basic unit corresponds to a\nsingle POS tag. https://universaldependencies.\norg/u/overview/tokenization.html\n10Equivalent to the AllTags evaluation metric\ndeﬁned in the CoNLL18 shared task. https:\n//universaldependencies.org/conll18/\nresults-alltags.html\n• Morpheme-Based NER\nInput: A Hebrew sentence containing raw\nspace-delimited tokens\nOutput: Segmentation of the tokens to basic\nprocessing as above where segment is tagged\nwith a BIOSE tags indicating entity spans,\nalong with the entity-type label.\nAn illustration of these tasks is given in Table 3.\nAs opposed to ﬁne-tuning the PLM model pa-\nrameters, as done in sentence-based and token-\nbased classiﬁcation tasks, segmented morphemes\nare not readily available in the BERT represen-\ntation. In order to provide proper segmentation\nand labeling for the four aforementioned tasks we\ndeveloped a model designated to produce the mor-\nphological segments of each token in context.\nThe morphological segmentation model which\nwe designed is composed of a PLM responsible\nfor transforming input tokens into contextualized\nembedded vectors, which we then feed into a char-\nbased seq2seq module that extracts the output seg-\nments. The seq2seq module is composed of an\nencoder implemented as a simple char-based BiL-\nSTM, and a decoder implemented as a char-based\nLSTM generating the output character symbols, or\na space symbol signalling the end of a morpholog-\nical segment. We train the model for 15 epochs,\noptimizing next-character prediction loss function.\nFor the other tasks, involving both segmenta-\ntion and labeling we deploy an MTL (multi-task\nlearning) setup. That is, when generating an end-\nof-segment symbol, the model then predicts task\nlabels which can be one or more of the following:\nPOS-tag, NER-tag, morphological features. In or-\nder to guide the training to learn we optimize the\ncombined segmentation and label prediction loss\nvalues. Currently we simply add together the loss\nvalues, but we note that as a future improvement it\nis likely that assigning different weights to the dif-\nferent loss values could prove to be beneﬁcial. All\nof these morphological labeling models are trained\nfor 15 epochs and evaluated on both the UD (Sadde\net al., 2018) and SPMRL data (Seddah et al., 2013).\nIn addition, we design another setup for run-\nning the various morphological labeling tasks in\nwhich we ﬁrst segment the text (using the above-\nmentioned segmentation model) and then perform\nﬁne-tuning with a token classiﬁcation attention\nhead directly applied to the PLM (similar to the\nway we ﬁne-tune the PLM for the token-based\nNER task described in the previous section). In\nthis pipeline setup we utilize the PLM twice; as\npart of the segmentation model to generate seg-\nments, which we then feed directly into the PLM\n(augmented with a token classiﬁcation head) which\nis ﬁne-tuned for the speciﬁc labeling task. We\nacknowledge the fact that we are ﬁne-tuning the\nPLM using morphological segments even though it\nwas originally pre-trained without any knowledge\nof sub-token units. But, as we shall see shortly,\nthis seemingly unintuitive strategy performs sur-\nprisingly well.\n6 Results\nSentence-Based Tasks The Sentiment analysis\nexperimental results are provided in Table 5. As\ncan be seen, all BERT-based models substan-\ntially outperform the original CNN Baseline re-\nported by Amram et al. (2018). Interestingly, both\nAlephBERT-small and AlephBERT-base outper-\nform all BERT-based variants, with BERT-base set-\nting new SOTA results on the new (ﬁxed) dataset.\nToken-Based Tasks For our two NER bench-\nmarks, we report the NER F1 scores on the token-\nbased ﬁne-tuned model in Table 4.\nHere, although we see noticeable improvements\nfor the mBERT and HeBert variants over the cur-\nrent SOTA, the most signiﬁcant increase is in the\nAlephBERT-base model. We also see a substan-\ntial difference between the AlephBERT-small and\nAlephBERT-base models, with the latter provid-\ning a new SOTA results on these both data sets.\nCrucially, this holds for the token-based evaluation\nmetrics (as deﬁned in Bareket and Tsarfaty (2020)).\nMorpheme-Based Tasks As a particular novelty\nof this work, we report BERT-based results on sub-\ntoken (segment-level) information. Speciﬁcally, we\nevaluate segmentation F1, POS F1, Morphological\nFeatures F1 and morphem-base NER F1, compared\nagainst the disambiguated labeled segments. In all\ncases we use raw space-delimited tokens as input,\nletting the BERT-based models perform both the\nsegmentation and labeling.\nTable 6 presents the segmentation, POS tags,\nand morphological tags F1 for the SPMRL dataset,\nall evaluated at the granularity of morphological\nsegments. We report the aligned multiset F1 Scores\nas in previous work on Hebrew (More et al., 2019).\nWe see that segmentation results for all BERT-\nbased models are similar, and they are already at\nRaw inputלביתהלב \nSpace-delimited tokensהלבלבית\nSegmentationלבהביתהל\nPOS ADJ DET NOUN DET ADP\nMorphology Gender=Masc|Number=Sing PronType=Art Gender=Masc|Number=Sing PronType=Art -\nToken-level NER E-ORG B-ORG\nMorpheme-level NER E-ORG I-ORG I-ORG B-ORG O\nTable 3: Illustration of Evaluated Token and Morpheme-Based Downstream Tasks. The input is the two-word\ninput phrase “לביתהלב (” to the White House). Sequence and Hebrew text goes from right to left.\nNEMO BMC\nPrevious SOTA 77.75 85.22\nmBERT 79.07 87.77\nHeBERT 81.48 89.41\nAlephBERT-small 78.69 89.07\nAlephBERT-base 84.91 91.12\nTable 4: Token-Based NER Results on the NEMO and\nthe Ben-Mordecai Corpora. Previous SOTA on both\ncorpora has been reported by the NEMO models of\nBareket and Tsarfaty (2020).\nthe higher range of 97-98 F1 scores, which are\nhard to improve further.11 For POS tagging and\nmorphological features, all BERT-based models\nsigniﬁcantly outperform the previous SOTA pro-\nvided by (Seker and Tsarfaty, 2020) (referred to\nas PtrNet) for POS tags and (More et al., 2019)\n(referred to as YAP) for morphological features.\nWith respect to all BERT-based variants, we see an\nimprovement for AlephBERT on all other alterna-\ntives, but on a small scale. That said, we do notice a\nrepeating trend that places AlephBERT-base as the\nbest model for all of our morphological tasks, indi-\ncating that the improvement provided by the depth\nof the model and a larger dataset does also improve\nthe ability to capture token-internal structure.\nThese trends are replicated on the UD Hebrew\ncorpus, for two different evaluation metrics — the\nAligned MultiSet F1 Scores as in previous work on\nHebrew (More et al., 2019), (Seker and Tsarfaty,\n2020), and the Aligned F1 scores metrics in the UD\nshared task (Zeman et al., 2018) — as reported in\nTables 7 and 8 respectively. AlephBERT obtains\nthe best results for all tasks, even if not by a large\nmargin.\nMorpheme-Based NER Earlier in this section\nwe considered NER as a token-based task that sim-\n11Some of these errors are due to annotation errors, or truly\nambiguous cases.\nply requires ﬁne-tuning on the token labels. How-\never, this setup is not accurate enough and less\nuseful for downstream tasks, since the exact en-\ntity boundaries are often token internal (Bareket\nand Tsarfaty, 2020). We hence also report here\nmorpheme-based NER evaluation, respecting the\nexact boundaries of the Entity mentions. To ob-\ntain morpheme-based labeled-span of Named En-\ntities as discussed above we could either employ\na pipeline, ﬁrst predicting segmentation and then\napplying a ﬁne tuned labeling model directly on\nthe segments, or we can use the MTL model and\npredict NER labels while performing the segmenta-\ntion.\nTable 9 presents segmentation and NER results\nfor three different scenarios: (i) a pipeline assum-\ning gold segmentation (ii) a pipeline assuming the\nbest predicted segmentation (as predicted above)\n(iii) obtaining the segmentation and NER labels\njointly in the MTL setup.\nAs our results indicate, AlephBERT-base con-\nsistently scores highest in both pipeline (oracle\nand predicted) and multi-task setups. Looking\nat the Pipeline-Predicted scores, there is a clear\ncorrelation between a higher segmentation qual-\nity of a PLM and its ability to produce better\nNER results. Moreover, the differences in NER\nscores between the models are considerable (un-\nlike the subtle differences in segmentation, POS\nand morphological features scores) and draw our\nattention to the relationship between the size of\nthe PLM, the size of the pre-training data and\nthe quality of the ﬁnal NER models. Speciﬁcally,\nHeBERT and AlephBERT-small were pre-trained\nwith similar datasets - HeBERT with Oscar and\nWikipedia, AlephBERT-small with Oscar only (the\nWikipedia portion is order of magnitude smaller\ncompared with Oscar) and comparable vocabulary\nsizes (heBERT with 30K and AlephBERT-small\nwith 52K). However we notice that HeBERT, with\nits 12 hidden layers, performs signiﬁcantly better\nOld(leak) token Old(leak) morph New(ﬁxed) token New(ﬁxed) morph\nPrevious SOTA 89.2 87.5 NA NA\nmBERT 92.12 92.18 84.21 85.58\nHeBERT 92.48 92.27 87.13 86.88\nAlephBERT-small 93.15 92.70 88.3 87.38\nAlephBERT-base 91.63 92.01 89.02 88.71\nTable 5: Sentiment Analysis Scores on the Facebook Corpus. Previous SOTA is reported by Amram et al. (2018).\nSegmentation F1 POS F1 Morphological Features F1\nPrevious SOTA NA 90.49 85.98\nmBERT-morph 97.36 93.37 89.36\nHeBERT-morph 97.97 94.61 90.93\nAlephBERT-small-morph 97.71 94.11 90.56\nAlephBERT-base-morph 98.10 94.90 91.41\nTable 6: Morpheme-Based Aligned MultiSet (mset) Results on the SPMRL Corpus. Previous SOTA is as reported\nby (Seker and Tsarfaty, 2020) (POS) and (More et al., 2019) (morphological features)\nSegmentation F1 POS F1 Morphological Features F1\nPrevious SOTA NA 94.02 NA\nmBERT-morph 97.70 94.76 90.98\nHeBERT-morph 98.05 96.07 92.53\nAlephBERT-small-morph 97.86 95.58 92.06\nAlephBERT-base-morph 98.20 96.20 93.05\nTable 7: Morpheme-Based Aligned MultiSet (mset) Results on the UD Corpus. Previous SOTA is as reprted by\n(Seker and Tsarfaty, 2020) (POS)\nSegmentation F1 POS F1 Morphological Features F1\nPrevious SOTA 96.03 93.75 91.24\nmBERT-morph 97.17 94.27 90.51\nHeBERT-morph 97.54 95.60 92.15\nAlephBERT-small-morph 97.31 95.13 91.65\nAlephBERT-base-morph 97.70 95.84 92.71\nTable 8: Morpheme-Based Aligned (CoNLL shared task) Results on the UD Corpus. Previous SOTA is as reported\nby Minh Van Nguyen and Nguyen (2021)\nArchitecture Pipeline Pipeline MultiTask\nSegmentation (Oracle) (Predicted)\nScores (aligned mset F1) Seg NER Seg NER Seg NER\nPrevious SOTA (NEMO) 100.00 79.10 95.15 69.52 97.05 77.11\nmBERT 100.00 77.92 97.68 72.72 97.24 72.97\nHeBERT 100.00 82 98.15 76.74 97.92 74.86\nAlephBERT-small 100.00 79.44 97.78 73.08 97.74 72.46\nAlephBERT-base 100.00 83.94 98.29 80.15 98.19 79.15\nTable 9: Morpheme-Based NER Evaluation on the NEMO Corpus. Previous SOTA is as reported by Bareket and\nTsarfaty (2020) for the Pipeline (Oracle), Pipeline (Predicted) and a Hybrid (almost-joint) Scenarios, respectively.\ncompared to AlephBERT-small which is composed\nof only 6 hidden layers. It thus appears that seman-\ntic information is learned in those deeper layers\nwhich helps in both learning to discriminate entities\nand improve the overall morphological segmenta-\ntion capacity.\nIn addition, comparing HeBERT to AlephBERT-\nbase we point to the fact that they are both modeled\nwith the same 12 hidden layer architecture, the\nonly differences between them are in the size of\ntheir vocabularies (30K vs 52K respectively) and\nthe size of the training data (Oscar-Wikipedia vs\nOscar-Wikipedia-Tweets). The improvements ex-\nhibited by AlephBERT-base, compared to HeBERT,\nsuggests that it is a result of the large amounts of\ntraining data and larger vocabulary available in\nour setup. By exposing AlephBERT-base to an\namount of text which order of magnitude larger we\nincreased its NER capacity.\nFinally, our NER experiments suggest that\na pipeline composed of our near-to-perfect\nmorphological segmentation model followed by\nAlephBERT-base augmented with a token classi-\nﬁcation head is the best strategy for generating\nmorphologically-aware NER labels.\n7 Qualitative Assessment\nTo allow for qualitative assessment of the PLMs,\nwe deliver an online demo where one can compare\nthe masked-word predictioncapacities of the differ-\nent models, and get the impression of the strengths\nand weaknesses. Our demo, available at https://\nnlp.biu.ac.il/˜elronbandel/alephbert/, of-\nfers friendly graphical interface that allows one\nto mask an item in a running Hebrew text and ob-\ntain the top-N list of alternatives predicted by each\nof the models. The demo allows to explore the\npredictions of our models both at token level and\nat sub-token level, masking individual word-pieces.\nNote that the AlephBERT family of models is still\nunder development, and we will add new model\nvariants as we proceed. Stay tuned!\n8 Conclusion\nModern Hebrew, a morphologically rich and\nresource-scarce language, has for long suffered\nfrom a gap in the resources available for NLP ap-\nplications, and lower level of empirical results than\nobserved in other, resource-rich languages. This\nwork provides the ﬁrst step in remedying the situ-\nation, by making available a large Hebrew PLM,\nnicknamed AlephBERT, with larger vocabulary and\nlarger training set than any Hebrew PLM before,\nand with clear evidence as to its empirical advan-\ntages. Our AlephBERT-base model obtains state-\nof-the- art results on the tasks of segmentation,\nPart of Speech Tagging, Named Entity Recogni-\ntion, and Sentiment Analysis. We outperform both\ngeneral multilingual PLMs (mBERT) as well as\nlanguage speciﬁc instantiations (HeBERT). More\nimportantly, using the new AlephBERT models we\nare now gaining similar beneﬁts as achieved in high\nresource languages from PLMs.\n9 Acknowledgements\nWe are enormously grateful to Roee Aharoni from\nGoogle and Yoav Goldberg from Bar Ilan Univer-\nsity for technical advise during the project. No less\nimportantly, we are indebted to Roee Aharoni for\ncoining the brand-name AlephBERT. The research\nreported in this paper is funded by an individual\ngrant by the Israel Science Foundation (ISF grant\n#1739/26) and a starting grant by the European\nResearch Council (ERC-StG Grant #677352), for\nwhich we are grateful.\nAlephBERT-base AlephBERT-small HeBERT mBERT-cased\nmax position embeddings 512 512 512 512\nnum attention heads 12 12 12 12\nnum hidden layers 12 6 12 12\nvocab size 52K 52K 30K 120K†\nTable 10: Huggingface BERT Conﬁgurations Comparison. †Only 2450 vocabulary entries contain Hebrew letters\nReferences\nAdam Amram, Anat Ben-David, and Reut Tsarfaty.\n2018. Representations and architectures in neu-\nral sentiment analysis for morphologically rich lan-\nguages: A case study from modern hebrew. In Pro-\nceedings of the 27th International Conference on\nComputational Linguistics, COLING 2018, Santa\nFe, New Mexico, USA, August 20-26, 2018, pages\n2242–2252.\nGiusepppe Attardi. 2015. Wikiextractor. https://\ngithub.com/attardi/wikiextractor.\nDan Bareket and Reut Tsarfaty. 2020. Neural mod-\neling for named entities and morphology (nemoˆ2).\nCoRR, abs/2007.15620.\nNaama Ben Mordecai and Michael Elhadad. 2005. He-\nbrew named entity recognition.\nAvihay Chriqui and Inbal Yahav. 2021. Hebert —&\nhebemo: a hebrew bert model and a tool for polarity\nanalysis and emotion recognition.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nStav Klein and Reut Tsarfaty. 2020. Getting the\n##life out of living: How adequate are word-pieces\nfor modelling complex morphology? In Proceed-\nings of the 17th SIGMORPHON Workshop on Com-\nputational Research in Phonetics, Phonology, and\nMorphology, SIGMORPHON 2020, Online, July 10,\n2020, pages 204–209.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach.\nAmir Pouran Ben Veyseh Minh Van Nguyen, Viet Lai\nand Thien Huu Nguyen. 2021. Trankit: A light-\nweight transformer-based toolkit for multilingual\nnatural language processing. In Proceedings of the\n16th Conference of the European Chapter of the\nAssociation for Computational Linguistics: System\nDemonstrations.\nAmir More, Amit Seker, Victoria Basmova, and Reut\nTsarfaty. 2019. Joint transition-based models for\nmorpho-syntactic parsing: Parsing strategies for\nmrls and a case study from modern hebrew. Trans.\nAssoc. Comput. Linguistics, 7:33–48.\nPedro Javier Ortiz Su´arez, Laurent Romary, and Benoˆıt\nSagot. 2020. A monolingual approach to contextual-\nized word embeddings for mid-resource languages.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n1703–1714, Online. Association for Computational\nLinguistics.\nShoval Sadde, Amit Seker, and Reut Tsarfaty.\n2018. The hebrew universal dependency tree-\nbank: Past present and future. In Proceedings of\nthe Second Workshop on Universal Dependencies,\nUDW@EMNLP 2018, Brussels, Belgium, November\n1, 2018, pages 133–143.\nDjam´e Seddah, Reut Tsarfaty, Sandra K ¨ubler, Marie\nCandito, Jinho D. Choi, Rich ´ard Farkas, Jen-\nnifer Foster, Iakes Goenaga, Koldo Gojenola Gal-\nletebeitia, Yoav Goldberg, Spence Green, Nizar\nHabash, Marco Kuhlmann, Wolfgang Maier, Joakim\nNivre, Adam Przepi ´orkowski, Ryan Roth, Wolf-\ngang Seeker, Yannick Versley, Veronika Vincze,\nMarcin Wolinski, Alina Wr´oblewska, and ´Eric Ville-\nmonte de la Clergerie. 2013. Overview of the\nSPMRL 2013 shared task: A cross-framework eval-\nuation of parsing morphologically rich languages.\nIn Proceedings of the Fourth Workshop on Statis-\ntical Parsing of Morphologically-Rich Languages,\nSPMRL@EMNLP 2013, Seattle, Washington, USA,\nOctober 18, 2013, pages 146–182.\nAmit Seker and Reut Tsarfaty. 2020. A pointer net-\nwork architecture for joint morphological segmen-\ntation and tagging. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n4368–4378, Online. Association for Computational\nLinguistics.\nReut Tsarfaty, Dan Bareket, Stav Klein, and Amit\nSeker. 2020. From SPMRL to NMRL: what did\nwe learn (and unlearn) in a decade of parsing\nmorphologically-rich languages (mrls)? In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, ACL 2020, Online,\nJuly 5-10, 2020, pages 7396–7408.\nReut Tsarfaty, Shoval Sadde, Stav Klein, and Amit\nSeker. 2019. What’s wrong with hebrew nlp?\nand how to make it right. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\nber 3-7, 2019 - System Demonstrations, pages 259–\n264.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020.\nTransformers: State-of-the-art natural language pro-\ncessing. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nDaniel Zeman, Jan Haji ˇc, Martin Popel, Martin Pot-\nthast, Milan Straka, Filip Ginter, Joakim Nivre, and\nSlav Petrov. 2018. CoNLL 2018 shared task: Mul-\ntilingual parsing from raw text to Universal Depen-\ndencies. In Proceedings of the CoNLL 2018 Shared\nTask: Multilingual Parsing from Raw Text to Univer-\nsal Dependencies, pages 1–21, Brussels, Belgium.\nAssociation for Computational Linguistics.\nA AlephBERT Training Details\nFor reference and to make our work reproducible\nwe specify here the main steps taken and param-\neters used during training of AlephBERT. We uti-\nlized the Huggingface Transformers framework\nwith most of the default training parameter val-\nues. Table-10 lists all of the training parameters\nthat we have manually speciﬁed in our code. We\nalso list the values used by the other models.\nTraining our AlephBERT-base model using the\nentire dataset proved to be technically challenging\ndue to the model size and data size. With the naive\napproach training the entire dataset without split-\nting it into chunks did not utilize the full processing\ncapacity of the GPUs and would have taken sev-\neral weeks to complete. To overcome this issue we\nfollowed the advice to split the dataset into chunks\nbased on the number of tokens in a sentence. The\nﬁrst chunk consisted of 70M senetences with 32 or\nless tokens. By limiting the maximum number to-\nkens we consequently limit the size of the training\nmatrices used by this chunk which consequently\nallowed for signiﬁcantly increasing the batch size\nwhich resulted in dramatically shorter training time\n- these 70M sentences took only 2.5 days to com-\nplete 5 epochs. The second chunk consisted of\nsentences having between 32 and 64 tokens, the\nthird chunk between 64 and 128 and the ﬁnal last\nchunk all sentences with more than 128 tokens. We\ntrained each chunk for 5 epochs with setting the\nlearning rate to 1e-4. Once we went over the en-\ntire dataset we trained for another 5 epochs with a\nlearning rate set to 5e-5 for a total of 10 epochs. We\ntrained our base model over the entire dataset for\n10 epochs on a NVidia DGX server with 8 V100\nGPUs which took 8 days. The small model was\ntrained over 10 epochs using 4 GTX 2080ti GPUs\nfor 5 days in total.",
  "topic": "Hebrew",
  "concepts": [
    {
      "name": "Hebrew",
      "score": 0.9671745300292969
    },
    {
      "name": "Computer science",
      "score": 0.7622023820877075
    },
    {
      "name": "Natural language processing",
      "score": 0.6893057823181152
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6456308364868164
    },
    {
      "name": "Language model",
      "score": 0.5864872336387634
    },
    {
      "name": "Vocabulary",
      "score": 0.51514732837677
    },
    {
      "name": "Point (geometry)",
      "score": 0.4392692744731903
    },
    {
      "name": "Linguistics",
      "score": 0.4269883334636688
    },
    {
      "name": "Speech recognition",
      "score": 0.394214391708374
    },
    {
      "name": "Mathematics",
      "score": 0.06169036030769348
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": []
}