{
  "title": "The perils and promises of fact-checking with large language models",
  "url": "https://openalex.org/W4391610230",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5093115684",
      "name": "Dorian Quelle",
      "affiliations": [
        "University of Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A2002850600",
      "name": "Alexandre Bovet",
      "affiliations": [
        "University of Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A5093115684",
      "name": "Dorian Quelle",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2002850600",
      "name": "Alexandre Bovet",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2970716846",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W2792307011",
    "https://openalex.org/W4383045293",
    "https://openalex.org/W4388996954",
    "https://openalex.org/W4378214315",
    "https://openalex.org/W4387120109",
    "https://openalex.org/W4312158082",
    "https://openalex.org/W4377864352",
    "https://openalex.org/W4324130084",
    "https://openalex.org/W2954444514",
    "https://openalex.org/W6730783224",
    "https://openalex.org/W2913649461",
    "https://openalex.org/W3196268181",
    "https://openalex.org/W2751368487",
    "https://openalex.org/W3211686893",
    "https://openalex.org/W3033187248",
    "https://openalex.org/W4362599805",
    "https://openalex.org/W4320167623",
    "https://openalex.org/W6810416661",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3196618497",
    "https://openalex.org/W3116194654",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W6848180329",
    "https://openalex.org/W3082241915",
    "https://openalex.org/W6995607057",
    "https://openalex.org/W6841886424",
    "https://openalex.org/W3197517448",
    "https://openalex.org/W4388067663",
    "https://openalex.org/W6744571630",
    "https://openalex.org/W4252076394",
    "https://openalex.org/W3034195663",
    "https://openalex.org/W3159190891",
    "https://openalex.org/W6748942964",
    "https://openalex.org/W6756207399",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3099977667",
    "https://openalex.org/W3180536457",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W3206435361",
    "https://openalex.org/W4364387438",
    "https://openalex.org/W3104758113",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4237040408",
    "https://openalex.org/W4401042451",
    "https://openalex.org/W4232664349",
    "https://openalex.org/W2613995098",
    "https://openalex.org/W3189373506"
  ],
  "abstract": "Automated fact-checking, using machine learning to verify claims, has grown vital as misinformation spreads beyond human fact-checking capacity. Large language models (LLMs) like GPT-4 are increasingly trusted to write academic papers, lawsuits, and news articles and to verify information, emphasizing their role in discerning truth from falsehood and the importance of being able to verify their outputs. Understanding the capacities and limitations of LLMs in fact-checking tasks is therefore essential for ensuring the health of our information ecosystem. Here, we evaluate the use of LLM agents in fact-checking by having them phrase queries, retrieve contextual data, and make decisions. Importantly, in our framework, agents explain their reasoning and cite the relevant sources from the retrieved context. Our results show the enhanced prowess of LLMs when equipped with contextual information. GPT-4 outperforms GPT-3, but accuracy varies based on query language and claim veracity. While LLMs show promise in fact-checking, caution is essential due to inconsistent accuracy. Our investigation calls for further research, fostering a deeper comprehension of when agents succeed and when they fail.",
  "full_text": "TYPE Original Research\nPUBLISHED /zero.tnum/seven.tnum February /two.tnum/zero.tnum/two.tnum/four.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/four.tnum/one.tnum/six.tnum/nine.tnum/seven.tnum\nOPEN ACCESS\nEDITED BY\nRafael Berlanga,\nUniversity of Jaume I, Spain\nREVIEWED BY\nIsmael Sanz,\nUniversity of Jaume I, Spain\nKevin Matthe Caramancion,\nUniversity of Wisconsin-Stout, United States\n*CORRESPONDENCE\nDorian Quelle\ndorian.quelle@uzh.ch\nRECEIVED /two.tnum/zero.tnum November /two.tnum/zero.tnum/two.tnum/three.tnum\nACCEPTED /two.tnum/two.tnum January /two.tnum/zero.tnum/two.tnum/four.tnum\nPUBLISHED /zero.tnum/seven.tnum February /two.tnum/zero.tnum/two.tnum/four.tnum\nCITATION\nQuelle D and Bovet A (/two.tnum/zero.tnum/two.tnum/four.tnum) The perils and\npromises of fact-checking with large language\nmodels. Front. Artif. Intell./seven.tnum:/one.tnum/three.tnum/four.tnum/one.tnum/six.tnum/nine.tnum/seven.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/four.tnum/one.tnum/six.tnum/nine.tnum/seven.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/four.tnum Quelle and Bovet. This is an\nopen-access article distributed under the\nterms of the\nCreative Commons Attribution\nLicense (CC BY) . The use, distribution or\nreproduction in other forums is permitted,\nprovided the original author(s) and the\ncopyright owner(s) are credited and that the\noriginal publication in this journal is cited, in\naccordance with accepted academic practice.\nNo use, distribution or reproduction is\npermitted which does not comply with these\nterms.\nThe perils and promises of\nfact-checking with large\nlanguage models\nDorian Quelle /one.tnum,/two.tnum* and Alexandre Bovet /one.tnum,/two.tnum\n/one.tnumDepartment of Mathematical Modeling and Machine Learning, Uni versity of Zurich, Zurich,\nSwitzerland, /two.tnumDigital Society Initiative, University of Zurich, Zurich, Swi tzerland\nAutomated fact-checking, using machine learning to verify claims , has grown\nvital as misinformation spreads beyond human fact-checking capacit y. Large\nlanguage models (LLMs) like GPT-/four.tnum are increasingly trusted to write academic\npapers, lawsuits, and news articles and to verify information, emphasizing their\nrole in discerning truth from falsehood and the importance of be ing able to\nverify their outputs. Understanding the capacities and limita tions of LLMs in fact-\nchecking tasks is therefore essential for ensuring the health o f our information\necosystem. Here, we evaluate the use of LLM agents in fact-checking b y having\nthem phrase queries, retrieve contextual data, and make decisio ns. Importantly,\nin our framework, agents explain their reasoning and cite the re levant sources\nfrom the retrieved context. Our results show the enhanced prowe ss of LLMs\nwhen equipped with contextual information. GPT-/four.tnum outperformsGPT-/three.tnum, but\naccuracy varies based on query language and claim veracity. While LL Ms show\npromise in fact-checking, caution is essential due to inconsiste nt accuracy. Our\ninvestigation calls for further research, fostering a deeper comprehension of\nwhen agents succeed and when they fail.\nKEYWORDS\nfact-checking, misinformation, large language models, human computer interaction,\nnatural language processing, low-resource languages\n/one.tnum Introduction\nFact-checking has become a vital tool to reduce the spread of misinformation online,\nshown to potentially reduce an individual’s belief in false news and rumors ( Morris et al.,\n2020; Porter and Wood, 2021 ) and to improve political knowledge ( Nyhan and Reiﬂer,\n2015). While verifying or refuting a claim is a core task of any journalist, a variety of\ndedicated fact-checking organizations have formed to correct misconceptions, rumors, and\nfake news online. A pivotal moment in the rise of fact-checking happened in 2009 when\nthe prestigious Pulitzer Prize in the national reporting category was awarded to Politifact.\nPolitifact’s innovation was to propose the now standard model of an ordinal rating, which\nadded a layer of structure and clarity to the fact check and inspired dozens of projects\naround the world (\nMantzarlis, 2018). The second wave of fact-checking organizations and\ninnovation in the fact-checking industry was catalyzed by the proliferation of viral hoaxes\nand fake news during the 2016 US presidential election (\nBovet and Makse, 2019 ; Grinberg\net al., 2019 ) and Brexit referendum ( Mantzarlis, 2018 ). Increased polarization ( Flamino\net al., 2023 ), political populism, and awareness of the potentially detrimental eﬀects of\nmisinformation have ushered in the “rise of fact-checking” ( Graves and Cherubini, 2016 ).\nAlthough fact-checking organizations play a crucial role in the ﬁght against\nmisinformation, notably during the COVID-19 pandemic ( Siwakoti et al., 2021 ), the\nprocess of fact-checking a claim is an extremely time-consuming task. A professional\nFrontiers in Artiﬁcial Intelligence /zero.tnum/one.tnum frontiersin.org\nQuelle and Bovet /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/four.tnum/one.tnum/six.tnum/nine.tnum/seven.tnum\nfact-checker might take several hours or days on any given claim\n(\nHassan et al., 2015 ; Adair et al., 2017 ). Due to an ever-increasing\namount of information online and the speed at which it spreads,\nrelying solely on manual fact-checking is insuﬃcient and makes\nautomated solutions and tools that increase the eﬃciency of fact-\ncheckers necessary.\nRecent research has explored the potential of using large\nartiﬁcial intelligence language models as a tool for fact-checking\n(\nHe et al., 2021 ; Caramancion, 2023 ; Choi and Ferrara, 2023 ; Hoes\net al., 2023 ; Sawi´nski et al., 2023 ). However, signiﬁcant challenges\nremain when employing large language models (LLMs) to assess\nthe veracity of a statement. One primary issue is that fact-checks\nare potentially included in some of the training data for LLMs.\nTherefore, successful fact-checking without additional context may\nnot necessarily be attributed to the model’s comprehension of facts\nor argumentation. Instead, it may simply reﬂect the LLM’s retention\nof training examples. While this might suﬃce for fact-checking past\nclaims, it may not generalize well beyond the training data.\nLarge language models (LLMs) like GPT-4 are increasingly\ntrusted to write academic papers, lawsuits, news articles,\n/one.tnumor to\ngather information ( Choudhury and Shamszare, 2023 ). Therefore,\nan investigation into the models’ ability to determine whether a\nstatement is true or false is necessary to understand whether LLMs\ncan be relied upon in situations where accuracy and credibility are\nparamount. The widespread adoption and reliance on LLMs pose\nboth opportunities and challenges. As they take on more signiﬁcant\nroles in decision-making processes, research, journalism, and legal\ndomains, it becomes crucial to understand their strengths and\nlimitations. The increasing use of advanced language models in\ndisseminating misinformation online highlights the importance of\ndeveloping eﬃcient automated systems. The 2024 WEF Global\nRisk Report ranks misinformation and disinformation as the\nmost dangerous short-term global risk as LLMs have enabled an\n“explosion in falsiﬁed information” removing the necessity of niche\nskills to create “synthetic content” (\nWorld Economic Forum, 2024 ).\nOn the other hand, artiﬁcial intelligence models can help identify\nand mitigate false information, thereby helping to maintain a more\nreliable and accurate information environment. The ability of LLMs\nto discern truth from falsehood is not just a measure of their\ntechnical competence but also has broader implications for our\ninformation ecosystem.\nA signiﬁcant challenge in automated fact-checking systems\nrelying on machine learning models has been the lack of\nexplainability of the models’ prediction. This is a particularly\ndesirable goal in the area of fact-checking as explanations of\nverdicts are an integral part of the journalistic process when\nperforming manual fact-checking (\nKotonya and Toni, 2020 ). While\nthere has been some progress in highlighting features that justify\na verdict, a relatively small number of automated fact-checking\nsystems have an explainability component (\nKotonya and Toni,\n2020).\nSince the early 2010’s, a diverse group of researchers have\ntackled automated fact-checking with various approaches. This\n/one.tnumhttps://cybernews.com/news/academic-cheating-chatgpt-ope nai/;\nhttps://www.nytimes.com//two.tnum/zero.tnum/two.tnum/three.tnum//zero.tnum/six.tnum//zero.tnum/eight.tnum/nyregion/lawyer-chatgpt-\nsanctions.html\nsection introduces the concept of automated fact-checking and\nthe diﬀerent existing approaches. Diﬀerent shared tasks, where\nresearch groups tackle the same problem or dataset with a\ndeﬁned outcome metric, have been announced with the aim of\nautomatically fact-checking claims. For example, the shared task\nRUMOUR EVAL provided a dataset of “dubious posts and ensuing\nconversations in social media, annotated both for stance and\nveracity” (\nGorrell et al., 2019 ). CLEF C HECK THAT! prepared three\ndiﬀerent tasks, aiming to solve diﬀerent problems in the fact-\nchecking pipeline (\nNakov et al., 2022b ). First, “Task 1 asked to\npredict which posts in a Twitter stream are worth fact-checking,\nfocusing on COVID-19 and politics in six languages” (\nNakov et al.,\n2022a). Task 2 “asks to detect previously fact-checked claims (in\ntwo languages)” ( Nakov et al., 2022c ). Lastly, “Task 3 is designed\nas a multi-class classiﬁcation problem and focuses on the veracity\nof German and English news articles” (\nKöhler et al., 2022 ). The\nFact Extraction and VERiﬁcation shared task (FEVER) “challenged\nparticipants to classify whether human-written factoid claims could\nbe SUPPORTED or REFUTED using evidence retrieved from\nWikipedia” (\nThorne et al., 2018b ). In general, most of these\nchallenges and proposed solutions disaggregate the fact-checking\npipeline into a multi-step problem, as detection, contextualization,\nand veriﬁcation all require speciﬁc approaches and methods (\nDas\net al., 2023 ). For example, ( Hassan et al., 2017 ) proposed four\ncomponents to verify a web document in their C LAIM BUSTER\npipeline. First, a claim monitor that performs document retrieval\n(1), a claim spotter that performs claim detection (2), a claim\nmatcher that matches a detected claim to fact-checked claims (3),\nand a claim checker that performs evidence extraction and claim\nvalidation (4) (\nZeng et al., 2021 ).\nIn their summary of automated fact-checking ( Zeng et al., 2021 )\ndeﬁne entailment as “cases where the truth of hypothesis h is highly\nplausible given text t.” More stringent deﬁnitions that demand\nthat a hypothesis is true in “every possible circumstance where t\nis true” fail to handle the uncertainty of Natural Language. Claim\nveriﬁcation today mostly relies on ﬁne-tuning a large pre-trained\nlanguage model on the target dataset (\nZeng et al., 2021 ). State-\nof-the-art entailment models have generally relied on transformer\narchitecture such as BERT (\nKenton and Toutanova, 2019 ) and\nRoBERTa ( Liu et al., 2019 ). Hoes et al. (2023) tested GPT-3.5’s\nclaim veriﬁcation performance on a dataset of PolitiFact statements\nwithout adding any context. They found that GPT-3.5 performs\nwell on the dataset and argue that it shows the potential of\nleveraging GPT-3.5 and other LLMs for enhancing the eﬃciency\nand expediency of the fact-checking process. Novel large language\nmodels have been used by\nSawi´nski et al. (2023) in assessing the\ncheck-worthiness. The authors test various models ability to predict\nthe check-worthiness of English language content. The authors\ncompared GPT-3.5 with various other language models. They ﬁnd\nthat a ﬁne-tuned version of GPT3.5 slightly ourperforms DeBerta-\nv3 (\nHe et al., 2021 ), an improvement over the original DeBERTa\narchitecture ( He et al., 2020 ). Choi and Ferrara (2023) use fact-\nchecks to construct a synthetic dataset of contradicting, entailing\nor neutral claims. They create the synthetic data using GPT-4 and\npredict the entailment using a smaller ﬁne-tuned LLM. Similarly,\nCaramancion (2023) test the ability of various LLMs to discern fake\nnews by providing Bard, BingAI, GPT-3.5, and GPT-4 on a list of\n100 fact-checked news items. The authors ﬁnd that all LLMs achieve\nFrontiers in Artiﬁcial Intelligence /zero.tnum/two.tnum frontiersin.org\nQuelle and Bovet /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/four.tnum/one.tnum/six.tnum/nine.tnum/seven.tnum\nperformances of around 64–71% accuracy, with GPT 4 receiving\nthe highest score among all LLMs.\nCuartielles Saura et al. (2023)\ninterview fact-checking platforms about their expectations of Chat-\nGPT as a tool for both misinformation fabrication, detection,\nand veriﬁcation. They ﬁnd that while professional fact-checkers\nhighlight the potential perils such as the reliability of sources, the\nlack of insights into the training process, and the enhanced ability\nof malevolent actors to fabricate false content, they nevertheless\nview it as a useful resource for both information gathering and\nthe detection and debunking of false news (\nCuartielles Saura et al.,\n2023).\nWhile earlier eﬀorts in claim veriﬁcation did not retrieve\nany evidence beyond the claim itself (for example, see Rashkin\net al., 2017 ), augmenting claim veriﬁcation models with evidence\nretrieval has become standard for state-of-the-art models ( Guo\net al., 2021 ). In general, evidence retrieval aims to incorporate\nrelevant information beyond the claim. For example, from\nencyclopedias (e.g., Wikipedia\nThorne et al., 2018a ), scientiﬁc\npapers ( Wadden et al., 2020 ), or search engines such as Google\n(Augenstein et al., 2019 ). Augenstein et al. (2019) submit a claim\nverbatim as a query to the Google Search API and use the\nﬁrst ten search results as evidence. A crucial issue for evidence\nretrieval lies in the fact that it implicitly assumes that all available\ninformation is trustworthy and that veracity can be gleaned from\nsimply testing the coherence of the claim with the information\nretrieved. An alternative approach that circumvents the issue of\nthe inclusion of false information has been to leverage knowledge\ndatabases (also knowledge graphs) that aim to “equip machines\nwith comprehensive knowledge of the world’s entities and their\nrelationships” (\nWeikum et al., 2020 ). However, this approach\nassumes that all facts pertinent to the checked claim are present in\na graph. An assumption that (\nGuo et al., 2021 ) called unrealistic.\nOur primary contributions in this study are 2-fold. First,\nwe conduct a novel evaluation of two of the most used LLMs,\nGPT-3.5, and GPT-4, on their ability to perform fact-checking\nusing a specialized dataset. An original part of our examination\ndistinguishes the models’ performance with and without access to\nexternal context, highlighting the importance of contextual data\nin the veriﬁcation process. Second, by allowing the LLM agent\nto perform web searches, we propose an original methodology\nintegrating information retrieval and claim veriﬁcation for\nautomated fact-checking. By leveraging the ReAct framework, we\ndesign an iterative agent that decides whether to conclude a web\nsearch or continue with more queries, striking a balance between\naccuracy and eﬃciency. This enables the model to justify its\nreasoning and cite the relevant retrieved data, therefore addressing\nthe veriﬁability and explainability of the model’s verdict. Lastly, we\nperform the ﬁrst assessment of GPT-3.5’s capability to fact-check\nacross multiple languages, which is crucial in today’s globalized\ninformation ecosystem.\nWe ﬁnd that incorporating contextual information signiﬁcantly\nimproves accuracy. This highlights the importance of gathering\nexternal evidence during automated veriﬁcation. We ﬁnd that\nthe models show good average accuracy, but they struggle\nwith ambiguous verdicts. Our evaluation shows that GPT-\n4 signiﬁcantly outperforms GPT-3.5 at fact-checking claims.\nHowever, performance varies substantially across languages.\nNon-English claims see a large boost when translated to English\nbefore being fed to the models. We ﬁnd no sudden decrease in\naccuracy after the oﬃcial training cutoﬀ dates for GPT-3.5 and\nGPT-4. This suggests that the continued learning from human\nfeedback may expand these models’ knowledge.\n/two.tnum Materials and methods\n/two.tnum./one.tnum Approach\nThis paper contributes to both the evidence retrieval and claim\nveriﬁcation steps of the automated fact-checking pipeline. Our\napproach focuses on verifying claims—assessing if a statement\nis true or false, while simultaneously retrieving contextual\ninformation to augment the ability of the LLM to reason about\nthe given claims. We use large artiﬁcial intelligence language\nmodels GPT-3.5 and GPT-4. We combine state-of-the-art language\nmodels, iterative searching, and agent-based reasoning to advance\nautomated claim veriﬁcation.\nGPT-3.5 and GPT-4 are neural networks trained on vast\namounts of textual data to generate coherent continuations of text\nprompts (\nVaswani et al., 2017 ). They comprise multiple layers\nof transformer blocks, which contain self-attention mechanisms\nthat allow the model to learn contextual representations of words\nand sentences (\nVaswani et al., 2017 ). LLMs are trained using self-\nsupervision, where the model’s objective is to predict the next\ntoken in a sequence of text. The GPT models are trained on\nvast amounts of unstructured textual data like the common crawl\ndataset, which is the largest dataset to be included in the training.\nCommon Crawl is a web archive that consists of terabytes of\ndata collected since 2008 (\nBuck et al., 2014 ; Brown et al., 2020 ).\nGPT-3.5 and GPT-4 are additionally trained with reinforcement\nlearning from human feedback (Rlhf), where human feedback is\nincorporated to enhance the models’ usability. OpenAI states that\nthey regularly update their models based on human feedback,\npotentially leading to knowledge of current events that expands\nupon the initial training regime, which was stopped in September\nof 2021.\n/two.tnum\nWe are evaluating the performance of GPT-3.5 and GPT-4\nbased on two conditions. First, we query the models with the\nstatement, the author, and the date of the statement. The model\ndoes not possess any means of retrieving further information that\nmight enable it to make an informed decision. Rather this approach\nrelies on the model having knowledge of the events described in\nthe claim. In the second condition, we enable the LLM to query a\nGoogle Search engine to retrieve relevant information surrounding\nthe claim. To prevent the model from uncovering the fact-check\nitself, we ﬁlter the returned Google search results for all domains\npresent in the dataset. We present the LLM with information\nreturned from the Google Search Engine API, comprising previews\nof search results. These previews included the title of the website,\nthe link, and an extract of relevant context, mirroring the\ntypical user experience on Google. To reﬁne our approach, we\nexperimented with integrating additional information from the\nfull HTML content of websites. We quickly realized that this\nvoluminous data was overwhelming the LLM’s context window,\n/two.tnumhttps://openai.com/blog/chatgpt\nFrontiers in Artiﬁcial Intelligence /zero.tnum/three.tnum frontiersin.org\nQuelle and Bovet /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/four.tnum/one.tnum/six.tnum/nine.tnum/seven.tnum\nleading to suboptimal performance. To address this, we employed\nBM25, an information retrieval function (\nRobertson and Zaragoza,\n2009), to distill the most critical parts of each website. We found no\nimprovement to the performance of the LLMs, as Google already\nuses machine learning methods to identify the most important\ninformation to include in the preview. We equip the agent with\nthe ability to query Google by leveraging the Reasoning and Acting\n(ReAct) framework, proposed by\nYao et al. (2023), which allows\nan LLM to interact with tools. The goal of the ReAct framework\nis to combine reasoning and take actions. Reasoning refers to the\nmodel planning and executing actions based on observations from\nthe environment. Actions are function calls or API calls by the\nmodel to retrieve additional information from external sources.\nThe model is initially prompted with the claim and then decides if\nit needs to take actions. As we equip both models with the ability\nto query Google, the model then is able to retrieve information\nand receives its next observation. Based on this observation, the\nmodel can decide to either return a ﬁnal answer or retrieve\ninformation with a diﬀerent query. If the model fails to answer\nafter three iterations of retrieving information, we terminate the\nsearch. The model retrieves 10 Google search results per iteration.\nTo test the claim veriﬁcation in a realistic scenario, any result\nfrom a fact-checking website present in the dataset is removed\nfrom the results. If the model was terminated because it reached\nthe maximum number of iterations, it is prompted to provide a\nﬁnal answer based on all of its previous Google searches. In this\npaper, we employ the LangChain library (\nChase, 2022 ) to create\nan agent within the ReAct framework. We show in Figure 1 the\nworkﬂow we implemented and an example of the treatment of a\nclaim.\nWe employed the 16k context window which is standard\nfor the GPT API. Since our experiments larger context windows\nhave been introduced (OpenAI, 2023)\n/three.tnum, which could enable\nfuture LLM powered fact-checking applications to incorporate\nmore search engine results or to include more content from\neach website.\nTo illustrate the capabilities of the proposed system,\nFigures 2, 3 showcase two correctly classiﬁed and two incorrectly\nassessed verdicts. The full transcripts from these examples,\nincluding all retrieved Google links, are available in the\nSupplementary material.\n/two.tnum./two.tnum Experiment\nWe conduct two experiments to evaluate how well our agents\ncan fact-check diﬀerent claims. First, we evaluate whether GPT-\n4 is signiﬁcantly better at fact-checking than GPT-3.5. GPT-4 has\nbeen shown to outperform GPT-3.5 on a variety of benchmarks,\nparticularly in zero-shot reasoning tasks (\nEspejel et al., 2023 ). We\ntherefore compare the performance of GPT-3.5 and GPT-4 on two\ndatasets, a dataset of US political fact-checked claims provided by\nPolitiFact (\nMisra, 2022 ) and a dataset of multilingual fact-checked\nclaims provided by Data Common. As GPT-3.5 is openly available\nto the public for free, it is used substantially more used than\n/three.tnum OpenAI (/two.tnum/zero.tnum/two.tnum/three.tnum).Models—openAI api .\nGPT-4, thereby increasing the importance of understanding its\nperformance.\n/two.tnum./two.tnum./one.tnum Experiment on the PolitiFact dataset\nThe dataset used in this experiment consists of a database of\nfact-checked claims by PolitiFact (\nMisra, 2022 ). Each observation\nhas a statement originator (the person who made the statement\nbeing fact-checked), a statement (the statement being fact-\nchecked), a statement date, and a verdict. The verdict of a fact-\ncheck is one of six ordinal categories, indicating to which degree\na statement is true or false: True, Mostly-True, Half-True, Mostly-\nFalse, False, and Pants-Fire. Pants-Fire indicates that a statement is\nutterly false. In total, the dataset has 21,152 fact-checks, spanning a\ntime-frame of 2007–2022. The number of fact-checks per month\nis shown in\nFigure 4. We sampled 500 claims for each response\ncategory, leading to a total of 3,000 unique fact-checks to be parsed.\nWe thereby ensured an equal distribution of outcome labels in the\nﬁnal dataset.\nWhile comparing the two models, we also compare the\naccuracy of the model with and without additional context. We\ntherefore run four conditions, GPT-3.5 with context and without\ncontext, and GPT-4 with and without context. The No-Context\ncondition refers to the model being prompted to categorize a claim\ninto the veracity categories, without being given any context, or\nthe ability to retrieve context. In addition to the veracity labels,\nthe model is able to return a verdict of “uncertain” to indicate,\nthat it isn’t capable of returning an assessment. In the Context\ncondition, the agent is capable of formulating Google queries to\nretrieve information pertinent to the claim. When the model is able\nto retrieve contextual information, any results from PolitiFact are\nexcluded from the results.\n/two.tnum./two.tnum./two.tnum Experiment on the multilingual dataset\nSecondly, we investigate whether the ability of LLMs to\nfact-check claims depends signiﬁcantly on the language of\nthe initial claim. While both GPT-3.5 and GPT-4 exhibit\nimpressive multilingual understanding, a variety of empirical\nresults have shown that large language models struggle to\nadequately understand and generate non-English language text\n(\nBang et al., 2023 ; Jiao et al., 2023 ; Zhu et al., 2023 ). However,\nmisinformation is a problem that is not restricted to high-\nresource languages. Conversely, fact-checking has become a global\nendeavor over the last decade, with a growing number of dedicated\norganizations in non-English language countries.\nThe dataset used is a fact-check dump by Data Commons.\nEach fact-check has an associated author and date. Additionally,\nit contains the Claim that is being fact-checked, a Review, and a\nverdict.\nThe dataset contains a breadth of fact-checking organizations\nand languages. In total, we detect 78 unique languages in\nthe dataset. By extracting the domain of each fact-check link\nwe ﬁnd that it contains fact-checks from 454 unique fact-\nchecking organizations. The largest domains, factly (7,277),\nfactcrescendo (5,664), youturn (3,582), boatos (2,829), dpa-\nfact-checking (2,394), veraﬁles (1,972), uol (1,729), and\ntempo (1,133), have more than one thousand fact-checks,\nFrontiers in Artiﬁcial Intelligence /zero.tnum/four.tnum frontiersin.org\nQuelle and Bovet /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/four.tnum/one.tnum/six.tnum/nine.tnum/seven.tnum\nFIGURE /one.tnum\nWorkﬂow showing how we enable LLM agents to interact with a context to assess the veracity of a claim (top). Example of the treatment of a speciﬁc\nclaim (bottom).\nand make up 73% of the dataset. In contrast, there are 266\ndiﬀerent domains which only have one associated fact-check.\nFigure 4 shows the total amount of fact-checks in the dataset\nper month. While the oldest fact-check in the dataset was\npublished in 2011, we see that very few fact-checks were\nuploaded until early 2019. Since then the dataset contains a\nrelatively stable amount of around eight hundred fact-checks\nper month.\nTo use the Data Commons dataset we need to standardize\nthe dataset. We reduced the number of unique verdicts, as\nmany diﬀerent fact-checking organizations use diﬀerent ordinal\nscales that are hard to unify. By mapping all scales onto a\ncoarse 4-level scale (“False, ” “Mostly False, ” “Mostly True, ” and\n“True”), we can then compare how the ability of our model to\nfact-check depends on the correct verdict assigned by the fact-\nchecker. We translated all present verdicts from their original\nlanguage to English. We then manually mapped all verdicts which\nappeared at least twice in the dataset ( n = 468) to the four\ncategories. We removed all observations that we were unable to\nmap to one of these four categories. This included observations\nwith a verdict such as “Sarcasm, ” “Satire, ” or “unconﬁrmed.”\nSubsequently, we discarded all languages, that did not have at\nleast 50 observations and languages that did not have at least\n10 “true” or “mostly true” observations. From the remaining\nlanguages, we sampled up to 500 observations. To compare\nthe ability of our approach across languages, we then used\nGoogletrans,\n/four.tnuma free python library that utilizes the Google\nTranslate API, to translate all claims from their original language\nto English.\nIn this experiment, we compare the performance of GPT-\n3.5 both with context and without context in English and in\nthe original language. In the context condition, we removed\n/four.tnumhttps://pypi.org/project/googletrans/\nFrontiers in Artiﬁcial Intelligence /zero.tnum/five.tnum frontiersin.org\nQuelle and Bovet /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/four.tnum/one.tnum/six.tnum/nine.tnum/seven.tnum\nFIGURE /two.tnum\nExamples of PolitiFact statements for which the model returns c orrect responses. The LLM is tasked with verifying a statement m ade by Donald\nTrump, indicating that Ted Cruz is mathematically out of the r ace. The LLM uses Google to retrieve information on the delegate count and correctly\nconcludes the statement is mostly true. We show the Google queries p erformed by the LLM and the ﬁrst results of each query. In the se cond\nexample, the LLM is tasked to verify a statement claiming Donal d Trump’s driver “did burnouts” during a race. The LLM ﬁnds infor mation that Donald\nTrump did a lap around the race but correctly concludes that no in formation indicates that he did “burnouts.” The full examples, i ncluding all Google\nresults, are shown in the Supplementary material .\nFrontiers in Artiﬁcial Intelligence /zero.tnum/six.tnum frontiersin.org\nQuelle and Bovet /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/four.tnum/one.tnum/six.tnum/nine.tnum/seven.tnum\nFIGURE /three.tnum\nExamples of PolitiFact statements for which the model returns i ncorrect responses. We show the Google queries performed by the LLM a nd the ﬁrst\nresults of each query. The LLM is asked to verify whether the Ob ama administration paid a ransom payment to Iran. The LLM ﬁnds i nformation on the\npayment but can’t conclusively conﬁrm the purpose of the payment . It concludes that the statement is half-true. PolitiFact argue s that the statement\nis mostly-false, as the payment is not necessarily a ransom payme nt. In the second example, the LLM is asked to verify whether a be er brand is\nAmerican. It ﬁnds information indicating that the company is Am erican and returns False. The company has, however, been bought b y foreign\ninvestors, making the statement true. The full examples, inclu ding all Google results, are shown in the Supplementary material .\nFrontiers in Artiﬁcial Intelligence /zero.tnum/seven.tnum frontiersin.org\nQuelle and Bovet /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/four.tnum/one.tnum/six.tnum/nine.tnum/seven.tnum\nFIGURE /four.tnum\nNumber of fact-checks per month in the Data Commons and PolitiFact datasets. Number of fact-checks Per Month in the Data Commons and\nPolitiFact datasets. In blue (dashed) the number of fact-chec ks in the PolitFact Dataset are shown. The orange (solid) line indi cates the number of\nfact-checks in the Data Commons dataset.\nTABLE /one.tnumComparison of accuracy of all conditions on the PolitiFact dataset.\nNo context Context No context Context\nCorrect verdict GPT-/three.tnum./five.tnumGPT-/four.tnumGPT-/three.tnum./five.tnumGPT-/four.tnumGPT-/three.tnum./five.tnumGPT-/four.tnumGPT-/three.tnum./five.tnumGPT-/four.tnum\nPants ﬁre 92.19 91.80 93.42 93.92 25.81 28.94 0.00 12.17\nFalse 81.63 88.08 88.75 86.13 64.17 47.49 86.32 55.82\nMostly false 79.64 68.70 71.11 55.82 8.82 50.00 10.67 40.75\nHalf true 36.16 51.90 51.58 67.26 2.75 11.58 0.00 3.57\nMostly true 42.35 79.29 67.27 80.88 9.41 59.91 36.82 61.13\nTrue 49.11 71.30 67.61 84.92 37.05 22.22 33.80 33.00\nFor the table on the left, accuracy is calculated as the percentage of ti me predicting the correct overall category. Predicting any of pants- ﬁre, false, or mostly false is correct when the claim is\nlabeled as pants-ﬁre, false, or mostly false. Conversely, predicting any half-true, mostly true, or true is correct when the claim is labeled as on e of these categories. For the table on the right,\naccuracy is calculated as the percentage of time predicting the Poli tiFact category. For example, predicting “half-true” when the claim i s labeled as “mostly-true” is treated as a false prediction.\nall fact-checking websites from the dataset from the Google\nsearch results.\n/three.tnum Results\n/three.tnum./one.tnum Experiment on the PolitiFact dataset\nTable 1 presents a comparative analysis of the GPT-3.5 and\nGPT-4 models in the task of automated fact-checking using the\nPolitiFact dataset. The table categorizes the performance of both\nmodels across diﬀerent veracity labels ranging from “pants-ﬁre” to\n“true.” In the left table, the accuracy of the model is computed by\nconsidering only two categories, i.e., grouping “pants-ﬁre, ” “false, ”\nand “mostly-false” together to false and “half-true, ” “mostly-true, ”\nand “true” to true. In the right table, the accuracy is computed using\nall the PolitiFact categories.\nIn the no-context condition, considering the overall accuracy\n(left table), GPT-4 generally outperforms GPT-3.5. GPT-3.5\npredicts the veracity of a claim to be false in 58.2% of the\ncases (compared to 22.89% for GPT-4) achieving an accuracy\nof 36–49% in the true-label categories. In the false-label\ncategories, the diﬀerence between the two models is signiﬁcantly\nsmaller and GPT-3.5 outperforms GPT-4 in the mostly false\ncategory. Both models achieve an accuracy of over 90% for\nclaims that are labeled with “pants-on-ﬁre.” In the context\ncondition, GPT-3.5 is signiﬁcantly better calibrated, meaning\nit exhibits a more balanced accuracy between true and false\nverdicts. While the accuracy in the false categories only diﬀers\ninsigniﬁcantly, GPT-3.5 achieves signiﬁcantly better results in\npredicting true verdicts in the context condition than in the\nno-context condition. Similarly, the diﬀerence in false categories\nis smaller for GPT-4 than in true categories, where it achieves\nan increase of 10.19 percentage points on average. In the\ncontext condition, both GPT-3.5 and GPT-4 outperform the\nno-context conditions on average. Context both increases the\nability of the models to discriminate true from false claims, but\nadditionally, calibrates both models better, predicting true for more\ncases correctly.\nFrontiers in Artiﬁcial Intelligence /zero.tnum/eight.tnum frontiersin.org\nQuelle and Bovet /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/four.tnum/one.tnum/six.tnum/nine.tnum/seven.tnum\nFIGURE /five.tnum\nAccuracy of GPT-/three.tnum./five.tnum and GPT-/four.tnum overtime on the PolitiFact dataset. Yearly rolling average of the accuracy of LLMs over Time. (A) Displays the\naccuracy of GPT-/four.tnum.(B) Shows the accuracy of GPT-/three.tnum./five.tnum. The blue line indicates the context condition, and the orange line indicates the no-context\ncondition. The vertical line represents the training end date of both models according to OpenAI. A faint line is the /three.tnum-month average. The x-axis\nrepresents the date of the claim. The bands represent one stan dard error.\nTABLE /two.tnumPerformance on the multilingual dataset without context.\nLanguage Accuracy English Accuracy multilingual F/one.tnum EnglishF/one.tnum multilingualNumber of samples\nTurkish 84.19 81.50 83.59 81.91 500\nIndonesian 86.68 84.59 87.52 79.59 500\nFrench 74.67 81.67 75.57 72.30 166\nEnglish - 60.98 - 68.65 500\nThai 48.21 54.34 50.41 66.45 69\nPortuguese 77.22 65.56 77.02 64.28 500\nTamil 70.33 59.19 67.68 51.59 500\nSpanish 56.92 51.26 57.69 49.51 500\nItalian 45.25 50.29 48.25 47.07 427\nChinese 64.86 43.75 68.56 42.88 174\nHebrew 49.61 41.17 56.51 40.65 287\nFarsi 68.28 40.54 71.96 36.26 259\nTelugu 59.79 26.99 67.03 27.24 500\nAzerbaijani 37.23 22.89 42.26 15.64 98\nAccuracy and F1 score for the fact-checking without context on the m ultilingual dataset. Accuracy English and F1 English are the scor es for the case where the models are provided with English\ntranslation of the claims while the other scores are computed in the cas e where the models are provided with the original claims.\nAcross all conditions, both models generally fare better in\nidentifying false statements (“pants-ﬁre, ” “mostly-false, ” and “false”)\nthan true ones (“half-true, ” “mostly-true, ” and “true”), mirroring\nthe ﬁndings of\nHoes et al. (2023) in the no-context settings.\nThis could be attributed to the inherent complexity of verifying\na statement’s absolute truth compared to identifying falsehoods.\nLastly, both models have reduced performance in less extreme\ncategories like “half-true” and “mostly-false.” These categories are\ninherently ambiguous and represent statements that have elements\nof both truth and falsehood, making them more challenging to\nclassify accurately. The table on the right only considers a claim to\nbe correctly classiﬁed when the model predicts the exact category.\nThe extremely low scores show that the models are unable to\npredict the exact shade of truth that PolitiFact assigned to the\nstatement.\nBoth GPT-3.5 and GPT-4 were trained with data up to\nSeptember of 2021.\n/five.tnumAs the training data of both models is private,\ndata leakage is a signiﬁcant concern for fact-checks published\nbefore that date. To investigate, whether the accuracy of the LLMs\n/five.tnumhttps://platform.openai.com/docs/models/gpt-/four.tnum\nFrontiers in Artiﬁcial Intelligence /zero.tnum/nine.tnum frontiersin.org\nQuelle and Bovet /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/four.tnum/one.tnum/six.tnum/nine.tnum/seven.tnum\nTABLE /three.tnumPerformance on the multilingual dataset with context.\nLanguage Accuracy English Accuracy multilingual F/one.tnum EnglishF/one.tnum multilingualNumber of samples\nPortuguese 87.42 89.21 75.97 80.78 500\nIndonesian 87.16 89.98 77.26 73.36 500\nEnglish - 80.16 - 71.33 500\nTelugu 83.15 83.80 77.21 69.97 500\nThai 77.27 50.00 55.66 66.66 69\nFrench 89.55 87.09 79.58 56.13 166\nChinese 75.81 84.78 71.86 55.35 174\nFarsi 77.78 59.00 69.15 48.58 259\nTurkish 72.28 71.79 70.79 47.32 500\nSpanish 82.68 75.18 63.01 46.30 500\nItalian 56.93 56.29 40.15 45.54 427\nTamil 80.56 55.22 62.95 34.86 500\nHebrew 73.61 85.71 63.81 34.42 287\nAzerbaijani 62.07 44.00 43.43 33.23 97\nAccuracy and F1 score for the fact-checking with context on the mult ilingual dataset. Accuracy English and F1 English are the scores for the case where the models are provided with English\ntranslation of the claims while the other scores are computed in the cas e where the models are provided with the original claims.\ndiﬀers over time we plot the accuracy of all four conditions (2\nmodels x 2 context conditions) in\nFigure 5.\nThe accuracy of all four conditions exhibits an upward\ntrend over time. For the context condition, this improvement\ncan likely be attributed to an increasing availability of relevant\ninformation on Google over recent years. More surprisingly, the\nno-context condition also displays a similar trajectory of improving\nperformance. One potential explanation is that the ground truth\nlabels of some statements in the dataset may have evolved as\nmore facts emerged. We do not observe any sudden decrease in\naccuracy after the oﬃcial training cutoﬀ for GPT-3.5 and GPT-\n4. This suggests that post-training reﬁnements to the models\nvia reinforcement learning from human feedback (RLHF) may\nintroduce new knowledge, particularly for more recent events\ncloser to the RLHF time period.\n/three.tnum./two.tnum Experiment on the multilingual dataset\nTable 2 shows the accuracy of GPT-3.5 in evaluating the\nveracity of fact-checking claims. The columns Accuracy English\nand Accuracy Multilingual show the percentage of correctly\nclassiﬁed claims by language. English refers to the translations\nof the claims while multilingual refers to the original language.\nThe F1 columns show the F1 score of the models for the\nMultilingual and English condition. We add the F1 score to the\nevaluation to account for the fact that the class distribution diﬀers\nby language.\nIn all languages, except for Thai, we see an increase in the\nF1 score of the model when fact-checking the translated claim as\ncompared to the original. This shows that the models that were\ntested are signiﬁcantly better at predicting the verdict of a statement\nwhen it is presented in English, mirroring prior research indicating\nthat the models struggle to correctly model and classify non-English\nlanguage text. Similarly, the accuracy was signiﬁcantly higher for\nthe English language condition.\nTable 3 shows the accuracy of GPT-3.5 in evaluating the veracity\nof fact-checking claims with additional context provided. Similarly\nto\nTable 2 the English language condition is signiﬁcantly more\naccurate than the Multilingual condition. In all but three languages\n(Portugese, Thai, and Italian), translating increased the F1 score of\nthe model. This diﬀerence was again signiﬁcant.\nFigure 6 summarizes the ﬁndings oﬀered in this section\nand displays a visualization that compares the F1 scores under\nthe two conditions—“No Context” and “Context Retrieval”—\nacross various languages. Each language is represented once on\nthe y-axis, with two corresponding horizontal lines plotted at\ndiﬀerent heights. The blue dashed lines indicate the performance\nunder the “No Context” condition, while the red solid lines\nrepresent the “Context Retrieval” condition. Each line connects\ntwo points, corresponding to the F1 scores achieved when the\nmodel is trained on either English-only or Multilingual data.\nThe circle marker displays the F1 score in the English language\ncondition, while the square portrays the eﬃcacy under the\nmultilingual condition.\nSimilarly, to the PolitiFact experiment, we analyzed the\nperformance of the model over time.\nFigure 7 shows the F1 score\nof all conditions over time. The conﬁdence intervals represent\nthe standard errors and are bootstrapped by sampling with\nreplacement from the dataset and repeatedly calculating the F1\nscore. As in the PolitiFact experiment, we again see an increase\nin the performance of the models over time. Mirroring the\nprevious ﬁndings, we do not see a decrease in the performance\nof the models following the oﬃcial cut-oﬀ date for the training\nof GPT-3.5.\nFrontiers in Artiﬁcial Intelligence /one.tnum/zero.tnum frontiersin.org\nQuelle and Bovet /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/four.tnum/one.tnum/six.tnum/nine.tnum/seven.tnum\nFIGURE /six.tnum\nComparison of all conditions on the Data Commons fact-checking datase t. The x-axis displays the F/one.tnum score of each language and condition.The\ny-axis displays each language for which the model was prompted. Th e blue, dashed line indicates the diﬀerence in the performanc e for the no\ncontext condition. The red, solid line shows the diﬀerence in perf ormance for the context condition. The circles show the F/one.tnum score by language for\nthe translations, the squares show the F/one.tnum score for the original scores.\n/four.tnum Conclusion\nIn this study, we investigated OpenAIs GPT-3.5 and GPT-4s\nability to fact-check claims. We found that GPT-4 outperformed\nGPT-3.5. Notably, even without contextual information, GPT-\n3.5 and GPT-4 demonstrate good performance of 63–75%\naccuracy on average, which further improves to above 80 and\n89% for non-ambiguous verdicts when context is incorporated.\nAnother crucial observation is the dependency on the language\nof the prompt. For non-English fact-checking tasks, translated\nprompts frequently outperformed the originals, even if the\nrelated claims pertained to non-English speaking countries\nand retrieved predominantly non-English information. The\naccuracy of these models varied signiﬁcantly across languages,\nunderscoring the importance of the original language of\nthe claim.\nAs the training data of the GPT models includes vast amounts\nof web data, including a ﬁltered version of the Common Crawl\nCorpus (\nBrown et al., 2020 ), there is a signiﬁcant risk of data leakage\nfor the task of fact-checking. Previous research has shown that\nmisinformation does not exist in isolation, but is repeated (\nShaar\net al., 2020 ) across platforms ( Micallef et al., 2022 ) and languages\n(Kazemi et al., 2022 ; Quelle et al., 2023 ). As misinformation is\nrepeated and re-occurs, the ability of models to retain previously\nfact-checked claims can potentially be seen as a beneﬁt rather\nthan a drawback. Nevertheless, this presents a signiﬁcant risk\nfor the task of fact-checking novel misinformation. We address\nthis in the paper by testing the ability of the models to detect\nmisinformation after the training end date of the models. We\nfound no reduction in performance for fact-checks after the\ntraining end date. It seems that incorporating real-time context\nfor novel misinformation enables the models to reason about\nFrontiers in Artiﬁcial Intelligence /one.tnum/one.tnum frontiersin.org\nQuelle and Bovet /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/four.tnum/one.tnum/six.tnum/nine.tnum/seven.tnum\nFIGURE /seven.tnum\nF/one.tnum score of context(A) and no-context-condition (B) over time. The orange (dashed) line shows the multilingual conditi on. The blue (solid) line\nshowcases the F/one.tnum score for the English condition. The vertical lineindicates the oﬃcial training end date for GPT /three.tnum./five.tnum.\nnovel information. In summary, while data-leakage is a signiﬁcant\nconcern for any application that tests the abilities of LLMs, we\nargue that for the task of fact-checking it does not seem to\ndegrade performance and might even be beneﬁcial in view of\nrecurring misinformation.\nComparing the performance of the models across languages\nis inherently diﬃcult, as the fact-checks are not standardized\nacross languages. Fact-checking organizations diﬀer signiﬁcantly in\ntheir choice of which fact-checks to dedicate time to, with some\nfocussing exclusively on local issues and others researching any\nclaims that are viral on social media. Similarly, some fact-checking\norganizations focus on current claims, while others debunk long-\nstanding misinformation claims. All of these factors inﬂuence the\nability of a large language model to discern the veracity of a claim.\nIt is therefore possible that fact-checks in low-resource languages\nperform better than higher resource languages. The most salient\npoint in the analysis of the multilingual misinformation is that\nthe LLMs outperform the multilingual baseline when prompted\nwith the English translation of the fact-checks. The variability\nin model performance across languages and the improvement of\nthe accuracy when prompted with English language fact-checks\nindicates that the training regimen, in which the distribution of\nlanguages is highly skewed and English is dominant (Common\nCrawl Foundation, 2023)\n/six.tnum, signiﬁcantly impacts accuracy. This\nsuggests that the eﬀectiveness of LLMs in fact-checking is not\nuniformly distributed across languages, likely due to the uneven\nrepresentation of languages in training data.\nOur results suggest that these models cannot completely replace\nhuman fact-checkers as being wrong, even if infrequently, may\n/six.tnum Common Crawl Foundation (/two.tnum/zero.tnum/two.tnum/three.tnum).Statistics of Common Crawl\nMonthly Archives—Distribution of Languages . Available online at: https://\ncommoncrawl.github.io/cc-crawl-statistics/plots/languages.html\nhave devastating implications in today’s information ecosystem.\nTherefore, integrating mechanisms allowing for the veriﬁcation of\ntheir verdict and reasoning is paramount. In particular, they hold\npotential as tools for content moderation and accelerating human\nfact-checkers’ work.\nLooking ahead, it is important to delve deeper into the\nconditions under which large language models excel or falter.\nAs these models gain responsibilities in various high-stakes\ndomains, it is crucial that their factual reliability is well-\nunderstood and that they are deployed judiciously under\nhuman supervision.\nWhile our study concentrated on OpenAI’s GPT-3.5 and GPT-\n4, the rapid evolution of the ﬁeld means newer, possibly ﬁne-tuned,\nLLMs are emerging. One salient advantage of our methodology,\ndistinguishing it from others, is the LLM Agents’ capability to\njustify their conclusions. Future research should explore if, by\ncritically examining the reasons and references provided by the\nLLMs, users can enhance the models’ ability to fact-check claims\neﬀectively.\nData availability statement\nThe raw data supporting the conclusions of this article will be\nmade available by the authors, without undue reservation.\nAuthor contributions\nDQ: Conceptualization, Data curation, Formal\nanalysis, Methodology, Software, Visualization, Writing—\noriginal draft, Writing—review & editing. AB:\nConceptualization, Supervision, Writing—review & editing,\nProject administration.\nFrontiers in Artiﬁcial Intelligence /one.tnum/two.tnum frontiersin.org\nQuelle and Bovet /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/four.tnum/one.tnum/six.tnum/nine.tnum/seven.tnum\nFunding\nThe author(s) declare that no ﬁnancial support was received for\nthe research, authorship, and/or publication of this article.\nAcknowledgments\nThe authors thank Fabrizio Gilardi, Emma Hoes, and Meysam\nAlizadeh for their valuable input that enriched the quality of this\nwork.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those\nof the authors and do not necessarily represent those of\ntheir aﬃliated organizations, or those of the publisher,\nthe editors and the reviewers. Any product that may be\nevaluated in this article, or claim that may be made by\nits manufacturer, is not guaranteed or endorsed by the\npublisher.\nSupplementary material\nThe Supplementary Material for this article can be found\nonline at: https://www.frontiersin.org/articles/10.3389/frai.2024.\n1341697/full#supplementary-material\nReferences\nAdair, B., Li, C., Yang, J., and Yu, C. (2017). “Progress towa rd “the Holy Grail”: the\ncontinued quest to automate fact-checking, ” in Proceedings of the 2017 Computation+\nJournalism Symposium (Evanston, IL).\nAugenstein, I., Lioma, C., Wang, D., Lima, L. C., Hansen, C., Hansen, C., et al.\n(2019). Multifc: a real-world multi-domain dataset for evidenc e-based fact checking\nof claims. ArXiv. doi: 10.18653/v1/D19-1475\nBang, Y., Cahyawijaya, S., Lee, N., Dai, W., Su, D., Wilie, B., et al. (2023). A\nmultitask, multilingual, multimodal evaluation of chatgpt on reas oning, hallucination,\nand interactivity. ArXiv. doi: 10.48550/arXiv.2302.04023\nBovet, A., and Makse, H. A. (2019). Inﬂuence of fake news in Twi tter during the\n2016 US presidential election. Nat. Commun. 10:7. doi: 10.1038/s41467-018-07761-2\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhar iwal, P., et al.\n(2020). Language models are few-shot learners. Adv. Neural Inform. Process. Syst.\n33, 1877–1901. Available online at: https://proceedings.neurips.cc/paper_ﬁles/paper/\n2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html?utm_medium=email&\nutm_source=transaction\nBuck, C., Heaﬁeld, K., and Van Ooyen, B. (2014). N-gram counts and language\nmodels from the common crawl. LREC 2:4. Available online at: https://aclanthology.\norg/L14-1074/\nCaramancion, K. M. (2023). News veriﬁers showdown: a compara tive performance\nevaluation of chatgpt 3.5, chatgpt 4.0, bing ai, and bard in news fact-checking. ArXiv.\ndoi: 10.48550/arXiv.2306.17176\nChase, H. (2022). Langchain. Available online at: https://github.com/hwchase17/\nlangchain; https://github.com/langchain-ai/langchain/blob/master/CITATION.cﬀ\nChoi, E. C., and Ferrara, E. (2023). Automated claim matching with large\nlanguage models: empowering fact-checkers in the ﬁght against misinformation. ArXiv.\ndoi: 10.2139/ssrn.4614239\nChoudhury, A., and Shamszare, H. (2023). Investigating the impact of user trust\non the adoption and use of chatgpt: survey analysis. J. Med. Internet Res. 25:e47184.\ndoi: 10.2196/47184\nCuartielles Saura, R., Ramon Vegas, X., and Pont Sorribes, C. ( 2023). Retraining\nfact-checkers: the emergence of chatgpt in information veri ﬁcation. UPF Digit. Reposit.\n2023:15. doi: 10.3145/epi.2023.sep.15\nDas, A., Liu, H., Kovatchev, V., and Lease, M. (2023). The sta te of human-\ncentered NLP technology for fact-checking. Inform. Process. Manag. 60:103219.\ndoi: 10.1016/j.ipm.2022.103219\nEspejel, J. L., Ettifouri, E. H., Alassan, M. S. Y., Chouham, E. M ., and Dahhane,\nW. (2023). Gpt-3.5 vs. gpt-4: evaluating chatgpt’s reasoning perf ormance in zero-shot\nlearning. ArXiv. doi: 10.48550/arXiv.2305.12477\nFlamino, J., Galeazzi, A., Feldman, S., Macy, M. W., Cross, B., Z hou, Z., et al. (2023).\nPolitical polarization of news media and inﬂuencers on Twitter i n the 2016 and 2020 US\npresidential elections. Nat. Hum. Behav. 7, 904–916. doi: 10.1038/s41562-023-01550-8\nGorrell, G., Kochkina, E., Liakata, M., Aker, A., Zubiaga, A., Bo ntcheva, K., et al.\n(2019). “SemEval-2019 task 7: RumourEval, determining rumou r veracity and support\nfor rumours, ” inProceedings of the 13th International Workshop on Semantic Evaluatio n\n(Minneapolis, MN: Association for Computational Linguistics ), 845–854.\nGraves, L., and Cherubini, F. (2016). “The rise of fact-check ing sites in Europe, ” in\nDigital News Project Report, Reuters Institute for the Study of Journali sm (Oxford).\nGrinberg, N., Joseph, K., Friedland, L., Swire-Thompson, B., an d Lazer, D. (2019).\nFake news on Twitter during the 2016 U.S. presidential electio n. Science 363, 374–378.\ndoi: 10.1126/science.aau2706\nGuo, Z., Schlichtkrull, M., and Vlachos, A. (2021). A survey on aut omated fact-\nchecking. Trans. Assoc. Comput. Linguist. 10, 178–206. doi: 10.1162/tacl_a_00454\nHassan, N., Adair, B., Hamilton, J. T., Li, C., Tremayne, M., Y ang, J., et al. (2015).\n“The quest to automate fact-checking, ” in Proceedings of the 2015 Computation+\nJournalism Symposium. Citeseer.\nHassan, N., Zhang, G., Arslan, F., Caraballo, J., Jimenez, D., Ga wsane, S., et al.\n(2017). Claimbuster: the ﬁrst-ever end-to-end fact-checki ng system. Proc. VLDB\nEndowment 10, 1945–1948. doi: 10.14778/3137765.3137815\nHe, P., Gao, J., and Chen, W. (2021). Debertav3: improving deb erta using\nelectra-style pre-training with gradient-disentangled embed ding sharing. ArXiv.\ndoi: 10.48550/arXiv.2111.09543\nHe, P., Liu, X., Gao, J., and Chen, W. (2020). Deberta: decodi ng-\nenhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654 .\ndoi: 10.48550/arXiv.2006.03654\nHoes, E., Altay, S., and Bermeo, J. (2023). Leveraging chatgpt for eﬃcient fact-\nchecking. PsyArXiv. doi: 10.31234/osf.io/qnjkf\nJiao, W., Wang, W., Huang, J., Wang, X., and Tu, Z. (2023). Is c hatgpt a good\ntranslator? Yes with gpt-4 as the engine. ArXiv. doi: 10.48550/arXiv.2301.08745\nKazemi, A., Li, Z., Pérez-Rosas, V., Hale, S., and Mihalcea, R. (2022). “Matching\ntweets with applicable fact-checks across languages, ” in CEUR Workshop Proceedings\n(Aachen).\nKenton, J. D. M.-W. C., and Toutanova, L. K. (2019). “Bert: pre -training of deep\nbidirectional transformers for language understanding, ” i n Proceedings of naacL-HLT\n(Minneapolis, MN), 2.\nKöhler, J., Shahi, G. K., Struß, J. M., Wiegand, M., Siegel, M., M andl, T., et al. (2022).\n“Overview of the clef-2022 checkthat! lab task 3 on fake news de tection, ” in CEUR\nWorkshop Proceedings (Aachen).\nKotonya, N., and Toni, F. (2020). “Explainable automated fact- checking: a survey, ”\nin Proceedings of the 28th International Conference on Computational Lingui stics\n(Barcelona: International Committee on Computational Lingu istics), 5430–5443.\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., et al. (20 19). Roberta: a\nrobustly optimized bert pretraining approach. ArXiv. doi: 10.48550/arXiv.1907.11692\nMantzarlis, A. (2018). Fact-Checking 101. Paris: UNESCO Publishing Paris.\nMicallef, N., Sandoval-Castañeda, M., Cohen, A., Ahamad, M., K umar, S.,\nand Memon, N. (2022). “Cross-platform multimodal misinformat ion: taxonomy,\ncharacteristics and detection for textual posts and videos, ” in Proceedings of the\nInternational AAAI Conference on Web and Social Media (Washington, DC), 651–662.\nFrontiers in Artiﬁcial Intelligence /one.tnum/three.tnum frontiersin.org\nQuelle and Bovet /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/three.tnum/four.tnum/one.tnum/six.tnum/nine.tnum/seven.tnum\nMisra, R. (2022). Politifact Fact Check Dataset . Available online at: https://\nrishabhmisra.github.io/publications/\nMorris, D. S., Morris, J. S., and Francia, P. L. (2020). A fake news inoculation? fact\ncheckers, partisan identiﬁcation, and the power of misinform ation. Polit. Gr. Ident. 8,\n986–1005. doi: 10.1080/21565503.2020.1803935\nNakov, P., Barrón-Cedeño, A., Da San Martino, G., Alam, F., Kut lu, M., Zaghouani,\nW., et al. (2022a). “Overview of the clef-2022 checkthat! lab ta sk 1 on identifying\nrelevant claims in tweets, ” in CEUR Workshop Proceedings (Aachen).\nNakov, P., Barrón-Cedeño, A., da San Martino, G., Alam, F., Str uß, J. M., Mandl,\nT., et al. (2022b). “Overview of the clef–2022 checkthat! lab on ﬁghting the covid-19\ninfodemic and fake news detection, ” in International Conference of the Cross-Language\nEvaluation Forum for European Languages (Berlin: Springer), 495–520.\nNakov, P., Da San Martino, G., Alam, F., Shaar, S., Mubarak, H., and Babulkov,\nN. (2022c). “Overview of the clef-2022 checkthat! lab task 2 on detecting previously\nfact-checked claims, ” inCEUR Workshop Proceedings (Aachen).\nNyhan, B., and Reiﬂer, J. (2015). Estimating Fact-Checking’s Eﬀects. Arlington, VA:\nAmerican Press Institute .\nPorter, E., and Wood, T. J. (2021). The global eﬀectiveness of fact-checking:\nevidence from simultaneous experiments in Argentina, Nigeri a, South Africa,\nand the United Kingdom. Proc. Natl. Acad. Sci. U. S. A. 118:e2104235118.\ndoi: 10.1073/pnas.2104235118\nQuelle, D., Cheng, C., Bovet, A., and Hale, S. A. (2023). Lost in t ranslation–\nmultilingual misinformation and its evolution. arXiv preprint arXiv:2310.18089 .\ndoi: 10.48550/arXiv.2310.18089\nRashkin, H., Choi, E., Jang, J. Y., Volkova, S., and Choi, Y. (2 017). “Truth\nof varying shades: analyzing language in fake news and political fact-checking, ”\nin Proceedings of the 2017 Conference on Empirical Methods in Natural Langua ge\nProcessing (Stroudsburg, PA), 2931–2937.\nRobertson, S., and Zaragoza, H. (2009). The probabilistic rele vance framework:\nBm25 and beyond. Found. Trends Inform. Retriev. 3, 333–389. doi: 10.1561/15000\n00019\nSawi´nski, M., W˛ ecel, K., Ksi˛ e˙zniak, E. P., Stró ˙zyna, M., Lewoniewski, W., Stolarski,\nP., et al. (2023). “Openfact at checkthat! 2023: head-to-head gpt vs. bert-a comparative\nstudy of transformers language models for the detection of che ck-worthy claims, ” in\nCEUR Workshop Proceedings (Aachen), 3,497.\nShaar, S., Babulkov, N., Da San Martino, G., and Nakov, P. (2020 ). “That is a\nknown lie: detecting previously fact-checked claims, ” in Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics , eds. D. Jurafsky, J. Chai, N.\nSchluter and J. Tetreault (Association for Computational Ling uistics) (Stroudsburg,\nPA), 3607–3618.\nSiwakoti, S., Yadav, K., Bariletto, N., Zanotti, L., Erdogdu , U., and Shapiro, J. N.\n(2021). How COVID Drove the Evolution of Fact-Checking . Harvard Kennedy School\nMisinformation Review, Cambridge, Massachusetts, United States.\nThorne, J., Vlachos, A., Christodoulopoulos, C., and Mittal, A. ( 2018a). “FEVER:\na large-scale dataset for fact extraction and VERiﬁcation, ” i n Proceedings of the 2018\nConference of the North American Chapter of the Association for Computati onal\nLinguistics: Human Language Technologies, Volume 1 (Long Papers) (New Orleans, LA:\nAssociation for Computational Linguistics), 809–819.\nThorne, J., Vlachos, A., Cocarascu, O., Christodoulopoulos, C. , and Mittal, A.\n(2018b). “The fact extraction and VERiﬁcation (FEVER) shar ed task, ” inProceedings of\nthe First Workshop on Fact Extraction and VERiﬁcation (FEVER) (Brussels: Association\nfor Computational Linguistics), 1–9.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). Attention is all you need. Adv. Neural Inform. Process. Syst. 30, 5998–6008.\nWadden, D., Lin, S., Lo, K., Wang, L. L., van Zuylen, M., Cohan, A., et al. (2020).\n“Fact or ﬁction: verifying scientiﬁc claims, ” in Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Processing (EMNLP) (Stroudsburg, PA:\nAssociation for Computational Linguistics), 7534–7550.\nWeikum, G., Dong, L., Razniewski, S., and Suchanek, F. M. (202 0). Machine\nknowledge: creation and curation of comprehensive knowledge b ases. Found. Trends\nDatabases 10, 108–490. doi: 10.1561/1900000064\nWorld Economic Forum (2024). The Global Risk Report . Cologny.\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. , et al.\n(2023). React: synergizing reasoning and acting in language models. ArXiv.\ndoi: 10.48550/arXiv.2210.03629\nZeng, X., Abumansour, A. S., and Zubiaga, A. (2021). Automat ed fact-checking: a\nsurvey. Lang. Linguist. Compass 15:e12438. doi: 10.1111/lnc3.12438\nZhu, W., Liu, H., Dong, Q., Xu, J., Kong, L., Chen, J., et al. (20 23). Multilingual\nmachine translation with large language models: empirical result s and analysis. ArXiv.\ndoi: 10.48550/arXiv.2304.04675\nFrontiers in Artiﬁcial Intelligence /one.tnum/four.tnum frontiersin.org",
  "topic": "Misinformation",
  "concepts": [
    {
      "name": "Misinformation",
      "score": 0.8325591683387756
    },
    {
      "name": "Computer science",
      "score": 0.6470188498497009
    },
    {
      "name": "Context (archaeology)",
      "score": 0.620097815990448
    },
    {
      "name": "Comprehension",
      "score": 0.5349615216255188
    },
    {
      "name": "Phrase",
      "score": 0.4977867603302002
    },
    {
      "name": "Data science",
      "score": 0.3548525869846344
    },
    {
      "name": "Artificial intelligence",
      "score": 0.29494428634643555
    },
    {
      "name": "Computer security",
      "score": 0.2770204544067383
    },
    {
      "name": "Programming language",
      "score": 0.10303884744644165
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I202697423",
      "name": "University of Zurich",
      "country": "CH"
    }
  ],
  "cited_by": 30
}