{
    "title": "COVID-19 automatic diagnosis with CT images using the novel Transformer architecture",
    "url": "https://openalex.org/W3200638744",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A5110803076",
            "name": "Gabriel Sousa Silva Costa",
            "affiliations": [
                "Universidad Filadelfia de México"
            ]
        },
        {
            "id": "https://openalex.org/A4268872953",
            "name": "Anselmo C. Paiva",
            "affiliations": [
                "Universidad Filadelfia de México"
            ]
        },
        {
            "id": "https://openalex.org/A2148891858",
            "name": "Geraldo Braz Júnior",
            "affiliations": [
                "Universidad Filadelfia de México"
            ]
        },
        {
            "id": "https://openalex.org/A3201463438",
            "name": "Marco Melo Ferreira",
            "affiliations": [
                "Universidad Filadelfia de México"
            ]
        },
        {
            "id": "https://openalex.org/A5110803076",
            "name": "Gabriel Sousa Silva Costa",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4268872953",
            "name": "Anselmo C. Paiva",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2148891858",
            "name": "Geraldo Braz Júnior",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3201463438",
            "name": "Marco Melo Ferreira",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6803762308",
        "https://openalex.org/W6687483927",
        "https://openalex.org/W6603394118",
        "https://openalex.org/W3086462707",
        "https://openalex.org/W3021040136",
        "https://openalex.org/W3040660552",
        "https://openalex.org/W3087345531",
        "https://openalex.org/W4309793872",
        "https://openalex.org/W3114166611",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2955425717",
        "https://openalex.org/W3014924062",
        "https://openalex.org/W3015021600",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W4385245566"
    ],
    "abstract": "Even though vaccines are already in use worldwide, the COVID-19 pandemic is far from over, with some countries re-establishing the lockdown state, the virus has taken over 2 million lives until today, being a serious health issue. Although real-time reverse transcription-polymerase chain reaction (RTPCR) is the ﬁrst tool for COVID-19 diagnosis, its high false-negative rate and low sensitivity might delay accurate diagnosis. Therefore, fast COVID-19 diagnosis and quarantine, combined with effective vaccination plans, is crucial for the pandemic to be over as soon as possible. To that end, we propose an intelligent system to classify computed tomography (CT) of lung images between a normal, pneumonia caused by something other than the coronavirus or pneumonia caused by the coronavirus. This paper aims to evaluate a complete selfattention mechanism with a Transformer network to capture COVID-19 pattern over CT images. This approach has reached the state-of-the-art in multiple NLP problems and just recently is being applied for computer vision tasks. We combine vision transformer and performer (linear attention transformers), and also a modiﬁed vision transformer, reaching 96.00% accuracy.",
    "full_text": "COVID-19 automatic diagnosis with CT images using the\nnovel Transformer architecture\nGabriel Sousa Silva Costa1, Anselmo C. Paiva1, Geraldo Braz Junior1,\nMarco Melo Ferreira2\nN´ucleo de computac ¸˜ao aplicada – Universidade Federal do Maranh˜ao (UFMA)\n65.080-805 – S˜ao Lu´ıs – MA – Brasil1\nRodovia MA 201, Km 12, s/n - Pic ¸arreira, S˜ao Jos´e de Ribamar - MA, BR2\nAbstract. Even though vaccines are already in use worldwide, the COVID-19\npandemic is far from over, with some countries re-establishing the lockdown\nstate, the virus has taken over 2 million lives until today, being a serious health\nissue. Although real-time reverse transcription-polymerase chain reaction (RT-\nPCR) is the ﬁrst tool for COVID-19 diagnosis, its high false-negative rate and\nlow sensitivity might delay accurate diagnosis. Therefore, fast COVID-19 diag-\nnosis and quarantine, combined with effective vaccination plans, is crucial for\nthe pandemic to be over as soon as possible. To that end, we propose an intel-\nligent system to classify computed tomography (CT) of lung images between a\nnormal, pneumonia caused by something other than the coronavirus or pneu-\nmonia caused by the coronavirus. This paper aims to evaluate a complete self-\nattention mechanism with a Transformer network to capture COVID-19 pattern\nover CT images. This approach has reached the state-of-the-art in multiple NLP\nproblems and just recently is being applied for computer vision tasks. We com-\nbine vision transformer and performer (linear attention transformers), and also\na modiﬁed vision transformer, reaching 96.00% accuracy.\n1. Introduction\nReal-time reverse transcription polymerase chain reaction (RT-PCR) is the ﬁrst choice for\nCOVID-19 diagnosis, but its high false-negative rate and low sensitivity may delay ac-\ncurate diagnosis [1]. Thus, it is necessary to explore alternative diagnosis methods. For-\ntunately, the chest’s x-ray and CT scans show signals that differentiate a normal healthy\nthorax from an unhealthy one. With overloaded healthcare systems all around the world,\nit is important to quickly assess if incoming patients are infected with COVID-19 or not,\ngiving priority to treating patients who are more vulnerable and are infected with the virus.\nWith that in mind, a fast, low-sensitivity and precise diagnosis tool can be very helpful\nto radiologists to distinguish between patients infected and not infected with the virus.\nThe majority of recently published works applies Convolutional Neural Networks(CNN)\narchitectures [5, 6, 7] for this goal.\nRecent works on CT images classiﬁcation use a tailored deep convolutional net-\nwork [5] for the problem of classifying medical images. On the same problem, but with\nchest x-ray, images are also classiﬁed using CNNs, using techniques such as capsule\nbased-networks [6] or plain deep convolutional neural networks [7]. A deep features ap-\nproach with the Q-deformed entropy handcrafted features was evaluated in [13] reaching\naccuracy of 99.68%. A DenseNet201 with transfer learning was used to train a model\nto classify CT chest images between covid and non covid in [14] while comparing with\nother CNN pretrained models such as VGG16, ResNet152V2 and Inception-ResNetV2\nwhich reach an overall accuracy of 96.25% on the test set. A tailored convolutional neu-\nral network, made speciﬁcally for COVID-19 detection, reaches 99.1% accuracy on the\ntest set in [5].\nWe propose a method for COVID-19 diagnosis using CT scans from patients, us-\ning complete self-attention networks: Vision Transformer(ViT) [3] and some of its vari-\nations(detailed on Section 2.2). Transformers are mainly used for Neural Language Pro-\ncessing problems but we use it as a low-sensitivity and high precision diagnosis tool to\nclassify a given patient chest tomography between a healthy one, pneumonia caused by\nsomething other than the corona virus or pneumonia caused by the virus.\nThe paper is organized as follows. Section 2 describes the proposed methods.\nSection 3 discusses our ﬁndings, followed by Section 4 concludes the work.\n2. Proposed Method\nIn this work, we focused on diagnosis of COVID-19 using CT images with a Transformer\nNetwork. For that, we propose a method illustrated on Figure 1, that trains and evaluates\nwith some of the Transformer variants for the image classiﬁcation problem in order to\nperform neural architecture estimation.\nFigure 1. Proposed method\nThe ﬁrst step is to preprocess our data. The data is then fed into one of the\nproposed Vision Transformer variations (Vision Transformer (ViT), Data-efﬁcient Im-\nage Transformer (DeIT)). The Encoder’s output is then used by the trainable MLP Head,\nwhich results in the classiﬁcation of the image between the three classes we are using to\nevaluate our model. Each of these steps is explained in following sections.\nTable 1. COVIDNet-CT Dataset: Images distribution\nType Normal Other Pneumonia COVID-19 Pneumonia Total\nTraining 27201 22061 12520 61782\nValidation 9107 7400 4529 21036\nTest 9450 7395 4346 21191\n2.1. Data Acquisition and Preprocessing\nWe utilized the COVIDNet-CT [12] dataset that have training, testing and validation sets\ndeﬁned in Table 1.\nOur preprocessing consists on loading each set and applying its respective data\naugmentation: for the training set, we apply a horizontal and vertical random ﬂip, image\nrotation ranging from -30 to 30 degrees and a random brightness variation, ranging from\n-0.2 to 0.2. For every set, we apply a resizing according to the network required input we\nusing, (512x512 for ViT and 384x384 for DeIT) and data normalization.\n2.2. Vision Transformer\nIn our work, we use Vision Transformer as a feature extractor to classify CT images\nbetween healthy, pneumonia due to infection with the new Corona Virus or pneumonia\ndue to other kinds of infection. We also compare between different implementations of\nthe network.\nVision Transformer(ViT) [3] is a network proposed by Google’s Research team,\ncreated to assess the classical Transformer’s [2] limitation, which is used mainly for Neu-\nral Language Processing problems: its prohibitive large memory usage for images. ViT\nshowed promising results on the Image Net dataset, surpassing top-classifying solutions.\nThe big picture of the proposed method can be seen on Figure 1.\nViT’s[3] main idea is to make the Transformer architecture feasible for images\nwith minimal modiﬁcation from the original work [2], so the architecture process an im-\nage as if they were a sequence of input tokens, where a ﬁxed-size image is decomposed\ninto N patches of ﬁxed size and each patch is projected with a linear layer, generating\nembeddings. The embeddings are then concatenated with a class token, a trainable vec-\ntor that is lately projected with a linear layer to predict each class. Given the fact that a\nTransformer block is invariant to the order of the patches(a desirable feature for images,\ngiven its 2D structures), positional information is incorporated as positional embeddings,\nwhich are summed to the embeddings, and then fed to the Transformer blocks. Figure 2\ndepicts a detailed scheme of the Vision Transformer.\nThe main ViT concept is its self-attention mechanism, or the more elaborate Multi-\nhead self-attention(MSA). Attention mechanism is based on a trainable associative mem-\nory between key and value vector pairs, where a query vector q ∈Rd is matched against\na set of k key vectors K ∈Rk×d using inner products, which are then scaled and normal-\nized with a softmax function to obtain weights. The output is the weighted sum of a set\nof k value vectors V ∈Rk×d for a sequence of N query vectors Q ∈RN×d, producing\nthe output matrix of size N ×d:\nAttention(Q, K, V) =Softmax (QKT /\n√\nd)V (1)\nFigure 2. The Vision Transformer\nFinally, the MSA is deﬁned by considering h attention heads, h self-attention\nfunctions applied to the input, where each head provides a sequence of N ×d and these\nh sequences are rearranged into a N ×dh sequence re-projected by a linear layer into\nN ×D.\nA ViT is composed by multiple trainable Encoder blocks (see Figure 3), where\nthe ﬁrst one receives as inputs the embedded image patches, and its outputs is fed into\nthe next Encoder block and so on, until the last block’s output is fed into a MLP head,\nwhich then classiﬁes an image. Each one of these Encoder blocks is composed of two\nBatch Normalization layers and two residual layers, working together with the MSA and\na MLP, as depicted on Figure 3.\nThe Vision Transformer (or any Transformer architecture) can take any kind of En-\ncoder on its internal architecture. This ﬂexibility made possible for researchers to come up\nwith more efﬁcient Encoder blocks, and therefore speeding up and occupying less mem-\nory. While the classic Encoder, used on the original Transformer paper [2] is feasible to be\nused with the ViT, we can still make it more efﬁcient, and for that, we also experimented\nViT with the Performer [4] encoder, an Encoder that tries to approximate the original one\nwhile maintaining linear time and space complexity. It consists of a novel approach called\nFast Attention Via positive Orthogonal Random features(FA VOR+), which approximates\nsoftmax attention-kernels, allowing us to train our model faster with the same amount of\nparameters we used with a regular Transformer encoder.\nFor our last and most successful experiment, we used a pre-trained variation of\nthe ViT, the Data-efﬁcient image Transformer, or DeIT [11]. It aims to solve the large\npre-training dataset requirement that the base ViT architecture has to deal with, due to its\nlack of inductive bias for 2D structures(images) by using the same ViT but using trained\nCNN architectures so that the ViT can inherit its knowledge and thus understand images\na bit better, requiring less pre-train data because the Transformer network already has an\ninductive bias injected by the teacher CNN architecture.\nWe train Vision Transformer and Performer without transfer learning. For DeIT,\nwe used the pre-trained weights from the original paper. For every one of these varia-\ntions, we train with batches of 32 images per batch, resize the images to the required\nFigure 3. An encoder block\nTable 2. Models Hyperparameteres\nHyperparameter ViT w/ Trans-\nformer Encoder\nViT w/ Performer\nEncoder\nDeIT Base\nLearning Rate 6e-5 3e-5 8e-4\nDropout 0.5 0.5 0.5\nWeight Decay 0 1e-4 1e-4\nViT’s Patch Size 16 32 16\nNumber of Epochs 60 60 80\ninput size for each model (512x512 for Classical encoder ViT and Performer encoder ViT\nand 384x384 for DeIT) and apply some image augmentation techniques like randomly\napplying vertical and horizontal ﬂips, random rotation and brightness variation. We also\nused an ADAM Optimizer and a linear learning rate scheduler for each model. For every\nepoch, we take the output and assess performance on the validation set, using the metrics\ndescribed on Section 3, and at the end of training, we use the test set for the ﬁnal eval-\nuation. The main hyperparameters we used are described for each one of the models on\nTable 2.\n3. Results\nThis sections focuses on the discussion of the results of our method, comparing its per-\nformance with other papers and then we discuss our achievements.\nThe Transformer block’s output is fed into a simple MLP with one hidden layer,\nand is then translated into probabilities through a Softmax and then predicted into one of\nTable 3. ViT with Classic Transformer Encoder(Accuracy: 0.91)\nClass Precision Recall F1-Score\nNormal 0.95 0.98 0.97\nOther Pneumonia 0.91 0.82 0.86\nCovid-19 Pneumonia 0.81 0.88 0.84\nMacro Average 0.89 0.89 0.89\nWeighted Average 0.91 0.91 0.90\nTable 4. ViT with Efﬁcient Performer Encoder(Accuracy: 0.91)\nClass Precision Recall F1-Score\nNormal 0.96 0.99 0.98\nOther Pneumonia 0.84 0.92 0.88\nCovid-19 Pneumonia 0.92 0.72 0.81\nMacro Average 0.91 0.88 0.89\nWeighted Average 0.91 0.91 0.91\nthe three classes. At the end of each training epoch, we assess the training performance\non a validation set, and at the end of training, we evaluate our network on the test set.\nFor measuring our models performances, we used classic metrics for classiﬁcation\nproblems: accuracy, recall, precision and f1-score measured for each class.\nAccuracy = True Positive + True Negative\nTotal (2)\nPrecision = True Positive\nTrue Positive + False Positive (3)\nRecall = True Positive\nTrue Positive + False Negative (4)\nF1 Score = 2· Precision ·Recall\nPrecision + Recall (5)\nTo address class imbalance, for recall, precision and f1-score, we also calculate its\nmacro average, the unweighted mean for each one of the labels, and the weighted average,\nwhere it is calculated the weighted average.\nOur Results on the test set, for 21191 CT images are detailed on Table 3,4, and 6,\nwhere we present precision and recall by class, macro and weighted average for each one\nof the tested model: ViT with Classic Transformer Encoder, ViT with Efﬁcient Performer\nEncoder and Data efﬁcient Vision Transformer, respectively.\nWe also compare with other works, with respect to accuracy. This compari-\nson is shown on table 6. DenseNet201 based deep transfer learning only classify be-\ntween COVID/NON-COVID, while the rest of the networks on the table classify between\nHealthy, Pneumonia, and pneumonia caused by COVID-19.\nWe obtained promising results, with the best one being from the pre-trained DeIT.\nBoth ViT networks performed very well. With necessary pre-training, they are likely to\nTable 5. Data efﬁcient Vision Transformer(Accuracy: 0.96)\nClass Precision Recall F1-Score\nNormal 0.98 0.99 0.99\nOther Pneumonia 0.96 0.93 0.95\nCovid-19 Pneumonia 0.92 0.94 0.93\nMacro Average 0.96 0.96 0.96\nWeighted Average 0.96 0.96 0.96\nTable 6. Overall comparison with other works\nNetwork Accuracy\nDenseNet201 based deep transfer learning[14] 0.96\nQ-Deformed Entropy[13] 0.99\nCovidNET-CT[5] 0.99\nViT with Classic Transformer Encoder 0.91\nViT with Efﬁcient Performer Encoder 0.91\nData efﬁcient Vision Transformer 0.96\nsurpass DeIT’s performance, as shown on ImageNet’s top-performing networks1.\nAs observed in the overall results (Table 6), even though our accuracy is smaller\nthan every other work, we obtained competitive results for a novel neural network that\nwas initially meant for NLP problems, showing that the ViT modiﬁcation can be used for\ngeneral purpose computer vision problems with none to minimal modiﬁcation of the orig-\ninal network, while other works were tailored speciﬁcally for the problem of classifying\nCOVID-19 images.\nOur metrics for COVID-19 classiﬁcation were a little worse than the others, with\nhealthy image classiﬁcations showing almost 100% f1-score, which is expected, due to\nclass imbalance.\nWe can also observe that the difference between a ViT with a classical transformer\nencoder and one with an efﬁcient performer encoder is minimal, meaning that the Per-\nformer encoder does approximate well the Transformer encoder, as stated on [4], while\nrequiring less computing resources. The work we’ve done can be easily extended and\nﬁne-tuned for new kinds of viruses that may appear, resulting in a fast and precise low-\nsensitivity diagnosis tool. As the computation power available was limited, we didn’t do\nthe experiments using the original, pre-trained, top-performing Vision Transformer, ViT-\nHuge(ViT-H). Instead, we obtained a custom implementation of the network and tuned its\nhyperparameters to extract its potential.\n4. Conclusion\nThis work proposed the use of the transformer architecture for medical image classiﬁ-\ncation. Even though COVID-19 diagnosis can be made with more traditional methods,\ncomputed tomography can still be a signiﬁcant ﬁnancial or structural obstacle for some\nparts of the society, but this type of equipment is getting more and more accessible for\n1Benchmark on ImageNet at https://paperswithcode.com/sota/image-classiﬁcation-on-imagenet\npeople. To extract the most of this technology, we can use it to help radiologists ﬁlter\nhigher-risk patients who show signs of pneumonia caused by COVID-19 or some other\ndangerous virus.\nWe compared important metrics between different Deep Learning approaches, ex-\nploring the novel transformer and comparing it with classical CNNs. We demonstrate that\nmodest, pre-trained models like DeIT can achieve competitive results, and the original Vi-\nsion Transformer, even without pre-training and no tailoring for the task at hand can also\nachieve some good results. We hope our paper motivates other researchers to explore the\nTransformer Architecture for images and medical diagnosis.\nFor future work, we propose the use of pre-trained weights from Vision Trans-\nformer, but with a Performer Encoder, making the training faster and less computationally\nexpensive.\n5. Acknowledgements\nThe authors acknowledge the Brazilian institutions: Coordenac ¸˜ao de Aperfeic ¸oamento de\nPessoal de N´ıvel Superior (CAPES), Conselho Nacional de Desenvolvimento Cient´ıﬁco\ne Tecnol ´ogico (CNPq), and Fundac ¸˜ao de Amparo `a Pesquisa e ao Desenvolvimento\nCient´ıﬁco e Tecnol´ogico do Maranh˜ao (FAPEMA) for the ﬁnancial support.\nReferences\n[1] Alsharif and A. Qurashi, Effectiveness of COVID-19 diagnosis and management tools: A\nreview, Radiography, 10.1016/j.radi.2020.09.010\n[2] Vaswani, Ashish, et al.: Attention is all you need. Advances in neural information pro-\ncessing systems. 2017. https://arxiv.org/abs/1706.03762\n[3] Dosovitskiy, Alexey, et al.: An image is worth 16x16 words: Transform-\ners for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020).\nhttps://arxiv.org/abs/2010.11929\n[4] Choromanski, Krzysztof, et al.: Rethinking attention with performers. arXiv preprint\narXiv:2009.14794 (2020). https://arxiv.org/abs/2009.14794\n[5] Gunraj, Hayden, Linda Wang, and Alexander Wong.: Covidnet-ct: A tailored deep convo-\nlutional neural network design for detection of covid-19 cases from chest ct images.\nFrontiers in Medicine 7 (2020). https://arxiv.org/abs/2009.05383\n[6] Afshar, Parnian, et al.: Covid-caps: A capsule network-based framework for identiﬁcation\nof covid-19 cases from x-ray images. https://arxiv.org/abs/2004.02696\n[7] Abbas, Asmaa, Mohammed M. Abdelsamea, and Mohamed Medhat Gaber.: Classiﬁca-\ntion of COVID-19 in chest X-ray images using DeTraC deep convolutional neural\nnetwork. (2020). https://arxiv.org/abs/2003.13815\n[8] He, Kaiming, et al.: Deep residual learning for image recognition. Proceedings\nof the IEEE conference on computer vision and pattern recognition. 2016.\nhttps://arxiv.org/abs/1512.03385\n[9] Tan, Mingxing, and Quoc V . Le.: Efﬁcientnet: Rethinking model scaling\nfor convolutional neural networks. arXiv preprint arXiv:1905.11946 (2019).\nhttps://arxiv.org/abs/1905.11946\n[10] Silva, Pedro, et al.: COVID-19 detection in CT images with\ndeep learning: A voting-based scheme and cross-datasets anal-\nysis. Informatics in Medicine Unlocked 20 (2020): 100427.\nhttps://www.sciencedirect.com/science/article/pii/S2352914820305773\n[11] Touvron, Hugo, et al.: Training data-efﬁcient image transformers & distillation through\nattention. arXiv preprint arXiv:2012.12877 (2020). https://arxiv.org/abs/2012.12877\n[12] Gunraj, Hayden, et al.: COVIDNet-CT: A COVID-19 CT Dataset.\nhttps://github.com/haydengunraj/COVIDNet-CT\n[13] Hasan, A.M.; AL-Jawad, M.M.; Jalab, H.A.; Shaiba, H.; Ibrahim, R.W.; AL-Shamasneh\nA.R.: Classiﬁcation of Covid-19 Coronavirus, Pneumonia and Healthy Lungs in CT\nScans Using Q-Deformed Entropy and Deep Learning Features. Entropy 2020, 22,\n517. https://doi.org/10.3390/e22050517\n[14] Aayush Jaiswal, Neha Gianchandani, Dilbag Singh, Vijay Kumar Manjit Kaur\n(2020) Classiﬁcation of the COVID-19 infected patients using DenseNet201 based\ndeep transfer learning, Journal of Biomolecular Structure and Dynamics, DOI:\n10.1080/07391102.2020.1788642"
}