{
  "title": "An automated framework for assessing how well LLMs cite relevant medical references",
  "url": "https://openalex.org/W4409505176",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2072861797",
      "name": "Kevin Wu",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A1989318318",
      "name": "Eric Wu",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2033788510",
      "name": "Kevin Wei",
      "affiliations": [
        "Keck Hospital of USC"
      ]
    },
    {
      "id": "https://openalex.org/A2182129408",
      "name": "Angela Zhang",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A5117176857",
      "name": "Allison Casasola",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2120119196",
      "name": "Teresa Nguyen",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A5093875917",
      "name": "Sith Riantawan",
      "affiliations": [
        "Keck Hospital of USC"
      ]
    },
    {
      "id": "https://openalex.org/A2503482798",
      "name": "Patricia Shi",
      "affiliations": [
        "Loma Linda University"
      ]
    },
    {
      "id": "https://openalex.org/A2123842474",
      "name": "Daniel Ho",
      "affiliations": [
        "Stanford Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A2207784945",
      "name": "James Zou",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2072861797",
      "name": "Kevin Wu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1989318318",
      "name": "Eric Wu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2033788510",
      "name": "Kevin Wei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2182129408",
      "name": "Angela Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5117176857",
      "name": "Allison Casasola",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2120119196",
      "name": "Teresa Nguyen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5093875917",
      "name": "Sith Riantawan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2503482798",
      "name": "Patricia Shi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2123842474",
      "name": "Daniel Ho",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2207784945",
      "name": "James Zou",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4382678522",
    "https://openalex.org/W4387232979",
    "https://openalex.org/W4406152279",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4390581979",
    "https://openalex.org/W4381550225",
    "https://openalex.org/W4390114552",
    "https://openalex.org/W4389519219",
    "https://openalex.org/W6860318346",
    "https://openalex.org/W4388196756",
    "https://openalex.org/W4387772689",
    "https://openalex.org/W4389795250",
    "https://openalex.org/W4407772451",
    "https://openalex.org/W4380319827",
    "https://openalex.org/W4402670800",
    "https://openalex.org/W4381432831",
    "https://openalex.org/W4389520670",
    "https://openalex.org/W4226227340",
    "https://openalex.org/W4401042427",
    "https://openalex.org/W4311731003",
    "https://openalex.org/W4221164017",
    "https://openalex.org/W4389519598",
    "https://openalex.org/W4389518925",
    "https://openalex.org/W4388955808",
    "https://openalex.org/W4389518954",
    "https://openalex.org/W4361000349",
    "https://openalex.org/W4386910356",
    "https://openalex.org/W4400047099",
    "https://openalex.org/W4384561103",
    "https://openalex.org/W3162922479",
    "https://openalex.org/W2970482702"
  ],
  "abstract": "As large language models (LLMs) are increasingly used to address health-related queries, it is crucial that they support their conclusions with credible references. While models can cite sources, the extent to which these support claims remains unclear. To address this gap, we introduce SourceCheckup, an automated agent-based pipeline that evaluates the relevance and supportiveness of sources in LLM responses. We evaluate seven popular LLMs on a dataset of 800 questions and 58,000 pairs of statements and sources on data that represent common medical queries. Our findings reveal that between 50% and 90% of LLM responses are not fully supported, and sometimes contradicted, by the sources they cite. Even for GPT-4o with Web Search, approximately 30% of individual statements are unsupported, and nearly half of its responses are not fully supported. Independent assessments by doctors further validate these results. Our research underscores significant limitations in current LLMs to produce trustworthy medical references.",
  "full_text": "Article https://doi.org/10.1038/s41467-025-58551-6\nAn automated framework for assessing how\nwell LLMs cite relevant medical references\nKevin Wu1,9,E r i cW u2,9,K e v i nW e i3, Angela Zhang4, Allison Casasola5,\nTeresa Nguyen6, Sith Riantawan3,P a t r i c i aS h i7,D a n i e lH o8 & James Zou1,2,5\nAs large language models (LLMs) are increasingly used to address health-\nrelated queries, it is crucial that theysupport their conclusions with credible\nreferences. While models can cite sources, the extent to which these support\nclaims remains unclear. To address this gap, we introduceSourceCheckup,a n\nautomated agent-based pipeline thatevaluates the relevance and suppor-\ntiveness of sources in LLM responses. We evaluate seven popular LLMs on a\ndataset of 800 questions and 58,000 pairs of statements and sources on data\nthat represent common medical queries. Ourﬁndings reveal that between 50%\nand 90% of LLM responses are not fullysupported, and sometimes contra-\ndicted, by the sources they cite. Evenfor GPT-4o with Web Search, approxi-\nmately 30% of individual statements are unsupported, and nearly half of its\nresponses are not fully supported. Independent assessments by doctors fur-\nther validate these results. Our research underscores signiﬁcant limitations in\ncurrent LLMs to produce trustworthy medical references.\nL a r g el a n g u a g em o d e l s( L L M s )a r ei n c r e a s i n g l yc o n s i d e r e df o ru s e\nin healthcare. Although no commercially available LLMs are cur-\nrently approved by the FDA for use in medical decision support\nsettings\n1, top-performing LLMs like GPT-4o, Claude, and Med-\nPaLM have nonetheless demonstrated superior performance over\nclinicians on medical exams like the US Medical Licensing Exam\n(USMLE)\n2– 4. LLMs have already made their way into patient care\ntoday, from being used as chatbots for mental health therapy5,6 to\nusers ﬁnding diagnoses for uncommon diseases that physicians\nmissed7. A growing number of clinicians report using LLMs in their\nclinical practice or education8,9.\nHowever, LLMs are prone to hallucination, where the model\ngenerates statements not backed by any source10– 12. Particularly in the\nmedical domain, this can erode user trust and potentially harm\npatients by providing erroneous advice\n13,14 or discriminating based on\npatient backgrounds15. Lack of trust is commonly cited as the number\none deterrent against clinicians adopting LLMs in their clinical\npractice\n16,17, and in particular, the inability of LLMs to generate sup-\nporting sources for medical statements in their responses18.\nT h en e e dt oc i t et h es o u r c e sf o rm e d i c a ls t a t e m e n t sg o e sb e y o n d\ngaining clinician and patient trust -- there is also an urgent regulatory\ncase as well\n19. The US Food and Drug Administration (FDA) has\nrepeatedly called for regulating LLMs used as decision support\ntools\n20,21. Assessing the degree to which LLMs reliably convey existing,\ntrustworthy medical knowledge is important for informing future\nregulatory frameworks regarding medical LLMs.\nLLMs should be capable of reliably providing relevant sources to\nallow users and regulators to audit the reliability of their statements.\nRecent advancements in LLM capabilities (e.g., improved instruction\nﬁne-tuning) have enabled models to routinely provide sources upon\nrequest. Retrieval augmented generation (RAG), in particular, allows\nmodels to perform real-time searches for web references relevant to\nthe query. However, even if the references are from valid and legit-\nimate websites, it is still unclear the extent to which these provided\nsources contain content that actually supports the claims made in the\nmodel’s generated responses.\nTo address these important challenges, this paper makes the\nfollowing contributions. First, given the costly nature of high-quality\nReceived: 30 September 2024\nAccepted: 26 March 2025\nCheck for updates\n1Department of Biomedical Data Science, Stanford University, Stanford, CA, USA.2Department of Electrical Engineering, Stanford University, Stanford, CA,\nUSA. 3Keck Medicine of USC, Los Angeles, CA, USA.4Department of Genetics, Stanford University, Stanford, CA, USA.5Department of Computer Science,\nStanford University, Stanford, CA, USA.6Department of Anesthesiology, Stanford University, Stanford, CA, USA.7Loma Linda University School of Medicine,\nLoma Linda, CA, USA.8Stanford Law School, Stanford, CA, USA.9These authors contributed equally: Kevin Wu, Eric Wu.e-mail: jamesz@stanford.edu\nNature Communications|         (2025) 16:3615 1\n1234567890():,;\n1234567890():,;\nmedical expert annotations and the rapid pace of ongoing develop-\nment in LLMs, we propose an automated evaluation framework, called\nSourceCheckup, to create medical questions and to score how well\nLLMs can provide relevant sources in their answers to these questions.\nWe verify that this framework is highly accurate,ﬁnding 89% agree-\nment with a consensus of three US-licensed medical experts and a\nhigher rate of agreement than any pair of medical experts. Second, we\nevaluate top-performing, commercially available LLMs (GPT-4o (RAG\nand API), Claude v2.1, Mistral Medium, and Gemini (RAG and API)) and\nﬁnd that models without access to the Web only produce valid URLs\nbetween 40% to 70% of the time. Retrieval-augmented generation\n(RAG)-enabled GPT-4o and Gemini Ultra 1.0, which have search engine\naccess, do not suffer from URL hallucination, but still fail to produce\nreferences that support all the statements in the response nearly half\nof the time. Finally, we open source our dataset of 800 medical\nquestions created from webpages pulled from the Mayo Clinic and\nReddit, as well as a clinician-annotated subset of 400 question/answer\npairs. Ourﬁndings highlight an important gap in the viability of LLMs\nfor clinical medicine and have crucial implications for the medical\nadoption of LLMs.\nRelated works\nThere is a growing body of work on measuring and improving source\nattribution in language models\n22– 24. Benchmark datasets introduced in\nworks such as WebGPT25,E x p e r t Q A26,W e b C P M27,a n dH A G R I D28\naggregate open-domain subjects from web pages like Wikipedia in a\nquestion-answer format. However, the evaluations of these datasets\nwere performed by manual human veriﬁcation, which can be costly\nand time-intensive29 and difﬁcult to replicate.\nThere have been several works recently that have demonstrated\nthe usefulness of using language models themselves in automatically\nscoring source attribution from LLMs. For instance, ALCE\n24,\nAttributedQA30,a n dG o p h e r C i t e31 use supervised language models to\nperform automated evaluation of LLMs. More relevantly, given the\nadvent of powerful instruction-ﬁne-tuned LLMs, FactScore\n32 and\nAttrScore33 demonstrate that ChatGPT can be used as a useful eva-\nluator of source attribution, but ChatGPT itself performs poorly when\nevaluated for source attribution34,35.\nOur proposed method makes three contributions. First, we con-\nstruct a dedicated corpus of medical-speciﬁc statement-source pairs\n(nearly 58 K examples from over 800 reference documents). Second,\nwe provide evidence that GPT-4o is a highly effective evaluator of\nsource attribution in the medical domain by showing strong agree-\nment with a panel of three US-licensed medical doctors. Third, we use\nour automated framework to evaluate seven state-of-the-art, com-\nmercially available LLMs commonly used by patients and\nclinicians today.\nResults\nQuestion generation and response parsing\nBoth medical doctors tasked with verifying the generated questions\nfound that 100/100 of a random sample of generated questions are\naligned with the reference document and can be answered. For\nstatement parsing, theﬁrst and second doctors found 330/330 and\n329/330 of parsed statements were correctly contained in the full\nresponse, respectively. Conversely, they found 6 and 5 (out of 72 total)\nfull responses where a single statement was not parsed, respectively.\nSource veriﬁcation\nOur expert annotation of 400 statement-source pairings revealed that\nthe Source Veriﬁc a t i o nm o d e lp e r f o r m sa sw e l la se x p e r t sa td e t e r -\nmining whether a source supports a statement. We observed an 88.7%\nagreement between the Source Veriﬁcation model and the doctor\nconsensus and an 86.1% average inter-doctor agreement rate (Fig.1a,\nSupplementary Table 1, Supplementary Fig., 1). We found no\nstatistically signiﬁcant difference between the doctor consensus\nannotations and Source Veriﬁcation model annotations (p =0 . 2 1 ,\nunpaired sample two-sided t-test).\nEvaluation of bias of GPT-4o as the backbone LLM\nAs an evaluation agent, Claude Sonnet 3.5 agrees with the human\nexperts’consensus 87.0% (83.4– 90.4 95% CI) of the time, which is not\nstatistically different than GPT-4o’s 88.7% agreement with experts\n(p = 0.52, paired two-sided t-test). Moreover, we observed a 90.1%\n(89.7– 90.5 95% CI) agreement between Claude Sonnet 3.5 and GPT-4o\non source veriﬁcation decisions. Additionally, we performed a chi-\nsquared test and found no statistically signiﬁcant difference between\nusing Claude or GPT-4o as the question generator or response parser\n(p = 0.801, paired two-sided t-test) on downstream statement support\nmetrics (Supplementary Tables 2 and 3). Theseﬁndings indicate that\nour evaluation pipeline is not biased towards GPT-4o and can be\neffectively adapted to other high-performing LLMs. We also evaluate\nLlama 3.1 70B on the task of citation veriﬁcation and report 79.3%\n(75.4– 83.1 95% CI) concordance with human expert consensus. As\nsuch, weﬁnd that the open-source model is not yet on-par with top\nproprietary models for producing expert-level citation veriﬁcation.\nEvaluation of source veracity in LLMs\nOur full results of these three metrics across seven models are found in\nFig. 1b and Supplementary Table 4. We found that GPT-4o (RAG) is the\nhighest-performing model in terms of providing citations, mainly dri-\nven by its unique ability among the models to have access to the\ninternet via search. However, we still found that its response-level\nsupport is only 55%. We provide examples of failures from GPT-4o\n(RAG) in Supplementary Fig. 2, where one statement is not found due\nto it not being mentioned, and another is actually contradicted by a\nprovided source. Similarly, only 34.5% of Gemini Ultra 1.0 with RAG’s\nresponses are fully supported by the retrieved references. Addition-\nally, the other API-endpoint models all had much lower rates across the\nboard, largely due to the fact that they do not have access to the web.\nFor example, GPT-4o (API), the currently best-performing LLM\n36,o n l y\nproduced valid URLs around 70% of the time. On the other end, we\nfound that Gemini Pro’s API only produced fully supported responses\nabout 10% of the time. Additionally, we found that open source models\nLlama-2-70b and Meditron-7b both were unable to consistently com-\nplete the initial task of producing citation URLs at all ( <5% for Llama\nand <1% for Meditron). As such, we did not include them in the main\nresults.\nAs additional human expert validation, we randomly sampled 110\nstatement-source pairs produced by GPT-4o (RAG) that have been\ncategorized by the Source Veriﬁcation model as unsupported by any of\nthe sources provided and had doctors assess each pair. The doctors\nagree with the Source Veriﬁer 95.8% (91.8– 98.7%) of the time. Among\nthe 110 statement-source pairs provided by GPT-4o (RAG), the doctors\nconﬁrmed that 105 statements are not supported by any source pro-\nvided by GPT-4o (RAG). This result shows that retrieval augmentation\nby itself is not a silver bullet solution for making LLMs more factually\naccountable. In particular, while the four API-endpoint models pro-\nduced sources (either valid or invalid) in >99% of responses when\nprompted, weﬁnd that GPT-4o (RAG) fails to produce sources in over\n20% of responses, even when explicitly prompted to do so (Supple-\nmentary Fig. 2), partially contributing to its low response-level sup-\nport. Overall, the average length of a cited URL source was 6905\ntokens, with 99.9% of the cited URL sources falling within the context\nsize limit for GPT-4o of 128k tokens.\nBreakdown by question source\nWe ask whether the type of question affects the quality of sources\nprovided by LLMs. We found that the question source signiﬁcantly\naffected every model’s ability to produce supporting sources (detailed\nArticle https://doi.org/10.1038/s41467-025-58551-6\nNature Communications|         (2025) 16:3615 2\nFig. 1 | Validation ofSourceCheckupagainst doctors and evaluation of LLMs\nusing SourceCheckup.a Agreement between the Source Veriﬁcation model and\ndoctors on the task of source veriﬁcation. We asked three medical doctors (D1, D2,\nand D3) to determine whether pairs of statements and source texts are supported\nor unsupported. We found that the Source Veriﬁcation model has a higher agree-\nment with the doctor consensus than the average agreement between doctors. The\n95% conﬁdence intervals are computed with the bootstrap method and are shown\nin the error bars, and the total sample size isN =4 0 0 .b Evaluation of the quality of\nsource veriﬁcation in LLMs on medical queries. Each model is evaluated on three\nmetrics. Source URL Validitymeasures the proportion of generated URLs that\nreturn a valid webpage.Statement-level Supportmeasures the percentage of\nstatements that are supported by at least one source in the same response.\nResponse-level Supportmeasures the percentage of responses that have all their\nstatements supported. Full numerical results are displayed in Supplementary\nTable 4. The 95% conﬁdence intervals are computed with the bootstrap method\nand are shown in the error bars. The sample sizes used to compute each statistic are\nfound in Supplementary Table 8.\nArticle https://doi.org/10.1038/s41467-025-58551-6\nNature Communications|         (2025) 16:3615 3\nin Supplementary Fig. 3). For example, while the response-level sup-\nport for questions from MayoClinic is close to 80% for GPT-4o (RAG),\nthis drops precipitously to around 30% on Reddit r/AskDocs (p < 0.001\nusing unpaired sample t-test). Whereas questions from MayoClinic can\nbe more directly answered from single sources, the Reddit r/AskDocs\nquestions are more open-ended and often require pulling sources\nfrom a wide variety of domains.\nAdditional validation on HealthSearchQA\nIn addition to the real-world questions gathered from Reddit’s r/Ask-\nDocs subreddit, we also include HealthSearchQA4,ad a t a s e to fc o n -\nsumer health questions released by Google for the Med-PaLM paper.\nOn a random subset of 300 questions from this dataset, we evaluated\nGPT-4o w/ RAG and report citation URL validity of 100%, statement-\nlevel support of 75.7% (74.0– 77.2 95% CI), and response-level support\nof 38.4% (26.7, 49.3 95% CI). This is in line with the response-level\nsupport rate in our human-generated Reddit dataset of 31.0% (26.7,\n35.8 95% CI). This provides additional support for the claim that LLMs\nhave difﬁculty producing faithful citations on open-ended questions\nfrom users.\nEnd-to-end full human evaluation\nWe conduct a detailed end-to-end human expert evaluation of LLM\ncitations. In this task, a human clinician is tasked to assess a yes/no\ndecision on whetherall of the facts presented in a response are sup-\nported by the citations. This experiment evaluates GPT-4o w/ RAG on a\nsubset of 100 questions from HealthSearchQA. The human expert\nﬁnds that only 40.4% (30.7, 50.1) of the responses are fully supported\nby the citations provided. In comparison, on the same questions,\nSourceCheckupﬁnds 42.4% (32.7, 52.2 95% CI) of responses supported.\nBoth the human expert andSourceCheckup yield similar response-\nsupport rates on the same dataset, validating the robustness of our\nautomated approach. Notably, both evaluation approaches support\nour mainﬁnding that frontier LLM with RAG do not accurately reﬂect\nsources for many medical questions.\nURL analysis\nWe found that the URLs generated by the LLMs are predominantly from\nhealth information websites like mayoclinic.com or government health\nwebsites (e.g., nih.gov, cdc.gov) (Table1). We also found low rates of\nURLs coming from paywalled or defunct web pages (Table2). Interest-\ningly, most sources are from US-based websites (average of 92%), with\nGemini Ultra 1.0 (RAG) having the highest proportion of non-US sources\n(10.68%). Finally, we found thatmost sources are from.org or.gov\ndomain names, indicating an origin of professional/non-proﬁto r g a n i -\nzations and governmental resources (Supplementary Fig. 4).\nEditing model responses to improve statement relevance\nWe used ourSourceCleanup a g e n tt oe i t h e rr e m o v eo rm o d i f yp r e -\nviously unsupported statements from GPT-4o (RAG), GPT-4o (API),\nand Claude v2.1 (API). On 150 unsupported statements,SourceCleanup\nremoved 34.7% (52/150) of unsupported statements entirely. We had\nhuman experts re-evaluate the remaining 98SourceCleanupmodiﬁed\nstatements and found that 85.7% (84/98) of the statements were now\nsupported by the source after modiﬁcation. In aggregate, theSource-\nCleanupagent either removes or correctly edits 90.7% (136/150) of the\nsupported statements. We have included examples of the modiﬁca-\nt i o n sm a d ei nS u p p l e m e n t a r yT a b l e5 ,a n dt h ee n t i r es e to fm o d iﬁca-\ntions in our GitHub repository.\nDiscussion\nSourcing high-quality medical annotations can be prohibitively costly\nand difﬁcult toﬁnd. While previous works have used LLMs to conﬁrm\nsource attribution, our work validates automated medical source\nveriﬁcation with a panel of medical experts. Our fully automated\nTable 1 | Topﬁve websites cited by LLMs\nGPT-4o (RAG) Gemini Ultra 1.0 (RAG) GPT-4o (API) Claud e v2.1 Mistral Medium Mixtral Open Gemini Pro (API)\nmayoclinic.org (16%) ncbi.nlm.nih.gov (36%) ncbi .nlm.nih.gov (20%) ncbi.nlm.nih.gov (28%) mayoclinic.org (25%) ncbi.nlm.nih.gov (23%) mayoc linic.org (25%)\nncbi.nlm.nih.gov (10%) clevelandclinic.org (6%) mayoclinic.org (15%) m ayoclinic.org (11%) ncbi.nlm.nih.gov (18%) mayoclinic.org (21%) ncbi. nlm.nih.gov (14%)\nclevelandclinic.org (9%) mayoclinic.org (5%) cdc.gov (7 %) cdc.gov (5%) cdc.gov (5%) cdc.gov (5%) webmd.com (8%)\ndrugs.com (3%) cdc.gov (3%) uptodate.com (5%) aafp.org (4%) medlineplus.gov (2%) healthline.com (4%) hopkinsmedicine.org (8%)\ncdc.gov (2%) webmd.com (2%) medlineplus.gov (4%) medicalnewstoda y.com (3%) uptodate.com (2%) medlineplus.gov (3%) cdc.gov (7%)\nThe domain names of each model’s cited sources are extracted and ranked, with the topﬁve displayed in the table above. Of note, the NIH (ncbi.nlm.nih.gov), MayoClinic (www.mayoclinic.org), andCDC are among the top-cited URLs of all seven models.\nArticle https://doi.org/10.1038/s41467-025-58551-6\nNature Communications|         (2025) 16:3615 4\nframework allows for the rapid development of question-answering\ndatasets while reducing the need for additional manual annotations.\nThis capability is key, especially in theﬁeld of clinical medicine, where\nstandard-of-care and up-to-date knowledge are constantly evolving.\nAdditionally, our experiments withSourceCleanupalso show the pro-\nmise of an LLM-based approach to response editing to improve source\nfaithfulness.\nTo support future research, we have structured our dataset of\n58,000 statement-source pairs in a reusable format. Researchers can\nutilize the questions and associated sources to evaluate LLMs’ per-\nformance on source attribution across different model versions.\nAdditionally, the statement-source pairs serve as a valuable benchmark\nfor comparing model improvements in citation accuracy and source\nrelevance over time, enabling longitudinal studies on the reliability of\nLLM-generated medical references.\nOur results highlight a signiﬁcant gap in the current LLMs and the\ndesired behavior in medical settings. Regulators, clinicians, and\npatients alike require that model responses be trustworthy and ver-\niﬁable. Central to this is that they can provide reputable sources to\nback their medical claims. Given that LLMs are predominantly trained\non next-token prediction, it is unsurprising that“ofﬂine” models such\nas Mistral, Gemini-Pro, and Claude would provide hallucinated URLs or\nrelated, but incorrect, URLs as sources. We believe that if given access\nto the web search, these models would perform much better at pro-\nducing valid URLs. To remedy this issue, models should be trained or\nﬁne-tuned directly to provide accurate source veriﬁcation. RAG mod-\nels show promise, as they can directly pull information from articles via\nsearch engines. However, weﬁnd that a substantial fraction of the\nreferences provided by RAG do not fully support the claims in GPT-4o\n(RAG) or Gemini Ultra 1.0 (RAG)‘sr e s p o n s e s .T h i sm i g h tb ed u et ot h e\nLLM extrapolating the retrieved information with its pretraining\nknowledge or hallucination.\nAn important distinction of our work is that we emphasize ver-\nifying whether the LLMs’ generated statements aregrounded in ver-\niﬁable sources rather than directly assessing the correctness of each\nclaim. We take this approach because the nature of whether a claim is\ntrue or false can be up to subjective interpretation— indeed, even\nmedical experts may disagree over the degree to which a medical claim\nis fully factual. For example, our dataset contains a question,“What age\ngroup is most commonly affected by tennis elbow?”, which has mul-\ntiple overlapping answers (e.g., 30– 60 years, 30– 50 years, 40– 60\nyears). The process of ground-truthing opens up ambiguity around\nhow to adjudicate different answers.\nWe ﬁnd that models’responses to Reddit questions are on average,\nlonger than those from MayoClinic, given the more open-ended nature\nand tendency for users to ask about multiple related topics. Addition-\nally, the model is more speculative in its responses to Reddit questions,\noften providing more disclaimers and potential answers. These factors\nlead to both the statement-level response-level support being lower on\nReddit questions compared to the MayoClinic. In general, we notice that\nfor direct questions with straightforward answers, models are much\nstronger at providing relevant citations. When models are asked to\nspeculate or provide multiple answers, they tend to produce responses\nthat deviate further fromthe provided citations.\nUnder Section 230 of the Communications Decency Act, websites\nlike Twitter or WebMD are not regulated by the US FDA, since they\nsimply act as an intermediary for, rather than the author of, medical\ninformation\n37. However, it is unclear if this existing legal protection is\nlikely to apply to LLMs since they can extrapolate and hallucinate new\ninformation. Additionally, the existing regulatory framework for AI\nsoftware medical devices may also not apply to LLMs, as they do not\nhave constrained, deterministic outputs\n38.T h u s ,a s s e s s i n gt h ed e g r e e\nto which LLMs reliably convey existing, trustworthy medical knowl-\nedge is important for informing future regulatory frameworks\nregarding medical LLMs.\nT a b l e2|U R LS t a t i s t i c sb yL L M .A c r o s sa l ls e v e nm o d e l se v a l u a t e d, we found low rates of URLs originating from sites with paywalls\nMetric GPT-4o (RAG) Gemini Ultra 1.0 (RAG) GPT-4o (API) C laude v2.1 Mistral Medium Mixtral Open Gemini Pro (API)\n% of valid URLs from domain with paywall 3.40% 2.65% 7.07% 4.11% 4.36% 2.98% 4.76%\n% of invalid URLs with archived page (N = 250) 0.00% 0.00% 3.50% 6.50% 0.50% 0.024 3.50%\n% of URLs from United States 91.51% 89.32% 93.17% 94.05% 91.11% 92.25% 92.56%\nWe also found low rates of previously existing, but now defunct, pages, suggesting that the invalid links outputted by the LLMs were indeed hallucinated. Finally, we saw that the sources that the LLMs cite are predominantly from US-based websites.\nArticle https://doi.org/10.1038/s41467-025-58551-6\nNature Communications|         (2025) 16:3615 5\nIn our breakdown of source veriﬁcation by question source, we\nﬁnd that models perform signiﬁcantly worse on questions sourced\nfrom Reddit r/AskDocs versus MayoClinic. This is signiﬁcant, as ques-\ntions from Reddit are user-generated, whereas MayoClinic is vetted by\nmedical professionals. One potential reason for this divergence is that\nuser-generated questions tend to reﬂect a more diverse distribution of\ntopics and more variable reading levels than medical reference sites,\nwhich tend to use precise medical terminology. Another hypothesis is\nthat user-generated questions may contain erroneous premises for\nwhich LLMs have the propensity to afﬁrm, known as contra-factual\nbias\n39. In this same vein, we also found that the cited URLs are from US-\nbased sources over 90% of the time, which may potentially reﬂect\nAmerican patient-centric medical evidence and standard of care. It is\nimportant thus for LLMs to adequately perform source veriﬁcation to\nserve a wide range of users— both laypersons and medical profes-\nsionals, as well as sources that represent their intended demographic.\nMeasuring source veriﬁcation in models is also not intended to be\nconsidered in isolation. For example, one could trivially perform per-\nfectly by quoting verbatim from a set of known sources (e.g., Google\nsearch). Instead, this benchmark should be used with other quality-\nbased evaluation metrics to highlight inherent trade-offs in LLMs when\nthey extrapolate information. To this end, we are releasing all of our\ncurated data and expert annotations as a community resource.\nOur approach also has several limitations that motivate follow-up\nstudies. First, our automated pipeline can accrue errors in the question\ngeneration, statement parsing, citation extraction, and source ver-\niﬁcation stages. While we have performed spot checks on each com-\nponent, these errors are non-zero and can lead to smallﬂuctuations in\nthe ﬁnal reported results. Second, the task of source veriﬁcation can be\nambiguous, as shown by the lack of full agreement among the three\nmedical doctors in our study. As such, while we believe ourﬁnal results\nrepresent an accurate scale of the relevance of medical queries, indi-\nvidual data points may be more noisy and prone to interpretation.\nThird, our usage of a 1-1 mapping between statements and sources has\na tradeoff such that we do count cases where a statement can be\nsupported by aggregating multiple sources together. However, as an\nexperiment to evaluate the extent to which this effect is occurring, we\nanalyzed all statements judged as unsupported from GPT-4o (RAG)\nand re-ran the analysis with the sources merged for each statement. We\nﬁnd that 95.1% of the statements previously unsupported are still not\nsupported after merging the sources. Our URL analysisﬁndings report\na high rate of US-based websites, which could be in part due to the fact\nthat our questions are largely drawn from US-based sources (e.g.,\nMayoClinic). Finally, our URL content extraction module is prone to a\nsmall rate of 404 errors on websites that are otherwise accessible to\nindividuals in websites that prevent repeated requests from the same\nuser agents. Additionally, the ability to access scientiﬁc texts behind\npaywalls depends on each researcher’s access, meaning that there may\nbe discrepancies in URL validity when run across institutions.\nWe believe that going forward, source veriﬁcation is key to\nensuring that doctors have accurate and up-to-date information to\ninform their clinical decision-making and provide a legal basis for LLMs\nto be used in the clinic. Indeed, accurate source veriﬁcation extends\nbeyond the medical domain and has apt applications in otherﬁelds like\nlaw (e.g., case law) and journalism (e.g., fact-checking).\nMethods\nThis study was conducted in strict accordance with all applicable\nethical standards, guidelines, and regulations governing research\npractices.\nLLM evaluations\nOur analysis focuses on evaluating the following top-performing LLMs:\nGPT-4o (RAG), GPT-4o (API), Claude v2.1 (API), Mistral Medium (API),\nGemini Ultra 1.0 (RAG), and Gemini Pro (API). These models were\nchosen as they represent current leading LLMs\n3,36,40,41 as of February\n2024. Additionally, we consider the following open-source models:\nMixtral-8x7b (API), Llama-2-70b (API), and Meditron-7b, where\nMeditron-7b is a medical-domain open-source model. We usegpt-4o-\n2024-05-13 as the GPT-4o API endpoint, while Gemini Ultra 1.0 (RAG)\nwas evaluated on 3/28/24. All other model APIs were queried for\nresponses on 1/20/24. The parenthetical (RAG) refers to the model’s\nweb browsing capability powered by web search. When not labeled\nwith (RAG), GPT-4o refers to the standard API endpoint used in this\nstudy, without web browsing capability.\nSourceCheckupevaluation framework\nOur proposed pipeline consists of four modules: (1) Question Gen-\neration, (2) LLM Question Answering (3) Statement and URL Source\nParsing, and (4) Source Veriﬁcation. A schematic of this pipeline is\nfound in Fig.2, and an example is shown in Fig.3. The prompts used for\neach of the following sections are detailed in Supplementary Table 6.\nQuestion generation\nFirst, we collected 400 real-world medical queries from Reddit’sr /\nAskDocs, a subreddit for patients to ask medical professionals that has\nover 600 K members. These questions are typically presented as short\ncases with relevant symptoms provided by users. Additionally, given\nthat medical question datasets such as PubMedQA\n42 and MedQA43\nconsist ofﬁxed question sets susceptible to memorization, we propose\na question generation framework to create medical questions that\nreﬂect real-world clinical question/answering. A reference text was\ngiven to GPT-4o with a prompt to produce a question based on the\ncontent of the text. In this study, we select reference texts from\nMayoClinic, which provides patient-facing fact pages on common\nmedical queries. MayoClinic content allows us to generate text\ncomprehension-based questions, which may differ in style and tone\nfrom natural user queries. None of our reference documents were\ntaken from private datasets containing protected health information.\nWe used GPT-4o to generate a question from each of the 400 reference\ndocuments from MayoClinic. We then combined these 400 generated\nquestions with 400 real-world queries from r/AskDocs to produce our\nfull set of 800 questions. Finally, we posed each question to each of the\nseven LLMs. We include several examples of questions in Supple-\nmentary Table 7.\nLLM question answering\nWe queried each LLM to provide a short response to the question,\nalong with a structured list of sources that support the response. The\nprompt used for querying LLMs can be found in Supplementary\nTable 6. To gather responses from the GPT-4o (RAG) model with web\nbrowsing capabilities, we found that the standard prompt was unable\nto trigger the web search RAG capabilities, so we provided a modiﬁed\nversion of the prompt that explicitly asks the model to use Bing Search.\nIn a minority of cases, the model did not return a response or returned\nan incomplete response. In this event, we provided the LLM an addi-\ntional try before considering the response invalid.\nStatement parsing\nTo break up the response into individually veriﬁable statements, we\nused GPT-4o to parse the LLM responses. We deﬁne a“statement” to\nbe an independently veriﬁable part of a model’s response. For\nexample, the response“The proportion of HFE C282Y homozygotes\nwith documented iron overload-related disease is 28.4% for men and\n1.2% for women ” is broken into “The proportion of HFE C282Y\nhomozygotes with documented iron overload-related disease is 28.4%\nfor men” and “The proportion of HFE C282Y homozygotes with docu-\nmented iron overload-related disease is 1.2% for women”.C e r t a i n\nresponses did not return any parsed medical statements, largely\ndue to the nature of the question asked. For example, the model\nArticle https://doi.org/10.1038/s41467-025-58551-6\nNature Communications|         (2025) 16:3615 6\nresponse “Could you please provide the document or specify the\ndetails of the treatment options and durations for acute bacterial\nrhinosinusitis mentioned in it?” does not return any parsed medical\nstatements. Full details of the number of parsed statements, along\nwith source counts, are found in Supplementary Table 8. Addi-\ntionally, more details on the human instructions used in our vali-\ndation are in Supplementary Table 9. In general, weﬁnd that GPT-4o\n(API) and Claude v2.1 could consistently follow the instructions’\nJSON formatting, whereas other models had varying rates of suc-\ncess. In cases where the model fails to provide a structured source\nlist, we extracted and removed all URLs using regular expression\nmatching from the original text and treated them as the sources\nprovided.\nURL source parsing\nFor each URL source provided in the response, we downloaded the\nsource content of the URL. We only kept websites that returned a\n200 status code, meaning the content can be returned. A small\npercentage ( < 1%) of cases also included websites that cannot be\naccessed through our pipeline. We downloaded PDF documents\nlocally before extracting their text using a PDF-to-text converter.\nAfter the source content was extracted, we applied a pattern-\nmatching expression to strip code tags, leaving only the plain text.\nFinally, we excluded source contents that exceed the 128 K maximum\ntoken length of GPT-4o. This accounted for approximately 0.1% of all\ndownloaded URLs.\nSource veriﬁcation\nWe considered a statement to be supported if it can be attributed to at\nleast one source provided by the LLM. While not all sources are usually\nintended to support each statement, we found the task of determining\neach LLM’s intended statement-source pairings to be difﬁcult to\ndetermine. For example, some LLMs provided a footnote after each\nstatement, whereas others provided a list of links at the end of a\nparagraph. As such, we opted to simply consider all pairs of statements\nand sources in our evaluation. Additionally, we individually evaluate\neach source’s attribution to each statement individually rather than\ncombining all given sources together. This was done so for two rea-\nsons: (1) a 1-1 mapping for sources and statements allows us to report a\n“precision” metric (what % of sources do not support any statement in\nthe response). This measures each model’s ability to intentionally\nattribute their statements to sources rather than generate a long\nlaundry list of citations from a generic search result page, and (2) we\nfound the task of asking doctors to verify if the information has been\nproperly integrated across multiple documents to be more complex\nand ambiguous (eg. What is the proper way to perform meta-analyses\nacross studies, what if articles contradict one another, how does one\nweigh one article’s credibility vs another, etc.).\nGiven a list of statements and sources, each possible pair was\nchecked for whether the source contained the relevant information\nnecessary to support the statement. For instance, givenM statements\nand N sources, each of theM statements was checked against each of\nthe N sources for a total ofMxN pairs. For each pair, we prompted\nFig. 3 | An example of theSourceCheckupevaluation framework based on a real\nresponse from GPT-4o (RAG).A question is generated based on the contents of a\nmedical reference text. The question is posed to an LLM, and the response is parsed\ninto statements and sources. Each statement-source pair is automatically scored by\nt h eS o u r c eV e r iﬁcation model as supported (i.e., the source contains evidence to\nsupport the statement) or not supported.\nFig. 2 | Schematic of theSourceCheckupevaluation pipeline.To start, GPT-4o\ngenerates a question based on a given medical reference text. Each evaluated LLM\nproduces a response based on this question, which includes the response text\nalong with any URL sources. The LLM response is parsed for individual medical\nstatements while the URL sources are downloaded. Finally, the Source Veriﬁcation\nmodel is asked to determine whether a given medical statement is supported by the\nsource text and to provide a reason for the decision.\nArticle https://doi.org/10.1038/s41467-025-58551-6\nNature Communications|         (2025) 16:3615 7\nGPT-4o with the statement and source content and asked it to score\nthe pair. If a statement was supported by at least one source, it was\nconsidered“supported”; otherwise, it was considered“not supported”.\nTo disambiguate the use of GPT-4o for source veriﬁcation as well as\nevaluation, we refer to this task as the“Source Veriﬁcation model” and\nthe evaluated model by the full name (i.e., GPT-4o (RAG) or GPT-\n4o (API)).\nRashkin et al. propose\n23 and formalize a framework calledAttri-\nbutable to Identiﬁable Sources,w h i c hd eﬁnes the AIS score of a given\nlanguage model’s response,y, supported by evidenceAas 1 if a human\nreviewer would agree that“y is true, givenA” or 0 if not. Gao et al.\nextends24 this to measure the average sentence-level AIS score:\nAttr AIS ðy, AÞ = avgs 2 yAISðs, AÞ\nwhich is the percentage of statements within a response that is fully\nsupported byA.W ee x t e n dt h e s et w od eﬁnitions of statement-level\nand response-level AIS scores to statement-level and response-level\nsupport below.\nWe report three metrics to evaluate each model’s source ver-\niﬁcation capabilities:\nSource URL validity. Given all the source URLs produced by the model,\nwhat percent are valid?We deﬁne a valid URL as one that produces a\n200 status code when requested and returns valid text (non-empty\nresponse).\nSource URL Validity=\n#URLs with status code200\nTotal Number of URLs\nStatement-level support. What percent of medically relevant state-\nments produced by the model can be supportedby at least onesource?\nFor each statement parsed from the responses, we checked it against\nall sources produced by the model response. A statement was con-\nsidered supported if at least one of those sources was found to contain\nsupporting text.\nStatement Level Support=\nStatements Supported by≥1 Source\nTotal Number of Statements\nWe note that this metric does not penalize LLM responses for produ-\ncing many irrelevant sources. To this end, we also report the percent of\nURLs that are not used in supporting any statement, found in Sup-\nplementary Table 10.\nResponse-level support. What percent of responses have all their\nstatements supported?For each response, we checked whether that\nresponse contained all supported statements.\nResponse Level Support=\nResponses w=All Statements Supported\nTotal Number of Resonses\nExpert validation of GPT-4o automated tasks\nEach of the three GPT-4o automated tasks (Question Generation,\nResponse Parsing, and Source Veriﬁcation) was validated against the\nannotations of US-licensed practicing medical doctors.\nQuestion generation and response parser\nTo validate the performance of GPT-4o on the task of generating\nquestions from reference medical documents, we asked two medical\ndoctors to spot-check 100 pairs of documents and questions for\nrelevance and logical integrity. To validate GPT-4o’sp e r f o r m a n c eo n\nparsing medical statements from free-text responses, we also asked\ntwo medical doctors to analyze a sample of 330 statements from 72\nquestion/response pairs to check (1) if all the statements are found\nwithin the original response, and (2) if any statements are missing from\nthe list of parsed statements.\nSource veriﬁcation\nA subset of the statement and sources (N = 400) was selected from\nmodel responses from GPT-4o (RAG), GPT-4o (API), and Claude v2.1\n(API). Three medical doctors independently scored whether the LLM-\ngenerated source veriﬁcation decision correctly identiﬁed a statement\nas supported or not supported by the provided source. They also\noptionally provided a reason justifying their decision. We then calcu-\nlated the majority consensus of the doctors and reported the percent\nagreement among each doctor, the doctor consensus, and the LLM-\ngenerated decision.\nPotential bias of GPT-4o as an evaluator, parser, and question\ngenerator\nWe aim to evaluate whether our use of GPT-4o as the backbone of our\npipeline introduces downstream biases in the support rates of models.\nTo do so, we replicated our entire pipeline using Claude Sonnet 3.5, a\nleading LLM with comparable performance to GPT-4o. Using the same\nset of original documents from MayoClinic, we re-generate questions,\nparse model responses, and perform source veriﬁcation with Claude\nSonnet 3.5. Additionally, we benchmark Llama 3.1 70B on the task of\ncitation veriﬁcation to evaluate the capabilities of an open-source\nmodel. Finally, we compare the statement support rates between\nstatement-source pairs which have been produced by replacing GPT-\n4o with Claude Sonnet 3.5 in each part of our pipeline.\nImprove statement relevance to sources\nUnsupported statements often only partially deviate from their source.\nGiven this, we explore how effectively LLMs can revise these unsup-\nported statements to make them fully supported by the original\nsource. To address this, we developed an LLM agent called“Source-\nCleanup.” This agent uses GPT-4o as the backbone model and takes a\nsingle statement and its corresponding source as input and returns a\nmodiﬁed, fully supported version of the statement. The prompt used\nfor SourceCleanupis detailed in Supplementary Table 6.\nURL analysis\nWe computed several key statistics from the total set of URLs cited per\nmodel. First, we determined which domain names contain content that\ni sh i d d e nb e h i n dap a y w a l lo rs u b s c r i p t i o nm o d e l .S e c o n d ,o ft h eU R L s\nthat were deemed invalid due to a 404 error or similar“page not\nfound” response, we assessed how many URLs were previously valid\nbut are now outdated. To approximate this, we used the Internet\nArchive Wayback Machine API, which stores archived URLs. Second,\nwe reported the topﬁve domain names cited by each model.\nFinally, we analyzed the origin of URLs in two ways. First, we\ndetermined which domain names are US-based vs non-US-based\nby performing a whois lookup and looking for a valid country. If\nno country is returned, we default to count domains with TLDs\ncontained in.com,.org,.gov,.edu,.info,.net as US-based, and non-US-\nbased if not.\nStatistics and reproducibility\nThis study utilized an automated, agent-based evaluation framework,\nSourceCheckup, to systematically assess the relevance and suppor-\ntiveness of medical references cited by large language models (LLMs).\nNo statistical method was used to predetermine sample sizes, and no\ndata were excluded from the analyses. The experiments were not\nrandomized. The investigators were not blinded to allocation during\nexperiments and outcome assessment. Statistical analyses included\ncalculation of percent agreement between automated Source Ver-\niﬁcation models and human expert consensus, along with comparisons\nusing paired and unpaired two-sided t-tests and chi-squared tests, as\nArticle https://doi.org/10.1038/s41467-025-58551-6\nNature Communications|         (2025) 16:3615 8\nappropriate. Reproducibility was conﬁrmed through independent\nvalidation by US-licensed medical experts, and code and data for\nreproducibility are publicly available in our GitHub repository (https://\ngithub.com/kevinwu23/SourceCheckup).\nReporting summary\nFurther information on research design is available in the Nature\nPortfolio Reporting Summary linked to this article.\nData availability\nThe questions, responses, parsed statements, fact-citation pairs, and\nexpert annotations generated in this study have been deposited in a\nGoogle Drive accessible through the following open-access link:https://\ndrive.google.com/drive/folders/1a-i974g3XzLCtZLpTLBbqAwK0olpd5JY?\nusp=drive_link. This open-access data is available in perpetuity exclu-\nsively for non-commercial research purposes. The raw web pageﬁles\nfrom citations are not included in ourdata due to restrictions from some\ncontent providers; however, researchers can access the URLs themselves\nif they have appropriate permissions. For any inquiries regarding access,\nplease contact\nkevinywu@stanford.edu (please allow 3-5 days\nresponse time).\nCode availability\nT h ec o d et oi m p l e m e n tSourceCheckup’s pipeline, as well as the\nprompts used in each of the agent modules, can be found athttps://\ngithub.com/kevinwu23/SourceCheckup.D O Ii d e n t iﬁer: https://doi.\norg/10.5281/zenodo.15015209.\nReferences\n1 . G i l b e r t ,S . ,H a r v e y ,H . ,M e l v i n ,T . ,V o l l e b r e g t ,E .&W i c k s ,P .L a r g e\nlanguage model AI chatbots require approval as medical devices.\nNat. Med.29,2 3 9 6– 2398 (2023).\n2. Brin, D. et al. Comparing ChatGPT and GPT-4 performance in\nUSMLE soft skill assessments.Sci. Rep.13, 16492 (2023).\n3. Singhal, K. et al. Toward expert-level medical question answering\nwith large language models.Nat. Med.31,1 – 8 (2025).\n4. Singhal, K. et al. Large language models encode clinical knowl-\nedge. Nature 620,1 7 2– 180 (2023).\n5. Ingram, D. ChatGPT used by mental health tech app in AI experi-\nment with users.NBC News(2023).\n6 . M a p l e s ,B . ,C e r i t ,M . ,V i s h w a n a t h ,A .&P e a ,R .L o n e l i n e s sa n ds u i c i d e\nmitigation for students using GPT3-enabled chatbots.NPJ Ment.\nHealth Res.3,1 – 6 (2024).\n7. Holohan, M. A boy saw 17 doctors over 3 years for chronic pain.\nChatGPT found the diagnosis.TODAY (2023).\n8. Temsah, M.-H. et al. ChatGPT and the future of digital health: a study\non healthcare workers’ perceptions and expectations.Healthcare.\n11, 1812 (2023).\n9. Tangadulrat, P., Sono, S. & Tangtrakulwanich, B. Using ChatGPT for\nclinical practice and medical education: cross-sectional survey of\nmedical students’ and physicians’ perceptions.JMIR Med Educ.9,\ne50658 (2023).\n10. Pal, A., Umapathi, L. K. & Sankarasubbu, M. Med-HALT: medical\ndomain hallucination test for large language models.Proc. 27th\nConf. Comput. Nat. Lang. Learn. (CoNLL),3 1 4– 334 (2023).\n11. Sun, L. et al. TrustLLM: trustworthiness in large language models.\nProc. 41st Int. Conf. Mach. Learn.(2024).\n12. Ahmad, M. A., Yaramis, I. & Roy, T. D. Creating trustworthy LLMs:\ndealing with hallucinations in healthcare AI. arXiv 2311.01463\nAvailable at:https://arxiv.org/abs/2311.01463(2023).\n13. Dash, D. et al. Evaluation of GPT-3.5 and GPT-4 for supporting real-\nworld information needs in healthcare delivery. arXiv 2304.13714\nAvailable at:https://arxiv.org/abs/2304.13714(2023).\n14. Daws, R. Medical chatbot using OpenAI’s GPT-3 told a fake patient\nto kill themselves. AI News Available at:https://www.\nartiﬁcialintelligence-news.com/news/medical-chatbot-openai-\ngpt3-patient-kill-themselves/(2020).\n15. Nastasi, A.J., Courtright, K.R.,Halpern, S.D. et al. A vignette-based\nevaluation of ChatGPT’s ability to provide appropriate and equitable\nmedical advice across care contexts.Sci. Rep.13,1 7 8 8 5\n(2023).\n16. Zawiah, M. et al. ChatGPT and clinical training: perception, con-\ncerns, and practice of Pharm-D students.J. Multidiscip. Heal16\n,\n4099– 4110 (2023).\n17. Abouammoh, N. et al. Perceptions and earliest experiences of\nmedical students and faculty with ChatGPT in medical education:\nqualitative study.JMIR Med. Educ.11, e63400 (2025).\n1 8 . J a n s z ,J .&S a d e l s k i ,P .T .L a r g eLanguage Models in Medicine: The\npotential to reduce workloads, leverage the EMR for better com-\nmunication & more.Rheumatologist(2023).\n19. Hacker, P., Engel, A. & Mauer, M. Regulating ChatGPT and other\nLarge Generative AI Models. inProceedings of the 2023 ACM\nConference on Fairness, Accountability, and Transparency1112– 1123\n(Association for Computing Machinery, 2023).\n20. Baumann, J. ChatGPT Poses New Regulatory Questions for FDA,\nMedical Industry. Available at:https://news.bloomberglaw.com/\nhealth-law-and-business/chatgpt-poses-new-regulatory-\nquestions-for-fda-medical-industry(2023).\n21. Taylor, N. P. FDA calls for‘nimble’ regulation of ChatGPT-like models\nto avoid being‘swept up quickly’ by tech. Available at:https://www.\nmedtechdive.com/news/fda-calls-for-nimble-regulation-of-\nchatgpt-like-models-to-avoid-being-sw/649756/(2023).\n2 2 . L i ,X . ,C a o ,Y . ,P a n ,L . ,M a ,Y .&S u n ,A .T o w a r d sv e r iﬁable generation:\na benchmark for knowledge-aware language model attribution.\nFindings Assoc. Comput. Linguist.: ACL 2024,4 9 3– 516 (2024).\n23. Rashkin, H. et al. Measuring attribution in natural language gen-\neration models.Comput. Linguist.49,7 7 7– 840 (2023).\n24. Gao, T., Yen, H., Yu, J. & Chen, D. Enabling large language models to\ngenerate text with citations.Proc. 2023 Conf. Empir. Methods Nat.\nLang. Process., 6465– 6488 (2023).\n25. Nakano, R. et al. WebGPT: Browser-assisted question-answering\nwith human feedback. Available at:https://arxiv.org/abs/2112.\n09332 (2021).\n26. Malaviya, C. et al. ExpertQA: expert-curated questions and attrib-\nuted answers.P r o c .2 0 2 4C o n f .N o r t hA m .C h a p t e rA s s o c .C o m p u t .\nLinguist.: Hum. Lang. Technol.,3 0 2 5– 3045 (2024).\n27. Proposed Taxonomy for Gender Bias in Text - ACL Anthology.\nHttpsaclanthologyorg› Httpsaclanthologyorg›.\n28. Kamalloo, E., Jafari, A., Zhang, X., Thakur, N. & Lin, J. HAGRID: A\nHuman-LLM Collaborative Dataset for Generative Information-\nSeeking with Attribution. Available at:https://arxiv.org/abs/2307.\n16883 (2023).\n29. Chen, H.-T., Xu, F., Arora, S. & Choi, E. Understanding Retrieval\nAugmentation for Long-Form Question Answering. (2023).Proc.\n2024 Conf. Lang. Model. (COLM), (2024).\n30. Bohnet, B. et al. Attributed Question Answering: Evaluation and\nModeling for Attributed Large Language Models. Available at:\nhttps://arxiv.org/abs/2212.08037(2022).\n31. Menick, J. et al. Teaching language models to support answers with\nveriﬁed quotes. Available at:https://arxiv.org/abs/2203.11147\n(2022).\n32. Min, S. et al. FActScore:ﬁne-grained atomic evaluation of factual\nprecision in long form text generation.Proc. 2023 Conf. Empir.\nMethods Nat. Lang. Process.,1 2 0 7 6– 12100 (2023).\n33. Yue, X. et al. Automatic evaluation of attribution by large language\nmodels. Findings Assoc. Comput. Linguist.: EMNLP 2023,\n4615– 4635 (2023).\n34. Zuccon, G., Koopman, B. & Shaik, R. ChatGPT Hallucinates when\nAttributing Answers. Available at:https://arxiv.org/abs/2309.\n09401 (2023).\nArticle https://doi.org/10.1038/s41467-025-58551-6\nNature Communications|         (2025) 16:3615 9\n35. Liu, N., Zhang, T. & Liang, P. Evaluating veriﬁability in generative\nsearch engines.Findings Assoc. Comput. Linguist.: EMNLP 2023,\n7001– 7025 (2023).\n36. Tatsu. AlpacaEval Leaderboard. (2024).\n37. Haupt, C. E. & Marks, M. AI-generated medical advice-GPT and\nbeyond. JAMA 329,1 3 4 9– 1350 (2023).\n38. Gottlieb, S. & Silvis, L. How to safely integrate large language\nmodels into health care.JAMA Health Forum4, e233909 (2023).\n39. Dahl, M., Magesh, V., Suzgun, M. & Ho, D. E. Large legalﬁctions:\nproﬁling legal hallucinations in large language models.J. Leg. Anal.\n16,6 4– 93 (2024).\n40. Eriksen Alexander, V., Sören, M. öller & Jesper, Ryg Use of GPT-4 to\ndiagnose complex clinical cases.NEJM AI1, AIp2300031 (2023).\n41. Strong, E. et al. Chatbot vs medical student performance on free-\nresponse clinical reasoning examinations.JAMA Intern. Med.183,\n1028– 1030 (2023).\n4 2 . J i n ,D .e ta l .W h a td i s e a s ed o e st h is patient have? A large-scale open\ndomain question answering dataset from medical exams.Appl. Sci.\n11, 6421 (2021).\n43. Jin, Q., Dhingra, B., Liu, Z., Cohen, W. W. & Lu, X. PubMedQA: A\nDataset for Biomedical Research Question Answering. (2019).\nAcknowledgements\nWe thank Josiah Aklilu and Min Woo Sun for their helpful feedback on\nour paper.\nAuthor contributions\nK.Wu, E.W., and J.Z. conceived of the presented idea. K.Wu, E.W., D.H.,\nand J.Z. contributed to the core analysis and writing of the paper. K.Wu\nand E.W. performed the main experiments and analyses of data. K.Wu.,\nE.W., and A.C. contributed to the code for the experiments and analyses.\nK.Wei, A.Z., T.N., S.R., and P.S. contributed to expert annotations and\nanalysis of data.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41467-025-58551-6.\nCorrespondenceand requests for materials should be addressed to\nJames Zou.\nPeer review informationNature Communicationsthanks Yifan Peng,\nAixin Sun, and the other anonymous reviewer(s) for their contribution to\nthe peer review of this work. A peer reviewﬁle is available.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jur-\nisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution-NonCommercial-NoDerivatives 4.0 International License,\nwhich permits any non-commercial use, sharing, distribution and\nreproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the\nCreative Commons licence, and indicate if you modiﬁed the licensed\nmaterial. You do not have permission under this licence to share adapted\nmaterial derived from this article or parts of it. The images or other third\nparty material in this article are included in the article’s Creative\nCommons licence, unless indicatedotherwise in a credit line to the\nmaterial. If material is not included in the article’s Creative Commons\nlicence and your intended use is not permitted by statutory regulation or\nexceeds the permitted use, you will need to obtain permission directly\nfrom the copyright holder. To view a copy of this licence, visithttp://\ncreativecommons.org/licenses/by-nc-nd/4.0/.\n© The Author(s) 2025\nArticle https://doi.org/10.1038/s41467-025-58551-6\nNature Communications|         (2025) 16:3615 10",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.4828527569770813
    },
    {
      "name": "Data science",
      "score": 0.41144201159477234
    },
    {
      "name": "Computational biology",
      "score": 0.40130671858787537
    },
    {
      "name": "Biology",
      "score": 0.228753924369812
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2800154431",
      "name": "Keck Hospital of USC",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I26347476",
      "name": "Loma Linda University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210137306",
      "name": "Stanford Medicine",
      "country": "US"
    }
  ],
  "cited_by": 14
}