{
    "title": "SKILL: Structured Knowledge Infusion for Large Language Models",
    "url": "https://openalex.org/W4287888135",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4280843522",
            "name": "Moiseev, Fedor",
            "affiliations": [
                "École Polytechnique Fédérale de Lausanne",
                "Google (Switzerland)"
            ]
        },
        {
            "id": "https://openalex.org/A1692455591",
            "name": "Dong Zhe",
            "affiliations": [
                "Google (Switzerland)"
            ]
        },
        {
            "id": "https://openalex.org/A4224889332",
            "name": "Alfonseca, Enrique",
            "affiliations": [
                "Google (Switzerland)"
            ]
        },
        {
            "id": "https://openalex.org/A3161511077",
            "name": "Jaggi, Martin",
            "affiliations": [
                "École Polytechnique Fédérale de Lausanne"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3146844750",
        "https://openalex.org/W2970986510",
        "https://openalex.org/W3034671305",
        "https://openalex.org/W3102844651",
        "https://openalex.org/W2963448850",
        "https://openalex.org/W2963101081",
        "https://openalex.org/W3182414949",
        "https://openalex.org/W2094728533",
        "https://openalex.org/W2912924812",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3102659883",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3171434230",
        "https://openalex.org/W2963866616",
        "https://openalex.org/W3156789018",
        "https://openalex.org/W2081580037",
        "https://openalex.org/W2946088473",
        "https://openalex.org/W3003265726",
        "https://openalex.org/W2755637027",
        "https://openalex.org/W2963339397",
        "https://openalex.org/W2953958347",
        "https://openalex.org/W4309417034",
        "https://openalex.org/W4287547451",
        "https://openalex.org/W3169726359",
        "https://openalex.org/W2950339735",
        "https://openalex.org/W3098266846",
        "https://openalex.org/W2561529111",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4293569541",
        "https://openalex.org/W4292779060"
    ],
    "abstract": "Large language models (LLMs) have demonstrated human-level performance on a vast spectrum of natural language tasks. However, it is largely unexplored whether they can better internalize knowledge from a structured data, such as a knowledge graph, or from text. In this work, we propose a method to infuse structured knowledge into LLMs, by directly training T5 models on factual triples of knowledge graphs (KGs). We show that models pre-trained on Wikidata KG with our method outperform the T5 baselines on FreebaseQA and WikiHop, as well as the Wikidata-answerable subset of TriviaQA and NaturalQuestions. The models pretrained on factual triples compare competitively with the ones on natural language sentences that contain the same knowledge. Trained on a smaller size KG, WikiMovies, we saw 3x improvement of exact match score on MetaQA task compared to T5 baseline. The proposed method has an advantage that no alignment between the knowledge graph and text corpus is required in curating training data. This makes our method particularly useful when working with industry-scale knowledge graphs.",
    "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 1581 - 1588\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nSKILL: Structured Knowledge Infusion for Large Language Models\nFedor Moiseev∗1,2 Zhe Dong†2 Enrique Alfonseca2 Martin Jaggi1\n1EPFL, Switzerland 2Google, Switzerland\n{femoiseev, zhedong, ealfonseca}@google.com, martin.jaggi@epfl.ch\nAbstract\nLarge language models (LLMs) have demon-\nstrated human-level performance on a vast spec-\ntrum of natural language tasks. However, it\nis largely unexplored whether they can better\ninternalize knowledge from a structured data,\nsuch as a knowledge graph, or from text. In this\nwork, we propose a method to infuse structured\nknowledge into LLMs, by directly training T5\nmodels on factual triples of knowledge graphs\n(KGs). We show that models pre-trained on\nWikidata KG with our method outperform the\nT5 baselines on FreebaseQA and WikiHop, as\nwell as the Wikidata-answerable subset of Triv-\niaQA and NaturalQuestions. The models pre-\ntrained on factual triples compare competitively\nwith the ones on natural language sentences\nthat contain the same knowledge. Trained on\na smaller size KG, WikiMovies, we saw 3×\nimprovement of exact match score on MetaQA\ntask compared to T5 baseline. The proposed\nmethod has an advantage that no alignment be-\ntween the knowledge graph and text corpus is\nrequired in curating training data. This makes\nour method particularly useful when working\nwith industry-scale knowledge graphs.\n1 Introduction\nLarge pre-trained language models, such as BERT\n(Devlin et al., 2019), GPT-3 (Brown et al., 2020),\nT5 (Raffel et al., 2019), REALM (Guu et al., 2020)\nand ERNIE (Sun et al., 2021) have become the\nstate-of-the-art technology for many tasks. They\nare commonly pre-trained using unstructured text\ncorpora, on tasks such as next word prediction,\nnext sentence prediction (NSP) or masked lan-\nguage modelling (MLM). Especially for T5, self-\nsupervised learning on unlabelled text corpus with\nMLM has been a common pre-training recipe\n(Roberts et al., 2020). This is normally followed\nby a fine-tuning step on the task of interest (Ruder\n∗Work done during internship at Google.\n†Correspondence Author.\net al., 2019), although large language models have\nalso proved useful without this task-specific fine-\ntuning (Brown et al., 2020).\nBeyond the capacity of contextual understand-\ning, human-level language understanding pivots on\nthe knowledge about the world. The world knowl-\nedge is often expressed as factual triples (c.f. Ji\net al., 2020), in the form of (subject entity, relation,\nobject entity). A knowledge graph (KG) defined by\na set of factual triples consists of the subjects and\nobjects as vertices/nodes, and the relations form-\ning the edges connecting them. Most of the large\nscale KGs (e.g. Wikidata, Vrandeˇci´c and Krötzsch,\n2014) are stored in triple format.\nLLMs demonstrate some capacity of learning\nworld knowledge from the natural text corpus\n(Roberts et al., 2020), but it is unclear to what\ndegree they are also able to learn and memorize\nnew knowledge directly from structured KG triples,\nor from text describing them explicitly.\nIn order to infuse knowledge into a LLM, one\noption is to generate a textual version of the knowl-\nedge base, and apply the standard training objec-\ntives, e.g. MLM. This is unfortunately highly non-\ntrivial. One can either align sentences with KG\ntriples, as done in ERNIE (Sun et al., 2021), or\ngenerate sentences from triples, as done in KELM\n(Agarwal et al., 2021). These approaches are un-\nfortunately hard to port to knowledge graphs with\ndifferent schemas. These processes are also lossy\nin that not every triple can be aligned or produce\na valid sentence, and there is not a good under-\nstanding whether this can introduce unnecessary\nselection biases on top of biases existing in the\noriginal KG.\nIn this work, we propose a method of Knowl-\nedge Infusion for Large Language Models\n(SKILL), where LLMs directly learns from knowl-\nedge triples. Experiment results shows the check-\npoints trained with proposed method on Wikidata\nKG outperform the T5 baselines on four standard\n1581\nclosed-book question-answering (QA) tasks. With\na smaller KG, WikiMovies, the proposed method\ngain 3×exact match score performance improve-\nment on MetaQA task. The models learning di-\nrectly from knowledge triples performs competi-\ntively with the ones with the aligned natural sen-\ntences that contain the same amount of knowledge.\nBeing able to learn directly from knowledge triples\nenables easy addition of structured knowledge into\nlanguage modeling pre-training.\n2 Related work\nPrevious works that use knowledge graphs to en-\nhance the quality of knowledge-intensive down-\nstream tasks can be divided into two groups: using\nknowledge graphs at the inference time, and in-\nfusing knowledge into the model weights at the\npre-training time. The proposed method falls in the\nlatter group.\nExplicit usage of knowledge graphs. A\nretrieval-augmented model is commonly used,\nin order to retrieve and apply the knowledge\nfrom external memories or sources. FILM (Verga\net al., 2021) and EaE (Févry et al., 2020) extend\nTransformer (Vaswani et al., 2017) models with\nexternal entity (both FILM and EaE) and fact\n(FILM) memories. REALM (Guu et al., 2020)\nis pre-trained to perform reasoning over a large\ntextual knowledge corpus on-the-fly during infer-\nence. UniK-QA (Oguz et al., 2020) combines the\nstructured and unstructured information to improve\nthe open-domain QA tasks with a retriever-reader\nframework. The main difference between the\nproposed method, SKILL, and retrieval-augmented\nmodels is that SKILL doesn’t introduce retrieval\nsystem or external memories to the model, but\nit directly embeds knowledge into the model\nparameters, which introduces no extra cost at\ninference time.\nKnowledge infusion. A common way of param-\neterized knowledge infusion is to map or convert\nstructured knowledges into natural language text.\nERNIE 3.0 (Sun et al., 2021) trains a knowledge-\nenhanced model on a corpus combining triples and\ntheir aligned sentences, by randomly masking re-\nlation in a triple or words in a sentence. On the\ncontrary, SKILL trains only on triples.\nKnowBert (Peters et al., 2019) incorporates\nknowledge from Wikipedia and WordNet (Miller,\n1995) into a BERT model through entity\nembeddings with knowledge-attention and re-\ncontextualization mechanism. BERT-MK (He et al.,\n2020) is a BERT-based model that integrates graph\ncontextual knowledge of a medical KG, which\ndemonstrates the utility of graph-level knowledge.\nThese approaches requires entity linking and sen-\ntences contextualizing the knowledge graph infor-\nmation.\nKG-FiD (Yu et al., 2021) extends the Fusion-in-\nDecoder model (Izacard and Grave, 2021) with a\nmodule that filters and re-ranks passages based on\nstructural connections in knowledge graph between\nentities described in those passages. In contrast to\nthe SKILL method that we propose, it requires\nthe existence of natural text passages describing\neach knowledge graph entity, so Wikipedia corpus\nwas used since it naturally provides articles that\ndescribe entities.\nHeinzerling and Inui (2021) explored the ability\nof language models to memorize and understand\ninformation from knowledge graphs, but used nat-\nural language representation of triples based on\npredefined templates instead of structured represen-\ntation. Usage of predefined templates significantly\nlimits scalability and therefore only relatively small\nknowledge graphs were used, such as Google-RE1.\nIn contrast to the new method presented in this\npaper, all of these approaches require an explicit\nmapping between the knowledge graph entities\nor facts and corresponding natural language sen-\ntences, which can limit applications to industry-\nscale knowledge graphs that don’t have such a map-\nping.\nDifferent goals of using knowledge graphs.Be-\nsides that, some papers embed knowledge into\nmodel weights but pursue different goals rather\nthan improving performance on downstream tasks.\nCOMET (Bosselut et al., 2019) is most similar to\nour work and trains a commonsense-aware Trans-\nformer Language Model by learning to generate\nloosely structured commonsense descriptions in the\nnatural language given the structured knowledge.\nSimilar to us, it also uses KG triples in surface\nform as a source for training data, but in contrast\nto our research, the final goal of COMET is to gen-\nerate new knowledge instead of utilizing existing\nones. Another important difference is the scale:\nCOMET uses Atomic (Sap et al., 2019) and Con-\nceptNet (Speer et al., 2017) Knowledge Graphs\n1https://ai.googleblog.com/2013/04/50000-lessons-on-\nhow-to-read-relation.html\n1582\nthat are much smaller than Wikidata (Vrande ˇci´c\nand Krötzsch, 2014).\nKELM (Agarwal et al., 2021) fine-tunes a T5\nmodel to convert KGs to synthetic natural language\nsentences to augment existing pre-training corpora.\nWe build our research on top of it and use the\nKELM dataset to compare structured and natural\nlanguage representations of knowledge.\n3 Method\nThere are two components of knowledge infusion\nfor LLMs (SKILL): the corpus and the training\nmethod. We introduce the method based on Wiki-\ndata KG, but it can be applied to any other KGs.\nTraining corpus. We use two corpora with dif-\nferent knowledge representations: Wikidata KG\n(Vrandeˇci´c and Krötzsch, 2014) in triple format,\nand KELM corpus 2 (Agarwal et al., 2021) as\nsynthetic natural language sentences converted\nfrom Wikidata KG. The KELM corpus contains\n15, 628, 486 synthetic sentences. To ensure two\ncorpora share the same knowledge, we take the\nsnapshot of the Wikidata KG used to created the\nKELM corpus, which contains 35, 697, 715 triples.\nTo prevent the degradation of model perfor-\nmance on natural language understanding, we mix\nthe Wikidata corpus or KELM corpus with natural\ntext from C4 (Raffel et al., 2019), 50 : 50, for the\nknowledge infusion training data.\nTraining method. T5 (Raffel et al., 2019) was\ntrained through masked-language modelling with\nrandom span corruption on the C4 corpus. Roberts\net al. (2020) found that masking salient terms (Guu\net al., 2020) in pre-training T5 models, instead of\nmasking random token spans, could significantly\nimprove the performance on downstream tasks, e.g.\nclosed-book QA.\nWe apply salient span masking for unsupervised\nlearning in our knowledge-infusing training. To\nmask the same amount of information is for both\ncorpora, the following method is applied. For a\nknowledge triple, we mask either the subject or\nobject entity. For a KELM sentence, we identify\nthe aligned triple, with details in Appendix A, and\nmask the full spans corresponding to the subject\nor object in the triple. The relation tokens are\nnever masked, as there is no robust way to map\nthe abstract relation in knowledge triples to natural\n2Data is available at https://github.com/google-research-\ndatasets/KELM-corpus\nlanguage tokens in KELM sentences. Examples of\nthe inputs for both corpora are in Table 1.\n4 Experiments\nWe assess SKILL by training and evaluating the\nknowledge infused models on closed-book QA\ntasks, where questions are provided without sup-\nporting context and external knowledge.\n4.1 Experiment Setup\nSKILL pre-training. We apply SKILL on three\nT5.1.1 pre-trained checkpoints3, base, large, and\nXXL, with sizes of ∼250M, ∼800M and ∼11B\nparameters, respectively. For T5.1.1-base and -\nlarge, SKILL training is performed for 500K steps\nwith batch size 1024, which translates to ∼7.17\nepochs on Wikidata KG and ∼16.38 epochs in\nKELM sentences. For T5.1.1-XXL, the model is\ntrained for 100K steps to finish training in a feasible\ntime.\nAs baseline we use pre-trained T5 checkpoints\nof the same size. To make sure that improvements\ncome from knowledge infusion instead of from\nlonger C4 pre-training, we use a second baseline by\nfurther training the T5 checkpoints on C4 for half\nof the aforementioned steps, to match the amount\nof C4 pre-training used in SKILL.\nAll the model variations are optimized by\nAdaFactor (Shazeer and Stern, 2018) with 10−3\nlearning rate and 0.1 dropout rate, the same set-\ntings that were used for T5.\nFine-tuning on closed-book QA tasks. We\nevaluate the checkpoints by fine-tuning on the\nfollowing QA benchmarks: FreebaseQA (Jiang\net al., 2019), WikiHop (Welbl et al., 2018), Triv-\niaQA (Joshi et al., 2017) and NaturalQuestions\n(Kwiatkowski et al., 2019), with the aforemen-\ntioned hyper-parameters for optimization and 128\nbatch size. For the benchmarks without a test split,\nwe use the dev split for test, and the last 10% of\ntrain as dev split.\nThe Exact Match (EM) scores on the test sets\nare calculated after being fine-tuned for 50K steps\nfor T5.1.1-base and -large models, and 10K steps\nfor -XXL models. All models converged with no\nnoticeable over-fitting according to the EM scores\non validation sets.\nWikidata-answerable QA. We found that the\nmajority of the questions in FreebaseQA and Wiki-\n3https://goo.gle/t5-checkpoints\n1583\nWikidata triple KELM sentence Wikidata input KELM input Target\n(\"Pulp Fiction\",\n\"award received\",\n\"Palme d’Or\")\nQuentin Tarantino\nwon the Palme d’Or in 1994\nfor Pulp Fiction.\nPulp Fiction,\naward received,\n[MASK]\nQuentin Tarantino\nwon the [MASK] in 1994\nfor Pulp Fiction.\nPalme d’Or\nTable 1: Example inputs for SKILL pre-training with Wikidata and KELM corpora.\nModel FreebaseQA WikiHop TQA-matched TQA NQ-matched NQ\ndev test dev test dev test dev test dev test dev test\nbase 25.24 27 .55 19 .09 18 .38 31 .24 33 .55 22 .64 22 .93 36 .64 32 .68 25 .04 25 .48\nbase + C4 26.19 28 .33 19 .57 19 .36 32 .9 34 .4 24 .54 25 .39 36 .98 32 .03 25.88 25.84\nbase + WikiKG26.92 28.38 20.28 20.22 34.21 35.08 24 .73 25.77 37.41 33.33 25.51 25 .76\nbase + KELM 26.64 28 .15 20.62 19.81 33 .64 35.54 25.22 25.75 36 .98 32 .9 25 .31 26.2\nlarge 30.22 32 .88 20 .92 21 .12 36 .7 38 .09 29 .24 30 .03 39 .22 35 .06 27 .12 27 .15\nlarge + C4 32.55 34 .01 22 .5 21 .51 38 .78 40 .6 30 .32 30 .83 39 .74 35 .5 27 .46 28 .17\nlarge + WikiKG33.22 35.29 23.5 23 .4 39.19 41.02 29.74 30 .47 41.12 35.93 27.38 27 .89\nlarge + KELM 32.65 34 .16 23 .34 22 .91 39.45 40.76 30.51 30.65 40.95 35 .5 27.67 28.56\nXXL 43.67 45 .02 24 .76 24 .8 51 .73 53 .1 42 .44 42 .21 46 .47 43 .72 31 32 .27\nXXL + C4 42.01 44 .14 23 .34 22 .23 50 .59 52 .19 40 .66 40 .99 45 .43 40 .26 30 .35 31 .08\nXXL + WikiKG45.22 47.25 27.57 27.65 54.17 54.18 42 .55 43.54 49.14 44.37 31.11 32.74\nXXL + KELM45.42 45.9 26 .11 26 .26 53 .65 54.21 42.68 42.95 48 .53 44 .16 31.79 32.6\nTable 2: Exact match scores achieved by fine-tuning the checkpoints on closed-book QA tasks. base, large,\nXXL represent the corresponding T5.1.1-* checkpoints. *-C4 are the checkpoints additionally trained on C4 corpus\nas discussed in Section 3. *-WikiKG and *-KELM are the checkpoints trained on Wikidata KG triple corpus and\nKELM sentence corpus. The best performed checkpoints are in bold. Details about datasets are in Appendix B.\nHop can be answered directly from triples in Wiki-\ndata. This is because FreebaseQA was created by\nmatching question-answer pairs with triples in Free-\nbase (Bollacker et al., 2008), most of which was\nimported into Wikidata (Vrandeˇci´c and Krötzsch,\n2014). For WikiHop, the questions were generated\nfrom Wikidata triples.\nHowever, TriviaQA and NaturalQuestions were\ncreated independently of Wikidata, and not every\nquestion can be answered using this knowledge\nbase. We found frequent freshness issues, e.g. the\ngolden answer for question \"Who is the largest\nsupermarket chain in the UK?\" is \"Aldi\", while\ntoday it would be \"Tesco\". Some other questions\ncan not be answered by WikiData, e.g. \"Who, dur-\ning a radio microphone test in 1984 said, ’I just\nsigned legislation which outlaws Russia forever.\nThe bombing begins in five minutes?’\", with the\ngolden answer \"Ronald Reagan\".\nTo mitigate this, we created subsets of TriviaQA\n(TQA) and NaturalQuestions (NQ) that were some-\nwhat more likely to have answers in Wikidata. We\nselected all the items for which there exist a triple in\nWikidata that has the answer either as subject or ob-\nject, and the other entity in the triple is mentioned\nin the question. We match the entities by entity\nname, case-insensitive. We name the Wikidata-\naligned version of TQA and NQ as TQA-matched\nand NQ-matched, respectively. The dataset sizes\nof all QA tasks are summarized in Appendix B.\n4.2 Results\nThe results for closed-book QA tasks are sum-\nmarized in Table 2. SKILL pre-trained models\nshow improvements on FreebaseQA, WikiHop, and\nWikidata-answerable versions of TriviaQA and Nat-\nuralQuestions, but no significant improvement on\noriginal TriviaQA and NaturalQuestions. As dis-\ncussed in previous section, we believe this is due\nto the misalignment between the datasets and Wiki-\ndata.\nModels pre-trained on Wikidata KG gives com-\npetitive results with ones on KELM sentences. It\nshows that the triple representation is as good as\nnatural language representation, while being much\neasier to scale up for larger KG.\nFor T5.1.1-base and -large, additional pre-\ntraining on C4 boosts performance in comparison\nto the original baseline. For T5.1.1-XXL, this addi-\ntional pre-training leads to a performance regress.\nIn (Raffel et al., 2019), it is mentioned that training\non C4 for multiple times may reduce the perfor-\nmance of a T5 model.\nImpact of model size. As shown in Figure 1,\nSKILL pre-training introduces bigger improve-\nments when applied on larger models. With more\nthan 35M triples in Wikidata KG, it is harder for\n1584\nFigure 1: Performance improvements on closed-book\nQA tasks for different model sizes. The improvements\nare measured by the difference of exact match score\n(∆EM) between knowledge-infused model trained with\nWikidata triples and the baseline trained with C4 corpus.\nDataset Split Baseline + C4 + KG\n1-hop dev 24.3 23 .12 71.52\ntest 24.5 23 .53 71.47\n2-hop dev 32.05 32 .23 33.49\ntest 32.65 32 .78 33.57\n3-hop dev 42.08 39 .22 43.79\ntest 42.31 39 .66 43.41\nTable 3: Exact match scores achieved by fine-tuning\ndifferent T5.1.1-large checkpoints on MetaQA task.\nsmaller size models, e.g. T5.1.1.-base with 300M\nparameters, to memorize them efficiently. We view\nthis as an encouraging result, suggesting that as\nmodel size grows, gains from SKILL pre-training\nmay increase further.\nPerformance on a smaller KG. The Wiki-\nMovies KG (Miller et al., 2016) contains 134, 741\ntriples. T5.1.1-large should have enough parame-\nters to memorize the KG. We train a T5.1.1-large\nmodel on the KG for 100K steps, ∼380 epochs,\nwith the same hyperparameters as for Wikidata KG.\nWe evaluate the checkpoints with MetaQA (Zhang\net al., 2018) benchmark that was constructed over\nWikiMovies KG. The benchmark contains 3 dif-\nferent sub-tasks: 1-hop QA (e.g. \"What films\ndoes Paresh Rawal appear in?\"), 2-hop QA (e.g.\n\"Who are the directors of the films written by Laura\nKerr?\"), 3-hop QA (e.g. \"Who directed the movies\nwritten by the writer of Millennium Actress?\").\nThe results in Table 3 demonstrate the effective-\nness of SKILL pre-training, when it’s possible to\nmemorize the whole knowledge graph.\nAs 1-hop questions are supported by single\ntriples in the WikiMovies KG, a 3×improvement\non EM score is observed for the sub-task. In or-\nder to answer 2/3-hop questions it is not enough to\nmemorize the triples, the model needs to be able\nto reason with them. This requires a better un-\nderstanding of the graph structure. Training with\nsingle triples may not be enough, and the observed\nimprovement is notably smaller. The performance\ncould be further improved by representing more\nexplicitly the graph structure in the training data,\nwhich we leave for future work.\n5 Conclusion\nWe proposed a method to directly infuse knowledge\nfrom knowledge graphs into T5 models through\npre-training. Empirical results show that T5 can\nlearn directly from structured data and apply the\nlearned knowledge to improve closed-book QA\nresults. We also demonstrated that the models\npre-trained on factual triples perform competitively\nwith the ones on natural language sentences that\ncontain the same knowledge. By enabling knowl-\nedge infusion directly from triples, this method can\nbe very easily applied to industry-scale KGs.\n6 Ethical and Broader Impact\nIn this work, we are introducing a new method\nto pre-train a well known natural language under-\nstanding model, T5, on the full corpora of public\nknowledge graphs. To the best of our knowledge,\nthe method will not introduce extra bias to either\nthe model or the dataset beyond the one potentially\ninherited from Wikidata (Vrandeˇci´c and Krötzsch,\n2014) and WikiMovies (Miller et al., 2016) knowl-\nedge graphs. On the other hand, through knowl-\nedge fusion pre-training introduced in this work,\na language model will be able to learn factual\ninformation to improve the quality of parameter-\nized knowledge embedded in the model, which is\ndemonstrated by improvements on various closed-\nbook question-answering tasks. The proposed\nmethod and recipe will provide positive impact\nto the natural language processing community and\nhelp to improve the factualness in pre-trained large\nlanguage model checkpoints.\nLimitations. A factual triple is the basic ingredi-\nent of a knowledge graph. However, as a seman-\ntic network, the graph structure of a knowledge\ngraph describes how the factual triples are con-\nnected. This information is not easy to directly\nrepresent by random set of triples. We leave the ex-\nploration of how to infuse the information implied\nby the graph structure for future work. We expect\nthat this will further improve the results, especially\nfor multi-hop question-answering tasks.\n1585\nReferences\nOshin Agarwal, Heming Ge, Siamak Shakeri, and Rami\nAl-Rfou. 2021. Knowledge graph based synthetic\ncorpus generation for knowledge-enhanced language\nmodel pre-training. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 3554–3565, Online. As-\nsociation for Computational Linguistics.\nKurt Bollacker, Colin Evans, Praveen Paritosh, Tim\nSturge, and Jamie Taylor. 2008. Freebase: A col-\nlaboratively created graph database for structuring\nhuman knowledge. In Proceedings of the 2008 ACM\nSIGMOD International Conference on Management\nof Data, SIGMOD ’08, page 1247–1250, New York,\nNY , USA. Association for Computing Machinery.\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, Asli Celikyilmaz, and Yejin Choi.\n2019. COMET: Commonsense transformers for auto-\nmatic knowledge graph construction. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 4762–4779, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nThibault Févry, Livio Baldini Soares, Nicholas FitzGer-\nald, Eunsol Choi, and Tom Kwiatkowski. 2020. En-\ntities as experts: Sparse memory access with entity\nsupervision. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 4937–4951, Online. Association\nfor Computational Linguistics.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Mingwei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In International Con-\nference on Machine Learning , pages 3929–3938.\nPMLR.\nBin He, Di Zhou, Jinghui Xiao, Xin Jiang, Qun Liu,\nNicholas Jing Yuan, and Tong Xu. 2020. BERT-MK:\nIntegrating graph contextualized knowledge into pre-\ntrained language models. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2020,\npages 2281–2290, Online. Association for Computa-\ntional Linguistics.\nBenjamin Heinzerling and Kentaro Inui. 2021. Lan-\nguage models as knowledge bases: On entity repre-\nsentations, storage capacity, and paraphrased queries.\nIn Proceedings of the 16th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics: Main Volume, pages 1772–1791, Online.\nAssociation for Computational Linguistics.\nGautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\npages 874–880, Online. Association for Computa-\ntional Linguistics.\nShaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Martti-\nnen, and Philip S. Yu. 2020. A survey on knowledge\ngraphs: Representation, acquisition and applications.\nCoRR, abs/2002.00388.\nKelvin Jiang, Dekun Wu, and Hui Jiang. 2019. Free-\nbaseQA: A new factoid QA data set matching trivia-\nstyle question-answer pairs with Freebase. In Pro-\nceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume\n1 (Long and Short Papers) , pages 318–323, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1601–1611, Vancouver,\nCanada. Association for Computational Linguistics.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: A benchmark for question answering\nresearch. Transactions of the Association for Compu-\ntational Linguistics, 7:452–466.\nAlexander Miller, Adam Fisch, Jesse Dodge, Amir-\nHossein Karimi, Antoine Bordes, and Jason Weston.\n2016. Key-value memory networks for directly read-\ning documents. In Proceedings of the 2016 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1400–1409, Austin, Texas. Associ-\nation for Computational Linguistics.\n1586\nGeorge A. Miller. 1995. Wordnet: A lexical database\nfor english. Commun. ACM, 38(11):39–41.\nBarlas Oguz, Xilun Chen, Vladimir Karpukhin, Stan\nPeshterliev, Dmytro Okhonko, Michael Schlichtkrull,\nSonal Gupta, Yashar Mehdad, and Scott Yih. 2020.\nUniK-QA: Unified representations of structured and\nunstructured knowledge for open-domain question\nanswering. arXiv preprint arXiv:2012.14610.\nMatthew E. Peters, Mark Neumann, Robert Logan, Roy\nSchwartz, Vidur Joshi, Sameer Singh, and Noah A.\nSmith. 2019. Knowledge enhanced contextual word\nrepresentations. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 43–54, Hong Kong, China. Association for\nComputational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. CoRR, abs/1910.10683.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418–5426,\nOnline. Association for Computational Linguistics.\nSebastian Ruder, Matthew E. Peters, Swabha\nSwayamdipta, and Thomas Wolf. 2019. Transfer\nlearning in natural language processing. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational\nLinguistics: Tutorials , pages 15–18, Minneapo-\nlis, Minnesota. Association for Computational\nLinguistics.\nMaarten Sap, Ronan Le Bras, Emily Allaway, Chan-\ndra Bhagavatula, Nicholas Lourie, Hannah Rashkin,\nBrendan Roof, Noah A Smith, and Yejin Choi. 2019.\nAtomic: An atlas of machine commonsense for if-\nthen reasoning. In Proceedings of the AAAI Confer-\nence on Artificial Intelligence, volume 33.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\nCoRR, abs/1804.04235.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge. In Thirty-first AAAI conference on\nartificial intelligence.\nYu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding,\nChao Pang, Junyuan Shang, Jiaxiang Liu, Xuyi Chen,\nYanbin Zhao, Yuxiang Lu, et al. 2021. Ernie 3.0:\nLarge-scale knowledge enhanced pre-training for lan-\nguage understanding and generation. arXiv preprint\narXiv:2107.02137.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nPat Verga, Haitian Sun, Livio Baldini Soares, and\nWilliam Cohen. 2021. Adaptable and interpretable\nneural MemoryOver symbolic knowledge. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n3678–3691, Online. Association for Computational\nLinguistics.\nDenny Vrandeˇci´c and Markus Krötzsch. 2014. Wiki-\ndata: A free collaborative knowledgebase. Commu-\nnications of the ACM, 57(10):78–85.\nJohannes Welbl, Pontus Stenetorp, and Sebastian Riedel.\n2018. Constructing datasets for multi-hop reading\ncomprehension across documents. Transactions of\nthe Association for Computational Linguistics, 6:287–\n302.\nDonghan Yu, Chenguang Zhu, Yuwei Fang, Wenhao\nYu, Shuohang Wang, Yichong Xu, Xiang Ren, Yim-\ning Yang, and Michael Zeng. 2021. Kg-fid: In-\nfusing knowledge graph in fusion-in-decoder for\nopen-domain question answering. arXiv preprint\narXiv:2110.04330.\nYuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexan-\nder J Smola, and Le Song. 2018. Variational reason-\ning for question answering with knowledge graph. In\nAAAI.\nA Matching of entities in KELM\nsentences\nTo find Wikidata KG entities in corresponding\nKELM sentences, we use Algorithm 1. Additional\ncycle on line 22 is needed because some entities\nhave an information in brackets that should not\nbe in a sentence, for example John Doe (born\n1990). This algorithm matched at least one en-\ntity to 15, 383, 248 out of 15, 628, 486 KELM sen-\ntences.\nWe don’t try to match relation part of triples,\nbecause it could be represented in many different\nforms. For example, the triple (Pulp Fiction,\ncast member, John Travolta) could\nbe represented as \" John Travolta was\nan actor in Pulp Fiction\", \" John\nTravolta starred in Pulp Fiction\",\n\"John Travolta played Vincent\nVega in Pulp Fiction\", etc., and there is\nno way to robustly align a relation to all possible\nsurface forms.\n1587\nAlgorithm 1KELM-Wikidata matching algorithm\nthat finds spans in KELM sentences corresponding\nto Wikidata KG entities. a ⊂b means that a is a\nsubstring of b. ∗represents any string.\n1: KELMmatched ←∅\n2: for eachk ∈KELM sentences do\n3: for eacht ∈triples(k) do\n4: for eache ∈entities(t) do\n5: ep ←PREPROCESS (e)\n6: kp ←PREPROCESS (k)\n7: spans ←MATCH ENTITY (ep, kp)\n8: KELMmatched.insert([k, spans])\n9: end for\n10: end for\n11: end for\n12:\n13: function MATCH ENTITY (e: entity, k: KELM\nsentence)\n14: spans ←∅\n15: for eachs ⊂k : date(e) = date(s) do\n16: spans.insert(s)\n17: end for\n18: for each∃s ⊂k : e = s do\n19: spans.insert(s)\n20: end for\n21: if spans = ∅then\n22: for each∃s ⊂k : e = s+\" (*)\" do\n23: spans.insert(s)\n24: end for\n25: end if\n26: return spans\n27: end function\n28:\n29: function PREPROCESS (str: string)\n30: str ←Lowercase(str)\n31: str ←RemovePunctuation (str)\n32: return str\n33: end function\nB Dataset\nWikidata (Vrandeˇci´c and Krötzsch, 2014) was re-\nleased under the Creative Commons CC0 License.\nKELM (Agarwal et al., 2021) was released under\nthe Creative Commons CC BY-SA 2.0 License.\nNaturalQuestions (Kwiatkowski et al., 2019) and\nWikiHop (Welbl et al., 2018) were released un-\nder Creative Commons CC BY-SA 3.0 License.\nMetaQA (Zhang et al., 2018) was released under\nCreative Commons CC BY-ND 3.0 License. C4\n(Raffel et al., 2019) and TriviaQA (Joshi et al.,\n2017) were released under Apache-2.0 License.\nWikiMovies (Miller et al., 2016) was released un-\nder MIT License. FreebaseQA (Jiang et al., 2019)4\nwas released without a license.\ntrain dev test\nFreebaseQA 20, 358 3 , 994 3 , 996\nWikiHop 39, 364 4 , 374 5 , 129\nTQA 78, 785 8 , 837 11 , 313\nTQA-matched 20, 948 2 , 289 3 , 064\nNQ 79, 168 8 , 757 3 , 610\nNQ-matched 10, 487 1 , 160 462\nMetaQA-1hop 96, 106 9 , 992 9 , 947\nMetaQA-2hop 118, 980 14 , 872 14 , 872\nMetaQA-3hop 114, 196 14 , 274 14 , 274\nTable 4: Dataset sizes for the closed-book QA tasks.\nTQA and NQ stands for TriviaQA and NaturalQuestions,\nrespectively. *-matched are the selected dataset with\nthe Wikidata KG answerable questions, and the KG\nalignment details can be found in Section 4.1.\n4https://github.com/kelvin-jiang/\nFreebaseQA\n1588"
}