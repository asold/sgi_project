{
  "title": "What can we gain from language models for morphological inflection?",
  "url": "https://openalex.org/W2954161305",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2289912120",
      "name": "Alexey Sorokin",
      "affiliations": [
        "Lomonosov Moscow State University",
        "Moscow Institute of Physics and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963077125",
    "https://openalex.org/W2886452452",
    "https://openalex.org/W2963635689",
    "https://openalex.org/W2597891111",
    "https://openalex.org/W2508815538",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2517380710",
    "https://openalex.org/W2962680795",
    "https://openalex.org/W2963752701",
    "https://openalex.org/W2625720409",
    "https://openalex.org/W2952332047",
    "https://openalex.org/W2294942296"
  ],
  "abstract": "This paper investigates the attempts to augment neural-based inflection models with characterbased language models.We found that in most cases this slightly improves performance, however, the effect is marginal.We also propose another language-model based approach that can be used as a strong baseline in low-resource setting.",
  "full_text": "Proceedings of the CoNLL–SIGMORPHON 2018 Shared Task: Universal Morphological Reinﬂection, pages 99–104,\nBrussels, Belgium, October 31, 2018.c⃝2018 Association for Computational Linguistics\nWhat can we gain from language models for morphological inﬂection?\nAlexey Sorokin\nMoscow Institute of Physics and Technology / Institutskij per., 9,\nFaculty of Innovations and High Technologies, 141701, Dolgoprudny, Russia\nMoscow State University / GSP-1, Leninskie Gory, 1\nFaculty of Mathematics and Mechanics, 119991, Moscow, Russia\nalexey.sorokin@list.ru\nAbstract\nThis paper investigates the attempts to augment\nneural-based inﬂection models with character-\nbased language models. We found that in most\ncases this slightly improves performance, how-\never, the eﬀect is marginal. We also propose\nanother language-model based approach that\ncan be used as a strong baseline in low-resource\nsetting.\n1 Introduction\nMorphological inﬂection is a task of automatic\nreconstruction of surface word form given its\nsource form, called lemma, and morphological\ncharacteristic of required form. For example, in\nSpanish the input word contar together with fea-\ntures v;ﬁn;ind;imp;pst;3;pl should be transformed\nto contaban. The obvious way to solve such a\ntask is to handcode transformations using ﬁnite-\nstate rules. However, this approach requires an\nexpert knowledge of the language under consid-\neration and can be extremely time-consuming for\nthe languages with complex morphology. There-\nfore a machine learning algorithm should be devel-\noped to eﬃciently solve this task for any language.\nSuch an algorithm must be able to generalize from\nknown lemma-features-word triples to previously\nunseen ones, mimicking human behaviour when\ninﬂecting a neologism in its native language or\nan unknown word in a foreign one. In this set-\nting automatic inﬂection becomes an instance of\nstring transduction problem, which makes condi-\ntional random ﬁelds a natural baseline model as\nsuggested in (Nicolai et al., 2015). Another pop-\nular approach is to predict transformation pattern,\neither as a pair of preﬁx and suﬃx changes as\nused in the baseline model for Sigmorphon 2018\nShared Task (Cotterell et al., 2018). For example,\nconsider a Czech adjective kr´asn´y and its superla-\ntive form nejkr´aˇsnejˇs´ı. The inﬂection pattern can\nbe encoded as a pair of preﬁx rule $ →$nej and\na suﬃx rule ´y →ejˇs´ı. Such encoding is, however,\ntoo weak to deal with inﬁxation and root vowel al-\nterations, required, for example, for Spanish verb\nvolver and its +Pres+Sg+1 form vuelvo. An ab-\nstract paradigm approach (Ahlberg et al., 2015;\nSorokin, 2016) compresses this transformation to\n1+o+2+er#1+ue+2+o, where digits stand for vari-\nables (the parts of verb stem), and constant frag-\nments deﬁne the paradigm. In both cases in order\nto predict the inﬂected word form one suﬃces to\nguess the transformation pattern, thus solving a\nstandard classiﬁcation problem. Both mentioned\nmodels (CRFs and abstract paradigms) were quite\nsuccessful in Sigmorphon 2016 Shared Task (Cot-\nterell et al., 2016), however, they were clearly out-\nperformed by neural network approaches.\nIndeed, string transduction problems are suc-\ncessfully solved using neural methods, for exam-\nple, in machine translation. The work of (Kann and\nSch¨utze, 2016) adopts the seq2seq model with soft\nattention of (Bahdanau et al., 2014). It defeated\nnot only non-neural systems mentioned earlier, but\nalso other neural approaches. It shows that at-\ntention mechanism is crucial for word inﬂection.\nHowever, in contrast to machine translation, a sym-\nbol of output word is less prone to depend from\nmultiple input symbols, than a translated word\nfrom multiple source words. Consequently, the at-\ntention weight is usually concentrated on a single\nsource symbol, being more a pointer than a dis-\ntributed probability mass. Moreover, this pointer\ntraverses the source word from left to right in order\nto generate the inﬂected form. All that motivated\nthe hard attention model of (Aharoni and Gold-\nberg, 2017), which outperformed the soft attention\napproaches. The key feature of this model is that\nit predicts not only the output word, but also the\nalignment between source and target using an ad-\nditional step symbol which shifts the pointer to the\n99\nnext symbol. This model was further improved\nby (Makarov et al., 2017), whose system was the\nwinner of Sigmorphon 2017 evaluation campaign\n(Cotterell et al., 2017). The approach of Makarov\net al. was especially successful in low and medium\nresource setting, while in high resource setting it\nachieves an impressive accuracy of over 95% 1.\nDoes it mean that no further research is re-\nquired and hard attention method equipped with\ncopy mechanism is the ﬁnal solution for auto-\nmatic inﬂection problem? Actually, not, since the\nquality of the winning approach was much lower\non medium (about 85%) and low (below 50%)\ndatasets. This lower quality is easy to explain\nsince in low resource setting the system might\neven see no examples of the required form 2 or\nobserve just one or two inﬂection pairs which do\nnot cover all possible paradigms for this partic-\nular form. For example, Russian verbs has sev-\neral tens of variants to produce the +Pres+Sg+1\nform. Consequently, to improve the inﬂection ac-\ncuracy the system should extract more information\nfrom the whole language, not only the instances\nof the given form. This task is easier for aggluti-\nnative languages with regular inﬂection paradigm:\nto predict, say, the +Pres+Sg+1 form in Turkish,\nthe system has just to observe several singular verb\nform (not necessarily of the ﬁrst person) to extract\nthe singular suﬃx and several ﬁrst person form\n(of any number and tense). In presence of fusion,\nlike in Russian and other Slavonic languages, the\ndecomposition is not that easy or even impossible.\nHowever, this decomposition is already realised\nin model of (Makarov et al., 2017) since the gram-\nmatical features are treated as a list of atomic ele-\nments, not as entire label. A new source of infor-\nmation about the whole language are the laws of\nits phonetics. For example, to detect the vowel in\nthe suﬃx of the Turkish verb one do not need to\nobserve any verbs at all, but to extract the vowel\nharmony patterns from the inﬂection of nouns. A\nnatural way to capture the phonetic patterns are\ncharacter language models. They were already\napplied to the problem of inﬂection in (Sorokin,\n2016) and produced a strong boost over the base-\nline system. The work of Sorokin used simple\nngram models, however, neural language models\n1Averaged over all languages of (Cotterell et al., 2017)\ndataset\n2it was provided with 100 inﬂection pairs for entire lan-\nguage, which is often several times lower than the number of\npossible grammeme combinations\n(Tran et al., 2016) has shown their superiority over\nearlier approaches for various tasks.\nSummarizing, our approach was to enrich the\nmodel of (Makarov et al., 2017) with the language\nmodel component. We followed the architecture\nof (Gulcehre et al., 2017), whose approach is sim-\nply to concatenate the state of the neural decoder\nwith the state of the neural language model before\npassing it to the output projection layer. We ex-\npected to improve performance especially in low\nand medium resource setting, however, our ap-\nproach does not have clear advantages: our joint\nsystem is only slightly ahead the baseline system of\n(Makarov et al., 2017) for most of the languages.\nWe conclude that the language model job is already\nexecuted by the decoder. However, given the vi-\ntality of language model approach in other areas\nof modern NLP (Peters et al., 2018), we describe\nour attempts in detail to give other researchers the\nideas for future work in this direction.\n2 Model structure\n2.1 Baseline model\nAs the state-of-the-art baseline we choose the\nmodel of Makarov et al. (Makarov et al., 2017),\nthe winner of previous Sigmorphon Shared Task.\nThis system is based on earlier work of Aharoni\nand Goldberg (Aharoni and Goldberg, 2017). We\nbrieﬂy describe the structure of baseline model (we\ncall it AGM-model further) and refer the reader to\nthese two papers for more information. AGM-\nmodel consists of encoder and decoder, where an\nencoder is just a bidirectional LSTM. Each ele-\nment of the input sequence contains a 0-1 encod-\ning of a current letter and two LSTMs traverse this\nsequence in opposite directions. After encoding,\neach element of obtained sequence contains infor-\nmation about current letter and its context.\nThe main feature of the encoder is that it operates\non the level on alignments, not on the level of letter\nsequences. Assume a pair volver-vuelvo appears in\nthe training set. The natural alignment is\nv o l v e r\nv u e l v o\nIt is transformed to the source-target pair in Fig-\nure 1. Here the step symbol denotes pointer shift,\nfor precise algorithm of transformation see (Aha-\nroni and Goldberg, 2017):\nThe decoder is one-directional LSTM. It obtains\n100\nbegin v step u e step l step v step o step step end\nbegin v v o o o l l v v e e r end\nFigure 1: Transformation of alignment to source-target pair.\nas input the lower string of Figure 1. Let ibe the\nnumber of current timestep and jbe current posi-\ntion in the input string. On i-th step the decoder\ntakes a concatenation of 3 vectors: xj — the j-th\nelement in the output of the encoder, ˜f = Wfeatf\n— the embedding of the grammatical feature vec-\ntor and gi = Wembyi−1 — the embedding of previ-\nous output symbol. The feature vector is obtained\nas 0/1-encoding of the list of grammatical features.\nWe actually take the concatenation of output vec-\ntors for d≥1 previous output symbols as yi−1, in\nour experiments dwas set to 4.\nOn each step the decoder produces a vector zi\nas output and propagates updated hidden state vec-\ntor hi to the next timestep. zi is then passed to a\ntwo-layer perceptron with ReLU activation on the\nintermediate layer and softmax activation on the\noutput layer, which produces the output distribu-\ntion pi over output letters, formally:\nˆzi = max (Wpzi + bp,0),\npi = softmax( Woˆzi + bo),\nyi = argmax kpik\nIf yi is the index of step symbol, we move the\npointer to the next input letter. We also use the copy\ngate from (Makarov et al., 2017): since the neural\nnetwork copies the vast majority of its symbols,\nthe output distribution ˆpi is obtained as a weighted\nsum of singleton distribution which outputs current\ninput symbol and the preliminary distribution pi\nspeciﬁed above. The weight σi is the output of\nanother one-layer perceptron:\nσi = sigmoid( Wσzi + bσ),\nˆpi = σiI(k= cj) + (1−σi)pi,\nyi = argmax k ˆpik\n2.2 Character-based model\nOur proposal is to explicitly equip the decoder with\nthe information from the character-based language\nmodel. We suppose it will help the model to avoid\noutputting phonetically implausible sequences of\nletters. We choose the simplest possible architec-\nture of the language model, namely, on each step it\ntakes a concatenation of dprevious symbol embed-\ndings ui = [gi−d,...,g i−1] and applies an LSTM\ncell to obtain a vector vi and update LSTM hid-\nden state hi. vi is propagated through a two-layer\nperceptron to predict the next output symbol anal-\nogously to the output layer of the baseline model:\nˆui = max (WLM\np ui + bLM\np ,0),\npLM\ni = softmax( WLM\no ˆui + bLM\no ),\nyi = argmax kpLM\nik\nThe model is trained to predict next output sym-\nbol separately from the basic model. In principle,\none can use more complex neural architectures,\nfor example, a multilayer LSTM or apply atten-\ntion mechanism. However, our preliminary ex-\nperiments have shown that attention over recent\nhistory as in (Tran et al., 2016) leads to slightly\nworse performance.\nTo join the baseline model and the language\nmodel we concatenate the decoder output zi with\nthe analogous vector from the language model\nzLM\ni . The language model is conditioned over\npreviously output vectors (excluding step symbol).\nThat is the fusion mechanism as used in (Gulcehre\net al., 2017). We also experimented with concate-\nnating the pre-output vectors ˆzi,ˆzLM\ni , however, the\nformer variant leads to slightly better performance.\nTo avoid exposure bias we mask language model\nstate with all zeros with the probability of 0.4 (it\nteaches the model to recover from language model\nerrors).\n3 Data and implementation\n3.1 Implementation\nThe initial alignment was obtained using longest\ncommon subsequence (LCS) method. Then this\nalignment was optimized using Chinese Restau-\nrant process as in (Cotterell et al., 2016). The\noptimization phase did 5 passes over training data.\nThe aligner trained on the training set was also\nused to align the validation data.\nWe implemented our model using Keras library\nwith Tensorﬂow backend 3. For all the setting we\nused the encoder with 96 hidden units in each di-\nrection, the decoder contained 128 units and the\n3https://github.com/AlexeySorokin/\nSigmorphon2018SharedTask\n101\npre-output projection layer was of dimension 96.\nMorphological features were embedded to 48 di-\nmensions. We used batch size of 32 when training,\nthe batches contained the words of approximately\nthe same size to reduce the amount of padding. We\ntrained the model for 100 epochs with Adam opti-\nmizer, training was stopped when the accuracy on\nthe validation data did not improve for 15 epochs.\nDuring decoding, the beam search of width 10 was\napplied.\nWhen learning the weights of a language model,\nwe used the same training and validation sets as for\ninﬂection network. The language model used his-\ntory of 5 symbols and contained 64 units in LSTM\nlayers. The number of layers was set to 2. The\nrate of dropout was the same as for basic model.\nThe model was trained for 20 epochs, training was\nstopped when perplexity on validation set did not\nimprove for 5 epochs.\n3.2 Dataset\nWe tested our model in Sigmorphon 2018 Shared\nTask (Cotterell et al., 2018). For an extended de-\nscription we refer the reader to this papers. The\ndataset contained three subsets: high, medium and\nlow. The size of the training dataset was 10000\nwords in the high subset 4, 1000 in medium and\n100 in low. The dataset also contained a devel-\nopment set containing 1000 instances most of the\ntime, for all languages we used this subset as val-\nidation data. Overall, there were 86 languages in\nthe high setting, 102 in medium and 103 in low.\n4 Results and discussion\nWe submitted three systems, one replicating the\nalgorithm of (Makarov et al., 2017), the second\nequipped with language models. The third one\nused only the language models: we extracted all\npossible abstract inﬂection paradigms for a given\nset of grammatical features and created a set of pos-\nsible candidate forms applying all paradigms to the\nlemma. For example, consider the word делать\nand paradigms 1+ать#1+ет, 1+ать#1+ит, 1+ь#1\nand 1+чь#1+жет; the ﬁrst three produce the forms\nделает, делит, делат, while the fourth yields\nnothing since the given word does not end in -чь.\nThen all these forms are ranked using sum of log-\narithmic probabilities from forward and backward\nlanguage models.\n4For several languages it was smaller, but exceeded 1000\ninstances\nOur results are mostly negative, since our\nlanguage-model based architecture produced only\nmarginal improvement over the model of Makarov\net al. which it is based on. Moreover, for the low-\nresource setting the performance of both system\nwas mediocre, even our third paradigm-based sys-\ntem was able to overperform them despite its obvi-\nous weakness. The results are presented in Table\n15, M1 stands for the baseline model and M2 – for\nthe LM-based one. The numbers in brackets count\nthe number of large gaps (more than 2% for high\ndataset, 3% for medium and 5% for low).\nWe observe that the inﬂuence of language mod-\nels is marginal, the strength of this eﬀect grows\nwith the size of training data, which contradicts\nour expectations. In low and medium setting we\nexpected slightly higher performance, which prob-\nably implies that our choice of hyperparameters is\nsuboptimal. We made several observations when\ncomparing our two models: ﬁrst, the LM-based\none demonstrates the highest quality after reduc-\ning output history of the baseline model from 4 to\n2 and setting LM state dropout to 0.4. It shows\nthat memory containing last output symbols plays\nthe role of a language model for local dependencies\nand the memory of LSTM encoder – for global and\noften there is no need to duplicate them. However,\nmost of the time LM-based variant converges much\nfaster which implies that language model learns to\nthrow out incorrect sequences of letters, but seems\nto overﬁt in the same time. In any case, these\nquestions require future investigation.\nHowever, language models demonstrate its util-\nity even when little training data is available. The\nresults for low subtask (see 2) demonstrate that\nthey are powerful enough to discriminate between\ncorrect and incorrect variants proposed by the ab-\nstract paradigm generator. This is especially im-\npressive since this method simply returns the input\nform in case it has not seen the given set of gram-\nmatical features. So it cannot recover the value\nof missing paradigm cells generalizing other ele-\nments of the paradigm table, which clearly limits\nits performance. Moreover, even for an observed\ngrammatical values a small training set does not\ncover all possible inﬂection patterns, either due to\ntheir irregularity and multiplicity, like in case of\nArabic or Latin, or complex phonetic rules as in\ncase of Navajo. Nevertheless, this approach clearly\n5We measured accuracy on the test subset of the dataset\nand averaged the scores over all languages.\n102\nDataset M1 M2 M1 >M 2 M2 >M 1\nhigh 94.23 94.56 27(2) 45(6)\nmedium 79.37 79.51 40(8) 47(12)\nlow 39.13 39.18 49(7) 50(10)\nTable 1: Comparison of baseline and LM-equipped models\nbeats our neural models since it requires less data\nwhen the number of possible inﬂection patterns is\nsmall.\nSo language models are actually good in rank-\ning inﬂection variants even in case of little data\navailable. What remains is to generate enough\ncandidate forms to improve their recall. We tried\nto solve this problem by adding top 10 candidates\nproposed by the neural network model to the list\nof possible outputs. However, this approach fails:\nfor most languages the results fall below the level\nof neural models themselves. Doing a quick error\nanalysis, we found that in low setting neural net-\nworks often are not able to discriminate between\ndiﬀerent forms, predicting a correct variant for an-\nother tense or person. The language model also\ndoes not learn enough well to distinguish diﬀer-\nent inﬂectional aﬃxes due to the same lack of data.\nTherefore it favors either a shorter form or the end-\nings it has observed more frequently, even if these\nendings does not refer to the set of features under\nconsideration. On the contrary, abstract paradigms\nsimply do not produce these variants, making the\nchoice more easy. A possible workaround may be\nto predict the set of grammatical features for the\ngenerated form, however, we have not implemented\nthis method due to the lack of time.\nThis reranking approach appears to be less suc-\ncessful for medium and high datasets. In this case\nthe number of proposed candidate paradigms be-\ncomes too high. Some of these paradigms gener-\nate phonetically plausible forms but are applicable\nonly in particular conditions not satisﬁed by a given\nword. For example, consider the Russian input\nделать;v;prs;ind;3;sg; the paradigm 1+ать#1+ит\nproduces the form делит, which is correct, but\nfor another verb делить. Therefore the applica-\ntion of language models in case of more training\ndata looks problematic: we tried to use them to ﬁl-\nter out forms generated by neural models without\nreranking remaining candidates. That marginally\nimproved performance for complex languages like\nNavajo and Latin but had a slight negative eﬀect in\nmost other cases.\nAcknowledgements\nI thank the organizers of Sigmorphon2018 Shared\ntask, especially Ryan Cotterell, for preparing the\ndata and organizing the Shared Task. I am grate-\nful to all the members of Neural Networks and\nDeep Learning Lab at MIPT ( http://ipavlov.\nai ) for helpful discussions during my investiga-\ntions. The research was conducted under sup-\nport of National Technological Initiative Founda-\ntion and Sberbank of Russia. Project identiﬁer\n0000000007417F630002.\n5 Conclusion\nWe investigated the applications of character lan-\nguage models to automatic reinﬂection. Despite\ntheir usefulness for other task, they do not pro-\nduce signiﬁcant boost, though improve the quality\nfor all the settings. However, reranking-based ap-\nproach, which also uses language models, reaches\nslightly higher scores in case of low amount of\ntraining data. In case of larger training sets the\nphonetic plausibility is eﬀectively checked by the\nneural decoder itself without applying additional\nmechanisms. The relative success of paradigm-\nbased approach in low-resource setting implies that\nneural networks lack control mechanism provided\nby abstract paradigms. Therefore the combina-\ntion of neural networks with ﬁnite state techniques\nseems a perspective direction of study. Another\npromising direction not touched in the current work\nare diﬀerent methods of data augmentation, either\nby training on data from related languages, or by\ngenerating additional training instances. At least\nfor the second approach character language mod-\nels seem useful to check the quality of generated\nsource-target pairs.\nReferences\nRoee Aharoni and Yoav Goldberg. 2017. Morphologi-\ncal inﬂection generation with hard monotonic atten-\ntion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers) , volume 1, pages 2004–2015.\n103\nDataset M1 M2 M1 >M 2 M2 >M 1\nlow 39.13 41.61 49(37) 52(44)\nTable 2: Comparison of baseline and LM-paradigm models\nMalin Ahlberg, Markus Forsberg, and Mans Hulden.\n2015. Paradigm classiﬁcation in supervised learning\nof morphology. In Proceedings of the Conference of\nthe North American Chapter of the Association for\nComputational Linguistics Human Language Tech-\nnologies (NAACL-HLT 2015), Denver, CO , pages\n1024–1029.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473.\nRyan Cotterell, Christo Kirov, John Sylak-Glassman,\nG´eraldine Walther, Ekaterina Vylomova, Arya D.\nMcCarthy, Katharina Kann, Sebastian Mielke, Gar-\nrett Nicolai, Miikka Silfverberg, David Yarowsky,\nJason Eisner, and Mans Hulden. 2018. The CoNLL–\nSIGMORPHON 2018 shared task: Universal mor-\nphological reinﬂection. In Proceedings of the\nCoNLL–SIGMORPHON 2018 Shared Task: Univer-\nsal Morphological Reinﬂection , Brussels, Belgium.\nAssociation for Computational Linguistics.\nRyan Cotterell, Christo Kirov, John Sylak-Glassman,\nG˙eraldine Walther, Ekaterina Vylomova, Patrick\nXia, Manaal Faruqui, Sandra K ¨ubler, David\nYarowsky, Jason Eisner, et al. 2017. CoNLL–\nSIGMORPHON 2017 shared task: Universal mor-\nphological reinﬂection in 52 languages. Proceedings\nof the CoNLL SIGMORPHON 2017 Shared Task:\nUniversal Morphological Reinﬂection, pages 1–30.\nRyan Cotterell, Christo Kirov, John Sylak-Glassman,\nDavid Yarowsky, Jason Eisner, and Mans Hulden.\n2016. The SIGMORPHON 2016 shared task—\nmorphological reinﬂection. In Proceedings of the\n2016 Meeting of SIGMORPHON , Berlin, Germany.\nAssociation for Computational Linguistics.\nCaglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun\nCho, and Yoshua Bengio. 2017. On integrating a lan-\nguage model into neural machine translation. Com-\nputer Speech & Language , 45:137–148.\nKatharina Kann and Hinrich Sch ¨utze. 2016. Single-\nmodel encoder-decoder with explicit morphologi-\ncal representation for reinﬂection. arXiv preprint\narXiv:1606.00589.\nPeter Makarov, Tatiana Ruzsics, and Simon Clematide.\n2017. Align and copy: Uzh at sigmorphon 2017\nshared task for morphological reinﬂection. Pro-\nceedings of the CoNLL SIGMORPHON 2017 Shared\nTask: Universal Morphological Reinﬂection , pages\n49–57.\nGarrett Nicolai, Colin Cherry, and Grzegorz Kondrak.\n2015. Inﬂection generation as discriminative string\ntransduction. In Proceedings of the Conference of\nthe North American Chapter of the Association for\nComputational Linguistics Human Language Tech-\nnologies (NAACL-HLT 2015), Denver, CO , pages\n923–931.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. arXiv preprint arXiv:1802.05365 .\nAlexey Sorokin. 2016. Using longest common sub-\nsequence and character models to predict word\nforms. In Proceedings of the 14th SIGMORPHON\nWorkshop on Computational Research in Phonetics,\nPhonology, and Morphology , pages 54–61.\nKe Tran, Arianna Bisazza, and Christof Monz. 2016.\nRecurrent memory networks for language modeling.\narXiv preprint arXiv:1601.01272 .\n104",
  "topic": "Inflection",
  "concepts": [
    {
      "name": "Inflection",
      "score": 0.8931879997253418
    },
    {
      "name": "Computer science",
      "score": 0.7133409380912781
    },
    {
      "name": "Natural language processing",
      "score": 0.5039021372795105
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4095318615436554
    },
    {
      "name": "Linguistics",
      "score": 0.3870358169078827
    },
    {
      "name": "Philosophy",
      "score": 0.0669129490852356
    }
  ]
}