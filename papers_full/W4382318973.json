{
  "title": "AirFormer: Predicting Nationwide Air Quality in China with Transformers",
  "url": "https://openalex.org/W4382318973",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2129628882",
      "name": "Yuxuan Liang",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2767256338",
      "name": "Yutong Xia",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2808289958",
      "name": "Songyu Ke",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2097698393",
      "name": "Yiwei Wang",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2141035074",
      "name": "Qingsong Wen",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2114610031",
      "name": "Junbo Zhang",
      "affiliations": [
        "Jingdong (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2079030413",
      "name": "Yu Zheng",
      "affiliations": [
        "Jingdong (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2130854877",
      "name": "Roger Zimmermann",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2808289958",
      "name": "Songyu Ke",
      "affiliations": [
        "Jingdong (China)",
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2141035074",
      "name": "Qingsong Wen",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2114610031",
      "name": "Junbo Zhang",
      "affiliations": [
        "Jingdong (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2079030413",
      "name": "Yu Zheng",
      "affiliations": [
        "Jingdong (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2907770449",
    "https://openalex.org/W2059629689",
    "https://openalex.org/W2788381291",
    "https://openalex.org/W6683369195",
    "https://openalex.org/W2903871660",
    "https://openalex.org/W6685813158",
    "https://openalex.org/W3080466448",
    "https://openalex.org/W2808535700",
    "https://openalex.org/W4312899164",
    "https://openalex.org/W2901165057",
    "https://openalex.org/W3156351347",
    "https://openalex.org/W2950817888",
    "https://openalex.org/W4283315029",
    "https://openalex.org/W6712840351",
    "https://openalex.org/W6631476048",
    "https://openalex.org/W6742934215",
    "https://openalex.org/W2049070055",
    "https://openalex.org/W3115700835",
    "https://openalex.org/W2059009934",
    "https://openalex.org/W4285164624",
    "https://openalex.org/W3186681406",
    "https://openalex.org/W3028192203",
    "https://openalex.org/W6762978078",
    "https://openalex.org/W2809035759",
    "https://openalex.org/W3103720336",
    "https://openalex.org/W2528639018",
    "https://openalex.org/W2051530877",
    "https://openalex.org/W2991165231",
    "https://openalex.org/W1969865391",
    "https://openalex.org/W4225494949",
    "https://openalex.org/W2158153840",
    "https://openalex.org/W2519091744",
    "https://openalex.org/W4295177495",
    "https://openalex.org/W3194075177",
    "https://openalex.org/W4385763767",
    "https://openalex.org/W3000386982",
    "https://openalex.org/W3080253043",
    "https://openalex.org/W2965341826",
    "https://openalex.org/W2893350509",
    "https://openalex.org/W1959608418",
    "https://openalex.org/W2183659962",
    "https://openalex.org/W2963358464",
    "https://openalex.org/W3007845852",
    "https://openalex.org/W3203701986",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2073133301",
    "https://openalex.org/W2997848713",
    "https://openalex.org/W3197700565",
    "https://openalex.org/W4288574863",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3119876899",
    "https://openalex.org/W1524946308"
  ],
  "abstract": "Air pollution is a crucial issue affecting human health and livelihoods, as well as one of the barriers to economic growth. Forecasting air quality has become an increasingly important endeavor with significant social impacts, especially in emerging countries. In this paper, we present a novel Transformer termed AirFormer to predict nationwide air quality in China, with an unprecedented fine spatial granularity covering thousands of locations. AirFormer decouples the learning process into two stages: 1) a bottom-up deterministic stage that contains two new types of self-attention mechanisms to efficiently learn spatio-temporal representations; 2) a top-down stochastic stage with latent variables to capture the intrinsic uncertainty of air quality data. We evaluate AirFormer with 4-year data from 1,085 stations in Chinese Mainland. Compared to prior models, AirFormer reduces prediction errors by 5%∼8% on 72-hour future predictions. Our source code is available at https://github.com/yoshall/airformer.",
  "full_text": "AirFormer: Predicting Nationwide Air Quality in China with Transformers\nYuxuan Liang1, Yutong Xia1, Songyu Ke4,2, Yiwei Wang1, Qingsong Wen3\nJunbo Zhang2, Yu Zheng2, Roger Zimmermann1\n1National University of Singapore, Singapore\n2JD Intelligent Cities Research & JD iCity, JD Technology, Beijing, China\n3DAMO Academy, Alibaba Group, Hangzhou, China\n4Shanghai Jiao Tong University, Shanghai, China\n{yuxliang, songyu-ke,msjunbozhang, msyuzheng}@outlook.com;\n{xiayutong618,qingsongedu}@gmail.com; {y-wang,rogerz}@comp.nus.edu.sg\nAbstract\nAir pollution is a crucial issue affecting human health and\nlivelihoods, as well as one of the barriers to economic growth.\nForecasting air quality has become an increasingly important\nendeavor with significant social impacts, especially in emerg-\ning countries. In this paper, we present a novel Transformer\ntermed AirFormer to predict nationwide air quality in China,\nwith an unprecedented fine spatial granularity covering thou-\nsands of locations. AirFormer decouples the learning pro-\ncess into two stages: 1) a bottom-up deterministic stage that\ncontains two new types of self-attention mechanisms to effi-\nciently learn spatio-temporal representations; 2) a top-down\nstochastic stage with latent variables to capture the intrinsic\nuncertainty of air quality data. We evaluate AirFormer with\n4-year data from 1,085 stations in Chinese Mainland. Com-\npared to prior models, AirFormer reduces prediction errors\nby 5%∼8% on 72-hour future predictions. Our source code\nis available at https://github.com/yoshall/airformer.\nIntroduction\nAir pollution refers to the release of air pollutants, such as\ngases, dust, fumes, and odors, into the atmosphere that are\ndetrimental to human health and the environment. Accord-\ning to the World Health Organization (WHO), air pollution\nis one of the leading causes of death in the world today, ac-\ncounting for seven million fatalities per year (Vallero 2014).\nOver 90% of people breathe air that contains more contam-\ninants than the WHO’s recommended levels, with those in\nemerging countries suffering the most, especially in China\nwith 1.4 billion people (Wang and Hao 2012).\nTo inform citizens about real-time air quality, many Chi-\nnese cities have built a number of air quality monitoring sta-\ntions. These stations report time series readings every hour,\nincluding the concentration of particulate matter (PM2.5 and\nPM10), NO2, etc. For instance, a 50 µg/m3 concentration of\nPM2.5 refers to good air quality, indicating “a terrific day\nto be active outside” according to the definition of the U.S.\nEnvironmental Protection Agency (Lin et al. 2018). Beyond\nreal-time monitoring, forecasting air quality also becomes\nan increasingly important endeavor with the economic, eco-\nlogic, and human toll that air pollution takes, which signifi-\ncantly aids human health protection and policy-making.\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: Distribution of 1,085 air quality stations in Chinese\nmainland. Each station provides hourly air quality readings.\nDuring the past decades, there has been a long-established\nline of research conducted for air quality prediction, ranging\nfrom classical dispersion models (Vardoulakis et al. 2003) to\ndata-driven models (Zheng et al. 2015; Yi et al. 2018). Con-\nsidering the computational expense, they mostly focused on\npredicting the air quality at a city scale with dozens of mon-\nitoring stations, e.g., there are 35 stations in Beijing. With\nrecent advances in deep learning, researchers started to ex-\nplore nationwide air quality prediction, i.e., collectively pre-\ndict on nearly two hundred stations in China using Spatio-\nTemporal Graph Neural Networks (STGNNs) (Wang et al.\n2020; Chen et al. 2021). STGNNs couple Graph Neural Net-\nworks (GNNs) with deep sequential models, e.g., Recurrent\nNeural Networks (RNNs), in which GNNs are used to cap-\nture the spatial correlations among stations (i.e., dispersion),\nand RNNs are utilized to learn the temporal dependencies.\nFurther, attention-based models, in particular Transform-\ners, have become a strong alternative to capture spatial cor-\nrelations in air quality data (Wang et al. 2021, 2022). They\nhave two major merits over STGNNs. First, they jointly cap-\nture short and long-term interactions among different places\nat each layer, while STGNNs merely convolve the local sur-\nroundings. Second, the correlations of air quality between\nlocations is highly dynamic, changing over time (Liang et al.\n2018; Cheng et al. 2018). Using transformers can naturally\naddress this issue (Wen et al. 2022; Zhou et al. 2022).\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n14329\nIn this study, we broaden our scope to collectively predict\nair quality in Chinese mainland with an unprecedented fine\nspatial granularity using transformers, covering thousands of\nstations. As shown in Figure 1, our prediction targets encom-\npass all provinces of Chinese mainland, with dense distribu-\ntion in developed regions, e.g., Pearl River Delta. Such a\nfine coverage not only provides more useful information to\nthe public with high social impacts, but also comprises more\ndata samples that benefit model training (Zhao et al. 2015).\nDespite these benefits, forecasting air quality with a fine\nspatial granularity poses a significant modeling difficulty for\ntransformers – efficiency. Multi-head Self-Attention (MSA),\nwhich is the key operation of transformers for spatial model-\ning, takes quadratic computational complexity w.r.t the num-\nber of stations N. Such expense may become unaffordable\nwith the growth of N, especially for our fine-grained data.\nMeanwhile, the future air quality readings are intrinsically\nuncertain due to two factors – inaccurate or missing observa-\ntions, and some unpredictable factors, e.g., vehicle exhaust,\npolicy, and industrial emission. While earlier attempts have\nshown promising performance on air quality prediction via\ndeterministic approaches, most of them still fall short of cap-\nturing such uncertainty within large-scale air quality data.\nTo tackle these challenges, we present a novel transformer\narchitecture for nationwide air quality prediction in China,\nentitled AirFormer. Our method is motivated by the domain\nknowledge of air pollution, which enables us to build mod-\nels with more interpretations. AirFormer decouples the solu-\ntion to the two issues into dual stages – adeterministic stage\nand a stochastic stage. In the deterministic stage, we pro-\npose two new types of MSA to efficiently capture the spatial\nand temporal dependencies, respectively. In the stochastic\nstage, we explore the inclusion of latent random variables\ninto the transformer. These latent variables are sampled from\nprobability distributions that are learned from the determin-\nistic hidden states, thus capturing the uncertainty of the input\ndata. In summary, our contributions lie in four aspects:\n• Considering that the spatial correlations among nearby lo-\ncations are often stronger than those far away, we devise\nthe Dartboard Spatial MSA (DS-MSA) to efficiently cap-\nture spatial relations. As its name suggests, each location\nattends to close surroundings at a fine granularity and far-\naway stations at a coarse granularity (see Fig. 2). DS-MSA\nonly takes linear complexity w.r.t the number of stations.\n• We devise Causal Temporal MSA (CT-MSA) for learning\ntemporal dependencies. It ensures that the output of a step\nderives only from previous steps, i.e., causality. Locality is\nalso introduced to improve efficiency, where the receptive\nfield in each layer is gradually increased like convolutions.\n• Leveraging recent advances in variational models, we en-\nhance transformers withlatent variables to capture the un-\ncertainty of air quality data. To preserve the parallelism of\ntransformers, the latent random variables are arranged hi-\nerarchically with implicit temporal dependencies.\n• To the best of our knowledge, this is the first work for col-\nlectively forecasting air quality among thousands of loca-\ntions. The empirical results show that AirFormer obtains\n4.6%-8.2% lower prediction errors than existing models.\nFigure 2: Sketch of DS-MSA. For a query location (the black\npoint), we first divide its surroundings into regions bounded\nby three circles and four lines. Then we project other stations\nonto the regions to obtain regional representations. Finally,\nwe use the station feature as the query and the regional repre-\nsentations as the keys and values for attention computation.\nPreliminary\nProblem Formulation\nThe readings of N air quality monitoring stations at a given\ntime t can be denoted as Xt ∈ RN×D, where D is the num-\nber of measurements, including air pollutants (e.g., PM2.5,\nNO2) and external factors (e.g., weather, wind speed). Each\nentry xij indicates the value of the j-th measurement of the\ni-th station. Given the historical readings of all stations from\nthe past T time steps, we aim to learn a function F(·) that\npredicts D′ kinds of measurements over the next τ steps:\nX1:T\nF(·)\n→ Y1:τ , where X1:T ∈ RT×N×D is the history data\nand Y1:τ ∈ Rτ×N×D′\nis the future readings.\nMulti-head Self-Attention (MSA)\nMSA is the key operation of transformers to learn an align-\nment where each token in the sequence learns to gather mes-\nsages from other tokens (Vaswani et al. 2017). Let X ∈\nRS×C be the input sequence with length S and feature di-\nmension C. The operation of a single head is defined as:\nXh = Softmax\n\u0000\nαQhK⊤\nh\n\u0001\nVh, (1)\nwhere Xh ∈ RS×C/Nh is the output features; Qh = XWq,\nKh = XWk and Vh = XWv are queries, keys, and val-\nues, respectively; Wq, Wk, Wv ∈ RC×C/Nh are the learn-\nable parameters for linear projection, and Nh is the number\nof heads; α is a scaling factor. The computational complex-\nity of Eq. (1) is quadratic w.r.t the sequence length S.\nVariational Models with Latent Variables\nVariational autoencoders (V AE) (Kingma and Welling 2013)\nhave long been verified to be an effective modeling paradigm\nfor recovering complex multimodal distributions over the la-\ntent space. V AE addresses the data distributionp(x) using an\nunobserved latent variable z and is parameterized by θ as:\npθ(x) =\nZ\npθ(x|z)pθ(z)dz. (2)\nAs the integral\nis usually intractable, V AE introduces an ap-\nproximate posterior qϕ(z|x) and implicitly optimizes the ev-\nidence lower bound (ELBO) of the marginal log-likelihood:\nlog pθ(x) ≥ −KL (qϕ(z|x)∥pθ(z))+Eqϕ(z|x) [log pθ(x|z)] ,\nwhere KL denotes the KL divergence. The prior pθ(z) and\nthe posterior qϕ(z|x) of the latent variables are usually taken\nto be Gaussian distribution with diagonal covariance, which\ninherently encodes the uncertainty of the input data.\n14330\nMethodology\nFigure 3 shows the framework of AirFormer for nationwide\nair quality prediction, which is decoupled into two stages:\n• Bottom-up deterministic stage: We first transform the his-\ntorical readings X1:T into the feature space using a multi-\nlayer perceptron (MLP). The transformed features are then\nfed to L AirFormer blocks to learn deterministic spatio-\ntemporal representations. In each block, we factorize the\nspace-time modeling along temporal and spatial domains,\nleading to the dual levels of MSA: DS-MSA for learning\nspatial interactions with linear complexity, and CT-MSA\nfor capturing the temporal dependencies at each location.\nAs shown in Figure 3(a), the output state at the l-th block\nis a 3D tensor, denoted as Hl\n1:T ∈ RT×N×C.\n• Top-down stochastic stage: Once the deterministic repre-\nsentations are obtained, we produce latent variables Z at\neach level. To maintain the parallelism of transformers,\nwe adhere to (Sønderby et al. 2016; Aksan and Hilliges\n2018) not establishing explicit dependencies between var-\nious time steps. Instead, we implicitly build temporal de-\npendencies by conditioning a latent variable Zl−1\nt on its\nhigher-level variable Zl\nt as shown in Figure 3(b), where\nZi\nt ∈ RN×C and i = {1, . . . , L}. In this way, the lower-\nlevel latent variables focus more on the local information,\nwhile the upper-level ones have a larger receptive field due\nto their corresponding deterministic input. In our model,\nthe generation task is to predict the next time step given\nall past steps using the prior pθ(Zl\nt|X1:t−1), and the infer-\nence task is to approximate the posteriorqϕ\n\u0000\nZl\nt|X1:t\n\u0001\n. As\nAirFormer belongs to the V AE family, we train our model\nby jointly optimizing the prediction loss and the ELBO.\nIn the following parts, we start by introducing the mod-\nules for spatio-temporal representation learning in the deter-\nministic stage, i.e., DS-MSA and CT-MSA. Next, we elabo-\nrate on the generation and inference model of the stochastic\nstage. Finally, we delineate the optimization of our model.\nFigure 3: AirFormer has two stages: (a) One to leverage two\ntypes of MSA to learn deterministic spatio-temporal repre-\nsentations. (b) The other to capture the uncertainty via latent\nrandom variables. The temporal receptive field of the states\n(i.e., H and Z) is gradually increased from bottom to top.\nDartboard Spatial MSA (DS-MSA)\nMotivation. Besides local emissions, the air quality of a\nplace is impacted by its neighbors as air pollutants are dis-\npersed among various locations. Given the hidden state of\nN stations at time t (Ht ∈ RN×C), the complexity of stan-\ndard MSA for spatial modeling is O(N2C), where C is the\nfeature length. This quadratic cost makes it inefficient to ac-\ncommodate with fine-grained data, e.g., 1,000+ stations.\nOverview. We present a novel MSA termed Dartboard Spa-\ntial MSA (DS-MSA) for efficiently capturing the spatial de-\npendencies among different locations at each time step. No-\ntably, our DS-MSA possesses a large receptive field while\nonly taking linear complexity w.r.t the number of stations,\nwhich is inspired by the spatial feature extraction in (Zheng\net al. 2015). Figure 4(a) presents the pipeline of DS-MSA.\nAt the l-th block, DS-MSA takes the hidden state Hl−1\nt as\ninputs. The features are first normalized and used to gener-\nate the queries with a linear layer. For each query station, we\nproject its surroundings into the dartboard distribution to ob-\ntain the keys and values. As a result, the number of keys (or\nvalues) is reduced to M, where M is the region number. We\nthen perform MSA to learn spatial dependencies and finally\nutilize MLPs to produce the output ˆHl\nt ∈ RN×C.\nFigure 4: Pipeline of DS-MSA. PE: position encoding.\nDartboard Projection & MSA.For a certain station i, we\nunify the formulation of the dartboard projection by intro-\nducing a projection matrix Ai ∈ RM×N which denotes how\nnearby stations are mapped to M regions bound by the line\nfragments and circles, where each entryajk ≥ 0 denotes the\nlikelihood that the k-th station belongs to the region j. Ai\nhas each row summing to one and each non-zero entry in\nthe same row is evenly distributed (like average pooling).\nFigure 2 shows an example in which we partition the sur-\nroundings of a query station in a dartboard fashion. The out-\nmost circle has the largest semidiameter (e.g. 500km), while\nthe innermost one has the smallest (e.g., 50km). The three\ncircles focus on a common center (the query station) and are\nfurther divided by four lines heading in various directions.\nThe stations falling outside the outmost circle are not con-\nsidered for MSA. In this case, we have M = 24 + 1 = 25\n(+1 for including the query station itself).\n14331\nAssume the inputs of dartboard projection is P ∈ RN×C,\nwe project the station features into regional representations\nthat correspond to each station via the assignment matrix:\nRi = AiP, R = [R1, R2, ··· , RN ] , (3)\nwhere [·, ·] is concatenation; R ∈ RN×M×C is the regional\nfeatures after projection. Next, we generate the queries, keys\nand values using linear layers and perform MSA to capture\nthe spatial relations between the query station and its corre-\nsponding regions (notations are detailed in Eq. 1):\nXh = Softmax\n\u0000\nαQhK⊤\nh + Bh\n\u0001\nVh, (4)\nwhere Bh ∈ RN×M is learnable relative position encod-\ning (Wu et al. 2021) to incorporate position information. We\ncan also encode the impacts of external factors (e.g., wind\ndirection and speed) in Bh to improve performance. In par-\nticular, the regions without any stations are masked during\nMSA. Figure 4(b) can help better understand this process.\nDiscussion. DS-MSA is designed by jointly considering the\nfollowing factors: 1) Spatial dependencies: Considering the\ndomain knowledge of air pollution dispersion that the spatial\ncorrelations between nearby locations are always stronger\nthan those far away, each station attends to its surroundings\nin a dartboard fashion, i.e., close places at a fine granularity\nand the distant regions at a coarse granularity. 2) Efficiency:\nSince the number of regionsM is small (M ≪ N), the com-\nputational complexity O(MNC ) grows linearly with the in-\ncrease of the number of stations, which is more efficient than\nstandard MSA. 3) Lightweight: By using the dartboard pro-\njection, DS-MSA does not introduce extra learnable param-\neters into standard MSA, thus being lightweight in practice.\nCausal Temporal MSA (CT-MSA)\nIn addition to spatial dependencies, the air quality of a loca-\ntion depends on its history. Given the hidden state of a loca-\ntion across all past steps ˆHl\n:,n ∈ RT×C (the output of DS-\nMSA), adopting MSA to learn temporal dependencies in-\nduces quadratic cost w.r.t the number of time steps T. Here,\nwe propose Causal Temporal MSA (CT-MSA) as a strong\nand efficient alternative to standard MSA for temporal mod-\neling. As depicted in Figure 5, its pipeline is mostly identical\nto standard MSA. Leveraging the domain knowledge of time\nseries, CT-MSA has two major modifications:\nLocal window.Considering that nearby time steps usually\nhave stronger correlations than faraway slots, we perform\nMSA within non-overlapping windows to capture local in-\nteractions among time steps, leading to O(T W C) cost ( T\nW\ntimes less than standard MSA), whereW is the window size.\nTo preserve the large receptive field of standard MSA, we\ngradually increase the window size in different AirFormer\nblocks (from bottom to top), as shown in Figure 5.\nTemporal causality.Since the air quality at the current step\nis not conditioned on its future, we follow WaveNet (Oord\net al. 2016) to introduce causality into MSA (see Figure 5),\nwhich ensures the model cannot violate the temporal order\nof input data. Such causality can be easily implemented by\nmasking the specific entries in the attention map. To allow\nposition-aware MSA, we add learnable absolute position en-\ncoding (Vaswani et al. 2017) to the input of CT-MSA.\nFigure 5: Left: CT-MSA. Right: An example of CT-MSA\nwith local windows of size 2, 4, and 8 at each block.\nTop-Down Stochastic Stage\nAfter obtaining the deterministic representations, we build\nlatent random variables to learn theuncertainty of air quality\ndata, e.g., unpredictable factors and noisy observations.\nGeneration Model.As shown in Figure 3(b), the generation\nmodel aims to predict the next step given all past steps. As\nwe have encoded the spatial dependencies among locations\nin deterministic states Ht, we can factorize the prior distri-\nbution of a set of random variables Zt = {Z1\nt , . . . ,ZL\nt } as:\npθ (Zt|X1:t−1) =\nNY\nn=1\npθ\n\u0000\b\nz1\nt,n, . . . ,zL\nt,n\n\t\n|X1:t−1\n\u0001\n=\nNY\nn=1\npθ\n\u0000\nzL\nt,n|hL\nt−1,n\n\u0001L−1Y\nl=1\npθ\n\u0000\nzl\nt,n|zl+1\nt,n , hl\nt−1,n\n\u0001\n,\n(5)\nwhere zl\nt,n ∈ RC and hl\nt,n ∈ RC is the n-th row of Zl\nt and\nHl\nt, respectively. In Eq. (5), we follow V AE to set the prior\ndistribution at each layer as a Gaussian distribution, i.e.,\npθ\n\u0000\nzl\nt,n|zl+1\nt,n , hl\nt−1,n\n\u0001\n= N\n\u0000\nµl\nt, σl\nt\n\u0001\n, (6)\nwhere the mean µl\nt and the diagonal covariance σl\nt are pa-\nrameterized by a neural network fl(zl+1\nt,n , hl\nt−1,n) shared by\nall locations. In this way, we implicitly build connections\nbetween hidden states at different time steps, as the upper-\nlayer random variables (e.g., zl+1\nt,n ) contain more contextual\ninformation due to their larger receptive field.\nInference Model.In contrast, the inference model (see Fig-\nure 3(b)) is applied to approximate the posterior distribution\nof Zt given both the current and previous steps:\nqϕ (Zt|X1:t) =\nNY\nn=1\nqϕ\n\u0000\nzL\nt,n|hL\nt,n\n\u0001L−1Y\nl=1\nqϕ\n\u0000\nzl\nt,n|zl+1\nt,n , hl\nt,n\n\u0001\n,\nwhere qϕ\n\u0000\nzl\nt,n|zl+1\nt,n , hl\nt,n\n\u0001\n= N\n\u0000\nˆµl\nt, ˆσl\nt\n\u0001\n. (7)\nEq. (7) employs the same factorization as Eq. (5). The argu-\nments of the Gaussian distribution at each layer is parame-\nterized by a neural networkgl(zl+1\nt,n , hl\nt,n). Following a simi-\nlar procedure as the generation model, the random variables\ngenerated by the posterior distribution can also effectively\nconsider the spatio-temporal dependencies within air qual-\nity, thereby enhancing the predictive performance.\n14332\nPrediction & Optimization\nAirFormer makes predictions based on both the determinis-\ntic and stochastic hidden states at time T using an MLP:\nˆY1:τ = MLP([H1\nT , . . . ,HL\nT , Z1\nT , . . . ,ZL\nT ]). (8)\nTo train our model, we jointly optimize two loss functions:\nL = Lpred + LELBO. (9)\nLpred denotes the L1 loss between the prediction ˆY1:τ and\nthe corresponding ground truthY1:τ ; the second termLELBO\nindicates the sum of negative ELBO at all historical steps.\nConcretely, LELBO at each step is computed as:\nLELBO(θ, ϕ;t) =Lrec(θ, ϕ;t) +Lkl(θ, ϕ;t)\n= −Eqϕ(Zt|Xt) [log pθ (Xt|Zt)]\n+ KL (qϕ (Zt|X1:t) ∥pθ (Zt|X1:t−1)) ,\n(10)\nwhere the first term is the reconstruction likelihood and the\nsecond term is the KL divergence between the prior and the\nposterior; Zt = {Z1\nt , . . . ,ZL\nt } is a set of latent variables at\neach layer. We further simplify the KL term (Lkl) using the\nfactorization in Eq. (5) and Eq. (7), resulting in\nLkl =\nNX\nn=1\nKL\n\u0000\nqϕ\n\u0000\nzL\nt,n|hL\nt,n\n\u0001\n∥pθ\n\u0000\nzL\nt,n|hL\nt−1,n\n\u0001\u0001\n+\nNX\nn=1\nL−1X\nl=1\nEqϕ\n\u0002\nKL\n\u0000\nqϕ\n\u0000\nzl\nt,n|zl+1\nt,n , hl\nt,n\n\u0001\n∥pθ\n\u0000\nzl\nt,n|zl+1\nt,n , hl\nt−1,n\n\u0001\u0001\u0003\nExperiments\nDataset Description\nOur system collected nationwide air quality data and mete-\norological data for a four-year period (Jan. 1, 2015 to Dec.\n31, 2018) throughout Chinese mainland. The system details\ncan be found in (Yi et al. 2018).\n• Air quality data: We collected hourly air quality data from\n1,976 stations covering 342 cities in China. Each air qual-\nity record contains the concentration of six types of pol-\nlutants, including PM2.5, PM10, NO2, CO, O3, and SO2.\nAmong them, the primary pollutant of air quality is PM2.5,\nthus we employ its reading as the prediction target.\n• Meteorological data: This dataset is collected every hour\nfrom 2,575 monitoring stations in China. Each record con-\nsists of weather conditions (e.g., sunny, rainy), tempera-\nture, humidity, wind speed, and wind direction.\nWe choose the air quality monitoring stations with a miss-\ning rate of PM2.5 less than 20% to conduct our experiments,\nleading to a 1,085-station dataset. Figure 1 illustrates the sta-\ntion distribution. Our dataset has extensive coverage in both\nspace and time scales compared to previous datasets (see Ta-\nble 1). Following the prior studies (Liang et al. 2018; Wang\net al. 2020), we predict the concentration of PM 2.5 over the\nnext 24 steps given the past 24 steps, where each step stands\nfor 3 hours. In other words, we perform a 72-hour future pre-\ndiction based on the readings from the past 72 hours. The\ndataset is partitioned in chronological order with the first\ntwo years for training, the third year for validation, and the\nlast year for testing. Z-score normalization is applied to the\nmodel inputs for fast convergence.\nDataset Venue #Node Range Scale Joint?\n(Zheng et al. 2015) KDD 35 48m City ✗\n(Yi et al. 2018) KDD 35 48m City ✗\n(Lin et al. 2018) GIS 35 14m City ✓\n(Wang et al. 2020) GIS 184 48m Nation ✓\n(Xu et al. 2021) preprint 56 12m Province ✓\n(Chen et al. 2021) preprint 209 28m Nation ✓\nOur dataset AAAI 1,085 48m Nation ✓\nTable 1: Dataset comparison. m: month. Joint means collec-\ntively predicting the air quality at all locations.\nImplementation Details\nWe implement our model by PyTorch 1.10 using a Quadro\nRTX A6000 GPU. The Adam optimizer (Kingma and Ba\n2014) is utilized to train our model, and the batch size is 16.\nThe learning rate starts from 5 × 10−4, halved every three\nepochs. The number of AirFormer blocks is set to 4 with\nlocal window size W = 3,6, 12, 24, respectively. For the\nhidden dimension C in DS-MSA and CT-MSA, we conduct\na grid search over {8, 16, 32, 64}, and C = 32 obtains the\nbest result. The number of heads in DS-MSA and CT-MSA\nis 2. In DS-MSA, we partition the space into regions by two\ncircles with 50 and 200km semidiameter, respectively.f and\ng in the stochastic stage are parameterized by 3-layer MLPs.\nBaselines for Comparison\nWe compare our AirFormer with the following baselines that\nbelong to the following four categories:\n• Classical methods: HA is history average which predicts\nair quality by the average value of historical readings in\nthe corresponding periods. V ARis vector autoregression.\n• STGNN variants: we also take some popular STGNNs as\nbaselines, i.e., DCRNN (Li et al. 2017),STGCN (Yu, Yin,\nand Zhu 2018), GWNET (Wu et al. 2019), and MTGNN\n(Wu et al. 2020). They generalize well to our application.\n• Attention-based models: ASTGCN (Guo et al. 2019) and\nGMAN (Zheng et al. 2020) are attention-based baselines\nfor spatio-temporal forecasting. STTN (Xu et al. 2020) is\na variant of transformers for traffic forecasting, which can\nbe easily adapted to air quality prediction.\n• Air quality prediction: we select three strong models for a\ncomparison, including DeepAir (Yi et al. 2018), PM2.5-\nGNN (Wang et al. 2020) andGAGNN (Chen et al. 2021).\nSome (Pan et al. 2018; Wang et al. 2021, 2022; Liang et al.\n2022) are omitted due to out-of-GPU-memory (OOM).\nFor a fair comparison, we tune different hyperparameters\nfor all baselines, finding the best setting for each.\nEvaluation Metrics\nWe leverage Mean Absolute Error (MAE) and Root Mean\nSquared Error (RMSE) for evaluation, where a smaller met-\nric means better performance. Moreover, we follow (Zheng\net al. 2015; Yi et al. 2018) to discuss the errors on predict-\ning sudden changes. The sudden changes are defined as the\ncases where PM2.5 is larger than 75 µg/m3 and changes more\nthan a threshold (i.e., ±20 µg/m3) in the next three hours.\n14333\nModel #Param (K) 1-24h 25-48h 49-72h Sudden change\nMAE RMSE MAE RMSE MAE RMSE MAE RMSE\nHA (Zhang, Zheng, and Qi 2017) - 31.25 62.52 31.19 62.49 31.16 62.49 72.89 132.42\nV AR (Toda 1991) - 29.91 52.10 31.61 64.59 30.18 65.10 70.86 114.78\nDCRNN (Li et al. 2017) 397 19.35 46.40 24.06 57.38 25.30 58.24 63.22 112.30\nSTGCN (Yu, Yin, and Zhu 2018) 453 19.06 42.69 24.09 56.50 25.10 58.96 61.35 111.89\nGWNET (Wu et al. 2019) 825 17.79 39.49 23.32 53.17 25.00 57.01 59.33 105.75\nMTGNN (Wu et al. 2020) 207 18.15 38.99 23.47 52.21 24.77 55.73 59.24 103.71\nASTGCN (Guo et al. 2019) 4,812 20.76 50.29 24.37 56.04 25.22 57.77 63.19 114.57\nGMAN (Zheng et al. 2020) 269 19.60 45.70 23.79 54.25 24.89 56.33 61.83 109.57\nSTTN (Xu et al. 2020) 188 18.22 37.44 24.16 52.91 25.35 56.14 60.36 105.20\nDeepAir (Yi et al. 2018) 183 17.47 39.12 23.40 53.48 24.95 56.92 60.26 109.95\nPM2.5-GNN (Wang et al. 2020) 101 20.20 48.96 25.04 59.56 26.31 60.46 63.64 119.00\nGAGNN (Chen et al. 2021) 412 19.53 45.68 24.56 58.59 25.56 59.61 64.38 116.51\nAirFormer (ours) 246 16.03* 32.36* 21.65* 44.67* 23.64* 50.22* 54.92* 90.15*\nTable 2: 5-run results. The magnitude of #Param (the number of parameters) is Kilo. The bold/underlined font mean the best/the\nsecond best result. * denotes the improvement over the second best model is statistically significant at level 0.01 (Kim 2015).\nModel Comparison\nIn this section, we perform a model comparison in terms of\nMAE and RMSE. We run each method five times and report\nthe average metric of each model. As shown in Table 2, Air-\nFormer significantly outperforms all competing baselines on\nboth metrics according to the Student’s T-test (Kim 2015)\nat level 0.01. In contrast to the second best method (i.e.,\nDeepAir), AirFormer reduces the MAE by 8.2%, 7.5% and\n5.3% on 24-, 48- and 72-hour future prediction, respectively.\nWhen predicting the sudden changes, AirFormer achieves at\nleast 7.3% and 13.1% improvements on MAE and RMSE\nthanks to the robustness provided by stochastic latent spaces.\nFrom Table 2, we can also observe that: 1) Deep-learning-\nbased approaches surpass the classical methods (HA and\nV AR) by a large margin due to their greater learning capac-\nity. 2) Although STGNN-based models, e.g., GWNET and\nMTGNN, are originally evaluated on traffic forecasting, they\nalso generalize well on air quality prediction, showing com-\nparable results to the second best method DeepAir. 3) Our\nAirFormer clearly outperforms STTN which is also a trans-\nformer model. This verifies that the domain knowledge of\nair pollution not only helps us design our model with more\ninterpretations, but also enhances the predictive accuracy.\nAblation Study\nEffects of DS-MSA.To study the effects of DS-MSA, we\nconsider the following variants for comparison: a) w/o DS-\nMSA: We turn off DS-MSA in AirFormer, i.e., no spatial\nmodeling in this model. b) MSA: We replace DS-MSA with\nstandard MSA. c) Local MSA: DS-MSA is substituted by\nlocal MSA in which each query station attends to its neigh-\nbors within 500km. The results are shown in the upper part\nof Table 3. First, we find removing DS-MSA leads to a sig-\nnificant degradation on MAE, revealing the great importance\nof addressing spatial dependencies. Second, DS-MSA ob-\ntains lower prediction errors over all future horizons, while\nrunning 39% faster than vanilla MSA and 21% faster than\nlocal MSA. This merit suggests that our DS-MSA has great\npotential to be a basic building block for capturing spatial\ndependencies within air quality data in practice.\nWe then discuss different settings of the dartboard. In Ta-\nble 3, r1-r2-r3 means dividing the space by 3 circles withr1,\nr2, and r3 km semidiameter. Compared to the 1-circle parti-\ntion with 50km semidiameter, the 50-200 dartboard achieves\nlower errors due to a larger receptive field. Despite the low-\nest MAE of 50-200-500, it results in more computational\ncosts (33.8% slower than 50-200). To make a better trade-\noff, we choose 50-200 as our default setting.\nTo further investigate DS-MSA, we perform a case study\non the 50-200 dartboard centered by Xizhimen (a hybrid dis-\ntrict in Beijing). In Fig. 6, the attention weights are dispersed\n(almost <0.2) when there is no wind. If the wind blows from\nthe east or southwest, the attention weights concentrate more\non the corresponding directions. In light of this, DS-MSA is\nnot only effective but can also be easily interpreted.\nVariant Time/epoch 1-24h 25-48h 49-72h\nw/o DS-MSA 1,284 17.24 23.55 24.96\nMSA 2,818 18.19 23.24 24.23\nLocal MSA 2,157 18.63 23.79 24.98\nDS-MSA (50-200) 1,708 16.03 21.65 23.64\nDS-MSA (50) 1,547 16.43 22.87 24.32\nDS-MSA (50-200-500) 2,285 16.09 21.30 22.85\nTable 3: Effects of DS-MSA over MAE.\nS\nSE\nE\nNE\nN\nNW\nW\nSW\n(a) No wind \n (2017/12/27 21:00)\n0.0 0.1 0.2\nS\nSE\nE\nNE\nN\nNW\nW\nSW\n(b) East wind \n (2017/12/28 14:00)\n0.0 0.2 0.4\nS\nSE\nE\nNE\nN\nNW\nW\nSW\n(c) Southwest wind \n (2017/12/30 7:00)\n0.0 0.2 0.4 0.6\nFigure 6: Visualization of DS-MSA at the first block. We\nomit the attention weight at the center station (query) itself.\n14334\n0.10\n 0.05\n 0.00 0.05 0.1015\n16\n17\n18\n19\n20MAE\n(a) 1-24h 0.10\n 0.05\n 0.00 0.05 0.1019\n20\n21\n22\n23\n24\n(b) 25-48h 0.10\n 0.05\n 0.00 0.05 0.1020\n21\n22\n23\n24\n25\n(c) 49-72h\nw/o CT-MSA WaveNet MSA CT-MSA\nFigure 7: Effects of CT-MSA on MAE.\nVaraint Time/epo 1-72h Sudden change\nw/o Stochastic 1,559 20.97 57.52\nAirFormer 1,708 20.44 54.92\nTable 4: Effects of latent variables on MAE. w/o: without.\nEffects of CT-MSA.To examine the efficacy of CT-MSA\nfor capturing temporal dependencies, we compare our model\nwith its variants integrated with various temporal modules:\na) w/o CT-MSA: we remove CT-MSA from our AirFormer.\nb) MSA, WaveNet: we replace CT-MSA by standard MSA\nor WaveNet (Wu et al. 2019). The results are shown in Fig-\nure 7. Primarily, all the variants with temporal modules per-\nform much better than w/o CT-MSA, verifying the necessity\nof temporal modeling. Moreover, both MSA-based methods\nsurpass WaveNet, which reveals the superiority of MSA in\nair quality modeling. Remarkably, integrating causality and\nlocal windows into MSA consistently improves the perfor-\nmance over all future steps (see MSA vs. CT-MSA).\nEffects of Latent Variables.As a crucial component of Air-\nFormer, the stochastic stage empowers our model to capture\nthe uncertainty within air quality data, thus boosting the per-\nformance. To investigate its effectiveness, we compare our\nAirFormer with its variant that turns off the stochastic stage.\nAs shown in Table 4, integrating latent variables reduces the\nMAE on sudden changes by 4.5% while introducing little\nadditional time (149 seconds) per training epoch.\nHyperparameter Study.Next, we investigate the effects of\nthe number of AirFormer blocksL, the hidden dimensionC,\nand the number of heads Nh. Figure 8(a) shows the results\nof AirFormer with 2 or 4 blocks vs. differentC, from which\nwe observe: 1) Stacking more AirFormer blocks consistently\nachieves lower MAE. We encounter the OOM issue when\nL >4 and thus set L = 4 as our default setting. 2) Using\na very small C (e.g., 16) leads to worse performance due to\nits limited capacity. 3) C = 64 obtain similar accuracy. In\nFigure 8(b), we find that the performance of AirFormer is\nnot sensitive to the number of heads when Nh > 1.\n8 16 32 64\n(a) Hidden dimension C\n20.2\n20.7\n21.2\n21.7\n22.2MAE (1-72h)\nL= 2\nL= 4\n1 2 3 4\n(b) Number of heads Nh\n20.3\n20.6\n20.9\n21.2Acc\nL= 2\nL= 4\nFigure 8: Effects of hyperparameters on MAE.\n0.10\n 0.05\n 0.00 0.0515\n16\n17\n18MAE\n(a) 1-24h 0.10\n 0.05\n 0.00 0.0520\n21\n22\n23\n(b) 25-48h 0.10\n 0.05\n 0.00 0.0521\n22\n23\n24\n(c) 49-72h\nw/o Spatial PE w/o T emporal PE AirFormer\nFigure 9: Effects of position encoding (PE) on MAE.\nEffects of Position Encoding.Since MSA is permutation-\ninvariant, we integrate position encoding (PE) into DS-MSA\nand CT-MSA to consider the order information. As depicted\nin Figure 9, removing either Spatial PE in DS-MSA or Tem-\nporal PE in CT-MSA will cause degenerated performance\nacross all future horizons. Besides, the improvement of inte-\ngrating Spatial PE is slightly higher than Temporal PE.\nRelated Works\nForecasting air quality is one of the core tasks in smart cities.\nPhysics-based models represent the emission and disper-\nsion of air pollutants as a dynamic system and simulate the\nprocess using numerical functions. They investigate the pri-\nmary causes of air pollution, including chemicals, vehicles,\nand factories (Vardoulakis et al. 2003; Arystanbekova 2004;\nDaly and Zannetti 2007). However, it is non-trivial to ac-\nquire such a variety of data sources precisely.\nData-driven approacheshave recently emerged as the most\npopular approach for air quality forecasting. This research\nline uses parameterized models, such as neural networks, to\ncapture the spatio-temporal dependencies within air quality\ndata. As opposed to physics-based models, they require far\nless complex domain knowledge and are usually more flexi-\nble. Typically, Zheng et al. (2015) developed a hybrid data-\ndriven model that ensembles the prediction results from dif-\nferent views. Leveraging the large capacity of deep neural\nnetworks, DeepAir outperformed existing shallow models\non both short and long-term predictions (Yi et al. 2018).\nSome follow-ups proposed either STGNNs (Wang et al.\n2020) or attention-based models (Wang et al. 2021, 2022)\nto better address the spatio-temporal dependencies. These\nworks, however, present some difficulties (e.g., inefficiency,\ndegenerated performance) on a nationwide forecasting task.\nBesides, there are a stream of studies (Pan et al. 2019; Li\net al. 2020; Liu et al. 2021; Pan et al. 2021; Shao et al. 2022)\nexploring new learning paradigms for spatio-temporal data.\nConclusion and Future Work\nWe have devised a transformer model for nationwide air\nquality prediction in China. To the best of our knowledge,\nthis is the first work for collectively forecasting air quality\namong thousands of locations. Our model elaborately com-\nbines the spatio-temporal learning capabilities of transform-\ners with the uncertainty measurement of stochastic latent\nspaces. Compared to prior methods, our model reduces the\nprediction errors by 4.6-8.2%. In the future, we will explore\nonline learning and deploy our model to support public use.\n14335\nAcknowledgments\nThis research was supported by the MSIT (Ministry of Sci-\nence, ICT), Korea, under the ITRC (Information Technol-\nogy Research Center) support program (IITP-2022-2020-\n0-01789) supervised by the IITP (Institute for Information\n& Communications Technology Planning & Evaluation). It\nwas also supported by the National Natural Science Foun-\ndation of China (62172034) and the Beijing Nova Program\n(Z201100006820053).\nReferences\nAksan, E.; and Hilliges, O. 2018. STCN: Stochastic Tem-\nporal Convolutional Networks. In International Conference\non Learning Representations.\nArystanbekova, N. K. 2004. Application of Gaussian plume\nmodels for air pollution simulation at instantaneous emis-\nsions. Mathematics and Computers in Simulation, 67(4-5):\n451–458.\nChen, L.; Xu, J.; Wu, B.; Qian, Y .; Du, Z.; Li, Y .; and\nZhang, Y . 2021. Group-aware graph neural network for\nnationwide city air quality forecasting. arXiv preprint\narXiv:2108.12238.\nCheng, W.; Shen, Y .; Zhu, Y .; and Huang, L. 2018. A neural\nattention model for urban air quality inference: Learning the\nweights of monitoring stations. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 32.\nDaly, A.; and Zannetti, P. 2007. Air pollution modeling–An\noverview. Ambient air pollution, 15–28.\nGuo, S.; Lin, Y .; Feng, N.; Song, C.; and Wan, H. 2019. At-\ntention based spatial-temporal graph convolutional networks\nfor traffic flow forecasting. In AAAI, volume 33, 922–929.\nKim, T. K. 2015. T test as a parametric statistic. Korean\njournal of anesthesiology, 68(6): 540–546.\nKingma, D. P.; and Ba, J. 2014. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980.\nKingma, D. P.; and Welling, M. 2013. Auto-encoding varia-\ntional bayes. arXiv preprint arXiv:1312.6114.\nLi, T.; Zhang, J.; Bao, K.; Liang, Y .; Li, Y .; and Zheng,\nY . 2020. Autost: Efficient neural architecture search for\nspatio-temporal prediction. In Proceedings of the 26th ACM\nSIGKDD International Conference on Knowledge Discov-\nery & Data Mining, 794–802.\nLi, Y .; Yu, R.; Shahabi, C.; and Liu, Y . 2017. Diffusion con-\nvolutional recurrent neural network: Data-driven traffic fore-\ncasting. arXiv preprint arXiv:1707.01926.\nLiang, Y .; Ke, S.; Zhang, J.; Yi, X.; and Zheng, Y . 2018. Ge-\noMAN: Multi-Level Attention Networks for Geo-Sensory\nTime Series Prediction. In IJCAI.\nLiang, Y .; Ouyang, K.; Wang, Y .; Pan, Z.; Yin, Y .; Chen, H.;\nZhang, J.; Zheng, Y .; Rosenblum, D. S.; and Zimmermann,\nR. 2022. Mixed-Order Relation-Aware Recurrent Neural\nNetworks for Spatio-Temporal Forecasting. IEEE Transac-\ntions on Knowledge and Data Engineering.\nLin, Y .; Mago, N.; Gao, Y .; Li, Y .; Chiang, Y .-Y .; Shahabi,\nC.; and Ambite, J. L. 2018. Exploiting spatiotemporal pat-\nterns for accurate air quality forecasting using deep learning.\nIn SIGSPATIAL, 359–368.\nLiu, X.; Liang, Y .; Zheng, Y .; Hooi, B.; and Zimmermann,\nR. 2021. Spatio-temporal graph contrastive learning. arXiv\npreprint arXiv:2108.11873.\nOord, A. v. d.; Dieleman, S.; Zen, H.; Simonyan, K.;\nVinyals, O.; Graves, A.; Kalchbrenner, N.; Senior, A.; and\nKavukcuoglu, K. 2016. Wavenet: A generative model for\nraw audio. arXiv preprint arXiv:1609.03499.\nPan, Z.; Ke, S.; Yang, X.; Liang, Y .; Yu, Y .; Zhang, J.; and\nZheng, Y . 2021. AutoSTG: Neural Architecture Search for\nPredictions of Spatio-Temporal Graph. In Proceedings of\nthe Web Conference 2021, 1846–1855.\nPan, Z.; Liang, Y .; Wang, W.; Yu, Y .; Zheng, Y .; and Zhang,\nJ. 2019. Urban traffic prediction from spatio-temporal data\nusing deep meta learning. In Proceedings of the 25th ACM\nSIGKDD International Conference on Knowledge Discov-\nery & Data Mining, 1720–1730.\nPan, Z.; Liang, Y .; Zhang, J.; Yi, X.; Yu, Y .; and Zheng, Y .\n2018. Hyperst-net: Hypernetworks for spatio-temporal fore-\ncasting. arXiv preprint arXiv:1809.10889.\nShao, Z.; Zhang, Z.; Wang, F.; and Xu, Y . 2022. Pre-training\nEnhanced Spatial-temporal Graph Neural Network for Mul-\ntivariate Time Series Forecasting. In Proceedings of the\n28th ACM SIGKDD Conference on Knowledge Discovery\nand Data Mining, 1567–1577.\nSønderby, C. K.; Raiko, T.; Maaløe, L.; Sønderby, S. K.;\nand Winther, O. 2016. Ladder variational autoencoders. Ad-\nvances in neural information processing systems, 29.\nToda, H. 1991. Vector autoregression and causality. Yale\nUniversity.\nVallero, D. A. 2014. Fundamentals of air pollution. Aca-\ndemic press.\nVardoulakis, S.; Fisher, B. E.; Pericleous, K.; and Gonzalez-\nFlesca, N. 2003. Modelling air quality in street canyons: a\nreview. Atmospheric environment, 37(2): 155–182.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in neural information\nprocessing systems, 5998–6008.\nWang, C.; Zhu, Y .; Zang, T.; Liu, H.; and Yu, J. 2021.\nModeling inter-station relationships with attentive temporal\ngraph convolutional network for air quality prediction. In\nWSDM, 616–634.\nWang, S.; and Hao, J. 2012. Air quality management in\nChina: Issues, challenges, and options. Journal of Environ-\nmental Sciences, 24(1): 2–13.\nWang, S.; Li, Y .; Zhang, J.; Meng, Q.; Meng, L.; and Gao, F.\n2020. PM2.5-GNN: A Domain Knowledge Enhanced Graph\nNeural Network For PM2.5 Forecasting. In SIGSPATIAL,\n163–166.\nWang, S.; Qiao, L.; Fang, W.; Jing, G.; Sheng, V . S.; and\nZhang, Y . 2022. Air Pollution Prediction Via Graph Atten-\ntion Network and Gated Recurrent Unit. Computers, Mate-\nrials and Continua, 73(1): 673–687.\nWen, Q.; Zhou, T.; Zhang, C.; Chen, W.; Ma, Z.; Yan, J.; and\nSun, L. 2022. Transformers in time series: A survey. arXiv\npreprint arXiv:2202.07125.\n14336\nWu, K.; Peng, H.; Chen, M.; Fu, J.; and Chao, H. 2021. Re-\nthinking and improving relative position encoding for vision\ntransformer. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, 10033–10041.\nWu, Z.; Pan, S.; Long, G.; Jiang, J.; Chang, X.; and Zhang,\nC. 2020. Connecting the dots: Multivariate time series fore-\ncasting with graph neural networks. In Proceedings of the\n26th ACM SIGKDD International Conference on Knowl-\nedge Discovery & Data Mining, 753–763.\nWu, Z.; Pan, S.; Long, G.; Jiang, J.; and Zhang, C. 2019.\nGraph WaveNet for Deep Spatial-Temporal Graph Model-\ning. In IJCAI, 1907–1913.\nXu, J.; Chen, L.; Lv, M.; Zhan, C.; Chen, S.; and Chang,\nJ. 2021. Highair: A hierarchical graph neural network-\nbased air quality forecasting method. arXiv preprint\narXiv:2101.04264.\nXu, M.; Dai, W.; Liu, C.; Gao, X.; Lin, W.; Qi, G.-J.; and\nXiong, H. 2020. Spatial-temporal transformer networks for\ntraffic flow forecasting. arXiv preprint arXiv:2001.02908.\nYi, X.; Zhang, J.; Wang, Z.; Li, T.; and Zheng, Y . 2018. Deep\ndistributed fusion network for air quality prediction. In Pro-\nceedings of the 24th ACM SIGKDD International Confer-\nence on Knowledge Discovery & Data Mining, 965–973.\nYu, B.; Yin, H.; and Zhu, Z. 2018. Spatio-temporal Graph\nConvolutional Networks: A Deep Learning Framework for\nTraffic Forecasting. In IJCAI.\nZhang, J.; Zheng, Y .; and Qi, D. 2017. Deep Spatio-\nTemporal Residual Networks for Citywide Crowd Flows\nPrediction. In AAAI, 1655–1661.\nZhao, L.; Sun, Q.; Ye, J.; Chen, F.; Lu, C.-T.; and Ramakr-\nishnan, N. 2015. Multi-task learning for spatio-temporal\nevent forecasting. In Proceedings of the 21th ACM SIGKDD\ninternational conference on knowledge discovery and data\nmining, 1503–1512.\nZheng, C.; Fan, X.; Wang, C.; and Qi, J. 2020. Gman:\nA graph multi-attention network for traffic prediction. In\nAAAI, volume 34, 1234–1241.\nZheng, Y .; Yi, X.; Li, M.; Li, R.; Shan, Z.; Chang, E.; and\nLi, T. 2015. Forecasting fine-grained air quality based on\nbig data. In Proceedings of the 21th ACM SIGKDD interna-\ntional conference on knowledge discovery and data mining,\n2267–2276.\nZhou, T.; Ma, Z.; Wen, Q.; Wang, X.; Sun, L.; and Jin,\nR. 2022. FEDformer: Frequency Enhanced Decomposed\nTransformer for Long-term Series Forecasting. In Pro-\nceedings of the 39th International Conference on Machine\nLearning (ICML), 27268–27286.\n14337",
  "topic": "Granularity",
  "concepts": [
    {
      "name": "Granularity",
      "score": 0.7694301605224609
    },
    {
      "name": "Air quality index",
      "score": 0.7660723924636841
    },
    {
      "name": "Mainland China",
      "score": 0.6081841588020325
    },
    {
      "name": "Computer science",
      "score": 0.565211296081543
    },
    {
      "name": "China",
      "score": 0.5226830840110779
    },
    {
      "name": "Air pollution",
      "score": 0.48542818427085876
    },
    {
      "name": "Livelihood",
      "score": 0.4637162685394287
    },
    {
      "name": "Transformer",
      "score": 0.4631732106208801
    },
    {
      "name": "Data science",
      "score": 0.4222766160964966
    },
    {
      "name": "Environmental economics",
      "score": 0.35135412216186523
    },
    {
      "name": "Econometrics",
      "score": 0.3487027883529663
    },
    {
      "name": "Data mining",
      "score": 0.3413221836090088
    },
    {
      "name": "Geography",
      "score": 0.21830201148986816
    },
    {
      "name": "Meteorology",
      "score": 0.2128845751285553
    },
    {
      "name": "Engineering",
      "score": 0.1586105227470398
    },
    {
      "name": "Economics",
      "score": 0.14332056045532227
    },
    {
      "name": "Agriculture",
      "score": 0.11851945519447327
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I165932596",
      "name": "National University of Singapore",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I45928872",
      "name": "Alibaba Group (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210103986",
      "name": "Jingdong (China)",
      "country": "CN"
    }
  ],
  "cited_by": 111
}