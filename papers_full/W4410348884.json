{
  "title": "Benchmarking large language models GPT-4o, llama 3.1, and qwen 2.5 for cancer genetic variant classification",
  "url": "https://openalex.org/W4410348884",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2162574638",
      "name": "Kuan-Hsun Lin",
      "affiliations": [
        "National Taipei University of Nursing and Health Science",
        "Taipei Veterans General Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A5033468320",
      "name": "Tzu-Hang Kao",
      "affiliations": [
        "Taipei Veterans General Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2191886027",
      "name": "Lei-chi Wang",
      "affiliations": [
        "Taipei Veterans General Hospital",
        "National Yang Ming Chiao Tung University"
      ]
    },
    {
      "id": "https://openalex.org/A2112697636",
      "name": "Chen Tsung Kuo",
      "affiliations": [
        "Taipei Veterans General Hospital",
        "National Taipei University of Nursing and Health Science"
      ]
    },
    {
      "id": "https://openalex.org/A4261460789",
      "name": "Paul Chih-Hsueh Chen",
      "affiliations": [
        "National Yang Ming Chiao Tung University",
        "Taipei Veterans General Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2591525028",
      "name": "Yuan-Chia Chu",
      "affiliations": [
        "National Taipei University of Nursing and Health Science",
        "Taipei Veterans General Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2498019514",
      "name": "Yi-Chen Yeh",
      "affiliations": [
        "National Yang Ming Chiao Tung University",
        "Taipei Veterans General Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2162574638",
      "name": "Kuan-Hsun Lin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5033468320",
      "name": "Tzu-Hang Kao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2191886027",
      "name": "Lei-chi Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2112697636",
      "name": "Chen Tsung Kuo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4261460789",
      "name": "Paul Chih-Hsueh Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2591525028",
      "name": "Yuan-Chia Chu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2498019514",
      "name": "Yi-Chen Yeh",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4399036758",
    "https://openalex.org/W4213091130",
    "https://openalex.org/W3080458669",
    "https://openalex.org/W4379618497",
    "https://openalex.org/W2566723262",
    "https://openalex.org/W4400424823",
    "https://openalex.org/W4210412949",
    "https://openalex.org/W6738337739",
    "https://openalex.org/W2583911935",
    "https://openalex.org/W2888034354",
    "https://openalex.org/W4403156700",
    "https://openalex.org/W4311768404",
    "https://openalex.org/W2995993150",
    "https://openalex.org/W4398223047",
    "https://openalex.org/W4387809804",
    "https://openalex.org/W4407174849",
    "https://openalex.org/W4394019136",
    "https://openalex.org/W1960235058",
    "https://openalex.org/W4404523826",
    "https://openalex.org/W4394767601",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4213353324",
    "https://openalex.org/W4225751542"
  ],
  "abstract": null,
  "full_text": "npj |precision oncology Article\nPublished in partnership with The Hormel Institute, University of Minnesota\nhttps://doi.org/10.1038/s41698-025-00935-4\nBenchmarking large language models\nGPT-4o, llama 3.1, and qwen 2.5 for\ncancer genetic variant classiﬁcation\nCheck for updates\nKuan-Hsun Lin1,2,T z u - H a n gK a o3, Lei-Chi Wang3,4,C h e n - T s u n gK u o1,2, Paul Chih-Hsueh Chen3,4,\nYuan-Chia Chu1,2,5 & Yi-Chen Yeh3,4\nClassifying cancer genetic variants based on clinical actionability is crucial yet challenging in precision\noncology. Large language models (LLMs) offer potential solutions, but their performance remains\nunderexplored. This study evaluates GPT-4o, Llama 3.1, and Qwen 2.5 in classifying genetic variants\nfrom the OncoKB and CIViC databases, as well as a real-world dataset derived from FoundationOne\nCDx reports. GPT-4o achieved the highest accuracy (0.7318) in distinguishing clinically relevant\nvariants from variants of unknown clinical signiﬁcance (VUS), outperforming Qwen 2.5 (0.5731) and\nLlama 3.1 (0.4976). LLMs demonstrated better concordance with expert annotations for variants with\nstrong clinical evidence but exhibited greater inconsistencies for those with weaker evidence. All three\nmodels showed a tendency to assign variants to higher evidence levels, suggesting a propensity for\noverclassiﬁcation. Prompt engineering signiﬁcantly improved accuracy, while retrieval-augmented\ngeneration (RAG) further enhanced performance. Stability analysis across 100 iterations revealed\ngreater consistency with the CIViC system than with OncoKB. Theseﬁndings highlight the promise of\nLLMs in cancer genetic variant classiﬁcation while underscoring the need for further optimization to\nimprove accuracy, consistency, and clinical applicability.\nIn the era of precision medicine, cancer genetic testing has become essential\nin guiding the treatment of cancer patients. Multigene next-generation\nsequencing (NGS), and even comprehensive genomic proﬁling using large\nNGS panels, are now recommended to thoroughly assess genomic altera-\nt i o n si nt u m o r s\n1–3. Evidence suggests that patients managed with NGS-\ninformed targeted treatments achieve superior outcomes across multiple\ntumor types4. Consequently, tumor genomic proﬁling using NGS has\nincreasingly become a standard of care in clinical practice.\nOne signiﬁcant challenge in implementing NGS testing is the com-\nplexity of interpreting NGS results. Large NGS panels, such as compre-\nhensive genomic pro ﬁling panels, often detect numerous genomic\nalterations, many of which are rare and unfamiliar to clinicians or\noncologists. To enhance the understanding of NGS results and maximize\nthe utility of NGS testing, it is recommended that identiﬁed genomic\nalterations be annotated with their pathogenicity and clinical actionability\nin the clinical reports\n5,6. For pathogenicity classiﬁcation, the ﬁve-tier\nsystem recommended by ClinGen, CGC, and VICC categorizes variants\nbased on their biological and functional impact as pathogenic, likely\npathogenic, variant of unknown signiﬁcance, likely benign, or benign\ngroup\n7. For clinical actionability, the AMP/ASCO/CAP classiﬁcation\ncategorizes variants into four tiers: Tier I for variants of strong clinical\nsigniﬁcance, Tier II for variants of potential clinical signiﬁcance, Tier III\nfor variants of unknown clinical signiﬁcance (VUS), and Tier IV for\nbenign or likely benign variants. Other widely used frameworks include\nthe ESMO Scale for Clinical Actionability of Molecular Targets (ESCAT),\nthe OncoKB level of evidence classiﬁcation, and the Clinical Interpreta-\ntion of Variants in Cancer (CIViC) evidence levels\n5,8–10. These classiﬁca-\ntion systems play a crucial role in standardizing the assessment of genomic\nalterations in oncology. By integrating these frameworks into NGS\nreports, genomic alterations can be presented in a structured and stan-\ndardized format, improving report readability and facilitating inter-\npretation by clinicians and oncologists.\n1Department of Information Management, Taipei Veterans General Hospital, Taipei, Taiwan, ROC.2Department of Information Management, National Taipei\nUniversity of Nursing and Health Sciences, Taipei, Taiwan, ROC.3Department of Pathology and Laboratory Medicine, Taipei Veterans General Hospital,\nTaipei, Taiwan, ROC.4School of Medicine, National Yang Ming Chiao Tung University, Taipei, Taiwan, ROC.5Big Data\nCenter, Taipei Veterans General Hospital, Taipei, Taiwan, ROC.e-mail: xd.yuanchia@gmail.com;\nlordaaa@gmail.com\nnpj Precision Oncology|           (2025) 9:141 1\n1234567890():,;\n1234567890():,;\nAlthough variant classiﬁcation systems are effective and widely used in\nclinical practice, accurate classiﬁcation remains a complex challenge,\nrequiring a comprehensive evaluation of biological and functional evidence,\nmedical literature, clinical trial data, treatment guidelines, and FDA\napprovals. The process is highly expertise-dependent and labor-intensive.\nVariant knowledge databases, whether proprietary or public, can sig-\nniﬁcantly aid this process. However, maintaining up-to-date databases with\nthe latest literature is an equally demanding task, requiring expert input and\nmanual effort. This is exempliﬁed by the OncoKB and CIViC databases,\nwhich rely on specialized committe es and expert crowdsourcing,\nrespectively\n8,9. Furthermore, variant classiﬁcation is inherently subjective,\nwith studies showing signiﬁcant interobserver variability among experts11–13.\nArtiﬁcial Intelligence (AI), particularly large language models (LLMs),\nholds signiﬁcant potential in enhancing the variant classiﬁcation process. AI\nhas already proven its value across various medical domains, including\ndisease diagnosis, clinical decision support, medical image analysis, and the\nautomation of medical record processing14,15. LLMs excel at managing and\nanalyzing vast quantities of unstructured data, such as medical literature,\npatient records, and clinical reports. Their strength lies in their ability to\nprocess large volumes of information quickly, enabling efﬁcient under-\nstanding and summarization of key insights. In theﬁeld of variant classi-\nﬁcation, LLMs can be leveraged to analyze vast amounts of medical\nliterature, clinical trial data, and treatment guidelines to assess the clinical\nsigniﬁcance of genetic alterations. This capability holds great potential for\nautomating and continuously updating genetic variant classiﬁcations.\nSeveral previous studies have explored the use of LLMs in genetic\nvariant interpretation. For instance, Lu et al. utilized LLMs for variant\nannotation and demonstrated that retrieval-augmented generation (RAG)\nand ﬁne-tuning can enhance performance16. Paoli et al. developed a gen-\nerative AI assistant, VarChat, which facilitates efﬁcient literature retrieval\nand summarization from PubMed based on gene symbols and genomic\nvariants\n17. While these studies have highlighted the potential of LLMs in\nsupporting variant annotation and interpretation, their utility in assisting\ngenetic variant classiﬁcation, particularly in terms of clinical signiﬁcance or\nactionability scale, remains largely unexplored. To address this gap, this\nstudy aims to evaluate the effectiveness of LLMs in classifying genetic var-\niants based on clinical signiﬁcance or actionability, compare the perfor-\nmance of different models, and analyzes their respective strengths and\nlimitations.\nResults\nPerformance of LLMs in distinguishing between clinically rele-\nvant variants and VUS\nTo assess the ability of LLMs to distinguish clinically relevant variants from\nVUS, we analyzed 10,506 genetic variants from NGS testing reports of 612\npatients who underwent FoundationOne CDx testing at our hospital. In\nthese reports, genetic variants are categorized as either clinically relevant\n(listed in the Genomic Findings section of the reports,n = 5240) or (VUS,\nlisted in the APPENDIX: Variants of Unknown Signiﬁcance section of the\nreports,n = 5266).\nWe instructed the LLMs to classify genetic variants using the CIViC\nclassiﬁcation system, following the system prompts detailed in Supple-\nmentary Table 3 (Variant classiﬁcation (CIViC level of evidence system)—\nBasic prompt). Variants classiﬁed within CIViC levels A to E were grouped\nas clinically relevant, while those categorized as “VUS” were\nconsidered VUS.\nT h ep e r f o r m a n c eo ft h eL L M si nc l assifying variants from the Foun-\ndationOne dataset is summarized in Table1 and Fig.1.G P T - 4 od e m o n -\nstrated signiﬁcantly higher accuracy (0.7318) compared to Qwen 2.5\n(0.5731) and Llama 3.1 (0.4976). Figure2A presents the confusion matrix of\nLLM classiﬁcations compared to the ground truth annotations from the\nFoundationOne CDx report. Theﬁgure highlights distinct behavioral dif-\nferences among the LLMs. Both Llama 3.1 and Qwen 2.5 tended to overcall\nVUS as clinically relevant variants, while GPT-4o did not exhibit this bias.\nConversely, while GPT-4o accurately classiﬁed 94.1% of VUS variants, it\nmisclassiﬁed nearly half of the clinically relevant variants as VUS, suggesting\na more conservative approach in classifying variants as clinically relevant.\nThe distribution of LLM classiﬁcations based on the CIViC level of evidence\nsystem for ground truth clinically relevant variants and VUS is shown in\nSupplementary Fig. 1. Among VUS misclassiﬁed as clinically relevant by\nLLMs, most were assigned to CIViC levels C to E, which correspond to weak\nor indirect clinical evidence.\nAdditionally, we evaluated accuracy in cases where all three LLMs\nagreed on the classiﬁcation versus those with discordant results. Each LLM’s\nanswer was determined by its most frequently selected response across all\niterations. Among the 10,506 genetic variants, 2766 (26.3%) had identical\nclassiﬁcations across all three LLMs. In these cases, the three-model con-\nsensus achieved a high accuracy of 0.9732 (Supplementary Table 4).\nThe stability of the LLMs’responses in the FoundationOne dataset\nacross 100 iterations is shown in Fig.2B. All three LLMs exhibited high\nconsistency ratios across queries, with the majority of queries achieving a\nconsistency ratio above 90%, indicating that the same answer was provided\nin over 90% of iterations.\nTo compare LLM results with human experts, we randomly selected\n100 variants from the FoundationOne dataset and asked three pathologists\nto classify them as either clinically relevant variants or VUS. The agreement\nbetween FoundationOne annotations,LLM responses, and pathologists’\nclassiﬁcations is visualized in Supplementary Fig. 2. The high inter-\npathologist agreement suggests strong consistency among human experts.\nNotably, GPT-4o aligns more closely with the pathologists than other AI\nm o d e l s .I nc o n t r a s t ,L l a m a3 . 1a n dQ w e n2 . 5s h o wl o w e ra g r e e m e n tw i t h\nboth the pathologists and FoundationOne, indicating greater variability in\ntheir classiﬁcations. Theseﬁndings suggest that GPT-4o provides more\nconcordant assessments, whereas other AI models deviate more sig-\nniﬁcantly from both human judgment andthe FoundationOne annotations.\nPerformance of LLMs in classifying genetic variants into evi-\ndence tiers: analysis of variants from the OncoKB database\nThe performance of the LLMs in classifying variants from the OncoKB\ndatabase based on the OncoKB level of evidence system is summarized in\nTable 1 and Fig.1. Among the three LLM models (GPT-4o, Llama 3.1, and\nQwen 2.5), GPT-4o achieved the highest top-1 accuracy of 0.3393, while\nLlama 3.1 showed the lowest top-1 accuracy at 0.3066. In cases where all\nthree LLMs agreed on the top-1 classiﬁcation (43.7% of variants), the\naccuracy increased to 0.4286. (Supplementary Table 4).\nIn terms of top-2 and top-3 accuracy, Qwen 2.5 performed the best,\nwith scores of 0.4357 and 0.4567, respectively. In contrast, GPT-4o and\nLlama 3.1 showed only modest improvements in top-2 and top-3 accuracy.\n(Supplementary Fig. 3 and Supplementary Table 5). This can be attributed\nto the tendency of both GPT-4o and Llama 3.1 to provide only a single\nanswer in 87.2% and 93.9% of cases, respectively, compared to just 5.2% for\nQwen 2.5, despite explicit instructions in the system prompts to generate up\nto three answers.\nFigure 3A shows the confusion matrix of LLM classiﬁcation (top-1\nprediction only) compared with the ground truth expert annotation in the\nOncoKB database. As depicted in theﬁgure, we observed that for variants\nwith the highest clinical evidence (OncoKB level 1), the LLMs provided\nmore accurate classiﬁcations. In contrast, for variants with weaker clinical\nevidence (OncoKB levels 2, 3, and 4), the LLMs exhibited greater dis-\ncordance with the ground truth expert annotation. However, most of the\ndiscordance fell within one degree of difference (e.g., misclassifying level 2\nvariants as level 3). For OncoKB levels associated with drug resistance (R1\nand R2), the LLMs often misclassiﬁed these variants as levels 1–4 instead of\nR1 or R2. Notably, GPT-4o showed better performance in accurately clas-\nsifying more R1 variants compared to other LLMs.\nThe stability of the LLMs’responses across 100 iterations is shown in\nFig. 3B. As illustrated, Qwen 2.5 demonstrated signiﬁcantly higher stability\ncompared to GPT-4o and Llama 3.1. For most queries, Qwen 2.5 achieved a\nconsistency ratio above 90%, whereas GPT-4o and Llama 3.1 exhibited a\nbroader distribution of consistency ratios across queries.\nhttps://doi.org/10.1038/s41698-025-00935-4 Article\nnpj Precision Oncology|           (2025) 9:141 2\nTable 1 | Classiﬁcation accuracy of LLMs (GPT-4o, Llama 3.1, Qwen 2.5) based on top-1 answer in the FoundationOne, OncoKB,\nand CIViC datasets\nGPT-4o Llama 3 Qwen 2.5\nMean accuracy 95% CI Mean accuracy 95% CI Mean accuracy 95% CI p value\nFoundation\none\n0.7318 0.7307 –0.7329 0.4976 0.4974 –0.4978 0.5731 0.5725 –0.5736 <0.001\nOncoKB 0.3393 0.3369 –0.3417 0.3066 0.3041 –0.309 0.3328 0.3316 –0.334 <0.001\nCIViC 0.1865 0.1857 –0.1874 0.1212 0.1205 –0.1219 0.2485 0.2477 –0.2492 <0.001\nFig. 1 |Classiﬁcation accuracy comparison among\nGPT-4o, Llama 3.1, and Qwen 2.5 across the\nFoundationOne, OncoKB, and CIViC datasets.\nFig. 2 | Performance and consistency of LLM classiﬁcation on the FoundationOne dataset. AConfusion matrix of LLM classiﬁcation compared with the ground truth in\nthe FoundationOne dataset.B Consistency ratio of the LLMs’responses in the FoundationOne dataset across 100 iterations.\nhttps://doi.org/10.1038/s41698-025-00935-4 Article\nnpj Precision Oncology|           (2025) 9:141 3\nPerformance of LLMs in classifying genetic variants into evi-\ndence tiers: analysis of variants from the CIViC database\nThe performance of LLMs in classifying variants from the CIViC database is\nsummarized in Table1 and Fig.1. Among the three models, Qwen 2.5\nachieved the highest top-1 accuracy (0.2485), while GPT-4o (0.1865) and\nLlama 3.1 (0.1212) performed lower in comparison. For top-2 and top-3\naccuracy, Qwen 2.5 again outperformed the others, with mean accuracies of\n0.5476 and 0.6992, respectively. GPT-4o and Llama 3.1 showed only modest\nimprovements in these metrics. (Supplementary Fig. 4 and Supplementary\nTable 6) This trend is similar to what was observed in the OncoKB dataset,\nwhere both GPT-4o and Llama 3.1 predominantly provided a single\nresponse (GPT-4o: 97.2%, Llama 3.1: 91.5%, Qwen 2.5: 6.4%), limiting their\ntop-2 and top-3 accuracy gains. Unlike the trends observed in the Foun-\ndationOne and OncoKB datasets, accuracy did not improve when all three\nLLMs agreed on the classiﬁcation in the CIViC dataset (Supplementary\nTable 4).\nFigure 4A shows the confusion matrix of LLM classiﬁcation (top-1\nprediction only) compared with theground truth annotation in the CIViC\ndatabase. As depicted in theﬁgure, we observed that for variants with the\nhighest clinical evidence (CIViC level A), all three LLMs provided the most\naccurate classiﬁcations. In contrast, for variants with weaker clinical evi-\ndence (CIViC levels B, C, D, and E), the LLMs exhibited greater discordance\nwith the ground truth expert annotation. In addition, we observed that\nLLMs tend to classify variants at higher CIViC levels, such as classifying level\nB variants as level A, or level C variants as either level B or A.\nThe stability of the LLMs’responses in the CIViC dataset across 100\niterations is presented in Fig.4B. Compared to their stability in the OncoKB\ndataset, the LLMs’ responses in the CIViC dataset demonstrated sig-\nniﬁcantly higher consistency ratios across queries, with most queries\nachieving a consistency ratio above 90%.\nLLM reasoning for classiﬁcation\nTo gain a deeper understanding of the LLM’s rationale for classifying genetic\nvariants, we also examined its explanations for selected cases. Supplemen-\ntary Table 7 provides examples of these LLM responses. In these examples,\nwe identiﬁed several factors that contributed to the LLMs’misclassiﬁcations.\nFor instance, some LLMs appeared unaware of recent FDA approvals. In the\ncase of KRAS G12C in non-small cell lung cancer, GPT-4o recognized its\nFDA approval and correctly classiﬁed it as OncoKB level 1. By contrast,\nQwen 2.5 and Llama 3.1 seemed unaware of this approval and misclassiﬁed\nthe variant as level 3A, resulting in under-classiﬁcation. Moreover, LLMs do\nn o ta l w a y sa c c o u n tf o rt h es p e c iﬁc details of genetic alterations, as illustrated\nby the NTRK1 Q570* variant. This variant is best classiﬁed as VUS due to\nlimited data on its biological impact and clinical signiﬁcance, and there is no\nevidence indicating that it would result in NTRK1 activation, unlike NTRK1\nfusions. However, Llama 3.1 mistakenly concluded that the variant leads to\nconstitutive activation of the NTRK1 kinase and misclassiﬁed it as CIViC\nlevel A. Meanwhile, GPT-4o considered this NTRK1 alteration likely to\nrespond to TRK inhibitors and misclassiﬁed it as CIViC level B. Both\nmisclassiﬁcations resulted in over-classiﬁcation. Likewise, although MSH6\nK1358fs*2i saf r a m e s h i f tm u t a t i o n ,i ti sr e l a t i v e l yc o m m o ni nt h ep o p u -\nlation and is predicted to be nonpathogenic because it only deletes two\namino acids in the C-terminal region, potentially leaving the protein\nfunction intact\n18. Nevertheless, all three LLMs classiﬁed this variant as\npathogenic, assuming it would lead to microsatellite instability (MSI-high),\nand thus overclassiﬁed it as CIViC level B.\nExploring factors affecting LLMs’ performance and behavior\nFinally, we explored various factors that may inﬂuence LLM performance\nand behavior. First, we reﬁned the original basic system prompts, which\nsimply instructed the LLMs to provide the classiﬁcation level number. The\nrevised prompts speciﬁed the LLM’s role, objectives, scope, behavior, and\nexpected input/output format in greater detail (Supplementary Table 3). To\nassess the impact of these reﬁnements, we compared the performance of the\nQwen 2.5 model using basic versus reﬁned prompts (Figs.5 and 6). In the\nOncoKB and CIViC datasets, top-1 accuracy decreased with reﬁned\nprompts (from 0.3328 to 0.2994 in OncoKB and from 0.2485 to 0.1722 in\nCIViC). However, in the FoundationOne dataset, reﬁned prompts led to a\nsubstantial accuracy improvement (from 0.5731 to 0.7246). Additionally, as\ns h o w ni nt h ec o n f u s i o nm a t r i x ,Q w e n2 . 5w i t hb a s i cp r o m p t st e n d e dt o\novercall VUS as clinically relevant variants. This tendency was signiﬁcantly\nreduced with reﬁned prompts. Instead, nearly half of the clinically relevant\nFig. 3 | Comparing LLM classiﬁcation against expert annotations in the OncoKB dataset. AConfusion matrix showing the top-1 classiﬁcation predictions of GPT-4o,\nLlama 3.1, and Qwen 2.5 compared to expert annotations for genetic variants in the OncoKB dataset.B Consistency ratio of LLMs’responses across 100 iterations in the\nOncoKB dataset.\nhttps://doi.org/10.1038/s41698-025-00935-4 Article\nnpj Precision Oncology|           (2025) 9:141 4\nvariants were classiﬁed as VUS, suggesting that reﬁned prompts encourage a\nmore conservative classiﬁcation approach.\nNext, we evaluated the impact of using a binary classiﬁcation prompt\ninstead of the original CIViC level of evidence prompt for classifying clinically\nrelevant variants versus VUS in the FoundationOne dataset. Unlike the CIViC\nprompt, which instructs LLMs to provide a classiﬁcation based on CIViC\nevidence levels, the binary classiﬁcation prompt directly asks the LLMs to\ncategorize each variant as either clinically relevant or VUS (see Supplementary\nTable 3, Variant classiﬁcation - Binary classes). As shown in Fig.6,t h eb i n a r y\nclassiﬁcation prompt slightly improved accuracy from 0.5731 to 0.6119.\nAdditionally, similar to the reﬁned prompt, it resulted in a more conservative\nclassiﬁcation approach, with a tendency to classify variants as VUS.\nWe also explored the potential of RAG to enhance LLM performance\nby integrating data from the CIViC database and FDA oncology drug\napproval information. As shown in Fig.6, implementing RAG signiﬁcantly\nimproved classiﬁcation accuracy from 0.5731 to 0.6616 in the Foundatio-\nnOne dataset.\nFinally, we evaluated the impact of model temperature settings on the\nstability and consistency of LLM responses using Llama 3.1 on the OncoKB\ndataset. As shown in Fig.7, lowering the model temperature signiﬁcantly\nimproved response stability, achieving a 100% consistency ratio when the\ntemperature was set to 0. Interestingly, reducing the model temperature also\nled to a slight improvement in accuracy, with top-1 accuracy increasing\nfrom 0.3066 at a temperature of 0.8 to 0.3178 at 0.4 and 0.3312 at 0 (Fig.7).\nFig. 4 | Comparing LLM classiﬁcation against expert annotations in the CIViC dataset. AConfusion matrix showing the top-1 classiﬁcation predictions of GPT-4o, Llama\n3.1, and Qwen 2.5 compared to expert annotations for genetic variants in the CIViC dataset.B Consistency ratio of LLMs’responses across 100 iterations in the CIViC\ndataset.\nFig. 5 | Performance comparison of the Qwen 2.5 model using basic and reﬁned prompts. AResults on the OncoKB dataset.B Results on the CIViC dataset.\nhttps://doi.org/10.1038/s41698-025-00935-4 Article\nnpj Precision Oncology|           (2025) 9:141 5\nTaken together, these results demonstrate that LLM performance can\nbe substantially inﬂuenced by prompt design, classiﬁcation scheme, and\nmodel temperature. Table2 summarizes the comparative performance of\nLLMs under these varying conﬁgurations across different datasets, high-\nlighting the best-performing conditions for each model–dataset pair.\nDiscussion\nThe use of LLMs as a supportive tool for clinical practice demon-\nstrates signi ﬁcant potential 14,19. Previous studies have shown that\nLLMs can achieve physician-level performance on medical board\nexaminations 20,21. However, their capabilities in specialized medical\ndomains requiring high expertise, such as genetic variant classiﬁca-\ntion, remain largely unexplored. This study provides theﬁrst systemic\nanalysis of LLMs in the context of classifying cancer genetic variants\nbased on their clinical relevance and actionability. Our primary\nobjective is to assess the baseline performance of LLMs for this task,\nwithout any specialized training or reinforcement, aiming to mini-\nmize bias. This research highlights the unique strengths and limita-\ntions of each model, offering valuable insights into their potential\napplications in precision medicine.\nFig. 6 | Performance comparison of the Qwen 2.5 model across different prompt conﬁgurations in the FoundationOne dataset. AAccuracy comparison using basic,\nreﬁned, and binary prompts, as well as Retrieval-Augmented Generation (RAG).B Confusion matrix illustrating the classiﬁcation outcomes for each prompt condition.\nFig. 7 | Effect of temperature settings on LLM performance and consistency.Accuracy (A) and response consistency (B) of Llama 3.1 in the OncoKB dataset across 100\niterations. Results are shown for temperature settings of 0.8 (default), 0.4, and 0.\nTable 2 | Model accuracy under different prompt conﬁgurations and temperature settings\nModel Model conditions Dataset Accuracy Best-in- group\nQwen2.5 Basic prompt + Default temperature (0.8) FoundationOne 0.5731\nQwen2.5 Re ﬁned prompt+ Default temperature (0.8) FoundationOne 0.7246 ✔\nQwen2.5 Binary class prompt + Default temperature (0.8) FoundationOne 0.6119\nQwen2.5 RAG + Default temperature (0.8) FoundationOne 0.6616\nQwen2.5 Basic prompt + Default temperature (0.8) OncoKB 0.3328 ✔\nQwen2.5 Re ﬁned prompt+ Default temperature (0.8) OncoKB 0.2994\nQwen2.5 Basic prompt + Default temperature (0.8) CIViC 0.2485 ✔\nQwen2.5 Re ﬁned prompt+ Default temperature (0.8) CIViC 0.1722\nLlama 3.1 Basic prompt + Default temperature (0.8) OncoKB 0.3066\nLlama 3.1 Basic prompt + temperature (0.4) OncoKB 0.3178\nLlama 3.1 Basic prompt + temperature (0) OncoKB 0.3312 ✔\nThe best-performing conﬁguration for each model–dataset group is indicated with a check mark (✔).\nhttps://doi.org/10.1038/s41698-025-00935-4 Article\nnpj Precision Oncology|           (2025) 9:141 6\nRegarding stability of responses, we observed that LLMs may exhibit\ndifferent behaviors depending on the variant classiﬁcation system used, i.e.,\nOncoKB versus CIViC. Both GPT-4o and Llama 3.1 displayed signiﬁcantly\nhigher variability (lower consistencyratios) and tended to provide differing\nresponses across query iterations when using the OncoKB classiﬁcation\nsystem compared to the CIViC system. One possible explanation is that the\nsystem prompt for the CIViC classiﬁcation system includes examples for\neach variant category in addition to the category deﬁnitions themselves (see\nSupplementary Table 3). This additional context may provide clearer gui-\ndance for LLMs, enabling them to better determine the appropriate category\nfor a given variant and reduce uncertainty in classiﬁcation. However, it is\nnoteworthy that Qwen 2.5 does not exhibit this variability even when using\nthe OncoKB system prompt, highlighting inherent differences between\nthe LLMs.\nOne potential factor inﬂuencing this variability is the model tem-\nperature setting. The default temperature values set by each LLM API may\nhave inﬂuenced the observed differences in classiﬁcation consistency. Since\nhigher temperatures introduce more randomness in model outputs, future\nstudies could investigate whether adjusting temperature settings improves\nclassiﬁcation stability while maintaining accuracy. Systematic evaluation of\nmodel behavior at different temperature settings would help identify the\noptimal balance between stability andﬂexibility in clinical applications.\nIn terms of response accuracy in classifying variants into different tiers\nof clinical signiﬁcance, we observed that LLMs demonstrated higher accu-\nracy for variants with the strongest clinical evidence, while showing lower\naccuracy for those with weaker clinical evidence. Thisﬁnding aligns with a\nprevious multi-institutional interrater agreement evaluation among human\nexperts in cancer genetic variant classiﬁcation, which revealed that the\ngreatest disagreement occurred in distinguishing between AMP/ASCO/\nCAP variant categorization tiers II (Variants of Potential Clinical Sig-\nniﬁcance) and III (VUS)\n13. Notably, we also observed that when using the\nCIViC classiﬁcation system, all three LLMs tended to assign variants to\nhigher levels of evidence, such as classifying level B variants as level A, or\nclassifying level C variants as either level B or A. This is an important\nobservation, as it suggests a tendency toward overclassiﬁcation, which may\nimpact the clinical interpretation and actionability of the variants.\nIn the real-world dataset derived from FoundationOne CDx NGS\nreports, it is noteworthy that GPT-4o achieved an accuracy of 0.7318 in\ndistinguishing between clinically relevant variants and VUS, without any\nspecialized training or reinforcement. In comparison, the recent AMP\nVITAL Somatic Challenge, a variant interpretation challenge involving 134\nhuman expert participants, reported that 86% (range: 54%–94%) of\nresponses correctly distinguished clinically signiﬁcant variants from other\nvariants\n12. Although the current accuracy of LLMs is not yet comparable to\nthat of human experts, further adjustments or training could enhance their\nperformance. Previous studies have demonstrated that RAG andﬁne-\ntuning can enhance LLM performancein genetic variant annotation and\ninterpretation, with RAG outperformingﬁne-tuning\n16. RAG combines the\nstrengths of both pre-trained language models and external knowledge\nsources (such as PubMed), enabling the LLMs to retrieve and incorporate\nrelevant information from external databases or documents\n22.T h i s\napproach is particularly well-suited for specialized tasks requiring advanced\ndomain knowledge, such as genetic variant classiﬁcation. In our study, we\nalso evaluated the impact of RAG on LLM performance and found that\nimplementing RAG in Qwen 2.5 signiﬁcantly improved classiﬁcation\naccuracy in the FoundationOne dataset. Further studies are needed to more\ncomprehensively evaluate the effectiveness of RAG in improving LLM\nperformance, particularly in terms of accuracy, consistency, and the ability\nto integrate dynamic, up-to-date scientiﬁc knowledge and clinical evidence.\nOur study also demonstrated that system prompt design signiﬁcantly\ninﬂuences LLM performance and behavior in cancer genetic variant clas-\nsiﬁcation. In the FoundationOne dataset experiments, reﬁned prompts with\ndetailed instructions led to a substantial accuracy improvement compared\nto basic prompts. Moreover, reﬁned prompts resulted in a more con-\nservative classiﬁcation approach, with a tendency to classify variants as VUS.\nIn contrast, the basic prompt was morelikely to overcall VUS as clinically\nrelevant variants. A potential explanation is that the reﬁned prompt expli-\ncitly deﬁned the LLM’s role as an expert assistant specializing in cancer\ngenetic variant classiﬁcation, reinforcing a more cautious and evidence-\nbased decision-making approach.\nBy examining the LLM’s rationale for classifying genetic variants, we\nnoted some LLMs appeared unaware of recent FDA approvals. One\npotential factor that may contribute tothe observation is the data cut-off of\neach LLM. Since LLMs rely onﬁxed training datasets, their ability to\naccurately classify newly validated genetic variants or FDA-approved tar-\ngeted therapies is inherently constrained by the recency of their training\ndata. Among the models evaluated, GPT-4o’s training data cut-off was in\nOctober 2023, Llama 3.1’s was in December 2023, while Qwen 2.5’se x a c t\ncut-off date has not been disclosed. However, these data cut-off dates do not\nfully explain our case studyﬁndings on KRAS G12C in non-small cell lung\ncancer. GPT-4o recognized its FDA approval and correctly classiﬁed it as\nOncoKB Level 1, while Llama 3.1 misclassiﬁed the variant as a lower evi-\ndence tier, seemingly unaware of its approval. This discrepancy is unex-\npected, as the FDA approved KRAS G12C for non-small cell lung cancer in\nDecember 2022, well before the training data cut-off dates of both GPT-4o\nand Llama 3.1. Therefore, factors beyond training data recency— such as\ndifferences in data sources, weighting of biomedical knowledge, or rea-\nsoning mechanisms— may contribute to variations in LLM performance.\nUpdating data sources remains a signiﬁcant challenge in clinical\ngenomics, as genetic variant interpretation continuously evolves with new\nresearch ﬁndings and regulatory updates. To address this, integrating\nexternal knowledge retrieval and leveraging frequently updated models can\nhelp bridge the gap between static training data and dynamic, real-world\nclinical knowledge. One promising approach involves utilizing tools like\nDeep Research ( https://openai.com/index/introducing-deep-research/),\nwhich can perform in-depth, multi-step research across the internet to\ngather the latest information and generate comprehensive summaries.\nThese tools could enable the autonomous updating of knowledge bases with\nthe most current evidence, reﬁning variant classi\nﬁcations based on emerging\nliterature, clinical guidelines, and regulatory changes. This would help\nmitigate the lag between static datasets and real-world advancements in\ngenomics. Additionally, implementing quality control mechanisms— such\nas cross-referencing newly retrieved data against established databases (e.g.,\nCIViC, OncoKB) or expert review— is critical to ensuring reliability. Ulti-\nmately, a hybrid approach that combines AI-assisted knowledge retrieval,\nexpert evaluation, and structured database updates could help maintain\naccurate, clinically relevant, and up-to-date genomic interpretations.\nOur study has several limitations. First, the datasets used do not cover\nthe full spectrum of cancer genetic variants. The OncoKB and CIViC\ndatasets include only clinically relevant variants, while the FoundationOne\ndataset contains clinically relevantvariants and VUS. Neither dataset\nincludes benign or likely benign variants. Consequently, we were unable to\nassess LLM performance in distinguishing benign or likely benign variants\nfrom other classiﬁcations. Future studies should address this limitation by\nincorporating cancer genetic variant datasets with ground truth annotations\nfor benign and likely benign somatic variants. Second, in evaluating LLM\nperformance in classifying variants from the OncoKB and CIViC databases,\nit cannot be excluded that current models were at least partially trained on\nthese publicly available datasets, potentially inﬂuencing the results. Never-\ntheless, ourﬁndings indicate that even for variants documented in these\ndatasets, current LLMs still have substantial room for improvement in\naccurately stratifying variants into evidence tiers. Third, due to the lack of\nlarge-scale ground truth datasets, we did not evaluate the performance of\nLLMs using two other widely adopted cancer genetic variant classiﬁcation\nsystems: the AMP/ASCO/CAP variant categorization and the ESCAT fra-\nmework. As ourﬁndings suggest that the performance of LLMs may vary\ndepending on the classiﬁcation system used, it is imperative that future\nefforts focus on assessing the capabilities of LLMs within these alternative\nframeworks. Lastly, the three LLMs evaluated in our study are general-\np u r p o s em o d e l sr a t h e rt h a nb i o m e d i c al-specialized LLMs. Investigating\nhttps://doi.org/10.1038/s41698-025-00935-4 Article\nnpj Precision Oncology|           (2025) 9:141 7\nwhether biomedical-specialized models, such as BioMedLM and Open-\nBioLLM, could further enhance performance in genetic variant classiﬁca-\ntion would be highly valuable23,24.\nIn conclusion, this study shows that LLMs hold promise for assisting in\nthe classiﬁcation of cancer genetic variants. Each LLM has unique strengths\nand weaknesses that must be considered for clinical use. Future research\ns h o u l df o c u so nr eﬁning these models to improve accuracy and consistency\nacross various datasets, thereby enhancing their applications in clinical\ngenomics.\nMethods\nDataset\nWe included three datasets to evaluate the performance of cancer genetic\nvariant classiﬁcation by LLMs. Theﬁrst two datasets were obtained from\npublic available database, OncoKB and CIViC. The third dataset was\ncompiled from the real-world NGS testing reports of FoundationOne\nCDx assay.\nFor the OncoKB dataset, we downloaded the variant clinical implica-\ntions data table from the OncoKB website (https://www.oncokb.org/\nactionable-genes, last accessed: 2024/11/20). The table consists ofﬁve col-\numns: Level, Gene, Alterations, CancerTypes, and Drugs. The Level column\nindicates the evidence level of a speciﬁc variant, which is listed in the Gene\n(e.g., ABL1) and Alterations (e.g., BCR-ABL1 Fusion) columns, within a\nparticular cancer type (e.g., B-Lymphoblastic Leukemia/Lymphoma) as\nannotated by the expert committee according to the OncoKB classiﬁcation\nof evidence levels\n8. In total, there are 625 variant associations, including 182,\n154, 114, 80, 34, and 61 associations with evidence levels of Level 1, 2, 3, 4,\nR1, and R2, respectively. (Supplementary Table 1)\nFor the CIViC dataset, we downloaded the Clinical Evidence Summary\ndata table from the CIViC website (https://civicdb.org/releases/main,l a s t\naccessed: 2024/11/20). This table provides detailed information on the\nclinical impact of variants, evidence statements, and data sources. We\nretrieved three columns of data: molecular_proﬁle, disease, and evi-\ndence_level. The molecular_proﬁle column contains the gene and altera-\ntions, such as JAK2 V617F. The disease column speciﬁes the cancer type,\nsuch as Lymphoid Leukemia. The evidence_level column includes the\nCIViC evidence level, annotated through expert crowdsourcing and\nreviewed by expert editors\n9. In total, there are 4426 variant associations,\nincluding 166, 1465, 1547, 1216, and 32 associations with evidence levels of\nA, B, C, D, and E, respectively. (Supplementary Table 2)\nTo evaluate the performance ofLLMs in genetic variant classiﬁcation\nwithin real-world clinical settings, we extracted variant data from Foun-\ndationOne CDx assay NGS reports for 612 patients at our hospital. The\nvariants were categorized into two groups: (a) Clinically Relevant: Genetic\nalterations listed in the Genomic Findings section of the report. (b) Variants\nof Unknown Clinical Signiﬁcance (VUS): Genetic alterations listed in the\nAPPENDIX: Variants of Unknown Signiﬁcance section of the report. In\ntotal, the dataset included 10,506 genetic alterations, comprising 5,240\nclinically relevant alterations and 5266 VUS.\nModel selection\nThe models selected for this study included Qwen 2.5 (72B), Llama 3.1\n(70B), and GPT-4o (version 2024-05-13). Qwen 2.5 and Llama 3.1 were\nchosen as they represent state-of-the-art open-source models, widely\nrecognized for their pe rformance and accessibility in the research\ncommunity\n25,26. GPT-4o, representing proprietary models, was included to\nfacilitate a comparison with non-open-source models that excel in diverse\nlanguage tasks27. This selection enables a comprehensive evaluation of open-\nsource and closed-source models, emphasizing their differences in classiﬁ-\ncation accuracy and clinical genomics applicability. Among the models\ntested, GPT-4o’s training data cut-off is in October 2023, while Llama 3.1’s\ndata extends to December 2023. Qwen 2.5 (72B) has not publicly disclosed a\nprecise data cut-off, with only its release date of September 2024 available.\nModel temperature settings followed the default values speciﬁed by\neach LLM API: 0.8 for Llama 3.1 and Qwen 2.5, and 1.0 for GPT-4o.\nAdditionally, to assess the impact of temperature settings, we conducted\nextra tests on Llama 3.1 in the OncoKB dataset with the temperature set to\n0.4 and 0, and compared the results with its default setting.\nSystem prompts\nTo ensure consistency, system prompts were provided to each LLM, as\ndetailed in Supplementary Table 3. The system prompts included deﬁni-\ntions of the levels of evidence for the OncoKB and CIViC systems, along\nwith an additional category,“VUS.” This was added alongside the classiﬁ-\ncation categories from the OncoKB and CIViC levels of evidence to evaluate\nwhether the LLMs could accurately distinguish clinically relevant variants\nfrom VUS.\nFor the CIViC system, the CIViC website provided a variant example for\neach evidence level category (https://civic.readthedocs.io/en/latest/model/\nevidence/level.html, last accessed: 2024/11/20). These examples were incor-\nporated into the system prompts. In contrast, the OncoKB website (https://\nwww.oncokb.org/therapeutic-levels, last accessed: 2024/11/20) does not\nprovide example variants for evidence levels; thus, these were not included in\nthe prompts for OncoKB.\nTo examine the impact ofdifferent system prompt designs, we tested\nmultiple prompt sets. The basic system prompts simply instructed the LLMs\nto provide the corresponding classiﬁcation level number, without additional\ncontext. In contrast, the re ﬁned system prompts included detailed\ninstructions specifying the LLM’s role, objectives, scope, behavior, and\nexpected input/output format. Both versions explicitly instructed the LLMs\nto assign classiﬁc a t i o nl e v e l s( e . g . ,1 ,2 ,3 ,4 ,R 1 ,R 2f o rO n c o K B ;A ,B ,C ,D ,E\nfor CIViC), allowing up to three levels per response. Additionally, for the\nFoundationOne dataset, we tested a binary classiﬁcation prompt in which\nLLMs classiﬁed gene variants as either“Clinically Relevant” or “VUS”,\ninstead of assigning detailed evidence levels.\nTesting framework design\nWe developed a structured testing framework that applies a consistent\nmethod across all datasets. Queries were generated based on the OncoKB,\nCIViC, and FoundationOne datasets, using gene names, alterations, and\ntumor types as contextual inputs. Each query was formulated as a natural\nlanguage prompt, providing sufﬁcient context for the LLMs to classify\ngenetic variants into predeﬁned levels (e.g., A, B, C).\nExample query:“Given the gene EGFR, with alteration L858R in the\ncontext of non-small cell lung cancer, what is the appropriate classiﬁcation?”\nAll queries were automatically generated using customized Python\nscripts to maintain a standardized format. We sent each query to the LLMs\nvia APIs, including GPT-4o (via Azure OpenAI), Qwen 2.5 (72B), and\nLlama 3.1 (70B). The responses were systematically recorded and subse-\nquently analyzed to assess the models’classiﬁcation accuracy.\nTo assess the robustness and variability of LLM responses, we con-\nducted multiple iterations for each experiment— 100 iterations for basic\nsystem prompt experiments and 10 iterations for all others. This approach\nallowed us to evaluate each model’s stability and consistency across different\nquery iterations.\nExecution environment and hardware speciﬁcations\nTo support the batch testing of LLMs, we established a high-performance\nhardware and software environment. Four NVIDIA A100 GPUs were\nutilized to host the Ollama server, enabling high-performance computing\nfor LLM inference tasks related to Qwen 2.5 (72B) and Llama 3.1 (70B). The\nAzure OpenAI GPT-4o API was used to evaluate GPT-4o, providing direct\ncloud-based inference capabilities. Python 3.10.12 was used to implement\nthe testing framework, ensuring compatibility and stability for the batch\nprocessing scripts.\nRetrieval-augmented generation (RAG) implementation\nWe implemented RAG to assess its potential in improving the accuracy and\nperformance of LLMs for cancer genetic variant classiﬁcation. RAG inte-\ngrates retrieval mechanisms with generative AI, allowing LLMs to\nhttps://doi.org/10.1038/s41698-025-00935-4 Article\nnpj Precision Oncology|           (2025) 9:141 8\ndynamically access external knowledgeduring inference rather than relying\nsolely on their pre-trained corpus. This approach enhances the model’s\nability to classify genetic variants by incorporating newly retrieved evidence.\nTo build an institutional knowledgebase, we retrieved datasets from\nthe Clinical Evidence Summary, Features Summary,a n dMolecular Proﬁles\ntables from the CIViC database (https://civicdb.org/releases/main,l a s t\naccessed: 2024/11/20), as well as FDAOncology/Hematologic Malignancies\nApproval information (https://www.fda.gov/drugs/resources-information-\napproved-drugs/oncology-cancerhematologic-malignancies-approval-\nnotiﬁcations, last accessed: 2025/2/13). These datasets were converted into\nhigh-dimensional vector representations using Nomic-Embed-Text\n28,\nallowing for efﬁcient retrieval and contextual augmentation. When an LLM\nprocesses a genetic variant classiﬁcation query, the system retrieves relevant\ninformation from the knowledge base and supplies it as additional context,\nimproving the model’s classiﬁcation accuracy. To evaluate the impact of\nRAG, we compared the performance of Qwen 2.5 (72B) on the Founda-\ntionOne dataset with and without RAG.\nHuman evaluation of cancer genetic variants\nTo compare the classiﬁcation of cancer genetic variants between LLMs and\nhuman experts, we randomly selected 100 variants from the Foundatio-\nnOne dataset. Three pathologists— who regularly sign out molecular reports\nand actively participate in the molecular tumor board— independently\nclassiﬁed each variant as either“Clinically Relevant” or “VUS”.T h ec o n -\ncordance between pathologists, LLMs, and the ground truth annotations in\nthe FoundationOne dataset was then evaluated.\nData analysis and statistics\nThe accuracy of LLMs in classifying genetic variants was assessed by\ncomparing the LLMs’classiﬁcations with the ground truth annotations in\nthese datasets. Top-N accuracy measured how often the correct answer\nappears within a model’s N choices. Top-1 accuracy required the correct\nanswer to be the top choice, while top-2 and top-3 accuracy considered a\nresponse correct if the correct answerappeared within the top two or three\nchoices, respectively. This metric is particularly useful when multiple\nplausible answers exist. Mean accuracy, along with the 95% conﬁdence\nintervals, was calculated. Accuracy comparisons between different LLM\nmodels were analyzed using an ANOVA test, with ap-value of <0.05 con-\nsidered statistically signiﬁcant.\nThe stability of LLM-generated respon s e sa c r o s sm u l t i p l ei t e r a t i o n si s\nassessed using the consistency ratio. This metric quantiﬁes the uniformity of\nresponses to a given query by calculating the proportion of times the most\nfrequently selected answer appears relative to the total number of responses.\nA higher consistency ratio indicates greater stability, while a lower ratio\nsuggests higher variability. For this analysis, only Top-1 answers were used\nto compute the metric.\nData availability\nThe OncoKB and CIViC datasets used for the present study are available\nthrough OncoKB (https://www.oncokb.org/)a n dC I V i C(https://civicdb.\norg/) websites, respectively. The FoundationOne dataset used for the present\nstudy is not publicly available but is available from the corresponding author\non reasonable request.\nCode availability\nThe underlying code used for conducting LLM experiments in this study is\navailable on GitHub and can be accessed via this link. (https://github.com/\ngslin1224/LLMs-CancerVariant).\nReceived: 18 January 2025; Accepted: 2 May 2025;\nReferences\n1. Mosele, M. F. et al. Recommendations for the use of next-generation\nsequencing (NGS) for patients with advanced cancer in 2024: a report\nfrom the ESMO Precision Medicine Working Group.Ann. Oncol.35,\n588–606 (2024).\n2. Chakravarty, D. et al. Somatic genomic testing in patients with\nmetastatic or advanced cancer: ASCO provisional clinical opinion.J.\nClin. Oncol.40, 1231–1258 (2022).\n3. Mosele, F. et al. Recommendations for the use of next-generation\nsequencing (NGS) for patients with metastatic cancers: a report from\nthe ESMO Precision Medicine Working Group.Ann. Oncol.31,\n1491–1505 (2020).\n4. Gibbs, S. N. et al. Comprehensive review on the clinical impact of\nnext-generation sequencing tests for the management of advanced\ncancer. JCO Precis Oncol.7, e2200715 (2023).\n5. Li, M. M. et al. Standards and guidelines for the interpretation and\nreporting of sequence variants in cancer: a joint consensus\nrecommendation of the Association for Molecular Pathology,\nAmerican Society of Clinical Oncology, and College of American\nPathologists. J. Mol. Diagn.19,4 –23 (2017).\n6. van de Haar, J. et al. ESMO Recommendations on clinical reporting of\ngenomic test results for solid cancers.Ann. Oncol.35, 954–967 (2024).\n7. Horak, P. et al. Standards for the classiﬁcation of pathogenicity of\nsomatic variants in cancer (oncogenicity): joint recommendations of\nClinical Genome Resource (ClinGen), Cancer Genomics Consortium\n(CGC), and Variant Interpretation for Cancer Consortium (VICC).\nGenet Med.24, 986–998 (2022).\n8. Chakravarty, D. et al. OncoKB: a precision oncology knowledge base.\nJCO Precis. Oncol. 17, 00011 (2017).\n9. Grif ﬁth, M. et al. CIViC is a community knowledgebase for expert\ncrowdsourcing the clinical interpretation of variants in cancer.Nat.\nGenet. 49, 170–174 (2017).\n10. Mateo, J. et al. A framework to rank genomic alterations as targets for\ncancer precision medicine: the ESMO Scale for Clinical Actionability\nof molecular Targets (ESCAT).Ann. Oncol.29, 1895–1902 (2018).\n11. Lebedeva, A. et al. Multi-institutional evaluation of interrater\nagreement of biomarker-drug pair rankings based on the ESMO scale\nfor clinical actionability of molecular targets (ESCAT) and sources of\ndiscordance. Mol. Diagn. Ther. 29,9 1–101 (2024).\n12. Li, M. M. et al. Assessments of somatic variant classiﬁcation using the\nAssociation for Molecular Pathology/American Society of Clinical\nOncology/College of American Pathologists Guidelines: a report from\nthe Association for Molecular Pathology.J. Mol. Diagn.25,6 9–86 (2023).\n13. Sirohi, D. et al. Multi-institutional evaluation of interrater agreement of\nvariant classiﬁcation based on the 2017 Association for Molecular\nPathology, American Society of Clinical Oncology, and College of\nAmerican Pathologists Standards and guidelines for the interpretation\nand reporting of sequence variants in cancer.J. Mol. Diagn.22\n,\n284–293 (2020).\n14. Pressman, S. M. et al. Clinical and surgical applications of large\nlanguage models: a systematic review.J. Clin. Med. 13, 3041 (2024).\n15. Yu, P. et al. Leveraging generative AI and large language models: a\ncomprehensive roadmap for healthcare integration.Healthcare 11,\n2776 (2023).\n16. Lu, S. & Cosgun, E. Boosting GPT models for genomics analysis:\ngenerating trusted genetic variant annotations and interpretations\nthrough RAG and Fine-tuning.Bioinform. Adv.5, vbaf019 (2025).\n17. De Paoli, F. et al. VarChat: the generative AI assistant for the\ninterpretation of human genomic variations.Bioinformatics 40,\nbtae183 (2024).\n18. Hirotsu, Y. et al. Multigene panel analysis identiﬁed germline\nmutations of DNA repair genes in breast and ovarian cancer.Mol.\nGenet. Genom. Med.3, 459–466 (2015).\n19. Omar, M. et al. Large language models in medicine: a review of current\nclinical trials across healthcare applications.PLOS Digit Health3,\ne0000662 (2024).\n20. Katz, U. et al. GPT versus resident physicians— a benchmark based\non ofﬁcial board scores.NEJM AI.1, AIdbp2300192 (2024).\nhttps://doi.org/10.1038/s41698-025-00935-4 Article\nnpj Precision Oncology|           (2025) 9:141 9\n21. Kung, T. H. et al. Performance of ChatGPT on USMLE: potential for AI-\nassisted medical education using large language models.PLoS Digit\nHealth 2, e0000198 (2023).\n22. Lewis, P. et al. Retrieval-augmented generation for knowledge-\nintensive NLP tasks. NIPS'20: InProc. 34thInternational Conference\non Neural Information Processing Systems9459–9474 (Curran\nAssociates Inc., 2020).\n23. Ankit, Pal M. S. OpenBioLLMs: advancing open-source large language\nmodels for healthcare and life sciences.Hugging Face Repository.\nhttps://huggingface.co/aaditya/OpenBioLLM-Llama3-70B(2024).\n24. Bolton, E. et al. BioMedLM: a 2.7B parameter language model trained\non biomedical Text. arXiv:2403.18421; (2024).\n25. Yang, A. et al. Qwen2 technical report. arXiv preprint\narXiv:2407.10671. (2024).\n26. Dubey, A. et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783. (2024).\n27. Achiam, J. et al. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774. (2023).\n28. Nussbaum, Z., Morris J. X., Duderstadt, B., Mulyar, A. Nomic embed:\ntraining a reproducible long context text embedder. arXiv preprint\narXiv:2402.01613. (2024).\nAcknowledgements\nThis study was funded by National Science and Technology Council, Taiwan\n[grant numbers NSTC 112-2320-B-075-004-MY3] and Taipei Veterans\nGeneral Hospital, Taiwan [grant number V114E-004-3]. We also appreciate\nthe computational resources provided by TVGH Cloud 1. The funder played\nno role in study design, data collection, analysis and interpretation of data, or\nthe writing of this manuscript.\nAuthor contributions\nK.H.L. conceived and planned the project, set up the system, performed the\nexperiments, and was a major contributor in writing the manuscript. T.H.K.\nand L.C.W. conducted the pathologists’evaluation of cancer genetic\nvariants. C.T.K. contributed to the system setup. C.H.C. contributed to the\nstudy design and data collection. Y.C.C. contributed to the study design,\nsystem setup, and funding acquisition. Y.C.Y. conceived and planned the\nproject, acquired funding, performed data analysis, conducted pathologists’\nevaluation of cancer genetic variants, and was a major contributor to writing\nthe manuscript. All authors read and approved theﬁnal manuscript.\nCompeting interests\nThe authors declare no competing interests.\nEthics approval and consent to participate\nThis study was approved by the Taipei Veterans General Hospital\nInstitutional Review Board (2024-12-010BC), which waived the requirement\nfor informed consent. The study was conducted in accordance with the\nprinciples of the Declaration of Helsinki.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41698-025-00935-4\n.\nCorrespondenceand requests for materials should be addressed to\nYuan-Chia Chu or Yi-Chen Yeh.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution-NonCommercial-NoDerivatives 4.0 International License,\nwhich permits any non-commercial use, sharing, distribution and\nreproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the Creative\nCommons licence, and indicate if you modiﬁed the licensed material. You\ndo not have permission under this licence to share adapted material\nderived from this article or parts of it. The images or other third party\nmaterial in this article are included in the article’s Creative Commons\nlicence, unless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted use,\nyou will need to obtain permission directly from the copyright holder. To\nview a copy of this licence, visithttp://creativecommons.org/licenses/by-\nnc-nd/4.0/\n.\n© The Author(s) 2025\nhttps://doi.org/10.1038/s41698-025-00935-4 Article\nnpj Precision Oncology|           (2025) 9:141 10",
  "topic": "Benchmarking",
  "concepts": [
    {
      "name": "Benchmarking",
      "score": 0.8751003742218018
    },
    {
      "name": "Cancer",
      "score": 0.46759292483329773
    },
    {
      "name": "Computer science",
      "score": 0.4086074233055115
    },
    {
      "name": "Oncology",
      "score": 0.3562782406806946
    },
    {
      "name": "Computational biology",
      "score": 0.3536633849143982
    },
    {
      "name": "Internal medicine",
      "score": 0.34837597608566284
    },
    {
      "name": "Medicine",
      "score": 0.3379802703857422
    },
    {
      "name": "Biology",
      "score": 0.31683290004730225
    },
    {
      "name": "Business",
      "score": 0.13758382201194763
    },
    {
      "name": "Marketing",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I191969501",
      "name": "National Taipei University of Nursing and Health Science",
      "country": "TW"
    },
    {
      "id": "https://openalex.org/I2803004286",
      "name": "Taipei Veterans General Hospital",
      "country": "TW"
    },
    {
      "id": "https://openalex.org/I148366613",
      "name": "National Yang Ming Chiao Tung University",
      "country": "TW"
    }
  ],
  "cited_by": 5
}