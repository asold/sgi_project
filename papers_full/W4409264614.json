{
    "title": "Can large language models facilitate the effective implementation of nursing processes in clinical settings?",
    "url": "https://openalex.org/W4409264614",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2137618305",
            "name": "Yuqin Cao",
            "affiliations": [
                "First Affiliated Hospital of Chongqing Medical University",
                "Chongqing Medical University"
            ]
        },
        {
            "id": "https://openalex.org/A2039112420",
            "name": "Li Hu",
            "affiliations": [
                "Chongqing Medical University",
                "First Affiliated Hospital of Chongqing Medical University"
            ]
        },
        {
            "id": "https://openalex.org/A2102363819",
            "name": "Xu Cao",
            "affiliations": [
                "Middle East Institute"
            ]
        },
        {
            "id": "https://openalex.org/A2116369039",
            "name": "Jing-Jing Peng",
            "affiliations": [
                "First Affiliated Hospital of Chongqing Medical University",
                "Chongqing Medical University"
            ]
        },
        {
            "id": "https://openalex.org/A2137618305",
            "name": "Yuqin Cao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2039112420",
            "name": "Li Hu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2102363819",
            "name": "Xu Cao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2116369039",
            "name": "Jing-Jing Peng",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4394675809",
        "https://openalex.org/W4385172997",
        "https://openalex.org/W4323347432",
        "https://openalex.org/W4394715883",
        "https://openalex.org/W2901972951",
        "https://openalex.org/W4390705612",
        "https://openalex.org/W4385564356",
        "https://openalex.org/W4404017948",
        "https://openalex.org/W4397002060",
        "https://openalex.org/W4388942469",
        "https://openalex.org/W4394579747",
        "https://openalex.org/W2994606793",
        "https://openalex.org/W1550590706",
        "https://openalex.org/W638742776",
        "https://openalex.org/W2159313505",
        "https://openalex.org/W4386019759",
        "https://openalex.org/W2895645277",
        "https://openalex.org/W3185341429",
        "https://openalex.org/W4388129991",
        "https://openalex.org/W6838865847",
        "https://openalex.org/W3034963685",
        "https://openalex.org/W4392755271",
        "https://openalex.org/W4367174347",
        "https://openalex.org/W4401897279",
        "https://openalex.org/W4403247635",
        "https://openalex.org/W4383058425",
        "https://openalex.org/W2041937621",
        "https://openalex.org/W2945703515",
        "https://openalex.org/W4221002518",
        "https://openalex.org/W4225552392",
        "https://openalex.org/W4400348106"
    ],
    "abstract": "Our research further confirms the potential of LLMs in clinical nursing practice.However, significant challenges remain in the effective integration of LLM-assisted nursing processes into clinical environments.",
    "full_text": "RESEARCH Open Access\n© The Author(s) 2025. Open Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 \nInternational License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you \ngive appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the \nlicensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p  : / /  c r e a  t i  \nv e c  o m m  o n s .  o r  g / l  i c e  n s e s  / b  y - n c - n d / 4 . 0 /.\nCao et al. BMC Nursing          (2025) 24:394 \nhttps://doi.org/10.1186/s12912-025-03010-2\nIntroduction\nLarge Language Models (LLMs), based on the Trans -\nformer neural network architecture, are deep learn -\ning models with powerful natural language processing \ncapabilities. One of their most attractive features is their \nability to engage in human-like conversations, answer \nquestions across various professional domains, com -\nplete translation tasks, and generate programming code, \namong other complex language tasks [ 1, 2]. In recent \nyears, the application of LLMs in nursing research, edu -\ncation, and practice has grown exponentially [ 3–5]. \nIn clinical nursing practice, one of the most promis -\ning applications of LLMs is their capacity to generate \nBMC Nursing\n†Yuqin Cao and Li Hu are co-first authors.\n*Correspondence:\nJingjing Peng\n202599@hospital.cqmu.edu.cn\n1Nursing Department, Department of Neurosurgery, The First Affiliated \nHospital of Chongqing Medical University, Chongqing  \n400016, People’s Republic of China\n2Chong Qing Wondertek Software Corporation, Room 2, 10th Floor, \nBuilding C, Qilin, Huangshan Avenue Middle Section, Yubei District, \nChongqing 401121, People’s Republic of China\nAbstract\nBackground The quality of generative nursing diagnoses and plans reported in existing research remains a topic of \ndebate, and previous studies have primarily utilized ChatGPT as the sole large language mode.\nPurpose To explore the quality of nursing diagnoses and plans generated by a prompt framework across different \nlarge language models (LLMs) and assess the potential applicability of LLMs in clinical settings.\nMethods We designed a structured nursing assessment template and iteratively developed a prompt framework \nincorporating various prompting techniques. We then evaluated the quality of nursing diagnoses and care plans \ngenerated by this framework across two distinct LLMs(ERNIE Bot 4.0 and Moonshot AI), while also assessing their \nclinical utility.\nResults The scope and nature of the nursing diagnoses generated by ERNIE Bot 4.0 and Moonshot AI were similar \nto the “gold standard” nursing diagnoses and care plans.The structured assessment template effectively and \ncomprehensively captures the key characteristics of neurosurgical patients, while the strategic use of prompting \ntechniques has enhanced the generalization capabilities of the LLMs.\nConclusion Our research further confirms the potential of LLMs in clinical nursing practice.However, significant \nchallenges remain in the effective integration of LLM-assisted nursing processes into clinical environments.\nKeywords Large Language models, Nursing diagnosis, Nursing care plan, Nursing processes, Prompt\nCan large language models facilitate the \neffective implementation of nursing processes \nin clinical settings?\nYuqin Cao1†, Li Hu1†, Xu Cao2 and Jingjing Peng1*\nPage 2 of 11\nCao et al. BMC Nursing          (2025) 24:394 \npersonalized nursing diagnoses and care plans [ 6]. These \ntwo components are fundamental to the nursing process \n[7]. However, despite more than half a century since the \nconcept of the nursing process was introduced, its imple-\nmentation in clinical settings remains suboptimal. Nurs -\ning care is often based on routines and medical orders, \nwith unplanned care being prevalent [ 8]. The primary \nreasons for unplanned care are a lack of knowledge and \nskills in developing care plans, as well as nurse staff -\ning shortages [ 8]. LLMs may offer a novel approach to \naddress these challenges. Recent studies have explored \nthe use of ChatGPT for generating nursing diagnoses \nand care plans for various patient populations, includ -\ning obese patients, mental health care, perinatal care, \nand lung cancer patients [ 9–12]. While these stud -\nies acknowledge the potential value of LLMs in nursing \napplications, concerns about the accuracy and reliability \nof LLM-generated nursing texts remain [ 13]. The quality \nof generated content is partly influenced by the quality of \nthe prompts, which are the input texts or questions pro -\nvided when interacting with LLMs. Designing effective \nprompts is crucial for obtaining desired outcomes [ 14]. \nPrevious research has highlighted the value of structured \nprompts in enhancing the performance and generaliza -\ntion capabilities of LLMs [ 15]. Notably, all LLMs applied \nin related studies to date have been ChatGPT, and there \nis a lack of research on the use of other LLMs in nursing. \nTherefore, we selected two additional LLMs—ERNIE Bot \n4.0 and Moonshot AI—constructed a prompt framework, \nand designed specific questioning strategies. We applied \nthese to clinical cases of neurosurgical discharge patients \nto explore the potential of different LLMs in generating \nnursing diagnoses and care plans. This study aims to pro -\nvide a reference for the integration of LLM-assisted nurs-\ning processes in clinical settings.\nNorth American Nursing Diagnosis Association Inter -\nnational (NANDA-I), Nursing Interventions Classifica -\ntion (NIC), and Nursing Outcomes Classification (NOC) \nare collectively known as the NNN linkages, which cur -\nrently represent the most widely utilized standardized \nnursing terminology systems in the international nurs -\ning field [16]. NANDA-I provides a comprehensive set of \nnursing diagnostic terms that assist healthcare providers \nin identifying patients’ nursing problems [ 17]; NIC offers \na validated series of nursing interventions [ 18]; and NOC \nfocuses on the establishment of expected outcomes and \nthe evaluation of nursing effectiveness [ 19]. This system \nsignificantly supports the nursing process by providing \ntheoretical guidance and an operational framework for \nthe stages of nursing diagnosis, care planning, and out -\ncome evaluation. In the late 1990s, the nursing commu -\nnity in China began to gradually engage with, learn about, \nand explore the application of NNN linkages within the \nChinese context. Over the years, NNN linkages have \nbeen adopted by numerous nursing education institu -\ntions and large comprehensive hospitals in China, with \ntheir use gradually expanding. According to reports, \nin nursing education, the integration of NNN linkages \ninto the Chinese undergraduate nursing curriculum in \nthe course 《Fundamentals of Nursing 》 has facilitated \nthe development of students’ critical thinking skills [ 20]. \nIn specialized nursing fields, NNN linkages have been \napplied in clinical pathways for breastfeeding, home vis -\nits for hypertensive patients, cardiac rehabilitation nurs -\ning, and nursing practices for various patient populations \nand care scenarios [ 21–23]. As research progresses, the \napplication of NNN linkages in China has expanded from \ninitial uses in medical record documentation to more \nrecent applications in medical insurance reimbursement, \nnursing information exchange, and care resource alloca -\ntion. These applications demonstrate the significant role \nof NNN linkages in continuous documentation, clini -\ncal decision-making, and consistent evaluation. How -\never, the applicability of NNN linkages in China remains \nconstrained by language and regional factors, which do \nnot fully align with the national context. Therefore, this \nstudy includes neurosurgery clinical nursing experts and \nutilizes the Chinese translation of the NNN linkages to \nevaluate the quality of nursing diagnoses and care plans \ngenerated by different LLMs.\nMethods\nThe selection of LLMs\nThe two LLMs selected for this study are ERNIE Bot \n4.0 and Moonshot AI, with their most recent updates in \nSeptember 2024 and October 2024, respectively. These \nmodels were chosen based on their performance in the \nChinese local market and their specific technical advan -\ntages. ERNIE Bot 4.0, developed by Baidu, integrates \nadvanced technologies such as multimodal learning and \nreinforcement learning, making it particularly well-suited \nfor natural language processing tasks in the Chinese \ncontext [24]. Currently, ERNIE Bot 4.0 is widely applied \nacross various service platforms by Baidu, including \nintelligent customer service, search engines, and medi -\ncal information processing. Moonshot AI, developed by \na Chinese startup, aims to optimize its understanding \nof Chinese text through deep learning models and has \nshown strong adaptability and potential in the healthcare \nsector [ 25]. Additionally, GPT-4 and Bard are globally \nrecognized LLMs with significant influence in multilin -\ngual processing and cross-cultural adaptability. How -\never, compared to GPT-4 and Bard, ERNIE Bot 4.0 and \nMoonshot AI offer clear advantages in understanding \nand generating Chinese text. GPT-4 and Bard are pri -\nmarily trained on English-language corpora, and their \nprocessing of Chinese text may be influenced by differ -\nences in linguistic structure, particularly in grammar \nPage 3 of 11\nCao et al. BMC Nursing          (2025) 24:394 \nand context. In contrast, ERNIE Bot 4.0 and Moonshot \nAI have been specifically optimized for Chinese natural \nlanguage processing, enabling them to better handle the \nambiguity, grammatical complexity, and culturally/con -\ntextual nuances inherent in the Chinese language. The \ngoal of this study is to explore the performance of LLMs \nin specific nursing tasks, taking into account the unique \nlocalization needs of the nursing industry. Given the \nrestrictions on GPT-4 in mainland China, we selected \nERNIE Bot 4.0 and Moonshot AI, two Chinese-based \nLLMs, for this research.\nInclusion criteria for nursing experts\nThe scoring criteria used for the selection of experts and \nthe corresponding score assigned were as follows: nursing \nmaster’s degree, 4 points; nursing master’s degree with \ndissertation directed to relevant content of the nursing \ndiagnosis in this study, 1 point; publication of an article \nabout nursing diagnosis in reference journals, 2 points; \narticle published on nursing diagnosis and with con -\ntent relevant to the area, 2 points; doctorate in nursing \ndiagnosis, 2 points; clinical experience of at least 1 year \nin the area of the study, 1 point; and specialized experi -\nence of clinical practice relevant in the area of diagnosis, \n2 points. Experts who accumulated a total score of ≥ 5 \npoints from the above criteria were considered eligible as \nnursing experts [26].\nConstruction of structured nursing assessment template \nand prompt framework\nOur research methodology consists of two main com -\nponents: designing a structured nursing assessment \ntemplate and constructing an iterative prompt frame -\nwork. First, we designed a structured nursing assessment \ntemplate based on the Chinese editions Xinbian Hulixue \nJichu(4th Edition) and Waike Hulixue(7th Edition), with \na specific focus on neurosurgical diseases [ 27, 28]. The \ntemplate also incorporates discharge cases from five \ncommon neurosurgical conditions: craniocerebral injury, \naneurysmal subarachnoid hemorrhage, pituitary tumors, \nvestibular schwannomas, and facial spasm. This template \nis intended to be universally applicable in neurosurgical \nclinical settings. It includes six categories: general infor -\nmation, chief complaint, current diagnosis, past health \nhistory, current health status, and current psychological \nstatus. Each category is designed to ensure a comprehen -\nsive and objective assessment of the patient’s condition \nand needs (see Table  1). Second, we developed an itera -\ntive prompt framework to generate nursing diagnoses by \ngradually integrating structured nursing assessment con -\ntent. This process involved iterative adjustments to role-\nplaying, examples, output constraints, and thought-chain \nguidance [ 29]. The framework consists of four compo -\nnents: instructions, examples, structured assessment \ncontent, and output requirements (see Fig. 1 and Annex). \nFinally, following the framework of instructions, exam -\nples, and output requirements, we prompted the LLMs to \ngenerate corresponding expected outcomes and nursing \ninterventions (see Fig. 2 and Annex). The refinement pro-\ncess is detailed as follows:\nDesign of the structured assessment template (Iterations 1–3)\nBased on theoretical knowledge and five discharge cases, \nthree clinical nursing experts in neurosurgery reviewed \nand unanimously approved the categories and specific \nitems of the structured nursing assessment template. At \nthis stage, the content of the template was designed to \nalign with patient needs and disease characteristics. To \nTable 1 Structured nursing assessment template for neurosurgical disease patients\nAssessment category Explanation\n1. General Information Gender, age, caregivers, medical insurance status.\n2. Chief Complaint The primary health issue or discomfort symptoms described by the patient during consultation.\n3. Current Diagnosis Recent medical diagnosis.\n4. Past Health History Medical history, allergy history, infectious disease history, family history, etc.\n5. Current Health Status Includes the following four aspects:\n (1) Current Treatment Surgical treatment, pharmacological treatment, other treatments (e.g., physical therapy, radio-\ntherapy, chemotherapy, etc.).\n (2) Physical Status Assessment Consciousness, pupil reaction, neurological function assessment (language, movement, sensa-\ntion, cough ability, swallowing function, vision, visual fields, epilepsy, delirium, etc.), positive \nsigns (temperature, pulse, respiration, blood pressure, oxygen saturation, etc.), skin, tubes, diet, \nbowel and bladder function, sleep, activity, and symptoms from other systems, etc.\n (3) Positive Laboratory and Diagnostic Results Laboratory test results, other diagnostic findings.\n (4) Relevant Scale Scores Morse Fall Risk Score, Braden Pressure Injury Risk Score, Barthel Index for Activities of Daily Liv-\ning, Caprini Thrombosis Risk Score, Numerical Rating Scale for Pain, etc.\n6. Current Psychological Status Psychological manifestations and characteristics of the patient and family members when cop-\ning with the disease burden.\nNote: “Explanation” refers to the refinement and restriction of the “Assessment Category” based on the characteristics of neurosurgical diseases. Its purpose is to \nprompt the evaluator to conduct a detailed assessment according to the refined items. Among them, the past and current health conditions only record abnormal \nitems\nPage 4 of 11\nCao et al. BMC Nursing          (2025) 24:394 \nFig. 2 Flowchart of the Prompt Framework and Questioning Strategy. Note: The “First Input Prompt” shown in the figure is only a partial display.For de-\ntailed content, please refer to the Annex 1\n \nFig. 1 Prompt Framework and Tips for Generating Nursing Diagnoses. Note: The “First Input Prompt (glioma)” shown in the figure is only a partial display. \nFor detailed content, please refer to the Annex 1\n \nPage 5 of 11\nCao et al. BMC Nursing          (2025) 24:394 \nensure universality and operability, brief descriptions of \neach assessment category were provided, and limitations \non the scope of the assessment items were established.\nConstruction of the prompt framework (Iterations 4–8)\nBuilding upon the template, the three nursing experts \ndeveloped structured assessment content for the five dis -\ncharge cases. In collaboration with a computer expert, \nprompt technology was applied to construct a prelimi -\nnary prompt framework based on the structured assess -\nment content for each case. The prompts for each case \nwere repeatedly input into ERNIE Bot 4.0, and the quality \nof each response was recorded and analyzed. To evalu -\nate the quality of the generated responses, gold standards \nfor nursing diagnoses were established for each case, and \nthe outputs from ERNIE Bot 4.0 were compared against \nthese standards to optimize the prompt framework. The \naim of these iterative cycles was to refine the question -\ning logic, clarify prompting techniques, and stabilize the \nprompt framework.\nFollow-up questioning and fine-tuning (Iterations 9–12)\nOnce the scope and nature of the generated nursing \ndiagnoses achieved relative stability, follow-up questions \nwere progressively asked to ERNIE Bot 4.0 to generate \ncorresponding expected outcomes and nursing interven -\ntions. Gold standards for expected outcomes and nurs -\ning interventions were established for each case to assess \nthe quality of the generated nursing plans. Additionally, \nMoonshot AI, another large language model, was used \nto repeatedly generate nursing diagnoses and plans for \ncomparison. During this phase, detailed analyses of the \noutputs from both LLMs were conducted, with repeated \nadjustments to the prompt framework’s language \ndescriptions to assess its stability.\nWe have outlined the methodology for developing a \nprompt framework, based on the “Structured Thinking \nPrompt Framework. ” This framework involves extracting \nstructured case information and integrating role-playing, \ncontextual learning, and thought-chain prompting tech -\nniques to guide LLMs in step-by-step reasoning. The \ngoal is to improve the generalization ability of LLMs in \ngenerating high-quality nursing recommendations. NNN \nlinkages are primarily integrated into the prompt frame -\nwork as constraints for content generation. The LLM \nis instructed to generate nursing diagnoses based on \nNANDA-I, expected outcomes based on the NOC clas -\nsification, and nursing interventions based on the NIC \nclassification.In this study, we demonstrate the applica -\ntion of this framework in generating nursing plan outputs \nfor glioma patients using ERNIE Bot 4.0 and Moonshot \nAI (see Annex 1).\nEvaluation of LLMs-Generated nursing diagnoses and care \nplans\nBefore the study began, three human neurosurgery nurs -\ning experts who met the inclusion criteria underwent \nstandardized and homogeneous training. The training \ncovered the interpretation and application of the NNN \nlinkages standards [ 17–19], the application methods of \nthe Fehring model [ 30], and the operational procedures \nfor using LLMs. Subsequently, the three nursing experts \nheld detailed discussions to establish the ‘gold standard’ \nfor nursing diagnoses and care plans in the case of brain \nglioma. The LLMs-generated nursing diagnoses and care \nplans were then compared with the ‘gold standard’ to \nevaluate the quality of the generated nursing diagnoses \nand care plans. The evaluation focused on the following \naspects: 1) the scope and nature of the generated con -\ntent, 2) the prioritization of generated nursing diagnoses, \nand 3) the accuracy of nursing terminology descriptions \nin the generated content. The evaluation procedure was \nas follows: 1) The three nursing experts first reviewed \nwhether the LLMs-generated nursing diagnoses and \ncare plan entries conformed to the scope of the NNN \nlinkages [ 16]. They then applied the Fehring model [ 30] \nto evaluate the content validity of the generated nursing \ndiagnoses and care plans. For any controversial entries, \nan evidence-based literature review approach was used \nto discuss and reach a consensus. 2) The priority of the \nLLMs-generated nursing diagnoses was compared to that \nof the ‘gold standard’ nursing diagnoses. 3) The descrip -\ntions of the LLMs-generated nursing diagnoses and care \nplan entries were compared with the NNN linkages. \nGiven the differences in cognitive patterns, culture, and \nlanguage, the Chinese translations of NANDA-I Nursing \nDiagnoses: Definitions and Classification (2021–2023) by \nLi Xiaomei et al. [ 31] and Nursing Diagnosis, Outcomes, \nand Interventions (2nd Edition) by Wu Yuanjian et al. \n[32] were used as reference sources to compare the accu -\nracy of the terminology descriptions.\nResults\nThe scope and nature of the nursing diagnoses generated \nby ERNIE Bot 4.0 and Moonshot AI were similar to the \n“gold standard” nursing diagnoses (see Fig. 3 and Annex). \nThe “gold standard” nursing diagnoses included 11 items \n(based on NANDA-I), while ERNIE Bot 4.0 generated 10 \nitems, with 7 being an exact match, 3 being similar, and \n1 omitted. Moonshot AI also generated 10 items, with 6 \nbeing an exact match, 2 being similar, 1 omitted, and 2 \nincorrect. Both models omitted “Potential Complication: \nEpilepsy. ” The incorrect items generated by Moonshot AI \nwere “ Abnormal Urination” and “Knowledge Deficiency, ” \nwhich were deemed incorrect because no supporting evi -\ndence for these diagnoses was found in the structured \nassessment content provided as input.\nPage 6 of 11\nCao et al. BMC Nursing          (2025) 24:394 \nThe items that were an exact match or similar cor -\nresponded one-to-one with the structured assessment \ncontent, and their accuracy and practicality were satis -\nfactory, providing valuable references for clinical nurs -\ning. In terms of nursing priority ranking, both LLMs \nemphasized the importance of addressing “ Acute Pain” \nand “Potential Complication: Cerebral Hernia. ” ERNIE \nBot 4.0 also highlighted “Ineffective Airway Clearance, ” \nand Moonshot AI emphasized “Potential Complication: \nLung Infection, ” both placing them in the high-priority \ncategory for airway management, which aligns with the \npatient’s primary needs. The intermediate and low pri -\norities were ranked according to the patient’s existing \nproblems and potential risks, generally corresponding \nto patient needs and clinical scenarios (see Fig.  3 and \nAnnex).\nThe expected outcomes and nursing interventions gen -\nerated by ERNIE Bot 4.0 were closely aligned with the \nnursing diagnoses, and their scope and nature were con -\nsistent with the gold standard. In contrast, the expected \noutcomes and nursing interventions generated by Moon -\nshot AI were not listed in order of nursing diagnosis pri -\nority, but their scope and nature also closely matched the \ngold standard (see Fig. 4 and Annex). Regarding terminol-\nogy, the Chinese descriptions of nursing diagnoses and \nexpected outcomes generated by both LLMs were highly \nsimilar to the gold standard (based on NANDA-I, NOC, \nNIC). However, the accuracy of the nursing interven -\ntion descriptions was lower. NNN linkages clearly define \nthe classification of nursing interventions, emphasizing \nstandardization and consistency. Each nursing interven -\ntion is precisely defined with a standardized expression, \nensuring consistency and comparability across nursing \ndiagnoses and care plans. In contrast, the nursing inter -\nventions generated by the LLMs are expressed more \nfreely and do not strictly follow the standardized format \nof the terminology system. Furthermore, the LLMs-\ngenerated interventions tend to be more generalized \nand lack sufficient actionable details. A notable feature \nof both LLMs was their step-by-step reasoning and gen -\neration process based on the prompt requirements, with \nexplanations and necessary reminders for each response. \nThese features further underscore the potential value of \neffectively applying LLM-assisted nursing procedures in \nclinical settings.\nDiscussion\nIn this study, a prompt framework was developed by \nconstructing structured nursing assessments and apply -\ning prompting engineering techniques. By simply replac -\ning the relevant information in the structured nursing \nassessment, the framework was made adaptable to vari -\nous neurosurgical disease cases. The structured nursing \nassessments were performed manually, while the nurs -\ning diagnosis, expected outcomes, and nursing interven -\ntions were generated by the large language model based \non the structured information. This approach effectively \ncombined the strengths of both human expertise and \nmachine-generated insights. The nursing plans created \nusing this framework demonstrated strong performance \nFig. 3 Comparison between generative nursing diagnoses from two large language models and the gold standard. Note: All nursing diagnoses pre -\nsented are partial; for detailed content, please refer to Annex 2, 3, and 4\n \nPage 7 of 11\nCao et al. BMC Nursing          (2025) 24:394 \nin terms of content relevance, completeness, prioritiza -\ntion, and accuracy of descriptions, showing a notable \nlevel of credibility and applicability. Therefore, under the \nsupervision and review of clinical nursing experts, LLMs \nhave the potential to effectively support the application of \nnursing procedures in clinical practice.\nPrevious studies have highlighted significant contro -\nversy regarding the quality of nursing plans generated \nby LLMs. In earlier research by Gosak [ 9] and Woodnutt \n[10], the accuracy of generative nursing diagnoses and \nplans was relatively low. However, more recent work by \nDos Santos [ 12] indicated that generative nursing plans \nclosely approximated the “gold standard” nursing plans \ndeveloped by humans. All of these studies utilized Chat -\nGPT, with prompts consisting of narrative text. The \ncontroversy may stem from structural optimization and \niterative refinement of the prompts in Dos Santos [ 12], \nalong with the delegation of the nursing assessment step \nto human evaluators. Our study adopts a design approach \nsimilar to that of Dos Santos [ 12], yielding comparable \nresults. The key difference lies in the framework con -\nstruction: while Dos Santos [ 12] developed their prompt \nframework based on the Theory of Human Needs and \nthe Situation-Background-Assessment-Recommendation \nmodel, we have designed our framework using “Struc -\ntured Thinking Prompt Framework, ” which integrates \ndisease knowledge, clinical practice, and prompting tech-\nniques [29, 33]. Furthermore, our framework was applied \nto the outputs of LLMs from different companies (ERNIE \nBot 4.0 and Moonshot AI). The nursing plans generated \nby both LLMs showed high similarity to the “gold stan -\ndard” in terms of both scope and nature. This outcome \nsuggests that our prompt framework enhances the gener-\nalization ability of LLMs in generating high-quality nurs -\ning plans. Potential reasons for this include the effective \napplication of prompting strategies and techniques, par -\nticularly role-playing, exemplars, and chain-of-thought \nprompting, which improve the explainability and cred -\nibility of the generated results [ 34, 35]. These techniques, \ngrounded in human cognitive theory, enable LLMs to \nadopt human-like logical reasoning, thereby enhancing \ntheir performance [ 36]. Additionally, and importantly, \nour structured nursing assessment template comprehen -\nsively presents the background information, current sta -\ntus, and needs of neurosurgical patients. This template, \nsimilar to a neurosurgical nursing assessment checklist, \nincludes objective and easily obtainable assessment items \ntailored to the characteristics of neurosurgical patients. \nThe assessment process closely aligns with clinical \nnursing practices in collecting and organizing relevant \ninformation, a practice consistently followed in clinical \nsettings, thus demonstrating strong clinical applicability.\nIt is important to note that for this study, we deliber -\nately selected cases of patients who had recovered and \nbeen discharged, excluding disease-related information \nfrom hospitalized patients. Additionally, the generated \nFig. 4 Comparison between generative expected outcomes from two large language models and the gold standard. Note: The expected outcomes \npresented are partial; detailed content can be found in Annex 2, 3, and 4\n \nPage 8 of 11\nCao et al. BMC Nursing          (2025) 24:394 \nnursing care plans were not applied to clinical decision-\nmaking for similar hospitalized patients (with analogous \ndiagnoses) during the study period. Throughout the \nresearch process, we intentionally omitted sensitive, con -\nfidential, and identifiable information, such as patients’ \nnames and addresses [ 5]. This approach was guided by \nlegal, safety, and ethical considerations. It must also be \nacknowledged that the field of Large Language Models is \nrapidly evolving, with new solutions being developed on \na daily basis. As a result, ensuring the reproducibility and \nconsistency of generated outcomes remains challeng -\ning. Furthermore, the nursing diagnoses and care plans \ngenerated in this study showed some deviation from the \nestablished “gold standard. ” LLMs rely on complex algo -\nrithms and build knowledge structures through training \non large datasets to perform their functions [ 2], which \nmay explain why the Moonshot AI model produced \nbiased nursing diagnoses, such as “ Abnormal Urination” \nassociated with urinary catheters, as well as a diagnosis \nof “Knowledge Deficiency” that lacked sufficient sup -\nporting evidence in this study. Additionally, both LLMs \nfailed to identify “Potential Complication: Epilepsy, ” even \nafter multiple iterations of prompt input. This outcome \nmay be attributed to the absence of relevant descriptions \nin the structured assessment content or could be related \nto the specific training datasets used for the two LLMs—\nERNIE Bot 4.0 and Moonshot AI. Therefore, before \napplying generative nursing diagnoses and care plans to \nclinical practice, it is essential to give full attention to \nand carefully consider the ethical issues that may arise \nfrom incorrect or inaccurate generated results. Firstly, it \nis crucial to promptly and effectively identify incorrect \nor inaccurate outputs and prevent their implementation \nin clinical practice. Establishing a manual review mecha -\nnism is currently the most effective way to address this \nissue. Each nursing diagnosis and care plan generated by \nan LLMs must undergo review by clinical nurses or nurs -\ning experts to ensure it meets actual nursing needs and \nthe specific conditions of the patient before it is applied \nin clinical care practice. Secondly, in terms of improving \nLLMs performance, establishing a continuous optimiza -\ntion feedback mechanism, and maintaining LLMs trans -\nparency and traceability are also important measures to \nensure patient safety. By systematically collecting feed -\nback, particularly regarding errors or incorrect gener -\nated results, developers can further optimize the LLMs \nto enhance its accuracy and adaptability. By ensuring \nLLMs transparency and traceability—clearly present -\ning the generation process of each nursing diagnosis and \ncare plan (including the underlying clinical knowledge \nand algorithmic principles)—nurses can clearly under -\nstand the basis for the LLMs-generated outputs, allowing \nthem to make informed judgments and adjustments in \nuncertain situations. Therefore, it is essential to critically \nevaluate the recommendations generated by LLMs [ 37]. \nNurses must leverage their clinical expertise and expe -\nrience to assess the potential adverse effects of LLM \nsuggestions on patient needs, safety, and ethical consid -\nerations [37]. It is imperative that nurses understand that \nLLMs are tools, not replacements for the core functions \nof nursing practice [ 5]. Moreover, nurses must acknowl -\nedge that their professional knowledge, communication \nskills, and empathy are irreplaceable [ 38]. Furthermore, \nalthough LLMs have shown promise in generating nurs -\ning care plans, their effectiveness in assisting nursing \nprocedures is heavily influenced by individual nurses’ \nawareness, attitudes, learning capabilities, and profi -\nciency in utilizing LLMs [ 39, 40]. Despite claims in the \nexisting literature that LLMs can alleviate the burden of \nnursing documentation, direct evidence of their practical \napplication in clinical settings remains limited. The ques -\ntion of whether LLMs can genuinely save time or merely \nserve as a gimmick that redistributes tasks without effec -\ntively reducing time spent on activities, along with iden -\ntifying the specific contexts in which they can be most \neffectively prioritized, requires further investigation and \ndiscussion [41]. It can be inferred that the successful inte-\ngration of LLM-assisted nursing programs into clinical \npractice still faces significant challenges and represents \na long-term developmental process. The prompt frame -\nwork we developed and its application in neurosurgical \ndischarge cases further highlights the potential of LLMs \nto enhance nursing practice.\nNNN linkages are one of the most widely applied stan -\ndardized terminologies in the nursing field. In this study, \nby incorporating NNN linkages as a key constraint in the \noutput of LLMs, we facilitate more accurate understand -\ning and generation of nursing terminology and content \nthat meet the professional requirements of nursing. This \nenhances the professionalism and accuracy of the nurs -\ning diagnoses and care plans generated by LLMs. Beyond \nNNN linkages, other nursing terminology systems, such \nas the Omaha System and the International Classification \nfor Nursing Practice (ICNP), also hold similar potential \nto support the development and application of large lan -\nguage models [ 42]. Modifying the prompt framework \nto incorporate the Omaha System or ICNP can simi -\nlarly generate corresponding nursing diagnoses and care \nplans. Through continuous optimization and feedback \nmechanisms, this process enables LLMs to learn and \nadapt to a broader range of specialized nursing terminol -\nogies, thereby improving their practical value in clinical \nnursing practice.\nNursing shortages are one of the major challenges fac -\ning healthcare systems worldwide. The high-pressure \nwork environment and workload-induced fatigue are \ntwo key factors contributing to nursing shortages [ 43]. \nThe rational and effective application of LLMs may help \nPage 9 of 11\nCao et al. BMC Nursing          (2025) 24:394 \nimprove the work environment and reduce the documen-\ntation burden on nurses. On one hand, this study demon-\nstrated that the process of generating nursing diagnoses \nand care plans using prompt frameworks and LLMs is \nextremely time-efficient, enabling nurses to quickly \nobtain accurate and personalized nursing diagnoses and \ninterventions. This helps accelerate the decision-making \nprocess, improving the response time and quality of nurs-\ning services. On the other hand, the automation of nurs -\ning diagnosis and care plan generation can reduce the \ntime nurses spend on documentation, freeing up more \ntime for direct clinical care of patients. It is important \nto note, however, that the effective application of LLMs \nin the nursing field still requires some training resources \nand time, especially in regions with weaker technological \ninfrastructure. In such areas, nurses’ acceptance of tech -\nnology and training levels may be lower, which could limit \nthe effectiveness of its implementation. However, aside \nfrom the need for manual completion of structured nurs-\ning assessments, the questioning logic and prompt word \ndescriptions within the prompt framework are relatively \nfixed, making them easily applicable and input into LLMs \nwithout significantly increasing nurses’ workload. This \nis beneficial in addressing the global shortage of nursing \npersonnel. The compatibility between healthcare infor -\nmation systems and LLMs is fundamental to realizing the \nautomation and intelligence of nursing documentation. \nDifferent countries and regions have distinct healthcare \nsystems, and factors such as resource disparities and \nvariations in nursing practices can limit the further appli-\ncation and promotion of LLMs in healthcare. The prompt \nframework developed in this study is straightforward \nto use; as long as LLMs are available in a given country \nor region, this method can be applied to generate nurs -\ning diagnoses and care plans without requiring extensive \ninfrastructure or training. With the continuous develop -\nment of AI technology, the compatibility between health-\ncare information systems and LLMs will soon be realized \nand progressively implemented. Currently, large compre-\nhensive hospitals in China have fully integrated the large \nlanguage model ‘DeepSeek-R1, ’ which has established an \nintelligent medical information system that provides AI-\ndriven support in clinical decision-making, nursing care, \nand medical record generation, significantly enhancing \nwork efficiency and alleviating workload. Therefore, the \nstudy of prompt engineering and its application in the \nnursing field will inevitably become a focal point of inter -\nest and a key topic for the nursing community in the AI \nera. Our research offers a novel exploratory approach and \ndirection in this regard.\nLimitations\nThe limitations of this study are as follows: 1. The struc -\ntured nursing assessment template encompasses a broad \nrange of content, and the information it contains may \nvary from brief and general to lengthy and comprehen -\nsive, depending on the assessor’s professional compe -\ntence, language skills, and personal preferences. In this \nstudy, we have only interpreted and defined the items \nof this template based on the disease characteristics of \nneurosurgical patients. Further research is needed to \nexplore the potential application of this template to other \ndiseases. 2. The prompt framework is variable, and even \nminor adjustments to its textual description can signifi -\ncantly affect the response outcomes. This study primar -\nily provides an exploratory approach aimed at enhancing \nclinical nurses’ understanding of LLMs and promoting \nprompt learning. 3. The quality of texts generated by dif -\nferent LLMs is also influenced by the language type of \nthe training dataset (e.g., grammatical structures and lan-\nguage logic in English or Chinese). Selecting an appropri-\nate LLM based on user needs may be an effective strategy \nfor obtaining high-quality responses. Additionally, criti -\ncally synthesizing texts generated by different LLMs on \nthe same issue can be a valuable approach to achieving \nhigh-quality output. 4. Continuous vigilance is required \nregarding the potential legal, safety, and ethical risks \nassociated with nursing care plans generated by LLMs. \nThere is an urgent need to improve legislation and rel -\nevant medical and nursing systems to ensure the effective \nand responsible use of LLMs, enabling them to assist and \nenhance clinical nursing practice.\nConclusion\nThis study explored the process of applying structured \nnursing assessment combined with prompting tech -\nniques to generate nursing diagnoses and care plans \nusing various LLMs. It demonstrated the potential appli -\ncation of the prompt framework we developed in neuro -\nsurgical discharge cases, while also imposing necessary \nrestrictions from the user operation perspective to miti -\ngate risks related to legal issues, privacy breaches, and \nethical concerns. The study further confirmed the poten -\ntial of LLMs in clinical nursing practice. However, a gap \nremains between this potential and the actual implemen -\ntation of LLMs in assisting nursing procedures in clinical \nsettings. Urgent challenges include the need for improved \nlaws and regulations, enhanced privacy protection, and \nstronger ethical frameworks. Furthermore, there is con -\nsiderable variation in clinical nurses’ awareness and \nacceptance of LLMs, and currently, there is a lack of \nLLM-specific training and education for clinical nurses. \nIn the field of computer science, integrating or develop -\ning LLMs compatible with hospital information systems \npresents another significant challenge. Extensive future \nresearch and exploration are required to enable LLMs \nto serve clinical practice safely and effectively, alleviating \nnurses from burdensome procedural and repetitive tasks, \nPage 10 of 11\nCao et al. BMC Nursing          (2025) 24:394 \nenabling them to provide personalized care, and ulti -\nmately benefiting patient outcomes.\nSupplementary Information\nThe online version contains supplementary material available at  h t t p  s : /  / d o i  . o  r \ng /  1 0 .  1 1 8 6  / s  1 2 9 1 2 - 0 2 5 - 0 3 0 1 0 - 2.\nSupplementary Material 1\nAuthor contributions\nJP , YC1 and LH were responsible for the study conception and design; JP , \nYC1, LH and XC3 performed the construction and iteration of the prompt \nframework; YC1 and LH performed the organization and analysis of the textual \nresults. JP , YC1 and LH were responsible for the drafting of the manuscript. All \nauthors read and approved the final manuscript, and all authors agreed to be \naccountable for all aspects of the work.\nFunding\nThe authors received no external funding or grant to undertake this research.\nData availability\nNo datasets were generated or analysed during the current study.\nDeclarations\nEthical approval and consent to participate\nThe study was conducted in compliance with the Declaration of Helsinki \nand received ethical approval from the Medical Ethics Committee of the \nFirst Affiliated Hospital of Chongqing Medical University (Ethics review batch \nnumber K2023-191). All of the participants provided informed consent to \nparticipate in the study. Large Language Models were only used to generate \nnursing diagnoses and care plans in this study. Large Language Models were \nnot utilized for the writing of the manuscript. All writing was completed by \nhuman authors. This is hereby stated for clarification.\nConsent for publication\nNot applicable.\nClinical trial number\nNot applicable.\nCompeting interests\nThe authors declare no competing interests.\nReceived: 21 December 2024 / Accepted: 20 March 2025\nReferences\n1. Gleason N. ChatGPT and the rise of AI writers: how should higher education \nrespond? Times High Educ. 2022.  h t t p  s : /  / w w w  . t  i m e  s h i  g h e r  e d  u c a  t i o  n . c o  m /  c a \nm  p u s  / c h a  t g  p t -  a n d  - r i s  e -  a i -  w r i  t e r s  - h  o w -  s h o  u l d -  h i  g h e r - e d u c a t i o n - r e s p o n\n2. Open AI. 2023.  h t t p s :   /  / o p e n a   i .  c o  m  / n e   w s  / r e s e a r c h /\n3. Yalcinkaya T, Cinar Yucel S. Bibliometric and content analysis of ChatGPT \nresearch in nursing education: The rabbit hole in nursing education. Nurse \nEduc Pract. 2024;77:103956.\n4. Alessandri-Bonetti M, Liu HY, et al. The first months of life of ChatGPT and its \nimpact in healthcare: A bibliometric analysis of the current literature. Ann \nBiomed Eng. 2024 May;52(5):1107–10.\n5. Scerri A, Morin KH. Using chatbots like ChatGPT to support nursing practice. J \nClin Nurs. 2023;32(15–16):4211–3.\n6. Ruksakulpiwat S, Thorngthip S, et al. A systematic review of the application \nof artificial intelligence in nursing care: Where are we, and what’s next? J \nMultidiscip Healthc. 2024;17:1603–16.\n7. Toney-Butler TJ, Thayer JM, Nursing P . 2023 Apr 10. In: StatPearls [Internet]. \nTreasure Island (FL): StatPearls Publishing; 2024 Jan.\n8. Agyeman-Yeboah J, Korsah KA. Non-application of the nursing process at \na hospital in Accra, Ghana: Lessons from descriptive research. BMC Nurs. \n2018;17:45.\n9. Gosak L, Pruinelli L, et al. The ChatGPT effect and transforming nursing educa-\ntion with generative AI: Discussion paper. Nurse Educ Pract. 2024;75:103888.\n10. Woodnutt S, Allen C, et al. Could artificial intelligence write mental health \nnursing care plans? J Psychiatr Ment Health Nurs. 2024;31(1):79–86.\n11. Johnson LG, Madandola OO et al. Creating perinatal nursing care plans using \nChatGPT: A pathway to improve nursing care plans and reduce Documenta-\ntion burden. J Perinat Neonatal Nurs. 2024 Nov 1.\n12. Dos Santos FC, Johnson LG, et al. An example of leveraging AI for documen-\ntation: ChatGPT-generated nursing care plan for an older adult with lung \ncancer. J Am Med Inf Assoc. 2024;31(9):2089–96.\n13. Dağci M, Çam F et al. Reliability and quality of the nursing care planning texts \ngenerated by ChatGPT. Nurse Educ. 2024 May-Jun 01;49(3):E109–14.\n14. Sun GH. Prompt engineering for nurse educators. Nurse Educ. 2024 \nNov-Dec;01(6):293–9.\n15. Sivarajkumar S, Kelley M, et al. An empirical evaluation of prompting strate-\ngies for large Language models in Zero-Shot clinical natural Language \nprocessing: Algorithm development and validation study. JMIR Med Inf. \n2024;12:e55318.\n16. Herdmanh Kamitsurus. NANDA international nursing diagnoses definitions \nand classification. 2018–2020 [M]. 11th edition. NewYork: Thieme. 2019:1–5.\n17. NANDA International. Nursing diagnoses definitions & classification \n2018–2020. 11th ed. Thieme; 2018.\n18. Butcher HK, Bulechek GM, Dochterman JM, Wagner C, editors. Nursing inter-\nventions classification (NIC). 7th ed. Elsevier; 2018.\n19. Moorhead S, Swanson E, Johnson M, Maas ML, editors. Nursing outcomes \nclassification (NOC): Measurement of health Out-comes. 7th ed. Elsevier; \n2018.\n20. Jing, Chen, et al. Application of nursing interventions and nursing outcomes \nclassification in basic nursing education. J Nurs. 2012;27(8):72–4.\n21. Xiaoqin Guo. Development of a clinical pathway for breastfeeding using the \nOPT model and NNN linkage. Nurs Res. 2015;29(5 C):1852–5.\n22. Li X et al. Analysis of the effectiveness of home visits for elderly hypertensive \npatients in the community based on standardized nursing language. Chinese \nGeneral Practice. 2013;16(27):3231–3233.\n23. Duan X, Ding Y, Ning Y, Luo M. Application of NANDA-I nursing diagnoses, \nnursing interventions classification, and nursing outcomes classification in \nresearch and practice of cardiac rehabilitation nursing: A scoping review. Int J \nNurs Knowl. 2024;35(3):256–271. \n24. Wang SH, Sun Y, Xiang Y, et al. ERNIE 3.0 Titan: Exploring Larger-scale Knowl-\nedge Enhanced Pre-training for Language Understanding and Generation. \n2021.\n25. Qin R, Li Z, He W, et al. Mooncake: Kimi's KVCache-centric Architecture for \nLLM Serving. 2024.\n26. Santos ACFS, Mota ECH, Santos VD, et al. Validation of the nursing diagnosis \n\"labile emotional control\" in traumatic brain injury. J Nurs Scholarsh. \n2019;51(1):88–95.\n27. Cao M, Wang K, et al. Xinbian Hulixue Jichu. 4th ed. 2022.\n28. Li Y, Lu Q, et al. Waike Hulixue. 7th ed. 2021.\n29. Liu P , Yuan W, et al. Pre-train, Prompt, and Predict: A Systematic Survey of \nPrompting Methods in Natural Language Processing. ACM Comput Surv. \n2023;55(9):195.\n30. Fehring RJ. Methods to validate nursing diagnoses. Heart Lung. 1987 \nNov;16(6 Pt 1):625–9.\n31. Herdman TH, et al. NANDA-I HULI ZHENDUAN: DINGYI YU FENLEI. 1st ed. \n2023.\n32. Johnson M, et al. Nursing Diagnoses, Outcomes, & Interventions. 1st ed. 2010.\n33. O'Connor S, Peltonen LM, et al. Prompt engineering when using generative \nAI in nursing education. Nurse Educ Pract. 2024 Jan;74:103825.\n34. Xu B, Yang A, Lin J, Wang Q, Zhou C, Zhang Y, Mao Z. Expert Prompting: \nInstructing Large Language Models to be Distinguished Experts. arXiv. 2023 \nMay;2305.14688.\n35. Kojima T, Gu SS, et al. Large language models are zero-shot reasoners. Adv \nNeural Inf Process Syst. 2022;35:22199–22213.\n36. Patacchiola M, Cangelosi A. A Developmental Cognitive Architecture for \nTrust and Theory of Mind in Humanoid Robots. IEEE Trans Cybern. 2022 \nMar;52(3):1947–59.\n37. Hobensack M, von Gerich H, et al. A rapid review on current and poten-\ntial uses of large language models in nursing. Int J Nurs Stud. 2024 \nJun;154:104753.\nPage 11 of 11\nCao et al. BMC Nursing          (2025) 24:394 \n38. Abdulai AF, Hung L. Will ChatGPT undermine ethical values in nursing educa-\ntion, research, and practice? Nurs Inq. 2023 Jul;30(3):e12556.\n39. Tuncer GZ, Tuncer M. Investigation of nurses' general attitudes toward \nartificial intelligence and their perceptions of ChatGPT usage and influencing \nfactors. Digit Health. 2024 Aug 25.\n40. Chang LC, Wang YN, et al. Registered Nurses' Attitudes Towards ChatGPT and \nSelf-Directed Learning: A Cross-Sectional Study. J Adv Nurs. 2024 Oct 9.\n41. Krüger L, Krotsetis S, OpenAI’s Generative Pretrained Transformer 3 (GPT-3). \nModel; Nydahl P . ChatGPT: Fluch Oder Segen in der Pflege? [ChatGPT: \ncurse or blessing in nursing care?]. Med Klin Intensivmed Notfmed. 2023 \nOct;118(7):534–9. German.\n42. Tastan S, Linch GC, Keenan GM, Stifter J, McKinney D, Fahey L, Lopez KD, Yao Y, \nWilkie DJ. Evidence for the existing American Nurses Association-recognized \nstandardized nursing terminologies: A systematic review. Int J Nurs Stud. \n2014 Aug;51(8):1160–70.\n43. Drennan VM, Ross F. Global nurse shortages-the facts, the impact and action \nfor change. Br Med Bull. 2019 Jun;130(1):25–37.\nPublisher’s note\nSpringer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations."
}