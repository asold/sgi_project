{
    "title": "Language Models Are Poor Learners of Directional Inference",
    "url": "https://openalex.org/W4385573401",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2103815823",
            "name": "Tianyi Li",
            "affiliations": [
                "University of Edinburgh"
            ]
        },
        {
            "id": "https://openalex.org/A2156702240",
            "name": "Mohammad Javad Hosseini",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2124437155",
            "name": "Sabine Weber",
            "affiliations": [
                "University of Edinburgh"
            ]
        },
        {
            "id": "https://openalex.org/A2028447033",
            "name": "Mark Steedman",
            "affiliations": [
                "University of Edinburgh"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4285306964",
        "https://openalex.org/W2509913944",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2126170172",
        "https://openalex.org/W3103433205",
        "https://openalex.org/W2951286828",
        "https://openalex.org/W2919611234",
        "https://openalex.org/W2962736243",
        "https://openalex.org/W3044438666",
        "https://openalex.org/W2962985038",
        "https://openalex.org/W4297801719",
        "https://openalex.org/W3172642864",
        "https://openalex.org/W2406945108",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W4206510579",
        "https://openalex.org/W3173617765",
        "https://openalex.org/W3196347832",
        "https://openalex.org/W3156587088",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4200022262",
        "https://openalex.org/W2251044566",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3212160013",
        "https://openalex.org/W2970279348",
        "https://openalex.org/W2966024226",
        "https://openalex.org/W2962843521",
        "https://openalex.org/W2949911172",
        "https://openalex.org/W1840435438",
        "https://openalex.org/W2953213518",
        "https://openalex.org/W4226139377"
    ],
    "abstract": "We examine LMs’ competence of directional predicate entailments by supervised fine-tuning with prompts. Our analysis shows that contrary to their apparent success on standard NLI, LMs show limited ability to learn such directional inference; moreover, existing datasets fail to test directionality, and/or are infested by artefacts that can be learnt as proxy for entailments, yielding over-optimistic results. In response, we present BoOQA (Boolean Open QA), a robust multi-lingual evaluation benchmark for directional predicate entailments, extrinsic to existing training sets. On BoOQA, we establish baselines and show evidence of existing LM-prompting models being incompetent directional entailment learners, in contrast to entailment graphs, however limited by sparsity.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 903–921\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nLanguage Models Are Poor Learners of Directional Inference\nTianyi Li⋄ Mohammad Javad Hosseini⋄∗ Sabine Weber⋄ Mark Steedman⋄\n⋄ School of Informatics, University of Edinburgh\n{tianyi.li, javad.hosseini}@ed.ac.uk\ns.weber@sms.ed.ac.uk, steedman@inf.ed.ac.uk\nAbstract\nWe examine LMs’ competence of directional\npredicate entailments by supervised fine-tuning\nwith prompts. Our analysis shows that con-\ntrary to their apparent success on standard NLI,\nLMs show limited ability to learn such direc-\ntional inference; moreover, existing datasets\nfail to test directionality, and/or are infested\nby artefacts that can be learnt as proxy for en-\ntailments, yielding over-optimistic results. In\nresponse, we present BoOQA (Boolean Open\nQA), a robust multi-lingual evaluation bench-\nmark for directional predicate entailments, ex-\ntrinsic to existing training sets. On BoOQA, we\nestablish baselines and show evidence of exist-\ning LM-prompting models being incompetent\ndirectional entailment learners, in contrast to\nentailment graphs, however limited by sparsity.\n1 Introduction\nPre-trained language models have shown impres-\nsive performance in natural language understand-\ning (NLU), where prompting methods are widely\nused for fine-tuning (Raffel et al., 2020; Brown\net al., 2020; Schick and Schütze, 2021).\nIn this paper, we specifically investigate predi-\ncate entailment detection, an important sub-task of\nNLU and specifically, NLI. The task is to predict,\ngiven that predicate p holds between arguments\n<a, b>, whether it can be inferred that predicate\nq also holds between <a, b>. For instance, “John\nshopped in IKEA ” entails “John went to IKEA ”, but\nnot “John drove to IKEA ”.\nThe primary distinction between predicate en-\ntailments and semantic similarity, apart from being\nfocused on predicates, is that the former involves\ndirectional entailments as well as symmetric ones.\nDirectional entailments are those p⊨ q(pentails q)\nwhere q⊭ p; conversely, symmetric entailments are\nthose p⊨ qwhere q⊨ pas well (namely p≡q).\n∗ Now at Google Research.\nDirectional entailments are important for ques-\ntion answering, since they help filter out the spuri-\nous connections from knowledge sources to ques-\ntions: knowing that John went to IKEA , it is unsafe\nto infer that he shopped in IKEA , as he may have\nbeen there for other reasons. By symmetric sim-\nilarity (i.e. paraphrase), the two events would be\nconsidered related, so a spurious inference chain\nwould emerge; by directional entailments, it would\nbe concluded that while the two events are related,\nthe entailment holds only in the reverse direction,\nso the spurious connection would be avoided.\nCurrent LM-prompting methods have reported\npositive results on predicate entailment detection\n(Schmitt and Schütze, 2021b,a). Since the masked-\nlanguage-modelling objective naturally enables\nLMs to separate related and unrelated tokens, they\nare expected to be good paraphrase detectors; on\nthe other hand, it is less clear whether they also\ndistinguish the directionality of entailments.\nTo answer this question, we adapt the SOTA LM-\nprompting model (Schmitt and Schütze, 2021b) as\na gauge for the competence of its underlying LMs,\nin particular RoBERTa (Liu et al., 2019) and BERT\n(Devlin et al., 2019). We apply it to various sub-\nsets of the common benchmark LevyHolt (Levy\nand Dagan, 2016; Holt, 2019). We find that while\nit scores highly on the directional subset by Holt\n(2019), it otherwise shows poor ability in learn-\ning the directionality of predicate entailments. We\nfind instead that the LevyHolt directional subset\nis infested with artefacts, to which the supervised\nLM-prompting model is overfitting.\nThese observations show that we need a robust\nevaluation benchmark for directional predicate en-\ntailments, independent of training sets. Inspired by\nMcKenna et al. (2021) and Chen et al. (2017), we\npresent BoOQA, a Boolean Open QA dataset in En-\nglish and Chinese, which is closer to applications,\nadversarial to artefacts in supervision, and demands\nsensitivity to the directionality of entailments.\n903\nOn BoOQA, we re-examine supervised and un-\nsupervised LM methods along with various dis-\ncrete entailment graphs (EG) (Hosseini et al., 2018,\n2021; Chen et al., 2022; Li et al., 2022). We find\nthat the performances of supervised LM-prompting\nmethods are indifferent to directional supervision,\nand are generally less competitive than suggested\non LevyHolt; on the other hand, EGs reach decent\nprecisions with their strongest edges, but are hit by\nsparsity and noisy unsupervised signals.\nOur contributions can be summarized as follows:\n1) We show that LevyHolt, the common directional\npredicate entailments benchmark, is infested by\nartefacts, allowing supervised methods to perform\nwell by overfitting; 2) We prove that LMs, with su-\npervised fine-tuning, show limited ability to learn\ndirectional entailments; 3) We present BoOQA,\na robust, extrinsic, multilingual evaluation bench-\nmark for directional predicate entailments, where\nvarious baselines are provided and analysed.1\n2 Background and Setup\nLanguage models have been used under a pretrain-\nfinetune paradigm: the semantics of a token in\ncontext are learnt during pre-training and reflected\nin the dense encodings; when fine-tuning with a\ntask-specific dataset, the model learns which area\nof its encoding space to look at. Therefore, if a pre-\ntrained LM cannot be fine-tuned to solve a task,\nwe cannot reject the null hypothesis that it does\nnot encode the task. In §3, we look into RoBERTa\n(Liu et al., 2019) and BERT (Devlin et al., 2019) in\nparticular, and examine whether they can be fine-\ntuned to learn directional predicate entailments.\nModel We adapt the supervised SOTA (Schmitt\nand Schütze, 2021b), a prompt fine-tuning method,\nfor examining LMs.2 We call it S&S here and be-\nlow. S&S fits each premise-hypothesis pair into\na few natural language prompts, such as “ John\nshopped in IKEA , which means that John went to\nIKEA ”; they then convert the task into sentence\nclassification over instantiated prompts. It is a sim-\nple SOTA with few additional parameters, and the\narchitecture allows directional judgements. Thus,\nit is an ideal “gauge” for directional ability of LMs.\n1Our code and datasets are published at https://github.\ncom/Teddy-Li/LM-DirctionalInference.\n2There is a follow-up work (Schmitt and Schütze, 2021a)\nto this, but we found it to have inferior generalisation perfor-\nmance; see Appendix B for details and a brief introduction.\nDataset So far there are two popular predicate\nentailment datasets: LevyHolt (Levy and Da-\ngan, 2016; Holt, 2019) and Sherliic (Schmitt and\nSchütze, 2019). We use LevyHolt in our §3 ex-\nperiments, as it contains data entries ( p ⊨ q?)\nwith their converses (q ⊨ p?), making the ground\ntruth directionality annotations available. We use\nthe train/dev/test split as in Schmitt and Schütze\n(2021b).3 In each data split, We further classify the\nentries into the following 4 sub-groups, in paren-\ntheses are the sizes of each sub-group in each split:\n• DirTrue (251 / 64 / 892): directional true en-\ntailments where the premise entails the hy-\npothesis, but not vice versa; for instance, Per-\nson shopped in Location ⊨ Person went to\nLocation;\n• DirFalse (251 / 64 / 892): directional non-\nentailments where the hypothesis entails the\npremise, but not vice versa; for instance, Per-\nson went to Location ⊭ Person shopped in\nLocation;\n• Paraphrases (615 / 155 / 1939): symmetric\nparaphrases where the premise and the hy-\npothesis entail each other; for instance,Person\narrived at Location ≡Person got to Location;\n• Unrelated (3255 / 831 / 9198): unrelated pred-\nicate pairs where the premise and the hypoth-\nesis have no entailment relations; for instance,\nPerson shopped in Location ⊭ Person fell ill\nin Location.\nWe define various subsets with pairs of sub-\ngroups, which we introduce and discuss in §3.\nEvaluation Metric In predicate entailment detec-\ntion, Area-Under-the-Curve with precision >50%\n(AUC50%) has been the metric in use (Hosseini\net al., 2018; Schmitt and Schütze, 2021b). It is a\nsolid metric for comparison on the same dataset;\nhowever, we are comparing between different sub-\nsets, each with a different random baseline preci-\nsion (i.e. the ratio of true entailments). If we were\nto set a common precision lower-bound, we would\nbe biased toward those datasets with higher random\nbaseline precisions. To make performance on dif-\nferent datasets comparable, we propose the metric\nof Normalized AUC(AUCnorm ):\n3Except when an entry and its converse appear in different\nsplits (e.g. one in train, the other in dev), where we randomly\nassign both into the same split, so as to avoid information\nleakage.\n904\nAUCnorm = AUCξ −ξ\n1 −ξ (1)\nwhere ξis the random baseline precision. Intu-\nitively, AUCnorm measures the ratio of area-above-\nrandom (1 −ξ) that falls below the precision-recall\ncurve (AUCξ −ξ), see Appendix A for graphic\nillustration. AUCnorm allows us to take into ac-\ncount all gains against the random baseline, and\nlevel performance on all datasets to the same scale.\n3 Prompt Fine-tuning LM with LevyHolt\nIn this section, we test for LMs’ ability to learn\ndirectional entailments with the S&S prompt-based\ngauge model. We use RoBERTa-base as the pri-\nmary subject, as it is used by SOTA Schmitt and\nSchütze (2021b), and is sufficiently lightweight for\nexperiments to run efficiently. In Appendix C, we\nalso report results on RoBERTa-large and BERT\nmodels for key experiments, where results are con-\nsistent. We use S&Ssubset to denote S&S model\nfine-tuned on each subset.\nExperiments are graphically summarized in Fig-\nure 1. Each edge denotes a LevyHolt subset made\nof the two sub-groups; the number on each edge\nis the AUCnorm that S&S achieves on each sub-\nset. For separating an Entailed sub-group from a\nNon-entailed one, the original labels are used; for\nseparating two Entailed or two Non-entailed sub-\ngroups, the one with more similar predicates (more\nparaphrastic) is assigned label “1”, the other “0”.4\nNote that we fit S&S to a number of different\nsubsets, so we cannot simply re-use the original\nhyper-parameters. Instead, to provide a level play-\ning field, we follow Schmitt and Schütze (2021b) in\nlog-uniformly sampling 100 hyper-parameter sets\nfrom their specified ranges; for each subset, we\nchoose the one with best dev set result.\nIf a method is insensitive to directional entail-\nments, then it would treat entailments as similarity\nbetween unordered pairs of predicates; it would\nmodel Paraphrases, DirTrue and DirFalse simi-\nlarly, where DirTrue and DirFalse entries are con-\nceptually somewhat “semi-paraphrastic”.\n4We acknowledge that Schmitt and Schütze (2021b) use\nhand-crafted prompts tuned for entailment detection, so they\nmay be sub-optimal for separating same-label sub-groups; we\nargue that fixed-prompt LM tuning models are not too sensitive\nto their specific prompts (Logan IV et al., 2021; Webson and\nPavlick, 2022); nonetheless, we also report results from a\ncontinuous-prompt model (Schmitt and Schütze, 2021a) in\nAppendix B, where results are very similar.\nFigure 1: S&S models on mesh of pairs of sub-groups,\nresults in AUCnorm.\nFigure 2: Hypothesis-only artefact baselines on mesh of\npairs of sub-groups, results in AUCnorm.\nIf a method is sensitive to directional entailments,\nit should be able to discriminate between each pair\nof the four sub-groups. Particularly, it should ad-\nditionally be able to separate sub-groups in the up-\nper triangle of the mesh, coloured red in Figure 1.\nAmong these three tests of directionality, DirTrue-\nDirFalse is the most challenging: a method with\nno sensitivity to directionality should be at chance\nand get 0% AUCnorm ; this is traditionally called\nthe directional subset (Holt, 2019). For the other\ntwo subsets, a symmetric measure would do above\nrandom by identifying entries inDirTrue / DirFalse\nas statistically less similar than Paraphrases.\nBelow we discuss findings around the mesh\nand the triangle. The S&S model yields 77.7%\nAUCnorm when trained and tested on thefull Levy-\nHolt dataset, which we provide for readers’ refer-\nence.\n3.1 The S&S Triangle\nThe red triangle in Figure 1 presents mixed mes-\nsages about the directionality of RoBERTa LM:\non the most challenging DirTrue-DirFalse sub-\nset, it achieves an apparently excellent AUCnorm\nof 82.9%; however, on the other subsets, it gets\nmediocre results at 26.9% and 45.8% respectively.\n905\nTrain/Dev\nTest\nDirectional Symmetric Full\nDirectional 82.9 9.9 24.7\nSymmetric 0.2 79.1 61.3\nFull 46.4 84.8 77.7\nTable 1: Generalization performance of RoBERTa-base\nS&S classifier on the Directional and Symmetric subsets\nof LevyHolt. Values are in % of AUCnorm.\nAUCnorm (%) S&S ’S&S\nPara-DirTrue 26.9 19.8 (-7.1)\nPara-DirFalse 45.8 35.6 (-10.2)\nTable 2: Comparison between S&S with regular and\n’S&S with symmetric prompts. Paraphrases-DirTrue\nand Paraphrases-DirFalsesubsets are concerned.\nFor the directional subset (DirTrue-DirFalse),\nthe 82.9% AUCnorm not only surpasses the 77.7%\nfor Full LevyHolt, but is also on par with the 79.1%\nfor its mirroring Symmetric subset (Paraphrases-\nUnrelated), which should be easier by human\njudgement. Paradoxically for such a challenging\nsubset, the Directional subset enjoys the best per-\nformance in the mesh.\nTo understand this result, we did a generalisation\nexperiment between Directional and Symmetric,\nthe two disjoint, complementary subsets of Levy-\nHolt. As results in Table 1 show, classifiers from\nthe two subsets do not generalise to each other,\nand neither does S&SDirectional generalise to full\nLevyHolt. That is to say, either the two kinds of\n“entailments”, Directional and Symmetric, are dif-\nferent tasks from the LM’s perspective, or the S&S\nclassifier is overfitting to the directional subset.\nFor the Paraphrases-DirX subsets, results are far\nless impressive. To measure how well symmetric\nbaselines do in these subsets, we train two strictly-\nsymmetric S&S models, one on Paraphrases-\nDirTrue, the other on Paraphrases-DirFalse. For\nthese strictly-symmetric models we enforce all\nprompts to be in pairs of reverses (e.g. for the\nexample in §2, we would add “John went to IKEA ,\nwhich means that John shopped in IKEA ”). That\nway we guarantee from the input that no directional\njudgements can be made. We call these symmetric-\nprompt models ’S&S. From the results in Table\n2, we find that for both Paraphrases-DirTrueand\nParaphrases-DirFalse, there is only a modest differ-\nence between the performance of S&S classifier\nand ’S&S. This suggests that despite the results\nfrom the Directional Subset, RoBERTa LM shows\nlimited ability to detect directional entailments.\n3.2 The Artefacts Triangle\nFrom previous discussions, we notice that the\nscores for the directional subset is anomalously\nhigh. Below we reveal that this anomaly is an ef-\nfect of dataset artefacts, and that the artefacts in\nquestion are quite specific to the directional subset\nand generally less prominent in the other subsets.\nArtefacts aside, the S&S classifiers do not show\nstrong abilities to learn directional entailments.\nIt is difficult to identify sources of artefacts\nby manual inspection; on the other hand, Poliak\net al. (2018) have shown that hypothesis-only (H-\nonly) models can be a strong proxy for an artefact\nbaseline. Inspired by their findings, we instead\nuse H-only model as a proxy for the aggregated\nstrength of artefacts. For H-only model we use a\nrestricted version of S&S classifier, where we mask\nall premises with the word “true”.5\nWe report the H-only model’s results on the same\nmesh of subsets in Figure 2. For every subset, the\nH-only model still trails behind the S&S classifiers.\nThese gaps are partly explained by the fact that the\nH-only model does not capture all existing artefacts,\nbut is merely a proxy to their strengths.\nAs shown, the Directional subset indeed has par-\nticularly strong artefacts to exploit, with the H-only\nmodel reaching 48.9% AUCnorm , far above the\nother subsets. Between Paraphrases-DirTrueand\nParaphrases-DirFalse, the relative performance of\nS&S model on them is aligned with their relative\nstrengths of artefacts; this means, for RoBERTa,\nParaphrases is in fact similarly separable from\nDirTrue and DirFalse, in line with expectation.\nAlso interesting is the comparison between the\ndirectional and symmetric subsets. The two subsets\nhad similar S&S performances; however, there is a\ndifference of over 20% between their hypothesis-\nonly artefact strengths. That means the S&S clas-\nsifier is after all far better at the symmetric subset\nthan the directional one.\nFor a crude ranking, we inspect each sub-\nset according to the FullModel-HOnly ratio by\nAUCnorm (lower the stronger artifacts). We\nfind at rock bottom the Paraphrases-DirFalseand\nDirTrue-DirFalsesubsets with this ratio at 1.55 and\n5We use “true” to mask the premise because, when the\npremise is always true, the correctness of instantiated prompts\ndepends solely on the hypothesis.\n906\n1.70 respectively, indicating that their full-model\nscores are the heaviest over-estimations; next up\nis the 2.10 for DirFalse-Unrelated, all the other\nsubsets have this ratio around 3.\n3.3 The Story behind Artefacts - A Summary\nFrom the S&S triangle in Figure 1 and the artefact\ntriangle in Figure 2, we make the findings below.\nWe find that DirTrue and DirFalse look simi-\nlarly separable from Paraphrases, as well as from\nUnrelated. Moreover, for DirTrue and DirFalse,\nit is consistently harder to separate them from\nParaphrases than from Unrelated. This suggests\nRoBERTa is modelling both of them as closer to\nParaphrases than to Unrelated. This is typical for\nsymmetric measures, but also possible for direc-\ntional ones.\nHowever, as we examine the directionality tri-\nangles, we find that RoBERTa shows poor direc-\ntionality in Paraphrases-DirX subsets. It also does\nfar worse on the directional subset than on symmet-\nric, although the AUCnorm is inflated by dataset\nartefacts.\nIn conclusion, we find evidence that RoBERTa\n(and BERT, see Appendix C) are poor learners of\ndirectional inference. However, AUCnorm values\nare, after all, comparable but not additive, so we\nstill cannot measure the net competence by deduct-\ning H-only scores from S&S scores. Thus, in order\nto draw final conclusions on the directionality of\nLMs, we present BoOQA, a robust extrinsic evalu-\nation dataset.\n4 Boolean Open QA (BoOQA) Dataset\nWe have shown that the LevyHoltdirectional sub-\nset is especially infested with dataset artefacts and\nis not a robust benchmark for supervised methods.\nWith the directionality triangle, we have shown\nevidence that LMs have limited ability to learn di-\nrectional entailments. However, this experiment is\ntedious and inconclusive: even solving the triangle\ndoes not guarantee solving directionality, as arte-\nfacts always persist; besides, we are still unable\nto compare between supervised and unsupervised\nmethods.\nTo establish an extrinsic, robust and realistic eval-\nuation benchmark for directional predicate entail-\nments, and to encourage future research on the\ntopic, we present the Boolean Open-QA dataset\n(BoOQA).\nBoOQA is an automatically constructed dataset\nin English and Chinese. It draws inspirations from\nthe machine reading at scale task (Chen et al.,\n2017) as well as the QA evaluation in (McKenna\net al., 2021). The task is formalized as: given a\nproposition and a large pool of context articles,\ndetermine whether the queried proposition is true\nbased on the context pool.6\n4.1 Dataset Elicitation\nIn principle, we follow McKenna et al. (2021) in\nbuilding this dataset. We separate large news cor-\npora into context windows of 3-day time spans,\nselect the frequent argument pairs as starring argu-\nments, then select frequent relations about these\nstarring arguments as positive propositions. We\ngenerate negatives from positives, by keeping the\nargument pairs unchanged, and selecting negative\npredicates from the WordNet hyponyms of positive\npredicates. For instance, for a positive proposition\n“John goes to IKEA ”, we may derive a potential\nnegative proposition “John drives to IKEA ”.\nNegative predicates are hyponyms that are felici-\ntous but wrong in the current context: we check if a\nhyponym is felicitous by its corpus-wide presence;\nwe check if a hyponym is wrong for the current\ncontext by its context-window-wide absence with\nthe corresponding argument pair 7.\nWordNet hyponyms are good candidate negative\npredicates because: 1) they are guaranteed to be\nsimilar to their corresponding positives, thus adver-\nsarial to symmetric similarity; 2) they are more spe-\ncific than their positives and less likely to be true,\nso the risk of getting false negatives is reduced.\nWe take samples from the positive and negative\npropositions, and blend them to create the BoOQA\ndataset. For evaluation, we take these propositions\nas hypotheses, and retrieve premises from the cor-\nresponding context windows8. The task is then to\njudge the truthfulness of the hypotheses given the\ncontext windows. For a good entailment measure,\npositive hypotheses should be entailed by context\nmore confidently than negatives, where context\nmeans other mentions of the same starring argu-\n6We consider only propositions with binary relations;\nunary relations have only one argument for context disam-\nbiguation, thus are naturally noisier.\n7This is based on the assumption that a comprehensive\npool of news articles would follow the Gricean cooperative\nprinciple of communication (Davis, 2019), that it would in-\nclude all and only the known facts.\n8To avoid producing misleadingly optimistic scores, we\nforce all methods to ignore the exact sentences from which the\nhypotheses (or their corresponding hypernyms) are extracted.\n907\nments within the same context window.\nFor English, we follow McKenna et al. (2021)\nin using NewsCrawl (Barrault et al., 2019) as our\nsource corpus; for Chinese, we use the CLUE news\ncorpus (Xu et al., 2020). As we include entail-\nment graphs as unsupervised baselines, we remove\nfrom these corpora the articles seen in entailment\ngraph induction corpora; on the other hand, we re-\nfrain from deleting overlaps with the much larger\nLM-pretrain corpora, to avoid biasing our dataset\ntoward non-mainstream coverage.\nGiven the analogy between our BoOQA dataset\nand the evaluation in McKenna et al. (2021), we\nleave most details of dataset construction to Ap-\npendix D and refer readers also to their paper. Here\nwe highlight our additional efforts for the quality\nand robustness of BoOQA.\nQuality When generating negative predicates,\nwe look for hyponym replacements for every span\nin the positive predicates, instead of just the head\nverbs. For example, for “play game with”, we can\ngenerate candidate negative “foul game with” as\nwell as “play practice gamewith”. The former\nmay be absent from the whole corpus, the latter\nmay be present somewhere in the corpus but not the\ncurrent context window; then the former would be\ndiscarded and the latter kept. This allows us to gen-\nerate negatives for more complex predicates, avoid-\ning biasing toward shorter, simpler predicates; it is\nespecially useful in Chinese, where word bound-\naries are unmarked and WordNet entries are present\nacross multiple levels of granularity.\nWhen collecting hyponyms of WordNet matches,\ninstead of looking into the first synset of each\nmatch, for English we disambiguate matches to\ntheir most relevant synsets with a WSD method\n(Yap et al., 2020). The WSD method takes in a\nspan in sentential context as input, and produces\nthe most likely synset for that span given the con-\ntext. This way, we find more accurate hyponyms\nand generate more challenging negatives that are\nsimilar to their positives. Unfortunately, no such\nmodels or datasets are available for Chinese, so we\nhave to fall back to using the first synset.\nThirdly, we raise the bar for candidate negatives\nto be considered “wrong in the current context”:\nnot only must the predicate itself be absent from\nthe context window (with the current arguments),\nbut all its WordNet synonyms must also be absent.\nThis stricter filter further reduces false negatives.\nFourthly, we expand our source corpora to entire\nthe NewsCrawl / CLUE corpora instead of their\nslices, so we avoid biasing toward any particular\ntime spans. With larger corpora, we impose larger\nthresholds for starring-arguments, so the events in\nquestion are thoroughly discussed, fewer positives\ncannot be inferred from the context, namely fewer\nfalse positives9.\nLastly, we sample data entries in bundles of\npositives and negatives. Whenever a positive is\nsampled, its corresponding negatives must also\nbe sampled, and vice versa. This strengthens our\ndataset in the way that each method needs to as-\nsign higher confidence values to positives than their\ncorresponding negatives to get good results.\nRobustness While inspecting the QA dataset in\nMcKenna et al. (2021), we find that their positive\nand negative propositions are separable even with-\nout context: the generated negatives simply look\nless plausible than their corresponding positives.\nWe again use hypothesis-only (H-only) models to\nquantify this effect. Since we use the context to\nretrieve premises, the H-only model by definition\nmeans “without context”. Similarly to Schmitt and\nSchütze (2021b), we sample train / dev2 sub-splits\nout of the dev set for fine-tuning.10\nIdeally, without context, the negatives should be\ninseparable from the positives, the H-only model\nshould do badly. However, on McKenna dataset,\nit got 78.3% AUCnorm . That means McKenna\ndataset is also infiltrated by spurious correlations.\nAlthough this is not fatal since the dataset is only\nfor evaluation so models are not exposed to such\nartefacts, it is still desirable to improve robustness\nby bringing the negatives closer to the positives.\nTo this end, we also raise the bar for candidate\nnegatives to be considered “felicitous”: they have\nto be not just present in the corpus, but also frequent\nenough. We set the same frequency thresholds for\nfelicitousness between positives and negatives.\nIn addition, when sampling from the popula-\ntions, we create negatives samples such that they\nhave similar frequency distributions as the positives\npopulation. We calculate the ratio of predicates in\npositives population falling into each frequency\nrange, then sample the negatives proportionally. 11\nWe apply H-only model to BoOQA and report\nthe results in Table 3. BoOQAEN enjoys a rea-\n9False positives are still true events, but models should not\nbe expected to label them as “positive” given the context.\n10For details of this H-only model see Appendix E.1.\n11We sample positives/negatives in bundles, so the positive\nsamples are generally more frequent than the population.\n908\nTrain / Dev Set Test Set AUCnorm\nBoOQAEN BoOQAEN 51.0\nBoOQAZH BoOQAZH 84.2\nMcKenna McKenna 78.3\nLevyHoltFull EN BoOQAEN 2.8\nLevyHoltDir EN BoOQAEN 1.1\nLevyHoltSym EN BoOQAEN 0.8\nLevyHoltFull ZH BoOQAZH 4.0\nLevyHoltDir ZH BoOQAZH 1.2\nLevyHoltSym ZH BoOQAZH 2.4\nTable 3: Robustness measured by H-only model perfor-\nmance, values in %, lower is better. McKenna is the QA\ndataset in McKenna et al. (2021), LevyHoltDir and\nLevyHoltSym are directional and symmetric subsets.\nsonably significant drop against McKenna dataset\nfor H-only performance, meaning less exploitable\nspurious correlations. However, we acknowledge\nthat generated negatives are naturally hard to blend\nwith collected positives, and there remain clues in\nthe hypothesis themselves indicating their truthful-\nness. For BoOQAZH , the H-only result is still high\neven with our above efforts. Fortunately, for both\nlanguages, H-only models trained on LevyHolt sub-\nsets cannot identify the truthfulness of BoOQA hy-\npotheses, indicating that BoOQA remains safe as\nan extrinsic benchmark for evaluation only.\n# of positives negatives\nBoOQAEN dev 21809 36782\nBoOQAEN test 21773 36755\nBoOQAZH dev 18855 31117\nBoOQAZH test 19092 31479\nTable 4: Number of positive / negative entries in each\nsubset of BoOQA.\nStatistics and Discussions We provide dev/test\nsplits in BoOQA, each has circa 20k positives and\n40k negatives, as shown in Table 4. Positives and\nnegatives are sampled in bundles where at most 2\nnegatives correspond to each positive (subject to\navailability).\nNotably, the size of 60K entries is chosen empir-\nically, as we find results to have already stabalized\nat this size. Though the number of entries is 60K,\nthe number of entailments to be checked is magni-\ntudes larger: in assigning a score to each entry, a\nmethod must inspect the relation from each piece of\nevidence to the proposition at question. Moreover,\nsince the construction process is fully automatic, it\nis easily scalable and extendable to other corpora.\nIn Appendix E.2, we also present a human anal-\nysis for the felicitousness of BoOQA entries, and\nshow that 85%+ correctness can be expected for\nboth labels and both languages, which is rela-\ntively high and even comparable to some crowd-\nannotated datasets.\nOn BoOQA, directional entailments are not\nneeded to perform above random, but are needed\nto reach high precisions. Consider a positive propo-\nsition Aand its corresponding negative B; a third\nproposition C is retrieved from context articles. In\na simplified sense, a model is correct as long as it\nscores C ⊨ Ahigher than C ⊨ B. Now suppose\nthere are two perfect measures of entailments, ex-\ncept that one is directional, the other symmetric.\nWhen neither queried propositions entail the con-\ntext (i.e. A ⊭ C and B ⊭ C), the two measures\nbehave similarly; otherwise, when some of these\nreverse-direction entailments hold, chances are that\nthe symmetric measure would fall into the trap and\nproduce spurious results. Through this property,\ndirectionality is involved in BoOQA dataset; we\noffer a detailed analysis in Appendix E.3.\nMoreover, although BoOQA negatives are built\noff WordNet hyponyms of positives, the direc-\ntional entailments tested are not confined to hy-\nponymy/hypernymy. This is because BoOQA tests\nfor entailment strengths from context-propositions\nto the queries, instead of between the positive (hy-\npernym) and the negative (hyponym) queries them-\nselves. This allows us to attend to all kinds of direc-\ntional entailments as is present in the general news\ndomain, for instance, precondition-consequences.\n4.2 Experiments and Baselines\nIn Table 5, we present baselines on BoOQA test set\nwith 3 types of methods: unsupervised LMs, super-\nvised LM-driven methods and entailment graphs.\nEach method retrieves supporting evidence from\ncontext, which are the extracted propositions in\nthe same context window involving the same argu-\nments as the queried hypothesis; they then calculate\na score from each piece of evidence to the hypothe-\nsis, and take the highest score w.r.t any evidence as\nthe final confidence score for a hypothesis.\nUnsupervised LMsThey compute cosine similari-\nties between BERT-base encodings of the retrieved\nsupporting evidence and the hypothesis, as confi-\ndence scores. We also tried BERT-large, RoBERTa\nas well as alternative retrieval approaches, we leave\nthem to Appendix G, as their dev set results are not\n909\nAUCnorm (%) BoOQAEn BoOQAZh\nLMunsupervised 15.9 30.6\nS&SFull 25.6 23.1∗\n’S&SFull 26.2 -\nS&SSymmetric 25.1 -\nS&SDirectional 15.1 -\nEG BInc 29.8 39.5\nEG CNCE 34.5 -\nEG EGT2 26.8 -\nTable 5: Baselines on BoOQA test set in English and\nChinese. All methods are taken out-of-the-box. “EG\nXX” are entailment graphs with various entailment\nscores as described below.\nsuperior to the condition reported.\nSupervised LMs They use S&Ssubset models\nwith RoBERTa-base to compute entailment scores\nfrom retrieved evidence to the hypotheses. We\npresent S&Sfull as supervised LM baseline for\nBoOQAEN and BoOQAZH . On BoOQAEN , we\nalso compare S&SDirectional, S&SSymmetric and\n’S&SFull (trained on full LevyHolt with symmetric\nprompts, see §3.1). On BoOQAEN we also pro-\nvide large model results in Appendix G; those are\nnot higher than with base model.\nEntailment GraphsThey are unsupervised entail-\nment detection methods, where nodes in each graph\nare predicates, and directed edges between nodes\nrepresent entailment relations between predicates.\nEntailment graphs are induced from textual corpora\nwith directional co-occurrence signals. At test time,\nentailment scores are collected by looking up the\nedge weights from each predicate-in-evidence to\nthe predicate-in-hypothesis. For English, we evalu-\nate with BInc (Hosseini et al., 2018), CNCE (Hos-\nseini et al., 2021) and EGT2 (Chen et al., 2022); for\nChinese, we evaluate with BInc (Li et al., 2022).\nOn BoOQAEN, we re-examine the S&S\ngauge models in comparison to various unsuper-\nvised methods. LMunsupervised does poorly, yet\nS&SDirectional does even worse. This verifies\nthat its judgements are polluted by dataset arte-\nfacts. Moreover, it does not matter whether S&S\nis exposed to only symmetric entries or uses\npurely symmetric prompts, the performance is al-\nmost identical. This means RoBERTa is not learn-\ning from directional supervision (in LevyHolt) the\ndirectional ability to make progress on BoOQA.\nThese results echo our findings in §3. Though be-\ning supervised and only with domain transfer, the\nS&Smodels fail to outperform fully unsupervised\nFigure 3: Precision-recall curves for the representative\nmethods of each class on BoOQAEN test set.\nentailment graphs on BoOQA.\nEntailment graph methods generally have decent\nperformance. EG CNCE enjoys a considerable ad-\nvantage, particularly at the higher-precision end.\nWhile it may seem surprising for an unsupervised\nmethod to prevail in high-precisions range, this is\nexpected given that in BoOQA, directionality is\nrequired for methods to reach high precisions. On\nthe other hand, these discrete entailment graphs are\nhampered by sparsity; although a score is retrieved\nfor most queries with the help of fuzzy matching12,\nthe high precisions are short-lived measured by\nrecall, dropping rapidly to the moderate range of\nsub-60%. This suggests generalising unsupervised\ndirectional entailment graphs to continuous encod-\ning spaces could be a promising direction to follow.\n5 Related Work\nRelated work to our paper broadly fall into three\ncategories: Language-Model prompting, entail-\nment graphs and natural language inference.\nLM-prompting methods convert input in target\ntasks into natural language sentences (prompts),\nthereby converting target tasks into either language\nmodelling or sentence classification. Petroni et al.\n(2019) probed LM for factual knowledge, where\nJiang et al. (2020) proposed automatic prompt dis-\ncovery; on the other hand, GPT-3 (Brown et al.,\n2020) showed large LMs can do few-shot / zero-\nshot transfer with prompts, and Schick and Schütze\n(2021) showed a similar effect with smaller LMs\nand natural language prompts. Liu et al. (2021) pro-\nvided a comprehensive review of LM-prompting,\nwhich we refer readers to for more details.\nEntailment graphs (EGs) are graphs of predi-\ncates where directed edges represent entailment\n12Ignoring semantic role labels for arguments in the entail-\nment graph representations of predicates; see Appendix G for\nmore details and a comparison of Pr-Rec curves.\n910\nrelations between pairs of predicates. Weights on\nedges are entailment scores, induced from large\ntextual corpora via unsupervised learning, based\non directional co-occurrence signals; examples in-\nclude BInc (Szpektor and Dagan, 2008; Hosseini\net al., 2018), link prediction (Hosseini et al., 2019)\nand the LM-driven CNCE (Hosseini et al., 2021).\nBeing fully unsupervised, EGs are sheltered from\nartefacts in training data, but are at the same time\nsubject to noisy co-occurrence signals and sparsity\ncommon to discrete language resources.\nNotably, while CNCE EG (2021) uses LM, it in-\nvolves contextualized encodings for premises, and\nuncontextualized encodings for hypotheses, trained\nfrom scratch with millions of unsupervised data\nentries. The architecture and abundant supervision\nenables CNCE to encode directional entailments.\nEGT2 (Chen et al., 2022) is another LM-driven EG\ncontemporary to our paper, with intermediate fine-\ntuning on MNLI (Williams et al., 2018) and PPDB\n(Ganitkevitch et al., 2013) annotations. They report\ngood resuts on thedirectional subset of LevyHolt13,\nbut perform mediocrely on BoOQA. We speculate\nthat this could be due to instabilities of the small\nLevyHolt directional test set.\nNatural Language Inference is concerned with\nthe general task of “Does sentence X entail sen-\ntence Y?”. Classic NLI datasets such as SNLI\n(Bowman et al., 2015) or MNLI (Williams et al.,\n2018) are populated with entries involving hyper-\nnyms of nouns, logic reasoning like A∧B →B\nand “Aunder the condition of B” →A, whereas\npredicate entailments are rarely involved. Predi-\ncate entailment datasets like LevyHolt (Levy and\nDagan, 2016; Holt, 2019), SherlIic (Schmitt and\nSchütze, 2019) or our BoOQA dataset, are a sub-\nclass of NLI datasets focused on inference with\npredicates, where the arguments and adjuncts are\ncarried over from premise to hypothesis. In particu-\nlar, LevyHolt and our BoOQA dataset both feature\nthe property of directionality, where the entailment\nis valid in one direction, but invalid in the other.\nGeneral NLI datasets are prune to simple\ndataset artefacts such as length, lexical overlaps or\nhypothesis-only models (Gururangan et al., 2018;\nPoliak et al., 2018; Kalouli et al., 2021; McCoy\net al., 2019). These echo our observations on predi-\ncate entailments, and underscore the importance of\na robust and extrinsic evaluation.\n13Their reported directional subset is different from the\noriginal one in Holt (2019), see Appendix H for details.\n6 Conclusion\nIn this paper, we show that existing LM prompt-\ning methods show limited ability to learn direc-\ntional entailments, and instead overfit to dataset\nartefacts in the LevyHolt directional subset. We\npresent BoOQA, a robust and extrinsic multilin-\ngual evaluation benchmark on directional predicate\nentailments as a Boolean Open QA dataset. On\nBoOQA, we show that LM prompting methods do\nnot learn to pay attention to directionality from di-\nrectional supervision, while the entailment graph\nmethods remain limited by sparsity. We point to\nthe generalisation of entailment graphs to continu-\nous spaces and directionality-oriented intermediate\npre-training of LMs as directions of future work.\nLimitations\nThe remaining limitations of our work are two-fold.\nFirstly, our experiments are done primarily with\nRoBERTa and BERT models. This choice is taken\nfollowing the positive results reported in Schmitt\nand Schütze (2021b,a). We argue that the con-\nsistency among these models indicates a broader\ngeneralisability of our conclusions, however we are\nunfortunately unable to enumerate over the wide va-\nriety of language models to prove this point. Partic-\nularly, due to limits to our computational resources,\nwe are unable to experiment with extremely large\nlanguage models such as GPT-3. We release the\ncode and data for the experiments as described in\n§3 and §4, and encourage the community to try out\nany language models they might be interested in.\nThe second aspect of limitation is, despite our\nefforts to improve the robustness of our BoOQA\ndataset, there remain correlations between just the\nsurface forms of the propositions and their truthful-\nness, which could be exploitable under a supervised\nsetting. We argue that one cannot eliminate such\ncorrelations without heavily manipulating which\npropositions are of interest to the dataset, which is\nagainst our aim of preserving the natural distribu-\ntion of propositions in large-scale news corpora; we\nalso argue that this is harmless as long as BoOQA\nremains an extrinsic benchmark, namely, the train-\ning set of any supervised method should not have\nthe same kinds of correlations. We have checked\nthat the LevyHolt dataset and its various subsets\nsatisfy this criterion; we advise that readers attempt-\ning this dataset with their own methods do the same\ncheck to ensure comparable results.\n911\nAcknowledgements\nThe authors would like to thank Jeff Pan and Nick\nMckenna for helpful discussions and the anony-\nmous reviewers for their valuable feedback. This\nwork was supported partly by ERC Advanced Fel-\nlowship GA 742137 SEMANTAX, a Mozilla PhD\nscholarship at Informatics Graduate School and the\nUniversity of Edinburgh Huawei Laboratory.\nReferences\nLoïc Barrault, Ond ˇrej Bojar, Marta R. Costa-jussà,\nChristian Federmann, Mark Fishel, Yvette Gra-\nham, Barry Haddow, Matthias Huck, Philipp Koehn,\nShervin Malmasi, Christof Monz, Mathias Müller,\nSantanu Pal, Matt Post, and Marcos Zampieri. 2019.\nFindings of the 2019 Conference on Machine Trans-\nlation (WMT19). In Proceedings of the Fourth Con-\nference on Machine Translation (Volume 2: Shared\nTask Papers, Day 1), pages 1–61, Florence, Italy. As-\nsociation for Computational Linguistics.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Advances\nin neural information processing systems, volume 33,\npages 1877–1901. Curran Associates, Inc.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to Answer Open-\nDomain Questions. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1870–\n1879, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nZhibin Chen, Yansong Feng, and Dongyan Zhao. 2022.\nEntailment Graph Learning with Textual Entailment\nand Soft Transitivity. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 5899–\n5910, Dublin, Ireland. Association for Computational\nLinguistics.\nWayne Davis. 2019. Implicature. In Edward N. Zalta,\neditor, The Stanford Encyclopedia of Philosophy, fall\n2019 edition. Metaphysics Research Lab, Stanford\nUniversity.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nJuri Ganitkevitch, Benjamin Van Durme, and Chris\nCallison-Burch. 2013. PPDB: The Paraphrase\nDatabase. In Proceedings of the 2013 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 758–764, Atlanta, Georgia. As-\nsociation for Computational Linguistics.\nSuchin Gururangan, Swabha Swayamdipta, Omer Levy,\nRoy Schwartz, Samuel Bowman, and Noah A. Smith.\n2018. Annotation Artifacts in Natural Language In-\nference Data. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 107–112,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nXavier Holt. 2019. Probabilistic Models of Relational\nImplication. arXiv:1907.12048 [cs, stat] . ArXiv:\n1907.12048.\nMohammad Javad Hosseini, Nathanael Chambers, Siva\nReddy, Xavier R. Holt, Shay B. Cohen, Mark John-\nson, and Mark Steedman. 2018. Learning Typed En-\ntailment Graphs with Global Soft Constraints. Trans-\nactions of the Association for Computational Linguis-\ntics, 6:703–717.\nMohammad Javad Hosseini, Shay B. Cohen, Mark John-\nson, and Mark Steedman. 2019. Duality of Link\nPrediction and Entailment Graph Induction. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4736–\n4746, Florence, Italy. Association for Computational\nLinguistics.\nMohammad Javad Hosseini, Shay B. Cohen, Mark John-\nson, and Mark Steedman. 2021. Open-Domain Con-\ntextual Link Prediction and its Complementarity with\nEntailment Graphs. In Findings of the Association\nfor Computational Linguistics: EMNLP 2021, pages\n2790–2802, Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How Can We Know What Language\nModels Know? Transactions of the Association\nfor Computational Linguistics , 8:423–438. Place:\nCambridge, MA Publisher: MIT Press.\n912\nAikaterini-Lida Kalouli, Livy Real, Annebeth Buis,\nMartha Palmer, and Valeria de Paiva. 2021. Annota-\ntion Difficulties in Natural Language Inference. In\nAnais do Simpósio Brasileiro de Tecnologia da In-\nformação e da Linguagem Humana (STIL) , pages\n247–254. SBC. ISSN: 0000-0000.\nKeshav Kolluru, Vaibhav Adlakha, Samarth Aggarwal,\nMausam, and Soumen Chakrabarti. 2020. OpenIE6:\nIterative Grid Labeling and Coordination Analysis for\nOpen Information Extraction. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 3748–3761,\nOnline. Association for Computational Linguistics.\nOmer Levy and Ido Dagan. 2016. Annotating Rela-\ntion Inference in Context via Question Answering.\nIn Proceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n2: Short Papers), pages 249–255, Berlin, Germany.\nAssociation for Computational Linguistics.\nTianyi Li, Sabine Weber, Mohammad Javad Hosseini,\nLiane Guillou, and Mark Steedman. 2022. Cross-\nlingual Inference with A Chinese Entailment Graph.\nIn Findings of the Association for Computational\nLinguistics: ACL 2022 , pages 1214–1233, Dublin,\nIreland. Association for Computational Linguistics.\nXiao Ling and Daniel S. Weld. 2012. Fine-grained en-\ntity recognition. In Proceedings of the Twenty-Sixth\nAAAI Conference on Artificial Intelligence, AAAI’12,\npages 94–100, Toronto, Ontario, Canada. AAAI\nPress.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021. Pre-\ntrain, Prompt, and Predict: A Systematic Survey of\nPrompting Methods in Natural Language Processing.\narXiv:2107.13586 [cs]. ArXiv: 2107.13586.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv:1907.11692 [cs] . ArXiv:\n1907.11692.\nRobert L. Logan IV , Ivana Balaževi ´c, Eric Wallace,\nFabio Petroni, Sameer Singh, and Sebastian Riedel.\n2021. Cutting Down on Prompts and Parameters:\nSimple Few-Shot Learning with Language Mod-\nels. Technical Report arXiv:2106.13353, arXiv.\nArXiv:2106.13353 [cs] type: article.\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right\nfor the Wrong Reasons: Diagnosing Syntactic Heuris-\ntics in Natural Language Inference. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 3428–3448, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nNick McKenna, Liane Guillou, Mohammad Javad Hos-\nseini, Sander Bijl de Vroe, Mark Johnson, and Mark\nSteedman. 2021. Multivalent Entailment Graphs for\nQuestion Answering. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 10758–10768, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language Models as Knowl-\nedge Bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473, Hong Kong, China. Association\nfor Computational Linguistics.\nAdam Poliak, Jason Naradowsky, Aparajita Haldar,\nRachel Rudinger, and Benjamin Van Durme. 2018.\nHypothesis Only Baselines in Natural Language In-\nference. In Proceedings of the Seventh Joint Confer-\nence on Lexical and Computational Semantics, pages\n180–191, New Orleans, Louisiana. Association for\nComputational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the Lim-\nits of Transfer Learning with a Unified Text-to-Text\nTransformer. Journal of Machine Learning Research,\n21(140):1–67.\nSiva Reddy, Mirella Lapata, and Mark Steedman. 2014.\nLarge-scale Semantic Parsing without Question-\nAnswer Pairs. Transactions of the Association for\nComputational Linguistics, 2:377–392.\nTimo Schick and Hinrich Schütze. 2021. It’s Not Just\nSize That Matters: Small Language Models Are Also\nFew-Shot Learners. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 2339–2352, Online. As-\nsociation for Computational Linguistics.\nMartin Schmitt and Hinrich Schütze. 2019. SherLIiC:\nA Typed Event-Focused Lexical Inference Bench-\nmark for Evaluating Natural Language Inference. In\nProceedings of the 57th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 902–\n914, Florence, Italy. Association for Computational\nLinguistics.\nMartin Schmitt and Hinrich Schütze. 2021a. Contin-\nuous Entailment Patterns for Lexical Inference in\nContext. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 6952–6959, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nMartin Schmitt and Hinrich Schütze. 2021b. Language\nModels for Lexical Inference in Context. In Proceed-\nings of the 16th Conference of the European Chap-\nter of the Association for Computational Linguistics:\n913\nMain Volume, pages 1267–1280, Online. Association\nfor Computational Linguistics.\nIdan Szpektor and Ido Dagan. 2008. Learning Entail-\nment Rules for Unary Templates. In Proceedings of\nthe 22nd International Conference on Computational\nLinguistics (Coling 2008), pages 849–856, Manch-\nester, UK. Coling 2008 Organizing Committee.\nAlbert Webson and Ellie Pavlick. 2022. Do Prompt-\nBased Models Really Understand the Meaning of\ntheir Prompts? Technical Report arXiv:2109.01247,\narXiv. ArXiv:2109.01247 [cs] type: article.\nJulie Weeds and David Weir. 2003. A General Frame-\nwork for Distributional Similarity. In Proceedings\nof the 2003 Conference on Empirical Methods in\nNatural Language Processing, pages 81–88.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A Broad-Coverage Challenge Corpus for Sen-\ntence Understanding through Inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nLiang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao,\nYudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong\nYu, Yin Tian, Qianqian Dong, Weitang Liu, Bo Shi,\nYiming Cui, Junyi Li, Jun Zeng, Rongzhao Wang,\nWeijian Xie, Yanting Li, Yina Patterson, Zuoyu Tian,\nYiwen Zhang, He Zhou, Shaoweihua Liu, Zhe Zhao,\nQipeng Zhao, Cong Yue, Xinrui Zhang, Zhengliang\nYang, Kyle Richardson, and Zhenzhong Lan. 2020.\nCLUE: A Chinese Language Understanding Evalu-\nation Benchmark. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics,\npages 4762–4772, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\nBoon Peng Yap, Andrew Koh, and Eng Siong Chng.\n2020. Adapting BERT for Word Sense Disambigua-\ntion with Gloss Selection Objective and Example\nSentences. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020, pages 41–46,\nOnline. Association for Computational Linguistics.\nA Graphic Illustration forAUCnorm\nSee Figure 4 for a graphic illustration ofAUCnorm ,\nand a comparison between different metrics.\nB Details and Comparison of S&S Models\nB.1 Between Discrete / Continuous Prompts\nThe S&S models (Schmitt and Schütze, 2021b,a)\nare LM-prompting models based on RoBERTa.\nOf which, S&S-discrete (Schmitt and Schütze,\n2021b) falls into the category of “Fixed-prompt\nLM Tuning” according to the taxonomy in Liu\nFigure 4: Diagram illustration of AUCnorm in com-\nparison to other AUC metrics. Red and bolded is a\nsimplified precision-recall curve, each AUC metric is\nrepresented as ratios of area of the respective coloured\nblocks. The total area of all blocks equals to 1. Note that\nin practice the precision of any method rarely comes\nbelow random (as that would mean the least confident\nentries carry some kind of reverse judgements), and in\nany case not beyond the scale reasonable for random\nfluctuations. That is, real pr-rec curves do not cut deeply\ninto the dark grey area like the over-simplified demo.\net al. (2021); whereas its follow-up work S&S-\ncontinuous (Schmitt and Schütze, 2021a) falls into\nthe category of “Prompt+LM Tuning”.\nS&S-discrete uses human-engineered natural\nlanguage prompts. Given a premise-hypothesis\npair, it instantiates each prompt with the pair, feeds\nthem into the LM, thus converts the task into a\nregular binary classification task from the LM rep-\nresentations of the instantiated prompts.\nNotably, we discovered a bug in the implemen-\ntation of Schmitt and Schütze (2021b): due to\nmissing negations, for experiments on LevyHolt\ndataset, the manual prompt (d) and (e) listed in\nSchmitt and Schütze (2021b) were effectively con-\nverses of prompt (c) and (a) respectively. After\nfixing the bug, we found the model performance\nwith RoBERTa-base, measured byAUC50%, as re-\nported in their original paper, rises from 76.9% to\n80.7%; correspondingly, the AUCnorm value rises\nfrom 75.3% to 77.7%.\nS&S-continuous is in essence similar to S&S-\ndiscrete, except that it replaces the embeddings\nof human-engineered natural language prompt to-\nkens with sequences of trainable continuous em-\nbeddings. S&S-continuous reported SOTA results,\noutperforming S&S-discrete; however, we stick\n914\nModel LevyHolt SherlIic\nTrained with LevyHolt\nS&S-discrete 80.7 (77.7) 69.5 (59.5)\nS&S-continuous 79.2 (76.2) 65.1 (54.3)\nTrained with SherlIic\nS&S-discrete 35.5 (47.7) 63.8 (51.5)\nS&S-continuous 25.1 (32.8) 66.5 (57.9)\nTable 6: Generalization experiment between Levy-\nHolt and SherLIic datasets, examined models are S&S-\ndiscrete and S&S-continuous. Performance is measured\nin AUC50%, where the AUCnorm values are appended\nin brackets. Note that reported values are from our re-\nproduction with our 100 sets of hyper-parameter configs,\nso the exact values slightly differ from the originals.\nwith the S&S-discrete model for our experiments\nfor the following reasons:\n• After fixing the bug in S&S-discrete imple-\nmentation, its genuine performance is not\nworse than that reported for S&S-continuous;\n• We did a generalization experiment between\nthe two predicate entailment datasets, Levy-\nHolt (Levy and Dagan, 2016) and SherlIic\n(Schmitt and Schütze, 2019), where we found\nthat S&S-discrete is better at generalizing\nfrom one dataset to the other, as shown in\nTable 6; that suggests the S&S-discrete is less\nprone to overfitting;\n• For the same LM backbone, S&S-discrete has\na smaller computational footprint.\nB.2 Subsets with Same-label Sub-groups\nAUCnorm (%) Discrete Continuous\nPara-DirTrue 26.9 25.5\nDirFalse-Unrl 58.6 62.2\nTable 7: Performance of S&Smodel on Paraphrases-\nDirTrue and DirFalse-Unrelated subsets. Discrete and\nContinuous are S&S-discrete and S&S-continuous.\nFor the subsets with same-label sub-groups,\nnamely, the subsets whose two sub-groups are ei-\nther both true entailments or both non-entailments,\nwe assign positive labels to the sub-group with\nhigher level of relevance, and negative labels to the\nother. As discussed in §3, we acknowledge that\nSchmitt and Schütze (2021b) uses hand-crafted\nprompts which are designed for labelling the truth-\nfulness of a candidate entailment, so they may\nnot be ideal for separating same-label sub-groups.\nTrain/Dev\nTest\nDirectional Symmetric Full\nDirectional 71.1 2.6 11.6\nSymmetric 2.1 90.1 70.9\nFull 52.4 90.7 83.7\nTable 8: Generalization performance of RoBERTa-\nlarge S&S classifier on the DirTrue-DirFalse (Direc-\ntional) and Paraphrases-Unrelated(Symmetric) subsets\nof LevyHolt. Values are in % of AUCnorm. Rows are\ntrain/dev subsets, columns are test subsets.\nAUCnorm S&S ’S&S\nPara-DirTrue 43.4 33.2 (-10.2%)\nPara-DirFalse 59.4 49.9 (-9.5%)\nTable 9: Comparison between RoBERTa-largeS&S\nclassifiers (regular) and ’S&S (symmetric prompts).\nParaphrases-DirTrue and Paraphrases-DirFalse sub-\nsets are concerned.\nWhile we argue that fixed-prompt LM tuning mod-\nels are not too sensitive to their specific prompts\n(Logan IV et al., 2021; Webson and Pavlick, 2022),\nwe also supplement the experiment with continu-\nous prompt models.\nThe prompts in S&S-continuous are sequences\nof trainable embeddings, which can be tuned for\nany purpose, not just to separate true and false\nentailments; moreover, as Table 6 shows, S&S-\ncontinuous and S&S-discrete have similar perfor-\nmance when tested on the same dataset as train set.\nSo we use S&S-continuous as a control model, to\nunderstand how much the S&S-discrete is affected\nby prompt suitability.\nIn Table 7, we report results on the two same-\nlabel subsets with S&S-continuous in comparison\nto S&S-discrete. We find the results from the two\nmodels to be very similar. This means, as aligned\nwith our argument above, the prompt-suitability\nissue is not jeopardising our experiment.\nC LevyHolt Experiments on More LMs\nIn this section, we report more results from the\nLevyHolt experiments. In particular, we replicate\nthe generalisation experiment and the symmetric-\nprompt experiment in §3.1 with RoBERTa-large,\nBERT-base and BERT-large models.\nDirectional-Symmetric generalization results for\nthe three LMs are summarized in Table 8, 10 and\n12 respectively, mirroring Table 1. The conclusions\nare consistent, where S&S classifiers trained on the\n915\nTrain/Dev\nTest\nDirectional Symmetric Full\nDirectional 75.7 4.4 15.4\nSymmetric 0.3 67.5 53.0\nFull 30.1 69.5 62.4\nTable 10: Generalization performance of BERT-base\nS&S classifier on the DirTrue-DirFalse (Directional)\nand Paraphrases-Unrelated (Symmetric) subsets of\nLevyHolt. Values are in % of AUCnorm. Rows are\ntrain/dev subsets, columns are test subsets.\nAUCnorm S&S ’S&S\nPara-DirTrue 5.0 3.4 (-1.6%)\nPara-DirFalse 17.8 11.7 (-6.1%)\nTable 11: Comparison between BERT-base S&S\nclassifiers (regular) and ’S&S (symmetric prompts).\nParaphrases-DirTrue and Paraphrases-DirFalse sub-\nsets are concerned.\ntwo subsets are still not generalising to each other.\nThe comparison with symmetric-prompt base-\nlines for the above three LMs are in Table 9, 11\nand 13. There again, scaling up the language\nmodel or switching from RoBERTa to BERT can-\nnot lift the regular-prompt S&S model further away\nfrom the symmetric-prompt one. Particularly in\nTable 13, ’S&SPara−DirTrue actually outperforms\nS&SPara−DirTrue, highlighting that performances\nof LMs are unstable on smaller fine-tune datasets,\neven with 100 sets of hyper-parameters for choice.\nThese observations echo those listed in §3, and\nare verify that our conclusions in §3 are generalis-\nable at least to RoBERTa-large and BERT language\nmodels. We further argue that they are very likely\ngeneralisable to other language models following\nthe same training scheme as well.\nD Detailed Guide for BoOQA\nConstruction\nThis section is of interest to readers who wish to\ndig into the details of how BoOQA is built, or to\nbuild their own flavour of BoOQA dataset.\nThe BoOQA dataset is constructed off large-\nscale general-domain news corpora. For English,\nwe use the NewsCrawl corpus, which contains\n7.8M news articles published between 2007 and\n2017; for Chinese, we use the CLUE corpus, which\ncontains 2.4M news articles, all published in 2016.\nWith corpora at such scale, they naturally have\nTrain/Dev\nTest\nDirectional Symmetric Full\nDirectional 71.7 2.7 9.1\nSymmetric 0.0 77.1 58.2\nFull 49.4 80.9 75.4\nTable 12: Generalization performance of BERT-large\nS&S classifier on the DirTrue-DirFalse (Directional)\nand Paraphrases-Unrelated (Symmetric) subsets of\nLevyHolt. Values are in % of AUCnorm. Rows are\ntrain/dev subsets, columns are test subsets.\nAUCnorm S&S ’S&S\nPara-DirTrue 10.8 20.2 (+9.4%)\nPara-DirFalse 54.9 37.4 (-17.5%)\nTable 13: Comparison between BERT-large S&S\nclassifiers (regular) and ’S&S (symmetric prompts).\nParaphrases-DirTrue and Paraphrases-DirFalse sub-\nsets are concerned.\noverlaps with the corpora used for building models.\nOur introduced baselines include LM-driven meth-\nods and Entailment-Graph-based methods. The en-\ntailment graphs are induced from smaller corpora,\nwhich we can afford to exclude from NewsCrawl or\nCLUE; on the other hand, the LMs are pre-trained\non corpora of much larger scale, where it is imprac-\ntical to remove all overlaps between the corpora,\nso we refrain from such attempts.\nWe partition each corpus into context windows\nby disjoint 3-day time spans. We extract open re-\nlation triples (subject, predicate, object) from the\narticles with parsers (Reddy et al., 2014; Li et al.,\n2022). The use of automatic parsing-based rela-\ntion extraction enables us to collect data entries at\nlarge scales. The reason for using these parsers\ninstead of neural taggers such as OpenIE (Kolluru\net al., 2020), is that they have wider coverage and\nare capable of handling discontinuous predicates,\nwhereas OpenIE only extracts contiguous spans.\nFrom each context window, we look for frequent\nargument-pairs mentioned in at least 15 different\narticles with at least 15 different predicates. These\nargument-pairs star in thoroughly-discussed events\nin this context window, so that valid propositions\nabout these argument-pairs can be expected to be\ninferable from existing mentions.\nOn the other hand, in order to battle the noise\nintroduced by the automatic parsers, and include\nonly the propositions with felicitous predicates in\nour dataset, we introduce thresholds for predicates\n916\nto be taken into consideration. Each predicate has\nto be present with at least 30 different argument\npairs for it to be considered felicitous; these can be\nanywhere in the corpus.\nWe select propositions between felicitous predi-\ncates and starring argument-pairs as the population\nof positives.14 Then, we generate adversarial nega-\ntive propositions from positive propositions.\nNegative propositions need to be related to their\ncorresponding positive propositions, but they also\nneed to be false for the current context (i.e. with\nthe current arguments, within the current context\nwindow). In practice, we consider a proposition to\nbe false for the current context when it (and all its\nsynonyms) is absent from the articles in the current\ncontext window with the current arguments. This is\nbased on the assumption that a comprehensive pool\nof news articles would follow the Gricean coopera-\ntive principle of communication (Davis, 2019), that\nit would include all and only the known facts.\nWe generate negatives from WordNet hyponyms:\nhyponyms are naturally related to their correspond-\ning hypernyms; moreover, they are more specific,\nso they are less likely to be true. The effect of this\nproperty is, when a candidate negative proposition\ngenerated from WordNet hyponyms is absent from\nthe current context, it has an even smaller chance\nof actually being correct for the context. Namely,\nwe are in smaller danger of false negatives.\nWhen selecting negatives from WordNet hy-\nponyms, we iterate over all spans-of-tokens in the\nseed positive proposition. We check if each span\nmatches any entries in the WordNet; when a match\nis found, we scan through all its hyponyms, and\nreplace the original span in the proposition with\neach hyponym, in order to create candidate nega-\ntives. For instance, for a positive proposition “A\nplay game with B”, we can replace the span “play”\nwith its hyponym “foul” to create a candidate “A\nfoul game with B”; we can also replace the span\n“game” with its hyponym “practice game” to create\na candidate “A play practice gamewith B”.\nEach candidate negative has to satisfy two crite-\nria to be really considered a negative: it also has\nto be felicitous, and it has to be absent from the\ncurrent context. For felicitousness, we set the same\nthreshold as for positives, that is, mentioned with\nat least 30 different argument pairs anywhere in the\ncorpus. For absence from the current context, we\ncheck that not only the predicate in the candidate\n14Population in the sense of statistical population.\nproposition itself is absent from the current context\nwindow, but all its WordNet synonyms must be\nabsent from the current context window as well.\nAdditionally, for BoOQAEN, where we are\nable to make use of word-sense disambiguation\ntools (Yap et al., 2020), for each WordNet match,\nwe select the synset that best suits the context; for\nBoOQAZH, we select the first synset, since it is\nthe most common synset and has the best chance\nof being suitable for any given context.\nWith the positives selected from felicitous predi-\ncates between starring arguments, and the negatives\ngenerated from the WordNet hyponyms of posi-\ntives, we have two populations of propositions. We\nfinally create balanced samples from these popula-\ntions as the resulting BoOQA dataset. We sample\nno more than two negatives for each positive; we\ndictate that positives and negatives must be sam-\npled in bundles: no positives can be sampled with-\nout their corresponding negatives, and vice versa;\nwe also sample from each context window propor-\ntional to its number of articles.\nFurther, we bring the frequency distributions of\npositives and negatives closer to each other with\nbucket sampling. We collect the ratio of predicates\nin the population of positive propositions falling\ninto each of a pre-defined set of frequency buck-\nets15, and sample the negatives in each frequency\nbucket according to a quota proportional to the ratio\nof the positive population this bucket represents.\nE Analysis and Discussions for BoOQA\nE.1 Details of H-only Model in BoOQA\nRobustness Test\nFor testing the robustness of our BoOQA dataset,\nwe re-use the H-only model in §3.2, with (XLM-\n)RoBERTa-base LMs. Again we use the word\n“true” or its Chinese equivalent “ 正确” to mask\nthe premise slot in the prompts.\nUnlike the LevyHolt H-only setting, in BoOQA,\nthe negative hypotheses are not only not entailed by\n(any) premise, but also false statements in ground\ntruth; on the other hand, the hypotheses in negative\nentries of LevyHolt can have any truthfulness in\nreality. For this reason, if we directly use the hy-\npothesis propositions as training data, the model\nwould be learning not just the artefacts, but also\nidentifying the truthfulness of each statement using\n15The set of bucket frequency boundaries is: (60, 100, 300,\n500, 700, 1000, 1500, 2000, 2500, 3000, 4000, 5000, 6000,\n8000, 10000, 15000, 20000, 30000, 50000, 100000)\n917\nFelicitous Propositions\nPos: Mark Zuckerburg says in Facebook\nNeg: Mark Zuckerburg responds in Facebook\nInfelicitous Propositions\nPos: shown at time for time\nNeg: Police recites suspect\nTable 14: Examples of felicitous / infelicitous proposi-\ntions; “pos” denotes positives, “neg” negatives.\nits memory from pre-training. To get more precise\nreadings of artefact strengths, we mask the argu-\nments in the propositions with their entity types in\nFIGER (Ling and Weld, 2012) ontology. This way,\nwe balance between not letting the model know\nthe truthfulness of each proposition and letting the\nmodel understand the context of each proposition.\nBoOQA comes with only dev / test set. In order\nto get a train set, we follow Schmitt and Schütze\n(2021b) in sub-splitting the dev set into train / dev2\nsets; further, we enforce that these train / dev2 sets\nare of similar sizes to those in LevyHolt.\nIn BoOQA dataset, dev / test sets are sampled\nfrom different time spans, and only used for testing.\nSo it is harmless that a hypothesis appears in both\ndev and test sets, because their truthfulness could\nbe different depending on their different contexts\n(premises). However, for H-only model, since we\nno longer use the premises, we do need to elim-\ninate the overlap between train / dev / test splits.\nWe check for overlaps of hypothesis propositions\nbetween the data splits and assign them randomly\nto one set, and remove them from the other. Since\nthe same hypothesis can bear different truthfulness\nvalues in different context windows, we consider\ntwo propositions as overlaps as long as the propo-\nsitions themselves are identical, regardless of their\nassigned truthfulness value.\nE.2 Case Study\nWe conduct a case study of dataset quality. Since\nwe expect the premises of the entailment relations\nto come from large pools of news articles, there are\nno straight-forward ways of evaluating the label\ncorrectness for each context; however, it is possible\nto evaluate the felicitousness of the hypotheses.\nSimilarly to above, we again mask the actual ar-\nguments with their FIGER (2012) types, so human\njudgements for felicitousness don’t get polluted by\nprior expectations of correctness.\nWe draw a sample of 100 positive and 100 nega-\ntives from both BoOQAEN and BoOQAZH, and\nmanually label their felicitousness. We find the\nratio of felicitous samples to be consistent: for\nEnglish, 86/100 of positive hypotheses are indeed\nfelicitous, 87/100 of negative hypotheses are in-\ndeed felicitous; for Chinese, the numbers are 87 for\npositives and 85 for negatives.\nWe list some examples of felicitous and infelic-\nitous entries in Table 14. We find that the major\nsource of infelicitousness for positive propositions\nis propagated error from relation extraction16; for\nnegative propositions, the major source include the\nretrieved hyponym being semantically reasonable\nbut pragmatically inappropriate, and the retrieved\nhyponym corresponding to an incorrect word sense\nfor the respective positive.\nE.3 How Directionality Is Involved in BoOQA\nIn this section, we explain how BoOQA tests for\ndirectionality. Suppose in BoOQA we have a posi-\ntive proposition Aand its corresponding negative\nproposition B, which has WordNet hyponyms re-\nplaced into one of its spans. At evaluation, we\nretrieve from the context window a third proposi-\ntion C, which we know is true and is under the\nsame context as Aand B. We have two measures\nof entailments, one is directional, the other is sym-\nmetric; both are perfect in their own right.\nA perfect directional measure is contemplated\nas a measure that would assign score 1 to all true\nentailments and 0 to all non-entailments; a per-\nfect symmetric measure would assign score 1 to\nall paraphrastic pairs, 0.5 to all semi-paraphrastic\npairs (directional true entailments or directional\nnon-entailments), and 0 to all unrelated pairs. This\nis an oversimplified hypothetical situation, where\nwe ignore the non-categorical feature of predicate\nentailments; we note that this is only for the benefit\nof theoretical discussions.\nThere are 8 possible scenarios according to the\ngenuine correctness of C ⊨ A, A ⊨ C, B ⊨ C.\nWe only care about these three edge out of the 6,\nbecause: 1) whether Aand Bentail each other is\nunimportant for determining whether Aor B are\ntrue given C; 2) it must be the case the C ⊭ B for\nthe entry to be valid, otherwise the false proposition\nBwould be indorsed by the context.\nThe goal of a method is to rank the positive\nhigher than the negative in each scenario; when\nboth methods rank the positive and the negative\n16since we use automatic open relation extraction methods\nto replace human-annotated predicates, in order for scaling up\nthe number of entailments inspected in our dataset.\n918\na\n b\nc\n d\ne\n f\ng\n h\nFigure 5: Diagram illustration of each of the 8 possible scenarios of ground truth entailment relations between\n(A,C) and (B,C). When two methods show disparity in performance, the winning method is coloured red.\nequally, the one that assigns lower confidence\nscores to them is considered the winner. This is\nbecause, for each proposition, there are multiple\ncontext propositions to refer to. Therefore, when\nevidence from the current context is insufficient for\na judgement, being conservative harms less.\nWe graphically analyse the 8 possible scenar-\nios in Figure 5. In 4 of the 8 scenarios (scenario\na,b,d,h ) , a perfect symmetric measure would do\nequally well as a perfect directional measure; how-\never, in the other 4 scenarios (scenario c,e,f,g ) ,\na perfect directional measure would prevail.\nThis analysis illustrates how directional entail-\nments are involved in our BoOQA evaluation. It\n919\nalso illustrates how we manage to test for a broad\nset of directional entailments by building a dataset\nwith only hyponyms / hypernyms. The relation be-\ntween Aand Bis hyponym / hypernym, however\nthe entailments that we really test for are between\nthe third proposition C and each of A/ B, which\ncan be all kinds of entailments.\nF Computational Infrastructure and\nModel Footprint\nFor our experiments, we use RTX 2080Ti GPUs\nwith 11GB graphic memories. We set the compu-\ntational limit for each job to be 168 GPU hours\n(equivalent to 7 days on 1 GPU). Our experiments\nare not CPU-intensive, but we note that for evalua-\ntion of entailment graphs on BoOQA, ∼120GB of\nmemory is required for speedy processing with the\ncontext windows pre-loaded into memory.\nS&S fine-tuning on full LevyHolt dataset and\nthe symmetric subset typically take around 40 min-\nutes to finish on these GPU; we run the fine-tuning\nwith 100 different hyper-parameters for each ex-\nperiment, so the total duration is approximately\n60 GPU hours; S&S fine-tuning on smaller subset\ntypically take around 15 minutes for each run, and\naround 25 GPU hours for each experiment.\nFor evaluation on the BoOQA dataset, unsuper-\nvised LMs take ∼10 GPU hours; S&S with base\nmodels take ∼40 GPU hours, S&S with large mod-\nels take ∼100 GPU hours. Evaluation of entail-\nment graphs does not require GPU, which typically\ntake 40-50 CPU hours with ∼120GB memory.\nThe S&Smodels have a similar # of parameters\nto their underlying LMs, with only a linear layer\nadded upon them; the rest of methods are either\nout-of-the-box LMs or discrete language resources.\nG Details of BoOQA Baselines\nIn this section, we discuss some implementation\ndetails of BoOQA baselines, report AUCnorm val-\nues for baselines on BoOQA dev sets in Table 15,\nand discuss the presentation choices in Table 5; we\nalso present more baselines on the BoOQA test set.\nFor each method, we allow up to 3200 pieces\nof context evidence for each hypothesis. The only\nexception is S&Sfull for BoOQAZH, where the\nspeed is so slow that we had to cap this threshold\nto 90, hence the asterisk in Table 5.\nLMunsupervised Firstly is the context retrieval\napproaches for unsupervised LM methods. We\nexperimented with three variants:\nDev BoOQAEn BoOQAZh\nLM-base\nBERTtfidf 0.00 12.3\nBERTsent 0.3 9.8\nBERTrel 15.3 31.2\nRoBERTarel 5.0 9.1\nS&SFull 25.7 21.5\nLM-large\nBERTrel 16.9 26.9\nS&SFull 24.9 -\nEntailment Graphs\nEG BInc 29.9 39.6\nEG CNCE 33.6 -\nEG EGT2 25.9 -\nTable 15: Dev set results on English and Chinese\nBoOQA datasets for various baselines, values in % of\nAUCnorm. All methods are out-of-the-box with no\ntuning; for Chinese we do not train S&S model with\nXLM-RoBERTa-large due to computational limits.\nTest BoOQAEn BoOQAZh\nLM-base\nBERTtfidf 0.01 11.5\nBERTsent 0.08 10.8\nBERTrel 15.9 30.6\nRoBERTarel 4.8 8.5\nS&SFull 25.6 23.1\nLM-large\nBERTrel 17.7 26.0\nS&SFull 24.8 -\nTable 16: More test set baseline results on English and\nChinese BoOQA dataset, values in % of AUCnorm. All\nmethods are out-of-the-box with no tuning.\n• BERTtfidf : for each hypothesis, retrieves the\ntop 5 most relevant articles by TF-IDF mea-\nsure following Chen et al. (2017);\n• BERTsent : for each hypothesis, retrieves the\nhost-sentences of all relations involving the\nsame arguments as the queried hypothesis;\n• BERTrel: for each hypothesis, retrieves the\nrelations involving the same arguments as the\nqueried hypothesis (in short sentences);\nAmong them, BERTrel outperforms the other\ntwo, likely because it uses more focused context\ninput of concatenated relations rather than the more\nnoisy raw news sentences.\nBetween BERT and RoBERTa LMs, Although\nRoBERTa is shown (Schmitt and Schütze, 2021b)\nto be good for prompting with LevyHolt, BERT has\n920\nFigure 6: Precision-recall curves of CNCE entailment\ngraphs on BoOQA dev set, w. / w.o. fuzzy matching.\na clear advantage for unsupervised cosine similar-\nity. Between BERT-base and BERT-large, the more\ncomputationally expensive BERT-large model does\nnot show a clear advantage, so we stick to the base\nversion. Therefore, on test set we report BERTrel\nresults with BERT-base asLMunsupervised in Table\n5, and leave the rest to Table 16.\nS&S Models For S&S methods we addition-\nally report baselines trained on full LevyHolt with\nRoBERTa-large. Interestingly S&SFulllarge does\nnot outperform S&SFullbase on BoOQA, there-\nfore in Table 5, we stick with RoBERTa-base.\nEG Here for EG methods, we include a discus-\nsion on fuzzy matching and sparsity. Entailment\ngraphs represent predicates in the form like “(go.1,\ngo.to.2)”, where the subject and object semantic\nroles are indicated by the two parts separated by\na comma. In practice, multiple semantic role as-\nsignments can be given to one predicate in natural\nlanguage form; so with fuzzy matching, we ground\nthe natural language predicates in propositions to\nentailment graph nodes regardless of the semantic\nrole assignments they may be associated to.\nThe fuzzy matching also brings the entailment\ngraph methods to the same page as LM-driven\nmethods: looking only at the natural language\nforms. As shown in Figure 6, without fuzzy match-\ning the recall hit a wall at 64.1%, where sparsity is\nat full display. With fuzzy matching, much more\nentries are assigned some score, bringing the right\nboundary of recall to 91.4%; however, on one hand\nthis harms the precision for top-notch edges, on\nthe other hand the precision still drops rapidly with\nincreasing recall, meaning that beyond the most\nconfident entailment edges, the quality of entail-\nment graph edges deteriorates quickly. That is to\nsay, sparsity still hampers the performance of en-\nMethod AUC50%\nBInc (Szpektor and Dagan, 2008) 54.7\nWeeds (Weeds and Weir, 2003) 55.2\nCNCE (Hosseini et al., 2021) 55.9\nEGT2 (Chen et al., 2022) 63.3\nTable 17: Performance of various entailment graphs on\nthe LevyHolt directional subset, values in %.\ntailment graphs.\nH The Directional Subset with Chen et al.\n(2022)\nContemporary to us, Chen et al. (2022) presented\na new entailment score for building entailment\ngraphs, where they first fine-tune a DeBERTa\nmodel on MNLI, then re-fine-tune it with pairs\nof short propositions composed of (subject, pred-\nicate, object) triples, using PPDB annotations for\nsupervision, in order for domain transfer. They re-\nported results of their EGT2 graph on a version of\ndirectional subset of LevyHolt. However, we have\nfound their directional subset to be different from\nthat first presented by Holt (2019).\nWe re-evaluate the EGT2 graph on the Holt\n(2019) original version of directional subset and\nlist the results in Table 17, which are presented in\nAUC50%. Consistent with the results reported in\nChen et al. (2022), the EGT2 graph still outper-\nforms the rest of the graphs, albeit not by as much.\nThis introduces a slight inconsistency between re-\nsults on the LevyHolt directional subset and the\nBoOQA dataset; we trust the larger BoOQA dataset\nand speculatively argue that the instability of the\nsmall LevyHolt directional test set and some trans-\nferrable spurious correlations in MNLI related to\nthe behavior of human annotators may be to blame.\nOne last remark is, there is a slight difference\nin the calculation of AUC between Hosseini et al.\n(2018)/Chen et al. (2022), and Schmitt and Schütze\n(2021b), in terms of left boundaries of recall, where\nthe calculations by Schmitt and Schütze (2021b)\nyield higher numbers. Since the calculation by\nSchmitt and Schütze (2021b) is compatible with all\nmethods, we use the this calculation throughout.\n921"
}