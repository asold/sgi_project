{
  "title": "mRAT-SQL+GAP:A Portuguese Text-to-SQL Transformer",
  "url": "https://openalex.org/W3217777445",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5021455754",
      "name": "Marcelo Archanjo José",
      "affiliations": [
        "Universidade de São Paulo"
      ]
    },
    {
      "id": "https://openalex.org/A5006532598",
      "name": "Fábio Gagliardi Cozman",
      "affiliations": [
        "Universidade de São Paulo"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3046368065",
    "https://openalex.org/W3013119194",
    "https://openalex.org/W2488700629",
    "https://openalex.org/W2154268919",
    "https://openalex.org/W2929419961",
    "https://openalex.org/W2962713807",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3174726724",
    "https://openalex.org/W2891691255",
    "https://openalex.org/W2250701144",
    "https://openalex.org/W2799042489",
    "https://openalex.org/W2768409085",
    "https://openalex.org/W4233367421",
    "https://openalex.org/W3029828555",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3128171209",
    "https://openalex.org/W3102020135",
    "https://openalex.org/W2243052854",
    "https://openalex.org/W1496189301",
    "https://openalex.org/W2964271186",
    "https://openalex.org/W2269738476",
    "https://openalex.org/W4287694131",
    "https://openalex.org/W4289494028",
    "https://openalex.org/W2091671846",
    "https://openalex.org/W2536574992",
    "https://openalex.org/W3034835156",
    "https://openalex.org/W2111742432",
    "https://openalex.org/W3102480827",
    "https://openalex.org/W2795622940",
    "https://openalex.org/W2077302143",
    "https://openalex.org/W4287550997",
    "https://openalex.org/W3175488485",
    "https://openalex.org/W3046744391",
    "https://openalex.org/W3116083993",
    "https://openalex.org/W3103801878",
    "https://openalex.org/W3032766766",
    "https://openalex.org/W2890431379",
    "https://openalex.org/W3013544688",
    "https://openalex.org/W632432350",
    "https://openalex.org/W3086973390",
    "https://openalex.org/W2798753108",
    "https://openalex.org/W2032374895",
    "https://openalex.org/W2893905611",
    "https://openalex.org/W1981419611",
    "https://openalex.org/W2945102109",
    "https://openalex.org/W2032299694",
    "https://openalex.org/W4287659415",
    "https://openalex.org/W2163274265",
    "https://openalex.org/W2425879071",
    "https://openalex.org/W2047237057",
    "https://openalex.org/W1469575666",
    "https://openalex.org/W3091229675",
    "https://openalex.org/W1520082916",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2950067395",
    "https://openalex.org/W4289406345",
    "https://openalex.org/W2751448157",
    "https://openalex.org/W3096266342"
  ],
  "abstract": "The translation of natural language questions to SQL queries has attracted\\ngrowing attention, in particular in connection with transformers and similar\\nlanguage models. A large number of techniques are geared towards the English\\nlanguage; in this work, we thus investigated translation to SQL when input\\nquestions are given in the Portuguese language. To do so, we properly adapted\\nstate-of-the-art tools and resources. We changed the RAT-SQL+GAP system by\\nrelying on a multilingual BART model (we report tests with other language\\nmodels), and we produced a translated version of the Spider dataset. Our\\nexperiments expose interesting phenomena that arise when non-English languages\\nare targeted; in particular, it is better to train with original and translated\\ntraining datasets together, even if a single target language is desired. This\\nmultilingual BART model fine-tuned with a double-size training dataset (English\\nand Portuguese) achieved 83% of the baseline, making inferences for the\\nPortuguese test dataset. This investigation can help other researchers to\\nproduce results in Machine Learning in a language different from English. Our\\nmultilingual ready version of RAT-SQL+GAP and the data are available,\\nopen-sourced as mRAT-SQL+GAP at: https://github.com/C4AI/gap-text2sql\\n",
  "full_text": "mRAT-SQL+GAP:\nA Portuguese Text-to-SQL Transformer⋆\nMarcelo Archanjo Jos´ e1[0000−0001−7153−0402] and\nFabio Gagliardi Cozman2[0000−0003−4077−4935]\n1 Center for Artiﬁcial Intelligence (C4AI) and\nInstituto de Estudos Avan¸ cados, Universidade de S˜ ao Paulo, Brazil\nmarcelo.archanjo@usp.br\n2 Escola Polit´ ecnica, Universidade de S˜ ao Paulo and\nCenter for Artiﬁcial Intelligence (C4AI), Brazil\nfgcozman@usp.br\nAbstract. The translation of natural language questions to SQL queries\nhas attracted growing attention, in particular in connection with trans-\nformers and similar language models. A large number of techniques are\ngeared towards the English language; in this work, we thus investigated\ntranslation to SQL when input questions are given in the Portuguese\nlanguage. To do so, we properly adapted state-of-the-art tools and re-\nsources. We changed the RAT-SQL+GAP system by relying on a mul-\ntilingual BART model (we report tests with other language models),\nand we produced a translated version of the Spider dataset. Our ex-\nperiments expose interesting phenomena that arise when non-English\nlanguages are targeted; in particular, it is better to train with origi-\nnal and translated training datasets together, even if a single target\nlanguage is desired. This multilingual BART model ﬁne-tuned with a\ndouble-size training dataset (English and Portuguese) achieved 83% of\nthe baseline, making inferences for the Portuguese test dataset. This\ninvestigation can help other researchers to produce results in Machine\nLearning in a language diﬀerent from English. Our multilingual ready\nversion of RAT-SQL+GAP and the data are available, open-sourced as\nmRAT-SQL+GAP at: https://github.com/C4AI/gap-text2sql.\nKeywords: NL2SQL · Deep Learning · RAT-SQL+GAP· Spider dataset\n· BART · BERTimbau.\n1 Introduction\nA huge number of data is now organized in relational databases and typically\naccessed through SQL (Structured Query Language) queries. The interest in\nautomatically translating questions expressed in natural language to SQL (of-\nten referred to as NL2SQL) has been intense, as one can observe through a\n⋆ Supported by IBM and FAPESP (S˜ ao Paulo Research Foundation).\nBRACIS 2021, LNAI 13074, pp. 511-525, 2021. The ﬁnal authenticated version is\navailable online at https://doi.org/10.1007/978-3-030-91699-2_35\narXiv:2110.03546v2  [cs.CL]  29 Nov 2021\n2 Marcelo Archanjo Jos´ e and Fabio Gagliardi Cozman\nFig. 1.From a natural language question to a SQL query and to the database query\nresult. Database names appear in blue and red, while the primary key appears in green.\nnumber of excellent surveys in the literature [1,2,3]. Fig. 1 depicts the whole\nﬂow from a natural language question to a SQL query result; the SQL query\nrefers to database tables and their columns, using primary and secondary keys\nas appropriate.\nExisting approaches for NL2SQL can be divided into entity-based and ma-\nchine learning ones, the latter dominated by techniques based on deep learn-\ning [3].\nEntity-based approaches focus on the interpretation of input text based on\nrules so as to translate it to a SQL query. The translation often goes ﬁrst to an\nintermediary state and later to a ﬁnal SQL query. Relevant systems are Bela [4],\nSODA [5], NaLIR [6,7,8], TR Discover [9], Athena [10,11] , Athena++ [12] and\nDuoquest [13,14] .\nMachine learning approaches are based on supervised learning, in which train-\ning data contains natural language questions and paired SQL queries [3]. Several\narchitectures can be trained or ﬁne-tuned so as to run the translation. Relevant\nsystems are EchoQuery [15], Seq2SQL [41], SQLNet [16], DialSQL [17], Type-\nSQL [18], SyntaxSQLNet [19], AugmentedBI [21], IRNet [22], RAT-SQL [23],\nRAT-SQL+GAP [24], GraPPa [25], BRIDGE [26], DBPal [27,28,29], HydraNet [30],\nDT-Fixup [32] and LGESQL [31].\nThere are also hybrid approaches that combine entity-based and machine\nlearning [3]; relevant examples are Aqqu [34], MEANS [35] and QUEST [33].\nmRAT-SQL+GAP: A Portuguese Text-to-SQL Transformer 3\nTable 1.NL2SQL Benchmarks.\nDataset Questions SQL Queries Databases Domains Tables/DB References\nATIS 5,280 947 1 1 32 [36,42,43,44,45]\nGeoQuery 877 247 1 1 6 [40,39,36,37,38]\nWikiSQL 80,654 77,840 26,521 * 1 [41]\nSpider 10,181 5,693 200 138 5.1 [20]\n* The WikiSQL has multiple domains, but the organization of one table per\ndatabase does not allow exploring the complexity of the diﬀerent domains.\nThe previous paragraphs contain long lists of references that should suﬃce\nto demonstrate that translation from natural language to SQL is a well explored\nresearch topic. The ﬁeld is relatively mature and benchmarks for NL2SQL are\nnow widely used, containing training and testing data and ways to evaluate new\nproposals. Table 1 shows a few important datasets in the literature, reporting\ntheir number of questions, number of SQL queries, number of databases, number\nof domain and tables per database [20]. Particularly, the Spider dataset is a\npopular resource that contains 200 databases with multiple tables under 138\ndomains.3 The complexity of these tables allows testing complex nested SQL\nqueries. A solid test suite evaluation package for testing against Spider [46] is\navailable;4 in addition, there is a very active leaderboard rank for tests that use\nSpider.5\nCurrently, the best result in the Spider leaderboard for Exact set match\nwithout values , whereby a paper and code are available, is the entry by RAT-\nSQL+GAP [24].6 This system appears in the 6th rank position with Dev 0.718\nand Test 0.697.7 Note that the Spider leaderboard, as of August 2021, displays\nin the 1st rank position LGESQL [31] with Dev 0.751 and Test 0.720 for Exact\nset match without values . Thus RAT-SQL+GAP is arguably at the state-of-art\nin NL2SQL.\nRAT-SQL+GAP is based on the RAT-SQL package (Relation-Aware Trans-\nformer SQL) [23]. RAT-SQL was proposed in 2019 as a text to SQL parser based\non the BERT language model [47]. Package RAT-SQL version 3 with BERT\nis currently the 14th entry in the Spider leaderboard rank. RAT-SQL+GAP\nadds Generation-Augmented Pre-training (GAP) to RAT-SQL. GAP produces\nsynthetic data to increase the dataset size to improve pre-training; the whole\ngenerative models are trained by ﬁne-tuning a BART [48] large model.\n3 Spider dataset: https://yale-lily.github.io/spider.\n4 Spider test suite evaluation github:https://github.com/taoyds/test-suite-sql-\neval.\n5 Spider leaderboard rank: https://yale-lily.github.io/spider.\n6 RAT-SQL+GAP gitHub: https://github.com/awslabs/gap-text2sql.\n7 Dev results are obtained locally by the developer; to get oﬃcial score and Test\nresults, it is necessary to submit the model following guidelines in “Yale Semantic\nParsing and Text-to-SQL Challenge (Spider) 1.0 Submission Guideline” athttps://\nworksheets.codalab.org/worksheets/0x82150f426cb94c17b861ef4162817399/.\n4 Marcelo Archanjo Jos´ e and Fabio Gagliardi Cozman\nDespite the substantial number of techniques, systems and benchmarks for\nNL2SQL, most of them focus on the English language. Very few results can be\nfound for input questions in the Portuguese language, for example. The study by\nSilva et al. [51] presents an architecture for NL2SQL in which natural language\nquestions in Portuguese are translated to the English language on arrival, and\nare then then shipped to NL2SQL existing packages.\nThe goal of this paper is simple to state: we present a translator for queries\nin Portuguese natural language into SQL. We intend to study the eﬀect of re-\nplacing the questions in the Spider dataset with translated versions, and also to\ninvestigate how to adapt the RAT-SQL+GAP system to the needs of a diﬀerent\nlanguage. Using a new version of Spider with RAT-SQL+GAP to train models,\nwe produce inferences and compare results so as to understand the diﬃculties\nand limitations of various ideas.\nWhat we found is that, by focusing on Portuguese, we actually produced\nmethods and results that apply to any multilingual NL2SQL task. An impor-\ntant insight (and possibly the main contribution of this paper) is dealing with a\nnon-English language, such as Portuguese,we greatly beneﬁt from taking a multi-\nlingual approach that puts together English and the other language — in our case,\nEnglish and Portuguese. We later stress this idea when we discuss our experi-\nments. We thus refer to our “multilingual-ready” version of RAT-SQL+GAP as\nmRAT-SQL+GAP; all code and relevant data related to this system are freely\navailable8.\n2 Preliminary Tasks\nThe adaptation to language other than English demands at least the translation\nof the dataset and changing the code to read and write ﬁles UTF-8 encoding.\n2.1 Translating the Spider Dataset\nThe translation to the Portuguese language in the NL2SQL task evolves the\nnatural language part, the questions. The SQL queries must remain the same\nto make sense. To translate the questions, it is important to extract them\nfrom speciﬁc .json ﬁles. The Spider dataset has three ﬁles that contain input\nquestions and their corresponding SQL queries: dev.json, train others.json, and\ntrain spider.json. We extracted the questions and translated them using the\nGoogle Cloud Translation API.9. Table 2 presents the number of questions and\nnumber of characters per ﬁle (just for the questions). The code that reads the\noriginal ﬁles and that generates translated versions relies on the UTF-8 encod-\ning so as to accept Portuguese characters; several ﬁles were generated in the\nprocess (.txt just for the translated questions, .csv for the SQL queries and\noriginal/translated questions, .json for the translated questions).\n8 mRAT-SQL+GAP Github: https://github.com/C4AI/gap-text2sql\n9 Cloud Translation API: https://googleapis.dev/python/translation/latest/\nindex.html.\nmRAT-SQL+GAP: A Portuguese Text-to-SQL Transformer 5\nTable 2.Number of questions and characters per ﬁle.\nFile Number of questions Number of characters\ndev.json 1,034 70,362\ntrain others.json 1,659 80,571\ntrain spider.json 7,000 496.054\nWe then conducted a revision process by going through the text ﬁle and look-\ning for questions in the csv ﬁle (together with the corresponding SQL queries).\nAfter the revision, a new .json ﬁle was generated with these translated and re-\nvised questions. Table 3 shows four examples of translated questions.\nTable 3.Translation examples.\nEnglish Question Portuguese Question\nHow many singers do we have? Quantos cantores n´ os temos?\nFind the number of pets for each student\nwho has any pet and student id.\nEncontre o n´ umero de animais de es-\ntima¸ c˜ ao para cada aluno que possui algum\nanimal de estima¸ c˜ ao e a identiﬁca¸ c˜ ao do\naluno.\nHow many United Airlines ﬂights go to\nCity ’Aberdeen’?\nQuantos voos da United Airlines v˜ ao para\na cidade de ’Aberdeen’?\nWhat is the name of the shop that is hiring\nthe largest number of employees?\nQual ´ e o nome da loja que est´ a contratando\no maior n´ umero de funcion´ arios?\nAdapting the RAT-SQL+GAP System We had to change the RAT-SQL+\nGAP code to allow multilingual processing. For instance, the original Python\ncode is not prepared to handle UTF-8 ﬁles; thus, we had to modify the occur-\nrences of “open” and “json.dump” commands, together with a few other changes.\nWe ran a RAT-SQL+GAP test and checked whether all the characters employed\nin Portuguese were preserved. We also noticed lemmatization errors in prepro-\ncessed ﬁles. As the original code for RAT-SQL+GAP relies on the Stanford\nCoreNLP lemmatization tool that currently does not support Portuguese, it was\nreplaced by Simplemma. 10 The latter package supports multilingual texts, and\nparticularly supports Portuguese and English.\nTraining The original language model at the heart of RAT-SQL+GAP is\nBART-large11 [48], a language model pretrained for the English language. We\nhad to change that model to another one that was pretrained for the Por-\n10 Simplemma: a simple multilingual lemmatizer for Python at https://github.com/\nadbar/simplemma.\n11 Facebook BART-large: https://huggingface.co/facebook/bart-large.\n6 Marcelo Archanjo Jos´ e and Fabio Gagliardi Cozman\ntuguese language. A sensible option was to work with a multilingual Sequence-\nto-Sequence BART model; the choice was mBART-50 12[50] because it covers\nPortuguese and English languages (amongst many others). Another language\nmodel we investigated was the BERTimbau-base13[49], as RAT-SQL works with\nBERT; the move to BERTtimbau, a Portuguese-based version of BERT, seemed\npromising.\nDataset A total of 8,659 questions were used for training (7,000 questions in\ntrain spider.json and 1,659 questions in train others.json). The 1,034 questions\nin dev.json were used for testing. We later refer to three scenarios:\n– English train and test: questions are just in English for training and testing.\n– Portuguese train and test 14: questions are just in Portuguese for training\nand testing.\n– English and Portuguese (double-size) train and test 14: questions are in En-\nglish and Portuguese for training and testing (in this case, we thus have\ntwice as much data as in each of the two previous individual scenarios).\nEvaluation Metrics The main evaluation metric with respect to Spider is\nExact Set Match (ESM). We here present results for Exact Set Match (ESM)\nwithout values, as most results in the Spider leaderboard currently adopt this\nmetric. Some Spider metrics are also used to classify the SQL queries into 4\nlevels: easy, medium, hard and extra hard. Table 4 shows an example for each\nlevel; these queries correspond to the four questions in Table 3 in the same order.\nTo evaluate the results, we used the Spider test suite evaluation [46]. An aside:\nthe suite must receive a text ﬁle with the SQL query generated and another one\nwith the gold-standard SQL query. These ﬁles obviously do not change when we\nmove from English to any other input language.\nAs a digression, note that it is possible to plug values during evaluation. A\nquery with a value looks like this:\nSELECT Count(*) FROM airlines JOIN airports WHERE airports.City\n= “Abilene”\nA query without values has the word “terminal” instead of the value:\nSELECT Count(*) FROM airlines JOIN airports WHERE airports.City\n= “terminal”\n12 Facebook mBART-50 many for diﬀerent multilingual machine translations: https:\n//huggingface.co/facebook/mbart-large-50-many-to-many-mmt\n13 BERTimbau-base:https://huggingface.co/neuralmind/bert-base-portuguese-\ncased\n14 Spider dataset translated to Portuguese and double-size (English and Portuguese\ntogether): https://github.com/C4AI/gap-text2sql\nmRAT-SQL+GAP: A Portuguese Text-to-SQL Transformer 7\nTable 4.SQL query levels: easy/medium/hard/extra.\nLevel SQL Query\nEasy SELECT count(*) FROM singer\nMedium SELECT count(*), T1.stuid FROM student AS T1 JOIN has pet AS T2 ON\nT1.stuid=T2.stuid GROUP BY T1.stuid\nHard SELECT count(*) FROM FLIGHTS AS T1 JOIN AIRPORTS AS T2 ON\nT1.DestAirport = T2.AirportCode JOIN AIRLINES AS T3 ON T3.uid =\nT1.Airline WHERE T2.City = ”Aberdeen” AND T3.Airline = ”United Air-\nlines”\nExtra SELECT t2.name FROM hiring AS t1 JOIN shop AS t2 ON t1.shop id =\nt2.shop id GROUP BY t1.shop id ORDER BY count(*) DESC LIMIT 1\n3 Experiments\nExperiments were run in a machine with AMD Ryzen 9 3950X 16-Core Pro-\ncessor, 64GB RAM, 2 GPUs NVidia GeForce RTX 3090 24GB running Ubuntu\n20.04.2 LTS. Fig. 2 shows the architecture of the training, inference and evalu-\nation processes described in this section.\nResults can be found in Table 5. This table shows the results of Exact Set\nMatch without Values for RAT-SQL+GAP trained locally for all models. We\nhave 3 datasets, 5 trained model checkpoints and 7 distinct relevant results.\nThe ﬁrst line corresponds to the original model BART and original questions\nin English. Note that the result in line #1 achieved the same performance re-\nported by [24] for Exact Set Match without values in Spider: 0.718 (All) for Dev.\nThis indicates that our testing apparatus can produce state-of-the-art results.\nMoreover, line #1 shows a well-tuned model in English that can be attained.\nOur ﬁrst experiment was to change the questions to Portuguese and the\nmodel to BERTimbau-base which is pretrained in Portuguese. In Table 5 the\nresult in line #2 for BERTimbau 0,417(All) is quite low when compared to the\nresult in line #1. This happens for many reasons. The model is BERT and the\nbest result uses BART. Another important diﬀerence is that the Portuguese con-\ntent used in ﬁne-tuning has a mixture of Portuguese and English words because\nthe SQL query inevitably consists of English keywords, see examples of SQL\nqueries in Table 4. In fact, some questions demand untranslated words to make\nsense, for example ”Boston Red Stockings” in the translated question: Qual ´ e o\nsal´ ario m´ edio dos jogadores da equipe chamada ”Boston Red Stockings”?. This\nsuggested that a multilingual approach might be more successful. The mBART-\n50 language model was then tested within the whole architecture.\nmBART-50 was in fact ﬁne-tuned in three diﬀerent ways: with questions\nonly in English, only in Portuguese, and with questions both in English and in\nPortuguese (that is, a dataset with questions and their translations). Inferences\nwere run with English test questions and Portuguese test questions, while the\nmodel was ﬁne-tuned with the corresponding training questions language. For\nthe mBART-50 model ﬁne-tuned with the train dataset with two languages,\n8 Marcelo Archanjo Jos´ e and Fabio Gagliardi Cozman\nFig. 2.Architecture of the training, inference, evaluation. Results related to Table 5:\neach line in that table appears here as a square at the bottom of the ﬁgure.\nthree inferences were made: only English, only Portuguese, and the combined\nEnglish Portuguese test dataset.\nmBART-50 ﬁne-tuned with questions in English in line #3 achieved 0.651\n(All) when tested with questions in English. The same model mBART-50 ﬁne-\ntuned with questions in Portuguese in line #4 achieved 0.588(All) when tested\nwith questions in Portuguese.\nA multilingual model such as mBART-50 can be trained with the two lan-\nguages at the same time. This is certainly appropriate for data augmentation\nand to produce a ﬁne-tuning process that can better generalize. The results in\nTable 5 lines #5, #6 and #7 were obtained with mBART-50 ﬁne-tuned with\nthe double-size training dataset (English and Portuguese); the three inferences\nwere made using the same model checkpoint. The test datasets were in English\nfor line #5, in Portuguese for line #6, and the double-size test dataset in En-\nglish and Portuguese for line #7. The results demonstrate improvements, if we\ncompare inferences with English test dataset lines #3 and #5. Results went up\nmRAT-SQL+GAP: A Portuguese Text-to-SQL Transformer 9\nTable 5.Results.\nExact Set Match without Values\n# Model Train Infer Easy Medium Hard Extra All\n248 446 174 166 1034\n1 BART En En 0.899 0.744 0.667 0.428 0.718\n2 BERTimbau Pt Pt 0.560 0.422 0.333 0.277 0.417\n3 mBART-50 En En 0.851 0.679 0.546 0.386 0.651\n4 mBART-50 Pt Pt 0.762 0.599 0.529 0.361 0.588\n5 mBART-50 En/Pt En 0.863 0.682 0.569 0.422 0.664\n6 mBART-50 En/Pt Pt 0.827 0.596 0.511 0.331 0.595\nModel Train Infer Easy Medium Hard Extra All\n496 892 348 332 2068\n7 mBART-50 En/Pt En/Pt 0.847 0.639 0.537 0.380 0.630\nfrom 0.651 (All) to 0.664 (All), better for all levels of questions. If we compare\ninferences with the Portuguese test dataset in lines #4 and #6, the results went\nfrom 0.588 (All) to 0.595 (All). However, they are better just for an easy level\nof questions; this was enough to inﬂuence the overall results for line #6.\nThe inference made with the double-size test dataset in English and Por-\ntuguese, in line #7, cannot be compared with the other inferences because they\nused just one language. Nevertheless, the model mBART-50 trained with En-\nglish and Portuguese (double-size training dataset) produced good results with\nthis rather uncommon testing dataset.\nAll the detailed results presented in this paper are openly available 15.\n4 Analysis and Discussion\nThese experiments indicate that multilingual pretrained transformers can be\nextremely useful when dealing with languages other than English. There is al-\nways the need to integrate English processing with the additional languages of\ninterest, in our case, with Portuguese.\nOverall, questions in the English language have a closer similarity with SQL\nqueries, thus simplifying inferences. Conversely, questions in Portuguese require\nfurther work. Fig. 3 show a real example of correct predictions in English (Ta-\nble 5 line #5) and in Portuguese (Table 5 line #6). In Fig. 3, words such as\nsong, names, singers, average in the English question are keywords needed\nto resolve the query, and they are very close to the target word in the query. In\nPortuguese, the same keywords are m´ usicas, nomes, cantores, m´ ediathat\nmust respectively match song, name, singer, and avg. This introduced an addi-\ntional level of diﬃculty that explains the slightly worse results for inference with\nquestions in Portuguese: 0.595 (Table 5 line #6). This is to be compared with\nquestions in English: 0.664 (Table 5 line #5). In any case, it is surprising that the\nsame model checkpoint resolved the translation with such diﬀerent questions.\n15 mRAT-SQL+GAP Github: https://github.com/C4AI/gap-text2sql\n10 Marcelo Archanjo Jos´ e and Fabio Gagliardi Cozman\nFig. 3.Examples of keywords in the prediction of the SQL query in English and Por-\ntuguese languages. Top pair: question in English and corresponding SQL query pre-\ndicted from it. Bottom pair: question in Portuguese and corresponding SQL query\npredicted from it.\nSome translations actually keep a mix of languages, because some words rep-\nresent the value and cannot be translated. Fig. 4 shows an example. There the\nshow name ”The Rise of the Blue Beetle” should not be translated to main-\ntain the overall meaning of the question; these words in English must be part\nof the Portuguese question. This successful SQL query inference was produced\nwith mBART-50 ﬁne-tuned with the English and Portuguese training dataset\n(Table 5 line #6) and mBART-50 ﬁne-tuned in the Portuguese-only training\ndataset (Table 5 line #4). The show name was then replaced with “terminal”\nduring the RAT-SQL+GAP prediction process, as it is processed through the\nSpider Exact Set Match without Values evaluation. In any case, the show\nname is part of the input and will introduce diﬃculties in the prediction.\nIn addition, for real-world databases, it is a practice, at least in Brazil, to\nname tables and columns with English words even for databases with content in\nPortuguese. This practical matter is another argument in favor of a multilingual\napproach.\nFig. 5 shows a sample of failed translations evaluated by the Spider Exact Set\nMatch for inferences using mBART-50 (Table 5 line #6). It is actually diﬃcult\nto ﬁnd the errors without knowing the database schema related to every query.\nThe objective of this ﬁgure is to show that even when the query is incorrect, it\nis not composed of random or nonsensical words. Our manual analysis indicates\nthat this is true for queries failing with all other models.\n5 Conclusion and Future Work\nIn sum, we have explored the possible ways to create a translator that takes ques-\ntions in the Portuguese language and outputs correct SQL queries corresponding\nto the questions. By adapting a state-of-the-art NL2SQL to the Portuguese lan-\nmRAT-SQL+GAP: A Portuguese Text-to-SQL Transformer 11\nFig. 4.Example of words that represent the value in the prediction of the SQL query.\nEn Q: English question, Pt Q: Question translated to Portuguese, P: SQL query pre-\ndicted, and G: Gold SQL query.\nguage, our main conclusion is that a multilingual approach is needed: it is not\nenough to do everything in Portuguese; rather, we must simultaneously work\nwith English and Portuguese.\nIn Table 5, our best result is in line #5 0.664 (All) whereby we test with\nquestions in English (original test set from Spider) using mBART-50 model\nﬁne-tuned with a double-size training dataset (English and Portuguese). This\nyields 92% of the English-only performance of 0.718 (All) in line #1. Testing\nwith questions in Portuguese (using our translation) with the same BART-50\nmodel ﬁne-tuned with a double-size training dataset (English and Portuguese),\nwe achieve instead 0.595 (All) line #6. Now this is 83% of the English-only\nperformance of 0.718 (All). These results should work as a baseline for future\nNL2SQL research in Portuguese.\nOur multilingual RAT-SQL+GAP, or mRAT-SQL+GAP for short, the trans-\nlated datasets, the trained checkpoint, and the results; are open-source avail-\nable16.\nFuture work should try other multilingual transformers (and possibly other\nseq-to-seq models), always seeking ways to use English and Portuguese together.\nAnother possible future work is to ﬁne-tune BERTimbau-large 17 [49] so as to\nbetter understand the eﬀect of the size of the language model. Lastly, a transla-\ntion of the Spider dataset to other languages so as to work with several languages\nat the same time should produce valuable insights.\n16 mRAT-SQL+GAP Github: https://github.com/C4AI/gap-text2sql\n17 BERTimbau-large: https://huggingface.co/neuralmind/bert-large-\nportuguese-cased\n12 Marcelo Archanjo Jos´ e and Fabio Gagliardi Cozman\nFig. 5.Sample of failed queries evaluated by Spider Exact Set Match for inferences\nfrom mBART-50 (Table 5 line #6) . Q: question, P: SQL query predicted, and G: Gold\nSQL query.\n6 Acknowledgments\nThis work was carried out at the Center for Artiﬁcial Intelligence (C4AI-USP),\nsupported by the S˜ ao Paulo Research Foundation (FAPESP grant #2019/07665-\n4) and by the IBM Corporation. The second author is partially supported by the\nConselho Nacional de Desenvolvimento Cient´ ıﬁco e Tecnol´ ogico (CNPq), grant\n312180/2018-7.\nReferences\n1. Kim, H., So, B.H., Han, W.S., Lee, H.: Natural language to\nSQL: Where are we today? Proc. VLDB Endow. 13, 1737–1750\n(2020).https://doi.org/10.14778/3401960.3401970\n2. Aﬀolter, K., Stockinger, K., Bernstein, A.: A comparative survey of re-\ncent natural language interfaces for databases. VLDB J. 28, 793–819\n(2019).https://doi.org/10.1007/s00778-019-00567-8\n3. Ozcan, F., Quamar, A., Sen, J., Lei, C., Efthymiou, V.: State of the Art and Open\nChallenges in Natural Language Interfaces to Data. Proc. ACM SIGMOD Int. Conf.\nManag. Data. 2629–2636 (2020). https://doi.org/10.1145/3318464.3383128\n4. Walter, S., Unger, C., Cimiano, P., B¨ ar, D.: Evaluation of a layered approach to\nquestion answering over linked data. Lect. Notes Comput. Sci. (including Subser.\nmRAT-SQL+GAP: A Portuguese Text-to-SQL Transformer 13\nLect. Notes Artif. Intell. Lect. Notes Bioinformatics). 7650 LNCS, 362–374 (2012).\nhttps://doi.org/10.1007/978-3-642-35173-0-25\n5. Blunschi, L., Jossen, C., Kossmann, D., Mori, M., Stockinger, K.: SODA:\nGenerating SQL for business users. Proc. VLDB Endow. 5, 932–943 (2012).\nhttps://doi.org/10.14778/2336664.2336667\n6. Li, F., Jagadish, H. V: Constructing an interactive natural language in-\nterface for relational databases. Proc. VLDB Endow. 8, 73–84 (2014).\nhttps://doi.org/10.14778/2735461.2735468\n7. Li, F., Jagadish, H. V.: NaLIR: An Interactive Natural Language Interface for\nQuerying Relational Databases. In: Proceedings of the 2014 ACM SIGMOD In-\nternational Conference on Management of Data. pp. 709–712. ACM, New York,\nNY, USA (2014). https://doi.org/10.1145/2588555.2594519\n8. Li, F., Jagadish, H. V.: Understanding Natural Language Queries\nover Relational Databases. ACM SIGMOD Rec. 45, 6–13 (2016).\nhttps://doi.org/10.1145/2949741.2949744\n9. Song, D., Schilder, F., Smiley, C., Brew, C., Zielund, T., Bretz, H., Martin, R., Dale,\nC., Duprey, J., Miller, T., Harrison, J.: TR discover: A natural language interface\nfor querying and analyzing interlinked datasets. Lect. Notes Comput. Sci. (including\nSubser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics). 9367, 21–37 (2015).\nhttps://doi.org/10.1007/978-3-319-25010-6 2\n10. Saha, D., Floratou, A., Sankaranarayanan, K., Minhas, U.F., Mittal, A.R.,\n¨Ozcan, F.: ATHENA: An Ontology-Driven System for Natural Language Query-\ning over Relational Data Stores. Proc. VLDB Endow. 9, 1209–1220 (2016).\nhttps://doi.org/10.14778/2994509.2994536\n11. Lei, C., ¨Ozcan, F., Quamar, A., Mittal, A.R., Sen, J., Saha, D., Sankaranarayanan,\nK.: Ontology-Based Natural Language Query Interfaces for Data Exploration. IEEE\nData Eng. Bull. 41, 52–63 (2018).\n12. Sen, J., Lei, C., Quamar, A., ¨Ozcan, F., Efthymiou, V., Dalmia, A., Stager, G.,\nMittal, A., Saha, D., Sankaranarayanan, K.: ATHENA++: natural language query-\ning for complex nested SQL queries. Proc. VLDB Endow. 13, 2747–2759 (2020).\nhttps://doi.org/10.14778/3407790.3407858\n13. Baik, C., Arbor, A., Arbor, A., Arbor, A., Jagadish, H. V: Constructing Expressive\nRelational Queries with Dual-Speciﬁcation Synthesis. 10th Annu. Conf. Innov. Data\nSyst. Res. (CIDR ‘20). (2020).\n14. Baik, C., Jin, Z., Cafarella, M., Jagadish, H. V.: Duoquest: A Dual-Speciﬁcation\nSystem for Expressive SQL Queries. Proc. ACM SIGMOD Int. Conf. Manag. Data.\n2319–2329 (2020). https://doi.org/10.1145/3318464.3389776\n15. Lyons, G., Tran, V., Binnig, C., Cetintemel, U., Kraska, T.: Making the case for\nquery-by-voice with echoquery. Proc. ACM SIGMOD Int. Conf. Manag. Data. 26-\nJune-20, 2129–2132 (2016). https://doi.org/10.1145/2882903.2899394\n16. Xu, X., Liu, C., Song, D.: SQLNet: Generating structured queries from natural lan-\nguage without reinforcement learning. https://arxiv.org/abs/1711.04436. 1–13\n(2017).\n17. Gur, I., Yavuz, S., Su, Y., Yan, X.: DialSQL: Dialogue based structured query\ngeneration. ACL 2018 - 56th Annu. Meet. Assoc. Comput. Linguist. Proc. Conf.\n(Long Pap. 1, 1339–1349 (2018). https://doi.org/10.18653/v1/p18-1124\n18. Yu, T., Li, Z., Zhang, Z., Zhang, R., Radev, D.: TypeSQL: Knowledge-based type-\naware neural text-to-SQL generation. NAACL HLT 2018 - 2018 Conf. North Am.\nChapter Assoc. Comput. Linguist. Hum. Lang. Technol. - Proc. Conf. 2, 588–594\n(2018). https://doi.org/10.18653/v1/n18-2093\n14 Marcelo Archanjo Jos´ e and Fabio Gagliardi Cozman\n19. Yu, T., Yasunaga, M., Yang, K., Zhang, R., Wang, D., Li, Z., Radev, D.R.: Syn-\ntaxSQLNet: Syntax tree networks for complex and cross-domain text-to-SQL task.\nIn: Proceedings ofthe 2018 Conference on Empirical Methods in Natural Language\nProcessing. pp. 1653–1663. Association for Computational Linguistics, Brussels, Bel-\ngium (2018).\n20. Yu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z., Ma, J., Li, I., Yao, Q.,\nRoman, S., Zhang, Z., Radev, D.: Spider: A Large-Scale Human-Labeled Dataset\nfor Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task. https:\n//arxiv.org/abs/1809.08887v5. (2018).\n21. Francia, M., Golfarelli, M., Rizzi, S.: Augmented business intelligence. CEUR\nWorkshop Proc. 2324, (2019).\n22. Guo, J., Zhan, Z., Gao, Y., Xiao, Y., Lou, J.-G., Liu, T., Zhang, D.: Towards\nComplex Text-to-SQL in Cross-Domain Database with Intermediate Representa-\ntion. (2019).\n23. Wang, B., Shin, R., Liu, X., Polozov, O., Richardson, M.: RAT-SQL:\nRelation-aware schema encoding and linking for text-to-SQL parsers. (2019).\nhttps://doi.org/10.18653/v1/2020.acl-main.677\n24. Shi, P., Ng, P., Wang, Z., Zhu, H., Li, A.H., Wang, J., Santos, C.N. dos, Xiang,\nB.: Learning Contextual Representations for Semantic Parsing with Generation-\nAugmented Pre-Training. (2020).\n25. Yu, T., Wu, C., Lin, X.V., Wang, B., Tan, Y.C., Yang, X., Radev, D., Socher, R.,\nXiong, C.: GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing.\n(2020).\n26. Lin, X.V., Socher, R., Xiong, C.: Bridging Textual and Tabular Data\nfor Cross-Domain Text-to-SQL Semantic Parsing. 4870–4888 (2020).\nhttps://doi.org/10.18653/v1/2020.ﬁndings-emnlp.438\n27. Utama, P., Weir, N., Basik, F., Binnig, C., Cetintemel, U., H¨ attasch, B., Ilkhechi,\nA., Ramaswamy, S., Usta, A.: DBPal: An End-to-end Neural Natural Language\nInterface for Databases. (2018).\n28. Basik, F., H¨ attasch, B., Ilkhechi, A., Usta, A., Ramaswamy, S., Utama,\nP., Weir, N., Binnig, C., Cetintemel, U.: DBPal: A learned NL-interface for\ndatabases. Proc. ACM SIGMOD Int. Conf. Manag. Data. 1765–1768 (2018).\nhttps://doi.org/10.1145/3183713.3193562\n29. Weir, N., Utama, P., Galakatos, A., Crotty, A., Ilkhechi, A., Ramaswamy, S.,\nBhushan, R., Geisler, N., H¨ attasch, B., Eger, S., Cetintemel, U., Binnig, C.: DBPal:\nA Fully Pluggable NL2SQL Training Pipeline. In: Proceedings of the 2020 ACM\nSIGMOD International Conference on Management of Data. pp. 2347–2361. ACM,\nNew York, NY, USA (2020).https://doi.org/doi.org/10.1145/3318464.3380589\n30. Lyu, Q., Chakrabarti, K., Hathi, S., Kundu, S., Zhang, J., Chen, Z.: Hybrid ranking\nnetwork for text-to-SQL. https://arxiv.org/abs/2008.04759 1–12 (2020).\n31. Cao, R., Chen, L., Chen, Z., Zhao, Y., Zhu, S., Yu, K.: LGESQL: Line Graph En-\nhanced Text-to-SQL Model with Mixed Local and Non-Local Relations. 2541–2555\n(2021). https://doi.org/doi.org/10.18653/v1/2021.acl-long.198\n32. Xu, P., Kumar, D., Yang, W., Zi, W., Tang, K., Huang, C., Cheung, J.C.K., Prince,\nS.J.D., Cao, Y.: Optimizing Deeper Transformers on Small Datasets. 2089–2102\n(2021). https://doi.org/doi.org/10.18653/v1/2021.acl-long.163\n33. Bergamaschi, S., Guerra, F., Interlandi, M., Trillo-Lado, R., Velegrakis, Y.: Com-\nbining user and database perspective for solving keyword queries over relational\ndatabases. Inf. Syst. 55, 1–19 (2016).https://doi.org/10.1016/j.is.2015.07.005\nmRAT-SQL+GAP: A Portuguese Text-to-SQL Transformer 15\n34. Bast, H., Haussmann, E.: More accurate question answering on free-\nbase. Int. Conf. Inf. Knowl. Manag. Proc. 19-23-Oct-2015, 1431–1440\n(2015).https://doi.org/10.1145/2806416.2806472\n35. Ben Abacha, A., Zweigenbaum, P.: MEANS: A medical question-answering system\ncombining NLP techniques and semantic Web technologies. Inf. Process. Manag. 51,\n570–594 (2015). https://doi.org/10.1016/j.ipm.2015.04.006\n36. Iyer, S., Konstas, I., Cheung, A., Krishnamurthy, J., Zettlemoyer, L.:\nLearning a neural semantic parser from user feedback. ACL 2017 - 55th\nAnnu. Meet. Assoc. Comput. Linguist. Proc. Conf. (Long Pap. 1, 963–973\n(2017).https://doi.org/10.18653/v1/P17-1089\n37. Giordani, A., Moschitti, A.: Translating Questions to SQL Queries with Generative\nParsers Discriminatively Reranked. Coling. 401–410 (2012).\n38. Popescu, A.M., Etzioni, O., Kautz, H.: Towards a theory of natural language in-\nterfaces to databases. Int. Conf. Intell. User Interfaces, Proc. IUI. 149–157 (2003).\nhttps://doi.org/10.1145/604050.604070.\n39. Zelle, J.M., Mooney, R.J.: Learning t o P arse Database Queries Using I n ductive\nLogic Programming. Natl. Conf. Artif. Intell. 1050–1055 (1996).\n40. Zettlemoyer, L.S., Michael, C.: Learning to map sentences to logical form: Struc-\ntured classiﬁcation with probabilistic categorial grammars. Proc. 21st Conf. Uncer-\ntain. Artif. Intell. UAI 2005. 658–666 (2005).\n41. Zhong, V., Xiong, C., Socher, R.: Seq2Sql: Generating Structured Queries From\nNatural Language Using Reinforcement Learning. https://arxiv.org/abs/1709.\n00103v7. 1–12 (2017).\n42. Zettlemoyer, L.S., Collins, M.: Online learning of relaxed CCG grammars for pars-\ning to logical form. EMNLP-CoNLL 2007 - Proc. 2007 Jt. Conf. Empir. Methods\nNat. Lang. Process. Comput. Nat. Lang. Learn. 678–687 (2007).\n43. Price, P.J.: Evaluation of spoken language systems. In: Proceedings of\nthe workshop on Speech and Natural Language - HLT ’90. pp. 91–95.\nAssociation for Computational Linguistics, Morristown, NJ, USA (1990).\nhttps://doi.org/10.3115/116580.116612\n44. Dahl, D.A., Bates, M., Brown, M., Fisher, W., Hunicke-Smith, K., Pallett, D., Pao,\nC., Rudnicky, A., Shriberg, E.: Expanding the scope of the ATIS task. 43 (1994).\nhttps://doi.org/10.3115/1075812.1075823\n45. Hemphill, C.T., Godfrey, J.J., George, R.D.: The ATIS Spoken Language Sys-\ntems Pilot Corpus. In: Proceedings of the DARPA Speech and Natural Language\nWorkshop. , Hidden Valley, Pennsylvania (1990).\n46. Zhong, R., Yu, T., Klein, D.: Semantic evaluation for Text-to-SQL with distilled\ntest suites. (2020). https://doi.org/10.18653/v1/2020.emnlp-main.29\n47. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep\nbidirectional transformers for language understanding. NAACL HLT 2019 - 2019\nConf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol. - Proc.\nConf. 1, 4171–4186 (2019).\n48. Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoy-\nanov, V., Zettlemoyer, L., Bart, P.: BART: Denoising Sequence-to-Sequence Pre-\ntraining for Natural Language Generation, Translation, and Comprehension. (2019).\n49. Souza, F., Nogueira, R., Lotufo, R.: BERTimbau: Pretrained BERT Models for\nBrazilian Portuguese.In: BRACIS 2020. Lecture Notes in Computer Science. pp.\n403–417. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-61377-8_\n28\n16 Marcelo Archanjo Jos´ e and Fabio Gagliardi Cozman\n50. Tang, Y., Tran, C., Li, X., Chen, P.J., Goyal, N., Chaudhary, V., Gu, J., Fan, A.:\nMultilingual Translation with Extensible Multilingual Pretraining and Finetuning.\nhttps://arxiv.org/abs/2008.00401. (2020).\n51. da Silva C.F.M., Jindal R.: SQL Query from Portuguese Language Using Natural\nLanguage Processing. In: Garg D., Wong K., Sarangapani J., Gupta S.K. (eds)\nAdvanced Computing. IACC 2020. Communications in Computer and Information\nScience, vol 1367. Springer, Singapore. (2021)https://doi.org/10.1007/978-981-\n16-0401-0_25",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8845629692077637
    },
    {
      "name": "SQL",
      "score": 0.6777242422103882
    },
    {
      "name": "Portuguese",
      "score": 0.6703790426254272
    },
    {
      "name": "Transformer",
      "score": 0.5787109136581421
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5654313564300537
    },
    {
      "name": "Natural language processing",
      "score": 0.5562564730644226
    },
    {
      "name": "Data definition language",
      "score": 0.45081597566604614
    },
    {
      "name": "Programming language",
      "score": 0.3951474130153656
    },
    {
      "name": "Linguistics",
      "score": 0.13017326593399048
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I17974374",
      "name": "Universidade de São Paulo",
      "country": "BR"
    }
  ],
  "cited_by": 16
}