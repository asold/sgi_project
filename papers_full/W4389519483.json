{
    "title": "Incorporating Structured Representations into Pretrained Vision &amp; Language Models Using Scene Graphs",
    "url": "https://openalex.org/W4389519483",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2786453042",
            "name": "Roei Herzig",
            "affiliations": [
                "Tel Aviv University"
            ]
        },
        {
            "id": "https://openalex.org/A5111090181",
            "name": "Alon Mendelson",
            "affiliations": [
                "Tel Aviv University"
            ]
        },
        {
            "id": "https://openalex.org/A2523407370",
            "name": "Leonid Karlinsky",
            "affiliations": [
                "IBM (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2754477448",
            "name": "Assaf Arbelle",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2031692738",
            "name": "Rogerio Feris",
            "affiliations": [
                "IBM (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2174985400",
            "name": "Trevor Darrell",
            "affiliations": [
                "University of California, Berkeley",
                "Berkeley College"
            ]
        },
        {
            "id": "https://openalex.org/A1484279603",
            "name": "Amir Globerson",
            "affiliations": [
                "Tel Aviv University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2908510526",
        "https://openalex.org/W3091588028",
        "https://openalex.org/W3173909648",
        "https://openalex.org/W3206930349",
        "https://openalex.org/W2579549467",
        "https://openalex.org/W2786947658",
        "https://openalex.org/W3172112830",
        "https://openalex.org/W2277195237",
        "https://openalex.org/W4312910992",
        "https://openalex.org/W4366850747",
        "https://openalex.org/W4303648003",
        "https://openalex.org/W4312261477",
        "https://openalex.org/W4281557543",
        "https://openalex.org/W2951702519",
        "https://openalex.org/W3009131631",
        "https://openalex.org/W4311251292",
        "https://openalex.org/W3106759358",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W3205068155",
        "https://openalex.org/W4287751139",
        "https://openalex.org/W3209532394",
        "https://openalex.org/W4226182655",
        "https://openalex.org/W3205717164",
        "https://openalex.org/W4312936847",
        "https://openalex.org/W4312480274",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2805516822",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W2896659472",
        "https://openalex.org/W4287658071",
        "https://openalex.org/W4205991051",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3166396011",
        "https://openalex.org/W2077069816",
        "https://openalex.org/W4224246420",
        "https://openalex.org/W4312310776",
        "https://openalex.org/W4282935537",
        "https://openalex.org/W3126792443",
        "https://openalex.org/W2938667732",
        "https://openalex.org/W4309805337",
        "https://openalex.org/W3090449556",
        "https://openalex.org/W4318718936",
        "https://openalex.org/W4312238419",
        "https://openalex.org/W4362597733",
        "https://openalex.org/W3000279895",
        "https://openalex.org/W4283821388",
        "https://openalex.org/W4381802186",
        "https://openalex.org/W2963184176",
        "https://openalex.org/W2970231061",
        "https://openalex.org/W4309864938",
        "https://openalex.org/W4385573684",
        "https://openalex.org/W2955882737",
        "https://openalex.org/W4390873321",
        "https://openalex.org/W4366330503",
        "https://openalex.org/W3096682293",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2806331055",
        "https://openalex.org/W4386076015"
    ],
    "abstract": "Roei Herzig, Alon Mendelson, Leonid Karlinsky, Assaf Arbelle, Rogerio Feris, Trevor Darrell, Amir Globerson. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 2023.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 14077‚Äì14098\nDecember 6-10, 2023 ¬©2023 Association for Computational Linguistics\nIncorporating Structured Representations into Pretrained Vision &\nLanguage Models Using Scene Graphs\nRoei Herzig‚àó1,3, Alon Mendelson‚àó1,\nLeonid Karlinsky4, Assaf Arbelle3, Rogerio Feris4, Trevor Darrell2, Amir Globerson1\n1Tel-Aviv University, 2UC Berkeley, 3IBM Research, 4MIT-IBM Watson AI Lab\nAbstract\nVision and language models (VLMs) have\ndemonstrated remarkable zero-shot (ZS) per-\nformance in a variety of tasks. However, recent\nworks have shown that even the best VLMs\nstruggle to capture aspects of compositional\nscene understanding, such as object attributes,\nrelations, and action states. In contrast, ob-\ntaining structured annotations, such as scene\ngraphs (SGs), that could improve these models\nis time-consuming and costly, and thus cannot\nbe used on a large scale. Here we ask whether\nsmall SG datasets can provide sufficient infor-\nmation for enhancing structured understanding\nof pretrained VLMs. We show that it is indeed\npossible to improve VLMs when learning from\nSGs by integrating components that incorporate\nstructured information into both visual and tex-\ntual representations. For the visual side, we in-\ncorporate a special ‚ÄúSG Component‚Äù in the im-\nage transformer trained to predict SG informa-\ntion, while for the textual side, we utilize SGs\nto generate fine-grained captions that highlight\ndifferent compositional aspects of the scene.\nOur method improves the performance of sev-\neral popular VLMs on multiple VL datasets\nwith only a mild degradation in ZS capabilities.\n1 Introduction\nIn recent years, vision and language models\n(VLMs) such as CLIP (Radford et al., 2021) have\ndemonstrated impressive results and extraordinary\nzero-shot capabilities when trained on massive\ndatasets containing image-text pairs (e.g., LAION\n400M Schuhmann et al., 2021). However, recent\nempirical studies (Thrush et al., 2022; Yuksekgonul\net al., 2023; Ma et al., 2022) have shown that even\nthe strongest VLMs struggle to perform composi-\ntional scene understanding, including identifying\nobject attributes and inter-object relations.\nUnderstanding the structure of visual scenes is\na fundamental problem in machine perception and\n‚àóEqual contribution.\nhas been explored extensively in many previous\nworks (Xu et al., 2017; Herzig et al., 2020; Yang\net al., 2022). In particular, datasets with scene\ngraph (SG) annotations, such as the Visual Genome\n(VG) dataset (Krishna et al., 2017), have been col-\nlected and used to improve scene understanding\nmodels. However, such datasets are expensive to\ncollect at scale and relatively small compared to\nthose used in training VLMs.1 This raises the fol-\nlowing questions: (1) Can small datasets containing\nSG annotations be utilized to finetune VLMs and\nimprove compositional scene understanding? (2)\nHow should the model and training be adapted to\nbest use this data? Here we show that it is indeed\npossible to improve VLMs using image-SG pairs\nby integrating components that incorporate struc-\nture into both visual and textual representations.\nOur first step is to convert SGs into highly de-\ntailed captions. A naive approach would be to\nfinetune VLMs on these image-text pairs, however,\nwe have found that this approach does not suffi-\nciently improve performance.2 This is also aligned\nwith recent work (Doveh et al., 2022; Yuksekgonul\net al., 2023), showing that contrastive learning ap-\nproaches allow the model to concentrate mainly\non object labels disregarding other important as-\npects, such as relations and attributes. To alleviate\nthis issue, we take inspiration from these works\nand use the SG to generate hard-negative captions\nthat highlight structural aspects. For example, if an\nSG contains an edge ‚Äúdog-chasing-cat‚Äù, then we\ncan reverse that edge into ‚Äúcat-chasing-dog‚Äù and\ngenerate a corresponding negative caption.\nNext, we turn to introduce structure into the vi-\nsual representation. Inspired by prompt learning\napproaches (Jia et al., 2022; Herzig et al., 2022a;\nZhou et al., 2022a), we incorporate into the im-\nage transformer encoder a set of ‚ÄúAdaptive Scene\n1VG contains ‚àº100Kimage-SG pairs, which is √ó1000\nsmaller than the large-scale VLMs pretraining datasets.\n2See our ablations in section 4.5.\n14077\nPretrained\tVL\tModelText\tTransformer\tEncoderImage\tTransformer\tEncoder\nImage‚ÄìScene\tGraph\tPairs\nPatch\tTokensAdaptive\tScene\tGraph\tTokensImage-Text\tPairs\n‚ÄúAn\toutdoor\twalkway\ton\ta\tgrass\tcovered\thill‚Äù\nScene\tGraph\tPredictionman\nfrisbee\nthrowing\nwearinggray\tshirt\nVision\tand\tLanguageContrastive\tLearning\nPositive&NegativeCaptionGeneration\nVisual\tGenome\tImage‚ÄìScene\tGraph\tPairsLAION\tImage‚ÄìText\tPairs\nImageScene\tGraph\nFigure 1: Scene Graphs Improve Pretrained Vision-Language Models. Vision and Language models (VLMs) are typically\ntrained on large-scale image-text pairs (Left). We propose to improve pretrained VLMs by utilizing a small set of scene graph\n(SG) annotations (Right) from the Visual Genome dataset, that is richer and reflects structured visual and textual information.\nSpecifically, we design a specialized model architecture and a new finetuning scheme when learning from SGs as follows: (i) Use\nthe graph to generate fine-grained positive and negative captions that highlight different compositional aspects of the scene. (ii)\nFinetune the pretrained model using contrastive learning with the generated captions from the SGs, along with image-text pairs\nfrom LAION. (iii) Predict SG information (object, relations, and their coordinates) from an image by incorporating ‚ÄúAdaptive\nScene Graph Tokens‚Äù into the image transformer encoder. During inference, these learned tokens, which capture structured\ninformation from the SG, interact with the patch tokens and CLS tokens to improve compositional scene understanding.\nGraph Tokens‚Äù, which interact with the patch and\nCLS tokens via attention. By training these tokens\nto predict SG information, the encoder can capture\nbetter structured representations.\nHowever, the above task of predicting SG from\nthe image transformer encoder is challenging, as\nit deviates from the initial VLM training objective.\nToward this end, we designed a technique tailored\nto the SG tokens, which parameterized the SG to-\nkens independently from the patch tokens. In partic-\nular, we modify the transformer layers throughout\nthe network by decomposing the parameters into\ntwo distinct sets: SG and patch tokens. We have\nfound that this allows better learning of the graph\nprediction task while still maintaining zero-shot\nperformance. We name our proposed approach\nSGVL (Scene Graphs for Vision-Language Mod-\nels). See Figure 1 for an overview.\nTo summarize, our main contributions are as\nfollows: (i) We propose to exploit a small set\nof SG annotations that is rich with visual and\ntextual information for enhancing compositional\nscene understanding in pretrained VLMs; (ii) We\nintroduce a new finetuning scheme that captures\nstructure-related information using the visual and\ntextual components when learning from SG labels.\nSpecifically, for the visual side, we incorporate\nspecial ‚ÄúAdaptive SG tokens‚Äù in the image trans-\nformer encoder, and train these to predict SG in-\nformation; (iii) Our method shows improved per-\nformance on CLIP, BLIP, and BLIP2 on several\nbenchmarks: Winoground (Thrush et al., 2022),\nVL-CheckList (Zhao et al., 2022), ARO (Yuksek-\ngonul et al., 2023), and VSR (Liu et al., 2022),\nhighlighting the effectiveness of our approach.\n2 Related Work\nVision and Language Models . In recent years,\npopular VLMs, such as CLIP (Radford et al., 2021),\nBLIP (Li et al., 2022b), BLIP2 (Li et al., 2023),\nand others, have shown impressive results and ex-\ntraordinary zero-shot capabilities. These models\nare trained with image-text pairs to align the two\nmodalities in a joint embedding space. However,\nrecent empirical studies (e.g., VL-CheckList (Zhao\net al., 2022), Winoground (Thrush et al., 2022),\nand ARO (Yuksekgonul et al., 2023)) have shown\nthat these models do not perform well on tasks that\nrequire compositional understanding, including re-\nlations between objects, and their attributes. Specif-\nically, it has been shown that VLMs tend to learn\na ‚Äúbag of objects‚Äù representation, leading them to\nbe less structure-aware. Several recent works, such\nas NegCLIP (Yuksekgonul et al., 2023), Count-\nCLIP (Paiss et al., 2023), and SVLC (Doveh et al.,\n2022), haven shown that hard negative generation\nfrom image-text pairs using language augmenta-\ntions could improve fine-grained understanding.\nUnlike these works, we utilize image-SG pairs to\nimprove structured representations by predicting\nSG information from the image encoder.\nMulti-task Prompt Tuning . The concept of\nprompt tuning for efficient finetuning of language\nmodels was introduced by Lester et al. (2021), and\nlater explored in vision models (Jia et al., 2022;\nWang et al., 2022) and VLMs (Ju et al., 2022; Zhou\net al., 2022b). Several recent works (Asai et al.,\n2022; Sanh et al., 2022; Vu et al., 2022), have\n14078\nText\tTransformer\tEncoderImage\tTransformer\tEncoder\nman\nfrisbee\nthrowingGraph-Based\tNegatives\nPatch\tTokensAdaptive\tSG\tTokensmanfrisbeeman\twearing\tshirt\nImage-Scene\tGraph\tpairs\nwearinggray\tshirt\ngray\tshirtman\tthrowing\tfrisbee\nImage-Text\tLoss\t‚Ñí!\"\nScene\tGraph\tLoss‚Ñí12\nSG-to-Text\nRelationships\tMatchingObjects\tMatching\nGT\tClasses\tand\tBoxes\nPredicted\tClasses\tand\tBoxes\nAdapted\twith\tLoRATransformer\tLayer\twith\tAdaptive\tScene\tGraph\tTokens\nPatch\tTokensAdaptive\tSG\tTokens\nùëÑ<,ùêæ<,ùëâ< ùëÑ12,ùêæ12,ùëâ12\nAttention\nùëÄùêøùëÉ< ùëÄùêøùëÉ12\nFigure 2: Our Scene Graphs for Vision-Language Models (SGVL) Approach. Our key goal is to capture structure-related\ninformation in both visual and textual components when learning from SGs. For the textual side, we generate captions and\nnegative captions using the graph (Graph-Based Negatives and SG-to-text modules). For the visual side, we incorporate into the\nimage transformer an additional set of learnable ‚ÄúAdaptive Scene Graph Tokens‚Äù to predict SG information. The transformer\nparameters are partitioned into two distinct sets, one for patch tokens and one for SG tokens (shown on the right). These tokens\npredict objects (pink tokens & arrows) and relationships (cyan tokens & arrows) from the image, and these predictions are\nmatched with the ground-truth SGs via bipartite matching. To allow open vocabulary prediction, we embed the SG categories\nusing the text transformer encoder. Last, we use LoRA adapters for finetuning, keeping all other model parameters frozen.\nexplored prompt tuning in the context of multi-\ntask learning in NLP, followed by works in vi-\nsion, and specifically in video domain (Avraham\net al., 2022; Herzig et al., 2022a), and VL (Shen\net al., 2022). Unlike these works, we add multiple\nprompts (which we refer to as Adaptive SG tokens)\nto VLMs in order to learn compositional scene in-\nformation via the SG prediction task. We designed\na technique tailored to the SG tokens, which decom-\nposes the parameters throughout the transformer\nlayers into two distinct sets: SG and patch tokens.\nThis allows better learning of the graph prediction\ntask while still maintaining zero-shot capabilities.\nLearning Structured Representations . Struc-\ntured representations have been shown to be ben-\neficial in many applications: video understand-\ning (Herzig et al., 2019, 2022b; Wang and Gupta,\n2018), relational reasoning (Baradel et al., 2018;\nBattaglia et al., 2018; Jerbi et al., 2020), VL (Chen\net al., 2020; Li et al., 2020; Tan and Bansal, 2019),\nhuman-object interactions (Kato et al., 2018; Xu\net al., 2019), and even image & video genera-\ntion (Bar et al., 2021; Herzig et al., 2020). In\nparticular, SGs (Johnson et al., 2015; Xu et al.,\n2017; Herzig et al., 2018) have been extensively\nused to provide semantic representations in a wide\nrange of applications (Johnson et al., 2018; Raboh\net al., 2020; Yu et al., 2021; Cong et al., 2023). Un-\nlike these previous works, here we propose a novel\narchitecture design that utilizes scene graph data,\ndemonstrating that a small amount of scene-graph\ndata can be used to improve pretrained VLMs via\nfine-tuning. Finally, SG data has been recently\nused to evaluate for compositional understanding\nin VLMs (Ma et al., 2022). This works differs\nfrom our approach, as this work only proposed a\nbenchmark, and not a new training method, while\nour approach proposes a method for fine-tuning\npretrained VLMs using scene graphs.\n3 Scene Graphs for VL Models\nWe begin by describing the standard VL trans-\nformer architecture and the scene graph annotations\n(Section 3.1). We then introduce our structural com-\nponents for both language (Section 3.2) and vision\n(Section 3.3), and the training losses (Section 3.4).\nOur method is illustrated in Figure 2.\n3.1 Preliminaries\nVLMs are typically trained with image-text pairs:\nXi = (Ii,Ti). Each of these modalities is pro-\ncessed by a separate encoder, and the training ob-\njective is to map the embeddings using contrastive\nlearning. Next, we briefly describe the encoders.\nLanguage Transformer Encoder ET. The text\nencoder is a transformer (Vaswani et al., 2017) as\ndescribed in CLIP and others, where a CLS token is\nappended to the beginning of the text, and the final\nCLS embedding is used as the text embedding.\nVision Transformer EncoderEI. A typical vision\ntransformer model (Dosovitskiy et al., 2021) takes\nan image I as input, extracts N non-overlapping\npatches,3 and projects them into a lower-dimension\nd, followed by adding spatial position embeddings,\n3We refer to these patches as ‚Äúpatch tokens‚Äù.\n14079\nresulting in a new embedding zi. This forms the\ninput tokens to the vision transformer encoder:\nz= [zCLS,z1,z2,¬∑¬∑¬∑ ,zN] (1)\nwhere zCLS is a learnable token. The input zis fed\ninto a standard transformer, and the final represen-\ntation of the CLS token is the image embedding.\nScene Graphs (SGs). Our motivation is to improve\nVLMs through structured annotations from an SG\ndataset. Formally, an SG is a tuple G = (V,E)\ndefined as follows: (i) Nodes V - A set of nobjects.\nEvery object in the SG contains a class label, a\nbounding box, and attributes. (ii) Edges E- A set\nof medges. These relationships are triplets (i,e,j )\nwhere iand j are object nodes, and eis the cate-\ngory of the relation between objects iand j. More\ndetails of the SG preprocessing are in Section B.1.\nProblem Setup . As mentioned above, we use\nimage-SG pairs (IG,G) from VG during finetun-\ning to improve structure understanding along with\nstandard LAION image-text pairs (I,T ). Next, we\ndescribe the textual and visual components that\ncapture structure in both modalities.\n3.2 Structural Language Component\nWe begin by describing how we transform an SG\ninto text and then explain how to manipulate the SG\nwith our Graph-Based Negatives to further capture\nthe structure in the model. For a visualization,\nsee Figure 7 in the supplementary.\nScene Graph-to-Text. Given an image I and a\ncorresponding SG, G, we use Gto generate a tex-\ntual caption for the image. We iterate over the\nconnected components of Gone by one. For each\ncomponent, we iterate over the edges, and for each\nedge between object nodes o1 and o2 with relation\nr, we generate the text o1,r,o2 (e.g., ‚Äúcat chasing\ndog‚Äù). If a node has an attribute, we prepend it\nto the node‚Äôs object category. Last, we generate a\nsingle caption by concatenating the captions of the\nconnected components separated by a period.\nGraph-Based Negatives (GN) . We have found\nthat using the scene graph data solely as image-\ntext pairs with contrastive loss is not enough to\nforce the model to develop structural understand-\ning. As shown in recent work (Yuksekgonul et al.,\n2023), the commonly used contrastive learning al-\nlows the model to concentrate mainly on object\nlabels disregarding other important aspects, such\nas relations and attributes. In order to provide more\nfocus on such aspects, we exploit the SG structure\nmanbeachsitting\tonblue\tskygreen\tvegetableslarge\tsandwich\ninsmiling\tmanbikeriding\tondark\tblue\ttie\nBlue\thelmet\ninbackground\nFigure 3: ‚ÄúAdaptive SG Tokens‚Äù Visualization. The predic-\ntions of object tokens (pink) and relationship tokens (cyan)\nare shown for images that are not in the VG training data.\nand propose a set of predefined graph-based rules\n(See Section B.2) that modify SGs and make them\nsemantically inconsistent with the image. Next,\nthese SGs are transformed into negative textual\ncaptions, which are used with a specified loss to\nmotivate the model to focus on structural aspects.\n3.3 Structural Visual Component\nScene Graphs Tokens . We propose to capture\nstructure-related information in the image encoder\nby predicting SGs. Toward this end, we add a set\nof ‚ÄúSG tokens‚Äù that are learned prompts designed\nto predict objects and relationships. The SG tokens\nconsist of two groups: (i) ‚Äúobject tokens‚Äù that rep-\nresent objects, their locations and their attributes,\nand (ii) ‚Äúrelationship tokens‚Äù that represent rela-\ntionships and their locations.\nFormally, we define a fixed set of Àúnlearned ob-\nject prompts and denote them by po\n1,po\n2,¬∑¬∑¬∑ ,po\nÀún ‚àà\nR1√ód. Similarly, we define a fixed set of\nÀúm relationships prompts and denote them by\npr\n1,pr\n2,¬∑¬∑¬∑ ,pr\nÀúm ‚ààR1√ód. We refer to these prompts\nas the learned SG tokens. The SG tokens are con-\ncatenated with the standard CLS and patch tokens\nto obtain the following inputs to the transformer:\nz= [zCLS,z1,...,z N,po\n1,...,p o\nÀún,pr\n1,...,p r\nÀúm] (2)\nNext, the transformer processes the input z, re-\nsulting in a new representation for each token. We\nuse the new SG tokens representations to predict\nobject and relationship labels and localization, by\npassing them through two feed-forward networks\n(FFNs). These predictions are supervised with\nground-truth SG annotations through a matching\nprocess. Figure 3 visualizes the SG tokens learned\nby our model. More details are in Section 3.4.\nAdaptive SG tokens. We have found that the SG\nprediction task is challenging as it deviates from\nthe initial VLM training objective. To alleviate\n14080\nthis issue, we consider a modification of the im-\nage transformer tailored specifically to the SG to-\nkens. Specifically, we decompose the parameters\nthroughout the transformer layers into two distinct\nsets: SG and patch tokens. This allows better learn-\ning of the graph prediction task. In more detail,\nrecall that transformer layers have matrices for\nmapping tokens to queries, keys, and values. We\ndenote these existing matrices for the patch tokens\nby QP,KP,VP. We introduce a separate set of\nmatrices QSG,KSG,VSG that is used with the SG\ntokens. Importantly, the attention is performed over\nall tokens (patch and SG). Similarly, for the MLP\ncomponent in the transformer layer, we also have a\ndifferent version for the patch tokens (MLPP) and\nthe SG tokens (MLPSG).\nParameter Efficiency. To perform efficient fine-\ntuning, we use LoRA adapters for the VLM. Specif-\nically, for each trainable matrix W ‚ààRu√óv (e.g.,\nW = QP or W = QSG), we let W0 denote its\npretrained weights, and parameterize the learned\nmatrix as:\nW = W0 + AB (3)\nwhere A‚ààu√órand B ‚ààr√óvare r-rank matri-\nces and r is a hyperparameter.4 We note we use\ntwo distinct r: rp and rSG for weights associated\nwith the patch and SG tokens (as described above),\nrespectively. During training, W0 is kept frozen,\nwhile A,B are learned. Overall, our additional\ntrainable parameters are only 7.5% of the model.\nOpen Vocabulary SG Prediction. The SG tokens\npredict the annotated SGs category information for\nobjects, relationships, and attributes. A naive im-\nplementation of this idea would require a prediction\nhead for each category. However, the VG dataset\ncontains approximately 70K object categories and\n40K relationship categories, and thus poses a sig-\nnificant challenge. Previous work (Xu et al., 2017)\nintroduced a split containing only 100 object la-\nbels and 50 relation labels, which limits the dataset.\nRather than restricting our data in this way, we use\nan open vocabulary approach. Namely, we utilize\nthe text encoder to embed the categories from the\nSG components. Next, we use these embeddings in\ntraining to supervise the SG tokens. For example,\nif node ihas category ‚Äúdog‚Äù and attribute ‚Äúblack‚Äù,\nwe train one of the object tokens to predict the\nembedding of the phrase ‚Äúblack dog‚Äù. This also\napplies to the prediction of relationship tokens, e.g.,\n4For the SG-token parameters, we set W0 to the value of\nthe corresponding parameters of the patch tokens.\na relationship token predicts the embedding of the\nphrase ‚Äúdog chasing cat‚Äù. More details are in the\nnext section.\n3.4 Training and Losses\nDuring training our batch contains image-text pairs\n(I,T ) along with image-SG pairs (IG,G). We use\nthe latter to generate positive captions ( TP) and\nnegative captions (TN). We finetune our model us-\ning these inputs while optimizing the losses below.\nImage-Text Loss. Our image-text loss is com-\nprised of contrastive loss and graph negatives loss.\nContrastive Loss: We apply contrastive loss on\nimage-text pairs, as in Radford et al. (2021). Hence,\nthe loss is calculated as follows based on standard\npairs and those generated from SGs:\nLCont := Contrastive(EI(ÀúI),ET( ÀúT)) (4)\nwhere ÀúI = I‚à™IG and ÀúT = T ‚à™TP.\nGraph-Based Negative Loss: For each image-SG\npair we apply a loss that drives the embedding of\nIG to be more similar to that of TP than TN:\nLGN :=\n‚àë\nIG,TP,TN\n‚àílog\n(\neC(IG,TP)\neC(IG,TP) + eC(IG,TN)\n)\nwhere C(¬∑,¬∑) is the cosine similarity between the\nimage and text embeddings. Finally, the image-text\nloss is a weighted sum of both losses:\nLIT := LCont + ŒªGNLGN (5)\nand ŒªGN is a hyperparameter. For more informa-\ntion, see Section B.4 in supplementary.\nScene Graph Loss. The graph Gcontains several\nannotations: the set of object categories O, their\nset of bounding boxes, the set of relationship cate-\ngories R, and their bounding boxes. As we do not\naim to predict the object and relationship categories\ndirectly, but rather use the embeddings from the\nVLM, we extract the category embeddings with the\ntext encoder ET: ÀúO = ET(O) ‚ààR(n+1)√ód, and\nÀúR = ET(R) ‚ààR(m+1)√ód. We note that (n+ 1)\nand (m+ 1)classes are due to ‚Äúno object‚Äù and\n‚Äúno relationship‚Äù classes. These class embeddings\ntogether with the bounding boxes are the SG ele-\nments that we aim to predict from the SG tokens.\nWe next describe the prediction process. The\nimage encoder outputs a set of object tokens and a\nset of relationship tokens. We apply two separate\nFFNs to predict bounding boxes and class embed-\ndings. To calculate probabilities over n+ 1and\n14081\nWinoground VL-Checklist ARO VSR ZS\nAll Dataset All Datasets Avg Flickr30K COCO All Dataset 21 Tasks\nModel Text Image group Attribute Object Relation Order Order Avg Avg\nFLA V A 32.3 20.0 14.5 54.6 70.6 46.6 12.9 3.9 54.1 -\nViLT 34.8 14.0 9.3 73.3 85.0 62.0 22.4 18.7 - -\nLLaV A 24.8 25.0 13.0 65.5 83.1 83.0 98.1 97.5 - -\nMiniGPT-4 23.3 18.0 9.5 71.3 84.2 84.1 99.4 98.9 - -\nCLIP 30.7 10.5 8.0 65.5 80.6 78.0 59.5 46.0 - 56.4\nBLIP 39.0 19.2 15.0 75.2 82.2 81.5 27.9 24.9 56.5 49.0\nBLIP2 42.0 23.8 19.0 77.8 84.9 84.9 33.9 32.3 61.9 52.5\nNegCLIP 29.5 10.5 8.0 68.0 81.4 81.3 91.0 86.0 - 55.1\nNegBLIP 42.5 24.0 18.5 78.2 83.7 81.9 85.0 84.7 57.9 48.2\nNegBLIP2 41.5 26.0 20.5 79.0 87.0 88.2 91.8 88.2 62.1 51.7\nCLIP-SGVL (ours) 32.0 (+1.3)14.0 (+3.5) 9.8 (+1.8) 72.0 (+6.5)82.6 (+2.0)82.0 (+4.0) 82.0 (+22.5) 78.2 (+32.2) - 54.3 (-2.1)\nBLIP-SGVL (ours) 42.8 (+3.8)27.3 (+8.1)21.5 (+6.5) 81.8 (+6.6)85.2 (+3.0)81.9 (+0.4) 70.0 (+42.1 )71.0 (+46.1) 62.4 (+5.9) 48.0 (-1.0)\nBLIP2-SGVL (ours)42.8 (+0.8)28.5 (+4.5)23.3 (+4.3) 81.2 (+3.4)88.4 (+3.5)88.8 (+3.9) 77.0 (+43.1) 77.0 (+44.7) 63.4 (+1.5) 51.4 (-1.1)\nTable 1: Winoground, VL-Checklist, ARO, VSR, and Zero-Shot (ZS) Results. Gains & losses are relative to the base models.\nm+ 1classes, we compute the cosine similarity of\nthe predicted class embeddings with the GT class\nembeddings, ÀúOand ÀúRfollowed by a softmax.\nNext, to determine which SG tokens correspond\nto which GT objects and relationships, we match\nthe predictions of the SG tokens with the ground-\ntruth SG. We follow the matching process and loss\ncomputation as in DETR (Carion et al., 2020), ex-\ncept that in our case, objects and relationships are\nmatched separately. Our final SG loss LSG is the\nsum of the objects matching loss LO and the rela-\ntionships matching loss LR. The complete match-\ning process and losses details are in Section B.3.\nWe optimize our final loss as the sum of the\nscene graph loss and the image-text loss:\nLTotal := LIT + ŒªSGLSG (6)\nwhere ŒªSG is a hyperparameter.\nre\n4 Experiments and Results\nWe apply our SGVL approach to three popular\nVLMs: CLIP, BLIP, and BLIP2. 5 We evaluate\non several VL compositional datasets following\nthe standard protocol for each model. Additional\nresults and ablations are in Supp. Section A.\n4.1 Datasets\nWe describe the training and evaluation datasets\nbelow. More details are in Supp. Section D.\nTraining. For training, we use image-SG data from\nVisual Genome (VG), along with a small subset\n(less than 1%) of the LAION dataset as ‚Äústandard‚Äù\nimage-text pairs. VG is annotated with 108,077\n5More details of BLIP2 modifications are in Section B.5.\nimages and corresponding SGs, andLAION 400M\nis a large-scale image-text pair dataset that was\nautomatically curated from the Internet.\nEvaluation. We evaluate our method on four\nmain benchmarks for compositional scene under-\nstanding: VL-Checklist (VLC) (Zhao et al., 2022),\nWinoground (Thrush et al., 2022), ARO (Yuksek-\ngonul et al., 2023), VSR (Liu et al., 2022), and zero-\nshot classification. (1) VLC combines samples\nfrom the following datasets: VG (Krishna et al.,\n2017), SWiG (Pratt et al., 2020), V AW (Pham et al.,\n2021), and HAKE (Li et al., 2019). For each image,\ntwo captions are given, a positive and a negative.\nThe negative caption is constructed by modifying\none word in the positive caption that corresponds to\na structural visual aspect (e.g., attribute). We report\nresults on a combined VLC dataset excluding VG.\n(2) Winogroundprobes compositionality in VLMs.\nEach sample is composed of two image-text pairs\nthat have overlapping lexical content but are dif-\nferentiated by swapping an object, a relation, or\nboth. For each sample, two text-retrieval tasks (text\nscore), and two image-retrieval tasks (image score)\nare defined. The group score represents the com-\nbined performance. A recent study (Diwan et al.,\n2022) has shown that solving Winoground requires\nnot just compositionality but also other abilities.\nThe study suggested a subset (NoTag) that solely\nprobes compositionality. We report results on the\nfull dataset and the NoTag split. (3) ARO proposes\nfour tasks that test sensitivity to order and composi-\ntionality: VG Relation, VG Attribution, COCO &\nFlickr30k Order. Since our approach is trained on\nVG, we report only the COCO and Flickr30k order\ntasks. (4) VSR tests spatial understanding. The\n14082\nWinoground VL-Checklist\nNoTag Attribute Object Relation\nModel Text Image Group Action Color Material Size Location Size Action\nCLIP 30.4 11.1 8.2 68.1 70.2 73.1 52.9 81.0 80.1 78.0\nBLIP 44.8 23.8 19.2 79.5 83.2 84.7 59.8 83.0 81.3 81.5\nBLIP2 50.0 31.9 26.7 81.0 86.2 90.3 61.7 85.4 84.3 84.9\nCLIP-SGVL(ours) 33.3 (+2.8) 14.0 (+2.9) 8.7 (+0.5) 76.6 (+8.5) 78.7 (+8.5) 81.3 (+8.2) 59.7 (+6.8) 83.2 (+2.2)82.0 (+1.9)81.3 (+3.3)\nBLIP-SGVL(ours) 45.9 (+1.1) 34.3 (+10.5)25.6 (+6.4) 79.2 (-0.3) 94.5 (+11.3)91.9 (+7.2) 73.3 (+13.5)86.4 (+3.4)83.9 (+2.6)81.9 (+0.4)\nBLIP2-SGVL(ours)51.7 (+1.7) 37.2 (+5.3) 29.0 (+2.3)82.4 (+1.4) 91.7 (+5.5) 92.2 (+1.9) 70.1 (+8.4) 89.0 (+4.6)87.6 (+3.3)88.8 (+3.9)\nTable 2: Winoground and VL-Checklist Results. Results for SGVL and baselines on VL-Checklist subsets and the NoTag split\nof Winoground. For VL-Checklist we exclude the Visual Genome dataset.\nModel Adjacency Directional Orientation Projective Proximity Topological Unallocated Average\nBLIP 55.4 48.9 53.7 58.2 56.5 55.6 63.4 56.5\nBLIP2 56.2 47.9 59.8 62.5 55.8 66.7 66.3 61.9\nBLIP-SGVL (ours) 57.7 (+2.3) 54.1 (+5.2) 57.8 (+4.1) 63.8 (+5.6) 57.8 (+1.3) 64.8 (+8.8) 68.8 (+5.4) 62.4 (+5.9)\nBLIP2-SGVL (ours) 59.8 (+3.6) 56.9 (+9.0) 58.5 (-1.3) 61.5 (-1.0) 59.7 (+3.9) 70.0 (+3.3) 66.8 (+0.5) 63.4 (+1.5)\nTable 3: VSR Results. Results for SGVL and baselines on the VSR dataset.\ndataset consists of pairs of an image and a descrip-\ntion of the spatial relations between two objects\nshown in the image. The VLM should classify the\npairs as either true or false. We do not evaluate\nCLIP-based models here as CLIP does not allow\nsuch classification in a straightforward manner.\nZero-Shot (ZS) Classification . We evaluate 21\nclassification datasets following the protocol from\nELEV ATER (Li et al., 2022a). The evaluation in-\ncludes datasets such as ImageNet, CIFAR100, and\nothers. We report the average results over the 21\ntasks in Table 1 and Table 4b in Ablations.\n4.2 Implementation Details\nWe implemented SGVL using Pytorch. For code\nand pretrained models, visit the project page\nat https://alonmendelson.github.io/SGVL/.\nOur code and training procedures are based on\nCLIP, BLIP and BLIP2. For CLIP, we use the ML-\nFoundation Open-CLIP repository (Ilharco et al.,\n2021), and for BLIP/BLIP2 we use the official\nimplementations.6 We use the ViT/B-32 model ar-\nchitecture for CLIP, ViT/B-16 for BLIP and ViT-g\nfor BLIP2. The models are initialized with orig-\ninal weights released by the respective authors.\nThe training batch contains 256/32/32 image-text\npairs and 8 image-SG pairs for CLIP/BLIP/BLIP2-\nSGVL respectively. For more info, please refer to\nSupp. Section D. Last, we select the best check-\npoint for each experiment using the validation set,\nwhich is composed of VG samples from VLC that\nare not in the test set (as mentioned above, we\nreport VLC results excluding VG).\n6https://github.com/salesforce/LAVIS\n4.3 Baselines\nIn the experiments, we compare our SGVL ap-\nproach to three VLMs: CLIP (Radford et al., 2021),\nBLIP (Li et al., 2022b), and BLIP2 (Li et al.,\n2023) to which SGVL was applied. Addition-\nally, we compare with NegCLIP (Yuksekgonul\net al., 2023), which uses negative text augmenta-\ntions and also demonstrated strong results on these\ndatasets. For a fair comparison, we also implement\nBLIP/BLIP2 versions of the NegCLIP approach,\nwhich we refer to as NegBLIP/NegBLIP2. Last,\nwe also show comparison with the state-of-the-art\nVLMs: FLA V A (Singh et al., 2021) and ViLT (Kim\net al., 2021), as well as newly introduced genera-\ntive VLMs (GVLMs), such as LLaV A (Liu et al.,\n2023) and MiniGPT-4 (Zhu et al., 2023).\n4.4 Results\nResults are shown in Table 1, and demonstrate\nthat CLIP/BLIP/BLIP2-SGVL outperforms the pre-\ntrained base models across several datasets. These\nimprovements come at the price of a slight degra-\ndation in zero-shot performance. This may be due\nto several factors: (1) Finetuning with negative cap-\ntions may harm the ZS performance, as observed\nin Yuksekgonul et al. (2023); (2) Finetuning for SG\nprediction deviates from the original VLM training\nobjective; (3) We use only 1% of LAION image-\ntext pairs for finetuning. Additionally, our method\noutperforms the Neg baselines on all datasets ex-\ncept ARO, which measures the sensitivity to text\norder. For this task, the text augmentations in Yuk-\nsekgonul et al. (2023) are more appropriate. Finally,\nwe note that we do not evaluate CLIP-based models\non VSR (See VSR in Section 4.1).\n14083\n(a) Scene Graph Utilization\nModel Text Image Group\nBLIP 39.0 19.2 15.0\nBLIP+Graph Text (GT) 40.3 20.5 16.5\nBLIP+GT+Graph Neg. (GN) 40.5 25.5 19.0\nBLIP+GT+GN+SG Tokens 42.8 27.3 21.5\n(b) Adaptive SG Tokens\nModel WG ZS mAP\nBLIP 15.0 49.0 -\nBLIP + SG Tokens 20.0 47.5 16.1\nBLIP + Adap. SG Tokens 21.5 48.0 17.7\n(c) Sparse Vs. Dense SGs\nModel Text Image Group\n30% of Graph 33.2 26.5 18.0\n70% of Graph 40.7 26.5 20.0\nw/o Relations 40.5 20.5 15.8\nEntire Graph 42.8 27.3 21.5\nTable 4: Ablations on the Winoground Dataset. We show (a) The contribution of our proposed components, utilizing the SG\ninformation. (b) The benefits of our adaptive SG tokens. Reported metrics are: Winoground group score (WG), zero-shot (ZS) on\nELEV ATER, and mAP of SG prediction. (c) Importance of the image-SG comprehensiveness. More ablations are in Section A.\nTable 2 and Table 3 show the performance of\nour method on fine-grained Winoground, VLC,\nVSR splits. For most splits, our method is sig-\nnificantly better or comparable to the pretrained\nmodels. In Figure 4, we show where our model\nimproves upon the baseline and where it still fails.\nFor more results, please refer to Section A.3.\n4.5 Ablations\nWe perform a comprehensive ablation on the\nWinground dataset with our SGVL approach us-\ning the BLIP model7 (see Table 4). More ablations\nare in Section A.1. We also provide ablations on\nall datasets in Section A.2.\nFinetuning on VG without SGs . We com-\npare our approach to naive finetuning with\ncaptions provided by the VG dataset. We\nfinetuned BLIP on textual descriptions from\nVG, resulting in 40.0/20.5/16.0 for Winogroundf\nText/Image/Group scores, while our BLIP-SGVL\nachieves 42.8/27.3/21.5, indicating the improve-\nments are not solely due to the VG data.\nSG prediction without SG tokens . Our model\npredicts SG information from specialized SG to-\nkens. A naive approach might have been to predict\nfrom the CLS token. Thus, we consider a variant\nthat we refer to as BLIP MT (multi-task) , which\nis a simple implementation of object and relation\nprediction. This variant does not include SG to-\nkens and instead predicts the graph using object\nand relation MLPs on top of the CLS token. Using\nexactly the same training recipe and data as our\nBLIP-SGVL, this ablation achieves 38.5/22.5/17.5\nfor Winoground Text/Image/Group scores, while\nour BLIP-SGVL achieves 42.8/27.3/21.5, justify-\ning our design choice of SG tokens.\nScene graph utilization ablation . Our model\nconsists of three parts: converting SG into cap-\ntions, adding hard-negatives, and adding adaptive\nSG tokens. In Table 4a, we examine the con-\n7We chose BLIP due to its lower computational require-\nments (e.g., BLIP2 requires A100).\ntribution of each component. By finetuning the\nmodel with only positive captions generated from\nthe SGs ( BLIP+GT), we obtain +1.3/+1.3/+1.5\nfor Winoground Text/Image/Group scores over the\nBLIP baseline. When the graph-based negatives\n(BLIP+GT+GN) are added, an improvement of\n+1.5/+6.3/+3.2 is achieved compared to the base-\nline. Finally, when fully implementing our SGVL\napproach, i.e. adding the SG prediction task from\nthe SG tokens, we achieve improved results over\nthe baseline of +3.8/+8.1/6.5. This highlights the\ncontribution of the model components.\nAdaptive scene graph tokens. Our approach uses\ndifferent transformer parameters for SG and patch\ntokens, unlike the standard transformer, which uses\nthe same parameters for all tokens. Thus, we next\nexamine what happens when SG and patch tokens\nuse the same parameters. In Table 4b, we report\nthe performance of two variants on three tasks:\nWinoground group score (WG), Zero-shot classi-\nfication (ZS) on ELEV ATER, and SG prediction\n(mAP metric). The first variant, BLIP + SG Tokens,\nrefers to the addition of tokens dedicated to pre-\ndicting SGs with the same transformer parameters\nshared between them and other input tokens. The\nsecond variant, BLIP + Adaptive SG Tokens, refers\nto our technique that introduces parameters specific\nto the SG tokens in every transformer layer (see\nsection 3.3). We can see that the second variant out-\nperforms the SG token addition in all tasks. This\ndemonstrates how our modification to the image\ntransformer encoder improves SG prediction and\nVL performance without compromising ZS.\nUsing sparse vs. dense SGs. In this ablation, we\ninvestigate whether the density of the SG in VG\naffects performance, since SGs contain denser and\nricher information than standard captions. Towards\nthis end, we train SGVL with sparsified versions\nof the graph. Specifically, we train two variants\nwhere objects and relations from the graphs are ran-\ndomly removed (30% and 70%) and a third variant\nin which all relations are removed. As can be seen\n14084\nThe\tcar\tis\tunderneath\tthe\tpersonThe\tperson\tis\tunderneath\tthe\tcar\nYoung\tperson\tplaying\tbaseball\twith\ta\tgreen\tbat\tand\tblue\tball\nYoung\tperson\tplaying\tbaseball\twith\ta\tblue\tbat\tand\tgreen\tball\nAn\told\tkisses\ta\tyoung\tpersonA\tyoung\tperson\tkisses\tan\told\tperson\nThere\tare\tmore\tladybugs\tthan\tflowersThere\tare\tmore\tflowers\tthan\tladybugs\nWinoground\nVL-Checklist\nWooden\ttableBamboo\ttable\nShort\tlampTall\tlamp\nSmall\tbenchLarge\tbenchPerson\tsit\ton\tcouchPerson\twalk\ton\tcouch\nWinoground\nVL-Checklist\nPerson\thold\tcupPerson\tspin\tcup\nPerson\tcarry\ttennis\tracketPerson\tswing\ttennis\tracket\nSilver\tplatterGray\tplatter\nVSR VSR\nThe\tperson\tis\tbehind\tthe\tcake\nThe\tdog\tis\ton\tthe\ttoilet\n The\tbottle\tis\tbelow\tthe\tcat\nThe\tcow\tis\tin\tfront\tof\tthe\tperson\nThe\tdonut\tis\tright\tof\tthe\tbanana\nThe\tdog\tis\ttouching\tthe\ttoilet\nThe\thot\tdog\tis\tat\tthe\tedge\tof\tthe\ttable\nFigure 4: Predictions visualization on Winoground, VL-Checklist, and VSR. The left panel shows where our model succeeds\nand the baseline fails. The right panel shows where our model still fails. For VL-Checklist examples, true captions are in green\nand false captions in red. Our model outperforms the baseline in samples that require understanding relationships between\nobjects and binding attributes to objects. The failure cases illustrate the complexity and ambiguity of the samples.\nin Table 4c, our results show that our model per-\nforms better when the graphs are denser, richer, and\ndescribe the image more accurately, highlighting\nthe motivation to utilize SGs for VLMs.\nTraining without LAION image-text pairs. In\nSGVL, we train simultaneously with image-text\npairs from LAION and image-SG pairs from VG.\nTo test the effectiveness of simultaneous training,\nwe train only with Image-SG pairs and obtain a\ndegradation of -2.0/-1.2/-1.2 for Text/Image/Group\nscores compared to our BLIP-SGVL. In addition,\nwe observe a degradation of 1.1% in ZS perfor-\nmance compared to our BLIP-SGVL model. This\nindicates that simultaneous training is beneficial.\n5 Conclusions\nStructured understanding of complex scenes is a\nkey element of human perception, but its modeling\nstill remains a challenge. In this work, we pro-\npose a new approach for incorporating structured\ninformation into pretrained VLMs from SG data\nto improve scene understanding. We demonstrate\nimproved performance on four benchmarks prob-\ning compositional scene understanding with only a\nmild degradation in ZS performance. Our findings\nsuggest that only a small amount of high-quality\nannotations may be sufficient to improve such mod-\nels. We hope these findings will encourage future\nwork to generalize our approach beyond the VL\nregime or using other types of dense annotations,\nsuch as segmentation maps and depth maps.\n6 Limitations\nAs mentioned above, our work proposes a spe-\ncialized model architecture and a new finetuning\nscheme for learning from SGs. We demonstrate\nimproved performance on several different models\nand datasets. Nevertheless, our work has some lim-\nitations. First, we leverage SG annotations since\nthey are rich and reflect structured visual and tex-\ntual information. However, the quality of the data\nis crucially important to our method, and thus poor\ndata may result in lower performance. Finally, our\nimprovements may be restricted by the fact that\nthese annotations are rare and expensive to collect.\nAcknowledgements\nThis project has received funding from the Euro-\npean Research Council (ERC) under the European\nUnions Horizon 2020 research and innovation pro-\ngramme (grant ERC HOLI 819080). Prof. Dar-\nrell‚Äôs group was supported in part by DoD, includ-\ning PTG and/or LwLL programs, as well as BAIR‚Äôs\nindustrial alliance programs.\n14085\nReferences\nAkari Asai, Mohammadreza Salehi, Matthew E. Peters,\nand Hannaneh Hajishirzi. 2022. Attentional mixtures\nof soft prompt tuning for parameter-efficient multi-\ntask knowledge sharing. ArXiv, abs/2205.11961.\nElad Ben Avraham, Roei Herzig, Karttikeya Mangalam,\nAmir Bar, Anna Rohrbach, Leonid Karlinsky, Trevor\nDarrell, and Amir Globerson. 2022. Bringing image\nscene structure to video via frame-clip consistency of\nobject tokens. In Thirty-Sixth Conference on Neural\nInformation Processing Systems.\nAmir Bar, Roei Herzig, Xiaolong Wang, Anna\nRohrbach, Gal Chechik, Trevor Darrell, and\nA. Globerson. 2021. Compositional video synthe-\nsis with action graphs. In ICML.\nFabien Baradel, Natalia Neverova, Christian Wolf,\nJulien Mille, and Greg Mori. 2018. Object level vi-\nsual reasoning in videos. In ECCV, pages 105‚Äì121.\nPeter W Battaglia, Jessica B Hamrick, Victor Bapst, Al-\nvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz\nMalinowski, Andrea Tacchetti, David Raposo, Adam\nSantoro, Ryan Faulkner, et al. 2018. Relational in-\nductive biases, deep learning, and graph networks.\narXiv preprint arXiv:1806.01261.\nNicolas Carion, Francisco Massa, Gabriel Synnaeve,\nNicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. 2020. End-to-end object detection with\ntransformers. In Computer Vision ‚Äì ECCV 2020 ,\npages 213‚Äì229.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El\nKholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2020. Uniter: Universal image-text\nrepresentation learning. In ECCV.\nYuren Cong, Wentong Liao, Bodo Rosenhahn, and\nMichael Ying Yang. 2023. Learning similarity be-\ntween scene graphs and images with transformers.\nArXiv, abs/2304.00590.\nAnuj Diwan, Layne Berry, Eunsol Choi, David F.\nHarwath, and Kyle Mahowald. 2022. Why is\nwinoground hard? investigating failures in visuolin-\nguistic compositionality. In Conference on Empirical\nMethods in Natural Language Processing.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. 2021. An image\nis worth 16x16 words: Transformers for image\nrecognition at scale. ICLR.\nSivan Doveh, Assaf Arbelle, Sivan Harary, Rameswar\nPanda, Roei Herzig, Eli Schwartz, Donghyun Kim,\nRaja Giryes, Rogerio Feris, Shimon Ullman, et al.\n2022. Teaching structured vision&language con-\ncepts to vision&language models. arXiv preprint\narXiv:2211.11733.\nRoei Herzig, Ofir Abramovich, Elad Ben-Avraham, As-\nsaf Arbelle, Leonid Karlinsky, Ariel Shamir, Trevor\nDarrell, and Amir Globerson. 2022a. Promptono-\nmyvit: Multi-task prompt learning improves video\ntransformers using synthetic scene data.\nRoei Herzig, Amir Bar, Huijuan Xu, Gal Chechik,\nTrevor Darrell, and Amir Globerson. 2020. Learning\ncanonical representations for scene graph to image\ngeneration. In European Conference on Computer\nVision.\nRoei Herzig, Elad Ben-Avraham, Karttikeya Mangalam,\nAmir Bar, Gal Chechik, Anna Rohrbach, Trevor Dar-\nrell, and Amir Globerson. 2022b. Object-region\nvideo transformers. In Conference on Computer Vi-\nsion and Pattern Recognition (CVPR).\nRoei Herzig, Elad Levi, Huijuan Xu, Hang Gao, Eli\nBrosh, Xiaolong Wang, Amir Globerson, and Trevor\nDarrell. 2019. Spatio-temporal action graph net-\nworks. In Proceedings of the IEEE International\nConference on Computer Vision Workshops, pages\n0‚Äì0.\nRoei Herzig, Moshiko Raboh, Gal Chechik, Jonathan\nBerant, and Amir Globerson. 2018. Mapping images\nto scene graphs with permutation-invariant structured\nprediction. In Advances in Neural Information Pro-\ncessing Systems (NIPS).\nGabriel Ilharco, Mitchell Wortsman, Ross Wightman,\nCade Gordon, Nicholas Carlini, Rohan Taori, Achal\nDave, Vaishaal Shankar, Hongseok Namkoong, John\nMiller, Hannaneh Hajishirzi, Ali Farhadi, and Lud-\nwig Schmidt. 2021. Openclip.\nAchiya Jerbi, Roei Herzig, Jonathan Berant, Gal\nChechik, and Amir Globerson. 2020. Learning object\ndetection from captions via textual scene attributes.\nArXiv, abs/2009.14558.\nMenglin Jia, Luming Tang, Bor-Chun Chen, Claire\nCardie, Serge Belongie, Bharath Hariharan, and Ser-\nNam Lim. 2022. Visual prompt tuning. In European\nConference on Computer Vision (ECCV).\nJustin Johnson, Agrim Gupta, and Li Fei-Fei. 2018. Im-\nage generation from scene graphs. In Proceedings of\nthe IEEE conference on computer vision and pattern\nrecognition, pages 1219‚Äì1228.\nJustin Johnson, Ranjay Krishna, Michael Stark, Li-Jia\nLi, David Shamma, Michael Bernstein, and Li Fei-\nFei. 2015. Image retrieval using scene graphs. In\nProceedings of the IEEE conference on computer\nvision and pattern recognition, pages 3668‚Äì3678.\nChen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, and\nWeidi Xie. 2022. Prompting visual-language mod-\nels for efficient video understanding. In European\nConference on Computer Vision (ECCV).\nKeizo Kato, Yin Li, and Abhinav Gupta. 2018. Com-\npositional learning for human object interaction. In\nECCV.\n14086\nWonjae Kim, Bokyung Son, and Ildoo Kim. 2021. Vilt:\nVision-and-language transformer without convolu-\ntion or region supervision. In Proceedings of the\n38th International Conference on Machine Learning,\npages 5583‚Äì5594.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin John-\nson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A Shamma, et al.\n2017. Visual genome: Connecting language and vi-\nsion using crowdsourced dense image annotations.\nInternational Journal of Computer Vision, 123(1):32‚Äì\n73.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045‚Äì3059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nChunyuan Li, Haotian Liu, Liunian Harold Li,\nPengchuan Zhang, Jyoti Aneja, Jianwei Yang, Ping\nJin, Yong Jae Lee, Houdong Hu, Zicheng Liu, and\nJianfeng Gao. 2022a. Elevater: A benchmark and\ntoolkit for evaluating language-augmented visual\nmodels. Neural Information Processing Systems.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\n2023. BLIP-2: bootstrapping language-image pre-\ntraining with frozen image encoders and large lan-\nguage models. In ICML.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\n2022b. Blip: Bootstrapping language-image pre-\ntraining for unified vision-language understanding\nand generation. arXiv preprint arXiv:2201.12086.\nXiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu,\nPengchuan Zhang, Lei Zhang, Lijuan Wang,\nHoudong Hu, Li Dong, Furu Wei, Yejin Choi,\nand Jianfeng Gao. 2020. Oscar: Object-semantics\naligned pre-training for vision-language tasks. ECCV\n2020.\nYong-Lu Li, Liang Xu, Xinpeng Liu, Xijie Huang,\nYue Xu, Mingyang Chen, Ze Ma, Shiyi Wang,\nHao-Shu Fang, and Cewu Lu. 2019. Hake: Hu-\nman activity knowledge engine. arXiv preprint\narXiv:1904.06539.\nTsung-Yi Lin, M. Maire, Serge J. Belongie, James Hays,\nP. Perona, D. Ramanan, Piotr Doll√°r, and C. L. Zit-\nnick. 2014. Microsoft coco: Common objects in\ncontext. In ECCV.\nFangyu Liu, Guy Edward Toh Emerson, and Nigel\nCollier. 2022. Visual spatial reasoning. ArXiv,\nabs/2205.00363.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023. Visual instruction tuning. In NeurIPS.\nIlya Loshchilov and Frank Hutter. 2017. Decoupled\nweight decay regularization. In International Confer-\nence on Learning Representations.\nZixian Ma, Jerry Hong, Mustafa Omer Gul, Mona\nGandhi, Irena Gao, and Ranjay Krishna. 2022. Crepe:\nCan vision-language foundation models reason com-\npositionally? ArXiv, abs/2212.07796.\nRoni Paiss, Ariel Ephrat, Omer Tov, Shiran Zada, Inbar\nMosseri, Michal Irani, and Tali Dekel. 2023. Teach-\ning clip to count to ten. ArXiv, abs/2302.12066.\nKhoi Pham, Kushal Kafle, Zhe Lin, Zhihong Ding, Scott\nCohen, Quan Tran, and Abhinav Shrivastava. 2021.\nLearning to predict visual attributes in the wild. In\nProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 13018‚Äì\n13028.\nSarah Pratt, Mark Yatskar, Luca Weihs, Ali Farhadi,\nand Aniruddha Kembhavi. 2020. Grounded situation\nrecognition. In European Conference on Computer\nVision, pages 314‚Äì332. Springer.\nMoshiko Raboh, Roei Herzig, Gal Chechik, Jonathan\nBerant, and Amir Globerson. 2020. Differentiable\nscene graphs. In WACV.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models\nfrom natural language supervision. In International\nConference on Machine Learning, pages 8748‚Äì8763.\nPMLR.\nHamid Rezatofighi, Nathan Tsoi, JunYoung Gwak,\nAmir Sadeghian, Ian Reid, and Silvio Savarese. 2019.\nGeneralized intersection over union. In The IEEE\nConference on Computer Vision and Pattern Recog-\nnition (CVPR).\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala Neeraj, Jos Rozen, Abheesht Sharma, An-\ndrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan\nTeehan, Teven Le Scao, Stella Biderman, Leo Gao,\nThomas Wolf, and Alexander M Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization. In International Conference on Learning\nRepresentations.\nChristoph Schuhmann, Richard Vencu, Romain Beau-\nmont, Robert Kaczmarczyk, Clayton Mullis, Aarush\n14087\nKatta, Theo Coombes, Jenia Jitsev, and Aran Komat-\nsuzaki. 2021. Laion-400m: Open dataset of clip-\nfiltered 400 million image-text pairs. arXiv preprint\narXiv:2111.02114.\nSheng Shen, Shijia Yang, Tianjun Zhang, Bohan Zhai,\nJoseph Gonzalez, Kurt Keutzer, and Trevor Dar-\nrell. 2022. Multitask vision-language prompt tuning.\nArXiv, abs/2211.11720.\nAmanpreet Singh, Ronghang Hu, Vedanuj Goswami,\nGuillaume Couairon, Wojciech Galuba, Marcus\nRohrbach, and Douwe Kiela. 2021. Flava: A foun-\ndational language and vision alignment model. 2022\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 15617‚Äì15629.\nHao Tan and Mohit Bansal. 2019. Lxmert: Learning\ncross-modality encoder representations from trans-\nformers. pages 5099‚Äì5110. Association for Compu-\ntational Linguistics.\nTristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet\nSingh, Adina Williams, Douwe Kiela, and Candace\nRoss. 2022. Winoground: Probing vision and lan-\nguage models for visio-linguistic compositionality.\nIn Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 5238‚Äì\n5248.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, ≈Å ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nTu Vu, Brian Lester, Noah Constant, Rami Al-Rfou‚Äô,\nand Daniel Cer. 2022. SPoT: Better frozen model\nadaptation through soft prompt transfer. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 5039‚Äì5059, Dublin, Ireland. Association\nfor Computational Linguistics.\nXiaolong Wang and Abhinav Gupta. 2018. Videos as\nspace-time region graphs. In ECCV.\nZifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang,\nRuoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot,\nJennifer Dy, and Tomas Pfister. 2022. Learning to\nprompt for continual learning. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 139‚Äì149.\nBingjie Xu, Yongkang Wong, Junnan Li, Qi Zhao, and\nM. Kankanhalli. 2019. Learning to detect human-\nobject interactions with knowledge. 2019 IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition (CVPR), pages 2019‚Äì2028.\nDanfei Xu, Yuke Zhu, Christopher B. Choy, and Li Fei-\nFei. 2017. Scene Graph Generation by Iterative Mes-\nsage Passing. In CVPR, pages 3097‚Äì3106.\nJingkang Yang, Yi Zhe Ang, Zujin Guo, Kaiyang Zhou,\nWayne Zhang, and Ziwei Liu. 2022. Panoptic scene\ngraph generation. In European Conference on Com-\nputer Vision.\nFei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua\nWu, and Haifeng Wang. 2021. Ernie-vil: Knowledge\nenhanced vision-language representations through\nscene graphs. Proceedings of the AAAI Conference\non Artificial Intelligence, 35(4):3208‚Äì3216.\nMert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri,\nDan Jurafsky, and James Zou. 2023. When and why\nvision-language models behave like bags-of-words,\nand what to do about it? In International Conference\non Learning Representations.\nTiancheng Zhao, Tianqi Zhang, Mingwei Zhu, Haozhan\nShen, Kyusong Lee, Xiaopeng Lu, and Jianwei Yin.\n2022. Vl-checklist: Evaluating pre-trained vision-\nlanguage models with objects, attributes and relations.\narXiv preprint arXiv:2207.00221.\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and\nZiwei Liu. 2022a. Conditional prompt learning for\nvision-language models. 2022 IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition\n(CVPR), pages 16795‚Äì16804.\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and\nZiwei Liu. 2022b. Conditional prompt learning\nfor vision-language models. In IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition\n(CVPR).\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\nMohamed Elhoseiny. 2023. Minigpt-4: Enhancing\nvision-language understanding with advanced large\nlanguage models. arXiv preprint arXiv:2304.10592.\n14088\nSupplementary Material for ‚ÄúSGVL‚Äù\nHere we provide additional information about our\nexperimental results, qualitative examples, imple-\nmentation details, and datasets. Specifically, Sec-\ntion A provides more experiment results, Section B\nprovides more additional method details, Section C\nprovides qualitative visualizations to illustrate our\napproach, and Section D provides additional imple-\nmentation details.\nA Additional Experiment Results\nWe begin by presenting several additional ablations\n(Section A.1) that further demonstrate the benefits\nof our SGVL approach. Next, we present the BLIP\nmodel ablations for all datasets (Section A.2). Last,\nwe present additional results (Section A.3).\nA.1 Additional Ablations\nIn what follows, we provide additional ablations\nthat further illustrate the benefits of SGVL. For\nall ablations, we finetuned the BLIP model using\nthe same recipe, in which only LoRA adapters\nare learned, and the training batch consists of VG\nimage-SG pairs and LAION image-text pairs.\nThe importance of the SG data . To examine\nthe significance of the information provided by\nscene graphs, we suggest learning SGs without any\nuseful information. Thus, we run an experiment\nin which the SGs are completely random. This\nablation obtains on Winoground 37.8/18.7/14.0\ncompared to our BLIP-SGVL 42.8/27.3/21.5 for\nText/Image/Group scores (the BLIP baseline ob-\ntains 39.0/19.2/15.0). This illustrates that the ap-\nproach we employ is not merely a regularization,\nbut also provides important information about the\nSGs that can be used by pretrained VLMs.\nThe effect of image-SG data size. In this experi-\nment, we train our method using varying amounts\nof image-SG pairs of data (10%, 40%, 70%, and\n100% of the dataset) in order to examine the effect\nof the data portion. Figure 5 shows the Winoground\ngroup score performance as a function of the image-\nSG pairs data portion. As can be seen, the positive\nslope suggests that adding image-SG data consis-\ntently improves results.\nTraining with negatives from non-graph data .\nTo demonstrate the importance of structured in-\nformation in textual descriptions, we examine the\nperformance of the model when only LAION cap-\ntions are used. Specifically, we trained using gen-\nerated negatives that were not derived from SG\n0.25 0.5 0.75 1\n17.5\n18\n18.5\n19\n19.5\n20\n20.5\n21\n21.5\nSG Data Amount\nGroup Score\nFigure 5: Image-SG Pair Data Size. We report the perfor-\nmance of our model on Winoground group score as a function\nof the amount of scene-graph data used during training (per-\ncentage of the available data).\ndata but were generated in a manner that approx-\nimated our graph-based negatives. Since we do\nnot have the graphs in this setup, we apply the fol-\nlowing augmentations: (i) Swapping asymmetric\nrelations - We swap the nouns that are relevant to\nthe relation by using a standard parser. (ii) Re-\nlation falsification - The relation is replaced with\none from a closed set of relations we manually\nannotated in order to obtain the wrong semantic\nmeaning. (iii) Attributes swapping - We swap at-\ntributes from a closed set of attribute categories\nthat we manually annotated (e.g., color, etc.). The\nText/Image/Group scores compared to the BLIP\nbaseline are +4.0/-0.7/-0.8 while using BLIP with\nour graph-based augmentations (without SG to-\nkens) obtains +1.5/+6.3/+4.0 compared to the BLIP\nbaseline. It can be seen that the generated negatives\nfrom LAION improve only the Text score while\napplying our graph-based negatives improves all\nthe metrics. This indicates that the main reason\nfor the improvement is the structured information\ncontained in the descriptions generated from the\nscene graphs.\nScene graph token representations. To analyze\nwhat the scene graph tokens learned, we can eval-\nuate the ability of object and relationship tokens\nto be utilized explicitly for the auxiliary task as\na simple SG predictor in images. This is accom-\nplished by predicting the scene graphs on Visual\nGenome based on the learned SG tokens. We com-\npared the learned SG tokens with a BLIP model\nextended with object and relationship heads, as\nexplained in BLIP-MT variant in the main paper\n(See Section 4.5). Our model achieved an mAP of\n17.7, while the BLIP MT achieved an mAP of 14.4.\nThese results indicate that the scene graph tokens\nlearn meaningful and useful representations.\nToken specialization. Our SGVL approach learns\na different specialization for each scene graph to-\nken in Figure 6. We visualize the bounding box\ncenter coordinates predicted by 10 different object\n14089\nFigure 6: Tokens Specialization. We visualize the box predictions of 10 random object tokens (left) and 7 relationship tokens\n(right) on all images from the COCO validation set. Each box is represented as a point with the normalized coordinates of its\ncenter. Colors indicate the predictions made by different tokens.\nWinoground VL-Checklist ARO VSR\nAll Dataset All Datasets Avg Flickr30K COCO All Dataset\nModel Text Image group Attribute Object Relation Order Order Avg\nBLIP 39.0 19.2 15.0 75.2 82.2 81.5 27.9 24.9 56.5\nBLIP + Graph Text (GT) 40.3 20.5 16.5 76.0 80.8 77.5 28.0 24.6 57.2\nBLIP + GT + Graph Neg. (GN) 40.5 25.5 19.0 80.0 84.0 81.2 69.6 70.0 61.4\nBLIP + GT + GN + SG Tokens 42.8 27.3 21.5 81.8 85.2 81.9 70.0 71.0 62.4\nTable 5: Scene graph utilization ablation results for all datasets.\nWinoground VL-Checklist ARO VSR Graph Pred. ZS\nAll Dataset All Datasets Avg Flickr30K COCO All Dataset mAP 21 Tasks\nModel Text Image groupAttribute Object Relation Order Order Avg Score Avg\nBLIP 39.0 19.2 15.0 75.2 82.2 81.5 27.9 24.9 56.5 - 49.0\nBLIP + SG Tokens 39.8 26.5 20.0 81.0 84.5 81.2 67.6 67.0 61.9 16.1 47.5\nBLIP + Adaptive SG Tokens42.8 27.3 21.5 81.8 85.2 81.9 70.0 71.0 62.4 17.7 48.0\nTable 6: Adaptive SG tokens results for all datasets.\ntokens and 7 random relationship tokens for all im-\nages in the COCO val set. We observe that these\ntokens are specialized in different locations in the\nimage, whereas the relationship tokens tend to be\ncentered since their boxes are larger and spread\nover a greater area.\nA.2 Ablations on All Datasets\nThe main paper only includes ablation results for\nWinoground in Table 4a and in Table 4b. Here,\nin Table 5 and Table 6, we include additional abla-\ntion results. Specifically, we have performed a com-\nprehensive ablation using our SGVL approach with\nthe BLIP model on all datasets: Winoground, VL-\nChecklist, ARO, and VSR. It is evident from Ta-\nble 5 that the utilization of our approach for both vi-\nsual and textual components is important. Last, Ta-\nble 6 shows that the ‚ÄúAdaptive SG Tokens‚Äù improve\nSG prediction and VL performance for all datasets\nwith only a mild zero-shot degradation.\nA.3 Additional Results\nWe start by presenting zero-shot classification re-\nsults tested with linear probing. Next, we show\nadditional results when finetuning image-text pairs\nfrom COCO in Table 7. Moreover, we show addi-\ntional results on all splits in Winoground in Table 8,\nas suggested in Diwan et al. (2022). Last, we also\npresent fine-grained results of NegCLIP, NegBLIP,\nand NegBLIP2 baselines on VL-Cheklist, and VSR\ndatasets in Table 9 and Table 10. For all tables, the\ndifference between the baselines and our SGVL\napproach is denoted by gains (+X) and losses (-X).\nAs discussed in our paper, we believe that the\nslight degradation in zero-shot classification is due\nto fine-tuning with negative captions and graph pre-\ndiction tasks that deviate from the original VLM\ntraining objective. This phenomenon is also visible\n14090\nWinoground VL-Checklist ARO VSR\nAll Dataset All Datasets Avg Flickr30K COCO All Dataset\nModel Text Image group Attribute Object Relation Order Order Avg\nCLIP-SGVL (COCO) 32.8 11.8 10.0 73.5 83.3 81.0 83.0 79.4 -\nBLIP-SGVL (COCO) 43.8 26.0 21.3 81.9 85.5 82.0 70.7 71.2 61.7\nBLIP2-SGVL (COCO) 46.0 26.5 22.8 81.2 88.8 88.8 77.4 78.2 63.0\nTable 7: Winoground, VL-Checklist, ARO, VSR Results when finetuning on COCO image-text pairs instead of LAION\n.\nin recent works (Doveh et al., 2022; Yuksekgonul\net al., 2023). Here, we also report on the classifi-\ncation performance achieved with linear probing.\nFor CLIP-SGVL, when using 5-shot, 10-shot lin-\near probing, the difference on classification tasks\nwith respect to CLIP (in the same setting) changes\nto -0.3%, +0.1% and with 20-shot to +0.5%. This\ndemonstrates that partial fine-tuning does not cause\ndegregation as previously observed in other studies.\nIn order to demonstrate that our method can also\nbe used with image-text pairs other than LAION,\nwe report in Table 7 results when used with image-\ntext pairs from COCO (Lin et al., 2014). It can be\nseen that the results are comparable to our origi-\nnal version, which indicates the flexibility of our\napproach.\nTable 8 shows the performance on fine-grained\nsplits on Winoground. It can be observed that our\nmethod generally outperforms or is comparable to\npretrained models for most splits. Although we\npresent all splits, we note that Diwan et al. (2022)\nsuggested that only the samples from the NoTag\nsplit are actually probing compositionality, while\nthe other splits are difficult to solve for various\nadditional reasons. Thus, the NoTag is the most\nimportant split to evaluate.\nTable 9 shows the performance on fine-grained\nsplits of the VL-Checklist dataset, including the\n‚ÄúAttribute‚Äù, ‚ÄúObject‚Äù, and ‚ÄúRelation‚Äù splits. As can\nbe seen, our method improves both the pretrained\nbaselines CLIP, BLIP, and BLIP2, as well as the\nNegCLIP, NegBLIP, and NegBLIP2 baselines on\nthose fine-grained splits.\nTable 10 shows the performance on fine-grained\nsplits of the VSR dataset, which test spatial un-\nderstanding in VLMs. Similarly, we find that our\nmethod improves on most splits and across models\nwhen compared with pretrained and Neg baselines.\nOverall, our approach improves multiple archi-\ntectures (CLIP, BLIP, and BLIP2) on a variety of\nVL datasets with only mild degradation in zero-\nshot performance.\nB Additional Modeling Details\nWe begin by presenting additional model details\nregarding our graph preprocessing procedure (Sec-\ntion B.1). Next, we describe our method for creat-\ning graph-based negatives (Section B.2), which is\nillustrated in Figure 7. We provide more details on\nour scene graph loss (Section B.3) and some modi-\nfications made to our loss calculation when training\nBLIP (Li et al., 2022b) and BLIP2 (Li et al., 2023)\n(Section B.4). We conclude by describing more in\ndetail our approach when using the BLIP2 model\n(Section B.5).\nB.1 Graph Preprocessing\nWe next describe how we process the image-SG\npairs to create our training dataset. Our guiding\nprinciple is to create image-SG training pairs where\nthe graphs are dense enough but not too large, in\norder to allow structured and short descriptions. To\nthis end, given an image I and a corresponding\ngraph G(V,E), we extract the sub-graphs by tak-\ning a random walk on the graph. The random walk\nis initialized by randomly picking a relationship\nfrom the graph (edge e‚ààEand nodes v1,v2 ‚ààV\nsuch that e= (v1,v2)) and ends when a node that\nhas no outgoing edges is reached, resulting in a sub-\ngraph G1 = (V1,E1). Next, the image is cropped\nto the union of the bounding boxes of all objects\n(v ‚ààV1) in the extracted sub-graph, resulting im-\nage I1. We finish the process by adding new nodes\nand relationships to G1 from the residual graph\nGr = (V/V1,E/E1) that are visible in I1. We use\nG1 and I1 as a training sample only if the derived\nG1 contains at most 10 objects (i.e. |E1|‚â§ 10).\nThis process creates SGs composed of connected\ncomponents that are all DAGs with a single Hamil-\ntonian path, which facilitates caption generation.\nB.2 Graph-Based Negatives\nIn order to generate negative image captions, we\npropose a set of predefined rules that when ap-\nplied to an image-scene graph pair, result in a\n14091\nAmbiguosly Unusual Unusual Non Visually Complex No\nCorrect Image Text Compositional Difficult Reasoning Tag\nModel T I G T I G T I G T I G T I G T I G T I G\nCLIP 30.4 15.2 13.0 25.0 8.9 5.4 30.0 16.0 10.0 76.7 36.7 33.3 15.8 0.0 0.0 24.4 7.7 3.8 30.4 11.1 8.2\nBLIP 39.1 17.4 15.2 37.5 16.1 14.3 30.0 14.0 8.0 50.0 33.3 30.0 29.0 10.5 10.5 24.4 7.7 2.6 44.8 23.8 19.2\nBLIP2 41.3 28.3 19.6 30.4 17.9 14.3 38.0 14.0 12.0 53.3 26.7 26.7 34.2 13.1 10.5 20.5 7.6 2.5 50.0 32.0 26.7\nNegCLIP 28.2 4.3 4.3 17.9 7.2 3.6 36.0 8.0 8.0 66.7 30.0 26.7 10.5 2.6 2.6 21.8 7.7 5.1 33.2 12.2 8.7\nNegBLIP 43.5 21.7 8.7 42.9 19.6 14.3 28.0 18.0 12.0 56.7 43.3 33.3 28.9 15.8 10.5 32 10.2 3.8 47.0 27.3 22.6\nNegBLIP2 41.3 23.9 19.6 35.7 17.8 17.8 36.0 14.0 14.0 63.3 33.3 30.0 23.7 15.8 13.1 24.3 9.0 6.4 49.4 34.3 27.9\nCLIP-SGVL (ours) 37.0 13.0 8.7 28.6 10.7 7.1 32.0 10.0 8.0 70.0 40.0 30.0 18.4 5.3 5.3 24.4 16.7 12.8 33.2 14.0 8.7\nBLIP-SGVL (ours) 45.6 21.7 21.7 44.6 23.2 21.4 38.0 24.0 18.0 46.7 43.3 40.0 23.7 18.3 15.8 33.3 11.2 7.7 45.9 34.3 25.6\nBLIP2-SGVL (ours) 43.5 26.1 23.9 39.3 26.8 21.4 34.0 18.0 16.0 60.0 46.7 46.7 26.3 18.4 15.3 29.5 11.5 6.4 51.7 37.2 29.0\nTable 8: Results on Winoground for all the splits presented in Diwan et al. (2022). We report T, I, and G for Text retrieval,\nImage retrieval, and Group retrieval.\nAttribute Object Relation\nModel Action Color Material Size State Location Size Action\nCLIP 68.1 70.2 73.1 52.9 63.3 81.0 80.1 78.0\nBLIP 79.5 83.2 84.7 59.8 68.8 83.0 81.3 81.5\nBLIP2 81.0 86.2 90.3 61.7 70.1 85.4 84.3 84.9\nNegCLIP 66.7 74.9 78.4 54.8 63 81.9 80.9 81.3\nNegBLIP 82.6 88.3 89.3 60.8 70.1 81.0 79.6 83.0\nNegBLIP2 82.1 88.7 90.7 63.0 70.0 87.2 86.8 88.2\nCLIP-SGVL (ours) 76.6 (+8.5) 78.7 (+8.5) 81.3 (+8.2) 59.7 (+6.8) 62.0 (-1.3) 83.2 (+2.2) 82.0 (+1.9) 81.3 (+3.3)\nBLIP-SGVL (ours) 79.2 (-0.3) 94.5 (+11.3) 91.9 (+7.2) 73.3 (+13.5) 70.0 (+1.2) 86.4 (+3.4) 83.9 (+2.6) 81.9 (+0.4)\nBLIP2-SGVL (ours) 82.4 (+1.4) 91.7 (+5.5) 92.2 (+1.9) 70.1 (+8.4) 69.6 (-0.5) 89.0 (+4.6) 87.6 (+3.3) 88.8 (+3.9)\nTable 9: VL-Checklist Results. We report VL-Checklist (Zhao et al., 2022) on all splits of the Attribute, Object, and Relation\ntests, excluding the Visual Genome dataset. The difference between base models and SGVL is denoted by (+X).\nModel Adjacency Directional Orientation Projective Proximity Topological Unallocated Average\nBLIP 55.4 48.9 53.7 58.2 56.5 55.6 63.4 56.5\nBLIP2 56.2 47.9 59.8 62.5 55.8 66.7 66.3 61.9\nNegBLIP 56.0 48.0 55.7 59.0 55.6 58.7 63.2 57.8\nNegBLIP2 56.3 48.0 57.4 61.5 55.8 67.8 70.5 61.2\nBLIP-SGVL (ours) 57.7 (+2.3) 54.1 (+5.2) 57.8 (+4.1) 63.8 (+5.6) 57.8 (+1.3) 64.8 (+8.8) 68.8 (+5.4) 62.4 (+5.9)\nBLIP2-SGVL (ours) 59.8 (+3.6) 56.9 (+9.0) 58.5 (-1.3) 61.5 (-1.0) 59.7 (+3.9) 70.0 (+3.3) 66.8 (+0.5) 63.4 (+1.5)\nTable 10: VSR Results. We report accuracy on all splits of the VSR (Liu et al., 2022) dataset.\nnegative scene graph that incorrectly describes the\nimage. Our scene graph to caption scheme trans-\nforms negative scene graphs into negative captions\nthat are semantically inconsistent with the images\nthey accompany. For each training sample we ran-\ndomly apply one of the following negative rules\nfocused on object attributes and relationships in\nthe graph: (i) asymmetric relations swapping - we\ncall a relationship R asymmetric, if for two ob-\njects a,b, aRb =‚áí ¬¨bRa. We manually anno-\ntated a relation as asymmetric out of the 300 most\ncommon VG relations. We use these to generate\na negative scene graph by searching for an edge\ne= (v1,v2) ‚ààErepresenting an asymmetric rela-\ntionship, and modify the graph by replacing ewith\nan edge en = (v2,v1). For example, in the case\nof a graph describing the phrase ‚Äúdog chasing cat‚Äù,\nsuch a negative will result in the phrase ‚Äúcat chasing\ndog‚Äù. (ii) relation falsification - we replace rela-\ntions in the graph with false relations. For example,\nwe turn ‚Äúcup on table‚Äù to ‚Äúcup under table‚Äù. For\nnegatives focused on object attributes, we first scan\nthe dataset and split the attributes into the following\ncategories: color, material, size, state. Next, we\nuse this split to perform two types of negatives: (i)\nattributes falsification - we replace attributes for ob-\njects in the graph with false attribute from the same\ncategory. For example, turning ‚Äúblue ball‚Äù to ‚Äúred\nball‚Äù. (ii) attributes swapping - we search the graph\nfor two objects that are annotated with attributes\nfrom the same category. Given that such a pair has\nbeen found we switch between the attributes, result-\n14092\ning for example, ‚Äúsilver spoon and golden knife‚Äù\nfrom ‚Äúgolden spoon and silver knife‚Äù.\nB.3 Scene Graph Loss\nThis section provides a detailed explanation of our\nScene Graph loss, as mentioned in the main paper\nin Section 3.4.\nAs explained in the paper (See Section 3.4), we\nincorporate SG tokens into the model, which are\nused to predict the SG that corresponds to the im-\nage. We next explain this process. The graph G\ncontains several annotations: the set of object cat-\negories O = {oi}n\ni=1, the set of bounding boxes\nBO = {bo\ni}n\ni=1, and set of relationship categories\nR = {ri}m\ni=1. We also augment the relationships\nwith a set of bounding boxes, that are constructed\nas the union of the boxes of the corresponding ob-\njects, and denote these by BR = {br\ni}m\ni=1.\nAs we use an open vocabulary approach, we do\nnot aim to predict the object and relationship cate-\ngories directly, but rather use the embeddings from\nthe VL model. Thus, we extract the embeddings\nof these labels with the text encoder ET to get\nclass embeddings: ÀúO = ET(O) ‚ààR(n+1)√ód, and\nÀúR = ET(R) ‚ààR(m+1)√ód. We note that (n+ 1)\nand (m+ 1)classes are due to ‚Äúno object‚Äù and ‚Äúno\nrelationship‚Äù categories (denoted as ‚àÖ), which are\nrepresented with learned embeddings.\nThus far we described the elements of the SG\nthat we aim to predict from the SG tokens. We next\ndescribe the prediction process, and the correspond-\ning losses. The image encoder outputs a set of Àún\nobject queries and Àúmrelationship queries. We ap-\nply two feed-forward networks, FFNbb and FFNe,\non top of these queries to predict bounding boxes\nand label embeddings respectively. Specifically,\ngiven the final representations of jth object token\nand kth relationship token for image I, which we\ndenote Fj\nO(I) and Fk\nR(I) respectively, we predict:\nÀÜbj = FFNbb(Fj\nO(I)) , ÀÜej = FFNe(Fj\nO(I)) (7)\nÀÜbk = FFNbb(Fk\nR(I)) , ÀÜek = FFNe(Fk\nR(I)) (8)\nwhere ÀÜbj, ÀÜbk ‚ààR1√ó4 are bounding box predictions\nand ÀÜej, ÀÜek ‚ààR1√ód are class embeddings predic-\ntions. Next, we use the class embeddings matrices\nto predict probabilites overn+1 and m+1 classes:\nÀÜqo\nj = SoftMax(ÀÜej ÀúOT) (9)\nÀÜqr\nk = SoftMax(ÀÜek ÀúRT) (10)\nwhere ÀÜqo\nj ‚ààR1√ón+1 and ÀÜqr\nk ‚ààR1√óm+1. Next, we\nneed to match the predictions of the SG tokens with\nthe ground-truth SG, in order to determine which\nSG tokens correspond to which ground-truth ob-\njects and relationships. We follow the matching\napproach as in DETR (Carion et al., 2020), ex-\ncept that in our case, objects and relationships are\nmatched separately. We describe the object match-\ning below. Given a permutation œÉover the object\ntokens, we define the matching-cost between the\npermutation and the GT by:\ns(œÉ) =\nÀún‚àë\ni=1\n[\n1 {oiÃ∏=‚àÖ}ÀÜqo\nœÉ(i)(oi) +\n1 {oiÃ∏=‚àÖ}\n(\nLgiou(bo\ni,ÀÜbœÉ(i)) +‚à•bo\ni ‚àíÀÜbœÉ(i)‚à•1\n)]\n(11)\nNamely, we check the compatibility between the\nGT and permuted objects both in terms of object\ncategory (i.e., the probability assigned by the query\nto the GT objectoi) and in terms of bounding boxes\n(i.e., how well the predicted box matches the GT\none). Here Lgiou is from (Rezatofighi et al., 2019).\nThe optimal matching ÀÜœÉis found by optimizing\nthis score: ÀÜœÉ= arg minœÉ‚ààŒ£ s(œÉ). Finally, we use\nthe optimal matching ÀÜœÉfrom above to calculate the\nfollowing Objects loss:\nLObjects =\nÀún‚àë\ni=1\n[\n‚àílog ÀÜqo\nÀÜœÉ(i)(oi) +\n1 {oiÃ∏=‚àÖ}\n(\nLgiou(bo\ni,ÀÜbÀÜœÉ(i)) +‚à•bo\ni ‚àíÀÜbÀÜœÉ(i)‚à•1\n)]\n(12)\nThe relation matching and loss LRel is calculated\nin a similar manner, and the total scene graph loss\nis the sum of LObj and LRel:\nLSG := LObj + LRel (13)\nB.4 BLIP Image-Text Loss Details\nBesides image and text unimodal encoders trained\nusing a contrastive loss, BLIP (Li et al., 2022b)\nand BLIP2 (Li et al., 2023) also includes an image-\ngrounded text encoder that uses additional image-\ntext cross-attention layers. The encoder is equipped\nwith a binary classification head (a linear layer) and\nis trained to predict whether an image-text pair is\npositive (matching) or negative (unmatching). In\nthe training procedure described by the authors, the\nencoder uses a hard negative mining strategy to cal-\nculate an additional loss for all image-text pairs in\nthe batch. When training our BLIP/BLIP2-SGVL\nmodels, we apply this loss as well. Additionally,\n14093\nwe use this encoder to add another term to our\ngraph-based negative loss (LGN). Let EP\nIT(I,T )\ndenote the positive score given by the encoder to\nsome image-text pair ( I,T ), then the following\nterm is added to LGN:\n‚àë\nIG,TP ,TN\n‚àílog\n(\neEP\nIT (IG,TP )\neEP\nIT (IG,TP ) + eEP\nIT (IG,TN)\n)\n(14)\nwith IG,TP,TN denoting the VG images, positive\nand negative captions in the batch, respectively.\nB.5 BLIP2 Model Details\nIn this section, we describe how we incorporate our\n‚ÄúAdaptive SG Tokens‚Äù into the BLIP2 (Li et al.,\n2023) model. Recall that the BLIP2 model ar-\nchitecture is based on a Q-Former module that\nconsists of two transformer sub-modules: (a) an\nimage transformer that interacts with a frozen im-\nage encoder to produce the visual features used\nfor contrastive learning; and (b) a text transformer\nthat performs both the functions of a text encoder\n(producing the textual features required for con-\ntrastive learning) and a text decoder. The inputs\nto the image transformer sub-module are a set of\nlearnable query embeddings. These queries inter-\nact with each other through self-attention layers, as\nwell as with frozen image encoder features through\ncross-attention layers. When applying our SGVL\napproach to BLIP2, we add our SG tokens as an\nadditional set of prompts in parallel to the learnable\nqueries. Therefore, our SG tokens interact with the\nlearnable queries and the frozen image encoder fea-\ntures. We apply our adaptation technique to these\ntokens as we do in BLIP and CLIP (see Adaptive\nSG Tokens in section 3.3) and use the exact same\nprocedure for the graph prediction task.\nC Qualitative Visualizations\nFigure 7 shows a visualization of the generation\nprocess of captions, including positive captions\nas well as negative captions, based on our Graph-\nbased Negatives module. As shown in the figure,\ncaptions generated from scene graphs are much\nmore focused on describing fine-grained details.\nFurthermore, we show in Figure 8 visualizations of\nscene graph tokens predictions for images from Vi-\nsual Genome, which the model was not trained on.\nIt can be seen that although the model has not been\ntrained on these images, the predictions are reason-\nable. Finally, we show in Figure 9 and Figure 10\nerror analysis on Winoground and VL-Checklist to\nevaluate the success and errors of our method and\nthe baselines. This illustrates which examples our\nBLIP-SGVL model is successful on, in contrast to\nthe BLIP model.\nD Additional Implementation Details\nOur SGVL approach can be used on top of a va-\nriety of VL models. For our experiments, we\nchoose the CLIP (Radford et al., 2021), BLIP (Li\net al., 2022b) and BLIP2 (Li et al., 2023) mod-\nels as they are among the most popular and easy-\nto-use methods. These models are implemented\nbased on the Open-CLIP library (Ilharco et al.,\n2021) and the BLIP/BLIP2 code base (available at\nhttps://github.com/salesforce/LAVIS). We\nimplement SGVL based on these repositories. As\ndescribed above, our approach is trained using both\nthe original image-text pairs from LAION and the\nimage-SG pairs we curate from Visual Genome. In\nparticular, for CLIP-SGVL we use 3M image-text\npairs, while for BLIP/BLIP2-SGVL, we use 750K\ndue to computational constrains.\nIn our experiments, we trained our CLIP-SGVL\non 4 V100 GPUs for 32 epochs with a batch com-\nprised of 256 image-text pairs and 8 image-SG\npairs. We use AdamW optimizer (Kingma and Ba,\n2014; Loshchilov and Hutter, 2017) with Œ≤1 = 0.9,\nŒ≤2 = 0.98, and œµ = 1e‚àí6. We use lr = 5e‚àí5\nwith cosine scheduler, and a weight decay of 0.2\nfor regularization. For BLIP-SGVL, we trained on\n4 V100 GPUs and for BLIP2-SGVL on 4 A100\nGPUs. We train both for 8 epochs with a batch\ncomprised of 32 image-text pairs and 8 image-SG\npairs. We use AdamW optimizer (Kingma and Ba,\n2014) with Œ≤1 = 0.9, Œ≤2 = 0.99, and œµ= 1e‚àí8.\nWe use lr = 5e‚àí5 with cosine scheduler, and a\nweight decay of 0.05 for regularization.\nD.1 VL-Checklist\nDataset. VL-Checklist (Zhao et al., 2022) is a new\nstudy that combines the following datasets: Visual\nGenome (Krishna et al., 2017), SWiG (Pratt et al.,\n2020), V AW (Pham et al., 2021), and HAKE (Li\net al., 2019). For each image, two captions are\ngiven, a positive and a negative. The positive cap-\ntion is derived from the source dataset and is co-\nherent with the visual structure of the image. the\nnegative caption is constructed by modifying one\nword in the positive caption that corresponds to a\nstructural aspect in the image. To correctly solve\n14094\nman\nwhite\tfrisbeethrowingtreesingreen\tpark\nImage\t&\tScene\tGraph\tPair\nGraph\tNegatives Scene\tGraph\tto\tText\n‚Äútreesingreen\tpark.\tMan\tthrowing\twhite\tfrisbee\"\n‚Äútreesingreen\tpark.\tWhite\tfrisbee\tthrowing\tman\"\n‚Äútreesinwhite\tpark.\tman\tthrowing\tgreen\tfrisbee\"\nman\nwhite\tfrisbeethrowingtreesingreen\tpark\nman\nwhitefrisbeethrowingtreesingreenpark\nAsymettricalRelations\nAttributes\tBinding\nFigure 7: Visualization of some of our Graph-based Negatives as well as the SG-to-Text module. We show the generation\nprocess of positive captions (green) and negative captions using the graph (red).\na sample the model needs to identify the caption\nfaithfully describing the image. Specifically, VL-\nChecklist evaluates the following structured con-\ncepts: (1) Object: identifying whether objects men-\ntioned in the text appear in the image invariantly\nto their spatial location and size, (2) Relation: spa-\ntial or action relation between two objects, and (3)\nAttribute: color, material, size, state, and action\nbounded to objects. We report results on a com-\nbined VL-Checklist dataset excluding VG.\nInference details. We use the official data and code\nreleased by the authors which is available athttps:\n//github.com/om-ai-lab/VL-CheckList . A\ntest sample consists of an image and two captions.\nFor CLIP, we compute the cosine similarity be-\ntween the image and the captions and report the\npositive caption as the one with the higher similar-\nity. For BLIP/BLIP2 we use the ITM head, which\npredicts both a positive and negative score for each\npair. We consider the caption with the higher posi-\ntive score to be the correct one.\nD.2 Winoground\nDataset. Winoground (Thrush et al., 2022) is a\nnew challenging dataset that evaluates the ability\nof VL models to capture compositionality in vi-\nsion & language. The dataset contains 1600 tests\nacross 400 samples. Each sample is composed of\ntwo image-text pairs (I0,C0),(I1,C1). The pairs\nhave overlapping lexical content but are differenti-\nated by a swapping of an object, a relation, or both.\nTo correctly solve the sample the model needs to\ncorrectly solve two text retrieval and two image\nretrieval tasks. A recent study (Diwan et al., 2022)\nhas shown that solving Winoground requires not\njust compositional understanding but also other\nabilities such as commonsense reasoning. The\nstudy proposed a new split to the dataset differ-\nentiating the samples by the source of their hard-\nness. Specifically, the split of the samples into the\nfollowing categories is as follows: Non Composi-\ntional - There are 30 samples in this category that\ndo not require compositional reasoning. Visually\nDifficult - The model must be able to detect an item\nthat is visually difficult to identify (small, blurry,\nin the background, etc.) in order to sort these sam-\nples correctly. This category includes 38 samples.\nAmbiguously Correct - This category includes 46\nsamples where at least one caption accurately de-\nscribes both images or doesn‚Äôt quite describe any of\nthe images. Unusual Text & Unusual Image- There\nare 106 samples in these categories, all of which\ncontain unrealistic or awkward texts or images that\nmake it difficult to solve them with a VL model.\nComplex Reasoning - This category consists of 78\nsamples that require common sense reasoning or\nknowledge of the world around us. No Tag - These\nare vanilla Winoground examples that solely probe\ncompositional understanding.\nInference details . We use the official data\nand code released by the authors which is avail-\nable at https://huggingface.co/datasets/\nfacebook/winoground. For testing, the pairs are\ngiven, and a text score, an image score, and a group\nscore for a sample is computed in the following\nway: The text score is 1 if and only if image I0\n14095\nhas a higher similarity to caption C0 than C1, and\nimage I1 has a higher similarity to caption C1 than\nC0. Similarly the image score is 1 if and only if\ncaption C0 has a higher similarity to image I0 than\nimage I1 and C1 has a higher similarity to imageI1\nthan image I0. The group score is 1 if and only if\nboth text and image scores are 1. Thus, the random\nchances for both the image and text score, is 1/4\nwhile for group score it is1/6. Similarities between\nimage-text pairs is computed as in section D.1.\nD.3 ARO\nDataset. ARO (Yuksekgonul et al., 2023) (Attri-\nbution, Relation, and Order) is a new benchmark\nthat tests compositionality in VL models. The au-\nthors propose four tasks that are sensitive to order\nand composition, namely Visual Genome Relation,\nVisual Genome Attribution, COCO& Flickr30k\nOrder. Since our approach is trained on Visual\nGenome, we report only the COCO and Flickr30k\norder task (PRC). For the order task, image-text\npairs from the mentioned datasets are used. The\nwords in the text are reordered in order to create\nfalse captions for the image, according to the fol-\nlowing perturbations: nouns and adjectives shuffle,\neverything but nouns and adjectives shuffle , tri-\ngrams shuffle and words within trigrams shuffle.\nInference details . We use the official data\nand code released by the authors which is\navailable at https://github.com/mertyg/\nvision-language-models-are-bows . During\ninference, each sample consists of an image and\nfive textual descriptions. The similarity of each\ntext to the image is measured as in section D.1, and\nthe text with the highest similarity to the image is\nreported as the real caption.\nD.4 VSR\nDataset. VSR (Liu et al., 2022) VSR (Visual Spa-\ntial Reasoning) is a new benchmark for measuring\nthe spatial understanding of vision-language mod-\nels. The VSR dataset consists of natural image-text\npairs in English, each example contains an image\nand a natural language description of the spatial\nrelationship between two objects shown in the im-\nage. The VL model needs to classify images and\ncaptions as either true or false, indicating whether\na caption accurately describes the spatial relation-\nship. The dataset has more than 10K image-text\nsamples, derived from 6,940 COCO images and\ncovers 65 spatial relations. The dataset is split into\na train, validation and test sets, however, since we\nevaluate in a zero-shot manner we test our model\nand baselines using all samples from the train, val-\nidation, and test splits. The spatial relations are\ndivided into 7 meta-categories: Adjacency, Direc-\ntional, Orientation, Projctive, Proximity, Topolog-\nical, Unallocated. We report results according to\nthese categories, as well as the average over all\nspatial relations.\nInference details . We use the official data\nreleased by the authors which is available\nat https://github.com/cambridgeltl/\nvisual-spatial-reasoning We do not evaluate\nCLIP on this task since the task requires assigning\na true or false label to an image-text pair. CLIP,\nhowever, does not allow this to be done in a\nstraightforward manner, and Therefore only the\nBLIP/BLIP2 models can be used. We use the ITM\nhead to determine whether the sample is true or\nfalse.\nD.5 Finetuning Datasets\nIn our work, we use the LAION dataset as ‚Äústan-\ndard‚Äù image-text pairs, along with image-SG data\npair from Visual Genome (Krishna et al., 2017)\n(VG). Visual Genome is annotated with 108,077\nimages accompanied by their corresponding scene\ngraphs. On average, images have 35 entities, 21 re-\nlationships, and 26 attributes per image. Addition-\nally, there are approximately 70K object categories\nand 40K relationship categories. In general, Vi-\nsual Genome scene graphs can be viewed as dense\nknowledge representations for images, similar to\nthe format used for knowledge bases in natural\nlanguage processing.\nD.6 Licenses and Privacy\nThe license, PII, and consent details of each dataset\nare in the respective papers. In addition, we wish to\nemphasize that the datasets we use do not contain\nany harmful or offensive content, as many other\npapers in the field also use them. Thus, we do not\nanticipate a specific negative impact, but, as with\nany Machine Learning method, we recommend to\nexercise caution.\n14096\ngreen\tfield\ngraze\ton\t\nBlack\tcows\nmarble\tstatuebehindBrick\tbuildingstuff monirailon platform\nshadowundertall\tpostdogonsurfboardBlue\twave\nmanholdingpaper\nwhite\ttruck\tback\tdoorhasgreen\ttreesside\tmirrorofcar\ngreen\ttreesalongdark\troad\nred\tbrake\tlightsof carout\tatnight\n white\ttoiletnext\ttowhite\ttoilet\nFigure 8: Scene Graph Prediction. We show the predictions of the ‚Äúscene graph tokens‚Äù on images from Visual Genome that\nwere not trained by our model.\nA\tbottle\tis\tin\twaterWater\tis\tin\tbottle There\tare\tmore\tladybugs\tthan\tflowersThere\tare\tmore\tflowers\tthan\tladybugs\nOrange\tsocks\tand\tblack\tshoesBlack\tsocks\tand\torange\tshoes\nThe\tcar\tis\tunderneath\tthe\tpersonThe\tperson\tis\tunderneath\tthe\tcar\nYoung\tperson\tplaying\tbaseball\twith\ta\tgreen\tbat\tand\tblue\tball\nYoung\tperson\tplaying\tbaseball\twith\ta\tblue\tbat\tand\tgreen\tball\nThe\tperson\twearing\ta\tsweater\tstands\tin\tfront\tof\tthe\tperson\twho\tisn‚Äôt\twearing\ta\tsweater\nThe\tperson\twho\tisn‚Äôt\twearing\ta\tsweater\tstands\tin\tfront\tof\tthe\tperson\twearing\ta\tsweater\nClothing\ton\tlinesLines\ton\tclothing\nThere‚Äôs\tone\tyellow\tand\tmany\tblue\tballsThere‚Äôs\tone\tblue\tand\tmany\tyellow\tballs\nThe\tperson\tis\ttoo\tbig\tfor\tthe\tpantsThe\tpants\tis\ttoo\tbig\tfor\tthe\tperson\nAn\told\tkisses\ta\tyoung\tpersonA\tyoung\tperson\tkisses\tan\told\tperson\nA\tcar\tsmashed\tinto\ta\ttreeA\ttree\tsmashed\tinto\ta\tcar\nThere\tare\tmore\tsnowboarders\tthan\tskiersThere\tare\tmore\tskiers\tthan\tsnowboarders\nFigure 9: Error Analysis on Winoground. We demonstrate on the left in green where our BLIP-SGVL model succeeds, while\nthe baseline BLIP model fails. On the right, in red, we can observe examples in which our BLIP-SGVL model fails. As visible,\nour model improves in samples that require understanding relations between objects, binding attributes to objects, and counting\nobjects.\n14097\nPerson\tcarry\ttennis\tracketPerson\tswing\ttennis\tracket\n Silver\tplatterGray\tplatter\nPerson\thold\tdogPerson\tfeed\tdog\nWooden\tfloorTiled\tfloor\nTall\tspoonShort\tspoon\nPerson\thold\tbottlePerson\tsmell\tbottle\nPerson\thold\tcupPerson\tspin\tcup\nPerson\twear\tbackpackPerson\tmove\tbackpackPerson\tsit\ton\tcouchPerson\twalk\ton\tcouch\nPerson\tsit\ton\tchairPerson\tsit\tat\tchair\nLying\tbananasJumping\tbananas\nStanding\tbirdKneeling\tbird\nSilver\tdispenserBright\tgreen\tdispenser\nMetal\tcell\tphonePlastic\tcell\tphone\nWooden\ttableBamboo\ttable\nShort\tlampTall\tlamp\nSmall\tbenchLarge\tbench\nEmpty\tblenderFull\tblender\nFigure 10: Error Analysis on VL-Checklist. We demonstrate on the left in green where our BLIP-SGVL model succeeds,\nwhile the baseline BLIP model fails. On the right, in red, we can observe examples in which our BLIP-SGVL model fails. True\ncaptions are in green and false captions in red. As visible, some of the samples on which our model fails are ambiguous or\nvisually difficult to solve.\n14098"
}