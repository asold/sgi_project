{
  "title": "LLMParser: An Exploratory Study on Using Large Language Models for Log Parsing",
  "url": "https://openalex.org/W4394745212",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4323018377",
      "name": "Ma, Zeyang",
      "affiliations": [
        "Concordia University"
      ]
    },
    {
      "id": "https://openalex.org/A3143576725",
      "name": "Chen An Ran",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": "https://openalex.org/A2962936908",
      "name": "Kim Dong Jae",
      "affiliations": [
        "Concordia University"
      ]
    },
    {
      "id": "https://openalex.org/A4201774554",
      "name": "Chen Tse-Hsun",
      "affiliations": [
        "Concordia University"
      ]
    },
    {
      "id": "https://openalex.org/A2076143322",
      "name": "Wang Shaowei",
      "affiliations": [
        "University of Manitoba"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3208954537",
    "https://openalex.org/W2592771984",
    "https://openalex.org/W2999614244",
    "https://openalex.org/W4309623083",
    "https://openalex.org/W4394946189",
    "https://openalex.org/W2022686119",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W3211022409",
    "https://openalex.org/W2895810692",
    "https://openalex.org/W2102632804",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W2811120218",
    "https://openalex.org/W2888115557",
    "https://openalex.org/W2762028850",
    "https://openalex.org/W2754665629",
    "https://openalex.org/W2560021099",
    "https://openalex.org/W2994865335",
    "https://openalex.org/W4284692184",
    "https://openalex.org/W2786424616",
    "https://openalex.org/W4280534475",
    "https://openalex.org/W4221079409",
    "https://openalex.org/W2755402962",
    "https://openalex.org/W4206410067",
    "https://openalex.org/W3134079112",
    "https://openalex.org/W4376142471",
    "https://openalex.org/W3203565869",
    "https://openalex.org/W2403646140",
    "https://openalex.org/W2153470728",
    "https://openalex.org/W1661413208",
    "https://openalex.org/W3194274259",
    "https://openalex.org/W4242838928",
    "https://openalex.org/W3037424089",
    "https://openalex.org/W2947815220",
    "https://openalex.org/W3217001695",
    "https://openalex.org/W2963999143",
    "https://openalex.org/W2735591903"
  ],
  "abstract": "Logs are important in modern software development with runtime information. Log parsing is the first step in many log-based analyses, that involve extracting structured information from unstructured log data. Traditional log parsers face challenges in accurately parsing logs due to the diversity of log formats, which directly impacts the performance of downstream log-analysis tasks. In this paper, we explore the potential of using Large Language Models (LLMs) for log parsing and propose LLMParser, an LLM-based log parser based on generative LLMs and few-shot tuning. We leverage four LLMs, Flan-T5-small, Flan-T5-base, LLaMA-7B, and ChatGLM-6B in LLMParsers. Our evaluation of 16 open-source systems shows that LLMParser achieves statistically significantly higher parsing accuracy than state-of-the-art parsers (a 96% average parsing accuracy). We further conduct a comprehensive empirical analysis on the effect of training size, model size, and pre-training LLM on log parsing accuracy. We find that smaller LLMs may be more effective than more complex LLMs; for instance where Flan-T5-base achieves comparable results as LLaMA-7B with a shorter inference time. We also find that using LLMs pre-trained using logs from other systems does not always improve parsing accuracy. While using pre-trained Flan-T5-base shows an improvement in accuracy, pre-trained LLaMA results in a decrease (decrease by almost 55% in group accuracy). In short, our study provides empirical evidence for using LLMs for log parsing and highlights the limitations and future research direction of LLM-based log parsers.",
  "full_text": "LLMParser: An Exploratory Study on Using Large Language\nModels for Log Parsing\nZeyang Ma\nSoftware PErformance, Analysis and\nReliability (SPEAR) Lab\nConcordia University\nMontreal, Quebec, Canada\nm_zeyang@encs.concordia.ca\nAn Ran Chen\nElectrical and Computer Engineering\nDepartment\nUniversity of Alberta\nEdmonton, Alberta, Canada\nanran6@ualberta.ca\nDong Jae Kim\nSoftware PErformance, Analysis and\nReliability (SPEAR) Lab\nConcordia University\nMontreal, Quebec, Canada\nk_dongja@encs.concordia.ca\nTse-Hsun (Peter) Chen\nSoftware PErformance, Analysis and\nReliability (SPEAR) Lab\nConcordia University\nMontreal, Quebec, Canada\npeterc@encs.concordia.ca\nShaowei Wang\nDepartment of Computer Science\nUniversity of Manitoba\nWinnipeg, Manitoba, Canada\nshaowei@cs.umanitoba.ca\nABSTRACT\nLogs are important in modern software development with runtime\ninformation. Log parsing is the first step in many log-based analyses,\nthat involve extracting structured information from unstructured\nlog data. Traditional log parsers face challenges in accurately pars-\ning logs due to the diversity of log formats, which directly impacts\nthe performance of downstream log-analysis tasks. In this paper,\nwe explore the potential of using Large Language Models (LLMs)\nfor log parsing and propose LLMParser, an LLM-based log parser\nbased on generative LLMs and few-shot tuning. We leverage four\nLLMs, Flan-T5-small, Flan-T5-base, LLaMA-7B, and ChatGLM-6B\nin LLMParsers. Our evaluation of 16 open-source systems shows\nthat LLMParser achieves statistically significantly higher parsing\naccuracy than state-of-the-art parsers (a 96% average parsing ac-\ncuracy). We further conduct a comprehensive empirical analysis\non the effect of training size, model size, and pre-training LLM\non log parsing accuracy. We find that smaller LLMs may be more\neffective than more complex LLMs; for instance where Flan-T5-base\nachieves comparable results as LLaMA-7B with a shorter inference\ntime. We also find that using LLMs pre-trained using logs from\nother systems does not always improve parsing accuracy. While\nusing pre-trained Flan-T5-base shows an improvement in accuracy,\npre-trained LLaMA results in a decrease (decrease by almost 55%\nin group accuracy). In short, our study provides empirical evidence\nfor using LLMs for log parsing and highlights the limitations and\nfuture research direction of LLM-based log parsers.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nICSE â€™24, April 14â€“20, 2024, Lisbon, Portugal\nÂ© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0217-4/24/04. . . $15.00\nhttps://doi.org/10.1145/3597503.3639150\nKEYWORDS\nLog parsing, log analysis, large language model\nACM Reference Format:\nZeyang Ma, An Ran Chen, Dong Jae Kim, Tse-Hsun (Peter) Chen, and Shaowei\nWang. 2024. LLMParser: An Exploratory Study on Using Large Language\nModels for Log Parsing. In 2024 IEEE/ACM 46th International Conference on\nSoftware Engineering (ICSE â€™24), April 14â€“20, 2024, Lisbon, Portugal. ACM,\nNew York, NY, USA, 13 pages. https://doi.org/10.1145/3597503.3639150\n1 INTRODUCTION\nSoftware logs provide developers with valuable system runtime\ninformation to understand system execution and debugging. How-\never, due to the sheer volume and complexity of logs, analyzing\nlogs manually becomes infeasible. To assist with log analysis, re-\nsearchers have proposed many automated approaches for various\ntasks such as anomaly detection [26, 72], monitoring [5, 63], and\nroot cause analysis [25, 43, 67]. Among all log analysis tasks, log\nparsing often serves as the first step of log analysis.\nThe goal of log parsing is to convert raw log data into log tem-\nplates by identifying and separating static text and variable values\nin the log messages. As shown in Figure 1, logs contain dynamic in-\nformation such as the timestamp, log level, and log message (which\ncontains static message and dynamic variable values). Log parsing\nfirst extracts consistent information among all the logs using regu-\nlar expression (e.g., timestamps and log level), and then transforms\nlogs into a more structured format (i.e., log template) by identifying\nvariables in the log message [23, 37]. For instance, the log message\nin Log 1 from Figure 1 has the log message Got assigned task\n0 and the log can be parsed to the log template Got assigned\ntask <*>with an identified variable0. The variables record system\nruntime information that can be in various forms (e.g., string, digits,\nor symbols). Including such dynamic variable values in the logs\nmakes automated log analysis difficult. Hence, log parsing is an\nessential first step in log analysis, and having low accuracy in log\nparsing results directly impacts the performance of downstream\ntasks [23, 24, 37].\n1\narXiv:2404.18001v1  [cs.SE]  27 Apr 2024\nICSE â€™24, April 14â€“20, 2024, Lisbon, Portugal Zeyang Ma, An Ran Chen, Dong Jae Kim, Tse-Hsun (Peter) Chen, and Shaowei Wang\nParsed templatesGroup ID(parsed)LogIDDateTimeLevelComponentLogTemplate1 1 17/06/0920:10:45INFOexecutor.CoarseGrainedExecutorBackendGot assigned task <*>1 2 17/06/0920:10:45INFOexecutor.CoarseGrainedExecutorBackendGot assigned task <*>2 3 17/06/0920:10:45INFOexecutor.ExecutorRunning task <*> in stage <*> (TID 0)3 4 17/06/0920:10:52INFOstorage.BlockManagerFound block rdd_2_4 locally4 5 17/06/0920:10:52INFOpython.PythonRunnerTimes: total = <*>, boot = <*>, init = <*>, finish = <*>5 6 17/06/0920:10:52INFOpython.PythonRunnerTimes: total = <*>, boot = <*>, init = <*>, finish = 05 7 17/06/0920:10:52INFOpython.PythonRunnerTimes: total = <*>, boot = <*>, init = <*>, finish = 0\nLogsGroup ID(gtruth)Log ID Log content1 1 17/06/09 20:10:45 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 01 2 17/06/09 20:10:45 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 22 3 17/06/09 20:10:45 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)3 4 17/06/09 20:10:52 INFO storage.BlockManager: Found block rdd_2_4 locally4 5 17/06/09 20:10:52 INFO python.PythonRunner: Times: total = 41, boot = 15, init = 26, finish = 14 6 17/06/09 20:10:52 INFO python.PythonRunner: Times: total = 40, boot = 7, init = 33, finish = 04 7 17/06/09 20:10:52 INFO python.PythonRunner: Times: total = 42, boot = 12, init = 30, finish = 0Log Parsing\nFigure 1: An example of log parsing and validating the result\nfrom Spark. The incorrectly parsed results are highlighted\nin red.\nDespite the importance of log parsing, effectively parsing logs\nremains a challenging task. There are many prior research that\nproposed various log parsers [12, 19, 24, 59, 76]. Yet, recent stud-\nies [30, 75] show that these approaches often fail to identify pa-\nrameters in logs, which may affect the downstream log analysis\ntasks. Recently, Large Language Models (LLMs) have demonstrated\npromising results in text-related and code-related tasks, such as\ncode understanding and generation [35, 66]. Intuitively, log is com-\nposed of both natural language and code-like variables. LLMsâ€™\nstrong ability for language translation can be potentially lever-\naged for log parsing, which can also be viewed as translating from\nlogs to log templates.\nIn this paper, we investigate the potential of using LLMs for\nlog parsing, with a focus on studying the effect of varying LLMs,\nshot sizes, and pre-training particularly when working with limited\ntraining data. We proposed LLMParser, an innovative log parser.\nLLMParsers learn from few-shot examples on how to â€œtranslateâ€ a\nlog into a log template and evaluate LLMParser using four text-to-\ntext or text generation LLMs: Flan-T5-small [10], Flan-T5-base [10],\nLLaMA-7B [58], and ChatGLM-6B [68]. We train and evaluate LLM-\nParsers using a widely-used log benchmark that contains logs data\nfrom 16 open-source systems [27, 30]. Our evaluation shows that 1)\nLLMParsers can achieve an average parsing accuracy (PA) of 0.96,\nwhich is higher state-of-the-arts parsers Drain [24], Logram [12],\nand LogPPT [33] (among them, the highest PA is 0.92). 2) Few-shot\ntuning is more effective than in-context learning, where in-context\nlearning only results in an average PA of 0.46. 3) Increasing the\nnumber of training examples may not always give better parsing\nresults; data diversity and balance may be more important. 4) More\ncomplex LLMs may not always give better results. We find that\nFlan-T5-base, which only has 240M parameters, can achieve simi-\nlar results compared to LLaMA which has 7B parameters. 5) LLM\npre-trained using logs from other systems may not always help\nimprove PA. We find opposite findings between Flan-T5-base and\nLLaMA, where LLaMA experiences a decrease in parsing accuracy\nwhile Flan-T5-base has an improved parsing result.\nWe summarize the main contributions of this paper as follows:\nâ€¢We explore the use of LLMs for log parsing, and propose LLM-\nParser, a generative LLM-based approach for log parsing. LLM-\nParser achieves a higher parsing accuracy (PA) compared to\nstate-of-the-arts.\nâ€¢We compare in-context learning and few-shot tuning and found\nthat few-shot tuning achieves a much higher PA (up to 0.96 v.s.\n0.46). We also found that few-shot tuning is efficient, which only\ntakes from one to five minutes on an NVIDIA A100 GPU.\nâ€¢We found that increasing training shot sizes may not always\nimprove PA. Future studies should explore better sampling ap-\nproaches to improve LLM-based log parsers.\nâ€¢LLMs with more parameters may not always give better PA. We\nfind that a medium-size LLM (Flan-T5-base) achieves compara-\nble performance compared to LLaMA-7B. Future studies should\nconsider the trade-off between model complexity and accuracy.\nâ€¢We find that using LLMs pre-trained using logs from other sys-\ntems may not necessarily improve PA. We saw contradictory\nresults in LLaMA and Flan-T5-base, where the parsing accuracy\nusing LLaMA decreases. Future studies are needed to explore\nthe impact and effectiveness of pre-trained log models on log\nparsing.\nPaper Organization. Section 2 discusses background and related\nwork. Section 3 provides the details of LLMParser. Section 4 shows\nexperiment setup and our implementation. Section 5 evaluates\nLLMParser. Section 6 discusses the implications of our findings.\nSection 7 discusses threats to validity. Section 8 concludes the paper.\nData Availability: We made our source code and experimental re-\nsults publicly available at: https://github.com/zeyang919/LLMParser\n2 BACKGROUND AND RELATED WORK\nIn this section, we discuss the background of Large Language Mod-\nels (LLMs) and how to optimize LLMs on specific tasks. We also\ndiscuss related work, existing log parsing approaches, and applica-\ntions that use LLMs to solve log-related tasks.\n2.1 Background\nLarge Language Models. The Large Language Models (LLMs) are\nmostly developed based on the transformer architecture [50, 58, 68].\nLLMs have made important advancements in natural language\nprocessing (NLP) by providing models that have an extraordinary\ncapacity for understanding language and producing contextually\nrelevant and semantically consistent text. LLMs are generally pre-\ntrained on a large corpus of text data from diverse sources such as\nbooks, articles, websites, and even source code. Recent studies [35,\n66] have highlighted the capability of LLMs in code recognition,\nunderstanding, and code generation.\nAs logs consist of both natural language sequences and code-like\nvariables, prior work [6, 12, 22, 36] has leveraged language models\nto analyze logs. However, it remains unclear whether LLMs can be\neffectively used for log parsing due to the varying pre-training data\nand model characteristics. Adopting LLMs for log parsing brings\npotential advances in the research area. First, LLMs are shown to\nbe very powerful on text-related tasks [35, 66], which may be able\nto achieve more accurate log parsing results. Second, LLMs are\ngeneralizable on unseen data [29, 61, 73], which may be leveraged\n2\nLLMParser: An Exploratory Study on Using Large Language Models for Log Parsing ICSE â€™24, April 14â€“20, 2024, Lisbon, Portugal\nto parse new logs without continuous retraining. Nevertheless,\nthere is a need for a comprehensive study on using LLMs for log\nparsing and what kinds of adaptions are needed for logs.\nIn-context Learning and Fine-Tuning of Large Language Mod-\nels. To adapt LLMs to specific tasks, there are two main strategies:\nin-context learning and few-shot tuning. In-context learning [4, 62]\nis a method that incorporates task-specific demonstrations directly\ninto the input (i.e., prompt) during LLMsâ€™ inference, guiding the\nmodel to generate responses in a desired manner without the need\nfor retraining/changing the modelâ€™s parameters. In-context learning\nrelies on the modelâ€™s ability to generalize from the provided demon-\nstrations to understand and execute the task at hand. On the other\nhand, fine-tuning [15, 20, 49] involves re-training the pre-trained\nLLM on a dataset tailored to the specific task, allowing the model\nto adjust its internal parameters and better align its outputs with\nthe desired outcomes. In particular, few-shot tuning [38, 46] is a\nfine-tuning method that enables LLMs to generalize from limited ex-\namples, which may facilitate the extraction of relevant information\nacross diverse log formats, including log variables, log structure,\nand semantic patterns.\nWhile in-context learning provides quick adaptability, it has\nseveral drawbacks. First, processing prompts with several demon-\nstrations every time the model parses logs can contribute to further\ncomputational costs. In-context learning prompted with few-shot\ndemonstrations requires the model to process both the target in-\nstance and all the demonstrations during each inference, leading to\nan increased inference time. Second, the context size of the modelâ€™s\ninputs limits the number of demonstrations that can be used. For\nexample, performing in-context learning with four prompts on Flan-\nT5-Base [10] exceeds its context size of 512 tokens. This limitation\nposes a challenge for LLMs to learn from a larger number of demon-\nstrations and improve their performance. Finally, selecting effective\ndemonstrations is also crucial for improving the performance of\nin-context learning, as it is sensitive to the format and order of the\nprompts [16, 44, 45].\nIn contrast, few-shot tuning does not demand continuous in-\ncontext demonstrations for every inference, which can speed up\ninference time. Moreover, using fine-tuning, we can provide more\ndiverse log examples to train the model as the tuning is already\nperformed during training. Prior studies [4, 20, 38] have also demon-\nstrated that few-shot tuning offers better accuracy at lower compu-\ntational costs. Furthermore, since few-shot tuning only involves a\nsmall number of data samples, the entire fine-tuning process can\nbe fast (e.g., only a few minutes for our approach). As a result,\nthere is no significant time overhead incurred due to fine-tuning.\nTherefore, in this paper, we utilize few-shot tuning to integrate\ndomain-specific knowledge into LLMs.\n2.2 Related Work\nWe discuss related work along two directions: log parsing and using\nLLM for other log-related tasks.\nLog Parsing. To support log parsing for large volumes of logs,\nresearchers have proposed many automated log parsing techniques.\nExisting log parsers primarily use three approaches: frequent pat-\ntern mining, log clustering, and parsing trees. (1) Frequent pattern\nmining identifies static text and variables by counting the number\nof times a pattern or sequence recurs in the logs (e.g., SLCT [59],\nLogCluster [76], Logram [12]). (2) Log clustering groups logs us-\ning clustering algorithms, thereby categorizing logs into different\ngroups (e.g., LKE [19], LogSig [56], and LenMa [53]). (3) Parsing\ntree builds a parse tree with fixed depth to guide the log group\nsearch process (e.g., Drain [24]). These studies aim to strike a bal-\nance between optimizing accuracy and the size of pre-learned data\nfor log parsing tasks. While the accuracy of log parsing has shown\nimprovement over time, recent studies [ 30, 75] reveal that tradi-\ntional algorithm-based log parsing tools primarily emphasize log\nclustering. Although these approaches achieve high grouping ac-\ncuracy, they often fail to accurately identify all the variables in\nlogs [30]. Therefore, this limitation may hinder downstream log\nanalysis tasks, such as missing some anomalies recorded by unrec-\nognized variables during log anomaly detection [37].\nThe recent rise of LLMs has brought new possibilities for im-\nproving log parsing. Le and Zhang [33] proposed LogPPT, which\nis a log parser based on a masked language model (RoBERTa [40]).\nLogPPT improved the accuracy of log parsing compared to tradi-\ntional algorithm-based log parsers. LogPPT converts the log parsing\ntask into a token classification task to classify whether a token in\nthe log is variable or static. However, this process requires more\nmanual effort in labelling every token in a log on whether or not\na token is a static text. Le and Zhang [32] further evaluated using\nChatGPT [4] to parse logs. Their study shows that ChatGPT can\nparse the logs but the accuracy is worse than LogPPT. However,\ndue to the closed-source nature of ChatGPT, the monetary cost can\nbe high, the LLM is not fine-tunable, and the stability of the LLM is\nout of the control of the developers. More importantly, logs often\ncontain sensitive data that cannot be sent to third parties.\nIn this paper, we investigate the application of text generation\nand text2text generation LLMs to tackle the log parsing task. The\nrecent advancements and ongoing research in LLMs, particularly in\ntext2text and text generation, have significantly improved their text\nunderstanding and processing capabilities [4, 9, 58]. Intuitively, log\nparsing is similar to language translation, where a log is translated\ninto a log template. This allows us to eliminate the process of split-\nting logs and manually labeling individual tokens, and the parsed\nlog template is directly obtained. We leverage four open-source\nLLMs (Flan-T5-Base [10], Flan-T5-Small [10], LLaMA-7B [58], and\nChatGLM-6B [68]) to generate the log template by inputting the\nprompt and explore the performance of the LLMs compared with\nthe state-of-the-art approaches. Furthermore, we study the limita-\ntions of LLM-based log parsers and explore the potential of using\npre-trained LLM-based log parsers.\nUsing LLMs for Other Log-related Tasks. The recent advances\nin LLMs have shown success in both natural language processing\nand code generation [ 35, 51, 66]. Since logs are semi-structured\ntext composed of natural language with some code elements, re-\nsearchers have adopted LLMs for log-related tasks [7, 34, 39]. Some\nstudies [34, 39] used LLMs for log anomaly detection. Lee et al. [34]\nproposed LAnoBERT which is an anomaly log detector. LAnoBERT\nmasked the specific word in the log and then used BERT [13] to pre-\ndict the masked word and calculate the predictive probability of the\norigin masked word. When there are large differences between the\nactual and the predicted words, the respective log is identified as an\n3\nICSE â€™24, April 14â€“20, 2024, Lisbon, Portugal Zeyang Ma, An Ran Chen, Dong Jae Kim, Tse-Hsun (Peter) Chen, and Shaowei Wang\n16 system\nlog data\nSampled data\nModel\nRQ\nSampled\n50 logs\n15 system\nlogs\nLLMParsersL L a M A \nLLMParsersT 5 B a s e \nPre-trained  LLMParsersLLaMA \n Pre-trained  LLMParsersT5Base \nFine-tuned Pre-trained  LLMParsersLLaMA \nFine-tuned Pre-trained  LLMParsersT5Base \nLLMParsers\nLLMParsers\nParsing result\non target system\nSampled 25\nlogs from\ntarget system\nRQ3\nRQ2\nRQ1\nParsing result\non target system\nUnseen logs\nParsing result with \n50-shot fine-tuning\nParsing result with \n50-shot fine-tuning\nData\nsample\nPre-train\nExtract\nunseen\nlogs\nLog Parsing\nFine-tune RQ4\nDifferent\nsize of\nSampled \nlogs\nOutcome of\nthe study\nParsing result with\ndifferent shot fine-tuning\nFigure 2: An overview of the evaluation of LLMParsers.\nanomaly. Liu et al. [39] conducted a case study on logs from Huawei\nCloud and found that the effectiveness of the anomaly detected by\nChatGPT was partially consistent with that of the on-call engineers,\nwhich suggests that LLMs could potentially reduce manual verifi-\ncation efforts. Chen et al. [ 7] introduced RCACopilot, an on-call\nsystem integrated with OpenAIâ€™s GPT models for automating root\ncause analysis of cloud incidents.\n3 APPROACH\nIn this section, we introduce LLMParsers, which adopts LLMs for\nlog parsing. LLMParsers tackle log parsing as a text2text and text\ngeneration task using few-shot fine-tuning. Figure 2 illustrates our\noverall evaluation architecture of LLMParser. In RQ1, we sample\n50 logs from each system using our sampling algorithm for fine-\ntuning LLMParsers, and evaluate the log parsing result. In RQ2,\nwe conduct a sensitivity analysis on the number of shots used for\nfine-tuning. In RQ3, we evaluate the effectiveness of LLMParsers\non unseen log templates. Finally, in RQ4, evaluate the log parsing\naccuracy of using a pre-trained LLMParser using 15 other systems.\nBelow, we present our few-shot sampling algorithm, LLM selection,\nprompt selection, and fine-tuning process.\n3.1 Sampling Few-Shot Data\nLLMs require data samples to learn how to process different re-\nquests, a task often accomplished by providing a few training exam-\nples [32, 33]. Similar to most other log parsers (e.g., Logram, Drain,\nand LogPPT), LLMParser is an offline parser. Log parsers typically\nneed to scan all available logs to identify patterns and abstract\ndynamic values, a process that is inherently offline. Access to all\nnecessary logs is a prerequisite for this procedure, enabling the\napplication of various techniques like clustering and log sampling.\nHence, we can apply our clustering and log sampling techniques to\nsample a small number of logs and their associated log templates to\ntrain the LLMs. We prioritize the sampling of more commonly-seen\nlogs while ensuring data diversity. Prior studies [21, 55] also suggest\nthat increasing the diversity of training data is effective in improv-\ning the understanding and generalization ability of deep learning\nmodels. Therefore, we proposed a data sampling algorithm to sam-\nple logs with high frequency and variety to increase the coverage\nand diversity of the training dataset.\nAlgorithm 1: Few-shot Log Sampling\nInput : ğ·: Dataset containing raw logs\nInput : ğ‘: Number of logs to sample\nOutput: ğ·ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›: Dataset with ğ‘ sampled logs and log templates\n1 /* Extract timestamps and replace numbers */\n2 ğ‘ğ‘Ÿğ‘œğ‘ğ‘’ğ‘ ğ‘ ğ‘’ğ‘‘ _ğ‘™ğ‘œğ‘”ğ‘  â†process_raw_logs(ğ·);\n3 ğ‘™ğ‘œğ‘”_ğ‘ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿğ‘  â†mean_shift_clustering (ğ‘ğ‘Ÿğ‘œğ‘ğ‘’ğ‘ ğ‘ ğ‘’ğ‘‘ _ğ‘™ğ‘œğ‘”ğ‘ );\n4 ğ·ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› â†âˆ…;\n5 foreach ğ‘ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿ in sorted (ğ‘™ğ‘œğ‘”_ğ‘ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿğ‘  , â€œdescendâ€) do\n6 ğ‘™ğ‘œğ‘” â†sample_one_log(ğ‘ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿ);\n7 ğ·ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›.add (ğ‘™ğ‘œğ‘”, find_template(ğ‘™ğ‘œğ‘”));\n8 if length of ğ·ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› = ğ‘ then\n9 break;\n10 end if\n11 end foreach\n12 return ğ·ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›\nAlgorithm 1 demonstrates our log sampling process. The sam-\npling process does not require any pre-labelled data and is unsu-\npervised. Firstly, similar to other log parsers [33, 70], we process\nthe raw logs to separate and remove the information generated by\nlogging frameworks, such as dates and timestamps, and extract the\nlog messages. We further process the logs by using regular expres-\nsions to replace all the numbers in the log with a unified character\nto minimize the effect of the dynamically generated values on the\nnext step. Secondly, we apply the Mean Shift [8] clustering algo-\nrithm to cluster the processed logs. We choose Mean Shift because,\nunlike K-means, it does not need the user to specify the number of\nclusters in advance. However, other clustering algorithms can also\nbe used. Thirdly, we sort the generated clusters in descending order\nbased on the cluster size. Finally, we sample one log from every\ncluster and repeat the process until we reach the desired number\nof samples. By sampling and iterating from the largest cluster, we\nconsider both diversity and coverage (i.e., possibly covering more\nlogs) in the sampling process. We label the log templates for all the\nsampled logs to guide the LLM on how to parse logs.\n3.2 LLMParser: Using LLMs for Parsing Logs\nParsing Logs Using Text2text and Text Generation LLMs.\nWhen using text2text or text generation LLMs, we can parse a log\nby simply giving them a log message. The LLMs â€œtranslateâ€ the\nlog into a log template. Compared to LogPPT [ 33], which uses\nLLMs for token classification, log parsing using text2text and text\ngeneration LLMs eliminates the log splitting and output conversion\nprocess. The LLMs directly use the logs for input and output, which\nfully leverages LLMsâ€™ abilities and makes the parsing result more\nintuitive and easier to diagnose parsing issues.\nWe evaluate and compare four LLMs on their log parsing ac-\ncuracy: Flan-T5-small [10], Flan-T5-base [10], LLaMA-7B [58] and\nChatGLM-6B [68]. Table 1 shows the architecture of the LLMs and\nthe parameter size. The LLMs cover both text2text (Flan-T5) and\ntext generation (LLaMa and ChatGLM), vary in size (range from\n80M to 7B parameters), and are pre-trained using different architec-\ntures. Our goal is to explore the difference in log parsing accuracy\namong LLMs with different architectures and parameter sizes. Flan-\nT5-small and Flan-T5-base are the instruction fine-tuned versions\nof T5 [50] with different parameter sizes. Prior research [ 10, 52]\n4\nLLMParser: An Exploratory Study on Using Large Language Models for Log Parsing ICSE â€™24, April 14â€“20, 2024, Lisbon, Portugal\nTable 1: Information on the Large Language Models that we\nused for log parsing.\nLLM Name Architecture Pre-training Objective Parameter Size\nFlan-T5-small Encoder-decoder Text2text Generation 80M\nFlan-T5-base Encoder-decoder Text2text Generation 240M\nLLaMA-7B Causal Decoder Text Generation 7B\nChatGLM-6B Prefix Decoder Text Generation 6B\nshowed that Flan-T5 converges faster than T5 and achieves out-\nstanding results on fine-tuned tasks. LLaMA-7B is a publicly re-\nleased state-of-the-art foundational large language model by Meta,\nwhich offers smaller and more efficient models for researchers on\nmultiple NLP tasks. ChatGLM-6B was jointly built by Tsinghua\nUniversity and Zhipu AI Company [ 68] that enable widespread\naccess to researchers for question answering and information pro-\ncessing. Additionally, they are open-source models, which allows\nfor easy in-depth analysis and fine-tuning processes, as well as the\nreplication of our study in future research.\nPrompt Templates for Log Parsing. Prompts are user-provided\ninputs, such as queries, instructions, or questions, that guide LLMs\nand instruct their behaviour for specific tasks. Specifically, prompts\nare incorporated into the modelâ€™s embedding layer to guide its\ndecision-making process. Prompting involves priming an LLM by\nusing prompts to demonstrate examples of the downstream task.\nPrevious studies [20, 65, 74] have demonstrated that the quality\nof input prompts plays a crucial role in the performance of LLMs,\ninfluencing the generated output quality.\nA prompt template is often used to generate prompts in a con-\nsistent and reproducible way. As an example, in the case of log\nparsing, one possible prompt template can be formulated as â€œThe\nlog [X] belongs to the log template [Y]. â€, where the objective is\nto generate the corresponding log template for the input log [X],\nwith [Y] representing the answer. In this paper, we investigate the\neffectiveness of LLMs in log parsing using hard prompts, which are\nfixed natural language instructions. We use hard prompts [64] to\nminimize the variability caused by the prompts, and focus our study\non examining the impact of different LLMs and varying training\nsizes on parsing performance. Below, we discuss the prompts that\nwe used in LLMParsers.\nIn the design of its prompt, T5 leverages the colon symbol â€œ:â€ to\nseparate instructions and input data. Since our task shares similari-\nties with the machine translation task, which involves transforming\ninput data to output data, we leverage T5â€™s default prompt struc-\nture (i.e., â€œinstruction + input type + output type:\") to build our\nLLMParserğ‘‡5ğ‘†ğ‘šğ‘ğ‘™ğ‘™ and LLMParserğ‘‡5ğµğ‘ğ‘ ğ‘’ prompt template as:\nâ€œParse the raw log to log template: â€˜{Raw log}â€™. â€\nâ€œ{Log template}â€\nWe feed the log and log template pairs as examples during training.\nWhen parsing logs, we only give instructions about the raw log.\nLLaMA-7B and ChatGLM-6B are two large text generation mod-\nels that have been extensively studied and fine-tuned for various\ntasks [1, 69, 71]. One notable optimized version of LLaMA is Al-\npaca [57], which is highly regarded for its performance. The prompt\ntemplate used by Alpaca has been widely adopted, consistently\ndelivering excellent performance. In this paper, we use Alpacaâ€™s\nprompt template (defined below) to train and generate output for\nthe log parsing task using LLaMA-7B and ChatGLM-6B models.\nâ€œBelow is an instruction that describes a task, paired with an input\nthat provides further context. Write a response that appropriately\ncompletes the request.\n### Instruction: Parse the input log to the log template.\n### Input: â€˜{Raw log}â€™\n### Response: â€˜{Log Template}â€™\nWe give the task description, instruction, input, and response to the\nLLMs during training. When parsing, we only give the description,\ninstruction, and input, and ask the LLMs to generate the response.\nApplying Few-shot Tuning on the LLMs. When the training\ndataset is the same, the fine-tuning efficiency of a model is directly\nrelated to its parameter size, as larger models with more parameters\nrequire more computational resources and a longer time to con-\nverge [11]. Due to the small parameter size of the Flan-T5-small and\nFlan-T5-base, fine-tuning can be efficiently done without additional\noptimization mechanisms. For the larger models, LLaMA-7B and\nChatGLM-6B, we used LoRA [28] to accelerate the fine-tuning pro-\ncess. LoRA is an efficient parameter fine-tuning technique, which\nonly trains a very small portion of ADDITIONAL parameters while\nfreezing the original parameters of the model. LoRA uses low-\nrank parameterization and focuses on the most important layers of\nmodels, reducing both computational and memory requirements.\nConsequently, the model converges faster with minimal impact on\nperformance, enabling faster fine-tuning for various tasks, which\nachieves comparable speed to fine-tuning Flan-t5-base (even with\nmore than 25 times more parameters). Our fine-tuning process\ntakes from several seconds to less than five minutes.\n4 EXPERIMENT SETUP AND\nIMPLEMENTATION\nIn this section, we discuss our experiment setup to answer our\nresearch questions and the implementation details.\nStudied Dataset. We conduct our experiment on the log parsing\nbenchmark provided by He et al. [ 27]. This benchmark contains\nlogs from 16 open-source systems and is widely used to evaluate\nand compare the accuracy of log parsers [12, 17, 24]. Each system\nincludes 2,000 log messages along with their respective log tem-\nplates and parameters (the ground truth for evaluating log parsers).\nThe studied systems included in the dataset range from various\ndomains, such as distributed systems, operating systems, mobile\nsystems, supercomputers, server applications, and standalone soft-\nware. However, recent studies [12, 30] have identified instances\nof incorrectly labelled log templates in the dataset. As a result,\nwe adopted the corrected benchmark dataset released by Khan et\nal. [30], following recent research [31, 33].\nEnvironment and Implementation. Our experiments were con-\nducted on a server with an NVIDIA Tesla V100 GPU using CUDA\n11.0. For the fine-tuning process of the model, we used a maximum\nlearning rate of 5e-4 and use the AdamW [42] optimizer with a\nlinear learning rate decay schedule for optimization. For single sys-\ntem fine-tuning, we uniformly set the batch sizeto 5 and trained\n30 epochs for LLMParsers. For the cross-system scenario (RQ4),\nwe trained 20 epochs and boost the batch size to 20 in order to\nshorten the training time. We used OpenPrompt [14] to fine-tune\n5\nICSE â€™24, April 14â€“20, 2024, Lisbon, Portugal Zeyang Ma, An Ran Chen, Dong Jae Kim, Tse-Hsun (Peter) Chen, and Shaowei Wang\nLLMParserğ‘‡5ğ‘†ğ‘šğ‘ğ‘™ğ‘™ and LLMParserğ‘‡5ğµğ‘ğ‘ ğ‘’. We used LoRA [28] with\nPEFT v0.3.0 to fine-tune LLMParserğ¿ğ¿ğ‘ğ‘€ğ´ and LLMParserğ¶â„ğ‘ğ‘¡ğºğ¿ğ‘€\nNote that, fine-tuning with 50 data samples took from only a few\nseconds to less than five minutes for all the studied LLMs, so the\ncost of few-shot fine-tuning is small. For log parsing, we set the\ntemperatureto 0 and num_beamsto 2 in the generation configura-\ntion in order to generate consistent and stable parsed results. Due\nto the difference in the length of the prompt template, we set the\nmax_length to 256 (LLMParserğ‘‡5ğ‘†ğ‘šğ‘ğ‘™ğ‘™ and LLMParserğ‘‡5ğµğ‘ğ‘ ğ‘’) and\n512 (LLMParserğ¿ğ¿ğ‘ğ‘€ğ´ and LLMParserğ¶â„ğ‘ğ‘¡ğºğ¿ğ‘€ ) as the generation\nparameter, respectively.\nEvaluation Metrics for Log Parsing. Following prior studies [12,\n30, 33, 41, 48, 75], we use two metrics to evaluate the effectiveness\nof LLMs in log parsing: Group Accuracy and Parsing Accuracy.\nGroup Accuracy (GA): Group Accuracy [75] does not directly\nevaluate the correctness of the parsed logs. Instead, GA assesses the\naccuracy of the automatically grouped logs (e.g., GroupID ğ‘ğ‘ğ‘Ÿğ‘ ğ‘’ğ‘‘\nshown in Figure 1). GA is calculated as the ratio between the number\nof correctly grouped logs and the total number of logs. Specifically,\nGA first groups logs based on the parsed logs (i.e., generated by\na log parser), and compares the resulting groups with grouping\nresults from the ground truth (e.g., GroupIDğ‘”ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„). For instance,\nthe log parsing result in Figure 1 has a GA of 4/7. Once the raw logs\nare parsed, they are grouped into different groups, as illustrated in\nFigure 1. We can see that Log 1 and 2 are grouped together, Log\n6 and 7 are grouped together, and Log 3, 4, and 5 form a separate\ngroup by themselves. The grouping results (GroupID ğ‘ğ‘ğ‘Ÿğ‘ ğ‘’ğ‘‘) for\ngroup 1 to group 3 match the grouping results obtained by using\nthe ground truth log template (GroupIDğ‘”ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„). However, Log 5 to\n7 form two groups (GroupIDğ‘ğ‘ğ‘Ÿğ‘ ğ‘’ğ‘‘ 4 and 5) when using the parsed\nlog template, whereas there is only one group (GroupIDğ‘”ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ 4) if\nusing the log template from the ground truth. As a result, the GA\nfor this example is 4/7.\nAlthough GA is commonly used, prior studies [12, 30, 41] high-\nlighted its limitations. For instance, even if the logs are perfectly\ngrouped with a GA of 100%, the parsed log template may not fully\nmatch the ground truth template due to the misidentified parame-\nters in the parsed templates. As a result, GA cannot show whether\nor not the logs are parsed correctly. Furthermore, if the logs are not\nparsed correctly but are still grouped in a cluster (e.g., Log 3 and 4\nin Figure 1), GA will still be 100%.\nParsing Accuracy (PA): Parsing Accuracy [ 41] within the log\ntemplate must match with the ground truth template. For example,\nthe log parsing result in Figure 1 has a PA of 3/7 because there are\nmissed variables in Log 3, 4, 6, and 7 (highlighted inred). Hence, PA\nis a stricter metric compared to GA and aligns more closely with\npractical requirements [30, 33, 41]. Prior studies also found that\nparsing the variables can help downstream log analysis tasks [31,\n43, 54], which further shows the importance of PA over GA.\n5 EVALUATION\nRQ1: What is the accuracy of LLMParsers?\nMotivation. A recent work by Le and Zhang [ 33] proposed us-\ning Large Language Models (LLMs) to learn from labelled logs\nand accurately identify parameters in logs. However, their study\nonly provided initial evidence on the feasibility of using LLMs\nfor log parsing, as they solely utilized one masked-language LLM\n(RoBERTa-base). Yet, there is a lack of research exploring the impact\nof different types (e.g., text2text or text generation LLMs) and sizes\nof LLMs on log parsing accuracy. Hence, in this RQ, we aim to inves-\ntigate the differences among various types of LLMs and the impact\nof distinct LLM parameter sizes on log parsing. Such comparisons\ncan assist practitioners in identifying the most effective LLM for\nlog parsing, while also enabling researchers to identify potential\nfuture directions on LLM-based log parsing.\nApproach. For each system, we fine-tune the four LLMParsers\nusing 50 logs sampled using the sampling algorithm (Algorithm 1).\nThen, similar to prior studies [33, 37], we use each fine-tuned model\nto generate the log templates for all 2,000 logs for each of the 16 sys-\ntems. We compare the grouping and parsing accuracy against state-\nof-the-art approaches: Drain [24], Logram [12], and LogPPT [33].\nWe selected the state-of-the-art approaches based on their high\naccuracy and efficiency [12, 75].\nResults. LLMParsers have a higher (4.25% to 78.69% higher\nfor LLMParserğ¿ğ¿ğ‘ğ‘€ğ´) parsing accuracy compared to state-of-\nthe-arts log parsers. Table 2 shows both the grouping accuracy\n(GA) and parsing accuracy (PA) of the state-of-the-art and LLM-\nParsers. We find that, in general, LLMParsers have a higher GA\n(0.7546~0.8873) and PA (0.9076~0.9587) compared to the traditional\nlog parsers (GA of 0.5513 and 0.8605, and PA of 0.3353 and 0.1718, for\nDrain and Logram, respectively). LLMs such as LLMParserğ¿ğ¿ğ‘ğ‘€ğ´\nand LLMParserğ‘‡5ğµğ‘ğ‘ ğ‘’ achieve a GA of 0.88 and a PA of almost 0.96.\nLogPPT, on the other hand, achieves a high GA (0.9229) and PA\n(0.9162). Compared to LogPPT which uses a masked language model,\nwe find that our parsers that are based on text2text and text gener-\nation models achieve a better PA (except for LLMParserğ¶â„ğ‘ğ‘¡ğºğ¿ğ‘€ ).\nFor example, LLMParserğ¿ğ¿ğ‘ğ‘€ğ´ achieves a PA of 0.9587, which is\n4.6% higher than that of LogPPT.\nThe probabilistic nature of text generation and text2text LLMs\nmay have an impact on the grouping accuracy (GA) of LLM-\nParsers. Nevertheless, the differences in GA between LogPPT\nand two LLMParsers are not statistically significant. Unlike\ntraditional algorithms, text generation and text2text models have\na small probability of generating erratic output that leads to pars-\ning errors [ 3], which has a larger impact on GA. For example,\nwe observed that when we parsed 2,000 logs from Spark using\nLLMParserğ‘‡5ğ‘†ğ‘šğ‘ğ‘™ğ‘™, 1,999 logs were parsed correctly, resulting in a\nPA of 0.9995. Only one log was parsed incorrectly (one of the dy-\nnamic variables was not parsed correctly). However, this log shares\nthe same template with 299 other logs in the system. Although\n299/300 logs were correctly parsed, these 300 logs were not grouped\ntogether because that one incorrectly parsed log forms a group by\nitself. As a result, the GA for Spark becomes 0.85 (1,700/2,000). Note\nthat, we set the temperature of the LLMParsers to zero to ensure the\nconsistency in the parsed logs (i.e., given the same input prompt, the\noutput will be the same) [60]. However, logs contain dynamically\ngenerated values, so even if two logs have the same template, they\nare considered two distinct inputs and may have a small probability\nof resulting in parsed logs with small differences.\nNevertheless, compared to state-of-the-art parsers, the PA of both\nLLMParserğ‘‡5ğµğ‘ğ‘ ğ‘’ and LLMParser ğ¿ğ¿ğ‘ğ‘€ğ´ showed a statistically sig-\nnificant increase using paired t-test (p-value<0.05), while GA did not\n6\nLLMParser: An Exploratory Study on Using Large Language Models for Log Parsing ICSE â€™24, April 14â€“20, 2024, Lisbon, Portugal\nTable 2: A comparison of the grouping accuracy (GA) and parsing accuracy (PA) for the state-of-the-art (first three columns)\nand the LLMParser (the last four columns) parsers.\nDrain Logram LogPPT LLMParserğ‘‡5ğ‘†ğ‘šğ‘ğ‘™ğ‘™ LLMParserğ‘‡5ğµğ‘ğ‘ ğ‘’ LLMParserğ¿ğ¿ğ‘ğ‘€ğ´ LLMParserğ¶â„ğ‘ğ‘¡ğºğ¿ğ‘€\nGA PA GA PA GA PA GA PA GA PA GA PA GA PA\nAndroid 0.8305 0.5475 0.7420 0.2780 0.8845 0.7665 0.8015 0.9005 0.8680 0.9375 0.8485 0.9455 0.8315 0.8395\nApache 1 0.6935 0.3125 0.0065 1 0.9940 1 1 1 1 1 1 1 1\nBGL 0.9625 0.3420 0.5870 0.1245 0.9535 0.9695 0.5040 0.9745 0.4985 0.9770 0.9415 0.9805 0.9440 0.9640\nHadoop 0.9475 0.2690 0.4510 0.1125 0.9935 0.8950 1 0.9140 0.8055 0.9125 0.9805 0.9825 0.6440 0.8375\nHDFS 0.9975 0.3545 0.9300 0.0045 1 0.9025 1 1 1 1 0.9575 0.9880 0.9575 0.9965\nHealthApp 0.7800 0.2305 0.2665 0.1120 1 0.7885 0.8025 0.9560 0.8085 0.9010 0.8550 0.9955 0.5540 0.8190\nHPC 0.8870 0.6355 0.9105 0.6430 0.9430 0.9470 0.9685 0.9835 0.9740 0.9895 0.9700 0.9935 0.9700 0.9855\nLinux 0.6900 0.1835 0.3610 0.1240 0.9335 0.9485 0.1785 0.8515 0.8190 0.9385 0.5455 0.8385 0.8785 0.9495\nMac 0.7865 0.2175 0.5680 0.1685 0.7800 0.6725 0.7325 0.6725 0.7750 0.7090 0.7390 0.6765 0.6505 0.5205\nOpenSSH 0.7890 0.5080 0.6105 0.2980 0.6275 0.9795 0.8870 0.9860 1 1 0.7095 0.9935 0.5840 0.9450\nOpenStack 0.7325 0.0185 0.3255 0 0.9890 0.9065 0.9890 0.9895 1 0.9885 0.9785 0.9960 0.3125 0.8725\nProxifier 0.5265 0 0.5035 0 1 1 1 1 1 1 1 1 0.0310 0.9150\nSpark 0.9200 0.3595 0.2820 0.2585 0.9990 0.9910 0.8500 0.9995 0.8500 0.9905 0.9850 0.9850 0.7750 0.9585\nThunderbird 0.9550 0.0465 0.5540 0.0040 0.6790 0.9255 0.9630 0.9595 0.9705 0.9730 0.6925 0.9675 0.9560 0.9375\nWindows 0.9970 0.4620 0.6940 0.1405 0.9910 0.9830 0.7155 0.9885 0.7155 0.9950 0.9985 0.9965 0.9920 0.9880\nZookeeper 0.9665 0.4970 0.7235 0.4735 0.9935 0.9895 0.9945 0.9995 1 0.9995 0.9945 0.9995 0.9935 0.9930\nAverage 0.8605 0.3353 0.5513 0.1718 0.9229 0.9162 0.8367 0.9484 0.8803 0.9570 0.8873 0.9587 0.7546 0.9076\nNote: The highest values of GA and PA for each system are highlighted in bold. The results of Drain and Logram are based on the evaluation conducted by Khan et al. [30] on\nthe corrected log dataset.\nTable 3: Grouping accuracy (GA) and parsing accuracy (PA)\nfor logs outside the training dataset.\nLLMParserğ‘‡5ğ‘†ğ‘šğ‘ğ‘™ğ‘™LLMParserğ‘‡5ğµğ‘ğ‘ ğ‘’LLMParserğ¿ğ¿ğ‘ğ‘€ğ´LLMParserğ¶â„ğ‘ğ‘¡ğºğ¿ğ‘€\nGA PA GA PA GA PA GA PA\nAndroid 0.7716 0.8452 0.9058 0.8647 0.8789 0.8680 0.8148 0.8329\nApache 1 1 1 1 1 1 1 1\nBGL 0.3798 0.9720 0.3709 0.9752 0.9349 0.9758 0.9375 0.9585\nHadoop 1 0.8517 0.6914 0.8526 0.9768 0.9662 0.3443 0.8501\nHDFS 1 1 1 1 0.9605 0.9892 0.9605 0.9979\nHealthApp0.7547 0.9457 0.7622 0.8770 0.8192 0.9944 0.4426 0.7765\nHPC 0.9040 0.9584 0.9200 0.9744 0.9154 0.9789 0.9106 0.9610\nLinux 0.0474 0.8456 0.9465 0.9569 0.4392 0.7996 0.8834 0.9504\nMac 0.6583 0.5782 0.7106 0.6247 0.6651 0.5857 0.5678 0.4335\nOpenSSH 0.8500 0.9848 1 1 0.5129 0.9889 0.3036 0.9082\nOpenStack0.9865 0.9878 1 0.9865 0.9737 0.9955 0.1432 0.8504\nProxifier 1 1 1 1 1 1 0.0137 0.8901\nSpark 0.8443 0.9995 0.8443 0.9901 0.9854 0.9854 0.7695 0.9629\nThunderbird0.9550 0.9498 0.9648 0.9667 0.6638 0.9579 0.9487 0.9303\nWindows 0.5271 0.9825 0.5271 0.9925 0.9983 0.9975 0.9882 0.9891\nZookeeper0.9886 0.9989 1 1 0.9886 0.9989 0.9886 0.9874\nAverage 0.7917 0.9313 0.8527 0.9413 0.8570 0.9426 0.6886 0.8925\nexhibit a statistically significant difference compared to LogPPT (p-\nvalue>0.05). Prior studies [12, 30, 33] stated that PA evaluates the\npractical goal (i.e., correctly parsing logs) of log parsing, which\nmakes PA a better evaluation metric than GA. In short, LLMParsers\ncan better identify the variables of logs and generate correct log\ntemplates matching the ground truth.\nAfter we remove the logs used for few-shot fine-tuning during\nevaluation, the average GA and PA of LLMParser on the re-\nmaining logs decrease slightly but are still higher than other\nbaselines. Previous studies on log parsers usually evaluate and\ncompare the effectiveness of log parsers on the entire data set (i.e.,\nall 2,000 logs) [24, 33, 75]. However, such evaluation approaches\nmay include the training data in the evaluation process, causing\npotential data leakage issues. Hence, we re-evaluated the GA and\nPA of all four LLMParsers only on the logs by removing all the\nlogs from the test set that were exactly the same as those in\nthe training set and showing the results in Table 3. Compared\nwith the baseline result in Table 2 (evaluated using all the logs),\nall four LLMParsers â€™ average GA and average PA experienced a\nslight decrease, with the average PA decreasing by less than 1%.\nNevertheless, LLMParserğ‘‡5ğµğ‘ğ‘ ğ‘’ and LLMParserğ¿ğ¿ğ‘ğ‘€ğ´ still have\nhigher average PAs (0.9413 for LLMParser ğ‘‡5ğµğ‘ğ‘ ğ‘’ and 0.9426 for\nLLMParserğ¿ğ¿ğ‘ğ‘€ğ´) compared to LogPPT (PA is 0.9162, but LogPPT\nincluded the training data in the evaluation, having a potential\ndata leakage issue). Our findings show that, after removing the log\nsamples used for training in the evaluation process, LLMParsers\ncan still achieve higher PA than all the baselines.\nWhile more complex LLMParsers (more parameters) generally\ngive better parsing results, simpler models already give promis-\ning results. Future studies should consider the trade-off be-\ntween parsing accuracy and the complexity of LLMs. In general,\nmore complex models give better parsing results, with the exception\nof LLMParserğ¶â„ğ‘ğ‘¡ğºğ¿ğ‘€ . One reason that LLMParserğ¶â„ğ‘ğ‘¡ğºğ¿ğ‘€ has\na worse accuracy may be that it is a bilingual model and the logs\nare written in English. Another possible reason is that ChatGLM\nis engineered as a chatbot, which is fine-tuned to give human-like\nresponses. Between LLMParserğ‘‡5ğ‘†ğ‘šğ‘ğ‘™ğ‘™ and LLMParserğ‘‡5ğµğ‘ğ‘ ğ‘’, the\nGA and PA increased by 5.21% and 0.9%, respectively. We see a\nfurther improvement when using LLMParserğ¿ğ¿ğ‘ğ‘€ğ´ compared to\nLLMParserğ‘‡5ğµğ‘ğ‘ ğ‘’, although the improvement is small (0.8% and\n0.17% in GA and PA).\nHowever, more complex models may require more resources and\ntime to parse the logs. We randomly selected 100 logs from Sparkâ€™s\nlog dataset, and measured the average parsing time by repeating\nthe process 20 times. Under the same hardware environment, com-\nplex LLMParsers require a longer parsing time. LLMParserğ‘‡5ğ‘†ğ‘šğ‘ğ‘™ğ‘™\nand LLMParserğ‘‡5ğµğ‘ğ‘ ğ‘’ could parse 100 logs in an average of 1.27\nseconds and 4 seconds, respectively, while LLMParserğ¶â„ğ‘ğ‘¡ğºğ¿ğ‘€ and\nLLMParserğ¿ğ¿ğ‘ğ‘€ğ´ require 19.87 and 28.93 seconds, respectively.\nTherefore, future studies should encompass a trade-off between the\naccuracy and efficiency of using LLMs on log parsing.\nLLMParsers achieve better PA than state-of-the-art and similar\nGA compared to LogPPT. Although LLMParser ğ¿ğ¿ğ‘ğ‘€ğ´, which\nhas a larger number of model parameters, achieves the best GA\nand PA, the difference is small compared to smaller LLMs like\nLLMParserğ‘‡5ğµğ‘ğ‘ ğ‘’.\n7\nICSE â€™24, April 14â€“20, 2024, Lisbon, Portugal Zeyang Ma, An Ran Chen, Dong Jae Kim, Tse-Hsun (Peter) Chen, and Shaowei Wang\nTable 4: Grouping accuracy (GA) and parsing accuracy (PA) for different numbers of training shots.\nLLMParserğ‘‡5ğ‘†ğ‘šğ‘ğ‘™ğ‘™ LLMParserğ‘‡5ğµğ‘ğ‘ ğ‘’ LLMParserğ¿ğ¿ğ‘ğ‘€ğ´ LLMParserğ¶â„ğ‘ğ‘¡ğºğ¿ğ‘€\n25 shots50 shots75 shots100 shots25 shots50 shots75 shots100 shots25 shots50 shots75 shots100 shots25 shots50 shots75 shots100 shots\nGA PA GA PA GA PA GA PA GA PA GA PA GA PA GA PA GA PA GA PA GA PA GA PA GA PA GA PA GA PA GA PA\nAndroid 0.90 0.880.80 0.900.97 0.930.860.930.95 0.910.87 0.940.980.960.960.970.95 0.910.85 0.950.980.980.970.990.78 0.680.83 0.840.93 0.960.83 0.93\nApache 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0.99 1 1 1 1 1 1\nBGL 0.560.930.50 0.970.510.980.56 0.980.600.940.50 0.980.600.980.60 0.990.74 0.840.94 0.980.850.990.96 0.990.84 0.900.94 0.960.35 0.920.96 0.98\nHadoop 0.80 0.871 0.910.98 0.900.970.920.99 0.890.81 0.911 0.99 0.970.990.990.960.98 0.980.980.990.96 0.980.95 0.610.64 0.840.96 0.970.79 0.83\nHDFS 0.70 0.981 1 1 1 1 1 1 1 1 1 1 1 1 1 0.92 0.920.96 0.990.96 1 1 1 0.84 0.940.96 1 0.84 0.990.80 0.99\nHealthApp0.67 0.770.80 0.960.800.930.67 0.940.66 0.850.810.900.81 0.990.81 0.990.86 0.990.86 1 0.87 1 1 1 0.66 0.730.55 0.820.680.860.73 0.86\nHPC 0.95 0.980.97 0.980.970.990.98 0.990.97 0.980.97 0.990.97 0.991 1 0.990.990.97 0.990.98 1 0.99 1 0.77 0.910.97 0.990.950.990.930.99\nLinux 0.36 0.890.18 0.850.36 0.890.18 0.870.94 0.970.82 0.940.88 0.910.36 0.89 1 0.99 0.55 0.840.76 0.980.24 0.950.42 0.870.880.950.480.960.69 0.92\nMac 0.64 0.520.73 0.670.77 0.760.82 0.800.70 0.590.78 0.710.82 0.730.84 0.830.77 0.580.74 0.680.79 0.780.82 0.800.70 0.440.65 0.520.70 0.530.83 0.79\nOpenSSH 0.63 0.920.89 0.990.64 0.980.94 1 0.33 0.881 1 0.81 1 1 1 0.44 0.990.71 0.990.94 1 0.75 1 0.58 0.900.58 0.950.58 0.980.75 0.99\nOpenStack0.96 0.940.990.990.99 1 0.990.990.97 0.951 0.990.99 1 1 1 0.52 0.840.98 1 0.99 1 1 1 0.31 0.510.31 0.870.47 0.830.99 1\nProxifier 0.53 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0.52 1 1 1 0.05 1 0.53 1 0.50 0.950.03 0.920.53 1 0.98 1\nSpark 0.36 0.950.85 1 0.85 1 0.66 1 0.85 1 0.85 0.991 1 1 1 0.81 1 0.99 0.991 0.99 1 1 0.78 0.790.78 0.960.790.970.870.94\nThunderbird0.960.930.960.960.68 0.970.680.980.96 0.920.970.970.700.980.700.980.68 0.960.69 0.970.680.970.680.970.64 0.570.960.940.670.950.670.95\nWindows 0.71 0.980.72 0.991 1 1 1 0.99 0.990.72 1 1 1 1 1 1 1 1 1 1 1 1 1 0.990.840.99 0.990.99 0.990.99 0.99\nZookeeper 1 0.990.99 1 0.99 1 1 1 0.99 1 1 1 0.99 1 0.99 1 0.990.990.99 1 0.99 1 0.99 1 0.99 0.990.99 0.990.99 0.990.97 0.97\nAverage 0.73 0.910.840.950.84 0.960.830.960.87 0.930.88 0.960.910.970.890.980.82 0.930.890.960.860.980.870.980.73 0.790.75 0.910.75 0.930.86 0.95\nNote: The highest values of GA and PA for each LLM for each system are highlighted in bold.\nRQ2: How does the accuracy of log parsing vary\nunder different shot sizes?\nMotivation. In RQ1, we ascertain that LLMs exhibit superior ac-\ncuracy in log parsing compared to the state-of-the-art approaches.\nWhen using LLMs, one thing that researchers and practitioners\nneed to decide is the number of samples for few-shot tuning. Prior\nresearch [2, 20] has demonstrated that the efficacy of a model fine-\ntuned for an individual task is contingent upon the size and diversity\nof the training dataset. However, manually labelling data can be\ntime-consuming and manual-intensive. Hence, in this RQ, we exam-\nine the ramifications of varying training set sizes on the accuracy\nof log parsing tasks when employing distinct LLMs.\nApproach. For each system, we sample 25, 50, 75, and 100 log\nlines and their corresponding log template using our log sampling\nalgorithm (Algorithm 1). The same sets of logs are used as the fine-\ntuning dataset for all LLMParsers. We then evaluate the log parsing\nperformance (i.e., GA and PA) of LLMParsers using different sizes of\nfine-tuning datasets. We vertically compare the accuracy changes\nof each LLMParser after increasing the training data size. Simulta-\nneously, we also horizontally compare the accuracy differences of\ndifferent LLMParsers under the same training data size.\nTo compare, we also apply in-context learning on LLMParserğ‘‡5ğµğ‘ğ‘ ğ‘’\nand LLMParserğ¿ğ¿ğ‘ğ‘€ğ´ without fine-tuning to investigate the log\nparsing accuracy. We chose these two LLMs because of their large\nsize and good parsing results shown in RQ1. We use 3 and 15 shots\n(in-context log parsing demonstrations), respectively for the two\nLLMs, due to their limits on the size of the input tokens.\nResults. Increasing the number of shots increases the accu-\nracy of LLMParsers, but the difference is small (e.g., 1â€“2%) or\nfluctuates for most LLMParsers beyond 50 shots, except for\nLLMParserğ¶â„ğ‘ğ‘¡ğºğ¿ğ‘€ . Table 4 shows the accuracy of LLMParsers\nusing different numbers of shots. Although there are some im-\nprovements in PA and GA when the number of shots increases,\nthe accuracies stabilize after 50 shots. When the shot size is 25,\nboth GA and PA decrease (1% to 15% and 3% to 15%, respectively)\nfor all LLMParsers compared to using 50 shots. We also find that\nLLMParsers have different sensitivity on the shot sizes. For exam-\nple, LLMParserğ‘‡5ğµğ‘ğ‘ ğ‘’ is relatively stable across all shot sizes, while\nLLMParserğ¶â„ğ‘ğ‘¡ğºğ¿ğ‘€ has the largest improvement when the shot size\nincreases. When the shot size is increased to 100, the GA decreases\nfor all LLMParsers except for LLMParserğ¶â„ğ‘ğ‘¡ğºğ¿ğ‘€ , but the improve-\nment for PA is barely noticeable. We also find that LLMParserğ‘‡5ğµğ‘ğ‘ ğ‘’\nhas better GA and the same PA compared to LLMParserğ¿ğ¿ğ‘ğ‘€ğ´, the\nlargest LLM among the four studied LLMs. Our finding shows that\nmore complex LLMs may not achieve better PA and GA. For in-\nstance, the smallest model LLMParserğ‘‡5ğ‘†ğ‘šğ‘ğ‘™ğ‘™ achieves comparable\nresults to the second largest model LLMParserğ¶â„ğ‘ğ‘¡ğºğ¿ğ‘€ , and their\ndifference remains trivial even with the increased training shots.\nCompared to in-context learning, few-shot tuning achieves\nbetter parsing accuracy . Table 5 shows the accuracy for differ-\nent shots of in-context learning. However, in-context learning\nyielded worse results. LLMParser ğ¿ğ¿ğ‘ğ‘€ğ´ achieved only an aver-\nage GA and PA of 30% and 45%, respectively, across 16 systems.\nLLMParserğ‘‡5ğµğ‘ğ‘ ğ‘’ also performed poorly, resulting in an average GA\nand PA of only 22% and 1%. The finding aligns with the discussion\nprovided by a recent study [46] and Flan-T5 developers [10] where\nfew-shot tuning achieves better results than in-context learning.\nDifferent LLMParsers may require different shot sizes to achieve\ngood accuracy in log parsing. For example, LLMParserğ‘‡5ğµğ‘ğ‘ ğ‘’\nalready achieves very high accuracy (87% and 93% for GA and\nPA) when fine-tuned using only 25 shots. However, as the number\nof shots increases further, the improvement consistently plateaus\namong all systems, especially when the shot size is over 75. On the\nother hand, when the shot size is 100, the accuracy of LLMParserğ¶â„ğ‘ğ‘¡ğºğ¿ğ‘€\nbecomes comparable to the other parsers. This trend is particularly\nnoticeable in OpenStack and Proxifier, where both GA and PA show\nsubstantial improvements from 25 shots to 100 shots (GA increases\nfrom 0.31 to 0.99, and PA increases from 0.51 to 1 for OpenStack,\nand GA increases from 0.50 to 0.98, and PA increases from 0.95 to 1\nfor Proxifier).\nThe variation in the accuracy of log parsing across different\nLLMParsers may be attributed to the nature of LLMs as statistical\nmodels. Each model learns to parse logs by identifying distinct\n8\nLLMParser: An Exploratory Study on Using Large Language Models for Log Parsing ICSE â€™24, April 14â€“20, 2024, Lisbon, Portugal\nTable 5: Grouping accuracy (GA) and parsing accuracy (PA) for different shots of in-context learning.\nLLMParserğ¿ğ¿ğ‘ğ‘€ğ´ LLMParserğ‘‡5ğµğ‘ğ‘ ğ‘’\n5 shots 10 shots 15 shots 20 shots 1 shots 2 shots 3 shots 4 shots 5 shots\nGA PA GA PA GA PA GA PA GA PA GA PA GA PA GA PA GA PA\nAndroid 0.200 0.093 0.393 0.126 0.430 0.222 0.642 0.318 0.423 0.043 0.427 0.018 0.424 0.002 0.482 0.033 0.530 0.021\nApache 0.984 0.700 0.430 0.460 0.709 0.711 1 0.725 0.291 0 0.291 0 0.566 0 0.291 0 0.291 0\nBGL 0.240 0.118 0.258 0.211 0.368 0.418 0.238 0.665 0.126 0 0.085 0.001 0.130 0.002 0.133 0.009 0.154 0.008\nHadoop 0.335 0.244 0.407 0.395 0.180 0.164 0.255 0.321 0.116 0 0.188 0.003 0.213 0.003 0.456 0.004 0.284 0.006\nHDFS 0.001 0.071 0.041 0.178 0.011 0.335 0.001 0.241 0.001 0 0.001 0 0.001 0 0.001 0 0.001 0\nHealthApp 0.332 0.561 0.429 0.528 0.626 0.756 0.584 0.824 0.120 0.001 0.127 0 0.128 0 0.147 0.019 0.074 0.012\nHPC 0.295 0.490 0.155 0.506 0.354 0.592 0.244 0.478 0.635 0.001 0.384 0.001 0.595 0 0.390 0 0.530 0.081\nLinux 0.036 0.106 0.201 0.611 0.264 0.616 0.199 0.675 0.129 0.002 0.167 0.009 0.154 0.012 0.170 0.024 0.165 0.012\nMac 0.247 0.129 0.345 0.155 0.414 0.201 0.384 0.180 0.247 0.012 0.355 0.066 0.344 0.008 0.378 0.024 0.389 0.056\nOpenSSH 0.002 0.182 0.068 0.382 0.074 0.361 0.028 0.389 0.095 0 0.025 0 0.255 0 0.076 0 0.094 0\nOpenStack 0.041 0 0.049 0.054 0.083 0.025 0.070 0.012 0.194 0 0.147 0 0.102 0 0.112 0 0.097 0\nProxifier 0.050 0.627 0.050 0.981 0.050 0.980 0.050 0.963 0 0 0.001 0 0 0 0 0 0.003 0\nSpark 0.003 0.312 0.023 0.440 0.401 0.579 0.215 0.494 0.009 0 0.023 0.002 0.048 0.003 0.064 0.001 0.027 0.003\nThunderbird 0.079 0.062 0.118 0.326 0.165 0.398 0.019 0 0.203 0.010 0.178 0.004 0.168 0.004 0.123 0.004 0.101 0.004\nWindows 0.162 0.003 0.398 0.567 0.410 0.427 0.181 0.309 0.033 0 0.132 0 0.259 0.009 0.139 0.006 0.011 0\nZookeeper 0.171 0.142 0.017 0.206 0.294 0.380 0.248 0.469 0.171 0.158 0.152 0.001 0.173 0.133 0.042 0.020 0.037 0.027\nAverage 0.198 0.240 0.211 0.383 0.302 0.448 0.272 0.441 0.174 0.014 0.167 0.006 0.222 0.011 0.187 0.009 0.174 0.014\npatterns from the training shots. To determine the best shot size\nand reduce manual effort on data creation, future studies should\ninvestigate the relationship between the characteristics of the LLMs\n(e.g., architecture and training data) and the needed data to fine-\ntune the LLMs for log parsing.\nFor all LLMParsers except for LLMParserğ¶â„ğ‘ğ‘¡ğºğ¿ğ‘€ , the accuracy\nimprovement in log parsing becomes small or starts to fluctuate\nwhen the shot size is over 50. Different LLMs may also require\ndifferent shot sizes to achieve good parsing results, and more\nshots do not always give the best results.\nRQ3: How is the generalizability of LLMParsers\non unseen log templates?\nMotivation. In RQ2, we studied the accuracy of LLMParsers using\nvarious numbers of shots. We found that although GA and PA\nimprove noticeably when the shot size increases from 25 to 50,\nthe improvement is small or remains almost the same when the\nshot size is 50 or larger. One hypothesis is that the effectiveness\nof few-shot tuning is constrained by the diversity of the sampling\nalgorithm when presented with diverse log templates. Logs with\ndifferent variable values may still have the same log template. By\nincluding a single log in the fine-tuning process, it is possible to\nenhance the parsing accuracy for all other logs that share the same\nlog template. In other words, 50 shots may not capture all the unique\nlog templates, leading to a saturation point where the LLM fails\nto generalize well to unseen examples. Therefore, our objective is\nto investigate the extent to which few-shot tuning can generalize\nto unseen log templates. The finding of this RQ may help future\nresearch further improve the accuracy of LLM-based log parsers.\nApproach. We use the same set of LLMParsers that are trained\nusing 50 log samples from prior RQs. Specifically, we want to study\nif a logâ€™s log template was not included in the training , would\nsuch a log have lower parsing accuracy. We first identify the log\ntemplates and the corresponding logs that were not used for few-\nshot tuning (we call them logğ‘¢ğ‘›ğ‘ ğ‘’ğ‘’ğ‘›). Then, we calculate the PA for\nlogğ‘¢ğ‘›ğ‘ ğ‘’ğ‘’ğ‘› and compare it with the PA for logğ‘ ğ‘’ğ‘’ğ‘›.\nResults. The PA on log ğ‘¢ğ‘›ğ‘ ğ‘’ğ‘’ğ‘› are much lower (e.g., 0.638 for\nLLMParserğ¿ğ¿ğ‘ğ‘€ğ´) compared to the PA on log ğ‘ ğ‘’ğ‘’ğ‘› (e.g., 0.9539\nfor LLMParser ğ¿ğ¿ğ‘ğ‘€ğ´). Although only 4.4% of the logs have\nunseen log templates, they account for 50% of the incorrectly\nparsed logs. Table 6 shows the number of total templates, the num-\nber of unseen log templates, the number of logğ‘¢ğ‘›ğ‘ ğ‘’ğ‘’ğ‘›, and the PA of\nlogğ‘¢ğ‘›ğ‘ ğ‘’ğ‘’ğ‘› and PA of logğ‘ ğ‘’ğ‘’ğ‘›. We find that the PA decreases signifi-\ncantly for the logğ‘¢ğ‘›ğ‘ ğ‘’ğ‘’ğ‘› compared to the PA for logğ‘ ğ‘’ğ‘’ğ‘›. For example,\nas shown in Table 6, the PAs for logğ‘ ğ‘’ğ‘’ğ‘› of LLMParserğ‘‡5ğµğ‘ğ‘ ğ‘’ and\nLLMParserğ¿ğ¿ğ‘ğ‘€ğ´ are 0.9506 and 0.9539, whereas their average PAs\nfor the logğ‘¢ğ‘›ğ‘ ğ‘’ğ‘’ğ‘› are 0.6385 and 0.6507, respectively. After some\ninvestigation, we find that around 50% of the incorrectly parsed logs\namong all the 16 systems belong to one of the unseen log templates,\nand the finding is consistent across all LLMs. Given that, only 4.4%\nof the logs are logğ‘¢ğ‘›ğ‘ ğ‘’ğ‘’ğ‘› (1,406 out of all 32,000 logs from all the\nsystems), they are disproportionately more likely to be parsed in-\ncorrectly. Hence, our finding suggests that logğ‘¢ğ‘›ğ‘ ğ‘’ğ‘’ğ‘› is one of the\nbottlenecks to further improving parsing accuracy and shed light\non future research in log parsers. Nevertheless, we find that LLM-\nParsers still achieve better PA when parsing logğ‘¢ğ‘›ğ‘ ğ‘’ğ‘’ğ‘› compared to\ntraditional state-of-the-art approaches such as Drain and Logram,\nwhich have an average PA of 0.3353 and 0.1718, respectively.\nWe observe a decrease in the performance of LLMParsers when\nthey encounter unseen log templates, indicating their limited ability\nto generalize. This behaviour in LLMParsers may be attributed to\nthe limitation of the training data during fine-tuning, which pri-\nmarily focuses on identifying seen log templates. This limitation\nbecomes more apparent when log templates share high similarities.\nFor example, two logs with similar log templates, such as â€œ(<*>)\nCMD (<*> <*>)â€ and â€œ(<*>) CMD (run-parts <*>)â€, might be\nmistakenly parsed as the same log template due to their textual simi-\nlarity, resulting in reduced accuracy. However, if both logs and their\ntemplates were provided as training data, LLMParsers could better\n9\nICSE â€™24, April 14â€“20, 2024, Lisbon, Portugal Zeyang Ma, An Ran Chen, Dong Jae Kim, Tse-Hsun (Peter) Chen, and Shaowei Wang\nTable 6: LLMParsersâ€™ PA on log ğ‘¢ğ‘›ğ‘ ğ‘’ğ‘’ğ‘› and PA on log ğ‘ ğ‘’ğ‘’ğ‘› when using 50 log samples. Note that we excluded the systems where\nall the unique log templates were included in the shots.\nLLMParserğ‘‡5ğ‘†ğ‘šğ‘ğ‘™ğ‘™ LLMParserğ‘‡5ğµğ‘ğ‘ ğ‘’ LLMParserğ¿ğ¿ğ‘ğ‘€ğ´ LLMParserğ¶â„ğ‘ğ‘¡ğºğ¿ğ‘€\nTotal TemplateUnseen Template Unseen LogPA-unseen PA-seenPA-unseen PA-seenPA-unseen PA-seenPA-unseen PA-seen\nAndroid 158 108 224 0.5536 0.9443 0.5893 0.9814 0.7009 0.9764 0.5446 0.8767\nBGL 120 70 105 0.7714 0.9858 0.8476 0.9842 0.7714 0.9921 0.6762 0.9799\nHadoop 114 64 64 0.5469 0.9261 0.6719 0.9205 0.5156 0.9979 0.3906 0.8523\nHealthApp 75 25 25 0.7200 0.9590 0.7200 0.9033 0.8000 0.9980 0.8400 0.8187\nLinux 116 66 66 0.5152 0.8630 0.5909 0.9504 0.8182 0.8392 0.5909 0.9617\nMac 341 291 819 0.3297 0.9102 0.3993 0.9238 0.3761 0.8848 0.3907 0.6105\nThunderbird 149 99 103 0.5146 0.9837 0.6505 0.9905 0.5728 0.9889 0.5631 0.9578\nAverage 153 (Sum: 1073) 103 (Sum: 723)201 (Sum: 1406) 0.5645 0.9389 0.6385 0.9506 0.6507 0.9539 0.5709 0.8654\nTable 7: Grouping (GA) and parsing (PA) accuracy of using\npre-trained LLMParsers (i.e., pt), and LLMParsers that is fine-\ntuned based on the pre-trained LLMParsers (i.e., ft).\nLLMParserğ¿ğ¿ğ‘ğ‘€ğ´ LLMParserğ‘‡5ğµğ‘ğ‘ ğ‘’\nGAğ‘ğ‘¡ PAğ‘ğ‘¡ GAğ‘“ğ‘¡ PAğ‘“ğ‘¡ GAğ‘ğ‘¡ PAğ‘ğ‘¡ GAğ‘“ğ‘¡ PAğ‘“ğ‘¡\nAndroid 0.9325 0.7965 0.7655 0.6475 0.6805 0.6575 0.9455 0.9230\nApache 1 0.9940 0.7245 0.7300 1 1 1 1\nBGL 0.8250 0.5565 0.4415 0.8230 0.5840 0.9480 0.9575 0.9715\nHadoop 0.9595 0.5030 0.3270 0.4105 0.9560 0.6780 0.9955 0.9810\nHDFS 0.1335 0.2000 0.2710 0.7790 0.7470 0.4590 1 1\nHealthApp 0.6885 0.6825 0.6250 0.7560 0.7635 0.7560 0.8005 0.9400\nHPC 0.9445 0.8745 0.6490 0.8875 0.9150 0.9210 0.9780 0.9895\nLinux 0.5440 0.5185 0.1755 0.7350 0.5440 0.5275 0.4870 0.9415\nMac 0.7620 0.4805 0.2065 0.5005 0.7350 0.4335 0.8910 0.7240\nOpenSSH 0.2280 0.8710 0.1660 0.9105 0.3900 0.7875 0.5020 0.9745\nOpenStack 0.3740 0.4235 0.0385 0.6680 0.2670 0.7950 0.9890 0.9915\nProxifier 0.0010 0 0 0.1730 0.0010 0.0005 1 1\nSpark 0.9030 0.9010 0.1195 0.8755 0.7755 0.8980 1 1\nThunderbird 0.6615 0.8430 0.0825 0.1055 0.9465 0.8135 0.6955 0.9550\nWindows 0.4015 0.5795 0.2780 0.8620 0.9890 0.9810 1 0.9970\nZookeeper 0.8045 0.8770 0.7470 0.8955 0.9600 0.6775 0.9935 0.9930\nAverage 0.6352 0.6313 0.3511 0.6724 0.7034 0.7083 0.8897 0.9613\ndifferentiate between them and achieve higher accuracy. Future\nresearch may consider improving the generalization of LLMParsers\nby proposing sampling algorithms that can select a more diverse\nsampled set of logs and templates during fine-tuning.\nLLMParsers achieves bad results on logğ‘¢ğ‘›ğ‘ ğ‘’ğ‘’ğ‘› compared with\nresults on logğ‘ ğ‘’ğ‘’ğ‘›. The unseen logs, which only make up 4.4%\nof all logs, form 50% of the incorrectly parsed logs. Some types\nof variables may not be identified even if they appear in the\ntraining dataset.\nRQ4: Can pre-trained LLMParsers help improve\nparsing accuracy?\nMotivation. In the previous RQs, we investigated the log parsing\naccuracy when fine-tuning the LLMs using the logs from the same\nsystem. However, one major advantage of LLM is its ability to\ngeneralize on new datasets [29, 61, 73]. Therefore, in this RQ, we\nstudy if using a LLMParser that is pre-trained using logs from other\nsystems can further improve log parsing results.\nApproach. We consider LLMParserğ‘‡5ğµğ‘ğ‘ ğ‘’ and LLMParserğ¿ğ¿ğ‘ğ‘€ğ´\nin this RQ because of their high log parsing accuracy and represen-\ntative model size. For every system, we pre-train the LLM using 15\nother systems by following the same fine-tuning process as done\nbefore. For the first part of the evaluation, we apply the pre-trained\nLLMParsers directly to parse the logs of the target system (the\nsystem of which the logs are not used for pre-training). Then, we\nfurther fine-tune the pre-trained LLMParsers using 25 log samples\nfrom the target system and evaluate the accuracy of the parsed logs.\nResults. LLMParsers pre-trained using logs from other systems\nachieve considerably lower GA and PA compared to LLMParsers\nthat use few-shot tuning Table 7 shows the GA and PA of using\nthe pre-trained LLMParserğ¿ğ¿ğ‘ğ‘€ğ´ and LLMParserğ‘‡5ğµğ‘ğ‘ ğ‘’. By pre-\ntraining using only the logs from 15 other systems, LLMParserğ¿ğ¿ğ‘ğ‘€ğ´\nachieves a PA and GA of 0.6352 and 0.6313, and LLMParserğ‘‡5ğµğ‘ğ‘ ğ‘’\nachieves a PA and GA of 0.7034 and 0.7083. Compared to few-shot\ntuning using logs from the same system, the PA and GA for the\npre-trained LLMParsers decrease considerably. Interestingly, de-\nspite having more parameters, LLMParser ğ¿ğ¿ğ‘ğ‘€ğ´ achieves lower\nGA and PA compared to LLMParserğ‘‡5ğµğ‘ğ‘ ğ‘’. The reason may be that,\ncompared to LLMParserğ¿ğ¿ğ‘ğ‘€ğ´, LLMParserğ‘‡5ğµğ‘ğ‘ ğ‘’ converges faster\nand is better at avoiding underfitting during fine-tuning due to its\nsmaller parameter size. Nevertheless, we find that the GA and PA of\nthe pre-trained LLMParsers are still comparable to that of the tradi-\ntional log parsers (i.e., Drain and Logram), which further shows the\npotential of using LLMs for log parsing. Our finding shows that logs\nfrom different systems may have different characteristics, so using\nonly logs from other systems gives worse GA and PA.\nFurther tuning using logs from the target system shows oppo-\nsite results in different LLMParsers. The GA and PA improved\nconsiderably in LLMParserğ‘‡5ğµğ‘ğ‘ ğ‘’ but the GA of LLMParser ğ¿ğ¿ğ‘ğ‘€ğ´\ndecreased by almost 55% compared to using only the pre-trained\nLLMParserğ¿ğ¿ğ‘ğ‘€ğ´. We further fine-tune the pre-trained LLMParsers\nusing 25 log samples from the target system. We find that few-shot\ntuning the pre-trained LLMParsers improved the GA and PA of\nLLMParserğ‘‡5ğµğ‘ğ‘ ğ‘’ to 0.8897 and 0.9613, respectively (Table 7). The\nimprovement in GA and PA is large compared to using only the\npre-trained LLMParserğ‘‡5ğµğ‘ğ‘ ğ‘’ (from a GA and PA of around 0.7 to\n0.8897 and 0.9613 in GA and PA, respectively). The GA and PA are\nalso comparable to using 100 log samples from the target system\nto fine-tune LLMParserğ‘‡5ğµğ‘ğ‘ ğ‘’, which has a GA and PA of 0.89 and\n0.98, as shown in Table 2. This enables us to minimize the time\ncost and manual effort required for labelling training data and also\nreduces the fine-tuning time. However, we see an opposite result in\nLLMParserğ¿ğ¿ğ‘ğ‘€ğ´, where further tuning using 25 log samples from\nthe target system results in worse GA (decreased from 0.6352 to\n0.3511, a 55% decrease) and similar PA (increased from 0.6313 to\n0.6724, a 6% increase).\nSome studies [9, 18, 47] have shown that the amount of data and\nmodel parameter size required to achieve optimal performance on\ncertain tasks are not directly correlated across models with different\nparameter sizes and architectures. Sometimes, having too much or\ntoo little amount of data can result in model overfitting and poor\nmodel performance. The pre-trained LLMParserğ¿ğ¿ğ‘ğ‘€ğ´ may also\nneed more data to fine-tune due to its larger size. Future research\nshould explore the use of different quantities or types of log data\nfor pre-training models based on our study in order to build more\n10\nLLMParser: An Exploratory Study on Using Large Language Models for Log Parsing ICSE â€™24, April 14â€“20, 2024, Lisbon, Portugal\ngeneralized, high-accuracy log parsing models that require fewer\nfine-tuning samples and better align with practical needs.\nAlthough pre-trained LLMParsers achieves worse results com-\npared to few-shot tuning, they achieve similar results com-\npared to the prior state-of-the-art. Further tuning the pre-\ntrained LLMParsers shows opposite results in two LLMs, where\nthe result became worse for LLMParser ğ¿ğ¿ğ‘ğ‘€ğ´ but better for\nLLMParserğ‘‡5ğµğ‘ğ‘ ğ‘’.\n6 DISCUSSION\nLLMParsers achieve promising results with higher parsing accu-\nracy than state-of-the-arts and comparable grouping accuracy with\nLogPPT [33]. However, we also find some limitations and potential\nimprovements in LLM-based log parsing. In this section, we sum-\nmarize our observations and highlight future research directions.\nLLMParsers face challenges in parsing some specific logs. More\nadvanced or log-tailored LLMs are needed to further improve\nLLM-based log parsers. LLMParsers takes a few training samples\nas input and then learns how to parse logs. However, as shown\nin Table 6, even though LLMParsers achieve high accuracy (above\n90% PA), there are still some logs that were not parsed correctly.\nThrough manual investigation, we find that LLMParsers face chal-\nlenges in recognizing specific data types (e.g., datetime) as vari-\nables. For instance, LLMParserğ¿ğ¿ğ‘ğ‘€ğ´ fails to parse the log template\nâ€œconnection from <*> at <*> â€ correctly. Instead, the logs are\nparsed as â€œ connection from <*> at Mon Jul 25 23:24:09\n2005â€ without recognizing the timestamp value as the second vari-\nable. Due to having a limited number of samples, the LLMs are not\nable to learn how to parse some variables. One potential solution is,\nsimilar to RQ4, to use log data from other systems to help pre-train\nan LLM-based log parser so that the parsers can generalize and\nidentify more variables. The other potential solution is to use a\nmore complex LLM with more parameters. However, future studies\nshould consider the trade-off between more complex LLMs and\nhigher fine-tuning and inference costs.\nSince more complex LLMs may not always give better results,\nfuture studies are needed to find the balance between accuracy\nand efficiency. We find that simpler models, such as T5, can already\nachieve promising log parsing results and larger models may not\ngive better results. There may be significant cost implications when\nusing larger or even commercial LLMs. More importantly, as we\nfound, larger LLMs also need more inference time to parse logs.\nSince logs are often large in volume, having an efficient parser\nis important. Future research should explore the right balance in\nmodel size and parsing efficiency.\nFuture research should explore the most effective sampling\nalgorithms for identifying training log samples. In RQ3, we\nfind that LLMParsers have worse parsing results on unseen log\ntemplates and there are cases where increasing shot sizes result\nin worse parsing results. For example, when two very similar yet\ndifferent log templates (e.g., one template has one more variable) are\nincluded in the training, the models may get confused when parsing\nthe corresponding logs. Hence, future research should explore the\noptimal sampling strategy that can maximize diversity while also\nconsidering the characteristics of the logs and the corresponding\ntemplates (e.g., should certain types of logs be sampled more to\ndistinguish the similar templates).\nFine-tuning with samples from the target system is demon-\nstrated as the most effective way for log parsing. In RQ2, we\nfind that in-context learning shows bad performance on log parsing.\nAlso, in-context learning is not as effective as fine-tuning due to\nthe token limitation and efficiency concerns. In RQ3 and RQ4, we\nalso find that LLMParsers face challenges in generalizing parsing\nunseen logs. We found that fine-tuning LLM with samples from the\ntarget system gives the best result. Future studies should consider\nvalidating the findings on more complex LLMs (e.g., LLaMA 70B)\nand see if such models have better generalizability on log parsing.\n7 THREATS TO VALIDITY\nExternal validity. Similar to prior work [ 30, 32, 33], we train\nand validate LLMParser using logs and log templates from public\ndatasets that are commonly used in log-related research, However,\nrecent research [30] has indicated the dataset might contain data\nerrors. To mitigate this potential issue, we leverage the corrected\ndataset [30] to reduce such a threat. For a fair comparison, we also\ncompare our results with the results from the state-of-the-art based\non the corrected data [30, 33]. The log format may also affect our\nresult, but the used datasets cover logs from various systems with\ndifferent formats. Future studies are needed to evaluate LLM-based\nparsers on logs from other systems. Internal validity. While LLM-\nParser outperforms other state-of-the-art approaches, our primary\nfocus in this research is on the exploration of the performance of\nLLMs in log parsing tasks. Our research does not cover all LLMs and\ntraining data sizes. Future studies may explore the optimal solution\nfor LLM-based log parser, enabling further advancements in this\ndomain. Construct validity. Our approach requires pairs of logs\nand their log templates. Hence, the sampling process may affect\nthe parsing result. To mitigate the issue, we apply an unsupervised\ndata sampling algorithm that does not require any knowledge of\nthe ground truth. Future studies are needed to explore the effect of\nsuch sampling algorithms on the parsing results.\n8 CONCLUSION\nIn this study, we explore the potential of leveraging LLMs for log\nparsing. We propose LLMParser, a generative LLM-based log parser,\nto overcome the limitations of existing log parsers. LLMParser\nleverages few-shot tuning to learn from a limited set of training\nlogs , which were sampled using a clustering sampling algorithm.\nOur evaluation shows that LLMParsers achieve high accuracy, out-\nperforming state-of-the-arts log parsers. We then evaluate LLM-\nParsers under different pre-training settings. Our results show that,\ncompared to in-context learning, few-shot tuning achieves higher\nparsing accuracy and requires less inference time. Furthermore,\nour findings suggest that different LLMParser models may require\ndifferent numbers of training samples to achieve optimal perfor-\nmance. Instead of increasing the training shot sizes, future studies\nshould investigate how training log diversity and coverage affect\nlog parser accuracy. Our exploratory study leverages generative\nLLMs for log parsing and delivers comprehensive evaluations in\nvarious settings (architecture, shot sizes, and pre-training), which\nprovides empirical evidence for future research.\n11\nICSE â€™24, April 14â€“20, 2024, Lisbon, Portugal Zeyang Ma, An Ran Chen, Dong Jae Kim, Tse-Hsun (Peter) Chen, and Shaowei Wang\nREFERENCES\n[1] Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and\nAndriy Mulyar. 2023. Gpt4all: Training an assistant-style chatbot with large scale\ndata distillation from gpt-3.5-turbo. GitHub (2023).\n[2] Alexandre Bailly, Corentin Blanc, Ã‰lie Francis, Thierry Guillotin, Fadi Jamal,\nBÃ©chara Wakim, and Pascal Roy. 2022. Effects of dataset size and interactions\non the prediction performance of logistic regression and deep learning models.\nComputer Methods and Programs in Biomedicine 213 (2022), 106504. https://doi.\norg/10.1016/j.cmpb.2021.106504\n[3] Mark Belford, Brian Mac Namee, and Derek Greene. 2018. Stability of topic\nmodeling via matrix factorization. Expert Systems with Applications 91 (2018),\n159â€“169. https://doi.org/10.1016/j.eswa.2017.08.047\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot learners. Advances in neural\ninformation processing systems 33 (2020), 1877â€“1901.\n[5] Jinfu Chen, Weiyi Shang, Ahmed E Hassan, Yong Wang, and Jiangbin Lin. 2019.\nAn experience report of generating load tests using log-recovered workloads\nat varying granularities of user behaviour. In 2019 34th IEEE/ACM International\nConference on Automated Software Engineering (ASE) . IEEE, IEEE, 669â€“681.\n[6] Song Chen and Hai Liao. 2022. Bert-log: Anomaly detection for system logs\nbased on pre-trained language model. Applied Artificial Intelligence 36, 1 (2022),\n2145642.\n[7] Yinfang Chen, Huaibing Xie, Minghua Ma, Yu Kang, Xin Gao, Liu Shi, Yunjie\nCao, Xuedong Gao, Hao Fan, Ming Wen, et al. 2023. Empowering Practical Root\nCause Analysis by Large Language Models for Cloud Incidents. arXiv preprint\narXiv:2305.15778 (2023).\n[8] Yizong Cheng. 1995. Mean shift, mode seeking, and clustering. IEEE Transactions\non Pattern Analysis and Machine Intelligence 17, 8 (1995), 790â€“799. https://doi.\norg/10.1109/34.400568\n[9] Yew Ken Chia, Pengfei Hong, Lidong Bing, and Soujanya Poria. 2023. INSTRUCTE-\nVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models.\narXiv preprint arXiv:2306.04757 (2023).\n[10] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William\nFedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha\nChowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping\nHuang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin,\nAdam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling Instruction-\nFinetuned Language Models. https://doi.org/10.48550/ARXIV.2210.11416\n[11] Kenneth Ward Church, Zeyu Chen, and Yanjun Ma. 2021. Emerging trends: A\ngentle introduction to fine-tuning. Natural Language Engineering 27, 6 (2021),\n763â€“778.\n[12] Hetong Dai, Heng Li, Che-Shao Chen, Weiyi Shang, and Tse-Hsun Chen. 2020.\nLogram: Efficient Log Parsing Using ğ‘›n-Gram Dictionaries. IEEE Transactions\non Software Engineering 48, 3 (2020), 879â€“892.\n[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding.arXiv\npreprint arXiv:1810.04805 (2018).\n[14] Ning Ding, Shengding Hu, Weilin Zhao, Yulin Chen, Zhiyuan Liu, Hai-Tao Zheng,\nand Maosong Sun. 2021. Openprompt: An open-source framework for prompt-\nlearning. arXiv preprint arXiv:2111.01998 (2021).\n[15] Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and\nNoah Smith. 2020. Fine-tuning pretrained language models: Weight initializations,\ndata orders, and early stopping. arXiv preprint arXiv:2002.06305 (2020).\n[16] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu\nSun, Jingjing Xu, and Zhifang Sui. 2022. A survey for in-context learning. arXiv\npreprint arXiv:2301.00234 (2022).\n[17] Min Du and Feifei Li. 2019. Spell: Online Streaming Parsing of Large Unstructured\nSystem Logs. IEEE Transactions on Knowledge and Data Engineering 31, 11 (2019),\n2213â€“2227. https://doi.org/10.1109/TKDE.2018.2875442\n[18] Ronen Eldan and Yuanzhi Li. 2023. TinyStories: How Small Can Language Models\nBe and Still Speak Coherent English? arXiv preprint arXiv:2305.07759 (2023).\n[19] Qiang Fu, Jian-Guang Lou, Yi Wang, and Jiang Li. 2009. Execution Anomaly\nDetection in Distributed Systems through Unstructured Log Analysis. In 2009\nNinth IEEE International Conference on Data Mining . 149â€“158. https://doi.org/10.\n1109/ICDM.2009.60\n[20] Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making Pre-trained Language\nModels Better Few-shot Learners. In Proceedings of the 59th Annual Meeting of\nthe Association for Computational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Volume 1: Long Papers) . Association\nfor Computational Linguistics, Online, 3816â€“3830. https://doi.org/10.18653/v1/\n2021.acl-long.295\n[21] Zhiqiang Gong, Ping Zhong, and Weidong Hu. 2019. Diversity in Machine\nLearning. IEEE Access 7 (2019), 64323â€“64350. https://doi.org/10.1109/ACCESS.\n2019.2917620\n[22] Pinjia He, Zhuangbin Chen, Shilin He, and Michael R Lyu. 2018. Characterizing\nthe natural language descriptions in software logging statements. In Proceedings\nof the 33rd ACM/IEEE International Conference on Automated Software Engineering .\n178â€“189.\n[23] Pinjia He, Jieming Zhu, Shilin He, Jian Li, and Michael R Lyu. 2017. Towards\nautomated log parsing for large-scale log data analysis. IEEE Transactions on\nDependable and Secure Computing 15, 6 (2017), 931â€“944.\n[24] Pinjia He, Jieming Zhu, Zibin Zheng, and Michael R. Lyu. 2017. Drain: An\nOnline Log Parsing Approach with Fixed Depth Tree. In 2017 IEEE International\nConference on Web Services (ICWS) . 33â€“40. https://doi.org/10.1109/ICWS.2017.13\n[25] Shilin He, Pinjia He, Zhuangbin Chen, Tianyi Yang, Yuxin Su, and Michael R\nLyu. 2021. A survey on automated log analysis for reliability engineering. ACM\ncomputing surveys (CSUR) 54, 6 (2021), 1â€“37.\n[26] Shilin He, Jieming Zhu, Pinjia He, and Michael R Lyu. 2016. Experience re-\nport: System log analysis for anomaly detection. In 2016 IEEE 27th international\nsymposium on software reliability engineering (ISSRE) . IEEE, 207â€“218.\n[27] Shilin He, Jieming Zhu, Pinjia He, and Michael R Lyu. 2020. Loghub: a large\ncollection of system log datasets towards automated log analytics. arXiv preprint\narXiv:2008.06448 (2020).\n[28] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\nWang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large\nlanguage models. arXiv preprint arXiv:2106.09685 (2021).\n[29] Hong Jin Kang, TegawendÃ© F. BissyandÃ©, and David Lo. 2019. Assessing the\nGeneralizability of Code2vec Token Embeddings. In 2019 34th IEEE/ACM In-\nternational Conference on Automated Software Engineering (ASE) . 1â€“12. https:\n//doi.org/10.1109/ASE.2019.00011\n[30] Zanis Ali Khan, Donghwan Shin, Domenico Bianculli, and Lionel Briand. 2022.\nGuidelines for Assessing the Accuracy of Log Message Template Identification\nTechniques. In Proceedings of the 44th International Conference on Software Engi-\nneering (Pittsburgh, Pennsylvania) (ICSE â€™22). Association for Computing Machin-\nery, New York, NY, USA, 1095â€“1106. https://doi.org/10.1145/3510003.3510101\n[31] Zanis Ali Khan, Donghwan Shin, Domenico Bianculli, and Lionel Briand. 2023.\nImpact of Log Parsing on Log-based Anomaly Detection. arXiv preprint\narXiv:2305.15897 (2023).\n[32] Van-Hoang Le and Hongyu Zhang. 2023. An Evaluation of Log Parsing with\nChatGPT. arXiv preprint arXiv:2306.01590 (2023).\n[33] Van-Hoang Le and Hongyu Zhang. 2023. Log Parsing with Prompt-based Few-\nshot Learning. In 45th International Conference on Software Engineering: Software\nEngineering in Practice (ICSE) .\n[34] Yukyung Lee, Jina Kim, and Pilsung Kang. 2021. LAnoBERT: System log anomaly\ndetection based on BERT masked language model.arXiv preprint arXiv:2111.09564\n(2021).\n[35] Juho Leinonen, Paul Denny, Stephen MacNeil, Sami Sarsa, Seth Bernstein, Joanne\nKim, Andrew Tran, and Arto Hellas. 2023. Comparing code explanations created\nby students and large language models. arXiv preprint arXiv:2304.03938 (2023).\n[36] Heng Li, Tse-Hsun Chen, Weiyi Shang, and Ahmed E Hassan. 2018. Studying\nsoftware logging using topic models. Empirical Software Engineering 23 (2018),\n2655â€“2694.\n[37] Zhenhao Li, Chuan Luo, Tse-Hsun Chen, Weiyi Shang, Shilin He, Qingwei Lin,\nand Dongmei Zhang. 2023. Did We Miss Something Important? Studying and Ex-\nploring Variable-Aware Log Abstraction. arXiv preprint arXiv:2304.11391 (2023).\n[38] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang,\nMohit Bansal, and Colin A Raffel. 2022. Few-shot parameter-efficient fine-tuning\nis better and cheaper than in-context learning. Advances in Neural Information\nProcessing Systems 35 (2022), 1950â€“1965.\n[39] Jinyang Liu, Junjie Huang, Yintong Huo, Zhihan Jiang, Jiazhen Gu, Zhuangbin\nChen, Cong Feng, Minzhi Yan, and Michael R Lyu. 2023. Scalable and Adap-\ntive Log-based Anomaly Detection with Expert in the Loop. arXiv preprint\narXiv:2306.05032 (2023).\n[40] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A\nRobustly Optimized BERT Pretraining Approach. CoRR abs/1907.11692 (2019).\narXiv:1907.11692 http://arxiv.org/abs/1907.11692\n[41] Yudong Liu, Xu Zhang, Shilin He, Hongyu Zhang, Liqun Li, Yu Kang, Yong Xu,\nMinghua Ma, Qingwei Lin, Yingnong Dang, et al. 2022. Uniparser: A unified log\nparser for heterogeneous log data. In Proceedings of the ACM Web Conference\n2022. 1893â€“1901.\n[42] Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization.\nIn International Conference on Learning Representations . https://openreview.net/\nforum?id=Bkg6RiCqY7\n[43] Siyang Lu, BingBing Rao, Xiang Wei, Byungchul Tak, Long Wang, and Liqiang\nWang. 2017. Log-based abnormal task detection and root cause analysis for spark.\nIn 2017 IEEE International Conference on Web Services (ICWS) . IEEE, 389â€“396.\n[44] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2021.\nFantastically ordered prompts and where to find them: Overcoming few-shot\nprompt order sensitivity. arXiv preprint arXiv:2104.08786 (2021).\n[45] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh\nHajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations:\nWhat makes in-context learning work? arXiv preprint arXiv:2202.12837 (2022).\n12\nLLMParser: An Exploratory Study on Using Large Language Models for Log Parsing ICSE â€™24, April 14â€“20, 2024, Lisbon, Portugal\n[46] Marius Mosbach, Tiago Pimentel, Shauli Ravfogel, Dietrich Klakow, and Yanai\nElazar. 2023. Few-shot Fine-tuning vs. In-context Learning: A Fair Comparison\nand Evaluation. arXiv preprint arXiv:2305.16938 (2023).\n[47] Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and\nIlya Sutskever. 2021. Deep double descent: Where bigger models and more data\nhurt. Journal of Statistical Mechanics: Theory and Experiment 2021, 12 (2021),\n124003.\n[48] Sasho Nedelkoski, Jasmin Bogatinovski, Alexander Acker, Jorge Cardoso, and\nOdej Kao. 2021. Self-supervised log parsing. In Machine Learning and Knowledge\nDiscovery in Databases: Applied Data Science Track: European Conference, ECML\nPKDD 2020, Ghent, Belgium, September 14â€“18, 2020, Proceedings, Part IV . Springer,\n122â€“138.\n[49] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al . 2018.\nImproving language understanding by generative pre-training. (2018).\n[50] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of\ntransfer learning with a unified text-to-text transformer. The Journal of Machine\nLearning Research 21, 1 (2020), 5485â€“5551.\n[51] Md Saidur Rahaman, MM Tahmid Ahsan, Nishath Anjum, Harold Jan R Terano,\nand Md Mizanur Rahman. 2023. From ChatGPT-3 to GPT-4: a significant advance-\nment in ai-driven NLP tools. Journal of Engineering and Emerging Technologies 2,\n1 (2023), 1â€“11.\n[52] Google Research. 2023. The Flan Collection: Advancing open source methods\nfor instruction tuning â€“ Google Research Blog. https://ai.googleblog.com/2023/\n02/the-flan-collection-advancing-open.html. (Accessed on 07/16/2023).\n[53] Keiichi Shima. 2016. Length matters: Clustering system log messages using\nlength of words. arXiv preprint arXiv:1611.03213 (2016).\n[54] Donghwan Shin, Zanis Ali Khan, Domenico Bianculli, and Lionel Briand. 2021. A\nTheoretical Framework for Understanding the Relationship Between Log Parsing\nand Anomaly Detection. In Runtime Verification: 21st International Conference,\nRV 2021, Virtual Event, October 11â€“14, 2021, Proceedings . Springer-Verlag, Berlin,\nHeidelberg, 277â€“287. https://doi.org/10.1007/978-3-030-88494-9_16\n[55] Lili Song, Ying Wang, Yinhe Han, Xin Zhao, Bosheng Liu, and Xiaowei Li. 2016.\nC-Brain: A Deep Learning Accelerator That Tames the Diversity of CNNs through\nAdaptive Data-Level Parallelization. In Proceedings of the 53rd Annual Design\nAutomation Conference (Austin, Texas) (DAC â€™16). Association for Computing\nMachinery, New York, NY, USA, Article 123, 6 pages. https://doi.org/10.1145/\n2897937.2897995\n[56] Liang Tang, Tao Li, and Chang-Shing Perng. 2011. LogSig: Generating System\nEvents from Raw Textual Logs. In Proceedings of the 20th ACM International\nConference on Information and Knowledge Management (Glasgow, Scotland, UK)\n(CIKM â€™11) . Association for Computing Machinery, New York, NY, USA, 785â€“794.\nhttps://doi.org/10.1145/2063576.2063690\n[57] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos\nGuestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford Alpaca: An\nInstruction-following LLaMA model. https://github.com/tatsu-lab/stanford_\nalpaca.\n[58] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv\npreprint arXiv:2302.13971 (2023).\n[59] R. Vaarandi. 2003. A data clustering algorithm for mining patterns from event\nlogs. In Proceedings of the 3rd IEEE Workshop on IP Operations & Management\n(IPOM 2003) (IEEE Cat. No.03EX764) . 119â€“126. https://doi.org/10.1109/IPOM.2003.\n1251233\n[60] Chi Wang, Susan Xueqing Liu, and Ahmed H Awadallah. 2023. Cost-Effective\nHyperparameter Optimization for Large Language Model Generation Inference.\narXiv preprint arXiv:2303.04673 (2023).\n[61] Peifeng Wang, Filip Ilievski, Muhao Chen, and Xiang Ren. 2021. Do lan-\nguage models perform generalizable commonsense inference? arXiv preprint\narXiv:2106.11533 (2021).\n[62] Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. 2020. Generalizing\nfrom a few examples: A survey on few-shot learning. ACM computing surveys\n(csur) 53, 3 (2020), 1â€“34.\n[63] Zehao Wang, Haoxiang Zhang, Tse-Hsun Chen, and Shaowei Wang. 2021. Would\nyou like a quick peek? providing logging support to monitor data processing in\nbig data applications. In Proceedings of the 29th ACM Joint Meeting on European\nSoftware Engineering Conference and Symposium on the Foundations of Software\nEngineering. 516â€“526.\n[64] Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and\nTom Goldstein. 2023. Hard prompts made easy: Gradient-based discrete op-\ntimization for prompt tuning and discovery. arXiv preprint arXiv:2302.03668\n(2023).\n[65] Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert,\nAshraf Elnashar, Jesse Spencer-Smith, and Douglas C Schmidt. 2023. A prompt\npattern catalog to enhance prompt engineering with chatgpt. arXiv preprint\narXiv:2302.11382 (2023).\n[66] Jules White, Sam Hays, Quchen Fu, Jesse Spencer-Smith, and Douglas C Schmidt.\n2023. Chatgpt prompt patterns for improving code quality, refactoring, require-\nments elicitation, and software design. arXiv preprint arXiv:2303.07839 (2023).\n[67] Ding Yuan, Haohui Mai, Weiwei Xiong, Lin Tan, Yuanyuan Zhou, and Shankar\nPasupathy. 2010. Sherlog: error diagnosis by connecting clues from run-time logs.\nIn Proceedings of the fifteenth International Conference on Architectural support\nfor programming languages and operating systems . 143â€“154.\n[68] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding,\nZhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma,\nYufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong,\nand Jie Tang. 2023. GLM-130B: An Open Bilingual Pre-trained Model. In The\nEleventh International Conference on Learning Representations (ICLR) . https:\n//openreview.net/forum?id=-Aw0rrrPUF\n[69] zero_nlp contributors. 2023. \"A large collection of large language model-powered\nsolutions in Chinese\". https://github.com/yuanzhoulvpi2017/zero_nlp/tree/main/\nsimple_thu_chatglm6b. (Accessed on 06/25/2023).\n[70] Bo Zhang, Hongyu Zhang, Pablo Moscato, and Aozhong Zhang. 2020. Anomaly\nDetection via Mining Numerical Workflow Relations from Logs. In 2020 Inter-\nnational Symposium on Reliable Distributed Systems (SRDS) . 195â€“204. https:\n//doi.org/10.1109/SRDS51746.2020.00027\n[71] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hong-\nsheng Li, Peng Gao, and Yu Qiao. 2023. Llama-adapter: Efficient fine-tuning of\nlanguage models with zero-init attention. arXiv preprint arXiv:2303.16199 (2023).\n[72] Xu Zhang, Yong Xu, Qingwei Lin, Bo Qiao, Hongyu Zhang, Yingnong Dang,\nChunyu Xie, Xinsheng Yang, Qian Cheng, Ze Li, et al. 2019. Robust log-based\nanomaly detection on unstable log data. In Proceedings of the 2019 27th ACM\nJoint Meeting on European Software Engineering Conference and Symposium on\nthe Foundations of Software Engineering . 807â€“817.\n[73] Xin Zhou, DongGyun Han, and David Lo. 2021. Assessing Generalizability of\nCodeBERT. In 2021 IEEE International Conference on Software Maintenance and\nEvolution (ICSME) . 425â€“436. https://doi.org/10.1109/ICSME52107.2021.00044\n[74] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis,\nHarris Chan, and Jimmy Ba. 2022. Large language models are human-level\nprompt engineers. arXiv preprint arXiv:2211.01910 (2022).\n[75] Jieming Zhu, Shilin He, Jinyang Liu, Pinjia He, Qi Xie, Zibin Zheng, and Michael R\nLyu. 2019. Tools and benchmarks for automated log parsing. In 2019 IEEE/ACM\n41st International Conference on Software Engineering: Software Engineering in\nPractice (ICSE-SEIP) . IEEE, 121â€“130.\n[76] Chen Zhuge and Risto Vaarandi. 2017. Efficient Event Log Mining with Log-\nClusterC. In 2017 ieee 3rd international conference on big data security on cloud\n(bigdatasecurity), ieee international conference on high performance and smart\ncomputing (hpsc), and ieee international conference on intelligent data and security\n(ids). 261â€“266. https://doi.org/10.1109/BigDataSecurity.2017.26\n13",
  "topic": "Parsing",
  "concepts": [
    {
      "name": "Parsing",
      "score": 0.8804379105567932
    },
    {
      "name": "Computer science",
      "score": 0.7702615261077881
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5186287760734558
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.44855695962905884
    },
    {
      "name": "Natural language processing",
      "score": 0.37560001015663147
    },
    {
      "name": "Machine learning",
      "score": 0.3458007276058197
    }
  ]
}