{
  "title": "MeLT: Message-Level Transformer with Masked Document Representations as Pre-Training for Stance Detection",
  "url": "https://openalex.org/W3201336341",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2963139714",
      "name": "Matthew Matero",
      "affiliations": [
        "Stony Brook University"
      ]
    },
    {
      "id": "https://openalex.org/A2140644356",
      "name": "Nikita Soni",
      "affiliations": [
        "Stony Brook University"
      ]
    },
    {
      "id": "https://openalex.org/A2116597581",
      "name": "Niranjan Balasubramanian",
      "affiliations": [
        "Stony Brook University"
      ]
    },
    {
      "id": "https://openalex.org/A2170210807",
      "name": "H. Andrew Schwartz",
      "affiliations": [
        "Stony Brook University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2971297872",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3171391618",
    "https://openalex.org/W3104152666",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2962897394",
    "https://openalex.org/W2963777125",
    "https://openalex.org/W2460159515",
    "https://openalex.org/W4287247091",
    "https://openalex.org/W3029434347",
    "https://openalex.org/W3128513378",
    "https://openalex.org/W2963530187",
    "https://openalex.org/W2964230653",
    "https://openalex.org/W3117943531",
    "https://openalex.org/W2984259019",
    "https://openalex.org/W2251409655",
    "https://openalex.org/W2949676527",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3148568192",
    "https://openalex.org/W2768048428",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3086641547",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W3035156228",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3163360376",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2962785754",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2950670227"
  ],
  "abstract": "Much of natural language processing is focused on leveraging large capacity language models, typically trained over single messages with a task of predicting one or more tokens. However, modeling human language at higher-levels of context (i.e., sequences of messages) is under-explored. In stance detection and other social media tasks where the goal is to predict an attribute of a message, we have contextual data that is loosely semantically connected by authorship. Here, we introduce Message-Level Transformer (MeLT) – a hierarchical message-encoder pre-trained over Twitter and applied to the task of stance prediction. We focus on stance prediction as a task benefiting from knowing the context of the message (i.e., the sequence of previous messages). The model is trained using a variant of masked-language modeling; where instead of predicting tokens, it seeks to generate an entire masked (aggregated) message vector via reconstruction loss. We find that applying this pre-trained masked message-level transformer to the downstream task of stance detection achieves F1 performance of 67%.",
  "full_text": "MeLT: Message-Level Transformer with Masked Document\nRepresentations as Pre-Training for Stance Detection\nMatthew Matero, Nikita Soni,\nNiranjan Balasubramanian, and H. Andrew Schwartz\nDepartment of Computer Science, Stony Brook University\n{mmatero, nisoni, niranjan, has}@cs.stonybrook.edu\nAbstract\nMuch of natural language processing is focused\non leveraging large capacity language models,\ntypically trained over single messages with a\ntask of predicting one or more tokens. However,\nmodeling human language at higher-levels of\ncontext (i.e., sequences of messages) is under-\nexplored. In stance detection and other social\nmedia tasks where the goal is to predict an at-\ntribute of a message, we have contextual data\nthat is loosely semantically connected by au-\nthorship. Here, we introduce Message-Level\nTransformer (MeLT) – a hierarchical message-\nencoder pre-trained over Twitter and applied\nto the task of stance prediction. We focus\non stance prediction as a task benefiting from\nknowing the context of the message (i.e., the\nsequence of previous messages). The model\nis trained using a variant of masked-language\nmodeling; where instead of predicting tokens, it\nseeks to generate an entire masked (aggregated)\nmessage vector via reconstruction loss. We find\nthat applying this pre-trained masked message-\nlevel transformer to the downstream task of\nstance detection achieves F1 performance of\n67%.\n1 Introduction\nGenerated by people, natural language data in-\nherently spans multiple levels of analysis, from\nindividual tokens, to documents (or messages),\nand to sequences of messages. While the multi-\nlevel aspect is rarely looked at beyond words-to-\ndocuments, some work has suggested benefits to\nmodeling language as a hierarchy, such as building\ndocument representations from a collection of its\nsentences or a user vector given a history of their\nlanguage (Song et al., 2020; Acheampong et al.,\n2021; Grail et al., 2021; Matero et al., 2019; Gane-\nsan et al., 2021).\nWe consider stance detection, a message-level\ntask, where the social or personal context in which\nthe message appears (e.g., such as a person’s pro-\nfile) has been shown relevant to capturing the stance\nof the message (Lynn et al., 2019; Aldayel and\nMagdy, 2019). However, such work explicitly inte-\ngrated user- or social-context into the stance model,\nas a separate component. We ask if there is a more\ndirect integration of user context when processing\na target message. To this end, we process the target\nmessage as a part of the sequence of messages from\nthe user. This way of using historical language\nfrom a person enables us to both model within mes-\nsage information (word-level) and to process the\nmessage within the author context (message-level).\nWhile there have been some models that take ad-\nvantage of hierarchy through words and sequences\nof messages (Lynn et al., 2020; Yu et al., 2020;\nZhao and Yang, 2020) there has been little work\nin providing generic pre-training routines for large\ncapacity transfer learning style models beyond the\nword-level. Instead, many of these hierarchical\nmodels are either applied directly to a downstream\ntask or, if pre-trained, on an adjacent version of\nthe downstream task. Being able to pre-train gen-\neral message-level models could enable inclusion\nof message-level contextual information that is\nnot easily obtainable with task-specific training\nthat is limited in data sizes as compared to larger\nunlabeled corpora available for modeling at the\nmessage-level.\nIn this study, we propose a hierarchical message-\nlevel transformer (MeLT) trained over a novel pre-\ntraining routine of Masked Document Modeling1,\nwhere the goal is to encode documents in latent\nspace using surrounding contextual documents. We\nthen fine-tune MeLT to a stance detection dataset\nderived from Twitter as defined in the SemEval\n2016 shared task (Mohammad et al., 2016). Our\ncontributions include: (1) introduction of a new\npre-training routine for hierarchical message-level\ntransformers2, (2) demonstration of efficacy of our\n1In this work a document is a single tweet (referred to as a\nmessage)\n2Code: https://github.com/MatthewMatero/MeLT\npre-training routine for stance detection, and (3)\nexploratory analysis comparing model size with\nrespect to the number of additional message-level\nlayers and amount of user history leveraged in fine-\ntuning.\n2 Related Work\nOur approach is inspired by the success word-to-\ndocument level transfer learning has had since\npopularized by the BERT language model (De-\nvlin et al., 2018). Offering the idea of a “contex-\ntual embedding\" allows models to properly dis-\nambiguate words based on their surrounding con-\ntext. While other types of language models are also\nused, usually autoregressive based such as GPT\nand XLNet (Brown et al., 2020; Yang et al., 2019),\nmany models are variants of the BERT autoencoder\nstyle (Liu et al., 2019; Lan et al., 2019).\nBoth Zhang et al. (2019) and Liu and Lapata\n(2019) use hierarchical encoder models for summa-\nrization tasks. While both models encode sentences\nusing some surrounding context, their pre-training\ntasks are still that of text generation rather than\nlatent modeling. Yu et al. (2020) encode global\ncontext in conversation threads on social media by\ngenerating a history vector (concatenated represen-\ntations of each sub-thread) during the fine-tuning\nstep and Zhao and Yang (2020) propose a capsule\nnetwork to aggregate fine-tuned word representa-\ntions to perform automatic stance detection.\nStance detection is an ideal task to develop MeLT\nbecause while it is labeled at the message-level, the\nstance itself is presumed to be held by the author\nwith a history of messages. Previous successful ap-\nproaches to stance detection have used topic mod-\neling, multi-task modeling via sentiment, multi-\ndataset training (Lin et al., 2017; Li and Caragea,\n2019; Schiller et al., 2021), or user-level informa-\ntion (Lynn et al., 2019; Aldayel and Magdy, 2019).\nOur work builds on this by using a pre-trained trans-\nformer trained to model message representations\nin latent space across author histories to encode\nglobal user knowledge into individual messages.\n3 Hierarchical Message Modeling\nMessages are made up of individual words that\ncome together to give each other context and mean-\ning. Comparably, a collection of messages can\ncome together to show topics of conversation. Di-\nrectly encoding the interactions of messages and\ntheir underlying words can prove beneficial when\nmodeling language at the document or person-level.\nFor example, processing post history of a social me-\ndia user within context of their own language.\n3.1 Masked-Document Reconstruction\nWe adapt the masked-language modeling (MLM)\napproach popularized by use in the BERT model\nto work for masked documents, rather than words.\nNamely, we introduce themasked-document model-\ning task, as shown in equation 1, where a message\nsequence is ordered by created time within a user’s\nhistory, some messages are selected for masking,\nand every message is represented as the average of\ntheir word tokens.\nˆMt = f(Mt−k, ..., maskedt, ..., Mt+k) +ϵ (1)\nHere, ˆMt is the reconstruction of the\nmasked out message M at step t through\nfunction f using the contextual messages\nMt−k, ..., Mt−1, Mt+1, ..., Mt+k with error\nrepresented as ϵ. Loss is calculated, as mean-\nsquared-error, against the ground-truth label of\nthe average representation of all words, Wi, that\nare present in the individual masked message\nshown in equations 2 and 3. Thus, making the\ntask latent space reconstruction where our model\nlearns to encode messages by rebuilding their local\nrepresentation using global context.\nLabel = avg(W0, W1, ..., Wn) (2)\nLoss = MSE ( ˆMt, label) (3)\nOur masking strategy follows the same rules as\nintroduced in BERT. Specifically, a message has a\n15% chance of being selected for masking. Once\nselected they are then replaced with a message\nMASK token (80% chance), left unchanged (10%\nchance), or replaced with a random message vector\n(10% chance).\n3.2 Message-level Transformer (MeLT)\nArchitecture Description We first select a pre-\ntrained word-level language model on which we\nbuild MeLT. This allows us to leverage models that\nhave already shown success in many NLP tasks\nrather than training from scratch.\nAfter processing messages at the word-level, we\naverage all individual word tokens within a mes-\nsage into a single message vector to build a se-\nquence of message vectors and then select mes-\nsages for masking. This process and architecture\nFigure 1: Pre-training architecture of our MeLT model.\nThe bottom layer indicates a collection of a user’s indi-\nvidual messages being processed by a word-level lan-\nguage model. Words within individual messages are\naggregated as averages and then ordered into a sequence\nof 768-dimensional message-vectors per user and mask-\ning is performed, represented by a red X. Reconstruction\nloss is then calculated with the predicted masked vector.\nis highlighted in figure 1, we refer to models us-\ning this setup as a “Message-level Transfomer\"\n(MeLT). Since the loss calculation as described in\nEq 3 relies on output from the word-level model it-\nself, that portion of the model is kept frozen during\npre-training.\nWe build 2 versions of MeLT, one with 2 hierar-\nchical layers (2L) and a 6-layer model (6L). After\nthe last transformer layer there is a single dense\nlinear layer which generates the final reconstructed\nrepresentation of any masked out messages.\nThese versions of MeLT are built on top of Distil-\nBERT (base) (Sanh et al., 2019) for the following\nreasons: (1) it is a smaller model (6 layers) allow-\ning more GPU space for message-level layers and\n(2) while being roughly half the size of the origi-\nnal BERT it still offers upwards of 95% the perfor-\nmance. We also explore an alternate model built-on\ntop of DistilRoBERTa (base) to compare the utility\nof MeLT applied to other word-level models.\nTraining Instances For training we set the fol-\nlowing restrictions for individual users: (1) we set\na max history length of 40 for number of messages\nper sequence and (2) for users with more than 40\nmessages they are chunked and processed as sepa-\nrate sequences. Users with fewer than 40 total mes-\nModel F1 Prec Recall SemEval F1\nMFC 54 67 78 67\n(Zarrella, 2016) – – – 68†\n(Zhao, 2020) – – – 78†\nDistilBert 60 60 63 63\nDistilBert + Hist 63 64 65 68\n(Lynn, 2019) 66 – – –\nMeLT 67 68 67 73\nTable 1: Evaluation of various methods applied to\nSemEval stance detection. We report both weighted\nF1/Prec/Recall and Avg pos/neg F1 as defined in the\noriginal shared task. MFC is a most frequent class base-\nline, DistilBert and DistilBert + Hist represent an av-\nerage message vector extracted from DistilBERT with\nor without concatenation of an average vector repre-\nsenting user history, respectively. MeLT is our best\nperforming variant. Bold results are found significant\nwith p < .05 w.r.t DistilBert + Hist using a paired t-test.\n(†) indicates a model trained on the original version of\nthe SemEval2016 dataset (4,100 total tweets) which we\ndid not have available due to accounts or messages be-\ning deleted on twitter since release.\n.\nsages have message-level PAD tokens appended\nto their sequence. However, users that have multi-\nple sequences will not be assigned a PAD token, if\ntheir last sequence falls short of 40 we include the\namount of missing messages from their previous\nsequence.\nDataset For pre-training our model we select\nusers from publicly available tweets that were pre-\nviously used for other user-level predictions, such\nas demographic prediction or emotion forecast-\ning (V olkova et al., 2013; Matero and Schwartz,\n2020). A subset of data is selected as our pre-\ntraining dataset, approximately 10 million tweets\nsampled from 6 thousand users, resulting in a\ndataset 1.3 GB in size. We use a limited dataset\nto highlight the utility of the pre-training routine\nitself and not rely on “bigger is better\" mindset.\n4 Stance Detection with MeLT\nWe use the stance dataset available from the Se-\nmEval 2016 shared task (Mohammad et al., 2016).\nThis data includes tweets that were annotated either\nagainst, neutral, or favoring of a specific target men-\ntioned within the tweet, across 5 distinct targets in\nthe dataset. However, this data only includes la-\nbeled tweets from users and not any history, so we\nuse the extended dataset from Lynn et al. (2019).\nDuring fine-tuning we keep a max history length\nof 40 and a temporal ordering within sequence. We\nModel Abortion Atheism Climate Clinton Feminism All(Avg)\nWord-level Pre-train\nDistilBert 60 66 70 58 46 60\nDistilBert + Hist 64 62 70 64 54 63\nMsg-level Pre-train\n2L MeLT-rand 56 62 61 47 46 54\n6L MeLT-rand 56 62 61 47 46 54\n2L MeLT + frz word 58 64 66 54 51 59\n2L MeLT + unfrz word 66 67 74 58 59 65\n6L MeLT + frz word 62 66 68 60 53 62\n6L MeLT + unfrz word 66 66 71 67 63 67\nTable 2: Performance analysis on weighted F1 among all our models across each target within the SemEval dataset.\nMeLT-rand is our architecture applied directly to the task(no pre-train routine) and frz/unfrz word indicates whether\nthe underlying word-level model was also updated while fine-tuning. Bold indicates best in column.\nFigure 2: Average weighted-F1 performance across our\nmodels when we fine-tune using different amounts of\nuser history. Both size MeLTs improve when more\nhistory is available, with a plateau occurring on the 2-\nlayer model.\napply a 2-layer feed-forward neural net with a Sig-\nmoid activation on top of our MeLT and leave all\nmessage transformer layers unfrozen. Experiments\nwith both frozen and unfrozen word-level layers are\nalso explored. The message vector representation\nfrom the top transformer layer of MeLT is used as\ninput into the fine-tuning layers.\n5 Results\nWe show a comparison of our best MeLT model\nagainst other approaches in table 1. First, we in-\nclude a heuristic baseline of most-frequent-class\nprediction. Next, we compare against fine-tuning\nour word-level model of choice directly to the\ndownstream task using 2 configurations. The first\nis using only the message representation, while the\nsecond is “+ history\" where we concatenate it with\nModel F1 Prec Rec\nWord-level Pre-train\nDistilRoBERTa 59 55 57\nDistilRoBERTa + History 61 68 66\nMsg-level Pre-train\n2L MeLT DistilRoBERTa 62 69 66\n6L MeLT DistilRoBERTa 64 69 69\nTable 3: Evaluation of using a different word-level\nmodel for our experiments (DistilRoBERTa). All MeLT\nvariants are fine-tuned with the word-level model un-\nfrozen. While we do not see this version outperform the\nDistilBERT variant, there are still clear benefits from\nusing MeLT over just the word-level distil-RoBERTa.\nBold results are found to be significant with p < .05\nw.r.t DistilRoBERTa + History.\nthe average of 40 recent messages. This allows\nthe model to have a global context within user. We\nalso include the top participant from the shared task\nZarrella and Marsh (2016) which uses a different\nF1 score as defined for the shared task, referred\nto here as SemEval F13. Lastly, we compare our\nresults to the approach of Lynn et al. (2019), from\nwhom we received the extended history dataset,\nwhich uses the labeled tweet and a list of accounts\nthe author follows. However, they only report the\nweighted-F1 score for their best performing model.\nWe find that fine-tuning DistilBERT directly to\nthe task of stance detection proves difficult, only\nscoring a modest F1. However when we include\nsome context language from the user, an average\nrepresentation of their recent language concate-\n3This F1 score instead reports an average of the F-score\nfor the positive and negative classes. Not directly accounting\nfor neutral predictions.\nnated into the fine-tuning layer, there is a notice-\nable boost in performance highlighting that stance\nprediction is aided by knowing the context of the\nmessage. We find that MeLT can utilize this con-\ntextual information best and out-performs other\napproaches.\nNext, we break down the performance of various\nconfigurations of our models in table 2 across each\ntarget. Here, we compare against a small variant\nof MeLT (2Layers), randomly initialized MeLTs\n(No pre-train)4, and also experiments with frozen\nand unfrozen word-level models. Ultimately, we\nfind that fine-tuning both the word and message lev-\nels simultaneously consistently proves beneficial,\nlikely due to the word model being able to adapt to\ndiscourse on Twitter.\nWe also find that the 2-layer MeLT performs\ncompetitively - in figure 2 we show that it per-\nforms better or on-par with the large model until\n40 messages of history is reached, due to the 2-\nlayer model saturating at history of 30. Suggesting\nthat the larger the model, the more history it can\nefficiently track.\nLastly, we investigate using a different word-\nlevel model for our experiments. We choose Dis-\ntilRoBERTa, for similar reasons to our original\nchoice of DistilBERT, and apply the same tech-\nniques as done with DistilBERT shown in table 3.\nWe find that overall each DistilRoBERTa model\nachieves lower F1 score than the respective Distil-\nBERT variant. However we find that MeLT still im-\nproves over the base word-level model, suggesting\nthat MeLT often will improve the word-level model\nitself but the word-level model of choice plays an\nimportant role in downstream performance. Due to\nthis, it is likely to be beneficial to first evaluate a\nvariety of word-level models on your downstream\ntask and then build on top of the best one with\nMeLT.\n6 Conclusion\nWith a large number of tasks in NLP that rely\non social media as a domain, methods which can\nmodel language as a multi-level phenomena, from\nwords to documents to people, can offer a higher-\nlevel contextual representation of language. In this\nwork, we presented a new hierarchical pre-training\nroutine that, when fine-tuned to stance detection,\noutperforms other models utilizing both message\nand user-level information as well as improves re-\n4Both MeLT-rands learn the MFC baseline\nsults upon solely using the word-level model on\nwhich we build MeLT. We also find that during\nfine-tuning, it was always beneficial to unfreeze the\nword layers even though they had to be frozen dur-\ning pre-training. MeLT can be attached to the top\nof a word-level language model in order to directly\nencode sequences of message vectors, thus allow-\ning the modeling of historical context and leading\ntowards a way of approaching language modeling\nthat integrates its personal context.\n7 Acknowledgements\nThis work was supported in part by a grant from the\nNational Institutes of Health, R01 AA028032-01\nand in part by a grant from the National Science\nFoundation, IIS-1815358.\nReferences\nFrancisca Adoma Acheampong, Henry Nunoo-Mensah,\nand Wenyu Chen. 2021. Transformer models for\ntext-based emotion detection: a review of bert-based\napproaches. Artificial Intelligence Review, pages 1–\n41.\nTakuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru\nOhta, and Masanori Koyama. 2019. Optuna: A next-\ngeneration hyperparameter optimization framework.\nIn Proceedings of the 25rd ACM SIGKDD Interna-\ntional Conference on Knowledge Discovery and Data\nMining.\nAbeer Aldayel and Walid Magdy. 2019. Your stance\nis exposed! analysing possible factors for stance\ndetection on social media. Proceedings of the ACM\non Human-Computer Interaction, 3(CSCW):1–20.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\net al. Falcon, WA. 2019. Pytorch lightning. GitHub.\nNote: https://github.com/PyTorchLightning/pytorch-\nlightning, 3.\nAdithya V Ganesan, Matthew Matero, Aravind Reddy\nRavula, Huy Vu, and H Andrew Schwartz. 2021.\nEmpirical evaluation of pre-trained transformers for\nhuman-level nlp: The role of sample size and dimen-\nsionality. arXiv preprint arXiv:2105.03484.\nQuentin Grail, Julien Perez, and Eric Gaussier. 2021.\nGlobalizing BERT-based transformer architectures\nfor long document summarization. In Proceedings\nof the 16th Conference of the European Chapter of\nthe Association for Computational Linguistics: Main\nVolume, pages 1792–1810, Online. Association for\nComputational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv preprint\narXiv:1909.11942.\nYingjie Li and Cornelia Caragea. 2019. Multi-task\nstance detection with sentiment and stance lexicons.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 6300–6306.\nJunjie Lin, Wenji Mao, and Yuhao Zhang. 2017. An\nenhanced topic modeling approach to multiple stance\nidentification. In Proceedings of the 2017 ACM on\nConference on Information and Knowledge Manage-\nment, pages 2167–2170.\nYang Liu and Mirella Lapata. 2019. Hierarchical trans-\nformers for multi-document summarization. arXiv\npreprint arXiv:1905.13164.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2017. Decou-\npled weight decay regularization. arXiv preprint\narXiv:1711.05101.\nVeronica Lynn, Niranjan Balasubramanian, and H An-\ndrew Schwartz. 2020. Hierarchical modeling for user\npersonality prediction: The role of message-level\nattention. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5306–5316.\nVeronica Lynn, Salvatore Giorgi, Niranjan Balasubrama-\nnian, and H. Andrew Schwartz. 2019. Tweet classifi-\ncation without the tweet: An empirical examination\nof user versus document attributes. In Proceedings\nof the Third Workshop on Natural Language Process-\ning and Computational Social Science, pages 18–28,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nMatthew Matero, Akash Idnani, Youngseo Son, Sal-\nvatore Giorgi, Huy Vu, Mohammad Zamani, Parth\nLimbachiya, Sharath Chandra Guntuku, and H An-\ndrew Schwartz. 2019. Suicide risk assessment with\nmulti-level dual-context language and bert. In Pro-\nceedings of the Sixth Workshop on Computational\nLinguistics and Clinical Psychology, pages 39–44.\nMatthew Matero and H. Andrew Schwartz. 2020. Au-\ntoregressive affective language forecasting: A self-\nsupervised task. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics,\npages 2913–2923, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\nSaif Mohammad, Svetlana Kiritchenko, Parinaz Sob-\nhani, Xiaodan Zhu, and Colin Cherry. 2016. Semeval-\n2016 task 6: Detecting stance in tweets. In Proceed-\nings of the 10th International Workshop on Semantic\nEvaluation (SemEval-2016), pages 31–41.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch:\nAn imperative style, high-performance deep learning\nlibrary. In H. Wallach, H. Larochelle, A. Beygelz-\nimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems\n32, pages 8024–8035. Curran Associates, Inc.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nBenjamin Schiller, Johannes Daxenberger, and Iryna\nGurevych. 2021. Stance detection benchmark: How\nrobust is your stance detection? KI-Künstliche Intel-\nligenz, pages 1–13.\nXingyi Song, Johnny Downs, Sumithra Velupillai,\nRachel Holden, Maxim Kikoler, Kalina Bontcheva,\nRina Dutta, and Angus Roberts. 2020. Using deep\nneural networks with intra- and inter-sentence con-\ntext to classify suicidal behaviour. In Proceedings of\nthe 12th Language Resources and Evaluation Confer-\nence, pages 1303–1310, Marseille, France. European\nLanguage Resources Association.\nSvitlana V olkova, Theresa Wilson, and David Yarowsky.\n2013. Exploring demographic language variations\nto improve multilingual sentiment analysis in social\nmedia. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1815–1827.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv:1906.08237.\nJianfei Yu, Jing Jiang, Ling Min Serena Khoo,\nHai Leong Chieu, and Rui Xia. 2020. Coupled hier-\narchical transformer for stance-aware rumor verifica-\ntion in social media conversations. Association for\nComputational Linguistics.\nGuido Zarrella and Amy Marsh. 2016. Mitre at semeval-\n2016 task 6: Transfer learning for stance detection.\narXiv preprint arXiv:1606.03784.\nXingxing Zhang, Furu Wei, and Ming Zhou. 2019. Hib-\nert: Document level pre-training of hierarchical bidi-\nrectional transformers for document summarization.\narXiv preprint arXiv:1905.06566.\nGuangzhen Zhao and Peng Yang. 2020. Pretrained\nembeddings for stance detection with hierarchical\ncapsule network on social media. ACM Transactions\non Information Systems (TOIS), 39(1):1–32.\nA Appendix\nA.1 Implementation and Hardware Details\nPre-training of all models was performed across 3\nTitanXP GPUs(12GB mem each) while fine-tuning\nwas performed on a single TitanXP. All models\nwere implemented using PyTorch (Paszke et al.,\n2019) with the PyTorch Lightning Add-on (Falcon,\n2019).\nDuring pre-training batch size was set to 100\nusers and fine-tuning was performed using 10. For\npre-training runtime was around 2.5 hours per\nepoch and fine-tuning was a few minutes per epoch.\nMeLT 2L adds 11,621,632 trainable parameters on\ntop of DistilBERT and MeLT 6L adds 33,677,568,\nas counted by summing PyTorch tensor.numel()\nper parameter with gradients turned on 5. All ex-\nperiments (pre-training and fine-tuning) use the\nAdamW Optimizer (Loshchilov and Hutter, 2017)\nand use random seed set to 1337. Pre-training has\na warm-up period of 2,000 steps.\nPre-training is conducted over 5 epochs with\ncheckpoints saved for the epoch that scored the\nlowest MSE on a holdout development set. The\nversion of the model at that checkpoint is then used\nfor fine-tuning to the stance dataset.\nA.2 Hyperparams\nAll hyperparameters are selected via tuning using\nthe Optuna library (Akiba et al., 2019).\nA.2.1 Pre-training\nThe final set of hyperparameters used for the 6L\nMeLT model (pre-training) are as follows:\n• Learning Rate: 4e-3\n• Weight Decay: 0.1\n• Dropout: 0.1\n• FF dim: 2048\n• Embed dim: 768\n• Attn Heads: 8\n• Epochs: 5 (checkpoint at epoch 2)\n• batch size: 100 (users)\n• msg seq len: 40 (per user)\n• token seq len: 50 (per message)\n5https://discuss.pytorch.org/t/how-do-i-check-the-\nnumber-of-parameters-of-a-model/4325\nIf any parameter is not mentioned (e.g., Adam\nBetas) then it uses PyTorch defaults. For pre-\ntraining 10 trials were used for parameter tuning.\nFor pre-training only learning rate and weight de-\ncay were explored. Learning rate was searched\nbetween 5e-4 to 4e-1 and weight decay was set\nbetween 1 and 1e-4.\nA.2.2 Fine-Tuning\nAll hyperparameters were chosen based on min-\nimizing loss over a holdout development set for\neach target over 50 trials. Hyper-parameters that\nare tuned include learning rate, weight decay, and\ndropout. Dropout is applied directly to output from\nMeLT. Learning rate was searched between 6e-6\nand 3e-3, weight decay is between 1 and 1e-4, and\ndropout is 0.0 to 0.05. Additionally, early stopping\nwas also applied as a means of regularization.\nThe 2-layer FFNN on top of MeLT during fine-\ntuning has layer 1 of dimension 768 and layer 2 of\ndimension 384, with Sigmoid between.\nA.3 Data\nA.3.1 pre-training\nThe pre-training dataset is comprised of 6,000 users\nand 9,868,429 messages. For a development set\nwe select 3,000 users from our train set and set\naside an additional 20 of their messages, to measure\nreconstruction loss within these sequences.\nA.3.2 fine-tuning\nThe breakdown of number of examples (labeled\nmessages) across train/dev/test for each target in\nthe SemEval Stance data is shown in table 4. In\ntotal we have 3,021 instances with a split of 1658\ntrain, 418 dev, and 945 test across all targets. The\noriginal 2016 shared task had 4,100 instances, how-\never due to accounts or messages being deleted\nover time, we were unable to replicate the com-\nplete original dataset and instead used the smaller\nversion available from Lynn et al. (2019).\nTarget Train Dev Test\nAbortion 380 96 207\nAtheism 329 83 178\nClimate Change 257 65 145\nHilary Clinton 372 94 232\nFeminism 320 80 183\nTotal 1658 418 945\nTable 4: Number of examples per target in SemEval\ndata as broken down by split of the data.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8314186334609985
    },
    {
      "name": "Transformer",
      "score": 0.7739728689193726
    },
    {
      "name": "Language model",
      "score": 0.669213056564331
    },
    {
      "name": "Encoder",
      "score": 0.5593610405921936
    },
    {
      "name": "Natural language processing",
      "score": 0.5421586632728577
    },
    {
      "name": "Task (project management)",
      "score": 0.5392460227012634
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5043786764144897
    },
    {
      "name": "Focus (optics)",
      "score": 0.49039363861083984
    },
    {
      "name": "Context model",
      "score": 0.4542350172996521
    },
    {
      "name": "Natural language",
      "score": 0.43323901295661926
    },
    {
      "name": "Message passing",
      "score": 0.4156762361526489
    },
    {
      "name": "Speech recognition",
      "score": 0.40079447627067566
    },
    {
      "name": "Machine learning",
      "score": 0.36586105823516846
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Object (grammar)",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I59553526",
      "name": "Stony Brook University",
      "country": "US"
    }
  ]
}