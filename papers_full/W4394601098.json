{
    "title": "Vision transformer to differentiate between benign and malignant slices in 18F-FDG PET/CT",
    "url": "https://openalex.org/W4394601098",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2998864200",
            "name": "Daiki Nishigaki",
            "affiliations": [
                "Osaka University"
            ]
        },
        {
            "id": "https://openalex.org/A2101126442",
            "name": "Yuki Suzuki",
            "affiliations": [
                "Osaka University"
            ]
        },
        {
            "id": "https://openalex.org/A2164639163",
            "name": "Tadashi Watabe",
            "affiliations": [
                "Osaka University"
            ]
        },
        {
            "id": "https://openalex.org/A2484392903",
            "name": "Daisuke Katayama",
            "affiliations": [
                "Osaka University"
            ]
        },
        {
            "id": "https://openalex.org/A1998355279",
            "name": "Hiroki Kato",
            "affiliations": [
                "Osaka University"
            ]
        },
        {
            "id": "https://openalex.org/A3036709528",
            "name": "Tomohiro Wataya",
            "affiliations": [
                "Osaka University"
            ]
        },
        {
            "id": "https://openalex.org/A2142142809",
            "name": "Kosuke Kita",
            "affiliations": [
                "Osaka University"
            ]
        },
        {
            "id": "https://openalex.org/A2043548577",
            "name": "Junya Sato",
            "affiliations": [
                "Osaka University"
            ]
        },
        {
            "id": "https://openalex.org/A1944692852",
            "name": "Noriyuki Tomiyama",
            "affiliations": [
                "Osaka University"
            ]
        },
        {
            "id": "https://openalex.org/A2122706386",
            "name": "Shoji Kido",
            "affiliations": [
                "Osaka University"
            ]
        },
        {
            "id": "https://openalex.org/A2998864200",
            "name": "Daiki Nishigaki",
            "affiliations": [
                "Osaka University"
            ]
        },
        {
            "id": "https://openalex.org/A2101126442",
            "name": "Yuki Suzuki",
            "affiliations": [
                "Osaka University"
            ]
        },
        {
            "id": "https://openalex.org/A2164639163",
            "name": "Tadashi Watabe",
            "affiliations": [
                "Osaka University"
            ]
        },
        {
            "id": "https://openalex.org/A2484392903",
            "name": "Daisuke Katayama",
            "affiliations": [
                "Osaka University"
            ]
        },
        {
            "id": "https://openalex.org/A1998355279",
            "name": "Hiroki Kato",
            "affiliations": [
                "Osaka University"
            ]
        },
        {
            "id": "https://openalex.org/A3036709528",
            "name": "Tomohiro Wataya",
            "affiliations": [
                "Osaka University"
            ]
        },
        {
            "id": "https://openalex.org/A2142142809",
            "name": "Kosuke Kita",
            "affiliations": [
                "Osaka University"
            ]
        },
        {
            "id": "https://openalex.org/A2043548577",
            "name": "Junya Sato",
            "affiliations": [
                "Osaka University"
            ]
        },
        {
            "id": "https://openalex.org/A1944692852",
            "name": "Noriyuki Tomiyama",
            "affiliations": [
                "Osaka University"
            ]
        },
        {
            "id": "https://openalex.org/A2122706386",
            "name": "Shoji Kido",
            "affiliations": [
                "Osaka University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2043680485",
        "https://openalex.org/W2022380377",
        "https://openalex.org/W1991385862",
        "https://openalex.org/W2804618064",
        "https://openalex.org/W2170384969",
        "https://openalex.org/W1586312274",
        "https://openalex.org/W3188147279",
        "https://openalex.org/W2903867357",
        "https://openalex.org/W2994824962",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W3207549851",
        "https://openalex.org/W4226275002",
        "https://openalex.org/W2056453212",
        "https://openalex.org/W2963446712",
        "https://openalex.org/W3161761924",
        "https://openalex.org/W2560674852",
        "https://openalex.org/W3080406710",
        "https://openalex.org/W3139916062",
        "https://openalex.org/W2962858109",
        "https://openalex.org/W2006617902",
        "https://openalex.org/W4390042884",
        "https://openalex.org/W3138559567",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W4296708724",
        "https://openalex.org/W3011721937",
        "https://openalex.org/W2901954625",
        "https://openalex.org/W3041695882",
        "https://openalex.org/W3188036383",
        "https://openalex.org/W3082598879",
        "https://openalex.org/W3013734615",
        "https://openalex.org/W1843184004",
        "https://openalex.org/W4226325015",
        "https://openalex.org/W2067002864",
        "https://openalex.org/W2188831112"
    ],
    "abstract": null,
    "full_text": "1\nVol.:(0123456789)Scientific Reports |         (2024) 14:8334  | https://doi.org/10.1038/s41598-024-58220-6\nwww.nature.com/scientificreports\nVision transformer to differentiate \nbetween benign and malignant \nslices in 18F‑FDG PET/CT\nDaiki Nishigaki 1, Yuki Suzuki 1, Tadashi Watabe 2, Daisuke Katayama 2, Hiroki Kato 2, \nTomohiro Wataya 1, Kosuke Kita 1, Junya Sato 1, Noriyuki Tomiyama 3 & Shoji Kido 1*\nFluorine‑18‑fluorodeoxyglucose (18F‑FDG) positron emission tomography (PET)/computed \ntomography (CT) is widely used for the detection, diagnosis, and clinical decision‑making in \noncological diseases. However, in daily medical practice, it is often difficult to make clinical \ndecisions because of physiological FDG uptake or cancers with poor FDG uptake. False negative \nclinical diagnoses of malignant lesions are critical issues that require attention. In this study, Vision \nTransformer (ViT) was used to automatically classify 18F‑FDG PET/CT slices as benign or malignant. \nThis retrospective study included 18F‑FDG PET/CT data of 207 (143 malignant and 64 benign) patients \nfrom a medical institute to train and test our models. The ViT model achieved an area under the \nreceiver operating characteristic curve (AUC) of 0.90 [95% CI 0.89, 0.91], which was superior to the \nbaseline Convolutional Neural Network (CNN) models (EfficientNet, 0.87 [95% CI 0.86, 0.88], P < 0.001; \nDenseNet, 0.87 [95% CI 0.86, 0.88], P < 0.001). Even when FDG uptake was low, ViT produced an AUC \nof 0.81 [95% CI 0.77, 0.85], which was higher than that of the CNN (DenseNet, 0.65 [95% CI 0.59, \n0.70], P < 0.001). We demonstrated the clinical value of ViT by showing its sensitive analysis of easy‑to‑\nmiss cases of oncological diseases.\nFluorine-18-fluorodeoxyglucose (18F-FDG) positron emission tomography (PET)/computed tomography (CT) \nis a molecular imaging technique widely used for the detection, diagnosis, and clinical decision-making for \nmetabolically active lesions, including oncological  diseases1,2. 18F-FDG uptake provides functional information \non the metabolic activity of lesions and highlights where malignant tumors are present. PET/CT reliably differ-\nentiates benign tumors from malignant tumors by combining anatomical information from CT with functional \ninformation from  PET3–6. However, in daily medical practice, it is common to have difficulty in making clinical \ndecisions because of physiological FDG uptake or malignant lesions with poor FDG  uptake7,8. Therefore, a wealth \nof specialized knowledge and experience is required to detect and differentiate between various abnormalities. \nThis is particularly evident in the abdominopelvic region, where multiple organs exhibit physiological FDG \nuptake (such as the kidney, ureter, bladder, liver, intestinal tract, and adrenal gland), and where there is significant \ndiversity in cancer origin and uptake levels. Currently, the number of experienced specialists in nuclear medicine \nis limited, whereas the number of PET/CT examinations is  increasing9. As the burden on specialists in nuclear \nmedicine increases, the risk of overlooking malignant lesions and misdiagnosis increases. Thus, there is a need \nfor automated systems to analyze PET/CT images more efficiently.\nFor the automated classification of PET/CT images as benign or malignant, it is necessary to use functional \ninformation based on FDG uptake as well as anatomical information of the entire image (such as the distribution \nof lesions and their position relative to organs). Convolutional Neural Network (CNN) is a machine learning \nalgorithm that has performed well in computer vision applications. However, CNN was reported to have no \naccess to global information of the image, although it can obtain local  features10.  Sibille11 et al. input lesions with \nhigh FDG uptake into CNN instead of inputting the entire image. Their CNN-based system achieved high area \nunder the receiver operating characteristic curves (AUCs) for automated cancer classification (lung cancer, 0.98; \nlymphoma, 0.95). However, this method has the limitation of being unable to evaluate lesions with poor FDG \nuptake. In clinical practice, overlooking such lesions and false negatives are critical issues, and there is a great \nneed for a system to prevent these problems.\nOPEN\n1Department of Artificial Intelligence Diagnostic Radiology, Osaka University Graduate School of Medicine, \nSuita, Osaka, Japan. 2Department of Nuclear Medicine and Tracer Kinetics, Osaka University Graduate School of \nMedicine, Suita, Osaka, Japan. 3Department of Radiology, Osaka University Graduate School of Medicine, Suita, \nOsaka, Japan. *email: kido@radiol.med.osaka-u.ac.jp\n2\nVol:.(1234567890)Scientific Reports |         (2024) 14:8334  | https://doi.org/10.1038/s41598-024-58220-6\nwww.nature.com/scientificreports/\nVision Transformer (ViT) is an application of the transformer architecture developed for natural language \nprocessing to image  classification12,13. The advantage of ViT compared with CNN is that it can integrate informa-\ntion across the entire image; ViT has the potential to outperform CNN when trained with sufficient data. Even \nwith a small data set, it was reported that transfer learning with pretrained ViT achieved high  performance12. ViT \nmodels pretrained on a large natural image data set, ImageNet, are publicly  available14. Previous studies reported \nthat fine-tuning such pretrained models for the analysis of medical images produced better performance than \nexisting CNNs to detect COVID-19 positive  cases15,16.\nIn this study, we developed and evaluated a ViT model that differentiated PET/CT slices as benign or malig-\nnant. The primary aim was to compare the performance of ViT and baseline CNN models. The secondary aim \nwas to examine the impact of the degree of FDG activity in images on the performance of the models.\nMaterials and methods\nEthical approval\nThis retrospective study was approved by the institutional review boards of Osaka University Hospital (Suita, \nOsaka, Japan). Informed consent was waived due to the retrospective nature of the study. All procedures per -\nformed in this study involving human participants were in accordance with the ethical standards of the institu-\ntional research ethics committee and with the 1964 Helsinki declaration and its later amendments or comparable \nethical standards.\nClinical data\nWe retrospectively collected 143 patients with active abdominopelvic cancer and 64 patients without any active \ncancer, who underwent whole-body PET/CT at the Osaka University Hospital (Suita, Osaka, Japan) from January \n2020 to August 2021. First, we used keyword searches in the radiology information system to collect examina -\ntions of patients with and without abdominopelvic cancer. Next, examinations other than the first one for each \npatient were excluded (51 cases). Third, a radiologist (D.N., 2 years of experience, in-training) inspected clinical \ninformation including radiology reports and medical records to determine the presence or absence of malignant \nfindings. Cases without abdominopelvic cancer were excluded from the positive patient group (73 cases). Cases \nwith malignant findings in any part of the body were excluded from the negative patient group (185 cases). \nFinally, cases with missing image data were excluded (8 cases). The final enrollment of 207 patients was randomly \ndivided into training (60%), validation (15%), and test (25%) subsets while maintaining the positive-to-negative \ncase ratio. Figure 1 shows the flowchart of patient inclusion and data partitioning.\nPatients were scanned by scanner 1 (N = 81, Biograph Vision 600, Siemens Medical Solutions, Knoxville, TN, \nUSA) or scanner 2 (N  = 126, Discovery 710, GE, Milwaukee, WI, USA) at our institute. Patients fasted for six \nhours and were injected with 3.7 MBq per kilogram of body weight 18F-FDG and imaged 60 min after injection. \nThe image acquisition details are shown in Supplementary Table 1 online.\nData preprocessing\nWe acquired CT, PET, and PET/CT fusion image data in the DICOM format, which were used for clinical diag-\nnosis and analysis and were stored at our institution. PET/CT fusion images are color images that overlay func-\ntional maps from PET onto anatomical maps from CT to facilitate the interpretation of bimodal information in \nclinical  practice17,18. In our institution, PET/CT images were produced by fusing PET images with CT images at \na 1:1 ratio using the Hot Iron color scale. In this study, we used axial image data of each patient’s abdominopelvic \nregion from the diaphragm to the bladder. The flow chart of the image preprocessing is shown in Supplementary \nFig. 1 online. PET and CT images were separately converted from grayscale (1 channel) to RGB (3 channels) by \nduplicating channels. All images were converted to 256 × 256-pixel RGB images in a similar manner, combining \nmin–max normalization, center-cropping, connected-component labeling, resizing, and cutting margin. The \npreprocessing was performed using Python version 3.8.5, Pydicom version 2.1.2 (https:// github. com/ pydic om/ \npydic om), Pillow version 8.2.0 (https:// github. com/ python- pillow/ Pillow), and OpenCV version 4.5.3 (https:// \ngithub. com/ opencv/ opencv).\nReference standard\nOur reference standard was composed of two types of data: pathological diagnosis and image reading. The refer-\nence standard of malignant/benign diagnosis was determined according to pathological evidence when avail-\nable. Lesions without histopathological diagnosis were classified through nuclear medicine expert readings. The \nslice-level reading-based annotation was performed using preprocessed images by two board-certified nuclear \nmedicine experts. Images that contained potentially malignant FDG uptake (eg, primary tumor, metastases, dis-\nseminated lesions, and malignant ascites) were annotated as “positive. ” Images with no suspicion (eg, no findings, \nphysiologic uptake, bone degeneration, and inflammation) were annotated as “negative. ” First, an expert (D.K., \n9 years of experience) annotated all images. Then, the first annotations were double-checked by another expert \n(T.W ., 15 years of experience). Discrepancies between annotators were resolved by consensus agreement. The \nexperts holistically evaluated a set of three modality images (PET/CT, PET, and CT) for each slice, assigning \nthe same label to all three images of the same slice. All clinical information, including patient background and \nradiology reports, was blinded for the annotators during the annotation process. In the test data, the experts \nmanually placed bounding boxes around malignant/benign lesions as the foundation for their decisions. These \nbounding boxes were utilized for calculating lesion size (length of longer edge) and for qualitative evaluation of \nthe models. Microsoft VoTT version 2.2.0 (https:// github. com/ micro soft/ VoTT) was used for the data annotation.\n3\nVol.:(0123456789)Scientific Reports |         (2024) 14:8334  | https://doi.org/10.1038/s41598-024-58220-6\nwww.nature.com/scientificreports/\nViT model\nWe used the ViT model to classify PET/CT images as “positive” or “negative. ” We used the primary ViT model, \nB-16 (“Base” variant) without modification, which consists of 23 transformer encoder blocks stacked on top of \neach other, with a patch size of 16 × 16. The overall architecture is shown in Fig. 2, and the network architecture \ndetails are as follows. Because ViT treats image data as a sequence of small patches, the initial part of the network \nhas a patch encoder layer that reshapes the input image into multiple flattened patches. Next, position embed-\ndings are added to the patches to preserve the structural and neighborhood information. The sequence is then \nappended with the [class] embedding and input to the transformer encoder. The transformer encoder is the same \nas that of Vaswani et al.13, which contains multi-headed self-attention layers and multiple multi-layer perceptron \nblocks. Layer normalization is used before each block, which assists in reducing training time and improving \ngeneralization performance. The transformer encoder outputs feature vectors corresponding to the input patches. \nFollowing the standard method, we used the first feature vector corresponding to the [class] embedding, which \nrepresents the entire sequence. Finally, a learnable linear layer processes this feature vector and outputs a binary \nvector, followed by softmax activation.\nCNN models\nViT was compared with two baseline CNN models, DenseNet and EfficientNet. DenseNet is a CNN composed \nof DenseBlocks. DenseBlocks allows convolutional networks to learn more deeply, accurately, and efficiently \nthan conventional convolutional layers do by connecting each layer to the others in a feed-forward fashion, \nachieving high performance while reducing memory and computation. We used DenseNet-121, one type of \nDenseNet, that has proven effective at medical image  classification19,20. EfficientNet is a model that has achieved \nstate-of-the-art capabilities on various benchmark datasets while significantly reducing computational costs \nfor image recognition, utilizing a composite scaling method to enlarge network depth, width, and  resolution21. \nEfficientNet has been used for the classification of many medical images and its good performance has been \n demonstrated22–24. There are eight types of base models from B0 to B7, and each model has a different expected \ninput shape. Considering an input image size of 256 × 256 pixels, EfficientNet-B0, -B1, and -B2 were selected.\nFigure 1.  Flow chart of patient enrollment. PET Positron emission tomography, CT Computed tomography.\n4\nVol:.(1234567890)Scientific Reports |         (2024) 14:8334  | https://doi.org/10.1038/s41598-024-58220-6\nwww.nature.com/scientificreports/\nTraining method and determining optimal hyperparameters\nImage-wise fine-tuning was performed on ViT, EfficientNet, and DenseNet models that were pretrained on \nImageNet-21k14. We removed the pretrained linear (classification) layers and attached a new learnable linear \nlayer. Fine-tuning was performed for all layers, including the pretrained layers. All training images were input \nin random order, and not separated by case. Grid search was performed to determine optimal hyperparameters \nfrom the set of candidates shown in Supplementary Table 2 online using the accuracy on the validation set. The \noptimal batch size was 8 and learning rate was  1e-3 across all models. Drop rates were determined according to \nthe original study of each model (ViT, 0.1; EfficientNet, 0.2; DenseNet, 0.2)12,19,21. We used the stochastic gradi-\nent descent with momentum (momentum = 0.9) and cross-entropy loss function. The models were respectively \nfine-tuned for sufficient numbers of epochs to converge the validation accuracies, and weights with the lowest \nvalidation losses were used for the test. The learning environment was an NVIDIA Titan RTX graphics process-\ning unit and CUDA version 10.1 (https:// devel oper. nvidia. com/ cuda- 10.1- downl oad- archi ve- base). Our systems \nwere written entirely in Python version 3.8.5. We used PyTorch version 1.8.1, scikit-learn version 0.24.2 (https:// \nscikit- learn. org), timm version 0.4.12 (https:// github. com/ rwigh tman/ pytor ch- image- models), and pytorch-\npretrained-vit version 0.0.7 (https:// github. com/ lukem elas/ PyTor ch- Pretr ained- ViT) to build our models. We \ncalculated the evaluation metrics using NumPy version 1.21.2 and scikit-learn version 0.24.2.\nComparison of performance between ViT and CNN models\nWe compared the classification performance between ViT, EfficientNet, and DenseNet on 4,852 test PET/CT \nimages. Each model outputs a probability of malignancy for each input image. We calculated the AUC as the \nperformance metric. We also performed qualitative evaluation by visualizing important regions that contributed \nto the prediction of each model. The Gradient-weighted Class Activation Mappings (Grad-CAMs) of all models \nwere compared. Grad-CAM is a visualization method that uses gradient  information25. We computed importance \nscores from the gradient information for each class (“positive”/ “negative”) flowing into the final transformer \nblock or convolution layer. We set the cutoff point using the Y ouden Index in the validation data and converted \neach probability into a binary prediction of “positive”/“negative. ” Important regions were highlighted according \nto the importance score of each pixel in the input image. After min–max normalization, the score matrix of each \nimage was converted into a heat map for visualization.\nEvaluation of the influence of input image modality on classification performance\nWhen functional information in PET is insufficient to make a diagnosis, anatomical information in CT helps \nreaders to better understand the lesion; therefore, bimodal analysis using PET and CT images is crucial for a \ndiagnosis by PET/CT. To assess the role of each modality in PET/CT diagnosis, we trained and evaluated two \nadditional ViT models, each of which only utilized either PET or CT inputs. We used the same fine-tuning con-\nfiguration described in the “Training Method” section except for the input modalities. The performance of the \nFigure 2.  Architecture of the ViT-based binary classifier. Each image is divided into 16 × 16 patches. ViT Vision \nTransformer.\n5\nVol.:(0123456789)Scientific Reports |         (2024) 14:8334  | https://doi.org/10.1038/s41598-024-58220-6\nwww.nature.com/scientificreports/\nthree fine-tuned ViT models was evaluated using the test set of each modality. Qualitative evaluation was also \nperformed by comparing the Grad-CAMs of each model.\nStatistical analysis\nFor statistical comparisons, the AUCs and the 95% confidence intervals (CIs) were computed and compared \nusing the DeLong test. Statistical significance was indicated by P values < 0.05. Bonferroni correction was used for \nmultiple comparisons. R version 4.1.2 and pROC package version 1.18 were used for statistical  computations26.\nResults\nPatient characteristics and distribution of annotated image data\nOverall, 143 patients (mean age, 65 ± 16 [SD] years; 54 males) had cancer, and 64 patients (mean age, 64 ± 17 years; \n39 males) did not have cancer. Pathological diagnoses of study patients are presented in Table  1. Full details of \npatient characteristics, including primary cancer type, stage at initial diagnosis, and previous therapy, are sum-\nmarized in Supplementary Table 3 online. Supplementary Table 4 online shows the distribution of the annotated \nimage data: 6,587 (36.0%) of 18,301 PET/CT images and 6,575 (35.9%) of 18,302 PET or CT images were labeled \n“positive. ” The average lesion size was 95.1 ± 66.9 mm for malignant lesions and 60.0 ± 17.9 mm for benign lesions \nin the test data. The average size of the focal malignant lesions (excluding diffuse lesions such as malignant \nascites) was 71.5 ± 33.8 mm.\nComparison of performance between ViT and CNN models\nFigure  3 represents the receiver operating characteristic (ROC) curves for ViT, EfficientNet, and DenseNet \nmodels. For EfficientNet, the B0 model produced the highest validation accuracy (0.872) in the EfficientNet \nmodels (B1, 0.864; B2, 0.866) and was used for further analyses. ViT achieved an AUC of 0.90 [95% CI 0.89, \n0.91], which was higher than that of EfficientNet (0.87 [95% CI 0.86, 0.88]; P < 0.001) and DenseNet (0.87 [95% \nCI 0.86, 0.88]; P  < 0.001). ViT and EfficientNet fine-tuned models outperformed each respective from-scratch \nmodel, with no significant differences found for DenseNet (Supplementary Table 5 online); therefore, we only \nshowed the results of the fine-tuned models. We computed slice-level maximum standardized uptake values \n(SUVmax) for stratified analyses (Table  2). When FDG uptake was unremarkable (SUVmax ≤ 7.0), ViT had \nhigher AUCs (SUVmax 3.5–7.0, 0.88 [95% CI 0.86, 0.90]; SUVmax < 3.5, 0.81 [95% CI 0.77, 0.85]) compared \nwith EfficientNet (SUVmax 3.5–7.0, 0.85 [95% CI 0.83, 0.87], P  < 0.001) and DenseNet (SUVmax 3.5–7.0, 0.82 \n[95% CI 0.80, 0.84], P  < 0.001; SUVmax < 3.5, 0.65 [95% CI 0.59, 0.70], P  < 0.001). Supplementary Fig. 2 online \nshows accuracy and loss curves of the ViT, DenseNet, and EfficientNet. The average training time per epoch for \nEfficientNet was the shortest among the models (EfficientNet, 102 s; DenseNet, 167 s; ViT, 269 s). All pretrained \nmodels tended to overfit as the training progressed.\nComparison of heatmaps between models\nFigure  4 shows the predictions and Grad-CAMs of ViT, EfficientNet, and DenseNet on sample test images \nfrom the “positive” class. In example (a), the predictions of all models were correct and ViT recognized bone \nmetastasis in the left and right ilium. However, CNN focused only on the left lesion. In sample (b), ViT focused \non lymphadenopathy with faint FDG uptake (SUVmax: 2.95), whereas CNNs failed to capture that region. An \nTable 1.  Pathological diagnoses of lesions in study patients for which histological examinations were \nperformed.\nPathological diagnosis Number\nMalignant (Cancer)\n Stomach 12\n Colon and rectum 22\n   Liver 4\n Bile ducts 4\n Gallbladder 2\n Ampulla of vater 1\n Pancreas 17\n Soft tissues 3\n Gastrointestinal stromal tumor 4\n Cervix uteri 6\n Uterus—Endometrium 7\n Ovarian, fallopian tube, and primary peritoneal carcinoma 18\n Lymphoma, and other hematological malignancy 2\n Pediatric tumors 1\n Benign\n Colorectal polfyp 6\n6\nVol:.(1234567890)Scientific Reports |         (2024) 14:8334  | https://doi.org/10.1038/s41598-024-58220-6\nwww.nature.com/scientificreports/\nexample of false positive prediction is shown in Fig. 4C. No model was able to predict “negative” for a lesion that \nwas suspected to be malignant in PET/CT diagnosis but histologically diagnosed as a colon polyp.\nEvaluation of the influence of input image modality on classification performance\nThe AUCs of the ViT-based models with different input types are shown in Table  3. PET/CT fusion input had \nhigher AUCs than CT input across all SUVmax levels. No evidence of difference was found between the PET/CT \nand PET input for the whole data analysis (the All row). However, when FDG uptake was low (SUVmax < 3.5), \nthe AUC of ViT using PET/CT (0.81 [95% CI 0.77, 0.85]) was higher than using PET data alone (0.61 [95% CI \n0.55, 0.67]; P < 0.001) by a large margin. An example of a “positive” class slice with low FDG uptake (SUVmax: \n2.90) is shown in Fig. 5, where the predictions and Grad-CAMs of ViT models for PET/CT, PET, and CT inputs \nare compared. In the example, ViT recognized lymphadenopathy and predicted “positive” using the PET/CT \nimage, but it failed to detect the lesion in the PET image.\nDiscussion\nIn this study, we developed a ViT-based system to automatically differentiate 18F-FDG uptake of PET/CT at the \nslice level. The ViT model achieved an AUC of 0.90, which was superior to the CNN models for the classification \nof PET/CT slices as benign or malignant. Even when the FDG uptake was low (SUVmax < 3.5), ViT produced an \nAUC of 0.81, which was higher than that of the CNNs. This demonstrated the usefulness of ViT for classifying \nFDG uptake from PET/CT images.\nPrevious studies demonstrated the efficacy of utilizing deep learning in the classification of 18F-FDG PET/\nCT images as benign or malignant. Sibille et al. achieved a AUC of 0.98 in the classification of lesions with \nhigh FDG uptake in lung cancer and lymphoma  patients11. Häggström et al. attained an AUC of 0.939 in the \nFigure 3.  Receiver operating characteristic curves of ViT, EfficientNet, and DenseNet models. Data in square \nbrackets are 95% confidence intervals. “*” represents P < 0.001. ViT Vision Transformer, AUC  Area under the \nreceiver operating characteristic curve.\nTable 2.  Stratified analysis by the SUVmax of AUCs of ViT, EfficientNet, and DenseNet models on the test set. \nData in parentheses are numerators/denominators for percentages. Data in square brackets are 95% confidence \nintervals. AUC  Area under the receiver operating characteristic curve, SUVmax Maximum standardized uptake \nvalue, ViT Vision Transformer.\nViT EfficientNet DenseNet\nPercentage of positive \nimages AUC AUC P value (vs. ViT) AUC P value (vs. ViT)\nAll 35.5% (1724/4852) 0.90 [0.89, 0.91] 0.87 [0.86, 0.88] P < 0.001 0.87 [0.86, 0.88] P < 0.001\nSUVmax > 7.0 50.8% (989/1946) 0.89 [0.87, 0.90] 0.88 [0.86, 0.89] P = 0.100 0.91 [0.90, 0.92] P = 0.004\nSUVmax 3.5–7.0 40.9% (610/1490) 0.88 [0.86, 0.90] 0.85 [0.83, 0.87] P < 0.001 0.82 [0.80, 0.84] P < 0.001\nSUVmax < 3.5 8.8% (125/1416) 0.81 [0.77, 0.85] 0.77 [0.72, 0.81] P = 0.094 0.65 [0.59, 0.70] P < 0.001\n7\nVol.:(0123456789)Scientific Reports |         (2024) 14:8334  | https://doi.org/10.1038/s41598-024-58220-6\nwww.nature.com/scientificreports/\nclassification of 3D PET/CT images of lymphoma  patients27. However, these studies focused on specific diseases, \nwhich limits their applicability to the variety of lesions encountered in clinical practice. Eyuboglu et al. developed \na deep learning model for cross-disease abnormality detection at the organ level using weak supervision. Their \nmodel achieved a mean AUC exceeding 0.85 in 10 regions, including lungs, liver, and thoracic lymph  nodes28. \nTheir study was primarily aimed at detecting abnormal metabolic activity and no classification of the identified \nlesions as benign or malignant was performed. In our study, patients with various diseases were included and ViT \nachieved an AUC of 0.90 in differentiating PET/CT images (with histopathological evidence for some lesions). \nThis is more valuable in clinical practice compared to previous studies.\nWe demonstrated that the ViT model had significantly higher performance than the CNN models for the \nclassification of PET/CT images. Previous research suggested that a ViT model with architecture similar to ViT-\nB16 had no evident performance advantage over DenseNet-121 for the classification of radiological  images29,30. \nThey performed a diagnosis of disease on chest radiographs (eg, atelectasis, cardiomegaly, and effusion) and \nFigure 4.  Predictions and Grad-CAMs of ViT, EfficientNet, and DenseNet models on sample test images. The \nyellow bounding boxes indicate malignant lesions and the green bounding boxes present benign lesions. The \ntop rows of the Grad-CAMs show important areas for a “positive” prediction, and the bottom rows show areas \nfor a “negative” prediction. SUVmax Maximum standardized uptake value, ViT Vision Transformer, Grad-CAM \nGradient-weighted class activation mapping.\n8\nVol:.(1234567890)Scientific Reports |         (2024) 14:8334  | https://doi.org/10.1038/s41598-024-58220-6\nwww.nature.com/scientificreports/\nextremity radiographs (eg, bone fracture and amputation). Differentiating PET/CT images as benign or malignant \nis a more complex task than using X-ray images because it requires the integration of functional information \nfrom PET and anatomical information from CT (such as the distribution of lesions and their position relative to \norgans) from the entire image. ViT was superior to existing models in differentiating PET/CT images as benign \nor malignant because ViT can integrate information across the entire image better than CNN  can12. Figure 4A \nshowed that ViT tended to identify lesions more accurately than CNNs, which was consistent with the previous \n report30. In some cases, however, ViT made predictions that diverged from pathological diagnoses (Fig.  4C). \nIncreasing the number of training data linked to histopathological evidence will reduce such false positives and \nfalse negatives, avoiding unnecessary additional tests and improving patient prognosis. We showed that ViT \nhad a high performance when the FDG uptake was unremarkable in PET/CT images as shown in Table  2 and \nFig. 4B, whereas its performance was reduced when using PET information only. This indicates ViT can leverage \nanatomical information from CT images to disambiguate subtle FDG uptake in PET images.\nThe effects of hidden stratification can be problematic in machine learning for medical  imaging31. Previous \nresearch using CNN to identify pneumothorax in chest radiographs reported it was affected by hidden strati-\nfication where the presence of pneumothorax correlated with the presence of chest tubes that were placed for \nits treatment. CNN trained to identify pneumothorax in X-ray images had a higher AUC on images with chest \ntubes than on images without chest tubes, and Grad-CAM indicated that the CNN focused on chest tubes. These \nprevious studies highlighted the potential limitation of machine learning algorithms where classifiers can be \nfixated on salient features (chest tubes), and overlook clinically significant features (eg, collapsed lungs) 32,33. A \nshortcut for the classification of FDG uptake in PET/CT images as benign or malignant is to classify strong FDG \nuptake as malignant. Thus, there is a concern that models will be trained to focus on regions with high FDG \nuptake and to undervalue lesions with poor FDG uptake. Our stratified analysis showed that the ViT model \nachieved higher AUCs when the FDG uptake was unremarkable and recognized lesions with low FDG uptake \ncompared with the CNN models. This suggests that ViT is less susceptible to hidden stratification than CNN, \nFigure 4.  (continued)\nTable 3.  Stratified analysis by the SUVmax of AUCs of ViT models on the PET/CT, PET, and CT test set. ViT \nwas fine-tuned using training data of each modality. Data in parentheses are numerators/denominators for \npercentages. Data in square brackets are 95% confidence intervals. AUC  Area under the receiver operating \ncharacteristic curve, PET Positron emission tomography, CT Computed tomography, SUVmax Maximum \nstandardized uptake value, ViT Vision Transformer.\nPET/CT PET CT\nPercentage of positive \nimages AUC AUC P value (vs. PET/CT) AUC \nP value (vs. PET/\nCT)\nAll 35.5% (1724/4852) 0.90 [0.89, 0.91] 0.88 [0.87, 0.90] P = 0.029 0.70 [0.69, 0.72] P < 0.001\nSUVmax > 7.0 50.8% (989/1946) 0.89 [0.87, 0.90] 0.93 [0.92, 0.94] P < 0.001 0.71 [0.69, 0.73] P < 0.001\nSUVmax 3.5–7.0 40.9% (610/1490) 0.88 [0.86, 0.90] 0.86 [0.84, 0.88] P = 0.033 0.74 [0.70, 0.76] P < 0.001\nSUVmax < 3.5 8.8% (125/1416) 0.81 [0.77, 0.85] 0.61 [0.55, 0.67] P < 0.001 0.67 [0.61, 0.73] P < 0.001\n9\nVol.:(0123456789)Scientific Reports |         (2024) 14:8334  | https://doi.org/10.1038/s41598-024-58220-6\nwww.nature.com/scientificreports/\nin accordance with previous  studies30. Our results indicated that ViT might be used to address the problem of \npotential confounding features in medical imaging datasets used for machine learning.\nOur study had limitations. First, we used data from a single institution. Our system may have overfitted data to \nthe epidemiology specific to that hospital. Second, we used a single type of color scale for PET/CT fusion images. \nThe color scale of fusion images may vary by facility. Extending this study to other institutions with different \ncolor scales for PET/CT fusion images is an important future task. Third, not all lesions in our reference standard \ndata have histopathological evidence. There may be a discrepancy between imaging diagnosis and pathological \ndiagnosis. Finally, the set of candidates of hyperparameters was limited (see Supplementary Table 2 and Sup-\nplementary Fig. 2 for details) and we seek to investigate better optimal parameters for our models in the future.\nIn conclusion, we demonstrated that the ViT model performed better than the CNN models for the classifi-\ncation of PET/CT slices as benign or malignant. The ViT model retained a relatively high AUC for input slices \nwith a low SUVmax, which demonstrated the clinical value of ViT related to its sensitivity to easy-to-miss cases. \nWe expect that the ViT model will help users to differentiate between benign and malignant slices in PET/CT \nimages and prevent overlooking lesions with insignificant FDG uptake.\nData availability\nAll clinical information and PET/CT image data are limitedly available through formal approval procedures upon \nrequests to validated investigators. Further requests and inquiries are available to corresponding author (S.K.).\nReceived: 15 March 2023; Accepted: 26 March 2024\nReferences\n 1. Gambhir, S. S. Molecular imaging of cancer with positron emission tomography. Nat. Rev. Cancer 2, 683–693 (2002).\n 2. Kostakoglu, L., Agress, H. & Goldsmith, S. J. Clinical role of FDG PET in evaluation of cancer patients. Radiographics 23, 315–340 \n(2003).\n 3. Shreve, P . & Faasse, T. Role of positron emission tomography-computed tomography in pulmonary neoplasms. Radiol. Clin. North \nAm. 51, 767–779 (2013).\n 4. Kanoun, S., Rossi, C. & Casasnovas, O. [18F]FDG-PET/CT in hodgkin lymphoma: Current usefulness and perspectives. Cancers \n10, 145 (2018).\n 5. Baffour, F . I., Wenger, D. E. & Broski, S. M. 18F-FDG PET/CT imaging features of lipomatous tumors. Am. J. Nucl. Med. Mol. \nImaging 10, 74 (2020).\n 6. Blodgett, T. M. et al. Combined PET-CT in the head and neck: Part 1. Physiologic, altered physiologic, and artifactual FDG uptake. \nRadiographics 25, 897–912 (2005).\n 7. Y eung, H. W . D., Grewal, R. K., Gonen, M., Schöder, H. & Larson, S. M. Patterns of 18F-FDG uptake in adipose tissue and muscle: \nA potential source of false-positives for PET. J. Nucl. Med. 44, 1789 (2003).\n 8. Griffeth, L. K. Use of PET/CT scanning in cancer patients: Technical and practical considerations. Proc. Bayl. Univ. Med. Cent. 18, \n321 (2005).\n 9. Nishiyama, Y . et al. Nuclear medicine practice in Japan: A report of the eighth nationwide survey in 2017. Ann Nucl Med  33, \n725–732 (2019).\nFigure 5.  Predictions and Grad-CAMs of ViT-based models on sample PET/CT, PET, and CT test images \nfrom the “positive” class. ViT was fine-tuned using training data of each modality. The bounding boxes depicted \nin the figure indicate malignant lesions. The top row of the Grad-CAMs shows important areas for “positive” \npredictions, and the bottom row shows areas for “negative” predictions. PET Positron emission tomography, CT \nComputed tomography, SUVmax Maximum standardized uptake value, ViT Vision Transformer, Grad-CAM \nGradient-weighted class activation mapping.\n10\nVol:.(1234567890)Scientific Reports |         (2024) 14:8334  | https://doi.org/10.1038/s41598-024-58220-6\nwww.nature.com/scientificreports/\n 10. Baker, N., Lu, H., Erlikhman, G. & Kellman, P . J. Deep convolutional networks do not classify based on global object shape. PLoS \nComput. Biol. 14, e1006613 (2018).\n 11. Sibille, L. et al. 18F-FDG PET/CT uptake classification in lymphoma and lung cancer by using deep convolutional neural networks. \nRadiology 294, 445–452 (2020).\n 12. Dosovitskiy, A. et al. An image is Worth 16x16 words: Transformers for Image recognition at scale. In Proc. ICLR 1–22 (2021).\n 13. Vaswani, A. et al. Attention is all you need. Adv. Neural Inf. Process Syst. 30, 1–11 (2017).\n 14. Deng, J. et al. ImageNet: A large-scale hierarchical image database. In Proceedings of the 2009 IEEE Conference on Computer Vision \nand Pattern Recognition 248–255 (2009).\n 15. Shome, D. et al. Covid-transformer: Interpretable covid-19 detection using vision transformer for healthcare. Int. J. Environ. Res. \nPublic Health 18, 11086 (2021).\n 16. Mondal, A. K., Bhattacharjee, A., Singla, P . & Prathosh, A. P . XViTCOS: Explainable vision transformer based COVID-19 screening \nusing radiography. IEEE J. Transl. Eng. Health Med. 10, 1100110 (2022).\n 17. Ratib, O. PET/CT image navigation and communication. J. Nucl. Med. 45, 46S-55S (2004).\n 18. Li, T., Wang, Y ., Chang, C., Hu, N. & Zheng, Y . Color-appearance-model based fusion of gray and pseudo-color images for medical \napplications. Inform. Fus 19, 103–114 (2014).\n 19. Huang, G., Liu, Z., van der Maaten, L. & Weinberger, K. Q. Densely Connected Convolutional Networks. In Proceedings of the \nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) 4700–4708 (2017).\n 20. Hasan, N., Bao, Y ., Shawon, A. & Huang, Y . DenseNet convolutional neural networks application for predicting COVID-19 using \nCT image. SN Comput. Sci. 2, 389 (2021).\n 21. Tan, M. & Le, Q. V . EfficientNet: Rethinking model scaling for convolutional neural networks. In Proceedings of the 36th Interna-\ntional Conference on Machine Learning (ICML) 6105–6114 (2019).\n 22. Chetoui, M. & Akhloufi, M. A. Explainable diabetic retinopathy using EfficientNET. Annu. Int. Conf. IEEE Eng. Med. Biol. Soc.  \n2020, 1966–1969 (2020).\n 23. Marques, G., Agarwal, D. & de la Torre Díez, I. Automated medical diagnosis of COVID-19 through EfficientNet convolutional \nneural network. Appl. Soft Comput. 96, 106691 (2020).\n 24. Y ap, M. H. et al. Analysis towards classification of infection and ischaemia of diabetic foot ulcers. In 2021 IEEE EMBS International \nConference on Biomedical and Health Informatics, Proceedings (2021).\n 25. Selvaraju, R. R. et al. Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization. In Proceedings of the \nIEEE International Conference on Computer Vision, 618–626 (2017).\n 26. Robin, X. et al. pROC: An open-source package for R and S+ to analyze and compare ROC curves. BMC Bioinformatics  12, 77 \n(2011).\n 27. Häggström, I. et al. Deep learning for [18F]fluorodeoxyglucose-PET-CT classification in patients with lymphoma: a dual-centre \nretrospective analysis. Lancet Digit Health S2589–7500(23), 00203 (2023).\n 28. Eyuboglu, S. et al. Multi-task weak supervision enables anatomically-resolved abnormality detection in whole-body FDG-PET/\nCT. Nat. Commun. 12, 1880 (2021).\n 29. Touvron, H. et al. Training data-efficient image transformers & distillation through attention. In Proceedings of the 38th Interna-\ntional Conference on Machine Learning (PMLR) 139, 10347–10357 (2021).\n 30. Murphy, Z. R., Venkatesh, K., Sulam, J. & Yi, P . H. Visual transformers and convolutional neural networks for disease classification \non radiographs: A comparison of performance, sample efficiency, and hidden stratification. Radiol. Artif. Intell. 4, e220012 (2022).\n 31. Oakden-Rayner, L., Dunnmon, J., Carneiro, G. & Re, C. Hidden stratification causes clinically meaningful failures in machine \nlearning for medical imaging. In ACM CHIL 2020 - Proceedings of the 2020 ACM Conference on Health, Inference, and Learning  \n151–159 (2020).\n 32. Rajpurkar, P . et al. Deep learning for chest radiograph diagnosis: A retrospective comparison of the CheXNeXt algorithm to \npracticing radiologists. PLoS Med 15, e1002686 (2018).\n 33. Yi, P . H. et al. Can AI outperform a junior resident? Comparison of deep neural network to first-year radiology residents for \nidentification of pneumothorax. Emerg Radiol 27, 367–375 (2020).\nAcknowledgements\nThis work was supported by JSPS KAKENHI Grant Number 21H03840.\nAuthor contributions\nD.N. initiated the study; made the conception and design of the work; acquired the data; built the models; \nperformed interpretation and statistical analyses of the data; wrote the draft of the manuscript. T.W . and D.K. \nperformed the annotation. Y .S. and S.K. critically revised the manuscript for important intellectual content. All \nauthors have agreed to be accountable for all aspects of the work.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nSupplementary Information The online version contains supplementary material available at https:// doi. org/ \n10. 1038/ s41598- 024- 58220-6.\nCorrespondence and requests for materials should be addressed to S.K.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\n11\nVol.:(0123456789)Scientific Reports |         (2024) 14:8334  | https://doi.org/10.1038/s41598-024-58220-6\nwww.nature.com/scientificreports/\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2024"
}