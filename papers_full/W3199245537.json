{
  "title": "Pix2seq: A Language Modeling Framework for Object Detection",
  "url": "https://openalex.org/W3199245537",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2065736904",
      "name": "Chen Ting",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2669462718",
      "name": "Saxena, Saurabh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2601774862",
      "name": "Li Lala",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2745401947",
      "name": "Fleet, David J.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2551387618",
      "name": "Hinton, Geoffrey",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3106250896",
    "https://openalex.org/W3176659256",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3100859887",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3034978746",
    "https://openalex.org/W2068730032",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2102605133",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W2963659353",
    "https://openalex.org/W2996035354",
    "https://openalex.org/W2935837427",
    "https://openalex.org/W2962676885",
    "https://openalex.org/W2964121718",
    "https://openalex.org/W2555182955",
    "https://openalex.org/W607748843",
    "https://openalex.org/W2991391304",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W1895577753",
    "https://openalex.org/W3129576130",
    "https://openalex.org/W3035452548",
    "https://openalex.org/W1905882502",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W2964241181",
    "https://openalex.org/W2130306094",
    "https://openalex.org/W3169291081",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2331143823",
    "https://openalex.org/W2982770724",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W2177466532",
    "https://openalex.org/W1536680647",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3121480429",
    "https://openalex.org/W2949194345",
    "https://openalex.org/W2963069010",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2147800946",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W3037156242",
    "https://openalex.org/W2963037989",
    "https://openalex.org/W1904365287",
    "https://openalex.org/W2772283977",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2501369945",
    "https://openalex.org/W3035172746",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2152790380",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2031489346"
  ],
  "abstract": "We present Pix2Seq, a simple and generic framework for object detection. Unlike existing approaches that explicitly integrate prior knowledge about the task, we cast object detection as a language modeling task conditioned on the observed pixel inputs. Object descriptions (e.g., bounding boxes and class labels) are expressed as sequences of discrete tokens, and we train a neural network to perceive the image and generate the desired sequence. Our approach is based mainly on the intuition that if a neural network knows about where and what the objects are, we just need to teach it how to read them out. Beyond the use of task-specific data augmentations, our approach makes minimal assumptions about the task, yet it achieves competitive results on the challenging COCO dataset, compared to highly specialized and well optimized detection algorithms.",
  "full_text": "Published as a conference paper at ICLR 2022\nPIX2SEQ : A L ANGUAGE MODELING FRAMEWORK\nFOR OBJECT DETECTION\nTing Chen, Saurabh Saxena, Lala Li, David J. Fleet, Geoffrey Hinton\nGoogle Research, Brain Team\nABSTRACT\nWe present Pix2Seq, a simple and generic framework for object detection. Unlike\nexisting approaches that explicitly integrate prior knowledge about the task, we\ncast object detection as a language modeling task conditioned on the observed pixel\ninputs. Object descriptions (e.g., bounding boxes and class labels) are expressed\nas sequences of discrete tokens, and we train a neural network to perceive the\nimage and generate the desired sequence. Our approach is based mainly on the\nintuition that if a neural network knows about where and what the objects are, we\njust need to teach it how to read them out. Beyond the use of task-speciﬁc data\naugmentations, our approach makes minimal assumptions about the task, yet it\nachieves competitive results on the challenging COCO dataset, compared to highly\nspecialized and well optimized detection algorithms.1\nPix2Seq\ny\nmin\n=9 \nx\nmin\n=7 \ny\nmax\n=67 \nx\nmax\n=98 \ntrain \n......\ny\nmin\n=8 \nx\nmin\n=4 \ny\nmax\n=99 \nx\nmax\n=97 \nmotocycle \n......\ny\nmin\n=1 \nx\nmin\n=57 \ny\nmax\n=99 \nx\nmax\n=72 \nPerson \n......\nCmd: \ndetect \nobjects\nFigure 1: Illustration of Pix2Seq framework for object detection. The neural net perceives an image\nand generates a sequence of tokens that correspond to bounding boxes and class labels.\n1 I NTRODUCTION\nVisual object detection systems aim to recognize and localize all objects of pre-deﬁned categories in\nan image. The detected objects are typically described by a set of bounding boxes and associated\nclass labels. Given the difﬁculty of the task, most existing methods, such as (Girshick, 2015; Ren\net al., 2015; He et al., 2017; Lin et al., 2017b; Carion et al., 2020), are carefully designed and highly\ncustomized, with a signiﬁcant amount of prior knowledge in the choice of architecture and loss\nfunction. For example, many architectures are tailored to the use of bounding boxes (e.g., with region\nproposals (Girshick, 2015; Ren et al., 2015) and RoI pooling (Girshick et al., 2014; He et al., 2017)).\nOthers are tied to the use of object queries for object binding (Carion et al., 2020). Loss functions are\noften similarly tailored to the use of bounding boxes, such as box regression (Szegedy et al., 2013;\nLin et al., 2017b), set-based matching (Erhan et al., 2014; Carion et al., 2020), or by incorporating\nCorrespondence to: iamtingchen@google.com\n1Code and checkpoints available at https://github.com/google-research/pix2seq.\n1\narXiv:2109.10852v2  [cs.CV]  27 Mar 2022\nPublished as a conference paper at ICLR 2022\nspeciﬁc performance metrics, like intersection-over-union on bounding boxes (Rezatoﬁghi et al.,\n2019). Although existing systems ﬁnd applications in myriad domains, from self-driving cars (Sun\net al., 2020), to medical image analysis (Jaeger et al., 2020), to agriculture (Sa et al., 2016), the\nspecialization and complexity make them difﬁcult to integrate into a larger system, or generalize to a\nmuch broader array of tasks associated with general intelligence.\nThis paper advocates a new approach, based on the intuition that if a neural net knows about where\nand what the objects are, we just need to teach it to read them out. And by learning to “describe”\nobjects the model can learn to ground the “language” on pixel observations, leading to useful object\nrepresentations. This is realized with our Pix2Seq framework (see Figure 1). Given an image, our\nmodel produces a sequence of discrete tokens that correspond to object descriptions (e.g., object\nbounding boxes and class labels), reminiscent of an image captioning system (Vinyals et al., 2015b;\nKarpathy & Fei-Fei, 2015; Xu et al., 2015). In essence, we cast object detection as a language\nmodeling task conditioned on pixel inputs, for which the model architecture and loss function are\ngeneric and relatively simple, without being engineered speciﬁcally for the detection task. As such,\none can readily extend the framework to different domains or applications, or incorporate it into a\nperceptual system supporting general intelligence, for which it provides a language interface to a\nwide range of vision tasks.\nTo tackle the detection task with Pix2Seq, we ﬁrst propose a quantization and serialization scheme\nthat converts bounding boxes and class labels into sequences of discrete tokens. We then leverage\nan encoder-decoder architecture for perceiving pixel inputs and generating the target sequence. The\nobjective function is simply the maximum likelihood of tokens conditioned on pixel inputs and the\npreceding tokens. While both the architecture and loss function are task-agnostic (without assuming\nprior knowledge about object detection, e.g., bounding boxes), we can still incorporate task-speciﬁc\nprior knowledge with a sequence augmentation technique, proposed below, that alters both input\nand target sequences during training. Through extensive experimentation, we demonstrate that\nthis simple Pix2Seq framework can achieve competitive results on the COCO dataset compared to\nhighly customized, well established approaches, including Faster R-CNN (Ren et al., 2015) and\nDETR (Carion et al., 2020). By pretraining our model on a larger object detection dataset, its\nperformance can be further improved.\n2 T HE PIX2SEQ FRAMEWORK\nIn the proposed Pix2Seq framework we cast object detection as a language modeling task, conditioned\non pixel inputs (Figure 1). The system consists of four main components (Figure 2):\n• Image Augmentation: As is common in training computer vision models, we use image augmenta-\ntions to enrich a ﬁxed set of training examples (e.g., with random scaling and crops).\n• Sequence construction & augmentation: As object annotations for an image are usually represented\nas a set of bounding boxes and class labels, we convert them into a sequence of discrete tokens.\n• Architecture: We use an encoder-decoder model, where the encoder perceives pixel inputs, and\nthe decoder generates the target sequence (one token at a time).\n• Objective/loss function: The model is trained to maximize the log likelihood of tokens conditioned\non the image and the preceding tokens (with a softmax cross-entropy loss).\nImage\nX\nImage\n \nAugmentation\nBbox, \nclass\nY\ntarget\nSequence\nConstruction \n& \nAugmentation\nY\npred\nObjective\n(next \ntoken \nprediction)\nText\nEncoder\n y 1\ny 2\ny 3\nDecoder\nwith \ncausal \nmask\nFigure 2: Major components of the Pix2Seq learning framework.\n2\nPublished as a conference paper at ICLR 2022\n2.1 S EQUENCE CONSTRUCTION FROM OBJECT DESCRIPTIONS\nIn common object detection datasets, such as Pascal VOC (Everingham et al., 2010), COCO (Lin\net al., 2014), and OpenImages (Kuznetsova et al., 2020), images have variable numbers of objects,\nrepresented as sets of bounding boxes and class labels. In Pix2Seq we express them as sequences of\ndiscrete tokens.\nWhile class labels are naturally expressed as discrete tokens, bounding boxes are not. A bounding box\nis determined by two of its corner points (i.e., top-left and bottom-right), or by its center point plus\nheight and width. We propose to discretize the continuous numbers used to specify thex, ycoordinates\nof corner points (similarly for height and width if the other box format is used). Speciﬁcally, an\nobject is represented as a sequence of ﬁve discrete tokens, i.e. [ymin, xmin, ymax, xmax, c], where each\nof the continuous corner coordinates is uniformly discretized into an integer between [1, nbins], and\nc is the class index. We use a shared vocabulary for all tokens, so the vocabulary size is equal to\nnumber of bins + number of classes. This quantization scheme for the bounding boxes allows us to\nuse a small vocabulary while achieving high precision. For example, a 600×600 image requires only\n600 bins to achieve zero quantization error. This is much smaller than modern language models with\nvocabulary sizes of 32K or higher (Radford et al., 2018; Devlin et al., 2018). The effect of different\nlevels of quantization on the placement of bounding boxes is illustrated in Figure 3.\nWith each object description expressed as a short discrete sequence, we next need to serialize multiple\nobject descriptions to form a single sequence for a given image. Since order of objects does not matter\nfor the detection task per se, we use a random ordering strategy (randomizing the order objects each\ntime an image is shown). We also explore other deterministic ordering strategies, but we hypothesize\nthat random ordering will work just as well as any deterministic ordering, given a capable neural net\nand autoregressive modeling (where the net can learn to model the distribution of remaining objects\nconditioned on those observed).\nFinally, because different images often have different numbers of objects, the generated sequences\nwill have different lengths. To indicate the end of a sequence, we therefore incorporate an EOS token.\nThe sequence construction process with different ordering strategies is illustrated in Figure 4.\n0 100 200 300 400 500 600\n0\n100\n200\n300\n400\n(a) nbins = 10\n0 100 200 300 400 500 600\nTruth\n0\n100\n200\n300\n400 (b) nbins = 50\n0 100 200 300 400 500 600\nTruth\n0\n100\n200\n300\n400 (c) nbins = 100\n0 100 200 300 400 500 600\nTruth\n0\n100\n200\n300\n400 (d) nbins = 500\n0 100 200 300 400 500 600\nTruth\n0\n100\n200\n300\n400 (e) Original\nFigure 3: Applying the proposed discritization of bounding box on an image of 480 ×640. Only a\nquarter of the image is shown for better clarity. With a small number of bins, such as 500 bins (∼1\npixel/bin), it achieves high precision even for small objects.\n327  \n370  \n653  \n444 \n1001\n \n    \n544  \n135  \n987  \n338 \n1004\n     \n508  \n518  \n805  \n892  \n1004\n     \n0\nRandom \nordering \n(multiple \nsamples):\n544  \n135  \n987  \n338 \n1004\n \n    \n327  \n370  \n653  \n444 \n1001\n     \n508  \n518  \n805  \n892  \n1004\n     \n0\n508  \n518  \n805  \n892 \n1004\n     \n544  \n135  \n987  \n338 \n1004\n \n    \n327  \n370  \n653  \n444  \n1001\n     \n0\nArea \nordering:\nDist2ori \nordering:\n544  \n135  \n987  \n338 \n1004\n     \n327  \n370  \n653  \n444 \n1001\n     \n508  \n518  \n805  \n892  \n1004\n     \n0\n544  \n135  \n987  \n338 \n1004\n     \n508  \n518  \n805  \n892 \n1004\n     \n327  \n370  \n653  \n444  \n1001\n     \n0\nFigure 4: Examples of sequence construction with nbins = 1000, and 0 is EOS token.\n3\nPublished as a conference paper at ICLR 2022\n2.2 A RCHITECTURE , OBJECTIVE AND INFERENCE\nTreating the sequences that we construct from object descriptions as a “dialect”, we turn to generic\narchitectures and objective functions that have been effective in language modeling.\nArchitecture We use an encoder-decoder architecture. The encoder can be a general image encoder\nthat perceives pixels and encodes them into hidden representations, such as a ConvNet (LeCun et al.,\n1989; Krizhevsky et al., 2012; He et al., 2016), Transformer (Vaswani et al., 2017; Dosovitskiy et al.,\n2020), or their combination (Carion et al., 2020). For generation we use a Transformer decoder,\nwidely used in modern language modeling (Radford et al., 2018; Raffel et al., 2019). It generates\none token at a time, conditioned on the preceding tokens and the encoded image representation. This\nremoves the complexity and customization in architectures of modern object detectors, e.g., bounding\nbox proposal and regression, since tokens are generated from a single vocabulary with a softmax.\nObjective Similar to language modeling, Pix2Seq is trained to predict tokens, given an image and\npreceding tokens, with a maximum likelihood loss, i.e.,\nmaximize\nL∑\nj=1\nwj log P(˜yj|x, y1:j−1) , (1)\nwhere x is a given image, y and ˜y are input and target sequences associated with x, and L is the\ntarget sequence length. y and ˜y are identical in the standard language modeling setup, but they can\nalso be different (as in our later augmented sequence construction). Also, wj is a pre-assigned weight\nfor j-th token in the sequence. We set wj = 1, ∀j, however it would be possible to weight tokens by\ntheir types (e.g., coordinate vs class tokens), or by the size of the corresponding object.\nInference At inference time, we sample tokens from model likelihood, i.e., P(yj|x, y1:j−1). This\ncan be done by either taking the token with the largest likelihood (arg maxsampling), or using other\nstochastic sampling techniques. We ﬁnd that using nucleus sampling (Holtzman et al., 2019) leads\nto higher recall than arg maxsampling (Appendix C). The sequence ends when the EOS token is\ngenerated. Once the sequence is generated, it is straight-forward to extract and de-quantize the object\ndescriptions (i.e., obtaining the predicted bounding boxes and class labels).\n2.3 S EQUENCE AUGMENTATION TO INTEGRATE TASK PRIORS\nThe EOS token allows the model to decide when to terminate generation, but in practice we ﬁnd that\nthe model tends to ﬁnish without predicting all objects. This is likely due to 1) annotation noise (e.g.,\nwhere annotators did not identify all the objects), and 2) uncertainty in recognizing or localizing\nsome objects. While this only affects the overall performance by a small percentage (e.g., 1-2% in\naverage precision), it has a larger effect on recall. To encourage higher recall rates, one trick is to\ndelay the sampling of the EOS token by artiﬁcially decreasing its likelihood. However, this often\nleads to noisy and duplicated predictions. In part, this difﬁcult trade-off between precision and recall\nis a consequence of our model being task agnostic, unaware of the detection task per se.\nTo mitigate the problem we simply introduce a sequence augmentation technique, thereby incorporat-\ning prior knowledge about the task. The target sequence ˜y in conventional autoregressive language\nmodeling (i.e., with no sequence augmentation) is the same as the input sequence y. And all tokens\nin a sequence are real (e.g., converted from human annotations). With sequence augmentation, we\ninstead augment input sequences during training to include both real and synthetic noise tokens. We\nalso modify target sequences so that the model can learn to identify the noise tokens rather than\nmimic them. This improves the robustness of the model against noisy and duplicated predictions\n(particularly when the EOS token is delayed to increase recall). The modiﬁcations introduced by\nsequence augmentation are illustrated in Figure 5, and detailed below.\nAltered sequence construction We ﬁrst create synthetic noise objects to augment input sequences\nin the following two ways: 1) adding noise to existing ground-truth objects (e.g., random scaling or\nshifting their bounding boxes), and 2) generating completely random boxes (with randomly associated\nclass labels). It is worth noting that some of these noise objects may be identical to, or overlapping\nwith, some of the ground-truth objects, simulating noisy and duplicated predictions, as demonstrated\n4\nPublished as a conference paper at ICLR 2022\ny\n1\ny\n2\ny\n3\ny\n4\ny\n5\ny\n6\ny\n7\ny\n8\ny\n9\nstart\ny\n1\ny\n2\ny\n3\ny\n4\ny\n5\ny\n6\ny\n7\ny\n8\ny\n9\nend\nInput \nsequence\n(real \nonly)\nTarget sequence\ny\n10\ny\n10\ny\n1\ny\n2\ny\n3\ny\n4\ny\n5\ny\n6\ny\n7\ny\n8\ny\n9\nstart\ny\n11\ny\n12\ny\n13\ny\n14\ny\n1\ny\n2\ny\n3\ny\n4\ny\n5\ny\n6\ny\n7\ny\n8\ny\n9\nn/a\nn/a\nn/a\nn/a\nnoise\nInput \nsequence\n(real \n+ \nnoise)\nTarget \nsequence\nn/a\nn/a\nn/a\nn/a\nnoise\ny\n15\ny\n16\ny\n17\ny\n18\ny\n19\ny\n10\ny\n10\n.....\n...\n(a) \nConventional \nautoregressive \nlangage \nmodeling\n(b) \nLangage \nmodeling \nwith \nsequence \naugmentation \n(e.g. \nadding \nnoise \ntokens)\nFigure 5: Illustration of language modeling with / without sequence augmentation. With sequence\naugmentation, input tokens are constructed to include both real objects (blue) and synthetic noise\nobjects (orange). For the noise objects, the model is trained to identify them as the “noise” class, and\nwe set the loss weight of “n/a” tokens (corresponding to coordinates of noise objects) to zero since\nwe do not want the model to mimic them.\nFigure 6: Illustrations of randomly sampled noise objects (in white), vs. ground-truth objects (in\nred).\nin Figure 6. After noise objects are synthesised and discretized, we then append them in the end of\nthe original input sequence. As for the target sequence, we set the target tokens of noise objects to\n“noise” class (not belonging to any of the ground-truth class labels), and the coordinate tokens of\nnoise objects to “n/a”, whose loss weights are set to zero, i.e., setting wj = 1 [˜yj̸=“n/a”] in Eq 1.\nAltered inference With sequence augmentation, we are able to substantially delay the EOS token,\nimproving recall without increasing the frequency of noisy and duplicated predictions. Thus, we let\nthe model predict to a maximum length, yielding a ﬁxed-sized list of objects. When we extract the\nlist of bounding boxes and class labels from the generated sequences, we replace the “noise” class\nlabel with a real class label that has the highest likelihood among all real class labels. We use the\nlikelihood of the selected class token as a (ranking) score for the object.\n3 E XPERIMENTS\n3.1 E XPERIMENTAL SETUP\nWe evaluate the proposed method on the MS-COCO 2017 detection dataset (Lin et al., 2014),\ncontaining 118k training images and 5k validation images. To compare with DETR and Faster\nR-CNN, we report average precision (AP), an integral metric over multiple thresholds, on validation\nset at the last training epoch. We employ two training strategies: 1) training from scratch on COCO\nin order to compare fairly with the baselines, and also 2) pretraining+ﬁnetuning, i.e., pretrain the\nPix2Seq model on a larger object detection dataset, namely Objects365 (Shao et al., 2019), and then\nﬁnetune the model on COCO. Since our approach incorporates zero inductive bias / prior knowledge\nof the object detection task, we expect the second training strategy to be superior.\n5\nPublished as a conference paper at ICLR 2022\nTable 1: Comparison of average precision, over multiple thresholds and object sizes, on COCO\nvalidation set. Each section compares different methods of the similar ResNet “backbone”. Our\nmodels achieve competitive results to both Faster R-CNN and DETR baselines.\nMethod Backbone #params AP AP 50 AP75 APS APM APL\nFaster R-CNN R50-FPN 42M 40.2 61.0 43.8 24.2 43.5 52.0\nFaster R-CNN+ R50-FPN 42M 42.0 62.1 45.5 26.6 45.4 53.4\nDETR R50 41M 42.0 62.4 44.2 20.5 45.8 61.1\nPix2seq (Ours) R50 37M 43.0 61.0 45.6 25.1 46.9 59.4\nFaster R-CNN R101-FPN 60M 42.0 62.5 45.9 25.2 45.6 54.6\nFaster R-CNN+ R101-FPN 60M 44.0 63.9 47.8 27.2 48.1 56.0\nDETR R101 60M 43.5 63.8 46.4 21.9 48.0 61.8\nPix2seq (Ours) R101 56M 44.5 62.8 47.5 26.0 48.2 60.3\nFaster R-CNN R50-DC5 166M 39.0 60.5 42.3 21.4 43.5 52.5\nFaster R-CNN+ R50-DC5 166M 41.1 61.4 44.3 22.9 45.9 55.0\nDETR R50-DC5 41M 43.3 63.1 45.9 22.5 47.3 61.1\nPix2seq (Ours) R50-DC5 38M 43.2 61.0 46.1 26.6 47.0 58.6\nDETR R101-DC5 60M 44.9 64.7 47.7 23.7 49.5 62.3\nPix2seq (Ours) R101-DC5 57M 45.0 63.2 48.6 28.2 48.9 60.4\nFor training from scratch, we follow (Carion et al., 2020) using a ResNet backbone (He et al., 2016),\nfollowed by 6 layers of transformer encoder and 6 layers of (causal) transformer decoder (Vaswani\net al., 2017). We resize images (with a ﬁxed aspect ratio) so the longer side is 1333 pixels. For\nsequence construction, we use 2000 quantization bins, and we randomize the order of objects every\ntime an image is shown. We append noise objects to real objects such that each image contains 100\nobjects in total, and hence a sequence length of 500. The model is trained for 300 epochs with a\nbatch size of 128.\nFor pretraining on Objects365 dataset, we use similar settings as above with a few differences.\nNotably, instead of using the large 1333×1333 image size, we use a smaller image size of 640×640,\nand pretrain the models for 400K steps with batch size of 256. It is worth noting that this pretraining\nprocess is even faster than training from scratch due to the use of smaller image size. During the ﬁne-\ntuning on COCO dataset, only a small number of epochs (e.g., 20 to 60 epochs) are needed to achieve\ngood results. And we could use larger image size during ﬁne-tuning as well. Due to the use of larger\npretraining dataset, we also experiment with larger models with Vision Transformers (Dosovitskiy\net al., 2020).\nMore details for both training strategies can be found in Appendix B. As for ablations, we use a\nResNet-101 backbone with a smaller image size (the longer side is 640), and we train the model from\nscratch for 200 epochs.\n3.2 M AIN COMPARISONS\nTraining from scratch on COCO We mainly compare with two widely recognized baselines:\nDETR and Faster R-CNN. DETR and our model have comparable architectures, but our Transformer\ndecoder does not require learned “object queries” or separated heads for box regression and classiﬁ-\ncation, since our model generates different types of tokens (e.g., coordinate and class tokens) with\na single softmax. Faster R-CNN is a well established method, with optimized architectures such\nas feature-pyramid networks (FPN) (Lin et al., 2017a). Faster R-CNN is typically trained in fewer\nepochs than DETR or our model, likely because it explicitly incorporates prior knowledge of the\ntask in the architecture itself. Thus we also include an improved Faster R-CNN baseline, denoted\nas Faster R-CNN+, from (Carion et al., 2020), where Faster R-CNN models are trained with the\nGIoU loss (Rezatoﬁghi et al., 2019), train-time random crop augmentations, and the long 9x training\nschedule.\nResults are shown in Table 1, where each section compares different methods of the same ResNet\n“backbone”. Overall, Pix2Seq achieves competitive results to both baselines. Our model performs\ncomparably to Faster R-CNN on small and medium objects, but better on larger objects. Compared\n6\nPublished as a conference paper at ICLR 2022\nTable 2: Average precision of ﬁnetuned Pix2seq models on COCO with different backbone archi-\ntectures and image sizes. All models are pretrained on Objects365 dataset. As a comparison, our\nbest model without pretraining obtains 45.0 AP (in Table 1) with image size of 1333 ×1333. The\npretraining is with 640×640 image size while ﬁne-tuning (a few epochs) can use larger image sizes.\nBackbone # params Image size during ﬁnetuning\n640×640 1024 ×1024 1333 ×1333\nR50 37M 39.1 41.7 42.6\nR50-C4 85M 44.7 46.9 47.3\nViT-B 115M 44.2 46.5 47.1\nViT-L 341M 47.6 49.0 50.0\nwith DETR, our model performs comparably or slightly worse on large and medium objects, but\nsubstantially better (4-5 AP) on small objects.\nPretrain on Objects365 and ﬁnetune on COCO As shown in Table 2, the performances of\nObjects365 pretrained Pix2Seq models are strong across various model sizes and image sizes. The\nbest performance (with 1333 image size) is 50 AP which is 5% higher than the best model trained\nfrom scratch, and the performance holds up very well even with 640 image size. Notably, with a\nsmaller image size used for pretraining, the pretrain+ﬁnetune process is faster than training from\nscratch, and also generalizes better. Both factors are crucial for training larger and better models.\n3.3 A BLATION ON SEQUENCE CONSTRUCTION\nFigure 7a explores the effect of coordinate quantization on performance. For this ablation we consider\nimages the longest size of which is 640 pixels. The plot indicates that quantization to 500 bins or\nmore is sufﬁcient; with 500 bins there are approximately 1.3 pixels per bin, which does not introduce\nsigniﬁcant approximation error. Indeed, as long as one has as many bins as the number of pixels\n(along the longest side of the image) there should be no signiﬁcant error due to quantization of the\nbounding box coordinates.\nWe also consider different object ordering strategies in sequence construction during training. These\ninclude 1) random, 2) area (i.e., descending object size), 3) dist2ori (i.e., the distance of top-left\ncorner of the bounding box to the origin), 4) class (name), 5) class + area (i.e., the objects are ﬁrst\nordered by their class, and if there are multiple objects of the same class, they are ordered by area),\nand 6) class + dist2ori. Figure 7b shows average precision (AP) and Figure 7c shows average recall\n(AR) at the top-100 predictions. Both in terms of precision and recall, the random ordering yields the\nbest performance. We conjecture that with deterministic ordering, it may be difﬁcult for the model to\nrecover from mistakes of missing objects made earlier on, while with random ordering it would still\nbe possible to retrieve them later.\n50 100 200 500 10002000 5000\nQuantizatin bins\n20.0\n22.5\n25.0\n27.5\n30.0\n32.5\n35.0\n37.5\n40.0AP\n(a)\nRandom\nArea\nDist2OriClass\nClass + Area\nClass + Dist2Ori\n30\n32\n34\n36\n38\n40AP (b)\nRandom\nArea\nDist2OriClass\nClass + Area\nClass + Dist2Ori\n30\n35\n40\n45\n50\n55\n60AR@100 (c)\nFigure 7: Ablations on sequence construction. (a) Quantization bins vs. performance. (b) and (c)\nshow AP and AR@100 for different object ordering strategies.\n7\nPublished as a conference paper at ICLR 2022\n3.4 A BLATION ON SEQUENCE AUGMENTATION\nHere we study the impact of sequence augmentation (i.e., adding the noise objects) for both model\ntraining strategies: 1) training from scratch on COCO, and 2) pretraining on Objects365 and ﬁnetuning\non COCO. Results for training from scratch w/wo sequence augmentation are shown in Figure 8, and\nwe ﬁnd that without sequence augmentation, the AP is marginally worse if one delays the sampling\nof EOS token during the inference (via likelihood offsetting), but the recall is signiﬁcantly worse for\nthe optimal AP. Table 3 shows similar results for pretraining+ﬁnetuning setting (where we set a loss\nweight of 0.1 on ending token instead of tuning their likelihood offset), and we ﬁnd that AP is not\nsigniﬁcantly affected while recall is signiﬁcantly worse without sequence augmentation. It is also\nworth noting that sequence augmentation is mainly effective during the ﬁne-tuning.\n45.0 47.5 50.0 52.5 55.0 57.5\nAverage Recall @100\n34\n35\n36\n37\n38\n39\n40Average Precision\n39.2 AP\n37.6 AP\nW/o SeqAug\nW SeqAug\nFigure 8: Impact of sequence augmentation\non when training from scratch on COCO.\nSeqAug in SeqAug in AP AR@100Pretrain Finetune\n\u0017 \u0017 43.7 55.4\n\u0017 \u0013 44.5 61.6\n\u0013 \u0013 44.7 61.7\nTable 3: Impact of sequence augmentation when\npretraining on Objects365 and ﬁnetuning on\nCOCO. Sequence augmentation has a major im-\npact on average recall (@100) but a smaller inﬂu-\nence on AP. Most improvements can be achieved\nduring ﬁne-tuning.\n3.5 V ISUALIZATION OF DECODER ’S CROSS ATTENTION MAP\nWhen generating a new token, the transformer decoder uses self attention over the preceding tokens\nand cross attention over the encoded visual feature map. Here we visualize the cross attention\n(averaged over layers and heads) as the model predicts a new token. Figure 9 shows cross attention\nmaps as the ﬁrst few tokens are generated. One can see that the attention is very diverse when\npredicting the ﬁrst coordinate token (i.e ymin), but then quickly concentrates and ﬁxates on the object.\n(a)\n (b)\n (c)\nFigure 9: Decoder’s cross attention to visual feature map when predicting the ﬁrst 5 objects. (b)\nwe reshape a prediction sequence of 25 into a 5x5 grid, so each row represents a prediction for 5\ntokens [ymin, xmin, ymax, xmax, c]. The attention is diverse when selecting the ﬁrst token of the object,\nthen quickly concentrates on the object. (c) Overlay of the cross attention (when predicting the class\ntoken) on the original image.\n8\nPublished as a conference paper at ICLR 2022\n4 R ELATED WORK\nObject detection. Existing object detection algorithms incorporate explicit prior knowledge about the\ntask in their choice of architecture and loss function. To predict a set of bounding boxes, architectures\nof modern detectors are speciﬁcally designed to produce a large set of proposals (Girshick, 2015; Ren\net al., 2015; Cai & Vasconcelos, 2018), anchors (Lin et al., 2017b), or window centers (Tian et al.,\n2019; Zhou et al., 2019). Non-maximum suppression (Bodla et al., 2017) is often required to prevent\nduplicate predictions. While DETR (Carion et al., 2020) avoids sophisticated bounding box proposals\nand non-maximum suppression, it still requires a set of learned “object queries”, specially for object\nbinding. These detectors all require sub-networks (or extra layers) separately for regressing bounding\nboxes and class labels. Pix2Seq avoids such complexities by having a generic image encoder and\nsequence decoder, with a single softmax for producing coordinate tokens and class labels.\nBeyond architectures, the loss functions of existing detectors are also highly tailored for matching\nbounding boxes. For example, the loss function is often based on bounding box regression (Szegedy\net al., 2013; Lin et al., 2017b), intersection over union (Rezatoﬁghi et al., 2019), and set-based\nmatching (Erhan et al., 2014; Liu et al., 2016; Redmon et al., 2016; Stewart et al., 2016; Carion\net al., 2020). Pix2Seq avoids specialized losses, showing that a straightforward maximum likelihood\nobjective with softmax cross entropy can work well.\nOur work is also related to recurrent models in object detection (Stewart et al., 2016; Park & Berg,\n2015; Romera-Paredes & Torr, 2016; Salvador et al., 2017; Ren & Zemel, 2017), in which the\nsystem learns to predict one object at a time. As above, both architecture and loss functions in these\napproaches are often tailored to the detection task. Furthermore, these approaches are not based on\nTransformers, and have not been evaluated against modern baselines on larger datasets.\nLanguage modeling. Our work is inspired by recent success of modern language modeling (Radford\net al., 2019; Raffel et al., 2019; Brown et al., 2020). Although originally intended for natural\nlanguages, the underlying methodology has been shown capable of modeling various sequential data,\nsuch as machine translation (Sutskever et al., 2014; Bahdanau et al., 2014), image captioning (Vinyals\net al., 2015b; Karpathy & Fei-Fei, 2015; Xu et al., 2015), and many others (Vinyals et al., 2015a;\nHuang et al., 2018; Ramesh et al., 2021; Chen et al., 2021). Our work enriches this portfolio and\nshows that it works for even non-sequential data (by turning a set of objects into a sequence of\ntokens). We augment both input and target sequences for our model to incorporate task-speciﬁc\nprior knowledge; similar sequence corruption scheme have been used in language models (Devlin\net al., 2018; Clark et al., 2020), and bear some similarity to noise-contrastive learning (Gutmann &\nHyv¨arinen, 2010) and the discriminator in GANs (Goodfellow et al., 2014).\n5 C ONCLUSION AND FUTURE WORK\nThis paper introduces Pix2Seq, a simple yet generic framework for object detection. By casting\nobject detection as a language modeling task, our approach largely simpliﬁes the detection pipeline,\nremoving most of the specialization in modern detection algorithms. We believe that our framework\nnot only works for object detection, but can also be applied to other vision tasks where the output can\nbe represented by a relatively concise sequence of discrete tokens (e.g., keypoint detection, image\ncaptioning, visual question answering). To this end, we hope to extend Pix2Seq as a generic and\nuniﬁed interface for solving a large variety of vision tasks.\nA major limitation of our approach is that autoregressive modeling is expensive for long sequences\n(mainly during model inference). Practical measures to mitigate the issue includes: 1) stop inference\nwhen the ending token is produced (e.g., in COCO dataset, there are, in average, 7 objects per\nimage, leading to a relatively small number of ∼35 tokens), 2) applying it to ofﬂine inference, or\nonline scenarios where the objects of interest are relatively sparse (e.g. locate a speciﬁc object with\nlanguage description). However, future work is needed to make it faster for real-time object detection\napplications. Another limitation is that the current approach for training Pix2Seq is entirely based on\nhuman annotation, and by reducing such dependence, it can enable the model to beneﬁt from more\nunlabeled data.\n9\nPublished as a conference paper at ICLR 2022\nACKNOWLEDGEMENTS\nWe specially thank Xiuye Gu for preparing the Objects365 dataset. We thank Mohammad Norouzi,\nSimon Kornblith, Tsung-Yi Lin, Allan Jabri, and Kevin Swersky for the helpful discussions.\nREFERENCES\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to\nalign and translate. arXiv preprint arXiv:1409.0473, 2014.\nNavaneeth Bodla, Bharat Singh, Rama Chellappa, and Larry S Davis. Soft-nms–improving object detection with\none line of code. In Proceedings of the IEEE International Conference on Computer Vision, pp. 5561–5569,\n2017.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\narXiv preprint arXiv:2005.14165, 2020.\nZhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving into high quality object detection. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6154–6162, 2018.\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko.\nEnd-to-end object detection with transformers. In European Conference on Computer Vision, pp. 213–229.\nSpringer, 2020.\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind\nSrinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. arXiv\npreprint arXiv:2106.01345, 2021.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive\nlearning of visual representations. In International Conference on Machine Learning, pp. 1597–1607. PMLR,\n2020a.\nTing Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E Hinton. Big self-supervised\nmodels are strong semi-supervised learners. Advances in Neural Information Processing Systems , 33:\n22243–22255, 2020b.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and Christopher D. Manning. ELECTRA: Pre-training text\nencoders as discriminators rather than generators. In ICLR, 2020.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words:\nTransformers for image recognition at scale. In International Conference on Learning Representations, 2020.\nDumitru Erhan, Christian Szegedy, Alexander Toshev, and Dragomir Anguelov. Scalable object detection using\ndeep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\npp. 2147–2154, 2014.\nM. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal visual object classes\n(voc) challenge. International Journal of Computer Vision, 88(2):303–338, June 2010.\nGolnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D Cubuk, Quoc V Le, and Barret Zoph.\nSimple copy-paste is a strong data augmentation method for instance segmentation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2918–2928, 2021.\nRoss Girshick. Fast r-cnn. In Proceedings of the IEEE International Conference on Computer Vision , pp.\n1440–1448, 2015.\nRoss Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object\ndetection and semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 580–587, 2014.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial nets. Advances in Neural Information Processing\nSystems, 27, 2014.\n10\nPublished as a conference paper at ICLR 2022\nMichael Gutmann and Aapo Hyv¨arinen. Noise-contrastive estimation: A new estimation principle for unnormal-\nized statistical models. In Proceedings of the thirteenth International Conference on artiﬁcial intelligence\nand statistics, pp. 297–304. JMLR Workshop and Conference Proceedings, 2010.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778, 2016.\nKaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE\nInternational Conference on Computer Vision, pp. 2961–2969, 2017.\nElad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoeﬂer, and Daniel Soudry. Augment your batch:\nImproving generalization through instance repetition. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 8129–8138, 2020.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration.\narXiv preprint arXiv:1904.09751, 2019.\nAndrew G Howard. Some improvements on deep convolutional neural network based image classiﬁcation. arXiv\npreprint arXiv:1312.5402, 2013.\nCheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer, Ian Simon, Curtis Hawthorne,\nAndrew M Dai, Matthew D Hoffman, Monica Dinculescu, and Douglas Eck. Music transformer. arXiv\npreprint arXiv:1809.04281, 2018.\nGao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth.\nIn European Conference on Computer Vision, pp. 646–661. Springer, 2016.\nPaul F Jaeger, Simon AA Kohl, Sebastian Bickelhaupt, Fabian Isensee, Tristan Anselm Kuder, Heinz-Peter\nSchlemmer, and Klaus H Maier-Hein. Retina u-net: Embarrassingly simple exploitation of segmentation\nsupervision for medical object detection. In Machine Learning for Health Workshop, pp. 171–183. PMLR,\n2020.\nAndrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3128–3137, 2015.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization.arXiv preprint arXiv:1412.6980,\n2014.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural\nnetworks. Advances in Neural Information Processing Systems, 25:1097–1105, 2012.\nAlina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali,\nStefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4. International\nJournal of Computer Vision, 128(7):1956–1981, 2020.\nYann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and\nLawrence D Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):\n541–551, 1989.\nYi Li, Haozhi Qi, Jifeng Dai, Xiangyang Ji, and Yichen Wei. Fully convolutional instance-aware semantic\nsegmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp.\n2359–2367, 2017.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and\nC Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer\nVision, pp. 740–755. Springer, 2014.\nTsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature\npyramid networks for object detection. In Proceedings of the IEEE Conference on Computer Vision and\npattern recognition, pp. 2117–2125, 2017a.\nTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll´ar. Focal loss for dense object detection.\nIn Proceedings of the IEEE International Conference on Computer Vision, pp. 2980–2988, 2017b.\nWei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C\nBerg. Ssd: Single shot multibox detector. In European Conference on Computer Vision, pp. 21–37. Springer,\n2016.\n11\nPublished as a conference paper at ICLR 2022\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on\nLearning Representations, 2018.\nEunbyung Park and Alexander C Berg. Learning to decompose for object detection and instance segmentation.\narXiv preprint arXiv:1511.06449, 2015.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by\ngenerative pre-training. 2018.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are\nunsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. arXiv\npreprint arXiv:1910.10683, 2019.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen, and Ilya\nSutskever. Zero-shot text-to-image generation. arXiv preprint arXiv:2102.12092, 2021.\nJoseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Uniﬁed, real-time object\ndetection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 779–788,\n2016.\nMengye Ren and Richard S Zemel. End-to-end instance segmentation with recurrent attention. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6656–6664, 2017.\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with\nregion proposal networks. Advances in Neural Information Processing Systems, 28:91–99, 2015.\nHamid Rezatoﬁghi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, and Silvio Savarese. Generalized\nintersection over union. June 2019.\nBernardino Romera-Paredes and Philip Hilaire Sean Torr. Recurrent instance segmentation. In European\nConference on Computer Vision, pp. 312–329. Springer, 2016.\nInkyu Sa, Zongyuan Ge, Feras Dayoub, Ben Upcroft, Tristan Perez, and Chris McCool. Deepfruits: A fruit\ndetection system using deep neural networks. sensors, 16(8):1222, 2016.\nAmaia Salvador, Miriam Bellver, Victor Campos, Manel Baradad, Ferran Marques, Jordi Torres, and Xavier\nGiro-i Nieto. Recurrent neural networks for semantic instance segmentation.arXiv preprint arXiv:1712.00617,\n2017.\nShuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun.\nObjects365: A large-scale, high-quality dataset for object detection. In Proceedings of the IEEE/CVF\ninternational conference on computer vision, pp. 8430–8439, 2019.\nRussell Stewart, Mykhaylo Andriluka, and Andrew Y Ng. End-to-end people detection in crowded scenes. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2325–2333, 2016.\nPei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo,\nYin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo\nopen dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n2446–2454, 2020.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. InAdvances\nin Neural Information Processing Systems, pp. 3104–3112, 2014.\nChristian Szegedy, Alexander Toshev, and Dumitru Erhan. Deep neural networks for object detection. Advances\nin neural information processing systems, 26, 2013.\nZhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: Fully convolutional one-stage object detection. In\nProceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9627–9636, 2019.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,Łukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pp.\n5998–6008, 2017.\nOriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton. Grammar as a\nforeign language. Advances in Neural Information Processing Systems, 28:2773–2781, 2015a.\n12\nPublished as a conference paper at ICLR 2022\nOriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption\ngenerator. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3156–\n3164, 2015b.\nYuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2. https:\n//github.com/facebookresearch/detectron2, 2019.\nKelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and\nYoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In International\nConference on Machine Learning, pp. 2048–2057. PMLR, 2015.\nXingyi Zhou, Dequan Wang, and Philipp Kr ¨ahenb¨uhl. Objects as points. arXiv preprint arXiv:1904.07850,\n2019.\n13\nPublished as a conference paper at ICLR 2022\nA Q UANTIZATION AND DEQUANTIZATION OF COORDINATES\nAlgorithm 1 and 2 illustrate the quantization and dequantization process of (normalized) coordinates.\nAlgorithm 1 Quantization of (normalized) coordi-\nnates\ndef quantize(x, bins=1000):\n# x is a real number between [0, 1]\n# returns an integer between [0, bins-1]\nreturn int(x * (bins - 1))\nAlgorithm 2 Dequantization of discrete tokens of\ncoordinates\ndef dequantize(x, bins=1000):\n# x is an integer between [0, bins-1]\n# returns a real number between [0, 1]\nreturn float(x) / (bins - 1)\nB T RAINING DETAILS\nTraining from scratch on COCO For baseline architectures, we follow (Carion et al., 2020) using\na ResNet backbone (He et al., 2016), followed by 6 layers of transformer encoder and 6 layers of\n(causal) transformer decoder (Vaswani et al., 2017). The main dimension of transformer is set to\n256 with 8 attention heads, and the dimension of the feed-forward network is set to 1024. We use\nthe stochastic depth (Huang et al., 2016) with a rate of 10% to reduce overﬁtting. Per (Carion et al.,\n2020), we also experiment with the DC5 variant of ResNet (Li et al., 2017), which increases the\nresolution of its output feature map by a factor of two.2\nFor image augmentation during training, we perform scale jittering with random crops (Ghiasi et al.,\n2021; Wu et al., 2019) with strength of [0.1, 3]. We resize images (with a ﬁxed aspect ratio) so\nthe longer side is 1333 pixels. Following (Howard, 2013; Chen et al., 2020a;b), we also use color\ndistortion with a strength of 0.5. For sequence construction, we use 2000 quantization bins, and we\nrandomize the order of objects every time an image is shown. We append noise objects to real objects\nsuch that each image contains 100 objects in total, and hence a sequence length of 500.\nWe train the entire network from scratch for 300 epochs with a batch size of 128. For each image\nin a mini-batch, we perform two independent augmentations, similar to (Hoffer et al., 2020), re-\nsulting in a 256 effective batch size, which we ﬁnd helpful to reduce overﬁtting. We use AdamW\noptimizer (Kingma & Ba, 2014; Loshchilov & Hutter, 2018) with a learning rate of 0.003 and weight\ndecay of 0.05. We use a learning rate warmup for 10 epochs and then linearly decay the learning rate\nover the course of training.\nPretraining on Objects365 We explore a wider range of architecture variants including both\nhybrid ResNet and transformer models (Carion et al., 2020), as well as pure transformers based on\nimage patches (Dosovitskiy et al., 2020). The details of the architecture can be found in our released\ncode. Since Objects365 dataset is much larger than COCO (1.7M images vs 118K images), we use\na weaker image augmentation (scale jittering range of [0.3, 2] for ViT backbones, and [0.9, 1.2] for\nResNet backbones) without color distortion. For sequence construction, we use 1000 quantization\nbins. And we still apply sequence augmentation with sampled noise objects added by default.\nWe use a smaller image size of 640×640, and pretrain the models for 400K steps with batch size\nof 256. We do not perform two augmentations per batch as in training from scratch. And we use a\nsmaller learning rate of 0.001 with the same weight decay of 0.05. We use a cosine learning rate\ndecay with a initial warmup of 20K steps.\nAs for the ﬁnetuning on COCO dataset, we use a batch size of 128 for ResNet backbones, and 64\nfor ViT backbones. Most models are ﬁnetuned for 60 epochs with a learning rate of 3e−5, but even\nfewer epochs yield similar results. We still use scale jittering with a range of [0.3, 2] for image\naugmentation.\n2Adding a dilation to the last ResNet stage and removing the stride from the ﬁrst convolution of that stage.\n14\nPublished as a conference paper at ICLR 2022\nC A BLATION ON INFERENCE (arg maxVS NUCLEUS SAMPLING )\nNucleus sampling (Holtzman et al., 2019) has been applied to language modeling to reduce duplication\nand increase diversity in generated samples. Here we study its impact on sampling from our trained\nmodel.\nGiven the distributionP(yj|x, y1:j−1), to apply nucleus sampling, we ﬁrst deﬁne its top-p vocabulary\nV (p) ⊂V as the smallest set such that∑\nyj∈V (p)\nP(yj|x, y1:j−1) ≥p. (2)\nLet p′= ∑\nyj∈V (p) P(yj|x, y1:j−1), and we can re-calibrate the conditional likelihood as following\nfor sampling the next token.\nP′(yj|x, y1:j−1) =\n{\nP(yj|x, y1:i−1)/p′ if yj ∈V (p)\n0 otherwise. (3)\nWe vary the hyper-parameterp of nucleus sampling used in generating the output sequence (during\ninference). When p = 0, it corresponds to arg maxsampling, otherwise it samples from a truncated\nranked list of tokens that has a cumsum larger or equal to p. In Figure 10, we see that use of nucleus\nsampling (with p >0) improves object recall and thus also leads to better average precision. There is\na relatively ﬂat region of AP between 0.2 and 0.5, and we select p to be 0.4 as our default value for\nother experiments.\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nP\n34\n35\n36\n37\n38\n39\n40AP\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nP\n50\n51\n52\n53\n54\n55\n56Average Recall@100\nFigure 10: Varying parameter p in nucleus sampling during inference results in different AP and AR.\nWith p = 0, it is equivalent to argmax sampling. Sampling with p >0 is helpful for increasing recall\n(and precision).\nD V ISUALIZATION OF SIMILARITY AMONG COORDINATE TOKENS\nIn our model, bounding box coordinates are not represented as ﬂoating points, but encoded as discrete\ntokens. Here we study the similarity among these coordinate tokens via their embeddings. Note\nthat the discrete coordinate tokens and class name tokens are in the same vocabulary and share the\nsame embedding matrix. Speciﬁcally, we ﬁrst slice the learned embedding matrix corresponding to\ncoordinate tokens, and then compute the cosine similarity of embedding vectors for these coordinate\ntokens.\nFigure 11 shows cosine similarity among embeddings of coordinate tokens. We can see that nearby\ncoordinates have higher similarities in their token embeddings than far away ones. This emergent\nproperty of our model is likely due to the noises / uncertainties in bounding box annotations (i.e.\na bounding box annotation is a random sample from a distribution over potential bounding boxes\nwhich encodes locality of coordinates).\nE T HE ABILITY TO DIRECT THE ATTENTION WITH GIVEN COORDINATES\nWe explore the model’s ability topay attention to a pointed region speciﬁed via coordinates. We\ndivide an image evenly into an N ×N grid of rectangular regions, each speciﬁed by a sequence of\n15\nPublished as a conference paper at ICLR 2022\nCoordinate tokens\nCoordinate tokens\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(a)\nCoordinate tokens\nCoordinate tokens\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n (b)\n0 500 1000 1500 2000\nCoordinate tokens\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Cosine similarity\n(c)\n0 500 1000 1500 2000\nCoordinate tokens\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Cosine similarity (d)\n0 500 1000 1500 2000\nCoordinate tokens\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Cosine similarity (e)\nFigure 11: (a) Cosine similarity among embeddings of coordinate tokens. (b) is part of (a) covering\nonly the ﬁrst 100 tokens. (c), (d) and (e) are the 500-th, 1000-th and 1500-th rows of (a), respectively.\nNearby coordinates have higher similarities in their token embeddings.\ncoordinates for its bounding box. We then visualize the decoder’s cross attention to visual feature map\nafter reading the sequence of coordinates for each region, i.e., [ymin, xmin, ymax, xmax]. We shufﬂe the\npixels in the image to remove distraction from existing objects, and remove 2% of the top attentions\nfor clarity. Interestingly, as shown in Figure 12, it seems the model can pay attention to the speciﬁed\nregion at different scales.\n(a) 4 ×4 grids\n (b) 6 ×6 grids\n (c) 8 ×8 grids\n (d) 10 ×10 grids\nFigure 12: Each grid is a visualization of decoder’s attention after reading a small sequence of\ncoordinates, i.e., [ymin, xmin, ymax, xmax]. Visualization is done for grids of different sizes. The network\nlearns to pay attention to pointed region at different scales.\nF M ORE VISUALIZATION ON DECODER ’S CROSS ATTENTION\nIn Figure 13, we overlay the cross attention (when predicting the class token) on the original image\nfor several other images, and it shows that the decoder pays the most attention to the object when\npredicting the class token.\n16\nPublished as a conference paper at ICLR 2022\n(a)\n (b)\n (c)\nFigure 13: Visualization of Transformer decoder’s cross attention (when predicting class tokens)\nconditioned on the given bounding boxes.\nG V ISUALIZATION OF DETECTION RESULTS\nIn Figure 14, we visualize detection results of one of Pix2seq model (with 46 AP) on a subset of\nimages from COCO validation set that contain a crowded set of objects.\nFigure 14: Examples of the model’s predictions (at the score threshold of 0.5). Original images\naccessed by clicking the images in supported PDF readers.\n17",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7862571477890015
    },
    {
      "name": "Bounding overwatch",
      "score": 0.7463566064834595
    },
    {
      "name": "Task (project management)",
      "score": 0.6075590252876282
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5904667377471924
    },
    {
      "name": "Object detection",
      "score": 0.57365882396698
    },
    {
      "name": "Intuition",
      "score": 0.5550586581230164
    },
    {
      "name": "Object (grammar)",
      "score": 0.5317268371582031
    },
    {
      "name": "Artificial neural network",
      "score": 0.5145317912101746
    },
    {
      "name": "Class (philosophy)",
      "score": 0.47699764370918274
    },
    {
      "name": "Image (mathematics)",
      "score": 0.41426026821136475
    },
    {
      "name": "Machine learning",
      "score": 0.3763238191604614
    },
    {
      "name": "Natural language processing",
      "score": 0.3666830062866211
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3517380356788635
    },
    {
      "name": "Cognitive science",
      "score": 0.08458450436592102
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Psychology",
      "score": 0.0
    }
  ],
  "institutions": []
}