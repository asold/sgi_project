{
    "title": "Comparison of Diverse Decoding Methods from Conditional Language Models",
    "url": "https://openalex.org/W2951038425",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A2799006035",
            "name": "Daphne Ippolito",
            "affiliations": [
                "California University of Pennsylvania"
            ]
        },
        {
            "id": "https://openalex.org/A2798955190",
            "name": "Reno Kriz",
            "affiliations": [
                "California University of Pennsylvania"
            ]
        },
        {
            "id": "https://openalex.org/A2554339368",
            "name": "João Sedoc",
            "affiliations": [
                "California University of Pennsylvania"
            ]
        },
        {
            "id": "https://openalex.org/A2950830292",
            "name": "Maria Kustikova",
            "affiliations": [
                "California University of Pennsylvania"
            ]
        },
        {
            "id": "https://openalex.org/A4207984947",
            "name": "Chris Callison-Burch",
            "affiliations": [
                "California University of Pennsylvania"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2222235228",
        "https://openalex.org/W2125320996",
        "https://openalex.org/W1902237438",
        "https://openalex.org/W2790165607",
        "https://openalex.org/W2745461083",
        "https://openalex.org/W2963929190",
        "https://openalex.org/W1956340063",
        "https://openalex.org/W2952280909",
        "https://openalex.org/W2743149734",
        "https://openalex.org/W2353655624",
        "https://openalex.org/W2890276793",
        "https://openalex.org/W2963212250",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W2890969459",
        "https://openalex.org/W2963206148",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2143449221",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W2963141266",
        "https://openalex.org/W2898658996",
        "https://openalex.org/W2328886022",
        "https://openalex.org/W1591706642",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W2549599535",
        "https://openalex.org/W2506483933",
        "https://openalex.org/W2949555952",
        "https://openalex.org/W2963466651",
        "https://openalex.org/W2954116503",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2996287690",
        "https://openalex.org/W635530177",
        "https://openalex.org/W2963096510",
        "https://openalex.org/W2252065493",
        "https://openalex.org/W2557436004",
        "https://openalex.org/W2131744502",
        "https://openalex.org/W2130942839"
    ],
    "abstract": "While conditional language models have greatly improved in their ability to output high-quality natural language, many NLP applications benefit from being able to generate a diverse set of candidate sequences. Diverse decoding strategies aim to, within a given-sized candidate list, cover as much of the space of high-quality outputs as possible, leading to improvements for tasks that re-rank and combine candidate outputs. Standard decoding methods, such as beam search, optimize for generating high likelihood sequences rather than diverse ones, though recent work has focused on increasing diversity in these methods. In this work, we perform an extensive survey of decoding-time strategies for generating diverse outputs from conditional language models. We also show how diversity can be improved without sacrificing quality by over-sampling additional candidates, then filtering to the desired number.",
    "full_text": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3752–3762\nFlorence, Italy, July 28 - August 2, 2019.c⃝2019 Association for Computational Linguistics\n3752\nComparison of Diverse Decoding Methods from Conditional Language\nModels\nDaphne Ippolito⋆ Reno Kriz⋆ Maria Kustikova Jo ˜ao Sedoc Chris Callison-Burch\n⋆Authors contributed equally\nUniversity of Pennsylvania\n{daphnei,rekriz,mkust,joao,ccb}@seas.upenn.edu\nAbstract\nWhile conditional language models have\ngreatly improved in their ability to output\nhigh-quality natural language, many NLP ap-\nplications beneﬁt from being able to generate\na diverse set of candidate sequences. Diverse\ndecoding strategies aim to, within a given-\nsized candidate list, cover as much of the space\nof high-quality outputs as possible, leading to\nimprovements for tasks that re-rank and com-\nbine candidate outputs. Standard decoding\nmethods, such as beam search, optimize for\ngenerating high likelihood sequences rather\nthan diverse ones, though recent work has fo-\ncused on increasing diversity in these meth-\nods. In this work, we perform an extensive\nsurvey of decoding-time strategies for generat-\ning diverse outputs from conditional language\nmodels. We also show how diversity can be\nimproved without sacriﬁcing quality by over-\nsampling additional candidates, then ﬁltering\nto the desired number.\n1 Introduction\nConditional neural language models, which train\na neural net to map from one sequence to an-\nother, have had enormous success in natural lan-\nguage processing tasks such as machine transla-\ntion (Sutskever et al., 2014; Luong et al., 2015),\ntext summarization (Nallapati et al., 2016), and di-\nalog systems (Vinyals and Le, 2015). These mod-\nels output a probability distribution over the next\ntoken in the output sequence given the input and\nthe previously predicted tokens. Since comput-\ning the overall most likely output sequence is in-\ntractable, early work in neural machine translation\nfound that beam search is an effective strategy to\nheuristically sample sufﬁciently likely sequences\nfrom these probabilistic models (Sutskever et al.,\n2014). However, for more open-ended tasks, beam\nsearch is ill-suited to generating a set of diverse\ncandidate sequences; this is because candidates\nBeam Search\nA bus is stopped at a bus stop.\nA bus is parked at a bus stop.\nA bus stopped at a bus stop in a city.\nA bus stopped at a bus stop at a bus stop.\nA bus that is parked in front of a building.\nRandom Sampling\nA bus parked at a bus stop at a bus stop.\nThere is a bus that is at the station.\nA man standing by a bus in a city.\nA bus pulling away from the train station.\nA bus stopped at a stop on the sunny day.\nFigure 1: An image with the top ﬁve captions from\nstandard beam search and from random sampling. Note\nthe latter set is more diverse but lower quality.\noutputted from a large-scale beam search often\nonly differ by punctuation and minor morphologi-\ncal variations (Li and Jurafsky, 2016).\nThe term “diversity” has been deﬁned in a vari-\nety of ways in the literature, with some using it as\na synonym for sentence interestingness or unlike-\nliness (Hashimoto et al., 2019), and others consid-\nering it a measure of how different two or more\nsentences are from each other (Vijayakumar et al.,\n2016; Gimpel et al., 2013). We take the latter\napproach, and deﬁne diversity as the ability of a\ngenerative method to create a set of possible out-\nputs that are each valid given the input, but vary as\nwidely as possible in terms of word choice, topic,\nand meaning.\nThere are a number of reasons why it is de-\nsirable to produce a set of diverse candidate out-\nputs for a given input. For example, in collabo-\nrative story generation, the system makes sugges-\ntions to a user for what they should write next\n(Clark et al., 2018). In these settings, it would\nbe beneﬁcial to show the user multiple different\nways to continue their story. In image captioning,\nany one sentence-long caption is probably missing\nsome information about the image. Krause et al.\n(2017) show how a set of diverse sentence-length\nimage captions can be transformed into an entire\nparagraph about the image. Lastly, in applica-\n3753\ntions that involve reranking candidate sequences,\nthe reranking algorithms are more effective when\nthe input sequences are diverse. Reranking di-\nverse candidates has been shown to improve re-\nsults in both open dialog and machine translation\n(Li et al., 2016a; Li and Jurafsky, 2016; Gimpel\net al., 2013). Furthermore, in open-ended dialog,\nthe use of reranking to personalize a model’s re-\nsponses for each user is a promising research di-\nrection (Choudhary et al., 2017).\nWith these sorts of applications in mind, a vari-\nety of alternatives and extensions to beam search\nhave been proposed which seek to produce a set\nof diverse candidate responses instead of a single\nhigh likelihood one (Li et al., 2016a; Vijayakumar\net al., 2016; Kulikov et al., 2018; Tam et al., 2019).\nMany of these approaches show marked improve-\nment in diversity over standard beam search across\na variety of generative tasks. However, there has\nbeen little attempt to compare and evaluate these\nstrategies against each other on any single task.\nIn this paper, we survey existing methods for\npromoting diversity in order to systematically in-\nvestigate the relationship between diversity and\nperceived quality of output sequences of condi-\ntional language models. In addition to standard\nbeam search and greedy random sampling, we\ncompare several recently proposed modiﬁcations\nto both methods. In addition, we propose the use\nof over-sampling followed by post-decoding clus-\ntering to remove similar sequences.\nThe main contributions of this paper can be\nsummarized as follows:\n• A detailed comparison of existing diverse de-\ncoding strategies on two tasks: open-ended\ndialog and image captioning, and recommen-\ndations for a diverse decoding strategy.\n• A novel clustering-based algorithm that can\nbe used on the results of any decoding strat-\negy to increase quality and diversity.1\n2 Standard Decoding Methods\nConditional language models, which have wide\napplications across machine translation, text sim-\npliﬁcation, conversational agents, and more, gen-\nerally consist of an encoder, which transforms\nsome input x into a ﬁxed-size latent represen-\ntation, and a decoder which transforms these\nrepresentations in order to output a conditional\n1Code can be found at https://github.com/\nrekriz11/DeDiv.\nprobability of each word in the target sequence\ngiven the previous words and the input. Let\nzt = f(y1,...,y t−1,x) represent the output of\nan encoder-decoder model given input x and the\nsequence of tokens predicted so far, y1,...,y t−1,\nwhich for notational simplicity we write as y<t.\nThe output zt ∈RV (where V is the cardinality of\nthe enumerated vocabulary V)\nThe probability distribution over the next possi-\nble token being word wi ∈V is the softmax:\nP(yt = wi|y<t,x) = exp(zt,i)∑V\nj=1 exp (zt,j)\n∀i∈{1,...,V }\nMost decoding strategies strive to ﬁnd the most\nlikely overall sequence, i.e. pick a ˆ ysuch that:\nˆ y= arg maxy P(y|x) = arg maxy\n∏N\nt=1P(yt|y<t,x)\nUnlike Markovian processes, no sub-exponential\nalgorithm exists to ﬁnd the optimal decoded se-\nquence, and thus we instead use approximations.\nArg-max The simplest approach to decoding a\nlikely sequence is to greedily select a word at each\ntimestep:\nˆyt = arg max\nyt\nP(yt|y<t,x)\nHowever, because this deterministic approach typ-\nically yields repetitive and short output sequences,\nand does not permit generating multiple samples,\nit is rarely used in language modelling.\nRandom Sampling Another option is to ran-\ndomly sample from the model’s distribution at ev-\nery timestep. Often, a temperature parameter T\nis added to control the entropy of the distribution\nbefore sampling.\nP(yt = wi|y<t,x) = exp(zt,i/T)∑V\nj=1 exp (zt,j/T)\n∀i∈{1,...,V }\nˆyt ∼yt\nChoosing a temperature greater than one causes\noutputs to look increasingly more random, while\nbringing the temperature less than zero causes\nsequences to increasingly resemble greedy sam-\npling.\nRecently, top-srandom sampling has been pro-\nposed as an alternative to using temperature. Sam-\npling is restricted to the s most likely tokens\n3754\nat each step (Fan et al., 2018; Radford et al.,\n2019). We ﬁnd that top-srandom sampling’s hard-\nrestriction on generating low probability words is\nmore effective at controlling the stochasticity of\nsampled sequences than sampling with tempera-\nture.\nBeam Search Beam search approximates ﬁnd-\ning the most likely sequence by performing\nbreadth-ﬁrst search over a restricted search space.\nAt every step of decoding, the method keeps track\nof bpartial hypotheses. The next set of partial hy-\npotheses are chosen by expanding every path from\nthe existing set of bhypotheses, and then choosing\nthe bwith the highest scores. Most commonly, the\nlog-likelihood of the partial sequence is used as\nthe scoring function. We use this as our baseline.2\nSince beam search only explores a limited por-\ntion of the overall search space, it tends to yield\nmultiple variants of the same high-likelihood se-\nquence, sequences that often only differ in punc-\ntuation and minor morphological changes (Li and\nJurafsky, 2016). Therefore, standard beam search\nis not ideal for producing diverse outputs.\n3 Extensions to Beam Search\nIn this section, we will discuss a variety of meth-\nods that have been developed recently to eliminate\nredundancy during decoding and generate a wider\nrange of candidate outputs.\nNoisy Parallel Approximate Decoding Intro-\nduced by Cho (2016), NPAD is a technique than\ncan be applied to any decoding setting. The\nmain idea is that diversity can be achieved more\nnaturally by taking advantage of the continuous\nmanifold on which neural nets embed language.\nInstead of encouraging diversity by manipulat-\ning the probabilities outputted from the model,\ndiverse outputs are instead produced by adding\nsmall amounts of noise to the hidden state of the\ndecoder at each step. The noise is randomly sam-\npled from a normal distribution. The variance is\ngradually annealed from a starting σ0 to 0 as de-\ncoding progresses (that is σt = σ0\nt ) under the rea-\nsoning that uncertainty is greatest at the beginning\nof decoding. NPAD can be used in conjunction\nwith any decoding strategy; following the best re-\nsults from the original paper, we show results us-\ning NPAD with beam search.\nExtensions to NPAD have sought to learn the\ndirection in which to manipulate the hidden states\n2We present the beam search algorithm in the appendix.\nusing an arbitrary decoding objective (Gu et al.,\n2017). Since such objectives can be highly\ndomain-speciﬁc, we do not evaluate this method.\nTop-g Capping In beam search, it is often the\ncase that one hypothesis h is assigned a much\nhigher probability than all other hypotheses, caus-\ning all hypotheses in the next step to havehas their\nparent. Following Li and Jurafsky (2016) and Li\net al. (2016b), we add an additional constraint to\nstandard beam search to encourage the model to\nchoose options from diverse candidates. At each\nstep t, current hypotheses are grouped according\nto the parental hypothesis they come from. Af-\nter grouping candidates, only the top gfrom each\ngrouping are considered. The resulting b×gcan-\ndidates are ranked, and the top b are selected as\nhypotheses for the next beam step.\nHamming Diversity Reward Vijayakumar\net al. (2016) proposes adding an additional\ndiversity-promoting term, θ, to the log-likelihood\nbefore reranking. This term measures how differ-\nent a candidate hypothesis c(i)\n≤t is from the partial\nhypotheses selected in the previous step. Let\nHt−1 = {c(1)\n≤t−1, . . .c(b)\n≤t−1}be these partial hy-\npotheses. Then the beam search scoring function\nfor the ith candidate at timestep tbecomes:\nscore(c(i)\n≤t) =\nt∑\nj=1\n( logP(c(i)\nj |c(i)\n<j,x))\n+λθ(c(i)\n≤t,Ht−1)\nwhere λis a tunable hyperparameter. Vijayakumar\net al. (2016) try a variety of deﬁnitions for θ, in-\ncluding embedding diversity andn-gram diversity,\nbut they ﬁnd that Hamming distance, the number\nof tokens in the candidate sequence which exist in\nthe previously selected partial hypotheses, is most\neffective. We take the negative of the Hamming\ndistance as θ.\nIterative Beam Search In an attempt to im-\nprove the size of the search space explored without\nsacriﬁcing runtime, Kulikov et al. (2018) propose\nan iterative beam search method. Beam search\nis run many times, where the states explored by\nsubsequent beam searches are restricted based on\nthe intermediate states explored by previous itera-\ntions. Formally, we can deﬁne the set of all par-\ntial hypotheses for beam search instance iat time\nstep t as H(i)\nt . From here, the search space ex-\nplored by beam search instance ican be expressed\nas Si = ∪T\nt=1H(i)\nt . The ith beam search is pre-\n3755\nMethod Description Method Description\nRandom Sampling\nStandard decoding mechanism,\ngreedily samples a token from the\ndistribution at each time step.\nRandom Sampling\nwith Temperature\nBefore sampling, modify entropy\nof predicted distribution.\nTop-sRandom\nSampling\n(Fan et al., 2018)\nRestrict sampling to the s-most\nlikely words in the distribution.\n(story generation)\nBeam Search\nStandard decoding mechanism,\nkeeps the top bpartial hypotheses\nat every time step.\n(machine translation)\nNPAD Beam Search\n(Cho, 2016)\nAdd random noise to the hidden\nstate of the decoder at each time\nstep. (machine translation)\nTop-gCapping\nBeam Search\n(Li and Jurafsky, 2016)\nOnly consider the top c\nhypotheses from each parent\nhypothesis at each time step.\n(machine translation, dialog)\nHamming Diversity\nBeam Search\n(Vijayakumar et al.,\n2016)\nPenalize new hypotheses that have\nmany of the same tokens as\nexisting partial hypotheses.\n(image captioning)\nIterative Beam Search\n(Kulikov et al., 2018)\nRun beam search several times,\npreventing later iterations from\ngenerating intermediate states\nalready explored. (dialog)\nClustered Beam\nSearch\n(Tam et al., 2019)\nInitially consider more hypotheses\nat each time step, and then cluster\nsimilar hypotheses together.\n(dialog)\nPost-Decoding\nClustering (Ours)\nSample a large number of\ncandidates, and then cluster\nsimilar outputs together.\nTable 1: Brief high-level descriptions of each decoding method we consider in this paper. In parentheses we give\nthe applications on which the technique was originally applied.\nvented from generating any partial hypothesis that\nhas previously been generated, that is, any hypoth-\nesis found in S<i = ∪i−1\ni′=0Si′ .\nThe authors also attempt a soft inclusion cri-\nterion, where any states within ϵ Hamming dis-\ntance from a previously explored state are also\nexcluded. During the experimentation of Ku-\nlikov et al. (2018), however, the soft-inclusion was\nfound to not be beneﬁcial; thus, we only restrict\nexact matches of previous states in our implemen-\ntation. In practice, this means after the ﬁrst beam\nsearch instance runs as normal, the ﬁrst step of the\nsecond beam search instance will contain the b+1\nto 2b-most likely starting tokens; this pattern holds\nfor the third beam search instance, and so on.\nClustered Beam Search Most recently, Tam\net al. (2019) proposed a clustering-based beam\nsearch method to help condense and remove\nmeaningless responses from chatbots. Speciﬁ-\ncally, at each decoding stept, this method initially\nconsiders the top2∗bcandidates. From there, each\ncandidate sequence is embedded3, and the embed-\ndings are clustered into cclusters using K-means.\nFinally, we take the top b\nc candidates from each\ncluster. Note that in the case any clusters have size\nless than b\nc, we then include the highest-ranked\ncandidates not found after clustering.\n3We follow Tam et al. (2019) and used averaged GloVe\nword embeddings (Pennington et al., 2014).\n4 Clustering Post-Decoding (PDC)\nIn the previous section, we discuss several\ndiversity-promoting methods that can be applied\nduring the decoding process. However, it is also\npossible to encourage additional diversity post-\nhoc. On the task of sentence simpliﬁcation, after\ndecoding using a large-scale diversity-promoting\nbeam search (beam size 100), Kriz et al. (2019)\nthen clustered similar sentences together to fur-\nther increase the variety of simpliﬁcations from\nwhich to choose. Document embeddings gener-\nated via Paragraph Vector (Le and Mikolov, 2014)\nwere used as the sentence embeddings with which\nto perform K-means.\nIn this work, we extend this post-decoding clus-\ntering idea in three key ways. First, we make\nuse of sentence-level embeddings which lever-\nage the pre-trained language representations from\nthe Bidirectional Encoder Representations from\nTransformers (BERT) (Devlin et al., 2018).4 Sec-\nond, after clustering, Kriz et al. (2019) took the\nsentence closest to the centroid of each cluster as\nthe representative candidate; we instead choose\nthe highest-ranked candidate (according to log-\nlikelihood) from each cluster to ensure the best\ncandidates are still selected. Finally, after per-\nforming standard K-means clustering, we found\nthat it was often the case that some clusters con-\ntained large numbers of good candidates, while\nothers contained very few candidates that are also\n4BERT sentence-level embeddings were obtained using\nhttps://github.com/hanxiao/bert-as-service.\n3756\neither ungrammatical or otherwise inferior. Thus,\nin our implementation, we remove clusters con-\ntaining two or fewer sentences, and then sample a\nsecond candidate from each of the remaining clus-\nters, prioritizing selecting candidates from larger\nclusters ﬁrst.\n5 Experimental Setup\nWe evaluate the decoding strategies described in\nthe previous sections under the following settings.\nFor each of the published beam search algorithms,\nwe choose the hyperparameters that were found\nto be best in the original publications.\nRS Random sampling with temp = 0.5,\n0.7, 1.0, or 1.0 with top-10 capping.\nStandard BS Standard beam search\nTop5Cap BS Top- gcapping with g= 3\nIter5 BS Iterative beam search with 5 iterations\nHamDiv0.8 BS Hamming Diversity with λ= 0.8\nCluster5 BS Clustered beam search with 5 clusters\nNPAD0.3 BS Noisy Decoding with σ0 = 0.3\nFor random sampling, we sample 10 outputs,\nand with beam-search based methods, we use a\nbeam size of 10 to generate 10 outputs. In ad-\ndition, we show results from oversampling then\nﬁltering. We use a beam size of 100 or gener-\nate 100 samples through random sampling, and\nthen we select 10 from the 100, either through\npost-decoding clustering (PDC) or by taking the\n10 candidates with highest likelihood.\nWe examine these decoding strategies on two\ntasks: open ended dialog and image captioning.\nFor each task, we evaluate both the quality and di-\nversity of the 10 outputs from each strategy.\n5.1 Open-ended Dialog Task\nIn the dialog domain, we use an LSTM-based\nsequence-to-sequence (Seq2Seq) model imple-\nmented in the OpenNMT framework (Klein et al.,\n2017). We match the model architecture and train-\ning data of Baheti et al. (2018). The Seq2Seq\nmodel has four layers each in the encoder and de-\ncoder, with hidden size 1000, and was trained on\na cleaned version of OpenSubtitles (Tiedemann,\n2009) to predict the next utterance given the pre-\nvious one.\nEvaluation is performed on 100 prompts from\nthe Cornell Movie Dialog Corpus (Danescu-\nNiculescu-Mizil and Lee, 2011). These prompts\nare a subset of the 1000 prompts used in Baheti\net al. (2018), which were ﬁltered using item re-\nsponse theory for discriminative power.\nWe report perplexity (PpL), averaged over all\nthe top 10 outputs for each example. 5 Since\nthe quality of open-ended dialog is notoriously\ndifﬁcult to evaluate automatically, we ran a hu-\nman evaluation task on Amazon Mechanical Turk\nwhere annotators were shown a prompt and 5 po-\ntential responses generated by any of our decoding\nmethods. Evaluators were asked to provide binary\nratings on ﬂuency, adequacy, and interestingness\nfor each response. Overall, we collected 3 hu-\nman judgments for each of the top ten responses\nfor each of our decoding methods; in other words,\nwe collected 3,000 judgments per method.6\n5.2 Image Captioning Task\nFor image captioning, we use a state-of-the-\nart model introduced in Anderson et al. (2018).\nWe take advantage of Luo (2017)’s open-source\nimplementation and released model parameters\ntrained on MSCOCO (Lin et al., 2014). We evalu-\nate on a test set containing 5000 images.\nWe report Semantic Propositional Image Cap-\ntion Evaluation (SPICE) scores, an automatic eval-\nuation metric that has been shown to correlate\nwell with human judgments of quality(Anderson\net al., 2016). SPICE measures how well the se-\nmantic scene graph induced by the proposed cap-\ntion matches one induced by the ground truth. In\naddition to computing SPICE on the top-scoring\ncaption (SPICE@1), we follow Vijayakumar et al.\n(2016) in reporting Oracle SPICE@10 scores.\nThis is done to show the upper bound on the po-\ntential impact diversity can have. We also com-\npute the mean SPICE score across all of the candi-\ndate captions for an image. Unlike SPICE@1 and\nSPICE@10, this metric shows the overall quality\nof all of the candidate captions, which is useful to\nknow for applications that combine diverse candi-\ndate output sequences (Krause et al., 2017).\n5.3 Evaluating Diversity\nTo measure the diversity across the generated can-\ndidate sequences for a given input, we reportDist-\nk, the total number of distinct k-grams divided\nby the total number of produced tokens in all of\nthe candidate responses for a prompt (Li et al.,\n2016a). We report Dist-2 and Dist-4 averaged over\nthe prompts in the test set.\n5This differs from existing work which computes perplex-\nity over only the top output for each example. For our task we\nare interested in the quality of all of the generated responses.\n6The full instructions shown on AMT are in the appendix.\n3757\nMethod Fluency Adequacy Interestingness Ppl Dist-1 Dist-2 Ent-2 Ent-4\nReference 0.795 0.732 0.636 – – – – –\nRS 0.7 (sample 10) 0.758 0.399 0.388 35.98 0.63 0.80 4.08 3.84\nRS 1.0 (sample10) 0.550 0.303 0.386† 67.99 0.74 0.87 4.35 4.08\nRS 1.0,top10 (sample 10) 0.745† 0.418 0.387† 10.33 0.60 0.80 4.12 3.91\nStandard BS (10 beams) 0.950 0.621 0.336 4.01 0.37 0.45 3.16 3.01\nTop3Cap BS (10 beams) 0.942† 0.603 0.346 4.03 0.37 0.46 3.17 3.03\nIter5 BS (10 beams) 0.903 0.520 0.335 5.42 0.62 0.74 3.68 3.25\nHamDiv0.8 BS (10 beams) 0.923 0.599 0.366† 4.56 0.33 0.37 3.08 3.00\nCluster5 BS (10 beams) 0.936 0.582 0.381 4.23 0.39 0.46 3.24 3.06\nNPAD0.3 BS (10 beams) 0.942† 0.604† 0.335 4.05 0.36 0.44 3.13 2.99\nRS 1.0,top10 (sample 100, rank) 0.922 0.548 0.347 5.10 0.52 0.68 3.54 3.18\nRS 1.0,top10 (sample 100, PDC) 0.852 0.494 0.372 6.96 0.63 0.76 3.74 3.27\nStandard BS (100 beams, rank) 0.964 0.611 0.332† 4.01 0.44 0.61 3.33 3.05\nStandard BS (100 beams, PDC) 0.944 0.599 0.346 4.42 0.57 0.70 3.59 3.21\nTable 2: Results on 100 dialog prompts. The ﬁrst row shows the mean human ratings of the single reference\nresponse available for each prompt. The next three rows show results for random sampling, with 10 samples drawn\nper prompt. The next six rows are variants of beam search using beam size 10. The last four rows use random\nsampling or standard beam search to generate 100 outputs, then ﬁlter down to 10 outputs either through ranking\nby log-likelihood or by performing post-decoding clustering (PDC). In each section, the highest value is bolded,\nand statistical ties are marked †.\nSPICE\nMethod Mean @1 @10 Dist-1 Dist-2 Ent-2 Ent-4\nRS 0.7 (sample10) 0.170 0.192 0.278 0.31 0.52 3.67 4.00\nRS 1.0 (sample10) 0.133 0.167 0.247 0.44 0.71 4.17 4.26\nRS 1.0,top10 (sample10) 0.159 0.183 0.272 0.33 0.59 3.90 4.17\nStandard BS (10 beams) 0.194 0.193 0.283 0.18 0.26 2.94 3.18\nTop3Cap BS (10 beams) 0.195 0.196 0.282 0.17 0.26 2.93 3.17\nHamDiv0.8 BS (10 beams) 0.194 0.194 0.282 0.18 0.27 2.98 3.19\nCluster5 BS (10 beams) 0.191 0.194 0.285 0.19 0.28 3.04 3.25\nNPAD0.3 BS (10 beams) 0.191 0.192 0.280 0.18 0.26 2.94 3.17\nRS 1.0,top10 (sample100, rank) 0.182 0.188 0.284 0.25 0.41 3.31 3.64\nRS 1.0,top10 (sample100, PDC) 0.169 0.188 0.282 0.31 0.52 3.62 3.91\nStandard BS (100 beams, rank) 0.188 0.190 0.279 0.20 0.31 3.04 3.32\nStandard BS (100 beams, PDC) 0.186 0.192 0.288 0.24 0.38 3.25 3.57\nTable 3: Image captioning results for selected random sampling and beam search methods. SPICE@1 measures\nthe SPICE score of the most likely caption. SPICE@10 is the maximum score across the 10 candidates generated\nby each method. Mean SPICE is the mean score over all 10 candidates. In each section, the best value is bolded.\nA limitation of Dist-kis that allk-grams that ap-\npear at least once are weighted the same, ignoring\nthe fact that infrequentk-grams contribute more to\ndiversity than frequent ones. Zhang et al. (2018)\ninstead propose an entropy metric, Ent-k, deﬁned\nas:\nEnt-k = −1∑\nw∈SF(w)\n∑\nw∈S\nF(w) log F(w)∑\nw′∈SF(w′)\nwhere S is the set of all k-grams that appear in\ncandidate responses for an example, and F(w)\ndenotes the frequency of w in the candidate re-\nsponses.\n6 Results\nWe report results on dialog systems and image\ncaptioning in Tables 2 and 3, respectively. As ex-\npected, random sampling-based approaches yield\noutputs with greater diversity but worse qual-\nity than beam search-based approaches. Over-\nsampling then ﬁltering increases the quality of out-\nputs while still ensuring high diversity. In the fol-\nlowing sections, we discuss the diversity-quality\ntradeoff, and then delve further into the results for\neach method group.\n6.1 The Quality Diversity Tradeoff\nThe goal of diverse decoding strategies is to gen-\nerate high-quality candidate sequences which span\nas much of the space of valid outputs as possible.\n3758\nDist-2\nMean Score across Annotations\n0.25\n0.50\n0.75\n1.00\n0 0.25 0.5 0.75 1\nFluency Adequacy Interestingness\nDist-2 vs. Human Scores (corr = -0.41)\nPerplexity\nMean Score across Annotations\n0.25\n0.50\n0.75\n1.00\n3 3.25 3.5 3.75 4 4.25\nFluency Adequacy Interestingness\nEnt-4 vs. Human Scores (corr = -0.77)\nPerplexity\nMean Score across Annotations\n0.25\n0.50\n0.75\n1.00\n0 20 40 60\nFluency Adequacy Interestingness\nPerplexity vs. Human Scores (corr = -0.77)\nFigure 2: Each decoding strategy is plotted, showing that human-perceived quality is negatively correlated with\ndiversity. The Pearson Correlation coefﬁcients between each statistic and the average of ﬂuency, coherence, and\ninterestingness are shown in parentheses.\nPrompt: Look, nobody knows we did it.\nRS 0.5 Standard BS NPAD0.3 BS\nI don’t know what you’re talking about.\nWhat’s the matter with you?\nI don’t know what it is.\nI don’t think so.\nHe’s got to get out of here.\nWe’ve got to get out of here.\nWhat do you mean?\nI don’t think it’s a good idea.\nI don’t know what to say.\nI don’t know what’s going on.\nI don’t think it’s a good idea.\nI don’t know what to say.\nI don’t know what’s going on.\nI don’t know what to do.\nI don’t know what’s going on here.\nRS 1.0 Standard BS with PDC Cluster5 BS\nI can’t ﬁnd it.\nThey’re our ships.\nIt’s all right anyone is the right to interfere.\nWe didn’t have a plan I engineered a policy.\nSame time you pick us up at six and get we.\nI don’t know!\nI don’t think so.\nWhat do you mean?\nWhy didn’t you tell me?\nThat’s why we’re here.\nI don’t know why.\nWhat do you mean?\nI don’t think so.\nHow do you know that?\nI’ll tell you what.\nRS 1.0,top10 RS 1.0,top10 with PDC Top3Cap BS\nI don’t know what else to do.\nIt doesn’t have to be that way!\nWe’re in the air!\nI’ve seen a guy in his place in a it.\nAnd I’m not we any more.\nWhat do you mean?\nI don’t think so.\nThat’s why I’m here.\nIt’s all right we.\nWe’ve been through this before.\nWe’ve got to get out of here.\nWhat do you mean?\nI don’t think it’s a good idea.\nI don’t know what to say.\nI don’t know what’s going on.\nTable 4: Responses to an example prompt for selected methods. More examples can be seen in the appendix.\nHowever, we ﬁnd there to be a marked trade-off\nbetween diversity and quality. This can be seen in\nFigure 2, where we plot the human-judged quality\nscore for each dialog experiment against our pri-\nmary diversity descriptive statistics. Fluency and\nadequacy are both strongly negatively correlated\nwith diversity. While we had expected interest-\ningness to be positively correlated with diversity,\nthe fact that it is not suggests that existing diver-\nsity statistics are insufﬁcient for capturing what it\nmeans to humans for outcomes to be interesting.\nLikewise, in image captioning, the mean SPICE\nscore of the 10 candidate captions (averaged over\nall examples for each experimental setting) is\nstrongly anti-correlated with diversity, with a Pear-\nson correlation coefﬁcient of -0.83 with the Ent-4\nmeasure and -0.84 with Dist-2. Clearly it remains\nan open challenge to generate a diverse set of im-\nage captions that are all high-quality.\nWhen researchers choose to use a diverse de-\ncoding strategy, they must decide where on the\nquality-diversity tradeoff they would like to lie;\nselecting an optimal method depends strongly on\none’s tolerance for errors. In machine translation,\nwhere mistakes could severely impact coherence,\nbeam search-based methods, which tend to result\nin better ﬂuency and coherence, but worse diver-\nsity might be preferred. In more open-ended appli-\ncations, where novel text is of greater importance,\nincreased diversity could be worth the ﬂuency and\ncoherency hit. As state-of-the-art models continue\nto improve, one would hope that the quality cost of\n3759\nencouraging diversity will continue to decrease.\nIn the interest of reporting a single overall\nbest method for each task, we computed a sum-\nof-ranks score for each method. For dialog,\nwe ranked the methods each by ﬂuency, coher-\nence, interestingness, and Ent-4, and then took\na weighted sum of the four ranks, with 50% of\nthe weight assigned to Ent-4, and 50% distributed\nevenly among the human evaluation ranks. Over-\nall, clustered beam search and standard BS (beam\nsize 100, PDC) have the best scores, followed by\nclustered beam search (beam size 10). Similarly,\nfor image captioning, we rank the methods by their\nmean SPICE score and by Ent-4. Summing these\nranks, random sampling (temp 1.0, top-10 cap-\nping, PDC) came in ﬁrst. Standard beam search,\nHamming Diversity beam search, and Top- g cap-\nping beam search (beam size 10) tied for second.\n6.2 Random Sampling-based Methods\nHigher sampling temperatures result in both an in-\ncrease in diversity in generated responses and a re-\nduction in overall quality. In the dialog domain,\nevaluators consistently rate the responses sampled\nwith temperature 1.0 to have worse ﬂuency, co-\nherence, and interestingness when those sampled\nwith temperature 0.5. In the image captioning do-\nmain, lower temperature improves automatic eval-\nuation metrics for quality while reducing diversity.\nFor dialog, restricting sampling to the top-10\nvocabulary words is a more effective strategy than\nadjusting temperature for ensuring balance be-\ntween the quality and diversity of outputs. Top-\n10 random sampling has the highest ﬂuency, co-\nherence, and interestingness, as well as signiﬁ-\ncantly lower perplexity than other random sam-\npling methods. However, this trend did not ex-\ntend to image captioning, where top-10 random\nsampling results in both worse SPICE scores and\nlower diversity measures than setting the temper-\nature to 0.7. This may be because image caption-\ning is a less ambiguous task than open-ended di-\nalog, leading to a better-trained model that puts\nmore probability mass on high-quality vocabulary\nwords, ameliorating the challenge top- c ﬁltering\nis designed to eliminate: that of a long tail of\nlow probability vocabulary words taking up a large\namount of probability mass.\n6.3 Beam Search-based Methods\nFor dialog, clustered beam search (Cluster5 BS)\nperforms the best of all beam search methods in\nterms of human-judged interestingness. It ties for\nbest with NPAD0.3BS on ﬂuency and ties with\nStandard BS on coherence. Iterative beam search\n(Iter5 BS) achieves the greatest diversity, but at\nthe expensive of quality. It has the lowest human-\njudged coherence among beam search methods;\nthus, we do not evaluate this method on image cap-\ntioning. For image captioning, Cluster5 BS has the\nhighest diversity among beam search methods, but\nthis difference is quite small. Cluster5 BS also has\nthe highest SPICE@10 score, indicating it is the\nbest method for generating at least one high qual-\nity candidate. However, Top3Cap BS results in the\nhighest mean SPICE score, suggesting it is best at\nensuring all outputs are reasonable quality.\n6.4 Effect of Over-sampling\nIn our experiments, we explore over-sampling 100\noutputs, and then either using post-decoding clus-\ntering (PDC) or re-ranking by log-likelihood to ﬁl-\nter these 100 down to 10 diverse outputs.\nIn the dialog domain, this over-sampling ap-\nproach is a deﬁnite win. When over-sampling with\nrandom sampling both methods of ﬁltering sub-\nstantially improve human judgements of ﬂuency\nand adequacy compared to random sampling only\n10 outputs. However, interestingness scores go\ndown, and while the outputs are still more diverse\nthan beam search-based methods, they are less di-\nverse than random sampling without ﬁltering. In\nthe beam search methods that use a beam size of\n100 then ﬁlter down to 10, human-judged quality\nis on par with beam size 10 results, but diversity is\nconsiderably higher.\nWhen comparing the two types of ﬁltering,\nPDC results in higher interestingness and diver-\nsity statistics, while log-likelihood re-ranking im-\nproves ﬂuency and adequacy. This again demon-\nstrates the trade-off between quality and diversity.7\nFor image captioning, over-sampling with\nreranking does not consistently improve quality as\nit does in the dialog domain. Mean SPICE score\nis improved for random sampling but not for beam\nsearch. SPICE@1 becomes worse for both ran-\ndom sampling and decoding, while SPICE@10\nimproves for random sampling, and for beam\nsearch when PDC is applied. From these results,\n7In the appendix, we show results with every method\nwhere we generate 10 samples; generate 100 samples fol-\nlowed by selecting the 10 most likely outputs; and generate\n100 samples followed by post-decoding clustering to select\n10 outputs.\n3760\nwe can conclude that over-sampling then ranking\ndoes not have a sizeable effect, either negative or\npositive, on quality. Moreover, the diversity of\nthe captions generated by random sampling actu-\nally decreases when oversampling. The diversity\nof beam search-generated captions does improve\nwith over-sampling.\nWhile oversampling does generally improve\noutcomes on the diversity/quality tradeoff, it is\nmore computationally expensive, particularly with\nbeam search. Running PDC also requires generat-\ning sentence embeddings for every output, which\nadds additional computation time.\n7 Additional Related Work\nIn this paper, we have compared a variety of post-\ntraining diversity-promoting algorithms. Here, we\ndiscuss other related works that instead promote\ndiversity at train-time, as well as alternative qual-\nity evaluation methods. We also note that concur-\nrent work has proposed nucleus sampling as an im-\nprovement to the sampling strategies discussed in\nthis paper (Holtzman et al., 2019).\nDiversity Promotion During Training Sev-\neral works have attempted to encourage diver-\nsity during training by replacing the standard log-\nlikelihood loss with a diversity-promoting objec-\ntive. Li et al. (2016a) introduces an objective that\nmaximizes mutual information between the source\nand target. Zhang et al. (2018) uses an adversar-\nial information maximization approach to encour-\nage generated text to be simultaneously informa-\ntive and diverse. Xu et al. (2018) also uses an\nadversarial loss; their loss function rewards ﬂu-\nent text and penalizes repetitive text. We do not\nevaluate on these methods as they tend to be task-\nspeciﬁc and difﬁcult to implement. All of the di-\nversity strategies we evaluate share the trait that\nthey are agnostic to model architecture and to the\ndata type of the input, as long as the output of the\nmodel is a probability distribution over tokens in a\nsequence.\nAutomatic Quality Evaluation An impor-\ntant part of this work is how to accurately mea-\nsure not only the effect these methods have on\ncandidate diversity, but also on the overall qual-\nity of the candidates. In choosing to report hu-\nman scores and perplexity for the dialog domain,\nand SPICE for image captioning, we omitted some\nquality measures used in other papers.\nFor image captioning, BLEU (Papineni et al.,\n2001), ROUGE (Lin, 2004), METEOR (Elliott\nand Keller, 2013), and CIDer (Vedantam et al.,\n2015) scores are often reported, but SPICE has\nbeen shown to have higher correlation with hu-\nman judgments (Anderson et al., 2016). In the di-\nalog domain, single-reference BLEU score (Pap-\nineni et al., 2001) is sometimes used to measure re-\nsponse quality, but it has been shown to have little\ncorrelation with human-judged quality (Liu et al.,\n2016). Therefore, most works in dialog systems\nuse human evaluation as the ultimate measure of\nquality (Li et al., 2016a; Sedoc et al., 2018)\n8 Conclusion\nIn this work, we perform an analysis of post-\ntraining decoding strategies that attempt to pro-\nmote diversity in conditional language models.\nWe show how over-sampling outputs then ﬁlter-\ning down to the desired number is an easy way\nto increase diversity. Due to the computational\nexpense of running large beam searches, we rec-\nommend using random-sampling to over-sample.\nThe relative effectiveness of the various decoding\nstrategies differs for the two tasks we considered,\nwhich suggests that choice of optimal diverse de-\ncoding strategy is both task-speciﬁc and depen-\ndent on one’s tolerance for lower quality outputs.\nWhile we have focused on evaluating each de-\ncoding strategy under the speciﬁcs reported to be\nthe best in the original, further work is necessary\nto conclude whether observed differences in qual-\nity and diversity may simply be due to each work’s\nchosen hyperparameters. The ability to effectively\ngenerate a diverse set of responses while not de-\ngrading quality is extremely important in a variety\nof generation tasks, and is a crucial component to\nharnessing the power of state-of-the-art generative\nmodels.\n9 Acknowledgements\nWe thank our anonymous reviewers for helpful\nfeedback. We also thank Yun William Yu for as-\nsistance with statistical testing and proofreading.\nThis material is based in part on research spon-\nsored by DARPA under grant number HR0011-\n15-C-0115 (LORELEI). The U.S. Government is\nauthorized to reproduce and distribute reprints for\nGovernmental purposes. The views and conclu-\nsions in this publication are those of the authors\nand should not be seen as representing ofﬁcial en-\ndorsements of DARPA and the U.S. Government.\n3761\nReferences\nPeter Anderson, Basura Fernando, Mark Johnson, and\nStephen Gould. 2016. Spice: Semantic proposi-\ntional image caption evaluation. In European Con-\nference on Computer Vision.\nPeter Anderson, Xiaodong He, Chris Buehler, Damien\nTeney, Mark Johnson, Stephen Gould, and Lei\nZhang. 2018. Bottom-up and top-down attention for\nimage captioning and visual question answering. In\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 6077–6086.\nAshutosh Baheti, Alan Ritter, Jiwei Li, and Bill Dolan.\n2018. Generating more interesting responses in\nneural conversation models with distributional con-\nstraints. In Conference on Empirical Methods in\nNatural Language Processing (EMNLP 2018).\nKyunghyun Cho. 2016. Noisy parallel approximate de-\ncoding for conditional recurrent language model.\nSajal Choudhary, Prerna Srivastava, Lyle H. Ungar, and\nJo˜ao Sedoc. 2017. Domain aware neural dialog sys-\ntem. volume abs/1708.00897.\nElizabeth Clark, Anne Spencer Ross, Chenhao Tan,\nYangfeng Ji, and Noah A. Smith. 2018. Creative\nwriting with a machine in the loop: Case studies on\nslogans and stories. In 23rd International Confer-\nence on Intelligent User Interfaces, IUI ’18, pages\n329–340, New York, NY , USA. ACM.\nCristian Danescu-Niculescu-Mizil and Lillian Lee.\n2011. Chameleons in imagined conversations: A\nnew approach to understanding coordination of lin-\nguistic style in dialogs. In Proceedings of the 2nd\nWorkshop on Cognitive Modeling and Computa-\ntional Linguistics, pages 76–87, Portland, Oregon,\nUSA. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, , and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning.\nDesmond Elliott and Frank Keller. 2013. Image de-\nscription using visual dependency representations.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1292–1302, Seattle, Washington, USA. Association\nfor Computational Linguistics.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018.\nHierarchical neural story generation. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 889–898, Melbourne, Australia. Asso-\nciation for Computational Linguistics.\nKevin Gimpel, Dhruv Batra, Chris Dyer, and Gregory\nShakhnarovich. 2013. A systematic exploration of\ndiversity in machine translation. In Proceedings of\nthe 2013 Conference on Empirical Methods in Nat-\nural Language Processing, pages 1100–1111.\nJiatao Gu, Kyunghyun Cho, and Victor O.K. Li. 2017.\nTrainable greedy decoding for neural machine trans-\nlation. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 1968–1978, Copenhagen, Denmark. As-\nsociation for Computational Linguistics.\nTatsunori B. Hashimoto, Hugh Zhang, and Percy\nLiang. 2019. Unifying human and statistical eval-\nuation for natural language generation. CoRR,\nabs/1904.02792.\nAri Holtzman, Jan Buys, Maxwell Forbes, and Yejin\nChoi. 2019. The curious case of neural text degen-\neration. CoRR, abs/1904.09751.\nGuillaume Klein, Yoon Kim, Yuntian Deng, Jean\nSenellart, and Alexander M. Rush. 2017. Open-\nNMT: Open-source toolkit for neural machine trans-\nlation. In Proc. ACL.\nJonathan Krause, Justin Johnson, Ranjay Krishna, and\nLi Fei-Fei. 2017. A hierarchical approach for gen-\nerating descriptive image paragraphs. In Computer\nVision and Pattern Recognition (CVPR), 2017 IEEE\nConference on, pages 3337–3345. IEEE.\nReno Kriz, Jo˜ao Sedoc, Marianna Apidianaki, Carolina\nZheng, Gaurav Kumar, Eleni Miltsakaki, and Chris\nCallison-Burch. 2019. Complexity-weighted loss\nand diverse reranking for sentence simpliﬁcation.\nIlya Kulikov, Alexander H Miller, Kyunghyun Cho,\nand Jason Weston. 2018. Importance of a search\nstrategy in neural dialogue modelling.\nQuoc Le and Tomas Mikolov. 2014. Distributed repre-\nsentations of sentences and documents. In Proceed-\nings of the 31st International Conference on Inter-\nnational Conference on Machine Learning - Volume\n32, ICML’14, pages 1188–1196.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016a. A diversity-promoting ob-\njective function for neural conversation models. In\nProceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 110–119, San Diego, California. Association\nfor Computational Linguistics.\nJiwei Li and Dan Jurafsky. 2016. Mutual information\nand diverse decoding improve neural machine trans-\nlation.\nJiwei Li, Will Monroe, and Dan Jurafsky. 2016b. A\nsimple, fast diverse decoding algorithm for neural\ngeneration.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summa-\nrization Branches Out: Proceedings of the ACL-04\nWorkshop, pages 74–81, Barcelona, Spain. Associa-\ntion for Computational Linguistics.\n3762\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar,\nand C Lawrence Zitnick. 2014. Microsoft coco:\nCommon objects in context. In European confer-\nence on computer vision, pages 740–755. Springer.\nChia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Nose-\nworthy, Laurent Charlin, and Joelle Pineau. 2016.\nHow NOT to evaluate your dialogue system: An\nempirical study of unsupervised evaluation metrics\nfor dialogue response generation. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2122–2132, Austin,\nTexas. Association for Computational Linguistics.\nRuotian Luo. 2017. An image captioning code-\nbase in pytorch. https://github.com/\nruotianluo/ImageCaptioning.pytorch.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015. Effective Approaches to Attention-\nbased Neural Machine Translation. In Proceed-\nings of the 2015 Conference on Empirical Methods\nin Natural Language Processing, pages 1412–1421,\nLisbon, Portugal.\nRamesh Nallapati, Bowen Zhou, C ´ıcero Nogueira dos\nSantos, aglar G ¨ulehre, and Bing Xiang. 2016. Ab-\nstractive Text Summarization using Sequence-to-\nsequence RNNs and Beyond. In Proceedings of The\n20th SIGNLL Conference on Computational Natural\nLanguage Learning (CoNLL), pages 280–290.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2001. Bleu: a method for automatic eval-\nuation of machine translation. In Association for\nComputational Linguistics.\nJeffrey Pennington, Richard Socher, and Christo-\npher D. Manning. 2014. Glove: Global vectors for\nword representation. In Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 1532–\n1543.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1:8.\nJo˜ao Sedoc, Daphne Ippolito, Arun Kirubarajan, Jai\nThirani, Lyle Ungar, and Chris Callison-Burch.\n2018. Chateval: A tool for the systematic evalua-\ntion of chatbots. In Proceedings of the Workshop on\nIntelligent Interactive Systems and Language Gen-\neration (2IS&NLG), pages 42–44. Association for\nComputational Linguistics.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural net-\nworks. In Z. Ghahramani, M. Welling, C. Cortes,\nN. D. Lawrence, and K. Q. Weinberger, editors, Ad-\nvances in Neural Information Processing Systems\n27, pages 3104–3112. Curran Associates, Inc.\nYik-Cheung Tam, Jiachen Ding, Cheng Niu, and Jie\nZhou. 2019. Cluster-based beam search for pointer-\ngenerator chatbot grounded by knowledge. In Dia-\nlog System Technology Challenges 7 at AAAI 2019.\nJ¨org Tiedemann. 2009. News from opus-a collection\nof multilingual parallel corpora with tools and inter-\nfaces. In Recent advances in natural language pro-\ncessing, volume 5, pages 237–248.\nRamakrishna Vedantam, C. Lawrence Zitnick, and\nDevi Parikh. 2015. Cider: Consensus-based image\ndescription evaluation. pages 4566–4575.\nAshwin K Vijayakumar, Michael Cogswell, Ram-\nprasath R Selvaraju, Qing Sun, Stefan Lee, David\nCrandall, and Dhruv Batra. 2016. Diverse beam\nsearch: Decoding diverse solutions from neural se-\nquence models.\nOriol Vinyals and Quoc V . Le. 2015. A neural conver-\nsational model. volume abs/1506.05869.\nJingjing Xu, Xuancheng Ren, Junyang Lin, and\nXu Sun. 2018. Diversity-promoting gan: A cross-\nentropy based generative adversarial network for di-\nversiﬁed text generation. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 3940–3949.\nYizhe Zhang, Michel Galley, Jianfeng Gao, Zhe Gan,\nXiujun Li, Chris Brockett, and Bill Dolan. 2018.\nGenerating informative and diverse conversational\nresponses via adversarial information maximization.\nIn Advances in Neural Information Processing Sys-\ntems, pages 1815–1825."
}