{
  "title": "HPC-Coder: Modeling Parallel Programs using Large Language Models",
  "url": "https://openalex.org/W4383046408",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3163821903",
      "name": "Nichols, Daniel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3193622540",
      "name": "Marathe, Aniruddha",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2742724862",
      "name": "Menon, Harshitha",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2151002341",
      "name": "Gamblin, Todd",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743400547",
      "name": "Bhatele Abhinav",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3116350821",
    "https://openalex.org/W4220908681",
    "https://openalex.org/W4281619052",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3212496002",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4393510453",
    "https://openalex.org/W4283816733",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2979792666",
    "https://openalex.org/W4286530329",
    "https://openalex.org/W4311653698",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2964150020",
    "https://openalex.org/W2972087877",
    "https://openalex.org/W3174394143",
    "https://openalex.org/W4386185625",
    "https://openalex.org/W4388556611",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W4308642031",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W2340816002",
    "https://openalex.org/W4313185449",
    "https://openalex.org/W3179008990",
    "https://openalex.org/W4302797994",
    "https://openalex.org/W4376167329",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W2067673417",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4382704594",
    "https://openalex.org/W2768282280",
    "https://openalex.org/W4398768423",
    "https://openalex.org/W3034689979",
    "https://openalex.org/W4311887664",
    "https://openalex.org/W4389364446",
    "https://openalex.org/W4362659486"
  ],
  "abstract": "Parallel programs in high performance computing (HPC) continue to grow in complexity and scale in the exascale era. The diversity in hardware and parallel programming models make developing, optimizing, and maintaining parallel software even more burdensome for developers. One way to alleviate some of these burdens is with automated development and analysis tools. Such tools can perform complex and/or remedial tasks for developers that increase their productivity and decrease the chance for error. Until recently, such tools for code development and performance analysis have been limited in the complexity of tasks they can perform, especially for parallel programs. However, with recent advancements in language modeling, and the availability of large amounts of open-source code related data, these tools have started to utilize predictive language models to automate more complex tasks. In this paper, we show how large language models (LLMs) can be applied to tasks specific to high performance and scientific codes. We introduce a new dataset of HPC and scientific codes and use it to fine-tune several pre-trained models. We compare several pre-trained LLMs on HPC-related tasks and introduce a new model, HPC-Coder, fine-tuned on parallel codes. In our experiments, we show that this model can auto-complete HPC functions where generic models cannot, decorate for loops with OpenMP pragmas, and model performance changes in scientific application repositories as well as programming competition solutions.",
  "full_text": "HPC-Coder: Modeling Parallel Programs using\nLarge Language Models\nDaniel Nichols†, Aniruddha Marathe ∗, Harshitha Menon ∗, Todd Gamblin‡, Abhinav Bhatele †\n†Department of Computer Science, University of Maryland, College Park, MD, USA\n∗Center for Applied Scientific Computing, Lawrence Livermore National Laboratory, Livermore, CA, USA\n‡Livermore Computing, Lawrence Livermore National Laboratory, Livermore, CA, USA\nEmail: dnicho@umd.edu, {marathe1, gopalakrishn1, tgamblin }@llnl.gov, bhatele@cs.umd.edu\nAbstract—Parallel programs in high performance computing\n(HPC) continue to grow in complexity and scale in the exascale\nera. The diversity in hardware and parallel programming models\nmake developing, optimizing, and maintaining parallel software\neven more burdensome for developers. One way to alleviate some\nof these burdens is with automated development and analysis\ntools. Such tools can perform complex and/or remedial tasks\nfor developers that increase their productivity and decrease the\nchance for error. Until recently, such tools for code development\nand performance analysis have been limited in the complexity\nof tasks they can perform, especially for parallel programs.\nHowever, with recent advancements in language modeling, and\nthe availability of large amounts of open-source code related data,\nthese tools have started to utilize predictive language models\nto automate more complex tasks. In this paper, we show how\nlarge language models (LLMs) can be applied to tasks specific\nto high performance and scientific codes. We introduce a new\ndataset of HPC and scientific codes and use it to fine-tune several\npre-trained models. We compare several pre-trained LLMs on\nHPC-related tasks and introduce a new model, HPC-Coder, fine-\ntuned on parallel codes. In our experiments, we show that this\nmodel can auto-complete HPC functions where generic models\ncannot, decorate for loops with OpenMP pragmas, and model\nperformance changes in scientific application repositories as well\nas programming competition solutions.\nIndex Terms—large language models, parallel code generation,\nperformance modeling\nI. I NTRODUCTION\nIn recent years, large language models (LLMs) have become\nthe state of the art for many language modeling related\ntasks [1]. Their ability to model token probabilities within\na sequential context make them desirable for language tasks\nsuch as text generation and sequence classification. In addition\nto being used for natural language, such models have recently\nbeen applied to many programming language related tasks [2]–\n[4]. The predictive capabilities of these models translate well\nto coding tasks, and the wealth of open-source code available\nonline provides significant data for training large models.\nLLMs trained on source code data have been utilized to\nautomate numerous software development tasks such as code\ncompletion, malware detection, code refactoring, etc [3]–[12].\nAdditionally, they have been able to automate tasks previously\nconsidered impossible to automate such as code summariza-\ntion and generation using natural language. Training LLMs\nfor these tasks requires significant amounts of source code\ndata that is fortunately available online from open-source\ncode repositories on GitHub, gitlab etc. However, this data\nrequirement for training LLMs is prohibitive for tasks where\nsuch data may not exist. One such task is that of modeling\nperformance (execution time) based on source code. Another\ndifficult task is modeling parallel and HPC code where there\nis less data available and it is often more complex code.\nPerformance data for arbitrary code is difficult to obtain at\nscale with large numbers of samples. First and foremost, it\nis non-trivial to automate the collection of performance data\nfor arbitrary source code. The code needs to be built and\nrun in order to measure performance, and this process can\nvary significantly across repositories. This can be particularly\ndifficult for production scientific codes due to code complexity,\ndependence on external libraries, and the fact that it often\nneeds to be run in parallel with many resources. Second,\nperformance depends on numerous variables besides just the\ncode such as input problem, architecture, and current machine\nload/congestion. These either need to be fixed in the dataset\nor accounted for within the modeling pipeline. Finally, source\ncode needs to be considered holistically when modeling per-\nformance, since minor changes in one place may drastically\nimpact performance elsewhere. For example, changing the data\nlayout within a data structure will impact the performance of\ndata access where that structure is used. This means that the\nentirety of the source code needs to be included in the dataset\nand performance needs to be collected at a finer granularity.\nWhen a lack of data becomes a hurdle in machine learning\ntasks, it is typically solved through data augmentation and/or\ntransfer learning. Data augmentation involves extending and/or\nduplicating data in a manner that still preserves meaning\nand representational capacity. Transfer learning is done by\nfirst training a model on a related or simpler task and then\ntransferring that knowledge to a new problem requiring fewer\nsamples to learn. For our task we employ transfer learning\nby using LLMs that have learned to model source code and\nthen transferring that knowledge to then learn how to model\nperformance of source code using fewer samples. In particular,\nwe explore modeling parallel and HPC codes.\nIn this paper, we utilize LLMs to model high performance\nand scientific codes, and then apply that to the problem\nof performance modeling. In order to accomplish this, we\narXiv:2306.17281v2  [cs.DC]  14 May 2024\nintroduce a new dataset of HPC and scientific codes from\npopular open-source repositories. We first demonstrate how\nour trained model, HPC-Coder, outperforms other LLMs on\nHPC specific tasks such as code generation and OpenMP\npragma labeling. A set of code generation tests specific to\nHPC are introduced and the model can pass these at up to\n53% higher rate than the other models. Additionally, it is able\nto label for loops with OpenMP pragmas with 97% accuracy.\nFinally, we demonstrate how the model can predict relative\nperformance of source code changes with up to 92% accuracy.\nIn summary, this paper makes the following contributions:\n• A large curated dataset containing HPC and scientific\ncode from numerous open-source repositories.\n• We present an LLM, HPC-Coder, fine-tuned to model\nHPC and scientific code. We show that it trains to better\nlanguage modeling scores over HPC related code than\nother state-of-the-art models.\n• We introduce a set of HPC code generation tasks and\ndemonstrate that our model completes these tasks at\na significantly better rate than other models on HPC-\nspecific code.\n• We demonstrate how our model can be used to predict\nOpenMP pragmas with high accuracy.\n• We utilize our model to predict relative performance\nof source code changes for two distinct datasets from\nscientific application repositories and coding competition\nsolutions.\nII. B ACKGROUND\nThis section provides background on transformer-based\nlanguage models and how they can be applied to source code.\nA. Large Language Models\nWhen applying machine learning to textual data we need\na model that takes text as input and, through the process of\ntraining on previous data, learns how to predict some property\nof that text. In recent years such models have been mostly\ndominated by large transformer-based models. Transformers\nwere first introduced by Vaswani et al. [13]. They are designed\nto work with sequential data much like recurrent and long\nshort-term memory neural networks. However, they differ in\ntheir use of a self-attention mechanism to attribute importance\nweights to inputs into the model. Due to this mechanism\ntransformers also process entire sequences at once unlike\nrecurrent neural networks.\nThese self-attention units make up the basis of transformer\nnetworks. Weights are divided into query, key, and value\nweights (namely WQ, WK, WV ). These are multiplied by\neach input token i and stacked to form the matrices Q, K,\nand V , respectively. Given these matrices and the dimensions\nof the key vector dk the attention can be computed as below.\nAttention (Q, K, V) =softmax\n\u0012QKT\n√dk\n\u0013\nV\nThese weight matrices form a single attention head. Typ-\nically transformers employ several attention heads to form\na multi-attention head layer. Having multiple attention heads\nallows each of them to learn, or attend to, different abstractions\nin the input, such as parts-of-speech for natural language input.\nGenerally these networks are trained to model the condi-\ntional probability of observing a language token or a sequence\nof tokens. For instance, given a string of observed tokens\nt1t2 . . . ti−1 we may want the most likely next token ti.\nti = arg max\nt\nP (ti = t | t1t2 . . . ti−1)\nSimilarly we may want to know the probability of a se-\nquence of tokens occurring given the entire observed dataset\nP (t1, t2, . . . , tN ) (i.e. how likely is a given english sentence to\nbe real given my previous knowledge of the language). Using\nthis probability we can define a metric called perplexity.\nPerplexity(T) =\n\u0012 1\nP(t1, t2, . . . , tN )\n\u0013 1\nN\nWith this metric a model that scores a lower perplexity on\nits test set T is better as it assigns a higher probability to the\ntest data. The ratio is normalized to be invariant to the size of\nthe test set. Rewriting the formula for perplexity we can see\nthat it is equivalent to the exponential of the cross-entropy.\nPerplexity(T) = (P(t1, t2, . . . , tN ))− 1\nN\n= (exp logP(t1, t2, . . . , tN ))− 1\nN\n= exp\n\u0012\n− 1\nN log P(t1, t2, . . . , tN )\n\u0013\nThis allows us to train the language model with cross-\nentropy loss. Minimizing the loss will, in turn, minimize the\nperplexity. The perplexity is recovered by simply taking the\nexponential of the loss. It is important to note that perplexity\nmeasures model confidence and not accuracy. However, it has\nbeen demonstrated empirically that lower perplexity generally\nleads to better performance on downstream tasks.\nB. Text Generation\nA trained model can then be used to generate new text. Since\nthe LLM models token probability it may seem simple to select\nthe most probable next token, however, this can lead to poor\ntext generation. Often a model’s attention puts more focus on\non the most recent tokens causing this selection method to get\nstuck in loops or suddenly forget context. Most recent works\ncombat this issue by sampling from the model’s distribution,\nbut there are several important caveats when doing this. For\ninstance, we want to avoid sampling from the tail as this could\ndrastically throw off further tokens sampled. Here we discuss\nseveral of the sampling methods used later in this paper such\nas temperature, top- k, and nucleus sampling.\nTemperature: When sampling temperature controls how con-\nfident the model is in the sampled token. Lower temperature\nleads the model to assign more confidence in the most likely\ntokens in the distribution. On the other end, the model will\nmore uniformly assign confidence across the distribution when\nthe temperature is higher. This term comes from statistical\nthermodynamics where lower energy states are more frequent\nwith a higher temperature.\nTemperature is incorporated by dividing the logits by the\ntemperature, temp, before computing the softmax output. The\nlogits are the raw, un-normalized outputs of the model and the\nsoftmax is used to turn this vector into probabilities.\nsoftmax\n\u0012logits\ntemp\n\u0013\nThus, as temp → 0 the output becomes the argmax and as\ntemp → ∞it leads to a uniform sampling.\nTop-k Sampling: In top-k sampling the most likely k tokens\nare sampled from the model. This aims to exclude the distribu-\ntion’s tail and prevent the model from rapidly getting off-topic.\nHowever, this can also reduce the quality of predictions if the\nbody of the distribution is wider than k. A common choice\nfor k is 50.\nNucleus Sampling: Nucleus, or top-p, sampling aims to solve\nthe shortcomings of top- k sampling by choosing a more\nmeaningful cut-off point. In this method the CDF of the\ndistribution is computed and sampling is cut-off when the CDF\nexceeds p. A common choice for p is 0.9.\nC. Using LLMs for Code Generation\nLLMs can be trained on a variety of downstream tasks and\nobjectives. When applied to source code data they are typically\ntrained as left-to-right, masked, or encoder-decoder models.\nLeft-to-Right: Left-to-right or causal language models are\ntrained to predict the most probable next token in a sequence.\nThe model receives and generates text in a left-to-right fashion,\nwhich is where it gets its name. This limits the amount of\ncontext the model can see as it cannot use later tokens in its\nprediction even if they are present in the data. Left-to-right\nmodels are useful for text generation related tasks.\nMasked: Unlike left-to-right models, masked models can\npredict the most probable token for any position in the text.\nAfter removing random tokens in the samples and replacing\nthem with mask tokens, the model is trained to predict the\nmost probable tokens to replace the masks with. In this\nconfiguration masked models can make use of more context\nin their predictions.\nEncoder-Decoder: Another common approach is to train a\nleft-to-right model to decode a sequence after it has been\npassed through an encoder. This type of model can be com-\nbined with several different objectives and is often used with\nsequence-to-sequence prediction.\nTo apply left-to-right models, which are focused on in this\npaper, to source code you simply need to provide the model\nwith prior context as a sequence of tokens and then let it\ngenerate new tokens until some stopping threshold. The prior\ncontext is typically a natural language comment followed by\na function declaration. Tokens are then generated until the\nfunction is complete (a closing } bracket in the case of C/C++).\nAdditionally, when applying language models to code it is\ntypical to customize the training process slightly to take ad-\nvantage of the syntactic differences between natural language\nand code. For instance, the tokenizer, which is responsible for\nmapping text to a sequence of integers, is often set to group\nwhitespace into single tokens. This is not necessary in natural\nlanguage inputs as multiple consecutive spaces are uncommon.\nHowever, in code this can meaningfully reduce the sequence\nsize and a formatter can be applied after code generation to\nregain formatting.\nIII. O VERVIEW OF THE PROPOSED METHODOLOGY\nFigure 1 provides an overview of the data gathering, train-\ning, and downstream application in this paper. In order to train\na large HPC-specific language model we need a large dataset\nof HPC code. To obtain this, we gather a dataset of HPC\nsource code and use it to fine-tune a pre-trained language\nmodel. This data gathering is described in Section IV and\npresents what HPC sources are used and how they are pre-\nprocessed. Following this, the model fine-tuning and selection\nare detailed in Section V where we explain the training setup\nand methodology.\nFig. 1. Overview of the steps described in this paper to train an HPC specific\nmodel and run it on several downstream tasks. After collecting a large dataset\nof HPC code we fine-tune several pre-trained language models and select the\nbest one. The selected model is then used to generate code, label OpenMP\npragmas, and predict relative performance as part of several downstream tasks.\nWe need several realistic tests to study the performance of\nthe language model on relevant metrics. We present three main\ndownstream tasks for evaluation in Section VI. The first two,\ncode generation and OpenMP pragma labeling, test the model\non its ability to generate correct and meaningful code. The last\ntest, relative performance prediction, shows how this trained\nmodel can be used for useful tasks that require language\ncomprehension. Results from each of these tests are presented\nand discussed in Section VII.\nIV. D ATA GATHERING AND PRE-PROCESSING\nIn order to train a large language model to understand and\ngenerate HPC code, we need to show it lots of examples. We\nmust first build a dataset to accomplish this. In this section,\nwe detail our collected dataset and how it is processed. We\npresent two additional code datasets paired with performance\ndata for further fine-tuning model performance.\nA. HPC Source Code Data\nWe first collect a sufficiently large dataset of source code to\ntrain the model on HPC and scientific code. The HPC source\ndataset is collected from GitHub repositories. The source files\nare pulled from repositories with C/C++ marked as the primary\nlanguage and with ≥ 3 stars. The repositories are additionally\nfiltered by HPC related GitHub topics. Once cloned, we collect\nall the C/C++ source files based on their file extension.\nThis dataset is collected and structured in the same manner\nas the C/C++ source dataset from Xu et al. [14]. Their dataset\nis scraped from GitHub in a similar manner with the exception\nof only including repositories with ≥ 5 stars. Figure 2 shows\nthe distribution of lines of code (LOC) by file types in the\nHPC source dataset. There are roughly the same number of\nLOC in both C and C++ files. The distribution of actual file\ncounts follows the same trend.\n.c .h .cc .cpp .hpp .C\n0\n5\n10\n15\n20\n25\n30Lines of Code (in millions)\n24\n8\n0.9\n17\n2 0.9\nDistribution of Lines-of-Code by File Type\nFig. 2. Distribution of no. of lines of code in each file type. .cxx, .hh, .H, and\n.hxx files are included in the dataset, but omitted here due to small counts.\nB. Data Pre-processing\nAllamanis [15] shows how duplicate source data, which\nis prevalent across GitHub repositories, can adversely bias\nLLMs during training. To prevent this we filter our datasets by\nremoving duplicate files based on the hash of their contents.\nWe use sha256 to hash the contents of the file.\nIn addition to deduplicating we also filter out small and\nlarge files. Source files larger than 1 MB are designated as\nlarge files and removed. These are generally entire libraries\nin a single source file or contain raw data within the code.\nAdditionally, files containing less than 15 tokens, as defined\nby the language vocab, are not included. The reduced dataset\nsizes after deduplication and filtering are listed in Table I.\nApproximately 18% of the files are removed during this\nprocessing. Table I shows the properties of the dataset after\neach step of deduplication and filtering.\nTABLE I\nPROPERTIES OF THE HPC SOURCE CODE DATASET .\nFilter # Files # LOC Size (GB)\nNone 239,469 61,585,704 2.02\nDeduplicate 198,958 53,043,265 1.74\nDeduplicate + remove\nsmall/large files 196,140 50,017,351 1.62\nAfter filtering source files, we tokenize the dataset to obtain\ninteger values for the text that can be used as input into\nthe model. We use the pre-trained tokenizers for each of our\nselected models (see Section V). These are all GPT-2 [16]\nbased Byte-Pair Encoding (BPE) tokenizers.\nC. Performance Datasets\nIn addition to the large HPC source code dataset, we create\ntwo datasets of code paired with performance data. These\ndatasets contain code pairs with performance data for both\ncodes in the pair, and can be used to train an LLM to model\nperformance characteristics between them.\nWe create two datasets – one with pairs of code that are\nfunctionally different and one where they are the same. The\nfirst dataset is created by using version control history to\ncapture performance regressions. We run each commit for the\nKripke [17] and Laghos [18] applications. These are small\nHPC apps meant to mimic the computational behavior of larger\nscientific applications. We automate building and running each\ncommit to the best of our ability and collect performance\nresults for 830 commits in total.\nThe second dataset is a set of programming competition\nsolutions from the code contests dataset [19]. These are\naggregated from several online programming competitions:\nAizu, AtCoder, CodeChef, CodeForces, and HackerEarth. This\ndataset allows us to create pairs of code that solve the\nsame problem (the contest problem), but may be different\nin implementation. We run every correct solution for each\nproblem in the dataset, with the corresponding problem’s test\ncases as inputs, and record the run time. Using all the C++\nsolutions in the dataset we create ∼1.7 million samples of\ncode. Using the run times, we group the solutions into pairs\nand label them as slower and faster pairs.\nV. F INE -TUNING METHODOLOGY\nIn this section, we describe the models used and how they\nwere selected. We also discuss the methods used to fine-tune\nthem on our collected dataset.\nA. Models Selected For Fine-tuning\nRecent years have seen the introduction of a significant\nnumber of large language models. These models can range\nin size from 100 million to more than 100 billion parameters.\nSuch large models have been shown to work well for language\nmodeling, but pose significant hurdles to train and use in\npractice. They can take months to train on large GPU clusters\nand typically cannot feasibly run inference on consumer-grade\nhardware. Thus, choosing the right model requires selecting\none that can sufficiently model the language data, but also be\nreasonably deployed for downstream tasks.\nKeeping the above mentioned requirements in mind, we\nselect several models for fine-tuning and/or testing. These\nare listed in Table II. All of these are based on GPT-2 [16]\nand/or GPT-3 [23] architectures with slight variations in size,\nconfiguration, and pre-training data. GPT-2, the smallest in our\nexperiments, is pre-trained on the WebText [20] dataset, which\nTABLE II\nDESCRIPTION OF THE MODELS USED FOR FINE -TUNING .\nModel # Params. # Layers Hidden\nSize\nWindow\nSize\nPre-Training\nSet\nGPT-2 [16] 1.5B 48 1600 1024 WebText [20]\nGPT-Neo [21] 2.7B 32 2560 256 Pile [22]\nPolyCoder [14] 2.7B 32 2560 2048 Source Code\nis a collection of language data scraped from the internet.\nWe use the 1.5 billion parameter GPT-2 model variant in this\npaper. PolyCoder [14] is pre-trained on a collection of solely\nsource code data from GitHub that contains a mixture of 12\npopular programming languages [14]. Between these two is\nGPT-Neo [21] that is pre-trained on the Pile dataset [22]. This\ndataset contains a collection of approximately 800GB of text\ndata from the internet, academic articles, source code, etc.\nNotably this dataset has a mixture of natural language and\ncode. It has been demonstrated that pre-training over both\nnatural language and code can improve the performance of\nthe model.\nWe exclude models such as GPT-4 [24], the state-of-the-art\nmodel that powers GitHub CoPilot, from our experiments due\nto the model and its dataset being closed source. It is currently\nonly accessible for inference via a non-free API. GPT-4’s\ndataset being closed source is significant as we cannot remove\ndata it has trained on from the dataset we use to evaluate its\nperformance, so its results would be overly optimistic. This\nprevents a realistic evaluation and comparison.\nB. Fine-tuning Setup and Hyperparameters\nWe rely on the functionality provided in the Hugging-\nFace [25] Python library for fine-tuning the models. This\nlibrary automates many of the tasks related to loading and\npre-processing datasets, and running language models on\nthe datasets. In particular, we use the Trainer interface\nwith DeepSpeed [26] as the backend to optimize fine-tuning.\nDeepSpeed is a framework that provides distributed training\nfunctionality and several memory optimizations to enable large\nmodels to fit in GPU memory.\nStarting with the pre-trained models, we fine-tune them on a\nsingle node with an AMD EPYC 7763 CPU, 512 GB memory,\nand four 40 GB NVIDIA A100 GPUs. With DeepSpeed’s\nZeRO memory optimizations [27], all of the models fit entirely\nwithin a single A100 GPU and are, thus, fine-tuned using\npure data parallelism. We refer the reader to [28], [29] for\na comprehensive overview of training deep neural networks\nin parallel.\nWe use the AdamW [30] optimizer for all the models to up-\ndate model weights and minimize the loss. We set the learning\nrate to 5 × 10−5 and Adam parameters β1 and β2 to 0.9 and\n0.999, respectively. These hyperparameters are consistent with\ntypical values in the literature. 16-bit floating point precision\nis used to accelerate fine-tuning and reduce model size on the\nA100s. We record the perplexity of the model on the training\ndata during fine-tuning. This is calculated as the exponential\nof the training loss (see Section II-A). Every 1000 optimizer\nsteps, we also test the model using the validation dataset, and\nrecord the perplexity and accuracy at predicting tokens. The\nvalidation dataset is 5% of the full dataset, separate from the\ntraining dataset.\nVI. D OWNSTREAM INFERENCE TASKS AND EVALUATION\nMETRICS\nIn this section, we introduce the benchmarks and metrics\nused to evaluate the performance of the language models.\nA. Code Completion\nA standard benchmark for code generation tasks is the\nHumanEval benchmark [31]. This is comprised of 164 sample\nPython problems, where the input to the model is a natural\nlanguage description of a function and function header. The\nmodel generates code for the function implementation, and is\nscored on functional correctness rather than textual similarity\nor equivalence.\nWe introduce our own adaptation of this benchmark for HPC\nC/C++ programs. Our benchmark consists of 25 custom HPC\ncode generation problems including simple numerics, OpenMP\nparallel code, and MPI routines. Table III lists the tests used\nin our evaluation. Figure 3 shows a sample prompt (top) and\noutput (bottom) for a shared-memory parallel implementation\nof saxpy. The prompt is provided as input to the model and\nit is expected to generate text functionally equivalent to the\ntext on the bottom.\nTABLE III\nCODE GENERATION TESTS . OPEN MP AND MPI COLUMNS DENOTE IF THE\nTEST INCLUDES A VERSION WITH THAT PARALLEL BACKEND .\nName Description Seq. OpenMP MPI\nAverage Average of an array\nof doubles ✓ ✓ ✓\nReduce Reduce by generic\nfunction foo ✓ ✓ ✓\nSaxpy Saxpy ✓ ✓ ✓\nDaxpy Daxpy ✓ ✓ ✓\nMatmul Double-precision\nmatrix multiply ✓ ✓ ✓\nSimple Send Send MPI message ✓\nSimple Receive Receive MPI message ✓\nFFT Double-precision FFT ✓ ✓ ✓\nCholesky Single-precision Cholesky\nfactorization ✓ ✓ ✓\nPing-pong MPI ping-pong ✓\nRing pass MPI ring pass ✓\nEvaluation Metric: We first record the ratio of generated\nsamples that build correctly to those that do not. This indicates\nthe model’s ability to generate syntactically correct code. For\nthose that compile we compute the pass@k metric that denotes\nthe probability that at least one of k samples out of Np\ncode samples is correct. We do Np trials with each prompt\np to generate Np code samples, compile/run the samples,\nand record the number that are functionally correct ( cp). To\nestimate the probability that at least one of k samples chosen\nfrom Np samples is correct for a particular prompt, p, we\n(a) Prompt\n1 /*\n2 multiply scalar float a by vector x and add to y\n3 vectors x and y are length N\n4 use OpenMP to compute in parallel\n5 */\n6 void saxpy(float *x, float *y, float a, int N) {\n(b) Output\n1 #pragma omp parallel for\n2 for (int i = 0; i < N; i++) {\n3 y[i] += a * x[i];\n4 }\n5 }\nFig. 3. An example prompt asking the model to generate a parallel version of\nsaxpy. The comment and function header make up the prompt. The function\nbody on the bottom shows a potential model output.\ncan use the number of generated samples that are functionally\ncorrect, cp, out of the Np total samples generated to calculate\npass@k for a given k as,\npass@k = 1−\n\u0012Np − cp\nk\n\u0013\n/\n\u0012Np\nk\n\u0013\n(1)\nFor each model, we report the average pass@k metric as the\naverage pass@k over all P prompts as shown below:\naverage pass@k = 1\nP\nPX\ni=1\n\"\n1 −\n\u0000Ni−ci\nk\n\u0001\n\u0000Ni\nk\n\u0001\n#\n(2)\nThis metric provides insight into the probability of a model\ngenerating functionally correct code. In our experiments, we\ncalculate the pass @k score for several temperatures, namely\n0.1, 0.2, 0.4, 0.6, and 0.8, and select the best one. This is\nin line with experiments in related literature [14]. For each\ntemperature and prompt, we generate Np = 100samples. The\ncode is generated with nucleus sampling using 0.93 as the\ncutoff value in the CDF (see Section II).\nTo compile the generated code samples, we use g++ with\nthe “ -O2 -std=c++17 -fopenmp” flags. For tests that\nneed MPI we use the OpenMPI mpicxx compiler. If the build\nis successful, then a corresponding driver binary is called that\nwill call and test the generated function for correctness. These\nare run on a AMD EPYC 7763 CPUs with 64 physical cores\nat 2.45 GHz each. For tests that require OpenMP or MPI we\nonly denote them as correct if they used the corresponding\nparallel framework to compute their result.\nB. Predicting OpenMP Pragmas\nA common HPC coding task is decorating for\nloops with OpenMP pragmas. Every pragma starts with\n#pragma omp parallel for and is followed by a list\nof optional clauses that modify the behavior of the parallel\nfor. We test the model’s ability to write OpenMP pragmas\nfor arbitrary for loops.\nFurther Fine-tuning: We cannot directly use the existing\nmodels to generate pragmas before a for loop, since they are\nall left-to-right and can only append tokens to sequences. Thus,\nwe need to further fine-tune the models on a smaller dataset\nthat puts the for loop before the pragma. To accomplish this,\nwe first create a dataset of every for loop with an OpenMP\npragma from our HPC code dataset. 500 tokens of context\nfrom before the for loop are also included. This results in a\ndataset with 13,900 samples.\nSince our model is left-to-right, we format each sample by\nmoving the pragma to directly after the loop and a unique\nseparating token <begin-omp>. This allows us to use the\nmodel by providing a for loop plus some context and the\nmodel will generate an OpenMP pragma for the for loop.\nEach model is fine-tuned on this smaller dataset for three\nepochs (passes over the entire dataset). To prevent overfitting\nwe use a starting learning rate of 3 × 10−5. During training\n10% of the dataset is set aside for validation.\nEvaluation Metric: To measure the success of this test, we use\nthe accuracy of generating correct pragmas. This is calculated\nas shown in Equation 3.\naccuracy = # correct pragmas\ntotal pragmas tested (3)\nFor this problem, we define a correct pragma in two ways:\nsyntactic and functional. To measure syntactic correctness we\ncompare the generated pragma with the actual pragma for\ntextual equivalence. Since it is impossible to automate the\nrunning and evaluation of arbitrary for loops from our dataset\nwe measure functional correctness by comparing the generated\npragmas with the actual ones while ignoring differences that\ndo not contribute to functionality. For instance we ignore\nreordering of variables and clauses where these do not mat-\nter. Additionally, clauses such as schedule are ignored. This\ncorrectness check is done using a custom Python script that\nparses the pragmas and compares them. We record accuracy\nfrom both of these correctness metrics for each model.\nC. Relative Performance Prediction\nIn addition to text generation, we can also use the LLMs\nfor classification. Here we use them to predict performance\nslowdowns between two pairs of code.\nFurther Fine-tuning: In order to use the models for relative\nperformance classification we need to first fine-tune them on\nnew data for this output task. Using the Git commit data from\nSection IV-C we give the model text for a region of code\nbefore and after a Git commit. The codes are concatenated\nwith a unique token separating them, namely <COMMIT>. We\nrepeat a similar process for the code contest dataset, but instead\nseparate pairs by the token <PAIR>. With this data the model\nis fine-tuned to predict whether the second code will be slower\n(positive) or the same/faster ( negative).\nFor each dataset we fine-tune the model on 90% of the\ndata with the other 10% set aside for evaluation. The model\ntakes the concatenated sequences of the two versions of\nthe code implementation and is fine-tuned for the binary\nclassification problem of predicting relative performance. The\ntraining objective is classification accuracy, which we also use\nto measure success for this task.\nEvaluation Metric: To evaluate the performance on this\ntask we measure the model’s classification accuracy. This is\ncalculated as shown in Equation 4.\naccuracy = # correct performance predictions\ntotal performance predictions (4)\nFor this metric higher is better and a classification accuracy\nof 100% signifies a perfect score.\nVII. R ESULTS\nWe now present the fine-tuning and evaluation results using\nthe downstream tasks discussed in Section VI.\nA. Fine-tuning on HPC Source Code Data\nWe first show the results of fine-tuning the three models\nselected in Table II. Table IV shows the validation perplexity\nat the end of fine-tuning. Here perplexity is calculated as the\nexponential of the loss as described in Section II. Each model\nconverges to a low perplexity score over the separate testing\nset (between 2 and 4). GPT-Neo and PolyCoder achieve com-\nparable perplexity scores (within 0.01) while GPT2 achieves a\nhigher perplexity. All three have different pre-training datasets\nand the former two are of a larger size than GPT2 (see\nTable II). From this we can conclude that for this problem\nthe pre-training dataset had less of an impact on validation\nperplexity than the model size. The lower perplexity of the\nlarger models means that they model the language better.\nTABLE IV\nFINAL VALIDATION PERPLEXITIES FOR EACH MODEL AFTER FINE -TUNING\nON THE HPC SOURCE CODE DATASET .\nModel GPT-2 GPT-Neo PolyCoder\nFinal Validation Perplexity 4.47 2.23 2.24\nFor the rest of the results presented in this section we\nwill use PolyCoder+HPC, GPT-Neo+HPC, and GPT2+HPC to\nrefer to the respective models fine-tuned on the HPC dataset.\nAfter fine-tuning each of the models and evaluating them\non the downstream tasks we noticed that the perplexity would\nkeep improving with more fine-tuning, but the downstream\nevaluation performance would start to decrease. This is likely\nbecause LLMs are subject to catastrophic forgetting during\nfine-tuning. Catastrophic forgetting is the phenomenon where\npreviously learned information is lost or forgotten as the model\ncontinues training and updating its weights. It is typically\nprevented by minimizing the amount of fine-tuning and using\na sufficiently low learning rate.\nTo explore this phenomenon we ran the code generation\ntasks every 1000 samples during fine-tuning of the PolyCoder\nmodel. Figure 4 presents the results from our evaluation tests\nduring fine-tuning on the PolyCoder model. After seeing about\n45,000 samples during fine-tuning the model starts to decrease\nin evaluation performance. This is in contrast to the perplexity\nwhich keeps improving past 45,000 samples. Based on this\nresult we stop fine-tuning at 45,000 samples and use these\nweights for the rest of the evaluations. Additionally, due to the\ncomputation time needed to run this test we use the 45,000\nsamples stopping point for fine-tuning all the models.\n10 20 30 40 50 60\n1000x Samples\n0.5\n1.0\n1.5\n2.0Accuracy\nEvaluation Performance During Training\naverage_pass@1\naverage_pass@10\naverage_pass@100\nEval Perplexity\nFig. 4. Downstream evaluation performance across training iterations for\nPolyCoder+HPC. The model starts to perform worse around 45,000 samples\neven though the perplexity keeps improving.\nB. Code Completion\nHaving fine-tuned the three models, we now start using them\nfor the different downstream tasks described in Section VI.\nThe first downstream task is code generation, described in\nSection VI-A. Figure 5 shows the average pass@k rates for\nthe code generation tests. The average pass@k values are\ncomputed according to Equation 2. We use PolyCoder as a\nbaseline for comparison since it is a state-of-the-art LLM for\ncode generation. PolyCoder+HPC scores the best for average\npass@1, pass@10, and pass@100. For each value of k the\nmodels score in the order of PolyCoder+HPC, PolyCoder,\nGPT-Neo+HPC, and GPT2+HPC. PolyCoder+HPC gains the\nslight edge over the original PolyCoder by successfully gen-\nerating code for the HPC-specific tasks (see Figure 6).\nk=1 k=10 k=100\nk\n0\n20\n40\n60\n80\n100average_pass@k (%)19\n41\n66\n25\n43\n71\n17\n38\n60\n1 6\n13\nCode Generation Scores\nPolyCoder\nPolyCoder+HPC\nGPT -Neo+HPC\nGPT -2+HPC\nFig. 5. Comparison of models on code generation. The clusters represent the\naverage pass@k scores for k = 1, 10 and 100. Higher percentage is better.\nIn Figure 5 we see that GPT2+HPC scores significantly\nlower than the other models. This is likely due to the smaller\nmodel size and the fact that there is no source code in its pre-\ntraining dataset. In this instance fine-tuning is not enough to\nenable GPT-2 to generate correct C++ HPC code.\nAltogether, the scores are indicative that PolyCoder+HPC\nand GPT-Neo+HPC has learned how to generate valid C++\ncode. For instance, if the best model, PolyCoder+HPC, is\npermitted to generate 100 samples, then 71% of them are\ncorrect on average across all the tests. Similarly for 1 sample\ngenerated this is 25%. These numbers roughly align with\nresults from [14] on the HumanEval Python tests. However, the\nresults are not directly comparable since they are a different\nset of tests in a different programming language.\nTo demonstrate the generative capabilities of the specialized\nmodels we reduce the code generation tasks to those that\nare specific to HPC. This includes code that uses OpenMP\nand/or MPI parallelism. Figure 6 shows the performance when\nrestricted to these tests. We see that PolyCoder is unable to\ngenerate OpenMP and MPI code as it scores significantly\nlower than the rest. GPT2+HPC still performs fairly low,\nhowever, its score has actually improved slightly over Figure 5.\nThis is due to the fact that it has only seen HPC-specific code\nduring training and that is what is being tested here.\nk=1 k=10 k=100\nk\n0\n20\n40\n60\n80\n100average_pass@k (%)\n0 1\n8\n19\n38\n61\n14\n34\n49\n4\n12\n19\nHPC Code Generation Scores\nPolyCoder\nPolyCoder+HPC\nGPT -Neo+HPC\nGPT -2+HPC\nFig. 6. Comparison of models on code generation for HPC-specific functions.\nThe clusters represent the average pass@k scores for k = 1, 10 and 100.\nHigher percentage is better.\nAnother point of interest besides functional correctness\nis syntactic correctness. This can be measured by the total\nnumber of generated samples that compile successfully. This\nis how often the model generates valid code, whether it is func-\ntionally correct or not. This data is presented in Figure 7. Poly-\nCoder and PolyCoder+HPC both perform the best compared\nto the other models with 84% and 86% of samples compiling\ncorrectly, respectively. GPT-Neo+HPC performs slightly worse\nat 74% and GPT2-HPC has only 30% of samples compile. The\nworse performance of the latter two can likely be attribute to\ntheir pre-training datasets having less code. We also observe\nthat for all models there is a visual correlation between build\nand correctness rates, which is expected as a model needs to\ncompile in order to be functionally correct.\nPolyCoder PolyCoder+HPCGPT -Neo+HPC GPT -2+HPC\n0\n20\n40\n60\n80\n100Successful Build (%)\n86 84\n74\n30\nPercentage of Generated Samples That Compile\nFig. 7. Comparison of the models’ build rate. Both PolyCoder and Poly-\nCoder+HPC have the best percentage of total samples that successfully\ncompile. Higher percentage is better.\nThe code in Figure 8 shows example output from PolyCoder\nand PolyCoder+HPC on generating OpenMP code to compute\na sum in parallel. We see that PolyCoder is able to produce\ncorrect sequential code, however, it fails to add any OpenMP\npragmas. The PolyCoder+HPC model is able to correctly tag\nthe for loop with an OpenMP pragma.\n(a) Prompt\n1 /*\n2 Compute the sum of the array X and return the sum.\n3 X has N elements.\n4 Use OpenMP to compute the sum in parallel.\n5 */\n6 float sum(float *X, int N) {\n(b) PolyCoder output\n1 float sum = 0.0f;\n2 for (int i = 0; i < N; i++)\n3 sum += X[i];\n4 return sum;\n5 }\n(c) PolyCoder+HPC output\n1 float sum = 0.0f;\n2 #pragma omp parallel for reduction(+:sum)\n3 for (int i = 0; i < N; i++)\n4 sum += X[i];\n5 return sum;\n6 }\nFig. 8. Example OpenMP output from (b) PolyCoder and (c) Poly-\nCoder+HPC. The comment and function description (top) make up the prompt\nthat is given to the model, while the bottom two blocks are the generated\ntext. We see that PolyCoder is unable to generate OpenMP pragmas for the\nreduction in this example.\nA similar example with distributed memory is shown in\nFigure 9. The PolyCoder+HPC is able to generate correct MPI\ncode to compute an average in parallel across ranks. PolyCoder\noften generated long and incorrect code that demonstrated little\nunderstanding of how to write MPI routines. The example\npresented is indicative of the rest of the MPI samples generated\nby PolyCoder. Notably, some instances did contain MPI calls,\nso PolyCoder must have some MPI code in its pre-training\ndataset. However, it lacked the ability to properly use MPI.\n(a) Prompt\n1 /*\n2 Use MPI to compute the average of X across ranks\n3 and return the result on all ranks.\n4 Use the default communicator.\n5 */\n6 double mpiAverage(double X) {\n(b) PolyCoder output\n1 int rank;\n2 int nprocs;\n3 double average;\n4 MPI_Status status;\n5 MPI_Request request;\n6\n7 MPI_Init(&argc, &argv);\n8 MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n9 MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n10\n11 if (rank == 0) {\n12 MPI_Recv(&status, 1, MPI_INT, 0, 0,\nMPI_COMM_WORLD,\n13 &request);\n14 MPI_Send(&X, 1, MPI_INT, 0, 0,\nMPI_COMM_WORLD);\n15 MPI_Send(&average, 1, MPI_DOUBLE, 0, 0,\n16 MPI_COMM_WORLD);\n17 }\n18 return average;\n19 }\n(c) PolyCoder+HPC output\n1 int size;\n2 double sum;\n3\n4 MPI_Comm_size(MPI_COMM_WORLD, &size);\n5 MPI_Allreduce(&X, &sum, 1, MPI_DOUBLE, MPI_SUM,\n6 MPI_COMM_WORLD);\n7\n8 return sum / size;\n9 }\nFig. 9. Example MPI output from (b) PolyCoder and (c) PolyCoder+HPC.\nThe highlighted region is code generated by the model (reformatted to fit the\ncolumn). PolyCoder results varied significantly, however, the above example\ndemonstrates the general lack of understanding it had for MPI.\nFigure 10 shows the speedups for the code generated by\nPolyCoder+HPC over sequential baselines. These are hand-\nwritten efficient, sequential implementations for each test. We\nsee that PolyCoder+HPC is able to generate code that is faster\nthan the sequential baseline. This demonstrates that it is not\ngenerating very poor performing parallel code and is likely\nusing the parallelism correctly.\nSince PolyCoder+HPC scores the highest in training and\nthese code generation tests we select it for further compar-\nisons in the rest of the paper. PolyCoder+HPC is the fine-\ntuned model we present as HPC-Coder. We continue to use\nPolyCoder as a baseline.\naverage reduce axpy matmul fft cholesky\nProblem\n0\n2\n4\n6\n8Speedup2.8\n1.5\n6\n2.1 1.9 2.3\nSpeedup of Generated Parallel Code\nFig. 10. Comparison of the speedups for the code generation tests over\nsequential baselines. They are all above 1 demonstrating that the model is\nnot generating very poor performing parallel code.\nC. Predicting OpenMP Pragmas\nNext, we examine the result from the OpenMP prediction\ntests described in Section VI-B. Figure 11 shows the results\nfrom the OpenMP experiments detailed in Section VI-B.\nWe see that both models are able to generate functionally\ncorrect OpenMP pragmas with high accuracy (right plot).\nPolyCoder+HPC is able to do this with 97% accuracy and\nPolyCoder 94%. The LLMs are exemplary at understanding\nthe dependencies of the for loop and what clauses are\nrequired to correctly parallelize them. We see that the model\nthat has seen large amounts of OpenMP code performs better.\nWe can also look at how well the models reproduce the\npragmas exactly. This means all the clauses and variables\nwithin those clauses are in the same order in the dataset and\nin the output from the model. These results are shown in the\nleft plot in Figure 11. While less meaningful than functional\ncorrectness, it is interesting that the model is able to exactly\nreproduce pragmas it has not seen before with relatively high\naccuracy (67% and 61%). This is likely due to certain trends\nin the construction and ordering of OpenMP clauses that the\nLLMs are learning as they train.\nPolyCoder PolyCoder+HPC\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0T extual Equivalence Accuracy\nPolyCoder PolyCoder+HPC\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Correctness Accuracy\nOpenMP Pragma Generation Accuracy\nFig. 11. Comparison of models on predicting OpenMP pragmas. The left\nplot presents accuracy in predicting OpenMP pragmas exactly as they appear\nin the dataset. The right plot shows the accuracy in predicting functionally\ncorrect OpenMP pragmas. Higher accuracy is better.\nD. Relative Performance Prediction\nFinally, we look at the results from the relative performance\nprediction tests described in Section VI-C. Figure 12 shows the\nresults from the relative performance prediction tests (see Sec-\ntion VI-C). Both models achieve high classification accuracy\nwith PolyCoder+HPC being slightly better for the two proxy\napplications at 88% and PolyCoder at 86%. This means that\nfor 88% of the code changes in the two repositories version\ncontrol history PolyCoder+HPC is able to correctly identify\nif there will be a performance slowdown. Likewise for the\nprogramming competition dataset we see that PolyCoder+HPC\noutperforms the PolyCoder baseline with an accuracy of 92%\nvs 86%. This is a higher accuracy improvement than the proxy\napplications by 4 percentage points. This is likely due to the\nfact that the programming competition dataset is larger and\nPolyCoder+HPC has been trained on more C/C++ code.\nProxy Apps Coding Competitions\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\n0.86 0.860.88 0.92\nRelative Performance Classification Accuracy\nPolyCoder\nPolyCoder+HPC\nFig. 12. Comparison of models on predicting relative performance of code\nchanges. Both models achieve similarly high accuracy. The PolyCoder+HPC\nmodel performs slightly better on both datasets. Higher accuracy is better.\nThe success of this test demonstrates that the models are\nable to correlate their prior language understanding with\nperformance related properties of code. This means we can\nleverage LLMs and fine-tuning to model code performance\nwithout the need to collect large amounts data.\nVIII. R ELATED WORK\nIn this section we detail related work that uses LLMs to\nstudy source code and work that uses machine learning to\nmodel the performance of source code.\nA. LLMs for Code Generation\nWith the explosion in research in transformer models and\nLLMs there have been a large number of papers applying\nthese techniques to source code. Most of these methods have\nextended GPT-2 [16], GPT-3 [23], or BERT [32], [33] models\nand trained them on code. A notable instance is Codex [2],\nwhich is a modification of GPT-3 that is targeted for source\ncode generation. Following Codex’s introduction there have\nbeen several other works that have introduced state-of-the-art\nlarge language models [3], [4], [34]. While some of these\nare open source, the best, such as GPT-4 [24], keep their\narchitecture, weights, and training data closed source and only\ninference is available via a paid API.\nA large amount of this recent research has focused on code\ngeneration. These usually take a mix of code and natural\nlanguage and learn how to meaningfully finish the code. While\nseminal works have continued to improve code generation\nwith better and bigger models [2], [23], [33], other works\nhave explored how to better utilize these tools in software\nengineering workflows [35]–[37]. Some flip code generation\naround and learn to generate natural language code summaries\nfrom code snippets [7]–[10].\nThese models can even be trained for tasks such bug and\nmalware detection [11], [12]. LLMs can also be used to\nsuggest fixes in these cases rather than just identify prob-\nlematic code. Many other previously difficult to automate\nsoftware development tasks have since been tackled by ap-\nplying LLMs [6]. More recently some of these tasks have\nincluded HPC development tasks such as race detection [38]\nand OpenACC compiler validation [39].\nB. Machine Learning Applied to Source Code Performance\nHowever, one important problem in software development\nthat has not received much research with LLMs is that of\nperformance. Many of the reasons listed in Section I have\nprevented meaningful studies from being accomplished. Pre-\nviously approaches used code2vec [40], ir2vec [41], or a\nsimilar method to first map source code to an embedded space\nthat could then be learned on. These were successfully used\nfor some performance related analytical modeling such as\nOpenCL kernel device placement [41], but never leveraged\nLLMs for a full performance study.\nGarg et al. [42] recently introduced DeepDevPERF, which\nis a BART-based [43] LLM designed to suggest performance\nimprovements to arbitrary C# code. They overcome the issue\nof data collection by using code changes from Git commits that\nhave performance related keywords in their commit message,\nalbeit, this dataset is still noisy. This work is different than\nthat presented in this paper as it suggests code transformations\nrather than learn relative performance. The latter being useful\nin cases where two versions of a code already exist, such as\nwith Git commits. Additionally, our model is trained on real\nperformance data and can be used for HPC and parallel code\ngeneration tasks.\nIX. C ONCLUSION AND FUTURE WORK\nIn this paper, we have demonstrated the fine-tuning of an\nLLM using HPC code, and its ability to outperform other\nLLMs in HPC related tasks such as HPC code generation and\nperformance modeling. We have accomplished this by fine-\ntuning a model, and showing that it can generate functionally\ncorrect HPC code at up to a 53% higher pass@k rate and\ncan accurately label for loops with OpenMP pragmas with\n97% success. We have further demonstrated how this fine-\ntuned model can be utilized to study performance properties\nof source code with little data. These results demonstrate the\nneed for and usefulness of HPC-specific language models. The\nbest model in our experiments, PolyCoder+HPC, we present\nas HPC-Coder.\nIn the future, we plan to explore further analyses that can\nbe accomplished using our language model. We also plan on\nexploring how to tune the model to generate not just correct\nbut performant code. Additionally, we plan to investigate how\nto engineer these innovations into practical tools that can be\neasily used by computational scientists and HPC developers\nto enable them to produce better code more efficiently.\nACKNOWLEDGMENT\nThis material is based upon work supported in part by\nthe National Science Foundation under Grant No. 2047120.\nThis work was performed in part under the auspices of the\nU.S. Department of Energy by Lawrence Livermore National\nLaboratory under Contract DE-AC52-07NA27344 (LLNL-\nCONF-844549).\nREFERENCES\n[1] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y . Hou, Y . Min, B. Zhang,\nJ. Zhang, Z. Dong, Y . Du, C. Yang, Y . Chen, Z. Chen, J. Jiang, R. Ren,\nY . Li, X. Tang, Z. Liu, P. Liu, J.-Y . Nie, and J.-R. Wen, “A survey of\nlarge language models,” 2023.\n[2] M. Chen and et al, “Evaluating large language models trained on code,”\n2021.\n[3] R. Li, L. B. Allal, Y . Zi, N. Muennighoff, D. Kocetkov, C. Mou,\nM. Marone, C. Akiki, J. Li, J. Chim, Q. Liu, E. Zheltonozhskii, T. Y .\nZhuo, T. Wang, O. Dehaene, M. Davaadorj, J. Lamy-Poirier, J. Monteiro,\nO. Shliazhko, N. Gontier, N. Meade, A. Zebaze, M.-H. Yee, L. K.\nUmapathi, J. Zhu, B. Lipkin, M. Oblokulov, Z. Wang, R. Murthy,\nJ. Stillerman, S. S. Patel, D. Abulkhanov, M. Zocca, M. Dey, Z. Zhang,\nN. Fahmy, U. Bhattacharyya, W. Yu, S. Singh, S. Luccioni, P. Villegas,\nM. Kunakov, F. Zhdanov, M. Romero, T. Lee, N. Timor, J. Ding,\nC. Schlesinger, H. Schoelkopf, J. Ebert, T. Dao, M. Mishra, A. Gu,\nJ. Robinson, C. J. Anderson, B. Dolan-Gavitt, D. Contractor, S. Reddy,\nD. Fried, D. Bahdanau, Y . Jernite, C. M. Ferrandis, S. Hughes, T. Wolf,\nA. Guha, L. von Werra, and H. de Vries, “Starcoder: may the source be\nwith you!” 2023.\n[4] B. Rozi `ere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan,\nY . Adi, J. Liu, T. Remez, J. Rapin, A. Kozhevnikov, I. Evtimov,\nJ. Bitton, M. Bhatt, C. C. Ferrer, A. Grattafiori, W. Xiong, A. D ´efossez,\nJ. Copet, F. Azhar, H. Touvron, L. Martin, N. Usunier, T. Scialom, and\nG. Synnaeve, “Code llama: Open foundation models for code,” 2023.\n[5] J. Senanayake, H. Kalutarage, and M. O. Al-Kadri, “Android\nmobile malware detection using machine learning: A systematic\nreview,” Electronics, vol. 10, no. 13, 2021. [Online]. Available:\nhttps://www.mdpi.com/2079-9292/10/13/1606\n[6] “Ml4code,” https://ml4code.github.io/, accessed: 2022.\n[7] J. Gu, P. Salza, and H. C. Gall, “Assemble foundation models for\nautomatic code summarization,” 2022 IEEE International Conference\non Software Analysis, Evolution and Reengineering (SANER) , pp. 935–\n946, 2022.\n[8] T. Ahmed and P. Devanbu, “Learning code summarization from a small\nand local dataset,” ArXiv, vol. abs/2206.00804, 2022.\n[9] S. Haque, Z. Eberhart, A. Bansal, and C. McMillan, “Semantic similarity\nmetrics for evaluating source code summarization,” 2022 IEEE/ACM\n30th International Conference on Program Comprehension (ICPC) , pp.\n36–47, 2022.\n[10] W. U. Ahmad, S. Chakraborty, B. Ray, and K.-W. Chang, “A\ntransformer-based approach for source code summarization,” ArXiv, vol.\nabs/2005.00653, 2020.\n[11] C. Richter and H. Wehrheim, “Can we learn from developer mistakes?\nlearning to localize and repair real bugs from real bug fixes,” ArXiv, vol.\nabs/2207.00301, 2022.\n[12] A. Kharkar, R. Z. Moghaddam, M. Jin, X. Liu, X. Shi, C. B. Clement,\nand N. Sundaresan, “Learning to reduce false positives in analytic bug\ndetectors,” 2022 IEEE/ACM 44th International Conference on Software\nEngineering (ICSE), pp. 1307–1316, 2022.\n[13] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all\nyou need,” CoRR, vol. abs/1706.03762, 2017. [Online]. Available:\nhttp://arxiv.org/abs/1706.03762\n[14] F. F. Xu, U. Alon, G. Neubig, and V . J. Hellendoorn, “A\nSystematic Evaluation of Large Language Models of Code,” Feb. 2022,\nhttps://arxiv.org/abs/2202.13169. [Online]. Available: https://doi.org/10.\n5281/zenodo.6363556\n[15] M. Allamanis, “The adverse effects of code duplication in machine\nlearning models of code,” in Proceedings of the 2019 ACM SIGPLAN\nInternational Symposium on New Ideas, New Paradigms, and Reflections\non Programming and Software , ser. Onward! 2019. New York, NY ,\nUSA: Association for Computing Machinery, 2019, p. 143–153.\n[Online]. Available: https://doi.org/10.1145/3359591.3359735\n[16] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\n“Language models are unsupervised multitask learners,” 2019.\n[17] A. Kunen, T. Bailey, and P. Brown, “KRIPKE-a massively parallel\ntransport mini-app,” Lawrence Livermore National Laboratory (LLNL),\nLivermore, CA, Tech. Rep , 2015.\n[18] V . A. Dobrev, T. V . Kolev, and R. N. Rieben, “High-order curvilinear\nfinite element methods for lagrangian hydrodynamics,” SIAM Journal\non Scientific Computing, vol. 34, no. 5, pp. B606–B641, 2012. [Online].\nAvailable: https://doi.org/10.1137/120864672\n[19] Y . Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond,\nT. Eccles, J. Keeling, F. Gimeno, A. D. Lago, T. Hubert, P. Choy,\nC. d. M. d’Autume, I. Babuschkin, X. Chen, P.-S. Huang, J. Welbl,\nS. Gowal, A. Cherepanov, J. Molloy, D. J. Mankowitz, E. S.\nRobson, P. Kohli, N. de Freitas, K. Kavukcuoglu, and O. Vinyals,\n“Competition-level code generation with alphacode,” 2022. [Online].\nAvailable: https://arxiv.org/abs/2203.07814\n[20] A. Gokaslan and V . Cohen, “Openwebtext corpus,” http://Skylion007.\ngithub.io/OpenWebTextCorpus, 2019.\n[21] S. Black, G. Leo, P. Wang, C. Leahy, and S. Biderman, “GPT-Neo:\nLarge Scale Autoregressive Language Modeling with Mesh-Tensorflow,”\nMar. 2021, If you use this software, please cite it using these metadata.\n[Online]. Available: https://doi.org/10.5281/zenodo.5297715\n[22] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster,\nJ. Phang, H. He, A. Thite, N. Nabeshima, S. Presser, and\nC. Leahy, “The pile: An 800gb dataset of diverse text for language\nmodeling,” CoRR, vol. abs/2101.00027, 2021. [Online]. Available:\nhttps://arxiv.org/abs/2101.00027\n[23] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal,\nA. Herbert-V oss, G. Krueger, T. Henighan, R. Child, A. Ramesh,\nD. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler,\nM. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish,\nA. Radford, I. Sutskever, and D. Amodei, “Language models are few-\nshot learners,” CoRR, vol. abs/2005.14165, 2020. [Online]. Available:\nhttps://arxiv.org/abs/2005.14165\n[24] OpenAI, “Gpt-4 technical report,” 2023.\n[25] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi,\nP. Cistac, C. Ma, Y . Jernite, J. Plu, C. Xu, T. Le Scao,\nS. Gugger, M. Drame, Q. Lhoest, and A. M. Rush, “Transformers:\nState-of-the-Art Natural Language Processing.” Association for\nComputational Linguistics, Oct. 2020, pp. 38–45. [Online]. Available:\nhttps://www.aclweb.org/anthology/2020.emnlp-demos.6\n[26] Microsoft, “Deepspeed: Extreme-scale model training for\neveryone,” https://www.microsoft.com/en-us/research/blog/\ndeepspeed-extreme-scale-model-training-for-everyone/.\n[27] J. Ren, S. Rajbhandari, R. Y . Aminabadi, O. Ruwase, S. Yang,\nM. Zhang, D. Li, and Y . He, “Zero-offload: Democratizing billion-scale\nmodel training,” CoRR, vol. abs/2101.06840, 2021. [Online]. Available:\nhttps://arxiv.org/abs/2101.06840\n[28] T. Ben-Nun and T. Hoefler, “Demystifying parallel and distributed\ndeep learning: An in-depth concurrency analysis,” ACM Comput.\nSurv., vol. 52, no. 4, Aug. 2019. [Online]. Available: https:\n//doi.org/10.1145/3320060\n[29] D. Nichols, S. Singh, S.-H. Lin, and A. Bhatele, “A survey and empirical\nevaluation of parallel deep learning frameworks,” 2022.\n[30] I. Loshchilov and F. Hutter, “Fixing weight decay regularization\nin adam,” CoRR, vol. abs/1711.05101, 2017. [Online]. Available:\nhttp://arxiv.org/abs/1711.05101\n[31] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan,\nH. Edwards, Y . Burda, N. Joseph, G. Brockman, A. Ray, R. Puri,\nG. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan,\nS. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian,\nC. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis,\nE. Barnes, A. Herbert-V oss, W. H. Guss, A. Nichol, A. Paino, N. Tezak,\nJ. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,\nA. N. Carr, J. Leike, J. Achiam, V . Misra, E. Morikawa, A. Radford,\nM. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew,\nD. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba, “Evaluating\nlarge language models trained on code,” 2021.\n[32] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training\nof deep bidirectional transformers for language understanding,” in\nProceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) . Minneapolis,\nMinnesota: Association for Computational Linguistics, Jun. 2019,\npp. 4171–4186. [Online]. Available: https://www.aclweb.org/anthology/\nN19-1423\n[33] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,\nM. Lewis, L. Zettlemoyer, and V . Stoyanov, “Roberta: A robustly\noptimized BERT pretraining approach,” CoRR, vol. abs/1907.11692,\n2019. [Online]. Available: http://arxiv.org/abs/1907.11692\n[34] Y . Wei, Z. Wang, J. Liu, Y . Ding, and L. Zhang, “Magicoder: Source\ncode is all you need,” arXiv preprint arXiv:2312.02120 , 2023.\n[35] J.-B. D ¨oderlein, M. Acher, D. E. Khelladi, and B. Combemale, “Piloting\ncopilot and codex: Hot temperature, cold prompts, or black magic?”\nArXiv, vol. abs/2210.14699, 2022.\n[36] S. Barke, M. B. James, and N. Polikarpova, “Grounded copilot:\nHow programmers interact with code-generating models,” ArXiv, vol.\nabs/2206.15000, 2022.\n[37] A. Sarkar, A. D. Gordon, C. Negreanu, C. Poelitz, S. S. Ragavan, and\nB. G. Zorn, “What is it like to program with artificial intelligence?”\nArXiv, vol. abs/2208.06213, 2022.\n[38] L. Chen, X. Ding, M. Emani, T. Vanderbruggen, P. hung Lin, and\nC. Liao, “Data race detection using large language models,” 2023.\n[39] C. Munley, A. Jarmusch, and S. Chandrasekaran, “Llm4vv: Developing\nllm-driven testsuite for compiler validation,” 2023.\n[40] U. Alon, M. Zilberstein, O. Levy, and E. Yahav, “code2vec: Learning\ndistributed representations of code,” 2018. [Online]. Available:\nhttps://arxiv.org/abs/1803.09473\n[41] S. VenkataKeerthy, R. Aggarwal, S. Jain, M. S. Desarkar,\nR. Upadrasta, and Y . N. Srikant, “Ir2v¡span class=”smallcaps\nsmallercapital”¿ec¡/span¿: Llvm ir based scalable program embeddings,”\nACM Trans. Archit. Code Optim. , vol. 17, no. 4, dec 2020. [Online].\nAvailable: https://doi.org/10.1145/3418463\n[42] S. Garg, R. Z. Moghaddam, C. B. Clement, N. Sundaresan, and C. Wu,\n“Deepdev-perf: a deep learning-based approach for improving software\nperformance,” Proceedings of the 30th ACM Joint European Software\nEngineering Conference and Symposium on the Foundations of Software\nEngineering, 2022.\n[43] M. Lewis, Y . Liu, N. Goyal, M. Ghazvininejad, A. Mohamed,\nO. Levy, V . Stoyanov, and L. Zettlemoyer, “Bart: Denoising\nsequence-to-sequence pre-training for natural language generation,\ntranslation, and comprehension,” 2019. [Online]. Available: https:\n//arxiv.org/abs/1910.13461",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8313543200492859
    },
    {
      "name": "Supercomputer",
      "score": 0.675565242767334
    },
    {
      "name": "Code (set theory)",
      "score": 0.505320131778717
    },
    {
      "name": "Software",
      "score": 0.47990453243255615
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4548817276954651
    },
    {
      "name": "Software engineering",
      "score": 0.43649184703826904
    },
    {
      "name": "Parallel computing",
      "score": 0.4127435088157654
    },
    {
      "name": "Programming language",
      "score": 0.40339839458465576
    }
  ]
}