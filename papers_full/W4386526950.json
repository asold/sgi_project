{
  "title": "Transferring Vision-Language Models for Visual Recognition: A Classifier Perspective",
  "url": "https://openalex.org/W4386526950",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2099340509",
      "name": "Wenhao Wu",
      "affiliations": [
        "University of Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2558615216",
      "name": "Zhun Sun",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2118159804",
      "name": "Yuxin Song",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2124874746",
      "name": "Jingdong Wang",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2138640236",
      "name": "Wanli Ouyang",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2558615216",
      "name": "Zhun Sun",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2118159804",
      "name": "Yuxin Song",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2124874746",
      "name": "Jingdong Wang",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2138640236",
      "name": "Wanli Ouyang",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "Beijing Academy of Artificial Intelligence"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4214612132",
    "https://openalex.org/W12634471",
    "https://openalex.org/W3035254087",
    "https://openalex.org/W1927052826",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W4214746887",
    "https://openalex.org/W3145450063",
    "https://openalex.org/W2047643928",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W4214614183",
    "https://openalex.org/W2155904486",
    "https://openalex.org/W3034572008",
    "https://openalex.org/W2990503944",
    "https://openalex.org/W2904378456",
    "https://openalex.org/W3034658206",
    "https://openalex.org/W2980037812",
    "https://openalex.org/W3129873671",
    "https://openalex.org/W6790690058",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W2964194231",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W3126337491",
    "https://openalex.org/W2981385151",
    "https://openalex.org/W4312480274",
    "https://openalex.org/W3176125528",
    "https://openalex.org/W2138011018",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2126579184",
    "https://openalex.org/W2108950639",
    "https://openalex.org/W4312254032",
    "https://openalex.org/W2990152177",
    "https://openalex.org/W4312558481",
    "https://openalex.org/W2996901793",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4312560592",
    "https://openalex.org/W4312420092",
    "https://openalex.org/W4285606530",
    "https://openalex.org/W2963689837",
    "https://openalex.org/W4312614039",
    "https://openalex.org/W2533598788",
    "https://openalex.org/W1977295328",
    "https://openalex.org/W2963820951",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W6790978476",
    "https://openalex.org/W4226058394",
    "https://openalex.org/W2993751684",
    "https://openalex.org/W2337252826",
    "https://openalex.org/W2962843773",
    "https://openalex.org/W2887280559",
    "https://openalex.org/W2963155035",
    "https://openalex.org/W2984287396",
    "https://openalex.org/W2963645879",
    "https://openalex.org/W3175528717",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2955874753",
    "https://openalex.org/W3010010212",
    "https://openalex.org/W3174568846",
    "https://openalex.org/W3207340843",
    "https://openalex.org/W6640300118",
    "https://openalex.org/W4312266966",
    "https://openalex.org/W4312302951",
    "https://openalex.org/W2017814585",
    "https://openalex.org/W2883429621",
    "https://openalex.org/W4312658081",
    "https://openalex.org/W4313136445",
    "https://openalex.org/W3172942063",
    "https://openalex.org/W4312818263",
    "https://openalex.org/W4225414521",
    "https://openalex.org/W2770804203",
    "https://openalex.org/W4312310776",
    "https://openalex.org/W3041133507"
  ],
  "abstract": "Abstract Transferring knowledge from pre-trained deep models for downstream tasks, particularly with limited labeled samples, is a fundamental problem in computer vision research. Recent advances in large-scale, task-agnostic vision-language pre-trained models, which are learned with billions of samples, have shed new light on this problem. In this study, we investigate how to efficiently transfer aligned visual and textual knowledge for downstream visual recognition tasks. We first revisit the role of the linear classifier in the vanilla transfer learning framework, and then propose a new paradigm where the parameters of the classifier are initialized with semantic targets from the textual encoder and remain fixed during optimization. To provide a comparison, we also initialize the classifier with knowledge from various resources. In the empirical study, we demonstrate that our paradigm improves the performance and training speed of transfer learning tasks. With only minor modifications, our approach proves effective across 17 visual datasets that span three different data domains: image, video, and 3D point cloud.",
  "full_text": "International Journal of Computer Vision (2024) 132:392–409\nhttps://doi.org/10.1007/s11263-023-01876-w\nTransferring Vision-Language Models for Visual Recognition: A\nClassiﬁer Perspective\nWenhao Wu 1 · Zhun Sun 2 · Yuxin Song 2 · Jingdong Wang 2 · Wanli Ouyang 3\nReceived: 28 February 2023 / Accepted: 7 August 2023 / Published online: 7 September 2023\n© The Author(s) 2023\nAbstract\nTransferring knowledge from pre-trained deep models for downstream tasks, particularly with limited labeled samples, is a\nfundamental problem in computer vision research. Recent advances in large-scale, task-agnostic vision-language pre-trained\nmodels, which are learned with billions of samples, have shed new light on this problem. In this study, we investigate how\nto efﬁciently transfer aligned visual and textual knowledge for downstream visual recognition tasks. We ﬁrst revisit the role\nof the linear classiﬁer in the vanilla transfer learning framework, and then propose a new paradigm where the parameters of\nthe classiﬁer are initialized with semantic targets from the textual encoder and remain ﬁxed during optimization. To provide\na comparison, we also initialize the classiﬁer with knowledge from various resources. In the empirical study, we demonstrate\nthat our paradigm improves the performance and training speed of transfer learning tasks. With only minor modiﬁcations, our\napproach proves effective across 17 visual datasets that span three different data domains: image, video, and 3D point cloud.\nKeywords Visual recognition · Large vision model · Transfer learning\n1 Introduction\nIn the ﬁeld of optimizing neural network training efﬁ-\nciency, knowledge transfer aims to provide pre-learned\ninformation to downstream tasks. For visual recognition\ntasks, the approach typically involves leveraging feature rep-\nresentations derived from a task-agnostic model optimized\nwith large-scale universal datasets, followed by building a\nCommunicated by Kaiyang Zhou.\nB Wenhao Wu\nwenhao.wu@sydney.edu.au\nZhun Sun\nsunzhun@baidu.com\nY uxin Song\nsongyuxin02@baidu.com\nJingdong Wang\nwangjingdong@baidu.com\nWanli Ouyang\nouyangwanli@pjlab.org.cn\n1 The University of Sydney, Darlington, Australia\n2 Department of Computer Vision Technology, Baidu Inc.,\nBeijing, China\n3 Shanghai Artiﬁcial Intelligence Laboratory, Shanghai, China\nclassiﬁer on the top of the model. Former studies put more\nemphasis on learning the base model. Over the last decade,\nfor example, the dominant approach involved training models\non the ImageNet (Deng et al., 2009) dataset and subse-\nquently transferring them to downstream tasks. Owing to\nthe dramatically increasing computational capacity, general-\nproposed pre-trained models with several magnitudes more\nparameters and FLOPs have been successfully trained in both\nfull-/semi-supervised (Sun et al., 2017) and self-supervised\n(He et al., 2020, 2022) style. Recently, contrastive vision-\nlanguage models (Radford et al., 2021; Jia et al., 2021a; Y uan\net al., 2021) have garnered increasing interest as pre-training\nmodels in transfer learning due to their superior capabilities\nand effectiveness for visual recognition tasks. These models,\nwhich beneﬁt from the knowledge of the language modality,\nhave shown improved performance on various visual tasks,\nsuch as zero-shot classiﬁcation (Radford et al., 2021), cap-\ntioning (Mokady et al., 2021), and image generation (Ramesh\net al., 2021), to name a few.\nIn this study, we aim to enhance the transferability of\nvision-language pre-training models for downstream visual\nrecognition tasks by revisiting the knowledge-transferring\nprogress from the perspective of the classiﬁer. Speciﬁ-\ncally, we examine the properties of the pre-training models,\nand propose a simple yet effective paradigm to enhance\n123\nInternational Journal of Computer Vision (2024) 132:392–409 393\nFig. 1 Inter-class correlation maps of “embeddings of class labels” for 20 categories on Kinetics-400. Left: The extracted textual vectors of class\nlabels, Right: The “embeddings” from learned classiﬁer. The color thresholds are adjusted for a better view. Please zoom in for the best view\ntheir transferability. Our ﬁndings demonstrate that these\npre-training models hold three essential properties for our\nparadigm: (i) Semantic-rich representations , which are\nobtained by training the models with extensive weakly-\nrelated image-text sample pairs using large neural network\narchitectures. In contrast to supervised-style models learned\non standard image-label datasets, the semantic-rich rep-\nresentations are expected to contain more semantics and\ndiverse representations of concepts, which is crucial in the\nunknown target domain settings. (ii) Modality alignment ,\nwhich aligns the representation vectors from a paired sam-\nple’s visual and textual modality in semantic embedding\nspace. This property provides an advantage in the initializa-\ntion when the samples for downstream tasks are limited, i.e.,\nin the zero-/few-shot scenarios, compared to the visual-only\nclassiﬁer ﬁne-tuning approach. (iii) Intra-modality corre-\nlations. The contrastive training algorithm also provides\nweak intra-modality correlations. That is, the representation\nvectors of similar images or texts are close to each other\n(Radford et al., 2021; Sun, 2022). In contrast to the aforemen-\ntioned properties, intra-modality correlations from samples’\ninﬂuence are often overlooked. Concisely, a classiﬁer with\nappropriately correlated targets rather than one-hot labels\nlearns faster and performs better.\nTo demonstrate the importance of appropriately corre-\nlated classiﬁer targets, we conduct a toy experiment to depict\nthe intra-modality correlations in two scenarios. We employ\nthe Kinetics video recognition dataset (Kay et al., 2017)\nfor the analysis (The detailed conﬁgurations are provided in\nSect. 4.3). In the ﬁrst scenario, we extract the textual embed-\nding vectors of the name of class labels using the textual\nencoder of CLIP (Radford et al., 2021) and then calculate\nthe correlation among the textual embedding vectors. In the\nsecond scenario, we examine the ﬁnal projection head of a\nvanilla ﬁne-tuning framework. Precisely, we learn a classi-\nﬁer based on the visual encoder from the same CLIP model.\nThe projection head of the classiﬁer is a matrix of d ×c used\nto compute the pre-softmax logits, from the d-dimensional\nfeature vectors for the c classes. Therefore, we treat the d-\ndimensional row vectors as the “embeddings” of the class\nlabels. This non-rigorous setting allows us to explore the\nintra-modality correlation between these learned “embed-\ndings”. The results are plotted in Fig. 1. While we could\nobserve clear correlations among the embeddings of category\nnames since some of them contain the same keywords ( e.g.,\nplaying <something>.) Interestingly, in the second scenario,\nthese learned “embeddings” also reveal a similar correlation\nmap after the training, despite being initialized randomly and\noptimized without knowing any textual information (That is,\noptimized with the cross-entropy loss with one-hot labels).\nIn summary, we take full advantage of the large-scale con-\ntrastive image-language pre-trained models and build a novel\ngeneral paradigm for the transfer learning settings. Our main\ncontributions are as follows:\n– We revisit the transfer learning pipeline from the per-\nspective of classiﬁers and spot that properly correlated\ntargets, and pre-aligned semantic knowledge are crucial\nfor downstream visual recognition tasks.\n– We build a new paradigm to transfer textual knowledge\nfor visual recognition using contrastively pre-trained\nvision-language models. Our paradigm accelerates the\ntransfer learning progress while taking full advantage of\nthe pre-trained models.\n– Comprehensive experiments are conducted on 17 visual\ndatasets that span three distinct data domains: image,\n123\n394 International Journal of Computer Vision (2024) 132:392–409\nvideo, and 3D point cloud. For video recognition, we\nevaluate our model on 6 well-known video benchmarks,\nincluding single-label and multi-label recognition. while\nalso verifying its effectiveness in zero-shot and few-shot\nscenarios. For image classiﬁcation, we perform exper-\niments on 10 different image datasets, and the results\ndemonstrate that our method is an effective few-shot\nlearner. For 3D point cloud recognition, we validate\nour method on the ModelNet40 dataset, and ﬁnd that\nit outperforms the vision-only paradigm by a signiﬁcant\nmargin.\n– We open-source our code and models at https://github.\ncom/whwu95/Text4Vis.\n2 Related Works\n2.1 Visual Recognition Tasks and Transfer Learning\nVisual recognition is one of the most important tasks in the\ndesign of machine learning systems. From the perspective\nof the visual backbone, we could roughly divide the evolu-\ntion of the system into two eras: i) The Convolutional Neural\nNetwork (CNN) based architectures for image (Krizhevsky\net al., 2012; He et al., 2016; Simonyan & Zisserman, 2014;\nIoffe & Szegedy, 2015) or video recognition (Carreira & Zis-\nserman, 2017; Qiu et al., 2017; Xie et al., 2018; Tran et al.,\n2018; Wu et al., 2021a, b). ii) The Vision Transformer (ViT)\nbased architectures for image (Dosovitskiy et al., 2020;H a n\net al., 2021; Liu et al., 2021) or video recognition (Berta-\nsius et al., 2021; Arnab et al., 2021; Liu et al., 2022;F a n\net al., 2021). As ViT models are challenging to train from\nscratch without large-scale datasets, transfer learning tech-\nniques have regained popularity.\nTransfer learning aims to enhance target learners’ perfor-\nmance on target domains by transferring knowledge from\nrelated but different source domains (Tan et al., 2018; Ribani\n& Marengoni, 2019; Zhuang et al., 2020), thereby reduc-\ning the requirements of target domain data for learning the\ntarget model. A typical transfer learning system is built\nwith a pre-trained model trained with source domain data\nand a classiﬁer for the target domain data. This study dis-\ncusses a sub-family of transfer learning systems that utilize\nlarge-scale task-agnostic models. Related studies on this sub-\nfamily are discussed in Sect. 2.3.\n2.2 Image-Language Pre-training\nThe recent success of Contrastive Language-Image Pre-\nTraining (CLIP) (Radford et al., 2021) has paved the way for\ncoordinated vision-language pre-training models utilizing\nthe image-text InfoNCE contrastive loss (V an den Oord et al.,\n2018). After that, several works have since been proposed that\ncombine various learning tasks, including image-text match-\ning and masked image/language modeling, such as ALIGN\n(Jia et al., 2021b), BLIP (Li et al., 2022b), Florence (Y uan\net al., 2021), and CoCa (Y u et al., 2022). These contrastively\nlearned models exhibit two essential properties for down-\nstream tasks: rich visual feature representations and aligned\ntextual feature representations. Another recent study (Yang et\nal., 2022) has incorporated the downstream classiﬁcation task\ninto the pretraining process, resulting in improved accuracy\nover the standard cross-entropy loss. These developments\ndemonstrate the potential for coordinated pre-training of\nvision and language models and open up exciting opportuni-\nties for further advances in vision-language understanding.\n2.3 Transferring CLIP for Downstream Tasks\nThe transfer of pre-trained CLIP to downstream tasks is a\nrecent and emerging research direction. Several recent stud-\nies (Gao et al., 2021; Zhang et al., 2021b; Zhou et al., 2021,\n2022) have investigated the efﬁcient transfer of pre-trained\nCLIP to downstream image recognition tasks. In addition,\nCLIP has been leveraged to enhance dense prediction tasks\nsuch as object detection (Rao et al., 2022) and segmentation\n(Lüddecke & Ecker, 2022; Li et al., 2022a). In the video\ndomain, CLIP has also beneﬁted many text-video retrieval\nmethods (Zhao et al., 2022; Luo et al., 2021). For video recog-\nnition, ActionClip (Wang et al., 2021b) and VideoPrompt\n(Ju et al., 2022) extend CLIP (Radford et al., 2021)t ot r a i n\na downstream video-text matching model with contrastive\nloss and utilize the similarity between learned video and text\nembeddings during inference. Other methods, such as ST-\nAdapter (Pan et al., 2022) and EVL (Lin et al., 2022b), use\nonly the visual encoder for unimodality transferring with-\nout involving textual knowledge. This study investigates the\ncorrelation between the linear classiﬁer and efﬁcient feature\ntransfer in the standard visual recognition paradigm. We pro-\npose a direct transfer of visual and textual knowledge for\nvisual recognition, without using contrastive-based methods.\n3 Methodology\n3.1 Denotations\nIn this paper, we use bold letters to denote Vector,w h i l e\ncapital italic letters are used to denote Tensor or Matrix.\nFor example, we use z ∈ Rd to denote the feature vec-\ntor extracted from a pre-trained model of dimension d, and\nW ∈ Rd×c to denote the projection matrix for the c-class\nlinear classiﬁer. Without ambiguity, we also use capital italic\nletters to denote the modality in subscripts. Speciﬁcally, we\nuse V and T to denote the Visual modality and the Textual\nmodality, respectively. We also use lowercase italic letters to\n123\nInternational Journal of Computer Vision (2024) 132:392–409 395\nFig. 2 Illustration of transferring vision-language pre-trained models\nfor visual recognition. a The widely-used standard vision-only tuning\nparadigm with cross-entropy loss. b The vision-language contrastive\nlearning paradigm with contrastive loss, e.g., CLIP (Radford et al.,\n2021), ActionCLIP (Wang et al., 2021b). c Revisiting the role of the clas-\nsiﬁer to transfer knowledge from vision-language pre-trained models.\nc denotes the number of categories, b is the batch size and d represents\nthe dimension of embeddings\ndenote functions or neural networks, such as gV (·,Θ V ) and\ngT (·,Θ T ), which represent the visual and textual encoders,\nrespectively. Furthermore, we employ calligraphic letters,\nsuch as D, to denote sets of elements.\n3.2 Revisiting of Existing Learning Paradigms\nStandard Transfer Learning Paradigm In Fig. 2a, we\ndepict the conventional scenario, where a visual encoder\nmodel gV is trained on a large-scale dataset D containing\nvisual samples, with or without ground-truth labels. On our\nlabeled downstream dataset ˜D ={ (x1, y1), (x2, y2) ,... },\nour empirical learning target can be expressed as\ng∗\nV , W ∗ = argmin\nΘV ,W\nEx,y∼ ˜D\n[\nH(y|σ(W · gV (x)))\n]\n, (1)\nwhere H( ˆp|p) represents the CrossEntropy between the\npredicted distribution p and the ground-truth distribution ˆp.\nThe symbol σ denotes the softmax operation, W ∈ Rc×d\ndenotes the linear projection matrix for classiﬁcation. The\nformulation in Eq. 1 is a standard visual feature transferring\nparadigm, where the visual encoder gV and the projection\nmatrix W are learned jointly.\nVision-Language Contrastive Learning Paradigm As\nshown in Fig. 2b, we then review the contrastive learn-\ning paradigm of vision-language models, which has gained\nwidespread use in vision-language pre-training, such as CLIP\n(Radford et al., 2021), and extended to video-text ﬁne-tuning,\ne.g., ActionCLIP (Wang et al., 2021b), CLIP4Clip (Luo et\nal., 2021).\nGiven a dataset D ={ (x\nV ,1, x T ,1), (x V ,2, x T ,2),···} ,\nconsisting of weakly related vision-language pairs ( e.g.,\nimage-text, video-text). With slight abuse of the notations,\nwe employ the x\nV , x T to denote a mini-batch of size b, then\nwe minimize the following target:\ng∗\nV , g∗\nT = argmin\nΘV ,ΘT\nEx V ,x T ∼ ˜D\n[\nH(Q|σ(gV (x V )T · gT (x T )))\n]\n,\n(2)\nwhere Q is the set that contains b one-hot labels of size c, with\ntheir 1,2,..., b-th element being 1 ( b < c), representing the\npositive vision-language pairs. We note that the deﬁnition in\nEq. 2 is not the rigorous form of the Noise-Contrastive Esti-\nmation (NCE) loss proposed in V an den Oord et al. ( 2018).\nInstead, we employ the cross-entropy version implementa-\nt i o nu s e di nR a d f o r de ta l .( 2021); Chen et al. ( 2021). The\ncontrastive learning paradigm ﬁrst projects the visual feature\ng\nV (x V ) with a projection matrix gT (x T ), then follows the\nstandard transfer learning paradigm to match the similarity\nmatrix with the diagonal label set Q.\n3.3 Our Proposed Paradigm\nAs depicted in Fig. 2, we propose a more generalized\nparadigm by replacing the learnable, randomly initialized\nlinear projection matrix W with a pre-deﬁned matrix ˜W ,\nbuilding upon the classiﬁer perspective. Following Sect. 3.2,\nthe training target can be formulated as:\ng∗\nV = argmin\nΘV\nEx,y∼ ˜D\n[\nH(y|σ( ˜W · gV (x)))\n]\n. (3)\n123\n396 International Journal of Computer Vision (2024) 132:392–409\nFig. 3 Illustration of 6 types of\nprojection matrix initialization\nwhich develop different levels of\ncorrelation between the target\nembedding vectors. On the Left:\na trivial or no correlation\nbetween the target vector; b\ncorrelation calculated from the\nvisual statistic; c correlation\ncalculated from the textual\nsemantic knowledge. On the\nRight: Inter-class correlation\nmap obtained from the six types\nof initialization. Impressively,\ncorrelation maps yield a similar\nappearance from transferring\nvisual statistics and textual\nsemantic knowledge. See Fig. 1\nfor more details\nIn the following subsections, we investigate different initial-\nization methods for ˜W .\n3.4 Discussion on Initialization\nTo investigate the extent to which the correlation between\nsemantic information contained in the samples is helpful,\nwe examine several types of initialization, which represent\ndifferent degrees of intra-modality (or inter-class from the\nperspective of classiﬁer) correlation, as illustrated in Fig. 3.\n3.4.1 Trivial Inter-class Correlation\nRandomized Matrix We start with the simplest initialization\nmethod, which involves setting each row of ˜W to a random\nGaussian vector with zero mean and standard deviation. This\ncan be denoted as follows:\n˜W ∼ N (0, I\nd ), (4)\nwhere Id denotes the identity matrix of dimension d × d.\nWhile this method generates trivial correlations between the\nrows of ˜W due to its stochasticity, these correlations cannot\nreﬂect the actual correspondence between the visual classes.\nTherefore, we expect the model to have inferior performance\nsince it needs to avoid these incorrect correlations when\nlearning the visual feature representation.\nRandomized Orthogonal Matrix Next, we consider the\ncase where correlations are removed from the projection\nmatrix. We follow the approach of the randomized matrix\nand then remove the correlation by ensuring that the row vec-\ntors are orthogonal. This is achieved by QR decomposition.\nConcretely, since d > c, we ﬁrst generate a random matrix\nof size d × d and select the ﬁrst c rows as our projection\nmatrix. Formally, we have,\n˜W\nj ∼ QR(U)j , j = 1,2,..., c,\nUi ∼ N (0, Id ),i = 1,2,..., d,\n(5)\nwhere U is the intermediate randomized matrix, and QR (U)\nis the row orthogonal matrix obtained through the QR decom-\nposition. Similar to the randomized matrix, we expect this\ninitialization to have inferior performance. Since the one-hot\nlabel vectors are also orthogonal to each other, it will not be\nhelpful to project the visual feature vectors with an orthog-\nonal matrix, which may increase the difﬁculty of learning\nmeaningful visual features.\n3.4.2 Correlation from Visual Statistic Knowledge\nClass Center Projection To utilize the visual encoder’s sta-\ntistical knowledge, we randomly select a small subset of\nlabeled samples from the training dataset. For our exper-\niments on the Kinetics-400 dataset, we sample 60 videos\nfrom each class, which is approximately 10% of the train-\ning data. Next, we compute the mean value of each class’s\nvisual embeddings extracted from the visual encoder. These\nmean vectors are treated as the centers for each class and are\nused to initialize the classiﬁer’s parameters. The class cen-\nter initialization provides a basic approximation of the visual\nknowledge obtained from the pre-trained model. However,\nits effectiveness largely depends on the data used to compute\nthe projection matrix, and when the data is limited, the esti-\nmated correlation among visual embeddings may be biased.\nLinear Discriminant Projection We propose another approach\nto initializing the projection matrix using visual statistics. We\nuse multi-class Fisher’s linear discriminant analysis (LDA)\nto learn a linear classiﬁer and employ the weight matrix of\nthe classiﬁer as our initialization for the projection matrix.\n123\nInternational Journal of Computer Vision (2024) 132:392–409 397\nSpeciﬁcally, we ﬁrst use the same visual embeddings as\nthe previous approach to computing the LDA coefﬁcient,\nfollowing previous work (Li et al., 2006). Then, we use\nthe LDA coefﬁcient to initialize ˜W and freeze it for ﬁne-\ntuning the visual encoder on the dataset. Intuitively, the\nLDA simultaneously maximizes the inter-class covariance\nand minimizes intra-class covariance. Therefore, we term this\nas the maximal correlation initialization using visual statistic\nknowledge. However, the linear discriminant projection also\nsuffers from biased data sampling progress.\n3.4.3 Correlation from Textual Semantic Knowledge\nTextual Embedding Vectors We now describe how we\ntransfer textual semantic knowledge from a pre-trained tex-\ntual encoder to initialize the projection weight ˜W .G i v e na\nset of tokenized class labels L = l\n1, l 2,..., l c, we initialize\nthe i-th row vector in ˜W as follows:\n˜Wi ∼ gT (l i ), i = 1,2,..., c, (6)\nwhere gT is a function that maps a textual input to an embed-\nded feature vector using a pre-trained textual encoder. In\nour experiments, we investigate two types of textual fea-\nture encoders: i) The encoder that is trained solely using\ntextual samples on tasks such as masked language modeling,\ni.e., DistilBERT (Sanh et al., 2019); ii) The encoder that is\ntrained with a visual encoder in the contrastive style, i.e.,\nCLIP (Radford et al., 2021). Using the textual embeddings\nto initialize ˜W allows us to roughly pre-align the visual and\ntextual embeddings in the same embedding space.\n3.5 Discussion on Parameter Frozen\nIt is worth mentioning that, in our paradigms, ˜W is not in the\noptimization targets. This means we freeze it from updating\nduring the ﬁne-tuning of the downstream tasks. We have the\nfollowing reasons for this: ﬁrstly, since the textual knowledge\nis extracted by the textual encoder, freezing this part could\nsigniﬁcantly decrease the computational resources required\nfor ﬁne-tuning. As we showed in Sect. 4, freezing the parame-\nters of ˜W leads to a decrease in the training period. Secondly,\nfreezing the parameter helps to reduce biases brought by\nthe limited semantic knowledge of class names. By keeping\nthe feature embeddings distributed as they were learned on\nlarge-scale datasets, we improve the diversity of the represen-\ntations and the learning stability. Finally, this conﬁguration\nalso compares former studies that employ textual information\nfor vision transfer learning.\n4 Experiments: Video Recognition\nIn this section, we transfer the image-language pre-trained\nmodel to the video modality, i.e., the video recognition\ntask. To evaluate the effectiveness of the transferred model,\nwe conduct experiments on six well-known video datasets,\nwhich include both trimmed and untrimmed video data.\nSpeciﬁcally, the datasets are Kinetics-400 & 600 (Kay et\nal., 2017; Carreira et al., 2018), UCF-101 (Soomro et al.,\n2012), HMDB-51 (Kuehne et al., 2011), ActivityNet-v1.3\n(Caba Heilbron et al., 2015), and Charades (Sigurdsson et\nal., 2016). These datasets are selected to represent a wide\nrange of video recognition tasks, and are commonly used as\nbenchmarks in this ﬁeld.\nWe evaluate the transferred model in three distinct sce-\nnarios: zero-shot, few-shot, and regular video recognition.\nIn the zero-shot scenario, the model has not trained on the\ntarget dataset but is evaluated on it, allowing us to assess its\nability to generalize to new data. In the few-shot scenario,\nthe model is trained on a small subset of the target dataset\nand evaluated on the validation set, enabling us to explore\nits capacity to learn from limited labeled data. In the typical\nrecognition scenario, the model is trained on the entire target\ndataset and evaluated on the validation set, allowing us to\nmeasure its performance in a standard supervised learning\nconﬁguration. By evaluating the model in these three sce-\nnarios, we aim to provide a comprehensive assessment of its\nperformance under different conditions.\n4.1 Training\nThe video recognition task takes a video as input, then feeds\nit into a learned encoder to estimate the action category of the\nvideo. Given a video, we ﬁrst uniformly sample T (e.g., 8,\n16, 32) frames over the entire video. Then we utilize ResNet\n(He et al., 2016) or ViT (Dosovitskiy et al., 2020) as the video\nencoders. The classiﬁer in our paradigm is initialized from\nthe textual embedding of the class names and then frozen\n(ﬁxed), leaving only the parameters in the video encoder to\nbe learned.\nDefault Training Recipe Table 1 presents our training\ndetails for regular video recognition. We share the same\nrecipe on all the video datasets, i.e., Kinetics-400, Activi-\ntyNet, HMDB-51, UCF-101, and Charades.\nFew-Shot Video RecognitionAll training strategies employed\nin the training process are consistent with those presented in\nTable 1, with only one modiﬁcation: the number of epochs\nwas increased to 100.\nZero-Shot Video Recognition We use the Kinetics-400\npre-trained models to directly perform cross-dataset zero-\nshot video recognition without any additional training on\nother datasets, i.e., ActivityNet, HMDB-51, UCF-101 and\nKinetics-600.\n123\n398 International Journal of Computer Vision (2024) 132:392–409\nTable 1 Default training details for video recognition\nSetting V alue\nTraining hyper-parameter\nBatch size 256\nV ocabulary size 49408\nTraining epochs 30\nOptimizer AdamW\nLearning rate (base, minimal) (5e-5, 5e-6), cosine\nWeight decay 0.2\nLinear warm-up epochs 5\nAdam β\n1,β2 0.9, 0.999\nAugmentation\nResize RandomSizedCrop\nCrop size 224 (Default)\nRandom ﬂip 0.5\nRandom grayscale 0.2\nRandAugment N = 2, M = 9\n4.2 Inference\nTo trade off accuracy and speed, we consider two inference\nstrategies: (1) Single View: This strategy involves using only\na single clip per video and the center crop for efﬁcient evalua-\ntion, as shown in Table 4.3. (2) Multiple Views: This strategy,\nwhich is widely used in previous works, involves sampling\nmultiple clips per video with several spatial crops to improve\naccuracy. For comparison with state-of-the-art approaches,\nwe use four clips with three crops (“4 ×3V i e w s ” ) .\n4.3 Ablation Studies\nIn this section, we conduct extensive ablation experiments\non the Kinetics-400 dataset. Unless speciﬁed otherwise, we\nuse ViT-B/16 with 8 frames as the video backbone and a\nsingle view for testing. The default settings are marked in\nbold italics .\nDifferent Initializations to the Ofﬂine Classiﬁer We ﬁrst\nexamine how the initializations affect the learning of classi-\nﬁers. Then, we prepare our controlled environment using a\nclassiﬁer with parameters W ∈ R\nd×c, which is built on the\naverage of pooled temporal feature representations of all the\nframes. According to Sect. 3.3, we evaluate the performance\nof six types of initializations on both the few-shot and full-\nshot settings. For reference, we also provide the results using\nthe standard vision-only ﬁne-tuning ( i.e., online) classiﬁer\nwith trainable weights.\nTable 2 lists the results. Feeding the ofﬂine classiﬁer a\nrandom d-by-c matrix with a normal distribution leads to sig-\nniﬁcantly reduced performance. Furthermore, removing the\nclassiﬁer’s intra-modality correlation also results in inferior\nperformance. From this family of initialization, we under-\nstand the necessity of a proper correlation in the classiﬁer\ntargets. Next, we observe that providing correlation informa-\ntion using a small labeled sub-set from the visual side leads to\nimproved performance, with the classiﬁer no longer guessing\nthe results in the few-shot scenario and learning reasonably\nwell in the full-shot scenario, compared to the vision-only\nonline classiﬁer. Compared to learnable correlation, pre-\nextracted proper correlation provides a more explicit target,\nmaking it a more efﬁcient approach, especially in the pro-\ncess of transfer learning, particularly in few-shot learning.\nNotably, the class center initialization performs better than\nthe LDA initialization in the few-shot scenario, demonstrat-\ning the CLIP encoder has a naturally well-distributed feature\nembedding.\nFinally, we investigate the effect of the textual seman-\ntic family of initialization on the classiﬁer’s performance.\nWe observe that the embeddings from the textual encoder of\nCLIP signiﬁcantly improve the few-shot and full-shot accu-\nracy. Interestingly, the DistilBERT-based initialization also\nperforms remarkably well despite the semantics not being\ndirectly aligned with the visual modality. This result can\nbe explained by the fact that both DistillBERT and CLIP\nare pre-trained with large-scale data and have strong lan-\nguage modeling capabilities, allowing them to generate good\nsemantic targets. Therefore, we conclude that the visual\nTable 2 The effects of different initializations for the frozen (ofﬂine) classiﬁers\nOfﬂine classiﬁer Alignment Correlation Few-shot Acc. Full-shot Acc.\nRandom normal vectors ✗ Random 0.6 58.7\nRandom orthogonal vectors ✗ Non-correlation 0.6 57.7\nLinear discriminant projection ✗ Visual statistics 25.5 79.6\nClass center ✗ Visual statistics 32.3 79.0\nDistilBERT ✗ Textual semantic 32.2 77.8\nTextual encoder of CLIP ✓ Textual semantic 65.3 80.1\nVision-only (online) ✗ Learnt weight 21.6 75.3\nAlignment denotes if the cross-modality knowledge is pre-aligned. Correlation shows the source of the correlation among the class embeddings\n123\nInternational Journal of Computer Vision (2024) 132:392–409 399\nTable 3 Temporal modeling for video encoders\nBackbone Modeling Top-1 Top-5\nResNet-50 TAP 71.2 90.4\nT1D 67.2 88.5\nT-Trans 74.3 91.7\nVIT-B/16 TAP 80.1 95.0\nTokenT1D 80.4 95.0\nT-Trans 81.5 95.5\nembeddings beneﬁt from the correlation of semantic tar-\ngets, and the extract alignment further boosts the learning\nprogress, reducing the need for a large number of samples. 1\nWe also provide the visualizations of these classiﬁers in\nFig. 3. Apparently, the latter two families of initializations\nshare the same patterns among the correlation maps, which\ncould be easily distinguished from the random and orthogo-\nnal ones.\nTemporal ModelingIn this study, we explore several tempo-\nral modeling strategies for both ViT and ResNet, including:\n1. TAP: Temporal average pooling is a straightforward tem-\nporal modeling strategy that provides a simple baseline\nfor comparison.\n2. T1D: Channel-wise temporal 1D convolutions, which are\ncommonly used in previous works (Wu et al., 2021a;\nWang et al., 2021a; Liu et al., 2020), are employed to\nfacilitate efﬁcient temporal interaction in the later stages\n(res\n4−5)o fR e s N e t .\n3. T-Trans: This strategy involves feeding the embeddings\nof frames to a multi-layer ( e.g., 6-layer) temporal trans-\nformer encoder.\n4. TokenT1D: This approach involves using T1D to model\ntemporal relations for the [class] token features that are\naggregated from local features via attention in the vision\ntransformer. We apply TokenT1D to multiple positions\nof a vision transformer to model temporal dependencies\namong the tokens.\nOur experimental results are presented in Table 3.W e\nobserved that on both ViT and ResNet backbones, TAP pro-\nvides a simple baseline for temporal modeling, and T-Trans\nachieves the best top-1 accuracy. Interestingly, we found that\nT1D does not appear to be effective in this scenario. This\ncould be due to the potential for T1D to disrupt the strong\nrepresentations learned by CLIP . In contrast, TokenT1D is\nanother internal-backbone temporal modeling strategy that\nmodiﬁes only the global [class] token features instead of\npatch features. We observed that TokenT1D does not lead\n1 We also observe that the loss of DistillBERT is initially higher than\nthat of CLIP but quickly decreases to the same level.\nto a performance drop and even slightly improves the TAP\nbaseline. We believe that this is because TokenT1D results\nin minimal modiﬁcations to the pre-trained features, which\nallows the model to retain the learned representations while\nincorporating temporal dependencies among the tokens.\nOurs v.s. Contrastive-Based Paradigm we compare our\nproposed approach with the contrastive-based tuning method\nActionClip (Wang et al., 2021b), which is introduced in\nSect. 2.2. This paradigm treats the video recognition task as\na video-text matching problem with a contrastive loss, which\nrequires batch gathering to collect embeddings of all batches\nacross all GPUs and calculate cosine similarity for a given\nbatch across all other batches.\nTo ensure a fair comparison, we follow the ofﬁcial code\nand conﬁgurations from ActionClip (Wang et al., 2021b)\nin our experiments. In contrast to the contrastive-based\nparadigm, our recognition paradigm uses the Cross-Entropy\nloss to train the model, and we employ pre-extracted text\nembeddings as our classiﬁer. Thus, the only learned part\nin our paradigm is the visual encoder, whereas the pre-\ntrained textual encoder still needs to be updated in the\nContrastive-based paradigm, requiring larger GPU memory.\nIn Table 4, we compare our approach with the contrastive-\nbased paradigm and observe that the latter performs poorly\nwithout batch gathering. This is because contrastive learning\nfavors a large batch size, e.g., CLIP (Radford et al., 2021)\nused 256 GPUs with a batch size of 128 per GPU to maintain\na large 32768 ×32768 similarity matrix. Moreover, involving\nbatch gathering will multiply the training time.\nOur results demonstrate that our proposed approach\nachieves the best accuracy-cost trade-off. Speciﬁcally, our\nmethod achieves a performance of 81.5% with ViT-B/16,\nwhich takes only 10 h to run the training using 8 GPUs and\nis 2× faster than the matching counterpart. Our approach is\nmore efﬁcient and effective for video recognition tasks, espe-\ncially in applications with limited computational resources.\nPlease refer to Appendix § A.2 for further details on batch\ngathering.\nAdditionally, in order to mitigate the impact of dif-\nferent implementation details, we have incorporated the\ncontrastive-style training loss function based on our code. As\nobserved in Table 5, training with contrastive loss introduces\na reduction in training efﬁciency without signiﬁcant perfor-\nmance improvement. Moreover, we have further enhanced\nthe performance by incorporating a ﬁxed ofﬂine classiﬁer\nin the contrastive-style approach. This improvement can be\nattributed to the accelerated convergence achieved by the\nﬁxed textual target during training.\nText Input Forms We investigate several text input forms in\nTable6, including class names, single hard template, multiple\nhard templates, and learnable templates. The details are as\nfollows:\n123\n400 International Journal of Computer Vision (2024) 132:392–409\nTable 4 Ours vs. Contrastive-based paradigm with ViT-B/16 on Kinetics-400\nParadigm Contrastive Batch\nGather\nTextual\nEncoder\nTop-1 V100-days\nActionCLIP (Wang et al., 2021b) ✓✓ /unlock 81.2 6.7 (10 ∗)\n✓ ὑ2 80.7 6.6\n✗ /unlock 77.8 3.5\n✗ ὑ2 76.1 3.3\nOurs ✗✗ ὑ2 81.5 3.3\nTable 5 More ablations on contrastive-style paradigm\nContrastive Ofﬂine classiﬁer Top-1(%) Training time\n✗✗ 81.5 1 ×\n✓✗ 81.4 2 ×\n✓✓ 81.7 2 ×\nTable 6 Study on various text input forms\nText input form Top-1\nClass name 81.4\n“A video of a person” + class name 81.5\nMultiple ﬁxed templates + class name 80.9\nLearnable template + class name 81.2\n1. Class name. To generate textual embeddings, we utilize\nthe category names of the dataset as the text input, such as\n“eating hotdog” or “driving car” . The results show that\nusing only the label text can yield good performance.\n2. Single hard template. We use a hand-crafted template, “a\nvideo of a person {class name}.” as input. This template\nonly slightly improves performance over the label text’s\nbaseline.\n3. Multiple hard templates. CLIP\n2 provides 28 templates\nfor Kinetics, including the single template described\nabove. During training, we use these templates as text\naugmentation by randomly selecting one at each iteration.\nThen, we evaluate the model using the single template as\ninput. The performance decreases by 0.6% on Kinetics-\n400, which may be because various prompt templates\nintroduce extra noise during training.\n4. Learnable templates. We use the automated prompt\nCoOp (Zhou et al., 2021) to describe a prompt’s context\nusing a set of learnable vectors. Speciﬁcally, the prompt\ngiven to the text encoder is designed with the following\nform:\nt =[ V]\n1[V]2 ... [V]M [class name ], (7)\n2 https://github.com/openai/CLIP/blob/main/data/prompts.md .\nTable 7 Analysis on throughput\nMethod Top-1 FLOPs Params Throughput\nViViT-L/16-320 81.3 3992G 310.8M 4.2 vid/s ∗\nOurs ViT-B/32 78.5 23.7G 71.6M 322.5 vid/s\nOurs ViT-B/16 81.5 90.3G 69.9M 126.5 vid/s\nOurs ViT-L/14 85.4 415.4G 230.4M 35.5 vid/s\n\"vid/s” denotes the average number of videos processed per second. A\nhigher value of “vid/s” corresponds to greater efﬁciency. ∗ represents\nthe ofﬁcial result with TPU-v3\nwhere each [V]m (m ∈{1,..., M}) is a vector of the same\nsize as word embeddings, and M is the number of context\ntokens. We set M to 4 in our experiments.\nOur results suggest that different templates have little impact\non our model’s performance.\nComputational Cost and Efﬁciency Table 7 presents our\nmodels’ computational cost and efﬁciency, measured in terms\nof throughput using a single NVIDIA A100 GPU with a batch\nsize of 16, which aligns with standard inference settings. Our\nmodels exhibit a 29× faster throughput, and 44× fewer\nFLOPs than the previous transformer-based method ViViT\n(Arnab et al., 2021), while maintaining the same accuracy.\nThese results conﬁrm the high efﬁciency of our approach.\n4.4 Main Results\nRegular Video Recognition We evaluate the performance\nof our model on the Kinetics-400 dataset, a challenging\nbenchmark for regular video recognition. Table 8 provides a\ncomparison of our model with state-of-the-art methods that\nwere pre-trained on large-scale datasets such as ImageNet-\n21K (Deng et al., 2009), IG-65 M (Ghadiyaram et al.,\n2019), JFT-300 M (Sun et al., 2017), FLD-900 M (Y uan et\nal., 2021), and JFT-3B (Zhai et al., 2021). To date, none\nof the three largest datasets (JFT-300 M, FLD-900 M, and\nJFT-3B) are open-sourced, and pre-trained models are not\nprovided. Hence, we utilized the publicly available CLIP\n(Radford et al., 2021) checkpoints, which have been trained\non 400 million web image-text pairs (WIT-400 M). Signif-\n123\nInternational Journal of Computer Vision (2024) 132:392–409 401\nTable 8 Comparison with previous works on Kinetics-400\nMethod Input Pre-train Top-1 Top-5 FLOPs ×Views Param\nNL I3D-101 (Wang et al., 2018b) 128 ×2242 IN-1K 77.7 93.3 359 ×10×3 61.8\nMVFNetEn (Wu et al., 2021a)2 4 ×2242 IN-1K 79.1 93.8 188 ×10×3–\nSlowFast NL101 (Feichtenhofer et al., 2019)1 6 ×2242 Scratch 79.8 93.9 234 ×10×3 59.9\nX3D-XXL (Feichtenhofer, 2020)1 6 ×4402 Scratch 80.4 94.6 144 ×10×3 20.3\nMViT-B, 64×3( F a ne ta l . , 2021)6 4 ×2242 Scratch 81.2 95.1 455 ×3×3 36.6\nMethods with large-scale pre-training\nTimeSformer-L (Bertasius et al., 2021)9 6 ×2242 IN-21K 80.7 94.7 2380 ×1×3 121.4\nViViT-L/16×2( A r n a be ta l . ,2021)3 2 ×3202 IN-21K 81.3 94.7 3992 ×4×3 310.8\nVideoSwin-L (Liu et al., 2022)3 2 ×3842 IN-21K 84.9 96.7 2107 ×10×5 200.0\nip-CSN-152 (Tran et al., 2019)3 2 ×2242 IG-65M 82.5 95.3 109 ×10×3 32.8\nViViT-L/16×2( A r n a be ta l . ,2021)3 2 ×3202 JFT-300M 83.5 95.5 3992 ×4×3 310.8\nViViT-H/16×2( A r n a be ta l . ,2021)3 2 ×2242 JFT-300M 84.8 95.8 8316 ×4×3 647.5\nTokLearner-L/10 (Ryoo et al., 2021)3 2 ×2242 JFT-300M 85.4 96.3 4076 ×4×3 450\nMTV-H (Yan et al., 2022)3 2 ×2242 JFT-300M 85.8 96.6 3706 ×4×3–\nC o V e R( Z h a n ge ta l . ,2021a)1 6 ×4482 JFT-300M 86.3 – – ×1×3–\nFlorence (Y uan et al., 2021)3 2 ×3842 FLD-900M 86.5 97.3 - ×4×3 647\nC o V e R( Z h a n ge ta l . ,2021a)1 6 ×4482 JFT-3B 87.2 - – ×1×3–\nVideoPrompt ViT-B/16 (Ju et al., 2022)1 6 ×2242 WIT-400M 76.9 93.5 – –\nActionCLIP ViT-B/16 (Wang et al., 2021b)3 2 ×2242 WIT-400M 83.8 96.2 563 ×10×3 141.7\nST-Adapter ViT-L/14 (Pan et al., 2022)3 2 ×2242 WIT-400M 87.2 97.6 8248 –\nEVL ViT-L/14 (Lin et al., 2022b)3 2 ×2242 WIT-400M 87.3 – 8088 –\nEVL ViT-L/14 (Lin et al., 2022b)3 2 ×3362 WIT-400M 87.7 – 18196 –\nOurs ViT-L/14 32 ×2242 WIT-400M 87.6 97.8 1662 ×4×3 230.7\nOurs ViT-L/14 32 ×3362 WIT-400M 88.4 98.0 3829×4×3 230.7\nOurs ViT-L/14 32 ×3362 Merged-2B 89.4 98.1 3829×4×3 230.7\n\"Views” indicates # temporal clip × # spatial crop. The magnitudes are Giga (10 9) and Mega (10 6) for FLOPs and Param. “IN” denotes ImageNet\nicantly, by utilizing the same CLIP pre-trained backbones,\nour model demonstrates substantial performance improve-\nments over EVL (Lin et al., 2022b) and ST-Adapter (Pan\net al., 2022). Furthermore, our method achieves superior\nperformance compared to methods that were pre-trained\nwith JFT-300 M (Sun et al., 2017) or FLD-900 M (Y uan\net al., 2021), while requiring less computational cost or a\nsmaller resolution. Furthermore, with the signiﬁcant scale-up\nof the pre-training data to 2 billion samples (namely Merged-\n2B (Sun et al., 2023), which merges 1.6 billion samples from\nthe LAION-2B (Schuhmann et al., 2022) dataset with 0.4\nbillion samples from the COYO-700 M (Byeon et al., 2022)\ndataset), our method achieves an outstanding top accuracy of\n89.4%, solidifying its position as a state-of-the-art approach.\nTo verify the generalization ability of our method, we fur-\nther evaluate its performance on the widely-used untrimmed\nvideo benchmark, ActivityNet-v1.3. Speciﬁcally, we ﬁne-\ntune the Kinetics-400 pre-trained models, with ViT-L back-\nbone and 16 frames, on the ActivityNet-v1.3 dataset. The\ntop-1 accuracy and mean average precision (mAP) are\nTable 9 Comparisons with previous works on ActivityNet\nMethod Top-1 mAP\nListenToLook (Gao et al., 2020) – 89.9\nMARL (Wu et al., 2019b) 85.7 90.1\nDSANet (Wu et al., 2021b) – 90.5\nTSQNet (Xia et al., 2022a) 88.7 93.7\nNSNet (Xia et al., 2022b) 90.2 94.3\nOurs ViT-L 92.9 96.5\nOurs ViT-L (336 ↑) 93.3 96.9\nreported using the ofﬁcial evaluation metrics. As shown in\nTable9, our method outperforms recent state-of-the-art mod-\nels with a clear margin, with an mAP accuracy of 96.9%.\nWe also evaluate our method on the UCF-101 and\nHMDB-51 datasets to demonstrate its capacity to general-\nize to smaller datasets. We ﬁnetune our models on these two\ndatasets using the pre-trained ViT-L model on Kinetics-400\nand present the mean class accuracy on split one. We utilize\n123\n402 International Journal of Computer Vision (2024) 132:392–409\nTable 10 Mean class accuracy on UCF-101 and HMDB-51 achieved\nby different methods which are transferred from their Kinetics models\nwith RGB modality\nMethod UCF-101 HMDB-51\nARTNet (Wang et al., 2018a) 94.3% 70.9%\nI3D (Carreira & Zisserman, 2017) 95.6% 74.8%\nR(2+1)D (Tran et al., 2018) 96.8% 74.5%\nS3D-G (Xie et al., 2018) 96.8% 75.9%\nTSM (Lin et al., 2019) 95.9% 73.5%\nS T M( J i a n ge ta l . ,2019) 96.2% 72.2%\nTEINet (Liu et al., 2020) 96.7% 72.1%\nMVFNet (Wu et al., 2021a) 96.6% 75.7%\nTDN (Wang et al., 2021a) 97.4% 76.4%\nOurs ViT-L 98.1% 81.3 %\nOurs ViT-L (336 ↑) 98.2% 81.3 %\n16 frames as inputs. As shown in Table 10, our model exhib-\nited strong transferability, achieving a mean class accuracy\nof 98.2% on UCF-101 and 81.3% on HMDB-51.\nFew-Shot Video Recognition In few-shot video recognition,\nwhere only a few training samples are available, we inves-\ntigate a more challenging K -shot C-way situation, instead\nof the conventional 5-shot 5-way conﬁguration. We aim to\ncategorize all categories in the dataset with just K samples\nper category for training, where the lower and upper bounds\nare denoted by the terms “Zero-shot” and “All-shot”, respec-\ntively. Using the CLIP-pretrained ViT-L/14 with 8 frames\nand TAP for few-shot video recognition, we report the Top-1\naccuracy for the four datasets in Table 11. Despite the limited\namount of data, our method demonstrates remarkable trans-\nferability to diverse domain data. Furthermore, our approach\noutperforms previous methods signiﬁcantly, showing robust-\nness in these extremely data-poor situations. For instance,\nwhen comparing the accuracy on HMDB-51 with 2-shot, our\nmethod outperforms Swin (Liu et al., 2022) and X-Florence\n( N ie ta l . , 2022)b y +52.6% and +21.9%, respectively.\nMulti-Label Video Recognition We mainly focused on the\nsingle-label video recognition scenario in the previous exper-\nTable 12 Comparison with previous works on Multi-Label video\ndataset Charades\nMethod Frames mAP\nMultiScale TRN (Zhou et al., 2018) – 25.2%\nSTM (Jiang et al., 2019) 16 35.3%\nNonlocal (Wang et al., 2018b) – 37.5%\nSlowFast R50 (Feichtenhofer et al., 2019) 8+32 38.0%\nSlowFast R101 (Feichtenhofer et al., 2019) 16+64 42.5%\nLFB+NL (Wu et al., 2019a) 32 42.5%\nX3D-XL (312 ↑) (Feichtenhofer, 2020) 16 43.4%\nActionCLIP (Wang et al., 2021b) 32 44.3%\nOurs 16 46.0%\niments. To further validate the performance of our method,\nwe conducted experiments on multi-label video recogni-\ntion tasks. The Charades dataset is a multi-label untrimmed\nvideo dataset containing long-term activities with multiple\nactions. For this task, we utilized the Kinetics-400 pre-\ntrained ViT-L backbone for training and evaluated our results\nusing the Mean Average Precision (mAP) metric. As shown\nin Table 12, our method achieved the highest performance\nof 46.0 mAP , demonstrating its effectiveness in multi-label\nvideo classiﬁcation.\nZero-Shot Video Recognition In addition, we conducted\nexperiments in the open-set setting. We use our Kinetics-400\npre-trained models ( i.e., ViT-L with 8 frames) to perform\nzero-shot evaluations on four other video datasets. For UCF-\n101, HMDB-51, and ActivityNet, we follow two evaluation\nprotocols from E2E (Brattoli et al., 2020):\n1. To make a fair comparison with previous works, we ran-\ndomly selected half of the test dataset’s classes: 50 for\nUCF-101, 25 for HMDB-51, and 100 for ActivityNet, and\nevaluated our method on them. We repeated this process\nten times and averaged the results for each test dataset.\nWe refer to this setting as UCF\n∗,H M D B∗, and ANet ∗.\n2. In the second evaluation protocol, we directly evaluated\nthe full datasets to obtain more realistic accuracy scores.\nTable 11 Comparisons with previous works on few-shot action recognition\nMethod shot HMDB UCF ANet K400\nVideoSwin (Liu et al., 2022) 2 20.9 53.3 – –\nVideoPrompt (Ju et al., 2022) 5 56.6 79.5 – 58.5\nX-Florence (Ni et al., 2022) 2 51.6 84.0 – –\nOurs ViT-L 0 53.8 71.9 75.6 61.0\n1 72.7 96.4 89.0 75.8\n2 73.5 96.6 90.3 78.2\nAll 80.1 96.9 91.1 84.7\n123\nInternational Journal of Computer Vision (2024) 132:392–409 403\nTable 13 Comparison with previous works on zero-shot video recognition\nMethod UCF ∗ / UCF HMDB ∗ / HMDB ANet ∗/ ANet Kinetics-600\nGA (Mishra et al., 2018) 17.3 ±1.1 / – 19.3 ±2.1 / – - - –\nTS-GCN (Gao et al., 2019) 34.2 ±3.1 / – 23.2 ±3.0 / – – –\nE2E (Brattoli et al., 2020) 44.1 / 35.3 29.8 / 24.8 26.6 / 20.0 -\nDASZL (Kim et al., 2021) 48.9 ±5.8 / – – / – - –\nER (Chen & Huang, 2021) 51.8 ±2.9 / – 35.3 ±4.6 / – – 42.1 ±1.4\nResT (Lin et al., 2022a) 58.7 ±3.3 / 46.7 41.1 ±3.7 / 34.4 32.5 / 26.3 –\nOurs 85.8 ± 3.3 / 79.6 58.1 ± 5.7 / 49.8 84.6 ± 1.4 / 77.4 68.9 ± 1.0\nWe directly evaluate our method without any additional training on cross-dataset video recognition. ANet is short for ActivityNet. ∗ means half\nclasses evaluation\nFor Kinetics-600, we chose 220 new categories outside\nKinetics-400 for evaluation. We used the three splits pro-\nvided by Chen and Huang ( 2021) and sampled 160 categories\nfor evaluation from the 220 categories in Kinetics-600 for\neach split. We reported the mean accuracy for the three\nsplits. As shown in Table 13, our method demonstrates a\nstrong cross-dataset generalization ability, achieving signiﬁ-\ncant improvements over previous zero-shot video recognition\nmethods ( +27.1% on UCF-101, +17.0% on HMDB-51,\n+52.1% on ActivityNet, +26.8% on Kinetics-600).\n5 Experiments: Image Recognition\nIn this work, we also apply our method to image recognition.\nWe conduct a comprehensive evaluation on 10 datasets that\nrepresent a diverse set of visual recognition tasks, i.e., Ima-\ngeNet (Deng et al., 2009), StanfordCars (Krause et al., 2013),\nCaltech101 (Fei-Fei et al., 2004), OxfordPets (Parkhi et al.,\n2012), Flowers102 (Nilsback & Zisserman, 2008), Food101\n(Bossard et al., 2014), FGVCAircraft (Maji et al., 2013),\nSUN397 (Xiao et al., 2010), DTD (Cimpoi et al., 2014),\nEuroSA T (Helber et al., 2019). These tasks include clas-\nsifying generic objects, scenes, and ﬁne-grained categories\nand specialized tasks such as texture recognition and satellite\nimagery analysis.\n5.1 Training\nFor the pre-trained CLIP model, we use the ResNet-50 (He et\nal., 2016) as the default backbone for the image encoder, and\nthe image backbone is updated during training. We train the\nmodel using the AdamW optimizer with an initial learning\nrate of 5e-6 and a cosine annealing schedule to reduce the\nlearning rate gradually. We also employ a warmup strategy\nof 5 epochs. The maximum number of training epochs is set\nto 150.\n5.2 Main Results\nResults on 10 Image Datasets As illustrated in Fig. 4,t h e\nperformance of our method, vision-only method, and zero-\nshot method are evaluated on 10 image datasets, all trained\nwith 16 shots. The results of 10 datasets are arranged from\nleft to right, and the average results of the 10 datasets are\npresented on the far right. Our ﬁndings reveal that CLIP\nshowcases strong zero-shot performance on all 10 datasets.\nHowever, the vision-only method exhibits poor performance\non all datasets. We posit that this may be attributed to the\nabsence of a suitable classiﬁer target. Consequently, it may\nbe susceptible to biases in small samples, which can disrupt\nthe well-pretrained image encoder. Our approach demon-\nstrates a substantial improvement in recognition accuracy\ncompared to both the vision-only and zero-shot methods on\nall 10 datasets. Speciﬁcally, the average improvement over\nthe 10 datasets is 41% and 18% compared to the vision-only\nmethod and the zero-shot method, respectively. This indi-\ncates the effectiveness of our method in enhancing few-shot\nlearning performance.\nTable 14 presents a further comparison of our method\nwith two other transfer methods, speciﬁcally linear probe\nand CoOp (Zhou et al., 2021), on the widely used Ima-\ngeNet dataset. We implemented the linear probe method as\ninstructed in the original CLIP paper (Radford et al., 2021).\nOur ﬁndings indicate that the CoOp method contributes to\na signiﬁcant enhancement of the zero-shot model by 4.77%.\nNotably, our proposed approach surpasses this performance\nby achieving a further improvement of 8.33% on the zero-\nshot model, underscoring the effectiveness of our method in\nincorporating an appropriate classiﬁer target.\n6 Experiments: 3D Point Cloud Recognition\nWe further extend our approach to 3D point cloud recogni-\ntion and evaluated it on the ModelNet40 dataset (Wu et al.,\n2015). This dataset comprises 12,311 3D CAD models across\n123\n404 International Journal of Computer Vision (2024) 132:392–409\nFig. 4 Comparison of few-shot learning performance on 10 image datasets. Assessment of zero-shot CLIP , vision-only, and the proposed method\nunderlines the signiﬁcance of incorporating a suitable classiﬁer target to mitigate biases in small samples and achieve high accuracy on a diverse\nset of image datasets\nTable 14 Comparison of our method with other tuning methods on\nImageNet (using 16 shots)\nImageNet Δ\nZero-shot CLIP 58.18 –\nLinear probe 55.87 -2.31\nCoOp (Zhou et al., 2021) 62.95 +4.77\nOurs 66.51 +8.33\nΔ denotes the difference with the zero-shot model\n40 categories: airplanes, cars, plants, and lamps. The point\nclouds are normalized to a unit sphere and divided into 9,843\ntraining models and 2,468 testing models. ModelNet40 is a\nwidely used benchmark for point cloud recognition.\n6.1 Training\nFor the visual encoder, we use the ResNet-101 architecture\n(He et al., 2016) as the default backbone and apply multi-view\nperspective projection on the input point cloud following\nSimpleView (Goyal et al., 2021). SimpleView projects the\npoint cloud from six orthogonal views: front, right, back, left,\ntop, and bottom. In addition, we also include the views of the\nupper/bottom-front/back-left corners based on the observa-\ntion from Zhang et al. ( 2022) that the left view is the most\ninformative for few-shot recognition. For each view, a point\nwith a 3D coordinate is projected onto a pixel on the 2D image\nplane, and its depth value is used as the pixel intensity, which\nis repeated three times for the RGB channels. Finally, all the\nresulting images are upsampled to (224, 224) to align with\nCLIP’s settings.\nWe train the model using the Stochastic Gradient Descent\n(SGD) optimizer with an initial learning rate of 2e-4 and a\ncosine annealing schedule to reduce the learning rate grad-\nually. We also employ a warmup strategy of 10 epochs; the\nmaximum number of training epochs is set to 250.\nFig. 5 Results of 3D point cloud recognition on the ModelNet40\ndataset. Comparison of different tuning methods in the few-shot sce-\nnario\n6.2 Main Results\nComparison with the Vision-Only Paradigm in the Few-\nShot Scenario We evaluate our method using the few-shot\nevaluation protocol adopted in CLIP (Radford et al., 2021),\nwhich involves training with 1, 2, 4, 8, and 16 shots and\ndeploying models on the full test set. As shown in Fig. 5,w e\nﬁrst present our method’s zero-shot result (14.9%), which\nwas obtained by directly utilizing the CLIP model to clas-\nsify each view and averaging the results of the 10 views. We\nthen compare the performance of different tuning models on\n3D point cloud recognition. Our results show that all mod-\nels gradually improve in accuracy as the number of training\nsamples increases. Notably, our method (green curve) out-\nperforms the vision-only method (orange curve) by a large\nabsolute improvement of 30%-40%, which is consistent with\nﬁndings in image recognition and video recognition, and vali-\ndates the effectiveness of our approach. Additionally, we ﬁnd\nthat our method signiﬁcantly outperforms the linear probe\nmethod (blue curve) at all training sample levels, known as\na strong few-shot learning baseline. These results conﬁrm\n123\nInternational Journal of Computer Vision (2024) 132:392–409 405\nthe effectiveness and superiority of our proposed approach,\nwhich involves textual knowledge to improve transferability.\n7 Conclusion and Limitation\nThis study presents a new paradigm for enhancing the trans-\nferability of visual recognition tasks based on the knowledge\nfrom the textual encoder of a well-trained vision-language\nmodel. Speciﬁcally, we initialize the classiﬁer with semantic\ntargets from the textual encoder and freeze it during optimiza-\ntion. We conduct extensive experiments to examine how the\nparadigm functions: Firstly, we demonstrate that proper cor-\nrelation among target initialization is beneﬁcial. Secondly,\nwe show that alignment of visual and textual semantics is\nkey to improving few-shot performance and shortening the\nlearning progress. Finally, we verify the effectiveness of our\nproposed paradigm on three types of visual recognition tasks\n(i.e., image, video, and 3D point cloud recognition) across\n17 visual datasets.\nThe study still has some limitations worth diving into in\nfuture research. i) The performance of the proposed paradigm\nis restricted to how the category labels are represented. For\ninstance, in tasks such as human re-identiﬁcation, where\nthe labels are often numerical values such as 0, 1, 2, etc.\nIn this case, we cannot transfer any semantic information\nfrom the textual encoders, while transferring visual statistic\nknowledge (i.e., LDA classiﬁer) could be helpful. ii) The per-\nformance of the proposed paradigm relies on the capacity of\nthe vision-language pre-training models. Although we use\nCLIP as our source model in this study, obtaining models\nwith better performance remains an open problem. iii) The\nway category names are described also impacts performance.\nFor example, in the action recognition dataset Something-\nSomething, category names such as “Putting something into\nsomething” and “Covering something with something” lack\na clear target subject. Consequently, leveraging the prior\nknowledge of pre-aligned vision-language models becomes\nchallenging, resulting in subpar performance.\nAcknowledgements Wanli Ouyang was supported by the Australian\nResearch Council Grant DP200103223, Australian Medical Research\nFuture Fund MRFAI000085, CRC-P Smart Material Recovery Facility\n(SMRF) - Curby Soft Plastics, and CRC-P ARIA - Bionic Visual-Spatial\nProsthesis for the Blind.\nFunding Open Access funding enabled and organized by CAUL and\nits Member Institutions.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing, adap-\ntation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indi-\ncate if changes were made. The images or other third party material\nin this article are included in the article’s Creative Commons licence,\nunless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your\nintended use is not permitted by statutory regulation or exceeds the\npermitted use, you will need to obtain permission directly from the copy-\nright holder. To view a copy of this licence, visit http://creativecomm\nons.org/licenses/by/4.0/.\nAppendix\nA Additional Details\nIn this appendix, § A contains additional details for: the statis-\ntics of video datasets (§ A.1), visual encoder architectures\n(§ A.4), Batch Gather (§ A.2) and LDA (§ A.3).\nA.1 Statistics of Video Datasets\nKinetics-400 (Kay et al., 2017) is a large-scale video dataset,\nwhich consists of 240k training videos and 20k validation\nvideos in 400 different human action categories. Each video\nin the dataset is a 10-second clip of an action moment anno-\ntated from raw Y ouTube videos.\nKinetics-600 (Carreira et al., 2018) is an extensions of\nKinetics-400. Kinetics-600 consists of around 480k videos\nfrom 600 action categories. The 480K videos are divided\ninto 390k, 30k, and 60k for training, validation, and test sets,\nrespectively. In this paper, we use its test set for zero-shot\nevaluation.\nUCF-101 (Soomro et al., 2012) contains 13k videos spanning\nover 101 human actions.\nHMDB-51 (Kuehne et al., 2011) contains approximately 7k\nvideos belonging to 51 action class categories.\nActivityNet-v1.3 (Caba Heilbron et al., 2015) is a large-scale\nuntrimmed video benchmark, contains 19,994 untrimmed\nvideos of 5 to 10 min from 200 activity categories.\nCharades (Sigurdsson et al., 2016) is a video dataset\ndesigned for action recognition and localization tasks. It con-\ntains over 10,000 short video clips of people performing daily\nactivities, and consists of 157 action categories.\nA.2 Batch Gather for Distributed InfoNCE\nInstead of Data-Parallel Training (DP), which is single-\nprocess, multi-thread, and only works on a single machine,\nDistributed Data-Parallel Training (DDP) is a widely adopted\nsingle-program multiple-data training paradigm for single-\nand multi-machine training. Due to GIL contention across\nthreads, per-iteration replicated model, and additional over-\nhead introduced by scattering inputs and gathering outputs,\nDP is usually slower than DDP even on a single machine.\nHence, we develop the Distributed InfoNCE based on DDP\nfor large batch size and fast training. The core of the Dis-\ntributed InfoNCE implementation is batch gathering. Say\n123\n406 International Journal of Computer Vision (2024) 132:392–409\nAlgorithm 1 Numpy-like Pseudocode that illustrates the role\nof Batch Gather in Distributed InfoNCE.\n1 # text_encoder: encoder network for text\ninput\n2 # vision_encoder: encoder network for\nvision input , e.g., images or videos.\n3 # V: minibatch of vision inputs\n4 # T: minibatch of text inputs\n5 # N: the local batch size of each GPU, e.g\n.,16\n6 # M: the number of GPUs, e.g.,8\n7 # N * M: the global batch size for multi -\ngpu training , e.g. ,128\n8\n9 # extract feature representations of each\nmodality\n10 local_vision_features = vision_encoder(V) #\nshape: [N, embed_dim]\n11 local_text_features = text_encoder(T) #\nshape: [N, embed_dim]\n12\n13 # normalization\n14 local_vision_features = l2_normalize(\nlocal_vision_features , axis=1)\n15 local_text_features = l2_normalize(\nlocal_text_features , axis=1)\n16\n17 # batch_gather is a function gathering and\nconcatenating the tensors across GPUs.\n18 all_vision_features = batch_gather(\nlocal_vision_features) # shape: [N * M,\nembed_dim]\n19 all_text_features = batch_gather(\nlocal_text_features) # shape: [N * M,\nembed_dim]\n20\n21 # scaled pairwise cosine similarities\n22 # shape = [N, N * M]\n23 logits_vision = logit_scale *\nlocal_vision_features @ all_text_features.t\n()\n24 # shape = [N, N * M]\n25 logits_text = logit_scale *\nlocal_text_features @ all_vision_features.t\n()\n26\n27 # The logits are then used as inputs for N*\nM-way (e.g., 128-way) classification ,\n28 # resulting in a loss value corresponding\nto N inputs in each GPU.\n29 # Then Distributed Data Parallel mechanism\ntakes care of averaging these across GPUs,\n30 # which becomes equivalent to calculating\nthe loss over NMxNM (e.g. ,128 x128)\nsimilarities.\n31\nAlgorithm 2 The code generates the LDA coefﬁcient for\nKinetics-400 dataset.\n1 import numpy as np\n2 from sklearn. discriminant_analysis import\nLinearDiscriminantAnalysis as LDA\n3 input = np.load( ’feats_labels_400class.npz’\n) # pre-extracted visual features\n4 feats = input [’feats’ ] # size: [24000,\n512]\n5 labels = input [’labels’ ] # size: [24000,]\n6 lda = LDA()\n7 lda.fit(feats , labels)\n8 classifier = lda. coef_ # size: [400, 512]\n9\nthere are M GPUs and each GPU gets N input pairs, we need\nto calculate the NM ×NM similarity matrix across the GPUs\nfor InfoNCE loss. Without batch gathering, each GPU only\ncomputes a local N ×N matrix, s.t. N≪ NM, Then the cosine\nsimilarity and the InfoNCE loss would be calculated only for\nthe pairs within a single GPU and later their gradients would\nbe averaged and synced. That’s obviously not what we want.\nThe batch gathering for Distributed InfoNCE is presented\nas follows. When calculating the similarity matrix (and thus\nthe logit scores across text inputs for each image/video),\na GPU only needs to hold M vision features, and perform\nmatrix product with NM text features, yielding an M ×NM\nmatrix. This computation is distributed (i.e., sharded) across\nN GPUs, and we have calculated NM ×NM similarities\nacross the GPUs in total. The loss we employ is symmetric\nand the same happens w.r .t.text inputs. As shown in Algo-\nrithm 1, we also give an example pseudocode to help you\nunderstand the statement.\nA.3 LDA Classifier\nHere we provide the details of LDA classiﬁer. We directly\nuse the ofﬁcial CLIP-pretrained visual encoder to extract\nvideo embeddings, and the visual encoder is not ﬁnetuned\non Kinetics-400. Then we perform LDA on the pre-extracted\nvideo embeddings of the training set in Kinetics-400 to ini-\ntialize W and freeze it for ﬁnetuning the visual encoder on\nthe Kinetics-400 dataset.\nLDA is commonly used for feature classiﬁcation or feature\ndimensionality reduction. However, in this work, we only use\nLDA for feature classiﬁcation (in order to get “discriminant\ncoefﬁcients” as the classiﬁer) instead of feature dimensional-\nity reduction. For better understanding, we show the code in\nAlgorithm 2 which generates the LDA coefﬁcient and there\nis no dimension reduction.\nA.4 Visual Encoder Architectures\nWe provide the full architecture details of the visual encoder\nand textual encoders in this paper. Table 15 shows the\nCLIP-ResNet architectures. Table 16 shows the CLIP-ViT\narchitectures.\n123\nInternational Journal of Computer Vision (2024) 132:392–409 407\nTable 15 CLIP-ResNet hyperparameters\nModel Embedding Input ResNet Text Transformer\ndimension resolution blocks width layers width heads\nRN50 1024 224 (3, 4, 6, 3) 2048 12 512 8\nTable 16 CLIP-ViT hyperparameters\nModel Embedding Input Vision Transformer Text Transformer\ndimension resolution layers width heads layers width heads\nViT-B/32 512 224 12 768 12 12 512 8\nViT-B/16 512 224 12 768 12 12 512 8\nViT-L/14 768 224 24 1024 16 12 768 12\nViT-L/14-336px 768 336 24 1024 16 12 768 12\nReferences\nArnab, A., Dehghani, M., Heigold, G., Sun, C., Luˇ ci´c, M., & Schmid,\nC. (2021). Vivit: A video vision transformer. In ICCV (pp. 6836–\n6846).\nBertasius, G., Wang, H., & Torresani, L. (2021). Is space-time attention\nall you need for video understanding? In ICML, PMLR (pp. 813–\n824).\nBossard, L., Guillaumin, M., & V an Gool, L. (2014). Food-101–mining\ndiscriminative components with random forests. In ECCV.\nBrattoli, B., Tighe, J., Zhdanov, F., Perona, P ., & Chalupka, K. (2020).\nRethinking zero-shot video classiﬁcation: End-to-end training for\nrealistic applications. In CVPR (pp. 4613–4623).\nByeon, M., Park, B., Kim, H., Lee, S., Baek, W., & Kim, S.\n(2022). Coyo-700m: Image-text pair dataset. https://github.com/\nkakaobrain/coyo-dataset\nCaba Heilbron, F., Escorcia, V ., Ghanem, B., & Carlos Niebles, J.\n(2015). Activitynet: A large-scale video benchmark for human\nactivity understanding. In CVPR (pp. 961–970).\nCarreira, J., & Zisserman, A. (2017). Quo vadis, action recognition? A\nnew model and the kinetics dataset. In CVPR.\nCarreira, J., Noland, E., Banki-Horvath, A., Hillier, C., & Zisser-\nman, A. (2018). A short note about kinetics-600. arXiv preprint\narXiv:1808.01340\nChen, S., & Huang, D. (2021). Elaborative rehearsal for zero-shot action\nrecognition. In ICCV (pp. 13638–13647).\nChen, X., Xie, S., & He, K. (2021). An empirical study of training\nself-supervised vision transformers. In ICCV.\nCimpoi, M., Maji, S., Kokkinos, I., Mohamed, S., & V edaldi, A. (2014).\nDescribing textures in the wild. In CVPR.\nDeng, J., Dong, W., Socher, R., Li, L. J., Li, K., & Fei-Fei, L. (2009).\nImagenet: A large-scale hierarchical image database. In CVPR (pp.\n248–255).\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,\nUnterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly,\nS., et al. (2020). An image is worth 16x16 words: Transformers\nfor image recognition at scale. arXiv preprint arXiv:2010.11929\nFan, H., Xiong, B., Mangalam, K., Li, Y ., Yan, Z., Malik, J., & Feicht-\nenhofer, C. (2021). Multiscale vision transformers. In ICCV (pp.\n6824–6835).\nFei-Fei, L., Fergus, R., & Perona, P . (2004). Learning generative visual\nmodels from few training examples: An incremental Bayesian\napproach tested on 101 object categories. In Computer vision and\npattern recognition workshop .\nFeichtenhofer, C. (2020). X3d: Expanding architectures for efﬁcient\nvideo recognition. In CVPR (pp. 203–213).\nFeichtenhofer, C., Fan, H., Malik, J., & He, K. (2019). Slowfast net-\nworks for video recognition. In ICCV (pp. 6202–6211).\nGao, J., Zhang, T., & Xu, C. (2019). I know the relationships: Zero-shot\naction recognition via two-stream graph convolutional networks\nand knowledge graphs. In AAAI (vol. 33, pp. 8303–8311).\nGao, P ., Geng, S., Zhang, R., Ma, T., Fang, R., Zhang, Y ., Li, H., &\nQiao, Y . (2021). Clip-adapter: Better vision-language models with\nfeature adapters. arXiv preprint arXiv:2110.04544\nGao, R., Oh, T. H., Grauman, K., & Torresani, L. (2020). Listen to look:\nAction recognition by previewing audio. In CVPR (pp. 10457–\n10467).\nGhadiyaram, D., Tran, D., & Mahajan, D. (2019). Large-scale weakly-\nsupervised pre-training for video action recognition. In CVPR (pp.\n12046–12055).\nGoyal, A., Law, H., Liu, B., Newell, A., & Deng, J. (2021). Revisiting\npoint cloud shape classiﬁcation with a simple and effective base-\nline. In International conference on machine learning, PMLR (pp.\n3809–3820).\nHan, K., Xiao, A., Wu, E., Guo, J., Xu, C.,& Wang, Y . (2021). Trans-\nformer in transformer. In NeurIPS (pp. 15908–15919).\nHe, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for\nimage recognition. In CVPR (pp. 770–778).\nHe, K., Fan, H., Wu, Y ., Xie, S.,& Girshick, R. (2020). Momentum\ncontrast for unsupervised visual representation learning. In CVPR\n(pp. 9729–9738).\nHe, K., Chen, X., Xie, S., Li, Y ., Dollár, P ., & Girshick, R. (2022).\nMasked autoencoders are scalable vision learners. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern recog-\nnition (pp. 16000–16009).\nHelber, P ., Bischke, B., Dengel, A., & Borth, D. (2019). Eurosat: A\nnovel dataset and deep learning benchmark for land use and land\ncover classiﬁcation. IEEE Journal of Selected Topics in Applied\nEarth Observations and Remote Sensing., 12 (7), 2217–2226.\nIoffe, S., & Szegedy, C. (2015). Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift. In ICML,\nPMLR (pp. 448–456).\nJia, C., Yang, Y ., Xia, Y ., Chen, Y . T., Parekh, Z., Pham, H., Le, Q., Sung,\nY . H., Li, Z., & Duerig, T. (2021a). Scaling up visual and vision-\nlanguage representation learning with noisy text supervision. In\nInternational conference on machine learning, PMLR (pp. 4904–\n4916).\nJia, C., Yang, Y ., Xia, Y ., Chen, Y . T., Parekh, Z., Pham, H., Le, Q., Sung,\nY . H., Li, Z., & Duerig, T. (2021b). Scaling up visual and vision-\n123\n408 International Journal of Computer Vision (2024) 132:392–409\nlanguage representation learning with noisy text supervision. In\nICML, PMLR (pp. 4904–4916).\nJiang, B., Wang, M., Gan, W., Wu, W., & Yan, J. (2019). Stm: Spa-\ntiotemporal and motion encoding for action recognition. In ICCV\n(pp. 2000–2009).\nJu, C., Han, T., Zheng, K., Zhang, Y ., & Xie, W. (2022). Prompt-\ning visual-language models for efﬁcient video understanding. In\nECCV (pp. 105–124), Springer.\nKay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijaya-\nnarasimhan, S., Viola, F., Green, T., Back, T., Natsev, P ., et al.\n(2017). The kinetics human action video dataset. arXiv preprint\narXiv:1705.06950\nKim, T. S., Jones, J., Peven, M., Xiao, Z., Bai, J., Zhang, Y ., Qiu, W.,\nY uille, A., & Hager, G. D. (2021). Daszl: Dynamic action signa-\ntures for zero-shot learning. AAAI, (vol. 35, pp. 1817–1826).\nKrause, J., Stark, M., Deng, J., & Fei-Fei, L. (2013). 3D object rep-\nresentations for ﬁne-grained categorization. In 4th International\nIEEE workshop on 3D representation and recognition (3dRR-13) ,\nSydney, Australia.\nKrizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classi-\nﬁcation with deep convolutional neural networks. In NeurIPS (pp.\n25).\nKuehne, H., Jhuang, H., Garrote, E., Poggio, T., & Serre, T. (2011).\nHmdb: A large video database for human motion recognition. In\nICCV (pp. 2556–2563).\nLi, B., Weinberger, K. Q., Belongie, S., Koltun, V ., & Ranftl, R.\n(2022a). Language-driven semantic segmentation. arXiv preprint\narXiv:2201.03546\nLi, J., Li, D., Xiong, C., & Hoi, S. (2022b). Blip: Bootstrapping\nlanguage-image pre-training for uniﬁed vision-language under-\nstanding and generation. arXiv preprint arXiv:2201.12086\nLi, T., Zhu, S., & Ogihara, M. (2006). Using discriminant analysis for\nmulti-class classiﬁcation: An experimental investigation. Knowl-\nedge and Information Systems, 10 (4), 453–472.\nLin, C. C., Lin, K., Wang, L., Liu, Z., & Li, L. (2022a). Cross-modal\nrepresentation learning for zero-shot action recognition. In CVPR\n(pp. 19978–19988).\nLin, J., Gan, C., & Han, S. (2019). Tsm: Temporal shift module for\nefﬁcient video understanding. In ICCV.\nLin, Z., Geng, S., Zhang, R., Gao, P ., de Melo, G., Wang, X., Dai, J.,\nQiao, Y ., & Li, H. (2022b). Frozen clip models are efﬁcient video\nlearners. In ECCV (pp. 388–404), Springer.\nLiu, Z., Luo, D., Wang, Y ., Wang, L., Tai, Y ., Wang, C., Li, J., Huang,\nF., & Lu, T. (2020). Teinet: Towards an efﬁcient architecture for\nvideo recognition. In AAAI (pp. 11669–11676).\nLiu, Z., Lin, Y ., Cao, Y ., Hu, H., Wei, Y ., Zhang, Z., Lin, S., & Guo, B.\n(2021). Swin transformer: Hierarchical vision transformer using\nshifted windows. In ICCV (pp. 10012–10022).\nLiu, Z., Ning, J., Cao, Y ., Wei, Y ., Zhang, Z., Lin, S., & Hu, H. (2022).\nVideo swin transformer. In CVPR (pp. 3202–3211).\nLüddecke, T., & Ecker, A. (2022). Image segmentation using text and\nimage prompts. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition (pp. 7086–7096).\nLuo, H., Ji, L., Zhong, M., Chen, Y ., Lei, W., Duan, N., & Li, T. (2021).\nClip4clip: An empirical study of clip for end to end video clip\nretrieval. arXiv preprint arXiv:2104.08860\nMaji, S., Rahtu, E., Kannala, J., Blaschko, M., & V edaldi, A.\n(2013). Fine-grained visual classiﬁcation of aircraft. arXiv preprint\narXiv:1306.5151\nM i s h r a ,A . ,V e r m a ,V .K . ,R e d d y ,M .S .K . ,A r u l k u m a r ,S . ,R a i ,P . ,&\nMittal, A. (2018). A generative approach to zero-shot and few-shot\naction recognition. In WACV (pp. 372–380).\nMokady, R., Hertz, A., & Bermano, A. H. (2021). Clipcap: Clip preﬁx\nfor image captioning. arXiv preprint arXiv:2111.09734\nNi, B., Peng, H., Chen, M., Zhang, S., Meng, G., Fu, J., Xiang, S., &\nLing, H. (2022). Expanding language-image pretrained models for\ngeneral video recognition. In ECCV.\nNilsback, M. E., & Zisserman, A. (2008). Automated ﬂower classiﬁca-\nt i o no v e ral a r g en u m b e ro fc l a s s e s .I nICVGIP.\nV an den Oord, A., Li, Y ., & Vinyals, O. (2018). Representation learning\nwith contrastive predictive coding. arXiv e-prints (pp. arXiv–\n1807).\nPan, J., Lin, Z., Zhu, X., Shao, J., & Li, H. (2022). St-adapter: Parameter-\nefﬁcient image-to-video transfer learning for action recognition.\narXiv preprint arXiv:2206.13559\nParkhi, O. M., V edaldi, A., Zisserman, A., & Jawahar, C. (2012). Cats\nand dogs. In CVPR.\nQiu, Z., Yao, T., & Mei, T. (2017). Learning spatio-temporal representa-\ntion with pseudo-3d residual networks. In ICCV (pp. 5533–5541).\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S.,\nSastry, G., Askell, A., Mishkin, P ., Clark, J., et al. (2021). Learning\ntransferable visual models from natural language supervision. In\nICML, PMLR (pp. 8748–8763).\nRamesh, A., Pavlov, M., Goh, G., Gray, S., V oss, C., Radford, A., Chen,\nM., & Sutskever, I. (2021). Zero-shot text-to-image generation. In\nICML, PMLR (pp. 8821–8831).\nRao, Y ., Zhao, W., Chen, G., Tang, Y ., Zhu, Z., Huang, G., Zhou,\nJ., & Lu, J. (2022). Denseclip: Language-guided dense pre-\ndiction with context-aware prompting. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recogni-\ntion (pp. 18082–18091).\nRibani, R., & Marengoni, M. (2019). A survey of transfer learning for\nconvolutional neural networks. In 2019 32nd SIBGRAPI confer-\nence on graphics, patterns and images tutorials (SIBGRAPI-T)\n(pp. 47–57), IEEE.\nRyoo, M. S., Piergiovanni, A., Arnab, A., Dehghani, M., & Angelova,\nA. (2021). Tokenlearner: What can 8 learned tokens do for images\nand videos? arXiv preprint arXiv:2106.11297\nSanh, V ., Debut, L., Chaumond, J., & Wolf, T. (2019). Distilbert, a\ndistilled version of bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108\nSchuhmann, C., Beaumont, R., V encu, R., Gordon, C., Wightman, R.,\nCherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et al.\n(2022). Laion-5b: An open large-scale dataset for training next\ngeneration image-text models. arXiv preprint arXiv:2210.08402\nSigurdsson, G. A., V arol, G., Wang, X., Farhadi, A., Laptev, I., & Gupta,\nA. (2016). Hollywood in homes: Crowdsourcing data collection\nfor activity understanding. In Computer vision–ECCV 2016: 14th\nEuropean conference, Amsterdam, The Netherlands, proceedings,\npart I 14 , (pp. 510–526), Springer.\nSimonyan, K., & Zisserman, A. (2014). V ery deep convolutional\nnetworks for large-scale image recognition. arXiv preprint\narXiv:1409.1556\nSoomro, K., Zamir, A. R., & Shah, M. (2012). Ucf101: A dataset of\n101 human actions classes from videos in the wild. arXiv preprint\narXiv:1212.0402\nSun, C., Shrivastava, A., Singh, S.,& Gupta, A. (2017). Revisiting unrea-\nsonable effectiveness of data in deep learning era. In ICCV (pp.\n843–852).\nSun, Q., Fang, Y ., Wu, L., Wang, X.,& Cao, Y . (2023). Eva-clip:\nImproved training techniques for clip at scale. arXiv preprint\narXiv:2303.15389\nSun, Z. (2022). Design of the topology for contrastive visual-textual\nalignment. arXiv preprint arXiv:2209.02127\nTan, C., Sun, F., Kong, T., Zhang, W., Yang, C., & Liu, C. (2018). A\nsurvey on deep transfer learning. In Artiﬁcial neural networks and\nmachine learning–ICANN 2018: 27th international conference on\nartiﬁcial neural networks, Rhodes, Greece, proceedings, part III\n27 (pp. 270–279), Springer.\n123\nInternational Journal of Computer Vision (2024) 132:392–409 409\nTran, D., Wang, H., Torresani, L., Ray, J., LeCun, Y ., & Paluri, M.\n(2018). A closer look at spatiotemporal convolutions for action\nrecognition. In CVPR (pp. 6450–6459).\nTran, D., Wang, H., Torresani, L., & Feiszli, M. (2019). Video classi-\nﬁcation with channel-separated convolutional networks. In ICCV\n(pp. 5552–5561).\nWang, L., Li, W., Li, W., & V an Gool, L. (2018a). Appearance-and-\nrelation networks for video classiﬁcation. In CVPR.\nWang, L., Tong, Z., Ji, B., & Wu, G. (2021a). Tdn: Temporal dif-\nference networks for efﬁcient action recognition. In CVPR (pp.\n1895–1904).\nWang, M., Xing, J., & Liu, Y . (2021b). Actionclip: A new paradigm for\nvideo action recognition. arXiv preprint arXiv:2109.08472\nWang, X., Girshick, R., Gupta, A., & He, K. (2018b). Non-local neural\nnetworks. In CVPR (pp. 7794–7803).\nWu, C. Y ., Feichtenhofer, C., Fan, H., He, K., Krahenbuhl, P ., & Gir-\nshick, R. (2019a). Long-term feature banks for detailed video\nunderstanding. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition (pp. 284–293).\nWu, W., He, D., Tan, X., Chen, S., & Wen, S. (2019b). Multi-agent rein-\nforcement learning based frame sampling for effective untrimmed\nvideo recognition. In ICCV (pp. 6222–6231).\nWu, W., He, D., Lin, T., Li, F., Gan, C., & Ding, E. (2021). Mvfnet:\nMulti-view fusion network for efﬁcient video recognition. AAAI\n(vol. 35, pp. 2943–2951).\nW u ,W . ,Z h a o ,Y . ,X u ,Y . ,T a n ,X . ,H e ,D . ,Z o u ,Z . ,Y e ,J . ,L i ,Y . ,Y a o ,\nM., Dong, Z., et al. (2021b). Dsanet: Dynamic segment aggregation\nnetwork for video-level representation learning. In ACM MM (pp.\n1903–1911).\nWu, Z., Song, S., Khosla, A., Y u, F., Zhang, L., Tang, X., & Xiao, J.\n(2015). 3d shapenets: A deep representation for volumetric shapes.\nIn Proceedings of the IEEE conference on computer vision and\npattern recognition (pp. 1912–1920).\nXia, B., Wang, Z., Wu, W., Wang, H., & Han, J. (2022a). Temporal\nsaliency query network for efﬁcient video recognition. In ECCV\n(pp. 741–759).\nXia, B., Wu, W., Wang, H., Su, R., He, D., Yang, H., Fan, X., & Ouyang,\nW. (2022b). Nsnet: Non-saliency suppression sampler for efﬁcient\nvideo recognition. In ECCV (pp. 705–723).\nXiao, J., Hays, J., Ehinger, K. A., Oliva, A., & Torralba, A. (2010).\nSun database: Large-scale scene recognition from abbey to zoo. In\nCVPR.\nXie, S., Sun, C., Huang, J., Tu, Z., & Murphy, K. (2018). Rethink-\ning spatiotemporal feature learning: Speed-accuracy trade-offs in\nvideo classiﬁcation. In ECCV (pp. 305–321).\nYan, S., Xiong, X., Arnab, A., Lu, Z., Zhang, M., Sun, C., & Schmid,\nC. (2022). Multiview transformers for video recognition. In CVPR\n(pp. 3333–3343).\nYang, J., Li, C., Zhang, P ., Xiao, B., Liu, C., Y uan, L., & Gao, J. (2022).\nUniﬁed contrastive learning in image-text-label space. In CVPR,\n(pp. 19163–19173).\nY u, J., Wang, Z., V asudevan, V ., Yeung, L., Seyedhosseini, M., & Wu,\nY . (2022). Coca: Contrastive captioners are image-text foundation\nmodels. arXiv preprint arXiv:2205.01917\nY uan, L., Chen, D., Chen, Y . L., Codella, N., Dai, X., Gao, J., Hu, H.,\nHuang, X., Li, B., Li, C., et al. (2021). Florence: A new foundation\nmodel for computer vision. arXiv preprint arXiv:2111.11432\nZhai, X., Kolesnikov, A., Houlsby, N., & Beyer, L. (2021). Scaling\nvision transformers. arXiv preprint arXiv:2106.04560\nZhang, B., Y u, J., Fifty, C., Han, W., Dai, A. M., Pang, R., & Sha, F.\n(2021a). Co-training transformer with videos and images improves\naction recognition. arXiv preprint arXiv:2112.07175\nZhang, R., Fang, R., Zhang, W., Gao, P ., Li, K., Dai, J., Qiao, Y ., &\nLi, H. (2021b). Tip-adapter: Training-free clip-adapter for better\nvision-language modeling. arXiv preprint arXiv:2111.03930\nZhang, R., Guo, Z., Zhang, W., Li, K., Miao, X., Cui, B., Qiao, Y ., Gao,\nP ., & Li, H. (2022). Pointclip: Point cloud understanding by clip.\nIn Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition (pp. 8552–8562).\nZhao, S., Zhu, L., Wang, X., & Yang, Y . (2022). Centerclip: Token\nclustering for efﬁcient text-video retrieval. In SIRIR.\nZhou, B., Andonian, A., Oliva, A., & Torralba, A. (2018). Temporal\nrelational reasoning in videos. In ECCV\n.\nZhou, K., Yang, J., Loy, C. C., & Liu, Z. (2021). Learning to prompt\nfor vision-language models. arXiv preprint arXiv:2109.01134\nZhou, K., Yang, J., Loy, C. C., & Liu, Z. (2022). Conditional\nprompt learning for vision-language models. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition\n(pp. 16816–16825).\nZhuang, F., Qi, Z., Duan, K., Xi, D., Zhu, Y ., Zhu, H., Xiong, H., & He,\nQ. (2020). A comprehensive survey on transfer learning. Proceed-\nings of the IEEE, 109 (1), 43–76.\nPublisher’s Note Springer Nature remains neutral with regard to juris-\ndictional claims in published maps and institutional afﬁliations.\n123",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8363921642303467
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7351816296577454
    },
    {
      "name": "Classifier (UML)",
      "score": 0.6811918020248413
    },
    {
      "name": "Transfer of learning",
      "score": 0.5293717384338379
    },
    {
      "name": "Machine learning",
      "score": 0.49165040254592896
    },
    {
      "name": "Encoder",
      "score": 0.45996955037117004
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.43462300300598145
    },
    {
      "name": "Natural language processing",
      "score": 0.32141590118408203
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I129604602",
      "name": "The University of Sydney",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I98301712",
      "name": "Baidu (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4391012619",
      "name": "Shanghai Artificial Intelligence Laboratory",
      "country": null
    },
    {
      "id": "https://openalex.org/I4210100255",
      "name": "Beijing Academy of Artificial Intelligence",
      "country": "CN"
    }
  ]
}