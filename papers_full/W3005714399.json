{
  "title": "Transformer on a Diet",
  "url": "https://openalex.org/W3005714399",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5100329692",
      "name": "Chenguang Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101740226",
      "name": "Zihao Ye",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5049841140",
      "name": "Aston Zhang",
      "affiliations": [
        "Amazon (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A5100459167",
      "name": "Zheng Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5000245150",
      "name": "Alexander J. Smola",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963840672",
    "https://openalex.org/W2427527485",
    "https://openalex.org/W2963748792",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2549416390",
    "https://openalex.org/W2743945814",
    "https://openalex.org/W2525332836",
    "https://openalex.org/W2092045293",
    "https://openalex.org/W2417677256",
    "https://openalex.org/W2984864519",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2951672049",
    "https://openalex.org/W2936497627",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963938518",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2963537482",
    "https://openalex.org/W2091812280",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2996428491"
  ],
  "abstract": "Transformer has been widely used thanks to its ability to capture sequence information in an efficient way. However, recent developments, such as BERT and GPT-2, deliver only heavy architectures with a focus on effectiveness. In this paper, we explore three carefully-designed light Transformer architectures to figure out whether the Transformer with less computations could produce competitive results. Experimental results on language model benchmark datasets hint that such trade-off is promising, and the light Transformer reduces 70% parameters at best, while obtains competitive perplexity compared to standard Transformer. The source code is publicly available.",
  "full_text": "Transformer on a Diet\nChenguang Wang Zihao Ye Aston Zhang\nZheng Zhang Alexander J. Smola\nAmazon Web Services\n{chgwang, yeziha, astonz, zhaz, smola}@amazon.com\nAbstract\nTransformer has been widely used thanks to\nits ability to capture sequence information in\nan efﬁcient way. However, recent develop-\nments, such as BERT and GPT-2, deliver only\nheavy architectures with a focus on effective-\nness. In this paper, we explore three carefully-\ndesigned light Transformer architectures to ﬁg-\nure out whether the Transformer with less\ncomputations could produce competitive re-\nsults. Experimental results on language model\nbenchmark datasets hint that such trade-off is\npromising, and the light Transformer reduces\n70% parameters at best, while obtains compet-\nitive perplexity compared to standard Trans-\nformer. The source code is publicly avail-\nable 1.\n1 Introduction\nTransformer has shown its effectiveness in model-\ning sequence information due to the combination of\nself-attention mechanism and positional encoding.\nThe variants of Transformer architecture, such as\nBERT (Devlin et al., 2018) and GPT-2 (Radford\net al., 2018, 2019), have obtained the state-of-the-\nart results across a wide range of NLP tasks, includ-\ning GLUE benchmark dataset (Wang et al., 2018),\nand question answering dataset, e.g., SQuAD (Ra-\njpurkar et al., 2016). However, Transformer in\nnature is a fully-connected feed-forward neural net-\nwork and exhibits heavy computation characteris-\ntics. The recent BERT and GPT-2 are constructed\nas a stack of Transformer blocks, e.g., the largest\nGPT-2 is a stack of 48 Transformer blocks and con-\ntains 1542M parameters, BERT-large contains 24\nlayers of Transformer block and results in 340M pa-\nrameters, therefore the computational burden of the\nfully connected Transformer becomes heavier. A\nside effect in industrial applications is that this po-\ntentially makes it harder to deploy due to the huge\n1https://tinyurl.com/qkp3zf3\nsize of the model. Therefore a light version of the\nstandard Transformer architecture is expected to\nrelieve the heavy computation issue and compress\nthe model to ease the deployment in real world\napplications.\nIn this paper, we carefully design several light\nTransformer architectures. The intuition behind the\nlight Transformers is: preserving the Transformer\nconnections that are useful to capture the essential\nsequence information, while omitting the ones with\nless impact. In particular, we explore along two\ndirections: 1) better preserve the connections that\nare useful for capturing long-range dependency.\nWe adapt the idea of dilated convolutions (Yu and\nKoltun, 2015) to preserve the Transformer connec-\ntions that are useful to extend the effective history\nof the context, and 2) better preserve the connec-\ntions that are essential in capturing local context.\nWe leverage cascade connections that are capable\nto intensively incorporate the local context infor-\nmation in a ﬂexible manner.\nThe contributions of the paper are two-folds:\n•We explore three light Transformer architec-\ntures that are able to preserve the necessary\nconnections in standard Transformer. We\nshow that the light Transformer architectures\nreduce the computation from quadratic to lin-\near compared to the standard Transformer.\n•We conduct experiments on two language\nmodel benchmark datasets, one of the most\ntraditional sequence modeling tasks, where\nthe results indicate that the lightest architec-\nture could reduce 70% parameters of standard\nTransformer, and performs competitively with\nthe standard Transformer.\n2 Light Transformers\nWe describe the three proposed light Transformer\narchitectures in this section.\n1\narXiv:2002.06170v1  [cs.CL]  14 Feb 2020\n2.1 Background\nRevisiting Transformer architecture. Trans-\nformer (Vaswani et al., 2017) consists of an encoder\nand a decoder. We mainly focus on the sequence\ngeneration problem, thus we brieﬂy describe the\nTransformer decoder structure, full Transformer,\nbelow for sake of clarity, in the following sec-\ntions, we mean Transformer decoder when mention\nTransformer, unless other ways stated. As illus-\ntrated in Figure 1(a), the full Transformer block\ncontains two sub-layers: 1) a masked multi-head\nattention layer; 2) a position-wise fully connected\nfeed-forward network. Besides, there is a residual\nconnection (He et al., 2016) around each of the two\nsub-layers, followed by layer normalization (Ba\net al., 2016).\nTransformer computation complexity analy-\nsis. For each Transformer block, we assume that\nthe length of the sequence to be n, the size of hid-\nden states to be h, the computation of each Trans-\nformer block is O(n2 ·h).\nWe regard the following three architectures that\nhave less computation compared to the full Trans-\nformer as light Transformers.\n2.2 Dilated Transformer\nModel assumption.The key strength of the Trans-\nformer is that the combination of self-attention and\npositional encoding is able to capture long-term\ndependency in the sequence. Thus we need to pre-\nserve the long-term dependency when lightening\nup the Transformer blocks. Inspired by the dilated\nconvolutions (Yu and Koltun, 2015), we introduce\nthe idea of dilated Transformer to enable an expo-\nnentially large receptive ﬁeld in the Transformer\nscenario.\nModel architecture.The model architecture is\nillustrated in Figure 1(b), where the sub-layers are\nthe same to full Transformer, but with dilated con-\nnections across the sequence. To enable this, we\nintroduce d as the dilation factor, k as the ﬁlter size.\nSimilar to the common usage in dilated convolu-\ntions, we increase d exponentially with the depth of\nthe Transformer based network, i.e., d = O(2l) at\nlevel l of the Transformer, to increase the receptive\nﬁeld. By doing this, there is some ﬁlter that hits\neach input within the effective history, while also\nallowing for an extremely large effective history\nusing the deep Transformer architecture.\nComputation complexity analysis.In each di-\nlated Transformer block, there would be k nodes\nneed to compute the output of the current node, so\nthe computation complexity is O(n ·k ·h). The\ncomputation cost is signiﬁcantly lower compared to\nthat of the full Transformer when k is signiﬁcantly\nsmaller than n.\n2.3 Dilated Transformer with Memory\nModel assumption.Similar to the idea of dilated\nTransformer, we use dilated connections to pre-\nserve the long-range dependency in the sequence.\nAdditionally, in dilated Transformer with memory,\nwe try to cache more local contexts by memorizing\nthe nodes in the previous dilated connections.\nModel architecture.Figure 1(c) illustrates the\nmodel architecture, where the sub-layers are still\nthe same with full Transformer. Similar to dilated\nTransformer, we use the dilation factor and ﬁlter\nsize to construct the dilated connections. However,\nthe dilated connections in the previous layer are\npreserved. This will ensure a large effective history\nby with richer local history. This could potentially\npreserve the connections that are necessary to de-\ncode.\nComputation complexity analysis.In each di-\nlated Transformer with memory block, the com-\nputation of the connections in the previous layer\nwould add to the current computation, which re-\nsults in O(n ·k ·c ·h), where c indicates the extra\nconnections in the previous layer. If the sequence\nlength is inﬁnite, then c = 2.\n2.4 Cascade Transformer\nModel assumption. Instead of exploiting the di-\nlated Transformer idea, we instead explore cascade\nconnections idea to exponentially incorporate the\nlocal connections. By exploring this method, we\nwould see how the local connections in different\ndepths of the network contribute to the results.\nModel architecture. Figure 1(d) illustrate the\ncascade Transformer architecture, where the sub-\nlayers are still the same as full Transformer. We\nintroduce base window size as b, the cardinal num-\nber is m, then the number of cascade connections\nat level l of the Transformer is O(b ·ml). By do-\ning this, we can control the shape of the cascade\nacross the levels of the Transformer, which gives\nTransformer the ﬂexibility to learn from the cas-\ncade connections.\nComputation complexity analysis. In each\ncascade Transformer, the computation cost isO(n·\nb ·ml ·h). Compare to full Transformer, the com-\nplexity is still smaller since b ·ml < n.\n2\n(a) Full Transformer.\n (b) Dilated Transformer.\n (c) Dilated Transformer\nwith memory.\n(d) Cascade Transformer.\nFigure 1: Transformer architectures. (a): standard Transformer; (b)-(d): proposed light Transformers.\nModel Computation Complexity\nFull O(n2 · h)\nDilated O(n · k · h)\nDilated-Memory O(n · k · c · h)\nCascade O(n · b · ml · h)\nTable 1: Computation complexities of different Trans-\nformer architectures. Full: full Transformer; Dilated:\ndilated Transformer; Dilated-Memory: dilated Trans-\nformer with memory; Cascade: cascade Transformer;\nn is the length of the sequence. h is the size of the hid-\nden state. k is the ﬁlter size. b is the base window size.\nm is the cardinal number.\n3 Transformer Language Model\nWe select language model as the task to evaluate\nthe proposed Transformer architecture, since it is\none of the fundamental NLP tasks. In this section,\nwe introduce how the different Transformer blocks\nadapted to the task of language model.\nGiven a corpus of tokens X = (X1, . . . , XT ),\nthe objective of language model is described in\nEq. 1.\nL(X) =\n∑\nt\nlogP (Xt|X<t) (1)\nThen the Transformer (decoder) blocks are used\nto generate the output distribution over the vocabu-\nlary as indicated in Eq. 4.\nh0 = XWe + Wp (2)\nhl = transformerblock(hl−1) (3)\np(X) = softmax(hLWeT ) (4)\nwhere transformer block can be replaced with any\nof the three proposed Transformer architectures, hl\nis the hidden output of l-th layer, We is the word\nembedding matrix, andWp is positional embedding\nmatrix.\n4 Experiments\nWe compare the light Transformers with standard\nTransformers from both results and computation\nperspectives.\n4.1 Datasets and Metrics\nWe evaluate the proposed methods on three widely-\nused language model benchmark datasets. Penn\nTreeBank (PTB): we use the preprocessed version\nof (Mikolov et al., 2010), which contains 100M\ntokens. WikiText-2 (WT-2)is a small prepro-\ncessed version of Wikipedia, containing 200M to-\nkens (Merity et al., 2016). We use perplexity to\nevaluate the language model results.\n4.2 Training Details\nFor fair comparison, the full Transformer and the\nlight Transformer architectures are with 3 layers,\nembedding size equals to 320, number of heads in\nthe multi-head attention is 16. The dropout rate is\nset as 0.4 and 0.2 on PTB and WT-2 respectively.\nFor dilated Transformer and dilated Transformer\nwith memory, k = 3, the base of d = 2. For\ncascade Transformer, b = 4and m = 2. For the\nlight Transformers and full Transformer, the hidden\nsize equals to 2000. These settings are shared on\nthe two datasets.\nWe use truncated back-propagation through time\nto compute the gradients across all the experiment\nsettings. The batch size equals to 20 on both\ndatasets, whereas the sequence length equals to\n70 on both datasets. We use SGD for training with\nlearning rate equals to 10.\n4.3 Results Analysis\nWe compare the effectiveness of the pro-\nposed Transformer architectures with the full-\nTransformer architecture. From the results in Ta-\nble 2, we ﬁnd out that cascade Transformer per-\nforms closely to the full-Transformer structure.\n3\nModel Parameter PTB WT-2\nVal Test Val Test\nFull 30.0M 109.19 103.72 148.76 140.74\nDilated 8.8M 115.67 110.92 157.67 147.58\nDilated-Memory 11.1M 115.35 110.98 167.35 157.08\nCascade 13.5M 109.16 105.27 145.96 136.02\nTable 2: Results comparison (perplexity) of different Transformer language models on PTB and WT-2 data. Full:\nfull Transformer; Dilated: dilated Transformer; Dilated-Memory: dilated Transformer with memory; Cascade:\ncascade Transformer.\nThis indicates that local context is very important\nto language model tasks, and cascade Transformer\nis able to capture the meaning local dependency.\nWe also compare the parameter sizes between\nlight Transformers and full Transformer. Among\nall the architectures, dilated Transformer is lightest\none. Although it delivers moderate results, how-\never, when compare to full Transformer, we save\n70% model size and the computation could be more\nefﬁcient. Table 2 also shows the trade-off between\nparameter size and perplexities on the two datasets.\nIt would suggest the best Transformer architecture\ngiven the deployment constraints, such as model\nsize limit, latency requirement or the quality.\n5 Related Work\nTransformer architectureshave been proposed\nto compute the sequence input efﬁciently. The\nbasic Transformer block consists of a multi-head\nattention layer and a position-wise fully connected\nfeed-forward network. The original Transformer\narchitectures contains an encoder and decoder. The\nencoder and decoder share similar structures with\n6 layers of Transformer block. Instead, the decoder\nuses masked multi-head attention each block to pre-\nvent leftward information ﬂow. Recently, stacked\nTransformer architectures, such as BERT (Devlin\net al., 2018), GPT(-2) (Radford et al., 2018, 2019),\nand the most recent ones (Peters et al., 2018; Wang\net al., 2019; Raffel et al., 2019; Liu et al., 2019;\nYang et al., 2019) are proposed and shown the state-\nof-the-art results on a wide range of NLP tasks,\nsuch as GLUE benchmark (Wang et al., 2018)\nand question answering datasets (Rajpurkar et al.,\n2016). However, these Transformer architectures\nare heavy and it is hard to deploy in practice where\nthe environment has constraints. Lightened Trans-\nformer architectures (Ye et al., 2019; Qipeng Guo,\n2019) are proposed to speed up the computation.\nOur work is aligned with such Transformers but\nwith even less computation. Compared to ALBERT\n(Lan et al., 2019), the proposed method optimizes\nthe base Transformer and could be further inte-\ngrated into BERT.\nLanguage models have been studied exten-\nsively in NLP. Neural language models have sup-\nplanted traditional n-gram models in recent years\n(Bengio et al., 2003; Mnih and Hinton, 2007;\nMikolov et al., 2010). Particularly, recurrent neural\nnetworks (Inan et al., 2016; Merity et al., 2017;\nMelis et al., 2017; Krause et al., 2018), such as\nLSTMs have achieved state-of-the-art results on\nvarious benchmark datasets with different regu-\nlarization techniques and post-training methods\n(Grave et al., 2016; Krause et al., 2018). The mix-\nture of softmax (Yang et al., 2017) has helped ad-\ndress the low-rank embedding problem for word\nprediction. Recently, more advanced Transformer\narchitectures, such as GPT (Radford et al., 2018)\nand GPT-2 (Radford et al., 2019) are applied to the\ntask of language model. Due to the efﬁciency of the\nTransformer computation, these models have been\ntrained on large scale text corpora and shown good\nresults across language model datasets. We instead\nstudy how to lighten the Transformer, which could\nbe generalized to the idea of large Transformer ar-\nchitecture (e.g., GPT) to train on large corpora to\nobtain better results.\n6 Conclusion\nWe explore less computation-expensive Trans-\nformer architectures. The design principle is to\nstill preserve the long and short range dependency\nin the sequence but with less connections. Exper-\niments on language model datasets show that a\nlight weighted Transformer is able to perform com-\npetitively but with much improved computation\nefﬁciency. We plan to extend the Transformer ar-\nchitectures to experiment on deeper Transformer\narchitectures and more tasks (Wang et al., 2015,\n2016).\n4\nAcknowledgements We are grateful to Mu Li,\nDa Zheng, Haibin Lin, and Leyuan Wang for their\nhelpful inputs on the paper.\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nYoshua Bengio, R´ejean Ducharme, Pascal Vincent, and\nChristian Jauvin. 2003. A neural probabilistic lan-\nguage model. JMLR, 3(Feb):1137–1155.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nEdouard Grave, Armand Joulin, and Nicolas Usunier.\n2016. Improving neural language models with a con-\ntinuous cache. CoRR.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–\n778.\nHakan Inan, Khashayar Khosravi, and Richard Socher.\n2016. Tying word vectors and word classiﬁers:\nA loss framework for language modeling. arXiv\npreprint arXiv:1611.01462.\nBen Krause, Emmanuel Kahembwe, Iain Murray, and\nSteve Renals. 2018. Dynamic evaluation of neural\nsequence models. In ICML, pages 2771–2780.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv preprint\narXiv:1909.11942.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nG´abor Melis, Chris Dyer, and Phil Blunsom. 2017. On\nthe state of the art of evaluation in neural language\nmodels. arXiv preprint arXiv:1707.05589.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2017. Regularizing and optimizing LSTM\nlanguage models. CoRR.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture mod-\nels. CoRR.\nTom´aˇs Mikolov, Martin Karaﬁ ´at, Luk ´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2010. Recurrent\nneural network based language model. In Eleventh\nAnnual Conference of the International Speech Com-\nmunication Association.\nAndriy Mnih and Geoffrey Hinton. 2007. Three new\ngraphical models for statistical language modelling.\nIn ICML, pages 641–648.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In NAACL, pages 2227–2237.\nPengfei Liu Yunfan Shao Xiangyang Xue Zheng Zhang\nQipeng Guo, Xipeng Qiu. 2019. Star-transformer.\narXiv preprint arXiv:1902.09113.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. URL https://s3-\nus-west-2. amazonaws. com/openai-assets/research-\ncovers/language-unsupervised/language under-\nstanding paper. pdf.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. CoRR, abs/1910.10683.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS, pages 5998–6008.\nAlex Wang, Amapreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nChenguang Wang, Mu Li, and Alexander J. Smola.\n2019. Language models with transformers. CoRR,\nabs/1904.09408.\nChenguang Wang, Yangqiu Song, Ahmed El-Kishky,\nDan Roth, Ming Zhang, and Jiawei Han. 2015. In-\ncorporating world knowledge to document cluster-\ning via heterogeneous information networks. In\nSIGKDD, pages 1215–1224.\nChenguang Wang, Yangqiu Song, Haoran Li, Ming\nZhang, and Jiawei Han. 2016. Text classiﬁcation\nwith heterogeneous information network kernels. In\nAAAI, pages 2130–2136.\n5\nZhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and\nWilliam W. Cohen. 2017. Breaking the softmax bot-\ntleneck: A high-rank RNN language model. CoRR.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. CoRR, abs/1906.08237.\nZihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, and\nZheng Zhang. 2019. Bp-transformer: Modelling\nlong-range context via binary partitioning. arXiv\npreprint arXiv:1911.04070.\nFisher Yu and Vladlen Koltun. 2015. Multi-scale con-\ntext aggregation by dilated convolutions. arXiv\npreprint arXiv:1511.07122.\n6",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8057013750076294
    },
    {
      "name": "Perplexity",
      "score": 0.7913700342178345
    },
    {
      "name": "Computer science",
      "score": 0.6032935976982117
    },
    {
      "name": "Computation",
      "score": 0.5303987264633179
    },
    {
      "name": "Language model",
      "score": 0.37512776255607605
    },
    {
      "name": "Computer engineering",
      "score": 0.3483487367630005
    },
    {
      "name": "Reliability engineering",
      "score": 0.32901790738105774
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2796989679336548
    },
    {
      "name": "Engineering",
      "score": 0.20541808009147644
    },
    {
      "name": "Electrical engineering",
      "score": 0.19604945182800293
    },
    {
      "name": "Algorithm",
      "score": 0.1813783049583435
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}