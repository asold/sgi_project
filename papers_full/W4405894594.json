{
  "title": "Use of large language models as artificial intelligence tools in academic research and publishing among global clinical researchers",
  "url": "https://openalex.org/W4405894594",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4368947466",
      "name": "Tanisha Mishra",
      "affiliations": [
        "Kasturba Medical College, Manipal",
        "Manipal Academy of Higher Education"
      ]
    },
    {
      "id": "https://openalex.org/A2103220785",
      "name": "Edward Sutanto",
      "affiliations": [
        "University of Oxford",
        "University of Indonesia"
      ]
    },
    {
      "id": "https://openalex.org/A2171805861",
      "name": "Rini Rossanti",
      "affiliations": [
        "Dr. Hasan Sadikin General Hospital",
        "Padjadjaran University"
      ]
    },
    {
      "id": "https://openalex.org/A2616751457",
      "name": "Nayana Pant",
      "affiliations": [
        "Royal Free London NHS Foundation Trust"
      ]
    },
    {
      "id": "https://openalex.org/A2890888664",
      "name": "Anum Ashraf",
      "affiliations": [
        "Jinnah Postgraduate Medical Center",
        "Allama Iqbal Medical College"
      ]
    },
    {
      "id": "https://openalex.org/A2476329900",
      "name": "Akshay Raut",
      "affiliations": [
        "SUNY Upstate Medical University",
        "Guthrie Robert Packer Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A5115701927",
      "name": "Germaine Uwabareze",
      "affiliations": [
        "World Health Organization - Pakistan"
      ]
    },
    {
      "id": null,
      "name": "Ajayi Oluwatomiwa",
      "affiliations": [
        "Federal Neuro Psychiatric Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A5030100212",
      "name": "Bushra Zeeshan",
      "affiliations": [
        "Mayo Hospital",
        "Allama Iqbal Medical College",
        "Jinnah Postgraduate Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A4368947466",
      "name": "Tanisha Mishra",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103220785",
      "name": "Edward Sutanto",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2171805861",
      "name": "Rini Rossanti",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2616751457",
      "name": "Nayana Pant",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2890888664",
      "name": "Anum Ashraf",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2476329900",
      "name": "Akshay Raut",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5115701927",
      "name": "Germaine Uwabareze",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Ajayi Oluwatomiwa",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5030100212",
      "name": "Bushra Zeeshan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4366989525",
    "https://openalex.org/W3203912530",
    "https://openalex.org/W4386541616",
    "https://openalex.org/W4376866708",
    "https://openalex.org/W4392195219",
    "https://openalex.org/W4281487897",
    "https://openalex.org/W4392850680",
    "https://openalex.org/W4366769994",
    "https://openalex.org/W4367186868",
    "https://openalex.org/W4395052272",
    "https://openalex.org/W4388347957",
    "https://openalex.org/W4396931743",
    "https://openalex.org/W3193976855",
    "https://openalex.org/W4385417258",
    "https://openalex.org/W3034723486",
    "https://openalex.org/W1819662813",
    "https://openalex.org/W4317853296",
    "https://openalex.org/W4318263917",
    "https://openalex.org/W4318671267",
    "https://openalex.org/W3207456925",
    "https://openalex.org/W4380053021",
    "https://openalex.org/W4368367885",
    "https://openalex.org/W4318931874",
    "https://openalex.org/W4375864285",
    "https://openalex.org/W4321351832",
    "https://openalex.org/W4387144847"
  ],
  "abstract": "With breakthroughs in Natural Language Processing and Artificial Intelligence (AI), the usage of Large Language Models (LLMs) in academic research has increased tremendously. Models such as Generative Pre-trained Transformer (GPT) are used by researchers in literature review, abstract screening, and manuscript drafting. However, these models also present the attendant challenge of providing ethically questionable scientific information. Our study provides a snapshot of global researchers' perception of current trends and future impacts of LLMs in research. Using a cross-sectional design, we surveyed 226 medical and paramedical researchers from 59 countries across 65 specialties, trained in the Global Clinical Scholars' Research Training certificate program of Harvard Medical School between 2020 and 2024. Majority (57.5%) of these participants practiced in an academic setting with a median of 7 (2,18) PubMed Indexed published articles. 198 respondents (87.6%) were aware of LLMs and those who were aware had higher number of publications (p < 0.001). 18.7% of the respondents who were aware (n = 37) had previously used LLMs in publications especially for grammatical errors and formatting (64.9%); however, most (40.5%) did not acknowledge its use in their papers. 50.8% of aware respondents (n = 95) predicted an overall positive future impact of LLMs while 32.6% were unsure of its scope. 52% of aware respondents (n = 102) believed that LLMs would have a major impact in areas such as grammatical errors and formatting (66.3%), revision and editing (57.2%), writing (57.2%) and literature review (54.2%). 58.1% of aware respondents were opined that journals should allow for use of AI in research and 78.3% believed that regulations should be put in place to avoid its abuse. Seeing the perception of researchers towards LLMs and the significant association between awareness of LLMs and number of published works, we emphasize the importance of developing comprehensive guidelines and ethical framework to govern the use of AI in academic research and address the current challenges.",
  "full_text": "Use of large language models \nas artificial intelligence tools in \nacademic research and publishing \namong global clinical researchers\nTanisha Mishra 1, Edward Sutanto 2,3, Rini Rossanti 4, Nayana Pant 5, Anum Ashraf 6, \nAkshay Raut 7, Germaine Uwabareze 8, Ajayi Oluwatomiwa 9 & Bushra Zeeshan 10,11\nWith breakthroughs in Natural Language Processing and Artificial Intelligence (AI), the usage of Large \nLanguage Models (LLMs) in academic research has increased tremendously. Models such as Generative \nPre-trained Transformer (GPT) are used by researchers in literature review, abstract screening, and \nmanuscript drafting. However, these models also present the attendant challenge of providing \nethically questionable scientific information. Our study provides a snapshot of global researchers’ \nperception of current trends and future impacts of LLMs in research. Using a cross-sectional design, \nwe surveyed 226 medical and paramedical researchers from 59 countries across 65 specialties, trained \nin the Global Clinical Scholars’ Research Training certificate program of Harvard Medical School \nbetween 2020 and 2024. Majority (57.5%) of these participants practiced in an academic setting with \na median of 7 (2,18) PubMed Indexed published articles. 198 respondents (87.6%) were aware of LLMs \nand those who were aware had higher number of publications (p < 0.001). 18.7% of the respondents \nwho were aware (n = 37) had previously used LLMs in publications especially for grammatical errors \nand formatting (64.9%); however, most (40.5%) did not acknowledge its use in their papers. 50.8% \nof aware respondents (n = 95) predicted an overall positive future impact of LLMs while 32.6% were \nunsure of its scope. 52% of aware respondents (n = 102) believed that LLMs would have a major impact \nin areas such as grammatical errors and formatting (66.3%), revision and editing (57.2%), writing \n(57.2%) and literature review (54.2%). 58.1% of aware respondents were opined that journals should \nallow for use of AI in research and 78.3% believed that regulations should be put in place to avoid its \nabuse. Seeing the perception of researchers towards LLMs and the significant association between \nawareness of LLMs and number of published works, we emphasize the importance of developing \ncomprehensive guidelines and ethical framework to govern the use of AI in academic research and \naddress the current challenges.\nKeywords Large language models, Artificial intelligence, Academic writing, Biomedical research\nLarge Language Models (LLMs) represent a significant breakthrough in Natural Language Processing (NLP) \nand Artificial Intelligence (AI) 1. Prior to 2017, while NLP models could perform several language processing \ntasks, they were not easily accessible to non-domain experts. The introduction of the Transformer architecture \nin 2017 revolutionized the field, enabling NLP models to efficiently synthesize and analyze datasets using simple \nprompts. This allowed large-scale use by people worldwide, significantly broadening access to advanced language \nprocessing tools2. The Transformer technology led to the development of two game changers: Bidirectional \n1Kasturba Medical College, Manipal, Manipal Academy of Higher Education, Manipal, Karnataka 576104, India. \n2Nuffield Department of Medicine, Centre for Tropical Medicine and Global Health, University of Oxford, Oxford \nOX3 7LG, UK. 3Faculty of Medicine, Oxford University Clinical Research Unit Indonesia, Universitas Indonesia, \nJakarta 10430, Indonesia. 4Department of Child Health, Dr. Hasan Sadikin General Hospital/Faculty of Medicine, \nUniversitas Padjadjaran, Bandung, Indonesia. 5Royal Free NHS Foundation Trust Hospital, Pond Street, London \nNW32QG, UK. 6Department of Pharmacology & Therapeutics, Allama Iqbal Medical College, Jinnah Hospital, \nLahore, Pakistan. 7Department of Internal Medicine, Guthrie Robert Packer Hospital, Sayre, PA 18840, USA. 8World \nHealth Organization, Comoros Country Office, Moroni, Comoros. 9Federal Neuropsychiatric Hospital Yaba, Lagos, \nNigeria. 10Department of Dermatology, Niazi Hospital, Lahore, Pakistan. 11Allama Iqbal Medical College, Jinnah \nHospital, Lahore, Pakistan. email: dr_bushrahassan@hotmail.com\nOPEN\nScientific Reports |        (2024) 14:31672 1| https://doi.org/10.1038/s41598-024-81370-6\nwww.nature.com/scientificreports\n\nEncoder Representations from Transformers (BERT) and Generative Pretrained Transformer (GPT), that used \nsemi-supervised approach and acquired exceptional generalization capabilities with the ability to interpret and \ngenerate human-like text3. The launch of ChatGPT in 2022 gained public attention in almost every field of life \nowing to its accessibility and user-friendly interface. LLMs offer AI driven support particularly in literature \nreview, summarizing articles, abstract screening, extracting data and drafting manuscript. Due to workload \nreduction and the ease offered, there has been an increasing interest in the incorporation of LLMs like ChatGPT, \nPerplexity, Llama by Meta (formerly Facebook), Google Bard and Claude, in academic research, as indicated by \na rapid increase in the number of articles after ChatGPT’s release3,4.\nAlthough there are numerous efficiency gains in utilizing LLMs in research, they however cannot replace \nhumans particularly in contexts where meticulous understanding and original thought, along with accountability \nare crucial5,6. With deeper understanding of LLMs, it was found that LLMs are also capable of generating fake \ncitations, rapidly generating large volumes of questionable information, and also amplifying biases 3,7. This \nhas led to negative ethical implications like authorship integrity and a surge in predatory practices and as a \nconsequence, an “ AI-driven infodemic” has emerged5. There is also a risk of public health threat resulting from \nghost-written scientific articles, fake news and misinforming content3. In addressing these issues, as a first step, it \nis pertinent to understand attitudes of researchers towards LLMs in research by assessing researcher’s awareness \nand practices of the use of LLMs.\nOur study provides a unique analysis of a targeted group of medical and paramedical researchers enrolled \nin a one-year-certification course- Global Clinical Scholars Research Training (GCSRT) Program, at Harvard \nMedical School (HMS). We aim to provide insights into the current trends in AI usage in research and publication \nalong with a peek into the future scope and impact of LLMs. We strongly believe that the results of our study can \naid journals to formulate future policies regarding the use of AI tools in the process of publication, thus ensuring \ncredibility and maintaining integrity of medical publications.\nMethods\nStudy design and population\nThis global survey was carried out using a cross-sectional design. It was conducted between April and June 2024, \namongst a diverse group of medical and paramedical researchers who received training at the GCSRT program \nat Harvard. This program consists of researchers from over 50 countries and 6 continents spanning various \nspecialties, career stages, age groups and genders. At the program, all participants receive advanced training in \nevery stage of research including statistical analysis, publishing and grant writing 8. They are therefore an ideal \ngroup to assess AI tools usage in research.\nStudy objectives\nWe had three primary objectives for this study. First, to assess the level of awareness of LLMs amongst global \nresearchers. Second, to identify how LLMs are currently used in academic research and publishing amongst our \nsurvey respondents. And third, to analyze the potential future impact and ethical implications of AI tools in \nmedical research and publishing.\nEligibility criteria\n (a)  Inclusion Criteria: Medical and paramedical researchers who have been participants of the GCSRT pro -\ngram at HMS belonging to any cohort between 2020 and 2024, irrespective of their country of origin, \nresearch interests, active years in research, age or gender. Researchers who were members of the unofficial \nclass WhatsApp groups and were proficient in reading and writing in English language were included spe-\ncifically.\n (b)  Exclusion Criteria: Researchers from cohorts outside of the above specified years, those who were not ac -\ncessible through class WhatsApp groups, or were not proficient in reading and writing in English language \nwere excluded from the study. Medical and paramedical researchers who have not undergone training at \nthis program as well as non-medical researchers, were not invited for this study.\nQuestionnaire development and survey dissemination strategy\nThe survey was drafted using Google Forms, in English Language. It consisted of a total of 4 sections to cover \nour primary objectives- (1) Background, (2) Awareness of LLMs, (3) Impact of LLMs and (4) Future Policy. \nEach question was carefully reviewed for its relevance, validity, and unbiasedness. Data collectors for the study \nwere voluntarily chosen from amongst the participants of the GCSRT Program. The data collectors from each \nof the targeted cohorts were made primary in-charge of reaching out to our target population in their cohort \nvia personal messaging on WhatsApp and LinkedIn. The contact information of the survey respondents was \nobtained from the unofficial class WhatsApp groups and personal networks of the data collectors. A total of 3 \npersonal messages including 2 reminders, spaced 7 days apart each, were sent to each prospective participant. \nInformed consent was obtained, and Google survey forms were filled out by a total of 226 researchers from over \n59 countries.\nSample size and statistical methods\nThe link to the Google survey form was distributed to 5 cohorts of the GCSRT program consisting of a total of \n550 medical and paramedical researchers. A total sample size of 220 was calculated by considering a margin of \nerror of 5%, a confidence level of 95% and power of 0.8. Descriptive statistics of the survey respondents were \npresented as mean ± standard deviation for normally distributed continuous data, median (interquartile range) \nfor non-normally distributed continuous data, and frequencies & percentages for categorical data. Continuous \nScientific Reports |        (2024) 14:31672 2| https://doi.org/10.1038/s41598-024-81370-6\nwww.nature.com/scientificreports/\ndata were tested for normality using the Shapiro–Wilk test. Normally distributed data were analyzed using one \nway ANOV A while non-normally distributed data were analyzed using the Kruskal-Wallis test. Categorical \ndata were analyzed with Chi-squared test or Fisher’s exact test. Qualitative data from open-ended questions \nwere studied via thematic analysis. All statistical analyses were performed in Stata MP version 17.0 (StataCorp, \nCollege Station, TX, USA). All tests were two-tailed and considered significant at P < 0.05.\nEthical consideration\nIn accordance with the declaration of Helsinki8, this study was approved by the ethical review board at Allama \nIqbal Medical College/ Jinnah Hospital Lahore, Pakistan (Reference no: ERB 163/9/30-04-2024/S1 ERB). \nIt is not supported or endorsed by HMS. However, timely notification about the study was provided to the \nadministration of the GCSRT program. Consent to participate was collected from every respondent as the \nfirst, mandatory response to the questionnaire. All personal information like email ID, nationality, and age was \ncarefully de-identified and handled confidentially. The respondents were provided necessary information on the \nvoluntariness of the study as well as contact information of the principal investigator.\nResults\nWe analyzed the responses of 226 global researchers from over 59 countries and practicing across 65 different \nmedical and paramedical specialties. Across the various countries of origin (Supplementary Table S1), the two \nmost common regions of origin were the region of Americas (23.5%) and the South-East Asian region (23.5%).\nTable 1 Represents academic and demographic characteristics of our survey respondents and compares \nthem amongst respondents who were aware of LLMs to those who were not. The median of PubMed indexed \nCharacteristics\nOverall\n(N = 226)\nAwareness of LLMs\np-value\nYe s\n(n = 198)\nNo\n(n = 28)\nAge (in years)\n 21–40 133 (58.8) 118 (59.6) 15 (53.6)\n0.688 41–60 86 (38.0) 74 (37.4) 12 (42.9)\n 60+ 7 (3.2) 6 (3.0) 1 (3.5)\nRegion of Origin\n African Region 30 (13.3) 27 (13.6) 3 (10.7)\n0.728\n Region of the Americas 53 (23.5) 45 (22.7) 8 (28.6)\n South-East Asian Region 53 (23.5) 49 (24.8) 4 (14.3)\n European Region 29 (12.8) 26 (13.1) 3 (10.7)\n Eastern Mediterranean Region 41 (18.1) 34 (17.2) 7 (25.0)\n Western Pacific Region 20 (8.8) 17 (8.6) 3 (10.7)\nArea of Practice*\n Medical Speciality 146 (64.6) 127 (64.1) 19 (67.8)\n-**\n Surgical Specialty 57 (25.2) 52 (26.2) 5 (17.8)\n Pharmacy 8 (3.5) 7 (3.5) 1 (3.5)\n Clinical Research 62 (27.4) 53 (26.8) 9 (32.1)\n Other 14 (6.2) 14 (100.0) 0 (0.0)\nType of Practice*\n Academic (such as college or university) 129 (57.1) 117 (59.1) 12 (42.9)\n-**\n Public/Community hospital, Health \nCenter, or Clinic 68 (30.1) 57 (28.8) 11 (39.3)\n Private Hospital, Health Center, or Clinic 56 (24.8) 49 (24.7) 7 (25.0)\n Non-profit Organization 24 (10.6) 23 (11.6) 1 (3.6)\n Industry / Pharmaceutical Company 26 (11.5) 20 (10.1) 6 (21.4)\n Government, such as Ministry of Health 20 (8.8) 18 (9.1) 2 (7.1)\n Other 6 (2.7) 5 (2.5) 1 (3.6)\nNumber of PubMed Indexed Publications 7 (2–18) 8 (3–20) 3 (0–7) < 0.001\nActive Y ears in Medical Research\n 0–5 years 106 (46.9) 89 (45.0) 17 (60.7)\n0.101 6–10 years 71 (31.4) 67 (33.8) 4 (14.3)\n 10 + years 49 (21.7) 42 (21.2) 7 (25.0)\nTable 1. Academic and Demographic Characteristics of Survey Respondents Abbreviations: LLMs, large \nlanguage models. Data are presented in n (%) or median (interquartile range). *May add up to more than 100% \nas respondents were able to select multiple options. **No statistical test was conducted as individuals may \nselect multiple options.\n \nScientific Reports |        (2024) 14:31672 3| https://doi.org/10.1038/s41598-024-81370-6\nwww.nature.com/scientificreports/\npublications among survey respondents were 7 (interquartile range: 2–18). 198 (87.6%) survey respondents were \npreviously aware of LLMs. None of the characteristics were significantly associated with awareness of LLMs, \nexcept the number of PubMed indexed publications. Those who were aware of the use of LLMs have a higher \nnumber of publications compared to those who were not aware (p < 0.001).\nTable 2 represents aware respondents’ ( n = 198) knowledge, attitudes and practices with respect to LLMs. \nMost were somewhat and moderately familiar with LLMs (33.3% and 30.8%, respectively). Of these aware \nrespondents, the ones that have personally used LLMs previously (18.7%), mostly used it for grammatical error \nand formatting (64.9%), followed by writing (45.9%) and finally revision and editing (45.9%). When stratified \nwith the number of active years in medical research, none of these variables were significantly associated.\nFigure 1 displays the level of perceived future impact of LLMs in various stages of publication amongst aware \nrespondents. Majority believe that LLMs will have a major overall impact (52.0%). Areas that will be majorly \nimpacted are grammatical errors and formatting (66.3%), revision and editing (57.2%), and writing (57.2%). \nAreas that will not be impacted to moderately impacted were methodology (74.3%), journal selection (73.3%), \nand study ideas (71.1%).\nTable 3 represents aware respondent perceptions on future scope of LLMs. Majority perceived that it will \nbring a positive impact (50.8%), yet a significant proportion were unsure (32.6%). While most respondents \nbelieve that journal should allow usage of AI tools in publishing (58.1%), the majority (78.3%) also believe that \nsome regulations (i.e. modified journal policies, AI review boards, tools to detect LLM usage) should be put in \nVariables\nOverall\n(n = 198)\nActive Y ears In Academic Research\np-value\n0–5 years\n(n = 89)\n6–10 years old\n(n = 67)\n10 + years old\n(n = 42)\nLevel of Awareness\n Slightly familiar 57 (28.8) 22 (24.7) 21 (31.3) 14 (33.3)\n0.586\n Somewhat familiar 66 (33.3) 36 (40.5) 20 (29.9) 10 (23.8)\n Moderately familiar 61 (30.8) 26 (29.2) 21 (31.3) 14 (33.3)\n Extremely familiar 14 (7.1) 5 (5.6) 5 (7.5) 4 (9.5)\nAwareness of LLMs Prior to 2022\n Y es 27 (13.6) 10 (11.2) 12 (17.9) 5 (11.9)\n0.453\n No 171 (86.4) 79 (88.8) 55 (82.1) 37 (88.1)\nUsage of LLMs in Previous Research/Publication\n Y es 37 (18.7) 16 (18.0) 9 (13.4) 12 (28.6)\n0.139\n No 161 (81.3) 73 (82.0) 58 (86.6) 30 (71.4)\nLevel of Use\n 1 (Rarely) 6 (16.2) 2 (12.5) 1 (11.1) 3 (25.0)\n0.891\n 2 4 (10.8) 1 (6.3) 1 (11.1) 2 (16.7)\n 3 (Moderately) 11 (29.7) 5 (31.3) 3 (33.3) 3 (25.0)\n 4 8 (21.6) 4 (25.0) 3 (33.3) 1 (8.3)\n 5 (Frequently) 8 (21.6) 4 (25.0) 1 (11.1) 3 (25.0)\nStage of Publication LLM Used Previously*\n Study ideas 14 (37.8) 6 (37.5) 3 (33.3) 5 (41.7)\n-**\n Literature Review 15 (40.5) 10 (62.5) 2 (22.2) 3 (25.0)\n Methodology 9 (24.3) 4 (25.0) 3 (33.3) 2 (16.7)\n Data Analysis 10 (27.0) 3 (18.8) 3 (33.3) 4 (33.3)\n Writing 17 (45.9) 9 (56.3) 2 (22.2) 6 (50.0)\n Journal Selection 2 (5.4) 1 (6.3) 0 (0.0) 1 (8.3)\n Revision and Editing 17 (45.9) 8 (50.0) 5 (55.6) 4 (33.3)\n Grammatical error and formatting 24 (64.9) 10 (62.5) 6 (66.7) 8 (66.7)\n References 5 (13.5) 2 (12.5) 1 (11.1) 2 (16.7)\nAcknowledgement of Use of LLM in Previous Publication*\n Did not acknowledge the use of LLM 15 (40.5) 5 (31.3) 4 (44.4) 6 (50.0)\n-**\n Cited the specific LLM model used (e.g., GPT-3) in the \nacknowledgments 6 (16.2) 4 (25.0) 1 (11.1) 1 (8.3)\n Included a statement in the Methods section describing the use of LLMs 7 (18.9) 4 (25.0) 1 (11.1) 2 (16.7)\n Disclosed LLM usage in the funding or conflict of interest statement 3 (8.1) 1 (6.3) 0 (0.0) 2 (16.7)\n Provided a footnote or disclaimer regarding LLM-generated content 5 (13.5) 4 (25.0) 0 (0.0) 1 (8.3)\n Others 9 (24.3) 2 (12.5) 3 (33.3) 4 (33.3)\nTable 2. Knowledge, attitudes and practices of Aware respondents. Abbreviations: LLMs, large language \nmodels. Data are presented in n (%). *May add up to more than 100% as respondents were able to select \nmultiple options. **No statistical test was conducted as respondents may select multiple options.\n \nScientific Reports |        (2024) 14:31672 4| https://doi.org/10.1038/s41598-024-81370-6\nwww.nature.com/scientificreports/\nplace to make AI tools in publishing ethical. When stratified with the number of active years in medical research, \nnone of these variables were significantly associated.\nIn our survey, 79% ( n = 179) of the respondents were willing to share their overall opinion into future \nscope and challenges of LLMs. Their views fall into one or more categories in Fig.  2. 28% (n = 64) respondents \nexpressed that LLMs are helpful tools in the publication process, particularly in organizing and writing large \ntopics in a systematic way. Additionally, around a quarter of respondents ( n = 55) stated that with the use of \nLLMs, researchers are able to spend less time on different sections of their research projects such as literature \nreview, data analysis, and manuscript preparation. However, the survey respondents also revealed several \nconcerns and challenges associated with using LLMs in academic research. 14% ( n = 33) of the respondents \nexpressed uncertainty or their lack of experience with LLMs. Ethical apprehensions about the use of LLMs in \nacademic research and publication, including potential biases, privacy issues, and plagiarism, were noted in 8% \nof the participants (n = 18).\nDiscussion\nAI has generated seismic waves around the world; the field of research is no exception. Our study assessed the \nawareness, trends of usage and future scope of LLMs to better analyze this impact in the field of academia. It \ncaptured the perception of researchers from all walks of medical and paramedical research representing 59 \ncountries and spanning 65 specialties. Our respondents mainly belonged to Medical subspecialties (64.6%) \nrather than Surgical or Paramedical subspecialties, similar (68%) to respondent characteristics seen in a study \nby Abdelhafiz et al. 9 Our respondents were mostly working in academic settings (57.1%) followed by public \nand private healthcare settings, similar to the study by Abdelhafiz et al. where 75% of participants were from \nuniversities or research centers 9. The respondents with 10 +, 6–10 and 0–5 years of research experience were \n21.7%, 31.4% and 46.9% respectively, suggesting that our target population well represented academicians at \nvarying stages of their careers.\nA significant majority of our respondents (87.6%) were aware of LLMs, which was similar (85%) to a survey \nconducted among medical students in Jordan 10, and higher in comparison to a study done in Pakistan where \nonly 21.3% of the respondents had familiarity with AI11. A plausible explanation for the high level of awareness \namongst GCSRT participants could be that they had already completed an advanced training in research, and \nmight have come across the applications of LLMs in contemporary research and publication during this training \nperiod. Also, their keen interest in research might have rendered them to explore the latest advancements in this \nfield, amongst which usage of LLMs and AI tools probably tops the list 12,13. Interestingly, the participants who \nwere aware of LLMs had a higher number of publications compared to those who were not (p-value < 0.001).This \nfinding coincides with previous studies where it has been reported that greater familiarity and access to LLMs \nis associated with a greater pre-print and publication turn-out ratio amongst academic authors, probably due to \nthe fast-paced nature of LLMs research and the use of LLMs for writing assistance14. None of the other variables \nlike age, country of respondent, or field of practice were significantly associated with awareness of the use of \nLLMs. An overwhelmingly large proportion of the respondents who were aware of LLMs, reported that they \nFig. 1. Future of LLMs in various stages of the publication process.\n \nScientific Reports |        (2024) 14:31672 5| https://doi.org/10.1038/s41598-024-81370-6\nwww.nature.com/scientificreports/\nFig. 2. Overall Opinion on Future Scopes & Challenges (Thematic Categories).\n \nOverall\n(n = 198)\nActive Y ears in Academic Research\np-\nvalue\n0–5 years\n(n = 89)\n6–10 years old\n(n = 67)\n10 + years old\n(n = 42)\nAnticipated Future Impact of LLMs\n Strongly positive 18 (9.6) 7 (8.3) 5 (7.7) 6 (15.8)\n0.143\n Positive 95 (50.8) 44 (52.4) 27 (41.5) 24 (63.2)\n Cannot say 61 (32.6) 27 (32.1) 27 (41.5) 7 (18.4)\n Negative 9 (4.8) 5 (6.0) 4 (6.1) 0 (0.0)\n Strongly negative 4 (2.1) 1 (1.2) 2 (3.1) 1 (2.6)\nShould Journals Allow Usage of AI Tools in Publications?\n Y es 115 (58.1) 48 (53.9) 39 (58.2) 28 (66.7)\n0.741 No 31 (15.7) 15 (16.9) 11 (16.4) 5 (11.9)\n Cannot say 52 (26.3) 26 (29.2) 17 (25.4) 9 (21.4)\nPotential Problems with Using AI in Research Publications*\n Impaired creativity 119 (60.1) 52 (58.4) 43 (64.2) 24 (57.1)\n-**\n Data security 100 (50.5) 43 (48.3) 33 (49.3) 24 (57.1)\n Misinformation 132 (66.7) 60 (67.4) 41 (61.2) 31 (73.8)\n Unintended bias 130 (65.7) 57 (64.0) 45 (67.2) 28 (66.7)\n Lack of accountability 115 (58.1) 59 (66.3) 36 (53.7) 20 (47.6)\n Copyright \ninfringement 125 (63.1) 58 (65.2) 45 (67.2) 22 (52.4)\n Impersonation 92 (46.5) 38 (42.7) 33 (49.3) 21 (50.0)\n Others 16 (8.1) 7 (7.9) 5 (7.4) 4 (9.5)\nIs The Use of AI Tools in Publishing Ethical?\n Y es, in current form 21 (10.6) 9 (10.1) 8 (11.9) 4 (9.5)\n0.072\n Only if some \nregulations like \nmodified journal \npolicies, AI review \nboards or tools to \ndetect LLM usage are \nput in place\n155 (78.3) 65 (73.0) 54 (80.6) 36 (85.7)\n Never 9 (4.5) 9 (10.1) 0 (0.0) 0 (0.0)\n Others 13 (6.6) 6 (6.7) 5 (7.4) 2 (4.8)\nTable 3. Insights into the future scope of LLMs from aware respondents. LLMs, large language models; AI: \nArtificial intelligence. Data are presented in n (%). *May add up to more than 100% as respondents were able \nto select multiple options. **No statistical test was conducted as respondents may select multiple options.\n \nScientific Reports |        (2024) 14:31672 6| https://doi.org/10.1038/s41598-024-81370-6\nwww.nature.com/scientificreports/\nwere not aware of AI tools prior to 2022 (86.4%). This corresponds to a compelling trajectory of publications \npertaining to LLM in medical research, from May 2021 to July 202315.\n81.3% of our aware respondents never previously used LLMs in their research projects or publications. This \nis in contrast to Eppler et al.‘s earlier study16, where nearly half of the respondents reported having used LLMs in \ntheir academic practice. Amongst those who previously used LLMs in their publications, most rated their usage \nto be of moderate to frequent intensity for tasks such as grammatical error corrections, editing and manuscript \nwriting. These results were in concordance with the study by Eppler et al.16, which showed that the most common \nuse of LLMs in scientific publishing was for writing (36.6%) followed by checking grammar (30.6%). With the \nhelp of LLMs based on NLP , it is possible to conveniently rectify grammatical errors using categorization models \nand algorithm-based sentence construction 17,18. Despite the frequent use of LLMs for various components of \nacademic writing, a considerable proportion of these respondents (~ 40%) did not acknowledge its usage in \ntheir publications. There are multiple reasons as to why a researcher may not reveal the inclusion of AI tools in \ntheir research papers. First, is the lack of information or comprehension on part of the researchers regarding the \ntechnologies they are using, due to which they remain oblivious to the degree to which AI has been integrated \ninto their research19,20. And second, is the skepticism or negative perceptions associated with the use of AI – like \nthe notion that a machine was deployed to generate proposals or scientific discussion of their study 21. Thus, \nthe question of whether or not to acknowledge the use of AI in research studies remains an ethical imbroglio. \nPublishers may ask authors to submit or include a declaration about whether or not they have used AI systems \nin their writing22–24.\nFigures 1 and 2; Table 3 provide insights into future scopes and challenges of LLMs among global researchers. \nFigure 1 reveals a substantial belief in the transformative potential of LLMs, with slightly more than half of \nrespondents anticipating a major overall impact. Specific areas identified as being most significantly influenced \nby LLMs in the future included grammatical errors and formatting, revision and editing, writing, and literature \nreview. These results align with current literature suggesting that LLMs can greatly enhance the efficiency and \naccuracy of these tasks, thus facilitating quicker and higher-quality academic outputs25–27. Conversely, the areas \nperceived to be less impacted, such as methodology, journal selection, and study ideas, reflect apprehension \nabout AI to critically assess research design and journal suitability.\nAs shown in Table  3, slightly more than half of the participants view the impact of LLMs positively, yet \naround one-third of them remain uncertain. This uncertainty underscores a significant concern regarding the \nethical implications and potential misuse of AI technologies. Ethical concerns are well-documented in existing \nstudies that highlight issues such as data privacy, misinformation, and unintended biases that can arise from \nAI-generated content28,29. In addition, our study reveals that while the majority of respondents support the use \nof AI tools in publishing, there is a strong consensus on the necessity of implementing regulatory measures, \nsuch as modified journal policies, AI review boards, and tools to detect LLM usage. This finding is consistent \nwith broader ethical guidelines proposed in the literature, which advocate for robust oversight and ethical \nframeworks to mitigate the risks associated with AI deployment in sensitive fields, such as medical research27,30. \nInterestingly, the perception of AI’s ethical use varies with experience levels. In our study, participants with more \nthan 10 years of research experience were more likely to view AI tools as positive and support their use under \nregulated conditions compared to those with lesser years of research experience; however, this result was not \nstatistically significant.\nConclusions\nThe discipline of academic writing has seen a noticeable transformation following the advent of LLMs, with \nan increasing number of researchers incorporating these tools at varying stages of their research publications. \nHowever, as the applications of LLMs rise, there is a corresponding rise in the concerns regarding their validity, \naccountability, potential exploitation and ethical implications.\nWhile there is a broad recognition of the beneficial impact of LLMs on certain aspects of academic research \nand publishing, addressing associated ethical risks and apprehensions is of paramount importance. Our study \nemphasizes the need for developing comprehensive guidelines and ethical frameworks to govern the use of \nAI in medical and paramedical research. The growing utility of LLMs necessitates the implementation of such \nregulatory policies promptly to ensure their safe, responsible and effective usage.\nLimitations\nOur study has certain methodological limitations which need to be addressed. First, since this is a cross-sectional \nstudy, causal inferences cannot be drawn from the findings as well as the temporal relevance of our study \nfindings are subjected to change over time. Second, despite our extensive attempts to maintain the anonymity of \nsurvey responses, the study findings are prone to social desirability bias. Third, since our study population was \nexclusively limited to the participants of the GCSRT program with extensive knowledge surrounding academic \nresearch, a selection bias could have been introduced which limits the generalizability of the study findings. \nFourth, our study did not collect several respondent’s characteristics that may be associated with awareness \nof the use of LLMs, such as sex, level of education, and level of income. And finally, our study is susceptible \nto sampling bias due to the use of WhatsApp and LinkedIn for data collection. Participants using LinkedIn \nmight be concurrently using other platforms for their research, while participants using WhatsApp might be \nyounger and more well-versed in technology and AI. Hence, concerns about the overrepresentation of certain \ndemographics could affect the external validity of the findings.\nData availability\nThe data of this study will be shared upon reasonable request to the corresponding author.\nScientific Reports |        (2024) 14:31672 7| https://doi.org/10.1038/s41598-024-81370-6\nwww.nature.com/scientificreports/\nReceived: 25 June 2024; Accepted: 26 November 2024\nReferences\n 1. Naveed, H. et al. A comprehensive overview of large language models. (2023). https://arxiv.org/abs/2307.06435.\n 2. Radford, A., Narasimhan, K., Salimans, T. & Sutskever, I. Improving language understanding by generative pre-training (2018). \nhttps://cdn. openai.com/r esearch-cove rs/language -unsupervised/language_understanding_paper.pdf,.\n 3. De Angelis, L. et al. ChatGPT and the rise of large language models: The new AI-driven infodemic threat in public health. Front. \nPublic. Health 11, (2023).\n 4. Bornmann, L., Haunschild, R. & Mutz, R. Growth rates of Modern Science: A latent piecewise growth curve Approach to Model \npublication numbers from established and New Literature databases. Humanit. Soc. Sci. Commun. 8, (2021).\n 5. Kendall, G. & Teixeira Da Silva, J. A. Risks of abuse of large language models, like ChatGPT, in scientific publishing: Authorship, \npredatory publishing, and paper mills. Learn. Pub 37, (2024).\n 6. Rane, N., Choudhary, S. P ., Tawde, A. & Rane, J. ChatGPT is not capable of serving as an author: Ethical concerns and challenges \nof large language models in education. IRJMETS 5, (2023).\n 7. Hosseini, M. & Horbach, S. P . J. M. Fighting reviewer fatigue or amplifying bias? Considerations and recommendations for use of \nChatGPT and other large language models in scholarly peer review. Res. Integr. Peer Rev. 8, (2023).\n 8.  h t t    p s  : /  /    w w  w .  w m  a . n e  t  /  p o l  i  c i e s   - p o s t   /  w m a -   d e c l  a r a t  i o n  - o f - h e l s i n k i - e t h i c a l - p r i n c i p l e s - f o r - m e d i c a l - r e s e a r c h - i n v o l v i n g - h u m a n - s u b j \ne c t s /       \n 9. Abdelhafiz, A. S. et al. Knowledge, perceptions and attitude of researchers towards using ChatGPT in Research. J. Med. Syst. 48, \n(2024).\n 10. Al Saad, M. M. et al. Medical students’ knowledge and attitude towards Artificial Intelligence: An online survey. Open. Public. \nHealth J. 15, (2022).\n 11. Umer, M. et al. Investigating awareness of artificial intelligence in healthcare among medical students and professionals in Pakistan: \nA cross-sectional study. Ann. Med. Surg. 86, (2024).\n 12. Watkins, R. Guidance for researchers and peer-reviewers on the ethical use of large language models (LLMs) in scientific research \nworkflows. AI Ethics (2023).\n 13. Li, H. et al. Ethics of large language models in medicine and medical research. Lancet Digit. Health 5, (2023).\n 14. Liang, W . et al. Mapping the Increasing Use of LLMs in Scientific Papers. (2024). https://arxiv.org/abs/2404.01268v1,.\n 15. Meng, X. et al. The application of large language models in medicine: A scoping review. iScience 27, (2024).\n 16. Eppler, M. et al. Awareness and use of ChatGPT and large Language models: A prospective cross-sectional global survey in \nUrology. Eur. Urol. 85, (2024).\n 17. Cano, P . M. A. et al. Natural language processing of grammar checker tools for academic writing: A systematic literature review. J. \nElectr. Syst. 20, (2024).\n 18. Wu, L. & Pan, M. English grammar detection based on LSTM-CRF machine learning model. Comput Intell Neurosci (2021). \n(2021).\n 19. Christou, P . A. Critical perspective over whether and how to acknowledge the use of Artificial Intelligence (AI) in qualitative \nstudies. Qual. Rep. 28, (2023).\n 20. Bender, E. M. & Koller, A. Climbing towards NLU: On meaning, form, and understanding in the age of data. Proceedings of the 58th \nAnnual Meeting of the Association for Computational Linguistics (2020).\n 21. Barocas, S. & Selbst, A. D. Big Data’s disparate impact. Calif. L Rev. 104, (2016).\n 22. Tools such. As ChatGPT threaten transparent science; here are our ground rules for their use. Nature 613, (2023).\n 23. Thorp, H. H. ChatGPT is fun, but not an author. Science 379, (2023).\n 24. Flanagin, A., Bibbins-Domingo, K., Berkwits, M. & Christiansen, S. L. Nonhuman Authors and Implications for the Integrity of \nScientific Publication and Medical Knowledge. JAMA 329, (2023).\n 25. Wagner, G., Lukyanenko, R. & Paré, G. Artificial intelligence and the conduct of literature reviews. J. Inf. Technol. 37, (2022).\n 26. Patil, S. & Tovani-Palone, M. R. The rise of intelligent research: How should artificial intelligence be assisting researchers in \nconducting medical literature searches? Clinics 78, (2023).\n 27. Dave, T., Athaluri, S. A. & Singh, S. ChatGPT in medicine: An overview of its applications, advantages, limitations, future prospects, \nand ethical considerations. Front. Artif. Intell. 6, (2023).\n 28. Biswas, S. ChatGPT and the future of Medical writing. Radiology 307, (2023).\n 29. Preiksaitis, C. & Rose, C. Opportunities challenges, and future directions of generative artificial intelligence in medical education: \nScoping review. JMIR Med. Educ. 20, (2023).\n 30. Alkaissi, H. & McFarlane, S. I. Artificial hallucinations in ChatGPT: Implications in scientific writing. Cureus 19, (2023).\nAcknowledgements\nWe would like to sincerely acknowledge Dr. Le Huu Nhat Minh for his invaluable contribution in collecting 27 \nresponses for this study. Additionally, we express our heartfelt gratitude to all research scholars of the GCSRT \nprogram at Harvard for participating in our survey and helping us complete this study.\nAuthor contributions\nT.M. conceptualized the study, defined the context and purpose of the study and drafted the survey question -\nnaire; B.Z, T.M., A.R., G.U., R.R. and N.P . collected data for the study. E.S., G.U. and T.M. performed data \ncleaning, data analysis and drafted tables. T.M, B.Z., E.S., R.R., N.P ., A.A. A.R. and A.O. provided expertise for \nthe study, conducted literature review and drafted the first draft of the manuscript. All authors were involved in \nthe critical review of the manuscript. B.Z. and T.M. provided supervision, organization of the manuscript and \nproject administration. All authors have made significant contributions and reviewed the final version of the \nmanuscript.\nDeclarations\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary Information The online version contains supplementary material available at  h t t p s : / / d o i . o r g / 1 \nScientific Reports |        (2024) 14:31672 8| https://doi.org/10.1038/s41598-024-81370-6\nwww.nature.com/scientificreports/\n0 . 1 0 3 8 / s 4 1 5 9 8 - 0 2 4 - 8 1 3 7 0 - 6     .  \nCorrespondence and requests for materials should be addressed to B.Z.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives \n4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in \nany medium or format, as long as you give appropriate credit to the original author(s) and the source, provide \na link to the Creative Commons licence, and indicate if you modified the licensed material. Y ou do not have \npermission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence \nand your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to \nobtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p : / / c r e a t i v e c o m m o \nn s . o r g / l i c e n s e s / b y - n c - n d / 4 . 0 /     .  \n© The Author(s) 2024 \nScientific Reports |        (2024) 14:31672 9| https://doi.org/10.1038/s41598-024-81370-6\nwww.nature.com/scientificreports/",
  "topic": "Publishing",
  "concepts": [
    {
      "name": "Publishing",
      "score": 0.5927932262420654
    },
    {
      "name": "Disk formatting",
      "score": 0.49546945095062256
    },
    {
      "name": "Publication",
      "score": 0.4477003514766693
    },
    {
      "name": "Medical education",
      "score": 0.3939085602760315
    },
    {
      "name": "Medicine",
      "score": 0.3581943213939667
    },
    {
      "name": "Computer science",
      "score": 0.3130543828010559
    },
    {
      "name": "Political science",
      "score": 0.15608426928520203
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I76414455",
      "name": "Kasturba Medical College, Manipal",
      "country": "IN"
    },
    {
      "id": "https://openalex.org/I164861460",
      "name": "Manipal Academy of Higher Education",
      "country": "IN"
    },
    {
      "id": "https://openalex.org/I40120149",
      "name": "University of Oxford",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I29617571",
      "name": "University of Indonesia",
      "country": "ID"
    },
    {
      "id": "https://openalex.org/I4210086620",
      "name": "Dr. Hasan Sadikin General Hospital",
      "country": "ID"
    },
    {
      "id": "https://openalex.org/I91819753",
      "name": "Padjadjaran University",
      "country": "ID"
    },
    {
      "id": "https://openalex.org/I169248724",
      "name": "Royal Free London NHS Foundation Trust",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I205173300",
      "name": "Allama Iqbal Medical College",
      "country": "PK"
    },
    {
      "id": "https://openalex.org/I1312300495",
      "name": "Jinnah Postgraduate Medical Center",
      "country": "PK"
    },
    {
      "id": "https://openalex.org/I4210089004",
      "name": "Guthrie Robert Packer Hospital",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I20388574",
      "name": "SUNY Upstate Medical University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1290603121",
      "name": "World Health Organization - Pakistan",
      "country": "PK"
    },
    {
      "id": "https://openalex.org/I2800029501",
      "name": "Federal Neuro Psychiatric Hospital",
      "country": "NG"
    },
    {
      "id": "https://openalex.org/I4210145659",
      "name": "Mayo Hospital",
      "country": "PK"
    }
  ]
}