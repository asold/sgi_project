{
  "title": "AgglutiFiT: Efficient Low-Resource Agglutinative Language Model Fine-Tuning",
  "url": "https://openalex.org/W3048717973",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2104531756",
      "name": "Zhe Li",
      "affiliations": [
        "Xinjiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2104660571",
      "name": "Xiu-Hong Li",
      "affiliations": [
        "Xinjiang University"
      ]
    },
    {
      "id": "https://openalex.org/A3048641461",
      "name": "Jiabao Sheng",
      "affiliations": [
        "Xinjiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2329174702",
      "name": "Wushour Slamu",
      "affiliations": [
        "Xinjiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2104531756",
      "name": "Zhe Li",
      "affiliations": [
        "Xinjiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2104660571",
      "name": "Xiu-Hong Li",
      "affiliations": [
        "Xinjiang University"
      ]
    },
    {
      "id": "https://openalex.org/A3048641461",
      "name": "Jiabao Sheng",
      "affiliations": [
        "Xinjiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2329174702",
      "name": "Wushour Slamu",
      "affiliations": [
        "Xinjiang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2798838126",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W6745388339",
    "https://openalex.org/W2963326042",
    "https://openalex.org/W2892131163",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W6760456230",
    "https://openalex.org/W6767869522",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2973088264",
    "https://openalex.org/W6769263558",
    "https://openalex.org/W6767737316",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W2786149404",
    "https://openalex.org/W2970193165",
    "https://openalex.org/W2317786407",
    "https://openalex.org/W6734787708",
    "https://openalex.org/W6769851785",
    "https://openalex.org/W2980708516",
    "https://openalex.org/W2971296908",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6682132143",
    "https://openalex.org/W6774738212",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4298393544",
    "https://openalex.org/W2952190837",
    "https://openalex.org/W4299518610",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2982115308",
    "https://openalex.org/W2998230451",
    "https://openalex.org/W3006963874",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2922917409"
  ],
  "abstract": "Text classification tends to be difficult when data are inadequate considering the amount of manually labeled text corpora. For low-resource agglutinative languages including Uyghur, Kazakh, and Kyrgyz (UKK languages), in which words are manufactured via stems concatenated with several suffixes and stems are used as the representation of text content, this feature allows infinite derivatives vocabulary that leads to high uncertainty of writing forms and huge redundant features. There are major challenges of low-resource agglutinative text classification the lack of labeled data in a target domain and morphologic diversity of derivations in language structures. It is an effective solution which fine-tuning a pre-trained language model to provide meaningful and favorable-to-use feature extractors for downstream text classification tasks. To this end, we propose a low-resource agglutinative language model fine-tuning AgglutiFiT , specifically, we build a low-noise fine-tuning dataset by morphological analysis and stem extraction, then fine-tune the cross-lingual pre-training model on this dataset. Moreover, we propose an attention-based fine-tuning strategy that better selects relevant semantic and syntactic information from the pre-trained language model and uses those features on downstream text classification tasks. We evaluate our methods on nine Uyghur, Kazakh, and Kyrgyz classification datasets, where they have significantly better performance compared with several strong baselines.",
  "full_text": "Received June 20, 2020, accepted August 1, 2020, date of publication August 11, 2020, date of current version August 24, 2020.\nDigital Object Identifier 10.1 109/ACCESS.2020.3015854\nAgglutiFiT: Efficient Low-Resource Agglutinative\nLanguage Model Fine-Tuning\nZHE LI\n 1, XIUHONG LI2, JIABAO SHENG\n 1, AND WUSHOUR SLAMU3\n1Xinjiang Laboratory of Multi-Language Information Technology, Xinjiang Multilingual Information Technology Research Center, College of Software,\nXinjiang University, Urumqi 830046, China\n2College of Information Science and Engineering, Xinjiang University, Urumqi 830046, China\n3Xinjiang Laboratory of Multi-Language Information Technology, Xinjiang Multilingual Information Technology Research Center, College of Information\nScience and Engineering, Xinjiang University, Urumqi 830046, China\nCorresponding author: Xiuhong Li (xjulxh@xju.edu.cn)\nThis work was supported in part by the Xinjiang University Ph.D. Foundation Initiated Project under Grant 620312343, in part by the\nXinjiang Uygur Autonomous Region Graduate Research and Innovation Project Grant, in part by the Dark Web Intelligence Analysis and\nUser Identiﬁcation Technology under Grant 2017YFC0820702-3, in part by the National Language Commission Research Project under\nGrant ZDI135-96, and in part by the National Engineering Laboratory for Public Safety Risk Perception and Control by Big Data (PSRPC).\nABSTRACT Text classiﬁcation tends to be difﬁcult when data are inadequate considering the amount of\nmanually labeled text corpora. For low-resource agglutinative languages including Uyghur, Kazakh, and\nKyrgyz (UKK languages), in which words are manufactured via stems concatenated with several sufﬁxes\nand stems are used as the representation of text content, this feature allows inﬁnite derivatives vocabulary that\nleads to high uncertainty of writing forms and huge redundant features. There are major challenges of low-\nresource agglutinative text classiﬁcation the lack of labeled data in a target domain and morphologic diversity\nof derivations in language structures. It is an effective solution which ﬁne-tuning a pre-trained language\nmodel to provide meaningful and favorable-to-use feature extractors for downstream text classiﬁcation tasks.\nTo this end, we propose a low-resource agglutinative language model ﬁne-tuning AgglutiFiT, speciﬁcally,\nwe build a low-noise ﬁne-tuning dataset by morphological analysis and stem extraction, then ﬁne-tune\nthe cross-lingual pre-training model on this dataset. Moreover, we propose an attention-based ﬁne-tuning\nstrategy that better selects relevant semantic and syntactic information from the pre-trained language model\nand uses those features on downstream text classiﬁcation tasks. We evaluate our methods on nine Uyghur,\nKazakh, and Kyrgyz classiﬁcation datasets, where they have signiﬁcantly better performance compared with\nseveral strong baselines.\nINDEX TERMS Transfer learning, pre-training, low-resources text classiﬁcation, ﬁne-tuning.\nI. INTRODUCTION\nText classiﬁcation is the backbone of most natural lan-\nguage processing tasks such as sentiment analysis, classi-\nﬁcation of news topics, and intent recognition. Although\ndeep learning models have reached the most advanced level\non many Natural Language Processing(NLP) tasks, these\nmodels are trained from scratch, which makes them require\nlarger datasets. Still, many low-resource languages lack rich\nannotated resources that support various tasks in text clas-\nsiﬁcation. For UKK languages, as show in Table-1, words\nare derived from stem afﬁxes, so there is a huge vocabulary.\nStems represent of text content and afﬁxes provide seman-\ntic and grammatical functions. Diversity of morphological\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Yucong Duan\n.\nstructure leads to transcribe speech as they pronounce\nwhile writing and suffer from high uncertainty of writing\nforms on these languages which causes the personalized\nspelling of words especially less frequent words and terms\nAblimit et al. [2]. Data collected from the Internet are\nnoisy and uncertain in terms of coding and spelling\nAblimit et al. [1]. The main problems in NLP tasks for UKK\nlanguages are uncertainty in terms of spelling and coding\nand annotated datasets inadequate poses a big challenge for\nclassifying short and noisy text data.\nData augmentation can effectively solve the problem of\ninsufﬁcient marker corpus in low-resource language datasets.\nŞahin and Steedman [17] present two simple text aug-\nmentation techniques using ‘‘crops’’ sentences by removing\ndependency links, and ‘‘rotates’’ sentences by moving the\ntree fragments around the root. However, this may not be\nVOLUME 8, 2020 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 148489\nZ. Liet al.: AgglutiFiT: Efficient Low-Resource Agglutinative Language Model Fine-Tuning\nTABLE 1. Examples of Uyghur word variants.\nFIGURE 1. High-level illustration of AgglutiFiT.\nsufﬁcient for several other tasks such as cross-language text\nclassiﬁcation due to irregularities across UKK languages in\nthese kinds of scenarios. Pre-trained language models such\nas BERT Devlin et al. [7] or XLM Devlin et al. [5] have\nbecome an effective way in NLP and yields state-of-the-art\nresults on many downstream tasks. These models require only\nunmarked data for training, so they are especially useful when\nthere is very little market data. Fully exploring ﬁne-tuning\ncan go a long way toward solving this problem Xu et al. [25].\nSun et al. [21] conduct an empirical study on ﬁne-tuning,\nalthough these methods achieve better performance, they did\nnot perform well on UKK low-resource agglutinative lan-\nguages due to the morphologic diversity of derivations.\nThe signiﬁcant challenge of using language model ﬁne-\ntuning on low-resource agglutinative languages is how to cap-\nture feature information. To apprehend rich semantic patterns\nfrom plain text, Zhang et al. [27] incorporating knowledge\ngraphs (KGs), which provide rich structured knowledge facts\nfor better language understanding. Zhang et al. [28] propose\nto incorporate explicit contextual semantics from pre-trained\nsemantic role labeling (SemBERT) which can provide rich\nsemantics for language representation to promote natural\nlanguage understanding. UKK languages are a kind of mor-\nphologically rich agglutinative languages, in which words\nare formed by a root (stem) followed by sufﬁxes. These\nmethods are difﬁcult to capture the semantic information of\nUKK languages. As the stems are the notionally independent\nword particles with a practical meaning, and afﬁxes pro-\nvide grammatical functions in UKK languages, morpheme\nsegmentation can enable us to separate stems and remove\nsyntactic sufﬁxes as stop words, and reduce noise and capture\nrich feature in UKK languages texts in the classiﬁcation task.\nIn this paper, as depict in Figure-1, we propose a low-\nresource agglutinative language model ﬁne-tuning model:\nAgglutiFiT that is capable of addressing these issues. First,\nwe use XLM −R to pre-train a language model on a\nlarge cross-lingual corpus. Then we build a ﬁne-tuning\ndataset by stem extraction and morphological analysis as\nthe target task dataset to ﬁne-tune the cross-lingual pre-\ntraining model. Moreover, we introduce an attention-based\nﬁne-tuning strategy that selects relevant semantic and syn-\ntactic information from the pre-trained language model and\n148490 VOLUME 8, 2020\nZ. Liet al.: AgglutiFiT: Efficient Low-Resource Agglutinative Language Model Fine-Tuning\nuses discriminative ﬁne-tuning to capture different types\nof information on different layers. To evaluate our model,\nwe collect and annotate nine corpora for text classiﬁcation\nof UKK low-resource agglutinative language, including topic\nclassiﬁcation, sentiment analysis, intention classiﬁcation.\nThe experimental results show AgglutiFiT can signiﬁcantly\nimprove the performance with a small number of labeled\nexamples.\nThe contributions of this paper are summarized as follows:\n• We construct three low-resource agglutinative languages\nincluding Uyghur, Kazakh, and Kyrgyz nine datasets,\neach of languages datasets contains topic classiﬁcation,\nsentiment analysis, and intention classiﬁcation three\ncommon text classiﬁcation tasks.\n• We propose a ﬁne-tuning strategy on low-resource\nagglutinative language that builds a low-noise ﬁne-\ntuning dataset by stem extraction and morphologi-\ncal analysis to ﬁne-tune the cross-lingual pre-training\nmodel.\n• We propose an attention-based ﬁne-tuning method that\nbetter select relevant semantic and syntactic information\nfrom the pre-trained language model and uses discrim-\ninative ﬁne-tuning to capture across different types of\ninformation different layers.\nII. RELATED WORK\nIn the ﬁeld of NLP, low-resource text processing tasks receive\nincreasing attention such as Hangya et al. [10] utilizes a\ndelightfully simple method for domain adaptation of bilin-\ngual word embeddings overcoming data sparsity in the target\nlanguage. And Zhang et al. [28] proposes Semantics-aware\nBERT (SemBERT), which is capable of explicitly absorb-\ning contextual semantics over a BERT backbone and it\nobtains substantially improves results on typical NLP tasks.\nWe brieﬂy review three related directions: data augmentation,\nlanguage model pre-training, and ﬁne-tuning.\nA. DATA AUGMENTATION\nData Augmentation is that addresses the challenge of insuf-\nﬁcient data by creating composite examples that are gen-\nerated from but not identical to the original document.\nWei and Zou [24] present EDA, easy data augmentation\ntechniques to improve the performance of text classiﬁcation\ntask. For a given sentence in the training set, EDA randomly\nchooses and performs one of the following operations: syn-\nonym replacement, random insertion, random swap, random\ndeletion. UKK languages has few synonyms for a certain\nword, so the substitution of synonyms cannot add much data.\nIts words are formed by a root (stem) followed by sufﬁxes,\nand as the powerful sufﬁxes can reﬂect semantically and syn-\ntactically, random insertion, random swap, random deletion\nmay change the meaning of a sentence and cause the original\ntags to become invalid. In the text classiﬁcation, training doc-\numents are translated into another language by using an exter-\nnal system and then converted back to the original language\nto generate composite training examples, this technology\nknown as backtranslation. Shleifer [19] work experiments\nwith backtranslation as data augmentation strategies for text\nclassiﬁcation. The translation service quality of Uyghur is not\ngood, and Kazakh and Kyrgyz do not have mature and robust\ntranslation service, so it is difﬁcult to use the three languages\nin backtranslation. Şahin and Steedman [17] proposes an\neasily adaptable, multilingual text augmentation technique\nbased on dependency trees. It augments the training sets\nof these low-resource languages which are known to have\nextensive morphological case-marking systems and relatively\nfree word order including Uralic, Turkic, Slavic, and Baltic\nlanguage families.\nB. CROSS-LINGUAL PRE-TRAINED LANGUAGE MODEL\nRecently, Pre-training language models such as BERT\nDevlin et al. [8] and GPT-2 Radford et al. [15] have\nachieved enormous success in various tasks of natural lan-\nguage processing such as text classiﬁcation, machine transla-\ntion, question answering, summarization, etc. The early work\nin the ﬁeld of cross-language understanding has proven the\neffectiveness of cross-language pre-trained models on cross-\nlanguage understanding. The multilingual BERT model is\npre-trained on Wikipedia in 104 languages using a shared\nvocabulary of word blocks. LASER Artetxe and Schwenk [3]\nis trained on parallel data of 93 languages and those languages\nshare BPE vocabulary. Conneau and Lample [5] also use\nparallel data to pre-train BERT . These models can achieve\nzero distance migration, but the effect is poor compared with\nthe monolingual model. The XLM −R Conneau et al. [6] uses\nﬁltered common-crawled data over 2TB to demonstrate that\nusing a large-scale multilingual pre-training model can signif-\nicantly improve the performance of cross-language migration\ntasks.\nC. FINE-TUNING\nWhen we adapt the pre-training model to NLP tasks in\na target domain, a proper ﬁne-tuning strategy is desired.\nHoward and Ruder [11] proposes the universal language\nmodel ﬁne-tuning (ULMFiT ) with several novel ﬁne-tuning\ntechniques. ULMFiT consists of three steps, namely general-\ndomain LM pre-training, target task LM ﬁne-tuning, and\ntarget task classiﬁer ﬁne-tuning. Eisenschlos et al. [9] com-\nbines the ULMFiT with the quasi-recurrent neural net-\nwork (QRNN ) Bradbury et al. [4] and subword tokenization\nKudo [12] to propose multi-lingual language model ﬁne-\ntuning (MultiFit) to enable practitioners to train and ﬁne-tune\nlanguage models efﬁciently. The MultiFiT language model\nconsists of one subword embedding layer, four QRNN lay-\ners, one aggregation layer, and two linear layers. Moreover,\na bootstrapping method Ruder and Plank [16] is applied to\nreduce the complexity of training. Although those approaches\nare general enough and have achieved state-of-the-art results\non various classiﬁcation datasets, the method is consid-\nered can not solve the problem of morphologic diversity of\nderivations in language structures on low-resource aggluti-\nnative language. Tao et al. [22] proposes an attention-based\nVOLUME 8, 2020 148491\nZ. Liet al.: AgglutiFiT: Efficient Low-Resource Agglutinative Language Model Fine-Tuning\nFIGURE 2. Cross-lingual language model pre-training. The MLM objective is similar to the one in BERT Devlinet al.[7], but with continuous streams of\ntext as opposed to sentence pairs.\nﬁne-tuning algorithm. With this algorithm, the customers can\nuse the given language model and ﬁne-tune the target model\nby their own data, but that does not capture different levels\nof syntactic and semantic information on different layers of\na neural network. In this paper, we use a new ﬁne-tuning\nstrategy that provides a feature extractor to extract features\nand use these features for downstream text classiﬁcation\ntasks.\nIII. METHODOLOGY\nIn this section, we will explain our methodology, which is\nalso shown in Figure-1. Our training consists of four stages.\nWe ﬁrst pre-train a language model on a large scale cross-\nlingual text corpus. Then the pre-trained model is ﬁne-tuned\nby the ﬁne-tuning dataset on unsupervised language model-\ning tasks. The ﬁne-tuning dataset is constructed by means of\nstem extraction and morpheme analysis on the downstream\nclassiﬁcation datasets. Moreover, we use an attention-based\nﬁne-tuning to build our classiﬁcation model and uses discrim-\ninative ﬁne-tuning to capture different types of information\non different layers. Finally, train the classiﬁer using target\ntask datasets.\nA. CROSS-LINGUAL MODEL PRE-TRAINING\nGiven a text sequence X =(x1,x2,..., xT ) and a sequence\nY =(y1,y2,..., yT ′) to denote the sequence of input context\nand target response respectively. The conditional probability\np(xt |x0:t−1) can be modeled by a probability distribution over\nthe vocabulary given linguistic context x0:t−1. The context\nx0:t−1 is modeled by neural encoder fenc(·), and the condi-\ntional probability:\np(xt |x0:t−1) =gLM\n(\nfenc(x0:t−1)\n)\n(1)\nwhere gLM (·) is prediction layer.\nGiven a huge cross-lingual corpus, we can train the\nentire network with maximum likelihood estimation (MLE).\nWe have concatenated X and Y , then we can obtain prediction\nloss over the whole target response sequence as the loss\nfunction, the loss term for predicting the dialogue context X.\nThe loss function:\nLLM =−\nT∑\nt=1\nlog p(xt |x <t) (2)\nIn this paper, we aim to utilize XLM −R to model the\nconditional probability. XLM −R uses the same shared vocab-\nulary to process all languages through Byte Pair Encoding\n(BPE) Sennrich et al. [18]. As shown in Lample et al. [14],\nthis method greatly improves the alignment of embedding\nspaces across languages that share either the same alphabet or\nanchor tokens such as digits or proper nouns Smith et al. [20].\nWe learn the BPE splits on the concatenation of sentences\nsampled randomly from the monolingual corpora. Sentences\nare sampled according to a multinomial distribution with\nprobabilities. And sentences are sampled according to a prob-\nable multinomial distribution {qi}i=1,2,3...n, where:\nqi = pα\ni\n∑N\nj=1 pα\nj\n, (3)\nwhere pi = ni∑N\nk=1 nk\nand α =0.3. This distributed sampling\nmethod increases the number of tokens associated with low-\nresource languages and alleviates the bias to high-resource\nlanguages. In particular, this method prevents words in low-\nresource languages from being split at the character level.\nAs shown in Figure-2, XLM −R utilizes a transformer\nmodel Vaswani et al. [23] to train with the multilingual MLM\nobjective Devlin et al. ,Lample et al. [7], [14] using only\nmonolingual data. XLM −R samples streams of text from\neach language, is trained to predict the masked tokens in the\ninput and apply subword tokenization directly on raw text\ndata using sentence piece Kudo and Richardson [13] with\na unigram language model Kudo [12]. Masking multi-head\nself-attention is utilized as the core technical operation to\nconduct representation learning:\nAttention(Q,K,V ) =Softmax\n(QKT\n√dk\n)\nV (4)\nTo extend the ability of the model to focus on different loca-\ntions and to increase the representation learning capacity of\nsubspaces for attention units, Transformer adopts the ‘‘multi-\nhead’’ mode that can be expressed as:\nMultiHead(Q,K,V ) =Concat(head1,..., headh)W O (5)\nheadi =Attention(QW Q\ni ,KW K\ni ,VW K\ni ) (6)\nB. LM FINE-TUNING BASED ON UKK CHARACTERISTICS\nWhen we apply the pre-training model to text classiﬁcation\ntasks in a target domain, a proper ﬁne-tuning strategy is\n148492 VOLUME 8, 2020\nZ. Liet al.: AgglutiFiT: Efficient Low-Resource Agglutinative Language Model Fine-Tuning\nFIGURE 3. Morpheme segmentation flow chart.\ndesired. In this paper, we employ three ﬁne-tuning methods\nas below.\n1) FINE-TUNING DATASETS BASED ON\nMORPHEMIC ANALYSIS\nUKK languages are agglutinative languages, meaning that\nwords are formed by a stem augmented by an unlimited\nnumber of sufﬁxes. The stem is an independent semantic\nunit while the sufﬁxes are auxiliary functional units. Both\nstems and sufﬁxes are called morphemes. Morphemes are the\nsmallest functional units in agglutinative languages. Because\nof this agglutinative nature, the number of words of these\nlanguages can be almost inﬁnite, and most of the words\nappear very rarely in the text corpus. Modeling based on a\nsmaller unit like morpheme can provide stronger statistics\nhence robust models. The total number of sufﬁxes in each of\nUKK languages is around 120. New sufﬁxes may be created,\nbut this is the typical case.\nAs shown in Figure-3, we use a semi-supervised mor-\npheme segmenter based on the sufﬁx set Ablimit et al. [2].\nFor a candidate word, this tool designs an iterative searching\nalgorithm to produce all possible segmentation results by\nmatching the stem-set and the sufﬁx set. The phonemes on\nthe boundaries change their surface forms according to the\nphonetic harmony rules when the morphemes are merged into\na word. Morphemes will harmonize each other, and appeal to\nthe pronunciation of each other. When the pronunciation is\nprecisely represented, the phonetic harmony can be clearly\nobserved in the text. An independent statistical model can be\nadopted to pick the best result from N-best results in the UKK\ntext classiﬁcation task.\nWe adopt this tool to train a statistical model using word-\nmorpheme parallel training corpus, extraction, and greatly\nimprove the UKK text classiﬁcation task. which include\n10,000 Uyghur sentences, 5000 Kazakhh sentences, and\n5000 Kyrgyz sentences. We select 75% of them as the training\ncorpus. The remainder is used as the testing corpus to execute\nmorpheme segmentation and stem extraction experiments.\nWe can collect necessary terms compose a less noise ﬁne-\ntuning datasets by extracting stems in the UKK languages\nclassiﬁcation task. Then ﬁne-tuning with XLM −R on the\nﬁne-tuning datasets for better performance. As the examples\ngiven in Table-1 are shown below after morpheme analysis\nThe above sentence morphemes are segmented into the\nfollowing\nThere are 6 words in the above sentence divide into two\ngroups, in this way, a stem can grasp the features of other\nwords, and the feature will be greatly reduced.\n2) DISCRIMINATIVE FINE-TUNING\nDifferent layers of a neural network can capture different\nlevels of syntactic and semantic information Howard and\nRuder [11], Yosinski et al. [26]. Naturally, the lower layers of\nthe XLM −R model may contain more general information.\nTherefore, we can ﬁne-tune them with assorted learning rates.\nFollowing Howard and Ruder [11], we use the discrimina-\ntive ﬁne-tuning method. We separate the parameters θ into\n{θ1,...,θ L }, where θl contains the parameters of the l-th\nlayer. Then the parameters are updated as follows:\nθl\nt =θl\nt−1 −ηl ·∇θl J(θ), (7)\nwhere ηl represents the learning rate of the l −th layer and\nt denotes the update step. Following Sun et al. [21], we set\nthe base learning rate to ηL and use ηk−1 =ξ ·ηk , where\nξ is a decay factor and less than or equal to 1. When ξ <1,\nthe lower layer has a slower learning rate than the higher layer.\nVOLUME 8, 2020 148493\nZ. Liet al.: AgglutiFiT: Efficient Low-Resource Agglutinative Language Model Fine-Tuning\nTABLE 2. Statistics of the topic classification dataset.\nTABLE 3. Statistics of the sentiment analysis datasets.\nWhen ξ =1, all layers have the same learning rate, which is\nequivalent to the regular stochastic gradient descent (SGD).\n3) ATTENTION-BASED FINE-TUNING\nFor classiﬁcation tasks, we utilize an attention-based\nencoder-decoder architecture. Encoder learns the contextu-\nalized features from inputs of the dataset. Then the hidden\nstates over time steps denoted as H =h1,h2,..., hT , can\nbe seen as the representation of the classiﬁed data, which are\nalso the input of the attention layer. We use the self-attention\nto extract the relevant aspects from the input states since we\ndo not have any additional information from the decoder. The\nalignment is computed as\nut =tanh(Wuht +bu) (8)\nfor t =1,2,..., T , where Wu and bu are the weight matrix\nand bias term to be learned. Then the alignment scores are\ngiven by the following Softmax function:\nαt = exp(Wαut )\n∑T\ni=1 exp(Wαut )\n(9)\nThe ﬁnal context vector, which is also the input of the classi-\nﬁer, is computed by\nc =\nT∑\ni=1\nαt ut (10)\nC. TEXT CLASSIFIER\nFor the classiﬁer, we add two linear blocks with batch nor-\nmalization and dropout, and ReLU activations for the inter-\nmediate layer and a Softmax activation for the output layer\nthat calculates a probability distribution over target classes.\nConsider the output of the last linear block is So. Further,\ndenote by C =c1,c2,..., cM =XxY the target classiﬁcation\ndata, where ci =(xi,yi), xi is the input sequence of tokens and\nyi is the corresponding label. The classiﬁcation loss we use to\ntrain the model can be computed by:\nL2(C) =\n∑\n(x,y)∈C\nlog p(y|x) (11)\nwhere\np(y|x) =p(y|x1,x2,..., xm) :=softmax(Wso ) (12)\nIV. DATASETS\nA. DATA COLLECTION\nWe construct three low-resource agglutinative languages\nincluding Uyghur, Kazakh, and Kyrgyz nine datasets,\ndatasets cover common text classiﬁcation tasks: topic clas-\nsiﬁcation, sentiment analysis, and intention classiﬁcation.\nWe use the web crawler technology to collect our text data,\nand download from the Uyghur, Kazakh and Kyrgyz’s ofﬁcial\nwebsites as well as other main websites. 1\nB. CORPUS STATISTICS\nIn this section, we introduce the detailed information of\nthe corpus. We divided them into morpheme sequences and\nused morpheme segmentation tools to extract word stems.\nThe method of subword extraction based on stem afﬁx has\nachieved a good performance on the reduction of feature\nspace. As a result, the vocabulary of morpheme is greatly\nreduced to about 30%, as shown in Table 2, Table 3 and\nTable 4. In addition, when the types and numbers of corpora\nincrease, the accumulation of morphemes is only one-third of\nthe accumulation of words.\n1) TOPIC CLASSIFICATION\nThe corpus for the Uyghur language cover 9 topics: law,\nﬁnance, sports, culture, health, tourism, education, science,\nand entertainment. Each category has 1,200 texts, resulting in\na total of 10,800 texts. We name this corpus as ug-topic.\nThe corpus for the Kazakh language cover 8 topics: law,\nﬁnance, sports, culture, tourism, education, science, and\nentertainment. Each of them contains 1,200 texts, so there are\n9,600 texts totally. We name this corpus as kz-topic. The\ncorpus for the Kyrgyz language cover 7 topics: law, ﬁnance,\nsports, culture, tourism, education. Each category contains\n1,200 texts (totally 8,400 texts). We name this corpus as\nky-topics. The details are shown in Table-2.\n2) SENTIMENT ANALYSIS\nWe construct 3 sentiment analysis datasets for three-category\nclassiﬁcation, namely positive, negative, and neutral. Each\nlanguage is related to 900 texts and each category contains\n1www.uyghur.people.com.cn, uy.ts.cn, Kazakhh.ts.cn,\nwww.hawar.cn, Sina Weibo, Baidu Tieba and WeChat.\n148494 VOLUME 8, 2020\nZ. Liet al.: AgglutiFiT: Efficient Low-Resource Agglutinative Language Model Fine-Tuning\nTABLE 4. Statistics of the intention classification datasets.\nTABLE 5. Example from the UKK datasets.\n300 texts. We name these datasets as ug-sen, kz-sen and\nky-sen as shown in Table-3.\n3) INTENTION CLASSIFICATION\nWe construct 3 datasets of ﬁve-class user intent identiﬁcation:\nnews, life, travel, entertainment, and sports. Each language\ncontains 200 texts. We name these datasets as ug-intent,\nkz-intent and ky-intent as shown in Table-4.\nC. CORPUS EXAMPLES\nIn this section, we present some examples of various lan-\nguage categorization tasks. Different from Kazakhstan and\nKyrgyzstan, in China, the Kazakh language used by the\nKazakh people and the Kyrgyz language borrowed from the\nArabic alphabet. The red keywords indicate the words that\nhave the same meaning. The blue keywords represent their\nmeaning in English. As shown in Table-5 for details.\nVOLUME 8, 2020 148495\nZ. Liet al.: AgglutiFiT: Efficient Low-Resource Agglutinative Language Model Fine-Tuning\nV. EXPERIMENT\nA. DATASETS AND TASKS\nWe evaluate our method on nine agglutinative language\ndatasets which we construct of three common text classiﬁca-\ntion tasks: topic classiﬁcation, sentiment analysis, and inten-\ntion classiﬁcation. We use 75% of the data as the training set,\n10% as the validation set, and 15% as the test set. For cross-\nlingual pre-training language models, we use the XLM −R\nmodel loaded from the torch.Hub that It is trained on 2.5TB\nof CommonCrawl data, in 17 languages and uses a large\nvocabulary size of 95K. XLM −R shows the possibility of\ntraining one language model for many languages while not\nsacriﬁcing per-language performance.\nB. BASELINES\nWe compare our method with the cross-lingual classiﬁca-\ntion model ULMFiT Howard and Ruder [11], which intro-\nduces key techniques for ﬁne-tuning language models, and\nSemBERT Zhang et al. [28], which is capable of explic-\nitly absorbing contextual semantics over a BERT backbone.\nMoreover, we compare against the cross-lingual embedding\nmodel, namely LASER Artetxe and Schwenk [3], which\nuses a large parallel corpus. We also compare against BWEs\nHangya et al. [10], a cross-lingual domain adaptation method\nfor classiﬁcation text.\nC. HYPERPARAMETERS\nIn our experiment, we use the XLM −RBase model, which uses\na BERTBase architecture Vaswaniet al. [23] with a hidden size\nof 768, 12 Transformer blocks and 12 self-attention heads.\nWe ﬁne-tune the XLM −RBase model on 4 Tesla K80 GPUs\nand set the batch size to 24 to ensure that the GPU mem-\nory is fully utilized. The dropout probability is always 0.1.\nWe use Adam with β1 =0.9 and β2 =0.999. Following\nSun et al. [21], we use the discriminative ﬁne-tuning method\nHoward and Ruder [11], where the base learning rate is 2e −5,\nand the warm-up proportion is 0.1. We empirically set the\nmax number of the epoch to 20 and save the best model on\nthe validation set for testing.\nD. RESULTS AND ANALYSIS\nIn this section, we demonstrate the effectiveness of our\nlow-resource agglutinative language ﬁne-tuning model. Our\napproach signiﬁcantly outperforms the previous work on\ncross-lingual classiﬁcation. Separately, the best results in the\nmetric are bold, respectively.\nAs given in Table-6, Table-7, and Table-8, we show results\nfor topic classiﬁcation, sentiment analysis, and intention clas-\nsiﬁcation. Our AgglutiFiT outperform their cross-lingual and\ndomain adaptation method. Pre-training is most beneﬁcial for\ntasks with low-resource datasets and enables generalization\neven with 100 labeled examples when ﬁne-tuning with ﬁne-\ntuning dataset, our approach has a greater performance boost.\nCompared with ULMFiT, we perform better on all three\ntasks, although ULMFiT introduces techniques that are key\nTABLE 6. Results on topic classification accuracy.\nTABLE 7. Results on sentiment analysis accuracy.\nTABLE 8. Results on intention classification accuracy.\nfor ﬁne-tuning a language model including discriminative\nﬁne-tuning and target task classiﬁer ﬁne-tuning. The reason\ncan be partly explained as we adopt a less noisy datasets in\nthe ﬁne-tuning phase and attention-based ﬁne-tuning which\nmakes it possible to obtain a closer distribution of data in the\ngeneral domain to the target domain. LASER obtains strong\nresults in multilingual similarity search for low-resource lan-\nguages, but we work better than LASER contribute to we use\nattention-based ﬁne-tuning and different learning rates at a\ndifferent layer, which allows us to capture more syntactic\nand semantic information at each layer, moreover, LASER has\nno learn joint multilingual sentence representations for UKK\nlanguages. Experimental results on methods SemBERT are\nlower than AgglutiFiT on account of lacking the necessary\nsemantic role labels to embedding in the parallel, which\nleads to does not capture more accurate semantic information.\nBWEs is signiﬁcantly lower than other models, we conjecture\nthat the source language of method BWEs is English, which is\nquite different from the UKK languages in data distribution,\nmore importantly, the datasets of UKK languages are too\ninadequacy to create good BWEs. Our three task experiments\nalso show that using more high-quality datasets to ﬁne-tune\nthe results would be better.\n1) LOW-RESOURCE AGGLUTINATIVE LANGUAGE MODEL\nFor low-resource agglutinative language, we can use lan-\nguages with more resources with similar data, especially\nif their vocabularies are largely the same. The pre-training\ndataset CommonCrawl contains Uygur, Kazak, and Kyrgyz,\nand these languages are derived from Arabic which has a\nlarge amount of data, and many of these languages are the\nsame in the Shared BPE dictionary. To do so, we train a UKK\n148496 VOLUME 8, 2020\nZ. Liet al.: AgglutiFiT: Efficient Low-Resource Agglutinative Language Model Fine-Tuning\nFIGURE 4. Results on language modeling.\nlanguage model on Wikipedia, together with additional data\nfrom either Arabic. The gains in perplexity from cross-lingual\nlanguage modeling due to the n-grams anchor points that\nare shared across languages. Therefore, the cross-lingual lan-\nguage model can transfer the additional context provided by\nthe Arabic monolingual corpora through these anchor points\nto improve the UKK language model. Figure-4 shows the\nperplexity of the UKK language model.\nE. ABLATION STUDY\nTo evaluate the contributions of key factors in our method,\nwe perform an ablation study as shown in Figure-5. We run\nexperiments on nine corpora that are representative of differ-\nent tasks, genres, and sizes.\nFIGURE 5. Explore the influence of important factors on accuracy.\n1) THE EFFECT OF MORPHEMIC ANALYSIS\nIn order to gauge the impact of ﬁne-tuning datasets quality,\nwe compare the ﬁne-tuning on the constructed ﬁne-tuning\ndatasets with the target task datasets without stem-word\nextraction. The experimental results show that the perfor-\nmance of all tasks is greatly improved by using our ﬁne-\ntuning datasets. Stem is a practical unit of vocabulary. Stem\nextraction enables us to capture effective and meaningful\nfeatures and greatly reduce the repetition rate of features.\n2) THE EFFECT OF ATTENTION-BASED FINE-TUNING\nAs given in Figure-5, we can observe that by adding\nan attention ﬁne-tuning, our model advances accuracies.\nAttention-based ﬁne-tuning relies on a semantic between\nwords that would inﬂuence the overall model performance.\nIn order to see the effectiveness of the attention-based ﬁne-\ntuning more clearly, we visualize the attention scores with\nrespect to the input texts on Uyghur. The randomly chosen\nexamples of visualization with respect to different classes are\ngiven in Figure-6, where darker color means higher attention\nscores.\n3) THE EFFECT OF DISCRIMINATIVE FINE-TUNING\nWe compare with and without discriminative ﬁne-tuning on\nthe model. Discriminative ﬁne-tuning improve performance\nacross all three tasks, however, the role of improvement is\nlimited, we still need a better optimization method to explore\nhow discriminative ﬁne-tuning can be better applied in the\nmodel.\n4) FINE-TUNING DATASETS SIZE BEHAVIOR\nFrom Figure-5, we also observe the larger the size of ﬁne-\ntuning datasets, the higher the classiﬁcation accuracy tends\nto be, which is more obvious in the comparison between\ndifferent classiﬁcation tasks. The reason is that the larger the\nﬁne-tuning datasets, the more likely it is to get the same data\ndistribution as the target task.\n5) IMPACT OF LM QUALITY\nRegarding the language model pre-training, we have tried\nfour different ways: 1) XLM-R on nine low-resource agglu-\ntinative datasets of three languages; 2) XLM-R model of\nthe 15 languages. 3) XLM-R model of the 17 languages.\nand 4) XLM-R model of the 100 languages. As the results\nin Figure-7, we can have the following observations.\n• When we have a large enough pre-trained dataset,\nthe size of source data is not vital. This observation\nindicates the possibility that when the source dataset is\nlarge enough, the performance of language modeling is\na signiﬁcant factor in transfer learning.\n• Pre-training on larger source datasets does not always\nimprove downstream task performance. XLM-R with\n17 languages is a subset of XLM-R with 100 languages\nand it is much smaller than XLM-R with 100 languages.\nBut pre-training on XLM-R with 17 languages leads\nto the best performance among the three. We speculate\nthat the UKK languages borrow the Arabic alphabet and\nthat the XLM-R of 17 languages contains Arabic and\nhas a higher weight. The language model train by it has\nmore similar data distribution to the target task datasets.\nThe vast majority of XLM-R with 100 languages is\nunnecessary and can affect its data distribution.\nF. DISCUSSION AND FUTURE DIRECTIONS\nWhen tasks with scarce amounts of labeled data and pro-\nvide limited semantics for language representation, language\nmodel ﬁne-tuning will be particularly beneﬁcial. Transfer\nlearning and especially ﬁne-tuning for low-resource language\nVOLUME 8, 2020 148497\nZ. Liet al.: AgglutiFiT: Efficient Low-Resource Agglutinative Language Model Fine-Tuning\nFIGURE 6. Examples of attention visualization on Uyghur with respect to different classes.\nFIGURE 7. Explore the influence of important factors on accuracy.\nis to be explored, many future directions are possible.\nImprove language model pre-training and ﬁne-tuning and\nmake them more robust is a potential direction. Language\nmodeling can also be augmented with further pre-train with\ntarget domain data, ﬁne-Tuning Strategies, and multi-task\nﬁne-tuning. Another direction is that explicit contextual\nsemantics can be effectively integrated with state-of-the-art\npre-trained language representation for even better perfor-\nmance improvement. More studies are required to better\nunderstand what knowledge a pre-trained language model\ncaptures, how these changes during the ﬁne-tuning stage, and\nwhat features different tasks require.\nVI. CONCLUSION\nWe propose AgglutiFiT, an effective language model ﬁne-\ntuning method that can be applied to a low-resource agglu-\ntinative language classiﬁcation tasks. This novel ﬁne-tuning\ntechnique that via stem extraction and morphological analysis\nbuilds a low-noise ﬁne-tuning dataset as the target task dataset\nto ﬁne-tune the cross-lingual pre-training model. Moreover,\nwe propose an attention-based ﬁne-tuning strategy that bet-\nter selects relevant semantic and syntactic information from\nthe pre-trained language model to provide meaningful and\nfavorable-to-use feature for downstream text classiﬁcation\ntasks. We also use discriminative ﬁne-tuning to capture dif-\nferent types of information on different layers. Our method\nsigniﬁcantly outperformed existing strong baselines on nine\nlow-resource agglutinative language datasets of three repre-\nsentative low-resource agglutinative text classiﬁcation tasks.\nWe hope that our results will catalyze new developments in\nlow-resource agglutinative languages task for NLP.\nACKNOWLEDGMENT\n(Zhe Li and Xiuhong Li contributed equally to this work.)\nWe thank the anonymous reviewers for their valuable feed-\nback. This article was presented in part at The Nineteenth\nChina National Conference on Computational Linguistics,\nCCL 2020. They initial conference paper does not address the\nproblem of saturation. This manuscript addresses this issue\nand provides additional analysis of the trade-off between\nusing more data.\nREFERENCES\n[1] M. Ablimit, T. Kawahara, A. Pattar, and A. Hamdulla, ‘‘Stem-afﬁx based\nUyghur morphological analyzer,’’ Int. J. Future Gener. Commun. Netw.,\nvol. 9, no. 2, pp. 59–72, Feb. 2016.\n[2] M. Ablimit, S. Parhat, A. Hamdulla, and T. F. Zheng, ‘‘A multilin-\ngual language processing tool for Uyghur, Kazak and Kirghiz,’’ in Proc.\nAsia–Paciﬁc Signal Inf. Process. Assoc. Annu. Summit Conf. (APSIPA\nASC), Dec. 2017, pp. 737–740.\n148498 VOLUME 8, 2020\nZ. Liet al.: AgglutiFiT: Efficient Low-Resource Agglutinative Language Model Fine-Tuning\n[3] M. Artetxe and H. Schwenk, ‘‘Massively multilingual sentence embed-\ndings for zero-shot cross-lingual transfer and beyond,’’ Trans. Assoc. Com-\nput. Linguistics, vol. 7, pp. 597–610, Mar. 2019.\n[4] J. Bradbury, S. J. Merity, C. Xiong, and R. Socher, ‘‘Quasi-recurrent neural\nnetwork,’’ U.S. Patent 15 420 710, May 10, 2018.\n[5] A. Conneau and G. Lample, ‘‘Cross-lingual language model pretraining,’’\nin Proc. Adv. Neural Inf. Process. Syst., 2019, pp. 7057–7067.\n[6] A. Conneau, K. Khandelwal, N. Goyal, V . Chaudhary, G. Wenzek,\nF. Guzmán, E. Grave, M. Ott, L. Zettlemoyer, and V . Stoyanov,\n‘‘Unsupervised cross-lingual representation learning at scale,’’ 2019,\narXiv:1911.02116. [Online]. Available: http://arxiv.org/abs/1911.02116\n[7] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\nof deep bidirectional transformers for language understanding,’’ 2018,\narXiv:1810.04805. [Online]. Available: http://arxiv.org/abs/1810.04805\n[8] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘Bert: Pre-training\nof deep bidirectional transformers for language understanding,’’ in Proc.\nNAACL, 2019, pp. 4171–4186.\n[9] J. Eisenschlos, S. Ruder, P. Czapla, M. Kadras, S. Gugger, and J. Howard,\n‘‘MultiFiT: Efﬁcient multi-lingual language model ﬁne-tuning,’’ in Proc.\nConf. Empirical Methods Natural Lang. Process. 9th Int. Joint Conf.\nNatural Lang. Process. (EMNLP-IJCNLP), 2019, pp. 5706–5711.\n[10] V . Hangya, F. Braune, A. Fraser, and H. Schütze, ‘‘Two methods for domain\nadaptation of bilingual tasks: Delightfully simple and broadly applicable,’’\nin Proc. 56th Annu. Meeting Assoc. Comput. Linguistics, vol. 1, 2018,\npp. 810–820.\n[11] J. Howard and S. Ruder, ‘‘Universal language model ﬁne-tuning\nfor text classiﬁcation,’’ 2018, arXiv:1801.06146. [Online]. Available:\nhttp://arxiv.org/abs/1801.06146\n[12] T. Kudo, ‘‘Subword regularization: Improving neural network translation\nmodels with multiple subword candidates,’’ in Proc. 56th Annu. Meeting\nAssoc. Comput. Linguistics, vol. 1, 2018, pp. 66–75.\n[13] T. Kudo and J. Richardson, ‘‘SentencePiece: A simple and language inde-\npendent subword tokenizer and detokenizer for neural text processing,’’ in\nProc. Conf. Empirical Methods Natural Lang. Process., Syst. Demonstra-\ntions, 2018, pp. 66–71.\n[14] G. Lample, A. Conneau, L. Denoyer, and M. Ranzato, ‘‘Unsuper-\nvised machine translation using monolingual corpora only,’’ 2017,\narXiv:1711.00043. [Online]. Available: http://arxiv.org/abs/1711.00043\n[15] A. Radford, J. Wu, D. Amodei, D. Amodei, J. Clark, M. Brundage, and\nI. Sutskever. (2019). Better language models and their implications. Ope-\nnAI Blog. [Online]. Available: https://openai.com/blog/better-language-\nmodels\n[16] S. Ruder and B. Plank, ‘‘Strong baselines for neural semi-supervised\nlearning under domain shift,’’ in Proc. 56th Annu. Meeting Assoc. Comput.\nLinguistics, vol. 1, 2018, pp. 1044–1054.\n[17] G. G. Şahin and M. Steedman, ‘‘Data augmentation via dependency tree\nmorphing for low-resource languages,’’ 2019, arXiv:1903.09460. [Online].\nAvailable: http://arxiv.org/abs/1903.09460\n[18] R. Sennrich, B. Haddow, and A. Birch, ‘‘Neural machine translation of rare\nwords with subword units,’’ in Proc. 54th Annu. Meeting Assoc. Comput.\nLinguistics, vol. 1, 2016, pp. 1715–1725.\n[19] S. Shleifer, ‘‘Low resource text classiﬁcation with ULMFit and\nbacktranslation,’’ 2019, arXiv:1903.09244. [Online]. Available:\nhttp://arxiv.org/abs/1903.09244\n[20] S. L. Smith, D. H. P. Turban, S. Hamblin, and\nN. Y . Hammerla, ‘‘Ofﬂine bilingual word vectors, orthogonal\ntransformations and the inverted softmax,’’ 2017, arXiv:1702.03859.\n[Online]. Available: http://arxiv.org/abs/1702.03859\n[21] C. Sun, X. Qiu, Y . Xu, and X. Huang, ‘‘How to ﬁne-tune BERT for\ntext classiﬁcation?’’ in Proc. China Nat. Conf. Chin. Comput. Linguistics.\nCham, Switzerland: Springer, 2019, pp. 194–206.\n[22] Y . Tao, S. Gupta, S. Krishna, X. Zhou, O. Majumder, and\nV . Khare, ‘‘FineText: Text classiﬁcation via attention-based language\nmodel ﬁne-tuning,’’ 2019, arXiv:1910.11959. [Online]. Available:\nhttp://arxiv.org/abs/1910.11959\n[23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv.\nNeural Inf. Process. Syst., 2017, pp. 5998–6008.\n[24] J. Wei and K. Zou, ‘‘EDA: Easy data augmentation techniques for boost-\ning performance on text classiﬁcation tasks,’’ 2019, arXiv:1901.11196.\n[Online]. Available: http://arxiv.org/abs/1901.11196\n[25] Y . Xu, X. Qiu, L. Zhou, and X. Huang, ‘‘Improving BERT ﬁne-tuning\nvia self-ensemble and self-distillation,’’ 2020, arXiv:2002.10345. [Online].\nAvailable: http://arxiv.org/abs/2002.10345\n[26] J. Yosinski, J. Clune, Y . Bengio, and H. Lipson, ‘‘How transferable are\nfeatures in deep neural networks?’’ in Proc. Adv. Neural Inf. Process. Syst. ,\n2014, pp. 3320–3328.\n[27] Z. Zhang, X. Han, Z. Liu, X. Jiang, M. Sun, and Q. Liu, ‘‘ERNIE: Enhanced\nlanguage representation with informative entities,’’ in Proc. 57th Annu.\nMeeting Assoc. Comput. Linguistics, 2019, pp. 1441–1451.\n[28] Z. Zhang, Y . Wu, H. Zhao, Z. Li, S. Zhang, X. Zhou, and\nX. Zhou, ‘‘Semantics-aware BERT for language understanding,’’ 2019,\narXiv:1909.02209. [Online]. Available: http://arxiv.org/abs/1909.02209\nZHE LI is currently pursuing the master’s degree\nin software engineering with Xinjiang University.\nHis research interests include text generation and\nsocial computing. He is a member of the Chinese\nInformation Processing Society of China and the\nChinese Association for Artiﬁcial Intelligence.\nXIUHONG LI received the bachelor’s degree\nfrom Jilin University, and the master’s and Ph.D.\ndegrees from Xinjiang University. She is currently\nan Associate Professor and a Master’s Supervisor\nwith Xinjiang University. She has presided over\nfour key projects. She has published ten articles.\nHer research interests include natural language\nprocessing and computer architecture.\nJIABAO SHENG is currently pursuing the mas-\nter’s degree in software engineering with Xinjiang\nUniversity. Her research interest includes knowl-\nedge graph. She is a member of the Chinese Infor-\nmation Processing Society of China.\nWUSHOUR SLAMUis currently a Professor and\na Ph.D. Supervisor with Xinjiang University. He is\nan also an Academician of the Chinese Academy\nof Engineering. He is also an Executive Director\nof the Chinese Association for Artiﬁcial Intelli-\ngence. He has published more than 200 articles\nand presided over 65 key projects, including seven\nnational 863 projects and one national 973 project.\nHe has presided over the formulation of ﬁve\ninternational standards and more than 14 national\nstandards. His research interest includes multilingual natural language\nprocessing. He received three National Science and Technology Progress\nAward.\nVOLUME 8, 2020 148499",
  "topic": "Agglutinative language",
  "concepts": [
    {
      "name": "Agglutinative language",
      "score": 0.9590227603912354
    },
    {
      "name": "Computer science",
      "score": 0.895569384098053
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6161662936210632
    },
    {
      "name": "Natural language processing",
      "score": 0.5786048769950867
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.5482603907585144
    },
    {
      "name": "Kazakh",
      "score": 0.49652963876724243
    },
    {
      "name": "Vocabulary",
      "score": 0.45652878284454346
    },
    {
      "name": "Language model",
      "score": 0.4349377453327179
    },
    {
      "name": "Representation (politics)",
      "score": 0.4209184944629669
    },
    {
      "name": "Linguistics",
      "score": 0.08961206674575806
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Morpheme",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I96908189",
      "name": "Xinjiang University",
      "country": "CN"
    }
  ],
  "cited_by": 24
}