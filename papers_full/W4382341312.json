{
  "title": "Transformer-Based Feature Fusion Approach for Multimodal Visual Sentiment Recognition Using Tweets in the Wild",
  "url": "https://openalex.org/W4382341312",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5090557541",
      "name": "Fatimah Alzamzami",
      "affiliations": [
        "University of Ottawa"
      ]
    },
    {
      "id": "https://openalex.org/A5109797436",
      "name": "Abdulmotaleb El Saddik",
      "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence",
        "University of Ottawa"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2161969291",
    "https://openalex.org/W2074788634",
    "https://openalex.org/W2952667353",
    "https://openalex.org/W3088631780",
    "https://openalex.org/W6675224631",
    "https://openalex.org/W2041616772",
    "https://openalex.org/W6676019839",
    "https://openalex.org/W2770602608",
    "https://openalex.org/W2280370717",
    "https://openalex.org/W2253728219",
    "https://openalex.org/W3195841930",
    "https://openalex.org/W2513550067",
    "https://openalex.org/W2114968414",
    "https://openalex.org/W3003362227",
    "https://openalex.org/W3032087714",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2949662773",
    "https://openalex.org/W4238539449",
    "https://openalex.org/W6742835484",
    "https://openalex.org/W2738672149",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W6795475546",
    "https://openalex.org/W6864202710",
    "https://openalex.org/W3194949249",
    "https://openalex.org/W3209798173",
    "https://openalex.org/W2904483377",
    "https://openalex.org/W3175546442",
    "https://openalex.org/W3003720578",
    "https://openalex.org/W3143373604",
    "https://openalex.org/W4214736485",
    "https://openalex.org/W2594108779",
    "https://openalex.org/W2953391430",
    "https://openalex.org/W2904335795",
    "https://openalex.org/W3168420408",
    "https://openalex.org/W6804598379",
    "https://openalex.org/W3188943145",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W4312290555",
    "https://openalex.org/W3102488888",
    "https://openalex.org/W2745497104",
    "https://openalex.org/W3164024107",
    "https://openalex.org/W2101176070",
    "https://openalex.org/W2108113956",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4394645697",
    "https://openalex.org/W3104792420"
  ],
  "abstract": "We present an image-based real-time sentiment analysis system that can be used to recognize in-the-wild sentiment expressions on online social networks. The system deploys the newly proposed transformer architecture on online social networks (OSN) big data to extract emotion and sentiment features using three types of images: images containing faces, images containing text, and images containing no faces/text. We build three separate models, one for each type of image, and then fuse all the models to learn the online sentiment behavior. Our proposed methodology combines a supervised two-stage training approach and threshold-moving method, which is crucial for the data imbalance found in OSN data. The training is carried out on existing popular datasets (i.e., for the three models) and our newly proposed dataset, the Domain Free Multimedia Sentiment Dataset (DFMSD). Our results show that inducing the threshold-moving method during the training has enhanced the sentiment learning performance by 5-8&#x0025; more points compared to when the training was conducted without the threshold-moving approach. Combining the two-stage strategy with the threshold-moving method during the training process, has been proven effective to further improve the learning performance (i.e. by <inline-formula> <tex-math notation=\"LaTeX\">$\\approx ~12$ </tex-math></inline-formula>&#x0025; more enhanced accuracy compared to the threshold-moving strategy alone). Furthermore, the proposed approach has shown a positive learning impact on the fusion of the three models in terms of the accuracy and F-score.",
  "full_text": "Received 7 February 2023, accepted 1 April 2023, date of publication 10 May 2023, date of current version 17 May 2023.\nDigital Object Identifier 10.1 109/ACCESS.2023.3274744\nTransformer-Based Feature Fusion Approach\nfor Multimodal Visual Sentiment Recognition\nUsing Tweets in the Wild\nFATIMAH ALZAMZAMI\n 1 AND ABDULMOTALEB EL SADDIK\n1,2, (Fellow, IEEE)\n1Multimedia Communication Research Laboratory, School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, ON K1N 6N5, Canada\n2Department of Computer Vision, Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, United Arab Emirates\nCorresponding author: Fatimah Alzamzami (falza094@uottawa.ca)\nABSTRACT We present an image-based real-time sentiment analysis system that can be used to recognize\nin-the-wild sentiment expressions on online social networks. The system deploys the newly proposed\ntransformer architecture on online social networks (OSN) big data to extract emotion and sentiment features\nusing three types of images: images containing faces, images containing text, and images containing no\nfaces/text. We build three separate models, one for each type of image, and then fuse all the models to learn the\nonline sentiment behavior. Our proposed methodology combines a supervised two-stage training approach\nand threshold-moving method, which is crucial for the data imbalance found in OSN data. The training\nis carried out on existing popular datasets (i.e., for the three models) and our newly proposed dataset, the\nDomain Free Multimedia Sentiment Dataset (DFMSD). Our results show that inducing the threshold-moving\nmethod during the training has enhanced the sentiment learning performance by 5-8% more points compared\nto when the training was conducted without the threshold-moving approach. Combining the two-stage\nstrategy with the threshold-moving method during the training process, has been proven effective to further\nimprove the learning performance (i.e. by ≈12% more enhanced accuracy compared to the threshold-moving\nstrategy alone). Furthermore, the proposed approach has shown a positive learning impact on the fusion of\nthe three models in terms of the accuracy and F-score.\nINDEX TERMS Transformers, ViT, sentiment, online social media, transfer learning, threshold moving,\ntweets, images, feature extraction, multimodality, fusion, big data, deep learning.\nI. INTRODUCTION\nPeople express their feelings, thoughts, and opinions in dif-\nferent visual ways, such as posting laughing faces, sunny\nbeaches, or memes, on online social networks (OSNs), which\ncontain diverse types of images. Images and visuals some-\ntimes express feelings more clearly than words. On Twit-\nter alone, tweets with images receive 89% more likes and\n150% more retweets. In this work, we focus on exploiting\ndifferent pieces of information within images. The aim is to\nanalyze online social behavior (i.e., sentiment in this paper)\nas a support tool for textual-based analysis or to understand\nonline social behaviors on online social networks (OSNs)\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Barbara Guidi\n.\nwhen textual-based tools are limited or even unavailable. One\nmajor challenge of developing systems for OSNs is their\nuncontrolled content; users have the freedom to populate data\nunder open circumstances in an unstructured manner. OSN\nusers have the control to start a trend or share a thought, which\nmakes social media an open platform that is not restricted to\nany particular domain. This has raised a need to build flexible\nmodels and systems that are able to adapt or generalize to\ndifferent domains -which is the main objective of this work.\nThis introduces a technical challenge that arises from the\nlack of available datasets that can be used to train models\nthat can be adapted or generalized to different domains.\nResearchers tend to construct datasets according to their spe-\ncific domain of interest. We recognized this issue in our ear-\nlier study [1] and attempted to resolve it by proposing Domain\n47070\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ VOLUME 11, 2023\nF. Alzamzami, A. El Saddik: Transformer-Based Feature Fusion Approach for Multimodal Visual Sentiment Recognition\nFree-Multimedia-Sentiment Dataset (DFMSD). Our findings\nin [1] revealed that specific domain sentiment models do\nnot generalize well on different domain-specific data. Unlike\ndomain-specific models, general sentiment models have been\nshown to adapt well to domain-specific sentiment datasets.\nNote that our previous models were designed for textual\ntweets only. In this paper, we investigate domain-independent\nsentiment extraction from visual content and conduct exper-\nimental evaluations using our image dataset (DFMSD). The\nimages were collected under criteria-free conditions for use\nin models that work in the wild. The dataset contains different\ntypes of images that are categorized into three types: images\ncontaining faces, images containing text, and images contain-\ning no faces/text. Our assumption is that each image type\ncontributes valuable information that is embedded within the\nimages and can be used to enhance the learning of sentiment\nbehavior on online social media. A more robust performance\nin image classification tasks can be achieved using visual\ntransformers (ViTs), which have been shown to outperform\nCNNs [2], [3]. The attention mechanism has been shown\nto be the key element in achieving such high-performance\nrobustness. ViT uses the attention mechanisms directly on\nimage patches without depending on a CNN, where attention\nis either used to replace some components of the CNN or\nin conjunction with it (i.e., with the CNN). Furthermore,\nthe multihead self-attention layer allows the ViT to embed\ninformation globally (i.e., attend to global features) across\nthe overall image, which has been shown to improve the\nlearning performance of image classification by four times\ncompared to that of the CNN. Similar achievement is seen\nwith textual-based transformers models (i.e., BERT-based\nmodels) [4], [5]. In this work, we utilize the transformer\narchitecture to train three models, one for each image type,\nto extract high-quality features for each task (i.e., image type).\nThen, we use these features to model our final multimodal\nvisual sentiment classifier. Transformers have been shown to\nperform best when trained on large-scale datasets [2], [5].\nThis means that transformers are able to generalize well on\nclassification tasks when trained on large-scale datasets com-\npared to when trained on small datasets. With the limitation\nof existing datasets, which are small in size, researchers have\nexploited the transfer learning approach to benefit from trans-\nformer models pretrained on large-scale datasets. Given that\nit is expensive to curate large-scale datasets of high quality in\nterms of time, cost, and human labor, transfer learning offers\na reliable solution for learning classification tasks using small\ndatasets. To overcome the differences between source and tar-\nget tasks, the strategy of two-stage learning (i.e., finetuning)\nhas been recommended in the literature [6], [7] to compensate\nfor the limitation of small datasets.\nData imbalance is a common phenomenon in sentiment\ndatasets collected through OSNs [1], [8], [9]. This is also\nobserved in our DFSMD dataset. Although the literature [10]\nsuggests that a balanced dataset would improve model learn-\ning, it is too expensive to balance the data while simulta-\nneously preserving the natural distribution to avoid biases.\nTo overcome the class imbalance issue in this work, we pro-\npose fusing the threshold-moving approach with the sen-\ntiment learning process. It has also been proven that the\ntransformer architecture [4] is effective in dealing with class\nimbalance.\nIn this paper, we fuse the threshold-moving approach with\nthe learning process conducted using transformer architec-\nture as an attempt to address the class imbalance problem\nand to enhance the learning performance of our models.\nThe learning of multimodal sentiment recognition requires\ntwo components: a feature extraction method and a fusion\nstrategy. In this work, we adapt the multimodality approach\nin [11] and [12] and propose training three separate\ntransformer-based deep models to extract features from three\ntypes of images (i.e., tasks): images containing faces, images\ncontaining text, images containing no faces/text. We adopt the\nintermediate fusion approach to fuse the extracted features\nand feed them to an MLP architecture to build our final\nmultimodal sentiment model. To the best of our knowledge,\nwe are the first to use a transformer-based fusion approach\nwith pretrained transformer-based models to extract features\nfor multimodal sentiment analysis on OSNs. Additionally,\nwe believe that we are the first to fuse the threshold-moving\napproach with the learning process using the transformer\narchitecture.\nWe summarize the contributions of this work as follows:\n• A transformer-based two-stage learning strategy is uti-\nlized to alleviate the issue of our small sentiment dataset\n(DFMSD).\n• The threshold-moving approach is fused with the learn-\ning process using the transformer architecture to solve\nthe data imbalance of sentiment datasets.\n• Three transformer-based models are developed using\nthe proposed approach to extract deep features from\nDFMSD images to enhance the feature representation to\nbe used for learning the visual sentiment model.\n• A multimodality visual sentiment predictive model is\ndesigned and implemented using three types of deep\nfeatures extracted based on our transformer-based pre-\ntrained models. Our DFMSD is used for modeling and\nevaluation in this work.\nThe rest of this paper is organized as follows. Section II\npresents details of the related work and Section III explains\nthe datasets used for the modeling and analysis. Our proposed\nframework and methodology is presented in Section IV.\nSection V explains the experimental design and evaluation\nprotocol along with the results and analysis. Finally, in\nSection VI we conclude our proposed work and findings and\ndiscuss future directions.\nII. RELATED WORK\nFacial expression recognition (FER) for online social net-\nworks (OSNs) in the wild is extremely challenging due to\nthe uncontrolled condition of the images that can be shared.\nIn addition to real faces, animated faces can be seen in image-\nlike memes. Additionally, various head poses, occlusion, and\nVOLUME 11, 2023 47071\nF. Alzamzami, A. El Saddik: Transformer-Based Feature Fusion Approach for Multimodal Visual Sentiment Recognition\nface deformation and blur may be observed for faces under\nunconstrained conditions. However, in the past few decades,\ngreat progress has been made in FER, where different learn-\ning methods that achieve good performance have been used.\nSome methods, such as SVM and Bayesian networks, require\na preprocessing step to extract facial features before they are\nused for facial emotion classification, which adds substan-\ntial effort and computational overhead [13], [14], [15], [16].\nIn deep learning-based methods, both facial feature extrac-\ntion and facial emotion classification are combined into one\nsingle stage, breaking the dependency on hand-crafted fea-\ntures [17], [18]. Convolutional neural networks (CNNs) have\na natural inductive bias for learning feature representations\nfrom images and thus have shown promising performance in\nFER [19], [20], [21], [22]. However, CNN-based models can\nbe sensitive to complex image backgrounds with occlusion\nor variant head poses [23]. Recent studies have shown that\nvision transformers (ViTs) are robust against image occlusion\nand disturbance [24], [25], [26], which justifies our decision\nto use ViTs as the backbone of our visual models to be used\non OSN data.\nNovel transformers [2] have become the state-of-the-art\nmethod in NLP tasks, and recently, they have been applied\nin computer vision tasks [27], [28]. Visual transformers have\nachieved remarkable performance in image classification\ntasks [2], [3] and outperformed CNNs in terms of compu-\ntational efficiency and accuracy [2]. ViT was designed based\non the attention mechanism, which has proven to be a key\nelement for image classification to achieve high-performance\nrobustness. ViT uses the attention mechanisms directly on\na sequence of input image patches without depending on a\nCNN, where attention is either used in conjunction with the\nCNN or to replace some CNN components. When trained on\na sufficient amount of data, ViT outperforms similar state-of-\nthe-art CNNs with four times fewer computational resources\nand four times better efficiency and accuracy [2]. Unlike\nCNNs, which have small local receptive fields in each layer,\nthe multihead self-attention layer allows the ViT to embed\ninformation globally (i.e., attend to global features) across\nthe overall image. Moreover, the model learns to encode the\nrelative location of image patches to reconstruct the image\nstructure.\nViT has been shown to perform best when trained on\nlarge-scale datasets, which was manifested in its perfor-\nmance on ResNet against ImageNet [2]. This means that\nViT is able to generalize well on image classification tasks\nwhen trained on large-scale datasets compared to when\ntrained on small datasets. Researchers [23] can benefit from\nvision transformer models trained on large-scale datasets by\nexploiting a transfer learning approach that uses pretrained\nweights to finetune transformer architectures on smaller\ndatasets. The authors in [23] adopted ViT transfer learn-\ning to learn an FER model through one-stage finetuning\nusing pretrained weights from a transformer-based Deit-S\nmodel. Ma et al. [29] showed a positive impact of using\npretrained weights (i.e., obtained from ImageNet-21K) when\ntraining their transformer-based FER model compared to\nwhen the training was conducted from scratch. While vision\ntransformer-based FER models have been introduced, to the\nbest of our knowledge, we could not find studies that applied\nvision transformers to visual sentiment classification tasks.\nTransfer learning has addressed the problem of small datasets\n(i.e., given that it is time, cost, and resource consuming to\nbuild large-scale manual annotated datasets for image classi-\nfication problems, including FER and sentiment recognition).\nTo further compensate for small datasets, existing studies sug-\ngest overcoming the difference between the source task and\ntarget task when using transfer learning through a two-stage\nfinetuning strategy [30], [31]. In the first stage of the finetun-\ning strategy, there is a learning shift from the source task to the\ntarget task, while in the second stage, the learning is refined in\nthe target task [7]. The two-stage strategy has been shown to\noutperform one-stage finetuning on FER tasks that use small\ndatasets [6], [7]. In this work, we adopt a two-stage strategy to\nbuild our ViT-based FER and sentiment recognition models.\nSeveral efforts have been made to analyze sentiment\nand emotion using textual and visual modalities [32].\nIn the context of textual sentiment analysis, Alzamzami and\nEl Saddik [4] attempted domain-independent sentiment anal-\nysis using a DL transformer network and showed that their\nmodel can be used to adapt to various domains, including\nsports and movie reviews. Image sentiment recognition is also\nan area of research. Sun et al. [33] designed an algorithm that\ndiscovers affective regions and supplements local features in\nimages to improve the performance of visual sentiment analy-\nsis. Multimodal emotion and sentiment recognition has been\nan equally active research area in the last few years. Fortin\net al. [12] proposed a multimodal architecture for emotion\nrecognition systems, where predictions in the absence of one\nor two modalities are achieved by using a classifier for each\ncombination of text, image, and tags. In another work by\nXu et al. [34], an interplay of visual and textual content for\nsentiment recognition was modeled based on a comemory\nnetwork.\nThe learning of multimodal sentiment recognition requires\na feature extraction method and fusion strategy [35]. Most\nof the previous work on multimodal emotion and sentiment\nrecognition uses low-level features (e.g., SIFT for visual\nmodalities and GloVe for textual modalities) or deep fea-\ntures [35]. Features that are extracted from pretrained deep\nlearning models are called deep features. They are extracted\nafter a DL model is trained using a labeled dataset. In existing\nstudies on facial recognition, deep features were extracted\nfrom pretrained facial recognition networks, and similarly\npretrained text deep models have been used to extract text\nfeatures for emotion and sentiment analysis [35]. Such studies\nhighlight that deep features yield better performance than\nlow-level features. CNN pretrained models are widely used\nfor deep feature extraction due to their natural inductive\nbias. The authors in [12] used DenseNet-121 pretrained on\n47072 VOLUME 11, 2023\nF. Alzamzami, A. El Saddik: Transformer-Based Feature Fusion Approach for Multimodal Visual Sentiment Recognition\nImageNet to extract deep features for image, text, and tag\nmodels before they concatenated these features and fed them\nto two fully connected layers to obtain final predictions. Sim-\nilarly, the authors in [11] used CNN-based pretrained models\nfor deep feature extraction, VGG16 for image features and\nBalanceNet for textual features. A hybrid of intermediate and\nlate fusion approaches was implemented based on CNNs to\nconcatenate the features for the final sentiment prediction.\nIn this work, we follow the multimodal approach in [11]\nand [12] and propose the use of three transformer-based\npretrained deep models to extract features from images, faces\nand texts within the images. We adopt the intermediate fusion\napproach to fuse image, facial emotion, and textual features\nand feed them to an MLP architecture to build our multimodal\nsentiment classifier. To the best of our knowledge, we are the\nfirst to use a transformer-based fusion approach with three\npretrained transformer-based models to extract features for\nmultimodal sentiment analysis on OSNs.\nIII. DATASETS\nThis section presents popular datasets used for textual and\nvisual sentiment analysis and facial emotion recognition on\nbig data, in addition to our new sentiment multimedia dataset\n(DFSMD) [9].\n• Twitter for Sentiment Analysis (T4SA) [36]: T4SA\nis a multimedia (i.e., texts and images) sentiment\ndataset that contains 1 million tweets with 1.5 million\nimages. Texts were noisily annotated with three senti-\nment classes: positive, negative, and neutral. The images\nwere annotated based on the sentiment associated with\nthe texts. Due to the quality of the neutral class anno-\ntation observed during initial experiments, there was a\nconfusion between the neutral class and both positive\nand negative classes. As a result, we removed the neu-\ntral class from T4SA dataset for training purposes. The\nT4SA dataset is used for the first stage finetuning of the\nbasic visual sentiment modeling in this work.\n• FER-2013 [37]: FER-2013 is a well-known facial emo-\ntion recognition dataset that has been used extensively in\nFER models and applications. It consists of 35K images\nthat are grayscale with 48*48 pixels. The facial images\nwere manually annotated with seven emotions: anger,\ndisgust, fear, happy, sad, surprise, and neutral. The\nFER-2013 dataset was used to build our FER model.\n• AffectNet [38]: AffectNet is currently the largest man-\nually annotated facial emotion recognition in-the-wild\ndataset. It consists of 1 M facial images in greyscale with\n48*48 pixels. 44 K of which were manually annotated\nwith eight facial emotions: neutral, happy, sad, surprise,\nfear, disgust, anger, and contempt. Since we are studying\nsentiment in-the-wild on social media, animated and\ncartoon images exist, especially in memes. Therefore,\nwe converted AffectNet images into animated versions\nand combined them with the original images. The mod-\nified version of the AffectNet dataset was used for\nthe first stage finetuning of our facial emotion recog-\nnition modeling in this work. Note that we converted\nthe images of AffectNet to grayscale since we used\nFER-2013 data to build our final FER model.\n• DFMSD [9]: The domain-free multimedia sentiment\ndataset (DFMSD) is our newly introduced dataset for\nvisual and textual sentiment analysis. It was designed\nand constructed to work in the wild and to be able\nto deal with uncontrolled conditions in online social\nmedia. Data in DFSMD was collected using Twitter\nStream API. The protocol followed to collect and anno-\ntate DFSMD distinguishes it from other datasets, as the\ndata collection process was not restricted to any key-\nwords, domains, locations, or predefined retrieval cri-\nteria. The annotation questions and dataset annotators\nwere selected carefully to minimize any possible biases\nduring the annotation. Moreover, the annotators were\nselected on the basis of sentiment agreement using\nthree expert psychologists. The DFSMD consists of\n14,488 tweets that contain 10,244 images; 46% (i.e.,\n6683 tweets) of the tweets are positive, 33% (i.e., 4822)\nare negative, and 21% (i.e., 2983) are neutral. The image\ndistribution is as follows: 47% belonging to the pos-\nitive class, 10% belonging to the negative class, and\n43% belonging to the neutral class. Note that texts and\nimages were annotated separately in a way that does\nnot affect the annotation of the images. We decided to\nextend our sentiment image dataset by following the\nsame collection and annotation approach used earlier as\nan attempt to improve the deep learning performance\nand to minimize the problem of a severe class imbalance.\nThe first version was published in an earlier study [9].\nIV. METHOD\nThis section presents in detail the method we follow to\nmodel our multi-model visual social behavior analyzer; it\nincludes data preprocessing, datasets, and all the approaches\nwe adopted for building our final model.\nA. PREPROCESSING\nThe preprocessing step is very important for the learning\nprocess; we cleaned, denoised, and prepared the data before\nwe fed them to the VIT for training. Image preprocessing\nconsists of the following steps:\n• Face detection: We used the face detection algorithm\nproposed in [39] that uses facial keypoints to detect\nfaces. The face detector finds four coordinates for the\nregion of interest (ROI) of faces. Then, the detected faces\nare cropped, and all irrelevant background is discarded.\nAdditionally, faces that are far away or not clear, with\nrespect to the ratio of the face and the image size, are\ndiscarded. If the ratio is less than a predefined threshold\nwith respect to the image, and if the value is less than\na predefined threshold, we discarded the face. This step\nwas applied to the facial emotion recognition modeling\npart.\nVOLUME 11, 2023 47073\nF. Alzamzami, A. El Saddik: Transformer-Based Feature Fusion Approach for Multimodal Visual Sentiment Recognition\nFIGURE 1. The architecture of visual Transformers [2] used in training our models.\n• Color to grayscale conversion. This step was applied in\nthe facial emotion recognition modeling part.\n• Data augmentation: The size of the dataset is increased,\nand the imbalance between classes is addressed since\ndeep learning works better with more data. We used\n2 types of augmentation, one for training and another\nfor testing:\n– Training time: Images were randomly resized in the\nrange of (224,350) and then center-cropped by a\ncropping size of (224, 224). Then, the images were\nrandomly flipped.\n– Testing time: Images were resized to (256,256)\nand then cropped using the 10-crop technique.\nUsing ten-crop technology, an image is resized\nto (256,256), and 5 crops (upper-left, upper-right,\nlower-left, lower-right, center) with a cropping\nsize of (224,224) are made. Then, L-R flipping\nis applied, resulting in 10 cropped-flipped images.\nFinally, we used the average prediction of these\n10 images [40]. This step was applied in the\nbasic visual sentiment modeling and facial emotion\nrecognition modeling parts.\n• Text detection and extraction: We used optical character\nrecognition (OCR) to detect and recognize texts from\nimages. Extracted words are not in sorted order after\nOCR extraction; hence, we sorted the extracted words\nin order of their occurrence using contour detection to\nseparate the different lines. Then, we simply processed\nthe contours left to right to sort the words within lines.\nFinally, we applied quantization on heights with a prede-\nfined threshold to group words in the same line together.\nThis step was applied in the textual-images part.\nB. VISUAL TRANSFORMERS MODEL\nVisual transformer (ViT) [2], which was introduced in 2020,\nis used as the deep learning architecture in our work. ViT has\nbeen a competitive alternative to CNNs for image recognition\ntasks. It outperforms the current state-of-the-art CNNs by\nfour times in terms of computational efficiency and accu-\nracy [2], especially on big data regimes. In big data regimes,\nthe inductive biases of CNNs are not needed; instead, ViT\ncan learn those biases by itself. Shallower layers of the\nViT are able to localize attention (i.e., attend to local pix-\nels) and globalize attention (i.e., attend to global pixels)\n47074 VOLUME 11, 2023\nF. Alzamzami, A. El Saddik: Transformer-Based Feature Fusion Approach for Multimodal Visual Sentiment Recognition\nFIGURE 2. The proposed architecture for ViT-based multi-modality fusion for visual online social behavior analysis.\ncompared to CNNs, which only have local small receptive\nfields in shallower layers. The advantage of shallower layers\nbeing able to attend to local and global pixels in images\nis that it allows the ViT model to learn how and when to\nattend, and the bias is no longer needed. Because of its\nefficiency in handling big data regimes [2], it can be widely\nused in systems and applications related to online social\nnetworks—where data sizes are very large—with remarkable\nperformance.\nA high-level overview of the ViT architecture is given in\nFigure. 1. An input image is split into patches and then\nflipped in order and flattened out. A linear projection is\napplied to the flattened patches, and then positional encoding\nis added before the data are fed to the multilayer transformer\nencoder. The structure of a single encoder consists of a multi-\nhead attention module and multilayer perceptron module. The\noutput of the encoder is then fed to a multilayer perceptron\nhead, which is used as a classification module that yields class\npredictions.\nC. THRESHOLD-MOVING\nThe default decision threshold (i.e., 0.5) for classification\nproblems with class imbalance might negatively impact the\nlearning performance and hence yield poor results. The\ndefault decision threshold might not represent the optimal\ninterpretation of a model’s predicted probabilities. As such,\na simple approach to improve the classification performance\non imbalanced data is to tune the hyperparameter (i.e., thresh-\nold) that is used to map the predicted probabilities to class\nlabels. The process of tuning this hyperparameter is called\nthreshold moving. In this work, we calculated the optimal\nthreshold using a grid search approach. We searched thresh-\nold values for a model and considered the best value, i.e.,\nthe value that yielded the best performance in terms of our\nevaluation metric. We applied threshold moving during the\ntraining process in such a way that in each validation iteration\n(i.e., epoch), we examined a range of threshold values on the\npredicted class probabilities to find the best threshold. The\nthreshold that achieved the best performance (i.e., in terms\nof the evaluation metric) was then adopted for the model\n(i.e., at the current iteration or epoch). Upon the completion of\nthe training, the model with the best performance was chosen\nto make predictions on new data.\nD. TWO-STAGE STRATEGY\nTransfer learning offers a rich set of benefits, including\nimproving the efficiency of model training and saving time\nand resources, since building a high-performance model from\nscratch requires a large amount of data, time, resources, and\neffort. Therefore, we used the two-stage learning approach [6]\nto train ViT models as an attempt to solve the limitation\nof small labeled datasets. We implemented the first-stage\nVOLUME 11, 2023 47075\nF. Alzamzami, A. El Saddik: Transformer-Based Feature Fusion Approach for Multimodal Visual Sentiment Recognition\nfinetuning step on very large datasets to maximize the ben-\nefits of transfer learning. We used a ViT architecture with\npretrained weights from the ImageNet-21K dataset [2]. The\nwhole ViT architecture was retrained for the first stage of\nfinetuning. For the basic sentiment model, we used the TS4A\ndataset to finetune the ViT pretrained model. The model\nlearns two sentiment classes: positive and negative. To over-\ncome the class imbalance between the two classes, we fused\nthe threshold-moving approach with the sentiment learning\nprocess in this part. For the facial emotion expression model,\nwe finetuned the pretrained ViT model using the modified\nversion of the AffectNet dataset. The modified version of\nthe AffectNet dataset includes its original images in addition\nto the same images converted into animated versions. The\nmodel learns eight classes: neutral, happy, sad, surprise, fear,\ndisgust, anger, and contempt.\nFor the second-stage finetuning step, we initialized the ViT\narchitecture with the weights obtained from our first-stage\nfinetuning step. The last fully connected layer was replaced\nby a new MLP head for both models: basic sentiment and\nfacial emotion models.The basic sentiment classifier out-\nputs two classes: positive and negative. Our DFSM dataset\nwas used for the second-stage fine-tuning step by adopt-\ning the threshold-moving approach during the process of\nsentiment learning on the positive and negative classes.\nThe finetuning step in the second stage was performed by\ntraining the whole ViT architecture. We took the output\nof the last layer (i.e., the high-level representations of the\nmode) and fed it as features to our visual multimodal sen-\ntiment classifier. The facial emotion classifier outputs seven\nclasses: anger, disgust, fear, happy, sad, surprise, and neutral.\nThe FER-2013 dataset was used to finetune the AffectNet\npretrained model after the first-stage finetuning step. The\nfinetuning step in the second stage was performed by train-\ning the whole ViT architecture. We took the output of the\nlast layer (i.e., the high-level representations of the mode)\nand fed it as features to our visual multimodal sentiment\nclassifier.\nE. VISUAL DEEP MULTIMODAL FUSION\nWe adopted the architecture of intermediate fusion to fuse our\ndeep learning-based models with the goal of building a mul-\ntimodality online social behavior model. Figure. 2 shows an\nillustration of our proposed architecture for developing a mul-\ntimodal online social behavior classifier (i.e., a sentiment case\nstudy in this work). Intermediate fusion allows for data fusion\nat different stages of model learning as it offers flexibility to\nfuse features at different depths. Deep learning-based multi-\nmodal data fusion has shown great improvement in learning\nperformance [12], [35]. The input for the intermediate fusion\nis the higher-level representations (i.e., features) obtained\nthrough multiple layers of deep learning. Hence, the inter-\nmediate fusion in the context of multimodal deep learning\nis the simultaneous fusion of different model representations\ninto a hidden layer so that the model learns a representation\nfrom each of the individual models. The layer where fusion is\nperformed is called the fusion layer. In this work a ViT-based\nfusion is proposed for a multimodality visual online social\nbehavior analysis. Three models, namely: single-modality\nsentiment, facial emotion, and textual sentiment models are\ntrained using Transformer backbones (i.e., ViT for visual\ncontent and BERT for textual content). Then these models\nare used to extract deep features before all are fused to form\none joint feature that will be fed into an MLP classification\nhead (Figure. 2).\nV. EXPERIMENTAL RESULTS AND ANALYSIS\nThis section presents the experimental results and analy-\nsis for the visual models. The first-stage fine-tuning step\nof the ViT architecture was implemented with pretrained\nweights obtained from the ImageNet-21K dataset for both\nsingle-modality sentiment and FER models. Note that single\nmodality model refers to the images as they are without\nextracting facial nor textual features.\nA. PERFORMANCE OF THE BASIC VISUAL SENTIMENT\nMODEL\n1) FIRST STAGE FINETUNING\nWe implemented the first-stage fine-tuning step of ViT archi-\ntecture using T4SA dataset. Due to the poor quality anno-\ntation of the neutral class (i.e., after preliminary experimen-\ntation), we decided to train the model on the positive and\nnegative classes. Table. 1 shows the performance of our ViT-\nbased single-modality sentiment model in the first-stage fine-\ntuning step. Table. 1 shows the results of the performance\nwhen fusing threshold-moving with the training. Threshold-\nmoving has been shown to absolutely enhance the learning\nperformance by six points in terms of accuracy, eight points\nin terms of the positive F-score, and five points in terms of the\nnegative class. This model was used for image generic deep\nfeature extraction to learn our multimodality visual sentiment\nclassifier.\nNote that we present the learning performance (i.e., with-\nout threshold moving applied) for three classes. During the\ntraining, it was observed that the model confused the neutral\nwith both positive and negative classes.\n2) SECOND STAGE FINETUNING\nIn Table. 2, we demonstrate the performance of our\nsecond-stage ViT model using the images from our DFMS\ndataset. Based on the first-stage pretrained model, The per-\nformance accuracy of learning two classes (i.e., positive and\nnegative classes) i.e., with the threshold-moving technique\nfused during the training is 81% with F-scores of 0.86, 0.7 for\nthe positive and negative classes, respectively. Further, the\nresults of the second-stage fine-tuning step have shown the\neffectiveness of the two-strategy fine-tuning approach on\nlearning three classes (positive, negative, and neutral); the\nlearning performance greatly improved by 12 points in terms\nof accuracy, 29 points in terms of positive F-score, and 8 for\n47076 VOLUME 11, 2023\nF. Alzamzami, A. El Saddik: Transformer-Based Feature Fusion Approach for Multimodal Visual Sentiment Recognition\nTABLE 1. The performance of first-stage single-modality ViT sentiment model on T4SA dataset.\nTABLE 2. The performance of second-stage single-modality ViT sentiment model on images from our DFMSD dataset.\nthe negative F-score. We observe that the performance of the\nneutral class decreased in terms of precision which explains\nthe model confusion with the other classes. Overall, the model\nperforms well in distinguishing between positive and negative\nclasses since it has high positive F-score scores > 0.70 and\nnegative F-score scores ≈ 0.60 for both classes and that in the\npresence of the neutral class.\nB. PERFORMANCE OF THE FACIAL EMOTION MODEL\nTo train all FER models, all faces have be to be detected and\ncropped as explained in the preprocessing section.\n1) FIRST STAGE FINETUNING\nWe implemented the first-stage finetuning step of the ViT\narchitecture using the AffectNet dataset. An accuracy of 59%\nand an F-score of 0.59 were obtained for 8 classes using this\nmodel.\n2) SECOND STAGE FINETUNING\nBased on the weights obtained from the first-stage ViT fine-\ntuning step using AffectNet, we implemented the second-\nstage fine-tuning step with the pretrained weights obtained\nfrom the first stage using the FER-2013 dataset. Table. 3 illus-\ntrates the effectiveness of the two-stage strategy in enhanc-\ning the learning performance—in terms of precision, recall,\nand accuracy—between multiple classes. This can be obvi-\nously observed in the recall of the model, which greatly\nimproved by 7 points when applying the two-stage strat-\negy compared to when only using one-stage strategy for\nfinetuning. This model was used for facial emotion deep\nfeature extraction to learn our multimodal visual sentiment\nclassifier.\nTABLE 3. The performance of the second-stage ViT FER model on the\nFER-2013 dataset.\nC. PERFORMANCE OF VISUAL MULTI-MODAL FUSION\nTable. 4 shows the effect of using the extra information of\nfacial emotion and texts residing in images, in addition to the\ninformation from the images themselves. Fusing ViT features\nfrom our FER and single-modality models resulted in a slight\nimprovement in the performance compared to the perfor-\nmance of the single-modality ViT sentiment model, in terms\nof accuracy and F-score for both positive and negative classes.\nWhile fusing single-modality sentiment and FER features\nnoticeably improved the negative precision (i.e., by 6 points),\nthe recall of the positive class decreased (i.e., by 4 points).\nHowever, fusing textual and facial emotion features along\nwith the single-modality sentiment stabilizes the learning and\nfurther improves the overall performance for all the classes in\nterms of F-score. In more detail, negative precision improved\nby 4 points without affecting the positive recall and similarly\npositive recall improves by 3 points without affecting the\nnegative recall.\nWe further examined the effect of fusing facial emotion\nand text features with the single-modality sentiment on three\nclasses: positive, negative, and neutral classes. It can be seen\nfrom Table. 4 that the overall F-score of the model improved\nespecially for the negative and neutral classes when fusing\nVOLUME 11, 2023 47077\nF. Alzamzami, A. El Saddik: Transformer-Based Feature Fusion Approach for Multimodal Visual Sentiment Recognition\nTABLE 4. Performance of fusing three types of ViT-based deep features extracted from three pretrained models: single-modality sentiment, facial\nemotion, and textual sentiment. The performance is evaluated in terms of accuracy, precision, recall, and F-score.\nthe three types of features compared fusing to only two type\nof features.\nVI. CONCLUSION AND FUTURE WORK\nInspired by emojis, which speak a universal language through\niconic expressions, we adopted a supplementary language-\nindependent approach, where a universal language of visual\nimages and emotions is used to model online social behav-\nior and sentiment, in this work. Accordingly, we proposed\na multimodality classifier that leverages three types of\nimages in parallel: images with text, images with faces, and\nimages without faces or text. The corresponding experiments\nshow that exploiting facial emotion and textual information\nextracted from images contributes to enhancing the learn-\ning of visual online social sentiment. Note that sentiment\nbehavior has been considered as a case study to examine our\nproposed approach.\nIt has been found that fusing the threshold-moving\napproach with the learning process using the transformer\narchitecture has shown a great improvement in the learning\nperformance in terms of the F-score and accuracy. It also\nshows robustness in handling the class imbalance problem\nfound in OSN data. In addition, the two-stage finetuning\nstrategy is shown to work with the transformer architecture,\nconfirming its robustness in solving the problem of small and\ninsufficient datasets for both binary and multiclass classifi-\ncation tasks. This finding is consistent with the two-stage\nlearning strategy used with the CNN architecture [6], [7].\nOur proposed transformer-based feature extraction\napproach has been shown to be effective in the learning of\nmultimodal visual sentiment using social media data. The\nhigh F-score and accuracy for both binary and multiclass\nsentiment predictions are evidence that strong feature rep-\nresentations were obtained using our proposed approach.\nAdditionally, it is evident that the consideration of image\ntypes (i.e., to extract sentiment information) is successful\nbecause it actually contributes to the learning of sentiment\nfrom visual images. Given the proposed approach, the data\nused for training, and results, our proposed model could\neventually be used for handling the images in the wild\nof social media independently of particular domains they\nbelong to.\nIn terms of future directions, we are interested in exploring\nextra types of features that can contribute to improving the\nlearning of online sentiment behavior. We are also interested\nin incorporating more types of online social behaviors, such\nas hate speech. Furthermore, we plan to develop a lighter\nversion of the proposed model and deploy it in mobile smart\ndevices. The current version of our model is designed with\nhigh resource requirements for computations and memory,\nwhich makes it unsuitable for deployment on portable smart\ndevices such as smartphones.\nREFERENCES\n[1] F. Alzamzami, M. Hoda, and A. El Saddik, ‘‘Light gradient boosting\nmachine for general sentiment classification on short texts: A comparative\nevaluation,’’IEEE Access, vol. 8, pp. 101840–101858, 2020.\n[2] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, ‘‘An image is worth 16×16 words: Trans-\nformers for image recognition at scale,’’ 2020, arXiv:2010.11929.\n[3] L. Meng, H. Li, B.-C. Chen, S. Lan, Z. Wu, Y.-G. Jiang, and S.-N. Lim,\n‘‘AdaViT: Adaptive vision transformers for efficient image recognition,’’\n2021, arXiv:2111.15668.\n[4] F. Alzamzami and A. El Saddik, ‘‘Monitoring cyber SentiHate social\nbehavior during COVID-19 pandemic in North America,’’ IEEE Access,\nvol. 9, pp. 91184–91208, 2021.\n[5] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\nof deep bidirectional transformers for language understanding,’’ 2018,\narXiv:1810.04805.\n[6] H. Wang, D. P. Tobon V., M. S. Hossain, and A. El Saddik, ‘‘Deep\nlearning (DL)-enabled system for emotional big data,’’ IEEE Access, vol. 9,\npp. 116073–116082, 2021.\n[7] Y. Miao, H. Dong, J. M. A. Jaam, and A. E. Saddik, ‘‘A deep learning\nsystem for recognizing facial expression in real-time,’’ ACM Trans. Multi-\nmedia Comput., Commun., Appl., vol. 15, no. 2, pp. 1–20, May 2019.\n[8] V. Athanasiou and M. Maragoudakis, ‘‘A novel, gradient boosting frame-\nwork for sentiment analysis in languages where NLP resources are not\nplentiful: A case study for modern Greek,’’ Algorithms, vol. 10, no. 1, p. 34,\n2017.\n[9] R. Abaalkhail, F. Alzamzami, S. Aloufi, R. Alharthi, and A. El Saddik,\n‘‘Affectional ontology and multimedia dataset for sentiment analysis,’’ in\nProc. Int. Conf. Smart Multimedia. Cham, Switzerland: Springer, 2018,\npp. 15–28.\n47078 VOLUME 11, 2023\nF. Alzamzami, A. El Saddik: Transformer-Based Feature Fusion Approach for Multimodal Visual Sentiment Recognition\n[10] G. M. Weiss and F. Provost, ‘‘Learning when training data are costly:\nThe effect of class distribution on tree induction,’’ J. Artif. Intell. Res.,\nvol. 19, no. 1, pp. 315–354, Jul. 2003.\n[11] P. Kumar, V. Khokher, Y. Gupta, and B. Raman, ‘‘Hybrid fusion based\napproach for multimodal emotion recognition with insufficient labeled\ndata,’’ in Proc. IEEE Int. Conf. Image Process. (ICIP), Sep. 2021,\npp. 314–318.\n[12] M. Pagé Fortin and B. Chaib-draa, ‘‘Multimodal multitask emotion recog-\nnition using images, texts and tags,’’ in Proc. ACM Workshop Crossmodal\nLearn. Appl., Jun. 2019, pp. 3–10.\n[13] N. Dalal and B. Triggs, ‘‘Histograms of oriented gradients for human detec-\ntion,’’ in Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.,\nJun. 2005, pp. 886–893.\n[14] C. Shan, S. Gong, and P. W. McOwan, ‘‘Robust facial expression recogni-\ntion using local binary patterns,’’ in Proc. IEEE Int. Conf. Image Process.,\n2005, p. 370.\n[15] X. Feng, M. Pietikainen, and A. Hadid, ‘‘Facial expression recognition\nwith local binary patterns and linear programming,’’ Pattern Recognit.\nImage Anal. C/C Raspoznavaniye Obrazov I Analiz Izobrazhenii, vol. 15,\nno. 2, p. 546, 2005.\n[16] I. Buciu and I. Pitas, ‘‘Application of non-negative and local non negative\nmatrix factorization to facial expression recognition,’’ in Proc. 17th Int.\nConf. Pattern Recognit., 2004, pp. 288–291.\n[17] K. He, X. Zhang, S. Ren, and J. Sun, ‘‘Deep residual learning for image\nrecognition,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR),\nJun. 2016, pp. 770–778.\n[18] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, ‘‘Densely\nconnected convolutional networks,’’ in Proc. IEEE Conf. Comput. Vis.\nPattern Recognit. (CVPR), Jul. 2017, pp. 4700–4708.\n[19] S. Li, W. Deng, and J. Du, ‘‘Reliable crowdsourcing and deep locality-\npreserving learning for expression recognition in the wild,’’ in Proc. IEEE\nConf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 2852–2861.\n[20] Y. Li, J. Zeng, S. Shan, and X. Chen, ‘‘Occlusion aware facial expression\nrecognition using CNN with attention mechanism,’’ IEEE Trans. Image\nProcess., vol. 28, no. 5, pp. 2439–2450, May 2019.\n[21] K. Wang, X. Peng, J. Yang, D. Meng, and Y. Qiao, ‘‘Region attention\nnetworks for pose and occlusion robust facial expression recognition,’’\nIEEE Trans. Image Process., vol. 29, pp. 4057–4069, 2020.\n[22] Z. Zhao, Q. Liu, and F. Zhou, ‘‘Robust lightweight facial expression\nrecognition network with label distribution training,’’ in Proc. AAAI Conf.\nArtif. Intell., 2021, vol. 35, no. 4, pp. 3510–3519.\n[23] H. Li, M. Sui, F. Zhao, Z. Zha, and F. Wu, ‘‘MVT: Mask vision transformer\nfor facial expression recognition in the wild,’’ 2021, arXiv:2106.04520.\n[24] M. Naseer, K. Ranasinghe, S. Khan, M. Hayat, F. S. Khan, and M.-H. Yang,\n‘‘Intriguing properties of vision transformers,’’ 2021, arXiv:2105.10497.\n[25] F. Ma, B. Sun, and S. Li, ‘‘Facial expression recognition with visual\ntransformers and attentional selective fusion,’’ 2021, arXiv:2103.16854.\n[26] Q. Huang, C. Huang, X. Wang, and F. Jiang, ‘‘Facial expression recogni-\ntion with grid-wise attention and visual transformer,’’ Inf. Sci., vol. 580,\npp. 35–54, Nov. 2021.\n[27] S. He, H. Luo, P. Wang, F. Wang, H. Li, and W. Jiang, ‘‘TransReID:\nTransformer-based object re-identification,’’ 2021, arXiv:2102.04378.\n[28] K. Mahmood, R. Mahmood, and M. van Dijk, ‘‘On the robustness of vision\ntransformers to adversarial examples,’’ 2021, arXiv:2104.02610.\n[29] F. Ma, B. Sun, and S. Li, ‘‘Facial expression recognition with visual trans-\nformers and attentional selective fusion,’’ IEEE Trans. Affect. Comput.,\nearly access, Oct. 26, 2021, doi: 10.1109/TAFFC.2021.3122146.\n[30] H.-W. Ng, V. D. Nguyen, V. Vonikakis, and S. Winkler, ‘‘Deep learning\nfor emotion recognition on small datasets using transfer learning,’’ in Proc.\nACM Int. Conf. Multimodal Interact., Nov. 2015, pp. 443–449.\n[31] B.-K. Kim, J. Roh, S.-Y. Dong, and S.-Y. Lee, ‘‘Hierarchical commit-\ntee of deep convolutional neural networks for robust facial expression\nrecognition,’’ J. Multimodal User Interfaces, vol. 10, no. 2, pp. 173–189,\nJun. 2016.\n[32] A. Ortis, G. M. Farinella, and S. Battiato, ‘‘Survey on visual sentiment\nanalysis,’’IET Image Process., vol. 14, no. 8, pp. 1440–1456, Jun. 2020.\n[33] M. Sun, J. Yang, K. Wang, and H. Shen, ‘‘Discovering affective regions\nin deep convolutional neural networks for visual sentiment prediction,’’ in\nProc. IEEE Int. Conf. Multimedia Expo (ICME), Jul. 2016, pp. 1–6.\n[34] S. Siriwardhana, T. Kaluarachchi, M. Billinghurst, and S. Nanayakkara,\n‘‘Multimodal emotion recognition with transformer-based self supervised\nfeature fusion,’’ IEEE Access, vol. 8, pp. 176274–176285, 2020.\n[35] M. El Ayadi, M. S. Kamel, and F. Karray, ‘‘Survey on speech emo-\ntion recognition: Features, classification schemes, and databases,’’ Pattern\nRecognit., vol. 44, no. 3, pp. 572–587, 2011.\n[36] L. Vadicamo, F. Carrara, A. Cimino, S. Cresci, F. Dell’Orletta, F. Falchi,\nand M. Tesconi, ‘‘Cross-media learning for image sentiment analysis in\nthe wild,’’ in Proc. IEEE Int. Conf. Comput. Vis. Workshops (ICCVW),\nOct. 2017, pp. 308–317.\n[37] I. J. Goodfellow, D. Erhan, P. L. Carrier, A. Courville, M. Mirza,\nB. Hamner, W. Cukierski, Y. Tang, D. Thaler, and D.-H. Lee, ‘‘Challenges\nin representation learning: A report on three machine learning contests,’’\nin Proc. Int. Conf. Neural Inf. Process.Cham, Switzerland: Springer, 2013,\npp. 117–124.\n[38] A. Mollahosseini, B. Hasani, and M. H. Mahoor, ‘‘AffectNet: A database\nfor facial expression, valence, and arousal computing in the wild,’’ 2017,\narXiv:1708.03985.\n[39] A. Bulat and G. Tzimiropoulos, ‘‘How far are we from solving the 2D & 3D\nface alignment problem? (and a dataset of 230,000 3D facial Landmarks),’’\nin Proc. IEEE Int. Conf. Comput. Vis. (ICCV), Oct. 2017, pp. 1021–1030.\n[40] K. He, X. Zhang, S. Ren, and J. Sun, ‘‘Deep residual learning for image\nrecognition,’’ 2015, arXiv:1512.03385.\nFATIMAH ALZAMZAMI received the M.Sc. degree in computer science\nfrom the Faculty of Electrical Engineering and Computer Science, University\nof Ottawa, Ottawa, Canada, where she is currently pursuing the Ph.D. degree\nin computer science. Her research interests include machine learning, deep\nlearning, big data, social multimedia analysis, and mining. She is supervised\nby Prof. Abdulmotaled El Saddik.\nABDULMOTALEB EL SADDIK (Fellow, IEEE)\nis currently a Distinguished Professor with the\nSchool of Electrical Engineering and Computer\nScience, University of Ottawa, and a Professor\nwith the Mohamed bin Zayed University of Arti-\nficial Intelligence. He has supervised more than\n150 researchers. He has coauthored ten books\nand more than 550 publications and chaired more\nthan 50 conferences and workshops. He received\nresearch grants and contracts totaling more than\n$22 million. His research interests include the establishment of digital twins\nto facilitate the wellbeing of citizens using AI, the IoT, AR/VR, and 5G to\nallow people to interact in real time with one another and with their smart\ndigital representations in the metaverse. He is a fellow of the Royal Society\nof Canada, Engineering Institute of Canada, and Canadian Academy of\nEngineers. He is an ACM Distinguished Scientist. He received several inter-\nnational awards, such as the IEEE I&M Technical Achievement Award, the\nIEEE Canada C. C. Gotlieb (Computer) Medal, and the A. G. L. McNaughton\nGold Medal for important contributions to the field of computer engineering\nand science.\nVOLUME 11, 2023 47079",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7560195922851562
    },
    {
      "name": "Sentiment analysis",
      "score": 0.6496122479438782
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6337509751319885
    },
    {
      "name": "Fuse (electrical)",
      "score": 0.4896002411842346
    },
    {
      "name": "Artificial neural network",
      "score": 0.4215202033519745
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4196263551712036
    },
    {
      "name": "Machine learning",
      "score": 0.4184091091156006
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.4132930636405945
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I153718931",
      "name": "University of Ottawa",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I4210113480",
      "name": "Mohamed bin Zayed University of Artificial Intelligence",
      "country": "AE"
    }
  ]
}