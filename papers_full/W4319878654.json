{
  "title": "Self-Supervised Image Denoising for Real-World Images With Context-Aware Transformer",
  "url": "https://openalex.org/W4319878654",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2100424385",
      "name": "Dan Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2111431716",
      "name": "Fangfang Zhou",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2056370875",
    "https://openalex.org/W2048695508",
    "https://openalex.org/W2887194216",
    "https://openalex.org/W2508457857",
    "https://openalex.org/W2983315964",
    "https://openalex.org/W3090412929",
    "https://openalex.org/W2963725279",
    "https://openalex.org/W3035383808",
    "https://openalex.org/W2963111219",
    "https://openalex.org/W2967502735",
    "https://openalex.org/W3178192988",
    "https://openalex.org/W4312444885",
    "https://openalex.org/W6759001153",
    "https://openalex.org/W2902857081",
    "https://openalex.org/W2947930026",
    "https://openalex.org/W6749271710",
    "https://openalex.org/W3034902810",
    "https://openalex.org/W2953231542",
    "https://openalex.org/W6762734363",
    "https://openalex.org/W3044818160",
    "https://openalex.org/W3097262261",
    "https://openalex.org/W4312575110",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2985577924",
    "https://openalex.org/W3080848135",
    "https://openalex.org/W2966295362",
    "https://openalex.org/W4290671759",
    "https://openalex.org/W2952323569",
    "https://openalex.org/W2963200935",
    "https://openalex.org/W3099686304",
    "https://openalex.org/W3111081528",
    "https://openalex.org/W4312624488",
    "https://openalex.org/W4224104371",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3207918547",
    "https://openalex.org/W4312574643",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W6800374665",
    "https://openalex.org/W6761835442",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2121927366",
    "https://openalex.org/W1791560514",
    "https://openalex.org/W2799192307",
    "https://openalex.org/W2963571608",
    "https://openalex.org/W3035542568",
    "https://openalex.org/W3174829522",
    "https://openalex.org/W2047710600",
    "https://openalex.org/W2219841864",
    "https://openalex.org/W2071005004",
    "https://openalex.org/W1999046526",
    "https://openalex.org/W3098337560"
  ],
  "abstract": "In recent years, the development of deep learning has been pushing image\\ndenoising to a new level. Among them, self-supervised denoising is increasingly\\npopular because it does not require any prior knowledge. Most of the existing\\nself-supervised methods are based on convolutional neural networks (CNN), which\\nare restricted by the locality of the receptive field and would cause color\\nshifts or textures loss. In this paper, we propose a novel Denoise Transformer\\nfor real-world image denoising, which is mainly constructed with Context-aware\\nDenoise Transformer (CADT) units and Secondary Noise Extractor (SNE) block.\\nCADT is designed as a dual-branch structure, where the global branch uses a\\nwindow-based Transformer encoder to extract the global information, while the\\nlocal branch focuses on the extraction of local features with small receptive\\nfield. By incorporating CADT as basic components, we build a hierarchical\\nnetwork to directly learn the noise distribution information through residual\\nlearning and obtain the first stage denoised output. Then, we design SNE in low\\ncomputation for secondary global noise extraction. Finally the blind spots are\\ncollected from the Denoise Transformer output and reconstructed, forming the\\nfinal denoised image. Extensive experiments on the real-world SIDD benchmark\\nachieve 50.62/0.990 for PSNR/SSIM, which is competitive with the current\\nstate-of-the-art method and only 0.17/0.001 lower. Visual comparisons on public\\nsRGB, Raw-RGB and greyscale datasets prove that our proposed Denoise\\nTransformer has a competitive performance, especially on blurred textures and\\nlow-light images, without using additional knowledge, e.g., noise level or\\nnoise type, regarding the underlying unknown noise.\\n",
  "full_text": "Received 25 January 2023, accepted 3 February 2023, date of publication 10 February 2023, date of current version 15 February 2023.\nDigital Object Identifier 10.1 109/ACCESS.2023.3243829\nSelf-Supervised Image Denoising for Real-World\nImages With Context-Aware Transformer\nDAN ZHANG\n AND FANGFANG ZHOU\nSenslab Technology Company Ltd., Shanghai 201203, China\nCorresponding author: Dan Zhang (zhangdan fiona@163.com)\nABSTRACT In recent years, the development of deep learning has been pushing image denoising to a new\nlevel. Among them, self-supervised denoising is increasingly popular because it does not require any prior\nknowledge. Most of the existing self-supervised methods are based on convolutional neural networks (CNN),\nwhich are restricted by the locality of the receptive field and would cause color shifts or textures loss. In this\npaper, we propose a novel Denoise Transformer for real-world image denoising, which is mainly constructed\nwith Context-aware Denoise Transformer (CADT) units and Secondary Noise Extractor (SNE) block. CADT\nis designed as a dual-branch structure, where the global branch uses a window-based Transformer encoder to\nextract the global information, while the local branch focuses on the extraction of local features with small\nreceptive field. By incorporating CADT as basic components, we build a hierarchical network to directly\nlearn the noise distribution information through residual learning and obtain the first stage denoised output.\nThen, we design SNE in low computation for secondary global noise extraction. Finally the blind spots\nare collected from the Denoise Transformer output and reconstructed, forming the final denoised image.\nExtensive experiments on the real-world SIDD benchmark achieve 50.62/0.990 for PSNR/SSIM, which is\ncompetitive with the current state-of-the-art method and only 0.17/0.001 lower. Visual comparisons on public\nsRGB, Raw-RGB and greyscale datasets prove that our proposed Denoise Transformer has a competitive\nperformance, especially on blurred textures and low-light images, without using additional knowledge, e.g.,\nnoise level or noise type, regarding the underlying unknown noise.\nINDEX TERMS Image denoising, self-supervised, real-world, transformer, dual-branch.\nI. INTRODUCTION\nDuring the image acquisition process of image sensors, CCD\nand CMOS, various noises are introduced due to the influence\nof sensor material properties, working environment, elec-\ntronic components and circuit structure.In addition, due to the\nimperfection of transmission media and recording equipment,\ndigital images are often attacked by various noises. There are\nbasically four types of common noise in images: Gaussian\nnoise, Poisson noise, multiplicative noise, and salt and pepper\nnoise. Image denoising is an inevitable step in image process-\ning, and its denoising effect has a huge impact on the subse-\nquent image processing process. Traditional image denoising\nalgorithms [1], [2] are slow and less robust. With the devel-\nopment of deep learning, image denoising algorithms have\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Gerardo Di Martino\n.\nmade great progress. Although some progress has been made\nin traditional methods [3], supervised denoising models [4],\n[5], [6], [7] have relatively better denoising effects on public\ndatasets.\nHowever, supervised image denoising requires noisy-clean\ndata pairs, which are very difficult to obtain in practical appli-\ncations. The most common approach is to add Additive White\nGaussian Noise (AWGN) or other simulated real-world noise\nto a clean image and artificially synthesise a noisy image to\nform noisy-clean pairs [4], [7], [8], [9], [10]. However, there\nis an unavoidable gap between the noise synthesised by noise\nmodelling and the real-world noise. Therefore, the denoising\nperformance of this synthesised type of denoising model will\nbe greatly reduced when denoising real-world images.\nUnder these circumstances, many self-supervised training\nmethods [11], [12], [13], [14], [15], [16] have emerged that\ndo not require clean images. Noise2Noise [16] trains the\n14340 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/VOLUME 11, 2023\nD. Zhang, F. Zhou: Self-Supervised Image Denoising for Real-World Images With Context-Aware Transformer\nmodel with only two noisy images and achieves denois-\ning performance comparable to other supervised algorithms.\nHowever, it requires two fully aligned noisy images, which\nare also difficult to obtain in practical applications. Then\nNoisier2Noise [17], NAC [18] are proposed to add the same\ntype of noise to the existing noisy image to form a noiser-\nnoise pair, which means that they need to know the exact\ntype of noise in the image as a prior. IDR [12] adopts an\niterative approach, taking the noisy images as inputs to the\nexisting denoising model trained with noiser-noise pairs and\ntreating the trained output as the next round of optimisation\ntargets to further refine the denoising model. In this way,\nthe denoising model is optimised by iteration, which can\neasily cause the final denoised image to be unduly over-\nsmoothened. Noise2Void [14] proposes a blind spot network\n(BSN) denoising method based on the assumption that pixel\nsignals in the image are spatial correlated, while noise signals\nare spatially independent with zero mean. In recent years,\nmany publications [14], [19], [20], [21] have proved that\nBSN is very effective in denoising synthetic noise. However,\nnoise in real-world is usually spatially continuous. In order to\nbreak the spatial connection of noise, AP-BSN [22] performs\n5 times pixel-shuffle downsampling (PD) on the input before\ntraining. What’s more, AP-BSN [22] adoptes centre-masked\nconvolution kernel and dilated convolution layer (DCL) to\nobtain the effect of blind spots during forward propagation.\nHowever, if the step of PD is too large, it will cause irreparable\ndamage to the spatial information, but if it is too small,\nit will not be able to break the spatial connection of the\nnoise [22].\nMost of state-of-the-art BSN denoising methods [13], [14],\n[15], [20], [21], [22] ultilize CNN-based networks, which are\nrestricted by the locality of the receptive field. Therefore,\nthey are difficult to denoise in the presence of high noise\nor noisy blurred textures, while repeated training leads to\nover-smoothing of images. In this case, we consider it is\ncomprehensive to combine local features with the global\ninformation extracted by Transformer. The addition of global\ninformation efficiently compensates for the defects of local\nreceptive field extraction features. In this paper, we propose\nthe Context-aware Denoise Transformer for self-supervised\ndenoising in two stages. For the first stage, we use a hier-\narchical CADTs to directly extract noise information, which\nwill be subtracted from the original image to obtain the first\ndenoised result. For the second stage, we use the SNE module\nto extract the global noise on the first denoised result and\nthe final denoised result can be obtained by residual learn-\ning. Extensive experiments demonstrate the effectiveness and\nsuperiority of our Denoise Transformer.\nThe main contributions of our work are as follows:\n1. We are the first to propose a Transformer-based network\nsuitable for self-supervised image denoising.\n2. We propose a new structural unit CADT that can fuse\nglobal information and local features.\n3. We add a secondary global noise extractor to realise two\nstage denoising.\n4. Our Denoise Transformer is competitive with current\nstate-of-the-art methods in self-supervised real-world image\ndenoising.\nII. RELATED WORK\nA. SUPERVISED IMAGE DENOISING BASED ON CNN\nZhang et al. [4] first proposed image denoising based on\ndeep learning. They proposed DnCNN trained with gener-\nated noisy-clean pairs by artificially adding AWGN to clean\nimages. Then, U-Net [23] became a more widely used denois-\ning baseline with a characteristic of multi-scale features.\nAfterwards, many publications proposed deep learning image\ndenoising methods based on adding AWGN to the noisy\nimage [5], [6], [8], [9], [10], [24], [25], [26], [27]. However,\nthere exists a significant gap between the artificially added\nAWGN and the real-world noise, and these methods are\nnot ideal for denoising in real-world applications. Several\npublications [28], [29] added Poisson noise corresponding to\nshot noise and Gaussian noise corresponding to read noise,\nto Raw-RGB images. After denoising in Raw-RGB space,\nthe final denoised result image was converted back to sRGB\nspace using ISP tools. For this denoising method, accurate\nnoise estimation and modelling are essential for success.\nAlthough the noise obtained by statistical modelling reduced\nthe gap between the synthetic noise and the real noise, the\ninjected noise was not real after all. This method can mit-\nigate the performance degradation of the denoising model\nin practical applications, but not enough to eliminate it. The\nmismatch between the training process and the application\ndeployment limits the practical application of this method.\nTo solve this problem perfectly, it is undoubtedly that a direct\nand effective method is to use the noisy-clean pairs in the\nreal world [30], [31]. However, generating such noisy-clean\npairs requires massive human labour and cost, which makes\nit impractical.\nB. SELF-SUPERVISED IMAGE DENOISING METHODS\nNoise2Noise [16] used two perfectly aligned noisy images\ntaken in the same scene as the input and the tar-\nget respectively. When training the denoising model, L2\nloss was used to minimise the difference between the\nnoise-noise pairs, making the model capable of denois-\ning. Then, Noise2Void [14], Noise2Self [13], Probabilistic\nNoise2Void [15], Neigbor2Neigbor [11], IDR [12], CVF-\nSID [32], Blind2Unblind [33], and AP-BSN [22] were pro-\nposed to use only noisy images for training, instead of\nnoisy-clean pairs. Neighbor2Neighbor [11] directly selected\ntwo adjacent pixels in 2*2 neighbourhood within a single\nRaw-RGB image at random to synthesise one sub-noise\nimage, and the remaining pixels formed another sub-noise\nimage. Two sub-noise images obtained in this way formed\na noise-noise pair, and one of them would participate in\nbackpropagation during training. However, training with only\nsub-images would inevitably lose some detail. Noise2Void\ntried to take the whole image into account. Noise2Void took\nVOLUME 11, 2023 14341\nD. Zhang, F. Zhou: Self-Supervised Image Denoising for Real-World Images With Context-Aware Transformer\nthe noisy image with the masked pixels as input, and the\ncomplete noisy image was regarded as the target. In this\nway, the masked pixels would never be seen during training,\nwhich could easily lead to details loss or over-smoothing\nin the image. Blind2Unblind [33] used a global masker to\ngenerate interleaved blind spots and collected blind spots\nafter model denoising to reconstruct the denoised output.\nThe use of global masker solves this problem perfectly,\nbut when the noise is large, the point-like mask used in\nBlind2Unblind [33] will be powerless. For spatially con-\nnected noise, AP-BSN [22] used pixel-shuffle downsampling\n(PD) to break the spatial connection of the noise, and then\nused the masked convolution kernel to extract features at the\nvery beginning of the model. Thus, the features of the masked\npatches were obtained from the surrounding pixels. The effect\nof blind spots could be achieved by combining DCLs with\nthe subsequent steps corresponding to the size of the convo-\nlution kernel. Finally, the current pixel can be restored from\nthe noise with the surrounding pixels through the inference\nprocess. However, if the noise is strongly spatially connected\nover a large area, PD will fail to break the spatial connection,\nand the pixels recovered from the surroundings should still be\nnoise, resulting in color shifts or textures loss in the desoised\nresults.\nC. TRANSFORMER DENOISING METHODS\nTransformers have achieved great success in the field of com-\nputer vision [34], [35], [36], [37], [38]. These methods are\nbased on window-based image blocks and utilize a multi-head\nattention mechanism, which has outstanding advantages in\ncapturing global image information. ViT [36] showed that\npure Transformers can be applied directly to sequences\nof image patches without overlap, and performed well on\nimage classification tasks. Liu et al. proposed the Swin\nTransformer [34], which had a hierarchical structure where\ncross-window connections were captured by a shifted win-\ndowing scheme. Chen et al. developed IPT [38], a pre-trained\nTransformer model for low-level computer vision tasks.\nLiang et al. extended Swin Transformer for image restoration\nand proposed SwinIR [35], which achieved state-of-the-art\nperformance on image super-resolution and denoising. To the\nbest of our knowledge, the existing transformer-based image\ndenoising methods are supervised image denoising [34], [35],\n[39]. Due to the global feature capturing capability of Trans-\nformer, the image features obtained based on BSNs are close\nto the identity transformation with revisible loss. Alterna-\ntively, the original pixel can be easily recovered from the sur-\nroundings, resulting in unsatisfactory denoising performance.\nInspired by [34] and [37], we propose a new CADT\nstructure aimed at self-supervised denoising for real-world\nimages. CADT contains two branches, a global branch and\na local branch. These two branches can effectively combine\nthe global information and the local textures. Taking CADT\nas the basic component, we construct a hierarchical CADT\nstructure to extract noise features. Once the noise information\nis obtained, it will be subtracted from the original image by\nresidual learning and we get the first stage denoised result.\nWhat’s more, we propose to add a Secondary Noise Extractor\n(SNE) module based on the first denoised output and to\nextract the global noise information on it. SNE is designed\nusing LN and MLP with low computational complexity. The\nfinal denoised result can be obtained by removing the SNE\noutput as a residual from the first denoised result.\nIII. METHOD\nA. CADT\nSince the pure Transformer will lead to poor self-supervised\nimage denoising effect, we propose a new dual-branch\nTransformer structure, Context-aware Denoise Transformer\n(CADT), as shown in Fig. 1. CADT includes a global feature\nextraction branch and a local feature extraction branch, and\nthe framework is shown in Fig. 1a, and the workflow is shown\nin Fig. 1b.\n1) GLOBAL TRANSFORMER ENCODER\nFor the global branch, we use a window-based multi-head\nTransformer encoder in Swin Transformer [34] to extract\nthe global information. The Transformer encoder contains a\nmulti-head self-attention (MSA) module and a multi-layer\nperception (MLP) module. A LayerNorm (LN) layer is\napplied before each MSA module and each MLP, and a\nresidual connection is applied after each module.\nAssuming that the token embedding input is E ∈\nRH×W ×C , the global self-attention computation can be for-\nmulated as the following Eq. (1):\nE = MSA(LN(E)) + E\nCADT global = MLP(LN(E)) + E (1)\nwhere LN denotes the LayerNorm, CADT global denotes the\nglobal feature extracted using Transformer Encoder.\n2) LOCAL FEATURE EXTRACTOR\nFor the local branch, we design a local feature extractor (LFE)\nto capture the local context CADT local , where the features of\nadjacent pixels and features of cross-channels are fused. The\nlocal feature extraction can be formulated as Eq. (2):\nCADT local = LFE(LN(E)) (2)\nFor the token embedding vector E, we normalize it by the\nLN layer and reshape it into a feature vector with shape of\nN ×H ×W ×C. Then we use a set of convolutions to extract\nand output a feature vector shaped in N × H × W × C/8 for\ndimensionality reduction. We then use another ordinary con-\nvolution to undertake the feature after dimension reduction,\nfollowed by two convolutions with deformable kernels [40] to\nfuse contextual details with large changes and cross-channel\nfeatures, which is a key design to preserve image textures\nduring denoising. Now we have two vectors whose shapes\nare N × H × W × C/4 and N × H × W × C/2 respectively.\nThe final shape is expanded from N × H × W × C/2 to\n14342 VOLUME 11, 2023\nD. Zhang, F. Zhou: Self-Supervised Image Denoising for Real-World Images With Context-Aware Transformer\nFIGURE 1. The representation of CADT. (a) shows that CADT is designed as a dual-branch structure. The left part shows the global\nbranch of the Transformer-based method for global featrue extraction, and the right part shows the local branch, which focuses on\nthe local feature extraction. (b) shows the workflow of CADT for feature extracting. In the global branch, token embedding is\nperformed on image patches, and the global information is extracted by the the multi-head attention mechanism. The local branch\nis composed of several convolutions with small receptive fields. In particular, the deformable convolutions are used to extract the\ndetails of the local information mutation, which helps to preserve denoised image details.\nFIGURE 2. The framework of denoise transformer.\nN × H × W × C by another convolution. Each convolution,\nexcept the first and the last, is followed by a LeakyReLU\nactivation layer to improve feature selection. We summarize\neach computational step into the formulas as Eq. (3):\nfreduction = Conv(LN(E))\nflocal = Leaky(D(Leaky(D(freduction)))\nfexpansion = Conv(flocal )\nCADT local = fexpansion (3)\nFinally, element-wise addition is used to fuse global and\nlocal information, effectively reducing additional parameters\ncompared to linear or convolutional layers. The local branch\nadds local features to the Transformer Encoder, changing the\npure transformer mapping relationship, and preserving local\nmutation information and texture detail.\nB. SNE BLOCK\nWe propose a secondary noise extraction module with low\ncomputation at the end of the Denoise Transformer. We want\nto further distinguish true noise from textures in a relatively\nclean image. And this is easier to realize on a relatively\nclean image than on a noisy image. Therefore, we design a\nlow computational global information extraction mechanism\ninspired by the multi-layer perception mechanism. First, the\ndenoised image is reshaped into N × C × (H × W ) and\nLayerNorm operation is performed to normalize the global\ninformation of each channel. Multi-layer perception is used\nto exchange information between channels. With residual\nlearning, the output of SNE would be subtracted from the first\nstage denoised image. That is to say, SNE learns the second\nnoise information mapping using the first stage denoised\nimage and the final image. Although SNE has a relatively\nsimple design, it achieves further noise reduction while pre-\nserving image detail. The module works as a plug-in and\ncan be portable if required. The SNE block is calculated as\nfollows Eq. (4):\nfSNE = MLP(LN(x)) (4)\nwhere, x denotes the denoised image. LN denotes the Layer-\nNorm, and MLP denotes the multi-layer perception.\nC. A CHOICE OF LOSS\nWith the baseline of Transformer, the local information\nextraction will be severely compromised if we follow [11]\nand [33] to pass the original images through the network\nin a non-gradient manner. When all the noisy images and\nthe corresponding denoised results are included in the total\nloss calculation, the global feature captured by Transformer\nencoder can easily recover the noise information of the orig-\ninal image. It seems that transformer can recover what it sees\nbefore, even though the pixels do not contribute to the gradi-\nent descent. To avoid this kind of the identity transformation,\nwe do not compute the invisible loss. During the training\nprocess, only the images after the global masker are used as\ntraining objects, and the L2 loss is performed using the blind\ninput and the corresponding output. The loss function can be\nformulated as the following Eq. (5):\nL2 = (Noisy − denoised)2 (5)\nD. OVERALL ARCHITECTURE OF DENOISE TRANSFORMER\nBased on the CADT structure and the SNE block introduced\nabove, we propose the Denoise Transformer framework,\nas shown in Fig. 2. The detailed architecture is shown in\nFig. 3.\nVOLUME 11, 2023 14343\nD. Zhang, F. Zhou: Self-Supervised Image Denoising for Real-World Images With Context-Aware Transformer\nFIGURE 3. The architecture of denoise transformer.\nFirstly, we perform PD on the original image to break\nthe spatial noise connection [22], and then we use the\nGlobal-aware Mask Mapper [33] to create the blind spot\neffect. The mask mapper performs global denoising on the\nblind spots. This mechanism constrains all pixels, promotes\ninformation exchange across whole masked regions, and\nimproves denoising performance. In this paper, the mask\nwidth is set to 4, which means that one noisy image will\nbe mapped into 16 blind sub-images. The blind sub-images\nare sent to the network as inputs. We do the feature dimen-\nsion mapping of the blind input with the patch embedding\ndimension through a convolution layer. And then the features\ngo through hierarchical CADTs. In this paper the number of\nCADT groups is set to 3, each of which contains 6 CADT\nunits for feature extraction, and a residual connection is\napplied after each CADT group. Next, the output of the\nhierarchical CADTs goes through another convolution layer,\nwhich maps the feature dimension back to the input dimen-\nsion, forming the residual noise. We subtract the noise pattern\nfrom the blind image and obtain a denoised image. The\ndenoised image then passes through SNE, and the residual\nsecondary noise features are extracted. Finally, the denoised\nimage is obtained by subtracting the SNE output and is\nsubjected to the reverse operation of the Global-aware Mask\nMapper to form the final denoised result. Ablation experi-\nments for several modules are presented in detail in Section\nIV-B.\nIV. EXPERIMENTS\nA. IMPLEMENTATION DETAILS\n1) TRAINING DETAILS\nAll models are trained with most of the same set-\ntings. Although our model aimed at real-world denoising,\nwe perform real-world deoising experiments on real-world\nFIGURE 4. Denoise Transformer test results for SIDD validation in\ndifferent epochs, with maximum PSNR and SSIM marked on the curve.\nRaw-RGB images and greyscale images, as well as synthetic\nsRGB with Gaussian noise, for a more comprehensive com-\nparison with state-of-the-art models. The detailed network\nis shown in Fig. 2 and Fig. 3, where the token embedding\ndimension C is set to 60 and all convolutions used in the\nnetwork have a kernel size of 3 × 3 and a stride of 1. The\nbatch size is set to 4 and the masker step is set to 4. We use\nthe Adam optimizer with a weight decay of 1e-8, an initial\nlearning rate of 0.0003 for synthetic denoising experiments in\nsRGB space and 0.0001 for real-world denoising experiments\nin Raw-RGB space and fluorescence microscopy (FM). The\nlearning rate is multiplied by 0.25 every 20 epochs, for\n100 training epochs. The images are randomly cropped into\n128 × 128 patches, masked with a step size of 4, and entered\ninto the model for training after random rotation within\n90◦ and random horizontal or vertical flipping. All models\nare trained on python3.8.0, pytorch1.12.0 and Nvidia Tesla\nT4 GPUs. Fig. 4 shows a plotted curve for our proposed\nmodel testing on SIDD validation in different epochs, proving\nthe effectiveness of our network structure.\n14344 VOLUME 11, 2023\nD. Zhang, F. Zhou: Self-Supervised Image Denoising for Real-World Images With Context-Aware Transformer\n2) DATASETS FOR SYNTHETIC DENOISING\nWe perform synthetic experiments with varied Gaussian\nnoise, considering a varied Gaussian noise distribution with\nσ ∈ [5, 50] in sRGB space are considered. We follow\nthe settings in [11], [33] and select 44,328 images from\nImageNet [41] validation set with resolutions ranging from\n256 × 256 to 512 × 512, as the clean set. For the test sets,\nwe follow [33] to repeat Kodak [2], BSD300-test [42] and\nSet14 [43] by 10, 3 and 20 times, respectively, and calculate\nthe average PSNR/SSIM.\n3) DATASETS FOR REAL-WORLD DENOISING\nFor real-world denoising in Raw-RGB space, we take the\npublic dataset of SIDD for experiments. The SIDD Medium\ndataset contains 320 pairs of noisy-clean images. We take\nonly Raw-RGB images in SIDD Medium [44] dataset as\ntrain set, and we take Raw-RGB images in SIDD Valida-\ntion and Benchmark datasets as the valid set and test set\nrespectively. For real-world denoising on FM with greyscale\nimages, we use FMDD [45] dataset for experiments. FMDD\ncontains real fluorescence microscopy images and 60,000\nnoisy images with different noise levels, which are obtained\nwith commercial confocal, two-photon, and widefield micro-\nscopes and representative biological samples such as cells,\nzebrafish, and mouse brain tissue. Following the setting\nin [33], we train on three datasets including Confocal Fish,\nConfocal Mice and Two Photon Mice, considering each set\nas both the training set and validation set at the same time\nand use the 50th view for visual testing.\n4) DETAILS OF EXPERIMENTS\nWe use PSNR and SSIM as evaluation metrics. PSNR/SSIM\non SIDD validation set is obtained by using python\ntoolkit based on clean images and denoised images. And\nPSNR/SSIM on SIDD benchmark is reported from the offi-\ncial website through submitting our denoised result.\nFor fair comparison, we follow the experimental set-\ntings of Blind2Unblind [33]. We compare our proposed\nDenoising Transformer method with two supervised denois-\ning methods (N2C [23] and N2N [16]) for the baseline.\nWe also quantitatively compare the Denosing Transformer\nmethod with a traditional approach (BM3D [1]) and sev-\neral self-supervised denoising algorithms (Laine19 [19],\nSelf2Self [46], DBSN [21], N2V [14], R2R [47], Nois-\nier2Noise [17], NBR2NBR [11] and Blind2Unblind [33]).\nFor the estimation of synthetic denoising, we use pre-\ntrained models provided by [19] for N2C, N2N, and\nLaine19 [19], keeping the same network architecture as [11]\nand [19]. And then, we use a multi-channel version of BM3D,\nnamely CBM3D [48], to denoise Gaussian noise using the σ\nnoise level estimation method in [49]. For Self2Self, Nois-\nier2Noise, DBSN, R2R, NBR2NBR and Blind2Unblind, the\nofficial implementations are used for experiments. Since Pos-\nsion noise can be transformed into Gaussian distribution [50],\nTABLE 1. Quantitative comparison of various branches and their\ncombinations performance on the SIDD validation. (1) BL: Baseline\nmodel. CADT: Dual branch composed of global and loacal extractors. SNE:\nSecondary noise extractor. (2)✓ denotes using the corresponding branch.\n(3) The best of PSNR/SSIM is highlighted inbold.\nwe will not show the denoising performance specifically for\nPossion noise.\nFor estimating of real-world denoising in Raw-RGB space,\nall the methods we use are based on their official implemen-\ntations, trained on the SIDD Medium dataset and well shown\nthrough ISP tools [44]. For BM3D, we use BM3D-CFA [51]\nfor Raw-RGB denoising. For the learning-based networks,\nwe split the single channel raw image into four sub-images\nfollowing the Bayer pattern. We gather four sub-images to\nform a four-channel image for denoising and then recon-\nstruct the Raw-RGB space from the denoised image. For the\nestimation of the real-world denoising on FM greyscale set,\nall compared methods are retrained using the corresponding\nauthors’ implementation.\nB. ABLATION STUDY\nIn order to quantitatively evaluate the performance of differ-\nent networks and training processes, several comprehensive\nablation experiments are conducted based on the metric of\nPSNR/SSIM of real-world Raw-RGB images from SIDD\nvalidation set.\n1) ABLATION ON THE NETWORK ARCHITECTURE\nFor the design of the network structure, we are inspired\nto introduce Transformer, which has achieved good perfor-\nmance in computer vision, as a baseline for self-supervised\ndenoising. Based on this, CADT component, SNE block, and\nthe overall Denoise Transformer are successively proposed.\nTherefore, in the ablation experiments, we set the following\nvariables:\n−Baseline. We use a tiny version of SwinIR [35], which\nconsists of pure transformer encoders.\n−CADT. This module extends the pure transformer based\non the baseline, which consists of a global branch and a local\nbranch, forming a dual-branch structure.\n−SNE. It is a secondary noise extractor. It is designed to be\nconnected after the first stage denoised result, and to extract\nthe global noise on the first stage denoised result to realize\nthe secondary denoising.\n−+CADT+SNE. The overall network architecture of our\nproposed Denoise Transformer.\nThe quantitative evaluation of the SIDD validation in\nRaw-RGB space is shown in Table 1. It can be seen that\ndirect use of pure Transformer method does not perform\nwell in self-supervised denoising. Our possible reason is\nthat over-extraction of global features prevents local fea-\ntures from being effectively denoised. Based on the original\nVOLUME 11, 2023 14345\nD. Zhang, F. Zhou: Self-Supervised Image Denoising for Real-World Images With Context-Aware Transformer\nFIGURE 5. Visual comparison of different self-supervised methods denoising for Raw-RGB images in SIDD validation.\nglobal branch, the addition of the local branch to CADT and\nSNE can both improve the self-supervised denoising ability.\nIn particular, CADTs show the surprising performance for\ndenoising. To show the performance of SNE, we also present\na set of visual intermediate results in Fig. 5. From Fig. 5 we\ncan see that SNE can slightly contribute to denoising, which\nis in line with our expectations. In the ideal case, there is little\nnoise present in the first stage denoised result and therefore\nwe design SNE in low computation. SNE is the icing on\nthe cake. By effectively combining it with global features\nand local information, the global noise estimation, denoising\nand local texture reconstruction can be better optimized and\ncomplement each other.\n2) ABLATION ON TRANSFORMER LOSS\nBased on the overall architecture of Denoise Transformer,\nwe conduct experiments on three different losses. Re-visible\nLoss1 means that the forward propagation and the total loss\ncalculation are consistent with NBR2NBR [11]. Re-visible\nLoss2 means that the forward propagation and the total loss\ncalculation are consistent with Blind2Unblind [33]. The com-\nparison are shown in Table 2.\nIt can be seen from Tabel 2 that when the Transformer\nbranch is used in the self-supervised denoising model,\nits powerful global extraction ability is enhanced under\nthe re-visible action, making the mapping result of the\nself-supervised denoising network onto the original noisy\nimage close to identity transformation. This makes it easier to\nrecover the noise information of the blind spot, which greatly\nreduces the denoising ability. In this paper we use L2 loss\ninstead of the re-visible loss for Denoise Transformer.\nC. COMPARISONS WITH STATE-OF-THE-ART\n1) RESULTS FOR SYNTHETIC DENOISING\nFor the widely used Gaussian noise, the quantitative com-\nparison in synthetic denoising is shown in Fig. 6. To bet-\nter evaluate the denosing ability of state-of-the-art models,\nwe have varied the Gaussian noise in a wide range σ ∈\n[5, 50]. Fig. 6 shows that our method outperforms the tradi-\ntional denoising method BM3D and several self-supervised\nTABLE 2. Quantitative comparison of different losses. The best of\nPSNR/SSIM is highlighted inbold.\nFIGURE 6. The curve of quantitative comparison of several self denoising\nmethods with Gaussian noiseσ ∈ [5, 50].\ndenoising methods, such as Self2Self, Laine19-mu, DBSN\nand NBR2NBR. Fig. 7 shows the visual comparison of the\ndenoised images injected with Gaussian noise with noise\nlevel σ = 25. Denoise Transformer achieves competitive\nvisual quality among traditional and self-supervised CNN\ndenoisers. What’s more, its performance is close to the SOTA\nof Blind2Unblind [34] and even surpasses it on single images\nwith low saturated textures, proving the advantage of pre-\nserving image textures of our proposed method. However,\nFig. 7 also shows that Denoise Transformer does not perform\nas well as Blind2Unblind when processing images with high\nsaturation. The possible reason is that in the high saturation\nregion, the extracted global features are at a high level while\nthe local details have relatively small changes. And the small\nchanges in texture would be considered as noise and removed.\n2) RESULTS FOR REAL-WORLD DENOISING\nFor real-world denoising, Table 3 quantitatively compares the\ndenoising performance of different methods on Raw-RGB\nspace of SIDD benchmark and validation, and Table 4 quanti-\ntatively compares the denoising performance on FM dataset.\nOur proposed Denoise Transformer outperforms the tradi-\ntional method BM3D and Laine19 to a large extent, and it\n14346 VOLUME 11, 2023\nD. Zhang, F. Zhou: Self-Supervised Image Denoising for Real-World Images With Context-Aware Transformer\nFIGURE 7. Visual comparison of different methods denoising for images in sRGB space with Gaussion noiseσ = 25.\nTABLE 3. Quantitative comparison of denoising methods on synthetic\ndatasets in Raw-RGB space of SIDD benchmark. DT denotes our Denoised\nTransformer. The highest PSNR/SSIM is highlighted inbold, while the\nsecond is underlined.\nFIGURE 8. Visual comparison of different models denoising for Raw-RGB\nimages in SIDD benchmark. PSNR/SSIM can not be calculated since clean\nimages are not available. The images are well shown from Raw-RGB to\nsRGB using the official ISP tool [44].\neven outperforms Neighbor2Neighbor by 0.15dB and 0.10dB\nfor SIDD benchmark and validation, and it outperforms\nNBR2NBR more on FMDD dataset.\nAlthough the method proposed in this paper does not yet\noutperform the SOTA Blind2Unblind in Raw-RGB and FM,\nTABLE 4. Quantitative comparison of denoising results on synthetic\ndatasets in FMDD. DT denotes our Denoised Transformer. The highest\nPSNR/SSIM among unsupervised denoising methods is highlighted in\nbold, and the second is underlined.\nFIGURE 9. Visual comparison of different methods denoising for FM\nimages.\nDenoise Transformer has demonstrated its potential to handle\ncomplex noise patterns in self-supervised denoising, which is\ncomparable to the performance of SOTA. Fig. 5, Fig. 8 and\nFig. 9 show the visual comparison of SIDD validation, SIDD\nbenchmark and FMDD. There are color shifts and texture loss\nin NBR2NBR and Blind2Unblind, especially for the top row\nimages in Fig. 5. Compared to other models, our model has\nsimilar performance in general scene images, but has the best\ndenoising performance in low saturated textures and low-light\nscenes.\n3) COMPUTATIONAL COMPLEXITY COMPARISON\nWe compute the model size and the FLOPs of state-of-the-\nart self-supervised image denoising methods that have sim-\nilar preprocessing and inference flows, as in the following\nTable 5.\nVOLUME 11, 2023 14347\nD. Zhang, F. Zhou: Self-Supervised Image Denoising for Real-World Images With Context-Aware Transformer\nTABLE 5. Computational complexity comparison of several methods.\nTransformer is known to be computationally intensive.\nWe have made some reductions in the baseline, but it is\nstill more computationally intensive than other models. Given\nthe huge potential of Transformer-based methods in self-\nsupervise denoising, we will try to find a more effective\nway of combining Transformers with CNN basic modules.\nAt this stage, we can replace some of the CADT units with\npure convolution modules to reduce the number of Trans-\nformers, which may reduce the amount of computation while\nmaintaining good denoising performance. We believe that our\nfuture research on how to reduce the computational complex-\nity of the model is of great importance in order to successfully\nland in the industry.\nV. CONCLUSION\nIn this paper, in order to improve the local limitations of\nCNN networks in self-supervised image denoising, we extend\nthe Transformer-based method, inspired by its successful\napplication in computer vision. We propose a basic network,\ncalled Denoise Transformer, which is composed of CADTs\nand SNE to realize self-supervised denoising in two stages.\nEach CADT contains a global feature extraction branch and\na local feature extraction branch, and the output of the CADTs\nis our desired noise information. When we subtract the noise\nfrom the input through residual learning, we get the denoised\noutput of the first stage. We then design a low computa-\ntional complexity SNE to be connected after the denoised\noutput to extract its global noise for secondary denoising.\nSufficient experiments indicate that abandoning the re-visible\nbranch is suitable for our proposed Denoise Transformer,\nand both CADTs and SNE contribute a better performance\nthan traditional methods and most self-supervised training\nmethods. Our method demonstrates the strong denoising\nability of Transformer under the complex noise pattern and\nits potential for self-supervised image denoising. We are\nthe first to propose a Transformer-based method for self-\nsupervised image denoising, which fills the gap in the cor-\nresponding field and achieves state-of-the-art competitive\nperformance on the public dataset, especially in denoising\nthe noisy images with low-saturated textures and low-light\nscenes. Although we extend the Transformer based method\nin new application of self-supervised image denoising, its\nlarge computational load is difficult as well as the denosing of\nhigh-saturated region. We attribute them to the effectiveness\nof the combination of Transformer and CNN. In the future,\nwe will investigate the effectiveness of possible parameter\nreduction schemes and the balance between global and the\nlocal features. Given Transformer’s strong performance in\ncomputer vision, we believe that it has a greater hidden poten-\ntial for self-supervised image denoising, and hope that the\nmethod proposed in this paper can provide more inspiration\nto researchers.\nREFERENCES\n[1] K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian, ‘‘Image denoising by\nsparse 3-D transform-domain collaborative filtering,’’ IEEE Trans. Image\nProcess., vol. 16, no. 8, pp. 2080–2095, Aug. 2007.\n[2] S. Gu, L. Zhang, W. Zuo, and X. Feng, ‘‘Weighted nuclear norm minimiza-\ntion with application to image denoising,’’ in Proc. IEEE Conf. Comput.\nVis. Pattern Recognit., Jun. 2014, pp. 2862–2869.\n[3] A. Khmag, A. R. Ramli, and N. Kamarudin, ‘‘Clustering-based natural\nimage denoising using dictionary learning approach in wavelet domain,’’\nSoft Comput., vol. 23, no. 17, pp. 8013–8027, 2019.\n[4] K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang, ‘‘Beyond a Gaussian\nDenoiser: Residual learning of deep CNN for image denoising,’’ IEEE\nTrans. Image Process., vol. 26, no. 7, pp. 3142–3155, Jul. 2017.\n[5] S. Anwar and N. Barnes, ‘‘Real image denoising with feature atten-\ntion,’’ in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2019,\npp. 3155–3164.\n[6] M. Chang, Q. Li, H. Feng, and Z. Xu, ‘‘Spatial-adaptive network for single\nimage denoising,’’ in Proc. Eur. Conf. Comput. Vis.Cham, Switzerland:\nSpringer, Aug. 2020, pp. 171–187.\n[7] S. Guo, Z. Yan, K. Zhang, W. Zuo, and L. Zhang, ‘‘Toward convolutional\nblind denoising of real photographs,’’ in Proc. IEEE/CVF Conf. Comput.\nVis. Pattern Recognit. (CVPR), Jun. 2019, pp. 1712–1722.\n[8] Y. Kim, J. W. Soh, G. Y. Park, and N. I. Cho, ‘‘Transfer learning from\nsynthetic to real-noise denoising with adaptive instance normalization,’’ in\nProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2020,\npp. 3482–3492.\n[9] P. Liu, H. Zhang, W. Lian, and W. Zuo, ‘‘Multi-level wavelet convolutional\nneural networks,’’ IEEE Access, vol. 7, pp. 74973–74985, 2019.\n[10] B. Park, S. Yu, and J. Jeong, ‘‘Densely connected hierarchical network\nfor image denoising,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern\nRecognit. Workshops (CVPRW), Jun. 2019, pp. 1–10.\n[11] T. Huang, S. Li, X. Jia, H. Lu, and J. Liu, ‘‘Neighbor2Neighbor: Self-\nsupervised denoising from single noisy images,’’ in Proc. IEEE/CVF Conf.\nComput. Vis. Pattern Recognit. (CVPR), Jun. 2021, pp. 14781–14790.\n[12] Y. Zhang, D. Li, K. L. Law, X. Wang, H. Qin, and H. Li, ‘‘IDR:\nSelf-supervised image denoising via iterative data refinement,’’ in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2022,\npp. 2098–2107.\n[13] J. Batson and L. Royer, ‘‘Noise2Self: Blind denoising by self-supervision,’’\nin Proc. Int. Conf. Mach. Learn., 2019, pp. 524–533.\n[14] A. Krull, T.-O. Buchholz, and F. Jug, ‘‘Noise2Void—Learning denoising\nfrom single noisy images,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern\nRecognit. (CVPR), Jun. 2019, pp. 2129–2137.\n[15] A. Krull, T. Vičar, M. Prakash, M. Lalit, and F. Jug, ‘‘Probabilistic\nNoise2 Void: Unsupervised content-aware denoising,’’ Frontiers Comput.\nSci., vol. 2, p. 5, Feb. 2020.\n[16] J. Lehtinen, J. Munkberg, J. Hasselgren, S. Laine, T. Karras, M. Aittala, and\nT. Aila, ‘‘Noise2Noise: Learning image restoration without clean data,’’\n2018, arXiv:1803.04189.\n[17] N. Moran, D. Schmidt, Y. Zhong, and P. Coady, ‘‘Noisier2Noise: Learning\nto denoise from unpaired noisy data,’’ in Proc. IEEE/CVF Conf. Comput.\nVis. Pattern Recognit. (CVPR), Jun. 2020, pp. 12064–12072.\n[18] J. Xu, Y. Huang, M.-M. Cheng, L. Liu, F. Zhu, Z. Xu, and L. Shao, ‘‘Noisy-\nas-clean: Learning self-supervised denoising from corrupted image,’’ IEEE\nTrans. Image Process., vol. 29, pp. 9316–9329, 2020.\n[19] S. Laine, T. Karras, J. Lehtinen, and T. Aila, ‘‘High-quality self-supervised\ndeep image denoising,’’ in Proc. Adv. Neural Inf. Process. Syst., vol. 32,\n2019, pp. 1–11.\n[20] D. Honzátko, S. A. Bigdeli, E. Turetken, and L. A. Dunbar, ‘‘Efficient\nblind-spot neural network architecture for image denoising,’’ in Proc. 7th\nSwiss Conf. Data Sci. (SDS), Jun. 2020, pp. 59–60.\n[21] X. Wu, M. Liu, Y. Cao, D. Ren, and W. Zuo, ‘‘Unpaired learning of deep\nimage denoising,’’ in Proc. Eur. Conf. Comput. Vis.Cham, Switzerland:\nSpringer, 2020, pp. 352–368.\n[22] W. Lee, S. Son, and K. M. Lee, ‘‘AP-BSN: Self-supervised denoising\nfor real-world images via asymmetric PD and blind-spot network,’’ in\nProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2022,\npp. 17725–17734.\n14348 VOLUME 11, 2023\nD. Zhang, F. Zhou: Self-Supervised Image Denoising for Real-World Images With Context-Aware Transformer\n[23] O. Ronneberger, P. Fischer, and T. Brox, ‘‘U-Net: Convolutional networks\nfor biomedical image segmentation,’’ in Proc. Int. Conf. Med. Image\nComput. Comput.-Assist. Intervent.Cham, Switzerland: Springer, 2015,\npp. 234–241.\n[24] S. Gu, Y. Li, L. Van Gool, and R. Timofte, ‘‘Self-guided network for fast\nimage denoising,’’ in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV),\nOct. 2019, pp. 2511–2520.\n[25] F. Fang, J. Li, Y. Yuan, T. Zeng, and G. Zhang, ‘‘Multilevel edge features\nguided network for image denoising,’’ IEEE Trans. Neural Netw. Learn.\nSyst., vol. 32, no. 9, pp. 3956–3970, Sep. 2020.\n[26] R. Lan, H. Zou, C. Pang, Y. Zhong, Z. Liu, and X. Luo, ‘‘Image denoising\nvia deep residual convolutional neural networks,’’ Signal, Image Video\nProcess., vol. 15, no. 1, pp. 1–8, Feb. 2021.\n[27] A. Khmag, ‘‘Additive Gaussian noise removal based on generative adver-\nsarial network model and semi-soft thresholding approach,’’ Multimedia\nTools Appl., vol. 82, pp. 1–21, Aug. 2022.\n[28] T. Brooks, B. Mildenhall, T. Xue, J. Chen, D. Sharlet, and J. T. Barron,\n‘‘Unprocessing images for learned raw denoising,’’ in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2019,\npp. 11036–11045.\n[29] B. Mildenhall, J. T. Barron, J. Chen, D. Sharlet, R. Ng, and R. Carroll,\n‘‘Burst denoising with kernel prediction networks,’’ in Proc. IEEE/CVF\nConf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 2502–2510.\n[30] Z. Yue, Q. Zhao, L. Zhang, and D. Meng, ‘‘Dual adversarial network:\nToward real-world noise removal and noise generation,’’ inProc. Eur. Conf.\nComput. Vis.Cham, Switzerland: Springer, 2020, pp. 41–58.\n[31] S. M. A. Sharif, R. A. Naqvi, and M. Biswas, ‘‘Learning medical image\ndenoising with deep dynamic residual attention network,’’ Mathematics,\nvol. 8, no. 12, p. 2192, Dec. 2020.\n[32] R. Neshatavar, M. Yavartanoo, S. Son, and K. M. Lee, ‘‘CVF-SID: Cyclic\nmulti-variate function for self-supervised image denoising by disentan-\ngling noise from image,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern\nRecognit. (CVPR), Jun. 2022, pp. 17583–17591.\n[33] Z. Wang, J. Liu, G. Li, and H. Han, ‘‘Blind2Unblind: Self-supervised\nimage denoising with visible blind spots,’’ in Proc. IEEE/CVF Conf.\nComput. Vis. Pattern Recognit. (CVPR), Jun. 2022, pp. 2027–2036.\n[34] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo,\n‘‘Swin Transformer: Hierarchical vision transformer using shifted win-\ndows,’’ in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2021,\npp. 10012–10022.\n[35] J. Liang, J. Cao, G. Sun, K. Zhang, L. Van Gool, and R. Timofte, ‘‘SwinIR:\nImage restoration using swin transformer,’’ in Proc. IEEE/CVF Int. Conf.\nComput. Vis., Oct. 2021, pp. 1833–1844.\n[36] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszko-\nreit, and N. Houlsby, ‘‘An image is worth 16 × 16 words: Transformers for\nimage recognition at scale,’’ 2020, arXiv:2010.11929.\n[37] J. Liang, J. Cao, G. Sun, K. Zhang, L. Van Gool, and R. Timofte, ‘‘Ghost-\nfree high dynamic range imaging with context-aware transformer,’’ in\nProc. Eur. Conf. Comput. Vis., Springer, Cham, 2022, pp. 344–360.\n[38] H. Chen, Y. Wang, T. Guo, C. Xu, Y. Deng, Z. Liu, S. Ma, C. Xu,\nC. Xu, and W. Gao, ‘‘Pre-trained image processing transformer,’’ in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2021,\npp. 12299–12310.\n[39] A. Luthra, H. Sulakhe, T. Mittal, A. Iyer, and S. Yadav, ‘‘Eformer: Edge\nenhancement based transformer for medical image denoising,’’ 2021,\narXiv:2109.08044.\n[40] X. Xu, M. Li, and W. Sun, ‘‘Learning deformable kernels for image and\nvideo denoising,’’ 2019, arXiv:1904.06903.\n[41] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ‘‘ImageNet:\nA large-scale hierarchical image database,’’ in Proc. IEEE Conf. Comput.\nVis. Pattern Recognit., Jun. 2009, pp. 248–255.\n[42] D. Martin, C. Fowlkes, D. Tal, and J. Malik, ‘‘A database of human\nsegmented natural images and its application to evaluating segmentation\nalgorithms and measuring ecological statistics,’’ in Proc. 8th IEEE Int.\nConf. Comput. Vis. (ICCV), vol. 2, Jun. 2001, pp. 416–423.\n[43] R. Zeyde, M. Elad, and M. Protter, ‘‘On single image scale-up using\nsparse-representations,’’ in Proc. Int. Conf. Curves Surf.Berlin, Germany:\nSpringer, 2010, pp. 711–730.\n[44] A. Abdelhamed, S. Lin, and M. S. Brown, ‘‘A high-quality denoising\ndataset for smartphone cameras,’’ in Proc. IEEE/CVF Conf. Comput. Vis.\nPattern Recognit., Jun. 2018, pp. 1692–1700.\n[45] Y. Zhang, Y. Zhu, E. Nichols, Q. Wang, S. Zhang, C. Smith, and\nS. Howard, ‘‘A Poisson–Gaussian denoising dataset with real fluorescence\nmicroscopy images,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern\nRecognit. (CVPR), Jun. 2019, pp. 11710–11718.\n[46] Y. Quan, M. Chen, T. Pang, and H. Ji, ‘‘Self2Self with dropout: Learning\nself-supervised denoising from single image,’’ in Proc. IEEE/CVF Conf.\nComput. Vis. Pattern Recognit. (CVPR), Jun. 2020, pp. 1890–1898.\n[47] T. Pang, H. Zheng, Y. Quan, and H. Ji, ‘‘Recorrupted-to-recorrupted:\nUnsupervised deep learning for image denoising,’’ in Proc. IEEE/CVF\nConf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2021, pp. 2043–2052.\n[48] K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian, ‘‘Color image denois-\ning via sparse 3D collaborative filtering with grouping constraint in\nluminance-chrominance space,’’ in Proc. IEEE Int. Conf. Image Process.,\nSep. 2007, pp. 1–4.\n[49] G. Chen, F. Zhu, and P. A. Heng, ‘‘An efficient statistical method for image\nnoise level estimation,’’ in Proc. IEEE Int. Conf. Comput. Vis. (ICCV),\nDec. 2015, pp. 477–485.\n[50] M. Makitalo and A. Foi, ‘‘Optimal inversion of the Anscombe transforma-\ntion in low-count Poisson image denoising,’’ IEEE Trans. Image Process.,\nvol. 20, no. 1, pp. 99–109, Jan. 2011.\n[51] A. Danielyan, M. Vehvilainen, A. Foi, V. Katkovnik, and K. Egiazarian,\n‘‘Cross-color BM3D filtering of noisy raw data,’’ in Proc. Int. Workshop\nLocal Non-Local Approximation Image Process., Aug. 2009, pp. 125–129.\nDAN ZHANG was born in Shanxian, Shandong,\nChina, in 1987. She received the B.S. degree\nin communication engineering from the North-\nwest University of Nationalities, Lanzhou, China,\nin 2012, and the master’s degree in communication\nand information system from Shandong Univer-\nsity, Jinan, China, in 2015. She has participated\nin many projects and activities as the commis-\nsary in charge of studies. She is currently working\ntoward image processing related algorithms that\nare mostly deployed in security scene. Her research interests include image\nlow level algorithms, such as ISP, learning-based image denoising, learning-\nbased image high dynamic range, learning-based ISP, and image high level\nalgorithms, such as object detection and tracking, gesture estimation, image\nsegmentation, face recognition, and iris recognition.\nFANGFANG ZHOU was born in Wanzhou,\nChongqing, China, in 1991. She received the\nbachelor’s degree from Northeastern University,\nin 2017, and the master’s degree in radio physics\nfrom East China Normal University, in 2020. She\nhas been working on image processing algorithms\nsince postgraduate studies, such as object detec-\ntion, image segmentation, classification, image\ndenoising, and image HDR. From 2017 to 2020,\nshe mainly engaged in medical image processing,\nsuch as pancreas segmentation in abdominal CT, pancreas segmentation in\nbrain MRI images, and quality control in chest CT. From 2020 to 2021,\nshe mainly engaged in skin disease detection, classification, and retrieval\nin natural images, and musical note detection, text correction, and OCR in\nnatural images. Since 2021, she has been mainly engaged in image denoising\nand HDR.\nVOLUME 11, 2023 14349",
  "topic": "Artificial intelligence",
  "concepts": [
    {
      "name": "Artificial intelligence",
      "score": 0.76594078540802
    },
    {
      "name": "Computer science",
      "score": 0.7331226468086243
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5228405594825745
    },
    {
      "name": "Noise reduction",
      "score": 0.5017459392547607
    },
    {
      "name": "Computer vision",
      "score": 0.49351704120635986
    },
    {
      "name": "Feature extraction",
      "score": 0.47377538681030273
    },
    {
      "name": "Transformer",
      "score": 0.4251227080821991
    },
    {
      "name": "Engineering",
      "score": 0.08828186988830566
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": []
}