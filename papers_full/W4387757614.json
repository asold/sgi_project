{
  "title": "An Anchor-Free Method Based on Transformers and Adaptive Features for Arbitrarily Oriented Ship Detection in SAR Images",
  "url": "https://openalex.org/W4387757614",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5042449139",
      "name": "Bingji Chen",
      "affiliations": [
        "Aerospace Information Research Institute",
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5018856414",
      "name": "Chunrui Yu",
      "affiliations": [
        "China Telecom (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5041827776",
      "name": "Shuang Zhao",
      "affiliations": [
        "Aerospace Information Research Institute",
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5101459832",
      "name": "Hongjun Song",
      "affiliations": [
        "Aerospace Information Research Institute",
        "Chinese Academy of Sciences"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2079299474",
    "https://openalex.org/W2071192366",
    "https://openalex.org/W2128513227",
    "https://openalex.org/W2922412082",
    "https://openalex.org/W3135436966",
    "https://openalex.org/W4281762445",
    "https://openalex.org/W6605852207",
    "https://openalex.org/W2076063977",
    "https://openalex.org/W2890700095",
    "https://openalex.org/W2897902589",
    "https://openalex.org/W2564429410",
    "https://openalex.org/W3162000904",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W2774244034",
    "https://openalex.org/W6694401350",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2904480641",
    "https://openalex.org/W4285338346",
    "https://openalex.org/W3000097815",
    "https://openalex.org/W4313886592",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W4322707254",
    "https://openalex.org/W3108927275",
    "https://openalex.org/W2193145675",
    "https://openalex.org/W6850210107",
    "https://openalex.org/W4295916812",
    "https://openalex.org/W4367276728",
    "https://openalex.org/W4365801661",
    "https://openalex.org/W6798838024",
    "https://openalex.org/W4365448756",
    "https://openalex.org/W4362519158",
    "https://openalex.org/W2963669006",
    "https://openalex.org/W6810653034",
    "https://openalex.org/W6795475546",
    "https://openalex.org/W4379984088",
    "https://openalex.org/W4378194596",
    "https://openalex.org/W4221044012",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4283809070",
    "https://openalex.org/W4306964296",
    "https://openalex.org/W4377085399",
    "https://openalex.org/W4311080910",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W3175515048",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3034502973",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W3121020412",
    "https://openalex.org/W2601564443",
    "https://openalex.org/W2966926453",
    "https://openalex.org/W3092663126",
    "https://openalex.org/W2982363097",
    "https://openalex.org/W6639331287",
    "https://openalex.org/W3200733355",
    "https://openalex.org/W6874182738",
    "https://openalex.org/W4225125774",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2963857746",
    "https://openalex.org/W3034971973",
    "https://openalex.org/W4214648418",
    "https://openalex.org/W639708223",
    "https://openalex.org/W3170033848",
    "https://openalex.org/W3174873843",
    "https://openalex.org/W3136761610",
    "https://openalex.org/W4380080321",
    "https://openalex.org/W1866072925",
    "https://openalex.org/W3184998487",
    "https://openalex.org/W4225871896",
    "https://openalex.org/W142575998",
    "https://openalex.org/W4404196705",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3184439416",
    "https://openalex.org/W3106250896",
    "https://openalex.org/W4404090460"
  ],
  "abstract": "Ship detection is a crucial application of synthetic aperture radar (SAR). Most recent studies have relied on convolutional neural networks (CNNs). CNNs tend to struggle in gathering adequate contextual information through local receptive fields and are also susceptible to noise. Inshore scenes in SAR images are plagued by substantial background noise, so achieving high-accuracy ship detection of arbitrary orientations within complex scenes remains an ongoing challenge when relying solely on CNNs. To address the above challenges, this article presents an anchor-free method based on transformers and adaptive features, namely, SAD-Det, which can detect rotationally invariant ship targets with high average precision in SAR images. Specifically, a transformer-based backbone network called the ship spatial pooling pyramid vision transformer is proposed to enhance the long-range dependencies and obtain sufficient contextual information for ships in SAR images. In addition, a neck network called the adaptive feature pyramid network is designed to enhance the ability of ship feature adaptation by adding fusion factors to feature layers in SAR images. Finally, a head network called the deformable head is constructed to make the network more adaptable to the characteristics of ships by adaptively detecting the spatial sampling positions of the targets in SAR images. The effectiveness of the proposed method is verified by experiments on two publicly available datasets, i.e., SAR ship detection dataset and rotated ship detection dataset in SAR images. Compared with other arbitrarily oriented object detection methods, the proposed method achieves state-of-the-art detection performance.",
  "full_text": "IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 1\nAn Anchor-Free Method Based on Transformers\nand Adaptive Features for Arbitrarily Oriented Ship\nDetection in SAR Images\nBingji Chen\n˙ID , Chunrui Yu, Shuang Zhao\n˙ID , Hongjun Song\nAbstract—Ship detection is a crucial application of synthetic\naperture radar (SAR). Most recent studies have relied on con-\nvolutional neural networks (CNNs). CNNs tend to struggle in\ngathering adequate contextual information through local recep-\ntive fields and are also susceptible to noise. Inshore scenes in SAR\nimages are plagued by substantial background noise, so achieving\nhigh-accuracy ship detection of arbitrary orientations within\ncomplex scenes remains an ongoing challenge when relying solely\non CNNs. To address the above challenges, this paper presents an\nanchor-free method based on transformers and adaptive features,\nnamely, SAD-Det, which can detect rotationally invariant ship\ntargets with high average precision in SAR images. Specifically,\na transformer-based backbone network called the ship spatial\npooling pyramid vision transformer (SSP-PVT) is proposed to\nenhance the long-range dependencies and obtain sufficient con-\ntextual information for ships in SAR images. In addition, a neck\nnetwork called the adaptive feature pyramid network (AFPN)\nis designed to enhance the ability of ship feature adaptation by\nadding fusion factors to feature layers in SAR images. Finally, a\nhead network called the deformable head (DeHead) is constructed\nto make the network more adaptable to the characteristics of\nships by adaptively detecting the spatial sampling positions of\nthe targets in SAR images. The effectiveness of the proposed\nmethod is verified by experiments on two publicly available\ndatasets, i.e., SAR ship detection dataset (SSDD) and rotated ship\ndetection dataset in SAR images (RSDD-SAR). Compared with\nother arbitrarily oriented object detection methods, the proposed\nmethod achieves state-of-the-art detection performance.\nIndex Terms—Ship detection, transformer, adaptive features,\narbitrarily oriented detector, anchor-free, synthetic aperture\nradar.\nI. I NTRODUCTION\nS\nYNTHETIC aperture radar (SAR) is a type of active\nmicrowave imaging radar. Unlike optical images, SAR\nis not affected by daylight or weather and can achieve all-\nday and all-weather observations of the Earth. It also has\ngood information acquisition capability in complex situations\n[1]. Due to its excellent performance, SAR has been widely\n(Corresponding author: Hongjun Song.)\nBingji Chen and Shuang Zhao are with the Department of Space Mi-\ncrowave Remote Sensing System, Aerospace Information Research Insti-\ntute, Chinese Academy of Sciences, Beijing 100190, China, and with the\nSchool of Electronic, Electrical and Communication Engineering, University\nof Chinese Academy of Sciences, Beijing 101408, China (e-mail: chen-\nbingji21@mails.ucas.ac.cn; zhaoshuang18@mails.ucas.ac.cn).\nChunrui Yu is with the Beijing Institute of Tracking and Telecommunica-\ntions Technology, Beijing 100094, China (e-mail: ycrzxc@163.com).\nHongjun Song is with the Department of Space Microwave Remote Sensing\nSystem, Aerospace Information Research Institute, Chinese Academy of\nSciences, Beijing 100190, China (e-mail: songhj@aircas.ac.cn).\ndeveloped and utilized in various fields, including ocean mon-\nitoring [2], topographic mapping [3], agricultural monitoring\n[4], and disaster detection [5]. Among them, research on ship\nobject detection based on SAR ocean images is an important\napplication of SAR [6] and is of great significance in the fields\nof maritime supervision, fishery management, disaster rescue,\netc.\nThe main characteristic of traditional ship detection methods\nin SAR images is manual feature extraction, which typically\nconsists of several stages: land masking, preprocessing, pre-\nscreening, and discrimination [7]. These traditional detection\nmethods mainly include methods based on a constant false\nalarm rate (CFAR) [8], methods based on a global threshold\n[9], methods based on visual saliency [10], methods based\non wavelet transform [11], and methods based on polarization\ninformation [12]. Among them, the most widely used method\nis the CFAR method based on the sea clutter statistical distri-\nbution. Its basic idea is to statistically model the sea clutter\naround the pixels to be detected by sliding the window under\nthe preset false alarm rate to adaptively determine the detection\nthreshold and then compare the gray value of the pixel to be\ndetected with the detection threshold to perform ship detection.\nHowever, these traditional methods, including CFAR, require\nmanual design and the extraction of ship features, resulting in\ncumbersome algorithmic processes and low robustness. These\ndrawbacks impede the advancement of detection performance,\nrendering them inadequate for contemporary ship detection\ntasks in SAR images.\nIn recent years, deep learning [13] has exhibited exceptional\nperformance in various domains, such as image classification,\nobject detection, semantic segmentation, and instance segmen-\ntation. Li et al. [14] released the first public SAR ship de-\ntection dataset, named SSDD, which enables end-to-end ship\ndetection in SAR images based on deep learning. Currently,\nseveral neural network architectures are employed for image\nprocessing using deep learning, including convolutional neural\nnetworks (CNNs) [15] and transformers [16].\nCNN-based object detection methods have been widely\napplied to ship detection in SAR images due to their excel-\nlent performance. In terms of two-stage detectors, multiple\nresearchers have combined various improved modules and\nattention mechanisms based on Faster R-CNN [17], Cascade\nR-CNN [18], and Mask R-CNN [19] to improve the perfor-\nmance of ship detection in SAR images. In terms of one-\nstage detectors, Lin et al. [20] designed a feature enhancement\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3325573\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 2\npyramid and shallow feature reconstruction network based on\nRetinanet [21] to mitigate the adverse effects of scattering\nnoise in SAR images and improve detection accuracy for\nsmall ships. Zhang et al. [22] introduced a frequency attention\nmodule based on YOLO V5 [23], which can adaptively process\nthe frequency domain information of SAR images to suppress\nnoise such as sea clutter. Zhang et al. [24] proposed a network\nstructure based on SSD [25], which takes raw SAR images\nand saliency maps as input and fuses their features in order\nto reduce the computational complexity and the number of\nnetwork parameters. The aforementioned detection algorithms\nare all based on the horizontal bounding box (HBB), which\ncan effectively detect ships in SAR images. However, unlike\nnatural scene images, remote sensing images, including SAR\nimages, are usually obtained from a bird’s-eye view and have\ndistinctive characteristics [26]. For example, the ship features\nhave arbitrary orientations, dense arrangements, scale varia-\ntion, complex backgrounds, etc. Especially in ports, where a\nlarge number of ships are densely arranged and have a large\naspect ratio, the HBB can introduce substantial interference\nfrom the background area and adjacent ships.\nThe oriented bounding box (OBB) is well suited for ship\ndetection in SAR images because it preserves the direction-\nality of the targets in the image, leading to more accurate\npositioning. For example, Zhao et al. [27] proposed a single-\nstage detection method that efficiently detects ships in any\norientation in SAR images through multiscale feature fusion\nand calibration. Guo et al. [28] introduced a new encoding\nrepresentation to describe the OBB and incorporated a feature\nadaptive module to refine each feature pyramid layer. Zhou et\nal. [29] presented a simple ellipse parameters representation\nmethod for objects in any direction, utilizing the YOLOX\n[30] algorithm for directional ship detection and yielding\nfavourable results. Zhou et al. [31] designed a new anchor-\nfree keypoint-based detection method called KeyShip for high-\nprecision detection of oriented ships in SAR images. These\napproaches are all CNN-based ship detection methods for SAR\nimages. CNNs can obtain an image-specific inductive bias,\nincluding locality and translation equivariance, thus improving\nthe ability to learn image features. It is worth noting that SAR\nimages in inshore ship scenes are subject to severe background\nnoise. However, the convolution operation in CNNs can only\nobtain local receptive fields, making it incapable of capturing\nthe long-range dependencies that can bolster the represen-\ntation ability, thereby limiting the utilization of contextual\ninformation [32]. In addition, CNNs are sensitive to geometric\nperturbations of images, such as random translations, rotations\nand flips, which makes their anti-interference ability poor and\ntheir generalization ability weak [33].\nIn contrast to the convolution operation, the multi-head\nself-attentions (MSAs) of the transformer can capture long-\nrange dependencies [34]. Furthermore, related studies [35]\nhave shown that the transformer exhibits strong adaptability\nto perturbations, occlusions and domain shifts. In the field of\nobject detection in remote sensing images, Yao et al. [36] pro-\nposed a new multimodal deep learning framework for land use\nand land cover classification tasks, which outperforms other\nbackbone models based on transformers or CNNs. Li et al.\n[37] introduced a baseline network for hyperspectral anomaly\ndetection, which combines a low-rank representation model\nwith deep learning techniques to enhance the performance of\nhyperspectral anomaly detection. In the field of ship detection\nin SAR images, some researchers have also conducted relevant\nresearch based on transformers. For example, Xia et al. [38]\nutilized the Cascade Mask R-CNN as the basic architec-\nture, combined with the Swin Transformer [39], to obtain\na visual transformer framework based on contextual joint-\nrepresentation learning. Li et al. [40] introduced a feature en-\nhancement module based on the Swin Transformer to improve\nfeature extraction capabilities, along with an adjacent feature\nfusion module to optimize feature pyramids for enhancing\nship recognition and positioning capabilities in SAR images.\nShi et al. [41] developed a deformable attention mechanism\ninto a Swin Transformer and proposed a new contour-guided\nshape enhancement module to improve the accuracy of ship\ndetection in SAR images. Zhou et al. [42] created an edge\nsemantic decoupling module and integrated a transformer into\nthe detection layer to achieve dense ship detection in inshore\nareas. The above algorithms are all based on the HBB, and a\nsmall number of researchers have explored detection methods\nbased on the OBB. For example, Zhou et al. [43] modified\nthe pyramid vision transformer (PVT) [44] model, designed\na multiscale feature fusion module and adopted a new loss\nfunction to improve the detection ability for small targets\nand mitigate the influence of the ship’s boundary scattering\ninterference. However, the abovementioned transformer-based\nmethods have limited context information and feature infor-\nmation based on ships in SAR images, so their detection\nperformance needs to be further improved.\nTo address the above problems, this paper presents an\nanchor-free method based on transformers and adaptive fea-\ntures for arbitrarily oriented ship detection in SAR images,\ncalled SAD-Det. The method is a hybrid structure of the\ntransformer and CNN. First, to obtain more contextual in-\nformation related to ships in SAR images, the ship spatial\npooling pyramid vision transformer (SSP-PVT) is proposed.\nThis module employs PVT to generate multilayer feature\nmaps and then introduces the last layer of feature maps into\nthe ship spatial pooling module (SSPM) to modulate the\nfeature maps from different dimensions. Second, to improve\nthe performance of ship target feature fusion, we design the\nadaptive feature pyramid network (AFPN). By incorporat-\ning the adaptive weight module (AWM) in different feature\nlayers of the feature pyramid network (FPN), we leverage\nits self-attention characteristics to assign different weights to\nthe feature layers of the neck network. Third, to make the\nnetwork more adaptable to the characteristics of ships in SAR\nimages, we propose the deformable head (DeHead) based on\ndeformable convolution, which enhances the adaptive ability\nof the detection head network. The residual connection is\nintroduced to solve the problem of vanishing gradients.\nThe main contributions of this paper are as follows:\n1) An anchor-free method based on transformers and adap-\ntive features for arbitrarily oriented ship detection in\nSAR images is proposed, called SAD-Det. The detection\nperformance of the arbitrarily oriented ship method\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3325573\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 3\nFig. 1. The overall architecture of SAD-Det.\nfor SAR images based on the anchor-free framework\nachieves the highest average precision, which demon-\nstrates the potential of combining the transformer and\nCNN.\n2) The ship spatial pooling pyramid vision transformer\n(SSP-PVT) is proposed, which utilizes PVT and incor-\nporates a module called the ship spatial pooling module\n(SSPM) to enhance the long-range dependencies of ships\nin SAR images, thereby obtaining sufficient contextual\ninformation to improve the detection performance of\nships in SAR images.\n3) The adaptive feature pyramid network (AFPN) is pro-\nposed, which incorporates the adaptive weight module\n(AWM) into the neck network based on FPN. The aim of\nAWM is to add fusion factors to different feature layers,\nthereby assigning different weights to the feature layers\nin order to improve the performance of feature fusion for\nships in SAR images. Furthermore, the deformable head\n(DeHead) is proposed, which implements deformable\nconvolution to adaptively detect the spatial sampling\npositions of the targets and incorporates residual con-\nnections to optimize the network for ship characteristics\nin SAR images.\n4) Extensive experiments on SAR ship detection dataset\n(SSDD) and rotated ship detection dataset in SAR im-\nages (RSDD-SAR) validate the effectiveness of the pro-\nposed module. Compared with other arbitrarily oriented\nobject detectors, the proposed method achieves state-of-\nthe-art detection performance.\nThe remaining sections of this paper are organized as fol-\nlows. Section II provides a detailed description of the proposed\nmethod. Section III presents and analyses the experimental\nresults. Section IV summarizes the conclusions.\nII. M ETHODOLOGY\nIn this section, we first explain the overall architecture of the\nproposed method and then describe SSP-PVT of the backbone\nnetwork, AFPN of the neck network, and DeHead of the head\nnetwork. Finally, we illustrate the loss function used by this\nmethod.\nA. Overall Architecture\nThe overall architecture of the proposed method, named\nSAD-Det, is illustrated in Fig. 1. The backbone network is\nSSP-PVT we propose, which is based on PVT, as depicted\nby the blue cube. To maintain consistency with the nam-\ning conventions of CNN-based backbone networks such as\nResNet, the feature maps outputted by SSP-PVT are named\n{C2,C3,C4,C5}. Then, the last layer of feature layer C5 is\nfed into the proposed SSPM, where it is transformed into\na feature map C5’ represented by the red cube. Therefore,\nthe feature map generated by the backbone network becomes\n{C2,C3,C4,C5’}. Subsequently, the multilayer feature map is\ninput to the proposed AFPN. Initially, lateral connections are\nutilized to adjust them to a feature map {M2, M3, M4, M5 }\nwith an equal number of channels, as shown by the orange\ncube. Then, the top-down pathway and the proposed AWM\nare employed to merge the feature maps, resulting in the fused\nfeature map. Finally, convolution operations are applied to\nobtain the output feature map {P2, P3, P4, P5 }, depicted by\nthe green cube. The feature map is then fed into the proposed\nDeHead, where deformable convolution (DC) and residual\nconnections are utilized in the two branches to further process\nthe feature map. Finally, the loss function is computed for\nthree parts, namely, classification, regression, and center-ness.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3325573\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 4\nFig. 2. The structure of PVT-V2-B2.\nB. Ship Spatial Pooling Pyramid Vision Transformer (SSP-\nPVT)\nWith the promising advancements of transformers in the\nfield of natural language processing, researchers have extended\ntransformers to the field of computer vision and achieved\npromising outcomes. For example, the Vision Transformer\n(ViT), proposed by Dosovitskiy et al. [45], demonstrated\nfor the first time that a transformer could be applied to\ncomputer vision, and it achieved state-of-the-art performance\nin numerous experiments at that time. However, ViT is not a\ngeneric backbone, as it is only suitable for image classification,\nnot for dense prediction tasks such as object detection and\nimage segmentation.\nPVT [44] is the first hierarchical design of the ViT, which\nincorporates a pyramid structure into the transformer, enabling\nseamless integration with downstream tasks such as object\ndetection. This design is similar to that of ResNet and other\nCNN-based backbone networks. PVT plays a crucial role\namong a multitude of outstanding backbone networks and\nhas yielded superior outcomes compared to Swin-Transformer,\nwhich is another frequently employed backbone network.\nThis will be elucidated through subsequent experiments and\nanalyses. Unlike traditional convolutional backbones, PVT is a\nnonconvolutional backbone that consists of multiple indepen-\ndent transformer encoders stacked together. The resolution of\nthe input image is gradually reduced through patch embedding.\nAdditionally, a related team proposed PVT-V2 [46], which\nreduces the computational complexity to linear and solves\nthe problem of the high computational complexity of PVT\nfor high-resolution images. PVT-V2 offers different models\naccording to different parameter quantities and task require-\nments. To ensure a fair comparison with existing backbones\nsuch as ResNet-50 [47] and Swin-T [39], we selected PVT-\nV2-B2, which possesses similar parameter quantities, as the\nbaseline for the proposed method’s backbone.\nThe structure of PVT-V2-B2 is illustrated in Fig. 2. Similar\nto the CNN-based backbone, PVT-V2-B2 contains four stages,\neach of which consists of a patch embedding layer and some\ntransformer encoder layers, producing feature maps at various\nscales. In the first stage, the input image is set to a size of\nH×W×3, and then it is divided into HW\n42 patches, where each\npatch has a size of 4 ×4×3. Then, the patches are fed into an\noverlapping patch embedding layer, and multiple embedded\npatches with a size of HW\n42 ×64 can be obtained. Subsequently,\nthe embedded patches are passed through three consecutive\ntransformer encoder layers, followed by a reshaping operation\nto generate a feature map C2 with a size of H\n4 × W\n4 × 64. In\ncomparison with reference [16], a transformer encoder layer\nof PVT-V2-B2 contains a spatial-reduction attention (SRA)\nlayer, which means adding convolutions before the multihead\nattention layer to reduce the computational complexity, and\na convolutional feed forward layer, which means adding a\ndepthwise convolution between the first FC layer and the\nGELU layer. By applying a similar approach to the remaining\nthree stages, we can obtain feature maps C3, C4, and C5 with\ndifferent sizes.\nCompared to other targets, ship targets in SAR images have\na substantial aspect ratio. PVT-V2-B2 is a general backbone\nnetwork that usually detects input feature maps within a\nsquare window. However, it lacks specific optimization for\nship targets, leaving room for improvement in its detection\nperformance. Inspired by reference [48], we propose the SSPM\nto enhance the long-range dependencies of ships in SAR\nimages. The specific structure of SSPM is illustrated in Fig. 3.\nWe first set the feature map C5 as the input x ∈ RH′×W′×C′\nand then pass x into two branches. Each branch contains\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3325573\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 5\nFig. 3. The structure of SSPM.\na horizontal or vertical strip pooling layer. The outputs of\nthese layers are labelled yh ∈ RH′×C′\nand yv ∈ RW′×C′\n,\nrespectively. The specific formulas are as follows:\nyh\ni,c = 1\nW′\nX\n0≤j≤W′\nxi,j,c (1)\nyv\nj,c = 1\nH′\nX\n0≤i≤H′\nxi,j,c (2)\nThen, the feature map passes through two one-dimensional\nconvolutional layers with kernel sizes of 3 and 5, restores\nthe feature map to its original size through the expansion\noperation, and performs elementwise addition on the feature\nmap. The specific process is as follows:\nyh\ni,c = EP (Conv3×1(yh\ni,c)) +EP (Conv5×1(yh\ni,c)) (3)\nyv\nj,c = EP (Conv1×3(yv\nj,c)) +EP (Conv1×5(yv\nj,c)) (4)\nyi,j,c = yh\ni,c + yv\nj,c (5)\nwhere Conv3×1, Conv5×1, Conv1×3 and Conv1×5 represent\nconvolutional layers of sizes 3×1, 5×1, 1×3 and 1×5, respec-\ntively. EP represents the expansion operation. After that, the\noutput z is calculated as follows:\nz = x + Prod (x, σ(Conv1×1(y))) (6)\nwhere Conv1×1 represents the convolutional layer of size\n1×1. σ represents the sigmoid function, and Prod represents\nthe elementwise product. At the same time, the residual\nconnection [47] is introduced to improve the detection results.\nThrough the above process, the feature map C5 is transformed\ninto C5’. In conclusion, the output feature map of SSP-PVT\nis composed of {C2,C3,C4,C5’}.\nC. Adaptive Feature Pyramid Network (AFPN)\nFeature fusion is a crucial step in many object detection\ntasks. FPN [49] is a typical feature fusion method that is\nwidely used in various object detection networks. It can con-\nnect deep and shallow feature maps through top-down path-\nways and lateral connections and subsequently fuses objects of\ndifferent scales to enhance the detection performance. How-\never, the feature fusion method represented by FPN simply\ninvolves element-by-element addition or concatenation of fea-\ntures within the channel dimension without fully considering\nthe characteristics of the specific detection tasks. In the field of\nship detection in SAR images, ships are typically small in size,\nas observed in several publicly available datasets. Therefore,\ninvestigating methods of enhancing detection performance for\nsmall targets represented by ships is of great significance.\nAdding fusion factors between adjacent feature layers to\nassign different weights to each layer is an efficient approach\nto enhancing the performance of FPN. Inspired by reference\n[50], we propose the AWM depicted in Fig. 4. The overall\nneck network is shown in Fig. 1, which we call the adaptive\nfeature pyramid network or AFPN.\nConsidering that the majority of the ships in a SAR image\nare small in size and that the shallow feature map contains\nmore information about small targets, we modified the input of\nthe neck from the usual {C3,C4,C5’} to {C2,C3,C4,C5’}. The\ninput feature map is first adjusted to the middle feature map\n{M2, M3, M4, M5 } with a channel number of 256 through\nlateral connections. M5 then doubles the values of H and W\nthrough interpolation and performs scalar multiplication with\nthe weight w5\n4 obtained by AWM. The result is then added to\nM4 to create a new feature map M4. This new feature map\nM4 is passed through a 3 ×3 convolution to obtain the output\nfeature map P4. Similarly, the feature maps P2 and P3 can be\nobtained, and P5 can be directly obtained from M5 through a\n3×3 convolution. The formulas are presented as follows:\nMl = Conv1×1(Cl), l = 2, 3, 4, 5 (7)\nMl = Ml + wl+1\nl × Upsample(Ml+1), l = 2, 3, 4 (8)\nPl = Conv3×3(Ml), l = 2, 3, 4, 5 (9)\nwhere Conv1×1 and Conv3×3 represent convolutional layers\nof sizes 1×1 and 3×3, respectively, and Upsample represents\nthe nearest neighbour interpolation. wl+1\nl represents the weight\nfactor resulting from AWM, as shown in Fig. 4. The input\nvalue is obtained by the elementwise addition of the feature\nmaps Ml and Ml+1, and then the feature factor wl+1\nl is\nobtained by convolution, global average pooling (GAP), global\nmax pooling (GMP), sigmoid and other methods. The formula\nis as follows:\nwl+1\nl = f(Ml + Upsample(Ml+1)), l = 2, 3, 4 (10)\nwhere f represents AWM. After the above process, the output\nfeature map of AFPN consists of {P2, P3, P4, P5 }.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3325573\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 6\nFig. 4. The structure of AWM.\n(a)\n (b)\nFig. 5. Comparison of standard convolution and deformable convolution on\nthe receptive field. (a) Standard convolution, (b) deformable convolution.\nD. Deformable Head (DeHead)\nAs previously mentioned, the ship target in a SAR image\nhas a large aspect ratio, and because standard convolution\nsamples the fixed position of the input feature map, it cannot\nmeet the precise requirements of ship detection in SAR\nimages. Therefore, it is necessary to adaptively adjust the size\nof the receptive field to achieve more accurate positioning.\nDeformable convolutions [51], [52] can mitigate this problem.\nFig. 5 shows a comparison between standard convolution and\ndeformable convolution. The area represented by each point in\nFig. 5(a) and (b) indicates the location of the receptive field.\nOur focus is on the activation point of the uppermost feature\nmap, which is set at the center of the ship. This point needs to\nbe derived from the nine points marked by the feature map of\nthe middle layer through 3x3 convolution. These nine points\nneed to be derived from multiple points of the feature map in\nthe lowest layer through 3x3 convolution. After comparison,\nit is evident that the standard convolution in Fig. 5(a) has a\nfixed receptive field, while the deformable convolution in Fig.\n5(b) has an adaptive receptive field. The sampling positions of\ndeformable convolution are more in line with the shape and\nsize of the object itself.\nThe head design of the proposed method is illustrated in\nFig. 1. Similar to the head network of FCOS [53], the head\nof the proposed method is shared among different feature\nlayers. However, a notable difference is the inclusion of an\nadditional angle branch to facilitate the merging process in\nthe regression task. The head of each layer consists of two\nbranches: one branch predicts classification tasks, while the\nother predicts regression and center-ness tasks. The 4-layer\nfeature map output by the neck network is used as the input of\nthe head network. Initially, the feature maps pass through four\n3×3 convolutional layers with 256 channels. Subsequently,\nthey are passed through a deformable convolutional layer,\nwhich takes into account the residual connection [47] that\nwe propose to improve the detection results. Finally, a 3 ×3\nconvolutional layer, with respective channel numbers of 1, 5,\nand 1, is employed for classification tasks, regression tasks,\nand center-ness tasks.\nE. Loss Function\nThe loss function of the proposed method consists of three\nparts, which can be calculated as\nLoss = 1\nNpos\nX\nx,y\nLcls(px,y, c∗\nx,y)\n+ λ1\nNpos\nX\nx,y\n1{c∗x,y>0}Lreg(tx,y, t∗\nx,y)\n+ λ2\nNpos\nX\nx,y\n1{c∗x,y>0}Lctrness(sx,y, s∗\nx,y)\n(11)\nwhere Lcls represents the classification loss using the focal\nloss function [54]. Lreg represents the regression loss using the\nrotated IoU loss function [55]. Lctrness represents the center-\nness loss using the cross entropy loss function [56]. Npos\nrepresents the number of positive samples. λ1 and λ2 represent\nthe balance weights for Lreg and Lctrness, respectively, and\nboth default to 1. 1{c∗x,y>0} is 1 when the (x,y) point in\nthe feature map is identified as a positive sample; otherwise,\nit is 0. px,y indicates the score predicted as a ship at that\npoint. c∗\nx,y represents the real label corresponding to that\npoint, with 1 for a ship and 0 for the background. tx,y\nrepresents the target bounding box information predicted at\nthe point, and t∗\nx,y represents the real target bounding box\ninformation corresponding to the point. sx,y represents the\npredicted center-ness at that point, and s∗\nx,y represents the true\ncenter-ness corresponding to that point.\nIII. E XPERIMENTAL RESULTS\nA. Datasets and Experimental Settings\nWe use two datasets to evaluate the performance of the\nproposed method, namely, SAR ship detection dataset (SSDD)\n[14], [57] and rotated ship detection dataset in SAR images\n(RSDD-SAR) [58].\nSSDD is the first public ship dataset for SAR images. It has\ntwo versions, and we specifically chose the 2021 version due\nto its unified OBB annotations and detailed usage standards\ncompared to the 2017 version. This dataset contains a total\nof 1160 images depicting 2456 ships. It encompasses various\npolarization methods, resolutions, and image sizes. As one of\nthe most widely used ship datasets in SAR images, SSDD\nserves as a baseline for evaluating the performance of the\nproposed method. Specific details can be found in Table I.\nRSDD-SAR is one of the few publicly available ship\ndatasets with OBB annotations for SAR images. It comprises\n7000 images, encompassing multiple imaging modes, polar-\nization modes, and resolutions. The images in this dataset\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3325573\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 7\nTABLE I\nDETAILS OF SSDD AND RSDD-SAR\nDataset SSDD RSDD-SAR\nDate 2017 or 2021 2022\nSensors RadarSat-2,\nTerraSAR-X,\nSentinel-1\nGaofen-3,\nTerraSAR-X\nPolarization HH, VV , VH, HV HH, VV , VH, HV ,\nDH, DV\nResolution (m) 1–15 2–20\nScenes Inshore, Offshore Inshore, Offshore\nImage Size (pixels) 160–668 512 ×512\nImage Number 1160 7000\nShip Number 2456 10263\nAnnotation Vertical, Oriented,\nPolygon\nOriented\ncontain a total of 10263 ships. The model trained by this\ndataset demonstrates a strong generalization ability, thus mak-\ning RSDD-SAR an ideal choice for evaluating the performance\nof ship detection based on the OBB. Details can be found in\nTable I.\nThe SSDD dataset comprises images of various sizes, which\nwe resized to 500 ×350 pixels for our experiments. Following\nthe recommendation in reference [57], we divided the images\ninto a training set and a testing set using an 8:2 ratio.\nRegarding the RSDD-SAR dataset, the default image input\nsize is specified as 512 ×512 pixels in reference [58]. This\ndataset is randomly split into a training set and a test set using\na 5:2 ratio.\nThe experiment involving the proposed method employed\nthe AdamW optimizer with an initial learning rate of 0.0001\nand a weight decay of 0.0005. The batch size was set to\n8. For training on the SSDD dataset, the model underwent\n120 epochs, with the learning rate decreasing by 0.1 times at\n60, 90, and 110 epochs. On the other hand, the training of\nthe RSDD-SAR dataset involved 36 epochs, with the learning\nrate decreasing by 0.1 times at 24 and 33 epochs. The other\nmethods utilized a stochastic descent gradient (SGD) optimizer\nwith an initial learning rate of 0.001, a weight decay of 0.0001,\nand a momentum of 0.9. All experiments were implemented on\nthe following software and hardware platforms. The software\nplatform employed a toolbox called MMRotate [59], which\nis based on the deep learning framework PyTorch, and was\nrun on the Ubuntu 22.04 operating system. The hardware\nplatform consisted of a computer equipped with an Intel Core\ni9-12900KF processor and two Nvidia GeForce RTX 3090\nGPUs.\nB. Evaluation Metrics\nTo quantitatively evaluate the performance of the pro-\nposed method, we utilized three evaluation metrics from the\nMS COCO dataset [60]: AP, AP50 and AP75. Additionally,\nwe used floating point operations (FLOPs) and parameters\n(Params) to measure the time complexity and space complexity\nof the model. AP is the average precision calculated from\nintersection over union (IoU) values ranging from 0.5 to 0.95\nwith a step size of 0.05. AP50 represents the average precision\nat an IoU threshold of 0.5, aligning with the evaluation\nmetric used in Pascal VOC [61]. AP75 represents the average\nprecision at an IoU threshold of 0.75.\nGenerally, the definition of AP is the area under the\nprecision-recall curve (PRC). A higher AP value indicates\nbetter detection performance of the method. The formula for\nAP is given by:\nAP =\nZ 1\n0\nP(R)dR (12)\nwhere P represents the precision, which is the proportion of\ncorrectly detected ship samples to all predicted positive ship\nsamples. R represents the recall, which is the proportion of\ncorrectly detected ship samples to all labelled positive ship\nsamples. The formulas for both are as follows:\nPrecision = TP\nTP + FP (13)\nRecall = TP\nTP + FN (14)\nwhere TP represents true positives, referring to the correctly\ndetected positive samples. FP represents false positives, in-\ndicating incorrectly detected positive samples. FN represents\nfalse negatives, denoting the incorrectly detected negative\nsamples. Given the IoU threshold, and setting recall as the\nabscissa and precision as the ordinate, we can obtain the PRC\nmentioned earlier.\nC. Analysis of the Proposed Method\n1) Ablation Experiments: To verify the effectiveness of\nthe proposed method, we gradually incorporate each module\ninto the baseline and analyse the impact of each module\non SSDD. The corresponding results are presented in Table\nII. The baseline network we utilize is FCOS(OBB), which\nincorporates an angle branch solely to facilitate the detection\nof rotationally invariant objects, in contrast to FCOS. To\nensure a fair comparison, we modify the neck feature layer\nof the baseline network from {P3, P4, P5, P6, P7 } to {P2, P3,\nP4, P5} as employed by the proposed method, and we adopt\nthe same training strategy as the proposed method. Unless\notherwise specified, the following experiments are conducted\non SSDD by default.\nAccording to the results presented in Table II, some con-\nclusions can be drawn from the analysis. The three proposed\nmodules demonstrate a significant improvement in detection\nperformance compared to the baseline network, thus validating\ntheir effectiveness. Furthermore, the proposed method exhibits\nan increase of 3.8%, 2.4%, and 7.6% in the value of AP, AP50,\nand AP75, respectively, compared to the baseline network.\nAdditionally, it is evident from the values of FLOPs and\nParams in the table that SSP-PVT has similar space com-\nplexity and time complexity compared to ResNet-50 from the\nbaseline network, thereby demonstrating the effectiveness of\nthe transformer-based architecture in ships in SAR images.\nMoreover, the other two modules incorporate adaptive methods\nthat aid in detecting ship features. However, this leads to\nan increase in the parameter quantity, consequently affecting\nthe high-speed performance of detection. Fig. 6 illustrates the\ncorresponding PRCs for different module combinations at an\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3325573\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 8\nTABLE II\nABLATION EXPERIMENTS WITH THE PROPOSED METHOD ON SSDD\nSSP-PVT AFPN DeHead AP AP50 AP75 FLOPs (G) Params (M)\nBaseline – – – 55.5% 94.4% 60.3% 95.94 31.37√ – – 58.2% 95.9% 65.6% 92.01 37.59\nSAD-Det √ √ – 58.4% 96.7% 66.3% 118.25 44.55√ √ √ 59.3% 96.8% 67.9% 125.61 45.39\nThe bold items indicate the best value for each column.\nFig. 6. PRCs with IoU=0.75 when adding different modules\nIoU of 0.75 and provides a more intuitive representation of\nthe benefits brought by each module. Subsequently, a detailed\nanalysis is conducted to examine the impacts of the three\nproposed modules.\n2) The effect of SSP-PVT: Next, ablation experiments and\nextensive discussions are conducted for SSP-PVT. To com-\nprehensively examine the impact of SSP-PVT, it is compared\nagainst other widely employed backbone networks, including\nResNet-50, Swin-T, and PVT-V2-B2. The experimental results\nare presented in Table III. Each experiment utilizes FPN\nas the neck network and FCOS(OBB) head as the head\nnetwork. The similarity in the values of AP, FLOPs and Params\nbetween Swin-T and ResNet-50 validates the viability of the\ntransformer architecture in ship detection for SAR images.\nFurthermore, PVT-V2-B2 exhibits superior performance com-\npared to Swin-T, resulting in increments of 3.0%, 1.0% and\n4.6% for the value of AP, AP50, and AP75, respectively. This\nphenomenon is likely due to Swin-T’s utilization of windows\nmulti-head self-attention (W-MSA) for self-attention in local\nareas at the expense of direct global relationship modelling,\nan essential feature of vision transformers. Additionally, SSP-\nPVT outperforms PVT-V2-B2, yielding increments of 0.4%,\n0.2% and 1.2% for the value of AP, AP50, and AP75,\nrespectively. This is likely because the inclusion of SSPM\nstrengthens the long-range dependencies of ships in SAR\nimages. These experiments demonstrate that SSP-PVT can\nefficiently extract the features of ships in SAR images.\nWe also represent these methods visually. Fig. 7 depicts\nsome sparse small targets in an offshore scene of the SSDD\nTABLE III\nCOMPARISON OF PERFORMANCE BETWEEN SSP-PVT AND OTHER\nBACKBONE NETWORKS\nType AP AP50 AP75 FLOPs (G) Params (M)\nResNet-50 55.5% 94.4% 60.3% 95.94 31.37\nSwin-T 54.8% 94.7% 59.8% 97.71 34.99\nPVT-V2-B2 57.8% 95.7% 64.4% 91.86 32.21\nSSP-PVT 58.2% 95.9% 65.6% 92.01 37.59\nThe bold items indicate the best value for each column.\ndataset. The detection results using ResNet-50 and Swin-T\nas the backbone network reveal the presence of some false\npositives. However, when utilizing PVT and SSP-PVT as the\nbackbone network, all objects are detected accurately, thereby\nintuitively verifying the effectiveness of this module.\n3) The effect of AFPN: Subsequently, ablation experiments\nand further discussion of AFPN are conducted. AFPN is\ncompared to other commonly used neck networks, and the\nresults are presented in Table IV. For these experiments,\nconsidering the influence of the transformer on the experiment,\nthe backbone network of each experiment utilizes PVT-V2-B2,\nand the head network adopts FCOS(OBB) head. The results\ndemonstrate that FPN-based variants such as PAFPN [62] and\nBiFPN [63] are not as effective as the baseline structure FPN,\nleading to decreased values of AP, AP50, and AP75. This\nis likely because the complex neck networks, involving more\nupsampling and downsampling, result in the loss of feature\ninformation, thereby negatively impacting detection result.\nHowever, when compared to FPN, AFPN exhibits improved\nperformance with increases of 0.1%, 0.7% and 1.1% in the\nvalue of AP, AP50, and AP75, respectively. This improvement\ncan be attributed to AWM incorporated in AFPN, which\nadaptively changes the weight of each feature map based\non the specific characteristics of each group of images, thus\nachieving more accurate detection. It is worth noting that\ndue to the inclusion of AWM, the FLOPs value and Params\nvalue in AFPN are slightly higher than those in FPN, mildly\naffecting the network’s high-speed performance.\nThe visualization of these methods is given in Fig. 8,\ndepicting an inshore scene with densely packed targets from\nthe SSDD dataset. It is evident that all methods exhibit various\nlevels of missed detections. Notably, AFPN demonstrates\nsuperior performance compared to other methods, with the\nfewest false positives in the detection results of the neck\nnetwork. This illustrates the effectiveness of AFPN, but there\nis still potential for improvement in dense scenes.\n4) The effect of DeHead: In the following analysis, we\nexamine DeHead. The head network of FCOS(OBB) is used\nas a baseline and compared with DeHead. The results are\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3325573\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 9\n(a)\n (b)\n (c)\n (d)\n (e)\nFig. 7. Visualization of the detection results for different backbone networks. (a) Ground truth, (b) ResNet-50, (c) Swin-T, (d) PVT-V2-B2, (e) SSP-PVT.\nThe blue boxes indicate the ground truth, the red boxes indicate true positives, and the orange circles indicate false positives.\n(a)\n (b)\n (c)\n (d)\n (e)\nFig. 8. Visualization of the detection results for different neck networks. (a) Ground truth, (b) FPN, (c) PAFPN, (d) BiFPN, (e) AFPN. The blue boxes\nindicate the ground truth, the red boxes indicate true positives, and the orange circles indicate false positives.\n(a)\n (b)\n (c)\nFig. 9. Visualization of the detection results for different head networks. (a) Ground truth, (b) Baseline, (c) DeHead. The blue boxes indicate the ground\ntruth, the red boxes indicate true positives, and the light blue circles indicate false negatives.\nTABLE IV\nCOMPARISON OF PERFORMANCE BETWEEN AFPN AND OTHER NECK\nNETWORKS\nType AP AP50 AP75 FLOPs (G) Params (M)\nFPN 57.8% 95.7% 64.4% 91.86 32.21\nPAFPN 57.2% 94.8% 65.1% 96.22 35.75\nBiFPN 57.0% 95.6% 62.5% 95.78 34.21\nAFPN 57.9% 96.4% 65.5% 118.1 39.30\nThe bold items indicate the best value for each column.\npresented in Table V. In each experiment, the backbone\nnetwork utilizes PVT-V2-B2, while the neck network adopts\nFPN. It can be seen that DeHead outperforms the baseline,\nresulting in the value of AP, AP50, and AP75 increasing by\n0.4%, 0.2% and 1.2%, respectively. This improvement stems\nfrom the introduction of deformable convolutions, which en-\nhance the network’s adaptability to ship characteristics through\nadaptive detection of the objects’ spatial sampling positions.\nAdditionally, the introduction of residual connections at deeper\npositions alleviates the issue of vanishing gradients. Further-\nmore, the inclusion of modules results in a slight increase in\ntime complexity and space complexity.\nDifferent head networks are also shown, revealing a sparse\nTABLE V\nCOMPARISON OF PERFORMANCE BETWEEN DEHEAD AND THE\nBASELINE\nType AP AP50 AP75 FLOPs(G) Params(M)\nBaseline 57.8% 95.7% 64.4% 91.86 32.21\nDeHead 58.4% 95.9% 65.8% 99.31 33.05\nThe bold items indicate the best value for each column.\nsmall target in an offshore scene of the SSDD dataset, as\ndepicted in Fig. 9. It is evident that the baseline’s head network\nexhibits false negatives, whereas DeHead network displays\nnone, thereby substantiating the validity of the proposed\nmodule from a qualitative perspective.\nD. Comparison with General Arbitrarily Oriented Object De-\ntection Methods\nTo further assess the effectiveness of the proposed method,\nthis section compares the proposed SAD-Det with several\ncommonly used arbitrarily oriented object detection methods\non both SSDD and RSDD-SAR. These methods include two-\nstage and anchor-based algorithms such as Oriented R-CNN\n[64], Faster R-CNN(OBB) [65], and ReDet [66]. Additionally,\none-stage and anchor-based algorithms such as R3Det [67] and\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3325573\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 10\nTABLE VI\nDETECTION RESULTS OF DIFFERENT ARBITRARILY ORIENTED OBJECT\nDETECTION METHODS ON SSDD\nMethods AP AP50 AP75 FLOPs\n(G)\nParams\n(M)\nTwo-stage,\nanchor-based\nOriented R-CNN 51.9% 90.4% 55.0% 47.85 41.13\nFaster R-CNN(OBB) 43.1% 86.2% 40.7% 47.83 41.12\nReDet 54.8% 91.6% 63.6% 36.80 31.54\nOne-stage,\nanchor-based\nR3Det 46.4% 89.2% 43.6% 56.57 41.58\nS2A-Net 52.3% 92.7% 56.2% 33.76 36.18\nOne-stage, anchor-free\nFCOS(OBB) 52.2% 91.4% 57.3% 35.47 31.89\nBased on a transformer\nSAD-Det 59.3% 96.8% 67.9% 125.61 45.39\nThe bold items indicate the best value for each column.\nTABLE VII\nDETECTION RESULTS OF DIFFERENT ARBITRARILY ORIENTED OBJECT\nDETECTION METHODS ON RSDD-SAR\nMethods AP AP50 AP75 FLOPs\n(G)\nParams\n(M)\nTwo-stage,\nanchor-based\nOriented R-CNN 48.0% 89.6% 46.5% 63.28 41.13\nFaster R-CNN(OBB) 40.6% 85.7% 32.2% 63.25 41.12\nReDet 50.4% 90.7% 51.7% 40.88 31.54\nOne-stage,\nanchor-based\nR3Det 45.4% 89.5% 39.9% 82.17 41.58\nS2A-Net 48.6% 90.9% 47.5% 49.05 36.18\nOne-stage, anchor-free\nFCOS(OBB) 47.9% 89.3% 47.1% 51.55 31.89\nBased on a transformer\nSAD-Det 52.0% 93.8% 53.9% 182.82 45.39\nThe bold item indicates the best value for each column.\nFig. 10. PRCs of different methods on SSDD when IoU=0.75\nS2A-Net [68], as well as one-stage and anchor-free algorithms\nsuch as FCOS(OBB) [53].\nThe results of different arbitrarily oriented object detection\nmethods on SSDD are presented in Table VI. The results\nFig. 11. PRCs of different methods on RSDD-SAR when IoU=0.75\nTABLE VIII\nDETECTION RESULTS OF DIFFERENT ARBITRARILY ORIENTED\nSHIP DETECTION METHODS ON RSDD-SAR\nMethods All–AP50 Inshore–AP50 Offshore–AP50\nKeyShip 89.8% – –\nSaDet* 91.7% 74.3% 95.5%\nAEDet 90.1% 77.8% 90.5%\nMT-FANet 90.8% 66.9% 95.7%\nSAD-Det 93.8% 78.7% 96.5%\n1 * indicates that the structure is reproduced.\n2 The bold items indicate the best value for each column.\ndemonstrate that SAD-Det outperforms the other methods in\nterms of AP, AP50, and AP75, achieving respective values\nof 59.3%, 96.8%, and 67.9%. Taking the AP50 indicator as\nan example, SAD-Det’s performance witnessed a noticeable\nimprovement of 5.2%, 4.1%, and 5.4% compared to that\nof ReDet, S2A-Net, and FCOS(OBB), respectively. These\nexperimental findings verify that SAD-Det, which combines\ntransformer and CNN architectures, yields superior detection\noutcomes relative to other CNN-based arbitrarily oriented\ndetection methods and can even rival the object detection\nperformance of HBB-based methods. It is evident that the\nproposed method has higher time and space complexity com-\npared to the other methods, as indicated by its FLOPs and\nParams. The primary reason for this discrepancy is that the\nbackbone network of the proposed method produces feature\nmaps {C2, C3, C4, C5 }, while other methods that neglect\nthe largest feature map C2 only output feature maps {C3,\nC4, C5}. Furthermore, it should be noted that the calculation\nof FLOPs is partly influenced by the area of the feature\nmaps. Consequently, although the value of FLOPs of the\nproposed method is significantly higher than that of other\nmethods, it is still within an acceptable range. Moreover,\nFig. 10 illustrates the PRCs corresponding to these methods\nat IoU=0.75, enabling an enhanced understanding of their\nperformance and further substantiating the advantages of the\nproposed method from an alternative perspective.\nWe select several methods for visualization. The pictures\nconsist of two offshore scenes and two inshore scenes from the\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3325573\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 11\nTABLE IX\nTHE SPECIFIC CONFIGURATION OF A LARGE -SCALE\nALOS-2 SAR IMAGE\nParameter Value\nPosition Tokyo Bay\nPolarization HH\nWaveband L\nResolution (m) 3\nImage Size (pixels) 10389 ×6487\nTime 2014-04-10\nSSDD dataset, as depicted in Fig. 12. Fig. 12(a) portrays sparse\nsmall objects against an ocean background, and all methods\nachieve good detection results for this particular scenario. In\ncontrast to a few methods that produce a small number of\nFPs or FNs, other methods, including SAD-Det, accurately\ndetect all objects. In Fig. 12(b), in addition to sparse small\ntargets, the scene contains disturbances such as islands. At this\njuncture, most methods generate a considerable number of FPs\nand FNs. Nonetheless, SAD-Det only has one FP, highlighting\nits superior resistance to interference compared to that of\nother methods. Fig. 12(c) portrays dense large-scale ship\ntargets in an inshore scene. In contrast to other methods, the\nproposed method detected all targets accurately, showcasing\nits effectiveness in this type of environment. In Fig. 12(d),\ndense ship targets are once again present in an inshore scene,\nwith ships positioned close to the shore, thereby increasing the\ndifficulty of detection. All methods exhibit a high number of\nFPs, yet SAD-Det demonstrates superior detection accuracy.\nThis further emphasizes the need to enhance the proposed\nmethod’s resilience to interference in inshore scenes with\ndense targets.\nTo evaluate the generalization ability of the proposed\nmethod, experiments are conducted on RSDD-SAR. The re-\nsults of different arbitrarily oriented object detection methods\ncan be seen in Table VII. Compared to the other methods,\nSAD-Det achieves high scores in the AP, AP50, and AP75\nindicators, reaching 52.0%, 93.8%, and 53.9%, respectively.\nFor instance, in terms of the AP50 indicator, SAD-Det out-\nperforms ReDet, S2A-Net, and FCOS(OBB) with improve-\nments of 3.1%, 2.9%, and 4.5%, respectively, highlighting\nthe advantages of the proposed method in detecting OBBs.\nThe corresponding PRCs are shown in Fig. 11, confirming\nthe superior performance of the proposed method compared\nto other approaches.\nWe show the results of several methods on RSDD-SAR in\nFig. 13. Fig. 13(a) displays some ship targets under high-sea\nstates. While the other methods exhibit FPs and FNs, SAD-Det\naccurately detects all objects. The other three images in Fig.\n13(b)(c)(d) depict ship targets in inshore scenes. Due to the\ninterference of artificial facilities on the shore, small islands in\nthe sea and high-sea states, most methods generate missed and\nfalse detections, and some FPs and FNs appear. In contrast,\nSAD-Det accurately detects all objects in these three images.\nThis indicates that the proposed method possesses superior\nrobustness and anti-interference ability compared to the other\nmethods.\nE. Comparison with Arbitrarily Oriented Ship Detection\nMethods\nTo further validate the effectiveness of the proposed method\nfor ship detection, this section compares the proposed SAD-\nDet with several arbitrarily oriented ship detection methods\non RSDD-SAR from both inshore and offshore perspectives.\nThese methods include KeyShip [69], SaDet [27], AEDet [70],\nand MT-FANet [71]. Since the aforementioned methods are not\nopen source, we reproduce the structure of SaDet and conduct\nexperiments, citing the best experimental results from the other\nstudies as our comparison values. The specific results are\npresented in Table VIII, which demonstrates that the AP50 of\nSAD-Det achieves the best value in all scenes, inshore scenes,\nand offshore scenes. This clearly illustrates the effectiveness\nof the proposed method in the field of ship detection.\nF . Verification on a Large-Scene SAR Image\nTo assess the performance of the proposed method on a\nlarge-scene SAR image, we conduct experiments using a large-\nscale ALOS-2 SAR image that includes both inshore and\noffshore scenes. This image is not part of the SSDD and\nRSDD-SAR datasets, and its specific details can be found in\nIX. The model weights trained on SSDD are applied to this\nimage, and the results can be seen in Fig. 14. The performance\nis satisfactory for the seaside region, with only one missed\ndetection observed in the case of multiple small targets. In\ncontrast, the inshore scenes exhibit a higher number of false\nand missed detections. This discrepancy can be attributed to\nthe presence of interference, such as artificial facilities in\ninshore scenes, as well as the absence of large land scenes on\nSSDD. This suggests the necessity of further improving the\nanti-interference capability of the proposed method in inshore\nscenes and its robustness when applied to other datasets.\nIV. D ISCUSSION\nFor deep learning or machine learning models, we not only\nrequire them to fit well with the training dataset, which means\nhaving a small training error, but also hope that they can fit\nwell with unknown datasets such as the testing dataset, which\nmeans having strong generalization ability. The effectiveness\nof generalization ability can be evaluated by examining the\noccurrence of model overfitting and underfitting, as illustrated\nin Fig. 15. As the number of epochs increases, the training\nloss gradually decreases, and the validation loss initially\ndecreases and then increases. Overfitting and underfitting are\ntwo conditions used to describe the model’s behavior during\nthe training process. Both conditions adversely impact the\nneural network’s generalization ability, with overfitting being\nthe primary concern in current research.\nIn the field of ship detection from SAR images, overfitting\nis a concern due to the limited number of samples in the ship\ndataset. To address this issue, several methods are employed in\nthis study to prevent overfitting. Specifically, data augmenta-\ntion is applied to expand the size of the dataset. This involves\npre-processing the images by applying techniques such as\nimage rotation, scaling, and random cropping to generate\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3325573\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 12\n(a)\n (b)\n (c)\n (d)\nFig. 12. Visualization of the detection results of various methods on SSDD. (a) Offshore Scene 1, (b) Offshore Scene 2, (c) Inshore Scene 1, (d) Inshore\nScene 2. From top to bottom: Ground truth, Oriented R-CNN, Faster R-CNN(OBB), R3Det, S2A-Net, FCOS(OBB), SAD-Det. The blue boxes indicate the\nground truth, the red boxes indicate true positives, the orange circles indicate false positives, and the light blue circles indicate false negatives.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3325573\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 13\n(a)\n (b)\n (c)\n (d)\nFig. 13. Visualization of the detection results of various methods on RSDD-SAR. (a) Offshore Scene 1, (b) Inshore Scene 1, (c) Inshore Scene 2, (d) Inshore\nScene 3. From top to bottom: Ground truth, Oriented R-CNN, Faster R-CNN(OBB), R3Det, S2A-Net, FCOS(OBB), SAD-Det. The blue boxes indicate the\nground truth, the red boxes indicate true positives, the orange circles indicate false positives, and the light blue circles indicate false negatives.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3325573\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 14\nFig. 14. Visualization of the detection results for a large-scene ALOS-2 SAR image. The red boxes indicate true positives, the orange circles indicate false\npositives, and the light blue circles indicate false negatives.\nadditional images. In addition, L2 regularization and dropout\nregularization are employed to reduce model complexity.\nThe performance of the proposed model is evaluated as\nfollows. Firstly, increasing the number of epochs leads to a rise\nin validation loss and a subsequent decrease in the AP value\nif the model is overfitting. Fig. 16 illustrates the relationship\nbetween AP value and epochs for the proposed method in this\nstudy. Fig. 16(a) presents the test results on SSDD, while Fig.\n16(b) shows the test results on RSDD-SAR. It can be observed\nthat as the number of epochs increases, the values of AP,\nAP50, and AP75 initially increase and then stabilize without\ndecreasing. This indicates that the proposed method is not\naffected by overfitting. Furthermore, a hallmark of overfitting\nis limited generalization ability. Since the proposed model\nyields impressive outcomes across two datasets and a large-\nscene SAR image, it can be inferred to a certain extent that\nthe model avoids overfitting.\nJudging from the experimental results, it can be observed\nthat the trained model did not exhibit signs of overfitting\non the two datasets, thereby confirming the effectiveness of\nthe proposed method. Furthermore, it is important to explore\nlightweight and high-speed techniques for ship detection in\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3325573\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 15\nFig. 15. Schematic diagram of overfitting and underfitting\n(a)\n(b)\nFig. 16. The relationship between AP and epochs. (a) results on SSDD, (b)\nresults on RSDD-SAR.\nSAR images, as this will be the primary focus of my future\nresearch.\nV. C ONCLUSION\nIn this article, we propose an anchor-free method based\non transformers and adaptive features for arbitrarily oriented\nship detection in SAR images, namely, SAD-Det. SAD-Det\nis a hybrid structure of a transformer and CNN that can\ndetect rotationally invariant ship targets with high average\nprecision in SAR images. SAD-Det comprises three main\ncomponents: SSP-PVT, AFPN and DeHead. SSP-PVT is a\nbackbone network that is based on PVT and combined with\nSSPM. SSP-PVT can enhance the long-range dependencies of\nships in SAR images and obtain sufficient context information\nto improve the detection performance of ships in SAR images.\nAFPN is a neck network that is based on FPN and augmented\nwith AWM. The addition of fusion factors allows different\nweights to be assigned to feature layers, thereby enhancing the\nperformance of feature fusion in ships in SAR images. DeHead\nis a head network that utilizes deformable convolutions to\nadaptively detect the spatial sampling positions of the targets\nand combines residual connections to make the network more\nadaptable to the characteristics of ships in SAR images. The\neffectiveness of each module in the proposed method is veri-\nfied by experiments. Compared with other arbitrarily oriented\nobject detection methods, this method achieves state-of-the-art\ndetection performance. Additionally, the inclusion of specific\nmodules and targeted network designs significantly increases\nthe computational complexity of the proposed method. Future\nresearch will explore high-speed ship detection methods for\nSAR images.\nREFERENCES\n[1] A. Moreira, P. Prats-Iraola, M. Younis, G. Krieger, I. Hajnsek, and\nK. P. Papathanassiou, “A tutorial on synthetic aperture radar,” IEEE\nGeoscience and Remote Sensing Magazine, vol. 1, no. 1, pp. 6–43, 2013,\ndoi: 10.1109/MGRS.2013.2248301.\n[2] R. Gens, “Oceanographic applications of sar remote sensing,” GIScience\n& Remote Sensing, vol. 45, no. 3, pp. 275–305, 2008, doi: 10.2747/1548-\n1603.45.3.275.\n[3] R. B ¨urgmann, P. A. Rosen, and E. J. Fielding, “Synthetic aperture\nradar interferometry to measure earth’s surface topography and its\ndeformation,” Annual Review of Earth and Planetary Sciences , vol. 28,\nno. 1, pp. 169–209, 2000, doi: 10.1146/annurev.earth.28.1.169.\n[4] C. Liu, Z. Chen, Y . Shao, J. Chen, T. Hasi, and H. Pan, “Research\nadvances of sar remote sensing for agriculture applications: A review,”\nJournal of Integrative Agriculture , vol. 18, no. 3, pp. 506–525, 2019,\ndoi: https://doi.org/10.1016/S2095-3119(18)62016-7.\n[5] A. C. Mondini, F. Guzzetti, K.-T. Chang, O. Monserrat, T. R.\nMartha, and A. Manconi, “Landslide failures detection and\nmapping using synthetic aperture radar: Past, present and\nfuture,” Earth-Science Reviews , vol. 216, p. 103574, 2021, doi:\nhttps://doi.org/10.1016/j.earscirev.2021.103574.\n[6] J. Li, C. Xu, H. Su, L. Gao, and T. Wang, “Deep learning for sar ship\ndetection: Past, present and future,” Remote Sensing , vol. 14, no. 11,\n2022, doi: 10.3390/rs14112712.\n[7] D. J. Crisp, “The state-of-the-art in ship detection in synthetic aperture\nradar imagery,” 2004.\n[8] G. Gao, “Statistical modeling of sar images: A survey,” Sensors, vol. 10,\nno. 1, pp. 775–795, 2010, doi: 10.3390/s100100775.\n[9] A. Renga, M. D. Graziano, and A. Moccia, “Segmentation of marine sar\nimages by sublook analysis and application to sea traffic monitoring,”\nIEEE Transactions on Geoscience and Remote Sensing , vol. 57, no. 3,\npp. 1463–1477, 2019, doi: 10.1109/TGRS.2018.2866934.\n[10] L. Li, L. Du, and Z. Wang, “Target detection based on dual-domain\nsparse reconstruction saliency in sar images,” IEEE Journal of Selected\nTopics in Applied Earth Observations and Remote Sensing , vol. 11,\nno. 11, pp. 4230–4243, 2018, doi: 10.1109/JSTARS.2018.2874128.\n[11] C. P. Schwegmann, W. Kleynhans, and B. P. Salmon, “Synthetic\naperture radar ship detection using haar-like features,” IEEE Geoscience\nand Remote Sensing Letters , vol. 14, no. 2, pp. 154–158, 2017, doi:\n10.1109/LGRS.2016.2631638.\n[12] T. Liu, Z. Yang, Y . Jiang, and G. Gao, “Review of ship detection in\npolarimetric synthetic aperture imagery,” Journal of Radars , vol. 10,\nno. R20155, p. 1, 2021, doi: 10.12000/JR20155.\n[13] Y . LeCun, Y . Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521,\nno. 7553, pp. 436–444, 2015, doi: https://doi.org/10.1038/nature14539.\n[14] J. Li, C. Qu, and J. Shao, “Ship detection in sar images based on an\nimproved faster r-cnn,” in 2017 SAR in Big Data Era: Models, Methods\nand Applications (BIGSARDATA), 2017, pp. 1–6, doi: 10.1109/BIGSAR-\nDATA.2017.8124934.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3325573\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 16\n[15] K. O’Shea and R. Nash, “An introduction to convolutional neural\nnetworks,” arXiv e-prints, p. arXiv:1511.08458, Nov. 2015.\n[16] A. Vaswani et al. , “Attention is all you need,” in Advances in Neural\nInformation Processing Systems , vol. 30, 2017.\n[17] Z. Lin, K. Ji, X. Leng, and G. Kuang, “Squeeze and excitation\nrank faster r-cnn for ship detection in sar images,” IEEE Geoscience\nand Remote Sensing Letters , vol. 16, no. 5, pp. 751–755, 2019, doi:\n10.1109/LGRS.2018.2882551.\n[18] M. Li, S. Lin, and X. Huang, “Sar ship detection based on enhanced\nattention mechanism,” in2021 2nd International Conference on Artificial\nIntelligence and Computer Engineering (ICAICE) , 2021, pp. 759–762,\ndoi: 10.1109/ICAICE54393.2021.00148.\n[19] X. Nie, M. Duan, H. Ding, B. Hu, and E. K. Wong, “Attention\nmask r-cnn for ship detection and segmentation from remote sensing\nimages,” IEEE Access, vol. 8, pp. 9325–9334, 2020, doi: 10.1109/AC-\nCESS.2020.2964540.\n[20] L. Bai, C. Yao, Z. Ye, D. Xue, X. Lin, and M. Hui, “Feature en-\nhancement pyramid and shallow feature reconstruction network for sar\nship detection,” IEEE Journal of Selected Topics in Applied Earth\nObservations and Remote Sensing , vol. 16, pp. 1042–1056, 2023, doi:\n10.1109/JSTARS.2022.3230859.\n[21] T. Y . Lin, P. Goyal, R. Girshick, K. He, and P. Dollar, “Focal loss\nfor dense object detection,” in Proceedings of the IEEE International\nConference on Computer Vision (ICCV) , 2017.\n[22] L. Zhang, Y . Liu, W. Zhao, X. Wang, G. Li, and Y . He, “Frequency-\nadaptive learning for sar ship detection in clutter scenes,” IEEE Trans-\nactions on Geoscience and Remote Sensing , vol. 61, pp. 1–14, 2023,\ndoi: 10.1109/TGRS.2023.3249349.\n[23] G. Jocher, “YOLOv5 by Ultralytics,” May 2020. [Online]. Available:\nhttps://github.com/ultralytics/yolov5\n[24] G. Zhang, Z. Li, X. Li, C. Yin, and Z. Shi, “A novel salient feature\nfusion method for ship detection in synthetic aperture radar images,”\nIEEE Access , vol. 8, pp. 215 904–215 914, 2020, doi: 10.1109/AC-\nCESS.2020.3041372.\n[25] W. Liu et al., “Ssd: Single shot multibox detector,” in Computer Vision\n– ECCV 2016, 2016, pp. 21–37, doi: https://doi.org/10.1007/978-3-319-\n46448-0 2.\n[26] K. Wang et al. , “Oriented object detection in optical remote sensing\nimages using deep learning: A survey,” p. arXiv:2302.10473, Feb. 2023.\n[27] S. Zhao, Q. Liu, W. Yu, and J. Lv, “A single-stage arbitrary-oriented\ndetector based on multiscale feature fusion and calibration for sar\nship detection,” IEEE Journal of Selected Topics in Applied Earth\nObservations and Remote Sensing , vol. 15, pp. 8179–8198, 2022, doi:\n10.1109/JSTARS.2022.3206822.\n[28] P. Guo, T. Celik, N. Liu, and H. Li, “Break through the border restriction\nof horizontal bounding box for arbitrary-oriented ship detection in sar\nimages,” IEEE Geoscience and Remote Sensing Letters, vol. 20, pp. 1–5,\n2023, doi: 10.1109/LGRS.2023.3270897.\n[29] K. Zhou et al. , “Arbitrary-oriented ellipse detector for ship detection\nin remote sensing images,” IEEE Journal of Selected Topics in Applied\nEarth Observations and Remote Sensing , vol. 16, pp. 7151–7162, 2023,\ndoi: 10.1109/JSTARS.2023.3267240.\n[30] Z. Ge, S. Liu, F. Wang, Z. Li, and J. Sun, “Yolox: Exceeding yolo series\nin 2021,” p. arXiv:2107.08430, Jul. 2021.\n[31] J. Ge, Y . Tang, K. Guo, Y . Zheng, H. Hu, and J. Liang, “Keyship:\nTowards high-precision oriented sar ship detection using key points,”\nRemote Sensing, vol. 15, no. 8, 2023, doi: 10.3390/rs15082035.\n[32] A. A. Aleissaee et al. , “Transformers in remote sensing: A survey,”\nRemote Sensing, vol. 15, no. 7, 2023, doi: 10.3390/rs15071860.\n[33] E. Rodner, M. Simon, R. B. Fisher, and J. Denzler, “Fine-grained\nrecognition in the noisy wild: Sensitivity analysis of convolutional neural\nnetworks approaches,” arXiv e-prints, p. arXiv:1610.06756, Oct. 2016.\n[34] N. Park and S. Kim, “How do vision transformers work?” p.\narXiv:2202.06709, Feb. 2022.\n[35] M. M. Naseer, K. Ranasinghe, S. H. Khan, M. Hayat, F. Shahbaz Khan,\nand M. H. Yang, “Intriguing properties of vision transformers,” in\nAdvances in Neural Information Processing Systems , vol. 34, 2021, pp.\n23 296–23 308.\n[36] J. Yao, B. Zhang, C. Li, D. Hong, and J. Chanussot, “Extended\nvision transformer (exvit) for land use and land cover classifica-\ntion: A multimodal deep learning framework,” IEEE Transactions\non Geoscience and Remote Sensing , vol. 61, pp. 1–15, 2023, doi:\n10.1109/TGRS.2023.3284671.\n[37] C. Li, B. Zhang, D. Hong, J. Yao, and J. Chanussot, “Lrr-net: An inter-\npretable deep unfolding network for hyperspectral anomaly detection,”\nIEEE Transactions on Geoscience and Remote Sensing , vol. 61, pp. 1–\n12, 2023, doi: 10.1109/TGRS.2023.3279834.\n[38] R. Xia et al., “Crtranssar: A visual transformer based on contextual joint\nrepresentation learning for sar ship detection,” Remote Sensing, vol. 14,\nno. 6, 2022, doi: 10.3390/rs14061488.\n[39] Z. Liu et al., “Swin transformer: Hierarchical vision transformer using\nshifted windows,” in Proceedings of the IEEE/CVF International Con-\nference on Computer Vision (ICCV) , 2021, pp. 10 012–10 022.\n[40] K. Li, M. Zhang, M. Xu, R. Tang, L. Wang, and H. Wang, “Ship\ndetection in sar images based on feature enhancement swin transformer\nand adjacent feature fusion,” Remote Sensing, vol. 14, no. 13, 2022, doi:\n10.3390/rs14133186.\n[41] H. Shi, B. Chai, Y . Wang, and L. Chen, “A local-sparse-information-\naggregation transformer with explicit contour guidance for sar\nship detection,” Remote Sensing , vol. 14, no. 20, 2022, doi:\n10.3390/rs14205247.\n[42] Y . Zhou, F. Zhang, Q. Yin, F. Ma, and F. Zhang, “Inshore dense\nship detection in sar images based on edge semantic decoupling and\ntransformer,” IEEE Journal of Selected Topics in Applied Earth Ob-\nservations and Remote Sensing , vol. 16, pp. 4882–4890, 2023, doi:\n10.1109/JSTARS.2023.3277013.\n[43] Y . Zhou, X. Jiang, G. Xu, X. Yang, X. Liu, and Z. Li, “Pvt-sar:\nAn arbitrarily oriented sar ship detector with pyramid vision trans-\nformer,” IEEE Journal of Selected Topics in Applied Earth Observations\nand Remote Sensing , vol. 16, pp. 291–305, 2023, doi: 10.1109/JS-\nTARS.2022.3221784.\n[44] W. Wang et al., “Pyramid vision transformer: A versatile backbone for\ndense prediction without convolutions,” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV) , 2021, pp. 568–\n578.\n[45] A. Dosovitskiy et al. , “An image is worth 16x16 words: Transformers\nfor image recognition at scale,” p. arXiv:2010.11929, Oct. 2020.\n[46] W. Wang et al. , “Pvt v2: Improved baselines with pyramid vision\ntransformer,” Computational Visual Media , vol. 8, no. 3, pp. 415–424,\n2022, doi: https://doi.org/10.1007/s41095-022-0274-8.\n[47] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) , 2016.\n[48] Q. Hou, L. Zhang, M. Cheng, and J. Feng, “Strip pooling: Rethinking\nspatial pooling for scene parsing,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), 2020.\n[49] T. Y . Lin, P. Dollar, R. Girshick, K. He, B. Hariharan, and S. Belongie,\n“Feature pyramid networks for object detection,” in Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) ,\n2017.\n[50] Y . Gong, X. Yu, Y . Ding, X. Peng, J. Zhao, and Z. Han, “Effective fusion\nfactor in fpn for tiny object detection,” in Proceedings of the IEEE/CVF\nWinter Conference on Applications of Computer Vision (WACV) , 2021,\npp. 1160–1168.\n[51] J. Dai et al. , “Deformable convolutional networks,” in Proceedings of\nthe IEEE International Conference on Computer Vision (ICCV) , 2017.\n[52] X. Zhu, H. Hu, S. Lin, and J. Dai, “Deformable convnets v2: More\ndeformable, better results,” in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR) , 2019.\n[53] Z. Tian, C. Shen, H. Chen, and T. He, “Fcos: A simple and strong\nanchor-free object detector,” IEEE Transactions on Pattern Analysis\nand Machine Intelligence , vol. 44, no. 4, pp. 1922–1933, 2022, doi:\n10.1109/TPAMI.2020.3032166.\n[54] T. Y . Lin, P. Goyal, R. Girshick, K. He, and P. Dollar, “Focal loss\nfor dense object detection,” in Proceedings of the IEEE International\nConference on Computer Vision (ICCV) , 2017.\n[55] D. Zhou et al. , “Iou loss for 2d/3d object detection,” in 2019 In-\nternational Conference on 3D Vision (3DV) , 2019, pp. 85–94, doi:\n10.1109/3DV .2019.00019.\n[56] S. Sukhbaatar, J. Bruna, M. Paluri, L. Bourdev, and R. Fergus,\n“Training convolutional networks with noisy labels,” arXiv e-prints , p.\narXiv:1406.2080, Jun. 2014.\n[57] T. Zhang et al., “Sar ship detection dataset (ssdd): Official release and\ncomprehensive data analysis,” Remote Sensing , vol. 13, no. 18, 2021,\ndoi: 10.3390/rs13183690.\n[58] C. Xu et al. , “Rsdd-sar: Rotated ship detection dataset in sar im-\nages,” Journal of Radars , vol. 11, no. R22007, p. 581, 2022, doi:\n10.12000/JR22007.\n[59] Y . Zhou et al. , “Mmrotate: A rotated object detection benchmark\nusing pytorch,” in Proceedings of the 30th ACM International Confer-\nence on Multimedia , New York, NY , USA, 2022, p. 7331–7334, doi:\n10.1145/3503161.3548541.\n[60] T. Y . Lin et al. , “Microsoft coco: Common objects in context,” in\nComputer Vision – ECCV 2014 , 2014, pp. 740–755.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3325573\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 17\n[61] M. Everingham and J. Winn, “The pascal visual object classes challenge\n2012 (voc2012) development kit,” Pattern Anal. Stat. Model. Comput.\nLearn., Tech. Rep, vol. 2007, no. 1-45, p. 5, 2012.\n[62] S. Liu, L. Qi, H. Qin, J. Shi, and J. Jia, “Path aggregation network\nfor instance segmentation,” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR) , 2018.\n[63] M. Tan, R. Pang, and Q. V . Le, “Efficientdet: Scalable and efficient\nobject detection,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR) , 2020.\n[64] X. Xie, G. Cheng, J. Wang, X. Yao, and J. Han, “Oriented r-cnn\nfor object detection,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV) , 2021, pp. 3520–3529.\n[65] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time\nobject detection with region proposal networks,” in Advances in Neural\nInformation Processing Systems , vol. 28, 2015.\n[66] J. Han, J. Ding, N. Xue, and G. Xia, “Redet: A rotation-equivariant\ndetector for aerial object detection,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), 2021,\npp. 2786–2795.\n[67] X. Yang, J. Yan, Z. Feng, and T. He, “R3det: Refined single-stage\ndetector with feature refinement for rotating object,” Proceedings of the\nAAAI Conference on Artificial Intelligence , vol. 35, no. 4, pp. 3163–\n3171, May 2021, doi: 10.1609/aaai.v35i4.16426.\n[68] J. Han, J. Ding, J. Li, and G. Xia, “Align deep features for oriented object\ndetection,” IEEE Transactions on Geoscience and Remote Sensing ,\nvol. 60, pp. 1–11, 2022, doi: 10.1109/TGRS.2021.3062048.\n[69] J. Ge, Y . Tang, K. Guo, Y . Zheng, H. Hu, and J. Liang, “Keyship:\nTowards high-precision oriented sar ship detection using key points,”\nRemote Sensing, vol. 15, no. 8, 2023, doi: 10.3390/rs15082035.\n[70] K. Zhou et al. , “Arbitrary-oriented ellipse detector for ship detection\nin remote sensing images,” IEEE Journal of Selected Topics in Applied\nEarth Observations and Remote Sensing , vol. 16, pp. 7151–7162, 2023,\ndoi: 10.1109/JSTARS.2023.3267240.\n[71] Q. Liu, D. Li, R. Jiang, S. Liu, H. Liu, and S. Li, “Mt-fanet: A\nmorphology and topology-based feature alignment network for sar\nship rotation detection,” Remote Sensing , vol. 15, no. 12, 2023, doi:\n10.3390/rs15123001.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3325573\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8189352750778198
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7240579128265381
    },
    {
      "name": "Synthetic aperture radar",
      "score": 0.6910290122032166
    },
    {
      "name": "Computer vision",
      "score": 0.5877189636230469
    },
    {
      "name": "Convolutional neural network",
      "score": 0.509753406047821
    },
    {
      "name": "Object detection",
      "score": 0.4698081612586975
    },
    {
      "name": "Transformer",
      "score": 0.4437372088432312
    },
    {
      "name": "Feature extraction",
      "score": 0.4155101776123047
    },
    {
      "name": "Pyramid (geometry)",
      "score": 0.41105639934539795
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3818102777004242
    },
    {
      "name": "Engineering",
      "score": 0.07081064581871033
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210137199",
      "name": "Aerospace Information Research Institute",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I19820366",
      "name": "Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210136246",
      "name": "China Telecom (China)",
      "country": "CN"
    }
  ]
}