{
  "title": "MoqaGPT : Zero-Shot Multi-modal Open-domain Question Answering with Large Language Model",
  "url": "https://openalex.org/W4389520156",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2104362018",
      "name": "Le Zhang",
      "affiliations": [
        "Mila - Quebec Artificial Intelligence Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2111092269",
      "name": "Yihong Wu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3103285559",
      "name": "Fengran Mo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2777189136",
      "name": "Jian-Yun Nie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117267436",
      "name": "Aishwarya Agrawal",
      "affiliations": [
        "Mila - Quebec Artificial Intelligence Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4361866031",
    "https://openalex.org/W4366566341",
    "https://openalex.org/W3035140194",
    "https://openalex.org/W4377866048",
    "https://openalex.org/W2185175083",
    "https://openalex.org/W4312643954",
    "https://openalex.org/W4294534134",
    "https://openalex.org/W4287214436",
    "https://openalex.org/W2998536339",
    "https://openalex.org/W2964118342",
    "https://openalex.org/W4386566832",
    "https://openalex.org/W3101082165",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4384615637",
    "https://openalex.org/W4285249364",
    "https://openalex.org/W4323717348",
    "https://openalex.org/W4385573236",
    "https://openalex.org/W2045257906",
    "https://openalex.org/W2963143606",
    "https://openalex.org/W3099944244",
    "https://openalex.org/W3118668786",
    "https://openalex.org/W4320165837",
    "https://openalex.org/W4390872747",
    "https://openalex.org/W2131391163",
    "https://openalex.org/W4311997173",
    "https://openalex.org/W4386065691"
  ],
  "abstract": "Multi-modal open-domain question answering typically requires evidence retrieval from databases across diverse modalities, such as images, tables, passages, etc. Even Large Language Models (LLMs) like GPT-4 fall short in this task. To enable LLMs to tackle the task in a zero-shot manner, we introduce MoqaGPT, a straightforward and flexible framework. Using a divide-and-conquer strategy that bypasses intricate multi-modality ranking, our framework can accommodate new modalities and seamlessly transition to new models for the task. Built upon LLMs, MoqaGPT retrieves and extracts answers from each modality separately, then fuses this multi-modal information using LLMs to produce a final answer. Our methodology boosts performance on the MMCoQA dataset, improving F1 by +37.91 points and EM by +34.07 points over the supervised baseline. On the MultiModalQA dataset, MoqaGPT surpasses the zero-shot baseline, improving F1 by 9.5 points and EM by 10.1 points, and significantly closes the gap with supervised methods. Our codebase is available at https://github.com/lezhang7/MOQAGPT.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1195–1210\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nMOQAGPT: Zero-Shot Multi-modal Open-domain Question Answering\nwith Large Language Models\nLe Zhang1,2, Yihong Wu2, Fengran Mo2, Jian-Yun Nie2, Aishwarya Agrawal1,2\n1 Mila - Québec AI Institute\n2 Université de Montréal\n{le.zhang,aishwarya.agrawal}@mila.quebec\nAbstract\nMulti-modal open-domain question answer-\ning typically requires evidence retrieval from\ndatabases across diverse modalities, such as\nimages, tables, passages, etc. Even Large Lan-\nguage Models (LLMs) like GPT-4 fall short in\nthis task. To enable LLMs to tackle the task in a\nzero-shot manner, we introduceMOQA GPT1, a\nstraightforward and flexible framework. Using\na divide-and-conquer strategy that bypasses in-\ntricate multi-modality ranking, our framework\ncan accommodate new modalities and seam-\nlessly transition to new models for the task.\nBuilt upon LLMs, MOQA GPT retrieves and ex-\ntracts answers from each modality separately,\nthen fuses this multi-modal information using\nLLMs to produce a final answer. Our method-\nology boosts performance on the MMCoQA\ndataset, improving F1 by +37.91 points and EM\nby +34.07 points over the supervised baseline.\nOn the MultiModalQA dataset, MOQA GPT\nsurpasses the zero-shot baseline, improving F1\nby 9.5 points and EM by 10.1 points, and signif-\nicantly closes the gap with supervised methods.\n1 Introduction\nLarge Language Models (LLMs) including Chat-\nGPT (OpenAI, 2022b), LLaMA (Touvron et al.,\n2023), PaLM2 (Anil et al., 2023), and the recently\ndeveloped GPT4 (OpenAI, 2022c), have fundamen-\ntally transformed the manner in which humans in-\nteract with machines. Due to their vast knowledge\nrepositories and chain-of-thought reasoning capa-\nbility (Wei et al., 2023), these models have proven\nto be capable of providing answers to a broad range\nof questions across domains, without the need for\ntraining on specific tasks. Nevertheless, these mod-\nels face two significant challenges. First is the issue\nof hallucination (Li et al., 2023), attributable to\nthe fact that LLMs store their knowledge in their\nparameters. Hallucination can seriously hamper\n1Our codebase is available at https://github.com/\nlezhang7/MOQAGPT.\nWhere was the movie The Shape of Water made?\nQuestion\n[Table 1] 2000 Suspicious River Ball cap\n[Table 2] 1997 Waterloo Short film 2006 Driving\n[Table 3] 1985 The Color Purple Nominated\n[Table 4] 1992 Cool Worldcharacter designercharacter2004 Shrek 2 story artist2004 Shark Tale additional\n[Table 5] 2012 Beasts of the …2013 Drinking Buddies … 2014 The Fault in …\n[Passage 1] Dark Water is a 2005 American horror drama \n[Passage 2] The Shape of Water\" was the DFWFCA\\'s most awarded film of 2017, \n[Passage 3] Dark Water is a 2005 American horror drama …\n[Passage 4] The Shape of Water\" is a 2017 American …\n[Passage 5] Filming began on August 15, 2016, in Toronto and Hamilton, Ontario …\nLibrary\nCandidatesInvalid answerRelated answer\nA_tab1: UnknowA_tab2: UnknowA_tab3: UnknowA_tab4: UnknowA_tab5: Unknow\nA_txt1:The movie The Shape of Water was made in Toronto.A_txt2: The Shape of Water was filmed in London, Hertfordshire, and at Leavesden Studios in EnglandA_txt3: UnknowA_txt4: UnknowA_txt5: Unknow\nA_img1: In CanadaA_img2: In IndiaA_img3: In CanadaA_img4: In CanadaA_img5: In US\nCanada, India, US, Toronto,London\nToronto\nSearch & Reasoning\nRetrieveThinkingCandidatesInvalid answerRelated answer\nEvidence\nFigure 1: An illustration of how human adopt divide-\nand-conquer strategy to answer multimodal open-\ndomain question\nthe accuracy and reliability of question answering,\nas it can introduce plausible, yet incorrect informa-\ntion, thus exacerbating the problem. Second, while\nLLMs are designed to process the text modality\nonly, there are numerous other non-textual sources\nof information, such as images, videos, audios,\nand tables, that could provide suitable answers to\nmost real-world questions. Some queries may even\nrequire a synthesis of information from across dif-\nferent modalities for accurate responses. Thus, the\ninability to process non-textual inputs restricts the\neffectiveness of current LLMs.\nConsider an example outlined in fig. 1 where\nthe question asked is, \"Where was the movie ‘The\nShape of Water’ made?\" . To effectively answer\nthis question, a human would employ a divide-\nand-conquer approach. This strategy involves first\nretrieving relevant documents such as the movie\nposter, news reports, and table records for The\nShape of Water . The individual would then de-\nrive the answer from the obtained references that\nmight include terms like Canada, Toronto, or Lon-\ndon, Hertfordshire in England. Comprehensive rea-\n1195\nSystemTraining  File\nMulti-modality Encoder & Ranker\nQuestionEncoder\nTableEncoder\nImageEncoder\n TextEncoder\n Modality Detection\nMulti-modality Question AnswerExtra InformationExtract & EncodeSpecialModule\nLarge Multi-modal Open-domain QA AnnotationsLarge Multi-modal Alignment DatasetSpecial Annotations ( Graph, NER, Relation, etc. )QuestionType Annotations\nOld Supervised Paradigm\nNo task-specific Annotations needed anymore !Addasmuch modality as possible!\nZero-shot Powered by LLM\nLLMtofuse information and reason answer\nModality-Independently Retrieval and QA \n…\nFigure 2: Comparison of two paradigmns for multimodal open-domain question answering.Fire symbol\nindicates modules require training, ice symbol indicates forzen models.\nsoning would be applied to all potential answers,\nfor instance, recognizing that Toronto is related\nto Canada. Given this relationship, and the lack\nof strong ties between London, Hertfordshire in\nEngland and other candidate answers, Toronto is\ndeemed the most probable answer and is selected\nas the final response.\nHowever, many existing models rely on ajoint\nstrategy (Liu et al., 2023; Li et al., 2022; Yang et al.,\n2022; Chen et al., 2022) where they attempt to re-\ntrieve and rank all modalities by training a joint\nembedding space. This approach, despite its appli-\ncation, has several shortcomings. Firstly, the joint\nstrategy lacks flexibility, necessitating retraining\nwhen new models and modalities are introduced.\nSecondly, it poses considerable difficulties to train\na joint embedding space and rank references en-\ncompassing more than two modalities. Although\nsome divide-and-conquer models have been pro-\nposed (Talmor et al., 2021), they also come with\nlimitations. These models require training a ques-\ntion type classifier, followed by the training of var-\nious question-answering models. Each of these\nstages requires a significant amount of annotations,\nthus presenting a considerable challenge in their\ndevelopment and implementation.\nTo enable LLMs to solve this task in a zero-shot\nmanner, we propose Multi-modal Open-domain\nQuestion Answering GPT ( MOQA GPT ). MO-\nQAGPT utilizes a divide-and-conquer approach\nand employs robust models to extract answers from\nvarious modalities. It further leverages LLMs as a\nreasoning mechanism, applying in-context learning\nto process the extracted information and generate\nthe final response. Compared with the traditional\nsupervised methods, our framework, as depicted in\nfig. 2, has three main advantages: Flexibility: The\nMOQA GPT operates in zero-shot mode without re-\nlying on joint representation or inference, which al-\nlows easy replacement of individual modules with\nmore advanced ones as they become available. Fur-\nthermore, it can accommodate a wider range of\nmodalities and eliminates the need to curate a mul-\ntimodal open-domain dataset for training. Trust-\nworthiness: The framework’s responses are based\non the retrieved results. Consequently, each an-\nswer can be traced back to its original source, thus\nmaking the model more trustworthy and reducing\nthe risk of hallucination. Interpretability: All in-\ntermediate outputs, including the retrieval results,\ncandidate answers, and the final reasoning for an-\nswer synthesis, are produced in natural language,\nrendering the process of answering open-domain\nquestions transparent and interpretable.\nWe corroborate these advantages by present-\ning experimental results on two multi-modal open-\ndomain question answering (MMOQA) datasets:\nMMCoQA (Li et al., 2022) and MultModalQA\n(Talmor et al., 2021). Both datasets require ques-\ntions to be answered based on information retrieved\nfrom text, image, and table references. We con-\nducted experiments using several of the latest mod-\nels and demonstrated that our method is effective\nacross all of them, highlighting our framework’s\nflexibility. To demonstrate the trustworthiness of\nour method, we compared our outputs to those pro-\nduced by directly querying LLMs. Our outputs\nare less prone to hallucination, making them more\ntrustworthy. Lastly, we examined several success\nand failure cases of our method. Thanks to the inter-\npretable nature of our framework, we could identify\nthe sources of errors. Overall, our method exhibits\n1196\nrobust zero-shot performance on both datasets, un-\nderscoring its promising potential.\nOur contributions in this paper are threefold: (1)\nWe propose MOQA GPT , a simple and effective\nframework, which is the first to enable LLMs to\ntackle multi-modal open-domain queries in a zero-\nshot setting. (2) We conduct extensive experiments\ninvolving multiple LLMs and Vision-Language\nModels (VLMs), thus validating the effectiveness\nof our approach. (3) We present empirical evidence\nthat LLMs are capable of efficiently addressing\nMMOQA tasks when paired with other modali-\nties. Furthermore, we demonstrate that replacing\neach module with its superior version enhances\nperformance, establishing this as a foundational\nframework for future zero-shot question-answering\nsystems.\n2 Related Work\nMulti-modal Open-domain QA MMOQA rep-\nresents a challenging yet realistic task that is cru-\ncial for all future automated question-answering\nsystems. This task necessitates the retrieval of\npertinent references, after which answers are ex-\ntracted from these references. This often involves\ncomplex processes such as modality selection and\ncross-modal reasoning. In light of this, several\ndatasets have been introduced to benchmark the de-\nvelopment of solutions in this area, such as Many-\nModalQA (Hannan et al., 2020), HYBRIDQA\n(Chen et al., 2020), WebQA (Chang et al., 2022),\nMultiModalQA (Talmor et al., 2021), and MM-\nCoQA (Li et al., 2022).\nEarlier works in this field focus on model train-\ning. These include methodologies for joint embed-\nding of multiple modalities (e.g. MAE (Li et al.,\n2022) and ManyModelQA (Hannan et al., 2020)),\nthe structured knowledge and unified retrieval-\ngeneration based method SKURG (Yang et al.,\n2022), and the Multimodal Graph Transformer\n(He and Wang, 2023), which employs a graph-\nbased quasi-attention mechanism for integrating\nmulti-modal graph information. To the best of our\nknowledge, we are the first to introduce a zero-shot\nmethod for multi-modal open-domain question an-\nswering, marking a significant contribution.\nLLM-based Modular Systems The develop-\nment of modular neural networks, rooted in biol-\nogy and neuroscience, can be traced back to the\n1990s (Azam, 2000; Auda and Kamel, 1999). Be-\nfore the rise of LLMs, modular neural networks\nlike (Andreas et al., 2016, 2015) aimed to handle\ncompositional tasks. They did this by decompos-\ning them into sub-tasks, utilizing off-the-shelf lan-\nguage parsers, and then learning specialized neural\nmodules for each. However, their applicability was\nlimited, being constrained by parser performance\nand the need for hand-specified module types.\nThe emergence of LLMs has renewed interest\nin this area. LLMs address the parsing challenges\nwithout necessitating additional training. Conse-\nquently, this led to the proposition of various LLM-\nbased systems targeting an array of compositional\nreasoning challenges. Examples include:\nToolformer (Schick et al., 2023), which trains\nlanguage models to select tools. Visual ChatGPT\n(Wu et al., 2023), HuggingGPT (Shen et al., 2023),\nand Chameleon (Lu et al., 2023), all utilizing GPT\nto deduce tool sequences for response generation.\nViperGPT (Surís et al., 2023) and Visprog (Gupta\nand Kembhavi, 2022), leveraging Codex and GPT3\n(Brown et al., 2020) respectively to produce python\nprograms for visual reasoning tasks. Yet, these\nmethods don’t address MMOQA. MMOQA inher-\nently involves multiple steps, making it apt for a\nmodular approach. Our framework, therefore, cap-\nitalizes on LLMs to integrate and reason about\ninformation retrieved from various modalities for\nMMOQA. Our approach is distinct as it requires\nno training, setting it apart from prior works like\nSchick et al. (2023). It also differentiates itself\nfrom research such as Surís et al. (2023); Gupta\nand Kembhavi (2022); Shen et al. (2023) with its\nemphasis on retrieval-based question answering.\nAlthough Chameleon (Lu et al., 2023) supports\nboth retrieval and question answering, it doesn’t\naddress the MMOQA tasks, especially those need-\ning cross-modal information integration and rea-\nsoning. Moreover, our method operates in a zero-\nshot setting, avoiding the need for intermediate\nprograms, unlike Chameleon which requires a few-\nshot Python intermediate program.\n3 M OQAGPT\nMOQA GPT presents a general approach to gener-\nate answers to queries using a multi-modal knowl-\nedge base collection C, which encompasses text\nCtxt, tables Ctab, and images Cimg. The divide-\nand-conquer strategy is accomplished through a\ntwo-stage process.\nFirst, the Multi-modal Question Answer Extrac-\ntion stage (§3.1) extracts answer candidates from\n1197\nStage1: Zero-shot Multimodal Question Answer Extraction\nQuestion\nStage2: Zero-shot Answers Infusion\nAnswer_Candidates =[A_IMG, A_TEXT, A_TAB , \nA_GPT]Prompt=  Given the question {Question}, please select the best answer from the following candidates {Answer_Candidates}:\nVLMs\nLLMs\nText &Table QA\nGPTs\nDirect QA\nRule-based Strategy\nTable KB\nImage KB\nVisual QA\nImage Retriever\nText RetrieverText KB\nTable Retriever\n Re-extract answerGPTs as Reasoner\nFinal Answer\nFigure 3: Overview of MOQAGPT. Snow symbol indicates the model is frozen.\ndifferent modalities of the knowledge base inde-\npendently and utilizes rule-based strategies to sift\nthrough these responses. Second, the Answers In-\nfusion stage (§3.2) employs LLMs’ reasoning abil-\nities to integrate information from across various\nmodalities and select the most plausible answer. It\nis noteworthy that MOQA GPT operates with ex-\nisting models and requires no additional training,\nthus exhibiting zero-shot inference capabilities. A\ncomprehensive depiction of this methodology is\nprovided in fig. 3, and the prompts used are de-\nscribed in table 1. It’s worth noting that we do not\nperform prompt engineering due to the API cost\nincurred.\n3.1 Multi-modal Question Answer Extraction\nIn this stage, queries are answered independently\nfor each modality, and a strategy is employed to re-\nfine the responses. It’s important to emphasize that\nevery retrieval modality and question-answering\nmodel within our framework is frozen and can be\ninterchanged. We delve into the details of this stage\nin the subsequent sections.\n3.1.1 Retrieval\nDifferent modalities utilize pre-established, highly\neffective models for retrieval, eliminating the need\nto map all modalities into a joint embedding space.\n(i) For Text Retrieval, we use the ANCE model\n(Xiong et al., 2020), which adopts a dense retrieval\napproach to encode both passages and queries. Re-\ntrieval is then based on the cosine similarity be-\ntween these encoded representations. (ii) For Im-\nage Retrieval, the renowned CLIP model (Radford\net al., 2021) is employed. This zero-shot retriever,\ntrained on web-crawled caption-image pairs us-\ning an image-text contrastive loss, has shown im-\npressive performance in image-text retrieval bench-\nmarks (Lin et al., 2014; Young et al., 2014). The\nsimilarity between queries and images for retrieval\nis determined through their inner product. (iii) For\nTable Retrieval, tables are typically converted into\na textual format, and then language models are\nutilized for encoding (Herzig et al., 2020). Fol-\nlowing this protocol, we employ OpenAI’s Ada\n(OpenAI, 2022a), a robust embedding model, to\nencode linearized tables and queries, with similar-\nity measured using the inner product. The retrieval\nprocess can be expressed as:\nRimg = ImageRetriever(q, Cimg)\nRtxt = TextRetriever(q, Ctxt)\nRtab = TableRetriever(q, linearize(Ctab))\nwhere Rrepresents the retrieved references, q is\nthe question, and Cis the knowledge collection.\n3.1.2 Question Answering\nUpon retrieving the references, the next step is to\nextract an answer from each reference based on\nthe question q. (i) For Visual QA, we use vision-\nlanguage models (VLMs) with robust zero-shot\ncapabilities to generate the responses. In this step,\nthe question is fed into these VLMs using a simple\nprompt: ‘Question: Q Answer:’ . (ii) Since table\ndata can be linearized into text, both Textual QA\nand Tabular QA can be tackled with a single LLM.\nAs we’re aiming for extractive question answering,\nthe final response should be a concise text span\nfrom the provided input. We direct the LLMs with\na specific prompt (refer to Prompt QA in table 1)\nfor this purpose.\nMoreover, we found that some questions could\nbe addressed directly by prompting LLMs. Hence,\n1198\nName Prompt\nPrompt QA\nYou are performing extractive question answering. Given the document: {reference} ,\nextract a short answer to the question: {Q} from the document. If insufficient\ninformation is available to answer the question, respond with ‘Unknown’. The\nanswer should be one or two words long.\nPrompt Direct-QA Question: {questions}. Please provide a concise response,\nlimited to one or two words, No explanation and further question. Answer:\nPrompt Answer-FusionGiven question {Q}, please select the best answer from the following candidates: {Candidates}\nPrompt Re-extract Given the question {Q}, please extract the answer span from {final answer}, without\nproviding additional sentences or explanations. The response should be a single word.\nTable 1: Prompts used in MOQAGPT\nwe also incorporate answers from (iii) Direct QA,\nwhere responses are obtained by directly querying\nthe LLMs using a prompt (see Prompt Direct-QA\nin table 1). The overall process can be represented\nas:\nAi\nimg = VLM(q, ri\nimg)\nAi\ntxt = LLM(PromptQA(q, ri\ntxt))\nAi\ntab = LLM(PromptQA(q, linearize(ri\ntab)))\nAdirect = LLM(PromptDirectQA(q))\nwhere ri signifies the ith reference from R.\n3.1.3 Rule-based Strategy\nAt this stage, we possess answer candidates de-\nrived from various modalities. Nonetheless, the\nautoregressive generation of responses by LLMs\nand VLMs can sometimes produce invalid outputs,\nsuch as \"sorry, I can’t \". Through empirical obser-\nvation, we identified that: (i) The VLMs tend to\nconsistently produce answers, even if relevant in-\nformation is missing. (ii) The most accurate answer\nisn’t necessarily found in the top-1 (most similar)\nretrieved reference. (iii) Using our prompts, LLMs\ncan discern when to provide a specific answer and\nwhen to default to \"unknown\", especially when the\navailable information is insufficient.\nWith these insights, we crafted a task-agnostic,\nrule-based strategy to filter out invalid spans and\nprioritize the most likely answers generated by the\nLLMs and VLMs: (1) If the direct answer, Adirect,\nis found within any of the sets Aimg, Atxt, Atab,\nit’s deemed reliable and is chosen as the final an-\nswer. (2) Any answer containing phrases like \"un-\nknown\" or \"sorry\" is discarded. (3) Rather than\nexclusively relying on the top-1 retrieved reference,\nwe choose the most frequent response from the top-\nK retrieved references. If all responses are distinct,\nwe opt for the response from the top-1 reference.\nThese rules are enforced in sequence. If\nrule 1 isn’t satisfied, we will have a curated set\nof valid answer candidates, denoted as ˜A =\n˜Aimg, ˜Atxt, ˜Atab, ˜Adirect. This set will then be\nused to pinpoint the final answer span by the rea-\nsoner, detailed in the subsequent section.\n3.2 Answer Infusion\nFor the majority of queries, even though it’s hard\nto decide which modality contains the answer, the\nformat of the answer is usually predetermined. For\ninstance, a question like “What color is the Santa\nAnita Park logo?\" should yield a color as the an-\nswer, not a date or a name. Inspired by this ob-\nservation, we leverage LLMs to infer the correct\nanswer format and select the appropriate answer\nfrom the candidates. To achieve this, we designed\na prompt (refer to Prompt Answer-Fusion in table\n1) that enables LLMs to determine the final answer.\nAs the gold-standard answers are typically short,\nif the final answer contains more than three words,\nwe guide LLMs to select the correct text span using\nthe Prompt Re-extract.\n4 Experiments & Results\nThis section is organized as follows. In §4.1, we\noutline MMOQA datasets, metrics, and baselines.\nWe then discuss retrieval performance in §4.2, ques-\ntion answering in §4.3, and MMOQA results in\n§4.4. Lastly, we present an ablation study in §4.5\nfollowed by a detailed case study in §4.6.\n4.1 Implementation details\nDataset and Metrics. We evaluate our method\non two MMOQA datasets (refer to table 2 for\ndataset statistics). Though they share the same\nreferences from tables, text, and images, they have\ndifferent settings, questions, and answers. In all our\nexperiments, we utilize only the top 5 references\nper modalityfrom the knowledge collection.\n1199\nDataset #Questions #Images #Tables #Texts\nMMCoQA 590 57,058 10,042 218,285\nMultiModalQA 2441 57,058 10,042 218,285\nTable 2: Dataset statistics\nThe MMCoQA dataset (Li et al., 2022) evaluates\na model’s proficiency in identifying the appropriate\nanswer modality. Each question is uniquely tied\nto a specific modality. While the dataset employs\nconversational structures with historical context,\nwe only utilized the gold question for all test set\nexperiments to emphasize a non-conversational ap-\nproach.\nThe MultiModalQA dataset (Talmor et al., 2021)\nis designed for multi-modal comprehension and\nmulti-hop reasoning QA. In alignment with prior\nstudies (Li et al., 2022; Talmor et al., 2021; Yang\net al., 2022), we test on the development set (the\ntest set is unlabeled and no online evaluation is\navailable), using Exact Match and F1 as evaluation\nmetrics. The MultiModalQA dataset already pro-\nvides 1-15 reference candidates for each modality\nper question.\nBaselines To the best of our knowledge, our ap-\nproach is the first to enable LLMs to perform zero-\nshot MMOQA. For zero-shot baselines, we select\nDirect QA by Vicuna, OpenChat, Llama2, Chat-\nGPT, and GPT4 2. Additionally, we benchmark\nour results against supervised methods: (i) For\nthe MMCoQA dataset, we compare with the previ-\nous state-of-the-art models, MAE (Li et al., 2022),\na joint embedding model trained on MMOQA\ndatasets, and the ManyModelQA model (Hannan\net al., 2020). These are the only models reported\nfor this dataset. (ii) For the MultiModalQA dataset,\nwe draw comparisons with the previous SOTA\nmodel SKURG (Yang et al., 2022), a structured\nknowledge and unified retrieval generation-based\nmethod, and the Multimodal Graph Transformer\n(He and Wang, 2023), a model that employs a\ngraph-based quasi-attention mechanism to inte-\ngrate multi-modal graph information.\n4.2 Retrieval Results\nFor the MultiModalQA dataset, previous works\ndirectly use the gold reference set without any\nretrieval. We perform retrieval on the provided\nreference candidates (1-15 per modality) and find\nthat Recall@5 was consistently 100%. As a re-\nsult, we present the retrieval results for MMCoQA\n2Details are described in Appendix appendix B.\nModality MRR NDCG Recall@5 Recall@2000\nJoint (NDCG@2000)\nORConvQA† - 2.3 - 19.1\nMAE† - 6.1 - 63.4\nDivide-and-Conquer\nImage(CLIP) 31.2 34.0 40.8 -\nTable(Ada) 50 53.9 65.5 -\nText(ANCE) 35.6 39.0 49.0 -\nOverall - - 51.0 -\nTable 3: Retrieval results for MMCoQA† represents quoted\nresults. Joint NDCG are computed for 2000 items. Ques-\ntions are classified into categories based on the gold reference\nmodality, scores are computed for each modality indepen-\ndently. Overall result is computed on concatenated references\nfrom all modalities, which can be viewed as Recall@5x3.\nonly. Each question has a designated gold refer-\nence modality, and we group questions based on\nthis attribute to report a breakdown of results across\nmodalities in table 3. This evaluation focuses solely\non the retrieval of candidates.\nThe table indicates that our divide-and-conquer\napproach provides significant benefits in terms of\neffectiveness compared to the joint method. The\nprior state-of-the-art methodology, MAE, trains\nknowledge encoders for tables, images, and tex-\ntual documents. Once trained, these knowledge\nencoders are frozen, and a contrastive loss func-\ntion is employed to train the query encoder. This\napproach seek to align the embedding spaces of ta-\nbles, images, and textual documents via the query\nembedding, without incorporating an actual multi-\nmodality alignment. In contrast, our methodol-\nogy disentangles intricate multimodal knowledge\nrelationships by retrieving each modality indepen-\ndently and then assembling them using LLMs, elim-\ninating the need for complex ranking. It registers a\nsignificant improvement, with its Recall@5x3 (51)\nbeing close to MAE’s Recall@2000 (63.4).\n4.3 Question Answering Results\nWe conduct question answering on the retrieved\nreferences, obtaining 5 answer candidates for each\nmodality. The primary assessment criterion is to\ndetermine if the gold answers were present among\nthese 5 candidates. As such, we use Recall@5 to\nevaluate the quality of the generated answers. Addi-\ntionally, Vicuna’s outputs often appear to be noisy,\nlengthy, and non-specific. For instance, for a ques-\ntion with the gold answer \"Joss Whedon\", Vicuna\nmight produce a response like \"1. Joss Whedon 2.\nDavid Greenwalt 3. David Boreanaz 4. Unknown\",\nwhich complicates the extraction of the final an-\nswer, even if the recall score is high. This recall\n1200\nVLM LLM MMCoQA MultiModalQA\nImage Table Text Overall Single Multiple Overall\nBLIP2 Vicuna 28.6 25.5 33.9 41.5 46.8 37.0 42.6\nInstructBLIP Vicuna 31.3 25.5 33.9 42.9 50.7 38.7 45.5\nBLIP2 OpenChatV2 28.6 23.4 40.3 44.2 50.7 40.5 46.3\nInstructBLIP OpenChatV231.3 23.4 40.3 45.4 53.9 42.0 48.8\nBLIP2 Llam2 28.6 30.3 44.0 44.1 54.6 36.8 46.9\nInstructBLIP Llam2 31.3 30.3 44.0 45.1 56.7 38.4 48.8\nBLIP2 ChatGPT 28.6 35.9 48.0 45.8 56.1 35.2 47.2\nInstructBLIP ChatGPT 31.3 35.9 48.0 47.6 58.4 38.1 49.7\nTable 4: Question Answering Recall@5.We group questions based on gold reference modality to report results breakdown\nacross modalities as in table 3. Similarly, we group questions answerable by a single modality or those requiring multi-modality.\nThe overall R@5x3 are calculated based on whether the gold answer is found within concatenated 15 answer candidate\nscore represents the potential maximumperfor-\nmance, as the answers from different modalities\ncould be harmonized through LLM reasoning.\nThe results in table 4 indicate that VQA is the\nmost challenging task in MMCoQA due to its low\nrecall. In contrast, ChatGPT effectively manages\ntextual question answering for both text and lin-\nearized tables. In the context of the MultiModalQA\ndataset, multi-hop reasoning or cross-modal under-\nstanding is frequently required, rendering tasks that\ninvolve multiple modalities more demanding than\nthose relying on a single modality. Our empiri-\ncal observations shows that some questions can be\naddressed using references from different modali-\nties, achieving an overall Recall@5x3 of nearly 50\nacross both datasets.\n4.4 Multimodal Open-domain QA Results\nFollowing the rule-based strategy, valid results are\nprocessed with the Prompt-Answer Infusion and\nsubsequently reasoned by the LLM. The results are\nshown in table 5 and table 6.\nThe supervised methods on the MMCoQA\ndataset underperform, largely due to the subpar\njoint retrieval results as highlighted in table 3.\nWhen MAE is provided with a gold reference,\nthereby eliminating the retrieval step, its perfor-\nmance notably improves — witnessing a 30.64\nincrease in F1 score and a 24.58 rise in EM. This\nimplies that the main challenge lies in the retrieval\nand ranking of references. On the other hand, the\nDirect QA methods of LLMs effectively handle\nquestions involving textual references, thanks to\nModels Image Table Text Overall\nVQA Textual QA Direct QA & Reasoner F1 EM F1 EM F1 EM F1 EM\nSupervised\nORConvQA†(Qu et al., 2020) - - - - - - 1.87 1.06\nManyModelQA†(Talmor et al., 2021) - - - - - - 1.82 0.96\nMAE†(Li et al., 2022) - - - - - - 6.29 3.73\nMAE + Gold reference† - - - - - - 36.93 28.31\nDirect QA\nVicuna-7B (Chiang et al., 2023) 14.1 11.6 12.8 9.0 22.1 17.1 17.8 13.7\nOpenChat-v2-w-13b (Wang et al., 2023) 11.3 10.2 19.7 15.9 32.5 25.5 24.1 19.3\nLlama2Chat-13b 17.2 15.0 20.3 14.5 29.9 23.2 24.4 19.0\nChatGPT (OpenAI, 2022b) 17.1 14.3 25.9 21.4 45.3 35.6 33.5 26.8\nGPT4(OpenAI, 2022c) 22.0 16.3 32.1 26.2 51.2 44.0 39.2 32.7\nZero-shot MOQAGPT\nBLIP2 Vicuna-7B ChatGPT 23.4 19.7 25.0 20.7 43.6 34.6 34.0 27.5\nBLIP2 OpenChatV2-13b ChatGPT 25.9 22.4 28.0 23.4 42.8 35.2 35.0 29.2\nBLIP2 Llama2Chat-13b ChatGPT 23.9 21.1 35.8 30.3 45.3 36.9 37.7 31.4\nBLIP2 ChatGPT ChatGPT 24.8 21.1 38.9 33.1 47.5 37.6 39.7 32.4\nInstructBLIP ChatGPT ChatGPT 28.5 25.9 36.4 29.7 46.4 37.6 39.5 32.7\nInstructBLIP Llama2Chat-13b Llama2Chat-13b 22.9 18.4 31.5 26.4 41.5 34.5 34.2 27.8\nInstructBLIP ChatGPT Llama2Chat-13b 23.5 18.9 32.8 27.2 42.9 35.4 35.6 28.9\nBLIP2 ChatGPT GPT4 28.3 22.4 41.3 35.9 52.8 45.0 43.9 37.1\nInstructBLIP ChatGPT GPT4 27.6 23.1 42 35.2 53.6 46.3 44.2 37.8\nTable 5: Results on MMCoQA† represents quoted results. VQA represent models to extract answers from image, Textual QA\nrepresents models to extract answer from text and linearlized table. Direct QA & Reasoner represents model which is used to\ndirectly ask for and model to infuse information and reasoning and generate final answer\n1201\nModels Single Modality Multi Modality Overall\nVQA Textual QA Direct QA & Reasoner F1 EM F1 EM F1 EM\nSupervised\nMGT†(He and Wang, 2023) - - - - 57.7 52.1\nSKURG†(Yang et al., 2022) 70.2 66.3 56.4 51.3 63.8 59.4\nDirect QA\nVicuna-7B(Chiang et al., 2023) 20.3 17.1 16.4 11.9 18.6 14.9\nOpenChat-v2-w-13b (Wang et al., 2023) 25.3 22.0 18.9 15.5 22.5 19.2\nLlama2Chat-13b 24.6 21.3 17.3 13.0 21.5 17.7\nChatGPT (OpenAI, 2022b) 36.9 29.8 22.3 17.4 30.6 24.5\nGPT4 (OpenAI, 2022c) 42.9 36.4 27.2 23.9 36.1 31.0\nZero-shot MOQAGPT\nBLIP2 Vicuna-7B ChatGPT 41.3 34.4 24.7 20.2 34.2 28.3\nBLIP2 OpenChatV2-13b ChatGPT 40.9 34.4 24.8 20.6 34.0 28.5\nBLIP2 Llama2Chat-13b ChatGPT 43.3 36.8 27.5 23.2 36.5 31.0\nBLIP2 ChatGPT ChatGPT 43.9 37.0 26.9 22.6 36.6 30.8\nInstructBLIP ChatGPT ChatGPT 43.5 37.2 27.4 23.4 36.6 31.3\nInstructBLIP Llama2Chat-13b Llama2Chat-13b 38.5 33.4 24.6 19.8 31.4 28.0\nInstructBLIP ChatGPT Llama2Chat-13b 38.6 33.9 24.5 20.4 32.0 28.9\nBLIP2 ChatGPT GPT4 50.6 44.0 31.4 27.5 42.3 36.9\nInstructBLIP ChatGPT GPT4 54.6 49.1 33.8 30.5 45.6 41.1\nTable 6: Results on MultiModalQA† represents quoted results.\ntheir expansive knowledge storage. Yet, they falter\nfor queries demanding image references, primarily\ndue to modality limitations. Our zero-shot method\noutshines the supervised baseline because of su-\nperior retrieval, question answering, and answer\ninfusion capabilities. It also elevates the Direct\nQA approach when grounded in retrieved results,\nshowing up to a 6.0 F1 and 5.9 EM boost over\nChatGPT and a 5.0 F1 and 5.1 EM enhancement\nover GPT4. Overall, our methodology exhibits a\nsignificant improvement across all tested models.\nThe MultiModalQA dataset provides 1-15 refer-\nences for each modality, diminishing the criticality\nof retrieval from an extensive multi-modal knowl-\nedge base. Thus, our divide and conquer approach\nmight not realize its utmost potential here. Con-\nsequently, our zero-shot method trails the super-\nvised baselines. This is in stark contrast to MM-\nCoQA, where retrieval across modalities is impera-\ntive. Such foundational differences underscore the\nvaried baseline results between the two datasets.\nHowever, given that our approach operates in a\nzero-shot fashion, devoid of task-specific annota-\ntions and specialized tools like question classifiers,\nMOQA GPT notably betters zero-shot baselines and\ncloses the performance gap with supervised meth-\nods.\nAs illustrated in table 6, in comparison to Di-\nrect QA, our method boosts the overall metrics\nby 6.0 F1 and 6.8 EM over ChatGPT, and 9.5 F1\nand 10.1 EM over GPT4. The most significant\nleap comes from the Single Modality category, un-\nderscoring the efficacy of our approach for one-\nhop tasks. We also register improved scores in\nthe Multi Modality category, showcasing the abil-\nity of our GPT to amalgamate different modalities.\nPredictably, GPT4, employed for direct QA and\nreasoning, exhibits superior gains than ChatGPT\nacross both Single/Multi Modality categories. This\naligns with our hypothesis: given the task’s em-\nphasis on cross-modal reasoning, our method leans\nheavily on robust reasoning capabilities to merge in-\nformation across modalities. Thus, the more adept\nthe reasoner, the higher the performance. Moreover,\nGPTs capably filter out noise from Vicuna’s output,\nmarkedly enhancing the performance against the\ndirect QA by Vicuna for both datasets.\nIn conclusion, it’s essential to underscore that\nreal-world situations more closely mirror the MM-\nCoQA setup, where evidence isn’t readily available\nbut requires retrieval from vast repositories. In such\nscenarios, the strengths and merits of our method\nshine through, substantially surpassing supervised\nmethods, heralding broader acceptance and use.\n4.5 Ablation study\nIn our pursuit to assess the efficiency of the pro-\nposed rule-based strategy, especially its efficacy in\nnoise mitigation, we conduct experiments on MM-\nCoQA. We utilize InstrucBLIP for VQA, ChatGPT\n1202\nQuestion1: what is the competition of Gtutrttarfelagwith match against Rangers?Gold Answer: uefachampion leagueCandidates:[football, Europa league]Final Answer : Europa league\nQuestion2: What Rangers 2010-11 player has a shaved head?Gold Answer: madjidbougherraCandidates: [allan, madjid bougherra]Final Answer : allan\nQuestion3: what's the team with a logo featuring a hand holding a flaming torch?Gold Answer: new yorklibertiCandidates: [fire, indiana pacer]Final Answer : indianapacer\nQuestion4: which country was the first to land on the moonGold Answer: soviet unionCandidates: [usa, usa, usa]Final Answer : usa\nQuestion5: What are the players of Rangers 2010-11 in PreierLeague Team of the Year ?Gold Answer: allanmcgregorCandidates: [allan mcgregor, steven davi]Final Answer : allanmcgregor\nQuestion6: where was the Shape of Water filmed?Gold Answer: TorontoCandidates: [toronto ontario, Toronto]Final Answer : Toronto\nQuestion7: How many colors are in Ford's logo in 1962?Gold Answer: 2Candidates: [2, 2]Final Answer : 2\nQuestion8: How many different pictures of Copenhagen are seen here?Gold Answer: 4Candidates: [4, 4 differ picture, unclear]Final Answer : 4\nFigure 4: Cases. Blue text are Direct GPT answers. Q 1-4 are failure cases, Q 5-8 are successful cases.\nMethod Image Table Text Overall #API\n(F1/EM) (F1/EM) (F1/EM) (F1/EM) times\nMOQAGPT 27.1/23.142/35.253.6/46.344.2/37.8423\nw/o Rule126.7/22.5 41.6/34.9 52.4/44.2 43.3/36.2 590\nw/o Rule224.6/19.8 38.6/31.4 49.8/43.2 40.8/32.9 423\nw/o Rule326.6/21.4 39.4/32.7 50.2/43.9 41.8/33.4 423\nTable 7: Ablations on rule-based strategy\nfor TextualQA, and GPT-4 for Direct QA and Rea-\nsoning. Detailed findings from these experiments\nare presented in table 7.\nRule 1 proves to be essential, leading to a 23%\nreduction in GPT activations. This obviates the\nneed for reasoning over potentially noisy answers,\nthereby enhancing response accuracy and curtail-\ning inference time. The sensitivity of LLMs to\ninput noise, as underscored in Zhang et al. (2023),\nreinforces the importance of Rule 2. Excluding\nthis rule introduces detrimental noise during the\nreasoning stage, adversely affecting the outcomes,\nas corroborated by ablation studies. Rule 3, which\nrefines response selection by assessing the con-\nsensus among top references, is further validated\nthrough ablation study. Collectively, these find-\nings cement the role of our rule-based strategy as a\npivotal, optimized element, rather than just a rudi-\nmentary heuristic.\n4.6 Case Study\nFig 4 presents various instances of model perfor-\nmance. Questions 1-4 show failures: Question 1\nshows the string matching metric failing to pro-\ncess similar meanings. Question 2 illustrates the\nmodel’s inability to choose the correct answer from\nthe candidate list. Question 3 and 4 highlight the\nproposal of incorrect candidates and the existence\nof a knowledge bias in the model, respectively.\nConversely, Questions 5-8 exemplify successes:\nQuestion 5 shows hallucination errors being recti-\nfied via grounded retrieval-based answers. Ques-\ntion 6 suggests a link between retrieval-sourced\nanswers and Direct GPT responses. Question 7\ndepicts that queries can be solved through retrieval\nmethods or directly querying GPT. Lastly, Question\n8 demonstrates that our framework equips LLMs\nwith the capability to address tasks which would\ntypically confound vanilla LLMs. More examples\nto demonstrate interpretability are described in ap-\npendix C.\n5 Conclusion\nIn this study, we introduce the first zero-shot multi-\nmodal open-domain question answering frame-\nwork, MOQA GPT , which enables LLMs to per-\nform the MMOQA task. This framework is flex-\nible, accommodating new models and modalities\nwithout requiring additional training. It stands out\nfor its trustworthiness, being grounded on reliable\nretrieval results, and its interpretability, which is\nensured by transparent intermediate outcomes. By\nleveraging LLMs and VLMs, our model surpasses\nsupervised methods in MMCoQA performance and\nsignificantly narrows the gap between zero-shot\nand supervised methods in multi-hop Multimodal\nQA datasets. Furthermore, our results indicate that\nmodels without robust knowledge storage capabil-\nities, such as Vicuna, are less suited for this task.\nWe hope that our approach offers some insights and\nservers as a general and promising framework for\nmulti-modal open-domain question answering.\nLimitation\nCentral to our work is the dependency on Large\nLanguage Models (LLMs), particularly the GPT\nfamily, which being proprietary, necessitates an\nAPI call, incurring both financial and temporal\ncosts to replicate our results. While the datasets\nused in our studies incurred minimal costs (2$ and\n5$), larger datasets like WebQA could demand\n1203\nmore3. The consistent updates to the GPT version\nimply that results, while not precisely reproducible,\nshould only improve compared to those reported in\nthis paper. Furthermore, we provide results from\nopen-source LLMs, ensuring reproducibility.\nEthics Statement\nThe proposed method, MOQA GPT, offers substan-\ntial advances in the field of multi-modal open-\ndomain question answering, an area of growing\nimportance in AI. By leveraging large language\nmodels (LLMs) like GPT-4, it fosters their ability\nto handle tasks in a zero-shot manner and provides\na robust solution for extracting and ranking an-\nswers from databases encompassing a variety of\nmodalities such as images, tables, passages, etc.\nThe impact of this work extends across various\nsectors. By improving the efficiency and effective-\nness of question-answering systems, we anticipate\nthat this research will enhance user interactions\nin digital environments, streamline the retrieval of\ninformation, and significantly contribute to the de-\nvelopment of more intuitive, accessible AI tools.\nIn the education sector, for example, the frame-\nwork can be used to create more interactive learn-\ning systems, making it easier for students to extract\naccurate and comprehensive information from di-\nverse learning materials. Additionally, in business\nand research domains, it could expedite data analy-\nsis by facilitating the retrieval of relevant data from\nvast, multi-modal databases.\nWhile the enhancement in performance is no-\ntable, as with all AI technology, this research also\npresents potential societal risks. There might be\nan increased reliance on AI for answering ques-\ntions, potentially reducing critical thinking abilities\nif over-relied upon. As the proposed method can\ncope with any modality, misuse of the technology\nmight lead to privacy issues if it’s used to retrieve\nsensitive information from various modalities with-\nout consent.\nAcknowledgements\nWe are grateful to the Mila IDT team for their tech-\nnical support with the computational infrastructure.\nThe authors acknowledge the material support of\nNVIDIA in the form of computational resources.\nDuring this project, Aishwarya Agrawal was sup-\nported by the Canada CIFAR AI Chair award.\n3Pricing as of June 2023\nReferences\nJacob Andreas, Marcus Rohrbach, Trevor Darrell, and\nDan Klein. 2015. Neural module networks. 2016\nIEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 39–48.\nJacob Andreas, Marcus Rohrbach, Trevor Darrell, and\nDan Klein. 2016. Learning to compose neural net-\nworks for question answering. In North American\nChapter of the Association for Computational Lin-\nguistics.\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, Eric Chu, Jonathan H. Clark, Laurent El\nShafey, Yanping Huang, Kathy Meier-Hellstern, Gau-\nrav Mishra, Erica Moreira, Mark Omernick, Kevin\nRobinson, Sebastian Ruder, Yi Tay, Kefan Xiao,\nYuanzhong Xu, Yujing Zhang, Gustavo Hernandez\nAbrego, Junwhan Ahn, Jacob Austin, Paul Barham,\nJan Botha, James Bradbury, Siddhartha Brahma,\nKevin Brooks, Michele Catasta, Yong Cheng, Colin\nCherry, Christopher A. Choquette-Choo, Aakanksha\nChowdhery, Clément Crepy, Shachi Dave, Mostafa\nDehghani, Sunipa Dev, Jacob Devlin, Mark Díaz,\nNan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu\nFeng, Vlad Fienber, Markus Freitag, Xavier Gar-\ncia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-\nAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua\nHowland, Andrea Hu, Jeffrey Hui, Jeremy Hur-\nwitz, Michael Isard, Abe Ittycheriah, Matthew Jagiel-\nski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun,\nSneha Kudugunta, Chang Lan, Katherine Lee, Ben-\njamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li,\nJian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu,\nFrederick Liu, Marcello Maggioni, Aroma Mahendru,\nJoshua Maynez, Vedant Misra, Maysam Moussalem,\nZachary Nado, John Nham, Eric Ni, Andrew Nys-\ntrom, Alicia Parrish, Marie Pellat, Martin Polacek,\nAlex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif,\nBryan Richter, Parker Riley, Alex Castro Ros, Au-\nrko Roy, Brennan Saeta, Rajkumar Samuel, Renee\nShelby, Ambrose Slone, Daniel Smilkov, David R.\nSo, Daniel Sohn, Simon Tokumine, Dasha Valter,\nVijay Vasudevan, Kiran V odrahalli, Xuezhi Wang,\nPidong Wang, Zirui Wang, Tao Wang, John Wiet-\ning, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting\nXue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven\nZheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav\nPetrov, and Yonghui Wu. 2023. Palm 2 technical\nreport.\nGasser Auda and Mohamed Kamel. 1999. Modular\nneural networks: a survey. International journal of\nneural systems, 9(02):129–151.\nFarooq Azam. 2000. Biologically inspired modular\nneural networks. Ph.D. thesis, Virginia Polytechnic\nInstitute and State University.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\n1204\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nYingshan Chang, Mridu Narang, Hisami Suzuki, Gui-\nhong Cao, Jianfeng Gao, and Yonatan Bisk. 2022.\nWebqa: Multihop and multimodal qa.\nWenhu Chen, Hexiang Hu, Xi Chen, Pat Verga,\nand William W. Cohen. 2022. Murag: Multi-\nmodal retrieval-augmented generator for open ques-\ntion answering over images and text. ArXiv,\nabs/2210.02928.\nWenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong,\nHong Wang, and William Yang Wang. 2020. Hy-\nbridQA: A dataset of multi-hop question answering\nover tabular and textual data. In Findings of the Asso-\nciation for Computational Linguistics: EMNLP 2020,\npages 1026–1036, Online. Association for Computa-\ntional Linguistics.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nTanmay Gupta and Aniruddha Kembhavi. 2022. Vi-\nsual programming: Compositional visual reasoning\nwithout training.\nDarryl Hannan, Akshay Jain, and Mohit Bansal. 2020.\nManymodalqa: Modality disambiguation and qa over\ndiverse inputs.\nXuehai He and Xin Eric Wang. 2023. Multimodal\ngraph transformer for multimodal question answer-\ning. ArXiv, abs/2305.00581.\nJonathan Herzig, Paweł Krzysztof Nowak, Thomas\nMüller, Francesco Piccinno, and Julian Martin Eisen-\nschlos. 2020. Tapas: Weakly supervised table parsing\nvia pre-training. arXiv preprint arXiv:2004.02349.\nJunyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun\nNie, and Ji-Rong Wen. 2023. Helma: A large-scale\nhallucination evaluation benchmark for large lan-\nguage models. arXiv preprint arXiv:2305.11747.\nYongqi Li, Wenjie Li, and Liqiang Nie. 2022. Mmcoqa:\nConversational question answering over text, tables,\nand images. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 4220–4231.\nTsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir\nBourdev, Ross Girshick, James Hays, Pietro Perona,\nDeva Ramanan, C. Lawrence Zitnick, and Piotr Dol-\nlár. 2014. Microsoft coco: Common objects in con-\ntext.\nZhenghao Liu, Chenyan Xiong, Yuanhuiyi Lv, Zhiyuan\nLiu, and Ge Yu. 2023. Universal vision-language\ndense retrieval: Learning a unified representation\nspace for multi-modal retrieval.\nPan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-\nWei Chang, Ying Nian Wu, Song-Chun Zhu, and Jian-\nfeng Gao. 2023. Chameleon: Plug-and-play compo-\nsitional reasoning with large language models. ArXiv,\nabs/2304.09842.\nOpenAI. 2022a. Ada: Openai’s multi-modal research\nplatform. https://openai.com/research/ada.\nOpenAI. 2022b. Chatgpt. https://openai.com/\nresearch/chatgpt.\nOpenAI. 2022c. Gpt-4. https://openai.com/gpt-4.\nChen Qu, Liu Yang, Cen Chen, Minghui Qiu, W. Bruce\nCroft, and Mohit Iyyer. 2020. Open-retrieval con-\nversational question answering. In Proceedings of\nthe 43rd International ACM SIGIR Conference on\nResearch and Development in Information Retrieval.\nACM.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural language\nsupervision.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\nArXiv, abs/2302.04761.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. 2023. Hugging-\ngpt: Solving ai tasks with chatgpt and its friends in\nhugging face.\nDídac Surís, Sachit Menon, and Carl V ondrick. 2023.\nVipergpt: Visual inference via python execution for\nreasoning.\nAlon Talmor, Ori Yoran, Amnon Catav, Dan Lahav,\nYizhong Wang, Akari Asai, Gabriel Ilharco, Han-\nnaneh Hajishirzi, and Jonathan Berant. 2021. Mul-\ntimodalqa: Complex question answering over text,\ntables and images.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nGuan Wang, Sijie Cheng, Qiying Yu, and Changling Liu.\n2023. OpenChat: Advancing Open-source Language\nModels with Imperfect Data.\n1205\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2023. Chain-of-thought prompting elic-\nits reasoning in large language models.\nChenfei Wu, Sheng-Kai Yin, Weizhen Qi, Xiaodong\nWang, Zecheng Tang, and Nan Duan. 2023. Visual\nchatgpt: Talking, drawing and editing with visual\nfoundation models. ArXiv, abs/2303.04671.\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul N Bennett, Junaid Ahmed, and\nArnold Overwijk. 2020. Approximate nearest neigh-\nbor negative contrastive learning for dense text re-\ntrieval. In International Conference on Learning\nRepresentations.\nQian Yang, Qian Chen, Wen Wang, Baotian Hu, and\nMin Zhang. 2022. Enhancing multi-modal and multi-\nhop question answering via structured knowledge and\nunified retrieval-generation. ArXiv, abs/2212.08632.\nPeter Young, Alice Lai, Micah Hodosh, and Julia Hock-\nenmaier. 2014. From image descriptions to visual\ndenotations: New similarity metrics for semantic in-\nference over event descriptions. Transactions of the\nAssociation for Computational Linguistics, 2:67–78.\nZhen Zhang, Guanhua Zhang, Bairu Hou, Wenqi\nFan, Qing Li, Sijia Liu, Yang Zhang, and Shiyu\nChang. 2023. Certified robustness for large lan-\nguage models with self-denoising. arXiv preprint\narXiv:2307.07171.\n1206\nFigure 5: QA with/without CoT.Same metrics as ta-\nble 4, VQA is InstructBLIP.\nA Does chain-of-thought help?\nCoT reasoning represents an emerging capability\nwithin LLMs. Given that we employ LLMs as our\nTextual QA and Reasoner, it is pertinent to examine\nif CoT aids in our setup. To address this, we im-\nplement a straightforward strategy, adopting a spell\nprompt that reads: {reference} {question} Let’s\nthink step by step. This results in the model gener-\nating a step-by-step reasoning response, which con-\ntemplates the reference in relation to the question,\nand evaluates if sufficient information is available\nfor a response. Subsequently, we prompt GPT to\nextract an answer, considering the question, reason-\ning process, and reference with the prompt: Rea-\nsoning:{reasoning} Question:{question} Give me\na very short answer, in one or two words. Upon\nconducting this process, we observe the following\nfindings:\nFirstly, CoT proves beneficial for retrieval-based\nquestion answering as demonstrated in fig. 5. The\ntechnique enables LLMs to better extract potential\nanswers from references, significantly improving\nrecall for table, text, and overall data. However,\nfor Direct QA, all metrics except GPT4’s F1 score\ndecrease. This is because CoT focuses on reason-\ning which isn’t necessary for answering general\nknowledge questions. We’ve noticed that with CoT,\nGPT4 tends to generate longer results, thus improv-\ning its F1 score.\nReasoner CoT Recall@15 F1 EM\nChatGPT × 47.6 39.5 32.7\nChatGPT ✓ 55.3 39.6 32.9\nGPT4 × 47.6 44.2 37.8\nGPT4 ✓ 55.3 44 36.8\nTable 8: CoT Ablation Results with Different Rea-\nsoners VQA model is InstructBLIP and Textual QA is\nChatGPT\nSecondly, it’s unexpected that the increased an-\nswer recall due to CoT does not aid in final answer\nextraction fig. 5. A possible reason is that CoT\nextracts a larger quantity of information from the\nreference material, both useful and irrelevant. It\neven attempts to provide answers when sufficient\ninformation isn’t available, leading not only to in-\nclude correct answers but also incorrect ones in the\ncandidate pool. For instance, in MMCoQA, the\naverage number of valid answer candidates with\nCoT is 3, compared to 2.5 without it. This addition\nof noise could confuse GPT4, impairing its ability\nto make the correct decision.\nB Implementation details\nAs our method employs a zero-shot approach in-\nvolving only inference, all experiments were con-\nducted on a single A100 GPU. We employed\nCLIP:ViT-B/32for image retrieval, text-embedding-\nada-002 for table retrieval, and ANCE-roberta\nfor text retrieval. We apply BLIP2-FlanT5xl and\nInstructBLIP-vicuna-7B for Visual Question An-\nswering (VQA), gpt-3.5-turbo, OpenChat-v2-w-\n13b,Llama2Chat-13b and vicuna-7B for textual\nQA.\nC Detailed Example\nOur methodology is detailed in table 9 table 10 and\ntable 11, showcasing the retrieval results, question-\nanswering process, strategy outputs, and final an-\nswer fusion. For these examples, BLIP2 serves as\nthe VQA model, ChatGPT as the textual QA model,\nand GPT4 as the reasoner and direct QA model.\nThe retrieval results across all modalities are ratio-\nnal, despite several ’Unknown’ instances, which\nare filtered out through the strategy. While the fi-\nnal answers are expected to be correct, they do not\nmeet the current exact match criteria. The veracity\nof the results is established by grounded answer\nsources. For instance, in table 10, while GPT’s\nanswer is ungrounded and incorrect, our method-\nology provides accurate information enabling the\nLLM to select the correct choice.\n1207\nQuestion: What is the competition of Gtu trttarfelag with match against Rangers?\nGold reference modality: table; Answer: uefa champion leagu\n# Image Retrieval\n# Text Retrieval\n1. Knattspyrnufélagið Ægir is an Icelandic sports club from the town of Þorlákshöfn, mainly known\nfor its football team. The club has a football team playing in the fifth tier of Icelandic football.\n2. Torfnesvöllur, known as Olísvöllurinn for sponsorship reasons, is a football stadium in Ísafjörður,\nIceland and the home of Vestri and Knattspyrnufélagið Hörður. It broke ground in 1963 ...\n3. Tvillingderbyt (, \"The Twin Derby\") is a football fixture in Stockholm, Sweden, between cross-town\nrivals AIK and Djurgårdens IF . Both clubs were founded in Stockholm in 1891, just three weeks ...\n4. The Asturian derby (, or \"Derbi astur\"), is the name given to any association football match contested\nbetween Sporting de Gijón and Real Oviedo, the two biggest clubs in Asturias. The rivalry ...\n5. Knattspyrnufélagið Hörður was founded on 27 May 1919 as a football club with Þórhallur Leósson\nbeing its first chairman. Its first official game was against Fótboltafélag Ísafjarðar on 17 June 1921. ...\n# Table Retrieval\n1. European Cup / UEFA Champions League European Cup / UEFA Champions League European\nCup / UEFA Champions League European Cup / UEFA Champions ...\n2. 2005–06 UEFA Cup 1Q FC Nistru Otaci 1–2 1–3 2–5 2007–08 UEFA Champions League 1Q NK\nDinamo Zagreb 1–1 1–3 (aet) 2–4 2008–09 UEFA Cup ...\n3. 1970–71 UEFA Cup Winners’ Cup 1 Partizani 3–2 2–1 5–3 2 Real Madrid 0–2 1–0 1–2 1971–72\nEuropean Cup 1 Benfica 1–3 0–4 1–7 1972–73 ...\n4. 1963–64 European Cup Preliminary round Borussia Dortmund 2–4 1–3 3–7 1964–65 European\nCup Preliminary round Reipas Lahti 3–0 1–2 4–2 ...\n5. 1968–69 European Cup Winners’ Cup First Round FC Barcelona 0–1 0–3 0–4 1971–72 UEFA Cup\nFirst Round Legia Warszawa 1–3 0–0 1–3 1993–94 ...\n# Question Answering Prompt\nYou are performing extractive question answering. Given the document: {reference} , extract a short\nanswer to the question: question from the document. If insufficient information is available to answer\nthe question, respond with ‘Unknown’. The answer should be one or two words long.\n# Image Answer\nwuppertaler, woman football, league cup, league cup, league cup\n# Text Answer\nUnknown., Unknown., Unknown., Unknown., Unknown.\n# Tabble Answer\nUnknown., Unknown., Unknown., Unknown., Unknown.\n# Chatgpt Answer\neuropa leagu\nValid Answer Candidates(after strategy)\nleague cup,wuppertal,europa leagu\n# Answer Fusion\nGiven question {Q}, please select the best answer from the following candidates: {Candidates}\n# Final Answer\neuropa leagu\nTable 9: Example1, detailed results of MoqaGPT solve the task, note that there are repeated images exist in the\ndataset\n1208\nQuestion: how many songs were written by john lennon and paul mccartney\nGold reference modality: text; Answer: 180\n# Image Retrieval\n# Text Retrieval\n1. Lennon McCartney was the songwriting partnership between English musicians John Lennon (9\nOctober 1940 ˘00a0 8 December 1980) and Paul McCartney ...\n2. Lennon McCartney was the songwriting partnership between English musicians John Lennon (9\nOctober 1940 ˘00a0 8 December 1980) and Paul McCartney ...\n3. Lennon McCartney was the songwriting partnership between English musicians John Lennon and\nPaul McCartney of the Beatles, the partnership published approximately 180 jointly credited songs ...\n4. Paul McCartney is an English musician who has recorded hundreds of songs over the course of his\nover 60-year career. As a member of the Beatles, he formed a songwriting partnership with bandmate\nJohn\n5. Unlike many songwriting partnerships that comprise separate lyricist and composer, such as Jerry\nLeiber and Mike Stoller, Rodgers and Hammerstein, ...\n# Table Retrieval\n1. 1 75px Dylan May 24, 1941 present Mixed-Up Confusion (1962), performed by himself 2 75px\nMcCartney June 18, 1942 present Love Me Do/P .S. I Love You ...\n2. 1974 McGear Mike McGear 1980 The Reluctant Dog Steve Holley 1981 Somewhere in England All\nThose Years Ago George Harrison 1982 Tug of War ...\n3. 196? Words and Music by Paul Williams Big Seven Music Corp. 1970 Someday Man Reprise...\n4. 1 If You Be My Baby Peter Green/Clifford Adams Fleetwood Mac Mr. Wonderful (1968) 6:38 2 Long\nGrey Mare Peter Green Fleetwood Mac ...\n5. Disc 1 (23971): Disc 1 (23971): Disc 1 (23971): Disc 1 (23971): Disc 1 (23971): A. I Love You\nTruly Carrie Jacobs Bond April 18, 1945 John Scott Trotter and His Orchestra 2:56 B. Just, ...\n# Question Answering Prompt\nYou are performing extractive question answering. Given the document: {reference} , extract a short\nanswer to the question: question from the document. If insufficient information is available to answer\nthe question, respond with ‘Unknown’. The answer should be one or two words long.\n# Image Answer\nmore than one hundred, just like starting over, the john lennon story, imagine, more than one hundred\n# Text Answer\nApproximately 180., Approximately 180., Approximately 180., Unknown., Unknown.\n# Tabble Answer\nTwo songs were written., Unknown., Unknown., Unknown., Unknown.\n# Chatgpt Answer\nOver 150\nValid Answer Candidates(after strategy)\nmore than 1 hundr,2 song,approxim 180 jointli credit song,over 150\n# Answer Fusion\nGiven question {Q}, please select the best answer from the following candidates: {Candidates}\n# Final Answer\napproxim 180\nTable 10: Example2, detailed results of MoqaGPT solve the task\n1209\nQuestion: What kind of coaster is shown in this photo collage of Copenhagen?\nGold reference modality: image; Answer: roller\n# Image Retrieval\n# Text Retrieval\n1. The ride’s station is located on the midway directly across from Top Thrill Dragster and was the\nfirst coaster to have inversions featuring a walkway underneath. The ride consists of ...\n2. Kong is a steel Suspended Looping Coaster, made by Vekoma, located at Six Flags Discovery\nKingdom in Vallejo, California ...\n3. Diavlo is a steel roller coaster at Himeji Central Park in Japan which is a clone of Batman the Ride.\nIt is one of the first ...\n4. The first inversion in roller coaster history was part of the Centrifugal Railway of Paris, France,\nbuilt in 1848. It consisted of a 43-foot (13-meter) sloping track ...\n5. Colossus is a steel roller coaster at Thorpe Park in Surrey, England, and the park’s first major\nattraction. It was built by Swiss manufacturers Intamin and designed ...\n# Table Retrieval\n1. Wolverine Wildcat Michigan’s Adventure United States 1988 Raging Wolf Bobs Geauga Lake United\nStates 1988 Timber Wolf Worlds of Fun United States 1989 Hercules Dorney Park United States 1989 ...\n2. The Bat 1987 Vekoma A Vekoma Boomerang roller coaster. It was the seventh roller coaster added\nto the park. The Bats train was originally from the parks Dragon Fire coaster. During the 2008 season\n...\n3. 1985 Congo Carousel Robert Tidman Classic gallopers ride, operated previously at Happy Hour\nAmusement Park, Colwyn Bay 1986 Jungle Swings A classic chair-o-plane ride 1986 Jungle Cat ...\n4. All Saints’ Church Church of Denmark 1932 150px Drag Church Church of Denmark 1885 150px\nHans Tausen’s Church Church of Denmark 1924 150px Hdevang Church Church of Denmark 1885\n150px ...\n5. Ny Ellebjerg F , A, E 16 November 2006 16 November 2006 transfer to K00f8ge radial Gl. K00f8ge\nLandevej 2014 8 January 2005 8 January 2005 Temporary terminus; ...\n# Question Answering Prompt\nYou are performing extractive question answering. Given the document: {reference} , extract a short\nanswer to the question: question from the document. If insufficient information is available to answer\nthe question, respond with ‘Unknown’. The answer should be one or two words long.\n# Image Answer\nroller coaster, a roller coaster, roller coaster, a wooden coaster, heart,\n# Text Answer\nUnknown., Unknown., Unknown., Unknown., Unknown.\n# Tabble Answer\nUnknown., Unknown., Unknown., Unknown., Unknown.\n# Chatgpt Answer\nTivoli Gardens\nValid Answer Candidates(after strategy)\nroller coaster, tivoli garden\n# Answer Fusion\nGiven question {Q}, please select the best answer from the following candidates: {Candidates}\n# Final Answer\nroller\nTable 11: Example3, detailed results of MoqaGPT solve the task\n1210",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7574408054351807
    },
    {
      "name": "Baseline (sea)",
      "score": 0.6794716119766235
    },
    {
      "name": "Modality (human–computer interaction)",
      "score": 0.6774966716766357
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.6665792465209961
    },
    {
      "name": "Task (project management)",
      "score": 0.6385133266448975
    },
    {
      "name": "Modalities",
      "score": 0.5909887552261353
    },
    {
      "name": "Artificial intelligence",
      "score": 0.546862006187439
    },
    {
      "name": "Codebase",
      "score": 0.4835929274559021
    },
    {
      "name": "Modal",
      "score": 0.4793008863925934
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.46777644753456116
    },
    {
      "name": "Shot (pellet)",
      "score": 0.43864428997039795
    },
    {
      "name": "Language model",
      "score": 0.4264790415763855
    },
    {
      "name": "Information retrieval",
      "score": 0.4145914316177368
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.41080430150032043
    },
    {
      "name": "Machine learning",
      "score": 0.403830885887146
    },
    {
      "name": "Natural language processing",
      "score": 0.3982560634613037
    },
    {
      "name": "Programming language",
      "score": 0.14087694883346558
    },
    {
      "name": "Source code",
      "score": 0.11043316125869751
    },
    {
      "name": "Mathematics",
      "score": 0.08280974626541138
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    }
  ]
}