{
  "title": "Membership Inference Attack Susceptibility of Clinical Language Models",
  "url": "https://openalex.org/W3153896080",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4226896120",
      "name": "Jagannatha, Abhyuday",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4280834971",
      "name": "Rawat, Bhanu Pratap Singh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1978159643",
      "name": "Yu Hong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2040228409",
    "https://openalex.org/W2135930857",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2784621220",
    "https://openalex.org/W2973154071",
    "https://openalex.org/W2971258845",
    "https://openalex.org/W2950279864",
    "https://openalex.org/W2535690855",
    "https://openalex.org/W2473418344",
    "https://openalex.org/W2785361959",
    "https://openalex.org/W3190860428",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2930926105",
    "https://openalex.org/W2594311007",
    "https://openalex.org/W2967985550",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2027595342",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2795435272",
    "https://openalex.org/W2963310665"
  ],
  "abstract": "Deep Neural Network (DNN) models have been shown to have high empirical privacy leakages. Clinical language models (CLMs) trained on clinical data have been used to improve performance in biomedical natural language processing tasks. In this work, we investigate the risks of training-data leakage through white-box or black-box access to CLMs. We design and employ membership inference attacks to estimate the empirical privacy leaks for model architectures like BERT and GPT2. We show that membership inference attacks on CLMs lead to non-trivial privacy leakages of up to 7%. Our results show that smaller models have lower empirical privacy leakages than larger ones, and masked LMs have lower leakages than auto-regressive LMs. We further show that differentially private CLMs can have improved model utility on clinical domain while ensuring low empirical privacy leakage. Lastly, we also study the effects of group-level membership inference and disease rarity on CLM privacy leakages.",
  "full_text": "Membership Inference Attack Susceptibility of Clinical Language Models\nAbhyuday Jagannatha1, Bhanu Pratap Singh Rawat1, Hong Yu1,2\n1College of Information and Computer Sciences,University of Massachusetts Amherst\n2Department of Computer Science,University of Massachusetts Lowell\n{abhyuday, brawat hongyu}@cs.umass.edu\nAbstract\nDeep Neural Network (DNN) models have\nbeen shown to have high empirical privacy\nleakages. Clinical language models (CLMs)\ntrained on clinical data have been used to im-\nprove performance in biomedical natural lan-\nguage processing tasks. In this work, we\ninvestigate the risks of training-data leakage\nthrough white-box or black-box access to\nCLMs. We design and employ membership\ninference attacks to estimate the empirical pri-\nvacy leaks for model architectures like BERT\nand GPT 2. We show that membership infer-\nence attacks on CLMs lead to non-trivial pri-\nvacy leakages of up to 7%. Our results show\nthat smaller models have lower empirical pri-\nvacy leakages than larger ones, and masked\nLMs have lower leakages than auto-regressive\nLMs. We further show that differentially pri-\nvate CLMs can have improved model utility on\nclinical domain while ensuring low empirical\nprivacy leakage. Lastly, we also study the ef-\nfects of group-level membership inference and\ndisease rarity on CLM privacy leakages.\n1 Introduction\nLarge contextual word embedding models have\nbeen proposed and used for obtaining the state-of-\nthe-art performance on various NLP tasks (Wang\net al., 2018, 2019). The high performance and\nrelatively lower sample complexity requirements\nachieved by domain-speciﬁc pretrained language\nmodels (LMs) (Peng et al., 2019; Devlin et al.,\n2018) have propelled their use in private data do-\nmains like healthcare and ﬁnance. A primary con-\ncern in deploying such models in a public setting\nis that models trained on sensitive private data may\nleak information about sensitive training samples.\nIn application domains such as healthcare, such a\nleak may violate patient rights and cause potential\nharm to the participants of the study.\nDe-identiﬁcation or obfuscation of the under-\nlying text may not necessarily mitigate all pri-\nvacy concerns since anonymized data can be iden-\ntiﬁed using additional sources (Narayanan and\nShmatikov, 2008). Also, de-identiﬁcation of large\ntext datasets is only feasible through automated de-\nidentiﬁcation, which may lead to imperfect results.\nDifferential Privacy (DP) (Dwork et al., 2014) is a\nsystematic approach to limiting worst-case training\ndata leakage in the context of machine learning.\nIntuitively, DP limits training data leakage by lim-\niting the effect of adding any random data sample\nto the training set. This is a general deﬁnition for\nestimating privacy budgets and is usually deﬁned\nindependently of the nature of the underlying data.\nAs a result, DP may be overly conservative for sys-\ntems using high dimensional input with lower rank\ndata space. For instance, in NLP where a topically\nrelevant sentence is a very small subset of all ran-\ndomly constructed sentences, mitigating the risk\nfor any random data sample may not provide an\nefﬁcient privacy mechanism. Moreover, DP based\nprivacy accountants (Jayaraman and Evans, 2019)\nvastly overestimate the privacy leakages of large\nneural network models and may be practically un-\nusable.\nOn the other hand, empirical privacy leakage\nestimation may provide more accurate privacy esti-\nmates. However, training data extraction methods\nsuch as Carlini et al. (2020) may be model-speciﬁc\nand do not provide a standardized way to estimate\nprivacy leakages across models architectures. Also,\ndue to the high dimensional nature of inputs to\nLMs, inference attacks that demonstrate the poten-\ntial of data leakages from such models are difﬁcult\nto construct and study. This poses a problem for\nthe study of privacy leakage in large LMs. Regu-\nlations like HIPPAA and EU 2016/679 have been\nenacted to ensure patient privacy rights. These laws\ndictate the use of patient data for secondary uses\narXiv:2104.08305v1  [cs.CL]  16 Apr 2021\nlike research. However, they do not provide ex-\nplicit guidelines for data leakage through release\nof Machine Learning or NLP model ﬁles. Due to\na lack of clarity, sensitive data models are mostly\nused in-house. Systematic privacy leakage studies\nfor large NLP systems can inform and help policy\ndecisions in this domain. This can accelerate the\ndevelopment of private, high utility NLP models.\nIn this work, we design general membership\ninference attacks (Shokri et al., 2017) that can\nbe used to quantitatively estimate the discrepancy\nin a large LM’s response to training and out-of-\ntraining (OOT) samples. Recent work shows that\nthis discrepancy, which contributes to the general-\nization gap, is related to the model’s privacy leak-\nage (Yeom et al., 2018). We use white-box and\nblack-box membership inference adversaries to es-\ntablish a standardized framework for our study.\nOur main focus in this study is to examine the\nsusceptibility of CLMs to membership attacks and\nempirically estimate their privacy leakage. We also\ninvestigate DP methods to make these modes more\nprivate. To the best of our knowledge, this is the\nﬁrst attempt to study empirical privacy leakages in\nCLMs and use differentially private CLM training.\nOur main ﬁndings are as follows:\n1. Large LMs can have higher empirical privacy\nleakages (7%) than smaller LMs (2%).\n2. Randomly masked LMs have lower privacy\nleakages than autoregressive LMs.\n3. (ϵ,δ)−DP training using DP-SGD (Dwork\net al., 2014) can reduce empirical privacy leak-\nages while ensuring increased model utility.\n4. Group-level membership inference attacks\nlead to higher privacy leakage than sample-\nlevel.\n5. Patients with rarer disease proﬁles may be\nmore vulnerable to higher privacy leakages.\n2 Background\nPre-trained Language Models: Language\nModels (LMs)1 such as BERT , RoBERT a or GPT 2\nare pretrained using large unsupervised text\ncorpora such as WikiText that contains text from\nmultiple domains. These pre-trained LMs are used\nfor extracting contextual embeddings which are\nused for multiple downstream NLP tasks. For\n1All useful notations are also provided together in Ap-\npendix B.5.\ndomain speciﬁc NLP tasks such as MedNLI or\nemrQA, the LMs are further ﬁne-tuned on text\nfrom a particular domain such as clinical BERT\n(Alsentzer et al., 2019), bioBERT (Lee et al., 2020)\nand sciBERT (Beltagy et al., 2019).\nDifferential Privacy: (ϵ,δ)−DP (Dwork et al.,\n2014) can be used to limit the privacy leakage of\nmachine learning models trained through stochastic\ngradient descent(SGD). A randomized mechanism\nM: D→R — in our case, this refers to the SGD\ntraining algorithm — is (ϵ,δ)-DP compliant if for\nall S ⊂R and x,x′∈D\nPr[M(x) ∈S] ≤eϵPr[M(x′) ∈S] +δ. (1)\nAn ML model with (ϵ,δ)-DP compliant train-\ning limits the privacy leakage to a function of\nϵ with failure probability δ. An important prop-\nerty of (ϵ,δ)-DP systems is Group Differential\nPrivacy. Group DP provides (ϵ,δ) values for k\ncorrelated inputs in a dataset. In such a case the\nprivacy degrades to (kϵ,ke(k−1)ϵδ) (Dwork et al.,\n2014). In private applications, ideally we would\nkeep δ≤1/N, for dataset size N and ϵ<< 1.\nMembership Inference: Machine learning\nmodel parameters can be probed to extract\nsensitive information about the training data\nsamples (Homer et al., 2008; Yeom et al., 2018).\nShokri et al. (2017) introduce the membership\ninference attacks, which refer to a class of attack\nmodels that predict whether a given data sample\nwas present in the training data for a trained model.\nThe advantage of an attacker using membership\ninference attack depends on the attacker’s ability\nto distinguish between the target model’s response\nto training and out-of-training data samples.\nShokri et al. (2017) proposed a shadow model\nbased attack that can try to understand differences\nin model’s response for training data samples vs\nout-of-training(OOT) data samples. Computation\nand memory costs of shadow models scale with\nthe number of model parameters and therefore,\nthese attacks are not feasible for large CLM models\nsuch as BERT . Yeom et al. (2018) propose black-\nbox attacks that achieve privacy leakage similar to\nshadow model attacks.\n3 Data\nWe use three US hospital datasets, namely MIMIC-\nIII, UMM (UMass Memorial Health Care)and\nVHA (Veterans Health Administration) Hospitals\n(a)\n (b)\n (c)\nFigure 1: Sample-level privacy leakage using black box and white-box membership inference attacks for non-DP\nCLM models. Plots show results on the MIMIC-III data. Plot (a) shows S-BBA attack leakage. Plot (b) and (c)\nshow S-AWBA and S-GWBA white box attack leakages respectively. TheGPT 2 model consistently has the highest\nleakage and DistilBERT has the lowest.\nfor our study. These datasets are used to train our\nCLMs and evaluate their privacy leakage. MIMIC-\nIII (Johnson et al., 2016) is an publicly available\ndataset, while UMM and VHA data sources are\nprivate. Our main experiments are conducted with\nMIMIC-III and UMM datasets. Due to in-house\ncomputational constraints, we use VHA dataset\nto only evaluate privacy leakage for one of our\nCLM models. We extract EHR repositories from all\nthree data sources along with patient and admission\nlevel information. We select all available patient\nnotes from MIMIC-III. We use a manageable sub-\nset of patients from UMM and VHA repositories,\ndetailed information provided in Appendix A.4.\nIn MIMIC-III, each patient’s records are com-\nposed of a collection of EHRs of clinical visits or\nadmissions. Each visit is composed of a group of\nEHRs that were recorded during that visit. Each\nEHR note is, in turn, a document that may be com-\nposed of multiple text samples. We use this hier-\narchical structure to study the effects of correlated\nsamples on privacy leakage. In UMM and VHA ,\nwe could not use admission information. Therefore\nwe treat each EHR record as an admission. Corre-\nlated samples can extend the notion of privacy to\ngroup-level privacy using the deﬁnition provided\nin Section 2. Dataset size statistics for all three\ndatasets are provided in Table 1.\n4 Methods\nFor our investigations, we use two main sets of\nmethods. We use membership inference attacks\nto estimate empirical privacy leakage of a method.\nAnd we use differentially private stochastic gradi-\nFigure 2: Admission-level membership attack (A-\nBBA) privacy leakage using MIMIC-III data for non-\nDP CLMs.\nent descent training (DP-SGD)(Abadi et al., 2016)\nto train DP clinical LM models.\n4.1 Membership Inference Attacks\nMembership inference attacks are accomplished\nby exploiting the difference in a model’s responses\nto training and OOT data samples. We use the\nadvantage of membership inference attacks to es-\ntimate the empirical privacy leakage. The exper-\nimental framework for obtaining privacy leakage\nusing a membership inference adversary is deﬁned\nby (Yeom et al., 2018) as follows:\nMembership Inference Experiment\nExpM(A,A,n,D ): Let A be an adversary,\nA be a learning algorithm, n be a positive integer,\nand D be a distribution over data points (x,y).\nThe membership experiment in (Yeom et al., 2018)\nproceeds as follows:\n• Sample S ∈Dn.\nFigure 3: Patient-level membership attack (P-BBA) pri-\nvacy leakage using MIMIC-III data for non-DP CLMs.\n• Choose b←{0,1}uniformly at random.\n• Draw z∈Sif b= 0, or z∈Dif b= 1.\n• ExpM(A,A,n,D ) = 1 if\nA(z,A(S),n,D ) = b and 0 otherwise.\nAmust output either 0 or 1.\nThe membership advantage of the adversary\nAdvM or alternatively the empirical privacy leak-\nage observed in the experiment is\nPL(A) =Pr[A= 0|b= 0]−Pr[A= 0|b= 1].\n(2)\nNLP models may be released with API or white-\nbox access to the internal model. Therefore, we\ndesign both black-box and white-box adversaries\nfor our study.\nSamples For membership inference experiments\nin CLMs, the sample S consists of the input sen-\ntence and the constructed language modeling target.\nIn case of an auto-regressive language model such\nas GPT-2, the input sequence is a ﬁxed length token\nsequence and the target is the next token sequence.\nIn case of MLM objective models like BERT, the in-\nput sequence is a randomly masked ﬁxed length to-\nken sequence and the output is the relevant masked\ntoken values. It should be noted that masks are\ngenerated at random in MLM training procedures,\nso the masks generated for membership inference\nexperiment will be different than those generated\nduring training of the CLM.\n4.1.1 Black-Box Adversary\nWe adapt the black box attack deﬁned by Yeom\net al. (2018) ( Adversary 2). This attack model\nassumes access to the error probability density\nfunctions of the trained model, f(ϵ|b = 0) and\nf(ϵ|b= 1).\nSample-level Attack For a sample x, the output\nof the adversary A(x) is argmaxb∈(0,1)f(ϵ(x)|b).\nIn context of language models with non-zero er-\nror means, we can deﬁne f(ϵ|b) using error mean\nand unit variances for both b=1 and 0. However,\nwe use a simpler scheme that only assumes access\nto the mean of the training error. In preliminary\nexperiments this choice did not reduce the attack\nadvantage. We use the mean of training error µtr\nas the threshold to predict A(x). Our sample level\nattack is very similar to threshold adversary pro-\nposed by Yeom et al. (2018). A random samplexis\npredicted to be a training data sample ife(x) <µtr\nelse it is predicted to be an OOT data sample. We\ndenote this sample-level black-box membership\ninference attack as S-BBA.\nGroup-level Attack To investigate group-level\nprivacy leakages, we devise group-level member-\nship inference attacks for our black-box access ad-\nversary. For group-level membership inference, we\ntreat each group as a single data sample. To esti-\nmate the error of a group gwe take a mean of all\nsamples in that group. We use the same condition\nand threshold ( e(g) < µtr) as that in S-BBA for\nour group-level privacy attacks. The group-level\nattacks based on patient and admission level groups\nare referred as P-BBA and A-BBA respectively.\n4.1.2 White-Box Adversary\nWith white-box access to the model, we can use hid-\nden layer outputs, gradients and other model spe-\nciﬁc information to identify differences in model’s\nbehavior between training and out-of-training data\nsamples. Based on preliminary experiments, gra-\ndient and attention values proved to be more infor-\nmative than hidden layer outputs.\nAttention-based white-box attack (S-A WBA):\nIn this attack we use the self-attention outputs of\nCLMs as the input to the attack model. A k-head\ntransformer layer with a sequence input length of\nn, produces kn2 attention values. For a 12 layer\nLM such as BERT , with 12 attention heads for each\ntransformer layer, the number of attention outputs\nis 144n2. For an input sequence of 512 tokens, a\nmodel like BERT will have around 37 million at-\ntention values. To reduce the size of attack model\ninput, we instead use the “concentration” of the\nattention vector a= {ai}n\n0 using\nC(a) =\nn∑\ni=0\nailog(ai). (3)\nC(a) is higher when the attention is diffused\nacross all token positions and lower when high at-\ntention values are focused on speciﬁc tokens. We\nobtain one C(a) value for each token on each atten-\ntion layer. To further reduce the size of the attention\ninformation, we use four estimates computed over\nthe sentence length {C(a)j}n\n0 , namely mean, me-\ndian, 5-percentile and 95-percentile values. Com-\nputation of C(a) values and aggregation over the\nsentence provides us with a compressed 144 ×4\ndimensional attention representation for any input\nsample. We use this attention representation as an\ninput to the attention-based attack model.\nGradient-based white-box attack(S-GWBA):\nGradient values have been previously (Nasr et al.,\n2019) used in membership inference attack mod-\nels against DNNs. Intuitively, the gradient values\nof a training sample’s loss are expected to have\nlower absolute value than an OOT data sample’s\nloss derivative. Membership inference attacks us-\ning gradient values from large LM models pose\nsimilar computational challenges as the attention-\nbased attacks. An LM model, such as BERT has\n110 million parameter vectors. As a result, it pro-\nduces 110 million partial derivatives. We aggregate\nthe gradient values for each neural network layer\nby taking the squared-norm of all parameters in\nthat layer. For a model like BERT , this results in a\n202 dimensional aggregate. We use this gradient\nrepresentation as an input for the attack model.\n4.2 Differentially Private Model Training\nWe use differentially private SGD (DP-SGD) pro-\nposed by Abadi et al. (2016) to train our(ϵ,δ)−DP\nmodels. DP-SGD uses per-sample gradient clip-\nping to bound the effect of each sample and uses\nadditive Gaussian noise to inject noise into the mini-\nbatch gradients. In our experiments, we use the\nnoise standard deviation (std) as a hyperparame-\nter and denote it as σ. We use R ´enyi Differential\nPrivacy (RDP) framework (Mironov, 2017) to es-\ntimate the total privacy loss due to the DP-SGD\ntraining.\n5 Experiments and Results\nWe run our experiments on non-DP and DP CLMs\nthat are trained on EHR datasets described in Sec-\ntion 3. DP and non-DP CLMs use SGD and DP-\nSGD training procedures. Training details are pro-\nvided in Appendix A. We use BERT , DistilBERT\nDataset #Token #Admission #Patient\nMIMIC-III 695M 57k 46k\nUMM 1.33B 1184k 58k\nVHA 8.31B 2019k 35k\nTable 1: Number of tokens, admission and patients in\nthe three datasets. The number of tokens are computed\nusing BERT -base-cased tokenizer. We do not have ad-\nmissions information for UMM and 3, so we consider\none EHR note per admission.\nand GPT 2 base models to initialize the CLM train-\ning. These CLMs are used in the following sec-\ntions to study the patterns of privacy leakage across\ndatasets and models.\n5.1 What is the sample level empirical\nprivacy leakage of CLM models?\nWe examine privacy leakage for clinical-domain\ntuned CLMs using both white-box and black-box\nmembership inference attacks on text snippets from\nelectronic health records. MIMIC-III and UMM\ndata are used for the main set of experiments. Due\nto in-house computational constraints for VHA , we\nwere unable to replicate all experiments on VHA\ndata. However we do provide black box inference\nattack based PL results for BERT models in Ap-\npendix B. The general trends discussed in the fol-\nlowing sub-sections hold true also for the limited\nresults from VHA .\nExperimental Design: Formally, we estimate\nthe leakage by calculating S-BBA, S-AWBA and\nS-GWBA adversary advantage for non-DP CLMs.\nTo evaluate the privacy leakage using Equation 2,\nwe use the membership experiment deﬁned in Sec-\ntion 4.1. We draw a sample zfrom the training or\ntest data for b= 0 or 1, respectively. Estimation of\nPr[A= 0|b = 0]and Pr[A= 0|b = 1]use train-\ning and test data samples. White-box attacks use\na multi-dimensional vector of gradient or attention\naggregates. To estimate the difference in attention\nand gradient values for training and OOT data sam-\nple, we train a logistic regression model ( A) on\n20% split of the train and test datasets. Recall from\nsection 4.1 that Ais used to predictbfor a data sam-\nple z. The remaining 80% data samples are used\nto estimate Pr[A= 0|b= 0]and Pr[A= 0|b= 1]\nfor provided A.\nResults: Privacy leakage plots using MIMIC-III\nfor all non-DP CLMs are provided in Figures 1.\nThe privacy leakage of all models for all epochs re-\nModel /σ 2 1 0.1 1e-2 1e-3 1e-4 1e-20\nGPT 2 0/.009 0/.014 0/.014 0/.014 0/.017 4e-3/.016 8e-3/.018\nBERT 9e-2/.011 2e-3/.011 3e-3/.010 1e-3/.011 3e-4/.017 3e-3/.016 5e-3/.016\nDistilBERT 0/8e-3 0/5e-2 0/9e-2 0/8e-3 1e-5/.010 1e-3/.010 0/.012\nTable 2: Empirical privacy leakage for DP models trained on MIMIC-III data using S-BBA and S-AWBA attacks.\nBlack box S-BBA always leads to lower PL than white box S-AWBA attack. PL values reported in this table are\nthe maximum PL values over all three epochs of these models. Underlined models have RDP computedϵ< 1. All\nother models have ϵ> 1. RDP accounting results are in Appendix B.\nmains lower than 10%. DistilBERT model that has\nthe lowest number of parameters amongst all three\nmodels, on average has the lowest sample level pri-\nvacy leakage. This behaviour is consistent across\nall attack methods, both black-box and white-box.\nA consistent trend in our results is that the white-\nbox access attacks lead to higher PL values. This\nis expected. Gradient based white-box attack is the\nmost effective attack. The leakiest model is GPT 2\nwith privacy leakages is as high as 7%. On average,\nall PL values increase with the number of epochs.\n5.2 Can we train (ϵ,δ)-DP CLM models with\nimproved clinical domain performance?\nModels with 7% PL, as shown in the previous sec-\ntion, should be further studied before releasing\nthem in the public domain. To this end, we use\nDP-SGD training methods to enforce privacy con-\nstraints. We then study the trade-off between the\nstrictness of privacy constraints and model utility.\nExperimental Design: We use the DP-SGD and\nRDP for DP model training and privacy accounting\nas deﬁned in Section 4.2. To limit the group based\nprivacy degradation in ϵcalculation, we limit the\nnumber of samples from each patient to 50. We\nuse Gaussian mechanism with σranging from 2.0\nto 1e-4 in the DP-SGD algorithm. We also use a\nnegligible σof 1e-20 to ablate the effect of additive\nnoise from DP-SGD training.\nResults: We see a decline in empirical privacy\nleakage across all models for all attack methods\nwhen we apply a Gaussian noise within the range\n{1e-4, 2.0 }. An empirical privacy leakage for\na sample-level white-box attack (S-AWBA) and\nblack box attack (S-BBA) are provided in Table\n2. There is a signiﬁcant decrease in empirical\nprivacy leakage. Most models exhibit less than\n1% (often less than 0.1%) privacy leakage. We\ndo see increased privacy leakage through atten-\ntion access in S-AWBA, however, these leakages\nare around 1-2% and are negligible for DistilBERT\nmodel. Empirical privacy leakages for all models-\nattack combinations are provided in the Appendix\nB for MIMIC-III and UMM data. One consistent\npattern across all results show that models with\nσ ∈{2.0,1.0,0.1,0.01}have negligible privacy\nleakages for all sample and group membership in-\nference attacks. For σ∈{0.001,0.0001,1e−20}\nwe see more than 1% privacy leakages in S-AWBA\nand S-GWBA. The most successful attack on non-\nDP models is conducted using S-GWBA, providing\nPL as high as 2% for GPT 2. Group membership\ninference attacks are advantageous even for a DP\nmodel. Group PL for models are as high as 5%\nfor σ= 1e−20 (S-GWBA). In conclusion, group-\nlevel membership inference attacks and white box\nattacks are able to extract more than 1% privacy\nleakages from our DP models for very low epsilon\nvalues. However, most models with σ ≤1e−3\nshow negligible privacy leakage. Results for other\nmodels on MIMIC-III and UMM datasets are avail-\nable in Appendix B. They show similar behaviour.\nTo understand the trade-off betweenσand model\nutility, we compute the LM loss and MedNLI per-\nformance for each non-DP model epoch. Model\nperformances for ﬁrst two epoch BERT models (DP-\nSGD, epoch=1) on MIMIC-III data are provided in\nTable 3. All models with σ≤0.01 have improved\nMLM test loss as compared to BERT -base-cased.\nσ∈{0.01,0.001}provides MLM loss lower than\nthe BERT -base-cased model while ensuring that em-\npirical PL from all attack methods are kept below\n1%. MedNLI experiments and results for MIMIC-\nIII BERT models are provided in Appendix A and\nB respectively. We observe that most MedNLI per-\nformances for DP BERT models show improved\nmodel utility with accuracy between BERT -base\nand ClinicalBERT .\nModel / Epochs 1 2\nNon-DP BERT 0.59 0.57\nDP BERT (2.0) 6.15 7.02\nDP BERT (1.0) 5.65 6.76\nDP BERT (0.1) 3.82 4.86\nDP BERT (1e-2) 2.00 2.33\nDP BERT (1e-3) 1.18 1.23\nDP BERT (1e-4) 0.85 0.86\nDP BERT (1e-20) 0.75 0.76\nTable 3: DP BERT (σ) denotes BERT -base-cased mod-\nels trained on MIMIC-III using DP-SGD with noise\nstd σ. MLM loss is obtained by validating the model\non test split.Untrained BERT -base-cased model’s MLM\nloss on MIMIC-III test is 3.49. DP models for σ ≤\n1e−3 have improved model utility due to DP training.\nFor non-DP models increasing epochs increases model\nutility, whereas for DP model utility decreases with in-\ncreased epochs.\n5.3 Does group level information provide\nmore advantage to the membership\ninference attacker ?\nTo study the phenomenon of privacy degradation\ndue to correlated data samples, we use admission\nand patient level membership inference attacks as\ndeﬁned in Section 4.1.\nExperimental Details: We use the patient and\nadmission ids provided in MIMIC-III dataset to\nconstruct the groups. For UMM data, we did not\nhave access to admission level information, so each\nEHR is treated as one admission. The patient ids\nare used to create patient groups. The threshold\nused for the A-BBA and P-BBA attack adversaries\nare the same as that computed for S-BBA attack.\nResults: Group level privacy attacks for MIMIC-\nIII and UMM are provided in Figures 2 and 3. Com-\nparing these plots with Figure 1 we observe an\nincrease in the empirical privacy leakage for all\nmodels when group-level averages are used. The\nperformance of larger group samples (patient-level\naggregation) is higher than smaller group samples\n(admission or EHR level aggregation). This gap is\nmore pronounced for UMM (ﬁgure in Appendix)\nwhere the EHR level group is much smaller than\nthe patient-level group.\n6 Discussion\nLeakage in Non-DP models: Our results are\nconsistent with existing efforts connecting over-\nﬁtting and privacy leakage (Yeom et al., 2018).\nLarge LM models pretrained with dataset sizes in\nmillions tend to have lower generalization gaps\ncompared to supervised deep neural net models\ntrained on more limited, expensive labelled data.\nTherefore we see a maximum sample level privacy\nleakage of around 7% for non-DP CLMs. Addition-\nally, losses like MLM use a random mask to pro-\nduce a self-supervised sample. This random mask\nadds an additional layer of obfuscation, thereby re-\nducing the empirical privacy leakage. GPT 2 model,\nwhich uses an auto-regressive loss has higher pri-\nvacy leakage than an equal sized MLM model. Dis-\ntilBERT LM model that has the smallest network\nsize, shows the lowest privacy leakage as expected.\nTrade-offs with privacy budgets: For theoreti-\ncal privacy budgets computed using RDP accoun-\ntant, only DP-SGD models with σ ∈1,2 provide\nan ϵ <1. All other models with σ <1.0 have ϵ\nvalues greater than 1, with the lower values like\nσ= 1e−3 having ϵvalues as high as 104. ϵcom-\nputed for all models are provided in Appendix B.\nThe privacy leakage predicted by RDP is a vast\noverestimate of our empirical PL values. All mod-\nels with σ > 1e−3 show negligible empirical\nprivacy leakage for both black-box and white-box\nattacks. Privacy budgets computed through DP\naccountants are vastly conservative and in the cur-\nrent form are not useful for deep neural net based\nNLP models. Based on our experiments, we ﬁnd\nthat keeping σhigher than 1e-3 may prevent non-\ntrivial privacy leakages from white-box member-\nship inference and black-box group membership\ninference attacks. Conversely we see high model\nutility (close to non-DP model utility) for low σ\nvalues (Table 3). According to group DP, for a\ngroup size of k, ϵvalues should degrade to 50 ×ϵ\nand δshould degrade even more. While group level\nmembership inference attacks increase the privacy\nleakage, they do not show such signiﬁcant privacy\ndegradation. This may be partially due to the na-\nture of the EHR dataset, that has large text spans\nof standard instructions and copy-pasted text. This\ntext results in several nearly-identical data samples\nacross patients in both training and OOT data sets.\nIt is also possible that our group-level attack is\ntoo simplistic, and more sophisticated methods are\nneeded.\nPractical Attack Scenarios: As discussed in\nSection 1, membership inference attacks against an\nNLP system may not be applicable in a practical\nscenario due to the high dimensional input space\nand the lower rank semantic space of sentences.\nOur attacks can be used practically when partial\ninformation about the patient, such as a snippet\nof their EHR, is known to the attacker. Neverthe-\nless, membership inference attacks serve as a good\nstandardized framework for comparing relative pri-\nvacy leakage of models and study the effects of\nenforcing privacy constraints on them.\nPrivacy for Rare Disease Patients: To under-\nstand if patients the behaviour of privacy leakage\nfor outlier patients (or those with rarer disease pro-\nﬁles), we examine the black-box patient member-\nship inference attacks in more detail for MIMIC-III\ntrained CLMs. We group patients into buckets\nbased on the rarity of their disease proﬁles. We\nuse the coded ICD (International Classiﬁcation of\nDiseases) entries for each patient to estimate the\nprobability of that patient’s disease proﬁle. De-\ntails for the probability estimation are provided in\nAppendix A.\nWe divide the patients into 100 buckets based on\nlog-normalized ranges of disease proﬁle probabil-\nity. We discard buckets with less than 10 patients.\nWe calculate privacy leakage using Equation 2 for\neach bucket individually. We use the same thresh-\nold used in S-BBA attack. We observe that for most\nnon-DP models with epoch >1, bucket’s average\nprobability is inversely correlated to privacy leak-\nage. Patients with rarer disease proﬁles tend to\nexhibit higher privacy leakage. The relevant graphs\nare provided in Appendix B.\n7 Related Work\nPre-trained Language Models are extensively used\nin current state-of-the-art pipelines for different\nNLP tasks. These models are generally pre-trained\nusing large unsupervised text from multiple do-\nmains (Devlin et al., 2018; Liu et al., 2019; Rad-\nford et al., 2019). For domain-speciﬁc tasks such\nas MedNLI or emrQA, these LMs are further ﬁne-\ntuned on domain-speciﬁc data which could be pri-\nvate and not publicly available (Alsentzer et al.,\n2019; Lee et al., 2020; Beltagy et al., 2019).\nThe language models when ﬁne-tuned on pri-\nvate data and released publicly are prone to privacy\nattacks and can leak individual training examples\n(Carlini et al., 2020). This poses even higher risks\nwhen the models are trained on private clinical data\nas it may lead to leaking sensitive patient infor-\nmation. Membership inference attack (Hisamoto\net al., 2020; Nasr et al., 2019) is the most common\nprivacy attack, where the adversary tries to predict\nwhether a particular example was used for training\nthe model or not. Model inversion attack recon-\nstruct representative views of the training examples.\n(Carlini et al., 2020) showed that large LMs such as\nGPT -2 can memorize sentences and using extrac-\ntion attack were able to extract verbatim sequences\nfrom the training set including identiﬁable informa-\ntion (public) such as names, phone numbers and\nemail addresses. To counter these attacks, differen-\ntial private training techniques (Abadi et al., 2016;\nMcMahan et al., 2018) are used to train deep learn-\ning models. These differentially private training\ntechniques can lead to a reduction in model accu-\nracy (Jayaraman and Evans, 2019) and increase\nthe pre-training time, which is quite signiﬁcant for\nlarge LMs.\n8 Conclusion and Future Work\nOur experiments show that large LM models ex-\nhibit lower privacy leakages compared to super-\nvised DNNs studied by Jayaraman and Evans\n(2019); Shokri et al. (2017); Yeom et al. (2018).\nBERT and GPT 2 do still exhibit non-trivial pri-\nvacy leakages of up to 7% for white-box attacks.\nIn private datasets like EHR repositories, this sug-\ngests potential susceptibility to training data ex-\ntraction attacks. While the exact privacy leakage\nrequirements are subjective to the application, we\nshould strive for lower than 1% PL values in clini-\ncal domain. Our experiments with DP-SGD train-\ning show that when used with low σvalues, it can\nreduce empirical privacy leakages to less than 1%\nwhile maintaining improvements in model utility\nduring training. We also show that patients with\nrarer diseases may exhibit higher privacy leakages.\nOur work represents the ﬁrst standardized compar-\nison of privacy leakages in commonly used LM\narchitectures.\nFuture Work: This work is the ﬁrst step in study-\ning privacy properties of CLMs, and has several\nfuture directions. A major research direction is to\npropose better model-agnostic and model-speciﬁc\nmembership inference and training data extraction\nattacks. Our evaluation framework, along with a\npublicly available MIMIC-III is well suited to pro-\nvide a testing framework for privacy attacks. Our\nfuture work involves establishing such a bench-\nmark that provides the DP and non-DP models to\nresearchers who already have access to MIMIC-\nIII. An automated testing framework that computes\npatient-level privacy leakage can be established\nusing our deﬁned data splits. Another research di-\nrection is the use of data-dependent DP methods\nsuch as PATE (Papernot et al., 2018), which may be\nable to provide better (ϵ,δ) values. Use of PATE\nfor training large LMs is prohibited by the high\ncomputational cost of multiple-teacher training in\nPATE, and further study to reduce the computa-\ntional complexity of such methods is required.\nReferences\nMartin Abadi, Andy Chu, Ian Goodfellow, H Bren-\ndan McMahan, Ilya Mironov, Kunal Talwar, and\nLi Zhang. 2016. Deep learning with differential pri-\nvacy. In Proceedings of the 2016 ACM SIGSAC con-\nference on computer and communications security ,\npages 308–318.\nEmily Alsentzer, John R Murphy, Willie Boag, Wei-\nHung Weng, Di Jin, Tristan Naumann, and Matthew\nMcDermott. 2019. Publicly available clinical bert\nembeddings. arXiv preprint arXiv:1904.03323.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scib-\nert: A pretrained language model for scientiﬁc text.\narXiv preprint arXiv:1903.10676.\nNicholas Carlini, Florian Tramer, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom Brown, Dawn Song, Ul-\nfar Erlingsson, et al. 2020. Extracting training\ndata from large language models. arXiv preprint\narXiv:2012.07805.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nCynthia Dwork, Aaron Roth, et al. 2014. The algorith-\nmic foundations of differential privacy.\nSorami Hisamoto, Matt Post, and Kevin Duh. 2020.\nMembership inference attacks on sequence-to-\nsequence models: Is my data in your machine trans-\nlation system? Transactions of the Association for\nComputational Linguistics, 8:49–63.\nNils Homer, Szabolcs Szelinger, Margot Redman,\nDavid Duggan, Waibhav Tembe, Jill Muehling,\nJohn V Pearson, Dietrich A Stephan, Stanley F Nel-\nson, and David W Craig. 2008. Resolving individu-\nals contributing trace amounts of dna to highly com-\nplex mixtures using high-density snp genotyping mi-\ncroarrays. PLoS Genet, 4(8):e1000167.\nBargav Jayaraman and David Evans. 2019. Evaluating\ndifferentially private machine learning in practice.\nIn 28th {USENIX}Security Symposium ({USENIX}\nSecurity 19), pages 1895–1912.\nAlistair EW Johnson, Tom J Pollard, Lu Shen,\nH Lehman Li-Wei, Mengling Feng, Moham-\nmad Ghassemi, Benjamin Moody, Peter Szolovits,\nLeo Anthony Celi, and Roger G Mark. 2016. Mimic-\niii, a freely accessible critical care database. Scien-\ntiﬁc data, 3(1):1–9.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So, and\nJaewoo Kang. 2020. Biobert: a pre-trained biomed-\nical language representation model for biomedical\ntext mining. Bioinformatics, 36(4):1234–1240.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nH Brendan McMahan, Daniel Ramage, Kunal Talwar,\nand Li Zhang. 2018. Learning differentially private\nrecurrent language models. In International Confer-\nence on Learning Representations.\nIlya Mironov. 2017. R ´enyi differential privacy. In\n2017 IEEE 30th Computer Security Foundations\nSymposium (CSF), pages 263–275. IEEE.\nArvind Narayanan and Vitaly Shmatikov. 2008. Ro-\nbust de-anonymization of large sparse datasets. In\n2008 IEEE Symposium on Security and Privacy (sp\n2008), pages 111–125. IEEE.\nMilad Nasr, Reza Shokri, and Amir Houmansadr.\n2019. Comprehensive privacy analysis of deep\nlearning: Passive and active white-box inference at-\ntacks against centralized and federated learning. In\n2019 IEEE symposium on security and privacy (SP),\npages 739–753. IEEE.\nNicolas Papernot, Shuang Song, Ilya Mironov, Ananth\nRaghunathan, Kunal Talwar, and ´Ulfar Erlingsson.\n2018. Scalable private learning with pate. arXiv\npreprint arXiv:1802.08908.\nYifan Peng, Shankai Yan, and Zhiyong Lu. 2019.\nTransfer learning in biomedical natural language\nprocessing: An evaluation of bert and elmo on ten\nbenchmarking datasets. In Proceedings of the 2019\nWorkshop on Biomedical Natural Language Process-\ning (BioNLP 2019), pages 58–65.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nAlexey Romanov and Chaitanya Shivade. 2018.\nLessons from natural language inference in the clin-\nical domain. arXiv preprint arXiv:1808.06752.\nReza Shokri, Marco Stronati, Congzheng Song, and\nVitaly Shmatikov. 2017. Membership inference at-\ntacks against machine learning models. In 2017\nIEEE Symposium on Security and Privacy (SP) ,\npages 3–18. IEEE.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman. 2019. Superglue: A\nstickier benchmark for general-purpose language un-\nderstanding systems. In Advances in Neural Infor-\nmation Processing Systems, pages 3266–3280.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nSamuel Yeom, Irene Giacomelli, Matt Fredrikson, and\nSomesh Jha. 2018. Privacy risk in machine learning:\nAnalyzing the connection to overﬁtting. In 2018\nIEEE 31st Computer Security Foundations Sympo-\nsium (CSF), pages 268–282. IEEE.\nA Experimental Details\nA.1 Training Details\nWe use the same procedure as used by clinicalBERT\nand bioBERT to train our non-DP models. We split\nthe total number of patients for each data source\ninto a 70-30 train-test split. The dataset statistics\nfor each data source is presented in Table 1. BERT ,\nDistilBERT and GPT 2 are trained on all notes from\ntraining split using non-DP SGD based training.\nThese models are referred to as non-DP models.\nThe learning rate of 1e−5 is used and the models\nare trained for 4 epochs.\nWe use DP-SGD with a gradient clipping thresh-\nold of 1 and varyingσvalues to produce (ϵ,δ)−DP\nmodels with varying ϵ budgets. Allowed failure\nprobability δ is kept constant at 1e−6. To limit\nthe privacy degradation due to group-level privacy\nlosses, we limit each patient’s contribution to the\ntraining dataset to 50 randomly selected samples.\nTo manage additional computational complexity\nintroduced by per sample gradient clipping, we run\nDP-SGD training for 3 epochs instead of 4 in the\ncase of non-DP models. We refer to these models\nas DP models in our paper.\nA.2 Estimation of Patient Disease Proﬁle\nProbability\nICD coding uses a standardized vocabulary (ICD9\nor ICD10) to document the relevant disease for a\npatient. For an admission, a patient may be coded\nwith multiple ICD codes depending on the presen-\ntation of their symptoms and the course of diag-\nnosis. To simplify our analysis, we assume that\neach ICD code is attributed independently. As a\nresult the probability of observing a patient with\nc = {c1,...ck}codes is\np(c) =\n∏\nc\np(ci)\n∏\nC\\c\n(1 −p(ci)). (4)\nHere Cis the set of all ICD codes and c is the set\nof ICD codes for a patient. Since we assume that\neach ICD code is attributed independently, maxi-\nmum likelihood estimate of any code c′is\np(c′) =# ofpatientswithcodec ′\n# oftotalpatients . (5)\nThe same procedure can be followed for admis-\nsion or note level analysis.\nA.3 MedNLI Experiment Details\nAll the model training details for MedNLI experi-\nments are provided in Table 4. All the models were\ntrained over 8 seeds and their ﬁnal performance was\ncalculated by averaging over the top three perfor-\nmance values. MedNLI is a classiﬁcation dataset\nand hence accuracy is used as an evaluation metric\nfor all models.\nMedNLI Dataset\nTrain datapoints 12,626\nTest datapoints 1,421\nLearning Rate 5e−5\nBatch Size 12\nMax Seq. Len. 200\nEpochs 3\nTable 4: Training details for MedNLI experiments.\nA.4 Training Data Details\nMIMIC-III (Johnson et al., 2016) is a publicly\navailable critical care database made available via\nhttps://mimic.physionet.org/.\nHospital 2 data is collected by selecting patients\nwho are suffering from any form of cardiovascular\ndiseases or patients who are suffering from any\ncancerous diseases. The patients were selected\nwith the help of ICD9 and ICD10 codes mentioned\nin their structured medical data. The ICD codes\nfor cardiovascular diseases were used according\nto https://www.questdiagnostics.com/dms/\nDocuments/Other/PDF_MI4632_ICD_9-10_\nCodes_for_Cardio_38365_062915.pdf. The\nICD codes for cancerous diseases were used\naccording to https://seer.cancer.gov/tools/\nconversion/2014/ICD9CM_to_ICD10CM_2014CF.\npdf\nVHA data was collected by randomly selecting\npatients who had hospital visits during the year\n2020. All 2020 EHRs for the selected patients\nwere extracted to produce the dataset.\nB Detailed Results and Plots\nB.1 RDP accounting for DP-SGD models\nWe use RDP (Mironov, 2017) accountant 2 to cal-\nculate ϵ values for δ = 1e−6. For MIMIC-III\nDP BERT models (epoch 1) with σ values of 2.0,\n1.0, 0.1, the ϵ values are 0.223, 0.626, 403. For\n2https://github.com/tensorﬂow/privacy\nepoch 3 the ϵvalues are 0.223, 0.628, 733. Models\nwith σ <0.1 have very large ϵvalues. Since RDP\naccounting is independent of model accounting, ϵ\nvalues for DistilBERT and GPT 2 models show sim-\nilar behavior. Group DP ϵvalues can be obtained\nby multiplying ϵwith group size (k = 50for our\nexperiments).\nThe RDP calculated ϵ values for UMM BERT\nmodels (epoch 1) with σvalues of 2.0, 1.0, 0.1, are\n0.219, 0.553, 391. For epoch 3 the ϵ values are\n0.220,0.553, 619. Similar to MIMIC-III ϵvalues,\nmodels with σ <0.1 have very large ϵvalues.\nB.2 Detailed Results for MIMIC-III Dataset\nIn this section we provide privacy leakage, and\nmodel utility results for non-DP and DP models\ntrained on MIMIC-III dataset.\nB.2.1 Privacy Leakage Results\nFigures 5, 6 and 4 show sample level black box (S-\nBBA) attack privacy leakage for BERT , DistilBERT\nand GPT 2 models.\nFigures 8, 9 and 7 show admission level black\nbox (A-BBA) attack privacy leakage forBERT , Dis-\ntilBERT and GPT 2 models.\nFigures 11, 12 and 10 show patient level black\nbox (P-BBA) attack privacy leakage for BERT , Dis-\ntilBERT and GPT 2 models.\nFigures 14, 15 and 13 show S-AWBA attack\nprivacy leakage for BERT , DistilBERT and GPT 2\nmodels.\nFigures 17, 18 and 16 show S-GWBA attack\nprivacy leakage for BERT , DistilBERT and GPT 2\nmodels.\nB.2.2 LM Loss Results\nFigure 19 and 20 provide masked language mod-\neling loss for all BERT and distilBERT DP models,\ntrained on MIMIC-III. Figure 21 shows autoregres-\nsive LM loss for GPT 2 DP models.\nB.2.3 Privacy Leakage based on Patient\nProﬁles\nIn this section we provide detailed results for corre-\nlations between patient privacy proﬁles and privacy\nleakage. We divide MIMIC-III admissions and\npatients into buckets based on the log normalized\nprobability of their disease proﬁle. The probability\nof an admission or patient’s disease proﬁle is obtain\nusing the procedure outlined in Section A.2. Plots\nof correlation between a bucket’s (admission-level)\nprivacy leakage and its patient disease probability\nare in Figure 22, 23 and 24. Trends for patient\nlevel disease proﬁle and privacy leakage are same\nas shown in the aforementioned plots.\nB.2.4 MedNLI Results for MIMIC-III\nModels\nIn Table 5, we see that Non-DP BERT model\nachieves an accuracy of 0.8023 on MedNLI\ndataset (Romanov and Shivade, 2018) and BERT-\nbase-cased achieved a performance of 0.776 per\n(Alsentzer et al., 2019). All the DP trained model\nachieved a performance higher than BERT-base-\ncased model and quite close to Non-DP trained\nBERT model which shows the utility of ﬁne-tuning\nmodels with differentially private training tech-\nnique.\nB.3 Detailed Results for UMM Dataset\nWe see similar privacy leakage pattern for UMM\nDataset as MIMIC Dataset. We observe highest\nprivacy leakage for Non-DP GPT 2 model using\ngradient-based white box attack (S-GWBA) as\nshown in Fig. 32.\nFig. 27 show sample level black box attack (S-\nBBA) privacy leakages for BERT , DistilBERT and\nGPT 2 models.\nFig. 28 show hospital admission level black box\nattack (A-BBA) privacy leakages for BERT , Dis-\ntilBERT and GPT 2 models.\nFig. 29 show patient level black box attack (P-\nBBA) privacy leakages for BERT , DistilBERT and\nGPT 2 models.\nFig. 30 show negative log likelihood loss for\nBERT , DistilBERT and GPT 2 models.\nFig. 31 show sample level attention-based white\nbox attack (S-AWBA) privacy leakages forBERT ,\nDistilBERT and GPT 2 models.\nFig. 32 show sample level gradient-based white\nbox attack (S-GWBA) privacy leakages for BERT ,\nDistilBERT and GPT 2 models.\nB.4 Results for VHA Dataset\nDue to computational and access constraints for\nVHA dataset, we were only able to evaluate black-\nbox-attack on BERT model for VHA data. The pa-\ntient and sample-level black-box privacy leakages\nare provided in Table 6. We see patterns consistent\nwith MIMIC-III and UMM results. Group-level\nprivacy leakage is higher than sample-level leak-\nage, and increasing epochs increases the privacy\nleakage.\nB.5 Useful Notations\nLM: Language Model\nCLM: Clinical Language Model\nDP: Differentially Private\nRDP: R´enyi Differential Privacy\nPL: Privacy Leakage\nOOT: out-of-training\nA-BBA: Admission level Black Box Attack\nS-BBA: Sample level Black Box Attack\nP-BBA: Patient level Black Box Attack\nS-GWBA: Sample level Gradient-based White Box\nAttack\nS-AWBA: Sample level Attention-based White Box\nAttack\nMedNLI: A Natural Language Inference Dataset\nfor the Clinical Domain (Romanov and Shivade,\n2018)\nBERT Accuracy\nσ Epoch 1 Epoch 2 Epoch 3\nNon-DP - 0.7922 0.7938 0.8023\nDP\n1e-4 0.8095 0.7978 0.8086\n1e-3 0.7903 0.8037 0.7952\n1e-2 0.7933 0.8069 0.7926\n1e-1 0.8055 0.7954 0.7861\n1.0 0.7926 0.8023 0.7936\n2.0 0.7962 0.8015 0.7929\n1e-20 0.7884 0.8098 0.7896\nTable 5: Results for all three epochs of DP and Non-DPBERT models. Gradient clip value for DP models is set to 1\nand group samples are limited to 50. The Non-DP trained BERT model at Epoch 3 is similar to clinicalBERT model\n(Alsentzer et al., 2019) which has the reported performance of 0.808 on MedNLI dataset. The BERT-base-cased\nmodel which is not ﬁnetuned on MIMIC-III data achieved an accuracy of 0.776 per (Alsentzer et al., 2019).\nFigure 4: Privacy leakage for GPT 2 models with varying σvalues using MIMIC-III data. Leakage obtained using\nsample level black box attack (S-BBA). Gradient clip value is 1, group samples are limited to 50.\nFigure 5: Privacy leakage for BERT models with varying σvalues using MIMIC-III data. Leakage obtained using\nsample level black box attack (S-BBA). Gradient clip value is 1, group samples are limited to 50.\nFigure 6: Privacy leakage for Distil BERT models with varying σvalues using MIMIC-III data. Leakage obtained\nusing sample level black box attack (S-BBA). Gradient clip value is 1, group samples are limited to 50.\nFigure 7: Group Privacy leakage for GPT 2 models with varying σvalues using MIMIC-III data. Leakage obtained\nusing admission level black box attack (A-BBA). Gradient clip value is 1, group samples are limited to 50.\nFigure 8: Group privacy leakage for BERT models with varying σvalues using MIMIC-III data. Leakage obtained\nusing admission level black box attack (A-BBA). Gradient clip value is 1, group samples are limited to 50.\nFigure 9: Group privacy leakage for Distil BERT models with varying σ values using MIMIC-III data. Leakage\nobtained using admission level black box attack (A-BBA). Gradient clip value is 1, group samples are limited to\n50.\nFigure 10: Group privacy leakage forGPT 2 models with varyingσvalues using MIMIC-III data. Leakage obtained\nusing patient level black box attack (P-BBA). Gradient clip value is 1, group samples are limited to 50.\nFigure 11: Group privacy leakage forBERT models with varyingσvalues using MIMIC-III data. Leakage obtained\nusing patient level black box attack (P-BBA). Gradient clip value is 1, group samples are limited to 50.\nFigure 12: Group privacy leakage for Distil BERT models with varying σ values using MIMIC-III data. Leakage\nobtained using patient level black box attack (P-BBA). Gradient clip value is 1, group samples are limited to 50.\nFigure 13: Privacy leakage for GPT 2 models with varying σvalues using MIMIC-III data. Leakage obtained using\nAttention based white box attack (S-AWBA). Gradient clip value is 1, group samples are limited to 50.\nFigure 14: Privacy leakage for BERT models with varying σvalues using MIMIC-III data. Leakage obtained using\nAttention based white box attack (S-AWBA). Gradient clip value is 1, group samples are limited to 50.\nFigure 15: Privacy leakage for DistilBERT models with varying σvalues using MIMIC-III data. Leakage obtained\nusing Attention based white box attack (S-AWBA). Gradient clip value is 1, group samples are limited to 50.\nFigure 16: Privacy leakage for GPT 2 models with varying σvalues using MIMIC-III data. Leakage obtained using\nGradient based white box attack (S-GWBA). Gradient clip value is 1, group samples are limited to 50.\nFigure 17: Privacy leakage for BERT models with varying σvalues using MIMIC-III data. Leakage obtained using\nGradient based white box attack (S-GWBA). Gradient clip value is 1, group samples are limited to 50.\nFigure 18: Privacy leakage for DistilBERT models with varying σvalues using MIMIC-III data. Leakage obtained\nusing Gradient based white box attack (S-GWBA). Gradient clip value is 1, group samples are limited to 50.\nFigure 19: Test data LM loss for DP BERT models with varying σ values using MIMIC-III data. Gradient clip\nvalue is 1, group samples are limited to 50. Loss of BERT -base-cased model that is not trained on clinical data is\n3.49. All DP models with less than 3.49 loss plots have increased model utility due to DP training. Non-DP CLM\nmodel loss is 0.60,0.57,0.55,0.53 for epoch 1 - 4. Non-DP mlm loss is lower than all DP models.\nFigure 20: Test data LM loss for DP Distil BERT models with varying σ values using MIMIC-III data. Gradient\nclip value is 1, group samples are limited to 50. Loss of Distil BERT -base model that is not trained on clinical data\nis 3.49. All DP models with less than 3.49 loss plots have increased model utility due to DP training. Non-DP\nCLM model loss is 0.63,0.59,0.58,0.58 for epoch 1 - 4. Non-DP mlm loss is lower than all DP models.\nFigure 21: Test data LM loss for GPT 2 models with varying σvalues using MIMIC-III data. Gradient clip value\nis 1, group samples are limited to 50. Loss of GPT 2-base model that is not trained on clinical data is 3.49. All DP\nmodels with less than 3.49 loss plots have increased model utility due to DP training. Non-DP CLM model loss is\n1.2,1.19,1.17,1.16 for epoch 1 - 4.\n(a)\n (b)\n(c)\n (d)\nFigure 22: Privacy leakage vs log-normalized probability plots for Non-DP trained BERT models on MIMIC-III\ndata. We see the negative correlation between probability of disease proﬁle (admission-level) and privacy leakage.\n(a)\n (b)\n(c)\n (d)\nFigure 23: Privacy leakage vs log-normalized probability plots for Non-DP trained GPT 2 models on MIMIC-III\ndata. We see the negative correlation between probability of disease proﬁle (admission-level) and privacy leakage.\n(a)\n (b)\n(c)\n (d)\nFigure 24: Privacy leakage vs log-normalized probability plots for Non-DP trained DistilBERT models on MIMIC-\nIII data. We see the negative correlation between probability of disease proﬁle (admission-level) and privacy\nleakage.\nBERT epoch S-BBA 1 P-BBA\n1 1.27 0.92\n2 1.61 1.42\n3 1.89 2.09\n4 2.03 2.91\nTable 6: Sample and patient-level black-box attack pri-\nvacy leakage for BERT model using VHA data.\nFigure 25: Black box attack privacy leakage for\nMIMIC-III data with hospital admission level aggre-\ngate.\nFigure 26: Black box attack privacy leakage for\nMIMIC-III data with patient level aggregate.\nFigure 27: Sample-level Black box attack (S-BBA) privacy leakage using Hospital 2 data for Non-DP and DP\ntrained variants of BERT , DistilBERT and GPT 2. For all DP models, the Gradient clip value is set to 1 and group\nsamples are limited to 50.\nFigure 28: Admission-level Black box attack (A-BBA) group privacy leakage using Hospital 2 data for Non-DP\nand DP trained variants of BERT , DistilBERT and GPT 2. For all DP models, the Gradient clip value is set to 1 and\ngroup samples are limited to 50.\nFigure 29: Patient-level Black box attack (P-BBA) group privacy leakage using Hospital 2 data for Non-DP and\nDP trained variants of BERT , DistilBERT and GPT 2. For all DP models, the Gradient clip value is set to 1 and group\nsamples are limited to 50.\nFigure 30: Negative Log Likelihood Loss for Non-DP CLMs and their DP trained variants with varying σvalues.\nFor all DP models, the Gradient clip value is set to 1 and group samples are limited to 50.\nFigure 31: Privacy leakage using Attention-based white-box attack (S-AWBA) for Hospital 2 at sample level.\nModels shown are Non-DP and DP trained variants ofBERT , DistilBERT and GPT 2. For all DP models, the Gradient\nclip value is set to 1 and group samples are limited to 50.\nFigure 32: Privacy leakage using Gradient-based white box attack (S-GWBA) for Hospital 2 at sample-level.\nModels shown are Non-DP and DP trained variants ofBERT , DistilBERT and GPT 2. For all DP models, the Gradient\nclip value is set to 1 and group samples are limited to 50.",
  "topic": "Inference",
  "concepts": [
    {
      "name": "Inference",
      "score": 0.8401315808296204
    },
    {
      "name": "Computer science",
      "score": 0.7497168779373169
    },
    {
      "name": "Machine learning",
      "score": 0.5499057173728943
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5247640609741211
    },
    {
      "name": "Private information retrieval",
      "score": 0.49390512704849243
    },
    {
      "name": "Empirical research",
      "score": 0.48605209589004517
    },
    {
      "name": "Information leakage",
      "score": 0.4835178852081299
    },
    {
      "name": "Artificial neural network",
      "score": 0.46954020857810974
    },
    {
      "name": "Deep neural networks",
      "score": 0.44359689950942993
    },
    {
      "name": "Data mining",
      "score": 0.3599771559238434
    },
    {
      "name": "Computer security",
      "score": 0.31138986349105835
    },
    {
      "name": "Statistics",
      "score": 0.13171443343162537
    },
    {
      "name": "Mathematics",
      "score": 0.08636462688446045
    }
  ],
  "institutions": [],
  "cited_by": 20
}