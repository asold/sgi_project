{
  "title": "Improved Tasmanian devil optimization algorithm for parameter identification of electric transformers",
  "url": "https://openalex.org/W4389202877",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5015767236",
      "name": "Rizk M. Rizk‐Allah",
      "affiliations": [
        "Menoufia University"
      ]
    },
    {
      "id": "https://openalex.org/A5019940948",
      "name": "Ragab A. El‐Sehiemy",
      "affiliations": [
        "Kafrelsheikh University"
      ]
    },
    {
      "id": "https://openalex.org/A5007876633",
      "name": "Mohamed I. Abdelwanis",
      "affiliations": [
        "Kafrelsheikh University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2736591037",
    "https://openalex.org/W3216228099",
    "https://openalex.org/W3151993534",
    "https://openalex.org/W2789557361",
    "https://openalex.org/W2099709349",
    "https://openalex.org/W2130178280",
    "https://openalex.org/W2135802903",
    "https://openalex.org/W2099997740",
    "https://openalex.org/W1614409008",
    "https://openalex.org/W2325006535",
    "https://openalex.org/W2049085337",
    "https://openalex.org/W2783385030",
    "https://openalex.org/W3004829472",
    "https://openalex.org/W2964279372",
    "https://openalex.org/W2344167889",
    "https://openalex.org/W2998500962",
    "https://openalex.org/W2888113735",
    "https://openalex.org/W2912469071",
    "https://openalex.org/W3009241639",
    "https://openalex.org/W3162718815",
    "https://openalex.org/W2018710384",
    "https://openalex.org/W2317513259",
    "https://openalex.org/W2760572903",
    "https://openalex.org/W2754358426",
    "https://openalex.org/W2942778406",
    "https://openalex.org/W2604464897",
    "https://openalex.org/W2000082448",
    "https://openalex.org/W2766509183",
    "https://openalex.org/W2876345111",
    "https://openalex.org/W2539927220",
    "https://openalex.org/W3182073119",
    "https://openalex.org/W2972312022",
    "https://openalex.org/W2897177582",
    "https://openalex.org/W3120614823",
    "https://openalex.org/W2969742549",
    "https://openalex.org/W4213364920",
    "https://openalex.org/W2730530191",
    "https://openalex.org/W3005653817",
    "https://openalex.org/W3103375983",
    "https://openalex.org/W2151554678",
    "https://openalex.org/W2955425038",
    "https://openalex.org/W2171287075",
    "https://openalex.org/W3168798875",
    "https://openalex.org/W3159284717",
    "https://openalex.org/W4367846823",
    "https://openalex.org/W2991568040",
    "https://openalex.org/W3140492707",
    "https://openalex.org/W2768181028",
    "https://openalex.org/W2276743969",
    "https://openalex.org/W3136781413",
    "https://openalex.org/W4360997947",
    "https://openalex.org/W2580486137",
    "https://openalex.org/W2559135426",
    "https://openalex.org/W2585690110",
    "https://openalex.org/W2884948943",
    "https://openalex.org/W2971277042",
    "https://openalex.org/W4286322077",
    "https://openalex.org/W2969632122",
    "https://openalex.org/W3006746053",
    "https://openalex.org/W3125643987",
    "https://openalex.org/W2156773695",
    "https://openalex.org/W4246622333"
  ],
  "abstract": "Abstract Tasmanian devil optimization (TDO) algorithm represents one of the most recent optimization algorithms that were introduced based on the nature behavior of Tasmanian devil behavior. However, as a recent optimizer, its performance may provide inadequate balance among the exploitation and exploration abilities, especially when dealing with the multimodal and high-dimensional natures of optimization tasks. To overcome this shortage, a novel variant of the TDO, called improved Tasmanian devil optimization (ITDO), is introduced in this paper. In ITDO, two competitive strategies are embedded into TDO to enrich the scope of the searching capability with the aim of improving the diversification and identification of the algorithm. The effectiveness of the ITDO algorithm is examined by validating its performance on CEC 2020 benchmark functions with different landscape natures. The recorded results proved that the ITDO is very competitive with other counterparts. After ITDO exhibited a sufficient performance, then, it was applied to estimate the parameters of the 1 kVA, 230/230 V, single-phase transformer. Some assessment metrics along with convergence analysis are conducted to affirm the performance of the proposed algorithm. The recorded results confirm the competitive performance of the proposed method in comparison with the other optimization methods for the benchmark functions and can identify the accurate parameters for the single-phase transformer as the estimated parameters by ITDO are highly coincident with the experimental parameters.",
  "full_text": "ORIGINAL ARTICLE\nImproved Tasmanian devil optimization algorithm for parameter\nidentification of electric transformers\nRizk M. Rizk-Allah1 • Ragab A. El-Sehiemy2 • Mohamed I. Abdelwanis2\nReceived: 11 December 2022 / Accepted: 3 November 2023 / Published online: 30 November 2023\n/C211The Author(s) 2023\nAbstract\nTasmanian devil optimization (TDO) algorithm represents one of the most recent optimization algorithms that were\nintroduced based on the nature behavior of Tasmanian devil behavior. However, as a recent optimizer, its performance may\nprovide inadequate balance among the exploitation and exploration abilities, especially when dealing with the multimodal\nand high-dimensional natures of optimization tasks. To overcome this shortage, a novel variant of the TDO, called\nimproved Tasmanian devil optimization (ITDO), is introduced in this paper. In ITDO, two competitive strategies are\nembedded into TDO to enrich the scope of the searching capability with the aim of improving the diversiﬁcation and\nidentiﬁcation of the algorithm. The effectiveness of the ITDO algorithm is examined by validating its performance on CEC\n2020 benchmark functions with different landscape natures. The recorded results proved that the ITDO is very competitive\nwith other counterparts. After ITDO exhibited a sufﬁcient performance, then, it was applied to estimate the parameters of\nthe 1 kVA, 230/230 V, single-phase transformer. Some assessment metrics along with convergence analysis are conducted\nto afﬁrm the performance of the proposed algorithm. The recorded results conﬁrm the competitive performance of the\nproposed method in comparison with the other optimization methods for the benchmark functions and can identify the\naccurate parameters for the single-phase transformer as the estimated parameters by ITDO are highly coincident with the\nexperimental parameters.\nKeywords Single-phase transformer /C1Tasmanian devil optimization /C1Parameter estimation /C1Equivalent circuit /C1\nCEC benchmark problems\n1 Introduction\nIn electrical power systems, the transformer is a critical\ncomponent [ 1–3]. The transformer is represented by its\nsteady-state model. The trans former model’s parameters\nindicate its performance un der various scenarios [ 4]. The\nproper estimation of transformer steady-state model parame-\nters aids in the power transformer monitoring procedure. The\nrequirement to improve the performance characteristics of\ntransformers in both steady-state and transient circumstances\nnecessitates the use of an accurate estimate process [5, 6]. The\nparameter estimation procedure is inﬂuenced by the presence\nof harmonics, saturation and tr ansient circumstances in the\ntransformer. To obtain an accu rate calculation of the trans-\nformer parameters, real-tim ed a t aw e r ee m p l o y e di nc o n -\njunction with frequency response [7] or time-domain analysis\n[8–11]. By measuring the inrush cu rrent, the saturation of the\ntransformer core was taken i nto account while estimating\nparameters [12]. The load terminal data, phase measurement\nunit (PMU), inrush current test , and open-circuit and short-\ncircuit tests are all part of the real-time measurement process.\nIn most circumstances, this technique necessitates turning off\nthe transformer, which is regarded as an unrealistic approach.\n& Mohamed I. Abdelwanis\nmohamed.soliman4@eng.kfs.edu.eg\nRizk M. Rizk-Allah\nrizk_masoud@sh-eng.menoﬁa.edu.eg\nRagab A. El-Sehiemy\nelsehiemy@eng.kfs.edu.eg\n1 Basic Engineering Science Department, Faculty of\nEngineering, Menouﬁa University, Shebin El-Kom 32511,\nEgypt\n2 Department of Electrical Engineering, Faculty of\nEngineering, Kafrelsheikh University,\nKafr El-Sheikh, 33516, Egypt\n123\nNeural Computing and Applications (2024) 36:3141–3166\nhttps://doi.org/10.1007/s00521-023-09240-2(0123456789().,-volV)(0123456789().,- volV)\nOptimization approaches are one of the most preferred\nmethods for resolving various electrical issues such as\nElectric machines, transformers, power lines, fuel cells and\nphotovoltaic modules, batteries, management of a soft open\npoint electrical distribution system, optimal power ﬂow\nproblem [ 13–18]. The optimization approaches evaluate\nthe real and estimated value to reduce the variance between\nboth cases [ 19–21]. The nameplate value is used as real\nvalue in several optimization algorithms [ 8, 11]. Coyote\noptimization algorithm (COA) [ 22], Jaya optimization\nalgorithm (JOA) and particle swarm optimization (PSO)\nwere used to estimate the parameters of equivalent circuit\nand motor design [ 14, 23]. Finally, the ideal estimated\nparameters are obtained by reducing the error between the\nreading values of power and voltage at various loading\ncircumstances and the estimated values [ 5, 24–27].\nIn [ 28, 29], the chaotic optimization algorithm and\ngravitational search algorithms (GSA) are used to estimate\nthe single-phase transformer parameters using the load test\nand nameplate data. By using short- and open-circuit tests,\nthe bacterial foraging algorithm (BFA) is utilized to esti-\nmate the parameters data of three and single-phase trans-\nformer in [ 30]. Additionally, the artiﬁcial bee colony\nalgorithm is used to estimate parameters of transformer in\n[31]. All these methods can be used with nameplate and\nload information while the transformer is in operation,\neliminating the need to shut it down for testing.\nFurthermore, these methods can be used to estimate the\nparameters of three-phase transformer like single-phase\ntransformer parameters [ 32–34] and for the best design of a\nﬂux reversal high-speed three-phase machine [ 35]. Refer-\nences [ 36, 37] developed the coyote optimization algorithm\n(COA), a revolutionary metaheuristic optimization tech-\nnique. COA has been used for a variety of purposes in the\nliterature, including optimizing estimated parameters of\nfuel cell in [ 38] and estimating solar cell in [ 39]. TDO\npresents one of most recent optimization algorithms that\nwere proposed by Dehghani et al. [ 40] to simulate the\nnature behavior of the Tasmanian devil during the\nportaging process. The Tasmanian devil insurance popu-\nlation is under constant observation, and imperfect detec-\ntion and risk optimization has presented itself in [ 41].\nPSO is a well-liked approach; however, because of its\nextremely basic features, applying it to a problem is not\ndifﬁcult. Similar to GA, one of PSO’s biggest drawbacks is\nthat, if its search range is too broad, it might easily fall into\nlocal search regions. Its convergence to a global optimum\nis therefore seldom certain. It is obvious that COA pre-\nsently has major drawbacks that prevent convergence to the\nglobal optimum [ 42]. Despite these beneﬁts, the Jaya\nalgorithm has many drawbacks, such as unintended early\nconvergence and the potential for getting stuck in local\nminima because of inadequate population [ 43].\nMultiple approaches have been proposed in the literature\nto estimate transformers parameters without experimental\nwork, using nameplate values used to determines the best\nvalues of transformer parameters to minimize absolute\nsquare error of computed and nameplate currents and\nvoltages [ 22\n].\nDespite the fact that metaheuristic methodologies can\noffer more reliable and accurate ﬁndings, the effectiveness\nof these methodologies is impacted by the depth of the\nsearch involved with the multimodal prospective of the\nelectric transformer models and the reliance on adjusting\nalgorithmic parameters, such as generation cycle and\npopulation size. Additionally, some algorithms include a\nnumber of particular control settings that signiﬁcantly\naffect their performances and any inappropriate settings\ncan raise the computational burden, and degrade the opti-\nmization accuracy with increasing the chance of getting\ntrapped in local optima. Hence, there is still room for\nenhancement of the performance of the existing opti-\nmization schemes or developing new variants which can be\nsupported by the perspective of the ‘‘no free lunch (NFL)’’\ntheory [ 44]. The relevant theory brieﬂy implies that there is\nno single efﬁcient algorithm that can address all opti-\nmization problems and outperform all the others. There-\nfore, the NFL theorem has prompted numerous researchers\nto enhance/modify the performance of current methods or\ndevelop new optimization schemes hoping to guarantee a\nstrong performance while tackling various complicated\noptimization tasks. The aforementioned justiﬁes why the\nTasmanian devil optimization (TDO) [ 22] is presented as a\npromising alternative in this work to assess its efﬁcacy in\ncharacterizing the parameters of electrical transformer at\naiming to ensure a reliable operation for power system\nstation.\nRecently, the TDO was proposed by Dehghani et al. [ 40]\nto simulate the nature behavior of the Tasmanian devil\nduring the food chasing process. The Tasmanian devil uses\ntwo distinct ways of feeding. The Tasmanian devil initially\neats any carcass it comes upon. The second way is that it\nstrikes the prey before eating it. The Tasmanian devil\neating process was modeled to develop the TDO. The\nperformance of TDO was assessed on 23 benchmark\nfunctions and the four-engineering design optimization.\nThe results show the efﬁcacy and efﬁciency of TDO\nthrough comparisons with some well-known state-of-the-\nart methods [ 40].\nDespite the fact that TDO does not include any algo-\nrithm-speciﬁc controls, it still has two speciﬁc shortcom-\nings. One is that there is not enough diversity in the\npopulation, which reduces the coverage of some promising\nareas and degrades the searching efﬁciency, and the other\none is the lack for the leading strategy as the updating\nprocess is performed based on random searching which\n3142 Neural Computing and Applications (2024) 36:3141–3166\n123\nmay lead to an imbalance among explorative and\nexploitative phases while increasing the chance of trapping\nin the local optima. Besides, no endeavors to adopt TDO\nfor parameters estimation of electrical transformer have\nbeen purported in the literature.\nThis paper suggests an improved version of TDO using\nthe Levy ﬂight and spiralize learning strategies, named\nITDO, for estimating transformer’ parameters precisely\nand reliably. In the ITDO, Levy ﬂight strategy is integrated\nto scan the immediate areas of the promising solutions\nwithin the search space based on the Levy distribution\nperspective, thus maintaining the diversity of solutions.\nFurthermore, the spiralize learning strategy is promoted\nbased on the logarithmic spiral conception to deepen the\nsearching in the neighborhood of the promising solutions,\nthus emphasizing the exploitation ability. By this way, it is\nattempted to strike a good balance among the phases of\nexplorative and exploitative abilities. The effectiveness of\nthe ITDO algorithm is tested and examined through vali-\ndating its performance on 33 benchmark problems which\ninclude classical test functions and CEC 2020 benchmarks\nwith different landscape natures, where the comparisons\nwere made with other optimization well-known methods\n[45, 46], including Harris hawk optimization (HHO) [ 47],\nsine–cosine algorithm (SCA) [ 48], Elman neural network\nfor predicting the compressive and ﬂexural strengths [ 49],\njellyﬁsh (JS) [ 34], grey wolf optimization (GWO) [ 50, 51],\ndifferential evolution (DE) [ 52, 53], particle swarm opti-\nmization (PSO) [ 54], double internal loop higher-order\nrecurrent [ 55], adaptive control using a diagonal recurrent\nneural network [ 56], radial basis function network [ 57],\nadaptive dynamic programming for control and detection\nof Lyapunov stability [ 58], identiﬁcation and adaptive\npredictive control using self-recurrent wavelet neural net-\nworks [ 59], neural network with external recurrence and\nLyapunov stability analysis [ 60]. The statistical evaluations\nand nonparametric test using Wilcoxon test have ascer-\ntained the competitive and progressive performance of the\nITDO when compared with other counterparts. On the\nother hand, the applicability of the ITDO is veriﬁed\nthrough characterizing the parameters of single-phase\ntransformer.\nThe results revealed that the ITDO algorithm outper-\nforms the other methods while achieving more accurate\nparameters of the electrical transformer model. The fol-\nlowing are the primary characteristics of this work.\n1. A newly developed ITDO approach based on the Levy\nﬂight and spiralize learning strategies, named ITDO, is\npresented to deal with the classical test functions, CEC\n2020 benchmarks and electrical transformer model.\n2. Levy ﬂight is purported with the TDO to retain the\nsolutions’ diversity, thus emphasizing the exploration\nsearches, while spiralize learning strategy is integrated\nto deepen the neighborhood pattern while enhancing\nthe exploitation propensities.\n3. The efﬁcacy of the suggested ITDO algorithm is\nveriﬁed through the comprehensive comparisons using\neight state-of-the-art methods, where some perfor-\nmance indices include statistical tendency measures,\nconvergence rate and descriptive analysis using box\nplot diagram, and Wilcoxon test has afﬁrmed the great\nperformance of the proposed variant.\n4. The ITDO’ applicability is discovered by characteriz-\ning the accurate design parameters of the transformer\nequivalent circuit, and the reported outcomes proved\nthe superiority of the ITDO as it highly achieves\ncoincidence characteristics in terms of the efﬁciency\nand voltage regulation.\nThe sections of this study are organized as follows:\nSect. 2 provides the statement for the function optimization\nand mathematical model of transformer characteristics.\nSection 3 presents the mathematical formulation of the\noriginal TDO and introduces the improved ITDO variant.\nSection 4\nis to evaluate the performance of the ITDO on\nglobal optimization functions and extracting effective\nparameters of the transformer equivalent circuit. Finally, in\nSect. 5, conclusions of this study are discussed.\n2 Problem statements\nThis section describes the function optimization statement\nas well as the equivalent circuit of transformer with its\noptimization modeling.\n2.1 Function optimization (FO)\nFO is the process of ﬁnding the optimum solution for the\ncandidate function of a several control variables over a\ncertain search space. Several real-world situations may be\nformulated as FO problem using the following form [ 22]:\nFind x ¼ x\n1; x2; ... ; xnðÞ T 2 Rn such that\nMin f ðxÞ\nSubject to x\nminjmax\nj\nj\nð1Þ\nwhere x stands for the solution vector that involves n\ndimensions, Rn is the set of real numbers, f(x) indicates the\ngoal function, and xmax\ni and xmin\ni deﬁne, respectively, the jth\nvariable’ supper and the lower bounds.\nNeural Computing and Applications (2024) 36:3141–3166 3143\n123\n2.2 Transformer equivalent circuit\nand optimization model\n2.2.1 Mathematical model of transformer characteristics\nThe current section discusses the mathematical represen-\ntation of transformer models and the optimization of\nparameter estimation model. Figure 1 shows the per phase\ntransformer’s steady-state equivalent circuit [ 22].\nThe following relationships can be found by applying\nKirchhoff voltage and current rules to the transformer per\nphase equivalent circuit using Fig. 1:\nE1 ¼ V0\n2 þ I0\n2\nZ0\n2\n: ð2Þ\nV1 ¼ E1 þ I1Z1 ð3Þ\nI1 ¼ Io þ I0\n2 ð4Þ\nE1 ¼ IoZm ð5Þ\nThe primary current I1 and primary induced voltage E1\ncan be calculated using the four equations as follows:\nI1 ¼ V1 þ I0\n2Z0\n2\nZ1 þ Zm\nð6Þ\nE1 ¼ V1 /C0 I1Z1 ð7Þ\nThe secondary referred voltage can be calculated from:\nV0\n2 ¼ E1 /C0 I0\n2\nZ0\n2\nð8Þ\nAs an equal value for the core magnetizing current value\nIm and the core active loss current value Im, the core current\nIo can be calculated as follows:\nIo ¼ E1\nZm\n¼ E1\nRm\n/C0 j E1\nXm\n¼ Ieþh /C0 jIm ð9Þ\nThe voltage regulation of the transformer is as follows:\nVR ¼ V1 /C0 V0\n2\nV1\nð10Þ\nThe following formulas can be used to calculate the, the\npower input Pin, the power output Po, and primary power\nfactor pf1:\nPo ¼ realðV0\n2 /C2 I0/C3\n2 Þð 11Þ\nPin ¼ realðV1 /C2 I/C3\n1 Þð 12Þ\npf1 ¼ cosðangleðI1ÞÞ ð 13Þ\nThe transformer’s efﬁciency can be estimated using the\nfollowing formula:\ngr ¼ Po\nPin\nð14Þ\nThe corresponding circuit parameters R2, R1, Rm, X2, X1\nand Xm must be identiﬁed as precisely as feasible. Opti-\nmization methods are used to achieve this goal.\n2.2.2 Optimization of parameter estimation model\nMinimizing the difference between the estimated and\nmanufacturer’s values represents the objective or goal\nfunction regarding the parameter estimation issue of elec-\ntric power transformer. In this sense, the estimated\nparameters, R\n1; X1; R2; X2; Reþh; Xm, can affect Eqs. ( 2)–\n(14) that, in turn, affects efﬁciency, the primary current and\nthe power factor of the load. Therefore, it is necessary to\nassess the estimated values with the manufacturer’s data.\nThe fractional error of the transformer efﬁciency, pri-\nmary current and primary power factor can be calculated\nfrom:\nf1 ¼ egr /C0 mgr\nmgr\nð15Þ\nf2 ¼ eI1 /C0 mI1\nmI1\nð16Þ\nf3 ¼ epf /C0 mpf\nmpf ð17Þ\nIn this case, achieving an accurate estimate requires\nminimizing the sum of relative error squares (SRRS)\nbetween the estimated and manufacturer’s values. The\nfollowing formula can be used to express the SRRS.\nSRRS ¼ f 2\n1 þ f 2\n2 þ f 2\n3 ð18Þ\nThe following is the presentation of this work’s objec-\ntive function (OF):\nOF ¼ min ðSRRSÞð 19Þ\nBoundary restrictions on the control variables are sub-\njected to Eq. ( 20) as follows:\nFig. 1 Per phase equivalent circuit of transformer\n3144 Neural Computing and Applications (2024) 36:3141–3166\n123\nRmin\n1 /C20R1 /C20Rmax\n1 ; Xmin\n1 /C20X1 /C20Xmax\n1\nRmin\n2 /C20R2 /C20Rmax\n2 ; Xmin\n2 /C20X2 /C20Xmax\n2\nRmin\nm\n/C20Rm /C20Rmax\nm ; Xmin\nm /C20Xm /C20Xmax\nm\nð20Þ\n3 The proposed methodology\nThe primary framework for the Tasmanian devil opti-\nmization (TDO) is presented in this section. Additionally,\nthe proposed ITDO variant’s structure is described.\n3.1 The basics of TDO algorithm\nIn this section, the mathematical model regarding the\nTasmanian devil optimization (TDO) is introduced [ 40].\nTDO presents one of most recent optimization algorithms\nthat were proposed by Dehghani et al. [ 40] to simulate the\nnature behavior of the Tasmanian devil during the\nportaging process. TDO was proposed based on Tasmanian\ndevil behavior while searching for food source. It offers\ntwo strategies: attacking and eating live prey or carrion on\ndead animals. TDO is investigated on 23 benchmark\nfunction to evaluate its performance. For further analysis, it\nis applied for optimizing four engineering design tasks. Its\nperformance is evaluated by comparisons with eight well-\nknown optimization algorithms, where the simulation\nresults have been afﬁrmed its competitive performance.\nThe main stages of TDO are described in detail as follows.\n3.1.1 Initialization\nTDO is like the other population-based algorithms that start\nits iterative process with some searcher agents named\nTasmanian devils. In this regard, a population of agents is\ncreated randomly within the search space; each agent of a\npopulation represents a vector that contains several ele-\nments equal to the number variables. This initialization is\nexpressed as follows.\nX\nij ¼ xmin\nj þ rand: xmax\nj /C0 xmin\nj\n/C16/C17\n;\ni ¼ 1; 2; ... ; M; j ¼ 1; 2; ... ; n\nð21Þ\nwhere rand denotes for a random value generated based on\nuniform distribution from the interval [0,1]. xmin\nj and xmax\nj\nstand for lower and maximum limits regarding the jth\ndimension search space. M and n deﬁne the size of Tas-\nmanian devils and total number of variables. After the\ninitialization, the quality of the candidate solution is eval-\nuated by the objective function (OF). Based on the OF, the\nbest objective function value is regarded as best member in\nthe population. This best member is updated each iteration\nby Tasmanian devil feeding tactics. Any Tasmanian devil\nhas the ability to consume carrion or seek food. In TDO, it\nis presumed that there is a 50% chance that one of these\ntactics will be chosen. According to this idea, just one of\nthese two procedures is used to update each Tasmanian\ndevil in TDO iteration.\n3.1.2 Eating carrion: exploration stage\nSometimes, rather than going hunting, Tasmanian devils\nchoose to eat local carrion. In the vicinity of the Tasmanian\ndevil, there are other predatory creatures that seek enor-\nmous prey but are unable to consume it all. Additionally,\nthese creatures might not be able to consume enough of\ntheir prey until the Tasmanian devil shows up. In this\ncontext, Tasmanian devil prefers to eat these carrions. The\nbehavior of the Tasmanian devil when searching for car-\nrion in its habitat is similar to the algorithm iterative pro-\ncess in a problem-solving environment. The Tasmanian\ndevil tactic truly exempliﬁes the potential of TDO explo-\nration in identifying the initial optimal location by scanning\nseveral regions of the search ﬁeld. In terms of optimization\nviewpoint, for each Tasmanian devil, it is presumed that\nother population members’ location within the search area\ncorresponds to carrion locations. In this line, one of these\nsituations is selected randomly as the target carrion for the\nith Tasmaniandevil as follows.\nC\ni ¼ Xk; i ¼ 1; 2; ... ; M; k 2 1; 2; ... ; Mjk 6¼ ifg ð22Þ\nwhere Ci stands for the chosen carrion by ith Tasmanian\ndevil and k is randomly chosen from 1 to M.\nAccording to the selected carrion, the new location of\nthe Tasmanian devil is determined as follows.\nxnew;S1\ni;j ¼ xij þ r /C1cij /C0 I /C1xij\n/C0/C1\n; FCi \\Fi\nxij þ r /C1xij /C0 cij\n/C0/C1\n; otherwise\n/C26\nð23Þ\nXi ¼ Xnew;S1\ni ; Fnew;S1\ni \\Fi\nXi; otherwise\n/C26\nð24Þ\nHere, Xnew;S1\ni deﬁnes the new position of the ith Tas-\nmanian devil using the ﬁrst strategy, xnew;S1\ni;j is the element\nof the jth dimension (variable), Fnew;S1\ni deﬁnes the objective\nfunction value for the new status, FCi denotes objective\nfunction for the chosen carrion, r stands for a random\nnumber within the range [0, 1], and I denotes a random\ninteger number which can be 1 or 2.\n3.1.3 Eating prey (exploitation phase)\nIn this stage, the Tasmanian devil is to hunt and eat prey,\nwhere this is carried out by two phases. In the ﬁrst one, it is\nscanned the region to select the prey that can attack it.\nThen, in the second phase, after approaching the victim, the\nNeural Computing and Applications (2024) 36:3141–3166 3145\n123\nTasmanian devil chases it, stops it and begins to eat.\nTherefore, the modeling of the ﬁrst phase is expressed by\nEqs. ( 25)t o( 27). In the second phase, the updating of the\nith Tasmanian devil is performed by considering the\nlocation of other population members as the prey’s posi-\ntion. Therefore, the kth member is selected at random to\nrepresent prey position. The process of prey selection is\nexpressed as follows.\nP\ni ¼ Xk; i ¼ 1; 2; ... ; M; k 2 1; 2; ... ; Mjk 6¼ ifg ð25Þ\nwhere Pi stands for the chosen prey by ith Tasmanian devil,\nk is chosen randomly from 1 to M.\nAccording to the selected prey, the second phase is\ncarried out to obtain the new position of the Tasmanian\ndevil as follows.\nxnew;S2\ni;j ¼ xij þ r /C1pij /C0 I /C1xij\n/C0/C1\n; FPi \\Fi\nxij þ r /C1xij /C0 pij\n/C0/C1\n; otherwise\n/C26\nð26Þ\nXi ¼ Xnew;S2\ni ; Fnew;S2\ni \\Fi\nXi; otherwise\n/C26\nð27Þ\nHere, Xnew;S2\ni deﬁnes the new position of the ith Tas-\nmanian devil using the second strategy, xnew;S2\ni;j is the ele-\nment of the jth dimension (variable), Fnew;S2\ni deﬁnes the\nobjective function value for the new status, and FPi denotes\nobjective function for the chosen prey.\nThe Tasmanian devil chases its prey throughout the area\nof the attacked location to mimic this process of chasing.\nThe Tasmanian devil uses Eqs. ( 28)–(30) to imitate the\nstage of prey pursuit. At this step, the Tasmanian devil\nlocation is regarded as the center of a neighborhood where\nthe process of hunting prey takes place. The neighborhood\nradius indicates the area within which the Tasmanian devil\nfollows its prey, and it is determined using ( 28). Thus, a\nnew location according to the chasing process within this\nneighborhood is expressed by Eq. ( 29).\nR ¼ 0:01 1 /C0 t=TðÞ ; ð28Þ\nx\nnew\ni;j ¼ xij þ 2r /C0 1ðÞ /C1 R /C1xij ð29Þ\nXi ¼ Xnew\ni ; Fnew\ni \\Fi\nXi; otherwise\n/C26\nð30Þ\nwhere R deﬁnes the neighborhood radius of the attack\nlocation’s point. t and T deﬁne the iteration counter and\nmaximum size of iterations, respectively. Xnew\ni denotes the\nnew position of the ith Tasmanian devil in vicinity of Xi,\nxnew\ni;j deﬁnes the jth element or variable of Xnew\ni and Fnew\ni\ndenotes the new objective function value of Xnew\ni . The\nﬂowchart of TDO is illustrated in Fig. 2.\n3.2 The proposed ITDO\nFor any optimization process, an adequate balance between\nthe use of the search space’s exploration and exploitation is\nrequired to reach a globally optimal solution. Exploration,\nalso known as diversiﬁcation, in the search space entails\nsearching globally, whereas exploitation, also known as\nintensiﬁcation, entails searching locally, based on the best\nsolution available at the time. The algorithm’s performance\nis negatively impacted by excessive exploration and\nexploitation since it increases the chances of trapping in the\nlocal optima and increases the convergence time. The\nconventional TDO employs a randomly generated position\nfrom the population as the prey’s position whether for the\nexploration or exploitation phase. This can increase the\nstrength of the exploration capability of an algorithm but\ncan weaken the exploitation capability of the algorithms by\nexhibiting slow convergence and sometimes leading to\nstagnation at local optima. To avoid this shortage, a novel\nimproved Tasmanian devil optimization (ITDO) is pre-\nsented based on two strategies. Two tactics are embedded\nin the conventional TDO to improvise its solution accuracy\nas well as convergence acceleration, which are ( i) Levy\nﬂight strategy to offer further explorations and ( ii) spiralize\nelite learning strategy to emphasize the exploitation ability.\n3.2.1 Levy flight strategy\nThe Levy distribution (LD) was introduced by the French\nLevy [ 61] as a random ﬂight process based on a probability\ndistribution function. Due to be effective improvement on\nthe algorithm design, it is ﬂourished in some of the recently\nreported works [ 62–64]. The Levy ﬂight step can strength\nthe algorithm’s capacity, where the Levy ﬂights with tiny\nstep widths can improve the exploitation ability while the\nLevy ﬂights with huge step widths can raise the likelihood\nof avoiding the traps in local or misleading optimums. The\nmathematical deﬁnition of the Levy ﬂight distribution can\nbe expressed as follows [ 64].\nLs ; c; lðÞ ¼\nﬃﬃﬃﬃﬃﬃ\ny\n2p\nr\ne\n/C0 c\n2ðs/C0 lÞ\n/C16/C17\n1\nðs /C0 lÞ3=2\n !\n0\\l\\s\\1\n0 s /C200\n8\n><\n>:\nð31Þ\nwhere s deﬁnes the samples, c represents the scale\nparameter to control the distribution and l deﬁnes trans-\nmission factor. The Levy ﬂight distribution is deﬁned by\nthe following.\nFkðÞ ¼ e\n/C0 a kjj bðÞ ; b 2½ 0; 2/C138ð 32Þ\nwhere b deﬁnes a distribution index and a denotes the\n3146 Neural Computing and Applications (2024) 36:3141–3166\n123\nscaling parameter. The step length, s, is determined by the\nfollowing expression [ 64].\ns ¼ u\nvjj 1=bðÞ\n !\nð33Þ\nu and v are two parameters that have the Gaussian distri-\nbutions as follows:\nuN 0; r2\nu\n/C0/C1\n; vN 0; r2\nv\n/C0/C1\nwhere r2\nu and r2\nv deﬁne the standard deviations that are\ngiven as follows.\nru ¼\nC 1 þ bðÞ sin pb\n2\n/C16/C17\nb:C 1þb\n2\n/C16/C17\n/C12\nðb/C0 1Þ\n2\n8\n<\n:\n9\n=\n;\n1=b\n; rv ¼ 1 ð34Þ\nwhere C :ðÞ denotes the Gramma function.\nIn this step, the LF is used to update the solution\nobtained TDO to intensify the searching around best\nobtained solution so far ( xbest) by the following equation.\nxLF\ni ¼ xi þð 2 /C1rand /C0 1Þ/C1levyðbÞ/C1ðxbest /C0 xiÞð 35Þ\nxi ¼ xLF\ni Fx LF\ni\n/C0/C1\n\\FðxiÞ\nxi otherwise\n/C26\nð36Þ\nwhere xLF\ni denotes the generated new position using the\nLevy ﬂight process, xi is the current solution and rand\nstands for random number from the interval [0, 1]. Equa-\ntion ( 30) says that the current solution is substituted with\nupdated one using Levy ﬂight concept if it is better;\notherwise, current one is continued.\n3.2.2 Spiralize elite learning strategy\nSince the original TDO performs the updating process by\ntaking into account the positions of two randomly gener-\nated solutions which may lead to lose the promising\nregions, so it is vital to emphasize the exploitation ability\nby performing further searches around the best so far\nsolution. In this context, the logarithmic spiral step (LSS) is\nintroduced around the best solution. This step is performed\nsuch that it decreases gradually with the growth of\niteration.\nLSS ¼ Ae\n/C0 b/C1tcosð2cptÞð 37Þ\nFig. 2 Flowchart of TDO\nNeural Computing and Applications (2024) 36:3141–3166 3147\n123\nA ¼ Ui /C0 Li\n2\n/C18/C19\n/C3 T /C0 t\nT\n/C18/C19 2\n; Ui ¼ max XiðÞ ; Li ¼ min XiðÞ\nð38Þ\nxLSS\ni ¼ xbest þ LSS ð39Þ\nwhere xLSS\ni denotes the generated new position using the\nlogarithmic spiral step, A deﬁne shrinking radius, b denotes\na constant that determines the logarithmic spiral shape, the\nt is a parameter that determine how much the next position\nshould be close to the best solution and c is the number of\nspirals. The ﬂowchart of ITDO is shown in Fig. 3.\nFig. 3 Block diagram for the main stages of the problem algorithm\n3148 Neural Computing and Applications (2024) 36:3141–3166\n123\n4 Experiment results and validations\nTo assess the effectiveness of ITDO, two validation anal-\nysis series are utilized, and they can be summed up as\nfollows: The initial numerical veriﬁcation is carried out on\n10 classical benchmark problems and the latest IEEE CEC\n2020 benchmark problems that encompass of unimodal,\nmultimodal, hybrid and composition natures. The details\nregarding the studied functions are presented in Tables 9\nand 10 in Appendix. These tables contain the function\nname, nature of the function, description, number of vari-\nables ðdÞ, range of the variable (RV) and optimum solution\n(F\nopt). A comprehensive comparisons between the pro-\nposed ITDO and some well-known algorithms including\njellyﬁsh (JS) [ 34], Harris hawk optimization (HHO) [ 47],\ngrey wolf optimization (GWO) [ 51], sine–cosine algorithm\n(SCA) [ 48], DE [ 52], PSO [ 54], and original TDO [ 40].\nFurthermore, to demonstrate its purported performance,\nITDO is put up against some other competitors reported\nfrom the literature. The second task aims to gauge the\nperformance of ITDO on the parameter’s estimation of the\nsingle-phase transformer.\nAll experiments are carried out on a Laptop with AMD\nRyzen 5 5600u with Radeon Graphics 2.30 GHz CPU,\n16 GB RAM under 64-bit Windows 10OS. MATLA-\nB_R2020a is used for the implementation of the presented\nalgorithms.\nThe parameter conﬁgurations for the employed algo-\nrithms are suggested as in their corresponding original\nworks and they are listed in Table 1. For fair comparison,\nall parameters are considered the same. To ensure an\nequitable evaluation between the proposed ITDO algorithm\nand the compared ones (namely, DE, PSO, JS, HHO,\nGWO, SCA and the original TDO), they are executed with\nan identical maximum number of iterations for each run.\nMoreover, each algorithm underwent 20 independent runs\nto avoid the haphazard situation. The parameter settings for\nthe implemented optimization methods were adopted from\ntheir respective original sources and are detailed in Table 1.\nNotably, common parameters such as the population size\n(M) for the presented optimizers were determined as 60\nfollowing a series of trials.\n4.1 Simulation results on the basic and CEC 2020\nbenchmark functions\nTo analysis the performance of the ITDO, it is carried out\non 33 benchmark functions which include 23 classical\nbenchmark functions accompanied with unimodal, multi-\nmodal and ﬁxed multimodal, and IEEE CEC2020 that\nincludes 10 benchmark functions of hybrid optimization\nnature. In this regard, the performance of the ITDO is\ninvestigated using some of the well-known optimization\nalgorithms. To diminish the chances of the outcome, the\nconducted algorithms are carried out for 20 independent\nruns with displaying some statistical measures such as the\nbest (Min.), mean, median, worst (Max.) and standard\ndeviation (SD) values of each problem. The results of the\nITDO and the comparative algorithms are presented in\nTable 2, where the best or signiﬁcant results are high-\nlighted with bold font in terms of mean value recorded by\neach algorithm. Moreover, the experimental results are\nillustrated based on the number of occasions at which the\nmethod performs worse than/equal to/better than while\ntaking into consideration the lowest mean values obtained\nfor the benchmark problems. In light of this, it is indicated\nthat the number of occasions satisﬁed by the suggested\nITDO are 0/6/17, 0/4/19, 0/7/16, 2/7/14, 0/4/19, 0/3/20 and\n0/9/14 against the competing optimization algorithms, i.e.,\nDE, PSO, JS, HHO, GWO, SCA and TDO, respectively. In\nthis sense, the HHO is better than ITDO for F\n5 and F8\nwhile all competing optimization algorithms have same\noccasions for some benchmark problems. In summary, the\nITDO has possessed progressive performance in\nTable 1 Parameter’ settings for the implemented algorithms\nAlgorithm Refs Parameters Values\nDE [ 52] Mutation factor; Crossover rate 0.5; 0.5\nPSO [ 54] c1; c1; w 2; 2; Linearly reduced from 2 to 0\nJS [ 34] c; b 0.1; 3\nHHO [ 47] E0; E1 Random number between - 1 and 1; Linearly reduced from 2 to 0\nGWO [ 51] a Linearly reduced from 2 to 0\nSCA [ 48] a Linearly reduced from 2 to 0\nTDO [ 40] R Linearly reduced from 0.01 to 0\nITDO R; b Linearly reduced from 0.01 to 0; 1.5\nAll algorithms M; T 60; 500\nNeural Computing and Applications (2024) 36:3141–3166 3149\n123\nTable 2 Results on the basic benchmark functions\nFun Measures DE PSO JS HHO GWO SCA TDO ITDO\nF1 Min 8.54E ?02 1.40E -13 7.54E -20 4.23E -107 9.36E -14 1.06E -04 1.44E -92 0.00E ?00\nMean 2.10E ?03 2.55E -07 5.97E -18 5.23E -95 1.71E -12 7.45E ?00 1.38E -90 0.00E100\nMedian 1.95E ?03 1.49E -09 4.14E -18 8.18E -101 1.12E -12 2.31E ?00 4.71E -91 0.00E ?00\nMax 5.32E ?03 3.10E -06 2.69E -17 9.01E -94 6.83E -12 4.39E ?01 1.04E -89 0.00E ?00\nSD 9.70E ?02 6.98E -07 6.47E -18 2.01E -94 1.88E -12 1.27E ?01 2.47E -90 0.00E ?00\nF2 Min 1.18E ?01 2.26E -03 1.40E -10 2.41E -58 1.30E -08 4.38E -04 7.14E -48 3.40E -179\nMean 2.74E ?01 1.22E -01 3.59E -10 2.84E -52 5.27E -08 1.19E -02 4.88E -47 5.10E2174\nMedian 2.53E ?01 6.36E -02 2.39E -10 3.92E -54 4.74E -08 5.34E -03 2.75E -47 1.17E -175\nMax 5.71E ?01 5.39E -01 1.60E -09 3.99E -51 1.19E -07 6.39E -02 2.79E -46 8.89E -173\nSD 1.13E ?01 1.39E -01 3.31E -10 9.07E -52 2.69E -08 1.50E -02 6.23E -47 0.00E ?00\nF3 Min 1.54E ?04 1.42E ?01 4.36E -03 6.49E -97 1.29E -01 5.25E ?02 1.47E -30 1.46E -182\nMean 2.77E ?04 5.66E ?01 7.67E -02 3.17E -79 1.49E ?00 8.44E ?03 1.18E -24 1.36E2167\nMedian 2.70E ?04 3.93E ?01 3.15E -02 1.43E -88 9.69E -01 8.10E ?03 4.00E -27 4.93E -172\nMax 4.58E ?04 1.55E ?02 3.85E -01 6.34E -78 6.32E ?00 2.03E ?04 2.29E -23 2.56E -166\nSD 6.63E ?03 3.98E ?01 1.07E -01 1.42E -78 1.68E ?00 5.40E ?03 5.11E -24 0.00E ?00\nF4 Min 2.48E ?01 5.25E -01 1.04E -07 8.44E -59 1.14E -03 1.83E ?01 1.59E -39 2.20E -150\nMean 3.99E ?01 2.27E ?00 2.71E -07 1.93E -46 5.78E -03 3.43E ?01 2.09E -38 3.38E2146\nMedian 3.90E ?01 2.02E ?00 2.26E -07 6.77E -53 4.27E -03 3.46E ?01 8.87E -39 7.10E -148\nMax 6.06E ?01 4.61E ?00 6.55E -07 3.86E -45 1.61E -02 5.35E ?01 9.80E -38 2.37E -145\nSD 7.94E ?00 1.18E ?00 1.46E -07 8.64E -46 4.54E -03 1.14E ?01 2.59E -38 7.11E -146\nF5 Min 7.58E ?04 1.61E ?01 7.05E -02 2.60E -07 2.66E ?01 5.12E ?01 2.52E ?01 2.27E ?01\nMean 7.01E ?05 6.12E ?01 1.70E ?00 1.78E202 2.73E?01 3.75E ?04 2.62E ?01 2.54E ?01\nMedian 5.19E ?05 7.47E ?01 6.03E -01 6.35E -03 2.71E ?01 1.18E ?04 2.61E ?01 2.54E ?01\nMax 2.41E ?06 1.80E ?02 1.16E ?01 1.14E -01 2.85E ?01 3.40E ?05 2.75E ?01 2.66E ?01\nSD 5.95E ?05 4.20E ?01 3.16E ?00 3.05E -02 5.30E -01 8.11E ?04 4.78E -01 8.84E -01\nF6 Min 6.43E ?02 2.00E ?00 1.15E -03 2.67E -07 9.71E -02 4.71E ?00 6.37E -04 9.58E -16\nMean 1.51E ?03 1.07E ?01 2.48E -02 1.12E -04 7.58E -01 1.87E ?01 1.05E -02 5.48E213\nMedian 1.37E ?03 8.50E ?00 1.18E -02 2.34E -05 7.46E -01 1.04E ?01 1.82E -03 1.87E -13\nMax 2.99E ?03 3.00E ?01 2.66E -01 6.95E -04 1.26E ?00 5.02E ?01 1.45E -01 1.83E -12\nSD 6.55E ?02 8.47E ?00 5.73E -02 1.80E -04 2.86E -01 1.51E ?01 3.19E -02 6.20E -13\nF7 Min 1.89E -01 9.61E -03 7.34E -04 7.95E -06 1.19E -03 2.91E -02 1.39E -04 4.66E -06\nMean 6.88E -01 2.56E -02 1.64E -03 1.56E -04 3.84E -03 1.14E -01 5.39E -04 1.19E204\nMedian 5.98E -01 2.66E -02 1.54E -03 1.21E -04 2.62E -03 9.54E -02 3.67E -04 1.20E -04\nMax 1.87E ?00 5.60E -02 2.85E -03 6.59E -04 9.14E -03 4.21E -01 1.27E -03 2.52E -04\nSD 4.15E -01 1.18E -02 6.81E -04 1.55E -04 2.34E -03 9.06E -02 3.34E -04 7.39E -05\nF8 Min - 5.94E?03 - 7.61E?03 - 7.98E?03 - 1.26E?04 - 4.59E?03 - 4.23E?03 - 7.97E?03 - 9.28E?03\nMean - 4.59E?03 - 6.45E?03 - 6.06E?03 - 1.26E104 - 4.27E?03 - 3.74E?03 - 6.11E?03 - 8.05E?03\nMedian - 4.55E?03 - 6.52E?03 - 6.02E?03 - 1.26E?04 - 4.36E?03 - 3.73E?03 - 5.89E?03 - 8.28E?03\nMax - 4.03E?03 - 5.42E?03 - 4.50E?03 - 1.26E?04 - 3.74E?03 - 3.31E?03 - 5.12E?03 - 6.21E?03\nSD 3.93E ?02 5.31E ?02 9.09E ?02 3.02E -01 2.32E ?02 2.56E ?02 7.31E ?02 9.07E ?02\nF9 Min 2.25E ?02 3.78E ?01 4.25E ?00 0.00E ?00 3.66E -09 1.88E -01 0.00E ?00 0.00E ?00\nMean 2.63E ?02 5.93E ?01 1.40E ?01 0.00E100 9.85E?00 3.31E ?01 0.00E100 0.00E 100\nMedian 2.68E ?02 5.27E ?01 1.42E ?01 0.00E ?00 6.71E ?00 3.56E ?01 0.00E ?00 0.00E ?00\nMax 2.90E ?02 9.95E ?01 2.46E ?01 0.00E ?00 3.77E ?01 8.00E ?01 0.00E ?00 0.00E ?00\nSD 2.14E ?01 1.73E ?01 5.61E ?00 0.00E ?00 8.86E ?00 2.71E ?01 0.00E ?00 0.00E ?00\n3150 Neural Computing and Applications (2024) 36:3141–3166\n123\nTable 2 (continued)\nFun Measures DE PSO JS HHO GWO SCA TDO ITDO\nF10 Min 7.66E ?00 5.51E -06 7.79E -11 8.88E -16 2.00E ?01 2.14E -02 4.44E -15 8.88E -16\nMean 9.49E ?00 1.47E ?00 4.49E -10 8.88E216 2.01E?01 1.33E ?01 6.22E -15 8.88E216\nMedian 9.51E ?00 1.34E ?00 3.52E -10 8.88E -16 2.01E ?01 2.02E ?01 6.22E -15 8.88E -16\nMax 1.19E ?01 2.96E ?00 1.60E -09 8.88E -16 2.02E ?01 2.03E ?01 7.99E -15 8.88E -16\nSD 1.25E ?00 7.75E -01 3.64E -10 0.00E ?00 5.17E -02 9.48E ?00 1.82E -15 0.00E ?00\nF11 Min 6.50E ?00 4.60E -11 0.00E ?00 0.00E ?00 3.79E -13 2.43E -01 0.00E ?00 0.00E ?00\nMean 1.48E ?01 2.81E -02 1.55E -16 0.00E100 6.96E-03 1.18E ?00 0.00E100 0.00E 100\nMedian 1.34E ?01 1.48E -02 0.00E ?00 0.00E ?00 9.12E -12 1.06E ?00 0.00E ?00 0.00E ?00\nMax 2.82E ?01 1.30E -01 1.33E -15 0.00E ?00 3.81E -02 5.37E ?00 0.00E ?00 0.00E ?00\nSD 5.86E ?00 3.64E -02 3.57E -16 0.00E ?00 1.28E -02 1.03E ?00 0.00E ?00 0.00E ?00\nF12 Min 3.20E ?01 2.44E -12 5.95E -05 7.18E -09 1.88E -02 3.59E -01 1.19E -05 1.57E -32\nMean 3.04E ?05 1.87E -01 9.76E -04 1.04E -05 4.83E -02 4.20E ?04 4.62E -05 1.61E232\nMedian 7.74E ?04 1.04E -01 3.92E -04 2.41E -06 4.37E -02 1.79E ?01 2.93E -05 1.60E -32\nMax 2.05E ?06 1.04E ?00 7.12E -03 7.20E -05 8.29E -02 7.38E ?05 1.85E -04 1.70E -32\nSD 5.78E ?05 2.64E -01 1.84E -03 1.74E -05 2.00E -02 1.65E ?05 4.27E -05 3.42E -34\nF13 Min 1.28E ?04 2.02E -12 3.19E -04 3.42E -07 2.35E -01 2.51E ?00 2.00E -03 1.47E -32\nMean 1.18E ?06 5.64E -02 3.17E -02 1.88E -04 6.46E -01 2.42E ?04 2.57E -01 1.94E232\nMedian 6.17E ?05 5.62E -03 1.27E -02 6.19E -05 6.44E -01 2.38E ?01 1.64E -01 1.84E -32\nMax 5.50E ?06 4.71E -01 2.97E -01 8.86E -04 1.06E ?00 3.48E ?05 6.26E -01 3.08E -32\nSD 1.38E ?06 1.25E -01 6.69E -02 2.42E -04 2.32E -01 7.88E ?04 2.02E -01 5.10E -33\nF14 Min 9.98E -01 9.98E -01 9.98E -01 9.98E -01 9.98E -01 9.98E -01 9.98E -01 9.98E -01\nMean 1.10E ?00 4.23E ?00 9.98E201 1.20E?00 1.20E ?00 1.89E ?00 9.98E201 9.98E 201\nMedian 9.98E -01 2.98E ?00 9.98E -01 9.98E -01 9.98E -01 1.00E ?00 9.98E -01 9.98E -01\nMax 2.98E ?00 1.27E ?01 9.98E -01 1.99E ?00 2.98E ?00 2.98E ?00 9.98E -01 9.98E -01\nSD 4.44E -01 3.97E ?00 5.09E -17 4.08E -01 6.11E -01 1.01E ?00 0.00E ?00 0.00E ?00\nF15 Min 3.07E -04 3.07E -04 3.07E -04 3.09E -04 3.66E -04 3.60E -04 3.07E -04 3.07E -04\nMean 3.07E204 7.76E-04 3.12E -04 3.39E -04 8.29E -04 9.82E -04 3.10E -04 3.07E204\nMedian 3.07E -04 3.18E -04 3.07E -04 3.25E -04 6.76E -04 8.19E -04 3.07E -04 3.07E -04\nMax 3.07E -04 1.59E -03 4.03E -04 4.24E -04 1.22E -03 1.54E -03 3.63E -04 3.07E -04\nSD 2.01E -19 5.85E -04 2.13E -05 3.35E -05 3.17E -04 4.11E -04 1.24E -05 0.00E ?00\nF16 Min - 1.03E?00 - 1.03E?00 - 1.03E?00 - 1.03E?00 - 1.03E?00 - 1.03E?00 - 1.03E?00 - 1.03E?00\nMean - 1.03E100 - 1.03E100 - 1.03E100 - 1.03E100 - 1.03E100 - 1.03E100 - 1.03E100 - 1.03E100\nMedian - 1.03E?00 - 1.03E?00 - 1.03E?00 - 1.03E?00 - 1.03E?00 - 1.03E?00 - 1.03E?00 - 1.03E?00\nMax - 1.03E?00 - 1.03E?00 - 1.03E?00 - 1.03E?00 - 1.03E?00 - 1.03E?00 - 1.03E?00 - 1.03E?00\nSD 2.16E -16 2.04E -16 2.04E -16 1.03E -09 5.53E -05 6.05E -05 1.44E -16 2.28E -16\nF17 Min 3.98E -01 3.98E -01 3.98E -01 3.98E -01 3.98E -01 3.98E -01 3.98E -01 3.98E -01\nMean 3.98E201 3.98E 201 3.98E 201 3.98E 201 3.98E 201 4.01E-01 3.98E201 3.98E 201\nMedian 3.98E -01 3.98E -01 3.98E -01 3.98E -01 3.98E -01 4.00E -01 3.98E -01 3.98E -01\nMax 3.98E -01 3.98E -01 3.98E -01 3.98E -01 3.99E -01 4.11E -01 3.98E -01 3.98E -01\nSD 0.00E ?00 0.00E ?00 0.00E ?00 2.82E -05 1.98E -04 4.05E -03 0.00E ?00 0.00E ?00\nF18 Min 3.00E ?00 3.00E ?00 3.00E ?00 3.00E ?00 3.00E ?00 3.00E ?00 3.00E ?00 3.00E ?00\nMean 3.00E100 3.00E 100 3.00E 100 3.00E 100 3.00E 100 3.00E 100 3.00E 100 3.00E 100\nMedian 3.00E ?00 3.00E ?00 3.00E ?00 3.00E ?00 3.00E ?00 3.00E ?00 3.00E ?00 3.00E ?00\nMax 3.00E ?00 3.00E ?00 3.00E ?00 3.00E ?00 3.00E ?00 3.00E ?00 3.00E ?00 3.00E ?00\nSD 7.89E -16 6.60E -16 8.94E -16 9.45E -07 5.59E -05 1.41E -04 9.72E -16 9.11E -16\nNeural Computing and Applications (2024) 36:3141–3166 3151\n123\ncomparison with other counterparts for most functions of\nthe benchmark problems, which conﬁrms its ability to deal\nwith function optimization challenges.\nNext, the proposed ITDO and the compared algorithms\nwere evaluated on the CEC2020 benchmark problems, one\nof the recent datasets in the ﬁeld of numerical optimization.\nThis dataset is made up of ten extremely difﬁcult opti-\nmization issues and the characteristics of this dataset fall\ninto four groups, unimodal, multimodal, hybrid and com-\nposite functions. The results of CEC 2020 benchmark\nfunctions are displayed in Table 3. To get the results more\nprecisely and thoroughly, each algorithm is carried out for\n20 independent runs, where the minimum value (described\nas Min.), average (described as mean), median (denoted as\nmedian), maximum (denoted as Max.) and standard devi-\nation of (denoted as SD) are recorded. The signiﬁcant\noptimization results in terms of mean values among the\npresented algorithms for each test function are marked in\nbold font. In this sense, the number of occasions that\nproposed algorithm performs worse than/equal to/better\nthan the other counterparts are adopted to demonstrate the\nobtained results. In light of this, it is revealed that the\nnumber of occasions satisﬁed by the suggested ITDO are 1/\n0/9, 0/4/19, 0/0/10, 0/0/10, 0/0/10, 0/0/10 and 1/0/9 against\nthe competing optimization algorithms, i.e., DE, PSO, JS,\nHHO, GWO, SCA and TDO, respectively. In this sense,\nthe DE and TDO are better than ITDO for F\n10 and F4,\nrespectively, and ITDO is better than the reset of com-\npeting optimization algorithms for all test functions. Based\non the above analysis and results, ITDO has a promising\nwith good comprehensive performance to the other\nadvanced optimizers on most functions. This superior\nperformance lies behind the inclusion of improvement\nstrategies that support a positive inﬂuence on the original\nTDO.\n4.2 Convergence analysis\nConvergence behavior is an essential criterion to assess the\neffectiveness of any optimization technique. In this sense,\nto justify concise information about the algorithm speed in\nﬁnding more ﬁtting results, the convergence rates for the\nimplemented optimizers during the optimization process\nare depicted as in Fig. 4. For the comparison of conver-\ngence behavior among the 33 benchmark issues, 18 are\nchosen as an example, and their convergence curves are\nTable 2 (continued)\nFun Measures DE PSO JS HHO GWO SCA TDO ITDO\nF19 Min - 3.86E?00 - 3.86E?00 - 3.86E?00 - 3.86E?00 - 3.86E?00 - 3.86E?00 - 3.86E?00 - 3.86E?00\nMean - 3.86E100 - 3.86E100 - 3.86E100 - 3.86E100 - 3.86E100 - 3.86E100 - 3.86E100 - 3.86E100\nMedian - 3.86E?00 - 3.86E?00 - 3.86E?00 - 3.86E?00 - 3.86E?00 - 3.85E?00 - 3.86E?00 - 3.86E?00\nMax - 3.86E?00 - 3.86E?00 - 3.86E?00 - 3.85E?00 - 3.85E?00 - 3.85E?00 - 3.86E?00 - 3.86E?00\nSD 2.28E -15 2.28E -15 2.22E -15 3.53E -03 3.79E -03 3.39E -03 1.95E -15 2.28E -15\nF20 Min - 3.32E?00 - 3.32E?00 - 3.32E?00 - 3.28E?00 - 3.20E?00 - 3.15E?00 - 3.32E?00 - 3.32E?00\nMean - 3.27E?00 - 3.25E?00 - 3.32E100 - 3.11E?00 - 3.04E?00 - 3.00E?00 - 3.32E100 - 3.32E100\nMedian - 3.32E?00 - 3.20E?00 - 3.32E?00 - 3.13E?00 - 3.13E?00 - 3.02E?00 - 3.32E?00 - 3.32E?00\nMax - 3.20E?00 - 3.20E?00 - 3.32E?00 - 2.82E?00 - 1.84E?00 - 2.57E?00 - 3.32E?00 - 3.32E?00\nSD 5.98E -02 5.98E -02 7.26E -14 1.11E -01 3.06E -01 1.49E -01 4.20E -16 4.56E -16\nF21 Min - 1.02E?01 - 1.02E?01 - 1.02E?01 - 9.93E?00 - 1.02E?01 - 7.41E?00 - 1.02E?01 - 1.02E?01\nMean - 1.02E101 - 6.53E?00 - 1.01E?01 - 5.96E?00 - 4.71E?00 - 2.21E?00 - 1.02E101 - 1.02E101\nMedian - 1.02E?01 - 7.63E?00 - 1.02E?01 - 5.05E?00 - 5.06E?00 - 1.19E?00 - 1.02E?01 - 1.02E?01\nMax - 1.02E?01 - 2.63E?00 - 9.87E?00 - 5.03E?00 - 4.97E-01 - 4.97E-01 - 1.02E?01 - 1.02E?01\nSD 3.05E -15 3.76E ?00 6.41E -02 1.88E ?00 2.54E ?00 1.97E ?00 1.54E -06 0.00E ?00\nF22 Min - 1.04E?01 - 1.04E?01 - 1.04E?01 - 5.09E?00 - 1.04E?01 - 7.88E?00 - 1.04E?01 - 1.04E?01\nMean - 1.01E?01 - 7.08E?00 - 1.04E101 - 5.08E?00 - 8.27E?00 - 3.76E?00 - 1.01E?01 - 1.04E101\nMedian - 1.04E?01 - 1.04E?01 - 1.04E?01 - 5.09E?00 - 1.04E?01 - 4.58E?00 - 1.04E?01 - 1.04E?01\nMax - 3.72E?00 - 2.75E?00 - 1.04E?01 - 5.08E?00 - 5.09E?00 - 5.22E-01 - 5.09E?00 - 1.04E?01\nSD 1.49E ?00 3.80E ?00 2.50E -07 3.13E -03 2.67E ?00 2.25E ?00 1.19E ?00 0.00E ?00\nF23 Min - 1.05E?01 - 1.05E?01 - 1.05E?01 - 1.05E?01 - 1.05E?01 - 6.24E?00 - 1.05E?01 - 1.05E?01\nMean - 1.02E?01 - 7.50E?00 - 1.02E?01 - 5.39E?00 - 9.28E?00 - 3.95E?00 - 1.03E?01 - 1.05E101\nMedian - 1.05E?01 - 1.05E?01 - 1.05E?01 - 5.13E?00 - 1.05E?01 - 4.70E?00 - 1.05E?01 - 1.05E?01\nMax - 3.84E?00 - 2.42E?00 - 3.84E?00 - 5.12E?00 - 3.28E?00 - 5.54E-01 - 5.13E?00 - 1.05E?01\nSD 1.50E ?00 3.86E ?00 1.50E ?00 1.19E ?00 2.59E ?00 1.70E ?00 1.21E ?00 1.82E -15\n3152 Neural Computing and Applications (2024) 36:3141–3166\n123\nTable 3 Results on CEC 2020 functions\nFunction Measures DE PSO JS HHO GWO SCA TDO Proposed\nCEC20-F1 Min 4.23E -02 3.89E ?01 3.13E ?00 6.04E ?04 7.13E ?03 1.53E ?08 1.09E ?00 0.00E ?00\nMean 9.80E -02 1.91E ?03 5.33E ?02 1.40E ?05 7.67E ?04 4.49E ?08 5.99E ?02 0.00E100\nMedian 8.38E -02 1.12E ?03 5.22E ?02 1.30E ?05 2.01E ?04 4.53E ?08 6.10E ?01 0.00E ?00\nMax 1.69E -01 5.32E ?03 1.55E ?03 2.85E ?05 5.64E ?05 7.53E ?08 2.13E ?03 0.00E ?00\nSD 4.43E -02 1.91E ?03 5.34E ?02 7.62E ?04 1.72E ?05 1.93E ?08 8.42E ?02 0.00E ?00\nCEC20-F2 Min 7.48E ?02 1.30E ?02 1.51E ?01 4.34E ?02 6.53E ?01 9.39E ?02 2.46E ?02 1.02E ?01\nMean 1.09E ?03 4.36E ?02 3.75E ?02 7.65E ?02 5.95E ?02 1.15E ?03 3.60E ?02 3.33E102\nMedian 1.09E ?03 3.94E ?02 3.55E ?02 7.58E ?02 3.86E ?02 1.11E ?03 3.60E ?02 3.09E ?02\nMax 1.31E ?03 9.54E ?02 7.36E ?02 1.32E ?03 1.39E ?03 1.43E ?03 4.54E ?02 9.62E ?02\nSD 1.67E ?02 2.74E ?02 2.60E ?02 2.62E ?02 4.71E ?02 1.60E ?02 7.03E ?01 2.63E ?02\nCEC20-F3 Min 4.08E ?01 1.42E ?01 1.63E ?01 4.85E ?01 1.45E ?01 5.71E ?01 1.57E ?01 1.40E ?01\nMean 4.63E ?01 2.14E ?01 2.55E ?01 7.44E ?01 2.59E ?01 7.17E ?01 1.90E ?01 1.88E101\nMedian 4.72E ?01 2.11E ?01 2.51E ?01 6.90E ?01 2.32E ?01 6.87E ?01 1.80E ?01 1.92E ?01\nMax 4.95E ?01 3.34E ?01 3.62E ?01 9.95E ?01 4.45E ?01 9.22E ?01 2.61E ?01 2.15E ?01\nSD 3.13E ?00 6.51E ?00 7.03E ?00 1.82E ?01 1.01E ?01 9.89E ?00 3.15E ?00 2.70E ?00\nCEC20-F4 Min 1.89E ?00 5.59E -01 5.52E -01 2.68E ?00 8.51E -01 9.02E ?00 9.60E202 3.72E-01\nMean 2.73E ?00 1.06E ?00 7.99E -01 4.91E ?00 2.02E ?00 1.21E ?01 5.49E -01 8.99E -01\nMedian 2.85E ?00 1.18E ?00 7.70E -01 5.02E ?00 1.97E ?00 1.27E ?01 5.92E -01 7.91E -01\nMax 3.16E ?00 1.48E ?00 1.14E ?00 7.42E ?00 3.50E ?00 1.39E ?01 1.07E ?00 1.91E ?00\nSD 4.01E -01 3.55E -01 1.87E -01 1.79E ?00 8.00E -01 1.77E ?00 2.72E -01 4.72E -01\nCEC20-F5 Min 9.92E ?00 1.89E ?02 2.09E ?02 1.11E ?03 1.57E ?03 5.85E ?03 4.87E ?02 0.00E ?00\nMean 2.62E ?01 8.41E ?02 6.34E ?02 1.57E ?04 9.65E ?03 1.59E ?04 7.67E ?02 3.12E201\nMedian 2.65E ?01 8.10E ?02 5.59E ?02 8.20E ?03 8.87E ?03 8.78E ?03 7.25E ?02 4.16E -01\nMax 4.03E ?01 1.48E ?03 1.60E ?03 7.51E ?04 1.68E ?04 4.55E ?04 1.12E ?03 6.24E -01\nSD 1.01E ?01 4.34E ?02 4.12E ?02 2.22E ?04 5.79E ?03 1.30E ?04 2.49E ?02 2.02E -01\nCEC20-F6 Min 4.90E -01 8.02E -01 3.06E -01 6.41E -01 6.21E -01 7.61E -01 3.08E -01 3.05E -01\nMean 4.90E -01 1.09E ?01 3.16E -01 9.14E ?00 7.35E -01 9.50E -01 3.72E -01 3.05E201\nMedian 4.90E -01 1.72E ?01 3.09E -01 9.25E ?00 6.33E -01 9.02E -01 3.10E -01 3.05E -01\nMax 4.90E -01 1.81E ?01 3.41E -01 1.79E ?01 1.13E ?00 1.27E ?00 6.19E -01 3.05E -01\nSD 0.00E ?00 8.67E ?00 1.24E -02 8.59E ?00 1.65E -01 1.87E -01 1.30E -01 0.00E ?00\nCEC20-F7 Min 5.09E -01 7.55E ?01 2.71E ?01 4.18E ?02 2.56E ?02 1.96E ?03 4.36E ?01 9.66E -07\nMean 1.11E ?00 2.69E ?02 7.94E ?01 7.67E ?03 4.00E ?03 6.89E ?03 9.71E ?01 9.41E202\nMedian 9.84E -01 2.58E ?02 7.27E ?01 5.68E ?03 2.88E ?03 6.25E ?03 8.31E ?01 5.97E -04\nMax 1.99E ?00 5.23E ?02 1.48E ?02 2.56E ?04 1.32E ?04 1.29E ?04 1.64E ?02 3.13E -01\nSD 5.17E -01 1.26E ?02 3.69E ?01 7.50E ?03 4.11E ?03 3.88E ?03 3.87E ?01 1.51E -01\nCEC20-F8 Min 1.05E ?02 1.01E ?02 9.00E ?01 1.07E ?02 2.06E ?01 7.82E ?01 1.00E ?02 0.00E ?00\nMean 1.05E ?02 1.01E ?02 9.96E ?01 1.13E ?02 9.60E ?01 1.44E ?02 1.00E ?02 9.03E101\nMedian 1.05E ?02 1.01E ?02 1.01E ?02 1.12E ?02 1.03E ?02 1.46E ?02 1.00E ?02 1.00E ?02\nMax 1.07E ?02 1.03E ?02 1.01E ?02 1.23E ?02 1.08E ?02 1.87E ?02 1.01E ?02 1.01E ?02\nSD 6.37E -01 7.82E -01 3.38E ?00 4.41E ?00 2.66E ?01 2.84E ?01 2.53E -01 3.17E ?01\nCEC20-F9 Min 1.00E ?02 3.40E ?02 1.00E ?02 1.01E ?02 3.33E ?02 1.53E ?02 1.00E ?02 1.00E ?02\nMean 3.07E ?02 3.50E ?02 2.97E ?02 3.83E ?02 3.46E ?02 3.35E ?02 2.48E ?02 1.00E102\nMedian 3.58E ?02 3.49E ?02 3.39E ?02 4.06E ?02 3.42E ?02 3.80E ?02 3.23E ?02 1.00E ?02\nMax 3.64E ?02 3.65E ?02 3.45E ?02 4.83E ?02 3.66E ?02 3.87E ?02 3.31E ?02 1.00E ?02\nSD 1.09E ?02 7.55E ?00 8.43E ?01 1.06E ?02 1.22E ?01 9.46E ?01 1.06E ?02 0.00E ?00\nNeural Computing and Applications (2024) 36:3141–3166 3153\n123\ngiven in Fig. 4. As seen from the depicted curves, ITDO\noffers the best performance for most of the studied func-\ntions. ITDO exhibits a quicker convergence rate than the\nother ones and tries to converge to the optimum value with\na signiﬁcantly higher convergence rate. Furthermore, it is\nseen that ITDO virtually always comes in the ﬁrst place in\nall assessments when compared with other methods. Taken\ntogether, the ITDO can signiﬁcantly boost search effec-\ntiveness, solution accuracy and increase the speed of con-\nvergence. These characteristics afﬁrm that the ITDO\npossesses a strong global search ability, results of newly\nembedded strategies, Levy ﬂight and spiralize learning\nstrategies. These strategies also try to strike an appropriate\nbalance between exploration and exploitation. The\ndescription above demonstrates that ITDO efﬁciently\nsolves optimization issues with an impressive convergence\nrate.\n4.3 Box plot analysis\nA box plot displays another essential graphic criterion that\nshows the distribution of each algorithm’s best results over\nthe course of independent runs. In this regard, to justify\nconcise information about the stability of the algorithm in\nﬁnding more reliable results, box plot analysis is repre-\nsented for the compared algorithms as provided in Fig. 5.\nIn each ﬁgure, the box’s length (interquartile range)\ndetermines the variance, where the shorter length denotes a\nsmaller deviation between the ﬁndings achieved during\nindependent runs. Additionally, when the plot is lower, the\nresult is close to the ideal ﬁtness, which is a good sign. The\nmedian of the ﬁndings across the independent runs is\nrepresented by the red line inside the box. Therefore, the\ndepicted box plot reveals that the proposed ITDO has a\ncompact and very consistent distribution with the global\noptimum value as it provides a narrower interquartile range\nfor most of the studied benchmark suits. This simulation\nadvocates the supremacy of the ITDO against most of the\nother counterparts.\n4.4 Performance assessment by a pairwise\nWilcoxon test\nTo analyze the achieved outcomes statistically, a non-\nparametric statistical hypothesis test, named Wilcoxon’s\nrank-sum test [ 65], is used to verify whether the two\ncompared algorithms differ signiﬁcantly from one another.\nHere, the null hypothesis is set up as follows. The two\ncompared algorithms do not signiﬁcantly differ from one\nanother. When p is less than 0.05, it implies that the null\nhypothesis can be rejected with a level of signiﬁcance of\n5%. In this sense, Wilcoxon’s test is applied on the mean\nvalues of the benchmark functions achieved by the com-\npared algorithms. The results obtained by the Wilcoxon’s\ntest are recorded in Table 4 in terms of the sum of positive\nrank ðR\nþÞ; opposite rank ( R/C0 ) and p value. As illustrated in\nTable 4, ITDO outperforms all the compared algorithms as\nit provides a signiﬁcance level less than 5% for most cases,\nwhich conﬁrms the superiority of ITDO over the other\ncounterparts.\nDespite the fact that the competitive and progressive\nresults have demonstrated the practicality and efﬁcacy of\nthe offered optimization approach for classical benchmark\nproblems and complex CEC 2020 problems and parameter\nidentiﬁcation of electric transformer model, there are still\nsome perspectives that are worth addressing in future\nstudies. Firstly, the ITDO can be redesigned as a fusion\nstrategy for dealing with more challenging optimization\nproblems such as many-objective-constrained optimization\nwith including some constraint handling insights, and bi-\nlevel optimization for estimating the contingencies in\npower transmission systems to exert its capability thor-\noughly. Secondly, extending the ITDO to deal with clean\nhydrogen production using optimized PV panel as an\ninteresting perspective for sustainable energy. Thirdly,\nfurther investigation into the search performance of the\nITDO is still warranted, and continuous experimentation on\nhow to implement this proposal in various application sit-\nuations is essential. It’s essential to acknowledge that the\nITDO cannot be considered the ultimate universal opti-\nmization tool, as no optimization framework can claim this\nTable 3 (continued)\nFunction Measures DE PSO JS HHO GWO SCA TDO Proposed\nCEC20-F10 Min 3.98E ?02 3.98E ?02 3.98E ?02 3.99E ?02 3.98E ?02 4.22E ?02 4.00E ?02 3.98E ?02\nMean 3.98E102 4.21E?02 4.28E ?02 4.44E ?02 4.38E ?02 4.44E ?02 4.20E ?02 3.98E102\nMedian 3.98E ?02 4.22E ?02 4.46E ?02 4.46E ?02 4.46E ?02 4.46E ?02 4.16E ?02 3.98E ?02\nMax 3.98E ?02 4.44E ?02 4.52E ?02 5.54E ?02 4.48E ?02 4.61E ?02 4.43E ?02 3.98E ?02\nSD 3.46E -01 2.38E ?01 2.49E ?01 4.45E ?01 1.64E ?01 1.37E ?01 2.09E ?01 1.49E -01\n3154 Neural Computing and Applications (2024) 36:3141–3166\n123\nFig. 4 Convergence curves for the basic functions and CEC 2020 using the proposed ITDO and the compared ones\nNeural Computing and Applications (2024) 36:3141–3166 3155\n123\nFig. 5 Box plot analysis for the basic functions and CEC 2020 using the proposed ITDO and the compared ones\n3156 Neural Computing and Applications (2024) 36:3141–3166\n123\nstatus, as indicated by the NFL theorem. Furthermore,\nsince the ITDO is crafted based on the principles of\nstochastic rule-based optimization, it might still grapple\nwith challenges like stagnation or premature convergence\nwhen dealing with more complex and demanding datasets.\nFinally, we aspire for this study to serve as an inspiration\nfor fellow researchers interested in exploring applications-\noriented optimization methods. This work holds promise\nfor future endeavors in the realm of real-world system\napplications.\n4.5 Parameter estimation on the single-phase\ntransformer\nThe effectiveness and efﬁciency of the proposed ITDO is\nconﬁrmed veriﬁed by estimating the parameters of single-\nphase transformers with the following characteristics:\n1 kVA, 230/230 V, 50 Hz. The testing of short circuit,\nopen circuit, DC and load are performed to determine the\nequivalent circuit actual parameters.\nTable 5 displays the recorded readings for the single-\nphase transformer under no load and during short-circuit\ntests. In this case, open- and short-circuit experimental tests\nwere used to gather the transformer’s actual data. The\nopen-circuit test is conducted at nominal voltage, and the\ncore resistance and magnetizing reactance, denoted by\n/C20R\nm and Xm, respectively, are determined by measuring\nthe measured current and power. Additionally, the primary\nand secondary resistances as well as the leakage reactances,\ndenoted by R\n1; X1, and R2; X2 are determined using short-\ncircuit test measurements (current, voltage and power).\nToward the parameter’s estimation issue, the proposed\nITDO algorithms as well as the compared algorithms are\nconducting to optimally obtain the equivalent circuit\nparameters. The achieved results by the competitive opti-\nmizers and the actual values are offered in Table 6. The\ncalculated parameters are utilized to compute the\nTable 4 Wilcoxon test at 5% signiﬁcant level on results of Tables 2\nand 3\nCompared methods Solution evaluations\nProposed Competitors p value R- R? Winner\nITDO DE 1.20E -05 0 325 ITDO\nITDO PSO 6.00E -06 0 378 ITDO\nITDO JS 8.67E -04 28.5 247.5 ITDO\nITDO HHO 3.71E -03 32 199 ITDO\nITDO GWO 6.00E -06 0 378 ITDO\nITDO SCA 4.00E -06 0 406 ITDO\nITDO TDO 7.24E -04 11 179 ITDO\nTable 5 Experimental evaluations of (case study) single-phase\ntransformer\nVariables No load test SC test DC test\nCase study Case study Case study\nVoltage (V) 228 20.8 –\nCurrent (A) 0.05 0.18 –\nP (W) 3.28 2.58 –\nR\n1 (X) – – 52.5\nR2 (X) – – 27.12\nTable 6 Extracted parameters\nusing the ITDO and compared\nalgorithms for the single-phase\ntransformer\nAlgorithm R\n1 (X) X1 (X) R2 (X) X2 (X) Rm (X) Xm (X)\nActual [ 22] 52.50 41.85 27.12 41.85 15,849 47,589\nJS 50.93 40.69 29.19 45.51 15,906.80 47,251.05\nHHO 53.83 42.69 25.92 42.56 15,862.86 47,357.46\nGWO 55.00 39.63 24.64 40.66 15,847.36 47,396.40\nSCA 51.85 43.38 28.11 46.78 15,886.98 47,577.51\nTDO 52.84 41.78 27.03 43.28 15,876.60 47,337.24\nITDO 54.23 40.78 25.44 40.10 15,847.03 47,526.10\nTable 7 Statistical measures of\nITDO and compared algorithms\nfor the single-phase transformer\nMeasures JS HHO GWO SCA TDO Proposed\nMin 5.3274E -18 1.0857E -12 5.8917E -10 6.5992E -09 1.2006E -15 0.0000E ?00\nMean 2.2795E -14 1.6385E -08 1.2087E -08 7.7218E -08 5.8922E -13 4.4315E -28\nMedian 1.3874E -15 6.6136E -09 4.5226E -09 7.2552E -08 4.1141E -13 0.0000E ?00\nMax 2.1387E -13 8.2539E -08 5.7901E -08 2.3124E -07 2.5926E -12 1.2729E -26\nSD 5.6127E -14 2.0751E -08 1.6205E -08 4.4548E -08 6.7605E -13 2.3224E -27\nNeural Computing and Applications (2024) 36:3141–3166 3157\n123\ntransformer current, voltage regulation, power factor and\nefﬁciency under full-load conditions.\nMoreover, the statistical results regarding the estimated\nparameters using the proposed algorithm and the compared\nones are demonstrated in Table 7. The acquired ﬁndings\ndemonstrate that the proposed ITDO produces the most\naccurate operational results. In this sense, the best mean\nvalues of the OF achieved are 4.4315E -28, 2.2795E -14,\n5.8922E-13, 1.2087E -08, 1.6385E -08 and 7.7218E -08,\nfor ITDO, JS, TDO, GWO, HHO and SCA, respectively.\nAlso, the convergence assessment is presented to offer the\nperformance of the algorithm during the iterative process.\nIn this sense, the convergence behaviors of the imple-\nmented algorithms are depicted as in Fig. 6a. It is observed\nthat ITDO converges quicker than the other counterparts\nand shows higher error improvement than the others.\nTo further validate the accurateness of the ﬁndings, the\nbest extracted parameters of ITDO are presented for\nreconstructing the transforming characteristics at different\nloading percentage, voltage regulation, power factor and\nefﬁciency, as shown in Fig. 5a. The estimated results\nacquired by ITDO strongly match with the actual data,\nwhich emphasizes the accuracy of the estimated parame-\nters. Moreover, to verify the robustness of the ITDO, 20\nindependent runs are carried out, and the mean of the OF is\noffered over the course of runs. Based on the exhibited\nbehaviors, the ITDO acquires the highest stability than the\nothers. As shown in Fig. 6b, while dealing with the single-\nphase transformer models, the suggested ITDO delivers\nexceptionally thin width for the inter quartile range as\ncompared to the basic and advanced state of the art. Then,\nthe box plots that are portrayed support the superiority of\nITDO’s over other algorithms.\nFigure 7 compares the performance of a single-phase\ntransformer for estimated parameters to the performance of\nthe actual value. At varied loading percentages to the real\nperformance, high similarity between estimated input\npower factor and efﬁciency is observed using JS; follow,\nITDO; follow TDO and ﬁnally HHO. The voltage regula-\ntion computed using ITDO, on the other hand, is the closest\nto the real value. The suggested ITDO’s estimated voltage\nregulation and efﬁciency are extremely near to the real\nvalues.\n4.6 Computational complexity\nAssessing an algorithm’s computational complexity is an\nessential factor when evaluating its runtime performance. It\noffers insights into the behavior of the optimization algo-\nrithm and aids in identifying appropriate parameters and\nresources for effective execution. In this context, the focus\nof this study is to analyze the ITDO algorithm’s computing\ncomplexity. The complexity of ITDO is impacted by dif-\nferent factors. The size of the population is one of these\nfactors; a larger population necessitates more time for\nassessing and updating search agents. Another factor is the\ntotal number of decision variables; more decision variables\nresult in a larger problem space and increase computational\nefforts. The maximum number of iterations has an impact\non computation time because more iterations may lead to\nbetter responses but also take longer to get done. Further-\nmore, the complexity of the algorithm can be affected by\nthe nature of the objective function (i.e., unimodal, multi-\nmodal, hybrid and compositional natures) and the mecha-\nnisms involved in the updating process. The goal of this\nwork is to develop new optimization incorporating differ-\nent updating mechanisms, which is an unconventional\nvariant. In this regard, compromising on different updating\nmechanisms can demand more time than conventional ones\nthat rely on a single updating scheme. Table 8 reports the\ncomputation time of the ITDO and the compared ones.\nFrom the obtained results, it can be noted that the ITDO\nprovides less time for some of the competing algorithms as\nwell as being slightly higher for some of the studied\nbenchmark suites. However, the proposed optimization\nmethod surpasses the other competing methods in terms of\nproducing superior outcomes for most of the studied\nproblems. The goal is to achieve a balance between com-\nputation time and achieving the best outcomes.\n4.7 Advantages and limitations of the suggested\nalgorithm\n4.7.1 Advantages of the algorithm\nImproved exploration: The algorithm embeds the Levy\nﬂight concept-based searching scheme into the TDO to\ndetect new regions comprising the promising zones\n3158 Neural Computing and Applications (2024) 36:3141–3166\n123\n(a) Objective function (SRRE) (b) Computitive algorithm box plot\nFig. 6 Box plot and convergence curve analysis by competitively suggested and traditional optimizers\n(a) Efficiency%, (b) Power factor\n(c) Voltage regulation\nFig. 7 Performance of characteristic of 230/230 V, 1 kVA, 50 Hz single-phase transformer\nNeural Computing and Applications (2024) 36:3141–3166 3159\n123\nduring the iterative process, enhancing the exploration\ntendency along the entire search space.\nGuidance-based exploitation: The utilization of the\nspiralize elite learning strategy in the TDO assists in\nguiding the search toward better vicinities, leading to\nemphasize the intensiﬁcation inclination while enhanc-\ning the accuracy of the search.\nRapid convergence: The ITDO algorithm can screen out\npromising solutions faster than conventional optimiza-\ntion methods due to its capacity to explore different\ntrends in the search region more effectively while\npreventing it from getting stuck in local optima.\nRobustness: The algorithm demonstrates superior per-\nformance compared to its counterparts on most of the\nTable 8 Comparative analysis\nof average computational time\n(in s)\nSuite Fun DE PSO JS HHO GWO SCA TDO Proposed ITDO\nBasic benchmark suite F1 0.53 0.37 0.03 0.05 0.09 0.07 0.04 0.10\nF2 0.54 0.40 0.03 0.05 0.10 0.07 0.04 0.11\nF3 0.62 0.49 0.15 0.32 0.20 0.18 0.21 0.13\nF4 0.60 0.41 0.03 0.07 0.10 0.08 0.04 0.12\nF5 0.64 0.45 0.05 0.12 0.12 0.09 0.06 0.12\nF6 0.57 0.37 0.03 0.09 0.10 0.08 0.04 0.11\nF7 0.64 0.47 0.11 0.23 0.17 0.14 0.14 0.12\nF8 0.58 0.43 0.11 0.12 0.11 0.09 0.06 0.11\nF9 0.53 0.39 0.05 0.09 0.10 0.08 0.04 0.10\nF10 0.56 0.40 0.04 0.09 0.11 0.08 0.04 0.10\nF11 0.61 0.43 0.05 0.12 0.12 0.10 0.06 0.11\nF12 0.78 0.62 0.23 0.52 0.29 0.26 0.32 0.16\nF13 0.78 0.61 0.23 0.53 0.28 0.26 0.32 0.16\nF14 0.99 0.80 0.40 1.00 0.39 0.39 0.59 0.15\nF15 0.58 0.37 0.03 0.05 0.03 0.02 0.03 0.04\nF16 0.53 0.35 0.03 0.06 0.02 0.02 0.03 0.04\nF17 0.52 0.33 0.04 0.04 0.02 0.01 0.02 0.03\nF18 0.51 0.34 0.02 0.04 0.02 0.01 0.02 0.03\nF19 0.56 0.36 0.03 0.06 0.03 0.03 0.04 0.04\nF20 0.58 0.36 0.04 0.07 0.04 0.03 0.04 0.05\nF21 0.60 0.38 0.04 0.08 0.04 0.03 0.05 0.05\nF22 0.58 0.39 0.05 0.10 0.04 0.04 0.06 0.05\nF23 0.66 0.43 0.06 0.12 0.05 0.05 0.07 0.05\nCEC 2020 test suite FCEC2020-1 0.64 0.93 0.58 1.32 0.74 0.63 0.81 0.95\nFCEC2020-2 0.73 1.02 0.98 1.85 0.97 0.87 1.16 1.21\nFCEC2020-3 0.66 0.95 0.69 1.63 0.87 0.78 1.00 1.00\nFCEC2020-4 0.67 0.96 0.71 1.64 0.88 0.78 1.00 1.03\nFCEC2020-5 0.71 1.00 0.90 1.85 0.97 0.87 1.14 1.11\nFCEC2020-6 0.78 1.07 1.20 1.63 0.77 0.68 0.87 1.01\nFCEC2020-7 0.70 0.99 0.86 1.68 0.89 0.79 1.03 1.04\nFCEC2020-8 0.77 1.06 1.14 2.77 1.36 1.26 1.65 1.24\nFCEC2020-9 0.82 1.11 1.34 3.05 1.48 1.38 1.90 1.28\nFCEC2020-10 0.75 1.04 1.04 2.46 1.21 1.11 1.50 1.14\n3160 Neural Computing and Applications (2024) 36:3141–3166\n123\nstudied problems by effectively balancing its exploitative\nand explorative characteristics\n4.7.2 Limitations of the algorithm\nLimited applicability: The suggested algorithm may be\nsusceptible to worsening while tackling more compli-\ncated problems, such as the parametrized CEC 2021\nbenchmark problems, which consist of 8 variants of\ntransformations such as rotation, bias and shift. More\nspeciﬁcally, since ITDO is in its infancy with simple\nimprovement strategies, degrading the exploration and\nconvergence abilities of the algorithm can occur. So, the\nsuggested algorithm can be further enhanced with other\nmutation strategies to deliver a more potent and reliable\nperformance for solving complicated optimization tasks.\nLimited savings: The suggested algorithm demands\nslightly more execution time than some of the compared\nalgorithms for some of the studied problems due to the\nembedded efforts of the improvement strategies during\nthe exploration and exploitation phases. So, the utiliza-\ntion of termination conditions with speciﬁed tolerance\nrather than performing the course of all iterations or\nusing the number of function evaluations can be useful\nfor saving computational efforts.\n5 Conclusion\nThis paper presents an improved TDO based on Levy ﬂight\nand spiralize learning strategies, termed ITDO, for esti-\nmating the equivalent circuit optimal parameters for the\nsingle-phase transformer. The ITDO embeds the Levy\nﬂight strategy to enrich the exploration scope within the\nsearch space while spiralize learning strategy is integrated\nto promote the vicinity searches around the promising\nareas. By these strategies, it attempts to enhance the orig-\ninal TDO’s capability to strike a balance between diversi-\nﬁcation and identiﬁcation abilities with higher processing\nefﬁciency. The effectiveness of the ITDO algorithm is\nexamined through validating its performance on set of\ntwenty-three standard functions and CEC 2020 suit of ten\nfunctions accompanied with different landscape natures.\nThe comprehensive comparisons with other well-known\noptimization methods have proved the competitive per-\nformance of the proposed ITDO. Furthermore, the\nevaluations based on the statistical metrics, box plot anal-\nysis, convergence graphs and nonparametric Wilcoxon’s\nrank-sum test have conﬁrmed the ITDO outperforms other\nalgorithms in terms of reliability and accuracy of search\noperations. The results achieved by the Wilcoxon signed\nrank-sum test afﬁrm that the ITDO exhibits amazing per-\nformance over its competitors as it achieves small p values\n(\\0:05), displaying a signiﬁcant difference and promising\nperformance. Afterward, the applicability of the ITDO is\nafﬁrmed by estimating the parameters of the 1 kVA,\n230/230 V, single-phase transformer. An extensive exper-\nimental result demonstrates that ITDO can provide com-\npetitive performance in terms of accuracy and reliability\nfor estimating the transformer model parameters when\ncompared with other well-established optimizers. The\nITDO realizes steady, smooth and rapid convergence than\nthe other counterparts. According to estimated parameters,\nITDO highly provides coincidence characteristics with the\nactual data over the loading range, which afﬁrms its ability\nand stability in estimating the parameters of single-phase\ntransformer model. ITDO has a number of advantages that\ncan be presented as follows. ITDO is characterized by its\neasy implementation and powerful exploitation and\nexploration abilities. Although ITDO is slightly slower\nthan some of its competitors, it afﬁrms the best results in\nmost functions when compared to the others. Therefore, the\nfollowing suggestions could be taken into consideration by\nresearchers in the future. The integration/hybridization\nwith a number of stochastic operators and chaos theory can\naccelerate the convergence performance of ITDO. ITDO\ncan be applied to address constrained optimization prob-\nlems, multi-/many-objective optimization tasks, real-world\napplications. Also, ITDO can be applied for several\napplications in power systems engineering such as the\noptimal power ﬂow, reactive power compensation and\ncombined AC/DC grids. Additionally, the integration of\nthe ITDO with the long short-term memory (LSTM) and\nneural network models can be addressed to improve the\naccuracy of forecasting applications. Finally, we hope that\nthe proposed work will serve as a source of inspiration for\nother researchers who are interested in solving real-world\napplication-based optimization schemes.\nAppendix\nSee Tables 9 and 10.\nNeural Computing and Applications (2024) 36:3141–3166 3161\n123\nTable 9 Basic benchmark test functions\nFunction name Mathematical form RV d F opt\nUnimodal benchmark functions\nSphere F1 ¼ Pn\ni¼1\nx2\ni\n[- 100,100] 30 0\nSchwefel 2.22 F2 ¼ Pn\ni¼1\njxijþ Qn\ni¼1\njxij [- 10,10] 30 0\nSchwefel 1.2\nF3 ¼ Pn\ni¼1\nPi\nj/C0 1\nxj\n ! 2 [- 100,100] 30 0\nSchwefel 2.21 F4 ¼ maxifjxij; 1 /C20i /C20ng [- 100,100] 30 0\nRosenbrock F5 ¼ Pn/C0 1\ni¼1\n½100ðxiþ1 /C0 x2\ni Þ2 þð xi /C0 1Þ2/C138 [- 30,30] 30 0\nStep F6 ¼ Pn\ni¼1\nð½xi þ 0:5/C138Þ2 [- 100,100] 30 0\nQuartic F7 ¼ Pn\ni¼1\nix4\ni þ random½0; 1Þ [- 1.28,1.28] 30 0\nMultimodal benchmark functions\nSchwefel F8 ¼ Pn\ni¼1\n/C0 xi sinð\nﬃﬃﬃﬃﬃﬃ\njxij\np\nÞ [- 500,500] 30 - 418.9829*5\nRastrigin F9 ¼ Pn\ni¼1\n½x2\ni /C0 10 cosð2pxiÞþ 10/C138 [- 5.12, 5.12] 30 0\nAckley\nF10 ¼/C0 20 exp /C0 0:2\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\n1\nn\nPn\ni¼1\nx2\ni\ns !\n/C0 exp 1\nn\nPn\ni¼1\ncosð2pxiÞ\n/C18/C19\nþ 20 þ e\n[- 32,32] 30 0\nGriewank F11 ¼ 1\n4000\nPn\ni¼1\nx2\ni /C0 Qn\n1¼1\ncos xiﬃ\ni\np\n/C16/C17\nþ 1 [- 600,600] 30 0\nPenalized F12 ¼ p\nn 10 sinðpyiÞþ Pn/C0 1\ni¼1\nðyi /C0 1Þ2½1 þ 10 sin2ðpyiþ1Þ/C138 þ ðyn /C0 1Þ2\n/C26/C27\nþ Pn\ni¼1\nuðxi; 10; 100; 4Þ\nyi ¼ 1 þ xi þ 1\n4 uðxi; a; k; mÞ¼\nkðxi /C0 aÞmxi [ a\n0 /C0 a\\xi\\a\nkð/C0 xi /C0 aÞmxi\\ /C0 a\n8\n<\n:\n[- 50,50] 30 0\nPenalize 2\nF13 ¼ 0:1 sin 2ð3px1Þþ\nXn\ni¼1\nðxi /C0 1Þ2½1 þ sin2ð3pxi þ 1Þ/C138\n(\nþðxn /C0 1Þ2½1 þ sin2ð2pxnÞ/C138\no\nþ\nXn\ni¼1\nuðxi; 5; 100; 4Þ\n[- 50,50] 30 0\n3162 Neural Computing and Applications (2024) 36:3141–3166\n123\nTable 9 (continued)\nFunction name Mathematical form RV d F opt\nMultimodal benchmark functions with ﬁxed dimension\nFoxholes\nF14 ¼ 1\n500 þ P25\nj¼1\n1\njþ\nP2\ni¼1ðxi/C0 aijÞ6\n ! /C0 1 [- 65,65] 2 1\nKowalik F15 ¼ P11\ni¼1\nai /C0 x1ðb2\ni þbix2 Þ\nðb2\ni þbix3þx4Þ\nhi 2 [- 5,5] 4 0.00030\nSix-hump camelback F16 ¼ 4x2\n1 /C0 2:1x4\n1 þ 1\n3 x6\n1 þ x1x2 /C0 4x2\n2 þ 4x4\n2 [- 5,5] 2 - 1.0316\nBranin F17 ¼ x2 /C0 5:1\n4p2 x2\n1 þ 5\np x1 /C0 6\n/C0/C1 2\nþ10 1 /C0 1\n8p\n/C0/C1\ncos x1 þ 10 [- 5,5] 2 0.398\nGoldstein–Price F18 ¼½ 1 þð x1 þ x2 þ 1Þ2ð19 /C0 14x1 þ 3x2\n1 /C0 14x2 þ 6x1x2 þ 3x2\n2Þ/C138\nþ½ 30 þð 2x1 /C0 3x2Þ2ð18 /C0 32x1 þ 12x2\n1 þ 48x2 /C0 36x1x2 þ 27x2\n2Þ/C138\n[- 2,2] 2 3\nHartman 3\nF19 ¼/C0 P4\ni¼1\nci exp /C0 P3\nj¼1\naijðxj /C0 pijÞ2\n ! [1,3] 3 - 3.86\nHartman 6\nF20 ¼/C0 P4\ni¼1\nci exp /C0 P6\nj¼1\naijðxj /C0 pijÞ2\n ! [0, 1] 6 - 3.32\nShekel5 F21 ¼/C0 P5\ni¼1\n½ðX /C0 aiÞðX /C0 aiÞT þ ci/C138/C0 1 [0,10] 4 - 10.1532\nShekel7 F22 ¼/C0 P7\ni¼1\n½ðX /C0 aiÞðX /C0 aiÞT þ ci/C138/C0 1 [0,10] 4 - 10.4028\nShekel10 F23 ¼/C0 P10\ni¼1\n½ðX /C0 aiÞðX /C0 aiÞT þ ci/C138/C0 1 [0,10] 4 - 10.5363\nNeural Computing and Applications (2024) 36:3141–3166 3163\n123\nFunding Open access funding provided by The Science, Technology\n& Innovation Funding Authority (STDF) in cooperation with The\nEgyptian Knowledge Bank (EKB).\nData availability Data sharing is not applicable to this article as no\ndatasets were generated or analyzed during the current study.\nDeclarations\nConflict of interest The authors have no conflicts of interest among\nthe authors.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indicate\nif changes were made. The images or other third party material in this\narticle are included in the article’s Creative Commons licence, unless\nindicated otherwise in a credit line to the material. If material is not\nincluded in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright\nholder. To view a copy of this licence, visit http://creativecommons.\norg/licenses/by/4.0/.\nReferences\n1. Rana MJ, Shahriar MS, Shaﬁullah M (2019) Levenberg–Mar-\nquardt neural network to estimate UPFC-coordinated PSS\nparameters to enhance power system stability. Neural Comput\nAppl 31:1237–1248. https://doi.org/10.1007/s00521-017-3156-8\n2. Abdelwanis MI, Rashad EM, Taha IBM, Selim FF (2021)\nImplementation and control of six-phase induction motor driven\nby a three-phase supply. Energies 14:1–16\n3. Suja K, Yuvaraj T (2021) transformer health monitoring system\nusing android device. In: 2021 7th international conference on\nelectrical energy systems (ICEES). IEEE, pp 460–462\n4. H. D Mehta R. P, (2014) A review on transformer design opti-\nmization and performance analysis using artiﬁcial intelligence\ntechniques. Int J Sci Res 3:726–733\n5. Aghmasheh R, Rashtchi V, Rahimpour E (2018) Gray box\nmodeling of power transformer windings based on design\ngeometry and particle swarm optimization algorithm. IEEE Trans\nPower Deliv 33:2384–2393. https://doi.org/10.1109/TPWRD.\n2018.2808518\n6. Shintemirov A, Tang WH, Wu QH (2010) Transformer core\nparameter identiﬁcation using frequency response analysis. IEEE\nTrans Magn 46:141–149. https://doi.org/10.1109/TMAG.2009.\n2026423\n7. Mitchell SD, Welsh JS (2011) Modeling Power transformers to\nsupport the interpretation of frequency-response analysis. IEEE\nTrans Power Deliv 26:2705–2717. https://doi.org/10.1109/\nTPWRD.2011.2164424\n8. Keyhani A, Miri SM, Hao S (1986) Parameter estimation for\npower transformer models from time-domain data. IEEE Trans\nPower Deliv 1:140–146. https://doi.org/10.1109/TPWRD.1986.\n4307985\n9. Dirik H, Gezegin C, Ozdemir M (2014) A novel parameter\nidentiﬁcation method for single-phase transformers by using real-\ntime data. IEEE Trans Power Deliv 29:1074–1082. https://doi.\norg/10.1109/TPWRD.2013.2284243\n10. Martinez JA, Walling R, Mork BA et al (2005) Parameter\ndetermination for modeling system transients—Part III: trans-\nformers. IEEE Trans Power Deliv 20:2051–2062. https://doi.org/\n10.1109/TPWRD.2005.848752\n11. Vicol B (2014) On-line overhead transmission line And trans-\nformer parameters identiﬁcation based on PMU measurements.\nIn: 2014 international conference and exposition on electrical and\npower engineering (EPE). IEEE, pp 1045–1050\n12. Bogarra S, Font A, Candela I, Pedra J (2009) Parameter esti-\nmation of a transformer with saturation using inrush measure-\nments. Electr Power Syst Res 79:417–425. https://doi.org/10.\n1016/j.epsr.2008.08.009\n13. Bhowmick D, Manna M, Chowdhury SK (2018) Estimation of\nequivalent circuit parameters of transformer and induction motor\nfrom load data. IEEE Trans Ind Appl 5:2784–2791. https://doi.\norg/10.1109/TIA.2018.2790378\n14. El-Sehiemy RA, Abd-Elwanis MI, Kotb AB EM (2010) Syn-\nchronous motor design using particle swarm optimization tech-\nnique. In: Proceedings of the 14th international middle east power\nsystems conference (MEPCON’10), Cairo University, Egypt,\npp 795–800\n15. El-Sehiemy RA, Hamida MA, Mesbahi T (2020) Parameter\nidentiﬁcation and state-of-charge estimation for lithium-polymer\nbattery cells using enhanced sunﬂower optimization algorithm.\nTable 10 CEC 2020 benchmark functions\nNature Name Description RV d F opt\nUnimodal nature CEC20- F1 Shifted and Rotated Bent Cigar [ - 100,100] 10 100\nMultimodal nature CEC20- F2 Shifted and Rotated Schwefel’s [ - 100,100] 10 1100\nCEC20-F3 Shifted and Rotated Lunacekbi-Rastrigin [ - 100,100] 10 700\nCEC20-F4 Expanded Rosen brock’s plus Grie wangk’s [ - 100,100] 10 1900\nHybrid nature CEC20- F5 HybridFunction1 ( N =3 ) [ - 100,100] 10 1700\nCEC20-F6 HybridFunction2 ( N =4 ) [ - 100,100] 10 1600\nCEC20-F7 HybridFunction3 ( N =5 ) [ - 100,100] 10 2100\nComposition nature CEC20- F8 CompositionFunction1 ( N =3 ) [ - 100,100] 10 2200\nCEC20-F9 CompositionFunction2 ( N =4 ) [ - 100,100] 10 2400\nCEC20-F10 CompositionFunction3 ( N =5 ) [ - 100,100] 10 2500\n3164 Neural Computing and Applications (2024) 36:3141–3166\n123\nInt J Hydrogen Energy 45:8833–8842. https://doi.org/10.1016/J.\nIJHYDENE.2020.01.067\n16. Shaﬁk MB, Chen H, Rashed GI et al (2019) Adequate topology\nfor efﬁcient energy resources utilization of active distribution\nnetworks equipped with soft open points. IEEE Access\n7:99003–99016. https://doi.org/10.1109/ACCESS.2019.2930631\n17. Shaheen AM, El-Sehiemy RA, Farrag SM (2018) Adequate\nplanning of shunt power capacitors involving transformer\ncapacity release beneﬁt. IEEE Syst J 12:373–382. https://doi.org/\n10.1109/JSYST.2015.2491966\n18. Bentouati B, Chettih S, El Sehiemy R, Wang G-G (2017) Ele-\nphant herding optimization for solving non-convex optimal\npower ﬂow problem. J Electr Electron Eng 10:31–36\n19. Chenouard R, El-Sehiemy RA (2020) An interval branch and\nbound global optimization algorithm for parameter estimation of\nthree photovoltaic models. Energy Convers Manag. https://doi.\norg/10.1016/j.enconman.2019.112400\n20. Mohamed MA, ZakiDiab AA, Rezk H (2019) Partial shading\nmitigation of PV systems via different meta-heuristic techniques.\nRenew Energy 130:1159–1175. https://doi.org/10.1016/j.renene.\n2018.08.077\n21. Abdalla O, Rezk H, Ahmed EM (2019) Wind driven optimization\nalgorithm based global MPPT for PV system under non-uniform\nsolar irradiance. Sol Energy 180:429–444. https://doi.org/10.\n1016/j.solener.2019.01.056\n22. Abdelwanis MI, Abaza A, El-Sehiemy RA et al (2020) Parameter\nestimation of electric power transformers using coyote opti-\nmization algorithm with experimental veriﬁcation. IEEE Access\n8:50036–50044. https://doi.org/10.1109/ACCESS.2020.2978398\n23. Abdelwanis MI, Sehiemy RA, Hamida MA (2021) Hybrid opti-\nmization algorithm for parameter estimation of poly-phase\ninduction motors with experimental veriﬁcation. Energy AI\n5:100083. https://doi.org/10.1016/j.egyai.2021.100083\n24. Thilagar SH, Rao GS (2002) Parameter estimation of three-\nwinding transformers using genetic algorithm. Eng Appl Artif\nIntell 15:429–437. https://doi.org/10.1016/S0952-\n1976(02)00087-8\n25. Mossad MI, Azab M, Abu-Siada A (2014) Transformer param-\neters estimation from nameplate data using evolutionary pro-\ngramming techniques. IEEE Trans Power Deliv 29:2118–2123.\nhttps://doi.org/10.1109/TPWRD.2014.2311153\n26. Yu K, Liang JJ, Qu BY et al (2017) Parameters identiﬁcation of\nphotovoltaic models using an improved JAYA optimization\nalgorithm. Energy Convers Manag 150:742–753. https://doi.org/\n10.1016/j.enconman.2017.08.063\n27. Du D-C, Vinh H-H, Trung V-D et al (2018) Efﬁciency of Jaya\nalgorithm for solving the optimization-based structural damage\nidentiﬁcation problem based on a hybrid objective function. Eng\nOptim 50:1233–1251. https://doi.org/10.1080/0305215X.2017.\n1367392\n28. C´alasan M, Mujic ˇic´ D, Rubezˇic´ V, Radulovic´ M (2019) Esti-\nmation of equivalent circuit parameters of single-phase trans-\nformer by using chaotic optimization approach. Energies\n12:1697. https://doi.org/10.3390/en12091697\n29. Illias HA, Mou KJ, Bakar AHA (2017) Estimation of transformer\nparameters from nameplate data by imperialist competitive and\ngravitational search algorithms. Swarm Evol Comput 36:18–26.\nhttps://doi.org/10.1016/j.swevo.2017.03.003\n30. Subramanian S, Padma S (2011) Bacterial foraging algorithm\nbased parameter estimation of three WINDING transformer.\nEnergy Power Eng 03:135–143. https://doi.org/10.4236/epe.2011.\n32017\n31. Yilmaz Z, Oksar M, Basciftci F (2017) Multi-objective artiﬁcial\nbee colony algorithm to estimate transformer equivalent circuit\nparameters. Period Eng Nat Sci. https://doi.org/10.21533/pen.\nv5i3.103\n32. Wang Y, Shangguan Y, Yuan J (2018) Determination approach\nfor the parameters of equivalent circuit model of deep saturated\nthree-phase integrative transformers. IEEE Trans Magn 54:1–5.\nhttps://doi.org/10.1109/TMAG.2018.2848234\n33. Kazemi R, Jazebi S, Deswal D, de Leon F (2017) Estimation of\ndesign parameters of single-phase distribution transformers from\nterminal measurements. IEEE Trans Power Deliv 32:2031–2039.\nhttps://doi.org/10.1109/TPWRD.2016.2621753\n34. Youssef H, Hassan MH, Kamel S, Elsayed SK (2021) Parameter\nestimation of single phase transformer using jellyﬁsh search\noptimizer algorithm. In: 2021 IEEE international conference on\nautomation/24th Congress of the Chilean Association of Auto-\nmatic Control, ICA-ACCA 2021\n35. Prakht V, Dmitrievskii V, Kazakbaev V et al (2019) Optimal\ndesign of a novel three-phase high-speed ﬂux reversal machine.\nAppl Sci 9:3822. https://doi.org/10.3390/app9183822\n36. Pierezan J, Dos Santos Coelho L (2018) Coyote optimization\nalgorithm: a new metaheuristic for global optimization problems.\nIn: 2018 IEEE Congress on evolutionary computation, CEC\n2018—proceedings\n37. Abou El-Ela AA, El-Sehiemy RA, Shaheen AM, Diab AE-G\n(2021) Enhanced coyote optimizer-based cascaded load fre-\nquency controllers in multi-area power systems with renewable.\nNeural Comput Appl 33:8459–8477\n38. Abaza A, El Sehiemy RA, Bayoumi ASA (2020) Optimal\nparameter estimation of solid oxide fuel cell model using coyote\noptimization algorithm. In: Farouk MH, Hassanein MA (eds)\nRecent advances in engineering mathematics and physics.\nSpringer, Cham\n39. Qais MH, Hasanien HM, Alghuwainem S, Nouh AS (2019)\nCoyote optimization algorithm for parameters extraction of three-\ndiode photovoltaic models of photovoltaic modules. Energy.\nhttps://doi.org/10.1016/j.energy.2019.116001\n40. Dehghani M, Hubalovsky S, Trojovsky P (2022) Tasmanian\nDevil Optimization: a new bio-inspired optimization algorithm\nfor solving optimization algorithm. IEEE Access\n10:19599–19620. https://doi.org/10.1109/ACCESS.2022.\n3151641\n41. Rout TM, Baker CM, Huxtable S, Wintle BA (2018) Monitoring,\nimperfect detection, and risk optimization of a Tasmanian devil\ninsurance population. Conserv Biol 32:267–275. https://doi.org/\n10.1111/cobi.12975\n42. Nguyen TT, Pham TD, Kien LC, Van Dai L (2020) Improved\ncoyote optimization algorithm for optimally installing solar\nphotovoltaic distribution generation units in radial distribution\npower systems. Complexity 2020:1–34. https://doi.org/10.1155/\n2020/1603802\n43. Kaveh A, Hosseini SM, Zaerreza A (2021) Improved Shufﬂed\nJaya algorithm for sizing optimization of skeletal structures with\ndiscrete variables. Structures 29:107–128. https://doi.org/10.\n1016/j.istruc.2020.11.008\n44. Wolpert DH, Macready WG (1997) No free lunch theorems for\noptimization. IEEE Trans Evol Comput 1:67–82. https://doi.org/\n10.1109/4235.585893\n45. Cauteruccio F, Terracina G, Ursino D (2020) Generalizing\nidentity-based string comparison metrics: framework and tech-\nniques. Knowl Based Syst 187:104820. https://doi.org/10.1016/j.\nknosys.2019.06.028\n46. El-Ela AAA, Bishr M, Allam S, El-Sehiemy R (2005) Optimal\npreventive control actions using multi-objective fuzzy linear\nprogramming technique. Electr Power Syst Res 74:147–155.\nhttps://doi.org/10.1016/j.epsr.2004.08.014\n47. Gharehchopogh FS, Abdollahzadeh B (2022) An efﬁcient Harris\nHawk optimization algorithm for solving the travelling salesman\nproblem. Clust Comput 25:1981–2005. https://doi.org/10.1007/\ns10586-021-03304-5\nNeural Computing and Applications (2024) 36:3141–3166 3165\n123\n48. Zhang L, Hu T, Yang Z et al (2021) Elite and dynamic opposite\nlearning enhanced sine cosine algorithm for application to plat-ﬁn\nheat exchangers design problem. Neural Comput Appl. https://\ndoi.org/10.1007/s00521-021-05963-2\n49. Gupta T, Kumar R (2023) A novel feed-through Elman neural\nnetwork for predicting the compressive and ﬂexural strengths of\neco-friendly jarosite mixed concrete: design, simulation and a\ncomparative study. Soft Comput. https://doi.org/10.1007/s00500-\n023-08195-9\n50. Zhao G, Wang H, Jia D, Wang Q (2019) Feature selection of grey\nwolf optimizer based on quantum computing and uncertain\nsymmetry rough set. Symmetry (Basel) 11:1470. https://doi.org/\n10.3390/sym11121470\n51. Uzlu E (2021) Estimates of greenhouse gas emission in Turkey\nwith grey wolf optimizer algorithm-optimized artiﬁcial neural\nnetworks. Neural Comput Appl 33:13567–13585. https://doi.org/\n10.1007/s00521-021-05980-1\n52. Biswas PP, Suganthan PN, Mallipeddi R, Amaratunga GAJ\n(2018) Optimal power ﬂow solutions using differential evolution\nalgorithm integrated with effective constraint handling tech-\nniques. Eng Appl Artif Intell 68:81–100\n53. Shaheen AM, El-Sehiemy RA, Farrag SM (2016) A novel ade-\nquate bi-level reactive power planning strategy. Int J Electr\nPower Energy Syst. https://doi.org/10.1016/j.ijepes.2015.12.004\n54. Rizk-Allah RM, Hassanien AE (2021) Locomotion-based hybrid\nsalp swarm algorithm for parameter estimation of fuzzy repre-\nsentation-based photovoltaic modules. J Mod Power Syst Clean\nEnergy 9:384–394. https://doi.org/10.35833/MPCE.2019.000028\n55. Kumar R (2023) Double internal loop higher-order recurrent\nneural network-based adaptive control of the nonlinear dynamical\nsystem. Soft Comput 27:17313–17331. https://doi.org/10.1007/\ns00500-023-08061-8\n56. Kumar R, Srivastava S, Gupta JRP (2017) Diagonal recurrent\nneural network based adaptive control of nonlinear dynamical\nsystems using Lyapunov stability criterion. ISA Trans\n67:407–427. https://doi.org/10.1016/j.isatra.2017.01.022\n57. Kumar R, Srivastava S, Gupta JRP (2017) Modeling and adaptive\ncontrol of nonlinear dynamical systems using radial basis func-\ntion network. Soft Comput 21:4447–4463. https://doi.org/10.\n1007/s00500-016-2447-9\n58. Kumar R, Srivastava S, Gupta JRP (2017) Lyapunov stability-\nbased control and identiﬁcation of nonlinear dynamical systems\nusing adaptive dynamic programming. Soft Comput\n21:4465–4480. https://doi.org/10.1007/s00500-017-2500-3\n59. Kumar R, Srivastava S, Gupta JR, Mohindru A (2018) Self-re-\ncurrent wavelet neural network–based identiﬁcation and adaptive\npredictive control of nonlinear dynamical systems. Int J Adapt\nControl Signal Process 32:1326–1358. https://doi.org/10.1002/\nacs.2916\n60. Kumar R, Srivastava S (2020) Externally recurrent neural net-\nwork based identiﬁcation of dynamic systems using Lyapunov\nstability analysis. ISA Trans 98:292–308. https://doi.org/10.1016/\nj.isatra.2019.08.032\n61. Ekinci S, Izci D, Abu Zitar R et al (2022) Development of Le ´vy\nﬂight-based reptile search algorithm with local search ability for\npower systems engineering design problems. Neural Comput\nAppl 34:20263–20283. https://doi.org/10.1007/s00521-022-\n07575-w\n62. Abdulwahab HA, Noraziah A, Alsewari AA, Salih SQ (2019) An\nenhanced version of black hole algorithm via Levy ﬂight for\noptimization and data clustering problems. IEEE Access\n7:142085–142096. https://doi.org/10.1109/ACCESS.2019.\n2937021\n63. Zhang Y, Jin Z, Zhao X, Yang Q (2020) Backtracking search\nalgorithm with Le ´vy ﬂight for estimating parameters of photo-\nvoltaic models. Energy Convers Manag 208:112615. https://doi.\norg/10.1016/j.enconman.2020.112615\n64. Mohseni S, Brent AC, Burmester D, Browne WN (2021) Le ´vy-\nﬂight moth-ﬂame optimisation algorithm-based micro-grid\nequipment sizing: an integrated investment and operational\nplanning approach. Energy AI 3:100047. https://doi.org/10.1016/\nj.egyai.2021.100047\n65. Garcı´a S, Molina D, Lozano M, Herrera F (2009) A study on the\nuse of non-parametric tests for analyzing the evolutionary algo-\nrithms’ behaviour: a case study on the CEC’2005 special session\non real parameter optimization. J Heuristics 15:617–644. https://\ndoi.org/10.1007/s10732-008-9080-4\nPublisher’s Note Springer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\n3166 Neural Computing and Applications (2024) 36:3141–3166\n123",
  "topic": "Benchmark (surveying)",
  "concepts": [
    {
      "name": "Benchmark (surveying)",
      "score": 0.6818963885307312
    },
    {
      "name": "Computer science",
      "score": 0.6410899758338928
    },
    {
      "name": "Economic shortage",
      "score": 0.5933465361595154
    },
    {
      "name": "Optimization algorithm",
      "score": 0.567535936832428
    },
    {
      "name": "Transformer",
      "score": 0.5134478807449341
    },
    {
      "name": "Algorithm",
      "score": 0.47805386781692505
    },
    {
      "name": "Benchmarking",
      "score": 0.4716338813304901
    },
    {
      "name": "Diversification (marketing strategy)",
      "score": 0.44833287596702576
    },
    {
      "name": "Mathematical optimization",
      "score": 0.44642093777656555
    },
    {
      "name": "Mathematics",
      "score": 0.19285547733306885
    },
    {
      "name": "Engineering",
      "score": 0.13157805800437927
    },
    {
      "name": "Voltage",
      "score": 0.12933030724525452
    },
    {
      "name": "Economics",
      "score": 0.08469539880752563
    },
    {
      "name": "Business",
      "score": 0.07254251837730408
    },
    {
      "name": "Marketing",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Government (linguistics)",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ]
}