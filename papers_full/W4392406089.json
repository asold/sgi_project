{
  "title": "Feature-Based Text Search Engine Mitigating Data Diversity Problem Using Pre-Trained Large Language Model for Fast Deployment Services",
  "url": "https://openalex.org/W4392406089",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A1993472872",
      "name": "Yongwoo Jeong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2154746175",
      "name": "Jiseon Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099556365",
      "name": "In Ho Choi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096257435",
      "name": "Ju-Yeon Lee",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2577597748",
    "https://openalex.org/W2150665743",
    "https://openalex.org/W2088381487",
    "https://openalex.org/W2320638839",
    "https://openalex.org/W6773642575",
    "https://openalex.org/W6767904763",
    "https://openalex.org/W2296073425",
    "https://openalex.org/W2037866349",
    "https://openalex.org/W2889326796",
    "https://openalex.org/W2971296908",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2043909051",
    "https://openalex.org/W2574239693",
    "https://openalex.org/W2112006025",
    "https://openalex.org/W4210931461",
    "https://openalex.org/W3115479046",
    "https://openalex.org/W2147152072",
    "https://openalex.org/W1981257123",
    "https://openalex.org/W2107310198",
    "https://openalex.org/W1967082914",
    "https://openalex.org/W2055527244",
    "https://openalex.org/W2163969215",
    "https://openalex.org/W2739853341",
    "https://openalex.org/W3046206337",
    "https://openalex.org/W2087082746",
    "https://openalex.org/W2087006489",
    "https://openalex.org/W2906920259",
    "https://openalex.org/W2111298664",
    "https://openalex.org/W6682132143",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W2147717514",
    "https://openalex.org/W3201221410",
    "https://openalex.org/W4288064589",
    "https://openalex.org/W6767111925",
    "https://openalex.org/W6757904846",
    "https://openalex.org/W4224057314",
    "https://openalex.org/W2125559251",
    "https://openalex.org/W1965555277",
    "https://openalex.org/W6637131181",
    "https://openalex.org/W1987971958",
    "https://openalex.org/W2051224630",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W4287867774",
    "https://openalex.org/W2971292190"
  ],
  "abstract": "The fairness &#x0026; bias of narrow coverage of AI becomes another challenge for AI researchers. If a commercial AI trains with a biased dataset, there will be severe gender or racial fairness and bias issues. Since the researchers use primary language datasets to train AI, the broad audience cannot be satisfied if a novel LLM (Large Language Model) AI shows a knowledge or creativity limitation on their specific spoken language. Narrow coverage of the LLMs can lead the audience to misinterpretation and confusion if the service involves STT (Speech-To-Text). In this paper, to overcome this issue of data diversity, we propose the idea that the embedded, extracted features have captured semantic proximity information that can be useful to mitigate diversity issues. This project focused on the Korean language food dataset for STT services, where a narrow-trained A.I. is prone to show its limitations, such as lifestyle-related elements. To present our proof of concept, we trained a baseline model, GPT2, with the Korean Wikipedia dataset in 2022. Then, we employed DistilBERT and KoBERT for comparison. The extracted hidden&#x005F;state&#x005F;output features from each model were utilized to build feature-extraction-based text search engines. We used the same idea of Local Sensitive Hashing (LSH) but effectively located a similar hash by applying transposed weights. We also present conventional classification benchmarks for performance comparison using top-k measurements, times for training and memory &#x0026; disc consumptions. In the discussion, we proposed that our idea can mitigate the diversity problem without re-training the model and tokenizer.",
  "full_text": " \nVOLUME XX, 2017  \n 1 \nDate of publication xxxx 00, 0000, date of current version xxxx 00, 0000. Digital Object Identifier 10.1109/ACCESS.2022.Doi Number \nFeature Based Text Search Engine Mitigating Data Diversity Problem Using Pre-Trained Large Language Model for Fast Deployment Services YONGWOO JEONG1, (Member, IEEE), JISEON YANG1, IN HO CHOI1, AND JUYEON LEE1 1Rowan, Inc., Seoul, 04527, Republic of Korea  Corresponding author: Yongwoo Jeong (e-mail: oscar.eaglewatch@gmail.com or eaglewatch@rowan.kr). This work was supported by Rowan, Inc.  \nABSTRACT The fairness & bias of narrow coverage of AI becomes another challenge for AI researchers. If a commercial AI trains with a biased dataset, there will be severe gender or racial fairness and bias issues. [43,44]. Since the researchers use primary language datasets to train AI, the broad audience cannot be satisfied if a novel LLM (Large Language Model) AI shows a knowledge or creativity limitation on their specific spoken language. Narrow coverage of the LLMs can lead the audience to misinterpretation and confusion if the service involves STT (Speech-To-Text). In this paper, to overcome this issue of data diversity, we propose the idea that the embedded, extracted features have captured semantic proximity information that can be useful to mitigate diversity issues. This project focused on the Korean language food dataset for STT services, where a narrow-trained A.I. is prone to show its limitations, such as lifestyle-related elements. To present our proof of concept, we trained a baseline model, GPT2, with the Korean Wikipedia dataset in 2022. Then, we employed DistilBERT and KoBERT for comparison. The extracted hidden_state_output features from each model were utilized to build feature-extraction-based text search engines. We used the same idea of Local Sensitive Hashing (LSH) but effectively located a similar hash by applying transposed weights [38]. We also present conventional classification benchmarks for performance comparison using top-k measurements, times for training and memory & disc consumptions. In the discussion, we proposed that our idea can mitigate the diversity problem without re-training the model and tokenizer. INDEX TERMS Data diversity, DistilBERT, embedded, GPT2, KoBERT, Korean, Language, Local Sensitive Hashing, proximity, semantic, Wikipedia \nI. INTRODUCTION As the importance of managing health through nutrition grows, studies have indicated that caloric restriction can lengthen lifespan and delay age-related diseases in various organisms, without causing malnutrition [1]. The quantity and source of protein, predominantly plant-based proteins, can also impact longevity [2]. Additionally, certain foods like red wine, peanuts, and berries contain Resveratrol, a compound known to activate Sirtuins ‚Äì a family of signaling proteins that aid in metabolic regulation and are associated with longevity [3]. It is crucial to note that cultural factors can heavily influence dietary habits. There are also geographic places called Blue Zones; The residents \nof Blue Zones, where people live long lives, tend to focus on plant-based foods, moderate caloric intake, and limited processed foods [4]. Compared to those good diet habits, as there is a rising need for a health management system in Korean food culture, we have extensively researched the Korean speech-to-text (STT) data inquiry-based Korean food name classification because most Korean food names are predominantly composed of tense Korean consonants. The problem is that tensed consonants are often misspelled after the speech-to-text process. To accurately determine the nutrient information of a person's diet, it is vital to know what they ate. If the STT services employ a narrow-trained Large Language Model (LLM) that has never been trained \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3373470\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                                                                                                                                                                                                                         2 VOLUME XX, 2017 \nwith the Korean tense consonant dataset, the STT services won't be able to deliver the correct information.  In natural language projects, if an LLM suffers from narrow domain dataset issues - a lack of diversity, there are known solutions to mitigate this problem: fine-tuning [5], data augmentation [10], domain adaptation [6], curriculum learning [7] and active learning [8]. Also, it is recommended to use data augmentations for sparse source data when a project focuses on a specific language and a pre-trained LLM also shows a narrow coverage. There are many effective augmentation methods available. However, one must consider that there might be a data imbalance issue if a specific language is only used for the project. If there is a shortage of data, sentence augmentation is more straightforward than word augmentation. This can be done by replacing words with synonyms, using similar sentences, back-translation, swapping words randomly, inserting words, or using a combination of these methods [9,10]. There are also different methods for word augmentation: synonym replacement, morphological variations, spelling variations, character-level augmentation, word embeddings, phonetic replacement, back-translation, and expanding contractions. However, the task of this paper is based on STT for the Korean food names; we chose spelling variations and character-level augmentation. Some words were hand-made to mimic the spelling variations. This data shortage issue can be significant if the task aims to classify words rather than sentences. Even with the help of data augmentation, the collected dataset still suffered from a lack of data. The total number of classes of the words is 775, while the number of train datasets is 2,337, and the test is just 584. This lack of a dataset will lead to a significant performance drop if one can engage in a word classification project without having pre-trained LLMs. However, if the pre-trained LLMs have no proficiency in the target language, there will be a significant performance issue. The tokenizers of LLMs would produce UNK (unknown) tokens for specific inputs, which negatively affects the classification accuracy. The original author of the BERT once hinted about the embedding space that their model BERT can capture complex semantic relationships and proximity in the learned embeddings [12]. We assume that this proximity relationship in the embedding space is essential not only for understanding the meaning of the whole sentence but also for mitigating narrow-trained AI if we expand the meaning of embedding space from a sentence understanding to the embedded, extracted weights of AI. Therefore, we assume that the proximity semantic relationship in the embedding, hidden_state_output, extracted from an LLM, is the critical factor in mitigating narrow-trained AI issues because underlying unseen data points with similar characteristics in proximity can be mapped and utilized. Therefore, in this study, we suggest a solution to the diversity issue of a pre-trained LLM model by using an extracted feature-\nmatching-based approach. To prove this idea, we built feature-matching-based text search engines for specific language word classification tasks. Since the original BERT and its distillation model DistilBERT have trained with 104 Wikipedia articles, including Korean languages, but have never been trained with tense Korean consonants, we assume that the word classification tasks using the Korean food dataset with the DistilBERT are excellent examples of demonstrating a combination of a narrow-trained LLM and possible misspelling incidents in STT service. In addition, we conducted a comparison between our approach and traditional pre-trained classification models. The codes and the Korean food dataset are available at GitHub ‚Äúoscar7eaglewatch/aging-project‚Äù repository.  II. RELATED WORK A. GPT2, DistilBERT and KoBERT In this research, we chose three significant, well-known models for comparison: GPT2, DistilBERT, and KoBERT.  The GPT2, which stands for Generative Pre-trained Transformer 2, is a large language model developed and introduced by OpenAI in 2018 [11]. The basic Transformer architecture consists of both an encoder and decoder structure. However, GPT2 solely utilizes a decoder for its main generative tasks; GPT2 was designed to predict the next word in a sequence like autoregressive language modelling, rendering an encoder ineffective in the design concept. GPT2 has many variants; the largest has about 1.5 billion parameters. The smallest version has about 125M, and there are other versions with 355M and 774M parameters. The version used in this study is the smallest model with 125M parameters, which is referred to as ‚Äúgpt2‚Äù in HuggingFace, is still twice bigger than the DistilBERT exported with around 66M parameters. Bidirectional Encoder Representations from Transformer (BERT) [12], which Google developed in 2018, is a pre-trained language model based on Transformer [13] that is designed to deal with natural language tasks. Unlike its predecessors, BERT was just not trained to predict the following words from left to right; it was designed to understand the full context both from the left and the right. There are various models like BERT-Base, BERT-Large, and multilingual BERT.  We utilized DistilBERT in this study, which is 40% smaller and 60% faster due to the knowledge distillation [14]. Even though DistilBERT has 6 layers of Transformer blocks while its teacher model, the BERT base, has 12 layers, DistilBERT is known to retain almost its original performance. About the internal structure, unlike GPT2, the BERT model only utilizes the encoder part of the Transformer architecture, making it well-suited for context-understanding tasks. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3373470\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                                                                          Jeong et al: Feature Based Text Search Engine Mitigating Data Diversity Problem \nVOLUME XX, 2017  \n 3 \nKoBERT is another BERT variant model made by SKT-Brain from South Korea [15]. It is based on the BERT-Base model that has 110M parameters. We selected this model as a performance reference because of the following reasons. They claimed that the model was trained using 5M sentences and 54M words from Korean Wikipedia. The Korean vocabulary size is 8,002, and when it was tested with Naver Sentiment Movie Corpus v1.0 dataset [16], KoBERT recorded the best accuracy of 0.901. Even though the specific year of the Wikipedia article they used for training is not mentioned yet in their documentation, we believe that the Korean tense consonant words were well covered in their dataset. We also proved that KoBERT fully understands our Korean food dataset with the Korean tense consonant words in the benchmark. Since KoBERT has 15M fewer parameters than GPT2 on a similar dataset training dataset, also with a structure advantage as shown in Table 1, we expect that KoBERT will outperform our custom-trained, baseline model GPT2 in this study.  Since their benchmark scores are at the top of the chart, the underlying internal proximity relationship within the embedding space of the KoBERT is already well-constructed for the Korean language dataset. However, we still wanted to conduct the same experiment on KoBERT by applying the feature-extraction method, which is covered in the method section, to find whether there is any adverse effect. \nThe utilization of two pre-trained models for word classification was implemented. Specifically, because we applied the Korean dataset for this study, 'distilbert-base-multilingual-cased' was utilized for DistilBERT, while our custom pre-trained model, referred to as \"eaglewatch/gpt2-ko-wikipedia\" on HuggingFace, was opted for GPT2. Since KoBERT is a variant of the original BERT, we employed the original BertModel and the load pre-trained model was 'skt/kobert-base-v1' from HuggingFace. The tokenizer we utilized for KoBERT is the KoBERTTokenizer, as required in their documentation. For the DistilBERT, our method incorporated DistilBERTModel and built a custom classifier for the word classification job. Besides the pre-trained head models, the main structure of the classifier remained identical for all DistilBERT, GPT2, and KoBERT. The main difference between the two models, DistilBERT and GPT2, is that GPT2 generates the Korean Wikipedia words from GPT2LMHeadModel with 100,000 embeddings, while DistilBERT does not. Also, the BERTPooler output of KoBERT was designed to create an output feature in a size of 768; we can expect KoBERT to be trained for the classification without any problem. In GPT2, there would be a huge loss expected between the layer lm_head and output layer, while the loss could be minimal in DistilBERT and KoBERT, as shown in Table 1.     TABLE 1. Network Structures for Conventional Classification Task (DistilBERT, GPT2, KoBERT). DistilBertClassifier(   (bert): DistilBertModel(     (embeddings): Embeddings(       (word_embeddings): Embedding(119547, 768, padding_idx=0)       (position_embeddings): Embedding(512, 768)       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)       (dropout): Dropout(p=0.1, inplace=False)     )     (transformer): Transformer(       (layer): ModuleList(         (0-5): 6 x TransformerBlock(           (attention): MultiHeadSelfAttention(             (dropout): Dropout(p=0.1, inplace=False)             (q_lin): Linear(in_features=768, out_features=768, bias=True)             (k_lin): Linear(in_features=768, out_features=768, bias=True)             (v_lin): Linear(in_features=768, out_features=768, bias=True)             (out_lin): Linear(in_features=768, out_features=768, bias=True)           )           (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)           (ffn): FFN(             (dropout): Dropout(p=0.1, inplace=False)             (lin1): Linear(in_features=768, out_features=3072, bias=True)             (lin2): Linear(in_features=3072, out_features=768, bias=True)             (activation): GELUActivation()           )           (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)         )       )     )   )   (pre_classifier): Linear(in_features=768, out_features=768, bias=True)   (classifier): Linear(in_features=768, out_features=775, bias=True)   (dropout): Dropout(p=0.1, inplace=False) )  \nGPT2Classifier(   (gpt2): GPT2LMHeadModel(     (transformer): GPT2Model(       (wte): Embedding(100000, 768)       (wpe): Embedding(1024, 768)       (drop): Dropout(p=0.1, inplace=False)       (h): ModuleList(         (0-11): 12 x GPT2Block(           (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)           (attn): GPT2Attention(             (c_attn): Conv1D()             (c_proj): Conv1D()             (attn_dropout): Dropout(p=0.1, inplace=False)             (resid_dropout): Dropout(p=0.1, inplace=False)           )           (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)           (mlp): GPT2MLP(             (c_fc): Conv1D()             (c_proj): Conv1D()             (act): NewGELUActivation()             (dropout): Dropout(p=0.1, inplace=False)           )         )       )       (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)     )     (lm_head): Linear(in_features=768, out_features=100000, bias=False)   )   (drop): Dropout(p=0.1, inplace=False)   (out): Linear(in_features=100000, out_features=775, bias=True) )  \nkoBertClassifier(   (bert): BertModel(     (embeddings): BertEmbeddings(       (word_embeddings): Embedding(8002, 768, padding_idx=1)       (position_embeddings): Embedding(512, 768)       (token_type_embeddings): Embedding(2, 768)       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)       (dropout): Dropout(p=0.1, inplace=False)     )     (encoder): BertEncoder(       (layer): ModuleList(         (0-11): 12 x BertLayer(           (attention): BertAttention(             (self): BertSelfAttention(               (query): Linear(in_features=768, out_features=768, bias=True)               (key): Linear(in_features=768, out_features=768, bias=True)               (value): Linear(in_features=768, out_features=768, bias=True)               (dropout): Dropout(p=0.1, inplace=False)             )             (output): BertSelfOutput(               (dense): Linear(in_features=768, out_features=768, bias=True)               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)               (dropout): Dropout(p=0.1, inplace=False)             )           )           (intermediate): BertIntermediate(             (dense): Linear(in_features=768, out_features=3072, bias=True)             (intermediate_act_fn): GELUActivation()           )           (output): BertOutput(             (dense): Linear(in_features=3072, out_features=768, bias=True)             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)             (dropout): Dropout(p=0.1, inplace=False)           )         )       )     )     (pooler): BertPooler(       (dense): Linear(in_features=768, out_features=768, bias=True)       (activation): Tanh()     )   )   (pre_classifier): Linear(in_features=768, out_features=768, bias=True)   (classifier): Linear(in_features=768, out_features=775, bias=True)   (dropout): Dropout(p=0.1, inplace=False) ) \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3373470\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                                                                                                                                                                                                                         4 VOLUME XX, 2017 \nB. INFORMATION RETRIEVAL Regarding information retrieval, there have been many studies introduced. One of them is Okapi BM25. Its base algorithm was initially developed by Stephen E. Robertson, Karen Sp√§rck Jones, and others in the 1970s and 1980s [17]. As Okapi BM25 is an effective and traditional search algorithm, numerous research studies have been conducted based on it. Barlacchi et al. proposed a search engine, Appetitoso, for restaurant menu information retrieval based on dishes integrating BM25 [18]. Trotman et al. compared search engine ranking functions (BM25 and its variants) and reported that there was little difference in performance [19]. Kim et al. researched the retrieval and inference tasks related to case law and statute law [20]. They applied a Transformer-based approach and BM25 for entailment and retrieval tasks. An improved version of BM25 has been used to support precision medicine by retrieving gene and disease information from biomedical abstracts [21]. Besides Okapi BM25, Latent Semantic Indexing (LSI) is another information retrieval method based on Singular Value Decomposition (SVD) [22]. Wei et al. presented LSI-based multilingual document clustering technology, contributing to multilingual documents management [23]. Clustering clinical documents with LSI was studied, and it turned out that LSI is powerful enough to collect similar notions [24]. A novel emotion classification approach based on LSI was also proposed by Saqib et al. [25]. This study maximized LSI capability by creating automatic sentiment queries. Gong et al. conducted comparison experiments with a traditional information retrieval method leveraging relevance score and LSI method to generate text summarization [26].  C. FOOD RECOGNITION FOR NUTRIENT ESTIMATION To find accurate nutrient information, it is crucial to precisely recognize food types and food portions. Several studies have tried to develop food recognition technologies. Yang et al. suggested pairwise statistical techniques that leverage the positional characteristics of eight food ingredients [27]. Most food recognition research focused on computer vision technologies using image datasets. Various deep learning techniques help to offer high-quality food recognition results. Convolutional Neural Network (CNN) was applied to recognize food types through several food images [28,29]. Transfer learning and Word2Vec were implemented to extract text data from images of food and distribute food classes [30]. Several food recognition systems are also introduced. Pouladzadeh et al. presented a nutrient recognition system which recognizes food types and predicts calories of the food by leveraging food image processing [31]. Foodcam categorizes real-time food images by utilizing linear SVM [32]. Yunus et al. proposed a two-stage system, using CNN in the first stage to classify the given image, and then using the output as input for the second stage to estimate nutrition, ingredients, and attributes [33]. Noronha et al. suggested \nPlateMate, which divides tasks into three stages and uses crowdsourcing to extract nutritional estimates from images  [34]. However, significant challenges may lie ahead when one only uses visual information to recognize food nutrition.  D. EMBEDDING SPACE, FEW SHOT LEARNING, AND COMPUTATIONAL EFFICIENCY It is wise and necessary to reduce the re-training time of AI while the size of the dataset increases every year. Vision researchers have reported most related studies since image dataset dimensions are relatively higher than others. Yan et al. suggested an idea in the paper that their ContrastZSD optimizes the visual feature distribution and simultaneously alleviates the bias problem for improved zero-shot detection in the embedding space [39]. Their method was, firstly, they pulled an embedding from the pre-trained image that has the known Region of Interest (ROI) and class information. Then, they mapped the extracted embedding with the embeddings from the seen image and unseen image by using embedding functions. Finally, in the created embedding space, region-region contrastive learning was conducted with the projection head, which is in the pre-trained image, to optimize the visual feature distribution with higher discriminability. They also claimed that region-category contrastive learning was performed in the embedding space simultaneously [39].  In natural language processing studies, Bao et al. also proposed a paper about few-shot text classification, which can be achieved by embedded features by the attention generator and a ridge regressor. As they claimed in their paper, when they compared their work with CNN+PROTO, unseen text classification can be easily improved by 20.0% on average in 1-shot classification [41]. Jiang and Nachum proposed a paper on mitigating bias by reweighting the biased dataset features. The authors claimed that reweighting the data points in the training dataset can fix the biased dataset without changing the observed labels. The basic concept of the reweighting dataset features in their research is combining the biased embedding features with underlying, unbiased embedding features. The authors also delivered that their proposed idea could provide fairness guarantees in demographic parity and equal opportunities [42].  For the fast-paced deployment services, efficient training & inference time is also essential. Regarding the AI computational performance, Li et al. reported a paper that their proposed dynamic slice-able network (DS-Net++) applied ViT model (DS-ViT++) achieved 2.24√ó and 3.62√ó computation reduction over DeiT-S and ViT-S/16 with just slight accuracy drop (-0.3% and -0.6%). Their main idea relies on a dynamic weight-slicing scheme that achieves good hardware efficiency by predictively slicing network parameters during inference concerning different inputs [40].    \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3373470\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                                                                          Jeong et al: Feature Based Text Search Engine Mitigating Data Diversity Problem \nVOLUME XX, 2017  \n5 \nIII. METHOD A. BASELINE MODEL Prior to our primary undertaking, the Korean food name identification and its matching, there was a need for training a massive Korean language model; we chose the GPT2 model and its tokenizer for this task in the middle of 2022 as a baseline model. We collected the Korean language Wikipedia dataset for the GPT2 train and validation as of August 1st, 2022. The dataset can be downloaded from the link offered by Wikipedia. The link contains the latest, entire dump of the Korean language on Wikipedia. As of August 1st, 2022, we used the same link dataset and split it into the train and validation datasets; the train and validation dataset Wikipedia article counts were 334,420 and 83,605, respectively. According to the Pareto Principle, the articles were split randomly and 80% vs. 20%. This dataset is also available on the author‚Äôs account page of HuggingFace.  It is referred as ‚ÄúKorean_Wikipedia_Dataset_for_GPT2_August_2022\" under \"eaglewatch\" user account of HuggingFace. We trained the GPT2 tokenizer and then the GPT2LMHeadModel from scratch; there was no pre-trained weight. Since we used the basic structure of GPT2, we didn‚Äôt alter the number of the attention head and one of the Transformer layers; both remain the same - 12. Also, the number of embedding positions is set as the default 1024, and the dimensionality of embedding for both word embeddings and the hidden states in the Transformer layers remains 768 as the default. The maximum training step was 1,254,090, and the total number of floating-point operations (FLOPs) performed during the training was about 1.175e+18. The training and validation batch sizes were 32 and 16, respectively. The default activation function used was \"gelu_new\". The optimizer was AdamW, and its learning rate was 1e-7. The training loss was 0.1531, and the validation loss was 7.30387. After conducting a thorough analysis, it has become evident that our baseline model GPT2, which boasts 125M parameters, is experiencing clearly over-fitting. If the model is not properly regularized or the data is insufficient, it may resort to memorizing the training dataset instead of learning from it. It is also plausible that the training dataset, consisting of 334,420 articles, is not extensive enough to support a model of GPT2's size. Since we focused only on the Korean dataset, the Korean Wikipedia dataset was one of the most enormous Korean datasets we could find as of August 1st, 2022. Compared to the BERT model, which was known to be trained with BookCorpus containing approximately 11,000 unpublished books from various genres, amounting to over 1 billion words, and English Wikipedia containing almost 2.5 billion words, there is a disadvantage for our trained GPT2 model since it has more parameters. The weakness of our trained baseline model GPT2 is even more apparent if we talk about the multi-language BERT model, which can be referred to as \"bert-base-multilingual-cased\" model on HuggingFace, \nbecause this multi-language BERT model was trained with 104 Wikipedia articles including the Korean language. However, for the specific language task, we suspect that BERT or its knowledge-distillation version, DistilBERT, fully covers the specific consonant - tense consonants in the Korean language. Therefore, we assume our baseline model GPT2 might even perform better in this role. Also, we assume that embedded features have additional details, enabling it to mitigate diversity issues; it might be more beneficial to have an extra internal embedding dimension like baseline model GPT2 (100,000) compared to DistilBERT (768). This is why we chose GPT2 as a baseline model and trained this from scratch for the Korean food classification project.  B. DATA COLLECTION AND DATA PREPROCESSING We used the Korean Food Nutrient Database [35] offered by the Ministry of Food and Drug Safety, South Korea. The Korean Food Nutrient Database is a collection of nutritional data produced by numerous institutions. In the database, 775 food types representing Korean standard food types were selected with the value ‚Äònationwide(representative)‚Äô from the ‚Äòregion/manufacturer‚Äô column. The final variables are shown in Table 2. TABLE 2. Final Variables Description for Matched Food Items  Variables Description Food item Name of food item Serving Size Food amount per serving (g) Calories The calorie amount per serving size of food (g) Protein The protein amount per serving size of food (g) Fat The fat amount per serving size of food (g) Carbohydrate The carbohydrate amount per serving size of food (g) Sugar The sugar amount per serving size of food (g)  \n FIGURE 1. Character level augmentation with OpenAI API interface for Korean food names (tense consonants)  Moreover, it is crucial for text search engines to convert typos into proper text. Since the dataset only contains 775 \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3373470\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                                                                                                                                                                                                                         6 VOLUME XX, 2017 \nfood types, which do not have any typos, we created food typos data by utilizing ChatGPT, a global chatbot service offered by OpenAI. Since Korean food names use mostly tense consonants in their dish names, we use character level augmentation using OpenAI API, as shown in Figure 1. The total number of food typos data is 2,921, and the data were divided into an 8:2 ratio for train and test datasets, according to Pareto Principle.   \n  FIGURE 2. Feature builder and matcher diagram. The feature matcher is the text search engine and utilizes the transposed matrix made by the feature builder.   \n FIGURE 3. Dataset indices versus the similarity chart. This is the output of the text search engine. The red arrow indicates the top 1 prediction.  \nC. FEATURE BASED TEXT SEARCH ENGINE We used the hidden_state_output of each model output to build a text search engine. In our baseline model GPT2, hidden_state_output refers to the output activations from the final Transformer layer of the GPT2 model for each token in the input sequence. Since GPT2 is unidirectional, these activations for each token incorporate information from all previous tokens in the sequence. About BERT, even though, since BERT is bidirectional, each token's activations incorporate information from previous and subsequent tokens, the fact that the hidden_state_output also refers to the output activations from its final Transformer layer for each token in the input sequence is the same as GPT2.   Therefore, we constructed the text search engine as follows. Firstly, we tokenized the entire train database and collected all hidden_state_output features from each model [36]. For example, if the train dataset length is M and the hidden_state_output size is N, the assembled feature array will be ùëÄ√óùëÅ. Then, we built a two-layer neural network with an input size of N and an output size of M. This neural network \nhas yet to be trained, and the weight size is ùëÅ√óùëÄ. However, there is a similar meaningful array we have already obtained above. If we transpose the assembled feature ùëÄ√óùëÅ into ùëÅ√óùëÄ, then we can insert it into the newly built neural network; the neural network will produce a meaningful output of M whenever a Korean word is tokenized, and its feature in a size of N is acquired. The result indicates the closest indices of the dataset to the input feature. The M-sized indices output can be sorted and counted for the K-nearest classification task. The activation function of the fully-connected output is a linear function to preserve the feature information. The benefit of the suggested method is that it does not require the training process. The building feature vector itself is the training. This strategy also can be done in parallel to accelerate. We tried to normalize the hidden_state_output feature before inserting it into the feature matcher; however, there were no performance differences. This procedure can be repeated using a solid, pre-trained large language model whenever a new set of words is available. Figure 2 indicates how the feature of each model is collected and used for each text search engine. In Figure 3, the red arrow indicates the top 1 index while all arrows mark for the top k suggestions. This similarity can be calculated as follows.   S = ‚àëùë•!\"!#$√óùë§!                              (1)   In Equation (1), S is similarity, x is input, and w is weights. Since we used a linear activation function in this study, if x and w are identical or close to each other, S will be maximized. This equation resembles the word classification function after the attention generator mentioned in the paper of Bao et al. [39]. Our suggested method is not a quantization method but an embedding based method and a variant of Local Sensitive Hashing (LSH) [38]. In the LSH, searching the nearest neighbor is, firstly, finding the bucket of the hash of the query items. Then, compare every single item in the bucket. This operation can be easily archived if we employ a pre-calculated transposed matrix, as indicated in Equation (1). We map hidden_state_output vectors to a finite set of known values - indices of the dataset. A similar study was reported by Facebook in 2021, and their method involves K-mean algorithms and multi-level, hierarchical quantization of indexing [37]. In our suggested method, we don‚Äôt quantize the indexing structure in multiple levels. Since we employed a fully-connected layer with a linear activation function for the feature matcher, we rely on the cosine similarity between input and weights and whenever a similar feature to the training dataset is acquired, the most expected index of the dataset will be matched as an output. However, in this method, it is essential to have a linear activation function to preserve the feature information. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3373470\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                                                                          Jeong et al: Feature Based Text Search Engine Mitigating Data Diversity Problem \nVOLUME XX, 2017  \n7 \n FIGURE 4. The Tensorflow Code Example of Feature Extraction and Building Figure 4 shows the Tensorflow code examples of feature extraction. The code shows the transposed feature is used for the weights of the feature matcher. Moreover, as mentioned earlier, the activation of the Dense layer ‚Äòfc1‚Äô must be a linear function, which not only preserves the information but also enables the network to learn from the known data to unseen data by identifying patterns and semantic similarities in the embedding space [12, 39].  IV. RESULTS AND DISCUSSION We evaluated the performance of three feature-based matching models, namely GPT2, DistilBERT, and KoBERT, by calculating their top K accuracies.  TABLE 3. Top K Accuracy Comparisons  Models Top 1 Accuracy Top 3 Accuracy Top 5 Accuracy Top 1 FMM/CCM ratio GPT2 FMM 65.92% 84.59% 90.58% 2.73 GPT2 CCM 24.14% 45.89% 60.62% DistilBERT FMM 66.44% 74.14% 79.11% 1.054 DistilBERT CCM 63.01% 68.32% 69.52% KoBERT FMM 79.79% 90.24% 93.84% 0.910 KoBERT CCM 87.62% 94.35% 95.03% FMM: Feature-Matching Method CCM: Conventional Classification Method \nIn addition to the LLM-based feature-matching models, we also performed traditional classification tasks using the models listed in Table 1. We compared the top K accuracies of both methods and calculated the ratio of the top 1 feature-matching method accuracy to the top 1 conventional classification accuracy. Table 3 shows the results of these comparisons. Among the three models, KoBERT shows the best top K accuracy score results. GPT2 reveals the worst performance in top 1 Accuracy, but the top 3 and top 5 accuracy results of GPT2 are better than those of DistilBERT. Even though the top 1 accuracy of DistilBERT feature matching is better than that of GPT2 feature matching, the performance of DistilBERT feature matching does not improve well in top 3 and top 5 accuracy because the tokenizer of DistilBERT can only tokenize 75.42% of total food typos data. We suspect the DistilBERT model has a data coverage issue and was not trained adequately for Korean tense consonant words. DistilBERT often returns UNK tokens for the Korean tense consonant words in the training dataset. It may not be happening or a sporadic case for the DistilBERT tokenizer if it tokenizes English words. However, unfortunately, Korean cuisines often use words made up of tense consonants in their dish names. This can pose a challenge for the DistilBERT model, which may struggle to process these names accurately; The DistilBERT tokenizer often returns UNK tokens for the Korean dish names. This suggests that both BERT and DistilBERT suffer from dataset diversity problems.  TABLE 4. Time Comparisons  \nModels FB Time  (sec) CCM Training Time (sec) epoch: 400  batch size: 16 Time Advantage FB / CCM ratio GPT2 1793  4834.5 2.69 DistilBERT 224 1830 8.169 KoBERT 344 2367 6.88 FB: Feature-Building CCM: Conventional Classification Method  The benefit of the suggested method is also significantly shown in Table 3. When the model still suffers from an under-trained, narrow-domain tokenizer issue (DistilBERT), which does not fully cover the Korean tense consonants, its top 1 feature-matching performance is still out-performing top 1 conventional classification performance. It indicates that embedding space, hidden_state_output, contains critical, unseen proximity relationships between data points and feature-matching process can mitigate biased datasets [42].   However, if the model is well-prepared and pre-trained, like KoBERT, the model performs better in the conventional classification task. If the model and tokenizer were well-\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3373470\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                                                                                                                                                                                                                         8 VOLUME XX, 2017 \ntrained, reconstructing an embedding space via a feature-matching method might disrupt proximity relationships and have a negative effect. Interestingly, the baseline GPT2 model, which has more parameters than KoBERT and was over-fitted in pre-training, received a massive benefit from our suggested method. It suffers 24.14% top 1 accuracy issues in the conventional classification task, but due to its output size of 100,000, feature-matching method top 1 accuracy increases about 2.73 times. We assume that GPT2 has benefited from this method with 786,432 (1024 x 768) features per token input, while KoBERT and DistilBERT have 294,912 (384 x 768) features.  We examined the hidden_state_output of each model with manifold methods. As described in Figure 2, we fed the Korean food training dataset into each model to build a feature-matcher. After making each of them, we extracted the hidden_state_output and exported it as a Numpy NPY file [45].   When each hidden_state_output was selectively extracted using the feature-matching method, the feature-building method dismembered the original proximity relationship within its embedding space and aligned a new feature mapping to the narrow-trained embedding space during the feature-building. Since the extracted hidden_state_output still retains the original hidden relationship within the embedding space, we assumed that if we plot it in a 2D plot through a manifold method, it would reveal how the DistilBERT embedding space differs from GPT2 and Kobert, which were only trained with Korean Wikipedia datasets. The extracted hidden_state_output has a minimal data range; we had to rescale it with RobustScaler [46]. Then, we employed IncrementalPCA and TSNE from NVIDIA Rapids.ai API for the manifold methods [47, 48, 49]. We kept the same parameters for each manifold method, as depicted in Figure 5. We used Kullback-Leibler divergence loss (KL divergence loss) to measure the performance of the dimensionality reduction conversion from each hidden_state_output to a 2D [50]. The measured train dataset KL divergence loss was 0.312, 0.242, and 0.177 for DistilBERT, GPT2, and KoBERT, respectively.  \nFIGURE 5. Dimensionality reduction process example and parameters (PCA num. of components: 25, batch size: 32, TSNE perplexity:5, num. of neighbors: 50, method: ‚Äòexact‚Äô. The parameters were fixed and shared per model.) \nFIGURE 6. Manifold processed hidden state output of the DistilBERT (Korean food dataset, left: train, right: test) Unique ball-shaped data points appeared. \nFIGURE 7. Manifold processed hidden state output of the GPT2 (Korean food dataset, left: train, right: test)  \nFIGURE 8. Manifold processed hidden state output of the KoBERT (Korean food dataset, left: train, right: test) \nFIGURE 9. Left: cluster analysis of the DistilBERT on Korean food train dataset (pale dots are UNK tokens), right: manifold processed hidden state output of the DistilBERT on random 775 English words from WordNet.   \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3373470\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                                                                          Jeong et al: Feature Based Text Search Engine Mitigating Data Diversity Problem \nVOLUME XX, 2017  \n9 \nWe processed the Korean food test dataset for each model as well and all manifold parameters were kept identical. The measured test dataset KL divergence loss was 0.368, 0.287, and 0.254 for DistilBERT, GPT2, and KoBERT, respectively. Plotted hidden_state_output with the same Korean food dataset are shown in Figure 6-8. As discussed earlier, GPT2 and KoBERT used the same Korean Wikipedia dataset during pre-training session and also showed similarly shaped data points. Interestingly, the DistilBERT has uniquely shaped data points. In Figure 6, the mid and lower parts are similar to those of GPT2 and KoBERT in Figure 7-8, while it has uniquely shaped ball-looking dense data points at the top-right area. These might be the data points of UNK tokens or completely different from Korean food datasets. To investigate it further, we used Density-based spatial clustering of applications with noise (DBSCAN) clustering algorithm to determine whether the ball-shaped cluster is a full of the UNK tokens [51]. Since we already knew the number of UNK tokens that the DistilBERT tokeninzer can't understand ‚Äì 718, we tried to match the number of noise clusters to this number by optimizing the number of the cluster by measuring Silhouette scores and Davies-Bouldin Index while adjusting the EPS (\"epsilon,\" the radius of the neighborhood around a point) which is range from 0.001 to 50 with a step of 0.2 [52, 53]. We obtained the best optimized EPS value of 3.801, creating a total of 669 noises for the noise cluster by minimizing the Davies-Bouldin Index and maximizing Silhouette scores simultaneously. The ball-shaped cluster was assigned as Cluster 7, and its element count was 347. We were able to locate 93% (669 out of 718) of total UNK tokens - noise cluster elements; none of the found noise elements were close to Cluster 7. Therefore, we can confirm that Cluster 7 is not full of UNK tokens; instead, Cluster 7 could be a set of different embedding features inherited from its original embedding space. Manifold methods clearly separated the dense ball-shaped data points area from others in Figure 6. Since we fed the same Korean food dataset into those, also GPT2 and KoBERT share identical pre-training datasets, we assumed that DistilBERT has a different embedding space, separated from the one commonly found in both GPT2 and KoBERT. We wanted to compare the result of the DistilBERT with other public English dataset ‚Äì WordNet. For the DistilBERT model, we randomly chose 775 English words from the WordNet dataset, fed them into the DistilBERT, and extracted the hidden_state_output to inspect if there were dense ball-shaped data points. As shown in the left image of Figure 9, the extracted hidden_state_output embedding space doesn't show the abnormal ball-shaped dense data points but random-shaped clusters. This also confirms that Cluster 7 of the left plot in Figure 9 means an entirely distinct embedding space inherited from its original embedding relationship, and the manifold algorithm divided Korean food names from it. The DistilBERT model still can't match Korean tense consonants (UNK tokens) with their correct pairs by the feature-matching \nmethod. However, despite being narrow-trained, the feature-matching method re-aligned a new embedding space and found the nearest proximity relationship within the newly aligned embedding features. The left image of Figure 9 shows the 2D representation of the features made by the feature-matching would look like. This indicates that an LLM with a narrow coverage of a specific spoken language holds its original sets of characteristics separately while it also contains newly learned features without having trained with an entirely new set of brand-new datasets.  The conventional classification task only looks for the best strategy to avoid local minima and have the best gradient descent simultaneously. We presumed that the unevenly disconnected hidden_state_output data point, the ball-shaped embedding structure, in DistilBERT could also existed even before being extracted from its original embedding space and created the local minima during the conventional classification tasks. Even though GPT2 doesn't have the strangely looking dense data points in the 2D plot, the performance of the traditional classification task suffered from dimension differences between the lm_head output (100,000) and the classifier's output (775). KoBERT has data points similar to GPT2 and has a better conventional classification structure; as shown in Table 1,  it marked the best performance. Suppose an LLM is trained with the narrow trimmed dataset. In that case, the feature-extraction method contributes to an embedding disruption of the original embedding space, and feature-building does to a different embedding distribution over a new embedding space. Figure 6 and the left image of Figure 9 also indicate that even if A.I. was once narrow-trained, it could be mitigated with the feature-matching method. Furthermore, it became relatively more straightforward for the feature-matching method to achieve better accuracies while the same model struggled to make a good gradient descent in conventional classification tasks. Moreover, we measured feature-building time for the training dataset to compare the computational efficiency of the three models. Table 4 reveals the required time to collect hidden_state_output features in the train datasets. DistilBERT was the fastest and just took only 224 sec. KoBERT did in 344 sec, which was similar to DistilBERT, while GPT2 sluggishly made it in 1793 sec compared to both DistilBERT and KoBERT, which means BERT-based models have better computational efficiency than GPT2.  Table 5 shows the hardware environment that was used in our study. TABLE 5. Hardware Environment  System Description CPU 16 cores RAM DDR4 128GB GPU NVIDIA 4090 RTX 24GB OS Windows 11 with WSL2 \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3373470\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                                                                                                                                                                                                                         10 VOLUME XX, 2017 \n \n FIGURE 10. The example of text search engine prediction \nFIGURE 11. The right Korean name of Corn Salad and its typo  Since our proposed study can match similar features of hidden_state_output, Figure 10 depicts its demonstration. Our proposed feature-matching-based text search engine can assign food typos to the right text. Figure 10 shows an example of DistilBERT feature-matching text search engine prediction results. The Korean typo, as you can see in Figure 11, which means ‚Äòcorn salad‚Äô, was fed into the text search engine. According to Figure 10, the model predicts the right name, corn salad, and nutritional information of the food was found with the prediction result.  TABLE 6. Memory and Disc Consumption Comparisons  Models Memory Consumption FMM HDF5 File Sizes FMM Pytorch PTH File Sizes CCM GPT2 14.7 GB 6.84 GB 960.6 MB DistilBERT 5.51 GB 2.56 GB 541.4 MB KoBERT 5.51 GB 2.56 GB 373.6 MB FMM: Feature-Matching Method CCM: Conventional Classification Method   We must mention a memory and disc consumption issue of our suggested model. Since we collected the whole hidden_state_output of each model for the entire train dataset, it is inevitable to have a memory consumption issue in this study. In this approach, we found that GPT2 suffers the most and consumes almost 15GB for 775 Korean word classes. For the feature matching, we used the TensorFlow framework; as soon as the features were collected, we saved them to HDF5 files to reduce memory usage. We used the Pytorch framework for the classification task, and it shows the minimal usage of \nthe disc. Table 6 is memory & disc consumption comparisons of three LLM models. As shown in both Tables 3 and 4, one can notice that the DistilBERT model suffers from an under-trained tokenizer issue even though it was trained with a massive number of documents. If they fine-tune or make a domain adaptation process on the DistilBERT tokenizer and the model once again for the Korean tense consonants, the DistilBERT might be the top model in both conventional classification performance and energy efficiency issues. Also, as shown in Table 4, DistilBERT has a considerable advantage in fast deployment systems.  As mentioned previously, if an LLM suffers from a narrow domain dataset issue, there is a similar statistical term called - 'distribution mismatch.'  It might work well with the data seen during training and validation. However, this A.I. may be different from the broader population. If the dataset represents something related to individuals or ethnic groups or something sensitive to cultural aspects - foods or even lifestyle, it becomes critical. If a retail company trains its commercial AI on a narrow dataset and uses it publicly, it may provide inaccurate results for unseen data, leading to suboptimal decisions and possible legal issues. Besides, the narrow-trained A.I. can't even follow the real meaning of cultural context and subtle nuances specific to a language and its speakers. If so, the A.I. needs support to grasp these aspects in languages with narrower coverage, leading to responses that lack cultural relevance or acuity. Furthermore, crafting literary or poetic content requires a deep understanding of a language's stylistic and rhythmic elements. For languages with narrow, limited representation in training datasets, A.I. may not effectively capture the essence of literary creativity specific to those languages. Unfortunately, this literature mainly relates to their lifestyle and food culture; if A.I. could not comprehend the sub-meaning of a word related to its cultural food name, the crafting creativity of A.I. would be limited.  Although the feature-extraction-based text search engine shows limited performance compared to classification methods, the text search engine can be built fast by leveraging pre-trained LLM models. We expect a better performance by utilizing numerous data which contain real-life food pronunciation STT data in the future work. Furthermore, since our text search engine can be applied to various healthcare mobile applications, such as diet management apps and nutrition checking apps for older adults, the future work may build healthcare applications  with this text search engine with reduced usage of memory and disc by employing a dimensionality reduction or weights quantization for hidden_state_output. In this study, we only talked about the Korean food names dataset - tense consonant issues and narrow-trained LLMs that might cause personal discomfort if there is a misinterpretation after a speech-to-text service process, the significance of overcoming narrow-trained LLMs in speech-to-text services is critical. Because the emergency dispatch headquarters and business customer call centers use \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3373470\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                                                                          Jeong et al: Feature Based Text Search Engine Mitigating Data Diversity Problem \nVOLUME XX, 2017  \n11 \nspeech-to-text services in their data inquiries, if a narrow-trained LLM is involved in their services, severe confusion or misinterpretation might occur. Therefore, it is advantageous to overcome the narrow-trained LLMs without re-training the whole model and its tokenizer in a fast-paced deployment system. V. CONCLUSION In this paper, we proposed a novel idea that the extracted embedding space, hidden_state_output, is the critical factor in mitigating narrow-trained AI issues because if we build a feature-matching network with it, the feature-matching network will map underlying unseen data points with similar characteristics in proximity, which resulted in enhanced performance even though the AI model still was not re-trained with a well-organized, unbiased dataset. As a part of the demonstration, we built text search engines to utilize the hidden_state_output of three LLM models (GPT2, DistilBERT, and KoBERT) and conducted comparison experiments with the traditional classification of the three LLM models to prove the performance and computational efficiency of the proposed text search engine idea. Still, having full-coverage, well-prepared, and pre-trained LLMs for the task is the best practice. However, as novel LLMs are introduced every year, they are primarily trained for major languages (e.g. English etc.) and specific language models of the pre-trained LLMs do not have equal level proficiency for their language needs (e.g. tense consonants) when they are initially introduced by the first party; they still need to be re-trained by the other researchers and when this happens, besides the domain adaptation and other issues, the data sparsity and diversity problem must be mitigated. For this issue, we proposed our study with the Korean food dataset in this paper; even though we showed some good results, there is still room to improve our suggested method to reduce the usage of memory and disc if we conduct a weights quantization for hidden_state_output in the subsequent research. REFERENCES [1] J. A. Mattison, R. J. Colman, T. M. Beasley, D. B. Allison, J. W. Kemnitz, G. S. Roth, et al., \"Caloric restriction improves health and survival of rhesus monkeys,\" Nat. Commun., vol. 8, no. 1, p. 14063, 2017. DOI: https://doi.org/10.1038/ncomms14063 [2] M. E. Levine, J. A. Suarez, S. Brandhorst, P. Balasubramanian, C. W. Cheng, F. Madia, et al., \"Low protein intake is associated with a major reduction in IGF-1, cancer, and overall mortality in the 65 and younger but not older population,\" Cell Metab., vol. 19, no. 3, pp. 407-417, 2014. DOI: https://doi.org/10.1016/j.cmet.2014.02.006 [3] K. T. Howitz, K. J. Bitterman, H. Y. Cohen, D. W. Lamming, S. Lavu, J. G. Wood, et al., \"Small molecule activators of sirtuins extend Saccharomyces cerevisiae lifespan,\" Nature, vol. 425, no. 6954, pp. 191-196, 2003. DOI: https://doi.org/10.1038/nature01960 [4] D. Buettner and S. Skemp, \"Blue zones: lessons from the world's longest lived,\" Am. J. Lifestyle Med., vol. 10, no. 5, pp. 318-321, 2016. DOI: https://doi.org/10.1177/1559827616637066 [5] J. Dodge, G. Ilharco, R. Schwartz, A. Farhadi, H. Hajishirzi, and N. Smith, \"Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping,\" arXiv preprint \narXiv:2002.06305, 2020. DOI: https://doi.org/10.48550/arXiv.2002.06305 [6] A. Rietzler, S. Stabinger, P. Opitz, and S. Engl, \"Adapt or get left behind: Domain adaptation through BERT language model finetuning for aspect-target sentiment classification,\" arXiv preprint arXiv:1908.11860, 2019. DOI: https://doi.org/10.48550/arXiv.1908.11860 [7] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, \"Curriculum learning,\" in Proceedings of the 26th Annual International Conference on Machine Learning, pp. 41-48, June 2009. DOI: https://doi.org/10.1145/1553374.1553380 [8] J. Kremer, K. Steenstrup Pedersen, and C. Igel, \"Active learning with support vector machines,\" Wiley Interdiscip. Rev. Data Min. Knowl. Discov., vol. 4, no. 4, pp. 313-326, 2014. DOI: https://doi.org/10.1002/widm.1132 [9] S. Edunov, M. Ott, M. Auli, and D. Grangier, \"Understanding back-translation at scale,\" arXiv preprint arXiv:1808.09381, 2018. DOI: https://doi.org/10.48550/arXiv.1808.09381 [10] J. Wei and K. Zou, \"Eda: Easy data augmentation techniques for boosting performance on text classification tasks,\" arXiv preprint arXiv:1901.11196, 2019. DOI: https://doi.org/10.48550/arXiv.1901.11196 [11] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, \"Language models are unsupervised multitask learners,\" OpenAI Blog, vol. 1, no. 8, p. 9, 2019. [Online]. Available: https://openai.com/blog/better-language-models/  [12] J. Devlin, M. W. Chang, K. Lee, and K. Toutanova, \"Bert: Pre-training of deep bidirectional transformers for language understanding,\" arXiv preprint arXiv:1810.04805, 2018. DOI: https://doi.org/10.48550/arXiv.1810.04805 [13] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, et al., \"Attention is all you need,\" in Advances in Neural Information Processing Systems, vol. 30, 2017. [14] G. Hinton, O. Vinyals, and J. Dean, \"Distilling the knowledge in a neural network,\" arXiv preprint arXiv:1503.02531, 2015. DOI: https://doi.org/10.48550/arXiv.1503.02531 [15] Github, Korean BERT pre-trained cased (KoBERT), https://github.com/SKTBrain/KoBERT [16] Github, Naver Sentiment Movie Corpus, https://github.com/e9t/nsmc [17] S. E. Robertson and K. S. Jones, \"Relevance weighting of search terms,\" J. Am. Soc. Inf. Sci., vol. 27, no. 3, pp. 129-146, 1976. DOI: https://doi.org/10.1002/asi.4630270302 [18] G. Barlacchi, A. Abad, E. Rossinelli, and A. Moschitti, \"Appetitoso: A Search Engine for Restaurant Retrieval based on Dishes,\" in CLiC-it/EVALITA, December 2016. [19] A. Trotman, A. Puurula, and B. Burgess, \"Improvements to BM25 and language models examined,\" in Proceedings of the 19th Australasian Document Computing Symposium, pp. 58-65, November 2014. DOI: https://doi.org/10.1145/2682862.2682863 [20] M. Y. Kim, J. Rabelo, K. Okeke, and R. Goebel, \"Legal information retrieval and entailment based on BM25, transformer, and semantic thesaurus methods,\" The Review of Socionetwork Strategies, vol. 16, no. 1, pp. 157-174, 2022. DOI: https://doi.org/10.1007/s12626-022-00103-1 [21] Z. Zhang, \"An improved BM25 algorithm for clinical decision support in Precision Medicine based on co-word analysis and Cuckoo Search,\" BMC Med. Inform. Decis. Mak., vol. 21, pp. 1-15, 2021. DOI: https://doi.org/10.1186/s12911-021-01454-5 [22] S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman, \"Indexing by latent semantic analysis,\" J. Am. Soc. Inf. Sci., vol. 41, no. 6, pp. 391-407, 1990. DOI: https://doi.org/10.1002/(SICI)1097-4571(199009)41:6%3C391::AID-ASI1%3E3.0.CO;2-9 [23] C. P. Wei, C. C. Yang, and C. M. Lin, \"A latent semantic indexing-based approach to multilingual document clustering,\" Decision Support Systems, vol. 45, no. 3, pp. 606-620, 2008. DOI: https://doi.org/10.1016/j.dss.2007.07.008 [24] C. Han and J. Choi, \"Effect of Latent Semantic Indexing for Clustering Clinical Documents,\" 2010 IEEE/ACIS 9th International Conference on Computer and Information Science, Yamagata, Japan, 2010, pp. 561-566, DOI: https://doi.org/10.1109/ICIS.2010.138 \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3373470\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                                                                                                                                                                                                                         12 VOLUME XX, 2017 \n[25] S. M. Saqib, F. M. Kundi, and S. Ahmad, \"Unsupervised learning method for sorting positive and negative reviews using LSI (latent semantic indexing) with automatic generated queries,\" IJCSNS Int. J. Comput. Sci. Netw. Secur, vol. 18, no. 1, pp. 56-62, 2018. [26] Y. Gong and X. Liu, \"Generic text summarization using relevance measure and latent semantic analysis,\" in Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 19-25, September 2001. DOI: https://doi.org/10.1145/383952.383955 [27] S. Yang, M. Chen, D. Pomerleau and R. Sukthankar, \"Food recognition using statistics of pairwise local features,\" 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, San Francisco, CA, USA, 2010, pp. 2249-2256, DOI: https://doi.org/10.1109/CVPR.2010.5539907 [28] H. Kagaya, K. Aizawa, and M. Ogawa, \"Food detection and recognition using convolutional neural network,\" in Proceedings of the 22nd ACM International Conference on Multimedia, pp. 1085-1088, November 2014. DOI: https://doi.org/10.1145/2647868.2654970 [29] H. Chen, J. Xu, G. Xiao, Q. Wu, and S. Zhang, \"Fast auto-clean CNN model for online prediction of food materials,\" Journal of Parallel and Distributed Computing, vol. 117, pp. 218-227, 2018. DOI: https://doi.org/10.1016/j.jpdc.2017.07.004 [30] Z. Shen, A. Shehzad, S. Chen, H. Sun, and J. Liu, \"Machine learning based approach on food recognition and nutrition estimation,\" Procedia Comput. Sci., vol. 174, pp. 448-453, 2020. DOI: https://doi.org/10.1016/j.procs.2020.06.113 [31] P. Pouladzadeh, S. Shirmohammadi and R. Al-Maghrabi, \"Measuring Calorie and Nutrition From Food Image,\" in IEEE Transactions on Instrumentation and Measurement, vol. 63, no. 8, pp. 1947-1956, Aug. 2014, DOI: https://doi.org/10.1109/TIM.2014.2303533 [32] Y. Kawano and K. Yanai, \"Foodcam: A real-time food recognition system on a smartphone,\" Multimedia Tools Appl., vol. 74, pp. 5263-5287, 2015. DOI: https://doi.org/10.1007/s11042-014-2000-8 [33] R. Yunus et al., \"A Framework to Estimate the Nutritional Value of Food in Real Time Using Deep Learning Techniques,\" in IEEE Access, vol. 7, pp. 2643-2652, 2019, DOI: https://doi.org/10.1109/ACCESS.2018.2879117 [34] J. Noronha, E. Hysen, H. Zhang, and K. Z. Gajos, \"Platemate: crowdsourcing nutritional analysis from food photographs,\" in Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology, pp. 1-12, October 2011. DOI: https://doi.org/10.1145/2047196.2047198 [35] Ministry of Food and Drug Safety, Food Nutrient Database, https://various.foodsafetykorea.go.kr/nutrient/ [36] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, \"How transferable are features in deep neural networks?,\" in Advances in Neural Information Processing Systems, vol. 27, 2014. [37] J. Johnson, M. Douze and H. J√©gou, \"Billion-Scale Similarity Search with GPUs,\" in IEEE Transactions on Big Data, vol. 7, no. 3, pp. 535-547, 1 July 2021, DOI: https://doi.org/10.1109/TBDATA.2019.2921572  [38] P. Indyk and R. Motwani, \"Approximate nearest neighbors: towards removing the curse of dimensionality,\" in Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, 1998, pp. 604‚Äì613. DOI: https://doi.org/10.1145/276698.276876 [39] C. Yan, X. Chang, M. Luo, H. Liu, X. Zhang, and Q. Zheng, \"Semantics-guided contrastive network for zero-shot object detection,\" IEEE Trans. Pattern Anal. Mach. Intell., 2022. DOI: https://doi.org/10.1109/TPAMI.2021.3140070    [40] C. Li, G. Wang, B. Wang, X. Liang, Z. Li and X. Chang, \"DS-Net++: Dynamic Weight Slicing for Efficient Inference in CNNs and Vision Transformers,\" in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 4, pp. 4430-4446, 1 April 2023, DOI: https://doi.org/10.48550/arXiv.2109.10060   [41] Y. Bao, M. Wu, S. Chang, and R. Barzilay, ‚ÄúFew-shot text classifi- cation with distributional signatures,‚Äù in Proc. Int. Conf. Learn. Representations, 2020, pp. 1‚Äì24. DOI:  https://doi.org/10.48550/arXiv.1908.06039  [42] [42] H. Jiang and O. Nachum, \"Identifying and correcting label bias in machine learning,\" in International Conference on Artificial Intelligence and Statistics, pp. 702-712, June 2020. DOI: https://doi.org/10.48550/arXiv.1901.04966  \n[43] Larry Hardesty | MIT News Office. \"Study finds gender and skin-type bias in commercial artificial-intelligence systems.\" MIT News. [Online]. Available: https://news.mit.edu/2018/study-finds-gender-skin-type-bias-artificial-intelligence-systems-0212.[Accessed: Nov. 7, 2023]. [44] Jethro Mullen, CNN. \"Google apologizes after photo app tags black people as 'gorillas'.\" CNN Tech. [Online]. Available: https://edition.cnn.com/2015/07/02/tech/google-image-recognition-gorillas-tag/index.html. [Accessed: Nov. 7, 2023]  [45] NumPy. (n.d.). \"NumPy: A fundamental package for scientific computing with Python.\" Retrieved from https://numpy.org. [46] H. Qian, Q. Wen, L. Sun, J. Gu, Q. Niu, and Z. Tang, \"RobustScaler: QoS-Aware Autoscaling for Complex Workloads,\" in 2022 IEEE 38th International Conference on Data Engineering (ICDE), pp. 2762-2775, Kuala Lumpur, Malaysia, 2022. DOI: https://doi.org/10.1109/ICDE53745.2022.00252  [47] R. Hou, H. Wang, Y. Xiao, and W. Xu, \"Incremental PCA based online model updating for multivariate process monitoring,\" in Proceedings of the 10th World Congress on Intelligent Control and Automation, pp. 3422-3427, Beijing, China, 2012. DOI: https://doi.org/10.1109/WCICA.2012.6359039 [48] L. van der Maaten and G. Hinton, \"Visualizing Data using t-SNE,\" Journal of Machine Learning Research, vol. 9, pp. 2579-2605, 2008. DOI: https://doi.org/10.1007/978-3-642-98243-1_28  [49] Rapids.ai. (n.d.). \"Rapids.ai: Open GPU Data Science.\" Retrieved from https://rapids.ai. [50] S. Kullback and R. A. Leibler, \"On Information and Sufficiency,\" Annals of Mathematical Statistics, vol. 22, no. 1, pp. 79-86, 1951 [51] M. Ester, H.-P. Kriegel, J. Sander, and X. Xu, \"A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise,\" in Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD-96), pp. 226-231, 1996. DOI: https://doi.org/10.1109/ICDM.1996.566975  [52] P. J. Rousseeuw, \"Silhouettes: A graphical aid to the interpretation and validation of cluster analysis,\" Journal of Computational and Applied Mathematics, vol. 20, pp. 53-65, 1987. DOI: https://doi.org/10.1016/0377-0427(87)90125-7  [53] D. L. Davies and D. W. Bouldin, \"A Cluster Separation Measure,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. PAMI-1, no. 2, pp. 224-227, 1979. DOI: https://doi.org/10.1109/TPAMI.1979.4766909    YONGWOO JEONG (Member, IEEE) is a lead AI scientist at Rowan Inc. He received a Ph.D. in electrical engineering from the State University of New York at Buffalo, NY, U.S.A. in 2011, and his dissertation focus was on bio-inspired artificial intelligence implementation on CMOS sensor systems. With a background in recent various Transformer projects (e.g., Pegasus Transformer-based stock market news summarization) he is currently interested in research on Google T5 Transformer-based Tinnitus THI score and depression score PHQ-8 predictions.  JISEON YANG received a B.S. degree in Mathematics (double major: Statistics) from Ewha Womans University, Seoul, Republic of Korea, in 2022. Since 2022, she has been working at Rowan, Inc., which is a Digital Therapeutics company in South Korea, as an AI researcher. She wrote a paper about analysis of digital therapeutics app log data. Her current research interests include medical data analysis and medical application of Transformer.  \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3373470\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                                                                          Jeong et al: Feature Based Text Search Engine Mitigating Data Diversity Problem \nVOLUME XX, 2017  \n13 \nIN HO CHOI received a B.S. degree in computer science from the Korean Academic Credit Bank System, South Korea, in 2022. He has been working as an AI Engineer since 2020, working for companies like Samsung. He has experience in making semiconductor defect detecting systems, and AI integrated IoT devices using microprocessors. He is currently an AI Engineer at Rowan, Inc. in South Korea, and has special interest in reinforcement learning and multi-modal models.  JUYEON LEE received a B.S. degree with a major in Life Science and a minor in Computer Science, from Kookmin University, in Seoul. She was an undergraduate research assistant at the University of California, Irvine, CA, USA, in the winter quarter of 2020. From 2020 to 2023, she worked as a Researcher at AIDX Inc., a medical AI company in South Korea. With her biological background, she conducted research and published a paper on the risk prediction of disease. She is currently interested in life-logging data analysis and Recommendation System areas. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3373470\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Software deployment",
  "concepts": [
    {
      "name": "Software deployment",
      "score": 0.8234670758247375
    },
    {
      "name": "Computer science",
      "score": 0.7991165518760681
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.6270096302032471
    },
    {
      "name": "Diversity (politics)",
      "score": 0.5898780822753906
    },
    {
      "name": "Search engine",
      "score": 0.5306885242462158
    },
    {
      "name": "Information retrieval",
      "score": 0.3878556191921234
    },
    {
      "name": "World Wide Web",
      "score": 0.38086578249931335
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3420674204826355
    },
    {
      "name": "Database",
      "score": 0.3262888789176941
    },
    {
      "name": "Operating system",
      "score": 0.1952577531337738
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Anthropology",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 3
}