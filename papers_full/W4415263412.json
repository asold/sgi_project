{
  "title": "Evaluating the performance of general purpose large language models in identifying human facial emotions",
  "url": "https://openalex.org/W4415263412",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2161868073",
      "name": "Benjamin W. Nelson",
      "affiliations": [
        "Beth Israel Deaconess Medical Center",
        "Harvard University",
        null
      ]
    },
    {
      "id": "https://openalex.org/A1517887898",
      "name": "Ari Winbush",
      "affiliations": [
        "University of Oregon"
      ]
    },
    {
      "id": "https://openalex.org/A5104423840",
      "name": "Steven Siddals",
      "affiliations": [
        "Harvard University",
        "Beth Israel Deaconess Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A5098779094",
      "name": "Matthew Flathers",
      "affiliations": [
        "Beth Israel Deaconess Medical Center",
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2168625451",
      "name": "Nicholas B. Allen",
      "affiliations": [
        "University of Oregon"
      ]
    },
    {
      "id": "https://openalex.org/A2233486890",
      "name": "John Torous",
      "affiliations": [
        "Harvard University",
        "Beth Israel Deaconess Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2161868073",
      "name": "Benjamin W. Nelson",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1517887898",
      "name": "Ari Winbush",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5104423840",
      "name": "Steven Siddals",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5098779094",
      "name": "Matthew Flathers",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2168625451",
      "name": "Nicholas B. Allen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2233486890",
      "name": "John Torous",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4403863303",
    "https://openalex.org/W4398143508",
    "https://openalex.org/W2056415169",
    "https://openalex.org/W4321277158",
    "https://openalex.org/W4410629676",
    "https://openalex.org/W4390202391",
    "https://openalex.org/W4405971183",
    "https://openalex.org/W4393397034",
    "https://openalex.org/W4387583347",
    "https://openalex.org/W2725560969",
    "https://openalex.org/W2108223677",
    "https://openalex.org/W4390116171",
    "https://openalex.org/W3214214739",
    "https://openalex.org/W3049568452",
    "https://openalex.org/W2098958400",
    "https://openalex.org/W2164777277",
    "https://openalex.org/W4386070832",
    "https://openalex.org/W2094267229",
    "https://openalex.org/W4308906267",
    "https://openalex.org/W3209797242",
    "https://openalex.org/W4304172680",
    "https://openalex.org/W4394845619",
    "https://openalex.org/W4399049950"
  ],
  "abstract": null,
  "full_text": "npj |digital medicine Article\nPublished in partnership with Seoul National University Bundang Hospital\nhttps://doi.org/10.1038/s41746-025-01985-5\nEvaluating the performance of general\npurpose large language models in\nidentifying human facial emotions\nCheck for updates\nBenjamin W. Nelson1,2 , Ari Winbush3,S t e v e nS i d d a l s1, Matthew Flathers1, Nicholas B. Allen3,4 &\nJohn Torous1,4\nWe evaluated the ability of three leading LLMs (GPT-4o, Gemini 2.0 Experimental, and Claude 3.5\nSonnet) to recognize human facial expression using the NimStim dataset. GPT and Gemini matched or\nexceeded human performance, especially for calm/neutral and surprise. All models showed strong\nagreement with ground truth, though fear was often misclassiﬁed. Findings underscore the growing\nsocioemotional competence of LLMs and their potential for healthcare applications.\nGenerative artiﬁcial intelligence (GenAI) based on large language models\n(LLMs) is becoming central to human– computer interactions (HCIs),\ndemonstrating impressive capabilities in interpreting human intentions1,2\nand understanding human cognitive, social, and emotional processes. Facial\nexpressions are a key aspect of social– emotional functioning and provide\nvaluable information about human goals, emotions, and psychological states3.\nLLMs have expanded their capabilities beyond traditional text-based\ntasks, enabling them to process and integrate multimodal inputs such as\nvision, speech, and text. They have shown promise in social cognition such\nas “theory of mind” tasks, sometimes matching or exceeding human per-\nformance on mentalistic inference\n2. However, these results are largely based\non text-only examples and are less robust in assessments where context is\ncritical\n2,4,5. Studies evaluating LLMs' visual emotion recognition have mixed\nresults, with some models performing no better than chance6.\nGenAI’s ability to interpret facial expressions holds promise for HCI\napplications, particularly in behavioral healthcare7– 9. Subtle expression\nchanges may indicate mental health conditions like depression, anxiety, or\neven suicidal ideation\n10,11. AI-powered systems trained to recognize these\nnuanced expressions could potentiallyenable earlier diagnosis, real-time\nmonitoring, and adaptive interventions.\nFacial expressions and interpretation can vary by culture12 and\ncontext13, highlighting the importance of using diverse stimuli with vali-\ndated ground truth labels and normative human performance data.\nMoreover, the need to evaluate performance across diverse actors (i.e., sex/\nracial/ethnicity) is well recognized\n6,14.\nResults\nAgreement\nCohen’sK a p p a(κ) across all stimuli and expressions was 0.83 (95% CI:\n0.80– 0.85) for ChatGPT 4o, 0.81 (95% CI: 0.77– 0.84) for Gemini 2.0\nExperimental, and 0.70 (95% CI: 0.67– 0.74) for Claude 3.5 Sonnet. Speciﬁc\nKappas by emotion class can be found in Table1 and Fig.1b.\nConfusion matrix\nOverall accuracy across all actors and expressions was 86% (95% CI:\n84– 89%) for ChatGPT 4o, 0.84% (95% CI: 81– 87%) for Gemini 2.0\nExperimental, and 74% (95% CI: 71– 78%) for Claude 3.5 Sonnet. Accuracy\nby emotion class can be found in Table1 and Fig.1a. For ChatGPT 4o and\nGemini 2.0 Experimental, there was little variability in the performance\nacross different emotion categories, except for fear, which was misclassiﬁed\nas surprise 52.50% and 36.25% of the time, respectively (see Figs.2a-b\nand 3a-b). For Claude 3.5 Sonnet, there was more variability in the per-\nformance across different emotioncategories with sadness being mis-\nclassiﬁed as disgust 20.24% of the time and fear being misclassiﬁed as\nsurprise 36.25% of the time (see Figs.2ca n d3c).\nLastly, there were no signiﬁcant differences in model performance for\naccuracy, recall, or kappa based on the sex or race of the actor (see Table2).\nDiscussion\nThis study evaluated three leading LLMs, ChatGPT 4o, Gemini 2.0\nExperimental, and Claude 3.5 Sonnet,on facial emotion recognition using\nthe NimStim dataset. ChatGPT 4o and Gemini 2.0 Experimental demon-\nstrated“almost perfect”\n15,16 agreement and high accuracy with ground truth\nlabels overall, with ChatGPT 4o and Gemini 2.0 Experimental performance\ncomparable to or exceeding human raters on some emotions. Claude 3.5\nSonnet exhibited lower overall agreement and accuracy as compared to the\nother two models.\nThere was signiﬁcant variability in Cohen’s Kappa and Recall within\nand between emotion classes. All models performed relatively well on\nHappy, Calm/Neutral, andSurprise, but showed difﬁculty recognizing Fear,\n1Division of Digital Psychiatry, Department of Psychiatry, Beth Israel Deaconess Medical Center, Harvard Medical School, Boston, MA, USA.2Verily Life Sciences,\nSan Francisco, CA, USA.3University of Oregon, Eugene, OR, USA.4These authors contributed equally: Nicholas B. Allen, John Torous.\ne-mail: bnelson9@bidmc.harvard.edu\nnpj Digital Medicine|           (2025) 8:615 1\n1234567890():,;\n1234567890():,;\noften misclassifying it as Surprise. ChatGPT 4o achieved the best perfor-\nmance across emotions and signiﬁcantly outperformed Claude 3.5 Sonnet\non several emotions, including Calm/Neutral, Sad, Disgust, and Surprise.\nGemini 2.0 Experimental also outperformed Claude 3.5 Sonnet for Calm/\nNeutral, Disgust, and Surprise. When comparing these models’ perfor-\nmance to human observers in the NimStim dataset, the overall 95% con-\nﬁdence intervals for kappa overlapped for humans, ChatGPT, and Gemini,\nindicating similar levels of reliabilityacross all emotion categories. In con-\ntrast, Claude’s 95% CI did not overlap with that of humans, suggesting lower\noverall reliability. At the level of individual model-by-emotion comparisons,\nmost 95% CIs overlapped; however, three exceptions emerged such that\nChatGPT 4o showed higher reliability than humans for Surprise and Calm/\nNeutral, Gemini 2.0 Experimental outperformed humans for Surprise, and\nClaude 3.5 Sonnet was less reliable than humans for Calm/Neutral.\nLiterature has previously shown LLM biases, but currentﬁndings\nindicate that facial emotion recognition did not differ by sex or race. Fur-\nthermore, prior CNN models on this dataset achieved moderate classiﬁ-\ncation performance (42% accuracy overall, with large emotion-speciﬁc\nvariability\n17. In contrast, zero-shot vision-language models without train-\ning, ﬁne-tuning, or architectural customization may offer stronger\ngeneralization.\nAlthough these ﬁndings show promise for foundation models in\naffective computing, limitations remain. All stimuli featured static\nimages18,a c t o r sa g e d2 1– 30, and most images were European Amer-\nican, which may limit generalizability. The context of verbal signals can\nmodify facial expression meaning, highlighting the need for future\nmultimodal emotion classi ﬁcation with auditory stimuli 19.\nFurthermore, although we selectedthe NimStim dataset because it is\naccessible only to researchers upon request and has not appeared in\nLLM publications, thereby minimizing the likelihood it was included in\nmodel training and positively biasing results, relying on a single dataset\nmay limit the generalizability of ourﬁndings. While we tested three\ngeneral-purpose models, specialized large models designed for facial\nexpression and micro-expression recognition (e.g., ExpLLM, MELLM)\na r ea l s oa v a i l a b l e .F u t u r er e s e a r c hs h o u l de v a l u a t et h e s em o d e l so nt h i s\ndataset to compare their perform ance with general LLMs. Prompt\nwording varied slightly across models due to interface constraints,\npotentially affecting results. Speciﬁc healthcare applications may want\nto ﬁne-tune models or incorporate the Facial Action Coding System\ninto retrieval-augmented generat ion frameworks to improve recog-\nnition of more subtle or complex e motions, such as fear. Under-\nstanding when and why models succeed or fail will be critical for\nguiding responsible integration. Futu r er e s e a r c hs h o u l de v a l u a t eo p e n -\nweight models like Llama or DeepSeek, which can support more\ntransparent evaluation, local deployment, and stronger privacy pro-\ntections, important model considerations for clinical applications.\nOverall, this study provides an initial benchmark for evaluating LLMs’\nsocioemotional capabilities. Although ChatGPT and Gemini demonstrated\nreliability comparable to human observers across emotion categories, cau-\ntion is warranted when translating theseﬁndings and using general-purpose\nLLMs in applied settings, as Claude, by contrast, showed lower overall\nreliability. Further testing with ecologically valid, multimodal, and demo-\ngraphically diverse stimuli is essential to understand their limitations and\npotential.\nTable 1 | Description of validity ratings for LLM emotional expression estimates across all emotions\nLLM Emotion Cohen ’s Kappa\n(95% CI)\nAccuracy\n(95% CI)\nRecall\n(95% CI)\nPrecision\n(95% CI)\nF1\n(95% CI)\nChatGPT 4o Overall 0.83 (0.80 – 0.85) 0.86 (0.84 – 0.89) 0.85 (0.82 – 0.87) 0.86 (0.83 – 0.88) 0.83 (0.80 – 0.85)\nAngry 0.88 (0.82 – 0.93) 0.88 (0.81 – 0.95) 0.94 (0.88 – 0.99) 0.91 (0.87 – 0.95)\nCalm/Neutral 0.96 (0.95 – 0.99) 0.98 (0.96 – 1.00) 0.95 (0.92 – 0.98) 0.97 (0.95 – 0.98)\nDisgust 0.85 (0.78 – 0.92) 0.85 (0.78 – 0.93) 0.91 (0.84 – 0.97) 0.88 (0.83 – 0.93)\nFear 0.45 (0.36 – 0.54) 0.42 (0.32 – 0.53) 0.87 (0.77 – 0.98) 0.57 (0.47 – 0.67)\nHappy 0.93 (0.90 – 0.96) 0.93 (0.88 – 0.97) 1.00 (1.00 – 1.00) 0.96 (0.94 – 0.99)\nSad 0.84 (0.78 – 0.91) 0.87 (0.8 – 0.94) 0.88 (0.81 – 0.95) 0.87 (0.82 – 0.92)\nSurprise 0.91 (0.89 – 0.93) 1.00 (1.00 – 1.00) 0.45 (0.35 – 0.55) 0.62 (0.53 – 0.71)\nGemini\nExperimental 2.0\nOverall 0.81 (0.77 – 0.84) 0.84 (0.81 – 0.87) 0.83 (0.8 – 0.85) 0.84 (0.82 – 0.87) 0.81 (0.79 – 0.84)\nAngry 0.76 (0.69 – 0.84) 0.76 (0.67 – 0.85) 0.93 (0.87 – 0.99) 0.84 (0.78 – 0.9)\nCalm/Neutral 0.90 (0.87 – 0.93) 0.95 (0.91 – 0.98) 0.86 (0.82 – 0.91) 0.9 (0.87 – 0.93)\nDisgust 0.87 (0.82 – 0.93) 0.89 (0.82 – 0.96) 0.85 (0.77 – 0.92) 0.87 (0.82 – 0.92)\nFear 0.58 (0.49 – 0.69) 0.56 (0.45 – 0.67) 1 (1 – 1) 0.72 (0.63 – 0.81)\nHappy 0.94 (0.91 – 0.98) 0.94 (0.89 – 0.98) 1 (1 – 1) 0.97 (0.94 – 0.99)\nSad 0.68 (0.60 – 0.77) 0.7 (0.6 – 0.8) 0.79 (0.69 – 0.88) 0.74 (0.67 – 0.81)\nSurprise 0.91 (0.89 – 0.94) 0.98 (0.93 – 1) 0.48 (0.38 – 0.58) 0.64 (0.55 – 0.73)\nClaude 3.5 Sonnet Overall 0.70 (0.67 – 0.74) 0.74 (0.71 – 0.78) 0.74 (0.7 – 0.77) 0.72 (0.69 – 0.75) 0.71 (0.69 – 0.74)\nAngry 0.86 (0.81 – 0.91) 0.88 (0.81 – 0.95) 0.83 (0.76 – 0.91) 0.86 (0.81 – 0.91)\nCalm/Neutral 0.69 (0.64 – 0.74) 0.71 (0.64 – 0.78) 0.92 (0.88 – 0.97) 0.8 (0.76 – 0.85)\nDisgust 0.70 (0.63 – 0.77) 0.73 (0.64 – 0.83) 0.72 (0.63 – 0.82) 0.73 (0.66 – 0.8)\nFear 0.52 (0.44 – 0.61) 0.54 (0.43 – 0.65) 0.62 (0.51 – 0.74) 0.58 (0.5 – 0.66)\nHappy 0.88 (0.85 – 0.91) 0.91 (0.85 – 0.96) 0.88 (0.83 – 0.94) 0.89 (0.86 – 0.93)\nSad 0.53 (0.45 – 0.63) 0.58 (0.48 – 0.69) 0.65 (0.55 – 0.76) 0.62 (0.54 – 0.69)\nSurprise 0.72 (0.63 – 0.81) 0.82 (0.71 – 0.93) 0.39 (0.29 – 0.49) 0.53 (0.44 – 0.62)\nCohen’s Kappa= The agreement between ground truth and estimation; Accuracy= The proportion of correctly classiﬁed samples out of the total dataset; Recall= The fraction of samples belonging to a\ngiven emotion that the model correctly identiﬁes as that emotion; Precision= The fraction of samples predicted as a given emotion that truly belong to that emotion; F1= The harmonic mean of precision and\nrecall, providing a single measure that balances both. At the class level, accuracy and recall are identical, therefore accuracy is only calculated across all classes overall.\nhttps://doi.org/10.1038/s41746-025-01985-5 Article\nnpj Digital Medicine|           (2025) 8:615 2\nMethods\nStudy design\nThe current study was IRB-exempt from Beth Israel Deaconess Medical\nCenter (2025P000198).\nFacial expression stimuli. The NimStim, a large multiracial image\ndataset, was used as facial expression stimuli15.T h eN i m S t i mS e to f\nFacial Expressions is a comprehe nsive collection of 672 images\ndepicting facial expressions posed by 43 professional actors (18\nfemale, 25 male) aged between 21 and 30 years. The actors represent\ndiverse racial backgrounds, including African-American (10 actors),\nAsian-American (6 actors), European-American (25 actors), and\nLatino-American (2 actors). Each actor portrays eight distinct emo-\ntional expressions: neutral, happy, sad, angry, surprised, fearful,\ndisgusted, and calm. Psychometric evaluations with naive observers\nhave demonstrated a high proportion correct at 0.81 (SD = 0.19; 95%\nCI: 0.77– 0.85), high agreement between raters (kappa = 0.79, SD =\n0.17; 95% CI = 0.75– 0.83), and high test-retest reliability at 0.84\n(SD = 0.08; 95% CI: 0.82– 0.86)\n15. This dataset has been extensively\nutilized in various research studies with over 2000 citations20– 23.T h e\nauthors have obtained written consent to publish images of models\n#01, 03, 18, 21, 28, 40, and 45.\nFig. 1 | LLM model. aAgreement with NimStim human performance benchmark andb overall accuracy and recall by emotion class. Pink = NimStim Benchmark;\nBlue = ChatGPT 4o; Green = Gemini 2.0 Experimental; Red = Claude 3.5 Sonnet.\nhttps://doi.org/10.1038/s41746-025-01985-5 Article\nnpj Digital Medicine|           (2025) 8:615 3\nThe NimStim dataset provides an independent benchmark, as it is\nproprietary and restricted to authorized research institutions through\nlicensing agreements that explicitly prohibit public distribution. Our ver-\niﬁcation process, including extensive web searches, found no public\navailability of the NimStim data, suggesting it was unlikely to have been\nincluded in LLM training datasets. NimStim calm and neutral expressions\nwere recoded ascalm_neutral, consistent with Tottenham et al.\n15,w h on o t e d\nminimal perceptual differences between the two and treated either label as\ncorrect. Results separating calm and neutral are provided in the Supple-\nmentary Table 2.\nL a r g el a n g u a g em o d e l s\nOpenAI GPT-4o Google Gemini 2.0 Experimental, and Anthropic Claude\n3.5 Sonnet were used for facial expression recognition.\nProcedures. All NimStim 672 images were individually uploaded twice to\neach LLM model for facial emotion processing using the user-facing interface,\nrather than the API, due to the fact that at the time of testing, only OpenAI\noffered the ability to batch multiple image inputs through the API for the\nselected models. Standardizing the methodology with the user interface\nensured that the model’s response remained grounded in the initial\ninstruction. Prompts varied slightly across LLM models due to initial model\nresponses indicating an inability to follow the prompt, likely due to built-in\nconstraints and safety barriers (see Supplementary Table 1).\nAnalyses\nAll analyses were conducted with R v 4.3.1.\nAgreement. We assessed agreement between each LLM model output and\nthe ground truth label by calculating a stratiﬁed bootstrap analysis of Cohen’s\nkappa (κ), to address repeated measures within participants and imbalances\nin emotion categories via oversampling. For each of 1000 bootstrap iterations,\nparticipants were sampled with replacement, and within-participant emotion\ncategories were balanced via oversampling. We report meanκ and 95%\nconﬁdence intervals and interpreted agreement using standard thresholds\n(moderate: 0.4– 0.6, substantial: 0.6– 0.8, and almost perfect:≥0.815,16.W e\napplied the same oversampling bootstrap method to calculateκ for emotion\nclass, sex, and race categories separately. Finally, we benchmarked model\nperformance againstκ values reported in the NimStim dataset by comparing\n95% conﬁdence interval overlap15.\nConfusion matrix, accuracy, recall, precision, and F1. To evaluate the\nclassiﬁcation performance of each LLM, we computed confusion\nmatrices and derived standard metrics including accuracy, precision,\nrecall, and F1-score for each model across emotion categories. The matrix\nquantiﬁes the performance of the classiﬁcation model by showing the\ncount of samples for each combination of actual and predicted emotions,\nas well as the corresponding row and column totals to reﬂect the total\noccurrences of each actual emotion across the dataset, the number of\ntimes each emotion was predicted by the model represented by the\ndiagonal elements, and a grand total representing the overall number of\nsamples in the analysis. Note that the per-class balanced accuracy was\nequivalent to recall, a common metric in multi-class classi ﬁcation.\nMetrics were calculated per class and overall, with 95% conﬁdence\nintervals estimated.κ and accuracy were also stratiﬁed by sex and race.\nMethods of model comparison to NimStim\nWe benchmarked the performance of LLM models against theκ reported\nfor untrained human observers in the NimStim dataset. However, it is\nimportant to note that the original authors did not specify how they cal-\nculatedκ. Tottenham et al.\n15 presentedκ f o re a c he m o t i o nb ym o u t hs t a t eo f\nmouth open and closed. To obtain a singleκ estimate per emotion category\nto allow for comparability to results in the current study, we aggregatedκ\nfrom the two mouth-states. First,κ and their associated standard deviations\n(SD) were extracted separately for mouth open and closed. The meanκ for\neach emotion was computed as the arithmetic average of theκ values from\nmouth-states. To account for variability across mouth-state conditions, we\ncalculated the pooled SD using the square root of the mean of squared SD\nFig. 2 | Confusion matrix. aChatGPT 4o,b Gemini 2.0 Experimental, andc Claude\n3.5 Sonnet.\nhttps://doi.org/10.1038/s41746-025-01985-5 Article\nnpj Digital Medicine|           (2025) 8:615 4\nFig. 3 | Alluvial plot. aChatGPT 4o,b Gemini 2.0 Experimental,c Claude 3.5 Sonnet. Left column in ground truth and right column is model.\nTable 2 | LLM agreement by sex and race\nModel Variable Subgroup Cohen ’s Kappa (95% CI) Accuracy (95% CI)\nChatG-\nPT 4o\nSex Male 0.82 (0.79 – 0.85) 0.86 (0.83 – 0.9)\nFemale 0.83 (0.78 – 0.89) 0.87 (0.83 – 0.91)\nRace European\nAmerican\n0.81 (0.77– 0.85) 0.85 (0.81 – 0.88)\nAfrican American 0.85 (0.79 – 0.90) 0.88 (0.83 – 0.93)\nAsian American 0.87 (0.82 – 0.92) 0.91 (0.86 – 0.97)\nLatino American 0.83 (0.67 – 0.96) 0.87 (0.75 – 0.99)\nGemini\n2.0\nExperi-\nmental\nSex Male 0.79 (0.75 – 0.83) 0.83 (0.8 – 0.87)\nFemale 0.82 (0.77 – 0.87) 0.85 (0.81 – 0.89)\nRace European\nAmerican\n0.81 (0.77– 0.86) 0.84 (0.8 – 0.87)\nAfrican American 0.78 (0.73 – 0.84) 0.83 (0.77 – 0.89)\nAsian American 0.82 (0.71 – 0.92) 0.87 (0.8 – 0.94)\nLatino American 0.81 (0.58 – 0.96) 0.84 (0.71 – 0.97)\nClaude\n3.5\nSonnet\nSex Male 0.72 (0.67 – 0.77) 0.77 (0.73 – 0.81)\nFemale 0.67 (0.62 – 0.72) 0.71 (0.66 – 0.76)\nRace European\nAmerican\n0.67 (0.62– 0.72) 0.72 (0.67 – 0.76)\nAfrican American 0.72 (0.65 – 0.80) 0.77 (0.7 – 0.83)\nAsian American 0.76 (0.65 – 0.85) 0.8 (0.72 – 0.88)\nLatino American 0.75 (0.62 – 0.88) 0.74 (0.59 – 0.9)\nhttps://doi.org/10.1038/s41746-025-01985-5 Article\nnpj Digital Medicine|           (2025) 8:615 5\nvalues, ensuring equal weighting across conditions. This approach provided\na single, representative estimate ofκ for each emotion while preserving the\ncontributions from both facial conﬁgurations. Finally, to determine if the\nLLM models performed similarly, we assessed whether the 95% conﬁdence\nintervals of theseκ values overlap, indicating comparable (or different) levels\nof agreement.\nData availability\nThe NimStim data is available to researchers upon request athttps://danlab.\npsychology.columbia.edu/content/nimstim-set-facial-expressions.\nCode availability\nAll code is available on Open Science Framework athttps://osf.io/dhkuy/.\nReceived: 13 May 2025; Accepted: 30 August 2025;\nReferences\n1. Kosinski, M. Evaluating large language models in theory of mind\ntasks. Proc. Natl Acad. Sci. USA121, e2405460121 (2024).\n2. Strachan, J. W. A. et al. Testing theory of mind in large language\nmodels and humans.Nat. Hum. Behav.8, 1285– 1295 (2024).\n3. Cohn, J. F. Foundations of human computing: facial expression and\nemotion. InProc. 8th International Conference on Multimodal\nInterfaces 233– 238 (ACM, 2006).\n4. Ullman, T. Large language models fail on trivial alterations to theory-\nof-mind tasks. Preprint athttps://doi.org/10.48550/ARXIV.2302.\n08399 (2023).\n5. Refoua, E. et al. The Next Frontier in Mindreading? Assessing\nGenerative Artiﬁcial Intelligence (GAI)’s Social-Cognitive Capabilities\nusing Dynamic Audiovisual Stimuli.Comput. Hum. Behav. Rep.\n100702 (2025).\n6. Elyoseph, Z. et al. Capacity of generative AI to interpret human\nemotions from visual and textual data: pilot evaluation study.JMIR\nMent. Health11, e54369 (2024).\n7. Feuerriegel, S. et al. Using natural language processing to analyse text\ndata in behavioural science.Nat. Rev. Psychol.https://doi.org/10.\n1038/s44159-024-00392-z (2025).\n8. Stade, E. C. et al. Large language models could change the future of\nbehavioral healthcare: a proposal for responsible development and\nevaluation. Npj Ment. Health Res.3, 12 (2024).\n9. Meskó, B. The impact of multimodal large language models on health\ncare’s future.J. Med. Internet Res.25, e52865 (2023).\n10. Laksana, E., Baltrusaitis, T., Morency, L.-P. & Pestian, J. P.\nInvestigating facial behavior indicators of suicidal ideation. In2017\n12th IEEE International Conference on Automatic Face & Gesture\nRecognition (FG 2017)770– 777 (IEEE, 2017).\n11. Girard, J. M. & Cohn, J. F. Automated audiovisual depression analysis.\nCurr. Opin. Psychol.4,7 5– 79 (2015).\n12. Chen, C. et al. Cultural facial expressions dynamically convey emotion\ncategory and intensity information.Curr. Biol.34, 213– 223.e5 (2024).\n13. Durán, J. I. & Fernández-Dols, J.-M. Do emotions result in their predicted\nfacial expressions? A meta-analysis of studies on the co-occurrence of\nexpression and emotion.Emotion21,1 5 5 0–1569 (2021).\n14. Ferrer, X., Nuenen, T. V., Such, J. M., Cote, M. & Criado, N. Bias and\ndiscrimination in AI: a cross-disciplinary perspective.IEEE Technol.\nSoc. Mag.40,7 2– 80 (2021).\n15. Tottenham, N. et al. The NimStim set of facial expressions: Judgments\nfrom untrained research participants.Psychiatry Res.168, 242– 249\n(2009).\n16. Landis, J. R. & Koch, G. G. The measurement of observer agreement\nfor categorical data.Biometrics 33, 159– 174 (1977).\n17. Sannasi, M. V., Kyritsis, M. & Gray, K. L. H.What Does A Typical CNN\n“See” In An Emotional Facial Image?https://doi.org/10.11159/\nmvml23.114 (2023).\n18. Arsalidou, M., Morris, D. & Taylor, M. J. Converging evidence for the\nadvantage of dynamic facial expressions.Brain Topogr.24, 149– 163\n(2011).\n19. Tang, G., Xie, Y., Li, K., Liang, R. & Zhao, L. Multimodal emotion\nrecognition from facial expression and speech based on feature\nfusion. Multimed. Tools Appl.82, 16359– 16373 (2023).\n20. Dawel, A., Miller, E. J., Horsburgh, A. & Ford, P. A systematic survey of\nface stimuli used in psychological research 2000– 2020. Behav. Res.\nMethods 54, 1889– 1901 (2021).\n21. Manelis, A. et al. Working memory updating in individuals with bipolar and\nunipolar depression: fMRI study.Transl. Psychiatry12, 441 (2022).\n22. Fan, X. et al. Brain mechanisms underlying the emotion processing bias in\ntreatment-resistant depression.Nat. Ment. Health2,5 8 3–592 (2024).\n23. Martens, M. A. G. et al. Acute neural effects of the mood stabiliser\nlamotrigine on emotional processing in healthy volunteers: a\nrandomised control trial.Transl. Psychiatry14, 211 (2024).\nAcknowledgements\nNo funding was granted for this study. We would like to thank Dr. Nim\nTottenham for providing access to the NimStim dataset for research purposes.\nAuthor contributions\nB.W.N. conceptualized and drafted the manuscript. A.W. performed\nanalyses. M.F. and S.S. edited the manuscript. N.A. and J.T. edited and\nreviewed the manuscript.\nCompeting interests\nA.W., S.S., M.F. and J.T. declare no competing interests. B.W.N. is\nemployed and has equity ownership in Verily Life Sciences. N.A. is employed\nand has equity ownership in Ksana Health.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41746-025-01985-5\n.\nCorrespondenceand requests for materials should be addressed to\nBenjamin W. Nelson.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution-NonCommercial-NoDerivatives 4.0 International License,\nwhich permits any non-commercial use, sharing, distribution and\nreproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the Creative\nCommons licence, and indicate if you modiﬁed the licensed material. You\ndo not have permission under this licence to share adapted material\nderived from this article or parts of it. The images or other third party\nmaterial in this article are included in the article’s Creative Commons\nlicence, unless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted use,\nyou will need to obtain permission directly from the copyright holder. To\nview a copy of this licence, visithttp://creativecommons.org/licenses/by-\nnc-nd/4.0/\n.\n© The Author(s) 2025\nhttps://doi.org/10.1038/s41746-025-01985-5 Article\nnpj Digital Medicine|           (2025) 8:615 6",
  "topic": null,
  "concepts": [],
  "institutions": [
    {
      "id": "https://openalex.org/I1316535847",
      "name": "Beth Israel Deaconess Medical Center",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I136199984",
      "name": "Harvard University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I181233156",
      "name": "University of Oregon",
      "country": "US"
    }
  ]
}