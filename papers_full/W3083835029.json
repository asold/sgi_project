{
  "title": "Generative Language Modeling for Automated Theorem Proving",
  "url": "https://openalex.org/W3083835029",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4226591321",
      "name": "Polu, Stanislas",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226591326",
      "name": "Sutskever, Ilya",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2962800603",
    "https://openalex.org/W2946011656",
    "https://openalex.org/W3034144525",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3126568801",
    "https://openalex.org/W2581336110",
    "https://openalex.org/W2102113734",
    "https://openalex.org/W2964161785",
    "https://openalex.org/W2981030070",
    "https://openalex.org/W2807324060",
    "https://openalex.org/W2982316857",
    "https://openalex.org/W2945576559",
    "https://openalex.org/W2912007050",
    "https://openalex.org/W2994278562",
    "https://openalex.org/W2951122437",
    "https://openalex.org/W2962905782",
    "https://openalex.org/W3133204645",
    "https://openalex.org/W2996037775",
    "https://openalex.org/W2772709170",
    "https://openalex.org/W2509751917",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2945720633",
    "https://openalex.org/W2962770929",
    "https://openalex.org/W2549482841",
    "https://openalex.org/W2996380287",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2257979135",
    "https://openalex.org/W2618097077",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W2193413348",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W2951939640",
    "https://openalex.org/W2786776430",
    "https://openalex.org/W2173520492",
    "https://openalex.org/W2963938535",
    "https://openalex.org/W2766527293",
    "https://openalex.org/W2964308564"
  ],
  "abstract": "We explore the application of transformer-based language models to automated theorem proving. This work is motivated by the possibility that a major limitation of automated theorem provers compared to humans -- the generation of original mathematical terms -- might be addressable via generation from language models. We present an automated prover and proof assistant, GPT-f, for the Metamath formalization language, and analyze its performance. GPT-f found new short proofs that were accepted into the main Metamath library, which is to our knowledge, the first time a deep-learning based system has contributed proofs that were adopted by a formal mathematics community.",
  "full_text": "Generative Language Modeling for Automated\nTheorem Proving\nStanislas Polu\nOpenAI\nspolu@openai.com\nIlya Sutskever\nOpenAI\nilyasu@openai.com\nAbstract\nWe explore the application of transformer-based language models to automated\ntheorem proving. This work is motivated by the possibility that a major limitation\nof automated theorem provers compared to humans – the generation of original\nmathematical terms – might be addressable via generation from language models.\nWe present an automated prover and proof assistant, GPT-f, for the Metamath\nformalization language, and analyze its performance.GPT-f found new short proofs\nthat were accepted into the main Metamath library, which is to our knowledge, the\nﬁrst time a deep learning based system has contributed proofs that were adopted by\na formal mathematics community.\n1 Introduction\nArtiﬁcial neural networks have enjoyed a spectacularly successful decade, having made considerable\nadvances in computer vision [1, 2], translation [3, 4, 5], speech recognition [6, 7], image generation\n[8, 9, 10, 11, 12], game playing [13, 14, 15], and robotics [16, 17]. Especially notable is the recent\nrapid progress in language understanding and generation capabilities [18, 19, 20, 21, 22].\nWith the possible exception of AlphaGo [13] and AlphaZero [23], reasoning tasks are conspicuously\nabsent from the list above. In this work we take a step towards addressing this absence by applying a\ntransformer language model to automated theorem proving.\nAutomated theorem proving [24] is an appealing domain for exploring reasoning in general and the\nreasoning capabilities of language models in particular for several reasons:\n• Reasoning-complete: Proving theorems very likely require general and ﬂexible reasoning;\nthus an advance in theorem proving is also an advance in reasoning more broadly.\n• Search: Automated theorem proving systems can quickly check the correctness of proofs,\nmaking it a productive environment for the use and development of search methods.\n• Automated data generation: The ability to verify proofs makes it possible to automatically\ngenerate new problems that could then be used as training data. This is especially important,\nsince collecting high quality data for reasoning tasks can be difﬁcult.\nLearning to prove theorems is somewhat analogous to learning to play Go: both offer an automated\nway of determining success (the game of Go is a miniature formal system), and both offer an\nautomated way for generating new data via self play-type approaches. This similarity, together with\nthe clear success of AlphaZero, suggests that automated theorem proving might prove to be a fruitful\ndomain for the study of reasoning in neural networks where signiﬁcant progress may be possible.\nPreprint. Under review.\narXiv:2009.03393v1  [cs.LG]  7 Sep 2020\n1.1 Contribution\nOur contributions are the following:\n• We verify that generative pre-training substantially improves performance and that pre-\ntraining on mathematical data (such as arXiv) leads to better performance compared to\npre-training on generic text from the web.\n• We ﬁnd that model size is positively correlated with performance, even though the size of\nthe Metamath dataset is relatively small.\n• We demonstrate that iteratively training a value function on statements generated by our\nlanguage model leads to improved prover performance, which immediately suggests a\nstrategy for continuous self improvement: keep training on proofs generated by the prover.\n• We also achieve a new state of the art for the Metamath environment with our best model\ncapable of closing 56.22% of proofs from a held-out test set (vs 21.16% for the current\nstate of the art, MetaGen-IL [25]), demonstrating that the Transformer architecture may be\nsuitable to formal reasoning.\n2 Related Work\nDeep learning applied to premise selection and proof guidance Research on automated theorem\nproving dates back to the 50s [24], but mainstream proof assistants still suffer from combinatorial\nexplosion of their search space as they are scaled to large corpuses, motivating the use of deep learning.\nEarly applications of deep learning to formal mathematics focused primarily on premise selection and\nproof guidance. DeepMath [26] explored the use of CNNs and RNNs to predict whether a premise is\nuseful to demonstrate a given conjecture, their results were later improved with FormulaNet [27] by\nthe use of graph neural networks, reminiscent of NeuroSAT [28]. Proof guidance consists in selecting\nthe next clause to process inside an automated theorem prover. Loos et al. [29] investigated the use of\nmodels similar to DeepMath’s for proof guidance and demonstrated a signiﬁcant uplift on the Mizar\nlibrary.\nDeep learning applied to automated theorem-proving HOList [30] proposes a formal environ-\nment based on HOL Light. They achieve their best performance [31] with a GNN model designed\nfor premise selection and the use of exploration. More recently, the same team studied the use of\nthe BERT objective with Transformers on formal statements [ 32], demonstrating the potential of\nleveraging Transformers for formal reasoning. Their study focuses on preliminary tasks that are\nrelated but not directly consisting of proving formal theorems (such as typing and conjecturing).\nGamePad [33] and CoqGymn/ASTactic [34] introduce environments based on the Coq theorem\nprover. ASTactic generates tactics as programs by sequentially expanding a partial abstract syntax\ntree. Holophrasm [35] and MetaGen-IL [25] propose RNN-based models to generate proofs for\nMetamath (the formal system we focus on). They rely on three different models, one to value goals,\none to select premises and one to generate substitutions. MetaGen-IL also demonstrates an uplift in\nperformance by generating synthetic data by forward proving.\nUse of Transformers for symbolic tasks Several lines of work have been exploring language\nmodeling using Transformers [18]. Language modeling improvements have been demonstrated from\nbetter pre-training tasks, using various objectives such as auto-regressive generation [19, 20, 21], token\nmasking [22] or sequence masking [36], but resulting language models have so far felt short when\napplied to reasoning oriented tasks such as algebraic word problems [37, 38]. Recently, Lample and\nCharton [39] successfully applied Transformers to anti-derivative calculus and solving differential\nequations, hinting that Transformers are capable of generating the exogenous terms involved in\nthe substitutions required for successful symbolic integration. The Universal Transformer [ 40], a\nTransformer with tied weights, was also shown to be successful at more algorithmic tasks. Also,\nSaxton et al. [41] evaluated the Transformer architecture on a variety of mathematical problems.\n2\n3 Formal Environment\nWe chose Metamath [42] as our formal environment. Metamath is powered by a simple meta logic\nsystem based on a single substitution rule [43].\nThe main Metamath library is called set.mm, which is a collection of ∼38kproofs based on ZFC set\ntheory (while other formalisms can also be used on top of Metamath’s meta logic, they are not used\nin set.mm).\nMetamath has several advantages that make it convenient to use with neural networks:\n• Veriﬁcation is fast and can be implemented in several hundreds lines of code.\n• Proof steps are context-free: a goal or subgoal that we wish our system to prove, together\nwith a list of the statements of the theorems proven so far, completely deﬁne the state of the\nMetamath system at any stage of a proof. Other formal systems are generally wrapped in\nhigh-level programming languages that make them easier to use for humans (by including\nconvenient features like module imports or custom user-deﬁned tactics) but are harder to\nintegrate with a neural network. While proofs in such systems are generally shorter and\nmore human-readable, they are impacted by long-distance interactions which makes the\ncomplete description of the intermediary states of proofs longer, and therefore less suitable\nfor neural language models.\n• Access to clean and compact subgoal representations makes searching the proof tree rela-\ntively straightforward. It is not the case for systems where the proving objective resembles\nprogram synthesis more than an explicit proof tree search.\n• set.mm is one of the largest libraries available and its foundations are accepted as compatible\nwith modern mathematics.\nBut it also has a number of weaknesses:\n• Metamath does not have high-level tactics, which means that all of its proof steps are\nvery low-level. As an example, the de-bruijn factor [44]–the quotient of the size of a\nformalization of a mathematical text and the size of its informal original version– of a\nMetamath proof is ∼10 −20 while it is around ∼1 −3 in Coq, HOL Light or Lean. Lower\nlevel proof steps mean longer proofs with greater chance of compounding errors during\nsearch.\n• The current state of the tooling around Metamath makes it a very “DIY” system, one that is\nnot yet ready for broad adoption by the mathematics community.\nWhile our approach would be applicable to other formal systems (such as Lean, Coq, or HOL Light),\nMetamath’s features allow faster prototyping and reduced iteration time in the near term, which is\nwhy we chose it for this project.\nThe set.mm library contains the background theorems required to demonstrate most Olympiad or\nundergraduate Mathematics type of problems. For example, assisted by the GPT-f proof assistant\ndescribed in this work in section 6.2, we formalized IMO 1972 problem B21.\n3.1 Proving in Metamath\nProving in Metamath consists of applying a previously demonstrated theorem or axiom by providing a\nsubstitution of the variables appearing in the hypotheses and conclusion of the theorem being applied,\nsuch that the substituted conclusion uniﬁes to (which means that it \"matches\") the current goal which\nwe wish to prove. The substituted hypotheses, if any, become the new subgoals left to prove.\nThis mechanism, a proof step, can be used in a forward manner (where we start with the axioms\nand reach the desired statement, one proof step at a time) and a backward manner (where we start\nwith the statement we wish to prove and, after applying enough proof steps, end up at axioms or\npreviously demonstrated theorems with whose hypothesis we already determined to be true). As it is\nmore naturally amenable to proof search, we will be operating backward.\n1Metamath Proof Explorer - imo72b2http://us.metamath.org/mpeuni/imo72b2.html\n3\nAs an example, assume we want to prove ⊢(3 + 2) = 5using the deﬁnition of 4 and 5 as respective\nsuccessors of 3 and 4. As a ﬁrst step, we should use an equality transitivity theorem such as:\n[[\n|- A = B # first hypothesis\n|- C = B # second hypothesis\n]]\n|- A = C # conclusion\nTo apply the transitivity theorem, we need to provide a substitutions that substitutesAwith (3 + 2)\nand Bwith 5 such that the conclusion of the theorem uniﬁes to the current goal. We are left with\nproviding a substitution for Bwhich can hardly be discovered mechanically (hence the appeal to use\ngenerative language modeling). We can substitute Bwith (4 + 1)as is the case in the actual proof2\nin Metamath’sset.mm library.\nPutting it all together, the goal here is:\n|- ( 3 + 2 ) = 5\nThe proof step we apply:\n[[\n|- A = B # first hypothesis\n|- C = B # second hypothesis\n]]\n|- A = C # conclusion\n{{ A : ( 3 + 2 ) }} # substitution of A\n{{ B : ( 4 + 1 ) }} # substitution of B\n{{ C : 5 }} # substitution of C\nAnd ﬁnally the new subgoals are:\n|- ( 3 + 2 ) = ( 4 + 1 )\n|- ( 4 + 1 ) = 5\nApplying the following proof step with no hypothesis (the deﬁnition of 53) to the second subgoal\nallows us to prove it.\n[[ ]] |- ( 4 + 1 ) = 5\nNote that this proof step has no hypothesis and no substitution involved. It therefore closes that\nbranch of the proof tree. From there the proof can be continued with the ﬁrst subgoal, proving\nbackward, until no subgoal is left. Also note that a proof for a given theorem of the library can only\nuse theorems proven before the appearance of the theorem to prove; we enforce that constraint when\nbenchmarking our models despite them being trained on the library as a whole.\nIn most formal systems, a proof step, consists of a goal and a mechanism that, given a goal produces\nnew subgoals, generally referred to as a tactic. In Metamath, there is only one type of tactic based on\nsubstitution as illustrated above. Additionally since the substituted theorem must unify to the current\ngoal, the current goal can be deduced from the tactic itself (theorem and substitution pair), which is\nnot generally the case in other systems. As such, we’ll usetactic and proof stepinterchangeably in\nthe rest of the paper.\nThis informal presentation of Metamath is sufﬁcient to understand the objectives we use to train our\nmodels. A more formal deﬁnition of Metamath’s meta-logic can be found in the Metamath Book [42].\n3.2 Dataset\nMetamath’sset.mm uses a binary compressed format to represent proofs of statements. We process the\nlibrary and extract a dataset of proof steps, stored as JSON blobs using the representation presented\n2Metamath Proof Explorer - 3p2e5http://us.metamath.org/mpeuni/3p2e5.html\n3Metamath Proof Explorer - df-5http://us.metamath.org/mpeuni/df-5.html\n4\nabove. For each proof step we store a GOAL, a PROOFSTEP and a reference to the parent goal if any,\nencoding the tree structure of the proofs:\n{\n\"proof_label\": \"unidmrn\",\n\"goal\": \"[[ ]] |- U. U. ‘’ A = ( dom A u. ran A )\",\n\"proof_step\": \"[[ |- A = B |- C = B ]] |- A = C \\\\\n{{ A : U. U. ‘’ A }} \\\\\n{{ B : ( ran ‘’ A u. dom ‘’ A ) }} \\\\\n{{ C : ( dom A u. ran A ) }}\",\n\"proof_step_hash\": \"37yZVNorgF8=\",\n\"parent_hash\": [\"n4Kl7judEN4=\"]\n}\nThe dataset contains ∼3mof such proof steps for ∼38ktheorems (different proof labels). We split\nthat dataset between a train set and two valid and test sets each containing ∼1kproofs sampled\nrandomly (∼90kproof steps each).\n3.3 Glossary\nterm A string that comply to the Metamath grammar.\nstatement or proposi-\ntion\nA potentially empty set of hypotheses (terms) and a conclusion (term)\nentailed by the hypotheses.\ntheorem A proven statement.\naxiom An assumed statement.\ngoal A statement in the context of a proof search.\nsubstitutions A list of pairs of variables and terms (to substitute the variables within a\ntheorem or an axiom).\ntactic A theorem and substitutions that unify to a goal.\nsubgoals Goals generated by a tactic (the substituted hypotheses of the tactic’s\ntheorem).\nproof step A goal and a tactic, potentially generating new subgoals.\nproof A tree of goals and tactics whose root is the demonstrated theorem; leaves\nof the tree are tactics with no subgoals or goals that are hypotheses of the\nroot theorem.\n4 Model\n4.1 Architecture\nWe use decoder-only Transformers similar to GPT-2 [20] and GPT-3 [21]. The largest model we\nstudy has 36 layers and 774m trainable parameters.\n4.2 Training Objective\nThe proofstep objectivewe use for training is a conditional language modeling objective that is asked\nto generate the PROOFSTEP given a GOAL, which is directly applicable to proof searches. To do so,\nwe format our data in the following way:\nGOAL <GOAL> PROOFSTEP <PROOFSTEP><EOT>\nThere is one such objective for each JSON line in our dataset. We train with only one sentence per\ncontext (no-chunking), masking the rest of the context by assigning a loss weight wloss = 0. As we\ntrain we track the valid loss and sequence accuracy while masking the query part of the objective:\nGOAL <GOAL> PROOFSTEP\n5\nWe regularize the training by early-stopping at the point of minimum valid loss and applying a weight\ndecay wd = 0.1.\nHere is a randomly sampled context as presented to our models for training:\nGOAL [[ ]] |- ( ( ( J e. Nrm /\\ f e. ( J Homeo K ) ) /\\ ( x e. K /\\ y e. (\n( Clsd ‘ K ) i^i ~P x ) ) ) -> ( ‘’ f \" x ) e. J ) PROOFSTEP [[ |- ( ph ->\nps ) |- ( ph -> ch ) |- ( ( ps /\\ ch ) -> th ) ]] |- ( ph -> th ) {{ ch :\nx e. K }} {{ ph : ( ( J e. Nrm /\\ f e. ( J Homeo K ) ) /\\ ( x e. K /\\ y e.\n( ( Clsd ‘ K ) i^i ~P x ) ) ) }} {{ ps : f e. ( J Cn K ) }} {{ th : ( ‘’ f\n\" x ) e. J }} <|endoftext|>\n4.3 Proof Search\n4.3.1 Goal Expansion\nWe ﬁnd proofs by running proof searches. A proof search maintains a proof tree and a queue of open\ngoals sorted by their cumulative logprob, initialized with the root goal that we wish to demonstrate\n(see ﬁgure 1). The cumulative logprob of a goal is deﬁned by the sum of the logprobs of the tactics\nthat were used to reach that goal from the root goal. Intuitively we expand goals for which the\ngenerative model is the most conﬁdent globally. This has a tendency to explore breadth ﬁrst as deeper\ngoals have more parent tactics and therefore typically a higher cumulative logprob.\nEach time we expand an open goal we sample e = 32tactics (the proofstep objectivedescribed\nabove) from the model at temperature t = 1.0, deduplicate them, and apply the valid tactics (of\nwhich there are at most e) to the goal being expanded. Each successful tactic application generates\nnew subgoals that are added to the proof tree and the proof search queue. The expanded goal is then\nremoved from the queue. Note that the subgoals associated with a successfully applied tactic all\nshare the same cumulative logprob and will eventually be expanded together (as subgoals generated\nfrom their own expansion will mechanically have a higher cumulative logprob, and will therefore be\ninserted behind in the queue). We denote the process of selecting the minimal cumulative logprob\ngoal and expanding it as a proof search expansion.\nEach proof search involves d= 128goal expansions, so proofs generated have at most dproof steps.\nWhen evaluating our models, we attempt a proof search for each statement in the valid set a = 4\ntimes, starting from an empty proof tree each time. In the above, a, e, and dare hyperparameters of\nthe search process that we can vary to achieve better performance (at the cost of more compute), but\nkeep constant as we compare models.\n4.3.2 Formal Veriﬁer\nPerforming such proof searches requires to tightly couple a Metamath veriﬁer with our models. We\nimplemented a Metamath kernel in Python to avoid the performance cost and brittleness of interacting\nwith an external kernel over its REPL through standard I/O. It also provides us with a ﬂexible\nenvironment to experiment with new ideas in ways that were not anticipated by existing veriﬁers.\nThe kernel consists of a modiﬁed LR(0) parser to check that terms generated by our models comply\nwith the Metamath grammar, along with Goal and Tactic objects that implement the Metamath\nsubstitution and represent proof trees. Our implementation is capable of exporting our in-memory\nrepresentations to both our JSON marshalled format and the ofﬁcial set.mm proof format. The latter\nallows us to verify the proofs we generate with an external Metamath kernel implementation such as\nmmverify.py or metamath-exe.\nCollectively, this proof search procedure and the formal veriﬁer tied with it are what we refer to as\nthe GPT-f automated prover.\n4.4 Evaluation\nWe report the performance Perfvalid\na,e,d(θ) of a model θ, as the percentage of proofs found by this\nprocedure within the valid or test set. We evaluate our models on the valid set of ∼1ktheorems and\nonce at the end of this paper on the held-out test set.\n6\nFigure 1: Proof search consists in maintaining a proof tree where multiple tactics are explored for\neach goal, starting from the root goal. Goals are expanded by cumulative (tactic) logprob priority.\nThe hyperparameters we chose to evaluate our models attempt to minimize the variance in the\nevaluation process while using the least amount of compute. Decreasing the number of expansions\nper goal eincreases the variance as we less systematically explore the action space when expanding\neach goal. The evalue can be taken quite high as each auto-regressive proof step generation uses the\nsame query for a given goal and can therefore be batched together. Increasing etoo much may also\nhurt performance given the breadth ﬁrst nature of the cumulative logprob ordering. Increasing the\nnumber of attempts per proposition adecreases variance and consistently improves performance up\nto reasonably high values (We usea= 32attempts for our ﬁnal benchmarking). We found that a= 4\nlimited the variance in our evaluations while remaining tractable given the amount of compute we\nhad available. Finally the proof search depth dhas little impact on variance but naturally improves\nperformance (we take d= 128to evaluate our models and d= 256for our ﬁnal benchmarking).\nThe number of expansions we use per proof search may appear as relatively low, but it’s important\nto realize that it already requires a substantial amount of compute as each expansion consists in the\nauto-regressive generation of e = 32tactics (generally hundreds of tokens and therefore forward\npasses each). Empirically, the hyperparameters we chose, require on average around∼1kGPU.hours\n(with V100s) to evaluate our 700m parameters model (which leverages GPT-3’s sparse attention as\nwell as key-value caching).\n4.5 Pre-training\nWe study the effect of pre-training on the performance of our models. We pre-train our models on\nboth GPT-3’s post-processed version of CommonCrawl as well as a more reasoning-focused mix of\nGithub, arXiv and Math StackExchange.\n7\nGithub is downloaded using BigQuery4 and ﬁltered to only include deduplicated ﬁles from selected\nprogramming languages (excluding markdown, stylesheets, HTML). arXiv is downloaded using Bulk\nData Access5 and ﬁltered to only include articles labeled as Mathematics and whose LaTeX source\nis available. Math StackExchange is downloaded from their snapshot on the Internet Archive6 and\npost-processed to remove HTML tags and correlate questions and answers. We demote the mix\nreported in the table below as WebMath:\nTable 1: Mix and source of data involved in the WebMath dataset.\nDataset Size Mix\nGithub 23 GB 33%\narXiv Math 10 GB 33%\nMath StackExchange 2 GB 33%\n4.6 Synthetic Datasets\nDespite being among the largest formal mathematics libraries, the Metamath library remains scarce in\nthe context of deep learning, especially in light of the advantages demonstrated on various NLP tasks\nby pre-training on large corpora. Also set.mm mostly focuses on well-known high-level theorems and\ndoes not include a large number of technical lemmas resembling the type of mathematics exercises\nused as curriculum for humans. Finally, Metamath lacking high level tactics such as HOL Light’s\nARITH_RULE7, or Lean’sring8, it is critical to ensure that our models are capable of proving at least\nbasic technical theorems generally handled by high-level tactics in other systems (in domains such as\narithmetic or ring equalities and inequalities)\nTo achieve this goal we designed synthetic datasets allowing us to generate proofs for each of these\ndomains at will while controlling precisely by how many proofs we augment our training set.\nWe describe below the synthetic datasets we designed and report in section 5 the sample complexity\nassociated with these synthetic tasks.\n4.6.1 n-digit Arithmetic\nWe synthetically generate proofs for arithmetic formulas such as 11 ∗22 = 242by following the\nbasic algorithm for addition and multiplication, repeatedly applying theorems such as decadd9 or\ndecaddc10. Divisions and subtractions are translated to their equivalent additions and multiplications\ntheorems in one proof step. We also support generating modulos and exponentiations.\nWe accept one hyperparameter for these synthetic proof generators,ndigits which controls the number\nof digits involved in these arithmetic tasks. When generating a new proof we sample uniformly\nin [−10ndigits ,10ndigits ] each of the numbers involved. To illustrate the level at which Metamath\noperates, Table 2 shows the average number of proof steps generated as a function ofndigits for each\ngenerator. These statements are generally proved with one tactic application in other higher-level\nsystems, which is a good example of one of Metamath’s drawbacks we identiﬁed earlier.\nTable 2: Average number of proofsteps produced by our synthetic generators for ndigits = 3,9,18.\n3 9 18\nAddition (in Z) 19 48 94\nDivision 13 93 292\nModulo 25 82 206\nExponentiation 7 27 68\n4https://console.cloud.google.com/marketplace/details/github/github-repos\n5https://arxiv.com/help/bulk_data\n6https://archive.org/details/stackexchange\n7https://www.cl.cam.ac.uk/~jrh13/hol-light/HTML/ARITH_RULE.html\n8https://leanprover-community.github.io/mathlib_docs/algebra/ring/basic.html#ring\n9Metamath Proof Explorer - decaddhttp://us.metamath.org/mpeuni/decadd.html\n10Metamath Proof Explorer - decaddchttp://us.metamath.org/mpeuni/decaddc.html\n8\nOur goal is to leverage these synthetic generators to ensure our models are conﬁdent when faced with\nsuch subgoals in order to mitigate the large number of proof steps they require.\n4.6.2 Ring Algebra\nOur ring equalities generator is largely inspired by the INT inequality generator [45] . They propose an\ninequality generator that starts from simple formulas (such asA= A) and iteratively transforms them\ninto more complex equalities or inequalities using a predeﬁned list of axioms (such as commutativity\nof addition or distributivity of addition-multiplication). At each transformation, the axiom to be\napplied is chosen uniformly.\nOur generator operates similarly within the Metamath formalism based on theorems equivalent to the\naxioms they propose. We accept two hyperparameters, the number of variables nbvar involved in\nthe seed formulas (of the form A= A) as well as the number of theorems applied to transform the\nexpression, denoted as depth. In addition, we use hand-crafted weights as we sample theorems in\norder to obtain formulas that we judged qualitatively better.\nHere is a list of the theorems we use and their associated sampling weights.\nTable 3: Metamath theorems use by our Ring Algebra synthetic generators. Theorems are available\nin the Matmath Proof Explorer.\nTheorem Weight Description\neqcomd 1 Commutative law for class equality.\nint-addcomd 1 Addition commutativity.\nint-addassocd 1 Addition associativity.\nint-mulcomd 1 Multiplication commutativity.\nint-mulassocd 1 Multiplication associativity.\nint-leftdistd 3 Left distribution of multiplication over addition.\nint-rightdistd 3 Right distribution of multiplication over addition.\nint-sqdefd 5 Deﬁnition of the square.\nmuladdd2 5 Product of two sums\nExamples of equalities produced by the generator:\nABBA(AB)2 + (C+ A) =A+ (ABBA)2 + C\n(AA)2 = A2AA\n((BA+ CA)2)2 = (BA+ CA)2(BAAB+ ACCA + BAAC + ABCA)\n((A+ B)2)2(A+ A) = ((A+ B)2(AB+ AB+ AA+ BB) + (A+ B)2(AB+ AB+ AA+ BB))A\n4.6.3 Default augmented Dataset\nBy default in all of our experiments we add synthetically generated proofs to the dataset extracted from\nset.mm as shown in Table 4. We’ll denote this dataset as ouraugmented dataset. The synthetically\ngenerated proofs account for approximately 1% of our training data which empirically appeared\nas big enough to achieve decent performance on the tasks we cared about and small enough not to\nhurt performance on the valid set, especially for small models. We attempted scaling the portion of\nsynthetic proofs to 5% of the dataset and found out that it hurt performance for the model sizes we\nstudied. It is nonetheless possible that including more synthetic data may turn out to be beneﬁcial for\nlarger models than the ones studied in this paper.\n4.7 Learned Value Function\nTo achieve better performance, we also iteratively train a value function to guide the proof search, in\nplace of the cumulative logprob priority described above.\nWe implement the value function by means of an outcome objectiveas follows. Any time we attempt\nto prove a statement, we will generate a signiﬁcant number of intermediate goals. Some of these\n9\nTable 4: Number of proofs and proofsteps adjunct to constitute our augmented dataset.\nGenerator Number of Proofs Number of Proofsteps\n9-digit Addition (in Z) 100 4541\n9-digit Division 100 10047\n9-digit Modulo 50 4438\n9-digit Exponentiation 50 910\nRing Equalities (depth = 6, nbvar = 2) 50 1373\nRing Equalities (depth = 6, nbvar = 3) 50 1499\ngoals will lead to the proof, other goals will be proved without being part of the ﬁnal proof, while\nothers will not be resolved at all. To obtain a value function, we simply train our model to predict\nwhether a goal produced during proof search ended up being resolved by generating a new dataset of\nthe following form:\nGOAL <GOAL> OUTCOME <P|N><EOT>\nWhere a goal ends with a \"P\" if was resolved, and \"N\" otherwise.\nThe binary nature of the OUTCOME allows the deﬁnition of a provability function fP as the\nconditional probability of token P given a GOAL without having to introduce a separate value head.\nGiven a goal g, for a model parametrized by θ:\nfθ\nP(g) =pθ(\"P\"|g) ≈trained 1 −pθ(\"N\"|g)\nWe then deﬁne our value function V on goals with:\nVθ(g) =\n∏\ng′∈siblings(g)\nfθ\nP(g′)\nNot having to introduce a separate value head greatly simpliﬁes the overall architecture. Training\nonly involves augmenting the dataset with outcome objectives(as additional masked sentences) and\nsampling the \"provability\" function simply consists in reading the probability distribution for the\ntoken following the OUTCOME keyword (which can be done in one forward pass).\n4.7.1 Iterative Data Generation and Training\nHaving access to a formal veriﬁer enables us to generate the training data for fP in a fully synthetic\nmanner by ﬁrst training a model on the proofstep objective, then sampling proofs (using cumulative\nlogprob priority) for statements from the training set, and ﬁnally, annotating goals visited by the\nproof searches as positives if they were proved and as negatives otherwise.\nThese annotations are used to train fP and the entire process can be run iteratively, similarly toExpert\nIteration [46], sampling proofs using the newly trained V (instead of cumulative logprob) to guide\nproof search for subsequent iterations.\nAt each iteration we entirely re-train the model on both objectives at the same time on the dataset\nconstructed as follows:\n• We extract the full proofs that were found for statements from the training set at each\nprevious iteration and merge them with the original training set. We deduplicate proof steps\nat the proof level. This dataset becomes our new train set for the proofstep objective.\n• We extract the annotated goals visited by the proof searches for statements from the train set\nas well as the goals from the original train set (annotated positively) and deduplicate the\ngoals giving priority to positive outcomes annotations. This dataset becomes our new train\nset for the outcome objective.\nThis iterative training allows controlling for overﬁtting on both objectives by processing in the same\nway the data generated by proof searches on statements from the valid set and using the resulting\ndatasets to track their associated valid loss.\n10\nTraining a value function gives an opportunity to the model to learn from its errors on data it generates.\nIt also shifts proof searches from breadth ﬁrst exploration to one that is more focused, adaptively\nbased on the level of conﬁdence modeled by V.\n5 Experiments\nWe ﬁne-tune all of our models with 1024 examples per global batch and a context size of 2048 tokens,\nfor at most 32B tokens (our augmented dataset contains ∼1Btokens), early stopping at min valid\nloss when applicable. We anneal the learning-rate to zero (over 32B tokens). We found that restarting\nthe training with an annealing to zero that matches the early-stopping for a given model only provides\na marginal improvement, and avoided doing so.\nThe models are trained with the BPE encoding reported in [21], the same tokenization being used for\ntext, code or formalized statements. We leave as future work a thorough ablation of the encoding as\npreliminary experimental results demonstrate possible gains with specialized tokenization techniques.\n5.1 Baselines\nWe report three baselines: (i) the state of the art for Metamath’s set.mm as reported in MetaGen-\nIL[25] (their methodology for benchmarking their solution is close to ours so the numbers are directly\ncomparable); (ii) a 160m parameters trained from scratch on our raw dataset using the proofstep\nobjective; and (iii) a 160m parameters trained from scratch on ouraugmented dataset (same objective).\nTable 5: Baseline performance from MetaGen-IL as well as a 160m parameters model trained on the\nraw and augmented datasets.\nModel Performance # Tokens\nMetaGen-IL[25] 21.16% N/A\n160m raw dataset(ours) 29.22% 18B\n160m augmented dataset(ours) 28.96% 18B\nWe explain the improvement overMetaGen-IL (despite not relying on forward proving data genera-\ntion techniques) by our use of a simpler architecture (one unique Transformer vs 3 separate GRU\nnetworks); a more straightforward objective (direct auto-regressive generation of the full tactic as text\nvs separate premise selection and generation of the substitutions); more learnable parameters (160m\nvs 300k (3 2-layers bi-directional GRUs with 128 hiddens)); and more compute at training as well as\ntest time.\nNote that the dataset augmentation may have a marginal negative effect on performance on the valid\nset with our 160m model (but we’re within typical variance). We report in section 5.5 a more reliably\npositive effect with a pre-trained 700m model.\n5.2 Model Size\nTable 6: Performance for various model sizes trained on the augmented datasets.\nModel Performance Perplexity # Tokens\n160m augmented 28.96% 1.041 18B\n400m augmented 30.23% 1.042 18B\n700m augmented 31.58% 1.040 18B\nThese results demonstrate that model size positively impacts performance in our formal setup, despite\nthe training dataset being limited in size (we train for ∼18 epochs). Note that the bigger the model\nthe more compute we use at training time as well as benchmarking.\n5.3 Pre-training\nModels are pre-trained on CommonCrawl using GPT-3’s[21] methodology for 260B tokens. When\nstudying the effect of pre-training on WebMath we start from a CommonCrawl pre-trained model and\n11\ncontinue pre-training on WebMath for 16B additional tokens. We also report results after pre-training\non GitHub only instead of WebMath for the same number of tokens.\nTable 7: Performance for various model sizes and pre-training datasets.\nModel Performance Perplexity # Tokens\n160m from scratch 28.96% 1.041 18B\n160m CommonCrawl 32.34% 1.030 16B\n160m Github 33.61% 1.030 16B\n160m WebMath 34.79% 1.029 16B\n700m from scratch 31.58% 1.040 18B\n700m CommonCrawl 39.61% 1.026 15B\n700m Github 41.55% 1.025 15B\n700m WebMath 42.56% 1.024 15B\nWe hypothesize that the positive pre-training effect is primarily driven by the emergence and transfer\nof features that are relevant to formal reasoning. It is possible to argue that most of these features\nare probably shallow and mostly relevant at the syntactical level but the lower performance achieved\nwith Github only in comparison to WebMath suggests that some features may be more elaborate. We\nleave as future work a broader investigation of this question, which could be achieved by studying\nthe performance of linear probes on the features of the different pre-trained models with respect to a\nformal objective, such as the truthiness of a set of statements provided in the Metamath (or any other\nformal) language.\nTable 8: Performance for model sizes ranging from 160m to 1.5b parameters, pre-trained onWebMath.\nModel Performance Perplexity # Tokens\n160m (WebMath) 34.79% 1.029 16B\n400m (WebMath) 39.94% 1.026 15B\n700m (WebMath) 42.56% 1.024 15B\n1p5b (WebMath) 42.39% 1.024 13B\nIt is unclear why we do not observe a smooth improvement in performance between the 700m and\nthe 1.5b models in table 8. The lack of guarantee that the valid set has a smooth difﬁculty pattern\nmay play a role here. Another effect may originate from the limited size of the training set, leading\nthe training dynamics to saturate as we grow the number of parameters. We leave as future work\na closer study of this effect which could be accomplished by training on various fractions of the\ntraining dataset and checking for similar saturation plateaux.\n5.4 Learned Value Function\nWe report the performance of our models as we iteratively train on data generated by sampling proofs\nagainst the veriﬁer.\nTable 9: Performance of the 160m and 700m parameters models as we iterate through the learned\nvalue function data generation and re-training process. policy onlyconsists in adding new positive\nproofs found to the policy training (without training a value function) while policy+value consists in\nthe full iterative data-generation and training described in section 4.7.\nModel Iteration 0 Iteration 1 Iteration 2\n160m (WebMath) policy only 34.79% 38.17% 38.34%\n160m (WebMath) policy+value 39.27% 40.70%\n700m (WebMath) policy only 42.56% 42.23% 43.15%\n700m (WebMath) policy+value 44.59% 47.21%\nWhile overﬁtting on the train set does not generally appear to negatively impact performance on the\nvalid set (and can even often help noticeably if not too catastrophic), we discovered that it dramatically\nhurts our iterative training process. We hypothesize that overﬁtting collapses the data generation in\na mode where exploration is weakened, the model being overly optimistic about its predictions on\n12\nthe train set. We therefore carefully avoid overﬁtting by tracking the loss on the associated valid set,\nearly stopping as we reach a minimum.\nThere is probably additional performance to be extracted by running more iterations given how\ncontinuous this iterative process appears to be. We leave as future work the design of an iterative\ndata generation process that is less compute intensive. Indeed, we believe that a lot of computation is\nspent on subgoals that are not necessarily providing a lot of signal for the value function, and each\niteration is quite compute intensive as it requires sampling proofs for the entire training set (which\ntakes ∼20kGPU.hours on V100s in our current setup).\n5.5 Sample Complexity\nAblation of our synthetic dataset augmentation demonstrates that synthetically generated proofs\ngeneralize to some extent and provide a noticeable uplift in performance on the valid set for larger\nmodels.\nTable 10: Ablation of the augmented dataset for 160m and 700m parameters models.\nModel Performance Perplexity # Tokens\n160m (WebMath) raw dataset 34.12% 1.029 16B\n160m (WebMath) augmented dataset 34.79% 1.029 16B\n700m (WebMath) raw dataset 40.28% 1.024 15B\n700m (WebMath) augmented dataset 42.56% 1.024 15B\nOur main motivation for including synthetic proofs in our training, beyond the relative uplift achieved,\nis the study of the effect of model size and training a value function on the sample complexity\nof our models, as we control exactly how many examples from the synthetic domain we use for\ntraining. Table 11 reports the performance on 100 synthetically generated statements (different from\nthe train set) as well as the number of synthetic proofs present in the training set for each model (in\nparenthesis).\nTable 11: Performance of our models on 100 test statements from our synthetic generators (run with\nthe same parameters used to augment the training set (see table 4).\nModel 9-digit addition 9-digit division Ring equalities\n160m raw 13% (0) 4% (0) 6% (0)\n160m augmented 78% (100) 27% (100) 77% (100)\n160m policy+value (iteration 1) 87% (100) 24% (100) 71% (100)\n160m policy+value (iteration 2) 90% (100) 28% (100) 79% (100)\n700m raw 12% (0) 5% (0) 7% (0)\n700m augmented 76% (100) 32% (100) 82% (100)\n700m policy+value (iteration 1) 90% (100) 40% (100) 78% (100)\n700m policy+value (iteration 2) 92% (100) 47% (100) 88% (100)\nThis demonstrates the close (yet not perfectly correlated) relationship between sample complexity\nand performance in our formal reasoning setup, suggesting that sample complexity is an important\ndriver of improved performance with formal mathematics.\nMore importantly it demonstrates that our models are capable of acquiring new non-trivial capabilities\nwith a number of training examples that is compatible with manual formalization. We plan in the\nfuture to study similar learning dynamics for more challenging tasks for which we don’t have a\nsynthetic generator.\n5.6 Results\nWe attempted to push the performance of our models by increasing both the number of expansions\nper proof search from d= 128to d= 256, and the number of attempts per proofs from a= 4to\na= 32. We report the achieved performance as a function of the number of attempts per statements\non the valid set in Table 12.\nFinally, we performed a ﬁnal evaluation with d= 256and a= 32of our 700m model policy+value\n(iteration 2) on the held-out test set:\n13\nTable 12: Performance of our 700m model policy+value (iteration 2) as we double the number of\nattempts aper proposition (with d= 256).\nAttempts Performance Delta\na= 2 42.90%\na= 4 47.29% +4.39%\na= 8 51.26% +3.97%\na= 16 54.05% +2.99%\na= 32 56.50% +2.45%\nPerftest\na=32,e=32,d=256(θ700m) = 56.22%\n6 Output\nWe describe in this section two projects we executed, aimed at sharing with the Metamath community\nresults and tools based on our work.\n6.1 Proof Shortening\nWe contributed 23 shortened proofs1112 of theorems to the Metamath library. These proofs were\ngenerated by the GPT-f automated prover. To discover shorter proofs, we sampled proofs for\nstatements from the set.mm library, comparing the length of the solutions found by our models to\ntheir ground truth versions, also verifying that the shorter proofs didn’t rely on additional axioms.\nThe reception13 from the Metamath community was positive, proof length being a metric the commu-\nnity care about:\n“I had a look at the proofs—very impressive results! Especially because we\nhad a global minimization recently, and your method found much shorter proofs\nnevertheless.”\n“Any ML-based system is impressive if it can ﬁnd many shorter proofs than the\nones we already have. Nice work.”\n“The shorter proof is easier to translate. It’s more symmetric in that it treats A\nand B identically. It’s philosophically more concise in that it doesn’t rely on the\nexistence of a universal class of all sets.”\nTo our knowledge, these shortened proofs are the ﬁrst effective contribution of a deep learning system\nto a formal mathematics library14\n6.2 GPT-f Proof Assistant\nWe created an online proof assistant15 to allow interactive proof constructions with the assistance of\nour models.\nWe used it to formalize more than 200 theorems and exercises. We found our models to be particularly\nuseful to automatically generate a variety of technical low level proofsteps required in most Metamath\nproofs, search the library by adapting existing theorems to the format needed by the user (e.g.,\n11https://github.com/metamath/set.mm/pull/1547\n12https://github.com/metamath/set.mm/pull/1561\n13https://groups.google.com/g/metamath/c/-FNsw2wyllI\n14To determine whether other deep learning-based provers have made contributions to their respective libraries,\nwe looked for such contributions in the following systems: Holist family in HOL Light, CoqGym+ASTatic in\nCoq, TacticToe in HOL4. In addition, we interviewed 6 experts in formal mathematics and/or deep learning\napplied to formal mathematics.\n15https://groups.google.com/g/metamath/c/D09W2QVR-_I/m/g_rsqGj0AAAJ\n14\nFigure 2: Screenshot of the GPT-f Proof Assistant\ndeduction form16) and suggest theorems to use. Even when mistaken, our models generally go for\nthe right theorems, whose erroneous substitutions are often easy to ﬁx by humans.\nWe shared the proof assistant with the Metamath community with the objective for it to be mutually\nbeneﬁcial, helping the community to be more productive and reciprocally helping us improve our\nmodels’ accuracy by automatically gathering human feedback. We also plan to extendGPT-f to other\nformal systems.\n7 Conclusion\nIn this paper, we present the GPT-f automated prover and proof assistant and show that the Trans-\nformer is suitable to formal reasoning, achieving a new state of the art result on the Metamath library.\nIn particular we demonstrate the importance of pre-training as well as iterative training of a value\nfunction. Our results suggest that tightly coupling a deep learning system with a formal system opens\nup interesting opportunities for further research, with the goal of better leveraging the generative\npower of the former and the veriﬁcation capabilities of the latter.\nAcknowledgments\nSzymon Sidor, Jakub Pachocki, Harri Edwards, Yura Burda and Vedant Misra inspired many of\nthe ideas presented in this work, offering their guidance throughout the process of building GPT-f.\nAuguste Poiroux implemented the synthetic dataset generators presented in this paper, and formalized\na large number of theorems using the proof assistant, providing invaluable feedback in the process.\nSzymon Sidor, Pranav Shyam, John Schulman, Jared Kaplan, Ryan Lowe and Jack Clark slogged\nthrough drafts of this paper, identifying errors and sources of confusion as well as providing helpful\nsuggestions. Finally, the authors would like to thank the whole Metamath community for their\nsupport, feedback, and encouragement, in particular, David A. Wheeler for his motivating enthusiasm\nand Mario Carneiro for his precious help on a wide variety of technical questions.\n16Deduction Form and Natural Deductionhttp://us.metamath.org/mpeuni/mmnatded.html\n15\nReferences\n[1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep\nconvolutional neural networks. In Advances in neural information processing systems, pages\n1097–1105, 2012.\n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 770–778, 2016.\n[3] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural\nnetworks. In Advances in neural information processing systems, pages 3104–3112, 2014.\n[4] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint arXiv:1409.0473, 2014.\n[5] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144, 2016.\n[6] Alex Graves and Navdeep Jaitly. Towards end-to-end speech recognition with recurrent neural\nnetworks. In International conference on machine learning, pages 1764–1772, 2014.\n[7] Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Battenberg,\nCarl Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang Chen, et al. Deep speech\n2: End-to-end speech recognition in english and mandarin. In International conference on\nmachine learning, pages 173–182, 2016.\n[8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural\ninformation processing systems, pages 2672–2680, 2014.\n[9] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with\ndeep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.\n[10] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for\nimproved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.\n[11] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative\nadversarial networks. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 4401–4410, 2019.\n[12] Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan,\nand Ilya Sutskever. Generative pretraining from pixels. In Proceedings of the 37th International\nConference on Machine Learning, 2020.\n[13] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driess-\nche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mas-\ntering the game of go with deep neural networks and tree search. nature, 529(7587):484–489,\n2016.\n[14] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław D˛ ebiak, Christy\nDennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large\nscale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.\n[15] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Jun-\nyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster\nlevel in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\n[16] Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur\nPetron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, et al. Solving rubik’s cube\nwith a robot hand. arXiv preprint arXiv:1910.07113, 2019.\n16\n[17] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep\nvisuomotor policies. The Journal of Machine Learning Research, 17(1):1334–1373, 2016.\n[18] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. InAdvances in neural information\nprocessing systems, pages 5998–6008, 2017.\n[19] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\nunderstanding by generative pre-training, 2018.\n[20] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\nLanguage models are unsupervised multitask learners. OpenAI Blog, 1(8):9, 2019.\n[21] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. arXiv preprint arXiv:2005.14165, 2020.\n[22] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,\n2018.\n[23] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur\nGuez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering\nchess and shogi by self-play with a general reinforcement learning algorithm. arXiv preprint\narXiv:1712.01815, 2017.\n[24] John Harrison, Josef Urban, and Freek Wiedijk. History of interactive theorem proving. In\nComputational Logic, volume 9, pages 135–214, 2014.\n[25] Mingzhe Wang and Jia Deng. Learning to prove theorems by learning to generate theorems.\narXiv preprint arXiv:2002.07019, 2020.\n[26] Geoffrey Irving, Christian Szegedy, Alexander A Alemi, Niklas Eén, François Chollet, and\nJosef Urban. Deepmath-deep sequence models for premise selection. In Advances in Neural\nInformation Processing Systems, pages 2235–2243, 2016.\n[27] Mingzhe Wang, Yihe Tang, Jian Wang, and Jia Deng. Premise selection for theorem proving\nby deep graph embedding. In Advances in Neural Information Processing Systems, pages\n2786–2796, 2017.\n[28] Daniel Selsam, Matthew Lamm, Benedikt Bünz, Percy Liang, Leonardo de Moura, and David L\nDill. Learning a sat solver from single-bit supervision. arXiv preprint arXiv:1802.03685, 2018.\n[29] Sarah Loos, Geoffrey Irving, Christian Szegedy, and Cezary Kaliszyk. Deep network guided\nproof search. arXiv preprint arXiv:1701.06972, 2017.\n[30] Kshitij Bansal, Sarah Loos, Markus Rabe, Christian Szegedy, and Stewart Wilcox. Holist: An\nenvironment for machine learning of higher order logic theorem proving. In International\nConference on Machine Learning, pages 454–463, 2019.\n[31] Kshitij Bansal, Sarah M Loos, Markus N Rabe, and Christian Szegedy. Learning to reason in\nlarge theories without imitation. arXiv preprint arXiv:1905.10501, 2019.\n[32] Markus N Rabe, Dennis Lee, Kshitij Bansal, and Christian Szegedy. Mathematical reasoning\nvia self-supervised skip-tree training. arXiv preprint arXiv:2006.04757, 2020.\n[33] Daniel Huang, Prafulla Dhariwal, Dawn Song, and Ilya Sutskever. Gamepad: A learning\nenvironment for theorem proving. arXiv preprint arXiv:1806.00608, 2018.\n[34] Kaiyu Yang and Jia Deng. Learning to prove theorems via interacting with proof assistants.\narXiv preprint arXiv:1905.09381, 2019.\n[35] Daniel Whalen. Holophrasm: a neural automated theorem prover for higher-order logic. arXiv\npreprint arXiv:1608.02644, 2016.\n17\n[36] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed\ntext-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.\n[37] Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by ratio-\nnale generation: Learning to solve and explain algebraic word problems. arXiv preprint\narXiv:1705.04146, 2017.\n[38] Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh\nHajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based\nformalisms. arXiv preprint arXiv:1905.13319, 2019.\n[39] Guillaume Lample and François Charton. Deep learning for symbolic mathematics. arXiv\npreprint arXiv:1912.01412, 2019.\n[40] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Uni-\nversal transformers. arXiv preprint arXiv:1807.03819, 2018.\n[41] David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical\nreasoning abilities of neural models. arXiv preprint arXiv:1904.01557, 2019.\n[42] Norman D. Megill and David A. Wheeler. Metamath: A Computer Language for Pure Mathe-\nmatics, 2019. http://us.metamath.org/downloads/metamath.pdf.\n[43] Norman Megill. How Metamath Proofs Work , 2006.\nhttp://us.metamath.org/mpeuni/mmset.html#proofs.\n[44] Freek Wiedijk. The \"de Bruijn factor\", 2014. http://www.cs.ru.nl/ freek/factor/.\n[45] Yuhuai Wu, Albert Jiang, Jimmy Ba, and Roger Grosse. Int: An inequality benchmark for\nevaluating generalization in theorem proving. arXiv preprint arXiv:2007.02924, 2020.\n[46] Thomas Anthony, Zheng Tian, and David Barber. Thinking fast and slow with deep learning\nand tree search. In Advances in Neural Information Processing Systems, pages 5360–5370,\n2017.\nA Key Results\nTable 13: Key results described in this paper (on the valid set) with a summary of the source of\nperformance gains.\nModel Performance Gain Main ablation\nMetaGen-IL [25] 21.16% Baseline and state of the art.\n160m (ours) 28.96% +7.8% Use of Transformers.\n700m (ours) 31.58% +2.5% Increase in parameters count.\n700m WebMath (ours) 42.56% +10.9% Pre-training.\n700m policy+value (ours) 47.21% +4.6% Iterated learned value function.\n700m policy+value a= 32(ours) 56.50% +9.2% Increased test-time compute.\nB Example Proofs Generated\nIn this appendix, we display a selection of proofs generated by GPT-f (from our valid set). The right\ncolumn contains the current goal. The left column displays the name of the theorem applied by to the\ngoal. Proofs are read bottom-up and the statement being demonstrated is the last goal of the table.\nThe subgoals generated by a proof step can be retrieved by looking at the theorem names that are\nindented by one additional space. The statement of the theorems can be retrieved with the Metamath\nProof Explorer. Substitutions are omitted for clarity, but can be inferred by looking at the statement\nof the theorem being applied and comparing it with the current goal and associated subgoals.\n18\nB.1 Proof of nn0onn0ex\nThis proof demonstrates that n∈N ∧n+1\n2 ∈N =⇒ ∃m∈N : n= 2m+ 1. It is interesting for\nits ﬁrst proof step. syl2anc17 states that assuming P =⇒ Q,P =⇒ R,Q ∧R =⇒ S then\nP =⇒ S. P is mechanically uniﬁed with n∈N ∧n+1\n2 ∈N, and Swith ∃m∈N : n= 2m+ 1\nbut the model freely generates substitutions for Qand R. Looking at the subgoals, Qis substituted\nwith n−1\n2 ∈N and Rwith n= 2n−1\n2 + 1which materialises a witness for the existence of m.\nThe model is left to demonstrate n∈N ∧n+1\n2 ∈N =⇒ n−1\n2 ∈N, then n∈N ∧n+1\n2 ∈N =⇒\nn = 2n−1\n2 + 1and ﬁnally n−1\n2 ∈N ∧n = 2n−1\n2 + 1 =⇒ ∃m ∈N : n = 2m+ 1using the\nexistential specialization provided by rspcev18.\n+ nn0o |- ( ( N e. NN0 /\\ ( ( N + 1 ) / 2 ) e. NN0 ) ->\n( ( N - 1 ) / 2 ) e. NN0 )\n+ nn0cn |- ( N e. NN0 -> N e. CC )\n+ ax1cn |- 1 e. CC\n+ subcl |- ( ( N e. CC /\\ 1 e. CC ) -> ( N - 1 ) e. CC )\n+ sylancl |- ( N e. NN0 -> ( N - 1 ) e. CC )\n+ 2cnd |- ( N e. NN0 -> 2 e. CC )\n+ 2ne0 |- 2 =/= 0\n+ a1i |- ( N e. NN0 -> 2 =/= 0 )\n+ divcan2d |- ( N e. NN0 -> ( 2 x. ( ( N - 1 ) / 2 ) ) = ( N - 1 ) )\n+ adantr |- ( ( N e. NN0 /\\ ( ( N + 1 ) / 2 ) e. NN0 ) ->\n( 2 x. ( ( N - 1 ) / 2 ) ) = ( N - 1 ) )\n+ oveq1d |- ( ( N e. NN0 /\\ ( ( N + 1 ) / 2 ) e. NN0 ) ->\n( ( 2 x. ( ( N - 1 ) / 2 ) ) + 1 ) =\n( ( N - 1 ) + 1 ) )\n+ nn0cn |- ( N e. NN0 -> N e. CC )\n+ ax1cn |- 1 e. CC\n+ npcan |- ( ( N e. CC /\\ 1 e. CC ) -> ( ( N - 1 ) + 1 ) = N )\n+ sylancl |- ( N e. NN0 -> ( ( N - 1 ) + 1 ) = N )\n+ adantr |- ( ( N e. NN0 /\\ ( ( N + 1 ) / 2 ) e. NN0 ) ->\n( ( N - 1 ) + 1 ) = N )\n+ eqtr2d |- ( ( N e. NN0 /\\ ( ( N + 1 ) / 2 ) e. NN0 ) ->\nN = ( ( 2 x. ( ( N - 1 ) / 2 ) ) + 1 ) )\n+ oveq2 |- ( m = ( ( N - 1 ) / 2 ) -> ( 2 x. m ) =\n( 2 x. ( ( N - 1 ) / 2 ) ) )\n+ oveq1d |- ( m = ( ( N - 1 ) / 2 ) -> ( ( 2 x. m ) + 1 ) =\n( ( 2 x. ( ( N - 1 ) / 2 ) ) + 1 ) )\n+ eqeq2d |- ( m = ( ( N - 1 ) / 2 ) -> ( N = ( ( 2 x. m ) + 1 )\n<-> N = ( ( 2 x. ( ( N - 1 ) / 2 ) ) + 1 ) ) )\n+ rspcev |- ( ( ( ( N - 1 ) / 2 ) e. NN0 /\\\nN = ( ( 2 x. ( ( N - 1 ) / 2 ) ) + 1 ) ) ->\nE. m e. NN0 N = ( ( 2 x. m ) + 1 ) )\n+ syl2anc |- ( ( N e. NN0 /\\ ( ( N + 1 ) / 2 ) e. NN0 ) ->\nE. m e. NN0 N = ( ( 2 x. m ) + 1 ) )\nSuch generation of exogenous terms, here to demonstrate an existence proof, is exactly what motivated\nour work. It’s therefore encouraging to witness it effectively happening in practice.\nB.2 Proof of uznn0sub\nThis proof demonstrates that n ≥m ∈Z =⇒ (n−m) ∈N. It exhibits another form of term\ngeneration. Here, sylibr19 states that assuming P =⇒ Q,R ⇔Qthen P =⇒ R. Again, P is\nmechanically uniﬁed to n≥m∈Z, and Rwith (n−m) ∈N. The model is left to generate freely a\nsubstitution for Q: (n−m) ∈Z ∧0 ≤(n−m). The equivalence R⇔Qto demonstrate becomes\n17Metamath Proof Explorer - syl2anchttp://us.metamath.org/mpeuni/syl2anc.html\n18Metamath Proof Explorer - rspcevhttp://us.metamath.org/mpeuni/rspcev.html\n19Metamath Proof Explorer - sylibrhttp://us.metamath.org/mpeuni/sylibr.html\n19\n(n−m) ∈N ⇔(n−m) ∈Z ∧0 ≤(n−m) which is exactly the statement of a theorem available\nin the Metamath library, elnn0z20. The statement of elnn0z is memoized by the model, and the\ngeneration of the substitution term for Qis driven by this memoization.\n+ eluzelz |- ( N e. ( ZZ>= ‘ M ) -> N e. ZZ )\n+ eluzel2 |- ( N e. ( ZZ>= ‘ M ) -> M e. ZZ )\n+ zsubcld |- ( N e. ( ZZ>= ‘ M ) -> ( N - M ) e. ZZ )\n+ eluzle |- ( N e. ( ZZ>= ‘ M ) -> M <_ N )\n+ eluzelre |- ( N e. ( ZZ>= ‘ M ) -> N e. RR )\n+ eluzel2 |- ( N e. ( ZZ>= ‘ M ) -> M e. ZZ )\n+ zred |- ( N e. ( ZZ>= ‘ M ) -> M e. RR )\n+ subge0d |- ( N e. ( ZZ>= ‘ M ) -> ( 0 <_ ( N - M ) <-> M <_ N ) )\n+ mpbird |- ( N e. ( ZZ>= ‘ M ) -> 0 <_ ( N - M ) )\n+ jca |- ( N e. ( ZZ>= ‘ M ) -> ( ( N - M ) e. ZZ /\\\n0 <_ ( N - M ) ) )\n+ elnn0z |- ( ( N - M ) e. NN0 <-> ( ( N - M ) e. ZZ /\\\n0 <_ ( N - M ) ) )\n+ sylibr |- ( N e. ( ZZ>= ‘ M ) -> ( N - M ) e. NN0 )\nB.3 Proof of pm4.78\nThis proof displays the model capabilities to demonstrate non-trivial propositional logic statements, a\ntask of interest because of its relationship to SAT solving.\n+ pm2.21 |- ( -. ph -> ( ph -> ps ) )\n+ orcd |- ( -. ph -> ( ( ph -> ps ) \\/ ( ph -> ch ) ) )\n+ ax-1. |- ( ps -> ( ph -> ps ) )\n+ ax-1 |- ( ch -> ( ph -> ch ) )\n+ orim12i |- ( ( ps \\/ ch ) -> ( ( ph -> ps ) \\/ ( ph -> ch ) ) )\n+ ja |- ( ( ph -> ( ps \\/ ch ) ) -> ( ( ph -> ps ) \\/\n( ph -> ch ) ) )\n+ orc |- ( ps -> ( ps \\/ ch ) )\n+ imim2i |- ( ( ph -> ps ) -> ( ph -> ( ps \\/ ch ) ) )\n+ olc |- ( ch -> ( ps \\/ ch ) )\n+ imim2i |- ( ( ph -> ch ) -> ( ph -> ( ps \\/ ch ) ) )\n+ jaoi |- ( ( ( ph -> ps ) \\/ ( ph -> ch ) ) ->\n( ph -> ( ps \\/ ch ) ) )\n+ impbii |- ( ( ph -> ( ps \\/ ch ) ) <-> ( ( ph -> ps ) \\/\n( ph -> ch ) ) )\n+ bicomi |- ( ( ( ph -> ps ) \\/ ( ph -> ch ) ) <->\n( ph -> ( ps \\/ ch ) ) )\n20Metamath Proof Explorer - elnn0zhttp://us.metamath.org/mpeuni/elnn0z.html\n20",
  "topic": "Mathematical proof",
  "concepts": [
    {
      "name": "Mathematical proof",
      "score": 0.8383594155311584
    },
    {
      "name": "Automated theorem proving",
      "score": 0.7755177021026611
    },
    {
      "name": "Computer science",
      "score": 0.6586559414863586
    },
    {
      "name": "Generative grammar",
      "score": 0.6572368144989014
    },
    {
      "name": "Proof assistant",
      "score": 0.6243970990180969
    },
    {
      "name": "Programming language",
      "score": 0.5761286616325378
    },
    {
      "name": "Automated proof checking",
      "score": 0.5545454025268555
    },
    {
      "name": "Automated reasoning",
      "score": 0.5173975229263306
    },
    {
      "name": "Computer-assisted proof",
      "score": 0.5084847211837769
    },
    {
      "name": "Transformer",
      "score": 0.45485663414001465
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4330950081348419
    },
    {
      "name": "Natural language processing",
      "score": 0.38250428438186646
    },
    {
      "name": "Calculus (dental)",
      "score": 0.34649336338043213
    },
    {
      "name": "Theoretical computer science",
      "score": 0.33129972219467163
    },
    {
      "name": "Mathematics",
      "score": 0.2271501123905182
    },
    {
      "name": "Medicine",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Dentistry",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210161460",
      "name": "OpenAI (United States)",
      "country": "US"
    }
  ]
}