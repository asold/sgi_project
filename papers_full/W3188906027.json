{
  "title": "PoseGTAC: Graph Transformer Encoder-Decoder with Atrous Convolution for 3D Human Pose Estimation",
  "url": "https://openalex.org/W3188906027",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2154863663",
      "name": "Yiran Zhu",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2111849503",
      "name": "Xing Xu",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2101410376",
      "name": "Fu-min Shen",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2102334518",
      "name": "Yan-Li Ji",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2166790520",
      "name": "Lianli Gao",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2116772828",
      "name": "Heng Tao Shen",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3030520226",
    "https://openalex.org/W6863994431",
    "https://openalex.org/W2787091153",
    "https://openalex.org/W2924460655",
    "https://openalex.org/W2765789968",
    "https://openalex.org/W2934361577",
    "https://openalex.org/W2624431344",
    "https://openalex.org/W3098473649",
    "https://openalex.org/W2519887557",
    "https://openalex.org/W6803771590",
    "https://openalex.org/W3016522925",
    "https://openalex.org/W3035637814",
    "https://openalex.org/W2967417633",
    "https://openalex.org/W2797184202",
    "https://openalex.org/W2554247908",
    "https://openalex.org/W2798646183",
    "https://openalex.org/W2903549000",
    "https://openalex.org/W2968510372",
    "https://openalex.org/W2810607340",
    "https://openalex.org/W6759503086",
    "https://openalex.org/W6864014924",
    "https://openalex.org/W6772177647",
    "https://openalex.org/W2784435047",
    "https://openalex.org/W2795089319",
    "https://openalex.org/W6696085341",
    "https://openalex.org/W6755811877",
    "https://openalex.org/W2964318832",
    "https://openalex.org/W2756050327",
    "https://openalex.org/W2981661132",
    "https://openalex.org/W2983925976",
    "https://openalex.org/W2964309882",
    "https://openalex.org/W2997240763",
    "https://openalex.org/W2963840672",
    "https://openalex.org/W2895689136",
    "https://openalex.org/W2989465897",
    "https://openalex.org/W2963076818",
    "https://openalex.org/W2970285700",
    "https://openalex.org/W2101032778",
    "https://openalex.org/W2939208918",
    "https://openalex.org/W2984313141",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2962896489",
    "https://openalex.org/W2963175980",
    "https://openalex.org/W3098612954",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W2969450957",
    "https://openalex.org/W4294558607",
    "https://openalex.org/W3034895839",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "Graph neural networks (GNNs) have been widely used in the 3D human pose estimation task, since the pose representation of a human body can be naturally modeled by the graph structure. Generally, most of the existing GNN-based models utilize the restricted receptive Ô¨Åelds of Ô¨Ålters and single-scale information, while neglecting the valuable multi-scale contextual information. To tackle this issue, we propose a novel Graph Transformer Encoder-Decoder with Atrous Convolution, named PoseGTAC, to effectively extract multi-scale context and long-range information. In our proposed PoseGTAC model, Graph Atrous Convolution (GAC) and Graph Transformer Layer (GTL), respectively for the extraction of local multi-scale and global long-range information, are combined and stacked in an encoder-decoder structure, where graph pooling and unpooling are adopted for the interaction of multi-scale information from local to global (e.g., part-scale and body-scale). Extensive experiments on the Human3.6M and MPI-INF-3DHP datasets demonstrate that the proposed PoseGTAC model exceeds all previous methods and achieves state-of-the-art performance.",
  "full_text": "PoseGTAC: Graph Transformer Encoder-Decoder with Atrous Convolution\nfor 3D Human Pose Estimation\nYiran Zhu , Xing Xu\u0003 , Fumin Shen , Yanli Ji, Lianli Gao and Heng Tao Shen\nCenter for Future Media & School of Computer Science and Engineering\nUniversity of Electronic Science and Technology of China, China\nfyiranupup, fumin.sheng@gmail.com, fxing.xu, yanliji, lianli.gaog@uestc.edu.cn,\nshenhengtao@hotmail.com\nAbstract\nGraph neural networks (GNNs) have been widely\nused in the 3D human pose estimation task, since\nthe pose representation of a human body can be nat-\nurally modeled by the graph structure. Generally,\nmost of the existing GNN-based models utilize the\nrestricted receptive Ô¨Åelds of Ô¨Ålters and single-scale\ninformation, while neglecting the valuable multi-\nscale contextual information. To tackle this issue,\nwe propose a novel model named Graph Trans-\nformer Encoder-Decoder with Atrous Convolution\n(PoseGTAC), to effectively extract multi-scale con-\ntext and long-range information. SpeciÔ¨Åcally, our\nPoseGTAC model has two key components: Graph\nAtrous Convolution (GAC) and Graph Transformer\nLayer (GTL), which are respectively for the ex-\ntraction of local multi-scale and global long-range\ninformation. They are combined and stacked in\nan encoder-decoder structure, where graph pool-\ning and unpooling are adopted for the interaction\nof multi-scale information from local to global as-\npect (e.g., part-scale and body-scale). Extensive ex-\nperiments on the Human3.6M and MPI-INF-3DHP\ndatasets demonstrate that the proposed PoseGTAC\nmodel achieves state-of-the-art performance.\n1 Introduction\nIn recent years, 3D human pose estimation is attracting in-\ntensive attention in various human-related research Ô¨Åelds,\nsuch as action recognition [Yan et al., 2018; Ji et al., 2019],\nhuman-object interaction [Li et al., 2020] and motion predic-\ntion [Mao et al., 2019]. Its goal is to estimate 3D coordinates\nof human body joints from 2D poses or images. Compared\nwith the methods [Zhou et al., 2017; Wu and Xiao, 2020] us-\ning RGB images, the 2D-to-3D methods [Zhao et al., 2019;\nLiu et al., 2020] using only 2D poses can avoid the inÔ¨Çu-\nence of background noise and greatly reduce the computa-\ntional complexity, thus achieving competitive performance.\nIn this paper, we focus on the topic of 2D-to-3D pose es-\ntimation, which aims to predict 3D poses only given the 2D\n\u0003Corresponding author\nroot node\nnode involved node not involved\nhuman skeleton\nPrevious methods Our PoseGTACmethod\nLocal multi-scale contextual information Global long-range \ninformation\nFigure 1: Previous methods commonly adopt the restricted recep-\ntive Ô¨Åelds of Ô¨Ålters and ignore various multi-scale contextual infor-\nmation. Differently, our proposed PoseGTAC method enlarges the\nreceptive Ô¨Åelds of Ô¨Ålters for capturing multi-scale context.\npose data. Recently, as an extension of the standard convo-\nlutional network, graph neural network (GNN) has shown its\nnatural superiority in capturing the irregular structures in vi-\nsual data that CNN cannot handle. For human pose estima-\ntion, both the 2D and 3D pose information can be regarded\nas a graph and intuitively be modeled by GNN. The GNN-\nbased methods take the body joints as the nodes and the bones\nphysically connecting body joints as the edges to build the\ngraph. Compared with the traditional methods, the GNN-\nbased methods have achieved better performance. For exam-\nple, ST-GCN [Yan et al., 2018] Ô¨Årst utilized graph convolu-\ntional network (GCN) to aggregate the skeleton features and\nachieved impressive performance. Later, SemGCN [Zhao et\nal., 2019] introduced the semantic graph convolutional net-\nwork to capture local and non-local information. Besides,\nSD-HNN [Liu et al., 2020] leveraged hypergraphs to model\nthe dynamics of the human body for 3D pose estimation.\nThough the above approaches have achieved good perfor-\nmance in 3D human pose estimation, they generally adopt\nthe restricted receptive Ô¨Åelds of Ô¨Ålters and aggregate the\nsingle-scale joint information. As illustrated in Fig. 1,\nexisting approaches limitedly consider the 1-hop neighbors\nwhen calculating the graph convolution, while neglecting\nthe valuable multi-scale contextual information. Actually,\nthe multi-scale contextual information contains rich features\nthat are essential to facilitate the prediction performance,\nProceedings of the Thirtieth International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-21)\n1359\nGraphConv\nGraphAtrousConv\nGraphTransformer\nGraphConv\nInput 2D Pose Predicted 3D Pose\nPoseGTAC\nGraphAtrousConv\nGraphTransformer\nGraphAtrousConv\nGraphTransformer\nGraphAtrousConv\nGraphTransformer\nGraphAtrousConv\nGraphTransformer\nc c\ncGraph pooling Graph unpooling Concatenation\nFigure 2: The overall framework of the proposed PoseGTAC model. It is a hierarchical encoder-decoder architecture that consists of stacked\ngraph atrous convolution (GAC) layers and graph transformer layers (GTL) at different scales. In addition, at the beginning and end of the\nmodel, two graph convolution layers are used for the input encoding and output decoding procedures.\nthough it implicitly resides among the higher-order neigh-\nbors. Moreover, several recent approaches[Zhao et al., 2019;\nLiu et al., 2020] attempt to extract non-local information,\nbut ignore the semantic positional information (i.e., the joint\ntype). For instance, two joints of the same coordinates with\ndifferent semantics may convey totally different information.\nTo overcome the limitations of existing GNN-based ap-\nproaches, we propose a novel Graph Transformer Encoder-\nDecoder with Atrous Convolution, dubbedPoseGTAC, to en-\nhance the extraction of multi-scale context and long-range re-\nlationships in the pose. As the overall framework is shown\nin Fig. 2, our proposed PoseGTAC consists of stacked graph\natrous convolution layers and graph transformer layers in an\nencoder-decoder structure which exploits multi-scale features\nbased on human kinetics. Notably, the Graph Atrous Convo-\nlution (GAC) can effectively enlarge the receptive Ô¨Åelds of\nÔ¨Ålters and densely learn multi-scale pose context, and Graph\nTransformer Layer (GTL) is used to capture global long-\nrange information. Moreover, graph pooling and graph un-\npooling are adopted in PoseGTAC to ensure the interaction\nof multi-scale information from local to global.\nOur main contributions are three-fold: (1) We propose a\nnovel PoseGTAC method that can effectively extract local\nmulti-scale context and global long-range relationships for\n3D human pose estimation. (2) We design an advanced Graph\nAtrous Convolution (GAC) to enlarge the receptive Ô¨Åelds of\nÔ¨Ålters and learn multi-scale pose context, and a Graph Trans-\nformer Layer (GTL) to capture global long-range relation-\nships. The two key components are Ô¨Çexibly combined in an\nencoder-decoder structure. (3) We conduct extensive exper-\niments on two widely-used datasets Human3.6M and MPI-\nINF-3DHP to demonstrate the superiority of our proposed\nPoseGTAC model comparing to the state-of-the-art methods.\n2 Related Work\n2D-to-3D Pose Estimation. With a lot of research into 3D\nhuman pose estimation, the existing methods can be grouped\ninto three directions, 2D-to-3D pose estimation [Zhao et al.,\n2019; Liu et al., 2020], monocular image-based 3D pose es-\ntimation [Zhao et al., 2019; Wu and Xiao, 2020] and multi-\nview image-based 3D pose estimation [Qiu et al., 2019]. The\nrecent GNN-based methods have greatly improved prediction\nperformance in 2D-to-3D pose estimation. ST-GCN [Yan et\nal., 2018 ] Ô¨Årst applied graph convolution to aggregate fea-\ntures in the skeleton. SemGCN [Zhao et al., 2019] utilized\nsemantic graph convolution to lift 2D pose to 3D pose by ex-\ntracting local and non-local information. SD-HNN[Liu et al.,\n2020] introduced static and dynamic hypergraphs to represent\na human body for 3D pose estimation. However, they do not\ntake full advantage of multi-scale contextual information.\nGraph Neural Networks. As a generalization of standard\nconvolution and pooling, many methods of graph convolu-\ntion and graph pooling have recently been proposed. Inspired\nby the graph Laplacian methods, [Kipf and Welling, 2017 ]\nproposed graph convolution networks by the Chebyshev ap-\nproximation, which is the most widely used form of graph\nconvolution. GraphSAGE [Hamilton et al., 2017] embedded\nnode features by sampling and aggregating and introduced\ntransductive graph convolution. SAGPool [Lee et al., 2019]\nattempted to use a learnable mask to select the node features\nretained. In this work, we propose a novel graph transformer\nencoder-decoder with atrous convolution.\nAttention Mechanism. The pioneering work of [Vaswani et\nal., 2017] introduced a new attention based network named\nTransformer for the machine translation task of natural lan-\nguage processing (NLP). Transformer is mainly composed of\nmulti-head attention module and feedforward network. There\nhas also been a lot of work based on attention mechanism in\ncomputer vision recently [Wang et al., 2018; Carion et al.,\n2020]. Unlike non-local network based methods [Zhao et al.,\n2019], our PoseGTAC method introduces the transformer op-\neration on the graph to capture long-range relationships.\n3 Proposed Method\n3.1 Preliminaries\nGraph DeÔ¨Ånition. The raw pose data, i.e., the joint keypoint\nvector, is a set of 2D coordinates. Usually, in GNN-based\napproaches, the vector can be represented as a graph G =\n(V;E), where the vertices are all joints and edges are physical\nProceedings of the Thirtieth International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-21)\n1360\nconnections between two joints. Here Vis the set of N joints\nand Eis characterized by the adjacency matrix A 2RN\u0002N.\nIn this way, the pose data is transformed into a graph sequence\nand speciÔ¨Åcally represented as a tensor X 2RN\u0002C, where\nN and Cdenote the numbers of joints and channels.\nGraph Convolution. Based on the above deÔ¨Ånition, the ex-\nisting GNN-based methods generally use the stacked graph\nconvolution module to extract the high-level skeleton infor-\nmation to regress 3D pose. The commonly used graph con-\nvolution operation can be represented as follows:\nX(i+1) = \u001b(WX(i)(\u0003 \fM)); (1)\nwhere \u001b(\u0001) denotes the activation function, iis the index of\nthe current layer and \u0003 denotes the normalized Laplace ma-\ntrix. \u0003 = D\u00001\n2 AD\u00001\n2 , where A is the adjacency matrix\nwith the self-loop, and D denotes the degree matrix of the\ngraph. W denotes a Cout \u0002Cin \u00021 \u00021 learnable weight\nmatrix and M denotes an N \u0002N attention mask matrix. \f\ndenotes the element-level dot product.\nIn addition to using local GCN to extract skeleton features,\nnon-local module is introduced to extract long-range infor-\nmation, which is generally represented as extracting the rela-\ntionships between the current node and all other nodes.\nRoot\n1-Hop\n2-Hop\n3-Hop\nPooling\nMLP\nùíÄ0\n(ùëô)\nùíÄ1\n(ùëô)\nùíÄ2\n(ùëô)\nùíÄ3\n(ùëô)\nùíÄùëùùëúùëúùëô\n(ùëô)\nùíÄ(ùëô)\nùëø(ùëô) ùëø(ùëô+1)Figure 3: Illustration of the graph atrous convolution (GAC) layer,\nwhich consists of paralleled graph convolutions and graph pooling.\nHere ‚ÄúMLP‚Äù denotes multi-layer perceptron.\n3.2 Graph Atrous Convolution (GAC)\nAs shown in Eq. 1, the previous methods use the restricted\nÔ¨Ålters to convolve only 1-hop neighbors, ignoring the multi-\nscale contextual information from higher-order neighbors.\nWe consider that multi-scale context is indispensable for\n3D human pose estimation. To this end, we introduce a\nmulti-scale graph convolution termed Graph Atrous Convo-\nlution (GAC) to capture the multi-scale context residing in\nthe higher-order neighbors. Inspired by the atrous convolu-\ntion [Yu and Koltun, 2016; Chen et al., 2018] in image seg-\nmentation, convolution operations with different dilation fac-\ntors are used in parallel. As illustrated in Fig. 3, in our graph\nconvolution, the dilation factor is deÔ¨Åned as the distance to\nthe root node, and graph atrous convolution is represented as\nparalleled convolutions with root node, 1-hop, 2-hop and 3-\nhop neighbors, etc. We Ô¨Årst formally deÔ¨Åne the k-hop matrix\nAk as follows:\n[Ak]i;j =\n8\n<\n:\n1 d(vi;vj) =k,\n1 d(vi;vj) = 0,\n0 otherwise,\n(2)\nwhere d(vi;vj) denotes the distance of the shortest path be-\ntween vi and vj on the skeleton graph, and Ak is the k-hop\nadjacency matrix with the self-loop.\nY(l)\nk = \u001b(WkX(l)(\u0003k \fMk)); (3)\nwhere \u0003k denotes the normalized k-hop Laplace matrix.\n\u0003k = D\n\u00001\n2\nk AkD\n\u00001\n2\nk , where Dk denotes the degree matrix\nof the graph. Wk denotes a learnable weight matrix for node\nembedding and Mkdenotes a N\u0002Nlearnable attention mask\nmatrix. Y(l)\nk 2 RN\u0002C denotes the output of k-hop graph\natrous convolution.\nMoreover, in order to facilitate global context information,\nskeleton features pooled globally are concatenated with the\noutput of parallel graph atrous convolution in Eq. 3 and then\nfed to a multi-layer perceptron (MLP) to aggregate multi-\nscale and global context features.\n8\n>><\n>>\n:\nY(l)\npool = AvgPool(X(l));\nY(l) = Cat([Y(l)\n0 ;:::; Y(l)\nk\u00001;Y(l)\npool]);\nX(l+1) = WY(l);\n(4)\nwhere AvgPool(\u0001) and Cat( \u0001) respectively denote the aver-\nage pooling and concatenation operation. Y(l)\npool 2 RN\u0002C\nrepresents the output features pooled globally, Y(l) 2\nRN\u0002[(k+1)\u0002C] is the features of concatenating all branches.\nW is a learnable weight matrix for feature aggregation and\ndimension reduction. X(l+1) denotes the Ô¨Ånal output features\nof the GAC layer.\n3.3 Graph Transformer Layer (GTL)\nAlthough the multi-scale contextual information has been\nwell extracted by the GAC module based on the local physical\nconnection of the human body, there is a lack of long-range\ninformation that can effectively promote pose representation\nlearning. To determine whether there is a connection between\ntwo joints and how strong the connection is, we introduce the\ngraph transformer layer (GTL) to better capture long-range\ninformation.\nSince the joints in the pose have no order to uniquely iden-\ntify their types from the input, the position encoding needs to\nbe added to complement the position information. In partic-\nular, we follow the sine and cosine functions as the position\nencoding functions in [Vaswani et al., 2017]:\nPE(pos;2i) = sin(pos=100002i=Cin );\nPE(pos;2i+1) = cos(pos=100002i=Cin );\n(5)\nwhere posis the position and iis the dimension of the posi-\ntion encoding vector. As shown in Fig. 4, in the GTL, the raw\nProceedings of the Thirtieth International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-21)\n1361\ninput is Ô¨Årst added with the position encoding, and then fed\nto two embedding functions (e.g., \u0012and \u001e) for obtaining the\nhigh-level features. The dot product is adopted to measure the\nsimilarity of the two joints in an embedding space. SpeciÔ¨Å-\ncally, we can calculate the attention matrix representing the\nstrength of the relationships between the joints as follows:\nMatt = Softmax(Xin\nTWT\n\u0012 W\u001eXin); (6)\nwhere Softmax(\u0001) denotes the softmax operation used for nor-\nmalization, and Matt denotes the attention map. W\u0012 and\nW\u001eare the learnable weight matrices of the embedding func-\ntions \u0012and \u001e, respectively.\nIn addition, we add an extra N \u0002N global attention ma-\ntrix Mglobal to pay more attention on unconstrained learning.\nSpeciÔ¨Åcally, the global attention matrix is added to the atten-\ntion matrix Matt and then multiplied by the original input.\nAs a result, the multi-head attention and feedforward network\ncan be used to obtain long-range features with rich attention.\nLinearAttention \nMap\nGlobal\nAttention \nLinearLinear\nPosition Encoding\nc\nElement-wise sum Matrix multiply Multi-head concatenationc\nùëøùíäùíè\nùëæùúΩ\nùëæùùã ùë¥ùíÇùíïùíï\nùë¥ùíàùíçùíêùíÉùíÇùíç\nùëØ\nùëøùíêùíñùíï\nFigure 4: Illustration of the graph transformer layer (GTL) that is\ncomposed of H self-attention modules and a feedforward network.\n3.4 The PoseGTAC Architecture\nTo obtain multi-scale (e.g., joint-scale and part-scale) infor-\nmation based on human kinetics, graph pooling and graph\nunpooling are adopted to capture effectively the interaction\nof multi-scale information in the pose. Before graph pool-\ning and unpooling, the nodes of each scale sare divided into\ndifferent regions Rs according to the physical priors of the\nhuman body, such as the upper left leg, the lower left leg, the\ntorso, etc. For the upper scale features, we implement the\npooling by using average pooling to aggregate the point fea-\ntures that are divided into a region in the current scale to get\nthe lower scale features. In other words, multiple points are\naveraged into one point. SpeciÔ¨Åcally, the formula is imple-\nmented as follows:\nXs+1\ni = AvgPool(fXs\nj j8Xs\nj 2Rs\nig); (7)\nwhere Rs\ni denotes the region feature under scale s, and Xs\nj\ndenotes a joint feature element belonging to Rs\ni set. Xs+1\ni\ndenotes a new joint feature under scales+1 obtained through\ngraph pooling.\nDue to the independence of the partitioned regions, the un-\npooling is realized by copying an upper scale feature multi-\nple times and then concatenating them together in the corre-\nsponding lower scale region.\nRs\ni = Cat(fXs+1\ni ;:::; Xs+1\ni g); (8)\nwhere the number of repetitions ofXs+1\ni is determined by the\nsize of the corresponding set Rs\ni.\nIn addition, the features obtained by unpooling are con-\ncatenated with the corresponding features on the contraction\npath, and then fed to the next layer. The graph atrous con-\nvolution layer and graph transformer layer are used to extract\nlocal and global information, while pooling and unpooling\nare used to facilitate the interaction of information from local\nto global. Fig. 2 illustrates the architecture of our proposed\nPoseGTAC, which stacks Ô¨Åve graph atrous convolution lay-\ners and Ô¨Åve graph transformer layers at different scales. Two\ngraph convolution layers are used for input encoding and out-\nput decoding procedures. Each layer is followed by a BN\nlayer and a ReLU layer. The mean squared error (MSE) be-\ntween the predicted pose and the ground-truth is used as our\nloss function, which can be trained in an end-to-end manner.\n4 Experiments\n4.1 Experimental Setup\nDatasets and Evaluation Protocols. Following previous\nstudies [Liu et al., 2020], we adopt two benchmark datasets\nHuman3.6M and MPI-INF-3DHP in our experiments.\nHuman3.6M [Ionescu et al., 2014] is the largest 3D human\npose estimation dataset. It contains 3.6 million images, where\n11 professional actors perform 15 actions such as walking,\ngreeting, smoking and making a phone call. Both 2D and 3D\nground-truth data are available for supervised 3D human pose\nestimation. We use Ô¨Åve subjects (S1, S5, S6, S7, and S8) for\ntraining and two subjects (S9 and S11) for testing. In order to\nreduce redundancy, we follow [Zhao et al., 2019] and down-\nsample the raw videos from 50fps to 10fps for the training\nand testing datasets. MPI-INF-3DHP [Mehta et al., 2017] is\nthe dataset obtained using MoCap system for 3D human pose\nestimation. The test set consists of 2,929 frames from 6 sub-\njects performing 7 actions.\nThree evaluation protocols are adopted in our experiments:\nProtocol #1is the mean per-joint position error (MPJPE) in\nmillimeters which measures the error between the ground-\ntruth and predictions. Protocol #2is P-MPJPE which reports\nthe error between the ground-truth and predictions through\nthe rigid transformation including translation, rotation and\nscale. As an auxiliary, we also measure the mean per-joint\nvelocity error (MPJVE) as Protocol #3, which is obtained by\nthe MPJPE of the Ô¨Årst derivative and represents the smooth-\nness of predicted results. Two standard metrics of PCK (Per-\ncentage of Correct Keypoints) under 150mm radius and AUC\n(Area under the ROC Curve) are used for quantitative evalu-\nation for MPI-INF-3DHP.\nImplement Details. Our PoseGTAC model consists of two\ngraph convolution layers, Ô¨Åve graph transformer layers (GTL)\nwith three heads and Ô¨Åve graph atrous convolution (GAC)\nlayers. The number of channels for all layers is 128, except\nfor GTL, which has 32 intermediate channels for reducing\nthe computational complexity of the model. We set up three\ndifferent scales containing 16,10 and 5 joints. The Adam op-\ntimizer is adopted as our optimizer with the initial learning\nrate 0.001 and the decay factor 0.96 per 100K steps. We train\nour model for 50 epochs with the batch size 256. All exper-\nProceedings of the Thirtieth International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-21)\n1362\nMethod Direct\nDiscuss Eating Greet Phone Photo Pose Purch Sitting SittingD Smoke Wait WalkD Walk WalkT Avg. #\nPa\nvlakos [Pavlakos et al., 2017] 67.4 71.9 66.7 69.1 72.0 77.0 65.0 68.3 83.7 96.5 71.7 65.8 74.9 59.1 63.2 71.9\nFang [Fang et al., 2018] 50.1 54.3 57.0 57.1 66.6 73.3 53.4 55.7 72.8 88.6 60.3 57.7 62.7 47.5 50.6 60.4\nYang [Yang et al., 2018] 51.5 58.9 50.4 57.0 62.1 65.4 49.8 52.7 69.2 85.2 57.4 58.4 43.6 60.1 47.7 58.6\nLee [Lee et al., 2018] 43.8 51.7 48.8 53.1 52.2 74.9 52.7 44.6 56.9 74.3 56.7 66.4 47.5 68.4 45.6 55.8\nTrumble [Trumble et al., 2018] 41.7 43.2 52.9 70.0 64.9 83.0 57.3 63.5 61.0 95.0 70.0 62.3 66.2 53.7 52.4 62.5\nChen [Chen et al., 2019] 45.9 53.5 50.1 53.2 61.5 72.8 50.7 49.4 68.4 82.1 58.6 53.9 57.6 41.1 46.0 56.9\nWandt [Wandt and Rosenhahn, 2019] 50.0 53.5 44.7 51.6 49.0 58.7 48.8 51.3 51.1 66.0 46.6 50.6 42.5 38.8 60.4 50.9\nZhao [Zhao et al., 2019] 37.8 49.4 37.6 40.9 45.1 41.4 40.1 48.3\n50.1 42.2 53.5 44.3 40.5 47.3 39.0\n43.8\nZhou [Zhou et al., 2019] 34.4 42.4 36.6 42.1 38.2 39.8 34.7 40.2\n45.6 60.8 39.0 42.6 42.0 29.8 31.7 39.9\nWu [W\nu and Xiao, 2020] 34.9 40.8 37.5 47.2\n41.5 46.6 35.9 39.5 52.6 72.5\n42.3 45.8 42.0 31.6 33.8 43.2\nLiu [Liu et\nal., 2020] 42.1 45.6 38.2 41.4 41.5 47.4 45.8 39.9 44.7 53.0 42.6\n44.0 42.1 34.0 37.6 42.7\nPoseGT\nAC (Ours) 37.2 42.2 32.6 38.6\n38.0 44.0 40.7 35.2 41.0 45.5 38.2 39.5\n38.2 29.8 33.0 38.2\nTable 1: Quantitative evaluation using Mean Per Joint Position Error (MPJPE) in millimeter between estimated pose and the ground-truth on\nHuman3.6M under Protocol #1. The best results are in bold and the second-best results are underlined.\nMethod Direct\nDiscuss Eating Greet Phone Photo Pose Purch Sitting SittingD Smoke Wait WalkD Walk WalkT Avg. #\nLee [Lee et al.\n, 2018] 38.0 39.3 46.3 44.4 49.0 55.1 40.2 41.1 53.2 68.9 51.0 39.1 33.9 56.4 38.5 46.2\nFang [Fang et al., 2018] 38.2 41.7 43.7 44.9 48.5 55.3 40.2 38.2 54.5 64.4 47.2 44.3 47.3 36.7 41.7 45.7\nRayat [Hossain and Little, 2018] 35.2 40.8 37.2 37.4 43.2 44.0 38.9 35.6 42.3 44.6 39.7 39.7 40.2 32.8 35.5 39.2\nChen [Chen et al., 2019] 36.5 41.0 40.9 43.9 45.6 53.8 38.5 37.3 53.0 65.2 44.6 40.9 44.3 32.0 38.4 44.1\nWandt [Wandt and Rosenhahn, 2019] 33.6 38.8 32.6 37.5 36.0 44.1 37.8 34.9 39.2 52.0 37.5 39.8 34.1 40.3 34.9 38.2\nZhou [Zhou et al., 2019] 29.1 34.9 29.9 32.6 31.2 32.3 27.0 33.3\n37.6 45.9 32.2 31.5 34.5 22.9 25.9 32.1\nWu [W\nu and Xiao, 2020] 29.9 33.6 31.4 37.1\n33.9 36.8 28.4 30.7 42.6\n52.2 35.3 35.2 34.0 24.9 27.9 34.6\nLiu [Liu et al.\n, 2020] 29.6 34.9 31.7 31.6 32.9 37.4\n33.3 30.5 37.6 43.0 34.2 34.3\n33.2 27.0 29.2\n33.4\nPoseGT\nAC (Ours) 25.8 31.7 25.8 29.3 28.8 34.1 29.6 26.4 33.2\n37.2 30.5 30.0 29.8 23.4 25.9 29.4\nTable 2: Quantitative evaluation using P-MPJPE in millimeter between estimated pose and the ground-truth on Human3.6M under Protocol\n#2. Procrustes alignment is used to preprocess the ground-truth. The best results are in bold and the second-best results are underlined.\niments are conducted on PyTorch deep learning framework\nwith a single RTX-2080Ti GPU.\nMethod PCK \" AUC \"\nZhou [Zhou et al., 2017] 69.2 32.5\nYang [Yang et al., 2018] 69.0 32.0\nPavlakos [Pavlakos et al., 2018] 71.9 35.3\nHabibie [Habibie et al., 2019] 70.4 36.0\nLiu [Liu et al., 2020] 74.9 37.5\nPoseGTAC (Ours) 76.4 39.3\nTable 3: Quantitative evaluation on MPI-INF-3DHP dataset using\nPCK and AUC. The higher values mean better performance. The\nbest results are in bold and the second-best results are underlined.\n4.2 Quantitative Results\nTo better evaluate the performance of our proposed PoseG-\nTAC model, we show quantitative results and compare them\nto the state-of-the-art methods on 3D human pose estimation.\nTable 1 and Table 2 show the experiment results on the\nHuman3.6M dataset for Protocol #1and Protocol #2, respec-\ntively. We can observe that the proposed PoseGTAC method\noutperforms all the compared models on both two proto-\ncols and obtains the best results in terms of the average and\nmost individual actions. In particular, the MPJPE reduces by\n1:7mm with an error reduction of 4:3%, and the P-MPJPE\ndecreases by 2:7mm with an 8:4% error reduction, respec-\ntively. It proves that our PoseGTAC extracts rich multi-scale\ncontext and promotes long-range feature interaction.\nMoreover, we also provide supplementary to evaluate the\nsmoothness of the predicted pose by our PoseGTAC model\nas Protocol #3. Additionally, the results on Protocol #3also\nreÔ¨Çect the effectiveness and precision of our model from the\nside. As reported in Table 4, the MPJVE in our PoseGTAC\n0\n1\n2\nRFoot\n4\n5\nLFoot\n7\n8\n9\n10\n11\nLWrist\n13\n14\nRWrist\n0\n1\n2\nRFoot\n4\n5\nLFoot\n7\n8\n9\n10\n11\nLWrist\n13\n14\nRWrist\n 0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(a) Adjacency matrix of GAC.\nHip\nRHip\n2\n3\n4\n5\n6\nSpine\n8\n9\n10\n11\n12\n13\n14\n15\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n 0.0\n0.2\n0.4\n0.6\n0.8\n1.0 (b) Attention matrix of GTL.\nFigure 5: The learned weight matrices of our PoseGTAC model: (a)\nthe adjacency matrix of graph atrous convolution (GAC) and (b) the\nattention matrix of graph transformer layer (GTL).\nmodel reduces the results of the previous work [Pavllo et al.,\n2019] by 80%, which clearly validates that the pose predicted\nby our PoseGTAC model is smoother and more precise.\nFor the MPI-INF-3DHP dataset, we train our model with\nHuman3.6M data without post-process of Ô¨Åne-tuning or re-\ntraining. As shown in Table 3, comparing to the best coun-\nterpart of [Liu et al., 2020], our PoseGTAC model achieves\nan improvement of 1:5% PCK and 1:8% AUC, which consis-\ntently indicates the effectiveness of our PoseGTAC model.\n4.3 Ablation Studies\nEffectiveness of Each Module. To verify the effectiveness\nof different modules in our proposed model, we conduct a\nseries of ablation studies on the Human3.6M dataset under\nProtocol #1. For the ablation of the overall network architec-\nture, we take SemGCN [Zhao et al., 2019] as our baseline.\nWe set whether to use or combine graph transformer layer\n(GTL) and graph atrous convolution (GAC), and Ô¨Ånally com-\nProceedings of the Thirtieth International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-21)\n1363\nMethod Direct\nDiscuss Eating Greet Phone Photo Pose Purch Sitting SittingD Smoke Wait WalkD Walk WalkT Avg. #\nPa\nvllo* [Pavllo et al., 2019] 12.8 12.6 10.3 14.2 10.2 11.3 11.8 11.3 8.2 10.2 10.3 11.3 13.1 13.4 12.9 11.6\nPoseGT\nAC (Ours) 2.3 2.6 1.9 2.9 1.9 2.1 2.3 2.4 1.3 1.7 1.8 2.2 3.4 3.6 2.9 2.3\nTable 4: Quantitative evaluation using Mean Per Joint Velocity Error (MPJVE) between estimated pose and the ground-truth on Human3.6M\nunder Protocol #3. The best results are in bold. * denotes the methods based on single frame.\nMethod MPJPE # Branches MPJPE #\nGTL w/o PE & GA 41.5 4s 39.3\nGTL + PE 40.2 4s+pool 38.9\nGTL + PE + GA 39.4 6s 39.6\nTable 5: The MPJPE obtained by our proposed model with different\nconÔ¨Ågurations of GAC and GTL.\nMethod MPJPE (mm) #\nSemGCN [Zhao et al., 2019] 43.8\nwith GAC 38.9\nwith GTL 39.4\nwith GAC & GTL 38.6\nPoseGTAC (Ours) 38.2\nTable 6: Ablation study on the Human3.6M dataset for the MPJPE\nbetween the predicted pose and the ground-truth.\npare our PoseGTAC model with the encoder-decoder frame-\nwork. As shown in Table 6, compared to our baseline, adding\nonly GAC or GTL is able to obtain remarkable error reduc-\ntion, and adopting the encoder-decoder framework achieves\nthe best results.\nWe further explore the conÔ¨Åguration of the two main\nmodules we designed in detail. Table 5 shows the perfor-\nmance comparison of GAC module with different numbers of\nbranches and whether the pooling is used in GAC. We conÔ¨Åg-\nure our GAC with four branches (i.e., ‚Äú4s‚Äù) and six branches\n(i.e., ‚Äú6s‚Äù). By contrast, the error of our GAC with four\nbranches is the lowest, and then is further reduced by adding\npooling (i.e., ‚Äú4s+pool‚Äù). We also evaluate the components of\nGTL, including the position encoding (PE) and global atten-\ntion (GA). Based on our ‚Äúvanilla‚Äù GTL, adding the position\nencoding or global attention can obtain 1:3% and 0:8% error\nreduction, showing that the position encoding can well com-\nplement the semantic information in the joint sequence.\nVisualization of the Learned Matrices. Furthermore, to ex-\nplore how the information is aggregated between the joints,\nwe visualize the learned weight matrices of the Ô¨Årst GAC and\nGTL of our PoseGTAC model. SpeciÔ¨Åcally, we obtain the Ô¨Å-\nnal adjacency matrix in Fig. 5(a) by adding the weight matri-\nces of the four branches of GAC. From the adjacency matrix,\ni.e., the GAC Ô¨Ålter, we can see that the receptive Ô¨Åeld of our\nÔ¨Ålter is sufÔ¨Åciently enlarged and contains all joints from the\nroot to the 3-hop neighbors. It is probable that our model fo-\ncuses more on the wrists and feet that incorporate multi-scale\ninformation. In addition, the attention matrix is obtained by\nadding the attention map and global attention matrix in GTL.\nAs shown in Fig. 5(b), long-range relationships can be effec-\ntively extracted in each joint, especially in the hip and spine.\nIt indicates that our proposed model not only focuses on lo-\ncal multi-scale contextual information, but also concentrate\nFigure 6: Qualitative results obtained by our proposed PoseGTAC\non the Human3.6M dataset.\non global long-range relationships.\nQualitative Pose Estimation Results. We Ô¨Ånally illustrate\ntypical visualizations of the poses predicted by our PoseG-\nTAC model in Fig. 6, where Ô¨Åve different actions, ‚Äúgreeting‚Äù,\n‚Äúposing‚Äù, ‚Äúsitting‚Äù, and ‚Äúpurchases‚Äù are included. We can\nobserve that the poses for all sequences predicted by PoseG-\nTAC are considerably accurate comparing to the ground-truth\nannotations, even in the challenging scenario with occlusion\nproblem (e.g., ‚Äúpurchases‚Äù).\n5 Conclusion\nIn this work, we proposed a novel Graph Transformer\nEncoder-Decoder with Atrous Convolution named PoseG-\nTAC to extract effectively multi-scale contextual information\nand capture accurately global long-range relationships for 3D\nhuman pose estimation. Moreover, we designed two modules,\ngraph atrous convolution (GAC) and graph transformer layer\n(GTL), respectively for the extraction of multi-scale and long-\nrange information, and combine them in the encoder-decoder\nstructure. The extensive experiments on the Human3.6M and\nMPI-INF-3DHP datasets validated that our PoseGTAC model\ncan extract abundant multi-scale and global long-range infor-\nmation, which is beneÔ¨Åcial for 3D human pose estimation.\nAcknowledgements\nThis work was supported in part by the National Natural Sci-\nence Foundation of China under Grants 61976049, 62072080\nand U20B2063; the Sichuan Science and Technology Pro-\ngram 2018GZDZX0032, 2019ZDZX0008, 2019YFG0003,\n2019YFG0533 and 2020YFS0057.\nProceedings of the Thirtieth International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-21)\n1364\nReferences\n[Carion et al., 2020] Nicolas Carion, Francisco Massa, Gabriel\nSynnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers. In\nECCV, pages 213‚Äì229, 2020.\n[Chen et al., 2018] Liang-Chieh Chen, Yukun Zhu, George Papan-\ndreou, Florian Schroff, and Hartwig Adam. Encoder-decoder\nwith atrous separable convolution for semantic image segmen-\ntation. In ECCV, pages 833‚Äì851, 2018.\n[Chen et al., 2019] Xipeng Chen, Kwan-Yee Lin, Wentao Liu,\nChen Qian, and Liang Lin. Weakly-supervised discovery of\ngeometry-aware representation for 3d human pose estimation. In\nCVPR, pages 10895‚Äì10904, 2019.\n[Fang et al., 2018] Haoshu Fang, Yuanlu Xu, Wenguan Wang, Xi-\naobai Liu, and Song-Chun Zhu. Learning knowledge-guided\npose grammar machine for 3d human pose estimation. In AAAI,\n2018.\n[Habibie et al., 2019] Ikhsanul Habibie, Weipeng Xu, Dushyant\nMehta, Gerard Pons-Moll, and Christian Theobalt. In the wild\nhuman pose estimation using explicit 2d features and intermedi-\nate 3d representations. In CVPR, pages 10905‚Äì10914, 2019.\n[Hamilton et al., 2017] William L. Hamilton, Zhitao Ying, and Jure\nLeskovec. Inductive representation learning on large graphs. In\nNIPS, 2017.\n[Hossain and Little, 2018] Mir Rayat Imtiaz Hossain and James J.\nLittle. Exploiting temporal information for 3d human pose esti-\nmation. In ECCV, pages 69‚Äì86, 2018.\n[Ionescu et al., 2014] Catalin Ionescu, Dragos Papava, Vlad Olaru,\nand Cristian Sminchisescu. Human3.6m: Large scale datasets\nand predictive methods for 3d human sensing in natural environ-\nments. IEEE Trans. Pattern Anal. Mach. Intell., pages 1325‚Äì\n1339, 2014.\n[Ji et al., 2019] Yanli Ji, Yue Zhan, Yang Yang, Xing Xu, Fumin\nShen, and Heng Tao Shen. A knowledge map guided coarse-to-\nÔ¨Åne action recognition. In IEEE Trans. Image Processing, 2019.\n[Kipf and Welling, 2017] Thomas N. Kipf and Max Welling. Semi-\nsupervised classiÔ¨Åcation with graph convolutional networks. In\nICLR, 2017.\n[Lee et al., 2018] Kyoungoh Lee, Inwoong Lee, and Sanghoon Lee.\nPropagating LSTM: 3d pose estimation based on joint interde-\npendency. In ECCV, pages 123‚Äì141, 2018.\n[Lee et al., 2019] Junhyun Lee, Inyeop Lee, and Jaewoo Kang.\nSelf-attention graph pooling. In ICML, 2019.\n[Li et al., 2020] Yonglu Li, Xinpeng Liu, Han Lu, Shiyi Wang,\nJunqi Liu, Jiefeng Li, and Cewu Lu. Detailed 2d-3d joint repre-\nsentation for human-object interaction. In CVPR, pages 10163‚Äì\n10172, 2020.\n[Liu et al., 2020] Shengyuan Liu, Pei Lv, Yuzhen Zhang, Jie Fu,\nJunjin Cheng, Wanqing Li, Bing Zhou, and Mingliang Xu. Semi-\ndynamic hypergraph neural network for 3d pose estimation. In\nIJCAI, pages 782‚Äì788, 2020.\n[Mao et al., 2019] Wei Mao, Miaomiao Liu, Mathieu Salzmann,\nand Hongdong Li. Learning trajectory dependencies for human\nmotion prediction. In ICCV, pages 9488‚Äì9496, 2019.\n[Mehta et al., 2017] Dushyant Mehta, Helge Rhodin, Dan Casas,\nPascal Fua, Oleksandr Sotnychenko, Weipeng Xu, and Christian\nTheobalt. Monocular 3d human pose estimation in the wild using\nimproved CNN supervision. In 3DV, pages 506‚Äì516, 2017.\n[Pavlakos et al., 2017] Georgios Pavlakos, Xiaowei Zhou, Kon-\nstantinos G. Derpanis, and Kostas Daniilidis. Coarse-to-Ô¨Åne vol-\numetric prediction for single-image 3d human pose. In CVPR,\npages 1263‚Äì1272, 2017.\n[Pavlakos et al., 2018] Georgios Pavlakos, Xiaowei Zhou, and\nKostas Daniilidis. Ordinal depth supervision for 3d human pose\nestimation. In CVPR, pages 7307‚Äì7316, 2018.\n[Pavllo et al., 2019] Dario Pavllo, Christoph Feichtenhofer, David\nGrangier, and Michael Auli. 3d human pose estimation in video\nwith temporal convolutions and semi-supervised training. In\nCVPR, pages 7753‚Äì7762, 2019.\n[Qiu et al., 2019] Haibo Qiu, Chunyu Wang, Jingdong Wang,\nNaiyan Wang, and Wenjun Zeng. Cross view fusion for 3d human\npose estimation. In ICCV, pages 4341‚Äì4350, 2019.\n[Trumble et al., 2018] Matthew Trumble, Andrew Gilbert, Adrian\nHilton, and John P. Collomosse. Deep autoencoder for combined\nhuman pose estimation and body model upscaling. In ECCV,\npages 800‚Äì816, 2018.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki Par-\nmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NIPS,\npages 5998‚Äì6008, 2017.\n[Wandt and Rosenhahn, 2019] Bastian Wandt and Bodo Rosen-\nhahn. Repnet: Weakly supervised training of an adversarial re-\nprojection network for 3d human pose estimation. In CVPR,\npages 7782‚Äì7791, 2019.\n[Wang et al., 2018] Xiaolong Wang, Ross B. Girshick, Abhinav\nGupta, and Kaiming He. Non-local neural networks. In CVPR,\npages 7794‚Äì7803, 2018.\n[Wu and Xiao, 2020] Haiping Wu and Bin Xiao. 3d human pose\nestimation via explicit compositional depth maps. InAAAI, pages\n12378‚Äì12385, 2020.\n[Yan et al., 2018] Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spa-\ntial temporal graph convolutional networks for skeleton-based ac-\ntion recognition. In AAAI, 2018.\n[Yang et al., 2018] Wei Yang, Wanli Ouyang, Xiaolong Wang,\nJimmy S. J. Ren, Hongsheng Li, and Xiaogang Wang. 3d human\npose estimation in the wild by adversarial learning. In CVPR,\npages 5255‚Äì5264, 2018.\n[Yu and Koltun, 2016] Fisher Yu and Vladlen Koltun. Multi-scale\ncontext aggregation by dilated convolutions. In ICLR, 2016.\n[Zhao et al., 2019] Long Zhao, Xi Peng, Yu Tian, Mubbasir Kapa-\ndia, and Dimitris N. Metaxas. Semantic graph convolutional net-\nworks for 3d human pose regression. InCVPR, pages 3425‚Äì3435,\n2019.\n[Zhou et al., 2017] Xingyi Zhou, Qixing Huang, Xiao Sun, Xi-\nangyang Xue, and Yichen Wei. Towards 3d human pose esti-\nmation in the wild: A weakly-supervised approach. In ICCV,\npages 398‚Äì407, 2017.\n[Zhou et al., 2019] Kun Zhou, Xiaoguang Han, Nianjuan Jiang,\nKui Jia, and Jiangbo Lu. Hemlets pose: Learning part-centric\nheatmap triplets for accurate 3d human pose estimation. InICCV,\npages 2344‚Äì2353, 2019.\nProceedings of the Thirtieth International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-21)\n1365",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6691446304321289
    },
    {
      "name": "Graph",
      "score": 0.6223981380462646
    },
    {
      "name": "Encoder",
      "score": 0.6173702478408813
    },
    {
      "name": "Transformer",
      "score": 0.5410245060920715
    },
    {
      "name": "Pose",
      "score": 0.4938686490058899
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.45844578742980957
    },
    {
      "name": "Pooling",
      "score": 0.4417303800582886
    },
    {
      "name": "Artificial intelligence",
      "score": 0.42198747396469116
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3240697383880615
    },
    {
      "name": "Algorithm",
      "score": 0.3234155774116516
    },
    {
      "name": "Engineering",
      "score": 0.07799592614173889
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}