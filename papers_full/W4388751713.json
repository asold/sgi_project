{
  "title": "Large Language Models for Qualitative Research in Software Engineering: Exploring Opportunities and Challenges",
  "url": "https://openalex.org/W4388751713",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2119523307",
      "name": "Muneera Bano",
      "affiliations": [
        "Commonwealth Scientific and Industrial Research Organisation",
        "Data61"
      ]
    },
    {
      "id": "https://openalex.org/A2109764959",
      "name": "Rashina Hoda",
      "affiliations": [
        "Monash University",
        "Australian Regenerative Medicine Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2170890522",
      "name": "Didar Zowghi",
      "affiliations": [
        "Commonwealth Scientific and Industrial Research Organisation",
        "Data61"
      ]
    },
    {
      "id": "https://openalex.org/A293351943",
      "name": "Christoph Treude",
      "affiliations": [
        "University of Melbourne"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4321351832",
    "https://openalex.org/W4387928717",
    "https://openalex.org/W4383818780",
    "https://openalex.org/W6853576525",
    "https://openalex.org/W4366594543",
    "https://openalex.org/W4383498485",
    "https://openalex.org/W1564623840",
    "https://openalex.org/W4385837349",
    "https://openalex.org/W2204762366",
    "https://openalex.org/W3141666254",
    "https://openalex.org/W4386081573",
    "https://openalex.org/W4379539707",
    "https://openalex.org/W4376643691",
    "https://openalex.org/W4388092696",
    "https://openalex.org/W4367059011",
    "https://openalex.org/W6603091391",
    "https://openalex.org/W4317910584",
    "https://openalex.org/W4327993060",
    "https://openalex.org/W4376643711",
    "https://openalex.org/W2141437167",
    "https://openalex.org/W4365816617"
  ],
  "abstract": "Abstract The recent surge in the integration of Large Language Models (LLMs) like ChatGPT into qualitative research in software engineering, much like in other professional domains, demands a closer inspection. This vision paper seeks to explore the opportunities of using LLMs in qualitative research to address many of its legacy challenges as well as potential new concerns and pitfalls arising from the use of LLMs. We share our vision for the evolving role of the qualitative researcher in the age of LLMs and contemplate how they may utilize LLMs at various stages of their research experience.",
  "full_text": "Page 1/12\nLarge Language Models for Qualitative Research in\nSoftware Engineering: Exploring Opportunities and\nChallenges\nMuneera Bano  (  muneera.bano@csiro.au )\nCSIRO's Data61\nRashina Hoda \nMonash University\nDidar Zowghi \nCSIRO's Data61\nChristoph Treude \nUniversity of Melbourne\nResearch Article\nKeywords:\nPosted Date: November 17th, 2023\nDOI: https://doi.org/10.21203/rs.3.rs-3614628/v1\nLicense:     This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: No competing interests reported.\nVersion of Record: A version of this preprint was published at Automated Software Engineering on\nDecember 21st, 2023. See the published version at https://doi.org/10.1007/s10515-023-00407-8.\nPage 2/12\nAbstract\nThe recent surge in the integration of Large Language Models (LLMs) like ChatGPT into qualitative\nresearch in software engineering, much like in other professional domains, demands a closer inspection.\nThis vision paper seeks to explore the opportunities of using LLMs in qualitative research to address\nmany of its legacy challenges as well as potential new concerns and pitfalls arising from the use of\nLLMs. We share our vision for the evolving role of the qualitative researcher in the age of LLMs and\ncontemplate how they may utilize LLMs at various stages of their research experience.\nIntroduction\nThe advent of Large Language Models (LLMs) such as OpenAI’s ChatGPT and Google’s Bard has been\nnothing short of a paradigm shift in academia, much like in many other professions. Within a year of their\ninception, these models have become a focal point of academic scrutiny, with researchers exploring their\npotential across a plethora of domains (Bano, Zowghi, and Whittle 2023). From analyzing its pivotal role\nin research and academia to understanding its transformative potential in educational settings, the\nemerging body of literature paints an intriguing picture of the far-reaching implications of LLMs.\nWe found an increased interest from researchers in the use of LLMs in Software Engineering (SE), and the\nwork is characterized by a multifaceted evaluation of potential bene\u0000ts and inherent challenges. Ozkaya\n(2023) projects an AI-augmented software development lifecycle, with AI assistants contributing to\nvarious SE tasks like speci\u0000cation generation and legacy code translation. Concurrently, Jalil et al. (2023)\nanalyze ChatGPT's role in software testing education, revealing its potential and the risks of overreliance\ndue to varying response accuracy. Ebert and Louridas (2023) discuss how generative AI can automate SE\ntasks, urging a balanced integration considering ethical and privacy concerns. Scoccia (2023) gathers\nearly adopter experiences with ChatGPT's code generation, indicating its signi\u0000cant impact yet mixed\nusage outcomes. The empirical study by Kuhail et al. (2023) presents a nuanced perspective on AI's role\nin SE, suggesting increased trust with frequent use but also heightened job security concerns. Arora et al.\n(2023) propose a SWOT analysis for LLMs in Requirements Engineering, suggesting a cautiously\noptimistic view towards AI's role in elicitation and validation. Nguyen-Duc et al. (2023) outline a research\nagenda for Generative AI, emphasizing its potential for partial automation in SE tasks. Finally, Hou et al.\n(2023) provide a systematic literature review of LLMs in SE offering a comprehensive roadmap for future\nresearch and practical applications in SE. Each of these contributions underscores the transformative\npotential of LLMs in SE, while also acknowledging the complexity of their integration and the need for\nongoing research to navigate the challenges they present.\nHowever, augmenting research processes with LLMs is yet to be fully investigated. In the evolving\nlandscape of SE research, the intertwining of technological advancements with human-driven insights\nhas been a constant. While no one can claim they saw the exact nature and shape of LLMs emerging,\npredictions have been made about “further advancements in technology and arti\u0000cial intelligence”\noffering “unexplored potential in supplementing, augmenting, and automating parts of qualitative data\nPage 3/12\nanalysis to ease human effort and improve both the quality and scale of theory development” (Hoda\n2021).\nThe work of Byun et al. (2023) shows that LLMs like GPT-3 have the capacity to produce text that is\ncomparable to that written by humans, even in qualitative analysis, which traditionally relies heavily on\nhuman insight. Their work demonstrates that AI can not only generate text but also identify themes and\nprovide detailed analysis similar to that of human researchers. They suggest that AI could potentially\nmatch human capabilities in interpreting qualitative data. Their \u0000ndings indicate a promising avenue for\nusing AI in qualitative research, where it could serve as a tool to both augment and potentially replace\nhuman analysis, raising important questions about the future role of humans in research processes. \nBano et al. (2023) challenge these claims. They acknowledge the potential of AI to align with human\nanalysis in some cases but caution against an overreliance on AI due to signi\u0000cant disparities between AI\nand human reasoning. Their study reveals that while AI, speci\u0000cally LLMs like ChatGPT 3.5 and GPT-4,\ncan sometimes provide logical classi\u0000cations, there is often a lack of consensus between AI-generated\nand human-generated insights, raising questions about the AI's capability to fully grasp the complexities\nof human language and the contextual nuances important in qualitative research.\nDespite preliminary progress, there remains a signi\u0000cant lack of clarity on how LLMs compare to human\nintelligence in qualitative research (Bender et al. 2021). The role of LLMs in SE qualitative research\npresents both unprecedented opportunities and inherent risks, especially when viewed from the\nperspective of researchers at different stages of their academic journey.\nAddressing Legacy Challenges of SE Qualitative Research with LLMs\nHistorically, qualitative research in SE has grappled with challenges like the time-intensive nature of the\nwork, limitations to scalability due to its manual nature, and the inherent subjectivity that qualitative\nmethodologies can sometimes entail.\nTime-intensive Work\nConducting qualitative research often requires intensive data analysis, which can be time-consuming.\nLLMs can help to automate or expedite parts of these processes. For example, they could help to make\nsense of large amounts of textual data, identify themes and patterns within data, and generate initial\ncodes or categories. Such technical assistance could signi\u0000cantly speed up the data analysis process\nand allow researchers to handle larger datasets, thereby allowing them to scale qualitative analysis in\nways hardly possible through a commensurate amount of manual effort\nGeneralizability\nQualitative research is hard to generalize universally or to wider populations outside the originally studied\ncontext, which is typically a relatively narrow phenomenon. Based on the constructive worldview, it may\neven be undesirable. However, the use of AI-based models and advanced natural language processing,\nPage 4/12\nsuch as those offered by LLMs, can help improve the relevance and generalizability of the qualitative\n\u0000ndings, such as descriptive \u0000ndings, taxonomies, and theories by expanding the contexts studied (Hoda\n2021).\nConsistency\nVariations in qualitative data analysis are expected to exist across different researchers, but consistency\ncan still be an issue for individual researchers. Depending on several factors not limited to external and\npersonal circumstances, achieving high levels of human consistency is a known challenge (Gentles et al.\n2015; Watson 2006). On the other hand, LLMs, being computing entities, can process and analyze data in\na consistent way, considering the consistency in prompts. Improved consistency is likely to lend itself to\nbetter repeatability of the process and higher reproducibility of the research outcomes. This may be\nparticularly desirable from a positive perspective.\nSubjectivity\nWhile it may be impossible or even undesirable to eliminate human subjectivity from qualitative research,\nLLMs could potentially add an additional layer to the analysis. For example, the use of LLMs can help a\nteam of qualitative researchers discuss and agree on the concepts emerging from their individual\nanalyses. Furthermore, the concepts generated by LLMs can act as a ‘third party’ reference to help\naddress and reconcile differences emerging from personal beliefs, experiences, or emotions. It seems\nearly and somewhat naive to suggest that an LLM can act as an objective baseline or a source of a\ndeciding ‘expert opinion’. LLMs, like humans, are known to harbor their own set of biases based on the\ntraining data and parameters that can in\u0000uence their inference logic when it comes to qualitative\nresearch (Navigli, Conia, and Ross 2023). With rapid enhancements in LLM capabilities, these aspects\ncan be reexamined in the future.\nNew Frontiers, New Challenges\nWhile LLMs may seem to be the panacea for many traditional qualitative research issues, they bring with\nthem a set of unique challenges. We summarize these below.\nEthical and Privacy Concerns\nIncorporating LLMs into data analysis poses ethical and privacy challenges, especially with sensitive\ndata. Ethical issues include ensuring data consent, proper anonymization to enable de-identi\u0000cation, and\naddressing biases that AI may perpetuate (Arora, Grundy, and Abdelrazek 2023; Ebert and Louridas 2023).\nThese concerns necessitate a responsible AI framework that respects individual privacy and data rights.\nFor ethical usage, Nguyen-Duc et al. (2023) recommend integrating AI with an awareness of ethical\nimplications and privacy risks, such as by using AI to enhance rather than replace human decision-\nmaking, and keeping sensitive raw data local to avoid exposure. Ozkaya (2023) further suggests robust\nPage 5/12\ndata governance to ensure AI applications adhere to ethical standards and privacy regulations, balancing\nAI's potential with necessary oversight.\nModel Biases\nLike all machine learning models, LLMs can have inherent biases based on the data they were trained on,\nwhich can be \u0000awed or insu\u0000cient. This could potentially skew the analysis or conclusions drawn from\ntheir use in qualitative research. For example, in SE qualitative research, if an LLM is trained on data that\npredominantly consists of contributions from male developers, it may inadvertently downplay or overlook\nthe communication styles, coding preferences, or problem-solving approaches more common among\nfemale developers or those from underrepresented groups. In such cases, researchers have the\nresponsibility to be aware of and acknowledge the inherent biases in the underlying data on which the\nLLMs are trained, as part of the limitations of their research.\nLack of Contextual and Philosophical Understanding\nWhile LLMs can process and generate text based on patterns learned, they lack a true understanding of\nthe context, which is crucial in qualitative research. This could lead to oversights and misinterpretations.\nFor example, in a SE qualitative study analyzing developer communication on issue trackers, an LLM\nmight interpret technical jargon or project-speci\u0000c slang literally, missing the nuanced meaning intended\nby the developers. While LLMs could identify and summarize discussions on a given research topic from\nvarious sources, articles, and grey literature, but they might not fully grasp the subtleties of concerns that\nrequire a deeper philosophical understanding and contextual awareness, which human researchers\nprovide. In such cases, the researchers should be paying special attention to any missing or\nmisinterpreted contexts.\nDependency on Technology\nThere is a risk of becoming overly dependent on technology for research. While LLMs can assist in data\nanalysis, they should not replace the human element of research, which includes critical thinking,\ncontextual understanding, and ethical judgment (Bano et al. 2023). To educate and train the next\ngeneration of qualitative researchers it is important to not overly rely on augmented research\ntechnologies such as LLMs. We elaborate further on the level of expertise of researchers later in this\npaper.\nQuality Control\nEnsuring the quality and accuracy of the results generated by LLMs can be challenging. Researchers\nneed to be vigilant and critical when interpreting the outputs of LLMs. For example, ChatGPT is known to\nbe prone to hallucinations, instances where LLMs generate inaccurate or entirely fabricated information.\nNot checking for inaccurate and fake information generated by LLMs can land researchers in trouble. To\naddress the issue of hallucinations the involvement of human researchers is imperative. As pointed out\nby Rudolph et al. (2023) and Alkaissi and McFarlane (2023), these hallucinations can lead to\nPage 6/12\nmisinterpretation of research outcomes, compromise the validity of results, and introduce bias or error. To\ncounteract this, researchers must scrutinize, verify, and interpret the outputs of LLMs meticulously,\nensuring that the conclusions are aligned with the actual context and maintain the integrity of the\nresearch. This human intervention is necessary not only for validation but also to continually re\u0000ne and\ncalibrate the models, thereby improving their understanding and minimizing potential drawbacks\n(Watkins 2023).\nReproducibility\nAs LLMs are continuously updated, and old models are deprecated, the ability to reproduce an analysis\nwith the same precision diminishes over time, a phenomenon known as model drift. Researchers may\nprovide exhaustive details on their methodology, including data sets, prompts, parameters, and the\nversions of models used, but this does not guarantee that the same analysis can be reproduced in the\nfuture by LLMs. Unlike human researchers, where insights and analytical reasoning can be revisited or\nclari\u0000ed, LLMs do not offer the possibility to revisit the reasoning behind their outputs once the model\nversion is no longer available.\nContext of Related Work\nIntegrating an LLM’s data analysis within the broader context of related work poses a signi\u0000cant\nchallenge, primarily because the model cannot access the entirety of potentially relevant literature due to\nconstraints on data availability and access rights due to paywalls. This limitation hampers the LLM’s\nability to draw comprehensive connections and insights that are informed by the existing research,\npotentially narrowing the scope and depth of its analytical outputs. In the future, if LLMs are capable of\nhandling large quantities of raw data from literature along with the context of related work, this could\nlead to augmenting systematic literature reviews (Kitchenham 2004) with LLMs.\nCritical Thinking\nDeveloping critical thinking in LLMs is a complex challenge, as it involves the model's exposure to a\nvariety of data, including incorrect statements, to enhance its evaluative capabilities (Emmert-Streib\n2023). To ensure LLMs are exposed to such a range of data, researchers could deliberately include\ndatasets with known errors or contradictory information during the training phase. This method could\npotentially help LLMs learn to discern and evaluate the accuracy of information they analyze. However,\nthis approach also raises concerns about how to effectively teach LLMs to recognize and appropriately\nhandle incorrect information without perpetuating or amplifying these errors in their outputs. Currently, it’s\nunclear how critical thinking might be incorporated in LLMs when analysing qualitative data.\nIntellectual property (IP): IP concerns are another dimension to consider in the use of LLMs in research.\nThe contribution of LLMs’ responses and analyses to the creation of a research output could raise\nquestions about authorship, such as whether ChatGPT should be credited as a co-author, re\u0000ecting the\nmodel's role in data processing and knowledge generation (Balel 2023). Another layer of complication is\nPage 7/12\nthe copyright and IP of the data on which LLMs are trained on. Determining the extent of LLMs’\ncontribution, and that of underlying sources, and its implications for IP rights and academic recognition is\nan ongoing debate in the research community (Polonsky and Rotman 2023).\nThe Evolving Role of Human Researcher\nAmid the LLM revolution, the role of the human researcher is undergoing a nuanced shift.\nEnsuring Ethical Practices\nResearchers must ensure that their studies are conducted ethically. This includes obtaining informed\nconsent from participants, ensuring privacy and con\u0000dentiality, and treating the data in a way that\nrespects the rights and dignity of the participants/sources (Watkins 2023).\nPrompt Engineering\nPrompt engineering is emerging as a crucial skill, underscoring the fact that the quality of LLM outputs\nhinges signi\u0000cantly on the inputs it receives. It's important to note that prompt engineering can also be a\nstage where researchers might unintentionally introduce bias, as the way questions are framed can\nin\u0000uence the direction and nature of the LLM's response, potentially reinforcing certain perspectives or\nexcluding others.\nDe\u0000ning Research Questions\nAlthough LLMs can be used to brainstorm research topics and ideas, the researcher must de\u0000ne the\nresearch questions and objectives. An LLM can help process data, but LLMs do not have intellectual\ncuriosity, intention, motivation, or enough information to set research directions, which will depend on the\nresearcher.\nData Collection\nWhile LLMs can help process and analyze large amounts of data, and now, with web searchability can\ncollect data as well, it is still the researcher's responsibility to collect the data in certain qualitative\nresearch contexts such as interviews or surveys. However, in some instances where it is extremely di\u0000cult\nto recruit real participants for research, e.g. in health domain patients with chronic ailments, LLMs can be\nused to simulate and role-play certain personas for data collection. The known limitations of using\npersonas in research, as well as the lack of lived human experience in simulated data, will continue to be\na challenge.\nInterpreting Outputs of the LLM\nAn LLM can pro\u0000ciently identify patterns and themes within a dataset, presenting a synthesized analysis.\nHowever, it remains the domain of a human researcher to ascribe meaning to these \u0000ndings,\ncontextualizing them within the framework of the research objectives. One might wonder why the task of\nPage 8/12\ninterpretation cannot also be delegated to an LLM. The reason lies in the nuanced understanding and\nsubjective judgment required—qualities that are distinctly human and currently beyond the ability of\nLLMs. Additionally, while it is possible for one LLM to analyze another LLM’s output (Jiang, Ren, and Lin\n2023), this still does not replace the depth of insight and complex reasoning a human brings to the\ninterpretation of research data.\nQuality Checking\nIt is important for researchers to check the quality of the work done by the LLM. For instance, they need to\nlook for biases in the analysis and ensure that the LLM is correctly interpreting and coding the data.\nTheorizing\nDeveloping rich theories that are grounded in evidence requires a deep understanding of the data, the\nability to see connections and patterns, and the creativity to formulate a theory. These are all skills that\nare currently beyond the reach of LLMs.\nWriting and Dissemination\nFinally, the researcher is responsible for writing up the results of the study and disseminating them, and is\ngenerally accountable for the research and its results. For example, the Journal of Information and\nSoftware Technology allows the use of Generative AI for improving readability and language, provided\nthat authors have to give explicit acknowledgment statements for the accountability of their produced\nwork.\nThis includes presenting the \u0000ndings in a way that is understandable and useful to others and publishing\nor sharing the results in relevant forums.\nThe Promise of LLMs Across Varied Research Expertise\nQualitative research is often rooted in a constructivist paradigm emphasizing the non-replicable human\ncapacity to understand and contextualize social phenomena (Easterbrook et al. 2008, Hoda 2021). The\nconstructivist paradigm in SE research is concerned with socio-technical realities that are not objective\nbut constructed through human experiences and contexts  . This paradigm values the researcher's role in\ninterpreting data, where their involvement and perspective are considered integral to the analysis,\nespecially in methods like ethnography, participant observation  , and grounded theory.\nQualitative research in SE also offers unique advantages in exploring complex socio-technical processes\nand aiding in theory construction. It can reveal underlying reasons behind intricate socio-technical\ndynamics and is often used to generate new research questions and insights  . These aspects underscore\nthe necessity of the human element in data interpretation, despite the analytical capabilities of LLMs.\nThe expertise of a researcher is crucial across all research modalities, including the application of LLMs,\nas it guides the critical interpretation of data, the strategic questioning that leads to deeper insights, and\nPage 9/12\nthe contextual understanding that LLMs alone cannot provide.\nFurther to the opportunities and challenges presented by LLMs in SE qualitative research discussed\nabove, we present our collective thoughts on how these may vary by the experience level of the\nresearchers. Firstly, and most importantly, with the introduction of LLMs, ethical considerations come to\nthe fore. It is crucial for researchers at all stages to understand and uphold ethical practices, especially\nconcerning data privacy, possible plagiarism, and potential biases that the LLMs might introduce or\nperpetuate (Treude and Hata 2023).\nFor novices in qualitative research in SE, LLMs can be both an assistive tool and a challenge. LLMs can\nbe used to sift through extensive datasets, identify initial patterns, and assist in some basic data coding,\nmaking the initiation phases smoother. However, novice researchers must be cautious. Relying heavily on\nLLMs without understanding the underlying domain of inquiry or the principles of qualitative data\nanalysis can compromise the quality of research outputs and their own capabilities as researchers. It is\nessential to strike a balance to ensure data integrity and true learning of the research process.\nIntermediate researchers will \u0000nd LLMs useful as they dive into more complex data. LLMs can aid in\nidentifying recurring themes and intricate patterns, potentially elevating the quality of the analysis\nthrough its comprehensive approach. However, there is a potential risk of overreliance on the technology,\nleading to overcon\u0000dence in automated outputs. It is crucial for researchers to maintain a critical eye,\nensuring that their growing reliance on LLMs does not overshadow the need for rigorous human oversight\nand contextual interpretation that their increasing experience affords them.\nFor seasoned qualitative researchers, LLMs present an opportunity to explore new breadth and depth\nwithin data analysis. For example, LLMs can be used to scale qualitative research beyond what is\ntypically possible through human effort. Experienced qualitative researchers can boost their practice by\ntaking on larger datasets for analysis, training bespoke LLMs where accessible, and developing\ndescriptive \u0000ndings, taxonomies, and theories that capture a wider range of contexts and are, therefore,\nmore widely generalizable. But with this deeper dive comes a heightened responsibility for research\nintegrity and accountability. The research outputs, while possibly enhanced by LLMs, must be thoroughly\nreviewed for inadvertent errors or biases. Furthermore, while LLMs can handle the heavy lifting of data\nanalysis, experts must remain fully accountable for the interpretations and conclusions drawn.\nFor all levels of researchers, LLMs can expedite the data processing phase, but it is paramount that\nresearchers do not bypass the essential learning and understanding phases of the research process.\nLLMs should be tools to enhance the process, not shortcuts that diminish the depth and richness of\nqualitative research in software engineering. The use of LLMs should not eclipse the importance of\nhuman judgment and insight.\nConclusion\nPage 10/12\nAs LLMs entrench themselves into most disciplines, SE research will not remain untouched. For\nqualitative SE research, LLMs offer a landscape rife with opportunities and challenges. Researchers,\nwhether novices, intermediates, or experts, can embrace the potential of LLMs while remaining vigilant\nand anchored in the core tenets of qualitative inquiry.\nAmidst the rising discourse on the potential threats of AI and LLMs, accentuated by media narratives,\nthere exists a palpable concern within professional communities about AI's capability to replace human\nroles. Contrarily, empirical \u0000ndings (Bano et al. 2023), rooted in an understanding of LLM capabilities and\nextant research, debunk the AI doomsday notion, particularly for qualitative researchers in software\nengineering. We project a harmonious future where LLMs and human researchers collaboratively further\nqualitative research. However, while LLMs, like GPT-4 and ChatGPT, show promise, the irreplaceable role\nof the human researcher in ensuring ethical conduct, well-motivated studies, the validity and reliability of\nresearch \u0000ndings, and appropriate dissemination remains pivotal.\nConsidering the broader interaction between humans and LLMs, while the latter's adeptness in qualitative\ndata analysis can optimize certain facets of research, it is imperative to note their limitations in capturing\nthe intricate nuances inherent to human researchers. This sentiment is echoed in seminal anthropological\nand sociological works that emphasize the human touch in interpreting and understanding data.\nCritically, the ethical considerations surrounding LLM use, ranging from data privacy to intellectual\nproperty rights, call for rigorous scrutiny.\nDeclarations\nAuthor Contribution\nM.B., R.H., and D.Z. contributed to the ideation.M.B. and R.H. wrote the main manuscript text.D.Z. and C.T.\nreviewed, updated, and improved the manuscript.M.B. and R.H. prepared the \u0000nal version.\nReferences\n1. Alkaissi, H., Samy, I.M.F.: 'Arti\u0000cial hallucinations in ChatGPT: implications in scienti\u0000c writing',\nCureus, 15. (2023)\n2. Arora, C., John Grundy, and, Abdelrazek, M.: 'Advancing Requirements Engineering through\nGenerative AI: Assessing the Role of LLMs', arXiv preprint arXiv:2310.13976. (2023)\n3. Balel, Y.: 'The Role of Arti\u0000cial Intelligence in Academic Paper Writing and Its Potential as a Co-\nAuthor', Eur. J. Ther. (2023)\n4. Bano, M., Zowghi, D.: and Jon Whittle. 'Exploring Qualitative Research Using LLMs', arXiv preprint\narXiv:2306.13298. (2023)\n5. Bender, E.M., Timnit Gebru, A., McMillan-Major: and Shmargaret Shmitchell. \"On the Dangers of\nStochastic Parrots: Can Language Models Be Too Big??\" In Proceedings of the 2021 ACM\nPage 11/12\nconference on fairness, accountability, and transparency, 610 – 23. (2021)\n\u0000. Byun, C., Vasicek, P., Kevin Seppi: \"Dispensing with Humans in Human-Computer Interaction\nResearch.\" In Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing\nSystems, 1–26. (2023)\n7. Ebert, C., and Panos Louridas: Generative AI for software practitioners. IEEE Softw. 40, 30–38 (2023)\n\u0000. Easterbrook, S., Singer, J., Storey, M.A., Damian, D.: Selecting empirical methods for software\nengineering research. Guide to advanced empirical software engineering, 285–311. (2008)\n9. Emmert-Streib, F.: 'Importance of critical thinking to understand ChatGPT', Eur. J. Hum. Genet.: 1–2.\n(2023)\n10. Gentles, S.J., Cathy Charles, J., Ploeg, Ann McKibbon, K.: Sampling in qualitative research: Insights\nfrom an overview of the methods literature. qualitative Rep. 20, 1772–1789 (2015)\n11. Hoda, R.: Socio-technical grounded theory for software engineering. IEEE Trans. Software Eng. 48,\n3808–3832 (2021)\n12. Hou, X., Zhao, Y., Liu, Y., Yang, Z., Wang, K., Li, L., Luo, X., Lo, D., Grundy, J., Haoyu Wang: and. 'Large\nlanguage models for software engineering: A systematic literature review', arXiv preprint\narXiv:2308.10620. (2023)\n13. Jalil, S., Ra\u0000, S., LaToza, T.D., Moran, K., Wing Lam: \"Chatgpt and software testing education:\nPromises & perils.\" In 2023 IEEE International Conference on Software Testing, Veri\u0000cation and\nValidation Workshops (ICSTW), 4130-37. IEEE. (2023)\n14. Jiang, D., Ren, X., Bill Yuchen Lin: 'LLM-Blender: Ensembling Large Language Models with Pairwise\nRanking and Generative Fusion', arXiv preprint arXiv:2306.02561. (2023)\n15. Kitchenham, B.: Procedures for performing systematic reviews. Keele, UK, Keele University, 33(2004),\n1–26. (2004)\n1\u0000. Kuhail, M., Amin, S.S., Mathew, A., Khalil, J., Berengueres: and Syed Jawad Shah. '“Will I Be\nReplaced?” Assessing Chatgpt's Effect on Software Development and Programmer Perceptions of Ai\nTools', Assessing Chatgpt's Effect on Software Development and Programmer Perceptions of Ai\nTools\n17. Navigli, R., Conia, S., Björn, Ross: 'Biases in Large Language Models: Origins, Inventory and\nDiscussion', ACM J. Data Inform. Qual. (2023)\n1\u0000. Nguyen-Duc, A., Przybylek, B.C.-D.A., Arora, C., Khanna, D., Herda, T., Ra\u0000q, U.: Jorge Melegati, Eduardo\nGuerra, and Kai-Kristian Kemell. 2023. 'Generative Arti\u0000cial Intelligence for Software Engineering–A\nResearch Agenda', arXiv preprint arXiv:2310.18648\n19. Ozkaya, I.: Application of Large Language Models to Software Engineering Tasks: Opportunities,\nRisks, and Implications. IEEE Softw. 40, 4–8 (2023)\n20. Polonsky, M., Jay, and Jeffrey D Rotman: Should Arti\u0000cial Intelligent Agents be Your Co-author?\nArguments in Favour, Informed by ChatGPT, pp. 91–96. SAGE Publications Sage UK, London,\nEngland (2023)\nPage 12/12\n21. Rudolph, J., Tan, S., Tan, S.: 'ChatGPT: Bullshit spewer or the end of traditional assessments in higher\neducation?', J. Appl. Learn. Teach., 6. (2023)\n22. Scoccia, G.L.: \"Exploring Early Adopters' Perceptions of ChatGPT as a Code Generation Tool.\" In 2023\n38th IEEE/ACM International Conference on Automated Software Engineering Workshops (ASEW),\n88–93. IEEE. (2023)\n23. Treude, C.: and Hideaki Hata. 'She Elicits Requirements and He Tests: Software Engineering Gender\nBias in Large Language Models', arXiv preprint arXiv:2303.10131. (2023)\n24. Watkins, R.: 'Guidance for researchers and peer-reviewers on the ethical use of Large Language\nModels (LLMs) in scienti\u0000c research work\u0000ows', AI and Ethics: 1–6. (2023)\n25. Watson, C.: Unreliable narrators?‘Inconsistency’(and some inconstancy) in interviews. Qualitative\nRes. 6, 367–384 (2006)\nFootnotes\n1. https://www.csiro.au/en/news/all/articles/2023/june/humans-and-ai-hallucinate\n2. https://www.cyberdaily.au/digital-transformation/9779-researchers-apologies-to-big-4-consultancy-\n\u0000rms-for-false-ai-based-accusations\n3. https://c3.ai/glossary/data-science/model-drift/\n4. https://www.sciencedirect.com/journal/information-and-software-technology/publish/guide-for-\nauthors",
  "topic": "Qualitative research",
  "concepts": [
    {
      "name": "Qualitative research",
      "score": 0.7451305985450745
    },
    {
      "name": "Engineering ethics",
      "score": 0.4234694540500641
    },
    {
      "name": "Sociology",
      "score": 0.2743300795555115
    },
    {
      "name": "Engineering",
      "score": 0.2455974519252777
    },
    {
      "name": "Social science",
      "score": 0.2100932002067566
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1292875679",
      "name": "Commonwealth Scientific and Industrial Research Organisation",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I42894916",
      "name": "Data61",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I2801239119",
      "name": "Australian Regenerative Medicine Institute",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I56590836",
      "name": "Monash University",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I165779595",
      "name": "The University of Melbourne",
      "country": "AU"
    }
  ],
  "cited_by": 2
}