{
  "title": "Neural language models as psycholinguistic subjects: Representations of syntactic state",
  "url": "https://openalex.org/W2921890305",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2004367435",
      "name": "Richard Futrell",
      "affiliations": [
        "University of California, Irvine",
        "Language Science (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A2201425052",
      "name": "Ethan Wilcox",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2109140092",
      "name": "Takashi Morita",
      "affiliations": [
        "Kyoto University",
        "Institute of Primate Research"
      ]
    },
    {
      "id": "https://openalex.org/A1975658276",
      "name": "Peng Qian",
      "affiliations": [
        "Institute of Cognitive and Brain Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2159534029",
      "name": "Miguel Ballesteros",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1982032445",
      "name": "Roger Lévy",
      "affiliations": [
        "Institute of Cognitive and Brain Sciences"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2611669587",
    "https://openalex.org/W2949296763",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4255690937",
    "https://openalex.org/W2130494162",
    "https://openalex.org/W2112106114",
    "https://openalex.org/W2121227244",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W2043146406",
    "https://openalex.org/W2054518132",
    "https://openalex.org/W2108010971",
    "https://openalex.org/W2942054564",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2864832950",
    "https://openalex.org/W2888922637",
    "https://openalex.org/W2963073938",
    "https://openalex.org/W2963751529",
    "https://openalex.org/W2076332735",
    "https://openalex.org/W4229741674",
    "https://openalex.org/W2125001590",
    "https://openalex.org/W2061311021",
    "https://openalex.org/W2154424448",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W1955233831",
    "https://openalex.org/W2054125330",
    "https://openalex.org/W2625014264",
    "https://openalex.org/W2118276816",
    "https://openalex.org/W1975081431",
    "https://openalex.org/W2962941914",
    "https://openalex.org/W2795342569",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2741831486",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2768794963",
    "https://openalex.org/W2152068514",
    "https://openalex.org/W2141845152",
    "https://openalex.org/W2798727047",
    "https://openalex.org/W2962961857",
    "https://openalex.org/W2022741721",
    "https://openalex.org/W2606089314",
    "https://openalex.org/W2962733492",
    "https://openalex.org/W2951714314",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W4255682693",
    "https://openalex.org/W1667107303",
    "https://openalex.org/W2531882892",
    "https://openalex.org/W4254816979",
    "https://openalex.org/W4210984920",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2506931122",
    "https://openalex.org/W2474191362",
    "https://openalex.org/W2964335542"
  ],
  "abstract": "Richard Futrell, Ethan Wilcox, Takashi Morita, Peng Qian, Miguel Ballesteros, Roger Levy. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019.",
  "full_text": "Proceedings of NAACL-HLT 2019, pages 32–42\nMinneapolis, Minnesota, June 2 - June 7, 2019.c⃝2019 Association for Computational Linguistics\n32\nNeural Language Models as Psycholinguistic Subjects: Representations of\nSyntactic State\nRichard Futrell1, Ethan Wilcox2, T akashi Morita3,4, Peng Qian5, Miguel Ballesteros6, and Roger Levy5\n1Department of Language Science, UC Irvine, rfutrell@uci.edu\n2Department of Linguistics, Harvard University , wilcoxeg@g.harvard.edu\n3Primate Research Institute, Kyoto University , tmorita@alum.mit.edu\n4Department of Linguistics and Philosophy , MIT\n5Department of Brain and Cognitive Sciences, MIT , {pqian,rplevy}@mit.edu\n6IBM Research, MIT -IBM W atson AI Lab, miguel.ballesteros@ibm.com\nAbstract\nW e investigate the extent to which the behav-\nior of neural network language models reﬂects\nincremental representations of syntactic state.\nT o do so, we employ experimental method-\nologies which were originally developed in\nthe ﬁeld of psycholinguistics to study syntac-\ntic representation in the human mind. W e ex-\namine neural network model behavior on sets\nof artiﬁcial sentences containing a variety of\nsyntactically complex structures. These sen-\ntences not only test whether the networks have\na representation of syntactic state, they also re-\nveal the speciﬁc lexical cues that networks use\nto update these states. W e test four models:\ntwo publicly available LSTM sequence mod-\nels of English (\nJozefowicz et al. , 2016; Gulor-\ndava et al. , 2018) trained on large datasets; an\nRNN Grammar ( Dyer et al. , 2016) trained on a\nsmall, parsed dataset; and an LSTM trained on\nthe same small corpus as the RNNG. W e ﬁnd\nevidence for basic syntactic state representa-\ntions in all models, but only the models trained\non large datasets are sensitive to subtle lexical\ncues signalling changes in syntactic state.\n1 Introduction\nIt is now standard practice in NLP to derive sen-\ntence representations using neural sequence mod-\nels of various kinds (\nElman, 1990; Sutskever et al. ,\n2014; Goldberg, 2017; Peters et al. , 2018; De-\nvlin et al. , 2018). However, we do not yet have a\nﬁrm understanding of the precise content of these\nrepresentations, which poses problems for inter-\npretability , accountability , and controllability of\nNLP systems. More speciﬁcally , the success of\nneural sequence models has raised the question\nof whether and how these networks learn robust\nsyntactic generalizations about natural language,\nwhich would enable robust performance even on\ndata that differs from the peculiarities of the train-\ning set.\nHere we build upon recent work studying neural\nlanguage models using experimental techniques\nthat were originally developed in the ﬁeld of psy-\ncholinguistics to study language processing in\nthe human mind. The basic idea is to examine\nlanguage models’ behavior on targeted sentences\nchosen to probe particular aspects of the learned\nrepresentations. This approach was introduced by\nLinzen et al. (2016), followed more recently by\nothers ( Bernardy and Lappin , 2017; Enguehard\net al. , 2017; Gulordava et al. , 2018), who used\nan agreement prediction task ( Bock and Miller ,\n1991) to study whether RNNs learn a hierarchical\nmorphosyntactic dependency: for example, that\nThe key to the cabinets. . . can grammatically con-\ntinue with was but not with were. This dependency\nturns out to be learnable from a language mod-\neling objective (\nGulordava et al. , 2018). Subse-\nquent work has extended this approach to other\ngrammatical phenomena, with positive results for\nﬁller–gap dependencies (\nChowdhury and Zampar-\nelli, 2018; Wilcox et al. , 2018) and negative results\nfor anaphoric dependencies ( Marvin and Linzen ,\n2018).\nIn this work, we consider syntactic representa-\ntions of a different kind. Previous studies have fo-\ncused on relationships of dependency: one word\nlicenses another word, which is tested by asking\nwhether a language model favors one (grammat-\nically licensed) form over another in a particular\ncontext. Here we focus instead on whether neu-\nral language models show evidence for incremen-\ntal syntactic state representations: whether behav-\nior of neural language models reﬂects the kind\nof generalizations that would be captured using a\nstack-based incremental parse state in a symbolic\ngrammar-based model. For example, during the\nunderlined portion of Example\n(1), an incremen-\ntal language model should represent and maintain\nthe knowledge that it is currently inside a subordi-\nnate clause, implying (among other things) that a\nfull main clause must follow .\n33\n(1) As the doctor studied the textbook, the\nnurse walked into the ofﬁce.\nIn this work, we use a targeted evaluation ap-\nproach (\nMarvin and Linzen , 2018) to elicit ev-\nidence for syntactic state representations from\nlanguage models. That is, we examine language\nmodel behavior on artiﬁcially constructed sen-\ntences designed to expose behavior that is cru-\ncially dependent on syntactic state representa-\ntions. In particular, we study complex subordinate\nclauses and garden path effects (based on main-\nverb/reduced-relative ambiguities and NP/Z am-\nbiguities). W e ask three general questions: (1) Is\nthere basic evidence for the representation of syn-\ntactic state? (2) What textual cues does a neural\nlanguage model use to infer changes to syntactic\nstate? (3) Do the networks maintain knowledge\nabout syntactic state over long spans of complex\ntext, or do the syntactic state representations de-\ngrade?\nAmong neural language models, we study both\ngeneric sequence models (LSTMs), which have no\nexplicit representation of syntactic structure, and\nan RNN Grammar (RNNG) (\nDyer et al. , 2016),\nwhich explicitly calculates Penn Treebank-style\ncontext-free syntactic representations as part of\nthe process of assigning probabilities to words.\nThis comparison allows us to evaluate the ex-\ntent to which explicit representation of syntactic\nstructure makes models more or less sensitive to\nsyntactic state. RNNGs have been found to out-\nperform LSTMs not only in overall test-set per-\nplexity (\nDyer et al. , 2016), but also in modeling\nlong-distance number agreement in Kuncoro et al.\n(2018) for certain model conﬁgurations; our work\nextends this comparison to a variety of syntactic\nstate phenomena.\n2 General methods\nW e investigate neural language model behavior\nprimarily by studying the surprisal, or log inverse\nprobability , that a language model assigns to each\nword in a sentence:\nS(xi ) =− log2 p(xi |hi− 1),\nwhere xi is the current word or character, hi− 1 is\nthe model’s hidden state before consuming xi , the\nprobability is calculated from the network’s soft-\nmax activation, and the logarithm is taken in base\n2, so that surprisal is measured in bits. Surprisal\nis equivalent to the pointwise contribution to the\nlanguage modeling loss function due to a word.\nIn psycholinguistics, the common practice is to\nstudy reaction times per word (for example, read-\ning time as measured by an eyetracker), as a mea-\nsure of the word-by-word difﬁculty of online lan-\nguage processing. These reading times are often\ntaken to reﬂect the extent to which humans ex-\npect certain words in context, and may be gener-\nally proportional to surprisal given the comprehen-\nder’s probabilistic language model (\nHale, 2001;\nLevy, 2008; Smith and Levy , 2013; Futrell and\nLevy, 2017). In this study , we take language model\nsurprisal as the analogue of human reading time,\nusing it to probe the neural networks’ expecta-\ntions about what words will follow in certain con-\ntexts. There is a long tradition linking RNN per-\nformance to human language processing (\nElman,\n1990; Christiansen and Chater , 1999; MacDonald\nand Christiansen , 2002) and grammaticality judg-\nments ( Lau et al. , 2017), and RNN surprisals are\na strong predictor of human reading times ( Frank\nand Bod , 2011; Goodkind and Bicknell , 2018).\nRNNGs have also been used as models of human\nonline language processing (\nHale et al. , 2018).\n2.1 Experimental methodology\nIn each experiment presented below , we design\na set of sentences such that the word-by-word\nsurprisal values will show evidence for syntac-\ntic state representations. The idea is that certain\nwords will be surprising to a language model only\nif the model has a representation of a certain syn-\ntactic state going into the word. W e analyze word-\nby-word surprisal proﬁles for these sentences us-\ning regression analysis. Except where otherwise\nnoted, all statistics are derived from linear mixed-\neffects models (\nBaayen et al. , 2008) with sum-\ncoded ﬁxed-effect predictors and maximal random\nslope structure (\nBarr et al. , 2013). This method lets\nus factor out by-item variation in surprisal and fo-\ncus on the contrasts between conditions.\n2.2 Models tested\nW e study the behavior of four models of English:\ntwo LSTMs trained on large data, an an RNNG\nand an LSTM trained on matched, smaller data\n(the Penn Treebank). The models are summarized\nin T able\n1. All models are trained on a language\nmodeling objective.\nOur ﬁrst L TSM is the model presented in Joze-\nfowicz et al. (2016) as “BIG LSTM+CNN Inputs”,\nwhich we call “JRNN”, which was trained on\nthe One Billion W ord Benchmark (\nChelba et al. ,\n2013) with two hidden layers of 8196 units each\n34\nModel Architecture Training data Data size (tokens) Reference\nJRNN LSTM One Billion W ord ∼ 800 million Jozefowicz et al. (2016)\nGRNN LSTM Wikipedia ∼ 90 million Gulordava et al. (2018)\nRNNG RNN Grammar Penn Treebank ∼ 1 million Dyer et al. (2016)\nTinyLSTM LSTM Penn Treebank ∼ 1 million —\nT able 1: Models tested, by architecture, training data, and training data size.\nand CNN character embeddings as input. The sec-\nond large LSTM is the model described in the sup-\nplementary materials of\nGulordava et al. (2018),\nwhich we call “GRNN”, trained on 90 million to-\nkens of English Wikipedia with two hidden layers\nof 650 hidden units each.\nOur RNNG is trained on syntactically labeled\nPenn Treebank data (\nMarcus et al. , 1993), us-\ning 256-dimensional word embeddings for the in-\nput layer and 256-dimensional hidden layers, and\ndropout probability 0.3. Next-word predictions are\nobtained through hierarchical softmax with 140\nclusters, obtained with the greedy agglomerative\nclustering algorithm of\nBrown et al. (1992). W e\nestimate word surprisals using word-synchronous\nbeam search (\nStern et al. , 2017; Hale et al. , 2018):\nat each word wi a beam of incremental parses is\nﬁlled, the summed forward probabilities ( Stolcke,\n1995) of all candidates on the beam is taken as a\nlower bound on the preﬁx probability: Pmin(w1...i),\nand the surprisal of the i-th word in the sentence\nis estimated as log Pmin (w1...i )\nPmin (w1...i− 1 ) . Our action beam is\nsize 100, and our word beam is size 10. Finally ,\nto disentangle effects of training set from model\narchitecture, we use an LSTM trained on string\ndata from the Penn Treebank training set, which\nwe call TinyLSTM. For TinyLSTM we use 256-\ndimensional word-embedding inputs and hidden\nlayers and dropout probability 0.3, just as with the\nRNNG.\n3 Subordinate clauses\nW e begin by studying subordinate clauses, a key\nexample of a construction requiring stack-like rep-\nresentation of syntactic state. In such construc-\ntions, as shown in Example\n(1), a subordinator\nsuch as “as” or “when” serves as a cue that the\nfollowing clause is a subordinate clause, meaning\nthat it must be followed by some main (matrix)\nclause. In an incremental language model, this\nknowledge must be maintained and carried for-\nward while processing the words inside subordi-\nnate clause. A grammar-based symbolic language\nmodel (e.g.,\nStolcke, 1995; Manning and Carpen-\nter, 2000) would maintain this knowledge by keep-\ning track of syntactic rules representing the incom-\nplete subordinate clause and the upcoming main\nclause in a stack data structure. Psycholinguis-\ntic research has clearly demonstrated that humans\nmaintain representations of this kind in syntactic\nprocessing (\nStaub and Clifton , 2006; Lau et al. ,\n2006; Levy et al. , 2012). Here we ask whether the\nstring completion probabilities produced by neu-\nral language models show evidence of the same\nknowledge.\nW e can detect the knowledge of syntactic state\nin this case by examining whether the network li-\ncenses and requires a matrix clause following the\nsubordinate clause. These expectations can be de-\ntected by examining surprisal differences between\nsentences of the form in Example\n(2):\n(2) a. As the doctor studied the textbook,\nthe nurse walked into the ofﬁce.\n[S U Bordinator, M AT R I X]\nb. *As the doctor studied the textbook.\n[S U B, N O-M AT R I X]\nc. ?The doctor studied the textbook,\nthe nurse walked into the ofﬁce.\n[N O-S U Bordinator, M AT R I X]\nd. The doctor studied the textbook.\n[N O-S U B, N O-M AT R I X]\nIf the network licenses a matrix clause follow-\ning the subordinate clause—and maintains knowl-\nedge of that licensing relationship throughout the\nclause, from the subordinator to the comma—then\nthis should be manifested as lower surprisal at the\nmatrix clause in\n(2-a) as compared to (2-c). W e\ncall this the matrix licensing effect : the surprisal\nof the condition [ S U B, M AT R I X] minus [ N O S U B,\nM AT R I X], which will be negative if there is a li-\ncensing effect. If the network requires a follow-\ning matrix clause, then this will be manifested\nas higher surprisal at the matrix clause for\n(2-b)\ncompared with (2-d). W e call this the no-matrix\npenalty effect: the surprisal of [ S U B,N O M AT R I X]\nminus [ N O S U B, N O M AT R I X], which will be posi-\ntive if there is a penalty .\n35\nRNNG tinylstm\nGRNN JRNN\nno−matrix matrix no−matrix matrix\n−10\n−5\n0\n5\n−10\n−5\n0\n5\nContinuation\n(Sub. present − sub. absent) surprisal difference\nFigure 1: Effect of subordinator absence/presence on\nsurprisal of continuations. Red: no-matrix penalty ef-\nfect. Blue: matrix licensing effect. In this and all other\nﬁgures, unless otherwise noted, error bars represent\n95% conﬁdence intervals of the contrasts between con-\nditions shown, computed from the standard error of the\nby-item and by-condition mean surprisals after sub-\ntracting out the by-item means (\nMasson and Loftus ,\n2003).\nW e designed 23 experimental items on the pat-\ntern of (2) and calculated difference in the sum sur-\nprisal of the words in the matrix clause. 1 Figure 3\nshows the matrix licensing effect (in blue) and the\nno-matrix penalty effect (in red), averaged across\nitems. For all models, we see a facilitative matrix\nlicensing effect ( p < .001 for all models), small-\nest in TinyLSTM. However, we only ﬁnd a signif-\nicant no-matrix penalty for GRNN and the RNNG\n( p < .001 in both): the other models do not sig-\nniﬁcantly penalize an ungrammatical continuation\n( p = .9 for JRNN; p = .5 for TinyLSTM). That\nis, JRNN and TinyLSTM give no indication that\n(2-b) is less probable than (2-c).\nW e found that all models at least partially repre-\nsent the licensing relationship between a subordi-\nnate and matrix clause. However, in order to fully\nrepresent the syntactic requirements induced by a\nsubordinator, it seems that a model needs either\nlarge amounts of data (as in GRNN) or explicit\nrepresentation of syntax (as in the RNNG, as op-\nposed to TinyLSTM).\n1 Note that it would not be sufﬁcient to look at surprisal\nonly at the punctuation token, because the comma could in-\ndicate the beginning of a conjoined NP .\n3.1 Maintenance and degradation of\nsyntactic state\nThe foregoing results show that neural language\nmodels use the presence of a subordinator as a\ncue to the onset of a subordinate clause, and that\nthey maintain knowledge that they are in a sub-\nordinate clause throughout the intervening mate-\nrial up to the comma. Now we probe the ability\nof models to maintain this knowledge over long\nspans of complex intervening material. T o do so,\nwe use sentences on the template of\n(2) and add in-\ntervening material modifying the NPs in the subor-\ndinate clause. T o both of these NPs (in subject and\nobject position), we add modiﬁers of increasing\nsyntactic complexity: PPs, subject-extracted rela-\ntive clauses (SRCs), and object-extracted relative\nclauses (ORCs), as shown in Figure\n2. W e study\nthe extent to which these modiﬁers weaken the\nlanguage models’ expectations about the upcom-\ning matrix clause.\nAs a summary measure of the strength of lan-\nguage models’ expectations about an upcoming\nmatrix clause, we collapse the two measures of the\nprevious section into one: the matrix licensing in-\nteraction, consisting of the difference between the\nno-matrix penalty effect and the matrix licensing\neffect (the two bars in Figure\n1). A similar mea-\nsure was used to detect ﬁller–gap dependencies by\nWilcox et al. (2018).\nFigure 3 shows the strength of the matrix li-\ncensing interaction given sentences with various\nmodiﬁers inserted. For the large LSTMs, GRNN\nexhibits a strong interaction when the intervening\nmaterial is short and syntactically simple, and the\ninteraction gets progressively weaker as the inter-\nvening material becomes progressively longer and\nmore complex ( p < 0.001 for subject postmodi-\nﬁers and p < 0.01 object postmodiﬁers). The other\nmodels show less interpretable behavior.\nOur results indicate that at least some large\nLSTMs, along with the RNNG, are capable of\nmaintaining a representation of syntactic state over\nspans of complex intervening material. Quanti-\nﬁed as a licensing interaction, this representation\nof syntactic state exhibits the most clearly un-\nderstandable behavior in GRNN, which shows a\ngraceful degradation of syntactic expectations as\nthe complexity of intervening material increases.\nThe representation is maintained most strongly in\nthe RNNG, except for one particular construction\n(object-position SRCs).\n36\nAs the doctor   \nin a white lab coat (PP)\nwho was wearing a white lab coat (SRC)\nwho the administrator had recently hired (ORC)\n(Subject interveners)\nstudied the textbook   \nabout several recent advances in cancer therapy (PP)\nthat described several recent advances in cancer therapy (SRC)\nthat colleagues had written on cancer therapy (ORC)\n(Object interveners)\n. . .\nFigure 2: Scheme for lengthening the subordinate clause in Section 3.1.\n4 Garden path effects\nThe major phenomenon that has been used to\nprobe incremental syntactic representations in hu-\nmans is garden path effects . Garden path effects\narise from local ambiguities, where a context leads\na comprehender to believe one parse is likely , but\nthen a disambiguating word forces her to dras-\ntically revise her beliefs, resulting in high sur-\nprisal/reading time at the disambiguating word. In\neffect, the comprehender is “led down the garden\npath” by a locally likely but ultimately incorrect\nparse (\nBever, 1970). Garden-pathing in LSTMs\nhas recently been demonstrated by van Schijndel\nand Linzen (2018a,b) in the context of modeling\nhuman reading times.\nGarden path effects allow us to detect represen-\ntations of syntactic state because if a person or lan-\nguage model shows a garden path effect at a word,\nthat means that the person or model had some be-\nlief about syntactic state which was disconﬁrmed\nby that word. In psycholinguistics, these effects\nhave been used to study the question of what in-\nformation determines people’s beliefs about likely\nparses given locally ambiguous contexts: for ex-\nample, whether factors such as world knowledge\nplay a role (\nFerreira and Clifton , 1986; Trueswell\net al. , 1994).\nHere we study two major kinds of local ambigu-\nities inducing garden path effects. For each ambi-\nguity , we ask two main questions. First, whether\nthe network shows the basic garden path effect,\nwhich would indicate that it had a syntactic state\nrepresentation that made a disambiguating word\nsurprising. Second, whether the network is sen-\nsitive to subtle lexical cues to syntactic structure\nwhich may modulate the size of the garden path\neffect: this question allows us to determine what\ninformation the network uses to determine the be-\nginnings and endings of certain syntactic states.\n4.1 NP/Z Ambiguity\nThe NP/Z ambiguity\n2 refers to a local ambiguity\nin sentences of the form given in Example (3).\n2 For Noun Phrase/Zero ambiguity . At ﬁrst the embedded\nverb appears to take an NP object, but later it turns out that it\nwas a zero (null) object.\n(3)a. When the dog scratched\nthe vet with his new\nassistant took off the muzzle. [ T R A N S I T I V E,\nN O C O M M A]\nb. When the dog scratched, the vet with his new\nassistant took off the muzzle. [ T R A N S I T I V E,\nC O M M A]\nc. When the dog struggled the vet with\nhis new assistant took off the muzzle.\n[I N T R A N S I T I V E, N O C O M M A]\nd. When the dog struggled, the vet with\nhis new assistant took off the muzzle.\n[I N T R A N S I T I V E, C O M M A]\nWhen a comprehender reads the underlined\nphrase “ the vet with his new assistant” in (3-a),\nshe may at ﬁrst believe that this phrase is the di-\nrect object of the verb “scratched” inside the sub-\nordinate clause. However, upon reaching the verb\n“took off”, she realizes that the underlined phrase\nwas not in fact an object of the verb “scratched”,\nrather it was the subject of a new clause, and\nthe subordinate clause in fact ended after the\nverb “scratched”. The key region of the sentence\nwhere the garden path disambiguation happens—\ncalled the disambiguator—is the phrase “took\noff”, marked in bold.\nWhile a garden path should obtain in\n(3-a), no\nsuch garden path should exist for (3-b), because\na comma clearly demarcates the end of the sub-\nordinate clause. Therefore a basic garden path ef-\nfect would be indicated by the difference in sur-\nprisal at the disambiguator for\n(3-a) minus (3-b).\nFurthermore, if a comprehender is sensitive to the\nrelationship between verb argument structure and\nclause boundaries, then there should be no gar-\nden path in\n(3-c), because the verb “struggled”\nis I N T R A N S I T I V E: it cannot take an object in En-\nglish, so an incremental parser should never be\nmisled into believing that “\nthe vet...” is its object.\nThis lexical information about syntactic structure\nis subtle enough that there has been controversy\nabout whether even humans are sensitive to it in\nonline processing (\nStaub, 2007).\n4.1.1 NP/Z Garden Path Effect\nW e tested whether neural language models would\nshow the basic garden path effect and if this ef-\n37\nNone\nPP\nSRC\nORC\nNone PP SRC ORC\nSubject interveners\nObject interveners\nsurprisal (bits) 7.0 7.5 8.0 8.5 9.0\nJRNN\nNone\nPP\nSRC\nORC\nNone PP SRC ORC\nSubject interveners\nObject interveners\nsurprisal (bits) 6 7 8 9 10\nGRNN\nNone\nPP\nSRC\nORC\nNone PP SRC ORC\nSubject interveners\nObject interveners\nsurprisal (bits)\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\nRNNG\nNone\nPP\nSRC\nORC\nNone PP SRC ORC\nSubject interveners\nObject interveners\nsurprisal (bits) 0.5 1.0\ntinylstm\nFigure 3: Size of matrix clause licensing interaction (see text) given various intervening elements in the subordinate\nclause. Note that the heatmaps are on different scales across models.\nfect would be modulated by verb transitivity . W e\nconstructed 32 items based of the same structure\nas\n(3), based on materials from Staub (2007), ma-\nnipulating the transitivity of the embedded verb\n(“scratched” vs. “struggled”), and the presence of\na disambiguating comma at the end of the subor-\ndinate clause. An NP/Z garden path effect would\nshow up as increased surprisal at the main verb\n“took off” in the absence of a comma. If the net-\nworks use the transitivity of the embedded verb as\na cue to clause structure, and maintain that infor-\nmation over the span of six words between the em-\nbedded verb and the main verb, then there should\nbe a garden path effect for the transitive verb, but\nnot for the intransitive verb. More generally we\nwould expect a stronger garden path given the\ntransitive verb than given the intransitive verb.\nFigure\n4 shows the mean surprisals at the dis-\nambiguator for all four models, for both transi-\ntive and intransitive embedded verbs. The over-\nall per-region surprisals, averaged over words in\neach region, are shown in Figure\n5. W e see that\na garden path effect exists in all models (though\nvery small in TinyLSTM): all models show sig-\nniﬁcantly higher surprisal at the main verb when\nthe disambiguating comma is absent ( p < .001 for\nall models). However, only the large LSTMs ap-\npear to be sensitive to the transitivity of the em-\nRNNG tinylstm\nGRNN JRNN\ntransitive intransitive transitive intransitive\n0\n2\n4\n6\n0\n2\n4\n6\nEmbedded verb transitivity\nGarden path effect (bits)\nFigure 4: A verage garden path effect (surprisal at dis-\nambiguator in N O-C O M M A condition minus C O M M A\ncondition) by model and embedded verb transitivity .\nbedded verb, showing a smaller garden path effect\nfor intransitive verbs. Statistically , there is a sig-\nniﬁcant interaction of comma presence and verb\ntransitivity only in GRNN and JRNN (GRNN:\np < .01; JRNN: p < .001; RNNG: p = .3, TinyL-\nSTM: p = .3).\nAll models show NP/Z garden path effects, indi-\ncating that they are sensitive to some cues indicat-\n38\nGRNNJRNNRNNGtinylstm\nWhen the dogstruggled/scratched\n,\nthe vet\nwith his new assistant\ntook off the muzzle . <eos>\n5\n10\n15\n5\n10\n15\n0\n5\n10\n0\n5\n10Mean surprisal in region\ntransitive intransitive no comma comma\nFigure 5: Region-by-region surprisal values for NP/Z garden path materials. Surprisal values are averaged across\nitems and across words in regions. The critical region where the garden path effect is visible is the verb “took off”.\ning end-of-clause boundaries. However, only the\nlarge LSTMs appear to use verb argument struc-\nture information as a cue to these boundaries. The\nresults suggest that very large amounts of data may\nbe necessary for current neural models to discover\nsuch ﬁne-grained dependencies between syntactic\nproperties of verbs and sentence structure.\n4.1.2 Maintenance and degradation of state\nW e can probe the maintenance and degradation\nof syntactic state information by manipulating the\nlength of the intervening material between the on-\nset of the local ambiguity and the disambiguator\nin examples such as\n(3). The question is whether\nthe networks maintain the knowledge, while pro-\ncessing the intervening material, that the inter-\nvening noun phrase is probably the object of the\nembedded verb inside a subordinate clause, or\nwhether they gradually lose track of this infor-\nmation. T o study this question we used materials\non the pattern of\n(4): these materials manipulate\nthe length of the intervening material (underlined)\nwhile holding constant the distance between the\nsubordinator (“ As”) and the disambiguator ( grew).\n(4)a. As the author studying Babylon in ancient\ntimes wrote\nthe book grew. [ S H O RT, N O-\nC O M M A]\nb. As the author studying Babylon in an-\ncient times wrote, the book grew. [ S H O RT,\nC O M M A]\nc. As the author wrote the book describing\nBabylon in ancient times grew. [ L O N G, N O-\nC O M M A]\nd. As the author wrote, the book describing\nBabylon in ancient times grew. [ L O N G,\nC O M M A]\nIf neural language models show degradation of\nsyntactic state, then the garden path effect (mea-\nsured as the difference in surprisal between the\nC O M M A and N O-C O M M A conditions at the disam-\nbiguator) will be smaller for the L O N G conditions.\nW e tested 32 sentences of the form in\n(4), based\non materials from T abor and Hutchins (2004). The\ngarden path effect sizes are shown in Figure 6.\nW e ﬁnd a signiﬁcant garden effect in all mod-\nels in the S H O RT condition ( p < .001 in JRNN\nand GRNN; p < .01 in the RNNG and p = .03 in\nTinyLSTM). In the long condition, we ﬁnd the gar-\nden path effect in all models except TinyLSTM:\n( p < .001 in JRNN; p < .01 in GRNN; p = .02 in\nthe RNNG; and p = .2 in TinyLSTM). The cru-\ncial interaction between length and comma pres-\nence (indicating that syntactic state degrades) is\nsigniﬁcant in GRNN ( p < .01) and TinyLSTM\n( p < .001) but not JRNN ( p = .7) nor the RNNG\n( p = .6). The pattern is reminiscent of the results\non degradation of state information about subor-\ndinate clauses in Section\n3, where GRNN and\nTinyLSTM showed the clearest evidence of degra-\ndation.\n39\nRNNG tinylstm\nGRNN JRNN\nshort long short long\n0\n2\n4\n6\n0\n2\n4\n6\nLength of ambiguous region\nGarden path effect (bits)\nFigure 6: A verage garden path effect by model and\nlength of ambiguous region.\nNote that the pattern found here is the opposite\nof the pattern of human reading times. Humans ap-\npear to show “digging-in” effects: the longer the\nspan of time between the introduction of a local\nambiguity and its resolution, the larger the garden\npath effect (\nT abor and Hutchins , 2004; Levy et al. ,\n2009).\n4.2 Main V erb/Reduced Relative Ambiguity\nNext we turn to garden path effects induced by the\nclassic Main V erb/Reduced Relative (MV/RR)\nambiguity, in which a word is locally ambiguous\nbetween being the main verb of a sentence or in-\ntroducing a reduced relative clause (reduced RC:\na relative clause with no explicit complementizer,\nheaded by a passive-participle verb). That ambi-\nguity can be maintained over a long stretch of ma-\nterial:\n(5)a. The woman brought\nthe sandwich from\nthe kitchen tripped on the carpet.\n[R E D U C E D, A M B I Guous]\nb. The woman who was brought the sand-\nwich from the kitchen tripped on the carpet.\n[U N R E D U C E D, A M B I G]\nc. The woman given the sandwich from\nthe kitchen tripped on the carpet.\n[R E D U C E D, U NA M B I Guous]\nd. The woman who was given the sandwich\nfrom the kitchen tripped on the carpet.\n[U N R E D U C E D, U NA M B I G]\nIn Example (5-a), the verb “brought” is ini-\ntially analyzed as a main verb phrase, but upon\nRNNG tinylstm\nGRNN JRNN\nambig unambig ambig unambig\n0\n2\n4\n6\n0\n2\n4\n6Garden path effect (bits)\nFigure 7: Garden path effect size for MV/RR ambiguity\nby model and verb-form ambiguity .\nreaching the verb “tripped”—the disambiguator\nin this case—the reader must re-analyze it as an\nRC. The garden path should be eliminated in sen-\ntences such as\n(5-b), the U N R E D U C E D condition,\nwhere the words “who was” clarify that the verb\n“brought” is part of an RC, rather than the main\nverb of the sentence. Therefore we quantify the\ngarden path effect as the surprisal at the disam-\nbiguator for the R E D U C E D minus U N R E D U C E D\nconditions.\nThere is another possible cue that the initial verb\nis the head of an RC: the morphological form of\nthe verb. In examples such as\n(5-c), the the verb\n“given” is unambiguously in its past-participle\nform, indicating that it cannot be the main verb\nof the sentence. If a language model is sensitive\nto morphological cues to syntactic structure, then\nit should either not show a garden path effect in\nthis U NA M B I Guous condition, or it should show a\nreduced garden path effect.\nW e constructed 29 experimental items follow-\ning the template of\n(5). Figure 7 shows the garden\npath effect sizes by model and verb-form ambigu-\nity . All networks show the basic garden path effect\n( p < .001 in JRNN, GRNN, and RNNG; p < 0.01\nin TinyLSTM). However, the garden path effect in\nTinyLSTM is much smaller than the other mod-\nels: RC reduction causes an additional .3 bits of\nsurprisal at the disambiguating verb, as compared\nto 2.8 bits in the RNNG, 1.9 in JRNN, and 3.6\nin GRNN (TinyLSTM’s garden path effect is sig-\nniﬁcantly smaller than each other model at p <\n0.001).\nIf the network is using the morphological form\n40\nPhenomenon GRNN JRNN RNNG TinyLSTM\nSubordination ✓✓ ✓✗ ✓✓ ✓✗\nNP/Z Garden Path ✓✓ ✓✓ ✓✗ ✓✗\nMV/RR Garden Path ✓✓ ✓✓ ✓✓ ✓✗\nT able 2: Summary of results by model and phenomenon. The ﬁrst check mark indicates basic evidence of syntactic\nstate representation. The second check mark indicates the ability to capture more ﬁne-grained phenomena: for\nsubordination, the no-matrix penalty effect; for the NP/Z garden path, the effect of verb transitivity; and for the\nMV/RR garden path, the effect of verb morphology .\nof the verb as a cue to syntactic structure, then it\nshould show the garden path effect more strongly\nin the A M B I G condition than the U NA M B I G condi-\ntion. The large language models and the RNNG do\nshow this pattern: at the critical main-clause verb,\nsurprisal is superadditively highest in the reduced\nambiguous condition (the dotted blue line; a posi-\ntive interaction between the reduced and ambigu-\nous conditions is signiﬁcant in the three models at\np < 0.001). However, TinyLSTM does not show\nevidence for superadditive surprisal for the am-\nbiguous verbform and the reduced RC ( p = .45).\nThe three large LSTMs and the RNNG replicate\nthe key human-like garden-path disambiguation\neffect due to to ambiguity in verb form. But strik-\ningly , even when the participial verbform is un-\nambiguous, there is still a signiﬁcant garden path\neffect in all models ( p < 0.01 in all models except\nTinyLSTM, where p = .08). Apparently , these\nnetworks treat an unambiguous passive-participial\nverb as only a noisy cue to the presence of an RC.\n5 General Discussion and Conclusion\nIn all models studied, we found clear evidence\nof basic incremental state syntactic representation.\nHowever, models varied in how well they fully\ncaptured the effects of such state and the poten-\ntially subtle lexical cues indicating the beginnings\nand endings of such states: only the large LSTMs\ncould sometimes reliably infer clause boundaries\nfrom verb argument structure (Section\n4.1) and\nmorphological verb-form (Section 4.2), and only\nGRNN and the RNNG fully captured the proper\nbehavior of subordinate clauses. The results are\nsummarized in T able\n2. W e suggest that repre-\nsentation of course-grained syntactic structure re-\nquires either syntactic supervision or large data,\nwhile exploiting ﬁne-grained lexical cues to struc-\nture requires large data.\nMore generally , we believe that the psycholin-\nguistic methodology employed in this paper pro-\nvides a valuable lens on the internal represen-\ntations of black-box systems, and can form the\nbasis for more systematic tests of the linguistic\ncompetence of NLP systems. W e make all exper-\nimental items, results, and analysis scripts avail-\nable online at\ngithub.com/langprocgroup/nn_\nsyntactic_state.\nReferences\nR. Haraald Baayen, D.J. Davidson, and D.M. Bates.\n2008. Mixed-effects modeling with crossed random\neffects for subjects and items. Journal of Memory\nand Language, 59(4):390–412.\nDale J. Barr, Roger P . Levy , Christoph Scheepers, and\nHarry J Tily . 2013. Random effects structure for\nconﬁrmatory hypothesis testing: Keep it maximal.\nJournal of Memory and Language, 68(3):255–278.\nJean-Philippe Bernardy and Shalom Lappin. 2017. Us-\ning deep neural networks to learn syntactic agree-\nment. Linguistic Issues in Language T echnology ,\n15:1–15.\nThomas G. Bever. 1970. The cognitive basis for lin-\nguistic structures. In J. R. Hayes, editor, Cogni-\ntion and the Development of Language. Wiley , New\nY ork.\nKathryn Bock and Carol A Miller. 1991. Broken agree-\nment. Cognitive Psychology, 23(1):45–93.\nPeter F . Brown, Peter V . deSouza, Robert L. Mer-\ncer, V incent J. Della Pietra, and Jenifer C. Lai.\n1992.\nClass-based n-gram models of natural lan-\nguage. Comput. Linguist., 18(4):467–479.\nCiprian Chelba, T omas Mikolov , Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and T ony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\nShammur Absar Chowdhury and Roberto Zamparelli.\n2018. RNN simulations of grammaticality judg-\nments on long-distance dependencies. In Proceed-\nings of the 27th International Conference on Com-\nputational Linguistics, pages 133–144.\nMorten H. Christiansen and Nick Chater. 1999. T o-\nward a connectionist model of recursion in hu-\nman linguistic performance. Cognitive Science ,\n23(2):157–205.\n41\nJacob Devlin, Ming-W ei Chang, Kenton Lee, and\nKristina T outanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,\nand Noah A Smith. 2016. Recurrent neural net-\nwork grammars. In Proceedings of the 2016 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage T echnologies, pages 199–209.\nJ.L. Elman. 1990. Finding structure in time. Cognitive\nScience, 14(2):179–211.\nEmile Enguehard, Y oav Goldberg, and T al Linzen.\n2017. Exploring the syntactic abilities of\nRNNs with multi-task learning. arXiv preprint\narXiv:1706.03542.\nFernanda Ferreira and Charles Clifton. 1986. The inde-\npendence of syntactic processing. Journal of Mem-\nory and Language, 25(3):348–368.\nStefan L. Frank and Rens Bod. 2011. Insensitivity\nof the human sentence-processing system to hierar-\nchical structure. Psychological Science, 22(6):829–\n834.\nRichard Futrell and Roger Levy . 2017. Noisy-\ncontext surprisal as a human sentence processing\ncost model. In Proceedings of the 15th Confer-\nence of the European Chapter of the Association for\nComputational Linguistics: V olume 1, Long P apers,\npages 688–698, V alencia, Spain.\nY oav Goldberg. 2017. Neural network methods for nat-\nural language processing. Synthesis Lectures on Hu-\nman Language T echnologies, 10(1):1–309.\nAdam Goodkind and Klinton Bicknell. 2018. Predic-\ntive power of word surprisal for reading times is a\nlinear function of language model quality . In Pro-\nceedings of the 8th W orkshop on Cognitive Modeling\nand Computational Linguistics (CMCL 2018), pages\n10–18, Salt Lake City , UT . Association for Compu-\ntational Linguistics.\nK. Gulordava, P . Bojanowski, E. Grave, T . Linzen,\nand M. Baroni. 2018. Colorless green recurrent\nnetworks dream hierarchically . In Proceedings of\nNAACL.\nJohn Hale, Chris Dyer, Adhiguna Kuncoro, and\nJonathan R Brennan. 2018. Finding syntax in hu-\nman encephalography with beam search. In Pro-\nceedings of the 56th Annual Meeting of the Associ-\nation for Computational Linguistics (Long P apers),\nMelbourne, Australia.\nJohn T . Hale. 2001. A probabilistic Earley parser as a\npsycholinguistic model. In Proceedings of the Sec-\nond Meeting of the North American Chapter of the\nAssociation for Computational Linguistics and Lan-\nguage T echnologies, pages 1–8.\nRafal Jozefowicz, Oriol V inyals, Mike Schuster, Noam\nShazeer, and Y onghui Wu. 2016. Exploring the lim-\nits of language modeling. arXiv, 1602.02410.\nAdhiguna Kuncoro, Chris Dyer, John Hale, Dani Y o-\ngatama, Stephen Clark, and Phil Blunsom. 2018.\nLSTMs can learn syntax-sensitive dependencies\nwell, but modeling structure makes them better. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (V olume 1:\nLong P apers), volume 1, pages 1426–1436.\nEllen Lau, Clare Stroud, Silke Plesch, and Colin\nPhillips. 2006. The role of structural prediction in\nrapid syntactic analysis. Brain & Language, 98:74–\n88.\nJey Han Lau, Alexander Clark, and Shalom Lappin.\n2017. Grammaticality , acceptability , and probabil-\nity: a probabilistic view of linguistic knowledge.\nCognitive Science, 41(5):1202–1241.\nRoger Levy . 2008. Expectation-based syntactic com-\nprehension. Cognition, 106(3):1126–1177.\nRoger Levy , Evelina Fedorenko, Mara Breen, and T ed\nGibson. 2012.\nThe processing of extraposed struc-\ntures in English . Cognition, 122(1):12–36.\nRoger P Levy , Florencia Reali, and Thomas L Grifﬁths.\n2009. Modeling the effects of memory on human\nonline sentence processing with particle ﬁlters. In\nAdvances in neural information processing systems,\npages 937–944.\nT al Linzen, Emmanuel Dupoux, and Y oav Goldberg.\n2016. Assessing the ability of LSTMs to learn\nsyntax-sensitive dependencies. Transactions of the\nAssociation for Computational Linguistics , 4:521–\n535.\nMaryellen C. MacDonald and Morten H. Christiansen.\n2002.\nReassessing working memory: Comment on\nJust and Carpenter (1992) and W aters and Caplan\n(1996)\n. Psychological Review, 109(1):35–54.\nChristopher D Manning and Bob Carpenter. 2000.\nProbabilistic parsing using left corner language\nmodels. In Advances in probabilistic and other\nparsing technologies, pages 105–124. Springer.\nMitchell P . Marcus, Mary Ann Marcinkiewicz, and\nBeatrice Santorini. 1993.\nBuilding a large annotated\ncorpus of english: The penn treebank . Comput. Lin-\nguist., 19(2):313–330.\nRebecca Marvin and T al Linzen. 2018. T argeted syn-\ntactic evaluation of language models . In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 1192–1202.\nAssociation for Computational Linguistics.\nMichael EJ Masson and Geoffrey R Loftus. 2003. Us-\ning conﬁdence intervals for graphically based data\ninterpretation. Canadian Journal of Experimen-\ntal Psychology/Revue canadienne de psychologie\nexp´erimentale, 57(3):203.\n42\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of NAACL.\nMarten van Schijndel and T al Linzen. 2018a. Model-\ning garden path effects without explicit hierarchical\nsyntax. In Proceedings of the 40th Annual Meeting\nof the Cognitive Science Society.\nMarten van Schijndel and T al Linzen. 2018b. A neural\nmodel of adaptation in reading. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing.\nNathaniel J. Smith and Roger Levy . 2013. The effect of\nword predictability on reading time is logarithmic.\nCognition, 128(3):302–319.\nAdrian Staub. 2007. The parser doesn’t ignore intran-\nsitivity , after all. Journal of Experimental Psychol-\nogy: Learning, Memory, and Cognition, 33(3):550.\nAdrian Staub and Charles Clifton. 2006. Syntactic pre-\ndiction in language comprehension: Evidence from\neither . . . or. Journal of Experimental Psychology:\nLearning, Memory, & Cognition, 32(2):425–436.\nMitchell Stern, Daniel Fried, and Dan Klein. 2017.\nEffective inference for generative neural parsing .\nIn Proceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1695–1700. Association for Computational Linguis-\ntics.\nAndreas Stolcke. 1995. An efﬁcient probabilis-\ntic context-free parsing algorithm that computes\npreﬁx probabilities. Computational Linguistics ,\n21(2):165–201.\nIlya Sutskever, Oriol V inyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural net-\nworks. In Advances in Neural Information Process-\ning Systems, pages 3104–3112.\nWhitney T abor and Sean Hutchins. 2004. Evidence for\nself-organized sentence processing: Digging-in ef-\nfects. Journal of Experimental Psychology: Learn-\ning, Memory, and Cognition, 30(2):431.\nJohn C. Trueswell, Michael K. T anenhaus, and S. Gar-\nnsey . 1994. Semantic inﬂuences on parsing: Use of\nthematic role information in syntactic ambiguity res-\nolution. Journal of Memory and Language, 33:285–\n318.\nEthan Wilcox, Roger Levy , T akashi Morita, and\nRichard Futrell. 2018.\nWhat do rnn language mod-\nels learn about ﬁller–gap dependencies? In Proceed-\nings of the 2018 EMNLP W orkshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for\nNLP, pages 211–221. Association for Computa-\ntional Linguistics.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.627077579498291
    },
    {
      "name": "Cognitive science",
      "score": 0.5556654930114746
    },
    {
      "name": "Computational linguistics",
      "score": 0.5275527834892273
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5208030939102173
    },
    {
      "name": "Natural language processing",
      "score": 0.4844939410686493
    },
    {
      "name": "Association (psychology)",
      "score": 0.4691195785999298
    },
    {
      "name": "State (computer science)",
      "score": 0.4500756561756134
    },
    {
      "name": "Linguistics",
      "score": 0.4155215322971344
    },
    {
      "name": "Philosophy",
      "score": 0.24376040697097778
    },
    {
      "name": "Programming language",
      "score": 0.23739734292030334
    },
    {
      "name": "Psychology",
      "score": 0.23475614190101624
    },
    {
      "name": "Epistemology",
      "score": 0.10359513759613037
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210107233",
      "name": "Language Science (South Korea)",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I204250578",
      "name": "University of California, Irvine",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I136199984",
      "name": "Harvard University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I22299242",
      "name": "Kyoto University",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I4210107899",
      "name": "Institute of Primate Research",
      "country": "KE"
    },
    {
      "id": "https://openalex.org/I4210104429",
      "name": "Institute of Cognitive and Brain Sciences",
      "country": "US"
    }
  ]
}