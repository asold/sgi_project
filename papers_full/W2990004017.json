{
  "title": "Non-autoregressive Transformer by Position Learning",
  "url": "https://openalex.org/W2990004017",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2097502890",
      "name": "Bao Yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2106821041",
      "name": "Zhou Hao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1950390696",
      "name": "Feng Jiangtao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1917300918",
      "name": "Wang Mingxuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2377881058",
      "name": "Huang, Shujian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2020951600",
      "name": "Chen Jia-jun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1978908041",
      "name": "Li Lei",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2402268235",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2124807415",
    "https://openalex.org/W2798569372",
    "https://openalex.org/W2907945666",
    "https://openalex.org/W2963958388",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2507756961",
    "https://openalex.org/W2966746916",
    "https://openalex.org/W2948381505",
    "https://openalex.org/W2963463583",
    "https://openalex.org/W2906111771",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2963434219",
    "https://openalex.org/W2962969034",
    "https://openalex.org/W2143612262",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2962915948",
    "https://openalex.org/W2963541420",
    "https://openalex.org/W2964026424",
    "https://openalex.org/W2789543585",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W1821462560"
  ],
  "abstract": "Non-autoregressive models are promising on various text generation tasks. Previous work hardly considers to explicitly model the positions of generated words. However, position modeling is an essential problem in non-autoregressive text generation. In this study, we propose PNAT, which incorporates positions as a latent variable into the text generative process. Experimental results show that PNAT achieves top results on machine translation and paraphrase generation tasks, outperforming several strong baselines.",
  "full_text": "Non-autoregressive Transformer by Position Learning\nYu Bao1∗ Hao Zhou2 Jiangtao Feng2 Mingxuan Wang2\nShujian Huang1 Jiajun Chen1 Lei Li2\n1National Key Laboratory for Novel Software Technology, Nanjing University, China\n2ByteDance AI Lab, Beijing, China\nbaoy@smail.nju.edu.cn, {huangsj,chenjj}@nju.edu.cn\n{zhouhao.nlp,fengjiangtao,wangmingxuan.89,lileilab}@bytedance.com\nAbstract\nNon-autoregressive models are promising on\nvarious text generation tasks. Previous work\nhardly considers to explicitly model the posi-\ntions of generated words. However, the posi-\ntion modeling is an essential problem in non-\nautoregressive text generation. In this study,\nwe propose PNAT, which incorporates po-\nsitions as a latent variable into the text gen-\nerative process. Experimental results show\nthat PNAT achieves top results on machine\ntranslation and paraphrase generation tasks,\noutperforming several strong baselines.\n1 Introduction\nTransformer (Vaswani et al., 2017) has been widely\nused in many text generation tasks, which is ﬁrst\nproposed in neural machine translation, achieving\ngreat success for its promising performance. Never-\ntheless, the auto-regressive property of Transformer\nhas been a bottleneck. Speciﬁcally, the decoder of\nTransformer generates words sequentially, and the\nlatter words are conditioned on previous ones in\na sentence. Such bottleneck prevents the decoder\nfrom higher efﬁciency in parallel computation, and\nimposes strong constrains in text generation, with\nwhich the generation order has to be left to right (or\nright to left) (Shaw et al., 2018; Vaswani et al.,\n2017).\nRecently, many researches (Gu et al., 2018; Lee\net al., 2018; Wang et al., 2019; Wei et al., 2019) are\ndevoted to break the auto-regressive bottleneck by\nintroducing non-autoregressive Transformer (NAT)\nfor neural machine translation, where the decoder\ngenerates all words simultaneously instead of se-\nquentially. Intuitively, NAT abandons feeding pre-\nvious predicted words into decoder state at the\nnext time step, but directly copy encoded repre-\nsentation at source side to the decoder inputs (Gu\n∗This work was done when Yu Bao interned in Bytedance\nAI-Lab.\net al., 2018). However, without the auto-regressive\nconstrain, the search space of the output sentence\nbecomes larger (Wei et al., 2019), which brings\nthe performance gap (Lee et al., 2018) between\nNAT and auto-regressive Transformer (AT). Re-\nlated works propose to include some inductive pri-\nors or learning techniques to boost the performance\nof NAT. But most of previous work hardly consider\nexplicitly modeling the position of output words\nduring text generation.\nWe argue that position prediction is an essential\nproblem of NAT. Current NAT approaches do not\nexplicitly model the position of output words, and\nmay ignore the reordering issue in generating out-\nput sentences. Compared to machine translation,\nthe reorder problem is much more severe in tasks\nsuch as table-to-text (Liu et al., 2018) and dialog\ngenerations (Shen et al., 2017). Additionally, it\nis straightforward to explicitly model word posi-\ntions in output sentences, as position embeddings\nare used in Transformer, which is natively non-\nautoregressive, to include the order information. In-\ntuitively, if output positions are explicitly modeled,\nthe predicted position combined with Transformer\nto realize non-autoregressive generation would be-\ncome more natural.\nIn this paper, we propose non-autoregressive\ntransformer by position learning (PNAT). PNAT is\nsimple yet effective, which explicitly models posi-\ntions of output words as latent variables in the text\ngeneration. Speciﬁcally, we introduce a heuristic\nsearch process to guide the position learning, and\nmax sampling is adopted to inference the latent\nmodel. The proposed PNAT is motivated by learn-\ning syntax position (also called syntax distance).\nShen et al. (2018) show that syntax position of\nwords in a sentence could be predicted by neural\nnetworks in a non-autoregressive fashion, which\neven obtains top parsing accuracy among strong\nparser baselines. Given the observations above, we\ntry to directly predict the positions of output words\narXiv:1911.10677v1  [cs.CL]  25 Nov 2019\nto build a NAT model for text generation.\nOur proposed PNAT takes following advan-\ntages:\n•We propose PNAT, which ﬁrst includes po-\nsitions of output words as latent variables for\ntext generation. Experiments show that PNAT\nachieves very top results in non-autoregressive\nNMT, outperforming many strong baselines.\nPNAT also obtains better results than AT in\nparaphrase generation task.\n•Further analysis shows that PNAT has great\npotentials. With the increase of position\nprediction accuracy, performances of PNAT\ncould increase signiﬁcantly. The observations\nmay shed light on the future direction of NAT.\n•Thanks to the explicitly modeling of position,\nwe could control the generation by facilitating\nthe position latent variable, which may enable\ninteresting applications such as controlling\none special word left to another one. We leave\nthis as future work.\n2 Background\n2.1 Autoregressive Decoding\nA target sequence Y=y1:M is decomposed into a\nseries of conditional probabilities autoregressively,\neach of which is parameterized using neural net-\nworks. This approach has become a de facto stan-\ndard in language modeling(Sundermeyer et al.,\n2012), and has been also applied to conditional\nsequence modeling p(Y|X) by introducing an ad-\nditional conditional variable X=x1:N:\np(Y|X) =\nM∏\nt=1\np(yt|y<t,X; θ) (1)\nWith different choices of neural network ar-\nchitectures such as recurrent neural networks\n(RNNs) (Bahdanau et al., 2014; Cho et al., 2014),\nconvolutional neural networks (CNNs) (Krizhevsky\net al., 2012; Gehring et al., 2017), as well as self-\nattention based transformer (Vaswani et al., 2017),\nthe autoregressive decoding has achieved great suc-\ncess in tasks such as machine translation (Bahdanau\net al., 2014), paraphrase generation (Gupta et al.,\n2018), speech recognition (Graves et al., 2013), etc.\n2.2 Non-autoregressive Decoding\nAutoregressive model suffers from the issue of\nslow decoding in inference, because tokens are\ngenerated sequentially and each of them depends\non previous ones. As a solution to this issue, Gu\net al. (2018) proposed Non-Autoregressive Trans-\nformer (denoted as NAT) for machine translation,\nbreaking the dependency among the target tokens\nthrough time by decoding all the tokens simultane-\nously. Put simply, NAT (Gu et al., 2018) factorizes\nthe conditional distribution over a target sequence\ninto a series of conditionally independent distribu-\ntions with respect to time:\np(Y|X) = pL(M|X : θ) ·\nM∏\nt=1\np(yt|X) (2)\nwhich allows trivially ﬁnding the most likely target\nsequence by arg maxY p(Y|X) for each timestep\nt, effectively bypassing computational overhead\nand sub-optimality in decoding from an autoregres-\nsive model.\nAlthough non-autoregressive models achieves\n15×speedup in machine translation compared with\nautoregressive models, it comes at the expense\nof potential performance degradation (Gu et al.,\n2018). The degradation results from the removal\nof conditional dependencies within the decoding\nsentence(yt depend on y<t). Without such depen-\ndencies, the decoder is hard to leverage the inherent\nsentence structure in prediction.\n2.3 Latent Variables for Non-Autoregressive\nDecoding\nA non-autoregressive model could be incorporated\nwith conditional dependency as latent variable to\nalleviate the degradation resulted from the absence\nof dependency:\nP(Y|X) =\n∫\nz\nP(z|X)\nM∏\nt=1\nP(yt|z,X)dz (3)\nFor example, NAT-FT (Gu et al., 2018) models the\ninherent sentence structure with a latent fertility\nvariable, which represents how many target tokens\nthat a source token would translate to. Lee et al.\n(2018) introduces Lintermediate predictions Y1:L\nas random variables , and to reﬁne the predictions\nfrom Y1 to YL in a iterative manner.\n3 PNAT: Position-Based\nNon-Autoregressive Transformer\nWe propose position-based non-autoregressive\ntransformer (PNAT), an extension to transformer in-\ncorporated with non auto-regressive decoding and\nposition learning.\n3.1 Modeling Position with Latent Variables\nLanguages are usually inconsistent with each other\nin word order. Thus reordering is usually required\nwhen translating a sentence from a language to\nanother. In NAT family, words representations or\nencoder states at source side are copied to the target\nside to feed into decoder as its input. Previously,\nGu et al. (2018) utilizes positional attention which\nincorporates positional encoding into decoder at-\ntention to perform local reordering. But such im-\nplicitly reordering mechanism by position attention\nmay cause a repeated generation problem, because\nposition learning module is not optimized directly,\nand is likely to be misguided by target supervision.\nTo tackle with this problem, we propose to ex-\nplicitly model the position as a latent variable. We\nrewrite the target sequence Y with its correspond-\ning position latent variable z = z1:M as a set Yz =\nyz1:zM . The conditional probability P(Y|X) is fac-\ntorized with respect to the position latent variable:\nP(Y|X) =\n∑\nz∈π(M)\nP(z|X) ·P(Y|z,X) (4)\nwhere π(M) is a set consisting of permutations\nwith M elements. At decoding time, the factoriza-\ntion allows us to decode sentences in parallel by\npre-predicting the corresponding position variables\nz.\n3.2 Model Architecture\nAs shown in Figure 1, PNAT is composed of four\nmodules: an encoder stack, a bridge block, a po-\nsition predictor as well as a decoder stack. Be-\nfore detailing each component of PNAT model, we\noverview the architecture for a brief understanding.\nLike most sequence-to-sequence models, PNAT\nﬁrst encodes a source sequence X=x1:N into its\ncontextual word representations E=e1:N with the\nencoder stack. With generated contextual word rep-\nresentation E at source side, the bridge block is\nleveraged to computed the target length M as well\nas the corresponding features D=d1:M, which is\nfed into the decoder as its input. It is worth noting\nthat the decoder inputs Dis computed without re-\nordering. Thus the position predictor is introduced\nto deal with this issue by predicting a permutation\nz=z1:M over D. Finally, PNAT generates the target\nsequence from the decoder input Dand its permu-\ntation z.\nDecoder  \nInputs\nLength Predictor\nSoft-copy\nMulti-Head \nAttention\nAdd & Norm\nx M\nPositions\nMulti-Head  \nAttention\nAdd & Norm\nInput \nEmbedding\nFigure 1: The Transformer - model architecture.\nDecoder: The decoder is also composed of a stack of N =6 identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i .\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension d k , and values of dimension d v . We compute the dot products of the\nquery with all keys, divide each by\np\nd k , and apply a softmax function to obtain the weights on the\nvalues.\n3\nInputs\nN x\nFeed Forward\nAdd & Norm\nFigure 1: The Transformer - model architecture.\nDecoder: The decoder is also composed of a stack of N =6 identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i .\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension d k , and values of dimension d v . We compute the dot products of the\nquery with all keys, divide each by\np\nd k , and apply a softmax function to obtain the weights on the\nvalues.\n3\nFeed Forward\nAdd & Norm\nMulti-Head  \nAttention\nAdd & Norm\nAdd & Norm\nSelf-Masked  \nMulti-Head  \nAttention\nLinear\nSoftmax\nOutput  \nProbabilities\nx N\nc\nBridge Position Predictor\nPointer Predictor\nFigure 1: Illustration of the proposed model, where the\nblack solid arrows represent differentiable connections\nand the dashed arrows are non-differentiable operations.\nEncoder and Decoder Given a source sentence\nX with length N, PNAT encoder produces its con-\ntextual word representations E. The contextual\nword representations Eare further used in comput-\ning target length M and decoder initial states D,\nand are also used as memory of attention at decoder\nside.\nGenerally, PNAT decoder can be considered as a\ntransformer with a broader vision, because it lever-\nages future word information that is blind to the\nautoregressive transformer. Intuitively, we use rela-\ntive position encoding in self-attention(Shaw et al.,\n2018), rather than absolute one that is more likely\nto cause position errors. Following Shaw et al.\n(2018) with a clipping distance d(usually d≥2)\nset for relative positions, we preserve d = 4 rela-\ntions.\nBridge The bridge module predicts the target\nlength M, and initializes the decoder inputsDfrom\nthe source representations E. The target length M\ncould be estimated from the source encoder repre-\nsentation:\nM = N + arg maxφ(E) (5)\nwhere φ(·) produces a categorical distribution\nranged in [−B,B] (B = 20 ). It is notable that\nwe use the predicted length at inference stage, al-\nthough during training, we simply use the length\nof each reference target sequence. Then, we adopt\nthe method proposed by Li et al. (2019) to compute\nD. Given the source representation Eand the es-\ntimated target length M, we linearly combine the\nembeddings of the neighboring source tokens to\ngenerate Das follows:\ndj =\n∑\ni\nwji ·ei (6)\nwji = softmax(−|j−i|/τ) (7)\nwhere wji is a normalized weight that reﬂects the\ncontribution of ei to dj, and τ is a hyperparameter\nindicating the sharpness of the weight distribution.\nPosition Predictor For the proposed PNAT, we\nmodel position permutations with a position pre-\ndictor. As shown in Figure 1, the position predictor\ntakes the decoder inputs Dand the source represen-\ntation Eto predict a permutation z. The position\npredictor has a sub-encoder which stacks multiple\nlayers of encoder units to predict its predicted input\nR=r1:M.\nWith the predicted inputs R, we conduct an\nautoregressive position predictor, denoted as AR-\nPredictor. The AR-Predictor searches a permuta-\ntion z with:\nP(z|D,E) =\nM∏\nt=1\np(zt|z<t,D,E ; θ) (8)\nwhere θis the parameter of AR-Predictor, which\nincludes a RNN-based model incorporated with a\npointer network (Vinyals et al., 2015).\nTo purse the efﬁciency of decoding, we also ex-\nplore a non-autoregressive version for the position\npredictor, denoted as NAR-Predictor, to model the\nposition permutation probabilities with:\nP(z|D,E) =\nM∏\nt=1\np(zt|D,E; θ) (9)\nTo obtain the permutation z, AR-Predictor per-\nforms greedy search whereas NAR-Predictor per-\nforms direct arg max. We chose the AR-Predictor\nas our mainly position module in PNAT, and we\nalso analyze the effectiveness of position modeling\nin Sec. 4.4.\n3.3 Training\nTraining requires maximizing the marginalized\nlikelihood in Eqn. 4. However, this is intractable\nsince we need to enumerate all the M! permuta-\ntions of tokens. We therefore optimize this ob-\njective by Monte Carlo sampling method with a\nheuristic search algorithm.\nAlgorithm 1 Heuristic Search for Positions\nInput: The candidates set of decoder inputs: D=\n{d1,··· ,dM}and target embeddings: Y =\n{y1,··· ,yM};\nOutput: The position of the decoder inputs ˆz.\n1: initial A= {}, ˆD= D, ˆY = Y;\n2: compute the similarity matrix Sim D,Y: the\nSim[i,j] in the matrix is the the similarity be-\ntween the di and yj computing with simi,j =\ncosine (di,yj);\n3: repeat\n4: extract the similarity matrix Sim ˆD,ˆY from\nthe SimD,Y;\n5: select: (i,j) = arg max(i,j) Sim ˆD,ˆY\n6: update: A←A∪{(i,j)}, ˆD← ˆD\\{di},\nˆY ←ˆY \\{yj};\n7: until ˆD= {}and ˆY = {}\n8: for each pair (i,j) in Ado\nset ˆzi = j\n9: end for;\n10: return ˆz\nHeuristic Search for Positions Intuitively, each\ntarget token should have a corresponding decoder\ninput, and meanwhile each decoder input should\nbe assigned to a target token. Based on this idea,\nwe design a heuristic search algorithm to allocate\npositions. Given the decoder inputs and its target\ntokens, we ﬁrst estimate the similarity between\neach pair of the decoder input di and the target\ntoken embedding yj, which is also the weights of\nthe target word classiﬁer:\nsimi,j = cosine (di,yj) (10)\nBased on the cosine similarity matrix, HSP is de-\nsigned to ﬁnd a perfect matching between decoder\ninputs and target tokens:\nHSP(z) = arg max\nz\nM∑\ni=0\n(simi,zi) (11)\nAs shown in Algorithm 1, we perform a greedy\nalgorithm to select the pair with the highest sim-\nilarity score iteratively until the permutation ˆz is\ngenerated.\nThe complexity of this algorithm is o(M3) (M\nis the length of output sentence). Speciﬁcally, the\ncomplexity to select the maximum from the simi-\nlarity matrix is o(M2) for each loop. We need M\nloops of greedy search to allocate positions for all\ndecoder inputs.\nThe intuition behind is that, if the decoder input\ndi is already the most similar one to a target word,\nit would be easier to keep and even reinforce this\nassociation in learning the model. We also analyze\nthe effectiveness of the HSP in the Sec. 4.4.\nObjective Function With the heuristically dis-\ncovered positions as reference positions zref, the\nposition predictor could be trained with a position\nloss:\nLp = −log P(zref|D,E) (12)\nGrounding on the referenced positions, the gener-\native process of target sequences is optimized by:\nLg = −\nM∑\nt=1\nlog P(Y|zref; X) (13)\nFinally, combining two loss functions mentioned\nabove, a full-ﬂedged loss is derived as\nL= Lg + αLp (14)\nThe length predictor is a classiﬁer that follows the\nprevious settings. We also follow the previous prac-\ntice (Gu et al., 2018; Wei et al., 2019) and perform\nan extra training process for the length predictor af-\nter the model trained and do not tune the parameter\nof the encoder.\n3.4 Inference\nWe follow the common choice of approximating\ndecoding algorithms (Gu et al., 2018; Lee et al.,\n2018) to reduce the search space of latent variable\nmodel.\nArgmax Decoding Following Gu et al. (2018),\none simple and effective method is to select the\nbest sequence by choosing the highest-probability\nlatent sequence z:\nz∗= arg max\nz\nP(z|D,E)\nY∗= arg max\ny\nP(Y|z∗,X)\nwhere identifying Y∗only requires independently\nmaximizing the local probability for each output\nposition.\nLength Parallel Decoding We also consider the\ncommon practice of noisy parallel decoding (Gu\net al., 2018), which generates a number of decoding\ncandidates in parallel and selects the best via re-\nscoring using a pre-trained autoregressive model.\nFor PNAT, we ﬁrst predict the target length as\nˆM, then generate output sequence with argmax\ndecoding for each target length candidate M ∈\n[ ˆM−∆M, ˆM+∆M] (M = 4 in our experiments),\nwhich was called length parallel decoding (LPD).\nThen we use the pre-trained autoregressive model\nto rank these sequences and identify the best overall\noutput as the ﬁnal output.\n4 Experiments\nWe test PNAT on several benchmark sequence gen-\neration tasks. We ﬁrst describe the experimental\nsetting and implementation details and then present\nthe main results, followed by some deep studies.\n4.1 Experimental Setting\nTo show the generation ability of PNAT, we con-\nduct experiments on the popular machine transla-\ntion and paraphrase generation tasks. These se-\nquence generation task evaluation models from\ndifferent perspectives. Translation tasks test the\nability of semantic transforming across bilingual\ncorpus. While paraphrase task focuses on substi-\ntution between the same languages while keeping\nthe semantics.\nMachine Translation We valid the effectiveness\nof PNAT on the most widely used benchmarks\nfor machine translation — WMT14 EN-DE(4.5M\npairs) and IWSLT16 DE-EN(196K pairs). The\ndataset is processed with Moses script (Koehn et al.,\n2007), and the words are segmented into subword\nunits using byte-pair encoding (Sennrich et al.,\n2016, BPE). For both WMT datasets, the source\nand target languages share the same set of subword\nembeddings while for IWSLT we use separate em-\nbeddings.\nParaphrase Generation We conduct experi-\nments following previous work (Miao et al., 2019)\nfor paraphrase generation. We make use of the\nestablished Quora dataset 1 to evaluate on the para-\nphrase generation task. We consider the supervised\nparaphrase generation and split the Quora dataset\nin the standard setting. We sample 100k pairs sen-\ntence as training data, and holds out 3k, 30k for\nvalidation and testing, respectively.\n4.2 Implementation Details\nModule Setting For machine translation, we fol-\nlow the settings from Gu et al. (2018). In the case\nof IWSLT task, we use a small setting ( dmodel =\n1https://www.kaggle.com/c/quora-question-pairs/data\n278, dhidden = 507, pdropout = 0.1, nlayer = 5 and\nnhead = 2) suggested by Gu et al. (2018) for Trans-\nformer and NAT models. For WMT task, we use\nthe base setting of the Vaswani et al. (2017) (dmodel\n= 512, dhidden = 512, pdropout = 0.1, nlayer = 6).\nFor paraphrase generation, we follow the set-\ntings from Miao et al. (2019), and set the\n300-dimensional GRU with 2 layer for Seq-to-\nSeq (GRU). We empirically select a Transformer\nand NAT models with hyperparameters (dmodel =\n400, dhidden = 800, pdropout = 0.1, nlayer = 3 and\nnhead = 4).\nOptimization We optimize the parameter with\nthe Adam optimizer (Kingma and Ba, 2014). The\nhyperparameter α used in Eqn. 14 was be set to\n1.0 for WMT, 0.3 for IWSLT and Quora. We\nalso use inverse square root learning rate schedul-\ning (Vaswani et al., 2017) for the WMT, and using\nlinear annealing (from 3e−4 to 1e−5, suggested\nby Lee et al. (2018)) for the IWSLT and Quora.\nEach mini-batch consists of approximately 2K to-\nkens for IWSLT and Quora, 32K tokens for WMT.\nKnowledge Distillation Sequence-level knowl-\nedge distillation is applied to alleviate multi-\nmodality problem while training, using Trans-\nformer as a teacher (Hinton et al., 2015). Previous\nstudies on non-autoregressive generation (Gu et al.,\n2018; Lee et al., 2018; Wei et al., 2019) have used\ntranslations produced by a pre-trained Transformer\nmodel as the training data, which signiﬁcantly im-\nproves the performance. We follow this setting in\ntranslation tasks.\n4.3 Main Results\nMachine Translation We compare the PNAT\nwith strong NAT baselines, including the NAT\nwith fertility (Gu et al., 2018, NAT-FT), the NAT\nwith iterative reﬁnement (Lee et al., 2018, IR-\nNAT), the NAT with regularization (Wang et al.,\n2019, NAT-REG), the NAT with enhanced decoder\ninput (Guo et al., 2019, ENAT), the NAT with\nlearning from auto-regressive model (Wei et al.,\n2019, imitate-NAT), the NAT build on latent vari-\nables (Kaiser et al., 2018, LT), and the ﬂow-based\nNAT model (Ma et al., 2019, Flowseq).\nThe results are shown in Table 1. We basically\ncompare the proposed PNAT against the autore-\ngressive counterpart both in terms of generation\nquality, which is measured with BLEU (Papineni\net al., 2002) and inference speedup. For all our\ntasks, we obtain the performance of competitors\nby either directly using the performance ﬁgures\nreported in the previous works if they are available\nor producing them by using the open source imple-\nmentation of baseline algorithms on our datasets.2\nClearly, PNAT achieves a comparable or better re-\nsult to previous NAT models on both WMT and\nIWSLT tasks.\nWe list the result of the NAT models trained\nwithout using knowledge distillation in the second\nblock of the Table 1. The PNAT achieves signiﬁ-\ncant improvements (more than 13.0 BLEU points)\nover the naive baselines, which indicate that po-\nsition learning greatly contributes to improve the\nmodel capability of NAT model. The PNAT also\nachieves a better result than the Flowseq around\n1.0 BLEU, which demonstrates the effectiveness\nof PNAT in modeling dependencies between the\ntarget outputs.\nAs shown in the third block of the Table 1,\nwithout using reranking techniques, the PNAT out-\nperforms all the competitors with a large margin,\nachieves a balance between performance and ef-\nﬁciency. In particular, the previous state-of-the-\nart(WMT14 DE-EN) Flowseq achieves good per-\nformance with the slow speed(1.1×), while PNAT\ngoes beyond Flowseq in both respects.\nOur best results are obtained with length parallel\ndecoding which employ autoregressive model to\nrerank the multiple generation candidates of differ-\nent target length. Speciﬁcally, on the large scale\nWMT14 DE-EN task, PNAT (+LPD) surpass the\nNAT-REG by 0.76 BLEU score. Without reranking,\nthe gap has increased to 2.4 BLEU score (27.18\nv.s. 24.77). The experiments shows the power of\nexplicitly position modeling which reduces the gap\nbetween non-autoregressive and the autoregressive\nmodels.\nParaphrase Generation Given a sentence, para-\nphrase generation aims to synthesize another sen-\ntence that is different from the given one, but\nconveys the same meaning. Comparing with\ntranslation task, paraphrase generation prefers a\nmore similar order between source and target sen-\ntence, which possibly learn a trivial position model.\nPNAT can potentially yield better results with the\nposition model to infer the relatively ordered align-\nment relationship.\nThe results of the paraphrase generation are\nshown in Table 2. In consist with our intuition,\n2For the sake of fairness, we have chosen the base setting\nfor all competitors.\nModel WMT 14 IWSLT16 SpeedupEN-DE DE-EN DE-EN\nAutoregressive Methods\nTransformer-base (Vaswani et al., 2017) 27.30 / / /\n*Transformer(Beam=4) 27.40 31.33 34.81 1.0 ×\nNon-Autoregressive Methods\nFlowseq (Ma et al., 2019) 18.55 23.36 / /\n*NAT-base / 11.02 / /\n*PNAT 19.73 24.04 / /\nNAT w/ Knowledge Distillation\nNAT-FT (Gu et al., 2018) 17.69 21.47 / 15.6 ×\nLT (Kaiser et al., 2018) 19.80 / / 5.8 ×\nIR-NAT (Lee et al., 2018) 13.91 16.77 27.68 9.0 ×\nENAT (Guo et al., 2019) 20.65 23.02 / 24.3 ×\nNAT-REG (Wang et al., 2019) 20.65 24.77 / -\nimitate-NAT (Wei et al., 2019) 22.44 25.67 / 18.6 ×\nFlowseq (Ma et al., 2019) 21.45 26.16 / 1.1 ×\n*NAT-base / 16.69 / 13.5 ×\n*PNAT 23.05 27.18 31.23 7.3 ×\nNAT w/ Reranking or Iterative Reﬁnments\nNAT-FT (rescoring 10 candidates) 18.66 22.42 / 7.7 ×\nLT (rescoring 10 candidates) 22.50 / / /\nIR-NAT (reﬁnement 10) 21.61 25.48 32.31 1.3 ×\nENAT (rescoring 9 candidates) 24.28 26.10 / 12.4 ×\nNAT-REG (rescoring 9 candidates) 24.61 28.90 / -\nimitate-NAT (rescoring 9 candidates) 24.15 27.28 / 9.7 ×\nFlowseq (rescoring 30 candidates) 23.48 28.40 / /\n*PNAT (LPDn=9,∆M=4) 24.48 29.16 32.60 3.7×\nTable 1: Performance on the newstest-2014 for WMT14 EN-DE and test2013 for IWSLT EN-DE. ‘-’ denotes same\nnumbers as above. ‘*’ indicates our implementation. The decoding speed is measured sentence-by-sentence and the\nspeedup is computed by comparing with Transformer.\nModel Paraphrase(BLEU)\nValid Test\nSeq-to-seq(GRU) 24.68 24.75\nTransformer 25.88 25.46\nNAT-base 19.80 20.34\nPNAT 29.30 29.00\nTable 2: Results on validation set and test set of Quora.\nPNAT achieves the best result on this task and\neven surpass Transformer around 3.5 BLEU. The\nNAT model is not powerful enough to capture the\nlatent position relationship. The comparison be-\ntween NAT-base and PNAT shows that explicit\nposition modeling in PNAT plays a crucial role in\ngenerating sentences.\n4.4 Analysis\nEffectiveness of Heuristic Searched Position\nFirst, we analyze whether the position derived from\nthe heuristic search is suitable for use as super-\nvision to the position predictor. We evaluate the\neffectiveness of the searched position by training\na PNAT as before and testing with the heuristic\nsearched position instead of the predicted position.\nAs shown in the second block of the Table 3, it is\neasier noticed that as PNAT w/ HSP achieves a\nsigniﬁcant improvement over the NAT-base and the\nTransformer, which demonstrates that the heuristic\nsearch for the position is effective.\nEffectiveness and Efﬁciency of Position Model-\ning We are also analysis the accuracy of our po-\nsition modeling and its inﬂuence on the quality of\ngeneration on the WMT14 DE-EN task. For evalu-\nModel Position Accuracy(%) WMT14 DE-EN Speed Uppermutation-acc relative-acc(r=4) BLEU\nTransformer(beam=4) / / 30.68 1.0 ×\nNAT-base / / 16.71 13.5 ×\nPNAT w/ HSP 100.00 100.00 46.03 12.5 ×\nPNAT w/ AR-Predictor 25.30 59.27 27.11 7.3 ×\nPNAT w/ NAR-Predictor 23.11 55.57 20.81 11.7 ×\nTable 3: Results on validation set of WMT14 DE-EN with different position strategy. “HSP” means the reference\nposition sequence derived from the heuristic position searching.\nating the position accuracy, we adopt the heuristic\nsearched position as the position reference (denoted\nas “HSP”), which is the training target of the posi-\ntion predictor. PNAT requires the position informa-\ntion at two places. The ﬁrst is the mutual relative\nrelationship between the states that will be used\nduring decoding. And the second is to reorder the\ndecoded output after decoding. We then propose\nthe corresponding metrics for evaluation, which is\nthe relative position accuracy (with relation thresh-\nold r= 4) and the permutation accuracy.\nAs shown in Table 3, better position accuracy al-\nways yields better generation performance. The\nnon-autoregressive position model is less effec-\ntive than the current autoregressive position model,\nboth in the accuracy of the permutation and the\nrelative position. Even though the current PNAT\nwith a simple AR-Predictor has surpassed the pre-\nvious NAT model, the position accuracy is still less\ndesirable (say, less than 30%) and has a great ex-\nploration space. We provide a few examples in\nAppendix A. There is also a trade-off between the\neffectiveness and efﬁciency, the choice of the non-\nautoregressive means the efﬁciency and the choice\nof autoregressive means the effectiveness.\nModel Paraphrase(BLEU) ∆BLEUw/ RR w/o RR\nNAT-base 20.34 19.45 0.89\nPNAT 29.00 28.95 0.05\nTable 4: Results on test set of Quora datasets. “RR”:\nremove repeats.\nRepeated Generation Analysis Previous NAT\noften suffers from the repeated generation prob-\nlem due to the lack of sequential position informa-\ntion. NAT is less effective to distinguish adjacent\ndecoder hidden states, which is copied from the\nadjacent source representation. To further study\nthis problem, we proposed to evaluate the gains of\nsimply remove the repeated tokens. As shown in\nTable 4, we perform the repeated generation analy-\nsis on the paraphrase generation tasks. Removing\nrepeated tokens has little impact for PNAT model,\nwith only 0.05 BLEU differences. However for\nthe NAT-base model, the gap comes with almost 1\nBLEU (0.89). The results clearly demonstrate that\nthe explicitly position model essentially learns the\nsequential information for sequence generation.\n0 50000 100000 150000 200000 250000 300000\nTrain Steps\n0\n5\n10\n15\n20\n25\n30BLEU\nTransformer\nIR-NAT(iter=1)\nIR-NAT(iter=10)\nNAT-Base\nPNAT\nFigure 2: The learning curves from training of models\non evaluation set of IWSLT-16 DE-EN. Mini-batch size\nis 2048 tokens.\nConvergence Efﬁciency We also perform the\ntraining efﬁciency analysis in IWSLT16 DE-EN\nTranslation task. The learning curves are shown\nin 2. The curve of the PNAT is on the top-left\ncorner. Remarkably, PNAT has the best conver-\ngence speed compared with the NAT competitors\nand even a strong autoregressive model. The re-\nsults are in line with our intuition, that the position\nlearning brings meaningful information of position\nrelationship and beneﬁts the generation of the tar-\nget sentence.\n5 Related Work\nGu et al. (2018) ﬁrst develops a non-autoregressive\nTransformer for neural machine translation (NMT)\ntasks, which produces the outputs in parallel and\nthe inference speed is thus signiﬁcantly boosted.\nDue to the removal of the dependencies between\nthe target outputs, it comes at the cost that the\ntranslation quality is largely sacriﬁced. A line of\nwork has been proposed to mitigate such perfor-\nmance degradation. Some previous work is focused\non enhancing the decoder inputs by replacing the\ntarget words as inputs, such as Guo et al. (2019)\nand Lee et al. (2018). Lee et al. (2018) proposed a\nmethod of iterative reﬁnement based on the latent\nvariable model and denoising autoencoder. Guo\net al. (2019) enhances decoder input by introducing\nthe phrase table in statistical machine translation\nand embedding transformation. Another part of pre-\nvious work focuses on improving the supervision\nof NAT’s decoder states, including imitation learn-\ning from autoregressive models (Wei et al., 2019)\nor regularizing the decoder state with backward\nreconstruction error (Wang et al., 2019). There is\nalso a line studies build upon latent variables, such\nas Kaiser et al. (2018) and Roy et al. (2018) utilize\ndiscrete latent variables for making decoding more\nparallelizable. Moreover, Shao et al. (2019) also\nproposed a method to retrieve the target sequen-\ntial information for NAT models. Unlike previous\nwork, we explicitly model the position, which has\nshown its importance to the autoregressive model\nand can well model the dependence between states.\nTo the best of our knowledge, PNAT is the ﬁrst\nwork to explicitly model position information for\nnon-autoregressive text generation.\n6 Conclusion\nWe proposed PNAT, a non-autoregressive trans-\nformer by explicitly modeled positions, which\nbridge the performance gap between the non-\nautoregressive decoding and autoregressive decod-\ning. Speciﬁcally, we model the position as latent\nvariables, and training with heuristic searched posi-\ntions with MC algorithms. As a result, PNAT leads\nto signiﬁcant improvement and move more close to\nthe performance gap between the NAT and AT on\nmachine translation tasks. Besides, the experimen-\ntal results of the paraphrase generation task show\nthat the performance of the PNAT can exceed that\nof the autoregressive model, and at the same time,\nit also has a large improvement space. According\nto our further analysis on effectiveness of position\nmodeling, in future work, we can still enhance the\nperformance of the NAT model by strengthening\nposition learning.\nA Case Study of Predicted Positions\nWe also provide a few examples in Table 5. For\neach source sentence, we ﬁrst analyze the genera-\ntion quality of the PNAT with a heuristic searched\nposition. Besides, we also show the translation\nwith the predicted position. We have the follow-\ning observations: First, the output generated by\nthe PNAT using the heuristic searched position\nalways keeps the high consistency with the ref-\nerence, shows the effectiveness of the heuristic\nsearched position. Second, better position accu-\nracy always yields better generation performance\n(Case 1,2 against Case 3). Third, as we can see in\ncase 4, though the permutation accuracy is lower,\nit still generates a good result, the reason why we\nchose to use the relative self-attention instead of\nabsolute self-attention.\nSource bei dem deutschen Gesetz geht es um die\nZuweisung bei der Geburt .\nReference German law is about assigning it at birth .\nHeuristic Searched Position(HSP) 3, 6, 1, 2, 10, 0, 5, 4, 7, 8, 9\nPNAT w/ HSP German law is about assigning them at\nbirth .\nPredicted Position 3, 6, 1, 2, 10, 0, 5, 4, 7, 8, 9\nPNAT w/ Predicted Postion German law is about assigning them at\nbirth .\nSource weiÃ§ er Ãijber das Telefon @-@ Hacking\nBescheid ?\nReference does he know about phone hacking ?\nHeuristic Searched Position(HSP) 2, 1, 3, 4, 8, 5, 6, 0, 7, 9\nPNAT w/ HSP does he know the telephone hacking ?\nPredicted Position 1, 0, 3, 4, 8, 5, 6, 2, 7, 9\nPNAT w/ Predicted Postion he know about the telephone hacking ?\nSource was CCAA bedeutet , mÃ˝ uchte eine Be-\nsucherin wissen .\nReference one visitor wants to know what CCAA\nmeans .\nHeuristic Searched Position(HSP) 5, 6, 7, 8, 9, 3, 2, 0, 1, 11, 4, 10\nPNAT w/ HSP a visitor wants to know what CCAA means\n.\nPredicted Position 5, 0, 1, 2, 3, 7, 4, 8, 9, 11, 6, 10\nPNAT w/ Predicted Postion CCAA means wants to know to a visitor .\nSource eines von 20 Kindern in den Vereinigten\nStaaten hat inzwischen eine Lebensmitte-\nlallergie .\nReference one in 20 children in the United States now\nhave food allergies .\nHeuristic Searched Position(HSP) 14, 1, 2, 3, 4, 5, 6, 7, 9, 8, 0, 10, 11, 12, 13\nPNAT w/ HSP one of 20 children in the United States now\nhas food allergy .\nPredicted Position 14, 0, 1, 2, 3, 4, 5, 6, 8, 7, 9, 10, 11, 12, 13\nPNAT w/ Predicted Postion of 20 children in the United States now has\na food allergy .\nTable 5: Examples of translation outputs from PNAT with different setting on WMT14 DE-EN. It is should be\nnoted that the length is different between the position sequence and the output sequence because we keep the origin\nposition output and combine the BPE sequence to word sequence.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua\nBengio. 2014. Neural machine translation by\njointly learning to align and translate. arXiv\npreprint arXiv:1409.0473.\nKyunghyun Cho, Bart van Merrienboer, Caglar\nGulcehre, Dzmitry Bahdanau, Fethi Bougares,\nHolger Schwenk, and Yoshua Bengio. 2014.\nLearning phrase representations using rnn\nencoder–decoder for statistical machine trans-\nlation. In EMNLP, pages 1724–1734.\nJonas Gehring, Michael Auli, David Grangier, De-\nnis Yarats, and Yann N Dauphin. 2017. Convolu-\ntional sequence to sequence learning. In ICML,\npages 1243–1252.\nAlex Graves, Abdel-rahman Mohamed, and Ge-\noffrey Hinton. 2013. Speech recognition with\ndeep recurrent neural networks. In 2013 IEEE\ninternational conference on acoustics, speech\nand signal processing, pages 6645–6649. IEEE.\nJiatao Gu, James Bradbury, Caiming Xiong, Vic-\ntor O.K. Li, and Richard Socher. 2018. Non-\nautoregressive neural machine translation. In\nICLR.\nJunliang Guo, Xu Tan, Di He, Tao Qin, Linli Xu,\nand Tie-Yan Liu. 2019. Non-autoregressive neu-\nral machine translation with enhanced decoder\ninput. In AAAI, volume 33, pages 3723–3730.\nAnkush Gupta, Arvind Agarwal, Prawaan Singh,\nand Piyush Rai. 2018. A deep generative frame-\nwork for paraphrase generation. In Thirty-\nSecond AAAI Conference on Artiﬁcial Intelli-\ngence.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean.\n2015. Distilling the knowledge in a neural net-\nwork. arXiv preprint arXiv:1503.02531.\nLukasz Kaiser, Samy Bengio, Aurko Roy, Ashish\nVaswani, Niki Parmar, Jakob Uszkoreit, and\nNoam Shazeer. 2018. Fast decoding in sequence\nmodels using discrete latent variables. In ICML,\npages 2395–2404.\nDiederik P Kingma and Jimmy Ba. 2014. Adam:\nA method for stochastic optimization. arXiv\npreprint arXiv:1412.6980.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch,\nChris Callison-Burch, Marcello Federico, Nicola\nBertoldi, Brooke Cowan, Wade Shen, Christine\nMoran, Richard Zens, Chris Dyer, Ondˇrej Bojar,\nAlexandra Constantin, and Evan Herbst. 2007.\nMoses: Open source toolkit for statistical ma-\nchine translation. In ACL, pages 177–180.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E\nHinton. 2012. Imagenet classiﬁcation with deep\nconvolutional neural networks. In NIPS, pages\n1097–1105.\nJason Lee, Elman Mansimov, and Kyunghyun Cho.\n2018. Deterministic non-autoregressive neural\nsequence modeling by iterative reﬁnement. In\nEMNLP, pages 1173–1182.\nZhuohan Li, Di He, Fei Tian, Tao Qin, Liwei Wang,\nand Tie-Yan Liu. 2019. Hint-based training for\nnon-autoregressive translation. In NeuralIPS (to\nappear).\nTianyu Liu, Kexiang Wang, Lei Sha, Baobao\nChang, and Zhifang Sui. 2018. Table-to-text\ngeneration by structure-aware seq2seq learning.\nIn Thirty-Second AAAI Conference on Artiﬁcial\nIntelligence.\nXuezhe Ma, Chunting Zhou, Xian Li, Graham Neu-\nbig, and Eduard Hovy. 2019. FlowSeq: Non-\nautoregressive conditional sequence generation\nwith generative ﬂow. In EMNLP-IJCNLP, pages\n4273–4283, Hong Kong, China.\nNing Miao, Hao Zhou, Lili Mou, Rui Yan, and\nLei Li. 2019. CGMH: Constrained sentence\ngeneration by Metropolis-Hastings sampling. In\nAAAI.\nKishore Papineni, Salim Roukos, Todd Ward, and\nWei-Jing Zhu. 2002. BLEU: A method for auto-\nmatic evaluation of machine translation. In ACL,\npages 311–318.\nAurko Roy, Ashish Vaswani, Niki Parmar, and\nArvind Neelakantan. 2018. Towards a better\nunderstanding of vector quantized autoencoders.\narXiv.\nRico Sennrich, Barry Haddow, and Alexandra\nBirch. 2016. Neural machine translation of rare\nwords with subword units. In ACL, pages 1715–\n1725.\nChenze Shao, Yang Feng, Jinchao Zhang, Fan-\ndong Meng, Xilin Chen, and Jie Zhou. 2019.\nRetrieving sequential information for non-\nautoregressive neural machine translation. In\nACL.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position repre-\nsentations. In NAACL-HLT, pages 464–468.\nXiaoyu Shen, Hui Su, Yanran Li, Wenjie Li, Shuzi\nNiu, Yang Zhao, Akiko Aizawa, and Guoping\nLong. 2017. A conditional variational frame-\nwork for dialog generation. In Proceedings of\nthe 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Pa-\npers), pages 504–509.\nYikang Shen, Zhouhan Lin, Athul Paul Jacob,\nAlessandro Sordoni, Aaron Courville, and\nYoshua Bengio. 2018. Straight to the tree: Con-\nstituency parsing with neural syntactic distance.\nIn ACL, pages 1171–1180.\nMartin Sundermeyer, Ralf Schlüter, and Hermann\nNey. 2012. Lstm neural networks for language\nmodeling. In Thirteenth annual conference of\nthe international speech communication associ-\nation.\nAshish Vaswani, Noam Shazeer, Niki Parmar,\nJakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. 2017. At-\ntention is all you need. In NIPS, pages 5998–\n6008.\nOriol Vinyals, Meire Fortunato, and Navdeep Jaitly.\n2015. Pointer networks. In NIPS, pages 2692–\n2700.\nYiren Wang, Fei Tian, Di He, Tao Qin, ChengX-\niang Zhai, and Tie-Yan Liu. 2019. Non-\nautoregressive machine translation with auxil-\niary regularization. In AAAI.\nBingzhen Wei, Mingxuan Wang, Hao Zhou, Jun-\nyang Lin, and Xu Sun. 2019. Imitation learning\nfor non-autoregressive neural machine transla-\ntion. In ACL.",
  "topic": "Autoregressive model",
  "concepts": [
    {
      "name": "Autoregressive model",
      "score": 0.8676340579986572
    },
    {
      "name": "Paraphrase",
      "score": 0.8583479523658752
    },
    {
      "name": "Generative grammar",
      "score": 0.7035353183746338
    },
    {
      "name": "Computer science",
      "score": 0.6722686290740967
    },
    {
      "name": "Transformer",
      "score": 0.6200722455978394
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6000638604164124
    },
    {
      "name": "Position (finance)",
      "score": 0.5622124075889587
    },
    {
      "name": "Generative model",
      "score": 0.5363205075263977
    },
    {
      "name": "Latent variable",
      "score": 0.5141404271125793
    },
    {
      "name": "STAR model",
      "score": 0.4972856342792511
    },
    {
      "name": "Machine translation",
      "score": 0.46168556809425354
    },
    {
      "name": "On the fly",
      "score": 0.4584192633628845
    },
    {
      "name": "Machine learning",
      "score": 0.42536044120788574
    },
    {
      "name": "Natural language processing",
      "score": 0.3634306788444519
    },
    {
      "name": "Econometrics",
      "score": 0.18899136781692505
    },
    {
      "name": "Mathematics",
      "score": 0.14309540390968323
    },
    {
      "name": "Time series",
      "score": 0.11076620221138
    },
    {
      "name": "Engineering",
      "score": 0.10753527283668518
    },
    {
      "name": "Autoregressive integrated moving average",
      "score": 0.08287012577056885
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Finance",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I881766915",
      "name": "Nanjing University",
      "country": "CN"
    }
  ]
}