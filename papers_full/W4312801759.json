{
  "title": "Exploring Advances in Transformers and CNN for Skin Lesion Diagnosis on Small Datasets",
  "url": "https://openalex.org/W4312801759",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5075072841",
      "name": "Leandro M. de Lima",
      "affiliations": [
        "Universidade Federal do Espírito Santo"
      ]
    },
    {
      "id": "https://openalex.org/A5015026639",
      "name": "Renato A. Krohling",
      "affiliations": [
        "Universidade Federal do Espírito Santo"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6803916128",
    "https://openalex.org/W3170224286",
    "https://openalex.org/W3206824631",
    "https://openalex.org/W2982220924",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W3151130473",
    "https://openalex.org/W3157528469",
    "https://openalex.org/W3211088544",
    "https://openalex.org/W3174068320",
    "https://openalex.org/W2892342267",
    "https://openalex.org/W2986445670",
    "https://openalex.org/W3166225737",
    "https://openalex.org/W4213019189",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2302255633",
    "https://openalex.org/W4214636423",
    "https://openalex.org/W4200221062",
    "https://openalex.org/W4206706211",
    "https://openalex.org/W3097217077",
    "https://openalex.org/W3026316716",
    "https://openalex.org/W4312349930",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3133356497",
    "https://openalex.org/W2984412793",
    "https://openalex.org/W3081500256",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W2144049370",
    "https://openalex.org/W3172509117",
    "https://openalex.org/W6605052884",
    "https://openalex.org/W3192208986",
    "https://openalex.org/W4214634256",
    "https://openalex.org/W3179869055",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3034552520",
    "https://openalex.org/W6931259419",
    "https://openalex.org/W3035160371",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W4214709605",
    "https://openalex.org/W3209859545",
    "https://openalex.org/W4214808100",
    "https://openalex.org/W3195960585",
    "https://openalex.org/W4226512186",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3164024107",
    "https://openalex.org/W3213836217",
    "https://openalex.org/W3117450517",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3145444543",
    "https://openalex.org/W3128633047",
    "https://openalex.org/W4308831279",
    "https://openalex.org/W3170863103",
    "https://openalex.org/W4376983087",
    "https://openalex.org/W2943152387",
    "https://openalex.org/W4294498805",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W4226157573",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W3100676321",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W3171206729",
    "https://openalex.org/W3185035581",
    "https://openalex.org/W4287116734"
  ],
  "abstract": null,
  "full_text": "Exploring Advances in Transformers and CNN for\nSkin Lesion Diagnosis on Small Datasets\nLeandro M. de Lima\nGraduate Program in Computer Science, PPGI\nFederal University of Espirito Santo, UFES\nVit´oria, Brazil\nleandro.m.lima@ufes.br\nRenato A. Krohling\nGraduate Program in Computer Science, PPGI\nProduction Engineering Department, LABCIN\nFederal University of Espirito Santo, UFES\nVit´oria, Brazil\nrkrohling@inf.ufes.br\nAbstract—Skin cancer is one of the most common types of\ncancer in the world. Different computer-aided diagnosis systems\nhave been proposed to tackle skin lesion diagnosis, most of them\nbased in deep convolutional neural networks. However, recent\nadvances in computer vision achieved state-of-art results in many\ntasks, notably Transformer-based networks. We explore and eval-\nuate advances in computer vision architectures, training methods\nand multimodal feature fusion for skin lesion diagnosis task.\nExperiments show that PiT ( 0.800±0.006), CoaT (0.780±0.024)\nand ViT (0.771±0.018) backbone models with MetaBlock fusion\nachieved state-of-art results for balanced accuracy metric in PAD-\nUFES-20 dataset.\nIndex Terms—transformer, CNN, skin lesion, multimodal fu-\nsion, classiﬁcation\nI. I NTRODUCTION\nA third of cancer diagnoses in the world are skin cancer\ndiagnoses according to the World Health Organization (WHO).\nIn order to diagnose skin cancer, dermatologists screen the\nskin lesion, assess the patient clinical information, and use\ntheir experience to classify the lesion [1]. The high incidence\nrate and the lack of experts and medical devices, speciﬁcally\nin rural areas [2] and emerging countries [3], have increased\nthe demand for computer-aided diagnosis (CAD) systems for\nskin cancer.\nOver the past decades, different computer-aided diagnosis\n(CAD) systems have been proposed to tackle skin cancer\ndetection [4], [5]. In general, these systems are based on\nclinical information from the patient and information extracted\nfrom lesion images [6]. Image features and features extracted\nfrom clinical information need to be merged. This task is\nknown as multimodal data fusion.\nThe use of neural network has become the de-facto standard\nas a backbone for extracting visual features in various tasks.\nDeep Convolutional Neural Network (CNN) based architec-\ntures are widely used for this. However, recently, Transformer-\nbased networks stand out for achieving comparable perfor-\nmance in various tasks. Other recent advances, such as new\ntraining methods [7] and the proposal of new architectures [8],\nmay also contribute to an improvement in the performance of\nthe skin lesion diagnosis task.\nThis work has as main objectives investigate the perfor-\nmance of the most recent architectures for computer vision in\nthe problem of skin lesion detection. We also investigate the\nperformance of feature fusion methods for the problem of skin\nlesion detection. Additionally, we investigate how these new\narchitectures and these fusion methods can be integrated and\nthe resulting performance is compared.\nThe following are the key research contributions of the\nproposed work.\n• we conduct extensive experiments on the open dataset\nPAD-UFES-20 [9] and achieved performance comparable\nto state-of-art methods.\n• we show that Transformer-based image feature extrac-\ntors achieve competitive performance against CNN-based\nbackbones to skin lesion diagnosis task.\n• we show that Transformer-based image extracted features\ncan be fused with clinical information using already\nexistent fusion methods.\n• we show that recent training methods (distillation and\nsemi-weakly supervised training) and recent architectures\n(ResNeXt, PiT and CoaT) effectively can improve perfor-\nmance in skin lesion diagnosis task.\nII. L ITERATURE REVIEW\nA. CNN-based vision backbones\nA Deep Convolutional Neural Network (CNN) model con-\nsists of several convolution layers followed by activation func-\ntions and pooling layers. Additionally, it has several fully con-\nnected layers before prediction. It comes into deep structure\nto facilitate ﬁltering mechanisms by performing convolutions\nin multi-scale feature maps, leading to highly abstract and\ndiscriminative features [10].\nSeveral architectures have been developed since the AlexNet\narchitecture, considered as the foundation work of modern\ndeep CNN, with great emphasis on architectures such as\nResNet, DenseNet and EfﬁcientNet [11]. More recently, im-\nprovements in these networks have been proposed as in ResNet\nV2 [12], ResNexT [13] and EfﬁcientNet V2 [14]. In addition\nto the development of architectures, there are proposals for\nnew mechanisms (e.g. Efﬁcient Channel Attention [15]) and\nnew training methods (e.g. Semi-weakly Supervised Learning\n[16], Distillation [17]).\narXiv:2205.15442v1  [cs.CV]  30 May 2022\nB. Transformer-based vision backbones\nWith the great success of Transformer-based architectures\nfor NLP tasks, there has recently been a great interest in\nresearching Transformer-based architectures for computing\nvision [18]–[21]. Vision Transformer (ViT) [22] was one of\nthe pioneers to present comparable results with the CNN\narchitectures, until then dominant. Its architecture is heavily\nbased on the original Transformer model [23] and it process\nthe input image as a sequence of patches, as shown in Figure\n1.\nFig. 1. ViT architecture overview (left) and details of Transformer Encoder\n(right). Image from [22].\nInspired by the advances achieved by the ViT model,\nvarious models and modiﬁcations (e.g. TNT [24], Swin [25]\n/ SwinV2 [26], CrossViT [27], XCiT [28], PiT [29], CaiT\n[30] ) were proposed and presented promising results. In\naddition, several improvements in training methods (e.g. BeiT\n[31], DeiT [32], iBOT [33], DINO [34]) have also contributed\nto an improvement in the performance of Transformer-based\nmodels.\nAn interesting ability of Transformer-based models is that\nViTs trained on ImageNet exhibit higher shape-bias in compar-\nison to similar capacity CNN models [35]. Transformer-based\nmodels can reach human-level shape-bias performance when\ntrained on a stylized version of ImageNet (SIN [36]) and they\ncan even model contradictory cues (as in shape/texture-bias)\nwith distinct tokens [35].\nC. Multimodal Fusion\nThere are some aggregation-based multimodal fusion ap-\nproaches. The most common method of multimodal fusion is\nan aggregation via concatenation of features [1]. This fusion\nmethod consists of concatenating the features into a single\ntensor with all features.\nChannel-Exchanging-Network (CEN) [37], a parameter-free\nmultimodal fusion framework, is another approach that pro-\nposes an exchanging in CNN channels of sub-networks. There\nis an intrinsic limitation in it as the framework assumes that\nall sub-networks have a CNN architecture.\nAn interesting alternative is the Metadata Processing Block\n(MetaBlock) [38], that is an attention-based method that uses\na LSTM-like gates to enhance the metadata into the feature\nmaps extracted from an image for skin cancer classiﬁcation.\nThis method main limitation is that it is proposed for only\none feature map and one metadata source and there is no clear\ninformation how to scale it for multiple sources.\nAlso, there is MetaNet [39], a multiplication-based data\nfusion to make the metadata directly interact with the visual\nfeatures. It proposes use the metadata to control the importance\nof each feature channel at the last convolutional layer.\nMutual Attention Transformer (MAT) [6] uses an attention-\nbased multimodal fusion method (Transformer and Fusion\nunit) that is inspired in transformer architecture. Being pro-\nposed for only two features sources is a limitation in it.\nMAT presents a guided-attention module which has some\nsimilarities with the cross-attention module in the Cross-\nAttention Fusion [27]. Cross-Attention Fusion is the proposed\nmethod for multi-scale feature fusion in CrossViT architecture.\nIII. P ROPOSED EVALUATION METHODOLOGY\nFocused on investigating performance, techniques and re-\ncent architectures that bring advantage to the skin lesion clas-\nsiﬁcation task, we selected 20 models with 30M parameters\nor less and that needed less than 12GB GPU memory (due\nhardware limitation). In case of identical architectures and\ntechniques, we use only the best model of them. We took the\n20 best top- 1 validation scores in TIMM pre-trained models\ncollection [40] evaluated in ImageNet ”Reassessed Labels”\n(ImageNet-ReaL) [41], the usual ImageNet-1k validation set\nwith a fresh new set of labels intended to improve on mistakes\nin the original annotation process. All selected models are\nlisted in Table I with their number of parameters and what\nkind of architecture they are based on.\nTABLE I\nNUMBER OF PARAMETERS AND BASE ARCHITECTURE OF THE TOP -20\nSELECTED MODELS\nModel Based Param. Ref.\ncait xxs24 384 Transformer 12.03 M [30]\ngc efﬁcientnetv2 rw t CNN 13.68 M [14], [42]\nrexnet 200 CNN 16.37 M [43]\ntf efﬁcientnet b4 ns CNN 19.34 M [44], [45]\nregnety 032 CNN 19.44 M [46]\ncoat lite small Transformer 19.84 M [47]\ntf efﬁcientnetv2 s in21ft1k CNN 21.46 M [14]\nvit small patch16 384 Transformer 22.20 M [22], [48]\nhalo2botnet50ts 256 Hybrid 22.64 M [49], [50]\nhalonet50ts Hybrid 22.73 M [50], [51]\ntnt s patch16 224 Transformer 23.76 M [24]\npit s distilled 224 Transformer 24.04 M [29]\ntwins svt small Transformer 24.06 M [52]\neca nfnet l0 CNN 24.14 M [15], [53]\nswsl resnext50 32x4d CNN 25.03 M [13], [16]\nresnetv2 50x1 bit distilled CNN 25.55 M [12], [54], [55]\necaresnet50t CNN 25.57 M [51]\nxcit small 12 p16 384 dist Transformer 26.25 M [28]\nregnetz d8 CNN 27.58 M [56]\ncrossvit 15 dagger 408 Transformer 28.50 M [27]\nFor the evaluation of all models, we based our methodology\non the one proposed by [1], as shown in Figure 2. The input\ndata is composed of a clinical image of the lesion and clinical\ninformation. The clinical image features are extracted by a\nbackbone model pre-trained for general image tasks. Since\nwe test a large range of different architectures, we need a\nimage feature adapter to reformat it to a standard shape. That\nadapter consist of a 2D adaptive average pooling layer for\nhybrid or CNN-based image features. For transformers-based\nimage features, the adapter only selects the class token and\ndiscard other image information, except for CrossViT that big\nand small class tokens are concatenated and selected. The\nadapter also has a ﬂatten layer after it to properly reformat\nthe image features to a standard shape. Next, a fusion block\nadd clinical information to reformatted image features. In this\npaper we analyze four alternatives to the fusion block, i.e.,\nConcatenation, MetaBlock, MetaNet, MAT (Transformer and\nFusion unit only). For a fair comparison, before the classiﬁer,\nwe apply a feature reducer to keep the classiﬁer input the same\nsize to all models, as most of those image feature extractor\nmodels have different image features size.\nFig. 2. Fusion architecture based on [38].\nThe selected pre-trained models are ﬁne-tuned with su-\npervised learning on PAD-UFES-20 dataset [9]. The dataset\nhas 2298 samples of 6 types of skin lesions, consisting of a\nclinical image collected from smartphone devices and a set of\npatient clinical data containing 21 features. The skin lesions\nare Basal Cell Carcinoma, Squamous Cell Carcinoma, Actinic\nKeratosis, Seborrheic Keratosis, Melanoma, and Nevus. The\nclinical features include patient’s age, skin lesion location,\nFitzpatrick skin type, skin lesion diameter, family background,\ncancer history, among others. That dataset was chosen pre-\ncisely because it deals with multimodal data and is one of\nthe few containing this type of information for skin lesion\ndiagnosis. Most deal only with dermoscopic images without\nclinical information of the lesion.\nTABLE II\nTOP-20 SELECTED MODELS CONCATENATION FUSION PERFORMANCE\nORDERED BY BALANCED ACCURACY (BCC). M EAN AND STANDARD\nDEVIATION OF BCC AND AREA UNDER THE ROC CURVE (AUC) METRICS .\nModel BCC AUC\nresnetv2 50x1 bit distilled 0.765 ± 0.013 0.934 ± 0.002\npit s distilled 224 0.763 ± 0.025 0 .928 ± 0.009\ncoat lite small 0.759 ± 0.024 0 .929 ± 0.002\nvit small patch16 384 0.751 ± 0.017 0 .926 ± 0.011\nregnety 032 0.748 ± 0.018 0 .927 ± 0.010\ntwins svt small 0.747 ± 0.027 0 .927 ± 0.005\necaresnet50t 0.742 ± 0.039 0 .924 ± 0.008\ntf efﬁcientnetv2 s in21ft1k 0.741 ± 0.027 0.934 ± 0.004\nregnetz d8 0.739 ± 0.021 0 .930 ± 0.007\ngc efﬁcientnetv2 rw t 0.739 ± 0.026 0 .926 ± 0.010\neca nfnet l0 0.736 ± 0.037 0 .926 ± 0.007\nswsl resnext50 32x4d 0.731 ± 0.028 0 .925 ± 0.004\nrexnet 200 0.728 ± 0.028 0 .928 ± 0.007\nxcit small 12 p16 384 dist 0.727 ± 0.032 0 .921 ± 0.010\ntf efﬁcientnet b4 ns 0.726 ± 0.017 0 .923 ± 0.016\ntnt s patch16 224 0.725 ± 0.025 0 .925 ± 0.007\ncrossvit 15 dagger 408 0.718 ± 0.033 0 .919 ± 0.008\nhalo2botnet50ts 256 0.701 ± 0.023 0 .916 ± 0.011\ncait xxs24 384 0.660 ± 0.027 0 .910 ± 0.006\nhalonet50ts 0.644 ± 0.054 0 .903 ± 0.015\nIV. E XPERIMENTS\nOur experiments follows the setup used in [38]. Training\nruns for 150 epochs using batch size 30 and reducer block\nsize of 90. An early stop strategy is set if training for 15\nepochs without improvement. We use the SGD optimizer\n(initial learning rate is 0.001, momentum is 0.9 and weight\ndecay is 0.001) with reduce learning rate on plateau strategy\n(patience is 10, reduce factor is 0.1 and lower bound on the\nlearning rate is 10−6). For evaluation, we adopted 5-fold cross-\nvalidation.\nA. General Analysis\nTable II list the balanced accuracy (BCC) and area\nunder the ROC curve (AUC) performance of each pre-\ntrained backbone model using Concatenation fusion. The\nmodel ”resnetv2 50x1 bit distilled” achieved the best bal-\nanced accuracy and area under the ROC curve per-\nformance with 0.765 ± 0.013 and 0.934 ± 0.002. The\n”tf efﬁcientnetv2 s in21ft1k” backbone model achieved the\nsame best area under the ROC curve performance 0.934 ±\n0.004.\nFrom the results listed in Table II we concluded that the use\nof Transformer-based backbone can help improve skin lesion\ndiagnosis performance, since in the top 5 balanced accuracy\nperformance three of them are Transformer-based architectures\nand two of them are CNN-based architectures. It can also be\nnoted the presence of two architectures that used the model\ndistillation [17] technique for training.\nNext, we will detail the main architectures and present the\nmain characteristics that may have contributed to its good\nresult in the top- 5 pre-trained models.\n1) Model Distillation: Also known as Knowledge Dis-\ntillation [17], refers to the training paradigm in which a\nstudent model leverages ”soft” labels coming from a strong\nteacher network. The output vector of the teacher’s softmax\nfunction is used rather than just the maximum of scores,\nwhich gives a “hard” label. This process can be seen as a\nway of compressing the teacher model into a reduced student\nmodel. In Transformer-based models, a distillation token can\nbe added to the model, along class and patch tokens, and used\nto improve learning [32].\n2) ResNet V2: The Residual Neural Network (ResNet) V2\n[12] mainly focuses on making the second non-linearity as\nan identity mapping by removing the last ReLU activation\nfunction, after the addition layer, in the residual block. That\nis, using the pre-activation of weight layers instead of post-\nactivation.\nThe arrangement of the layers in the residual block moves\nthe batch normalization and ReLU activation to comes before\n2D convolution, as shown in Figure 3.\nFig. 3. Differences in residual block of ResNet versions. Based on image\nfrom [12].\n3) PiT: Pooling-based Vision Transformer (PiT) architec-\nture [29] is inspired in ResNet-style dimensions settings to im-\nprove the model performance, as shown in Figure 4. PiT uses a\nnewly designed pooling layer based on depth-wise convolution\nto achieve channel multiplication and spatial reduction.\nFig. 4. Pooling-based Vision Transformer (PiT) architecture. Image from [29].\n4) CoaT: Co-scale conv-attentional image Transformers\n(CoaT) [47] is a Transformer-based model equipped with co-\nscale and conv-attentional mechanisms. It empowers image\nTransformers with enriched multi-scale and contextual model-\ning capabilities. The conv-attentional module realize relative\nposition embeddings with convolutions in the factorized atten-\ntion module, which improves computational efﬁciency.\nCoaT designs a series of serial and parallel blocks to realize\nthe co-scale mechanism as shown in Figure 5. The serial block\nmodels image representations in a reduced resolution. Then,\nCoaT realizes the co-scale mechanism between parallel blocks\nin each parallel group.\nFig. 5. Co-scale conv-attentional image Transformers (CoaT) architecture.\nImage from [47].\n5) ViT: Due the success of Transformer-based architectures\nfor NLP tasks, there has recently been a great interest in\nresearching Transformer-based architectures for computing\nvision [18]–[21]. Vision Transformer (ViT) [22] was one of\nthe pioneers to present comparable results with the CNN\narchitectures, until then dominant. Its architecture is heavily\nbased on the original Transformer model [23] and it process\nthe input image as a sequence of patches, as shown in Figure\n1.\n6) RegNetY: RegNetY [46] is one of a family of models\nproposed by a methodology to design network design spaces,\nwhere a design space is a parametrized set of possible model\narchitectures. Each RegNet network consists of a stem, fol-\nlowed by the network body that performs the bulk of the\ncomputation, and then a head (average pooling followed by\na fully connected layer) that predicts n output classes. The\nnetwork body is composed of a sequence of stages that operate\nat progressively reduced resolution. Each stage consists of\na sequence of identical blocks, except the ﬁrst block which\nuses stride-two convonvolution. While the general structure is\nsimple, the total number of possible network conﬁgurations\nis vast. For RegNetY models, the blocks in each stage are\nbased on the standard residual bottleneck block with group\nconvolution and with the addition of a Squeeze and Excite\nBlock after the group convolution, as shown in Figure 6.\nB. Feature Fusion Analysis\nAiming to analyze feature extraction models alternatives\nand their impact for the fusion of clinical image and clinical\ninformation features, the top- 5 models with the best balanced\naccuracy performance in Table II is compared using multiple\nFig. 6. RegNetY Y block diagram (when stride >1).\nfusion methods. Each selected model is evaluated using the\nfusion methods: Concatenation (already presented in Table II),\nMetaBlock, MetaNet and MAT (Transformer and Fusion unit\nonly). As baseline we use the best performance result in [1],\nwhich is a balanced accuracy of 0.770±0.016 (EfﬁcientNet-B4\nwith MetaBlock fusion) and area under the ROC curve (AUC)\nof 0.945±0.005 (EfﬁcientNet-B4 with Concatenation fusion).\nCurrently, those are the best results published for PAD-UFES-\n20 dataset.\nResults listed in Table III show that most of CNN-\nbased models evaluated have a better BCC and AUC\nwith Concatenation Fusion and most of Transformer-\nbased models evaluated have a better BCC and AUC\nwith MataBlock fusion. Analysing balanced accuracy met-\nric, ”pit s distilled 224” ( 0.800 ± 0.006), ”coat lite small”\n(0.780 ±0.024) and ”vit small patch16 384” (0.771 ±0.018)\nmodels with MetaBlock fusion achieved state-of-art results.\nFor area under the ROC curve metric, ”pit s distilled 224”\n(0.941 ± 0.006) model with MetaBlock fusion has the best\nmean result among proposed models, but EfﬁcientNet-B4 with\nConcatenation fusion baseline has the best AUC among all of\nthem. All models highest average AUC (in bold at Table III)\ndo note achieved state-of-art results.\nV. C ONCLUSION\nComputer-aided diagnosis (CAD) systems for skin cancer\nhas an increasing demand. Computer vision has largely helped\nthe development of efﬁcient predictive models for skin lesion\ndiagnosis. Recent advances in computer vision, as newly\narchitectures and training methods, has provided performance\nimprovements in many tasks. This work seeks to clarify\nand guide researchers about which architectures and other\nadvances present an effective improvement in skin lesion\ndiagnosis task. It also attempts to elucidate which are the most\nsuitable pre-trained models available for the task.\nThe average AUC of ”pit s distilled 224” (0.941 ± 0.006)\nand ”coat lite small” (0.940±0.002) models with MetaBlock\nfusion achieved competitive results. Experiments also show\nthat ”pit s distilled 224” ( 0.800 ± 0.006), ”coat lite small”\n(0.780 ±0.024) and ”vit small patch16 384” (0.771 ±0.018)\nmodels with MetaBlock fusion achieved balanced accuracy\nmetric state-of-art results. It shows that distillation and trans-\nformer architectures (PiT, CoaT and ViT) can improve perfor-\nmance in skin lesion diagnosis task. Our ﬁndings are not in\naccordance with that provided by [57], as they conclude that\nCNN-based models obtained better results in small datasets\n(840 images and 21 labels, smaller dataset than PAD-UFES-\nTABLE III\nTOP-5 SELECTED MODELS WITH PERFORMANCE FOR FUSION METHOD\nCOMPARISON . IN BOLD IS HIGHLIGHTED THE HIGHEST AVERAGE FOR\nEACH BACKBONE MODEL .\nModel BCC AUC\nBaseline [1]\nEfﬁcientNet-B4 w/ MetaBlock 0.770 ± 0.016 0.944 ± 0.004\nEfﬁcientNet-B4 w/ Concatenation 0.758 ± 0.012 0.945 ± 0.005\nConcatenation Fusion\ncoat lite small 0.759 ± 0.024 0.929 ± 0.002\npit s distilled 224 0.763 ± 0.025 0.928 ± 0.009\nregnety 032 0.748 ± 0.018 0.927 ± 0.010\nresnetv2 50x1 bit distilled 0.765 ± 0.013 0.934 ± 0.002\nvit small patch16 384 0.751 ± 0.017 0.926 ± 0.011\nMAT Fusion\ncoat lite small 0.685 ± 0.015 0.909 ± 0.009\npit s distilled 224 0.704 ± 0.027 0.913 ± 0.013\nregnety 032 0.678 ± 0.012 0.913 ± 0.006\nresnetv2 50x1 bit distilled 0.716 ± 0.022 0.917 ± 0.007\nvit small patch16 384 0.729 ± 0.032 0.920 ± 0.015\nMetaBlock Fusion\ncoat lite small 0.780 ± 0.024 0.940 ± 0.002\npit s distilled 224 0.800 ± 0.006 0.941 ± 0.006\nregnety 032 0.705 ± 0.015 0.920 ± 0.005\nresnetv2 50x1 bit distilled 0.702 ± 0.015 0.918 ± 0.004\nvit small patch16 384 0.771 ± 0.018 0.936 ± 0.008\nMetaNet Fusion\ncoat lite small 0.732 ± 0.038 0.927 ± 0.011\npit s distilled 224 0.754 ± 0.028 0.932 ± 0.008\nregnety 032 0.717 ± 0.013 0.923 ± 0.010\nresnetv2 50x1 bit distilled 0.752 ± 0.020 0.936 ± 0.006\nvit small patch16 384 0.767 ± 0.021 0.938 ± 0.011\n20) and here experiments indicate that Transformer-based\nbackbones achieved state-of-art results.\nFor future work, one can investigate a fusion method that\ntakes advantage of the speciﬁcs of the format of the features\nextracted by Transformer-based backbones. Novel architec-\ntures or training methods can be further investigated and devel-\noped. There is also room for improvement in the understand\nof the impact of the advances in model interpretability and\nexplainability in skin lesion diagnosis.\nREFERENCES\n[1] A. G. Pacheco and R. A. Krohling, “The impact of patient clinical\ninformation on automated skin cancer detection,” Computers in Biology\nand Medicine, vol. 116, p. 103545, 2020.\n[2] H. Feng, J. Berk-Krauss, P. W. Feng, and J. A. Stein, “Comparison of\ndermatologist density between urban and rural counties in the united\nstates,” JAMA Dermatology, vol. 154, no. 11, pp. 1265–1271, 2018.\n[3] R. M. Schefﬂer, J. X. Liu, Y . Kinfu, and M. R. Dal Poz, “Forecasting the\nglobal shortage of physicians: an economic-and needs-based approach,”\nBulletin of the World Health Organization, vol. 86, pp. 516–523B, 2008.\n[4] A. Takiddin, J. Schneider, Y . Yang, A. Abd-Alrazaq, M. Househ et al.,\n“Artiﬁcial intelligence for skin cancer detection: scoping review,” Jour-\nnal of Medical Internet Research, vol. 23, no. 11, p. e22934, 2021.\n[5] T. Das, V . Kumar, A. Prakash, and A. M. Lynn, “Artiﬁcial intelligence\nin skin cancer: diagnosis and therapy,” in Skin Cancer: Pathogenesis\nand Diagnosis. Springer, 2021, pp. 143–171.\n[6] L. Zhou and Y . Luo, “Deep features fusion with mutual attention\ntransformer for skin lesion diagnosis,” in International Conference on\nImage Processing (ICIP). IEEE, 2021, pp. 3797–3801.\n[7] K. Sirotkin, M. E. Vi ˜nolo, P. Carballeira, and J. C. SanMiguel, “Im-\nproved skin lesion recognition by a self-supervised curricular deep\nlearning approach,” arXiv preprint arXiv:2112.12086, 2021.\n[8] R. Karthik, T. S. Vaichole, S. K. Kulkarni, O. Yadav, and F. Khan,\n“Eff2Net: an efﬁcient channel attention-based convolutional neural net-\nwork for skin disease classiﬁcation,” Biomedical Signal Processing and\nControl, vol. 73, p. 103406, 2022.\n[9] A. G. Pacheco, G. R. Lima, A. S. Salom ˜ao, B. Krohling, I. P. Biral,\nG. G. de Angelo, F. C. Alves Jr, J. G. Esgario, A. C. Simora, P. B.\nCastro et al., “PAD-UFES-20: a skin lesion dataset composed of patient\ndata and clinical images collected from smartphones,” Data in Brief,\nvol. 32, p. 106221, 2020.\n[10] X. Feng, Y . Jiang, X. Yang, M. Du, and X. Li, “Computer vision al-\ngorithms and hardware implementations: a survey,” Integration, vol. 69,\npp. 309–320, 2019.\n[11] D. Bhatt, C. Patel, H. Talsania, J. Patel, R. Vaghela, S. Pandya, K. Modi,\nand H. Ghayvat, “CNN variants for computer vision: history, archi-\ntecture, application, challenges and future scope,” Electronics, vol. 10,\nno. 20, p. 2470, 2021.\n[12] K. He, X. Zhang, S. Ren, and J. Sun, “Identity mappings in deep residual\nnetworks,” in European Conference on Computer Vision. Springer,\n2016, pp. 630–645.\n[13] S. Xie, R. Girshick, P. Doll ´ar, Z. Tu, and K. He, “Aggregated residual\ntransformations for deep neural networks,” in Computer Vision and\nPattern Recognition. IEEE, 2017, pp. 1492–1500.\n[14] M. Tan and Q. Le, “EfﬁcientNetV2: smaller models and faster training,”\nin International Conference on Machine Learning, vol. 139. PMLR,\n2021, pp. 10 096–10 106.\n[15] Q. Wang, B. Wu, P. Zhu, P. Li, W. Zuo, and Q. Hu, “ECA-Net: efﬁcient\nchannel attention for deep convolutional neural networks,” in Computer\nVision and Pattern Recognition. IEEE, 2020.\n[16] I. Z. Yalniz, H. J ´egou, K. Chen, M. Paluri, and D. Mahajan, “Billion-\nscale semi-supervised learning for image classiﬁcation,” arXiv preprint\narXiv:1905.00546, 2019.\n[17] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural\nnetwork,” arXiv preprint arXiv:1503.02531, 2015.\n[18] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah,\n“Transformers in vision: a survey,” ACM Computing Surveys, 2021.\n[19] K. Han, Y . Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y . Tang, A. Xiao,\nC. Xu, Y . Xu et al., “A survey on visual transformer,” arXiv preprint\narXiv:2012.12556, 2020.\n[20] Y . Liu, Y . Zhang, Y . Wang, F. Hou, J. Yuan, J. Tian, Y . Zhang, Z. Shi,\nJ. Fan, and Z. He, “A survey of visual transformers,” arXiv preprint\narXiv:2111.06091, 2021.\n[21] Y . Xu, H. Wei, M. Lin, Y . Deng, K. Sheng, M. Zhang, F. Tang, W. Dong,\nF. Huang, and C. Xu, “Transformers in computational visual media: a\nsurvey,” Computational Visual Media, vol. 8, no. 1, pp. 33–62, 2022.\n[22] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\n“An image is worth 16x16 words: transformers for image recognition at\nscale,” in International Conference on Learning Representations, 2020.\n[23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances\nin Neural Information Processing Systems, 2017, pp. 5998–6008.\n[24] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y . Wang, “Transformer\nin transformer,” in Advances in Neural Information Processing Systems,\n2021.\n[25] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and\nB. Guo, “Swin transformer: hierarchical vision transformer using shifted\nwindows,” in International Conference on Computer Vision, 2021, pp.\n10 012–10 022.\n[26] Z. Liu, H. Hu, Y . Lin, Z. Yao, Z. Xie, Y . Wei, J. Ning, Y . Cao,\nZ. Zhang, L. Dong et al., “Swin transformer V2: Scaling up capacity\nand resolution,” arXiv preprint arXiv:2111.09883, 2021.\n[27] C.-F. R. Chen, Q. Fan, and R. Panda, “CrossViT: cross-attention\nmulti-scale vision transformer for image classiﬁcation,” in International\nConference on Computer Vision, 2021, pp. 357–366.\n[28] A. El-Nouby, H. Touvron, M. Caron, P. Bojanowski, M. Douze,\nA. Joulin, I. Laptev, N. Neverova, G. Synnaeve, J. Verbeek, and H. Jegou,\n“XCiT: cross-covariance image transformers,” in Advances in Neural\nInformation Processing Systems, 2021.\n[29] B. Heo, S. Yun, D. Han, S. Chun, J. Choe, and S. J. Oh, “Rethinking\nspatial dimensions of vision transformers,” in International Conference\non Computer Vision, 2021, pp. 11 936–11 945.\n[30] H. Touvron, M. Cord, A. Sablayrolles, G. Synnaeve, and H. J ´egou,\n“Going deeper with image transformers,” in International Conference\non Computer Vision, 2021, pp. 32–42.\n[31] H. Bao, L. Dong, and F. Wei, “BEiT: BERT pre-training of image\ntransformers,” arXiv preprint arXiv:2106.08254, 2021.\n[32] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and\nH. J ´egou, “Training data-efﬁcient image transformers & distillation\nthrough attention,” in International Conference on Machine Learning.\nPMLR, 2021, pp. 10 347–10 357.\n[33] J. Zhou, C. Wei, H. Wang, W. Shen, C. Xie, A. Yuille, and T. Kong,\n“iBOT: image BERT pre-training with online tokenizer,” arXiv preprint\narXiv:2111.07832, 2021.\n[34] M. Caron, H. Touvron, I. Misra, H. J ´egou, J. Mairal, P. Bojanowski, and\nA. Joulin, “Emerging properties in self-supervised vision transformers,”\nin International Conference on Computer Vision, 2021, pp. 9650–9660.\n[35] M. Naseer, K. Ranasinghe, S. Khan, M. Hayat, F. Khan, and M.-H. Yang,\n“Intriguing properties of vision transformers,” in Advances in Neural\nInformation Processing Systems, 2021.\n[36] R. Geirhos, P. Rubisch, C. Michaelis, M. Bethge, F. A. Wichmann, and\nW. Brendel, “ImageNet-trained CNNs are biased towards texture; in-\ncreasing shape bias improves accuracy and robustness.” in International\nConference on Learning Representations, 2019.\n[37] Y . Wang, W. Huang, F. Sun, T. Xu, Y . Rong, and J. Huang, “Deep mul-\ntimodal fusion by channel exchanging,” Advances in Neural Information\nProcessing Systems, vol. 33, pp. 4835–4845, 2020.\n[38] A. G. Pacheco and R. A. Krohling, “An attention-based mechanism\nto combine images and metadata in deep learning models applied to\nskin cancer classiﬁcation,” IEEE Journal of Biomedical and Health\nInformatics, vol. 25, no. 9, pp. 3554–3563, 2021.\n[39] W. Li, J. Zhuang, R. Wang, J. Zhang, and W.-S. Zheng, “Fusing metadata\nand dermoscopy images for skin disease diagnosis,” in International\nSymposium on Biomedical Imaging. IEEE, 2020, pp. 1996–2000.\n[40] R. Wightman, “PyTorch image models,” https://github.com/rwightman/\npytorch-image-models, 2019.\n[41] L. Beyer, O. J. H ´enaff, A. Kolesnikov, X. Zhai, and A. v. d. Oord, “Are\nwe done with ImageNet?” arXiv preprint arXiv:2006.07159, 2020.\n[42] Y . Cao, J. Xu, S. Lin, F. Wei, and H. Hu, “GCNet: non-local networks\nmeet squeeze-excitation networks and beyond,” in International Confer-\nence on Computer Vision Workshops, 2019.\n[43] D. Han, S. Yun, B. Heo, and Y . Yoo, “Rethinking channel dimensions\nfor efﬁcient model design,” in Computer Vision and Pattern Recognition,\n2021, pp. 732–741.\n[44] M. Tan and Q. Le, “EfﬁcientNet: rethinking model scaling for con-\nvolutional neural networks,” in International Conference on Machine\nLearning. PMLR, 2019, pp. 6105–6114.\n[45] Q. Xie, M.-T. Luong, E. Hovy, and Q. V . Le, “Self-training with noisy\nstudent improves ImageNet classiﬁcation,” in Conference on Computer\nVision and Pattern Recognition, 2020, pp. 10 687–10 698.\n[46] I. Radosavovic, R. P. Kosaraju, R. Girshick, K. He, and P. Doll ´ar,\n“Designing network design spaces,” in Computer Vision and Pattern\nRecognition, 2020, pp. 10 428–10 436.\n[47] W. Xu, Y . Xu, T. Chang, and Z. Tu, “Co-scale conv-attentional image\ntransformers,” in International Conference on Computer Vision, 2021,\npp. 9981–9990.\n[48] A. Steiner, A. Kolesnikov, X. Zhai, R. Wightman, J. Uszkoreit, and\nL. Beyer, “How to train your ViT? data, augmentation, and regularization\nin vision transformers,” arXiv preprint arXiv:2106.10270, 2021.\n[49] A. Srinivas, T.-Y . Lin, N. Parmar, J. Shlens, P. Abbeel, and A. Vaswani,\n“Bottleneck transformers for visual recognition,” in Computer Vision\nand Pattern Recognition, 2021, pp. 16 519–16 529.\n[50] A. Vaswani, P. Ramachandran, A. Srinivas, N. Parmar, B. Hechtman,\nand J. Shlens, “Scaling local self-attention for parameter efﬁcient visual\nbackbones,” in Computer Vision and Pattern Recognition, 2021, pp.\n12 894–12 904.\n[51] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Computer Vision and Pattern Recognition, 2016, pp.\n770–778.\n[52] X. Chu, Z. Tian, Y . Wang, B. Zhang, H. Ren, X. Wei, H. Xia, and\nC. Shen, “Twins: Revisiting the design of spatial attention in vision\ntransformers,” in Advances in Neural Information Processing Systems,\n2021.\n[53] A. Brock, S. De, S. L. Smith, and K. Simonyan, “High-performance\nlarge-scale image recognition without normalization,” arXiv preprint\narXiv:2102.06171, 2021.\n[54] L. Beyer, X. Zhai, A. Royer, L. Markeeva, R. Anil, and A. Kolesnikov,\n“Knowledge distillation: A good teacher is patient and consistent,” arXiv\npreprint arXiv:2106.05237, 2021.\n[55] A. Kolesnikov, L. Beyer, X. Zhai, J. Puigcerver, J. Yung, S. Gelly, and\nN. Houlsby, “Big transfer (BiT): General visual representation learning,”\nin European Conference on Computer Vision. Cham: Springer, 2020,\npp. 491–507.\n[56] P. Doll ´ar, M. Singh, and R. Girshick, “Fast and accurate model scaling,”\nin Conference on Computer Vision and Pattern Recognition, 2021, pp.\n924–932.\n[57] P. Zhao, C. Li, M. M. Rahaman, H. Yang, T. Jiang, and M. Grzegorzek,\n“A comparison of deep learning classiﬁcation methods on small-scale\nimage data set: from convolutional neural networks to visual transform-\ners,” arXiv preprint arXiv:2107.07699, 2021.",
  "topic": "Convolutional neural network",
  "concepts": [
    {
      "name": "Convolutional neural network",
      "score": 0.760729968547821
    },
    {
      "name": "Transformer",
      "score": 0.633745014667511
    },
    {
      "name": "Computer science",
      "score": 0.6219271421432495
    },
    {
      "name": "Skin lesion",
      "score": 0.610558032989502
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6020261645317078
    },
    {
      "name": "Lesion",
      "score": 0.5488723516464233
    },
    {
      "name": "Deep learning",
      "score": 0.5374979972839355
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4558819532394409
    },
    {
      "name": "Engineering",
      "score": 0.18134260177612305
    },
    {
      "name": "Medicine",
      "score": 0.165880024433136
    },
    {
      "name": "Dermatology",
      "score": 0.14856719970703125
    },
    {
      "name": "Pathology",
      "score": 0.09379824995994568
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I51235708",
      "name": "Universidade Federal do Espírito Santo",
      "country": "BR"
    }
  ]
}