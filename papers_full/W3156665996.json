{
  "title": "mT6: Multilingual Pretrained Text-to-Text Transformer with Translation Pairs",
  "url": "https://openalex.org/W3156665996",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2969024387",
      "name": "Zewen Chi",
      "affiliations": [
        "Microsoft Research Asia (China)",
        "Microsoft (United States)",
        "Beijing Microelectronics Technology Institute"
      ]
    },
    {
      "id": "https://openalex.org/A1974723233",
      "name": "Li Dong",
      "affiliations": [
        "Beijing Microelectronics Technology Institute",
        "Microsoft Research Asia (China)",
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2099570760",
      "name": "Shuming Ma",
      "affiliations": [
        "Beijing Microelectronics Technology Institute",
        "Microsoft Research Asia (China)",
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2612910427",
      "name": "Shaohan Huang",
      "affiliations": [
        "Microsoft (United States)",
        "Beijing Microelectronics Technology Institute",
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A3043020393",
      "name": "Saksham Singhal",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4208379621",
      "name": "Xian-Ling Mao",
      "affiliations": [
        "Beijing Microelectronics Technology Institute",
        "Microsoft Research Asia (China)",
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2101832271",
      "name": "Heyan, Huang",
      "affiliations": [
        "Microsoft (United States)",
        "Beijing Microelectronics Technology Institute",
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2102676534",
      "name": "Xia Song",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2171151462",
      "name": "Furu Wei",
      "affiliations": [
        "Microsoft Research Asia (China)",
        "Beijing Microelectronics Technology Institute",
        "Microsoft (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3035497479",
    "https://openalex.org/W2971207485",
    "https://openalex.org/W2952468927",
    "https://openalex.org/W2964114970",
    "https://openalex.org/W2742113707",
    "https://openalex.org/W3034715004",
    "https://openalex.org/W3001434439",
    "https://openalex.org/W630532510",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W3175898847",
    "https://openalex.org/W3175746962",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3105813095",
    "https://openalex.org/W4297808394",
    "https://openalex.org/W3177035927",
    "https://openalex.org/W2970752815",
    "https://openalex.org/W2572474373",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4300427991",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3106445907",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2995230342",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W3045462440",
    "https://openalex.org/W2995118574",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3171975879",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W3169425228",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W3093517588",
    "https://openalex.org/W2989539713",
    "https://openalex.org/W3013840636",
    "https://openalex.org/W2960374072",
    "https://openalex.org/W4288284086",
    "https://openalex.org/W2971863715",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2963877297",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3107826490",
    "https://openalex.org/W3007759824",
    "https://openalex.org/W2996822578",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W3051275803",
    "https://openalex.org/W3097879195",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W3119175506",
    "https://openalex.org/W3186903869",
    "https://openalex.org/W3168481568",
    "https://openalex.org/W2970854433",
    "https://openalex.org/W2973088264",
    "https://openalex.org/W3092973241",
    "https://openalex.org/W3042711927",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2995015695",
    "https://openalex.org/W2944815030",
    "https://openalex.org/W4301187301",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2998653236",
    "https://openalex.org/W3118106810",
    "https://openalex.org/W2842511635"
  ],
  "abstract": "Zewen Chi, Li Dong, Shuming Ma, Shaohan Huang, Saksham Singhal, Xian-Ling Mao, Heyan Huang, Xia Song, Furu Wei. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1671‚Äì1683\nNovember 7‚Äì11, 2021.c‚Éù2021 Association for Computational Linguistics\n1671\nmT6: Multilingual Pretrained Text-to-Text Transformer\nwith Translation Pairs\nZewen Chi‚Ä†‚Ä°‚àó, Li Dong ‚Ä°, Shuming Ma ‚Ä°, Shaohan Huang ‚Ä°\nXian-Ling Mao‚Ä†, Heyan Huang ‚Ä†, Furu Wei ‚Ä°\n‚Ä†Beijing Institute of Technology\n‚Ä°Microsoft Research\n{czw,maoxl,hhy63}@bit.edu.cn\n{lidong1,shumma,shaohanh,fuwei}@microsoft.com\nAbstract\nMultilingual T5 ( MT5; Xue et al. 2020) pre-\ntrains a sequence-to-sequence model on mas-\nsive monolingual texts, which has shown\npromising results on many cross-lingual tasks.\nIn this paper, we improve multilingual text-\nto-text transfer Transformer with translation\npairs ( MT6). SpeciÔ¨Åcally, we explore three\ncross-lingual text-to-text pre-training tasks,\nnamely, machine translation, translation pair\nspan corruption, and translation span corrup-\ntion. In addition, we propose a partially non-\nautoregressive objective for text-to-text pre-\ntraining. We evaluate the methods on eight\nmultilingual benchmark datasets, including\nsentence classiÔ¨Åcation, named entity recogni-\ntion, question answering, and abstractive sum-\nmarization. Experimental results show that the\nproposed MT6 improves cross-lingual transfer-\nability over MT5.\n1 Introduction\nMultilingual pretrained language models, such as\nmBERT (Devlin et al., 2019), have attracted in-\ncreasing attention. They not only improve the\nperformance on downstream multilingual NLP\ntasks (Conneau and Lample, 2019; Conneau et al.,\n2020; Liu et al., 2020; Chi et al., 2021c), but\nalso show an impressive cross-lingual transferabil-\nity (Wu and Dredze, 2019; K et al., 2020; Hu et al.,\n2020b; Chi et al., 2021a).\nMultilingual pretrained models are typically\ntrained on multilingual unlabeled text with unsu-\npervised language modeling tasks, e.g., masked\nlanguage modeling (Devlin et al., 2019), causal\nlanguage modeling (Conneau and Lample, 2019),\nand span corruption (Raffel et al., 2020). These\nunsupervised tasks are built upon large-scale mono-\nlingual texts. In addition, several studies pro-\npose cross-lingual tasks that utilize translation data\nfrom multilingual parallel corpora, such as trans-\nlation language modeling (Conneau and Lample,\n‚àóContribution during internship at Microsoft Research.\n2019), cross-lingual contrast (Chi et al., 2021a),\nand bidirectional word alignment (Hu et al., 2020a).\nThanks to the translation data, the pretrained mod-\nels produce better-aligned cross-lingual representa-\ntions and obtain better cross-lingual transferability.\nRecently, the multilingual text-to-text transfer\nTransformer (MT5; Xue et al. 2020) achieves state-\nof-the-art performance on several cross-lingual un-\nderstanding benchmarks. MT5 inherits the beneÔ¨Åts\nof T5 (Raffel et al., 2020) that treats every text pro-\ncessing problem as a text-to-text problem, i.e., the\nproblem of generating some target text conditioned\non the input text. Despite the effectiveness of MT5,\nhow to improve MT5 with translation data is still\nan open problem.\nIn this paper, we present MT6, standing for\nimproving multilingual text-to-text transfer Trans-\nformer with translation data. MT6 differs from\nMT5 in terms of both pre-training tasks and the\ntraining objective. We present three cross-lingual\ntasks for text-to-text Transformer pre-training, i.e.,\nmachine translation, translation pair span corrup-\ntion, and translation span corruption. In the trans-\nlation span corruption task, the model is trained to\npredict the text spans based on the input translation\npair. The cross-lingual tasks encourage the model\nto align representations of different languages.\nWe also propose a new objective for text-to-text\npre-training, called partially non-autoregressive\n(PNAT) decoding. The PNAT objective divides the\ntarget sequence into several groups, and constrains\nthat the predictions should be only conditioned on\nthe source tokens and the target tokens from the\nsame group.\nWe conduct experiments on both multilingual un-\nderstanding and generation tasks. Our MT6 model\nyields substantially better performance than MT5\non eight benchmarks. We also provide an empirical\ncomparison of the cross-lingual pre-training tasks,\nwhere we evaluate several variants of MT6 under\nthe same pre-training and Ô¨Åne-tuning procedure.\n1672\nMoreover, our analysis indicates that the represen-\ntations produced by MT6 are more cross-lingual\ntransferable and better-aligned than MT5.\nThe contributions are summarized as follows:\n‚Ä¢ We introduce three cross-lingual tasks for text-\nto-text Transformer pre-training, which im-\nproves MT5 with translation data.\n‚Ä¢ We propose a partially non-autoregressive ob-\njective that pretrains the decoder to use more\ninformation from the source sequence.\n‚Ä¢ We provide extensive evaluation results of var-\nious pre-training tasks and training objectives.\n2 Background on T5 and MT5\nMultilingual text-to-text transfer Transformer\n(MT5; Xue et al. 2020) is the multilingual vari-\nant of T5 (Raffel et al., 2020) pretrained on the\nmC4 (Xue et al., 2020) dataset, which consists of\nnatural text in 101 languages drawn from the public\nCommon Crawl web scrape.\nThe backbone architecture of MT5 is the sim-\nple encoder-decoder Transformer (Vaswani et al.,\n2017), which is trained in a uniÔ¨Åed text-to-text\nmanner. In speciÔ¨Åc, text-based NLP problems are\nformulated as text-to-text transfer, i.e., the model\nis trained to predict the target text conditioned on\nthe input source text. For example, in text clas-\nsiÔ¨Åcation, the model predicts the label text rather\nthan a class index. This feature enables the MT5\nto be Ô¨Åne-tuned with the same training objective\nfor every task. Formally, let x and y denote the\ninput sequence and the output sequence, the loss\nfunction of training the x‚Üíytransfer is\nL(x‚Üíy) =‚àí\n|y|‚àë\ni=1\nlog p(yi|x,y<i), (1)\nwhere y<i = y1,¬∑¬∑¬∑ ,yi‚àí1. With the uniÔ¨Åed text-\nto-text formulation, the pre-training task can be\ndesigned by constructing the input and output text\nsequences. SpeciÔ¨Åcally, MT5 employs the span\ncorruption task as the pre-training task, which is\nan unsupervised masked language modeling task.\nAs shown in Figure 1, we provide an example of\nconstructing the input and output sequences for\nspan corruption. Given a natural sentence s, it Ô¨Årst\nrandomly selects several spans of sas the spans to\nbe masked. Then, the input sequence is constructed\nby replacing the selected spans with unique mask\nThanks [M1] invitation [M2] .\n[M1] for your [M2] last week [M3]\nInputs\nTargets\nThanks for your invitation last week . \nOriginal text\nFigure 1: Example of the span corruption task (Raffel\net al., 2020) used in T5 and MT5.\ntokens. The output sequence is the concatenation\nof the original tokens of the masked spans, each of\nwhich starts with a unique mask token to indicate\nthe span to be decoded. We denote the above two\noperations as gi and go, standing for converting\nthe original sentence sinto the input or the output\nformats of span corruption. Thus, the loss function\nof the span corruption task can be written as\nLSC(s) =L(gi(s) ‚Üígo(s)). (2)\n3 Methods\nIn this section, we Ô¨Årst present three text-to-text\npre-training tasks for improving MT5 with trans-\nlation data. Then, we introduce the partially non-\nautoregressive decoding objective, and provide the\ndetailed Ô¨Åne-tuning procedures for the classiÔ¨Åca-\ntion, question answering, and named entity recog-\nnition tasks.\n3.1 Cross-lingual Pre-training Tasks with\nTranslation Pairs\nAs shown in Figure 2, we illustrate an overview\nof our cross-lingual text-to-text pre-training tasks.\nGiven the same translation pair, the three tasks\nconstruct different input and output sequences.\n3.1.1 Machine Translation\nMachine translation (MT) is a typical text-to-text\ntask with the goal of translating a sentence from the\nsource language into a target language. It is a natu-\nral design to use MT as a text-to-text pre-training\ntask for sequence-to-sequence learning (Chi et al.,\n2020). Let eand f denote a sentence and its cor-\nresponding translation. We directly use eand f as\nthe input and output sequences, respectively. The\nloss function of MT is\nLMT(e,f) =L(e‚Üíf). (3)\n1673\nThanks for your invitation last week . \nMerci pour votre invitation la \nsemaine derni√®re .\nThanks for your invitation last week . \nMerci pour votre invitation la \nsemaine derni√®re .\nThanks for your invitation last week .\nMerci pour votre invitation la \nsemaine derni√®re .\nThanks [M1] invitation [M2] . Merci \npour votre [M3] la semaine derni√®re .\n[M1] for your [M2] last week [M3] \ninvitation [M4]\nInputs\nTargets\nThanks for your invitation last week . \nOriginal translation pair\nMachine Translation\nInputs\nTargets\nTranslation Pair Span Corruption\nThanks [M1] invitation [M2] . Merci pour \nvotre invitation la semaine derni√®re .\n[M1] for your [M2] last week [M3]\nInputs\nTargets\nTranslation Span Corruption\nOriginal translation pair Original translation pair\n(masking only one language)\nMerci pour votre invitation la \nsemaine derni√®re .\nEnglish\nFrench\nEnglish\nFrench\nEnglish\nFrench\nFigure 2: Overview of three cross-lingual text-to-text pre-training tasks. For each task, we provide an example of\nthe input and target text. The words marked with ‚Äú √ó‚Äù are randomly replaced with unique mask tokens like [M1].\nNotice that in the translation span corruption task, we mask tokens only in one language.\n3.1.2 Translation Pair Span Corruption\nInspired by the translation masked language model-\ning (Conneau and Lample, 2019) task, we propose\nthe translation pair span corruption (TPSC) task\nthat aims to predict the masked spans from a trans-\nlation pair instead of a monolingual sentence. Let e\nand fdenote a sentence and its corresponding trans-\nlation. We concatenate eand f as a single sentence,\nand perform the span corruption on the concate-\nnated sentence. Formally, we construct the input\nand output sequences by gi([e; f]) and go([e; f]),\nwhere [e; f] stands for the concatenation of eand\nf. With the resulting input and output sequences,\nthe loss function of TPSC can be written as\nLTPSC(e,f) =L(gi([e; f]) ‚Üígo([e; f])). (4)\n3.1.3 Translation Span Corruption\nA potential issue of translation pair span corruption\nis that the spans in the target sequence can be or-\nganized in unnatural word order. As shown in Fig-\nure 2, the output sequence of TPSC is organized as\n‚Äú[M1] for your [M2] last week [M3] invitation [M4]‚Äù.\nIt can be found that the French word ‚Äúinvitation‚Äù is\nafter the English word ‚Äúweek‚Äù, which could harm\nthe language model of the decoder. This motivates\nus to propose the translation span corruption (TSC)\ntask where we only mask and predict the spans in\none language. Given a translation pair ( e, f), we\nrandomly select the eor f to perform span corrup-\ntion. Without loss of generality, we consider eas\nthe sentence for span corruption. Then, the input\nand output sequences are constructed by [gi(e); f]\nand go(e), respectively. With the resulting input\nand output sequences, the loss function of TSC can\nbe written as\nLTSC(e,f) =L([gi(e); f]) ‚Üígo(e))). (5)\n3.2 Pre-training Objective: Partially\nNon-autoregressive Decoding\nRecall that the predictions in MT5 are conditioned\non both the source tokens and the target tokens\nto the left. When predicting the tokens closer to\nthe end, the model can use more information from\nthe target sequence, resulting in the insufÔ¨Åcient\ntraining of the encoder.\nTo encourage the model to utilize more infor-\nmation from the encoding side while preserving\nthe ability of autoregressive decoding, we pro-\npose a new training objective for text-to-text train-\ning, called partially non-autoregressive decoding\n(PNAT). In Figure 3, we provide an example for\nPNAT. SpeciÔ¨Åcally, given a target sequence con-\ntaining several spans, we divide the target sequence\ninto groups, and train the model to decode each\ngroup separately. With the PNAT objective, a pre-\ndiction is only conditioned on the source tokens\nand the target tokens from the same group. Con-\nsider the target sequence consisting ofmspans. We\ndivide the spans into ng groups, each of which con-\ntains m/ng consecutive spans. For the j-th group,\nwe denote lj and rj as the start position and the\nend position, respectively. The PNAT objective is\ndeÔ¨Åned as\nLPNAT(x‚Üíy) =‚àí\nng‚àë\nj=1\nrj‚àë\ni=lj\nlog p(yi|x,ylj ...y i‚àí1).\nThe text-to-text loss L(x‚Üíy) is a specially case\nof LPNAT(x‚Üíy) with ng = 1.\nThe MT6 model is jointly pretrained on both\nmonolingual and parallel corpora, where we use the\nspan corruption and one of the three cross-lingual\ntext-to-text tasks. For both tasks, we use the par-\ntially non-autoregressive decoding as the training\nobjective where we divide the target sequence into\n1674\nThanks [M1] for [M2] me [M3] your party [M4] . \n[M1] you [M2] inviting [M3]\nInputs\nTargets of ùíàùíìùíêùíñùíëùüè\nThank you for inviting me to your party last week .\nOriginal text\n[M3] to [M4] last week [M5]\nTargets of ùíàùíìùíêùíñùíëùüê\nencoder\ndecoder\ndecoder\nùë†ùëùùëéùëõ1\nshared\nInputs\nùë†ùëùùëéùëõ2\nùë†ùëùùëéùëõ3 ùë†ùëùùëéùëõ4\nFigure 3: Partially non-autoregressive objective.\nng groups. The overall pre-training objective is to\nminimize\nLMT6 = LPNAT\nSC (s) +LPNAT\nX (e,f), (6)\nX ‚àà{MT,TPSC,TSC},\nwhere LPNAT\nX stands for the one of the loss functions\nof machine translation (MT; Section 3.1.1), transla-\ntion pair span corruption (TPSC; Section 3.1.2) and\ntranslation span corruption (TSC; Section 3.1.3),\nwith PNAT as the training objective.\n3.3 Cross-lingual Fine-tuning\nWe Ô¨Åne-tune all parameters of the MT6 model with\nEquation (1) regardless of the end task. Unlike\nlanguage generation tasks, language understanding\ntasks should be pre-processed as the text-to-text\nformat. We introduce how to convert the following\nthree types of the language understanding task into\nthe text-to-text format, i.e., constructing the input\nand output sequences from the original examples.\nClassiÔ¨Åcation The goal of the text classiÔ¨Åcation\ntask is to predict the label of a given text. Follow-\ning T5 (Raffel et al., 2020), we directly use the\nlabel text as the output text sequence. We provide\nan example for the MNLI natural language infer-\nence task (Williams et al., 2018). Given an input\nsentence pair of ‚Äú You have access to the facts . ‚Äù\nand ‚ÄúThe facts are accessible to you .‚Äù, the goal is\nto classify the input into the relationships of ‚Äú en-\ntailment‚Äù, ‚Äúcontradiction‚Äù, or ‚Äúneutral‚Äù. The input\nand target sequences are constructed as\nInput: ‚ü®bos‚ü©You have access to the facts. ‚ü®eos‚ü©\nThe facts are accessible to you. ‚ü®eos‚ü©\nOutput: ‚ü®bos‚ü©entailment ‚ü®eos‚ü©\nSince multi-task Ô¨Åne-tuning is not the focus of\nthis work, we do not prepend a task preÔ¨Åx in the\ninput text. We also adopt a constrained decoding\nprocess, where the decoded text is constrained to\nbe one of the labels.\nQuestion Answering For the extractive question\nanswering (QA) task, we concatenate the passage\nand the question as the input, and directly use the\nanswer text as the target instead of predicting the\nanswer span positions. We provide an example of\nconverting a QA training example into the text-to-\ntext format.\nInput: ‚ü®bos‚ü©It has ofÔ¨Åces in Seoul, South Korea.\n‚ü®eos‚ü©Where is the ofÔ¨Åce in South Korea? ‚ü®eos‚ü©\nOutput: ‚ü®bos‚ü©Seoul ‚ü®eos‚ü©\nWe use the constrained decoding for the QA\ntasks where we use the tokens shown in the input\npassage as the decoding vocabulary.\nNamed Entity Recognition In named entity\nrecognition (NER), we do not directly use the orig-\ninal tag sentence as the output. We Ô¨Ånd that the\nmodel tends to repeat decoding the ‚ÄúO‚Äù tag if the\nmodel directly learns to decode the tag sequences.\nAlternately, we construct the target text by con-\ncatenating the entity spans, each of which starts\nwith the entity tag and ends with the entity tokens.\nWe show an example of converting a NER training\nexample into the text-to-text format.\nInput: ‚ü®bos‚ü©Italy recalled Marcello Cuttitta .\n‚ü®eos‚ü©\nOutput: ‚ü®bos‚ü©‚ü®loc‚ü©Italy ‚ü®sep‚ü©‚ü®per‚ü©Marcello\nCuttitta ‚ü®sep‚ü©‚ü®eos‚ü©\n‚ü®loc‚ü©and ‚ü®per‚ü©are entity tags denoting location\nand person. The ‚ü®sep‚ü©tag means the end of entity\nspan. We use the following constrained decoding\nrules: (1) The model should decode entity tags or\nthe end-of-sentence tag (‚ü®eos‚ü©) after a ‚ü®bos‚ü©token\nor a ‚ü®sep‚ü©token; (2) Otherwise, the model should\ndecode the tokens from the input sentence or the\n‚ü®sep‚ü©token for the other situations.\n4 Experiments\n4.1 Setup\nData Following previous work on cross-lingual\npre-training (Conneau et al., 2020; Chi et al.,\n2021a), we use the natural sentences from CC-\nNet (Wenzek et al., 2019) in 94 languages\nfor monolingual text-to-text tasks. For cross-\nlingual text-to-text tasks, we use parallel corpora\nof 14 English-centric language pairs, collected\nfrom MultiUN (Ziemski et al., 2016), IIT Bom-\nbay (Kunchukuttan et al., 2018), OPUS (Tiede-\nmann, 2012), and WikiMatrix (Schwenk et al.,\n1675\n2019). Details of the pre-training data are described\nin Appendix.\nTraining Details In the experiments, we con-\nsider the small-size Transformer model (Xue et al.,\n2020), with dmodel = 512,dff = 1,024, 6 attention\nheads, and 8 layers for both the encoder and the de-\ncoder1. We use the vocabulary provided by XLM-\nR (Conneau et al., 2020), and extend it with 100\nunique mask tokens for the span corruption tasks.\nWe pretrain our MT6 for 0.5M steps with batches\nof 256 length-512 input sequences. The model is\noptimized by the Adam optimizer (Kingma and Ba,\n2015) with a linear learning rate scheduler. The\npre-training procedure takes about 2.5 days on an\nNvidia DGX-2 Station. Details of the pre-training\nhyperparameters are described in Appendix.\n4.2 Results\n4.2.1 XTREME Cross-lingual Understanding\nTo validate the performance of MT6, we eval-\nuate the pretrained models on XTREME (Hu\net al., 2020b), which is a widely used bench-\nmark for cross-lingual understanding. Following\nMT5 (Xue et al., 2020), we consider six down-\nstream tasks included by XTREME: the named\nentity recognition (NER) task on the WikiAnn (Pan\net al., 2017; Rahimi et al., 2019) dataset in 40\nlanguages, the question answering (QA) task on\nMLQA (Lewis et al., 2020b), XQuAD (Artetxe\net al., 2020), and TyDiQA-GoldP (Clark et al.,\n2020), the cross-lingual natural language inference\ntask on XNLI (Conneau et al., 2018), and cross-\nlingual paraphrase adversaries on PAWS-X (Yang\net al., 2019). The models are evaluated under the\ncross-lingual transfer setting (Conneau et al., 2020;\nHu et al., 2020b). Under this setting, the models\nshould be Ô¨Åne-tuned only on English training data\nbut evaluated on all target languages. Moreover,\nfor each pretrained model, only one model is used\nfor all languages rather than selecting Ô¨Åne-tuned\nmodels separately. Details of the Ô¨Åne-tuning hyper-\nparameters are described in Appendix.\nAs shown in Table 1, we present the evaluation\nresults of the pretrained models on the XTREME\nbenchmark. We observe that MT6 achieves the best\nperformance on XTREME, improving the average\nscore from 45.0 to 50.4, as we go from MT5 to\nMT6. It is worth mentioning that pre-training the\n1Notice that the ‚Äúsmall-size‚Äù deÔ¨Åned in T5 and MT5 are\ndifferent. Here we follow the setting of MT5-small.\nmodel only with the machine translation task per-\nforms even worse than MT5. We have noticed that\nseveral target languages in TyDiQA and WikiAnn\nare not covered by our parallel corpora. However,\nthe NMT pretrained model still shows poor results\non the other four tasks, where all target languages\nare covered by the training data. Detailed results\ncan be found in Appendix.\n4.2.2 Comparison of Pre-training Tasks\nTo provide a clear comparison among the pre-\ntraining tasks, we implement the text-to-text pre-\ntraining methods presented in Section 3, and pre-\ntrain variants of MT6 with the same training data\nand resources for fair comparisons.\nTable 1 compares the evaluation results of the\nmodels pretrained with seven different combina-\ntions of span corruption (SC), machine transla-\ntion (MT), translation pair span corruption (TPSC),\ntranslation span corruption (TSC), and partially\nnon-autoregressive decoding (PNAT). It can be ob-\nserved that jointly training SC+TSC with PNAT\nachieves the best overall performance on the\nXTREME benchmark, with substantial gains over\nthe models trained on monolingual data only. The\nsame trend can be observed for the other models\npretrained on both monolingual data and parallel\ndata. This demonstrates that introducing transla-\ntion data to text-to-text pre-training can improve the\nperformance on the end tasks of cross-lingual un-\nderstanding. Moreover, PNAT provides consistent\ngains over SC and SC+TSC, showing that PNAT\nis effective on both monolingual and cross-lingual\ntasks. Surprisingly, SC+PNAT obtains comparable\nresults to SC+MT without any parallel data. Com-\nparing TSC with MT and TPSC, we observe that\nSC+TSC brings noticeable improvements on ques-\ntion answering tasks. Although SC+MT shows\ncompetitive results on XNLI, the results on the\nother tasks are relatively low, indicating that sim-\nply jointly training SC with MT is not the most\neffective way to pretrain MT6.\n4.3 Abstractive Summarization\nMultilingual Summarization In addition to lan-\nguage understanding tasks, we also evaluate our\nMT6 model on the abstractive summarization task.\nAbstractive summarization aims to generate a sum-\nmary of the input document while preserving its\noriginal meaning. We use the Gigaword dataset\nprovided by Chi et al. (2020). The dataset is con-\nstructed by extracting the Ô¨Årst sentences and head-\n1676\nModel ConÔ¨Åguration Structured (F1) Question Answering (F1) ClassiÔ¨Åcation (Acc.)\nSC PNAT MT TPSC TSC WikiAnn XQuAD MLQA TyDiQA XNLI PAWS-X\nNMT \u0017 \u0017 \u0013 \u0017 \u0017 27.3 12.5 14.9 16.8 64.8 55.0\nMT5 \u0013 \u0017 \u0017 \u0017 \u0017 43.1 42.1 37.6 30.7 57.2 78.0\nMT6 (ours) \u0013 \u0013 \u0017 \u0017 \u0013 44.7 50.4 44.1 36.0 64.7 82.2\nAblations\n\u0013 \u0013 \u0017 \u0017 \u0017 43.7 45.1 38.5 32.3 57.9 77.5\n\u0013 \u0017 \u0013 \u0017 \u0017 43.9 38.5 33.3 29.4 65.9 79.3\n\u0013 \u0017 \u0017 \u0013 \u0017 42.3 46.2 40.8 35.3 64.0 78.9\n\u0013 \u0017 \u0017 \u0017 \u0013 43.8 47.6 40.5 36.7 65.4 80.3\nPre-training with larger batch size and more training steps\nMT5 (Xue et al., 2020) 50.5 58.1 54.6 35.2 67.5 82.4\nTable 1: Evaluation results on XTREME under the cross-lingual transfer setting, where models are only Ô¨Åne-tuned\non the English training data but evaluated on all target languages. We pretrain models with different combina-\ntions of span corruption (SC), machine translation (MT), translation pair span corruption (TPSC), translation span\ncorruption (TSC), and partially non-autoregressive decoding (PNAT). All results are averaged over Ô¨Åve runs.\nModel #Param en fr zh\nRG-1 RG-2 RG-L RG-1 RG-2 RG-L RG-1 RG-2 RG-L\nLarger model size\nXLM (Chi et al., 2020) 800M 48.15 26.35 45.04 56.27 39.20 52.84 55.30 42.57 52.95\nXNLG (Chi et al., 2020) 800M 48.76 26.82 45.57 57.84 40.81 54.24 57.65 44.93 54.95\nOur re-implementation (Fine-tuning with full training data)\nMT5 (reimpl) 300M 46.58 24.45 43.32 54.12 36.78 50.61 57.30 44.08 54.65\nMT6 300M 46.82 24.65 43.50 54.82 37.61 51.30 57.38 44.20 54.66\nOur re-implementation (Fine-tuning with 1K training data)\nMT5 300M 28.00 10.89 26.13 32.56 17.25 29.75 44.16 31.20 41.86\nMT6 300M 28.80 11.44 26.45 35.07 18.70 31.39 46.48 33.17 44.02\nTable 2: Evaluation results on Gigaword multilingual abstractive summarization. RG is short for ROUGE. Results\nof XLM and XNLG are taken from (Chi et al., 2020). Results of MT5 and MT6 are averaged over three runs.\nlines as the input documents and summaries, re-\nspectively. The dataset consists of examples in the\nlanguages of English, French, and Chinese. For\neach language, it contains 500K, 5K, and 5K ex-\namples for the training, validation, and test, respec-\ntively. We Ô¨Åne-tune the models for 20 epochs with\na batch size of 32 and a learning rate of 0.00001.\nDuring decoding, we use the greedy decoding for\nall evaluated models.\nAs shown in Table 2, we report the ROUGE (Lin,\n2004) scores of the models on Gigaword multilin-\ngual abstractive summarization. We observe that\nMT6 consistently outperforms MT5 on all the three\ntarget languages. Comparing with the XLM (Con-\nneau and Lample, 2019) and XNLG (Chi et al.,\n2020) models with 800M parameters, our MT6\nmodel achieves a similar performance with only\n300M parameters. Besides, under the setting with\nfewer training data, MT6 shows more improve-\nments over MT5.\nCross-Lingual Summarization The cross-\nlingual summarization task aims to generate\nsummaries in a different language. We use the\nModel es-en ru-en vi-en tr-en\nMT5 11.36 8.77 8.98 10.57\nMT6 11.83 9.49 9.52 10.80\nTable 3: ROUGE-2 scores on Wikilingua cross-lingual\nsummarization. Results are averaged over three runs.\nModel XQuAD MLQA TyDiQA XNLI PA WS-X\nMT5 30.4 27.5 27.5 19.5 16.0\nMT6 28.6 27.2 25.9 14.6 13.2\nTable 4: The cross-lingual transfer gap scores on the\nXTREME tasks. A lower transfer gap score indicates\nbetter cross-lingual transferability. We use the EM\nscores to compute the gap scores for the QA tasks.\nWikilingua (Ladhak et al., 2020) dataset containing\npassage-summary pairs in four language pairs. We\nÔ¨Åne-tune the models for 100K steps with a batch\nsize of 32 and a learning rate of 0.0001. We use\nthe greedy decoding for all evaluated models. The\nevaluation results are shown in Table 3, where\nMT6 outperforms MT5 on the test sets of four\nlanguage pairs.\n1677\n2 4 6 8\nLayer\n10\n20\n30\n40Averaged Accuracy\nmT5\nmT6\nFigure 4: Evaluation results of different layers on\nTatoeba cross-lingual sentence retrieval. We illustrate\nthe average accuracy@1 scores on the Tatoeba test sets\nof the 14 language pairs covered by the parallel data.\n4.4 Cross-lingual Transfer Gap\nTo explore whether our MT6 model achieves better\ncross-lingual transferability, we compare the cross-\nlingual transfer gap scores of our MT6 with MT5.\nCross-lingual transfer gap (Hu et al., 2020b) is de-\nÔ¨Åned as the difference between the performance on\nthe English test set and the average performance\non the non-English test sets. The transfer gap indi-\ncates how much the end-task knowledge preserves\nwhen transferring from English to the other tar-\nget languages. Empirically, a lower transfer gap\nscore indicates better cross-lingual transferability.\nFollowing Hu et al. (2020b), we compute the trans-\nfer gap scores over the sentence classiÔ¨Åcation and\nquestion answering tasks. As shown in Table 4,\nMT6 consistently reduces the transfer gap across\nall the Ô¨Åve tasks, demonstrating that our model is\nmore effective for cross-lingual transfer than MT5.\n4.5 Cross-lingual Representations\nWe analyze the cross-lingual representations pro-\nduced by our MT6 model. Following Chi et al.\n(2021a), we evaluate the representations on the\nTatoeba (Artetxe and Schwenk, 2019) cross-lingual\nsentence retrieval task. The test sets consist of 14\nEnglish-centric language pairs covered by the par-\nallel data in our experiments. Figure 4 illustrates\nthe average accuracy@1 scores of cross-lingual\nsentence retrieval. The scores are averaged over\n14 language pairs and both the directions of xx\n‚Üíen and en ‚Üíxx. From the Ô¨Ågure, we observe\nthat MT5 shows a parabolic trend across different\nlayers, which also appears in other cross-lingual\nencoder models (Jalili Sabet et al., 2020; Chi et al.,\n2021a). Differently, we obtain better performance\nModel en-de en-fr en-ro Avg\nMT5 35.84 19.05 45.24 33.38\nMT6 23.69 12.11 42.56 26.12\nTable 5: Evaluation results on word alignment. We re-\nport the alignment error rate scores (lower is better).\nWe use the hidden vectors from the last encoder layer,\nand apply the SimAlign (Jalili Sabet et al., 2020) tool\nto obtain the resulting word alignments.\nNoise Density NER QA ClassiÔ¨Åcation Avg\n15% 41.7 33.5 71.9 47.4\n30% 41.3 35.9 72.2 48.9\n50% 43.8 35.5 72.9 49.4\n100% (MT) 43.9 29.1 72.6 46.1\nTable 6: Effects of noise density. We report the average\nresults over different task types and the average results\nover all the six tasks on the XTREME benchmark. We\nvary the noise density of the translation span corruption\ntask from 15% to 100%. All results are averaged over\nÔ¨Åve runs.\nas we use higher layers of our MT6 model. At\nlayer-8, our MT6 model achieves an average ac-\ncuracy@1 of 43.2, outperforming the MT5 model\nby 35.6, which means our MT6 model produces\nbetter-aligned text representations. We believe the\nbetter-aligned representations potentially improve\nthe cross-lingual transferability. Furthermore, the\nresults also indicate that our pre-training objective\nis more effective for training the encoder thanMT5.\n4.6 Word Alignment\nIn addition to cross-lingual sentence retrieval that\nevaluates sentence-level representations, we also\nexplore whether the representations produced by\nMT6 are better-aligned at token-level. Thus, we\ncompare our MT6 with MT5 on the word align-\nment task, where the goal is to Ô¨Ånd corresponding\nword pairs in a translation pair. We use the hidden\nvectors from the last encoder layer, and apply the\nSimAlign (Jalili Sabet et al., 2020) tool to obtain\nthe resulting word alignments. Table 5 shows the\nalignment error rate (AER) scores on the test sets\nprovided by Jalili Sabet et al. (2020). Among the\nthree language pairs, MT6 achieves lower AER\nscores than MT5, indicating that the cross-lingual\nrepresentations produced by MT6 are also better-\naligned at token-level.\n1678\n4.7 Effects of Noise Density\nIn the translation span corruption (TSC) task, the\ninput parallel sentences provide redundant informa-\ntion in two languages, which is different from the\nstandard monolingual span corruption task. Thus,\nwe explore the effects of noise density by varying\nthe noise density in the translation span corrup-\ntion task, with the other hyperparameters Ô¨Åxed. To\nreduce the computational load, we do not apply\nthe partially non-autoregressive decoding, i.e., we\npretrain the models with the original text-to-text\nobjective. We pretrain MT6 models with the noise\ndensity of 0.15, 0.3, 0.5, and 1.0 respectively. It\nmeans 15%, 30%, 50%, or all of the source or tar-\nget tokens are replaced with the masked tokens.\nNotice that setting the noise density as 1.0 is iden-\ntical to machine translation, where the decoder is\nrequired to decode the whole target sentence.\nIn Table 6, we report the average scores on the\nXTREME benchmark. From the results, we ob-\nserve that MT6 achieves the best results with the\nnoise density of 0.5, rather than a higher noise\ndensity such as 1.0. The results indicate that the\nTSC task prefers a higher noise density, so that the\nmodel can learn to use more cross-lingual informa-\ntion. This Ô¨Ånding is different from that reported by\nT5 (Raffel et al., 2020), where the span corruption\ntask works better with the noise density of 0.15\nunder the monolingual setting.\n5 Related Work\nCross-lingual LM Pre-training Cross-lingual\nlanguage models are typically built with the Trans-\nformer (Vaswani et al., 2017) architecture, and pre-\ntrained with various pre-training tasks on large-\nscale text data. Multilingual BERT (mBERT; De-\nvlin et al. 2019) and XLM-R (Conneau et al., 2020)\nare pretrained with masked language modeling\n(MLM; Devlin et al. 2019) on large-scale unla-\nbeled text in about 100 languages. MASS (Song\net al., 2019) and mBART (Liu et al., 2020) are\npretrained in an auto-encoding manner, which pro-\nvides improvements on the neural machine trans-\nlation tasks. MT5 (Xue et al., 2020) is pretrained\nwith the span corruption (Raffel et al., 2020) task\nunder the text-to-text formulation (Raffel et al.,\n2020). Cross-lingual pretrained models also beneÔ¨Åt\nfrom translation data. XLM (Conneau and Lam-\nple, 2019) jointly learns MLM and the translation\nlanguage modeling (TLM) task. Unicoder (Huang\net al., 2019) presents three cross-lingual tasks to\nlearn mappings among languages. ALM (Yang\net al., 2020) converts the translation pairs into\ncode-switched sequences as the training examples.\nWord-aligned BERT models (Cao et al., 2020; Zhao\net al., 2020) improves the cross-lingual represen-\ntations by Ô¨Åne-tuning the mBERT with the objec-\ntive of minimizing the distance between aligned\ntokens. AMBER (Hu et al., 2020a) propose to\nmaximize the agreement between the forward and\nbackward attention matrices of the input transla-\ntion pair. InfoXLM (Chi et al., 2021a) proposes the\ncross-lingual contrastive learning task that maxi-\nmizes the InfoNCE (Oord et al., 2018) lower bound\nof the mutual information between the input transla-\ntion pair. XLM-Align (Chi et al., 2021b) leverages\ntoken-level alignments implied in translation pairs\nto improve cross-lingual transfer. XNLG (Chi et al.,\n2020) introduces the cross-lingual transfer for NLG\ntasks, and achieves zero-shot cross-lingual transfer\nfor question generation and abstractive summariza-\ntion. VECO (Luo et al., 2020) pretrains a variable\ncross-lingual pre-training model that learns uni-\nÔ¨Åed language representations for both NLU and\nNLG. ERNIE-M (Ouyang et al., 2020) utilizes the\nback-translation masked language modeling task\nthat generates pseudo parallel sentence pairs for\nlearning TLM.\nEncoder-Decoder Pre-training Raffel et al.\n(2020) use span corruption to pretrain text-to-text\nTransformer, where both language understanding\nand generation tasks are formulated as sequence-\nto-sequence Ô¨Åne-tuning. Song et al. (2019) pro-\npose masked sequence-to-sequence pre-training\nwhere the model predicts a randomly masked span.\nBART (Lewis et al., 2020a) design various de-\nnoised autoencoding tasks to recover the whole\noriginal sentence. PEGASUS (Zhang et al., 2020)\nintroduces the gap sentence generation task for ab-\nstractive summarization pre-training. Chi et al.\n(2020) use both denoised autoencoding and ma-\nchine translation for cross-lingual language gener-\nation. Another strand of research follows uniÔ¨Åed\nlanguage model pre-training (Dong et al., 2019;\nBao et al., 2020; Luo et al., 2020), where the en-\ncoder and the decoder share parameters. Ma et al.\n(2020, 2021) reuse pretrained multilingual encoder\nfor sequence-to-sequence pre-training.\n6 Conclusion\nIn this paper, we propose MT6 that improves the\nmultilingual text-to-text transfer Transformer with\n1679\ntranslation data. We introduce three text-to-text\npre-training tasks that are built on parallel corpora,\nand a training objective for improving text-to-text\npre-training. Nonetheless, we present a compre-\nhensive comparison of the text-to-text tasks, and\nshow that our MT6 model outperforms MT5 on\nboth cross-lingual understanding and generation\nbenchmarks. For future work, we would like to\npretrain MT6 models at a larger scale, and explore\nmore applications, such as machine translation.\nAcknowledgements\nWe would like to acknowledge Bo Zheng for the\nhelpful discussions. The work is supported by Na-\ntional Key R&D Plan (No. 2018YFB1005100),\nNational Natural Science Foundation of China\n(No. 61751201, 61602197, 61772076, and\n61732005), Natural Science Fund of Beijing (No.\nZ181100008918002), and the funds of Beijing Ad-\nvanced Innovation Center for Language Resources\n(No. TYZ19005). Heyan Huang is the correspond-\ning author.\nReferences\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 4623‚Äì4637, Online. Asso-\nciation for Computational Linguistics.\nMikel Artetxe and Holger Schwenk. 2019. Mas-\nsively multilingual sentence embeddings for zero-\nshot cross-lingual transfer and beyond. Transac-\ntions of the Association for Computational Linguis-\ntics, 7(0):597‚Äì610.\nHangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan\nYang, Xiaodong Liu, Yu Wang, Songhao Piao, Jian-\nfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2020.\nUniLMv2: Pseudo-masked language models for uni-\nÔ¨Åed language model pre-training. arXiv preprint\narXiv:2002.12804.\nSteven Cao, Nikita Kitaev, and Dan Klein. 2020. Mul-\ntilingual alignment of contextual word representa-\ntions. In International Conference on Learning Rep-\nresentations.\nZewen Chi, Li Dong, Furu Wei, Wenhui Wang, Xian-\nLing Mao, and Heyan Huang. 2020. Cross-lingual\nnatural language generation via pre-training. In The\nThirty-Fourth AAAI Conference on ArtiÔ¨Åcial Intelli-\ngence, AAAI 2020, New York, NY, USA, February\n7-12, 2020, pages 7570‚Äì7577. AAAI Press.\nZewen Chi, Li Dong, Furu Wei, Nan Yang, Sak-\nsham Singhal, Wenhui Wang, Xia Song, Xian-Ling\nMao, Heyan Huang, and Ming Zhou. 2021a. In-\nfoXLM: An information-theoretic framework for\ncross-lingual language model pre-training. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n3576‚Äì3588, Online. Association for Computational\nLinguistics.\nZewen Chi, Li Dong, Bo Zheng, Shaohan Huang, Xian-\nLing Mao, Heyan Huang, and Furu Wei. 2021b.\nImproving pretrained cross-lingual language mod-\nels via self-labeled word alignment. In Proceed-\nings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 3418‚Äì3430,\nOnline. Association for Computational Linguistics.\nZewen Chi, Shaohan Huang, Li Dong, Shuming Ma,\nSaksham Singhal, Payal Bajaj, Xia Song, and\nFuru Wei. 2021c. XLM-E: Cross-lingual lan-\nguage model pre-training via ELECTRA. ArXiv,\nabs/2106.16138.\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan\nGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and\nJennimaria Palomaki. 2020. TyDi QA: A bench-\nmark for information-seeking question answering in\ntypologically diverse languages. Transactions of the\nAssociation for Computational Linguistics , 8:454‚Äì\n470.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm¬¥an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8440‚Äì\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances\nin Neural Information Processing Systems , pages\n7057‚Äì7067. Curran Associates, Inc.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. XNLI: Evaluating\ncross-lingual sentence representations. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 2475‚Äì2485,\nBrussels, Belgium. Association for Computational\nLinguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171‚Äì4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\n1680\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. UniÔ¨Åed language\nmodel pre-training for natural language understand-\ning and generation. In Advances in Neural Informa-\ntion Processing Systems , pages 13063‚Äì13075. Cur-\nran Associates, Inc.\nJunjie Hu, Melvin Johnson, Orhan Firat, Aditya Sid-\ndhant, and Graham Neubig. 2020a. Explicit align-\nment objectives for multilingual bidirectional en-\ncoders. arXiv preprint arXiv:2010.07972.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020b. XTREME: A massively multilingual multi-\ntask benchmark for evaluating cross-lingual general-\nization. arXiv preprint arXiv:2003.11080.\nHaoyang Huang, Yaobo Liang, Nan Duan, Ming Gong,\nLinjun Shou, Daxin Jiang, and Ming Zhou. 2019.\nUnicoder: A universal language encoder by pre-\ntraining with multiple cross-lingual tasks. In Pro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language\nProcessing, pages 2485‚Äì2494, Hong Kong, China.\nAssociation for Computational Linguistics.\nMasoud Jalili Sabet, Philipp Dufter, Franc ¬∏ois Yvon,\nand Hinrich Sch ¬®utze. 2020. SimAlign: High qual-\nity word alignments without parallel training data us-\ning static and contextualized embeddings. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2020 , pages 1627‚Äì1643, Online. As-\nsociation for Computational Linguistics.\nKarthikeyan K, Zihan Wang, Stephen Mayhew, and\nDan Roth. 2020. Cross-lingual ability of multilin-\ngual bert: An empirical study. In International Con-\nference on Learning Representations.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Interna-\ntional Conference on Learning Representations, San\nDiego, CA.\nAnoop Kunchukuttan, Pratik Mehta, and Pushpak Bhat-\ntacharyya. 2018. The IIT Bombay English-Hindi\nparallel corpus. In Proceedings of the Eleventh In-\nternational Conference on Language Resources and\nEvaluation, Miyazaki, Japan. European Language\nResources Association.\nFaisal Ladhak, Esin Durmus, Claire Cardie, and Kath-\nleen McKeown. 2020. WikiLingua: A new bench-\nmark dataset for cross-lingual abstractive summa-\nrization. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020 , pages 4034‚Äì\n4048, Online. Association for Computational Lin-\nguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020a. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871‚Äì7880, Online. Association\nfor Computational Linguistics.\nPatrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian\nRiedel, and Holger Schwenk. 2020b. MLQA: Evalu-\nating cross-lingual extractive question answering. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 7315‚Äì\n7330, Online. Association for Computational Lin-\nguistics.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summa-\nrization Branches Out: Proceedings of the ACL-04\nWorkshop, pages 74‚Äì81, Barcelona, Spain.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising\npre-training for neural machine translation. arXiv\npreprint arXiv:2001.08210.\nFuli Luo, Wei Wang, Jiahao Liu, Yijia Liu, Bin Bi,\nSongfang Huang, Fei Huang, and Luo Si. 2020.\nVeco: Variable encoder-decoder pre-training for\ncross-lingual understanding and generation. arXiv\npreprint arXiv:2010.16046.\nShuming Ma, Li Dong, Shaohan Huang, Dong-\ndong Zhang, Alexandre Muzio, Saksham Sing-\nhal, Hany Hassan Awadalla, Xia Song, and Furu\nWei. 2021. DeltaLM: Encoder-decoder pre-training\nfor language generation and translation by aug-\nmenting pretrained multilingual encoders. ArXiv,\nabs/2106.13736.\nShuming Ma, Jian Yang, H. Huang, Zewen Chi,\nLi Dong, Dongdong Zhang, Hany Hassan Awadalla,\nAlexandre Muzio, Akiko Eriguchi, Saksham Sing-\nhal, Xia Song, Arul Menezes, and Furu Wei. 2020.\nXLM-T: Scaling up multilingual machine transla-\ntion with pretrained cross-lingual transformer en-\ncoders. ArXiv, abs/2012.15547.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals.\n2018. Representation learning with contrastive pre-\ndictive coding. arXiv preprint arXiv:1807.03748.\nXuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun,\nHao Tian, Hua Wu, and Haifeng Wang. 2020. Ernie-\nm: Enhanced multilingual representation by align-\ning cross-lingual semantics with monolingual cor-\npora. arXiv preprint arXiv:2012.15674.\nXiaoman Pan, Boliang Zhang, Jonathan May, Joel\nNothman, Kevin Knight, and Heng Ji. 2017. Cross-\nlingual name tagging and linking for 282 languages.\nIn Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers) , pages 1946‚Äì1958, Vancouver,\nCanada. Association for Computational Linguistics.\n1681\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\nthe limits of transfer learning with a uniÔ¨Åed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch, 21(140):1‚Äì67.\nAfshin Rahimi, Yuan Li, and Trevor Cohn. 2019. Mas-\nsively multilingual transfer for NER. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 151‚Äì164, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nHolger Schwenk, Vishrav Chaudhary, Shuo Sun,\nHongyu Gong, and Francisco Guzm ¬¥an. 2019. Wiki-\nMatrix: Mining 135M parallel sentences in 1620\nlanguage pairs from wikipedia. arXiv preprint\narXiv:1907.05791.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2019. MASS: Masked sequence to se-\nquence pre-training for language generation. arXiv\npreprint arXiv:1905.02450.\nJ¬®org Tiedemann. 2012. Parallel data, tools and inter-\nfaces in OPUS. In Proceedings of the Eighth In-\nternational Conference on Language Resources and\nEvaluation, pages 2214‚Äì2218, Istanbul, Turkey. Eu-\nropean Language Resources Association.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\nKaiser, and Illia Polosukhin. 2017. Attention is\nall you need. In Advances in Neural Information\nProcessing Systems, pages 5998‚Äì6008. Curran As-\nsociates, Inc.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzman, Ar-\nmand Joulin, and Edouard Grave. 2019. CCNet: Ex-\ntracting high quality monolingual datasets from web\ncrawl data. arXiv preprint arXiv:1911.00359.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In NAACL,\npages 1112‚Äì1122, New Orleans, Louisiana.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, becas:\nThe surprising cross-lingual effectiveness of BERT.\nIn Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing, pages 833‚Äì844, Hong Kong,\nChina. Association for Computational Linguistics.\nLinting Xue, Noah Constant, Adam Roberts, Mi-\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2020. mT5: A mas-\nsively multilingual pre-trained text-to-text trans-\nformer. arXiv preprint arXiv:2010.11934.\nJian Yang, Shuming Ma, Dongdong Zhang, Shuangzhi\nWu, Zhoujun Li, and Ming Zhou. 2020. Alternating\nlanguage modeling for cross-lingual pre-training. In\nThirty-Fourth AAAI Conference on ArtiÔ¨Åcial Intelli-\ngence.\nYinfei Yang, Yuan Zhang, Chris Tar, and Jason\nBaldridge. 2019. PAWS-X: A cross-lingual ad-\nversarial dataset for paraphrase identiÔ¨Åcation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 3687‚Äì\n3692, Hong Kong, China. Association for Computa-\ntional Linguistics.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and\nPeter Liu. 2020. PEGASUS: Pre-training with ex-\ntracted gap-sentences for abstractive summarization.\nIn Proceedings of the 37th International Conference\non Machine Learning , volume 119 of Proceedings\nof Machine Learning Research, pages 11328‚Äì11339.\nPMLR.\nWei Zhao, Steffen Eger, Johannes Bjerva, and Is-\nabelle Augenstein. 2020. Inducing language-\nagnostic multilingual representations. arXiv\npreprint arXiv:2008.09112.\nMicha≈Ç Ziemski, Marcin Junczys-Dowmunt, and Bruno\nPouliquen. 2016. The united nations parallel corpus\nv1. 0. In LREC, pages 3530‚Äì3534.\nA Pre-Training Data\nWe reconstruct CCNet2 and follow (Conneau et al.,\n2020) to reproduce the CC-100 corpus for mono-\nlingual data. The resulting corpus contains 94 lan-\nguages. We present the language codes and data\nsize in Table 7 and Table 8 for the monolingual\ncorpus and parallel corpus, respectively. Table 7 re-\nports the language codes and data size in our work.\nWe apply the multilingual sampling strategy (Con-\nneau and Lample, 2019) with Œ± = 0.7 for both\nmonolingual and parallel data.\nB Hyperparameters for Pre-Training\nAs shown in Table 9, we present the hyperparam-\neters for pre-training MT6. We extend the vocab-\nulary of the XLM-R (Conneau et al., 2020) with\nexternal 100 unique mask tokens as the vocabulary\nof MT6 and our MT5 re-implementation.\nC Hyperparameters for Fine-Tuning\nIn Table 10, we present the hyperparameters for\nÔ¨Åne-tuning MT6 on the end tasks.\n2github.com/facebookresearch/cc_net\n1682\nCode Size (GB) Code Size (GB) Code Size (GB)\naf 0.2 hr 1.4 pa 0.8\nam 0.4 hu 9.5 pl 28.6\nar 16.1 hy 0.7 ps 0.4\nas 0.1 id 17.2 pt 39.4\naz 0.8 is 0.5 ro 11.0\nba 0.2 it 47.2 ru 253.3\nbe 0.5 ja 86.8 sa 0.2\nbg 7.0 ka 1.0 sd 0.2\nbn 5.5 kk 0.6 si 1.3\nca 3.0 km 0.2 sk 13.6\nckb 0.6 kn 0.3 sl 6.2\ncs 14.9 ko 40.0 sq 3.0\ncy 0.4 ky 0.5 sr 7.2\nda 6.9 la 0.3 sv 60.4\nde 99.0 lo 0.2 sw 0.3\nel 13.1 lt 2.3 ta 7.9\nen 731.6 lv 1.3 te 2.3\neo 0.5 mk 0.6 tg 0.7\nes 85.6 ml 1.3 th 33.0\net 1.4 mn 0.4 tl 1.2\neu 1.0 mr 0.5 tr 56.4\nfa 19.0 ms 0.7 tt 0.6\nÔ¨Å 5.9 mt 0.2 ug 0.2\nfr 89.9 my 0.4 uk 13.4\nga 0.2 ne 0.6 ur 3.0\ngl 1.5 nl 25.9 uz 0.1\ngu 0.3 nn 0.4 vi 74.5\nhe 4.4 no 5.5 yi 0.3\nhi 5.0 or 0.3 zh 96.8\nTable 7: Statistics of CCNet used for pre-training.\nISO Code Size (GB) ISO Code Size (GB)\nen-ar 5.88 en-ru 7.72\nen-bg 0.49 en-sw 0.06\nen-de 4.21 en-th 0.47\nen-el 2.28 en-tr 0.34\nen-es 7.09 en-ur 0.39\nen-fr 7.63 en-vi 0.86\nen-hi 0.62 en-zh 4.02\nTable 8: Parallel data used for pre-training.\nD Results on XTREME Cross-Lingual\nUnderstanding\nWe present the detailed results of the MT6 and\nour re-implemented MT5 models on XTREME in\nTable 11-16.\nE Results on Wikilingua Cross-Lingual\nSummarization\nAs shown in Table 17, we present the detailed re-\nsults of the MT6 and our re-implemented MT5\nmodels on Wikilingua cross-lingual summariza-\ntion.\nHyperparameters Value\nLayers 8\nHidden size 512\nFFN inner hidden size 1,024\nAttention heads 6\nTraining steps 500K\nBatch size 256\nInput length 512\nAdam œµ 1e-6\nAdam Œ≤ (0.9, 0.9999)\nLearning rate 1e-4\nLearning rate schedule Linear\nWarmup steps 10,000\nGradient clipping 1.0\nNoise density 0.5\nPNAT group number 3\nTable 9: Hyperparameters used for pre-training MT6.\n1683\nHyperparameters WikiAnn XQuAD MLQA TyDiQA XNLI PAWS-X Gigaword Wikilingua\nBatch size 32 32 32 32 32 32 32 32\nLearning rate 7e-5 3e-5 3e-5 5e-5 2e-5 3e-5 1e-5 1e-4\nLR schedule Linear Linear Linear Linear Linear Linear Linear Linear\nWarmup 10% 10% 10% 10% 10% 10% 10K steps 2.5K steps\nEpochs/Steps 5 epochs 3 epochs 3 epochs 40 epochs 10 epochs 10 epochs 20 epochs 100K steps\nTable 10: Hyperparameters used for Ô¨Åne-tuning MT6 on the end tasks.\nModel ar he vi id jv ms tl eu ml ta te af nl en de el bn hi mr ur\nMT5 26.5 24.0 60.7 43.5 43.7 49.2 65.2 52.4 13.1 26.4 20.2 58.2 69.4 77.5 63.6 51.7 28.3 37.9 27.2 19.6\nMT6 39.6 22.2 63.8 43.7 40.4 54.7 62.9 42.9 14.2 26.4 15.7 58.9 66.0 78.5 67.1 59.6 39.2 47.5 31.8 25.5\nModel fa fr it pt es bg ru ja ka ko th sw yo my zh kk tr et Ô¨Å hu Avg\nMT5 15.5 69.8 69.1 67.7 57.6 61.1 49.5 24.1 26.2 23.8 3.0 54.2 56.3 2.8 29.0 23.4 52.8 57.0 62.6 60.9 43.1\nMT6 21.7 70.7 65.9 67.8 64.9 65.8 51.6 23.4 25.3 21.9 4.9 65.2 53.6 8.5 26.3 28.6 55.9 49.3 58.2 57.1 44.7\nTable 11: Results on WikiAnn named entity recognition.\nModel en es de el ru tr ar vi th zh hi Avg\nMT5 68.6 / 56.7 50.2 / 35.6 47.2 / 34.1 30.3 / 18.5 41.4 / 28.5 35.9 / 21.9 25.1 / 14.7 48.6 / 31.6 31.7 / 24.6 54.7 / 34.9 29.7 / 18.6 42.1 / 29.1\nMT6 74.2 / 62.4 57.8 / 43.1 53.1 / 38.7 41.6 / 28.2 51.1 / 35.6 39.2 / 26.0 40.4 / 25.2 53.6 / 35.2 41.9 / 33.9 61.7 / 45.8 39.8 / 26.0 50.4 / 36.4\nTable 12: Results on XQuAD question answering.\nModel en es de ar hi vi zh Avg\nMT5 61.2 / 47.8 41.7 / 27.1 37.8 / 25.4 21.1 / 10.8 22.6 / 13.7 40.5 / 24.2 38.4 / 20.6 37.6 / 24.2\nMT6 65.5 / 52.7 47.8 / 32.0 43.2 / 29.8 32.4 / 18.7 31.8 / 20.2 45.0 / 28.3 42.4 / 23.6 44.1 / 29.3\nTable 13: Results on MLQA question answering.\nModel en ar bn Ô¨Å id ko ru sw te Avg\nMT5 55.4 / 44.7 35.3 / 18.3 18.4 / 9.2 33.3 / 22.2 37.3 / 24.8 22.6 / 16.9 37.3 / 27.7 25.5 / 13.6 11.2 / 4.5 30.7 / 20.2\nMT6 58.1 / 48.0 40.8 / 23.6 24.1 / 14.2 39.7 / 27.3 39.9 / 26.1 26.9 / 18.4 41.9 / 31.4 35.9 / 24.5 16.3 / 10.9 36.0 / 24.9\nTable 14: Results on TyDiQA question answering.\nModel en fr es de el bg ru tr ar vi th zh hi sw ur Avg\nMT5 75.4 62.0 62.1 58.9 58.9 57.7 59.0 55.7 52.7 58.4 55.0 55.2 53.6 42.4 50.7 57.2\nMT6 78.4 70.6 69.8 64.8 65.7 66.6 65.8 61.6 63.3 66.6 63.1 66.2 60.3 51.5 56.9 64.7\nTable 15: Results on XNLI natural language inference.\nModel en fr de es ja ko zh Avg\nMT5 91.6 81.2 79.9 80.7 70.7 68.2 73.5 78.0\nMT6 93.5 87.0 85.4 87.3 72.4 70.1 79.8 82.2\nTable 16: Results on PAWS-X cross-lingual paraphrase adversaries.\nModel es-en ru-en vi-en tr-en\nRG-1 RG-2 RG-L RG-1 RG-2 RG-L RG-1 RG-2 RG-L RG-1 RG-2 RG-L\nMT5 33.12 11.36 27.32 29.14 8.77 23.29 28.96 8.98 22.77 29.31 10.57 23.44\nMT6 33.79 11.83 27.90 30.40 9.49 24.32 29.96 9.52 23.72 29.55 10.80 23.82\nTable 17: Evaluation results on Wikilingua cross-lingual abstractive summarization. RG is short for ROUGE.\nResults of MT5 and MT6 are averaged over three runs.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7397856712341309
    },
    {
      "name": "Computer science",
      "score": 0.7279225587844849
    },
    {
      "name": "Natural language processing",
      "score": 0.7085444927215576
    },
    {
      "name": "Machine translation",
      "score": 0.5519866943359375
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5396454930305481
    },
    {
      "name": "Speech recognition",
      "score": 0.4178958535194397
    },
    {
      "name": "Linguistics",
      "score": 0.37319809198379517
    },
    {
      "name": "Engineering",
      "score": 0.09896838665008545
    },
    {
      "name": "Electrical engineering",
      "score": 0.08243191242218018
    },
    {
      "name": "Voltage",
      "score": 0.078301340341568
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I125839683",
      "name": "Beijing Institute of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210124949",
      "name": "Microsoft Research (India)",
      "country": "IN"
    }
  ]
}