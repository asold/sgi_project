{
  "title": "Few-shot Sequence Learning with Transformers",
  "url": "https://openalex.org/W3110667630",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5003257470",
      "name": "Lajanugen Logeswaran",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5084509956",
      "name": "Ann Lee",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5076248976",
      "name": "Myle Ott",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5108652283",
      "name": "Honglak Lee",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5111465122",
      "name": "Marc’Aurelio Ranzato",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5053145694",
      "name": "Arthur Szlam",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2601450892",
    "https://openalex.org/W2794363191",
    "https://openalex.org/W2137825550",
    "https://openalex.org/W2980433389",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963424430",
    "https://openalex.org/W2964112702",
    "https://openalex.org/W2787501667",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W2963341924",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2914752403",
    "https://openalex.org/W2970279348",
    "https://openalex.org/W2911300548",
    "https://openalex.org/W2995322030",
    "https://openalex.org/W2945337239",
    "https://openalex.org/W2963921132",
    "https://openalex.org/W2604763608",
    "https://openalex.org/W2115733720",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2535697732",
    "https://openalex.org/W2089217417",
    "https://openalex.org/W2971008823",
    "https://openalex.org/W2963775850",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2950129230",
    "https://openalex.org/W1942758450",
    "https://openalex.org/W2915573484",
    "https://openalex.org/W2620076854",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W1525859397",
    "https://openalex.org/W2144656844"
  ],
  "abstract": "Few-shot algorithms aim at learning new tasks provided only a handful of training examples. In this work we investigate few-shot learning in the setting where the data points are sequences of tokens and propose an efficient learning algorithm based on Transformers. In the simplest setting, we append a token to an input sequence which represents the particular task to be undertaken, and show that the embedding of this token can be optimized on the fly given few labeled examples. Our approach does not require complicated changes to the model architecture such as adapter layers nor computing second order derivatives as is currently popular in the meta-learning and few-shot learning literature. We demonstrate our approach on a variety of tasks, and analyze the generalization properties of several model variants and baseline approaches. In particular, we show that compositional task descriptors can improve performance. Experiments show that our approach works at least as well as other methods, while being more computationally efficient.",
  "full_text": "Few-shot Sequence Learning with Transformers\nLajanugen Logeswaran1, Ann Lee2, Myle Ott2, Honglak Lee1,\nMarc’Aurelio Ranzato2, Arthur Szlam2\n1University of Michigan, 2Facebook AI Research\nAbstract\nFew-shot algorithms aim at learning new tasks provided only a handful of training\nexamples. In this work we investigate few-shot learning in the setting where the\ndata points are sequences of tokens and propose an efﬁcient learning algorithm\nbased on Transformers. In the simplest setting, we append a token to an input\nsequence which represents the particular task to be undertaken, and show that the\nembedding of this token can be optimized on the ﬂy given few labeled examples.\nOur approach does not require complicated changes to the model architecture such\nas adapter layers nor computing second order derivatives as is currently popular in\nthe meta-learning and few-shot learning literature. We demonstrate our approach\non a variety of tasks, and analyze the generalization properties of several model\nvariants and baseline approaches. In particular, we show that compositional task\ndescriptors can improve performance. Experiments show that our approach works\nat least as well as other methods, while being more computationally efﬁcient.\n1 Introduction\nThe problem of learning a classiﬁer from a handful examples has received considerable attention in\nthe vision domain under the name of few-shot learning (Fink, 2005; Fei-Fei et al., 2006). However,\nless work exists in the space of few-shot problems involving discrete sequences, such as sequences\nof discrete actions in reinforcement learning or sequences of words in natural language processing.\nIn this work, we study the problem of sequence classiﬁcation and modeling in the few-shot regime.\nSpeciﬁcally, we assume there are several training tasks available for learning and, at test time, we are\ninterested in performing few-shot adaptation to a given new task.\nTransformers (Vaswani et al., 2017) have been very successful at modeling discrete sequences (Bar-\nrault et al., 2019; Devlin et al., 2018; Parisotto et al., 2019). Further, they have been shown to use\ncontext tokens appended to an input to adapt their generations or to switch between different tasks\n(Lample et al., 2019; Shen et al., 2019; Zellers et al., 2019; Keskar et al., 2019). Thus, one might hope\nthat such context tokens could be effectively used in the meta-learning setting for discrete sequences.\nIn this work, we show that this is indeed the case. Our approach to few-shot learning introduces a set\nof task speciﬁc parameters (a task embedding), in addition to the parameters of the model that are\nshared among all tasks. Unlike other approaches that require architectural changes (Houlsby et al.,\n2019), task embeddings are simply fed to the input of the transformer. Learning a new task consists\nof inferring an appropriate task embedding for the task, leaving the shared model parameters intact.\nTowards this end, we propose a simple training algorithm where the task embedding is found via\ngradient based optimization, which is simpler and computationally less expensive than second order\noptimization methods (Finn et al., 2017; Zintgraf et al., 2019).\nTo summarize, our contributions in this work are as follows. First, we show that a simple alternating-\nminimization approach for few-shot learning works well in combination with the transformer ar-\nchitecture. Second, we show that a simple yet effective way to condition the transformer with task\ninformation is via input conditioning (i.e., feeding task information as input to the transformer);\n4th Workshop on Meta-Learning at NeurIPS 2020, Vancouver, Canada.\narXiv:2012.09543v1  [cs.LG]  17 Dec 2020\nthis naturally extends to compositional task information. Third, we introduce a battery of synthetic\nsequence classiﬁcation and modeling tasks to benchmark in a controlled setting various baseline ap-\nproaches and model variants for few shot learning of discrete sequences. And ﬁnally, we demonstrate\nthat the proposed approach offers a better trade-off between few-shot performance and run time cost\ncompared to other baselines, including meta-learning approaches.\n2 Problem Deﬁnition\nWe assume a distribution pdata(T) over tasks from which disjoint sets of training, validation and\ntest sets of tasks are drawn. The set of training tasks is denoted by {Ttrain\ni }N\ni=1, where each task\nTtrain\ni has an associated set of training examples {(xi\nj,yi\nj)Ni\nj=1}. Validation and test tasks are deﬁned\nsimilarly, except each test task only has ktraining examples. We use the training tasks to learn the\nmodel parameters and the validation tasks to determine hyperparameters. The optimal parameters\nand hyperparameters identiﬁed are used for evaluating average model performance on test tasks. This\ninvolves ﬁrst (optionally) training on the small set of training examples accompanied by a test task,\nfollowed by testing on the corresponding test set.\nWe focus on two types of tasks that involve discrete sequences as inputs and outputs: sequence\nclassiﬁcation and transduction. In sequence classiﬁcation, the inputs xare sequences and the output\nyis a discrete categorical label. In sequence transduction, each task consists of modeling the joint\ndistribution of a sequence y, conditioned on some input context sequence x. The performance metrics\nfor these settings are respectively accuracy and perplexity, averaged across the test tasks.\n3 Approach\n3.1 Architecture\nIn this work, we explore an adaptation of transformers to the few-shot regime. Previous works have\nshown that the behavior of a transformer model can be conditioned by appending special tokens\ndescribing the task to be performed on the input sequence (Lample et al., 2019; Zellers et al., 2019).\nWe append a task embedding vector which represents information about the task of interest to the\ninput sequence of token embeddings. We intend to control the overall behavior of the model for a\nparticular task by altering the task embeddings while keeping the rest of the model parameters intact.\nFor classiﬁcation tasks, we use a transformer encoder similar to the BERT model (Devlin et al., 2018).\nA classiﬁcation head sits on the ﬁnal layer representation of a special token at the beginning of the\nsequence. We replace this special token with the task embedding vector zin our model. We use a\ntransformer decoder architecture for the transduction tasks and append the task embedding vector to\nthe input sequence similar to the classiﬁcation setting.\nIn both settings we compute a log-likelihood of the form log p(y|x,z; θ), where x is the input\nsequence, zis the task embedding and θthe model parameters. In the classiﬁcation setting yis a\nsingle categorical value. In the sequence transduction setting, yis a sequence and the log likelihood\ndecomposes as the sum of the conditional log-likelihood terms via the chain rule of probability theory:\nlog p(y|x,z; θ) = ∑\nilog p(yi|yi−1,··· ,y1,x,z ; θ).\nNote that in practical applications θcan be high-dimensional, in the order of hundreds of millions.\nOur goal is to alleviate overﬁtting in the few-shot regime by adapting only z to learn a new task,\nwhere zis a small vector with at most a few hundred components. Next, we describe how we learn\nthe model parameters θand how we estimate the task embedding zfor a given task.\n3.2 Training and Inference Algorithm\nWe train our models with an alternating-minimization scheme similar to Maurer et al. (2013) and Ku-\nmar & Daume III (2012), that can be considered a simpliﬁcation of the CA VIA approach in Zintgraf\net al. (2019) (or as a reﬁnement of the “ﬁrst-order” method in that work). See Algorithm 1 for\npseudo-code. We separate the weights of the network deﬁning the model into shared weights θ, and\nper-task weights, as in CA VIA. In our case, the per-task weights form the embeddingz, one for each\ntask; while all other parameters θare shared.\nGiven a few examples from the training task Ttrain\ni (line 3), we alternate training zTtrain\ni\n(the task\nembedding of the Ttrain\ni task) for a few gradient descent steps keeping θﬁxed (see line 5 and 6), and\n2\nAlgorithm 1: TAM for k-shot Learning\nInput : Training tasks Ttrain\n1 ,..., Ttrain\nN\nOutput :Model parameters θ\n1 repeat\n2 Sample a training task: Ttrain\ni\n3 Sample Ni ≥ktraining examples from the task {(xj,yj)j=1,···,Ni }∼T train\ni\n4 Initialize: zTtrain\ni\n= 0,∆θ= 0\n5 while loss improves and max number of updates not reached do\n6 zTtrain\ni\n←zTtrain\ni\n−∇zT train\ni\n∑\nj −log p(yj|xj,zTtrain\ni\n; θ)\n7 ∆θ←∆θ−∇θ\n∑Ni\nj=1 −log p(yj|xj,zTtrain\ni\n; θ)\n8 θ←θ+ ∆θ\n9 until max training iterations;\nthen update θbased on the optimal task embedding. In practice, however, we found it helpful to update\nθbased on gradients accumulated for the intermediate values of the task embedding encountered in\nthe inner loop optimization (line 7). We surmise that this optimization choice helps the model ﬁnding\nbetter task embeddings as the whole parameter vector θis updated to account for this search. Task\nembedding gradient updates (line 6) are performed until the loss no longer improves or the maximum\nnumber of update steps has been reached. Note that unlike prior methods such as MAML or CA VIA\nwe do not backpropagate gradients through an optimization process, which simpliﬁes and speeds up\nour optimization. We call our method, transformer trained with Alternating Minimization (TAM) –\nalthough the alternating minimization algorithm could be applied to other architectures as well. At\ntest time, given a new task Ttest, zTtest is trained with a few steps of gradient descent (similar to line\n6), with all other parameters held ﬁxed. Since TAM is trained to optimize task embeddings on the ﬂy,\nwe expect it to ﬁnd good embeddings of the new task at test time as well.\n4 Related work\nFew-shot learning and meta-learning There is now a vast literature on learning methods designed\nfor quickly adapting to new settings. At a coarse level, one can consider classes of methods that adapt\nthe learning algorithm based on the task (and so are “meta-learners”) (Schmidhuber, 1987; Hochreiter\net al., 2001; Andrychowicz et al., 2016; Finn et al., 2017; Nichol & Schulman, 2018), or describe\nmodel architectures that can adapt to learn sample-efﬁciently over a task distribution (Vinyals et al.,\n2016; Snell et al., 2017). Many methods have elements of both of these, e.g. Mishra et al. (2018);\nRusu et al. (2019); Zintgraf et al. (2019).\nThe method we describe in this work can be considered squarely in the class of model architectures\nfor sample efﬁcient learning. It is a descendant of Hinton & Plaut (1987); Schmidhuber (1992); Ba\net al. (2016) and is closely related to Rusu et al. (2019); Zintgraf et al. (2019) in that we pick a subset\nof the weights of the model that are task speciﬁc (the “fast” weights), and update them using the\ntraining examples for a speciﬁc task; but update the other (“slow”) weights on all training examples\nfor all tasks. Our approach is closest to Zintgraf et al. (2019), but differs in the way the fast weights\nare used by the model. We do not use higher-order gradients for the slow weights, instead we use an\nalternating minimization type update.\nTask transfer for transformers Our approach is also related to other recent work in natural\nlanguage processing. We leverage the particular structure of the transformer architecture (Vaswani\net al., 2017), which has been successful in many NLP tasks. Several works have shown that adding a\ntoken to an input can be used to switch between different tasks (Lample et al., 2019; Shen et al., 2019;\nZellers et al., 2019; Keskar et al., 2019). Transformer language models trained on large corpora have\nalso been recently shown to have impressive few-shot learning capabilities (Brown et al., 2020). More\ngenerally, with the success of methods based on pretraining transformer models (Devlin et al., 2018),\nand ﬁnetuning on target tasks, there have been several works discussing how to adapt a pre-trained\nmodel without full ﬁnetuning (Houlsby et al., 2019; Stickland & Murray, 2019) but their focus has\nbeen on reducing the number of parameters subject to optimization at ﬁnetuning time as opposed to\nreducing the number of examples as in this study.\n3\n5 Experiments\n5.1 Model and Training Details\nIn the classiﬁcation setting, TAM is a bidirectional transformer that takes the input sequence xand\nthe task embedding zas input, and outputs a distribution over classes. In the sequence transduction\nsetting, TAM is a transformer decoder with a causal attention mechanism and takes as additional input\nthe output sequence yup to the token before the last. In this case the model is trained to predicted the\nsequence yat the last |y|(length of sequence y) time steps. Since model parameters are shared across\ntasks, TAM needs to leverage the task embedding to perform the tasks well. Both classiﬁcation and\ntransduction models are trained with cross-entropy loss.\nUnless otherwise speciﬁed our transformer has 4 layers with an embedding size of 128. We use the\nAdam optimizer (Kingma & Ba, 2015) for both outer and inner loop optimization. The maximum\nnumber of task embedding optimization steps is set to 25 during training. We train a single model\nusing N samples at training time, treating N as a hyperparameter and apply it to k-shot problems\nwith different values of kat test time. The size of the task embedding was set to match the embedding\ndimension of the transformer (128). We discuss more details about hyperparameter choices and how\nthey inﬂuence model performance in section 5.6.\n5.2 Baselines\nTask-Agnostic transformer This baseline uses the same architecture as TAM but is not informed\nabout the existence of different tasks at training time, i.e., no task embedding is fed at the input. At\ntest time, the model is ﬁne-tuned on the ktraining examples from the test task.\nMultitask transformer This is a transformer that is conditioned on the current task both in the\nclassiﬁcation and transduction settings. It is identical to TAM except all parameters including task\nembeddings are trained by standard back-propagation, without any alternating minimization.\nMatching Networks (Vinyals et al., 2016) We consider Matching Networks only in our classiﬁca-\ntion setting, as it is not straightforward to use it for transduction. We use a transformer to model the\nsimilarity between a query instance and support set instance which takes the concatenation of the\ntwo sequences as input and outputs a similarity score. The prediction is a convex sum of the training\nexample labels, the weights being similarity scores.\nSNAIL (Mishra et al., 2018) This model is similar to the task-agnostic transformer except the input\nis augmented with the concatenation of all input-output training pairs. For both Matching Networks\nand SNAIL, we construct training episodes by samplingktraining examples to deﬁne a task, to match\nthe test scenario. We train different models for each k-shot problem. Both Matching networks and\nSNAIL are trained using the multi-task training loss and applied to test tasks without any ﬁnetuning.\nMAML (Finn et al., 2017) All model parameters are trained using MAML, with the same model\narchitecture as TAM. The entire model is ﬁne-tuned on test tasks.\nCA VIA (Zintgraf et al., 2019) Similar to TAM, CA VIA has a set of task-speciﬁc parameters and\nshared parameters. The training algorithm is similar to MAML, but inner loop updates are performed\non the task-speciﬁc parameters as opposed to the entire model.\n5.3 Sequence Classiﬁcation and Transduction\nMost prior work on few-shot learning have focused on computer vision benchmarks such as Omniglot\n(Lake, 2019) and Mini-ImageNet (Vinyals et al., 2016). In the sequential data setting, Bao et al.\n(2019) constructed synthetic benchmarks from existing text datasets but the number of tasks is rather\nlimited. In this work we construct a new set of benchmarks involving synthetic sequential data,\nallowing us to evaluate models in a more controlled setting after training on a larger number of tasks.\n5.3.1 Synthetic Benchmarks\nWe construct a synthetic few-shotclassiﬁcation benchmark as follows. The benchmark consists of\ntasks that involve a non-negative integer sequence as input and a discrete label as output. A task is\nconstructed by applying a sequence of mathematical transformations to input sequences as follows:\nElement-wise transform (T1) →Subsequence extraction (T2) →Labeling function (T3). The arrows\nindicate function composition and the sequence of transformations maps an input sequence to a\nsingle integer. The transformations are deﬁned as T1 ∈S1,T2 ∈S2,T3 ∈S3 where, S1 = {mul\n4\nModel Sequence Classiﬁcation Sequence Transduction Path Finding\n1 5 10 20 1 5 10 20 1 5 10 20\nTask 40.50 64.75 74.25 82.50 6.27 5.26 4.71 4.01 3.17 1.75 1.55 1.39\nAgnostic ±1.73 ±1.29 ±1.29 ±1.58 ±0.17 ±0.04 ±0.05 ±0.07 ±0.27 ±0.03 ±0.02 ±0.01\nMultitask 38.75 66.00 77.50 87.50 13.80 6.80 5.18 2.91 6.39 1.98 1.64 1.44\n±0.96 ±0.82 ±1.29 ±0.58 ±3.36 ±0.26 ±1.01 ±0.49 ±1.96 ±0.12 ±0.05 ±0.02\nMatching 43.00 58.75 64.50 67.00 – –Network ±1.15 ±2.45 ±1.83 ±1.41\nSNAIL 43.00 44.00 68.25 67.50 2.48 3.80 4.98 4.11 2.63 1.95 3.47 3.04\n±1.41 ±2.00 ±1.26 ±4.43 ±0.38 ±0.38 ±0.03 ±2.94 ±0.27 ±0.25 ±1.56 ±1.01\nMAML 39.60 63.40 71.80 78.80 6.73 5.84 5.19 4.20 5.49 2.05 1.65 1.44\n±0.55 ±0.89 ±0.84 ±0.84 ±0.16 ±0.2 ±0.08 ±0.10 ±0.85 ±0.03 ±0.01 ±0.01\nCA VIA 43.00 78.00 87.00 91.00 11.05 2.75 1.78 1.53 2.21 1.31 1.25 1.21\n±0.58 ±1.26 ±0.50 ±0.58 ±2.94 ±0.51 ±0.14 ±0.06 ±0.21 ±0.03 ±0.03 ±0.02\nTAM 40.50 75.50 89.50 94.50 8.47 2.92 1.47 1.15 1.82 1.27 1.22 1.17\n±0.82 ±0.50 ±0.58 ±0.82 ±1.42 ±0.67 ±0.18 ±0.03 ±0.08 ±0.01 ±0.01 ±0.01\nTable 1: k-shot sequence classiﬁcation and sequence transduction experiments on our three benchmarks for\nk ∈ {1, 5, 10, 20}. The metric for sequence classiﬁcation is average accuracy on test tasks (higher is better).\nOn the transduction tasks, the performance metric is average perplexity on test tasks (lower is better). Random\nperformance is at 25% accuracy (classiﬁcation) and 12 perplexity points (other two tasks). Entries in smaller\nfont are error bars, and they are estimated on 4 trials varying the model initialization.\nv, add v, div v, mod v}; S2 = {(not) multiple of v, (not) greater than v, (do not) have exactly\nv divisors}; S3 = {count, min, max, mean, median, mode, ﬁrst, last, max-min, middle }, where\nv∈{1 ···n}for some integer n. We randomly generate a large number of sequences X of integers\nfrom {0 ···N}. We apply the transformation sequence T1,T2,T3 to these sequences x ∈X and\nget the corresponding outputs T3(T2(T1(x))). The C most frequent outputs are then deﬁned to be\nthe C classes of interest. We obtain a uniform amount of data from each class and discard input\nsequences for which the output does not belong to one of the chosen C classes. Cases where at\nleast Cdistinct outputs cannot be obtained are discarded. These Cclasses then constitute a C-way\nclassiﬁcation task. An example task is mul 2 → less than 5 → count, where the goal is to\ncount the number of input elements which, when multiplied by 2, are less than 5 (i.e., count number\nof input integers less than 3). The semantics of each of the transforms are deﬁned in the appendix.\nWe set C = 4 in our experiments. V ocabulary size and input sequence length are set to 12 and 5,\nrespectively. Combinations of transforms that have identical input-output relationship are identiﬁed\nand removed during task construction. All tasks are thus unique in terms of input-output mapping.\nWe also construct two sequence transduction benchmarks. The ﬁrst benchmark is constructed in a\nway similar to the classiﬁcation tasks where we consider a sequence of transformations mapping an\ninput sequence to an output sequence T1 →T2 →T3, where T1 ∈S1,T2 ∈S2,T3 ∈S3; S1 =\n{mul v, add v, div v, mod v}; S2 = {replace v with v′, replace xi with f(xi,xj) }; S3 = {sort\nascending, sort descending, reverse, swap(xi,xj), shift right v}, and v,v′,i,j are integers chosen at\nrandom, xp represents the element at position pin the input sequence, f is a mathematical function\n(Eg: f(a,b) ∈{a+ b,abs(a−b),b, ···}). An example task is add 2 → replace 2 with 1 →\nreverse, and an (input, output) sample drawn from this task is: ([0,5,0,3,6],[8,5,1,7,1]).\nOur second transduction benchmark is a path ﬁnding task in a grid world (see appendix B for an\nillustration). A task is deﬁned by start and end positions in a square grid of size N ×N. Given the\nlocations of obstacles in this grid, the objective of the task is to ﬁnd the shortest path connecting\nstart and end positions that avoids the obstacles. The source and target sequences correspond to the\nlocations of obstacles and optimal path from start to end position avoiding the obstacles, respectively.\nWe use 500, 16, 64 tasks respectively for training, validation and testing for all three setups. Tasks are\nunique and randomly assigned to these sets, in other words we test generalization under the condition\nof distributional match between the training and the test set. Each training task has 500 examples.\n5.3.2 Results\nTable 1 reports the results on this benchmark. In the extreme few-shot setting (k= 1), all methods\nperform poorly, although memory based methods such as matching networks and SNAIL fare the\nbest. However, they start performing relatively worse when more labelled data is available, where\n5\nModel Sequence Classiﬁcation Sequence Transduction Path Finding\n1 5 10 20 1 5 10 20 1 5 10 20\nMultitask 57.50 74.00 81.00 88.5 43.32 7.50 3.48 2.16 3.08 1.62 1.32 1.23\n±3.51 ±5.48 ±4.24 ±2.08 ±10.87 ±0.43 ±0.12 ±0.05 ±0.61 ±0.33 ±0.05 ±0.02\nMatching 61.25 69.50 72.25 67.5 – –Network ±4.35 ±5.45 ±6.08 ±5.92\nSNAIL 63.5 71.75 76.25 80.25 7.00 6.24 6.93 17.10 1.53 1.71 3.36 4.22\n±4.80 ±4.86 ±2.75 ±2.87 ±2.03 ±0.27 ±3.25 ±9.66 ±0.29 ±0.09 ±0.57 ±0.79\nCA VIA 57.25 66.25 67.50 68.50 36.72 6.01 3.99 3.27 1.99 1.27 1.21 1.17\n±12.09 ±14.73 ±15.67 ±16.42 ±8.83 ±0.88 ±0.50 ±0.24 ±0.11 ±0.01 ±0.00 ±0.00\nTAM 63.00 76.50 82.75 88.5 6.15 3.43 2.69 2.13 2.33 1.30 1.23 1.19\n(Comp) ±5.35 ±4.65 ±3.86 ±2.65 ±0.91 ±0.05 ±0.04 ±0.02 ±0.18 ±0.02 ±0.01 ±0.01\nTAM 45.25 72.5 81.5 89.75 7.80 5.08 3.60 2.42 4.37 1.27 1.17 1.11\n(Non-comp) ±3.59 ±3.70 ±2.89 ±0.96 ±0.09 ±0.24 ±0.15 ±0.08 ±3.59 ±0.01 ±0.00 ±0.00\nTable 2: Compositional models for few-shot sequence classiﬁcation and sequence transduction. All models\n(except non-compositional TAM) get information on the primitives present in the tasks via extra tokens appended\nto the input sequence, except that one such primitive is unseen at test time. Non-compositional TAM is not given\ninformation about primitives, and estimates a single task embedding instead.\nﬁne-tuning part or all of the model parameters could be beneﬁcial. Both SNAIL and matching\nnetworks sometimes perform absolutely worse when more labeled examples are present, suggesting\nthey are failing to effectively use their memory when confronted with longer sequences. Fine-tuning\nthe whole model, particularly in the multitask setting, works remarkably well for larger values of\nk, although the best performance is achieved by TAM, suggesting the need for sample efﬁcient task\nadaptation methods. For k >1, TAM performs comparably or better than all baselines, including\nMAML and CA VIA. Furthermore, TAM is more efﬁcient to train than CA VIA (see section 5.6).\n5.4 Compositional Task Representations\nCompositional reasoning is arguably an important skill for few-shot learning (Lake, 2019; Purush-\nwalkam et al., 2019). The underlying assumption is that there exist primitive skills which can be\nlearned and combined together to solve entirely new tasks. If a learner can leverage the compositional\nstructure of the learning task, it may learn with even less labeled data.\nIn this section we assess how much better TAM works when we expose the compositional structure\nof the tasks described in §5.3.1. Speciﬁcally, we assess the ability to learn new tasks which are\ncomposed of primitives, some of which were unseen during training. To present an example from\nthe classiﬁcation setting, assume the models know that tasks are composed of three transforms\nT1 ∈S1,T2 ∈S2,T3 ∈S3. We henceforth refer to the elements of S1 ∪S2 ∪S3 as primitives.\nFurther assume the model never saw the add kprimitive during training. Given a new test task for\nwhich T1 = add 3 (and T2,T3 are known primitives seen during training), we expect the model to\ninfer the concept of add from the few training examples of the test task.\n5.4.1 Task Construction\nIn the compositional setting, we provide models with information about the primitives used to\nconstruct the task. For the classiﬁcation and transduction tasks, the training and test tasks are\nconstructed as follows. Assume the set of primitives available for the three transforms to beS1,S2,S3.\nWe hold out a subset of primitives S′\n1,S′\n2,S′\n3 respectively from each of these three sets, which shall\nconstitute the unseen primitives. The training tasks are made up of primitives from S1 −S′\n1,S2 −\nS′\n2,S3 −S′\n3, which we will refer to as seen primitives. The test tasks are made up of seen and unseen\nprimitives where exactly one primitive is unseen (For instance,T1 ∈S1 −S′\n1,T2 ∈S′\n2,T3 ∈S3 −S′\n3).\nModel performance is averaged over multiple (8) different choices of S′\n1,S′\n2,S′\n3.\nWe also deﬁne a compositional path-ﬁnding task as follows. In addition to ﬁnding the optimal path\nfrom start, end positions while avoiding obstacles, we now require the path to lie on a speciﬁed\nway-point. The locations of the start, end and way points thus deﬁne the primitives that make up a\ntask. Similar to the previous settings, we hold out sets of values for each of these points and construct\nthe train/test tasks in an analogous manner.\n6\nModel Sequence Classiﬁcation (Accuracy)\n1-shot 5-shot 10-shot 20-shot\nInput token 0.41 0.76 0.89 0.94\nAdapters 0.43 0.75 0.88 0.94\nLayer Norm 0.42 0.64 0.78 0.88\nTable 3: k-shot classiﬁcation accuracy when plug-\nging the task embedding in various ways for differ-\nent values of k.\nArch Training Sequence Classiﬁcation (Acc.)\n1-shot 5-shot 10-shot 20-shot\nLSTM Multi 0.38 0.57 0.72 0.87\nAlt 0.35 0.78 0.83 0.85\nTransf. Multi 0.39 0.68 0.80 0.88\nAlt 0.41 0.76 0.89 0.94\nTable 4: k-shot accuracy for different architectures with\nmultitask and the proposed training algorithms.\n5.4.2 Training\nFor all the models, a sequence of primitive ids representing the primitives that make up the task is\nappended to the input sequence. These primitive embeddings θe are learned along with the other\nmodel parameters. To simulate the testing conditions, at training time we pretend some primitives\nare unknown. For the multitask, matching network and SNAIL baselines we learn an unknown\nprimitive embedding, which is used to initialize embeddings of unknown primitives encountered at\ntest time. Although the tasks themselves are harder (because entire primitives are unseen), modeling\nthem is easier because the primitive information is given to the model. For CA VIA and TAM, we\ninfer embeddings for unknown primitives on the ﬂy using gradient descent during train and test.\nSee Algorithm 2 in the appendix for the complete algorithm. We use 5000 training tasks, and 100\nvalidation and test tasks each. Each training task has 500 examples.\n5.4.3 Results\nTable 2 summarizes the results in the compositional setting. We observe similar trends as before for\nthe non-compositional case. Multitask learning becomes competitive only for larger values of k. Vice\nversa, matching networks and SNAIL suffer with long sequences (larger values of k). TAM performs\nat least comparably if not better than methods relying on second order derivatives like CA VIA. In fact,\nCA VIA sometimes fail to converge as shown by the rather large error bars. Finally, the compositional\nversion of TAM often yields higher accuracy than the corresponding non-compositional version,\nshowing that the model is able to cleverly leverage the additional knowledge about a subset of\nprimitives (two out of three) that compose the new task. Compositionality is particularly helpful with\nfewer shots (e.g., 1-shot) – with sufﬁcient training examples (e.g., 20-shot) models beneﬁt less from\ncompositionality.\n5.5 Ablation experiments\nWhere to plug task embeddings The experiments in the paper so far consider a simple condition-\ning scheme where the task embedding appears as an additional embedding in the input sequence of\ntoken embeddings. We compare this against other ways of incorporating task-speciﬁc parameters into\nthe model. Houlsby et al. (2019) introduce adapter layers, parameter modules that are inserted at every\nlayer of a pre-trained transformer. An adapter layer down-projects its input, applies a non-linearity,\nand up-projects the representation back to the original size. Our ﬁrst baseline considers parameters in\nthe adapter layers as the task embedding. Another popular method for adapting pre-trained networks\nto new tasks is adapting parameters in normalization layers (Perez et al., 2018; Ghiasi et al., 2017).\nIn our second baseline, we consider the scale and bias parameters in the Layer Normalization layers\nof the transformer as the task embedding.\nThe results in Table 3 on non-compositional classiﬁcation tasks show that using adapter layer\nparameters as task embedding yields similar results to the simplest conditioning scheme where the\ntask embedding is fed as an additional input. Using normalization parameters as the task embedding\ninstead performs slightly worse. This shows that our input conditioning scheme is simple yet effective.\nImportance of transformer architecture The experiments presented in this paper so far have\nused a transformer architecture. Although transformers are a natural choice for problems involving\nsequences owing to their recent success, the proposed training algorithm applies equally well to\nother architectures. We study the impact of swapping out the transformer with a recurrent model in\nTable 4 on non-compositional tasks. We use a bidirectional LSTM with a comparable number of\nparameters to our transformer model. The classiﬁer head acts on the ﬁnal representation of the ﬁnal\n7\nFigure 1: 2D PCA projections of task embeddings\nlearned by our algorithm for the gridworld domain.\nTasks visualized here have the same start position\n(4,4). Points are color coded based on horizontal\n(left plot) and vertical (right plot) coordinates of\nthe end position corresponding to each task.\nModel Classiﬁcation Transduction Path-ﬁnding\nAcc., Time Ppl., Time Ppl., Time\nMultitask 67.4, 30min 7.2, 23min 2.9, 20min\nCA VIA 74.8, 3h 4.5, 5.3h 1.5, 3.7h\nTAM 75.0, 2h 1.5, 2.3h 1.3, 2.7h\nTable 5: Training efﬁciency: Time taken by each train-\ning algorithm to reach the best model (identiﬁed using\nvalidation tasks) and corresponding model performance\n(non-compositional setting). Performance and time are\naveraged across k∈{1,5,10,20}shots.\nlayer of the LSTM. We examine the performance of the two architectures when trained using both\nmultitasking and the proposed alternating minimization training algorithm. First, we observe that\nthe transformer generally performs better than the recurrent model. Second, the proposed training\nalgorithm yields consistent improvements over the multitask baseline for the transformer. This shows\nthat the proposed algorithm is general, but particularly effective when used in conjunction with the\ntransformer architecture.\nVisualizing learned task embeddings In Figure 1 we visualize task embeddings learned by the\nnon-compositional TAM model in our gridworld task. We visualize the ﬁrst two principal components\nof task embeddings corresponding to tasks which have the same start position. The projections are\ncolor coded by the horizontal and vertical coordinates of the end position for each task. This shows\nthat the task embeddings have learned the structure of the tasks.\n5.6 Discussion\nOptimizing Task Embeddings We observed that both CA VIA and TAM generally attain better\nperformance when trained with a larger number of inner loop updates. In this work, we use a\nmaximum of 25 inner loop updates for TAM because it strikes a good balance between ﬁnding an\noptimal task embedding and containing training time. CA VIA performed best with 10 inner loop\nupdates, beyond which we hit the computational limitations of our hardware. We also found that\nTAM works better when trained with a number of examples per task much greater than k, in our case\n300. All these empirical ﬁndings suggest that optimizing for the task embedding and replacing the\nsecond order optimization with TAM’s ﬁrst order is an intrinsically difﬁcult problem that requires\nmore iterations and a larger number of examples.\nTraining Efﬁciency We discuss the training efﬁciency of different models in Table 5. The multitask\nbaseline is not expensive to train, but it doesn’t perform well on few-shot scenarios. CA VIA does\nwell especially in the extreme few-shot scenarios, but has stability issues. TAM is simple, easy to\nimplement, performs comparably or better than the baselines and trains more efﬁciently than CA VIA.\nFirst vs. Second Order Gradients Double backprop to optimize the test optimization has become\na standard method of meta-learning. In the appendix of Finn et al. (2017), and in Zintgraf et al. (2019)\n(the “ﬁrst order” variant), similar approaches to TAM were shown to perform relatively worse than\nthe methods with second order gradients. In contrast, in our settings, we have found that ﬁrst order\ngradients (via alternating minimization) are sufﬁcient if done correctly, despite being simpler and\nmore efﬁcient. Although CA VIA (Zintgraf et al., 2019) sometimes outperforms TAM, especially for\nvery small numbers of test examples, TAM is always competitive; with more test examples, TAM is\nusually superior. TAM always outperforms MAML (Finn et al., 2017).\n6 Conclusion\nIn this work we demonstrate a simple and effective approach to adapt transformer models to new tasks\nwith limited data. TAM is trained to adapt to new tasks by inferring a small set of parameters called\nthe task embedding using gradient descent. On synthetic sequence classiﬁcation and transduction\nbenchmarks we constructed TAM yields comparable or superior performance to approaches relying\non second order derivatives, while being computationally more efﬁcient.\n8\nReferences\nAndrychowicz, M., Denil, M., Gomez, S., Hoffman, M. W., Pfau, D., Schaul, T., Shillingford, B., and\nDe Freitas, N. Learning to learn by gradient descent by gradient descent. In Advances in neural\ninformation processing systems, pp. 3981–3989, 2016.\nBa, J., Hinton, G. E., Mnih, V ., Leibo, J. Z., and Ionescu, C. Using fast weights to attend to the recent\npast. In Advances in Neural Information Processing Systems, pp. 4331–4339, 2016.\nBao, Y ., Wu, M., Chang, S., and Barzilay, R. Few-shot text classiﬁcation with distributional signatures.\narXiv preprint arXiv:1908.06039, 2019.\nBarrault, L., Bojar, O., Costa-juss`a, M. R., Federmann, C., Fishel, M., Graham, Y ., Haddow, B., Huck,\nM., Koehn, P., Malmasi, S., Monz, C., M¨uller, M., Pal, S., Post, M., and Zampieri, M. Findings of\nthe 2019 conference on machine translation (WMT19). In Proceedings of the Fourth Conference\non Machine Translation (Volume 2: Shared Task Papers, Day 1), pp. 1–61, Florence, Italy, August\n2019. Association for Computational Linguistics.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A.,\nShyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. arXiv preprint\narXiv:2005.14165, 2020.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nFei-Fei, L., Fergus, R., and Perona, P. One-shot learning of object categories. IEEE transactions on\npattern analysis and machine intelligence, 28(4), 2006.\nFink, M. Object classiﬁcation from a single example utilizing class relevance metrics. In In Advances\nin Neural Information Processing Systems, pp. 449––456, 2005.\nFinn, C., Abbeel, P., and Levine, S. Model-agnostic meta-learning for fast adaptation of deep\nnetworks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,\npp. 1126–1135. JMLR. org, 2017.\nGhiasi, G., Lee, H., Kudlur, M., Dumoulin, V ., and Shlens, J. Exploring the structure of a real-time,\narbitrary neural artistic stylization network. arXiv preprint arXiv:1705.06830, 2017.\nHinton, G. E. and Plaut, D. C. Using fast weights to deblur old memories. In Proceedings of the\nninth annual conference of the Cognitive Science Society, pp. 177–186, 1987.\nHochreiter, S., Younger, A. S., and Conwell, P. R. Learning to learn using gradient descent. In\nInternational Conference on Artiﬁcial Neural Networks, pp. 87–94. Springer, 2001.\nHoulsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan,\nM., and Gelly, S. Parameter-efﬁcient transfer learning for nlp. In International Conference on\nMachine Learning, 2019.\nKeskar, N. S., McCann, B., Varshney, L. R., Xiong, C., and Socher, R. Ctrl: A conditional transformer\nlanguage model for controllable generation. arXiv preprint arXiv:1909.05858, 2019.\nKingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In International Conference\non Learning Representations, 2015.\nKumar, A. and Daume III, H. Learning task grouping and overlap in multi-task learning. In\nInternational Conference on Machine Learning, 2012.\nLake, B. M. Compositional generalization through meta sequence-to-sequence learning. arXiv\npreprint arXiv:1906.05381, 2019.\nLample, G., Subramanian, S., Smith, E., Denoyer, L., Ranzato, M., and Boureau, Y .-L. Multiple-\nattribute text rewriting. In International Conference on Learning Representations, 2019.\nMaurer, A., Pontil, M., and Romera-Paredes, B. Sparse coding for multitask and transfer learning. In\nInternational conference on machine learning, pp. 343–351, 2013.\n9\nMishra, N., Rohaninejad, M., Chen, X., and Abbeel, P. A simple neural attentive meta-learner. In\nInternational Conference on Learning Representations, 2018.\nNichol, A. and Schulman, J. Reptile: a scalable metalearning algorithm. arXiv preprint\narXiv:1803.02999, 2, 2018.\nParisotto, E., Song, F., Rae, J., Pascanu, R., Gulcehre, C., Jayakumar, S., Jaderberg, M., Kaufman,\nR. L., Clark, A., Noury, S., Botvinick, M., Heess, N., and Hadsell, R. Stabilizing transformers for\nreinforcement learning. arXiv:1910.06764, 2019.\nPerez, E., Strub, F., De Vries, H., Dumoulin, V ., and Courville, A. Film: Visual reasoning with a\ngeneral conditioning layer. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018.\nPurushwalkam, S., Nickel, M., Gupta, A., and Ranzato, M. Task-driven modular networks for\nzero-shot compositional learning. arXiv preprint arXiv:1905.05908, 2019.\nRusu, A. A., Rao, D., Sygnowski, J., Vinyals, O., Pascanu, R., Osindero, S., and Hadsell, R.\nMeta-learning with latent embedding optimization. In International Conference on Learning\nRepresentations, 2019.\nSchmidhuber, J. Evolutionary principles in self-referential learning, or on learning how to learn: the\nmeta-meta-... hook. PhD thesis, Technische Universit¨at M¨unchen, 1987.\nSchmidhuber, J. Learning to control fast-weight memories: An alternative to dynamic recurrent\nnetworks. Neural Computation, 4(1):131–139, 1992.\nShen, T., Ott, M., Auli, M., and Ranzato, M. Mixture models for diverse machine translation: Tricks\nof the trade. In International Conference on Machine Learning, 2019.\nSnell, J., Swersky, K., and Zemel, R. Prototypical networks for few-shot learning. In Advances in\nNeural Information Processing Systems, pp. 4077–4087, 2017.\nStickland, A. C. and Murray, I. Bert and pals: Projected attention layers for efﬁcient adaptation in\nmulti-task learning. In International Conference on Machine Learning, 2019.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and\nPolosukhin, I. Attention is all you need. In Advances in neural information processing systems, pp.\n5998–6008, 2017.\nVinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al. Matching networks for one shot learning.\nIn Advances in neural information processing systems, pp. 3630–3638, 2016.\nZellers, R., Holtzman, A., Rashkin, H., Bisk, Y ., Farhadi, A., Roesner, F., and Choi, Y . Defending\nagainst neural fake news. In Neural Information Processing Systems, 2019.\nZintgraf, L. M., Shiarlis, K., Kurin, V ., Hofmann, K., and Whiteson, S. Fast context adaptation via\nmeta-learning. In International Conference on Machine Learning, 2019.\n10\nA Sequence transformations used to construct classiﬁcation and\ntransduction tasks\nIn tables 6, 7 we describe the transformations used to construct classiﬁcation and transduction tasks,\nrespectively.\nTransformation Description\nS1\nmul v Elementwise multiply by v\nadd v Elementwise add v\ndiv v Elementwise integer division by v\nmod v Elementwise modulo voperation\nS2\n(not) multiple of v Extract subset of integers that are (not) multiples of v\n(not) greater of v Extract subset of integers that are (not) greater than v\n(do not) have exactlyvdi-\nvisors\nExtract subset of integers that (do not) have exactly vdivisors\nS3\ncount Sequence length\nmin Smallest integer in sequence\nmax Largest integer in sequence\nmean Mean of sequence elements\nmedian Median of sequence elements\nmode Mode of sequence elements\nﬁrst First element in sequence\nlast Last element in sequence\nmax-min Difference between largest and smallest elements in sequence\nmiddle Element in the middle position of sequence\nTable 6: Sequence transformations used to construct classiﬁcation tasks and their descriptions. Each transfor-\nmation takes a sequence as input and outputs a sequence (transformations in S1 and S2), or a single integer\n(transformations in S1).\nTransformation Description\nS1\nmul v Elementwise multiply by v\nadd v Elementwise add v\ndiv v Elementwise integer division by v\nmod v Elementwise modulo voperation\nS2\nreverse vwith v′ Replace all occurrences of vin the sequence with v′\nreplace xi with f(xi,xj) Replace element xi with one of the following:\n{axi + b, xj, abs (xi −xj), xi + xj}where a,b are integer\nconstants and xi,xj are elements of the sequence at position i,j\nrespectively\nS3\nsort ascending Sort the sequence in ascending order\nsort descending Sort the sequence in descending order\nreverse Reverse the sequence\nswap(xi,xj) Swap elements at positions i,j of the sequence\nshift right v Cyclic shift the sequence right by vpositions\nTable 7: Sequence transformations used to construct transduction tasks and their descriptions. Each transforma-\ntion takes a sequence as input and outputs a sequence.\n11\nB Path-ﬁnding task\nB.1 Non-compositional path-ﬁnding task\nWe present an example task from the path-ﬁnding task below. The grids are 10 ×10. The following\ntask is deﬁned by the start position (7, 0) and end position (1, 4), indicated by the green and red\nsquares, respectively. Each example in the task corresponds to a particular conﬁguration of obstacles\nin the grid. The source sequence represents the locations of obstacles. The obstacles are represented\nby the top left position of a 2 ×2 blob. The target sequence represents the optimal path from source\nto target. Source and target sequences consist of rasterized grid coordinates (Eg. rasterized start and\nend positions are 70 and 14, respectively). In addition, elements of the target sequence have an offset\nof 100 (Eg. rasterized position 14 is represented as 114).\n• Source: [39, 78, 51, 9, 31, 63, 44, 69], Target: [170, 160, 150, 140, 130, 121, 112, 103, 114]\n• Source: [12, 35, 99, 22, 62, 44, 25, 21], Target: [170, 161, 152, 143, 134, 124, 114]\n• Source: [90, 99, 1, 96, 34, 50, 94, 31], Target: [170, 171, 162, 152, 143, 133, 123, 114]\nB.2 Compositional path-ﬁnding task\nIn the compositional setting, we require the optimal path to pass through a way-point, indicated in\nyellow in the following grids. A task is thus deﬁned by a start position, end position and way-point\nposition. The possible values for each of these three parameters represent the primitives in this\ncompositional setting.\n• Source: [63, 38, 90, 93, 73, 68, 18, 67], Target: [126, 115, 124, 133, 142, 131, 122]\n• Source: [95, 60, 95, 70, 23, 34, 83, 85], Target: [126, 115, 104, 113, 122, 131, 142, 131, 122]\n• Source: [91, 29, 57, 96, 8, 53, 77, 13], Target: [126, 125, 134, 133, 142, 131, 122]\n12\nC Compositional TAM\nAlgorithm 2 presents the training algorithm for compositional TAM. We draw a training taskTtrain\nwith primitive ids T1 = i1,T2 = i2,T3 = i3 respectively in line 3. These primitive ids index into the\nprimitive embedding table θe. We pretend that one of the primitives is unknown, and to illustrate\nthe algorithm, we assume without loss of generality that T2 = i2 is unknown (line 5). In the inner\nloop optimization, we infer an embedding zfor this unknown primitive using gradient descent, while\nusing the primitive embedding table to load the known primitive embeddings (θe[i1],θe[i3] in this\ncase (lines 8, 9)).\nAlgorithm 2: Compositional TAM for k-shot Learning\nInput : Training tasks Ttrain\n1 ,..., Ttrain\nN\nOutput :Model parameters θ, primitive embeddings θe\n1 θ′= θ∪θe\n2 repeat\n3 Sample training task Ttrain with primitive ids T1 = i1,T2 = i2,T3 = i3\n4 Sample ktraining examples from the task {(xj,yj)j=1,···,k}∼T train\n5 Pretend one of the primitives (chosen at random) is unknown, say T2\n6 Initialize z= 0,∆θ′= 0\n7 while loss improves and max iterations not reached do\n8 z←z−∇z\n∑k\nj=1 −log p(yj|xj,z1 = θe[i1],z2 = z,z3 = θe[i3]; θ′)\n9 ∆θ′←∆θ′−∇θ′\n∑k\nj=1 −log p(yj|xj,z1 = θe[i1],z2 = z,z3 = θe[i3]; θ′)\n10 θ′←θ′+ ∆θ′\n11 until max training iterations;\nD Model Architecture\nFigure 2 shows an illustration of how we use transformers for sequence classiﬁcation (left) and\nsequence transduction (right) problems. In the classiﬁcation setting the input is a sequence(x1 ···xn)\nand the output is a discrete label y. In the transduction setting, the input (x1 ···xn) and output\n(y1 ···ym) are sequences. zis an embedding vector we refer to as the task embedding and appears in\nthe input to the transformer, in addition to the input sequence. The task embedding zis task speciﬁc,\nand is inferred on the ﬂy for each task during training. Learning a new task T at test time involves\ninferring the corresponding task embedding zT, leaving the rest of the model parameters untouched.\nTransformer\nz x1 x2 x3\ny\nTransformer\nx1 x2 z y1\ny1 y2\nFigure 2: Illustration of how we use transformers for sequence classiﬁcation (left) and sequence transduction\n(right) problems.\n13",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7943519353866577
    },
    {
      "name": "Security token",
      "score": 0.7634592652320862
    },
    {
      "name": "Transformer",
      "score": 0.6567496061325073
    },
    {
      "name": "Adapter (computing)",
      "score": 0.5643739700317383
    },
    {
      "name": "Artificial intelligence",
      "score": 0.558363676071167
    },
    {
      "name": "Embedding",
      "score": 0.5315952897071838
    },
    {
      "name": "Multi-task learning",
      "score": 0.4767014682292938
    },
    {
      "name": "Generalization",
      "score": 0.45437759160995483
    },
    {
      "name": "Sequence (biology)",
      "score": 0.44072195887565613
    },
    {
      "name": "Machine learning",
      "score": 0.4099341034889221
    },
    {
      "name": "Task (project management)",
      "score": 0.40183794498443604
    },
    {
      "name": "Theoretical computer science",
      "score": 0.34546661376953125
    },
    {
      "name": "Mathematics",
      "score": 0.08137694001197815
    },
    {
      "name": "Engineering",
      "score": 0.07411524653434753
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    }
  ]
}