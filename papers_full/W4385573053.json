{
    "title": "AraDepSu: Detecting Depression and Suicidal Ideation in Arabic Tweets Using Transformers",
    "url": "https://openalex.org/W4385573053",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2056302466",
            "name": "Mariam Hassib",
            "affiliations": [
                "Alexandria University"
            ]
        },
        {
            "id": "https://openalex.org/A5092596441",
            "name": "Nancy Hossam",
            "affiliations": [
                "Alexandria University"
            ]
        },
        {
            "id": "https://openalex.org/A5092596442",
            "name": "Jolie Sameh",
            "affiliations": [
                "Alexandria University"
            ]
        },
        {
            "id": "https://openalex.org/A2308215818",
            "name": "Marwan Torki",
            "affiliations": [
                "Alexandria University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3008110149",
        "https://openalex.org/W2471147443",
        "https://openalex.org/W3156584161",
        "https://openalex.org/W2250553926",
        "https://openalex.org/W2894059694",
        "https://openalex.org/W3106433641",
        "https://openalex.org/W3116641301",
        "https://openalex.org/W4214710927",
        "https://openalex.org/W3133440961",
        "https://openalex.org/W2957537811",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W2767566483",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2623212788",
        "https://openalex.org/W3101860695",
        "https://openalex.org/W3215748564",
        "https://openalex.org/W2106686523",
        "https://openalex.org/W2250594687",
        "https://openalex.org/W2531569042",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W4297823766",
        "https://openalex.org/W4200425003",
        "https://openalex.org/W3176169354",
        "https://openalex.org/W2901770030",
        "https://openalex.org/W3153642904",
        "https://openalex.org/W2402700"
    ],
    "abstract": "Among mental health diseases, depression is one of the most severe, as it often leads to suicide which is the fourth leading cause of death in the Middle East. In the Middle East, Egypt has the highest percentage of suicidal deaths; due to this, it is important to identify depression and suicidal ideation. In Arabic culture, there is a lack of awareness regarding the importance of diagnosing and living with mental health diseases. However, as noted for the last couple years people all over the world, including Arab citizens, tend to express their feelings openly on social media. Twitter is the most popular platform designed to enable the expression of emotions through short texts, pictures, or videos. This paper aims to predict depression and depression with suicidal ideation. Due to the tendency of people to treat social media as their personal diaries and share their deepest thoughts on social media platforms. Social data contain valuable information that can be used to identify user's psychological states. We create AraDepSu dataset by scrapping tweets from twitter and manually labelling them. We expand the diversity of user tweets, by adding a neutral label (\"neutral\") so the dataset include three classes (\"depressed\", \"suicidal\", \"neutral\"). Then we train our AraDepSu dataset on 30+ different transformer models. We find that the best-performing model is MARBERT with accuracy, precision, recall and F1-Score values of 91.20%, 88.74%, 88.50% and 88.75%.",
    "full_text": "Proceedings of the The Seventh Arabic Natural Language Processing Workshop (WANLP), pages 302 - 311\nDecember 8, 2022 ©2022 Association for Computational Linguistics\nAraDepSu: Detecting Depression and Suicidal Ideation in Arabic Tweets\nUsing Transformers\nMariam Hassib Nancy Hossam Jolie Sameh Marwan Torki\nFaculty of Engineering, Alexandria University, Egypt\n{mariamhassib1990,nancyhossam441999,joliesameh99}@gmail.com\nmtorki@alexu.edu.eg\nAbstract\nAmong mental health diseases, depression is\none of the most severe, as it often leads to\nsuicide which is the fourth leading cause of\ndeath in the Middle East. In the Middle East,\nEgypt has the highest percentage of suicidal\ndeaths; due to this, it is important to identify\ndepression and suicidal ideation. 1 In Arabic\nculture, there is a lack of awareness regard-\ning the importance of diagnosing and living\nwith mental health diseases. However, as noted\nfor the last couple of years people all over the\nworld, including Arab citizens, tend to express\ntheir feelings openly on social media. Twitter\nis the most popular platform designed to en-\nable the expression of emotions through short\ntexts, pictures, or videos. This paper aims to\npredict depression and depression with suici-\ndal ideation. Due to the tendency of people to\ntreat social media as their personal diaries and\nshare their deepest thoughts on social media\nplatforms. Social media data contains valuable\ninformation that can be used to identify users’\npsychological states. We create the AraDepSu\ndataset by scrapping tweets from Twitter and\nmanually labeling them. We expand the di-\nversity of user tweets, by adding a neutral la-\nbel (“neutral”) so the dataset includes three\nclasses (“depressed”, “suicidal”, and “neutral”).\nThen we train our AraDepSu dataset on 30+\ndifferent Transformer-based models. We find\nthat the best-performing model is MARBERT\nwith accuracy, macro-average precision, macro-\naverage recall, and macro-average F1-score val-\nues of 91.20%, 88.74%, 88.50%, and 88.75%.\n1 Introduction\nThe well-being of a person comprises physical\nhealth and mental health. The mental health of a\nperson shows the individual’s state of mind. Mental\ndisorders are a worldwide health problem affecting\na large number of people and causing numerous\ndeaths every year (Musleh et al., 2022).\n1Suicide: The Fourth Cause of Death Among Young Peo-\nple. URL: https://www.bbc.com/arabic/59568886.\nDepression is one of the most well-known men-\ntal health disorders and it is considered a major\nissue for mental health practitioners. Depression\nis a mood disorder that causes a persistent feeling\nof sadness and loss of interest. Also called a major\ndepressive disorder or clinical depression. It affects\nhow you feel, think and behave and can lead to a\nvariety of emotional and physical problems. Fortu-\nnately, it is also treatable especially if we identify\nit in the early stage.2 In Arabic culture, early diag-\nnosis of mental illness is difficult, because of the\nstigma of mental illness and lack of awareness in\nthe field of psychiatry.3 Depression has become a\nsilent killer as it increases suicide risk. 4\nPeople tend to express their feelings openly on\nsocial media, especially on Twitter. Twitter pro-\nvides a platform where users share their thoughts,\nemotions, feelings, and expressions. These tweets\ncan aid in determining a person’s thought process,\nmental health, and behavioral traits.\nIn this paper, our objective is to come up with\na methodology to accurately classify and analyze\nArabic tweets. We classify whether they are suf-\nfering from depression or depression with suicidal\nideation which can help prevent suicidal deaths.\nWe focus on the potential of Natural language pro-\ncessing (NLP) and machine learning techniques\nthat can be utilized in the mental health field. NLP\nis very helpful when it comes to understanding the\ncontext of natural human language. As a result, it\nextracts latent meaning from text and creates AI-\nbased solutions using text data available on social\n2What is Depression? URL: https://psychiatry.org/\npatients-families/depression/what-is-depression .\n3Egypt: Mental health barriers URL:\nhttps://english.ahram.org.eg/NewsContent/\n50/1209/422608/AlAhram-Weekly/Focus/\nEgypt-Mental-health-barriers.aspx .\n4Mental Health and Substance Abuse: Does Depression\nIncrease The Risk For Suicide? URL: https://www.hhs.\ngov/answers/mental-health-and-substance-abuse/\ndoes-depression-increase-risk-of-suicide/index.\nhtml.\n302\nmedia platforms.\n2 Background\nDepression is considered a global concern. It is a\nvery common illness, as it affects people across all\nnations. Approximately 280 million people have re-\ncently been afflicted with depression in the world.5\nDepression can cause the affected person to suffer\ngreatly and function poorly at work, at school, and\nin the family. At its worst, depression can lead to\nsuicide. Over 700,000 people die due to suicide\nevery year.\nDepression is different from usual mood fluctua-\ntions and short-lived emotional responses to chal-\nlenges in everyday life.6 It comes in many forms,\neach accompanied by its own symptoms. The most\ncommon and known form is major depressive dis-\norder (MDD), which influences the ability of indi-\nviduals to do daily tasks (Aldarwish and Ahmad,\n2017). Depression does not have a target age, as\nit may begin at a young age. Curbing depression\nis essential to saving people’s lives (Marcus et al.,\n2012).\nIn Arabic culture, the stigma on mental illness is\ndeeply entrenched, and there is a lack of awareness\nregarding this issue. The review reveals that be-\nyond society and culture, the persistence of mental\nillness stigma. In the Arab world may be explained\nby inefficient monitoring mechanisms of mental\nhealth legislation and policies within the healthcare\nsetting (Merhej, 2019).\n3 Related Work\nThis section presents a summary of prior studies\nthat have been conducted on the prediction and\nmonitoring of depression and suicide using social\nmedia.\nVarious related data have been in the literature\nfor the prediction of depression via various ap-\nproaches. Two main strategies were reported in\nthe literature to collect data and detect depression\nthrough social media.\nThe first strategy is crowd-sourcing data collec-\ntion from social media publicly available. This\nallows researchers to cheaply outsource simple\n5Institute of Health Metrics and Evaluation. Global Health\nData Exchange. URL: http://ghdx.healthdata.org/\ngbd-results-tool?params=gbd-api-2019-permalink/\nd780dffbe8a381b25e1416884959e88b.\n6World Health Organization: Depression. URL:\nhttps://www.who.int/news-room/fact-sheets/\ndetail/depression.\ntasks or questionnaires, and gather data in real-\ntime. It also helps to obtain far more numerous and\nwidespread observations than in traditional data\ncollection given its relatively low cost. The crowd-\nsourcing strategy is mainly conducted in two stages\n(De Choudhury et al., 2013a,b). First, responses\nfrom an online clinical depression survey are gath-\nered. Then, contents are collected by accessing\nthe Twitter data of the consented participants. This\nmain strategy limitation is time-consuming.\nThe second alternative strategy is characterized\nby gathering data quickly and cheaply (Copper-\nsmith et al., 2014). As such, the data is collected di-\nrectly from social media that are publicly available\nfor participants with self-identified mental illnesses.\nThe disadvantage of this strategy is its low reliabil-\nity. Unfortunately, very few of these collected data\nand applied models were found in Arabic.\nConducting a sentiment analysis of texts in the\nArabic language is more complex than that directed\ntoward English texts. That is because the Arabic\nlanguage is characterized by more forms than other\nlanguages. The formal variant of Arabic is Modern\nStandard Arabic (MSA), but this is rarely used in\nspoken interactions. The most frequently used in-\nformal variant is Dialectal Arabic (DA), especially\nfor communication purposes. A total of 30 major\nArabic dialects differ from MSA, the approaches\nused to translate difficult MSA terms are ineffective\nwhen applied to DA translation. Recently, Arabic\nresearchers have developed solutions for different\ndialects, but these remain minimally inaccurate and\ncover only a few dialects (Al-Twairesh et al., 2017).\nOur work mainly focuses on Egyptian Dialectal\nwith it the most studied and widely spoken DA.\nA common challenge that faces most depression\ndetection trials is to identify the symptoms of men-\ntal illness in online health communities. This is\ndue to the symptom overlapping between multiple\nmental illnesses. To the best of our knowledge, no\nprevious trials went deeply into the Arabic Twit-\nter data for detecting whether a user’s tweet is de-\npressed or depressed with suicidal ideation. To fill\nthis literature gap, our solution helps in detecting\nsigns relevant to depression using Arabic language\ntweets. To avoid the limitations related to data\ncollection we faced in the beginning, we combine\nlow-cost and reliable data collection strategies. We\ncollected more than 20k tweets data from public\ntweets and labeled them manually.\n303\nDataset Number of examples\nArTwitter (Abdulla et al., 2013) 3,543\nTEAD (Abdellaoui and Zrigui, 2018) 2,000\nBRAD (Elnagar et al., 2018) 2,000\nASTD (Nabil et al., 2015) 1,590\nTotal 9,133\nTable 1: Number of examples from different datasets.\nFigure 1: Word clouds for different classes in the\nAraDepSu dataset. Top: Depression words. Mid-\ndle: Suicidal ideation words. words. Bottom: Non-\ndepression words\n4 Data\n4.1 Data Collection\nWe extracted more than 10k tweets from differ-\nent users with special keywords to get tweets with\ndepression and suicidal ideation, posted between\n2016 and 2022. We added to our dataset 1,230\nrecords from the available data of the Modern Stan-\ndard Arabic mood changing and depression dataset\n(Maghraby and Ali, 2022). Table 2 shows some of\nthe depression keywords and Table 3 shows some\nof the depression with suicidal ideation keywords.\nTweets in this study were a mixture of Modern\nStandard Arabic and Arabic dialects. Similar to the\nprevious work on depression detection on English\ndatasets (Babu and Kanaga, 2022), we collected\ndata from different sentiment analysis datasets as\nshown in Table 1.\n4.2 Cleaning and Pre-Processing Data\nOur dataset annotation procedure includes two\nphases. In the first phase, we sanitized each tweet\nso that they do not contain irrelevant text, so they\nwould be suitable input for our various models.\nFirst, we removed hyperlinks because they do not\nadd much to the actual content of the tweet. Then,\nwe removed empty columns and duplicate records.\n4.3 Manually Labeling Process\nIn the second phase, each record is labeled by one\ncategory name, whether it is depression, depres-\nsion with suicidal ideation, or non-depression. The\nannotators followed the authors’ instructions in la-\nbeling the data. Each record was labeled by a single\nannotator. Then, the authors revised the annotated\ndata sample by sample. In case of disagreement,\nthe authors’ decision is favored. Finally, we ob-\ntained a dataset with 20,213 tweets 5,472 classified\nwith depression, 2,167 with suicidal ideation, and\n12,574 as non-depression as shown in Table 4\nIn Figure 1 we show the word clouds for the dif-\nferent classes in AraDepSu dataset. The keywords\nfor the depression class are highlighted in the de-\npression words such as “life is hard”, “I want to\ncry\" and similar keywords. We observe the same\nkind of keywords for the suicidal ideation class\nsuch as “ kill my self”, “I wan to die” and sim-\nilar keywords. For the non-depression class, the\nhighlighted keywords are not relevant to a specific\n304\nKeyword Example\n/char10/char49/char4a/char2e/charaa/char10/char4b /char10/chare9/char6b/char2e/char41/char67 /charc9/charbf/char09/chare1/chard3 /charf8/char0a/char51/char09/char6b/char40/char10/char49/char4a/char2e/char6b/char2e/charf0/char10/char49/char10/charae/chareb/char09/char50 /charf0/char10/char49/char4a/char2e/charaa/char10/char4b\nExhausted I am exhausted and fed up with everything.\n/char49/char2e/char0d/char1c/char10/char4a/charba/chard3 /char81/char1d/char2e/charfa/char0a/charce/chareb/char40/char09/chare0/char41/char11/char82/charab/charfa/char0a/charce/char67/char2e/char50/charfa/charce/charab/char09/charad/char10/charaf/char40/charf0 /charc9/char09/char92/char09/charaf/char40 /charc8/charf0/char41/char6d/char1a/char27/char2e/charf0 /char58/char41/char67 /char48/char2e/char41/char0d/char4a/char10/char4a/charbb/char0d/char40 /char49/char2e/char0d/char1c/char10/char4a/charba/chard3 /char41/char09/char4b/char40\nDepressed I am severely depressed, just trying to withstand it for my family’s sake\n/char10/chare8/char50/char41/charee/char09/char44/chard3 /chara1/char4a/char0a/charaa/char4b/char2e/charf0/char10/chare8/char50/char41/charee/char09/char44/chard3/char10/chare9/char10/charae/char4a/char0a/char10/charae/char6d/charcc/char27/char40 /charfa/char0a\n/char09/charaf /char41/char09/char4b/char0d/char40 /char41/chard2/char09/char4a/char1c/char0a/char4b/char2e/char10/chare9/char10/char4a/char4b/char2e/char41/char11/char4b/charf0/char10/chare9/char4b/char0a/charf1/char10/charaf /charfa/char0a\n/char09/char47/char40/char0d/chard5/charba/charcb /charc9/char4a/char0a/char09/char6d/char1a/char0c/char27/char0a/char59/char10/charaf\nBroken You might think I’m strong and steady when in fact I’m broken and weeping.\n/char10/chare9/char09/char4a/char4b/char0a/char09/char51/char6b /chard0/char41/char09/char4b/char40/charf0 /chare8/char58 /charc9/charbf/char09/chare1/chard3 /char48/char2e/char51/chareb/char40 /chare9/char09/charaf/char50/char41/charab/char11/char81/chard3 /charfa/char0a/char10/chare6/char6b /chare9/char3c/charcb/char40/charf0 /charf8/char0a/charf0/char0d/char40/char10/chare8/char50/charf1/charea/char10/charae/chard3/charf0/char10/chare9/char09/char4a/char4b/char0a/char09/char51/char6b /char41/char09/char4b/char0d/char40\nMiserable I’m so miserable and defeated,I do not even know how to escape from all this and just sleep\n/char48/char2e/char41/char0d/char4a/char10/char4a/charbb /char40 /char58/char41/char67 /char48/char2e/char41/char0d/char4a/char10/char4a/charbb /char40 /charfa/char0a/char09/chare6/char4a/char0a/char09/charaf\nDepression I have severe depression\n/charfa/char0a/char09/chare6/char4a/char2e/char6a/char1c/char0a/char4b/char2e/char11/char80/char59/char6d/chard7 /char3f /char59/char67 /chara9/chard3/char10/char86/char51/char09/charae/char4b/char2e/char42/charf0 /chare9/char4a/char2e/char6d/char1a/char27/char2e/char41/chard3 /charf8/char0a/char09/char50 /charfa/char0a/char09/chare6/char4a/char2e/char6a/char1c/char0a/char4b/char2e/char11/char80/char59/char6d/chard7/chare9/char4a/char0a/charcb /char41/char09/char4b/char40 /char81/char1d/char2e/char09/charac/char51/charab/char40/char10/chare8/char09/char51/char4b/char0a/char41/charab/char10/char49/char09/char4a/charbb\nNo one loves me I just want to know, why no one loves me as much as I do and I do not matter to anyone?\n/chara1/char4a/char0a/charab/char40 /chare8/char09/char51/char4b/char0a/char41/charab /chard0/char41/char09/char4b/char40 /charf0/char40 /chara1/char4a/char0a/charab/char40 /chare8/char09/char51/char4b/char0a/char41/charab /char41/char09/char4b/char40\nWant to Cry I want to cry or sleep\nTable 2: Examples for depression from the annotated corpus.\nKeyword Example\n/char51/char6a/char10/char4a/char09/char4b/char40/char09/char51/char4b/char0a/char41/charab /char91/charca/char09/char67/char40 /charf0 /charfa/char0a/chare6/char84/char09/charae/char09/char4b/char10/char48/charf1/chard3/char40 /charf0 /char51/char6a/char10/char4a/char09/char4b/char40/char09/char51/char4b/char0a/char41/charab\nI want to commit suicide I want to commit suicide and just end my life\n/charfa/char0a/chare6/char84/char09/charae/char09/char4b /charc9/char10/char4a/char10/charaf/char40 /char11/char49/charcb/char41/char11/char4b /char50/char41/char4a/char0a/char09/char6b /chare9/char4a/char0a/char09/charaf/char41/chard3 /charfa/char0a/chare6/char84/char09/charae/char09/char4b /charc9/char10/char4a/char10/charaf/char40 /charf0/char40 /charfa/char0a/chare6/char84/char09/charae/char09/char4b /charc9/char10/char4a/char10/charaf/char40\nkill myself It is either killing myself or killing myself there is no other option.\n/char51/char6a/char10/char4a/char09/char4b/char40 /charf8/char0a/char58/charf0 /chare9/char3c/charcb/char40/charf0/char10/char49/char4a/char2e/charaa/char10/char4b /chare9/char6b/char40/char51/chare5/char95 /char51/char6a/char10/char4a/char09/char4b/char40 /charf8/char0a/char58/charf0\nI want to commit suicide I want to commit suicide, honestly I’m tired, I swear\n/char10/char48/charf1/chard3/char40/char09/char51/char4b/char0a/char41/charab /char10/char48/charf1/chard3/char40/char09/char51/char4b/char0a/char41/charab /char51/chare5/char09/char95/char41/char67 /char42/charf0 /charfa/char0a/chare6/char09/char95/char41/chard3 /char42\nI want to die No past no future, I want to die.\n/char11/char81/char1c/char0a/charab/char40/char10/chare8/char09/char51/char4b/char0a/char41/charab/char11/char81/chard3 /char10/chare8/char59/charbb/char10/chare9/char4b/char0a/char41/char09/charae/charbb/char11/char81/char1c/char0a/charab/char40/char10/chare8/char09/char51/char4b/char0a/char41/charab/char11/char81/chard3 /char41/char09/char4b/char0d/char40 /char48/char2e/char50 /char41/char4b/char0a/charfa/char0a/char09/chare6/char10/char4b/charf1/chard3\nI don’t want to live Just kill me God, I do not want to live anymore that is enough.\n/char51/char6a/char10/char4a/char09/char4b/char40 /char51/charc2/char09/charae/char4b/char2e /char51/char6a/char10/char4a/char09/char4b/char40 /char51/charc2/char09/charae/char4b/char2e/charf0 /chara1/char4a/char0a/charaa/char4b/char2e/char10/chare8/char59/charab/char41/char10/charaf\nThinking about committing suicide Crying and thinking about committing suicide\n/charfa/char0a\n/char09/char47/char59/char09/char67 /char48/char2e/char50/char41/char4b/char0a /char10/char49/char10/charaf/charf0 /char48/char2e/char51/char10/charaf/char40 /charfa/char0a\n/char09/charaf /charf8/char0a/char58/char10/chare9/charca/char4a/char0a/charaa/charcb/char40/char09/chare1/chard3 /charfa/char0a\n/char09/char47/char59/char09/char67 /char48/char2e/char50/char41/char4b/char0a\nJust take me God Just take me God from this family as soon as possible\nTable 3: Examples for depression with suicidal ideation from the annotated corpus.\ntopic.\n5 Experiments and Results\n5.1 Dataset\nThe final dataset consists of 20,213 tweets divided\ninto 15,159 training tweets and 5,054 testing tweets.\nTable 4 provides the statistical details of the dataset.\n5.2 Models\nIn our experiments, we use the following models:\n5.2.1 mBERT\nMultilingual BERT model (Devlin et al., 2018), is\na single language model pre-trained from monolin-\ngual corpora on data from the Wikipedia dumps of\n104 languages.\n5.2.2 GigaBERT\nGigaBERT is a customized bilingual BERT for En-\nglish and Arabic. We use two variants of this model,\nGigaBERT-v3 and GigaBERT-v4. GigaBERT-v3\nis a customized bilingual BERT for English and\nArabic. It is pre-trained on a large-scale corpus\nwith 10B tokens. GigaBERT-v4 is a continued pre-\ntraining of GigaBERT-v3 on code-switched data\n(Lan et al., 2020).\n5.2.3 XLM-RoBERTa\nXLM-RoBERTa is an Unsupervised Cross-lingual\nRepresentation Learning at Scale (Conneau et al.,\n2019). This model is pre-trained on 2.5TB of fil-\ntered data containing 100 languages. We use two\nvariants of this model, XLM-RoBERTa-base, and\n305\nSet Non-depression Depression Depression With Suicidal Ideation Total\nTraining 9,408 4,117 1,634 15,159\nTesting 3,166 1,355 533 5,054\nTotal 12,574 5,472 2,167 20,213\nTable 4: Distribution of depression and depression with suicidal ideation.\nModel Precision Recall F1-Score Accuracy\nmBERT 84.25 87.04 85.55 87.93\nGigaBERT(v3) 86.21 87.44 86.80 88.90\nGigaBERT(v4) 87.05 87.59 87.32 89.35\nXLM-RoBERTa-base 86.32 87.31 86.79 89.06\nXLM-RoBERTa-large 85.95 87.28 86.59 88.88\nAraBERT-base(v01) 86.23 86.39 86.30 88.66\nAraBERT-base(v1) 85.78 86.78 86.27 88.29\nAraBERT-base(v02) 87.42 87.75 87.58 89.73\nAraBERT-base(v02)-twitter 87.02 88.66 87.81 89.73\nAraBERT-base(v2) 86.36 86.15 86.25 88.68\nAraBERT-large(v02)-twitter 87.48 88.33 87.90 89.93\nAraELECTRA(discriminator) 86.36 87.97 87.14 89.24\nAraELECTRA(generator) 82.82 87.27 84.78 87.34\nArabic BERT-base 86.05 86.78 86.41 88.52\nArabic BERT-mini 83.80 86.23 84.94 87.67\nArabic BERT-medium 84.97 84.82 84.89 87.57\nArabic BERT-large 86.64 86.91 86.76 88.74\nArabic ALBERT-base 85.86 86.38 86.12 88.43\nArabic ALBERT-large 86.43 86.01 86.20 88.48\nArabic ALBERT-xlarge 86.82 85.62 86.21 88.70\nMARBERT 88.74 88.50 88.75 91.20\nMARBERT(v2) 87.75 88.50 88.12 90.07\nARBERT 86.42 86.21 86.31 88.60\nQARiB 88.20 88.26 88.23 90.13\nAraGPT2-base 83.34 85.70 84.45 86.94\nAraGPT2-medium 81.08 83.57 82.31 83.83\nAraGPT2-large 83.97 84.60 84.26 84.67\nAraT5-base 84.44 88.68 86.35 88.70\nAraT5-msa-base 82.74 88.66 85.26 87.73\nAraT5-tweet-base 86.06 88.93 87.40 89.65\nAraT5-msa-small 74.90 82.77 77.74 81.58\nAraT5-tweet-small 80.47 85.95 82.12 85.26\nTable 5: Performance comparison of different models on our dataset.\nXLM-RoBERTa-large.\n5.2.4 AraBERT\nAraBERT is an Arabic pretrained language model\nbased on Google’s BERT architecture and uses the\nsame BERT-Base config (Antoun et al.). There\nare many versions of the model. AraBERTv0.1\nand AraBERTv1, with the difference being that\nAraBERTv1 uses Farasa Segmenter (Durrani and\nMubarak). AraBERT(v01/1) was trained on 23GB\nof text while AraBERT(v02/2) was trained on\n77GB of text. AraBERTv0.2-Twitter-base/large\nare two new models for Arabic dialects and tweets.\n306\nThey are trained on 60M Arabic tweets with emo-\njis in their vocabulary in addition to common\nwords that were not present at earlier versions.\nWe use many variants of this model, AraBERT-\nbase(v01/1/02/2) and AraBERT-base/large(v02)-\ntwitter.\n5.2.5 AraELECTRA\nELECTRA is a method for self-supervised lan-\nguage representation learning. AraELECTRA\nwas trained on the same 77GB of text used for\nAraBERT (Antoun et al., 2021a). We use two vari-\nants of this model, AraELECTRA generator and\nAraELECTRA discriminator.\n5.2.6 Arabic BERT\nArabic BERT Base model was pretrained on 8.2\nBillion words of the Arabic version of OSCAR\n(Suárez et al., 2020) filtered from Common Crawl\nand a recent dump of Arabic Wikipedia and other\nArabic resources which sum up to 95GB of text\n(Safaya et al., 2020). We use four variants of this\nmodel, Arabic BERT-base/mini/medium/large.\n5.2.7 Arabic ALBERT\nAn Arabic edition of ALBERT model which was\npretrained on 4.4 Billion words from the Arabic\nversion of the unshuffled OSCAR corpus (Suárez\net al., 2020) and the Arabic Wikipedia (Safaya,\n2020). We use three variants of this model, Arabic\nALBERT-base/large/xlarge.\n5.2.8 ARBERT and MARBERT\nARBERT and MARBERT are based on the BERT-\nbase architecture. ARBERT is a language model\nthat is focused on Modern Standard Arabic (MSA)\nand was trained on 61GB of text from news articles.\nMARBERT is a language model that is focused\non both Dialectal Arabic (DA) and MSA. MAR-\nBERT was trained on randomly sampled 1B Ara-\nbic tweets from a dataset of about 6B tweets, the\ndataset makes up 128GB of text. (Abdul-Mageed\net al., 2021). MARBERTv2 was further trained on\nthe same data as ARBERT in addition to AraNews\ndataset (Ali et al., 2021).\n5.2.9 QARiB\nQARiB is a QCRI Arabic and Dialectal BERT\nmodel, which was trained on 420 Million tweets\nand 180 Million sentences of text (Abdelali et al.,\n2021).\n5.2.10 AraGPT2\nAraGPT2 is an advanced Arabic language genera-\ntion model, trained from scratch on a large Arabic\ncorpus of internet text and news articles (Antoun\net al., 2021b). We use three variants of this model,\nAraGPT2-base/medium/large.\n5.2.11 AraT5\nAraT5 Text-to-Text Transformers for Arabic Lan-\nguage Generation that is focused on both Dialectal\nArabic (DA) and MSA. AraT5-MSA was trained\non 70GB of text. AraT5-Tweet was trained on\n178GB of text (Nagoudi et al., 2022).\n5.3 Hyper-parameters Setting and Evaluation\nIn our experiments, we use the implementation pro-\nvided by HuggingFace Transformers library (Wolf\net al., 2019). We train our models for 5 epochs\nwith a learning rate of 2e–5 and a maximum se-\nquence length set to 128 tokens. Table 5 shows the\nresults of different models on our dataset. The best-\nperforming model is MARBERT with a macro-\naverage F1-score of 88.75%.\n6 Discussion\nModels pre-trained on multiple languages\nTable 6 compares the models pre-trained on mul-\ntiple languages. GiagBERT outperforms mBERT\nand XLM-RoBERTa on AraDepSu dataset. We\nthink the reason is that AraDepSu contains Arabic\ndialectic tweets and GiagBERT is trained only on\nEnglish and Arabic data.\nModels pre-trained on tweets\nTable 7 compares the models pre-trained on tweets\nwith the nWords of the pre-trained dataset and the\nf1-score results. MARBERT outperforms AraT5\neven though it is trained on more data. We think the\nreason is that the majority of AraT5-tweet data is\nMSA according to the analyses done by (Nagoudi\net al., 2022), and the majority of our dataset is from\ndialect tweets.\nModels pre-trained on Modern Standard Ara-\nbic\nTable 8 compares the models pre-trained on MSA\nwith the size of the pre-trained dataset. AraBERT-\nbase(v02) outperforms models pre-trained on larger\ndatasets. In these models, the performance relies\nmore on the architecture than on the dataset size.\nQualitative EvaluationAs shown in the study,\npre-trained models produced reliable results and\naccuracy. However, there were some drastic dif-\nferences in their training circumstances. As stated\n307\nModel Pre-trained languages F1-Score\nmBERT 104 85.55\nGigaBERT(v3) En-Ar 86.80\nGigaBERT(v4) En-Ar 87.32\nXLM-RoBERTa-base 100 86.79\nXLM-RoBERTa-large 100 86.59\nTable 6: Comparison of different models pre-trained on multiple languages.\nModel Pre-trained tweets F1-Score\nAraBERT-base(v02)-twitter 60M 87.81\nAraBERT-large(v02)-twitter 60M 87.90\nMARBERT 1B 88.75\nQARiB 420M 88.23\nAraT5-tweet-base 1.5B 87.40\nAraT5-tweet-small 1.5B 82.12\nTable 7: Comparison of different models pre-trained on tweets.\nModel DataSet Size F1-Score\nAraBERT-base(v01) 23GB 86.30\nAraBERT-base(v1) 23GB 86.27\nAraBERT-base(v02) 77GB 87.58\nAraBERT-base(v2) 77GB 86.25\nAraELECTRA(discriminator) 77GB 87.14\nAraELECTRA(generator) 77GB 84.78\nArabic BERT-base 95GB 86.41\nArabic BERT-mini 95GB 84.94\nArabic BERT-medium 95GB 84.89\nArabic BERT-large 95GB 86.76\nArabic ALBERT-base 35GB 86.12\nArabic ALBERT-large 35GB 86.20\nArabic ALBERT-xlarge 35GB 86.21\nAraGPT2-base 77GB 84.45\nAraGPT2-medium 77GB 82.31\nAraGPT2-large 77GB 84.26\nTable 8: Comparison of different models pre-trained on MSA.\npreviously, the core difference is that MARBERT\nfocuses on Dialectic data in its training, while\nAraBERT focuses on Modern Standard Arabic\n(MSA) data. Since AraDepSu dataset is mainly\ncomposed of scraped tweets, there were many dif-\nferent dialects. This justifies why MARBERT pro-\nduced the best accuracy and is considered the best\nmodel for this study.\nWe show in Table 9 the predictions of MAR-\nBERT and AraBERT-base(v02) on some test tweets.\nWe observe that MARBERT excels with different\ndialects and tricky tweets. Those tricky tweets may\naddress depression or suicidal depression in gen-\neral, but can not be used as evidence that the user\nis depressed, or define their current state. This\nmay result in a conflict between the prediction and\nthe ground truth. The main reason for this error\nwas believed to be that the pattern the model was\nsearching for to label the string, as depression, for\nexample, was found successfully but the human\n308\nSentence Ground Truth Prediction Pre-trained Model\nData\n/chare9/char3c/charcb/char40/charf0 /charf8/char0a/char59/char67/charf1/charcb/char09/charad/char4a/char0a/char92/charcb/char40 /char49/char2e/char6d/char1a/char27/char2e/char41/char09/char4b/char40/char09/chare0/char40 /char81/char83/char41/char67Non-depression Non-depression Dialectic MARBERT\nI feel that I love summer alone, I swear to God. Depression MSA AraBERT-base(v02)\n/chare9/char4b/char0a/char40/char10/chare8/char09/char51/char4b/char0a/char41/charab /char41/char09/char4b/char40/char10/chare9/char09/charaf/char50/char41/charab/char11/char81/chard3/char10/char48/charf1/chard3/char40 /char41/chard3/char10/chare9/char4b/char0a/char41/char09/charaa/charcb /charc9/char09/char92/char09/charae/chareb /char40/char59/charbb/charfa/char0a/charce/charbe/char11/char83 Non-depression Depression Dialectic MARBERT\nIt looks like I will not know what I want until I die. Depression MSA AraBERT-base(v02)\n/chare9/char09/char4a/char4b/char0a/char09/char51/char6b /char50/char41/charee/char45/char2e/char51/char1e/char0a/char09/charab/char09/charac/charf1/char11/char82/char09/char1c/char4a/char2e/chard3/char10/char48/charf1/chard2/char10/char4a/char4b/char2e/charfa/charce/char4a/char0a/charcb /chare9/char4b/char0a/char41/charee/char09/char44/charcb/char40 /char40/char51/char10/char4b Non-depression Non-depression Dialectic MARBERT\nAt the end Layla will die and you will only see Bahar sad. Depression MSA AraBERT-base(v02)\n/char69/char2e/charab/char09/char51/chard3 /char5a/charfa/char0a/chare6/char11/char84/charcb/char41/chareb/charf0 /char41/char4a/char0a/char82/char09/charae/char09/char4b /chare8/char51/chard3/char59/chard3 /char80/char41/char09/char4b /char48/char2e/char09/char59/char67/char2e/char40 /char41/char09/char4b/char40 Depression Depression Dialectic MARBERT\nI attract psychologically destructive people and this is annoying. Non-depression MSA AraBERT-base(v02)\n/char10/char49/char11/char82/char09/charae/chara3 /char90/char43/char09/char67/char10/char48/charf1/chard3/char40 /charc9/char4a/char2e/char10/charaf /charbc/charf1/char10/char4b /charbd/char4a/char0a/char10/char4a/charcb/char40 /charc9/chard4/char67/char40 /charfa/char0a/char47/char2e/char40 /char51/chare5/char84/charba/char09/char4a/char10/char1c/char4b/char2e/charf8/char0a/char59/char4b/char0a/char81/char6b/char40Depression Depression Dialectic MARBERT\nI feel my hand breaking, want to download Tik Tok before I die, I am so bored. Non-depression MSA AraBERT-base(v02)\n/charf8/char0a/char41/char4b/char0a/charf0/char10/char87/char4b/char0a/char41/char09/char92/char10/char4a/char4b/char0a/char59/char67 /charfa/char0a/char6b/charf0/char50/char10/char87/char4b/char0a/char41/char09/char92/char10/char1d/char40 /char41/char09/char4b/char40/charf0/char10/char49/char4a/char0a/charca/chard3 Depression Depression Dialectic MARBERT\nI am bored of being upset, I want someone to be upset with me. Depression MSA AraBERT-base(v02)\n/char91/charca/char09/char67/char40 /charf0 /charfa/char0a/chare6/char84/char09/charae/char09/char4b/char10/char48/charf1/chard3/char40 /charf0 /char51/char6a/char10/char4a/char09/char4b/char40 Suicidal Ideation Suicidal Ideation Dialectic MARBERT\nIs the only resort to commit suicide and end my life. Suicidal Ideation MSA AraBERT-base(v02)\n/charc9/chard2/charbb /char40/char09/char51/char4b/char0a/char41/charab/char11/char81/chard3 /char41/char09/char4b/char40 /char2e/char2e /charf8/char0a/charf0/char40 /chare9/char4a/char2e/charaa/char93/char10/chare8/char41/char4a/char0a/char6d/charcc/char27/char40/char10/chare9/char6b/char40/char51/chare5/char94/char1d/char2e Suicidal Ideation Suicidal Ideation Dialectic MARBERT\nLife is too hard I do not want to continue with it. Suicidal Ideation MSA AraBERT-base(v02)\n/chare9/char3c/charcb/charf0 /chare8/char50/char58/char41/char10/charaf /charf1/chard3 /char48/char2e/char50/char41/char4b/char0a/char09/chare1/char1e/char0a/char6d/charcc/char27/char40 /chare9/char82/char6b/char40 /charfa/char0a/charcd/char40 /char48/char2e/char81/char6b/char40 /char42/charf0/char10/char48/charf1/chard3/char40 /charfa/char0a\n/char09/char47/char40 /charfa/char09/chare6/chard6/char10/chardf/char40 Suicidal Ideation Suicidal Ideation Dialectic MARBERT\nI wish to die and not feel what I am feeling now, Lord I just cannot anymore. Suicidal Ideation MSA AraBERT-base(v02)\nTable 9: Qualitative Evaluation: Predictions of different models on sample tweets from the test data.\ncommon sense factor was missing.\n7 Conclusion\nThis study enables intelligent instruments to iden-\ntify and predict depression symptoms and suicide\nideation from Arabic text based on depression-\nrelated words. This paper proposed computational\napproaches for the utilization of Arabic tweets. We\nscraped data from tweeter with keywords that act\nas depression triggers and labeled them manually.\nIn conclusion and based on the results discussed\nabove, Arabic people do share their feelings on\nTwitter. The results prove that depressed people\nshow specific behaviors within their tweets. They\noften use negative words to describe their symp-\ntoms, like suicidal thoughts or sleeping disorders.\nWe built a predictive model to predict whether\na user’s tweet is depressed or depressed with suici-\ndal ideation. We examined the performance of all\nthe above classifiers using a dataset collected from\nTwitter and labeled manually with truth labels (“de-\npressed”, “suicidal”, “neutral”). We found the best\naccuracy with the MARBERT classifier at 91.20%.\nAcknowledgement\nWe thank many volunteers at Alexandria University\nfor their help in labeling and cleaning our dataset,\nin particular, Maram Attia, Amina Mohamed, Sara\nKhaled, Khlod Mohamed, Hagar Abouroumia,\nZiyad Mohamed and Karim Elsayed.\nReferences\nAhmed Abdelali, Sabit Hassan, Hamdy Mubarak, Ka-\nreem Darwish, and Younes Samih. 2021. Pre-\ntraining bert on arabic tweets: Practical consider-\nations.\nHoussem Abdellaoui and Mounir Zrigui. 2018. Using\ntweets and emojis to build tead: an arabic dataset\nfor sentiment analysis. Computación y Sistemas,\n22(3):777–786.\nMuhammad Abdul-Mageed, AbdelRahim Elmadany,\nand El Moatez Billah Nagoudi. 2021. ARBERT &\nMARBERT: Deep bidirectional transformers for Ara-\nbic. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n7088–7105, Online. Association for Computational\nLinguistics.\nNawaf Abdulla, N Mahyoub, M Shehab, and Mah-\nmoud Al-Ayyoub. 2013. Arabic sentiment analysis:\nCorpus-based and lexicon-based. In Proceedings of\nThe IEEE conference on Applied Electrical Engineer-\ning and Computing Technologies (AEECT).\nNora Al-Twairesh, Hend Al-Khalifa, AbdulMalik Al-\nSalman, and Yousef Al-Ohali. 2017. Arasenti-tweet:\nA corpus for arabic sentiment analysis of saudi tweets.\nProcedia Computer Science, 117:63–72.\nMaryam Mohammed Aldarwish and Hafiz Farooq Ah-\nmad. 2017. Predicting depression levels using social\nmedia posts. In 2017 IEEE 13th international Sympo-\nsium on Autonomous decentralized system (ISADS),\npages 277–280. IEEE.\nZien Sheikh Ali, Watheq Mansour, Tamer Elsayed, and\nAbdulaziz Al-Ali. 2021. Arafacts: the first large\narabic dataset of naturally occurring claims. In Pro-\nceedings of the Sixth Arabic Natural Language Pro-\ncessing Workshop, pages 231–236.\nWissam Antoun, Fady Baly, and Hazem Hajj. Arabert:\nTransformer-based model for arabic language under-\nstanding. In LREC 2020 Workshop Language Re-\nsources and Evaluation Conference 11–16 May 2020,\npage 9.\n309\nWissam Antoun, Fady Baly, and Hazem Hajj. 2021a.\nAraELECTRA: Pre-training text discriminators for\nArabic language understanding. In Proceedings of\nthe Sixth Arabic Natural Language Processing Work-\nshop, pages 191–195, Kyiv, Ukraine (Virtual). Asso-\nciation for Computational Linguistics.\nWissam Antoun, Fady Baly, and Hazem Hajj. 2021b.\nAraGPT2: Pre-trained transformer for Arabic lan-\nguage generation. In Proceedings of the Sixth Ara-\nbic Natural Language Processing Workshop, pages\n196–207, Kyiv, Ukraine (Virtual). Association for\nComputational Linguistics.\nNirmal Varghese Babu and E Kanaga. 2022. Sentiment\nanalysis in social media data for depression detection\nusing artificial intelligence: A review. SN Computer\nScience, 3(1):1–20.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. CoRR,\nabs/1911.02116.\nGlen Coppersmith, Mark Dredze, and Craig Harman.\n2014. Quantifying mental health signals in twitter.\nIn Proceedings of the workshop on computational\nlinguistics and clinical psychology: From linguistic\nsignal to clinical reality, pages 51–60.\nMunmun De Choudhury, Scott Counts, and Eric Horvitz.\n2013a. Social media as a measurement tool of de-\npression in populations. In Proceedings of the 5th\nannual ACM web science conference, pages 47–56.\nMunmun De Choudhury, Michael Gamon, Scott Counts,\nand Eric Horvitz. 2013b. Predicting depression via\nsocial media. In Seventh international AAAI confer-\nence on weblogs and social media.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nAhmed Abdelali Kareem Darwish Nadir Durrani and\nHamdy Mubarak. Farasa: A fast and furious seg-\nmenter for arabic.\nAshraf Elnagar, Leena Lulu, and Omar Einea. 2018.\nAn annotated huge dataset for standard and collo-\nquial arabic reviews for subjective sentiment analysis.\nProcedia computer science, 142:182–189.\nWuwei Lan, Yang Chen, Wei Xu, and Alan Ritter.\n2020. An empirical study of pre-trained transform-\ners for arabic information extraction. arXiv preprint\narXiv:2004.14519.\nAshwag Maghraby and Hosnia Ali. 2022. Modern stan-\ndard arabic mood changing and depression dataset.\nData in Brief, 41:107999.\nMarina Marcus, M Taghi Yasamy, Mark van van Om-\nmeren, Dan Chisholm, and Shekhar Saxena. 2012.\nDepression: A global public health concern.\nRita Merhej. 2019. Stigma on mental illness in the arab\nworld: beyond the socio-cultural barriers. Interna-\ntional Journal of Human Rights in Healthcare.\nDhiaa A Musleh, Taef A Alkhales, Reem A Almakki,\nShahad E Alnajim, Shaden K Almarshad, Rana S\nAlhasaniah, Sumayh S Aljameel, and Abdullah A\nAlmuqhim. 2022. Twitter arabic sentiment anal-\nysis to detect depression using machine learning.\nCMC-COMPUTERS MATERIALS & CONTINUA,\n71(2):3463–3477.\nMahmoud Nabil, Mohamed Aly, and Amir Atiya. 2015.\nAstd: Arabic sentiment tweets dataset. In Proceed-\nings of the 2015 conference on empirical methods in\nnatural language processing, pages 2515–2519.\nEl Moatez Billah Nagoudi, AbdelRahim Elmadany, and\nMuhammad Abdul-Mageed. 2022. AraT5: Text-\nto-text transformers for Arabic language generation.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 628–647, Dublin, Ireland.\nAssociation for Computational Linguistics.\nAli Safaya. 2020. Arabic-albert.\nAli Safaya, Moutasem Abdullatif, and Deniz Yuret.\n2020. KUISAIL at SemEval-2020 task 12: BERT-\nCNN for offensive speech identification in social me-\ndia. In Proceedings of the Fourteenth Workshop on\nSemantic Evaluation, pages 2054–2059, Barcelona\n(online). International Committee for Computational\nLinguistics.\nPedro Javier Ortiz Suárez, Laurent Romary, and Benoît\nSagot. 2020. A monolingual approach to contextual-\nized word embeddings for mid-resource languages.\narXiv preprint arXiv:2006.06202.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2019. Huggingface’s transformers: State-of-\nthe-art natural language processing. arXiv preprint\narXiv:1910.03771.\nAppendix\nIn figure 2, we show the confusion matrices for the\nbest model MARBERT and AraBERT. MARBERT\nis the best model based on Dialectal Arabic and\nAraBERT is the best model based on MSA. Both\nmodels produce close results for the depression\nclass. However, the confusion between the non-\ndepression and suicidal ideation is more present in\nthe AraBERT confusion matrix.\n310\nFigure 2: Top: MARBERT confusion matrix. Bottom:\nAraBERT-base(v02) confusion matrix.\n311"
}