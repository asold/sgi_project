{
  "title": "MarianCG: a code generation transformer model inspired by machine translation",
  "url": "https://openalex.org/W4309700390",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5081981515",
      "name": "Ahmed Soliman",
      "affiliations": [
        "Cairo University"
      ]
    },
    {
      "id": "https://openalex.org/A5021627580",
      "name": "Mayada Hadhoud",
      "affiliations": [
        "Cairo University"
      ]
    },
    {
      "id": "https://openalex.org/A5005159397",
      "name": "Samir I. Shaheen",
      "affiliations": [
        "Cairo University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3126095862",
    "https://openalex.org/W3198659451",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W3198571508",
    "https://openalex.org/W4221145545",
    "https://openalex.org/W4285169833",
    "https://openalex.org/W6600466347",
    "https://openalex.org/W3156012351",
    "https://openalex.org/W2964343359",
    "https://openalex.org/W2964315653",
    "https://openalex.org/W2242083635",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2963794306",
    "https://openalex.org/W2963617989",
    "https://openalex.org/W2962728167",
    "https://openalex.org/W2890867094",
    "https://openalex.org/W2949215742",
    "https://openalex.org/W2997847174",
    "https://openalex.org/W3034976548",
    "https://openalex.org/W3173485235",
    "https://openalex.org/W3174199721",
    "https://openalex.org/W3184995367",
    "https://openalex.org/W4285108092",
    "https://openalex.org/W6600553734",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4220967417",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2964325845"
  ],
  "abstract": "Abstract The idea that computers can build their own programs is extremely significant, and many researchers are working on this challenge. Code generation is described as the process of generating executable code that can be run directly on the computer and fulfills the natural language requirements. It is an intriguing topic that might assist developers to learn a new software technology or programming language, or it could be a simple technique to help in coding through the description of the natural language code developer. In this paper, we present MarianCG, a code generation Transformer model used to tackle the code generation challenge of generating python code from natural language descriptions. Marian neural machine translation (NMT), which is the core model of the Microsoft Translator, is the basis for our NL-to-Code translation engine and is the heart of the teaching model. MarianMT is the teacher language model in our study, and it is one of the most successful machine translation transformers. In our approach, we use a sinusoidal positional embedding technique to represent the position of each token in the text, as well as no layer normalization embedding. Our code generation approach, MarianCG, is based on fine-tuning a machine translation pre-trained language model. This allows us to demonstrate that the pre-trained translation model can also operate and work as a code generation model. The proposed model outperforms recent state-of-the-art models in the problem of code generation when trained on the CoNaLa and DJANGO datasets. MarianCG model scores a BLEU score of 34.43 and an exact match accuracy of 10.2% on the CoNaLa dataset. Also, this model records a BLEU score of 90.41 and an exact match accuracy of 81.83% on the DJANGO dataset. The implementation of MarianCG model and relevant resources are available at https://www.github.com/AhmedSSoliman/MarianCG-NL-to-Code .",
  "full_text": "MarianCG: a code generation transformer \nmodel inspired by machine translation\nAhmed S. Soliman1,2*  , Mayada M. Hadhoud1 and Samir I. Shaheen1 \nIntroduction\nCode generation is a significant field that can predict and generate suitable code as \noutput from the natural language as the input source. The increasing of code genera -\ntion tools with accuracy and optimization tools can help to increase the productiv -\nity of the programming tools [1 ]. Application Programming Interfaces or APIs make \nsoftware development and innovation easier by allowing applications to share data \nAbstract \nThe idea that computers can build their own programs is extremely significant, and \nmany researchers are working on this challenge. Code generation is described as \nthe process of generating executable code that can be run directly on the computer \nand fulfills the natural language requirements. It is an intriguing topic that might \nassist developers to learn a new software technology or programming language, or it \ncould be a simple technique to help in coding through the description of the natu-\nral language code developer. In this paper, we present MarianCG, a code generation \nTransformer model used to tackle the code generation challenge of generating python \ncode from natural language descriptions. Marian neural machine translation (NMT), \nwhich is the core model of the Microsoft Translator, is the basis for our NL-to-Code \ntranslation engine and is the heart of the teaching model. MarianMT is the teacher \nlanguage model in our study, and it is one of the most successful machine translation \ntransformers. In our approach, we use a sinusoidal positional embedding technique \nto represent the position of each token in the text, as well as no layer normalization \nembedding. Our code generation approach, MarianCG, is based on fine-tuning a \nmachine translation pre-trained language model. This allows us to demonstrate that \nthe pre-trained translation model can also operate and work as a code generation \nmodel. The proposed model outperforms recent state-of-the-art models in the prob-\nlem of code generation when trained on the CoNaLa and DJANGO datasets. MarianCG \nmodel scores a BLEU score of 34.43 and an exact match accuracy of 10.2% on the \nCoNaLa dataset. Also, this model records a BLEU score of 90.41 and an exact match \naccuracy of 81.83% on the DJANGO dataset. The implementation of MarianCG model \nand relevant resources are available at https:// www. github. com/ Ahmed SSoli man/ \nMaria nCG- NL- to- Code.\nKeywords: Code generation, Natural language programming, MarianCG, CoNaLa, \nMarianMT, Marian NMT, Neural machine translation\nOpen Access\n© The Author(s) 2022. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// \ncreat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco mmons. org/ publi \ncdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nRESEARCH\nSoliman et al. \nJournal of Engineering and Applied Science          (2022) 69:104  \nhttps://doi.org/10.1186/s44147-022-00159-4\nJournal of Engineering\nand Applied Science\n*Correspondence:   \nahmed.shokry@eng1.cu.edu.eg\n1 Department of Computer \nEngineering, Cairo University, \nGiza, Egypt\n2 Department of Computer \nEngineering, Al-Azhar University, \nNasr City, Egypt\nPage 2 of 23Soliman et al. Journal of Engineering and Applied Science          (2022) 69:104 \nand functions in a simple and safe manner. An API is a set of computer instructions \nand procedures that may be used to get access to a website or web-based software \napplication. Automatic code generation might help developers learn a new program -\nming language or deal with new APIs.\nNowadays, pre-trained language models witnessed tremendous success in the NLP \nfield [2 ]. A pre-trained model is a model that has been trained on a big benchmark \ndataset to tackle some problem and then save this network with weights to be trained \nand reused for another task. Pre-trained models are commonly used to be the core \nof the transfer learning job. Through pre-training and fine-tuning, we can enhance \nmodel robustness and uncertainty. There are several approaches that enable pre-\ntrained language models to train massive models with billions of parameters from \nlarge-scale unlabeled corpora in a self-supervised manner. Recent researches [2 –7] \nhave shown using pre-trained models and also demonstrated the benefits of employ -\ning pre-trained language models for many tasks such as question answering, text clas -\nsification and machine translation.\nTransformers contain numerous pre-trained models that can be used for a variety \nof tasks and datasets [8 ]. Transformers have demonstrated that they can both be few-\nshot [9 ] and unsupervised multitask [10] learners. Transformers prove that they can \nbe applied to any pipeline tasks like machine translation, text-to-text generation, clas -\nsification, and other tasks. Furthermore, researchers demonstrated that massive pre-\ntrained language models can be few-shot semantic parsers [11].\nContributors can use the Transformers library to publish language datasets and \ngenerate and distribute pre-trained models to get new models with high performance \nand huge results. In May 2020, the University of Helsinki’s Language Technology \nResearch Group (Helsinki-NLP) developed and submitted a huge set of translation \nmodels to the Transformers library called MarianMT [12]. They constructed their \nmodels depending on Marian [13] neural machine translation (MarianNMT) frame -\nwork which is accessible at https:// www. marian- nmt. github. io, and it is published \nunder the MIT license. The MarianNMT framework and the Open Parallel Corpus \n(OPUS) dataset were used to train the Helsinki-NLP machine translation models to \nget MarianMT model.\nWith limited computing resources, it is possible to train translation models that are \ncompetitive with state-of-the-art models. Adapting a pre-trained language model with \nthe same architecture from one task to another is a crucial stage in generating a new \ntrustworthy, reliable, and effective model.\nWe implemented MarianCG which is a Transformer language model that can work \nin the code generation task. This is accomplished by fine-tuning MarianMT which is a \npre-trained language model with CoNaLa [14] and DJANGO [15] datasets. MarianCG \nmodel is shown in Fig.  1. We applied the BLEU score measure [16] and exact match \naccuracy to solve the code generation problem, which other researchers used to quantify \nthe quality of the generated output.\nThe experimental findings on the CoNaLa and DJANGO datasets reveal that the Mari-\nanCG transformer model outperforms other state-of-the-art models in respect of the \nrelevant evaluation criteria.\nOur main contributions are: \nPage 3 of 23\nSoliman et al. Journal of Engineering and Applied Science          (2022) 69:104 \n \n1 Introducing MarianCG transformer model, which is a code generation model capa -\nble of creating code from natural language\n2 Testing the effectiveness of using Marian machine translation model for solving the \nproblem of code generation\n3 Demonstrate that a machine translation model may be used as a code generation \nmodel\n4 Setting the new code generation challenge contributors, with a BLEU score of 34.43 \nand 10.2% exact match accuracy on the CoNaLa dataset. Also, we recorded the high-\nest accurate results on the DJANGO challenge reaching 81.83% exact match accu -\nracy, and a BLEU score of 90.41\nThe rest of the paper is organized as follows: Section  2 summarizes the relevant related \nwork and discusses the previous techniques to solve the code generation task. Also, it \nsets this work apart from the relevant related work. Section  3 provides a description of \nthe core model, Marian, and what inspired us to use MarianMT transformer machine \ntranslation model in the code generation problem. Section 4 provides an overview of the \nproposed model and its components. Section  5 contains a list of the datasets that we use \nin our experiments. Following that is a section covering implementation and experimen-\ntal work, which includes the evaluation metrics and experimental setup. We gain results \ncompared to other researchers through the studies after the implementation section \nthen the discussion section that discusses our work. Finally, the section that concludes \nthe paper to demonstrates how our technique adds value to the code generation task and \nthe future work of our study.\nRelated work\nThe problem of transforming natural language (NL) descriptions to generate executable \ncode is known as code generation, which is a sub-task of semantic parsing. There are \nsome difficulties in this problem because the output has a well-defined structure and the \ndomain, structure of the input, and output are not similar. Techniques that are used for \nsolving this problem can be divided into tree-based techniques and deep learning based \ntechniques.\nTree‑based techniques\nTree-based techniques are considered one of the task-driven forms of semantic pars -\ning that translate the natural language input to formal machine executable representa -\ntion. These techniques can represent code as abstract syntax trees (ASTs) which can be \nFig. 1 MarianCG model for code generation\nPage 4 of 23Soliman et al. Journal of Engineering and Applied Science          (2022) 69:104 \ndescribed as the syntactic tree representation of the target code or the cleaned-up ver -\nsion of the parse tree that captures the structure of expressions, the program’s control \ncomponents.\nThe goal of the ASTs is simply to describe the semantic structure of sentences in a \ncomputer language as trees. Semantics can be stated with attribute grammar, but most \nsemantic approaches are significantly more intelligible when based on a clearer repre -\nsentation of a language’s phrases. There are standard templates for the various compo -\nnents of a programming language definition when simulating the code as AST. Also, \nkeep in mind to define the code as AST you need to know the collection of syntactic \ncategories or domains and a set of rules to describe how to connect these categories with \neach other.\nCode generation and semantic parsing need to convert unstructured (or partially \nstructured) inputs to well-formed, executable outputs. So, researchers have used \nsequence-to-tree models for code generation, with the tree representing the AST of the \ntarget source code [11, 17–25], because they wanted to improve the process of creating \ncode snippets by the ASTs.\nAdvantages and disadvantages of tree‑based techniques\nThere are several benefits to implementing tree-based approaches in this task, such as \nhandling the code generation problem by converting the natural language input to the \nmatching AST, which can assist improve accuracy by requiring the output code to be \nrepresented with a well-formed structure. Furthermore, tree-based techniques may \nbe used to any type of data and can also manage data that is not generally distributed. \nFurthermore, tree-based techniques are easy to visualize, making a complex predictive \nmodel much easier to understand. Finally, because variable transformations are unnec -\nessary, tree-based techniques need the minimum amount of data preprocessing.\nOn the other hand, there are some lacks for using these techniques because describing \ncode as AST is difficult way because the number of nodes in the tree frequently surpasses \nthe length of the natural language description. For this reason, tree-based techniques \nare not frequently able to produce correct code for the related natural language descrip -\ntion which is uncommon in the training data. Also, generating AST is synchronous (the \noutput structure diverges from the input structure). The use of ASTs has achieved less \naccurate results compared to deep learning-based models. There has been relatively less \nwork on utilizing the parse trees of the natural language input. Because of these reasons, \nresearchers turned their direction to deep learning based techniques, where there is no \nneed to construct a tree to generate code.\nDeep learning‑based techniques\nSource code generation is considered as text-to-text or sequence-to-sequence, which \ncan be developed and maintained by deep learning models. Machine intelligence that \nunderstands and creates the complex structures of software has a lot of applications in \nsoftware engineering. There are some sequence-to-sequence models, and these models \ncan convert the target code into other sequence domains.\nUsing deep learning to solve and deal with many problems has become an impor -\ntant technology in various domains; therefore, numerous research projects are focused \nPage 5 of 23\nSoliman et al. Journal of Engineering and Applied Science          (2022) 69:104 \n \non deep learning technology and pre-training models. Additionally, transfer learning \nproved great results to generate new models depending on another pre-trained model. \nTransfer learning is the process of fine-tuning a model that has been trained to execute \none job to perform on another task. A pre-trained model can be defined as a stored net -\nwork that has already been trained on a large dataset, typically on a large-scale task.\nAs a result, recent researchers [24, 26–31] in the code generation problem focused on \nfine-tuning and training the pre-trained model in order to create a new task-oriented \nmodel. The amazing potential for using transfer learning to adapt the pre-trained model \nto a specific job further provide consistent outcomes and findings for the seq2seq code \ngeneration task.\nPrevious contributors’ work\nIn 2016, Dong and Lapata proposed a methodology for learning from natural language \ndescriptions and meaning representations [17]. They used recurrent neural networks \n(RNNs) with long short-term memory (LSTM) units to encode phrases and decode log -\nical structures for considering the task of semantic parsing. They created a technique \nthat is based on an attention enhanced encoder-decoder model, and this technique can \nconvert input utterances into vector representations and produce their logical forms. \nThis is done by conditioning output sequences or trees on the vector representations. \nThese encoded and decoded input utterances and their logical structures, and the atten -\ntion layer is used to directly control the program synthesis process. Their testing results \nrevealed that adding a hierarchical tree decoder and the attention mechanism to the \nmodel enhanced performance across the board.\nIn 2017, Yin and Neubig proposed a syntax-driven neural code generation technique \n[18] that constructs an abstract syntax tree by progressively applying actions from a \ngrammar model. They designed a probabilistic grammar model for AST generation. The \nPython abstract grammar has a set of production rules, and an AST was created by com-\nbining numerous production rules, each of which consists of a head node and multiple \nchild nodes.\nIn 2018, Yin and Neubig proposed TRANX [20] which parses the utterance into a for -\nmal meaning representation. TRANX was built through a transition system, and it uses \nthis transition system to convert a natural language utterance into an abstract syntax \ntree (AST) through a series of tree construction actions given an input natural language \nutterance. The parser is then used to turn the intermediate AST into a domain-specific \nmeaning representation, bringing the parsing process to a close. TRANX scores each \nhypothesis AST using a probabilistic model specified by a neural network. But the neu -\nral semantic parser, TRANX indicated an obvious issue of incoherence in generation and \ngot results with the CoNaLa dataset as 24.30 for the BLEU score metric. Also, TRANX \ngot accuracy of 73.7% for the DJANGO dataset.\nIn 2019, Yin and Neubig proposed the Reranking model [21]. They used the previ -\nous TRANX semantic parser to get the meaning representation of the input natural \nlanguage as an abstract syntax tree. They added a reranking method to output the \nmost suitable meaning representation. The reranking model is presented as a fast-\niterating method to enhance the accuracy of parsing and rerank the n-best list of the \nrepresentation of meaning. This can be done by using characteristics designed to \nPage 6 of 23Soliman et al. Journal of Engineering and Applied Science          (2022) 69:104 \naddress problems in baseline models. This model is used and get results with four \ndatasets GEO, ATIS, DJANGO, and CoNaLa. The result obtained is 30.11 of BLEU \nscore with the developing and testing with the CoNaLa dataset. Also, the results on \nthe DJANGO dataset were recorded with 80.2% accuracy.\nIn 2019, Shin et al. introduced PATOIS [22] which is a program synthesizer and also \na neural program synthesizer that trains a tree-based neural synthesizer to use the \ncode idioms while coding generation. The PATOIS system was built on top of struc -\ntural generative models like graph neural networks and sequence-to-tree models.\nIn 2020, Xu et al. proposed a deep learning model by data re-sampling, fine-tuning \nthe pre-trained model, and using incorporating external knowledge [24] to predict \nexecutable python code. To include external knowledge in code generation models, \nthey suggested a model-agnostic strategy based on data augmentation, retrieval, and \ndata re-sampling, which obtained new results on the CoNaLa open-domain code gen -\neration task. They used the CoNaLa-Mined [14] dataset, which was automatically \nmined from StackOverflow and contained 600,000 NL-code pairs in Python. They \nsorted all pairings by confidence scores and discovered that the top 100K examples \nhave a good level of code accuracy and NL-code correlation. As a result, the top 100K \ncouples are chosen for the tests. They generated roughly 13K different NL-code pair -\nings (without resampling) from Python API documentation after pre-processing. \nThey also sampled the same number of pairings for the re-sampling setting to provide \na fair comparison. They used the NL-to-code generation model TRANX [20] as the \nbasic model, with hypothesis reranking [21]. They also used length normalization [32] \nto make sure that beam search didn’t favor shorter results over longer ones. They got \n30.69 BLEU score with external knowledge with the API model, and when they added \nreranking to external knowledge with API they got 32.26 BLEU score metric.\nIn 2021, Dahal et  al. proposed a paper [25] which describes the analysis of Tree-\nstructured architecture and their effect on the code generation problem. They ran \nand tested text-to-tree, structured tree-to-tree, and linearized tree-to-tree models \non constituency-based parse trees where their goal was generating the corresponding \nASTs of the code. They used CoNaLa and ATIS datasets. Constituency or depend -\nency trees are describing the syntactic structure of the input, and these trees can be \nused to accomplish subtree alignment with the destination code matching the AST \nand benefiting the downstream job. Their tree-to-tree model achieved good results.\nIn 2021, Orlanski and Gittens worked on expanding the original CoNaLa dataset \nto include the multimodal textual question bodies and thus the pertinent contextual \ninformation they contain such as inputs, outputs, and required libraries [27]. They \ndeveloped a new transformer model trained in the BART [33] encoder-decoder \nmodel. For both the annotated and mined cases in the CoNaLa corpus, they obtained \nthese textual bodies from Stackoverflow. Then, they used the question bodies and \nconcatenated intents as inputs for a huge pre-trained language model and then used \nbeam search to construct the answer code snippet. They used Python and Hugging -\nFace’s transformer package to build their model. Finally, for text generation, they \nemployed a BART model with a linear layer and a distinct bias. They got a 26.24 \nBLEU score when using the BART base model and when they used the BART model \nwith mined data, they got a 30.55 BLEU score.\nPage 7 of 23\nSoliman et al. Journal of Engineering and Applied Science          (2022) 69:104 \n \nIn 2021, Norouzi et al. showed that transformer-based seq2seq models can compete \nwith or outperform models created expressly for code generation [26]. They created a \nseq2seq transformer model, and they built this model by fine-tuning the pre-trained \ntransformer BERT model as an encoder the decoder was the original transformer \ndecoder with 4-layers transformer decoder. The key is to create a new model and com -\nbine the relatively large monolingual corpora of the meaning representation with tradi -\ntional large-scale pre-trained encoders. They got the highest BLEU score with CoNaLa \ndataset that reached 33.41. Also, the accuracy scored on the DJANGO dataset was \n81.03%. A Seq2Seq model transformer with a little specialized prior could potentially \nachieve results superior to or competitive with models specially developed for code gen -\neration and semantic parsing by leveraging a sufficiently large monolingual corpus of the \nprogramming language.\nIn 2022, Beau and Crabbé developed a new code generation model which has the \nencoder-decoder architecture [28]. They used BERT as an encoder and decoder as a \ngrammar-based. This is some change in TranX [20] seq2seq architecture for generating \ncode from the natural language description. Their proposed architecture can obtain an \nabstract syntax tree (AST) is constrained by the programming language grammar. They \ntrained and tested their model on the CoNaLa and DJANGO datasets. This transition \nsystem is open to guarantee the generation of syntactically correct code. Their research \nemphasizes the significance of grammatical limitations as well as particular techniques \nfor managing variables, list naming, and typing. They scored a 34.2 BLEU score on the \nCoNaLa dataset, and an accuracy of 81.03% on the DJANGO benchmark.\nMarian and inspiration for code generation\nMarian NMT and MarianMT\nMarian is the core engine for the Microsoft Translator Neural Machine Translation ser -\nvices. Marian is a self-contained, free open source, and efficient neural machine transla -\ntion framework which is a built-in automated differentiation engine based on dynamic \ncomputation graphs. This framework was entirely developed in C++, and it demon -\nstrated a research-friendly toolkit with high training and translation speeds. Training \nMarian was performed on raw texts, with data processing employing the Sentence -\nPiece. It is being employed in several projects and is the primary translation and training \nengine as well as it is used by a wide range of enterprises, organizations, and research \ngroups.\nMarian holds its own position in the developing ecosystem of open-source NMT tool -\nkits, and it has powerful translation features, best defined by these features: \n1 It is self-contained, having its own back end that does reverse-mode automated dif -\nferentiation using dynamic graphs.\n2 It supports working on single GPU/CPU and multi GPUs/CPUs. It provides GPU/\nCPU translation style as well as quick multi-GPU training. Also, this model contains \nthe feature of batch translation.\n3 Marian has the feature of creating word alignments and attention output with the \nability to rescore the n-best lists and parallel files.\nPage 8 of 23Soliman et al. Journal of Engineering and Applied Science          (2022) 69:104 \nMarian was used by the University of Helsinki’s Natural Language Processing lab to train \nhundreds of translation models using parallel data acquired from Opus, and those mod -\nels were later open-sourced and called MarianMT. Researchers in this lab subsequently \nadapted the trained model into Huggingface Transformers and made them accessible \nthrough the Huggingface Hub. Each MarianMT model is a transformer encoder-decoder \nwith six layers. MarianCG was inspired by Marian machine translation, which served as \nthe base for our code generation challenge.\nInspired by Marian transformer models for machine translation\nMarian was chosen as the backbone for our code generation approach for many rea -\nsons. To begin, we can talk about the importance of pre-training transformer models \nand the added value of this methodology in solving many problems. Pre-training trans -\nformer models are extensively employed nowadays for a variety of tasks [34], and it has \nbeen applied in code generation in recent years with great impacts. Pre-training is also \nbeneficial for machine translation and code generation activities. For example, the Face -\nbook BART model is a machine translation transformer model, and it was employed \nand fine-tuned for the code generation problem using CoNaLa, yielding a BLEU score of \n26.24. Also, the pre-trained BERT model was merged with the Transformer decoder and \nscored 32.57 BLEU for the code generation challenge. This motivated our new effort to \nuse the pre-trained model and fine-tune this model to get a significant improvement for \nthe code generation challenge. In addition, it is a simple technique to fine-tune the pre-\ntrained model and start training the model from its final weight rather than the original \nweights. Furthermore, we found that there are many machine translation models with \nhuge architecture, such as T5. To be considered these huge models to deploy, powerful \nresources are needed with a strong GPU and plenty of memory with high processing \ncapability.\nMarian was chosen since it is a quick neural machine translation service with accurate \nmachine translation outputs for several languages. MarianMT’s creators, Helsinki-NLP , \nhave over 1000 pre-trained language models for MarianMT translation models. We have \na vision of using a pre-trained model from one language to work in another area. This \nincludes setting up and building AraT5 from T5 to work in another language or domain. \nMarian models are smaller than many other translation models in the machine transla -\ntion model collection, making them perfect for fine-tuning experimentation and integra-\ntion testing. Marian NMT, the core of Microsoft translator, serves as the primary fully \nbasic model for our training to execute the MarianCG model. As a result of these obser -\nvations and insights, we developed MarianCG, a code generation model influenced by \nthe Machine Translation Transformer approach.\nProposed code generation model\nMarianCG model is built and developed using Marian neural machine translation (Mar -\nianNMT). We fine-tuned MarianMT transformer which is a pre-trained model from \nHelsinki-NLP and got our model, MarianCG. It is a multi-head attention transformer \nwith zero-shot learning which observes samples that were not shown during training \nand predicts the sentences that are the right outputs. MarianCG model got high and \naccurate results for the code generation problem with fast performance.\nPage 9 of 23\nSoliman et al. Journal of Engineering and Applied Science          (2022) 69:104 \n \nMarianCG model architecture\nMarianCG is trained and fine-tuned on MarianMT model that was built using Marian -\nNMT. MarianNMT allows rapid training and translation. The architecture of the code \ngeneration model, MarianCG is shown in Fig. 2. It starts by loading the dataset, then the \npreprocessing phase of the input and the output as shown in Fig.  3. Preprocessing con-\ntains tokenization of the sentence, then get embeddings of each token and do positional \nembedding for each token to learn each position of all tokens concerning to the specific \nFig. 2 MarianCG model architecture\nFig. 3 Preprocessing phase for the data\nPage 10 of 23Soliman et al. Journal of Engineering and Applied Science          (2022) 69:104 \ntoken. In addition, padding and truncation are part of the preprocessing phase. Finally, \nthe input and the output sentences are directly inserted into the encoder-decoder model. \nMarianCG model consists of a stack of 6 layers in the encoder and a stack of 6 layers in \nthe decoder.\nMarianCG model is similar to BartForConditionalGeneration with a few minor \nmodifications:\n• Static (sinusoidal) positional embeddings.\n• No layer normalization embedding.\n• The first token in the generation task is the pad token which has 0 as a token embed -\nding.\nAfter the attention softmax, the encoder’s attention weights are used to compute the \nweighted average in the self-attention heads. MarianCG is a PyTorch model with coding \nand implementation of the Marian neural machine translation transformer.\nFigure 4 shows the two representations of the same example of code generation. The \nfirst representation in Fig.  4a is how to find a good solution for code generation using \nthe representation of manual abstract syntax tree (AST). The second representation in \nFig.  4b shows the same example with MarianCG encoder decoder transformer model \nthat successfully gets highly accurate results compared to AST.\nMarianCG tokenization\nMarian tokenizer is developed and mainly depends on SentencePiece [35]. Sentence -\nPiece is a text generation neural network with a text tokenizer and detokenizer which \nhas the predetermined prior vocabulary size to the training of the neural model. Sen -\ntencePiece extends direct training from raw sentences to implement subword units like \nunigram language model [36] and byte-pair-encoding (BPE) [37]. Marian tokenizer \nis derived from PreTrainedTokenizer in the huggingface transformer library, which \nincludes the majority of the essential methods.\nMarianCG embedding and sinusoidal positional embedding\nMarianCG model does not contain convolution or recurrent neural networks, so the \nrole of the embedding and positional embedding now is important and clear. So, by \ndetermining data about the relative or absolute location of the tokens in the sequence to \nhave the order of the sequence. The positional and word embeddings are shared between \nthe encoder and decoder. MarianCG model contains sinusoidal positional embeddings \nto the input embeddings at the encoder and decoder. The job of positional embedding is \nto provide information about the location of each token. This enables the attention layer \nto compute context-dependent responses, such that two tokens with the same value in \nthe input phrase receive distinct representations.\nTransformers employ sinusoidal positional encoding to represent the position of the \ninput. Sinusoidal positional embedding calculates the position encoding as a mix of sine \nand cosine functions with geometrically increasing wavelengths. The sinusoidal repre -\nsentation works as well as a learned representation and better generalizes sequences that \nare longer than the training sequences.\nPage 11 of 23\nSoliman et al. Journal of Engineering and Applied Science          (2022) 69:104 \n \nPositional encoding is defined and formulized in paper [38] where the sum of posi -\ntional encoding and token embedding is given to the encoder and decoder input lay -\ners of the transformer.\nLet dmodel be the embedding dimension of words, and pos ∈ [0, L − 1 ] be the posi -\ntion of a w  word in the w = (w0 ,... ,wL−1) input sequence. Mathematically, the posi -\ntional encoding of w  is defined in Eq. 1 .\nFig. 4 Example of code generation\nPage 12 of 23Soliman et al. Journal of Engineering and Applied Science          (2022) 69:104 \nwhere the positional encoding follows a specific, learned pattern to identify word posi -\ntion or the distance between words in the sequence [39].\nMarianCG has no layer normalization embedding. So, positional embedding gets the \norder of position identifier added to vectors for the transformer to know the order of the \nsequence.\nMarian encoder and decoder architecture\nAfter tokenization and embedding with positional embedding, the next step is to input \nthese embeddings to Marian Encoder and Marian Decoder. Marian Encoder as shown in \nFig. 5 is the multi-head self-attention encoder layers connected with layer normalization \nand after that there are two fully connected layers and final layer normalization.\nAs the encoder architecture is constructed, we can show and make the decoder in \na clear construction. The first steps are tokenization and embedding with positional \nembedding, and the next step is to input these embeddings to Marian encoder and Mar -\nian decoder. Marian decoder as shown in Fig. 6 has the same architecture as the encoder \nbut with adding the encoder attention followed by encoder attention layer normaliza -\ntion. These layers can be added before the two fully connected layers.\nThe attention component in this Transformer model performs its computations \nnumerous times in parallel. Each of these is referred to as an Attention Head. The Atten-\ntion module divides its Query, Key, and Value arguments N times and routes each split \nthrough a separate Head. The results of all these comparable Attention calculations are \nthen added together to produce a final Attention score. This is known as multi-head \nattention, and it allows the Transformer to encode many associations and nuances for \neach word [38].\nThe following are the main features of the MarianCG construction: \n(1)PE (pos,i) =\nsin pos\n10000 2i/dmodel , i = 2k\ncos pos\n10000 2i/dmodel , i = 2k + 1\nFig. 5 Marian encoder architecture\nPage 13 of 23\nSoliman et al. Journal of Engineering and Applied Science          (2022) 69:104 \n \n1 Marian tokenizer depends on SentencePiece\n2 MarianCG contains sinusoidal positional embedding to represent the position of \neach token\n3 No layer normalization embedding for this approach\nDatasets\nIn our research, we used two widely available and well-known data sets, CoNaLa and \nDjango. These datasets were created with the intention of generating code from the cor -\nresponding natural language descriptions.\nCoNaLa dataset\nOne of the most common datasets in the code generation task is called CoNaLa [14], \nand it is created by Carnegie Mellon University NeuLab and STRUDEL Lab. It is called \nCoNaLa for the name Code/Natural Language Challenge. It has the input as natural lan -\nguage description, with the output as the corresponding python code for this specific \ninput. Table 1 shows some examples of CoNaLa NL-code pairs where the input is the \nintent which describes the natural language, and the output is the snippet which is the \ncorresponding code for the natural language description.\nThis dataset has 2,879 annotated NL-code pairings with about 600K mined pairs from \nover 40,000 distinct stackoverflow questions in the dataset.\nFig. 6 Marian decoder architecture\nPage 14 of 23Soliman et al. Journal of Engineering and Applied Science          (2022) 69:104 \nDJANGO dataset\nThe DJANGO dataset [15] is one of the most commonly used datasets in the code gener-\nation task. It has about 19K examples. Each data example is made up of a line of Python \ncode and an annotated natural language description. These examples are divided into \n16000 training, 1000 development, and 1805 test annotations in the Django dataset.\nTable 2 provides various DJANGO NL-code pair instances. The input is the natural \nlanguage text, and the output is the corresponding code for this input.\nImplementation and experimental work\nEvaluation metrics\nBLEU score evaluation metric\nThe BLEU [16] (bilingual evaluation understudy) method evaluates the quality of \nmachine-translated text from one natural language to another. The BLEU statistic counts \nhow many words overlap in a given translation when compared to a reference transla -\ntion, with successive words scoring better. The connection between a machine’s output \nand that of a person is believed to be quality: “the closer a machine translation is to a \nprofessional human translation, the better it is. ” This is the core notion underlying BLEU.\nThe BLEU indicator assigns a translation score from 0 to 1; however, it is commonly \nreported as a percentage number as shown in Table  3. The closer the translation is near \n1, the more it resembles a human translation.\nExact match accuracy\nThis measure is quite simple to compute. This metric is used to compare the similarities \nand differences between two texts. Equation  2 shows how to calculate and measure this \nmetric for the two sentences y(i) and ˆy(i) . The first sentence y(i) is the reference sentence, \nwhich contains 100% of the genuine needed sentence. The second sentence ˆy(i) , is the \npredicted sentence created by the model. Exact match accuracy = 1 if the characters \nof the model’s prediction completely match all the characters of the genuine reference \nTable 1 Intent and snippet examples from CoNaLa dataset\nExamples Intent (natural language) Snippet (code)\nExample 1 Convert a list to a dictionary in python b = dict(zip(a[0::2], a[1::2]))\nExample 2 Sort a list of nested lists l.sort(key=sumnested)\nExample 3 How to get the size of a string in python? print(len(’string’))\nTable 2 Code generation examples in the DJANGO dataset\nExamples Natural language Code\nExample 1 Define the function do_filter with 2 arguments: parser and \ntoken\ndef do_filter ( parser , token ) :\nExample 2 Convert priority into a floating point integer, substitute it for \npriority\npriority = float ( priority )\nExample 3 Define the method as_bytes with arguments self and unix-\nfrom set to boolean False\ndef as_bytes ( self , unixfrom = False ):\nPage 15 of 23\nSoliman et al. Journal of Engineering and Applied Science          (2022) 69:104 \n \nresponse; else, exact match accuracy is determined using comparable characters and is \nin the range between 0 and 1. Exact match accuracy is zero if all characters in the mod -\nel’s prediction do not match all characters in the genuine reference text.\nwhere n is the number of examples, y(i) is the true labels for the ith examples in the refer-\nence text, and ˆy(i) is the predicted labels for the ith examples.\nImplementation\nWe obtained MarianCG, a novel transformer model based on the pre-trained trans -\nformer, by fine-tuning the MarianMT transformer model using the CoNaLa and \nDJANGO datasets.\nFor the CoNaLa dataset, we followed [24, 27] and chose the top mined samples based \non the likelihood that the NL-Code combination is accurate. The produced CoNaLa \ndataset had around 13K distinct NL-Code. This is to ensure a fair comparison that if we \nwant to participate in the CoNaLa challenge, so training the model is done by using the \nconala-train and/or conala-mined datasets, then taking the rewritten intent field from \nthe conala-test dataset as input and generate output from it.\nThe dataset as mentioned contains about 13K different NL-Code. This dataset con -\ntains the conala-train and examples from conala-mined and the 500 examples in conala-\ntest to compare by the same benchmarks as other state-of-the-art contributors. Also, \nwe implement MarianCG to adapt DJANGO dataset which has about 19K pairs of \nNL-Code.\nWe noticed high and accurate results on the DJANGO training and testing. So, we \ndecided to do another training process on the CoNaLa dataset with more data and a lit -\ntle batch size. Table 4 displays the datasets employed in each experiment, as well as the \ndataset size and the number of records in the training, validation, and test sets of data.\nExperimental setup\nThe proposed model was implemented and trained with the dataset using Google Colab \nPro service. This allowed us to use 512 input tokens with a batch size of 8. We used \nPython programming and PyTorch framework to build our model with the HuggingFace \ntransformer module.\n(2)ExactMatchAccuracy = 1\nn\nn∑\ni=1\n[\nI\n(\ny(i) == ˆy(i)\n)]\nTable 3 BLEU score\nBLEU score Interpretation\nLess than 10 Almost useless\n10–19 Hard to get the gist\n20–29 The gist is clear, but has significant grammatical errors\n30–40 Understandable to good translations\n40–50 High quality translations\n50–60 Very high quality, adequate, and fluent translations\nGreater than 60 Quality often better than human\nPage 16 of 23Soliman et al. Journal of Engineering and Applied Science          (2022) 69:104 \nFor training, we depend on the HuggingFace trainer and their implementation of the \nlearning rate scheduler. As well as MarianCG model accepts natural language input and \ngenerates Python code in the output. Table 5 lists the configuration parameters that were \nemployed throughout the training phase. For text generation, we adopted the MarianCG \nmodel with a linear layer and a distinct bias. Also, beam search and early stopping were \nemployed in the generation phase.\nResults\nExperiment 1\nWe began our work by fine-tuning and generating MarianCG transformer model using \nthe CoNaLa dataset. This experiment yields a dataset of around 13K pairings of natural \nlanguage and code.\nTable 6 shows the results of the first experiment, where MarianCG model predictions \npropelled this model to be one of the top accurate code generation models in terms of \naccuracy and BLEU score.\nOur model got the highest exact match accuracy through all models. This experiment \nobtained a BLEU score of 30.92, and achieved 6.2 % in the exact match accuracy with \nthe advantage of multi-head attention, and ease to use through the huggingface hub \nat https:// huggi ngface. co/ Ahmed SSoli man/ Maria nCG- CoNaLa.\nExperiment 2\nIn our second attempt, we trained the MarianCG model using another dataset from the \ncode generation challenge, which has more examples. DJANGO is the name of this data-\nset, which contains 19K natural language and code pairing entries. This dataset is one of \nTable 4 Datasets in each experiment and distribution of the data\nExperiment Dataset Dataset size Dataset split\nTrain Validation Test\nExperiment 1 CoNaLa 13K 11125 1237 500\nExperiment 2 DJANGO 19K 16000 1000 1805\nExperiment 3 CoNaLa 26K 24687 1237 500\nTable 5 Configuration parameters on the training MarianCG model\nParameter Value\noptimizer Adam optimizer\nLearning rate 5e−5\nWeight decay 0.01\nMaximum position embeddings 512\nNumber of hidden layers 6\nscale embedding TRUE\nActivation function swish\nLearning rate scheduler Linear\nWarmup ratio 0.05\nLength penalty 0.9\nPage 17 of 23\nSoliman et al. Journal of Engineering and Applied Science          (2022) 69:104 \n \nthe most commonly used for this job, and our implementation and training MarianCG \nmodel produced highly accurate predictions.\nTable 7 displays the results of the code generation models on the DJANGO dataset. \nMarianCG is regarded to be the greatest model in the DJANGO challenge. MarianCG \nhas a BLEU score of 90.41 and records an exact match accuracy of 81.83%. A compar -\nison of all values is shown in Fig.  7. Our DJANGO-trained model is available via the \nhuggingface hub, which can be found at  https:// huggi ngface. co/ Ahmed SSoli man/ Maria \nnCG- DJANGO.\nExperiment 3\nWe discovered that obtaining additional data resulted in more accurate results. So, for \nour third experiment, we trained the MarianCG model with an increased amount of \ntraining samples. In this experiment, we used the CoNaLa dataset again, but this time \nwith 26K records. This reached our expectations and placed MarianCG model at the top \nof the CoNaLa challenge. The new testing data results are more dissimilar to our initial \ntrial. This experiment yielded a 34.43 BLEU score and a 10.2% exact match accuracy. \nComparing our model to the CoNaLa benchmark models after this experiment revealed \nthat the MarianCG model has the most accurate predictions when compared to other \nstate-of-the-art models. This is displayed in Table  8 which compares the results of this \nexperiment to other models in the CoNaLa code generation challenge, showing the \nBLEU Score and exact match accuracy of each model.\nTable 6 Results of the first experiment on CoNaLa\nModel BLEU score Exact match \naccuracy\nYear\nTranX + BERT w/ mined [28] 34.2 5.8 2022\nBERT + TAE [26] 33.41 - 2021\nExternal Knowledge With API + Reranking [24] 32.26 - 2020\nMarianCG (Ours) 30.92 6.2 2022\nExternal Knowledge With API [24] 30.69 - 2020\nBART W/ Mined [27] 30.55 - 2021\nReranker [21] 30.11 2.8 2019\nBART Base [27] 26.24 - 2021\nTranX [20] 24.3 - 2018\nTable 7 Results of MarianCG model on DJANGO dataset\nRank Model BLEU score Exact match \naccuracy\nYear\n1 MarianCG (Ours) 90.41 81.83 2022\n2 TranX + BERT w/ mined [28] 79.86 81.03 2022\n3 BERT + TAE [26] - 81.77 2021\n4 Reranker [21] - 80.2 2019\n5 TranX [20] - 73.7 2018\n6 ipn [40] 77.6 62.3 2016\n7 Phrasal Statistical MT [40] 47.6 31.5 2016\nPage 18 of 23Soliman et al. Journal of Engineering and Applied Science          (2022) 69:104 \nAlso, Fig.  8 depicts all the CoNaLa dataset results. In addition, MarianCG model is \navailable through the huggingface hub at https:// huggi ngface. co/ Ahmed SSoli man/ Maria \nnCG- CoNaLa- Large.\nDiscussion\nMarianCG model is ranked as the first model for its accurate predictions in terms of \nBLEU score and exact match accuracy. This model has a fewer size architecture. It has \nsix layers in the encoder and six in the decoder, whereas other models have larger model \nsizes. Table  9 shows the deep learning models employed in the code generation chal -\nlenge and the number of layers obtained in each encoder and decoder. TranX + BERT \ntrained their model on 100K CoNaLa samples. On the CoNaLa dataset, we trained our \nmodel on 13K and 26K records, respectively; hence, the trained data was little compared \nto others.\nThis demonstrates that we trained our model with less data than previous SOTA mod-\nels, and our model is also smaller. As a result of the deep learning architectures and the \nquantity of the dataset, our model is both fast and accurate.\nFig. 7 Results on DJANGO\nTable 8 Results of MarianCG model on the CoNaLa dataset\nRank Model BLEU score Exact match \naccuracy\nYear\n1 MarianCG (Ours) 34.43 10.2 2022\n2 TranX + BERT w/ mined [28] 34.2 5.8 2022\n3 BERT + TAE [26] 33.41 - 2021\n4 External Knowledge With API + Reranking [24] 32.26 - 2020\n5 External Knowledge With API [24] 30.69 - 2020\n6 BART W/ Mined [27] 30.55 - 2021\n7 Reranker [21] 30.11 2.8 2019\n8 BART Base [27] 26.24 - 2021\n9 TranX [20] 24.3 - 2018\nPage 19 of 23\nSoliman et al. Journal of Engineering and Applied Science          (2022) 69:104 \n \nConclusions\nFor the code generation challenge, we proposed MarianCG, a Transformer language \nmodel to predict and generate code from the natural language description. We car -\nried out three experiments in which we developed the MarianCG model on numerous \nexamples in the CoNaLa and DJANGO datasets. This research demonstrated that we are \nable to employ a machine translation model as a solution model for the code generation \nproblem. Transfer learning enabled us to obtain a precise model for the code generation \nchallenge, where implementation is dependent on fine-tuning a pre-trained language \nmodel.\nMarianCG was fine-tuned using MarianMT, a machine translation language model \nthat was created with a dependency on the Microsoft Marian toolkit. Our model has the \nbenefit of zero-shot learning, as well as a sinusoidal positional embedding architecture, \nmulti-head attention, and Marian tokenizer depending on SentencePiece. MarianCG \nreceived a BLEU score of 30.92 and an exact match accuracy of 6.2% in our first attempt \nwith the CoNaLa dataset. The second experiment was performed on the DJANGO data -\nset and yielded a BLEU score of 90.41 with an exact match accuracy of 81.83%.\nFinally, the third effort used the CoNaLa dataset, but with double the number of exam-\nples compared to the first attempt. The final experiment yields excellent predictions, \nand the MarianCG model rises to the top of the demanding models. MarianCG model \nachieved a 34.43 BLEU score with a 10.2% exact match accuracy. This model has the \nadvantage of its small size, and it is fast and accurate.\nOur long-term goal is to develop a code generation model so that it can predict opti -\nmized code with high accuracy and consider code theory topics like SOLID principles \nand OOP concepts. In addition, we will continue in demonstrating that employing \nmachine translation language models may work well in the code generation task. Fur -\nthermore, we will strive toward writing numerous lines of code and demonstrate to \nshow how to generate source code for a certain programming language from another \ncomputer programming language. This is known as a code translation task because the \nFig. 8 Results on CoNaLa\nPage 20 of 23Soliman et al. Journal of Engineering and Applied Science          (2022) 69:104 \nTable 9 Comparing the deep learning models contributed to the code generation task\nModel Encoder Decoder CoNaLa dataset DJANGO dataset\nName Encoder size Name Decoder size Dataset size BLEU score Exact \nmatch \nscore\nDataset size BLEU score Exact \nmatch \nscore\nMarianCG (Ours) MarianEncoder 6 layers MarianDecoder 6 layers 26K 34.43 10.2 19K 90.41 81.83\nTranX + BERT w/ mined [28] bert-base-uncased 12 layers Grammar based - 100K 34.2 5.8 19K 79.86 81.03\nBERT + TAE [26] bert-base-uncased 12 layers Transformer Decoder 4 layers 100K 33.41 - 19K - 81.77\nMarianCG (Ours) MarianEncoder 6 layers MarianDecoder 6 layers 13K 30.9 6.2\nBART W/ Mined [27] facebook/bart-base 6 layers facebook/bart-base 6 layers 13K + Question bodies 30.55 -\nBART Base [27] facebook/bart-base 6 layers facebook/bart-base 6 layers 13K 26.24 -\nPage 21 of 23\nSoliman et al. Journal of Engineering and Applied Science          (2022) 69:104 \n \ninput is a programming language such as java and the output is another programming \nlanguage such as python.\nAbbreviations\nMarianNMT  Marian neural machine translation\nMarianMT  Marian machine translation\nMarianCG  Marian code generation\nAPI  Application Programming Interface\nBLEU  Bilingual evaluation understudy\nOPUS  Open Parallel Corpus\nNL  Natural language\nCoNaLa  Code/natural language dataset\nAST  Abstract syntax tree\nBPE  Byte-pair-encoding\nAcknowledgements\nMany thanks to my family: my wife, father, mother, and brothers for their support. I would like to express my gratitude to \nmy supervisors in this work Prof. Samir Shaheen and Dr. Mayada Hadhoud, and many thanks to Prof. Mohamed Zaki, who \nguided me a lot throughout my Masters work. I would also like to thank my friends who supported me and offered deep \ninsight into the study.\nAuthors’ contributions\nWe are aiming to address the code generation problem and create a transformer model that can provide very accurate \nresults. We presented the MarianCG transformer model, which is a code generation model capable of generating code \nfrom natural language. This paper discusses the significance of adopting the Marian machine translation model to solve \nthe problem of code generation. In our implementation, we demonstrated that a machine translation model may be \nused as a code generation model.\n We become new contributors and state-of-the-art in tackling this challenge using the CoNaLa and DJANGO datasets \nbased on the model’s greatest output predictions, achieving a BLEU score of 34.43 and an exact match accuracy of \n10.2% with CoNaLa. In addition, DJANGO has a BLEU score of 90.41 and an exact match accuracy of 81.83%. The struc-\nture of MarianCG model includes sinusoidal positional embedding but no layer normalization embedding; the tokenizer \ndepends on SentencePiece.\nThe authors read and approved the final manuscript.\nFunding\nNo specific funding has to be declared for this work.\nAvailability of data and materials\nAll the data used and/or analyzed during the current study are available from the corresponding author upon reason-\nable request.\n• Datasets\n We used CoNaLa and DJANGO datasets in our experiments.\n1 CoNaLa\n The CoNaLa dataset is the code generation corpus from Carnegie Mellon University NeuLab and STRUDEL Lab. It is avail-\nable at: https:// conala- corpus. github. io/.\n CoNaLa dataset contains an automatically mined dataset with 600K examples, and each example contains a pair of \nintent and the corresponding snippet. Additionally, each example obtains more information about the number of ques-\ntions, how accurate this answer is, and other information. For the test dataset, the creators of this dataset put a bench-\nmark to test on another 500 records for your experiment. We followed other researchers in their work to extract the most \naccurate examples from the 600K records, with only intent and snippet to work on the code generation problem. This is \nvery helpful to do the number of experiments, and get the results for each experiment. Also, this can be a good thing to \nlet anyone work on this task where he/she doesn’t have a powerful GPU.\n We have two subsets from the CoNaLa mined dataset:\n The first dataset contains 13K records of intent and snippet pairs. It is available at: https:// huggi ngface. co/ datas ets/ \nAhmed SSoli man/ CoNaLa.\n 2 CoNaLa-Large\n This version of CoNaLa has 26K records of intent and snippet pairs. It is available at: https:// huggi ngface. co/ datas ets/ \nAhmed SSoli man/ CoNaLa- Large.\n 3 DJANGO dataset\n It has 19K examples for the code generation task. It is one of the most common datasets in this task, and it is available at: \nhttps:// github. com/ odashi/ ase15- django- datas et.\n Also, we uploaded DJANGO dataset on the huggingface hub to be available at: https:// huggi ngface. co/ datas ets/ Ahmed \nSSoli man/ DJANGO.\n• Implementation\n Implementation and everything about how implementation was done, datasets, evaluation metrics, notebooks of Mari-\nanCG model on the CoNaLa and DJANGO datasets are available in this GitHub repository at: https:// github. com/ Ahmed \nSSoli man/ Maria nCG- NL- to- Code.\n• MarianCG model\n MarianCG models are available now at the huggingface hub, and can be used or tested and integrated with any project. \nYou can find this model and easily deal with various datasets through the following links:\nPage 22 of 23Soliman et al. Journal of Engineering and Applied Science          (2022) 69:104 \n 1 MarianCG-CoNaLa https:// huggi ngface. co/ Ahmed SSoli man/ Maria nCG- CoNaLa\n 2 MarianCG-DJANGO https:// huggi ngface. co/ Ahmed SSoli man/ Maria nCG- DJANGO\n 3 MarianCG-CoNaLa-Large https:// huggi ngface. co/ Ahmed SSoli man/ Maria nCG- CoNaLa- Large\nDeclarations\nEthics approval and consent to participate\nNot applicable\nConsent for publication\nNot applicable\nCompeting interests\nThe authors declare that they have no competing interests.\nReceived: 10 January 2022   Accepted: 31 October 2022\nReferences\n 1. Le TH, Chen H, Babar MA (2020) Deep learning for source code modeling and generation: models, applications, and \nchallenges. ACM Comput Surv (CSUR) 53(3):1–38\n 2. Han X, Zhang Z, Ding N, Gu Y, Liu X, Huo Y, Qiu J, Yao Y, Zhang A, Zhang L, Han W, Huang M, Jin Q, Lan Y, Liu Y, Liu Z, \nLu Z, Qiu X, Song R, Tang J, Wen JR, Yuan J, Zhao WX, Zhu J (2021) Pre-trained models: past, present and future. AI \nOpen 2:225–250. https:// doi. org/ 10. 1016/j. aiopen. 2021. 08. 002\n 3. Peters ME, Neumann M, Iyyer M, Gardner M, Clark C, Lee K, Zettlemoyer L (2018) Deep contextualized word \nrepresentations. Proceedings of the 2018 Conference of the North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies, Volume 1 (Long Papers). Association for Computational \nLinguistics, New Orleans, pp 2227–2237. https:// doi. org/ 10. 18653/ v1/ N18- 1202\n 4. Devlin J, Chang MW, Lee K, Toutanova K (2018) Bert: pre-training of deep bidirectional transformers for language \nunderstanding. arXiv preprint arXiv: 1810. 04805\n 5. Gu Y, Han X, Liu Z, Huang M (2022) PPT: Pre-trained prompt tuning for few-shot learning. In: Proceedings of the 60th \nAnnual Meeting of the Association for Computational Linguistics, vol 1: Long Papers. Association for Computational \nLinguistics, Dublin, p 8410–8423. https:// doi. org/ 10. 18653/ v1/ 2022. acl- long. 576\n 6. Ding N, Qin Y, Yang G, Wei F, Yang Z, Su Y, Hu S, Chen Y, Chan CM, Chen W, Yi J, Zhao W, Wang X, Liu Z, Zheng H, \nChen J, Liu Y, Tang J, Li J, Sun M (2022) Delta tuning: a comprehensive study of parameter efficient methods for pre-\ntrained language models. ArXiv arxiv: 2203. 06904\n 7. Qin Y, Zhang J, Lin Y, Liu Z, Li P , Sun M, Zhou J (2022) ELLE: Efficient lifelong pre-training for emerging data. In: Find-\nings of the Association for Computational Linguistics: ACL 2022. Association for Computational Linguistics, Dublin, p \n2789–2810. https:// doi. org/ 10. 18653/ v1/ 2022. findi ngs- acl. 220\n 8. Phuong M, Hutter M (2022) Formal algorithms for transformers. ArXiv arxiv: 2207. 09238\n 9. Brown TB, Mann B, Ryder N, Subbiah M, Kaplan J, Dhariwal P , Neelakantan A, Shyam P , Sastry G, Askell A, et al (2020) \nLanguage models are few-shot learners. arXiv preprint arXiv: 2005. 14165\n 10. Radford A, Wu J, Child R, Luan D, Amodei D, Sutskever I et al (2019) Language models are unsupervised multitask \nlearners. OpenAI blog 1(8):9\n 11. Shin R, Lin CH, Thomson S, Chen C, Roy S, Platanios EA, Pauls A, Klein D, Eisner J, Van Durme B (2021) Constrained \nlanguage models yield few-shot semantic parsers. arXiv preprint arXiv: 2104. 08768\n 12. Marianmt model. https:// www. huggi ngface. co/ docs/ trans forme rs/ model_ doc/ marian. Accessed Oct 2021\n 13. Junczys-Dowmunt M, Grundkiewicz R, Dwojak T, Hoang H, Heafield K, Neckermann T, Seide F, Germann U, Aji AF, \nBogoychev N, et al (2018) Marian: fast neural machine translation in c++. arXiv preprint arXiv: 1804. 00344\n 14. Yin P , Deng B, Chen E, Vasilescu B, Neubig G (2018) Learning to mine aligned code and natural language pairs from \nstack overflow. In: 2018 ieee/acm 15th international conference on mining software repositories (msr). IEEE\n 15. Oda Y, Fudaba H, Neubig G, Hata H, Sakti S, Toda T, Nakamura S (2015) Learning to generate pseudo-code from \nsource code using statistical machine translation. In: 2015 30th IEEE/ACM International Conference on Automated \nSoftware Engineering (ASE). pp 574–584. https:// doi. org/ 10. 1109/ ASE. 2015. 36\n 16. Papineni K, Roukos S, Ward T, Zhu WJ (2002) Bleu: a method for automatic evaluation of machine translation. In: \nProceedings of the 40th annual meeting of the Association for Computational Linguistics. Association for Computa-\ntional Linguistics, Philadelphia, p 311–318. https:// doi. org/ 10. 3115/ 10730 83. 10731 35\n 17. Dong L, Lapata M (2016) Language to logical form with neural attention. arXiv preprint arXiv: 1601. 01280\n 18. Yin P , Neubig G (2017) A syntactic neural model for general-purpose code generation. arXiv preprint arXiv: 1704. \n01696\n 19. Rabinovich M, Stern M, Klein D (2017) Abstract syntax networks for code generation and semantic parsing. arXiv \npreprint arXiv: 1704. 07535\n 20. Yin P , Neubig G (2018) Tranx: A transition-based neural abstract syntax parser for semantic parsing and code genera-\ntion. arXiv preprint arXiv: 1810. 02720\n 21. Yin P , Neubig G (2019) Reranking for neural semantic parsing. In: Proceedings of the 57th Annual Meeting of the \nAssociation for Computational Linguistics. Association for Computational Linguistics, Florence, p 4553–4559. \nhttps:// doi. org/ 10. 18653/ v1/ P19- 1447\nPage 23 of 23\nSoliman et al. Journal of Engineering and Applied Science          (2022) 69:104 \n \n 22. Shin EC, Allamanis M, Brockschmidt M, Polozov A (2019) Program synthesis and semantic parsing with learned code \nidioms. In: Proceedings of the 33rd International Conference on Neural Information Processing Systems (NeurIPS \n2019). Advances in Neural InformationProcessing Systems, Vancouver, p 10825–10835. https:// dl. acm. org/ doi/ 10. \n5555/ 34542 87. 34552 58\n 23. Sun Z, Zhu Q, Xiong Y, Sun Y, Mou L, Zhang L (2020) Treegen: a tree-based transformer architecture for code genera-\ntion. Proceedings of the AAAI Conference on Artificial Intelligence, vol 34 No. 05. AAAI-20 Technical Tracks 5, Palo \nAlto, p 8984-8991. https:// doi. org/ 10. 1609/ aaai. v34i05. 6430 \n 24. Xu FF, Jiang Z, Yin P , Vasilescu B, Neubig G (2020) Incorporating external knowledge through pre-training for natural \nlanguage to code generation. arXiv preprint arXiv: 2004. 09015\n 25. Dahal S, Maharana A, Bansal M (2021) Analysis of tree-structured architectures for code generation. In: Findings of \nthe Association for Computational Linguistics: ACL-IJCNLP 2021. Association for Computational Linguistics, Bangkok, \np 4382–4391. https:// doi. org/ 10. 18653/ v1/ 2021. findi ngs- acl. 384\n 26. Norouzi S, Tang K, Cao Y (2021) Code generation from natural language with less prior knowledge and more mono-\nlingual data. In: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the \n11th International Joint Conference on Natural Language Processing, vol 2: Short Papers. Association for Computa-\ntional Linguistics, Bangkok, p 776–785. https:// doi. org/ 10. 18653/ v1/ 2021. acl- short. 98\n 27. Orlanski G, Gittens A (2021) Reading stackoverflow encourages cheating: adding question text improves extractive \ncode generation. arXiv preprint arXiv: 2106. 04447\n 28. Beau N, Crabbé B (2022) The impact of lexical and grammatical processing on generating code from natural lan-\nguage. arXiv preprint arXiv: 2202. 13972\n 29. Wang Z, Cuenca G, Zhou S, Xu FF, Neubig G (2022) Mconala: a benchmark for code generation from multiple natural \nlanguages. arXiv preprint arXiv: 2203. 08388\n 30. Kusupati U, Ailavarapu VRT (2022) Natural language to code using transformers. ArXiv arxiv: 2202. 00367\n 31. Al-Hossami E, Shaikh S (2022) A survey on artificial intelligence for source code: a dialogue systems perspective. \nArXiv arxiv: 2202. 04847\n 32. Cho K, Van Merriënboer B, Gulcehre C, Bahdanau D, Bougares F, Schwenk H, Bengio Y (2014) Learning phrase repre-\nsentations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv: 1406. 1078\n 33. Lewis M, Liu Y, Goyal N, Ghazvininejad M, Mohamed A, Levy O, Stoyanov V, Zettlemoyer L (2019) Bart: denoising \nsequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint \narXiv: 1910. 13461\n 34. Subramanyam Kalyan K, Rajasekharan A, Sangeetha S (2021) Ammus: a survey of transformer-based pretrained \nmodels in natural language processing. arXiv e-prints arXiv –2108\n 35. Kudo T, Richardson J (2018) Sentencepiece: a simple and language independent subword tokenizer and deto-\nkenizer for neural text processing. arXiv preprint arXiv: 1808. 06226\n 36. Kudo T (2018) Subword regularization: improving neural network translation models with multiple subword candi-\ndates. arXiv preprint arXiv: 1804. 10959\n 37. Sennrich R, Haddow B, Birch A (2015) Neural machine translation of rare words with subword units. arXiv preprint \narXiv: 1508. 07909\n 38. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, Polosukhin I (2017) Attention is all you \nneed. In: Advances in neural information processing systems 30 (NIPS 2017), Annual Conference on Neural Informa-\ntion Processing Systems 2017, Long Beach, CA, USA. Curran Associates, Inc., p 5998–6008. https:// papers. nips. cc/ \npaper/ 7181- atten tion- is- all- you- need\n 39. Alammar J (2018) The illustrated transformer. http:// jalam mar. github. io/ illus trated- trans former/. Accessed May 2021\n 40. Ling W, Blunsom P , Grefenstette E, Hermann KM, Kočiský T, Wang F, Senior A (2016) Latent predictor networks for \ncode generation. In: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics \n(Volume 1: Long Papers). Association for Computational Linguistics, Berlin, pp 599–609. https:// doi. org/ 10. 18653/ \nv1/ P16- 1057\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8388226628303528
    },
    {
      "name": "Machine translation",
      "score": 0.8044239282608032
    },
    {
      "name": "Code generation",
      "score": 0.7250242829322815
    },
    {
      "name": "Programming language",
      "score": 0.6754130125045776
    },
    {
      "name": "Python (programming language)",
      "score": 0.6567863821983337
    },
    {
      "name": "Executable",
      "score": 0.5727617144584656
    },
    {
      "name": "Machine code",
      "score": 0.5546622276306152
    },
    {
      "name": "Artificial intelligence",
      "score": 0.545007586479187
    },
    {
      "name": "Transformer",
      "score": 0.5343152284622192
    },
    {
      "name": "Embedding",
      "score": 0.5033077597618103
    },
    {
      "name": "Natural language processing",
      "score": 0.4662935137748718
    },
    {
      "name": "Natural language",
      "score": 0.4448617398738861
    },
    {
      "name": "Compiler",
      "score": 0.21172162890434265
    },
    {
      "name": "Operating system",
      "score": 0.09944427013397217
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Key (lock)",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}