{
  "title": "VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning",
  "url": "https://openalex.org/W3143728625",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1902374701",
      "name": "Chen Jun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2147105040",
      "name": "Guo Han",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117304021",
      "name": "Yi Kai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1980430440",
      "name": "Li, Boyang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2747543368",
      "name": "Elhoseiny, Mohamed",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2550553598",
    "https://openalex.org/W3104279398",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W2964195337",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W2990818246",
    "https://openalex.org/W2963048642",
    "https://openalex.org/W1987835821",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W1897761818",
    "https://openalex.org/W2607579284",
    "https://openalex.org/W2604178507",
    "https://openalex.org/W2031648200",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2986670728",
    "https://openalex.org/W2795151422",
    "https://openalex.org/W2962968835",
    "https://openalex.org/W2988647680",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963084599",
    "https://openalex.org/W3098325931",
    "https://openalex.org/W1969616664",
    "https://openalex.org/W3034984754",
    "https://openalex.org/W2171361956",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2912664121",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W2962964995",
    "https://openalex.org/W2995460200",
    "https://openalex.org/W2983141445",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W1947481528",
    "https://openalex.org/W3103211586",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2949376505",
    "https://openalex.org/W2963101956",
    "https://openalex.org/W2506483933",
    "https://openalex.org/W3104609094",
    "https://openalex.org/W2979747405",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2901988662",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W2575842049",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W2152772232",
    "https://openalex.org/W2963546667",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2963758027",
    "https://openalex.org/W2768282280",
    "https://openalex.org/W2963088515",
    "https://openalex.org/W2964049455",
    "https://openalex.org/W2963743213",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W2058556535",
    "https://openalex.org/W2173180041",
    "https://openalex.org/W2890531016",
    "https://openalex.org/W3104027471",
    "https://openalex.org/W2062955551",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W1905882502",
    "https://openalex.org/W2463955103",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W2913618459",
    "https://openalex.org/W3034655362",
    "https://openalex.org/W2803206166",
    "https://openalex.org/W2953039584",
    "https://openalex.org/W2962735233"
  ],
  "abstract": "The ability to quickly learn from a small quantity oftraining data widens the range of machine learning applications. In this paper, we propose a data-efficient image captioning model, VisualGPT, which leverages the linguistic knowledge from a large pretrained language model(LM). A crucial challenge is to balance between the use of visual information in the image and prior linguistic knowledge acquired from pretraining. We designed a novel self-resurrecting encoder-decoder attention mechanism to quickly adapt the pretrained LM as the language decoder ona small amount of in-domain training data. The proposed self-resurrecting activation unit produces sparse activations but has reduced susceptibility to zero gradients. We train the proposed model, VisualGPT, on 0.1%, 0.5% and 1% of MSCOCO and Conceptual Captions training data. Under these conditions, we outperform the best baseline model by up to 10.8% CIDEr on MS COCO and upto 5.4% CIDEr on Conceptual Captions. Further, Visual-GPT achieves the state-of-the-art result on IU X-ray, a medical report generation dataset. To the best of our knowledge, this is the first work that improves data efficiency of image captioning by utilizing LM pretrained on unimodal data. Our code is available at: https://github.com/Vision-CAIR/VisualGPT.",
  "full_text": "VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image\nCaptioning\nJun Chen1 , Han Guo2, Kai Yi 1, Boyang Li3, Mohamed Elhoseiny1\n1 King Abdullah University of Science and Technology (KAUST),\n2Carnegie Mellon University, 3 Nanyang Technological University\n{jun.chen,kai.yi,mohamed.elhoseiny}@kaust.edu.sa\nhanguo@cs.cmu.edu, boyang.li@ntu.edu.sg\nAbstract\nThe limited availability of annotated data often hinders\nreal-world applications of machine learning. To efficiently\nlearn from small quantities of multimodal data, we lever-\nage the linguistic knowledge from a large pre-trained lan-\nguage model (PLM) and quickly adapt it to new domains\nof image captioning. To effectively utilize a pretrained\nmodel, it is critical to balance the visual input and prior\nlinguistic knowledge from pretraining. We propose Visu-\nalGPT, which employs a novel self-resurrecting encoder-\ndecoder attention mechanism to quickly adapt the PLM\nwith a small amount of in-domain image-text data. The\nproposed self-resurrecting activation unit produces sparse\nactivations that prevent accidental overwriting of linguis-\ntic knowledge. When trained on 0.1%, 0.5% and 1% of\nthe respective training sets, VisualGPT surpasses the best\nbaseline by up to 10.0% CIDEr on MS COCO [45] and\n17.9% CIDEr on Conceptual Captions [69]. Furthermore,\nVisualGPT achieves the state-of-the-art result on IU X-ray\n[15], a medical report generation dataset. Our code is\navailable at https://github.com/Vision-CAIR/\nVisualGPT.\n1. Introduction\nRecent performance gains in image captioning [13, 25,\n29, 33, 81] are achieved on top of large-scale data corpora\nsuch as MS COCO [45] or Conceptual Captions [69], each\ncontaining hundreds of thousands of captions. Manual an-\nnotation of captions requires considerable time and effort.\nOn the other hand, semi-automatic collection of image-\ncaption pairs from the Internet, as used by Conceptual Cap-\ntions [69], may generate incorrect or undesirable training\ndata even after multiple rounds of cleaning. Data for spe-\ncialized domains like medical report generation [15,42] and\nlow-resource language captioning [18, 80] cannot be easily\nA cop on brown horse on \nsidewalk next to truck.\nDecoder Layer 1\nPretrained \nLM Weights\nEncoder Layer 1\nSelf Attention\nFeed Forward\n‚Ä¶ Masked Self \nAttention\nCross Attention\nFeed Forward\nEncoder Layer K\nDecoder Layer K\n‚Ä¶\nSelf-resurrecting \nEncoder-decoder\nAttention\nùú∑ùë£ùëñùë† ùú∑ùëôùëéùëõ\nFigure 1. Our VisualGPT model transfers the knowledge from\na pre-trained language model to the caption decoder. A self-\nresurrecting encoder-decoder attention is designed to connect the\nmulti-level visual features and caption decoder.\nscaled. Improving the data efficiency of image captioning\nnetworks would enable quick data curation, description of\nrare objects, and applications in specialized domains.\nIn this paper, we investigate the data efficiency prob-\nlem for image captioning. This problem is distinct from\nthe novel object captioning problem [1,24], which relies on\nabundant in-domain data but zero out-of-domain data. In-\nstead, we aim to improve the performance of image caption-\ning systems trained on a small subset of in-domain data.\nWe propose to improve data efficiency by leveraging pre-\ntrained language models (PLMs) [17, 36, 48, 63], such as\nBERT [16], XLNet [83], and GPT [6, 61, 62]. Via self-\nsupervised learning, these models acquire rich linguistic\nand semantic knowledge, which has been shown to inform\ndownstream tasks in NLP [7, 21]. However, the adapta-\ntion of PLMs pretrained on unimodal textual data for mul-\ntimodal tasks remain under-investigated.\nFigure 2. Comparison of the part-of-speech distributions of the\nMS COCO and WikiText-2 datasets [54]. We use the spacy parser\nand show only the most important categories.\nA key challenge in utilizing PLMs is to bridge the do-\nmain gap between multi-modal data and the unimodal tex-\ntual data the PLMs are pre-trained on. In Figure 2, we\ncompare the part-of-speech distributions of MS COCO and\nWikiText-2 [54]. MS COCO employs 75% more nouns but\n14% fewer verbs, which indicates a bias toward descriptions\nof static objects rather than actions. This suggests that, in\norder to effectively utilize PLMs in image captioning, we\nmust balance prior linguistic knowledge acquired from pre-\ntraining and visual input information.\nFigure 1 depicts the overall architecture of our pro-\nposed model, dubbed as VisualGPT. In the commonly used\nencoder-decoder architecture for image captioning, we ini-\ntialize the parameters of the decoder from PLMs such as\nGPT-2 [62], whereas the encoder layers are randomly ini-\ntialized. In addition, we propose an attention mechanism\nwith self-resurrecting activation units (SRAUs), which bal-\nances the input from the visual encoder and the linguistic\ninput from the previous decoder layer. The proposed mech-\nanism can produce sparse activations while not being as vul-\nnerable to the zero-gradient problem as regular gates; the\nself-resurrecting gates can be ‚Äúturned on‚Äù again after being\nzeroed out.\nEmpirical results demonstrate that, when trained on\n0.1%, 0.5%, and 1% of the MS COCO and Conceptual\nCaptions data, VisualGPT outperforms several strong base-\nline models. We achieve the state-of-the-art result on IU X-\nray [15], a medical report generation dataset. With several\nablation experiments, we verify the effectiveness of PLMs\nand the proposed self-resurrecting attention mechanism.\nContributions. We make the following contributions:\n‚Ä¢ We explore the data efficiency problem for image\ncaptioning by utilizing pretrained language models\n(PLMs) as the caption decoder. With only a small\namount of in-domain training data, the proposed tech-\nnique quickly adapts PLMs to the cross-modal task of\nimage captioning. To our knowledge, this is the first\nwork that focuses on efficiently adapting large pre-\ntrained language models for image captioning.\n‚Ä¢ We propose a novel encoder-decoder attention with\nself-resurrecting activation units (SRAUs), which can\nbalance features from the visual and textual modalities.\nSRAU produces sparse activations that reduce acciden-\ntal overwriting of pretrained weights.\n2. Related Work\nImage Captioning. Image captioning has been exten-\nsively studied in computer vision research. Early methods\n[19, 33, 39, 71, 85] focus on filling templates with extracted\nobjects, attributes, and relationships. With the advent of\ndeep learning, researchers proposed end-to-end neural net-\nworks that encode an image into vector representations and\ndecode a caption word by word [28, 77]. Many improve-\nments to the encoder [11, 40, 52, 81, 82, 86, 87], the decoder\n[78, 79, 84], and the attention mechanism [8, 13, 25, 35, 38]\nhas since been proposed. Encoding the image using object\nregions has proven beneficial [2]. Reinforcement learning\nenables model optimization with non-differentiable evalua-\ntion metrics [14, 47, 65, 70]. [9, 12] investigate fine-grained\ncontrol of caption generation. [14, 70] adopt GAN-like ar-\nchitectures that encourage human-like captions.\nA few formulations of the image captioning problem\ndeviate from the traditional supervised learning paradigm.\nNovel object captioning aims to describe objects that do\nnot exist in the training data [1, 24, 43, 53, 76]. Feng et al.\n[20] propose unsupervised captioning without using paired\nimage-caption supervision. Kim et al [30] focus on learning\nefficiency and improve the data efficiency by learning from\nauxiliary unpaired image-caption data.\nSelf-supervised NLP Models. Self-supervised training of\nlarge neural networks on textual data proves to be an im-\nportant technique in the creation of high-performance NLP\nmodels. Several self-supervision signals have been pro-\nposed, such as autoregressive language modeling [5, 55],\nwhich includes the GPT series of models [6, 61, 62], and\nmasked language modeling, which includes ELMo [59] and\nBERT-related methods [16, 34, 49].\nIn this paper, we propose a quick adaptation technique\nfor network weights obtained using the language model-\ning (LM) objective. However, the proposed technique can\neasily be applied to other models, as the masked language\nmodeling objective can be converted to the LM objective\nby masking only the last word in the textual sequence. Un-\nlike neural networks pretrained on multimodal data (e.g.,\n[41,51,60,72,73,88,89]), our method only requires a small\namount of multimodal training data and focuses on adapting\nlinguistic knowledge learned from the textual modality.\nDecoder Layer m\nMasked Self Attention\nCross Attention\nM2TransformerDecoder Layer m\nMasked Self Attention\nFeed Forward\nDecoder Layer mAoATransformerTransformer\nCrossAttention\nAdd & Norm\nAdd & Norm\nMaskedSelf Attention\nFeed Forward\nCrossAttention\nAdd & Norm\nAdd & Norm\n Add & Norm\nAdd & Norm\nII\nMeshed Connection Sum\nZm-1\nVisualGPT (Ours)\nMasked Self Attention\nCross Attention\nFeed Forward\nAdd & Norm\nAdd & Norm\nDecoder Layer m\nùë©!ùë©\"I\nAdd & Norm\nAdd & Norm\nMeshed Connection Sum\nConcat\nFeed Forwardùú∂\nùú∂\nrandom init.\n(a) (b) (c) (d)\nI\nZm-1 Zm-1 Zm-1\nH\npretrained LM init.\nùú∂BH H H gated units\nFigure 3. Architectures of vanilla Transformer [74], Transformer with AoA module [25] (AoA Transformer), M2 Transformer [13], and\nVisualGPT. We denoteI and H as the visual and language features, respectively. Zm‚àí1 is the output from decoder layer m ‚àí 1. Within\nthe circles, Œ±, BV and BL represent different gating units.\n3. Preliminaries: Transformer for Captioning\nThe Transformer [74] has become one of the standard\nmodels for image captioning. At its core lies the multi-head\ndot-product attention mechanism. Taking three input matri-\nces, query Q, key K, and value V , the attention function\ncan be written as\nAttn(Q, K, V) =softmax\n\u0012(WqQ)(WkK)‚ä§\n‚àö\nD\n\u0013\nWvV,\n(1)\nwhere Wq, Wk, and Wv are trainable parameters and D\nis a scaling factor. Intuitively, the attention operation can\nbe seen as encoding WqQ as convex combination of the\nrow vectors of WvV . The multi-head attention repeats the\nprocess with multiple sets of Wq, Wk, and Wv; the results\nare concatenated and linearly projected back to the same\ndimensionality.\nIn visual captioning tasks, we apply a visual encoder\nwhose output is I ‚àà RO√óS. O is the length of the input\nsequence, which in this work is a sequence of objects in the\nimage. S is the hidden dimension size. The decoder net-\nwork outputs words in the caption sequentially.\nWhen decoding wordt+1, the encoder-decoder attention\ntakes as input the visual encoding I and the current state of\nthe decoder H ‚àà Rt√óS. We apply the attention operation\nwith H as the query and I as both the key and the value.\nThe encoder-decoder attention is then\nEncDecAttn(H, I) =Attn(H, I, I). (2)\nAfter that, we apply the AddNorm operator, which contains\na residual connection and layer normalization [3] and can\nbe written as LayerNorm(EncDecAttn(H, I) +H).\nResearchers have proposed other variants of the encoder-\ndecoder attention. In Figure 3, we contrast these decoder\narchitectures with the proposed VisualGPT model. The\nAttention-on-Attention (AoA) module [25] provides an al-\nternative method for combining the visual encoding I and\nthe linguistic information H from the decoder. For an-\nother method for combining visual and linguistic informa-\ntion, M2 Transformer [13] connects all decoder layers to\nall encoder layers. In Figure 3, it is represented by the box\nlabeled as Meshed Connection Sum.\n4. VisualGPT\nPretrained language models (PLMs) such as GPT-2 [62]\nare trained on data from a single modality. We use a PLM as\nthe caption decoder and feed visual information to the PLM\nvia the encoder-decoder attention, which plays a crucial role\nin quickly adapting the PLMs.\nWith the design of the encoder-decoder attention, we aim\nto carefully balance visual information from the encoder\nand linguistic knowledge stored in the PLM. During the\ngeneration of visual words, such as ‚Äúperson‚Äù, ‚Äútruck‚Äù, or\n‚Äúdog‚Äù, the model should attend to visual information. In\ncontrast, the generation of determiners or connectives re-\nquires only linguistic knowledge. Ideally, we would like to\nexploit the massive amount of linguistic knowledge stored\nin the PLM weights (e.g., [46]), while referring to the vi-\nsual input only when required. To achieve this goal, we\nintroduce a pair of specialized gating units.\n4.1. Self-Resurrecting Activation Unit\nThe encoder-decoder attention EncDecAttn (H, I) may\nbe seen as encoding the linguistic informationH with visual\ninformation I. In VisualGPT, we control the balance be-\ntween these two modalities using two complementary gates\nBvis and Blan. The output of this module is\nBvis ‚äó EncDecAttn(H, I) +Blan ‚äó H, (3)\nwhere ‚äó denotes element-wise multiplication. Letting\nBvis[i, j] and Blan[i, j]] denote the elements in the matrices,\nthey are computed in pairs as\nBvis[i, j] =œÉ(H[i, j])1 (œÉ(H[i, j]) > œÑ),\nBlan[i, j] = (1‚àí œÉ(H[i, j])1 (1 ‚àí œÉ(H[i, j]) > œÑ),\n(4)\nwhere œÑ is a predefined threshold hyperparameter and 1 (¬∑)\nis the indicator function, which returns 1 if the inner state-\nment is true and 0 otherwise.\nAn alternative to SRAU is ordinary complementary gates\n(OCG), computed as œÉ(H[i, j]) and 1‚àíœÉ(H[i, j]) (see Fig-\nure 4, top left). OCG can output values that are very close\nto zero. In contrast, with the indicator functions SRAU di-\nrectly sets values less than the threshold œÑ to zero, thereby\nintroducing sparsity. When œÑ is set to 0, SRAU becomes\nOCG. As the gradient cannot backpropagate through zero\ngates, SRAU prevents optimization from disrupting pre-\ntrained weights that capture linguistic knowledge. This\nproperty is crucial in effective utilizing of pretrained mod-\nels. In contrast, when the OCG gates output near-zero val-\nues, some small but non-zero gradients may still overwrite\nexisting linguistic knowledge.\nAnother advantage of SRAU is its ability to escape from\nzero outputs. It is possible for one gate to output zero and\nhave zero gradient while the gradient for the other gate\nremains usable (e.g., when x in Fig 4 is close to 1.3 or\n‚àí1.3). The asymmetry allows gradient-based optimization\nto change the zero-outputting gate by changing the other\ngate. For this reason, we name these gates self-resurrecting\nactivation units.\nThe asymmetry of SRAU may appear counter-intuitive.\nWe contrast SRAU with a ‚Äúnormalized‚Äù version where the\ntwo gates ÀúBvis[i, j] and ÀúBlan[i, j] become symmetric.\nÀúBvis[i, j] = Bvis[i, j]\nBvis[i, j] +Blan[i, j],\nÀúBlan[i, j] = Blan[i, j]\nBvis[i, j] +Blan[i, j].\n(5)\nThese gates lose the asymmetry that enables the self-\nresurrecting property.\nIn Figure 4, we visualize OCG, SRAU, and normalized\nSRAU. In ablation experiments, we show that SRAU out-\nperforms than both OCG and normalized SRAU.\n4.2. The Architecture and Training of VisualGPT\nFor completeness, we introduce the overall architecture\nfor VisualGPT. The image encoder comprising K Trans-\nformer layers. Given an image, we extract objects in the\n10.0\n 7.5\n 5.0\n 2.5\n 0.0 2.5 5.0 7.5 10.0\nx\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0y\nOrdinary Complementary Sigmoid Gates\nBlan\nm\nBvis\nm\n10.0\n 7.5\n 5.0\n 2.5\n 0.0 2.5 5.0 7.5 10.0\nx\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0y\nNormalized SRAU: =0.2\nBlan\nm\nBvis\nm\n10.0\n 7.5\n 5.0\n 2.5\n 0.0 2.5 5.0 7.5 10.0\nx\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0y\nSRAU: =0.2\nBlan\nm\nBvis\nm\nFigure 4. Top Left: Ordinary complementary sigmoid gates. Top\nRight: Normalized SRAU œÑ=0.2. Bottom: SRAU œÑ=0.2. The x-\naxis indicates the function inputs and the y-axis indicates outputs.\nimage using an off-the-shelf object detection network. After\nthat, we feed the spatial location into the image encoder. As\nsuch, the image encoder outputsI of dimension S √óO√óK.\nThe caption decoder contains M layers and its param-\neters are initialized from a PLM. We insert the encoder-\ndecoder module, which is randomly initialized. We also\napply meshed connections between the encoder and the de-\ncoder like in M2 Transformer. The network is trained to\nmaximize the probability of the next token wt conditioned\non tokens w1, . . . , wt‚àí1 and the encoder output I. Af-\nter a predefined number of epochs on supervised learning,\nwe switch to self-critical reinforcement learning [65] with\nCIDEr as the reward.\n5. Experiments\n5.1. Datasets and Evaluation Metrics\nWe evaluate our model on three datasets, MS COCO\n[45], Conceptual Captions [69], and IU X-ray [15]. MS\nCOCO contains 123, 287 images and each of them is an-\nnotated with 5 different captions. We follow the Karpa-\nthy split [29] for the validation and test set. The Concep-\ntual Captions dataset [69] contains around 3.3M images for\ntraining and 28K for validation, with much higher diversity\nthan COCO. As the test data is not publicly available, we\ninstead use the public validation data as our test set, and ran-\ndomly sample 5000 different image-caption pairs from the\ntraining set as the validation set. To create the small train-\ning data setup for MS COCO and Conceptual Captions, we\nrandomly sample 0.1%, 0.5% and 1% image-caption pairs\nas training data, which matches to (567, 2,835 and 5,670\npairs) for COCO and (3,300, 16,500, and 33,000 pairs) for\nConceptual Captions. We repeat the experiments 4 times\nwith different random seeds, and report the average perfor-\nmance. We report metrics for BLEU [57], METEOR [4],\nCOCO Conceptual\nMethod PLM B1 B4 M R C B1 B4 M R C\n0.1% training data\nTransformer [74] None 57.4 13.1 16.7 40.7 40.8 12.4 2.4 4.9 15.2 21.2\nM2 Transformer [13] None 56.9 13.1 16.9 40.6 40.9 13.1 2.8 4.8 15.5 23.5\nAoA Transformer [25] None 56.6 13.5 15.9 40.7 38.4 11.4 2.4 4.6 14.7 20.9\nX-Transfomrer [56] None 56.7 12.9 16.5 40.6 40.4 12.8 2.7 4.7 15.3 23.1\nOSCAR [41] BERT 53.8 11.9 17.1 39.5 41.0 12.2 2.4 4.3 14.8 21.9\nTransformer GPT 56.8 15.3 17.0 41.2 42.9 13.2 2.5 5.0 15.1 21.9\nM2 Transformer GPT 54.9 14.7 16.6 41.1 41.0 11.9 2.6 4.9 15.4 24.0\nAoA Transformer GPT 55.5 14.4 16.2 40.7 40.1 11.8 2.8 4.6 13.9 20.5\nVisualGPT (Normalized SRAU) GPT 55.7 15.0 16.8 41.2 42.4 13.3 2.9 5.1 15.8 25.8\nVisualGPT (Our SRAU) GPT 58.2 16.4 18.5 41.9 45.1 13.9 3.2 5.6 16.7 27.7\n0.5% training data\nTransformer None 62.8 18.8 19.4 25.2 59.2 13.2 3.3 5.5 16.3 29.6\nM2 Transformer None 63.3 19.4 19.8 45.6 61.3 14.5 3.6 6.0 17.1 32.0\nAoA Transformer None 63.5 20.2 19.4 45.8 63.9 13.8 3.3 5.6 17.9 31.8\nX-Transformer None 62.9 19.0 19.6 45.7 62.0 14.2 3.5 5.8 17.3 32.1\nOSCAR BERT 59.2 18.0 21.0 45.3 60.2 14.4 3.7 6.1 17.2 33.5\nTransformer GPT 65.1 21.8 20.6 46.6 69.5 16.2 3.8 6.5 18.3 35.6\nM2 Transformer GPT 64.7 21.8 20.7 47.1 68.5 13.9 3.6 6.0 17.2 34.1\nAoA Transformer GPT 64.2 21.2 20.5 46.5 67.2 14.8 3.6 6.2 17.6 34.1\nVisualGPT (Normalized SRAU) GPT 65.3 21.8 20.9 47.0 69.3 14.9 3.9 6.1 18.0 35.9\nVisualGPT (Our SRAU) GPT 66.2 22.1 21.1 47.3 70.3 15.9 4.2 6.7 18.5 37.2\n1% training data\nTransformer None 66.0 21.9 21.1 47.3 71.9 13.9 3.7 6.3 18.1 37.9\nM2 Transformer None 67.1 23.4 21.3 48.3 73.0 16.0 4.1 6.8 18.9 39.8\nAoA Transformer None 67.6 23.6 21.5 48.4 75.5 14.9 4.1 6.5 18.6 39.0\nX-Transformer None 67.0 23.6 21.2 48.1 47.1 15.6 4.0 6.6 18.7 39.5\nOSCAR BERT 67.2 23.3 22.5 49.1 78.4 16.1 4.2 6.7 18.9 40.6\nTransformer GPT 68.5 25.1 22.1 49.0 80.5 17.8 4.2 6.7 19.0 40.2\nM2 Transformer GPT 68.2 25.0 22.4 49.2 80.4 15.4 3.9 6.5 17.9 39.1\nAoA Transformer GPT 68.5 24.6 22.0 48.6 78.4 15.4 3.9 6.5 17.9 38.5\nVisualGPT (Normalized SRAU) GPT 68.7 25.2 22.3 49.2 80.6 15.3 4.2 6.7 18.3 40.3\nVisualGPT (Our SRAU) GPT 69.5 25.6 22.6 49.6 80.9 16.3 4.3 6.9 19.3 40.9\nTable 1. Performance of the compared methods training on 0.1%, 0.5% and 1% of MS COCO and Conceptual Caption image-caption\npairs. The best performance in each configuration is in bold. Ablated models are marked in gray .\nROUGE [44], and CIDEr [75].\nIU X-ray [15] is a radiography dataset containing 7, 470\nchest X-ray images and 3, 955 human-written reports. As\nthe dataset is already small, we follow the original split,\nwhich has a training set of5, 226 images and 2, 770 reports.\nMost reports have two images corresponding to the frontal\nand lateral viewpoints.\n5.2. Experimental Settings\nBaselines. We compare our model with several state-of-the-\nart transformer-based models, including:\n‚Ä¢ Plain Transformer [74].\n‚Ä¢ AoA Transformer, which inserts an attention-on-\nattention (AoA) module [25] into every transformer\nlayer, as depicted by Figure 3 (b). Following [13], we\nslightly update the original AoA network in [25] by re-\nplacing the LSTM with Transformers in order to create\na fair Transformer-to-Transformer comparison.\n‚Ä¢ M2 Transformer [13], which proposes a meshed con-\nnection between encoder and decoder and is one of the\nbest-performing models on MS COCO.\n‚Ä¢ X-Transformer [56], which employs bilinear pooling\nto selectively capitalize on visual information and is\nModels B-1 B-2 B-3 B-4 R M C\nAtt2in 22.4 12.9 8.9 6.8 30.8 - 29.7\nCoAtt 45.5 28.8 20.5 15.4 36.9 - 27.7\nHRGR 43.8 29.8 20.8 15.1 32.2 - 34.3\nCMAS-RL 46.4 30.1 21.0 15.4 37.1 - 27.5\nChen et al. 47.0 30.4 21.9 16.5 37.1 18.7 -\nVisualGPT (ours) 48.0 31.3 22.2 15.9 37.4 20.5 49.7\nTable 2. Performance on the IU X-ray dataset.\none of best-performing models on MS COCO.\n‚Ä¢ OSCAR [41], which finetunes BERT initialization on\nimage-language dataset.\nSince VisualGPT has GPT as the pretrained decoder, for fair\ncomparisons, we also create variants of Transformer, AoA\nTransformer andM2 Transformer with GPT as the decoder.\nFor VisualGPT, we set œÑ to 0.2 in all experiments. We also\nexplored the effect of different œÑ and find œÑ in the range\nof [0, 0.2] to offer the right level of sparsity. For all other\nbaselines, we tune the hyperparameters on the validation\nset of MS COCO. We train our model and all the baselines\nin reinforcement learning setting following the work in [13].\nPlease see the supplemental material for more details on hy-\nperparameters and experimental results.\n5.3. Quantitative Results\nSmall In-domain Training Data. Results on MS COCO\nand Conceptual Captions are presented in Tables 1. Visu-\nalGPT outperforms the best-performing baseline model by\n4.1 CIDEr when trained on 0.1% of MS COCO data, 6.4\nCIDEr when trained on 0.5% data and 2.5 CIDEr with 1%\ntraining data. On Conceptual Caption dataset, VisualGPT\nalso outperforms all the baselines. It outperforms the best\nbaseline model by 4.2 CIDEr under 0.1% training data, 3.5\nCIDEr under 0.5% data and 0.3 CIDEr under 1% data.\nComparison with BERT-based model. We compared\nwith OSCAR [41] which is a BERT-based [16] model with\ngood performing results in many benchmarks. We run their\nmodel without pretraining on a large-scale image-language\ncorpus for the fair comparison with our model. The main\ndifference between BERT and GPT is their different pre-\ntraining objectives, where BERT uses masked language\nmodeling and GPT is the autoregressive prediction of the\nnext word. GPT has more similar learning behaviors to the\nimage captioning model compared to BERT since they are\nboth optimized by autoregressively generating the next lan-\nguage word. The experimental result in Table 1 shows that\nVisualGPT is better than OSCAR in both datasets, which\nconfirms our selection choice of using GPT as a decoder.\nMedical Report Generation. We compared VisualGPT\nagainst state-of-the-art medical report generation models\nModels B-1 B-4 M R C\nKim et al. [31] 58.1 13.4 15.9 - 36.0\nKim et al. + unpaired 63.0 18.7 20.7 - 55.2\nGu et al. [22] 46.2 5.4 13.2 - 17.7\nFeng et al. [20] 58.9 18.6 17.9 - 54.9\nVisualGPT (ours) 67.1 24.3 21.9 48.6 75.8\nTable 3. Comparison with unsupervised and semi-supervised\nlearning methods using Kim et al.‚Äôs split of MS COCO. Kim et\nal. employ only 1% images for training in contrast to 1% image-\ncaption pairs from Table 1. Note that Kim et al. + unpaired also\nuse the rest of training data as unpaired images and texts. The\ngray shading denotes baselines that use a large amount of un-\npaired images and texts during training.\nincluding Att2in [65], CoAtt [27], HRGR [37], CMAS-\nRL [26] and the model from Chen et al. [10]. This dataset\nonly contains around 2, 770 medical reports in the training\nset, which is less than 1% COCO data and poses a data-\nefficiency challenge. We follow the same experimental set-\nting as in [10]. The results show that VisualGPT outper-\nforms the baselines for most evaluation metrics and creates\na new state-of-the-art. It shows the value of leveraging GPT\nknowledge into the highly specific domain which has very\n‚Äúexpensive‚Äù and insufficient paired data. We hope our find-\ning could inspire future work in other domains.\nComparison Against Semi-supervised and Unsuper-\nvised Methods. Kim et al. [31] proposed a semi-supervised\nlearning method to improve the data efficiency of image\ncaptioning. They used 1% of images and all their captions\nas training data, rather than 1% of all the image-caption\npairs in Table 1, hence they cover less images since each\nimage is associated to more than 1 caption. For Kim et al.\n+ unpaired, they also employ the other 99% of MS COCO\nas unpaired images and captions for training. We replicate\ntheir setup by only training with 1% of images. As shown in\nTable 3, without using additional unpaired images and cap-\ntions, the proposed VisualGPT method outperforms Kim et\nal. [31] by 20.6 CIDEr score.\nWe also compare VisualGPT against unsupervised meth-\nods of Gu et al. [22] and Feng et al. [20], which use tens\nof millions of unpaired images and captions. Even though\nthese are not fair comparisons, it is encouraging to see Vi-\nsualGPT surpassing these baselines by utilizing the super-\nvision of only 1133 training images.\n5.4. Ablation Studies\nAblation on cross-attention: To fairly compare our SRAU\nwith other cross-attention mechanisms in the baselines, we\nalso initialize their decoder with 12-layer GPT and keep\nthe same encoder as VisualGPT. We contrast between plain\ncross-attention, meshed cross-attention, and attention-on-\n44.3 42.7 45.1\n69 67.9 70.3\n82.5 81.5 80.9\n99.5 100.5 103\n35\n45\n55\n65\n75\n85\n95\n105\n115\n0 0.1 0.2\nCIDEr\nThreshold (œÑ)\nCIDEr performance on different thresholds for SRAU\n0.10% 0.50% 1% 5%\nFigure 5. CIDEr performance v.s. different thresholdsœÑ with 0.1%\n0.5%, 1% and 5% training data.\nattention (AoA) modules. For AoA Transformer, we add the\nAoA module on top of cross-attention. Table 1 shows the\nresults, which demonstrate that SRAU is better than other\ncross-attention modules in exploiting the GPT knowledge\nwithin the image-caption task.\nAblation on SRAU: We create an ablation called Normal-\nized SRAU, where we replace the SRAU with the normal-\nized SRAU (see Figure 4) and use GPT2 initialization. We\nprovided the results in table 1. The normalized SRAU\nresults in substantially lowered performance, decreasing\nCIDEr from full VisualGPT by 2.7, 1.0, and 0.3 respec-\ntively on the three setups on MS COCO, and it also de-\ncreases from Full VisualGPT by 2.2, 1.3 and 0.6 respec-\ntively on Conceptual Caption. This demonstrates that the\nself-resurrecting property is beneficial for learning from\nsmall data. We experimented with Leaky ReLU and GELU,\nwhich ameliorate zero gradients, but the training crashed\ndue to the lack of upper limits for function values.\nWe explored different œÑ among (0, 0.1 0.2) and show\ntheir CIDEr performance on different percentage of COCO\ntraining data in the Figure 5. œÑ=0 is equivalent to ordinary\ncomplementary sigmoid gates. We can observe that œÑ = 0.2\ncan give us the best performance in most cases, indicating\nthe usefulness of incorporating sparsity in our SRAU com-\nplementary gates.\n5.5. Human Study\nIn addition to automatic evaluation metrics, we conduct\ntwo human studies to further evaluate the quality of gen-\nerated captions. In the first study, we asked participants\ndirectly for preference over generated captions. We ran-\ndomly selected 250 test images from the three setups of\nMethod 0.1% data 0.5% data 1% data\nTransformer 18.4% 17.2% 16.8%\nAoA Transformer 11.5% 20.9% 25.0%\nM2 Transformer 30.9% 22.8% 20.8%\nVisualGPT 39.2% 39.1% 37.4%\nTable 4. The percentage of votes received by VisualGPT and base-\nline models under different quantity of training data.\nQ1. Does the caption miss things shown in the image?\nAnswer Ours M2Transformer Transformer AoA GT\nNo 719 624 633 621 973\nYes 367 438 456 447 73\nNo Rate 0.66 0.59 0.58 0.58 0.93\nQ2. Does the caption describe things not in the image?\nAnswer Ours M2Transformer Transformer AoA GT\nNo 720 692 633 655 448\nYes 360 418 423 412 43\nNo Rate 0.67 0.62 0.60 0.61 0.96\nTable 5. Human evaluation of object hallucination and omission.\nGT denotes the ground-truth captions.\nGT:theladyissittingonthewoodbench\nGT:a laptop with a keyboard and mouse are on this desk\nGT:acatissittinginfrontofatelevision\nGT:a number of people sitting on a snowy surface with skis\nOurs a cat is sitting in front of a television\nattention 0.8 0.86 0.8 0.83 0.7 0.72 0.6 0.71 0.93\nOurs a woman sitting on a bench in a park\nattention 0.7 0.78 0.82 0.76 0.8 0.96 0.8 0.69 0.85\nOurs a laptop sitting on a desk with a mouse\nattention 0.7 0.78 0.81 0.7 0.7 0.92 0.85 0.64 0.76\nOurs a couple of people sitting on a snowy surface\nattention 0.8 0.87 0.71 0.85 0.91 0.76 0.71 0.94 0.95\nFigure 6. Visual scores of words in generated captions. We show\nthe raw visual scores and highlight them according to normalized\nvisual scores. High visual scores are in blue and low scores in red.\n0.1%, 0.5%, and 1% training data. For every image, we\ngenerated one caption from VisualGPT and each of three\nhigh-performing baselines from Table 1, Transformer [74],\nM2 Transformer [13], and AoA Transformer [25], all with\nthree decoder layers. Every image was evaluated by 5 dif-\nferent Turkers, who chose the caption that most accurately\ndescribed the image content. We received3750 (250 images\nmaxvisualbenchwoodensittingclocktoilet\nminvisualtoofonthea\nFigure 7. Distributions of linguistic attention ( Blan) and visual\nattention (Bvis) at every decoding layer. We also show the words\ngenerated with the highest and lowest visual attention.\n√ó5 Turkers √ó3 setups) valid responses.\nWe summarize the results in Table 4. Overall, the cap-\ntions generated by VisualGPT received the largest share of\nvotes, 39.2% for the 0.1% training data split, 39.1% for the\n0.5% split, and 37.4% for the 1% split. For each train-\ning setup, we conducted Pearson‚Äôs Chi-square test [58],\nwhich shows the differences are statistical significant with\np <0.05 in all cases.\nIn the second study, we evaluate if using pretrained\nlanguage models introduces excessive linguistic prior that\ncould cause the known object hallucination problem [67].\nFrom the models trained using 1% COCO data. We ran-\ndomly sampled 250 images with the generated caption from\neach model. For each image, we asked 5 different partic-\nipants if the caption (1) described non-existent objects or\n(2) missed objects existing in the image. To catch random\nclickers, we created 5 images with verified captions, so that\nwe knew the right answers of these questions. Participants\nwho answered these questions wrongly were considered un-\nreliable and removed from the results.\nThe results are in Table 5. Compared to the baselines,\nVisualGPT has less hallucination and higher coverage of\nobjects. The study also finds that the ground-truth captions\nhas the least amount of hallucination and highest coverage\nof objects in the image. This finding lends positive support\nto the validity of the experimental protocol.\n5.6. Analysis\nIn this section, we visually examine examples from the\nVisualGPT model trained on 1% of MS COCO. First, we\nshow example captions generated by VisualGPT in Figure\n6 and the associated Bvis at the last decoder layer. Note\nthat for every word generated, we have a 768-dimensional\nvisual gate vector, which is a slice of Bvis at different de-\ncoding time steps. We take the mean of the gate vector as\nthe visual score for that word. After that, we normalize\nthe visual scores across the dataset to the [0, 1] interval and\nhighlight the words accordingly. Blue indicates high visual\nscores and red indicates low visual scores. We observe that,\nin agreement with our intuition, VisualGPT assigns high vi-\nsual scores to words like ‚Äúdesk‚Äù and ‚Äúsnowy surface‚Äù and\nlow visual scores to determiners and prepositions.\nIn Figure 7, we plot the distribution of Bvis and Blan at\nevery decoder layer as a box-and-whisker diagram. We also\nshow the words with the highest and lowest visual scores,\nwhich are again in line with our expectations. Additionally,\nwe observe that, going from layer 0 to layer 9, the decoder\nmakes increasing use of visual information, but the upper-\nmost layers, 10 and 11, make more balanced use of informa-\ntion. We hypothesize that the low layers focus on low-level\nlinguistics like syntax, whereas the middle layers learn to\nfuse linguistic information with visual information. Finally,\nthe two information sources become balanced in the upper-\nmost layers.\n5.7. Limitation\nOne limitation of our proposal is that, as experiments in\nthe supplementary material show, the gap between baseline\nmodels and VisualGPT gradually vanishes as in-domain\ntraining data increase. The phenomenon is more pro-\nnounced in COCO than Conceptual Captions, which has\na more diverse vocabulary. We hypothesize that linguistic\nknowledge from pretrained models is the most useful when\nthe training data are small and do not provide sufficient cov-\nerage of the vocabulary.\n6. Conclusions\nWe present VisualGPT, a data efficient image caption-\ning model which leverages the linguistic knowledge from\nthe pretrained language model. To bridge the semantic gap\nbetween different modalities, we design a novel encoder-\ndecoder attention mechanism with an unsaturated rectified\ngating function. We evaluate our model on 0.1%, 0.5% and\n1.0% of MS COCO and Conceptual Captions, and IU X-\nray, a small medical imaging report dataset. VisualGPT\nachieves the state-of-the-art result on IU X-ray and outper-\nforms strong baseline models.\nVisualGPT may solve the realistic need when training\ncaptioning models on low-resource languages or highly spe-\ncialized domains, where it could be challenging to find an-\nnotators to collect a large amount of data.\nAcknowledgments. This work is funded by KAUST\nBAS/1/1685-01-0, KAUST-FCC/1/2533-17-01, and Na-\ntional Research Foundation Fellowship (NRF-NRFF13-\n2021-0006), Singapore.\nReferences\n[1] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen,\nRishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Ste-\nfan Lee, and Peter Anderson. nocaps: novel object caption-\ning at scale. In ICCV, 2019. 1, 2\n[2] Peter Anderson, Xiaodong He, Chris Buehler, Damien\nTeney, Mark Johnson, Stephen Gould, and Lei Zhang.\nBottom-up and top-down attention for image captioning and\nvisual question answering. In CVPR, 2018. 2, 12\n[3] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton.\nLayer normalization. arXiv 1607.06450, 2016. 3\n[4] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic\nmetric for mt evaluation with improved correlation with hu-\nman judgments. In Proceedings of the acl workshop on in-\ntrinsic and extrinsic evaluation measures for machine trans-\nlation and/or summarization, 2005. 4\n[5] Yoshua Bengio, R ¬¥ejean Ducharme, Pascal Vincent, and\nChristian Jauvin. A neural probabilistic language model.\nJournal of machine learning research, 3(Feb), 2003. 2\n[6] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. arXiv preprint\narXiv:2005.14165, 2020. 1, 2\n[7] Pawel Budzianowski and Ivan Vulic. Hello, it‚Äôs GPT-2 -\nhow can I help you? towards the use of pretrained language\nmodels for task-oriented dialogue systems. In Alexandra\nBirch, Andrew M. Finch, Hiroaki Hayashi, Ioannis Konstas,\nThang Luong, Graham Neubig, Yusuke Oda, and Katsuhito\nSudoh, editors, EMNLP-IJCNLP. Association for Computa-\ntional Linguistics, 2019. 1\n[8] Long Chen, Hanwang Zhang, Jun Xiao, Liqiang Nie, Jian\nShao, Wei Liu, and Tat-Seng Chua. Sca-cnn: Spatial and\nchannel-wise attention in convolutional networks for image\ncaptioning. In CVPR, 2017. 2\n[9] Shizhe Chen, Qin Jin, Peng Wang, and Qi Wu. Say as you\nwish: Fine-grained control of image caption generation with\nabstract scene graphs. In CVPR, 2020. 2\n[10] Zhihong Chen, Yan Song, Tsung-Hui Chang, and Xiang\nWan. Generating radiology reports via memory-driven trans-\nformer. In EMNLP, 2020. 6\n[11] Cesc Chunseong Park, Byeongchang Kim, and Gunhee Kim.\nAttend to you: Personalized image captioning with context\nsequence memory networks. In CVPR, 2017. 2\n[12] Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara.\nShow, control and tell: A framework for generating control-\nlable and grounded captions. In CVPR, 2019. 2\n[13] Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi, and\nRita Cucchiara. Meshed-memory transformer for image cap-\ntioning. In CVPR, 2020. 1, 2, 3, 5, 6, 7, 12, 13\n[14] Bo Dai, Sanja Fidler, Raquel Urtasun, and Dahua Lin. To-\nwards diverse and natural image descriptions via a condi-\ntional gan. In ICCV, 2017. 2\n[15] Dina Demner-Fushman, Marc D Kohli, Marc B Rosen-\nman, Sonya E Shooshan, Laritza Rodriguez, Sameer Antani,\nGeorge R Thoma, and Clement J McDonald. Preparing a\ncollection of radiology examinations for distribution and re-\ntrieval. Journal of the American Medical Informatics Asso-\nciation, 23(2), 2016. 1, 2, 4, 5\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. In NAACL, 2019. 1, 2,\n6\n[17] Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu,\nYu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon.\nUnified language model pre-training for natural language un-\nderstanding and generation. In NeurIPS, 2019. 1\n[18] Obeida ElJundi., Mohamad Dhaybi., Kotaiba Mokadam.,\nHazem Hajj., and Daniel Asmar. Resources and end-to-\nend neural network models for arabic image captioning. In\nProceedings of the 15th International Joint Conference on\nComputer Vision, Imaging and Computer Graphics Theory\nand Applications - Volume 5: VISAPP ,, pages 233‚Äì241. IN-\nSTICC, SciTePress, 2020. 1\n[19] Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Pe-\nter Young, Cyrus Rashtchian, Julia Hockenmaier, and David\nForsyth. Every picture tells a story: Generating sentences\nfrom images. In ECCV. Springer, 2010. 2\n[20] Yang Feng, Lin Ma, Wei Liu, and Jiebo Luo. Unsupervised\nimage captioning. In CVPR, 2019. 2, 6\n[21] Sergey Golovanov, Rauf Kurbanov, Sergey Nikolenko,\nKyryl Truskovskyi, Alexander Tselousov, and Thomas Wolf.\nLarge-scale transfer learning for natural language genera-\ntion. In ACL, 2019. 1\n[22] Jiuxiang Gu, Shafiq Joty, Jianfei Cai, and Gang Wang. Un-\npaired image captioning by language pivoting. In ECCV,\n2018. 6\n[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\n2016. 12\n[24] Lisa Anne Hendricks, Subhashini Venugopalan, Marcus\nRohrbach, Raymond Mooney, Kate Saenko, and Trevor Dar-\nrell. Deep compositional captioning: Describing novel ob-\nject categories without paired training data. In CVPR, 2016.\n1, 2\n[25] Lun Huang, Wenmin Wang, Jie Chen, and Xiao-Yong Wei.\nAttention on attention for image captioning. In ICCV, 2019.\n1, 2, 3, 5, 7, 13\n[26] Baoyu Jing, Zeya Wang, and Eric Xing. Show, describe and\nconclude: On exploiting the structure information of chest\nx-ray reports. In ACL, 2019. 6\n[27] Baoyu Jing, Pengtao Xie, and Eric Xing. On the automatic\ngeneration of medical imaging reports. In ACL, 2018. 6\n[28] Justin Johnson, Andrej Karpathy, and Li Fei-Fei. Densecap:\nFully convolutional localization networks for dense caption-\ning. In CVPR, 2016. 2\n[29] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic align-\nments for generating image descriptions. In CVPR, 2015. 1,\n4\n[30] Dong-Jin Kim, Jinsoo Choi, Tae-Hyun Oh, and In So\nKweon. Image captioning with very scarce supervised data:\nAdversarial semi-supervised learning approach. In Kentaro\nInui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors,\nEMNLP-IJCNLP. Association for Computational Linguis-\ntics, 2019. 2\n[31] Dong-Jin Kim, Jinsoo Choi, Tae-Hyun Oh, and In So\nKweon. Image captioning with very scarce supervised data:\nAdversarial semi-supervised learning approach. In EMNLP,\nHong Kong, China, Nov. 2019. Association for Computa-\ntional Linguistics. 6\n[32] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A Shamma, et al. Visual genome:\nConnecting language and vision using crowdsourced dense\nimage annotations. IJCV, 123(1), 2017. 12\n[33] Girish Kulkarni, Visruth Premraj, Vicente Ordonez, Sag-\nnik Dhar, Siming Li, Yejin Choi, Alexander C Berg, and\nTamara L Berg. Babytalk: Understanding and generating\nsimple image descriptions. TPAMI, 35(12), 2013. 1, 2\n[34] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin\nGimpel, Piyush Sharma, and Radu Soricut. Albert: A lite\nbert for self-supervised learning of language representations.\nIn ICLR, 2019. 2\n[35] Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, and Xi-\naodong He. Stacked cross attention for image-text matching.\nIn ECCV, 2018. 2\n[36] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvinine-\njad, Abdelrahman Mohamed, Omer Levy, Veselin Stoy-\nanov, and Luke Zettlemoyer. BART: Denoising sequence-to-\nsequence pre-training for natural language generation, trans-\nlation, and comprehension. 2019. 1\n[37] Christy Y Li, Xiaodan Liang, Zhiting Hu, and Eric P Xing.\nHybrid retrieval-generation reinforced agent for medical im-\nage report generation. In NeurIPS, 2018. 6\n[38] Guang Li, Linchao Zhu, Ping Liu, and Yi Yang. Entangled\ntransformer for image captioning. In ICCV, 2019. 2\n[39] Siming Li, Girish Kulkarni, Tamara Berg, Alexander Berg,\nand Yejin Choi. Composing simple image descriptions using\nweb-scale n-grams. In CoNLL, 2011. 2\n[40] Xiangyang Li and Shuqiang Jiang. Know more say less: Im-\nage captioning based on scene graphs.IEEE Transactions on\nMultimedia, 21(8), 2019. 2\n[41] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei\nHu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu\nWei, et al. Oscar: Object-semantics aligned pre-training for\nvision-language tasks. In ECCV. Springer, 2020. 2, 5, 6\n[42] Yuan Li, Xiaodan Liang, Zhiting Hu, and Eric P Xing. Hy-\nbrid retrieval-generation reinforced agent for medical image\nreport generation. In NeurIPS. 2018. 1\n[43] Yehao Li, Ting Yao, Yingwei Pan, Hongyang Chao, and Tao\nMei. Pointing novel objects in image captioning. In CVPR,\n2019. 2\n[44] Chin-Yew Lin and Eduard Hovy. Manual and automatic eval-\nuation of summaries. In Proceedings of the ACL-02 Work-\nshop on Automatic Summarization-Volume 4, 2002. 5\n[45] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll¬¥ar, and C Lawrence\nZitnick. Microsoft COCO: Common objects in context. In\nECCV. Springer, 2014. 1, 4\n[46] Nelson F Liu, Matt Gardner, Yonatan Belinkov, Matthew E\nPeters, and Noah A Smith. Linguistic knowledge and trans-\nferability of contextual representations. In NAACL, 2019. 3\n[47] Siqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama, and\nKevin Murphy. Improved image captioning via policy gradi-\nent optimization of SPIDEr. In ICCV, 2017. 2\n[48] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar\nJoshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-\nmoyer, and Veselin Stoyanov. RoBERTa: A robustly opti-\nmized BERT pretraining approach. arXiv Preprint, arXiv\n1907.11692, 2019. 1\n[49] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar\nJoshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-\nmoyer, and Veselin Stoyanov. Roberta: A robustly optimized\nbert pretraining approach. 2019. 2\n[50] Ilya Loshchilov and Frank Hutter. Fixing weight decay reg-\nularization in adam. 2018. 12\n[51] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vil-\nbert: Pretraining task-agnostic visiolinguistic representa-\ntions for vision-and-language tasks. In Hanna M. Wallach,\nHugo Larochelle, Alina Beygelzimer, Florence d‚ÄôAlch ¬¥e-\nBuc, Emily B. Fox, and Roman Garnett, editors, NeurIPS,\n2019. 2\n[52] Jiasen Lu, Caiming Xiong, Devi Parikh, and Richard Socher.\nKnowing when to look: Adaptive attention via a visual sen-\ntinel for image captioning. In CVPR, 2017. 2\n[53] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh.\nNeural baby talk. In CVPR, 2018. 2\n[54] Stephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. Pointer sentinel mixture models. 2017. 2\n[55] Tom ¬¥aÀás Mikolov, Stefan Kombrink, Luk ¬¥aÀás Burget, Jan\nÀáCernock`y, and Sanjeev Khudanpur. Extensions of recurrent\nneural network language model. In ICASSP. IEEE, 2011. 2\n[56] Yingwei Pan, Ting Yao, Yehao Li, and Tao Mei. X-linear\nattention networks for image captioning. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10971‚Äì10980, 2020. 5\n[57] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing\nZhu. Bleu: a method for automatic evaluation of machine\ntranslation. In ACL, 2002. 4\n[58] Karl Pearson. X. on the criterion that a given system of de-\nviations from the probable in the case of a correlated system\nof variables is such that it can be reasonably supposed to\nhave arisen from random sampling. The London, Edinburgh,\nand Dublin Philosophical Magazine and Journal of Science,\n50(302), 1900. 8\n[59] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gard-\nner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\nDeep contextualized word representations. In NAACL-HLT,\n2018. 2\n[60] Di Qi, Lin Su, Jia Song, Edward Cui, Taroon Bharti, and\nArun Sacheti. Imagebert: Cross-modal pre-training with\nlarge-scale weak-supervised image-text data. arXiv preprint\narXiv:2001.07966, 2020. 2\n[61] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya\nSutskever. Improving language understanding by generative\npre-training. 2018. 1, 2\n[62] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, and Ilya Sutskever. Language models are unsuper-\nvised multitask learners. OpenAI blog, 1(8), 2019. 1, 2, 3\n[63] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J. Liu. Exploring the limits of transfer learning with a\nunified text-to-text transformer. Journal of Machine Learn-\ning Research, 21:1‚Äì67, 2020. 1\n[64] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks. In Advances in neural information pro-\ncessing systems, 2015. 12\n[65] Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret\nRoss, and Vaibhava Goel. Self-critical sequence training for\nimage captioning. In CVPR, 2017. 2, 4, 6\n[66] Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret\nRoss, and Vaibhava Goel. Self-critical sequence training for\nimage captioning. In CVPR, 2017. 12\n[67] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor\nDarrell, and Kate Saenko. Object hallucination in image cap-\ntioning. In EMNLP, 2018. 8\n[68] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural\nmachine translation of rare words with subword units. In\nACL. The Association for Computer Linguistics, 2016. 12\n[69] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\nSoricut. Conceptual captions: A cleaned, hypernymed, im-\nage alt-text dataset for automatic image captioning. In ACL,\n2018. 1, 4\n[70] Rakshith Shetty, Marcus Rohrbach, Lisa Anne Hendricks,\nMario Fritz, and Bernt Schiele. Speaking the same language:\nMatching machine to human captions by adversarial training.\nIn ICCV, 2017. 2\n[71] Richard Socher and Li Fei-Fei. Connecting modalities:\nSemi-supervised segmentation and annotation of images us-\ning unaligned text corpora. In CVPR. IEEE, 2010. 2\n[72] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu\nWei, and Jifeng Dai. Vl-bert: Pre-training of generic visual-\nlinguistic representations. In ICLR, 2020. 2\n[73] Hao Tan and Mohit Bansal. LXMERT: learning cross-\nmodality encoder representations from transformers. In Ken-\ntaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors,\nEMNLP-IJCNLP. ACL, 2019. 2\n[74] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 3,\n5, 7, 13\n[75] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi\nParikh. Cider: Consensus-based image description evalua-\ntion. In CVPR, 2015. 5\n[76] Subhashini Venugopalan, Lisa Anne Hendricks, Marcus\nRohrbach, Raymond Mooney, Trevor Darrell, and Kate\nSaenko. Captioning images with diverse objects. In CVPR,\n2017. 2\n[77] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Du-\nmitru Erhan. Show and tell: Lessons learned from the 2015\nMSCOCO image captioning challenge. TPAMI, 39(4), 2016.\n2\n[78] Qingzhong Wang and Antoni B. Chan. Cnn+cnn: Convo-\nlutional decoders for image captioning. arXiv 1805.09019,\n2018. 2\n[79] Yufei Wang, Zhe Lin, Xiaohui Shen, Scott Cohen, and Garri-\nson W Cottrell. Skeleton key: Image captioning by skeleton-\nattribute decomposition. In CVPR, 2017. 2\n[80] Yike Wu, Shiwan Zhao, Jia Chen, Ying Zhang, Xiaojie Yuan,\nand Zhong Su. Improving captioning for low-resource lan-\nguages by cycle consistency. In 2019 IEEE International\nConference on Multimedia and Expo (ICME) , pages 362‚Äì\n367, 2019. 1\n[81] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron\nCourville, Ruslan Salakhudinov, Rich Zemel, and Yoshua\nBengio. Show, attend and tell: Neural image caption gen-\neration with visual attention. In ICML, 2015. 1, 2\n[82] Xu Yang, Kaihua Tang, Hanwang Zhang, and Jianfei Cai.\nAuto-encoding scene graphs for image captioning. InCVPR,\n2019. 2\n[83] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell,\nRuss R Salakhutdinov, and Quoc V Le. Xlnet: General-\nized autoregressive pretraining for language understanding.\nIn NeurIPS, 2019. 1\n[84] Zhilin Yang, Ye Yuan, Yuexin Wu, William W Cohen, and\nRuss R Salakhutdinov. Review networks for caption genera-\ntion. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R.\nGarnett, editors, NeurIPS, volume 29, 2016. 2\n[85] Benjamin Z Yao, Xiong Yang, Liang Lin, Mun Wai Lee, and\nSong-Chun Zhu. I2t: Image parsing to text description. Pro-\nceedings of the IEEE, 98(8), 2010. 2\n[86] Ting Yao, Yingwei Pan, Yehao Li, and Tao Mei. Exploring\nvisual relationship for image captioning. In ECCV, 2018. 2\n[87] Ting Yao, Yingwei Pan, Yehao Li, and Tao Mei. Hierarchy\nparsing for image captioning. In ICCV, 2019. 2\n[88] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang,\nLei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao.\nVinvl: Revisiting visual representations in vision-language\nmodels. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 5579‚Äì\n5588, 2021. 2\n[89] Mingchen Zhuge, Dehong Gao, Deng-Ping Fan, Linbo Jin,\nBen Chen, Haoming Zhou, Minghui Qiu, and Ling Shao.\nKaleido-bert: Vision-language pre-training on fashion do-\nmain. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition , pages 12647‚Äì12657,\n2021. 2\nA. Supplementary material\nA.1. Additional implementation details\nImage and Word Features. Following [2], we use a Faster\nR-CNN networks [64] with ResNet-101 [23] as a backbone\nto train on Visual Genome dataset [32], and we extract a\n2048-dimensional feature vector for each object.\nWe use the Byte Pair Encoding (BPE) [68], which ef-\nfectively incorporate sub-word information and is benefi-\ncial for dealing with out-of-vocabulary words. We employ\nlearnable positional encoding and initialize token embed-\nding from pretrained weights of GPT-2.\nArchitecture and Hyperparameters. We have 3 layers in\nthe encoder and 12 layers in the decoder with 12 heads in\neach layer. The hidden size D in each layer is 768. We load\nthe GPT-2 (small) pretrained weights, which has 117M pa-\nrameters into the decoder. We use the learning rate of 1e‚àí4\nunder XE loss and 1e‚àí5 during the reinforcement learning.\nWe train the models with the AdamW optimizer [50] and a\nbatch size 25. The beam size is equal to 5. The threshold œÑ\nis tuned on the validation set for different training data.\nTraining Details. We train all the models in two steps.\nWe first train the models with cross-entropy (XE) loss and\nthen finetune them using reinforcement learning. The cross-\nentropy loss LXE is the traditional autoregressive classifi-\ncation loss\nLXE = ‚àí\nTX\nt=1\nlog((wt|w1:t‚àí1)) (6)\nwhere w1:T represents the target ground truth sequence.\nFor reinforcement learning, we employ a variant of Self-\nCritical Sequence training [66]. Following [13], we sample\nL sentences, ÀÜw1\n1:T , . . . ,ÀÜwL\n1:T , with beam search and use the\nmean reward from the L sentences as the baseline b. The\ngradient is\n‚àáŒ∏LRL(Œ∏) =‚àí1\nk\nLX\ni=1\n\u0012\n(r( ÀÜwi\n1:T ) ‚àí b)‚àáŒ∏ log p( ÀÜwi\n1:T )\n\u0013\n(7)\nwhere r(¬∑) represents the CIDEr-D reward.\nA.2. Train VisualGPT with more COCO and Con-\nceptual Caption Datasets\nFigure 8 shows other results obtained by training net-\nworks on the 5% , 10%, 20%, 50% and 100% (82,783 im-\nages) MS COCO data. Figure 9 shows the performance with\nthe data scaling up to 2.5% (82,958 images) Conceptual\nCaptions, in which the dataset scale is similar to the whole\nCOCO data. For MS COCO, VisualGPT outperforms other\nbaseline models when we sample ‚â§ 20% training data. For\nConceptual Caption, VisualGPT consistently outperforms\nall the baselines when we sample ‚â§ 2.5% training images.\nFigure 8. Evaluation on different percentage of COCO data\nFigure 9. Evaluation on different percentage of Conceptual Cap-\ntions\nThe whole experiments highlight our model‚Äôs effectiveness\non low data regimes.\nOn the other hand, we should also notice thatM2 Trans-\nformer surpasses the VisualGPT‚Äôs performance when there\nare 50% and 100% COCO training data. But when we train\nwith the same number of Conceptual images, VisualGPT\ncontinuously outperforms all the baselines. This leads us\nto think of the reason why VisualGPT show different per-\nforming behaviors on these two datasets. The difference\nbetween these two datasets is that the Conceptual Captions\ncontain more diverse vocabularies and image contents. In\ncontrast, COCO captions only cover 80 common image ob-\njects. Therefore, the appearance frequency for each word\nin COCO is much higher than that in Conceptual Captions\nand COCO vocabulary diversity is also much lower than\nConceptual Caption. We hypotheses the reason for this per-\nformance difference is that when the captions have a small\ncoverage of each word, the caption generation will be ben-\nefited a lot from the GPT inherent knowledge and GPT can\nhelp the model quickly adapt into the new domain. But\nwhen there is a lot of in-domain data, the current image-\ncaptioning models can already generalize well on it and it\npotentially contradicts to the GPT original knowledge.\nA.3. Attention over Different types of words\nWe use the Spacy parser to detect the part-of-speech of\nwords in captions and calculate the mean value of the vi-\nsual attention score. The result is presented in Fig. 10.\nWe found PoS that tend to visual content, like noun (0.71),\nverb (0.71) and adjective (0.72), have high visual attention\nscores, whereas linguistic PoS like pronoun (0.53), punctu-\nation (0.58), and determiner (0.61) receive low attention.\nFigure 10. Attention Scores over different part-of-speech words\nA.4. More Qualitative Examples\nIn Figure 11, we provide more examples of visual atten-\ntions. Blue indicates high visual scores and red indicates\nlow visual scores. We can observe that VisualGPT assigns\nhigher scores to words like ‚Äústeam engine‚Äù, ‚Äúelephants‚Äù,\n‚Äúhorse‚Äù, ‚Äúlush‚Äù and ‚Äúcabinets‚Äù, and it assigns low visual\nscores to determiners and prepositions like ‚Äúto‚Äù and ‚Äúat‚Äù.\nWe also show some examples of generated captions by\nour VisualGPT and several strong baseline models includ-\ning Transformer ( 3 layers) [74], M2 Transformer (3 lay-\ners) [13] and AoA Transformer [25] in the Table 6, Table 7\nand Table 8. Overall, we can observe that our VisualGPT is\nable to describe the image content more accurately than the\nbaseline models.\nGT:the large red flower is inside of a clear glass vase\nGT:a motorcycle parked next to a white building\nGT:a kitchen with wooden cabinets a sink and a dishwasher\nGT:a tennis player jumps and hits a ball\nGT:asmallboatsinabodyofwater\nOurs a red vase of rosessittingon top of a glass\nattention0.8 0.93 0.94 0.64 0.87 0.84 0.67 0.55 0.57 0.43 0.86\nOurs a tennisplayerjumpingon a tenniscourtholdinga ball\nattention0.7 0.77 0.75 0.72 0.670.640.89 0.79 0.74 0.6 0.76\nOurs a motorcycle parked next to a building\nattention 0.6 0.78 0.85 0.74 0.34 0.6 0.75\nOurs a large boat sits on a field with a lake\nattention 0.6 0.77 0.78 0.83 0.71 0.6 0.74 0.66 0.63 0.73\nOurs a kitchen with  a whitecabinetsand a sink\nattention 0.73 0.86 0.8 0.7 0.9 0.91 0.8 0.8 0.9\nGT:a train sitting under a display inside a building\nGT:two captive elephants stand bored behind the fake stone fence\nGT:a white horse standing in a field on top of grass\nGT:a man in a restaurant smiling while holding up a camera\nGT:a man sitting on a bench next to a few bags\nOurs a white horse grazing on a lush green field\nattention 0.67 0.75 0.83 0.74 0.65 0.66 0.85 0.8 0.77\nOurs elephants standing next to a stone fence\nattention 0.8 0.74 0.77 0.47 0.5 0.77 0.76\nOurs a steam engine sitting in a display\nattention 0.69 0.84 0.79 0.8 0.7 0.6 0.83\nOurs a man in a store looking at his camera\nattention 0.65 0.69 0.72 0.67 0.77 0.65 0.47 0.49 0.7\nOurs a young man holding a backpackon a bench\nattention0.7 0.82 0.74 0.7 0.54 0.84 0.59 0.55 0.83\nFigure 11. More examples of visual attention for each word in\ngenerated captions. High visual scores are in blue and low scores\nin red.\nImage Generated Captions Ground Truth\nTransformer: a woman riding some skis on skis\nM2 Transformer: a couple of skiers are\nstanding near the snow\nAoA Transformer: a man with skis in the snow\nVisualGPT (ours): a group of people walk on a\nsnowy mountain\nGT1: the people are walking through snow\nin a wooded area\nGT2: two people wearing skis traveling\nthrough the snow\nGT3: a man is walking down a path covered\nin a snow\nGT4: a couple is skiing through the\nsnowy woods\nGT5: a couple of people that are in a\nsnowy field\nTransformer: a street that has some street\nin it\nM2 Transformer: a traffic light over a street\nlight under a traffic light\nAoA Transformer: a street with people on a\ncity street\nVisualGPT (ours): a street with tall signs and\ntraffic signs\nGT1: a yellow traffic light above a street\nnext to houses\nGT2:a street scene of an intersection with a\nstreet light\nGT3: a stop light hanging over an\nintersection in a residential area\nGT4: a traffic signal at an intersection is\nsuspended on wire\nGT5: a street intersection with a traffic\nlight over it\nTransformer: some pizza are sitting on a\nplate\nM2 Transformer: a plate with food and\na knife on it\nAoA Transformer: a plate of pizza on a table\nVisualGPT (ours): a plate of bread are served\non a table\nGT1: a batch of bread slices sitting on a\nplate\nGT2: a plate with some pieces of bread on it\nGT3: sliced french bread is on a plat that\nis lying on a table\nGT4: bread that is sitting on a plate that is\non a table\nGT5: a white plate with lots topped with\ngarlic bread\nTransformer: two tennis player playing tennis\non the ball\nM2 Transformer: a tennis player about to\nhit a ball\nAoA Transformer: a baseball players on a game\nplaying a game\nVisualGPT (ours): a tennis player hits a ball\nwith a racket\nGT1: a man holding a racquet on top of a\ntennis court\nGT2: a man with a tennis racket reaches\nfor a ball\nGT3: a man with a tennis racket is running\non a court\nGT4: a young man is playing a game of\ntennis\nGT5: a tennis player in a blue shirt\nruns toward a ball\nTransformer: a group of birds that are\nstanding in the grass\nM2 Transformer: a flock of birds perched\nin a tree branch\nAoA Transformer: several giraffe are standing\nnext to each trees\nVisualGPT (ours): a bird standing in the\nmiddle of a pond\nGT1: a bird is perched a top a branch over\na river\nGT2: a bird sits on a branch above a stream\nGT3: a bird on top of a tree branch over\nwater\nGT4: a picture of an outside region that\nappears incredible\nGT5: a bird on a fallen branch in a body of\nwater\nTable 6. Caption generated by our VisualGPT, Transformer,M2 Transformer and AoA Transformer on 0.1% MS COCO data split\nImage Generated Captions Ground Truth\nTransformer: several boats are sitting in\nthe middle of a lake\nM2 Transformer: a boat filled with\nboats floating in the water\nAoA Transformer: an empty boat that has\nwater and water\nVisualGPT (ours): a canal filled with boats\nin the water\nGT1: a blue boat docked on a green lush\nshore\nGT2: a small marina with boats docked there\nGT3: a group of boats sitting together with\nno one around\nGT4: some boats parked in the water at\na dock\nGT5: boats sitting around the side of a\nlake by a tree\nTransformer:pizza slices and pizza in a\nplate covered pizza\nM2 Transformer: people sitting at a\ntable eating pizza and other salad\nAoA Transformer: two pizza eating a table with\npizza on the table\nVisualGPT (ours): a group of pizza on a\niron plate with toppings\nGT1: a set of five pizzas sitting next\nto each other each with different toppings\nGT2:a handful of prepared pizzas sit next\nto each other\nGT3: five uncooked pizzas with a variety\nof different toppings\nGT4: five unbaked pizzas that include\nvarious types of cheeses\nGT5: five different pizzas are being\nprepared over a metal tray\nTransformer: a dog holding a frisbee in the water\nM2 Transformer: a dog holding a frisbee in\na body of water\nAoA Transformer: a dog walking during a frisbee\nin a stone day\nVisualGPT (ours):a dog walking through the water\nwith a frisbee\nGT1:two dogs are playing on the beach\ncatching a frisbee\nGT2: of two dogs only one may be the victor\nGT3: a dog catching a frisbee by another\ndog on a beach\nGT4: dog jumping up in the air to catch a\nfrisbee in the summer time\nGT5: a dog jumping up into the air to\ncatch a frisbee\nTransformer: a group of people taking a\nchild in a in a building\nM2 Transformer: a group of people in\nan airport with their hands\nAoA Transformer: a picture of a young\ngroup of people standing for men\nVisualGPT (ours): a group of people\nstanding around a tv\nGT1: a group of men standing around a room\nGT2: some people are waiting in a long room\nGT3: people are standing in a room looking\nat a television screen\nGT4: a person sitting on a bench while the\nrest look somehwere else\nGT5: a man in red winter clothes sits on\na bench with people behind him gather in\nfront of a tv\nTransformer: an elephant eating a elephant\nhas a elephant\nM2 Transformer: elephant with its trunk\nwith their elephant with its trunk\nAoA Transformer: two elephants standing at\na lot of trees\nVisualGPT (ours): three elephants standing\nnext to some trees\nGT1: two adult elephants are surrounding\na baby elephant\nGT2: a baby elephant kneeling in front of\ntwo bigger elephants\nGT3: a baby elephant and it ‚Äôs parents\neat fruit\nGT4: elephants eat fruit a baby elephant\nrummaging in the food\nGT5: a pair of adult elephants with a baby\nelephant eat from a pile of fruit\nTable 7. Caption generated by our VisualGPT, Transformer,M2 Transformer and AoA Transformer on 0.5% MS COCO data split\nImage Generated Captions Ground Truth\nTransformer: a man in a suit and a woman\nstanding in a shop\nM2 Transformer: a man is standing in\na shop with a people holding people\nAoA Transformer: a man is working on a bus\nin a\nVisualGPT (ours): a group of people standing\nat an airport with their luggage\nGT1: several people are purchasing tickets\nat a bus station\nGT2: some people are checking in at the\nticket counter somewhere in asia\nGT3: people waiting in line with luggage\nat a ticket counter\nGT4: people are standing near an airport\nticket kiosk\nGT5: customers stand at a kiosk waiting\nfor tickets\nTransformer: a bus that is parked in front\nof a building\nM2 Transformer: a couple of people walking\ndown the side of a street\nAoA Transformer: a bus is parked in a city\nstreet\nVisualGPT (ours): a while and blue bus is\nparked on the side of a city street\nGT1: people standing outside of a blue and\nwhite bus\nGT2: an image of a tour bus that is picking\npeople up\nGT3: several people standing around buses\nand most wearing orange vests\nGT4: a public transit bus pulling up to pick\nup passengers\nGT5: a city bus at a stop waiting to pick up\npassengers\nTransformer: a blue and white airplane flying\nthrough a sky\nM2 Transformer: an air plane flying in the\nair\nAoA Transformer: a plane airplane flying\ndown in the sky\nVisualGPT (ours): a plane is flying in the air\nover the trees\nGT1: there ‚Äôs and airplane in the sky flying\nover some trees\nGT2: a large plane is flying over a crowd\nof trees\nGT3: a aeroplane soaring high in the sky\nabove the trees\nGT4: a passenger plane flies in the sky\nover a forest\nGT5: an airplane is seen flying over several\ntrees\nTransformer: a white toilet sitting in a\nwhite bathroom next to a sink\nM2 Transformer: a cat sitting in the toilet\nAoA Transformer: a bathroom with a toilet\nand a sink\nVisualGPT (ours): a cat sitting on top of a\nbathroom sink\nGT1: a cat climbing into a bathroom sink\nlooking at someone\nGT2: a cat looks up as it stands in the\nbathroom sink\nGT3: a large cat stands inside of a clean\nbathroom sink\nGT4: cat is caught stepping in to the\nbathroom sink\nGT5: a cute kitty cat in the sink of a\nbathroom near a brush and other items\nTransformer: a little girl is eating a\nbirthday cake\nM2 Transformer: a child and a child are\nsitting at a table with table with table\nAoA Transformer: two children sitting at a\ntable with a laptop computer\nVisualGPT (ours): a woman and a girl sitting\nat a table with a birthday cake\nGT1: a woman and child stand next to a\ntable with cake on it\nGT2: a lady standing near the table with a\nbaby is posing for the camera\nGT3: a woman stands beside a baby in a\nhigh chair a table is set with a birthday\ncake and champagne\nGT4: a woman setting up her house for a\nparty\nGT5: a person standing next to a child in a\nbooster seat\nTable 8. Caption generated by our VisualGPT, Transformer,M2 Transformer and AoA Transformer on 1% MS COCO data split",
  "concepts": [
    {
      "name": "Closed captioning",
      "score": 0.9831681847572327
    },
    {
      "name": "Computer science",
      "score": 0.8303794264793396
    },
    {
      "name": "Encoder",
      "score": 0.7057794332504272
    },
    {
      "name": "Language model",
      "score": 0.6545464992523193
    },
    {
      "name": "Image (mathematics)",
      "score": 0.6394948959350586
    },
    {
      "name": "Code (set theory)",
      "score": 0.6105616092681885
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5391010046005249
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4850232005119324
    },
    {
      "name": "Natural language processing",
      "score": 0.4584363102912903
    },
    {
      "name": "Decoding methods",
      "score": 0.44584453105926514
    },
    {
      "name": "Baseline (sea)",
      "score": 0.41280314326286316
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.4113812744617462
    },
    {
      "name": "Speech recognition",
      "score": 0.3972465395927429
    },
    {
      "name": "Machine learning",
      "score": 0.35646528005599976
    },
    {
      "name": "Programming language",
      "score": 0.10418882966041565
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ],
  "topic": "Closed captioning",
  "institutions": [],
  "cited_by": 14
}