{
  "title": "Deep Learning Approaches Based on Transformer Architectures for Image Captioning Tasks",
  "url": "https://openalex.org/W4220798687",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2104876903",
      "name": "Roberto Castro",
      "affiliations": [
        "Universidad Yachay Tech"
      ]
    },
    {
      "id": "https://openalex.org/A2631852102",
      "name": "Israel Pineda",
      "affiliations": [
        "Universidad Yachay Tech"
      ]
    },
    {
      "id": "https://openalex.org/A2120424394",
      "name": "Wansu Lim",
      "affiliations": [
        "Kumoh National Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2225692485",
      "name": "Manuel Eugenio Morocho Cayamcela",
      "affiliations": [
        "Universidad Yachay Tech"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963084599",
    "https://openalex.org/W1969616664",
    "https://openalex.org/W6639118148",
    "https://openalex.org/W1600535962",
    "https://openalex.org/W2160746645",
    "https://openalex.org/W2038175477",
    "https://openalex.org/W2302086703",
    "https://openalex.org/W3083321496",
    "https://openalex.org/W3215023232",
    "https://openalex.org/W6680834391",
    "https://openalex.org/W6679434410",
    "https://openalex.org/W6682137061",
    "https://openalex.org/W6628927728",
    "https://openalex.org/W6685230081",
    "https://openalex.org/W6638742206",
    "https://openalex.org/W1895577753",
    "https://openalex.org/W1947481528",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6767415207",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W3109530205",
    "https://openalex.org/W6780169642",
    "https://openalex.org/W6789705400",
    "https://openalex.org/W2997591391",
    "https://openalex.org/W3148223059",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3013128006",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W1484210532",
    "https://openalex.org/W2971675178",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3124149278",
    "https://openalex.org/W4287755362",
    "https://openalex.org/W1811254738"
  ],
  "abstract": "This paper focuses on <italic>visual attention</italic>, a state-of-the-art approach for image captioning tasks within the computer vision research area. We study the impact that different hyperparemeter configurations on an encoder-decoder visual attention architecture in terms of efficiency. Results show that the correct selection of both the cost function and the gradient-based optimizer can significantly impact the captioning results. Our system considers the cross-entropy, Kullback-Leibler divergence, mean squared error, and negative log-likelihood loss functions; the adaptive momentum (Adam), AdamW, RMSprop, stochastic gradient descent, and Adadelta optimizers. Experimentation shows that a combination of cross-entropy with Adam is the best alternative returning a Top-5 accuracy value of 73.092 and a BLEU-4 value of 20.10. Furthermore, a comparative analysis of alternative convolutional architectures demonstrated their performance as an encoder. Our results show that ResNext-101 stands out with a Top-5 accuracy of 73.128 and a BLEU-4 of 19.80; positioning itself as the best option when looking for the optimum captioning quality. However, MobileNetV3 proved to be a much more compact alternative with 2,971,952 parameters and 0.23 Giga fixed-point Multiply-Accumulate operations per Second (GMACS). Consequently, MobileNetV3 offers a competitive output quality at the cost of lower computational performance, supported by values of 19.50 and 72.928 for the BLEU-4 and Top-5 accuracy, respectively. Finally, when testing vision transformer (ViT), and data-efficient image transformer (DeiT) models to replace the convolutional component of the architecture, DeiT achieved an improvement over ViT, obtaining a value of 34.44 in the BLEU-4 metric.",
  "full_text": "Received February 23, 2022, accepted March 9, 2022, date of publication March 22, 2022, date of current version March 31, 2022.\nDigital Object Identifier 10.1 109/ACCESS.2022.3161428\nDeep Learning Approaches Based on Transformer\nArchitectures for Image Captioning Tasks\nROBERTO CASTRO\n 1, (Student Member, IEEE), ISRAEL PINEDA\n1, (Member, IEEE),\nWANSU LIM\n 2, (Member, IEEE), AND\nMANUEL EUGENIO MOROCHO-CAYAMCELA\n 1, (Member, IEEE)\n1Deep Learning for Autonomous Driving, Robotics, and Computer Vision Research Group (DeepARC Research), School of Mathematical and Computational\nSciences, Yachay Tech University, Urcuquí 100119, Ecuador\n2Future Communications and Systems Laboratory (FCSL), Department of Aeronautics, Mechanical and Electronic Convergence Engineering, Kumoh National\nInstitute of Technology, Gumi-si, Gyeongbuk 39177, Republic of Korea\nCorresponding authors: Wansu Lim (wansu.lim@kumoh.ac.kr) and Manuel Eugenio Morocho-Cayamcela\n(mmorocho@yachaytech.edu.ec)\nThis work was supported in part by the Ministry of SMEs and Start-ups, South Korea, under Grant S3010704; and in part by the National\nResearch Foundation of Korea under Grant 2020R1A4A101777511 and Grant 2021R1I1A3056900.\nABSTRACT This paper focuses on visual attention, a state-of-the-art approach for image captioning tasks\nwithin the computer vision research area. We study the impact that different hyperparemeter conﬁgurations\non an encoder-decoder visual attention architecture in terms of efﬁciency. Results show that the correct\nselection of both the cost function and the gradient-based optimizer can signiﬁcantly impact the captioning\nresults. Our system considers the cross-entropy, Kullback-Leibler divergence, mean squared error, and\nnegative log-likelihood loss functions; the adaptive momentum (Adam), AdamW, RMSprop, stochastic\ngradient descent, and Adadelta optimizers. Experimentation shows that a combination of cross-entropy\nwith Adam is the best alternative returning a Top-5 accuracy value of 73.092 and a BLEU-4 value of\n20.10. Furthermore, a comparative analysis of alternative convolutional architectures demonstrated their\nperformance as an encoder. Our results show that ResNext-101 stands out with a Top-5 accuracy of\n73.128 and a BLEU-4 of 19.80; positioning itself as the best option when looking for the optimum\ncaptioning quality. However, MobileNetV3 proved to be a much more compact alternative with 2,971,952\nparameters and 0.23 Giga ﬁxed-point Multiply-Accumulate operations per Second (GMACS). Consequently,\nMobileNetV3 offers a competitive output quality at the cost of lower computational performance, supported\nby values of 19.50 and 72.928 for the BLEU-4 and Top-5 accuracy, respectively. Finally, when testing\nvision transformer (ViT), and data-efﬁcient image transformer (DeiT) models to replace the convolutional\ncomponent of the architecture, DeiT achieved an improvement over ViT, obtaining a value of 34.44 in the\nBLEU-4 metric.\nINDEX TERMS Image captioning, visual attention, computer vision, supervised learning, artiﬁcial\nintelligence.\nI. INTRODUCTION\nImage captioning is a branch of computer vision whose\nmain objective is the generation of accurate and organic\ntext descriptions of any type of scenario portrayed in an\nimage or frame [1]. Traditional approaches (i.e., before\nthe neural network’s era) tackled the image captioning\nproblem using classical image processing methodologies\nthat usually relied on the generation of templates together\nwith object detection to produce the caption given an input\nimage [2], [3]. Following a similar line to the use of\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Shadi Alawneh\n.\nimage templates, the construction of pattern recognition\nsystems has made a meritorious historical space in the\nresolution of computer vision tasks involving images, as in\nthe case of content-based image retrieval problems [4].\nMoreover, the incorporation of fuzzy logic was of great\ninterest over time as it positioned itself as a popular method\nthat maps labels from previously extracted features [5], [6].\nAs a consequence of the emerging techniques, joined\nto the usage of neural structures, visual attention has\nemerged as a high potential alternative, proposing to replicate\nhuman vision by enabling an emulation of attention by\nthe neural network on the most relevant sections of an\nimage [7].\nVOLUME 10, 2022 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 33679\nR. Castroet al.: Deep Learning Approaches Based on Transformer Architectures for Image Captioning Tasks\nSeveral researchers have replicated the state-of-the-art\nimplementation proposed by Xu et al.for further study [8].\nThe latter convolutional architecture can be broadly divided\ninto two well-deﬁned structures. On the one hand, a convo-\nlutional network, which takes as input the raw images to be\nprocessed, while it outputs a set of feature vectors, each of\nwhich represents a D-dimensional part of a section of the\nillustration. Thus, the decoding part of the model will be\nable to selectively focus on speciﬁc parts of the image by\nmaking use of subsets of the feature vectors. In addition,\na long short-term memory (LSTM) network makes use of\nthe previous output to generate a word at each time instant in\ndependence on a context vector, previously generated words,\nand the previous hidden state.\nModern artiﬁcial intelligence models provide promising\nresults for the captioning problem. However, one of the\nremaining challenges is the optimization of hyperparameters\nwhich is far from trivial and remains a challenge for\ncaptioning and other applications [9].\nIn this paper, three experimental scenarios are examined\nwith the Show, Attend and Tellarchitecture as the object of\nstudy. First, we conduct a study that serves as complementary\ncontent to our paper, seeking to leave tangible evidence that\nsupport the general conﬁguration of the original contribution.\nOtherwise stated, alternatives that equal or exceed the perfor-\nmance obtained in the benchmark work. In order to achieve\nthe previously mentioned objective, it was decided to study\nthe performance impact of different model hyperparameters,\nconducting a comparative study to select the cost function\nthat minimizes the training error over a certain number of\nepochs for our speciﬁc application, setting the optimizer as\na ﬁxed variable. Then, the same principle is applied to test\ndifferent gradient-based optimizers with the cost function as\nan independent variable. As a second experiment, once the\noptimal conﬁguration of hyperparameters was established,\nwe sought to study the performance and computational\nrequirements that various convolutional models can achieve\nby replacing the original encoder. And ﬁnally, to analyze\nthe viability of recent models that leave aside the notion\nof convolutions, we tested the performance of architectures\nbased on transformers, replacing the encoder component of\nthe baseline original work.\nIn response to the uncertainties raised by the previously\ndescribed experimental scenarios, the combination of cross-\nentropy loss and Adam optimizer was highlighted as the\nbest hyperparameter conﬁguration according to the Top-\n5 Accuracy, BLEU-4, and loss value metrics. By reusing\nthis conﬁguration for the following experiment, different\ndecisions can be made depending on the ﬁnal purpose of\nthe researcher [10]. If the architecture with the best metrics\nconcerning response quality is required, the convolutional\nmodels ResNet-152 and ResNeXt-101 provided the best\nresults in the metrics used in the previous experimentation.\nOn the other hand, looking for the alternative with the lowest\ncomputational demands, the MobileNet V3 model is the\nmost attractive, decreasing the number of parameters, training\ntime, and inference, together with the giga ﬁxed-point\nmultiply-accumulate operations per second (GMACs), with-\nout sacriﬁcing the accuracy metrics considerably. Finally,\nas the last experimental scene, it was decided to dispense\nwith the original encoder used by the benchmark architecture\nin order to decide for alternatives outside the convolutional\nprinciples. Two different transformer-based models, initially\nconceived for image classiﬁcation tasks, were selected\nfor this last examination. According to the corresponding\nresults, an improvement of state of the art in terms of the\nBLEU-4 metric was obtained when using the Vision Trans-\nformer (ViT) and Data-efﬁcient Image Transformer (DeiT)\nmodels. However, the best results were obtained when using\nthe second of these couple of models, in conjunction with a\ntraining process consisting of an initial phase where only the\ndecoder of the architecture is subjected to training, while as a\nsecond stage, the parameters that conform the last transformer\nencoder block are also optimized.\nII. RELATED WORKS\nAccording to the historical summary presented in Table 1, one\nof the pioneering research works incorporating an attention\nsystem is the one proposed by Larochelle & Hinton, based\non a variant of the restricted Boltzmann machine (RBM)\nmainly used for digit classiﬁcation. They used the benchmark\nMNIST dataset, where a limited set of pixels is provided from\nwhich the architecture collects both high- and low-resolution\ninformation about neighboring pixels [11]. Moving forward\nin the timeline, Bahdanau et al.reused the notion of attention\napplied to different convolutional architectures. In this case,\na much more novel model such as an encoder-decoder makes\nuse of a reduced but visible attention system to take into\nconsideration certain parts of a sentence when performing\nthe translation of a speciﬁc word [12]. The idea of taking\nadvantage of the beneﬁts offered by recurrent architectures\nwas a common factor that persisted in later works, among\nwhich stand out research-oriented to digit classiﬁcation such\nas that presented by Mnih et al.[13], and the one proposed\nby Ba et al.[14].\nIn order to substantiate the evolution within the area\nof image captioning, a brief historical review of relevant\nworks is presented in Table 2. Throughout this summary,\nwe can ﬁnd contributions such as the one proposed by\nKiros et al., using a multi-log bilinear modelfor exploiting\nthe characteristics of images to generate a biased version\nof this architecture [15]. Followed this research, the same\nauthor incorporated recurrent structures within an encoder-\ndecoder model, a common factor among image captioning\nproposals. This fact is mainly due to the nature of human\nspeech that is sought to be incorporated into the learning\nalgorithm. Furthermore, authors such as Mao et al. [16],\nVinyals et al.[17], and Donahue et al.[18] have reused this\nidea in their respective research efforts.\nFinally, Table. 3 contains an excerpt from previous\nworks that promote our hypothesis of incorporating a\nnon-convolutional model within the proposed benchmark.\n33680 VOLUME 10, 2022\nR. Castroet al.: Deep Learning Approaches Based on Transformer Architectures for Image Captioning Tasks\nTABLE\n1. Summary of visual attention related works.\nTABLE\n2. Summary of image captioning related works.\nTABLE\n3. Summary of related works about transformer architectures.\nVOLUME 10, 2022 33681\nR. Castroet al.: Deep Learning Approaches Based on Transformer Architectures for Image Captioning Tasks\nThe transformer architecture originates with the proposition\nthat attentional systems are sufﬁcient tools to replace\napproaches that employ recurrent networks for machine\ntranslation tasks. This architecture uses multi-head attention\nas the cornerstone of the transformer blocks contained in the\nencoding and decoding part. The authors of this work use a\nsimile with database information retrieval systems to propose\nits attentional principle, generating the key K, the query\nQ, and the value V matrices from linear projections on the\ninput. This technique is intended to divide the aforementioned\nmatrices for each attention head in order to compute the\nattention as follows:\nAttention(Q,K,V ) =softmax\n(QKT\n√dk\n)\nV (1)\nwhere dk corresponds to the embedding size used to represent\neach word [19].\nThe achievement obtained in this work is evidenced by\nan improvement in the BLEU metric for English-to-German\ntranslation tasks compared to the state-of-the-art.\nThe novel transformer architecture attracted the attention\nof engineers and practitioners by dispensing the conventional\nconvolutional or recurrent models, usually used to build\nencoders and decoders. Hence, researchers were fast to\nevaluate the feasibility of both parts that constructed this\noutstanding model.\nOn the one hand, regarding the machine translation tasks,\nthe encoder of the transformer has been sought to be used as\nan alternative for the encoding of the content coming from\nan input text. One of the main attractions of this speciﬁc\npart of the transformer is the high parallelization capacity\ndue to the nature of the multi-head attention modules. On the\nother hand, the decoder, similar to recurrent models, requires\nprevious states when generating a new word during the\ninference process. Thus, Wang et al.proposed to counteract\nthe impact of the large number of parameters of a transformer\ndecoder by replacing it with a classical LSTM network to\nperform the translation task given the output generated by\nthe transformer encoder. Thereby, the authors end up with an\narchitecture capable of decoding four times faster than using\nthe classical transformer, with a slightly lower performance\nin terms of BLEU metric [20].\nAs time went by, the scientiﬁc community became much\nmore aware of the role that both transformer parts played\nin performing translation tasks. During training, the encoder\nacquires the general understanding of the source language,\nconsidering the context in which each word was initiated.\nAt the same time, the decoder is trained to map the words\nfrom the source language to the target language. Therefore,\nthe underlying knowledge of the language that both neural\nnetwork architectures had separately granted to the scientiﬁc\ncommunity, have provided two great weapons to tackle\nnatural language tasks. On the one hand, by exploiting the\ndecoder modules of the transformer we obtain the GPT\narchitecture, whose later versions leave a hegemony mainly\nin text generation [21]. On the other hand, models such as\nBERT have been proposed to take advantage of the encoder\nmodules. The versatility of this model is undeniable at the\nmoment of performing almost any task in the area of natural\nlanguage processing by executing ﬁne-tuning according to\nthe speciﬁc application [22].\nOnce the precedent set by BERT was established, its use\nin conjunction with recurrent networks continued to be a\ngreat experimental attraction thanks to the computational\nbeneﬁts mentioned above. Thus, Chen et al. proposed the\nacceleration of sentence correction tasks in Chinese, using a\nBERT-RNN model trained by applying the TF technique as\nan additional measure to accelerate the training process. After\nexperimentation with various recurrent models functioning as\ndecoder, the BERT-GRU combination outperformed the best\nBLEU metric, and improved the inference time of the base\ntransformer model by 1131% [23].\nDespite the progressive dominance of transformer-based\nnetworks in natural language processing, the feasibility of\nthis type of architecture in the world of computer vision has\nbeen the focus of many researchers in the last couple of years.\nAn example of the ﬁrst approach to this new challenge can\nbe found in the work of Patel and Varier. They contributed\nto the research community with a comparison between a\nCNN-LSTM model and a CNN-Transformer architecture\nfor image captioning tasks on the Flickr8k dataset. This\nwork concludes by showing the feasibility of the transformer\ndecoder within the proposed architecture. However, the\nperformance metrics remained slightly behind in terms of\nBLEU, METEOR, ROGUE and CIDER in comparison to the\nclassical alternatives using LSTM networks as a decoder [24].\nSubsequently, because of the considerable impact caused\nby the work ‘‘An image is worth16×16 words: transformers\nfor image recognition at scale’’ by Dosovitskiy et al., the\nViT model was considered as a viable approach to the use\nof transformer-based architectures for computer vision. The\nauthors of this work proposed an architecture that uses the\ntransformer encoder reusing conﬁgurations from the BERT\nmodel. The output of this encoder part is then reused within\nan multi-layer percepton (MLP) layer to perform image\nclassiﬁcation. The modiﬁcation that allows this architecture\nto take an image as input, is that the corresponding input\nis previously divided into N patches, each one containing\nan speciﬁc section of the image, ensuring no overlapping\nbetween them. These image portions are then ﬂattened and\neach of these structures is treated as if it were a word within\nthe classical transformer architecture. The impact that this\npaper generated was not only due to the alternative proposed\nto use an image as input, but also for being a new state-of-\nthe-art in the task of image classiﬁcation [25].\nAfter this recent approach of using transformers for\ntasks involving images had been consolidated, the desire\nto use a full-transformer architecture for this type of tasks\ncontinued to be studied. Liu et al.proposed the use of such\nan architecture, using the ViT model as the coding part\ntogether with the classical decoder of the transformer [26].\nThis proposal was tested in image captioning tasks on the\n33682 VOLUME 10, 2022\nR. Castroet al.: Deep Learning Approaches Based on Transformer Architectures for Image Captioning Tasks\nFIGURE 1. Overall representation of the convolutional encoder-decoder architecture built to generate real captioning.\nMSCOCO dataset, obtaining an improvement of the state-of-\nthe-art in terms of BLEU, METEOR, ROGUE and CIDER\nmetrics.\nAs mentioned so far, the current trend corresponds to the\nexploitation of attentional systems based on transformers,\neven pursuing the possibility of consolidating a model\ncapable of being specialized in multiple vision-language\ntasks after a short period of ﬁne-tuning [27]. However, new\napproaches inspired by the one proposed in the Show, Attend\nand Tellwork remain on the table as ﬁerce competitors in\nthe area of image captioning. Thus, progress continues to\nbe made in the generation of descriptions in Chinese, using\narchitectures that not only continue to employ convolutional\nstructures for the extraction of features present in the images,\nbut the decoding process remains in charge of a recurrent net-\nwork, more speciﬁcally using bidirectional LSTM networks\nsupported by a fuzzy attentional module [28].\nIII. SYSTEM MODEL AND DESIGN\nThe convolutional model employed for this study is built\nfollowing an encoder-decoder architecture supported by a\nvisual attention model. The proposed neural architecture is\nschematized in Fig. 1, where an instance of the dataset is out-\nlined in order to show its operation. The encoder makes use\nof transfer learning by borrowing the original convolutional\narchitecture of Resnet [29], taking the pre-trained model from\nthe PyTorch repository. 1 This operation aims to generate an\nencoded version of the input RGB image composed by a set\nof L D-dimensional annotation/feature vectors, where each\none corresponds to a simpliﬁed representation of a part of the\noriginal image.\na ={a 1,a2,..., aL }, aL ∈RD (2)\n1https://github.com/pytorch/vision/blob/main/torchvision/models/\nresnet.py\nOn the decoder side, given the sequential nature of the\nproblem to be solved, an LSTM recursive architecture is\nconstructed [30]. Up to this point, the description of the\ninput image is generated in a word-by-word basis. At each\ndecoding step, the Att-MLP attention network uses the set of\nannotation vectors together with the previous hidden state,\npassing this output through a softmax function.\nλti =Att(ai,ht−1) (3)\nαti = exp(λti)\n∑L\nk=1 exp(λtk )\n(4)\nOnce the corresponding weights have been computed for\neach annotation vector at time t, we proceed to compute the\nvector ˆzt , which is a dynamic representation of the relevant\nparts of an image for an speciﬁc time. For the present\nwork, we analyze the deterministic approach of the original\narchitecture, parsing the context vector as a soft attention-\nweighted annotation vector.\nˆzt =\nL∑\ni=1\nαtiai (5)\nThrough this outcome, the previously generated word and\nthe previous hidden state, the LSTM network generates the\ncorresponding output word probability:\np(yt |a,yt−1\n1 ) ∝exp(L0(Eyt−1 +Lhht +Lz ˆzt )) (6)\nwhere L0, Lh, Lz, and E are learnable parameters initialized\nrandomly. The objects in Fig. 1 denoted with discontinuous\ncontours are the groundtruth components extracted from the\ndataset. Notwithstanding, those objects are only used during\nthe training phase of the model. Their nature is described in\nthe next section of the paper.\nVOLUME 10, 2022 33683\nR. Castroet al.: Deep Learning Approaches Based on Transformer Architectures for Image Captioning Tasks\nIV. THE DATASET STRUCTURE\nThe dataset used for training the network is the 2014 version\nof the MS COCO variant oriented to image captioning\ntasks [31]. Three inputs are structured in the dataset to be\nused by the neural network during the training stage. It should\nbe noted that these three components are prepared for the\ntraining, testing, and validation sets.\nA. INPUT IMAGES\nThe set of images obtained from MS COCO must have\npixels values in the domain b ∈ {0,1}to be compatible\nwith the pre-trained convolutional model used as the encoder\nblock. For the effect, a normalization of the RGB channels\nis applied using the values of µ = [0.485,0.456,0.406]\nand σ =[0.229,0.224,0.225], where µand σ represent the\nmean and the standard deviation of the ImageNet dataset [32],\nrespectively. Each image in the dataset is represented as\nX(i) ∈R256×256, where X(i) is a matrix of 256 ×256 pixels.\nWe let m be the total number of images on MS COCO dataset,\nand represent the entire dataset as X ≜ {X(1),..., X(m)},\nwhere each image X(i) is mapped to a ground truth caption\nY(i) that represents the corresponding ground-truth encoded\ncaption.\nB. ENCODED CAPTIONS\nIn order to be able to manipulate the descriptions associated\nwith each image in the dataset, the model uses a mapping\nsystem supported by a dictionary. Within this ﬁle, each\nword used in the captioning of the entire dataset has\nan identiﬁcation number. In this way, each ground-truth\nwill be represented as a numerical array according to the\nequivalences deﬁned by the mapping system.\nIn addition, the inclusion of three special characters within\nthe mapping ﬁle is required. On the one hand, the neural\nnetwork requires a start and end signal to delimit the\nextension of the descriptions. On the other hand, since not all\nthe descriptions occupy the same sentence size, it is required\nto ﬁll the missing spaces of the encoded caption with a\npadding character. Consequently, taking the longest ground-\ntruth as referral, the content of the rest of the captions is\nupdated to match the reference length by incorporating the\npadding operator. The proposed methodology normalizes the\nMS COCO dataset in arrays of 52 elements.\nAs an example, in Fig. 2 it can be seen an instance included\nin the validation group. This image is associated with\na corresponding C description: ‘‘a man with a red\nhelmet on a small moped on a dirt road’’.\nReferring to the ﬁle, which contains its encoded description\nEC , one can ﬁnd an encoding of the form:\nEC =[9488,1,2,3,1,4,5,6,1,7,8,6,1,9,10,\n9489,0,0,..., 0],\nconsidering that it has been generated from the equivalences\ncontained in the mapping ﬁle, the contents of which are\npresented in Table 4.\nFIGURE 2. Image taken from the training set with an associated\ngroundtruth caption: ‘‘a man with a red helmet on a small moped\non a dirt road. ’’\nTABLE 4. Mapping system used to encode the caption the example\nimage.\nC. CAPTION LENGHTS\nFinally, the last ﬁle is generated whose purpose is to house\nan array, whose elements represent the number of words that\nmake up the description associated with each of the images.\nV. HYPERPARAMETER NOTIONS\nThis section describes the loss and optimizer functions\nemployed by the reference benchmark. In addition, Algo-\nrithm 1 details the intervention of these components during\nthe training phase of the neural network.\nA. CROSS-ENTROPY LOSS FUNCTION\nTo describe the loss function of our attention model, we let\na be the function parametrized by θ, the caption output of\nthe network is represented as C =a(X,θ), where C is the\ncollection of words inferred from the MS COCO dictionary.\nThe loss function measures the inference performance of our\nattention model when compared with its respective ground\ntruth. In order to measure the difference between the ground\ntruth distribution and the distribution of the caption outcome,\n33684 VOLUME 10, 2022\nR. Castroet al.: Deep Learning Approaches Based on Transformer Architectures for Image Captioning Tasks\nwe deﬁne J(θ) as the cross-entropy. The cross-entropy loss\nfunction penalizes the attention model when it infers a low\nprobability for a given caption. Our attention model works\nby updating the values of θ, moving the loss towards the\nminimum of J(θ) [33].\nFor our training set of (X (i),Y(i)) for i ∈ {1,..., m},\nwe estimate the parameters θ ={θ (1),...,θ (n)}that mini-\nmizes J(θ) by computing:\nJ(θ) =− 1\nm\nm∑\ni=1\nL(X(i),Y(i),θ)\n=− 1\nm\nm∑\ni=1\nY(i)log\n(\nˆp(i)\n)\n, (7)\nwhere Y(i) represents the expected caption C of the ith image,\nand ˆp(i) constitutes the probability that the ith image outcomes\nthe intended value of C.\nB. ADAPTIVE MOMENT OPTIMIZER\nIn order to optimize our attention model through a gradient-\nbased optimization method, we express the gradient vector of\n(7) with respect to θas\ng =∇θJ(θ)\n= 1\nm∇θ\nm∑\ni=1\nL(X(i),Y(i),θ)\n= 1\nm\nm∑\ni=1\n(\nˆp(i) −Y(i)\n)\nX(i). (8)\nTo locate the minimum of J(θ), the proposed optimization\nalgorithm moves to the negative direction of (8) iteratively.\nOur model computes individual adaptive learning rates for\ndifferent parameters from estimates of ﬁrst and second\nmoments of g [34].\nVI. EXPERIMENTAL SETTINGS\nIt is essential to point out that for the three study cases,\nthe training of the corresponding models was performed\nconsidering that the aim was to take advantage of the use of\ntransfer learning on the encoder part. Therefore, only the part\nof the architecture directly in charge of generating the words\nof the ﬁnal captioning was subjected to training. In addition,\nthe TF technique (mentioned in the related works section)\nwas applied so that training can be accelerated by allowing\nthe recurrent network to access the ground-truths during the\ninference process.\nAll the experiments presented in the next sections were\nobtained using a HPC node with an AMD EPYC 7742\n64-Core Processor and a 40 Gb Nvidia A-100 graphic\ncard.\nA. HYPERPARMETER TUNING\nAs a ﬁrst experiment, we maintain all the default hyper-\nparameters of the model to study the impact of the\nAlgorithm 1Parameter Optimization and Training\nInput: Set of images X, set of ground-truths Y , set of caption sizes\nS, initial learning rate γ, batch size β.\nOutput: Predicted caption C, Set of individual attention masks α.\nInitialization:\n1: Initialize γ to 4e-4 and βto 32. ⊿Value of γ will depend on\nthe training type.\n2: Initial memory and hidden LSTM states are initialized by using\nseparate MLPs given an image:\nc0 =finit,c0 ( 1\nL\n∑L\ni=1 ai)\nh0 =finit,h0 ( 1\nL\n∑L\ni=1 ai)\nDATA ACQUISITION AND PRE-PROCESSING. (IN SECT. II-A.)\n3: Get MSCOCO dataset ⊿From online server.\n4: for each image do\n5: Resize and Normalize.\n6: end for\n7: Sample a minibatch of m′tr examples from the training\nset B =\n{[\nX(1) :Y(1)]\n,...,\n[\nX(m′tr ) :Y(m′\ntr )]}\nCROSS-ENTROPY COST FUNCTION DEFINITION (SECT. V-A.)\n8: J(θ) =− 1\nm′tr\n∑m′tr\ni=1 L(X(i),Y(i),θ)\nPARAMETER OPTIMIZATION FOR CONVOL. ENC.-DEC. (V-B.)\n9: while stopping criterion not met do\n10: Compute gradient estimate:\ng ← 1\nm′tr\n∇θ\n∑m′\ntr\ni=1 L\n(\nX(i),Y(i),θ\n)\n11: Update parameters: θ ←θ +g\n12: end while\nCAPTION GENERATION OF UNSEEN IMAGE.\n13: Get input image.\n14: Generate the caption for the input image using optimized θ\nparameters.\n15: Extract caption matrix C and the set of masks α from line 14.\ndifferent cost functions. Since the cross-entropy cost\nfunction was used to train the benchmark model, we con-\ntrasted the performance of the architecture using the neg-\native log-likelihood (NLL), mean squared error (MSE),\nand the Kullback-Leibler Divergence (KLDIVLOSS) cost\nfunctions.\nOnce the ﬁrst experimental phase is completed, the aim\nis to keep the cost function as an independent variable\nto sweep different optimizers. Once again, in addition\nto the optimizer used in the benchmark implementation\n(Adam), we examined the effect of AdamW, root mean\nsquare propogation optimizer (RMSprop), stochastic gradient\ndescent (SGD), and Adadelta optimizers.\nB. ENCODER ANALYSIS\nIn this scenario, once the optimal conﬁguration of hyper-\nparameters has been found, both the cost function and the\nnetwork optimizer are set as ﬁxed variables, allowing us\nto proceed with the second part of the experiment. Within\nthis ﬁnal stage, it is proposed to evaluate the performance\nof the architecture, both in terms of response quality and\ncomputational requirements, using different convolutional\nstructures to replace the Oxford VGG model used in the\nencoder of the default implementation. The alternatives to be\nevaluated in this work correspond to the ResNet-101, ResNet-\n152, ResNeXt-101, and MobileNetV3 models.\nVOLUME 10, 2022 33685\nR. Castroet al.: Deep Learning Approaches Based on Transformer Architectures for Image Captioning Tasks\nC. TRANSFORMER-BASED APPROACHES\nFor this last experimental environment, the objective is to\nstudy the alternative of replacing the convolutional encoder\nof the original architecture by a model that dispenses\nwith the traditional convolutional principles forged within\nthe computer vision area, more speciﬁcally, focusing on\nincorporating transformer-based models in this speciﬁc part\nof the image captioning system. Despite its origin related to\nnatural language processing, the ViT model demonstrated its\nviability for image classiﬁcation tasks. Given the potential of\nthis network to surpass our state-of-the-art, it was proposed\nas an experiment to verify the performance of such a model\nto carry out image captioning tasks. Consequently, it was\ndecided to use both the original version of ViT and its version\nwith distillation (DeiT).\nIt should be noted that since the present work does\nnot require image classiﬁcation tasks, both architectures\nwere stripped of the last MLP layer since the attentional\nmodel will reuse the output of the transformer model. The\nschematization of the ﬁnal model for image captioning is\nshown in Fig. 3.\nFinally, it is worth mentioning that both the ViT and\nDeiT models correspond to models retrieved from the\nHuggingface repository, being pre-trained in the ImageNet-\n21k and ImageNet-1k datasets respectively.\nOn the one hand, the ﬁrst method to be studied consists\nof deﬁning γ = 4e −4 to train only the learnable\nparameters belonging to the decoder system architecture.\nThis method is taken into consideration since the aim is to\ntake advantage of the knowledge contained in the pre-trained\nmodels. By contrast, the second proposed methodology\ncorresponds entirety with the previously described approach,\nwith the difference that γ = 1e −4 is deﬁned. Lastly,\nand as a ﬁnal modality, we seek to rescue the model\nobtained with the second training experiment so that, in the\nlast four iterations of the process, not only the decoder\nparameters are subjected to training, but also those that\nmake up the last transformer block of both the ViT and\nDeiT models.\nThe ﬁnal objective of this experiment was to use the\nBLEU-4 metric on both versions of the image captioning\nmodel to contrast the margin of improvement achieved\nconcerning the state-of-the-art.\nVII. RESULTS\nFrom Table 5, it is possible to highlight an evident\nimprovement in the performance of the model when using\nthe cross-entropy as a loss function. Although the MSE loss\nis positioned as the second-best alternative throughout the\nexperimental process, a difference of 31.584 in the Top-5\naccuracy indicator and 0.187 in BLEU-4 metric shows a large\ngap between the cross-entropy function and this alternative.\nConsidering this signiﬁcant difference, the results obtained\nby the KLDIVLOSS and the NLL position them as unsuitable\nalternatives for the model to be trained on.\nTABLE 5. Experimental results usingTop-5 accuracy and theBLEU-4\nperformance metric for each one of the loss functions under study.\nTABLE 6. Experimental results using the training loss, theTop-5 Accuracy,\nand theBLEU-4 performance metrics for each one of the optimizers\nunder study.\nIn addition to the quantitative results, Fig. 4 illustrates\na captioning example generated using each one of the loss\nfunctions under study. The outcomes prove that the cross-\nentropy loss function is positioned not only as the one with\nthe best results, but also the only loss function capable of\ngenerating a complete and meaningful description for an\nillustration that has never been seen by our model.\nProceeding with the second part of this scene, the results\noffered in Table 6 reveal a tighter situation when deﬁning\nan optimal alternative. In the ﬁrst instance, the optimizer\nAdam is positioned with the best results according to the three\ndeﬁned metrics. However, its variation, AdamW, not only\nreturns the same BLEU-4 value as Adam, but it represents\nonly a 0.005 and 0.133 of difference in the loss and Top-5\nAccuracy indicators, respectively. This closeness in terms of\nresults can be visualized using Fig. 5. In this illustration,\neach optimizer is tested by predicting the captioning for an\nimage consisting of a child in front of a laptop computer.\nWhen contrasting both variations of the Adam optimizer, it is\nobserved that the predictions only differ when mentioning the\ngender of the person in the image.\nIt is worth highlighting the performance of the RMSprop,\nwhich ranks as the third-best alternative, presenting a loss\nvalue of 3.663, along with 71.444 and 19.20 for Top-5\naccuracy and BLEU-4, respectively. RMSprop shows promis-\ning results when comparing the output caption with the\nexample image shown in Fig. 5. This optimizer is capable of\ngenerating a fully meaningful captioning by portrying to the\ncontent of the image. However, it missed minor details like\nnot including a reference to the elderliness of the person in\nthe illustration.\nFinally, the SGD and Adadelta optimizers provided the\nworst results. Although both optimizers presented slightly\ndifferent metrics, it is observed that neither of them were able\nto create a model capable of generating meaningful captions.\nNow, referring to the results of the encoder testing phase\nshown in Table 7, two isolated analyses were conducted.\nAt ﬁrst, when looking for the convolutional model that\n33686 VOLUME 10, 2022\nR. Castroet al.: Deep Learning Approaches Based on Transformer Architectures for Image Captioning Tasks\nFIGURE 3. Overall representation of the ViT adaptation proposal for solving images captioning tasks. The MLP of the original\nimplementation is replaced by the decoder used in previous experimental scenes.\nFIGURE 4. Image captioning results using an attention model with: (a) cross entropy loss, (b) MSE loss, (c) NLL loss, and (d) KLDIVLOSS. The results reveal\nan inadequate inference of MSE, NLL and KLDIVLOSS functions. By far, cross entropy is the only loss function that allows a proper training of our\nattention model.\nallows the best captioning quality, the superiority of the\nResNeXt-101 model is evidenced. This model stands out\nwith a Top-5 Accuracy of 73.128 and a loss value of\n3.404, surpassing the original encoder based on the VGG-16\narchitecture and the rest of the convolutional alternatives.\nOn the other hand, the picture changes when looking for\nthe architecture with lower computational requirements,\ntrying to minimize the sacriﬁce of the output quality as\nmuch as possible. Therefore, MobileNetV3 demonstrates its\ninherent qualities as an architecture oriented to embedded\nenvironments, requiring 2,971,952 parameters, 3.5379 hours\nof training time, and 0.07975 seconds of average inference\ntime. Such indicators become much more meaningful when\nreferring to the BLEU-4, Top-5 Accuracy, and loss metrics,\nreturning 19.50, 72.928, and 3.424, respectively.\nThe evident closeness between the results, in terms of\nresponse quality, can be seen in the example of captioning\nincluded in Fig. 6. The ability of each of the models to\nVOLUME 10, 2022 33687\nR. Castroet al.: Deep Learning Approaches Based on Transformer Architectures for Image Captioning Tasks\nTABLE 7. Once the experimental phase has been completed with each proposed architecture for the system encoder, the quantitative results are shown.\nThe chosen metrics denote both the quality of the response generated and the computational performance of each architecture.\nFIGURE 5. Image captioning results using: (a) SGD and Adadelta optimizers, (b) RMSprop optimizer, (c) AdamW optimizer, and (d) Adam optimizer. The\nimage illustrates the inadequate inference results of SGD and Adadelta when compared with their alternatives. Also, note that Adam optimizer yields the\nfinest result over the test image (a recurrent outcome obtained for further experiments using images from the test set).\ngenerate descriptions according to the scenario depicted in\nthe input image, including different details regarding colors,\npositions, and environmental conditions, can be perceived.\nLikewise, this example provides a visualization of possible\nminor failures when generating the corresponding caption.\nIn the aforementioned image, the encoder based on the VGG-\n16 architecture returns a description with redundancy, which\ncan be justiﬁed by the training period established for the\npresent experimentation.\nRelying on a second example, Fig. 7 once again demon-\nstrates the ability to generate a fully meaningful sentence\nby all architectures; however, not all of them manage to\nmatch the context of the image despite occasional errors\nin speciﬁc words. Under this scenario, the MobileNetV3\nnetwork generates an output that is entirely far from a\npossible ground-truth for the given image. Although this\nspeciﬁc example is not a compelling reason to contradict\nthe quantitative results previously shown, this example is\nintended to demonstrate a scenario where the robustness of\na model for mobile environments becomes evident.\nAs for the results concerning the transformer-based\narchitectures, Fig. 8 evidences the loss curves generated\nfrom the inference process on the validation group. Although\nboth the ViT and DeiT based models show the lowest\nlosses using the training method with the highest gamma\nvalue, it should be taken into account that from the ﬁfth\n33688 VOLUME 10, 2022\nR. Castroet al.: Deep Learning Approaches Based on Transformer Architectures for Image Captioning Tasks\nFIGURE 6. Image captioning results using as encoder: (a) ResNet-152, (b) ResNet-101, (c) VGG-16, (d) ResNext-101, and (e) MobileNet V.3. All the\nconvolutional architectures allowed the generation of sentences with complete meaning matching considerably to the scenario presented in the input\nimage. Reduced redundancy errors are appreciated when using VGG16.\niteration onwards, these models seem to suffer from possible\noverﬁtting. On the other hand, the loss curves behave more\nregularly throughout the iterations analyzed, showing little or\nno overﬁtting when using the alternative training methods.\nTherefore, beyond taking these values as indicators of the\nperformance of the models, the aim is to show the evident\nconvergence that exists throughout each training lapse.\nHaving contemplated the convergence of the models,\nit is worthwhile to perform a similar visualization now\nusing a metric related to the nature of natural language.\nThus, Fig. 9 shows the evolution of the BLEU-4 with the\npassing of the iterations. Furthermore, within this graph, the\nresults during the inference process on the validation set\nare shown. Therefore, when analyzing the impact of using\na higher gamma value, both ViT and DeiT-based models\npresent a relatively early learning plateau when reaching the\nﬁfth iteration. Conversely, the other two training methods\npresent a signiﬁcant improvement of BLEU-4. remaining in\noptimization even when reaching the last iterations. Both\nprocedures allow a progressive improvement of the metric\neven during the last iterations; however, the methodology that\ncontemplates the re-training of the transformer component\nstands out slightly.\nHowever, considering that the inferences generated for the\nrealization of this graph involved the use of TF, such values\nmight not fully represent the capabilities of the models, since\nwhen seeking to caption an image devoid of a ground-truth,\nTF could not be applied. For this reason, it was decided to\nconstruct the results included in Fig. 10.\nBy employing much more realistic conditions for the\ninference process, it can be seen that the models trained with\na lower γ outperformed the performance metrics of those\nVOLUME 10, 2022 33689\nR. Castroet al.: Deep Learning Approaches Based on Transformer Architectures for Image Captioning Tasks\nFIGURE 7. Image captioning results using as encoder: (a) ResNet-152, (b) ResNet-101, (c) VGG-16, (d) ResNext-101, and (e) MobileNet V.3. It can be seen\nthat the first four architectures generated results that were significantly close to the content of the input image. On the contrary, when using MobiliNet V.\n3, the generated result consists of a description completely unrelated to the target scenario, even though the sentence was grammatically correct and\nmade complete sense.\nwith a slightly higher γ in a very few number of iterations.\nMoreover, when using these results, a clear metrics boost is\nperceived, in contrast to when TF was used during inference.\nThus, to contrast the best checkpoints obtained in each stage\nof this experimental scene, Table 8 allows to have a superior\ncontrast of the maximum performance obtained when using\nViT and DeiT through the application of each of the three\ntraining.\nAs a result, it can be veriﬁed that the use of TF during the\ninference process camouﬂaged the real performance of both\nmodels. Simulation results show that the DeiT-based model\ncan be selected as the alternative with enhanced outcomes,\nspeciﬁcally reaching a BLEU-4 of 34.44 through the training\nprocess involving the calculation of gradients for the last\ntransformer block. Additionally, when reviewing the partial\nTABLE 8. BLEU-4 metric obtained by the best checkpoint generated from\neach training process applied to the ViT and DeiT based models using a\nbeam size of 3.\nresults of each training method, it is observed that regardless\nof the method applied, the DeiT-based model achieves the\nbest BLEU-4 metrics.\nAs a complement to the quantitative results shown above,\nFig. 11 provides a brief sample of the accuracy that ViT- and\nDeiT-based models can provide when generating inference.\nThe images used for this section were extracted from the\n33690 VOLUME 10, 2022\nR. Castroet al.: Deep Learning Approaches Based on Transformer Architectures for Image Captioning Tasks\nFIGURE 8. Evolution of the loss obtained during each of the corresponding iterations. These results were recovered using TF during the inference process\non the validation set.\nFIGURE 9. Evolution of the BLEU-4 metric obtained during each of the corresponding iterations. These results were recovered using TF during the\ninference process on the validation set.\nvalidation set to use the ground truth linked to each image\nas a referential description.\nWithin this brief comparative scheme, we observe the\nability of the models not only to describe relationships\nbetween objects or people, but also qualities related to the\ncapture of physical aspects and generalization of similar\nentities. On the one hand, when working on the ﬁrst\nimage of Fig. 11, the DeiT model can not only denote\nthe interaction of the dog with the frisbee, but it can also\ncontribute with additional information about the colors of\nboth entities. Also, when the image is presented with food,\nboth models can recognize that the main content of the\nVOLUME 10, 2022 33691\nR. Castroet al.: Deep Learning Approaches Based on Transformer Architectures for Image Captioning Tasks\nFIGURE 10. Evolution of the BLEU-4 metric obtained during each of the corresponding iterations. These results were retrieved without using TF during the\ninference process on the validation set.\nFIGURE 11. Examples of inference using images from the validation group. Models based on ViT and DeiT with best BLEU-4 metrics are used to contrast\nwith the ground-truth provided by the dataset.\ndish is pasta, however, the DeiT model can identify the\npresence of multiple vegetables within the dish, therefore, this\narchitecture generalizes these foods into a single category.\nVIII. CONCLUSION\nDuring the ﬁrst experimental stage, it was possible to\ndetermine that the cross-entropy was the loss function that\nachieved the best results, returning a Top-5 accuracy and\nBLEU-4 metrics of 73.092 and 0.201, respectively. On the\nother hand, once the loss function is set as an independent\nvariable, the Adam optimizer returned the best indicators,\ncompleting the ﬁrst training period with a loss value of\n3.414, a Top-5 Accuracy of 73.092, and a BLEU-4 of 0.201.\nHowever, it is worth noting the good results obtained by the\nAdamW optimizer, matching in the BLEU-4 metric its Adam\ncounterpart.\nFurthermore, the comparative study focused on the convo-\nlutional model and its use as an encoder to yield two attractive\nalternatives depending on the ﬁnal objective. On one hand,\nusing the ResNeXt-101 architecture generated the best results\nin terms of response quality. This architecture returned\nvalues of 73.128 in Top-5 Accuracy, and 3.404 for the\nloss value, denoting an improvement with respect to the\nresults obtained using VGG-16. On the other hand, when\nanalyzing the models under lower computational demands,\nthe encoder based on MobileNetV3 registered 2,971,952\nparameters, a training time of 3.5379 hours, an inference time\nof 0.07975 seconds, and 0.23 GMACs. Thus, MobileNetV3\nemerges as the most compact alternative without neglecting\nthe quality of the generated captioning, which is evidenced\nby its great closeness in the BLEU-4, Top-5 Accuracy, and\nloss value metrics.\n33692 VOLUME 10, 2022\nR. Castroet al.: Deep Learning Approaches Based on Transformer Architectures for Image Captioning Tasks\nRegarding the study involving the use of transformer-based\narchitectures as a replacement for convolutional models,\nboth the ViT and DeiT models demonstrate their viability\nby verifying their convergence through the evolution of the\nloss throughout the iterations. In addition, the DeiT-LSTM\nmodel stands out as the alternative with the best BLEU-4\nmetric when trained in two phases: the ﬁrst one in attempt to\noptimize only the decoder parameters, and the second phase\nincorporating the parameters of the last transformer block\nto be optimized using a value of γ =1e −4. As a result,\nthe model achieved a BLEU-4 of 34.44, surpassing the state-\nof-the-art from the paper Show, Attend and Tell, whose best\nresults consisted in a BLEU-4 of 24.3 in its soft-attention\nbased model, and 25.0 for its hard-attention alternative.\nAlthough we have proved that the three optimizers and two\nencoder options offer feasible results for this architecture,\nfuture works can beneﬁt from the individual training epoch to\nfurther study the convergence pace of the model under limited\nedge-computational devices. In addition, future researchers\ncan study the viability of not only using different encoder\narchitectures than the presented ones, but also analyze\nthe impact of other alternatives to LSTM models for the\ndecoding step, together with an extended investigation on\nthe architectural frameworks. Another element concerning\nthe training stage of our model is the decision to use the\nMSCOCO 2014 dataset. The selection was made based\non: i) the need of a large image set, and ii) the need\nto replicate the results of the benchmark paper. However,\nboth the convolutional and transformer-based variants have\npotential for further research, where the reader can study\nthe performance and behavior of our model when trained\nwith other datasets such as Flickr8k or Flickr30k. Finally,\nanother alternative to foster this work would be to include\nfurther hyperparameters to the study (e.g., dropout rate, batch\nsize, different types of stride and pooling, size of the kernels,\nweight initialization methods, model depth, weight decay,\netc.). Also, different methodologies of optimization such as\nRandom Search, Grid Search, etc can be applied supported by\na Hyperparameter Tuning Framework, enabling an in-depth\nresearch of the attention architecture\nREFERENCES\n[1] S. J. Rennie, E. Marcheret, Y . Mroueh, J. Ross, and V . Goel, ‘‘Self-critical\nsequence training for image captioning,’’ in Proc. IEEE Conf. Comput. Vis.\nPattern Recognit. (CVPR), Jul. 2017, pp. 7008–7024.\n[2] G. Kulkarni, V . Premraj, V . Ordonez, S. Dhar, S. Li, Y . Choi, A. C. Berg,\nand T. L. Berg, ‘‘Babytalk: Understanding and generating simple image\ndescriptions,’’ IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 12,\npp. 2891–2903, Oct. 2013.\n[3] Y . Yang, C. Teo, H. Daumé, and Y . Aloimonos, ‘‘Corpus-guided sentence\ngeneration of natural images,’’ in Proc. Conf. Empirical Methods Natural\nLang. Process., 2011, pp. 444–454.\n[4] K. Kpalma and J. Ronsin, ‘‘An overview of advances of pattern recognition\nsystems in computer vision,’’ Vis. Syst., vol. 4, p. 26, May 2007.\n[5] C. G. Amza and D. T. Cicic, ‘‘Industrial image processing using fuzzy-\nlogic,’’Proc. Eng., vol. 100, pp. 492–498, Oct. 2015.\n[6] A. Rastogi, R. Arora, and S. Sharma, ‘‘Leaf disease detection and grading\nusing computer vision technology & fuzzy logic,’’ in Proc. 2nd Int. Conf.\nsignal Process. Integr. Netw. (SPIN), 2015, pp. 500–505.\n[7] Q. You, H. Jin, Z. Wang, C. Fang, and J. Luo, ‘‘Image captioning with\nsemantic attention,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.\n(CVPR), Jun. 2016, pp. 4651–4659.\n[8] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinov,\nR. Zemel, and Y . Bengio, ‘‘Show, attend and tell: Neural image caption\ngeneration with visual attention,’’ in Proc. Int. Conf. Mach. Learn., 2016,\npp. 2048–2057.\n[9] D. Carrión-Ojeda, R. Fonseca-Delgado, and I. Pineda, ‘‘Analysis\nof factors that inﬂuence the performance of biometric systems\nbased on eeg signals,’’ Expert Syst. Appl. , vol. 165, Feb. 2021,\nArt. no. 113967. [Online]. Available: https://www.sciencedirect.\ncom/science/article/pii/S095741742030748X\n[10] R. Castro, I. Pineda, and M. E. Morocho-Cayamcela, ‘‘Hyperparameter\ntuning over an attention model for image captioning,’’ in Information\nCommunication Technology, J. P. Salgado Guerrero, J. C. Espinosa,\nM. C. Lozada, and S. Berrezueta-Guzman, Eds. Cham, Switzerland:\nSpringer, 2021, pp. 172–183.\n[11] H. Larochelle and G. Hinton, ‘‘Learning to combine foveal glimpses\nwith a third-order Boltzmann machine,’’ in Advance Neural Information\nProcessing System, vol. 1, J. Lafferty, C. Williams, J. Shawe-Taylor,\nR. Zemel, and A. Culotta, Eds. Red Hook, NY , USA: Curran Associates,\n2010, pp. 1243–1251.\n[12] D. Bahdanau, K. Cho, and Y . Bengio, ‘‘Neural machine translation by\njointly learning to align and translate,’’ Tech. Rep., 2016.\n[13] V . Mnih, N. Heess, A. Graves, and K. Kavukcuoglu, ‘‘Recurrent models of\nvisual attention,’’ in Proc. Adv. Neural Inf. Process. Syst., 2014, pp. 1–9.\n[14] J. Ba, V . Mnih, and K. Kavukcuoglu, ‘‘Multiple object recognition with\nvisual attention,’’ 2015, arXiv:1412.7755.\n[15] R. Kiros, R. Salakhutdinov, and R. Zemel, ‘‘Multimodal neural language\nmodels,’’ in Proc. Int. Conf. Mach. Learn., 2014, pp. 595–603.\n[16] J. Mao, W. Xu, Y . Yang, J. Wang, Z. Huang, and A. Yuille, ‘‘Deep\ncaptioning with multimodal recurrent neural networks (M-RNN),’’ 2015,\narXiv:1412.6632.\n[17] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, ‘‘Show and tell:\nA neural image caption generator,’’ in Proc. IEEE Conf. Comput. Vis.\nPattern Recognit. (CVPR), Jun. 2015, pp. 3156–3164.\n[18] J. Donahue, L. Anne Hendricks, S. Guadarrama, M. Rohrbach,\nS. Venugopalan, K. Saenko, and T. Darrell, ‘‘Long-term recurrent convo-\nlutional networks for visual recognition and description,’’ in Proc. IEEE\nConf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2015, pp. 2625–2634.\n[19] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. U. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Advance\nNeural Information Processing System, vol. 30, I. Guyon, U. V . Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett,\nEds. Red Hook, NY , USA: Curran Associates, 2017. [Online]. Available:\nhttps://proceedings.neurips.cc/paper/2017/ﬁle/3f5ee243547dee91f\nbd053c1c4a845aa-Paper.pdf\n[20] C. Wang, S. Wu, and S. Liu, ‘‘Accelerating transformer decoding\nvia a hybrid of self-attention and recurrent neural network,’’ 2019,\narXiv:1909.02279.\n[21] A. Radford and K. Narasimhan, ‘‘Improving language understanding by\ngenerative pre-training,’’ Tech. Rep., 2018.\n[22] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘Bert: Pre-training\nof deep bidirectional transformers for language understanding,’’ 2019,\narXiv:1810.04805.\n[23] J. W. Chen, X. K. Sigalingging, J.-S. Leu, and J.-I. Takada,\n‘‘Applying a hybrid sequential model to Chinese sentence correction,’’\nSymmetry, vol. 12, no. 12, p. 1939, 2020. [Online]. Available:\nhttps://www.mdpi.com/2073-8994/12/12/1939\n[24] A. Patel and A. Varier, ‘‘Hyperparameter analysis for image captioning,’’\n2020, arXiv:2006.10923.\n[25] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, ‘‘An image is worth 16×16 words:\nTransformers for image recognition at scale,’’ Tech. Rep., 2021.\n[26] W. Liu, S. Chen, L. Guo, X. Zhu, and J. Liu, ‘‘CPTR: Full transformer\nnetwork for image captioning,’’ 2021, arXiv:2101.10804.\n[27] L. Zhou, H. Palangi, L. Zhang, H. Hu, J. Corso, and J. Gao, ‘‘Uniﬁed\nvision-language pre-training for image captioning and vqa,’’ in Proc. AAAI\nConf. Artif. Intell., 2020, vol. 34, no. 7, pp. 13041–13049.\n[28] H. Lu, R. Yang, Z. Deng, Y . Zhang, G. Gao, and R. Lan, ‘‘Chinese image\ncaptioning via fuzzy attention-based DenseNet-BiLSTM,’’ ACM Trans.\nMultimedia Comput. Commun. Appl., vol. 17, no. 1, Mar. 2021, Art. no. 48,\ndoi: 10.1145/3422668.\nVOLUME 10, 2022 33693\nR. Castroet al.: Deep Learning Approaches Based on Transformer Architectures for Image Captioning Tasks\n[29] K. He, X. Zhang, S. Ren, and J. Sun, ‘‘Deep residual learning for image\nrecognition,’’ in 2016 IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR),\n2016, pp. 770–778.\n[30] S. Hochreiter and J. Schmidhuber, ‘‘Long short-term memory,’’ Neural\nComput., vol. 9, no. 8, pp. 1735–1780, 1997.\n[31] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,\nand C. L. Zitnick, ‘‘Microsoft COCO: Common objects in context,’’ in\nComputer Vision, D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars, Eds.\nCham, Switzerland: Springer, 2014, pp. 740–755.\n[32] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ‘‘ImageNet:\nA large-scale hierarchical image database,’’ in Proc. IEEE Conf. Comput.\nVis. Pattern Recognit., Sep. 2009, pp. 248–255.\n[33] M. E. Morocho-Cayamcela, H. Lee, and W. Lim, ‘‘Machine learning to\nimprove multi-hop searching and extended wireless reachability in V2X,’’\nIEEE Commun. Lett., vol. 24, no. 7, pp. 1477–1481, Sep. 2020.\n[34] D. P. Kingma and J. Ba, ‘‘Adam: A method for stochastic optimization,’’\nin Proc. 3rd Int. Conf. Learn. Represent., 2015, pp. 1–15.\nROBERTO CASTRO(Student Member, IEEE) is\ncurrently pursuing the degree in IT with Yachay\nTech University, Urcuquí, Ecuador. Since 2021,\nhe has been afﬁliated with the DeepArc Research\nGroup, in which he has started his career as a\nYoung Researcher, making his ﬁrst publication\nin Communications in Computer and Informa-\ntion Science, and he collaborates as a V olunteer\nResearcher with the SDAS Research Group.\nHis research interests include deep learning,\ncomputer vision, natural language processing, and data science.\nISRAEL PINEDA (Member, IEEE) received the\nB.E. degree in computer systems from Universidad\nPolitécnica Salesiana, Ecuador, in 2011, the M.S.\ndegree in computer science and engineering\nfrom Chonbuk National University, South Korea,\nin 2015, and the Ph.D. degree, in 2018. He is\ncurrently a full-time Professor with Yachay Tech\nUniversity, Ecuador. His research interests include\ncomputer graphics, ﬂuid simulations, physically-\nbased simulation, rendering, and scientiﬁc\ncomputing.\nWANSU LIM(Member, IEEE) received the M.Sc.\nand Ph.D. degrees from the Gwangju Institute\nScience and Technology (GIST), South Korea,\nin 2007 and 2010, respectively. He is currently\nan Associate Professor with the Kumoh National\nInstitute of Technology (KIT), South Korea, lead-\ning the activities of the communication technolo-\ngies, machine learning research, and computer\nvision processing. From 2010 to 2013, he was a\nResearch Fellow with the University of Hertford-\nshire, U.K., and then a Postdoctoral Researcher with the Institut national de\nla recherche scientiﬁque (INRS), Canada, from 2013 to 2014. He is also a\nTechnical Committee Member of Elsevier Computer Communications and\na member of IEEE Communications Society. He has authored over 60 peer-\nreviewed journals and conference papers and served as a reviewer for several\nIEEE conferences and journals.\nMANUEL EUGENIO MOROCHO-CAYAMCELA\n(Member, IEEE) received the B.S. degree in\nelectronic engineering from Universidad Politéc-\nnica Salesiana, Cuenca, Ecuador, in 2012, the\nM.Sc. degree in communications engineering and\nnetworks from The University of Birmingham,\nEngland, U.K., in 2016, and the Ph.D. degree in\nelectronic engineering from the Kumoh National\nInstitute of Technology, Gumi-si, Republic of\nKorea.\nFrom 2017 to 2020, he was a Senior Researcher with the KIT Future\nCommunications and Systems Laboratory, Gumi-si. Since 2020, he has\nbeen working as a Fellow Researcher with ESPOL, Guayaquil, Ecuador.\nHe is currently a full-time Professor with Yachay Tech University, Urcuquí,\nEcuador. His research interests include artiﬁcial intelligence, deep learning,\ncomputer vision, wireless communications, and optimization.\nMr. Morocho-Cayamcela was a recipient of the SENESCYT Fellowship\nfrom The National Secretariat for Higher Education, Science, Technology\nand Innovation of Ecuador, in 2015, and the KIT Doctoral Grant from the\nKumoh National Institute of Technology, in 2017.\n33694 VOLUME 10, 2022",
  "topic": "Closed captioning",
  "concepts": [
    {
      "name": "Closed captioning",
      "score": 0.8733842372894287
    },
    {
      "name": "Computer science",
      "score": 0.7892216444015503
    },
    {
      "name": "Stochastic gradient descent",
      "score": 0.5385400652885437
    },
    {
      "name": "Encoder",
      "score": 0.5256444215774536
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5088024735450745
    },
    {
      "name": "BLEU",
      "score": 0.4931318461894989
    },
    {
      "name": "Mean squared error",
      "score": 0.45714783668518066
    },
    {
      "name": "Kullback–Leibler divergence",
      "score": 0.4566679298877716
    },
    {
      "name": "Cross entropy",
      "score": 0.4505586326122284
    },
    {
      "name": "Entropy (arrow of time)",
      "score": 0.4338894188404083
    },
    {
      "name": "Deep learning",
      "score": 0.41523459553718567
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.30439677834510803
    },
    {
      "name": "Image (mathematics)",
      "score": 0.27922484278678894
    },
    {
      "name": "Machine translation",
      "score": 0.25580257177352905
    },
    {
      "name": "Artificial neural network",
      "score": 0.19166859984397888
    },
    {
      "name": "Mathematics",
      "score": 0.152145117521286
    },
    {
      "name": "Statistics",
      "score": 0.11888375878334045
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I3130401523",
      "name": "Universidad Yachay Tech",
      "country": "EC"
    },
    {
      "id": "https://openalex.org/I113409471",
      "name": "Kumoh National Institute of Technology",
      "country": "KR"
    }
  ],
  "cited_by": 54
}