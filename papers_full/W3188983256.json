{
  "title": "Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval",
  "url": "https://openalex.org/W3188983256",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2734668338",
      "name": "Luyu Gao",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2148123616",
      "name": "Jamie Callan",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1614298861",
    "https://openalex.org/W4299585995",
    "https://openalex.org/W3118668786",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3005296017",
    "https://openalex.org/W3105949871",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W3092683697",
    "https://openalex.org/W2951534261",
    "https://openalex.org/W3038572442",
    "https://openalex.org/W3173783447",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W3033406728",
    "https://openalex.org/W3024786184",
    "https://openalex.org/W3021282678",
    "https://openalex.org/W2997200074",
    "https://openalex.org/W2982096936",
    "https://openalex.org/W3168875417",
    "https://openalex.org/W2996064239",
    "https://openalex.org/W3172119680",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3145630588",
    "https://openalex.org/W3157758108",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3153507317",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W4206121183",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3184402450",
    "https://openalex.org/W3185250692",
    "https://openalex.org/W3115295967",
    "https://openalex.org/W4298323699",
    "https://openalex.org/W3034696692",
    "https://openalex.org/W2951434086",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W4221164176",
    "https://openalex.org/W3176182290",
    "https://openalex.org/W2593864460",
    "https://openalex.org/W3005680577"
  ],
  "abstract": "Recent research demonstrates the effectiveness of using fine-tuned language models (LM) for dense retrieval. However, dense retrievers are hard to train, typically requiring heavily engineered fine-tuning pipelines to realize their full potential. In this paper, we identify and address two underlying problems of dense retrievers: i) fragility to training data noise and ii) requiring large batches to robustly learn the embedding space. We use the recently proposed Condenser pre-training architecture, which learns to condense information into the dense vector through LM pre-training. On top of it, we propose coCondenser, which adds an unsupervised corpus-level contrastive loss to warm up the passage embedding space. Experiments on MS-MARCO, Natural Question, and Trivia QA datasets show that coCondenser removes the need for heavy data engineering such as augmentation, synthesis, or filtering, and the need for large batch training. It shows comparable performance to RocketQA, a state-of-the-art, heavily engineered system, using simple small batch fine-tuning.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 2843 - 2853\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nUnsupervised Corpus Aware Language Model Pre-training\nfor Dense Passage Retrieval\nLuyu Gao and Jamie Callan\nLanguage Technologies Institute\nCarnegie Mellon University\n{luyug, callan}@cs.cmu.edu\nAbstract\nRecent research demonstrates the effective-\nness of using ﬁne-tuned language mod-\nels (LM) for dense retrieval. However, dense\nretrievers are hard to train, typically requiring\nheavily engineered ﬁne-tuning pipelines to re-\nalize their full potential. In this paper, we iden-\ntify and address two underlying problems of\ndense retrievers: i) fragility to training data\nnoise and ii) requiring large batches to robustly\nlearn the embedding space. We use the re-\ncently proposed Condenser pre-training archi-\ntecture, which learns to condense information\ninto the dense vector through LM pre-training.\nOn top of it, we propose coCondenser, which\nadds an unsupervised corpus-level contrastive\nloss to warm up the passage embedding space.\nExperiments on MS-MARCO, Natural Ques-\ntion, and Trivia QA datasets show that coCon-\ndenser removes the need for heavy data engi-\nneering such as augmentation, synthesis, or ﬁl-\ntering, and the need for large batch training. It\nshows comparable performance to RocketQA,\na state-of-the-art, heavily engineered system,\nusing simple small batch ﬁne-tuning.1\n1 Introduction\nBuilding upon the advancements of pre-trained lan-\nguage models (LM; Devlin et al. (2019); Liu et al.\n(2019)), dense retrieval has become an effective\nparadigm for text retrieval (Lee et al., 2019; Chang\net al., 2020; Karpukhin et al., 2020; Qu et al., 2021;\nGao et al., 2021a; Zhan et al., 2022). Recent re-\nsearch has however found that ﬁne-tuning dense\nretrievers to realize their capacity requires carefully\ndesigned ﬁne-tuning techniques. Early works in-\nclude iterative negative mining (Xiong et al., 2021)\nand multi-vector representations (Luan et al., 2020).\nThe recent RocketQA system (Qu et al., 2021) sig-\nniﬁcantly improves the performance of a dense\nretriever by designing an optimized ﬁne-tuning\n1Our code is available at https://github.com/\nluyug/Condenser.\npipeline that includes i) denoising hard negatives,\nwhich corrects mislabeling, and ii) large batch train-\ning. While this is very effective, the entire pipeline\nis very heavy in computation and not feasible for\npeople who do not have tremendous hardware re-\nsources, like those in academia. In this paper, we\nask, instead of directly using the pipeline, can we\ntake the insights of RocketQA to perform language\nmodel pre-training such that the pre-trained model\ncan be easily ﬁne-tuned on any target query set.\nConcretely, we ask what the optimized training\nin RocketQA solves. We hypothesize that typi-\ncal LMs are sensitive to mislabeling, which can\ncause detrimental updates to the model weights.\nDenoising can effectively remove the bad samples\nand their updates. On the other hand, for most\nLMs, the CLS vectors are either trained with a\nsimple task (Devlin et al., 2019) or not explicitly\ntrained at all (Liu et al., 2019). These vectors are\nfar from being able to form an embedding space\nof passages (Lee et al., 2019). The large training\nbatches in RocketQA help the LM to stably learn\nto form the full embedding space. To this end,\nwe want to pre-train an LM such that it is locally\nnoise-resistant and has a well-structured global em-\nbedding space. For noise resistance, we borrow\nthe Condenser pre-training architecture (Gao and\nCallan, 2021), which performs language model pre-\ntraining actively conditioning on the CLS vector. It\nproduces an information-rich CLS representation\nthat can robustly condense an input sequence. We\nthen introduce a simple corpus level contrastive\nlearning objective: given a target corpus of docu-\nments to retrieve from, at each training step sample\ntext span pairs from a batch of documents and train\nthe model such that the CLS embeddings of two\nspans from the same document are close and spans\nfrom different documents are far apart. Combin-\ning the two, we propose coCondenser pre-training,\nwhich unsupervisedly learns a corpus-aware pre-\ntrained model for dense retrieval.\n2843\nIn this paper, we test coCondenser pre-training\non two popular corpora, Wikipedia and MS-\nMARCO. Both have served as information sources\nfor a wide range of tasks. This popularity justi-\nﬁes pre-training models speciﬁcally for each of\nthem. We directly ﬁne-tune the pre-trained coCon-\ndenser using small training batches without data\nengineering. On Natural Question, TriviaQA, and\nMS-MARCO passage ranking tasks, we found that\nthe resulting models perform on-par or better than\nRocketQA and other contemporary methods.\n2 Related Work\nDense Retrieval Transformer LM has advanced\nthe state-of-the-art of many NLP tasks (Devlin\net al., 2019; Liu et al., 2019; Yang et al., 2019;\nLan et al., 2020) including dense retrieval. Lee\net al. (2019) are among the ﬁrst to demonstrate\nthe effectiveness of Transformer dense retriev-\ners. They proposed a simple Inverse Cloze Task\n(ICT) method to further pre-train BERT (Devlin\net al., 2019). Follow-up works explored other pre-\ntraining tasks (Chang et al., 2020) as well end-to-\nend co-training of reader and retriever (Guu et al.,\n2020). Karpukhin et al. (2020) is the ﬁrst to dis-\ncover that careful ﬁne-tuning can learn effective\ndense retriever directly from BERT. Later works\nthen started to investigate ways to further improve\nﬁne-tuning (Xiong et al., 2021; Qu et al., 2021).\nAmong them, Qu et al. (2021) proposed the Rock-\netQA ﬁne-tuning pipeline which hugely advanced\nthe performance of dense retrievers.\nUntil very recently, pre-training for dense re-\ntrieval has been left unexplored. A concurrent\nwork, DPR-PAQ (O˘guz et al., 2021), revisits pre-\ntraining and proposes domain matched pre-training,\nusing a 65-million-size synthetic QA pair dataset\ngenerated with pre-trained Natural Question and\nTrivia QA pipelines to pre-train dense retrievers.\nThis paper uses a recently proposed dense re-\ntrieval pre-training architecture, Condenser (Gao\nand Callan, 2021). Unlike previous works that\ndesign pre-training tasks, Condenser explored the\nidea of designing a special pre-training architecture\nto improve representation effectiveness.\nOne reason why dense retrieval is of immedi-\nate great value is that there is a rich literature that\nstudies efﬁcient dense retrieval for ﬁrst stage re-\ntrieval (Johnson et al., 2017; Guo et al., 2020).\nThere are also mature dense retrieval libraries, such\nas FAISS (Johnson et al., 2017). By pre-encoding\nthe corpus into a MIPS index, retrieval can run on-\nline with millisecond-level latency (Johnson et al.,\n2017; Guo et al., 2020).\nContrastive Learning Contrastive learning has\nbecome a very popular topic in computer vi-\nsion (Chen et al., 2020; He et al., 2020). Recent\nworks have brought the idea to natural language\nprocessing to learn high-quality sentence represen-\ntation (Giorgi et al., 2020; Wu et al., 2020). In\nthis work, we use contrastive learning to do pre-\ntraining for dense retrieval. Different from earlier\nwork, instead of individual representations (Giorgi\net al., 2020), we are interested in the full learned\nembedding space, which we will use to warm start\nthe retriever.\nThe large batch requirement had been a limiting\nfactor in contrastive learning (Chen et al., 2020)\nunder resource-limited setups where GPU (accel-\nerator) memory is not sufﬁciently large. In gen-\neral, this extends to any training procedure that\nuses contrastive loss, including dense retrieval pre-\ntraining (Guu et al., 2020; Chang et al., 2020). Gao\net al. (2021b) recently devised a gradient cache\ntechnique that upper-bounds peak memory usage of\ncontrastive learning to almost constant. In subsec-\ntion 3.3, we show how to adapt it for coCondenser\npre-training.\n3 Method\nIn this section, we ﬁrst give a brief review of Con-\ndenser. Then we discuss how to extend it to co-\nCondenser and how to perform memory-efﬁcient\ncoCondenser pre-training.\nOven [MASK] apple pie[CLS]\nOven [MASK] apple pie[CLS]\nOven [MASK] apple pie[CLS]\nOven [MASK] apple pie[CLS]\nOven [MASK] apple pie[CLS]\nHead (Pre-train Only)\nOven [MASK] apple pie[CLS]\nOven [MASK] apple[CLS] pie\nLate\nEarly\nFigure 1: Condenser: Shown are 2 early and 2 late\nbackbone layers. In our experiments each have 6 layers.\nCondenser Head is dropped during ﬁne-tuning. This il-\nlustration is taken from the Condenser paper (Gao and\nCallan, 2021).\n2844\n3.1 Condenser\nIn this paper, we adopt a special pre-training archi-\ntecture, Condenser (Gao and Callan, 2021). Con-\ndenser is a stack of Transformer blocks. As shown\nin Figure 1, these Transformer blocks are divided\ninto three groups, early backbone encoder layers,\nlate backbone encoder layers, and head layers. An\ninput x = [x1, x2, ..] is ﬁrst prepended a CLS, em-\nbedding, and run through the backbone layers.\n[h0\ncls; h0] = Embed([CLS; x]) (1)\n[hearly\ncls ; hearly] = Encoderearly([h0\ncls; h0]) (2)\n[hlate\ncls ; hlate] = Encoderlate([hearly\ncls ; hearly]) (3)\nThe head takes the CLS representation from the late\nlayers but using a short circuit, the token represen-\ntations from the early layers. This late-early pair\nthen runs through the head’s Transformer blocks.\n[hcd\ncls; hcd] = Head([hlate\ncls ; hearly]) (4)\nThe head’s outputs are then used to do masked lan-\nguage model (MLM; Devlin et al. (2019)) training.\nLmlm =\n∑\ni∈masked\nCrossEntropy(Whcd\ni , xi) (5)\nTo utilize the capacity of the late layers, Condenser\nis forced to learn to aggregate information into the\nCLS token, which will then participate in the LM\nprediction. Leveraging the rich and effective train-\ning signal produced by MLM, Condenser learns\nto utilize the powerful Transformer architecture\nto generate dense CLS representations. We hy-\npothesize that with this LM objective typically\nused to train token representations now put on the\ndense CLS representation, the learned LM gains\nimproved robustness against noise.\n3.2 coCondenser\nWhile Condenser can be trained on a diverse col-\nlection of corpora to produce a universal model,\nit is not able to solve the embedding space issue:\nwhile information embedded in the CLS can be\nnon-linearly interpreted by the head, inner prod-\nucts between these vectors still lack semantics.\nConsequently, they do not form an effective em-\nbedding space. To this end, we augment the Con-\ndenser MLM loss with a contrastive loss. Unlike\nprevious work that pre-trains on artiﬁcial query\npassage pairs, in this paper, we propose to sim-\nply pre-train the passage embedding space in a\nquery-agnostic fashion, using a contrastive loss de-\nﬁned over the target search corpus. Concretely,\ngiven a random list of n documents [d1, d2, ..., dn],\nwe extract randomly from each a pair of spans,\n[s11, s12, ..., sn1, sn2]. These spans then form a\ntraining batch of coCondenser. Given a span\nsij’s corresponding late CLS representation hij,\nits corpus-aware contrastive loss is deﬁned over the\nbatch as shown below.\nLco\nij = −log exp(⟨hi1, hi2⟩)∑n\nk=1\n∑2\nl=1 Iij̸=kl exp(⟨hij, hkl⟩)\n(6)\nFamiliar readers may recognize this as the con-\ntrastive loss from SimCLR (Chen et al., 2020),\nfor which we use random span sampling as aug-\nmentation. Others may see a connection to noise\ncontrastive estimation (NCE). Here we provide an\nNCE narrative. Following the spirit of the distribu-\ntional hypothesis, passages close together should\nhave similar representations while those in differ-\nent documents should have different representa-\ntions. Here we use random spans as surrogates of\npassages and enforce the distributional hypothe-\nsis through NCE, as word embedding learning in\nWord2Vec (Mikolov et al., 2013). We can also rec-\nognize this as a span-level language model objec-\ntive, or “skip-span”. Denote spansij’s Condenser\nMLM loss Lmlm\nij . The batch’s loss is deﬁned as an\naverage sum of MLM and contrastive loss, or from\nan alternative perspective, word and span LM loss.\nL= 1\n2n\nn∑\ni=1\n2∑\nj=1\n[Lmlm\nij + Lco\nij ] (7)\n3.3 Memory Efﬁcient Pre-training\nThe RocketQA pipeline uses supervision and large-\nbatch training to learn the embedding space. We\nwould also like to run large-batch unsupervised pre-\ntraining to construct effective stochastic gradient\nestimators for the contrastive loss in Equation 6.\nTo remind our readers, this large-batch pre-training\nhappens only once for the target search corpus. We\nwill show that this allows effective small batch ﬁne-\ntuning on task query sets.\nHowever, due to the batch-wise dependency of\nthe contrastive loss, it requires ﬁtting the large\nbatch into GPU (accelerator) memory. While this\ncan be done naively with interconnected GPU\nnodes or TPU pods, which can have thousands\nof gigabytes of memory, academia, and smaller or-\nganizations are often restricted to machines with\n2845\nfour commercial GPUs. To break the memory con-\nstraint and perform effective contrastive learning,\nwe adjust the gradient caching technique (Gao et al.,\n2021b) for our setup. We describe the procedure\nhere for people who want to perform coCondenser\npre-training but have limited resources. Denote\nLco = ∑\ni\n∑\nj Lco\nij , we can write Equation 7 as,\nL= 1\n2n[Lco +\n∑\ni\n∑\nj\nLmlm\nij ] (8)\nThe spirit of gradient caching is to decouple rep-\nresentation gradient and encoder gradient compu-\ntation. Before computing the model weight up-\ndate, we ﬁrst run an extra backbone forward for\nthe entire batch. This provides numerical values of\n[h11, h12, ...., hn1, hn2], from which we compute:\nvij = ∂\n∂hij\n∑\ni\n∑\nj\nLco\nij = ∂Lco\n∂hij\n(9)\ni.e. the contrastive loss gradient with respect to the\nCLS vector. We store all these vectors in a gradi-\nent cache, C = [ v11, v12, .., vn1, vn2]. Using vij,\ndenote the model parameter Θ, we can write the\nderivative of the contrastive loss as shown below.\n∂Lco\n∂Θ =\n∑\ni\n∑\nj\n∂Lco\n∂hij\n∂hij\n∂Θ (10)\n=\n∑\ni\n∑\nj\nv⊤\nij\n∂hij\n∂Θ (11)\nWe can then write the gradient of Equation 8.\n∂L\n∂Θ = 1\n2n\n∑\ni\n∑\nj\n[v⊤\nij\n∂hij\n∂Θ +\n∂Lmlm\nij\n∂Θ ] (12)\nSince vij is already in the cache C, each summa-\ntion term now only concerns span sij and its activa-\ntion, meaning that we can compute the full batch’s\ngradient in an accumulation fashion over small sub-\nbatches. In other words, the full batch no longer\nneeds to concurrently reside on the GPUs.\n3.4 Fine-tuning\nAt the end of pre-training, we discard the Con-\ndenser head, keeping only the backbone layers.\nConsequently, the model reduces to its backbone,\nor effectively a Transformer Encoder. We use the\nbackbone weights to initialize query encoder fq\nand passage encoder fp. Each outputs the last layer\nCLS. Recall that they have already been warmed\nup in pre-training. A (query q, passage p) pair\nsimilarity is deﬁned as an inner product.\ns(q, p) = ⟨fq(q), fp(p)⟩ (13)\nQuery and passage encoders are supervisedly ﬁne-\ntuned on the target task’s training set. We train with\na supervised contrastive loss and compute for query\nq, negative log likelihood of a positive document\nd+ against a set of negatives {d−\n1 , d−\n2 , ..d−\nl ..}.\nL= −log exp(s(q, d+))\nexp(s(q, d+)) + ∑\nl\nexp(s(q, d−\nl ))\n(14)\nWe run a two-stage training as described in the\nDPR (Karpukhin et al., 2020) toolkit. As shown\nin Figure 2b, in the ﬁrst stage, the retrievers are\ntrained with BM25 negatives. The ﬁrst-stage re-\ntriever is then used to mine hard negatives to com-\nplement the negative pool. The second stage re-\ntriever trains with the negative pool generated in\nthe ﬁrst round. This is in contrast to the multi-stage\npipeline of RocketQA shown in Figure 2a.\n4 Experiments\nIn this section, we ﬁrst describe the implementa-\ntion details of coCondenser pre-training. We then\nconduct dense retrieval experiments to test the ef-\nfectiveness of ﬁne-tuned coCondenser retrievers.\n4.1 Pre-training\nThe coCondenser pre-training starts with vanilla\nBERT and goes in two stages, universal Condenser\npre-training and corpus aware coCondenser pre-\ntraining. In the ﬁrst stage, we pre-train a Condenser\nand warm start the backbone layers with pre-trained\n12-layer BERTbase weights (Devlin et al., 2019).\nThe backbone uses an equal split, 6 early layers,\nand 6 late layers. The Condenser pre-training uses\nthe same data as BERT: English Wikipedia and\nthe BookCorpus. The ﬁrst stage Condenser pre-\ntraining takes roughly a week on 4 RTX 2080 Ti\nGPUs or 2 days on a v3-8 cloud TPU.\nThe Condenser model from stage one, including\nboth backbone and head, is taken to warm start\nstage two coCondenser pre-training on the target\ncorpus (Wikipedia or MS-MARCO web collection).\nWe keep the Condenser architecture unchanged in\nthe second step. We use AdamW optimizer with\na learning rate 1e-4, weight decay of 0.01, and\nlinear learning rate decay. Each model weight up-\ndate uses 2K documents. We train using gradient\n2846\n(a) RocketQA retriever training pipeline (taken from Qu et al. (2021)).\nRetriever 1 Retriever 2Hard\nNegatives\nTrain Train\ncoCondenser Initialize\n(b) coCondenser retriever training pipeline.\nFigure 2: RocketQA training pipelines and two-round retriever training pipeline in coCondenser.\ncache update, as described in subsection 3.3. We\nused the released Condenser model for the ﬁrst\nstage. The second stage takes roughly 2 days on 4\nRTX 2080 Ti GPUs or 19 hours on a v3-8 cloud\nTPU. Our GPU implementations are based on Py-\ntorch (Paszke et al., 2019) and TPU implementa-\ntions on JAX (Bradbury et al., 2018).\nAfter the second stage ﬁnishes, we discard the\nCondenser head, resulting in a model of the exact\nsame architecture as BERTbase.\n4.2 Dense Passage Retrieval\nNext, we ﬁne-tune the learned coCondenser to test\nretrieval performance. Following RocketQA, we\ntest on Natural Question and MS-MARCO passage\nranking. We also report performance on Trivia QA,\nwhose pre-processed version is released in DPR.\n4.2.1 Setup\nDataset We use MS-MARCO passage rank-\ning (Bajaj et al., 2018), Natural Question(NQ;\nKwiatkowski et al. (2019)) and Trivia QA(TQA;\nJoshi et al. (2017)). MS-MARCO is constructed\nfrom Bing’s search query logs and web documents\nretrieved by Bing. Natural Question contains ques-\ntions from Google search. Trivia QA contains a\nset of trivia questions. We report ofﬁcial metrics\nMRR@10, Recall@1000 for MS-MARCO, and\nRecall at 5, 20, and 100 for NQ and TQA.\nData Preparation We use Natural Question,\nTrivia QA, and Wikipedia cleaned and released\nwith DPR toolkit (Karpukhin et al., 2020). NQ\nand TQA each have about 60K training data post-\nprocessing. Similarly, we use the MS-MARCO cor-\npus released with RocketQA open-source code (Qu\net al., 2021). For reproducibility, we use the ofﬁ-\ncial relevance ﬁle instead of RocketQA’s extended\none, which has about 0.5M training queries. The\nBM25 negatives for MS-MARCO are taken from\nthe ofﬁcial training triples.\nTraining MS-MARCO models are trained with\nTevatron toolkit (Gao et al., 2022) using AdamW\nwith a 5e-6 learning rate, linear learning rate sched-\nule, and batch size 64 for 3 epochs. NQ and TQA\nmodels are trained with the DPR toolkit follow-\ning published hyperparameters by Karpukhin et al.\n(2020). All models are trained on one RTX 2080\nTi. We added gradient caching to DPR to deal with\nmemory constraints. The models are trained only\non each task’s corresponding training set. We note\nthat RocketQA is trained on a concatenation of\nseveral datasets (Qu et al., 2021).\nModel Validation Since for dense retrieval, vali-\ndating a checkpoint requires encoding the full cor-\npus, evaluating a checkpoint becomes very costly.\nDue to our computation resource limitation, we\nfollow the suggestion in the DPR toolkit and take\n2847\nMethod MS-MARCO Dev Natural Question Test Trivia QA Test\nMRR@10 R@1000 R@5 R@20 R@100 R@5 R@20 R@100\nBM25 18.7 85.7 - 59.1 73.7 - 66.9 76.7\nDeepCT 24.3 90.9 - - - - - -\ndocT5query 27.7 94.7 - - - - - -\nGAR - - 60.9 74.4 85.3 73.1 80.4 85.7\nDPR - - - 74.4 85.3 - 79.3 84.9\nANCE 33.0 95.9 - 81.9 87.5 - 80.3 85.3\nME-BERT 33.8 - - - - - - -\nRocketQA 37.0 97.9 74.0 82.7 88.5 - - -\nCondenser 36.6 97.4 - 83.2 88.4 - 81.9 86.2\nDPR-PAQ\n- BERTbase 31.4 - 74.5 83.7 88.6 - - -\n- BERTlarge 31.1 - 75.3 84.4 88.9 - - -\n- RoBERTabase 32.3 - 74.2 84.0 89.2 - - -\n- RoBERTalarge 34.0 - 76.9 84.7 89.2 - - -\ncoCondenser 38.2 98.4 75.8 84.3 89.0 76.8 83.2 87.3\nTable 1: Retrieval performance on MSMARCO dev, Natural Question test and Trivia QA test. We mark bold the\nbest performing models as well as the best performing 12-layer base models. Results unavailable are left blank.\nthe last model training checkpoint. We do the same\nfor MS-MARCO.\nComparison Systems We take RocketQA (Qu\net al., 2021), the state-of-the-art ﬁne-tuning tech-\nnique, as our main baseline.\nWe borrowed several other baselines from\nthe RocketQA paper, including lexical sys-\ntems BM25, DeepCT (Dai and Callan, 2019),\nDocT5Query (Nogueira and Lin, 2019) and\nGAR (Mao et al., 2020); and dense systems\nDPR (Karpukhin et al., 2020), ANCE (Xiong et al.,\n2021), and ME-BERT (Luan et al., 2020).\nWe also included the concurrent work DPR-\nPAQ (O ˘guz et al., 2021), which pre-trains us-\ning a 65-million-size synthetic QA pair dataset.\nThe pre-training data is created by using retriever-\nreader pairs trained on Natural Question and Trivia\nQA. Designing the synthesis procedure also re-\nquires domain knowledge, thus under the con-\ntext of this paper, we refer to this as a semi-\nsupervised pre-training method. We include 4 DPR-\nPAQ variants based on base/large architectures of\nBERT/RoBERTa models.\nFinally, we ﬁne-tune a Condenser model which\nis produced in the ﬁrst stage of pre-training.\n4.2.2 Results\nTable 1 shows development (dev) set performance\nfor MS-MARCO passage ranking and test set per-\nformance for Natural Question and Trivia QA.\nAcross three query sets, dense systems show supe-\nrior performance compared to sparse systems. We\nalso see a big performance margin between systems\ninvolving either careful ﬁne-tuning or pre-training\n(RocketQA, DPR-PAQ, Condenser, coCondenser)\nover earlier dense systems. This result conﬁrms re-\ncent ﬁndings that low dimension embeddings pos-\nsess a strong capacity for dense retrieval, a capacity\nhowever hard to exploit naively.\ncoCondenser shows small improvements over\nRocketQA. Importantly, this is achieved with\ngreatly reduced computation and data engineer-\ning effort in ﬁne-tuning. Notably on MS-MARCO,\ncoCondenser reduced the RocketQA’s 4096 batch\nsize to 64 (Table 5). A comparison of the two train-\ning pipelines of RocketQA and coCondenser can\nbe found in Figure 2.\nComparison with DPR-PAQ shows several\ninteresting ﬁndings. Combining large semi-\nsupervised pre-training with the better and larger\nLM RoBERTalarge, DPR-PAQ achieves the best\nresults on Natural Question. On the other hand,\nwhen starting from BERT (base/large), DPR-PAQ\nshows similar performance to coCondenser, which\nis based on BERT base. This suggests that large-\nscale semi-supervised pre-training is still the way\nto go to get the very best performance. However,\nwhen computational resources are limited and a\nlarge pre-training set is missing, the unsupervised\ncoCondenser is a strong alternative. On the other\nhand, as it moves to MS-MARCO where DPR-\nPAQ’s pre-training supervision becomes distant,\n2848\nMS-MARCO Passage Ranking Leaderboard\nRank Method MRR@10\n1 Adaptive Batch Scheduling + CoCondenser 43.1\n2 coCondenser* 42.8\nMS-MARCO Document Ranking Leaderboard\nRank Method MRR@100\n1 UniRetriever 44.0\n2 coCondenser + MORES+* 43.6\nTable 2: Performance on the MS-MARCO leader-\nboards with reranking. *Our submissions.\nwe observe that DPR-PAQ becomes less effective\nthan RocketQA and coCondenser.\nThe comparison between Condenser and coCon-\ndenser demonstrates the importance of the con-\ntrastive loss in coCondener: coCondenser can be\nrobustly ﬁne-tuned thanks to its pre-structured em-\nbedding space, allowing it to have better Recall\n(fewer false negatives) across all datasets.\n4.3 Reranking on MS-MARCO Eval\nDue to the leaderboard nature of MS-MARCO Eval\nset, we cannot do ablation studies on it but have\nonly made two submissions. We follow other top-\nperforming systems and add some form of reranker.\nFor the passage ranking leaderboard, we rerank\nthe top 1000 retrieved passages with an ensemble\nof ERNIE (Sun et al., 2020) and RoBERTa. We\nalso ﬁne-tuned a coCondenser on the MS-MARCO\ndocument ranking dataset. As passage retrieval\nis the focus of this paper, we retrieve based on\nthe ﬁrst passage of 512 tokens. The top100 are\nreranked by a fast modular reranker (Gao et al.,\n2020). Performance of best systems and ours are\nrecorded in Table 2. At the time of this paper’s\nsubmission, both of our systems are the 2nd best on\nthe two leaderboards. For passages ranking, we are\nexcited to see that other people are able to further\nimprove coCondenser with additional ﬁne-tuning\ntechniques. For document, we leave the study of\nretrieval beyond ﬁrst passage to future work and\nrefer readers to other leaderboard systems.\n5 Analysis\nRecall the two desired properties of coCondernser\nare local noise resistance to mislabeling and a well-\nstructured, pre-trained embedding space. In this\nsection, to investigate the former, we introduce\nand compare with a knowledge distillation (Hin-\nton et al., 2015) setup where we substitute noisy\nhard labels with soft labels from a cross-encoder.\nFor the latter, we measure the quality of 1st stage\nretriever mined negatives to see if coCondenser\nembedding can help ﬁnd related but not relevant\nhard negatives. We also provide ablation studies of\nloss components and, pre-training and ﬁne-tuning\nstages.\n5.1 Learning with Soft Labels\nTo analyze the local robustness of coCondenser,\nwe introduce a knowledge distillation upper-bound\nmodel: instead of training using the noisy labels,\nwe ﬁrst train a cross encoder and then ﬁne-tune a co-\nCondenser model using soft labels generated from\nthe cross-encoder. Unlike Qu et al. (2021) that uses\ncross encoder for ﬁltering, here we directly expose\nthe logits as soft labels. Concretely, given cross\nencoder g, a batch of M queries {q1, q2, .., qM },\neach paired with N passages (positive and hard\nnegatives) {p11, .., p1N , .., pMN }, for a query ql\nwe deﬁne its soft target distribution T,\nTij = softmaxj(g(ql, dij)) if i = l else 0 (15)\ni.e., the soft labels are normalized logits from g\nfor the local passages and 0 for the rest. Let\nSij = softmaxij(s(q, pij)), the normalized bi-\nencoder similarities. Loss is deﬁned as Kullback-\nLeibler divergence between S and T,\nLkd = DKL(S||T) (16)\nThis setup a) focuses on improving labels for lo-\ncal hard negatives and positives while b) avoids\nevaluating cross encoder g for in-batch negatives.\nIn Table 3, we compare coCondenser trained with\nMS-MARCO Dev\nModel Label MRR@10 R@100 R@1K\nBERT Hard 33.4 85.1 95.4\ncoCondenser Hard 38.2 91.3 98.4\ncoCondenser Soft 39.1 91.9 98.6\nTable 3: Fine-tuning with hard v.s. soft labels.\nthe original hard labels and the cross-encoder gen-\nerated soft labels. Using soft labels indeed pro-\nduces some improvement. On the other hand, with-\nout help from the cross-encoder, coCondenser still\nyields performance within small margins, showing\ncoCondenser’s superior local noise resistance.\n5.2 Quality of Mined Negatives\nIntuitively, a globally better-structured embedding\nspace will be less likely to collapse, producing a\nmore accurate set of mined negatives for the second\nstage retriever to learn over. In other words, the\n2849\n2nd stage retriever will produce less unexpected\ntop (hard) negatives. To quantitatively measure\nthis, we propose a new metric, top n neighborhood\nrecall at depth k ( nb-recalln@k): for a query, the\ncoverage over the 2nd stage retriever’s topn can-\ndidates by the 1st stage retrievers top k candidates,\nnb-recalln@k = {stage1 top k}∩{stage2 top n}\nn (17)\nEssentially, this measures how the mined hard neg-\natives agree with the actual negatives, or simply,\nhow well the 1st stage retriever locating hard nega-\ntives. We measure neighborhood recall of BERT,\nCondenser and coCondenser retrievers, averaged\nover all MS-MARCO Dev queries, forn = 50, 100\nand various k values, in Figure 3, We see consis-\n55.00\n65.00\n75.00\n85.00\n95.00\n50 100 150 200 250\nBERT Condenser coCondenser\nnb-recall n=50\nDepth K\n55.00\n65.00\n75.00\n85.00\n95.00\n100 200 300 400 500\nBERT Condenser coCondenser\nnb-recall n=100\nFigure 3: MS-MARCO Dev Neighborhood Recall\ntent higher nb-recall of Condenser over BERT and\ncoCondenser over Condenser. The former comes\nfrom stronger CLS representation while the latter\nis due to the globally better-structured embedding\nspace.\n5.3 Pre-training Loss Ablation\nIn this section, we conduct an ablation study to\nunderstand the second stage pre-training loss com-\nponents’ inﬂuence on the ﬁnal quality. In particular,\nwe consider a Condenser model further pre-trained\nwith only the contrastive loss.\nMS-MARCO Dev\nModel MRR@10 R@100 R@1K\nCondenser 36.6 89.4 97.4\n+ Contrative loss 36.7 90.3 98.0\n+ MLM loss† 38.2 91.3 98.4\nTable 4: Effect of loss components. †: This is the co-\nCondenser model.\nIn Table 4, we see that further pre-training Con-\ndenser with only contrastive loss leads to better\nrecall but similar MRR. The contrastive loss learns\na better embedding space but by itself cannot keep\nthe CLS locally discriminative. The original Con-\ndenser is able to rank better locally, producing\nsimilar MRR with fewer recalled passages. When\nboth contrastive and Condenser MLM loss are used,\nwe see improvements on all metrics. This again\nstresses the importance of the Condenser MLM\nloss during the second contrastive learning stage.\n5.4 Training Stages\nWe seek to understand the contribution of each\npre-training and ﬁne-tuning stage of coCondenser\nretriever. We consider pre-trained Condenser from\nthe ﬁrst stage and coCondenser from the second\nstage. For each, we consider retrievers trained with\nand without hard negatives (HN). For reference,\nwe compare also with various RocketQA training\nstages. Results are shown in Table 5. We see\nMethod Batch Size MS-MARCO Dev\nMRR@10 R@1000\nRocketQA\nCross-batch negatives 8192 33.3 -\n+ Hard negatives 4096 26.0 -\n+ Denoising 4096 36.4 -\n+ Data augmentation 4096 37.0 97.9\ncoCondenser\nCondenser w/o HN 64 33.8 96.1\n+ Hard negatives 64 36.6 97.4\ncoCondenser w/o HN 64 35.7 97.8\n+ Hard negatives 64 38.2 98.4\nTable 5: MS-MARCO Dev performance for various\ntraining stages RocketQA and coCondenser.\nthat each stage of RocketQA is critical. As each\nis added, performance improves steadily. On the\nother hand, this also suggests the full pipeline has\nto be executed to get the best performance. In com-\nparison, we see Condenser with hard negatives has\n2850\nperformance very close to the full RocketQA sys-\ntem. Condenser with hard negatives also has better\nMRR than coCondenser without hard negatives,\nmeaning that Condenser from the ﬁrst pre-training\nstage is already very strong locally but the embed-\nding space trained from a relatively cold start is\nstill not optimal, causing global misses. Adding\nthe corpus aware loss, coCondenser without hard\nnegatives has Recall very close to the full Rock-\netQA system, using only a size 64 batch. This again\nconﬁrms our hypothesis that ﬁne-tuning can beneﬁt\nfrom a pre-trained passage embedding space. Fur-\nther adding hard negatives, we get the strongest co-\nCondenser system that is both locally and globally\neffective. Note that all Condenser systems achieve\ntheir performance without denoising, showing the\nsuperior noise resistance capability learned using\nthe Condenser architecture. Practically, our sys-\ntems also do not require data augmentation, which\nremoves engineering effort in designing augmenta-\ntion techniques and deﬁning augmentation data.\n6 Conclusion\nThis paper introduces coCondenser, an unsuper-\nvised corpus-aware language model pre-training\nmethod. We demonstrate proper pre-training can\nestablish not only language understanding ability\nbut also corpus-level representation power. Lever-\naging the Condenser architecture and a corpus\naware contrastive loss, coCondenser acquires two\nimportant properties for dense retrieval, noise re-\nsistance, and structured embedding space. This\ncorpus-aware pre-training needs to be done once\nfor a search corpus and is query agnostic. The\nlearned model can be shared among various types\nof end task queries.\nExperiments show that coCondenser can drasti-\ncally reduce the costs of ﬁne-tuning a strong dense\nretriever. We also ﬁnd coCondenser yields per-\nformance close or similar to semi-supervised pre-\ntrained models that are several times larger.\nImportantly, coCondenser provides a hands-off\nway to pre-train a very effective LM for dense re-\ntrieval. In particular, it effectively removes the\neffort for designing and testing pre-training as well\nas ﬁne-tuning techniques. With our models, prac-\ntitioners can use limited resources to train dense\nretrieval systems with state-of-the-art level perfor-\nmance. Future works may also investigate inte-\ngrating additional pre-training and/or ﬁne-tuning\nmethods to further improve performance.\nAcknowledgments\nThis research was supported by NSF grant IIS-\n1815528. Any opinions, ﬁndings, and conclusions\nin this paper are the authors’ and do not necessarily\nreﬂect those of the sponsors. The authors would\nlike to thank Google’s TPU Research Cloud (TRC)\nfor access to Cloud TPUs and the anonymous re-\nviewers for the reviews.\nReferences\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder,\nAndrew McNamara, Bhaskar Mitra, Tri Nguyen,\nMir Rosenberg, Xia Song, Alina Stoica, Saurabh Ti-\nwary, and Tong Wang. 2018. Ms marco: A human\ngenerated machine reading comprehension dataset.\nJames Bradbury, Roy Frostig, Peter Hawkins,\nMatthew James Johnson, Chris Leary, Dougal\nMaclaurin, George Necula, Adam Paszke, Jake\nVanderPlas, Skye Wanderman-Milne, and Qiao\nZhang. 2018. JAX: composable transformations of\nPython+NumPy programs.\nWei-Cheng Chang, Felix X. Yu, Yin-Wen Chang, Yim-\ning Yang, and Sanjiv Kumar. 2020. Pre-training\ntasks for embedding-based large-scale retrieval. In\nInternational Conference on Learning Representa-\ntions.\nTing Chen, Simon Kornblith, Mohammad Norouzi,\nand Geoffrey E. Hinton. 2020. A simple frame-\nwork for contrastive learning of visual representa-\ntions. ArXiv, abs/2002.05709.\nZhuyun Dai and J. Callan. 2019. Context-aware sen-\ntence/passage term importance estimation for ﬁrst\nstage retrieval. ArXiv, abs/1910.10687.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nLuyu Gao and Jamie Callan. 2021. Condenser: a pre-\ntraining architecture for dense retrieval. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing , pages 981–993,\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nLuyu Gao, Zhuyun Dai, and Jamie Callan. 2020. Mod-\nularized transfomer-based ranking framework. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4180–4190, Online. Association for Computa-\ntional Linguistics.\n2851\nLuyu Gao, Zhuyun Dai, Tongfei Chen, Zhen Fan, Ben-\njamin Van Durme, and Jamie Callan. 2021a. Com-\nplement lexical retrieval model with semantic resid-\nual embeddings. In Advances in Information Re-\ntrieval - 43rd European Conference on IR Research,\nECIR 2021, Virtual Event, March 28 - April 1, 2021,\nProceedings, Part I.\nLuyu Gao, Xueguang Ma, Jimmy J. Lin, and Jamie\nCallan. 2022. Tevatron: An efﬁcient and ﬂexible\ntoolkit for dense retrieval. ArXiv, abs/2203.05765.\nLuyu Gao, Yunyi Zhang, Jiawei Han, and Jamie Callan.\n2021b. Scaling deep contrastive learning batch size\nunder memory limited setup. In Proceedings of the\n6th Workshop on Representation Learning for NLP\n(RepL4NLP-2021), pages 316–321, Online. Associ-\nation for Computational Linguistics.\nJohn Michael Giorgi, Osvald Nitski, Gary D Bader, and\nBo Wang. 2020. Declutr: Deep contrastive learn-\ning for unsupervised textual representations. ArXiv,\nabs/2006.03659.\nRuiqi Guo, Philip Sun, Erik Lindgren, Quan Geng,\nDavid Simcha, Felix Chern, and Sanjiv Kumar. 2020.\nAccelerating large-scale inference with anisotropic\nvector quantization. In International Conference on\nMachine Learning.\nKelvin Guu, Kenton Lee, Z. Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training. ArXiv,\nabs/2002.08909.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and\nRoss B. Girshick. 2020. Momentum contrast for\nunsupervised visual representation learning. 2020\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 9726–9735.\nGeoffrey E. Hinton, Oriol Vinyals, and J. Dean. 2015.\nDistilling the knowledge in a neural network. ArXiv,\nabs/1503.02531.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2017.\nBillion-scale similarity search with gpus. arXiv\npreprint arXiv:1702.08734.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale dis-\ntantly supervised challenge dataset for reading com-\nprehension. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers) , pages 1601–1611, Van-\ncouver, Canada. Association for Computational Lin-\nguistics.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 6769–\n6781, Online. Association for Computational Lin-\nguistics.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nﬁeld, Michael Collins, Ankur Parikh, Chris Al-\nberti, Danielle Epstein, Illia Polosukhin, Jacob De-\nvlin, Kenton Lee, Kristina Toutanova, Llion Jones,\nMatthew Kelcey, Ming-Wei Chang, Andrew M. Dai,\nJakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\nNatural questions: A benchmark for question an-\nswering research. Transactions of the Association\nfor Computational Linguistics, 7:452–466.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Sori-\ncut. 2020. Albert: A lite bert for self-supervised\nlearning of language representations. ArXiv,\nabs/1909.11942.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised open\ndomain question answering. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 6086–6096, Florence,\nItaly. Association for Computational Linguistics.\nY . Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar\nJoshi, Danqi Chen, Omer Levy, M. Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. Roberta:\nA robustly optimized bert pretraining approach.\nArXiv, abs/1907.11692.\nY . Luan, Jacob Eisenstein, Kristina Toutanova, and\nMichael Collins. 2020. Sparse, dense, and at-\ntentional representations for text retrieval. ArXiv,\nabs/2005.00181.\nYuning Mao, Pengcheng He, Xiaodong Liu, Yelong\nShen, Jianfeng Gao, Jiawei Han, and Weizhu Chen.\n2020. Generation-augmented retrieval for open-\ndomain question answering.\nTomas Mikolov, Kai Chen, G. S. Corrado, and J. Dean.\n2013. Efﬁcient estimation of word representations\nin vector space. In ICLR.\nRodrigo Nogueira and Jimmy Lin. 2019. From\ndoc2query to doctttttquery.\nBarlas O ˘guz, Kushal Lakhotia, Anchit Gupta, Patrick\nLewis, Vladimir Karpukhin, Aleksandra Piktus,\nXilun Chen, Sebastian Riedel, Wen tau Yih, Sonal\nGupta, and Yashar Mehdad. 2021. Domain-matched\npre-training tasks for dense retrieval.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Py-\ntorch: An imperative style, high-performance deep\nlearning library. In H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 32. Curran Associates, Inc.\n2852\nYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang\nRen, Wayne Xin Zhao, Daxiang Dong, Hua Wu,\nand Haifeng Wang. 2021. RocketQA: An opti-\nmized training approach to dense passage retrieval\nfor open-domain question answering. In Proceed-\nings of the 2021 Conference of the North Ameri-\ncan Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n5835–5847, Online. Association for Computational\nLinguistics.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao\nTian, Hua Wu, and Haifeng Wang. 2020. Ernie 2.0:\nA continual pre-training framework for language un-\nderstanding. Proceedings of the AAAI Conference\non Artiﬁcial Intelligence, 34(05):8968–8975.\nZ. Wu, Sinong Wang, Jiatao Gu, Madian Khabsa, Fei\nSun, and Hao Ma. 2020. Clear: Contrastive learning\nfor sentence representation. ArXiv, abs/2012.15466.\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul N. Bennett, Junaid Ahmed, and\nArnold Overwijk. 2021. Approximate nearest neigh-\nbor negative contrastive learning for dense text re-\ntrieval. In International Conference on Learning\nRepresentations.\nZ. Yang, Zihang Dai, Yiming Yang, J. Carbonell,\nR. Salakhutdinov, and Quoc V . Le. 2019. Xlnet:\nGeneralized autoregressive pretraining for language\nunderstanding. In NeurIPS.\nJingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min\nZhang, and Shaoping Ma. 2022. Learning discrete\nrepresentations via constrained clustering for effec-\ntive and efﬁcient dense retrieval. In Proceedings of\nthe Fifteenth ACM International Conference on Web\nSearch and Data Mining, pages 1328–1336.\n2853",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8105398416519165
    },
    {
      "name": "Embedding",
      "score": 0.7103980183601379
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5807958245277405
    },
    {
      "name": "Training (meteorology)",
      "score": 0.5435175895690918
    },
    {
      "name": "Language model",
      "score": 0.513263463973999
    },
    {
      "name": "Training set",
      "score": 0.4322953224182129
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.42554113268852234
    },
    {
      "name": "Noise (video)",
      "score": 0.41706836223602295
    },
    {
      "name": "Natural language processing",
      "score": 0.3922465741634369
    },
    {
      "name": "Machine learning",
      "score": 0.3737885355949402
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Meteorology",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    }
  ],
  "cited_by": 128
}