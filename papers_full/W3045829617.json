{
  "title": "On-The-Fly Information Retrieval Augmentation for Language Models",
  "url": "https://openalex.org/W3045829617",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5103861470",
      "name": "Hai Wang",
      "affiliations": [
        "Toyota Technological Institute at Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A5033089246",
      "name": "David McAllester",
      "affiliations": [
        "Toyota Technological Institute at Chicago"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963951265",
    "https://openalex.org/W2964092725",
    "https://openalex.org/W2951672049",
    "https://openalex.org/W2970401203",
    "https://openalex.org/W2583687993",
    "https://openalex.org/W2952556884",
    "https://openalex.org/W2275625487",
    "https://openalex.org/W2971155257",
    "https://openalex.org/W2250635077",
    "https://openalex.org/W2962781483",
    "https://openalex.org/W2131494463",
    "https://openalex.org/W2963781647",
    "https://openalex.org/W2540419089",
    "https://openalex.org/W2962966012",
    "https://openalex.org/W2473344385",
    "https://openalex.org/W2962691502",
    "https://openalex.org/W2963448850",
    "https://openalex.org/W2741075451",
    "https://openalex.org/W2964091467",
    "https://openalex.org/W2526108667",
    "https://openalex.org/W2963015836",
    "https://openalex.org/W2252123151",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2769099080",
    "https://openalex.org/W4297827933",
    "https://openalex.org/W2946224300",
    "https://openalex.org/W4294611325",
    "https://openalex.org/W2963346557",
    "https://openalex.org/W2604685013",
    "https://openalex.org/W4288284003",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2890961898",
    "https://openalex.org/W2400680200",
    "https://openalex.org/W2891501508",
    "https://openalex.org/W2985074250",
    "https://openalex.org/W2963033680",
    "https://openalex.org/W2251552857",
    "https://openalex.org/W2798416089"
  ],
  "abstract": "Here we experiment with the use of information retrieval as an augmentation for pre-trained language models. The text corpus used in information retrieval can be viewed as form of episodic memory which grows over time. By augmenting GPT 2.0 with information retrieval we achieve a zero shot 15% relative reduction in perplexity on Gigaword corpus without any re-training. We also validate our IR augmentation on an event co-reference task.",
  "full_text": "Proceedings of the 1st Joint Workshop on Narrative Understanding, Storylines, and Events, pages 114–119\nJuly 9, 2020.c⃝2020 Association for Computational Linguistics\n114\nOn-The-Fly Information Retrieval Augmentation for Language Models\nHai Wang David McAllester\nToyota Technological Institute at Chicago, Chicago, IL, USA\n{haiwang,mcallester}@ttic.edu\nAbstract\nHere we experiment with the use of infor-\nmation retrieval as an augmentation for pre-\ntrained language models. The text corpus\nused in information retrieval can be viewed as\nform of episodic memory which grows over\ntime. By augmenting GPT 2.0 with informa-\ntion retrieval we achieve a zero shot 15% rel-\native reduction in perplexity on Gigaword cor-\npus without any re-training. We also validate\nour IR augmentation on an event co-reference\ntask.\n1 Introduction\nWe are interested in exploring the value of long\nterm episodic memory in language modeling. For\nexample, a language model can be used in January\nto assign a probability distribution over the state-\nments that will appear in the newspaper in March.\nBut one month later, in February, the distribution\nover the predictions for March should be updated\nto take into account factual developments since\nthe previous prediction. Long term episodic mem-\nory should be taken into account when assigning a\nprobability to a statement.\nHere we take a simple approach in which a pre-\ntrained GPT language model (Radford et al., 2018a,\n2019) is zero-shot augmented with an episodic\nmemory consisting simply of a corpus of past news\narticles. Conceptually the past news articles are\nviewed as additional training data which can be\nlegitimately accessed when evaluating on future\ntext. In our most basic experiment we calculate\nthe probability of a future article by ﬁrst calculat-\ning the probability of its ﬁrst k sentences using\nthe pre-trained GPT model. We then use the ﬁrst\nksentences as a query in an information retrieval\nsystem to extract a relevant past article. We then\ninsert the past article following the ﬁrst ksentences\nwhen calculating the probability of the remainder\nof the future article using the same pre-trained GPT\nmodel. This is a zero-shot augmentation in the\nsense that there is no additional training or ﬁne tun-\ning of the pre-trained model. Our results show that\nthis augmentation signiﬁcantly reduces perplexity.\nWe also present various other experiments includ-\ning results on ﬁne-tuning the model in the presence\nof the memory and the effect of this memory on\nevent co-reference.\n2 Related Work\nVarious language models have utilized external\nknowledge or long contexts (Paperno et al., 2016;\nYang and Mitchell, 2017; Peng et al., 2019; Khan-\ndelwal et al., 2018; Ghosh et al., 2016; Lau et al.,\n2017; Grave et al., 2016; Parthasarathi and Pineau,\n2018). But these papers do not address the question\nof whether additional context or external knowl-\nedge is useful as a zero-shot augmentation of large\nscale pre-trained NLP models.\nThe value of external knowledge has previously\nbeen demonstrated for NLP tasks such as natu-\nral language inference (Chen et al., 2018; Yang\net al., 2019), language generation (Parthasarathi\nand Pineau, 2018), knowledge base comple-\ntion (Toutanova et al., 2015; Das et al., 2017) and\nquestion answering (Sun et al., 2019, 2018; Dhin-\ngra et al., 2017). However, all those prior works\nassume the model is small and trained from scratch.\nAs large scale pre-trained models have become\nmore powerful it is not immediately clear whether\nexternal resources can still add value. The only\nwork we know of on using external resources in\nmodern large scale models is Yang et al. (2019)\nwhere a human curated external lexical resource is\nused to improve BERT.\nOur approach bears some resemblance to neural\ncache models (Grave et al., 2016). However, neural\ncache models store past hidden states as memory\n115\nand accesses them through a dot product with the\ncurrent hidden states. This is different from retriev-\ning knowledge from a corpus-sized memory.\nOur approach is also somewhat related to mem-\nory networks (Weston et al., 2014). Memory net-\nworks have a memory module which can be learnt\njointly with other components. It has shown suc-\ncess in applications such as machine reading com-\nprehension (Kumar et al., 2016a,b; Shi et al., 2016)\nand visual question answering (Na et al., 2017; Ma\net al., 2018; Su et al., 2018). Signiﬁcant progress in\nmemory networks has been achieved in both archi-\ntecture (Chandar et al., 2016; Miller et al., 2016;\nGulcehre et al., 2017) and model scale (Rae et al.,\n2016; Lample et al., 2019).\nSeveral papers have formulated, and experi-\nmented with, scalable memory networks — mem-\nory networks that employ some method of efﬁ-\nciently reading and writing to very large neural\nmemories. This is done with approximate nearest\nneighbor methods in Rae et al. (2016) and with\nproduct keys in Lample et al. (2019). These large\nmemories are used to provide additional model\ncapacity where the memory contents are trained\nover a large data set using gradient descent train-\ning, just as one would train the parameters of a\nvery large network. It is shown in Lample et al.\n(2019) that it is possible to insert a large memory\nas a layer in a transformer architecture resulting a\nmodel where the same number of parameters and\nthe same performance can be achieved with half\nthe layers and with much faster training time than\na standard transformer architecture. Here, however,\nwe are proposing zero-shot augmentation with an\nexternal data source used as an episodic memory.\nThe use of key-value memories in Miller et al.\n(2016) is particularly similar to our model. Key-\nvalue memories were used there in treating a corpus\nof Wikipedia movie pages as a memory for answer-\ning questions about movies. As in our system,\narticles were extracted using word based informa-\ntion retrieval. Each article was encoded as a vector\nwhich was then given to a question answering archi-\ntecture. This was shown to improve on automated\nknowledge base extraction from the same corpus\nbut was still not competitive with human curated\nknowledge graphs for movies. Here we give the\ntext of the retrieved article directly to the language\nmodel architecture and focus on augmenting large\nscale language models.\n3 Model\nWe use the pre-trained transformer GPT 2.0 (Rad-\nford et al., 2019). Let Ww and Wp be the subword\nand position embeddings respectively. Let M de-\nnote the total number of layers, for a token at time\nstep t, the m-th layer’s hidden statehm\nt is given by:\nhm\nt =\n{\nWw + Wp if m= 0\nTB(hm−1\nt ) if 1 ≤m≤M\nwhere TB stands for Transformer Block. We use\nlast layer’s hidden statehM\nt as the presentation Ht\nfor the token at time step t. We augment GPT 2.0\nwith a large episodic memory component, and the\noverall architecture is shown in Figure 1.\nFigure 1: GPT with large episodic memory component\nFor a sequence Swith T tokens, let S1, ... , Sp\nbe the tokens of the ﬁrst ksentences. Let C be a\nsequence (article) retrieved from memory using the\nﬁrst ksentences as the query, the vector Ht is:\nHt =\n{\nGPT(S1,...,S t),if t≤p\nGPT(S1,...,S p,C,...,S t),otherwise\nThat’s to say, for the ﬁrstksentences, we directly\nfeed them to GPT to obtain their representations.\nFor remaining sentences, their representations are\nconditioned on both the ﬁrst ksentences and the\nretrieved context C. Table 1 compares features of\nour simple memory augmentation with those of\nother memory models.\n4 Experiments\nWe focus on two tasks: document level language\nmodelling and event co-retrieved . In both tasks we\ntake a document as input and use ﬁrst ksentences\nto query the memory. To calculate the perplexity\nof a document, we compute the log-probability of\na document by multiplying byte level probability,\n116\nModel episodic search memory size\nDMN yes exact ∼1K words\nSAM: no approx ∼100K slots\nKVM: yes exact ≤1M slots\nLMN: no exact ∼1M slots\nOurs: yes approx ∼10M documents\nTable 1: Comparison between different models. DMN:\nDynamic Memory Network (Kumar et al., 2016b);\nSAM: Sparse Access Memory (Rae et al., 2016); KVM:\nKey Value Memory (Miller et al., 2016); LMN: Large\nMemory Network (Lample et al., 2019). Memory size\nis measured in their own words.\nthen divide the log-probability by the actual word\ncount in the query document.\nWe use Gigaword (Parker et al., 2011) as both\nour language modeling test set and as our external\nmemory. Gigaword contains news from different\nsources such as NY Times and XinHua News etc.\nFor language modelling we use the NY Times por-\ntion because it is written by native English speakers.\nSince GPT 2.0 is trained on Common Crawl which\ncontains news collections started from 2008. To\navoid testing on GPT-2 training data, we use Gi-\ngaword articles collected prior to 2008. For the\npre-trained language model we use GPT 2.0 (Rad-\nford et al., 2019) 1. It contains three pre-trained\nmodels: GPT Small, Medium and Large.\nFor information retrieval we use Lucene due to\nits simplicity. Given a query document we ﬁrst do\nsentence and word tokenization and then use the\nﬁrst k sentences to retrieve top 20 retrieved doc-\numents with the default TF-IDF distance metric\nprovided by Lucene. Since too distant document\npairs are uninformative and too related document\npairs tends to be duplicates of the test article, we\nfurther ﬁlter those top ranked documents by time\nstamp, news source and cosine similarity. More\nspeciﬁcally, we choose the highest ranked retrieved\ndocument that simultaneously satisﬁes the follow-\ning three conditions: it comes from a different news\nsource; it appears earlier but within two weeks\ntime window of the test document, and the bag of\nword cosine similarity between the test and the re-\ntrieved cannot be larger than 0.6αwhere αis the\nlargest bag of word cosine similarity between the\ntest article and any retrieved articles. To support\nﬁne-tuning experiments we constructed a corpus\nof pairs of a query article and a cached retrieved\n1https://github.com/huggingface/pytorch-transformers\ndocument. We split the dataset into train/dev/test\nby query document’s time stamp. The train/dev/test\nsize is: 79622,16927,8045. For zero-shot experi-\nments we use the test set of 8045 articles. We do\nexperiments with k∈{1,2,5}.\nTo check the quality ofquery-retrieved pairs, we\nrandomly sample 100 pairs from dev set and com-\npute the bag of word cosine similarity between the\ntwo documents. The mean cosine similarity is 0.15.\nWe also manually inspect them: we ask two NLP\nresearchers to annotate the query-retrieved pair\nas “BAD” or “OK” independently, i.e., if two doc-\numents are almost duplicates or totally unrelated,\nthen it’s “BAD”, otherwise, it’s “OK”. Among\n100 pairs, 83 pairs are “OK”, 17 pairs are “BAD”\ndue to irrelevance. The Cohen’s kappa coefﬁcient\nbetween two annotations is 0.94.\n4.1 Language modelling\nFor language modeling we try zero-shot memory\naugmentation, ﬁne-tuned memory augmentation,\nand training a small memory-augmented network\nfrom scratch. When training, we use the Adam\noptimizer from GPT 1.0 (Radford et al., 2018b).\nThe learning rate is 0.001, weight decay parameter\nis 0.01, the warm up proportion is 0.1. For other\nparameters, we use the default values from GPT\n2.0. The ﬁne-tuning on Gigaword takes less than\none day with a single GPU.\nZero-shot and ﬁne-tuning results Following\nRadford et al. (2019), we ﬁrst evaluate our model\non Gigaword with zero-shot setting and then ﬁne-\ntune the model. The results are given in Table 2.\nModel Size woc k=1 k=2 k=5\nGPT-Small 35.15 29.29 30.54 32.38\nGPT-Medium 22.78 19.84 20.54 21.48\nGPT-Large 19.90 17.41 18.00 18.80\nGPT-Small 23.03 21.01 21.89 22.66\nTable 2: Perplexity for zero-shot (top 3 rows) and ﬁne-\ntuning (last row) settings when use different k to re-\ntrieve the context. woc: without retrieved context.\nFrom Table 2, we see that with additional context\nretrieved from episodic memory, for all different\nGPT models, we obtain signiﬁcantly lower perplex-\nity than using original GPT 2.0. When ﬁne tuning\nthe model with context, we can further reduce the\noverall perplexity. We only ﬁne tune GPT small\ndue to our GPU memory constraints. Preliminary\n117\nanalysis indicates that most of the perplexity re-\nduction comes at content words and semantically\nrich words where predictions require broader con-\ntext. This is consistent with the phenomena found\nin Khandelwal et al. (2018). We further ﬁnd that\nsmaller k leads to slightly worse retrieval qual-\nity, however, more continued sentences will ben-\neﬁt from the retrieved context. Since Gigaword\ncontains newswire, the ﬁrst several sentences usu-\nally are importation summarizations, thus overall,\nsmaller kwill result in lower perplexity.\nTrain from scratch We also investigate train-\ning this form of memory-augmented model from\nscratch on our query-retrieved pairs. For these ex-\nperiments we train smaller transformers and the\nresults are given in Table 3. From Table 3, we see\nthat additional context still helps and we can get\ndecent perplexity even with quite small models.\nModel Conﬁg woc k=1 k=2 k=5\nE=384,H=6,L=6 35.62 31.94 33.18 35.26\nE=384,H=8,L=8 33.67 29.62 30.76 32.73\nE=576,H=8,L=8 31.32 27.38 28.54 30.63\nTable 3: Perplexity when train from scratch. E: hidden\nstates dimensionality; H: # of head; L: # of layer. GPT-\nSmall has the conﬁguration: E=764, H=12, L=12.\nWhen context is irrelevant We also evaluate\nour method on Wikitext-2/103, in which the re-\ntrieved context is irrelevant due to domain differ-\nence between Wikipedia and Gigaword. In this\ncase, we use the most top ranked document from\nGigaword as reference. Table 4 shows that irrele-\nvant contexts have very little impact on perplexity.\nDataset woc k=1 k=2 k=5\nWikitext-2 28.67 28.96 28.95 28.70\nWikitext-103 25.38 25.68 25.56 25.39\nTable 4: Zero-shot perplexity using GPT-Small\n4.2 Event Co-reference\nIntuitively episodic memory is useful because it\ncontains information about the particular events\nmentioned in the test document. With this in mind\nwe evaluate our approach on the event co-reference\ndataset ECB+ (Cybulska and V ossen, 2014). ECB+\ncontains 982 documents clustered into 43 topics,\nand has two evaluation settings: coreferring men-\ntions occurring within a single document (within\ndocument) or across a document collection (cross\ndocument). For the event co-reference pipeline, we\nfollow the joint modeling method of Barhom et al.\n(2019) where they jointly represented entity and\nevent mentions with various features and learned\na pairwise mention/entity scorer for coreference\nclassiﬁcation. We augment their mention features\nwith the mention’s vector representations extracted\nfrom either GPT 2.0 or our zero-shot augmented\nGPT 2.0. For event co-reference, we use the whole\ntest document to retrieve the context from Giga-\nword. From Table 5, we see that the context can\nhelp boost the CONLL F1 score.\nSystem MUC B 3 CONLL\nWithin Document\nKCP 63.0 92.0 81.0\nJM 70.9 93.5 85.1\nJM+GPT 80.1 93.5 85.2\nJM+GPT+CTX♣ 80.2 93.9 85.4\nCombined Within and Cross Document\nCV 73.0 74.0 73.0\nKCP 69.0 69.0 69.0\nJM 80.9 80.3 79.5\nJM+GPT 81.2 80.2 79.6\nJM+GPT+CTX♣ 81.3 80.5 79.8\nTable 5: F1 score on ECB+ dataset. KCP: Kenyon-\nDean et al. (2018) where they add a clustering-oriented\nregularization term; CV: Cybulska and V ossen (2015)\nwhere they add the feature calculated from “event tem-\nplate”; JM: Barhom et al. (2019). ♣: we also feed the\nretrieved context to GPT to get the representation.\n5 Conclusion\nIn this paper we propose a method to augment a\npre-trained NLP model with a large episodic mem-\nory. Unlike previous work, we use information\nretrieval to handle a large external corpus of text\nand feed retrieved documents directly to language\nmodels. Evaluation results on language modelling\nand event co-reference show the promise of our\nmethod. To the best of our knowledge, this is\nthe ﬁrst work that augments pre-trained NLP mod-\nels with large episodic memory. In principle, the\nmemory-augmented GPT-2 can be used as a variant\nof GPT-2 for any downstream tasks, such as GLUE\ntasks (Wang et al., 2018), although we have not\nexperimented with that here.\n118\nReferences\nShany Barhom, Vered Shwartz, Alon Eirew, Michael\nBugert, Nils Reimers, and Ido Dagan. 2019. Re-\nvisiting joint modeling of cross-document entity and\nevent coreference resolution. pages 4179–4189.\nSarath Chandar, Sungjin Ahn, Hugo Larochelle, Pascal\nVincent, Gerald Tesauro, and Yoshua Bengio. 2016.\nHierarchical memory networks. arXiv preprint\narXiv:1605.07427.\nQian Chen, Xiaodan Zhu, Zhen-Hua Ling, Diana\nInkpen, and Si Wei. 2018. Neural natural language\ninference models enhanced with external knowledge.\nIn Proceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 2406–2417.\nAgata Cybulska and Piek V ossen. 2014. Using a\nsledgehammer to crack a nut? lexical diversity and\nevent coreference resolution. In Proceedings of\nthe Ninth International Conference on Language Re-\nsources and Evaluation (LREC-2014) , pages 4545–\n4552.\nAgata Cybulska and Piek V ossen. 2015. Translating\ngranularity of event slots into features for event\ncoreference resolution. In Proceedings of the the 3rd\nWorkshop on EVENTS: Deﬁnition, Detection, Coref-\nerence, and Representation, pages 1–10.\nRajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer,\nLuke Vilnis, Ishan Durugkar, Akshay Krishna-\nmurthy, Alex Smola, and Andrew McCallum. 2017.\nGo for a walk and arrive at the answer: Reasoning\nover paths in knowledge bases using reinforcement\nlearning. arXiv preprint arXiv:1711.05851.\nBhuwan Dhingra, Zhilin Yang, William W Cohen, and\nRuslan Salakhutdinov. 2017. Linguistic knowledge\nas memory for recurrent neural networks. arXiv\npreprint arXiv:1703.02620.\nShalini Ghosh, Oriol Vinyals, Brian Strope, Scott Roy,\nTom Dean, and Larry Heck. 2016. Contextual lstm\n(clstm) models for large scale nlp tasks. arXiv\npreprint arXiv:1602.06291.\nEdouard Grave, Armand Joulin, and Nicolas Usunier.\n2016. Improving neural language models with a con-\ntinuous cache. arXiv preprint arXiv:1612.04426.\nCaglar Gulcehre, Sarath Chandar, and Yoshua Ben-\ngio. 2017. Memory augmented neural net-\nworks with wormhole connections. arXiv preprint\narXiv:1701.08718.\nKian Kenyon-Dean, Jackie Chi Kit Cheung, and Doina\nPrecup. 2018. Resolving event coreference with\nsupervised representation learning and clustering-\noriented regularization. In Proceedings of the Sev-\nenth Joint Conference on Lexical and Computa-\ntional Semantics, pages 1–10.\nUrvashi Khandelwal, He He, Peng Qi, and Dan Juraf-\nsky. 2018. Sharp nearby, fuzzy far away: How neu-\nral language models use context. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 284–294.\nAnkit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer,\nJames Bradbury, Ishaan Gulrajani, Victor Zhong,\nRomain Paulus, and Richard Socher. 2016a. Ask\nme anything: Dynamic memory networks for natu-\nral language processing. In International conference\non machine learning, pages 1378–1387.\nAnkit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer,\nJames Bradbury, Ishaan Gulrajani, Victor Zhong,\nRomain Paulus, and Richard Socher. 2016b. Ask\nme anything: Dynamic memory networks for natu-\nral language processing. In International conference\non machine learning, pages 1378–1387.\nGuillaume Lample, Alexandre Sablayrolles,\nMarc’Aurelio Ranzato, Ludovic Denoyer, and\nHerv´e J ´egou. 2019. Large memory layers with\nproduct keys. In Advances in Neural Information\nProcessing Systems, pages 8546–8557.\nJey Han Lau, Timothy Baldwin, and Trevor Cohn.\n2017. Topically driven neural language model. In\nProceedings of the 55th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 355–365.\nChao Ma, Chunhua Shen, Anthony Dick, Qi Wu, Peng\nWang, Anton van den Hengel, and Ian Reid. 2018.\nVisual question answering with memory-augmented\nnetworks. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages\n6975–6984.\nAlexander Miller, Adam Fisch, Jesse Dodge, Amir-\nHossein Karimi, Antoine Bordes, and Jason Weston.\n2016. Key-value memory networks for directly read-\ning documents. In Proceedings of the 2016 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1400–1409.\nSeil Na, Sangho Lee, Jisung Kim, and Gunhee Kim.\n2017. A read-write memory network for movie story\nunderstanding. In Proceedings of the IEEE Interna-\ntional Conference on Computer Vision , pages 677–\n685.\nDenis Paperno, Germ ´an Kruszewski, Angeliki Lazari-\ndou, Ngoc Quan Pham, Raffaella Bernardi, San-\ndro Pezzelle, Marco Baroni, Gemma Boleda, and\nRaquel Fernandez. 2016. The lambada dataset:\nWord prediction requiring a broad discourse context.\nIn Proceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 1525–1534.\nRobert Parker, David Graff, Junbo Kong, Ke Chen, and\nKazuaki Maeda. 2011. English gigaword. Linguis-\ntic Data Consortium.\n119\nPrasanna Parthasarathi and Joelle Pineau. 2018. Ex-\ntending neural generative conversational model us-\ning external knowledge sources. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing, pages 690–695.\nHaoruo Peng, Qiang Ning, and Dan Roth. 2019.\nKnowSemLM: A Knowledge Infused Semantic Lan-\nguage Model. In Proc. of the Conference on Compu-\ntational Natural Language Learning (CoNLL).\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018a. Improving language under-\nstanding by generative pre-training. OpenAI Blog.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018b. Improving language under-\nstanding by generative pre-training. In Preprint.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8).\nJack Rae, Jonathan J Hunt, Ivo Danihelka, Timo-\nthy Harley, Andrew W Senior, Gregory Wayne,\nAlex Graves, and Timothy Lillicrap. 2016. Scal-\ning memory-augmented neural networks with sparse\nreads and writes. In Advances in Neural Information\nProcessing Systems, pages 3621–3629.\nJing Shi, Yiqun Yao, Suncong Zheng, Bo Xu, et al.\n2016. Hierarchical memory networks for answer se-\nlection on unknown words. In Proceedings of COL-\nING 2016, the 26th International Conference on\nComputational Linguistics: Technical Papers, pages\n2290–2299.\nZhou Su, Chen Zhu, Yinpeng Dong, Dongqi Cai,\nYurong Chen, and Jianguo Li. 2018. Learning vi-\nsual knowledge memory networks for visual ques-\ntion answering. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition ,\npages 7736–7745.\nHaitian Sun, Tania Bedrax-Weiss, and William W Co-\nhen. 2019. Pullnet: Open domain question answer-\ning with iterative retrieval on knowledge bases and\ntext. arXiv preprint arXiv:1904.09537.\nHaitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn\nMazaitis, Ruslan Salakhutdinov, and William Cohen.\n2018. Open domain question answering using early\nfusion of knowledge bases and text. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 4231–4242.\nKristina Toutanova, Danqi Chen, Patrick Pantel, Hoi-\nfung Poon, Pallavi Choudhury, and Michael Gamon.\n2015. Representing text for joint embedding of text\nand knowledge bases. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1499–1509.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. Glue:\nA multi-task benchmark and analysis platform for\nnatural language understanding. In Proceedings\nof the 2018 EMNLP Workshop BlackboxNLP: An-\nalyzing and Interpreting Neural Networks for NLP ,\npages 353–355.\nJason Weston, Sumit Chopra, and Antoine Bor-\ndes. 2014. Memory networks. arXiv preprint\narXiv:1410.3916.\nBishan Yang and Tom Mitchell. 2017. Leveraging\nknowledge bases in lstms for improving machine\nreading. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1436–1446.\nXiaoyu Yang, Xiaodan Zhu, Huasha Zhao, Qiong\nZhang, and Yufei Feng. 2019. Enhancing unsuper-\nvised pretraining with external knowledge for natu-\nral language inference. In Canadian Conference on\nArtiﬁcial Intelligence, pages 413–419. Springer.",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9497464895248413
    },
    {
      "name": "Computer science",
      "score": 0.8548312783241272
    },
    {
      "name": "Natural language processing",
      "score": 0.6784112453460693
    },
    {
      "name": "Language model",
      "score": 0.6771949529647827
    },
    {
      "name": "Task (project management)",
      "score": 0.6646880507469177
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6406562328338623
    },
    {
      "name": "Question answering",
      "score": 0.6010562777519226
    },
    {
      "name": "Information retrieval",
      "score": 0.5146380662918091
    },
    {
      "name": "On the fly",
      "score": 0.46509066224098206
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I160992636",
      "name": "Toyota Technological Institute at Chicago",
      "country": "US"
    }
  ]
}