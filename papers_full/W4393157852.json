{
  "title": "Learning Pattern-Based Extractors from Natural Language and Knowledge Graphs: Applying Large Language Models to Wikipedia and Linked Open Data",
  "url": "https://openalex.org/W4393157852",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A3041369049",
      "name": "Célian Ringwald",
      "affiliations": [
        "Institut national de recherche en informatique et en automatique"
      ]
    },
    {
      "id": "https://openalex.org/A3041369049",
      "name": "Célian Ringwald",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3214342214",
    "https://openalex.org/W2970808735",
    "https://openalex.org/W6846307307",
    "https://openalex.org/W4385574107"
  ],
  "abstract": "Seq-to-seq transformer models have recently been successfully used for relation extraction, showing their flexibility, effectiveness, and scalability on that task. In this context, knowledge graphs aligned with Wikipedia such as DBpedia and Wikidata give us the opportunity to leverage existing texts and corresponding RDF graphs in order to extract, from these texts, the knowledge that is missing in the corresponding graphs and meanwhile improve their coverage. The goal of my thesis is to learn efficient extractors targeting specific RDF patterns and to do so by leveraging the latest language models and the dual base formed by Wikipedia on the one hand, and DBpedia and Wikidata on the other hand.",
  "full_text": "Learning Pattern-Based Extractors from Natural Language and Knowledge\nGraphs: Applying Large Language Models to Wikipedia and Linked Open Data\nC´elian Ringwald\nUniversit´e Cˆote d’Azur, Inria, CNRS, I3S\ncelian.ringwald@inria.fr\nAbstract\nSeq-to-seq transformer models have recently been success-\nfully used for relation extraction, showing their ﬂexibility, ef-\nfectiveness and scalability on that task. In this context, knowl-\nedge graphs aligned with Wikipedia such as DBpedia and\nWikidata give us the opportunity to leverage existing texts\nand corresponding RDF graphs in order to extract, from these\ntexts, the knowledge that is missing in the corresponding\ngraphs and meanwhile improve their coverage. The goal of\nmy thesis is to learn efﬁcient extractors targeting speciﬁc\nRDF patterns and to do so by leveraging the latest language\nmodels and the dual base formed by Wikipedia on the one\nhand, and DBpedia & Wikidata on the other hand.\nIntroduction\nWhether automatically extracted from structured elements\nof articles or manually populated, the open and linked data\npublished in DBpedia and Wikidata offer rich and structured\ncomplementary views of the textual descriptions found in\nWikipedia. However, the unstructured text of Wikipedia ar-\nticles contains a lot of information that is still missing in\nDBpedia and Wikidata. Extracting them would be interest-\ning in order to improve the coverage and quality of these\nknowledge graphs (KG) since they have an important impact\non all downstream tasks. This thesis proposes to exploit the\ndual bases formed from Wikipedia pages and Linked Open\nData (LOD) bases covering the same subjects in natural lan-\nguage and in RDF, in order to produce RDF extractors tar-\ngeting speciﬁc RDF patterns and tuned for a given language.\nTherefore, the main research question addressed in my the-\nsis is:\nRQ – Can we learn efﬁcient customized extractors tar-\ngeting speciﬁc RDF patterns from the dual base formed by\nWikipedia on one hand, and DBpedia and Wikidata on the\nother hand?\nFormally, let Db be a dual base, subset of W \u0002G where\nW is the set of Wikipedia articles and G is the set of cor-\nresponding RDF graphs in DBpedia and Wikidata. The aim\nis to learn from this dual base an extractor EDb : W !\nL; (t; S) 7!EDb(t; S) = g, where L is the LOD, t is an\ninput text, S is a set of RDF patterns expressing constraints\nCopyright c\r 2024, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nrepresented as SHACL shapes, and g is an RDF graph im-\nplied by t and valid against S.\nThis question is closely tied to the relations extrac-\ntion (RE) task which consists of retrieving relations\nfrom unstructured texts. Until recently, the RE task was\nsolved by complex pipelines including multiple steps. But\nthis approach leads to error accumulations and propaga-\ntion (Mesquita et al. 2019). However, the latest progresses\nin NLP including pre-trained language models (PLM) have\ndrastically improved the performance of many downstream\ntasks (Ye et al. 2022).\nAn Incremental Research Plan\nFollowing an incremental methodology, I intend to general-\nize the approach by relaxing one constraint at a time: starting\nfrom the generation of a single triple pattern before gener-\nalizing to arbitrary basic graph patterns. I will do so by ad-\ndressing sequentially the following sub-questions:\nSRQ.1 – How to survey and follow the latest trends in\nPLM-based KG extraction?\nThe landscape of the research ﬁeld drawn at the intersection\nof language models and knowledge graphs is very dynamic\nand quickly evolving. Consequently, a systematic literature\nreview to closely follow it, is crucial in this context.\nThen, leveraging some of the latest techniques identiﬁed,\nthe next research questions are set to ﬁnd the currently best-\nperforming approaches for a gradually more complex ver-\nsion of our task.\nSRQ.2 – Which aspects of the task formulation impact the\ngeneration of triples with datatype properties?\nThe performance of the task learned on top of an existing\nlanguage model crucially depends on the formulation of this\ntask. In this end, the choice of a speciﬁc RDF syntax for\nthe extraction, as well as the content of the prompt given as\ninput, are sensitive parameters in order to take full advantage\nof the PLM. One of the ﬁrst steps in this work will be to\ndetermine the combination leading to the best results when\nfocusing only on the datatype properties.\nSRQ.3 – How to jointly extract datatype properties and\nobject properties for a KG?\nThe scientiﬁc community has recently underlined the “hal-\nlucination” problem of PLMs. In practice, this issue may af-\nfect a large proportion of the triples containing literals (e.g.\nattributes such as dates, measures, textual descriptions, etc.).\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23411\nThese relations are deﬁned in the OWL semantic web lan-\nguage as datatype properties. In the literature, the research\nconducted until today was more focused on the extraction of\nobject properties that link together two objects. I will have to\npropose a method to jointly extract both types of properties.\nSRQ.4 – How to support fact extraction relying on differ-\nent document granularity?\nRelations can span different levels of a document (sentences,\nparagraphs, sections, etc.). The recent development of infor-\nmation retrieval techniques based on embedding seems to be\na promising solution that must be adapted to our context.\nSRQ.5 – What is the best strategy to extract rare relations\nand under-represented instances of classes?\nThe state-of-the-art models struggle with the under-\nrepresentation of some facts. A lot of improvement must be\nmade in this direction to be able to extract relations beyond\nthose that are highly represented. In that respect, data aug-\nmentation methods for generating negative or rare relation\nare a ﬁrst step to expand few-shot learning capacities of the\ncurrent models.\nPreliminary Results\nTo this day, I have built a tool to automate my literature re-\nview and conducted two preliminary experiments, in order\nto approach SRQ.1 and SRQ.2. This work and the results\nobtained from them will soon be formalized, extended, and\nsubmitted for publication.\nA Living and Systematic Review\nThe answer to SRQ.1 requires kick-starting a literature ex-\nploration from existing surveys related to a given task via\nquerying several digital library APIs. The scientiﬁc corpus\ncollected was extended with: (1) the dataset and models re-\nlated to our task, and the various metadata found on Paper-\nWithCode; (2) the retrieval of the citation network of each\npaper using the OpenCitation API. The ﬁrst run of the liter-\nature exploration allowed me to draw the current trends in\nRE. It suggests that the ﬁeld is shifting from discriminative\n(encoder-based models derived from BERT) to generative\nmodelling (based on encoder-decoder and decoder-only ar-\nchitectures). The latter models have the advantage of being\nﬂexible and enabling the conception of end-to-end systems.\nMoreover, they better handle overlapping triples extraction\n(Ye et al. 2022). Finally, to my knowledge, no system is cur-\nrently trying to perform semantic relation extraction where\ntriples explicitly follow an RDF syntax.\nA First Focus on Datatype Properties\nIn a ﬁrst experiment I tackled the ﬁrst part of SRQ.2,\nby focusing on datatype properties that are primarily\naffected by the hallucination problem. I built a ﬁrst\ndataset of relations based only on the English chap-\nter of DBpedia and Wikipedia, restricted to entities\nof type dbo:Person, and focusing on the follow-\ning relations: dbo:birthDate, dbo:deathDate and\nrdfs:label. I employed a SHACL shape to ﬁlter\nthe graphs respecting the following criteria: an instance\nof dbo:Person must have a dbo:birthDate, an\nrdfs:label and a dbo:deathDate. On the example\nof REBEL (Huguet Cabot and Navigli 2021), I ﬁne-tuned a\nBART model by giving as input the identiﬁer of the entity\nfollowed by its Wikipedia abstract. The model was trained\nto generate RDF Turtle triples including the set of relations\nselected and found in DBpedia. A qualitative analysis of the\nerrors allowed me to underline 3 sources of errors: (1) the\nfact is in the text but not in DBpedia, (2) the values in the\ntext and in the DBpedia are different, (3) the fact is in DB-\npedia but not in the text.\nI conducted a second experiment to understand if one syn-\ntax is easier to learn for a pre-trained model. In the literature,\nthe choice of syntax is related to the “linearization process”\nwhere triples are serialized as a string (a list or a tagged\nsequence). Until now, different methods have been investi-\ngated but they were not rigorously compared. For this rea-\nson, I extended the dataset of my ﬁrst experiment to repre-\nsent the triples according to seven different syntaxes: a sim-\nple list, a tagged sequence, a light Turtle syntax without the\npreﬁxes deﬁnition, full Turtle, N-Triples, XML and JSON-\nLD. My approach also considered the ﬁne-tuning of two\nsizes of the BART model (base, large) and the training time\nneeded before the F1-micro saturation (>0.9). The results of\nthe experiments showed us that (1) the model quickly mas-\ntered the simpliﬁed Turtle syntax, followed by the list and\nthe tags. (2) Some RDF syntaxes took longer to learn for\nBART : the easiest was Turtle, followed equally by JSON-\nLD and RDF-XML. (3) We have also pointed out that N-\nTriples syntax were harder to learn. However, this exper-\niment must be extended by incorporating cross-validation\nand other measures better suited to assess the generated\ntriples.\nFuture Works\nUntil the date of the Workshop, my research plan will be\nto continue to answer SRQ.2 and will extend to SRQ.3,\nby incrementally investigating new and more complex RDF\ngraph patterns and integrating Wikidata to go beyond our\ncurrent use of DBpedia. Aside from that, I will continue the\nanalysis allowed by the systematic review launched to an-\nswer SRQ.1. I will then have another two years to cover\nSRQ.4 and SRQ.5. Depending on my advancement some\nextensions of the current plan could consider including ap-\nproaches evaluating transfer learning and active learning.\nReferences\nHuguet Cabot, P.-L.; and Navigli, R. 2021. REBEL: Re-\nlation Extraction By End-to-end Language generation. In\nFindings of the ACL: EMNLP 2021, 2370–2381. ACL.\nMesquita, F.; Cannaviccio, M.; Schmidek, J.; Mirza, P.; and\nBarbosa, D. 2019. KnowledgeNet: A Benchmark Dataset for\nKnowledge Base Population. In Proc. of the 2019 EMNLP\nConference and the 9th International IJCNLP, 749–758.\nACL.\nYe, H.; Zhang, N.; Chen, H.; and Chen, H. 2022. Generative\nKnowledge Graph Construction: A Review. In Proc. of the\n2022 EMNLP Conference, 1–17. ACL.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23412",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8361608982086182
    },
    {
      "name": "RDF",
      "score": 0.6946743726730347
    },
    {
      "name": "Knowledge graph",
      "score": 0.6865724325180054
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.6162150502204895
    },
    {
      "name": "Scalability",
      "score": 0.5926330089569092
    },
    {
      "name": "Entity linking",
      "score": 0.5680477619171143
    },
    {
      "name": "Linked data",
      "score": 0.535257875919342
    },
    {
      "name": "Relationship extraction",
      "score": 0.5265990495681763
    },
    {
      "name": "Natural language processing",
      "score": 0.5021657943725586
    },
    {
      "name": "Knowledge base",
      "score": 0.50091552734375
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47376683354377747
    },
    {
      "name": "Natural language understanding",
      "score": 0.43012022972106934
    },
    {
      "name": "Natural language",
      "score": 0.4158710837364197
    },
    {
      "name": "Information retrieval",
      "score": 0.4058472216129303
    },
    {
      "name": "Semantic Web",
      "score": 0.3659977912902832
    },
    {
      "name": "Information extraction",
      "score": 0.29623597860336304
    },
    {
      "name": "Database",
      "score": 0.14846578240394592
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210117840",
      "name": "Institut de Biologie Valrose",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I201841394",
      "name": "Université Côte d'Azur",
      "country": "FR"
    }
  ],
  "cited_by": 3
}