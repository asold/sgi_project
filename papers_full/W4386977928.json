{
  "title": "Bad Actor, Good Advisor: Exploring the Role of Large Language Models in Fake News Detection",
  "url": "https://openalex.org/W4386977928",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4382333949",
      "name": "Hu, Beizhe",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2149193555",
      "name": "Sheng Qiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097673293",
      "name": "CAO Juan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2318338986",
      "name": "Shi Yuhui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1993907199",
      "name": "Li Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222439653",
      "name": "Wang, Danding",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096350588",
      "name": "Qi Peng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3035268925",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3034020579",
    "https://openalex.org/W4376654497",
    "https://openalex.org/W4378506863",
    "https://openalex.org/W4283452667",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W4386566461",
    "https://openalex.org/W3201124874",
    "https://openalex.org/W4224310546",
    "https://openalex.org/W4389520264",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3098829544",
    "https://openalex.org/W4224316507",
    "https://openalex.org/W3210022786",
    "https://openalex.org/W4221122068",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3206237685",
    "https://openalex.org/W4307907065",
    "https://openalex.org/W3031781733",
    "https://openalex.org/W3119467012",
    "https://openalex.org/W2742330194",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W2991596147",
    "https://openalex.org/W4321524280",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W4378465130",
    "https://openalex.org/W4383045293",
    "https://openalex.org/W2951307134",
    "https://openalex.org/W3152907744",
    "https://openalex.org/W2890801081",
    "https://openalex.org/W4386501849",
    "https://openalex.org/W3212591930",
    "https://openalex.org/W4221168025",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2809476703",
    "https://openalex.org/W4385565280",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4379259169"
  ],
  "abstract": "Detecting fake news requires both a delicate sense of diverse clues and a profound understanding of the real-world background, which remains challenging for detectors based on small language models (SLMs) due to their knowledge and capability limitations. Recent advances in large language models (LLMs) have shown remarkable performance in various tasks, but whether and how LLMs could help with fake news detection remains underexplored. In this paper, we investigate the potential of LLMs in fake news detection. First, we conduct an empirical study and find that a sophisticated LLM such as GPT 3.5 could generally expose fake news and provide desirable multi-perspective rationales but still underperforms the basic SLM, fine-tuned BERT. Our subsequent analysis attributes such a gap to the LLM's inability to select and integrate rationales properly to conclude. Based on these findings, we propose that current LLMs may not substitute fine-tuned SLMs in fake news detection but can be a good advisor for SLMs by providing multi-perspective instructive rationales. To instantiate this proposal, we design an adaptive rationale guidance network for fake news detection (ARG), in which SLMs selectively acquire insights on news analysis from the LLMs' rationales. We further derive a rationale-free version of ARG by distillation, namely ARG-D, which services cost-sensitive scenarios without querying LLMs. Experiments on two real-world datasets demonstrate that ARG and ARG-D outperform three types of baseline methods, including SLM-based, LLM-based, and combinations of small and large language models.",
  "full_text": "Bad Actor, Good Advisor:\nExploring the Role of Large Language Models in Fake News Detection\nBeizhe Hu1,2 Qiang Sheng1,∗ Juan Cao1,2 Yuhui Shi1,2\nYang Li1,2 Danding Wang1 Peng Qi3\n1Key Lab of Intelligent Information Processing of Chinese Academy of Sciences,\nInstitute of Computing Technology, Chinese Academy of Sciences\n2University of Chinese Academy of Sciences 3National University of Singapore\n{hubeizhe21s,shengqiang18z,caojuan,shiyuhui22s}@ict.ac.cn\n{liyang23s,wangdanding}@ict.ac.cn, pengqi.qp@gmail.com\nAbstract\nDetecting fake news requires both a delicate\nsense of diverse clues and a profound under-\nstanding of the real-world background, which\nremains challenging for detectors based on\nsmall language models (SLMs) due to their\nknowledge and capability limitations. Recent\nadvances in large language models (LLMs)\nhave shown remarkable performance in vari-\nous tasks, but whether and how LLMs could\nhelp with fake news detection remains under-\nexplored. In this paper, we investigate the po-\ntential of LLMs in fake news detection. First,\nwe conduct an empirical study and find that a\nsophisticated LLM such as GPT 3.5 could gen-\nerally expose fake news and provide desirable\nmulti-perspective rationales but still underper-\nforms the basic SLM, fine-tuned BERT. Our\nsubsequent analysis attributes such a gap to the\nLLM’s inability to select and integrate ratio-\nnales properly to conclude. Based on these find-\nings, we propose that current LLMs may not\nsubstitute fine-tuned SLMs in fake news detec-\ntion but can be a good advisor for SLMs by pro-\nviding multi-perspective instructive rationales.\nTo instantiate this proposal, we design an adap-\ntive rationale guidance network for fake news\ndetection (ARG), in which SLMs selectively ac-\nquire insights on news analysis from the LLMs’\nrationales. We further derive a rationale-free\nversion of ARG by distillation, namely ARG-\nD, which services cost-sensitive scenarios with-\nout querying LLMs. Experiments on two real-\nworld datasets demonstrate that ARG and ARG-\nD outperform three types of baseline methods,\nincluding SLM-based, LLM-based, and combi-\nnations of small and large language models.\n1 Introduction\nThe wide and fast spread of fake news online has\nposed real-world threats in critical domains like\npolitics (Fisher et al., 2016), economy (CHEQ,\n∗ ∗Corresponding author.\nSmallLanguage Model\n[Label: FAKE] Detailed photos of Xiang Liu's tendon surgery exposed. Stop complaints and please show sympathy and blessings!\nLargeLanguage Model\nREAL\nThe answer is real.\n[News]\nLargeLanguage Model\n- Commonsense: Real surgery generally won’t be exposed…- Textual Description: The language is emotional and tries to attract audience…\n[News][Perspective-specific Prompting]+\n[News][Prompting]+\nSmallLanguage Model\n[News] Prediction: FAKE\n(a)\n(b)\nFigure 1: Illustration of the role of large language mod-\nels (LLMs) in fake news detection. In this case, (a) the\nLLM fails to output correct judgment of news veracity\nbut (b) helps the small language model (SLM) judge\ncorrectly by providing informative rationales.\n2019), and public health (Naeem and Bhatti, 2020).\nAmong the countermeasures to combat this issue,\nautomatic fake news detection, which aims at dis-\ntinguishing inaccurate and intentionally misleading\nnews items from others automatically, has been a\npromising solution in practice (Shu et al., 2017;\nRoth, 2022).\nThough much progress has been made (Hu\net al., 2022a), understanding and characterizing\nfake news is still challenging for current models.\nThis is caused by the complexity of the news-faking\nprocess: Fake news creators might manipulate any\npart of the news, using diverse writing strategies\nand being driven by inscrutable underlying aims.\nTherefore, to maintain both effectiveness and uni-\nversality for fake news detection, an ideal method is\nrequired to have: 1) a delicate sense of diverse clues\n(e.g., style, facts, commonsense); and 2) a profound\nunderstanding of the real-world background.\nRecent methods (Zhang et al., 2021; Kaliyar\net al., 2021; Mosallanezhad et al., 2022; Hu et al.,\n2023) generally exploit pre-trainedsmall language\nmodels (SLMs)1 like BERT (Devlin et al., 2019)\n1The academia lacks a consensus regarding the size bound-\nary between small and large language models at present, but it\narXiv:2309.12247v2  [cs.CL]  22 Jan 2024\nand RoBERTa (Liu et al., 2019) to understand news\ncontent and provide fundamental representation,\nplus optional social contexts (Shu et al., 2019; Cui\net al., 2022), knowledge bases (Popat et al., 2018;\nHu et al., 2022b), or news environment (Sheng\net al., 2022) as supplements. SLMs do bring im-\nprovements, but their knowledge and capability\nlimitations also compromise further enhancement\nof fake news detectors. For example, BERT was\npre-trained on text corpus like Wikipedia (Devlin\net al., 2019) and thus struggled to handle news\nitems that require knowledge not included (Sheng\net al., 2021).\nAs a new alternative to SLMs, large language\nmodels (LLMs) (OpenAI, 2022; Anthropic, 2023;\nTouvron et al., 2023), which are usually trained\non the larger-scale corpus and aligned with human\npreferences, have shown impressive emergent abil-\nities on various tasks (Wei et al., 2022a) and are\nconsidered promising as general task solvers (Ma\net al., 2023). However, the potential of LLMs in\nfake news detection remains underexplored:\n• Can LLMs help detect fake news with their\ninternal knowledge and capability?\n• What solution should we adopt to obtain better\nperformance using LLMs?\nTo answer these two questions, we first conduct\na deep investigation of the effective role of LLMs in\nfake news detection and attempt to provide a prac-\ntical LLM-involved solution. Unlike contemporary\nworks (Pelrine et al., 2023; Caramancion, 2023)\nwhich simply prompt LLMs to provide predictions\nwith the task instruction, we conduct a detailed em-\npirical study to mine LLMs’ potential. Specifically,\nwe use four typical prompting approaches (zero-\nshot/few-shot vanilla/chain-of-thought prompting)\nto ask the LLM to make veracity judgments of\ngiven news items (Figure 1(a)) and find that even\nthe best-performing LLM-based method still under-\nperforms task-specific fine-tuned SLMs. We then\nperform an analysis of the LLM-generated explana-\ntory rationales and find that the LLM could provide\nreasonable and informative rationales from several\nperspectives. By subsequently inducing the LLM\nwith perspective-specific prompts and performing\nrule-based ensembles of judgments, we find that ra-\ntionales indeed benefit fake news detection, and at-\ntribute the unsatisfying performance to the LLM’s\ninability to select and integrate rationales properly\nis widely accepted that BERT (Devlin et al., 2019) and GPT-3\nfamily (Brown et al., 2020) are respectively small and large\nones (Zhao et al., 2023).\n#\nChinese English\nTrain Val Test Train Val Test\nReal 2,331 1,172 1,137 2,878 1,030 1,024\nFake 2,873 779 814 1,006 244 234\nTotal 5,204 1,951 1,951 3,884 1,274 1,258\nTable 1: Statistics of the fake news detection datasets.\nto conclude.\nBased on these findings, we propose that the\ncurrent LLM may not be a good substitute for the\nwell-fine-tuned SLM but could serve as a good\nadvisor by providing instructive rationales, as pre-\nsented in Figure 1(b). To instantiate our proposal,\nwe design the adaptive rationale guidance (ARG)\nnetwork for fake news detection, which bridges the\nsmall and large LMs by selectively injecting new\ninsight about news analysis from the large LM’s ra-\ntionales to the small LM. The ARG further derives\nthe rationale-free ARG-D via distillation for cost-\nsensitive scenarios with no need to query LLMs.\nExperiments on two real-world datasets show that\nARG and ARG-D outperform existing SLM/LLM-\nonly and combination methods. Our contributions\nare as follows:\n• Detailed investigation: We investigate the\neffective role of LLMs in fake news detection\nand find the LLM is bad at veracity judgment\nbut good at analyzing contents;\n• Novel and practical solution: We design a\nnovel ARG network and its distilled version\nARG-D that complements small and large\nLMs by selectively acquiring insights from\nLLM-generated rationales for SLMs, which\nhas shown superiority based on extensive ex-\nperiments;\n• Useful resource: We construct a rationale\ncollection from GPT-3.5 for fake news detec-\ntion in two languages (Chinese and English)\nand will make it publicly available to facilitate\nfurther research.2\n2 Is the LLM a Good Detector?\nIn this section, we evaluate the performance of the\nrepresentative LLM, i.e., GPT-3.5 in fake news de-\ntection to reveal its judgment capability. We exploit\nfour typical prompting approaches and perform a\ncomparison with the SLM (here, BERT) fine-tuned\n2https://github.com/ICTMCG/ARG\nLLM\nNewsTask Description\nPredictionNewsTask DescriptionEliciting Sentence(e.g., “Let’s think step by step”)\nNewsTask DescriptionNews-Label Pairs           1 N…\nNewsTask DescriptionNews-Label-Rationale Triplets1 N…\nPredictionRationale\n(a) Zero-Shot Prompting\n(b) Zero-Shot CoT Prompting\n(c) Few-Shot Prompting\n(d) Few-Shot CoT Prompting\nfor (a) (c)\nfor (b) (d)\nFigure 2: Illustration of prompting approaches for\nLLMs.\non this task. Formally, given a news item x, the\nmodel aims to predict whether x is fake or not.\n2.1 Experimental Settings\nDataset We employ the Chinese dataset\nWeibo21 (Nan et al., 2021) and the English dataset\nGossipCop (Shu et al., 2020) for evaluation.\nFollowing existing works (Zhu et al., 2022; Mu\net al., 2023), we preprocess the datasets with\ndeduplication and temporal data split to avoid\npossible performance overrating led by data\nleakage for the SLM. Table 1 presents the dataset\nstatistics.\nLarge Language Model We evaluate GPT-3.5-\nturbo, the LLM developed by OpenAI and support-\ning the popular chatbot ChatGPT (OpenAI, 2022),\ndue to its representativeness and convenient calling.\nThe large scale of parameters makes task-specific\nfine-tuning almost impossible for LLMs, so we\nuse the prompt learning paradigm, where an LLM\nlearns tasks given prompts containing instructions\nor few-shot demonstrations (Liu et al., 2023a). In\ndetail, we utilize the following four typical prompt-\ning approaches to elicit the potential of the LLM in\nfake news detection (Figure 2):\n• Zero-Shot Promptingconstructs prompt only\ncontaining the task description and the given\nnews. To make the response more proficient\nand decrease the refusal ratio, we optionally\nadopt the role-playing technique when de-\nscribing our task (Liu et al., 2023b; Ram-\nlochan, 2023).\n• Zero-Shot CoT Prompting (Kojima et al.,\n2022) is a simple and straightforward chain-\nof-thought (CoT) prompting approach to en-\nModel Usage Chinese English\nGPT-3.5-\nturbo\nZero-Shot 0.676 0.568\nZero-Shot CoT 0.677 0.666\nFew-Shot 0.725 0.697\nFew-Shot CoT 0.681 0.702\nBERT Fine-tuning 0.753 0.765\n(+3.8%) (+9.0%)\nTable 2: Performance in macro F1 of the large and small\nLMs. The best two results are bolded and underlined,\nrespectively. The relative increases over the second-best\nresults are shown in the brackets.\ncourage the LLM to reason. In addition to the\nelements in zero-shot prompting, it adds an\neliciting sentence such as “Let’s think step by\nstep. ”\n• Few-Shot Prompting (Brown et al., 2020)\nprovides task-specific prompts and several\nnews-label examples as demonstrations. After\npreliminary tests of {2,4,8}-shot settings, we\nchoose 4-shot prompting which includes two\nreal and two fake samples.\n• Few-Shot CoT Prompting(Wei et al., 2022b)\nnot only provides news-label examples but\nalso demonstrates reasoning steps with previ-\nously written rationales. Here, we obtain the\nprovided rationale demonstrations from the\ncorrect and reasonable outputs of zero-shot\nCoT prompting.\nSmall Language Model We adopt the pre-\ntrained small language models, BERT (Devlin et al.,\n2019) as the representative, given its wide use in\nthis task (Kaliyar et al., 2021; Zhu et al., 2022;\nSheng et al., 2022). Specifically, we limit the max-\nimum length of the text to 170 tokens and use\nchinese-bert-wwm-ext and bert-base-uncased from\nTransformers package (Wolf et al., 2020) for the\nChinese and English evaluation, respectively. We\nuse Adam (Kingma and Ba, 2014) as the optimizer\nand do a grid search for the optimal learning rate.\nWe report the testing result on the best-validation\ncheckpoint.\n2.2 Comparison between Small and Large\nLMs\nTable 2 presents the performance of GPT-3.5-turbo\nwith four prompting approaches and the fine-tuned\nBERT on the two datasets. We observe that: 1)\nThough the LLM is generally believed powerful,\nthe LLM underperforms the fine-tuned SLM us-\ning all four prompting approaches. The SLM has\nPerspective\nChinese English\nProp. macF1 Prop. macF1\nTextual Description 65% 0.706 71% 0.653\nNews: Everyone! Don’t buy cherries anymore: Cherries\nof this year are infested with maggots, and nearly 100%\nare affected.\nLLM Rationale: ...The tone of the news is extremely\nurgent, seemingly trying to spread panic and anxiety.\nPrediction: Fake Ground Truth: Fake\nCommonsense 71% 0.698 60% 0.680\nNews: Huang, the chief of Du’an Civil Affairs Bureau,\ngets subsistence allowances of 509 citizens, owns nine\nproperties, and has six wives...\nLLM Rationale: ...The news content is extremely outra-\ngeous...Such a situation is incredibly rare in reality and\neven could be thought impossible.\nPrediction: Fake Ground Truth: Fake\nFactuality 17% 0.629 24% 0.626\nNews: The 18th National Congress has approved that\nindividuals who are at least 18 years old are now eligible\nto marry...\nLLM Rationale: First, the claim that Chinese individuals\nat least 18 years old can register their marriage is real, as\nthis is stipulated by Chinese law...\nPrediction: Real Ground Truth: Fake\nOthers 4% 0.649 8% 0.704\nTable 3: Analysis of different perspectives of LLM’s\nrationales in the sample set, including the data ratio,\nLLM’s performance, and cases. Prop.: Proportion.\na relative increase of 3.8%∼11.3% in Chinese and\n9.0%∼34.6% in English over the LLM, indicating\nthat the LLM lacks task-specific knowledge while\nthe SLM learns during fine-tuning.\n2) Few-shot versions outperform zero-shot ones,\nsuggesting the importance of task samples. How-\never, introducing several samples only narrow the\ngap with the SLM but does not lead to surpassing.\n3) CoT prompting brings additional performance\ngain in general, especially under the zero-shot set-\nting on the English dataset (+17.3%). However,\nwe also observe some cases where CoT leads to a\ndecrease. This indicates that effective use of ratio-\nnales may require more careful design.\nOverall, given the LLM’s unsatisfying perfor-\nmance and higher inference costs than the SLM,\nthe current LLM has not been a “good enough” de-\ntector to substitute task-specific SLMs in fake news\ndetection.\n2.3 Analysis on the Rationales from the LLM\nThough the LLM is bad at news veracity judgment,\nwe also notice that the rationales generated through\nModel Usage Chinese English\nGPT-3.5-turbo\nZero-Shot CoT 0.677 0.666\nfrom Perspective TD 0.667 0.611\nfrom Perspective CS 0.678 0.698\nBERT Fine-tuning 0.753 0.765\nEnsemble Majority V oting 0.735 0.724\nOracle V oting 0.908 0.878\nTable 4: Performance of the LLM using zero-shot CoT\nwith perspective specified and other compared models.\nTD: Textual description; CS: Commonsense.\nzero-shot CoT prompting exhibit a unique multi-\nperspective analytical capability that is challenging\nand rare for SLMs. For further exploration, we sam-\nple 500 samples from each of the two datasets and\nmanually categorize them according to the perspec-\ntives from which the LLM performs the news analy-\nsis. Statistical results by perspectives and cases are\npresented in Table 3.3 We see that: 1) The LLM is\ncapable of generating human-like rationales on\nnews content from various perspectives, such as\ntextual description, commonsense, and factuality,\nwhich meets the requirement of the delicate sense\nof diverse clues and profound understanding of the\nreal-world background in fake news detection. 2)\nThe detection performance on the subset using cer-\ntain perspectives is higher than the zero-shot CoT\nresult on the full testing set. This indicates the\npotential of analysis by perspectives, though the\ncoverage is moderate. 3) The analysis from the\nperspective of factuality leads to the performance\nlower than average, indicating the unreliability of\nusing the LLM for factuality analysis based on its\ninternal memorization. We speculate this is caused\nby the hallucination issue (Ji et al., 2023; Zhang\net al., 2023).\nWe further investigate the LLM’s performance\nwhen asked to perform analysis from a specific\nperspective on the full testing set (i.e., 100% cover-\nage).4 From the first group in Table 4, we see that\nthe LLM’s judgment with single-perspective analy-\nsis elicited is still promising. Compared with the\ncomprehensive zero-shot CoT setting, the single-\nperspective-based LLM performs comparatively\non the Chinese dataset and is better on the English\ndataset (for the commonsense perspective case).\n3Note that a sample may be analyzed from multiple per-\nspectives and thus the sum of proportions might be larger than\n100%.\n4We exclude the factuality to avoid the impacts of hal-\nlucination. The eliciting sentence is “Let’s think from the\nperspective of [textual description/commonsense].”\nThe results showcase that the internal mechanism\nof the LLM to integrate the rationales from diverse\nperspectives is ineffective for fake news detection,\nlimiting the full use of rationales. In this case, com-\nbining the small and large LMs to complement each\nother is a promising solution: The former could\nbenefit from the analytical capability of the latter,\nwhile the latter could be enhanced by task-specific\nknowledge from the former.\nTo exhibit the advantages of this solution, we\napply majority voting and oracle voting (assum-\ning the most ideal situation where we trust the\ncorrectly judged model for each sample, if any)\namong the two single-perspective-based LLMs and\nthe BERT. Results show that we are likely to gain\na performance better than any LLM-/SLM-only\nmethods mentioned before if we could adaptively\ncombine their advantages, i.e., the flexible task-\nspecific learning of the SLM and the informative\nrationale generated by the LLM. That is, the LLM\ncould be possibly a good advisor for the SLM by\nproviding rationales, ultimately improving the\nperformance of fake news detection.\n3 ARG: Adaptive Rationale Guidance\nNetwork for Fake News Detection\nBased on the above findings and discussion, we\npropose the adaptive rationale guidance (ARG) net-\nwork for fake news detection. Figure 3 overviews\nthe ARG and its rationale-free version ARG-D, for\ncost-sensitive scenarios. The objective of ARG\nis to empower small fake news detectors with the\nability to adaptively select useful rationales as ref-\nerences for final judgments. Given a news item x\nand its corresponding LLM-generated rationales\nrt (textual description) and rc (commonsense), the\nARG encodes the inputs using the SLM at first (Fig-\nure 3(a)). Subsequently, it builds news-rationale\ncollaboration via predicting the LLM’s judgment\nthrough the rationale, enriching news-rationale fea-\nture interaction, and evaluating rationale useful-\nness (Figure 3(b)). The interactive features are\nfinally aggregated with the news feature x for the\nfinal judgment of x being fake or not (Figure 3(c)).\nARG-D is derived from the ARG via distillation\nfor scenarios where the LLM is unavailable (Fig-\nure 3(d)).\n3.1 Representation\nWe employ two BERT models separately as the\nnews and rationale encoder to obtain semantic rep-\nresentations. For the given news item x and two\ncorresponding rationales rt and rc, the representa-\ntions are X, Rt, and Rc, respectively.\n3.2 News-Rationale Collaboration\nThe step of news-rationale collaboration aims at\nproviding a rich interaction between news and ra-\ntionales and learning to adaptively select useful\nrationales as references, which is at the core of our\ndesign. To achieve such an aim, ARG includes\nthree modules, as detailed and exemplified using\nthe textual description rationale branch below:\n3.2.1 News-Rationale Interaction\nTo enable comprehensive information exchange\nbetween news and rationales, we introduce a news-\nrationale interactor with a dual cross-attention\nmechanism to encourage feature interactions. The\ncross-attention can be described as:\nCA(Q, K, V) = softmax\n\u0010\nQ′ · K′/\n√\nd\n\u0011\nV′,\n(1)\nwhere Q′ = WQQ, K′ = WKK, and V′ =\nWVV. d is the dimensionality. Given represen-\ntations of the news X and the rationale Rt, the\nprocess is:\nft→x = AvgPool (CA(Rt, X, X)) , (2)\nfx→t = AvgPool (CA(X, Rt, Rt)) , (3)\nwhere AvgPool(·) is the average pooling over the\ntoken representations outputted by cross-attention\nto obtain one-vector text representation f.\n3.2.2 LLM Judgement Prediction\nUnderstanding the judgment hinted by the given\nrationale is a prerequisite for fully exploiting the in-\nformation behind the rationale. To this end, we con-\nstruct the LLM judgment prediction task, whose\nrequirement is to predict the LLM judgment of the\nnews veracity according to the given rationale. We\nexpect this to deepen the understanding of the ra-\ntionale texts. For the textual description rationale\nbranch, we feed its representationRt into the LLM\njudgment predictor, which is parametrized using a\nmulti-layer perception (MLP)5:\nˆmt = sigmoid(MLP(Rt)), (4)\nLpt = CE( ˆmt, mt), (5)\n5For brevity, we omit the subscripts of all independently\nparametrized MLPs.\nRc\nRt\nNewsEncoder\nRationaleEncoder\nX\nft→xfx→t\n Rationale UsefulnessEvaluatorLetwt\nLpt\n Classifier\nfcls\nNews     Encoder\n Rationale-AwareFeature Simulator X\n Classifierfcls\nx\nAttention\nAttention\n(a) Representation(b) News-Rationale Collaboration\n(d) Distillation for Rationale-Free Model\nNews Item\nTextualDescriptionRationale\nCommonsenseRationale\nNews-Rationale Interactor\nLLM Judgment Predictor\nft→x’\nfc→xfx→c\n Rationale UsefulnessEvaluatorLecwc\nLpc\nNews-Rationale Interactor\nLLM Judgment Predictor\nfc→x’\nFeatureAggregator\nLLMRationales\n Vector/MatrixModuleLoss\nLce\n(c) Prediction\nNews Item Lced\n↑ ARG Network↓ ARG-D Network\nLkddistill from   fcls\nContent\nRationales\ninitialized frommodule in (c)initialized frommodule in (a)\nFigure 3: Overall architecture of our proposed adaptive rationale guidance (ARG) network and its rationale-\nfree version ARG-D. In the ARG, the news item and LLM rationales are (a) respectively encoded into X and\nR∗(∗ ∈ {t, c}). Then the small and large LMs collaborate with each other via news-rationale feature interaction,\nLLM judgment prediction, and rationale usefulness evaluation. The obtained interactive features f′\n∗→x (∗ ∈ {t, c}).\nThese features are finally aggregated with attentively pooled news feature x for the final judgment. In the ARG-D,\nthe news encoder and the attention module are preserved and the output of the rationale-aware feature simulator is\nsupervised by the aggregated feature fcls for knowledge distillation.\nwhere mt and ˆmt are respectively the LLM’s ac-\ntual judgment (extracted from the response) and\nits prediction. The loss Lpt is a cross-entropy loss\nCE(ˆy, y) = −y log ˆy − (1 − y) log(1− ˆy). The\ncase is similar for commonsense rationale Rc.\n3.2.3 Rationale Usefulness Evaluation\nThe usefulness of rationales from different perspec-\ntives varies across different news items and im-\nproper integration may lead to performance degra-\ndation. To enable the model to adaptively select\nappropriate rationale, we devise a rationale useful-\nness evaluation process, in which we assess the\ncontributions of different rationales and adjust their\nweights for subsequent veracity prediction. The\nprocess comprises two phases, i.e., evaluation and\nreweighting. For evaluation, we input the news-\naware rationale vector fx→t into the rationale use-\nfulness evaluator (parameterized by an MLP) to\npredict its usefulness ut. Following the assump-\ntion that rationales leading to correct judgments are\nmore useful, we use the judgment correctness as\nthe rationale usefulness labels.\nˆut = sigmoid(MLP(fx→t)), (6)\nLet = CE(ˆut, ut). (7)\nIn the reweighting phase, we input vector fx→t\ninto an MLP to obtain a weight number wt, which\nis then used to reweight the rationale-aware news\nvector ft→x. The procedure is as follows:\nft→x′ = wt · ft→x. (8)\nWe also use attentive pooling to transform the rep-\nresentation matrix X into a vector x.\n3.3 Prediction\nBased on the outputs from the last step, we now\naggregate news vector x and rationale-aware news\nvector f′\nt→x, f′\nc→x for the final judgment. For news\nitem x with label y ∈ {0, 1}, we aggregate these\nvectors with different weights:\nfcls = wcls\nx · x + wcls\nt · f′\nt→x + wcls\nc · f′\nc→x, (9)\nwhere wcls\nx , wcls\nt and wcls\nc are learnable parame-\nters ranging from 0 to 1. fcls is the fusion vector,\nwhich is then fed into the MLP classifier for final\nprediction of news veracity:\nLce = CE(MLP(fcls), y). (10)\nThe total loss function is the weighted sum of the\nloss terms mentioned above:\nL = Lce + β1(Let + Lec) +β2(Lpt + Lpc), (11)\nwhere β1 and β2 are hyperparameters.\nModel\nChinese English\nmacF1 Acc. F1 real F1fake macF1 Acc. F1 real F1fake\nG1: LLM-Only GPT-3.5-turbo 0.725 0.734 0.774 0.676 0.702 0.813 0.884 0.519\nG2: SLM-Only\nBaseline 0.753 0.754 0.769 0.737 0.765 0.862 0.916 0.615\nEANNT 0.754 0.756 0.773 0.736 0.763 0.864 0.918 0.608\nPublisher-Emo 0.761 0.763 0.784 0.738 0.766 0.868 0.920 0.611\nENDEF 0.765 0.766 0.779 0.751 0.768 0.865 0.918 0.618\nG3: LLM+SLM\nBaseline + Rationale 0.767 0.769 0.787 0.748 0.777 0.870 0.921 0.633\nSuperICL 0.757 0.759 0.779 0.734 0.736 0.864 0.920 0.551\nARG 0.784 0.786 0.804 0.764 0.790 0.878 0.926 0.653\n(Relative Impr. over Baseline) (+4.2%)(+4.3%)(+4.6%)(+3.8%)(+3.2%)(+1.8%)(+1.1%)(+6.3%)\nw/o LLM Judgment Predictor 0.773 0.774 0.789 0.756 0.786 0.880 0.928 0.645\nw/o Rationale Usefulness Evaluator 0.781 0.783 0.801 0.761 0.782 0.873 0.923 0.641\nw/o Predictor & Evaluator 0.769 0.770 0.782 0.756 0.780 0.874 0.923 0.637\nARG-D 0.771 0.772 0.785 0.756 0.778 0.870 0.921 0.634\n(Relative Impr. over Baseline) (+2.4%)(+2.3%)(+2.1%)(+2.6%)(+1.6%)(+0.9%)(+0.6%)(+3.2%)\nTable 5: Performance of the ARG and its variants and the LLM-only, SLM-only, LLM+SLM methods. The best\ntwo results in macro F1 and accuracy are respectively bolded and underlined. For GPT-3.5-turbo, the best results in\nTable 2 are reported.\n3.4 Distillation for Rationale-Free Model\nThe ARG requires sending requests to the LLM\nfor every prediction, which might not be affordable\nfor cost-sensitive scenarios. Therefore, we attempt\nto build a rationale-free model, namely ARG-D,\nbased on the trained ARG model via knowledge\ndistillation (Hinton et al., 2015). The basic idea\nis simulated and internalized the knowledge from\nrationales into a parametric module. As shown in\nFigure 3(d), we initialize the news encoder and\nclassifier with the corresponding modules in the\nARG and train a rationale-aware feature simulator\n(implemented with a multi-head transformer block)\nand an attention module to internalize knowledge.\nBesides the cross-entropy loss Lce, we let the fea-\nture fd\ncls to imitate fcls in the ARG, using the mean\nsquared estimation loss:\nLkd = MSE(fcls, fd\ncls). (12)\n4 Evaluation\n4.1 Experimental Settings\nBaselines We compare three groups of methods:\nG1 (LLM-Only): We list the performance of the\nbest-performing setting on each dataset in Table 2,\ni.e., few-shot in Chinese and few-shot CoT in En-\nglish.\nG2 (SLM-Only)6: 1) Baseline: The vanilla BERT-\nbase model whose setting remains consistent with\n6As this paper focuses on text-based news, we use the text-\nonly variant of the original EANN following (Sheng et al.,\n2021) and the publisher-emotion-only variant in (Zhang et al.,\n2021).\nthat in Section 2. 2) EANNT (Wang et al., 2018):\nA model that learns effective signals using auxil-\niary adversarial training, aiming at removing event-\nrelated features as much as possible. We used pub-\nlication year as the label for the auxiliary task. 3)\nPublisher-Emo (Zhang et al., 2021): A model\nthat fuses a series of emotional features with textual\nfeatures for fake news detection. 4) ENDEF (Zhu\net al., 2022): A model that removes entity bias\nvia causal learning for better generalization on\ndistribution-shifted fake news data. All methods in\nthis group used the same BERT as the text encoder.\nG3 (LLM+SLM): 1) Baseline+Rationale: It con-\ncatenates features from the news encoder and ra-\ntionale encoder and feeds them into an MLP for\nprediction. 2) SuperICL (Xu et al., 2023): It\nexploits the SLM as a plug-in for the in-context\nlearning of the LLM by injecting the prediction\nand the confidence for each testing sample into the\nprompt.\nImplementation Details We use the same\ndatasets introduced in Section 2 and keep the set-\nting the same in terms of the pre-trained model,\nlearning rate, and optimization method. For the\nARG-D network, the parameters of the news en-\ncoder and classifier are derived from the ARG\nmodel. A four-head transformer block is imple-\nmented in the rationale-aware feature simulator.\nThe weight of loss functions Let, Lpt, Lec, Lpc in\nthe ARG and Lkd in the ARG-D are grid searched.\n4.2 Performance Comparison and Ablation\nStudy\nTable 5 presents the performance of our proposed\nARG and its variants and the compared methods.\nFrom the results, we observe that: 1) The ARG out-\nperforms all other compared methods in macro F1,\ndemonstrating its effectiveness. 2) The rationale-\nfree ARG-D still outperforms all compared meth-\nods except ARG and its variants, which shows the\npositive impact of the distilled knowledge from\nARG. 3) The two compared LLM+SLM methods\nexhibit different performance. The simple combi-\nnation of features of news and rationale yields a\nperformance improvement, showing the usefulness\nof our prompted rationales. SuperICL outperforms\nthe LLM-only method but fails to consistently out-\nperform the baseline SLM on the two datasets. We\nspeculate that this is due to the complexity of our\ntask, where injecting prediction and confidence of\nan SLM does not bring sufficient information. 4)\nWe evaluate three ablation experiment groups to\nevaluate the effectiveness of different modules in\nARG network. From the result, we can see that w/o\nLLM Judgement Predictor or w/o Rationale Use-\nfulness Evaluator both bring a significant decrease\nin ARG performance, highlighting the significance\nof these two structures. Besides, we found that\neven the weakest one among the variants of ARG\nstill outperforms all other methods, which shows\nthe importance of the news-rationale interaction\nstructure we designed.\n4.3 Result Analysis\nTo investigate which part the additional gain of the\nARG(-D) should be attributed to, we perform sta-\ntistical analysis on the additional correctly judged\nsamples of ARG(-D) compared with the vanilla\nBERT. From Figure 4, we observe that: 1) The\nproportions of the overlapping samples between\nARG(-D) and the LLM are over 77%, indicating\nthat the ARG(-D) can exploit (and absorb) the valu-\nable knowledge for judgments from the LLM, even\nits performance is unsatisfying. 2) The samples cor-\nrectly judged by the LLM from both two perspec-\ntives contribute the most, suggesting more diverse\nrationales may enhance the ARG(-D)’s training. 3)\n20.4% and 22.1% of correct judgments should be\nattributed to the model itself. We speculate that it\nproduces some kinds of “new knowledge” based\non the wrong judgments of the given knowledge.\nFor analysis of success and failure cases and\n77.9%\n20.4% 22.1%\n79.6%\n43.3% 45.3%\n15.5%\n20.9% 16.8%\n15.8%\n(a) right(ARG) – right(Baseline)\nLLM ARG\n✓ ✓\n✓✗\nTD CS\n✓ ✓\n✓✗\n✗✓\n(b) right(ARG-D) – right(Baseline)\nFigure 4: Statistics of additional correctly judged sam-\nples of (a) ARG and (b) ARG-D over the BERT base-\nline. right(·) denotes samples correctly judged by the\nmethod (·). TD/CS: Textual description/commonsense\nperspective.\nP (0.23, 0.784)\nFigure 5: Performance as the shifting threshold changes.\nadditional analysis, please refer to the appendix.\n4.4 Cost Analysis in Practice\nWe showcase a possible model-shifting strategy\nto balance the performance and cost in practical\nsystems. Inspired by Ma et al. (2023), we simu-\nlate the situation where we use the more economic\nARG-D by default but query the more powerful\nARG for part of the data. As presented in Fig-\nure 5, by sending only 23% of the data (according\nto the confidence of ARG-D) to the ARG, we could\nachieve 0.784 in macro F1, which is the same as\nthe performance fully using the ARG.\n5 Related Work\nFake News Detection Fake news detection is\ngenerally formulated as a binary classification task\nbetween real and fake news items. Research on this\ntask could be roughly categorized into two groups:\nsocial-context-based and content-based methods.\nMethods in the first group aim at differentiating\nfake and real news during the diffusion procedure\nby observing the propagation patterns (Zhou and\nZafarani, 2019), user feedback (Min et al., 2022),\nand social networks (Nguyen et al., 2020). The\nsecond group focuses on finding hints based on\nthe given content, including text (Przybyla, 2020)\nand images (Qi et al., 2021) and may require ex-\ntra assistance from knowledge bases (Popat et al.,\n2018) and news environments (Sheng et al., 2022).\nBoth two groups of methods obtain textual repre-\nsentation from pre-trained models like BERT as\na convention but rarely consider its potential for\nfake news detection. We conducted an exploration\nin this paper by combining large and small LMs\nand obtained good improvement only using textual\ncontent.\nLLMs for Natural Language Understanding\nLLMs, though mostly generative models, also have\npowerful natural language understanding (NLU)\ncapabilities, especially in the few-shot in-context\nlearning scenarios (Brown et al., 2020). Recent\nworks in this line focus on benchmarking the latest\nLLM in NLU. Results show that LLMs may not\nhave comprehensive superiority compared with a\nwell-trained small model in some types of NLU\ntasks (Zhong et al., 2023; Koco´n et al., 2023). Our\nresults provide empirical findings in fake news de-\ntection with only textual content as the input.\n6 Conclusion and Discussion\nWe investigated if large LMs help in fake news\ndetection and how to properly utilize their advan-\ntages for improving performance. Results show\nthat the large LM (GPT-3.5) underperforms the\ntask-specific small LM (BERT), but could provide\ninformative rationales and complement small LMs\nin news understanding. Based on these findings,\nwe designed the ARG network to flexibly combine\nthe respective advantages of small and large LMs\nand developed its rationale-free version ARG-D for\ncost-sensitive scenarios. Experiments showed the\nsuperiority of the ARG and ARG-D.\nDiscussion Our findings in fake news detection\nexemplify the current barrier for LLMs to be com-\npetent in applications closely related to the sophis-\nticated real-world background. Though having su-\nperior analyzing capability, LLMs may struggle\nto properly make full use of their internal capabil-\nity. This suggests that “mining” their potential may\nrequire novel prompting techniques and a deeper\nunderstanding of its internal mechanism. We then\nidentified the possibility of combining small and\nLLMs to earn additional improvement and pro-\nvided a solution especially suitable for situations\nwhere the better-performing models have to “select\ngood to learn” from worse ones. We expect our\nsolution to be extended to other tasks and foster\nmore effective and cost-friendly use of LLMs in\nthe future.\nLimitations We identify the following limita-\ntions: 1) We do not examine other well-known\nLLMs (e.g., Claude7 and Ernie Bot 8) due to the\nAPI unavailability for us when conducting this re-\nsearch; 2) We only consider the perspectives sum-\nmarized from the LLM’s response and there might\nbe other prompting perspectives based on a concep-\ntualization framework of fake news; 3) Our best\nresults still fall behind the oracle voting integration\nof multi-perspective judgments in Table 4, indi-\ncating that rooms still exist in our line regarding\nperformance improvements.\nAcknowledgements\nThe authors would like to thank the anonymous re-\nviewers for their insightful comments. This work is\nsupported by the National Natural Science Founda-\ntion of China (62203425), the Zhejiang Provincial\nKey Research and Development Program of China\n(2021C01164), the Project of Chinese Academy of\nSciences (E141020), the Postdoctoral Fellowship\nProgram of CPSF (GZC20232738) and the CIPSC-\nSMP-Zhipu.AI Large Model Cross-Disciplinary\nFund.\nReferences\nAnthropic. 2023. Model card and evaluations for claude\nmodels. https://www-files.anthropic.com/\nproduction/images/Model-Card-Claude-2.pdf .\nAccessed: 2023-08-13.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems,\npages 1877–1901. Curran Associates Inc.\nKevin Matthe Caramancion. 2023. News verifiers\nshowdown: A comparative performance evalua-\ntion of ChatGPT 3.5, ChatGPT 4.0, bing AI,\n7https://claude.ai/\n8https://yiyan.baidu.com/\nand bard in news fact-checking. arXiv preprint\narXiv:2306.17176.\nCHEQ. 2019. The economic cost of bad actors on the in-\nternet. https://info.cheq.ai/hubfs/Research/\nTHE_ECONOMIC_COST_Fake_News_final.pdf. Ac-\ncessed: 2023-08-13.\nJian Cui, Kwanwoo Kim, Seung Ho Na, and Seung-\nwon Shin. 2022. Meta-path-based fake news detec-\ntion leveraging multi-level social context information.\nIn Proceedings of the 31st ACM International Con-\nference on Information & Knowledge Management,\npages 325–334. ACM.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186. ACL.\nMarc Fisher, John Woodrow Cox, and Peter Hermann.\n2016. Pizzagate: From rumor, to hashtag, to gunfire\nin dc. The Washington Post.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nBeizhe Hu, Qiang Sheng, Juan Cao, Yongchun Zhu,\nDanding Wang, Zhengjia Wang, and Zhiwei Jin.\n2023. Learn over past, evolve for future: Forecasting\ntemporal trends for fake news detection. In Proceed-\nings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 5: Industry\nTrack), pages 116–125. ACL.\nLinmei Hu, Siqi Wei, Ziwang Zhao, and Bin Wu. 2022a.\nDeep learning for fake news detection: A comprehen-\nsive survey. AI Open, 3:133–155.\nXuming Hu, Zhijiang Guo, GuanYu Wu, Aiwei Liu,\nLijie Wen, and Philip Yu. 2022b. CHEF: A pilot\nChinese dataset for evidence-based fact-checking. In\nProceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 3362–3376. ACL.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput-\ning Surveys, 55:1–38.\nRohit Kumar Kaliyar, Anurag Goswami, and Pratik\nNarang. 2021. FakeBERT: Fake news detection\nin social media with a BERT-based deep learn-\ning approach. Multimedia tools and applications ,\n80(8):11765–11788.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. In International\nConference on Learning Representations.\nJan Koco´n, Igor Cichecki, Oliwier Kaszyca, Mateusz\nKochanek, Dominika Szydło, Joanna Baran, Julita\nBielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil\nKanclerz, Anna Koco ´n, Bartłomiej Koptyra, Wik-\ntoria Mieleszczenko-Kowszewicz, Piotr Miłkowski,\nMarcin Oleksy, Maciej Piasecki, Łukasz Radli´nski,\nKonrad Wojtasik, Stanisław Wo´ zniak, and Prze-\nmysław Kazienko. 2023. ChatGPT: Jack of all trades,\nmaster of none. Information Fusion, 99:101861.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. InAdvances in\nNeural Information Processing Systems, volume 35,\npages 22199–22213. Curran Associates, Inc.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023a. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys, 55(9):1–35.\nYi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen\nZheng, Ying Zhang, Lida Zhao, Tianwei Zhang, and\nYang Liu. 2023b. Jailbreaking ChatGPT via prompt\nengineering: An empirical study. arXiv preprint\narXiv:2305.13860.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nYubo Ma, Yixin Cao, YongChing Hong, and Aixin Sun.\n2023. Large language model is not a good few-shot\ninformation extractor, but a good reranker for hard\nsamples! arXiv preprint arXiv:2303.08559.\nErxue Min, Yu Rong, Yatao Bian, Tingyang Xu, Peilin\nZhao, Junzhou Huang, and Sophia Ananiadou. 2022.\nDivide-and-conquer: Post-user interaction network\nfor fake news detection on social media. In Pro-\nceedings of the ACM Web Conference 2022, pages\n1148–1158. ACM.\nAhmadreza Mosallanezhad, Mansooreh Karami, Kai\nShu, Michelle V . Mancenido, and Huan Liu. 2022.\nDomain adaptive fake news detection via reinforce-\nment learning. In Proceedings of the ACM Web Con-\nference 2022, pages 3632–3640. ACM.\nYida Mu, Kalina Bontcheva, and Nikolaos Aletras. 2023.\nIt’s about time: Rethinking evaluation on rumor de-\ntection benchmarks using chronological splits. In\nFindings of the Association for Computational Lin-\nguistics: EACL 2023, pages 736–743. ACL.\nSalman Bin Naeem and Rubina Bhatti. 2020. The\nCOVID-19 ‘infodemic’: a new front for informa-\ntion professionals. Health Information & Libraries\nJournal, 37(3):233–239.\nQiong Nan, Juan Cao, Yongchun Zhu, Yanyan Wang,\nand Jintao Li. 2021. MDFEND: Multi-domain fake\nnews detection. In Proceedings of the 30th ACM\nInternational Conference on Information and Knowl-\nedge Management. ACM.\nVan-Hoang Nguyen, Kazunari Sugiyama, Preslav\nNakov, and Min-Yen Kan. 2020. FANG: Leveraging\nsocial context for fake news detection using graph\nrepresentation. In Proceedings of the 29th ACM Inter-\nnational Conference on Information and Knowledge\nManagement, pages 1165–1174. ACM.\nOpenAI. 2022. ChatGPT: Optimizing language mod-\nels for dialogue. https://openai.com/blog/\nchatgpt/. Accessed: 2023-08-13.\nKellin Pelrine, Meilina Reksoprodjo, Caleb Gupta,\nJoel Christoph, and Reihaneh Rabbany. 2023. To-\nwards reliable misinformation mitigation: Gener-\nalization, uncertainty, and GPT-4. arXiv preprint\narXiv:2305.14928v1.\nKashyap Popat, Subhabrata Mukherjee, Andrew Yates,\nand Gerhard Weikum. 2018. DeClarE: Debunking\nfake news and false claims using evidence-aware\ndeep learning. In Proceedings of the 2018 Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 22–32. ACL.\nPiotr Przybyla. 2020. Capturing the style of fake news.\nIn Proceedings of the AAAI Conference on Artifi-\ncial Intelligence, volume 34, pages 490–497. AAAI\nPress.\nPeng Qi, Juan Cao, Xirong Li, Huan Liu, Qiang Sheng,\nXiaoyue Mi, Qin He, Yongbiao Lv, Chenyang Guo,\nand Yingchao Yu. 2021. Improving fake news detec-\ntion by using an entity-enhanced framework to fuse\ndiverse multimodal clues. In Proceedings of the 29th\nACM International Conference on Multimedia, pages\n1212–1220. ACM.\nSunil Ramlochan. 2023. Role-playing in\nlarge language models like ChatGPT.\nhttps://www.promptengineering.org/role-playing-\nin-large-language-models-like-chatgpt/. Accessed:\n2023-08-13.\nYoel Roth. 2022. The vast majority of content we\ntake action on for misinformation is identified proac-\ntively. https://twitter.com/yoyoel/status/\n1483094057471524867. Accessed: 2023-08-13.\nQiang Sheng, Juan Cao, Xueyao Zhang, Rundong Li,\nDanding Wang, and Yongchun Zhu. 2022. Zoom out\nand observe: News environment perception for fake\nnews detection. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 4543–4556.\nACL.\nQiang Sheng, Xueyao Zhang, Juan Cao, and Lei Zhong.\n2021. Integrating pattern-and fact-based fake news\ndetection via model preference learning. In Proceed-\nings of the 30th ACM international conference on\ninformation & knowledge management, pages 1640–\n1650. ACM.\nKai Shu, Limeng Cui, Suhang Wang, Dongwon Lee,\nand Huan Liu. 2019. dEFEND: Explainable fake\nnews detection. In Proceedings of the 25th ACM\nSIGKDD International Conference on Knowledge\nDiscovery & Data Mining, pages 395–405. ACM.\nKai Shu, Deepak Mahudeswaran, Suhang Wang, Dong-\nwon Lee, and Huan Liu. 2020. FakeNewsNet: A\ndata repository with news content, social context and\nspatiotemporal information for studying fake news\non social media. Big data, 8:171–188.\nKai Shu, Amy Sliva, Suhang Wang, Jiliang Tang, and\nHuan Liu. 2017. Fake news detection on social me-\ndia: A data mining perspective. ACM SIGKDD Ex-\nplorations Newsletter, 19:22–36.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. LLaMA: Open\nand efficient foundation language models. arXiv\npreprint arXiv:2302.13971.\nYaqing Wang, Fenglong Ma, Zhiwei Jin, Ye Yuan,\nGuangxu Xun, Kishlay Jha, Lu Su, and Jing Gao.\n2018. EANN: Event adversarial neural networks for\nmulti-modal fake news detection. In Proceedings of\nthe 24th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining , pages 849–\n857. ACM.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. 2022a. Emer-\ngent abilities of large language models. Transactions\non Machine Learning Research.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc V Le,\nand Denny Zhou. 2022b. Chain-of-thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems,\nvolume 35, pages 24824–24837. Curran Associates,\nInc.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. ACL.\nCanwen Xu, Yichong Xu, Shuohang Wang, Yang Liu,\nChenguang Zhu, and Julian McAuley. 2023. Small\nmodels are valuable plug-ins for large language mod-\nels. arXiv preprint arXiv:2305.08848.\nXueyao Zhang, Juan Cao, Xirong Li, Qiang Sheng, Lei\nZhong, and Kai Shu. 2021. Mining dual emotion\nfor fake news detection. In Proceedings of the web\nconference 2021, pages 3465–3476. ACM.\nYue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu,\nTingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang,\nYulong Chen, Longyue Wang, Anh Tuan Luu, Wei\nBi, Freda Shi, and Shuming Shi. 2023. Siren’s song\nin the AI ocean: A survey on hallucination in large\nlanguage models. arXiv preprint arXiv:2309.01219.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen\nYang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,\nPeiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023.\nA survey of large language models. arXiv preprint\narXiv:2303.18223.\nQihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and\nDacheng Tao. 2023. Can ChatGPT understand too? a\ncomparative study on ChatGPT and fine-tuned BERT.\narXiv preprint arXiv:2302.10198.\nXinyi Zhou and Reza Zafarani. 2019. Network-based\nfake news detection: A pattern-driven approach.\nACM SIGKDD Explorations Newsletter, 21(2):48–\n60.\nYongchun Zhu, Qiang Sheng, Juan Cao, Shuokai Li,\nDanding Wang, and Fuzhen Zhuang. 2022. Gener-\nalizing to the future: Mitigating entity bias in fake\nnews detection. In Proceedings of the 45th Inter-\nnational ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, pages 2120–\n2125. ACM.\nA Case Analysis\nTables 6 and 7 present cases in the testing set.\nThe former includes cases where at least one of\nthe compared methods and large language model\n(LLM) predictions is correct, while the latter in-\ncludes those in the complementary set (i.e., neither\nof them provides correct predictions).\nIn Table 6, Case 1 shows a case in which the\nBaseline made a correct prediction while the LLM\nwas wrong. The ARG stayed unaffected by the\nprobably misleading LLM rationales and main-\ntained the correct prediction. Cases 2 and 3 show\nthe situation where the baseline was incorrect while\nthe LLM could be seen partially right (i.e., only that\nused the specific perspective of the two is correct).\nIn these cases, our ARG selected the correct ra-\ntionale based on its adaptive selection capability,\nresulting in the correct prediction.\nIn Case 4 in Table 7, ARG successfully iden-\ntified the case where both the baseline and LLM\nfailed to provide accurate prediction, which sur-\npasses our expectations. After conducting a com-\nparative analysis, we found that within the training\nset, the use of phrases like \"compact writing style\"\nin textual description rationales often leads to in-\ncorrect judgments. We hypothesize that guided\nby the signals provided by the rationale usefulness\nevaluator, ARG recognized that pattern and was\nable to make judgments contrary to erroneous ra-\ntionales and chose the correct prediction. While in\nCase 5, ARG was unable to invert the erroneous\njudgment as in Case 4. We speculate that this is\ndue to the absence of a pattern as clear as that in\nCase 4. Constrained by the natural language un-\nderstanding performance of the BERT-base model,\nARG struggled to capture more complex logical\nrelationships.\nB Prompting Examples\nIn this section, we aim to showcase the prompting\ntemplates along with real examples for facilitating\nreaders to know the settings directly.\nIn Tables 8 and 9, we illustrate prompt-\ning examples for zero-shot prompting, zero-shot\nCoT prompting, zero-shot CoT prompting with\nperspective-specific prompts, few-shot prompting,\nand few-shot CoT prompting from Chinese dataset.\nFor brevity, we only present a news pair compris-\ning one real news and one fake news as a demo for\nfew-shot prompting and few-shot CoT prompting.\nIn practice, we utilize multiple sets of news pairs\naccording to how many shots are required.\nDue to a notable likelihood of eliciting refusals\nfrom GPT-3.5-turbo when using prompts related to\nfake news detection directly on the English dataset,\nwe employ the role-playing technique. Specifically,\nwe built appropriate contexts in the prompt to let\nthe LLM be in a scene and ultimately decreased the\nrefusal likelihood. (Liu et al., 2023b; Ramlochan,\n2023)\nCase 1: Both the Baseline and the ARG predicted correctly, while the LLM did incorrectly.\nNews: Wow! A robot in Russia was captured heroically saving a playful little girl from being crushed by a shelf This act\nwent viral online, showing the robot’s strength, amazing!\nLLM Rationale (Textual Description): This message uses exaggerated, emotive language, while providing\nno credible sources or evidence.\nLLM Rationale (Commonsense): Based on common knowledge and experience, robots have not yet reached a level where\nthey can act autonomously and save human lives.\nGround Truth: Real\nBaseline Pred.: Real LLM TD Pred.: Fake LLM CS Pred.: Fake ARG Pred.: Real\nCase 2: The Baseline predicted incorrectly, the ARG did correctly, and the LLM using textual description perspective\nalso did correctly.\nNews: #NingboRevelation# [Ningbo Xiangshan Fishermen Catch a Strange Fish, Dubbed the ’Panda of the Wa-\nter’]#NingboFreshNews# Around 7:00 PM on March 2nd, the ZXY02206 vessel, at the coordinates of 121 °20’898\"\nE, 29°09’898\" N, caught a strange fish. Upon verification, the fish was identified as a juvenile wild Chinese sturgeon,\nmeasuring 64 centimeters in length and weighing approximately 2 kilograms. Around 10:00 AM on March 3rd, this juvenile\nwild Chinese sturgeon was returned to the embrace of the sea.\nLLM Rationale (Textual Description): This message uses a more formal language and provides specific details about the\ntime, location, and species of the fish caught, including its size. It also mentions that the information has been verified.\nLLM Rationale (Commonsense): Based on common knowledge and experience, the Chinese sturgeon is a critically\nendangered and rare species, making it unlikely to be commonly caught by ordinary fishermen.\nGround Truth : Real\nBaseline Pred. : Fake LLM TD Pred. : Real LLM CS Pred. : Fake ARG Pred. : Real\nCase 3: The Baseline predicted incorrectly, the ARG did correctly, and the LLM using commonsense perspective\nalso did correctly.\nNews: After recovery, the British Prime Minister commutes to work by bicycle every day, though he owns a second-hand\nV olkswagen. He is not allowed to use an official car for commuting, and since there’s no fixed parking spot at the Prime\nMinister’s Office, he opts for cycling. Two bodyguards follow him in a car but often fall behind due to traffic congestion.\nLLM Rationale (Textual Description): The style of this message is straightforward and clear, without excessive embel-\nlishment or exaggeration. The content described also appears to be realistic and credible.\nLLM Rationale (Commonsense): Considering the nature of the British Prime Minister’s job and security considerations,\nthe claim that he commutes to work by bicycle every day is not very credible.\nGround Truth : Fake\nBaseline Pred. : Real LLM TD Pred. : Real LLM CS Pred. : Fake ARG Pred. : Fake\nTable 6: Cases in the testing set where at least one in the Baseline and the LLM using perspective-specific prompting\npredicted correctly. TD/CS represents textual description/commonsense perspective, respectively. Patterns in\nrationales are underlined and correct predictions are in bold italics.\nCase 4: Both the Baseline and the LLM using perspective-specific prompting made incorrect predictions, while the\nARG did it correctly.\nNews: According to Russian media reports, recently, Japan entrusted US military transport planes stationed in Japan to\ntransport 2,000 test kits donated by our country to Japan. However, during the transportation process, the US suddenly\ninformed Japan that these test kits had been transported to the US mainland. A US Air Force official later explained that\ndue to a mistake, these materials were wrongly transported to the US mainland. The Japanese side does not accept the\nexplanation of the US military official! Do you think the actions of the US military were really due to a mistake?\nLLM Rationale (Textual Description): The style of the message is quite compact, with language that is concise and clear.\nThere are no apparent grammatical or spelling errors, making it consistent with the style of news reporting.\nLLM Rationale (Commonsense): Based on common experience and knowledge, it’s not uncommon for mistakes to occur\nduring the transportation of goods. Therefore, it’s more likely that the actions of the U.S. military were due to an error\nrather than intentional.\nGround Truth: Fake\nBaseline Pred.: Real LLM TD Pred.: Real LLM CS Pred.: Real ARG Pred.: Fake\nCase 5: The Baseline, the LLM using perspective-specific prompting, and the ARG all made incorrect predictions.\nNews: #ShenzhenEpidemic# Shenzhen’s Sungang area has started entry and exit controls. Everyone should pay attention to\nwearing masks properly! Protect yourselves! The epidemic is not over yet, everyone should be extra careful.\nLLM Rationale (Textual Description): The message utilizes a hashtag \"#ShenzhenEpidemic#\", which aligns with the\ncommon method of posting on social media. The tone is also quite formal, serving as a reminder for everyone to take\nprotective measures and expressing concern and attention towards the epidemic situation.\nLLM Rationale (Commonsense): Given the current global situation regarding the pandemic, various regions are im-\nplementing measures to control the spread of the virus. Therefore, Shenzhen may also adopt similar control measures.\nAdditionally, the message mentions wearing masks and protecting oneself, which is in line with protective measures during\nthe pandemic.\nGround Truth: Fake\nBaseline Pred.: Real LLM TD Pred.: Real LLM CS Pred.: Real ARG Pred.: Real\nTable 7: Cases in the testing set where both the Baseline and the LLM using perspective-specific prompting made\nincorrect predictions. TD/CS represents textual description/commonsense perspective, respectively. Patterns in\nrationales are underlined and correct predictions are in bold italics.\n❶ Zero-Shot Prompting\nQ: Given the following message, predict its veracity. If it is more likely to be a real message, return 1; otherwise,\nreturn 0. Please refrain from providing ambiguous assessments such as undetermined: Detailed photos of Xiang Liu’s\ntendon surgery exposed. Stop complaints and please show sympathy and blessings!\nA: 1\n❷ Zero-Shot CoT Prompting\nQ: Given the following message, predict its veracity. If it is more likely to be a real message, return 1; otherwise,\nreturn 0. Please refrain from providing ambiguous assessments such as undetermined: Detailed photos of Xiang Liu’s\ntendon surgery exposed. Stop complaints and please show sympathy and blessings!\nA: Let’s think step by step. Firstly, this message mentions that Xiang Liu underwent tendon surgery, which can be verified\nthrough other channels. Secondly, the message refers to detailed photo exposure, which can also be verified by searching\nrelated reports. Lastly, the message urges people to show sympathy and offer blessings, which aligns with common practices.\nTherefore, it can be considered that this message is more likely to be true information, and the response is 1. Therefore, the\nanswer (arabic numerals) is 1\n❸ Zero-Shot CoT with Perspective Specific Prompting (Textual Description)\nQ: Given the following message, predict its veracity. If it is more likely to be a real message, return 1; otherwise,\nreturn 0. Please refrain from providing ambiguous assessments such as undetermined: Detailed photos of Xiang Liu’s\ntendon surgery exposed. Stop complaints and please show sympathy and blessings!\nA: Let’s think from the perspective of textual description. This message employs emotional language to convey\nsympathy and blessings for Xiang Liu, while also mentioning the detailed photo exposure of the surgery. Considering that\nsuch a message is likely to attract public attention and resonance, it is more likely to be true information. Return 1.\n❹ Zero-Shot CoT with Perspective Specific Prompting (Commonsense)\nQ: Given the following message, predict its veracity. If it is more likely to be a real message, return 1; otherwise,\nreturn 0. Please refrain from providing ambiguous assessments such as undetermined: Detailed photos of Xiang Liu’s\ntendon surgery exposed. Stop complaints and please show sympathy and blessings!\nA: Let’s think from the perspective of commonsense. Based on common knowledge and experience, usually, real surgery\nwill not be exposed, as this involves patient privacy and medical institutions’ confidentiality obligations. Therefore, this\nmessage is more likely to be false. Return 0.\nTable 8: Zero-shot prompting, zero-shot CoT prompting, and zero-shot CoT with Perspective Specific prompting\ntemplates, along with the news case and the replies from the LLM (i.e., GPT-3.5-turbo). Template prompts are in\nbold and the target news is in italics.\n❺ Few-Shot Prompting\nQ: Given the following message, predict its veracity. If it is more likely to be a real message, return 1; otherwise, return 0.\nPlease refrain from providing ambiguous assessments such as undetermined: [Russian Official Reiterates Denial: Land\nTransfer to China is False News] On the 10th local time, the Russian Ministry of Foreign Affairs stated that there is no\nterritorial dispute between Russia and China. The claim that Russia transferred 4.7 square kilometers of land to China is\nfalse. After the Russia-China border was established, there have been no changes in the border demarcation. Previously,\nboth the government of Hunchun, China, and the Russian border defense department had denied the land transfer news.\nhttp://t.cn/ruole6x\nA: 1\nQ: Given the following message, predict its veracity. If it is more likely to be a real message, return 1; otherwise, return\n0. Please refrain from providing ambiguous assessments such as undetermined: Breaking News: Tianyi Li’s successful\nexoneration! Just received news that the owner of the \"rivuai160\" bar in Sanlitun has been taken into custody by the police.\nThe authorities have initiated a prostitution investigation on the bar. Tianyi Li and five others are expected to shift from\nbeing defendants to witnesses, potentially being released as soon as this weekend!\nA: 0\nQ: Given the following message, predict its veracity. If it is more likely to be a real message, return 1; otherwise,\nreturn 0. Please refrain from providing ambiguous assessments such as undetermined: Detailed photos of Xiang Liu’s\ntendon surgery exposed. Stop complaints and please show sympathy and blessings!\nA: 1\n❻ Few-Shot CoT Prompting\nQ: Given the following message, predict its veracity. If it is more likely to be a real message, return 1; otherwise, return 0.\nPlease refrain from providing ambiguous assessments such as undetermined: [Russian Official Reiterates Denial: Land\nTransfer to China is False News] On the 10th local time, the Russian Ministry of Foreign Affairs stated that there is no\nterritorial dispute between Russia and China. The claim that Russia transferred 4.7 square kilometers of land to China is\nfalse. After the Russia-China border was established, there have been no changes in the border demarcation. Previously,\nboth the government of Hunchun, China, and the Russian border defense department had denied the land transfer news.\nhttp://t.cn/ruole6x\nA: Let’s think step by step. Firstly, the source of the message is a Weibo link, and we need to consider the credibility of this\nlink. Secondly, the content of the message pertains to a Russian denial of transferring land to China, and we need to assess\nif this aligns with reality. Lastly, we should check if the message aligns with reports from other reliable media sources.\nBased on this analysis, the message is more likely to be true information. The Russian Ministry of Foreign Affairs is an\nofficial entity, lending some credibility to their denial. Additionally, the message is consistent with the denials from the\ngovernment of Hunchun, China, and the Russian border defense department. Therefore, return 1. Therefore, the answer\n(arabic numeral) is: 1.\nQ: Given the following message, predict its veracity. If it is more likely to be a real message, return 1; otherwise, return\n0. Please refrain from providing ambiguous assessments such as undetermined: Breaking News: Tianyi Li’s successful\nexoneration! Just received news that the owner of the \"rivuai160\" bar in Sanlitun has been taken into custody by the police.\nThe authorities have initiated a prostitution investigation on the bar. Tianyi Li and five others are expected to shift from\nbeing defendants to witnesses, potentially being released as soon as this weekend!\nA: Let’s think step by step. Firstly, this message claims that Tianyi Li’s exoneration was successful, but it doesn’t specify\nthe case in question and lacks any supporting evidence. Therefore, this part is unreliable. Secondly, the message states that\nthe owner of the \"rivuai160\" bar in Sanlitun has been taken into custody by the police, and the authorities have initiated\na prostitution investigation on the bar. This part could be true, as police investigations into establishments suspected of\ninvolvement in prostitution are not uncommon. However, we cannot verify the source and authenticity of this message, so\nit’s not entirely trustworthy either. Lastly, the message suggests that Li Tianyi and five others are expected to shift from\nbeing defendants to witnesses and could be released as soon as this weekend. This part also lacks specific evidence, thus\nmaking it unreliable. In conclusion, the credibility of this message is low, and it’s more likely to be untrue. Therefore, it\nshould return 0. Hence, the answer (Arabic numeral) is: 0.\nQ: Given the following message, predict its veracity. If it is more likely to be a real message, return 1; otherwise,\nreturn 0. Please refrain from providing ambiguous assessments such as undetermined: Detailed photos of Xiang Liu’s\ntendon surgery exposed. Stop complaints and please show sympathy and blessings!\nA: Firstly, this message mentions that Xiang Liu underwent tendon surgery, which can be verified through other channels.\nSecondly, the message refers to detailed photo exposure, which can also be verified by searching related reports. Lastly, the\nmessage urges people to show sympathy and offer blessings, which aligns with common practices. Therefore, it can be\nconsidered that this message is more likely to be true information, and the response is 1.\nTable 9: Few-shot prompting and few-shot CoT prompting templates, along with the news case and the replies from\nthe LLM (i.e., GPT-3.5-turbo). Demos are above the dashed line. Below the dashed line, template prompts are in\nbold and the target news is in italics.",
  "topic": "Perspective (graphical)",
  "concepts": [
    {
      "name": "Perspective (graphical)",
      "score": 0.6541830897331238
    },
    {
      "name": "Fake news",
      "score": 0.6087364554405212
    },
    {
      "name": "Computer science",
      "score": 0.5165952444076538
    },
    {
      "name": "Psychology",
      "score": 0.34890204668045044
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2899462580680847
    },
    {
      "name": "Internet privacy",
      "score": 0.2614382803440094
    }
  ]
}