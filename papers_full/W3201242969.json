{
    "title": "Towards Incremental Transformers: An Empirical Analysis of Transformer Models for Incremental NLU",
    "url": "https://openalex.org/W3201242969",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A3099951961",
            "name": "Patrick Kahardipraja",
            "affiliations": [
                "University of Potsdam"
            ]
        },
        {
            "id": "https://openalex.org/A3042716993",
            "name": "Brielen Madureira",
            "affiliations": [
                "University of Potsdam"
            ]
        },
        {
            "id": "https://openalex.org/A2042143543",
            "name": "David Schlangen",
            "affiliations": [
                "University of Potsdam"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1533861849",
        "https://openalex.org/W2010421551",
        "https://openalex.org/W3174892288",
        "https://openalex.org/W3034573343",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2103076621",
        "https://openalex.org/W2951456627",
        "https://openalex.org/W2002654918",
        "https://openalex.org/W2085750684",
        "https://openalex.org/W3034302643",
        "https://openalex.org/W2067624665",
        "https://openalex.org/W2160580906",
        "https://openalex.org/W3139537596",
        "https://openalex.org/W2061311021",
        "https://openalex.org/W3123615524",
        "https://openalex.org/W3100031205",
        "https://openalex.org/W2140438673",
        "https://openalex.org/W2951299559",
        "https://openalex.org/W2045804781",
        "https://openalex.org/W2276542473",
        "https://openalex.org/W2047237057",
        "https://openalex.org/W4309793872",
        "https://openalex.org/W3037798801",
        "https://openalex.org/W2251766657",
        "https://openalex.org/W4297683418",
        "https://openalex.org/W4206688402",
        "https://openalex.org/W2963285578",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2056367827",
        "https://openalex.org/W2098921539",
        "https://openalex.org/W2077302143",
        "https://openalex.org/W2963066655",
        "https://openalex.org/W2107206008",
        "https://openalex.org/W2023612782",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2183341477"
    ],
    "abstract": "Incremental processing allows interactive systems to respond based on partial inputs, which is a desirable property e.g. in dialogue agents. The currently popular Transformer architecture inherently processes sequences as a whole, abstracting away the notion of time. Recent work attempts to apply Transformers incrementally via restart-incrementality by repeatedly feeding, to an unchanged model, increasingly longer input prefixes to produce partial outputs. However, this approach is computationally costly and does not scale efficiently for long sequences. In parallel, we witness efforts to make Transformers more efficient, e.g. the Linear Transformer (LT) with a recurrence mechanism. In this work, we examine the feasibility of LT for incremental NLU in English. Our results show that the recurrent LT model has better incremental performance and faster inference speed compared to the standard Transformer and LT with restart-incrementality, at the cost of part of the non-incremental (full sequence) quality. We show that the performance drop can be mitigated by training the model to wait for right context before committing to an output and that training with input prefixes is beneficial for delivering correct partial outputs.",
    "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1178–1189\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n1178\nTowards Incremental Transformers: An Empirical Analysis of\nTransformer Models for Incremental NLU\nPatrick Kahardipraja Brielen Madureira David Schlangen\nComputational Linguistics, Department of Linguistics\nUniversity of Potsdam, Germany\n{kahardipraja,madureiralasota,david.schlangen}@uni-potsdam.de\nAbstract\nIncremental processing allows interactive sys-\ntems to respond based on partial inputs, which\nis a desirable property e.g. in dialogue agents.\nThe currently popular Transformer architec-\nture inherently processes sequences as a whole,\nabstracting away the notion of time. Recent\nwork attempts to apply Transformers incre-\nmentally via restart-incrementality by repeat-\nedly feeding, to an unchanged model, increas-\ningly longer input preﬁxes to produce partial\noutputs. However, this approach is computa-\ntionally costly and does not scale efﬁciently\nfor long sequences. In parallel, we witness\nefforts to make Transformers more efﬁcient,\ne.g. the Linear Transformer (LT) with a re-\ncurrence mechanism. In this work, we exam-\nine the feasibility of LT for incremental NLU\nin English. Our results show that the recur-\nrent LT model has better incremental perfor-\nmance and faster inference speed compared to\nthe standard Transformer and LT with restart-\nincrementality, at the cost of part of the non-\nincremental (full sequence) quality. We show\nthat the performance drop can be mitigated by\ntraining the model to wait for right context be-\nfore committing to an output and that training\nwith input preﬁxes is beneﬁcial for delivering\ncorrect partial outputs.\n1 Introduction\nOne fundamental property of human language pro-\ncessing is incrementality (Keller, 2010). Humans\nprocess language on a word-by-word basis by main-\ntaining a partial representation of the sentence\nmeaning at a fast pace and with great accuracy\n(Marslen-Wilson, 1973). The garden path effect,\nfor example, shows that language comprehension is\napproximated incrementally before committing to\na careful syntactic analysis (Frazier and Rayner,\n1982; Altmann and Steedman, 1988; Trueswell\net al., 1994, inter alia).\nThe notion of order along the time axis during\ncomputation is a key aspect of incremental pro-\ncessing and thus a desirable property both of cog-\nnitively plausible language encoders as well as in\napplications such as interactive systems (Skantze\nand Schlangen, 2009). RNNs, for example, are\ninherently able to process words sequentially while\nupdating a recurrent state representation. However,\nthe Transformer architecture (Vaswani et al., 2017),\nwhich has brought signiﬁcant improvements on\nseveral NLP tasks, processes the input sequence\nas a whole, thus prioritising parallelisation to the\ndetriment of the notion of linear order.\nOne way to employ non-incremental models in\nincremental settings is resorting to an incremental\ninterface, like in Beuck et al. (2011), where a com-\nplete recomputation of the available partial input\nhappens at each time step to deliver partial output.\nMadureira and Schlangen (2020) examined the out-\nput stability of non-incremental encoders in this\nrestart-incremental fashion. While qualitatively\nfeasible, this procedure is computationally costly,\nespecially for long sequences, since it requires as\nmany forward passes as the number of input tokens.\nIn parallel, there is ongoing research on ways\nto make Transformers more efﬁcient, e.g. the Lin-\near Transformer (LT) introduced by Katharopoulos\net al. (2020). Besides being more efﬁcient, LTs can\nbe employed with a recurrence mechanism based\non causal masking that turns them into models sim-\nilar to RNNs. In this work, we examine the suit-\nability of using LTs in incremental processing for\nsequence tagging and classiﬁcation in English. We\nalso inspect the use of the delay strategy (Baumann\net al., 2011; Oda et al., 2015; Ma et al., 2019) to ex-\namine the effect of right context availability on the\nmodel’s incremental performance. Our hypothesis\nis that recurrence will allow LTs to be better in in-\ncremental processing as it captures sequence order.\nAs LTs use an approximation of softmax attention,\nwe also expect a performance drop compared to\nthe standard Transformer while being faster in the\nincremental setting due to its linear time attention.\n1179\n2 Related Work\nIn recent years, neural approaches, including\nTransformer-based architectures (Vaswani et al.,\n2017), have become more popular for incremental\nprocessing. Given that Transformer models are not\ninherently incremental, employing them for incre-\nmental processing demands adaptation.\nIn simultaneous translation, for instance, Ma\net al. (2019) proposed to use an incremental en-\ncoder by limiting each source word to attend to\nits predecessors and recompute the representation\nfor previous source words when there is new input.\nZhang et al. (2021) introduced an average embed-\nding layer to avoid recalculation when using an\nincremental encoder, while exploiting right context\nthrough knowledge distillation. An investigation of\nthe use of non-incremental encoders for incremen-\ntal NLU in interactive systems was conducted by\nMadureira and Schlangen (2020). The authors em-\nployed BERT (Devlin et al., 2019) for sequence tag-\nging and classiﬁcation using restart-incrementality,\na procedure with high computational cost.\nThe computational cost of a restart-incremental\nTransformer can be reduced with more efﬁcient\nmodels or even avoided if an inherently incre-\nmental Transformer architecture existed. Recent\nworks have proposed modiﬁcations that could\nhelp achieve that. For instance, by approximat-\ning the softmax attention with a recurrent state\n(Katharopoulos et al., 2020; Choromanski et al.,\n2021; Peng et al., 2021). The Linear Transformer\nmodel (Katharopoulos et al., 2020, LT henceforth)\ncan be viewed as an RNN when the attention is\ncausal (see also, very recently, Kasai et al., 2021).\n3 Methods\n3.1 Overview of the Linear Transformer\nIn LTs, the similarity score between a query and a\nkey for the i-th position is computed using a kernel\nfunction. The causal attention can be written as:\nAtti(Q,K,V ) = φ(Qi)T Si\nφ(Qi)T Zi\n(1)\nSi =\ni∑\nj=1\nφ(Kj)VT\nj ; Zi =\ni∑\nj=1\nφ(Kj) (2)\nwith feature map φ(x) = elu(x)+1 where elu(·)\ndenotes the exponential linear unit (Clevert et al.,\n2016). Hence, Si and Zi can be viewed as a recur-\nrent state:\nSi = Si−1 + φ(Ki)VT\ni (3)\nZi = Zi−1 + φ(Ki) (4)\nwith S0 = Z0 = 0. As an RNN, the run-time\ncomplexity is linear with respect to the sequence\nlength and constant for each added token, which\npromises faster inference compared to the restart-\nincremental approach.\n3.2 Models\nWe examine the behaviour of Transformer models\nused as incremental processors on token level, in\nﬁve conﬁgurations (Table 1):\n1. Baseline: the standard Transformer encoder\nincrementalised via restart-incrementality,\ntrained with access to full sequences.\n2. LT: the LT encoder incrementalised via\nrestart-incrementality, trained with access to\nfull sequences.\n3. LT+R: the LT encoder trained as in (2) but\nduring test time we use its recurrent state vec-\ntor to predict the label at each time step, as in\nan RNN.\n4. LT+R+CM: the LT encoder trained with\ncausal masking to ensure each token repre-\nsentation can only attend to previous tokens.\nDuring inference, we convert the model to an\nRNN as in (3). Training with input preﬁxes\naims at encouraging the learning of intermedi-\nate structures (Köhn and Menzel, 2014) and\nthe anticipation of future output (Ma et al.,\n2019).\n5. LT+R+CM+D: similar to (4), but, during\ntraining, the output for the input token xt is\nobtained at time t+ d, where d ∈{1,2}is\nthe delay, following the approach in Turek\net al. (2020). There is evidence that additional\nright context improve the models’ incremen-\ntal performance (Baumann et al., 2011; Ma\net al., 2019; Madureira and Schlangen, 2020),\nwhich results in a trade-off between providing\ntimely output or waiting for more context to\ndeliver more stable output.\nWe also delay the output by 1 and 2 time steps\nfor the baseline and LT following Madureira and\nSchlangen (2020), to provide a fair comparison on\nincremental metrics. Note that outputs from both\n(1) and (2) are non-monotonic, as labels can be\n1180\nreassigned when a new input token is observed.\nThe other models deliver monotonic output for se-\nquence tagging as RNNs. A slight modiﬁcation is\nneeded for sequence classiﬁcation as each sequence\nis mapped to a single label. We average the hidden\nrepresentation at the last layer and project it lin-\nearly, followed by a softmax to obtain the sequence\nlabel ˆyt based on the consumed input until time\nt. For LT+ models, we use incremental averaging\nto avoid recomputation. By doing this, sequence\nclassiﬁcation is performed similarly for all models.\nModels Restart\nIncremental Recurrence Causal\nMasking Delay\nBaseline ✓ - - -*\nLT ✓ - - -*\nLT+R - ✓ - -\nLT+R+CM - ✓ ✓ -\nLT+R+CM+D - ✓ ✓ ✓\nTable 1: Overview of the Transformer models. * means\nwe perform further comparisons with a delayed variant.\n4 Experimental Setup\n4.1 Datasets\nWe evaluate our models on 9 datasets in English,\nwhich were also used in Madureira and Schlangen\n(2020). The tasks consist of sequence tagging: slot\nﬁlling (ATIS, Hemphill et al. (1990); Dahl et al.\n(1994) and SNIPS, Coucke et al. (2018)), chunk-\ning (CoNLL-2000, Tjong Kim Sang and Buch-\nholz (2000)), NER and PoS tagging (OntoNotes\n5.0, WSJ section, Weischedel et al. (2013)); and\nsequence classiﬁcation: intent detection (ATIS\nand SNIPS) and sentiment classiﬁcation (posi-\ntive/negative, Kotzias et al. (2015) and pros/cons,\nGanapathibhotla and Liu (2008)). More details are\navailable in the Appendix.\n4.2 Evaluation\nThe overall performance of the models is mea-\nsured with accuracy and F1 Score, according to\nthe task. For the incremental evaluation, we report\nthe diachronic metrics proposed by Baumann et al.\n(2011) and adapted in Madureira and Schlangen\n(2020): edit overhead (EO, the proportion of unnec-\nessary edits over all edits), correction time score\n(CT, the average proportion of time steps necessary\nto reach a ﬁnal decision), and relative correctness\n(RC, the proportion of time steps in which the out-\nput is a correct preﬁx of the ﬁnal, non-incremental\noutput).\nTo focus on the incremental quality of the mod-\nels and allow a clear separation between incremen-\ntal and non-incremental evaluation, we follow the\napproach by Baumann et al. (2011) and Madureira\nand Schlangen (2020), evaluating incremental out-\nputs with respect to the ﬁnal output produced by the\nmodels. While the ﬁnal output may differ from the\ngold standard, it serves as the target for the incre-\nmental output, as the non-incremental performance\nis an upper bound for incremental processing (Bau-\nmann et al., 2011).\n4.3 Implementation\nWe re-implement the Transformer and use the orig-\ninal implementation of the LT. 1 All models are\ntrained to minimise cross-entropy with the AdamW\noptimiser (Loshchilov and Hutter, 2019). We use\n300-D GloVe embeddings (Pennington et al., 2014)\nwhich are passed through a linear projection layer\nwith size dmodel. All experiments were performed\non a GPU GeForce GTX 1080 Ti. Details on the im-\nplementation, hyperparameters and reproducibility\nare available in the Appendix. Our implementation\nis publicly available.2\n5 Results and Discussion\nTasks Baseline LT LT+R LT+R\n+CM\nLT+R+\nCM+D1\nLT+R+\nCM+D2\nATIS-Slot 94.51 93.67 86.84 93.78 94.38 93.54\nSNIPS-Slot 90.13 87.98 63.16 81.88 85.72 86.91\nChunk 91.27 88.42 67.54 86.63 89.42 89.33\nNER 89.55 86.13 52.04 69.09 81.39 85.55\nPoS Tagging 96.88 96.49 89.30 95.11 96.49 96.55\nATIS-Intent 97.20 97.09 95.63 95.74 96.53 96.53\nSNIPS-Intent 97.14 97.14 83.71 96.43 97.14 96.86\nPos/Neg 86.00 85.17 68.00 80.33 81.16 82.67\nPros/Cons 94.42 94.21 90.97 94.37 94.56 94.46\nTable 2: Non-incremental performance of our models\non test sets (ﬁrst group, F1, second group, accuracy).\nHere, the baseline performs generally better than the\nLT variants.\nUltimately, the quality of the output when all\ninput has been seen matters; hence, we ﬁrst look at\nthe non-incremental or full-sequence performance,\nin Table 2. We can see that LTs do not outperform\nthe baseline here,3 although they have the advan-\ntage of being faster (Table 3). We see two possible\nreasons for this: First, as the LT variants strictly go\nleft-to-right through the sequence, they have less\ninformation for each token to make their decision.\nThis can be alleviated by allowing LT+R+CM to\n1https://linear-transformers.com\n2https://github.com/pkhdipraja/\ntowards-incremental-transformers\n3Notice that the baseline differs from Madureira and\nSchlangen (2020) who used a pretrained BERT model.\n1181\nwait for 1 or 2 tokens before producing partial out-\nput, and indeed we see an overall performance in-\ncrease of 0.1% - 16.5% for the +D variants. Second,\nwe suspect that the chosen feature map in LTs to\napproximate the softmax attention is sub-optimal,\nand a further gating mechanism could yield a better\nperformance (Peng et al., 2021).\nComparing LT+R+CM against LT+R, we ob-\nserve that training on preﬁxes yields a better result\nduring test time, as LT+R+CM may learn antici-\npation as a by-product, in line with the work of\nMa et al. (2019). LT+R+CM+D performs com-\npetitively with LT, outperforming the latter in 4\nout of 9 datasets. This is likely due to the bet-\nter capability of the delayed network in modelling\nboth non-linear and acausal functions that appear\nin some of the tasks (Turek et al., 2020).\nFigure 1 depicts the incremental metrics of all\nmodels. The EO and CT score for sequence tag-\nging is low for all models, which indicates that the\nmodels are capable, in general, of producing stable\nand accurate partial outputs. Notice that the LT+\nmodels are not able to revise the output in sequence\ntagging. For sequence classiﬁcation, the models\nhave higher EO and CT score due to the fact that the\nlabel is a function of the whole sequence and the\nmodel might be unable to reach an early decision\nwithout enough right context.\nLT+R+CM performs better in incremental met-\nrics compared to the baseline and LT in sequence\nclassiﬁcation. This is evidence that the notion of\norder is important for incremental processing, as\nthe recurrent state in the LT allows partial represen-\ntation updates along the time axis when process-\ning partial input. Here, sequence classiﬁcation is\ntreated in a similar fashion for all models, by using\nthe average of the hidden representation in the last\nlayer.\nAll the models have high RC score in general\nfor both sequence tagging and classiﬁcation. This\nmeans that most of the partial outputs are a cor-\nrect preﬁx of the ﬁnal (“non-incremental”) output\nand could fruitfully be used as input to subsequent\nprocessors in an incremental pipeline. For RC,\nLT+R+CM also outperforms both the baseline and\nLT in all tasks. A delay of 1 or 2 tokens before com-\nmitting to an output also helps to improve the incre-\nmental performance across all models. In terms of\nincremental inference speed, we see that the recur-\nrent mode is more than 10 times faster compared\nto using restart-incrementality (Table 3).\nTo understand the models’ behaviour better, es-\npecially pertaining to their potential for real-time\napplications, we also examine their incremental\ninference speed for different sequence lengths as\nshown in Figure 2. As expected, LT+R+CM scales\nlinearly and outperforms the baseline and LT con-\nsiderably as the sequence becomes longer. The\nrun-time performance of LT is slightly better than\nthe baseline because of its linear time attention,\nhowever it is still slower compared to LT+R+CM\nas it is restart-incremental.\n−0.04 −0.02 0.00 0.02 0.04\n−0.04\n−0.02\n0.00\n0.02\n0.04\nChunk\nNER\nPoS\nATIS-Slot\nSNIPS-Slot\nATIS-Int\nSNIPS-Int\nPos/Neg\nPros/Cons\nBaseline LT LT+R LT+R+CM\n0.1\n0.2\n0.3\n0.4Mean Edit Overhead\nBaseline LT LT+R LT+R+CM\n0.1\n0.2Mean Correction Time\nBaseline LT LT+R LT+R+CM\n0.5\n0.6\n0.7\n0.8\n0.9Mean Relative Correctness\nFigure 1: Incremental evaluation on the test sets. EO,\nCT and RC∈[0, 1], y-axes are clipped to improve read-\nability. Lower is better for EO and CT, higher for RC.\nFor EO, the lines on the bars refer to original, delay=1\nand delay=2, from top to bottom, and vice versa for RC,\nshowing that delay improves the results. LT+R+CM\nperforms better compared to the baseline and LT.\n1182\nTasks Baseline LT LT+R+CM\nATIS-Slot 0.983 1.025 13.780\nSNIPS-Slot 1.021 1.137 14.957\nChunk 0.393 0.448 6.023\nNER 0.382 0.436 5.745\nPoS Tagging 0.383 0.435 5.831\nATIS-Intent 0.883 1.005 13.310\nSNIPS-Intent 0.995 1.129 14.907\nPos/Neg 0.725 0.767 9.962\nPros/Cons 1.073 1.228 14.979\nAverage 0.76 (1 ×) 0.85 (1.12 ×) 11.06 (14.55×)\nTable 3: Comparison of incremental inference speed\non test sets, measured in sequences/sec. All the models\nhave similar size with 4 layers, feed-forward dimension\nof 2048 and self-attention dimension of 512.\n0 25 50 75 100 125 150 175 200\nSequence Length\n0\n5\n10\n15\n20Time (seconds)\nBaseline\nLT\nLT+R+CM\nFigure 2: Incremental inference speed of models from\nTable 3 with increasing sequence length. LT+R+CM\nscales linearly with sequence length unlike the baseline\nand LT. Note that the incremental inference speed of\nLT+R+CM is similar to LT+R.\n5.1 Ablations\nWe examine the importance of word and positional\nembeddings on the baseline and LT+R+CM for\nnon-incremental metrics (Table 4). We ﬁnd that us-\ning pre-trained GloVe (Pennington et al., 2014) em-\nbeddings is beneﬁcial for the models’ performance.\nOn average, it contributes 2.74 accuracy and 5.16\nF1 for the baseline, while improving LT+R+CM\nby 1.6 and 2.55 points for accuracy and F1. On the\nother hand, we observe that positional embeddings\nplay a less signiﬁcant role in LT+R+CM compared\nto the baseline. Without them the performance,\non average, for LT+R+CM improves in accuracy\nby 0.15 and the F1 score degrades by 1.35. The\nbaseline, however, experiences degradation in per-\nformance by 1.79 and 18.46 points for accuracy and\nF1, on average. The recurrence mechanism may\nbe a reason for the effect of positional embeddings\nbeing less pronounced in LT+R+CM.\nTasks Score – GloVe – Pos – Glove\n& Pos\nBaseline\nF1\nATIS-Slot 94.51 -3.02 -17.99 -20.44\nSNIPS-Slot 90.13 -6.74 -16.39 -21.47\nChunk 91.27 -5.28 -18.19 -19.36\nNER 89.55 -5.61 -21.27 -26.07\nAccuracy\nPoS Tagging 96.88 -0.79 -3.31 -4.15\nATIS-Intent 97.20 -2.35 -2.69 -3.81\nSNIPS-Intent 97.14 -0.57 +0.72 -0.43\nPos/Neg 86.00 -8.83 -3.83 -12.00\nPros/Cons 94.42 -1.18 +0.15 -1.64\nLT+R+CM\nF1\nATIS-Slot 93.78 -0.42 -0.25 -1.37\nSNIPS-Slot 81.88 -2.44 -0.55 -2.79\nChunk 86.63 -3.82 -3.35 -7.30\nNER 69.09 -3.52 -1.24 -6.69\nAccuracy\nPoS Tagging 95.11 -0.74 -0.79 -1.58\nATIS-Intent 95.74 -0.89 -0.11 -0.67\nSNIPS-Intent 96.43 +0.14 0.00 +0.28\nPos/Neg 80.33 -6.00 +1.17 -9.00\nPros/Cons 94.37 -0.53 +0.46 -0.83\nTable 4: Ablation of GloVe and positional embeddings\non the baseline and LT+R+CM for non-incremental\nmetrics.\n6 Conclusion\nWe studied the use of Transformer encoders for\nincremental processing and concluded that it is\npossible to deploy them as incremental processors\nwith certain trade-offs. With recurrent computa-\ntion, the Linear Transformer (LT) has inferior non-\nincremental performance compared to the regular\nTransformer and the LT with restart-incrementality.\nHowever, it has the great advantage of being much\nmore efﬁcient for incremental processing, since\nrecomputation at each time step is avoided. The\noutput of the recurrent LT is generally more stable\nfor sequence classiﬁcation and monotonic for tag-\nging. Its non-incremental performance drop can\nbe mitigated by introducing delay, which also im-\nproves the incremental metrics. It is also beneﬁcial\nto train such model with input preﬁxes, allowing it\nto learn more robust predictions.\nAcknowledgements\nWe thank the anonymous reviewers for their criti-\ncal reading of our manuscript and their insightful\ncomments and suggestions. This work is partially\nfunded by the Deutsche Forschungsgemeinschaft\n(DFG, German Research Foundation) – Project ID\n423217434 (Schlangen).\nReferences\nGerry Altmann and Mark Steedman. 1988. Interac-\ntion with context during human sentence processing.\nCognition, 30(3):191–238.\n1183\nTimo Baumann, Okko Buß, and David Schlangen.\n2011. Evaluation and optimisation of incremen-\ntal processors. Dialogue & Discourse , 2(1):113–\n141. Special Issue on Incremental Processing in Di-\nalogue.\nNiels Beuck, Arne Köhn, and Wolfgang Menzel. 2011.\nDecision strategies for incremental POS tagging. In\nProceedings of the 18th Nordic Conference of Com-\nputational Linguistics (NODALIDA 2011) , pages\n26–33, Riga, Latvia. Northern European Associa-\ntion for Language Technology (NEALT).\nKrzysztof Marcin Choromanski, Valerii Likhosherstov,\nDavid Dohan, Xingyou Song, Andreea Gane, Tamas\nSarlos, Peter Hawkins, Jared Quincy Davis, Afroz\nMohiuddin, Lukasz Kaiser, David Benjamin Be-\nlanger, Lucy J Colwell, and Adrian Weller. 2021.\nRethinking attention with performers. In 9th Inter-\nnational Conference on Learning Representations,\nICLR 2021, Vienna, Austria, May 3-7, 2021.\nDjork-Arné Clevert, Thomas Unterthiner, and Sepp\nHochreiter. 2016. Fast and accurate deep network\nlearning by exponential linear units (elus). In 4th\nInternational Conference on Learning Representa-\ntions, ICLR 2016, San Juan, Puerto Rico, May 2-4,\n2016, Conference Track Proceedings.\nAlice Coucke, Alaa Saade, Adrien Ball, Théodore\nBluche, Alexandre Caulier, David Leroy, Clément\nDoumouro, Thibault Gisselbrecht, Francesco Calt-\nagirone, Thibaut Lavril, Maël Primet, and Joseph\nDureau. 2018. Snips voice platform: an em-\nbedded spoken language understanding system for\nprivate-by-design voice interfaces. arXiv preprint,\narXiv:1805.10190.\nDeborah A. Dahl, Madeleine Bates, Michael Brown,\nWilliam Fisher, Kate Hunicke-Smith, David Pallett,\nChristine Pao, Alexander Rudnicky, and Elizabeth\nShriberg. 1994. Expanding the scope of the ATIS\ntask: The ATIS-3 corpus. In Human Language Tech-\nnology: Proceedings of a Workshop held at Plains-\nboro, New Jersey, March 8-11, 1994.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nHaihong E, Peiqing Niu, Zhongfu Chen, and Meina\nSong. 2019. A novel bi-directional interrelated\nmodel for joint intent detection and slot ﬁlling. In\nProceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n5467–5471, Florence, Italy. Association for Compu-\ntational Linguistics.\nLyn Frazier and Keith Rayner. 1982. Making and cor-\nrecting errors during sentence comprehension: Eye\nmovements in the analysis of structurally ambiguous\nsentences. Cognitive Psychology, 14(2):178–210.\nMurthy Ganapathibhotla and Bing Liu. 2008. Mining\nopinions in comparative sentences. In Proceedings\nof the 22nd International Conference on Compu-\ntational Linguistics (Coling 2008) , pages 241–248,\nManchester, UK. Coling 2008 Organizing Commit-\ntee.\nXavier Glorot and Yoshua Bengio. 2010. Understand-\ning the difﬁculty of training deep feedforward neu-\nral networks. In Proceedings of the Thirteenth\nInternational Conference on Artiﬁcial Intelligence\nand Statistics, volume 9 of Proceedings of Machine\nLearning Research , pages 249–256, Chia Laguna\nResort, Sardinia, Italy. PMLR.\nCharles T. Hemphill, John J. Godfrey, and George R.\nDoddington. 1990. The ATIS spoken language sys-\ntems pilot corpus. In Speech and Natural Language:\nProceedings of a Workshop Held at Hidden Valley,\nPennsylvania, June 24-27,1990.\nJungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama,\nGabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu\nChen, and Noah A. Smith. 2021. Finetuning pre-\ntrained transformers into RNNs. arXiv preprint ,\narXiv:2103.13076.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-\npas, and François Fleuret. 2020. Transformers are\nRNNs: Fast autoregressive transformers with linear\nattention. In Proceedings of the 37th International\nConference on Machine Learning , volume 119 of\nProceedings of Machine Learning Research , pages\n5156–5165. PMLR.\nFrank Keller. 2010. Cognitively plausible models of\nhuman language processing. In Proceedings of the\nACL 2010 Conference Short Papers , pages 60–67,\nUppsala, Sweden. Association for Computational\nLinguistics.\nArne Köhn and Wolfgang Menzel. 2014. Incremen-\ntal predictive parsing with TurboParser. In Proceed-\nings of the 52nd Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers), pages 803–808, Baltimore, Maryland. Associ-\nation for Computational Linguistics.\nDimitrios Kotzias, Misha Denil, Nando de Freitas, and\nPadhraic Smyth. 2015. From group to individual\nlabels using deep features. In Proceedings of the\n21th ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining , KDD ’15,\npage 597–606, New York, NY , USA. Association for\nComputing Machinery.\nIlya Loshchilov and Frank Hutter. 2019. Decou-\npled weight decay regularization. In 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\n1184\nMingbo Ma, Liang Huang, Hao Xiong, Renjie Zheng,\nKaibo Liu, Baigong Zheng, Chuanqiang Zhang,\nZhongjun He, Hairong Liu, Xing Li, Hua Wu, and\nHaifeng Wang. 2019. STACL: Simultaneous trans-\nlation with implicit anticipation and controllable la-\ntency using preﬁx-to-preﬁx framework. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics , pages 3025–3036,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nBrielen Madureira and David Schlangen. 2020. In-\ncremental processing in the age of non-incremental\nencoders: An empirical assessment of bidirectional\nmodels for incremental NLU. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pages 357–374,\nOnline. Association for Computational Linguistics.\nWilliam Marslen-Wilson. 1973. Linguistic structure\nand speech shadowing at very short latencies. Na-\nture, 244:522–3.\nYusuke Oda, Graham Neubig, Sakriani Sakti, Tomoki\nToda, and Satoshi Nakamura. 2015. Syntax-based\nsimultaneous translation through prediction of un-\nseen syntactic constituents. In Proceedings of the\n53rd Annual Meeting of the Association for Compu-\ntational Linguistics and the 7th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers) , pages 198–207, Beijing,\nChina. Association for Computational Linguistics.\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy\nSchwartz, Noah Smith, and Lingpeng Kong. 2021.\nRandom feature attention. In 9th International Con-\nference on Learning Representations, ICLR 2021, Vi-\nenna, Austria, May 3-7, 2021.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP) , pages 1532–1543, Doha,\nQatar. Association for Computational Linguistics.\nSameer Pradhan, Alessandro Moschitti, Nianwen Xue,\nHwee Tou Ng, Anders Björkelund, Olga Uryupina,\nYuchen Zhang, and Zhi Zhong. 2013. Towards ro-\nbust linguistic analysis using OntoNotes. In Pro-\nceedings of the Seventeenth Conference on Computa-\ntional Natural Language Learning , pages 143–152,\nSoﬁa, Bulgaria. Association for Computational Lin-\nguistics.\nGabriel Skantze and David Schlangen. 2009. Incre-\nmental dialogue processing in a micro-domain. In\nProceedings of the 12th Conference of the European\nChapter of the ACL (EACL 2009) , pages 745–753,\nAthens, Greece. Association for Computational Lin-\nguistics.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\nJon Shlens, and Zbigniew Wojna. 2016. Rethinking\nthe inception architecture for computer vision. In\n2016 IEEE Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 2818–2826.\nErik F. Tjong Kim Sang and Sabine Buchholz. 2000.\nIntroduction to the CoNLL-2000 shared task chunk-\ning. In Fourth Conference on Computational Nat-\nural Language Learning and the Second Learning\nLanguage in Logic Workshop.\nJohn C. Trueswell, Michael K. Tanenhaus, and Su-\nsan M. Garnsey. 1994. Semantic inﬂuences on pars-\ning: Use of thematic role information in syntactic\nambiguity resolution. Journal of Memory and Lan-\nguage, 33(3):285–318.\nJavier Turek, Shailee Jain, Vy V o, Mihai Capot ˘a,\nAlexander Huth, and Theodore Willke. 2020. Ap-\nproximating stacked and bidirectional recurrent ar-\nchitectures with the delayed recurrent neural net-\nwork. In Proceedings of the 37th International\nConference on Machine Learning , volume 119 of\nProceedings of Machine Learning Research , pages\n9648–9658. PMLR.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30, pages 5998–6008. Cur-\nran Associates, Inc.\nRalph Weischedel, Martha Palmer, Mitchell Marcus,\nEduard Hovy, Sameer Pradhan, Lance Ramshaw, Ni-\nanwen Xue, Ann Taylor, Jeff Kaufman, Michelle\nFranchini, Mohammed El-Bachouti, Robert Belvin,\nand Ann Houston. 2013. Ontonotes release 5.0\nldc2013t19.\nShaolei Zhang, Yang Feng, and Liangyou Li. 2021.\nFuture-guided incremental transformer for simulta-\nneous translation. Proceedings of the AAAI Confer-\nence on Artiﬁcial Intelligence.\nLukáš Žilka and Filip Jur ˇcíˇcek. 2015. Lectrack: In-\ncremental dialog state tracking with long short-term\nmemory networks. In Text, Speech, and Dialogue ,\npages 174–182, Cham. Springer International Pub-\nlishing.\n1185\nA Reproducibility\nWe describe in more detail the hyperparameters\nand implementation of our models.\nData\nWe mostly follow Madureira and Schlangen (2020).\nWe use only the WSJ section of OntoNotes with\nsplits following Pradhan et al. (2013). For Pos/Neg\nand Pros/Cons datasets, we split them randomly\nwith a proportion of 70% train, 10% validation,\nand 20% test set due to the unavailability of an\nofﬁcial splitting scheme. We removed sentences\nlonger than 200 words as they were infeasible to\ncompute. We use the preprocessed data and splits\nfor SNIPS and ATIS made available by E et al.\n(2019).\nTraining details\nOur models are trained for 50 epochs, using early\nstopping with patience of 10 and dropout of 0.1.\nFor AdamW (Loshchilov and Hutter, 2019), we\nuse β1 = 0 .9 and β2 = 0 .98. The learning rate\nis increased linearly for the ﬁrst 5 epochs. After\n30, 40, and 45 epochs, we decay the learning rate\nby 0.5. Xavier initialisation (Glorot and Bengio,\n2010) is applied to all parameters. The number of\nattention heads is set to 8, where the dimension of\neach head is self-attention dimension d/8.\nWe also apply label smoothing (Szegedy et al.,\n2016) with ϵ= 0.1 for sequence classiﬁcation to\nmake the model more robust for incremental pro-\ncessing. For OOV words, we randomly replace\ntokens by \"UNK\" token with p = 0 .02 during\ntraining and use it for testing (Žilka and Jur ˇcíˇcek,\n2015). We perform hyperparameter search using\nComet’s Bayesian search algorithm4, maximising\nF1 score for sequence tagging and accuracy for\nsequence classiﬁcation on the validation set. The\nhyperparameter search trials are limited to 20 for\nall of our experiments. The hyperparameters for\nLT were also used for LT+R. We use similar hyper-\nparameters for LT+R+CM and LT+R+CM+D. We\nset the seed to 42119392 for all of our experiments.\n4https://www.comet.ml/docs/python-sdk/\nintroduction-optimizer/\nHyperparameters\nLayers 1, 2, 3, 4\nGradient clipping no clip, 0.5, 1\nLearning rate 5e−5, 7e−5, 1e−4\nBatch size 32, 64, 128\nFeed-forward dimension 1024, 2048\nSelf-attention dimension 256, 512\nTable 5: Hyperparameter search space. We use the\nsame search space for all of our models.\nTasks Average sequence length\nATIS-Slot 10.26\nSNIPS-Slot 9.08\nChunk 23.55\nNER 24.14\nPoS Tagging 24.14\nATIS-Intent 10.26\nSNIPS-Intent 9.08\nPos/Neg 13.95\nPros/Cons 8.99\nTable 6: Average sequence length on test sets for each\ntask.\n1186\nTasks Layers Gradient clip Learning rate Batch size Feed-forward Self-attention\nBaseline\nATIS-Slot 2 no clip 1e−4 32 1024 256\nSNIPS-Slot 3 0.5 1e−4 64 1024 512\nChunk 4 1 7e−5 32 2048 512\nNER 4 no clip 7e−5 32 2048 512\nPoS Tagging 4 0.5 1e−4 32 1024 512\nATIS-Intent 4 no clip 1e−4 32 1024 256\nSNIPS-Intent 3 0.5 1e−4 64 1024 512\nPos/Neg 2 0.5 5e−5 64 2048 256\nPros/Cons 3 1 7e−5 64 2048 512\nLT\nATIS-Slot 3 1 7e−5 32 2048 512\nSNIPS-Slot 3 no clip 1e−4 32 1024 512\nChunk 3 0.5 1e−4 64 1024 512\nNER 2 1 1e−4 32 1024 512\nPoS Tagging 2 0.5 7e−5 32 2048 512\nATIS-Intent 2 1 7e−5 32 2048 512\nSNIPS-Intent 3 0.5 1e−4 32 1024 256\nPos/Neg 4 1 1e−4 64 2048 512\nPros/Cons 3 no clip 7e−5 32 2048 256\nLT+R+CM\nATIS-Slot 3 no clip 1e−4 64 2048 512\nSNIPS-Slot 3 no clip 7e−5 32 1024 512\nChunk 3 1 7e−5 32 1024 512\nNER 4 1 7e−5 64 1024 512\nPoS Tagging 2 no clip 1e−4 64 1024 512\nATIS-Intent 3 1 5e−5 32 2048 512\nSNIPS-Intent 3 no clip 1e−4 32 2048 512\nPos/Neg 4 1 1e−4 32 2048 256\nPros/Cons 2 0.5 5e−5 32 1024 512\nTable 7: Hyperparameters used for our experiments. The best conﬁguration for LT was also used for LT+R, while\nthe best conﬁguration for LT+R+CM was also used for LT+R+CM+D1 and LT+R+CM+D2.\nTasks Baseline LT LT+R LT+R\n+CM\nLT+R+\nCM+D1\nLT+R+\nCM+D2\nATIS-Slot 2.0M 9.9M 9.9M 9.9M 9.9M 9.9M\nSNIPS-Slot 10.0M 10.0M 10.0M 10.0M 10.0M 10.0M\nChunk 18.5M 12.2M 12.2M 12.2M 12.2M 12.2M\nNER 24.4M 16.0M 16.0M 20.2M 20.2M 20.2M\nPoS Tagging 20.2M 18.1M 18.1M 16.0M 16.0M 16.0M\nATIS-Intent 3.5M 6.7M 6.7M 9.9M 9.9M 9.9M\nSNIPS-Intent 10.0M 6.0M 6.0M 13.1M 13.1M 13.1M\nPos/Neg 4.3M 14.4M 14.4M 6.9M 6.9M 6.9M\nPros/Cons 14.1M 8.5M 8.5M 8.8M 8.8M 8.8M\nTable 8: Number of parameter for each model.\nTasks Datasets Labels Train Valid Test\nSlot ﬁlling ATIS (Hemphill et al., 1990; Dahl et al., 1994) 127 4,478 500 893\nSlot ﬁlling SNIPS (Coucke et al., 2018) 72 13,084 700 700\nChunking CoNLL 2000 (Tjong Kim Sang and Buchholz, 2000) 23 7,922 1,014 2,012\nNamed entity recognition OntoNotes 5.0, WSJ section (Weischedel et al., 2013) 37 30,060 5,315 1,640\nParts-of-Speech tagging OntoNotes 5.0, WSJ section (Weischedel et al., 2013) 48 30,060 5,315 1,640\nIntent detection ATIS (Hemphill et al., 1990; Dahl et al., 1994) 26 4,478 500 893\nIntent detection SNIPS (Coucke et al., 2018) 7 13,084 700 700\nSentiment classiﬁcation Positive/Negative (Kotzias et al., 2015) 2 2,100 300 600\nSentiment classiﬁcation Pros/Cons (Ganapathibhotla and Liu, 2008) 2 32,088 4,602 9,175\nTable 9: Tasks, datasets and their size.\n1187\nTasks Baseline LT LT+R LT+R\n+CM\nLT+R+\nCM+D1\nLT+R+\nCM+D2\nF1\nATIS-Slot 96.93 95.88 89.67 96.07 96.72 94.60\nSNIPS-Slot 91.36 90.02 58.91 83.41 86.44 87.81\nChunk 92.14 89.66 69.49 87.58 90.46 90.26\nNER 86.43 81.87 51.09 67.50 78.54 82.47\nAccuracy\nPoS Tagging 96.34 95.72 90.02 94.86 95.84 95.88\nATIS-Intent 98.40 98.40 93.20 98.60 98.00 98.00\nSNIPS-Intent 99.43 98.86 89.43 99.00 99.00 99.00\nPos/Neg 87.67 84.67 68.67 83.67 83.33 82.67\nPros/Cons 95.13 94.98 90.59 94.85 95.11 94.98\nTable 10: Non-incremental performance of our models on validation sets for reproducibility purpose.\nTasks/Models EO CT RC EO ∆1 EO∆2 RC∆1 RC∆2\nATIS-Slot\nBaseline 0.029 0.012 0.963 0.008 0.002 0.986 0.995\nLT 0.038 0.017 0.947 0.016 0.007 0.972 0.985\nLT+R 0.000 0.000 1.000 - - - -\nLT+R+CM 0.000 0.000 1.000 - - - -\nLT+R+CM+D1 - 0.000 - 0.000 - 1.000 -\nLT+R+CM+D2 - 0.000 - - 0.000 - 1.000\nSNIPS-Slot\nBaseline 0.147 0.078 0.805 0.054 0.026 0.906 0.945\nLT 0.189 0.103 0.738 0.075 0.033 0.868 0.929\nLT+R 0.000 0.000 1.000 - - - -\nLT+R+CM 0.000 0.000 1.000 - - - -\nLT+R+CM+D1 - 0.000 - 0.000 - 1.000 -\nLT+R+CM+D2 - 0.000 - - 0.000 - 1.000\nChunk\nBaseline 0.166 0.046 0.743 0.063 0.045 0.852 0.874\nLT 0.184 0.056 0.680 0.105 0.081 0.757 0.786\nLT+R 0.000 0.000 1.000 - - - -\nLT+R+CM 0.000 0.000 1.000 - - - -\nLT+R+CM+D1 - 0.000 - 0.000 - 1.000 -\nLT+R+CM+D2 - 0.000 - - 0.000 - 1.000\nNER\nBaseline 0.072 0.019 0.898 0.033 0.019 0.935 0.952\nLT 0.078 0.022 0.883 0.036 0.021 0.926 0.944\nLT+R 0.000 0.000 1.000 - - - -\nLT+R+CM 0.000 0.000 1.000 - - - -\nLT+R+CM+D1 - 0.000 - 0.000 - 1.000 -\nLT+R+CM+D2 - 0.000 - - 0.000 - 1.000\nPoS Tagging\nBaseline 0.114 0.032 0.812 0.045 0.033 0.886 0.904\nLT 0.125 0.036 0.777 0.054 0.036 0.854 0.879\nLT+R 0.000 0.000 1.000 - - - -\nLT+R+CM 0.000 0.000 1.000 - - - -\nLT+R+CM+D1 - 0.000 - 0.000 - 1.000 -\nLT+R+CM+D2 - 0.000 - - 0.000 - 1.000\nTable 11: Mean value of Edit Overhead, Correction Time Score and Relative Correctness on test sets for sequence\ntagging. ∆tdenotes delay for ttime steps.\n1188\nTasks/Models EO CT RC EO ∆1 EO∆2 RC∆1 RC∆2\nATIS-Intent\nBaseline 0.488 0.238 0.812 0.304 0.196 0.874 0.923\nLT 0.320 0.152 0.885 0.157 0.095 0.934 0.959\nLT+R 0.276 0.191 0.836 - - - -\nLT+R+CM 0.174 0.097 0.925 - - - -\nLT+R+CM+D1 - 0.056 - 0.097 - 0.958 -\nLT+R+CM+D2 - 0.032 - - 0.073 - 0.976\nSNIPS-Intent\nBaseline 0.294 0.176 0.857 0.171 0.108 0.896 0.925\nLT 0.241 0.172 0.867 0.173 0.132 0.895 0.919\nLT+R 0.120 0.131 0.883 - - - -\nLT+R+CM 0.188 0.112 0.915 - - - -\nLT+R+CM+D1 - 0.073 - 0.130 - 0.944 -\nLT+R+CM+D2 - 0.044 - - 0.083 - 0.969\nPos/Neg\nBaseline 0.358 0.249 0.829 0.245 0.191 0.859 0.881\nLT 0.363 0.272 0.821 0.272 0.216 0.843 0.867\nLT+R 0.103 0.081 0.931 - - - -\nLT+R+CM 0.317 0.205 0.852 - - - -\nLT+R+CM+D1 - 0.162 - 0.250 - 0.876 -\nLT+R+CM+D2 - 0.117 - - 0.167 - 0.908\nPros/Cons\nBaseline 0.150 0.112 0.927 0.089 0.054 0.951 0.965\nLT 0.158 0.113 0.925 0.086 0.053 0.952 0.966\nLT+R 0.095 0.071 0.943 - - - -\nLT+R+CM 0.098 0.069 0.952 - - - -\nLT+R+CM+D1 - 0.040 - 0.059 - 0.972 -\nLT+R+CM+D2 - 0.027 - - 0.042 - 0.981\nTable 12: Mean value of Edit Overhead, Correction Time Score and Relative Correctness on test sets for sequence\nclassiﬁcation. ∆tdenotes delay for ttime steps.\n1189\nTasks/Models EO CT RC\nATIS-Slot\nBaseline\n– GloVe 0.030 0.016 0.955\n– Pos 0.109 0.058 0.836\n– GloVe & Pos 0.108 0.059 0.834\nLT+R+CM\n– GloVe 0.000 0.000 1.000\n– Pos 0.000 0.000 1.000\n– GloVe & Pos 0.000 0.000 1.000\nSNIPS-Slot\nBaseline\n– GloVe 0.154 0.086 0.778\n– Pos 0.264 0.172 0.597\n– GloVe & Pos 0.236 0.149 0.645\nLT+R+CM\n– GloVe 0.000 0.000 1.000\n– Pos 0.000 0.000 1.000\n– GloVe & Pos 0.000 0.000 1.000\nChunk\nBaseline\n– GloVe 0.177 0.059 0.675\n– Pos 0.325 0.190 0.296\n– GloVe & Pos 0.360 0.200 0.276\nLT+R+CM\n– GloVe 0.000 0.000 1.000\n– Pos 0.000 0.000 1.000\n– GloVe & Pos 0.000 0.000 1.000\nNER\nBaseline\n– GloVe 0.079 0.021 0.875\n– Pos 0.096 0.040 0.792\n– GloVe & Pos 0.114 0.046 0.759\nLT+R+CM\n– GloVe 0.000 0.000 1.000\n– Pos 0.000 0.000 1.000\n– GloVe & Pos 0.000 0.000 1.000\nPoS Tagging\nBaseline\n– GloVe 0.124 0.035 0.790\n– Pos 0.184 0.079 0.567\n– GloVe & Pos 0.195 0.087 0.545\nLT+R+CM\n– GloVe 0.000 0.000 1.000\n– Pos 0.000 0.000 1.000\n– GloVe & Pos 0.000 0.000 1.000\nTable 13: Ablation of GloVe and positional embed-\ndings on the baseline and LT+R+CM for incremental\nmetrics in sequence tagging.\nTasks/Models EO CT RC\nATIS-Intent\nBaseline\n– GloVe 0.505 0.248 0.798\n– Pos 0.472 0.235 0.807\n– GloVe & Pos 0.497 0.239 0.802\nLT+R+CM\n– GloVe 0.179 0.101 0.922\n– Pos 0.166 0.091 0.927\n– GloVe & Pos 0.190 0.115 0.916\nSNIPS-Intent\nBaseline\n– GloVe 0.247 0.172 0.869\n– Pos 0.263 0.165 0.872\n– GloVe & Pos 0.195 0.139 0.892\nLT+R+CM\n– GloVe 0.162 0.093 0.925\n– Pos 0.179 0.105 0.921\n– GloVe & Pos 0.176 0.102 0.919\nPos/Neg\nBaseline\n– GloVe 0.452 0.368 0.769\n– Pos 0.356 0.256 0.826\n– GloVe & Pos 0.477 0.375 0.774\nLT+R+CM\n– GloVe 0.324 0.197 0.859\n– Pos 0.284 0.192 0.856\n– GloVe & Pos 0.306 0.180 0.865\nPros/Cons\nBaseline\n– GloVe 0.162 0.116 0.924\n– Pos 0.167 0.120 0.924\n– GloVe & Pos 0.188 0.142 0.910\nLT+R+CM\n– GloVe 0.096 0.065 0.955\n– Pos 0.095 0.064 0.955\n– GloVe & Pos 0.094 0.066 0.954\nTable 14: Ablation of GloVe and positional embed-\ndings on the baseline and LT+R+CM for incremental\nmetrics in sequence classiﬁcation."
}