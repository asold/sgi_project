{
  "title": "Waterline Extraction for Artificial Coast With Vision Transformers",
  "url": "https://openalex.org/W4214728481",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5038852702",
      "name": "Le Yang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100609321",
      "name": "Xing Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5043355976",
      "name": "Jingsheng Zhai",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6745633707",
    "https://openalex.org/W6775803810",
    "https://openalex.org/W6725280201",
    "https://openalex.org/W2899018893",
    "https://openalex.org/W2386798478",
    "https://openalex.org/W2565813159",
    "https://openalex.org/W2757094569",
    "https://openalex.org/W3108467103",
    "https://openalex.org/W2283971183",
    "https://openalex.org/W3136784992",
    "https://openalex.org/W3104818455",
    "https://openalex.org/W6728092187",
    "https://openalex.org/W6646145903",
    "https://openalex.org/W3139125786",
    "https://openalex.org/W6729668174",
    "https://openalex.org/W6758381192",
    "https://openalex.org/W2963859992",
    "https://openalex.org/W2092826670",
    "https://openalex.org/W2612676323",
    "https://openalex.org/W2085033961",
    "https://openalex.org/W2552163652",
    "https://openalex.org/W2292302010",
    "https://openalex.org/W2603234305",
    "https://openalex.org/W2995504921",
    "https://openalex.org/W3116080608",
    "https://openalex.org/W2560703724",
    "https://openalex.org/W2408718570",
    "https://openalex.org/W4231421387",
    "https://openalex.org/W2900347003",
    "https://openalex.org/W6756040250",
    "https://openalex.org/W2046304175",
    "https://openalex.org/W2224298490",
    "https://openalex.org/W1990632734",
    "https://openalex.org/W2965521953",
    "https://openalex.org/W2987881603",
    "https://openalex.org/W6753287596",
    "https://openalex.org/W2768954657",
    "https://openalex.org/W2977103245",
    "https://openalex.org/W2931973139",
    "https://openalex.org/W3112991035",
    "https://openalex.org/W2300376979",
    "https://openalex.org/W2787048340",
    "https://openalex.org/W2766479134",
    "https://openalex.org/W2790066311",
    "https://openalex.org/W3013015556",
    "https://openalex.org/W2507814723",
    "https://openalex.org/W2910436688",
    "https://openalex.org/W2762430914",
    "https://openalex.org/W1983470924",
    "https://openalex.org/W2526933980",
    "https://openalex.org/W2899771611",
    "https://openalex.org/W2546440660"
  ],
  "abstract": "Accurate acquisition for the positions of the waterlines plays a critical role in coastline extraction. However, waterline extraction from high-resolution images is a very challenging task because it is easily influenced by the complex background. To fulfill the task, two types of vision transformers, segmentation transformers (SETR) and semantic segmentation transformers (SegFormer), are introduced as an early exploration of the potential of transformers for waterline extraction. To estimate the effects of the two methods, we collect the high-resolution images from the web map services, and the annotations are created manually for training and test. Through extensive experiments, transformer-based approaches achieved state-of-the-art performances for waterline extraction in the artificial coast.",
  "full_text": "Waterline Extraction for Artiﬁcial\nCoast With Vision Transformers\nLe Yang, Xing Wang* and Jingsheng Zhai\nSchool of Marine Science and Technology, Tianjin University, Tianjin, China\nAccurate acquisition for the positions of the waterlines plays a critical role in coastline\nextraction. However, waterline extraction from high-resolution images is a very challenging\ntask because it is easily inﬂuenced by the complex background. To fulﬁll the task, two types\nof vision transformers, segmentation transformers (SETR) and semantic segmentation\ntransformers (SegFormer), are introduced as an early exploration of the potential of\ntransformers for waterline extraction. To estimate the effects of the two methods, we\ncollect the high-resolution images from the web map services, and the annotations are\ncreated manually for training and test. Through extensive experiments, transformer-based\napproaches achieved state-of-the-art performances for waterline extraction in the\nartiﬁcial coast.\nKeywords: coastline extraction, vision transformer, SegFormer, sea–land segmentation, waterline extraction\nINTRODUCTION\nA coastline is the boundary between the dry and wet part in the coastal area when the high tide water\nis in the mean levelToure et al. (2018). The coastline is a critical geographic information source, and\nit is of great signiﬁcance to autonomous navigation, coastal resource management, and protection of\nthe environment Liu et al. (2013). Coastline extraction is a very challenging problem because it is\nobtained from a region not an instantaneous line. The waterline extraction is the precondition for\ncomputing the natural coastline, so the waterline extraction is very important and meaningful. The\nwaterline is the instantaneous boundary between the land and sea. It can be extracted from the high-\nresolution images without other tools. In the artiﬁcial coast, the waterline can be considered as the\ncoastline because the waterline is very slightly inﬂuenced by the tides.\nWith the development of satellite remote sensing technology, it supplies tons of high-resolution\nimages of the coastal area, and they can be used for waterline extractionRoelfsema et al. (2013).\nBesides buying these remote sensing images directly from the remote sensing image providers, users\ncan obtain many satellite map images freely from the web map services. All these data can be used for\nthe waterline extraction.\nThe waterline extraction methods mainly include threshold segmentation methods, edge-based\nmethods, object-oriented methods, active contour method, conventional machine learning methods,\nand deep learning methods. The threshold segmentation methods are intuitive methods that set a\nthreshold value according to the image intensity to segment the land and water.Guo et al. (2016)\nproposed a method that utilized a normalized difference water index to segment water and land.\nChen et al. (2019)used the components of the tasseled cap transformation to extract waterline\ninformation. Wernette et al. (2016)presented a threshold-based multi-scale relative relief method to\nextract the barrier island morphology from high-resolution DEM. These methods are handy and\neffective for the simple image segmentation task. In these methods, threshold selection is the key and\ndifﬁcult problem. In addition, the methods cannot deal with the images with occlusions or a complex\nbackground.\nEdited by:\nPeng Liu,\nInstitute of Remote Sensing and Digital\nEarth (CAS), China\nReviewed by:\nZq Gao,\nChinese Academy of Sciences (CAS),\nChina\nMin Chen,\nSouthwest Jiaotong University, China\n*Correspondence:\nXing Wang\nxing.wang@tju.edu.cn\nSpecialty section:\nThis article was submitted to\nEnvironmental Informatics and Remote\nSensing,\na section of the journal\nFrontiers in Environmental Science\nReceived: 21 October 2021\nAccepted: 10 January 2022\nPublished: 28 February 2022\nCitation:\nYang L, Wang X and Zhai J (2022)\nWaterline Extraction for Artiﬁcial Coast\nWith Vision Transformers.\nFront. Environ. Sci. 10:799250.\ndoi: 10.3389/fenvs.2022.799250\nFrontiers in Environmental Science | www.frontiersin.org February 2022 | Volume 10 | Article 7992501\nORIGINAL RESEARCH\npublished: 28 February 2022\ndoi: 10.3389/fenvs.2022.799250\nThe edge-based methods utilize the distinctive edge feature\nfrom the abrupt transition. The common methods including\nSobel, Roberts ( Yang et al., 2018 ), Laplacian, and Canny\noperators (Lin et al., 2013; Paravolidakis et al., 2016; Ao et al.,\n2017; Widyantara et al., 2017; Paravolidakis et al., 2018) can be\nadopted to extract the waterline.Wang and Liu (2019)proposed a\nrobust ridge-tracing method utilizing the statistical properties of\nthe pixel intensities in the land and sea to detect the boundary.\nThese methods are easy to detect clear, continuous boundaries.\nHowever, in the waterline images with a complex background,\nthey are greatly affected by noise. The continuity of the extracted\nwaterline is hardly guaranteed.\nObject-oriented methods no longer use the pixel as the basic\nprocessing unit; instead, they use an object composed of\nhomogeneous pixels (Gucluer et al., 2010; Rasuly et al., 2010;\nBayram et al., 2017). Ge et al. (2014)presented an object-oriented\nmulti-scale segmentation method using interpretation rule sets\nfor automated waterline extraction from remote sensing imagery.\nWu et al. (2018)used the object-oriented classiﬁcation method to\nextract the waterline from Landsat images of Shenzhen city.\nThese methods use higher level features to classify images,\nwhich can reduce the impact ofﬁne texture characteristics on\nthe results of image classiﬁcation. However, in face of a large\namount of information in high-resolution images, object-\noriented classiﬁcation methods may ignore some of the hidden\nuseful information, and it is difﬁcult to achieve the desired\nclassiﬁcation accuracy.\nThe active contour methods can achieve better results for\nremote sensing images of waterlines with simple backgrounds,\nstrong contrast, and continuous boundaries.Cao et al. (2016)\nproposed a new geometric active contour model for waterline\ndetection from SAR images, which is adaptive to the speckle\nnoises. Fan et al. (2016)proposed a level set approach with a\nparticle swarm optimization algorithm for waterline automatic\ndetection in SAR images. Elkhateeb et al. (2021) adopted a\nmodiﬁed Chan – Vese method for sea – land segmentation,\nwhich is initiated by a superpixel-based fuzzy c-means\nautomatically. In the study by Modava and Akbarizadeh,\n(2017), a waterline extraction method– based active contour for\nSAR images is proposed, in which the initial contour is obtained\nfrom a fuzzy clustering with spatial constraints. In the study by\nLiu C et al., (2016), the waterline is extracted hierarchically by the\nlevel set techniques from single-polarization and four-\npolarization SAR images. Liu et al. (2017)integrated an edge-\nbased and a region-based active contour model in different scales\nto fulﬁll the waterline detection from SAR Images. Due to the\ncharacteristics of the active contour model method, the\napplication of this method is feasible for waterline images with\na simple background, strong contrast, and continuous\nboundaries. However, the iterative method inevitably produces\na large amount of calculation, which restricts its efﬁciency.\nConventional machine learning methods distill useful\ninformation and hidden knowledge based on a variety of data\nto extract the waterline.Rigos et al. (2016)and Vos et al. (2019)\nused a shallow neural network to extract the shoreline from\nsatellite images and video images, separately.Sun et al. (2019)\nbuilt a superpixel-based conditional random ﬁeld model to\nsegment the sea and land area. Dewi et al. (2016) presented\nfuzzy c-means methods to detect positions of the coastline and\nestimate the uncertainty of the coastline change.Cheng et al.\n(2016)\nproposed a graph cut method to segment the sea and land,\nin which the seed points are achieved by a probabilistic support\nvector machine. Compared with the traditional waterline\nextraction method, the shallow neural network, clustering\nanalysis technology, fuzzy logic technology, and support vector\nmachine use intelligent means toﬁnd out frequent regular things\nfrom a large number of data information effectively. These\nmethods can automatically and ef ﬁciently extract regular\nobjects. However, for more complex objects in high-resolution\nimages, the extraction accuracy is unsatisfactory. Some other\ntraditional methods, such as the polarization method (Nunziata\net al., 2016), wavelet transform method (Toure et al., 2018),\nregion growing method (Liu Z et al., 2016), and decision tree\nalgorithm (Wang et al., 2020) , are all inﬂuenced by noise and\ncannot process the high-resolution images easily.\nIn these years, deep learning methods have been rapidly\ndeveloping with the quickly growing performance of computer\nhardware. Different from traditional machine learning, it can\nlearn the characteristics of the target more accurately. Some\nconvolutional neural network (CNN) methods are naturally\nintroduced in waterline extraction by segmenting the land and\nsea. In the study byLiu et al., (2019), a simple CNN with multi-\nscale features and leaky recti ﬁed linear unit (leaky-ReLU)\nactivation function is used for waterline extraction. Liu W\net al. (2021) proposed an end-to-end lightweight multitask\nCNN without downsampling to obtain lakes and shorelines\nfrom remote sensing images. Shamsolmoali et al. (2019)\nadopted a residual dense UNet to facilitate the hierarchical\nfeatures from the original images for sea– land segmentation.\nTsekouras et al. (2018)presented a novel Hermite polynomial\nneural network to detect the shoreline at a reef-fronted beach.\nCheng et al. (2017a)proposed a local smooth regularized deep\nCNN that can obtain the segmentation and edge results of the sea\nand land simultaneously. Cheng et al. (2017b) employed a\nmultitasking edge– aware CNN for sea– land segmentation and\nedge detection simultaneously.Cui et al. (2021)presented a scale-\nadaptive CNN for sea– land segmentation, which fused multiscale\ninformation and emphasized the boundaries’ features actively. A\nsea– land segmentation approach utilizing the fast structured edge\nnetwork and the waterline database was taken from the study by\nHe et al., (2018). A novel UNet-like CNN was proposed for\nsea– land segmentation, and the network can be deeper, and the\nconvergence can be faster based on local and global information\n(Li et al., 2018). Erdem et al. (2021)proposed a majority voting\nmethod based on different deep learning architectures to obtain\nshorelines automatically.Lin et al. (2017)presented a multi-scale\nend-to-end CNN for sea– land segmentation and ship detection,\nwhich can increase the receptive ﬁeld while maintaining ﬁne\ndetails. Even though the CNNs have achieved great performances,\nthe limited receptiveﬁeld affected the performance because of the\nstructure of the CNN.\nTransformers, as the most advanced methods in the semantic\nsegmentation, are migrated to compute vision tasks to solve the\nproblem of long-distance dependence by the self-attention\nFrontiers in Environmental Science | www.frontiersin.org February 2022 | Volume 10 | Article 7992502\nYang et al. Waterline Extraction With Vision Transformers\nmechanism, which is the core of transformers. It determines the\nglobal contextual information of each item by capturing its\ninteraction amongst all items. A vision transformer (ViT) is\nthe ﬁrst work that uses a pure transformer for image\nclassiﬁcation, which proves that the transformer can achieve\nthe state-of-the-art (Dosovitskiy et al., 2021 ). It treats each\nimage as a sequence of tokens and then feeds them to\nmultiple transformer layers to make the classi ﬁcation.\nSubsequently, the dual intent and entity transformer (DeiT)\n(Touvron et al., 2021) further explores a data-efﬁcient training\nstrategy and a distillation approach for ViTs. The pyramid vision\ntransformer (PVT) is the ﬁrst work to introduce a pyramid\nstructure in a transformer, demonstrating the potential of a\npure transformer backbone compared to CNN counterparts in\ndense prediction tasks (Wang et al., 2021). After that, methods\nsuch as shifted windows (Swin) transformer (Liu Z et al., 2021),\nconvolution transformers (CvT) (Xu et al., 2021), and twin\ntransformer (Chu et al., 2021) enhance the local continuity of\nfeatures and removeﬁxed size position embedding to improve the\nperformance of transformers in dense prediction tasks.\nSegmentation transformers (SETRs) adopt the ViT as a\nbackbone to extract features, achieving impressive performance\nin segmentation (Zheng et al., 2021). Following it, semantic\nsegmentation transformers (SegFormer) achieved even better\nresults later (Xie et al., 2021).\nTherefore, we use the most advanced transformer methods to\nextract the waterline as an early exploration. This study mainly\nfocuses on the process of extraction of the waterlines for artiﬁcial\ncoasts and presents the early research for investigating the\npotential of transformers in waterline extraction from very\nhigh-resolution images.\nThe rest of the study has the following sections.Materials and\nMethods suggests details about the dataset and methodology.\nResults reports experimental results with a discussion. Finally, the\nconclusion section concludes and discusses future research\ndirections.\nMATERIALS AND METHODS\nDataset\nFor this research, we selected Tianjin, Zhoushan, Shanghai, and\nShenzhen four ports as research areas, which are shown in\nFigure 1. The waterline images are collected from Mapbox\n(Mapbox, 2021), Google Maps (Google Maps, 2021), and Bing\nMaps (Microsoft, 2021) guided by OpenStreetMap (OSM) tiles.\nThe images are in 18 levels in the map, and the initial resolution is\n256 × 256. The ground sampling distance (GSD) is about 0.48 m.\nWe combine each neighboring four tiles into a 512 × 512 image. A\ntotal of 600 images are chosen as the initial data, and the ground\ntruths of waterlines are created by hand. We also augment it with\nthe random rotation, ﬂip, scale, contrast, brightness, and\nsaturation to 6000 images. Among them, 3600 images are\nconsidered as the training set, 1200 images and 1200 images\nfor validation and test, respectively. The images and\ncorresponding annotations are indicated in Figure 2 .T o\nevaluate the effects of transformers, six CNN segmentation\nmethods are introduced in the experiments.\nMethodology\nSETR\nSETR is an Encoder– Decoder architecture, as seen inFigure 3.\nSETR adopted a high resolution of local features extracted by a\nCNN and the global information encoded by transformers to\nsegment pixels in an image. Because of quadratic model\ncomplexity of the transformer,ﬂattening the whole image as a\nFIGURE 1 |Research area.\nFrontiers in Environmental Science | www.frontiersin.org February 2022 | Volume 10 | Article 7992503\nYang et al. Waterline Extraction With Vision Transformers\nsequence makes a huge amount of computation. To speed up, an\nimage is divided into 256 even patches, and then, each patch is\nﬂattened into a sequence for input separately.\nAll the sequences are entered into the pure transformer-based\nencoder. Therefore, all the transformer layers have a global receptive\nﬁeld, which improve the limited receptiveﬁeld problem from the\nCNN. There are 24 layers of transformers in the encoder, in which\nthere are multi-head self-attention (MSA), multilayer perceptron\n(MLP), and layer normalization blocks residually connected.\nThe decoder is called the multi-level feature aggregation (MLA).\nSome feature representations from the transformer layers areﬁrst\nreshaped from 2D to 3D and then aggregated. A 3-layer convolution\nnetwork downsamples the features at theﬁrst and third layer. To\nenhance the interactions of different levels of features, a top-down\naggregation design is introduced. The fused feature is obtainedvia\nchannel-wise concatenation after the third layer. At last, the outputs\nare upscaled by bilinear operation to the original resolution.\nSegFormer\nThe architecture of SegFormer is depicted in Figure 4. The\nSegFormer consists of two main modules, encoder and\ndecoder. An image as the input isﬁrst divided into patches in\n4 × 4. Then, these patches are imported to the hierarchical\ntransformer encoder to obtain multi-level features. These\nmulti-level features are passed to the MLP decoder to predict\nthe segmentation mask at aH/4 ×W/4 ×N\ncls resolution, where\nH, W, Ncls are the height, width of the image, and the number of\ncategories in the image, respectively.\nFIGURE 2 |Dataset.\nFIGURE 3 |Architecture of the SETR.\nFrontiers in Environmental Science | www.frontiersin.org February 2022 | Volume 10 | Article 7992504\nYang et al. Waterline Extraction With Vision Transformers\nIn the encoder, an input image is one with a resolution of\nH × W × 3, andCi is the channel number in the feature mapFi.\nA hierarchical feature map Fi with a resolution of\nH/2i+1 × W/2i+1 × Ci is obtained after each transformer block,\nwhere i ∈ {1, 2, 3, 4}, and Ci+1 is larger thanCi.\nThe transformer consists of efﬁcient self-attention, Mix-FFN,\nand overlap patch merging blocks. Ef ﬁcient self-attention\nimproves the computational efﬁciency of the self-attention. In\nthe original multi-head self-attention process, each of the heads\nhas the same dimensionN × C, whereN /equals H × W is the length\nof the sequence, andC is the channel number. The self-attention\nis expressed as follows:\nAttention(Q, K, V) /equals Softmax(\nQK⊤\n/radicaltpext/radicaltpext\ndh\n√ )V.( 1 )\nIn the equation, dh is the dimension of the head. The\ncomputational complexity of this process is O(N2).T o\nalleviate it, the sequenceK is reduced with a reduction ratioR.\nIt is ﬁrst reshaped intoN/R×C·R and then simpliﬁed by a fully\nconnected layer. Therefore, the newK has dimensionsN/R×C.A s\na result, the complexity of the self-attention mechanism is\nreduced from O(N\n2) to O(N2/R).\nViT uses positional encoding (PE) to express the location\ninformation. It inﬂuences the test accuracy when the image\nresolution is not the same with that in the training because\nthe positional code needs to be interpolated. To address it,\nMix-FFN considers the effect of zero padding to leak location\ninformation, and a 3 × 3 convolution is used in the feed-forward\nnetwork (FFN). Mix-FFN can be formulated as follows:\nx\nout /equals MLP(GELU(Conv3×3(MLP(xin))))+xin, (2)\nwhere xin is the feature from the self-attention module. Mix-FFN\nmixes a 3 × 3 convolution and an MLP into each FFN. The\nGaussian error linear unit (GELU) (Hendrycks and Gimpel,\n2020) is an activation function.xout is the output of the Mix-FFN.\nTo preserve the local continuity around those patches, an\noverlapping patch merging process is used. K is the patch size, S is\nthe stride between two adjacent patches, and P is the padding size.\nK = 7, S = 4, P= 3, and K = 3, S = 2, P= 1 are set to perform\noverlapping patch merging to produce features with the same size\nas the non-overlapping process.\nThe SegFormer incorporates a lightweight decoder consisting\nonly of MLP layers. The proposed All-MLP decoder consists of\nfour main steps. First, multi-level features from the encoder go\nthrough an MLP layer to unify the channel dimension. Then,\nfeatures are upsampled to 1/4th and concatenated together.\nThird, an MLP layer is adopted to fuse the concatenated\nfeatures. Finally, another MLP layer takes the fused feature to\npredict the segmentation mask.\nThe Proposed Method\nIn this section, we demonstrated a method used in the task of\nwaterline extraction. The workﬂow is shown inFigure 5. The\ntransformer ﬁrst learns the coast features from the training\nsamples. This step is the most time-consuming since most\nlayers of the network are trained in this step. After the\nlearning step, parameters of the model are convergent, and it\ncan infer other new coast images for waterline extraction. Then, a\nbinary mask of the coast is obtained from each input image; the\nwaterline can be extracted from the mask easily. It is worth noting\nthat the contours of the coast at the edges of the image should be\nexcluded because this part is truncated when slicing image tiles.\nMetrics\nThe proposed approaches are evaluated by precision, recall, F1-\nscore, and IoU.\nFIGURE 4 |Architecture of the SegFormer.\nFrontiers in Environmental Science | www.frontiersin.org February 2022 | Volume 10 | Article 7992505\nYang et al. Waterline Extraction With Vision Transformers\nP /equals TP\nTP + FP\n, (3)\nR /equals TP\nTP + FN\n, (4)\nF1 /equals 2× P × R\nP + R , (5)\nIU /equals TP\nTP + FP + FN\n.( 6 )\nAmong them, the P and R stand for precision and recall,\nrespectively; the true positive (TP) stands for the rightly extracted\nland area; the false positive (FP) represents the area mistaken as\nthe land; the false negative (FN) means omitted land pixels. In our\nstudy, the reference land images are drawn manually. Precision\nand recall are contradictory in most cases. To address this,\ncomprehensive metrics F1 ( F1) and IoU ( IU) are employed\ncommonly. Inference time is de ﬁned as the average\nsegmentation’s time using our test data. The ﬂoating-point\noperations (Flops) represent the computation of the model,\nand it is a metric for the computational complexity.\nRESULTS\nExperiment Setting\nThe proposed transformers were developed under\nMMsegmentation ( MMSegmentation, 2020 ) by PyTorch\n(Paszke et al., 2017 ). Training and testing were performed\nwith eight NVIDIA TITAN Xp GPUs and one NVIDIA\nTITAN Xp GPU, respectively. In our experimental dataset,\nthere are 3600 images for training, 1200 images for validation,\nand 1200 images for testing. All the annotations are manually\nannotated. The resolution of all images is 512 × 512. The SETR\nuses a learning rate value of 10\n– 3, the number of iterations is\n160,000, and the weights are pretrained on ImageNet-21K. The\nSegFormer was tested using a learning rate value of 10\n– 6, the\nnumber of iterations is 40,000, and the weights are pretrained on\nImageNet-1K. The other compared methods are all run in 40,000\niterations.\nExperimental Results\nQualitative Results\nThe results of the eight methods are displayed inFigure 6. From\nthe results, we can see that PSPNet-UNet, DeepLabV3-UNet, and\nSETR cannot obtain good results in Image 1, Image 4, and Image\n6. A large area in the land is missed, and theﬁne dock structure is\nnot extracted in all the three methods. For the CNN methods, the\nmethods with the ResNet101 backbone are better than the\nmethods with HRNet, and the methods with the UNet\nbackbone achieve the cheapest results. Only the methods with\nResNet101 and HRNet extract the small striped object in Image 6,\nbut no methods can avoid the inﬂuence of the ship. In Image 8,\nonly the FCN-ResNet101 and DeepLabV3-ResNet101 gain\nterriﬁc results. Other CNNs get a lot of false-positive or false-\nnegative parts. For the transformer methods, the SegFormer\nachieves very nice results in all the images, especially for the\nﬁne structures. In contrast, the SETR can also extract the large\nobject effectively in Image 3, but it struggles to the small and thin\nobjects in Image 6 and Image 8. Overall, DeepLabV3-ResNet101,\nFCN-ResNet101, FPN-ResNet101, and SegFormer are all\noutstanding, and PSPNet-UNet, DeepLabV3-UNet, and SETR\nare relatively weak.\nWe can see in Figure 7, the SETR cannot extract theﬁne\nobjects in Image1, Image 3, and Image 4. The dock and\ninfrastructure are all not complete in the three images. It\nFIGURE 5 |Workﬂow of the proposed method.\nFrontiers in Environmental Science | www.frontiersin.org February 2022 | Volume 10 | Article 7992506\nYang et al. Waterline Extraction With Vision Transformers\nhardly ﬁnds the land area near the boundary in Image 2 and\nImage 3. The connection part in the dock is neglected, and the\nshape of the harbor is not regular due to the incomplete extraction\nin Image 2. There are missed land pixels near the frame in Image\n3. For large objects, it can perform well, although the edges are not\nkept ﬁne in Image 1, Image 3, and Image 4.\nFigure 8depicts the results of SegFormer results. We can see\nthat it correctly segments nearly all the pixels. It can even keep the\ndetails of objects well, especially in Image 1 and Image 3. The\nspindly parts in Image 1 and Image 3 are all ﬁne and\nunmistakable. The integrity and differentiation are impressive.\nThe minor complaints are the small leaks near the edges in Image\n1, Image 3, and Image 4 and the small holes in Image 2. The\nSegFormer can extract the land features so good that the waterline\ncan be extracted completely and accurately.\nQuantitative Results\nAll the experimental methods are reported inTable 1. For the\nCNN methods, the models with ResNet101 achieved best results.\nAmong them, the DeepLabV3 is the best with 0.9056 in precision,\nFIGURE 6 |Results of all methods in comparison.\nFrontiers in Environmental Science | www.frontiersin.org February 2022 | Volume 10 | Article 7992507\nYang et al. Waterline Extraction With Vision Transformers\n0.8814 in recall, 0.8674 in F1, and 0.8169 in IoU. The FCN\nand FPN with the backbone of ResNet101 also reach or\napproach 0.9 in precision, 0.88 in recall, 0.86 in F1, and\n0.81 in IoU. By comparison, the methods with UNet get\nlowest scores. PSPNet with UNe t has the least scores, with\nthe precision 0.8298, F1 0.8333, and IoU 0.7632. The\nperformances of DeepLabV3 with UNet are slightly higher\nthan that of PSPNet with UNet. The FCN with HRNet48 is\nmoderate, which achieves 0.8964 in precision, 0.8766 in\nrecall, 0.8581 in F1, and 0.8052 in IoU.\nFor the vision Transformer methods, we can see that the\nSegFormer reaches the precision 0.9121, recall 0.9104, F1-score\n0.8883, and IoU 0.8439, respectively, which prove its accurate and\nrobust performance to segment the land and sea. SETR gets the\nlowest scores in all metrics. The scores match the results in\nFigures 6, 7; it cannot acquire the ideal land area, and the shapes\nare very incomplete.\nThe ﬂoating-point operations (Flops) represent the model\ncomplexity. Table 1shows that the DeepLabV3 models occupy\nmore computing power, followed by SETR and PSPNet. The\nFCN, FPN, and SegFormer use the least resource in all the\nmethods. Specially, the DeepLabV3-ResNet101 consume the\nlargest computing units, and the SegFormer is the most\nresource-saving. For the inference time, except the FCN-\nHRNet48 and SETR with 0.77 and 0.32, other methods are\nall under 0.3 s. The FCN with ResNet101, FPN with\nResNet101, and SegFormer can even infer an image in 0.2 s.\nFCN-ResNet101 is the fastest method in inference.\nDISCUSSION\nPerformance Analysis of the Superior CNNs\nIn the six CNN methods, the networks with the backbone of\nResNet101 are the best extractors, and they occupy the top three\nfor accuracy. It is followed by the HRNet48, and the UNet is the\nlast. In ResNet101, the convolution layers are very deep, and the\nfeatures are connected with residual blocks. It can keep more\ndetail features and avoid gradient vanishing by this way.\nMeanwhile, this ResNet101 uses dilated convolution to\nincrease the receptive ﬁeld, which makes it more powerful.\nThe HRNet generates high-resolution and low-resolution\nparallel subnetworks. It can merge the high-resolution features\nand low-resolution features through the different stages by\nconnecting the multi-resolution parallel subnetworks.\nTherefore, it can obtain rich high-resolution and low-\nresolution representations. The best header is DeepLabV3\nbecause it achieves the best scores when different methods use\nthe same backbone of UNet or ResNet101. In DeepLabV3, the\natrous spatial pyramid pooling (ASPP) adds a series of atrous\nconvolutions with different dilated rates to increase global\ncontextual information. Global average pooling (GAP) also\nFIGURE 7 |Results of SETR.\nFrontiers in Environmental Science | www.frontiersin.org February 2022 | Volume 10 | Article 7992508\nYang et al. Waterline Extraction With Vision Transformers\ncombines image-level features. These all make the DeepLabV3\noutstanding.\nPerformance Analysis of the Superior\nTransformer\nIn the two vision transformers, the SETR obtain F1 with 0.8018\nand IoU with 0.7268, and the SegFormer achieves 0.8883 in F1-\nscore and 0.8439 in IoU. The SegFormer wins the SETR in\naccuracy completely. It can also be seen from theFigures 7, 8,\nthe SETR cannot extract the integrated and continuous\nstructures in Image 2 and Image 3, and the SegFormer can\nextract nearly the whole and accurate structures. In SETR, the\nfeature maps after the transformer layers are in the same size,\nand in the SegFormer, it generates multi-level feature maps.\nT h ed i f f e r e n ts c a l e so ff e a t u r em a p si n c l u d et h eh i g h -\nresolution coarse features and low-resolution ﬁne-grained\nfeatures, so it can adapt to large and small object\nextractions. At the same time, the decoder in the SegFormer\nis made up of only MLP, which is lighter and has a larger\neffective ﬁeld than traditional CNN encoders. These all make\nthe SegFormer perform better than the SETR. On the other\nside, the SETR has more computational complexity with 212.4\nGFLOPs than the SegFormer with just 51.83 GFLOPs. The\nhuge amount of computation of the SETR is from the self-\nattention in the transformer. Because the computational\ncomplexity of self-attention is O(N\n2), N is the length of the\ninput sequence. The SegFormer uses the efﬁcient self-attention\nFIGURE 8 |Results of SegFormer.\nTABLE 1 |Comparison for all the methods in metrics.\nMethod Backbone Flops (GFLOPs) Inference Time(s) Precision Recall F1 IoU\nPSPNet UNet 197.76 0.25 0.8298 0.8782 0.8333 0.7632\nDeepLabV3 UNet 203.43 0.25 0.8518 0.8703 0.8406 0.7719\nFCN HRNet48 93.38 0.77 0.8964 0.8766 0.8581 0.8052\nFCN ResNet101 76.07 0.08 0.8995 0.8814 0.8636 0.8113\nFPN ResNet101 64.73 0.09 0.9010 0.8793 0.8637 0.8096\nDeepLabV3 ResNet101 347.33 0.23 0.9056 0.8814 0.8674 0.8169\nSETR T-Large 212.4 0.32 0.8244 0.8397 0.8018 0.7268\nSegFormer MiT-B5 51.83 0.15 0.9121 0.9104 0.8883 0.8439\nFrontiers in Environmental Science | www.frontiersin.org February 2022 | Volume 10 | Article 7992509\nYang et al. Waterline Extraction With Vision Transformers\nwhich reduces the computational complexity by some\nt r a n s f o r m s .T h e r e f o r e ,t h eS e g F o r m e ri sm u c he a s i e rt o\ncompute.\nThe Most Robust Method\nThe top CNN DeepLabV3 and the transformer SegFormer are all\nvery competitive. However, the vision transformer SegFormer is\nsuperior to DeepLabV3 in precision, recall, F1, and IoU. It also\nhas a smaller complexity and shorter inference time. The limited\nreceptive ﬁeld in DeepLabV3 requires the ASPP module to\nenlarge the receptive ﬁeld, but the model inevitably becomes\nheavy. The SegFormer beneﬁts from the non-local attention in\ntransformers and enjoys a larger receptiveﬁeld. The transformer\nintegrates with the MLP decoder an can produce both highly local\nand non-local attention by adding fewer parameters. These all\nmake the SegFormer more efﬁcient and lighter in waterline\nextraction.\nCONCLUSION\nWe propose a new method based on the vision transformers for\nthe waterline extraction by sea – land segmentation. Two\ntransformers, the SegFormer and SETR, are adapted to\nsegment and identify land pixels by a custom dataset from\nsatellite maps. The performances of the two transformers are\ncompared with other state-of-the-art CNN methods, PSPNet,\nDeepLabV3, FCN, and FPN. The SETR with a pure transformer\nstructure, as an early comer to image segmentation, achieves a\nnearly equivalent performance compared with the developed\nCNN methods. More surprisingly, the latter method, the\nSegFormer outperforms state-of-the-art CNN methods and\ndemonstrates an extraordinary ability to segment land pixels\nunder different conditions. For future work, we hope to\nimprove the method in accuracy and robustness, though it has\nachieved a fairly good performance.\nDATA AVAILABILITY STATEMENT\nThe raw data supporting the conclusions of this article will be\nmade available by the authors, without undue reservation.\nAUTHOR CONTRIBUTIONS\nLY conceived and designed the analysis, collected the data,\nperformed the analysis, wrote the original draft, and discussed\nthe results. XW veriﬁed the analytical methods, discussed the\nresults, and reviewed and edited the manuscript. JZ supervised\nthe work and reviewed and edited the manuscript.\nFUNDING\nThis study was supported by the National Natural Science\nFoundation of China (Grant nos. 41701480, 42006168, and\n41806116) and the Key Research and Development Program\nof Tianjin (Grant no. 20YFZCSN01040) and the Natural Science\nFoundation of Tianjin (Grant nos. 20JCQNJC01230). Tianjin\nPhilosophy and Social Science Planning Project of China (Grant\nno. TJKS20XSX-015).\nREFERENCES\nAo, D., Dumitru, O., Schwarz, G., and Datcu, M. P. (2017).“Coastline Detection\nwith Time Series of SAR Images,” in Remote Sensing of the Ocean, Sea Ice,\nCoastal Waters, and Large Water Regions 2017, San Diego, CA, August 6– 10,\n2017 (SPIE), 70– 78. doi:10.1117/12.2278318\nBayram, B., Avsar, O., Seker, D. Z., Kayi, A., Erdogan, M., Eker, O., et al. (2017).\nThe Role of National and International Geospatial Data Sources in Coastal\nZone Management. Fresenius Environ. Bull.26, 383– 391.\nCao, K., Fan, J., Xinxin Wang, X., Xiang Wang, X., Jianhua Zhao, J., and Fengshou\nZhang, F. (2016).“Coastline Automatic Detection Based on High Resolution\nSAR Images,” in 2016 4th International Workshop on Earth Observation and\nRemote Sensing Applications, Guangzhou, China, July 4– 6, 2016 (EORSA),\n43– 46. doi:10.1109/EORSA.2016.7552763\nC h e n ,C . ,F u ,J . ,Z h a n g ,S . ,a n dZ h a o ,X .( 2 0 1 9 ) .C o a s t l i n eI n f o r m a t i o n\nExtraction Based on the Tasseled Cap Transformation of Landsat-8\nOLI Images. Estuarine Coastal Shelf Sci. 217, 281 – 291. doi:10.1016/j.\necss.2018.10.021\nCheng, D., Meng, G., Xiang, S., and Pan, C. (2016). Ef ﬁcient Sea-Land\nSegmentation Using Seeds Learning and Edge Directed Graph Cut.\nNeurocomputing 207, 36– 47. doi:10.1016/j.neucom.2016.04.020\nCheng, D., Meng, G., Cheng, G., and Pan, C. (2017a). SeNet: Structured Edge\nNetwork for Sea-Land Segmentation. IEEE Geosci. Remote Sensing Lett.14,\n247– 251. doi:10.1109/LGRS.2016.2637439\nCheng, D., Meng, G., Xiang, S., and Pan, C. (2017b). FusionNet: Edge Aware Deep\nConvolutional Networks for Semantic Segmentation of Remote Sensing Harbor\nImages. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sensing10, 5769– 5783. doi:10.\n1109/JSTARS.2017.2747599\nChu, X., Tian, Z., Wang, Y., Zhang, B., Ren, H., Wei, X., et al. (2021). Twins:\nRevisiting the Design of Spatial Attention in Vision Transformers. arXiv\n[Preprint]. Available at: http://arxiv.org/abs/2104.13840 (Accessed October\n20, 2021).\nCui, B., Jing, W., Huang, L., Li, Z., and Lu, Y. (2021). SANet: A Sea-Land\nSegmentation Network via Adaptive Multiscale Feature Learning. IEEE\nJ. Sel. Top. Appl. Earth Obs. Remote Sensing 14, 116– 126. doi:10.1109/\nJSTARS.2020.3040176\nDewi, R., Bijker, W., Stein, A., and Marfai, M. (2016). Fuzzy Classiﬁcation for\nShoreline Change Monitoring in a Part of the Northern Coastal Area of Java,\nIndonesia. Remote Sensing 8, 190. doi:10.3390/rs8030190\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., et al. (2021). An Image Is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. arXiv [Preprint]. Available at: http://arxiv.org/abs/2010.\n11929 (Accessed October 20, 2021).\nElkhateeb, E., Soliman, H., Atwan, A., Elmogy, M., Kwak, K.-S., and Mekky,\nN. (2021). A Novel Coarse-To-Fine S ea-Land Segmentation Technique\nBased on Superpixel Fuzzy C-Means Clustering and Modi ﬁed Chan-\nVese Model. IEEE Access 9, 53902 – 53919. doi:10.1109/ACCESS.2021.\n3065246\nErdem, F., Bayram, B., Bakirman, T., Bayrak, O. C., and Akpinar, B. (2021). An\nEnsemble Deep Learning Based Shoreline Segmentation Approach (WaterNet)\nfrom Landsat 8 OLI Images.Adv. Space Res. 67, 964– 974. doi:10.1016/j.asr.\n2020.10.043\nFan, J., Cao, K., Zhao, J., Jiang, D., and Tang, X. (2016).“A Hybrid Particle Swarm\nOptimization Algorithm for Coastline SAR Image Automatic Detection,\n” in\n2016 12th World Congress on Intelligent Control and Automation, Guilin,\nChina, June 12 – 15, 2016 (WCICA), 822 – 825. doi:10.1109/WCICA.2016.\n7578256\nFrontiers in Environmental Science | www.frontiersin.org February 2022 | Volume 10 | Article 79925010\nYang et al. Waterline Extraction With Vision Transformers\nGe, X., Sun, X., and Liu, Z. (2014).“Object-oriented Coastline Classiﬁcation and\nExtraction from Remote Sensing Imagery, ” in Remote Sensing of the\nEnvironment: 18th National Symposium on Remote Sensing of China,\nWuhan, China, October 20– 23, 2012 (SPIE), 131– 137. doi:10.1117/12.2063845\nGoogle Maps (2021). Available at: https://www.google.com/maps/ (Accessed\nAugust 15, 2020).\nGucluer, D., Bayram, B., and Maktav, D. (2010).“Land Cover and Coast Line\nChange Detection by Using Object Oriented Image Processing in Alacati,\nTurkey,” in Imagin[e,g] Europe, Chania, Greece, 158– 165. doi:10.3233/978-1-\n60750-494-8-158\nGuo, Q., Pu, R., Zhang, B., and Gao, L. (2016).“A Comparative Study of Coastline\nChanges at Tampa Bay and Xiangshan Harbor During the Last 30 Years,” in\n2016 IEEE International Geoscience and Remote Sensing Symposium, Beijing,\nChina, July 10– 15, 2016 (IGARSS), 5185– 5188. doi:10.1109/IGARSS.2016.\n7730351\nHe, L., Xu, Q., Hu, H., and Zhang, J. (2018).“Fast and Accurate Sea-Land Segmentation\nBased on Improved SeNet and Coastline Database for Large-Scale Image,” in 2018\nFifth International Workshop on Earth Observation and Remote Sensing\nApplications, Xi’an, China, June 18 – 20, 2018 (EORSA), 1 – 5. doi:10.1109/\nEORSA.2018.8598546\nHendrycks, D., and Gimpel, K. (2020). Gaussian Error Linear Units (GELUs). arXiv\n[Preprint]. Available at: http://arxiv.org/abs/1606.08415 (Accessed December 6, 2021).\nLi, R., Liu, W., Yang, L., Sun, S., Hu, W., Zhang, F., et al. (2018). DeepUNet: A Deep Fully\nConvolutional Network for Pixel-Level Sea-Land Segmentation.IEEE J. Sel. Top. Appl.\nEarth Obs. Remote Sensing11, 3954– 3962. doi:10.1109/JSTARS.2018.2833382\nLin, L., Pan, Z., Xiao, K., and Ye, N. (2013).“The Coastline Extraction for Fujian\nProvince Based on Long Time Series of Remote Sensing Image,” in Proceedings\nof the 2013 International Conference on Remote Sensing,Environment and\nTransportation Engineering, Nanjing, China, July 26– 28, 2013, 63– 66.\nLin, H., Shi, Z., and Zou, Z. (2017). Maritime Semantic Labeling of Optical Remote\nSensing Images with Multi-Scale Fully Convolutional Network.Remote Sensing\n9, 480. doi:10.3390/rs9050480\nLiu, Y., Huang, H., Qiu, Z., and Fan, J. (2013). Detecting Coastline Change from\nSatellite Images Based on beach Slope Estimation in a Tidal Flat.Int. J. Appl.\nEarth Obs. Geoinf.23, 165– 176. doi:10.1016/j.jag.2012.12.005\nLiu C, C., Yang, J., Yin, J., and An, W. (2016). Coastline Detection in SAR Images\nUsing a Hierarchical Level Set Segmentation.IEEE J. Sel. Top. Appl. Earth Obs.\nRemote Sensing 9, 4908– 4920. doi:10.1109/JSTARS.2016.2613279\nLiu Z, Z., Li, F., Li, N., Wang, R., and Zhang, H. (2016). A Novel Region-Merging\nApproach for Coastline Extraction from Sentinel-1A IW Mode SAR Imagery.\nIEEE Geosci. Remote Sensing Lett.13, 1– 5. doi:10.1109/LGRS.2015.2510745\nLiu, C., Xiao, Y., and Yang, J. (2017). A Coastline Detection Method in Polarimetric\nSAR Images Mixing the Region-Based and Edge-Based Active Contour Models.\nIEEE Trans. Geosci. Remote Sensing55, 3735– 3747. doi:10.1109/TGRS.2017.\n2679112\nLiu, X.-Y., Jia, R.-S., Liu, Q.-M., Zhao, C.-Y., and Sun, H.-M. (2019). Coastline\nExtraction Method Based on Convolutional Neural Networks-A Case Study of\nJiaozhou Bay in Qingdao, China.IEEE Access7, 180281– 180291. doi:10.1109/\nACCESS.2019.2959662\nLiu W, W., Chen, X., Ran, J., Liu, L., Wang, Q., Xin, L., et al. (2021). LaeNet: A\nNovel Lightweight Multitask CNN for Automatically Extracting Lake Area and\nShoreline from Remote Sensing Images.Remote Sensing 13, 56. doi:10.3390/\nrs13010056\nL i uZ ,Z . ,L i n ,Y . ,C a o ,Y . ,H u ,H . ,W e i ,Y . ,Z h a n g ,Z . ,e ta l .( 2 0 2 1 ) .S w i nT r a n s f o r m e r :\nHierarchical Vision Transformer Using Shifted Windows. arXiv [Preprint].\nAvailable at: http://arxiv.org/abs/2103.14030 (Accessed October 20, 2021).\nMapbox (2021). Available at: https://www.mapbox.com/ (Accessed December 7, 2021).\nMicrosoft (2021). Bing Maps Imagery API. Available at: https://docs.microsoft.\ncom/en-us/bingmaps/rest-services/imagery (Accessed December 7, 2021).\nMMSegmentation (2020). MMSegmentation: OpenMMLab Semantic\nSegmentation Toolbox and Benchmark. Available at: https://github.com/\nopen-mmlab/mmsegmentation (Accessed December 7, 2021).\nModava, M., and Akbarizadeh, G. (2017). Coastline Extraction from SAR Images\nUsing Spatial Fuzzy Clustering and the Active Contour Method.Int. J. Remote\nSens. 38, 355– 370. doi:10.1080/01431161.2016.1266104\nNunziata, F., Buono, A., Migliaccio, M., and Benassai, G. (2016). Dual-Polarimetric\nC- and X-Band SAR Data for Coastline Extraction.IEEE J. Sel. Top. Appl. Earth\nObs. Remote Sensing9, 4921– 4928. doi:10.1109/JSTARS.2016.2560342\nParavolidakis, V., Moirogiorgou, K., Ragia, L., Zervakis, M., and Synolakis, C.\n(2016). “Coastline Extraction from Aerial Images Based on Edge Detection,” in\nXXIII Congress of International Society for Photogrammetry and Remote\nSensing (ISPRS 2016), Prague, Czech Republic, July 12– 19, 2016, 153– 158.\ndoi:10.5194/isprsannals-III-8-153-2016\nParavolidakis, V., Ragia, L., Moirogiorgou, K., and Zervakis, M. (2018). Automatic\nCoastline Extraction Using Edge Detection and Optimization Procedures.\nGeosciences 8, 407. doi:10.3390/geosciences8110407\nPaszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., et al. (2017).\n“Automatic Differentiation in PyTorch, ” in 31st Conference on Neural\nInformation Processing Systems (NIPS 2017), Long Beach, CA.\nRasuly, A., Naghdifar, R., and Rasoli, M. (2010). Monitoring of Caspian Sea\nCoastline Changes Using Object-Oriented Techniques.Proced. Environ. Sci.2,\n416– 426. doi:10.1016/j.proenv.2010.10.046\nRigos, A., Tsekouras, G. E., Vousdoukas, M. I., Chatzipavlis, A., and Velegrakis, A.\nF. (2016). A Chebyshev Polynomial Radial Basis Function Neural Network for\nAutomated Shoreline Extraction from Coastal Imagery. ICA 23, 141– 160.\ndoi:10.3233/ICA-150507\nRoelfsema, C., Kovacs, E. M., Saunders, M. I., Phinn, S., Lyons, M., and Maxwell, P.\n(2013). Challenges of Remote Sensing for Quantifying Changes in Large\nComplex Seagrass Environments. Estuarine Coastal Shelf Sci. 133, 161– 171.\ndoi:10.1016/j.ecss.2013.08.026\nShamsolmoali, P., Zareapoor, M., Wang, R., Zhou, H., and Yang, J. (2019). A Novel\nDeep Structure U-Net for Sea-Land Segmentation in Remote Sensing Images.\nIEEE J. Sel. Top. Appl. Earth Obs. Remote Sensing12, 3219– 3232. doi:10.1109/\nJSTARS.2019.2925841\nSun, B., Li, S., and Xie, J. (2019).“Sea-Land Segmentation for Harbour Images with\nSuperpixel CRF,” in IGARSS 2019 - 2019 IEEE International Geoscience and\nRemote Sensing Symposium, Yokohama, Japan, July 28 – August 2, 2019,\n3899– 3902. doi:10.1109/IGARSS.2019.8899001\nToure, S., Diop, O., Kpalma, K., and Maiga, A. S. (2018).“Coastline Detection\nUsing Fusion of over Segmentation and Distance Regularization Level Set\nEvolution,” in The International Archives of the Photogrammetry, Remote\nSensing and Spatial Information Sciences\n(Istanbul, Turkey: Copernicus\nGmbH), 513– 518. doi:10.5194/isprs-archives-XLII-3-W4-513-2018\nTouvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jegou, H. (2021).\n“Training Data-Ef ﬁcient Image Transformers & Distillation Through\nAttention,” in Proceedings of the 38th International Conference on Machine\nLearning (PMLR), Virtual Event, July 18– 24, 2021, 10347– 10357. Available at:\nhttps://proceedings.mlr.press/v139/touvron21a.html (Accessed October 20,\n2021).\nT s e k o u r a s ,G .E . ,T r y g o n i s ,V . ,M a n i a t o p o u l o s ,A . ,R i g o s ,A . ,C h a t z i p a v l i s ,A . ,\nTsimikas, J., et al. (2018). A Hermi te Neural Network Incorporating\nArti ﬁcial Bee Colony Optimization to Model Shoreline Realignment at\na Reef-Fronted beach. Neurocomputing 280, 32– 45. doi:10.1016/j.neucom.\n2017.07.070\nVos, K., Splinter, K. D., Harley, M. D., Simmons, J. A., and Turner, I. L. (2019).\nCoastSat: A Google Earth Engine-Enabled Python Toolkit to Extract Shorelines\nfrom Publicly Available Satellite Imagery.Environ. Model. Softw.122, 104528.\ndoi:10.1016/j.envsoft.2019.104528\nWang, D., and Liu, X. (2019). Coastline Extraction from SAR Images Using Robust\nRidge Tracing.Mar. Geodesy42, 286– 315. doi:10.1080/01490419.2019.1583147\nWang, C., Yang, J., Li, J., and Chu, J. (2020). Deriving Natural Coastlines Using\nMultiple Satellite Remote Sensing Images.J. Coastal Res.102, 296– 302. doi:10.\n2112/SI102-036.1\nWang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., et al. (2021). Pyramid\nVision Transformer: A Versatile Backbone for Dense Prediction without\nConvolutions. arXiv [Preprint]. Available at: http://arxiv.org/abs/2102.12122\n(Accessed October 20, 2021).\nWernette, P., Houser, C., and Bishop, M. P. (2016). An Automated Approach for\nExtracting Barrier Island Morphology from Digital Elevation Models.\nGeomorphology 262, 1– 7. doi:10.1016/j.geomorph.2016.02.024\nWidyantara, I. M. O., Wirastuti, N. M. A. E. D., and Asana, I. M. D. P. (2017).\n“Gamma Correction-Based Image Enhancement and Canny Edge Detection for\nShoreline Extraction from Coastal Imagery, ” in 2017 1st International\nConference on Informatics and Computational Sciences (ICICoS),\nSemarang, Central Java, Indonesia, November 15– 16, 2017, 17– 22. doi:10.\n1109/icicos.2017.8276331\nFrontiers in Environmental Science | www.frontiersin.org February 2022 | Volume 10 | Article 79925011\nYang et al. Waterline Extraction With Vision Transformers\nW u ,X . ,L i u ,C . ,a n dW u ,G .( 2 0 1 8 ) .S p a t i a l - T e m p o r a lA n a l y s i sa n dS t a b i l i t yI n v e s t i g a t i o n\nof Coastline Changes: A Case Study in Shenzhen, China.IEEE J. Sel. Top. Appl. Earth\nObs. Remote Sensing11, 45– 56. doi:10.1109/JSTARS.2017.2755444\nXie, E., Wang, W., Yu, Z., Anandkumar, A., Alvarez, J. M., and Luo, P. (2021).\nSegFormer: Simple and Ef ﬁcient Design for Semantic Segmentation with\nTransformers. arXiv [Preprint]. Available at: http://arxiv.org/abs/2105.15203\n(Accessed October 20, 2021).\nXu, W., Xu, Y., Chang, T., and Tu, Z. (2021). Co-Scale Conv-Attentional Image\nTransformers. arXiv [Preprint]. Available at: http://arxiv.org/abs/2104.06399\n(Accessed October 20, 2021).\nYang, C.-S., Park, J.-H., and Rashid, A. H.-A. (2018). An Improved Method of Land\nMasking for Synthetic Aperture Radar-Based Ship Detection.J. Navigation71,\n788– 804. doi:10.1017/S037346331800005X\nZ h e n g ,S . ,L u ,J . ,Z h a o ,H . ,Z h u ,X . ,L u o ,Z . ,W a n g ,Y . ,e ta l .( 2 0 2 1 ) .R e t h i n k i n gS e m a n t i c\nSegmentation from a Sequence-To-Sequence Perspective with Transformers. arXiv\n[ P r e p r i n t ] .A v a i l a b l ea t :h t t p : / / a r x i v . o r g/abs/2012.15840 (Accessed August 12, 2021).\nConﬂict of Interest:The authors declare that the research was conducted in the\nabsence of any commercial orﬁnancial relationships that could be construed as a\npotential conﬂict of interest.\nPublisher’s Note:All claims expressed in this article are solely those of the authors\nand do not necessarily represent those of their afﬁliated organizations, or those of\nthe publisher, the editors, and the reviewers. Any product that may be evaluated in\nthis article, or claim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nCopyright © 2022 Yang, Wang and Zhai. This is an open-access article distributed\nunder the terms of the Creative Commons Attribution License (CC BY). The use,\ndistribution or reproduction in other forums is permitted, provided the original\nauthor(s) and the copyright owner(s) are credited and that the original publication\nin this journal is cited, in accordance with accepted academic practice. No use,\ndistribution or reproduction is permitted which does not comply with these terms.\nFrontiers in Environmental Science | www.frontiersin.org February 2022 | Volume 10 | Article 79925012\nYang et al. Waterline Extraction With Vision Transformers",
  "topic": "Waterline",
  "concepts": [
    {
      "name": "Waterline",
      "score": 0.964972972869873
    },
    {
      "name": "Computer science",
      "score": 0.5860069990158081
    },
    {
      "name": "Segmentation",
      "score": 0.5683701634407043
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5526387095451355
    },
    {
      "name": "Transformer",
      "score": 0.5394189953804016
    },
    {
      "name": "Computer vision",
      "score": 0.40095803141593933
    },
    {
      "name": "Remote sensing",
      "score": 0.32402193546295166
    },
    {
      "name": "Geography",
      "score": 0.25079792737960815
    },
    {
      "name": "Engineering",
      "score": 0.20120477676391602
    },
    {
      "name": "Marine engineering",
      "score": 0.14003115892410278
    },
    {
      "name": "Electrical engineering",
      "score": 0.07284209132194519
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Hull",
      "score": 0.0
    }
  ]
}