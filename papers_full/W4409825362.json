{
  "title": "Large language models for intelligent RDF knowledge graph construction: results from medical ontology mapping",
  "url": "https://openalex.org/W4409825362",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2586141264",
      "name": "Apostolos Mavridis",
      "affiliations": [
        "Aristotle University of Thessaloniki"
      ]
    },
    {
      "id": "https://openalex.org/A2017622080",
      "name": "Stergios Tegos",
      "affiliations": [
        "Aristotle University of Thessaloniki"
      ]
    },
    {
      "id": "https://openalex.org/A2184719347",
      "name": "Christos Anastasiou",
      "affiliations": [
        "Aristotle University of Thessaloniki"
      ]
    },
    {
      "id": "https://openalex.org/A2760005194",
      "name": "Maria Papoutsoglou",
      "affiliations": [
        "Aristotle University of Thessaloniki"
      ]
    },
    {
      "id": "https://openalex.org/A2310061145",
      "name": "Georgios Meditskos",
      "affiliations": [
        "Aristotle University of Thessaloniki"
      ]
    },
    {
      "id": "https://openalex.org/A2586141264",
      "name": "Apostolos Mavridis",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2017622080",
      "name": "Stergios Tegos",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2184719347",
      "name": "Christos Anastasiou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2760005194",
      "name": "Maria Papoutsoglou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2310061145",
      "name": "Georgios Meditskos",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2084124103",
    "https://openalex.org/W6683529373",
    "https://openalex.org/W6639031244",
    "https://openalex.org/W2036469985",
    "https://openalex.org/W3136643193",
    "https://openalex.org/W2945022921",
    "https://openalex.org/W2599384653",
    "https://openalex.org/W6786286154",
    "https://openalex.org/W3013983520",
    "https://openalex.org/W2077217970",
    "https://openalex.org/W4396870811",
    "https://openalex.org/W109317495",
    "https://openalex.org/W6864885487",
    "https://openalex.org/W1964538083",
    "https://openalex.org/W2790401843",
    "https://openalex.org/W6890022222",
    "https://openalex.org/W2889294310",
    "https://openalex.org/W3003265726",
    "https://openalex.org/W6868286370",
    "https://openalex.org/W2980047958",
    "https://openalex.org/W2593429823",
    "https://openalex.org/W6852800763",
    "https://openalex.org/W2979564631",
    "https://openalex.org/W2275216626",
    "https://openalex.org/W6862941916",
    "https://openalex.org/W2214382539",
    "https://openalex.org/W4394774863",
    "https://openalex.org/W6775029156",
    "https://openalex.org/W2811446355",
    "https://openalex.org/W3035075081",
    "https://openalex.org/W4391855109",
    "https://openalex.org/W4396669430",
    "https://openalex.org/W3009017335",
    "https://openalex.org/W4385584410",
    "https://openalex.org/W3210179814",
    "https://openalex.org/W6852105821",
    "https://openalex.org/W2111607948",
    "https://openalex.org/W6847020018",
    "https://openalex.org/W6713400116",
    "https://openalex.org/W2890036314",
    "https://openalex.org/W4396242252",
    "https://openalex.org/W6851775633",
    "https://openalex.org/W4391765649",
    "https://openalex.org/W6775906995",
    "https://openalex.org/W4375959300",
    "https://openalex.org/W3104147584",
    "https://openalex.org/W4376864921",
    "https://openalex.org/W1857126957",
    "https://openalex.org/W3107870069",
    "https://openalex.org/W4406870927",
    "https://openalex.org/W4385573928",
    "https://openalex.org/W2161851024",
    "https://openalex.org/W4392822529",
    "https://openalex.org/W2990714382",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W2402236473",
    "https://openalex.org/W4399062282",
    "https://openalex.org/W3012955422"
  ],
  "abstract": "The exponential growth of digital data, particularly in specialized domains like healthcare, necessitates advanced knowledge representation and integration techniques. RDF knowledge graphs offer a powerful solution, yet their creation and maintenance, especially for complex medical ontologies like Systematized Nomenclature of Medicine - Clinical Terms (SNOMED CT), remain challenging. Traditional methods often struggle with the scale, heterogeneity, and semantic complexity of medical data. This paper introduces a methodology leveraging the contextual understanding and reasoning capabilities of Large Language Models (LLMs) to automate and enhance medical ontology mapping for Resource Description Framework (RDF) knowledge graph construction. We conduct a comprehensive comparative analysis of six systems–GPT-4o, Claude 3.5 Sonnet v2, Gemini 1.5 Pro, Llama 3.3 70B, DeepSeek R1, and BERTMap—using a novel evaluation framework that combines quantitative metrics (precision, recall, and F1-score) with qualitative assessments of semantic accuracy. Our approach integrates a data preprocessing pipeline with an LLM-powered semantic mapping engine, utilizing BioBERT embeddings and ChromaDB vector database for efficient concept retrieval. Experimental results on a dataset of 108 medical terms demonstrate the superior performance of modern LLMs, particularly GPT-4o, achieving a precision of 93.75% and an F1-score of 96.26%. These findings highlight the potential of LLMs in bridging the gap between structured medical data and semantic knowledge representation, toward more accurate and interoperable medical knowledge graphs.",
  "full_text": "TYPE Original Research\nPUBLISHED /two.tnum/five.tnum April /two.tnum/zero.tnum/two.tnum/five.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/four.tnum/six.tnum/one.tnum/seven.tnum/nine.tnum\nOPEN ACCESS\nEDITED BY\nAntonio Sarasa-Cabezuelo,\nComplutense University of Madrid, Spain\nREVIEWED BY\nKausik Basak,\nJIS Institute of Advanced Studies and\nResearch, India\nCatherine Kosten,\nZurich University of Applied Sciences,\nSwitzerland\n*CORRESPONDENCE\nGeorgios Meditskos\ngmeditsk@csd.auth.gr\nRECEIVED /one.tnum/six.tnum December /two.tnum/zero.tnum/two.tnum/four.tnum\nACCEPTED /zero.tnum/seven.tnum April /two.tnum/zero.tnum/two.tnum/five.tnum\nPUBLISHED /two.tnum/five.tnum April /two.tnum/zero.tnum/two.tnum/five.tnum\nCITATION\nMavridis A, Tegos S, Anastasiou C,\nPapoutsoglou M and Meditskos G (/two.tnum/zero.tnum/two.tnum/five.tnum)\nLarge language models for intelligent RDF\nknowledge graph construction: results from\nmedical ontology mapping.\nFront. Artif. Intell./eight.tnum:/one.tnum/five.tnum/four.tnum/six.tnum/one.tnum/seven.tnum/nine.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/four.tnum/six.tnum/one.tnum/seven.tnum/nine.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/five.tnum Mavridis, Tegos, Anastasiou,\nPapoutsoglou and Meditskos. This is an\nopen-access article distributed under the\nterms of the\nCreative Commons Attribution\nLicense (CC BY) . The use, distribution or\nreproduction in other forums is permitted,\nprovided the original author(s) and the\ncopyright owner(s) are credited and that the\noriginal publication in this journal is cited, in\naccordance with accepted academic practice.\nNo use, distribution or reproduction is\npermitted which does not comply with these\nterms.\nLarge language models for\nintelligent RDF knowledge graph\nconstruction: results from\nmedical ontology mapping\nApostolos Mavridis, Stergios Tegos, Christos Anastasiou,\nMaria Papoutsoglou and Georgios Meditskos *\nSchool of Informatics, Aristotle University of Thessaloniki, Thes saloniki, Greece\nThe exponential growth of digital data, particularly in speciali zed domains like\nhealthcare, necessitates advanced knowledge representation an d integration\ntechniques. RDF knowledge graphs oﬀer a powerful solution, yet their creation\nand maintenance, especially for complex medical ontologies like Systematized\nNomenclature of Medicine - Clinical Terms (SNOMED CT), remain chall enging.\nTraditional methods often struggle with the scale, heterogene ity, and semantic\ncomplexity of medical data. This paper introduces a methodolog y leveraging the\ncontextual understanding and reasoning capabilities of Large Language Models\n(LLMs) to automate and enhance medical ontology mapping for Res ource\nDescription Framework (RDF) knowledge graph construction. We cond uct a\ncomprehensive comparative analysis of six systems–GPT-/four.tnumo, Claude /three.tnum./five.tnum Sonnet\nv/two.tnum, Gemini /one.tnum./five.tnum Pro, Llama /three.tnum./three.tnum /seven.tnum/zero.tnumB, DeepSeek R/one.tnum, and BERTMap—using a novel\nevaluation framework that combines quantitative metrics (pre cision, recall, and\nF/one.tnum-score) with qualitative assessments of semantic accuracy. Our approach\nintegrates a data preprocessing pipeline with an LLM-powered se mantic\nmapping engine, utilizing BioBERT embeddings and ChromaDB v ector database\nfor eﬃcient concept retrieval. Experimental results on a datase t of /one.tnum/zero.tnum/eight.tnum medical\nterms demonstrate the superior performance of modern LLMs, p articularly\nGPT-/four.tnumo, achieving a precision of /nine.tnum/three.tnum./seven.tnum/five.tnum% and an F/one.tnum-score of /nine.tnum/six.tnum./two.tnum/six.tnum%. These\nﬁndings highlight the potential of LLMs in bridging the gap b etween structured\nmedical data and semantic knowledge representation, toward mo re accurate\nand interoperable medical knowledge graphs.\nKEYWORDS\nLLM, ontology, knowledge graph, health data, RDF, SNOMED CT\n/one.tnum Introduction\nThe accelerating digitization of information across all sectors has created an\nunprecedented surge in the volume, velocity, and variety of data ( Venkatachaliah, 2011 ;\nChakraborty et al., 2017 ). This “data deluge” presents signiﬁcant challenges for traditional\ndata management and integration techniques, which often struggle to eﬀectively handle\nthe complexity and scale of modern datasets (\nNashipudimath et al., 2020 ). This challenge\nbecomes even more evident in ﬁelds such as healthcare, where vast amounts of data\nmust be managed. The complexity arises from the diversity of formats, the nuanced\ninterconnections within the data, and the presence of sensitive information. As a result,\nthe demand for scalable, resilient, and semantically enriched methods for representing and\nintegrating knowledge has never been greater.\nFrontiers in Artiﬁcial Intelligence /zero.tnum/one.tnum frontiersin.org\nMavridis et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/four.tnum/six.tnum/one.tnum/seven.tnum/nine.tnum\nThe Resource Description Framework (RDF) knowledge\ngraphs have emerged as a paradigm for addressing these challenges\n(\nWu and Banerjee, 2014 ). RDF oﬀers a ﬂexible and expressive\nframework for representing interconnected data in a machine-\nreadable format. By utilizing Uniform Resource Identiﬁers (URIs)\nto uniquely identify entities and deﬁning relationships between\nthem, RDF graphs enable the creation of semantically rich\nrepresentations that facilitate advanced querying, reasoning and\nanalysis (\nZou, 2020). The visual nature of knowledge graphs further\nenhances their utility, providing intuitive means for exploring and\ninterpreting datasets (\nSayed Ahmed Soliman and Tabak, 2020 ).\nThese characteristics make RDF knowledge graphs particularly\nsuited for applications that require sophisticated data integration,\nsuch as drug discovery, personalized medicine, and clinical decision\nsupport systems.\nDespite the advantages of RDF, several key challenges hinder\nthe widespread adoption and eﬀective utilization of knowledge\ngraphs. The construction and maintenance of large-scale RDF\ngraphs often require signiﬁcant manual eﬀort, particularly for\nontology mapping and data integration (\nSingh et al., 2023 ).\nConventional approaches to constructing knowledge graphs,\nincluding manual curation and rule-based techniques, often face\nchallenges in scaling eﬀectively to accommodate the continuous\nexpansion of data. Additionally, interacting with RDF databases\nthrough SPARQL can be daunting for users unfamiliar with\nsuch technologies (\nHan et al., 2015 ). This diﬃculty reduces the\naccessibility of RDF knowledge graphs and hinders their broader\npractical adoption.\nMapping structured data, especially from common formats\nlike CSV ﬁles, to RDF presents speciﬁc obstacles (\nChaves-Fraga\net al., 2020 ). Domain-speciﬁc terminology, abbreviations, and\ninconsistent data formats require signiﬁcant preprocessing and\ninterpretation to ensure accurate semantic representation within\nthe RDF graph. Numeric data, prevalent in many datasets, needs\ncareful contextualization and alignment with relevant ontologies, as\nraw numbers lack the inherent symbolic meaning crucial for RDF’s\nsemantic expressiveness. These challenges are particularly evident\nin the medical domain, where data is often highly structured but\nrequires extensive domain knowledge for accurate mapping to\nestablished medical ontologies like SNOMED CT.\nThe recent advent of Large Language Models (LLMs)\nhas revolutionized the ﬁeld of natural language processing,\noﬀering promising new solutions for knowledge representation\nand semantic integration (\nXue, 2024 ; Kulkarni, 2023 ). Trained\non massive text corpora, LLMs like GPT-4o, Claude, and\nGemini possess remarkable capabilities in understanding context,\ndisambiguating terminology, and inferring relationships within\ntext (\nRaiaan et al., 2024 ). Their ability to process and generate\nhuman-like text has opened up new possibilities for automating\ncomplex tasks, including knowledge graph construction, ontology\nmapping, and semantic enrichment (\nKommineni et al., 2024 ;\nTrajanoska et al., 2023 ; Jia et al., 2024 ).\nThis paper introduces a novel methodology that harnesses\nthe power of LLMs to address the persistent challenges in\ncreating and utilizing RDF knowledge graphs, particularly\nin the context of medical ontology mapping. Our approach\noperates on two interconnected levels. First, we implement a\nrobust data preprocessing pipeline that addresses the inherent\nheterogeneity and ambiguity of real-world medical data.\nThis pipeline incorporates techniques such as terminology\nnormalization, abbreviation expansion, and unit standardization,\nensuring that the input data is consistent and amenable to\nsemantic interpretation.\nSecond, we leverage the contextual understanding and\nreasoning capabilities of LLMs to perform intelligent ontology\nmapping. This involves formulating targeted prompts designed\nto elicit speciﬁc mappings between medical terms and concepts\nwithin the SNOMED CT ontology. To optimize retrieval and\ncomparison of semantic representations, we employ a vector\ndatabase (ChromaDB) populated with pre-computed BioBERT\nembeddings of both the input medical terms and the SNOMED CT\nconcepts. This allows us to eﬃciently identify the most semantically\nsimilar concepts within the ontology based on cosine similarity\nbetween the embedding vectors. The integration of these two\ncomponents—a robust data preprocessing pipeline and an LLM-\npowered semantic mapping engine—facilitates the automated\ngeneration of context-aware RDF triples, capturing the rich\nrelationships embedded within the medical data and aligning them\nwith the established semantic framework of SNOMED CT. This\napproach builds upon recent advancements in cloud-based RDF\nstores and distributed graph processing (\nJanke and Staab, 2018 ),\nenabling scalable and eﬃcient knowledge graph construction. We\nspeciﬁcally focus on the medical domain, demonstrating how\nLLMs, combined with a tailored preprocessing strategy and vector\ndatabase integration, can be eﬀectively employed to map complex\nmedical terminology and structured data to the SNOMED CT\nontology. This work aims to bridge the gap between structured data\nand semantic knowledge representation, facilitating the creation\nof more accurate, comprehensive, and interoperable medical\nknowledge graphs.\nOur key contributions are threefold. First, we present a\ncomprehensive comparative analysis of six distinct systems—\nGPT-4o, Claude 3.5 Sonnet v2, Gemini 1.5 Pro, Llama 3.3 70B,\nDeepSeek R1, and BERTMap—for medical ontology mapping and\nRDF knowledge graph construction. This analysis examines the\nadvantages and drawbacks of each method, providing insights for\nresearchers and practitioners looking to utilize LLMs in knowledge\ngraph construction. Additionally, we propose a new evaluation\nframework for measuring the eﬀectiveness of language models\nin mapping medical terminology. This framework integrates\nboth quantitative metrics and qualitative evaluations of semantic\naccuracy, ensuring a comprehensive and rigorous assessment.\nLastly, we showcase the enhanced capabilities of modern LLMs,\nparticularly GPT-4o, in processing intricate medical concepts\nand relationships. Our ﬁndings indicate substantial advancements\nover the BERTMap baseline, with GPT-4o demonstrating a 44.91\npercentage point improvement in precision (93.75% vs. 48.84%)\nand a 38.33 percentage point increase in F1-score (96.26%\nvs. 57.93%), highlighting the potential of LLMs to automated\nknowledge graph construction. This study highlights the impact\nof combining LLMs with RDF knowledge graphs to develop\nmore intelligent, adaptive, and semantically enriched information\nsystems. It paves the way for harnessing the vast and intricate\nrealm of digital information to extract deeper insights and improve\nFrontiers in Artiﬁcial Intelligence /zero.tnum/two.tnum frontiersin.org\nMavridis et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/four.tnum/six.tnum/one.tnum/seven.tnum/nine.tnum\ndecision-making across multiple ﬁelds, which marks a crucial\nadvancement in reshaping how we access, integrate, and manage\ninformation in complex settings (\nLenzerini, 2018).\nThe remainder of this paper is structured as follows: Section 2\nreviews related work in RDF knowledge graphs, ontology mapping,\nand the application of LLMs in semantic web technologies. Section\n3 details our methodology, including the system architecture,\ntesting framework, and evaluation metrics. Section 4 presents the\nexperimental results, providing a detailed performance analysis of\nthe evaluated systems. Finally, Section 5 discusses the ﬁndings,\nhighlights the implications of our work, and outlines future\nresearch directions.\n/two.tnum Related work\nThe Resource Description Framework (RDF) serves as a\ncornerstone of the Semantic Web, enabling structured data sharing\non a global scale. As\nBerners-Lee (2019) explains, RDF extends\nthe hypertext Web by using URIs to identify and describe\nresources, employing various serializations for data exchange. Its\nfocus on data content meaning rather than structure makes it\nparticularly suitable as a semantic data model for cloud computing\n(\nKanmani et al., 2017 ). However, the growing size of RDF datasets\npresents signiﬁcant challenges for data management systems,\nnecessitating eﬃcient storage techniques, indexing strategies, and\nquery execution mechanisms (\nWylot et al., 2018 ).\nSPARQL, the standard query language for RDF, has\nmotivated extensive theoretical studies of RDF and SPARQL\nfundamentals (\nArenas et al., 2013 ). While RDF data can be handled\nusing relational tables, querying large triple tables becomes\ncomputationally expensive due to the multiple nested joins\nrequired for graph queries (\nWylot et al., 2018 ). The increasing\navailability of RDF data on the Web underscores the urgent need to\naddress scalability issues in RDF data management (\nArenas et al.,\n2013).\nKnowledge graphs have emerged as powerful tools for\nrepresenting complex, interconnected information in a machine-\nreadable format. They are increasingly utilized across various\ndomains, including libraries, digital humanities, and explainable\nmachine learning (\nHaslhofer et al., 2018 ; Tiddi and Schlobach,\n2022). These graphs represent concepts and their semantic\nrelationships, supporting resource discovery, navigation, and\nvisualization (\nHaslhofer et al., 2018 ). Recent research has\nfocused on knowledge graph representation learning, acquisition,\ncompletion, and temporal aspects (\nJi et al., 2022 ). Embedding\nmethods have gained popularity, enabling various applications with\nimplicit semantics derived from context (\nKejriwal et al., 2019 ).\nThe process of generating RDF knowledge graphs from raw\ndata has shifted from manual eﬀorts to automated techniques.\nRecent approaches utilize existing ontologies, knowledge bases, and\nmachine learning models to derive structured insights from various\nsources, including research publications (\nConstantopoulos and\nPertsas, 2020 ). Advanced algorithms and tools, such as the SDM-\nRDFizer, have been designed to streamline the transformation\nof heterogeneous datasets into RDF representations (\nIglesias\net al., 2020 ). Additionally, real-time extraction of RDF triples\nfrom unstructured data streams has been explored through a\ncombination of statistical analysis and machine learning strategies\n(\nGerber et al., 2013 ).\nOntology mapping is essential for addressing heterogeneity\nchallenges and ensuring semantic interoperability across diverse\ninformation sources (\nBenslimane et al., 2008 ). It plays a key\nrole in knowledge graph construction by integrating multiple\nheterogeneous datasets (\nIglesias-Molina et al., 2019 ). Various\nmethods have been proposed, such as declarative mappings\nand language-independent templates using spreadsheets, which\nenhance maintainability and accessibility for non-experts (\nIglesias-\nMolina et al., 2019 ). Additionally, frameworks, such as MapSDI,\noptimize semantic data integration through pre-processing based\non mapping rules (\nJozashoori and Vidal, 2019 ).\nFurthermore, recent research has focused on enhancing\nthe semantic interpretation of structured data sources in\nprivacy-preserving environments.\nKaralka et al. (2023) propose\nSemCrypt, a framework for schema enrichment through\nsemantic annotations and mappings to knowledge bases and\nontologies, aiming to assess privacy-preserving technologies\nbased on data sensitivity. This approach builds upon earlier\nwork in semantic integration of heterogeneous data sources\n(\nBergamaschi et al., 1999 ) and semi-automatic mapping of\nstructured sources to ontologies ( Knoblock et al., 2012 ),\nwhich facilitate the creation of shared ontologies, semantic\nrelationships, and mapping rules for integrated data access.\nThe importance of privacy preservation in data analysis is\nemphasized by\nDwork (2011) . By integrating semantic analysis\nwith privacy-aware methodologies, researchers aim to create more\nsophisticated and intelligent strategies for managing sensitive\ndata across domains, such as healthcare, ﬁnance and cyber\nthreat intelligence.\nThe emergence of Large Language Models (LLMs) has\nsigniﬁcantly transformed natural language processing, showcasing\nexceptional performance in grasping context, meaning, and\nintricate relationships (\nXue, 2024 ). Evolving from rule-based\nmethods to highly advanced architectures, these models leverage\ntransformer-based frameworks and sophisticated training\nparadigms to execute a wide range of tasks, including text\ngeneration, sentiment analysis and question answering (\nKulkarni,\n2023; Xue, 2024 ). Their capabilities extend well beyond artiﬁcial\nintelligence, shaping advancements in diverse ﬁelds, such as\nmedicine, engineering, social sciences, and the humanities (\nFan\net al., 2024 ).\nRecent studies use Machine Learning in diﬀerent aspects\nof knowledge graph development and ontology alignment,\ndemonstrating potential in areas, such as entity learning, ontology\nlearning, and knowledge reasoning (\nZhao et al., 2023 ). They have\nbeen applied to key tasks, including entity extraction, relation\nextraction, entity linking and link prediction (\nZhao et al., 2023 ).\nMoreover, machine learning has been leveraged to automate\ndata preparation and cleaning for knowledge graph curation, as\nwell as data integration (\nBerti-Equille, 2019 ). Large Language\nModels (LLMs), in particular, show promise in streamlining data\nextraction and resolution processes for heterogeneous data sources\n(\nRemadi et al., 2024 ). For example, the LLMs4OM framework\ndemonstrates the eﬀectiveness of LLMs in ontology matching\ntasks, potentially surpassing traditional systems (\nGiglou et al.,\n2024). Additionally, integrating LLMs with scholarly knowledge\nFrontiers in Artiﬁcial Intelligence /zero.tnum/three.tnum frontiersin.org\nMavridis et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/four.tnum/six.tnum/one.tnum/seven.tnum/nine.tnum\nFIGURE /one.tnum\nLLM-based system architecture.\ngraphs enhances query processing, enabling comprehensive and\neﬃcient information retrieval from academic research artifacts\n(\nJia et al., 2024 ). In addition, transfer learning and pre-\ntrained language models have signiﬁcantly advanced natural\nlanguage processing tasks, with domain-speciﬁc pre-training\nemerging as a powerful technique to enhance performance on\nspecialized tasks (\nZhong and Goodfellow, 2024 ). In this context,\nmemory-augmented models have been proposed to address the\npotential loss of general knowledge during domain adaptation,\ncombining domain-speciﬁc learning with preserved general\nknowledge (\nWan et al., 2022 ).\nDespite these advancements, several challenges remain in\ninformation extraction and query processing. Examples include\nhandling ambiguity, scaling to large datasets and adapting to\ndomain-speciﬁc terminology. Recently proposed solutions, such as\nsystems to resolve query ambiguity using large-scale user behavioral\ndata (\nKorayem et al., 2015 ), models for automatic term ambiguity\ndetection ( Baldwin et al., 2013 ), and query relaxation approaches,\nleverage external knowledge sources ( Lei et al., 2020 ). These studies\ndemonstrate progress, highlighting the need for advancements\nin handling ambiguity, scalability and domain adaptation in\nknowledge representation and retrieval systems.\nWhile signiﬁcant progress has been made in RDF knowledge\ngraph construction, ontology mapping and the application\nof machine learning to these domains, the need for more\nsophisticated, context-aware approaches that can handle the\ncomplexity and nuance of real-world data remains. Our research\nis driven by this need and leverages recent advancements in LLMs\nfor semantically rich RDF knowledge graphs.\n/three.tnum Method\n/three.tnum./one.tnum Implementation\nOur study embarked on a meticulous comparative analysis\nto evaluate the eﬃcacy of six distinct computational systems in\nthe realm of medical ontology mapping and RDF knowledge\ngraph construction (see\nFigure 1). This endeavor was motivated\nby the critical need for accurate and scalable methods to process\nthe increasingly voluminous and complex data prevalent in\nmodern healthcare settings. The systems under scrutiny were\nGPT-4o, Claude 3.5 Sonnet v2, Gemini 1.5 Pro, Llama 3.3\n70B, DeepSeek R1 representing state-of-the-art Large Language\nModels (LLMs), and BERTMap, a well-established baseline system\nin ontology mapping. The central objective was to rigorously\nassess the potential of LLMs to surpass traditional methodologies\nin handling the intricacies of medical terminology, thereby\nfacilitating the creation of semantically rich and accurate RDF\nknowledge graphs.\nThe selection of LLMs for this paper was based on several\ncriteria, including their representation of both cutting-edge\nand open-source models, their proven eﬀectiveness in handling\ncomplex tasks, and their accessibility for ensuring reproducibility.\nAmong the state-of-the-art models, we included GPT-4o,\nClaude 3.5 Sonnet v2, and Gemini 1.5 Pro, developed by\nOpenAI, Anthropic, and Google, respectively. These models\nrepresent the forefront of LLM advancements, each with\ndistinct architectural strategies and strong performance in\ncomplex reasoning and domain-speciﬁc applications, while\ntheir stable API access facilitates experimental reproducibility.\nTo provide a more comprehensive analysis and acknowledge\nthe increasing signiﬁcance of open-source alternatives in\nhealthcare, we incorporated two high-performing open-source\nLLMs: Llama 3.3 70B from Meta, which boasts 70 billion\nparameters and excels across diverse tasks, and DeepSeek\nR1, a recently introduced open-source model demonstrating\npromising capabilities in language understanding and generation.\nBERTMap was selected as the baseline due to its well-established\nrole in ontology mapping research and its widespread use as a\nbenchmark system.\nImplementation parameters:\n• GPT-4o: Temperature = 0.1, max tokens = 2,048, cost =\n$0.03/1K tokens.\n• Claude 3.5 Sonnet v2: Temperature = 0.1, max tokens = 2048,\ncost = $0.015/1K tokens.\n• Gemini 1.5 Pro: Temperature = 0.1, max tokens = 2,048, cost\n= $0.01/1K tokens.\n• Llama 3.3 70B: Temperature = 0.1, max tokens = 2,048,\nindirect costs for setup.\n• DeepSeek R1: Temperature = 0.1, max tokens = 2,048, indirect\ncosts for setup.\n• BERTMap: Default parameters as per public implementation.\nFrontiers in Artiﬁcial Intelligence /zero.tnum/four.tnum frontiersin.org\nMavridis et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/four.tnum/six.tnum/one.tnum/seven.tnum/nine.tnum\nThe proposed framework in this paper is a key component\nof the ENCRYPT project, /one.tnumwhich focuses on addressing privacy\nand security challenges in critical sectors such as healthcare,\nﬁnance, and entertainment. Within the ENCRYPT framework,\nKnowledge Graphs (KGs) play a central role in enabling data\ninteroperability, semantic understanding, and eﬃcient information\nsharing across heterogeneous datasets. This work presents\na comprehensive approach to constructing semantically rich\nRDF Knowledge Graphs speciﬁcally tailored for healthcare\ndata. By leveraging advanced privacy-preserving technologies\nsuch as Fully Homomorphic Encryption (FHE), Secure Multi-\nParty Computation (SMPC), and Diﬀerential Privacy (DP), the\nENCRYPT project ensures that sensitive data can be securely\nmanaged and analyzed in compliance with GDPR regulations. Our\napproach integrates semantic web standards, utilizing ontologies\nsuch as SNOMED CT and DICOM to enable accurate semantic\nrepresentation of medical data. This paper highlights the full\nmethodology, combining LLM-powered ontology mapping, a\nrobust preprocessing pipeline, and vector database integration, to\ndemonstrate the transformative potential of Knowledge Graphs in\naddressing the complexities of healthcare data management.\nIn constructing our evaluation pipeline, a critical component\nwas the selection and implementation of a vector database to\neﬃciently manage and retrieve the embeddings representing our\nmedical terms ( Csource) and the corresponding concepts within the\nSNOMED CT ontology ( Ctarget). For this purpose, we employed\nChromaDB, an open-source vector database renowned for its ease\nof use, scalability, and seamless integration with natural language\nprocessing workﬂows. ChromaDB served as the central repository\nfor storing and indexing the vector representations generated for\nboth the input medical terms and the SNOMED CT concepts.\nTo generate these vector representations, we leveraged pre-\ntrained embedding models tailored for biomedical text. Speciﬁcally,\nwe utilized the BioBERT model, a domain-speciﬁc variant of BERT\ntrained on a large corpus of biomedical literature. BioBERT has\ndemonstrated superior performance in capturing the semantic\nnuances of medical terminology compared to general-purpose\nembedding models. Each medical term in our evaluation dataset\nand each concept in the relevant subset of SNOMED CT were\nprocessed through BioBERT to produce dense vector embeddings.\nThese embeddings encapsulated the semantic meaning of each term\nor concept, allowing for eﬃcient similarity comparisons within the\nvector space.\nFor the Large Language Models (LLMs)—GPT-4o, Claude\n3.5 Sonnet v2, Gemini 1.5 Pro, Llama 3.3 70B, and DeepSeek\nR1—we interacted with them via their respective API endpoints.\nThe API calls were programmatically managed using Python,\nfacilitating seamless integration with our evaluation framework.\nThe prompts presented to the LLMs were carefully engineered to\nguide them toward generating mappings to SNOMED CT concepts.\nSpeciﬁcally, we employed a structured prompt formatter (prompt\nengine) that included the medical term to be mapped, a clear\ninstruction to provide the corresponding SNOMED CT identiﬁer,\nand a request to provide a conﬁdence score for the proposed\nmapping. This structured approach ensured consistency in our\n/one.tnumhttps://encrypt-project.eu/\ninteractions with the LLMs and facilitated the extraction of the\nrelevant information from their responses.\nThe responses obtained from the LLMs were then parsed\nprogrammatically to extract the proposed SNOMED CT identiﬁers\nand conﬁdence scores. To identify the most relevant concepts,\nwe performed a nearest neighbor search within the ChromaDB\nvector store. The vector embedding of the input medical term,\nas generated by BioBERT, was used as the query vector. The\ndistance metric employed for the nearest neighbor search was\ncosine similarity, which measures the cosine of the angle between\ntwo vectors, providing a robust indicator of semantic similarity.\nThe top-k nearest neighbors, as determined by cosine similarity,\nwere retrieved from ChromaDB, representing the SNOMED CT\nconcepts most semantically similar to the input medical term.\nFor the baseline system, BERTMap, we utilized the publicly\navailable implementation and adhered to the recommended\nparameter settings. BERTMap operates by generating\ncontextualized word embeddings for both the source and target\nontologies (in our case, the input medical terms and SNOMED\nCT) and subsequently computes a similarity matrix based on\ncosine similarity. Alignment candidates are then identiﬁed through\na greedy matching algorithm.\nTo ensure the reproducibility of our results and to manage\nthe complexities of our experimental setup, we employed a\ncontainerization approach using Docker. Each component of\nour pipeline, including the vector database, the embedding\ngeneration scripts, the LLM interaction modules and the evaluation\nscripts, was encapsulated within a Docker container. This strategy\nensured consistency across diﬀerent environments and streamlined\nthe deployment of our evaluation framework. Additionally, we\nemployed a robust version control system via Git to systematically\ntrack code modiﬁcations, experimental parameters, and evaluation\noutcomes. This approach ensured detailed documentation of\nour methodology and streamlined collaboration among the\nresearchers involved. Beyond the core technological aspects,\nwe implemented thorough data preprocessing procedures to\nmaintain the quality and consistency of input data. These steps\nencompassed standardizing the formatting of medical terms,\nmanaging abbreviations and acronyms, and resolving terminology\ninconsistencies. All preprocessing procedures were carefully\ndocumented to enhance transparency and reproducibility.\nThroughout the evaluation process, we monitored the\nperformance of our system, paying close attention to computational\nresource utilization, processing times and potential bottlenecks.\nThis performance monitoring allowed us to identify areas for\noptimization and to ensure the scalability of our approach. The\ndatasets and prompts used for this study can be accessed at the\nGitHub repository.\n/two.tnum\n/three.tnum./two.tnum Evaluation\nTo conduct a robust and representative evaluation, we\nassembled a comprehensive dataset comprising 108 distinct\nmedical terms. This dataset was meticulously curated to reﬂect\n/two.tnumhttps://github.com/enchatted/llms-kgs\nFrontiers in Artiﬁcial Intelligence /zero.tnum/five.tnum frontiersin.org\nMavridis et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/four.tnum/six.tnum/one.tnum/seven.tnum/nine.tnum\nthe diverse spectrum of clinical information routinely encountered\nin healthcare, encompassing patient demographics, physiological\nmeasurements, disease classiﬁcations, therapeutic interventions,\nand intricate diagnostic and procedural terms. This diversity was\nparamount to ensuring that the evaluation probed the full breadth\nof the systems’ capabilities, challenging them with a range of\nlinguistic complexities from straightforward measurements like\nage and weight to nuanced concepts such as disease staging and\ntreatment withdrawal. The selection of these terms was informed\nby extensive consultation with domain experts, ensuring their\nrelevance and representativeness within the medical ﬁeld.\nPrior to the computational analysis, we established a deﬁnitive\nground truth to serve as the benchmark for evaluating the\nperformance of the six systems. This was achieved through a\nrigorous expert elicitation process involving a panel of seasoned\nmedical professionals and ontology engineers. Each expert\nindependently reviewed the dataset, proposing mappings for each\nmedical term to corresponding concepts within the SNOMED CT\nontology. The selection of SNOMED CT, a globally recognized\ncomprehensive medical ontology, was deliberate, given its extensive\ncoverage of clinical concepts and its pivotal role in enabling\ninteroperability in healthcare information systems. Following the\nindividual review, a collaborative session was convened wherein\nthe experts discussed discrepancies, negotiated disagreements, and\nultimately reached a consensus on the most appropriate mappings\nfor each term. This meticulous consensus-building process was\nparticularly vital for terms exhibiting ambiguity or multiple valid\ninterpretations, reﬂecting the complex and multifaceted nature\nof medical language. The resultant expert-validated ground truth\nprovided a reliable standard against which the computational\nsystems’ performance could be measured.\nWhile medical ontologies, like SNOMED CT, involve nuanced\nrelationships, temporal data, and hierarchical structures, the dataset\nused in our study focused speciﬁcally on real-world terms extracted\nfrom patient data within a clinical setting. This focus inherently\nsimpliﬁed the mapping task. The most frequent relationships\nwere those directly associated with a patient (e.g., “patient has\ncondition X”). The temporal scope primarily involved the patient’s\ncurrent condition and history, limiting the complexity of temporal\nrelationships. Additionally, the terms used often focused on speciﬁc\ndiagnoses, procedures, and medications, resulting in a relatively\nﬂattened hierarchy for the mapping task. In essence, the dataset\nreﬂected a practical use case where the complexities of SNOMED\nCT were naturally constrained by the context of patient data. This\nallowed us to focus on evaluating the models’ ability to accurately\nmap real-world medical terms to SNOMED CT concepts within\nthis speciﬁc domain.\nWith the ground truth established, we proceeded to\nsystematically evaluate the six computational systems under\ncontrolled conditions. Each of the 108 medical terms was presented\nas input to GPT-4o, Claude 3.5 Sonnet v2, Gemini 1.5 Pro, Llama\n3.3 70B, DeepSeek R1, and BERTMap. For the LLMs, we employed\ntheir default model conﬁgurations without any task-speciﬁc\nﬁne-tuning, simulating a real-world scenario where researchers\nmight utilize these models out-of-the-box. The prompts provided\nto these models were carefully designed to elicit mappings to\nSNOMED CT concepts, framed in a clear and unambiguous\nmanner. BERTMap, serving as the baseline, was executed using its\nstandard parameter settings, representing the typical usage of this\nsystem within the ontology mapping community. To eliminate\npotential bias during the evaluation process, we implemented a\nblind validation protocol. The mappings generated by each system\nwere presented to the evaluators without disclosing the originating\nsystem, thereby preventing any subconscious inﬂuence on the\njudgment of the results’ quality.\nThe quantitative evaluation of the systems’ performance was\nbased on standard classiﬁcation metrics: precision, recall, and F1-\nscore. These metrics were derived from a comprehensive confusion\nmatrix, categorizing each mapping outcome as a true positive\n(TP), false positive (FP), false negative (FN), or true negative\n(TN). Precision measured the proportion of correctly identiﬁed\nmappings out of all mappings proposed by a system, while recall\nassessed the proportion of correctly identiﬁed mappings out of\nall possible correct mappings in the ground truth. The F1-score\n(the harmonic mean of precision and recall) provided a balanced\nmeasure of the system’s overall accuracy. Beyond these quantitative\nmetrics, we have also performed a qualitative analysis to assess\nthe semantic relevance, clinical appropriateness and contextual\naccuracy of the generated mappings. This analysis had an important\nrole in assessing the systems’ capability to process complex medical\nconcepts, including temporal relationships (e.g., symptom onset),\ndetailed anatomical descriptions and multi-step procedural terms.\nDomain experts conducted the review, examining the mappings\nnot only for their technical accuracy but also for their clinical\nrelevance and meaningfulness within the medical context.\nFurthermore, we applied a chi-square test of independence\nto determine whether the performance diﬀerences among the\nsix systems were statistically signiﬁcant. This test allowed us to\ndetermine whether the distribution of TP , FP , FN, and TN across\nthe systems deviated signiﬁcantly from what would be expected by\nchance. To quantify the magnitude of any detected associations,\nCramer’s V was calculated as a measure of eﬀect size. This\nanalytical approach that combines quantitative metrics, qualitative\nevaluation and statistical testing, provided a robust assessment of\nthe performance of GPT-4o, Claude 3.5 Sonnet v2, Gemini 1.5 Pro,\nLlama 3.3 70B, DeepSeek R1, and BERTMap in the task of medical\nontology mapping.\nFurthermore, we documented the speciﬁc challenges\nencountered by each system, including recurring error patterns\nand areas of ambiguity. This error analysis provided useful insights\ninto the inherent strengths and limitations of each approach.\nThe transparency and thoroughness of our methodology, from\ndataset construction to evaluation, were designed to ensure the\nreproducibility of our ﬁndings and to provide a solid foundation\nfor subsequent advancements in the ﬁeld of automated medical\nknowledge representation and semantic integration.\n/four.tnum Results\nOur evaluation comparing the LLMs and BERTMap revealed\nsubstantial diﬀerences in performance across multiple metrics.\nAnalysis of the 108 medical terms in our dataset demonstrated\nvarying levels of eﬀectiveness among the systems, with GPT-4o\nFrontiers in Artiﬁcial Intelligence /zero.tnum/six.tnum frontiersin.org\nMavridis et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/four.tnum/six.tnum/one.tnum/seven.tnum/nine.tnum\nFIGURE /two.tnum\nA comparison between the results of the baseline method and the t op performance LLM.\nTABLE /one.tnum Comparison of method performance metrics.\nMethod Precision Recall F/one.tnum-score\nGPT-4o 93.75 98.90 96.26\nClaude 3.5 Sonnet v2 53.75 69.35 60.56\nGemini 1.5 Pro 60.27 66.67 63.31\nBERTMap 48.84 71.19 57.93\nLlama 3.3 70B 19.19 70.37 30.16\nDeepSeek R1 25.76 32.69 28.81\nshowing superior overall performance. Figure 2 depicts an example\nof a comparison between the results of the baseline method and the\ntop performance LLM.\nGPT-4o achieved the highest precision (93.75%) among all\nsystems, signiﬁcantly outperforming Gemini 1.5 Pro (60.27%),\nClaude 3.5 Sonnet v2 (53.75%), BERTMap (48.84%), DeepSeek\nR1 (25.76%), and Llama 3.370B (19.19%). In terms of recall,\nGPT-4o led with 98.90%, followed by Llama 3.370B (70.37%),\nBERTMap (71.19%), Claude 3.5 Sonnet v2 (69.35%), Gemini 1.5\nPro (66.67%), and DeepSeek R1 (32.69%). The combined eﬀect\nof these metrics resulted in F1-scores of 96.26% for GPT-4o,\n63.31% for Gemini 1.5 Pro, 60.56% for Claude 3.5 Sonnet v2,\n57.93% for BERTMap, 30.16% for Llama 3.370B, and 28.81% for\nDeepSeek R1, demonstrating GPT-4o’s substantial advantage in\noverall performance (see\nTable 1).\nDetailed analysis of the confusion matrix metrics revealed\nsigniﬁcant diﬀerences among the systems ( Table 2). GPT-4o\nTABLE /two.tnum Contingency table.\nMethod True\npositives\nFalse\npositives\nTrue\nnegatives\nFalse\nnegatives\nGPT-4o 90 6 11 1\nClaude 3.5\nSonnet v2\n43 37 9 19\nGemini 1.5\nPro Latest\n44 29 13 22\nBERTMap 42 44 5 17\nLlama 3.3\n70B\n19 80 1 8\nDeepSeek\nR1\n17 49 7 35\ndemonstrated the best performance, generating 90 true positives,\n6 false positives, 1 false negative, and 11 true negatives. Gemini\n1.5 Pro produced 44 true positives, 29 false positives, 22 false\nnegatives, and 13 true negatives, while Claude 3.5 Sonnet v2\nachieved 43 true positives, 37 false positives, 19 false negatives,\nand 9 true negatives. BERTMap produced 42 true positives, 44\nfalse positives, 17 false negatives, and 5 true negatives. Llama\n3.3 70B generated 19 true positives, 80 false positives, 8 false\nnegatives, and 1 true negatives, while DeepSeek R1 produced 17\ntrue positives, 49 false positives, 35 false negatives, and 7 true\nnegatives. The notably lower number of false positives and false\nnegatives in the GPT-4o system (6 and 1, respectively) compared\nFrontiers in Artiﬁcial Intelligence /zero.tnum/seven.tnum frontiersin.org\nMavridis et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/four.tnum/six.tnum/one.tnum/seven.tnum/nine.tnum\nFIGURE /three.tnum\nA depiction of the performance for all methods.\nto the other systems indicates superior discrimination ability in\nmapping medical terminology.\nPerformance analysis across diﬀerent categories of medical\nterms revealed speciﬁc patterns (see\nFigure 3). All systems\nperformed relatively well with basic clinical measurements such as\nsex, age, weight, height, and BMI. However, GPT-4o demonstrated\nsuperior performance in mapping complex medical concepts,\nparticularly in areas of diagnostic procedures and examinations,\ntreatment-related terminology, temporal relationships, and disease\ncomplications and classiﬁcations. Llama 3.3 70B and DeepSeek\nR1 showed particular weakness in handling these more complex\nconcepts, with high rates of incorrect mappings.\nThe systems’ performance diverged most notably in handling\ncomplex procedural terminology and multi-component medical\nconcepts. GPT-4o maintained high accuracy in these cases, while\nthe other systems showed increased error rates. For instance,\nGPT-4o accurately mapped complex terms such as “Exam Type, ”\n“Diagnostic question, ” and “withdrawal of therapy, ” where the other\nsystems produced more inconsistent results.\nError analysis revealed that GPT-4o’s few misclassiﬁcations\nwere primarily concentrated in speciﬁc areas of complex\nquantitative measurements, multi-parameter clinical assessments,\nand certain anatomical treatment locations. These included terms\nsuch as “min_ECGstress, ” “ECGstress_Result, ” and anatomical\ntreatment speciﬁcations. While all systems demonstrated\ncompetence in basic medical terminology mapping, GPT-\n4o oﬀered substantial improvements in handling complex,\ncontext-dependent medical concepts and relationships.\nA chi-square test of independence was performed to examine\nthe relationship between the six ontology matching methods (GPT-\n4o, Claude 3.5 Sonnet v2, Gemini 1.5 Pro Latest, Llama 3.3 70B,\nDeepSeek R1, and BERTMap) and their performance outcomes\n(TP , FP , TN, and FN; see\nTable 1). The test revealed a statistically\nsigniﬁcant diﬀerence between the methods, χ 2 (15, N = 648) =\n196.94, p < 0.001, Cramer’s V = 0.32 (indicating a large eﬀect size).\nPost-hoc examination of the standardized residuals indicates\nthat GPT-4o demonstrated signiﬁcantly higher true positive\nrates and lower false positive rates than expected under the\nnull hypothesis. Speciﬁcally, GPT-4o achieved 90 true positives\ncompared to 17–44 for the other methods, representing a\nsubstantial performance advantage.\n/five.tnum Discussion\nThe results of our comparative study demonstrate signiﬁcant\nvariations in performance across six systems evaluated for\nmedical ontology mapping and RDF knowledge graph creation.\nGPT-4o’s superior performance, with precision of 93.75%,\nstands in stark contrast to Gemini 1.5 Pro (60.27%), Claude\n3.5 Sonnet v2 (53.75%), BERTMap (48.84%), DeepSeek R1\n(25.76%), and Llama 3.370B (19.19%). This wide performance\ngap illustrates the substantial advancement in automated\nmapping accuracy achieved by the most advanced LLMs,\nwhile also highlighting the challenges faced by smaller or less\nsophisticated models.\nThe performance hierarchy among the systems provides\nvaluable insights into the evolution of language models for\nspecialized tasks. GPT-4o’s exceptional performance suggests that\nits architecture and training approach are particularly well-\nsuited for handling medical terminology. The signiﬁcant drop\nin performance across other models creates a clear stratiﬁcation:\nGemini 1.5 Pro’s intermediate performance (F1-score 63.31%)\npositions it as the second-best option, followed by Claude 3.5\nSonnet v2 (60.56%) and BERTMap (57.93%), with DeepSeek\nR1 (28.81%) and Llama 3.370B (30.16%) showing substantially\nlower capabilities.\nOf particular interest is Llama 3.370B’s relatively high recall\n(70.37%) despite its low precision (19.19%), suggesting a tendency\ntoward over-generation of mappings. In contrast, DeepSeek R1’s\nFrontiers in Artiﬁcial Intelligence /zero.tnum/eight.tnum frontiersin.org\nMavridis et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/four.tnum/six.tnum/one.tnum/seven.tnum/nine.tnum\nmore balanced but lower precision (25.76%) and recall (32.69%)\nindicate consistent but limited capabilities across all aspects of\nthe task.\nAll systems demonstrated basic competence in mapping simple\nclinical measurements, but their performance diverged signiﬁcantly\nwhen handling complex medical concepts. GPT-4o maintained\nhigh accuracy across sophisticated scenarios, including disease\nterms, temporal relationships, and procedural terminology. The\nprogressive degradation in performance as concept complexity\nincreased was most pronounced in the open source models like\nLlama 3.370B and DeepSeek R1, which struggled particularly with\nnuanced medical terminology.\nError analysis reveals distinct patterns across the systems.\nGPT-4o’s minimal error proﬁle (6 false positives and 1 false\nnegative) stands in sharp contrast to the higher error rates of\nother systems. Gemini 1.5 Pro and Claude 3.5 Sonnet v2 showed\nsimilar error patterns, with moderate false positive and false\nnegative rates, while BERTMap exhibited a moderate false positive\nrate (44). The high false positive rates in Llama 3.370B and\nDeepSeek R1 suggest that smaller models struggle signiﬁcantly with\ndiscriminating valid mappings from invalid ones, highlighting the\nimportance of model scale and training data quality in achieving\nreliable performance.\nWe certainly acknowledge limitations across all systems.\nPerformance variability depends on training data quality and\ncoverage, with rare or highly specialized concepts presenting\nchallenges for all methods. GPT-4o’s few misclassiﬁcations centered\non complex quantitative measurements and multi-parameter\nclinical assessments, while other systems showed broader patterns\nof error across multiple concept types. The need for domain\nexpert validation remains, particularly for critical applications,\nthough the level of required oversight varies signiﬁcantly among\nthe systems.\nA possible reason for GPT-4o’s struggle with complex\nquantitative measurements and multi-parameter clinical\nassessments lies in the inherent challenges of such data.\nQuantitative medical data often require precise numerical\nreasoning and context-speciﬁc interpretation, which can be\ndiﬃcult for language models primarily trained on textual\ndata. Multi-parameter clinical evaluation involves intricate\nrelationships among numerous variables, requiring not\nonly precise recognition of individual terms but also the\nability to synthesize and contextualize multiple data points\nsimultaneously. While GPT-4o demonstrated the highest\noverall performance, the few misclassiﬁcations were mainly\nrelevant to these more complex areas, suggesting that the\nstrengths in language comprehension do not fully extend\nto the complex quantitative reasoning that is necessary for\nmulti-parameter clinical assessments. This underlines the need\nfor further improvements in numerical reasoning capabilities\nin LLMs, particularly in highly specialized domains, such\nas healthcare.\nFuture research presents several promising directions. The\nsigniﬁcant performance gap we observed between GPT-4o\nand smaller models, like Llama 3.3 70B and DeepSeek R1,\nindicates that architectural optimizations and reﬁned training\nstrategies could enhance the capabilities of more compact\nmodels. A major goal here is to develop more eﬃcient\narchitectures that can match GPT-4o’s performance, while\nrequiring fewer computational resources. These insights have\npractical implications. Although GPT-4o excels in medical\nterminology mapping, its high computational demands and cost\nmay limit its feasibility for certain applications. The varying\nperformance characteristics across models emphasize the necessity\nof balancing accuracy, eﬃciency and resource constraints when\nselecting a system for speciﬁc use cases.\nFinally, the dataset of 108 medical terms, derived from real-\nworld usage in a public hospital, was curated to reﬂect the\npractical challenges of medical language in clinical settings. A\nkey advantage of this dataset is the availability of expert-validated\nground truth, ensuring a reliable standard for evaluating LLMs,\nan aspect often diﬃcult to achieve with larger datasets. While\na broader dataset could enhance generalizability, the focus here\nwas on assessing model performance in a realistic scenario\nwith high-quality annotations. The selected terms span diverse\nclinical information, including patient demographics, physiological\nmeasurements, disease classiﬁcations, and treatments, providing a\nrepresentative sample for medical ontology mapping. Additionally,\nqualitative analysis by domain experts oﬀers critical insights into\nhandling complex medical concepts. Future work aims to expand\nthis evaluation with a larger dataset covering more medical\nsubﬁelds and data formats to further test the models’ scalability\nand adaptability.\n/six.tnum Conclusions\nOur research highlights the evolving role of diﬀerent LLM\narchitectures in advancing RDF knowledge graphs. By comparing\nmultiple systems, we’ve demonstrated not only the current state\nof the art but also the progression of capabilities in this\nﬁeld. The superior performance of GPT-4o suggests a pathway\ntoward more intelligent, adaptive, and semantically rich knowledge\nrepresentation systems, while the varying capabilities of other\nsystems provide insights into alternative approaches and potential\nareas for improvement.\nOur comparative study reveals a clear hierarchy in current LLM\ncapabilities for knowledge graph creation and ontology mapping,\nwith performance ranging from GPT-4o’s exceptional results, even\nwithout ﬁne-tuning, to the more limited capabilities of smaller\nmodels like Llama 3.370B and DeepSeek R1. This spectrum of\nperformance highlights both the remarkable progress in the ﬁeld\nand the continuing challenges in developing more eﬃcient and\naccessible solutions. As these technologies evolve, we anticipate\nfurther improvements across all systems, potentially narrowing\nthe current performance gaps while maintaining high standards\nof accuracy and reliability. Future work will explore the models’\nperformance with datasets that involve more diverse relationships,\ntemporal data, and hierarchical structures. Also the investigation of\nhybrid approaches that combine symbolic reasoning with machine\nlearning, as well as the performance of diﬀerent LLMs in niche\nareas of medical terminology mapping, especially for contexts with\nlimited computational resources, would signiﬁcantly expand the\nscope of this work.\nFrontiers in Artiﬁcial Intelligence /zero.tnum/nine.tnum frontiersin.org\nMavridis et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/four.tnum/six.tnum/one.tnum/seven.tnum/nine.tnum\nData availability statement\nThe original contributions presented in the study are included\nin the article/supplementary material, further inquiries can be\ndirected to the corresponding author.\nAuthor contributions\nAM: Formal analysis, Investigation, Methodology, Software,\nVisualization, Writing – original draft, Writing – review &\nediting. ST: Formal analysis, Investigation, Methodology, Software,\nVisualization, Writing – original draft, Writing – review & editing.\nCA: Software, Validation, Visualization, Writing – original draft,\nWriting – review & editing. MP: Validation, Writing – original\ndraft, Writing – review & editing. GM: Conceptualization, Funding\nacquisition, Project administration, Supervision, Writing – original\ndraft, Writing – review & editing.\nFunding\nThe author(s) declare that ﬁnancial support was received for\nthe research and/or publication of this article. This research has\nreceived funding from the Horizon Europe Framework Program\nunder Grant Agreement No. 101070670 (ENCRYPT). The content\nof this publication reﬂects the opinion of its authors and does\nnot, in any way, represent opinions of the funders. The European\nCommission are not responsible for any use that may be made of\nthe information that this publication contains.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nGenerative AI statement\nThe author(s) declare that no Gen AI was used in the creation\nof this manuscript.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\nArenas, M., Gutierrez, C., Miranker, D. P., Pérez, J., and Sequ eda, J.\nF. (2013). Querying semantic data on the web? SIGMOD Rec . 41, 6–17.\ndoi: 10.1145/2430456.2430458\nBaldwin, T., Li, Y., Alexe, B., and Stanoi, I. R. (2013). “Autom atic term\nambiguity detection, ” in Proceedings of the 51st Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers) , 804–809.\nBenslimane, S. M., Merazi, A., Malki, M., and Bensaber, D. A. (2 008). Ontology\nmapping for querying heterogeneous information sources. INFOCOMP J. Comput. Sci.\n7, 52–59.\nBergamaschi, S., Castano, S., and Vincini, M. (1999). Seman tic integration\nof semistructured and structured data sources. SIGMOD Rec . 28, 54–59.\ndoi: 10.1145/309844.309897\nBerners-Lee, T. (2019). I invented the world wide web. here’s how we can ﬁx it . The\nNew York Times 24. doi: 10.1145/1283920.3240733\nBerti-Equille, L. (2019). “Ml-based knowledge graph curation: cu rrent solutions\nand challenges, ” in Companion Proceedings of The 2019 World Wide Web Conference,\nWWW ’19, page 938–939, New York, NY, USA (Association for Computing Machinery).\ndoi: 10.1145/3308560.3316522\nChakraborty, J., Padki, A., and Bansal, S. K. (2017). “Semant ic etl—state-of-the-art\nand open research challenges, ” in 2017 IEEE 11th International Conference on Semantic\nComputing (ICSC), pages 413–418. doi: 10.1109/ICSC.2017.94\nChaves-Fraga, D., Pozo-Gilo, L., Toledo, J., Ruckhaus, E., and Corcho, O.\n(2020). “Morph-csv: virtual knowledge graph access for tabular data, ” in ISWC\n(Demos/Industry), 11–16.\nConstantopoulos, P., and Pertsas, V. (2020). “From publication s to knowledge\ngraphs, ” in Information Search, Integration, and Personalization , eds. G. Flouris, D.\nLaurent, D. Plexousakis, N. Spyratos, and Y. Tanaka (Cham: Spri nger International\nPublishing), 18–33. doi: 10.1007/978-3-030-44900-1_2\nDwork, C. (2011). A ﬁrm foundation for private data analysis. Commun. ACM 54,\n86–95. doi: 10.1145/1866739.1866758\nFan, L., Li, L., Ma, Z., Lee, S., Yu, H., and Hemphill, L. (2024). A bibliometric review\nof large language models research from 2017 to 2023. ACM Trans. Intell. Syst. Technol .\n15:930. doi: 10.1145/3664930\nGerber, D., Hellmann, S., Bühmann, L., Soru, T., Usbeck, R., and Ngonga\nNgomo, A.-C. (2013). “Real-time rdf extraction from unstruc tured data streams, ” in\nThe Semantic Web-ISWC 2013 , eds. H. Alani, L. Kagal, A. Fokoue, P. Groth, C.\nBiemann, J. X. Parreira et al. (Berlin, Heidelberg: Springer Ber lin Heidelberg), 135–150.\ndoi: 10.1007/978-3-642-41335-3_9\nGiglou, H. B., D’Souza, J., Engel, F., and Auer, S. (2024). Llms4o m: Matching\nontologies with large language models. arXiv preprint arXiv:2404.10317 .\nHan, L., Finin, T., Joshi, A., and Cheng, D. (2015). “Queryin g RDF data with text\nannotated graphs, ” in SSDBM ’15 (New York, NY, USA: Association for Computing\nMachinery). doi: 10.1145/2791347.2791381\nHaslhofer, B., Isaac, A., and Simon, R. (2018). Knowledge Graphs in the Libraries\nand Digital Humanities Domain . Cham: Springer International Publishing, 1–8.\ndoi: 10.1007/978-3-319-63962-8_291-1\nIglesias, E., Jozashoori, S., Chaves-Fraga, D., Collarana, D. , and Vidal, M.-E. (2020).\n“SDM-rdﬁzer: an RML interpreter for the eﬃcient creation of R DF knowledge graphs, ”\nin Proceedings of the 29th ACM International Conference on Information Knowl edge\nManagement, CIKM ’20(New York, NY, USA: Association for Computing Machinery),\n3039–3046. doi: 10.1145/3340531.3412881\nIglesias-Molina, A., Chaves-Fraga, D., Priyatna, F., and Cor cho, O. (2019). “Towards\nthe deﬁnition of a language-independent mapping template for know ledge graph\ncreation, ” in Proceedings of the Third International Workshop on Capturing Scient iﬁc\nKnowledge, pages 33–36.\nJanke, D., and Staab, S. (2018). Storing and Querying Semantic Data in the Cloud .\nCham: Springer International Publishing, 173–222. doi: 10.1 007/978-3-030-00338-8_7\nJi, S., Pan, S., Cambria, E., Marttinen, P., and Yu, P. S. (202 2). A survey on\nknowledge graphs: Representation, acquisition, and applications . IEEE Trans. Neural\nNetw. Learn. Syst . 33, 494–514. doi: 10.1109/TNNLS.2021.3070843\nJia, R., Zhang, B., Méndez, S. J. R., and Omran, P. G. (2024). Le veraging large\nlanguage models for semantic query processing in a scholarly knowle dge graph. arXiv\npreprint arXiv:2405.15374.\nJozashoori, S., and Vidal, M.-E. (2019). “Mapsdi: a scaled-up s emantic data\nintegration framework for knowledge graph creation, ” in On the Move to Meaningful\nInternet Systems: OTM 2019 Conferences , eds. H. Panetto, C. Debruyne, M. Hepp, D.\nFrontiers in Artiﬁcial Intelligence /one.tnum/zero.tnum frontiersin.org\nMavridis et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/four.tnum/six.tnum/one.tnum/seven.tnum/nine.tnum\nLewis, C. A. Ardagna, and R. Meersman (Cham: Springer Interna tional Publishing),\n58–75. doi: 10.1007/978-3-030-33246-4_4\nKanmani, A. C., Chockalingam, T., and Guruprasad, N. (2017). “A n exploratory\nstudy of RDF: a data model for cloud computing, ” in Proceedings of the 5th International\nConference on Frontiers in Intelligent Computing: Theory and Application s, eds. S. C.\nSatapathy, V. Bhateja, S. K. Udgata, and P. K. Pattnaik (Singa pore: Springer Singapore),\n641–649. doi: 10.1007/978-981-10-3156-4_68\nKaralka, C., Meditskos, G., and Bassiliades, N. (2023). “Towar ds semantic\ninterpretation of structured data sources in privacy-preserv ing environments, ” in\nKGCW@ ESWC.\nKejriwal, M., Sequeda, J. F., and Lopez, V. (2019). Knowledge gra phs: construction,\nmanagement and querying. Semant. Web 10, 961–962. doi: 10.3233/SW-190370\nKnoblock, C. A., Szekely, P., Ambite, J. L., Goel, A., Gupta, S., Ler man, K., et al.\n(2012). “Semi-automatically mapping structured sources into t he semantic web, ” in\nThe Semantic Web: Research and Applications , eds. E. Simperl, P. Cimiano, A. Polleres,\nO. Corcho, and V. Presutti (Berlin, Heidelberg: Springer Berlin Heidelberg), 375–390.\ndoi: 10.1007/978-3-642-30284-8_32\nKommineni, V. K., König-Ries, B., and Samuel, S. (2024). From human experts to\nmachines: an LLM supported approach to ontology and knowledge graph construction.\narXiv preprint arXiv:2403.08345 .\nKorayem, M., Ortiz, C., AlJadda, K., and Grainger, T. (2015). “ Query sense\ndisambiguation leveraging large scale user behavioral data, ” in 2015 IEEE International\nConference on Big Data (Big Data) , 1230–1237. doi: 10.1109/BigData.2015.7363877\nKulkarni, C. (2023). The evolution of large language models in nat ural\nlanguage understanding. J. Artif. Intell. Mach. Learn. Data Sci . 1, 49–53.\ndoi: 10.51219/JAIMLD/chinmay-shripad-kulkarni/28\nLei, C., Efthymiou, V., Geis, R., and Özcan, F. (2020). “Expand ing query answers\non medical knowledge bases, ” in International Conference on Extending Database\nTechnology. OpenProceedings. org .\nLenzerini, M. (2018). Managing data through the lens of an ont ology. AI Mag. 39,\n65–74. doi: 10.1609/aimag.v39i2.2802\nNashipudimath, M. M., Shinde, S. K., and Jain, J. (2020). An eﬃ cient integration and\nindexing method based on feature patterns and semantic analys is for big data. Array\n7:100033. doi: 10.1016/j.array.2020.100033\nRaiaan, M. A. K., Mukta, M. S. H., Fatema, K., Fahad, N. M., Sak ib, S.,\nMim, M. M. J., et al. (2024). A review on large language models: arc hitectures,\napplications, taxonomies, open issues and challenges. IEEE Access 12, 26839–26874.\ndoi: 10.1109/ACCESS.2024.3365742\nRemadi, A., El Hage, K., Hobeika, Y., and Bugiotti, F. (2024) . To prompt or\nnot to prompt: navigating the use of large language models for inte grating and\nmodeling heterogeneous data. Data Knowl. Eng. 152:102313. doi: 10.1016/j.datak.2024.\n102313\nSayed Ahmed Soliman, H. A., and Tabak, A. (2020). Visualizing R DF and knowledge\ngraphs interactive framework to support analysis decision. J. Global Econ. Manag. Bus.\nRes. 12, 43–46.\nSingh, G., Bhatia, S., and Mutharaju, R. (2023). “Neuro-symb olic RDF and\ndescription logic reasoners: the state-of-the-art and challen ges, ” in Compendium of\nNeurosymbolic Artiﬁcial Intelligence (IOS Press), 29–63. doi: 10.3233/FAIA230134\nTiddi, I., and Schlobach, S. (2022). Knowledge graphs as tools fo r explainable\nmachine learning: a survey. Artif. Intell . 302:103627. doi: 10.1016/j.artint.2021.\n103627\nTrajanoska, M., Stojanov, R., and Trajanov, D. (2023). Enha ncing knowledge graph\nconstruction using large language models. arXiv preprint arXiv:2305.04676 .\nVenkatachaliah, G. (2011). “Emerging challenges in informatio n management:\na perspective from the industry, ” in 5th IEEE International Conference\non Digital Ecosystems and Technologies (IEEE DEST 2011) , 339–339.\ndoi: 10.1109/DEST.2011.5936589\nWan, Z., Yin, Y., Zhang, W., Shi, J., Shang, L., Chen, G., et al. ( 2022). G-map: general\nmemory-augmented pre-trained language model for domain task s. arXiv preprint\narXiv:2212.03613.\nWu, Z., and Banerjee, J. (2014). “Eﬃcient application of complex g raph analytics on\nvery large real world RDF datasets, ” in ISWC (Industry Track) .\nWylot, M., Hauswirth, M., Cudré-Mauroux, P., and Sakr, S. (20 18). RDF data\nstorage and query processing schemes: a survey. ACM Comput. Surv . 51, 1–16.\ndoi: 10.1145/3177850\nXue, Q. (2024). Unlocking the potential: a comprehensive explorat ion of large\nlanguage models in natural language processing. Appl. Comput. Eng . 57, 247–252.\ndoi: 10.54254/2755-2721/57/20241341\nZhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., et al. (2023). A survey of\nlarge language models. arXiv preprint arXiv:2303.18223 .\nZhong, Y., and Goodfellow, S. D. (2024). Domain-speciﬁc language models pre-\ntrained on construction management systems corpora. Automat. Constr . 160:105316.\ndoi: 10.1016/j.autcon.2024.105316\nZou, X. (2020). “A survey on application of knowledge graph, ” in Journal of\nPhysics: Conference Series (IOP Publishing), 012016. doi: 10.1088/1742-6596/1487/\n1/012016\nFrontiers in Artiﬁcial Intelligence /one.tnum/one.tnum frontiersin.org",
  "topic": "RDF",
  "concepts": [
    {
      "name": "RDF",
      "score": 0.8283963799476624
    },
    {
      "name": "Computer science",
      "score": 0.7339398264884949
    },
    {
      "name": "RDF Schema",
      "score": 0.6351689696311951
    },
    {
      "name": "Ontology",
      "score": 0.6336495876312256
    },
    {
      "name": "Knowledge graph",
      "score": 0.5179599523544312
    },
    {
      "name": "Natural language processing",
      "score": 0.5051723122596741
    },
    {
      "name": "SPARQL",
      "score": 0.475040465593338
    },
    {
      "name": "Graph",
      "score": 0.4613509178161621
    },
    {
      "name": "Information retrieval",
      "score": 0.4511104226112366
    },
    {
      "name": "Web Ontology Language",
      "score": 0.43924906849861145
    },
    {
      "name": "Semantic Web",
      "score": 0.33563682436943054
    },
    {
      "name": "Theoretical computer science",
      "score": 0.20329371094703674
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I21370196",
      "name": "Aristotle University of Thessaloniki",
      "country": "GR"
    }
  ]
}