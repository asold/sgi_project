{
    "title": "Long Short-Term Transformer for Online Action Detection",
    "url": "https://openalex.org/W3179019763",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5078029945",
            "name": "Mingze Xu",
            "affiliations": [
                "Amazon (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A5005925571",
            "name": "Yuanjun Xiong",
            "affiliations": [
                "Amazon (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A5100353664",
            "name": "Hao Chen",
            "affiliations": [
                "Amazon (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A5100406106",
            "name": "Xinyu Li",
            "affiliations": [
                "Amazon (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A5067662547",
            "name": "Wei Xia",
            "affiliations": [
                "Amazon (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A5001760915",
            "name": "Zhuowen Tu",
            "affiliations": [
                "Amazon (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A5038328783",
            "name": "Stefano Soatto",
            "affiliations": [
                "Amazon (Germany)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963524571",
        "https://openalex.org/W2911109671",
        "https://openalex.org/W2963720581",
        "https://openalex.org/W2955874753",
        "https://openalex.org/W3098366850",
        "https://openalex.org/W2924644257",
        "https://openalex.org/W2472970127",
        "https://openalex.org/W3137884587",
        "https://openalex.org/W2919015029",
        "https://openalex.org/W2805042136",
        "https://openalex.org/W2755876276",
        "https://openalex.org/W2939062286",
        "https://openalex.org/W3126337037",
        "https://openalex.org/W2559767995",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W2612445135",
        "https://openalex.org/W2142308810",
        "https://openalex.org/W1805361895",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2746726611",
        "https://openalex.org/W2788932810",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W1923404803",
        "https://openalex.org/W2995575179",
        "https://openalex.org/W1522734439",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W2963782415",
        "https://openalex.org/W3202141913",
        "https://openalex.org/W3134144764",
        "https://openalex.org/W1947481528",
        "https://openalex.org/W2953153458",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W3128626728",
        "https://openalex.org/W2964274041",
        "https://openalex.org/W3126721948",
        "https://openalex.org/W3159612540",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W3174114091",
        "https://openalex.org/W2336403884",
        "https://openalex.org/W2131774270",
        "https://openalex.org/W1836465849",
        "https://openalex.org/W2883429621",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2110485445",
        "https://openalex.org/W2950971447",
        "https://openalex.org/W2342437993",
        "https://openalex.org/W2274287116",
        "https://openalex.org/W3175859344",
        "https://openalex.org/W2953229046",
        "https://openalex.org/W1498436455",
        "https://openalex.org/W2016776918",
        "https://openalex.org/W2706729717",
        "https://openalex.org/W3137120824",
        "https://openalex.org/W2989506443",
        "https://openalex.org/W3033529678",
        "https://openalex.org/W3123394884",
        "https://openalex.org/W2394849137",
        "https://openalex.org/W3098872093",
        "https://openalex.org/W1924770834",
        "https://openalex.org/W2964216549",
        "https://openalex.org/W3092388717",
        "https://openalex.org/W3104613728",
        "https://openalex.org/W2933875464",
        "https://openalex.org/W2963247196",
        "https://openalex.org/W2963448607",
        "https://openalex.org/W3124982122",
        "https://openalex.org/W3119786062",
        "https://openalex.org/W2994508843",
        "https://openalex.org/W2962925365",
        "https://openalex.org/W3035130921",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2963155035",
        "https://openalex.org/W3204419213",
        "https://openalex.org/W2143158675"
    ],
    "abstract": "We present Long Short-term TRansformer (LSTR), a temporal modeling algorithm for online action detection, which employs a long- and short-term memory mechanism to model prolonged sequence data. It consists of an LSTR encoder that dynamically leverages coarse-scale historical information from an extended temporal window (e.g., 2048 frames spanning of up to 8 minutes), together with an LSTR decoder that focuses on a short time window (e.g., 32 frames spanning 8 seconds) to model the fine-scale characteristics of the data. Compared to prior work, LSTR provides an effective and efficient method to model long videos with fewer heuristics, which is validated by extensive empirical analysis. LSTR achieves state-of-the-art performance on three standard online action detection benchmarks, THUMOS'14, TVSeries, and HACS Segment. Code has been made available at: https://xumingze0308.github.io/projects/lstr",
    "full_text": "Long Short-Term Transformer for\nOnline Action Detection\nMingze Xu Yuanjun Xiong Hao Chen Xinyu Li\nWei Xia Zhuowen Tu Stefano Soatto\nAmazon/AWS AI\n{xumingze,yuanjx,hxen,xxnl,wxia,ztu,soattos}@amazon.com\nAbstract\nWe present Long Short-term TRansformer (LSTR), a temporal modeling algo-\nrithm for online action detection, which employs a long- and short-term memory\nmechanism to model prolonged sequence data. It consists of an LSTR encoder\nthat dynamically leverages coarse-scale historical information from an extended\ntemporal window (e.g., 2048 frames spanning of up to 8 minutes), together with\nan LSTR decoder that focuses on a short time window (e.g., 32 frames spanning\n8 seconds) to model the Ô¨Åne-scale characteristics of the data. Compared to prior\nwork, LSTR provides an effective and efÔ¨Åcient method to model long videos with\nfewer heuristics, which is validated by extensive empirical analysis. LSTR achieves\nstate-of-the-art performance on three standard online action detection benchmarks,\nTHUMOS‚Äô14, TVSeries, and HACS Segment. Code has been made available at:\nhttps://xumingze0308.github.io/projects/lstr.\n1 Introduction\nGiven an incoming stream of video frames, online action detection [14] is concerned with the task\nof classifying what is happening at each frame without seeing the future. Unlike ofÔ¨Çine methods\nthat assume the entire video is available, online methods process the data causally, up to the current\ntime. In this paper, we present an online temporal modeling algorithm capable of capturing temporal\nrelations on prolonged sequences up to 8 minutes long, while retaining Ô¨Åne granularity of the event\nin the representation. This is achieved by modeling activities at different temporal scales, so as to\ncapture a variety of events ranging from bursts to slow trends.\nSpeciÔ¨Åcally, we propose a method, named Long Short-term TRansformer (LSTR), to jointly model\nlong- and short-term temporal dependencies. LSTR has two main advantages over prior work. 1)\nIt stores the history directly thus avoiding the pitfalls of recurrent models [ 18, 50, 28, 10]. Back-\npropagation through time, BPTT, is not needed as the model can directly attend to any useful frames\nfrom memory. 2) It separates long- and short-term memories, which allows modeling short-term\ncontext while extracting useful correlations from the long-term history. This allows us to compress\nthe long-term history without losing important Ô¨Åne-scale information.\nAs shown in Fig. 1, we explicitly divide the entire history into the long- and short-term memories and\nbuild our model with an encoder-decoder architecture. SpeciÔ¨Åcally, the LSTR encodercompresses\nand abstracts the long-term memory into a latent representation of Ô¨Åxed length, and theLSTR decoder\nuses a short window of transient frames to perform self-attention and cross-attention operations\non the extracted token embeddings from the LSTR encoder. In the LSTR encoder, an extended\ntemporal support becomes beneÔ¨Åcial in dealing with untrimmed, streaming videos by devising two-\nstage memory compression, which is shown to be computationally efÔ¨Åcient in both training and\ninference. Our overall long short-term Transformer architecture gives rise to an effective and efÔ¨Åcient\nrepresentation for modeling prolonged sequence data.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021).\narXiv:2107.03377v3  [cs.CV]  22 Dec 2021\nLSTR DecoderLSTR EncoderLong-Term Memory\nShort-Term Memory\nT\nLong Short-term TRansformer (LSTR)\nStore\nClassification\nat time T\nStore\nFigure 1: Overview of Long Short-term TRansformer (LSTR). Given a live streaming video,\nLSTR sequentially identiÔ¨Åes the actions happening in each incoming frame by using an encoder-\ndecoder architecture, without future context. The dashed brown arrows indicate the data Ô¨Çow of the\nlong- and short-term memories following the Ô¨Årst-in-Ô¨Årst-out (FIFO) logic. (Best viewed in color.)\nWe validate LSTR on standard benchmark datasets (THUMOS‚Äô14 [30], TVSeries [14], and HACS\nSegment [76]). These have distinct characteristics such as video length spanning from a few seconds\nto tens of minutes. Experimental results establish LSTR as the state-of-the-art for online action\ndetection. Ablation studies further showcase LSTR‚Äôs abilities in modeling long video sequences.\n2 Related Work\nOnline Action Detection.Temporal action localization aims to detect the onset and termination\nof action instances after observing the entire video [ 52, 71, 23, 53, 78, 5, 37, 39, 38]. Embodied\nperception, however, requires causal processing [73], where we can only process the data up to the\npresent. Online action detection focuses on this setting [14]. RED [22] uses a reinforcement loss to\nencourage recognizing actions as early as possible. TRN [72] models greater temporal context by\nsimultaneously performing online action detection and anticipation. IDN [19] learns discriminative\nfeatures and accumulates only relevant information for the present. LAP-Net [ 46] proposes an\nadaptive sampling strategy to obtain optimal features. PKD [77] transfers knowledge from ofÔ¨Çine to\nonline models using curriculum learning. As with early action detection [ 27, 40], Shou et al. [54]\nfocus on online detection of action start (ODAS). StartNet [24] decomposes ODAS into two stages\nand learns with policy gradient. WOAD [25] uses weakly-supervised learning with video-level labels.\nTemporal/Sequence Modeling.Causal time series analysis has traditionally assumed the existence\nof a latent ‚Äústate‚Äù variable that captures all information in past data, and is updated using only the\ncurrent datum [49, 28, 10]. While the Separation Principle ensures that such a state exists for linear-\nGaussian time series, in general it is not possible to summarize all past history of complex data in a\nÔ¨Ånite-dimensional sufÔ¨Åcient statistic. Therefore, we directly model the history, in accordance with\nother work on video understanding [69, 45, 68]. Earlier work on action recognition usually relies on\nheuristic sub-sampling (typically 3 to 7 video frames) for more feasible training [74, 62, 21, 41, 57].\n3D ConvNets [59, 8, 60] are used to perform spatio-temporal feature modeling on more frames,\nbut they fail to capture temporal correlations beyond their receptive Ô¨Åeld. Recently, Wuet al. [69]\npropose long-term feature banks to capture objects and scene features, but discarding their temporal\norder which is clearly informative. Most of work above does not explicitly separate the long- and\nshort-term context modeling, but instead integrates all observed features with simple mechanisms\nsuch as pooling or concatenation. We are motivated by work in Cognitive Science [ 44, 12, 9, 35]\nthat has shed light on the design principles for modeling long-term dependencies with attention\nmechanism [66, 13, 48, 75].\nTransformers for Action Understanding.Transformers have achieved breakthrough success in\nNLP [47, 15] and are adopted in computer vision for image recognition [17, 58] and object detec-\ntion [7]. Recent papers exploit Transformers for temporal modeling tasks in videos, such as action\nrecognition [43, 51, 36, 4, 3] and temporal action localization [42, 56], and achieve promising results.\nHowever, computational and memory demands result in most work being limited to short video clips,\nwith few exceptions [13, 6] that focuses on designing Transformers to model long-range context. The\nmechanism for aggregating long- and short-term information is relatively unexplored [32].\n3 Long Short-Term Transformer\nGiven a live streaming video, our goal is to identify the actions performed in each video frame\nusing only past and current observations. Future information is not accessible during inference.\nFormally, a streaming video at time tis represented by a batch of œÑ past frames It = {It‚àíœÑ,¬∑¬∑¬∑ ,It},\n2\n‚Ä¶\n1√ó\nTransformer \nDecoder\nUnit\nùìµùíÜùíèùíÑ√ó\nTransformer \nDecoder\nUnits\nùìµùíÖùíÜùíÑ√ó\nTransformer \nDecoder\nUnits\nK+1 Classification\nùëì!\nLive Streaming Video\nT\nLong-Term Memory (ùêåùêã) Short-Term Memory (ùêåùêí) Future\nùëé!\nLSTR Encoder LSTR Decoder\nToken Embeddings 0\nLong-Term Memory Compressed Memory\nPositional\nEncoding\nToken Embeddings 1\n‚Ä¶\nShort-Term Memory\nPositional\nEncoding\nDirectional\nAttention Mask\nEncoded Memory\n‚Ä¶‚Ä¶‚Ä¶\n‚Ä¶ ‚Ä¶ ‚Ä¶\n‚Ä¶‚Ä¶‚Ä¶\n‚Ä¶\nùíèùüé\nùíéùë≥ ùíèùüé\nùíèùüè\nùíèùüè\nùíéùë∫\nCurrent\nTime\nMasked \nMulti-Head \nAttention\nMulti-Head \nAttention\nFeed\nForward\nAdd & Norm\nAdd & Norm\nAdd & Norm\nTransformer\nDecoder\nUnit\nFigure 2: Visualization of Long Short-Term Transformer (LSTR), which is formulated in an\nencoder-decoder manner. SpeciÔ¨Åcally, the LSTR encoder compresses the long-term memory of size\nmL to n1 encoded latent features, and the LSTR decoder references related context information from\nthe encoded memory with the short-term memory of size mS for action recognition of the present.\nThe LSTR encoder and decoder are built with Transformer decoder units [61], which take the input\ntokens (dark green arrows) and output tokens (dark blue arrows) as inputs. During inference, LSTR\nprocesses every incoming frame in an online manner, absent future context. (Best viewed in color.)\nwhich reads ‚ÄúI up to time t.‚Äù The online action detection system receivesIt as input, and classiÔ¨Åes\nthe action category ÀÜyt belonging to one of (K+ 1)classes, ÀÜyt ‚àà{0,1,¬∑¬∑¬∑ ,K}, ideally using the\nposterior probability P(ÀÜyt = k|It), where k= 0denotes the probability that no event is occurring\nat frame t. We design our method by assuming that there is a pretrained feature extractor [62] that\nprocesses each video frame It into a feature vector ft ‚ààRC of Cdimensions1. These vectors form a\n(œÑ √óC)-dimensional temporal sequence that serves as the input of our method.\n3.1 Overview\nOur method is based on the intuition that frames observed recently provide precise information about\nthe ongoing action instance, while frames over an extended period offer contextual references for\nactions that are potentially happening right now. We propose Long Short-term TRansformer (LSTR)\nin an explicit encoder-decoder manner, as shown in Fig. 2. In particular, the feature vectors ofmL\nframes in the distant past are stored in a long-term memory, and a short-term memorystores the\nfeatures of mS recent frames. The LSTR encoder compresses and abstracts features in long-term\nmemory to an encoded latent representation of n1 vectors. The LSTR decoder queries the encoded\nlong-term memory with the short-term memory for decoding, leading to the action prediction ÀÜyt.\nThis design follows the line of thought in combining long- and short-term information for action\nunderstanding [16, 62, 69], but addresses several key challenges to efÔ¨Åciently achieve this goal,\nexploiting the Ô¨Çexibility of Transformers [61].\n3.2 Long- and Short-Term Memories\nWe store the streaming input of feature vectors into two consecutive memories. The Ô¨Årst is the\nshort-term memory which stores only a small number of frames that are recently observed. We\nimplement it with a Ô¨Årst-in-Ô¨Årst-out (FIFO) queue of mS slots. At time T, it stores the feature vectors\nas MS = {fT,¬∑¬∑¬∑ ,fT‚àímS+1}. When a frame becomes ‚Äúolder‚Äù than mS time steps, it graduates\nfrom MS and enters into the long-term memory, which is implemented with another FIFO queue\nof mL slots. The long term memory stores ML = {fT‚àímS,¬∑¬∑¬∑ ,ft‚àímS‚àímL+1}. The long-term\nmemory serves as the input memory to the LSTR encoder and the short-term memory serves as the\nqueries for the LSTR decoder. In practice, the long-term memory stores a much longer time span than\nthe short-term memory (mS ‚â™mL). A typical choice is mL = 2048, which represents 512 seconds\nworth of video contents with 4 frames per second (FPS) sampling rate, and mS = 32representing\n1In practice, some feature extractors [59] take consecutive frames to produce one feature vector. Nonetheless,\nit is still temporally ‚Äúcentered‚Äù on a single frame. Thus we use the single frame notation here for simplicity.\n3\n8 seconds. We add a sinusoidal positional encoding s [61] to each frame feature in the memories\nrelative to current time T (i.e., the frame at T ‚àíœÑ receives a positional embedding of sœÑ).\n3.3 LSTR Encoder\nThe LSTR encoder aims at encoding the long-term memory of mL feature vectors into a latent\nrepresentation that LSTR can use for decoding useful temporal context. This task requires a large\ncapacity in capturing the relations and temporal context across a span of hundreds or even thousands\nof frames. Prior work on modeling long-term context for action understanding relies on heuristic\ntemporal sub-sampling [69, 62] or recurrent networks [16] to make training feasible, at the cost of\nlosing speciÔ¨Åc information of each time step. Attention-based architectures, such as Transformer [61],\nhave recently been shown promising for similar tasks that require long-range temporal modeling [51].\nA straightforward choice for LSTR encoder would be to use a Transformer encoder based on\nself-attention. However, its time complexity, O(m2\nLC), grows quadratically with the memory\nsequence length mL. This limits our ability to model long-term memory with sufÔ¨Åcient length to\ncover long videos. Though recent work [ 64] has explored self-attention with linear complexity,\nrepeatedly referencing information from the long-term memory with multi-layer Transformers is still\ncomputationally heavy. In LSTR, we propose to use the two-stage memory compression mechanism\nbased on Transformer decoder units [61] to achieve more effective memory encoding.\nThe Transformer decoder unit[61] takes two sets of inputs. The Ô¨Årst set includes a Ô¨Åxed number of\nnlearnable output tokens Œª ‚ààRn√óC, where Cis the embedding dimension. The second set includes\nanother minput tokens Œ∏ ‚ààRm√óC, where mcan be a rather large number. It Ô¨Årst applies one layer\nof multi-head self-attention on Œª. The outputs Œª‚Ä≤ are then used as queries in an ‚ÄúQKV cross-attention‚Äù\noperation and the input embeddings Œ∏ serve as key and value. The two steps can be written as\nŒª‚Ä≤ = SelfAttn(Œª) =Softmax(Œª ¬∑ŒªT\n‚àö\nC\n)Œª and CrossAttn(œÉ(Œª‚Ä≤),Œ∏) =Softmax(œÉ(Œª‚Ä≤) ¬∑Œ∏T\n‚àö\nC\n)Œ∏,\nwhere œÉ : Rn√óC ‚ÜíRn√óC denotes the intermediate layers between the two attention operations.\nOne appealing property of this design is that it transforms the m√óCdimensional input tokens into\noutput tokens of n√óC dimensions in O(n2C+ nmC) time complexity. When n‚â™m, the time\ncomplexity becomes linear to m, making it an ideal candidate for compressing long-term memory.\nThis property is also utilized in [32] to efÔ¨Åciently process large volume inputs, such as image pixels.\nTwo-Stage Memory Compression.Stacking multiple Transformer decoder units on the long-term\nmemory, as in [61], can form a memory encoder with linear complexity with respect to the memory\nsize mL. However, running the encoder at each time step can still be time consuming. We can further\nreduce the time complexity with a two-stage memory compression design. The Ô¨Årst stage has one\nTransformer decoder unit with n0 output tokens. Its input tokens are the entire long-term memory\nof size mL. The outputs of the Ô¨Årst stage are used as the input tokens to the second stage, which\nhas ‚Ñìenc stacked Transformer decoder units and n1 output tokens. Then, the long-term memory of\nsize mL √óCis compressed into a latent representation of size n1 √óC, which can then be efÔ¨Åciently\nqueried in the LSTR decoder later. This two-stage memory compression design is illustrated in Fig. 2.\nCompared to an (1 +‚Ñìenc)-layer Transformer encoder with O(m2\nL(1 +‚Ñìenc)C) time complexity or\nstacked Transformer decoder units with noutput-tokens having O((n2 + nmL)(1 +‚Ñìenc)C) time\ncomplexity, the proposed LSTR encoder has complexity ofO(n2\n0C+ n0mLC+ (n2\n1 + n1n0)‚ÑìencC).\nBecause both n0 and n1 are much smaller than mL, and ‚Ñìenc is usually larger than 1, using two-stage\nmemory compression could be more efÔ¨Åcient. In Sec. 3.6, we will show that, during online inference,\nit further enables us to reduce the runtime of the Transformer decoder unit of the Ô¨Årst stage. In\nSec. 4.5, we empirically found this design also leads to better performance for online action detection.\n3.4 LSTR Decoder\nThe short-term memory contains informative features for classifying actions on the latest time step.\nThe LSTR decoder uses the short-term memory as queries to retrieve useful information from the\nencoded long-term memory produced by the LSTR encoder. The LSTR decoder is formed by stacking\n‚Ñìdec layers of Transform decoder units. It takes the outputs of the LSTR encoder as input tokens and\nthe mS feature vectors in the short-term memory as output tokens. It outputs mS probability vectors\n{pT,¬∑¬∑¬∑ ,pT‚àímS+1}‚àà [0,1]K+1, each pt representing the predicted probability distribution of K\naction categories and one ‚Äúbackground‚Äù class at timet. During inference, we only take the probability\nvector pT from the output token corresponding to the current time T for the classiÔ¨Åcation result.\n4\nHowever, having the additional outputs on the older frames allows the model to leverage more\nsupervision signals during training. The details will be described below.\n3.5 Training LSTR\nLSTR can be trained without temporal unrolling and Backpropagation Through Time (BPTT) as in\nLSTM [28], which is a common property of Transformers [61]. We construct each training sample\nby randomly sampling an ending time T and Ô¨Ålling the long- and short-term memories by tracing\nback in time for mS + mL frames. We use the empirical cross entropy loss between the predicted\nprobability distribution pT at time T and the ground truth action label yT ‚àà{0,1,¬∑¬∑¬∑ ,K}as\nL(yT,pT; T) =‚àí\nK‚àë\nk=0\nŒ¥(k‚àíyT) logpk\nT, (1)\nwhere pk\nT is the k-th element of the probability vector pT, predicted on the latest frame at T.\nAdditionally, we add a directional attention mask[61] to the short-term memory so that any frame in\nthe short-term memory can only depend on its previous frames. In this way, we can make predictions\non all frames in the short-term memory as if they are the latest ones. Thus we can provide supervision\non every frame in the short-term memory, and the complete loss function Lis then\nLT =\nT‚àë\nt=T‚àíms+1\nL(yt,pt; T), (2)\nwhere pt denotes the prediction from the output token corresponding to time t.\n3.6 Online Inference with LSTR\nDuring online inference, the video frame features are streamed to the model as time passes. Running\nLSTR‚Äôs long-term memory encoder from scratch for each frame results in a time complexity of\nO(n2\n0C+ n0mLC) and O((n2\n1 + n1n0)‚ÑìencC) for the Ô¨Årst and second memory compression stages,\nrespectively. However, at each time step, there is only one new video frame to be updated. We show\nit is possible to achieve even more efÔ¨Åcient online inference by storing the intermediate results for the\nTransformer decoder unit of the Ô¨Årst stage. First, the queries of the Ô¨Årst Transformer decoder unit are\nÔ¨Åxed. So their self-attention outputs can be pre-computed and used throughout the inference. Second,\nthe cross-attention operation in the Ô¨Årst stage can be written as\nCrossAttn(qi,{fT‚àíœÑ + sœÑ}) =\nmS+mL‚àí1‚àë\nœÑ=mS\nexp((fT‚àíœÑ + sœÑ) ¬∑qi/\n‚àö\nC)‚àëmS+mL‚àí1\nœÑ=mS\nexp((fT‚àíœÑ + sœÑ) ¬∑qi/\n‚àö\nC)\n¬∑(fT‚àíœÑ + sœÑ), (3)\nwhere the index œÑ = T ‚àítis the relative position of a frame tin the long-term memory to the latest\ntime T. This calculation depends on the un-normalized attention weight matrix A ‚ààRmL√ón0 , with\nelements aœÑi = (fT‚àíœÑ + sœÑ) ¬∑qi. A can be decomposed into the sum of two matrices Af and As.\nWe have their elements as af\nœÑi = fT‚àíœÑ ¬∑qi and as\nœÑi = sœÑ ¬∑qi. The queries after the Ô¨Årst self-attention\noperation, Q = [q1,..., qn0 ], and the position embedding sœÑ are Ô¨Åxed during inference. Thus the\nmatrix As can be pre-computed and used for every incoming frame. We additionally maintain a\nFIFO queue of vectors at = Q‚ä§ft of size mL. Af at any time step T can be obtained by stacking\nall vectors currently in this queue. Updating this queue at each time step requires O(n0C) time\ncomplexity for the matrix-vector product. Now we can obtain the matrix A with only n0 √ómL\nadditions by adding As and Af together, instead of n0 √ómL √óC multiplications and additions\nusing Eq. (3). This means the amortized time complexity for computing the attention weights can be\nreduced to O(n0(mL + C)). Although the time complexity of the cross-attention operation is still\nO(n0mLC) due to the inevitable operation of weighted sum, sinceCis usually larger than 1024 [26],\nthis is still a considerable reduction of runtime. LSTR‚Äôs walltime efÔ¨Åciency is discussed in Sec. 4.6.\n4 Experiments\n4.1 Datasets\nWe evaluate our model on three publicly-available datasets: THUMOS‚Äô14[30], TVSeries [14] and\nHACS Segment[76]. THUMOS‚Äô14 includes over 20 hours of sports video annotated with 20 actions.\nWe follow prior work [72, 19] and train on the validation set (200 untrimmed videos) and evaluate on\nthe test set (213 untrimmed videos). TVSeries contains 27 episodes of 6 popular TV series, totaling\n16 hours of video. The dataset is annotated with 30 realistic, everyday actions ( e.g., open door).\nHACS Segment is a large-scale dataset of web videos. It contains 35,300 untrimmed videos over 200\nhuman action classes for training and 5,530 untrimmed videos for validation.\n5\n4.2 Settings\nFeature Encoding.We follow the experimental settings of state-of-the-art methods [72, 19]. We\nextract video frames at 24 FPS and set the video chunk size to 6. Decisions are made at the chunk\nlevel, and thus accuracy is evaluated at every 0.25 second. For feature encoding, we adopt the\nTSN [62] models implemented in an open-source toolbox [11]. SpeciÔ¨Åcally, the features are extracted\nby one visual model with the ResNet-50 [ 26] architecture from the central frame of each chunk\nand one motion model with the BN-Inception [31] architecture from the stacked optical Ô¨Çow Ô¨Åelds\nbetween 6 consecutive frames [ 62]. The visual and motion features are concatenated along the\nchannel dimension as the Ô¨Ånal feature f. We experiment with feature extractors pretrained on two\ndatasets, ActivityNet and Kinetics.\nImplementation Details.We implemented our proposed model in PyTorch [1], and performed all\nexperiments on a system with 8 Nvidia V100 graphics cards. For all Transformer units, we set their\nnumber of heads as 16 and hidden units as 1024 dimensions. To learn model weights, we used the\nAdam [34] optimizer with weight decay 5 √ó10‚àí5. The learning rate was linearly increased from zero\nto 5 √ó10‚àí5 in the Ô¨Årst 2/5 of training iterations and then reduced to zero following a cosine function.\nOur models were optimized with batch size of 16, and the training was terminated after 25 epochs.\nEvaluation Protocols. We follow prior work and use per-frame mean average precision (mAP)\nto evaluate the performance of online action detection. We also use per-frame calibrated average\nprecision (cAP)[14] that was proposed for TVSeries to correct the imbalance between positive and\nnegative samples, cAP = ‚àë\nkcPrec(k) ‚àóI(k)/P, where cPrec = TP/(TP + FP/w), I(k) is 1\nif frame kis a true positive, P is the number of true positives, and wis the negative and positive ratio.\n4.3 Comparison with the State-of-the-art Methods\nTable 1: Online action detection and anticipation resultson THUMOS‚Äô14 and TVSeries in terms\nof mAP and cAP, respectively. For online action detection, LSTR outperforms the state-of-the-art\nmethods on THUMOS‚Äô14 by 3.7% and 2.4% in mAP and on TVSeries by 2.8% and 2.7% in cAP,\nusing ActivityNet and Kinetics pretrained features, respectively. LSTR also achieves promising\nresults for action anticipation. *Results are reproduced by us using their papers‚Äô default settings.\n(a) Results of online action detec-\ntion using ActivityNet features\nTHUMOS‚Äô14 TVSeries\nmAP (%) mcAP (%)\nCDC [53] 44.4 -\nRED [22] 45.3 79.2\nTRN [72] 47.2 83.7\nFATS [33] 51.6 81.7\nIDN [19] 50.0 84.7\nLAP [46] 53.3 85.3\nTFN [20] 55.7 85.0\nLFB* [69] 61.6 84.8\nLSTR(ours) 65.3 88.1\n(b) Results of online action detec-\ntion using Kinetics features\nTHUMOS‚Äô14 TVSeries\nmAP (%) mcAP (%)\nFATS [33] 59.0 84.6\nIDN [19] 60.3 86.1\nTRN [72] 62.1 86.2\nPKD [77] 64.5 86.4\nWOAD [25] 67.1 -\nLFB* [69] 64.8 85.8\nLSTR(ours) 69.5 89.1\n(c) Results of action anticipation\nusing ActivityNet features\nTHUMOS‚Äô14 TVSeries\nmAP (%) mcAP (%)\nEFC [22] 34.4 72.5\nED [22] 36.6 74.5\nRED [22] 37.5 75.1\nTRN [72] 38.9 75.7\nTTM [65] 40.9 77.9\nLAP [46] 42.6 78.7\nLSTR(ours) 50.1 80.8\nTable 2: Online action detection results when only portions of videos are consideredin cAP (%)\non TVSeries (e.g., 80%-90% means only frames of this range of action instances were evaluated).\nLSTR outperforms existing methods at every time stage, especially on boundary locations.\nFeatures Portion of Video\n0-10% 10-20% 20-30% 30-40% 40-50% 50-60% 60-70% 70-80% 80-90% 90-100%\nTRN [72]\nActivityNet\n78.8 79.6 80.4 81.0 81.6 81.9 82.3 82.7 82.9 83.3\nIDN [19] 80.6 81.1 81.9 82.3 82.6 82.8 82.6 82.9 83.0 83.9\nTFN [20] 83.1 84.4 85.4 85.8 87.1 88.4 87.6 87.0 86.7 85.6\nLSTR(ours) 83.6 85.0 86.3 87.0 87.8 88.5 88.6 88.9 89.0 88.9\nIDN [19]\nKinetics\n81.7 81.9 83.1 82.9 83.2 83.2 83.2 83.0 83.3 86.6\nPKD [77] 82.1 83.5 86.1 87.2 88.3 88.4 89.0 88.7 88.9 87.7\nLSTR(ours) 84.4 85.6 87.2 87.8 88.8 89.4 89.6 89.9 90.0 90.1\nWe compare LSTR against other state-of-the-art methods [72, 19, 25] on THUMOS‚Äô14, TVSeries,\nand HACS Segment. SpeciÔ¨Åcally, on THUMOS‚Äô14 and TVSeries, we implement LSTR with the\nlong- and short-term memories of 512 and 8 seconds, respectively. On HACS Segment, we reduce\n6\nthe long-term memory to 256 seconds, considering that its videos are strictly shorter than 4 minutes.\nFor LSTR, we implement the two-stage memory compression using Transformer decoder units. We\nset the token numbers to n0 = 16and n1 = 32and the Transformer layers to ‚Ñìenc = 2and ‚Ñìdec = 2.\n4.3.1 Online Action Detection\nTHUMOS‚Äô14.We compare LSTR with recent work on THUMOS‚Äô14, including methods that use\n3D ConvNets [53] and RNNs [70, 19, 25], reinforcement learning [22], and curriculum learning [77].\nTable 1a and 1b shows that LSTR signiÔ¨Åcantly outperforms the the state-of-the-art methods [69, 25]\nby 3.7% and 2.4% in terms of mAP using ActivityNet and Kinetics pretrained features, respectively.\nTVSeries. Table 1a and 1b show the online action detection results that LSTR outperforms the\nstate-of-the-art methods [20, 77] by 2.8% and 2.7% in terms of cAP using ActivityNet and Kinetics\npretrained features, respectively. Following prior work [14], we also investigate LSTR‚Äôs performance\nat different action stages by evaluating each decile (ten-percent interval) of the video frames separately.\nTable 2 shows that LSTR outperforms existing methods at every stage of action instances.\nHACS Segment.LSTR achieves 82.6% on HACS Segment in term of mAP using Kinetics pretrained\nfeatures. Note that HACS Segment is a new large-scale dataset with only a few previous results.\nLSTR outperforms existing methods RNN [28] (77.6%) by 5.0% and TRN [72] (78.9%) by 3.7%.\n4.3.2 Action Anticipation\nWe extend the idea of LSTR to action anticipation for up to 2 seconds (i.e., 8 steps in 4 FPS) into the\nfuture. SpeciÔ¨Åcally, we concatenate another 8 learnable output tokens (with positional embedding)\nafter the short-term memory in the LSTR decoder to produce the prediction results accordingly.\nTable 1c shows that LSTR signiÔ¨Åcantly outperforms the state-of-the-art methods [72, 46] by 7.5%\nmAP on THUMOS and 2.1% cAP on TVSeries, using ActivityNet pretrained features.\n4.4 Design Choices of Long- and Short-Term Memories\nTable 3: Results of LSTR using downsampled long-term memoryon THUMOS‚Äô14 in mAP (%).\nIn particular, we use long-term memory of 512 seconds and short-term memory as 8 seconds.\nTemporal Stride 1 2 4 8 16 32 64 128\nLSTR 69.5 69.5 69.5 69.2 68.7 67.3 66.6 65.9\nWe experiment for design choices of long- and short-term memories. Unless noted otherwise, we use\nTHUMOS‚Äô14, which contains various video lengths, and Kinetics pretrained features.\nLengths of long- and short-term memories.We Ô¨Årst analyze the effect of different lengths of\nlong-term mL and short-term mS memory. In particular, we test mS ‚àà{4,8,16}seconds with mL\nstarting from 0 second (no long-term memory). Note that we choose the max length (1024 seconds for\nTHUMOS‚Äô14 and 256 seconds for HACS Segment) to cover lengths of 98% videos, and do not have\nproper datasets to test longer mL. Fig. 3 shows that LSTR is beneÔ¨Åcial from larger mL in most cases.\nIn addition, when mL is short (‚â§16 in our cases), using larger mS obtains better results and when\nmL is sufÔ¨Åcient (‚â•32 in our cases), increasing mS does not always guarantee better performance.\n0 8 16 32 64 128 256 512 1024\nLong-Term Memory Length (secs)\n63.0\n64.0\n65.0\n66.0\n67.0\n68.0\n69.0\n70.0mAP (%)\nEÔ¨Äect of Memory Length on THUMOS‚Äô14\n4 secs Short-Term Memory\n8 secs Short-Term Memory\n8 secs Short-Term Memory w/ RNN\n16 secs Short-Term Memory\n0 8 16 32 64 128 256\nLong-Term Memory Length (secs)\n75.0\n76.0\n77.0\n78.0\n79.0\n80.0\n81.0\n82.0\n83.0mAP (%)\nEÔ¨Äect of Memory Length on HACS Segment\n4 secs Short-Term Memory\n8 secs Short-Term Memory\n16 secs Short-Term Memory\nFigure 3: Effect of using different lengths of long- and short-term memories.\nCan we downsample long-term memory?We implement LSTR with mS as 8 seconds and mL as\n512 seconds, and test the effect of downsampling long-term memory. Table 3 shows the results that\ndownsampling with strides smaller than 4 does not cause performance drop, but more aggressive\n7\nstrides dramatically decrease the detection accuracy. Note that, when extracting frame features in\n4 FPS, both LSTR encoder (n0 = 16) and downsampling with stride 128 compress the long-term\nmemory to 16 features, but LSTR achieves much better performance (69.5% vs. 65.9% in mAP). This\ndemonstrates the effectiveness of our ‚Äúadaptive compression‚Äù compared to heuristics downsampling.\nCan we compensate reduced memory length with RNN?We note that LSTR‚Äôs performance\nnotably decreases when it can only access very limited memory (e.g., mL+ mS ‚â§16 seconds). Here\nwe test if RNN can compensate LSTR‚Äôs reduced memory or even fully replace the LSTR encoder.\nWe implement LSTR using mS = 8seconds with an extra Gated Recurrent Unit (GRU) [ 10] (its\narchitecture is visualized in the Supplementary Material) to capture all history outside the long- and\nshort-term memories. The dashed line in Fig. 3 shows the results. Plugging-in RNNs indeed improves\nthe performance when mL is small, but when mL is large (‚â•64 seconds), it does not improve the\naccuracy anymore. Note that RNNs are not used in any other experiments in this paper.\n4.5 Design Choices of LSTR\nTable 4: Results of different designs of the LSTR encoder and decoder. The length of short-term\nmemory is set to 8 seconds. ‚ÄúTR‚Äù denotes Transformer. The last row is our proposed LSTR design.\nLSTR Encoder LSTR Decoder Length of Long-Term Memory mL (secs)\n8 16 32 64 128 256 512 1024\nN/A TR Encoder 65.7 66.8 67.1 67.2 67.3 66.8 66.5 66.2\nN/A TR Decoder 66.5 67.3 67.7 68.1 68.3 67.9 67.0 66.5\nTR Encoder TR Decoder 65.9 66.4 66.7 67.4 67.5 67.2 67.0 66.6\nProjection Layer TR Decoder 66.2 67.1 67.4 67.7 67.5 67.2 66.9 66.8\nTR Decoder TR Decoder 66.1 67.1 67.4 68.0 68.5 68.6 68.7 68.7\nTR Decoder + TR Encoder TR Decoder 66.2 67.3 67.6 68.4 68.6 68.8 68.9 69.0\nTR Decoder + TR Decoder N/A 64.0 64.7 65.9 66.1 66.5 66.2 65.4 65.2\nTR Decoder+ TR Decoder TR Decoder 66.6 67.8 68.2 68.8 69.2 69.4 69.5 69.5\nWe continue to explore the design trade-offs of LSTR. Unless noted otherwise, we use short-term\nmemory of 8 seconds, long-term memory of 512 seconds, and Kinetics pretrained features.\nNumber of layers and tokens.First, we assess using different numbers of token embeddings (i.e.,\nn0 and n1) in LSTR encoder. Fig 4 (left) shows that LSTR is quite robust to different choices (the\nbest and worst performance gap is only about 1.5%), but using n0 = 16and n1 = 32gets highest\naccuracy. Second, we experiment for the effect of using different numbers of Transformer decoder\nunits (i.e., ‚Ñìenc and ‚Ñìdec). As shown in Fig 4 (right), LSTR does not need a large model to get the\nbest performance, and in practice, using more layers can cause overÔ¨Åtting.\n1 2 4 8 16 32 64\nn0\n67.5\n68.0\n68.5\n69.0\n69.5\n70.0mAP (%)\nDiÔ¨Äerent Number of Token Embeddings\nn1 = 16\nn1 = 32\nn1 = 64\n1 2 3 4 5 6\n‚Ñìdec\n67.5\n68.0\n68.5\n69.0\n69.5\n70.0mAP (%)\nDiÔ¨Äerent Number of Transformer Decoder Units\n‚Ñìenc = 1\n‚Ñìenc = 2\n‚Ñìenc = 4\nFigure 4: Left: Results of different number of token embeddings for our two-stage memory compres-\nsion. Right: Results of different number of Transformer decoder units for ‚Ñìenc and ‚Ñìdec.\nCan we unify the temporal modeling using only self-attention models?We test if long-term ML\nand short-term MS memory can be learned as a whole using self-attention models. SpeciÔ¨Åcally, we\nconcatenate ML and MS and feed them into a standard Transformer Encoder [61] with a similar\nmodel size to LSTR. Table 4 (row 8 vs. row 1) shows that LSTR achieves better performance\nespecially when mL is large (e.g., 69.5% vs. 66.5% when mL = 512and 66.6% vs. 65.7% when\nmL = 8). This shows the advantage of LSTR for temporal modeling on long- and short-term context.\nCan we remove the LSTR encoder?We explore this by directly feedingMLinto the LSTR decoder,\nand using MS as tokens to reference useful information. Table 4 shows that LSTR outperforms this\n8\nbaseline (row 8 vs. row 2), especially when mL is large. We also compare it with self-attention\nmodels (row 1) and observe that although neither of them can effectively model prolonged memory,\nthis baseline outperforms the Transformer Encoder. This also demonstrates the effectiveness of our\nidea of using short-term memory to query related context from long-range context.\nCan the LSTR encoder learn effectively using self-attention?To evaluate the ‚Äúbottleneck‚Äù design\nwith cross-attention in LSTR encoder, we try modeling ML using standard Transformer Encoder\nunits [61]. Note that this still captures ML and MS with the similar workÔ¨Çow of LSTR, but does\nnot compress and encode ML using learnable tokens. Table 4 (row 8 vs. row 3) shows that LSTR\noutperforms this baseline with all mL settings. In addition, the performance of this baseline decreases\nwhen mL gets larger, which suggests the superior ability of LSTR for modeling long-range patterns.\nHow to design the memory compression for the LSTR encoder?First, we use a projection layer,\nconsisting of a learnable matrix of size n0 √ómL followed by MLP layers, to compress the long-term\nmemory along the temporal dimension. Table 4 shows that using this simple projection layer (row 4)\nslightly outperforms the model without long-term memory (row 1), but is worse than attention-based\ncompression methods. Second, we evaluate the one-stage design with ‚Ñìenc + 1Transformer decoder\nunits. Table 4 (row 8 vs. row 5) shows that two-stage compression is stably better than one-stage,\nand their performance gap gets larger when using larger mL (0.5% when mL = 8and 0.8% when\nmL = 512). Third, we compare cross-attention and self-attention for two-stage compression by\nreplacing the second Transformer decoder with Transformer encoder. LSTR stably outperforms this\nbaseline (row 6) by about 0.5% in mAP. However, its performance is still better than models with\none-stage compression of about 1.3% (row 6 vs. row 3) and 0.3% (row 6 vs. row 5) on average.\nCan we remove the LSTR decoder?We remove the LSTR decoder to evaluate its contribution.\nSpeciÔ¨Åcally, we feed the entire memory to LSTR encoder and attach a multi-layer (MLP) classiÔ¨Åer\non its output tokens embeddings. Similar to the above experiments, we increase the model size to\nensure a fair comparison. Table 4 shows that LSTR outperforms this baseline (row 7) by about4%\non large mL (e.g., 512 and 1024) and about 2.5% on relative small mL (e.g., 8 and 16).\nTable 5: Results of LSTR using different memory integration methodsin mAP (%). Our pro-\nposed integration method using cross-attention stably outperforms the heuristic methods.\nMemory Integration Methods Length of Long-Term Memory mL (secs)\n8 16 32 64 128 256 512 1024\nAverage Pooling 66.1 67.0 67.3 67.5 68.2 68.4 68.6 68.6\nConcatenation 65.9 67.2 67.5 67.7 68.4 68.5 68.7 68.6\nCross-Attention (ours) 66.6 67.8 68.2 68.8 69.2 69.4 69.5 69.5\nCross-attention vs. heuristics for integrating long- and short-term memories.We explore inte-\ngrating the long- and short-term memories by using average pooling and concatenation. SpeciÔ¨Åcally,\nthe encoded long-term features of size n1 √óCis converted to a vector ofCelements by channel-wise\naveraging, and the short-term memory of size mS √óCis encoded by ‚Ñìdec Transformer encoder units.\nThen, each slot of the short-term features is either averaged or concatenated with the long-term feature\nvector for action classiÔ¨Åcation. Note that these models still beneÔ¨Åt from LSTR‚Äôs effectiveness for\nlong-term modeling. Table 5 shows that using average pooling and concatenation obtain comparable\nresults, but LSTR with cross-attention stably outperforms these baselines.\n4.6 Runtime\nTable 6: Runtime of LSTR with different design choices. The last row is our proposed LSTR design.\nLSTR Encoder LSTR Decoder\nFrames Per Second (FPS)\nOptFlow\nComputation\nRGB Feature\nExtraction\nOptFlow Feature\nExtraction LSTR\nN/A TR Encoder\n8.1 70.5 14.6\n43.2\nTR Encoder TR Decoder 50.2\nTR Decoder TR Decoder 59.5\nTR Decoder+ TR Decoder TR Decoder 91.6\nWe report LSTR‚Äôs runtime in frames per second (FPS) on a system with a single V100 GPU, and use\nthe videos from THUMOS‚Äô14 dataset. The results are shown in Table 6.\n9\nWe start by comparing the runtime between LSTR‚Äôs different design choices without considering\nthe pre-processing ( e.g., feature extraction). First, LSTR runs at 91.6 FPS using our two-stage\nmemory compression (row 4), whereas using the one-stage design runs at a slower 59.5 FPS (row\n3). Our two-stage design is more efÔ¨Åcient because it does not need to reference information from\nthe long-term memory multiple times, and can be further accelerated during online inference (see\nSec. 3.6). Second, we test the LSTR encoder using self-attention mechanisms (row 2). This design\ndoes not compress the long-term memory, thus increasing the computational cost of both the LSTR\nencoder and decoder, leading to a slower speed of 50.2 FPS. Third, we test the standard Transformer\nEncoder [61] (row 1), whose runtime speed, 43.2 FPS, is about 2√óslower than LSTR.\nWe also compare LSTR with state-of-the-art recurrent models. As we are not aware of any prior\nwork that reports their runtime, we test TRN [ 72] using their ofÔ¨Åcial open-source code [ 2]. The\nresult shows that TRN runs at 123.3 FPS, which is faster than LSTR. This is because recurrent\nmodels abstract the entire history as a compact representation but LSTR needs to process much more\ninformation. On the other hand, LSTR achieves much higher performance, outperforming TRN by\nabout 7.5% in mAP on THUMOS‚Äô14 and about4.5% in cAP on TVSeries.\nFor end-to-end online inference, we follow the state-of-the-art methods [72, 19, 25] and build LSTR\non two-stream features [62]. LSTR together with pre-processing techniques run at 4.6 FPS. Table 6\nshows that the speed bottleneck is the motion feature extraction ‚Äî it accounts for about 90% of the\ntotal runtime including the optical Ô¨Çow computation with DenseFlow [ 63]. One can improve the\nefÔ¨Åciency largely by using real-time optical Ô¨Çow extractors (e.g., PWC-Net [55]) or using only visual\nfeatures extracted by a light-weight backbone (e.g., MobileNet [29] and FBNet [67]).\n4.7 Error Analysis\nTable 7: Action classes with highest and lowest performanceon THUMOS‚Äô14.\nAction Classes HammerThrow PoleVault LongJump Diving BaseballPitch FrisbeeCatch Billiards CricketShot\nAP (%) 92.8 89.7 86.9 86.7 55.4 49.4 39.8 38.6\nFigure 5: Failure caseson THUMOS‚Äô14. Action classes from left to right are ‚ÄúBaseballPitch‚Äù,\n‚ÄúFrisbeeCatch‚Äù, ‚ÄúBilliards‚Äù, and ‚ÄúCricketShot‚Äù. Red circle indicates where the action is happening.\nIn Table 7, we list the action classes from THUMOS‚Äô14 where LSTR gets the highest (color green)\nand the lowest (color red) per-frame APs. In Fig. 5, we illustrate four sample frames with incorrect\npredictions. More visualizations are included in the Supplementary Material. We observe that LSTR\nsees a decrease in detection accuracy when the action incurs only tiny motion or the subject is very\nfar away from the camera, but excels at recognizing actions with long temporal span and multiple\nstages, such as ‚ÄúPoleVault‚Äù and ‚ÄúLong Jump‚Äù. This suggests we may explore extending the temporal\nmodeling capability of LSTR to both spatial and temporal domains.\n5 Conclusion\nWe present LSTR which captures both long- and short-term correlations in past observations of a\ntime series by compressing long-term memory into encoded latent features and referencing related\ntemporal context from them with short-term memory. This demonstrates the importance of separately\nmodeling long- and short-term information and then integrating them for online inference tasks.\nExperiments on multiple datasets and ablation studies validate the effectiveness and efÔ¨Åciency of\nthe LSTR design in dealing with prolonged video sequences. However, we note that LSTR is\noperating only on the temporal dimension. An end-to-end video understanding system requires\nsimultaneous spatial and temporal modeling for optimal results. Therefore extending the idea of\nLSTR to spatio-temporal modeling remains an open yet challenging problem.\n6 Acknowledgments and Disclosure of Funding\nWe thank the anonymous reviewers for their helpful suggestions. This work was funded by Amazon.\n10\n7 Appendix\n7.1 Qualitative Results\nFig. 6a shows some qualitative results. We can see that, in most cases, LSTR can quickly recognize\nthe actions and make relatively consistent predictions for each action instance. Two typical failure\ncases are shown in Fig. 6b. The top sample contains the ‚ÄúBilliards‚Äùaction that incurs only tiny motion.\nAs discussed in Sec. 4.7, LSTR‚Äôs detection accuracy is observed to decrease on this kind of actions.\nThe bottom sample is challenging ‚Äî the ‚ÄúOpen door‚Äù action occurs behind the female reporter and is\nbarely visible. Red circle indicates where the action is happening in each frame.\nBackground Background\nGround Truth\nBackground Background\nGround Truth\nDrink\nPour\nBackground Background\nBackground Background\nGround Truth\nGround Truth\nDiving\nBasketballDunk\n(a) Qualitative resultson THUMOS‚Äô14 (top two samples) and TVSeries (bottom two samples).\nBackground Background\nBackground Background\nGround Truth\nGround Truth\nOpen door\nBilliards\n(b) Failure caseson THUMOS‚Äô14 (top sample) and TVSeries (bottom sample).\nFigure 6: Qualitative results and failure cases of LSTR. The curves indicate the predicted scores\nof the ground truth and ‚Äúbackground‚Äù classes. (Best viewed in color.)\n11\n7.2 Can we compensate reduced memory length with RNN? Cont‚Äôd\nLSTR DecoderLSTR EncoderLong-Term Memory\nShort-Term Memory\nT\nStore\nClassification\nat time T\nStore\nConcat\nGRU\nLong Short-term TRansformer (LSTR)\nFigure 7: Overview of the architecture of using LSTR with an extra GRU.\nTo better understand the design choice in Sec. 4.4, we show its overall structure in Fig. 7. SpeciÔ¨Åcally,\nin addition to the long- and short-term memories, we use an extra GRU to capture all the history\n‚Äúoutside‚Äù the long- and short-term memories as a compact representation, g ‚ààR1√óC. We then\nconcatenate the outputs of the LSTR encoder and the GRU as more comprehensive temporal features\nof size (n1 + 1)√óC, and feed them into the LSTR decoder as input tokens.\n7.3 Action Anticipation Cont‚Äôd\nLSTR Decoder\nLong-Term Memory\nShort-Term Memory\nT\nStore\nStore\nConcat\nLSTR Encoder\nLong Short-term TRansformer (LSTR)\nùëö! Token Embeddings Anticipation\ntime T+1 to T+ùëö!\nFigure 8: Overview of the architecture of using LSTR for action anticipation.\nIn Sec. 4.3.2, we extended the idea of LSTR to action anticipation for up to 2 seconds into the future,\nand compared it with state-of-the-art methods in Table 1c. Here we provide more details about its\nstructure. As shown in Fig. 8, we concatenate mF token embeddings after the short-term memory\nas mS + mF output tokens into the LSTR decoder. These mF ‚Äúanticipation tokens‚Äù are added\nwith the positional embedding and directional attention mask together with the short-term memory.\nThen the outputs of the mF tokens make predictions for the next mF steps accordingly. As LSTR‚Äôs\nperformance is evaluated in 4 FPS (see Sec. 4.2), mF is set to 8 for action anticipation of 2 seconds.\nReferences\n[1] http://pytorch.org/.\n[2] https://github.com/xumingze0308/TRN.pytorch.\n[3] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu Àáci¬¥c, and Cordelia\nSchmid. Vivit: A video vision transformer. arXiv:2103.15691, 2021.\n[4] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for\nvideo understanding? arXiv:2102.05095, 2021.\n[5] Shyamal Buch, Victor Escorcia, Chuanqi Shen, Bernard Ghanem, and Juan Carlos Niebles.\nSST: Single-stream temporal action proposals. In CVPR, 2017.\n[6] Mikhail S Burtsev, Yuri Kuratov, Anton Peganov, and Grigory V Sapunov. Memory transformer.\narXiv:2006.11527, 2020.\n[7] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020.\n12\n[8] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? A new model and the\nKinetics dataset. In CVPR, 2017.\n[9] Marvin M Chun. Visual working memory as visual attention sustained internally over time.\nNeuropsychologia, 2011.\n[10] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. arXiv:1412.3555, 2014.\n[11] MMAction2 Contributors. Openmmlab‚Äôs next generation video understanding toolbox and\nbenchmark. https://github.com/open-mmlab/mmaction2, 2020.\n[12] Nelson Cowan. Attention and memory: An integrated framework. Oxford University Press,\n1998.\n[13] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov.\nTransformer-XL: Attentive language models beyond a Ô¨Åxed-length context. ACL, 2019.\n[14] Roeland De Geest, Efstratios Gavves, Amir Ghodrati, Zhenyang Li, Cees Snoek, and Tinne\nTuytelaars. Online action detection. In ECCV, 2016.\n[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of\ndeep bidirectional transformers for language understanding. NAACL, 2019.\n[16] Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini\nVenugopalan, Kate Saenko, and Trevor Darrell. Long-term recurrent convolutional networks\nfor visual recognition and description. In CVPR, 2015.\n[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.\nAn image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021.\n[18] Jeffrey L Elman. Finding structure in time. Cognitive science, 1990.\n[19] Hyunjun Eun, Jinyoung Moon, Jongyoul Park, Chanho Jung, and Changick Kim. Learning to\ndiscriminate information for online action detection. In CVPR, 2020.\n[20] Hyunjun Eun, Jinyoung Moon, Jongyoul Park, Chanho Jung, and Changick Kim. Temporal\nÔ¨Åltering networks for online action detection. Pattern Recognition, 2021.\n[21] Christoph Feichtenhofer, Axel Pinz, and Richard P Wildes. Spatiotemporal multiplier networks\nfor video action recognition. In CVPR, 2017.\n[22] Jiyang Gao, Zhenheng Yang, and Ram Nevatia. RED: Reinforced encoder-decoder networks\nfor action anticipation. In BMVC, 2017.\n[23] Jiyang Gao, Zhenheng Yang, Chen Sun, Kan Chen, and Ram Nevatia. TURN TAP: Temporal\nunit regression network for temporal action proposals. In ICCV, 2017.\n[24] Mingfei Gao, Mingze Xu, Larry S Davis, Richard Socher, and Caiming Xiong. Startnet: Online\ndetection of action start in untrimmed videos. In ICCV, 2019.\n[25] Mingfei Gao, Yingbo Zhou, Ran Xu, Richard Socher, and Caiming Xiong. WOAD: Weakly\nsupervised online action detection in untrimmed videos. In CVPR, 2021.\n[26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In CVPR, 2016.\n[27] Minh Hoai and Fernando De la Torre. Max-margin early event detectors. In IJCV, 2014.\n[28] Sepp Hochreiter and J√ºrgen Schmidhuber. Long short-term memory. Neural Computation,\n1997.\n[29] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias\nWeyand, Marco Andreetto, and Hartwig Adam. Mobilenets: EfÔ¨Åcient convolutional neural\nnetworks for mobile vision applications. arXiv:1704.04861, 2017.\n13\n[30] Haroon Idrees, Amir R Zamir, Yu-Gang Jiang, Alex Gorban, Ivan Laptev, Rahul Sukthankar,\nand Mubarak Shah. The THUMOS challenge on action recognition for videos ‚Äúin the wild‚Äù.\nCVIU, 2017.\n[31] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training\nby reducing internal covariate shift. In ICML, 2015.\n[32] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao\nCarreira. Perceiver: General perception with iterative attention. arXiv:2103.03206, 2021.\n[33] Young Hwi Kim, Seonghyeon Nam, and Seon Joo Kim. Temporally smooth online action\ndetection using cycle-consistent future anticipation. Pattern Recognition, 2021.\n[34] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR,\n2015.\n[35] Anastasia Kiyonaga and Tobias Egner. Working memory as internal attention: Toward an\nintegrative account of internal and external selection processes. Psychonomic bulletin & review,\n2013.\n[36] Xinyu Li, Yanyi Zhang, Chunhui Liu, Bing Shuai, Yi Zhu, Biagio Brattoli, Hao Chen, Ivan\nMarsic, and Joseph Tighe. Vidtr: Video transformer without convolutions. arXiv:2104.11746,\n2021.\n[37] Tianwei Lin, Xu Zhao, Haisheng Su, Chongjing Wang, and Ming Yang. BSN: Boundary\nsensitive network for temporal action proposal generation. In ECCV, 2018.\n[38] Tianwei Lin, Xiao Liu, Xin Li, Errui Ding, and Shilei Wen. BMN: Boundary-matching network\nfor temporal action proposal generation. In ICCV, 2019.\n[39] Yuan Liu, Lin Ma, Yifeng Zhang, Wei Liu, and Shih-Fu Chang. Multi-granularity generator for\ntemporal action proposal. In CVPR, 2019.\n[40] Shugao Ma, Leonid Sigal, and Stan Sclaroff. Learning activity progression in lstms for activity\ndetection and early detection. In CVPR, 2016.\n[41] Antoine Miech, Ivan Laptev, and Josef Sivic. Learnable pooling with context gating for video\nclassiÔ¨Åcation. arXiv:1706.06905, 2017.\n[42] Megha Nawhal and Greg Mori. Activity graph transformer for temporal action localization.\narXiv:2101.08540, 2021.\n[43] Daniel Neimark, Omri Bar, Maya Zohar, and Dotan Asselmann. Video transformer network.\narXiv:2102.00719, 2021.\n[44] Klaus Oberauer. Working memory and attention‚Äìa conceptual analysis and review. Journal of\ncognition, 2019.\n[45] Seoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon Joo Kim. Video object segmentation\nusing space-time memory networks. In CVPR, 2019.\n[46] Sanqing Qu, Guang Chen, Dan Xu, Jinhu Dong, Fan Lu, and Alois Knoll. LAP-Net: Adaptive\nfeatures sampling via learning action progression for online action detection. arXiv:2011.07915,\n2020.\n[47] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\nunderstanding by generative pre-training. 2018.\n[48] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive\ntransformers for long-range sequence modelling. ICLR, 2020.\n[49] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by\nback-propagating errors. Nature, 1986.\n[50] Mike Schuster and Kuldip K Paliwal. Bidirectional recurrent neural networks. ITSS, 1997.\n14\n[51] Gilad Sharir, Asaf Noy, and Lihi Zelnik-Manor. An image is worth 16x16 words, what is a\nvideo worth? arXiv:2103.13915, 2021.\n[52] Zheng Shou, Dongang Wang, and Shih-Fu Chang. Temporal action localization in untrimmed\nvideos via multi-stage cnns. In CVPR, 2016.\n[53] Zheng Shou, Jonathan Chan, Alireza Zareian, Kazuyuki Miyazawa, and Shih-Fu Chang. CDC:\nConvolutional-de-convolutional networks for precise temporal action localization in untrimmed\nvideos. In CVPR, 2017.\n[54] Zheng Shou, Junting Pan, Jonathan Chan, Kazuyuki Miyazawa, Hassan Mansour, Anthony\nVetro, Xavier Giro-i Nieto, and Shih-Fu Chang. Online action detection in untrimmed, streaming\nvideos-modeling and evaluation. In ECCV, 2018.\n[55] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. PWC-Net: Cnns for optical Ô¨Çow\nusing pyramid, warping, and cost volume. In CVPR, 2018.\n[56] Jing Tan, Jiaqi Tang, Limin Wang, and Gangshan Wu. Relaxed transformer decoders for direct\naction proposal generation. arXiv:2102.01894, 2021.\n[57] Yongyi Tang, Xing Zhang, Lin Ma, Jingwen Wang, Shaoxiang Chen, and Yu-Gang Jiang.\nNon-local netvlad encoding for video classiÔ¨Åcation. In ECCVW, 2018.\n[58] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles,\nand Herv√© J√©gou. Training data-efÔ¨Åcient image transformers & distillation through attention.\narXiv:2012.12877, 2020.\n[59] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning\nspatiotemporal features with 3d convolutional networks. In ICCV, 2015.\n[60] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A\ncloser look at spatiotemporal convolutions for action recognition. In CVPR, 2018.\n[61] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv:1706.03762, 2017.\n[62] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool.\nTemporal segment networks: Towards good practices for deep action recognition. In ECCV,\n2016.\n[63] Shiguang Wang*, Zhizhong Li*, Yue Zhao, Yuanjun Xiong, Limin Wang, and Dahua Lin.\ndenseÔ¨Çow. https://github.com/open-mmlab/denseflow, 2020.\n[64] Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention\nwith linear complexity. arXiv:2006.04768, 2020.\n[65] Wen Wang, Xiaojiang Peng, Yanzhou Su, Yu Qiao, and Jian Cheng. TTPP: Temporal transformer\nwith progressive prediction for efÔ¨Åcient action anticipation. Neurocomputing, 2021.\n[66] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks.\nIn CVPR, 2018.\n[67] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong\nTian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. FBNet: Hardware-aware efÔ¨Åcient convnet\ndesign via differentiable neural architecture search. In CVPR, 2019.\n[68] Chao-Yuan Wu and Philipp Kr√§henb√ºhl. Towards long-form video understanding. In CVPR,\n2021.\n[69] Chao-Yuan Wu, Christoph Feichtenhofer, Haoqi Fan, Kaiming He, Philipp Krahenbuhl, and\nRoss Girshick. Long-term feature banks for detailed video understanding. In CVPR, 2019.\n[70] Huazhe Xu, Yang Gao, Fisher Yu, and Trevor Darrell. End-to-end learning of driving models\nfrom large-scale video datasets. In CVPR, 2017.\n15\n[71] Huijuan Xu, Abir Das, and Kate Saenko. R-C3D: Region convolutional 3d network for temporal\nactivity detection. In ICCV, 2017.\n[72] Mingze Xu, Mingfei Gao, Yi-Ting Chen, Larry S Davis, and David J Crandall. Temporal\nrecurrent networks for online action detection. In ICCV, 2019.\n[73] Yu Yao, Mingze Xu, Yuchen Wang, David J Crandall, and Ella M Atkins. Unsupervised trafÔ¨Åc\naccident detection in Ô¨Årst-person videos. In IROS, 2019.\n[74] Joe Yue-Hei Ng, Matthew Hausknecht, Sudheendra Vijayanarasimhan, Oriol Vinyals, Rajat\nMonga, and George Toderici. Beyond short snippets: Deep networks for video classiÔ¨Åcation. In\nCVPR, 2015.\n[75] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti,\nSantiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird:\nTransformers for longer sequences. In NeurIPS, 2020.\n[76] Hang Zhao, Antonio Torralba, Lorenzo Torresani, and Zhicheng Yan. Hacs: Human action clips\nand segments dataset for recognition and temporal localization. In ICCV, 2019.\n[77] Peisen Zhao, Jiajie Wang, Lingxi Xie, Ya Zhang, Yanfeng Wang, and Qi Tian. Privileged\nknowledge distillation for online action detection. arXiv:2011.09158, 2020.\n[78] Yue Zhao, Yuanjun Xiong, Limin Wang, Zhirong Wu, Xiaoou Tang, and Dahua Lin. Temporal\naction detection with structured segment networks. In ICCV, 2017.\n16"
}