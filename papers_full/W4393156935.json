{
  "title": "How Teachers Can Use Large Language Models and Bloom’s Taxonomy to Create Educational Quizzes",
  "url": "https://openalex.org/W4393156935",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A3153151626",
      "name": "Sabina Elkins",
      "affiliations": [
        "McGill University"
      ]
    },
    {
      "id": "https://openalex.org/A317237851",
      "name": "Ekaterina Kochmar",
      "affiliations": [
        "Kord Technologies (United States)",
        "Mohamed bin Zayed University of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A4287891628",
      "name": "Jackie C. K. Cheung",
      "affiliations": [
        "Canadian Institute for Advanced Research",
        "McGill University"
      ]
    },
    {
      "id": "https://openalex.org/A2263368991",
      "name": "Iulian Serban",
      "affiliations": [
        "Kord Technologies (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3153151626",
      "name": "Sabina Elkins",
      "affiliations": [
        "McGill University",
        "Kord Technologies (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A317237851",
      "name": "Ekaterina Kochmar",
      "affiliations": [
        "Kord Technologies (United States)",
        "Mohamed bin Zayed University of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A4287891628",
      "name": "Jackie C. K. Cheung",
      "affiliations": [
        "Canadian Institute for Advanced Research",
        "McGill University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6683908837",
    "https://openalex.org/W2989613245",
    "https://openalex.org/W4294214797",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W222164142",
    "https://openalex.org/W4318480794",
    "https://openalex.org/W2102065370",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2046116203",
    "https://openalex.org/W3197661515",
    "https://openalex.org/W4288059451",
    "https://openalex.org/W4288059420",
    "https://openalex.org/W4229084000",
    "https://openalex.org/W4382567310",
    "https://openalex.org/W4323655724",
    "https://openalex.org/W4225387478",
    "https://openalex.org/W4386290290",
    "https://openalex.org/W2162821268"
  ],
  "abstract": "Question generation (QG) is a natural language processing task with an abundance of potential benefits and use cases in the educational domain. In order for this potential to be realized, QG systems must be designed and validated with pedagogical needs in mind. However, little research has assessed or designed QG approaches with the input of real teachers or students. This paper applies a large language model-based QG approach where questions are generated with learning goals derived from Bloom's taxonomy. The automatically generated questions are used in multiple experiments designed to assess how teachers use them in practice. The results demonstrate that teachers prefer to write quizzes with automatically generated questions, and that such quizzes have no loss in quality compared to handwritten versions. Further, several metrics indicate that automatically generated questions can even improve the quality of the quizzes created, showing the promise for large scale use of QG in the classroom setting.",
  "full_text": "How Teachers Can Use Large Language Models and Bloom’s Taxonomy\nto Create Educational Quizzes\nSabina Elkins1,2, Ekaterina Kochmar2, 3, Jackie C.K. Cheung1, 4, Iulian Serban2\n1McGill University & MILA\n2Korbit Technologies Inc.\n3MBZUAI\n4Canada CIFAR AI Chair\nAbstract\nQuestion generation (QG) is a natural language pro-\ncessing task with an abundance of potential benefits\nand use cases in the educational domain. In order\nfor this potential to be realized, QG systems must\nbe designed and validated with pedagogical needs in\nmind. However, little research has assessed or de-\nsigned QG approaches with the input from real teach-\ners or students. This paper applies a large language\nmodel-based QG approach where questions are gener-\nated with learning goals derived from Bloom’s taxon-\nomy. The automatically generated questions are used\nin multiple experiments designed to assess how teach-\ners use them in practice. The results demonstrate that\nteachers prefer to write quizzes with automatically\ngenerated questions, and that such quizzes have no\nloss in quality compared to handwritten versions. Fur-\nther, several metrics indicate that automatically gen-\nerated questions can even improve the quality of the\nquizzes created, showing the promise for large scale\nuse of QG in the classroom setting.\nIntroduction\nQuestion generation (QG) is a popular natural language pro-\ncessing (NLP) task. The goal is to generate natural-language\nquestions that are useful and fluent. Many approaches also\nattempt to generate the corresponding answers, or use the\nanswer to generate the question (Kurdi et al. 2020; Mulla\nand Gharpure 2023). Due to their recent success in NLP,\nrecent QG research has been dominated by the use of\nTransformer-based large language models (LLMs) (Kurdi\net al. 2020; Liu et al. 2023).\nAn obvious use case for QG is educational applications. A\nrobust QG system could, for example, reduce the time spent\nby teachers to create educational content such as homework,\nquizzes, tests, in-class learning activities, and more. Alter-\nnatively, it could serve as a practice tool for students. The\nrange of potential uses for educational question generation\n(EQG) is expansive, especially considering the recent suc-\ncess of LLMs (Kasneci et al. 2023; Kurdi et al. 2020).\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nUnfortunately, there is minimal documented real-world\ndeployment of such systems (Kasneci et al. 2023; Kurdi\net al. 2020). Potential reasons for this lack of adoption may\ninclude poor performance of older approaches, rigidity of\nthe systems, and mistrust of users. Wang et al. (2022a) con-\nduct a need-finding study with the aim to explore why QG\nsystems are not being used in classrooms. One of their key\nfindings is that QG systems must meet the needs of the ed-\nucators who are using them in order to be effective and\nadopted. To achieve this, it is imperative that research in\ndeveloping educational QG systems takes into account the\nopinions of their end users.\nGenerate questions in each level of Bloom’s\ntaxonomy.\nPassage: {example_context}\nRemembering = {example_question}\nUnderstanding = {example_question}\nApplying = {example_question}\nAnalyzing = {example_question}\nEvaluating = {example_question}\nCreating = {example_question}\nPassage: {context}\nRemembering =\n(a) Controlled prompt template. Due to space limits, a one-shot\ntemplate is demonstrated here. In actuality, five examples (with\ndifferent contexts and questions) are used.\nCandidates generated using a context about\nconvergent evolution:\nRemembering: What is convergent evolution?\nApplying: Can you provide an example of\nconvergent evolution?\nAnalyzing: What is the difference between\nanalogous and homologous structures or\ntraits?\n(b) Controlled prompting strategy generation examples. 1 All 6\nlevels of Bloom’s taxonomy have generations for each input con-\ntext, omitted here to save space.\nFigure 1: Controlled prompting strategy.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23084\nGiven the success of LLMs in other tasks, our hypoth-\nesis is that they can generate different types of questions\nfrom a given context that teachers find useful for creating a\nquiz with quality comparable to a handwritten version. Fur-\nther, we predict that teachers will find the generated can-\ndidates more useful when they are generated to correspond\nwith the levels of Bloom’s taxonomy (Krathwohl 2002). Fig-\nure 1 depicts our few-shot prompting strategy to generate\neducational questions corresponding to Bloom’s taxonomy\n(more details about this approach can be found in Section\n). In order to evaluate our predictions, we conducted quiz\nwriting experiments designed to compare and contrast three\ndifferent quiz types. Multiple aspects of ‘usefulness’ of a\nquiz and quiz writing approach are considered, including\nthe quality of the resulting quiz , the efficiency (w.r.t. time),\nand the teacher’s preferences. Our results show that the three\ntypes of quizzes are of similar quality. Some metrics demon-\nstrate an improvement in quality when automatically gener-\nated questions are used. We also find that teachers have a\nstrong preference for writing quizzes with the help of gen-\nerated questions corresponding to Bloom’s taxonomy. These\nresults demonstrate the huge potential of EQG for real-world\nclassrooms and the importance of considering the needs of\nteachers when designing an EQG approach.\nBackground\nRecent research in EQG, and QG more generally, revolves\naround the use of Transformer-based LLMs. These LLMs\nare deep learning models trained on massive corpora of data\nto improve their generative performance (Zhang et al. 2022).\nThe reason for applying this approach in QG research is in\nlarge part due to its significant performance improvements\nover earlier rule-based and other types of systems (Kurdi\net al. 2020; Steuer et al. 2021; Mulla and Gharpure 2023).\nThe typical training goal for Transformer-based LLMs is\nnext-token prediction, meaning that they learn to predict a\nprobable completion to an initial input text. Recent models\nhave begun to also include reinforcement learning in their\ntraining procedure. This is the case for GPT-3.5, which\nis the LLM used in the experiments reported in this paper.\nFine-tuning with reinforcement learning from human feed-\nback allows GPT-3.5 to outperform its predecessors in the\nGPT family (Ouyang et al. 2022).\nAligning with the common LLM training objective of the\nnext-token prediction, the emerging paradigm for QG is to\nfeed a textual input, called a prompt, to an LLM for the\nmodel to complete (Mulla and Gharpure 2023). Designing\nthis prompt to generate a desired output can be a difficult\ntask, which has resulted in a new research direction called\nprompt engineering. One of the most common approaches\nto prompt engineering involves prepending a string to the\ncontext given to an LLM for generation, which is called a\nprefix style prompt (Liu et al. 2023). For instance, say a ma-\nchine learning teacher wished to generate questions about\ngradient descent. A simple strategy they could apply is to\nprompt an LLM with the input:\nGenerate a question about gradient descent.\nTo increase the specificity of the generated questions, the\nteacher could provide more context. For example, they\nmight instead write a prompt with a textbook passage about\na specific aspect of gradient descent, such as:\nGenerate a question from the following passage\n:\n<...>\nTo further control generation, the teacher’s input could con-\ntain a control element – a keyword that will guide the gen-\neration (Mulla and Gharpure 2023). For instance, they could\nprompt an LLM with:\nGenerate a multiple-choice question\nfrom the follow-\ning passage: <...>\nIn this simple example, we have presented three differ-\nent prompts, all of which can produce different questions.\nAdding in different word choices and other strategies for\ncontrolling the generation quickly makes prompt engineer-\ning a complex problem to optimize.\nAnother aspect in prompt engineering is the inclusion of\nexamples of the desired output format and style within the\nprompt itself. This is often calledfew-shot learning. In brief,\nfew-shot learning prompts consist of an instruction, a few\nexamples, and the task at hand. The examples are used to\nadapt LLMs to unseen scenarios without additional training\nor fine-tuning (Liu et al. 2023). For instance, following the\nearlier example, a teacher might prompt an LLM to gener-\nate a true-or-false type of question by including examples of\nsuch questions.\nThe ability to add more specificity to educational QG has\nallowed researchers to generate questions at different levels\nof difficulty, with different pedagogical goals in mind, and\nmore. For example, Wang et al. (2022b) try a collection of\ndifferent prompting strategies in an effort to optimize ed-\nucational QG. They conclude that using shorter input con-\ntexts and few-shot learning results in higher quality candi-\ndate questions. Recently, Elkins et al. (2023) have demon-\nstrated how to generate questions at different levels of ques-\ntion taxonomies, which are organizational structures taken\nfrom pedagogical literature, such as Bloom’s taxonomy of\nlearning goals. The authors demonstrate successful genera-\ntion of questions at various complexity levels and with dif-\nferent learning goals.\nDespite these exciting recent research developments with\neducational QG, there are only a few documented cases of\nthese techniques being used in real-world classrooms (Kas-\nneci et al. 2023; Kurdi et al. 2020). As previously mentioned,\nWang et al. (2022a) find that the lack of alignment between\nresearch goals and what teachers actually need and want\nfrom EQG is the reason that such systems are not deployed\nin real-world classrooms. Prior work aiming to explore the\nneeds, opinions, and attitude of teachers and students to-\nwards using automatically generated content is few and far\nbetween. Nevertheless, there exist a few relevant precedent\npapers:\n• The work by Van Campenhout, Hubertz, and Johnson\n(2022) outlines a NLP system for translating textbooks\ninto interactive courseware. The authors conduct a large\nuser study which shows equal student performance on\nmachine-generated and human-written questions. How-\never, their QG system is mostly rule-based, and their\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23085\ngenerated questions are relatively simple (i.e., concept-\nmatching and fill-in-the-blank).\n• The aforementioned work by Elkins et al. (2023) evalu-\nated their generated candidate questions with real teach-\ners in an effort to more accurately assess their pedagogi-\ncal usefulness. They find that their generated questions\nare highly rated by teachers. However, this work only\nevaluates questions at the individual question-level rather\nthan at a quiz-level.\n• The work by Laban et al. (2022) moves beyond question-\nlevel evaluations to a quiz writing task, similar to our\nwork in this paper. The authors design a task where teach-\ners make a quiz exclusively with candidate questions that\nwere automatically generated. The teachers also marked\nthe candidates as acceptable or not as they went; how-\never, the final global acceptance rate was only 52%.\nThus, while the authors take important steps towards the\nevaluation of QG in a realistic scenario, their generations\nthemselves appear to leave room for improvement.\nMethods\nThe EQG in this work was conducted by prompting\nGPT-3.5 to generate questions from a given input passage.\nTwo different strategies are used in order to compare the\npedagogically designed generation approach to a more sim-\nplistic approach to QG. The strategies are referred to ascon-\ntrolled and simple, respectively. Individually, both types of\ncandidate generations are supplied to teachers in controlled\nexperiments to assess their usefulness in practice (the details\nof these experiments are explained in Section ). The follow-\ning subsections will cover details of the input contexts, and\nthe two prefix style prompting strategies used.\nContexts\nCandidate educational questions were generated from a set\nof 24 passages sourced from Wikipedia, with each passage\ncontaining 5 context paragraphs, 6 to 9 sentences in length.\nThe total number of input contexts used to generate ques-\ntions is, thus, 120. The length of these contexts was deter-\nmined by empirical results in preliminary work. This set\nof passages was manually gathered. The specific Wikipedia\narticles were generally chosen via hyperlinks from the do-\nmain’s main Wikipedia or glossary page in order to ensure\nthey were relevant to the foundations of the given domain.\nTwo domains were used: biology (BIO) and machine learn-\ning (ML). More than one domain is included in order to take\nsteps towards demonstrating domain-agnostic results. Each\ndomain has 12 passages, totaling 60 input contexts. The con-\ntexts underwent minor pre-processing before use: this in-\ncluded removal of citations, hyperlinks, footnotes and pho-\nnetic spellings, re-formatting of full sentence bullet-point\nlists into paragraphs, and other minor data cleaning steps.1\n1 The input contexts, more details about Bloom’s taxonomy,\nthe human authored few-shot examples, all of the generated can-\ndidates and quizzes, the annotator demographics, and more can\nbe found at https://anonymous.4open.science/r/EQG in practice-\n2752/README.md.\nSimple Prompting Strategy\nThe simple prompting strategy uses a generic strategy to\ngenerate questions with GPT-3.52 in an attempt to as-\nsess how well the model can generate pedagogically use-\nful questions without any additional prompt engineering.\nThe prompt template can be seen in Figure 2. The num-\nber of questions generated at once is six in order to pro-\nduce the same amount of questions as thecontrolled strategy\ndescribed below. Empirical results from preliminary exper-\nimentation showed that generating all of the questions to-\ngether produced more diverse outputs, whereas generating\nthem separately produced duplicate questions.\nGenerate 6 questions.\nPassage: {context}\nQuestions:\n(a) Simple prompt template.\nCandidates generated using a context about\nconvergent evolution:\nWhat is the definition of convergent\nevolution?\nWhat are some common functions found in\nbird, bat, and pterosaur wings?\n(b) Simple prompting strategy generation examples. 1 There are\nactually 6 generations, omitted here to save space.\nFigure 2: Simple prompting strategy.\nControlled Prompting Strategy\nThe controlled prompting strategy uses a pedagogical ques-\ntion taxonomy to generate questions with different learning\ngoals in mind. Bloom’s taxonomy is a popular framework\nfor categorizing learning objectives in educational materi-\nals (Krathwohl 2002). The taxonomy contains six levels of\nlearning, arranged in a hierarchical order from ‘lower’- to\n‘higher’-level thinking skills: remembering, understanding,\napplying, analyzing, evaluating, and creating.1 Bloom’s tax-\nonomy helps teachers design instructional content that tar-\ngets specific learning goals.\nThe generation strategy used in this paper is designed\nto reduce the overlap between generated candidates. Rather\nthan generating one question and taxonomic level at a time,\nall six questions for a given context are generated at once,\nas shown in Figure 1. In preliminary experimentation, this\napproach empirically showed a reduction in the generation\nof identical questions and a greater diversity and adherence\nto the taxonomic levels.\nThe controlled prompting strategy also uses few-shot\nlearning. Following success with preliminary experimen-\ntation, five-shot learning (i.e., five examples within the\nprompt) is used. As seen in Figure 1, a single example\ncontains a prompt and six questions, one for each level of\n2The model text-davinci-003 was accessed through the\nOpenAI API. The dates on which the model was queried are avail-\nable in the supplementary material.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23086\nBloom’s taxonomy. The examples were handcrafted by a do-\nmain expert, and reviewed by another expert for both ques-\ntion quality and adherence to the intended taxonomic level.\nEach domain had five sets of contexts and examples, totaling\n10 contexts and 60 questions.1\nEvaluation\nIn order to assess the applicability of our two varieties\nof automatically generated questions to a real-world aca-\ndemic setting, we must actually ask real teachers to use\nthem. Therefore, we designed an experiment to replicate a\nteacher’s creation of a reading quiz. We wanted to com-\npare the resulting quizzes’ quality when teachers are writing\nquizzes from scratch and when they have access to automat-\nically generated questions while writing the quiz. Section\nintroduces the measures of quiz quality considered in this\npaper. Then, Section explains the quiz writing experiments\nconducted by real teachers.\nQuiz Quality\nTo be able to compare the quality of questions and quizzes\nwritten with the aid of GPT-3.5 to handwritten ones, we\nmust first establish how to measure the quality of a quiz\nin the first place. However, the quality of a quiz is often a\nsubjective metric. Teachers have differing opinions on what\nmakes a good quiz based on their individual teaching styles.\nWe defined a set of metrics to measure individual aspects of\na quiz that capture different aspects of its quality, with only\nusefulness designed to purely reflect the annotator’s opinion.\nBased on previous research, we identified the following\nfour quiz-level metrics to include.3 A good reading quiz will\nbe relevant to the teaching material. It will be natural and\ncoherent, meaning that it will not confuse a student taking\nit. Above all, a good quiz will be approved by a teacher for\nclassroom use. The following metrics attempt to assess each\nof these aspects:\n• Coverage is a numerical metric in [0, 1] which measures\nhow much of the input passage is reflected in the final\nquiz. To measure this, we mapped each question to the\nsentences in the passage that contained pertinent infor-\nmation to the answer. For consistency, all questions are\nmapped to any possible answer in the text (i.e., both in-\nstances of repeated information are selected, open-ended\nquestions select more of the text, etc.). A coverage ratio\nis then calculated from the length of the mapped text and\nthe whole passage’s length. This metric is inspired by the\npyramid method for annotating summaries, which uses a\nsimilar strategy (Nenkova and Passonneau 2004).\n• Structure is an ordinal metric from1 to 3 which measures\nwhether the set of questions make sense together. In other\nwords, if they are intuitively linked together with a nat-\nural/understandable flow (e.g., from easy to difficult, or\nfrom start to finish of the context). Previously, a similar\nmetric has been used for conversational QG where ques-\ntions must be logically linked in order for a conversation\nto be natural (Mulla and Gharpure 2023).\n3The ordinal metrics also have definitions for each category\navailable in the supplementary material.\n• Redundancy is an ordinal metric from 1 to 3 which mea-\nsures if there is redundancy/repetition within the quiz, for\nexample if there are two questions that ask for the same\nanswer without any different perspective or thought pro-\ncess required from the student. Previously, a similar met-\nric has been used for conversational QG where questions\nmust not be repetitive in order for a conversation to be\nnatural (Mulla and Gharpure 2023).\n• Usefulness is an ordinal metric from 1 to 4 which mea-\nsures if a teacher would use the quiz in an assessment\nthey create for their own class. Note that the quiz does\nnot necessarily need to be entirely answerable from the\ncontext in order to be considered useful. Previously, sim-\nilar ordinal metrics have been used in Elkins et al. (2023),\nand on different scales in Steuer et al. (2021) and Mulla\nand Gharpure (2023).\nBeyond the quality of the whole quiz, it is important to\nensure that the individual questions are also of high quality.\nThus we also outlined three question-level metrics, meaning\nthey are evaluated for each question in a quiz. This is not an\nexhaustive evaluation, but the following covers basic aspects\nof a question’s quality:\n• Relevancy is a binary metric which measures whether\nthe question is semantically relevant to the input context.\nPreviously, similar binary approaches have been used in\nSteuer et al. (2021) and Elkins et al. (2023), and on dif-\nferent scales by Mulla and Gharpure (2023).\n• Fluency is a binary metric which measures whether the\nset of questions are grammatically correct and use clear\nlanguage. Similarly to the previous metric, previous ap-\nproaches by Mazidi and Nielsen (2014) and Elkins et al.\n(2023) have applied this binary metric, and it has been\nused more generally on different scales in Mulla and\nGharpure (2023).\n• Answerability is a binary metric which measures whether\nthe question can be answered from the input context. It is\nnot necessary to be able to find a passage from the input\nthat is an answer to the question; it is enough if a stu-\ndent could reasonably answer the question from the con-\ntext (for example, applying logic explained in the passage\nto a new situation makes the question ‘answerable’). As\nabove, previous work by Steuer et al. (2021) and Elkins\net al. (2023) uses a similar binary metric, and Mulla and\nGharpure (2023) suggests similar metrics on different\nscales.\nQuiz Writing Experiments\nThe quiz writing experiments were designed to mimic a\nteacher’s creation of a reading quiz. In order to be able to\nmeasure and compare teacher’s quiz writing processes, the\nsetting was tightly controlled. Discussion of the potential\nlimitations this may introduce can be found in Section . Be-\nfore conducting the experiments, a pilot with four teachers\nin the ML domain was conducted to ensure the metrics and\nannotator training were unambiguous. This pilot resulted in\nminor changes to the wording in the training and metric def-\ninitions, but no major experimental flaws were discovered.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23087\nThere were 24 quiz writing teachers, 12 in each domain.\nThe BIO teachers were found through the freelance platform\nUpwork and have at least a high-school level of teaching/tu-\ntoring experience. The ML teachers were recruited through\nword-of-mouth at the institutions of the first author and have\nat least a university level of teaching/tutoring experience. All\nteachers were fairly monetarily compensated and signed a\nconsent form before participating. They are all proficient in\nthe English language, and are from relatively diverse demo-\ngraphics.1 The teachers completed a training module where\nthey were provided with example quizzes in their domain\nand were guided through the creation of each quiz type to\nensure they understood the task at hand.\nThe process a single teacher underwent, irrespective of\ntheir domain, is depicted in Figure 3. Each teacher wrote\nthree quizzes, between five and ten questions in length. With\n12 teachers and 12 passages per domain, each passage was\nused to create each type of quiz. Note that each teacher re-\nceived three different passages, to reduce the potential bias\ndue to a teacher working with material they have already\nseen. The three quiz types were: handwritten, simple, and\ncontrolled. To create a handwritten quiz, the teacher simply\nread the passage and wrote a quiz from scratch. To create\na simple quiz, the teacher read the passage and related can-\ndidates generated with the simple prompting strategy. They\nthen created a quiz with the freedom to copy generated ques-\ntions directly, copy and alter them, or write questions from\nscratch. Similarly, to create a controlled quiz, the teacher\nread the passage and related controlled candidates, and then\nwrote a quiz. The quizzes were written in a random order to\nreduce any potential for biased results from the ordering of\nthe quiz writing subtasks.\n+ Simple\nGenerations + Controlled Generations\nInput P\nassage A Input P\nassage B Input P\nassage C\nTeacher\nHandwritten Quiz Simple Quiz Controlled Quiz\nFigure 3: Quiz writing experiment diagram depicting the\nthree quiz writing settings each teacher completed.\nThe teachers were asked to record their screen during the\nquiz writing process. These videos were analyzed to assess\nteachers’ experiences writing each kind of quiz. The time\ntaken to write the quiz was measured, including how long the\nreading of the passage and additional candidates took. The\nlength of the final quiz was measured, with a minimum of\nfive and a maximum of 10 questions. And finally, the source\nof the questions was noted. In other words, it was recorded\nif the question was directly copied from the GPT-3.5 gen-\nerations, if it was copied and altered by the teacher, or if it\nwas written completely from scratch.\nUpon completion of the experiment, the teachers were\nasked to complete a brief post-quiz to better understand their\nfeedback on the three quiz writing tasks. There was a sec-\ntion for free-form comments about the experiment. Then,\nthe teachers were asked to pick which of the three quiz types\nwas their preferred type and provide their reasoning.\nFinally, eight other annotators (four per domain) ana-\nlyzed the quality of the resulting quizzes using the metrics\nintroduced in Section . They were recruited in the same\nway as outlined above, and have similar teaching experi-\nence and English proficiency. Six quizzes were seen by all\nfour of the annotators in each domain in order to measure\ninter-annotator agreement. The rest of the quizzes were seen\nby two annotators. Having more than one annotator eval-\nuate each quiz enables a more robust measurement of the\nquizzes’ quality. The annotators were not informed which\nquizzes included automatically generated questions.\nResults\nThis section covers the key results for the evaluations of quiz\nquality and the quiz writing process. First, Section will com-\npare and contrast the measured quality of the three different\nquiz types. Then, Section will discuss the teacher’s experi-\nences in the different quiz writing settings.\nQuiz Quality\nOverall, the results from the quiz quality evaluations show\nthat there is no notable loss of quality between the hand-\nwritten and the two generation quiz types. There is even an\nargument that an increase in quiz quality can be seen in some\nof the following results.\nThe three question-level metrics demonstrate a low count\nof irrelevant, disfluent, and unanswerable questions across\nall cohorts and quiz types. When comparing across quiz\ntypes, these results are relatively consistent. This implies\nthat the use of generated candidates does not significantly in-\ncrease or decrease the quality of quiz questions along these\nthree aspects. The annotator agreement on these metrics is\n‘fair’, with an average pairwise Cohen’s κ value of 0.3 for\nthe BIO cohort and 0.6 for the ML cohort (Landis and Koch\n1977). 4\nThe coverage results show significant improvement when\nteachers utilize generated questions during their quiz writing\nprocess.5 Figure 4 depicts the increasing ratio of input text\nthat is covered between handwritten and simple, as well as\nsimple and controlled quiz types. In the ML cohort, a sig-\nnificant difference is observed between the coverage ratios\n4Due to low percentages ofirrelevant, disfluent, and unanswer-\nable questions the agreement values are calculated on a unbalanced\ndataset and may not be an accurate representation of the real agree-\nment on problem cases.\n5N.b., a single coverage value for each quiz was annotated by\nthe first author of this paper, so no measurement of agreement is\navailable for this metric. We leave this for future work.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23088\nMetric Handwritten Simple Controlled\nBIO ML BIO ML BIO ML\nIrrelevant 0.03 0.00 0.05 0.05 0.04 0.00\nDisfluent 0.14 0.11 0.09 0.08 0.14 0.11\nUnanswer-\nable 0.08 0.10 0.10 0.08 0.12 0.10\nTable 1: Question-level quality metrics by cohort and quiz\ntype. The values are the proportion of questions that are ir-\nrelevant, disfluent, or unanswerable.\nin the handwritten and controlled quizzes. In the BIO co-\nhort, we observe the same significant difference, as well as\nthe difference between the coverage ratios in the simple and\ncontrolled quizzes. While it is not the only important aspect\nof a reading quiz, the increased coverage provided by au-\ntomatically generated questions can benefit teachers in their\nwriting process.\nFigure 4: Context coverage by cohort and quiz type. * repre-\nsents a significant difference at the α = 0.05 level, and ***\nrepresents a significant difference at the α = 0.001 level.\nThe error bars represent 95% confidence intervals.\nThe three ordinal quiz-level metrics similarly have pos-\nitive results across the board, as seen in the mean column\nof Table 2. The annotator agreement values are quite high.\nThese metric’s agreement is measured with Kendall’sτ as it\nis suitable for ordinal scales (Schaeffer and Levitt 1956). All\naverage pairwise values are above0.5, and most above 0.81.\nFigure 5 shows the difference in quiz-level metrics between\nMetric Mean Kendall’s τ\nBIO ML BIO ML\nStructure\n[1,3] 2.35 2.43 0.81 0.83\nRedundancy\n[1,3] 2.55 2.80 0.59 0.80\nUsefulness\n[1,4] 3.12 2.97 0.95 0.52\nTable 2: Quiz-level quality metrics and annotator agreement.\nThe structure and redundancy metrics are on ordinal scales\nfrom 1 to 3, and the usefulness metric is on an ordinal scale\nfrom 1 to 4.\nthe three quiz types. Notably, for all three metrics and in both\ncohorts, one of the generation type quizzes is the highest\nrated. In the ML cohort there is even a significant difference\nin the usefulness ratings between the handwritten and sim-\nple quizzes. This points to the fact that the generations can\nhelp improve the quality of quizzes. Further optimization of\nthe generation process, perhaps with input from the teachers\ndirectly on what types of questions will be useful in a given\nsetting, could show even stronger quality improvements.\nFigure 5: Quiz-level quality metrics. The error bars repre-\nsent 95% confidence intervals. * represents a significant dif-\nference at the α = 0.05 level.\nQuiz Writing Experience\nThe quiz writing results demonstrate no consistent loss of ef-\nficiency when teachers use generated questions, at least with\nrespect to the time taken. Figure 6 depicts the mean time\nto write each type of quiz for the two cohorts. The mean\nvalues are all relatively close to one another, and the 95%\nconfidence intervals demonstrate that there appears to be no\ndistinct difference in the quiz types with respect totime. Fur-\nther analysis of these results demonstrate that thetime metric\nis more dependent on the teacher and the particular passage\nthan the generated candidate questions.\nFigure 6: Time taken to create a quiz by cohort and quiz\ntype. The error bars represent 95% confidence intervals.\nThe average length of the quizzes was8.14 in the BIO co-\nhort and 7.11 in the ML cohort. The mean number of ques-\ntions in each quiz type was within one question to the mean\nacross all types, demonstrating that the use of generations\ndid not alter this variable. A more interesting comparison\ncan be seen in Figure 7, where thesimple and controlled quiz\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23089\ntypes’ question sources are compared. In both cohorts, the\nteachers hand write fewer questions when they have thecon-\ntrolled generations at their disposal. In fact, in the ML co-\nhort, this difference is statistically significant. Again in both\ncohorts, teachers directly copy more of the controlled gen-\nerations into their quizzes than the simple generations; and\ncorrespondingly they copy and edit fewer questions. This\nfinding demonstrates that teachers choose to use the ques-\ntions generated with Bloom’s taxonomy more than other\ngenerations, providing motivation to continue QG research\nwith pedagogical goals in mind.\nFigure 7: Teachers use of generated questions by cohort. The\nerror bars represent 95% confidence intervals. * represents a\nsignificant difference at the α = 0.05 level.\nThe post-quiz results demonstrate a strong preference\nfrom the teachers in both cohorts towards the use of con-\ntrolled generations, as depicted in Figure 8. The following\ncomments from the teachers provide additional evidence for\nthe fact that they find the controlled generations most useful\nfor their quiz writing. Teachers stated that “The generated\nquestions for each specific type were incredibly useful. ”and\n“I particularly liked the ‘creating’ questions, as I wouldn’t\nhave come up with most of these myself. ” Another teacher\ncommented that they “tried to incorporate different com-\nmand terms and level questions to prepare a quiz that tests\nthe depth of a student’s understanding”, indicating the in-\ntended usage of the questions in different levels of Bloom’s\ntaxonomy. This finding corroborates this paper’s guiding in-\ntuition that QG methods should be designed with pedagogi-\ncal goals in mind.\nFigure 8: Teacher quiz type preference by cohort.\nLimitations\nIt is important to note some limitations of this work. Firstly,\nwe acknowledge that the quiz writing setting is to some ex-\ntent contrived. In reality, teachers’ quiz writing experiences\nwill be subjective: they might use additional resources or ex-\nisting knowledge, write a draft and return to it later to edit,\nnot constrain the number of questions, have different learn-\ning goals in mind, and more. The quiz writing setting was\nnecessary to control certain variables for comparison within\nthis work, but it creates the possibility that the results do not\nreflect the reality of quiz writing for teachers. Future work\nshould aim to remove these constraints to better assess how\nteachers might realistically use generated questions. Sec-\nondly, this work only considers one LLM, two domains, the\nEnglish-language setting, and a limited number of teachers.\nAlthough out of scope for this work, future work should aim\nto expand the variety of these aspects to assess how gener-\nated questions might benefit additional educational settings.\nThirdly, a missing consideration in this work is the other\nhalf of the educational setting: students. Future work should\ninclude student goals, opinions, and performance in order\nto more comprehensively understand the implications of us-\ning automatically generated questions in the classroom. De-\nspite these limitations, this work ultimately takes a step in\nthe right direction for the future of QG research within real-\nistic educational use cases.\nConclusion\nThis paper aims to show that LLMs are capable of gener-\nating different types of questions from a given context that\nteachers find useful to create a quiz that is of comparable\nquality to a handwritten version. To do this, quiz writing ex-\nperiments were conducted comparing three types of quizzes:\nhandwritten, simple, and controlled quizzes. The controlled\nquizzes utilized questions generated to correspond with the\nlevels of Bloom’s taxonomy. The results demonstrate that\nteachers strongly prefer writing quizzes with the help ofcon-\ntrolled generations. They also directly copy more of thecon-\ntrolled generations than the simple generations, indicating\nthat these questions are of higher quality or better suited to\na teacher’s goals. This confirms our hypothesis that teachers\nfind automatically generated pedagogical questions useful\nfor quiz writing. Additionally, an evaluation of quiz qual-\nity showed that the quizzes with both controlled and sim-\nple generations are of comparable quality. Some metrics\neven point towards their superior quality, when compared to\nhandwritten quizzes. We hope that these findings will help\nto direct the future of educational QG research towards prac-\ntical applications that meet the goals and needs of teachers\nand students.\nAcknowledgements\nThe authors would like to thank Mitacs for their grant for\nthis project, and CIFAR for their continued support. Addi-\ntional thanks are extended to the teachers and annotators for\ntheir time and effort and the anonymous reviewers for their\nvaluable feedback.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23090\nReferences\nElkins, S.; Kochmar, E.; Serban, I.; and Cheung, J. C. 2023.\nHow Useful Are Educational Questions Generated by Large\nLanguage Models? In International Conference on Artificial\nIntelligence in Education, 536–542. Springer.\nKasneci, E.; Seßler, K.; K ¨uchemann, S.; Bannert, M.; De-\nmentieva, D.; Fischer, F.; Gasser, U.; Groh, G.; G¨unnemann,\nS.; H¨ullermeier, E.; et al. 2023. ChatGPT for good? On op-\nportunities and challenges of large language models for ed-\nucation. Learning and Individual Differences, 103: 102274.\nKrathwohl, D. R. 2002. A revision of Bloom’s taxonomy:\nAn overview. Theory into practice, 41(4): 212–218.\nKurdi, G.; Leo, J.; Parsia, B.; Sattler, U.; and Al-Emari, S.\n2020. A systematic review of automatic question generation\nfor educational purposes. International Journal of Artificial\nIntelligence in Education, 30: 121–204.\nLaban, P.; Wu, C.-S.; Murakhovs’ ka, L.; Liu, W.; and\nXiong, C. 2022. Quiz design task: Helping teachers create\nquizzes with automated question generation. arXiv preprint\narXiv:2205.01730.\nLandis, J. R.; and Koch, G. G. 1977. An application of hier-\narchical kappa-type statistics in the assessment of majority\nagreement among multiple observers. Biometrics, 363–374.\nLiu, P.; Yuan, W.; Fu, J.; Jiang, Z.; Hayashi, H.; and Neubig,\nG. 2023. Pre-train, prompt, and predict: A systematic survey\nof prompting methods in natural language processing. ACM\nComputing Surveys, 55(9): 1–35.\nMazidi, K.; and Nielsen, R. D. 2014. Pedagogical evalu-\nation of automatically generated questions. In Intelligent\nTutoring Systems: 12th International Conference, ITS 2014,\nHonolulu, HI, USA, June 5-9, 2014. Proceedings 12, 294–\n299. Springer.\nMulla, N.; and Gharpure, P. 2023. Automatic question gen-\neration: a review of methodologies, datasets, evaluation met-\nrics, and applications. Progress in Artificial Intelligence ,\n12(1): 1–32.\nNenkova, A.; and Passonneau, R. J. 2004. Evaluating con-\ntent selection in summarization: The pyramid method. In\nProceedings of the human language technology conference\nof the north american chapter of the association for compu-\ntational linguistics: Hlt-naacl 2004, 145–152.\nOuyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright,\nC. L.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray,\nA.; Schulman, J.; Hilton, J.; Kelton, F.; Miller, L.; Simens,\nM.; Askell, A.; Welinder, P.; Christiano, P.; Leike, J.; and\nLowe, R. 2022. Training language models to follow instruc-\ntions with human feedback.\nSchaeffer, M. S.; and Levitt, E. E. 1956. Concerning\nKendall’s tau, a nonparametric correlation coefficient. Psy-\nchological Bulletin, 53(4): 338.\nSteuer, T.; Bongard, L.; Uhlig, J.; and Zimmer, G. 2021. On\nthe linguistic and pedagogical quality of automatic question\ngeneration via neural machine translation. In Technology-\nEnhanced Learning for a Free, Safe, and Sustainable World:\n16th European Conference on Technology Enhanced Learn-\ning, EC-TEL 2021, Bolzano, Italy, September 20-24, 2021,\nProceedings 16, 289–294. Springer.\nVan Campenhout, R.; Hubertz, M.; and Johnson, B. G.\n2022. Evaluating AI-Generated Questions: A Mixed-\nMethods Analysis Using Question Data and Student Percep-\ntions. In Artificial Intelligence in Education: 23rd Interna-\ntional Conference, AIED 2022, Durham, UK, July 27–31,\n2022, Proceedings, Part I, 344–353. Springer.\nWang, X.; Fan, S.; Houghton, J.; and Wang, L. 2022a.\nTowards process-oriented, modular, and versatile question\ngeneration that meets educational needs. arXiv preprint\narXiv:2205.00355.\nWang, Z.; Valdez, J.; Basu Mallick, D.; and Baraniuk, R. G.\n2022b. Towards human-like educational question generation\nwith large language models. In International conference on\nartificial intelligence in education, 153–166. Springer.\nZhang, H.; Song, H.; Li, S.; Zhou, M.; and Song, D.\n2022. A survey of controllable text generation using\ntransformer-based pre-trained language models. arXiv\npreprint arXiv:2201.05337.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23091",
  "topic": "Bloom",
  "concepts": [
    {
      "name": "Bloom",
      "score": 0.6719796657562256
    },
    {
      "name": "Taxonomy (biology)",
      "score": 0.5886766910552979
    },
    {
      "name": "Bloom's taxonomy",
      "score": 0.5108713507652283
    },
    {
      "name": "Computer science",
      "score": 0.5051420331001282
    },
    {
      "name": "Mathematics education",
      "score": 0.4519476890563965
    },
    {
      "name": "Psychology",
      "score": 0.29670441150665283
    },
    {
      "name": "Biology",
      "score": 0.1537456214427948
    },
    {
      "name": "Ecology",
      "score": 0.09371671080589294
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Cognition",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I5023651",
      "name": "McGill University",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I4210113480",
      "name": "Mohamed bin Zayed University of Artificial Intelligence",
      "country": "AE"
    },
    {
      "id": "https://openalex.org/I4210163051",
      "name": "Kord Technologies (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I109736498",
      "name": "Canadian Institute for Advanced Research",
      "country": "CA"
    }
  ]
}