{
    "title": "Risks of AI Foundation Models in Education",
    "url": "https://openalex.org/W3206168414",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5001257783",
            "name": "Su Lin Blodgett",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5033757090",
            "name": "Michael Madaio",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3037831233",
        "https://openalex.org/W2974437893",
        "https://openalex.org/W3021882501",
        "https://openalex.org/W2547709956",
        "https://openalex.org/W3209052763",
        "https://openalex.org/W2038558034",
        "https://openalex.org/W2075243909",
        "https://openalex.org/W2999429616",
        "https://openalex.org/W2990872544",
        "https://openalex.org/W3093328573",
        "https://openalex.org/W3191391866",
        "https://openalex.org/W3213241618",
        "https://openalex.org/W3195577433",
        "https://openalex.org/W2564247057",
        "https://openalex.org/W3093438907",
        "https://openalex.org/W2578204315",
        "https://openalex.org/W2917746498",
        "https://openalex.org/W3034723486",
        "https://openalex.org/W3155263273",
        "https://openalex.org/W2999635142",
        "https://openalex.org/W2986331430",
        "https://openalex.org/W3133702157"
    ],
    "abstract": "If the authors of a recent Stanford report (Bommasani et al., 2021) on the opportunities and risks of \"foundation models\" are to be believed, these models represent a paradigm shift for AI and for the domains in which they will supposedly be used, including education. Although the name is new (and contested (Field, 2021)), the term describes existing types of algorithmic models that are \"trained on broad data at scale\" and \"fine-tuned\" (i.e., adapted) for particular downstream tasks, and is intended to encompass large language models such as BERT or GPT-3 and computer vision models such as CLIP. Such technologies have the potential for harm broadly speaking (e.g., Bender et al., 2021), but their use in the educational domain is particularly fraught, despite the potential benefits for learners claimed by the authors. In section 3.3 of the Stanford report, Malik et al. argue that achieving the goal of providing education for all learners requires more efficient computational approaches that can rapidly scale across educational domains and across educational contexts, for which they argue foundation models are uniquely well-suited. However, evidence suggests that not only are foundation models not likely to achieve the stated benefits for learners, but their use may also introduce new risks for harm.",
    "full_text": "RISKS OF AI F OUNDATION MODELS IN EDUCATION\nSu Lin Blodgett\nMicrosoft Research\nMontréal, Canada\nMichael Madaio\nMicrosoft Research\nNew York, NY\nIf the authors of a recent Stanford report [Bommasani et al., 2021] on the opportunities and risks of “foundation models”\nare to be believed, these models represent a paradigm shift for AI and for the domains in which they will supposedly be\nused, including education. Although the name is new (and contested [Field, 2021]), the term describes existing types of\nalgorithmic models that are “trained on broad data at scale”and “ﬁne-tuned” (i.e., adapted) for particular downstream\ntasks, and is intended to encompass large language models such as BERT or GPT-3 and computer vision models such\nas CLIP. Such technologies have the potential for harm broadly speaking [e.g., Bender et al., 2021], but their use in the\neducational domain is particularly fraught, despite the potential beneﬁts for learners claimed by the authors. In section\n3.3 of the Stanford report, Malik et al. argue that achieving the goal of providing education for all learners requires\nmore efﬁcient computational approaches that can rapidly scale across educational domains and across educational\ncontexts, for which they argue foundation models are uniquely well-suited. However, evidence suggests that not only\nare foundation models not likely to achieve the stated beneﬁts for learners, but their use may also introduce new risks\nfor harm.\nThat is, even if foundation models work as described, the nature of their design and use may lead to a homogenization\nof learning in ways that perpetuate the inequitable status quo in education [cf. Madaio et al., 2021], and which may\nlead to increasingly limited opportunities for meaningful roles for educational stakeholders in their design. In addition,\nthe all-encompassing vision for foundation models in education laid out by Malik et al. may privilege those aspects\nof education that are legible to large-scale data collection and modeling, further motivating increased surveillance of\nchildren under the guise of care and further devaluing the human experiences of learning that cannot be ingested into\nfoundation models.\n1 Risks of educational technologies at scale\nWhile perhaps well-meaning, the argument that developing large-scale models for education is for students’ beneﬁt is\nan argument that is often used to motivate the increasing (and increasingly harmful) use of educational technologies,\nparticularly educational surveillance technologies [cf. Collins et al., 2021]. Recent research suggests that equity is used\nby educational AI researchers as a motivation for designing and deploying educational AI, but it is rarely studied as a\ndownstream effect of such technologies [Holmes et al., 2021]. Indeed, educational AI, including computer vision and\nnatural language technologies, are increasingly deployed to monitor students in the classroom [Galligan et al., 2020], at\ntheir homes during high-stakes assessments [Cahn et al., 2020, Swauger, 2020, Barrett, 2021], and to monitor their\ndigital communication across platforms [Keierleber, 2021]. These applications are often motivated by appeals to care\nfor students’ well-being [Collins et al., 2021], academic integrity [Harwell, 2020], or supporting teachers [Ogan, 2019];\nand yet, these noble goals do not prevent these technologies from causing harm to learners. As we argue in a recent\npaper [Madaio et al., 2021], the fundamental assumptions of many educational AI systems, while motivated by logics\nof care, may instead reproduce structural inequities of the status quo in education.\nIn addition to students’ well-being, the argument for developing foundation models for education relies on economic\nlogics of efﬁciency — that given the rising costs of education, approaches are needed that can provide education\n“at scale.” However, this argument has been made many times before from educational technologists and education\nreformers, with outcomes that have not, in fact, beneﬁted all learners. It is thus worth taking seriously the lessons of\nhistorical arguments for using technology to provide education at scale. Historians of educational technology such\nas Larry Cuban and Audrey Watters have argued that a century of educational technologies supposedly designed to\nprovide more efﬁcient learning (e.g., educational radio, TV , and computers) have instead led to widespread second-order\neffects, such as promoting more impersonal, dehumanizing learning experiences and further de-valuing the role of\narXiv:2110.10024v1  [cs.CY]  19 Oct 2021\nRisks of AI Foundation Models in Education\nteachers, counterbalanced by teachers’ widespread resistance to the adoption and use of these technologies [Cuban,\n1986, Watters, 2021].\nIndeed, one can look to recent claims about Massive Open Online Courses (MOOCs) to provide education for learners\naround the world who could not afford to access a university [Pappano, 2012]. Although this may have been true for\nsome learners, the vast majority of learners who used and completed MOOCs were already well-educated learners from\nthe Global North, and the majority of courses on major MOOC platforms were offered in English by North American\ninstitutions [Kizilcec et al., 2017]. Even for learners who had access to them, the design and use of MOOC platforms\nthus ampliﬁed particular ideologies about teaching and learning, including an “instructionist” teaching paradigm with\nthe use of lecture videos that learning science research suggests is less effective than active learning [Koedinger et al.,\n2015] and which may not be effective across multiple cultural contexts. More generally, other (non-digital) technologies\nof scale, such as educational standards (e.g., Common Core State Standards) and standardized testing such as the SATs\nact as “racializing assemblages” [Dixon-Román et al., 2019] that may reproduce sociopolitical categories of difference\nin ways that reinforce longstanding social hierarchies.\nThese histories teach us that we should critically interrogate claims about the ability for technology to provide education\nat scale for all learners, much like claims about the beneﬁts of technologies of scale more generally [Hanna and\nPark, 2020]. In addition to interrogating the potential harms of the scalar logics of foundation models, several of\nwhich we identify here, we also suggest interrogating who beneﬁts from this drive to scale, and what alternatives it\nforecloses. Devoting time and resources towards educational technologies built atop foundation models not only diverts\nour attention away from other educational technologies we might develop (or the question of whether we should develop\neducational technology at all), but further entrenches the status quo, allowing us to avoid asking hard questions about\nhow existing educational paradigms shape learning processes and outcomes in profoundly inequitable ways [Madaio\net al., 2021].\n2 Risks of homogenization\nThe adaptability of foundation models, where a few large-scale pre-trained models enable a wide range of applications,\nbrings with it particular risks of homogenization1 for educational applications. That is, design decisions for foundation\nmodels — including decisions about tasks, training data, model output, and more — may lead to homogenization of\npedagogical approaches, of ideologies about learning, and of educational content in ways that may perpetuate existing\ninequities in education [cf. Madaio et al., 2021], particularly when such technologies are intended to be deployed “at\nscale” across contexts.\nSpeciﬁcally, the choices of data proposed for pre-trained models for education smuggle in particular ideologies of\nteaching and learning that may promote a homogenized vision of instruction in similar ways as previous technologies of\neducation at scale reproduced instructionist modes of teaching2 [Koedinger et al., 2015], which may lead to downstream\nharms for learners, despite claims for these technologies’ beneﬁts. For example, Malik et al. propose using feedback\nprovided to developers on forums such as StackOverﬂow to train feedback models, but the feedback provided on such\nforums may not be delivered in pedagogically effective ways, and often reproduces toxic cultures of masculinity in\ncomputer science in ways that actively exclude novice developers and women [Ford et al., 2016].\nFinally, much as Bender et al. [2021] have observed for NLP training corpora more generally, the corpora suggested by\nMalik et al. for training foundation models, such as Project Gutenberg, include texts largely written in English [Gerlach\nand Font-Clos, 2020], which may produce representational harms [cf. Blodgett et al., 2020] by reproducing dominant\nperspectives and language varieties and excluding others. Similarly, Dodge et al. [2021] have found that a ﬁlter used\nto create the Colossal Clean Crawled Corpus (C4, a large web-crawled corpus used to train large English language\nmodels), “disproportionately removes documents in dialects of English associated with minority identities (e.g., text in\nAfrican American English, text discussing LGBTQ+ identities).” In reproducing socially dominant language varieties,\nfoundation models may require speakers of minoritized varieties to accommodate to dominant varieties in educational\ncontexts, incurring higher costs for these speakers and denying them the legitimacy and use of their varieties [Baker-Bell,\n2020]. One setting in which such harms are likely to arise is the use of foundation models for feedback on open-ended\nwriting tasks, as the authors of the report propose. In other work in this space, automated essay scoring and feedback\nprovision have been shown to have roots in racialized histories of writing assessments that are difﬁcult for data-driven\ntechnologies trained on such rubrics to avoid [Dixon-Román et al., 2019], and automated approaches to writing scoring\nand feedback may induce students to adopt writing styles that mirror dominant cultures [Mayﬁeld, 2019].\n1Here we use “homogenization” to refer to the homogenization of outcomes emerging from the use of foundation models, as\nused in Section 5 of the report (as opposed to the homogenization of models and approaches, as used in Section 1).\n2Indeed, Malik et al., propose using lecture videos from online courses to train instructional models.\n2\nRisks of AI Foundation Models in Education\nIn this way, foundation models may reproduce harmful ideologies about what is valuable for students to know and\nhow students should be taught, including ideologies about the legitimacy and appropriateness of minoritized language\nvarieties. Given the broader risks of homogenization of foundation models, they may amplify these ideologies at scale.\n3 Risks of limited roles of stakeholders in designing foundation models\nIn education, decisions about educational curricula and pedagogy are often made with sustained critical evaluation and\npublic debates about what to teach and how to teach it [cf. Scribner, 2016]. However, by relying on foundation models\nfor broad swaths of education (as Malik et al. propose), decisions about what is to be taught and how students should be\ntaught may be made without the involvement of teachers or other educational stakeholders. Despite claims elsewhere\nin the Stanford report for foundation models to support human-in-the-loop paradigms [section 2.4], the pre-trained\nparadigm of foundation models will likely entail limited opportunities for educational stakeholders to participate in key\nupstream design decisions, limiting their involvement to the use of models once such models are trained or ﬁne-tuned.\nAs with AI more generally, despite rhetoric about the importance of stakeholder participation in designing AI systems\n[Kulynych et al., 2020], the reality of current industrial paradigms of training foundation models on massive datasets\nrequiring massive (and expensive) compute power may limit stakeholders’ ability to meaningfully shape choices about\ntasks, datasets, and model evaluation.\nThis narrow scope for involvement of key stakeholders such as teachers and students is at odds with participatory,\nlearner-centered paradigms from educational philosophy [e.g., Freire, 1996, Broughan and Prinsloo, 2020] and the\nlearning sciences [DiSalvo et al., 2017], where learners’ interests and needs shape teachers’ choices about what and\nhow to teach. In addition, this may have the effect of further disempowering teachers from having meaningful agency\nover choices about content or pedagogy, further contributing to the deskilling of the teaching profession, in ways seen\nin earlier technologies of scale in education [cf. Cuban, 1986, Watters, 2021].\n4 Risks of totalizing visions of foundation models in education\nAll of this raises concerns about the expansive claims made for the application of foundation models in education\nto “understand” students, educators, learning, teaching, and “subject matter.” The list of potential uses of foundation\nmodels in education claimed by Malik et al. is evocative of the totalizing rhetoric popular in computer science more\ngenerally, such as for software to “eat the world” [Andreessen, 2011]. Crucially, this totalizing vision suggests that\neverything that matters to learning can be rendered into grist for foundation models.\nFirst, we note that in the education section, “understanding” is used to refer to at least three distinct phenomena:\nstudents’ “understanding” of subject matter, foundation models’ “understanding” of subject matter, and foundation\nmodels’ “understanding” of pedagogy and student behavior. But as Bender and Koller [2020] have argued, NLP systems\n(including foundation models) do not have the capability to “understand” language or human behavior. Although\nthe pattern matching that underlies foundation models (for NLP or otherwise) may produce outputs that resemble\nhuman understanding, this rhetorical slippage is particularly harmful in educational contexts. Because supporting\nlearners’ conceptual understanding is one primary goal of education, the conﬂation of models’ representational ability\nwith comprehension of subject matter, as well as the conﬂation of students’ development of conceptual or procedural\nknowledge with foundation models’ pattern-matching capabilities, may lead teachers and other educational stakeholders\nto trust the capabilities of foundation models in education when such trust is not warranted.\nMore generally, the paradigm of foundation models as laid out by Malik et al. requires that teaching and learning\nbe formalized in ways that are legible to foundation models, without interrogating the potential risks of formalizing\nteaching and learning in such a way, nor the risks for fundamental aspects of education to be discarded if they do not ﬁt\ninto this paradigm. In other words, what forms of knowledge are privileged by rendering them tractable to foundation\nmodels? What is lost in such a partial, reductive vision of teaching and learning?\nAs one example, foundation models may be able to reproduce patterns of pedagogical decisions in the training corpora,\nbut those datasets or models may not be able to capture why those decisions were made. For instance, good teachers\ndraw on a wealth of contextual information about their students’ lives, motivations, and interests; information which\nmay not be legible to foundation models. In some cases, the response from AI researchers may be to simply collect more\ndata traces on students in order to make these aspects of students’ lives legible to modeling; however, this “rapacious”3\napproach to data collection is likely to harm students through the ever-increasing surveillance of students [Galligan\net al., 2020, Barrett, 2021].\n3https://mobile.twitter.com/hypervisible/status/1442473891381710858\n3\nRisks of AI Foundation Models in Education\nDespite expansive claims for the potential for foundation models to radically transform teaching and learning in ways\nthat beneﬁt learners, the history of educational technologies suggests that we should approach such claims with a\ncritical eye. In part, the proposed application of foundation models for education brings with it risks for reproducing\nand amplifying the existing inequitable status quo in education, as well as risks of reproducing dominant cultural\nideologies about teaching and learning, in ways that may be harmful for minoritized learners. In addition, the properties\nof the foundation model paradigm that lend it its appeal — large-scale, pre-trained models adaptable to downstream\ntasks — are precisely what would likely limit opportunities for meaningful participation of teachers, students, and\nother education stakeholders in key decisions about their design. Education is a fundamentally public good; rather\nthan centralizing power in the hands of institutions with sufﬁcient resources to develop large-scale models, educational\ntechnologies, if they are to be designed, should be designed in ways that afford more public participation and are\nresponsive to the needs and values of local contexts.\nReferences\nMarc Andreessen. Why Software Is Eating the World. The Wall Street Journal, 2011.\nApril Baker-Bell. Linguistic Justice: Black Language, Literacy, Identity, and Pedagogy. Routledge, 2020.\nLindsey Barrett. Rejecting Test Surveillance in Higher Education. 1 Mich. St. L. Rev , 2021. Available at SSRN:\nhttps://ssrn.com/abstract=3871423.\nEmily M. Bender and Alexander Koller. Climbing towards NLU: On Meaning, Form, and Understanding in the Age of\nData. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5185–5198,\nOnline, July 2020. Association for Computational Linguistics.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic\nparrots: Can language models be too big?\n . In Proceedings of the ACM Conference on Fairness, Accountability, and\nTransparency (FAccT), pages 610–623, 2021.\nSu Lin Blodgett, Solon Barocas, Hal Daumé, III, and Hanna Wallach. Language (Technology) is Power: A Critical\nSurvey of “Bias” in NLP. InProceedings of the 58th Annual Meeting of the Association for Computational Linguistics,\npages 5454–5476, Online, July 2020. Association for Computational Linguistics.\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein,\nJeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the Opportunities and Risks of Foundation Models.\narXiv preprint arXiv:2108.07258, 2021.\nChristine Broughan and Paul Prinsloo. (Re)centring students in learning analytics: in conversation with Paulo Freire.\nAssessment & Evaluation in Higher Education, 45(4):617–628, 2020.\nAlbert Fox Cahn, Caroline Magee, Eleni Manis, and Naz Akyol. Snooping Where We Sleep: The Invasiveness and\nBias of Remote Proctoring Systems. Surveillance Technology Oversight Project, 2020.\nSara Collins, Jasmine Park, Anisha Reddy, Yasamin Shariﬁ, and Amelia Vance. The Privacy and Equity Implications of\nUsing Self-Harm Monitoring Technologies. 2021. Future of Privacy Forum.\nLarry Cuban. Teachers and Machines: The Classroom Use of Technology Since 1920. Teachers College Press, 1986.\nBetsy DiSalvo, Jason Yip, Elizabeth Bonsignore, and Carl Disalvo. Participatory Design for Learning: Perspectives\nfrom Practice and Research. Routledge, 2017.\nEzekiel Dixon-Román, T. Philip Nichols, and Ama Nyame-Mensah. The racializing forces of/in AI educational\ntechnologies. Learning, Media and Technology, pages 1–15, 2019.\nJesse Dodge, Maarten Sap, Ana Marasovi ´c, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell,\nand Matt Gardner. Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus. In\nProceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2021.\nHayden Field. At Stanford’s “foundation models” workshop, large language model debate resurfaces.Emerging Tech\nBrew, 2021.\nDenae Ford, Justin Smith, Philip J. Guo, and Chris Parnin. Paradise Unplugged: Identifying Barriers for Female\nParticipation on Stack Overﬂow. In Proceedings of the 24th ACM SIGSOFT International Symposium on Foundations\nof Software Engineering (FSE), pages 846–857, 2016.\nPaulo Freire. Pedagogy of the Oppressed (revised). New York: Continuum, 1996.\nClaire Galligan, Hannah Rosenfeld, Molly Kleinman, and Shobita Parthasarathy. Cameras in the Classroom: Facial\nRecognition Technology in Schools. University of Michigan Science, Technology, and Public Policy, 2020.\n4\nRisks of AI Foundation Models in Education\nMartin Gerlach and Francesc Font-Clos. A Standardized Project Gutenberg Corpus for Statistical Analysis of Natural\nLanguage and Quantitative Linguistics. Entropy, 22(1), 2020.\nAlex Hanna and Tina M. Park. Against Scale: Provocations and Resistances to Scale Thinking. arXiv preprint\narXiv:2010.08850, 2020.\nDrew Harwell. Cheating-detection companies made millions during the pandemic. Now students are ﬁghting back. The\nWashington Post, 2020.\nWayne Holmes, Ka´ska Porayska-Pomsta, Ken Holstein, Emma Sutherland, Toby Baker, Simon Buckingham Shum,\nOlga C. Santos, Mercedes T. Rodrigo, Mutlu Cukurova, Ig Ibert Bittencourt, et al. Ethics of AI in Education: Towards\na Community-Wide Framework. International Journal of Artiﬁcial Intelligence in Education, pages 1–23, 2021.\nMark Keierleber. Exclusive Data: An Inside Look at the Spy Tech That Followed Kids Home for Remote Learning—and\nNow Won’t Leave.The 74 Million, 2021.\nRené F. Kizilcec, Andrew J. Saltarelli, Justin Reich, and Geoffrey L. Cohen. Closing global achievement gaps in\nMOOCs. Science, 355(6322):251–252, 2017.\nKenneth R. Koedinger, Jihee Kim, Julianna Zhuxin Jia, Elizabeth A. McLaughlin, and Norman L. Bier. Learning is\nNot a Spectator Sport: Doing is Better than Watching for Learning from a MOOC. In Proceedings of the ACM\nConference on Learning @ Scale, pages 111–120, 2015.\nBogdan Kulynych, David Madras, Smitha Milli, Inioluwa Deborah Raji, Angela Zhou, and Richard Zemel. Participatory\nApproaches to Machine Learning. 2020. International Conference on Machine Learning Workshop.\nMichael Madaio, Su Lin Blodgett, Elijah Mayﬁeld, and Ezekiel Dixon-Román. Beyond “Fairness:” Structural (In)justice\nLenses on AI for Education. In Wayne Holmes and Ka´ska Porayska-Pomsta, editors, Ethics in Artiﬁcial Intelligence\nin Education. Taylor and Francis, 2021.\nElijah Mayﬁeld. Individual Fairness in Automated Essay Scoring. In Proceedings of the Workshop on Contestability in\nAlgorithmic Systems (CSCW), 2019.\nAmy Ogan. Reframing classroom sensing: Promise and peril. interactions, 26(6):26–32, 2019.\nLaura Pappano. The Year of the MOOC. The New York Times, 2012.\nCampbell F. Scribner. The Fight for Local Control: Schools, Suburbs, and American Democracy. Cornell University\nPress, 2016.\nShea Swauger. Our Bodies Encoded: Algorithmic Test Proctoring in Higher Education. Critical Digital Pedagogy,\n2020.\nAudrey Watters. Teaching Machines: The History of Personalized Learning. MIT Press, 2021.\n5"
}