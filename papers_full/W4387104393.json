{
    "title": "Voice-Enabled Response Analysis Agent (VERAA): Leveraging Large Language Models to Map Voice Responses in SDoH Survey",
    "url": "https://openalex.org/W4387104393",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3097222891",
            "name": "Rishivardhan Krishnamoorthy",
            "affiliations": [
                "University of California, San Diego"
            ]
        },
        {
            "id": "https://openalex.org/A3217706802",
            "name": "Vishal Nagarajan",
            "affiliations": [
                "University of California, San Diego"
            ]
        },
        {
            "id": "https://openalex.org/A4365761600",
            "name": "Hayden Pour",
            "affiliations": [
                "University of California, San Diego"
            ]
        },
        {
            "id": "https://openalex.org/A2659836236",
            "name": "Supreeth P. Shashikumar",
            "affiliations": [
                "University of California, San Diego"
            ]
        },
        {
            "id": "https://openalex.org/A2513333116",
            "name": "Aaron Boussina",
            "affiliations": [
                "University of California, San Diego"
            ]
        },
        {
            "id": "https://openalex.org/A1988994192",
            "name": "Emilia Farcas",
            "affiliations": [
                "Qualcomm (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2136241170",
            "name": "Shamim Nemati",
            "affiliations": [
                "University of California, San Diego"
            ]
        },
        {
            "id": "https://openalex.org/A2276015558",
            "name": "Christopher S. Josef",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3097222891",
            "name": "Rishivardhan Krishnamoorthy",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3217706802",
            "name": "Vishal Nagarajan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4365761600",
            "name": "Hayden Pour",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2659836236",
            "name": "Supreeth P. Shashikumar",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2513333116",
            "name": "Aaron Boussina",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1988994192",
            "name": "Emilia Farcas",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2136241170",
            "name": "Shamim Nemati",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2276015558",
            "name": "Christopher S. Josef",
            "affiliations": [
                "CACI International (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2966341529",
        "https://openalex.org/W1957062247",
        "https://openalex.org/W3081484919",
        "https://openalex.org/W3171777526",
        "https://openalex.org/W4294740726",
        "https://openalex.org/W2990302455",
        "https://openalex.org/W4226066902",
        "https://openalex.org/W4365148739",
        "https://openalex.org/W3171952145",
        "https://openalex.org/W3206289796",
        "https://openalex.org/W4205170775",
        "https://openalex.org/W4389542889",
        "https://openalex.org/W2973113902"
    ],
    "abstract": "Abstract Social Determinants of Health (SDoH) have been shown to have profound impacts on health-related outcomes, yet this data suffers from high rates of missingness in electronic health records (EHR). Moreover, limited English proficiency in the United States can be a barrier to communication with health care providers. In this study, we have designed a multilingual conversational agent capable of conducting SDoH surveys for use in healthcare environments. The agent asks questions in the patient’s native language, translates responses into English, and subsequently maps these responses via a large language model (LLM) to structured options in a SDoH survey. This tool can be extended to a variety of survey instruments in either hospital or home settings, enabling the extraction of structured insights from free-text answers. The proposed approach heralds a shift towards more inclusive and insightful data collection, marking a significant stride in SDoH data enrichment for optimizing health outcome predictions and interventions.",
    "full_text": "Voice-Enabled Response Analysis Agent (VERAA): Leveraging Large\nLanguage Models to Map Voice Responses in SDoH Survey\nRishivardhan Krishnamoorthy, MS Candidate1, Vishal Nagarajan, MS Candidate1, Hayden\nPour, MS Candidate1, Supreeth P. Shashikumar, PhD1, Aaron Boussina, PhD Candidate1,\nEmilia Farcas, PhD3, Shamim Nemati, PhD1, Christopher S. Josef, MD2,#\n1University of California San Diego Health, Department of Biomedical Informatics, San\nDiego, CA;2Healcisio, Inc., La Jolla, CA;3Qualcomm Institute, the San Diego division of\nthe California Institute for Telecommunications and Information Technology (Calit2)\n#Corresponding author\nAbstract\nSocial Determinants of Health (SDoH) have been shown to have profound impacts on health-related outcomes, yet\nthis data suffers from high rates of missingness in electronic health records (EHR). Moreover, limited English\nproficiency in the United States can be a barrier to communication with health care providers. In this study, we have\ndesigned a multilingual conversational agent capable of conducting SDoH surveys for use in healthcare\nenvironments. The agent asks questions in the patient’ s native language, translates responses into English, and\nsubsequently maps these responses via a large language model (LLM) to structured options in a SDoH survey. This\ntool can be extended to a variety of survey instruments in either hospital or home settings, enabling the extraction of\nstructured insights from free-text answers. The proposed approach heralds a shift towards more inclusive and\ninsightful data collection, marking a significant stride in SDoH data enrichment for optimizing health outcome\npredictions and interventions.\nIntroduction\nOver the past decade, health systems and providers have invested significant efforts to simultaneously reduce\nburgeoning costs while improving the quality of care provided. Recent research has demonstrated that Social\nDeterminants of Health (SDoH) contributed to 40-50% of the cost structure in Medicare and Medicaid (1) and\nserved as the most important contributor towards health outcomes (2). SDoH encompass the societal and economic\nconditions influencing health. Variables such as education, socioeconomic status, neighborhood environment,\nemployment, and social support networks have begun to be viewed not just as background factors, but as critical\nelements that can steer the course of\nindividual health trajectories.\nRegrettably, structured SDoH for\nplanning or predictive modeling is not\nalways available (3). A recent survey\nfound that SDoH data elements can be\nmissing 20-89% of the time (4).\nAdditionally, new reporting\nrequirements stemming from the Center\nfor Medicare and Medicaid Services\n(CMS) Framework for Health Equity\nhave mandated that health systems\nbegin collecting SDoH data in 2024.\nTraditional approaches to collection of\nhealth survey data generally require a\none-to-one interview conducted as part\nof the intake or admission process,\nwhich can increase system workload\nand is limited by the languages spoken\nby the patient and interviewer. Limited\nEnglish proficiency (LEP) in the United\nStates can be a barrier to accessing\nhealth care services and understanding\nAll rights reserved. No reuse allowed without permission. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for thisthis version posted September 26, 2023. ; https://doi.org/10.1101/2023.09.25.23295917doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nhealth (5). While most providers use bilingual staff or interpreters to communicate with LEP patients, challenges\nsuch as availability, quality, and cost of these services limit their effectiveness. Furthermore, as patient surveillance\nextends into the home environment (6) the collection of accurate, up-to-date information presents additional\nchallenges since the availability of a broadband internet connection or a human interviewer can be limited.\nClinicians and researchers are thereby challenged to develop collection modalities that not only capture a wide\nbreadth of information but also resonate with the lived experiences of a heterogeneous populace.\nA recent study has established a strong association between LEP and sepsis mortality (7). While the causal\nmechanism is unknown, LEP is known to be associated with greater difficulties in accessing medical care, and\nlanguage barriers can impede providers’ ability to take an appropriate clinical history that may lead to clinical errors\nor delays in care. In our previous work, we have demonstrated that SDoH (8) and wearable (9,10) data can\ndramatically improve the accuracy of sepsis readmission scores. However, SDoH factors are often poorly captured\nin electronic health records (EHRs) and are not available at the time of hospital discharge. As such, there is an unmet\nneed for patient-centered communication technologies that can facilitate multilingual health data collection in\ndiverse settings including inpatient and outpatient healthcare facilities, as well as patients’ homes (11). V oice\nassistants provide novel opportunities for data collection as well as addressing some of the barriers that patients\nencounter daily while managing their health (12).\nHome implementation requires a device capable of 1) periodically accessing the EHR in a HIPAA-compliant\nmanner, 2) integrating wearable (e.g., Fitbit) data, and 3) gathering structured and unstructured responses from\npatients. To achieve this we have built the My Companion Pet (MCoPet) device as described in Figure 1. The\nbedside, stand-alone MCoPet utilizes a Raspberry Pi board in association with a speaker and microphone, and\nseamlessly integrates with an EHR and wearable sensors. MCoPet is capable of bi-directional EHR communication\nvia the HL7 FHIR (Fast Healthcare Interoperability Resources) APIs, retrieving wearable information from Fitbit\nvia the Fitbit Web API, and locally running predictive algorithms. The most distinguishing feature of MCoPet has\nbeen the incorporation of a large language model (LLM) (i.e., Llama-2 (13) ) in conjunction with a multilingual,\nvoice-to-text transcription system (via the Whisper (14) automatic speech recognition deep learning model). This\nsystem is capable of transcribing the free responses to survey questions and mapping them to specific survey\nanswers (e.g., multiple choice options) in a “conversational” manner. Owing to the recent development and release\nof publicly-available LLMs, there has been little to no evidence generated to support the use of LLMs for mapping\nfree responses to structured survey responses. Given the potential for transformative change, investigating the\napplication of LLMs for use in collecting SDoH information is not just a matter of improving understanding and\npredictive model performance, but a collaborative journey aimed at addressing the complex interplay of societal\ndynamics and individual health narratives.\nMethods\nWe have previously conducted predictive modeling work addressing sepsis readmission (9,10) using SDoH survey\ndata (8) from the All Of Us dataset (15). In this previous work, we identified a subset of 28 questions across four\nsurveys that significantly improved readmission prediction accuracy for sepsis. This subset of questions (see the\nResults section) were derived from the All of Us “The Basics Survey” (B), “The Lifestyle Survey” (L), “The\nHealthcare Access and Utilization Survey” (H), and “The Overall Health Survey” (O).\nFor each of the 28 questions, we\ngenerated 10 distinct scripted replies\nencompassing all answer choices\n(i.e, a total of 280 potential\nresponses). A sample of these\nscripted replies is provided in Table\n1 which are mapped to survey\nanswers contained in Table 2. A\npanel of two English speakers and\none Spanish speaker then read the\ngenerated scripts for a total of 20\nEnglish audio responses and 10\nSpanish audio responses per\nquestion. This amounted to a total of\nAll rights reserved. No reuse allowed without permission. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for thisthis version posted September 26, 2023. ; https://doi.org/10.1101/2023.09.25.23295917doi: medRxiv preprint \n840 distinct audio clips. The processing pipeline is described in Figure 2. The survey begins with a question from\nthe device which the respondent then answers. The respondent’s audio is then converted into text using voice-to-text\ntranscription via the Whisper automatic speech recognition deep learning model. The transcribed text is then passed\nto Llama-2 with instructions to map the text to the most appropriate survey response. Each of the 280 scripted\nresponses were examined by a panel of three reviewers who through consensus mapped the scripted response to one\nof the structured survey answers. This “ground truth” was then used for evaluating the LLM’s mapping accuracy.\nWhile our conversational agent has been purposely built for edge-computing devices, we leveraged cloud computing\nresources to speed prototyping and optimization of the conversational agent. We utilized a HIPAA-compliant AWS\ninstance of type g5.4xlarge, comprising an NVIDIA A10 GPU coupled with 24GB VRAM and 64GB memory\nrunning Windows Server 2022. The 3-bit quantized version of the Llama 2 70-Billion parameter variant was\nemployed where a total of 44 layers of the model were offloaded to GPU for accelerated performance. The pure C++\nimplementation of the Llama-2 model, called Llama.cpp, was used for inference resulting in generating 1.8 tokens\nper second using the AWS instance’s resources.\nTable 1: Sample of Free Text Responses in English and Spanish\nB8. Do you own or rent the place where you live? B8. Spanish Translation of question B8 removed per\nMEDRXIV policy\n1. \"I own my home; bought it a few years back.\"\n2. \"We rent a cozy apartment in the city center.\"\n3. \"I'm in another arrangement; I live with my parents for now.\"\n4. \"We own a small house on the outskirts of town.\"\n5. \"Currently, I rent a studio near my workplace.\"\n6. \"It's another arrangement; I'm staying with a friend\ntemporarily.\"\n7. \"I own a condominium downtown, which I purchased last\nyear.\"\n8. \"Renting for now, but hoping to buy in the next couple of\nyears.\"\n9. \"I'm in another arrangement; it's a housing cooperative, so it's\na bit different.\"\n10. \"I rent a townhouse with a couple of roommates.\"\n1. Translated response removed per MEDRXIV policy.\n2. Translated response removed per MEDRXIV policy.\n3. Translated response removed per MEDRXIV policy.\n4. Translated response removed per MEDRXIV policy.\n5. Translated response removed per MEDRXIV policy.\n6. Translated response removed per MEDRXIV policy.\n7. Translated response removed per MEDRXIV policy.\n8. Translated response removed per MEDRXIV policy.\n9. Translated response removed per MEDRXIV policy.\n10. Translated response removed per MEDRXIV policy.\nTable 2: Answer Choices for SDoH Survey Questions\nB1 B2, L1-5 B3 B4 B5\nLess than $10,000\n$10,000- $24,999\n$25,000-$34,999\n$35,000-$49,999\n$50,000- $74,999\n$75,000-$99,999\n$100,000- $149,999\n$150,000- $199,999\n$200,000 or more\nPrefer not to answer\nYes\nNo\nDon’t know\nPrefer not to answer\nEmployed for wages (This can\nbe part- time or full-time)\nSelf-employed\nOut of work for 1 year or more\nOut of work for less than 1 year\nA homemaker\nA student\nRetired\nUnable to work (disabled)\nPrefer not to answer\nNever attended school or only attended\nkindergarten\nGrades 1 through 4 (Primary)\nGrades 5 through 8 (Middle school)\nGrades 9 through 11 (Some high school)\nGrade 12 or GED (High school graduate)\n1 to 3 years after high school (Some college,\nAssociate’s degree, or technical school)\nCollege 4 years or more (College graduate)\nAdvanced degree (Master’s, Doctorate, etc.)\nPrefer not to answer\nYes\nNo\nPrefer not to answer\nB6 H1-12 O1 O2 O3-5\nOwn\nRent\nOther arrangement\nYes\nNo\nDon’t know\nExtremely\nQuite a bit\nSomewhat\nA little bit\nNot at all\nNever\nRarely\nSometimes\nOften\nAlways\nExcellent\nVery good\nGood\nFair\nPoor\nAll rights reserved. No reuse allowed without permission. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for thisthis version posted September 26, 2023. ; https://doi.org/10.1101/2023.09.25.23295917doi: medRxiv preprint \nWe performed three experiments: MCoPet Hybrid English, Cloud English, and Cloud Spanish. In the first\nexperiment, a hybrid pipeline was set up where the English audio was transcribed to text using Whisper’s base\nmodel (74 Million parameters) on the MCoPet device, and text files were then passed to Llama-2 70B hosted on\nAWS. In the second and third experiments, the English and Spanish audio files were passed to the AWS instance\nwhere Whisper’s large model (1.550 Billion parameters) was used to produce audio transcription that Llama-2 70B\nused for inference. Furthermore, in an effort to localize the computation pipeline to the MCoPet device, we ported\nthe 7B parameter variant of the Llama-2 model onto the MCoPet module and observed ~300 seconds or 5 minutes to\nprocess one response. In comparison, the Llama-2 70B parameter model on the AWS instance processed each\nresponse at an average rate of 1 minute and also generated accurate mappings better than the Llama-2 7B model.\nTherefore, for the purpose of this paper, we focused on the full Llama-2 70B model on AWS, and the Hybrid\nEnglish experiments were limited to just using Whisper on the MCoPet device.\nResults\nThe performance of MCoPet Hybrid English, Cloud English, and Cloud Spanish conversational agents are contained\nwithin Table 3. The accuracy (ACC) for a question represents the accuracy of matching 20 English responses and 10\nSpanish responses to the set of response choices for each question.\nA boxplot of the performance of our proposed system across the MCoPet Hybrid English approach, Cloud-based\nEnglish, and Cloud-based Spanish approach is shown in Figure 3. The median and interquartile accuracy for the\nMCoPet Hybrid English approach\nwas 85% [80%-95%]. Similarly, the\naccuracy for the Cloud English and\nCloud Spanish approaches was 100%\n[90%-100%] and 90% [80%-100%],\nrespectively. Since all the approaches\nuse the same LLM (i.e., Llama-2) in\nthe backend for mapping, the\nvariation in performance between the\nHybrid and Cloud English\napproaches can be attributed to\nWhisper. While the cloud based\napproaches support Whisper’s large\nmodel, MCoPet can only\naccommodate Whisper’s base model.\nThe difference between performance\nof the Whisper base model and large\nmodel is prominently showcased in\nFigure 3.\nIt can be observed that the\nperformance of Llama-2 is discernibly lower when the questions involve mathematical comparisons (e.g., question\nB1). For example, the task in question B1 is for the LLM to match the patient’s response to the correct bin of annual\nhousehold income range. Llama-2 can correctly match the response when the annual income is equal to the lower or\nupper bound of the range, but not when it is within the range. This suggests that Llama-2 lacks the ability to perform\nrelational operations on numerical values. This limitation is consistent with the findings of Imani et al., who showed\nthat LLMs can generate incorrect answers with certain formulations of math questions (16).\nAll rights reserved. No reuse allowed without permission. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for thisthis version posted September 26, 2023. ; https://doi.org/10.1101/2023.09.25.23295917doi: medRxiv preprint \nTable 3: LLM Survey Mapping Accuracy by Question\nQuestion\nHybrid\nEnglish\n(%ACC)\nCloud\nEnglish\n(%ACC)\nCloud\nSpanish\n(%ACC)\nB1 What is your approximate annual household income from all sources? 75 75 70\nB2 Are you covered by health insurance or some other kind of health care plan?100 100 100\nB3 What is your current employment status? Please select 1 or more of these\ncategories.\n90 100 100\nB4 What is the highest grade or year of school you completed? 90 90 100\nB5 In the past 6 months, have you been worried or concerned about NOT having\na place to live?\n95 100 100\nB6 Do you own or rent the place where you live? 100 100 100\nL1 Have you smoked at least 100 cigarettes in your entire life? (There are 20\ncigarettes in a pack)?\n100 100 100\nL2 Have you ever used an electronic nicotine product, even one or two times?\n(Electronic nicotine products include e- cigarettes, vape pens, hookah pens,\npersonal vaporizers and mods, e-cigars, e-pipes, and e-hookahs.)\n95 100 100\nL3 Have you ever smoked a traditional cigar, cigarillo, or filtered cigar, even one\nof two puffs?\n100 100 100\nL4 Have you ever smoked tobacco in a hookah, even one or two puffs? 95 100 100\nL5 Have you ever used smokeless tobacco products, even one or two times?\n(Smokeless tobacco products include snus pouches, Skoal Bandits, loose\nsnus, moist snuff, dip, spit, and chewing tobacco.)\n95 100 100\nDuring the past 12 months, were any of the following scenarios true?\nH1 You saw or talked to a general doctors who treated a variety of illnesses ( a\nphysician in general practice, primary care, family medicine, or internal\nmedicine)\n90 100 100\nH2 You used alternative therapies to save money. 85 90 90\nH3 You skipped medication doses to save money. 80 100 90\nThere are many reasons people delay getting medical care. Have you delayed getting\ncare for any of the following reasons in the PAST 12 MONTHS?\nH4 Could not afford the copay. 85 100 90\nH5 Your deductible was too high/or could not afford the deductible. 80 90 80\nH6 You had to pay out of pocket for some or all of the procedure. 80 100 90\nH7 Did not have transportation. 80 80 80\nH8 You provide care to an adult and could not leave him/her. 75 80 80\nH9 You could not get child care. 80 90 80\nH10 You live in a rural area where distance to the health care provider is too far.75 90 80\nDURING THE PAST 12 MONTHS, was there any time when you needed any of the\nfollowing, but didn't get it because you couldn't afford it?\nH11 To see a specialist. 80 80 80\nH12 Follow-up care. 90 90 90\nO1 How confident are you in filling out medical forms by yourself? 95 95 80\nO2 How often do you have someone help you read health-related material? 70 100 80\nO3 In general, how would you rate your physical health? 80 95 80\nO4 In general, would you say your quality of life is? 75 95 90\nO5 In general, how would you rate your satisfaction with your social activities\nand relationships?\n85 100 80\nAll rights reserved. No reuse allowed without permission. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for thisthis version posted September 26, 2023. ; https://doi.org/10.1101/2023.09.25.23295917doi: medRxiv preprint \nFurthermore, our experiments revealed consistent mispredictions in scenario-based question responses (e.g., H\nquestion series). These mispredictions often involved responses containing the phrase “Don’t know”, which, upon\ncloser examination, actually contained information indicative of a “Yes” or “No” prediction. For instance, consider\nthe question, “Did you delay getting medical care because of transportation?” and a patient’s response of “Don’t\nknow if it was solely due to transportation, but there were times I couldn’t make it.” Despite the response implying a\ndelay in medical care, Llama-2 focused on the “Don’t know” aspect and incorrectly predicted “Don’t know” instead\nof “Yes”. This issue may be attributed to the enforced Llama-2’s deterministic nature, which is to ensure consistency\nand accuracy across all question-response pairs.\nWe observed that the transcription by Whisper’s base model was inaccurate in certain instances, sometimes omitting\nspoken words at the end of sentences, which led to reduced accuracy for the Hybrid English compared to Cloud\nEnglish approach. Similarly, we noticed that Whisper’s large model on the cloud sometimes failed to accurately\ntranslate words from Spanish to English. For instance, the words “Rarely” and “Fair” that were answered in Spanish\nfor questions O2 and O3 were incorrectly transcribed and translated to “Almost never” and “Just” leading to\nmispredictions by the LLM. These factors contributed to reducing the accuracy of the Cloud Spanish approach as\nopposed to the Cloud English approach.\nDiscussion\nPrevious work by Nguyen et al. has identified that most prior work in the development of conversational agents has\nsuffered from the limitations of constrained input, namely that only certain responses can be accepted (17). The\nprimary finding of this proof of concept study is that LLMs can be utilized to map unconstrained, free responses to\ndiscrete survey answers with relatively high accuracy across more than one language, which demonstrates a move\ntowards unconstrained modalities of information collection. The incorporation of an LLM based conversational\nagent within MCoPet presents an opportunity to conduct “context aware” interactions that account for additional\nsensors and EHR data.\nImprovements in communication between patients and their care teams, especially for LEP patients are likely to\ncontribute to more timely and equitable care. Conversational agents like the one proposed in this study offer a\npathway for removing some of the barriers imposed by language by offering a location agnostic (e.g., home,\nhospital, etc.), low cost method for capturing accurate, complex information from patients without the presence of\nan interviewer. A successful implementation of such a tool could rapidly expand the amount of health-related\ninformation collected without a concomitant increase in labor. While the focus of this work was limited to producing\nresponses to an existing structured survey instrument, the findings are extensible to the collection and interpretation\nof multilingual free responses into structured data. Use cases include but are not limited to: soliciting symptoms,\ngauging responses to interventions or medications, patient satisfaction, among others. Currently, surveys issued at\nscale often omit free responses owing to the time and labor necessary to convert the responses into some form of\nusable information; however, applications of LLM’s greatly reduce if not eliminate these limitations. Future surveys\nmay now be able to avoid the use of structured responses which oftentimes don’t afford respondents the option of\nqualifying or explaining their answers.\nWe acknowledge that this investigation has certain limitations. In some circumstances a tablet based survey may\nafford users a shorter survey duration; however, this approach does not lend itself well to the collection and analysis\nof unstructured, free responses and is not as easily adaptable to populations with language or literacy barriers. The\nquestions utilized only represent a subset of a larger SDoH survey set curated and validated by the All of Us Data\nconsortium. The inclusion of additional questions may uncover problematic questions that are not appropriately\nmapped by a LLM. Additionally, the conducted experiments were limited to a set of pre-formed responses in order\nto compare accuracy across languages and speakers given the same response. The full spectrum of wild type\nresponses still stands to be evaluated. The experiments were limited to two languages and do not fully represent the\ndiverse range of cultural or lingual backgrounds treated within the US healthcare system; however, the method is\ncurrently extendable to 99 different languages and the number of languages and performance across languages is\nexpected to rapidly improve.\nFuture development of this work will increase the number of responses and compare them to ground truth answers\nfrom traditionally administered surveys gathered as part of the aforementioned CMS reporting requirement set to\nbegin in 2024. LLMs could also be applied to existing text in the medical record to prepopulate responses to survey\nquestions. The respondent could then confirm that this information is correct or adjust accordingly during their\nAll rights reserved. No reuse allowed without permission. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for thisthis version posted September 26, 2023. ; https://doi.org/10.1101/2023.09.25.23295917doi: medRxiv preprint \ninteractions. Lastly, focus group testing with community health representatives would better elucidate technological\nliteracy and cultural barriers that may lead to hesitancy or inaccuracies associated with verbal, health based surveys.\nConclusion\nThis paper presents a novel approach to collect and analyze multilingual SDoH survey data using a voice-enabled\nconversational agent that leverages automatic speech recognition and large language models. VERAA transcribes\nand maps voice responses in multiple languages to structured survey answers with high accuracy. The proposed\ntechnology can be integrated with a low-cost, edge-computing device that can communicate with EHRs and\nwearable sensors, enabling data collection in diverse outpatient settings. The results of this proof-of-concept study\ndemonstrate the potential of LLMs to facilitate more inclusive and insightful SDoH data collection, which can\nimprove health outcome predictions and interventions. Future work will involve testing VERAA with real patients\nand expanding the scope of survey instruments and languages supported.\nAcknowledgements\nS.N. is funded by the National Institutes of Health (R35GM143121). He is a co-founder of Healcisio Inc., which is\nfocused on commercialization of advanced analytical decision support tools. Mr. Boussina is funded by the National\nLibrary of Medicine (#2T15LM011271-11). The opinions or assertions contained herein are the private ones of the\nauthor and are not to be construed as official or reflecting the views of the NIH or any other agency of the US\nGovernment. We would like to thank Drs. Robert El-Kareh and Dr. Robert Owens for their discussions regarding\nworkflow implementation of the proposed conversational agent. Additionally, we would like to acknowledge Ised\nGongora for help with Spanish translation and audio recordings.\nData Sharing\nThe survey data used in this study, along with the evaluation codes and audio files, will be made available on\nGitHub for public access and reproducibility. The GitHub repository link will be provided in the final version of the\npaper.\nReferences\n1. Houlihan J, Leffler S. Assessing and Addressing Social Determinants of Health: A Key Competency for\nSucceeding in Value-Based Care. Prim Care. 2019 Dec;46(4):561–74.\n2. Hood CM, Gennuso KP, Swain GR, Catlin BB. County Health Rankings: Relationships Between Determinant\nFactors and Health Outcomes. Am J Prev Med. 2016 Feb;50(2):129–35.\n3. Farcas E, Hogarth M, Moore A. Observations on Documentation of Alcohol Use in Real-World Data. AMIA\n(American Medical Informatics Association) Annual Symposium, Poster track, San Diego, CA, October 3 -\nNovember 3, 2021.\n4. Feldman SS, Davlyatov G, Hall AG. Toward Understanding the Value of Missing Social Determinants of Health\nData in Care Transition Planning. Appl Clin Inform [Internet]. 2020 Aug [cited 2023 Sep 11];11(4):556–63.\nAvailable from: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7449791/\n5. Pandey M, Maina RG, Amoyaw J, Li Y , Kamrul R, Michaels CR, et al. Impacts of English language proficiency\non healthcare access, use, and outcomes among immigrants: a qualitative study. BMC Health Services Research\n[Internet]. 2021 Jul 26 [cited 2023 Sep 14];21(1):741. Available from:\nhttps://doi.org/10.1186/s12913-021-06750-4\n6. Haddad TC, Coffey JD, Deng Y , Glasgow AE, Christopherson LA, Sangaralingham LR, et al. Impact of a\nHigh-Risk, Ambulatory COVID-19 Remote Patient Monitoring Program on Utilization, Cost, and Mortality.\nMayo Clin Proc. 2022 Dec;97(12):2215–25.\n7. Jacobs ZG, Prasad PA, Fang MC, Abe-Jones Y , Kangelaris KN. The Association between Limited English\nProficiency and Sepsis Mortality. J Hosp Med. 2020 Mar;15(3):140–6.\n8. Amrollahi F, Shashikumar SP, Meier A, Ohno-Machado L, Nemati S, Wardi G. Inclusion of social determinants\nof health improves sepsis readmission prediction models. J Am Med Inform Assoc [Internet]. 2022 May 2 [cited\n2023 Jul 7];29(7):1263–70. Available from: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9196687/\n9. Yhdego HH, Nayebnazar A, Amrollahi F, Boussina A, Wardi G, Nemati S. Prediction of Unplanned Hospital\nReadmission using Clinical and Longitudinal Wearable Sensor Features.\n10.Amrollahi F, Shashikumar SP, Yhdego H, Nayebnazar A, Yung N, Wardi G, et al. Predicting Hospital\nReadmission among Patients with Sepsis using Clinical and Wearable Data [Internet]. Health Informatics; 2023\nApr [cited 2023 Jul 7]. Available from: http://medrxiv.org/lookup/doi/10.1101/2023.04.10.23288368\nAll rights reserved. No reuse allowed without permission. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for thisthis version posted September 26, 2023. ; https://doi.org/10.1101/2023.09.25.23295917doi: medRxiv preprint \n11. Arsenault-Lapierre G, Henein M, Gaid D, Le Berre M, Gore G, Vedel I. Hospital-at-Home Interventions vs\nIn-Hospital Stay for Patients With Chronic Disease Who Present to the Emergency Department: A Systematic\nReview and Meta-analysis. JAMA Netw Open. 2021 Jun 1;4(6):e2111568.\n12.Chen C, Johnson JG, Charles K, Lee A, Lifset ET, Hogarth M, et al. Understanding Barriers and Design\nOpportunities to Improve Healthcare and QOL for Older Adults through V oice Assistants. In: Proceedings of the\n23rd International ACM SIGACCESS Conference on Computers and Accessibility [Internet]. New York, NY ,\nUSA: Association for Computing Machinery; 2021 [cited 2023 Sep 14]. p. 1–16. (ASSETS ’21). Available from:\nhttps://dl.acm.org/doi/10.1145/3441852.3471218\n13.Touvron H, Martin L, Stone K, Albert P, Almahairi A, Babaei Y , et al. Llama 2: Open Foundation and\nFine-Tuned Chat Models [Internet]. arXiv; 2023 [cited 2023 Sep 12]. Available from:\nhttp://arxiv.org/abs/2307.09288\n14.Radford A, Kim JW, Xu T, Brockman G, McLeavey C, Sutskever I. Robust Speech Recognition via Large-Scale\nWeak Supervision [Internet]. arXiv; 2022 [cited 2023 Sep 12]. Available from: http://arxiv.org/abs/2212.04356\n15.The “All of Us” Research Program. New England Journal of Medicine [Internet]. 2019 Aug 15 [cited 2023 Sep\n12];381(7):668–76. Available from: https://doi.org/10.1056/NEJMsr1809937\n16.Imani S, Du L, Shrivastava H. MathPrompter: Mathematical Reasoning using Large Language Models [Internet].\narXiv; 2023 [cited 2023 Sep 14]. Available from: http://arxiv.org/abs/2303.05398\n17.Nguyen TT, Sim K, Kuen ATY , O’donnell RR, Lim ST, Wang W, et al. Designing AI-based Conversational\nAgent for Diabetes Care in a Multilingual Context [Internet]. arXiv; 2021 [cited 2023 Sep 14]. Available from:\nhttp://arxiv.org/abs/2105.09490\nAll rights reserved. No reuse allowed without permission. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for thisthis version posted September 26, 2023. ; https://doi.org/10.1101/2023.09.25.23295917doi: medRxiv preprint \nPaper Notes (Will Not be included in final manuscript)\nProgram Track: Data/Science Artificial Intelligence\nTheme: Harnessing the Power of Large Language Models in Health Data Science\nKeywords (limit 3):\n● Clinical decision support for translational/data science interventions\n● Clinical and research data collection, curation, preservation, or sharing\n● Ethical, Legal, and Social Issues\n● Fairness and disparity research in health informatics\n● Implementation Science and Deployment\n● Knowledge representation, management, or engineering\n● Learning healthcare system\n● Machine learning, Generative AI, and predictive modeling\n● Mobile Health, wearable devices and patient-generated health data\n● Natural Language Processing\n● Outcomes research, clinical epidemiology, population health\n● Patient centered research and care\n● Social determinants of health\nTitles:\n● \"The Future of Health Surveys: Unveiling the Potential of LLMs in Analyzing V oice Transcribed SDoH\nSurvey Data\"\n● \"Improving SDoH Survey Outcomes with LLM-Based V oice Transcription Mapping: A Case Study\"\n● \"A New Frontier in Health Equity: Leveraging LLMs for Enhanced SDoH Survey Analysis\"\n● \"Realizing the Potential of LLMs in Facilitating Comprehensive SDoH Survey Analyses\"\n● \"A Proof-of-Concept Study: LLM-Based Mapping of V oice Transcribed Answers to SDoH Surveys\"\n● \"Pilot Study on the Efficacy of LLMs in Analyzing V oice Transcribed Responses to SDoH Surveys\"\n● \"Exploring the Frontiers of AI in Healthcare: A Preliminary Study on LLM-Based SDoH Survey Analysis\"\n● \"V oice to Insight: Leveraging Large Language Models for SDoH Survey Analysis\"\n● \"Automated Mapping of V oice Transcribed Responses to SDoH Surveys: A Novel Approach\"\n● \"Towards Intelligent SDoH Surveys: A V oice-Activated Response Mapping System\"\n● \"V oice-Activated SDoH Survey Analysis through Large Language Model Integration\"\n● \"Innovations in Public Health: Utilizing LLMs for SDoH Survey V oice Responses\n● MUSA: Multilingual Survey Analysis using Automatic Speech Recognition and Large Language Models\n● \"V oice-Enabled Response Analysis Agent (VERAA): Leveraging Large Language Models to Map V oice\nResponses in SDoH Surveys\"\nLearning Objective:\nUpon completion of this educational experience individuals should be able to select and use appropriate\ntranscription software and large language models to map free text audio responses to answers fields on surveys.\nAll rights reserved. No reuse allowed without permission. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for thisthis version posted September 26, 2023. ; https://doi.org/10.1101/2023.09.25.23295917doi: medRxiv preprint "
}