{
  "title": "Pretrained Language Models for Text Generation: A Survey",
  "url": "https://openalex.org/W3164184720",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1999993190",
      "name": "Li, Junyi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2382488324",
      "name": "Tang, Tianyi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2752884039",
      "name": "Zhao, Wayne Xin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221399670",
      "name": "Wen, Ji-Rong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3035565536",
    "https://openalex.org/W3036120435",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W3034961030",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W3011574394",
    "https://openalex.org/W3100124323",
    "https://openalex.org/W2891068404",
    "https://openalex.org/W2963227052",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2949178656",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3105912780",
    "https://openalex.org/W2997591391",
    "https://openalex.org/W3022903564",
    "https://openalex.org/W2985808369",
    "https://openalex.org/W3153729783",
    "https://openalex.org/W3009445007",
    "https://openalex.org/W2962785754",
    "https://openalex.org/W3034408878",
    "https://openalex.org/W3093669700",
    "https://openalex.org/W3004304303",
    "https://openalex.org/W2970049541",
    "https://openalex.org/W3092327118",
    "https://openalex.org/W3116342879",
    "https://openalex.org/W3119558324",
    "https://openalex.org/W3105245805",
    "https://openalex.org/W3035317912",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W3177423701",
    "https://openalex.org/W2981851019",
    "https://openalex.org/W3040352674",
    "https://openalex.org/W3034731179",
    "https://openalex.org/W2914204778",
    "https://openalex.org/W3042876905",
    "https://openalex.org/W2950342809",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2944815030"
  ],
  "abstract": "Text generation has become one of the most important yet challenging tasks in natural language processing (NLP). The resurgence of deep learning has greatly advanced this field by neural generation models, especially the paradigm of pretrained language models (PLMs). In this paper, we present an overview of the major advances achieved in the topic of PLMs for text generation. As the preliminaries, we present the general task definition and briefly describe the mainstream architectures of PLMs for text generation. As the core content, we discuss how to adapt existing PLMs to model different input data and satisfy special properties in the generated text. We further summarize several important fine-tuning strategies for text generation. Finally, we present several future directions and conclude this paper. Our survey aims to provide text generation researchers a synthesis and pointer to related research.",
  "full_text": "arXiv:2105.10311v2  [cs.CL]  25 May 2021\nPretrained Language Models for T ext Generation: A Survey\nJunyi Li1,3† , Tianyi T ang2† , Wayne Xin Zhao1,3∗ and Ji-Rong Wen1,2,3\n1Gaoling School of Artiﬁcial Intelligence, Renmin Universi ty of China\n2School of Information, Renmin University of China\n3Beijing Key Laboratory of Big Data Management and Analysis M ethods\n{lijunyi,steven tang,jrwen}@ruc.edu.cn, batmanﬂy@gmail.com\nAbstract\nT ext generation has become one of the most im-\nportant yet challenging tasks in natural language\nprocessing (NLP). The resurgence of deep learning\nhas greatly advanced this ﬁeld by neural generation\nmodels, especially the paradigm of pretrained lan-\nguage models (PLMs). In this paper, we present\nan overview of the major advances achieved in the\ntopic of PLMs for text generation. As the pre-\nliminaries, we present the general task deﬁnition\nand brieﬂy describe the mainstream architectures\nof PLMs for text generation. As the core content,\nwe discuss how to adapt existing PLMs to model\ndifferent input data and satisfy special properties\nin the generated text. W e further summarize sev-\neral important ﬁne-tuning strategies for text gen-\neration. Finally, we present several future direc-\ntions and conclude this paper. Our survey aims to\nprovide text generation researchers a synthesis and\npointer to related research.\n1 Introduction\nT ext generation, which is often formally referred as natura l\nlanguage generation, has become one of the most important\nyet challenging tasks in natural language processing (NLP) .\nIt aims to produce plausible and readable text in human lan-\nguage from input data ( e.g., a sequence and keywords). Re-\nsearchers have developed numerous techniques for a wide\nrange of applications of text generation [Li et al. , 2021a ]. For\nexample, machine translation generates text in a different lan-\nguage based on the source text [Y ang et al. , 2020a ]; summa-\nrization generates an abridged version of the source text to\ninclude salient information [Guan et al. , 2020 ].\nWith the recent resurgence of deep learning, various works\nhave been proposed to solve text generation tasks based on\nrecurrent neural networks (RNN) [Li et al. , 2019 ], convolu-\ntional neural networks (CNN) [Gehring et al. , 2017 ], graph\nneural networks (GNN) [Li et al. , 2020 ], and attention mech-\nanism [Bahdanau et al. , 2015 ]. One of the advantages of\nthese neural models is that they enable end-to-end learning\n† Equal contribution.\n∗ Corresponding author.\nof semantic mappings from input to output in text generation .\nBesides, neural models are able to learn low-dimensional,\ndense vectors to implicitly represent linguistic features of\ntext, which is also useful to alleviate data sparsity.\nDespite the success of neural models for text generation, a\nmajor performance bottleneck lies in the availability of la rge-\nscale datasets. Existing datasets for most of supervised te xt\ngeneration tasks are rather small (except machine transla-\ntion). Deep neural networks usually have a large number of\nparameters to learn, which are likely to overﬁt on these smal l\ndatasets and do not generalize well in practice.\nIn recent years, the paradigm of pretrained language\nmodels (PLMs) is thriving [Peters et al. , 2018 ]. The idea\nis to ﬁrst pretrain the models in large-scale corpus and\nthen ﬁne-tune these models in various downstream tasks\nto achieve state-of-the-art results. It is widely recogniz ed\nthat PLMs can encode a large amount of linguistic knowl-\nedge from corpus and induce universal representations of\nlanguage. Therefore, PLMs are generally beneﬁcial for\ndownstream tasks and can avoid training a new model from\nscratch [Brown et al. , 2020 ]. Moreover, with the increas-\ning of computational power and the emergence of Trans-\nformer architecture [V aswani et al. , 2017 ], PLMs have ad-\nvanced from shallow to deep and achieved outstanding per-\nformance in many tasks, such as BER T [Devlin et al. , 2019 ]\nand GPT [Radford et al. , 2019 ]. Therefore, researchers have\nproposed various methods to solve text generation tasks bas ed\non PLMs. Pretrained on large-scale corpus, PLMs are able to\nunderstand natural language accurately and express in huma n\nlanguage ﬂuently, both of which are critical abilities to fu lﬁll\nthe text generation tasks. Existing surveys in this area hav e\nonly partially reviewed some related topics. Zaib et al. [2020]\nand Guan et al. [2020] provided a synthesis to the research\non some text generation subtasks, i.e., dialogue systems and\nsummarization, but did not go broader to the other important\ngeneration tasks. Qiu et al. [2020] summarized two gener-\nations of PLMs for the whole NLP domain and introduced\nvarious extensions and adaption approaches of PLMs. T o\nthe best of our knowledge, our survey is the ﬁrst work that\npresents a comprehensive review of PLMs for text generation .\nIt aims to provide text generation researchers a synthesis a nd\npointer to related research.\nT o start with, we ﬁrst present a general task deﬁnition with\nthe formulations of different text generation tasks in Sec-\ntion 2, and then brieﬂy describe the mainstream architectur es\nof PLMs that are used in text generation in Section 3. Since\nthe core of text generation is to model the semantic mappings\nfrom input to output, we further organize the major advances\nwith respect to the two aspects of input and output in Sec-\ntion 4-5. For input, we mainly discuss how to adapt existing\nPLMs to different data types. For output, we study how to\nsatisfy special properties for the generated text. Further more,\nwe summarize several important ﬁne-tuning strategies for t ext\ngeneration in Section 6. Finally, we present several future di-\nrections and conclude this paper in Section 7.\n2 T ask and T ypical Applications\nIn what follows, we formally deﬁne the text generation task.\nThe core of text generation is to generate a sequence of dis-\ncrete tokens Y = ⟨y1, . . . , yj , . . . , yn⟩, where each yj is\ndrawn from a word vocabulary V. In most cases, text gen-\neration is conditioned on input data, such as attributes, te xt\nand structured data, which is denoted as X . Formally, the\ntext generation task can be described as:\nP (Y|X ) =P (y1, . . . , yj, . . . , yn|X ). (1)\nAccording to input X , we next introduce several typical\napplications of text generation:\n• If X is not provided or a random noise vector z, this\ntask will degenerate into language modeling or uncondition al\ngeneration task [Radford et al. , 2019 ], which aims to gener-\nate text without any constraint.\n• If X is a set of discrete attributes ( e.g., topic words, sen-\ntiment labels), the task becomes topic-to-text generation or\nattribute-based generation [Keskar et al. , 2019 ]. The infor-\nmation in X plays the role of guiding the text generation pro-\ncess and controlling the modes of the generated text.\n• If X is structured data like knowledge graph or table, this\ntask will be considered as KG-to-text or table-to-text gene r-\nation, called data-to-text generation [Li et al. , 2021c ]. This\ntask aims to generate descriptive text about structured dat a.\n• If X is multimedia input such as image and speech,\nthe task becomes image caption [Xia et al. , 2020 ] or speech\nrecognition [Fan et al. , 2019 ]. The core of image caption is to\ngenerate a description of an image, while speech recognitio n\nenables programs to process human speech into a text format.\n• The most common form of X is also a text sequence,\nand there exist several applications such as machine transl a-\ntion, summarization and dialogue system. Machine transla-\ntion [Conneau and Lample, 2019 ] aims to translate text from\none language into another language automatically, summa-\nrization [Zhang et al. , 2019b ] is focused on generating con-\ndensed summary of a long document, and dialogue sys-\ntem [W olf et al. , 2019 ] is designed to converse with humans\nusing natural language.\nW e present the formulations for the major text generations\nin T able 1.\n3 Standard Architectures for T ext Generation\nPretrained language models (PLMs) are pretrained with a\nmass of unlabelled text data and can be ﬁne-tuned on down-\nstream generation tasks. Pretrained on large-scale corpus ,\nInput X T asks\nRandom noise Unconditional text generation\nDiscrete attributes\nT opic-to-text generation\nAttribute-based generation\nStructured data Data-to-text generation\nMultimedia\nImage caption\nSpeech recognition\nT ext sequence\nMachine translation\nSummarization\nDialogue system\nT able 1: Major tasks and inputs for text generation.\nPLMs encode massive linguistic and world knowledge into\nvast amounts of parameters, which can enhance the under-\nstanding of language and improve the generation quality. Th e\nidea of pretraining is inspired by human beings, i.e., we trans-\nfer and reuse our old knowledge of what we have learned in\nthe past to understand new knowledge and handle a variety\nof new tasks. In this way, PLMs can successfully perform on\nnew tasks with their old experience and knowledge.\nOwing to the great achievements that Trans-\nformer [V aswani et al. , 2017 ] has made, almost all PLMs\nemploy the backbone of Transformer. For the text generation\ntasks, some of PLMs utilize the standard Transformer\narchitecture following basic encoder-decoder framework,\nwhile the others apply a decoder-only Transformer. Next, we\nwill introduce these two methods successively.\nEncoder-decoder T ransformer . A standard Transformer\nutilizes the encoder-decoder architecture, which is compo sed\nof two stacks of Transformer blocks. The encoder is fed\nwith an input sequence, while the decoder aims to generate\nthe output sequence based on encoder-decoder self-attenti on\nmechanism. Based on aforementioned architecture, models\nsuch as MASS [Song et al. , 2019 ], T5 [Raffel et al. , 2020 ],\nand BAR T [Lewis et al. , 2020 ] have improved quality of the\ngenerated text.\nDecoder-only T ransformer . Models such as GPT\n[Radford et al. , 2019; Brown et al. , 2020 ] and CTRL\n[Keskar et al. , 2019 ] employ a single Transformer decoder\nblocks, which is typically used for language modeling. They\napply unidirectional self-attention masking that each tok en\ncan only attend to previous tokens.\nBesides language modeling, several works also utilize the\ndecoder-only achitecture to generate text conditioned on i n-\nput text. However, these models do not have an indepen-\ndent module to encode input sequence. Interestingly, they\nconcatenate the input and output sequence with a special\ntoken ( e.g., “ [SEP]”) and employ a novel seq2seq mask-\ning [Dong et al. , 2019 ] that each token in the input sen-\ntence can attend to each other and generated tokens can at-\ntend to all input tokens and previous generate ones. Com-\npared to unidirectional masking, seq2seq masking is a natur al\nway for decoder-only PLMs to solve conditional generation\ntasks, which is similar to the encoder-decoder architectur e.\nRaffel et al. [2020] has researched the performance between\nthe above two methods and made a conclusion that the addi-\ntion of an explicit encoder-decoder attention is beneﬁcial .\nThe core of text generation tasks is to learn the semantic\nmappings from input to output. On one hand, different tasks\nwill correspond to a variety of input data, and we need to\ndevelop special techniques to model different data types. O n\nthe other hand, the generated text should satisfy important\nproperties in order to cope with different task requirement s.\nNext, we discuss the recent advances with respect to the two\naspects, i.e., input and output.\n4 Modeling Different Data T ypes from Input\nAs discussed in Section 2, different text generation tasks u su-\nally involve speciﬁc kinds of input. In this section, we will\nintroduce three main kinds of input for text generation, i.e.,\nunstructured input, structured input, and multimedia inpu t,\nand discuss how to model these input data in PLMs.\n4.1 Unstructured Input\nIn NLP research, most of studies focus on modeling un-\nstructured text input ( e.g., sentence, paragraph, and docu-\nment). T o generate satisfactory output text, it requires an\nexcellent capacity of language understanding beyond sur-\nface meaning of individual words in the input text. Thus,\nLiu and Lapata [2019] and Zheng and Lapata [2019] em-\nployed PLMs ( e.g., BER T [Devlin et al. , 2019 ]) as text en-\ncoder for condensing text into low-dimensional vectors whi le\npreserving most of its meaning. Compared with traditional\nshallow neural models ( e.g., CNN), PLMs have a large num-\nber of parameters encoding massive world knowledge, which\nis potentially beneﬁcial to capture the core meaning of text .\nIn some cases, the input text might be a long document\nconsisting of several sentences and paragraphs. For PLMs\ntrained on sentences or short paragraphs, they are less ca-\npable of accurately modeling long-range dependencies in a\ndocument. Considering this challenge, Zhang et al. [2019b]\nand Xu et al. [2020b] proposed hierarchical BER T to learn\ninteractions between sentences with self-attention for do cu-\nment encoding. Besides, for capturing inter-sentential re la-\ntions, DiscoBER T [Xu et al. , 2020a ] stacked graph convolu-\ntional network (GCN) on top of BER T to model structural dis-\ncourse graphs. By directly operating on the discourse units ,\nDiscoBER T retains capacities to include more concepts or\ncontexts, leading to more concise and informative output te xt.\nW e observe that most recent PLMs are pretrained on En-\nglish text. While, many multilingual generation tasks such\nas machine translation involve multiple languages and cer-\ntain languages are low-resource. This challenge hinders th e\nwide application of monolingual PLMs to multilingual text\ngeneration tasks. Therefore, Conneau and Lample [2019]\nproposed to learn cross-lingual language models (XLMs)\nfor multilingual language understanding. Based on cross-\nlingual PLMs, text generation models can still obtain ef-\nfective input word embeddings even in a low-resource lan-\nguage [W ada and Iwata, 2018 ].\n4.2 Structured Input\nStructured data ( e.g., graph and table) is also a critical kind\nof input for text generation in many real-world application s\nsuch as weather report generation. However, in real-world\nscenario, it is difﬁcult to collect a large amount of labelle d\nstructured data with ground-truth text for training. Since pre-\ntrained on large-scale corpus, PLMs encode a large amount\nof linguistic knowledge and show excellent few-shot capabi li-\nties in many tasks. Motivated by this, Chen et al. [2020b] and\nGong et al. [2020] explored incorporating PLMs for data-to-\ntext generation, especially in few-shot settings.\nWhen applying PLMs to structured data, a major challenge\nis how to feed structured data into PLMs, which are originall y\ndesigned for sequential text. T o adapt to the sequential na-\nture of PLMs, Ribeiro et al. [2020] and Mager et al. [2020]\nlinearized input knowledge graph (KG) and abstract mean-\ning representation (AMR) graph into a sequence of triples,\nLi et al. [2021b] introduced an additional graph encoder to\nencode the input KG, and Gong et al. [2020] employed a\ntemplate-based method to serialize input table into text se -\nquence. For example, the attribute-value pair “ name: jack\nreynolds” will be serialized as a sentence “ name is jack\nreynolds”. However, direct linearization will lose the struc-\ntural information of original data, which may lead to gener-\nating unfaithful text about data. Thus, in addition to gener -\nating faithful text, Gong et al. [2020] proposed an auxiliary\nreconstruction task for recovering the structural informa tion\nof input data, which can enhance the capacity of modeling\nstructural information.\nIn general, the output text should retain as much as impor-\ntant information from structured data. Therefore, to gener -\nate high-ﬁdelity text adhereing to input, the pointer gener ator\nmechanism [See et al. , 2017 ] is adopted to copy words from\ninput knowledge data [Chen et al. , 2020b ]. Through ground-\ning PLMs on external knowledge, it is likely to endow a gen-\nerative model with both rich knowledge and good generaliza-\ntion ability. Besides, Gong et al. [2020] proposed a content\nmatching loss for measuring the distance between the infor-\nmation in input data and the output text.\n4.3 Multimedia Input\nIn addition to the above textual data, several attempts\nhave been made to take as input multimedia data ( e.g.,\nimage, video, and speech) such as image caption and\nspeech recognition. Both V ideoBER T [Sun et al. , 2019b ] and\nCBT [Sun et al. , 2019a ] conducted pretraining for the video\ncaption task. While, they performed pretraining only for\nthe BER T -based encoder to learn bidirectional joint distri bu-\ntions over sequences of visual and linguistic tokens. So the y\nhave to train a separate video-to-text decoder, which tends\nto cause a pretrain-ﬁnetune discrepancy . In contrast, Uni-\nﬁed VLP [Zhou et al. , 2020 ] used a shared multi-layer Trans-\nformer network for both encoding and decoding. Following\nUniLM [Dong et al. , 2019 ], they pretrained the model on two\nmasked language modeling (MLM) tasks, like cloze tasks de-\nsigned for sequence-to-sequence LM. Inspired by generativ e\npretraining objectives in GPT , Xia et al. [2020] proposed a\ncross-modal pretrained model (XGPT) by taking images as\ninputs and using the image caption task as the basic genera-\ntive task in the pretraining stage.\nBesides image and video, speech recognition is also hun-\ngry for human-transcripted supervised data. So a number of\nunsupervised and semi-supervised methods are developed to\nintegrate PLMs for weakly-supervised learning. For exam-\nple, Fan et al. [2019] proposed an unsupervised approach to\npretraining encoder-decoder model with unpaired speech an d\ntranscripts. T wo pretraining stages are used to extract aco us-\ntic and linguistic information with speech and transcripts ,\nwhich is useful for downstream speech recognition task.\n5 Satisfying Special Properties for Output\nT ext\nIn different text generation tasks, the generated text shou ld\nsatisfy several key properties. In this section, we will int ro-\nduce three key properties in text generation, i.e., relevance,\nfaithfulness, and order-preservation.\nRelevance. According to the linguistic litera-\ntures [Li et al. , 2021c ], in text generation, relevance refers\nthat the topics in output text is highly related to the input t ext.\nA representative example is the task of dialogue systems,\nwhich requires the generated response to be relevant to the\ninput dialogue history. In addition to the dialogue history , a\ncondition corresponding to the type of response might be als o\nprovided as an external input such as the topic of response\nand the persona of speaker. The generated responses should\nalso be relevant to the condition. Recently, due to the\nabsence of long-term memory, RNN-based models still tend\nto generate irrelevant output text and lack consistency wit h\ninput. Therefore, through applying PLMs to the task of\ndialogue systems, TransferTransfo [W olf et al. , 2019 ] and\nDialoGPT [Zhang et al. , 2020 ] were able to generate more\nrelevant and context-consistent responses than tradition al\nRNN-based models.\nFurthermore, to generalize to various types of conditions,\nZeng and Nie [2020] utilized elaborated condition blocks to\nincorporate external conditions. They used BER T for both\nencoder and decoder by utilizing different input represent a-\ntions and self-attention masks to distinguish the source an d\ntarget sides of dialogue. On the target (generation) side, a new\nattention routing mechanism is adopted to generate context -\nrelated words. Similar approaches have been used in non-\nconditioned dialogue [Bao et al. , 2020 ].\nFaithfulness. Similarly, faithfulness is also a critical prop-\nerty of text, which means the content in generated text shoul d\nnot contradict the facts in input text. Sometimes, it furthe r\nmeans the generated text is in accord with the world facts.\nA representative example is the task of text summarization,\nwhich aims to generate faithful text representing the most i m-\nportant information within the original content. Pretrain ed\non large collections of text, PLMs are potentially beneﬁcia l\nto generate faithful text by utilizing background knowledg e.\nRothe et al. [2020] experimented with a large number of set-\ntings to initialize the encoder and decoder with three out-\nstanding PLMs, i.e., BER T , GPT and RoBER T a. With pre-\ntraining, the models are more aware of the domain character-\nistics and less prone to language model vulnerabilities. Co n-\nsequently, they are more conﬁdent in predicting tokens from\nthe document, hence, improving faithfulness.\nT o improve the level of faithfulness of summary,\nKryscinski et al. [2018] proposed to decompose the decoder\ninto a contextual network that retrieves relevant parts of t he\nsource document and a PLM that incorporates prior knowl-\nedge about language generation. Also, to generate faith-\nful text in different target domains, Y ang et al. [2020b] ﬁne-\ntuned PLMs on target domains through theme modeling loss.\nThe role of the theme modeling module is to make the gener-\nated summary semantically close to the original article.\nOrder-preservation. In NLP area, order-preservation de-\nnotes that the order of semantic units (word, phrase, etc.)\nin both input and output text is consistent. The most\nrepresentative example is the task of machine translation.\nWhen translating from source language to target language,\nkeeping the order of phrases consistent in source language\nand target language will ensure the accuracy of the trans-\nlation results to some extent. One line of research to\nachieve the order-preservation property is to perform sema n-\ntic alignment in machine translation. Y ang et al. [2020a]\nproposed Code-Switching Pre-training (CSP) for machine\ntranslation. They extracted the word-pair alignment infor -\nmation from the source and target language, and then ap-\nplied the extracted alignment information to enhance order -\npreserving. Besides, it is more common to perform trans-\nlation across multiple languages, called multilingual ma-\nchine translation [Conneau and Lample, 2019 ]. However,\nlittle work can effectively enhance order-preservation fo r\nany pairs of languages. Thus, Lin et al. [2020] proposed\nmRASP , an approach to pretraining a universal multilin-\ngual machine translation model. The key to mRASP is the\ntechnique of randomly aligned substitution, which enforce s\nwords and phrases with similar meanings across multiple\nlanguages to be aligned in the representation space. Also,\nW ada and Iwata [2018] focused on aligning word represen-\ntations of each language, making it possible to preserve the\nword order consistent cross multiple languages.\n6 Fine-tuning Strategies for T ext Generation\nFor text generation with PLMs, a key factor is how to design\nsuitable ﬁne-tuning strategies. In this part, we review sev eral\ncommonly-used ﬁne-tuning strategies from different views .\n6.1 Data View\nWhen applying PLMs to text generation tasks especially in a\nnew domain, how to design suitable and effective ﬁne-tuning\nstrategies adapting to the characteristics of new domain is an\nimportant consideration.\nFew-shot Learning. In many text generations, it is difﬁcult\nand expensive to obtain sufﬁcient annotated data. Owing to\nthe success of pretraining, PLMs can encode massive linguis -\ntic and world knowledge, which provides an effective soluti on\nto data scarcity. A commonly adopted approach is to plug the\nexisting module with pretrained parameters. Then we ﬁne-\ntune it with a few , one, or even no examples for the studied\ntask, which are so-called few-shot, one-shot and zero-shot ,\nrespectively.\nFor example in multilingual translation, some low-\nresource languages lack sufﬁcient parallel corpus. XLM\nData Categories Methods\nInput\nUnstructured\nBER T acts as text encoders [Liu and Lapata, 2019; Zheng and Lapata, 2019 ],\nhierarchical PLMs for document modeling [Zhang et al. , 2019b; Xu et al. , 2020b ], and cross-\nlingual PLMs for multilingual input text [Conneau and Lample, 2019; W ada and Iwata, 2018 ].\nStructured\nLinearize KG and AMR graph as triple sequence [Mager et al. , 2020; Ribeiro et al. , 2020 ],\ngraph encoder for encoding KG [Li et al. , 2021b ], and serialize table into template-based\ntext sequence [Gong et al. , 2020 ].\nMultimedia V ideo caption [Sun et al. , 2019b; Sun et al. , 2019a ], image caption [Xia et al. , 2020 ],\nand speech recognition [Fan et al. , 2019 ].\nOutput\nRelevance\nFine-tune PLMs in dialogue systems for generating more rele vant and context related responses\n[W olf et al. , 2019; Zhang et al. , 2020 ], and generalize to any type of input conditions based on\nBER T [Zeng and Nie, 2020 ].\nFaithfulness\nImprove faithfulness with several PLMs [Rothe et al. , 2020 ], retrieve relevant parts from input\nand incorporate prior knowledge of PLMs [Kryscinski et al. , 2018 ], and generate faithful text in\ndifferent target domains through theme modeling loss [Y ang et al. , 2020b ].\nOrder-\npreservation\nW ord-pair alignment [Y ang et al. , 2020a ], a universal multilingual machine translation model\n[Lin et al. , 2020 ], and word representation alignment [W ada and Iwata, 2018 ].\nT able 2: Categories of input types and output properties for text generation.\n[Conneau and Lample, 2019 ] proposed to learn cross-lingual\nlanguage models and can leverage the knowledge learned\nin high-resource languages to low-resource languages. Us-\ning the method proposed in Section 4, few-shot learning\ncan also be applied in data-to-text tasks, such as table-\nto-text generation [Chen et al. , 2020b; Gong et al. , 2020 ] and\nKG-to-text generation [Li et al. , 2021b ]. Chen et al. [2020b]\ndirectly fed GPT -2 with a small amount of serialized attribu te-\nvalue pairs and Gong et al. [2020] further applied multi-\nple tasks to better leverage structured information of tabl es.\nMoreover, Li et al. [2021b] proposed representation align-\nment to bridge the semantic gap between KG encodings and\nPLMs in order to enhance the correspondence between KG\nand text.\nDomain T ransfer . Equipped with vast amounts of parame-\nters and pretrained on large-scale corpus, PLMs have pow-\nerful generalization capability. However, they still cann ot\ndirectly adapt to new domains with large distribution dis-\ncrepency from pretraining domain [Hendrycks et al. , 2020 ].\nAn effective solution is to continue training PLMs on spe-\nciﬁc data with pretraining objectives before ﬁne-tuning th em\non target tasks. Mask prediction is a widely used method,\nattempting to predict the masked tokens using the remaining\nones. There exist several variants of masking ways in domain\ntransfer. Zeng and Nie [2020] proposed TF-IDF based mask-\ning to select more condition-related tokens to mask, in orde r\nto focus on domain features. Document masking is usually\nutilized in summarization task to capture document-level f ea-\ntures of long documents [Zhang et al. , 2019b ].\n6.2 T ask View\nBesides characteristics of new domains, it is also meaningf ul\nto consider some special concerns such as language coher-\nence and text ﬁdelity in speciﬁc generation tasks when ﬁne-\ntuning PLMs.\nEnhancing Coherence. T o enhance the language coher-\nence, an important approach is to better model language con-\ntext during ﬁne-tuning. Models ﬁne-tuned by contrastive\nlearning are good at distinguishing whether a sentence pair is\nsimilar or not. Through this method, PLMs are forced to un-\nderstand the positional or semantic relationship between t wo\nsentences, so that they can derive better representations.\nNext sentence prediction (NSP) is a commonly adopted\nway to judge whether two input sentences are consec-\nutive segments, which can be applied to summarization\n[Y ang et al. , 2020b ] and dialog system [W olf et al. , 2019 ].\nZheng and Lapata [2019] proposed to rearrange the sen-\ntence order according to their semantic similarities. CBT\n[Sun et al. , 2019a ] proposed noise contrastive estimation\n(NCE) in cross-modal training to encourage the model to\nidentify the correct video-text pair compared to a set of neg a-\ntive distractors.\nDenoising autoencoding (DAE) takes the corrupted text as\ninput and aims to recover the original text. The model ﬁne-\ntuned with DAE has a strong ability to understand the overall\nsentences and capture longer-range correlations. For exam -\nple, TED [Y ang et al. , 2020b ] utilized DAE to reﬁne essential\nsemantic information for abstractive summarization. XGPT\n[Xia et al. , 2020 ] attempted to model the underlying text-\nimage alignments using image-conditioned denoising autoe n-\ncoding (IDA), in order to force the model to reconstruct the\nwhole sentence.\nPreserving Fidelity . T ext ﬁdelity refers that how the gen-\nerated text adheres to the original input information, whic h is\nan important aspect to consider in many text generation task s.\nThe universal structure in PLMs is unable to retain the text ﬁ -\ndelity in speciﬁc text generation tasks. For the table-to-t ext\ngeneration task, the structure information of table is requ ired\nto be encoded. Gong et al. [2020] proposed to utilize multi-\ntask learning, in order to reconstruct from table embedding s\nand enforce the match between table embeddings and content\nembeddings. Besides, the pointer generator [See et al. , 2017 ]\ncan be applied to KG-to-text generation to copy the entity an d\nrelation information in KG [Chen et al. , 2020b ].\n6.3 Model View\nT o enhance the quality of generated text, a key is to well trai n\nthe parameters of PLMs according to task-speciﬁc data, so\nthat PLMs can capture the semantic characteristics special ly\nfor the generation task. However, as mentioned above, task-\nspeciﬁc data is inadequate, thus it is likely to occur the ove r-\nﬁtting case when ﬁne-tuned on limited data. In this part, we\nwill introduce several ﬁne-tuning methods in view of models .\nGu et al. [2020] employed a ﬁxed teacher GPT to pre-\nserve the knowledge encoded in another ﬁne-tuned GPT .\nChen et al. [2020a] proposed to utilize a BER T model\n(teacher) as supervision to guide the Seq2Seq model\n(student) for better generation performance. Besides,\nLiu and Lapata [2019] utilized two optimizers to update the\nparameters of PLM and initial module separately, in order to\nsolve the discrepancy between two modules.\nThere also exist other ways to guide the ﬁne-tuning pro-\ncess. For example, Reinforcement learning can be ap-\nplied to directly guide models by non-differentiable metri cs\n[Zhang et al. , 2019a ], such as ROUGE. Zhao et al. [2020]\nutilized curriculum learning to let the model learn from\neasy documents to hard documents. Moreover, DIALOGPT\n[Zhang et al. , 2020 ] implemented a maximum mutual infor-\nmation (MMI) scoring function to alleviate generating blan d,\nuninformative responses.\n7 Conclusion and Future Outlooks\nThis paper presents an overview of the recent advances\nachieved in pretrained language models for text generation .\nW e mainly summarize the extensions of PLMs in modeling\ndifferent data types in input and satisfy special text prope r-\nties in output. W e also discussed several useful ﬁne-tuning\nstrategies for text generation.\nT o advance this ﬁeld, there are several promising future\ndirections for applying PLMs to text generation.\nModel Extension. Although various extensions have been\nproposed in Section 3, there still exist discrepancies be-\ntween pretraining and downstream generation tasks. For ex-\nample, the “ [MASK]” token in pretraining stage will not\nbe used in ﬁne-tuning stage, which further aggravates the\npretraining-ﬁnetuning discrepancy. Thus, it further desi res\nto design an appropriate pretraining paradigm for text gen-\neration. Moreover, incorporating external knowledge into\nPLMs during pretraining has been shown to be effective\n[Zhang et al. , 2019c ], and it is promising to investigate how\nto inject related knowledge for text generation.\nControllable Generation. Controllable text generation with\nPLMs is an interesting direction but still at a very early sta ge.\nControlling some attributes of the generated text has many\nuseful applications such as generating positive response t o pa-\ntients with depression in dialogue systems. However, PLMs\nare usually pretrained in universal corpus, which is difﬁcu lt\nto control the multi-grained attributes of the generated te xt\n(e.g., sentiment, topic, and coherence). Keskar et al. [2019]\nhas explored text generation with control codes that govern\nstyle, content and task-speciﬁc behavior. While, these con trol\ncodes are preset and coarse-grained. Future work can explor e\nmulti-grained control and develop PLMs that are sufﬁcientl y\nsteerable.\nModel Compression. Although PLMs with large-scale pa-\nrameters have achieved success in text generation, these mo d-\nels are challenging to be deployed in resource constrained\nenvironments. As a result, it is meaningful to study how\nto achieve competitive performance with a small number of\nparameters. Several methods have been proposed to com-\npress PLMs, such as parameter sharing [Lan et al. , 2020 ] and\nknowledge distillation [Sanh et al. , 2019 ], whereas most of\nthem focused on BER T -based models, and little attention has\nbeen paid to compressing PLMs for text generation.\nFine-tuning Exploration. The direct intention of pretrain-\ning is to distill the linguistic knowledge learned in PLMs to\ndownstream generation tasks. And, ﬁne-tuning is the pre-\ndominant transfer method at present. There could be vari-\nous ways to transfer knowledge from PLMs to downstream\nmodels. For example, Chen et al. [2020a] exploited knowl-\nedge distillation by adopting BER T as teacher model and a\nvanilla RNN generation model as student model. Through\nthis method, the linguistic knowledge of BER T can be dis-\ntilled into the downstream model.\nLanguage-agnostic PLMs. Nowadays, almost all the PLMs\nfor text generation are mainly based on English. These PLMs\nwill encounter challenges when dealing with non-English\ngeneration tasks. Therefore, language-agnostic PLMs are\nworthy to be investigated, which need to capture universal l in-\nguistic and semantic features across different languages. An\ninteresting direction is how to reuse existing English-bas ed\nPLMs for text generation in non-English languages.\nEthical Concern. Currently, PLMs are pretrained on large-\nscale corpus crawled from the web without ﬁne-grained ﬁl-\ntering, potentially causing ethical issues such as generat ing\nprivate content about users. Therefore, researchers shoul d try\ntheir best to prevent misusing PLMs. For this purpose, we\ncan follow the key steps provided by Blank [2011], such as\nidentifying threats and potential impacts and assessing li keli-\nhood. Besides, the text generated by PLMs might be preju-\ndiced, which is in line with the bias in training data along th e\ndimensions of gender, race, and religion [Brown et al. , 2020 ].\nHence, we ought to intervene PLMs for preventing such bi-\nases. The research on the general approach is extensive but\nstill preliminary for PLMs.\nAcknowledgement\nThis work was partially supported by the National Key R&D\nProgram of China under Grant No. 2020AAA0105200, Na-\ntional Natural Science Foundation of China under Grant No.\n61872369 and 61832017, Beijing Academy of Artiﬁcial In-\ntelligence (BAAI) under Grant No. BAAI2020ZJ0301, Bei-\njing Outstanding Y oung Scientist Program under Grant No.\nBJJWZYJH012019100020098, the Fundamental Research\nFunds for the Central Universities, and the Research Funds\nof Renmin University of China under Grant No.18XNLG22\nand 19XNQ047. Xin Zhao is the corresponding author.\nReferences\n[Bahdanau et al. , 2015 ] Dzmitry Bahdanau, Kyunghyun\nCho, and Y oshua Bengio. Neural machine translation by\njointly learning to align and translate. In ICLR, 2015.\n[Bao et al. , 2020 ] Siqi Bao, Huang He, Fan W ang, Hua Wu,\nHaifeng W ang, W enquan Wu, Zhen Guo, Zhibin Liu,\nand Xinchao Xu. PLA TO-2: towards building an open-\ndomain chatbot via curriculum learning. arXiv preprint\narXiv:2006.16779 , 2020.\n[Blank, 2011 ] Rebecca M Blank. Guide for conducting risk\nassessments. 2011.\n[Brown et al. , 2020 ] T om B. Brown, Benjamin Mann, and\nNick Ryder et al. Language models are few-shot learners.\nIn NeurIPS, 2020.\n[Chen et al. , 2020a ] Y en-Chun Chen, Zhe Gan, Y u Cheng,\nJingzhou Liu, and Jingjing Liu. Distilling knowledge\nlearned in BER T for text generation. In ACL, 2020.\n[Chen et al. , 2020b ] Zhiyu Chen, Harini Eavani, W enhu\nChen, Y inyin Liu, and William Y ang W ang. Few-shot\nNLG with pre-trained language model. In ACL, 2020.\n[Conneau and Lample, 2019 ] Alexis Conneau and Guil-\nlaume Lample. Cross-lingual language model pretraining.\nIn NeurIPS, 2019.\n[Devlin et al. , 2019 ] Jacob Devlin, Ming-W ei Chang, Ken-\nton Lee, and Kristina T outanova. BER T: pre-training of\ndeep bidirectional transformers for language understand-\ning. In NAACL-HLT, 2019.\n[Dong et al. , 2019 ] Li Dong, Nan Y ang, W enhui W ang, Furu\nW ei, Xiaodong Liu, Y u W ang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. Uniﬁed language model pre-\ntraining for natural language understanding and genera-\ntion. In NeurIPS, 2019.\n[Fan et al. , 2019 ] Zhiyun Fan, Shiyu Zhou, and Bo Xu. Un-\nsupervised pre-training for sequence to sequence speech\nrecognition. CoRR, arXiv preprint arXiv:1910.12418,\n2019.\n[Gehring et al. , 2017 ] Jonas Gehring, Michael Auli, David\nGrangier, Denis Y arats, and Y ann N. Dauphin. Convo-\nlutional sequence to sequence learning. In ICML, 2017.\n[Gong et al. , 2020 ] Heng Gong, Y awei Sun, Xiaocheng\nFeng, Bing Qin, W ei Bi, Xiaojiang Liu, and Ting Liu.\nT ablegpt: Few-shot table-to-text generation with table\nstructure reconstruction and content matching. In COL-\nING, 2020.\n[Gu et al. , 2020 ] Jing Gu, Qingyang Wu, Chongruo Wu,\nW eiyan Shi, and Zhou Y u. A tailored pre-training\nmodel for task-oriented dialog generation. arXiv preprint\narXiv:2004.13835 , 2020.\n[Guan et al. , 2020 ] W ang Guan, Ivan Smetannikov, and Man\nTianxing. Survey on automatic text summarization and\ntransformer models applicability. In CCRIS, 2020.\n[Hendrycks et al. , 2020 ] Dan Hendrycks, Xiaoyuan Liu,\nEric W allace, Adam Dziedzic, Rishabh Krishnan, and\nDawn Song. Pretrained transformers improve out-of-\ndistribution robustness. In ACL, 2020.\n[Keskar et al. , 2019 ] Nitish Shirish Keskar, Bryan McCann,\nLav R. V arshney, Caiming Xiong, and Richard Socher.\nCTRL: A conditional transformer language model for con-\ntrollable generation. arXiv preprint arXiv:1909.05858 ,\n2019.\n[Kryscinski et al. , 2018 ] W ojciech Kryscinski, Romain\nPaulus, Caiming Xiong, and Richard Socher. Improving\nabstraction in text summarization. In EMNLP, 2018.\n[Lan et al. , 2020 ] Zhenzhong Lan, Mingda Chen, Sebastian\nGoodman, Kevin Gimpel, Piyush Sharma, and Radu Sori-\ncut. ALBER T: A lite BER T for self-supervised learning of\nlanguage representations. In ICLR, 2020.\n[Lewis et al. , 2020 ] Mike Lewis, Y inhan Liu, and Na-\nman Goyal et al. BAR T: denoising sequence-to-sequence\npre-training for natural language generation, translatio n,\nand comprehension. In ACL, 2020.\n[Li et al. , 2019 ] Junyi Li, W ayne Xin Zhao, Ji-Rong W en,\nand Y ang Song. Generating long and informative reviews\nwith aspect-aware coarse-to-ﬁne decoding. In ACL, pages\n1969–1979, 2019.\n[Li et al. , 2020 ] Junyi Li, Siqing Li, W ayne Xin Zhao, Gaole\nHe, Zhicheng W ei, Nicholas Jing Y uan, and Ji-Rong W en.\nKnowledge-enhanced personalized review generation with\ncapsule graph neural network. In CIKM, pages 735–744,\n2020.\n[Li et al. , 2021a ] Junyi Li, Tianyi T ang, Gaole He, Jinhao\nJiang, Xiaoxuan Hu, Puzhao Xie, Zhipeng Chen, Zhuo-\nhao Y u, W ayne Xin Zhao, and Ji-Rong W en. T extbox:\nA uniﬁed, modularized, and extensible framework for text\ngeneration. arXiv preprint arXiv:2101.02046 , 2021.\n[Li et al. , 2021b ] Junyi Li, Tianyi T ang, W ayne Xin Zhao,\nZhicheng W ei, Nicholas Jing Y uan, and Ji-Rong W en.\nFew-shot knowledge graph-to-text generation with pre-\ntrained language models. In Findings of ACL , 2021.\n[Li et al. , 2021c ] Junyi Li, W ayne Xin Zhao, Zhicheng W ei,\nNicholas Jing Y uan, and Ji-Rong W en. Knowledge-based\nreview generation by coherence enhanced text planning.\nIn SIGIR, 2021.\n[Lin et al. , 2020 ] Zehui Lin, Xiao Pan, Mingxuan W ang,\nXipeng Qiu, Jiangtao Feng, Hao Zhou, and Lei Li. Pre-\ntraining multilingual neural machine translation by lever -\naging alignment information. In EMNLP, 2020.\n[Liu and Lapata, 2019 ] Y ang Liu and Mirella Lapata. T ext\nsummarization with pretrained encoders. In EMNLP,\n2019.\n[Mager et al. , 2020 ] Manuel Mager, Ram ´ on Fernandez As-\ntudillo, T ahira Naseem, Md. Arafat Sultan, Y oung-Suk\nLee, Radu Florian, and Salim Roukos. Gpt-too: A\nlanguage-model-ﬁrst approach for amr-to-text generation .\nIn ACL, 2020.\n[Peters et al. , 2018 ] Matthew E. Peters, Mark Neumann,\nMohit Iyyer, Matt Gardner, Christopher Clark, Kenton\nLee, and Luke Zettlemoyer. Deep contextualized word\nrepresentations. In NAACL-HLT, 2018.\n[Qiu et al. , 2020 ] Xipeng Qiu, Tianxiang Sun, Y ige Xu,\nY unfan Shao, Ning Dai, and Xuanjing Huang. Pre-trained\nmodels for natural language processing: A survey. arXiv\npreprint arXiv:2003.08271 , 2020.\n[Radford et al. , 2019 ] Alec Radford, Jeff Wu, Rewon Child,\nDavid Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. 2019.\n[Raffel et al. , 2020 ] Colin Raffel, Noam Shazeer, Adam\nRoberts, Katherine Lee, Sharan Narang, Michael Matena,\nY anqi Zhou, W ei Li, and Peter J. Liu. Exploring the limits\nof transfer learning with a uniﬁed text-to-text transforme r.\nJMLR, 2020.\n[Ribeiro et al. , 2020 ] Leonardo F . R. Ribeiro, Martin\nSchmitt, Hinrich Sch ¨ utze, and Iryna Gurevych. Inves-\ntigating pretrained language models for graph-to-text\ngeneration. arXiv preprint arXiv:2007.08426 , 2020.\n[Rothe et al. , 2020 ] Sascha Rothe, Shashi Narayan, and Ali-\naksei Severyn. Leveraging pre-trained checkpoints for se-\nquence generation tasks. TACL, 2020.\n[Sanh et al. , 2019 ] V ictor Sanh, Lysandre Debut, Julien\nChaumond, and Thomas W olf. Distilbert, a distilled ver-\nsion of BER T: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108 , 2019.\n[See et al. , 2017 ] Abigail See, Peter J. Liu, and Christo-\npher D. Manning. Get to the point: Summarization with\npointer-generator networks. In ACL, 2017.\n[Song et al. , 2019 ] Kaitao Song, Xu T an, T ao Qin, Jianfeng\nLu, and Tie-Y an Liu. MASS: masked sequence to se-\nquence pre-training for language generation. In ICML,\n2019.\n[Sun et al. , 2019a ] Chen Sun, Fabien Baradel, Kevin Mur-\nphy, and Cordelia Schmid. Contrastive bidirectional\ntransformer for temporal representation learning. arXiv\npreprint arXiv:1906.05743 , 2019.\n[Sun et al. , 2019b ] Chen Sun, Austin Myers, Carl V ondrick,\nKevin Murphy, and Cordelia Schmid. V ideobert: A joint\nmodel for video and language representation learning. In\nICCV, 2019.\n[V aswani et al. , 2017 ] Ashish V aswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In NIPS, 2017.\n[W ada and Iwata, 2018 ] T akashi W ada and T omoharu Iwata.\nUnsupervised cross-lingual word embedding by mul-\ntilingual neural language models. arXiv preprint\narXiv:1809.02306 , 2018.\n[W olf et al. , 2019 ] Thomas W olf, V ictor Sanh, Julien Chau-\nmond, and Clement Delangue. Transfertransfo: A transfer\nlearning approach for neural network based conversational\nagents. arXiv preprint arXiv:1901.08149 , 2019.\n[Xia et al. , 2020 ] Qiaolin Xia, Haoyang Huang, Nan Duan,\nDongdong Zhang, Lei Ji, Zhifang Sui, Edward Cui, T aroon\nBharti, Xin Liu, and Ming Zhou. XGPT: cross-modal gen-\nerative pre-training for image captioning. arXiv preprint\narXiv:2003.01473 , 2020.\n[Xu et al. , 2020a ] Jiacheng Xu, Zhe Gan, Y u Cheng, and\nJingjing Liu. Discourse-aware neural extractive text sum-\nmarization. In ACL, 2020.\n[Xu et al. , 2020b ] Shusheng Xu, Xingxing Zhang, Y i Wu,\nFuru W ei, and Ming Zhou. Unsupervised extractive sum-\nmarization by pre-training hierarchical transformers. In\nEMNLP, 2020.\n[Y ang et al. , 2020a ] Zhen Y ang, Bojie Hu, Ambyera Han,\nShen Huang, and Qi Ju. CSP: code-switching pre-training\nfor neural machine translation. In EMNLP, 2020.\n[Y ang et al. , 2020b ] Ziyi Y ang, Chenguang Zhu, Robert\nGmyr, Michael Zeng, Xuedong Huang, and Eric Darve.\nTED: A pretrained unsupervised summarization model\nwith theme modeling and denoising. In EMNLP (Find-\nings), 2020.\n[Zaib et al. , 2020 ] Munazza Zaib, Quan Z. Sheng, and\nW ei Emma Zhang. A short survey of pre-trained lan-\nguage models for conversational AI-A new age in NLP.\nIn ACSW, 2020.\n[Zeng and Nie, 2020 ] Y an Zeng and Jian-Y un Nie. General-\nized conditioned dialogue generation based on pre-trained\nlanguage model. arXiv preprint arXiv:2010.11140 , 2020.\n[Zhang et al. , 2019a ] Haoyu Zhang, Jingjing Cai, Jianjun\nXu, and Ji W ang. Pretraining-based natural language gen-\neration for text summarization. In CoNLL, 2019.\n[Zhang et al. , 2019b ] Xingxing Zhang, Furu W ei, and Ming\nZhou. HIBER T: document level pre-training of hierarchi-\ncal bidirectional transformers for document summariza-\ntion. In ACL, 2019.\n[Zhang et al. , 2019c ] Zhengyan Zhang, Xu Han, Zhiyuan\nLiu, Xin Jiang, Maosong Sun, and Qun Liu. ERNIE: en-\nhanced language representation with informative entities .\nIn ACL, 2019.\n[Zhang et al. , 2020 ] Y izhe Zhang, Siqi Sun, Michel Galley,\nY en-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng\nGao, Jingjing Liu, and Bill Dolan. DIALOGPT : Large-\nscale generative pre-training for conversational respons e\ngeneration. In ACL, 2020.\n[Zhao et al. , 2020 ] Xueliang Zhao, W ei Wu, Can Xu,\nChongyang T ao, Dongyan Zhao, and Rui Y an.\nKnowledge-grounded dialogue generation with pre-\ntrained language models. In EMNLP, 2020.\n[Zheng and Lapata, 2019 ] Hao Zheng and Mirella Lapata.\nSentence centrality revisited for unsupervised summariza -\ntion. In ACL, 2019.\n[Zhou et al. , 2020 ] Luowei Zhou, Hamid Palangi, Lei\nZhang, Houdong Hu, Jason J. Corso, and Jianfeng Gao.\nUniﬁed vision-language pre-training for image captioning\nand VQA. In AAAI, 2020.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7827544212341309
    },
    {
      "name": "Text generation",
      "score": 0.6970434784889221
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6339568495750427
    },
    {
      "name": "Natural language processing",
      "score": 0.5520249009132385
    },
    {
      "name": "Pointer (user interface)",
      "score": 0.5054261684417725
    },
    {
      "name": "Deep learning",
      "score": 0.4915972650051117
    },
    {
      "name": "Task (project management)",
      "score": 0.4766193926334381
    },
    {
      "name": "Language model",
      "score": 0.4760044515132904
    },
    {
      "name": "Field (mathematics)",
      "score": 0.4380420446395874
    },
    {
      "name": "Natural language generation",
      "score": 0.4177689850330353
    },
    {
      "name": "Natural language",
      "score": 0.2839867174625397
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 14
}