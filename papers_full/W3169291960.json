{
  "title": "R00 at NLP4IF-2021 Fighting COVID-19 Infodemic with Transformers and More Transformers",
  "url": "https://openalex.org/W3169291960",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5057650554",
      "name": "Ahmed Al-Qarqaz",
      "affiliations": [
        "Jordan University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5074825597",
      "name": "Dia Abujaber",
      "affiliations": [
        "Jordan University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5061516287",
      "name": "Malak Abdullah",
      "affiliations": [
        "Jordan University of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3008110149",
    "https://openalex.org/W2982953871",
    "https://openalex.org/W3002801259",
    "https://openalex.org/W3117847293",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3045210825",
    "https://openalex.org/W3119989665",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4324392584",
    "https://openalex.org/W3116641301",
    "https://openalex.org/W3153497191",
    "https://openalex.org/W3088592174",
    "https://openalex.org/W4297782574",
    "https://openalex.org/W3024622987",
    "https://openalex.org/W3131645232",
    "https://openalex.org/W3155586400",
    "https://openalex.org/W4287815367"
  ],
  "abstract": "This paper describes the winning model in the Arabic NLP4IF shared task for fighting the COVID-19 infodemic. The goal of the shared task is to check disinformation about COVID-19 in Arabic tweets. Our proposed model has been ranked 1st with an F1-Score of 0.780 and an Accuracy score of 0.762. A variety of transformer-based pre-trained language models have been experimented with through this study. The best-scored model is an ensemble of AraBERT-Base, Asafya-BERT, and ARBERT models. One of the study's key findings is showing the effect the pre-processing can have on every model's score. In addition to describing the winning model, the current study shows the error analysis.",
  "full_text": "Proceedings of the 4th NLP4IF Workshop on NLP for Internet Freedom, pages 104–109\nJune 6, 2021. ©2021 Association for Computational Linguistics\n104\nR00 at NLP4IF-2021: Fighting COVID-19 Infodemic with Transformers\nand More Transformers\nAhmed Qarqaz Dia Abujaber Malak A. Abdullah\nJordan University of Science and Technology\nIrbid, Jordan\nafalqarqaz17, daabujaber17@cit.just.edu.jo\nmabdullah@just.edu.jo\nAbstract\nThis paper describes the winning model in\nthe Arabic NLP4IF shared task for ﬁghting\nthe COVID-19 infodemic. The goal of the\nshared task is to check disinformation about\nCOVID-19 in Arabic tweets. Our proposed\nmodel has been ranked 1st with an F1-Score of\n0.780 and an Accuracy score of 0.762. A vari-\nety of transformer-based pre-trained language\nmodels have been experimented with through\nthis study. The best-scored model is an ensem-\nble of AraBERT-Base, Asafya-BERT, and AR-\nBERT models. One of the study’s key ﬁnd-\nings is showing the effect the pre-processing\ncan have on every model’s score. In addition\nto describing the winning model, the current\nstudy shows the error analysis.\n1 Introduction\nSocial media platforms are highly used for express-\ning and delivering ideas. Most people on social\nmedia platforms tend to spread and share posts\nwithout fact-checking the story or the source. Con-\nsequently, the propaganda is posted to promote a\nparticular ideology to create further confusion in\nunderstanding an event. Of course, it does not ap-\nply to all posts. However, there is a line between\npropaganda and factual news, blurred for people\nengaged in these platforms (Abedalla et al., 2019).\nAnd thus, social media can act as a distortion for\ncritical and severe events. The COVID-19 pan-\ndemic is one such event.\nSeveral previous works were published for us-\ning language models and machine learning tech-\nniques for detecting misinformation. Authors in\nHaouari et al. (2020b) presented a twitter data\nset for COVID-19 misinformation detection called\n\"ArCOV19-Rumors\". It is an extension of the\n\"ArCOV-19\" (Haouari et al., 2020a), which is a\ndata set of Twitter posts with \"Propagation Net-\nworks\". Propagation networks refer to a post’s\nretweets and conversational threads. Other authors\nin Shahi et al. (2021) performed an exploratory\nstudy of COVID-19 misinformation on Twitter.\nThey collected data from Twitter and identiﬁed\nmisinformation, rumors on Twitter, and misinfor-\nmation propagation. Authors in Müller et al. (2020)\npresented CT-BERT, a transformer-based model\npre-trained on English Twitter data. Other works\nthat used Deep Learning models to detect propa-\nganda in news articles (Al-Omari et al., 2019; Altiti\net al., 2020).\nThe NLP4IF (Shaar et al., 2021) shared-task of-\nfers an annotated data set of tweets to check disin-\nformation about COVID-19 in each tweet. The task\nasked the participants to propose models that can\npredict the disinformation in these tweets. This pa-\nper describes the winning model in the shared task,\nan ensemble of AraBERT-Base, Asafya-BERT, and\nARBERT pre-trained language models. The team\nR00’s model outperformed the other teams and\nbaseline models with an F1-Score of 0.780 and an\nAccuracy score of 0.762. This paper describes the\nDataset and the shared task in section 2. The Data\nPreprocessing step is presented in section 3. The\nexperiments with the pre-trained language models\nare provided in section 4. Finally, the proposed\nwinning model and methodology are discussed in\nsection 5.\n2 Dataset\nThe Data provided by the organizers Shaar et al.,\n2021 comprised of tweets, which are posts from the\nTwitter social media platform \"twitter.com\". The\nposts are related to the COVID-19 pandemic and\nhave been annotated in a \"Yes or No\" question style\nannotation. The annotator was asked to read the\npost/tweet and go to an afﬁliated weblink (if the\ntweet contains one). For each tweet, the seven main\nquestions that were asked are:\n1. Veriﬁable Factual Claim: Does the tweet contain a veri-\nﬁable factual claim?\n105\n2. False Information: To what extent does the tweet appear\nto contain false information?\n3. Interest to General Public:Will the tweet affect or be of\ninterest to the general public?\n4. Harmfulness: To what extent is the tweet harmful to the\nsociety/person(s)/company(s)/product(s)?\n5. Need of Veriﬁcation: Do you think that a professional\nfact-checker should verify the claim in the tweet?\n6. Harmful to Society: Is the tweet harmful the society and\nwhy?\n7. Require attention: Do you think that this tweet should\nget the attention of government entities?\nFor each question, the answer can be \"Yes\" or\n\"No\". However the questions two through ﬁve de-\npend on the ﬁrst question. If the ﬁrst question\n(Veriﬁable Factual Claim) is answered \"No\", ques-\ntions two through ﬁve will be labeled as \"NaN\".\n\"NaN\" is interpreted as there’s no need to ask the\nquestion. For example, for the following tweet:\n\"maybe if i develop feelings for covid-19 it will\nleave\".\nThis tweet is not a veriﬁable factual claim.\nTherefore asking whether it’s False Information\nor is in Need of Veriﬁcation is unnecessary. More-\nover, our model modiﬁed the values to be \" No\" for\nall text samples with labels annotated as \"NaN\".\nTask Our team participated in the Arabic text\nshared task. The Arabic data set consists of 2,536\ntweets for the training data, 520 tweets for the de-\nvelopment (validation) data, and 1,000 tweets for\nthe test data. It has been observed that the label\ndistribution in the training data is unbalanced, as\nshown in Figure 1.\nQ1 Q2 Q3 Q4 Q5 Q6 Q7\n500\n1,000\n1,500\n2,000\nFigure 1: label distribution in data. Unbalance labels\nfor questions.\n3 Data Pre-Processing\nSocial media posts can contain noisy features, par-\nticularly the special characters (#, @, emojis, we-\nblinks, etc..). Many elements within Arabic text\ncan act as distortions for the model. We Tokenize\nthe Arabic text 1, and for each sequence of tokens,\nwe remove stop-words, numbers, and punctuation\nfrom the text. We also remove any non-Arabic\nterms in the text. Stemming and Segmentation\nare two common pre-processing operations done\nin Arabic Natural Language Processing. However,\nwe do not apply them here, except in the case of\nAraBERT, where segmentation was applied.\n4 Fine-tuning Pre-Trained Language\nModels\nWe approach the problem as a multi-label classiﬁ-\ncation problem. For each label in a text sample, the\nlabel’s value can be one (yes) or zero (no). In the\ntraining phase, we load the pre-trained language\nmodel (along with its corresponding tokenizer) and\nstack a linear classiﬁer on top of the model.\nThis section describes the pre-trained Arabic lan-\nguage models that have been used in the study. The\nhyperparameters’ ﬁne-tuning is also detailed in this\nsection in addition to the experiments’ results.\n4.1 Pre-trained Arabic Language Models\nThis section goes over the pre-trained language\nmodels experimented with through the study:\nAraBERT, Asafaya-BERT, ARBERT, and MAR-\nBERT.\n• AraBERT (Antoun et al.) follows the orig-\ninal BERT pre-training (Devlin et al., 2018),\nemploying the Masked Language Modelling\ntask. It was pre-trained on roughly 70-\nmillion sentences amounting to 24GB of\ntext data. There are four variations of the\nmodel: AraBERTv0.2-base, AraBERTv0.2-\nlarge, AraBERTv2-base, AraBERTv2-large .\nThe difference is that the v2 variants were\ntrained on the pre-segmented text where pre-\nﬁxes and sufﬁxes were split, whereas the v0.2\nwere not. The models we used are the v0.2\nvariants. the Authors recommended using the\nArabert-Preprocessor powered by the faras-\napy2 python package for the v2 versions. Al-\nthough the v0.2 models don’t require it, we\n1Preprocessing was done using the NLTK Library\n2farasapy\n106\nhave found that the Arabert-Preprocessor im-\nproves the performance signiﬁcantly for some\nexperiments. So, we have used it with all the\nAraBERT models only.\n• Asafaya-BERT (Safaya et al., 2020) is a\nmodel also based on the BERT architecture.\nThis model was pre-trained on 8.2B words,\nwith a vocabulary of 32,000 word-pieces. The\ncorpus the model was pre-trained on was not\nrestricted to Modern Standard Arabic, as they\ncontain some dialectal Arabic, and as such\nSafaya et al. (2020) argue that this boosts the\nmodel’s performance on data gathered from\nsocial media platforms. There are four vari-\nants of the model: Large, Base, Medium, and\nMini. We only used Large and Base.\n• ARBERT (Abdul-Mageed et al., 2020) is a\npre-trained model focused on Modern Stan-\ndard Arabic (MSA). It was trained on 61GB\nof text data, with a vocabulary of 100K Word-\nPieces. There is only one variation of this\nmodel, which follows the BERT-Base archi-\ntecture. It uses 12-attention layers (each\nwith 12-attention heads) and 768 hidden-\ndimension. We use this model to possibly\nwrite some tweets (such as news updates) for-\nmally following MSA.\n• MARBERT (Abdul-Mageed et al., 2020) ar-\ngues that since AraBERT and ARBERT are\ntrained on MSA text, these models are not\nwell suited for tasks involving dialectal Ara-\nbic, which is what social media posts often\nare. MARBERT was trained on a large Twitter\ndata set comprised of 6B tweets, making up\nabout 128GB of text data. MARBERT follows\nthe BERT-Base architecture but without sen-\ntence prediction. MARBERT uses the same\nvocabulary as ARBERT (100K Word-Pieces).\n4.2 Fine-Tuning\nEach model has been trained for 20 epochs. We\nfound that after the 10th epoch, most of the model\nscores start to plateau. This is, of course, highly\ndependent on the learning rate used for each model.\nWe have not tuned the models’ learning rates, and\nrather we chose the learning rate we found best\nafter doing multiple experiments with each model.\nWe use a Training Batch-Sizeof 32 and a Valida-\ntion Batch-Sizeof 16 for all the models. For each\nmodel’s tokenizer we choose a Max Sequence-\nlength of 100.\nEach model has been trained on two versions of\nthe data set, one that has not been pre-processed\n(We refer to it as \"Raw\") and one that has been pre-\nprocessed (we refer to it as \"Cleaned\"). A model\nthat has been trained on cleaned data in training\ntime will also receive cleaned text at validation\nand testing time. We apply the post-processing\nstep, where for the labels Question-2, 3, 4, and\nQuestion-5, if a model predicts that Question-1 is\n\"No\" then the values of the mentioned Questions\n(Q2 through Q5) will be \"NaN\" Unconditionally.\nThis, of course, assumes that the model can per-\nform well on the ﬁrst question. We report the re-\nsults in Table 1.\nNote: It is worth noting that, initially, we save\nthe model on the ﬁrst epoch along with its score as\nthe \"best-score\". After each epoch, we compare the\nscore of the model on that epoch with the best score.\nIf the model’s current score is higher than the best\nscore, the model will be saved, and the model’s best\nscore will be overwritten as the current model’s\nscore. And as such, saying we train a model for\n20 epochs is not an accurate descriptionof the\nmodel’s training. The score we used as criteria for\nsaving was the Weighted F1-Score.\n4.3 Results\nWe see (in Table 1) that generally, training on\ncleaned data either gave slightly better scores or\nno signiﬁcant improvement, with ARBERT 4.1 be-\ning the exception. This is because ARBERT was\nspeciﬁcally trained on Arabic text that followed the\nModern Standard Arabic. Cleaning has normalized\ntext for the model and removed features in the text\nthat may otherwise act as noise. Furthermore, we\nconclude that Asafya-BERT 4.1 has a better perfor-\nmance when trained on Raw data, proving that a\nmodel pre-trained on Twitter data would perform\nbetter. Lastly, we observe that using a larger model\n(deeper network) does provide a slight improve-\nment over using the Base version. 3\n5 Ensemble Pre-trained language Models\nTo maximize the scores, we resort to ensembling\nsome of the models we ﬁne-tuned on the data set.\nEnsemble models are known to improve accuracy\n3Results and scores were generated using the Scikit-learn\nlibrary\n107\nID Model Data Learning Rate F1-Weighted F1-Micro Accuracy\n(1) AraBERT-Base Raw 3e−6 0.703 0.727 0.338\n(2) AraBERT-Base Cleaned 3e−5 0.735 0.725 0.394\n(3) AraBERT-Large Raw 3e−5 0.733 0.737 0.390\n(4) AraBERT-Large Cleaned 3e−5 0.747 0.749 0.425\n(5) MARBERT Raw 4e−5 0.737 0.741 0.382\n(6) MARBERT Cleaned 4e−6 0.735 0.735 0.413\n(7) ARBERT Raw 8e−6 0.715 0.728 0.407\n(8) ARBERT Cleaned 3e−5 0.734 0.745 0.398\n(9) Asafaya-Base Raw 5e−6 0.750 0.749 0.413\n(10) Asafaya-Base Cleaned 3e−5 0.707 0.743 0.382\n(11) Asafaya-Large Raw 5e−6 0.750 0.752 0.436\n(12) Asafaya-Large Cleaned 5e−6 0.737 0.743 0.373\nTable 1: Shows model scores on the validation data set. The Weighted F1-Score and the Micro F1-Score are the\naverage F1-Scores of the labels.\nID Model Q1 Q2 Q3 Q4 Q5 Q6 Q7\n(1) AraBERT-Base 0.73 0.11 0.71 0.22 0.37 0.43 0.84\n(2) AraBERT-Base 0.76 0.26 0.75 0.38 0.42 0.55 0.83\n(4) AraBERT-Large 0.81 0.16 0.79 0.32 0.42 0.50 0.85\n(5) MARBERT 0.78 0.12 0.78 0.36 0.43 0.44 0.84\n(6) MARBERT 0.75 0.10 0.74 0.52 0.48 0.54 0.84\n(8) ARBERT 0.78 0.19 0.78 0.36 0.44 0.53 0.83\n(10) Asafya-Base 0.78 0.11 0.77 0.30 0.22 0.39 0.84\n(12) Asafya-Large 0.79 0.18 0.78 0.40 0.35 0.48 0.84\nTable 2: Shows models F1-Scores for the labels on the validation data set.\nunder the right conditions. If two models can de-\ntect different data patterns, then ensembling these\ntwo models would perhaps (in theory) give a better\nprediction. Of course, the process of ﬁnding a good\nensemble is an empirical one. It involves a process\nof trial-and-error of combining different models\nand choosing the best one. However, as we show in\nTable 1 various combinations can be done, and as\na result, trying all combinations would perhaps be\nimpractical. We mention in Section-2 that the label\ndistribution in the data set is unbalanced, and hence\nfor labels like Question-2 (False Information), the\nmodel can give poor predictions for the answer to\nthat label. However, suppose we were to acquire a\nmodel (through experimentation) that tends to per-\nform well in predicting that label. In that case, we\ncould ensemble this model with one that generally\nperforms well to get a better overall score.\nStrategy Through experimentation and for each\nlabel, train a model that performs well on that label\nand save it for an ensemble. Then, train a model\nthat generally performs well on all labels (rela-\ntive to the models at hand) and save it for an en-\nsemble. After collecting several models, ensemble\nthese models through various combinations. And\nfor each ensemble, record the combination and its\nscore (performance on validation data). Choose the\nbest performing ensemble.\nWeighted-Average Our approach for an ensem-\nble is to take the weighted average of each’s model\npredictions for each sample. Each model produces\na vector of probabilities (whose length is equal\nto the number of labels) for each tweet. We take\nthe weighted average point-wise and then apply a\n0.5-threshold to decide if a label is one (yes) or\nzero (no). We suggest using the weighted average\nrather than a normal average with equal weights to\n108\nFigure 2: Shows Ensemble architecture. Each model has its classiﬁer stacked on top. The models receive the text\npre-processed and produce logits. Logits are then inserted into a Sigmoid layer making predictions. Prediction\nvectors are multiplied with a scalar (the weight), and the weighted average is calculated point-wise.\ngive higher conﬁdence to the generally performing\nmodel as opposed to the less generally perform-\ning one. The intuition is that you would want the\nmodel to be the deciding factor in predicting better\noverall performance. The models with the lesser\nweights are merely there to increase the models’\nconﬁdence in predicting some labels. The optimal\nweights for an ensemble are obtainable through ex-\nperimentation. As a hyperparameter, they can be\ntuned.\nProposed Model We ensemble ﬁve models as\nshown in Figure 2, all of them were trained on\ncleaned data. And so, the models were tested on\ncleaned data. The models are:\n1. Model (2): AraBERT-Base, with a weight of 3.\n2. Model (4): Asafya-BERT-Large, with a weight of 3\n3. Model (10): Asafya-BERT-Base, with a weight of 1.\n4. Model (12): AraBERT-Large, with a weight of 1.\n5. Model (8): ARBERT, with a weight of 3.\nOur model achieved an F1-Weighted Score of\n0.749, an F1-Micro Score of 0.763, and an Ac-\ncuracy of 0.405 on validation data. It also earned\nan F1-Weighted Score of 0.781 and an Accuracy\nof 0.763 on the Test data. These results made the\nmodel ranked the ﬁrst mode since it is the top-\nperforming model in the shared task. Figure 3\npresents the confusion matrix for the Ensemble-\nmodel predictions on the labels.\n6 Conclusion\nThis paper described the winning model in the\nNLP4IF 2021 shared task. The task aimed to check\nQ1 Q2 Q3 Q4 Q5 Q6 Q7 None\nQ1Q2Q3Q4Q5Q6Q7None\n186 0 0 0 0 0 0 39\n0 3 0 0 0 0 0 9\n0 0 181 0 0 0 0 40\n0 0 0 9 0 0 0 14\n0 0 0 0 33 0 0 74\n0 0 0 0 0 23 0 18\n0 0 0 0 0 0 356 23\n52 9 50 8 20 18 116 0\n0\n25\n50\n75\n100\n125\n150\n175\nFigure 3: Shows confusion matrix for the Ensemble-\nmodel predictions on the labels. The Y-axis represents\nthe True-Label while the X-axis represents the Pre-\ndicted-label.\ndisinformation about COVID-19 in Arabic tweets.\nWe have ensembled ﬁve pre-trained language mod-\nels to obtain the highest F1-score of 0.780 and\nan Accuracy score of 0.762. We have shown the\nperformances of every pre-trained language model\non the data set. We also have shown some of the\nmodels’ performances on each label. Moreover,\nwe have demonstrated the confusion matrix for\nthe ensemble model. We have illustrated that a\npre-trained model on Twitter data (Asafya-Bert in\nSection 4.1) will perform better relative to a model\nthat hasn’t.\n109\nReferences\nMuhammad Abdul-Mageed, AbdelRahim Elmadany,\nand El Moatez Billah Nagoudi. 2020. Arbert &\nmarbert: Deep bidirectional transformers for arabic.\narXiv preprint arXiv:2101.01785.\nAyat Abedalla, Aisha Al-Sadi, and Malak Abdullah.\n2019. A closer look at fake news detection: A deep\nlearning perspective. In Proceedings of the 2019 3rd\nInternational Conference on Advances in Artiﬁcial\nIntelligence, pages 24–28.\nHani Al-Omari, Malak Abdullah, Ola AlTiti, and\nSamira Shaikh. 2019. JUSTDeep at NLP4IF 2019\ntask 1: Propaganda detection using ensemble deep\nlearning models. In Proceedings of the Second\nWorkshop on Natural Language Processing for Inter-\nnet Freedom: Censorship, Disinformation, and Pro-\npaganda, pages 113–118, Hong Kong, China. Asso-\nciation for Computational Linguistics.\nOla Altiti, Malak Abdullah, and Rasha Obiedat. 2020.\nJUST at SemEval-2020 task 11: Detecting propa-\nganda techniques using BERT pre-trained model. In\nProceedings of the Fourteenth Workshop on Seman-\ntic Evaluation , pages 1749–1755, Barcelona (on-\nline). International Committee for Computational\nLinguistics.\nWissam Antoun, Fady Baly, and Hazem Hajj. Arabert:\nTransformer-based model for arabic language un-\nderstanding. In LREC 2020 Workshop Language\nResources and Evaluation Conference 11–16 May\n2020, page 9.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nFatima Haouari, Maram Hasanain, Reem Suwaileh,\nand Tamer Elsayed. 2020a. Arcov-19: The ﬁrst\narabic covid-19 twitter dataset with propagation net-\nworks. arXiv preprint arXiv:2004.05861, 3(1).\nFatima Haouari, Maram Hasanain, Reem Suwaileh,\nand Tamer Elsayed. 2020b. Arcov19-rumors: Ara-\nbic covid-19 twitter dataset for misinformation de-\ntection. arXiv preprint arXiv:2010.08768.\nMartin Müller, Marcel Salathé, and Per E Kummervold.\n2020. Covid-twitter-bert: A natural language pro-\ncessing model to analyse covid-19 content on twitter.\narXiv preprint arXiv:2005.07503.\nAli Safaya, Moutasem Abdullatif, and Deniz Yuret.\n2020. Kuisail at semeval-2020 task 12: Bert-cnn for\noffensive speech identiﬁcation in social media.\nShaden Shaar, Firoj Alam, Giovanni Da San Martino,\nAlex Nikolov, Wajdi Zaghouani, Preslav Nakov, and\nAnna Feldman. 2021. Findings of the NLP4IF-\n2021 shared task on ﬁghting the COVID-19 info-\ndemic and censorship detection. In Proceedings of\nthe Fourth Workshop on Natural Language Process-\ning for Internet Freedom: Censorship, Disinforma-\ntion, and Propaganda , NLP4IF@NAACL’ 21, On-\nline. Association for Computational Linguistics.\nGautam Kishore Shahi, Anne Dirkson, and Tim A Ma-\njchrzak. 2021. An exploratory study of covid-19\nmisinformation on twitter. Online Social Networks\nand Media, 22:100104.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6994206309318542
    },
    {
      "name": "Transformer",
      "score": 0.6969782114028931
    },
    {
      "name": "Arabic",
      "score": 0.6067990660667419
    },
    {
      "name": "Language model",
      "score": 0.5274577140808105
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49555647373199463
    },
    {
      "name": "Coronavirus disease 2019 (COVID-19)",
      "score": 0.48391085863113403
    },
    {
      "name": "Disinformation",
      "score": 0.47839677333831787
    },
    {
      "name": "F1 score",
      "score": 0.45512908697128296
    },
    {
      "name": "Natural language processing",
      "score": 0.43364885449409485
    },
    {
      "name": "Machine learning",
      "score": 0.4089427888393402
    },
    {
      "name": "World Wide Web",
      "score": 0.16311904788017273
    },
    {
      "name": "Engineering",
      "score": 0.14122098684310913
    },
    {
      "name": "Social media",
      "score": 0.13811758160591125
    },
    {
      "name": "Linguistics",
      "score": 0.10689735412597656
    },
    {
      "name": "Voltage",
      "score": 0.08458158373832703
    },
    {
      "name": "Infectious disease (medical specialty)",
      "score": 0.07689851522445679
    },
    {
      "name": "Medicine",
      "score": 0.0
    },
    {
      "name": "Pathology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Disease",
      "score": 0.0
    }
  ]
}