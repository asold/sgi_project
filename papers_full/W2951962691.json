{
  "title": "Language Modeling with Shared Grammar",
  "url": "https://openalex.org/W2951962691",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A5029130689",
      "name": "Yuyu Zhang",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5102898115",
      "name": "Le Song",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2171645483",
    "https://openalex.org/W2963084773",
    "https://openalex.org/W2963537482",
    "https://openalex.org/W2119717200",
    "https://openalex.org/W2108682071",
    "https://openalex.org/W2020382207",
    "https://openalex.org/W2554915555",
    "https://openalex.org/W2123893795",
    "https://openalex.org/W2133280805",
    "https://openalex.org/W2546938941",
    "https://openalex.org/W4300427683",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W1512626953",
    "https://openalex.org/W2963617989",
    "https://openalex.org/W2963355447",
    "https://openalex.org/W2574917025",
    "https://openalex.org/W2952264928",
    "https://openalex.org/W2126433015",
    "https://openalex.org/W2963012544",
    "https://openalex.org/W2581637843",
    "https://openalex.org/W2743945814",
    "https://openalex.org/W2963069010",
    "https://openalex.org/W2963754491",
    "https://openalex.org/W2951714314",
    "https://openalex.org/W2751185861",
    "https://openalex.org/W2962728167",
    "https://openalex.org/W2963248348",
    "https://openalex.org/W2751262944",
    "https://openalex.org/W2963068946",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2890560993",
    "https://openalex.org/W2510842514",
    "https://openalex.org/W1535015163",
    "https://openalex.org/W2962883166",
    "https://openalex.org/W1869752048",
    "https://openalex.org/W2097606805",
    "https://openalex.org/W2564486991",
    "https://openalex.org/W2963003490",
    "https://openalex.org/W4299838440",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2963748792",
    "https://openalex.org/W2963735467",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W2798471433",
    "https://openalex.org/W2757836268",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2963073938",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W1624142810",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2295582178",
    "https://openalex.org/W2798921788",
    "https://openalex.org/W2755637027",
    "https://openalex.org/W1591801644",
    "https://openalex.org/W2139621418",
    "https://openalex.org/W2964222296",
    "https://openalex.org/W2399550240",
    "https://openalex.org/W2888799392",
    "https://openalex.org/W2616180702",
    "https://openalex.org/W2949952998",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2899771611",
    "https://openalex.org/W2134036914",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2098050104",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W2116009284",
    "https://openalex.org/W2747236259"
  ],
  "abstract": "Sequential recurrent neural networks have achieved superior performance on language modeling, but overlook the structure information in natural language. Recent works on structure-aware models have shown promising results on language modeling. However, how to incorporate structure knowledge on corpus without syntactic annotations remains an open problem. In this work, we propose neural variational language model (NVLM), which enables the sharing of grammar knowledge among different corpora. Experimental results demonstrate the effectiveness of our framework on two popular benchmark datasets. With the help of shared grammar, our language model converges significantly faster to a lower perplexity on new training corpus.",
  "full_text": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4442–4453\nFlorence, Italy, July 28 - August 2, 2019.c⃝2019 Association for Computational Linguistics\n4442\nLanguage Modeling with Shared Grammar\nYuyu Zhang\nCollege of Computing\nGeorgia Institute of Technology\nyuyu@gatech.edu\nLe Song\nCollege of Computing\nGeorgia Institute of Technology\nlsong@cc.gatech.edu\nAbstract\nSequential recurrent neural networks have\nachieved superior performance on language\nmodeling, but overlook the structure informa-\ntion in natural language. Recent works on\nstructure-aware models have shown promis-\ning results on language modeling. However,\nhow to incorporate structure knowledge on\ncorpus without syntactic annotations remains\nan open problem. In this work, we propose\nneural variational language model (NVLM),\nwhich enables the sharing of grammar knowl-\nedge among different corpora. Experimen-\ntal results demonstrate the effectiveness of\nour framework on two popular benchmark\ndatasets. With the help of shared grammar, our\nlanguage model converges signiﬁcantly faster\nto a lower perplexity on new training corpus.\n1 Introduction\nLanguage modeling has been a long-standing fun-\ndamental task in natural language processing. In\nrecent years, sequential recurrent neural networks\n(RNNs) based language models have made aston-\nishing progress, which achieve remarkable results\non various benchmark datasets (Mikolov et al.,\n2010a; Jozefowicz et al., 2016; Melis et al., 2017;\nElbayad et al., 2018; Gong et al., 2018; Takase\net al., 2018; Dai et al., 2019). Despite the huge\nsuccess, the structure information in natural lan-\nguage is largely overlooked due to the structural\nlimit of sequential RNN-based language models.\nRecently, researchers have explored to ex-\nplicitly exploit the latent structures in natu-\nral language, such as recurrent neural network\ngrammars (RNNGs; Dyer et al., 2016; Kuncoro\net al., 2017) and parsing-reading-predict networks\n(PRPNs; Shen et al., 2017). These structure-\naware models have shown promising results on\nlanguage modeling, demonstrating that the latent\nnested structure in language indeed helps improve\nsequential language models. Models like RNNG\nexploit treebank data with syntactic annotations\nto learn grammar, which is then used to improve\nlanguage model performance by a signiﬁcant mar-\ngin. This is deﬁnitely intriguing, but we have to\npay the cost: accurate syntactic annotation is very\ncostly, and treebank data such as the Penn Tree-\nbank (Marcus et al., 1993) is typically small-scale\nand not open to the public for free.\nOn new corpus which has no syntactic anno-\ntations, how to improve language modeling with\ngrammar knowledge? This is an important and\nchallenging open problem. As a motivating ex-\nample, we conduct a simple experiment by train-\ning a RNN language model on one corpus and\ntesting it on another, and report the results in Ta-\nble 1. The RNN language model performs terri-\nbly when training and testing on different datasets,\nwhich is reasonable since the data distribution may\nvary dramatically on different corpora. Training\nfrom scratch on every new corpus is obviously not\ngood enough: 1) it is computationally expensive\nand not data-efﬁcient; 2) the size of target cor-\npus may be too small to train a decent RNN-based\nlanguage model; 3) the common grammar is not\nleveraged. Some recent works on transfer learn-\ning have made attempts on language model adap-\ntation (Yoon et al., 2017; Ma et al., 2017; Chen\net al., 2015), however, none of them explicitly ex-\nploits the common grammar knowledge shared be-\ntween corpora.\nTo bridge the gap of language modeling on dif-\nferent corpora, we believe that grammar is the\nkey since all corpora are in the same language\nand should share the same grammar. Motivated\nby that, we propose neural variational language\nmodel (NVLM). Speciﬁcally, our framework con-\nsists of two probabilistic components: a con-\nstituency parser and a joint generative model of\nsentence and parse tree. When treebank data is\n4443\nTrain on PTB Train on OBWB\nTest on PTB 112.3 419.9\nTest on OBWB 242.2 139.3\nTable 1: Test perplexity of RNN language model,\nwhich performs terribly when training and testing on\ndifferent datasets.\navailable, we can separately train both compo-\nnents. On new corpus without tree annotations,\nwe ﬁx the pre-trained parser and train the gen-\nerative model either from scratch or with warm-\nup. The pre-trained parser is armed with grammar\nknowledge, thus it boosts up our language model\nto land on new corpus. Our proposed frame-\nwork also supports end-to-end joint training of\nthe two components, so that we can ﬁne-tune the\nlanguage model. Experimental results show that\nour proposed framework is effective in all lean-\ning schemes, which achieves good performance on\ntwo popular benchmark datasets. With the help of\nshared grammar, our language model converges\nsigniﬁcantly faster to a lower perplexity on new\ncorpus.\nOur contributions in this paper are summarized\nas follows:\n• Grammar-sharing framework: We propose\na framework for grammar-sharing language\nmodeling, which incorporates the common\ngrammar knowledge into language model-\ning. With the shared grammar, our frame-\nwork helps language model efﬁciently trans-\nfer to new corpus with better performance\nand using shorter time.\n• End-to-end learning: Our framework can be\nend-to-end trained without syntactic annota-\ntions. To tackle the technical challenges in\nend-to-end learning, we use variational meth-\nods that exploit policy gradient algorithm for\njoint training.\n• Efﬁcient software package: We provide a\nhighly efﬁcient implementation of our work\non GPUs. Our parser is capable of parsing\none million sentences per hour on a single\nGPU. See AppendixD for details.\n2 Model\nIn this section, we ﬁrst provide an overview of the\nproposed framework, then brieﬂy introduce how\ncomponents work together, and ﬁnally present the\nprobabilistic formulation of each component.\n2.1 Framework\nAs shown in Figure1, neural variational language\nmodel (NVLM) consists of two probabilistic com-\nponents: 1) a constituency parserP✓1 (y|x) which\nmodels the conditional probability of the parse tree\ny (a syntax tree without terminal tokens) given the\ninput sentencex (a sequence of terminal tokens);\n2) a joint generative modelP✓2 (x, y) which mod-\nels the joint probability of the sentence and the\nparse tree.\nConstituency Parsing.Our parser can work inde-\npendently, which takes as input a sentencex and\nparses x according to\nargmax\ny02Y(x)\nP✓1 (y0|x), (1)\nwhere Y(x) denotes the collection of all possible\nparses ofx. Our parser can also cooperate with the\njoint generative model as\nargmax\ny0⇠P✓1 (y|x)\nP✓2 (x, y0), (2)\nwhere the parsing candidates y0 sampled from\nP✓1 (y|x) are fed into the generative model\nP✓2 (x, y) to be reranked.\nLanguage Modeling.Statistical language models\nare typically formulated as\nP (x)= P (x1,x 2,...,x Lx )\n=\nLxY\nt=1\nP (xt|x<t), (3)\nwhere xt denotes the t-th token in the sentence\nx, the length ofx is denoted asLx, and x<t in-\ndicates all tokens beforext. To evaluate NVLM\nas a language model, we need to marginalize the\njoint probability as P (x)= P\ny02Y(x) P (x, y0).\nThis is extremely hard to compute due to the ex-\nponentially large space ofY(x). We use impor-\ntance sampling technique to overcome this com-\nputational intractability, which is detailed in Sec-\ntion 4.\nWith treebank data such as Penn Tree-\nbank (Marcus et al., 1993), we have pairs of(x, y)\nto train the two components respectively, and get\nhigh-quality language model with the parser pro-\nviding grammar knowledge. However, due to\nthe expensive cost of accurate parsing annotation,\n4444\nsentence !\nparse tree \"\n#$%(!,\")#$)(\"|!)\nmixed tree +\nJohnhasa dog.\n.\nS\nNP VP\nNNPVBZNP\nDTNN\nEncoderDecoder\nJointGenerative Model\nmerge\nParser\n.\nS\nNP VP\nNNPVBZNP\nDTNNhasJohna dog\n.\nFigure 1: Overall framework of neural variational language model (NVLM). It consists of two probabilistic compo-\nnents: a constituency parserP✓1 (y|x) and a joint generative modelP✓2 (x, y). The parser takes as input a sentence\nx and predicts the corresponding parse treey. Speciﬁcally, we use an encoder-decoder structure to parameterize\nthe parser. The joint generative model deﬁnes a joint distribution on parse trees (y) and sentences (x). When tree-\nbank data is available, we can learn the parameters✓1 and ✓2 for each component respectively. To train language\nmodel on new corpus, we ﬁx the pre-trained✓1 and only update✓2. Our framework can also be end-to-end jointly\ntrained to ﬁne-tune the language model, where✓1 and ✓2 are co-updated together.\ntreebank data is typically scarce. For new corpus\nwithout parsing annotations, our proposed frame-\nwork can still leverage the parser to train high-\nquality language model adapted to the new corpus.\nAlso, we can co-train the two components together\nto ﬁne-tune the language model on new corpus.\nIn the rest of this section, we present our param-\neterization of the two probabilistic components\nP✓1 (y|x) and P✓2 (x, y). To avoid notational clut-\nter, we use standard RNN as the basic building\nblock in the rest of this section.1\n2.2 Constituency Parser\nTo parameterize the constituency parserP✓1 (y|x),\nit is natural to ﬁrst encode the input sentencex\ninto an embedding vector, then pass the vector\nto a decoder to generate the parse treey. There\nare quite a few choices for both encoder and\ndecoder, among which recurrent neural network\n(RNN) and convolutional neural network (CNN)\nare the most popular ones, since they are power-\nful to capture the structural patterns in natural lan-\nguage (Sutskever et al., 2014; Zhang et al., 2015).\nVinyals et al.(2015) have found that the RNN-\npowered sequence-to-sequence (Seq2Seq) archi-\ntecture with attention mechanism achieves state-\nof-the-art parsing performance. This architecture\n1The proposed neural variational language model is in-\ndependent of any speciﬁc implementation of the recurrent\nunit such as LSTM (Hochreiter and Schmidhuber, 1997),\nGRU (Cho et al., 2014) and SRU (Lei and Zhang, 2017), and\ncan be directly applied to their deep and bi-directional vari-\nants.\nis conceptually simple yet powerful and of large\nmodel capacity.\nIn this paper, we adapt the sequence-to-\nsequence architecture for NVLM. We linearize\na parse tree as a bracket representation (a se-\nquence of nodes and brackets ordered by a pre-\norder traversal of the tree), which is a one-to-\none mapping of the tree structure. For exam-\nple, the parse tree shown in Figure 1 can be\nlinearized as (S (NP NNP ) (VP VBZ (NP\nDT NN ) ) . ). Interestingly, the parser is\nnow similar to a neural translation model (Bah-\ndanau et al., 2014), which translates a sentence\ninto a linearized parse tree. Next we show how\nthe parser computesP✓1 (y|x) in detail.\nFormally, the input sentencex is fed into the\nencoder, and is encoded as a sequence of hid-\nden states H = {h1,h 2,...,h Lx }, where Lx\nis the length ofx, and hi = RNNenc\n\u0000\nxi,h i\u00001\n\u0000\nwhere xi is ﬁrst embedded into a vector (word\nembedding) and then fed into the recurrent unit,\nand h0 is a learnable vector for the special start-\nof-sentence token <SOS>. The decoder uses\na separate RNN to calculate the hidden states\nsj = RNNdec\n\u0000\n[yj\u00001; cj],s j\u00001\n\u0000\n, wherey0 is set as\n<SOS>, s0 is set ashLx (the last hidden state of\nthe encoder), andyj\u00001 is the decoder’s previous\noutput token sampled from the categorical distri-\nbution of the decoder’s softmax layer (or speci-\nﬁed in teacher-forcing training mode), embedded\nand then concatenated with the context vectorcj\n4445\nto serve as the RNN input. The context cj is\ncalculated as cj = PLx\ni=1 ↵i,jhi, where ↵i,j =\nexp\n⇣\nh>\ni sj\u00001\n⌘\nPLx\ni0=1 exp\n⇣\nh>\ni0 sj\u00001\n⌘. This is a simpliﬁed version of\nthe conventional attention mechanism (Bahdanau\net al., 2014) used in the Seq2Seq parser (Vinyals\net al., 2015). Our parser is designed to be lighter\nand faster so that it can efﬁciently work together\nwith the NVLM joint generative model.\nFinally, the likelihoodP✓1 (y|x) is computed as\nP✓1 (y|x)=\nLyY\nt=1\nP (yt|x, y<t)\n=\nLyY\nt=1\n\"\nsoftmax\n⇣\nf\n\u0000\n[st; ct]\n\u0000⌘#\nyt\n, (4)\nwhere f(· ) refers to a fully-connected layer with\ntanh activation, and the subscriptyt is to select\nits probability in the categorical distribution.Ly\ndenotes the length ofy, which is determined by the\ndecoder itself. Once the decoder emits the special\nend-of-sentence token<EOS>, the decoding phase\nis terminated.\nThe trainable parameters✓1 in the parser com-\nponent P✓1 (y|x) include the weights (and biases)\nin RNNenc and RNNdec, and all word embeddings.\nWe use separate weights and word embeddings for\nthe encoder and the decoder.\n2.3 Joint Generative Model\nSimilar to the parser, the joint generative model\nP✓2 (x, y) can also be parameterized in various\nways. For example, Choe and Charniak (2016)\nuses a LSTM language model trained on the parser\noutput (with terminal words), which is then used\nto rerank an existing parser and achieves state-of-\nthe-art parsing performance. Inspired by that, we\nparameterize the joint generative model as\nP✓2 (x, y)= P (z)=\nLzY\nt=1\nP (zt|z<t), (5)\nwhere z is the mixed parse tree ofx and y, which is\nthen mapped to a sequential representation follow-\ning a pre-order traversal. Figure1 illustrates how a\nsentence x and its parse treey can be merged into\na mixed tree. We use another RNNgen to compute\nthe likelihoodP (zt|z<t), and ﬁnally getP✓2 (x, y)\nusing Eq. (5).\nAlgorithm 1:Sentence word attaching\ninput : sentence x; parser output tokensy\noutput: mixed tree tokensz\n1 if y is not balancedthen\n2 y  BalanceTree(y)\n3 z  Ø\n4 j  1\n5 Lx  Length ofx\n6 Ly  Length ofy\n7 for i  1,...,L y do\n8 if yi 2 LeafNode(y) ^ j< = Lx then\n9 z  z [{ “(” + yi}\n10 z  z [{ xj}\n11 z  z [{ “)” + yi}\n12 j  j +1\n13 else\n14 z  z [{ yi}\nThe trainable parameters✓2 in the joint genera-\ntive modelP✓2 (x, y) include the weights (and bi-\nases) in RNNgen and all word embeddings.\nIn Choe and Charniak (2016), the parser is\nﬁxed and well-trained before training the gen-\nerative model. Unlike that, our parser can be\njointly trained with the generative model, where\nthe parser may not be fully trained yet. Therefore,\nthe generated parse tree can be malformed, which\nmismatches the sentence with incorrect number of\nleaves. An even worse case is when the parser’s\noutput is not balanced and not able to form a le-\ngitimate tree. To handle these cases, we propose a\nsentence word attaching algorithm, which guaran-\ntees to generate a well-formed mixed tree. We de-\nscribe our algorithm for mixed tree generation in\nAlgorithm 1. This algorithm takes as input a sen-\ntence and its parse tree, and generates a mixed tree\nby attaching the sentence words to the leaf nodes\nof the parse tree. To handle unbalanced parse tree,\nwhich is rare case but happens due to the nature of\nsequential parser, we simply add brackets to either\nthe head or tail to make the parse tree balanced.\n3 Learning\nIn this section, we describe our algorithms for\nlearning the model parameters in the constituency\nparser P✓1 (y|x) and the joint generative model\nP✓2 (x, y).\n4446\n3.1 Learning Schemes\nNVLM can be trained in three different schemes:\n1) fully supervised learning, where sentences (x)\nand their corresponding parse trees (y) are avail-\nable; 2) distant-supervised learning, where we\nhave a pre-trained parser and a new corpus without\nparsing annotations; 3)semi-supervised learning,\nwhere we have no parsing annotations available.\nLet DXY = {(x(i),y (i))}n\ni=1 denote the anno-\ntated training data, where each sentence is paired\nwith a parse tree. LetDX = {x(i)}m\ni=1 denote the\nunannotated training data, where only sentences\nare available. Next, we show how to train NVLM\nunder each setting respectively.\nSupervised: In the fully supervised setting, we\nuse DXY to separately train the parser and the gen-\nerative model, by maximizing their respective data\nlog likelihood\nJ✓1 (DXY )= 1\nn\nnX\ni=1\nlog P✓1 (y(i)|x(i)), (6)\nJ✓2 (DXY )= 1\nn\nnX\ni=1\nlog P✓2 (x(i),y (i)), (7)\nwhere P✓1 (· ) and P✓2 (· ) are deﬁned in Eq. (4) and\nEq. (5). We obtain the gradientsr✓1 J1 and r✓2 J2\nby chain rule, and iteratively update✓1 and ✓2 with\nstandard optimizers.\nDistant-supervised: In distant-supervised learn-\ning, we have pre-trained the parser on corpus\nDXY , and ﬁx the parser to train the joint gener-\native model on new corpusDX. The generative\nmodel can be either trained from scratch onDX\nor warmed-up onDXY . This setting is of practical\nimportance, since we often need a language model\non new corpus without annotations, and the parser\npre-trained on treebank data can help since it en-\ncodes common grammar knowledge of the lan-\nguage.\nUnder this setting, the pre-trained parser gen-\nerates parse trees using Eq. (4) for unannotated\nsentences, and form(x, y) pairs to train the joint\ngenerative model throughr✓2 J2. The parser’s pa-\nrameters ✓1 remain ﬁxed.\nSemi-supervised: NVLM can be end-to-end\ntrained with only unannotated dataDX. This is\nextremely hard if we train everything from scratch.\nHowever, it is very useful to ﬁne-tune the language\nmodel on new corpus. Unlike distant-supervised\nlearning, we now train the parser and the joint gen-\nerative model together, and co-update the param-\nAlgorithm 2:Semi-supervised learning\ninput : annotated training dataDXY ;\nunannotated training dataDX;\noptimizer G(· )\n1 Initialize ✓1 with DXY using Eq. (6)\n2 Initialize ✓2 with DXY using Eq. (7)\n3 while model not convergeddo\n4 Sample a sentencex(i) from DX\n5 Sample a parse treey(i) from P✓1 (y|x(i))\n6 Standardize the signalA(x(i),y (i))\n7 Update the baseline function withb(x(i))\n8 ✓1  ✓1 + G\n\u0000\nr✓1\n˜J (DX)\n\u0000\nusing Eq. (12)\n9 ✓2  ✓2 + G\n\u0000\nr✓2\n˜J (DX)\n\u0000\nusing Eq. (10)\neters ✓1 and ✓2. Here we also maximize the data\nlog likelihood\nJ (DX)= 1\nm\nmX\ni=1\nlog\n X\ny2Y(x(i))\nP✓2 (x(i),y )\n!\n.\n(8)\nUnfortunately, the derivative ofJ (DX) is com-\nputationally intractable due to the large space of\nY. To tackle this challenge, we use variational\nmethods to maximize the lower bound ofJ (DX),\nand exploit policy gradient algorithm to update the\nparser’s parameters. Details are described in Sec-\ntion 3.2 and Section3.3. Our algorithm for semi-\nsupervised learning is summarized in Algorithm2,\nwhere we assume mini-batch size as 1 to avoid no-\ntational clutter.\n3.2 Variational EM\nAs described above, to overcome the compu-\ntational intractability of maximizing J (DX)\ndirectly, we use variational expectation-\nmaximization (EM) algorithm to maximize\nthe evidence lower bound (ELBO):\n˜J (DX)= 1\nm\nmX\ni=1\nEP✓1 (y|x(i))\nh\nlog P✓2 (x(i),y ) \u0000 log P✓1 (y|x(i))\ni\n,\n(9)\nwhere we use our parser as the variational poste-\nrior P✓1 (y|x). For readability, from now on we\nassume m =1 and omit summing over training\nsamples. With the Monte Carlo method, we ob-\n4447\ntain the unbiased gradient\nr✓2\n˜J (DX)= EP✓1 (y|x)\nh\nr✓2 P✓2 (x, y)\ni\n. (10)\n3.3 Policy Gradient\nTo get the gradient of ˜J (DX) w.r.t. the parser\nparameters ✓1, we need more work since y is\nsampled from a series of categorical distributions.\nHere we use policy gradient algorithm (Williams,\n1992) to get an unbiased estimator of the gradient\nr✓1\n˜J (DX)= EP✓1 (y|x)\nh\nr✓1 P✓1 (y|x)A(x, y)\ni\n,\n(11)\nwhere A(x, y) = logP✓2 (x, y) \u0000 log P✓1 (y|x) is\nused as the learning signal. Due to the limit of\nspace, we provide detailed derivation of Eq. (11)\nin Appendix E. In order to stabilize the learning\nprocess, we use standard variance reduction tech-\nniques to reduce the variance of gradient (Green-\nsmith et al., 2004; Mnih and Gregor, 2014; Zhang\net al., 2017). Speciﬁcally, we ﬁrst standardize the\nsignal (rescaling it to zero mean and unit variance)\nand then subtract a baseline functionb(x). Then\nwe use a separate GRU as the baseline function\nand ﬁt the centered signal by minimizing the mean\nsquare loss. Finally, the gradient can be approxi-\nmated as\nr✓1\n˜J (DX) ⇡EP✓1 (y|x)\n\"\nr✓1 P✓1 (y|x)\n✓A(x, y) \u0000 ˜µ\n˜\u0000 \u0000 b(x)\n◆#\n, (12)\nwhere ˜µ is the sample mean and˜\u0000 is the sample\nstandard deviation, which estimate the mean and\nstandard deviation of the learning signalA(x, y).\n4 Inference\nWith the two components in NVLM, a parser\nP✓1 (y|x) and a joint generative modelP✓2 (x, y),\nwe can do three types of inference:\n• Parsing, where we sample the parser with\ngreedy decoding to generate the parse tree for\ninput sentence;\n• Evaluating P✓2 (x, y), which is obtained from\nthe joint generative model, and can be used to\nhelp rerank parsing candidates;\n• Estimating P (x)= P\ny02Y(x) P (x, y0) and\nevaluating the model perplexity, which is in-\ntractable due to the exponentially large space\nof Y(x). Similar to Dyer et al.(2016), we\nuse importance sampling technique to esti-\nmate P (x).\nSpeciﬁcally, we use our parserP✓1 (y|x) as the\nproposal distribution. The estimator ofP (x) is de-\nrived as\nP (x)=\nX\ny02Y(x)\nP✓1 (y0|x)P✓2 (x, y0)\nP✓1 (y0|x)\n= EP✓1 (y0|x)\nP✓2 (x, y0)\nP✓1 (y0|x) . (13)\n5 Experiments\n5.1 Settings\nDatasets. We conduct experiments on two pop-\nular datasets for language modeling: Penn Tree-\nbank (PTB;Marcus et al., 1993) and One Billion\nWord Benchmark (OBWB;Chelba et al., 2013).2\nThe PTB dataset has parsing annotations, while\nOBWB dataset has no annotations. For the PTB\ndataset, we adopt the standard train / validation /\ntest split. We build the vocabulary based on PTB,\nusing one unknown token for singleton words. For\nthe OBWB dataset, we use the original train/test\nsplit, and subsample each to have similar size of\nPTB (50K sentences for training, and 2.5K sen-\ntences for test). The subsampling is for the efﬁ-\nciency of evaluating perplexity using importance\nsampling, which requires to sample multiple (we\nuse 100) parse trees for each sentence. The train-\ning of our framework is actually much more scal-\nable than the perplexity evaluation, and not re-\nstricted to the size of downsampled dataset. Note\nthat our data preprocessing scheme follows what is\nstandard in parsing instead of language modeling,\nsince parsing typically requires more information\n(such as capital letters) and larger vocabulary. The\nvocabulary size for text is 26,620, and we have\na separate vocabulary of size 74 for nonterminal\nnodes of parsing, such as(NP and NNP. Refer to\nAppendix A for more data preprocessing details.\nTasks. We work on three different tasks: 1)super-\nvised learning: we separately train both the parser\n2The treebank data is publicly available through\nthe Linguistic Data Consortium (LDC): Penn Treebank\n(LDC99T42). The original OBWB dataset is downloaded\nfrom http://www.statmt.org/lm-benchmark/.\n4448\nand the joint generative model on PTB; 2)distant-\nsupervised learning: we pre-train the parser on\nPTB, and then ﬁx the parser to train the joint gen-\nerative model on OBWB either from scratch or\nwith PTB warmed-up model; 3)semi-supervised\nlearning: we jointly train the parser and the gen-\nerative model together on OBWB to ﬁne-tune the\nlanguage model.\nEvaluation. We mainly focus on language model-\ning, and use per-word perplexity to evaluate our\nframework and competitor models. As for the\nparser, we compare with state-of-the-art parsers in\nterms of training and testing speed.\nBaselines. On the PTB dataset, we compare our\nlanguage model with the following baselines: 1)\nKneser-Ney 5-gram language model; 2) LSTM\nlanguage model; 3) GRU language model im-\nplemented by ourselves; 4) recent state-of-the-art\nlanguage models that also incorporate grammar\nto improve language modeling, including RNNG,\nSO-RNNG and GA-RNNG (Dyer et al., 2016;\nKuncoro et al., 2017). On OBWB dataset, since\nthere is no parsing annotations available, we com-\npare with GRU language model as a strong base-\nline.\nOptimization. All our models are trained on a\nsingle NVIDIA GTX 1080 GPU. For all NVLM\nmodels, we use Adam optimizer (Kingma and Ba,\n2014) for the parser, and standard SGD for the\njoint generative model. Gradients are clipped at\n0.25. See AppendixB for more details.\n5.2 Supervised Learning\nWe ﬁrst experiment on PTB dataset in the super-\nvised learning setting. We separately train our\nparser and joint generative model on PTB train-\ning dataset, and then evaluate our language model\non PTB test dataset. Table 2 lists the perfor-\nmance of our framework and competitor models.\nGRU-256 LM is our implemented language model\nusing 2-layer GRU with hidden size 256, which\nis also used in other experiments. Parsing an-\nnotations are used by RNNG, SO-RNNG, GA-\nRNNG and NVLM. These grammar-aware models\nachieve signiﬁcantly better performance compared\nto state-of-the-art sequential RNN-based language\nmodels, showing that grammar indeed helps lan-\nguage modeling. NVLM substantially improves\nover the current state of the art, by 10% reduction\non test perplexity.\nWith respect to our parser, instead of pursu-\nModel Perplexity\nKN-5-gram (Kneser and Ney, 1995) 169.3\nLSTM-128 LM (Zaremba et al., 2014) 113.4\nGRU-256 LM 112.3\nRNNG (Dyer et al., 2016) 102.4\nSO-RNNG (Kuncoro et al., 2017) 101.2\nGA-RNNG (Kuncoro et al., 2017) 100.9\nNVLM 91.6\nTable 2: Test perplexity on PTB § 23. KN-5-gram\nrefers to Kneser-Ney 5-gram LM. Note that, since pars-\ning typically requires more information (e.g., capi-\ntal letters), we follow the standard data preprocessing\nof syntax-aware language modeling as inDyer et al.\n(2016), thus the vocabulary size (⇠27K) is much larger\nthan the capped vocabulary size (10K) in standard lan-\nguage modeling setting. Therefore, all the perplexity\nresults reported in this paper are not directly compara-\nble to that achieved by syntax-agnostic language mod-\nels with a much smaller vocabulary, such as the per-\nplexity 57.3 reported inMerity et al.(2017) and 54.5\nreported in Dai et al.(2019). This also applies to the\nperplexity results on the OBWB dataset.\ning state-of-the-art parsing performance, it is de-\nsigned to be light and fast to efﬁciently work to-\ngether with the NVLM joint generative model.\nOur parser achieves 90.7 F1 accuracy on PTB test\ndataset, which is comparable to state-of-the-art\nparsers. Due to the page limit, we report the de-\ntailed parsing performance in AppendixC.\n5.3 Distant-supervised Learning\nWe then experiment on learning language model\non new corpus without tree annotations. This is\nto verify whether the learned parser can help lan-\nguage model softly land on new corpus. We use\nthe subsampled OBWB dataset for model train-\ning and evaluation. GRU-256 LM is used as a\nstrong baseline. We have two different settings\nfor both GRU LM and our framework: 1)from-\nscratch: For GRU LM, we randomly initialize it\nbefore training on OBWB. For NVLM, we train\nthe parser on PTB and ﬁx it, and randomly initial-\nize the joint generative model; 2)warmed-up: For\nGRU LM, we pre-train GRU LM on PTB before\ntraining it on the OBWB dataset. For NVLM, we\ntrain the parser on PTB and ﬁx it, and pre-train\nthe joint generative model on PTB as warm-up. In\nboth cases, NVLM uses its parser (trained on PTB)\nto generate parse trees for the OBWB dataset, and\ntrain the joint generative model with these silver-\n4449\nstandard parse trees.\nFigure 2(a) shows the test perplexity curves\nalong with number of training epochs. In both\nfrom-scratch and warmed-up settings, NVLM per-\nforms signiﬁcantly better than GRU LM by 22.7\nand 7.8 points in perplexity reduction. The\nwarmed-up NVLM converges fast and achieves\nthe lowest perplexity. Even when trained from\nscratch, NVLM achieves better performance than\nthe warmed-up GRU LM, though it takes longer\nto converge. Note that the warm-up of GRU LM\nis directly training P (x) with more data, while\nthe warm-up of NVLM is only forP (x, y). This\nexplains why GRU LM seems to beneﬁt more\nfrom warm-up at beginning, and why NVLM from\nscratch takes longer to converge.\nUnlike the supervised learning setting, NVLM\ncan now be trained on new corpus without pars-\ning annotations, and still leverages the common\ngrammar knowledge. To further study the adap-\ntation speed of NVLM on new corpus, we train\nNVLM with variant proportion of training data in\nboth from-scratch and warmed-up settings. We\nalso train GRU LM as a strong baseline. Results\nare reported in Table3.\nAs shown in Table3, with smaller amount of\ndata, NVLM outperforms GRU LM even more\nsigniﬁcantly. We ﬁnd that with only 20% of train-\ning data, the warmed-up NVLM achieves test per-\nplexity 140.6, which is comparable to GRU LM\ntrained with full data from scratch (139.3). This\ndemonstrates that our framework is data-efﬁcient,\nand can quickly adapt to new corpus without pars-\ning annotations. Moreover, we notice that even\nwithout looking at the new corpus (0% train-\ning data), the warmed-up NVLM achieves rea-\nsonable perplexity (151.8) which is substantially\nlower than the warmed-up GRU LM (242.2). This\nagrees with our conjecture that grammar knowl-\nedge is sharable among different corpora. Fig-\nure 2(b) plots the test perplexity curves using 20%\nOBWB training data. The warmed-up NVLM\nquickly converges, and achieves much lower per-\nplexity compared to the warmed-up GRU LM.\n5.4 Semi-supervised Learning\nIn distant-supervised setting, the parser is ﬁxed\nwhen training NVLM on new corpus. We can ac-\ntually continue training the parser together with\nthe generative model, so that the language model\ncan be ﬁne-tuned in end-to-end fashion. This is es-\n(a) 100% training data\n (b) 20% training data\nFigure 2: Test perplexity curves on the subsampled\nOBWB dataset. Models are trained respectively on\n100% and 20% training data. Models randomly initial-\nized are marked as “scratch”, while models pre-trained\non the PTB dataset are marked as “warmed”.\nTraining\nData\nGRU LM NVLM\nscratch warmed scratch warmed\n0% >1000 242.2 >1000 151.8\n20% 298.8 173.1 168.2 140.6\n40% 210.3 147.3 143.3 138.5\n60% 177.6 136.5 135.8 133.5\n80% 152.5 130.5 128.6 125.6\n100% 139.3 121.7 116.6 113.9\nTable 3: Test perplexity on the subsampled OBWB\ndataset. Models are trained on variant proportion of\ntraining data. Models randomly initialized are marked\nas “scratch”, while models pre-trained on the PTB\ndataset are marked as “warmed”.\nsentially semi-supervised learning: the parser has\nto be updated without parsing annotations, and the\ngenerative model will be updated together. Due\nto the exponentially large space of parse trees,\nsuch joint training is computationally intractable.\nTo tackle the challenge, we use variational EM\n(Section 3.2) and exploit policy gradient (Sec-\ntion 3.3) algorithm to co-update both components\nof NVLM.\nTechnically, when the parser is ﬁxed, the model\nis also maximizing the lower bound of data log\nlikelihood. This empirically works well, as shown\nin the distant-supervised setting. With joint train-\ning, the model is essentially trying to ﬁnd a bet-\nter posterior on the new corpus and maximize a\ntighter lower bound. Therefore, the data log like-\nlihood in Eq. (8) can be better optimized. In semi-\nsupervised setting, we use full OBWB training\ndata (subsampled). As reported in Table4, NVLM\nachieves the lowest perplexity (110.2) with joint\ntraining. We also evaluate the parser on PTB after\njoint training. It couldn’t get improved since it has\n4450\nModel Perplexity\nGRU LM – from scratch 139.3\nGRU LM – warmed-up on PTB 121.7\nNVLM – from scratch 116.6\nNVLM – warmed-up on PTB 113.9\nNVLM – ﬁne-tuned by joint training 110.2\nTable 4: Test perplexity on the subsampled OBWB\ndataset. All models are trained with 100% training\ndata.\nbeen ﬁne-tuned on new corpus, and we have no\nannotations to evaluate the parser on new corpus.\n6 Related Work and Discussion\nDue to the remarkable success of RNN-based lan-\nguage models (Mikolov et al., 2010b,a; Jozefow-\nicz et al., 2016; Yang et al., 2017; Merity et al.,\n2017; Elbayad et al., 2018; Gong et al., 2018;\nTakase et al., 2018; Dai et al., 2019), not too\nmuch attention has been paid to incorporate syn-\ntactic knowledge into language model. Although\nRNN-based models achieve impressive results on\nlanguage modeling and other NLP tasks such as\nmachine translation (Cho et al., 2014; Bahdanau\net al., 2014; Xia et al., 2016) and parsing (Vinyals\net al., 2015; Dyer et al., 2015; Choe and Char-\nniak, 2016), it is far from perfect since it overlooks\nthe language structure and simply generates sen-\ntence from left to right. The words in natural lan-\nguage are largely organized in latent nested struc-\ntures rather than simple sequential order (Chom-\nsky, 2002).\nOur work is related to syntactic language mod-\nels, which has a long history. Traditional syn-\ntactic language models jointly generate syntactic\nstructure with words using either bottom-up (Je-\nlinek and Lafferty, 1991; Emami and Jelinek,\n2004, 2005; Henderson, 2004), or top-down strat-\negy (Charniak, 2000; Roark, 2001). Recently,\nsome studies show the beneﬁts of incorporat-\ning language structure into RNN-based language\nmodel, such as RNNG (Dyer et al., 2016; Kuncoro\net al., 2017). Different from our work, these mod-\nels mainly focus on parsing instead of language\nmodeling, and cannot be trained without parsing\nannotations. Works on programming code gener-\nation (Rabinovich et al., 2017; Yin and Neubig,\n2017) demonstrate that grammar is the key of ef-\nfective code generation. Compared to natural lan-\nguage, programming code is more regulated and\ntypically has well-deﬁned grammar, thus it is more\nchallenging to exploit the grammar knowledge in\nnatural language.\nOur work is also related to transfer learning\nof deep learning models (Bengio, 2012). There\nare some recent studies on neural language model\nadaptation (Yoon et al., 2017; Ma et al., 2017;\nChen et al., 2015). However, none of them ex-\nploits grammar knowledge. There exists other\nlines of work on a broad ﬁeld of general text\ngeneration (not for language modeling), such as\nGAN-based methods (Guo et al., 2017; Li et al.,\n2017) and V AE-based ones (Hu et al., 2017). It\nis a promising direction to incorporate syntac-\ntic knowledge into these generative models. Our\nwork is also inspired by works on syntactical\nstructured RNNs, such as tree LSTM (Tai et al.,\n2015), hierarchical RNNs (Chung et al., 2016)\nand doubly RNNs (Alvarez-Melis and Jaakkola,\n2016).\nLanguage models are widely used in a broad\nrange of applications. We believe that a high-\nquality language model can beneﬁt many down-\nstream tasks, such as machine translation, dia-\nlogue systems, and speech recognition. We con-\nsider to explore whether our framework can be\nseamlessly used in those applications, and leave\nit as future work.\n7 Conclusion\nIn this work, we aim to improve language model-\ning with shared grammar. Our framework contains\ntwo probabilistic components: a constituency\nparser and a joint generative model. The parser\nencodes grammar knowledge in natural language,\nwhich helps language model quickly land on new\ncorpus. We also propose algorithms for jointly\ntraining the two components to ﬁne-tune the lan-\nguage model on new corpus without parsing anno-\ntations. Experiments demonstrate that our method\nimproves language modeling on new corpus in\nterms of both convergence speed and perplexity.\nAcknowledgements\nThis project was supported in part by NSF IIS-\n1218749,NIH BIGDATA 1R01GM108341, NSF\nCAREER IIS-1350983, NSF IIS-1639792 EA-\nGER, NSF IIS-1841351 EA-GER, NSF CNS-\n1704701, ONR N00014-15-1-2340, IntelISTC,\nNVIDIA, Google and Amazon AWS.\n4451\nReferences\nDavid Alvarez-Melis and Tommi S Jaakkola. 2016.\nTree-structured decoding with doubly-recurrent\nneural networks.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473.\nYoshua Bengio. 2012. Deep learning of representa-\ntions for unsupervised and transfer learning. InPro-\nceedings of ICML Workshop on Unsupervised and\nTransfer Learning, pages 17–36.\nEugene Charniak. 2000. A maximum-entropy-inspired\nparser. In Proceedings of the 1st North American\nchapter of the Association for Computational Lin-\nguistics conference, pages 132–139. Association for\nComputational Linguistics.\nEugene Charniak and Mark Johnson. 2005. Coarse-\nto-ﬁne n-best parsing and maxent discriminative\nreranking. In Proceedings of the 43rd annual meet-\ning on association for computational linguistics ,\npages 173–180. Association for Computational Lin-\nguistics.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling.arXiv\npreprint arXiv:1312.3005.\nXie Chen, Tian Tan, Xunying Liu, Pierre Lanchantin,\nMoquan Wan, Mark JF Gales, and Philip C Wood-\nland. 2015. Recurrent neural network language\nmodel adaptation for multi-genre broadcast speech\nrecognition. In Sixteenth Annual Conference of the\nInternational Speech Communication Association.\nKyunghyun Cho, Bart Van Merri¨enboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using rnn encoder-decoder\nfor statistical machine translation. arXiv preprint\narXiv:1406.1078.\nDo Kook Choe and Eugene Charniak. 2016. Parsing\nas language modeling. InProceedings of the 2016\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 2331–2336.\nNoam Chomsky. 2002.Syntactic structures. Walter de\nGruyter.\nJunyoung Chung, Sungjin Ahn, and Yoshua Bengio.\n2016. Hierarchical multiscale recurrent neural net-\nworks. arXiv preprint arXiv:1609.01704.\nZihang Dai, Zhilin Yang, Yiming Yang, William W\nCohen, Jaime Carbonell, Quoc V Le, and Ruslan\nSalakhutdinov. 2019. Transformer-xl: Attentive lan-\nguage models beyond a ﬁxed-length context.arXiv\npreprint arXiv:1901.02860.\nChris Dyer, Miguel Ballesteros, Wang Ling, Austin\nMatthews, and Noah A Smith. 2015. Transition-\nbased dependency parsing with stack long short-\nterm memory.arXiv preprint arXiv:1505.08075.\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,\nand Noah A Smith. 2016. Recurrent neural network\ngrammars. In Proceedings of NAACL-HLT, pages\n199–209.\nMaha Elbayad, Laurent Besacier, and Jakob Verbeek.\n2018. Token-level and sequence-level loss smooth-\ning for RNN language models. InProceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n2094–2103, Melbourne, Australia. Association for\nComputational Linguistics.\nAhmad Emami and Frederick Jelinek. 2004. Exact\ntraining of a neural syntactic language model. In\nAcoustics, Speech, and Signal Processing, 2004.\nProceedings.(ICASSP’04). IEEE International Con-\nference on, volume 1, pages I–245. IEEE.\nAhmad Emami and Frederick Jelinek. 2005. A neural\nsyntactic language model.Machine learning, 60(1-\n3):195–227.\nChengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang,\nand Tie-Yan Liu. 2018. Frage: frequency-agnostic\nword representation. In Advances in Neural Infor-\nmation Processing Systems, pages 1334–1345.\nEvan Greensmith, Peter L Bartlett, and Jonathan Bax-\nter. 2004. Variance reduction techniques for gradi-\nent estimates in reinforcement learning.Journal of\nMachine Learning Research, 5(Nov):1471–1530.\nJiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong\nYu, and Jun Wang. 2017. Long text generation via\nadversarial training with leaked information.arXiv\npreprint arXiv:1709.08624.\nJames Henderson. 2004. Discriminative training of a\nneural network statistical parser. InProceedings of\nthe 42nd Annual Meeting on Association for Compu-\ntational Linguistics, page 95. Association for Com-\nputational Linguistics.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation,\n9(8):1735–1780.\nZhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan\nSalakhutdinov, and Eric P Xing. 2017. Toward con-\ntrolled generation of text. InInternational Confer-\nence on Machine Learning, pages 1587–1596.\nFrederick Jelinek and John D Lafferty. 1991. Compu-\ntation of the probability of initial substring genera-\ntion by stochastic context-free grammars.Computa-\ntional Linguistics, 17(3):315–323.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. 2016. Exploring\nthe limits of language modeling. arXiv preprint\narXiv:1602.02410.\n4452\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization.arXiv preprint\narXiv:1412.6980.\nNikita Kitaev and Dan Klein. 2018. Constituency\nparsing with a self-attentive encoder. InProceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 2676–2686, Melbourne, Australia. As-\nsociation for Computational Linguistics.\nDan Klein and Christopher D Manning. 2003. Accu-\nrate unlexicalized parsing. In Proceedings of the\n41st annual meeting of the association for compu-\ntational linguistics.\nReinhard Kneser and Hermann Ney. 1995. Improved\nbacking-off for m-gram language modeling. In\nAcoustics, Speech, and Signal Processing, 1995.\nICASSP-95., 1995 International Conference on, vol-\nume 1, pages 181–184. IEEE.\nLingpeng Kong and Noah A Smith. 2014. An empiri-\ncal comparison of parsing methods for stanford de-\npendencies. arXiv preprint arXiv:1404.4314.\nAdhiguna Kuncoro, Miguel Ballesteros, Lingpeng\nKong, Chris Dyer, Graham Neubig, and Noah A.\nSmith. 2017. What do recurrent neural network\ngrammars learn about syntax? In Proceedings of\nthe 15th Conference of the European Chapter of the\nAssociation for Computational Linguistics: Volume\n1, Long Papers, pages 1249–1258, Valencia, Spain.\nAssociation for Computational Linguistics.\nTao Lei and Yu Zhang. 2017. Training rnns as fast as\ncnns. arXiv preprint arXiv:1709.02755.\nJiwei Li, Will Monroe, Tianlin Shi, Alan Ritter,\nand Dan Jurafsky. 2017. Adversarial learning\nfor neural dialogue generation. arXiv preprint\narXiv:1701.06547.\nMin Ma, Michael Nirschl, Fadi Biadsy, and Shankar\nKumar. 2017. Approaches for neural-network lan-\nguage model adaptation. Proc. Interspeech 2017,\npages 259–263.\nMitchell P Marcus, Mary Ann Marcinkiewicz, and\nBeatrice Santorini. 1993. Building a large annotated\ncorpus of english: The penn treebank. Computa-\ntional linguistics, 19(2):313–330.\nG´abor Melis, Chris Dyer, and Phil Blunsom. 2017. On\nthe state of the art of evaluation in neural language\nmodels. arXiv preprint arXiv:1707.05589.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2017. Regularizing and optimizing lstm lan-\nguage models.arXiv preprint arXiv:1708.02182.\nTom´aˇs Mikolov, Martin Karaﬁ´at, Luk´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2010a. Re-\ncurrent neural network based language model. In\nEleventh Annual Conference of the International\nSpeech Communication Association.\nTomas Mikolov, Martin Karaﬁ´at, Lukas Burget, Jan\nCernock`y, and Sanjeev Khudanpur. 2010b. Recur-\nrent neural network based language model. InInter-\nspeech, volume 2, page 3.\nAndriy Mnih and Karol Gregor. 2014. Neural vari-\national inference and learning in belief networks.\narXiv preprint arXiv:1402.0030.\nAdam Paszke, Sam Gross, Soumith Chintala, Gre-\ngory Chanan, Edward Yang, Zachary DeVito, Zem-\ning Lin, Alban Desmaison, Luca Antiga, and Adam\nLerer. 2017. Automatic differentiation in pytorch.\nSlav Petrov, Leon Barrett, Romain Thibaux, and Dan\nKlein. 2006. Learning accurate, compact, and inter-\npretable tree annotation. InProceedings of the 21st\nInternational Conference on Computational Lin-\nguistics and the 44th annual meeting of the Associa-\ntion for Computational Linguistics, pages 433–440.\nAssociation for Computational Linguistics.\nMaxim Rabinovich, Mitchell Stern, and Dan Klein.\n2017. Abstract syntax networks for code gen-\neration and semantic parsing. arXiv preprint\narXiv:1704.07535.\nBrian Roark. 2001. Probabilistic top-down parsing\nand language modeling.Computational linguistics,\n27(2):249–276.\nYikang Shen, Zhouhan Lin, Chin-Wei Huang, and\nAaron Courville. 2017. Neural language model-\ning by jointly learning syntax and lexicon.arXiv\npreprint arXiv:1711.02013.\nRichard Socher, John Bauer, Christopher D Manning,\net al. 2013. Parsing with compositional vector gram-\nmars. In Proceedings of the 51st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), volume 1, pages 455–465.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural net-\nworks. In Advances in neural information process-\ning systems, pages 3104–3112.\nJun Suzuki, Sho Takase, Hidetaka Kamigaito, Makoto\nMorishita, and Masaaki Nagata. 2018. An empir-\nical study of building a strong baseline for con-\nstituency parsing. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 612–\n618, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nKai Sheng Tai, Richard Socher, and Christopher D\nManning. 2015. Improved semantic representations\nfrom tree-structured long short-term memory net-\nworks. arXiv preprint arXiv:1503.00075.\nSho Takase, Jun Suzuki, and Masaaki Nagata. 2018.\nDirect output connection for a high-rank language\nmodel. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 4599–4609, Brussels, Belgium. Associ-\nation for Computational Linguistics.\n4453\nOriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov,\nIlya Sutskever, and Geoffrey Hinton. 2015. Gram-\nmar as a foreign language. InAdvances in Neural\nInformation Processing Systems, pages 2773–2781.\nRonald J Williams. 1992. Simple statistical gradient-\nfollowing algorithms for connectionist reinforce-\nment learning. In Reinforcement Learning, pages\n5–32. Springer.\nYingce Xia, Di He, Tao Qin, Liwei Wang, Nenghai\nYu, Tie-Yan Liu, and Wei-Ying Ma. 2016. Dual\nlearning for machine translation. arXiv preprint\narXiv:1611.00179.\nZhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and\nWilliam W Cohen. 2017. Breaking the softmax bot-\ntleneck: a high-rank rnn language model. arXiv\npreprint arXiv:1711.03953.\nPengcheng Yin and Graham Neubig. 2017. A syntactic\nneural model for general-purpose code generation.\narXiv preprint arXiv:1704.01696.\nSeunghyun Yoon, Hyeongu Yun, Yuna Kim, Gyu-tae\nPark, and Kyomin Jung. 2017. Efﬁcient transfer\nlearning schemes for personalized language model-\ning using recurrent neural network.arXiv preprint\narXiv:1701.03578.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.\n2014. Recurrent neural network regularization.\narXiv preprint arXiv:1409.2329.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsiﬁcation. In Advances in neural information pro-\ncessing systems, pages 649–657.\nYuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexan-\nder J Smola, and Le Song. 2017. Variational reason-\ning for question answering with knowledge graph.\narXiv preprint arXiv:1709.04071.\nMuhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,\nand Jingbo Zhu. 2013. Fast and accurate shift-\nreduce constituent parsing. In Proceedings of the\n51st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), vol-\nume 1, pages 434–443.",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9201663732528687
    },
    {
      "name": "Computer science",
      "score": 0.8850923776626587
    },
    {
      "name": "Natural language processing",
      "score": 0.7149021625518799
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6771184206008911
    },
    {
      "name": "Grammar",
      "score": 0.6686451435089111
    },
    {
      "name": "Language model",
      "score": 0.6450858116149902
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5371291041374207
    },
    {
      "name": "Natural language",
      "score": 0.4835224151611328
    },
    {
      "name": "Linguistics",
      "score": 0.1774328052997589
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I130701444",
      "name": "Georgia Institute of Technology",
      "country": "US"
    }
  ]
}