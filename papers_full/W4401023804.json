{
    "title": "T-T: Table Transformer for Tagging-based Aspect Sentiment Triplet Extraction",
    "url": "https://openalex.org/W4401023804",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5100365369",
            "name": "Jiaqi Wang",
            "affiliations": [
                "Pennsylvania State University"
            ]
        },
        {
            "id": "https://openalex.org/A5101912906",
            "name": "Junyu Luo",
            "affiliations": [
                "Pennsylvania State University"
            ]
        },
        {
            "id": "https://openalex.org/A5024079930",
            "name": "Muchao Ye",
            "affiliations": [
                "Pennsylvania State University"
            ]
        },
        {
            "id": "https://openalex.org/A5100401925",
            "name": "Xiaochen Wang",
            "affiliations": [
                "Pennsylvania State University"
            ]
        },
        {
            "id": "https://openalex.org/A5100310779",
            "name": "Yuan Zhong",
            "affiliations": [
                "Pennsylvania State University"
            ]
        },
        {
            "id": "https://openalex.org/A5102622763",
            "name": "Aofei Chang",
            "affiliations": [
                "Pennsylvania State University"
            ]
        },
        {
            "id": "https://openalex.org/A5053986669",
            "name": "Guanjie Huang",
            "affiliations": [
                "Pennsylvania State University"
            ]
        },
        {
            "id": "https://openalex.org/A5100710229",
            "name": "Ziyi Yin",
            "affiliations": [
                "Pennsylvania State University"
            ]
        },
        {
            "id": "https://openalex.org/A5100645991",
            "name": "Cao Xiao",
            "affiliations": [
                "General Electric (Spain)"
            ]
        },
        {
            "id": "https://openalex.org/A5084279065",
            "name": "Jimeng Sun",
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ]
        },
        {
            "id": "https://openalex.org/A5001030192",
            "name": "Fenglong Ma",
            "affiliations": [
                "Pennsylvania State University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2964006392"
    ],
    "abstract": "Aspect sentiment triplet extraction (ASTE) aims to extract triplets composed of aspect terms, opinion terms, and sentiment polarities from given sentences. The table tagging method is a popular approach to addressing this task, which encodes a sentence into a 2-dimensional table, allowing for the tagging of relations between any two words. Previous efforts have focused on designing various downstream relation learning modules to better capture interactions between tokens in the table, revealing that a stronger capability in relation capture can lead to greater improvements in the model. Motivated by this, we attempt to directly utilize transformer layers as downstream relation learning modules. Due to the powerful semantic modeling capability of transformers, it is foreseeable that this will lead to excellent improvement. However, owing to the quadratic relation between the length of the table and the length of the input sentence sequence, using transformers directly faces two challenges: overly long table sequences and unfair local attention interaction. To address these challenges, we propose a novel Table-Transformer (T-T) for the tagging-based ASTE method. Specifically, we introduce a stripe attention mechanism with a loop-shift strategy to tackle these challenges. The former modifies the global attention mechanism to only attend to a 2-dimensional local attention window, while the latter facilitates interaction between different attention windows. Extensive and comprehensive experiments demonstrate that the T-T, as a downstream relation learning module, achieves state-of-the-art performance with lower computational costs.",
    "full_text": null
}