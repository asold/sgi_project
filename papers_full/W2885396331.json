{
  "title": "Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference",
  "url": "https://openalex.org/W2885396331",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2557182036",
      "name": "Reza Ghaeini",
      "affiliations": [
        "Oregon State University"
      ]
    },
    {
      "id": "https://openalex.org/A3205585518",
      "name": "Xiaoli Fern",
      "affiliations": [
        "Oregon State University"
      ]
    },
    {
      "id": "https://openalex.org/A1993564419",
      "name": "Prasad Tadepalli",
      "affiliations": [
        "Oregon State University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4302343710",
    "https://openalex.org/W2782363479",
    "https://openalex.org/W2963719234",
    "https://openalex.org/W2963039614",
    "https://openalex.org/W2964064432",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2964045325",
    "https://openalex.org/W2593833795",
    "https://openalex.org/W2756386045",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2963756346",
    "https://openalex.org/W2110951295",
    "https://openalex.org/W2964159778",
    "https://openalex.org/W2963077723",
    "https://openalex.org/W2413794162",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2963344337",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2562979205",
    "https://openalex.org/W2608787653",
    "https://openalex.org/W2962851944",
    "https://openalex.org/W1601924930"
  ],
  "abstract": "Deep learning models have achieved remarkable success in natural language inference (NLI) tasks. While these models are widely explored, they are hard to interpret and it is often unclear how and why they actually work. In this paper, we take a step toward explaining such deep learning based models through a case study on a popular neural model for NLI. In particular, we propose to interpret the intermediate layers of NLI models by visualizing the saliency of attention and LSTM gating signals. We present several examples for which our methods are able to reveal interesting insights and identify the critical information contributing to the model decisions.",
  "full_text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4952–4957\nBrussels, Belgium, October 31 - November 4, 2018.c⃝2018 Association for Computational Linguistics\n4952\nInterpreting Recurrent and Attention-Based Neural Models:\na Case Study on Natural Language Inference\nReza Ghaeini, Xiaoli Z. Fern, Prasad Tadepalli\nSchool of Electrical Engineering and Computer Science, Oregon State University\n1148 Kelley Engineering Center, Corvallis, OR 97331-5501, USA\n{ghaeinim, xfern, tadepall}@eecs.oregonstate.edu\nAbstract\nDeep learning models have achieved remark-\nable success in natural language inference\n(NLI) tasks. While these models are widely\nexplored, they are hard to interpret and it is of-\nten unclear how and why they actually work.\nIn this paper, we take a step toward explain-\ning such deep learning based models through a\ncase study on a popular neural model for NLI.\nIn particular, we propose to interpret the in-\ntermediate layers of NLI models by visualiz-\ning the saliency of attention and LSTM gat-\ning signals. We present several examples for\nwhich our methods are able to reveal interest-\ning insights and identify the critical informa-\ntion contributing to the model decisions.\n1 Introduction\nDeep learning has achieved tremendous success\nfor many NLP tasks. However, unlike traditional\nmethods that provide optimized weights for hu-\nman understandable features, the behavior of deep\nlearning models is much harder to interpret. Due\nto the high dimensionality of word embeddings,\nand the complex, typically recurrent architectures\nused for textual data, it is often unclear how and\nwhy a deep learning model reaches its decisions.\nThere are a few attempts toward explain-\ning/interpreting deep learning-based models,\nmostly by visualizing the representation of words\nand/or hidden states, and their importances (via\nsaliency or erasure) on shallow tasks like senti-\nment analysis and POS tagging (Bahdanau et al.,\n2014; Denil et al., 2014; Li et al., 2016; Arras\net al., 2017; Li et al., 2017; Rei and Søgaard,\n2018). In contrast, we focus on interpreting the\ngating and attention signals of the intermediate\nlayers of deep models in the challenging task of\nNatural Language Inference. A key concept in\nexplaining deep models is saliency, which deter-\nmines what is critical for the ﬁnal decision of a\ndeep model. So far, saliency has only been used to\nillustrate the impact of word embeddings. In this\npaper, we extend this concept to the intermediate\nlayer of deep models to examine the saliency of\nattention as well as the LSTM gating signals to\nunderstand the behavior of these components and\ntheir impact on the ﬁnal decision.\nWe make two main contributions. First, we in-\ntroduce new strategies for interpreting the behav-\nior of deep models in their intermediate layers,\nspeciﬁcally, by examining the saliency of the at-\ntention and the gating signals. Second, we provide\nan extensive analysis of the state-of-the-art model\nfor the NLI task and show that our methods reveal\ninteresting insights not available from traditional\nmethods of inspecting attention and word saliency.\nIn this paper, our focus was on NLI, which is\na fundamental NLP task that requires both under-\nstanding and reasoning. Furthermore, the state-of-\nthe-art NLI models employ complex neural archi-\ntectures involving key mechanisms, such as atten-\ntion and repeated reading, widely seen in success-\nful models for other NLP tasks. As such, we ex-\npect our methods to be potentially useful for other\nnatural understanding tasks as well.\n2 Task and Model\nIn NLI (Bowman et al., 2015), we are given two\nsentences, a premise and a hypothesis, the goal\nis to decide the logical relationship ( Entailment,\nNeutral, or Contradiction) between them.\nMany of the top performing NLI models\n(Ghaeini et al., 2018b; Tay et al., 2018; Peters\net al., 2018; McCann et al., 2017; Gong et al.,\n2017; Wang et al., 2017; Chen et al., 2017), are\nvariants of the ESIM model (Chen et al., 2017),\nwhich we choose to analyze in this paper. ESIM\nreads the sentences independently using LSTM at\nﬁrst, and then applies attention to align/contrast\n4953\nAlittle\nkid\nisplaying\nin\nagarden\nPremise\n(a) Contradiction Sample\nA kid is\ntaking\na\nnap in a\ngarden\nHypothesis (h1)\nAlittle\nkid\nisplaying\nin\nagarden\nPremise\n(b) Neutral Sample\nA kid is\nhaving\nfun in a\ngarden\nwithher\nfamily\nHypothesis (h2)\n(c) Entailment Sample\nA kid is\nhaving\nfun in a\ngarden\nHypothesis (h3)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 1: Normalized attention and attention saliency visualization. Each column shows visualization of one sam-\nple. Top plots depict attention visualization and bottom ones represent attention saliency visualization. Predicted\n(the same as Gold) label of each sample is shown on top of each column.\nthe sentences. Another round of LSTM reading\nthen produces the ﬁnal representations, which are\ncompared to make the prediction. Detailed de-\nscription of ESIM can be found in the Supplemen-\ntary Materials.\nUsing the SNLI (Bowman et al., 2015) data, we\ntrain two variants of ESIM, with dimensionality 50\nand 300 respectively, referred to as ESIM-50 and\nESIM-300 in the remainder of the paper.\n3 Visualization of Attention and Gating\nIn this work, we are primarily interested in the in-\nternal workings of the NLI model. In particular,\nwe focus on the attention and the gating signals\nof LSTM readers, and how they contribute to the\ndecisions of the model.\n3.1 Attention\nAttention has been widely used in many NLP tasks\n(Ghaeini et al., 2018a; Dhingra et al., 2017; Bah-\ndanau et al., 2014) and is probably one of the\nmost critical parts that affects the inference deci-\nsions. Several pieces of prior work in NLI have\nattempted to visualize the attention layer to pro-\nvide some understanding of their models (Ghaeini\net al., 2018b; Parikh et al., 2016). Such visualiza-\ntions generate a heatmap representing the similar-\nity between the hidden states of the premise and\nthe hypothesis (Eq. 3 of the Supplementary Mate-\nrials). Unfortunately the similarities are often the\nsame regardless of the decision.\nLet us consider the following example, where\nthe same premise “A kid is playing in the garden”,\nis paired with three different hypotheses:\nh1: A kid is taking a nap in the garden\nh2: A kid is having fun in the garden with her\nfamily\nh3: A kid is having fun in the garden\nNote that the ground truth relationships are Con-\ntradiction, Neutral, and Entailment, respectively.\nThe ﬁrst row of Fig. 1 shows the visualization of\nnormalized attention for the three cases produced\nby ESIM-50, which makes correct predictions for\nall of them. As we can see from the ﬁgure, the\nthree attention maps are fairly similar despite the\ncompletely different decisions. The key issue is\nthat the attention visualization only allows us to\nsee how the model aligns the premise with the hy-\npothesis, but does not show how such alignment\nimpacts the decision. This prompts us to consider\nthe saliency of attention.\n3.1.1 Attention Saliency\nThe concept of saliency was ﬁrst introduced in vi-\nsion for visualizing the spatial support on an im-\nage for a particular object class (Simonyan et al.,\n2013). In NLP, saliency has been used to study the\nimportance of words toward a ﬁnal decision (Li\net al., 2016) .\nWe propose to examine the saliency of atten-\ntion. Speciﬁcally, given a premise-hypothesis pair\nand the model’s decision y, we consider the sim-\nilarity between a pair of premise and hypothesis\nhidden states eij as a variable. The score of the\ndecision S(y) is thus a function of eij for all i and\nj. The saliency of eij is then deﬁned to be | ∂S(y)\n∂eij\n|.\n4954\nJohn\nordered\na\nbook\nfrom\namazon\nPremise (p1)\n(a) Attention, ESIM-50\n (b) Attention, ESIM-300\n (c) Saliency, ESIM-50\n (d) Saliency, ESIM-300\nA\nman\nordered\na\nbook\nHypothesis\nMary\nordered\na\nbook\nfrom\namazon\nPremise (p2)\nA\nman\nordered\na\nbook\nHypothesis\nA\nman\nordered\na\nbook\nHypothesis\nA\nman\nordered\na\nbook\nHypothesis\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 2: Normalized attention and attention saliency visualizations of two examples (p1 and p2) for ESIM-50 (a)\nand ESIM-300 (b) models. Each column indicates visualization of a model and each row represents visualization\nof one example.\nThe second row of Fig. 1 presents the atten-\ntion saliency map for the three examples acquired\nby the same ESIM-50 model. Interestingly, the\nsaliencies are clearly different across the exam-\nples, each highlighting different parts of the align-\nment. Speciﬁcally, for h1, we see the alignment\nbetween “is playing” and “taking a nap” and the\nalignment of “in a garden” to have the most promi-\nnent saliency toward the decision of Contradiction.\nFor h2, the alignment of “kid” and “her family”\nseems to be the most salient for the decision of\nNeutral. Finally, for h3, the alignment between “is\nhaving fun” and “kid is playing” have the strongest\nimpact toward the decision of Entailment.\nFrom this example, we can see that by inspect-\ning the attention saliency, we effectively pinpoint\nwhich part of the alignments contribute most criti-\ncally to the ﬁnal prediction whereas simply visual-\nizing the attention itself reveals little information.\n3.1.2 Comparing Models\nIn the previous examples, we study the behavior of\nthe same model on different inputs. Now we use\nthe attention saliency to compare the two different\nESIM models: ESIM-50 and ESIM-300.\nConsider two examples with a shared hypothe-\nsis of “A man ordered a book” and premise:\np1: John ordered a book from amazon\np2: Mary ordered a book from amazon\nHere ESIM-50 fails to capture the gender connec-\ntions of the two different names and predicts Neu-\ntral for both inputs, whereas ESIM-300 correctly\npredicts Entailment for the ﬁrst case and Contra-\ndiction for the second.\nIn the ﬁrst two columns of Fig. 2 (column a and\nb) we visualize the attention of the two examples\nfor ESIM-50 (left) and ESIM-300 (right) respec-\ntively. Although the two models make different\npredictions, their attention maps appear qualita-\ntively similar.\nIn contrast, columns 3-4 of Fig. 2 (column\nc and d) present the attention saliency for the\ntwo examples by ESIM-50 and ESIM-300 respec-\ntively. We see that for both examples, ESIM-50\nprimarily focused on the alignment of “ordered”,\nwhereas ESIM-300 focused more on the align-\nment of “John” and “Mary” with “man”. It is\ninteresting to note that ESIM-300 does not ap-\npear to learn signiﬁcantly different similarity val-\nues compared to ESIM-50 for the two critical pairs\nof words (“John”, “man”) and (“Mary”, “man”)\nbased on the attention map. The saliency map,\nhowever, reveals that the two models use these\nvalues quite differently, with only ESIM-300 cor-\nrectly focusing on them.\n3.2 LSTM Gating Signals\nLSTM gating signals determine the ﬂow of infor-\nmation. In other words, they indicate how LSTM\nreads the word sequences and how the informa-\ntion from different parts is captured and combined.\nLSTM gating signals are rarely analyzed, possibly\n4955\n0\n1output\n(a) Contradiction Sample\n0\n1\nInference LSTM\nforget\n0\n1input\n0\n1output\n0\n1\nInput LSTM\nforget\nA kid is\ntaking\na\nnap in a\ngarden\nHypothesis (h1)\n0\n1input\n0\n1\n(b) Neutral Sample\n0\n1\n0\n1\n0\n1\n0\n1\nA kid is\nhaving\nfun in a\ngarden\nwith her\nfamily\nHypothesis (h2)\n0\n1\n0\n1\n(c) Entailment Sample\n0\n1\n0\n1\n0\n1\n0\n1\nA kid is\nhaving\nfun in a\ngarden\nHypothesis (h3)\n0\n1\nSaliency Vector Norm Vector Norm\nFigure 3: Normalized signal and saliency norms for the input and inference LSTMs (forward) of ESIM-50 for\nthree examples. The bottom (top) three rows show the signals of the input (inference) LSTM. Each row shows one\nof the three gates (input, forget and output).\ndue to their high dimensionality and complexity.\nIn this work, we consider both the gating signals\nand their saliency, which is computed as the partial\nderivative of the score of the ﬁnal decision with re-\nspect to each gating signal.\nInstead of considering individual dimensions of\nthe gating signals, we aggregate them to consider\ntheir norm, both for the signal and for its saliency.\nNote that ESIM models have two LSTM layers,\nthe ﬁrst (input) LSTM performs the input encod-\ning and the second (inference) LSTM generates\nthe representation for inference.\nIn Fig. 3 we plot the normalized signal and\nsaliency norms for different gates (input, forget,\noutput)1 of the Forward input (bottom three rows)\nand inference (top three rows) LSTMs. These re-\nsults are produced by the ESIM-50 model for the\nthree examples of Section 3.1, one for each col-\numn.\nFrom the ﬁgure, we ﬁrst note that the saliency\ntends to be somewhat consistent across different\ngates within the same LSTM, suggesting that we\ncan interpret them jointly to identify parts of the\nsentence important for the model’s prediction.\nComparing across examples, we see that the\nsaliency curves show pronounced differences\nacross the examples. For instance, the saliency\npattern of the Neutral example is signiﬁcantly dif-\nferent from the other two examples, and heavily\nconcentrated toward the end of the sentence (“with\n1We also examined the memory cell but it shows very sim-\nilar behavior with the output gate and is hence omitted.\nher family”). Note that without this part of the\nsentence, the relationship would have been Entail-\nment. The focus (evidenced by its strong saliency\nand strong gating signal) on this particular part,\nwhich presents information not available from the\npremise, explains the model’s decision of Neutral.\nComparing the behavior of the input LSTM and\nthe inference LSTM, we observe interesting shifts\nof focus. In particular, we see that the infer-\nence LSTM tends to see much more concentrated\nsaliency over key parts of the sentence, whereas\nthe input LSTM sees more spread of saliency. For\nexample, for the Contradiction example, the input\nLSTM sees high saliency for both “taking” and\n“in”, whereas the inference LSTM primarily fo-\ncuses on “nap”, which is the key word suggesting\na Contradiction. Note that ESIM uses attention\nbetween the input and inference LSTM layers to\nalign/contrast the sentences, hence it makes sense\nthat the inference LSTM is more focused on the\ncritical differences between the sentences. This is\nalso observed for the Neutral example as well.\nIt is worth noting that, while revealing similar\ngeneral trends, the backward LSTM can some-\ntimes focus on different parts of the sentence (e.g.,\nsee Fig. 8 of the Supplementary Materials), sug-\ngesting the forward and backward readings pro-\nvide complementary understanding of the sen-\ntence.\n4956\n4 Conclusion\nWe propose new visualization and interpretation\nstrategies for neural models to understand how and\nwhy they work. We demonstrate the effective-\nness of the proposed strategies on a complex task\n(NLI). Our strategies are able to provide interest-\ning insights not achievable by previous explana-\ntion techniques. Our future work will extend our\nstudy to consider other NLP tasks and models with\nthe goal of producing useful insights for further\nimproving these models.\nReferences\nLeila Arras, Gr´egoire Montavon, Klaus-Robert M¨uller,\nand Wojciech Samek. 2017. Explaining recurrent\nneural network predictions in sentiment analysis.\nProceedings of the 8th Workshop on Computational\nApproaches to Subjectivity, Sentiment and Social\nMedia Analysis, WASSA@EMNLP 2017, Copen-\nhagen, Denmark, September 8, 2017 , pages 159–\n168.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua\nBengio. 2014. Neural machine translation by\njointly learning to align and translate. CoRR,\nabs/1409.0473.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large an-\nnotated corpus for learning natural language infer-\nence. Proceedings of the 2015 Conference on Em-\npirical Methods in Natural Language Processing,\nEMNLP 2015, Lisbon, Portugal, September 17-21,\n2015, pages 632–642.\nQian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui\nJiang, and Diana Inkpen. 2017. Enhanced LSTM\nfor natural language inference. Proceedings of the\n55th Annual Meeting of the Association for Compu-\ntational Linguistics, ACL 2017, Vancouver, Canada,\nJuly 30 - August 4, Volume 1: Long Papers , pages\n1657–1668.\nMisha Denil, Alban Demiraj, Nal Kalchbrenner, Phil\nBlunsom, and Nando de Freitas. 2014. Mod-\nelling, visualising and summarising documents with\na single convolutional neural network. CoRR,\nabs/1406.3830.\nBhuwan Dhingra, Hanxiao Liu, Zhilin Yang,\nWilliam W. Cohen, and Ruslan Salakhutdinov.\n2017. Gated-attention readers for text compre-\nhension. Proceedings of the 55th Annual Meeting\nof the Association for Computational Linguistics,\nACL 2017, Vancouver, Canada, July 30 - August 4,\nVolume 1: Long Papers, pages 1832–1846.\nReza Ghaeini, Xiaoli Z. Fern, Hamed Shahbazi, and\nPrasad Tadepalli. 2018a. Dependent gated reading\nfor cloze-style question answering. Proceedings of\nthe 27th International Conference on Computational\nLinguistics, COLING 2018, Santa Fe, New Mexico,\nUSA, August 20-26, 2018, pages 3330–3345.\nReza Ghaeini, Sadid A. Hasan, Vivek V . Datla, Joey\nLiu, Kathy Lee, Ashequl Qadir, Yuan Ling, Aaditya\nPrakash, Xiaoli Z. Fern, and Oladimeji Farri. 2018b.\nDr-bilstm: Dependent reading bidirectional LSTM\nfor natural language inference. NAACL HLT 2018,\nThe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies.\nYichen Gong, Heng Luo, and Jian Zhang. 2017.\nNatural language inference over interaction space.\nCoRR, abs/1709.04348.\nJiwei Li, Xinlei Chen, Eduard H. Hovy, and Dan Ju-\nrafsky. 2016. Visualizing and understanding neural\nmodels in NLP. NAACL HLT 2016, The 2016 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, San Diego California, USA,\nJune 12-17, 2016, pages 681–691.\nJiwei Li, Will Monroe, and Dan Jurafsky. 2017. Un-\nderstanding neural networks through representation\nerasure. CoRR, abs/1612.08220.\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. Advances in Neural Infor-\nmation Processing Systems 30: Annual Conference\non Neural Information Processing Systems 2017, 4-\n9 December 2017, Long Beach, CA, USA , pages\n6297–6308.\nAnkur P. Parikh, Oscar T¨ackstr¨om, Dipanjan Das, and\nJakob Uszkoreit. 2016. A decomposable attention\nmodel for natural language inference. Proceed-\nings of the 2016 Conference on Empirical Meth-\nods in Natural Language Processing, EMNLP 2016,\nAustin, Texas, USA, November 1-4, 2016 , pages\n2249–2255.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. Proceedings of the 2018 Conference\nof the North American Chapter of the Associa-\ntion for Computational Linguistics: Human Lan-\nguage Technologies, NAACL-HLT 2018, New Or-\nleans, Louisiana, USA, June 1-6, 2018, Volume 1\n(Long Papers), pages 2227–2237.\nMarek Rei and Anders Søgaard. 2018. Zero-shot se-\nquence labeling: Transferring knowledge from sen-\ntences to tokens. Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, NAACL-HLT 2018, New Or-\nleans, Louisiana, USA, June 1-6, 2018, Volume 1\n(Long Papers), pages 293–302.\n4957\nKaren Simonyan, Andrea Vedaldi, and Andrew Zisser-\nman. 2013. Deep inside convolutional networks: Vi-\nsualising image classiﬁcation models and saliency\nmaps. CoRR, abs/1312.6034.\nYi Tay, Luu Anh Tuan, and Siu Cheung Hui. 2018. A\ncompare-propagate architecture with alignment fac-\ntorization for natural language inference. CoRR,\nabs/1801.00102.\nZhiguo Wang, Wael Hamza, and Radu Florian. 2017.\nBilateral multi-perspective matching for natural lan-\nguage sentences. Proceedings of the Twenty-Sixth\nInternational Joint Conference on Artiﬁcial Intelli-\ngence, IJCAI 2017, Melbourne, Australia, August\n19-25, 2017, pages 4144–4150.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8206354379653931
    },
    {
      "name": "Artificial intelligence",
      "score": 0.698192834854126
    },
    {
      "name": "Inference",
      "score": 0.6902390718460083
    },
    {
      "name": "Deep learning",
      "score": 0.5629615187644958
    },
    {
      "name": "Natural language",
      "score": 0.5258781909942627
    },
    {
      "name": "Deep neural networks",
      "score": 0.5061916708946228
    },
    {
      "name": "Natural (archaeology)",
      "score": 0.4777902066707611
    },
    {
      "name": "Natural language processing",
      "score": 0.4638763666152954
    },
    {
      "name": "Machine learning",
      "score": 0.45958298444747925
    },
    {
      "name": "Artificial neural network",
      "score": 0.4466167986392975
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    }
  ],
  "cited_by": 84
}