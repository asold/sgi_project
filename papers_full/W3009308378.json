{
  "title": "An Empirical Investigation of Pre-Trained Transformer Language Models for Open-Domain Dialogue Generation",
  "url": "https://openalex.org/W3009308378",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2231753118",
      "name": "Li, Piji",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2962796276",
    "https://openalex.org/W2799037524",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2970680405",
    "https://openalex.org/W2963939249",
    "https://openalex.org/W2962717182",
    "https://openalex.org/W2807873315",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W2891416139",
    "https://openalex.org/W2990033294",
    "https://openalex.org/W2951004968",
    "https://openalex.org/W2963963856",
    "https://openalex.org/W2810840719",
    "https://openalex.org/W2964352131",
    "https://openalex.org/W2949747155",
    "https://openalex.org/W2945978556",
    "https://openalex.org/W2898875342",
    "https://openalex.org/W2963825865",
    "https://openalex.org/W2971199636",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3116376974",
    "https://openalex.org/W2581637843",
    "https://openalex.org/W2586847566",
    "https://openalex.org/W2949782788",
    "https://openalex.org/W2963790827",
    "https://openalex.org/W2963520511",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W2983962589",
    "https://openalex.org/W2890394457",
    "https://openalex.org/W2972088462",
    "https://openalex.org/W1518951372",
    "https://openalex.org/W2963879591",
    "https://openalex.org/W2989429271",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2970960706",
    "https://openalex.org/W2952355681",
    "https://openalex.org/W2962883855",
    "https://openalex.org/W3037026762",
    "https://openalex.org/W2806253709",
    "https://openalex.org/W1498238796",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2410983263",
    "https://openalex.org/W2805005636",
    "https://openalex.org/W2949433733",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W1591706642",
    "https://openalex.org/W2890969459",
    "https://openalex.org/W2756487349",
    "https://openalex.org/W2997173942",
    "https://openalex.org/W2998563994",
    "https://openalex.org/W3000779003",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2016089260",
    "https://openalex.org/W3091061106",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2963544536",
    "https://openalex.org/W2294065713",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W2963068985",
    "https://openalex.org/W2912521296",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963167310"
  ],
  "abstract": "We present an empirical investigation of pre-trained Transformer-based auto-regressive language models for the task of open-domain dialogue generation. Training paradigm of pre-training and fine-tuning is employed to conduct the parameter learning. Corpora of News and Wikipedia in Chinese and English are collected for the pre-training stage respectively. Dialogue context and response are concatenated into a single sequence utilized as the input of the models during the fine-tuning stage. A weighted joint prediction paradigm for both context and response is designed to evaluate the performance of models with or without the loss term for context prediction. Various of decoding strategies such as greedy search, beam search, top-k sampling, etc. are employed to conduct the response text generation. Extensive experiments are conducted on the typical single-turn and multi-turn dialogue corpora such as Weibo, Douban, Reddit, DailyDialog, and Persona-Chat. Detailed numbers of automatic evaluation metrics on relevance and diversity of the generated results for the languages models as well as the baseline approaches are reported.",
  "full_text": "An Empirical Investigation of Pre-Trained Transformer Language Models\nfor Open-Domain Dialogue Generation\nPiji Li\nTencent AI Lab\npijili@tencent.com\nAbstract\nWe present an empirical investigation of pre-\ntrained Transformer-based auto-regressive lan-\nguage models for the task of open-domain di-\nalogue generation. Training paradigm of pre-\ntraining and Ô¨Åne-tuning is employed to con-\nduct the parameter learning. Corpora of News\nand Wikipedia in Chinese and English are col-\nlected for the pre-training stage respectively.\nDialogue context and response are concate-\nnated into a single sequence utilized as the in-\nput of the models during the Ô¨Åne-tuning stage.\nA weighted joint prediction paradigm for both\ncontext and response is designed to evaluate\nthe performance of models with or without\nthe loss term for context prediction. Various\nof decoding strategies such as greedy search,\nbeam search, top-k sampling, etc. are em-\nployed to conduct the response text generation.\nExtensive experiments are conducted on the\ntypical single-turn and multi-turn dialogue cor-\npora such as Weibo, Douban, Reddit, Daily-\nDialog, and Persona-Chat. Detailed numbers\nof automatic evaluation metrics on relevance\nand diversity of the generated results for the\nlanguages models as well as the baseline ap-\nproaches are reported.1\n1 Introduction\nOpen-domain dialogue generation is an increas-\ningly important task in the area of natural language\nprocessing and has received considerable critical\nattention across a number of researchers in recent\nyears (Gao et al., 2019a). Inspired by the frame-\nworks utilized in the the task of neural machine\ntranslation (Bahdanau et al., 2015; Cho et al., 2014),\nthe pioneer researchers formulate the dialogue gen-\neration task as a sequence-to-sequence (Seq2Seq)\ntranslation problem (Shang et al., 2015; Vinyals\nand Le, 2015; Sordoni et al., 2015). Encoder is de-\nsigned for dialogue context modeling, and decoder\n1Code and data available at http://github.com/lipiji/Guyu.\nis constructed to conduct the target response predic-\ntion. Long Short-Term Memory (LSTM) (Hochre-\niter and Schmidhuber, 1997), Gated Recurrent Unit\n(GRU) (Cho et al., 2014), and Transformer Self-\nAttention Network (SAN) (Vaswani et al., 2017)\nare employed as the backbone of various encoders\nand decoders. For training, Maximum Likelihood\nEstimation (MLE) is usually as the parameter op-\ntimization approach. For decoding, beam search\nalgorithm (Koehn, 2004) is the preference to de-\ncode and generate the target response.\nTremendous progress has been achieved follow-\ning those pioneer works. Simultaneously, on one\nside, researchers propose some strategies to ad-\ndress the typical issues observed from the gener-\nated results, and on the other side, amount of new\ndirections and tasks have been explored broadly\n(Gao et al., 2019a). SpeciÔ¨Åcally, one well-know\ntypical phenomenon is that the generated responses\nare usually generic and boring. Quiet a few re-\nsults are neither relevant to the context nor infor-\nmative and diverse. This is caused by various of\nreasons, such as the MLE optimization objective,\nsemantic or posterior collapse of the input context\nrepresentations, one-to-many mapping issue (one\nsingle query can match several different but cor-\nresponding responses), etc. To address this issue,\nlots of model structures, training strategies, and\noperation tricks have been constructed and pro-\nposed, such as the Maximum Mutual Information\n(MMI) optimization objective (Li et al., 2016a),\nadversarial training (Li et al., 2017a; Xu et al.,\n2017; Zhang et al., 2018b) or reinforcement learn-\ning (Li et al., 2016c; Cuay ¬¥ahuitl, 2017; Dhingra\net al., 2017) tricks, continuous or discrete latent se-\nmantic modeling approaches (Zhao et al., 2017; Gu\net al., 2018; Gao et al., 2019b; Qiu et al., 2019; Gao\net al., 2019c), hierarchical encoder-decoder frame-\nworks (HRED) for multi-turn dialogue generation\n(Serban et al., 2016, 2017; Park et al., 2018), etc.\narXiv:2003.04195v1  [cs.CL]  9 Mar 2020\nMeanwhile, expanded from the general dialogue\ngeneration problem, more interesting and challeng-\ning tasks have been explored. For instance, per-\nsona context (Li et al., 2016b; Zhang et al., 2018a;\nMazare et al., 2018; Chan et al., 2019; Madotto\net al., 2019; Zheng et al., 2019; Xu et al., 2020)\nand emotional information (Zhou et al., 2018a,c,a;\nHuang et al., 2018; Rashkin et al., 2019; Li et al.,\n2019a) are incorporated into the basic framework to\nimprove the anthropomorphic characteristic. Exter-\nnal knowledge such as the Commonsense Knowl-\nedge (Liu and Singh, 2004) is also introduced to\nconduct the knowledge-grounded dialogue genera-\ntion (Ghazvininejad et al., 2018; Young et al., 2018;\nZhou et al., 2018b; Dinan et al., 2018). The prob-\nlem of sentence function controlled dialogue gen-\neration has also been investigated (Ke et al., 2018;\nBi et al., 2019).\nRecently, pre-trained transformer-based lan-\nguage models such as BERT (Devlin et al., 2019)\nand GPT-2 (Radford et al., 2019b,a) have achieved\ntremendous success in various tasks of natural\nlanguage processing such as text classiÔ¨Åcation,\ntext matching, question answering, summarization\n(Dong et al., 2019), storytelling (See et al., 2019),\netc. Researchers are also trying to construct the\ntransformer-based language models for the task\nof dialogue generation. Typically, Olabiyi and\nMueller (2019) conduct some experimental anal-\nysis about the performance of transformer-based\nauto-regressive language models on dialogue gen-\neration. Li et al. (2019b) propose a relevance-\npromoting language model by incorporating a topic\ninference component into the transformer language\nmodel to conduct diverse and informative response\ngeneration. Zhang et al. (2019) introduce a pre-\ntraining stage and MMI (Li et al., 2016a) is em-\nployed for results re-ranking and selection. Adiwar-\ndana et al. (2020) release a chat-bot named Meena\nwhich is constructed based on Evolved Transformer\n(ET) (So et al., 2019) with 2.6B parameters and is\ntrained on a large-scale multi-turn open domain\ndialogue corpus with 867M context-response pairs\n(341G text data with 40B words). A new human\nevaluation metric called Sensibleness and Speci-\nÔ¨Åcity Average (SSA) is proposed and experimental\nresults demonstrate that perplexity highly corre-\nlates with SSA.\nIn this paper, we present an empirical study of\npre-trained transformer-based auto-regressive lan-\nguage models for the task of open-domain dialogue\ngeneration. The purpose is to construct a vividly\nsense and understanding of the performance for the\n(pre-trained) transformer-based auto-regressive lan-\nguage models on the tasks of open-domain dialogue\ngeneration. Training paradigm of pre-training and\nÔ¨Åne-tuning is employed to conduct the parameter\nlearning. Corpora of News and Wikipedia in Chi-\nnese and English are collected for the pre-training\nstage respectively. Dialogue context and response\nare concatenated into a single sequence which is\nutilized as the input of the language model dur-\ning the Ô¨Åne-tuning stage. A weighted joint pre-\ndicting paradigm for both context and response is\ndesigned to evaluate the performance of models\nwith or without the loss term for the context pre-\ndiction results. Various of decoding strategies such\nas greedy search, beam search, top-k sampling,\netc. are employed to conduct the response text pre-\ndiction. Extensive experiments are conducted on\nthe typical single-turn and multi-turn dialogue cor-\npora such as Weibo (Shang et al., 2015; Gao et al.,\n2019b), Douban (Pan et al., 2019), Reddit (Zhou\net al., 2018b), DailyDialog (Li et al., 2017b), and\nPersona-Chat (Zhang et al., 2018a). Detailed num-\nbers of automatic evaluation metrics on relevance\n(BLEU) and diversity (Distinct n-gram ratios) of\nthe generated results are reported.\nIn the following sections, we will Ô¨Årst introduce\nthe model architecture (Section 2), and then the\nexperimental settings (Section 3) and results dis-\ncussions (Section 4). Finally, we will provide suf-\nÔ¨Åcient examples to demonstrate the performance\n(Section 5) and following the conclusions as well\nas the future plans (Section 6).\n2 Model Architecture\nAs shown in Figure 1, the core model architecture\ninvestigated in this work is a GPT-2 (Radford et al.,\n2019a) like transformer-based auto-regressive lan-\nguage model. We use X = (x0,x1,x2,...,x m)\nand Y = (y0,y1,y2,...,y n) to denote the se-\nquence of input context and target response respec-\ntively. mand nrepresent the length of context and\nresponse, respectively. It should be noted that we\nregard the input context as a whole sequence, either\nsingle-turn query or multi-turn historical context\nutterances. Considering that the basic framework\nis a auto-regressive language model, therefore, we\nconcatenate the context sequence X and the re-\nsponse sequence Y and obtain a whole sequence\nI = (X,Y ) as the input. For convenience, we as-\n<bos>Input\nTokenEmbeddings\nPositionalEmbeddings\nùêÑùíôùüé\nùêÑ1 ùêÑ2 ùêÑ3 ùêÑ4 ùêÑ5 ùêÑ7ùêÑ6ùêÑ0\n<eos>Output\nùë•0 ùë•1 ùë•2 ùë•3 ùë¶0 ùë¶1 ùë¶2\nùêÑ<bos>\n<bos>ùë•1 ùë•2 ùë•3 ùë¶0 ùë¶1 ùë¶2\nùêÑùíôùüè ùêÑùíôùüê ùêÑùíôùüë ùêÑùíöùüé ùêÑùíöùüè ùêÑùíöùüê\nContextPrediction ResponsePrediction\nFigure 1: Transformer-based auto-regressive language model for dialogue generation.\nsume that the separator token <bos> between X\nand Y is the last token xm ‚ààX, then we can de-\nnote the Ô¨Ånal sequence length as T = |I|= m+ n.\nMasking multi-head self-attention is utilized as\nthe core technical operation to conduct representa-\ntion learning:\nH1\nt = LN\n(\nFFN(H1\nt ) +H1\nt\n)\nH1\nt = LN\n(\nSLF-ATT(Q0\nt ,K0\n‚â§t,V0\n‚â§t) +H0\nt\n)\nQ0 = H0WQ\nK0,V0 = H0WK,H0WV\n(1)\nwhere SLF-ATT(¬∑), LN(¬∑), and FFN(¬∑) represent\nself-attention mechanism, layer normalization, and\nfeed-forward network respectively (Vaswani et al.,\n2017). Note that we only use the states whose\nindices ‚â§tas the attention context.\nTraining: During training, we employ Maximum\nLikelihood Estimation (MLE) to conduct the pa-\nrameter learning and treat negative log-likelihood\n(NLL) over the whole target response sequence as\nthe loss function:\nLy\nnll = ‚àí\nn‚àë\nt=1\nlog P(yt|y<t,X) (2)\nIntuitively, since we have concatenated X and\nY, then we can obtain another term of prediction\nloss, i.e., the loss term for predicting the dialogue\ncontext X.\nLx\nnll = ‚àí\nm‚àë\nt=1\nlog P(xt|x<t) (3)\nConsidering that context and response are mono-\nlingual pairs, then introduce the context predic-\ntion loss may beneÔ¨Åt the language modeling per-\nformance, since context texts contribute the other\nhalf of corpus to teach the model how to handle\na language. This assumption is also mentioned in\nthe previous works of Olabiyi and Mueller (2019)\nand Li et al. (2019b). Therefore, a weighted joint\npredicting paradigm for both context and response\nis constructed. By tuning the weight, we can also\nevaluate the performance of models with or without\nthe loss term for the context prediction results.\nLnll = Ly\nnll + Œª√óLx\nnll (4)\nDecoding: In the inference stage, various of decod-\ning strategies such as greedy search, beam search\n(Koehn, 2004), truncated top-ksampling (Fan et al.,\n2018; Radford et al., 2019b), and Nucleus Sam-\npling (top-psampling) (Holtzman et al., 2019) are\nemployed to conduct the response text generation.\nHyper-parameter such as kis tuned to trade-off the\nperformance of relevance and diversity.\n3 Experimental Settings\n3.1 Datasets\nPre-training Corpus: To investigate the perfor-\nmance of models in different language scenarios,\nwe conduct the pre-training procedure on two large\nscale corpora in Chinese and English respectively.\n‚Ä¢Chinese: We collect Chinese Wikipedia2 (1.7B\nCharacters) and a merged Chinese News corpus\n(9.2B Characters) from the Internet. We did not\nconduct the word segmenting operations on the\nChinese datasets, which means that we just use\nthe characters to build the vocabulary, and the\nsize is 27,681.\n‚Ä¢English: We employ English Wikipedia3 (2.4B\nwords) and BooksCorpus (980M words) (Zhu\n2https://dumps.wikimedia.org/zhwiki/latest/\n3https://dumps.wikimedia.org/enwiki/latest/\net al., 2015) to conduct the pre-training. Byte\nPair Encodings (BPE) (Sennrich et al., 2016) is\nconducted on this corpus as the tokenizer and the\nvocabulary size is 32,000.\nFine-tuning Dialogue Corpus Five typical dia-\nlogue corpora are collected to train the dialogue\ngeneration models: Weibo (Shang et al., 2015; Gao\net al., 2019b), Douban (Pan et al., 2019), Reddit\n(Zhou et al., 2018b), DailyDialog (Li et al., 2017b),\nand Persona-Chat (Zhang et al., 2018a). These cor-\npora are in different languages with single-turn or\nmulti-turn historical dialogue context. Moreover,\nsome pre-deÔ¨Åned external information are also pro-\nvided in the dataset of Persona-Chat besides the\npure dialogue utterances.\n‚Ä¢Weibo: A single-turn open-domain Chinese dia-\nlogue dataset, which is originally collected and\nreleased by Shang et al. (2015) from a Chinese\nsocial media platform Weibo4. Here, we use a\nreÔ¨Åned version released by Gao et al. (2019b) 5.\nThe number of samples for training, validation,\nand testing are 400M, 19357, and 3200 respec-\ntively. Character-based vocabulary size is 10231.\n‚Ä¢Douban: A multi-turn open-domain Chinese di-\nalogue corpus collected from Douban group6, a\nwell-known Chinese online community which\nis also a common data source for dialogue sys-\ntems (Wu et al., 2017). Here, we utilize a version\ncalled Restoration-200K dataset released by Pan\net al. (2019) 5. There are 193769 samples for\ntraining, 5095 for validation, and 5104 for test-\ning. V ocabulary size is 5800.\n‚Ä¢Reddit: A single-turn open-domain English cor-\npus. We use the version collected and processed\nby Zhou et al. (2018b) 7. The original version\nis designed for commonsense dialogue gener-\nation, and here we only keep the dialogue ut-\nterances and Ô¨Ålter the commonsense knowledge\nannotations. There are 300M samples for train-\ning, 10000 for validation, and 20000 for testing.\nBPE vocabulary size is 14820.\n‚Ä¢DailyDialog: A multi-turn open-domain English\ncorpus released by Li et al. (2017b) 8. There are\n11118 for training, 1000 for validation, and 1000\nfor testing. BPE vocabulary size is 12244.\n‚Ä¢Persona-Chat: Another multi-turn open-\ndomain English corpus, which contains 8939\n4https://www.weibo.com/\n5https://ai.tencent.com/ailab/nlp/dialogue/#datasets\n6https://www.douban.com/group\n7http://coai.cs.tsinghua.edu.cn/Ô¨Åle/commonsense conversation\ndataset.tar.gz\n8http://yanran.li/dailydialog.html\nsamples for training, 1000 for validation, and\n968 for testing after Ô¨Åltering (Zhang et al.,\n2018a). Moreover, this dataset provides 3-5 sen-\ntences to describe the persona information for\neach speaker (e.g. ‚Äú I like reading.‚Äù or ‚ÄúI am a\nnurse.‚Äù, etc). In our tasks, we concatenate the se-\nquence of persona sentences before the dialogue\ncontext as another kind of condition to control\nthe response generation. BPE vocabulary size is\n11362.\nThe speciÔ¨Åc statistic numbers of the dialogue\ncorpus are depicted in Table 1. We can observe\nthat Weibo and Reddit can be treated as large-scale\ncorpora, and Douban is a midle-size dataset. Com-\nparatively, DailyDialog and Persona-Chat are in\nsmall-scale.\n3.2 Metrics\nAs did in the previous works, we employ BLEU 9\nand Distinct n-gram ratios as the automatic evalu-\nation metrics. For BLEU, we provide the values\nof BLEU 1-4 as well as a weighted summation of\nthem. For Distinct, we calculate the uni-gram and\nbi-gram distinct rations and both macro-average\n(MADIST , average on instance level) and micro-\naverage (MIDIST , average on corpus level) val-\nues are reported. Moreover, we also provide the\naverage-length of the generated response for refer-\nence. More automatic metrics such as PPL, Em-\nbedding Matching, METEOR, etc., as well as the\nhuman evaluation performance will be completed\nin the future.\n3.3 Comparison Methods\n‚Ä¢Seq2Seq: The vanilla Transformer Sequence-to-\nSequence model used for machine translation\nwhere the network components are identical to\nthose in (Vaswani et al., 2017).\n‚Ä¢CV AE(Zhao et al., 2017): Variational Auto-\nEncoder (V AE) (Kingma and Welling, 2013) is\nintroduced into the LSTM-based seq2seq frame-\nwork for context semantic modeling and re-\nsponse generation.\n‚Ä¢MMI (Li et al., 2016a): LSTM-based seq2seq\nframework with Maximum Mutual Information\n(MMI) objective in decoding . In this paper, we\nset the number of responses for re-ranking as 50.\n‚Ä¢HRED (Serban et al., 2016): A hierarchical\nRNN-based encoder-decoder constructed for the\ntasks of multi-turn dialogue generation.\n9https://github.com/tensorÔ¨Çow/nmt/blob/master/nmt/scripts/bleu.py\nCorpus Language Type #Train #Dev #Test #V ocab\nWeibo Chinese Single-Turn 4,244,093 19,357 3,200 10,231\nDouban Chinese Multi-Turn 193,769 5,095 5,104 5,800\nReddit English Single-Turn 3,384,185 10,000 20,000 14,820\nDailyDialog English Multi-Turn 11,118 1,000 1,000 12,244\nPersona-Chat English Multi-Turn 8,939 1,000 968 11,362\nTable 1: Statistics of the dialogue datasets.\n‚Ä¢VHRED (Serban et al., 2017): Variational Auto-\nEncoder (V AE) (Kingma and Welling, 2013) is\nincorporated into the HRED model for better\nsemantic modeling for each dialogue utterance.\n‚Ä¢VHCR (Park et al., 2018): An extension of\nVHRED, where a hierarchical latent variable\nmodel with the utterance drop regularization is\nproposed to enhance the global semantic model-\ning of the dialogue context.\n‚Ä¢LM-12 and LM-24: Transformer-based auto-\nregressive language model with layers of 12 and\n24 respectively. No pre-training stage for these\nmodels. We only conduct the training purely\non the dialogue corpus. The sufÔ¨Åx of greedy,\nbm(b), tk(k), tp(p) denote the different decod-\ning strategies, i.e., greedy search, beam search\n(Koehn, 2004), truncated top- k sampling (Fan\net al., 2018; Radford et al., 2019b), and Nucleus\nSampling (top- p sampling) (Holtzman et al.,\n2019). b, k, and pare three hyper-parameters.\n‚Ä¢*-Œª(Eq. 4): Œªis the weight of loss term for dia-\nlogue context prediction. Œª0.5 and Œª1 denote that\nŒªis 0.5 and 1 respectively. The models without\nthe term Œªmeans that Œª= 0, i.e., only conduct\nthe response prediction given the dialogue con-\ntext.\n‚Ä¢BigLM-12 and BigLM-24: The language mod-\nels with both the pre-training and Ô¨Åne-tuning\nprocedures.\n3.4 Settings\nThe length limitation of dialogue context and re-\nsponse for single-turn dialogue dataset are both 64,\nthen the maximum of the whole sequence length is\n128. For the multi-turn tasks, the length limitation\nis 300 and 64 for context and response respectively.\nLM-12 and BigLM-12 are the typical GPT-2\nmodel with 117M parameters (768 dimensions, 12\nlayers, 12 heads). LM-24 and BigLM-24 only ex-\npand the number of layers from 12 to 24, and the\ntypical 345M models (1024 dimensions, 24 layers,\n16 heads) will be evaluated and provided later.\nFor LM and BigLM models, Adam (Kingma\nand Ba, 2014) optimization method with Noam\nlearning-rate decay strategy is employed to con-\nduct the parameter learning. Validation datasets are\nused to select the appropriate models. For Seq2Seq,\nAdam with learning rate 1e-5 is utilized for train-\ning.\n4 Results and Discussions\nTable 2‚àºTable 6 depict the detailed evaluation\nnumbers for all the models on datasets of Weibo,\nDouban, Reddit, DailyDialog, and Persona-Chat\nrespectively. We will conduct the analysis and dis-\ncussions according to the following aspects 10:\n‚Ä¢Compared with the vanilla transformer-based\nSeq2Seq framework, auto-regressive language\nmodels can indeed improve the performance on\nmost of the datasets. This phenomenon is more\nclear on the multi-turn datasets of Douban, Dai-\nlyDialog, and Persona-Chat. On those datasets,\nmetric values on BLEU or BLEU-n are much\nbetter than Seq2Seq. On Weibo, LM models\nalso obtain comparable performance. However,\non Reddit dataset, Seq2Seq achieves the best\nperformance.\n‚Ä¢Obviously, pre-training can beneÔ¨Åt the perfor-\nmance on relevance. From the results we can ob-\nserve that the models with pre-training stage, i.e.,\nBigLM-12 and BigLM-24, obtain better BLEU\nscores than LM-12 and LM-24 on all the Ô¨Åve\ndatasets.\n‚Ä¢Models with context prediction loss term (*-Œª)\ncan also improve the performance, especially on\nmulti-turn dialogue scenario. From the results\non Douban, DailyDialog, and Persona-Chat we\nsee that LM-12-Œª1 models are usually better than\n10There may be some shakes behind the numbers due to the\nrandom sampling issue. More robust evaluation approaches\nneed to be designed.\nthe original model LM-12 on metric of BLEU.\nAlso, LM-12- Œª1 is generally better than LM-\n12-Œª0.5. However, from datasets of Weibo and\nReddit, we cannot get this kind of observation\nand clue. The reason may be that long context\ncan provide more useful information than the\nsingle-turn query with only one sentence.\n‚Ä¢For top-k sampling decoding methods, k is a\nsensitive parameter to trade-off the relevance and\ndiversity. Large kintroduces better diversity but\ndecreases the relevance. Thus the tuning of kis\nalso an crucial issue.\n‚Ä¢Parameter k can affect the length of the gener-\nated response. Large kcauses longer responses.\nThe reason is that a large sampling pool will\ndecrease the opportunity of the special token\n<eos>.\n‚Ä¢Nucleus Sampling (top-psampling) sometimes\nobtain comparable or better performance than\ntop-ksampling. But the improvements are not\nstrong and steady.\n‚Ä¢CV AE and MMI are still the strong baseline\nmethods and always produce better performance\non BLEU and Distinct metrics, which demon-\nstrates that latent variable modeling and MMI\nstrategy can further improve the quality of the\ngenerated results. We believe that incorporat-\ning those strategies into the LM framework may\nfurther enhance the performance signiÔ¨Åcantly.\n5 Examples\nBigLM-24 models and top-ksampling method are\nemployed to conduct the decoding to generate the\nfollowing example results. Here k= 32. Some of\nthe example queries are extracted from the previ-\nous works (Zhang et al., 2019; Adiwardana et al.,\n2020).\nFor each input context (query)x, we can repeat it\nfor btimes (say 32) to construct a batch of queries,\nwhen we can conduct a parallel decoding approach\nand b candidate results will be generated within\none period. Then we can select the most appro-\npriate one with some ranking strategies such as\nlikelihood or mutual information. The likelihood\nranking function is deÔ¨Åned as:\nr= 2\n1\nn\n‚àën\nt=1 log P(yt) (5)\n5.1 Results with Ranking Scores\nTable 7‚àºTable 9 and Table 10 ‚àºTable 12 depict\nsome examples generated by the BigLM-24 mod-\nels Reddit-Bot and Weibo-Bot respectively. These\ntwo bots are both trained for single-turn dialogue\ngeneration. The ranking scores are also provided.\nFrom the results we can observe that even the can-\ndidate results with small ranking scores can still\nconvey some relevance information for the query.\nIt seems that choose the results randomly can also\nmeet the requirement given the query.\n5.2 Results of Bot-to-Bot Interactions\nWe also conduct bot-to-bot interaction experi-\nments and the example results are described in Ta-\nble 13‚àºTable 15, Table 16, and Table 17‚àºTable 18\nfor DailyDialog-Bot, Reddit-Bot, and Douban-Bot\nrespectively. We did not setup the ending single,\nthus those two-bots can interact inÔ¨Ånite turns. We\nextract the top turns for demonstration. It should be\nnoted that, actually, no two-bots scenario settings\nin our work , we just manage a context memory\nto generate the next utterance. Therefore, we can\nobserve that the roles for A and B may interchange\nwith each other during the interactions. And the\ntopic will drift as time goes on.\n6 Conclusions\nWe present an empirical investigation of pre-trained\nTransformer-based auto-regressive language mod-\nels for the task of open-domain dialogue generation.\nTraining paradigm of pre-training and Ô¨Åne-tuning\nis employed to conduct the parameter learning. Var-\nious of decoding strategies are employed to con-\nduct the response text generation. Extensive ex-\nperiments are conducted on the typical single-turn\nand multi-turn dialogue datasets. Detailed num-\nbers of automatic evaluation metrics on relevance\nand diversity of the generated results for the lan-\nguages models as well as the baseline approaches\nare reported.\nNevertheless, some serious issues also exist in\nthe generated results, such as the grammatical is-\nsue and the topic drift problem. More seriously,\nsometimes the bots will generate contrafactual or\noffensive content. Therefore, better model struc-\ntures, training paradigms, and decoding strategies\nneed to be investigated and constructed in the fu-\nture.\nModel Relevance Diversity LengthBLEU B-1 B-2 B-3 B-4 MADIST-1 M ADIST-2 M IDIST-1 M IDIST-2\nSeq2Seq 1.31 37.08 10.29 3.76 1.71 71.73 75.53 7.70 26.63 13.88\nCV AE 4.08 42.12 15.90 8.13 5.31 73.61 78.09 9.59 34.64 14.79\nMMI 3.25 43.98 13.94 6.56 3.88 82.00 87.06 10.69 38.25 11.51\nSeq2Seq-greedy 1.09 41.62 8.99 2.85 0.96 77.20 83.81 7.56 29.39 14.31\nSeq2Seq-bm(5) 1.31 37.08 10.29 3.76 1.71 71.73 75.53 7.70 26.63 13.88\nSeq2Seq-tk(5) 0.72 41.82 7.82 1.92 0.53 86.10 94.50 9.71 44.54 13.62\nSeq2Seq-tk(10) 0.59 41.21 7.05 1.75 0.54 87.22 95.94 10.01 50.17 13.90\nSeq2Seq-tk(20) 0.63 39.06 6.07 1.50 0.42 87.90 96.27 9.78 52.62 14.39\nSeq2Seq-tk(50) 0.22 35.72 4.73 0.74 0.17 89.45 97.59 10.45 58.09 14.87\nSeq2Seq-tk(500) 0.26 30.74 3.69 0.65 0.17 91.28 98.14 12.58 67.98 16.09\nSeq2Seq-tp(0.9) 0.59 39.10 6.43 1.52 0.50 88.35 96.56 9.99 53.01 14.43\nLM-12-greedy 1.32 42.16 8.41 2.74 1.40 81.43 86.94 6.44 21.24 13.50\nLM-12-bm(5) 2.56 35.04 11.45 5.39 3.29 63.66 66.96 4.70 14.55 21.19\nLM-12-tk(5) 0.79 41.27 6.72 1.65 0.65 85.77 94.49 7.76 35.55 13.64\nLM-12-tk(10) 0.65 40.06 6.17 1.40 0.51 87.10 95.54 7.98 40.21 14.17\nLM-12-tk(20) 0.35 37.98 5.14 1.03 0.25 88.63 96.79 8.91 46.42 14.46\nLM-12-tk(50) 0.31 36.53 4.67 0.82 0.24 89.85 97.53 9.24 52.24 15.03\nLM-12-tk(500) 0.25 29.81 3.07 0.54 0.19 91.71 98.43 12.11 67.41 15.77\nLM-12-tp(0.9) 0.47 38.58 5.34 1.04 0.28 88.75 96.85 8.63 45.48 14.42\nLM-12-greedy-Œª1 0.96 43.20 8.14 2.20 0.90 80.93 87.84 5.68 18.41 13.81\nLM-12-bm(5)-Œª1 1.86 30.34 8.90 4.04 2.46 55.86 59.11 3.30 10.00 24.33\nLM-12-tk(5)-Œª1 0.50 41.79 6.50 1.42 0.44 85.91 94.81 7.09 32.53 13.76\nLM-12-tk(10)-Œª1 0.53 40.33 6.02 1.28 0.37 87.25 96.04 7.73 39.05 14.15\nLM-12-tk(20)-Œª1 0.47 37.51 4.82 1.17 0.41 88.66 96.85 8.22 44.89 14.69\nLM-12-tk(50)-Œª1 0.31 35.78 4.38 0.79 0.23 89.78 97.40 9.12 52.50 15.05\nLM-12-tk(500)-Œª1 0.21 29.29 3.15 0.56 0.13 91.57 98.14 11.61 65.94 15.88\nLM-12-tp(0.9)-Œª1 0.40 38.56 5.35 1.15 0.37 88.41 96.92 8.04 43.98 14.66\nLM-12-greedy-Œª0.5 1.03 41.00 7.74 2.42 1.02 80.79 86.45 5.59 18.60 13.42\nLM-12-bm(5)-Œª0.5 1.60 31.58 8.80 3.69 2.00 59.20 62.23 3.72 11.56 22.87\nLM-12-tk(5)-Œª0.5 0.59 41.71 6.19 1.47 0.50 85.21 94.10 7.15 33.68 13.94\nLM-12-tk(10)-Œª0.5 0.36 39.85 5.50 1.05 0.25 87.76 96.03 7.57 38.20 13.98\nLM-12-tk(20)-Œª0.5 0.38 37.75 5.14 1.03 0.32 88.43 96.79 8.14 45.28 14.43\nLM-12-tk(50)-Œª0.5 0.34 35.01 4.27 0.89 0.32 89.27 96.93 8.95 51.60 14.68\nLM-12-tk(500)-Œª0.5 0.29 29.37 3.21 0.59 0.15 92.02 98.47 11.40 66.33 16.11\nLM-12-tp(0.9)-Œª0.5 0.26 37.65 4.87 0.89 0.18 88.52 96.83 8.37 45.25 14.27\nLM-24-greedy 1.12 39.21 7.52 2.42 1.19 75.69 81.98 5.21 17.79 16.02\nLM-24-bm(5) 2.02 34.18 9.80 4.22 2.52 64.63 68.30 4.73 15.61 19.86\nLM-24-tk(5) 0.90 41.04 6.82 1.76 0.70 84.90 94.00 7.00 32.57 14.64\nLM-24-tk(10) 0.48 39.68 5.60 1.16 0.38 86.18 95.42 7.43 37.57 14.99\nLM-24-tk(20) 0.60 37.38 4.88 1.09 0.48 87.32 95.93 7.77 43.03 15.07\nLM-24-tk(50) 0.33 34.39 4.03 0.77 0.24 89.10 97.13 8.81 51.42 15.67\nLM-24-tk(500) 0.21 28.20 2.92 0.51 0.24 91.07 97.80 11.12 65.57 16.83\nLM-24-tp(0.9) 0.45 37.17 5.25 1.16 0.39 86.64 95.85 7.86 43.87 15.56\nBigLM-12-greedy2.07 41.92 9.65 3.48 1.99 79.88 86.06 7.05 24.32 14.62\nBigLM-12-bm(5)4.00 38.96 14.77 7.70 5.14 72.61 75.97 7.50 25.97 18.62\nBigLM-12-tk(5) 1.01 43.18 8.08 2.31 1.01 85.81 93.47 8.74 39.23 13.78\nBigLM-12-tk(10)0.76 40.69 6.99 1.87 0.69 87.61 95.78 9.23 44.20 14.34\nBigLM-12-tk(20)0.95 39.28 6.25 1.78 0.76 88.62 96.76 9.02 48.06 14.54\nBigLM-12-tk(50)0.53 35.56 4.85 1.07 0.46 90.18 97.32 9.84 55.06 14.95\nBigLM-12-tk(500)0.18 29.49 3.34 0.45 0.11 91.09 97.68 12.26 67.49 15.56\nBigLM-12-tp(0.9)0.95 38.78 6.24 1.80 0.80 88.53 96.45 9.27 48.38 14.13\nBigLM-24-greedy2.32 43.38 10.68 4.13 2.39 81.06 86.31 7.17 24.72 14.22\nBigLM-24-bm(5)3.97 38.93 14.72 7.59 5.07 70.56 73.84 7.24 24.87 18.95\nBigLM-24-tk(5) 1.15 43.64 8.45 2.55 1.17 85.54 93.67 8.44 38.07 13.75\nBigLM-24-tk(10)1.22 41.23 8.01 2.49 1.22 86.71 95.42 8.81 43.83 14.45\nBigLM-24-tk(20)0.75 37.53 5.79 1.52 0.83 88.16 96.06 9.59 48.65 14.25\nBigLM-24-tk(50)0.57 36.36 5.12 1.23 0.54 89.39 97.24 9.78 53.64 14.61\nBigLM-24-tk(500)0.39 29.51 3.49 0.85 0.39 90.89 97.63 12.22 67.21 15.82\nBigLM-24-tp(0.9)0.93 39.00 5.97 1.73 0.76 88.57 96.48 9.21 48.53 14.31\nTable 2: Experimental results on Weibo dataset.\nModel Relevance Diversity LengthBLEU B-1 B-2 B-3 B-4 MADIST-1 M ADIST-2 M IDIST-1 M IDIST-2\nSeq2Seq 0.15 10.95 1.97 0.48 0.24 85.37 89.72 2.26 11.25 7.83\nCV AE 0.09 15.14 4.84 0.64 0.13 92.23 93.12 5.25 22.46 3.93\nMMI 0.06 9.83 1.91 0.35 0.09 89.95 94.52 5.46 37.55 5.20\nHRED 0.06 13.13 2.67 0.36 0.07 92.08 93.02 3.09 11.72 4.24\nVHRED 0.05 11.47 2.48 0.60 0.11 81.22 83.03 1.76 6.61 6.84\nVHCR 0.12 11.83 2.73 0.58 0.15 77.97 79.10 1.30 5.13 8.44\nSeq2Seq-greedy 0.12 11.74 1.64 0.35 0.15 88.17 92.14 2.35 15.98 9.49\nSeq2Seq-bm(5) 0.15 10.95 1.97 0.48 0.24 85.37 89.72 2.26 11.25 7.83\nSeq2Seq-tk(5) 0.05 10.34 1.21 0.24 0.05 91.33 97.06 3.10 26.71 8.62\nSeq2Seq-tk(10) 0.06 9.65 1.07 0.21 0.04 91.21 97.49 3.16 30.11 9.24\nSeq2Seq-tk(20) 0.05 8.62 0.80 0.10 0.04 92.17 98.11 3.31 34.11 9.53\nSeq2Seq-tk(50) 0.04 7.76 0.72 0.14 0.04 92.50 98.28 3.58 39.14 9.84\nSeq2Seq-tk(500) 0.04 6.60 0.62 0.13 0.03 93.96 98.83 4.89 49.16 10.18\nSeq2Seq-tp(0.9) 0.07 8.66 0.90 0.19 0.11 92.38 98.12 3.33 34.21 9.51\nLM-12-greedy 0.17 17.09 4.06 0.94 0.25 94.31 96.18 3.64 17.11 6.44\nLM-12-bm(5) 0.08 17.19 5.66 1.06 0.16 95.22 96.75 4.57 13.53 4.03\nLM-12-tk(5) 0.13 13.67 2.53 0.53 0.14 92.65 97.31 3.75 25.50 6.80\nLM-12-tk(10) 0.09 11.93 2.12 0.45 0.11 92.43 97.52 3.70 28.23 7.37\nLM-12-tk(20) 0.09 10.79 1.67 0.32 0.09 92.41 97.77 3.62 31.76 7.76\nLM-12-tk(50) 0.05 9.42 1.55 0.27 0.08 93.00 98.14 3.80 36.99 8.49\nLM-12-tk(500) 0.02 7.56 1.01 0.13 0.03 93.74 98.71 4.83 49.22 8.97\nLM-12-tp(0.9) 0.06 10.93 1.90 0.41 0.12 92.83 97.97 3.73 32.15 7.79\nLM-12-greedy-Œª1 0.18 17.09 3.96 1.07 0.26 94.67 96.77 3.91 18.43 6.01\nLM-12-bm(5)-Œª1 0.14 17.18 5.40 1.14 0.20 96.56 97.83 5.01 15.61 3.55\nLM-12-tk(5)-Œª1 0.07 13.43 2.43 0.55 0.10 92.77 97.37 3.78 25.83 6.89\nLM-12-tk(10)-Œª1 0.08 12.55 2.43 0.49 0.11 92.51 97.62 3.70 28.43 7.56\nLM-12-tk(20)-Œª1 0.11 10.88 1.83 0.42 0.11 92.28 97.88 3.72 31.77 8.05\nLM-12-tk(50)-Œª1 0.06 9.39 1.30 0.21 0.06 92.46 98.12 3.72 36.92 8.74\nLM-12-tk(500)-Œª1 0.04 7.26 0.99 0.15 0.05 93.42 98.67 4.54 48.47 9.52\nLM-12-tp(0.9)-Œª1 0.07 10.91 1.67 0.32 0.06 92.38 97.89 3.62 31.66 8.13\nLM-12-greedy-Œª0.5 0.14 16.87 4.22 1.06 0.25 93.76 95.88 3.66 17.14 6.49\nLM-12-bm(5)-Œª0.5 0.08 17.22 5.88 1.01 0.11 95.35 96.72 4.62 13.49 3.97\nLM-12-tk(5)-Œª0.5 0.12 13.59 2.59 0.46 0.11 92.69 97.36 3.82 26.14 6.87\nLM-12-tk(10)-Œª0.5 0.10 11.98 2.03 0.42 0.12 91.92 97.35 3.73 28.79 7.45\nLM-12-tk(20)-Œª0.5 0.09 10.87 1.82 0.43 0.11 92.45 97.86 3.84 32.53 7.82\nLM-12-tk(50)-Œª0.5 0.09 9.63 1.52 0.25 0.08 92.53 98.02 3.85 37.64 8.36\nLM-12-tk(500)-Œª0.5 0.04 7.27 0.90 0.15 0.04 93.64 98.54 4.71 49.50 9.16\nLM-12-tp(0.9)-Œª0.5 0.09 10.58 1.75 0.37 0.10 92.24 97.60 3.65 31.89 7.92\nLM-24-greedy 0.17 17.24 4.19 1.02 0.27 95.22 97.33 3.85 18.83 5.80\nLM-24-bm(5) 0.07 16.68 5.50 0.94 0.11 96.64 98.02 4.99 15.37 3.40\nLM-24-tk(5) 0.10 13.80 2.70 0.56 0.11 92.63 97.29 3.86 26.03 6.60\nLM-24-tk(10) 0.10 12.14 2.03 0.37 0.08 92.56 97.51 3.79 28.76 7.09\nLM-24-tk(20) 0.05 10.66 1.71 0.26 0.05 92.70 97.87 3.77 32.12 7.63\nLM-24-tk(50) 0.09 9.63 1.65 0.40 0.11 92.99 98.12 3.87 37.65 8.15\nLM-24-tk(500) 0.03 7.16 0.86 0.10 0.03 93.99 98.83 4.81 49.86 8.99\nLM-24-tp(0.9) 0.05 10.78 1.76 0.34 0.05 92.72 97.99 3.83 32.40 7.53\nBigLM-12-greedy0.17 17.09 4.25 1.07 0.28 93.39 95.76 4.19 20.70 6.71\nBigLM-12-bm(5)0.09 16.69 5.64 0.99 0.20 95.15 96.64 4.95 16.22 3.99\nBigLM-12-tk(5) 0.09 13.75 2.76 0.59 0.12 91.99 96.65 4.28 28.47 6.93\nBigLM-12-tk(10)0.09 12.48 2.36 0.49 0.12 92.35 97.36 4.13 31.06 7.45\nBigLM-12-tk(20)0.10 10.98 1.89 0.46 0.13 92.53 97.59 4.15 34.23 7.83\nBigLM-12-tk(50)0.05 9.57 1.51 0.28 0.05 93.00 98.03 4.27 38.92 8.20\nBigLM-12-tk(500)0.04 7.90 1.09 0.17 0.04 93.95 98.56 4.76 48.48 8.73\nBigLM-12-tp(0.9)0.09 11.15 2.02 0.34 0.11 92.51 97.57 4.13 34.72 7.84\nBigLM-24-greed 0.27 15.00 3.42 0.92 0.38 90.87 94.03 3.71 20.77 8.21\nBigLM-24-bm(5)0.32 14.39 4.26 1.26 0.59 85.13 89.79 4.18 19.51 6.94\nBigLM-24-tk(5) 0.10 12.82 2.26 0.48 0.15 90.47 95.94 3.95 27.48 7.87\nBigLM-24-tk(10)0.16 11.34 1.92 0.46 0.18 91.00 96.61 3.81 29.77 8.36\nBigLM-24-tk(20)0.05 10.35 1.55 0.24 0.07 92.01 97.46 3.76 33.28 8.58\nBigLM-24-tk(50)0.06 9.26 1.29 0.20 0.07 92.46 97.82 3.84 38.05 9.18\nBigLM-24-tk(500)0.06 7.34 0.84 0.15 0.05 93.59 98.45 4.47 47.90 9.52\nBigLM-24-tp(0.9)0.09 10.38 1.67 0.31 0.09 91.88 97.37 3.81 33.35 8.67\nTable 3: Experimental results on Douban dataset.\nModel Relevance Diversity LengthBLEU B-1 B-2 B-3 B-4 MADIST-1 M ADIST-2 M IDIST-1 M IDIST-2\nSeq2Seq 0.71 31.28 3.61 1.56 0.99 98.23 99.69 6.77 21.88 5.53\nCV AE 0.16 25.73 3.49 0.95 0.24 88.11 93.84 3.07 14.12 12.14\nMMI 0.10 23.69 2.68 0.73 0.19 96.92 98.46 2.24 8.73 8.96\nSeq2Seq-greedy 0.66 24.51 3.24 1.36 0.83 93.56 98.83 7.53 28.82 9.10\nSeq2Seq-bm(5) 0.71 31.28 3.61 1.56 0.99 98.23 99.69 6.77 21.88 5.53\nSeq2Seq-tk(5) 0.37 23.06 2.64 0.87 0.56 93.04 98.98 8.85 38.43 10.54\nSeq2Seq-tk(10) 0.09 22.29 2.17 0.54 0.09 93.58 99.30 9.56 42.48 10.97\nSeq2Seq-tk(20) 0.22 21.06 1.71 0.53 0.28 93.51 99.44 10.71 47.15 11.10\nSeq2Seq-tk(50) 0.14 20.71 1.82 0.41 0.14 93.74 99.56 11.83 52.02 11.12\nSeq2Seq-tk(500) 0.03 19.24 1.23 0.31 0.02 93.79 99.54 15.84 58.08 11.70\nSeq2Seq-tp(0.9) 0.16 21.50 1.99 0.48 0.11 93.39 99.47 10.41 46.65 10.94\nLM-12-greedy 0.18 18.97 2.34 0.51 0.15 73.35 84.89 1.64 8.47 21.87\nLM-12-bm(5) 0.21 24.13 3.21 0.83 0.26 85.16 92.41 2.35 11.05 13.61\nLM-12-tk(5) 0.09 19.35 1.82 0.28 0.06 83.68 96.22 1.82 13.83 21.65\nLM-12-tk(10) 0.05 18.35 1.48 0.20 0.04 85.70 97.60 1.90 16.43 22.16\nLM-12-tk(20) 0.03 17.37 1.27 0.16 0.02 87.08 98.22 2.04 19.49 22.46\nLM-12-tk(50) 0.03 16.48 1.06 0.12 0.02 88.45 98.75 2.37 23.60 22.49\nLM-12-tk(500) 0.02 15.34 0.80 0.08 0.01 90.36 99.32 3.58 33.84 22.19\nLM-12-tp(0.9) 0.05 17.42 1.26 0.16 0.04 87.02 98.21 2.05 19.44 22.36\nLM-12-greedy-Œª1 0.18 18.93 2.31 0.50 0.15 74.26 85.57 1.60 8.17 21.30\nLM-12-bm(5)-Œª1 0.21 23.26 3.07 0.80 0.26 85.27 93.22 2.10 9.62 14.38\nLM-12-tk(5)-Œª1 0.09 19.17 1.80 0.29 0.07 83.66 96.23 1.76 13.60 21.95\nLM-12-tk(10)-Œª1 0.06 18.28 1.54 0.22 0.04 85.70 97.53 1.86 16.26 22.30\nLM-12-tk(20)-Œª1 0.05 17.33 1.30 0.17 0.04 87.13 98.29 2.02 19.29 22.53\nLM-12-tk(50)-Œª1 0.03 16.25 1.04 0.13 0.02 88.40 98.76 2.33 23.37 22.71\nLM-12-tk(500)-Œª1 0.02 15.21 0.82 0.09 0.02 90.33 99.31 3.59 33.79 22.15\nLM-12-tp(0.9)-Œª1 0.04 17.31 1.26 0.16 0.03 87.12 98.24 2.01 19.18 22.58\nLM-12-greedy-Œª0.5 0.18 19.16 2.35 0.50 0.15 75.05 86.15 1.58 8.06 21.12\nLM-12-bm(5)-Œª0.5 0.22 23.51 3.13 0.81 0.27 84.93 92.81 2.16 9.94 14.24\nLM-12-tk(5)-Œª0.5 0.09 19.19 1.79 0.29 0.06 83.76 96.24 1.73 13.34 22.04\nLM-12-tk(10)-Œª0.5 0.05 18.14 1.47 0.20 0.03 85.61 97.54 1.83 16.12 22.60\nLM-12-tk(20)-Œª0.5 0.03 17.15 1.25 0.15 0.02 87.15 98.31 2.00 19.23 22.63\nLM-12-tk(50)-Œª0.5 0.03 16.39 1.06 0.12 0.02 88.41 98.74 2.32 23.29 22.83\nLM-12-tk(500)-Œª0.5 0.02 15.19 0.80 0.09 0.02 90.36 99.31 3.52 33.45 22.35\nLM-12-tp(0.9)-Œª0.5 0.04 17.30 1.26 0.16 0.03 87.14 98.24 1.98 19.16 22.65\nLM-24-greedy 0.15 18.04 2.14 0.44 0.11 71.42 82.96 1.14 5.93 22.66\nLM-24-bm(5) 0.20 22.67 2.88 0.72 0.23 83.17 90.72 1.53 7.48 14.65\nLM-24-tk(5) 0.08 19.38 1.80 0.27 0.05 83.66 96.10 1.47 11.79 21.43\nLM-24-tk(10) 0.06 18.22 1.46 0.20 0.04 85.56 97.38 1.58 14.44 21.83\nLM-24-tk(20) 0.03 17.34 1.19 0.13 0.02 87.27 98.17 1.75 17.54 21.85\nLM-24-tk(50) 0.02 16.43 1.05 0.11 0.01 88.67 98.80 2.07 21.81 22.02\nLM-24-tk(500) 0.02 15.33 0.77 0.07 0.01 90.46 99.28 3.34 32.64 21.54\nLM-24-tp(0.9) 0.04 17.35 1.21 0.15 0.03 87.18 98.21 1.72 17.39 21.97\nBigLM-12-greedy0.19 19.22 2.41 0.55 0.16 74.80 85.97 2.16 10.38 21.02\nBigLM-12-bm(5)0.21 24.18 3.32 0.86 0.25 85.81 93.01 3.55 16.11 13.37\nBigLM-12-tk(5) 0.07 19.60 1.82 0.28 0.05 84.13 96.19 2.27 15.61 21.25\nBigLM-12-tk(10)0.07 18.56 1.54 0.23 0.05 86.08 97.54 2.30 18.25 21.61\nBigLM-12-tk(20)0.05 17.63 1.31 0.17 0.03 87.46 98.21 2.43 21.17 21.67\nBigLM-12-tk(50)0.04 16.66 1.05 0.13 0.03 88.54 98.68 2.66 25.00 21.93\nBigLM-12-tk(500)0.01 15.57 0.82 0.08 0.01 90.38 99.16 3.72 34.18 21.40\nBigLM-12-tp(0.9)0.05 17.63 1.27 0.17 0.03 87.37 98.17 2.40 21.05 21.79\nBigLM-24-greedy0.20 18.68 2.36 0.53 0.15 73.15 85.16 1.97 9.65 22.24\nBigLM-24-bm(5)0.25 23.54 3.25 0.89 0.27 84.92 92.37 3.29 15.54 14.09\nBigLM-24-tk(5) 0.09 19.37 1.85 0.29 0.06 83.72 96.11 2.21 15.82 21.81\nBigLM-24-tk(10)0.06 18.42 1.57 0.22 0.04 85.35 97.34 2.25 18.35 22.34\nBigLM-24-tk(20)0.05 17.52 1.32 0.17 0.03 86.98 98.09 2.35 21.11 22.37\nBigLM-24-tk(50)0.04 16.63 1.13 0.15 0.03 88.33 98.61 2.64 25.17 22.38\nBigLM-24-tk(500)0.02 15.50 0.85 0.09 0.02 90.06 99.13 3.68 34.19 21.96\nBigLM-24-tp(0.9)0.04 17.56 1.35 0.18 0.03 86.98 98.10 2.35 21.14 22.29\nTable 4: Experimental results on Reddit dataset.\nModel Relevance Diversity LengthBLEU B-1 B-2 B-3 B-4 MADIST-1 M ADIST-2 M IDIST-1 M IDIST-2\nSeq2Seq 0.71 31.28 3.61 1.56 0.99 98.23 99.69 6.77 21.88 5.53\nCV AE 5.53 38.05 13.84 9.08 5.95 95.33 99.33 7.21 29.86 6.96\nMMI 11.91 35.53 16.31 13.55 12.07 92.78 96.03 12.30 50.78 8.36\nHRED 0.67 39.27 8.62 4.37 0.99 98.47 99.57 3.78 11.89 4.67\nVHRED 0.50 20.80 2.48 0.76 0.34 83.34 90.56 2.23 8.47 11.97\nVHCR 0.36 23.07 2.58 1.18 0.41 86.17 92.47 2.36 8.10 10.20\nSeq2Seq-greedy 0.66 24.51 3.24 1.36 0.83 93.56 98.83 7.53 28.82 9.10\nSeq2Seq-bm(5) 0.71 31.28 3.61 1.56 0.99 98.23 99.69 6.77 21.88 5.53\nSeq2Seq-tk(5) 0.37 23.06 2.64 0.87 0.56 93.04 98.98 8.85 38.43 10.54\nSeq2Seq-tk(10) 0.09 22.29 2.17 0.54 0.09 93.58 99.30 9.56 42.48 10.97\nSeq2Seq-tk(20) 0.22 21.06 1.71 0.53 0.28 93.51 99.44 10.71 47.15 11.10\nSeq2Seq-tk(50) 0.14 20.71 1.82 0.41 0.14 93.74 99.56 11.83 52.02 11.12\nSeq2Seq-tk(500) 0.03 19.24 1.23 0.31 0.02 93.79 99.54 15.84 58.08 11.70\nSeq2Seq-tp(0.9) 0.16 21.50 1.99 0.48 0.11 93.39 99.47 10.41 46.65 10.94\nLM-12-greedy 5.67 37.96 14.94 10.19 6.12 95.09 99.28 12.86 46.06 8.15\nLM-12-bm(5) 6.51 44.83 18.00 12.55 8.21 98.08 99.87 12.80 44.37 5.25\nLM-12-tk(5) 3.14 30.82 9.32 5.18 3.21 94.16 99.41 13.12 50.77 9.78\nLM-12-tk(10) 3.29 30.05 9.01 5.71 3.51 93.99 99.30 13.48 52.81 10.25\nLM-12-tk(20) 2.49 28.85 7.66 4.45 2.48 93.97 99.62 13.52 53.15 10.42\nLM-12-tk(50) 2.27 28.22 7.23 4.29 2.36 94.35 99.60 14.92 57.01 10.50\nLM-12-tk(500) 1.93 27.61 6.62 4.00 1.98 93.75 99.34 17.36 60.14 10.69\nLM-12-tp(0.9) 2.35 29.05 8.34 5.12 2.60 94.03 99.52 13.83 53.72 10.44\nLM-12-greedy-Œª1 6.21 39.56 16.48 11.55 7.11 94.76 99.23 12.60 46.27 8.31\nLM-12-bm(5)-Œª1 6.46 45.76 18.82 13.00 7.54 97.90 99.76 12.56 42.55 5.24\nLM-12-tk(5)-Œª1 3.49 31.20 9.24 5.78 3.37 93.64 99.40 12.86 50.89 10.38\nLM-12-tk(10)-Œª1 2.58 31.17 9.10 5.39 2.75 93.87 99.54 13.23 52.84 10.19\nLM-12-tk(20)-Œª1 2.20 28.51 8.59 4.75 2.22 93.48 99.39 13.63 55.13 11.01\nLM-12-tk(50)-Œª1 2.82 28.56 9.24 5.64 2.87 93.10 99.21 14.68 57.11 11.11\nLM-12-tk(500)-Œª1 2.43 28.38 7.94 4.78 2.46 93.79 99.55 17.16 60.29 11.22\nLM-12-tp(0.9)-Œª1 2.89 30.16 8.87 5.48 2.84 93.82 99.55 13.93 53.91 10.67\nLM-12-greedy-Œª0.5 6.24 37.54 15.06 10.71 6.79 94.09 99.23 11.78 43.82 8.72\nLM-12-bm(5)-Œª0.5 5.95 44.93 17.24 11.79 7.37 97.68 99.77 12.88 42.74 5.57\nLM-12-tk(5)-Œª0.5 2.76 30.49 8.47 4.84 2.81 93.10 99.23 12.51 49.73 10.53\nLM-12-tk(10)-Œª0.5 2.74 29.58 8.45 4.73 2.63 93.26 99.36 12.67 51.28 11.01\nLM-12-tk(20)-Œª0.5 2.37 27.97 7.03 4.25 2.38 93.58 99.52 13.31 53.43 11.25\nLM-12-tk(50)-Œª0.5 2.58 27.88 7.49 4.67 2.63 93.21 99.44 14.14 55.70 11.46\nLM-12-tk(500)-Œª0.5 1.91 26.12 6.37 3.68 1.96 93.34 99.51 16.58 59.99 12.09\nLM-12-tp(0.9)-Œª0.5 2.09 29.19 8.68 4.94 2.21 93.41 99.57 13.11 53.29 11.22\nLM-24-greedy 3.63 35.90 12.14 7.62 4.43 94.90 98.94 11.17 41.27 7.86\nLM-24-bm(5) 3.32 42.10 13.37 8.59 4.33 98.11 99.64 11.09 37.48 4.92\nLM-24-tk(5) 1.94 29.59 8.00 4.36 2.09 93.42 99.17 11.68 47.66 9.82\nLM-24-tk(10) 1.00 27.29 5.94 2.95 1.09 92.90 98.84 12.17 50.74 10.49\nLM-24-tk(20) 1.71 27.23 6.16 3.40 1.68 93.47 99.39 12.88 53.11 10.70\nLM-24-tk(50) 1.47 27.28 6.73 3.50 1.43 93.79 99.19 13.90 56.12 10.53\nLM-24-tk(500) 0.93 24.29 5.02 2.59 1.23 93.35 99.19 16.25 60.94 11.08\nLM-24-tp(0.9) 1.34 26.55 6.01 3.00 1.29 93.47 99.46 12.82 53.11 10.92\nBigLM-12-greedy6.27 34.61 11.79 8.33 6.97 94.19 97.32 13.32 48.13 8.72\nBigLM-12-bm(5)7.56 40.93 14.56 10.95 9.12 97.55 98.11 13.70 44.31 5.87\nBigLM-12-tk(5) 3.32 29.51 7.30 4.40 3.34 93.36 97.58 13.78 52.03 9.88\nBigLM-12-tk(10)3.16 28.31 7.30 4.46 3.26 93.49 97.95 14.68 55.37 9.71\nBigLM-12-tk(20)3.45 28.23 7.21 4.95 3.75 93.36 97.71 14.92 55.96 10.32\nBigLM-12-tk(50)3.15 28.09 6.88 4.27 3.33 93.22 97.78 15.32 57.74 10.02\nBigLM-12-tk(500)2.83 27.14 6.70 4.08 2.83 93.58 98.39 17.25 59.60 10.23\nBigLM-12-tp(0.9)3.67 28.25 7.48 5.19 3.75 94.07 99.42 14.68 56.40 10.27\nBigLM-24-greedy1.71 32.16 7.25 3.81 1.89 93.85 97.99 9.05 29.87 7.58\nBigLM-24-bm(5)1.34 37.38 7.72 3.65 1.84 99.11 99.73 8.06 23.85 4.84\nBigLM-24-tk(5) 0.62 27.11 4.36 2.05 0.82 93.61 99.09 11.37 43.66 9.74\nBigLM-24-tk(10)0.54 25.18 3.39 1.17 0.56 93.80 99.21 12.39 47.88 10.11\nBigLM-24-tk(20)0.62 24.70 3.83 1.89 0.65 93.48 99.43 12.67 51.53 10.83\nBigLM-24-tk(50)0.30 23.74 3.07 1.32 0.46 93.74 99.39 14.24 56.29 10.66\nBigLM-24-tk(500)0.19 21.93 2.93 1.04 0.19 94.28 99.57 16.28 61.74 11.33\nBigLM-24-tp(0.9)1.11 24.86 4.45 2.24 1.23 93.00 99.13 12.83 50.91 10.78\nTable 5: Experimental results on DailyDialog dataset.\nModel Relevance Diversity LengthBLEU B-1 B-2 B-3 B-4 MADIST-1 M ADIST-2 M IDIST-1 M IDIST-2\nSeq2Seq 0.11 14.85 1.31 0.26 0.09 97.42 99.90 6.71 24.71 7.89\nCV AE 0.04 17.20 2.04 0.28 0.04 95.22 98.71 4.65 20.80 8.48\nMMI 0.22 17.57 2.76 0.67 0.32 92.40 100.00 0.60 1.30 7.37\nHRED 0.30 21.71 3.71 0.99 0.36 94.55 97.74 1.71 4.43 8.12\nVHRED 0.33 18.44 2.83 0.90 0.28 87.96 95.31 1.57 4.90 9.99\nVHCR 0.30 17.32 2.43 0.65 0.21 91.16 98.15 1.89 6.18 11.10\nSeq2Seq-greedy 0.00 14.41 1.42 0.18 0.00 94.61 99.26 6.70 28.78 10.95\nSeq2Seq-bm(5) 0.11 14.85 1.31 0.26 0.09 97.42 99.90 6.71 24.71 7.89\nSeq2Seq-tk(5) 0.07 14.50 1.38 0.21 0.03 95.21 99.70 7.74 36.27 10.58\nSeq2Seq-tk(10) 0.16 13.75 1.49 0.35 0.11 95.03 99.61 8.72 40.45 10.56\nSeq2Seq-tk(20) 0.16 13.61 1.28 0.37 0.15 95.16 99.63 9.88 45.69 10.64\nSeq2Seq-tk(50) 0.02 13.32 0.91 0.08 0.01 95.57 99.68 10.91 49.35 10.58\nSeq2Seq-tk(500) 0.01 13.00 0.89 0.04 0.01 95.79 99.80 15.02 57.40 10.29\nSeq2Seq-tp(0.9) 0.07 14.04 1.30 0.17 0.06 95.34 99.63 9.49 44.72 10.67\nLM-12-greedy 0.35 19.14 3.73 0.85 0.23 91.22 98.56 9.68 37.12 10.84\nLM-12-bm(5) 0.21 19.94 3.44 0.71 0.16 96.67 99.61 9.56 34.09 7.72\nLM-12-tk(5) 0.27 17.46 2.74 0.76 0.25 93.45 99.18 10.68 44.71 10.98\nLM-12-tk(10) 0.25 16.56 2.17 0.47 0.16 94.05 99.49 11.23 47.16 10.74\nLM-12-tk(20) 0.16 15.97 2.24 0.51 0.16 94.16 99.50 12.15 50.40 10.89\nLM-12-tk(50) 0.15 15.26 1.72 0.28 0.10 94.22 99.48 12.59 52.14 10.91\nLM-12-tk(500) 0.25 14.88 1.74 0.41 0.17 94.53 99.61 16.16 56.93 10.78\nLM-12-tp(0.9) 0.15 16.72 2.18 0.36 0.09 94.08 99.36 11.63 49.34 10.78\nLM-12-greedy-Œª1 0.17 18.78 3.10 0.58 0.12 89.60 98.11 9.42 35.34 11.50\nLM-12-bm(5)-Œª1 0.76 20.57 4.10 1.39 0.66 96.07 99.64 9.12 33.29 8.15\nLM-12-tk(5)-Œª1 0.21 17.05 2.45 0.47 0.15 92.78 99.16 10.49 43.28 11.51\nLM-12-tk(10)-Œª1 0.23 17.20 2.43 0.58 0.27 93.36 99.39 11.11 46.89 11.30\nLM-12-tk(20)-Œª1 0.05 16.62 2.04 0.43 0.06 93.38 99.43 11.68 48.51 11.61\nLM-12-tk(50)-Œª1 0.15 15.78 1.83 0.45 0.13 94.05 99.49 13.15 51.80 11.27\nLM-12-tk(500)-Œª1 0.16 15.63 1.87 0.33 0.11 94.06 99.43 15.69 55.87 11.49\nLM-12-tp(0.9)-Œª1 0.08 16.40 2.15 0.28 0.06 93.29 99.48 11.69 48.04 11.52\nLM-12-greedy-Œª0.5 0.22 19.19 3.12 0.70 0.14 89.82 97.87 9.26 35.46 11.50\nLM-12-bm(5)-Œª0.5 0.30 20.80 3.45 0.92 0.22 95.83 99.34 9.58 34.31 8.22\nLM-12-tk(5)-Œª0.5 0.14 16.92 2.58 0.48 0.13 92.46 99.19 10.98 44.42 11.49\nLM-12-tk(10)-Œª0.5 0.09 16.33 1.93 0.41 0.16 92.95 99.02 10.84 46.77 11.46\nLM-12-tk(20)-Œª0.5 0.15 16.68 2.32 0.52 0.12 92.98 99.36 11.79 49.08 11.51\nLM-12-tk(50)-Œª0.5 0.07 16.32 1.83 0.30 0.04 93.95 99.38 13.09 53.65 11.41\nLM-12-tk(500)-Œª0.5 0.03 14.75 1.44 0.24 0.02 94.02 99.61 16.42 59.15 11.53\nLM-12-tp(0.9)-Œª0.5 0.10 16.46 2.10 0.29 0.07 93.42 99.36 12.18 50.98 11.29\nLM-24-greedy 0.35 19.14 3.73 0.85 0.23 91.22 98.56 9.68 37.12 10.84\nLM-24-bm(5) 0.21 19.94 3.44 0.71 0.16 96.67 99.61 9.56 34.09 7.72\nLM-24-tk(5) 0.20 17.77 2.71 0.66 0.18 93.29 99.22 10.80 44.20 10.90\nLM-24-tk(10) 0.26 16.38 2.21 0.61 0.19 94.11 99.40 11.13 46.97 10.93\nLM-24-tk(20) 0.17 15.79 1.91 0.44 0.10 94.05 99.42 11.74 49.72 11.10\nLM-24-tk(50) 0.21 15.59 1.97 0.45 0.19 94.43 99.58 13.30 53.02 10.79\nLM-24-tk(500) 0.11 14.71 1.67 0.37 0.07 94.27 99.46 15.79 57.61 10.82\nLM-24-tp(0.9) 0.20 15.94 2.12 0.49 0.17 93.71 99.46 11.69 49.60 11.07\nBigLM-12-greedy0.61 22.76 4.75 1.38 0.51 86.76 95.12 7.47 24.09 10.23\nBigLM-12-bm(5)0.90 24.95 5.90 2.23 1.02 96.60 99.24 7.73 22.96 7.46\nBigLM-12-tk(5) 0.43 20.02 3.55 0.93 0.36 92.11 98.59 9.10 35.54 10.84\nBigLM-12-tk(10)0.24 18.59 3.01 0.72 0.18 92.63 98.88 10.13 41.44 10.79\nBigLM-12-tk(20)0.27 18.48 2.71 0.68 0.19 93.69 99.37 10.98 46.45 10.64\nBigLM-12-tk(50)0.18 16.81 2.03 0.36 0.11 93.43 98.87 12.07 51.59 10.74\nBigLM-12-tk(500)0.16 15.86 2.10 0.46 0.10 94.38 99.08 14.90 58.31 10.67\nBigLM-12-tp(0.9)0.20 17.87 2.57 0.56 0.21 94.01 99.28 10.83 46.17 10.65\nBigLM-24-greedy0.75 23.37 5.15 1.51 0.58 87.06 95.36 7.56 24.42 10.22\nBigLM-24-bm(5)1.09 25.80 6.32 2.34 1.02 96.99 99.37 7.61 23.55 7.40\nBigLM-24-tk(5) 0.39 19.36 3.40 0.95 0.38 92.42 98.61 9.05 35.51 10.69\nBigLM-24-tk(10)0.22 18.84 2.74 0.66 0.17 93.40 98.98 10.18 42.14 10.66\nBigLM-24-tk(20)0.16 17.66 2.34 0.53 0.11 93.47 99.29 11.22 46.94 10.80\nBigLM-24-tk(50)0.15 17.03 1.93 0.49 0.11 93.96 98.92 12.09 51.98 10.65\nBigLM-24-tk(500)0.05 15.57 1.97 0.34 0.03 93.61 98.99 14.85 58.27 10.96\nBigLM-24-tp(0.9)0.11 17.61 2.46 0.44 0.07 93.26 99.05 10.24 46.21 10.94\nTable 6: Experimental results on Persona-Chat dataset.\nReferences\nDaniel Adiwardana, Minh-Thang Luong, David R So,\nJamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,\nApoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,\net al. 2020. Towards a human-like open-domain\nchatbot. arXiv preprint arXiv:2001.09977.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015.\nWei Bi, Jun Gao, Xiaojiang Liu, and Shuming Shi.\n2019. Fine-grained sentence functions for short-text\nconversation. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 3984‚Äì3993.\nZhangming Chan, Juntao Li, Xiaopeng Yang, Xiuy-\ning Chen, Wenpeng Hu, Dongyan Zhao, and Rui\nYan. 2019. Modeling personalization in continu-\nous space for response generation via augmented\nwasserstein autoencoders. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1931‚Äì1940.\nKyunghyun Cho, Bart van Merri ¬®enboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using rnn encoder‚Äìdecoder\nfor statistical machine translation. In Proceedings of\nthe 2014 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 1724‚Äì\n1734.\nHeriberto Cuay¬¥ahuitl. 2017. Simpleds: A simple deep\nreinforcement learning dialogue system. In Dia-\nlogues with social robots, pages 109‚Äì118. Springer.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171‚Äì4186.\nBhuwan Dhingra, Lihong Li, Xiujun Li, Jianfeng Gao,\nYun-Nung Chen, Faisal Ahmad, and Li Deng. 2017.\nTowards end-to-end reinforcement learning of dia-\nlogue agents for information access. In Proceedings\nof the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 484‚Äì495.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2018. Wizard\nof wikipedia: Knowledge-powered conversational\nagents. arXiv preprint arXiv:1811.01241.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. UniÔ¨Åed language\nmodel pre-training for natural language understand-\ning and generation. In Advances in Neural Informa-\ntion Processing Systems, pages 13042‚Äì13054.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi-\nerarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 889‚Äì898.\nJianfeng Gao, Michel Galley, Lihong Li, et al. 2019a.\nNeural approaches to conversational ai. Founda-\ntions and Trends R‚Éùin Information Retrieval , 13(2-\n3):127‚Äì298.\nJun Gao, Wei Bi, Xiaojiang Liu, Junhui Li, and Shum-\ning Shi. 2019b. Generating multiple diverse re-\nsponses for short-text conversation. In Proceedings\nof the AAAI Conference on ArtiÔ¨Åcial Intelligence ,\nvolume 33, pages 6383‚Äì6390.\nJun Gao, Wei Bi, Xiaojiang Liu, Junhui Li, Guodong\nZhou, and Shuming Shi. 2019c. A discrete cvae for\nresponse generation on short-text conversation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 1898‚Äì\n1908.\nMarjan Ghazvininejad, Chris Brockett, Ming-Wei\nChang, Bill Dolan, Jianfeng Gao, Wen-tau Yih, and\nMichel Galley. 2018. A knowledge-grounded neural\nconversation model. In Thirty-Second AAAI Confer-\nence on ArtiÔ¨Åcial Intelligence.\nXiaodong Gu, Kyunghyun Cho, Jung-Woo Ha, and\nSunghun Kim. 2018. Dialogwae: Multimodal re-\nsponse generation with conditional wasserstein auto-\nencoder. arXiv preprint arXiv:1805.12352.\nSepp Hochreiter and J ¬®urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735‚Äì1780.\nAri Holtzman, Jan Buys, Maxwell Forbes, and Yejin\nChoi. 2019. The curious case of neural text degener-\nation. arXiv preprint arXiv:1904.09751.\nChenyang Huang, Osmar R Zaiane, Amine Trabelsi,\nand Nouha Dziri. 2018. Automatic dialogue gener-\nation with expressed emotions. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 2 (Short Pa-\npers), pages 49‚Äì54.\nPei Ke, Jian Guan, Minlie Huang, and Xiaoyan Zhu.\n2018. Generating informative responses with con-\ntrolled sentence function. In Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n1499‚Äì1508.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nDiederik P Kingma and Max Welling. 2013. Auto-\nencoding variational bayes. arXiv preprint\narXiv:1312.6114.\nPhilipp Koehn. 2004. Pharaoh: a beam search de-\ncoder for phrase-based statistical machine transla-\ntion models. In Conference of the Association for\nMachine Translation in the Americas , pages 115‚Äì\n124. Springer.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016a. A diversity-promoting objec-\ntive function for neural conversation models. InPro-\nceedings of the 2016 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n110‚Äì119.\nJiwei Li, Michel Galley, Chris Brockett, Georgios Sp-\nithourakis, Jianfeng Gao, and Bill Dolan. 2016b. A\npersona-based neural conversation model. In Pro-\nceedings of the 54th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 994‚Äì1003.\nJiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky,\nMichel Galley, and Jianfeng Gao. 2016c. Deep rein-\nforcement learning for dialogue generation. In Pro-\nceedings of the 2016 Conference on Empirical Meth-\nods in Natural Language Processing , pages 1192‚Äì\n1202.\nJiwei Li, Will Monroe, Tianlin Shi, S ¬¥ebastien Jean,\nAlan Ritter, and Dan Jurafsky. 2017a. Adversarial\nlearning for neural dialogue generation. In Proceed-\nings of the 2017 Conference on Empirical Methods\nin Natural Language Processing, pages 2157‚Äì2169.\nQintong Li, Hongshen Chen, Zhaochun Ren, Zhumin\nChen, Zhaopeng Tu, and Jun Ma. 2019a. Emp-\ngan: Multi-resolution interactive empathetic dia-\nlogue generation. arXiv preprint arXiv:1911.08698.\nXin Li, Piji Li, Wei Bi, Xiaojiang Liu, and\nWai Lam. 2019b. Relevance-promoting language\nmodel for short-text conversation. arXiv preprint\narXiv:1911.11489.\nYanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang\nCao, and Shuzi Niu. 2017b. Dailydialog: A manu-\nally labelled multi-turn dialogue dataset. InProceed-\nings of the Eighth International Joint Conference on\nNatural Language Processing (Volume 1: Long Pa-\npers), pages 986‚Äì995.\nHugo Liu and Push Singh. 2004. Conceptneta practi-\ncal commonsense reasoning tool-kit. BT technology\njournal, 22(4):211‚Äì226.\nAndrea Madotto, Zhaojiang Lin, Chien-Sheng Wu, and\nPascale Fung. 2019. Personalizing dialogue agents\nvia meta-learning. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 5454‚Äì5459.\nPierre-Emmanuel Mazare, Samuel Humeau, Martin\nRaison, and Antoine Bordes. 2018. Training mil-\nlions of personalized dialogue agents. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 2775‚Äì2779.\nOluwatobi Olabiyi and Erik T Mueller. 2019. Multi-\nturn dialogue response generation with autore-\ngressive transformer models. arXiv preprint\narXiv:1908.01841.\nZhufeng Pan, Kun Bai, Yan Wang, Lianqiang Zhou,\nand Xiaojiang Liu. 2019. Improving open-domain\ndialogue systems via multi-turn incomplete utter-\nance restoration. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 1824‚Äì1833.\nYookoon Park, Jaemin Cho, and Gunhee Kim. 2018.\nA hierarchical latent structure for variational conver-\nsation modeling. In Proceedings of the 2018 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n1792‚Äì1801.\nLisong Qiu, Juntao Li, Wei Bi, Dongyan Zhao, and Rui\nYan. 2019. Are training samples correlated? learn-\ning to generate dialogue responses with multiple ref-\nerences. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 3826‚Äì3835.\nAlec Radford, Jeffrey Wu, Dario Amodei, Daniela\nAmodei, Jack Clark, Miles Brundage, and Ilya\nSutskever. 2019a. Better language models and\ntheir implications. OpenAI Blog https://openai.\ncom/blog/better-language-models.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019b. Lan-\nguage models are unsupervised multitask learners.\nOpenAI Blog, 1(8):9.\nHannah Rashkin, Eric Michael Smith, Margaret Li, and\nY-Lan Boureau. 2019. Towards empathetic open-\ndomain conversation models: A new benchmark and\ndataset. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5370‚Äì5381.\nAbigail See, Aneesh Pappu, Rohun Saxena, Akhila\nYerukola, and Christopher D Manning. 2019. Do\nmassively pretrained language models make better\nstorytellers? In Proceedings of the 23rd Confer-\nence on Computational Natural Language Learning\n(CoNLL), pages 843‚Äì861.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715‚Äì\n1725.\nIulian V Serban, Alessandro Sordoni, Yoshua Bengio,\nAaron Courville, and Joelle Pineau. 2016. Building\nend-to-end dialogue systems using generative hier-\narchical neural network models. In Thirtieth AAAI\nConference on ArtiÔ¨Åcial Intelligence.\nIulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,\nLaurent Charlin, Joelle Pineau, Aaron Courville, and\nYoshua Bengio. 2017. A hierarchical latent variable\nencoder-decoder model for generating dialogues. In\nThirty-First AAAI Conference on ArtiÔ¨Åcial Intelli-\ngence.\nLifeng Shang, Zhengdong Lu, and Hang Li. 2015. Neu-\nral responding machine for short-text conversation.\nIn Proceedings of the 53rd Annual Meeting of the\nAssociation for Computational Linguistics and the\n7th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers) , pages\n1577‚Äì1586.\nDavid So, Quoc Le, and Chen Liang. 2019. The\nevolved transformer. In International Conference on\nMachine Learning, pages 5877‚Äì5886.\nAlessandro Sordoni, Michel Galley, Michael Auli,\nChris Brockett, Yangfeng Ji, Margaret Mitchell,\nJian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015.\nA neural network approach to context-sensitive gen-\neration of conversational responses. In Proceedings\nof the 2015 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 196‚Äì205.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998‚Äì6008.\nOriol Vinyals and Quoc Le. 2015. A neural conversa-\ntional model. arXiv preprint arXiv:1506.05869.\nYu Wu, Wei Wu, Chen Xing, Ming Zhou, and Zhou-\njun Li. 2017. Sequential matching network: A\nnew architecture for multi-turn response selection\nin retrieval-based chatbots. In Proceedings of the\n55th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n496‚Äì505.\nMinghong Xu, Piji Li, Haoran Yang, Pengjie Ren,\nZhaochun Ren, Zhumin Chen, and Jun Ma. 2020.\nA neural topical expansion framework for unstruc-\ntured persona-oriented dialogue generation. arXiv\npreprint arXiv:2002.02153.\nZhen Xu, Bingquan Liu, Baoxun Wang, Cheng-Jie Sun,\nXiaolong Wang, Zhuoran Wang, and Chao Qi. 2017.\nNeural response generation via gan with an approx-\nimate embedding layer. In Proceedings of the 2017\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 617‚Äì626.\nTom Young, Erik Cambria, Iti Chaturvedi, Hao Zhou,\nSubham Biswas, and Minlie Huang. 2018. Aug-\nmenting end-to-end dialogue systems with common-\nsense knowledge. In Thirty-Second AAAI Confer-\nence on ArtiÔ¨Åcial Intelligence.\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur\nSzlam, Douwe Kiela, and Jason Weston. 2018a. Per-\nsonalizing dialogue agents: I have a dog, do you\nhave pets too? In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 2204‚Äì\n2213.\nYizhe Zhang, Michel Galley, Jianfeng Gao, Zhe Gan,\nXiujun Li, Chris Brockett, and Bill Dolan. 2018b.\nGenerating informative and diverse conversational\nresponses via adversarial information maximization.\nIn Advances in Neural Information Processing Sys-\ntems, pages 1810‚Äì1820.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\nLiu, and Bill Dolan. 2019. Dialogpt: Large-scale\ngenerative pre-training for conversational response\ngeneration. arXiv preprint arXiv:1911.00536.\nTiancheng Zhao, Ran Zhao, and Maxine Eskenazi.\n2017. Learning discourse-level diversity for neural\ndialog models using conditional variational autoen-\ncoders. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 654‚Äì664.\nYinhe Zheng, Rongsheng Zhang, Xiaoxi Mao, and\nMinlie Huang. 2019. A pre-training based personal-\nized dialogue generation model with persona-sparse\ndata. arXiv preprint arXiv:1911.04700.\nHao Zhou, Minlie Huang, Tianyang Zhang, Xiaoyan\nZhu, and Bing Liu. 2018a. Emotional chatting ma-\nchine: Emotional conversation generation with in-\nternal and external memory. In Thirty-Second AAAI\nConference on ArtiÔ¨Åcial Intelligence.\nHao Zhou, Tom Young, Minlie Huang, Haizhou Zhao,\nJingfang Xu, and Xiaoyan Zhu. 2018b. Com-\nmonsense knowledge aware conversation generation\nwith graph attention. In Proceedings of the 27th\nInternational Joint Conference on ArtiÔ¨Åcial Intelli-\ngence, pages 4623‚Äì4629.\nLi Zhou, Jianfeng Gao, Di Li, and Heung-Yeung Shum.\n2018c. The design and implementation of xiaoice,\nan empathetic social chatbot. Computational Lin-\nguistics, (Just Accepted):1‚Äì62.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE inter-\nnational conference on computer vision , pages 19‚Äì\n27.\nQuery Why do you like the ocean? Rank Scores\nResults i do n‚Äôt know how to feel about it , but my mouth is full of\nseaweed .\n0.3248\nyou ‚Äôll never know , but i ‚Äôd love to see the ocean at some\npoint in the future .\n0.2848\ni like the ocean , i ‚Äôll never live like that . 0.2548\nno , i ‚Äôm a girl , i enjoy the ocean ! 0.2472\nif you ever want a boat that is n‚Äôt a Ô¨Åsh , i ‚Äôd be glad to give\nyou one !\n0.2421\nit ‚Äôs really not that great . i have plenty of space , but it ‚Äôs\nlike 10 feet deep .\n0.2399\nthat was my favorite part . 0.2363\nit ‚Äôs a very nice place to live in anyways 0.2244\ni actually like the ocean , i thought it was pretty cool and\nthe atmosphere was a pretty good experience .\n0.2233\nlol ! i do not , but it was a good thing i did n‚Äôt drink and\nplay video games .\n0.2206\ni ca n‚Äôt really describe what i like on earth besides the ocean\n. and it ‚Äôs so clean , it ‚Äôs nice .\n0.2202\ni really like that ocean , i like the water . 0.2151\nyes , i agree . that ‚Äôs why i like a boat . 0.2111\nthere ‚Äôs no ocean . you need to get a ship . 0.2054\ni can understand sea . as long as water is in contact with the\nsurface , it ‚Äôs not a problem .\n0.2022\nlol , well , i ‚Äôm not sure what to believe in . they all die at\nthe end .\n0.2012\nyou mean the ocean ? you should stop living on the ocean ! 0.1958\nno , the water is so warm ! 0.1851\ni like the ocean , just i enjoy drinking the salt from it . 0.1809\ndo you want this to be true or something ? 0.1790\nwhat is the sea water ? 0.1757\nthe ocean is the coolest i can get . and the ocean . 0.1643\nnot anime , but i really like it . especially the way the water\nturns .\n0.1640\ni do too . the ocean is not a ocean . the ocean exists for the\npeople and for all us .\n0.1548\nlol , dude ? lol i like it there ! 0.1527\nthe ocean is amazing for water management purposes , you\nmight want to go out in the open and swim .\n0.1496\nhe just likes Ô¨Åsh . 0.1348\nyes , i always liked the sun ! 0.1346\nyeah xxxx that noise . 0.1303\nyeah but how else would a sea be a sea ! 0.1275\nso jealous of america ‚Äôs coast 0.1065\nis a lake good ? 0.1015\nTable 7: Single-turn Reddit-Bot example of parallel response generation and ranking. Batch-size is 32.\nQuery What is the meaning of life ? Rank Scores\nResults life is the meaning of death . 0.3715\nthe meaning of existence is to live , and the meaning of life\nis to live .\n0.3565\nit ‚Äôs a way of life where you do n‚Äôt lose everything when\nyou do n‚Äôt lose everything .\n0.3303\nwhat is life ? what is a word ? 0.3198\nlife is meaningless , it does n‚Äôt matter what makes you\nhappy\n0.3159\ndeath is a meaning of life . death is a form of life . life is\ndeath .\n0.2996\ndeath is meaningless . the meaning of death is n‚Äôt lost on\nyou .\n0.2847\nlife is life . it ‚Äôs fun . 0.2784\nlife ‚Äôs a joke , yo . 0.2418\nlife is not an act of god . 0.2389\nthe meaning of life : it ‚Äôs always your thing to do and then\nthere ‚Äôs your thing to do .\n0.2376\ni guess my question is do you have a good enough reason\nto think that life is meaningless ?\n0.2336\ni ‚Äôm going to need context . i ‚Äôm a girl . 0.2334\nit ‚Äôs for your life . it ‚Äôs for your money ! 0.2279\nlife is in the sky , and i ‚Äôm born out of this . 0.1988\ni know that . i feel bad for him . 0.1972\nthe ending is n‚Äôt very clear to me . it ‚Äôs very easy to die , it\n‚Äôs pretty easy to go through the effort of trying to live that\nkind of life after having had your life ruined .\n0.1960\nthe same way it ‚Äôs a good thing you ‚Äôve made a bad decision\nsince you were born\n0.1894\ni like my life more than my family , so it really is . 0.1879\nthat ‚Äôs a great answer ! life is what it is , to make you feel\nalive .\n0.1855\nit makes that meaning meaningful for everybody . i think\nthe fact that the meaning is that no one gives a xxxx about\nanyone else does n‚Äôt help .\n0.1839\nlife . the universe is what we call life now . 0.1793\nlife is just a thing you do when you are tired . a part of you\nis not born to love or love the person .\n0.1730\nyou can Ô¨Ånd it on google . 0.1727\ndo n‚Äôt feel bad . everyone on reddit and everywhere is\nalways saying about you .\n0.1639\nit is life - in the way that it is intended for us , which is why\ni would like it to remain .\n0.1625\nis it like a fantasy or a fantasy of nothing ? 0.1560\ni am so happy i can read this sub ! 0.1407\nnothing . people die . 0.1359\ndeath and destruction ! 0.1304\ni ‚Äôd be kind of drunk at dinner and eat my own food . 0.1268\njust one day when we Ô¨Ånally Ô¨Ågure out where life is and the\nfuture lives us\n0.1000\nTable 8: Another single-turn Reddit-Bot example.\nQuery Coronavirus symptoms: what are they and should I see a doctor? Rank Scores\nResults what do you mean by symptom ? 0.3381\nthere is no pain . it ‚Äôs not a sign of adhd , it ‚Äôs a sign of bad hygiene . 0.2909\nthey are n‚Äôt vaccinated . 0.2846\nthis is n‚Äôt one of the symptoms i ‚Äôm getting . not sure if that ‚Äôs a thing\nin the states , as i do n‚Äôt know if it ‚Äôs true in the uk .\n0.2713\nwhat kind of virus is that ? 0.2666\nthank you for clarifying , i will give them a try ! 0.2531\nwhat ‚Äôs the reasoning for removing that ? i could n‚Äôt Ô¨Ånd it on reddit . 0.2518\nnot to mention , it takes a long time to heal , so the only way to get\npast it is to eat a whole loaf of bread .\n0.2266\nthat would be my best guess . i ‚Äôm a little scared since i ‚Äôm going to\nsleep , but the more i think about it the less likely it is to spread to me\n0.2248\ni just checked , and i do n‚Äôt see the symptoms . i ‚Äôll give pills ! 0.2196\nyeah , that ‚Äôs what i was saying . i do n‚Äôt feel like he gave a shit about\nus being stuck on the skin for all of a sudden .\n0.2189\nthat does n‚Äôt work either : p 0.2142\ni do n‚Äôt see my own face when i look at her . 0.2130\nthe Ô¨Årst thing that popped into my head ! i was just about to say : o 0.2056\na doctor that does n‚Äôt need to go to the right subreddit for this to\nhappen . this should be covered .\n0.2006\ni ‚Äôll see that doc and see what they think . 0.2000\nyeah it ‚Äôs a virus , it ‚Äôs a bit of an oddity but it ‚Äôll cure itself once i get\nhome !\n0.1990\njust because one person is iskoron , does not mean that this is a normal\nvirus .\n0.1914\nare you referring to a virus or the fact that it is an ingame streaming\nmachine ? or are you referring to the fact this is a server - side exploit\nand not in real life ?\n0.1870\nno , that ‚Äôs a really serious infection . 0.1808\nthe thing that confuses me is there is no sign of an adderall sign but it\nseems like a pretty common sign .\n0.1770\nis this in - game or in the ofÔ¨Åcial store ? in - game or in the steam\nstore ?\n0.1768\ni am not sure about how it ‚Äôs supposed to work , or if it is the cause ,\nbut they do say that you need to use oem infused medicine .\n0.1753\nthe symptoms are usually not related to the condition . 0.1724\nhow can a person be that stupid for not knowing that they are having\nany symptoms ?\n0.1605\ni think the medicine ‚Äôs to slow down the stuttering . 0.1587\ni was just wondering how it feels . 0.1503\nwhat is causing the malfunction ? i see nothing . 0.1480\nyeah , its the worst , i could n‚Äôt get my eyes and brain to accept my\nown disease . if i get one , they look at me like some kind of god -\nlike angel .\n0.1428\ni need this checked my eyes . please tell me how this is caused so i\ncan tell .\n0.1342\nit is an anti virus . the symptoms of their are different . 0.1325\ni had the same problem , just a friend sent me this error once so it\nmay be wrong .\n0.1204\nTable 9: One more single-turn Reddit-Bot example.\nZm2`v ‰ªäÂ§©Â§©‚Ωì‰∏çÈîô _\u001cMF a+Q`2b\n_2bmHib ‰Ω†‰πüË¶ÅÊ≥®ÊÑè‚æù‰ΩìÂëÄ„ÄÇ yXjRj3\nÂåó‰∫¨Â§©‚Ωì‰∏çÈîôÁöÑËØ¥„ÄÇ yXjRRy\nÂ•Ω‰πÖÊ≤°ÁúãÁêÉ‰∫ÜÔºåÂæàÂñúÊ¨¢‰Ω†ÔºåÂä†Ê≤πÔºÅ yXjy33\nÂ•ΩÂ•Ω‰ºëÊÅØÔºåÂä†Ê≤πÂì¶ÔºåÂä†Ê≤πÂì¶ÔºÅ yXjyek\nÊó©ÂÆâÔºÅÊó©‰∏äÂ•ΩÔºÅÁæé‰∏ΩÁöÑÂåó‰∫¨ÔºÅ yXk3k3\nÂ•ΩÁæéÁöÑÂ§©‚ΩìÂïäÔºåÁúãÁùÄÁúºÁú∂ÈÉΩÊπøÊ∂¶‰∫Ü„ÄÇ yXkej9\nÁúãÂà∞‰Ω†‰∫ÜÔºåÊàë‰πüÊÉ≥ÁúãÂà∞‰Ω† yXk8jy\n‰Ω†Á°ÆÂÆöËøô‰∏çÊòØÊãçÁöÑÔºü ÔºÅ yXk8kj\nÁúüÊ≠£ÁöÑÂ§©‚Ωì‰∏çÈîôÔºåÂ•ΩÂ§©‚Ωì yXkjej\nËøôÊòØÂ§©‚ΩìÈ¢ÑÊä•‰πàÔºüÁúüÁöÑÊòØÁúüÁöÑ„ÄÇ yXkjek\nÂú®Âåó‰∫¨Â§©‚ΩìÂ¶Ç‰ΩïÂëÄÔºü yXkkkk\nÁúüÊòØÂæàÂñúÊ¨¢‰Ω†ÂíåÈòøËÅîÁöÑ‰ΩúÂìÅÔºå‚ΩÄÊåÅ‰Ω†ÔºÅ yXkky8\nÂ§©‚Ωì‰∏çÈîôÂêóÔºåËøòÊòØÂ§©‚ΩìÂ∑ÆÔºÅ yXkRyd\nËøôÂº†ÁÖß‚ΩöÂ•ΩÁæéÂëÄÔºå‰∏çËøáÊòØ‰Ω†ÊãçÁöÑ‰πàÔºÅ yXkRy8\n‰ªäÂ§©ÊàëÂú®Âé¶Èó®ÂºÄÊºîÂî±‰ºöÔºå Ôºå Ôºå yXkyj3\n‚º§ÂÆ∂Ôºå‰ªäÂ§©‚ΩìË¥®Â•ΩÔºÅÂ•ΩÂ•Ω‰ºëÊÅØÂ•ΩÂ•Ω‰ºëÊÅØÔºÅ yXRN89\nÊòØ‰ªÄ‰πàÊó∂ÂÄôËÆ©Â§©‚ΩìËøô‰πàÂ•Ω yXRNRj\nÊàë‰πüÁªèÂ∏∏Âú®‚ºØ‰ΩúÁöÑÔºåÂìàÂìàÂìà yXRNyN\nÊàë‰ªäÂ§©‰∏äÁè≠‰πüÂú®ÊãçÁÖßÔºåÂ§©‚ΩìË∂ÖÂ•ΩÂïäÔºåÊôö‰∏äËøòÂæóÂä†Ê≤πÔºåÂìàÂìà yXR39e\n‰ªäÂ§©‰∏ã‰∫Ü‚æ¨ÔºåÂ•ΩÂÜ∑ yXRdNe\nÊòØÂïäÔºÅÊàëÊò®Â§©ÂàöÊ¢¶ËßÅ‰∫ÜÔºÅ yXRdyy\n‰Ω†‰ª¨ÁöÑÊÑèÊÄùÊòØÊãçÊàèÔºü yXReNR\nÂú®Â§©Ê¥•Âì™Êù•ÁöÑËøô‰πà‚º§ÂïäÔºåÂìàÂìà yXRed9\n‰Ω†Â•ΩÔºÅ‰Ω†Áü•ÈÅìÊàë‰ªäÂ§©ÂêÉ‰∫ÜÊ≤°Ôºü yXReey\nÊòØ‰ªÄ‰πàÂìÅÁßçÔºü‚ºÄÁõ¥ÂæàÊºÇ‰∫Æ yXReRj\n‚ºÄ‰∏ãÈõ™Ôºå‚ººÊÉÖÂ∞±Â•Ω‰∫ÜÂë¢ yXR83k\nËøòÊúâËøôÁßçÊÑüËßâÂïäÔºÅ yXR8dj\nÊàë‰∏çËÆ§ËØÜËøôÊòØÂì™‚ºÄ‰ΩçÂì¶Ôºü yXR983\nÊÄé‰πà‰ºö‚ºÄÁÇπ‚ΩìÈÉΩÊ≤°ÊúâX ÊúâÁ©∫Êù•‰∏äÊµ∑Âïä yXRk3y\n‚Ωº‰ªªÔºå‰Ω†ËøòÁúüÂ§ü k ÁöÑ yXRRNR\n‰Ω†Â∞±Ë¶ÅÂõûÂ§¥‰∫ÜÂìàÂìàÊúâÊ≤°ÊúâÔºÅ yXyN3N\n‰Ω†‰∏çÈîôÔºåÂ•ΩÂÉèÂ§©‚ΩìËøòÊå∫Ê≠£ÂÆóÔºå yXy3d9\nR\nTable 10: Single-turn Weibo-Bot example of parallel response generation and ranking. Batch-size is 32.\nZm2`v ËøôÊòØ‰ªäÂ§©Âú∞ÈúáÁöÑÈúá‰∏≠Âõæ _\u001cMF a+Q`2b\n_2bmHib ÊàëËßâÂæóÊàë‰ª¨Ëøô‚æ•ÂæàÊòéÊòæÊÑüËßâÂà∞Âú∞Èúá‰∫Ü yXjkkk\nÊÑüËßâÂà∞‰∫ÜÂú∞ÈúáÔºåÁúüÁöÑ„ÄÇ yXk3Ny\nÈÇ£ÂæóÈúÄË¶ÅÂ§ö‚º§ÁöÑÂãá‚ΩìÂïäÔºÅ yXk3ye\nÊàë‰ª¨Ëøô‚æ•ÁöÑÂú∞Èúá‚ΩêËæÉ‰∏•Èáç„ÄÇÂæà‰∏•Èáç„ÄÇ yXkd9e\nÂæàÊòéÊòæÊúâ 8X8 Á∫ßÔºåÊòéÊòæÊòØ 9X8 Á∫ß yXkedN\nÊàëÂàöÊâç‰πüÊÑüËßâÂà∞‰∫ÜÂïä„ÄÇÂ•Ω‰∏•ÈáçÂïä„ÄÇ yXkedk\nÊàë‰ª¨ËøôÊúâÊòéÊòæÈúáÊÑüÁöÑ‰πàÔºåÊÑüËßâÂà∞‰∫Ü„ÄÇ yXk88R\nÂú∞ÈúáÊúâ‚ºèÊ¨°ÔºåÊúâÊ≤°ÊúâÊÑüËßâÔºü yXk9N8\n‰ªäÂ§©ÁöÑÂú∞ÈúáÈúá‰∏≠Âõæ‰∏çÊòØËøôÊ†∑ÂêóÔºü yXk939\nËøô‰πàÊòéÊòæÁöÑÊôÉÂä®ÂíåÈúáÊÑüÔºÅ yXkjNj\nÈÇ£ÊàëÁúüÁöÑÊ≤°ÊúâÊÑüËßâÂà∞Âë¢„ÄÇ yXkkdd\nÊòØËØ¥Âú∞ÈúáÂâç‚ºÄ‚º©Êó∂ËøòÊ≤°ÊúâÊÑüËßâÂà∞Âêó„ÄÇ yXkkyk\nÊúâÁÇπÂêì‚ºàÂïäÔºÅÊòØÂú∞Èúá‰∫ÜÂë¢ËøòÊòØÊòØË¢´Èúá‰∏≠‰∫ÜÂë¢ yXkRjj\nÊúâÈúáÊÑüÂêóÔºüËøôÊòØÁ•ûÁªèÁóÖÂêßÔºÅ yXky3y\nÂ•ΩÊÅ∂‚ººÂú∞ÈúáÔºå‰Ω†‰ª•‰∏∫Âú∞Èúá‰∫ÜÔºü yXRNd9\nÂ•ΩÊúâÊÑèÊÄùÔºÅÈúáÂêé‚ºÄÁßíÊâçÁü•ÈÅìÂú∞ÈúáÂéüÊù•ÂèØ‰ª•ËøôÊ†∑ yXR3dk\nËøôÁßçÁÖß‚ΩöÊòØÊúâÂ§ö‚ΩÜËÅä yXR38N\nÊàëÂú® kN Ê•ºÔºåËøòÊúâÂæà‚º§ÔºåÊÑüËßâÂà∞‰∫ÜÔºå‰ΩÜÂè™ÊúâÂú®Êàø‚º¶Â∫ï‰∏ã yXR3yR\n‰ªäÂ§©ÊàëÊÑüËßâÂà∞‰∫ÜXXX yXRd3y\nÂæàÊ≠£Á°ÆÔºåÂú®Âú∞Èúá‰πãÂâçÔºå‰Ω†Â∫îËØ•ÂÖàÊÑüÂèóÊÑüÂèóÂú∞Èúá„ÄÇ yXRd8d\nËøôÂõæÊ≤°Âï•ÔºåÂú®Âåó‰∫¨‰∏çÊòØÂ∑≤ÁªèÊ≤°Âú∞Èúá‰∫Ü„ÄÇ yXRdy9\nÂú∞ÈúáÁöÑÊÑüËßâÂ∞±ÊòØÂú®‚Ωî‚æ• yXRe3N\nÂìáÂìáÂìáÂìáÔºÅÈúáÁöÑÂú∞‚ΩÖÊ≤°Âä®ÈùôÔºÅ yXR889\n‰Ω†ÊÉ≥ËØ¥ËØ¥„ÄÇÊ≤°‰∫ã„ÄÇÂú∞ÁêÉÊòØ‰∏çÊòØË¶ÅÁàÜÁÇ∏‰∫Ü„ÄÇ yXR8yN\n‰ªäÂ§©ÊòØÂì™‚ºâÂïäÔºü‚º§Ê¶ÇÊúâ RN99 Âπ¥ÁöÑ yXR939\nËøô‰πàÊòéÊòæÔºüÊàëËøô‚æ•Êúâ d Á∫ß yXR9ey\nÊòéÊòæÊÑüËßâÊòØÂú®‚ºÄÊ†πÁª≥‚º¶ yXRj9y\nÂú∞ÂùÄÂú∞ÂùÄÊòØÂú®Âì™‚æ•Âïä‰∫≤ÔºÅ yXRjy8\nÊàëÂè™ËÉΩËØ¥Ôºö‰∏çË¶ÅËÆ©ÊàëÈúáÊíº yXRkd9\nÊúâÈúáÊÑüÔºåÁúã‰∏çÊòØÈÇ£ÂùóÂú∞ÈúáÁöÑÂéüÂõ† yXRk8k\nÂú®Ê≤≥ÂçóÊúâÊÑüËßâÂêóÔºü‚º§Ê¶Ç yXRk9j\nÂ•ΩÊÉ≥ÊääÂ§¥‰πüÂüã‰∫ÜÂ•ΩÂêó yXy3ee\nR\nTable 11: Another single-turn Weibo-Bot example.\nZm2`v ‰∏ñÁïå‚ºàÊ∞ëÂõ¢ÁªìËµ∑Êù•ÂÖ±ÂêåÊäóÂáªÊñ∞ÂÜ†Áñ´ÊÉÖÔºÅ _\u001cMF a+Q`2b\n_2bmHib ‰∏çË¶ÅÂøòËÆ∞‰∏≠ÂõΩÁöÑ‚ºàÊ∞ëÂõ¢ÁªìËµ∑Êù•ÂÖ±ÂêåÊäóÂáªÊñ∞ÂÜ†Áñ´ÊÉÖÔºÅ yXjN8e\nËøô‰∏™ÂøÖÈ°ªËΩ¨ÔºÅÂÖ±ÂêåÊäóÂáªÊñ∞ÂÜ†Áñ´ÊÉÖÔºÅ yXj9ed\nÁúãÂà∞ËøôÊù°ÂæÆÂçöÔºåÊàëÂ∞±ÊÉ≥Ëµ∑‰∫ÜÊàëÂÆ∂‰∏â‰ΩçÈ¢ÜÂØºÁöÑ‚Äú‚æ∞ÂëΩÂ∞öÊú™ÊàêÂäüÔºå ‚Äù yXkjN3\nÂìàÂìàÔºå‰Ω†‰ª¨ÈÉΩÂ•ΩÂéâÂÆ≥ÂëÄÔºåÂä†Ê≤πÂïä„ÄÇ yXkk9d\nËøôÊòØÁúüÁöÑÔºåËøòÊòØÂÅáÁöÑÔºåÊúâÈí±Â∞±ËÉΩÊäóÔºü yXkkke\nÊàëÊì¶„ÄÇÊàëÂãí‰∏™ÂéªÔºåÂ§™Á•ûÂ•á‰∫Ü yXkkRk\nËøô‰Ωç‚º§ÂßêÊòØÊÄé‰πàÊÉ≥Âá∫Êù•ÁöÑ yXkRek\nÂìàÂìàÔºåËøô‰∏™ÁúüÁöÑÊòØÂæàÁªèÂÖ∏ÔºåÁúã‰∏çÊáÇ yXkR89\nÁúüÁêÜÊÄªÊòØÂú®‰∏çÊñ≠Âú∞Âú®‰∏çÊñ≠Âú∞Âº∫ÂåñÔºÅ yXkR9d\nÁúãÂæóÊàëÈÉΩËõãÁñºÔºÅËøòÂ•ΩËøòÂ•ΩÔºÅ yXkyye\nÊúÄÂêé‚ºÄ‰∏™ÊàëÁü•ÈÅìÊòØÊúâÂ§ö‚ΩÜËÅä‰∫ÜÂìàÂìà yXkyyR\n‰Ω†ÊÉ≥ËØ¥Âì™‚ºÄ‰∏™ÔºüËØ¥Â§ö‰∫ÜÈÉΩÊòØÊ≥™ÔºÅÊÉ≥ËØ¥‰ªÄ‰πàÂë¢ÔºüËØ¥ËØ¥‰ªÄ‰πàÔºÅ yXRNN3\nÊñ∞ÂÜ†ÊòØ‰∏çÊòØËøô‰∏™ÁêÜÁî±Âïä„ÄÇ yXRNN9\n‰∏ç‰ºöÊòØ‚æÉ‚º∞ÊâìÁöÑÂêß„ÄÇ ÔºÅ yXRN9j\nËøô‰∏™ÂèØ‰ª•Êúâ„ÄÇ‚º§ÂÆ∂ÂèØ‰ª•ÂÜçÊù•‚ºÄÊ¨°Âë¢ yXRNke\n‰∏≠ÂõΩÊúÄËøë‚ºÄÁõ¥ÈÉΩÂú®ÂÅöËøôÁßç‰∫ãÔºåÈÉΩÊòØÂõ†‰∏∫‚ºàÊ∞ëÂ∏Å‰∏äÂá∫Áé∞ÁöÑÂéüÂõ†„ÄÇ yXR3NN\nÂõΩÂÆ∂‰πüÊòØ‚ºÄ‰∏™‰∫â‚ΩìÁöÑ‚º§ÂõΩ yXR3eN\nË¶ÅÊòØ‚ºÄÁÇπÈÉΩ‰∏çÂ•ΩÁ¨ëÂ∞±Â•Ω‰∫ÜÔºåÊàëÊÉ≥ÈóÆ‰∏ãËøôÊòØÂú®Âì™‚æ•Ôºü yXR3R9\nÂõΩÂÆ∂ÂæàÂèØËÉΩÔºåÊñ∞‚ºÄËΩÆÁöÑÊäóÊàòËøòÊ≤°ÁªìÊùü yXR3y3\nËøôËøòÁúüÊòØ‚ºÄÊ†∑„ÄÇÂ•Ω‚º§ÁöÑ‰ªáÊÅ®Âïä„ÄÇ yXRdRy\nÊ≤°Êúâ‰∫ÜËøô‰∏™È¢ÜÂØºÔºåÊòØË¶ÅÊÄéÊ†∑Âë¢Ôºü yXRed9\nÈÇ£ËøòËÉΩÊäóÊãíÂà´‚ºàÁöÑ‚ÄúÂº∫ÁÉàÊäóÊãí‚ÄùÔºü yXRe8e\n‚º§ÊàòÂïä‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‰∏çËøáËøòÊòØÊúâ‰∫õÊäóËÆÆ yXRejy\nËøôÂ∞±ÊòØÂõΩÂÜÖÁöÑÊäóÂáªÂÆ£‰º†‚ΩöÔºåÂ•ΩÂÉèÊõ¥Ë¥¥‚ºº‰∫õÔºÅ yXR8d8\nËøô‰Ωç‚º§Âèî„ÄÇÂ•ΩÂèØÁà±ÂëÄÔºÅ yXR8jy\nËøôÁÆóÊòØÊñ∞Âπ¥Á§ºÁâ©‰∫Ü„ÄÇÂ§™ÊúâÁà±‰∫Ü yXR9N9\n‚ºàÊ∞ëÁæ§‰ºóËÉΩÂøçÂæó‰ΩèÂøçÂæó‰∏ç‰ΩèÂêó yXRjNj\n‰Ω†ÂèØ‰ª•ÂΩì‚Äú‚ºÄ‰∏™Êâì‚øÅÔºå‚ºÄ‰∏™‰øùÁ•û‚ÄùÁöÑ‚Äú‰øùÂç´‰∏ñÁïå‚ÄùÁöÑ‚º§Â§öÊï∞‚ºà yXRkR9\nËøô‰∏çÁßëÂ≠¶„ÄÇÊúâÁöÑ‚ºàÁúãÊáÇ‰∫ÜÔºå‚ΩÜ‚ºÄÁâ©ËØ≠„ÄÇ yXRR3k\nËøô‰∏™ÂøÖÈ°ªÈ°∂È°∂Ôºå‰ª•ÂêéÊúâÊñ∞‚ºà‚Ω§Êâì‰ªóÂ∞±‚æèÂï¶ yXRReR\n‰∏çÂèØËÉΩÂÖ®‰∏∫ÊàëÂõΩÊâÄÂÅöÁöÑÂêßÔºÅ yXRR9N\nËøô‰∏™ÊòØ‰∏çÊòØËÉΩËÆ©Êñ∞‰∏≠ËçØ‚º§ÊàòÂõΩÂ§ñ‚º§ÊàòÂïäÂìàÂìà yXRyj3\nR\nTable 12: One more single-turn Weibo-Bot example.\nSeed Query hello\nResults Daily-Bot-A: hello\nDaily-Bot-B: hello , this is steven . i‚Äôm calling to conÔ¨Årm the date of my\nappointment .\nDaily-Bot-A: it‚Äôs july 5th .\nDaily-Bot-B: congratulations ! i hope you can make it as soon as possible .\nDaily-Bot-A: i‚Äôm afraid so . i have an appointment at three pm on july 10th .\nDaily-Bot-B: ok , i will see you then .\nDaily-Bot-A: very well , thank you for your co-operation .\nDaily-Bot-B: i am looking forward to hearing from you .\nDaily-Bot-A: good-bye .\nDaily-Bot-B: see you then .\nTable 13: An example of interactions between two multi-turn DailyDialog-Bots.\nSeed Query hello\nResults Daily-Bot-A: hello\nDaily-Bot-B: hello , this is lucy from abc company . i ll be glad to meet you .\nDaily-Bot-A: good to meet you , lucy .\nDaily-Bot-B: i m calling to inform you that we have arranged an interview for\nthis accountant position at 2 pm this thursday afternoon . please come on time .\nDaily-Bot-A: ok , thank you .\nDaily-Bot-B: you are welcome .\nDaily-Bot-A: i hope i can apply for the position as soon as possible .\nDaily-Bot-B: yes , please . please send your resume to our department .\nDaily-Bot-A: ok , thank you .\nDaily-Bot-B: you re welcome .\nDaily-Bot-A: it s my pleasure having your Ô¨Årm s christmas tree delivered to\nyou .\nDaily-Bot-B: it s my pleasure .\nDaily-Bot-A: we will send someone to your ofÔ¨Åce in a few minutes . see you\nthen .\nDaily-Bot-B: that s all right .\nDaily-Bot-A: that s all right .\nDaily-Bot-B: don t worry . i ll be straight home as soon as i have all of our\nchristmas ready .\nDaily-Bot-A: thanks a much .\nDaily-Bot-B: you re welcome .\nDaily-Bot-A: i m glad you could come so soon .\nDaily-Bot-B: i appreciate your help .\nDaily-Bot-A: don t worry . it s nothing .\nDaily-Bot-B: i m glad that we have a big christmas dinner .\nDaily-Bot-A: i hope so .\nDaily-Bot-B: i hope so , too .\nDaily-Bot-A: you re welcome .\nDaily-Bot-B: i hope so , too .\nDaily-Bot-A: bye-bye .\nTable 14: Another example of interactions between two multi-turn DailyDialog-Bots.\nSeed Query Why do you like the ocean?\nResults Daily-Bot-A: why do you like the ocean?\nDaily-Bot-B: it‚Äôs pleasant while the sun is shining .\nDaily-Bot-A: i like the feeling then that the world changes so quickly .\nDaily-Bot-B: i agree . people have personal styles and qualities that suit their\nneeds .\nDaily-Bot-A: i agree.it seems that many people like the seafood .\nDaily-Bot-B: well , what i would like is some sole .\nDaily-Bot-A: ok . i will bring it back to our shop tonight .\nDaily-Bot-B: sure , sir . have a nice day .\nDaily-Bot-A: thank you.thanks for inviting me .\nDaily-Bot-B: you are welcome .\nDaily-Bot-A: don‚Äôt mention it .\nDaily-Bot-B: it‚Äôs been a long time since i had sole .\nDaily-Bot-A: next time , please .\nDaily-Bot-B: that‚Äôs all right .\nDaily-Bot-A: than you would have liked , sir .\nDaily-Bot-B: it‚Äôs be nice .\nDaily-Bot-A: then , shall we shorten the distance ?\nDaily-Bot-B: let me see ... just a minute .\nDaily-Bot-A: you‚Äôre not quite right .\nDaily-Bot-B: i suppose i could , sir .\nDaily-Bot-A: yes , of course . that will be great .\nDaily-Bot-B: and for our bottles , please .\nDaily-Bot-A: yes , sir . we‚Äôll do it business through the atlas .\nDaily-Bot-B: thank you very much .\nDaily-Bot-A: we‚Äôre sure your wine would be appreciated .\nDaily-Bot-B: i‚Äôm glad to hear that .\nDaily-Bot-A: would you please Ô¨Åll out this form ? here you are .\nDaily-Bot-B: now let me check .\nDaily-Bot-A: by the way , please open this bottle for me . everything will be\npaid for in advance .\nDaily-Bot-B: certainly , sir .\nDaily-Bot-A: here is the forty dollars .\nDaily-Bot-B: wait a moment , please .\nDaily-Bot-A: ( after a while ) i am sorry you mustn‚Äôt enjoy the service .\nDaily-Bot-B: that‚Äôs all right .\nDaily-Bot-A: here is the money .\nDaily-Bot-B: thank you very much . i hope you take it back .\nDaily-Bot-A: it‚Äôs my pleasure .\nDaily-Bot-B: come back again a little later .\nDaily-Bot-A: i‚Äôve never paid much attention to the service in my life .\nDaily-Bot-B: it‚Äôs normal . i appreciate the time you‚Äôve prepared for it .\nDaily-Bot-A: i appreciate the time you‚Äôve prepared .\nDaily-Bot-B: it‚Äôs my pleasure .\nDaily-Bot-A: call me back any time you need help .\nDaily-Bot-B: i appreciate the time .\nDaily-Bot-A: don‚Äôt mention it .\nDaily-Bot-B: it‚Äôs my pleasure .\nDaily-Bot-A: our friendship is mutual .\nDaily-Bot-B: you‚Äôre welcome .\nDaily-Bot-A: it‚Äôs a pleasure .\nDaily-Bot-B: i‚Äôll glad to do well .\nTable 15: One more example of interactions between two multi-turn DailyDialog-Bots.\nSeed Query What is the meaning of life?\nResults Reddit-Bot-A: what is the meaning of life?\nReddit-Bot-B: the meaning of life ? the purpose of human existence ? like the purpose\nof love or the purpose of happiness ?\nReddit-Bot-A: life is not something you can live .\nReddit-Bot-B: that ‚Äôs why everything on the world ends when you die , and that is\nbecause you live forever . you cannot die when you die . life happens and you live\nforever .\nReddit-Bot-A: i just do n‚Äôt understand how you could think that .\nReddit-Bot-B: this is why i do n‚Äôt even consider myself a happy person .\nReddit-Bot-A: do you think life could ever be any sort of happy ending ? or just be a\nhappy happy ending ?\nReddit-Bot-B: because you die with a single soul ? and you die on a tangled web you\nca n‚Äôt move on from ?\nReddit-Bot-A: i ‚Äôm not sure . i think death will be better when everyone is able to\nsupport a human being without a child , especially in certain circumstances .\nReddit-Bot-B: he ‚Äôll be Ô¨Åne , he ‚Äôll die as long as he keeps his soul around .\nReddit-Bot-A: it ‚Äôs the same as people complaining how they die or people complaining\nhow they die because they got too worked up about things .\nReddit-Bot-B: i ca n‚Äôt believe someone said that . someone has to be a complete xxx\nto you to not try .\nReddit-Bot-A: i ‚Äôm assuming he was just trying to say there ‚Äôs a difference between\ndeath and cruelty\nReddit-Bot-B: i mean its pretty obvious that he made a mistake and you ‚Äôre just mad\nbecause of it\nReddit-Bot-A: nah , he just makes up thing to sound cool as hell .\nReddit-Bot-B: no i understand it ‚Äôs a joke but it ‚Äôs sad and offensive that someone\nwould think it ‚Äôs an okay thing to do to somebody that makes fun of them .\nReddit-Bot-A: if you like his joke then you probably do n‚Äôt like the way it works in\nreal life . i would think this might be the kind of person that ‚Äôd get his audience to\nlaugh when they see you .\nReddit-Bot-B: oh , i see what you mean . the thing being fun is probably a joke . i ‚Äôm\nsorry this happened to you .\nReddit-Bot-A: what other thing happens to you ? i ca n‚Äôt tell what else you ‚Äôre talking\nabout .\nReddit-Bot-B: we ‚Äôre a bunch of kids all about fun . sometimes funny thing happens\nto people too . it ‚Äôs funny . it makes me uncomfortable\nReddit-Bot-A: just cause i have to share my story does n‚Äôt mean i do n‚Äôt have too\nmuch fun in life . i like sharing that story without having to share every part of it with\nsomeone else .\nReddit-Bot-B: my brother has this issue too ! he tells everyone he knows every week .\nReddit-Bot-A: i ‚Äôve seen some guys go through the same thing . i know that ‚Äôs what it\nsounds like so i ‚Äôm just trying to share my story with the good ones .\nReddit-Bot-B: yea that ‚Äôs why i like it . i can get the story out of the way sometimes\nbut my best friend told him that i was going to tell him that if we talked to him .\nReddit-Bot-A: yeah , it ‚Äôs kind of hard to get what you want when everyone loves you\nso much so much . i ‚Äôm sorry for you .\nReddit-Bot-B: i ‚Äôm very sorry . it ‚Äôs not hard at all . i have a long lasting relationship\nand we ‚Äôve been together for a long time\nReddit-Bot-A: no worries . if you ever need advice , feel free to message me and i ‚Äôll\nanswer any questions i have .\nTable 16: An example of interactions between two single-turn Reddit-Bots.\na22/ Zm2`v ‰ªäÂ§©Â§©‚Ωì‰∏çÈîô\n_2bmHib .Qm#\u001cM@\"Qi@\u001b, ‰ªäÂ§©Â§©‚Ωì‰∏çÈîô\n.Qm#\u001cM@\"Qi@\", Ë¶ÅË¢´ÁÉ≠Ê≠ª‰∫Ü\n.Qm#\u001cM@\"Qi@\u001b, Âéª‰π∞ÁÇπÁ∫∏‚º±Âêß‰Ω†ËøòÂú®Âêó\n.Qm#\u001cM@\"Qi@\", ÊúâÂÜ∞ÈïáË•ø‚Ω†‰πà\n.Qm#\u001cM@\"Qi@\u001b, Ê≤°ÊúâÊàëËøôÊòØÂú®Ë•ø‚Ω†‚æ•‚æØÂä†‰∏ä‰∫ÜÁï™ËåÑÈÖ±\n.Qm#\u001cM@\"Qi@\", ÈÇ£Ë¶ÅÊòØÁÉ§ËøáÁöÑËØù‚ºÄÂÆöÂ•ΩÁúã\n.Qm#\u001cM@\"Qi@\u001b, ÈÇ£ËøòÊòØÁÇíÈ∏°ËõãÂ•ΩÂêÉÂ§ö‰∫Ü\n.Qm#\u001cM@\"Qi@\", ÂèØÊòØË•ø‚Ω†‚æ•‰πüÊúâË•ø‚Ω†Ê±Å‰Ω†ÂêÉËøá‰∏ç\n.Qm#\u001cM@\"Qi@\u001b, ÂøÖÈ°ªÁöÑ‰∏çÁÑ∂Êàë‰πüÊÉ≥ÂêÉ‰∫Ü\n.Qm#\u001cM@\"Qi@\", ÈÇ£Â∞±Êù•ÊùØÈ∏°Â∞æÈÖí\n.Qm#\u001cM@\"Qi@\u001b, ‰Ω†‰πüÂêÉËøá\n.Qm#\u001cM@\"Qi@\", ÊÉ≥ÂêÉÂ∞±Áªô\n.Qm#\u001cM@\"Qi@\u001b, Â§™Ëøú‰∫Ü‰Ω†Âú®Âì™Â•ΩÂ•Ω‰∏äÂ≠¶\n.Qm#\u001cM@\"Qi@\", ÊàëÂú®Âé¶Èó®‰Ω†Âú®Âì™Â•ΩÂ•Ω‰∏äÂ≠¶\n.Qm#\u001cM@\"Qi@\u001b, ÊàëÊÉ≥ÂéªÁúãÁúã\n.Qm#\u001cM@\"Qi@\", ÂéªÁúãÊù•‰∫Ü‰∏çÁÑ∂‰Ω†ÂæóÊâæ‰∏™‰º¥‚ºâ‰Ω†ÊÉ≥ÂêÉÂ∞±ÂêÉ\n.Qm#\u001cM@\"Qi@\u001b, ÂìéÈÉΩÂèØ‰ª•ÁöÑ\n.Qm#\u001cM@\"Qi@\", ËøòÊ≤°Âà∞Âπ¥Â∫ïÈòø\n.Qm#\u001cM@\"Qi@\u001b, ÊàëÂáÜÂ§áÂéªÂé¶Èó®Êâæ‰Ω†Âéª\n.Qm#\u001cM@\"Qi@\", Êâæ‰Ω†ÂêÉÈ•≠\n.Qm#\u001cM@\"Qi@\u001b, Êâæ‰Ω†ÂêÉÈ•≠Âêß\n.Qm#\u001cM@\"Qi@\", ÂèØÊòØÊàë‰∏çÊòØÂé¶Èó®‚ºàÂì¶ÊàëË¶ÅÂõûÂÆ∂ÂéªËßÅÁà∏Áà∏‰∫Ü\n.Qm#\u001cM@\"Qi@\u001b, Q\u0000\u0000Q\n.Qm#\u001cM@\"Qi@\", Q\u0000\u0000QÁ•ù‰Ω†Â•ΩËøê\n.Qm#\u001cM@\"Qi@\u001b, Á•ù‰Ω†Êó©‚ΩáÊâæÂà∞‚º•ÊúãÂèã\n.Qm#\u001cM@\"Qi@\", Ë∞¢Ë∞¢‰Ω†ÁöÑÁ•ùÁ¶è\n.Qm#\u001cM@\"Qi@\u001b, ‰∏çÂÆ¢‚Ωì\n.Qm#\u001cM@\"Qi@\", ‰Ω†‰πüÊòØ\n.Qm#\u001cM@\"Qi@\u001b, ‰Ω†‰πüÊòØÂé¶Èó®ÁöÑ\n.Qm#\u001cM@\"Qi@\", ÊàëÂú®Âåó‰∫¨\n.Qm#\u001cM@\"Qi@\u001b, ÂóØÂóØ‰Ω†ÁöÑÂùêÊ†áÂ•ΩÁ°¨\n.Qm#\u001cM@\"Qi@\", ÈÇ£‰πàÈóÆÈ¢òÊù•‰∫Ü\n.Qm#\u001cM@\"Qi@\u001b, Âé¶Èó®Ëøô‰πàÂ•ΩÁöÑ‚º©‰ºô‰º¥ËÉΩ‰∏çËÉΩ‰∏çË¶ÅÂ§™Ëøú‰∫Ü\n.Qm#\u001cM@\"Qi@\", Êàë‰∏çÂú®Âåó‰∫¨‰∏çËøáÊàë‰ª¨ÂèØ‰ª•ËÅäËÅä\n.Qm#\u001cM@\"Qi@\u001b, ÁúüÁöÑÁúüÁöÑÊàëÁé∞Âú®Âú®Âåó‰∫¨Âì¶\n.Qm#\u001cM@\"Qi@\", ÈÇ£‰Ω†Âú®Âåó‰∫¨‚ºØ‰ΩúÂòõ\n.Qm#\u001cM@\"Qi@\u001b, ‰Ω†Âì™‚ºâÁöÑ\n.Qm#\u001cM@\"Qi@\", ÊàëÂú®Â§©Ê¥•\n.Qm#\u001cM@\"Qi@\u001b, bQ;\u001cÊàë‰πüÊòØÂé¶Èó®ÁöÑ\n.Qm#\u001cM@\"Qi@\", Â•ΩÂêßÁé∞Âú®ÊàëÂéªÂ§©Ê¥•‰∫ÜÊúâÁ©∫Êù•Áé©\n.Qm#\u001cM@\"Qi@\u001b, Â•ΩÁöÑ‚æñÊ≤π‰Ω†ÂæÆ‰ø°\nR\nTable 17: Example of interactions between two multi-turn Douban-Bots.\na22/ Zm2`v ‰Ω†ÁöÑÊ¢¶ÊÉ≥ÊòØ‰ªÄ‰πàÔºü\n_2bmHib .Qm#\u001cM@\"Qi@\u001b, ‰Ω†ÁöÑÊ¢¶ÊÉ≥ÊòØ‰ªÄ‰πà\\\n.Qm#\u001cM@\"Qi@\", ÊúâÁöÑ‚ºà‚ºÄ‰∏äÊù•Â∞±ÊòØËøô‰∏™ÊÑèÊÄù\n.Qm#\u001cM@\"Qi@\u001b, ‰Ω†Áé∞Âú®ÁöÑÊ¢¶ÊÉ≥ÊòØ‰ªÄ‰πà\n.Qm#\u001cM@\"Qi@\", Ê¢¶ÊÉ≥Â∞±ÊòØÂéªÂÅöÂô©Ê¢¶ÁöÑ‰∫ãÂÅ∂Â∞îÊÉ≥ÁùÄÊ¢¶\n.Qm#\u001cM@\"Qi@\u001b, ÊàëÁé∞Âú®ÊúâÊ¢¶ÊÉ≥\n.Qm#\u001cM@\"Qi@\", ÊàëÊÉ≥Êó©Áù°Êó©Ëµ∑Â∞±Â•Ω‰∫Ü\n.Qm#\u001cM@\"Qi@\u001b, Á•ù‰Ω†Ê¢¶ÊÉ≥ÊàêÁúü\n.Qm#\u001cM@\"Qi@\", Ë∞¢Ë∞¢\n.Qm#\u001cM@\"Qi@\u001b, Âà´ÂÆ¢‚Ωì\n.Qm#\u001cM@\"Qi@\", Ê≤°ÊÉ≥Â•Ω‰Ω†Âú®ÊÉ≥‰ªÄ‰πàÊ¢¶ÊÉ≥‰ªÄ‰πà\n.Qm#\u001cM@\"Qi@\u001b, Ê¢¶ÊÉ≥\n.Qm#\u001cM@\"Qi@\", ÊÉ≥Âï•Â∞±‚º≤Âï•Ê¢¶ÊÉ≥‰ªÄ‰πà\n.Qm#\u001cM@\"Qi@\u001b, ÊÉ≥ÁùÄÊ¢¶ÊÉ≥Âï•\n.Qm#\u001cM@\"Qi@\", ÊàëÊÉ≥‰∫ÜÊÉ≥Âà∞‰∫ÜÊÉ≥‰ªÄ‰πà‰ΩÜÊòØÊÄªË¶ÅÊâæÁêÜÁî±Ê¢¶ÊÉ≥\n.Qm#\u001cM@\"Qi@\u001b, Áé∞ÂÆû\n.Qm#\u001cM@\"Qi@\", ÊÅ©ÊâÄ‰ª•Ê¢¶ÊÉ≥\n.Qm#\u001cM@\"Qi@\u001b, Ë¶ÅÂÅö‰∏™ÁúüÁöÑÂÆûÁé∞Ê¢¶Â∞±‚æèÁé∞ÂÆû‰∏≠Ë¶ÅÂÅöÂá∫Êù•‰∫Ü\n.Qm#\u001cM@\"Qi@\", ÂóØÂÅö‰πü‰∏çÈîô\n.Qm#\u001cM@\"Qi@\u001b, ÊÉ≥ÂΩìÂπ¥Ê¢¶ÊÉ≥ÂÅöÁöÑ‰∫ãÊÉÖÁé∞Âú®ÊÉ≥ÊÉ≥Â∞±ÂèØ‰ª•Êúâ‰∫Ü\n.Qm#\u001cM@\"Qi@\", ÂóØÊàëÁé∞Âú®‰πüÂæàÊÉ≥ÂÅöÂá∫ÂæàÂ§ö‚ºº‰∫ã\n.Qm#\u001cM@\"Qi@\u001b, ‰ΩÜÊÑø\n.Qm#\u001cM@\"Qi@\", ‚ºº‚æ•ÁöÑÂéã‚ºíÊòØ‰∏çÊòØË¶ÅÁúã‰Ω†‚æÉ‚º∞‰∫Ü\n.Qm#\u001cM@\"Qi@\u001b, ÊàëËÉΩËØ¥ÊàëÁé∞Âú®ÊØèÂ§©ÈÉΩÂú®Êå£Èí±Âêó\n.Qm#\u001cM@\"Qi@\", ‚ºÄ‰∏™‚Ωâ‚ºØËµÑÂ§ö\n.Qm#\u001cM@\"Qi@\u001b, ÈÇ£‰ºöÂæàÂ§ö‰∏çÁü•ÊÄé‰πàËµöÈí±ÂæàÂ§ö‰∫Ü\n.Qm#\u001cM@\"Qi@\", ‰Ω†ÂéªËØïËØï\n.Qm#\u001cM@\"Qi@\u001b, ÊÑüËßâÂÅöÂà∞Ê≤°Êúâ\n.Qm#\u001cM@\"Qi@\", ÂÅöÂà∞Ê≤°Êúâ\n.Qm#\u001cM@\"Qi@\u001b, ÂΩìÁÑ∂Êúâ\n.Qm#\u001cM@\"Qi@\", ÂéªÂì™ËØïËØïÊàëÈÉΩÂÅö‰∏çÂà∞\n.Qm#\u001cM@\"Qi@\u001b, ‚æÉ‚º∞ÂÅöÊâçÁü•ÈÅì\n.Qm#\u001cM@\"Qi@\", ÊàëÂ∑≤‰∏çÁü•ÈÅì\n.Qm#\u001cM@\"Qi@\u001b, Âú®Âì™Âú®Âì™‚æ•ÂÅö\n.Qm#\u001cM@\"Qi@\", ÁôæÂ∫¶\n.Qm#\u001cM@\"Qi@\u001b, Â∞±Áü•ÈÅì\n.Qm#\u001cM@\"Qi@\", Êàë‰πü‰∏çÁü•ÈÅìÊâæÁöÑÈÉΩÊòØ‰ªÄ‰πà\n.Qm#\u001cM@\"Qi@\u001b, ÁôæÂ∫¶ÈÉΩ‚ºÄÊ†∑Âè™ÊòØÊ≤°ÊúâÂá∫ÂõΩÁöÑÊâæÁöÑ‚Ωá‚º¶ÊÉ≥ÂéªÈÇ£‚æ•\n.Qm#\u001cM@\"Qi@\", ÊÉ≥Âà∞‰∫ÜÈÇ£‚æ•ÁúüÊòØ‰∏çÂÆπÊòì\n.Qm#\u001cM@\"Qi@\u001b, Ê≤°‰ªÄ‰πà\n.Qm#\u001cM@\"Qi@\", Êàë‰πüÊ≤°ÊúâÁúãËøáÊàë‰πüÂú®ÊÉ≥Ëøô‚æ•ÊúâÊ≤°ÊúâÊú∫‰ºöÊù•ÊâæÊàë\n.Qm#\u001cM@\"Qi@\u001b, ÂéªÂêßÂéªÂêßÂéªÂêß\n.Qm#\u001cM@\"Qi@\", Ëøô‰πàÂø´Â∞±ÊúâÊú∫‰ºö‰∫Ü\n.Qm#\u001cM@\"Qi@\u001b, ÂóØÂóØÈÇ£Âä†Ê≤π\nR\nTable 18: Another example of interactions between two multi-turn Douban-Bots.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8494299650192261
    },
    {
      "name": "Transformer",
      "score": 0.6728155612945557
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6113067269325256
    },
    {
      "name": "Natural language processing",
      "score": 0.5467818379402161
    },
    {
      "name": "Language model",
      "score": 0.520135223865509
    },
    {
      "name": "Relevance (law)",
      "score": 0.49369344115257263
    },
    {
      "name": "Context (archaeology)",
      "score": 0.46442878246307373
    },
    {
      "name": "Baseline (sea)",
      "score": 0.4424772262573242
    },
    {
      "name": "Machine learning",
      "score": 0.4266735911369324
    },
    {
      "name": "Decoding methods",
      "score": 0.4261839985847473
    },
    {
      "name": "Beam search",
      "score": 0.423510879278183
    },
    {
      "name": "Speech recognition",
      "score": 0.3278883099555969
    },
    {
      "name": "Search algorithm",
      "score": 0.16587382555007935
    },
    {
      "name": "Algorithm",
      "score": 0.08288145065307617
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": []
}