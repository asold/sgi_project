{
    "title": "XLM-K: Improving Cross-Lingual Language Model Pre-training with Multilingual Knowledge",
    "url": "https://openalex.org/W3202026069",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2106506284",
            "name": "Xiaoze Jiang",
            "affiliations": [
                "Beihang University"
            ]
        },
        {
            "id": "https://openalex.org/A2153945145",
            "name": "Yaobo Liang",
            "affiliations": [
                "Microsoft Research Asia (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2108390110",
            "name": "Wei‐Zhu Chen",
            "affiliations": [
                "Microsoft (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A1966337374",
            "name": "Nan Duan",
            "affiliations": [
                "Microsoft Research Asia (China)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2984147501",
        "https://openalex.org/W2891328459",
        "https://openalex.org/W2739945392",
        "https://openalex.org/W3123693044",
        "https://openalex.org/W3005680577",
        "https://openalex.org/W3042711927",
        "https://openalex.org/W2983040767",
        "https://openalex.org/W6767737316",
        "https://openalex.org/W2891555348",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2785611959",
        "https://openalex.org/W3197148020",
        "https://openalex.org/W3016309009",
        "https://openalex.org/W2567619939",
        "https://openalex.org/W6766860002",
        "https://openalex.org/W2987283559",
        "https://openalex.org/W3092973241",
        "https://openalex.org/W2971207485",
        "https://openalex.org/W3092939616",
        "https://openalex.org/W3127729136",
        "https://openalex.org/W6631190155",
        "https://openalex.org/W2980404057",
        "https://openalex.org/W3014635508",
        "https://openalex.org/W2973840669",
        "https://openalex.org/W3118106810",
        "https://openalex.org/W6784928963",
        "https://openalex.org/W2988395732",
        "https://openalex.org/W6718053083",
        "https://openalex.org/W6786438351",
        "https://openalex.org/W2250770256",
        "https://openalex.org/W3086027778",
        "https://openalex.org/W2250635077",
        "https://openalex.org/W2252213301",
        "https://openalex.org/W3005441132",
        "https://openalex.org/W2983102021",
        "https://openalex.org/W2571811098",
        "https://openalex.org/W6683675066",
        "https://openalex.org/W2989539713",
        "https://openalex.org/W2524017757",
        "https://openalex.org/W2994811754",
        "https://openalex.org/W2230586094",
        "https://openalex.org/W2996822578",
        "https://openalex.org/W3014308185",
        "https://openalex.org/W2946345909",
        "https://openalex.org/W2994915912",
        "https://openalex.org/W2963855739",
        "https://openalex.org/W2971869958",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W3102483398",
        "https://openalex.org/W3032816972",
        "https://openalex.org/W3034978746",
        "https://openalex.org/W3104723404",
        "https://openalex.org/W3151929433",
        "https://openalex.org/W3035524453",
        "https://openalex.org/W2080133951",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2561529111",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W3171975879",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3169425228",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W3105661746",
        "https://openalex.org/W3170611326",
        "https://openalex.org/W3104163040",
        "https://openalex.org/W2953109491",
        "https://openalex.org/W2998385486",
        "https://openalex.org/W2964207259",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3040558716",
        "https://openalex.org/W2953356739",
        "https://openalex.org/W3035497479",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W3175604467",
        "https://openalex.org/W2970049541",
        "https://openalex.org/W3034569646",
        "https://openalex.org/W3156170450",
        "https://openalex.org/W2997012196",
        "https://openalex.org/W3102844651",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2158028897",
        "https://openalex.org/W3155399633"
    ],
    "abstract": "Cross-lingual pre-training has achieved great successes using monolingual and bilingual plain text corpora. However, most pre-trained models neglect multilingual knowledge, which is language agnostic but comprises abundant cross-lingual structure alignment. In this paper, we propose XLM-K, a cross-lingual language model incorporating multilingual knowledge in pre-training. XLM-K augments existing multilingual pre-training with two knowledge tasks, namely Masked Entity Prediction Task and Object Entailment Task. We evaluate XLM-K on MLQA, NER and XNLI. Experimental results clearly demonstrate significant improvements over existing multilingual language models. The results on MLQA and NER exhibit the superiority of XLM-K in knowledge related tasks. The success in XNLI shows a better cross-lingual transferability obtained in XLM-K. What is more, we provide a detailed probing analysis to confirm the desired knowledge captured in our pre-training regimen. The code is available at https://github.com/microsoft/Unicoder/tree/master/pretraining/xlmk.",
    "full_text": "XLM-K: Improving Cross-Lingual Language Model Pre-training with\nMultilingual Knowledge\nXiaoze Jiang1*, Yaobo Liang2, Weizhu Chen3, Nan Duan2\n1Beihang University, Beijing, China\n2Microsoft Research Asia, Beijing, China\n3Microsoft Azure AI, Redmond, W A, USA\nxzjiang@buaa.edu.cn, {yalia, wzchen, nanduan}@microsoft.com\nAbstract\nCross-lingual pre-training has achieved great successes us-\ning monolingual and bilingual plain text corpora. However,\nmost pre-trained models neglect multilingual knowledge,\nwhich is language agnostic but comprises abundant cross-\nlingual structure alignment. In this paper, we propose XLM-\nK, a cross-lingual language model incorporating multilin-\ngual knowledge in pre-training. XLM-K augments existing\nmultilingual pre-training with two knowledge tasks, namely\nMasked Entity Prediction Task and Object Entailment Task.\nWe evaluate XLM-K on MLQA, NER and XNLI. Experi-\nmental results clearly demonstrate significant improvements\nover existing multilingual language models. The results on\nMLQA and NER exhibit the superiority of XLM-K in knowl-\nedge related tasks. The success in XNLI shows a better cross-\nlingual transferability obtained in XLM-K. What is more,\nwe provide a detailed probing analysis to confirm the de-\nsired knowledge captured in our pre-training regimen. The\ncode is available at https://github.com/microsoft/Unicoder/\ntree/master/pretraining/xlmk.\nIntroduction\nRecent development of pre-trained language model (Devlin\net al. 2019; Liu et al. 2019) has inspired a new surge of\ninterest in the cross-lingual scenario, such as Multilingual\nBERT (Devlin et al. 2019) and XLM-R (Conneau et al.\n2020). Existing models are usually optimized for masked\nlanguage modeling (MLM) tasks (Devlin et al. 2019) and\ntranslation tasks (Conneau and Lample 2019) using multi-\nlingual data. However, they neglect the knowledge across\nlanguages, such as entity resolution and relation reasoning.\nIn fact, the knowledge conveys similar semantic concepts\nand similar meanings across languages (Vuli ´c and Moens\n2013; Chen et al. 2021), which is essential to achieve cross-\nlingual transferability. Therefore, how to equip pre-trained\nmodels with knowledge has become an underexplored but\ncritical challenge for multilingual language models.\nContextual linguistic representations in language mod-\nels are ordinarily trained using unlabeled and unstructured\ncorpus, without the consideration of explicit grounding to\nknowledge (F´evry et al. 2020; Xiong et al. 2020; Fan et al.\n*Contribution during internship at Microsoft Research Asia.\nCopyright © 2022, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n2021), such as entity and relation. On one side, the structural\nknowledge data is abundant and could be a great comple-\nment to the unstructured corpus for building a better lan-\nguage model. Many works have demonstrated its impor-\ntance via incorporating basic knowledge into monolingual\npre-trained models (Zhang et al. 2019; Stali ¯unait˙e and Ia-\ncobacci 2020; Zhang et al. 2020; Wang et al. 2021b). On\nanother side, knowledge is often language agnostic, e.g.\ndifferent languages share the same entity via different sur-\nface forms. This can introduce a huge amount of alignment\ndata to learn a better cross-lingual representation (Cao et al.\n2018a). However, there are few existing works on explor-\ning the multilingual entity linking and relation in the cross-\nlingual setting for pre-training (Huang et al. 2019; Yang\net al. 2020). For example, the de-facto cross-lingual pre-\ntraining standard, i.e. MLM (Devlin et al. 2019) plus TLM\n(Conneau and Lample 2019), learns the correspondences be-\ntween the words or sentences across the languages, neglect-\ning the diverse background cross-lingual information behind\neach entity.\nTo address this limitation, we propose XLM-K, a cross-\nlingual language model incorporating multilingual knowl-\nedge in pre-training. The knowledge is injected into the\nXLM-K via two additional pre-trained tasks, i.e.masked en-\ntity prediction task and object entailment task. These two\ntasks are designed to capture the knowledge from two as-\npects: description semantics and structured semantics. De-\nscription semantics encourage the contextualized entity em-\nbedding in a sequence to be linked to the long entity de-\nscription in the multilingual knowledge base (KB). Struc-\ntured semantics, based on the triplet knowledge <subject,\nrelation, object>, connect cross-lingual subject and object\nbased on their relation and descriptions, in which the object\nis entailed by the joint of the subject and the relation. The\nobject and subject are both represented by their description\nfrom the KB. To facilitate the cross-lingual transfer ability,\non one hand, the entity and its description are from different\nlanguages. On the other hand, the textual contents of the sub-\nject and object also come from a distinct language source.\nWe employ the contrastive learning (He et al. 2020) during\npre-training to make XLM-K distinguish a positive knowl-\nedge example from a list of negative knowledge examples.\nThere are three main contributions in our work:\n• As the first attempt, we achieve the combination be-\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n10840\ntween the textual information and knowledge base in cross-\nlingual pre-training by proposing two knowledge related and\ncross-lingual pre-training tasks. The knowledge, connected\nvia different languages, introduces additional information\nfor learning a better multilingual representation.\n• We evaluate XLM-K on the entity-knowledge related\ndownstream tasks, i.e. MLQA and NER, as well as the stan-\ndard multilingual benchmark XNLI. Experimental results\nshow that XLM-K achieves new state-of-the-art results in\nthe setting without bilingual data resource. The improve-\nments in MLQA and NER show its superiority on knowl-\nedge related scenarios. The results on XNLI demonstrate the\nbetter cross-lingual transferability in XLM-K.\n•We further perform a probing analysis (Petroni et al.\n2019) on XLM-K, clearly reflecting the desired knowledge\nin the pre-trained models.\nRelated Work\nCross-Lingual Pre-training Works on cross-lingual pre-\ntraining have achieved a great success in multilingual tasks.\nMultilingual BERT (Devlin et al. 2019) trains a BERT model\nbased on multilingual masked language modeling task on\nthe monolingual corpus. XLM-R (Conneau et al. 2020) fur-\nther extends the methods on a large scale corpus. These\nmodels only use monolingual data from different languages.\nTo achieve cross-lingual token alignment, XLM (Conneau\nand Lample 2019) proposes translation language model-\ning task on parallel corpora. Unicoder (Huang et al. 2019)\npresents several pre-training tasks upon parallel corpora and\nInfoXLM (Chi et al. 2021) encourages bilingual sentence\npair to be encoded more similar than the negative exam-\nples, while ERNIE-M (Ouyang et al. 2021) learns seman-\ntic alignment among multiple languages on monolingual\ncorpora. These models leverage bilingual data to achieve\nbetter cross-lingual capability between different languages.\nOur method explores cross-lingual knowledge base as a new\ncross-lingual supervision.\nKnowledge-Aware Pre-training Recent monolingual\nworks, incorporating basic knowledge into monolingual\npre-trained models, result to a better performance in\ndownstream tasks (Rosset et al. 2020). For example, some\nworks introduce entity information via adding a knowledge\nspecific model structure (Broscheit 2019; Zhang et al. 2019;\nF´evry et al. 2020). Others consider the relation information\ncaptured in the knowledge graph triples (Hayashi et al.\n2020; Zhang et al. 2020; Wang et al. 2021a; Liu et al.\n2020). Meanwhile, Xiong et al. (2020); F ´evry et al. (2020)\nequip monolingual language model with diverse knowledge\nwithout extra parameters. These works are almost in mono-\nlingual domain without the consideration of cross-lingual\ninformation, while the cross-lingual knowledge is learned\nby our model via the proposed tasks. Moreover, the standard\noperation of aforementioned works are mostly based on\nthe entity names. The entity names are masked and then\npredicted by the model, namely the MLM task is conducted\non the masked entity names. While we predict the entity\ndescription for the purpose of disambiguation of different\nentities with the same entity name (detailed in above). It can\nhelp our model learn more fine-grained knowledge.\nWord Embedding and KB Joint LearningMany works\nleverage word embedding from text corpus to generate bet-\nter KB embedding. Wang et al. 2014; Yamada et al. 2016;\nCao et al. 2017 utilize the texts with entity mention and en-\ntity name to align word embedding and entity embedding.\nToutanova et al. 2015; Han, Liu, and Sun 2016; Wu et al.\n2016; Wang and Li 2016 utilize the sentences with two entity\nmentions as relation representation to generate better entity\nand relation embedding. These works mainly target to gen-\nerate better graph embedding with English corpus and each\nentity will have a trainable embedding. Our methods focus\non training a better contextualized representation for multi-\nple languages. Meanwhile, the entity representation is gen-\nerated by Transformer (Vaswani et al. 2017) model, which\ncould further align the textual and KB embedding, as well\nas achieving the less trainable parameters. For cross-lingual\nword embeddings, most of works rely on aligned words\nor sentences (Ruder, Vuli ´c, and Søgaard 2019). Cao et al.\n2018b; Pan et al. 2019; Chen et al. 2021 replace the entity\nmention to a special entity tag and regularize one entity’s\ndifferent mentions in different languages to have similar em-\nbedding. Vuli´c and Moens 2013 use topic tag of Wikipedia\npages to improve the cross-lingual ability. We also utilize\nentity mention in different languages as cross-lingual align-\nment supervision. Different from these works, we further\nexploit relation information to enhance the entity represen-\ntation. What’s more, we generate entity representation by\nTransformer model instead of training the separate embed-\nding for special entity tag.\nMethodology\nWe first present the knowledge construction strategy. Then,\nwe introduce our two knowledge-based pre-training tasks\nand the training objective.\nKnowledge Construction\nWe use Wikipedia and Wikidata (Vrandeˇci´c and Kr ¨otzsch\n2014) as the data source.\nKnowledge Graph A knowledge graph is a set of triplets\nin form <subject, relation, object>. We use Wikidata as our\nknowledge base. The triplets of Wikidata are extracted from\nWikipedia and each Wikipedia page corresponding to an en-\ntity in WikiData. WikiData contains 85 million entities and\n1304 relations. They formed 280 million triplets.\nEntity Mention For a sentence with l words, X =\n(x1, x2, ..., xl), a mention (s, t, e) means the sub-sequence\n(xs, x(s+1), ..., xt) corresponding to entity e. In our work,\nwe use Wikipedia as data source. For each anchor in\nWikipedia, it provides the link to the Wikipedia page of this\nentity, which can be further mapped to a unique entity in\nWikidata. Wikipedia pages are from 298 languages, and ev-\nery around 64 tokens contains an anchor.\nMultilingual Entity DescriptionWe treat a Wikipedia page\nas the description of its corresponding entity in WikiData.\nSince Wikipedia contains multiple languages, an entity may\nhave multiple descriptions and they come from different lan-\nguages. For each page, we only keep its first 256 tokens as\n10841\nApple 苹果\n사과 แอปเปิล\nEntity 1\nApple Inc. 苹果公司\n사과 แอปเปิล (บริษัท)\nEntity 2\n…\nFruit 水果\n과일 Φρούτα\nEntity N\nTransformer\nTransformer\nTransformer\nTransformer\n…\n…\n…\n…\nArabic, ar\nChinese, zh\nEnglish, en\nü\nû\nû\nEntityMultilingual DescriptionEntity Description Embedding\nLe\nMasked Entity: Apple\nZ+\nZ2\nzq\nZN\nApple 苹果\n사과 แอปเปิล\nEntity 1\nApple Inc. 苹果公司\n사과 แอปเปิล (บริษัท)\nEntity 2\n…\nFruit 水果\n과일 Φρούτα\nEntity N\nTransformer\nTransformer\nTransformer\n…\n…\n…\nArabic, ar\nChinese, zh\nEnglish, en\nEntityMultilingual DescriptionEntity Description Embedding\nz1\nz2\nz+\nTransformer Relation Encoder\nSubclass of\nzq\ns ~ r ~\n(Apple, Subclass of)         Fruit\nRelation r\nü\nû\nû\n“Apple” Description S \n(Hindi, hi)\nZ1\nZ2\nZ+\nz+\nz2\nzN\nSentence X (English, en) \nLo\n(a) Masked Entity Prediction Task\nApple 苹果\n사과 แอปเปิล\nEntity 1\nApple Inc. 苹果公司\n사과 แอปเปิล (บริษัท)\nEntity 2\n…\nFruit 水果\n과일 Φρούτα\nEntity N\nTransformer\nTransformer\nTransformer\nTransformer\n…\n…\n…\n…\nArabic, ar\nChinese, zh\nEnglish, en\nü\nû\nû\nEntityMultilingual DescriptionEntity Description Embedding\nLe\nMasked Entity: Apple\nZ+\nZ2\nzq\nZN\nApple 苹果\n사과 แอปเปิล\nEntity 1\nApple Inc. 苹果公司\n사과 แอปเปิล (บริษัท)\nEntity 2\n…\nFruit 水果\n과일 Φρούτα\nEntity N\nTransformer\nTransformer\nTransformer\n…\n…\n…\nArabic, ar\nChinese, zh\nEnglish, en\nEntityMultilingual DescriptionEntity Description Embedding\nz1\nz2\nz+\nTransformer Relation Encoder\nSubclass of\nzq\ns ~ r ~\n(Apple, Subclass of)         Fruit\nRelation r\nü\nû\nû\n“Apple” Description S \n(Hindi, hi)\nZ1\nZ2\nZ+\nz+\nz2\nzN\nSentence X (English, en) \nLo\n(b) Object Entailment Task\nFigure 1: XLM-K mainly consists of two cross-lingual pre-training tasks: (a) Masked Entity Prediction recognizes the masked\nentity with its knowledge description (the entity Apple is masked in sentence X); (b) Object Entailment predicts the textual\ncontents of object with the combination of subject and relation. All the Transformers are with shared parameters.\nits description. As shown in Figure 1, N multilingual entity\ndescriptions form the candidate list Z= {Z1, Z2, ..., ZN }.\nMasked Entity Prediction Task\nMasked entity prediction task is to encourage the contextu-\nalized entity embedding in a sequence to predict the long\nentity description in the multilingual knowledge base (KB),\nrather than the prediction of entity name. It can help the\ndisambiguation of the different entity with the same entity\nname. For example, as shown in Figure 1.a, the entity name\nof Apple and Apple Inc. are the same in Korean. It helps\nXLM-K learn the diverse implicit knowledge behind the\nmentioned words.\nGiven a sentence X = (x1, x2, ..., xl) from the cross-\nlingual corpus, where X is a sentence withl words from lan-\nguage ulg (e.g. ulg is en, shown in Figure 1.a), and a masked\nmention (s, t, e) (replaced by [MASK]), the task is to recog-\nnize the positive exampleZ+ from a candidate list Z, which\ncontains distracting pages from multiple languages but as-\nsociated with other entities. Z+ = (z1, z2, ..., zm) is the de-\nscription of entity e with m words from language tlg (e.g.\ntlg is ar, shown in Figure 1.a). Note that the description Z+\n(with a maximum 256 tokens) is extracted from the related\nWikipedia page of entitye. After X being fed into the Trans-\nformer encoder, the final hidden state of xs, denotes xt\ns, and\nthe [CLS] from Z+, denotes ez, are further fed into a non-\nlinear projection layer (Chen et al. 2020), respectively:\nzq = W2ReLU(W1xt\ns) (1)\nz+ = W4ReLU(W3ez) (2)\nwhere W1, W3 ∈Rdw×dp and W2, W4 ∈Rdp×dw . Then the\nmasked entity prediction loss Le can be calculated by Eq. 5.\nObject Entailment Task\nThe masked entity prediction task enriches XLM-K with\nsentence-level semantic knowledge, while object entailment\ntask is designed to enhance the structured relation knowl-\nedge. As shown in Figure 1.b, given thesubject and relation,\nthe model is forced to select the object from the candidate\nlist. For the purpose of entity disambiguation, the represen-\ntations of subject and object are also from the long entity\ndescription.\nFormally, given the subject entity’s description sentence\nS = (s1, s2, ..., sl) with l words from language ulg (e.g.\nulg is hi, shown in Figure 1.b), the object entity’s descrip-\ntion sentence Z+ = (z1, z2, ..., zm) with m words from lan-\nguage tlg (e.g. tlg is en, shown in Figure 1.b) and their re-\nlation r (language agnostic), the task is to predict the object\nZ+ from a cross-lingual candidate list Z, based on S and\n10842\nr. Firstly, the relation r is fed into the Relation Encoder (a\nlook-up layer to output the relation embedding), and subject\nentity description sentence S and object entity description\nsentence Z+ is fed into a separate Transformer encoder. We\ncan get the encoded relation er, the whole representation of\nsubject entity description sentence es and object entity de-\nscription sentence ez, based on their [CLS] in the last layer.\nThe joint embedding of es and er is constructed as follows:\nzq = W6ReLU(W5(es + er)) (3)\nwhere W5 ∈ Rdw×dp and W6 ∈ Rdp×dw are trainable\nweights. The object ez is also encoded by a non-linear pro-\njection layer:\nz+ = W8ReLU(W7ez) (4)\nwhere W7 ∈Rdw×dp and W8 ∈Rdp×dw . The object entail-\nment loss Lo is calculated by Eq. 5.\nJoint Pre-training Objective\nAlthough we can have different loss functions to optimise\nXLM-K, we choose contrastive learning due to its promis-\ning results in both visual representations (He et al. 2020;\nChen et al. 2020) and cross-lingual pre-training (Chi et al.\n2021; Pan et al. 2021). Intuitively, by distinguishing the pos-\nitive sample from the negative samples using the contrastive\nloss, the model stores expressive knowledge acquired from\nthe structure data. Formally, the loss can be calculated as:\nLe(and Lo) =−log exp(zqz+)\nPN\nk=1 exp(zqzk)\n(5)\nwhere z+ is the positive sample, zk is the k-th candidate\nsample (encoded by the same way likez+) and N is the size\nof the candidate list Z. To avoid catastrophic forgetting of\nthe learned knowledge from the previous training stage, we\npreserve the multilingual masked language modeling objec-\ntive (MLM) (Devlin et al. 2019), denotesLMLM. As a result,\nthe optimization objective of XLM-K is defined as:\nL= LMLM + Le + Lo (6)\nExperiments\nIn this section, we will introduce implementation details of\nXLM-K, then, evaluate the performance of XLM-K on the\ndownstream tasks. Lastly, we conduct probing experiments\non the pre-trained models to verify the knowledge can be\nstored via the proposed tasks.\nImplementation Details\nData and Model StructureFor the multilingual masked\nlanguage modeling task, we use Common Crawl dataset\n(Wenzek et al. 2020). The Common Crawl dataset is crawled\nfrom the whole web without restriction, which contains all\nthe corpus from the Wikipedia. For the proposed two tasks,\nwe use the corpus for the top 100 languages with the largest\nWikipedias. The settings to balance the instances from dif-\nferent languages are the same as XLM-Rbase (Conneau et al.\n2020). The architecture of XLM-K is set as follows: 768 hid-\nden units, 12 heads, 12 layers, GELU activation, a dropout\nrate of 0.1, with a maximal input length of 256 for the pro-\nposed knowledge tasks, and 512 for MLM task.\nDetails of Pre-trainingWe initialize the model with XLM-\nRbase (Conneau et al. 2020) (was trained on Common\nCrawl), and conduct continual pre-training with the gradient\naccumulation of 8,192 batch size. We utilize Adam (Kingma\nand Ba 2015) as our optimizer. The learning rate starts with\n10k warm-up steps and the peak learning rate is set to 3e-5.\nThe size of candidate list sizeN = 32k. The candidate list is\nimplemented as a queue, randomly initialized at the begin-\nning of the training stage and updated by the newly encoded\nentities. The pre-training experiments are conducted using\n16 V100 GPUs.\nDetails of Fine-TuningWe follow Liang et al. 2020 in these\nfine-tuning settings. In detail, we use Adam optimizer with\nwarm-up and only fine-tune XLM-K on the English training\nset. For MLQA, we fine-tune 2 epochs, with the learning\nrate set as 3e-5 and batch size of 12. For NER, we fine-tune\n20 epochs, with the learning rate set as 5e-6 and batch size of\n32. For XNLI, we fine-tune 10 epochs and the other settings\nare the same as for NER. We test all the fine-tuned models\non dev split of all languages for each fine-tuning epoch and\nselect the model based on the best average performance on\nthe dev split of all languages. To achieve a convincing com-\nparison, we run the fine-tuning experiments with 4 random\nseeds and report both the average and maximum results on\nall downstream tasks. We also run our baseline XLM-Rbase\nwith the same 4 seeds and report average results.\nDetails of ProbingFollowing Petroni et al. (2019), we con-\nduct probing analysis directly on the pre-trained models\nwithout any fine-tuning. The probing corpus are from four\nsources: Google-RE 1, T-REx (Elsahar et al. 2018), Con-\nceptNet (Speer and Havasi 2012) and SQuAD (Rajpurkar\net al. 2016). Except that ConceptNet tests for commonsense\nknowledge, others are all designed to probe Wiki-related\nknowledge.\nDownstream Task Evaluation\nTo evaluate the performance of our model using downstream\ntasks, we conduct experiments on MLQA, NER and XNLI.\nMLQA and NER are entity-related tasks, and XNLI is a\nwidely-used cross-lingual benchmark. Without using bilin-\ngual data in pre-training, we achieve new state-of-the-art re-\nsults on these three tasks. For the convenience of reference,\nwe display the results of the bilingual data relevant meth-\nods, namely the recently released models InfoXLM (Chi\net al. 2021) and ERNIE-M (Ouyang et al. 2021), in Table\n1 and Table 3 and omit the analysis. Applying bilingual data\nresources to XLM-K is left as future work. In the follow-\ning section, MEP means the ablation model of Masked En-\ntity Prediction + MLM and OE means Object Entailment +\nMLM.\nMLQA MLQA (Lewis et al. 2020) is a multilingual ques-\ntion answering dataset, which covers 7 languages including\nEnglish, Spanish, German, Arabic, Hindi, Vietnamese and\nChinese. As a big portion of questions in MLQA are factual\n1https://code.google.com/archive/p/relation-extraction-corpus/\n10843\nModel en es de ar hi vi zh Avg\nmBERT 77.7/65.2 64.3/46.6 57.9/44.3 45.7/29.8 43.8/29.7 57.1/38.6 57.5/37.3 57.7/41.6\nXLM 74.9/62.4 68.0/49.8 62.2/47.6 54.8/36.3 48.8/27.3 61.4/41.8 61.1/39.6 61.6/43.5\nmBERT + PPA† 79.8/ - 67.7/ - 62.3/ - 53.8/ - 57.9/ - - / - 61.5/ - 63.8/ -\nUnicoder 80.6/ - 68.6/ - 62.7/ - 57.8/ - 62.7/ - 67.5/ - 62.1/ - 66.0/ -\nXLM-Rbase 80.1/67.0 67.9/49.9 62.1/47.7 56.4/37.2 60.5/44.0 67.1/46.3 61.4/38.5 65.1/47.2\nMEP (avg) 80.6/67.5 68.7/50.9 62.8/48.2 59.0/39.9 63.1/46.1 68.2/47.5 62.1/38.1 66.4/48.3\nOE (avg) 80.8/67.8 69.1/51.2 63.2/48.6 59.0/39.6 63.7/46.3 68.5/47.3 63.0/39.5 66.7/48.6\nXLM-K (avg) 80.8/67.7 69.3/51.6 63.2/48.9 59.8/40.5 64.3/46.9 69.0/48.0 63.1/38.8 67.1/48.9\nXLM-K (max) 80.8/67.9 69.2/52.1 63.8/49.2 60.0/41.1 65.3/47.6 70.1/48.6 63.8/39.7 67.7/49.5\nwith Bilingual Data\nInfoXLM 81.3/68.2 69.9/51.9 64.2/49.6 60.1/40.9 65.0/47.5 70.0/48.6 64.7/41.2 67.9/49.7\nERNIE-M 81.6/68.5 70.9/52.6 65.8/50.7 61.8/41.9 65.4/47.5 70.0/49.2 65.6/41.0 68.7/50.2\nTable 1: The results of MLQA F1/EM (exact match) scores on each language († means Post-Pre-training Alignment). The\nmodels in the second block are our ablation models MEP and OE. We run our model and ablation models four times with\ndifferent seeds, where avg means the average results and max means the maximum results selected by the Avg metrics.\nModel en es de nl Avg\nmBERT † 90.6 75.4 69.2 77.9 78.2\nXLM-Rbase\n† 90.9 75.2 70.4 79.5 79.0\nMEP (avg) 90.6 75.6 72.3 80.2 79.6\nOE (avg) 90.9 76.0 72.7 80.1 79.9\nXLM-K (avg) 90.7 75.2 72.9 80.3 79.8\nXLM-K (max) 90.7 76.6 73.3 80.0 80.1\nTable 2: The results of NER F1 scores on each language,\nwhere †means the results from (Liang et al. 2020). The mod-\nels in the second block are our ablation models. The meaning\nof avg and max are the same as Table 1.\nones, we use it to evaluate XLM-K that is pre-trained using\nthe multilingual knowledge.\nThe results on MLQA are shown in Table 1, we compare\nour model with mBERT (Lewis et al. 2020), XLM (Lewis\net al. 2020), mBERT + PPA (Pan et al. 2021), Unicoder\n(Huang et al. 2019) and XLM-R base (Conneau et al. 2020).\nSince F1 and EM scores have similar observations, we take\nF1 scores for analysis:\n(1) The Effectiveness of XLM-K.For the avg report, XLM-\nK achieves 67.1 averaged accuracy on F1 score, outper-\nforming the baseline model XLM-R base by 2.0. For the\nmax report, the model can further obtain 0.6 additional gain\nover the avg report. This clearly illustrate the superiority of\nXLM-K on MLQA. In addition, the model MEP and OE\nprovide 1.3 and 1.6 improvements over XLM-Rbase, respec-\ntively, which reveals that each task can capture MLQA’s\ntask-specific knowledge successfully.\n(2) The Ablation Analysis of XLM-K. The models in the\nsecond block are the ablation models. Compared with the\nablation models, XLM-K outperforms each model by 0.7\nand 0.4 on Avg metrics. It indicates that the masked entity\nprediction and object entailment has complementary advan-\ntages on MLQA task, and the best result is achieved when\nusing them together.\nNER The cross-lingual NER (Liang et al. 2020) dataset cov-\ners 4 languages, including English, Spanish, German and\nDutch, and 4 types of named entities, namely Person, Lo-\ncation, Organization and Miscellaneous. As shown in Table\n2, compared with baseline model XLM-R base, XLM-K im-\nproves the Avg score to 79.8 on average and 80.1 on maxi-\nmum. It verifies the effectiveness of XLM-K when solving\nNER task. Meanwhile, the results of MEP and OE are also\nincreased by 0.6 and 0.9 on Avg F1 score. It displays that the\nentity-related pre-training task has significant improvements\non the entity-related downstream tasks.\nXNLI The XNLI (Conneau et al. 2018) is a popular eval-\nuation dataset for cross-lingual NLI which contains 15 lan-\nguages. It’s a textual inference tasks and not rerely relied\non knowledge base. We present the results, comparing with\nmBERT (Conneau et al. 2020), XLM (Conneau et al. 2020),\nUnicoder (Huang et al. 2019), AMBER (Hu et al. 2021) and\nXLM-Rbase (Conneau et al. 2020), in Table 3 with following\nobservations:\n(1) The Effectiveness of XLM-K. Although XNLI is not\nan entity or relation -aware multilingual task, our model\nobtains a 0.6 gain comparing to the baseline model XLM-\nRbase. Each ablation model of MEP and OE improve by\n0.4. These gains are marginal compared to MLQA and NER.\nThis shows that our model is mainly works on knowledge-\naware tasks. On other tasks, it won’t harm the performance\nand even could marginally help.\n(2) The Ablation Analysis of XLM-K.The ablation models\nof XLM-K on XNLI have similar results on XNLI, which in-\ncreasing by 0.4 compared to the XLM-Rbase baseline 74.2. It\nproves each task has its own contribution to the overall im-\nprovements. Meanwhile, the ablation models still have 0.2\ngap to the XLM-K, which implies the advantages towards\nthe combination of these two tasks.\nAblation Study\nThe ablation analysis mentioned above demonstrates the su-\nperiority of the combination scheme of the proposed two\npre-training tasks. In this section, we investigate the effec-\ntiveness of our key components.\nThe Effectiveness of Knowledge TasksOur baseline model\nXLM-Rbase (Conneau et al. 2020) was trained on Common\nCrawl dataset (Wenzek et al. 2020), which covers our train-\ning data Wikipedia. As shown in Table 1, 2, 3 and 5, our\n10844\nModel en fr es de el bg ru tr ar vi th zh hi sw ur Avg\nmBERT 82.1 73.8 74.3 71.1 66.4 68.9 69.0 61.6 64.9 69.5 55.8 69.3 60.0 50.4 58.0 66.3\nXLM 85.0 78.7 78.9 77.8 76.6 77.4 75.3 72.5 73.1 76.1 73.2 76.5 69.6 68.4 67.3 75.1\nUnicoder 82.9 74.7 75.0 71.6 71.6 73.2 70.6 68.7 68.5 71.2 67.0 69.7 66.0 64.1 62.5 70.5\nAMBER 84.7 76.6 76.9 74.2 72.5 74.3 73.3 73.2 70.2 73.4 65.7 71.6 66.2 59.9 61.0 71.6\nXLM-Rbase 84.6 78.2 79.2 77.0 75.9 77.5 75.5 72.9 72.1 74.8 71.6 73.7 69.8 64.7 65.1 74.2\nMEP † 84.9 78.5 78.8 77.0 76.2 78.1 76.1 73.4 72.0 75.2 72.4 74.7 69.8 65.7 66.0 74.6\nOE † 84.4 78.1 78.8 77.1 75.9 78.0 75.9 73.1 72.5 75.3 73.0 74.5 70.1 65.4 67.3 74.6\nXLM-K † 84.5 78.2 78.8 77.1 76.2 78.2 76.1 73.3 72.5 75.7 72.8 74.9 70.3 65.7 67.4 74.8\nXLM-K ∗ 84.9 79.1 79.2 77.9 77.2 78.8 77.4 73.7 73.3 76.8 73.1 75.6 72.0 65.8 68.0 75.5\nwith Bilingual Data\nInfoXLM 86.4 80.6 80.8 78.9 77.8 78.9 77.6 75.6 74.0 77.0 73.7 76.7 72.0 66.4 67.1 76.2\nERNIE-M 85.5 80.1 81.2 79.2 79.1 80.4 78.1 76.8 76.3 78.3 75.8 77.4 72.9 69.5 68.8 77.3\nTable 3: The results of XNLI test accuracy on 15 languages. The models in the second block are our ablation models, where †\nmeans avg results and ∗means max results. The meaning of avg and max are the same as Table 1.\nModel MLQA NER XNLI\nXLM-Rbase 65.1 79.0 74.2\nXLM-K w/o knowledge tasks 65.6 79.0 74.5\nMEP w/o multilingual description 65.9 79.3 74.5\nMEP 66.4 79.6 74.6\nOE w/o contrastive loss 65.7 79.6 74.5\nOE 66.7 79.9 74.6\nXLM-K 67.1 79.8 74.8\nTable 4: The ablation results on MLQA (F1 scores), NER\n(F1 scores) and XNLI (test accuracy) upon Avg. All the re-\nsults are the avg mentioned in Table 1.\nmodel XLM-K outperforms XLM-Rbase consistently. More-\nover, we replace Le and Lo in Eq. 6 with the MLM loss on\nmultilingual Wikipedia entity descriptions. The results are\nshown in the second block of Table 4. Without the knowl-\nedge tasks, the performance of XLM-K w/o knowledge tasks\ndrops by 1.5, 0.8 and 0.3 on MLQA, NER and XNLI re-\nspectively. It proves that the improvements are from the de-\nsigned knowledge tasks, rather than the domain adaptation\nto Wikipedia. We will design more knowledge-related tasks\nin the future.\nThe Effectiveness of Multilingual Entity DescriptionAs\nmentioned in above, the entity knowledge, i.e. the entity-\nrelated Wikipedia page, is converted to different language\nresources compared to the given entity. The same operation\nis conducted on the subject and object in triplets, leading\nto the multilingual resources between the subject and the\nobject. To study how this operation affects model perfor-\nmance, we report the results on the third block of Table 4.\nWithout multilingual entity description operation, the per-\nformance of MEP w/o multilingual description drops by 0.5,\n0.3 and 0.1 on MLQA, NER and XNLI respectively. It illus-\ntrates that the effectiveness of multilingual entity descrip-\ntion. On the other hand, compared to baseline XLM-R base,\nthe model MEP w/o multilingual description still achieves\n0.8, 0.3 and 0.3 improvements on MLQA, NER and XNLI,\nrespectively, which reflects that applying entity description\nexpansion without cross-lingual information in pre-training\nis still consistently effective for all downstream tasks.\nThe Effectiveness of Optimization StrategyA natural idea\nto introducing structural knowledge into the pre-trained lan-\nguage model is to classify therelation by the subject and ob-\nject from the triplets. Motivated by this opinion, we display\nthe results on the forth block in Table 4. The model of OE\nw/o contrastive loss classifies the relation by the concate-\nnation of subject and object with Cross Entropy loss. With-\nout contrastive loss, the performance drops by 1.0, 0.3 and\n0.1 on MLQA, NER and XNLI respectively. This indicates\nthe advantages of utilizing the contrastive learning towards\na better cross-lingual model. We conjecture contrastive loss\nintroduces a more challenging task than classification task.\nOn the other hand, OE w/o contrastive loss improves the per-\nformance of baseline model XLM-R base from 65.1 to 65.7,\n79.0 to 79.6 and 74.2 to 74.5 in MLQA, NER and XNLI,\nrespectively. This observation certifies the importance of the\nstructure knowledge in cross-lingual pre-training, though via\nan ordinary optimization strategy.\nProbing Analysis\nWe conduct a knowledge-aware probing task based on\nLAMA (Petroni et al. 2019). Note that the Probing is an\nanalysis experiment to evaluate how well the pre-trained\nlanguage model can store the desired (Wiki) knowledge, and\nto explain the reason for the improvements on downstream\ntasks by the proposed tasks. It means that the probing is not\nthe SOTA comparison experiment. We leave the analysis\non recent multilingual LAMA (Jiang et al. 2020; Kassner,\nDufter, and Sch¨utze 2021) as our future work.\nIn LAMA, factual knowledge, such as <Jack, born-in,\nCanada>, is firstly converted into cloze test question, such\nas “Jack was born in\n”. Then, a pre-trained language\nmodel is asked to predict the answer by filling in the blank\nof the question. There are 4 sub-tasks in the LAMA dataset.\nThe first is Google-RE, which contains questions generated\nbased on around 60k facts extracted from Wikidata and cov-\ners 3 relations. The second is T-REx, which contains ques-\ntions generated based on a subset of Wikidata triples as\nwell, but covers more relations (i.e. 41 relations in total).\nThe third is ConceptNet, which contains questions gener-\nated based on a commonsense knowledge base (Speer, Chin,\n10845\nCorpus Relation Statistics XLM-R base XLM-K w/o K MEP OE XLM-K\n# Facts # Rel P@1 P@1 P@1 P@1 P@1\nGoogle-RE\nbirth-place 2937 1 9.3 9.8 10.1 15.0 15.6\nbirth-date 1825 1 0.6 0.7 0.7 0.9 1.0\ndeath-place 765 1 8.0 9.1 13.4 13.8 17.0\nTotal 5527 3 7.4 7.8 8.0 9.9 11.2\nT-REx\n1-1 937 2 48.4 49.9 52.5 50.5 62.0\nN-1 20006 23 22.0 25.1 27.3 21.9 29.4\nN-M 13096 16 17.9 21.5 25.6 22.1 26.1\nTotal 34039 41 21.7 22.8 27.9 23.4 29.7\nConceptNet Total 11458 16 18.8 14.2 12.0 17.6 15.7\nSQuAD Total 305 - 5.5 6.4 10.1 9.7 11.5\nTable 5: The results of LAMA probing mean precision at one (P@1) for the baseline XLM-Rbase, XLM-K w/o K (XLM-K w/o\nknowledge tasks), MEP, OE and XLM-K. We also reported the statistics of the facts number and relation types involved by the\nreferred corpus.\nand Havasi 2017). The last is a popular open-domain ques-\ntion answering dataset SQuAD. The number of facts and\nrelation types covered by the each sub-task are shown in the\nTable 5 column Statistics.\nEvaluation on LAMA Probing TaskThe LAMA prob-\ning task is conducted on the baseline model XLM-R base,\nour two ablation models MEP (Masked Entity Prediction +\nMLM) and OE (Object Entailment + MLM), XLM-K w/o\nknowledge tasks, and our full model XLM-K. The results\nare shown in Table 5.\n•Comparison Results\nThe XLM-K w/o knowledge tasks improves the per-\nformance slightly (in Google-RE, T-REx and SQuAD). It\nproves the improvements are from the designed tasks, rather\nthan the domain adaptation to Wikipedia. We will detail the\nobservations of the results on each corpus.\nGoogle-RE XLM-K outperforms all the other models by\na substantial margin, especially the baseline model XLM-\nRbase. It is worth noting that the two ablation models,\nnamely MEP and OE in Table 5, realizes 0.6 and 2.5 gain re-\nspectively, which proves each knowledge-aware pre-training\ntask can independently help pre-trained models to embed\nfactual knowledge in a better way.\nT-REx This task contains more facts and relations com-\npared to Google-RE. XLM-K boosts the Total metrics from\n21.7 to 29.7. The model MEP and model OE improves the\nscores by 6.2 and 1.7, respectively. These results further\ndemonstrate the effectiveness of XLM-K on knowledge-\naware tasks.\nConceptNet The ConceptNet corpus calls for the com-\nmonsense knowledge, which is a different knowledge source\nfrom Wikipedia. In this work, we mainly take Wikipedia\nknowledge into consideration, which can explain the worse\nperformance on ConceptNet. Extending our model to cap-\nture more knowledge resources, such as commonsense\nknowledge, is our future work. Meanwhile, we notice that\nthe performance of model OE decreases slightly compared\nto model MEP and XLM-K. The reason for this phe-\nnomenon may lie in that the ConceptNet is collected as the\ntriplets-style and the relation prediction task has a great skill\nto handle the relation structure knowledge.\nCloze Statement XLM-R base XLM-K\nPhones may be made of . metal plastic\nGnocchi is a kind of . beer food\nTasila Mwale(born ). in 1984\nTable 6: Case study of LAMA probing, where the object la-\nbel is the ground truth of the given statement. We compare\nthe prediciton from our baseline XLM-R base and our full\nmodel XLM-K. The subject is highlighted with bold and\nthe object is highlighted with italic.\nSQuAD To investigate the performance of our model\non open-domain cloze-style question answering corpus, we\nfurther evaluate the results on SQuAD. Again, our model\nachieves a great success on SQuAD. In detail, XLM-K\nachieves 11.5, which has a 6.0 gain over XLM-Rbase.\n•Case Study\nTo make the analysis more explicit, as shown in Table 6,\nwe study three cases. Take the last two cases for example, to\nfill in the blank of “Gnoccchi is a kind of\n.”, XLM-Rbase\nfails to answer the question, while XLM-K successfully ac-\ncomplishes the blank with “food ”. In the last case “Tasila\nMwale (born\n).”, XLM-R base has no idea towards this\nfact and only predicts the answer with “in” to complete the\nphrase “born in”. XLM-K answers this question excellently\nvia the prediction of “1984”. It confirms that the XLM-K is\nindeed equipped with more specific knowledge.\nConclusion\nIn this work, we present a new cross-lingual language model\nXLM-K to associate pre-training language model with more\nspecific knowledge across multiple languages. Specifically,\nthe knowledge is obtained via two knowledge-related tasks:\nmaksed entity prediction and object entailment. Experimen-\ntal results on three benchmark datasets clearly demonstrate\nthe superiority of XLM-K. Our systematic analysis of the\nXLM-K advocates that XLM-K has great advantages in\nknowledge intensive tasks. Incorporating more diverse mul-\ntilingual knowledge and jointing more advanced pre-training\nschemes will be addressed in future work.\n10846\nReferences\nBroscheit, S. 2019. Investigating Entity Knowledge in\nBERT with Simple Neural End-To-End Entity Linking. In\nCoNLL, 677–685.\nCao, Y .; Hou, L.; Li, J.; Liu, Z.; Li, C.; Chen, X.; and Dong,\nT. 2018a. Joint Representation Learning of Cross-lingual\nWords and Entities via Attentive Distant Supervision. In\nEMNLP, 227–237.\nCao, Y .; Hou, L.; Li, J.; Liu, Z.; Li, C.; Chen, X.; and Dong,\nT. 2018b. Joint Representation Learning of Cross-lingual\nWords and Entities via Attentive Distant Supervision. In\nEMNLP, 227–237.\nCao, Y .; Huang, L.; Ji, H.; Chen, X.; and Li, J. 2017. Bridge\ntext and knowledge by learning multi-prototype entity men-\ntion embedding. In ACL, 1623–1633.\nChen, M.; Shi, W.; Zhou, B.; and Roth, D. 2021. Cross-\nlingual Entity Alignment with Incidental Supervision. In\nEACL, 645–658.\nChen, T.; Kornblith, S.; Norouzi, M.; and Hinton, G. 2020.\nA simple framework for contrastive learning of visual repre-\nsentations. In ICML, 1597–1607.\nChi, Z.; Dong, L.; Wei, F.; Yang, N.; Singhal, S.; Wang,\nW.; Song, X.; Mao, X.-L.; Huang, H.; and Zhou, M. 2021.\nInfoxlm: An information-theoretic framework for cross-\nlingual language model pre-training. InNAACL, 3576–3588.\nConneau, A.; Khandelwal, K.; Goyal, N.; Chaudhary, V .;\nWenzek, G.; Guzm ´an, F.; Grave, E.; Ott, M.; Zettlemoyer,\nL.; and Stoyanov, V . 2020. Unsupervised cross-lingual rep-\nresentation learning at scale. In ACL, 8440–8451.\nConneau, A.; and Lample, G. 2019. Cross-lingual language\nmodel pretraining. In NeurIPS, 7059–7069.\nConneau, A.; Rinott, R.; Lample, G.; Williams, A.; Bow-\nman, S.; Schwenk, H.; and Stoyanov, V . 2018. XNLI: Eval-\nuating Cross-lingual Sentence Representations. In EMNLP,\n2475–2485.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In NAACL, 4171–4186.\nElsahar, H.; V ougiouklis, P.; Remaci, A.; Gravier, C.; Hare,\nJ.; Laforest, F.; and Simperl, E. 2018. T-REx: A Large\nScale Alignment of Natural Language with Knowledge Base\nTriples. In LREC, 3448–3452.\nFan, Y .; Liang, Y .; Muzio, A.; Hassan, H.; Li, H.; Zhou, M.;\nand Duan, N. 2021. Discovering Representation Sprachbund\nFor Multilingual Pre-Training. In Findings of EMNLP, 881–\n894.\nF´evry, T.; Soares, L. B.; FitzGerald, N.; Choi, E.; and\nKwiatkowski, T. 2020. Entities as Experts: Sparse Memory\nAccess with Entity Supervision. In EMNLP, 4937–4951.\nHan, X.; Liu, Z.; and Sun, M. 2016. Joint representation\nlearning of text and knowledge for knowledge graph com-\npletion. In CORR.\nHayashi, H.; Hu, Z.; Xiong, C.; and Neubig, G. 2020. Latent\nrelation language models. In AAAI, 7911–7918.\nHe, K.; Fan, H.; Wu, Y .; Xie, S.; and Girshick, R. 2020.\nMomentum contrast for unsupervised visual representation\nlearning. In CVPR, 9729–9738.\nHu, J.; Johnson, M.; Firat, O.; Siddhant, A.; and Neubig, G.\n2021. Explicit Alignment Objectives for Multilingual Bidi-\nrectional Encoders. In NAACL, 3633–3643.\nHuang, H.; Liang, Y .; Duan, N.; Gong, M.; Shou, L.; Jiang,\nD.; and Zhou, M. 2019. Unicoder: A Universal Language\nEncoder by Pre-training with Multiple Cross-lingual Tasks.\nIn EMNLP, 2485–2494.\nJiang, Z.; Anastasopoulos, A.; Araki, J.; Ding, H.; and Neu-\nbig, G. 2020. X-FACTR: Multilingual Factual Knowledge\nRetrieval from Pretrained Language Models. In EMNLP,\n5943–5959.\nKassner, N.; Dufter, P.; and Sch¨utze, H. 2021. Multilingual\nLAMA: Investigating Knowledge in Multilingual Pretrained\nLanguage Models. In EACL, 3250–3258.\nKingma, D. P.; and Ba, J. 2015. Adam: A method for\nstochastic optimization. In ICLR.\nLewis, P.; Oguz, B.; Rinott, R.; Riedel, S.; and Schwenk, H.\n2020. MLQA: Evaluating Cross-lingual Extractive Question\nAnswering. In ACL, 7315–7330.\nLiang, Y .; Duan, N.; Gong, Y .; Wu, N.; Guo, F.; Qi, W.;\nGong, M.; Shou, L.; Jiang, D.; Cao, G.; Fan, X.; Zhang, R.;\nAgrawal, R.; Cui, E.; Wei, S.; Bharti, T.; Qiao, Y .; Chen, J.-\nH.; Wu, W.; Liu, S.; Yang, F.; Campos, D.; Majumder, R.;\nand Zhou, M. 2020. Xglue: A new benchmark dataset for\ncross-lingual pre-training, understanding and generation. In\nEMNLP, 6008–6018.\nLiu, W.; Zhou, P.; Zhao, Z.; Wang, Z.; Ju, Q.; Deng, H.; and\nWang, P. 2020. K-bert: Enabling language representation\nwith knowledge graph. In AAAI, 2901–2908.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .\n2019. Roberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nOuyang, X.; Wang, S.; Pang, C.; Sun, Y .; Tian, H.; Wu,\nH.; and Wang, H. 2021. ERNIE-M: Enhanced Multilingual\nRepresentation by Aligning Cross-lingual Semantics with\nMonolingual Corpora. In EMNLP, 27–38.\nPan, L.; Hang, C.-W.; Qi, H.; Shah, A.; Yu, M.; and Potdar,\nS. 2021. Multilingual BERT Post-Pretraining Alignment. In\nNAACL, 210–219.\nPan, X.; Gowda, T.; Ji, H.; May, J.; and Miller, S. 2019.\nCross-lingual Joint Entity and Word Embedding to Improve\nEntity Linking and Parallel Sentence Mining. In Workshop\non DeepLo, 56–66.\nPetroni, F.; Rockt¨aschel, T.; Lewis, P.; Bakhtin, A.; Wu, Y .;\nMiller, A. H.; and Riedel, S. 2019. Language Models as\nKnowledge Bases? In EMNLP, 2463–2473.\nRajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016.\nSQuAD: 100,000+ Questions for Machine Comprehension\nof Text. In EMNLP, 2383–2392.\nRosset, C.; Xiong, C.; Phan, M.; Song, X.; Bennett, P.; and\nTiwary, S. 2020. Knowledge-Aware Language Model Pre-\ntraining. arXiv preprint arXiv:2007.00655.\n10847\nRuder, S.; Vuli´c, I.; and Søgaard, A. 2019. A survey of cross-\nlingual word embedding models. Journal of Artificial Intel-\nligence Research, 65: 569–631.\nSpeer, R.; Chin, J.; and Havasi, C. 2017. Conceptnet 5.5:\nAn open multilingual graph of general knowledge. In AAAI,\n4444–4451.\nSpeer, R.; and Havasi, C. 2012. Representing General Rela-\ntional Knowledge in ConceptNet 5. In LREC, 3679–3686.\nStali¯unait˙e, I.; and Iacobacci, I. 2020. Compositional and\nLexical Semantics in RoBERTa, BERT and DistilBERT: A\nCase Study on CoQA. In EMNLP, 7046–7056.\nToutanova, K.; Chen, D.; Pantel, P.; Poon, H.; Choudhury, P.;\nand Gamon, M. 2015. Representing text for joint embedding\nof text and knowledge bases. In EMNLP, 1499–1509.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In NeurIPS, 5998–6008.\nVrandeˇci´c, D.; and Kr ¨otzsch, M. 2014. Wikidata: a free\ncollaborative knowledgebase. Communications of the ACM,\n57(10): 78–85.\nVuli´c, I.; and Moens, M.-F. 2013. Cross-Lingual Seman-\ntic Similarity of Words as the Similarity of Their Semantic\nWord Responses. In NAACL, 106–116.\nWang, R.; Tang, D.; Duan, N.; Wei, Z.; Huang, X.; Ji, J.;\nCao, G.; Jiang, D.; and Zhou, M. 2021a. K-Adapter: Infus-\ning Knowledge into Pre-Trained Models with Adapters. In\nFindings of ACL, 1405–1418.\nWang, X.; Gao, T.; Zhu, Z.; Zhang, Z.; Liu, Z.; Li, J.; and\nTang, J. 2021b. KEPLER: A unified model for knowledge\nembedding and pre-trained language representation. Trans-\nactions of the Association for Computational Linguistics, 9:\n176–194.\nWang, Z.; and Li, J.-Z. 2016. Text-enhanced representation\nlearning for knowledge graph. In IJCAI, 4–17.\nWang, Z.; Zhang, J.; Feng, J.; and Chen, Z. 2014. Knowl-\nedge graph and text jointly embedding. In EMNLP, 1591–\n1601.\nWenzek, G.; Lachaux, M.-A.; Conneau, A.; Chaudhary, V .;\nGuzman, F.; Joulin, A.; and Grave, E. 2020. Ccnet: Extract-\ning high quality monolingual datasets from web crawl data.\nIn LREC, 4003–4012.\nWu, J.; Xie, R.; Liu, Z.; and Sun, M. 2016. Knowledge rep-\nresentation via joint learning of sequential text and knowl-\nedge graphs. In CORR.\nXiong, W.; Du, J.; Wang, W. Y .; and Stoyanov, V . 2020.\nPretrained encyclopedia: Weakly supervised knowledge-\npretrained language model. In ICLR.\nYamada, I.; Shindo, H.; Takeda, H.; and Takefuji, Y . 2016.\nJoint Learning of the Embedding of Words and Entities for\nNamed Entity Disambiguation. In CoNLL, 250–259.\nYang, J.; Ma, S.; Zhang, D.; Wu, S.; Li, Z.; and Zhou, M.\n2020. Alternating Language Modeling for Cross-Lingual\nPre-Training. In AAAI, 9386–9393.\nZhang, H.; Liu, Z.; Xiong, C.; and Liu, Z. 2020. Grounded\nConversation Generation as Guided Traverses in Common-\nsense Knowledge Graphs. In ACL, 2031–2043.\nZhang, Z.; Han, X.; Liu, Z.; Jiang, X.; Sun, M.; and Liu,\nQ. 2019. ERNIE: Enhanced Language Representation with\nInformative Entities. In ACL, 1441–1451.\n10848"
}