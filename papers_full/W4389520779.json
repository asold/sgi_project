{
    "title": "StructGPT: A General Framework for Large Language Model to Reason over Structured Data",
    "url": "https://openalex.org/W4389520779",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2322417816",
            "name": "Jinhao Jiang",
            "affiliations": [
                "Beijing Institute of Big Data Research",
                "Renmin University of China"
            ]
        },
        {
            "id": "https://openalex.org/A2102854352",
            "name": "Kun Zhou",
            "affiliations": [
                "Beijing Institute of Big Data Research",
                "Renmin University of China"
            ]
        },
        {
            "id": "https://openalex.org/A5018732664",
            "name": "Zican Dong",
            "affiliations": [
                "Renmin University of China"
            ]
        },
        {
            "id": "https://openalex.org/A3016064066",
            "name": "Keming Ye",
            "affiliations": [
                "University of Electronic Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A2098250721",
            "name": "Zhao Xin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3212238123",
            "name": "Ji-Rong Wen",
            "affiliations": [
                "Beijing Institute of Big Data Research",
                "Renmin University of China"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963899988",
        "https://openalex.org/W2008742021",
        "https://openalex.org/W3207976211",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2890961898",
        "https://openalex.org/W4362515116",
        "https://openalex.org/W2963448850",
        "https://openalex.org/W3184222203",
        "https://openalex.org/W4320853865",
        "https://openalex.org/W4280516735",
        "https://openalex.org/W4309591663",
        "https://openalex.org/W3116847845",
        "https://openalex.org/W4361019453",
        "https://openalex.org/W4366388740",
        "https://openalex.org/W3186545525",
        "https://openalex.org/W4386566488",
        "https://openalex.org/W3170721718",
        "https://openalex.org/W4318908878",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4385567284",
        "https://openalex.org/W3104616515",
        "https://openalex.org/W3103667349",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W4383982659",
        "https://openalex.org/W2971822538",
        "https://openalex.org/W4385573708",
        "https://openalex.org/W4287888135",
        "https://openalex.org/W4229005866",
        "https://openalex.org/W3099655892",
        "https://openalex.org/W3034862985",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W4226053975",
        "https://openalex.org/W2751448157",
        "https://openalex.org/W4289494028",
        "https://openalex.org/W3035140194",
        "https://openalex.org/W4310744116",
        "https://openalex.org/W4303649020",
        "https://openalex.org/W4322718421",
        "https://openalex.org/W2755637027",
        "https://openalex.org/W4320165837",
        "https://openalex.org/W4385570040",
        "https://openalex.org/W3190271517",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W4309088836",
        "https://openalex.org/W2891991579",
        "https://openalex.org/W4385572953",
        "https://openalex.org/W2511149293"
    ],
    "abstract": "In this paper, we aim to improve the reasoning ability of large language models (LLMs) over structured data in a unified way. Inspired by the studies on tool augmentation for LLMs, we develop an Iterative Reading-then-Reasoning (IRR) framework to solve question answering tasks based on structured data, called StructGPT. In this framework, we construct the specialized interfaces to collect relevant evidence from structured data (i.e., reading), and let LLMs concentrate on the reasoning task based on the collected information (i.e., reasoning). Specially, we propose an invoking-linearization-generation procedure to support LLMs in reasoning on the structured data with the help of the interfaces. By iterating this procedure with provided interfaces, our approach can gradually approach the target answers to a given query. Experiments conducted on three types of structured data show that StructGPT greatly improves the performance of LLMs, under the few-shot and zero-shot settings.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 9237–9251\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nStructGPT: A General Framework for Large Language Model\nto Reason over Structured Data\nJinhao Jiang1,3∗, Kun Zhou2,3∗, Zican Dong1, Keming Ye4,\nWayne Xin Zhao1,3†and Ji-Rong Wen1,2,3\n1Gaoling School of Artificial Intelligence, Renmin University of China.\n2School of Information, Renmin University of China.\n3Beijing Key Laboratory of Big Data Management and Analysis Methods.\n4University of Electronic Science and Technology of China.\njiangjinhao@ruc.edu.cn, batmanfly@gmail.com\nAbstract\nIn this paper, we aim to improve the reason-\ning ability of large language models (LLMs)\nover structured data in a unified way. In-\nspired by the studies on tool augmentation for\nLLMs, we develop an Iterative Reading-then-\nReasoning (IRR) framework to solve question\nanswering tasks based on structured data, called\nStructGPT. In this framework, we construct\nthe specialized interfaces to collect relevant ev-\nidence from structured data (i.e., reading), and\nlet LLMs concentrate on the reasoning task\nbased on the collected information ( i.e., rea-\nsoning). Specially, we propose an invoking-\nlinearization-generation procedure to support\nLLMs in reasoning on the structured data with\nthe help of the interfaces. By iterating this\nprocedure with provided interfaces, our ap-\nproach can gradually approach the target an-\nswers to a given query. Experiments conducted\non three types of structured data show that\nStructGPT greatly improves the performance\nof LLMs, under the few-shot and zero-shot set-\ntings. Our codes and data are publicly available\nat https://github.com/RUCAIBox/StructGPT.\n1 Introduction\nRecently, large language models (LLMs) (Brown\net al., 2020; Zhao et al., 2023) have made remark-\nable advancements in the NLP field. Existing\nwork (Ouyang et al., 2022a; Zhang et al., 2022) has\ndemonstrated that LLMs (e.g., ChatGPT or GPT-\n4 (OpenAI, 2023)) have strong zero-shot capability\nto solve a broad range of tasks using specially de-\nsigned prompts, without task-specific fine-tuning.\nDespite the successes, recent work has also re-\nvealed that LLMs may generate unfaithful informa-\ntion in conflict with the factual knowledge (Li et al.,\n2023), and also fall short of mastering domain-\nspecific or real-time knowledge (Schick et al., 2023;\nPeng et al., 2023). A direct solution to the above\n∗Equal contributions.\n†Corresponding author.\nissues is to augment LLMs with external knowl-\nedge resources, so as to amend the incorrect gen-\nerations. Among these resources, structured data\n(e.g., knowledge graphs and databases), has been\nwidely used as the carrier of the required knowl-\nedge for LLMs. Unlike plain text, structured data is\norganized in a standardized format, conforming to\nsome logical data model. For example, knowledge\ngraphs (KGs) are often organized as fact triples\nthat state the relations between head entities and\ntail entities, and data tables are organized in the\nform of column-indexed records by rows. How-\never, as structured data has special data formats\nor schemas that LLMs have not seen during pre-\ntraining, they may be not fully grasped or under-\nstood by LLMs (Wei et al., 2021). A straightfor-\nward way to solve this problem is to linearize the\nstructured data into a sentence that LLMs can well\nunderstand. While, the amount of structured data\nis often vast, making it infeasible to include all the\ndata records in the input prompt.\nRegarding the above challenges, we are inspired\nby the tool manipulation strategy for augmenting\nthe abilities of LLMs (Schick et al., 2023; Nakano\net al., 2021). Our basic idea is to incorporate spe-\ncialized interfaces (e.g., extracting columns from\ntables) to manipulate the structured data records.\nWith these interfaces, we can effectively reduce the\nsearch space of the data records, and more accu-\nrately identify the required evidence to fulfill spe-\ncific tasks. In this way, LLMs can concentrate on\nreasoning based on the evidence obtained from the\ninterfaces. To implement the interface-augmented\napproach, there remain two key problems, namely\nhow to design suitable interfaces for specific tasks\nand how to utilize them for reasoning by LLMs,\nwhich are the focus of this work.\nTo design suitable interfaces, we regard multi-\nple types of structured data as black-box systems,\nand design the interfaces to provide accurate, effi-\ncient data access and filtering for LLMs. For each\n9237\ninterface, its implementation is dependent on the\ncharacteristic of the structured data, while its func-\ntionality is general to all LLMs, with just a few\narguments for specifying the data requirements.\nBased on these interfaces, we propose an Itera-\ntive Reading-then-Reasoning (IRR) framework for\nLLMs to utilize the interfaces to solve the tasks\nbased on structured data, namely StructGPT. This\nframework considers two major functions to ful-\nfill different tasks, namely collecting relevant evi-\ndence (reading) and inferring the answer or plan-\nning subsequent steps (reasoning). Specifically, we\npropose an invoking-linearization-generation pro-\ncedure to support LLMs in reading and reasoning\non the structured data with the help of the external\ninterfaces. By iterating this procedure with pro-\nvided interfaces, we can gradually approach the\ntarget answer to a given question.\nTo our knowledge, this is the first work that ex-\nplores how to support LLMs in reasoning on multi-\nple types of structured data (including tables, KGs,\nand DBs) in a unified paradigm. To evaluate the ef-\nfectiveness of our approach, we conduct extensive\nexperiments on a wide range of tasks ( e.g., KG-\nbased question answering (KGQA), Table-based\nquestion answering (TableQA), and DB-based Text-\nto-SQL). Experimental results on 8 datasets demon-\nstrate that our approach can effectively enhance the\nreasoning performance of LLMs on structured data\nin zero-shot and few-shot settings, even compara-\nble with competitive full-data supervised-tuning\nmethods. For example, in KGQA, TableQA, and\nText-to-SQL tasks, our approach yields an increase\nof 11.4% of Hits@1 on WebQSP, 4.2% of accu-\nracy in TabFact, and 4.7% of execution accuracy\nin Spider respectively, compared to directly using\nChatGPT in the zero-shot setting.\n2 Related Work\nReasoning over Structured Data. Structured\ndata (e.g., knowledge graphs, tables, and databases)\nis an important knowledge carrier for a variety of\nQA and reasoning tasks. Early work focuses on\ndesigning specific model architectures tailored for\neach type of structured data, such as graph neu-\nral networks (Sun et al., 2018), table Transform-\ners (Herzig et al., 2020), and tree-structured de-\ncoder (Wang et al., 2020). While achieving re-\nmarkable performance, these approaches lack gen-\nerality for various types of structured data and are\nhard to be transferred across different tasks. Re-\ncently, with the success of pre-trained language\nmodels (PLMs) ( e.g., T5 (Raffel et al., 2020),\nBART (Lewis et al., 2020)), several methods (Raf-\nfel et al., 2020; Khashabi et al., 2020) have adopted\nPLMs as the general encoder or solver for different\nstructured data and tasks. Among them, Unified-\nSKG (Xie et al., 2022) unifies a number of rea-\nsoning tasks over structured data into a text-to-text\nformat, which concatenates the question and the\nlinearized structured data as input, and then fine-\ntunes T5 to learn to generate the answer. However,\nUnifiedSKG also requires to tune the model param-\neters, and is unable to handle large-scale structured\ndata under the limitation of the maximum input\nlength. Instead, our method can utilize the LLM to\nperform reasoning on structured data without train-\ning, and also leverage the interfaces of structured\ndata to better manipulate vast structured data.\nLLMs for Structured Data. Benefitting from\nthe strong few-shot and zero-shot capability, re-\ncent studies have leveraged LLMs to perform rea-\nsoning over structured data (Chen et al., 2023; Li\net al., 2023; Cheng et al., 2022; Rajkumar et al.,\n2022). Existing work can be roughly divided into\ntwo types. The first type of method linearizes the\nstructured data into a sentence ( e.g., table rows),\nand feeds it into the LLMs to generate the answer\naccording to in-context exemplars (Cheng et al.,\n2022; Chen, 2023). For complex questions or struc-\ntured data, they first decompose it into multiple\nsimple and short ones and then perform lineariza-\ntion and generation (Ye et al., 2023). Another type\nof method leverages LLMs to evaluate the plausi-\nbility of the solution plan based on the knowledge\nbase (Gu et al., 2023), or first generate a solution\ndraft with in-context exemplars and then revise the\ndraft grounding on the knowledge base (Li et al.,\n2023). However, most of them only focus on a\nspecific type of structured data, and are lack of gen-\nerality across various data and tasks. In StructGPT,\nwe provide a unified paradigm that is general to\nvarious structured data and downstream tasks.\n3 Preliminary\nIn this section, we introduce the definition of struc-\ntured data, which mainly consists of three com-\nmonly used types. Then we present the unified\nproblem statement.\nStructured Data. Structured data ( e.g., data ta-\nbles and knowledge graphs) refers to the data that\n9238\nis in a standardized format, conforming to some\nlogical data model (Xie et al., 2022; Chen et al.,\n2009). Due to the formal structure, it is easy and\nefficient to access and query structured data us-\ning formal languages ( e.g., SQL and SPARQL\nfor databases) or specific algorithms (e.g., triples\nsearch for knowledge graphs). In this work, we\nmainly focus on three types of structured data,\nnamely knowledge graphs (KG), data tables (Ta-\nble), and databases (DB), since they play an impor-\ntant role as the knowledge source in helping solve\ncomplex reasoning tasks, described as follows.\n•Knowledge Graph. A knowledge graph (KG)\nconsists of a number of triples to store the fac-\ntual knowledge, denoted as G= {⟨e, r, e′⟩|e, e′∈\nE, r∈R}, where Eand Rdenote the set of enti-\nties and relations, respectively. A triple ⟨e, r, e′⟩\nrepresents the fact that there is a relation r between\nthe head entity e and the tail entity e′.\n•Data Table. A data table T(table in short) con-\ntains multiple columns {ci}C\ni=1 and rows {lj}R\nj=1,\nwhere each row lj denotes a data record formatted\nby the attributes indexed by columns {ci}C\ni=1, and\nvi,j denotes the content in the cell corresponding\nto the position at column i and row j.\n•Database. A database (DB) typically consists\nof N data tables, denoted as D= {T1, T2, ...,TN }.\nBesides the column names, the foreign keys across\nall tables are also available to link the data from\ntwo tables, denoted as {(c(k)\ni , c(h)\nj )}, where c(k)\ni\nand c(h)\nj denote the i-th and j-th columns in the\nk-th and h-th tables, respectively.\nProblem Statement. This work mainly focuses\non using LLMs to solve complex reasoning tasks\nbased on structured data. Formally, it can be de-\nscribed as a question answering task: given a nat-\nural language question q and an accessible struc-\ntured data S(e.g., a knowledge graph, a table, or\ndatabase), the LLM needs to extract useful evi-\ndence from Sand then generates the expected re-\nsult to answer the question q based on the extracted\nevidence. According to the task requirement, the\ngenerated result can be either free-form answers\nin natural language or structured expressions (e.g.,\nSQL statements) to be executed for obtaining the\nanswer from S. Since we consider three types of\nstructured data (Section 4), our tasks can be instan-\ntiated as follows:\n•KG based question answering (KGQA)\n•Table based question answering (TableQA)\n•DB based semantic parsing (Text-to-SQL)\n4 Approach\n4.1 Overview\nIn this work, we assume that LLMs have to rely\non the evidence contained in the structured data\nto solve the three tasks described in Section 3.\nAn intuitive idea is to conduct a two-stage frame-\nwork as prior studies on retrieval-augmented ap-\nproaches (Izacard et al., 2022; Oguz et al., 2022),\nin which LLMs are employed to first collect suf-\nficient evidence relating to the question and then\nfigure out the answer by the LLMs. However, such\nan approach is not directly applicable to structured\ndata. Although LLMs are capable of solving di-\nverse tasks in natural language, they have limited\ncapacities in accurately representing and under-\nstanding structured data, especially for their con-\ntained domain-specific knowledge (Moiseev et al.,\n2022; Emelin et al., 2022).\nTo address this difficulty, our solution is inspired\nby the use of specialized tools in solving complex\ntasks for LLMs (Nakano et al., 2021; Gao et al.,\n2022b; Schick et al., 2023). We noted that struc-\ntured data is well organized and supports easy ac-\ncess via formal language or queries (called inter-\nface for generality). The basic idea of our approach\nis to disentangle the two processes of reading and\nreasoning for LLMs: we utilize the interface of\nstructure data to implement accurate, efficient data\naccess and filtering ( obtaining the relevant evi-\ndence), and further utilize the reasoning ability of\nLLMs to figure out the final plan or result for the\nquestion (fulfilling the task). In this way, LLMs can\nconcentrate on the reasoning process in answering\nthe question, without considering the specialized\napproach to reading the structure data.\nSpecially, in our framework, we encapsulate\nthe structure data as a black-box system, and pro-\nvide specific interfaces for LLMs to access the\ncontained data. Further, we propose an invoking-\nlinearization-generation procedure that enables\nLLMs to read and extract useful evidence from\nstructured data via the corresponding interface. By\niterating the above procedure with provided inter-\nfaces, we can gradually obtain the answers by lever-\naging the superior reasoning abilities of LLMs.\n4.2 Interfaces for Structured Data\nDue to the standardized data formats, structured\ndata is often equipped with efficient data manage-\nment ways, e.g., SQL for the database. In our\napproach, we aim to provide LLMs with special-\n9239\nThe Jeff\nProbst Show\nnominee Jeff \nProbst\nLisa Ann \nRussell\nspouse\nLisa\nis_a\nTV    \nproducer\nSurvivor\nnominee\nis_a\nTalk\nshow\nExtract_Triples (e, {r})\n>>> input: ET(“Jeff Probst”,[“spouse”])\n>>> output: [(Jeff Probst,spouse,Lisa Ann Russell)]\nExtract_Neighbor_Relations (e)\n>>> input: ENR(“Jeff Probst”)\n>>> output: [spouse, is_a] Extract_Columns (T, {c})\n>>> input: EC(“Team”,[“Stadium”])\n>>> output: [(Provident),…,(DW)]\nTeam Stadium Capacity\nBradford Bulls Provident 27,000\n… … …\nWigan Warriors DW 25,138\nExtract_SubTable (T, {c}, {j})\n>>> input: ES(“Team”,[“Stadium”],[0,1,…,13])\n>>> output: [(Provident),…,(DW)]\nExtract_Table&Column_Name (D)\n>>> input: ETN(“mus_vis”)\n>>> output: [visit, visitor, museum]\nExtract_Tables_Infomration ({T})\n>>> input: ETI([“visitor”]])\n>>> output: [(visitor, (ID,…,Age), NULL)]\nMus_ID Num_Ticket visitor_ID Total_spent\n… … … …\nID Name Level_of_mem Age\n… … … …\nvisit\nvisitor\nMus_ID Name Num_Staff Open_Year\n… … … …museum\nvisit.Museum_ID→meseum.Museum_ID\nvisit.visitor_ID→visitor.ID\nExtract_Column_Name (T)\n>>> input: ECN(“Team”)\n>>> output: [Team,Stadium,Capacity]\n“spouse, is_a”\n“Team,…,Capacity”\n“visitor(*,ID,…,);…”\nLinearization\nWho is the wife of Jeff Probst?\nwhat is the last stadium listed on this chart?\nHow many visitors below age 30 are there?\nQuestion\nKGQA\nTableQA\nText-to-SQL\nspouse\nStadium\nvisitor\nLLM\nGeneration\nLisa Ann Russell\nFinal Answer or Executable SQL\nDW\nSELECT count(*) FROM visitor WHERE age < 30\nStructured Data\nInterfaces\nKG Table DB\nExtract_Neighbor_Relations(“Jeff Probst”)\nExtract_Column_Name(“Team”)\nExtract_Table&Column_Name(“mus_vis”)\nInvoking\nis_a\nproducer\nor\nFigure 1: The overview of the proposed iterative reading-then-reasoning approach. We design specialized interfaces\nfor reading structured data, and iterate the invoking-linearization-generation procedure to utilize LLMs for perform-\ning reasoning on the interfaces, until deriving the final answer or executable SQL.\nized interfaces, helping LLMs to read and utilize\nthe structured data. Next, we present the specially\ndesigned interfaces for KG, table, and DB.\nInterfaces for Knowledge Graph. When per-\nforming complex reasoning on a KG, existing\nwork (Sun et al., 2018) typically starts from a cer-\ntain entity (about the question topic), and jumps\nalong with the relations until reaching the answer.\nIn this process, LLMs should be aware of the neigh-\nboring relations of the current entity, and the neigh-\nboring triples with certain relations to the current\nentity. Based on it, LLMs can select the relevant\nrelations and triples from them to find the answer.\nFor this purpose, we devise two functions for as-\nsisting LLMs to accomplish the above operations.\n•Extract_Neighbor_Relations (e): extracts all\nthe neighboring relations of the entity e.\n•Extract_Triples (e, {r}): extracts all the triples\nwith the relation in {r}and head entity e.\nInterfaces for Table. Given a data table, LLMs\nneed to know its contained column names, and\ncan access the content by row or column, enabling\nLLMs to extract its sub-table containing relevant\ncolumns and rows. Thus, we define three functions:\n•Extract_Column_Name (T): extracts all the\ncolumn names of a table T.\n•Extract_Columns (T, {c}): extracts the con-\ntents of columns from a table T by indices {c}.\n•Extract_SubTable (T, {c}, {j}): extracts the\nsub-table specified by the column indices {c}and\nrow indices {j}from a table T.\nInterfaces for Database. Considering a simpli-\nfied setting when querying the database, LLMs\nshould be aware of all the contained tables and\ncolumns (by name) for relevant tables selection,\nand can also acquire the detailed columns and for-\neign keys from the selected tables to search for the\nanswer. Thus, we devise two functions as follows:\n•Extract_Table&Column_Name (D): extracts\nthe names of all the tables and their contained\ncolumns from the database.\n•Extract_Tables_Information ({T}): extracts\nthe table names, column names, and foreign keys\nfrom a set of tables {T}.\n9240\n4.3 Reading and Reasoning with Interfaces\nBased on the above interfaces, we propose a gen-\neral invoking-linearization-generation procedure\nthat can be iterated in multiple turns for utilizing\nLLMs to perform reading and reasoning on struc-\ntured data. For each iteration, based on the cur-\nrently collected data, we first invoke an interface to\nextract relevant evidence from structure data, then\nlinearize it into a textual prompt, and finally feed\nthe prompt into the LLM for generation (selecting\nuseful data or predicting the answer).\nInvoking an Interface. In this step, we aim to\ninvoke an interface for extracting the relevant infor-\nmation from the structured data. According to the\ndesigned interfaces in Section 4.2, we construct the\ninput based on the currently available data (e.g., en-\ntity and table), and then invoke the interface to ob-\ntain more detailed relevant information (e.g., neigh-\nboring relations and column names), which will be\nfed into LLMs for collecting useful information or\ngenerating the answer.\nInformation Linearization. Given the extracted\ninformation, we convert it into a textual sentence\nthat can be understood by LLMs. For the infor-\nmation from KG ( i.e., relations and triples), we\nconcatenate them into a long sentence marked by\nspecific separation and boundary symbols. For ta-\nble and database, we leverage the same way to lin-\nearize the extracted table names or column names.\nWhile for contents in columns and rows, we follow\nexisting work (Pasupat and Liang, 2015) that first\nconverts them into triples, where head entities are\nthe row indices, relations are column names, and\ntail entities are the content in the cell, e.g., “(row 1,\nyear, 1896)” and “(row 1, city, Athens)”. Then, for\neach row, we extract the row indices in the front\nand omit it in the triples, to compose a simplified\nsentence, e.g., “row 1: (year, 1896), (city, Athens)”.\nFor multiple rows, we concatenate them into a long\nsentence via a special separation symbol.\nLLM for Generation. After linearization, we\ndesign two types of input prompts for LLMs to\nfulfill different purposes1:\n•The first type of prompts mostly adopts the fol-\nlowing pattern: “Here are [Y]. Which [X] are most\nrelevant to answer the question [Q] ”. It aims to\nelicit the ability of LLMs to select useful evidence\n1Note that our used prompts are not always consistent with\nthe two examples, as we have rewritten them to better adapt\nto the specific datasets and extracted information.\n(i.e., [X] ) from linearized extracted information\n(i.e., [Y]), according to the question (i.e., [Q]).\n•The second type of prompt follows the pattern:\n“Based on [Y], please generate [Z] for the question\n[Q]”. It aims to predict the targeted results ( i.e.,\n[Z]) for the given question (i.e., [Q]) based on the\nlinearized extracted information ( i.e., [Y]). Note\nthat the targeted results can be either the answer\nstring or executable formal language ( e.g., SQL)\nthat can lead to the final answer.\nBy iterating the above invoking-linearization-\ngeneration procedure on designed interfaces, LLMs\ncan progressively capture more useful evidence for\nderiving the final answer.\n4.4 Instantiated Downstream Tasks\nIn the following, we describe the instances of the\nabove general workflow for the tasks described\nin Section 3, since they deal with very different\nstructure data and vary in the task settings.\nKG-based Question Answering (KGQA). This\ntask aims to find the answer entities for the question\nbased on the KG. Following existing work (Sun\net al., 2018), we denote the mentioned entity in\nthe given question q as the topic entity eT , and\nassume it has been linked to some specific en-\ntity on the KG through existing linking tools (e.g.,\nGoogle Knowledge Graph Search API) or mod-\nels (e.g., ELQ (Li et al., 2020)). Starting from eT ,\nwe perform the invoking-linearization-generation\nprocedure two times using the two interfaces in\nKG sequentially. First, we invoke the interface\nExtract_Neighbor_Relation(eT ) to extract the can-\ndidate one-hop relations, linearize them to com-\npose the input prompt, and then leverage the LLM\nto select the useful relations {r}according to the\nquestion. Then, based on {r}, we invoke the Ex-\ntract_Triples (eT , {r}) interface to collect the rel-\nevant triples for the head entity eT and relation in\n{r}, then linearize this information, and finally em-\nploy the LLM to select the most relevant triples,\nwhose tail entities will be considered as the final\nanswer. Besides, we can also consider the multi-\nhop KGQA task (Lan et al., 2021), where after\nselecting the triples of the current hop, the LLM\nshould assess whether the current information is\nsufficient to answer the question. Then, LLMs will\nmake according actions based on the assessment,\ni.e., stopping the iterations for producing the an-\nswer or continuing the iterations on next-hop tail\nentities from selected triples.\n9241\nTable-based Question Answering (TableQA).\nFor TableQA, we typically need to answer the\nquestion according to the content in the given ta-\nble. We also perform the above procedure by us-\ning the three interfaces in turn. Concretely, first,\nwe invoke Extract_Column_Name (T) to extract\nall column names of a table, linearize them, and\nleverage LLMs to select the relevant ones {c}\naccording to the question. Then, we invoke Ex-\ntract_Columns (T, {c}) to extract the contents of\nall relevant columns, and select the useful row in-\ndices {j}by LLMs. Subsequently, we further in-\nvoke Extract_SubTable (T, {c}, {j}) to generate\nthe sub-table for the question. Based on the lin-\nearized sub-table, the LLM finally generates the\nanswer to the question.\nDB-based Semantic Parsing (Text-to-SQL).\nThis task focuses on generating a SQL query that\ncan be executed to obtain the required information\nfrom a database. To achieve this goal, first, we\ninvoke Extract_Table&Column_Name (D) to ob-\ntain all the table names and their column names\nin the DB, linearize them, and utilize the LLM\nto select the relevant table names. Then, we in-\nvoke Extract_Tables_Information ({T}) to obtain\nall the relevant information (i.e., column names and\nforeign keys) from these tables. Similarly, by lin-\nearizing this information and composing the input\nprompt, the LLM can generate an executable SQL\nfor the given question.\n5 Experiment\nWe conduct experiments on three complex rea-\nsoning tasks over structured data, i.e., KGQA,\nTableQA, and DB based text-to-SQL.\n5.1 Datasets\nFor KG based QA (KGQA), we adopt two\nbenchmark datasets, i.e., WebQuestionsSP (We-\nbQSP) (Yih et al., 2016) and MetaQA (Zhang et al.,\n2018) for evaluation. The answer entities in We-\nbQSP require up to 2-hop reasoning on the Free-\nbase KG. In contrast, MetaQA contains questions\nin the movie domain, whose answer entities are up\nto 3 hops away from the topic entities on a movie\nKG (based on OMDb). According to the num-\nber of hops, it is split into three sub-datasets, i.e.,\nMetaQA-1hop, MetaQA-2hop, and MetaQA-3hop.\nFor Table based QA (TableQA), we adopt\nthree widely-used datasets, weakly-supervised Wik-\niSQL (WikiSQL) (Zhong et al., 2017), WikiTable-\nQuestions (WTQ) (Pasupat and Liang, 2015), and\nTabFact(Chen et al., 2020). The first two are typ-\nical table-based question answering datasets, and\nthe third one is a multiple-choice dataset that con-\ncentrates on table fact verification. WikiSQL re-\nquires filtering and aggregating information over\nthe table content, and the WTQ demands more\nadvanced reasoning capabilities (e.g., sorting). Tab-\nFact needs to judge whether the provided statement\nagrees with the facts stored in a table.\nFor DB based semantic parsing (Text-to-SQL),\nwe adopt three public datasets,i.e., Spider (Yu et al.,\n2018), Spider-SYN (Gan et al., 2021), and Spider-\nRealistic (Deng et al., 2021). Spider is a typical\nText-to-SQL dataset covering 20 databases with a\nset of 1034 evaluation samples. Spider-SYN and\nSpider-Realistic are two more challenging datasets\nderived from Spider. Concretely, Spider-SYN man-\nually substitutes the synonyms in natural language\nquestions, while Spider-Realistic removes the ques-\ntions in the evaluation set that explicitly mention\nthe required columns’ names.\n5.2 Evaluation Metrics\nFor KGQA, we employ Hits@1 which assesses\nwhether the top-1 predicted answer is correct. In\nour approach, we focus on generating the most con-\nfident answer and then checking if the prediction\nhits any target. As LLMs may generate multiple\nanswers, we also conducted a manual double-check\nfinally (Tan et al., 2023), to judge if wrong answers\nare included. For TableQA, we adopt two eval-\nuation metrics, namely denotation accuracy and\naccuracy. In WTQ and WikiSQL, denotation accu-\nracy is employed to evaluate whether the predicted\nanswer is the same as the gold answer based on\nset-level equivalence. In TabFact, we adopt ac-\ncuracy to assess the correctness of the prediction.\nFor Text-to-SQL, we adopt the execution accuracy\n(EX) to assess whether the execution results of the\npredicted SQL and the gold SQL are the same.\n5.3 Baselines\nWe compare our method with competitive full-\ndata supervised-tuning baselines tailored to these\ntasks. Specifically, our method is a general iterative\nreading-then-reasoning (IRR) framework that can\nbe used for different LLMs. And we test our IRR\nwith two different LLMs, i.e., Davinci-003 (text-\ndavinci-003 (Ouyang et al., 2022b)) and Chat-\n9242\nGPT (i.e., gpt-3.5-turbo 2), under zero-shot and\nfew-shot settings 3. Considering the evolution of\nthe closed large language model,e.g., ChatGPT, we\nhave further conducted supplementary experiments\non three datasets (i.e., WebQSP, WTQ, and Spider)\nusing the latest August version of ChatGPT. The re-\nsults are presented in Appendix A. For KGQA, we\nselect KV-Mem (Miller et al., 2016), GragtNet (Sun\net al., 2018), EmbedKGQA (Saxena et al., 2020),\nNSM (He et al., 2021), and UniKGQA (Jiang et al.,\n2023). For TableQA, we select MAPO (Liang\net al., 2018), TAPAS (Herzig et al., 2020; Eisensch-\nlos et al., 2020), UnifiedSKG (T5-3B) (Xie et al.,\n2022), TAPEX (Liu et al., 2022), and DATER (Ye\net al., 2023). For Text-to-SQL, we select RAT-\nSQL+BERTLarge (Wang et al., 2020), TKK-\nLarge (Gao et al., 2022a), T5-3B+PICARD (Raffel\net al., 2020), RASAT+PICARD (Qi et al., 2022),\nand RESDSQL-3B+NatSQL (Li et al., 2023).\nAdditionally, we incorporate baselines that em-\nploy Davinci-003 and ChatGPT directly for achiev-\ning the aforementioned tasks in a zero-shot setting.\nTo ensure a fair comparison, we utilize the same\ninstruction prompt to evaluate them, ensuring that\nthe only difference with our method is the usage\nof structured data. Specifically, in KGQA datasets,\nwe follow existing work (Tan et al., 2023) that uti-\nlizes LLMs to answer the questions without using\nKG. In TableQA and Text-to-SQL, we feed the re-\nquired information of tables with questions into\nLLMs (Liu et al., 2023c,a), without special treat-\nment for the overlength problem.\n5.4 Results and Analysis\nWe show the results on KGQA, TableQA, and Text-\nto-SQL tasks and analyze them respectively.\nEvaluation on KGQA.Table 1 shows the results\non KGQA datasets. First, LLMs can achieve per-\nformance comparable to the supervised learning\nmodel (i.e., 61.2 of ChatGPT v.s. 66.4 of GraftNet\nand 48.3 of Davinci-003 v.s. 46.7 of KV-Mem) on\nthe WebQSP dataset, in a zero-shot setting with-\nout using KGs. It demonstrates that LLMs indeed\ngrasp a certain amount of knowledge that can help\nthem answer complex questions. However, on\nmore difficult datasets that require multi-hop rea-\nsoning (e.g., MetaQA-2hop and MetaQA-3hop),\nthe two LLMs perform not well. It indicates that\n2https://platform.openai.com/docs/models/gpt-3-5\n3In our experiment, we use the June version of Davinci-003\nand ChatGPT.\nTable 1: Results of different methods for KGQA\n(Hits@1 in percent). We copy the results in the first\nblock from He et al. (2021) and Jiang et al. (2023). The\nbest results of each block are highlighted in bold.\nMethods WQSP MQA\n1hop\nMQA\n2hop\nMQA\n3hop\nKV-Mem 46.7 96.2 82.7 48.9\nGraftNet 66.4 97.0 94.8 77.7\nEmbedKGQA 66.6 97.5 98.8 94.8\nNSM 68.7 97.1 99.9 98.9\nUniKGQA 75.1 97.5 99.0 99.1\nDavinci-003 48.3 52.1 25.3 42.5\n+ IRR (ours) 71.9 94.4 59.5 70.2\n+ IRR (ours, few-shot) 71.0 97.1 93.5 75.3\nChatGPT 61.2 61.9 31.0 43.2\n+ IRR (ours) 72.6 94.2 93.9 80.2\n+ IRR (ours, few-shot) 69.6 97.1 97.3 87.0\nLLMs can not solely rely on their own knowledge\nto answer difficult questions, and their augmen-\ntation with KGs is necessary. In contrast, when\nincorporating our proposed method to access KG,\nthe performance of Davinci-003 and ChatGPT can\nbe both substantially improved, indicating the ef-\nfectiveness of our proposed method for supporting\nLLMs reasoning over KG. By adding a few in-\ncontext exemplars (i.e., 15 for WQSP and 32 for\nMQA) to LLMs, we can further improve the model\nperformance. In our approach, we devise interfaces\nfor KG to efficiently read the relevant information,\nand leverage LLMs to extract useful parts and per-\nform reasoning. We leverage the IRR procedure\non devised interfaces sequentially, which can pro-\ngressively capture more useful detailed evidence\nfor finally obtaining the answer.\nEvaluation on TableQA.Table 2 shows the re-\nsults on three TableQA datasets. First, with the\nfull table as the prompt, ChatGPT can also achieve\ncomparable performance on WTQ and TabFact as\nfull-data supervised-tuning methods, but performs\nnot well on more difficult WikiSQL datasets. It also\nindicates that LLMs have the capability of under-\nstanding the knowledge within table data to some\nextent. Second, our proposed method can consis-\ntently improve the performance of two LLMs a\nlot in both three datasets. At the same time, when\nadding 32 in-context exemplars to the LLMs, they\ncan obtain further performance improvements. It\nindicates the effectiveness of our proposed method\nin helping LLMs reasoning over Table. Our ap-\nproach provides a more effective way for LLMs to\n9243\nTable 2: Results of different methods for TableQA (de-\nnotation accuracy for WTQ and WikiSQL, accuracy for\nTabFact). We copy the results of TAPAS on TabFact\nfrom Eisenschlos et al. (2020), and others in the first\nblock from their original papers. The best results of\neach block are highlighted in bold.\nMethods WTQ WikiSQL TabFact\nMAPO 43.8 72.6 -\nTAPAS 48.8 83.6 81.0\nUnifiedSKG (T5-3B) 49.3 86.0 83.7\nTAPEX 57.5 89.5 84.2\nDATER 65.9 - 93.0\nDavinci-003 34.8 49.1 80.7\n+ IRR (ours) 39.2 51.8 76.5\n+ IRR (ours, few-shot) 57.0 64.6 87.3\nChatGPT 43.3 51.6 82.9\n+ IRR (ours) 48.4 54.4 87.1\n+ IRR (ours, few-shot) 52.2 65.6 87.6\niteratively access and utilize the relevant informa-\ntion from the table, which reduces the influence of\nirrelevant and redundant information.\nEvaluation on Text-to-SQL.Table 3 shows the\nresults on DB-based datasets. First, with all the\ninformation from DB (table names, column names,\nand foreign keys) as the prompt, the LLMs have\nthe capability of directly generating a suitable SQL\nquery of the question, performing well on all three\ndatasets. Whereas, the performance of LLMs is\nnot better than competitive full-data supervised-\ntuning methods, showing the difficulty of this task.\nAs our proposed method can extract relevant ta-\nbles and columns, it also alleviates the influence\nof irrelevant information for LLMs to generate the\nSQL query. Simultaneously, with the assistance of\n32 in-context exemplars, LLMs exhibit enhanced\ncomprehension of the mapping between natural\nlanguage questions and their corresponding SQL\nqueries. The consistent performance improvements\nover the three datasets whenever in zero-shot or\nfew-shot settings also indicate the effectiveness of\nour proposed method.\nCase Study. We show an example of KGQA in\nFigure 2, to help understand the working process\nof our method. Given the question, the interfaces\nof the structured data are sequentially invoked to\niteratively extract more useful and detailed infor-\nmation. In each iteration, we first invoke the Ex-\ntract_Neighbor_Relations function to extract the\nneighboring relations (e.g., birthplace, residence,\nand education) of the topic entity “ Harper Lee”,\nTable 3: Performance comparison of different methods\nfor Text-to-SQL (execution accuracy in percent). We\ncopy the results of RAT-SQL+BERTLarge and TKK-\nLarge from Deng et al. (2021) and Gao et al. (2022a),\nrespectively. And we copy the results of the other three\nmethods in the first block from Liu et al. (2023b). The\nbest results of each block are highlighted in bold.\nMethods Spider Spider-\nSYN\nSpider-\nRealistic\nRAT-SQL + BERTLarge 72.3 - 62.1\nTKK-Large 73.2 60.5 64.4\nT5-3B + PICARD 79.3 69.8 71.4\nRASAT + PICARD 80.5 70.7 71.9\nRESDSQL-3B + NatSQL 84.1 76.9 81.9\nDavinci-003 68.8 60.1 63.2\n+ IRR (ours) 69.5 60.3 64.2\n+ IRR (ours, few-shot) 72.7 63.2 70.7\nChatGPT 70.1 58.6 63.4\n+ IRR (ours) 74.8 62.0 70.3\n+ IRR (ours, few-shot) 77.8 64.0 72.0\nthen linearize them and compose the input prompt.\nHere, we utilize the instruction (i.e., provide only\none relevant relation that’s present in the candidate)\nto elicit the LLM to generate the most relevant rela-\ntion, i.e., education. Based on the selected relation,\nwe further invoke the Extract_Triples function to\nextract the triples with the relation to the topic en-\ntity. After linearization, another instruction ( i.e.,\nyou just need to provide only one answer entity), is\nadopted for guiding the LLM to generate the final\nanswer, i.e., Monroe County High School. Besides,\nwe show the representative examples of TableQA\nand Text-to-SQL in Appendix B.\nError Analysis.To systemically analyze the short-\ncomings of our approach, we first select three\ndatasets (i.e., WebQSP, WTQ, and Spider) with\ndifferent types of structured data, and randomly\nsample 100 error cases from each dataset. Then,\nwe manually examine these failures and classify\nthem into five categories:\n•Selection Error: the relevant information has\nnot been selected by the LLM.\n•Reasoning Error: given the extracted relevant\ninformation, the LLM fails to generate the ground-\ntruth answer or SQL.\n•Generation Format Error: the generated an-\nswer is in an abnormal format that fails to be iden-\ntified by our result parser.\n•Hallucination: the generated results are incon-\nsistent with the extracted information.\n•Other Errors: other uncategorizable errors.\n9244\nQuestion: What is the name of the breed with the most dogs?\nInvoke: Extract_Table&Column_Name (database)\nDogs | Breeds\n### Complete sqlite SQL query only and ….\n# Breeds(*, breed_code, breed_name); …\n### What is the name of the breed with the\nmost dogs? SELECT\nBreeds.breed_name FROM Dogs JOIN …\n### Here are the SqliteSQL tables …\n# Breeds(*, breed_code, breed_name); …\n### What is the name of the breed with the most \ndogs? Which tables do you need to complete the \nSQLite SQL query? \ndog_id ···breed_code\nLinearize: “Dogs … Breeds” \nInvoke: Extract_Tables_Information ([Dogs, Breeds])\nLinearize: “Dogs(dog_id, …);…”\nReturn: [Dogs … Breeds]\nbreed_code ···breed_name\nReturn: [Dogs(dog_id, …)…]\nGenerate\nGenerate\n(c) Case of  Spider (Text-to-SQL)\nDogs\nBreeds\n······\nDogs.breed_code →Breed.breed_code\nTable name Column names\ndog_id ···breed_code\nbreed_code ···breed_name\nDogs\nBreeds\n······\nDogs.breed_code →Breed.breed_code\nTable name Column names\nQuestion: what highschool did harper lee go to? Invoke: Extract_Neighbor_Relations (Harper Lee)\nReturn: [education … birthplace]\nGenerate\n“(Harper Lee, education, CVT_0); … ”\nExtract_Triples (Harper Lee, [education])\nThe candidate relations: education … birthplace.\nThe question is …\nProvide only one relevant relation that's present \nin the candidates …\nThe relevant relation: education. \nThe triples are: (Harper Lee, education, \nCVT_0) ... Based on these triples … give me \nthe final answer entity.  \nYou just need to provide only one answer entity. \nIf you think …\nAnswer: Monroe County High SchoolGenerate\nReturn: \n[(Harper Lee, education, CVT_0) … ]\nLinearize: \nLinearize: “education … birthplace”\n(a) Case of WebQSP (KGQA)\nCVT_0\nOxfordcollege\nHarpe Lee\nCVT_1\neducation\neducation\nMonroe \nCounty\nhigh \nschool\ntype\ntype\n···\n···\ninstitution\ninstitution\n···residence\nbirthplace\n···\nCVT_0\nOxfordcollege\nHarpe Lee\nCVT_1\neducation\neducation\nMonroe \nCounty\nhigh \nschool\ntype\ntype\n···\n···\ninstitution\ninstitution\n···residence\nbirthplace\n···\nInvoke: \nInvoke:\nExtract_Columns (table , [District…])\n1st ···\n···19st charles \nhawkins\nMarty \nWilliams\nJohn \nMiller …\nRobert \nHurt ...\nQuestion: In what district was the incumbent charles hawkins? Invoke: Extract_Column_Name (table)\nTo answer … first look at the available columns \nin the table: “District”, “Incumbent”, ... \nWhich columns are most relevant to answering \nthe question? …\nColumns: District, Incumbent.\nTo answer … Below is the list of rows ….\nrow 1: (District, 1st); (Incumbent, Marty\nWilliams) … (2007 Result … )\nwhich rows should be considered? …\nThe table contains:\nrow 1: (District, 19th); (Incumbent, Charles\nHawkins).\nUsing this information, In what district was the\nincumbent Charles Hawkins? …\nResturn: [Didtrict, …, 2007 Result]\nReturn: [(row 1, (District, 19th)…)…]\nInvoke:\nExtract_SubTable (table, [District…], [19])\nReturn: [(row 1, (District, 19th), …]\nLinearize: “District, …, 2007 Result”\nLinearize: “(row 1, (District, 19th), …)”\nLinearize: “row 1: (District, 1st); …”\nGenerate\nGenerate\nGenerate\nRows: row 19\nAnswer: 19th\n··· ··· ······\nDistrict Incumbent 2007 \nResult···\n1st ···\n···19st\ncharles \nhawkins\nMarty \nWilliams\nJohn \nMiller …\nRobert \nHurt ...\n··· ··· ······\nDistrict Incumbent 2007 \nResult···\n1st ···\n···19st charles \nhawkins\nMarty \nWilliams\nJohn \nMiller …\nRobert \nHurt ...\n··· ··· ······\nDistrict Incumbent 2007 \nResult···\n(b) Case of  TableQA (KGQA)\nFigure 2: Case study of our method on WebQSP.\n21%\n63%\n8%\n4%4%\nSpider (Text-to-SQL)\n28%\n30%\n34%\n8%\nWikiSQL (TableQA)\nSelection Error\nReasoning Error\nGenerating Format Error\nHallucination\nOther Errors\n74%\n2%\n8%\n3%\n13%\nWebQSP(KGQA)\nFigure 3: Proportions of different error types in three datasets over different types of structured data.\nWe show the statistics in Figure 3. First, for\nthe three datasets, the distributions of occurring\nerrors are different. In WikiSQL, the frequencies\nof generation format, selection, and reasoning er-\nrors are relatively uniform. Whereas, in WebQSP,\nthe selection error is the major error type (74%),\nsince the KGQA task requires selecting the most\nrelevant one from thousands of relations, which is\nnot easy work. In Spider, reasoning error occurs\nmore (62%), since the Text-to-SQL task requires\nLLMs to generate a SQL that can be executed to\nobtain the answer, which is also hard for LLMs.\nAccording to the error distributions, it is promis-\ning to refine the major error cases to specifically im-\nprove the performance on each dataset. Concretely,\nwe can devise more high-quality prompts that elicit\nLLMs to carefully make decisions when selecting\nand reasoning on KGQA and Text-to-SQL tasks, re-\nspectively. Besides, we also consider adding more\ninterfaces and iteration turns for decomposing the\nhard probelm into multiple simple ones, to simplify\nthe complex reasoning task for better performance.\nWe will try the above solutions in our future work.\n6 Conclusion\nIn this work, we proposed a general framework for\nimproving the zero-shot reasoning ability of LLMs\nover structured data, namely StructGPT. In our ap-\nproach, we first constructed the specialized inter-\nfaces that support accurate and efficient data ac-\ncess, and then proposed an invoking-linearization-\ngeneration procedure that leverages LLMs to read\nand perform reasoning based on the interface. By\niterating the above procedure using the interfaces\nsequentially, LLMs can progressively capture more\nuseful and detailed evidence and finally generate\nthe answer. To verify the effectiveness of our\napproach, we implemented our approach on KG\nbased QA, table based QA and DB based semantic\nparsing tasks. Experimental results on 8 datasets\nshow that our approach can boost the zero-shot per-\nformance of LLMs by a large margin, and achieve\ncomparable performance as full-data supervised-\ntuning methods. We also provide detailed error\nanalysis to point out the weakness of our approach,\nfor enlighting other researchers in related areas.\n9245\n7 Limitations\nAlthough StructGPT demonstrates remarkable per-\nformance across tasks over structured data, there\nare some limitations of our method. First, the\ntwo LLMs used in our model, i.e., ChatGPT and\nDavinci-003, have a strong capability of following\ninstructions. Hence, more experiments are required\nto evaluate our method with in-context learning\non other LLMs that perform poorly at instruction\nfollowing. Similarly, we only evaluate question\nanswering tasks based on structured data. Future\nwork should include wider evaluation scenarios\nto evaluate the universality of our method, e.g.,\ndata-to-text and formal-language-to-text (Xie et al.,\n2022). Finally, since it is difficult to control the an-\nswer format during the generation process of LLMs\nin different datasets, there are several format errors\nin generated texts as shown in Section 5. Therefore,\nthe performance of our method can be further im-\nproved by meticulously designing the prompt and\nanswer parsing for different datasets.\nAcknowledgments\nThis work was partially supported by National Nat-\nural Science Foundation of China under Grant No.\n62222215, Beijing Natural Science Foundation un-\nder Grant No. 4222027 and L233008. And this\nwork is also partially supported by the Outstanding\nInnovative Talents Cultivation Funded Programs\n2022 of Renmin University of China. Xin Zhao is\nthe corresponding author.\nReferences\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nWenhu Chen. 2023. Large language models are few(1)-\nshot table reasoners. In Findings of the Associa-\ntion for Computational Linguistics: EACL 2023,\nDubrovnik, Croatia, May 2-6, 2023 , pages 1090–\n1100. Association for Computational Linguistics.\nWenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai\nZhang, Hong Wang, Shiyang Li, Xiyou Zhou, and\nWilliam Yang Wang. 2020. Tabfact: A large-scale\ndataset for table-based fact verification. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020.\nYi Chen, Wei Wang, Ziyang Liu, and Xuemin Lin. 2009.\nKeyword search on structured and semi-structured\ndata. In Proceedings of the ACM SIGMOD Interna-\ntional Conference on Management of Data, SIGMOD\n2009, Providence, Rhode Island, USA, June 29 - July\n2, 2009, pages 1005–1010. ACM.\nZhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi\nWen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin,\nWenqi Fan, Hui Liu, and Jiliang Tang. 2023. Explor-\ning the potential of large language models (llms) in\nlearning on graphs. CoRR, abs/2307.03393.\nZhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu\nLi, Rahul Nadkarni, Yushi Hu, Caiming Xiong,\nDragomir Radev, Mari Ostendorf, Luke Zettlemoyer,\nNoah A. Smith, and Tao Yu. 2022. Binding language\nmodels in symbolic languages. CoRR.\nXiang Deng, Ahmed Hassan Awadallah, Christopher\nMeek, Oleksandr Polozov, Huan Sun, and Matthew\nRichardson. 2021. Structure-grounded pretraining\nfor text-to-sql. In Proceedings of the 2021 Confer-\nence of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, NAACL-HLT 2021, Online, June\n6-11, 2021, pages 1337–1350.\nJulian Martin Eisenschlos, Syrine Krichene, and\nThomas Müller. 2020. Understanding tables with\nintermediate pre-training. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2020,\nOnline Event, 16-20 November 2020, pages 281–296.\nDenis Emelin, Daniele Bonadiman, Sawsan Alqahtani,\nYi Zhang, and Saab Mansour. 2022. Injecting do-\nmain knowledge in language models for task-oriented\ndialogue systems. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2022, Abu Dhabi, United Arab\nEmirates, December 7-11, 2022, pages 11962–11974.\nAssociation for Computational Linguistics.\nYujian Gan, Xinyun Chen, Qiuping Huang, Matthew\nPurver, John R. Woodward, Jinxia Xie, and Peng-\nsheng Huang. 2021. Towards robustness of text-to-\nsql models against synonym substitution. In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing,\nACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual\nEvent, August 1-6, 2021, pages 2505–2515.\nChang Gao, Bowen Li, Wenxuan Zhang, Wai Lam, Bin-\nhua Li, Fei Huang, Luo Si, and Yongbin Li. 2022a.\nTowards generalizable and robust text-to-sql parsing.\narXiv preprint arXiv:2210.12674.\n9246\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2022b. PAL: program-aided language\nmodels. CoRR.\nYu Gu, Xiang Deng, and Yu Su. 2023. Don’t gener-\nate, discriminate: A proposal for grounding language\nmodels to real-world environments. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\nACL 2023, Toronto, Canada, July 9-14, 2023, pages\n4928–4949. Association for Computational Linguis-\ntics.\nGaole He, Yunshi Lan, Jing Jiang, Wayne Xin Zhao, and\nJi-Rong Wen. 2021. Improving multi-hop knowledge\nbase question answering by learning intermediate\nsupervision signals. In WSDM ’21, The Fourteenth\nACM International Conference on Web Search and\nData Mining, Virtual Event, Israel, March 8-12, 2021,\npages 553–561. ACM.\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\nMüller, Francesco Piccinno, and Julian Martin Eisen-\nschlos. 2020. Tapas: Weakly supervised table parsing\nvia pre-training. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, ACL 2020, Online, July 5-10, 2020, pages\n4320–4333.\nGautier Izacard, Patrick S. H. Lewis, Maria Lomeli,\nLucas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with\nretrieval augmented language models. CoRR,\nabs/2208.03299.\nJinhao Jiang, Kun Zhou, Xin Zhao, and Ji-Rong Wen.\n2023. Unikgqa: Unified retrieval and reasoning for\nsolving multi-hop question answering over knowl-\nedge graph. In The Eleventh International Confer-\nence on Learning Representations, ICLR 2023, Ki-\ngali, Rwanda, May 1-5, 2023. OpenReview.net.\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish Sab-\nharwal, Oyvind Tafjord, Peter Clark, and Hannaneh\nHajishirzi. 2020. Unifiedqa: Crossing format bound-\naries with a single QA system. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2020, Online Event, 16-20 November 2020, Findings\nof ACL.\nYunshi Lan, Gaole He, Jinhao Jiang, Jing Jiang,\nWayne Xin Zhao, and Ji-Rong Wen. 2021. A sur-\nvey on complex knowledge base question answering:\nMethods, challenges and solutions. In Proceedings\nof the Thirtieth International Joint Conference on\nArtificial Intelligence, IJCAI 2021, Virtual Event /\nMontreal, Canada, 19-27 August 2021, pages 4483–\n4491. ijcai.org.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguis-\ntics, ACL 2020, Online, July 5-10, 2020, pages 7871–\n7880.\nBelinda Z. Li, Sewon Min, Srinivasan Iyer, Yashar\nMehdad, and Wen-tau Yih. 2020. Efficient one-pass\nend-to-end entity linking for questions. In Proceed-\nings of the 2020 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2020, Online,\nNovember 16-20, 2020, pages 6433–6441. Associa-\ntion for Computational Linguistics.\nHaoyang Li, Jing Zhang, Cuiping Li, and Hong\nChen. 2023. Decoupling the skeleton parsing and\nschema linking for text-to-sql. arXiv preprint\narXiv:2302.05965.\nChen Liang, Mohammad Norouzi, Jonathan Berant,\nQuoc V . Le, and Ni Lao. 2018. Memory augmented\npolicy optimization for program synthesis and se-\nmantic parsing. In Advances in Neural Information\nProcessing Systems 31: Annual Conference on Neu-\nral Information Processing Systems 2018, NeurIPS\n2018, December 3-8, 2018, Montréal, Canada, pages\n10015–10027.\nAiwei Liu, Xuming Hu, Lijie Wen, and Philip S. Yu.\n2023a. A comprehensive evaluation of chatgpt’s zero-\nshot text-to-sql capability. CoRR, abs/2303.13547.\nAiwei Liu, Xuming Hu, Lijie Wen, and Philip S.\nYu. 2023b. A comprehensive evaluation of\nchatgpt’s zero-shot text-to-sql capability. CoRR,\nabs/2303.13547.\nQian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi\nLin, Weizhu Chen, and Jian-Guang Lou. 2022.\nTAPEX: table pre-training via learning a neural SQL\nexecutor. In The Tenth International Conference on\nLearning Representations, ICLR 2022, Virtual Event,\nApril 25-29, 2022.\nQian Liu, Fan Zhou, Zhengbao Jiang, Longxu Dou, and\nMin Lin. 2023c. From zero to hero: Examining the\npower of symbolic tasks in instruction tuning. CoRR,\nabs/2304.07995.\nAlexander H. Miller, Adam Fisch, Jesse Dodge, Amir-\nHossein Karimi, Antoine Bordes, and Jason Weston.\n2016. Key-value memory networks for directly read-\ning documents. In Proceedings of the 2016 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, EMNLP 2016, Austin, Texas, USA, Novem-\nber 1-4, 2016, pages 1400–1409.\nFedor Moiseev, Zhe Dong, Enrique Alfonseca, and Mar-\ntin Jaggi. 2022. SKILL: structured knowledge infu-\nsion for large language models. In Proceedings of the\n2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, NAACL 2022, Seattle,\nWA, United States, July 10-15, 2022 , pages 1581–\n1588. Association for Computational Linguistics.\n9247\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\nXu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen\nKrueger, Kevin Button, Matthew Knight, Benjamin\nChess, and John Schulman. 2021. Webgpt: Browser-\nassisted question-answering with human feedback.\nCoRR.\nBarlas Oguz, Xilun Chen, Vladimir Karpukhin,\nStan Peshterliev, Dmytro Okhonko, Michael Sejr\nSchlichtkrull, Sonal Gupta, Yashar Mehdad, and\nScott Yih. 2022. Unik-qa: Unified representations\nof structured and unstructured knowledge for open-\ndomain question answering. In Findings of the Asso-\nciation for Computational Linguistics: NAACL 2022,\nSeattle, WA, United States, July 10-15, 2022, pages\n1535–1546. Association for Computational Linguis-\ntics.\nOpenAI. 2023. GPT-4 technical report. CoRR,\nabs/2303.08774.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\nJohn Schulman, Jacob Hilton, Fraser Kelton, Luke\nMiller, Maddie Simens, Amanda Askell, Peter Welin-\nder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\n2022a. Training language models to follow instruc-\ntions with human feedback. In NeurIPS.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\nJohn Schulman, Jacob Hilton, Fraser Kelton, Luke\nMiller, Maddie Simens, Amanda Askell, Peter Welin-\nder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\n2022b. Training language models to follow instruc-\ntions with human feedback. In NeurIPS.\nPanupong Pasupat and Percy Liang. 2015. Compo-\nsitional semantic parsing on semi-structured tables.\nIn Proceedings of the 53rd Annual Meeting of the\nAssociation for Computational Linguistics and the\n7th International Joint Conference on Natural Lan-\nguage Processing of the Asian Federation of Natural\nLanguage Processing, ACL 2015, July 26-31, 2015,\nBeijing, China, Volume 1: Long Papers, pages 1470–\n1480.\nBaolin Peng, Michel Galley, Pengcheng He, Hao Cheng,\nYujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou\nYu, Weizhu Chen, and Jianfeng Gao. 2023. Check\nyour facts and try again: Improving large language\nmodels with external knowledge and automated feed-\nback. CoRR, abs/2302.12813.\nJiexing Qi, Jingyao Tang, Ziwei He, Xiangpeng Wan,\nChenghu Zhou, Xinbing Wang, Quanshi Zhang, and\nZhouhan Lin. 2022. Rasat: Integrating relational\nstructures into pretrained seq2seq model for text-to-\nsql. arXiv preprint arXiv:2205.06983.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485–5551.\nNitarshan Rajkumar, Raymond Li, and Dzmitry Bah-\ndanau. 2022. Evaluating the text-to-sql capabilities\nof large language models. CoRR.\nApoorv Saxena, Aditay Tripathi, and Partha P. Taluk-\ndar. 2020. Improving multi-hop question answering\nover knowledge graphs using knowledge base embed-\ndings. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, ACL\n2020, Online, July 5-10, 2020, pages 4498–4507.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\nCoRR.\nHaitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn\nMazaitis, Ruslan Salakhutdinov, and William W. Co-\nhen. 2018. Open domain question answering using\nearly fusion of knowledge bases and text. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, Brussels, Belgium,\nOctober 31 - November 4, 2018, pages 4231–4242.\nYiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu,\nYongrui Chen, and Guilin Qi. 2023. Evaluation of\nchatgpt as a question answering system for answering\ncomplex questions. CoRR, abs/2303.07992.\nBailin Wang, Richard Shin, Xiaodong Liu, Oleksandr\nPolozov, and Matthew Richardson. 2020. Rat-sql:\nRelation-aware schema encoding and linking for text-\nto-sql parsers. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 7567–7578.\nXiaokai Wei, Shen Wang, Dejiao Zhang, Parminder\nBhatia, and Andrew O. Arnold. 2021. Knowledge\nenhanced pretrained language models: A compresh-\nensive survey. CoRR, abs/2110.08455.\nTianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong,\nTorsten Scholak, Michihiro Yasunaga, Chien-Sheng\nWu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Vic-\ntor Zhong, Bailin Wang, Chengzu Li, Connor Boyle,\nAnsong Ni, Ziyu Yao, Dragomir Radev, Caiming\nXiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,\nLuke Zettlemoyer, and Tao Yu. 2022. Unifiedskg:\nUnifying and multi-tasking structured knowledge\ngrounding with text-to-text language models. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, EMNLP 2022,\nAbu Dhabi, United Arab Emirates, December 7-11,\n2022, pages 602–631.\nYunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei\nHuang, and Yongbin Li. 2023. Large language mod-\nels are versatile decomposers: Decompose evidence\nand questions for table-based reasoning. CoRR.\n9248\nWen-tau Yih, Matthew Richardson, Christopher Meek,\nMing-Wei Chang, and Jina Suh. 2016. The value of\nsemantic parse labeling for knowledge base question\nanswering. In Proceedings of the 54th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2016, August 7-12, 2016, Berlin, Germany, Vol-\nume 2: Short Papers. The Association for Computer\nLinguistics.\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,\nDongxu Wang, Zifan Li, James Ma, Irene Li,\nQingning Yao, Shanelle Roman, Zilin Zhang, and\nDragomir R. Radev. 2018. Spider: A large-scale\nhuman-labeled dataset for complex and cross-domain\nsemantic parsing and text-to-sql task. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, Brussels, Belgium,\nOctober 31 - November 4, 2018, pages 3911–3921.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona T. Diab, Xian Li, Xi Victoria Lin,\nTodor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shus-\nter, Daniel Simig, Punit Singh Koura, Anjali Srid-\nhar, Tianlu Wang, and Luke Zettlemoyer. 2022.\nOPT: open pre-trained transformer language mod-\nels. CoRR, abs/2205.01068.\nYuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexan-\nder J. Smola, and Le Song. 2018. Variational reason-\ning for question answering with knowledge graph. In\nProceedings of the Thirty-Second AAAI Conference\non Artificial Intelligence, (AAAI-18), the 30th inno-\nvative Applications of Artificial Intelligence (IAAI-\n18), and the 8th AAAI Symposium on Educational\nAdvances in Artificial Intelligence (EAAI-18), New\nOrleans, Louisiana, USA, February 2-7, 2018, pages\n6069–6076.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen\nYang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,\nPeiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A\nsurvey of large language models. CoRR.\nVictor Zhong, Caiming Xiong, and Richard Socher.\n2017. Seq2sql: Generating structured queries\nfrom natural language using reinforcement learning.\nCoRR.\n9249\nTable 4: Results of different version of ChatGPT for\nWebQSP, WTQ, and Spider.\nMethods WebQSP WTQ Spider\nChatGPT (June) 61.2 43.3 70.1\nChatGPT (June) + IRR 72.6 48.4 74.8\nChatGPT (August) 62.1 41.1 75.2\nChatGPT (August) + IRR 75.3 50.4 77.1\nA Experiment With Latest Version of\nLLM\nWe have noted that the ChatGPT is continuously\nevolving. Furthermore, we have conducted sup-\nplementary experiments on three datasets using\nthe latest August version of LLM. The results are\npresented in the Table 4. It is noteworthy that Chat-\nGPT indeed continuously evolves, as evidenced by\nits distinct performance compared to that of the\nJune version. Although the evolved ChatGPT un-\nderperforms compared to the June version on the\nWTQ dataset, our approach can consistently fur-\nther enhances the ChatGPT performance with the\nevolved version on all three tasks. It indicates the\nrobustness of our proposed method.\nB Case Study\nHere, we select one representative example for\neach type of structured data and present the case\nstudy in Figure 4. For KG, we first invoke the Ex-\ntract_Neighbor_Relations function to extract the\nneighboring relations (e.g., birthplace, residence,\nand education) of the topic entity “ Harper Lee”,\nthen linearize them and compose the input prompt.\nIn the prompt, we utilize the instruction (i.e., pro-\nvide only one relevant relation that’s present in\nthe candidate) to elicit the LLM to generate the\nmost relevant relation, i.e., education. Based on\nthe selected relation, we further invoke the Ex-\ntract_Triples function to extract the triples with\nthe relation to the topic entity. After linearization,\nanother instruction (i.e., you just need to provide\nonly one answer entity), is adopted for guiding\nthe LLM to generate the final answer, i.e., Monroe\nCounty High School.\nFor table, we first invoke the Ex-\ntract_Column_Name function to extract the\ncolumn names from the table for linearization,\nand then design the prompt ( i.e., which columns\nare most relevant to answering the question?)\nfor the LLM to select the useful columns, i.e.,\nDistrict and Incumbent . Then, by using the\nExtract_Columns and Extract_SubTable functions\nand proper instructions, we elicit the LLM to select\nthe useful row indices ( i.e., item 8) and finally\ngenerate the answer (i.e., 19th).\nFor database, we also first invoke the Ex-\ntract_Table&Column_Name to extract all the ta-\nble names and column names, linearize them and\nutilize the instruction ( i.e., which tables do you\nneed to complete the SQLite SQL query?) to\nprompt the LLM. Then, based on the selected ta-\nbles (i.e., Dogs and Breeds), we further invoke the\nExtract_Tables_Information function and prompt\nthe LLM via an instruction ( i.e., complete sqlite\nSQL query only with no explanation) to generate\nthe SQL for the question, which can be executed to\nobtain the final answer.\n9250\nQuestion: What is the name of the breed with the most dogs?\nInvoke: Extract_Table&Column_Name (database)\nDogs | Breeds\n### Complete sqlite SQL query only and ….\n# Breeds(*, breed_code, breed_name); …\n### What is the name of the breed with the\nmost dogs? SELECT\nBreeds.breed_name FROM Dogs JOIN …\n### Here are the SqliteSQL tables …\n# Breeds(*, breed_code, breed_name); …\n### What is the name of the breed with the most \ndogs? Which tables do you need to complete the \nSQLite SQL query? \ndog_id ···breed_code\nLinearize: “Dogs … Breeds” \nInvoke: Extract_Tables_Information ([Dogs, Breeds])\nLinearize: “Dogs(dog_id, …);…”\nReturn: [Dogs … Breeds]\nbreed_code ···breed_name\nReturn: [Dogs(dog_id, …)…]\nGenerate\nGenerate\n(c) Case of  Spider (Text-to-SQL)\nDogs\nBreeds\n······\nDogs.breed_code →Breed.breed_code\nTable name Column names\ndog_id ···breed_code\nbreed_code ···breed_name\nDogs\nBreeds\n······\nDogs.breed_code →Breed.breed_code\nTable name Column names\nQuestion: what highschool did harper lee go to? Invoke: Extract_Neighbor_Relations (Harper Lee)\nReturn: [education … birthplace]\nGenerate\n“(Harper Lee, education, CVT_0); … ”\nExtract_Triples (Harper Lee, [education])\nThe candidate relations: education … birthplace.\nThe question is …\nProvide only one relevant relation that's present \nin the candidates …\nThe relevant relation: education. \nThe triples are: (Harper Lee, education, \nCVT_0) ... Based on these triples … give me \nthe final answer entity.  \nYou just need to provide only one answer entity. \nIf you think …\nAnswer: Monroe County High SchoolGenerate\nReturn: \n[(Harper Lee, education, CVT_0) … ]\nLinearize: \nLinearize: “education … birthplace”\n(a) Case of WebQSP (KGQA)\nCVT_0\nOxfordcollege\nHarpe Lee\nCVT_1\neducation\neducation\nMonroe \nCounty\nhigh \nschool\ntype\ntype\n···\n···\ninstitution\ninstitution\n···residence\nbirthplace\n···\nCVT_0\nOxfordcollege\nHarpe Lee\nCVT_1\neducation\neducation\nMonroe \nCounty\nhigh \nschool\ntype\ntype\n···\n···\ninstitution\ninstitution\n···residence\nbirthplace\n···\nInvoke: \nInvoke: \nExtract_Columns (table , [District…])\n1st ···\n···19st charles \nhawkins\nMarty \nWilliams\nJohn \nMiller …\nRobert \nHurt ...\nQuestion: In what district was the incumbent charles hawkins? Invoke: Extract_Column_Name (table)\nTo answer … first look at the available columns \nin the table: “District”, “Incumbent”, ... \nWhich columns are most relevant to answering \nthe question? …\nColumns: District, Incumbent.\nTo answer … Below is the list of rows ….\nrow 1: (District, 1st); (Incumbent, Marty\nWilliams) … (2007 Result … )\nwhich rows should be considered? …\nThe table contains:\nrow 1: (District, 19th); (Incumbent, Charles\nHawkins).\nUsing this information, In what district was the\nincumbent Charles Hawkins? …\nResturn: [Didtrict, …, 2007 Result]\nReturn: [(row 1, (District, 19th)…)…]\nInvoke: \nExtract_SubTable (table, [District…], [19])\nReturn: [(row 1, (District, 19th), …]\nLinearize: “District, …, 2007 Result”\nLinearize: “(row 1, (District, 19th), …)”\nLinearize: “row 1: (District, 1st); …”\nGenerate\nGenerate\nGenerate\nRows: row 19\nAnswer: 19th\n··· ··· ······\nDistrict Incumbent 2007 \nResult···\n1st ···\n···19st\ncharles \nhawkins\nMarty \nWilliams\nJohn \nMiller …\nRobert \nHurt ...\n··· ··· ······\nDistrict Incumbent 2007 \nResult···\n1st ···\n···19st charles \nhawkins\nMarty \nWilliams\nJohn \nMiller …\nRobert \nHurt ...\n··· ··· ······\nDistrict Incumbent 2007 \nResult···\n(b) Case of  WTQ (TableQA)\nFigure 4: Case study of our method on KGQA, TableQA and Text-to-SQL task.\n9251"
}